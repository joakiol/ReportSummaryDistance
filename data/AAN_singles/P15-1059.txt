Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 606?615,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsBuilding a Scientific Concept Hierarchy Database (SCHBASE)Eytan AdarUniversity of MichiganAnn Arbor, MI 48104eadar@umich.eduSrayan DattaUniversity of MichiganAnn Arbor, MI 48104srayand@umich.eduAbstractExtracted keyphrases can enhance numer-ous applications ranging from search totracking the evolution of scientific dis-course.
We present SCHBASE, a hier-archical database of keyphrases extractedfrom large collections of scientific liter-ature.
SCHBASE relies on a tendencyof scientists to generate new abbrevia-tions that ?extend?
existing forms as aform of signaling novelty.
We demon-strate how these keyphrases/concepts canbe extracted, and their viability as adatabase in relation to existing collections.We further show how keyphrases canbe placed into a semantically-meaningful?phylogenetic?
structure and describe keyfeatures of this structure.
The com-plete SCHBASE dataset is available at:http://cond.org/schbase.html.1 IntroductionDue to the immense practical value to Informa-tion Retrieval and other text mining applications,keyphrase extraction has become an extremelypopular topic of research.
Extracted keyphrases,specifically those derived from scientific literature,support search tasks (Anick, 2003), classificationand tagging (Medelyan et al, 2009), informa-tion extraction (Wu and Weld, 2008), and higher-level analysis such as the tracking of influence anddynamics of information propagation (Shi et al,2010; Ohniwa et al, 2010).
In our own workwe use the extracted hierarchies to predict scien-tific emergence based on how rapidly new vari-ants emerge.
Keyphrases themselves capture adiverse set of scientific language (e.g., methods,techniques, materials, phenomena, processes, dis-eases, devices).Keyphrases, and their uses, have been stud-ied extensively (Gil-Leiva and Alonso-Arroyo,2007).
However, automated keyphrase extrac-tion work has often focused on large-scale statis-tical techniques and ignored the scientific com-munication literature.
This literature points tothe complex ways in which keyphrases are cre-ated in light of competing demands: expressive-ness, findability, succinct writing, signaling nov-elty, signaling community membership, and soon (Hartley and Kostoff, 2003; Ibrahim, 1989;Grange and Bloom, 2000; Gil-Leiva and Alonso-Arroyo, 2007).
Furthermore, the tendency to ex-tract keyphrases through statistical mechanismsoften leads to flat keyphrase spaces that make anal-ysis of evolution and emergence difficult.Our contention, and the main motivation be-hind our work, is that we can do better by lever-aging explicit mechanisms adopted by authorsin keyphrase generation.
Specifically, we focuson a tendency to expand keyphrases by addingterms, coupled with a pressure to abbreviate toretain succinctness.
As we argue below, scien-tific communication has evolved the use of ab-breviations to deal with various constraints.
Ab-breviations, and acronyms specifically, are rela-tively new in many scientific domains (Grange andBloom, 2000; Fandrych, 2008) but are now ubiq-uitous (Ibrahim, 1989; Cheng, 2010).Keyphrase selection is often motivated byincreasing article findability within a domain(thereby increasing citation).
This strategy leadsto keyphrase reuse.
A competing pressure, how-ever, is to signal novelty in an author?s work whichis often done by creating new terminology (e.g.,creating a ?brand?
around a system or idea).
For606example, a machine learning expert working ona new type of Support Vector Machine will wanttheir article found when someone searches for?Support Vector Machine,?
but will also want toadd their own unique brand.
In response, they willoften augment the original keyphrase (e.g., ?Least-Squares Support Vector Machine?)
rather than in-venting a completely new one.
Unfortunately,continuous expansion will soon render a paper un-readable (e.g., one of many extensions to Poly-merase Chain Reaction is Standard Curve Quan-titative Competitive Reverse Transcription Poly-merase Chain Reaction).
Thus emerges a secondstrategy: abbreviation.Our assertion is that abbreviations are a keymechanism for resolving competing demands.Authors can simultaneously expand keyphrases,thus maintaining both findability and novelty,while at the same time addressing the need to besuccinct and non-repetitive.
Of interest to us isthe phenomena that if a new keyphrase expandsan existing keyphrase that has an established ab-breviation, the new keyphrase will also be ab-breviated (e.g., LS-SVM and SVM).
This ten-dency allows us to construct hierarchies of evolvedkeyphrases (rather than assuming a flat keyphrasespace) which can be leveraged to identify emer-gence, keyphrase ?mash-ups,?
and perform otherhigh level analysis.
As we demonstrate below,edges represent the rough semantic of EXTENDSor ISSUBTYPEOF.
So if keyphrase A is connectedto B, we can say A is a subtype of B (e.g., A is?Least-Squares Support Vector Machine?
and B is?Support Vector Machine?
).In this paper we introduce SCHBASE, a hi-erarchical database of keyphrases.
We demon-strate how we can simply, but effectively, extractkeyphrases by mining abbreviations from scien-tific literature and composing those keyphrasesinto semantically-meaningful hierarchies.
We fur-ther show that abbreviations are a viable mech-anism for building a domain-specific keyphrasedatabase by comparing our extracted keyphrasesto a number of author-defined and automatically-created keyphrase corpora.
Finally, we illustratehow authors build upon each others?
terminologyover time to create new keyphrases.11Full database available at: http://cond.org/schbase.html2 Related WorkInitial work in keyphrase extraction utilizedheuristics that were based on the understood struc-ture of scientific documents (Edmundson, 1969).As more data became available, it was possibleto move away from heuristic cues and to lever-age statistical techniques (Paice and Jones, 1993;Turney, 2000; Frank et al, 1999) that could iden-tify keyphrases within, and between, documents.The guiding model in this approach is that phrasesthat appear as statistical ?anomalies?
(by somemeasure) are effective for summarizing a docu-ment or corpus.
This style of keyphrase extrac-tion represents much of the current state-of-the-art (Kim et al, 2010).
Specific extensions in thisspace involve the use of network structures (Mi-halcea and Tarau, 2004; Litvak and Last, 2008;Das Gollapalli and Caragea, 2014), part-of-speechfeatures (Barker and Cornacchia, 2000; Hulth,2003), or more sophisticated metrics (Tomokiyoand Hurst, 2003).However, as we note above, these statistical ap-proaches largely ignore the underlying tensions inscientific communication that lead to the creationof new keyphrases and how they are signaled toothers.
The result is that these techniques oftenfind statistically ?anomalous?
phrases which oftenare not valid scientific concepts (but are simply un-common phrasing), are unstructured and discon-nected, and inflexible to size variance (as in thecase of fixed length n-grams), and fail to captureextremely rare terminology.The idea that abbreviations may be useful forkeyphrase extraction has been partially realized.Nguyen et al, (2007) found that they could pro-duce better keyphrases by extending existing mod-els (Frank et al, 1999) to include an acronym in-dicator as a feature.
That is, if a candidate phrasehad an associated parenthetical acronym associ-ated with it in the text a binary feature would beset.
This approach has been implemented by oth-ers (Bordea and Buitelaar, 2010).
We propose toexpand on this idea by implementing a simple, buteffective, solution by performing abbreviation ex-traction to build a hierarchical keyphrase database?
a form of open-information extraction (Etzioniet al, 2008) on large scientific corpora.3 Keyphrases and HierarchiesOur high level strategy for finding an initial setof keyphrases is to mine a corpus for abbrevia-607tion expansions.
This is a simple strategy, butas we show below, highly effective.
Though theidea that abbreviations and keyphrases are linkedfits within our understanding of scientific writing,we confirmed our intuition through a small exper-iment.
Specifically, we looked at the 85 uniquekeyphrases (in this case, article titles) listed inthe Wikipedia entry for List of Machine LearningConcepts (Wikipedia, 2014).
These ranged fromwell known terms (e.g., Support Vector Machinesand Autoencoders) to less known (e.g., Informa-tion fuzzy networks).
In all 85 cases we were ableto find an abbreviation on the Web (using Google)alongside the expansion (e.g., searching for thephrases ?Support Vector Machines (SVMs)?
or?Information Fuzzy Networks (IFN)?).
Thoughthere may be bias in the use of abbreviations inthe Machine Learning literature, our experiencehas been that this holds in other domains as well.When a scientific keyphrase is used often enough,someone, somewhere, will have abbreviated it.3.1 Abbreviation ExtractionTo find all abbreviation expansions we use the un-supervised SaRAD algorithm (Adar, 2004).
Thisalgorithm is simple to implement, does not re-quire extremely large amounts of data, works forboth acronyms and more general abbreviations,and has been demonstrated as effective in variouscontexts (Adar, 2004; Schwartz and Hearst, 2003).However, our solution does not depend on a spe-cific implementation, only that we are able to ac-curately identify abbreviation expansions.Adar (2004) presents the full details for the al-gorithm, but for completeness we present the highlevel details.
The algorithm progresses by identi-fying abbreviations inside of parentheses (definedas single words with at least one capital letter).The algorithm then extracts a ?window?
of textpreceding the parenthesis, up to n words long(where n is the character length of the abbrevia-tion plus padding).
This window does not crosssentence boundaries.
Within the window all possi-ble ?explanations?
of the abbreviation are derived.An explanation consists of a continuous sub-sequence of words that contain all the charactersof the original abbreviation in order.
For example,the window ?determine the geographical distribu-tion of ribonucleic acid?
preceding the abbrevia-tion ?RNA?
includes the explanations: ?determinethe geographical,?
?graphical distribution of ri-bonucleic acid?
and ?ribonucleic acid?
(matchingcharacters in italics).
In the example above thereare ten explanations (five unique).
Each explana-tion is scored heuristically: 1 point for each ab-breviation character at the start of a word; 1 pointsubtracted for every word between the explanationand the parenthesis; 1 point bonus if the explana-tion is adjacent to the parenthesis; 1 point sub-tracted for each extra word beyond the abbrevia-tion length.
For the explanations above, the scoresare ?4, 0, and 3 respectively.
The highest scor-ing match (we require a minimum of 1 point) isreturned as the mostly likely expansion.In practice, pairs of extracted abbrevia-tions/expansions are pulled from a large textualcorpus.
This both allows us to identify vari-ants of expansions (e.g., different pluralization,spelling, hyphenation, etc.)
as well as findingmore plausible expansions (those that are repeatedmultiple times in a corpus).
Thus, each ex-pansion/abbreviation pair has an associated countwhich can be used to threshold and filter for in-creased quality.
To discard units of measurement,single letter abbreviations and single word expan-sions are removed.
We return to this decisionlater, but our experience is also that single wordkeyphrases are rare.
Additionally, expansions con-taining brackets are not considered as they usuallyrepresent mathematical formulae.3.1.1 The ABBREVCORPUSIn our experiments we utilize the ACM Digital Li-brary (ACMDL) as our main corpus.
Though theACMDL is more limited than other collections,it has a number of desirable properties: spanningnearly the entire history (1954-2011) of a domain(Computer Science) with full-text and clean meta-data.
The corpus itself contains both journal andconference articles (77k and 197k, respectively).In addition to the filtering rules describedabove, we manually constructed a set of fil-ter terms to remove publication venues, agen-cies, and other institutions: ?university?, ?confer-ence?, ?symposium?, ?journal?, ?foundation?, ?con-sortium?, ?agency?, ?institute?
and ?school?
are dis-carded.
We further normalize our keyphrases bylowercasing, removing hyphens, and using theSnowball stemmer (Porter, 2001) to merge plu-ral variants.
After stemming and normalizing, wefound a total of 155,957 unique abbreviation ex-pansions.
Among these, 48,890 expansions occurmore than once, 25,107 expansions thrice or more608and 16,916 expansions four or more times.
We re-fer to this collection as the ABBREVCORPUS.For each keyphrase we search within the full-text corpus to identify set of documents containingthe keyphrase.
This allowed us to find both theearliest mention of the keyphrase (the expansion,not the abbreviation) as well as overall popularityof keyphrases.
We do not argue that abbreviationsare the norm in the introduction of new keyphrasesand may, in fact, only happen much later when thedomain is familiar enough with the phrase.To find the expansions in the full-text we uti-lize a modified suffix-tree that greedily findsthe longest-matching phrase and avoids ?double-counting?.
For example, if the text containsthe phrase, ?.
.
.
we utilize a Least-Squares Sup-port Vector Machine for .
.
.
?
it will matchagainst Least-Squares Support Vector Machine butnot Least Squares, Support Vector Machines, orSupport Vector (also keyphrases in our collec-tion).
The distribution of keyphrase frequency is apower-law (many keyphrases appearing once witha long tail) with exponent (?)
of 2.17 (fit usingClauset et al, (2009)).3.2 Building Keyphrase HierarchiesWe employ a very simple method of text con-tainment to build keyphrase hierarchies from AB-BREVCORPUS.
If a keyphrase A is a substringof keyphrase B, A is said to be contained by B(B ?
A).
If a third keyphrase, C, containsB and is contained by A, the containment linkbetween A and B is dropped and two new ones(A?
C and C ?
B) are added.
For example forthe keyphrases, circuit switching, optical circuitswitching and dynamic optical circuit switching,there are links from optical circuit switching to cir-cuit switching, and dynamic optical circuit switch-ing to optical circuit switching, but there is no linkfrom dynamic optical circuit switching to circuitswitching.
The hierarchies formed in this mannerare mostly trees, but in rare cases a keyphrase canhave links to multiple branches.
Example hierar-chies are displayed in Figure 1.For efficiency we sort all keyphrases by length(from largest to shortest) and iterate over each one,testing for containment in all previously ?seen?keyphrases.
This is computationally intensive,O(n2), but can be parallelized.A potential issue with string containment isthat negating prefixes can also appear (e.g., non-monotonic reasoning and monotonic reasoning).Our algorithm uses a dictionary of negations andcan annotate the results.
However, in practicewe find that only .6% of our data has a leadingnegating-prefix (?internal?
negating prefixes canalso be caught in this way, but are similarly rare).It is an application-specific question if we want toconsider such pairs as ?siblings?
or ?parent-child?
(with both supported).4 Overlap with Keyphrase CorporaTo test our newly-constructed keyphrase databasewe generate a mixture of human- and machine-built datasets to compare.
Our goal is to char-acterize both the intersection (keyphrases appear-ing in our corpus as well as the external datasets)as well as those keyphrases uniquely captured byeach dataset.4.1 ACM Author keyphrases (ACMCORPUS)The metadata for articles in ACM corpus containauthor-provided keyphrases.
In the corpus de-scribed above, we found 145,373 unique author-provided keyphrases after stemming and normal-ization.
We discard 16,418 single-word keywordsand those that do not appear in the full-text of anydocument.
We retain 116,246 keyphrases whichwe refer to as the ACMCORPUS.ACMCORPUSWIKICORPUSMSRACORPUSMESHCORPUSMESHCORPUSWIKICORPUSMSRACORPUSACMCORPUSFigure 2: Keyphrase counts for the ACMCOR-PUS (powerlaw ?
= 2.36), WIKICORPUS (2.49),MSRACORPUS (2.55) and MESHCORPUS (2.7)within the ACM full-text.4.2 Microsoft Academic (MSRACORPUS)Our second keyphrase dataset comes from the Mi-crosoft Academic (MSRA) search corpus (Mi-crosoft, 2015).
While particularly focused on609fault tolerance (1969)fault tolerance index (2006)software fault tolerance (1973)algorithm based fault tolerance (1984)partial fault tolerance (1975)byzantine fault tolerance (1991)practical byzantine fault tolerance (2000)geographic information (1973)volunteered geographic information (2008)geographic information network (2011)geographic information science (1996)geographic information science and technology (2010)geographic information services (2000)geographic information system (1975)geographic information retrieval (1976)geographic information systems and science (2003)Figure 1: Keyphrase hierarchy for Fault Tolerance (top) and Geographic Information (Bottom).
Colorsencode earliest appearance (brighter green is earlier)Computer Science, this collection contains arti-cles and keyphrases from over a dozen domains2.MSRA provides a list of keyphrases with uniqueIDs and different stemming variations of eachkeyphrase.
There are a total of 46,978 (withoutcounting stemming variations) of which 30,477keyphrases occur in ACM full-text corpus afterstemming and normalization (64% coverage).4.3 MeSH (MESHCORPUS)Medical Subject Headings (MeSH) (Lipscomb,2000) is set of subject headings or descriptors inthe life sciences domain.
For the purpose of ourwork, we use the 27,149 keyphrases from the 2014MeSH dataset.
Similar to the other keyphrase listswe only use stemmed and normalized multi-wordkeywords that occur in in the ACM full-text cor-pus, which is 4,363 in case of MeSH.4.4 Wikipedia (WIKICORPUS)Scientific article headings in Wikipedia can oftenbe used as a proxy for keyphrases.
To collect rele-vant titles, we find Wikipedia articles that exactlymatch (in title name) existing MeSH and MSRAkeyphrases.
For these ?seed?
articles, we com-pile their categories and mark all the articles inthese categories as potentially ?relevant.?
How-ever, as this also captures scientist names (e.g., a2We know these keyphrases are algorithmically derived,but the details are not disclosed.researcher?s page may be placed under the ?Com-puter Science?
category), research institutes andother non-keyphrase matches, we use the page?sinfobox as a further filter.
Pages containing ?per-son,?
?place,?
infoboxes, in ?book,?
?video game,?
?TV show?
or other related ?media?
category, andthose with geographical coordinates are removed.After applying these filters, we obtain 110,102unique article titles (after stemming) which wetreat as keyphrases.
Of these, 39,974 occur in theACM full-text corpus.4.5 ResultsThe total overlap for ACMCORPUS, MESH-CORPUS, MSRACORPUS and WIKICORPUS are14.12%, 12.28%, 32.33% and 17.41% respec-tively.
While these numbers seem low, it is worthnoting that many of these terms only appear oncein the ACM full-text corpus (see Figure 2).Figure 3 illustrates the relationship betweenthe number of times a keyphrase appears in thefull-text and the probability that it will appearin ABBREVCORPUS.
In all cases, the more of-ten a keyphrase appears in the corpus, the morelikely it is to have an abbreviation.
If we quali-tatively examine popular phrases that do not ap-pear in ABBREVCORPUS we find mathematicalforms (e.g., of-the-form, well-defined or a priori),and nouns/entities that are largely unrelated to sci-entific keyphrases (e.g., New Jersey, GovernmentAgency, and Private Sector).
More importantly,610the majority of phrases that are never abbreviatedare simply not Computer Science keyphrases (wereturn to this in Section 4.6).We were somewhat surprised by the poor over-lap of the ACMCORPUS, even for terms that werevery common in the full-text.
We found that thecause was a large set of ?bad?
keyphrases.
Specif-ically, 69.3k (69.5%) of author-defined keyphrases(occurring in ACMCORPUS but not in AB-BREVCORPUS) are used as a keyword in only onepaper.
However, they appear more than once inthe full-text ?
often many times.
For example,one author (and only one) used if and only if asa keyphrase, which matched a great many articles.The result is that there is little correlation betweenthe number of times a keyphrase appears in thefull-text and how many times it used explicitly asa keyphrase in the document metadata.
Becausethese will never be found as an abbreviation, they?pull?
the mean probability down.Instead of counting the number of times akeyphrase occurs in the full-text we generate a fre-quency count based on the number of times au-thors explicitly use it in the metadata.
This newcurve, labeled as ACMCORPUS (KEY) in Figure 3displays a very different tendency, with a rapidupward slope that peaks at 100% for frequently-occurring keyphrases.
Notably, only 16k (16%)keyphrases appear once in full-text but are neverabbreviated (far fewer than the 69.5% above).It is worth briefly considering those termsthat appear in ABBREVCORPUS and not in theother keyphrases lists.
We find roughly 17.6k,24.7k, 19.4k, and 21.4k terms that appear in AB-BREVCORPUS (with a threshold of 2 to elimi-nate ?noisy?
expansions), but not in ACMCOR-PUS, MESHCORPUS, MSRACORPUS, and WI-KICORPUS respectively.
As MeSH keyphrasestend to be focused on the biological keyphrasesthis is perhaps unsurprising but the high numbersfor the author-provided ACM keyphrases is unex-pected.
We find that some of the keyphrases thatare in ABBREVCORPUS but not in ACMCORPUSare highly specific (e.g., Multi-object EvolutionaryAlgorithm Based on Decomposition or StochasticVariable Graph Model).
However, many are alsoextremely generic terms that one would expect tofind in a computer science corpus: Run-Time Er-ror Detection, Parallel Execution Tree, and LittleEndian.
Our hypothesis is that these are often notthe focus of a paper and are unlikely to be selectedProbabiltyof Appearance inABBRCORPUSACMCORPUS (TEXT)ACMCORPUS (KEY)WIKICORPUSMSRACORPUSMESHCORPUSMESHCORPUSWIKICORPUSMSRACORPUSACMCORPUS(TEXT)ACMCORPUS (KEY)Figure 3: The probability of inclusion ofkeyphrases in ABBREVCORPUS based on fre-quency of appearance in full text or, in the case ifACMCORPUS (KEY), frequency of use as a key-word.
At frequency x, the y value represents prob-ability of appearence in ABBREVCORPUS if weonly consider terms that appear at least x times inthe other corpus.by the author.
We believe this provides further evi-dence of the viability of the abbreviation approachto generating good keyphrase lists.4.6 Domain keyphrasesWhen looking at keyphrases that appear in MESH-CORPUS but not in the ABBREVCORPUS we findthat many phrases do, in fact, appear in the fulltext but are never abbreviated.
For example, ColorPerception and Blood Cell both appear in ACMarticles but are not abbreviated.
Our hypothesis?which is motivated by the tendency of scientists toabbreviate terms that are deeply familiar to theircommunity (Grange and Bloom, 2000)?is thatterms that are possibly distant from the core do-main focus tend not to be abbreviated.
This is sup-ported by the fact that these terms are abbreviatedin other collections (e.g., one can find CP as an ab-breviation for Color Perception in psychology andcognition work and BC, for Blood Cell, in medi-cal and biological journals).
Additional evidenceis apparent in Figure 3 which shows that ACM-CORPUS keyphrases are more likely to be abbre-viated (with far fewer repeats necessary).
MSRA-CORPUS, which contains many Computer Sciencearticles, also has higher probabilities (though notnearly matching the ACM).To test this systematically, we calculated se-mantic similarity between each keyphrase in611the WikiCorpus dataset to ?computer science.
?Specifically, we utilize Explicit Semantic Anal-ysis (Gabrilovich and Markovitch, 2009) to cal-culate similarity.
In this method, every segmentof text is represented in a very high dimensionalspace in terms of keyphrases (based on Wikipediacategories).
The similarity score for each term isbetween 0 (unrelated) and 1 (very similar).Figure 4 demonstrates that with increasing sim-ilarity, the likelihood of abbreviation increases.From this, one may infer that to generate adomain-specific database that excludes unrelatedkeyphrases, the abbreviation-derived corpus ishighly appropriate.
Conversely, to get coverage ofkeyphrases from all scientific domains it is insuffi-cient to mine for abbreviations in one specific do-main?s text.
Even though a keyphrase may appearin the full-text it will simply never be abbreviated.Figure 4: Probability of a keyphrase appearing inABBREVCORPUS (y-axis) based on semantic sim-ilarity of the keyphrase to ?Computer Science?
(x-axis, binned exponentially for readability).4.7 Keyphrase HierarchiesOur hierarchy generation process (see Section 3.2)generated 1716 hierarchies accounting for 8661unique keyphrases.
Most of the hierarchies (1002or 58%) only contained two nodes (a root and onechild).
The degree distribution, aggregated acrossall hierarchies, is again power-law (?
= 2.895).Hierarchy sizes are power-law distributed (?
=2.807) and an average ?diameter?
(max height) of1.135.
The hierarchies contain a giant componentwith 2302 nodes and 2436 edges.While most of our hierarchies are trees,keyphrases can connect to two independentbranches.
For example, Least-Squares SupportVector Machines (LS-SVMs) appears in both theLeast Squares and Support Vector hierarchies.In total, 649 keyphrases appear in multiple hi-erarchies, the majority appearing 2.
Only 17keyphrases appear in 3 hierarchies.
For exam-ple, the particularly long Single Instruction Mul-tiple Thread Evolution Strategy Pattern Searchappears in the Evolution(ary) Strategy, PatternSearch, and Single-Instruction-Multiple-Threadhierarchies.
These collisions are interesting inthat they reflect a mash-ups of different concepts,and by extension, different sub-disciplines or tech-niques.
In some situations, where there is anoverlap in many sub-keyphrases, this may indicatethat two root keyphrases are in fact equivalent orhighly related (e.g., likelihood ratio and log likeli-hood).
We do not currently handle such ambiguityin SCHBASE.To test the semantic interpretation of edges asEXTENDS/ISSUBTYPEOF we randomly sampled200 edges and manually checked these.
We foundthat in 92% (184) this interpretation was cor-rect.
The remaining 16 were largely an artifactof normalization errors rather than a wrong ?type?
(e.g., ?session identifier?
and ?session id?
whereclearly a more accurate interpretation is ISEXPAN-SIONOF).
We believe it is fair to say that the hier-archies we construct are the ?skeleton?
of a fullEXTENDS hierarchy but one that is nonethelessfairly encompassing.
Our qualitative analysis isthat most keyphrases that share a type also share aroot keyphrase (e.g., ?classifier?
).It is interesting to consider if edges which arederived by ?containment?
reflect a temporal pat-tern.
That is, if keyphrase A EXTENDS B, doesthe first mention of A in the literature happen af-ter B?
We find that this is almost always the case.Among the 7136 edges generated by our algorithmonly 165 (2.3%) are ?reversed.?
Qualitatively, wefind that these instances appear either due to miss-ing data (the parent keyphrase first appeared out-side the ACM) or publication ordering (in somecases the difference in first-appearance is only ayear).
In most situations the date is only 1-2 yearsapart.
This high degree of consistency lends fur-ther support to the tendency of scientists to expandupon keyphrases over time.Figure 5 depicts the mean change in length of?children?
in keyphrase hierarchies.
The numbersdepicted are relative change.
Thus, at year ?0?,the year the root keyphrase is introduced, there isno relative increase.
Within 1 year, new childrenof that root are 50% larger in character length andafter that children continue to ?grow?
as authorsadd additional keyphrases.
A particularly obvious612example of this is the branch for Petri Net (PN)which was extended as Queueing Petri Net (QPN)and then Hierarchically Combined Queueing PetriNets (HCQPN) and finally Extended Hierarchi-cally Combined Queueing Petri Nets (EHCQPN).Notably, this may have implications to other ex-tractors that assume fixed-sized entities over thehistory of the collection.Figure 5: Average increase in character length ofsub-keyphrases over time5 Discussion and Future WorkOur decision to eliminate single-word keyphrasesfrom consideration is an explicit one.
Of the145k keyphrases in the original ACMCORPUS(pre-filtering), 16,418 (11.29%) were single-wordkeyphrases.
Our experience with the ACM author-defined keyphrases is that such terms are toogeneric to be useful as ?scientific?
keyphrases.
Forexample, In all the ACM proceedings, the top-5 most common single-word keyphrases are se-curity, visualization, evaluation, design, and pri-vacy.
Even in specific sub-domains, such as rec-ommender systems (Proceedings of Recsys), themost popular single-word keyphrases are person-alization, recommendation, evaluation, and trust.Contrast these to the most popular multi-wordterms: recommender system(s), collaborative fil-tering, matrix factorization, and social network(s).Notably, in the MSRA corpus, which is algo-rithmically filtered, only .46% (226 keyphrases)were single word.
MeSH, in contrast, has a full37% of keyphrases as single-term.
In most sit-uations these reflect chemical names (e.g., 382single-word enzymes) or biological structures.
Insuch a domain, and if these keyphrases are desir-able, it may be advisable to retain single-word ab-breviations.
While it may seem surprising, evensingle words are often abbreviated (e.g., Transal-dolase is ?T?
and Ultrafiltration is ?U?
or ?U/F?
).A second key observation is that while theACM full-text corpus is large, it is by no means?big.?
We selected to use it because it controlledand ?clean.?
However, we have also run our al-gorithms on the MSRA Corpus (which containsonly abstracts) and CiteSeer (which contains full-text).
Because the corpora contain more text wefind significantly higher overlap with the differ-ent keyphrase corpora.
However, this comes atthe cost of not being able to isolate the domain-specific keyphrases.
To put it differently, thebroader full-text collections enable to us gener-ate a more fleshed out keyphrase hierarchies thattracks keyphrases across all domains but whichmay not be appropriate for certain workloads.Finally, it is worth considering the possibilityof building hierarchies (and connecting them) byrelations other than ?containment.?
We have be-gun to utilize metrics such as co-occurrence ofkeyphrases (e.g., PMI) as well as higher level cita-tion and co-citation structure in the corpora.
Thus,we are able to connect terms that are highly relatedbut are textually dissimilar.
When experimentingwith PMI, for example, we have found a diverseset of edge types including ISUSEDFOR (e.g., ?n-gram language model?
and ?machine translation?
)or ISUSEDIN (e.g., ?Expectation Maximization?and ?Baum-Welch?
or ?euclidean algorithm?
and?k-means?).
By necessity, edges generated by thistechnique require an additional classification.6 SummaryWe have introduced SCHBASE, a simple, robust,and highly effective system and database of sci-entific concepts/keyphrases.
By leveraging theincentive structure of scientists to expand exist-ing ideas while simultaneously signaling noveltywe are able to construct semantically-meaningfulhierarchies of related keyphrases.
The furthertendency by authors to succinctly describe newkeyphrases results in a general habit of utilizingabbreviations.
We have demonstrated a mecha-nism to identify these keyphrases by extracting ab-breviation expansions and have shown that thesekeyphrases cover the bulk of ?useful?
keyphraseswithin the domain of the corpus.
We believethat SCHBASE will enable a number of appli-cations ranging from search, categorization, andanalysis of scientific communication patterns.613AcknowledgmentsThe authors thank the Microsoft Academic team,Jaime Teevan, Susan Dumais, and Carl Lagoze forproviding us with data and advice.
This work issupported by the Intelligence Advanced ResearchProjects Activity (IARPA) via Department of In-terior National Business Center contract numberD11PC20155.
The U.S. government is authorizedto reproduce and distribute reprints for Govern-mental purposes notwithstanding any copyrightannotation thereon.
Disclaimer: The views andconclusions contained herein are those of the au-thors and should not be interpreted as necessarilyrepresenting the official policies or endorsements,either expressed or implied, of IARPA, DoI/NBC,or the U.S. Government.ReferencesEytan Adar.
2004.
SaRAD: a simple and robust abbre-viation dictionary.
Bioinformatics, 20(4):527?533.Peter Anick.
2003.
Using terminological feedback forweb search refinement: A log-based study.
In Pro-ceedings of the 26th Annual International ACM SI-GIR Conference on Research and Development inInformaion Retrieval, SIGIR ?03, pages 88?95, NewYork, NY, USA.
ACM.Ken Barker and Nadia Cornacchia.
2000.
Using nounphrase heads to extract document keyphrases.
InHoward J. Hamilton, editor, Advances in ArtificialIntelligence, volume 1822 of Lecture Notes in Com-puter Science, pages 40?52.
Springer Berlin Heidel-berg.Georgeta Bordea and Paul Buitelaar.
2010.
Deriunlp:A context based approach to automatic keyphraseextraction.
In Proceedings of the 5th internationalworkshop on semantic evaluation, pages 146?149.Association for Computational Linguistics.Tsung O. Cheng.
2010.
What?s in a name?
another un-explained acronym!
International Journal of Cardi-ology, 144(2):291 ?
292.Aaron Clauset, Cosma Rohilla Shalizi, and Mark EJNewman.
2009.
Power-law distributions in empiri-cal data.
SIAM Review, 51(4):661?703.Sujatha Das Gollapalli and Cornelia Caragea.
2014.Extracting keyphrases from research papers usingcitation networks.
In Twenty-Eighth AAAI Confer-ence on Artificial Intelligence.Harold P Edmundson.
1969.
New methods in auto-matic extracting.
Journal of the ACM, 16(2):264?285, April.Oren Etzioni, Michele Banko, Stephen Soderland, andDaniel S. Weld.
2008.
Open information extrac-tion from the web.
Communications of the ACM,51(12):68?74, December.Ingrid Fandrych.
2008.
Submorphemic elements in theformation of acronyms, blends and clippings 147.Lexis, page 105.Eibe Frank, Gordon W. Paynter, Ian H. Witten, CarlGutwin, and Craig G. Nevill-Manning.
1999.Domain-specific keyphrase extraction.
In Proceed-ings of the 16th International Joint Conference onArtificial Intelligence - Volume 2, IJCAI?99, pages668?673, San Francisco, CA, USA.
Morgan Kauf-mann Publishers Inc.Evgeniy Gabrilovich and Shaul Markovitch.
2009.Wikipedia-based semantic interpretation for naturallanguage processing.
Journal of Artificial Intelli-gence Research, 34(1):443?498, March.Isidoro Gil-Leiva and Adolfo Alonso-Arroyo.
2007.Keywords given by authors of scientific articlesin database descriptors.
Journal of the AmericanSociety for Information Science and Technology,58(8):1175?1187.Bob Grange and D.A.
Bloom.
2000.
Acronyms,abbreviations and initialisms.
BJU International,86(1):1?6.James Hartley and Ronald N. Kostoff.
2003.
How use-ful are ?key words?
in scientific journals?
Journal ofInformation Science, 29(5):433?438.Anette Hulth.
2003.
Improved automatic keyword ex-traction given more linguistic knowledge.
In Pro-ceedings of the 2003 Conference on Empirical Meth-ods in Natural Language Processing, EMNLP ?03,pages 216?223, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.A.M.
Ibrahim.
1989.
Acronyms observed.
Pro-fessional Communication, IEEE Transactions on,32(1):27?28, Mar.Su Nam Kim, Olena Medelyan, Min-Yen Kan, andTimothy Baldwin.
2010.
Semeval-2010 task 5: Au-tomatic keyphrase extraction from scientific articles.In Proceedings of the 5th International Workshop onSemantic Evaluation, pages 21?26.
Association forComputational Linguistics.Carolyn E. Lipscomb.
2000.
Medical subject headings(mesh).
Bull Med Libr Assoc.
88(3): 265266.Marina Litvak and Mark Last.
2008.
Graph-basedkeyword extraction for single-document summariza-tion.
In Proceedings of the Workshop on Multi-source Multilingual Information Extraction andSummarization, MMIES ?08, pages 17?24, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.614Olena Medelyan, Eibe Frank, and Ian H. Witten.2009.
Human-competitive tagging using automatickeyphrase extraction.
In Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing: Volume 3 - Volume 3, EMNLP?09, pages 1318?1327, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Microsoft.
2015.
Microsoft academic search.http://academic.research.microsoft.com.
Accessed:2015-2-26.Rada Mihalcea and Paul Tarau.
2004.
Textrank:Bringing order into texts.
In Dekang Lin and DekaiWu, editors, Proceedings of EMNLP 2004, pages404?411, Barcelona, Spain, July.
Association forComputational Linguistics.ThuyDung Nguyen and Min-Yen Kan. 2007.Keyphrase extraction in scientific publications.In Dion Hoe-Lian Goh, Tru Hoang Cao, Inge-borg Torvik S?lvberg, and Edie Rasmussen, edi-tors, Asian Digital Libraries.
Looking Back 10 Yearsand Forging New Frontiers, volume 4822 of Lec-ture Notes in Computer Science, pages 317?326.Springer Berlin Heidelberg.Ryosuke L. Ohniwa, Aiko Hibino, and KunioTakeyasu.
2010.
Trends in research foci in lifescience fields over the last 30 years monitored byemerging topics.
Scientometrics, 85(1):111?127.Chris D. Paice and Paul A. Jones.
1993.
The iden-tification of important concepts in highly structuredtechnical papers.
In Proceedings of the 16th AnnualInternational ACM SIGIR Conference on Researchand Development in Information Retrieval, SIGIR?93, pages 69?78, New York, NY, USA.
ACM.Martin F. Porter.
2001.
Snowball:A language for stemming algorithms.http://snowball.tartarus.org/texts/introduction.html.Accessed: 2015-2-26.Ariel S Schwartz and Marti A Hearst.
2003.
A sim-ple algorithm for identifying abbreviation definitionsin biomedical text.
Pacific Symposium on Biocom-puting.
Pacific Symposium on Biocomputing, page451462.Xiaolin Shi, Jure Leskovec, and Daniel A. McFarland.2010.
Citing for high impact.
In Proceedings ofthe 10th Annual Joint Conference on Digital Li-braries, JCDL ?10, pages 49?58, New York, NY,USA.
ACM.Takashi Tomokiyo and Matthew Hurst.
2003.
A lan-guage model approach to keyphrase extraction.
InProceedings of the ACL 2003 Workshop on Multi-word Expressions: Analysis, Acquisition and Treat-ment - Volume 18, MWE ?03, pages 33?40, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Peter D. Turney.
2000.
Learning algorithmsfor keyphrase extraction.
Information Retrieval,2(4):303?336, May.Wikipedia.
2014.
Wikipedia: List of ma-chine learning concepts.
http://en.wiki-pedia.org/wiki/List of machine learning concepts.Accessed: 2015-2-26.Fei Wu and Daniel S. Weld.
2008.
Automatically re-fining the wikipedia infobox ontology.
In Proceed-ings of the 17th International Conference on WorldWide Web, WWW ?08, pages 635?644, New York,NY, USA.
ACM.615
