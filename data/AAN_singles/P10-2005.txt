Proceedings of the ACL 2010 Conference Short Papers, pages 22?26,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsDiversify and Combine: Improving Word Alignment for MachineTranslation on Low-Resource LanguagesBing Xiang, Yonggang Deng, and Bowen ZhouIBM T. J. Watson Research CenterYorktown Heights, NY 10598{bxiang,ydeng,zhou}@us.ibm.comAbstractWe present a novel method to improveword alignment quality and eventually thetranslation performance by producing andcombining complementary word align-ments for low-resource languages.
Insteadof focusing on the improvement of a singleset of word alignments, we generate mul-tiple sets of diversified alignments basedon different motivations, such as linguis-tic knowledge, morphology and heuris-tics.
We demonstrate this approach on anEnglish-to-Pashto translation task by com-bining the alignments obtained from syn-tactic reordering, stemming, and partialwords.
The combined alignment outper-forms the baseline alignment, with signif-icantly higher F-scores and better transla-tion performance.1 IntroductionWord alignment usually serves as the startingpoint and foundation for a statistical machinetranslation (SMT) system.
It has received a signif-icant amount of research over the years, notably in(Brown et al, 1993; Ittycheriah and Roukos, 2005;Fraser and Marcu, 2007; Hermjakob, 2009).
Theyall focused on the improvement of word alignmentmodels.
In this work, we leverage existing align-ers and generate multiple sets of word alignmentsbased on complementary information, then com-bine them to get the final alignment for phrasetraining.
The resource required for this approachis little, compared to what is needed to build a rea-sonable discriminative alignment model, for ex-ample.
This makes the approach especially ap-pealing for SMT on low-resource languages.Most of the research on alignment combinationin the past has focused on how to combine thealignments from two different directions, source-to-target and target-to-source.
Usually people startfrom the intersection of two sets of alignments,and gradually add links in the union based oncertain heuristics, as in (Koehn et al, 2003), toachieve a better balance compared to using eitherintersection (high precision) or union (high recall).In (Ayan and Dorr, 2006) a maximum entropy ap-proach was proposed to combine multiple align-ments based on a set of linguistic and alignmentfeatures.
A different approach was presented in(Deng and Zhou, 2009), which again concentratedon the combination of two sets of alignments, butwith a different criterion.
It tries to maximize thenumber of phrases that can be extracted in thecombined alignments.
A greedy search methodwas utilized and it achieved higher translation per-formance than the baseline.More recently, an alignment selection approachwas proposed in (Huang, 2009), which com-putes confidence scores for each link and prunesthe links from multiple sets of alignments usinga hand-picked threshold.
The alignments usedin that work were generated from different align-ers (HMM, block model, and maximum entropymodel).
In this work, we use soft voting withweighted confidence scores, where the weightscan be tuned with a specific objective function.There is no need for a pre-determined thresholdas used in (Huang, 2009).
Also, we utilize var-ious knowledge sources to enrich the alignmentsinstead of using different aligners.
Our strategy isto diversify and then combine in order to catch anycomplementary information captured in the wordalignments for low-resource languages.The rest of the paper is organized as follows.22We present three different sets of alignments inSection 2 for an English-to-Pashto MT task.
InSection 3, we propose the alignment combinationalgorithm.
The experimental results are reportedin Section 4.
We conclude the paper in Section 5.2 Diversified Word AlignmentsWe take an English-to-Pashto MT task as an exam-ple and create three sets of additional alignmentson top of the baseline alignment.2.1 Syntactic ReorderingPashto is a subject-object-verb (SOV) language,which puts verbs after objects.
People have pro-posed different syntactic rules to pre-reorder SOVlanguages, either based on a constituent parse tree(Dra?bek and Yarowsky, 2004; Wang et al, 2007)or dependency parse tree (Xu et al, 2009).
Inthis work, we apply syntactic reordering for verbphrases (VP) based on the English constituentparse.
The VP-based reordering rule we apply inthe work is:?
V P (V B?, ?)
?
V P (?, V B?
)where V B?
represents V B, V BD, V BG, V BN ,V BP and V BZ .In Figure 1, we show the reference alignmentbetween an English sentence and the correspond-ing Pashto translation, where E is the original En-glish sentence, P is the Pashto sentence (in ro-manized text), and E?
is the English sentence afterreordering.
As we can see, after the VP-based re-ordering, the alignment between the two sentencesbecomes monotone, which makes it easier for thealigner to get the alignment correct.
During thereordering of English sentences, we store the in-dex changes for the English words.
After gettingthe alignment trained on the reordered English andoriginal Pashto sentence pairs, we map the Englishwords back to the original order, along with thelearned alignment links.
In this way, the align-ment is ready to be combined with the baselinealignment and any other alternatives.2.2 StemmingPashto is one of the morphologically rich lan-guages.
In addition to the linguistic knowledge ap-plied in the syntactic reordering described above,we also utilize morphological analysis by applyingstemming on both the English and Pashto sides.For English, we use Porter stemming (Porter,SS           CC            SNP              VP            NP              VPPRP  VBP        NP                  VBP        NP        ADVPPRP$         NNS                     PRP       RBE:    they  are   your employees and you   know    them      wellP:  hQvy  stAsO   kArvAl   dy    Av  tAsO   hQvy    smh      pOZnBE?
: they  your  employees  are   and  you   them    well      knowFigure 1: Alignment before/after VP-based re-ordering.1980), a widely applied algorithm to remove thecommon morphological and inflexional endingsfrom words in English.
For Pashto, we utilizea morphological decompostion algorithm that hasbeen shown to be effective for Arabic speechrecognition (Xiang et al, 2006).
We start from afixed set of affixes with 8 prefixes and 21 suffixes.The prefixes and suffixes are stripped off fromthe Pashto words under the two constraints:(1)Longest matched affixes first; (2) Remaining stemmust be at least two characters long.2.3 Partial WordFor low-resource languages, we usually sufferfrom the data sparsity issue.
Recently, a simplemethod was presented in (Chiang et al, 2009),which keeps partial English and Urdu words in thetraining data for alignment training.
This is similarto the stemming method, but is more heuristics-based, and does not rely on a set of available af-fixes.
With the same motivation, we keep the first4 characters of each English and Pashto word togenerate one more alternative for the word align-ment.3 Confidence-Based AlignmentCombinationNow we describe the algorithm to combine mul-tiple sets of word alignments based on weightedconfidence scores.
Suppose aijk is an alignmentlink in the i-th set of alignments between the j-thsource word and k-th target word in sentence pair(S,T ).
Similar to (Huang, 2009), we define theconfidence of aijk asc(aijk|S, T ) =?qs2t(aijk|S, T )qt2s(aijk|T, S),(1)23where the source-to-target link posterior probabil-ityqs2t(aijk|S, T ) =pi(tk|sj)?Kk?=1 pi(tk?
|sj), (2)and the target-to-source link posterior probabilityqt2s(aijk|T, S) is defined similarly.
pi(tk|sj) isthe lexical translation probability between sourceword sj and target word tk in the i-th set of align-ments.Our alignment combination algorithm is as fol-lows.1.
Each candidate link ajk gets soft votes fromN sets of alignments via weighted confidencescores:v(ajk|S, T ) =N?i=1wi ?
c(aijk|S, T ), (3)where the weight wi for each set of alignmentcan be optimized under various criteria.
Inthis work, we tune it on a hand-aligned de-velopment set to maximize the alignment F-score.2.
All candidates are sorted by soft votes in de-scending order and evaluated sequentially.
Acandidate link ajk is included if one of thefollowing is true:?
Neither sj nor tk is aligned so far;?
sj is not aligned and its left or rightneighboring word is aligned to tk so far;?
tk is not aligned and its left or rightneighboring word is aligned to sj so far.3.
Repeat scanning all candidate links until nomore links can be added.In this way, those alignment links with higherconfidence scores have higher priority to be in-cluded in the combined alignment.4 Experiments4.1 BaselineOur training data contains around 70K English-Pashto sentence pairs released under the DARPATRANSTAC project, with about 900K words onthe English side.
The baseline is a phrase-basedMT system similar to (Koehn et al, 2003).
Weuse GIZA++ (Och and Ney, 2000) to generatethe baseline alignment for each direction and thenapply grow-diagonal-final (gdf).
The decodingweights are optimized with minimum error ratetraining (MERT) (Och, 2003) to maximize BLEUscores (Papineni et al, 2002).
There are 2028 sen-tences in the tuning set and 1019 sentences in thetest set, both with one reference.
We use another150 sentence pairs as a heldout hand-aligned setto measure the word alignment quality.
The threesets of alignments described in Section 2 are gen-erated on the same training data separately withGIZA++ and enhanced by gdf as for the baselinealignment.
The English parse tree used for thesyntactic reordering was produced by a maximumentropy based parser (Ratnaparkhi, 1997).4.2 Improvement in Word AlignmentIn Table 1 we show the precision, recall and F-score of each set of word alignments for the 150-sentence set.
Using partial word provides the high-est F-score among all individual alignments.
TheF-score is 5% higher than for the baseline align-ment.
The VP-based reordering itself does not im-prove the F-score, which could be due to the parseerrors on the conversational training data.
We ex-periment with three options (c0, c1, c2) when com-bining the baseline and reordering-based align-ments.
In c0, the weights wi and confidence scoresc(aijk|S, T ) in Eq.
(3) are all set to 1.
In c1,we set confidence scores to 1, while tuning theweights with hill climbing to maximize the F-score on a hand-aligned tuning set.
In c2, we com-pute the confidence scores as in Eq.
(1) and tunethe weights as in c1.
The numbers in Table 1 showthe effectiveness of having both weights and con-fidence scores during the combination.Similarly, we combine the baseline with eachof the other sets of alignments using c2.
Theyall result in significantly higher F-scores.
Wealso generate alignments on VP-reordered partialwords (X in Table 1) and compared B + X andB + V + P .
The better results with B + V + Pshow the benefit of keeping the alignments as di-versified as possible before the combination.
Fi-nally, we compare the proposed alignment combi-nation c2 with the heuristics-based method (gdf),where the latter starts from the intersection of all 4sets of alignments and then applies grow-diagonal-final (Koehn et al, 2003) based on the links inthe union.
The proposed combination approach onB + V + S + P results in close to 7% higher F-scores than the baseline and also 2% higher than24gdf.
We also notice that its higher F-score ismainly due to the higher precision, which shouldresult from the consideration of confidence scores.Alignment Comb P R FBaseline 0.6923 0.6414 0.6659V 0.6934 0.6388 0.6650S 0.7376 0.6495 0.6907P 0.7665 0.6643 0.7118X 0.7615 0.6641 0.7095B+V c0 0.7639 0.6312 0.6913B+V c1 0.7645 0.6373 0.6951B+V c2 0.7895 0.6505 0.7133B+S c2 0.7942 0.6553 0.7181B+P c2 0.8006 0.6612 0.7242B+X c2 0.7827 0.6670 0.7202B+V+P c2 0.7912 0.6755 0.7288B+V+S+P gdf 0.7238 0.7042 0.7138B+V+S+P c2 0.7906 0.6852 0.7342Table 1: Alignment precision, recall and F-score(B: baseline; V: VP-based reordering; S: stem-ming; P: partial word; X: VP-reordered partialword).4.3 Improvement in MT PerformanceIn Table 2, we show the corresponding BLEUscores on the test set for the systems built on eachset of word alignment in Table 1.
Similar to theobservation from Table 1, c2 outperforms c0 andc1, and B + V + S + P with c2 outperformsB + V + S + P with gdf.
We also ran one ex-periment in which we concatenated all 4 sets ofalignments into one big set (shown as cat).
Over-all, the BLEU score with confidence-based com-bination was increased by 1 point compared to thebaseline, 0.6 compared to gdf, and 0.7 comparedto cat.
All results are statistically significant withp < 0.05 using the sign-test described in (Collinset al, 2005).5 ConclusionsIn this work, we have presented a word alignmentcombination method that improves both the align-ment quality and the translation performance.
Wegenerated multiple sets of diversified alignmentsbased on linguistics, morphology, and heuris-tics, and demonstrated the effectiveness of com-bination on the English-to-Pashto translation task.We showed that the combined alignment signif-icantly outperforms the baseline alignment withAlignment Comb Links Phrase BLEUBaseline 963K 565K 12.67V 965K 624K 12.82S 915K 692K 13.04P 906K 716K 13.30X 911K 689K 13.00B+V c0 870K 890K 13.20B+V c1 865K 899K 13.32B+V c2 874K 879K 13.60B+S c2 864K 948K 13.41B+P c2 863K 942K 13.40B+X c2 871K 905K 13.37B+V+P c2 880K 914K 13.60B+V+S+P cat 3749K 1258K 13.01B+V+S+P gdf 1021K 653K 13.14B+V+S+P c2 907K 771K 13.73Table 2: Improvement in BLEU scores (B: base-line; V: VP-based reordering; S: stemming; P: par-tial word; X: VP-reordered partial word).both higher F-score and higher BLEU score.
Thecombination approach itself is not limited to anyspecific alignment.
It provides a general frame-work that can take advantage of as many align-ments as possible, which could differ in prepro-cessing, alignment modeling, or any other aspect.AcknowledgmentsThis work was supported by the DARPATRANSTAC program.
We would like to thankUpendra Chaudhari, Sameer Maskey and Xiao-qiang Luo for providing useful resources and theanonymous reviewers for their constructive com-ments.ReferencesNecip Fazil Ayan and Bonnie J. Dorr.
2006.
A max-imum entropy approach to combining word align-ments.
In Proc.
HLT/NAACL, June.Peter Brown, Vincent Della Pietra, Stephen DellaPietra, and Robert Mercer.
1993.
The mathematicsof statistical machine translation: parameter estima-tion.
Computational Linguistics, 19(2):263?311.David Chiang, Kevin Knight, Samad Echihabi, et al2009.
Isi/language weaver nist 2009 systems.
InPresentation at NIST MT 2009 Workshop, August.Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.2005.
Clause restructuring for statistical machinetranslation.
In Proc.
of ACL, pages 531?540.25Yonggang Deng and Bowen Zhou.
2009.
Optimizingword alignment combination for phrase table train-ing.
In Proc.
ACL, pages 229?232, August.Elliott Franco Dra?bek and David Yarowsky.
2004.
Im-proving bitext word alignments via syntax-based re-ordering of english.
In Proc.
ACL.Alexander Fraser and Daniel Marcu.
2007.
Getting thestructure right for word alignment: Leaf.
In Proc.
ofEMNLP, pages 51?60, June.Ulf Hermjakob.
2009.
Improved word alignment withstatistics and linguistic heuristics.
In Proc.
EMNLP,pages 229?237, August.Fei Huang.
2009.
Confidence measure for word align-ment.
In Proc.
ACL, pages 932?940, August.Abraham Ittycheriah and Salim Roukos.
2005.
A max-imum entropy word aligner for arabic-english ma-chine translation.
In Proc.
of HLT/EMNLP, pages89?96, October.Philipp Koehn, Franz Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proc.NAACL/HLT.Franz Josef Och and Hermann Ney.
2000.
Improvedstatistical alignment models.
In Proc.
of ACL, pages440?447, Hong Kong, China, October.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proc.
of ACL,pages 160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-jing Zhu.
2002.
Bleu: a method for automatic evalu-ation of machine translation.
In Proc.
of ACL, pages311?318.Martin Porter.
1980.
An algorithm for suffix stripping.In Program, volume 14, pages 130?137.Adwait Ratnaparkhi.
1997.
A linear observed time sta-tistical parser based on maximum entropy models.In Proc.
of EMNLP, pages 1?10.Chao Wang, Michael Collins, and Philipp Koehn.2007.
Chinese syntactic reordering for statisticalmachine translation.
In Proc.
EMNLP, pages 737?745.Bing Xiang, Kham Nguyen, Long Nguyen, RichardSchwartz, and John Makhoul.
2006.
Morphologicaldecomposition for arabic broadcast news transcrip-tion.
In Proc.
ICASSP.Peng Xu, Jaeho Kang, Michael Ringgaard, and FranzOch.
2009.
Using a dependency parser to improvesmt for subject-object-verb languages.
In Proc.NAACL/HLT, pages 245?253, June.26
