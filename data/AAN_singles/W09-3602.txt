Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries, ACL-IJCNLP 2009, pages 10?18,Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLPAnchor Text Extraction for Academic SearchShuming Shi1     Fei Xing2*     Mingjie Zhu3*     Zaiqing Nie1     Ji-Rong Wen11Microsoft Research Asia2Alibaba Group, China3University of Science and Technology of China{shumings, znie, jrwen}@microsoft.comfei_c_xing@yahoo.com; mjzhu@ustc.eduAbstract*Anchor text plays a special important role inimproving the performance of general Websearch, due to the fact that it is relatively ob-jective description for a Web page by poten-tially a large number of other Web pages.Academic Search provides indexing andsearch functionality for academic articles.
Itmay be desirable to utilize anchor text in aca-demic search as well to improve the search re-sults quality.
The main challenge here is thatno explicit URLs and anchor text is availablefor academic articles.
In this paper we defineand automatically assign a pseudo-URL foreach academic article.
And a machine learningapproach is adopted to extract pseudo-anchortext for academic articles, by exploiting the ci-tation relationship between them.
The ex-tracted pseudo-anchor text is then indexed andinvolved in the relevance score computation ofacademic articles.
Experiments conducted on0.9 million research papers show that our ap-proach is able to dramatically improve searchperformance.1 IntroductionAnchor text is a piece of clickable text that linksto a target Web page.
In general Web search,anchor text plays an extremely important role inimproving the search quality.
The main reasonfor this is that anchor text actually aggregates theopinion (which is more comprehensive, accurate,and objective) of a potentially large number ofpeople for a Web page.
* This work was performed when Fei Xing and Mingjie Zhuwere interns at Microsoft Research Asia.In recent years, academic search (Giles et al,1998; Lawrence et al, 1999; Nie et al, 2005;Chakrabarti et al, 2006) has become an impor-tant supplement to general web search for re-trieving research articles.
Several academicsearch systems (including Google Scholar?, Cite-seer?, DBLP?, Libra**, ArnetMiner?
?, etc.)
havebeen deployed.
In order to improve the resultsquality of an academic search system, we mayconsider exploiting the techniques which aredemonstrated to be quite useful and critical ingeneral Web search.
In this paper, we study thepossibility of extracting anchor text for researchpapers and using them to improve the search per-formance of an academic search system.Figure 1.
An example of one paper citing other papersThe basic search unit in most academic searchsystems is a research paper.
Borrowing the con-cepts of URL and anchor-text in general Websearch, we may need to assign a pseudo-URL forone research paper as its identifier and to definethe pseudo-anchor text for it by the contextualdescription when this paper is referenced (ormentioned).
The pseudo-URL of a research pa-per could be the combination of its title, authorsand publication information.
Figure-1 shows anexcerpt where one paper cites a couple of other?
http://scholar.google.com/?
http://citeseerx.ist.psu.edu/?
http://www.informatik.uni-trier.de/~ley/db/** http://libra.msra.cn/??
http://www.arnetminer.org/10papers.
The grayed text can be treated as thepseudo-anchor text of the papers being refe-renced.
Once the pseudo-anchor text of researchpapers is acquired, it can be indexed and utilizedto help ranking, just as in general web search.However it remains a challenging task to cor-rectly identify and extract these pseudo-URLsand pseudo-anchor texts.
First, unlike the situa-tion in general web search where one uniqueURL is assigned to each web page as a naturalidentifier, the information of research papersneed to be extracted from web pages or PDF files.As a result, in constructing pseudo-URLs forresearch papers, we may face the problem of ex-traction errors, typos, and the case of one re-search paper having different expressions in dif-ferent places.
Second, in general Web search,anchor text is always explicitly specified byHTML tags (<a> and </a>).
It is however muchharder to perform anchor text extraction for re-search papers.
For example, human knowledgemay be required in Figure-1 to accurately identi-fy the description of every cited paper.To address the above challenges, we proposean approach for extracting and utilizing pseudo-anchor text information in academic search toimprove the search results quality.
Our approachis composed of three phases.
In the first phase,each time a paper is cited in another paper, weconstruct a tentative pseudo-URL for the citedpaper and extract a candidate anchor block for it.The tentative pseudo-URL and the candidateanchor block are allowed to be inaccurate.
In thesecond phase, we merge the tentative pseudo-URLs that should represent the same paper.
Allcandidate anchor blocks belong to the same pa-per are grouped accordingly in this phase.
In thethird phase, the final pseudo-anchor text of eachpaper is generated from all its candidate blocks,by adopting a SVM-based machine learning me-thodology.
We conduct experiments upon a data-set containing 0.9 million research papers.
Theexperimental results show that lots of useful anc-hor text can be successfully extracted and accu-mulated using our approach, and the ultimatesearch performance is dramatically improvedwhen anchor information is indexed and used forpaper ranking.The remaining part of this paper is organizedas follows.
In Section 2, we describe in detail ourapproach for pseudo-anchor text extraction andaccumulation.
Experimental results are reportedin Section 3.
We discuss related work in Section4 and finally conclude the paper in Section 5.2 Our Approach2.1 OverviewBefore describing our approach in detail, we firstrecall how anchor text is processed in generalWeb search.
Assume that there have been a col-lection of documents being crawled and storedon local disk.
In the first step, each web page isparsed and the out links (or forward links) withinthe page are extracted.
Each link is comprised ofa URL and its corresponding anchor text.
In thesecond step, all links are accumulated accordingto their destination URLs (i.e.
the anchor texts ofall links pointed to the same URL are merged).Thus, we can get al anchor text corresponding toeach web page.
Figure-2 (a) demonstrates thisprocess.Figure 2.
The main process of extracting (a) anchortext in general web search and (b) pseudo-anchor textin academic searchFor academic search, we need to extract andparse the text content of papers.
When a paper Amentions another paper B, it either explicitly orimplicitly displays the key information of B to letthe users know that it is referencing B instead ofother papers.
Such information can be extractedto construct the tentative pseudo-URL of B. Thepseudo-URLs constructed in this phase are tenta-tive because different tentative pseudo-URLsmay be merged to generate the same final pseu-do-URL.
All information related to paper B indifferent papers can be accumulated and treatedWeb pagesHTML parsingLinksAnchor textfor pagesGroup by linkdestinationPapersPaper parsingTentative pseudo-URLsCandidate anchor blocksAnchor block accumulationPapers with theircandidate anchor blocksPapers with theirpseudo-anchor textAnchor-text learning11as the potential anchor text of B.
Our goal is toget the anchor text related to each paper.Our approach for pseudo-anchor text extrac-tion is shown in Figure-2 (b).
The key process issimilar to that in general Web search for accumu-lating and utilizing page anchor text.
One prima-ry difference between Figure-2 (a) and (b) is thelatter accumulates candidate anchor blocks ratherthan pieces of anchor text.
A candidate anchorblock is a piece of text that contains the descrip-tion of one paper.
The basic idea is: Instead ofextracting the anchor text for a paper directly (adifficult task because of the lack of enough in-formation), we first construct a candidate anchorblock to contain the "possible" or "potential" de-scription of the paper.
After we accumulate allcandidate anchor blocks, we have more informa-tion to provide a better estimation about whichpieces of texts are anchor texts.
Following thisidea, our proposed approach adopts a three-phasemethodology to extract pseudo-anchor text.
Inthe first phase, each time a paper B appearing inanother paper A, a candidate anchor block is ex-tracted for B.
All candidate anchor blocks belongto the same paper are grouped in the secondphase.
In the third phase, the final pseudo-anchortext of each paper is selected among all candidateblocks.Extracting tentative pseudo-URLs and can-didate anchor blocks: When one paper citesanother paper, a piece of short text (e.g.
"[1]" or?
(xxx et al, 2008)?)
is commonly inserted torepresent the paper to be cited, and the detail in-formation (key attributes) of it are typically putat the end of the document (in the references sec-tion).
We call each paper listed in the referencessection a reference item.
The references sectioncan be located by searching for the last occur-rence of term 'reference' or 'references' in largerfonts.
Then, we adopt a rule-based approach todivide the text in the references section into ref-erence items.
Another rule-based approach isused to extract paper attributes (title, authors,year, etc) from a reference item.
We observedsome errors in our resulting pseudo-URLs causedby the quality of HTML files converted fromPDF format, reference item extraction errors,paper attribute extraction errors, and other fac-tors.
We also observed different reference itemformats for the same paper.
The pseudo-URL fora paper is defined according to its title, authors,publisher, and publication year, because thesefour kinds of information can readily be used toidentify a paper.For each citation of a paper, we treat the sen-tence containing the reference point (or citationpoint) as one candidate anchor block.
When mul-tiple papers are cited in one sentence, we treatthe sentence as the candidate anchor block ofevery destination paper.Candidate Anchor Block Accumulation:This phase is in charge of merging all candidateblocks of the same pseudo-URL.
As has beendiscussed, tentative pseudo-URLs are often inac-curate; and different tentative pseudo-URLs maycorrespond to the same paper.
The primary chal-lenge here is perform the task in an efficient wayand with high accuracy.
We will address thisproblem in Subsection 2.2.Pseudo-Anchor Generation: In the previousphase, all candidate blocks of each paper havebeen accumulated.
This phase is to generate thefinal anchor text for each paper from all its can-didate blocks.
Please refer to Subsection 2.3 fordetails.2.2 Candidate Anchor Block Accumulationvia Multiple Feature-String HashingConsider this problem: Given a potentially hugenumber of tentative pseudo-URLs for papers, weneed to identify and merge the tentative pseudo-URLs that represent the same paper.
This is likethe problems in the record linkage (Fellegi andSunter, 1969), entity matching, and data integra-tion which have been extensively studied in da-tabase, AI, and other areas.
In this sub-section,we will first show the major challenges and theprevious similar work on this kind of problem.Then a possible approach is described to achievea trade-off between accuracy and efficiency.Figure 3.
Two tentative pseudo-URLs representingthe same paper2.2.1 Challenges and candidate techniquesTwo issues should be addressed for this problem:similarity measurement, and the efficiency of thealgorithm.
On one hand, a proper similarity func-tion is needed to identify two tentative pseudo-URLs representing the same paper.
Second, the12integration process has to be accomplished effi-ciently.We choose to compute the similarity betweentwo papers to be a linear combination of the si-milarities on the following fields: title, authors,venue (conference/journal name), and year.
Thesimilarity function on each field is carefully de-signed.
For paper title, we adopt a term-level editdistance to compute similarity.
And for paperauthors, person name abbreviation is considered.The similarity function we adopted is fairly wellin accuracy (e.g., the similarity between the twopseudo-URLs in Figure-3 is high according toour function); but it is quite time-consuming tocompute the similarity for each pair of papers(roughly 1012 similarity computation operationsare needed for 1 million different tentative pseu-do-URLs).Some existing methods are available for de-creasing the times of similarity calculation opera-tions.
McCallum et al (2000) addresses this highdimensional data clustering problem by dividingdata into overlapping subsets called canopiesaccording to a cheap, approximate distance mea-surement.
Then the clustering process is per-formed by measuring the exact distances onlybetween objects from the same canopy.
There arealso other subspace methods (Parsons et al, 2004)in data clustering areas, where data are dividedinto subspaces of high dimensional spaces firstand then processing is done in these subspaces.Also there are fast blocking approaches forrecord linkage in Baxter et al (2003).
Thoughthey may have different names, they hold similarideas of dividing data into subsets to reduce thecandidate comparison records.
The size of data-set used in the above papers is typically quitesmall (about thousands of data items).
For effi-ciency issue, Broder et al (1997) proposed ashingling approach to detect similar Web pages.They noticed that it is infeasible to comparesketches (which are generated by shingling) ofall pairs of documents.
So they built an invertedindex that contains a list of shingle values andthe documents they appearing in.
With the in-verted index, they can effectively generate a listof all the pairs of documents that share any shin-gles, along with the number of shingles theyhave in common.
They did experiments on a da-taset containing 30 million documents.By adopting the main ideas of the above tech-niques to our pseudo-URL matching problem, apossible approach can be as follows.Figure 4.
The Multiple Feature-String Hashing algo-rithm for candidate anchor block accumulation2.2.2 Method adoptedThe method utilized here for candidate anchorblock accumulation is shown in Figure 4.
Themain idea is to construct a certain number of fea-ture strings for a tentative pseudo-URL (abbre-viated as TP-URL) and do hash for the featurestrings.
A feature string of a paper is a smallpiece of text which records a part of the paper?skey information, satisfying the following condi-tions: First, multiple feature strings can typicallybe built from a TP-URL.
Second, if two TP-URLs are different representations of the samepaper, then the probability that they have at leastone common feature string is extremely high.
Wecan choose the term-level n-grams of paper titles(referring to Section 3.4) as feature strings.The algorithm maintains an in-memory hash-table which contains a lot of slots each of whichis a list of TP-URLs belonging to this slot.
Foreach TP-URL, feature strings are generated andhashed by a specified hash function.
The TP-URL is then added into some slots according tothe hash values of its feature strings.
Any twoTP-URLs belonging to the same slot are furthercompared by utilizing our similarity function.
Iftheir similarity is larger than a threshold, the twoTP-URLs are treated as being the same andtherefore their corresponding candidate anchorblocks are merged.The above algorithm tries to achieve good bal-ance between accuracy and performance.
On onehand, compared with the na?ve algorithm of per-forming one-one comparison between all pairs ofTP-URLs, the algorithm needs only to computeAlgorithm Multiple Feature-String Hashing for candidate anchorblock accumulationInput: A list of papers (with their tentative pseudo-URLsand candidate anchor blocks)Output: Papers with all candidate anchor blocks of thesame paper aggregatedInitial: An empty hashtable h (each slot of h is a list of pa-pers)For each paper A in the input list {For each feature-string of A {Lookup by the feature-string in h to get a slot s;Add A into s;}}For each slot s with size smaller than a threshold {For any two papers A1, A2 in s {float fSim = Similarity(A1, A2);if(fSim > the specified threshold) {Merge A1 and A2;}}}13the similarity for the TP-URLs that share acommon slot.
On the other hand, because of thespecial property of feature strings, most TP-URLs representing the same paper can be de-tected and merged.The basic idea of dividing data into over-lapped subsets is inherited from McCallum et al(2000), Broder et al (1997), and some subspaceclustering approaches.
Slightly different, we donot count the number of common feature stringsbetween TP-URLs.
Common bins (or invertedindices) between data points are calculated inMcCallum et al (2000) as a ?cheap distance?
forcreating canopies.
The number of common Shin-gles between two Web documents is calculated(efficiently via inverted indices), such that Jac-card similarity could be used to measure the si-milarity between them.
In our case, we simplycompare any two TP-URLs in the same slot byusing our similarity function directly.The effective and efficiency of this algorithmdepend on the selection of feature strings.
For afixed feature string generation method, the per-formance of this algorithm is affected by the sizeof each slot, especially the number and size ofbig slots (slots with size larger than a threshold).Big slots will be discarded in the algorithm toimprove performance, just like removing com-mon Shingles in Broder et al (1997).
In Section4, we conduct experiments to test the perfor-mance of the above algorithm with different fea-ture string functions and different slot size thre-sholds.2.3 Pseudo-Anchor Text LearningIn this subsection, we address the problem ofextracting the final pseudo-anchor text for a pa-per, given all its candidate anchor blocks (seeFigure 5 for an example).2.3.1 Problem definitionA candidate anchor block is a piece of text withone or some reference points (a reference point isone occurrence of citation in a paper) specified,where a reference point is denoted by a<start_pos, end_pos> pair (means start positionand end position respectively): ref = <start_pos,end_pos>.
We represent a candidate anchorblock to be the following format,AnchorBlock = (Text, ref1, ref2, ?
)We define a block set to be a set of candidateanchor blocks for a paper,BlockSet = {AnchorBlock1, AnchorBlock2, ?
}Now the problem is: Given a block set con-taining N elements, extract some text excerptsfrom them as the anchor text of the paper.2.3.2 Learn term weightsWe adopt a machine-learning approach to assign,for each term in the anchor blocks, a discrete de-gree of being anchor text.
The main reasons fortaking such an approach is twofold: First, webelieve that assigning each term a fuzzy degreeof being anchor text is more appropriate than abinary judgment as either an anchor-term or non-anchor-term.
Second, since the importance of aterm for a ?link?
may be determined by manyfactors in paper search, a machine-learning couldbe more flexible and general than the approachesthat compute term degrees by a specially de-signed formula.Figure 5.
The candidate pseudo-anchor blocks of apaperThe features used for learning are listed in Ta-ble-1.We observed that it would be more effective ifsome of the above features are normalized beforebeing used for learning.
For a term in candidateanchor block B, its TF are normalized by theBM25 formula (Robertson et al, 1999),TFLBbbkTFkTFnorm ????????
)||)1(()1(11where L is average length of the candidate blocks,|B| is the length of B, and k1, b are parameters.DF is normalized by the following formula,)1log( DFNIDF ?
?where N is the number of elements in the blockset (i.e.
total number of candidate anchor blocksfor the current paper).Features RefPos and Dist are normalized as,RefPosnorm = RefPos / |B|Distnorm = (Dist-RefPos) / |B|And the feature BlockLen is normalized as,14BlockLennorm = log(1+BlockLen)Features DescriptionDFDocument frequency: Number of candidate blocks inwhich the term appears, counted among all candidateblocks of all papers.
It is used to indicate whether theterm is a stop word or not.BFBlock frequency: Number of candidate blocks inwhich the term appears, counted among all candidateblocks of this paper.CTFCollection term frequency: Total number of times theterm appearing in the blocks.
For multiple times ofoccurrences in one block, all of them are counted.IsInURLSpecify whether the term appears in the pseudo-URLof the paper.TFTerm frequency: Number of times the terms appearingin the candidate block.DistDirected distance from the nearest reference point tothe term locationRefPosPosition of the nearest reference point in the candidatepseudo-anchor block.BlockLen Length of the candidate pseudo-anchor blockTable 1.
Features for learningWe set four term importance levels, from 1(unrelated terms or stop words) to 4 (words par-ticipating in describing the main ideas of the pa-per).We choose support vector machine (SVM) forlearning term weights here, because of its power-ful classification ability and well generalizationability (Burges, 1998).
We believe some othermachine learning techniques should also workhere.
The input of the classifier is a feature vec-tor of a term and the output is the importancelevel of the term.
Given a set of training data?
?liii levelfeature 1, ?, a decision function f(x) can beacquired after training.
Using the decision func-tion, we can assign an importance level for eachterm automatically.3 Experiments3.1 Experimental SetupOur experimental dataset contains 0.9 millionpapers crawled from the web.
All the papers areprocessed according to the process in Figure-2(b).
We randomly select 300 queries from thequery log of Libra (libra.msra.cn) and retrievethe results in our indexing and ranking systemwith/without the pseudo-anchors generated byour approach.
Then the volunteer researchers andstudents in our group are involved to judge thesearch results.
The top 30 results of differentranking algorithms for each query are labeledand assigned a relevance value from 1 (meaning'poor match') to 5 (meaning 'perfect match').
Thesearch results quality is measured by NDCG(Jarvelin and Kekalainen, 2000).3.2 Overall Effect of our ApproachFigure 6 shows the performance comparison be-tween the results of two baseline paper rankingalgorithms and the results of including pseudo-anchor text in ranking.0.4660.4260.3880.5970.6190.6890.673 0.6720.62700.10.20.30.40.50.60.70.80.91NDCG@1 NDCG@3 NDCG@10Base(Without CitationCount)BasePseudo-Anchor IncludedFigure 6.
Comparison between the baseline approachand our approach (measure: nDCG)The ?Base?
algorithm considers the title, ab-stract, full-text and static-rank (which is a func-tion of the citation count) of a paper.
In a bitmore detail, for each paper, we adopt the BM25formula (Robertson et al, 1999) over its title,abstract, and full-text respectively.
And then theresulting score is linearly combined with the stat-ic-rank to get its final score.
The static-rank iscomputed as follows,StaticRank = log(1+CitationCount) (3.1)To test the performance of including pseudo-anchor text in ranking, we compute an anchorscore for each paper and linearly combine it withits baseline score (i.e.
the score computed by thebaseline algorithm).We tried two kinds of ways for anchor scorecomputation.
The first is to merge all pieces ofanchor excerpts (extracted in the previous section)into a larger piece of anchor text, and use BM25to compute its relevance score.
In another ap-proach called homogeneous evidence combina-tion (Shi et al, 2006), a relevance score is com-puted for each anchor excerpt (still using BM25),and all the scores for the excerpts are sorted des-cending and then combined by the followingformula,??????
?miianchor sicS 1 2))1(1(1 (3.2)where si (i=1, ?, m) are scores for the m anchorexcerpts, and c is a parameter.
The primary idea15here is to let larger scores to have relative greaterweights.
Please refer to Shi et al (2006) for ajustification of this approach.
As we get slightlybetter results with the latter way, we use it as ourfinal choice for computing anchor scores.From Figure 6, we can see that the overall per-formance is greatly improved by including pseu-do-anchor information.
Table 2 shows the t-testresults, where a ?>?
indicates that the algorithmin the row outperforms that in the column with ap-value of 0.05 or less, and a ?>>?
means a p-value of 0.01 or less.BaseBase (withoutCitationCount)Our approach > >>Base  >>Base (without Cita-tionCount)Table 2.
Statistical significance tests (t-test overnDCG@3)Table 3 shows the performance comparison byusing some traditional IR measures based on bi-nary judgments.
Since the results of not includ-ing CitationCount are much worse than the othertwo, we omit it in the table.MeasureApproachMAP MRR P@1 P@10Base (includingCitationCount)0.364 0.727 0.613 0.501Our Approach 0.381 0.734 0.625 0.531Table 3.
Performance compassion using binary judg-ment measures3.3 Sample Query AnalysisHere we analyze some sample queries to getsome insights about why and how pseudo-anchorimproves search performance.
Figure-7 and Fig-ure-8 show the top-3 results of two sample que-ries: {TF-IDF} and {Page Rank}.For query "TF-IDF", the top results of thebaseline approach have keyword "TF-IDF" ap-peared in the title as well as in other places of thepapers.
Although the returned papers are relevantto the query, they are not excellent because typi-cally users may want to get the first TF-IDF pa-per or some papers introducing TF-IDF.
Whenpseudo-anchor information is involved, someexcellent results (B1, B2, B3) are generated.
Themain reason for getting the improved results isthat these papers (or books) are described with"TF-IDF" when lots of other papers cite them.Figure 7.
Top-3 results for query TF-IDFFigure 8.
Top-3 results for query Page RankFigure-8 shows another example about howpseudo-anchor helps to improve search resultsquality.
For query "Page Rank" (note that there isa space in between), the results returned by thebaseline approach are not satisfactory.
In the pa-pers returned by our approach, at least B1 and B2are very good results.
Although they did not la-bel themselves "Page Rank", other papers do soin citing them.
Interestingly, although the resultB3 is not about the "PageRank" algorithm, it de-scribes another popular "Page Rank" algorithmin addition to PageRank.Another interesting observation from the twofigures is that our approach retrieves older papersthan the baseline method, because old paperstend to have more anchor text (due to more cita-tions).
So our approach may not be suitable forretrieve newer papers.
To overcome this problem,maybe publication year should be considered inour ranking functions.3.4 Anchor Accumulation ExperimentsWe conduct experiments to test the effectivenessand efficiency of the multiple-feature-string-hashing algorithm presented in Section 2.2.
Theduplication detection quality of this algorithm isdetermined by the appropriate selection of fea-A1.
V Safronov, M Parashar, Y Wang et al Optimizing Web serversusing Page rank prefetching for clustered accesses.
InformationSciences.
2003.A2.
AO Mendelzon, D Rafiei.
An autonomous page ranking method formetasearch engines.
WWW, 2002.A3.
FB Kalhoff.
On formally real Division Algebras and Quasifields ofRank two.
(a) Without anchorB1.
S Brin, L Page.
The Anatomy of a Large-Scale Hypertextual WebSearch Engine.
WWW, 1998B2.
L Page, S Brin, R Motwani, T Winograd.
The pagerank citationranking: Bringing order to the web.
1998.B3.
JM Kleinberg.
Authoritative sources in a hyperlinked environment.Journal of the ACM, 1999.
(b) With anchorA1.
K Sugiyama, K Hatano, M Yoshikawa, S Uemura.
Refinement of TF-IDF schemes for web pages using their hyperlinked neighboring pages.Hypertext?03A2.
A Aizawa.
An information-theoretic perspective of tf-idf measures.IPM?03.A3.
N Oren.
Reexamining tf.idf based information retrieval with Genet-ic Programming.
SAICSIT?02.
(a) Without anchorB1.
G Salton, MJ McGill.
Introduction to Modern Information Retriev-al.
McGraw-Hill, 1983.B2.
G Salton and C Buckley.
Term weighting approaches in automatictext retrieval.
IPM?98.B3.
R Baeza-Yates, B Ribeiro-Neto.
Modern Information Retrieval.Addison-Wesley, 1999(b) With anchor16ture strings.
When feature strings are fixed, theslot size threshold can be used to tune the tra-deoff between accuracy and performance.Feature StringsSlot Distr.Ungram Bigram Trigram 4-gram# of Slots 1.4*105 1.2*106 2.8*106 3.4*106# of Slots withsize > 1005240 6806 1541 253# of Slots withsize > 1000998 363 50 5# of Slots withsize > 1000059 11 0 0Table 4.
Slot distribution with different feature stringsWe take all the papers extracted from PDFfiles as input to run the algorithm.
Identical TP-URLs are first eliminated (therefore their candi-date anchor blocks are merged) by utilizing ahash table.
This pre-process step results in about1.46 million distinct TP-URLs.
The number islarger than our collection size (0.9 million), be-cause some cited papers are not in our paper col-lection.
We tested four kinds of feature strings allof which are generated from paper title: uni-grams, bigrams, trigrams, and 4-grams.
Table-4shows the slot size distribution corresponding toeach kind of feature strings.
The performancecomparison among different feature strings andslot size thresholds is shown in Table 5.
It seemsthat bigrams achieve a good trade-off betweenaccuracy and performance.FeatureStringsSlot SizeThresholdDup.
papersDetectedProcessingTime (sec)Unigram5000 529,717  119,739.0500 327,357 7,552.7Bigram 500 528,981 8,229.6TrigramInfinite 518,564 8,420.4500 516,369 2,654.94-gram 500 482,299 1,138.2Table 5.
Performance comparison between differentfeature strings and slot size thresholds4 Related WorkThere has been some work which uses anchortext or their surrounding text for various Webinformation retrieval tasks.
It was known at thevery beginning era of internet that anchor textwas useful to Web search (McBryan, 1994).Most Web search engines now use anchor text asprimary and power evidence for improvingsearch performance.
The idea of using contextualtext in a certain vicinity of the anchor text wasproposed in Chakrabarti et al (1998) to automat-ically compile some lists of authoritative Webresources on a range of topics.
An anchor win-dow approach is proposed in Chakrabarti et al(1998) to extract implicit anchor text.
Followingthis work, anchor windows were considered insome other tasks (Amitay  et al, 1998; Haveli-wala et al, 2002; Davison, 2002; Attardi et al,1999).
Although we are inspired by these ideas,our work is different because research papershave many different properties from Web pages.From the viewpoint of implicit anchor extractiontechniques, our approach is different from theanchor window approach.
The anchor windowapproach is somewhat simpler and easy to im-plement than ours.
However, our method is moregeneral and flexible.
In our approach, the anchortext is not necessarily to be in a window.Citeseer (Giles et al, 1998; Lawrence  et al,1999) has been doing a lot of valuable work oncitation recognition, reference matching, and pa-per indexing.
It has been displaying contextualinformation for cited papers.
This feature hasbeen shown to be helpful and useful for re-searchers.
Differently, we are using context de-scription for improving ranking rather than dis-play purpose.
In addition to Citeseer, some otherwork (McCallum et al, 1999; Nanba and Oku-mura, 1999; Nanba et al, 2004; Shi et al, 2006)is also available for extracting and accumulatingreference information for research papers.5 Conclusions and Future WorkIn this paper, we propose to improve academicsearch by utilizing pseudo-anchor information.As pseudo-URL and pseudo-anchor text are notas explicit as in general web search, more effortsare needed for pseudo-anchor extraction.
Ourmachine-learning approach has proven success-ful in automatically extracting implicit anchortext.
By using the pseudo-anchors in our academ-ic search system, we see a significant perfor-mance improvement over the basic approach.AcknowledgmentsWe would like to thank Yunxiao Ma and PuWang for converting paper full-text from PDF toHTML format.
Jian Shen has been helping us dosome reference extraction and matching work.Special thanks are given to the researchers andstudents taking part in data labeling.17ReferencesE.
Amitay.
1998.
Using common hypertext links toidentify the best phrasal description of target webdocuments.
In Proc.
of the SIGIR'98 Post Confe-rence Workshop on Hypertext Information Re-trieval for the Web, Melbourne, Australia.G.
Attardi, A. Gulli, and F. Sebastiani.
1999.
Theseus:categorization by context.
In Proceedings of the 8thInternational World Wide Web Conference.A.
Baxter, P. Christen, T. Churches.
2003.
A compar-ison of fast blocking methods for record linkage.
InACM SIGKDD'03 Workshop on Data Cleaning,Record Linkage and Object consolidation.
Wash-ington DC.A.
Broder, S. Glassman, M. Manasse, and G. Zweig.1997.
Syntactic clustering of the Web.
In Proceed-ings of the Sixth International World Wide WebConference, pp.
391-404.C.J.C.
Burges.
1998.
A tutorial on support vector ma-chines for pattern recognition.
Data Mining andKnowledge Discovery, 2, 121-167.S.
Chakrabarti, B. Dom, D. Gibson, J. Kleinberg, P.Raghavan, and S. Rajagopalan.
1998.
Automaticresource list compilation by analyzing hyperlinkstructure and associated text.
In Proceedings of the7th International World Wide Web Conference.K.
Chakrabarti, V. Ganti, J. Han, and D. Xin.
2006.Ranking objects based on relationships.
In SIG-MOD ?06: Proceedings of the 2006 ACM SIG-MOD international conference on Management ofdata, pages 371?382, New York, NY, USA.
ACM.B.
Davison.
2000.
Topical locality in the web.
In SI-GIR'00: Proceedings of the 23rd annual interna-tional ACM SIGIR conference on Research anddevelopment in information retrieval, pages 272-279, New York, NY, USA.
ACM.I.P.
Fellegi, and A.B.
Sunter.
A Theory for RecordLinkage, Journal of the American Statistical Asso-ciation, 64, (1969), 1183-1210.C.
L. Giles, K. Bollacker, and S. Lawrence.
1998.CiteSeer: An automatic citation indexing system.In IanWitten, Rob Akscyn, and Frank M. ShipmanIII, editors, Digital Libraries 98 - The Third ACMConference on Digital Libraries, pages 89?98,Pittsburgh, PA, June 23?26.
ACM Press.T.H.
Haveliwala, A. Gionis, D. Klein, and P. Indyk.2002.
Evaluating strategies for similarity search onthe web.
In WWW ?02: Proceedings of the 11th in-ternational conference on World Wide Web, pages432?442, New York, NY, USA.
ACM.K.
Jarvelin, and J. Kekalainen.
2000.
IR EvaluationMethods for Retrieving Highly Relevant Docu-ments.
In Proceedings of the 23rd Annual Interna-tional ACM SIGIR Conference on Research andDevelopment in Information Retrieval (SI-GIR2000).S.
Lawrence, C.L.
Giles, and K. Bollacker.
1999.
Dig-ital libraries and Autonomous Citation Indexing.IEEE Computer, 32(6):67?71.A.
McCallum, K. Nigam, J. Rennie, and K. Seymore.1999.
Building Domain-specific Search Engineswith Machine Learning Techniques.
In Proceed-ings of the AAAI-99 Spring Symposium on Intelli-gent Agents in Cyberspace.A.
McCallum, K. Nigam, and L. Ungar.
2000.
Effi-cient clustering of high-dimensional data sets withapplication to reference matching.
In Proc.
6thACM SIGKDD Int.
Conf.
on Knowledge Discov-ery and Data Mining.O.A.
McBryan.
1994.
Genvl and wwww: Tools fortaming the web.
In In Proceedings of the First In-ternational World Wide Web Conference, pages79-90.H.
Nanba, M. Okumura.
1999.
Towards Multi-paperSummarization Using Reference Information.
InProc.
of the 16th International Joint Conference onArtificial Intelligence, pp.926-931.H.
Nanba, T. Abekawa, M. Okumura, and S. Saito.2004.
Bilingual PRESRI: Integration of MultipleResearch Paper Databases.
In Proc.
of RIAO 2004,195-211.L.
Parsons, E. Haque, H. Liu.
2004.
Subspace cluster-ing for high dimensional data: a review.
SIGKDDExplorations 6(1): 90-105.S.E.
Robertson, S. Walker, and M. Beaulieu.
1999.Okapi at TREC-7: automatic ad hoc, filtering, VLCand filtering tracks.
In Proceedings of TREC?99.S.
Shi, R. Song, and J-R Wen.
2006.
Latent Additivity:Combining Homogeneous Evidence.
Techniquereport, MSR-TR-2006-110, Microsoft Research,August 2006.S.
Shi, F. Xing, M. Zhu, Z.Nie, and J.-R. Wen.
2006.Pseudo-Anchor Extraction for Search Vertical Ob-jects.
In Proc.
of the 2006 ACM 15th Conferenceon Information and Knowledge Management.
Ar-lington, USA.Z.
Nie, Y. Zhang, J.-R. Wen, and W.-Y.
Ma.
2005.Object-level ranking: bringing order to web objects.InWWW?05: Proceedings of the 14th internationalconference on World Wide Web, pages 567?574,New York, NY, USA.
ACM.18
