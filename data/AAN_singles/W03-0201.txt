Utterance Classification in AutoTutorAndrewOlneyMaxLouwerseEricMatthewsJohannaMarineauHeatherHite-MitchellArthurGraesserInstitute for Intelligent SystemsUniversity of MemphisMemphis, TN 38152aolney@memphis.eduAbstractThis paper describes classification of typedstudent utterances within AutoTutor, an intel-ligent tutoring system.
Utterances are classi-fied to one of 18 categories, including 16question categories.
The classifier presenteduses part of speech tagging, cascaded finitestate transducers, and simple disambiguationrules.
Shallow NLP is well suited to the task:session log file analysis reveals significantclassification of eleven question categories,frozen expressions, and assertions.1 IntroductionAutoTutor is a domain-portable intelligent tutoringsystem (ITS) with current versions in the domains ofphysics and computer literacy (Graesser et al 1999;Olney et al 2002).
AutoTutor, like many other ITSs, isan intersection of applications, including tutoring,mixed-initiative dialogue, and question answering.
Ineach of these, utterance classification, particularly ques-tion classification, plays a critical role.In tutoring, utterance classification can be used totrack the student's level of understanding.
Contributionand question classifications can both play a role: contri-butions may be compared to an expected answer(Graesser et al 2001) and questions may be scored byhow "deep" they are.
For example, The PREG model(Otero and Graesser 2001) predicts under what circum-stances students will ask "deep" questions, i.e.
those thatreveal a greater level of cognitive processing than who,what, when, or where questions.
A student who is onlyasking shallow questions, or no questions at all, is pre-dicted by PREG to not have a situation-level under-standing (van Dijk and Kintsch 1983) and thus to learnless and forget faster.
The key point is that differentmetrics for tracking student understanding are applica-ble to questions and contributions.
Distinguishing themvia classification is a first step to applying a metric.In mixed-initiative dialog systems, utterance classifi-cation can be used to detect shifts in initiative.
For ex-ample, a mixed-initiative system that asks, "Wherewould you like to travel", could respond to the question,"Where can I travel for $200?"
(Allen 1999) by giving alist of cities.
In this example, the user is taking the ini-tiative by requesting more information.
In order to re-spond properly, the system must detect that the user hastaken initiative before it can respond appropriately; oth-erwise it might try to interpret the user's utterance as atravel destination.
In this sense, questions mark redirec-tion of the dialogue, whereas contributions are continua-tions of the dialogue.
In order for a user to redirect thedialogue and thus exercise initiative, a mixed-initiativesystem must be able to distinguish questions and contri-butions.Question classification as early as Lehnert (1978)has been used as a basis for answering questions, a trendthat continues today (Voorhees 2001).
A common fea-ture of these question-answering systems is that theyfirst determine the expected answer type implicit in thequestion.
For example, "How much does a pretzel cost"might be classified according to the answer type ofMONEY or QUANTITY.
Knowledge of the expected an-swer type can be used to narrow the search space for theanswer, either online (Brill et al 2001) or in a database(Harabagiu et al 2000).
Accordingly, question answer-ing calls for a finer discrimination of question types asopposed to only distinguishing questions from contribu-tions.AutoTutor uses utterance classification to track stu-dent progress, to determine initiative, and to answerquestions.
By virtue of being embedded in AutoTutor,the utterance classifier presented here has an unusual setof constraints, both practical and theoretical.
On thepractical side, AutoTutor is a web-based application thatperforms in real time; thus utterance classification mustalso proceed in real time.
For that reason, the classifieruses a minimum of resources, including part of speechtagging (Brill 1995; Sekine and Grishman 1995) andcascaded finite state transducers defining the categories.Theoretically speaking, AutoTutor must also recognizequestions in a meaningful way to both question answer-ing and tutoring.
The question taxonomy utilized, thatof Graesser et al(1992), is an extension of Lehnert's(1978) taxonomy for question answering and has beenapplied to human tutoring (Graesser et al 1992;Graesser and Person 1994).This paper outlines the utterance classifier and quan-tifies its performance.
In particular, Section 2 presentsAutoTutor.
Section 3 presents the utterance taxonomy.Section 4 describes the classifier algorithm.
Section 5delineates the training process and results.
Section 6presents evaluation of the classifier on real AutoTutorsessions.
Section 7 concludes the paper.2 AutoTutorAutoTutor is an ITS applicable to any content domain.Two distinct domain applications of AutoTutor areavailable on the Internet, for computer literacy and con-ceptual physics.
The computer literacy AutoTutor,which has now been used in experimental evaluationsby over 200 students, tutors students on core computerliteracy topics covered in an introductory course, suchas operating systems, the Internet, and hardware.
Thetopics covered by the physics AutoTutor are groundedin basic Newtonian mechanics and are of a similar in-troductory nature.
It has been well documented thatAutoTutor promotes learning gains in both versions(Person et al 2001).AutoTutor simulates the dialog patterns and peda-gogical strategies of human tutors in a conversationalinterface that supports mixed-initiative dialog.
AutoTu-tor?s architecture is comprised of seven highly modularcomponents: (1) an animated agent, (2) a curriculumscript, (3) a speech act classifier,  (4) latent semanticanalysis (LSA), (5) a dialog move generator, (6) a Dia-log Advancer Network, and (7) a question-answeringtool (Graesser et al 1998; Graesser et al 2001;Graesser et al 2001; Person et al 2000; Person et al2001; Wiemer-Hastings et al 1998).A tutoring session begins with a brief introductionfrom AutoTutor?s three-dimensional animated agent.AutoTutor then asks the student a question from one oftopics in the curriculum script.
The curriculum scriptcontains lesson-specific tutor-initiated dialog, includingimportant concepts, questions, cases, and problems(Graesser and Person 1994; Graesser et al 1995;McArthur et al 1990; Putnam 1987).
The student sub-mits a response to the question by typing and pressingthe ?Submit?
button.
The student?s contribution is thensegmented, parsed (Sekine and Grishman 1995) andsent through a rule-based utterance classifier.
The clas-sification process makes use of only the contributiontext and part-of-speech tag provided by the parser.Mixed-initiative dialog starts with utterance classifi-cation and ends with dialog move generation, which caninclude question answering, repeating the question forthe student, or just encouraging the student.
Concur-rently, the LSA module evaluates the quality of the stu-dent contributions, and in the tutor-initiative mode, thedialog move generator selects one or a combination ofspecific dialog moves that is both conversationally andpedagogically appropriate (Person et al2000; Person etal.
2001).
The Dialog Advancer Network (DAN) is theintermediary of dialog move generation in all instances,using information from the speech act classifier andLSA to select the next dialog move type and appropriatediscourse markers.
The dialog move generator selectsthe actual move.
There are twelve types of dialogmove: Pump, Hint, Splice, Prompt, Prompt Response,Elaboration, Summary, and five forms of immediateshort-feedback (Graesser and Person 1994; Graesser etal.
1995; Person and Graesser 1999).3 An utterance taxonomyThe framework for utterance classification in Table 1 isfamiliar to taxonomies in the cognitive sciences(Graesser et al 1992; Graesser and Person 1994).
Themost notable system within this framework is QUALM(Lehnert 1978), which utilizes twelve of the questioncategories.
The taxonomy can be divided into 3 distinctgroups, questions, frozen expressions, and contribu-tions.
Each of these will be discussed in turn.The conceptual basis of the question categoriesarises from the observation that the same question maybe asked in different ways, e.g.
"What happened?"
and"How did this happen?"
Correspondingly, a single lexi-cal stem for a question, like "What" can be polysemous,e.g.
both in a definition category, "What is the definitionof gravity?"
and metacommunicative, "What did yousay?"
Furthermore, implicit questions can arise in tutor-ing via directives and some assertions, e.g.
"Tell meabout gravity" and "I don't know what gravity is."
InAutoTutor these information seeking utterances areclassified to one of the 16 question categories.The emphases on queried concepts rather than ortho-graphic forms make the categories listed in Table 1 beara strong resemblance to speech acts.
Indeed, Graesseret al (1992) propose that the categories be distinguishedin precisely the same way as speech acts, using seman-tic, conceptual, and pragmatic criteria as opposed tosyntactic and lexical criteria.
Speech acts presumablytranscend these surface criteria: it is not what is beingsaid as what is done by the saying (Austin, 1962; Searle,1975).The close relation to speech acts underscores what adifficult task classifying conceptual questions can be.Jurafsky and Martin (2000) describe the problem ofinterpreting speech acts using pragmatic and semanticinference as AI-complete, i.e.
impossible without creat-ing a full artificial intelligence.
The alternative ex-plored in this paper is cue or surface-basedclassification, using no context.It is particularly pertinent to the present discussionthat the sixteen qualitative categories are employed in aquantitative classification process.
That is to say thatfor the present purposes of classification, a questionmust belong to one and only one category.
On the onehand this idealization is necessary to obtain easily ana-lyzed performance data and to create a well-balancedtraining corpus.
On the other hand, it is not entirelyaccurate because some questions may be assigned tomultiple categories, suggesting a polythetic codingscheme (Graesser et al 1992).
Inter-rater reliability isused in the current study as a benchmark to gauge thispotential effect.Frozen expressions consist of metacognitive andmetacommunicative utterances.
Metacognitive utter-ances describe the cognitive state of the student, andthey therefore require a different response than ques-tions or assertions.
AutoTutor responds to metacogni-tive utterances with canned expressions such as, "Whydon't you give me what you know, and we'll take it fromthere."
Metacommunicative acts likewise refer to thedialogue between tutor and student, often calling for arepetition of the tutor's last utterance.
Two key pointsare worth noting: frozen expressions have a muchsmaller variability than questions or contributions, andfrozen expressions may be followed by some content,making them more properly treated as questions.
Forexample, "I don't understand" is frozen, but "I don't un-derstand gravity" is a more appropriately a question.Contributions in the taxonomy can be viewed asanything that is not frozen or a question; in fact, that isessentially how the classifier works.
Contributions inAutoTutor, either as responses to questions or un-prompted, are tracked to evaluate student performancevia LSA, forming the basis for feedback.4 Classifier AlgorithmThe present approach ignores the semantic and prag-matic context of the questions, and utilizes surface fea-tures to classify questions.
This shallow approachparallels work in question answering (Srihari and Li2000; Soubbotin and Soubbotin 2002; Moldovan et al1999).
Specifically, the classifier uses tagging providedby ApplePie (Sekine and Grishman 1995) followed bycascaded finite state transducers defining the categories.The finite state transducers are roughly described inTable 2.
Every transducer is given a chance to match,and a disambiguation routine is applied at the end toselect a single category.Category ExampleQuestionsVerificationDisjunctiveConcept CompletionFeature SpecificationQuantificationDefinitionExampleComparisonInterpretationCausal AntecedentCausal ConsequenceGoal OrientationInstrumental/ProceduralEnablementExpectationalJudgmentalDoes the pumpkin land in his hands?Is the pumpkin accelerating or decelerating?Where will the pumpkin land?What are the components of the forces acting on the pumpkin?How far will the pumpkin travel?What is acceleration?What is an example of Newton's Third Law?What is the difference between speed and velocity?What is happening in this situation with the runner and pumpkin?What caused the pumpkin to fall?What happens when the runner speeds up?Why did you ignore air resistance?How do you calculate force?What principle allows you to ignore the vertical component of the force?Why doesn't the pumpkin land behind the runner?What do you think of my explanation?Frozen ExpressionsMetacognitiveMetacommunicativeI don't understand.Could you repeat that?Contribution The pumpkin will land in the runner's handsTable 1.
AutoTutor?s utterance taxonomy.Immediately after tagging, transducers are applied tocheck for frozen expressions.
A frozen expression mustmatch, and the utterance must be free of any nouns, i.e.not frozen+content, for the utterance to be classified asfrozen.
Next the utterance is checked for questionstems, e.g.
WHAT, HOW, WHY, etc.
and questionmark punctuation.
If question stems are buried in theutterance, e.g.
"I don't know what gravity is", a move-ment rule transforms the utterance, placing the stem atthe beginning.
Likewise if a question ends with a ques-tion mark but has no stem, an AUX stem is placed at thebeginning of the utterance.
In this way the same trans-ducers can be applied to both direct and indirect ques-tions.
At this stage, if the utterance does not possess aquestion stem and is not followed by a question mark,the utterance is classified as a contribution.Two sets of finite state transducers are applied to po-tential questions, keyword transducers and syntacticpattern transducers.
Keyword transducers replace a setof keywords specific to a category with a symbol forthat category.
This extra step simplifies the syntacticpattern transducers that look for the category symbol intheir pattern.
The definition keyword transducer, forexample, replaces "definition", "define", "meaning","means", and "understanding" with "KEYDEF".
Formost categories, the keyword list is quite extensive andexceeds the space limitations of Table 2.
Keywordtransducers also add the category symbol to a list whenthey match; this list is used for disambiguation.
Syntac-tic pattern transducers likewise match, putting a cate-gory symbol on a separate disambiguation list.In the disambiguation routine, both lists are con-sulted, and the first category symbol found on both listsdetermines the classification of the utterance.
ClearlyUtterance Category Finite state transducer patternVerification ^AUXDisjunctive ^AUX ... orConcept Completion ^(Who|What|When|Where)Feature Specification ^What ... keywordkeywordQuantification ^What AUX ... keyword^How (ADJ|ADV)^MODAL you ... keywordDefinition ^What AUX ... (keyword|a?
(ADJ|ADV)* N^MODAL you ... keywordwhat a?
(ADJ|ADV)* N BEExample ^AUX ... keyword^What AUX ... keywordComparison ^What AUX ... keyword^How ... keyword^MODAL you ... keywordInterpretation keywordCausal Antecedent ^(Why|How) AUX ... (VBpast|keyword)^(WH|How) ... keywordCausal ConsequenceGoal Orientation ^(What|Why) AUX ART?
(NP|SUBJPRO|keyword)^What ... keywordInstrumental/Procedural ^(WH|How) AUX ART?
(N|PRO)^(WH|How) ... keyword^MODAL you ... keywordEnablement ^(WH|How) ... keywordExpectational ^Why AUX ... NEGJudgmental(you|your) ... keyword(should|keyword) (N|PRO)Frozen (no nouns) ^SUBJPRO ... keyword^VB ... keyword ... OBJPRO^AUX ... SUBJPRO ... keywordContribution Everything elseTable 2.
Finite state transducer patternsordering of transducers affects which symbols are clos-est to the beginning of the list.
Ordering is particularlyrelevant when considering categories like concept com-pletion, which match more freely than other categories.Ordering gives rarer and stricter categories a chance tomatch first; this strategy is common in stemming (Paice1990).5 TrainingThe classifier was built by hand in a cyclical process ofinspecting questions, inducing rules, and testing theresults.
The training data was derived from brainstorm-ing sessions whose goal was to generate questions aslexically and syntactically distinct as possible.
Of thebrainstormed questions, only when all five raters agreedon the category was a question used for training; thisapproach filtered out polythetic questions and left onlyarchetypes.Intuitive analysis suggested that the majority ofquestions have at most a two-part pattern consisting of asyntactic template and/or a keyword identifiable for thatcategory.
A trivial example is disjunction, whose syn-tactic template is auxiliary-initial and correspondingkeyword is ?or?.
Other categories were similarly de-fined either by one or more patterns of initial constitu-ents, or a keyword, or both.
To promotegeneralizability, extra care was given not to overfit thetraining data.
Specifically, keywords or syntactic pat-terns were only used to define categories when theyoccurred more than once or were judged highly diagnos-tic.ExpertClassifier present  ?presentpresent tp fp?present fn tnTable 3.
Contingency Table.The results of the training process are shown in Ta-ble 4.
Results from each category were compiled in 2 x2 contingency tables like Table 3, where tp stands for"true positive" and fn for "false negative".Recall, fallout, precision, and f-measure were calcu-lated in the following way for each category:Recall  =  tp / ( tp + fn )Fallout  =  fp / ( fp + tn )Precision  =  tp / ( tp + fp )F-measure = 2 * Recall * PrecisionRecall + PrecisionRecall and fallout are often used in signal detectionanalysis to calculate a measure called d?
(Green andSwets 1966).
Under this analysis, the performance ofthe classifier is significantly more favorable than underthe F-measure, principally because the fallout, or falsealarm rate, is so low.
Both in training and evaluation,however, the data violate assumptions of normality thatd?
requires.As explained in Section 3, a contribution classifica-tion is the default when no other classification can begiven.
As such, no training data was created for contri-butions.
Likewise frozen expressions were judged to beessentially a closed class of phrases and do not requiretraining.
Absence of training results for these categoriesis represented by double stars in Table 4.During the training process, the classifier was nevertested on unseen data.
A number of factors it difficult toobtain questions suitable for testing purposes.
Brain-stormed questions are an unreliable source of testingdata because they are not randomly sampled.
In gen-eral, corpora proved to be an unsatisfactory source ofquestions due to low inter-rater reliability and skeweddistribution of categories.Low inter-rater reliability often could be traced toanaphora and pragmatic context.
For example, thequestion "Do you know what the concept of group cellis?"
might license a definition or verification, dependingon the common ground.
"Do you know what it is?
"could equally license a number of categories, dependingon the referent of "it".
Such questions are clearly be-yond the scope of a classifier that does not use context.The skewed distribution of the question categoriesand their infrequency necessitates use of an extractionalgorithm to locate them.
Simply looking for questionmarks is not enough: our estimates predict that raterswould need to classify more than 5,000 questions ex-tracted from the Wall Street Journal this way to get amere 20 instances of the rarest types.
A bootstrappingapproach using machine learning is a possible alterna-tive that will be explored in the future (Abney 2002).Regardless of these difficulties, the strongest evalua-tion results from using the classifier in a real world task,with real world data.6 EvaluationThe classifier was used in AutoTutor sessions through-out the year of 2002.
The log files from these sessionscontained 9094 student utterances, each of which wasclassified by an expert.
The expert ratings were com-pared to the classifier's ratings, forming a 2 x 2 contin-gency table for each category as in Table 4.To expedite ratings, utterances extracted from thelog files were split into two groups, contributions andnon-contributions, according to their logged classifica-tion.
Expert judges were assigned to a group and in-structed to classify a set of utterances to one of the 18categories.
Though inter-rater reliability using thekappa statistic (Carletta 1996) may be calculated foreach group, the distribution of categories in the contri-bution group was highly skewed and warrants furtherdiscussion.Skewed categories bias the kappa statistic to lowvalues even when the proportion of rater agreement isvery high (Feinstein and Cicchetti 1990a; Feinstein andCicchetti 1990b).
In the contribution group, judges canexpect to see mostly one category, contribution,whereas judges in the non-contribution group can ex-pect to see the other 17 categories.
Expected agreementby chance for the contribution group was 98%.
Corre-spondingly, inter-rater reliability using the kappa statis-tic was low for the contribution group, .5 despite 99%proportion agreement, and high for non-contributiongroup, .93.However, the .93 inter-rater agreement can be ex-tended to all of the utterance categories.
Due to classi-fier error, the non-contribution group consisted of 38%contributions.
Thus the .93 agreement applies to contri-butions in this group.
Equal proportion of agreementfor contribution classifications in both groups, 99%,suggests that the differences in kappa solely reflect dif-ferences in category skew across groups.
Under thisanalysis, dividing the utterances into two groups im-proved the distribution of categories for the calculationof kappa (Feinstein and Cicchetti  1990b).Expert judges classified questions with a .93 kappa,which supports a monothetic classification scheme forthis application.
In Section 3 the possibility was raisedof a polythetic scheme for question classification, i.e.one in which two categories could be assigned to agiven question.
If a polythetic scheme were truly neces-sary, one would expect inter-rater reliability to suffer ina monothetic classification task.
High inter-rater reli-ability on the monothetic classification task renderspolythetic schemes superfluous for this application.The recall column for evaluation in Table 4 is gener-ally much higher than corresponding cells in the preci-sion column.
The disparity implies a high rate of falsepositives for each of the categories.
One possible ex-planation is the reconstruction algorithm applied duringclassification.
It was observed that, particularly in thelanguage of physics, student used question stems in ut-terances that were not questions, e.g.
?The ball will landwhen ??
Such falsely reconstructed questions accountfor 40% of the questions detected by the classifier.Whether modifying the reconstruction algorithm wouldimprove F-measure, i.e.
improve precision without sac-rificing recall, is a question for future research.The distribution of categories is highly skewed: 97%of the utterances were contributions, and example ques-tions never occurred at all.
In addition to recall, fallout,precision, and F-measure, significance tests were calcu-Training Data AutoTutor PerformanceCATEGORY Recall Fallout Precision F-measure Recall Fallout Precision F-measure Likelihood RatioContribution ** ** ** ** 0.983 0.054 0.999 0.991 1508.260Frozen ** ** ** ** 0.899 0.002 0.849 0.873 978.810ConceptCompletion 0.844 0.035 0.761 0.800 0.857 0.003 0.444 0.585 235.800Interpretation 0.545 0.009 0.545 0.545 0.550 0.000 0.917 0.688 135.360Definition 0.667 0.002 0.941 0.780 0.424 0.001 0.583 0.491 131.770Verification 0.969 0.004 0.969 0.969 0.520 0.004 0.255 0.342 103.880Comparison 0.955 0.011 0.778 0.857 1.000 0.004 0.132 0.233 55.460Quantification 0.949 0.002 0.982 0.966 0.556 0.003 0.139 0.222 43.710Expecational 0.833 0.010 0.833 0.833 1.000 0.000 0.667 0.800 33.870Procedural 0.545 0.009 0.545 0.545 1.000 0.000 1.000 1.000 20.230GoalOrientation 0.926 0.006 0.893 0.909 1.000 0.001 0.143 0.250 14.490Judgmental 0.842 0.010 0.865 0.853 0.500 0.001 0.167 0.250 12.050Disjunction 0.926 0.000 1.000 0.962 0.333 0.000 0.250 0.286 11.910CausalAntecedent 0.667 0.017 0.667 0.667 0.200 0.001 0.083 0.118 8.350*FeatureSpecification 0.824 0.006 0.824 0.824 0.000 0.000 0.000 0.000 0.000*Enablement 0.875 0.006 0.903 0.889 0.000 0.000 0.000 0.000 0.000*CausalConsequent 0.811 0.008 0.882 0.845 0.000 0.000 0.000 0.000 0.000*Example 0.950 0.008 0.826 0.884 ** ** ** ** **Table 4.
Training data and AutoTutor results.lated for each category's contingency table to insure thatthe cells were statistically significant.
Since most of thecategories had at least one cell with an expected valueof less than 1, Fisher's exact test is more appropriate forsignificance testing than likelihood ratios or chi-square(Pedersen 1996).
Those categories that are not signifi-cant are starred; all other categories are significant, p <.001.Though not appropriate for hypothesis testing in thisinstance, likelihood ratios provide a comparison of clas-sifier performance across categories.
Likelihood ratiosare particularly useful when comparing common andrare events (Dunning 1993; Plaunt and Norgard 1998),making them natural here given the rareness of mostquestion categories and the frequency of contributions.The likelihood ratios in the rightmost column of Table 4are on a natural logarithmic scale, -2ln?, so proceduralat e .
5 x 20.23 = 24711 is more likely than goal orientation,at e .
5 x 14.49 = 1401, with respect to the base rate, or nullhypothesis.To judge overall performance on the AutoTutor ses-sions, an average weighted F-measure may be calcu-lated by summing the products of all category F-measures with their frequencies:?
+?
?= N fntpmeasureFFavgThe average weighted F-measure reflects real worldperformance since accuracy on frequently occurringclasses is weighted more.
The average weighted F-measure for the evaluation data is .98, mostly due to thegreat frequency of contributions (.97 of all utterances)and the high associated F-measure.
Without weighting,the average F-measure for the significant cells is .54.With respect to the three applications mentioned, i)tracking student understanding, ii) mixed-initiative dia-logue, and iii) questions answering, the classifier is do-ing extremely well on the first two and adequately onthe last.
The first two applications for the most partrequire distinguishing questions from contributions,which the classifier does extremely well, F-measure =.99.
Question answering, on the other hand, can benefitfrom more precise identification of the question type,and the average unweighted F-measure for the signifi-cant questions is .48.7 ConclusionOne of the objectives of this work was to see how well aclassifier could perform with a minimum of resources.Using no context and only surface features, the classi-fier performed with an average weighted F-measure of.98 on real world data.However, the question remains how performancewill fare as rare questions become more frequent.
Scaf-folding student questions has become a hot topic re-cently (Graesser et al 2003).
In a system that greatlypromotes question-asking, the weighted average of .97will tend to drift closer to the unweighted average of.54.
Thus there is clearly more work to be done.Future directions include using bootstrapping meth-ods and statistical techniques on tutoring corpora andusing context to disambiguate question classification.8 AcknowledgementsThis research was supported by the Office of Naval Re-search (N00014-00-1-0600) and the National ScienceFoundation (SBR 9720314 and REC 0106965).
Anyopinions, findings, and conclusions or recommendationsexpressed in this material are those of the authors anddo not necessarily reflect the views of ONR or NSF.ReferencesAbney, Steven.
2002.
Bootstrapping.
In Proceedings ofthe 40th Annual Meeting of the Association for Com-putational Linguistics, 360-367.Allen, J.F.
1999.
Mixed Initiative Interaction.
Proc.IEEE Intelligent Systems 14(6).Austin, John.
1962.
How to do things with words.
Har-vard University Press, Cambridge, MA.Brill, Eric.
1995.
Transformation-based error-drivenlearning and natural language processing: a case studyin part-of-speech tagging.
Computational Linguistics,21(4), 543-566.Brill, Eric, J. Lin, M. Banko, S. Dumais, and  A. Ng.2001.
Data-intensive question answering.
Proceed-ings of the 10th Annual Text Retrieval Conference(TREC-10).Carletta, J.
1996.
Assessing agreement on classificationtasks: the kappa statistic.
Computational Linguistics,22(2), 249-254.Dunning, Ted.
1993.
Accurate methods for the statisticsof surprise and coincidence.
Computational Linguis-tics 19, 61-74.Feinstein, Alvan R. and Domenic V. Cicchetti.
1990a.High agreement but low kappa: the problems of twoparadoxes.
Journal of Clinical Epidemiology, 43(6),543-549.Feinstein, Alvan R. and Domenic V. Cicchetti.
1990b.High agreement but low kappa: II.
resolving the para-doxes.
Journal of Clinical Epidemiology, 43(6), 551-558.Graesser, Arthur, John Burger, Jack Carroll, Albert Cor-bett, Lisa Ferro, Douglas Gordon, Warren Greiff,Sanda Harabagiu, Kay Howell, Henry Kelly, DianeLitman, Max Louwerse, Allison Moore, Adrian Pell,John Prange, Ellen Voorhees, and Wayne Ward.2003.
Question generation and answering systems,R&D for technology-enabled learning systems: re-search roadmap.
Unpublished manuscript.Graesser, Arthur, Natalie Person, and John Huber.
1992.Mechanisms that generate questions.
In T. Lauer, E.Peacock, and A. Graesser (Eds), Questions and infor-mation systems.
Earlbaum, Hillsdale, NJ.Graesser, Arthur and Natalie Person.
1994.
Question ask-ing during tutoring.
American Educational ResearchJournal, 31(1), 104-137.Graesser, Arthur, Natalie Person, and J.P. Magliano.1995.
Collaborative dialog patterns in naturalisticone-on-one tutoring.
Applied Cognitive Psychology,9, 359-387.Graesser, Arthur, Kurt van Lehn, Carolyn Rose, PamelaJordan, and Derek Harter.
2001.
Intelligent tutoringsystems with conversational dialogue.
AI Magazine22(4), 39-52.Graesser, Arthur, Peter Wiemer-Hastings, K. Wiemer-Hastings, Roger Kreuz, and the TRG.
1999.
AutoTu-tor: A simulation of a human tutor.
Journal of Cogni-tive Systems Research 1, 35-51.Green, David and John Swets.
1966.
Signal detectiontheory and psychophysics.
John Wiley, New York.Harabagiu, Sanda, D. Moldovan, M. Pasca, R. Mihalcea,M.
Surdeanu, R. Bunescu, R. Girju, V. Rus, and P.Morarescu.
2000.
FALCON: Boosting knowledge foranswer engines.
In Proceedings of the 9th Text Re-trieval Conference (TREC-9).Jurafsky, Daniel and James Martin.
2000.
Speech andlanguage processing.
Prentice Hall, NJ.Lehnert, Wendy.
1978.
The Process of Question Answer-ing.
Lawrence Erlbaum Associates, Hillsdale, NJ.McArthur, D., C. Stasz, and M. Zmuidzinas.
1990.
Tu-toring techniques in algebra.
Cognition and Instruc-tion, 7, 197-244.Moldovan, Dan, Sanda Harabagiu, Marius Pasca, RadaMihalcea, Richard Goodrum, Roxana Girju, andVaslie Rus.
1999.
Lasso: a tool for surfing the an-swer net  Proceedings of the 8th Annual Text RetrievalConference (TREC-8), 65-73.Olney, Andrew, Natalie Person, Max Louwerse, and Ar-thur Graesser.
2002.
AutoTutor: a conversational tu-toring environment.
In Proceedings of the 40thAnnual Meeting of the Association for ComputationalLinguistics, Demonstration Abstracts, 108-109.Otero, J. and Arthur  Graesser.
2001.
PREG: Elementsof a model of question asking.
Cognition & Instruc-tion 19, 143-175.Paice, C.D.
1990.
Another stemmer.
SIGIR Forum 24 (3),56-61.Pedersen, Ted.
1996.
Fishing for exactness.
In Proceed-ings of the South-Central SAS Users Group Confer-ence, Austin, TX.Person, Natalie and Arthur Graesser.
1999.
Evolutionof discourse in cross-age tutoring.
In A.M.O?Donnell and A.
King (Eds.
), Cognitive perspec-tives on peer learning (pp.
69-86).
Erlbaum, Mah-wah, NJ.Person, Natalie, Arthur Graesser, L. Bautista, E.C.Mathews, and the Tutoring Research Group 2001.Evaluating student learning gains in two versions ofAutoTutor.
In J. D. Moore, C. L. Redfield, and W. L.Johnson (Eds.)
Artificial intelligence in education:AI-ED in the wired and wireless future (pp.
286-293).
IOS Press, Amsterdam.Person, Natalie, Arthur Graesser, Derek Harter, E. C.Mathews, and the Tutoring Research Group (2000).Dialog move generation and conversation manage-ment in AutoTutor.
Proceedings for the AAAI FallSymposium  Series: Building Dialogue Systems forTutorial Applications.
Falmouth, Massachusetts.Plaunt, Christian and Barbara Norgard.
1998.
An asso-ciation-based method for automatic indexing with acontrolled vocabulary.
Journal of the American Soci-ety of Information Science, 49(10), 888-902.Putnam, R. T. 1987.
Structuring and adjusting contentfor students: A study of  live and simulated tutoringof addition.
American Educational Research Jour-nal, 24, 13-48.Searle, John.
1975.
A taxonomy of illocutionary acts.
InK.
Gunderson, (Ed.
), Language, mind, and knowl-edge.
University of Minnesota Press, Minneapolis,MN.Sekine, S. and R. Grishman.
1995.
A corpus-basedprobabilistic grammar with only two nonterminals.Fourth International Workshop on Parsing Technol-ogy.Soubbotin, M. M., and S. M. Soubbotin.
2002.
Patternsof potential answer expressions as clues to the rightanswers.
Proceedings of the 10th Annual Text Re-trieval Conference (TREC-10).Srihari, Rohini and Wei  Li.
2000.
A question answeringsystem supported by information extraction.
Pro-ceedings of the 6th Applied Natural Language Proc-essing Conference (ANLP-2000), 166-172.Van Dijk, T. A., and W. Kintsch.
1983.
Strategies of dis-course comprehension.
New York: Academic.Voorhees, Ellen.
2001.
Overview of the TREC 2001question answering track.
Proceedings of the 10thAnnual Text Retrieval Conference (TREC-10), 400-410.
