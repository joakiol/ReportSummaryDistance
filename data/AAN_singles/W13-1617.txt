Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 120?128,Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational LinguisticsSentiment Analysis in Social Media TextsAlexandra BalahurEuropean Commission Joint Research CentreVie E. Fermi 274921027 Ispra (VA), Italyalexandra.balahur@jrc.ec.europa.euAbstractThis paper presents a method for sentimentanalysis specifically designed to work withTwitter data (tweets), taking into account theirstructure, length and specific language.
Theapproach employed makes it easily extendibleto other languages and makes it able to pro-cess tweets in near real time.
The main contri-butions of this work are: a) the pre-processingof tweets to normalize the language and gener-alize the vocabulary employed to express sen-timent; b) the use minimal linguistic process-ing, which makes the approach easily portableto other languages; c) the inclusion of higherorder n-grams to spot modifications in the po-larity of the sentiment expressed; d) the use ofsimple heuristics to select features to be em-ployed; e) the application of supervised learn-ing using a simple Support Vector Machineslinear classifier on a set of realistic data.
Weshow that using the training models generatedwith the method described we can improvethe sentiment classification performance, irre-spective of the domain and distribution of thetest sets.1 IntroductionSentiment analysis is the Natural Language Process-ing (NLP) task dealing with the detection and clas-sification of sentiments in texts.
Usually, the classesconsidered are ?positive?, ?negative?
and ?neutral?,although in some cases finer-grained categories areadded (e.g.
?very positive?
and ?very negative?)
oronly the ?positive?
and ?negative?
classes are takeninto account.
Another related task - emotion detec-tion - concerns the classification of text into severalclasses of emotion, usually the basic ones, as de-scribed by Paul Ekman (Ekman, 1992).
Althoughdifferent in some ways, some of the research in thefield has considered these tasks together, under theumbrella of sentiment analysis.This task has received a lot of interest from the re-search community in the past years.
The work doneregarded the manner in which sentiment can be clas-sified from texts pertaining to different genres anddistinct languages, in the context of various applica-tions, using knowledge-based, semi-supervised andsupervised methods (Pang and Lee, 2008).
The re-sult of the analyses performed have shown that thedifferent types of text require specialized methodsfor sentiment analysis, as, for example, sentimentsare not conveyed in the same manner in newspaperarticles and in blogs, reviews, forums or other typesof user-generated contents (Balahur et al 2010).In the light of these findings, dealing with sen-timent analysis in Twitter requires an analysis ofthe characteristics of such texts and the design ofadapted methods.Additionally, the sentiment analysis method em-ployed has to consider the requirements of the fi-nal application in which it will be used.
There isan important difference between deploying a systemworking for languages such as English, for whichnumerous linguistic resources and analysis tools ex-ist and a system deployed for languages with fewsuch tools or one that is aimed at processing datafrom a large set of languages.
Finally, a sentimentanalysis system working with large sets of data (suchas the one found in Twitter) must be able to processtexts fast.
Therefore, using highly complex methodsmay delay producing useful results.In the light of these considerations, this paper120presents a method for sentiment analysis that takesinto account the special structure and linguistic con-tent of tweets.
The texts are pre-processed in or-der to normalize the language employed and re-move noisy elements.
Special usage of language(e.g.
repeated punctuation signs, repeated letters)are marked as special features, as they contribute tothe expressivity of the text in terms of sentiment.Further on, sentiment-bearing words, as they arefound in three highly-accurate sentiment lexicons -General Inquirer (GI) (Stone et al 1966), Linguis-tic Inquiry and Word Count (LIWC) (Tausczik andPennebaker, 2010) and MicroWNOp (Cerini et al2007) - are replaced with unique labels, correspod-ing to their polarity.
In the same manner, modifiers(negations, intensifiers and diminishers) are also re-placed with unique labels representing their seman-tic class.
Finally, we employ supervised learningwith Support Vector Machines Sequential MinimalOptimization (SVM SMO) (Platt, 1998) using asimple, linear kernel (to avoid overfitting of data)and the unigrams and bigrams from the training setas features.
We obtain the best results by usingunique labels for the affective words and the mod-ifiers, unigrams and bigrams as features and posingthe condition that each feature considered in the su-pervised learning process be present in the trainingcorpora at least twice.The remainder of this article is structured as fol-lows: Section 2 gives an overview of the relatedwork.
In Section 3, we present the motivations anddescribe the contributions of this work.
In the fol-lowing section, we describe in detail the process fol-lowed to pre-process the tweets and build the classi-fication models.
In Section 5, we present the resultsobtained using different datasets and combinationsof features and discuss their causes and implications.Finally, Section 6 summarizes the main findings ofthis work and sketches the lines for future work.2 Related WorkOne of the first studies on the classification of po-larity in tweets was (Go et al 2009).
The au-thors conducted a supervised classification study ontweets in English, using the emoticons (e.g.
?:)?,?
:(?, etc.)
as markers of positive and negative tweets.
(Read, 2005) employed this method to generate acorpus of positive tweets, with positive emoticons?
:)?, and negative tweets with negative emoticons?:(?.
Subsequently, they employ different supervisedapproaches (SVM, Na?
?ve Bayes and Maximum En-tropy) and various sets of features and conclude thatthe simple use of unigrams leads to good results, butit can be slightly improved by the combination ofunigrams and bigrams.In the same line of thinking, (Pak and Paroubek,2010) also generated a corpus of tweets for sen-timent analysis, by selecting positive and negativetweets based on the presence of specific emoticons.Subsequently, they compare different supervised ap-proaches with n-gram features and obtain the bestresults using Na?
?ve Bayes with unigrams and part-of-speech tags.Another approach on sentiment analysis in tweetis that of (Zhang et al 2011).
Here, the authors em-ploy a hybrid approach, combining supervised learn-ing with the knowledge on sentiment-bearing words,which they extract from the DAL sentiment dictio-nary (Whissell, 1989).
Their pre-processing stageincludes the removal of retweets, translation of ab-breviations into original terms and deleting of links,a tokenization process, and part-of-speech tagging.They employ various supervised learning algorithmsto classify tweets into positive and negative, usingn-gram features with SVM and syntactic featureswith Partial Tree Kernels, combined with the knowl-edge on the polarity of the words appearing in thetweets.
The authors conclude that the most impor-tant features are those corresponding to sentiment-bearing words.
Finally, (Jiang et al 2011) classifysentiment expressed on previously-given ?targets?in tweets.
They add information on the context ofthe tweet to its text (e.g.
the event that it is relatedto).
Subsequently, they employ SVM and GeneralInquirer and perform a three-way classification (pos-itive, negative, neutral).3 Motivation and ContributionAs we have seen in the previous section, several im-portant steps have already been taken into analyzingthe manner in which sentiment can be automaticallydetected and classified from Twitter data.
The re-search we described in previous section has alreadydealt with some of the issues that are posed by short,121informal texts, such as the tweets.
However, thesesmall snippets of text have several liguistic peculiar-ities that can be employed to improve the sentimentclassification performance.
We describe these pecu-liarities below:?
Tweets are short, user-generated text that maycontain no more than 140 characters (stronglyrelated to the standard 160-character length ofSMS 1).
Users are marked with the ?@?
signand topics with the ?#?
(hashtag) sign.?
In general, the need to include a large quantityof information in small limit of characters leadsto the fact that tweets sometimes have no gram-matical structure, contain misspellings and ab-breviations.?
Some of the tweets are simply posted fromthe websites of news providers (news agencies,newspapers) and therefore they contain only ti-tles of news.
However, subjective tweets, inwhich users comment on an event, are highlymarked by sentiment-bearing expressions, ei-ther in the form of affective words, or by em-ployins specific modalities - e.g.
the use ofcapital letters or repeated punctuation signs tostress upon specific words.
Most of the times,these words are sentiment-bearing ones.?
The language employed in subjective tweets in-cludes a specific slang (also called ?urban ex-pressions?
2) and emoticons (graphical expres-sions of emotions through the use of punctua-tion signs).?
Most of the times, the topic that is discussesin the tweets is clearly marked using hashtags.Thus, there is no need to employ very complexlinguistic tools to determine it.?
In major events, the rate of tweets per minutecommenting or retweeting information sur-passes the rate of thousands per minute.?
Twitter is available in more than 30 languages.However, users tweet in more than 80 lan-guages.
The information it contains can be use-ful to obtain information and updates about, for1http://en.wikipedia.org/wiki/Twitter2http://www.urbandictionary.com/example, crisis events 3, in real time.
In order tobenefit from this, however, a system processingthese texts has to be easily adaptable to otherlanguages and it has to work in near real time.Bearing this in mind, the main contributions webring in this paper are:1.
The pre-processing of tweets to normalize thelanguage and generalize the vocabulary em-ployed to express sentiment.
At this stage, wetake into account the linguistic peculiarities oftweets, regarding spelling, use of slang, punc-tuation, etc., and also replace the sentiment-bearing words from the training data with aunique label.
In this way, the sentence ?I loveroses.?
will be equivalent to the sentence ?I likeroses.
?, because ?like?
and ?love?
are both pos-itive words according to the GI dictionary.
Ifexample 1 is contained in the training data andexample 2 is contained in the test data, replac-ing the sentiment-bearing word with a generallabel increases the chance to have example 2classified correctly.
In the same line of thought,we also replaced modifiers with unique corre-sponding labels.2.
The use of minimal linguistic processing,which makes the approach easily portable toother languages.
We employ only tokenizationand do not process texts any further.
The reasonbehind this choice is that we would like the fi-nal system to work in a similar fashion for asmany languages as possible and for some ofthem, little or no tools are available.3.
The inclusion of bigrams to spot modificationsin the polarity of the sentiment expressed.
Assuch, we can learn general patterns of senti-ment expression (e.g.
?negation positive?, ?in-tensifier negative?, etc.).4.
The use of simple heuristics to select featuresto be employed.
Although feature selection al-gorithms are easy to apply when employing adata mining environment, the final choice is in-fluenced by the data at hand and it is difficult to3http://blog.twitter.com/2012/10/hurricane-sandy-resources-on-twitter.html122employ on new sets of data.
After performingvarious tests, we chose to select the features tobe employed in the classification model basedon the condition that they should occur at leastonce in the training set.5.
The application of supervised learning using asimple Support Vector Machines linear classi-fier on a set of realistic data.We show that using the training models generatedwith the method described we can improve the sen-timent classification performance, irrespective of thedomain and distribution of the test sets.4 Sentiment Analysis in TweetsOur sentiment analysis system is based on a hybridapproach, which employs supervised learning with aSupport Vector Machines Sequential Minimal Opti-mization (Platt, 1998) linear kernel, on unigram andbigram features, but exploiting as features sentimentdictionaries, emoticon lists, slang lists and other so-cial media-specific features.
We do not employ anyspecific language analysis software.
The aim is tobe able to apply, in a straightforward manner, thesame approach to as many languages as possible.The approach can be extended to other languages byusing similar dictionaries that have been created inour team.
They were built using the same dictio-naries we employ in this work and their correctedtranslation to Spanish.
The new sentiment dictionar-ies were created by simultaneously translating fromthese two languages to a third one and consideringthe intersection of the trainslations as correct terms.Currently, new such dictionaries have been createdfor 15 other languages.The sentiment analysis process contains twostages: pre-processing and sentiment classification.4.1 Tweet Pre-processingThe language employed in Social Media sites is dif-ferent from the one found in mainstream media andthe form of the words employed is sometimes notthe one we may find in a dictionary.
Further on,users of Social Media platforms employ a special?slang?
(i.e.
informal language, with special expres-sions, such as ?lol?, ?omg?
), emoticons, and oftenemphasize words by repeating some of their letters.Additionally, the language employed in Twitter hasspecific characteristics, such as the markup of tweetsthat were reposted by other users with ?RT?, themarkup of topics using the ?#?
(hash sign) and ofthe users using the ?@?
sign.All these aspects must be considered at the time ofprocessing tweets.
As such, before applying super-vised learning to classify the sentiment of the tweets,we preprocess them, to normalize the language theycontain.
The pre-processing stage contains the fol-lowing steps:?
Repeated punctuation sign normalizationIn the first step of the pre-processing, we detectrepetitions of punctuation signs (?.
?, ?!?
and???).
Multiple consecutive punctuation signsare replaced with the labels ?multistop?, forthe fullstops, ?multiexclamation?
in the case ofexclamation sign and ?multiquestion?
for thequestion mark and spaces before and after.?
Emoticon replacementIn the second step of the pre-processing, weemploy the annotated list of emoticons fromSentiStrength4 and match the content of thetweets against this list.
The emoticons foundare replaced with their polarity (?positive?
or?negative?)
and the ?neutral?
ones are deleted.?
Lower casing and tokenization.Subsequently, the tweets are lower cased andsplit into tokens, based on spaces and punctua-tion signs.?
Slang replacementThe next step involves the normalization of thelanguage employed.
In order to be able toinclude the semantics of the expressions fre-quently used in Social Media, we employed thelist of slang from a specialized site 5.?
Word normalizationAt this stage, the tokens are compared to entriesin Rogets Thesaurus.
If no match is found, re-peated letters are sequentially reduced to two orone until a match is found in the dictionary (e.g.4http://sentistrength.wlv.ac.uk/5http://www.chatslang.com/terms/social media123?perrrrrrrrrrrrrrrrrrfeeect?
becomes ?perrfeect?,?perfeect?, ?perrfect?
and subsequently ?per-fect?).
The words used in this form are makedas ?stressed?.?
Affect word matchingFurther on, the tokens in the tweet are matchedagainst three different sentiment lexicons: GI,LIWC and MicroWNOp, which were previ-ously split into four different categories (?pos-itive?, ?high positive?, ?negative?
and ?highnegative?).
Matched words are replaced withtheir sentiment label - i.e.
?positive?, ?nega-tive?, ?hpositive?
and ?hnegative?.
A versionof the data without these replacements is alsomaintained, for comparison purposes.?
Modifier word matchingSimilar to the previous step, we employ a listof expressions that negate, intensify or dimin-ish the intensity of the sentiment expressed todetect such words in the tweets.
If such a wordis matched, it is replaced with ?negator?, ?in-tensifier?
or ?diminisher?, respectively.
As inthe case of affective words, a version of the datawithout these replacements is also maintained,for comparison purposes.?
User and topic labelingFinally, the users mentioned in the tweet, whichare marked with ?
@?, are replaced with ?PER-SON?
and the topics which the tweet refers to(marked with ?#?)
are replaced with ?TOPIC?.4.2 Sentiment Classification of TweetsOnce the tweets are pre-processed, they are passedon to the sentiment classification module.
We em-ployed supervised learning using SVM SMO with alinear kernel, based on boolean features - the pres-ence or absence of n-grams (unigrams, bigrams andunigrams plus bigrams) determined from the train-ing data (tweets that were previousely pre-processedas described above).
Bigrams are used specificallyto spot the influence of modifiers (negations, inten-sifiers, diminishers) on the polarity of the sentiment-bearing words.
We tested the approach on differ-ent datasets and dataset splits, using the Weka datamining software 6.
The training models are built ona cluster of computers (4 cores, 5000MB of mem-ory each).
However, the need for such extensive re-sources is only present at the training stage.
Oncethe feature set is determined and the models are builtusing Weka, new examples must only be representedbased on the features extracted from the training setand the classification is a matter of miliseconds.The different evaluations scenarios and results arepresented in the following section.5 Evaluation and DiscussionAlthough the different steps included to eliminatethe noise in the data and the choice of features havebeen refined using our in-house gathered Twitterdata, in order to evaluate our approach and make itcomparable to other methods, we employ three dif-ferent data sets, which are described in detail in thefollowing subsections.5.1 Data Sets?
SemEval 2013 DataThe first one is the data provided for trainingfor the upcoming SemEval 2013 Task 2 ?Sen-timent Analysis from Twitter?
7.
The initialtraining data has been provided in two stages:1) sample datasets for the first task and the sec-ond task and 2) additional training data for thetwo tasks.
We employ the joint sample datasetsas test data (denoted as t?)
and the data releasedsubsequently as training data (denoted as T?
).We employ the union of these two datasets toperform cross-validation experiments (the jointdataset is denoted as T ?
+t?.
The character-istics of the dataset are described in Table 1.On the last column, we also include the base-line in terms of accuracy, which is computed asthe number of examples of the majoritary classover the total number of examples:?
Set of tweets labeled with basic emotions.The set of emotion-annotated tweets by (Mo-hammad, 2012), which we will denote asTweetEm.
It contains 21051 tweets anno-tated according to the Ekman categories of ba-6http://www.cs.waikato.ac.nz/ml/weka/7http://www.cs.york.ac.uk/semeval-2013/task2/124sic emotion - anger, disgust, fear, joy, sadness,surprise.
We employ this dataset to test the re-sults of our best-performing configurations onthe test set.
This set contains a total of 21051tweets (anger - 1555, disgust - 761, fear - 2816,joy - 8240, sadness - 3830, surprise - 3849).
Asmentioned in the paper by (Mohammad, 2012),a system that would guess the classes, wouldperfom at aroung 49.9% accuracy.?
Set of short blog sentences labeled with basicemotions.The set of blog sentences employed by (Amanand Szpakowicz, 2007), which are annotatedaccording to the same basic emotions identi-fied by Paul Ekman, with the difference that the?joy?
category is labeled as ?happy?.
This testset contains also examples which contain noemotions.
These sentences were removed.
Wewill denote this dataset as BlogEm.
This setcontains 1290 sentences annotated with emo-tion (anger - 179, disgust - 172, fear - 115, joy -536, sadness - 173, surprise - 115).
We can con-sider as baseline the case in which all the ex-amples are assigned to the majority class (joy),which would lead to an accuracy of 41.5%.Data #Tweet #Pos.
#Neg.
#Neu.
Bl%T* 19241 4779 2343 12119 62t* 2597 700 393 1504 57T*+t* 21838 5479 2736 13623 62Table 1: Characteristics of the training (T*), testing (t*)and joint training and testing datasets.5.2 Evaluation and ResultsIn order to test our sentiment analysis approach, weemployed the datasets described above.
In the caseof the SemEval data, we performed an exhaustiveevaluation of the possible combination of featuresto be employed.
We tested the entire dataset oftweets (T*+t*) using 10-fold cross-validation.
Thefirst set of evaluations concerned the use of the pre-processed tweets in which the affective words andmodifiers were have not been replaced.
The com-bination of features tested were: unigrams (U ), bi-grams (B), unigrams and bigrams together (U +B)and unigrams and bigrams together, selecting onlythe features that appear at least twice in the data(U +B+FS).
The second set of evaluations aimedat quantifying the difference in performance whenthe affective words and the modifiers were replacedwith generic labels.
We tested the best performingapproaches from the first set of evaluations (U + Band U +B+FS), by replacing the words that werefound in the affect dictionaries and the modifierswith their generic labels.
These evaluations are de-noted as U + B + D and U + B + D + FS.
Theresults of these evaluations are shown in Table 2.Features 10-f-CV T*+t*U 71.82B 66.30U +B 82.01U +B +D 81.15U +B + FS 74.00U +B +D + FS 85.07Table 2: Results in terms of accuracy for 10-fold cross-validation using different combinations of features for thesentiment classification of tweets on the entire set of Se-mEval 2013 training data.The same experiments are repeated by employingT* as training data and t* as test data.
The aim ofthese experiments is to test how well the method canperform on new data.
The results of these evalu-ations are shown in Table 3.
In order to test if in-Features Train(T*) & test(t*)U 74.90B 63.27U +B 77.00U +B +D 76.45U +B + FS 75.69U +B +D + FS 79.97Table 3: Results in terms of accuracy for the differentcombination of features for the sentiment classificationof tweets, using T* as training and t* as test set.deed the use of sentiment dictionaries, modifiers andthe simple feature selection method improves on thebest performing approach that does not employ theseadditional features, we tested both the approaches onthe TweetEm and BlogEm datasets.
In this case,125however, the classification is done among 6 differ-ent classes of emotions.
Although the results arelower(as it can be seen in Table 4, they are compara-ble to those obtained by (Mohammad, 2012) (whenusing U+B) and show an improvement when usingthe affect dictionaries and simple feature selection.They also confirm the fact that the best performanceon the data is obtained replacing the modifiers andthe words found in affect dictionaries with genericlabels, using unigrams and bigrams as and eliminat-ing those n-grams that appear only once.Features Tweet Em Blog EmU +B 49.00 51.08U +B +D + FS 51.08 53.70Table 4: Results in terms of accuracy for the differentcombination of features for the emotion classification oftweets and short blog sentences.The results obtained confirm that the use of uni-gram and bigram features (appearing at least twice)with generalized affective words and modifiers ob-tains the best results.
Although there is a signifi-cant improvement in the accuracy of the classifica-tion, the most important difference in the classifica-tion performance is given by the fact that using thiscombination, the classifier is no longer biased by theclass with the highest number of examples.
We cannotice this for the case of tweets, for which the con-fusion matrices are presented in Table 5 and Table6.
In the table header, the correspondence is: a =joy, b = fear, c = surprise, d = anger, e = disgust, f= sadness.
In the first case, the use of unigrams andbigrams leads to the erroneous classification of ex-amples to the majoritary class.
When employing thefeatures in which affective words and modifiers havebeen replaced with generic labels, the results are notonly improved, but they classifier is less biased to-wards the majoritary class.
In this case, the incorrectassignments are made to classes that are more sim-ilar in vocabulary (e.g.
anger - disgust, anger - sad-ness).
In the case of surprise, examples relate bothto positive, as well as negative surprises.
Therefore,there is a similarity in the vocabulary employed toboth these classes.a b c d e fa 5879 178 865 246 349 723b 657 1327 339 67 59 367c 1243 248 1744 123 129 362d 549 189 79 419 48 271e 167 55 45 89 160 245f 570 405 611 625 233 1386Table 5: Confusion matrix for the emotion classificationof the TweetEm dataset employing the sentiment dictio-naries.a b c d e fa 6895 252 395 57 20 622b 1384 861 207 49 11 302c 1970 147 1258 39 13 421d 884 133 88 101 18 332e 433 54 60 32 40 142f 2097 192 287 72 23 1160Table 6: Confusion matrix for the emotion classificationof the TweetEm dataset without employing the senti-ment dictionaries.5.3 DiscussionFrom the results obtained, we can conclude that, onthe one hand, the best features to be employed insentiment analysis in tweets are unigrams and bi-grams together.
Secondly, we can see that the use ofgeneralizations, by employing unique labels to de-note sentiment-bearing words and modifiers highlyimproves the performance of the sentiment classi-fication.
The usefulness of pre-processing steps isvisible from the fact that among the bigrams thatwere extracted from the training data we can findthe unique labels employed to mark the use of re-peated punctuation signs, stressed words, affectivewords and modifiers and combinations among them.Interesting bigrams that were discovered using thesegeneralizations are, e.g.
?negative multiexclama-tion?, ?positive multiexclamation?, ?positive multi-stop?
- which is more often found in negative tweets-,?negator positive?, ?diminisher positive?, ?mostlydiminisher?, ?hnegative feeling?, ?hnegative day?,?eat negative?,?intensifier hnegative?.
All these ex-tracted features are very useful to detect and classifysentiment in tweets and most of them would be ig-nored if the vocabulary were different in the train-126ing and test data or if, for example, a stressed wordwould be written under different forms or a punctu-ation sign would be repeated a different number oftimes.
We can see that the method employed obtainsgood results, above the ones reported so far with thestate-of-the-art approaches.
We have seen that theuse of affect and modifier lexica generalization hasan impact on both the quantitative performance ofthe classification, as well as on the quality of the re-sults, making the classifier less biased towards theclass with a significantly larger number of exam-ples.
In practice, datasets are not balanced, so it isimporant that a classifier is able to assign (even in-correctly) an example to a class that is semanticallysimilar and not to a class with totally opposite affec-tive orientation.
In this sense, as we have seen in thedetailed results obtained on the TweetEm dataset,it is preferable that, e.g.
the examples pertaining tothe emotion classes of anger and sadness are mis-takenly classified as the other.
However, it is notacceptable to have such a high number of examplesfrom these classes labeled as ?joy?.
Finally, by in-specting some of the examples in the three datasets,we noticed that a constant reason for error remainsthe limited power of the method to correctly spot thescope of the negations and modifiers.
As such, weplan to study the manner in which skip-bigrams (bi-grams made up of non-consecutive tokens) can beadded and whether or not they will contribute to (atleast partially) solve this issue.6 Conclusions and Future WorkIn this article, we presented a method to classifythe sentiment in tweets, by taking into account theirpeculiarities and adapting the features employed totheir structure and content.
Specifically, we em-ployed a pre-processing stage to normalize the lan-guage and generalize the vocabulary employed toexpress sentiment.
This regarded spelling, slang,punctuation, etc., and the use of sentiment dictio-naries and modifier lists to generalize the patternsof sentiment expression extracted from the trainingdata.
We have shown that the use of such general-ized features significantly improves the results of thesentiment classification,when compared to the best-performing approaches that do not use affect dictio-naries.
Additionally, we have shown that we canobtain good results even though we employ min-imal linguistic processing.
The advantage of thisapproach is that it makes the method easily appli-cable to other languages.
Finally, we have shownthat the use of a simple heuristic, concerning filter-ing out features that appear only once, improves theresults.
As such, the method is less dependent on thedataset on which the classification model is trainedand the vocabulary it contains.
Finally, we employeda simple SVM SMO linear classifier to test our ap-proach on three different data sets.
Using such anapproach avoids overfitting the data and, as we haveshown, leads to comparable performances on differ-ent datasets.
In future work, we plan to evaluatethe use of higher-order n-grams (3-grams) and skip-grams to extract more complex patterns of sentimentexpressions and be able to identify more preciselythe scope of the negation.
Additionally, we plan toevaluate the influence of deeper linguistic process-ing on the results, by performing stemming, lem-matizing and POS-tagging.
Further on, we wouldlike to extend our approach on generalizing the se-mantic classes of words and employing unique la-bels to group them (e.g.
label mouse, cat and dog as?animal?).
Finally, we would like to study the per-formance of our approach in the context of tweetsrelated to specific news, in which case these shorttexts can be contextualized by adding further con-tent from other information sources.ReferencesSaima Aman and Stan Szpakowicz.
2007.
Identifyingexpressions of emotion in text.
In Proceedings of the10th international conference on Text, speech and di-alogue, TSD?07, pages 196?205, Berlin, Heidelberg.Springer-Verlag.Alexandra Balahur, Ralf Steinberger, Mijail Kabadjov,Vanni Zavarella, Erik van der Goot, Matina Halkia,Bruno Pouliquen, and Jenya Belyaeva.
2010.
Sen-timent analysis in the news.
In Nicoletta Calzo-lari (Conference Chair), Khalid Choukri, Bente Mae-gaard, Joseph Mariani, Jan Odijk, Stelios Piperidis,Mike Rosner, and Daniel Tapias, editors, Proceed-ings of the Seventh International Conference on Lan-guage Resources and Evaluation (LREC?10), Valletta,Malta, may.
European Language Resources Associa-tion (ELRA).S.
Cerini, V. Compagnoni, A. Demontis, M. Formentelli,and G. Gandini, 2007.
Language resources and lin-127guistic theory: Typology, second language acquisition,English linguistics., chapter Micro-WNOp: A goldstandard for the evaluation of automatically compiledlexical resources for opinion mining.
Franco AngeliEditore, Milano, IT.Paul Ekman.
1992.
An argument for basic emotions.Cognition & Emotion, 6(3-4):169?200, May.Alec Go, Richa Bhayani, and Lei Huang.
2009.
Twit-ter sentiment classification using distant supervision.Processing, pages 1?6.Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and TiejunZhao.
2011.
Target-dependent twitter sentiment clas-sification.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistics:Human Language Technologies - Volume 1, HLT ?11,pages 151?160, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Saif Mohammad.
2012.
#emotional tweets.
In *SEM2012: The First Joint Conference on Lexical and Com-putational Semantics ?
Volume 1: Proceedings of themain conference and the shared task, and Volume 2:Proceedings of the Sixth International Workshop onSemantic Evaluation (SemEval 2012), pages 246?255,Montre?al, Canada, 7-8 June.
Association for Compu-tational Linguistics.Alexander Pak and Patrick Paroubek.
2010.
Twit-ter as a corpus for sentiment analysis and opinionmining.
In Nicoletta Calzolari (Conference Chair),Khalid Choukri, Bente Maegaard, Joseph Mariani,Jan Odijk, Stelios Piperidis, Mike Rosner, and DanielTapias, editors, Proceedings of the Seventh conferenceon International Language Resources and Evaluation(LREC?10), Valletta, Malta; ELRA, may.
EuropeanLanguage Resources Association.
19-21.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Found.
Trends Inf.
Retr., 2(1-2):1?135, January.John C. Platt.
1998.
Sequential minimal optimization:A fast algorithm for training support vector machines.Technical report, Advances in Kernel Methods - Sup-port Vector Learning.Jonathon Read.
2005.
Using emoticons to reduce de-pendency in machine learning techniques for senti-ment classification.
In Proceedings of the ACL Stu-dent Research Workshop, ACLstudent ?05, pages 43?48, Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Philip J.
Stone, Dexter C. Dunphy, Marshall S. Smith,and Daniel M. Ogilvie.
1966.
The General Inquirer:A Computer Approach to Content Analysis.
MITPress.Yla R. Tausczik and James W. Pennebaker.
2010.
ThePsychological Meaning of Words: LIWC and Comput-erized Text Analysis Methods.
Journal of Languageand Social Psychology, 29(1):24?54, March.Cynthia Whissell.
1989.
The Dictionary of Affect inLanguage.
In Robert Plutchik and Henry Kellerman,editors, Emotion: theory, research and experience,volume 4, The measurement of emotions.
AcademicPress, London.Ley Zhang, Riddhiman Ghosh, Mohamed Dekhil, Me-ichun Hsu, and Bing Liu.
2011.
Combining lexicon-based and learning-based methods for twitter senti-ment analysis.
Technical Report HPL-2011-89, HP,21/06/2011.128
