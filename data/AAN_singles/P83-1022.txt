DESIGN OF A KNOWLEDGE-BASED REPORTGENERATORKaren KukichUniversity of PittsburghBell Telephone LaboratoriesMurray ~tll, NJ 07974ABSTRACTKnowledge-Based Report Generation is a techniquefor automatically generating natural language reportsfrom computer databases.
It is so named because itapplies knowledge-based xpert systems oftware to theproblem of text generation.
The first application of thetechnique, a system for generating natural languagestock reports from a daily stock quotes database, is par-tially implemented.
Three fundamental principles of thetechnique are its use of domain-specific semantic andlinguistic knowledge, its use of macro-level semantic andlinguistic constructs (such as whole messages, a phrasallexicon, and a sentence-combining grammar), and itsproduction system approach to knowledge representa-tion.I.
WHAT IS KNOWLEDGE-BASEDREPORT GENERATIONA knowledge-based report generator is a computerprogram whose function is to generate natural anguagesummaries from computer databases.
For example,knowledge-based report generators can be designed togenerate daily stock market reports from a stock quotesdatabase, daily weather reports from a meteorologicaldatabase, weekly sales reports from corporate databases,or quarterly economic reports from U. S. CommerceDepartment databases, etc.
A separate generator mustbe implemented for each domain of discourse becauseeach knowledge-based report generator containsdomain-specific knowledge which is used to inferinteresting messages from the database and to expressthose messages in the sublanguage of the domain ofdiscourse.
The technique of knowledge-based reportgeneration is generalizable across domains, however, andthe actual text generation component of the report gen-erator, which comprises roughly one-quarter of the code,is directly transportable and readily tailorable.Knowledge-based report generation is a practicalapproach to text generation.
It's three fundamentaltenets are the following.
First, it assumes that muchdomain-specific semantic, linguistic, and rhetoricknowledge is required in order for a computer toautomatically produce intelligent and fluent text.Second, it assumes that production system languages,such as those used to build expert systems, are well-suited to the task of representing and integrating seman-tic, linguistic, and rhetoric knowledge.
Finally, it holdsthat macro-level knowledge units, such as whole seman-tic messages, a phrasal lexicon, clausal grammaticalcategories, and a clause-combining grammar, provide anappropriate level of knowledge representation for gen-erating that type of text which may be categorized asperiodic summary reports.
These three tenets guide thedesign and implementation f a knowledge-based reportgeneration system.II.
SAMPLE OUTPUT FROM AKNOWLEDGE-BASED REPORT GENERATORThe first application of the technique ofknowledge-based report generation is a partially imple-mented stock report generator called Aria.
Data from aDow Jones stock quotes database serves as input to thesystem, and the opening paragraphs of a stock marketsummary are produced as output.
As more semantic andlinguistic knowledge about the stock market is added tothe system, it will be able to generate longer, moreinformative reports.Figure 1 depicts a portion of the actual data submit-ted to Ana for January 12, 1983.
A hand drawn graphof the same data is included.
The following text samplesare Ana's interpretation of the data on two differentruns .DOW JONES INDUSTRIALS AVERAGE -- 01/1218301/12 CLOSE 30 INDUS 1083.6101/12 330PM 30 INDUS 1089.4001/12 3PM 30 INDUS 1093.4401/12 230PM 30 INDUS 1100.0701/12 2PM 30 INDUS 1095.3801/12 130PM 30 INDUS 1095.7501/12 IPM 30 INDUS 1095.8401/12 1230PM 30 INDUS 1095.7501/12 NOON 30 INDUS 1092.3501/12 II30AM 30 INDUS I089.4001/12 IIAM 30 INDUS 1085.0801/12 1030AM 30 INDUS 1085.3601/11 CLOSE 30 INDUS 1083.79CLOSING AVERAGE 1083.61 DOWN 0.181451102109810941o9o ~/  ~,1086,...--------,t/ -~,108210 10:30 11 11:30 12 12:30 1 1:30 2 2:30 3 3:30 4Figure 1(1)after climbing steadily through most ofthe morning , the stock market was pusheddownhill late in the day stock prices posteda small loss , with the indexes turning in amixed showing yesterday in brisk trading .the Dow Jones average of 30 industrialssurrendered a 16.28 gain at 4pro and de-clined slightly , finishing the day at 1083.61,of f  0.18 po in ts .
(2)wall street's securities markets rosesteadily through most of the morning , beforesliding downhill late in the day the stockmarket posted a small loss yesterday , withthe indexes finishing with mixed results in ac-tive trading .the Dow Jones average of 30 industrialssurrendered a 16.28 gain at 4pro and de-clined slightly , to finish at 1083.61 , off0.18 points .III.
SYSTEM OVERVIEWIn order to generate accurate and fluent summaries,a knowledge-based report generator performs two maintasks: first, it infers semantic messages from the data inthe database; second, it maps those messages intophrases in its phrasal lexicon, stitching them togetheraccording to the rules of its clause-combining grammar,and incorporating rhetoric constraints in the process.
Asthe work of McKeown I and Mann and Moore 2 demon-strates, neither the problem of deciding what to say northe problem of determining how to say it is trivial, andas'Appelt 3 has pointed out, the distinction between themis not always clear.A.
System ArchitectureA knowledge-based report generator consists of thefollowing four independent, sequential components: 1) afact generator, 2) a message generator, 3) a discourseorganizer, and 4) a text generator.
Data from the data-base serves as input to the first module, which producesa stream of facts as output; facts serve as input to thesecond module, which produces a set of messages as out-put; messages form the input to the third module, whichorganizes them and produces a set of ordered messagesas output; ordered messages form the input to the fourthmodule, which produces final text as output.
Themodules function independently and sequentially for thesake of computational manageability at the expense ofpsychological validity.With the exception of the first module, which is astraightforward C program, the entire system is coded inthe OPS5 production system language.
4 At the time thatthe sample output above was generated, module 2, themessage generator, consisted of 120 production rules;module 3, the discourse organizer contained 16 produc-tion rules; and module 4, the text generator, included109 production rules and a phrasal dictionary of 519entries.
Real time processing requirements for eachmodule on a lightly loaded VAX 11/780 processor werethe following: phase 1 16 seconds, phase 2 - 34seconds, phase 3 - 24 seconds, phase 4 - 1 minute, 59seconds.B.
Knowledge ConstructsThe fundamental knowledge constructs of the sys-tem are of two types: 1) static knowledge structures, ormemory elements, which can be thought of as n-dimensional propositions, and 2) dynamic knowledgestructures, or production rules, which perform pattern-recognition operations on n-dimensional propositions,Static knowledge structures come in five flavors: facts.messages, lexicon entries, medial text elements, andvarious control elements.
Dynamic knowledge constructsoccur in ten varieties: inference productions, orderingproductions, discourse mechanics productions, phraseselection productions, syntax selection productions, ana-phora selection productions, verb morphology produc-tions, punctuation selection productions, writing produc-tions, and various control productions.C.
FunctionsThe function of the first module is to perform thearithmetic computation required to produce facts thatcontain the relevant information eeded to infer interest-ing messages, and to write those facts in the OPS5memory element format.
For example, the fact thatindicates the closing status of the Dow Jones Average of30 Industrials for January 12, 1983 is:(make fact "fname CLb-~rAT "iname DJI "itypeCOMPOS "date 01/12 "hour CLOSE "open-level 1084.25 "high-level 1105.13 "low-level1075.88 "close-level 1083.61 "cumul-dir DN"cumul-deg 0.18)The function of the second module is to interinteresting messages from the facts using inferencingproductions such as the following:146(p instan-mixedup(goal "stat act "op instanmixed)(fact "(name CLSTAT "iname DJI"cumul-dir UP "repdate <date>)(fact "(name ADVDEC "iname NYSE"advances <x> "declines {<y> > <x>})(make message "top GENMKT "subtop MIX"mix mixed "repdate <date>"subjclass MKT "tim close)(make goal "star pend "op writemessage)(remove 1))This production infers that if the closing status of theDow had a direction of "up', and yet the number ofdeclines exceeded the number of advances for the day,then it can be said that the market was mixed.
The mes-sage that is produced looks like this:(make message "repdate 01/12 "top GENMKT"subsubtop nil "subtop MIX "subjclass MKT"dir nil "deg nil "vardeg I nil \] "varlev I nil \[ "mixmixed "chg nil "sco nil "tim close "vartim I nil i"dur nil "vol nil "who nil )The inferencing process in phase 2 is hierarchically con-trolled.Module 3 performs the uncomplicated task ofgrouping messages into paragraphs, ordering messageswithin paragraphs, and assigning a priority number toeach message.
Priorities are assigned as a function oftopic and subtopic.
The system "knows" a default order-ing sequence, and it "knows" some exception rules whichassign higher priorities to messages of special signifi-cance, such as indicators hitting record highs.
As inmodule 2, processing is hierarchically controlled.
Even-tually, modules 2 and 3 should be combined so that theirknowledge could be shared.The most complicated processing is performed bymodule 4.
This processing is not hierarchically con-trolled, but instead more closely resembles control in anATN.
Module 4, the text generator, coordinates andexecutes the following activities: 1) selection of phrasesfrom the phrasal lexicon that both capture the semanticmeaning of the message and satisfy rhetorical con-straints; 2) selection of appropriate syntactic forms forpredicate phrases, such as sentence, participial clause,prepositional phrase, etc.
; 3) selection of appropriateanaphora for subject phrases 4) morphological processingof verbs; 5) interjection of appropriate punctuation; and6) control of discourse mechanics, such as inclusion ofmore than one clause per sentence and more than onesentence per paragraph.The module 4 processor is able to coordinate andexecute these activities because it incorporates andintegrates the semantic, syntactic, and rhetoricknowledge it needs into its static and dynamic knowledgestructures.
For example, a phrasal lexicon entry thatmight match the "mixed market" message is the follow-ing:(make phraselex "top GENMKT "subtop MIX"mix mixed "chg nil "tim close "subjtypeNAME "subjclass MKT *predfs turned Apredfplturned "predpart turning "predinf ~to turnl^predrem ~n a mixed showing\] "fen 9 "rand 5"imp 11)An example of a syntax selection production th,tt wouldselect the syntactic form subordinate-participial-clause san appropriate form for a phrase (a~) in "after risingsteadily through most of the morning") is the following:(p 5 .selectsu borpartpre-selectsyntax(goal ^ stat act "op selectsyntax) ; 1(sentreq "sentstat nil) ; 2(message "foc in "top <t> "tim <> nil"subjclass <sc>)  ; 3(message "foc nil "top <t> "tim <> nil"subjclass <sc>)  ; 4(paramsynforms " uborpartpre <set>)(randnum "randval < <set>)(lastsynform "form << initsent prepp >> )- (openingsynform "form< < suborsent suborpart > >)- (message "foc in "tim close)- .>(remove 1)(make synform "form suborpart )(modify 4 "foc peek )(make goal "star act "op selectsubor)D. Context-Dependent GrammarSyntax selection productions, such as the examt)leabove, comprise a context-dependent, right-branching,clause-combining grammar.
Because of the attribute-value, pattern-recognition nature of these grammar ulesand their use of the lexicon, they may be viewed as ahigh-level variant of a lexical functional grammar.
5 Theefficacy of a low-level functional grammar for text gen-eration has been demonstrated in McKeown's TEXT sys-tem.
6For each message, in sequence, the system firstselects a predicate phrase that matches the semantic on-tent of the message, and next selects a syntactic form.such as sentence or prepositional phrase, into which formthe predicate phrase may be hammered.
The system'sdefault goal is to form complex sentences by combining avariable number of messages expressed m a variety ofsyntactic forms in each sentence.
Every message may beexpressed in the syntactic form of a simple sentence.But under certain grammatical nd rhetorical conditions,which are specified in the syntax selection productions,and which sometimes include looking ahead at the nextsequential message, the system opts for a different syn-tactic form.The right-branching behavior of the system impliesthat at any point the system has the option to lay down aperiod and start a new ~ntence.
It also implies thatembedded subject-complement forms, such as relative;5;6;7147clauses modifying subjects, are trickier to implement(and have not been implemented as yet).
That embed-ded subject complements pose special difficulties houldnot be considered iscouraging.
Developmental linguis-tics research reveals that "operations on sentence sub-jects, including subject complementation a d relativeclauses modifying subjects" are among the last to appearin the acquisition of complex sentences, 7 and aknowledge-based report generator incorporates the basicmechanism for eventually matching messages to nominal-izations of predicate phrases to create subject comple-ments, as well as the mechanism for embedding relativeclauses.IV.
THE DOMAIN-SPECIFICKNOWLEDGE REQUIREMENT TENETHow does one determine what knowledge mustincorporated into a knowledge-based report generator?Because the goal of a knowledge-based report generatoris to produce reports that are indistinguishable fromreports written by people for the same database, it is log-ical to turn to samples of naturally generated text fromthe specific domain of discourse in order to gain insightsinto the semantic, linguistic, and rhetoric knowledgerequirements of the report generator.Research in machine translation s and text under-standing 9 has demonstrated that not only does naturallygenerated text disclose the lexicon and grammar of asublanguage, but it also reveals the essential semanticclasses and attributes of a domain of discourse, as wellas the relations between those classes and attributes.Thus, samples of actual text may be used to build thephrasal dictionary for a report generator and to definethe syntactic categories that a generator must haveknowledge of.
Similarly, the semantic lasses, attributesand relations revealed in the text define the scope andvariety of the semantic knowledge the system mustincorporate in order to infer relevant and interestingmessages from the database.Ana's phrasal lexicon consists of subjects, such as"wall street's ecurities markets", and predicates, uch as"were swept into a broad and steep decline", which areextracted from the text of naturally generated stockreports, The syntactic ategories Ann knows about arethe clausal level categories that are found in the sametext, such as, sentence, coordinate-sentence,subordinate-sentence, subordinate-participial-clause,prepositional-phrase, and others.Semantic analysis of a sample of natural text stockreports discloses that a hierarchy of approximately fortymessage classes accounts for nearly all of the semanticinformation contained in the "core market sentences" ofstock reports.
The term "core market sentences" wasintroduced by Kittredge to refer to those sentences whichcan be inferred from the data in the data base withoutreference to external events such as wars, strikes, andcorporate or government policy making.
1?
Thus, forexample, Ana could say "Eastman Kodak advanced 2 3/4to 85 3/4;" but it could not append "it announceddevelopment of the world's fastest color film for deliveryin 1983.".
Aria currently has knowledge of only six mes-sage classes.
These include the closing market statusmessage, the volume of trading message, and the mixedmarket message, the interesting market fluctuations mes-sage, the closing Dow status message, and the interestingDow fluctuations message.V.
THE PRODUCTION SYSTEMKNOWLEDGE REPRESENTATION TENETThe use of production systems for natural anguageprocessing was suggested as early as 1972 by Heidorn,llwhose production language NLP is currently being usedfor syntactic processing research.
A production systemfor language understanding has been implemented inOPS5 by Frederking.
12 Many benefits are derived fromusing a production system to represent he knowledgerequired for text generation.
Two of the more importantadvantages are the ability to integrate semantic, syntac-tic, and rhetoric knowledge, and the ability to extendand tailor the system easily.A.
Knowledge IntegrationKnowledge integration is evident in the productionrule displayed earlier for selecting the syntactic form ofsubordinate participial clause.
In English, that produc-tion said:IF1) there is an active goal to select a syntactic form2) the sentence requirement has not been satisfied3) the message currently in focus has topic <t>,subject class <sc>,  and some non-nil time4) the next sequential message has the same topic.subject class, and some non-nil time5) the subordinate-participial-clause parameteris set at value <set>6) the current random number is less than <set>7) the last syntactic form used was either aprepositional phrase or a sentence initializer8) the opening syntactic form of the last sentencewas not a subordinate sentence or asubordinate participial clause9) the time attribute of the message in focusdoes not have value 'close'THEN1) remove the goal of selecting a syntactic form2) make the current syntactic form a subordinateparticipial clause3) modify the next sequential message to put itin peripheral focus4) set a goal to select a subordinating conjunction.It should be apparent from the explanation that the ruleintegrates emantic knowledge, such as message topicand time, syntactic knowledge, such as whether thesentence requirement has been satisfied, and rhetoricknowledge, such as the preference to avoid using subor-dinate clauses as the opening form of two consecutivesentences.148B.
Knowledge Tailoring and ExtendingConditions number 5 and 6, the syntactic formparameter and the random number, are examples of con-trol elements that are used for syntactic tailoring.
Asyntactic form parameter may be preset at any valuebetween 1 and 11 by the system user.
A value of 8, forexample, would result in an 80 percent chance that therule in which the parameter occurs would be satisfied ifall its other conditions were satisfied.
Consequently, on20 percent of the occasions when the rule would havebeen otherwise satisfied, the syntactic form parameterwould prevent the rule from firing, and the systemwould be forced to opt for a choice of some other syn-tactic form.
Thus, if the user prefers reports that are lowon subordinate participial clauses, the subordinate parti-cipial clause parameter might be set at 3 or lower.The following production contains the bank ofparameters as they were set to generate text sample (2)above:(p _ l.setparams(goal "stat act "op setparams)(remove 1)(make paramsyllables "val 30)(make parammessages "val 3)(make paramsynforms"sentence 11"coorsent 11"suborsent 11"prepphrase 11"suborsentpre 5"suborpartpre 8"suborsentpost 8"suborpartpost 11"subol'partsentpost I 1When sample text (1) was generated, all syntactic formparameters were set at 11.
The first two parameters inthe bank are rhetoric parameters.
They control themaximum length of sentences in syllables (roughly) andin number of messages per sentence.Not only does production system knowledgerepresentation allow syntactic tailoring, but it also per-mits semantic tailoring.
Aria could be tailored to focuson particular stocks or groups of stocks to meet theinformation eeds of individual users.
Furthermore, aproduction system is readily extensible.
Currently, Anahas only a small amount of general knowledge about thestock market and is far from a stock market expert.
Butany knowledge that can be made explicit can be added tothe system prolonged incremental growth in theknowledge of the system could someday result in a sys-tem that truly is a stock market expert.Vl.
THE MACRO-LEVELKNOWLEDGE CONSTRUCTS TENETThe problem of dealing with the complexity ofnatural anguage is made much more tractable by work-ing in macro-level knowledge constructs, such as seman-tic units consisting of whole messages, lexical iter-?
~,~,a-sisting of whole phrases, syntactic categories at theclause level, and a clause-combining grammar.
Macro-level processing buys linguistic fluency at the cost ofsemantic and linguistic flexibility.
However, the loss offlexibility appears to be not much greater than the con-straints imposed by the grammar and semantics of thesublanguage of the domain of discourse.
Furthermore,there may be more to the notion of macro-level semanticand linguistic processing than mere computationalmanageability.The notion of a phrasal lexicon was suggested byBecker, 13 who proposed that people generate utterances"mostly by stitching together swatches of text that theyhave heard before.
Wilensky and Arens have experi-mented with a phrasal exicon in a language understand-ing system.
14 I believe that natural language behaviorwill eventually be understood in terms of a theory ofstratified natural language processing in which macro-level knowledge constructs, such as those used in aknowledge-based report generator, occur at one of thehigher cognitive gtrata.A poor but useful analogy to mechanical gear-shifting while driving a car can be drawn.
Just as driv-ing in third gear makes most efficient use of anautomobile's resources, so also does generating languagein third gear make most efficient use of human informa-tion processing resources.
That is, matching wholephrases and applying a clause-combining grammar iscognitively economical.
But when only a near match fora message can be found in a speaker's phrasal diction-ary, the speaker must downshift into second gear, andeither perform some additional processing on the nhraseto transform it into the desired form to match the mes-sage, or perform some processing on the message totransform it into one that matches the phrase.
And ifnot even a near match for a message can be found, thespeaker must downshift into first gear and either con-struct a phrase from elementary texicai items, includingwords, prefixes, and suffixes, or reconstruct the mes-sage.As currently configured, a knowledge-based textgenerator operates only in third gear.
Because the unitsof processing are linguistically mature whole phrases, thereport generation system can produce fluent text withouthaving the detailed knowledge-needed to constructmature phrases from their elementary components.
Butthere is nothing except the time and insight of a systemimplementor to prevent this detailed knowledge frombeing added to the system.
By experimenting with addi-tional knowledge, a system could gradually be extendedto shift into lower gears, to exhibit greater interactionbetween semantic and linguistic components, and to domore flexible, if not creative, generation of semantic149messages and linguistic phrases.
A knowledge-basedreport generator may be viewed as a starting tool formodeling a stratiform theory of natural language pro-cessing.VII.
CONCLUSIONKnowledge-based report generation is practicalbecause it tackles a moderately ill-defined problem withan effective technique, namely, a macro-level,knowledge-based, production system technique.
Stockmarket reports are typical instances of a whole class ofsummary-type periodic reports for which the scope andvariety of semantic and linguistic complexity is greatenough to negate a straightforward algorithmic solution,but constrained enough to allow a high-level cross-wiseslice of the variety of knowledge to be effectively incor-porated into a production system.
Even so, it will besome time before the technique is cost effective.
Thetime required to add knowledge to a system is greaterthan the time required to add productions to a traditionalexpert system.
Most of the time is spent doing seman-tic analysis for the purpose of creating useful semanticclasses and attributes, and identifying the relationsbetween them.
Coding itself goes quickly, but then thesystem must be tested and calibrated (if the guesses onthe semantics were close) or redone entirely (if theguesses were not close).
Still, the initial success of thetechnique suggests its value both as a basic research tool,for exploring increasingly more detailed semantic andlinguistic processes, and as an applied research tool, fordesigning extensible and tailorable automatic report gen-erators.ACKNOWLEDGEMENTl'wish to express my deep appreciation to MichaelLesk for his unfailing guidance and support in thedevelopment of this project.REFERENCES1.
Kathleen R. McKeown, "The TEXT System forNatural Language Generation: An Overview,"Proceedings of the Twentieth Annual Meeting of theAssociation for Computational Linguistics, Toronto,Canada (1982).2.
James A. Moore and William C. Mann, "'ASnapshot of KDS: A Knowledge Delivery System,"in Proceedings of the 17th Annual Meeting of theAssociation for Computational linguistics, La Jolla,California (11-12 August 1979).3.
Douglas E. Appelt, "Problem Solving Applied toLanguage Generation," pp.
59-63 in Proceedings ofthe 18th Annual Meeting of the Association for Com-putational Linguistics, University of Pennsylvania,Philadelphia, PA (June 19-22,1980).4.
C .L .
Forgy, "OPS-5 User's Manual," CMU-CS-81-135, Dept of Computer Science, Carnegie-Mellon University, Pittsburgh, PA 15213 (July1981).5.
Joan Bresnan and Ronald M. Kaplan, "Lexical-Functional Grammar: A Formal System for Gram-matical Representation," Occasional Paper #13,MIT Center for Cognitive Science (1982).6.
Kathleen Rose McKeown, "Generating NaturalLanguage Text in Response to Questions aboutDatabase Structure," Doctoral Dissertation,University of Pennsylvania Computer and Informa-tion Science Department (1982).7.
Melissa Bowerman, "The Acquisition of ComplexSentences," pp.
285-305 in Language Acquisition,ed.
Michael Garman, Cambridge University Press,Cambridge (1979).8.
Richard Kittredge and John Lehrberger, Sub-languages: Studies of Language in Restricted Seman-tic Domains, Walter DeGruyter, New York (inpress).9.
Naomi Sager, "Information Structures in Texts of aSublanguage," in The Information Communi~: Alli-ance for Progress - Proceedings of the 44th ASISAnnual Meeting, Volume 18, Knowlton IndustryPublications for the American Society for Informa-tion Science, White Plains, N.Y. (October 1981).IO.
Richard I. Kittredge, "Semantic Processing ofTexts in Restricted Sublanguages," Computers andMathematics with Applications 8(0), Pergamon Press(1982).11.
George E. Heidorn, "Natural Language Inputs to aSimulation Programming System,'" NPS-55HD72101A, Naval Postgraduate School, Mon-terey, CA (October 1972).12.
Robert E. Frederking, A Production SystemApproach to Language Understanding, To appear(1983).13.
Joseph Becket, "The Phrasal Lexicon," pp.
70-73in Theoretical Issues in Natural Language Process-ing, ed.
B. I. Nash-Webber, Cambridge, Mas-sachusetts (10-13 June 1975).14.
Robert Wilensky and Yigel Arens, "'PHRAN -- AKnowledge-Based Natural Language Under-stander," pp.
117-121 in Proceedings of the 18thAnnual Meeting of the Association for ComputationalLinguistics, University of Pennsylvania.
Philadel-phia, Pennsylvania (June 19-22, 1980).1-50
