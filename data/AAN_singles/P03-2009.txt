Discourse chunking: a tool in dialogue act taggingT.
Daniel MidgleySchool of Computer Science and Software EngineeringDiscipline of LinguisticsUniversity of Western Australiadmidgley@arts.uwa.edu.auAbstractDiscourse chunking is a simple way tosegment dialogues according to how dia-logue participants raise topics and negoti-ate them.
This paper explains a methodfor arranging dialogues into chunks, andalso shows how discourse chunking canbe used to improve performance for adialogue act tagger that uses a case-basedreasoning approach.1 Dialogue act taggingA dialogue act (hereafter DA) is an encapsulationof the speaker?s intentions in dialogue?what thespeaker is trying to accomplish by saying some-thing.
In DA tagging (similar to part-of-speechtagging), utterances in a dialogue are tagged withthe most appropriate speech act from a tagset.
DAtagging has application in NLP work, includingspeech recognition and language understanding.The Verbmobil-2 corpus was used for thisstudy, with its accompanying tagset, shown inTable 1.1.Much of the work in DA tagging (Reithinger,1997; Samuel, 2000; Stolcke et al 2000; Wright,1998) uses lexical information (the words or n-grams in an utterance), and to a lesser extentsyntactic and phonological information (as withprosody).
However, there has traditionally been alack of true discourse-level information in tasksinvolving dialogue acts.
Discourse information istypically limited to looking at surrounding DA tags(Reithinger, 1997; Samuel, 2000).
Unfortunately,knowledge of prior DA tags does not alwaystranslate to an accurate guess of what?s comingnext, especially when this information is imperfect.Theories about the structure of dialogue (forexample, centering [Grosz, Joshi, & Weinstein1995], and more recently Dialogue MacrogameTheory [Mann 2002]) have not generally beenapplied to the DA tagging task.
Their use amountsto a separate tagging task of its own, with theconcomitant time-consuming corpus annotation.In this work, I present the results from a DAtagging project that uses a case-based reasoningsystem (after Kolodner 1993).
I show how theresults from this DA tagger are improved by theuse of a concept I call ?discourse chunking.
?Discourse chunking gives information about thepatterns of topic raising and negotiation in dia-Tag ExampleACCEPT sounds good to meBACKCHANNEL mhmBYE see youCLARIFY I said the thirdCLOSE okay <uhm> so I guess that is itCOMMIT I will get that arranged thenCONFIRM well I will see you <uhm> at theairport on the thirdDEFER and I will get back to you on thatDELIBERATE so let us seeDEVIATE_SCENARIO oh I have tickets for the opera onFridayEXCLUDE January is basically shot for meEXPLAINED_REJECT I am on vacation thenFEEDBACK goshFEEDBACK_NEGATIVE not reallyFEEDBACK_POSITIVE okayGIVE_REASON because that is when the expressflights areGREET hello MiriamINFORM <uhm> I I have a list of hotelshereINIT so we need to schedule a trip toHanoverINTRODUCE Natalie this is ScottNOT_CLASSIFIABLE and <uh>OFFER <uhm> would you like me to callPOLITENESS_FORMULA good of you to stop byREFER_TO_SETTING want to step into your office sincewe are standing right outside of itREJECT no that is bad for me unfortunatelyREQUEST you think so?REQUEST_CLARIFY I thought we had said twelve noonREQUEST_COMMENT is that alright with youREQUEST_COMMIT can you take care of <uhm>arranging those reservationsREQUEST_SUGGEST do you have any preferenceSUGGEST we could travel on a MondayTHANK okay thanks JohnTable 1.1.
The tagset for the Verbmobil-2 corpus.
(Verbmobil 2003)logue, and where an utterance fits within thesepatterns.
It is also able to use existing DA taginformation within the corpus, without the need forseparate annotation.2 Discourse chunkingIn order to accomplish a mutual goal (for example,two people trying to find a suitable appointmenttime), dialogue participants engage in predictablekinds of activity, structuring the conversation in acoherent way in order to accomplish their goals.Alexandersson et al (1997) have noted thatthese conversations tend to follow certain patterns,particularly with regard to the way that topics getraised and dealt with:Hello The dialogue participants greet each other.
Theyintroduce themselves, unveil their affiliation, or theinstitution or location they are from.Opening The topic to be negotiated is introduced.Negotiation The actual negotiation, between openingand closing.Closing The negotiation is finished (all participantshave agreed), and the agreed-upon topic is (sometimes)recapitulated.Good Bye The dialogue participants say good bye toeach other.Within a conversation, the opening-negotiation-closing steps are often repeated in a cyclical pat-tern.This work on discourse chunking combines theopening, negotiation, and closing sections into asingle chunk.
One reason for this is that these partsof the conversation tend to act as a single chunk;when they appear, they regularly appear togetherand in the same order.
Also, some of these partsmay be missing; a topic of negotiation is frequentlybrought up and resolved without an explicit open-ing or closing.
Very often, the act of beginning atopic of negotiation defines the opening by itself,and the act of beginning a new negotiation entailsthe closing of the previous one.A slightly simplified model of conversation,then, appears in Figure 2.1.In this model, participants greet each other, en-gage in a series of negotiations, and finish theconversation when the goals of the dialogue aresatisfied.These three parts of the conversation are ?dia-logue chunks?.
These chunks are relevant from aDA tagging perspective.
For example, the DA tagsused in one of these chunks are often not used inother chunks.
For an obvious example, it would bealmost unheard of for the GREET tag to appear inthe ?Good Bye?
chunk.
Other DA?s (such asFEEDBACK_POSITIVE) can occur in any of thethree chunks.
Knowing which chunk we are in, andwhere we are within a chunk, can facilitate thetagging task.Within chunks, some patterns emerge.
Note thatin the example from the Verbmobil-2 corpus(shown in Table 2.1), a negotiation topic is raised,and dealt with (by an ACCEPT speech act).
Thent h e r e  f o l l o w s  a  s e q u e n c e  o fFEEDBACK_POSITIVEs as the negotiation topicwinds down.
This ?winding down?
activity iscommon at the end of a negotiation chunk.
Then anew topic is raised, and the process continues.One-word utterances such as ?okay?
or ?yeah?are particularly problematic in this kind of taskbecause they have rather general semantic contentand they are commonly used in a wide range ofcontexts.
The word ?yeah?
on its own, for exam-ple, can indicate acceptance of a proposition, mereSpeaker ID Words DA TagKNT some other time ohactually I see that Ihave got some freetime in like the fifthsixth and seventh ofJanuarySUGGESTKNT how does that NOT_CLASSIFIABLELMT yeah that is fine ACCEPTKNT great so let us do thatthenFEEDBACK_POSITIVELMT okay FEEDBACK_POSITIVEKNT okay FEEDBACK_POSITIVELMT okay good FEEDBACK_POSITIVETable 2.1 An example of tagged conversation fromthe Verbmobil-2 corpus.HelloNegotiationGood ByeFigure 2.1.
A slightly simplified model of conversation.acknowledgement of a proposition, feedback,deliberation, or a few of these at once (Core &Allen 1997).
In Verbmobil-2, these utterances canb e  l a b e l e d  e i t h e r  A C C E P T ,FEEDBACK_POSITIVE, BACK-CHANNEL, orREQUEST_COMMENT.
Without knowing wherethe utterance appears within the structure of thedialogue, these utterances are very difficult toclassify.Some previous work has used prosody to solvethis kind of problem (as with Stolcke 2000).
Ipropose discourse chunks as an alternative method.It can pull information from the text alone, withoutthe computational overhead that prosody canentail.3 Chunk segmentationJust where do the discourse chunk boundaries lie?For this exercise, I have constructed a very simpleset of rules to determine chunk boundaries.
Theserules come from my observations; future work willinvolve automatic chunk segmentation.
However,these rules do arise from a principled assumption:the raising of a new topic shows the beginning of adiscourse chunk.
Therefore, a speech act that(according to the definitions in Alexandersson1997) contains a topic or proposition represents thebeginning of a discourse chunk.By definition, only four DA?s contain or maycontain a topic or proposition.
These are INIT,EXCLUDE, REQUEST_SUGGEST, and SUGGEST.3.1 Chunking rulesThe chunking rules are as follows:1.
The first utterance in a dialogue is always thestart of chunk 1 (hello).2.
The first I N I T  or S U G G E S T  orREQUEST_SUGGEST or EXCLUDE in a dia-logue is the start of chunk 2 (negotiation).3.
INIT, SUGGEST, REQUEST_SUGGEST, orEXCLUDE marks the start of a subchunk withinchunk 2.4.
If the previous utterance is also the start of achunk, and if it is spoken by the same person,then this utterance is considered to be a con-tinuation of the chunk, and is not marked.5.
The first BYE is the start of chunk 3 (goodbye).Items within a chunk are numbered evenly from 1(the first utterance in a chunk) to 100 (the last), asshown in Table 3.1.
This normalizes the chunkdistances to facilitate comparison between utter-ances.4 The case-based reasoning (CBR) taggerA thorough discussion of this CBR tagger goesbeyond the scope of this paper, but a few com-ments are in order.Case-based reasoning (Kolodner 1993)  is aform of machine learning that uses examples.
Ingeneral, classification using a case-based reasonerinvolves comparing new instances (in this case,utterances) against a database of correctly-taggedinstances.
Each new instance is marked with thesame tag of its ?nearest neighbour?
(that is, theclosest match) from the database.
A k-nearestneighbour approach selects the closest k matchesfrom the database to be committee members, andthe committee members ?vote?
on the correctclassification.
In this implementation, each com-mittee member gets a vote equal to its similarity tothe test utterance.
Different values of k performedbetter in different aspects of the test, but this workuses k = 7 to facilitate comparison of results.SpkrIDWords DiscourseChunkDA TagKNT some other timeoh actually I seethat I have gotsome free time inlike the fifth sixthand seventh ofJanuary1 SUGGESTKNT how does that 17.5 NOT_CLASSIFIABLELMT yeah that is fine 34 ACCEPTKNT great so let us dothat then50.5 FEEDBACK_POSITIVELMT okay 67 FEEDBACK_POSITIVEKNT okay 83.5 FEEDBACK_POSITIVELMT okay good 100 FEEDBACK_POSITIVETable 3.1 An example from the corpus, now taggedwith discourse chunks.The choice of features largely follows those ofSamuel 2000, and are as follows:?
Speaker change?
Word number?
Word similarity?
n-gram similarity?
Previous DA tagand the following two features not included inthat study,?
2-previous DA tagInclusion of this feature enables more completeanalysis of previous DA tags.
Both ?previous DAtag?
and ?2-previous DA tag?
features use the ?bestguess?
for previous utterances rather than the?right answer?, so this run allows us to test per-formance even with incomplete information.?
Discourse chunk tagDistances for this tag were computed by dividingthe larger discourse chunk number from thesmaller.
Comparing two ?chunk starter?
utteranceswould give the highest similarity of 1, and com-paring a chunk starter (1) to a chunk-ender (100)would give a lower similarity (.01).Not all features are equally important, and so anEvolutionary Programming algorithm (adaptedfrom Fogel 1994) was used to weight the features.Weightings were initially chosen randomly foreach member of a population of 100, and the 10best performers were allowed to ?survive?
and?mutate?
their weightings by a Gaussian randomnumber.
This was repeated for 10 generations, andthe weightings from the highest performer wereused for the CBR tagging runs.A total of ten stopwords were used (the, of, and,a, an, in, to, it, is, was), the ten most commonwords from the BNC (Leech, Rayson, & Wilson2001).
These stopwords were removed whenconsidering word similarity, but not n-gram simi-larity, since these low-content words are useful fordistinguishing sequences of words that wouldotherwise be very similar.The database consisted of 59 hand-tagged dia-logues (8398 utterances) from the Verbmobil-2corpus.
This database was also automaticallytagged with discourse chunks according to therules above.
The test corpus consisted of 20 dia-logues (2604 utterances) from Verbmobil-2.
Thiscorpus was tagged with correct information ondiscourse chunks; however, no information wasgiven on the DA tags themselves.5 Discussion and future workTable 5.1 shows the results from two DA taggingruns using the case-based reasoning tagger: onerun without discourse chunks, and one with.Without discourse chunks With discourse chunks53.68%(1385/2604 utterances)65.44%(1704/2604 utterances)Table 5.1: Overall accuracy for the CBR taggerTo put these results in perspective, human per-formance has been estimated at about 84% (Stol-cke 2000), since human taggers sometimesdisagree about intentions, especially when speakersperform more than one dialogue act in the sameutterance.
Much of the recent DA tagging work(using 18-25 tags) scores around the mid-fifty tomid-sixty percentiles in accuracy (see Stolcke 2000for a review of similar work).
This work uses theVerbmobil-2 tagset of 32 tags.It could be argued that the discourse chunk in-formation, being based on tags, gives the DAtagger extra information about the tags themselves,and thus gives an unfair ?boost?
to the perform-ance.
At present it is difficult to say if this is theonly reason for the performance gains.
If this werethe case, we would expect to see improvement inrecognition for the four tags that are ?chunk start-ers?, and less of a gain in those that are not.In the test run with discourse chunks, however,we see across-the-board gains in almost all catego-ries, regardless of whether they begin a chunk ornot.
Table 5.2 shows performance measured interms of the well-known standards of precision,recall, and f-measure.One notable exception to the upward trend isEXCLUDE, a beginning-of-chunk marker, whichperformed slightly worse with discourse chunks.This would suggest that chunk information alone isnot enough to account for the overall gain.
BothACCEPT and FEEDBACK_POSITIVE improvedslightly, suggesting that discourse chunks wereable to help disambiguate these two very similartags.Table 5.3 shows the improvement in taggingscores for one-word utterances, often difficult totag because of their general use and low informa-tion.
These words are more likely to be taggedACCEPT when they appear near the beginning of achunk, and FEEDBACK_POSITIVE when theyappear nearer the end.
Discourse chunks help theirclassification by showing their place in the dia-logue cycle.One weakness of this project is that it assumesknowledge of the correct chunk tag.
The testcorpus was tagged with the ?right answers?
for thechunks.
Under normal circumstances, the corpuswould be tagged with the ?best guess,?
based onthe DA tags from an earlier run.
However, the goalfor this project was to see if, given perfect infor-mation, discourse chunking would aid DA taggingperformance.
The performance gains are persua-sive evidence that it does.
Ongoing work involvesseeing how accurately a new corpus can be taggedwith discourse chunks, even when the DA tags areunknown.6 AcknowledgementsThis work was supported by an Australian Post-graduate Award.
Thanks to Cara MacNish andShelly Harrison for supervision and advice.
Manythanks to Verbmobil for generously allowing useof the corpus which formed the basis of this pro-ject.ReferencesJ.
Alexandersson, B. Buschbeck-Wolf, T. Fujinami, E.Maier, N. Reithinger, B. Schmitz, and M. Siegel.1997.
Dialogue Acts in Verbmobil-2.
Verbmobil Re-port 204.M.
G. Core, and J. F. Allen.
1997.
Coding dialogs withthe DAMSL annotation scheme.
In Working Notes ofthe AAAI Fall Symposium on Communicative Actionin Humans and Machines.
Cambridge, MA.D.
Fogel.
1994.
An introduction to evolutionary com-putation.
Australian Journal of Intelligent Informa-tion Processing Systems, 2:34?42.B.
J. Grosz, A. K. Joshi, and S. Weinstein.
1995.
Cen-tering: A framework for modelling the local coher-ence of discourse.
Computational Linguistics,21(2):203?225J.
Kolodner.
1993.
Case-Based Reasoning.
AcademicPress/Morgan Kaufmann.G.
Leech, P. Rayson, and A. Wilson.
2001.
WordFrequencies in Written and Spoken English: basedon the British National Corpus.
Longman.W.
Mann.
2002.
Dialogue Macrogame Theory.
InProceedings of the 3rd SIGdial Workshop on Dis-course and Dialogue, pages 129?141, PhiladelphiaPA.N.
Reithinger and M. Klesen.
1997.
Dialogue actclassification using language models.
In G. Kokki-nakis, N. Fakotakis, and E. Dermatas, editors, Pro-ceedings of the 5th European Conference on SpeechCommunication and Technology, volume 4, pages2235-2238, Rhodes, Greece.K.
Samuel.
2000.
Discourse learning: An investigationof Dialogue Act tagging using transformation-basedlearning.
Ph.D. thesis, University of Delaware.A.
Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates,D.
Jurafsky, P. Taylor, R. Martin, C. Van Ess-Dykema, M. Meteer.
2000.
Dialogue act modelingfor automatic tagging and recognition of conversa-t ional  speech.
Computational Linguistics,26(3):339?373.Verbmobil.
2003.
?Verbmobil?
[online].
Available:<http://verbmobil.dfki.de/>.H.
Wright.
1998.
Automatic utterance type detectionusing suprasegmental features.
In ICSLP (Interna-tional Conference on Spoken Language Processing)'98.
Sydney, Australia.Without discourse chunks With discourse chunksTag precision recall f-measure precision recall f-measureINIT 0.590 0.411 0.484 0.735 0.446 0.556SUGGEST 0.446 0.399 0.421 0.778 0.912 0.839REQUEST_SUGGEST 0.308 0.078 0.125 0.550 0.216 0.310EXCLUDE 0.500 0.063 0.111 0.143 0.031 0.051GREET 0.926 0.926 0.926 0.926 0.926 0.926BACKCHANNEL 0.824 0.875 0.848 0.824 0.875 0.848BYE 0.719 0.976 0.828 0.816 0.952 0.879POLITENESS_FORMULA 0.821 0.742 0.780 0.889 0.774 0.828THANK 0.875 0.636 0.737 0.875 0.636 0.737FEEDBACK_POSITIVE 0.567 0.843 0.678 0.615 0.839 0.710COMMIT 0.778 0.500 0.609 0.733 0.393 0.512DELIBERATE 0.568 0.582 0.575 0.600 0.570 0.584INFORM 0.493 0.682 0.572 0.655 0.812 0.725FEEDBACK_NEGATIVE 0.700 0.304 0.424 0.667 0.348 0.457REQUEST_COMMENT 0.425 0.327 0.370 0.500 0.288 0.366REJECT 0.500 0.278 0.357 0.316 0.333 0.324NOT_CLASSIFIABLE 0.534 0.265 0.354 0.696 0.274 0.393DEFER 0.750 0.214 0.333 0.800 0.286 0.421ACCEPT 0.392 0.290 0.333 0.476 0.429 0.451REQUEST 0.351 0.191 0.248 0.525 0.456 0.488REQUEST_CLARIFY 0.400 0.130 0.197 0.600 0.196 0.295EXPLAINED_REJECT 0.333 0.133 0.190 0.600 0.600 0.600GIVE_REASON 0.200 0.077 0.111 0.182 0.077 0.108CLOSE 0.333 0.063 0.105 0.500 0.063 0.111CLARIFY 0.400 0.056 0.098 0.000 0.000 0.000CONFIRM 0.000 0.000 0.000 0.500 0.074 0.129DEVIATE_SCENARIO 0.000 0.000 0.000 0.000 0.000 0.000Table 5.2: Results for all DA types that appeared more than ten times in the corpus.
The first group of four DA?srepresents those that signal the beginning of a discourse chunk; the second group shows those that do not.Percent classified correctly withoutdiscourse chunk informationPercent classified correctly withdiscourse chunk informationokay 71.90 (151/210) 75.24 (158/210)yeah 69.90 (72/103) 74.76 (77/103)right 62.16 (23/37) 72.97 (27/37)mhm 88.23 (60/68) 88.23 (60/68)bye 93.33 (14/15) 93.33 (14/15)Table 5.3: Some examples of one-word utterances in the corpus, before and after discourse chunking.
