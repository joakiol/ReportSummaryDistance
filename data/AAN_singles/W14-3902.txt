Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 13?23,October 25, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsCode Mixing: A Challenge for Language Identification in the Language ofSocial MediaUtsab Barman, Amitava Das?, Joachim Wagner and Jennifer FosterCNGL Centre for Global Intelligent Content, National Centre for Language TechnologySchool of Computing, Dublin City University, Dublin, Ireland?Department of Computer Science and EngineeringUniversity of North Texas, Denton, Texas, USA{ubarman,jwagner,jfoster}@computing.dcu.ieamitava.das@unt.eduAbstractIn social media communication, multilin-gual speakers often switch between lan-guages, and, in such an environment, au-tomatic language identification becomesboth a necessary and challenging task.In this paper, we describe our work inprogress on the problem of automaticlanguage identification for the languageof social media.
We describe a newdataset that we are in the process of cre-ating, which contains Facebook posts andcomments that exhibit code mixing be-tween Bengali, English and Hindi.
Wealso present some preliminary word-levellanguage identification experiments usingthis dataset.
Different techniques areemployed, including a simple unsuper-vised dictionary-based approach, super-vised word-level classification with andwithout contextual clues, and sequence la-belling using Conditional Random Fields.We find that the dictionary-based approachis surpassed by supervised classificationand sequence labelling, and that it is im-portant to take contextual clues into con-sideration.1 IntroductionAutomatic processing and understanding of SocialMedia Content (SMC) is currently attracting muchattention from the Natural Language Processingresearch community.
Although English is still byfar the most popular language in SMC, its domi-nance is receding.
Hong et al.
(2011), for exam-ple, applied an automatic language detection algo-rithm to over 62 million tweets to identify the top10 most popular languages on Twitter.
They foundthat only half of the tweets were in English.
More-over, mixing multiple languages together (codemixing) is a popular trend in social media usersfrom language-dense areas (C?ardenas-Claros andIsharyanti, 2009; Shafie and Nayan, 2013).
Ina scenario where speakers switch between lan-guages within a conversation, sentence or evenword, the task of automatic language identifica-tion becomes increasingly important to facilitatefurther processing.Speakers whose first language uses a non-Roman alphabet write using the Roman alphabetfor convenience (phonetic typing) which increasesthe likelihood of code mixing with a Roman-alphabet language.
This can be especially ob-served in South-East Asia and in the Indian sub-continent.
The following is a code mixing com-ment taken from a Facebook group of Indian uni-versity students:Original: Yaar tu to, GOD hain.
tui JUte ki korchis?
Hail u man!Translation: Buddy you are GOD.
Whatare you doing in JU?
Hail u man!This comment is written in three languages: En-glish, Hindi (italics), and Bengali (boldface).
ForBengali and Hindi, phonetic typing has been used.We follow in the footsteps of recent work onlanguage identification for SMC (Hughes et al.,2006; Baldwin and Lui, 2010; Bergsma et al.,2012), focusing specifically on the problem ofword-level language identification for code mixingSMC.
Our corpus for this task is collected fromFacebook and contains instances of Bengali(BN)-English(EN)-Hindi(HI) code mixing.The paper is organized as follows: in Section 2,we review related research in the area of codemixing and language identification; in Section 3,we describe our code mixing corpus, the data it-13self and the annotation process; in Section 4, welist the tools and resources which we use in ourlanguage identification experiments, described inSection 5.
Finally, in Section 6, we concludeand provide suggestions for future research on thistopic.2 Background and Related WorkThe problem of language identification has beeninvestigated for half a century (Gold, 1967) andthat of computational analysis of code switchingfor several decades (Joshi, 1982), but there hasbeen less work on automatic language identifi-cation for multilingual code-mixed texts.
Beforeturning to that topic, we first briefly survey studieson the general characteristics of code mixing.Code mixing is a normal, natural product ofbilingual and multilingual language use.
Signif-icant studies of the phenomenon can be foundin the linguistics literature (Milroy and Muysken,1995; Alex, 2008; Auer, 2013).
These worksmainly discuss the sociological and conversationalnecessities behind code mixing as well as its lin-guistic nature.
Scholars distinguish between inter-sentence, intra-sentence and intra-word code mix-ing.Several researchers have investigated the rea-sons for and the types of code mixing.
Initial stud-ies on Chinese-English code mixing in Hong Kong(Li, 2000) and Macao (San, 2009) indicated thatmainly linguistic motivations were triggering thecode mixing in those highly bilingual societies.Hidayat (2012) showed that Facebook users tendto mainly use inter-sentential switching over intra-sentential, and report that 45% of the switchingwas instigated by real lexical needs, 40% was usedfor talking about a particular topic, and 5% forcontent clarification.
The predominance of inter-sentential code mixing in social media text wasalso noted in the study by San (2009), which com-pared the mixing in blog posts to that in the spokenlanguage in Macao.
Dewaele (2010) claims that?strong emotional arousal?
increases the frequencyof code mixing.
Dey and Fung (2014) presenta speech corpus of English-Hindi code mixing instudent interviews and analyse the motivations forcode mixing and in what grammatical contextscode mixing occurs.Turning to the work on automatic analysis ofcode mixing, there have been some studies on de-tecting code mixing in speech (Solorio and Liu,2008a; Weiner et al., 2012).
Solorio and Liu(2008b) try to predict the points inside a set of spo-ken Spanish-English sentences where the speak-ers switch between the two languages.
Otherstudies have looked at code mixing in differ-ent types of short texts, such as information re-trieval queries (Gottron and Lipka, 2010) and SMSmessages (Farrugia, 2004; Rosner and Farrugia,2007).
Yamaguchi and Tanaka-Ishii (2012) per-form language identification using artificial mul-tilingual data, created by randomly sampling textsegments from monolingual documents.
Kingand Abney (2013) used weakly semi-supervisedmethods to perform word-level language identifi-cation.
A dataset of 30 languages has been usedin their work.
They explore several languageidentification approaches, including a Naive Bayesclassifier for individual word-level classificationand sequence labelling with Conditional RandomFields trained with Generalized Expectation crite-ria (Mann and McCallum, 2008; Mann and Mc-Callum, 2010), which achieved the highest scores.Another very recent work on this topic is (Nguyenand Do?gru?oz, 2013).
They report on languageidentification experiments performed on Turkishand Dutch forum data.
Experiments have beencarried out using language models, dictionaries,logistic regression classification and ConditionalRandom Fields.
They find that language modelsare more robust than dictionaries and that contex-tual information is helpful for the task.3 Corpus AcquisitionTaking into account the claim that code mixing isfrequent among speakers who are multilingual andyounger in age (C?ardenas-Claros and Isharyanti,2009), we choose an Indian student communitybetween the 20-30 year age group as our datasource.
India is a country with 30 spoken lan-guages, among which 22 are official.
code mix-ing is very frequent in the Indian sub-continentbecause languages change within very short geo-distances and people generally have a basic knowl-edge of their neighboring languages.A Facebook group1and 11 Facebook users(known to the authors) were selected to obtainpublicly available posts and comments.
The Face-book graph API explorer was used for data collec-tion.
Since these Facebook users are from WestBengal, the most dominant language is Bengali1https://www.facebook.com/jumatrimonial14(Native Language), followed by English and thenHindi (National Language of India).
The postsand comments in Bengali and Hindi script werediscarded during data collection, resulting in 2335posts and 9813 comments.3.1 AnnotationFour annotators took part in the annotation task.Three were computer science students and theother was one of the authors.
The annotators areproficient in all three languages of our corpus.
Asimple annotation tool was developed which en-abled these annotators to identify and distinguishthe different languages present in the content bytagging them.
Annotators were supplied with 4basic tags (viz.
sentence, fragment, inclusion andwlcm (word-level code mixing)) to annotate differ-ent levels of code mixing.
Under each tag, six at-tributes were provided, viz.
English (en), Bengali(bn), Hindi (hi), Mixed (mixd), Universal (univ)and Undefined (undef).
The attribute univ is as-sociated with symbols, numbers, emoticons anduniversal expressions (e.g.
hahaha, lol).
The at-tribute undef is specified for a sentence or a wordfor which no language tags can be attributed orcannot be categorized as univ.
In addition, anno-tators were instructed to annotate named entitiesseparately.
What follows are descriptions of eachof the annotation tags.Sentence (sent): This tag refers to a sentenceand can be used to mark inter-sentential code mix-ing.
Annotators were instructed to identify a sen-tence with its base language (e.g.
en, bn, hi andmixd) or with other types (e.g.
univ, undef ) as thefirst task of annotation.
Only the attribute mixd isused to refer to a sentence which contains multi-ple languages in the same proportion.
A sentencemay contain any number of inclusions, fragmentsand word-level code mixing.
A sentence can be at-tributed as univ if and only if it contains symbols,numbers, emoticons, chat acronyms and no otherwords (Hindi, English or Bengali).
A sentence canbe attributed as undef if it is not a sentence markedas univ and has words/tokens that can not be cate-gorized as Hindi, English or Bengali.
Some exam-ples of sentence-level annotations are the follow-ing:1.
English-Sentence:[sent-lang=?en?]
what a.....6 hrs long...but re-ally nice tennis.... [/sent]2.
Bengali-Sentence:[sent-lang=?bn?]
shubho nabo borsho.. :)[/sent]3.
Hindi Sentence:[sent-lang=?hi?]
karwa sachh ..... :( [/sent]4.
Mixed-Sentence:[sent-lang=?mixd?]
[frag-lang=?hi?]
oyehoye ..... angreji me kahte hai ke [/frag][frag-lang=?en?]
I love u..
!!!
[/frag] [/sent]5.
Univ-Sentence:[sent-lang=?univ?]
hahahahahahah....!!!!![/sent]6.
Undef-Sentence:[sent-lang=?undef?]
Hablando de una tripleamenaza.
[/sent]Fragment (frag): This refers to a group of for-eign words, grammatically related, in a sentence.The presence of this tag in a sentence conveys thatintra-sentential code mixing has occurred withinthe sentence boundary.
Identification of fragments(if present) in a sentence was the second task ofannotation.
A sentence (sent) with attribute mixdmust contain multiple fragments (frag) with a spe-cific language attribute.
In the fourth exampleabove, the sentence contains a Hindi fragment oyehoye ..... angreji me kahte hai ke and an Englishfragment I love u..
!!
!, hence it is considered as amixd sentence.
A fragment can have any numberof inclusions and word-level code mixing.
In thefirst example below, Jio is a popular Bengali wordappearing in the English fragment Jio.. good joke,hence tagged as a Bengali inclusion.
One can ar-gue that the word Jio could be a separate Bengaliinclusion (i.e.
can be tagged as a Bengali inclu-sion outside the English fragment).
But lookingat the syntactic pattern and the sense expressed bythe comment, the annotator kept it as a single unit.In the second example below, an instance of word-level code mixing, typer, has been found in an En-glish fragment (where the root English word typehas the Bengali suffix r).1.
Fragment with Inclusion:[sent-lang=?mixd?]
[frag-lang=?en?]
[incl-lang=?bn?]
Jio.. [/incl] good joke [/frag] [fraglang=?bn?]
?amar Babin?
[/frag] [/sent]2.
Fragment with Word-Level code mixing:[sent-lang=?mixd?]
[frag-lang=?en?]
?
I willfind u and marry you ?
[/frag] [frag-lang=?bn?]
[wlcm-type=?en-and-bn-suffix?
]typer [/wlcm] hoe glo to!
:D [/frag] [/sent]15Inclusion (incl): An inclusion is a foreign wordor phrase in a sentence or in a fragment whichis assimilated or used very frequently in nativelanguage.
Identification of inclusions can be per-formed after annotating a sentence and fragment(if present in that sentence).
An inclusion within asentence or fragment also denotes intra-sententialcode mixing.
In the example below, seriously is anEnglish inclusion which is assimilated in today?scolloquial Bengali and Hindi.
The only tag that aninclusion may contain is word-level code mixing.1.
Sentence with Inclusion:[sent-lang=?bn?]
Na re [incl-lang=?en?]
seri-ously [/incl] ami khub kharap achi.
[/sent]Word-Level code mixing (wlcm): This is thesmallest unit of code mixing.
This tag was in-troduced to capture intra-word code mixing anddenotes cases where code mixing has occurredwithin a single word.
Identifying word-level codemixing is the last task of annotation.
Annotatorswere told to mention the type of word-level codemixing in the form of an attribute (Base Language+ Second Language) format.
Some examples areprovided below.
In the first example below, theroot word class is English and e is an Bengali suf-fix that has been added.
In the third example be-low, the opposite can be observed ?
the root wordKando is Bengali, and an English suffix z has beenadded.
In the second example below, a named en-tity suman is present with a Bengali suffix er.1.
Word-Level code mixing (EN-BN):[wlcm-type=?en-and-bn-suffix?]
classe[/wlcm]2.
Word-Level code mixing (NE-BN):[wlcm-type=?NE-and-bn-suffix?]
sumaner[/wlcm]3.
Word-Level code mixing (BN-EN):[wlcm-type=?bn-and-en-suffix?]
kandoz[/wlcm]3.1.1 Inter Annotator AgreementWe calculate word-level inter annotator agreement(Cohen?s Kappa) on a subset of 100 comments(randomly selected) between two annotators.
Twoannotators are in agreement about a word if theyboth annotate the word with the same attribute(en, bn, hi, univ, undef ), regardless of whetherthe word is inside an inclusion, fragment or sen-tence.
Our observations that the word-level anno-tation process is not a very ambiguous task andthat annotation instruction is also straightforwardare confirmed in a high inter-annotator agreement(IAA) with a Kappa value of 0.884.3.2 Data CharacteristicsTag-level and word-level statistics of annotateddata that reveal the characteristics of our data setare described in Table 1 and in Table 2 respec-tively.
More than 56% of total sentences and al-most 40% of total tokens are in Bengali, which isthe dominant language of this corpus.
English isthe second most dominant language covering al-most 33% of total tokens and 35% of total sen-tences.
The amount of Hindi data is substantiallylower ?
nearly 1.75% of total tokens and 2% of to-tal sentences.
However, English inclusions (84%of total inclusions) are more prominent than Hindior Bengali inclusions and there are a substantialnumber of English fragments (almost 52% of totalfragments) present in our corpus.
This means thatEnglish is the main language involved in the codemixing.Statistics of Different TagsTags En Bn Hi Mixd Univ Undefsent 5,370 8,523 354 204 746 15frag 288 213 40 0 6 0incl 7,377 262 94 0 1,032 1wlcm 477Name Entity 3,602Acronym 691Table 1: Tag-level statisticsWord-Level Tag CountEN 66,298BN 79,899HI 3,440WLCM 633NE 5,233ACRO 715UNIV 39,291UNDEF 61Table 2: Word-level statistics3.2.1 Code Mixing TypesIn our corpus, inter- and intra-sentential code mix-ing are more prominent than word-level code mix-ing, which is similar to the findings of (Hidayat,2012) .
Our corpus contains every type of codemixing in English, Hindi and Bengali viz.
in-ter/intra sentential and word-level as described inthe previous section.
Some examples of differenttypes of code mixing in our corpus are presentedbelow.161.
Inter-Sentential:[sent-lang=?hi?]
Itna izzat diye aapne mujhe!!!
[/sent][sent-lang=?en?]
Tears of joy.
:?
( :?
( [/sent]2.
Intra-Sentential:[sent-lang=?bn?]
[incl-lang=?en?]
by d way[/incl] ei [frag-lang=?en?]
my craving armsshall forever remain empty .. never hold uclose .. [/frag] line ta baddo [incl-lang=?en?
]cheezy [/incl] :P ;) [/sent]3.
Word-Level:[sent-lang=?bn?]
[incl-lang=?en?]
1st yr[/incl] eo to ei [wlcm-type=?en+bnSuffix?
]tymer [/wlcm] modhye sobar jute jay ..[/sent]3.2.2 Ambiguous WordsAnnotators were instructed to tag an English wordas English irrespective of any influence of wordborrowing or foreign inclusion but an inspection ofthe annotations revealed that English words weresometimes annotated as Bengali or Hindi.
To un-derstand this phenomenon we processed the listof language (EN,BN and HI) word types (total26,475) and observed the percentage of types thatwere not always annotated with the one languagethroughout the corpus.
The results are presented inTable 3.
Almost 7% of total types are ambiguous(i.e.
tagged in different languages during annota-tion).
Among them, a substantial amount (5.58%)are English/Bengali.Label(s) Count PercentageEN 9,109 34.40BN 14,345 54.18HI 1,039 3.92EN or BN 1,479 5.58EN or HI 61 0.23BN or HI 277 1.04EN or BN or HI 165 0.62Table 3: Statistics of ambiguous and monolingualword typesThere are two reasons why this is happening:Same Words Across Languages Some wordsare the same (e.g.
baba, maa, na, khali) in Hindiand Bengali because both of the languages orig-inated from a single language Sanskrit and sharea good amount of common vocabulary.
It alsooccurred in English-Hindi and English-Bengali asa result of word borrowing.
Most of these arecommonly used inclusions like clg, dept, ques-tion, cigarette, and topic.
Sometimes the anno-tators were careful enough to tag such words asEnglish and sometimes these words were taggedin the annotators?
native languages.
During crosschecking of the annotated data the same error pat-terns were observed for multiple annotators, i.e.tagging commonly used foreign words into nativelanguage.
It only demonstrates that these Englishwords are highly assimilated in the conversationalvocabulary of Bengali and Hindi.Phonetic Similarity of Spellings Due to pho-netic typing some words share the same surfaceform across two and sometimes across three lan-guages.
As an example, to is a word in the threelanguages: it has occurred 1209 times as English,715 times as Bengali and 55 times as Hindi in ourdata.
The meaning of these words (e.g.
to, bolo,die) are different in different languages.
This phe-nomenon is perhaps exacerbated by the trend to-wards short and noisy spelling in SMC.4 Tools and ResourcesWe have used the following resources and tools inour experiment.Dictionaries1.
British National Corpus (BNC): We com-pile a word frequency list from the BNC (As-ton and Burnard, 1998).2.
SEMEVAL 2013 Twitter Corpus (Se-mevalTwitter): To cope with the languageof social media we use the SEMEVAL 2013(Nakov et al., 2013) training data for theTwitter sentiment analysis task.
This datacomes from a popular social media site andhence is likely to reflect the linguistic proper-ties of SMC.3.
Lexical Normalization List (LexNorm-List): Spelling variation is a well-knownphenomenon in SMC.
We use a lexical nor-malization dictionary created by Han et al.
(2012) to handle the different spelling vari-ations in our data.Machine Learning Toolkits1.
WEKA: We use the Weka toolkit (Hall etal., 2009) for our experiments in decision treetraining.2.
MALLET: CRF learning is applied using theMALLET toolkit (McCallum, 2002).173.
Liblinear: We apply Support Vector Ma-chine (SVM) learning with a linear kernel us-ing the Liblinear package (Fan et al., 2008).NLP Tools For data tokenization we used theCMU Tweet-Tokenizer (Owoputi et al., 2013).5 ExperimentsSince our training data is entirely labelled at theword-level by human annotators, we address theword-level language identification task in a fullysupervised way.Out of the total data, 15% is set aside as ablind test set, while the rest is employed in our ex-periments through a 5-fold cross-validation setup.There is a substantial amount of token overlap be-tween the cross-validation data and the test set ?88% of total EN tokens, 86% of total Bengali to-kens and 57% of total Hindi tokens of the test setare present in the cross-validation data.2We address the problem of word-level in threedifferent ways:1.
A simple heuristic-based approach whichuses a combination of our dictionaries to clas-sify the language of a word2.
Word-level classification using supervisedmachine learning with SVMs but no contex-tual information3.
Word-level classification using supervisedmachine learning with SVMs and sequencelabelling using CRFs, both employing con-textual informationNamed entities and instances of word-levelcode mixing are excluded from evaluation.
Forsystems which do not take the context of a wordinto account, i.e.
the dictionary-based approach(Section 5.1) and the SVM approach without con-textual clues (Section 5.2), named entities and in-stances of word-level code mixing can be safelyexcluded from training.
For systems which dotake context into account, the CRF system (Sec-tion 5.3.1) and the SVM system with contextualclues (Section 5.3.2), these are included in train-ing, because to exclude them would result in un-realistic contexts.
This means that these systems2We found 25 comments and 17 posts common betweenthe cross-validation data and the test set.
The reason for thisis that users of social media often express themselves in aconcise way.
Almost all of these common data consisted of 1to 3 token(s).
In most of the cases these tokens were emoti-cons, symbols or universal expressions such as wow and lol.As the percentage of these comments is low, we keep thesecomments as they are.can classify a word to be a named entity or an in-stance of word-level code mixing.
To avoid this,we implement a post-processor which backs off inthese cases to a system which hasn?t seen namedentities or word-level code mixing in training (seeSection 5.3).5.1 Dictionary-Based DetectionWe start with dictionary-based language detec-tion.
Generally a dictionary-based language de-tector predicts the language of a word based onits frequency in multiple language dictionaries.
Inour data the Bengali and Hindi tokens are phoneti-cally typed.
As no such transliterated dictionary is,to our knowledge, available for Bengali and Hindi,we use the training set words as dictionaries.
Forwords that have multiple annotations in trainingdata (ambiguous words), we select the majoritytag based on frequency, e.g.
the word to will al-ways be tagged as English.Our English dictionaries are those describedin Section 4 (BNC, LexNormList, SemEvalTwit-ter) and the training set words.
For LexNorm-List, we have no frequency information, and sowe consider it as a simple word list.
To pre-dict the language of a word, dictionaries with nor-malized frequency were considered first (BNC,SemEvalTwitter, Training Data), if not found,word list look-up was performed.
The predictedlanguage is chosen based on the dominant lan-guage(s) of the corpus if the word appears in mul-tiple dictionaries with same frequency or if theword does not appear in any dictionary or list.A simple rule-based method is applied to pre-dict universal expressions.
A token is consideredas univ if any of the following conditions satisfies:?
All characters of the token are symbols ornumbers.?
The token contains certain repetitions identi-fied by regular expressions.(e.g.
hahaha).?
The token is a hash-tag or an URL ormention-tags (e.g.
@Sumit).?
Tokens (e.g.
lol) identified by a word listcompiled from the relevant 4/5th of the train-ing data.Table 4 shows the results of dictionary-baseddetection obtained from 5-fold cross-validationaveraging.
We try different combinations and fre-quency thresholds of the above dictionaries.
Wefind that using a normalized frequency is helpful18and that a combination of LexNormList and Train-ing Data dictionaries is suited best for our data.Hence, we consider this as our baseline languageidentification system.Dictionary Accuracy(%)BNC 80.09SemevalTwitter 77.61LexNormList 79.86Training Data 90.21LexNormList+TrainingData (Baseline) 93.12Table 4: Average cross-validation accuracy ofdictionary-based detection5.2 Word-Level Classification withoutContextual CluesThe following feature types are employed:1.
Char-n-grams (G): We start with a charactern-gram-based approach (Cavnar and Tren-kle, 1994), which is most common and fol-lowed by many language identification re-searchers.
Following the work of King andAbney (2013), we select character n-grams(n=1 to 5) and the word as the features in ourexperiments.2.
Presence in Dictionaries (D): We use pres-ence in a dictionary as a features for all avail-able dictionaries in previous experiments.3.
Length of words (L): Instead of using theraw length value as a feature, we follow ourprevious work (Rubino et al., 2013; Wagneret al., 2014) and create multiple features forlength using a decision tree (J48).
We uselength as the only feature to train a decisiontree for each fold and use the nodes obtainedfrom the tree to create boolean features.4.
Capitalization (C): We use 3 boolean fea-tures to encode capitalization information:whether any letter in the word is capitalized,whether all letters in the word are capitalizedand whether the first letter is capitalized.We perform experiments with an SVM classifier(linear kernel) for different combination of thesefeatures.3Parameter optimizations (C range 2-15to 210) for SVM are performed for each feature3According to (Hsu et al., 2010) the SVM linear kernelwith parameter C optimization is good enough when dealingwith a large number of features.
Though an RBF kernel canbe more effective than a linear one, it is possible only afterproper optimization of C and ?
parameters, which is compu-tational expensive for such a large feature set.Features Accuracy Features AccuracyG 94.62 GD 94.67GL 94.62 GDL 94.73GC 94.64 GDC 94.72GLC 94.64 GDLC 94.75Table 5: Average cross-validation accuracy forSVM word-level classification (without context),G = char-n-gram, L = binary length features, D= presence in dictionaries and C = capitalizationfeaturesGDLC: 94.75%GLC: 94.64% GDL: 94.73% GDC: 94.72%GL: 94.62% GC: 94.64% GD: 94.67%G: 94.62%Figure 1: Average cross-validation accuracy forSVM word-level classification (without context),G = char-n-gram, L = binary length features, D= presence in dictionaries and C = capitalizationfeatures: cube visualizationset and best cross-validation accuracy is found forthe GDLC-based run (94.75%) at C=1 (see Table 5and Fig.
1).We also investigate the use of a dictionary-to-char-n-gram back-off model ?
the idea is to ap-ply the char-n-gram model SVM-GDLC for thosewords for which a majority-based decision is takenduring dictionary-based detection.
However, itdoes not outperform the SVM.
Hence, we selectSVM-GDLC for the next steps of our experimentsas the best exemplar of our individual word-levelclassifier (without contextual clues).5.3 Language Identification with ContextualCluesContextual clues can play a very important role inword-level language identification.
As an exam-ple, a part of a comment is presented from cross-validation fold 1 that contains the word die whichis wrongly classified by the SVM classifier.
Thefrequency of die in the training set of fold 1 is 6for English, 31 for Bengali and 0 for Hindi.Gold Data: ..../univ the/en movie/enfor/en which/en i/en can/en die/en for/en19Features Order-0 Order-1 Order-2G 92.80 95.16 95.36GD 93.42 95.59 95.98GL 92.82 95.14 95.41GDL 93.47 95.60 95.94GC 92.07 94.60 95.05GDC 93.47 95.62 95.98GLC 92.36 94.53 95.02GDLC 93.47 95.58 95.98Table 6: Average cross-validation accuracy ofCRF-based predictions where G = char-n-gram, L= length feature, D = single dictionary-based la-bels (baseline system) and C = capitalization fea-tures...../univSVM Output: ..../univ the/enmovie/en for/en which/en i/en can/endie/bn for/en ...../univWe now investigate whether contextual informa-tion can correct the mis-classified tags.Although named entities and word-level codemixing are excluded from evaluation, when deal-ing with context it is important to consider namedentity and word-level code mixing during trainingbecause these may contain some important infor-mation.
We include these tokens in the trainingdata for our context-based experiments, labellingthem as other.
The presence of this new label mayaffect the prediction for a language token duringclassification and sequence labelling.
To avoid thissituation, a 4-way (bn, hi, en, univ) backoff classi-fier is trained separately on English, Hindi, Ben-gali and universal tokens.
During evaluation ofany context-based system we discard named en-tity and word-level code mixing from the predic-tion of that system.
If any of the remaining tokensis predicted as other we back off to the decisionof the 4-way classifier for that token.
For the CRFexperiments (Section 5.3.1), the backoff classifieris a CRF system, and, for the SVM experiments(Section 5.3.2), the backoff classifier is an SVMsystem.5.3.1 Conditional Random Fields (CRF)As our goal is to apply contextual clues, we firstemploy Conditional Random Fields (CRF), an ap-proach which takes history into account in pre-dicting the optimal sequence of labels.
We em-ploy a linear chain CRF with an increasing or-der (Order-0, Order-1 and Order-2) with 200 it-erations for different feature combinations (usedGDLC: 95.98%GLC: 95.02% GDL: 95.94% GDC: 95.98%GL: 95.41% GC: 95.05% GD: 95.98%G: 95.36%Figure 2: CRF Order-2 results: cube visualisationG = char-n-gram, L = binary length features, D= presence in dictionaries and C = capitalizationfeaturesContext Accuracy (%)GDLC + P194.66GDLC + P294.55GDLC + N194.53GDLC + N294.37GDLC + P1N195.14GDLC + P2N294.55Table 7: Average cross-validation accuracy ofSVM (GDLC) context-based runs, where P-i =previous i word(s) , N-i = next i word(s)in SVM-based runs).
However, we observe thataccuracy of CRF based runs decreases when bi-narized length features (see Section 5.2 and dic-tionary features (a feature for each dictionary) areinvolved.
Hence, we use the dictionary-based pre-dictions of the baseline system to generate a singledictionary feature for each token and only the rawlength value of a token instead of binarized lengthfeatures.
The results are presented in Table 6 andthe second order results are visualized in Fig.
2.As expected, the performance increases as theorder increases from zero to one and two.
The useof a single dictionary feature is also helpful.
Theresults for GDC, GDLC, and GD based runs arealmost similar (95.98%).
However, we choose theGDC system because it performed slightly better(95.989%) than the GDLC (95.983%) and the GD(95.983%) systems.5.3.2 SVM with ContextWe also add contextual clues to our SVM classi-fier.
To obtain contextual information we includethe previous and next two words as features inthe SVM-GDLC-based run.4All possible com-4We also experimented with extracting all GDLC featuresfor the context words but this did not help.20binations are considered during experiments (Ta-ble 7).
After C parameter optimization, the bestcross-validation accuracy is found for the P1N1(one word previous and one word next) run withC=0.125 (95.14%).5.4 Test Set ResultsWe apply our best dictionary-based system, ourbest SVM system (with and without context) andour best CRF system to the held-out test set.
Theresults are shown in Table 8.
Our best result isachieved using the CRF model (95.76%).5.5 Error AnalysisManual error analysis shows the limitations ofthese systems.
The word-level classifier withoutcontextual clues does not perform well with Hindidata.
The number of Hindi tokens is quite low.Only 2.4% (4,658) of total tokens of the trainingdata are Hindi, out of which 55.36% are bilin-gually ambiguous and 29.51% are tri-linguallyambiguous tokens.
Individual word-level systemsoften fail to assign proper labels to ambiguouswords, but adding context information helps toovercome this problem.
Considering the previ-ous example of die, both context-based SVM andCRF systems classify it properly.
Though the finalsystem CRF-GDC performs well, it also has somelimitations, failing to identify the language for thetokens which appear very frequently in three lan-guages (e.g.
are, na, pic).6 ConclusionWe have presented an initial study on automaticlanguage identification with Indian language codemixing from social media communication.
Wedescribed our dataset of Bengali-Hindi-EnglishFacebook comments and we presented the resultsof our word-level classification experiments onthis dataset.
Our experimental results lead us toconclude that character n-gram features are usefulfor this task, contextual information is also impor-tant and that information from dictionaries can beeffectively incorporated as features.In the future we plan to apply the techniquesand feature sets that we used in these experimentsto other datasets.
We have already started this byapplying variants of the systems presented here tothe Nepali-English and Spanish-English datasetswhich were introduced as part of the 2014 codemixing shared task (Solorio et al., 2014; Barmanet al., 2014).We did not include word-level code mixing inour experiments ?
in our future experiments wewill explore ways to identify and segment this typeof code mixing.
It will be also important to find thebest way to handle inclusions since there is a fineline between word borrowing and code mixing.AcknowledgementsThis research is supported by the Science Founda-tion Ireland (Grant 12/CE/I2267) as part of CNGL(www.cngl.ie) at Dublin City University.
Theauthors wish to acknowledge the DJEI/DES/SFI/HEA for the provision of computational facili-ties and support.
Our special thanks to SoumikMandal from Jadavpur University, India for co-ordinating the annotation task.
We also thank theadministrator of JUMatrimonial and the 11 Face-book users who agreed that we can use their postsfor their support and permission.ReferencesBeatrice Alex.
2008.
Automatic detection of Englishinclusions in mixed-lingual data with an applicationto parsing.
Ph.D. thesis, School of Informatics, TheUniversity of Edinburgh, Edinburgh, UK.Guy Aston and Lou Burnard.
1998.
The BNC hand-book: exploring the British National Corpus withSARA.
Capstone.Peter Auer.
2013.
Code-Switching in Conversation:Language, Interaction and Identity.
Routledge.Timothy Baldwin and Marco Lui.
2010.
Languageidentification: The long and the short of the mat-ter.
In Human Language Technologies: The 2010Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 229?237.
Association for Computational Lin-guistics.Utsab Barman, Joachim Wagner, Grzegorz Chrupa?a,and Jennifer Foster.
2014.
DCU-UVT: Word-level language classification with code-mixed data.In Proceedings of the First Workshop on Com-putational Approaches to Code-Switching.
EMNLP2014, Conference on Empirical Methods in NaturalLanguage Processing, Doha, Qatar.
Association forComputational Linguistics.Shane Bergsma, Paul McNamee, Mossaab Bagdouri,Clayton Fink, and Theresa Wilson.
2012.
Languageidentification for creating language-specific Twittercollections.
In Proceedings of the Second Workshopon Language in Social Media, pages 65?74.
Associ-ation for Computational Linguistics.21SystemPrecision (%) Recall (%) Accuracy(%)EN BN HI UNIV EN BN HI UNIVBaseline (Dictionary) 92.67 90.73 80.64 99.67 92.28 94.63 43.47 94.99 93.64SVM-GDLC 92.49 94.89 80.31 99.34 96.23 94.28 44.92 97.07 95.21SVM-P1N193.51 95.56 83.18 99.42 96.63 95.23 55.94 96.95 95.52CRF-GDC 94.77 94.88 91.86 99.34 95.65 96.22 55.65 97.73 95.76Table 8: Test set results for Baseline (Dictionary), SVM-GDLC, SVM-P1N1 and CRF-GDCMS C?ardenas-Claros and N Isharyanti.
2009.
Code-switching and code-mixing in internet chatting:Between?yes,?ya,?and?si?-a case study.
The Jalt CallJournal, 5(3):67?78.William B. Cavnar and John M. Trenkle.
1994.
N-gram-based text categorization.
In Theo Pavlidis,editor, Proceedings of SDAIR-94, Third AnnualSymposium on Document Analysis and InformationRetrieval, pages 161?175.Jean-Marc Dewaele.
2010.
Emotions in Multiple Lan-guages.
Palgrave Macmillan.Anik Dey and Pascale Fung.
2014.
A Hindi-English code-switching corpus.
In Proceedings ofthe Ninth International Conference on Language Re-sources and Evaluation (LREC?14), pages 2410?2413, Reykjavik, Iceland.
European Language Re-sources Association (ELRA).Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
LIBLINEAR:A library for large linear classification.
Journal ofMachine Learning Research, 9:1871?1874.Paulseph-John Farrugia.
2004.
TTS pre-processing is-sues for mixed language support.
In Proceedingsof CSAW?04, the second Computer Science AnnualWorkshop, pages 36?41.
Department of ComputerScience & A.I., University of Malta.E Mark Gold.
1967.
Language identification in thelimit.
Information and control, 10(5):447?474.Thomas Gottron and Nedim Lipka.
2010.
A compar-ison of language identification approaches on short,query-style texts.
In Advances in Information Re-trieval, pages 611?614.
Springer.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA data mining software: An up-date.
SIGKDD Explor.
Newsl., 11(1):10?18.Bo Han, Paul Cook, and Timothy Baldwin.
2012.Automatically constructing a normalisation dictio-nary for microblogs.
In Proceedings of the 2012Joint Conference on Empirical Methods in Natu-ral Language Processing and Computational Natu-ral Language Learning, pages 421?432.
Associationfor Computational Linguistics.Taofik Hidayat.
2012.
An analysis of code switch-ing used by facebookers: a case study in asocial network site.
Student essay for thestudy programme ?Pendidikan Bahasa Ing-gris?
(English Education) at STKIP SiliwangiBandung, Indonesia, http://publikasi.stkipsiliwangi.ac.id/files/2012/10/08220227-taofik-hidayat.pdf.Lichan Hong, Gregorio Convertino, and Ed H. Chi.2011.
Language matters in twitter: A large scalestudy.
In Proceedings of the Fifth InternationalAAAI Conference on Weblogs and Social Media(ICWSM-11), pages 518?521, Barcelona, Spain.
As-sociation for the Advancement of Artificial Intelli-gence.Chih-Wei Hsu, Chih-Chung Chang, and Chih-Jen Lin.
2010.
A practical guide to sup-port vector classification.
Technical re-port.
Department of Computer Science, Na-tional Taiwan University, Taiwan, https://www.cs.sfu.ca/people/Faculty/teaching/726/spring11/svmguide.pdf.Baden Hughes, Timothy Baldwin, Steven Bird, JeremyNicholson, and Andrew MacKinlay.
2006.
Recon-sidering language identification for written languageresources.
In Proc.
of the 5th edition of the Interna-tional Conference on Language Resources and Eval-uation (LREC 2006), pages 485?488, Genoa, Italy.Aravind K. Joshi.
1982.
Processing of sentences withintra-sentential code-switching.
In J. Horeck?y, ed-itor, Proceedings of the 9th conference on Compu-tational linguistics - Volume 1 (COLING?82), pages145?150.
Academia Praha, North-Holland Publish-ing Company.Ben King and Steven Abney.
2013.
Labeling the lan-guages of words in mixed-language documents us-ing weakly supervised methods.
In Proceedings ofthe 2013 Conference of the North American Chap-ter of the Association for Computational Linguis-tics: Human Language Technologies, pages 1110?1119, Atlanta, Georgia.
Association for Computa-tional Linguistics.David C. S. Li.
2000.
Cantonese-English code-switching research in Hong Kong: a Y2K review.World Englishes, 19(3):305?322.Gideon S. Mann and Andrew McCallum.
2008.Generalized expectation criteria for semi-supervisedlearning of conditional random fields.
In Proceed-ings of ACL-08: HLT, pages 870?878, Columbus,Ohio.
Association for Computational Linguistics.22Gideon S. Mann and Andrew McCallum.
2010.Generalized expectation criteria for semi-supervisedlearning with weakly labeled data.
The Journal ofMachine Learning Research, 11:955?984.Andrew Kachites McCallum.
2002.
Mal-let: A machine learning for language toolkit.http://mallet.cs.umass.edu.Lesley Milroy and Pieter Muysken, editors.
1995.
Onespeaker, two languages: Cross-disciplinary per-spectives on code-switching.
Cambridge UniversityPress.Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,Veselin Stoyanov, Alan Ritter, and Theresa Wilson.2013.
Semeval-2013 task 2: Sentiment analysis intwitter.
In Second Joint Conference on Lexical andComputational Semantics (*SEM), Volume 2: Pro-ceedings of the Seventh International Workshop onSemantic Evaluation (SemEval 2013), pages 312?320, Atlanta, Georgia, USA.
Association for Com-putational Linguistics.Dong Nguyen and A. Seza Do?gru?oz.
2013.
Wordlevel language identification in online multilingualcommunication.
In Proceedings of the 2013 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP 2013), pages 857?862, Seattle,Washington, USA.
Association for ComputationalLinguistics.Olutobi Owoputi, Brendan O?Connor, Chris Dyer,Kevin Gimpel, Nathan Schneider, and Noah A.Smith.
2013.
Improved part-of-speech tagging foronline conversational text with word clusters.
InProceedings of the 2013 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies(NAACL-HLT 2013), pages 380?390, Atlanta, Geor-gia.
Association for Computational Linguistics.Mike Rosner and Paulseph-John Farrugia.
2007.
Atagging algorithm for mixed language identifica-tion in a noisy domain.
In INTERSPEECH-2007,8th Annual Conference of the International SpeechCommunication Association, pages 190?193.
ISCAArchive.Raphael Rubino, Joachim Wagner, Jennifer Foster, Jo-hann Roturier, Rasoul Samad Zadeh Kaljahi, andFred Hollowood.
2013.
DCU-Symantec at theWMT 2013 quality estimation shared task.
In Pro-ceedings of the Eighth Workshop on Statistical Ma-chine Translation, pages 392?397, Sofia, Bulgaria.Association for Computational Linguistics.Hong Ka San.
2009.
Chinese-English code-switchingin blogs by Macao young people.
Master?s the-sis, The University of Edinburgh, Edinburgh, UK.http://hdl.handle.net/1842/3626.Latisha Asmaak Shafie and Surina Nayan.
2013.Languages, code-switching practice and primaryfunctions of facebook among university students.Study in English Language Teaching, 1(1):187?199.
http://www.scholink.org/ojs/index.php/selt.Thamar Solorio and Yang Liu.
2008a.
Learning to pre-dict code-switching points.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing, pages 973?981.
Association forComputational Linguistics.Thamar Solorio and Yang Liu.
2008b.
Part-of-speechtagging for English-Spanish code-switched text.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing, pages 1051?1060.
Association for Computational Linguistics.Thamar Solorio, Elizabeth Blair, Suraj Maharjan, SteveBethard, Mona Diab, Mahmoud Gonheim, AbdelatiHawwari, Fahad AlGhamdi, Julia Hirshberg, AlisonChang, and Pascale Fung.
2014.
Overview for thefirst shared task on language identification in code-switched data.
In Proceedings of the First Workshopon Computational Approaches to Code-Switching.EMNLP 2014, Conference on Empirical Methods inNatural Language Processing, Doha, Qatar.
Associ-ation for Computational Linguistics.Joachim Wagner, Piyush Arora, Santiago Cortes, UtsabBarman, Dasha Bogdanova, Jennifer Foster, andLamia Tounsi.
2014.
DCU: Aspect-based polarityclassification for SemEval task 4.
In Proceedingsof the International Workshop on Semantic Evalu-ation (SemEval-2014), pages 392?397, Dublin, Ire-land.
Association for Computational Linguistics.Jochen Weiner, Ngoc Thang Vu, Dominic Telaar, Flo-rian Metze, Tanja Schultz, Dau-Cheng Lyu, Eng-Siong Chng, and Haizhou Li.
2012.
Integrationof language identification into a recognition systemfor spoken conversations containing code-switches.In Proceedings of the 3rd Workshop on Spoken Lan-guage Technologies for Under-resourced Languages(SLTU?12), Cape Town, South Africa.
InternationalResearch Center MICA.Hiroshi Yamaguchi and Kumiko Tanaka-Ishii.
2012.Text segmentation by language using minimum de-scription length.
In Proceedings of the 50th An-nual Meeting of the Association for ComputationalLinguistics: Long Papers-Volume 1, pages 969?978.Association for Computational Linguistics.23
