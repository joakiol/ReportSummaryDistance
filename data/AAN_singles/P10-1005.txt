Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 40?49,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsIdentifying Generic Noun PhrasesNils Reiter and Anette FrankDepartment of Computational LinguisticsHeidelberg University, Germany{reiter,frank}@cl.uni-heidelberg.deAbstractThis paper presents a supervised approachfor identifying generic noun phrases incontext.
Generic statements express rule-like knowledge about kinds or events.Therefore, their identification is importantfor the automatic construction of know-ledge bases.
In particular, the distinctionbetween generic and non-generic state-ments is crucial for the correct encodingof generic and instance-level information.Generic expressions have been studied ex-tensively in formal semantics.
Buildingon this work, we explore a corpus-basedlearning approach for identifying genericNPs, using selections of linguistically mo-tivated features.
Our results perform wellabove the baseline and existing prior work.1 IntroductionGeneric expressions come in two basic forms:generic noun phrases and generic sentences.
Bothexpress rule-like knowledge, but in different ways.A generic noun phrase is a noun phrase thatdoes not refer to a specific (set of) individual(s),but rather to a kind or class of individuals.
Thus,the NP The lion in (1.a)1 is understood as a ref-erence to the class ?lion?
instead of a specific in-dividual.
Generic NPs are not restricted to occurwith kind-related predicates as in (1.a).
As seenin (1.b), they may equally well be combined withpredicates that denote specific actions.
In contrastto (1.a), the property defined by the verb phrase in(1.b) may hold of individual lions.
(1) a.
The lion was the most widespread mam-mal.b.
Lions eat up to 30 kg in one sitting.1All examples are taken from Wikipedia unless stated oth-erwise.Generic sentences are characterising sentencesthat quantify over situations or events, expressingrule-like knowledge about habitual actions or situ-ations (2.a).
This is in contrast with sentences thatrefer to specific events and individuals, as in (2.b).
(2) a.
After 1971 [Paul Erdo?s] also took am-phetamines.b.
Paul Erdo?s was born [...] on March 26,1913.The genericity of an expression may arise fromthe generic (kind-referring, class-denoting) inter-pretation of the NP or the characterising interpre-tation of the sentence predicate.
Both sources mayconcur in a single sentence, as illustrated in Ta-ble 1, where we have cross-classified the exam-ples above according to the genericity of the NPand the sentence.This classification is extremely difficult, be-cause (i) the criteria for generic interpretation arefar from being clear-cut and (ii) both sources ofgenericity may freely interact.S[gen+] S[gen-]NP[gen+] (1.b) (1.a)NP[gen-] (2.a) (2.b)Table 1: Generic NPs and generic sentencesThe above classification of generic expressionsis well established in traditional formal semantics(cf.
Krifka et al (1995))2.
As we argue in thispaper, these distinctions are relevant for semanticprocessing in computational linguistics, especiallyfor information extraction and ontology learningand population tasks.
With appropriate semanticanalysis of generic statements, we can not onlyformally capture and exploit generic knowledge,2The literature draws some finer distinctions including as-pects like specificity, which we will ignore in this work.40but also distinguish between information pertain-ing to individuals vs. classes.
We will argue thatthe automatic identification of generic expressionsshould be cast as a machine learning problem in-stead of a rule-based approach, as there is (i) notransparent marking of genericity in English (as inmost other European languages) and (ii) the phe-nomenon is highly context dependent.In this paper, we build on insights from for-mal semantics to establish a corpus-based ma-chine learning approach for the automatic classi-fication of generic expressions.
In principle ourapproach is applicable to the detection of bothgeneric NPs and generic sentences, and in fact itwould be highly desirable and possibly advanta-geous to cover both types of genericity simulta-neously.
Our current work is confined to genericNPs, as there are no corpora available at presentthat contain annotations for genericity at the sen-tence level.The paper is organised as follows.
Section 2 in-troduces generic expressions and motivates theirrelevance for knowledge acquisition and semanticprocessing tasks in computational linguistics.
Sec-tion 3 reviews prior and related work.
In section 4we motivate the choice of feature sets for the au-tomatic identification of generic NPs in context.Sections 5 and 6 present our experiments and re-sults obtained for this task on the ACE-2 data set.Section 7 concludes.2 Generic Expressions & their Relevancefor Computational Linguistics2.1 Interpretation of generic expressionsGeneric NPs There are two contrasting viewson how to formally interpret generic NPs.
Ac-cording to the first one, a generic NP involves aspecial form of quantification.
Quine (1960), forexample, proposes a universally quantified read-ing for generic NPs.
This view is confronted withthe most important problem of all quantification-based approaches, namely that the exact determi-nation of the quantifier restriction (QR) is highlydependent on the context, as illustrated in (3)3.
(3) a. Lions are mammals.
QR: all lionsb.
Mammals give birth to live young.
QR:less than half of all mammals3Some of these examples are taken from Carlson (1977).c.
Rats are bothersome to people.
QR: fewrats4In view of this difficulty, several approachesrestrict the quantification to only ?relevant?
(De-clerck, 1991) or ?normal?
(Dahl, 1975) individu-als.According to the second view, generic nounphrases denote kinds.
Following Carlson (1977),a kind can be considered as an individual that hasproperties on its own.
On this view, the generic NPcannot be analysed as a quantifier over individualspertaining to the kind.
For some predicates, thisis clearly marked.
(1.a), for instance, attributes aproperty to the kind lion that cannot be attributedto individual lions.Generic sentences are usually analysed using aspecial dyadic operator, as first proposed by Heim(1982).
The dyadic operator relates two semanticconstituents, the restrictor and the matrix:Q[x1, ..., xi]([x1, ..., xi]?
??
?Restrictor; ?y1, ..., yi[x1, .., xi, y1, ..., yi]?
??
?Matrix)By choosing GEN as a generic dyadic operator,it is possible to represent the two readings (a) and(b) of the characterising sentence (4) by variationin the specification of restrictor and matrix (Krifkaet al, 1995).
(4) Typhoons arise in this part of the pacific.
(a) Typhoons in general have a common ori-gin in this part of the pacific.
(b) There arise typhoons in this part of the pa-cific.(a?)
GEN[x; y](Typhoon(x);this-part-of-the-pacific(y)?arise-in(x, y))(b?)
GEN[x; y](this-part-of-the-pacific(x);Typhoon(y)?arise-in(y, x))In order to cope with characterising sentencesas in (2.a), we must allow the generic operatorto quantify over situations or events, in this case,?normal?
situations which were such that Erdo?stook amphetamines.2.2 Relevance for computational linguisticsKnowledge acquisition The automatic acquisi-tion of formal knowledge for computational appli-cations is a major endeavour in current research4Most rats are not even noticed by people.41and could lead to big improvements of semantics-based processing.
Bos (2009), e.g., describes sys-tems using automated deduction for language un-derstanding tasks using formal knowledge.There are manually built formal ontologiessuch as SUMO (Niles and Pease, 2001) or Cyc(Lenat, 1995) and linguistic ontologies like Word-Net (Fellbaum, 1998) that capture linguistic andworld knowledge to a certain extent.
However,these resources either lack coverage or depth.
Au-tomatically constructed ontologies or taxonomies,on the other hand, are still of poor quality (Cimi-ano, 2006; Ponzetto and Strube, 2007).Attempts to automatically induce knowledgebases from text or encyclopaedic sources are cur-rently not concerned with the distinction betweengeneric and non-generic expressions, concentrat-ing mainly on factual knowledge.
However, rule-like knowledge can be found in textual sources inthe form of generic expressions5.In view of the properties of generic expressionsdiscussed above, this lack of attention bears twotypes of risks.
The first concerns the distinctionbetween classes and instances, regarding the attri-bution of properties.
The second concerns mod-elling exceptions in both representation and infer-encing.The distinction between classes and instancesis a serious challenge even for the simplestmethods in automatic ontology construction, e.g.,Hearst (1992) patterns.
The so-called IS-A pat-terns do not only identify subclasses, but also in-stances.
Shakespeare, e.g., would be recognisedas a hyponym of author in the same way as templeis recognised as a hyponym of civic building.Such a missing distinction between classes andinstances is problematic.
First, there are predicatesthat can only attribute properties to a kind (1.a).Second, even for properties that in principle can beattributed to individuals of the class, this is highlydependent on the selection of the quantifier?s re-striction in context (3).
In both cases, it holds thatproperties attributed to a class are not necessarily5In the field of cognitive science, research on the ac-quisition of generic knowledge in humans has shown thatadult speakers tend to use generic expressions very oftenwhen talking to children (Pappas and Gelman, 1998).
Weare not aware of any detailed assessment of the proportionof generic noun phrases in educational text genres or ency-clopaedic resources like Wikipedia.
Concerning generic sen-tences, Mathew and Katz (2009) report that 19.9% of the sen-tences in their annotated portion of the Penn Treebank arehabitual (generic) and 80.1% episodic (non-generic).inherited by any or all instances pertaining to theclass.Zirn et al (2008) are the first to present fully au-tomatic, heuristic methods to distinguish betweenclasses and instances in the Wikipedia taxonomyderived by Ponzetto and Strube (2007).
They re-port an accuracy of 81.6% and 84.5% for differ-ent classification schemes.
However, apart from aplural feature, all heuristics are tailored to specificproperties of the Wikipedia resource.Modelling exceptions is a cumbersome butnecessary problem to be handled in ontologybuilding, be it manually or by automatic means,and whether or not the genericity of knowledgeis formalised explicitly.
In artificial intelligenceresearch, this area has been tackled for manyyears.
Default reasoning (Reiter, 1980) is con-fronted with severe efficiency problems and there-fore has not extended beyond experimental sys-tems.
However, the emerging paradigm of AnswerSet Programming (ASP, Lifschitz (2008)) seemsto be able to model exceptions efficiently.
In ASPa given problem is cast as a logic program, andan answer set solver calculates all possible answersets, where an answer set corresponds to a solutionof the problem.
Efficient answer set solvers havebeen proposed (Gelfond, 2007).
Although ASPmay provide us with very efficient reasoning sys-tems, it is still necessary to distinguish and markdefault rules explicitly (Lifschitz, 2002).
Hence,the recognition of generic expressions is an impor-tant precondition for the adequate representationand processing of generic knowledge.3 Prior WorkSuh (2006) applied a rule-based approach to auto-matically identify generic noun phrases.
Suh usedpatterns based on part of speech tags that iden-tify bare plural noun phrases, reporting a precisionof 28.9% for generic entities, measured againstan annotated corpus, the ACE 2005 (Ferro et al,2005).
Neither recall nor f-measure are reported.To our knowledge, this is the single prior work onthe task of identifying generic NPs.Next to the ACE corpus (described in more de-tail below), Herbelot and Copestake (2008) offer astudy on annotating genericity in a corpus.
Twoannotators annotated 48 noun phrases from theBritish National Corpus for their genericity (andspecificity) properties, obtaining a kappa value of0.744.
Herbelot and Copestake (2008) leave su-42pervised learning for the identification of genericexpressions as future work.Recent work by Mathew and Katz (2009)presents automatic classification of generic andnon-generic sentences, yet restricted to habitualinterpretations of generic sentences.
They use amanually annotated part of the Penn TreeBankas training and evaluation set6.
Using a selec-tion of syntactic and semantic features operatingmainly on the sentence level, they achieved preci-sion between 81.2% and 84.3% and recall between60.6% and 62.7% for the identification of habitualgeneric sentences.4 Characterising Generic Expressionsfor Automatic Classification4.1 Properties of generic expressionsGeneric NPs come in various syntactic forms.These include definite and indefinite singularcount nouns, bare plural count and singular andplural mass nouns as in (5.a-f).
(5.f) shows aconstruction that makes the kind reading unam-biguous.
As Carlson (1977) observed, the genericreading of ?well-established?
kinds seems to bemore prominent (g vs.
h).
(5) a.
The lion was the most widespread mam-mal.b.
A lioness is weaker [...] than a male.c.
Lions died out in northern Eurasia.d.
Metals are good conductors.e.
Metal is also used for heat sinks.f.
The zoo has one kind of tiger.g.
The Coke bottle has a narrow neck.h.
The green bottle has a narrow neck.Apart from being all NPs, there is no obvioussyntactic property that is shared by all examples.Similarly, generic sentences come in a range ofsyntactic forms (6).
(6) a. John walks to work.b.
John walked to work(when he lived in California).c.
John will walk to work(when he moves to California).6The corpus has not been released.Although generic NPs and generic sentencescan be combined freely (cf.
Section 1; Table 1),both phenomena highly interact and quite oftenappear in the same sentence (Krifka et al, 1995).Also, genericity is highly dependent on contex-tual factors.
Present tense, e.g., may be indica-tive for genericity, but with appropriate temporalmodification, generic sentences may occur in pastor future tense (6).
Presence of a copular con-struction as in (5.a,b,d) may indicate a generic NPreading, but again we find generic NPs with eventverbs, as in (5.e) or (1.b).
Lexical semantic fac-tors, such as the semantic type of the clause predi-cate (5.c,e), or ?well-established?
kinds (5.g) mayfavour a generic reading, but such lexical factorsare difficult to capture in a rule-based setting.In our view, these observations call for a corpus-based machine learning approach that is able tocapture a variety of factors indicating genericity incombination and in context.4.2 Feature set and feature classesIn Table 2 we give basic information about theindividual features we investigate for identifyinggeneric NPs.
In the following, we will structurethis feature space along two dimensions, distin-guishing NP- and sentence-level factors as well assyntactic and semantic (including lexical seman-tic) factors.
Table 3 displays the grouping into cor-responding feature classes.NP-level features are extracted from the localNP without consideration of the sentence context.Sentence-level features are extracted from theclause (in which the NP appears), as well as sen-tential and non-sentential adjuncts of the clause.We also included the (dependency) relations be-tween the target NP and its governing clause.Syntactic features are extracted from a parsetree or shallow surface-level features.
The featureset includes NP-local and global features.Semantic features include semantic featuresabstracted from syntax, such as tense and aspector type of modification, but also lexical semanticfeatures such as word sense classes, sense granu-larity or verbal predicates.Our aim is to determine indicators for genericityfrom combinations of these feature classes.43Feature DescriptionNumber sg, plPerson 1, 2, 3Countability ambig, no noun, count, uncountNoun Type common, proper, pronounDeterminer Type def, indef, demonGranularity The number of edges in the WordNet hypernymy graph between the synset of the entity anda top nodePart of Speech POS-tag (Penn TreeBank tagset; Marcus et al (1993)) of the head of the phraseBare Plural false, trueSense[0-3] WordNet sense.
Sense[0] represents the sense of the head of the entity, Sense[1] its directhypernym sense and so forth.Sense[Top] The top sense in the hypernym hierarchy (often referred to as ?super sense?
)Dependency Relation [0-4] Dependency Relations.
Relation[0] represents the relation between entity and its governor,Relation[1] the relation between the governor and its governor and so forth.Embedded Predicate.Pred Lemma of the head of the directly governing predicate of the entityC.Tense past, pres, futC.Progressive false, trueC.Perfective false, trueC.Mood indicative, imperative, subjunctiveC.Passive false, trueC.Temporal Modifier?
false, trueC.Number of Modifiers numericC.Part of Speech POS-tag (Penn TreeBank tagset; Marcus et al (1993)) of the head of the phraseC.Pred Lemma of the head of the clauseC.Adjunct.Time true, falseC.Adjunct.VType main, copularC.Adjunct.Adverbial Type vpadv, sadvC.Adjunct.Degree positive, comparative, superlativeC.Adjunct.Pred Lemma of the head of the adjunct of the clauseXLE.Quality How complete is the parse by the XLE parser?
fragmented, complete, no parseTable 2: The features used in our system.
C stands for the clause in which the noun phrase appears,?Embedding Predicate?
its direct predicate.
In most cases, we just give the value range, if necessary, wegive descriptions.
All features may have a NULL value.Syntactic SemanticNP-level Number, Person, Part of Speech, Determiner Type, Bare Plural Countability, Granularity, Sense[0-3, Top]S-level Clause.
{Part of Speech, Passive, Number of Modifiers}, De-pendency Relation[0-4], Clause.Adjunct.
{Verbal Type, Adver-bial Type}, XLE.QualityClause.
{Tense, Progressive, Perfective,Mood, Pred, Has temporal Modifier},Clause.Adjunct.
{Time, Pred}, EmbeddedPredicate.PredTable 3: Feature classesName Descriptions and FeaturesSet 1 Five best single features: Bare Plural, Person, Sense [0], Clause.Pred, Embedding Predicate.PredSet 2 Five best feature tuples:a.
Number, Part of Speechb.
Countability, Part of Speechc.
Sense [0], Part of Speechd.
Number, Countabilitye.
Noun Type, Part of SpeechSet 3 Five best feature triples:a.
Number, Clause.Tense, Part of Speechb.
Number, Clause.Tense, Noun Typec.
Number, Clause.Part of Speech, Part of Speechd.
Number, Part of Speech, Noun Typee.
Number, Clause.Part of Speech, Noun TypeSet 4 Features, that appear most often among the single, tuple and triple tests: Number, Noun Type,Part of Speech, Clause.Tense, Clause.Part of Speech, Clause.Pred, Embedding Predicate.Pred, Person, Sense[0], Sense [1], Sense[2]Set 5 Features performing best in the ablation test: Number, Person, Clause.Part of Speech, Clause.Pred,Embedding Predicate.Pred, Clause.Tense, Determiner Type, Part of Speech, Bare Plural, Dependency Relation[2], Sense [0]Table 4: Derived feature sets445 Experiments5.1 DatasetAs data set we are using the ACE-2 (Mitchell etal., 2003) corpus, a collection of newspaper textsannotated with entities marked for their genericity.In this version of the corpus, the classification ofentities is a binary one.Annotation guidelines The ACE-2 annotationguidelines describe generic NPs as referring to anarbitrary member of the set in question, rather thanto a particular individual.
Thus, a property at-tributed to a generic NP is in principle applicableto arbitrary members of the set (although not toall of them).
The guidelines list several tests thatare either local syntactic tests involving determin-ers or tests that cannot be operationalised as theyinvolve world knowledge and context information.The guidelines give a number of criteria to iden-tify generic NPs referring to specific properties.These are (i) types of entities (lions in 3.a), (ii)suggested attributes of entities (mammals in 3.a),(iii) hypothetical entities (7) and (iv) generalisa-tions across sets of entities (5.d).
(7) If a person steps over the line, they must bepunished.The general description of generic NPs as de-noting arbitrary members of sets obviously doesnot capture kind-referring readings.
However, theproperties characterised (i) can be understood toadmit kinds.
Also, some illustrations in the guide-lines explicitly characterise kind-referring NPs asgeneric.
Thus, while at first sight the guidelinesdo not fully correspond to the characterisation ofgenerics we find in the formal semantics literature,we argue that both characterisations have similarextensions, i.e., include largely overlapping setsof noun phrases.
In fact, all of the examplesfor generic noun phrases presented in this paperwould also be classified as generic according tothe ACE-2 guidelines.We also find annotated examples of generic NPsthat are not discussed in the formal semantics liter-ature (8.a), but that are well captured by the ACE-2guidelines.
However, there are also cases that arequestionable (8.b).
(8) a.
?It?s probably not the perfect world, butyou kind of have to deal with what youhave to work with,?
he said.b.
Even more remarkable is the Internet,where information of all kinds is availableabout the government and the economy.This shows that the annotation of generics is dif-ficult, but also highlights the potential benefit of acorpus-driven approach that allows us to gather awider range of realisations.
This in turn can con-tribute to novel insights and discussion.Data analysis A first investigation of the corpusshows that generic NPs are much less commonthan non-generic ones, at least in the newspapergenre at hand.
Of the 40,106 annotated entities,only 5,303 (13.2%) are marked as generic.
In or-der to control for bias effects in our classifier, wewill experiment with two different training sets, abalanced and an unbalanced one.5.2 PreprocessingThe texts have been (pre-)processed to add sev-eral layers of linguistic annotation (Table 5).
Weuse MorphAdorner for sentence splitting and Tree-Tagger with the standard parameter files for partof speech tagging and lemmatisation.
As wedo not have a word sense disambiguation systemavailable that outperforms the most frequent sensebaseline, we simply used the most frequent sense(MFS).
The countability information is taken fromCelex.
Parsing was done using the English LFGgrammar (cf.
Butt et al (2002)) in the XLE pars-ing platform and the Stanford Parser.Task ToolSentence splitting MorphAdorner 7POS, lemmatisation TreeTagger (Schmid, 1994)WSD MFS (according to WordNet 3.0)Countability Celex (Baayen et al, 1996)Parsing XLE (Crouch et al, 2010)Stanford (Klein and Manning, 2003)Table 5: Preprocessing pipelineAs the LFG-grammar produced full parses onlyfor the sentences of 56% of the entities (partialparses: 37% of the entities), we chose to integratethe Stanford parser as a fallback.
If we are unableto extract feature values from the f-structure pro-duced by the XLE parser, we extract them fromthe Stanford Parser, if possible.
Experimentationshowed using the two parsers in tandem yields bestresults, compared to individual use.7http://morphadorner.northwestern.edu45Feature Set Generic Non generic OverallP R F P R F P R FBaseline Majority 0 0 0 86.8 100 92.9 75.3 86.8 80.6Baseline Person 60.5 10.2 17.5 87.9 99.0 93.1 84.3 87.2 85.7Baseline Suh 28.9FeatureClassesUnbalancedNP 31.7 56.6 40.7 92.5 81.4 86.6 84.5 78.2 81.2S 32.2 50.7 39.4 91.8 83.7 87.6 83.9 79.4 81.6NP/Syntactic 39.2 58.4 46.9 93.2 86.2 89.5 86.0 82.5 84.2S/Syntactic 31.9 22.1 26.1 88.7 92.8 90.7 81.2 83.5 82.3NP/Semantic 28.2 53.5 36.9 91.8 79.2 85 83.4 75.8 79.4S/Semantic 32.1 36.6 34.2 90.1 88.2 89.2 82.5 81.4 81.9Syntactic 40.1 66.6 50.1 94.3 84.8 89.3 87.2 82.4 84.7Semantic 34.5 56.0 42.7 92.6 83.8 88.0 84.9 80.1 82.4All 37.0 72.1 49.0 81.3 87.6 87.4 80.1 80.1 83.6BalancedNP 30.1 71.0 42.2 94.4 74.8 83.5 85.9 74.3 79.7S 26.9 73.1 39.3 94.4 69.8 80.3 85.5 70.2 77.1NP/Syntactic 35.4 76.3 48.4 95.6 78.8 86.4 87.7 78.5 82.8S/Syntactic 23.1 77.1 35.6 94.6 61.0 74.2 85.1 63.1 72.5NP/Semantic 24.7 60.0 35.0 92.2 72.1 80.9 83.3 70.5 76.4S/Semantic 26.4 66.3 37.7 93.3 71.8 81.2 84.5 71.1 77.2Syntactic 30.8 85.3 45.3 96.9 70.8 81.9 88.2 72.8 79.7Semantic 30.1 67.5 41.6 93.9 76.1 84.1 85.5 75.0 79.9All 33.7 81.0 47.6 96.3 75.8 84.8 88.0 76.5 81.8FeatureSelectionUnbalanced Set 1 49.5 37.4 42.6 90.8 94.2 92.5 85.3 86.7 86.0Set 2a 37.3 42.7 39.8 91.1 89.1 90.1 84.0 82.9 83.5Set 3a 42.6 54.1 47.7 92.7 88.9 90.8 86.1 84.3 85.2Set 4 42.7 69.6 52.9 94.9 85.8 90.1 88.0 83.6 85.7Set 5 45.7 64.8 53.6 94.3 88.3 91.2 87.9 85.2 86.5BalancedSet 1 29.7 71.1 41.9 94.4 74.4 83.2 85.9 73.9 79.5Set 2a 36.5 70.5 48.1 94.8 81.3 87.5 87.1 79.8 83.3Set 3a 36.2 70.8 47.9 94.8 81.0 87.4 87.1 79.7 83.2Set 4 35.9 83.1 50.1 96.8 77.4 86.0 88.7 78.2 83.1Set 5 37.0 81.9 51.0 96.6 78.7 86.8 88.8 79.2 83.7Table 6: Results of the classification, using different feature and training sets5.3 Experimental setupGiven the unclear dependencies of features, wechose to use a Bayesian network.
A Bayesian net-work represents the dependencies of random vari-ables in a directed acyclic graph, where each noderepresents a random variable and each edge a de-pendency between variables.
In fact, a numberof feature selection tests uncovered feature depen-dencies (see below).
We used the Weka (Wittenand Frank, 2002) implementation BayesNet in allour experiments.To control for bias effects, we created balanceddata sets by oversampling the number of genericentities and simultaneously undersampling non-generic entities.
This results in a dataset of 20,053entities with approx.
10,000 entities for eachclass.
All experiments are performed on balancedand unbalanced data sets using 10-fold cross-validation, where balancing has been performedfor each training fold separately (if any).Feature classes We performed evaluation runsfor different combinations of feature sets: NP- vs.S-level features (with further distinction betweensyntactic and semantic NP-/S-level features), aswell as overall syntactic vs. semantic features.This was done in order to determine the effect ofdifferent types of linguistic factors for the detec-tion of genericity (cf.
Table 3).46Feature selection We experimented with twomethods for feature selection.
Table 4 shows theresulting feature sets.In ablation testing, a single feature in turn istemporarily omitted from the feature set.
The fea-ture whose omission causes the biggest drop in f-measure is set aside as a strong feature.
This pro-cess is repeated until we are left with an emptyfeature set.
From the ranked list of features f1 tofn we evaluate increasingly extended feature setsf1..fi for i = 2..n. We select the feature set thatyields the best balanced performance, at 45.7%precision and 53.6% f-measure.
The features aregiven as Set 5 in Table 4.As ablation testing does not uncover feature de-pendencies, we also experimented with single, tu-ple and triple feature combinations to determinefeatures that perform well in combination.
Weran evaluations using features in isolation and eachpossible pair and triple of features.
We select theresulting five best features, tuples and triples offeatures.
The respective feature sets are given asSet 1 to Set 3 in Table 4.
The features that appearmost often in Set 1 to Set 3 are grouped in Set 4.Baseline Our results are evaluated against threebaselines.
Since the class distribution is unequal,a majority baseline consists in classifying each en-tity as non-generic.
As a second baseline we chosethe performance of the feature Person, as this fea-ture gave the best performance in precision amongthose that are similarly easy to extract.
Finally, wecompare our results to (Suh, 2006).6 Results and DiscussionThe results of classification are summarised in Ta-ble 6.
The columns Generic and Non-generic givethe results for the respective class.
Overall showsthe weighted average of the classes.Comparison to baselines Given the bias fornon-generic NPs in the unbalanced data, the ma-jority baseline achieves high performance overall(F: 80.6).
Of course, it does not detect any genericNPs.
The Person-based baseline also suffers fromvery low recall (R: 10.2%), but achieves the high-est precision (P: 60.5 %).
(Suh, 2006) reportedonly precision of the generic class, so we can onlycompare against this value (28.9 %).
Most ofthe features and feature sets yield precision valuesabove the results of Suh.Feature classes, unbalanced data For theidentification of generic NPs, syntactic featuresachieve the highest precision and recall (P: 40.1%,R: 66.6 %).
Using syntactic features on the NP-or sentence-level only, however, leads to a drop inprecision as well as recall.
The recall achieved bysyntactic features can be improved at the cost ofprecision by adding semantic features (R: 66.6 ?72.1, P: 40.1 ?
37).
Semantic features in sep-aration perform lower than the syntactic ones, interms of recall and precision.Even though our results achieve a lower pre-cision than the Person baseline, in terms of f-measure, we achieve a result of over 50%, whichis almost three times the baseline.Feature classes, balanced data Balancing thetraining data leads to a moderate drop in perfor-mance.
All feature classes perform lower than onthe unbalanced data set, yielding an increase in re-call and a drop in precision.
The overall perfor-mance differences between the balanced and un-balanced data for the best achieved values for thegeneric class are -4.7 (P), +13.2 (R) and -1.7 (F).This indicates that (i) the features prove to performrather effectively, and (ii) the distributional bias inthe data can be exploited in practical experiments,as long as the data distribution remains constant.We observe that generally, the recall for thegeneric class improves for the balanced data.
Thisis most noticeable for the S-level features withan increase of 55 (syntactic) and 29.7 (semantic).This could indicate that S-level features are usefulfor detecting genericity, but are too sparse in thenon-oversampled data to become prominent.
Thisholds especially for the lexical semantic features.As a general conclusion, syntactic featuresprove most important in both setups.
We also ob-serve that the margin between syntactic and se-mantic features reduces in the balanced dataset,and that both NP- and S-level features contributeto classification performance, with NP-featuresgenerally outperforming the S-level features.
Thisconfirms our hypothesis that all feature classescontribute important information.Feature selection While the above figures wereobtained for the entire feature space, we now dis-cuss the effects of feature selection both on per-formance and the distribution over feature classes.The results for each feature set are given in Ta-ble 6.
In general, we find a behaviour similar to47Syntactic SemanticNP Number, Person, Part ofSpeech, Determiner Type, BarePluralSense[0]S Clause.Part of Speech, Depen-dency Relation[2]Clause.
{Tense,Pred}Table 7: Best performing features by feature classthe homogeneous classes, in that balanced train-ing data increases recall at the cost of precision.With respect to overall f-measure, the best sin-gle features are strong on the unbalanced data.They even yield a relatively high precision for thegeneric NPs (49.5%), the highest value among theselected feature sets.
This, however, comes at theprice of one of the lowest recalls.
The best per-forming feature in terms of f-measure on both bal-anced and unbalanced data is Set 5 with Set 4 as aclose follow-up.
Set 5 achieves an f-score of 53.6(unbalanced) and 51.0 (balanced).
The highest re-call is achieved using Set 4 (69.6% on the unbal-anced and 83.1% on the balanced dataset).
Theresults for Set 5 represent an improvement of 3.5respectively 2.6 (unbalanced and balanced) overthe best achieved results on homogeneous featureclasses.
In fact, Table 7 shows that these features,selected by ablation testing, distribute over all ho-mogeneous classes.We trained a decision tree to gain insights intothe dependencies among these features.
Figure 1shows an excerpt of the obtained tree.
The clas-sifier learned to classify singular proper namesas non-generic, while the genericity of singularnouns depends on their predicate.
At this point,the classifier can correctly classify some of theNPs in (5) as kind-referring (given the trainingdata contains predicates like ?widespread?, ?dieout?, ...).7 Conclusions and Future WorkThis paper addresses a linguistic phenomenon thathas been thoroughly studied in the formal se-mantics literature but only recently is starting tobe addressed as a task in computational linguis-tics.
We presented a data-driven machine learn-ing approach for identifying generic NPs in con-text that in turn can be used to improve tasks suchas knowledge acquisition and organisation.
Theclassification of generic NPs has proven difficulteven for humans.
Therefore, a machine learningapproach seemed promising, both for the identifi-cation of relevant features as for capturing contex-Figure 1: A decision tree trained on feature Set 5tual factors.
We explored a range of features usinghomogeneous and mixed classes gained by alter-native methods of feature selection.
In terms off-measure on the generic class, all feature sets per-formed above the baseline(s).
In the overall clas-sification, the selected sets perform above the ma-jority and close to or above the Person baseline.The final feature set that we established charac-terises generic NPs as a phenomenon that exhibitsboth syntactic and semantic as well as sentence-and NP-level properties.
Although our results aresatisfying, in future work we will extend the rangeof features for further improvements.
In particular,we will address lexical semantic features, as theytend to be effected by sparsity.
As a next step,we will apply our approach to the classificationof generic sentences.
Treating both cases simul-taneously could reveal insights into dependenciesbetween them.The classification of generic expressions is onlya first step towards a full treatment of the chal-lenges involved in their semantic processing.
Asdiscussed, this requires a contextually appropriateselection of the quantifier restriction8, as well asdetermining inheritance of properties from classesto individuals and the formalisation of defaults.ReferencesR.
Harald Baayen, Richard Piepenbrock, and Leon Gu-likers.
1996.
CELEX2.
Linguistic Data Consor-tium, Philadelphia.Johan Bos.
2009.
Applying automated deduction tonatural language understanding.
Journal of Applied8Consider example (1.a), which is contextually restrictedto a certain time and space.48Logic, 7(1):100 ?
112.
Special Issue: EmpiricallySuccessful Computerized Reasoning.Miriam Butt, Helge Dyvik, Tracy Holloway King, Hi-roshi Marsuichi, and Christian Rohrer.
2002.
TheParallel Grammar Project.
In Proceedings of Gram-mar Engineering and Evaluation Workshop.Gregory Norman Carlson.
1977.
Reference to Kinds inEnglish.
Ph.D. thesis, University of Massachusetts.Philipp Cimiano.
2006.
Ontology Learning and Popu-lating from Text.
Springer.Dick Crouch, Mary Dalrymple, Ron Ka-plan, Tracy King, John Maxwell, and PaulaNewman, 2010.
XLE Documentation.www2.parc.com/isl/groups/nltt/xle/doc/xle toc.html.O?sten Dahl.
1975.
On Generics.
In EdwardKeenan, editor, Formal Semantics of Natural Lan-guage, pages 99?111.
Cambridge University Press,Cambridge.Renaat Declerck.
1991.
The Origins of Genericity.Linguistics, 29:79?102.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
MIT Press.Lisa Ferro, Laurie Gerber, Janet Hitzeman, Eliza-beth Lima, and Beth Sundheim.
2005.
ACE En-glish Training Data.
Linguistic Data Consortium,Philadelphia.Michael Gelfond.
2007.
Answer sets.
In Handbook ofKnowledge Representation.
Elsevier Science.Marti A. Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In Proceedings ofthe 14th International Conference on ComputationalLinguistics, pages 539?545.Irene Heim.
1982.
The Semantics of Definite and In-definite Noun Phrases.
Ph.D. thesis, University ofMassachusetts, Amherst.Aurelie Herbelot and Ann Copestake.
2008.
Anno-tating genericity: How do humans decide?
(a casestudy in ontology extraction).
In Sam Featherstonand Susanne Winkler, editors, The Fruits of Empiri-cal Linguistics, volume 1. de Gruyter.Dan Klein and Christopher Manning.
2003.
Accurateunlexicalized parsing.
In Proceedings of the 41stMeeting of the Association for Computational Lin-guistics, pages 423?430.Manfred Krifka, Francis Jeffry Pelletier, Gregory N.Carlson, Alice ter Meulen, Gennaro Chierchia, andGodehard Link.
1995.
Genericity: An Introduc-tion.
In Gregory Norman Carlson and Francis JeffryPelletier, editors, The Generic Book.
University ofChicago Press, Chicago.Douglas B. Lenat.
1995.
Cyc: a large-scale invest-ment in knowledge infrastructure.
Commun.
ACM,38(11):33?38.Vladimir Lifschitz.
2002.
Answer set programmingand plan generation.
Artificial Intelligence, 138(1-2):39 ?
54.Vladimir Lifschitz.
2008.
What is Answer Set Pro-gramming?
In Proceedings of AAAI.Mitchell P. Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large anno-tated corpus of English: the Penn treebank.
Compu-tational Linguistics, 19(2):313?330.Thomas Mathew and Graham Katz.
2009.
SupervisedCategorization of Habitual and Episodic Sentences.In Sixth Midwest Computational Linguistics Collo-quium.
Bloomington, Indiana: Indiana University.Alexis Mitchell, Stephanie Strassel, Mark Przybocki,JK Davis, George Doddington, Ralph Grishman,Adam Meyers, Ada Brunstein, Lisa Ferro, and BethSundheim.
2003.
ACE-2 Version 1.0.
LinguisticData Consortium, Philadelphia.Ian Niles and Adam Pease.
2001.
Towards a StandardUpper Ontology.
In Proceedings of the 2nd Interna-tional Conference on Formal Ontology in Informa-tion Systems.Athina Pappas and Susan A. Gelman.
1998.
Genericnoun phrases in mother?child conversations.
Jour-nal of Child Language, 25(1):19?33.Simone Paolo Ponzetto and Michael Strube.
2007.Deriving a large scale taxonomy from wikipedia.In Proceedings of the 22nd Conference on the Ad-vancement of Artificial Intelligence, pages 1440?1445, Vancouver, B.C., Canada, July.Willard Van Orman Quine.
1960.
Word and Object.MIT Press, Cambridge, Massachusetts.Raymond Reiter.
1980.
A logic for default reasoning.Artificial Intelligence, 13:81?132.Helmut Schmid.
1994.
Probabilistic part-of-speechtagging using decision trees.
Proceedings of theconference on New Methods in Language Process-ing, 12.Sangweon Suh.
2006.
Extracting Generic Statementsfor the Semantic Web.
Master?s thesis, University ofEdinburgh.Ian H. Witten and Eibe Frank.
2002.
Data min-ing: practical machine learning tools and techniqueswith Java implementations.
ACM SIGMOD Record,31(1):76?77.Ca?cilia Zirn, Vivi Nastase, and Michael Strube.
2008.Distinguishing between instances and classes in theWikipedia taxonomy.
In Proceedings of the 5th Eu-ropean Semantic Web Conference.49
