Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 826?833,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPA Syntax-Free Approach to Japanese Sentence CompressionTsutomu HIRAO, Jun SUZUKI and Hideki ISOZAKINTT Communication Science Laboratories, NTT Corp.2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0237 Japan{hirao,jun,isozaki}@cslab.kecl.ntt.co.jpAbstractConventional sentence compression meth-ods employ a syntactic parser to compressa sentence without changing its mean-ing.
However, the reference compres-sions made by humans do not always re-tain the syntactic structures of the originalsentences.
Moreover, for the goal of on-demand sentence compression, the timespent in the parsing stage is not negligi-ble.
As an alternative to syntactic pars-ing, we propose a novel term weightingtechnique based on the positional infor-mation within the original sentence anda novel language model that combinesstatistics from the original sentence and ageneral corpus.
Experiments that involveboth human subjective evaluations and au-tomatic evaluations show that our methodoutperforms Hori?s method, a state-of-the-art conventional technique.
Because ourmethod does not use a syntactic parser, itis 4.3 times faster than Hori?s method.1 IntroductionIn order to compress a sentence while retainingits original meaning, the subject-predicate rela-tionship of the original sentence should be pre-served after compression.
In accordance with thisidea, conventional sentence compression methodsemploy syntactic parsers.
English sentences areusually analyzed by a full parser to make parsetrees, and the trees are then trimmed (Knight andMarcu, 2002; Turner and Charniak, 2005; Unnoet al, 2006).
For Japanese, dependency trees aretrimmed instead of full parse trees (Takeuchi andMatsumoto, 2001; Oguro et al, 2002; Nomoto,2008)1 This parsing approach is reasonable be-cause the compressed output is grammatical if the1Hereafter, we refer these compression processes as ?treetrimming.
?input is grammatical, but it offers only moderatecompression rates.An alternative to the tree trimming approachis the sequence-oriented approach (McDonald,2006; Nomoto, 2007; Clarke and Lapata, 2006;Hori and Furui, 2003).
It treats a sentence as a se-quence of words and structural information, suchas a syntactic or dependency tree, is encoded inthe sequence as features.
Their methods have thepotential to drop arbitrary words from the originalsentence without considering the boundary deter-mined by the tree structures.
However, they stillrely on syntactic information derived from fullyparsed syntactic or dependency trees.We found that humans usually ignored the syn-tactic structures when compressing sentences.
Forexample, in many cases, they compressed the sen-tence by dropping intermediate nodes of the syn-tactic tree derived from the source sentence.
Webelieve that making compression strongly depen-dent on syntax is not appropriate for reproducingreference compressions.
Moreover, on-demandsentence compression is made problematic by thetime spent in the parsing stage.This paper proposes a syntax-free sequence-oriented sentence compression method.
To main-tain the subject-predicate relationship in the com-pressed sentence and retain fluency without us-ing syntactic parsers, we propose two novel fea-tures: intra-sentence positional term weighting(IPTW) and the patched language model (PLM).IPTW is defined by the term?s positional informa-tion in the original sentence.
PLM is a form ofsummarization-oriented fluency statistics derivedfrom the original sentence and the general lan-guage model.
The weight parameters for thesefeatures are optimized within the Minimum Clas-sification Error (MCE) (Juang and Katagiri, 1992)learning framework.Experiments that utilize both human subjectiveand automatic evaluations show that our method is826?????
???
?
?
?
????
??
?Source Sentence???
??????
???????????
?Chunk 1Chunk 2 Chunk 3Chunk 4Chunk 5Chunk 6Chunk 7Compressed SentenceChunk7 = a part of Chunk6 + parts of Chunk4?????
??
?suitei shi tahaiten nitsuite fukutake gaedamonbubun nokouhyou shi te naicenter shiken de???
??????
???
?Chunk 1Chunk 2 Chunk 3suitei shi tahaiten nitsuite fukutake gacenter shikenedamon noedamon nocenter shikenCompressioncompound nouniFigure 1: An example of the dependency relation between an original sentence and its compressedvariant.superior to conventional sequence-oriented meth-ods that employ syntactic parsers while beingabout 4.3 times faster.2 Analysis of reference compressionsSyntactic information does not always yield im-proved compression performance because humansusually ignore the syntactic structures when theycompress sentences.
Figure 1 shows an exam-ple.
English translation of the source sentence is?Fukutake Publishing Co., Ltd. presumed prefer-ential treatment with regard to its assessed scoresfor a part of the questions for a series of CenterExaminations.?
and its compression is ?Fukutakepresumed preferential scores for questions for aseries of Center Examinations.
?In the figure, each box indicates a syntacticchunk, bunsetsu.
The solid arrows indicate de-pendency relations between words2.
We observethat the dependency relations are changed by com-pression; humans create compound nouns usingthe components derived from different portions ofthe original sentence without regard to syntacticconstraints.
?Chunk 7?
in the compressed sen-tence was constructed by dropping both contentand functional words and joining other contentwords contained in ?Chunk 4?
and ?Chunk 6?
of2Generally, a dependency relation is defined between bun-setsu.
Therefore, in order to identify word dependencies, wefollowed Kudo?s rule (Kudo and Matsumoto, 2004)the original sentence.
?Chunk 5?
is dropped com-pletely.
This compression cannot be achieved bytree trimming.According to an investigation in our corpus ofmanually compressed Japanese sentences, whichwe used in the experimental evaluation, 98.7% ofthem contain at least one segment that does notretain the original tree structure.
Human usuallycompress sentences by dropping the intermediatenodes in the dependency tree.
However, the re-sulting compressions retain both adequacy and flu-ency.
This statistic supports the view that sentencecompression that strongly depends on syntax isnot useful in reproducing reference compressions.We need a sentence compression method that candrop intermediate nodes in the syntactic tree ag-gressively beyond the tree-scoped boundary.In addition, sentence compression methods thatstrongly depend on syntactic parsers have twoproblems: ?parse error?
and ?decoding speed.
?44% of sentences output by a state-of-the-artJapanese dependency parser contain at least oneerror (Kudo and Matsumoto, 2005).
Even more, itis well known that if we parse a sentence whosesource is different from the training data of theparser, the performance could be much worse.This critically degrades the overall performanceof sentence compression.
Moreover, summariza-tion systems often have to process megabytes ofdocuments.
Parsers are still slow and users of on-827demand summarization systems are not preparedto wait for parsing to finish.3 A Syntax Free Sequence-orientedSentence Compression MethodAs an alternative to syntactic parsing, we pro-pose two novel features, intra-sentence positionalterm weighting (IPTW) and the patched languagemodel (PLM) for our syntax-free sentence com-pressor.3.1 Sentence Compression as aCombinatorial Optimization ProblemSuppose that a compression system reads sen-tence x= x1 , x2, .
.
.
, xj , .
.
.
, xN , where xjis the j-th word in the input sentence.
Thesystem then outputs the compressed sentence y=y1, y2, .
.
.
, yi, .
.
.
, yM , where yi is the i-th word in the output sentence.
Here, yi ?
{x1, .
.
.
, xN}.
We assume y0=x0=<s> (BOS)and yM+1=xN+1=</s> (EOS).
We define func-tion I(?
), which maps word yi to the index ofthe word in the original sentence.
For example,if source sentence is x = x1, x2, .
.
.
, x5 and itscompressed variant is y = x1, x3, x4, I(y1) = 1,I(y2) = 3, I(y3) = 4.We define a significance score f(x, y,?)
forcompressed sentence y based on Hori?s method(Hori and Furui, 2003).
?
= {?g, ?h} is a pa-rameter vector.f(x, y;?)
=M+1?i=1{g(x, I(yi);?g) +h(x, I(yi), I(yi?1);?h)} (1)The first term of equation (1) (g(?))
is the impor-tance of each word in the output sentence, and thesecond term (h(?))
is the the linguistic likelihoodbetween adjacent words in the output sentence.The best subsequence y?=argmaxyf(x, y;?)
isidentified by dynamic programming (DP) (Horiand Furui, 2003).3.2 FeaturesWe use IPTW to define the significance scoreg(x, I(yi);?g).
Moreover, we use PLM to definethe linguistic likelihood h(x, I(yi+1), I(yi);?h).3.2.1 Intra-sentence Positional TermWeighting (IPTW)IDF is a global term weighting scheme in that itmeasures the significance score of a word in atext corpus, which could be extremely large.
Bycontrast, this paper proposes another type of termweighting; it measures the positional significancescore of a word within its sentence.
Here, we as-sume the following hypothesis:?
The ?significance?
of a word depends on itsposition within its sentence.In Japanese, the main subject of a sentenceusually appears at the beginning of the sentence(BOS) and the main verb phrase almost alwaysappears at the end of the sentence (EOS).
Thesewords or phrases are usually more important thanthe other words in the sentence.
In order toadd this knowledge to the scoring function, termweight is modeled by the following Gaussian mix-ture.N(psn(x, I(yi));?g) =m11?2?
?1exp(?12(psn(x, I(yi)) ?
?1?1)2)+m21?2?
?2exp(?12(psn(x, I(yi)) ?
?2?2)2)(2)Here, ?g = {?k, ?k, mk}k=1,2.
psn(x, I(yi))returns the relative position of yi in the originalsentence x which is defined as follows:psn(x, I(yi)) =start(x, I(yi))length(x)(3)?length(x)?
denotes the number of characters inthe source sentence and ?start(x, I(yi))?
denotesthe accumulated run of characters from BOS to(x, I(yi)).
In equation (2), ?k,?k indicates themean and the standard deviation for the normaldistribution, respectively.
mk is a mixture param-eter.We use the distribution (2) in definingg(x, I(yi);?g) as follows:g(x, I(yi);?g) =??????
?IDF(x, I(yi)) ?
N(psn(x, I(yi);?g)if pos(x,I(yi)) = noun, verb, adjectiveConstant ?
N(psn(x, I(yi);?g)otherwise(4)828Here, pos(x, I(yi)) denotes the part-of-speech tagfor yi.
?g is optimized by using the MCE learningframework.3.2.2 Patched Language ModelMany studies on sentence compression employ then-gram language model to evaluate the linguisticlikelihood of a compressed sentence.
However,this model is usually computed by using a hugevolume of text data that contains both short andlong sentences.
N-gram distribution of short sen-tences may different from that of long sentences.Therefore, the n-gram probability sometimes dis-agrees with our intuition in terms of sentence com-pression.
Moreover, we cannot obtain a hugecorpus consisting solely of compressed sentences.Even if we collect headlines as a kind of com-pressed sentence from newspaper articles, corpussize is still too small.
Therefore, we proposethe following novel linguistic likelihood based onstatistics derived from the original sentences and ahuge corpus:PLM(x, I(yj), I(yj?1)) =??
?1 if I(yj) = I(yj?1) + 1?PLM Bigram(x, I(yj), I(yj?1))otherwise(5)PLM stands for Patched Language Model.Here, 0 ?
?PLM ?
1, Bigram(?)
indicates wordbigram probability.
The first line of equation (5)agrees with Jing?s observation on sentence align-ment tasks (Jing and McKeown, 1999); that is,most (or almost all) bigrams in a compressed sen-tence appear in the original sentence as they are.3.2.3 POS bigramSince POS bigrams are useful for rejecting un-grammatical sentences, we adopt them as follows:Ppos(x, I(yi+1)|I(yi)) =P (pos(x, I(yi+1))|pos(x, I(yi))).
(6)Finally, the linguistic likelihood between adja-cent words within y is defined as follows:h(x, I(yi+1), I(yi);?h) =PLM(x, I(yi+1), I(yi)) +?
(pos(x,I(yi+1))|pos(x,I(yi)))Ppos(x, I(yi+1)|I(yi))3.3 Parameter OptimizationWe can regard sentence compression as a two classproblem: we give a word in the original sentenceclass label +1 (the word is used in the compressedoutput) or ?1 (the word is not used).
In order toconsider the interdependence of words, we employthe Minimum Classification Error (MCE) learningframework (Juang and Katagiri, 1992), which wasproposed for learning the goodness of a sequence.xt denotes the t-th original sentence in the trainingdata set T .
y?t denotes the reference compressionthat is made by humans and y?t is a compressedsentence output by a system.When using the MCE framework, the misclas-sification measure is defined as the difference be-tween the score of the reference sentence and thatof the best non-reference output and we optimizethe parameters by minimizing the measure.d(y, x;?)
= {|T |?t=1f(xt, y?t ;?)?
maxy?t6=y?tf(xt, y?t;?)}
(7)It is impossible to minimize equation (7) becausewe cannot derive the gradient of the function.Therefore, we employ the following sigmoid func-tion to smooth this measure.L(d(x, y;?))
=|T |?t=111 + exp(?c ?
d(xt, yt;?
))(8)Here, c is a constant parameter.
To minimize equa-tion (8), we use the following equation.?L=?L?d(?d??1,?d?
?2, .
.
.
)=0 (9)Here, ?L?d is given by:?L?d=c1 + exp (?c ?
d)(1 ?11 + exp (?c ?
d))(10)Finally, the parameters are optimized by usingthe iterative form.
For example, ?w is optimizedas follows:?w(new) = ?w(old) ?
?L?
?w(old)(11)829Our parameter optimization procedure can bereplaced by another one such as MIRA (McDon-ald et al, 2005) or CRFs (Lafferty et al, 2001).The reason why we employed MCE is that it isvery easy to implement.4 Experimental Evaluation4.1 Corpus and Evaluation MeasuresWe randomly selected 1,000 lead sentences (a leadsentence is the first sentence of an article exclud-ing the headline.)
whose length (number of words)was greater than 30 words from the MainichiNewspaper from 1994 to 2002.
There were fivedifferent ideal compressions (reference compres-sions produced by human) for each sentence; allhad a 0.6 compression rate.
The average length ofthe input sentences was about 42 words and that ofthe reference compressions was about 24 words.For MCE learning, we selected the referencecompression that maximize the BLEU score (Pap-ineni et al, 2002) (= argmaxr?RBLEU(r, R\r))from the set of reference compressions and used itas correct data for training.
Note that r is a ref-erence compression and R is the set of referencecompressions.We employed both automatic evaluation and hu-man subjective evaluation.
For automatic evalua-tion, we employed BLEU (Papineni et al, 2002)by following (Unno et al, 2006).
We utilized 5-fold cross validation, i.e., we broke the whole dataset into five blocks and used four of them for train-ing and the remainder for testing and repeated theevaluation on the test data five times changing thetest block each time.We also employed human subjective evaluation,i.e., we presented the compressed sentences to sixhuman subjects and asked them to evaluate thesentence for fluency and importance on a scale 1(worst) to 5 (best).
For each source sentence, theorder in which the compressed sentences were pre-sented was random.4.2 Comparison of Sentence CompressionMethodsIn order to investigate the effectiveness of the pro-posed features, we compared our method againstHori?s model (Hori and Furui, 2003), which isa state-of-the-art Japanese sentence compressorbased on the sequence-oriented approach.Table 1 shows the feature set used in our exper-iment.
Note that ?Hori??
indicates the earlier ver-Table 1: Configuration setupLabel g() h()Proposed IPTW PLM + POSw/o PLM IPTW Bigram+POSw/o IPTW IDF PLM+POSHori?
IDF TrigramProposed+Dep IPTW PLM + POS +Depw/o PLM+Dep IPTW Bigram+POS+Depw/o IPTW+Dep IDF PLM+POS+DepHori IDF Trigram+DepTable 2: Results: automatic evaluationLabel BLEUProposed .679w/o PLM .617w/o IPTW .635Hori?
.493Proposed+Dep .632w/o PLM+Dep .669w/o IPTW+Dep .656Hori .600sion of Hori?s method which does not require thedependency parser.
For example, label ?w/o IPTW+ Dep?
employs IDF term weighting as functiong(?)
and word bigram, part-of-speech bigram anddependency probability between words as func-tion h(?)
in equation (1).To obtain the word dependency probability, weuse Kudo?s relative-CaboCha (Kudo and Mat-sumoto, 2005).
We developed the n-gram lan-guage model from a 9 year set of Mainichi News-paper articles.
We optimized the parameters byusing the MCE learning framework.5 Results and Discussion5.1 Results: automatic evaluationTable 2 shows the evaluation results yielded byBLUE at the compression rate of 0.60.Without introducing dependency probability,both IPTW and PLM worked well.
Our methodachieved the highest BLEU score.
Compared to?Proposed?, ?w/o IPTW?
offers significantly worseperformance.
The results support the view that ourhypothesis, namely that the significance score ofa word depends on its position within a sentence,is effective for sentence compression.
Figure 2shows an example of Gaussian mixture with pre-83000.050.10.150.20 N/4 N/2 3N/4 Nx1, x2, ,xj, ,xN<S> </S>xFigure 2: An example of Gaussian mixture withpredicted parametersdicted parameters.
From the figure, we can seethat the positional weights for words have peaksat BOS and EOS.
This is because, in many cases,the subject appears at the beginning of Japanesesentences and the predicate at the end.Replacing PLM with the bigram languagemodel (w/o PLM) degrades the performance sig-nificantly.
This result shows that the n-gram lan-guage model is improper for sentence compres-sion because the n-gram probability is computedby using a corpus that includes both short and longsentences.
Most bigrams in a compressed sentencefollowed those in the source sentence.The dependency probability is very helpful pro-vided either IPTW or PLM is employed.
For ex-ample, ?w/o PLM + Dep?
achieved the secondhighest BLEU score.
The difference of the scorebetween ?Proposed?
and ?w/o PLM + Dep?
is only0.01 but there were significant differences as de-termined by Wilcoxon signed rank test.
Comparedto ?Hori?
?, ?Hori?
achieved a significantly higherBLEU score.The introduction of both IPTW and PLM makesthe use of dependency probability unnecessary.
Infact, the score of ?Proposed + Dep?
is not good.We believe that this is due to overfitting.
PLMis similar to dependency probability in that bothfeatures emphasize word pairs that occurred asbigrams in the source sentence.
Therefore, byintroducing dependency probability, the informa-tion within the feature vector is not increased eventhough the number of features is increased.Table 3: Results: human subjective evaluationsLabel Fluency ImportanceProposed 4.05 (?0.846) 3.33 (?0.854)w/o PLM + Dep 3.91 (?0.759) 3.24 (?0.753)Hori?
3.09 (?0.899) 2.34 (?0.696)Hori 3.28 (?0.924) 2.64 (?0.819)Human 4.86 (?0.268) 4.66 (?0.317)5.2 Results: human subjective evaluationWe used human subjective evaluations to compareour method to human compression, ?w/o PLM +Dep?
which achieved the second highest perfor-mance in the automatic evaluation, ?Hori??
and?Hori?.
We randomly selected 100 sentences fromthe test corpus and evaluated their compressedvariants in terms of ?fluency?
and ?importance.
?Table 3 shows the results, mean score of alljudgements as well as the standard deviation.The results indicate that human compressionachieved the best score in both fluency and impor-tance.
Human compression significantly outper-formed other compression methods.
This resultssupports the idea that humans can easily compresssentences with the compression rate of 0.6.
Ofthe automatic methods, our method achieved thebest score in both fluency and importance while?Hori??
was the worst performer.
Our method sig-nificantly outperformed both ?Hori?
and ?Hori?
?on both metrics.
Moreover, our method outper-formed ?w/o PLM + Dep?
again.
However, thedifferences in the scores are not significant.
Webelieve that this is due to a lack of data.
If we usemore data for the significant test, significant dif-ferences will be found.
Although our method doesnot employ any explicit syntactic information, itsfluency and importance are extremely good.
Thisconfirms the effectiveness of the new features ofIPTW and PLM.5.3 Comparison of decoding speedWe compare the decoding speed of our methodagainst that of Hori?s method.We measured the decoding time for all 1,000test sentences on a standard Linux Box (CPU:Intel c?
CoreTM 2 Extreme QX9650 (3.00GHz),Memory: 8G Bytes).
The results were as follows:Proposed: 22.14 seconds(45.2 sentences / sec),831Hori: 95.34 seconds(10.5 sentences / sec).Our method was about 4.3 times faster thanHori?s method due to the latter?s use of depen-dency parser.
This speed advantage is significantwhen on-demand sentence compression is needed.6 Related workConventional sentence compression methods em-ploy the tree trimming approach to compress asentence without changing its meaning.
For in-stance, most English sentence compression meth-ods make full parse trees and trim them by ap-plying the generative model (Knight and Marcu,2002; Turner and Charniak, 2005), discrimina-tive model (Knight and Marcu, 2002; Unno etal., 2006).
For Japanese sentences, instead of us-ing full parse trees, existing sentence compressionmethods trim dependency trees by the discrim-inative model (Takeuchi and Matsumoto, 2001;Nomoto, 2008) through the use of simple lin-ear combined features (Oguro et al, 2002).
Thetree trimming approach guarantees that the com-pressed sentence is grammatical if the source sen-tence does not trigger parsing error.
However, aswe mentioned in Section 2, the tree trimming ap-proach is not suitable for Japanese sentence com-pression because in many cases it cannot repro-duce human-produced compressions.As an alternative to these tree trimmingapproaches, sequence-oriented approaches havebeen proposed (McDonald, 2006; Nomoto, 2007;Hori and Furui, 2003; Clarke and Lapata, 2006).Nomoto (2007) and McDonald (2006) employedthe random field based approach.
Hori et al(2003) and Clarke et al (2006) employed the lin-ear model with simple combined features.
Theysimply regard a sentence as a word sequence andstructural information, such as full parse tree ordependency trees, are encoded in the sequence asfeatures.
The advantage of these methods over thetree trimming approach is that they have the poten-tial to drop arbitrary words from the original sen-tence without the need to consider the boundariesdetermined by the tree structures.
This approach ismore suitable for Japanese compression than treetrimming.
However, they still rely on syntacticinformation derived from full parsed trees or de-pendency trees.
Moreover, their use of syntacticparsers seriously degrades the decoding speed.7 ConclusionsWe proposed a syntax free sequence-orientedJapanese sentence compression method with twonovel features: IPTW and PLM.
Our methodneeds only a POS tagger.
It is significantly supe-rior to the methods that employ syntactic parsers.An experiment on a Japanese news corpus re-vealed the effectiveness of the new features.
Al-though the proposed method does not employ anyexplicit syntactic information, it outperformed,with statistical significance, Hori?s method a state-of-the-art Japanese sentence compression methodbased on the sequence-oriented approach.The contributions of this paper are as follows:?
We revealed that in compressing Japanesesentences, humans usually ignore syntacticstructures; they drop intermediate nodes ofthe dependency tree and drop words withinbunsetsu,?
As an alternative to the syntactic parser, weproposed two novel features, Intra-sentencepositional term weighting (IPTW) and thePatched language model (PLM), and showedtheir effectiveness by conducting automaticand human evaluations,?
We showed that our method is about 4.3 timesfaster than Hori?s method which employs adependency parser.ReferencesJ.
Clarke and M. Lapata.
2006.
Models for sentencecompression: A comparison across domains, train-ing requirements and evaluation measures.
In Proc.of the 21st COLING and 44th ACL, pages 377?384.C.
Hori and S. Furui.
2003.
A new approach to auto-matic speech summarization.
IEEE trans.
on Multi-media, 5(3):368?378.H.
Jing and K. McKeown.
1999.
The Decompositionof Human-Written Summary Sentences.
In Proc.
ofthe 22nd SIGIR, pages 129?136.B.
H. Juang and S. Katagiri.
1992.
DiscriminativeLearning for Minimum Error Classification.
IEEETrans.
on Signal Processing, 40(12):3043?3053.K.
Knight and D. Marcu.
2002.
Summarization be-yond sentence extraction.
Artificial Intelligence,139(1):91?107.832T.
Kudo and Y. Matsumoto.
2004.
A Boosting Algo-rithm for Classification of Semi-Structured Text.
InProc.
of the EMNLP, pages 301?308.T.
Kudo and Y. Matsumoto.
2005.
Japanese De-pendency Parsing Using Relative Preference of De-pendency (in japanese).
IPSJ Journal, 46(4):1082?1092.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Condi-tional Random Fields: Probabilistic Models for Seg-menting and Labeling Sequence Data.
In Proc.
ofthe 18th ICML, pages 282?289.R.
McDonald, K. Crammer, and F. Pereira.
2005.
On-line Large Margrin Training of Dependency Parser.In Proc.
of the 43rd ACL, pages 91?98.R.
McDonald.
2006.
Discriminative sentence com-pression with soft syntactic evidence.
In Proc.
ofthe 11th EACL, pages 297?304.T.
Nomoto.
2007.
Discriminative sentence compres-sion with conditional random fields.
InformationProcessing and Management, 43(6):1571?1587.T.
Nomoto.
2008.
A generic sentence trimmer withcrfs.
In Proc.
of the ACL-08: HLT, pages 299?307.R.
Oguro, H. Sekiya, Y. Morooka, K. Takagi, andK.
Ozeki.
2002.
Evaluation of a japanese sentencecompression method based on phrase significanceand inter-phrase dependency.
In Proc.
of the TSD2002, pages 27?32.K.
Papineni, S. Roukos, T. Ward, and W-J.
Zhu.
2002.Bleu: a method for automatic evaluation of machinetranslation.
In Proc.
of the 40th Annual Meeting ofthe Association for Computational Linguistic (ACL),pages 311?318.K.
Takeuchi and Y. Matsumoto.
2001.
Acquisitionof sentence reduction rules for improving quality oftext summaries.
In Proc.
of the 6th NLPRS, pages447?452.J.
Turner and E. Charniak.
2005.
Supervised and un-supervised learning for sentence compression.
InProc.
of the 43rd ACL, pages 290?297.Y.
Unno, T. Ninomiya, Y. Miyao, and J. Tsujii.
2006.Trimming cfg parse trees for sentence compressionusing machine learning approach.
In Proc.
of the21st COLING and 44th ACL, pages 850?857.833
