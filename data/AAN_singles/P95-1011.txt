Encoding Lexical ized Tree Adjoining Grammars with aNonmonoton ic  Inheritance HierarchyRoger  EvansIn fo rmat ion  Techno logyResearch  Ins t i tu teUn ivers i ty  of  Br ightonrpe?itri, bton.
ac .
ukGera ld  GazdarSchool  of  Cogn i t iveComput ing  Sc iencesUn ivers i ty  of Sussexgeraldg?cogs, susx.
ac.
ukDav id  Wei rSchool  of  Cogn i t ive  ~zComput ing  Sc iencesUn ivers i ty  of  Sussexdav?dw?cogs ,  usx .
ac .
ukAbst ractThis paper shows how DATR, a widely usedformal language for lexical knowledge re-presentation, can be used to define an I_TAGlexicon as an inheritance hierarchy with in-ternal lexical rules.
A bottom-up featu-ral encoding is used for LTAG trees andthis allows lexical rules to be implemen-ted as covariation constraints within fea-ture structures.
Such an approach elimina-tes the considerable redundancy otherwiseassociated with an LTAG lexicon.1 In t roduct ionThe Tree Adjoining Grammar (lAG) formalism wasfirst introduced two decades ago (3oshi et al, 1975),and since then there has been a steady stream oftheoretical work using the formalism.
But it isonly more recently that grammars of non-trivial sizehave been developed: Abeille, Bishop, Cote & Scha-bes (1990) describe a feature-based Lexicalized TreeAdjoining Grammar (\[_'lAG) for English which sub-sequently became the basis for the grammar used inthe XTAG system, a wide-coverage \[_TAG parser (Do-ran et al, 1994b; Doran et al, 1994a; XTAG Rese-arch Group, 1995).
The advent of such large gram-mars gives rise to questions of efficient representa-tion, and the fully lexicalized character of the \[TAGformalism suggests that recent research into lexicalrepresentation might be a place to look for answers(see for example Briscoe ef a/.
(1993); Daelemans &Gazdar(1992)).
In this paper we explore this sugge-stion by showing how the lexical knowledge repre-sentation language (LKRL) DA'lR (Evans & Gazdar,1989a; Evans & Gazdar, 1989b) can be used to for-mulate a compact, hierarchical encoding of an \[-'lAG.The issue of efficient representation for I_'rAG 1 isdiscussed by Vijay-Shanker & Schabes (1992), who1As with all fully lexicMized grammar formalisms,there is really no conceptual distinction to be drawn inI_TAG between the lexicon and the grammar: tile gram-rnatical rules are just lexical properties.draw attention to the considerable redundancy in-herent in \[-TAG lexicons that are expressed in a flatmanner with no sharing of structure or propertiesacross the elementary trees.
For example, XTAG cur-rently includes over 100,000 lexemes, each of whichis associated with a family of trees (typically around20) drawn from a set of over 500 elementary trees.Many of these trees have structure in common, manyof the lexemes have the same tree families, and manyof the trees within families are systematically rela-ted in ways which other formalisms capture usingtransformations or metarules.
However, the \[TAGformalism itself does not provide any direct supportfor capturing such regularities.Vijay-Shanker & Schabes address this problem byintroducing a hierarchical lexicon structure with mo-notonic inheritance and lexical rules, using an ap-proach loosely based on that of Flickinger (1987)but tailored for \[TAG trees rather than HPSG sub-categorization lists.
Becker (1993; 1994) proposes aslightly different solution, combining an inheritancecomponent and a set of metarules 2.
We share theirperception of the problem and agree that adoptinga hierarchical approach provides the best availablesolution to it.
However, rather than creating a hier-archical exical formalism that is specific to the \[_TAGproblem, we have used DATR, an LKR.L that is al-ready quite widely known and used.
From an \[TAGperspective, it makes sense to use an already availa-ble LKRL that was specifically designed to addressthese kinds of representational issues.
From a DATRperspective, I_TAG presents interesting problems ari-sing from its radically lexicalist character: all gram-matical relations, including unbounded ependencyconstructions, are represented lexically and are thusopen to lexical generalization.There are also several further benefits to be gai-ned from using an established general purpose LKRLsuch as DATR.
First, it makes it easier to comparethe resulting \[TAG lexicon with those associated withother types oflexical syntax: there are existing DATR2See Section 6 for further discussion of theseapproaches.77lexicon fragments for HPSG, PATR and Word Gram-mar, among others.
Second, DATR is not restrictedto syntactic description, so one can take advantageof existing analyses of other levels of lexical descrip-tion, such as phonology, prosody, morphology, com-positional semantics and lexical semantics 3.
Third,one can exploit existing formal and implementationwork on the language 4.2 Represent ing LTAG treesSNPI VPV o NPI PPP o NP IFigure 1: An example LTAG tree for giveThe principal unit of (syntactic) information asso-ciated with an LTAG entry is a tree structure in whichthe tree nodes are labeled with syntactic ategoriesand feature information and there is at least oneleaf node labeled with a lexical  category (such lexi-cal leaf nodes are known as anchors) .
For example,the canonical tree for a ditransitive verb such as giveis shown in figure 1.
Following LTAG conventions(for the time being), the node labels here are grosssyntactic ategory specifications to which additionalfeatural information may be added 5, and are anno-tated to indicate node type:  <> indicates an anchornode, and I indicates a substitution ode (where a3See, for example, Bleiching (1992; 1994), Brown &Hippisley (1994), Corbett & Fraser (1993), Cahill (1990;1993), Cahill &: Evans (1990), Fraser &= Corbett (inpress), Gibbon (1992), Kilgarriff (1993), Kilgarriff &Gazdar (1995), Reinhard & Gibbon (1991).4See, for example, Andry et al (1992) on compila-tion, Kilbury et al (1991) on coding DAGs, Duda & Geb-hardi (1994) on dynamic querying, Langer (1994) on re-verse querying, and Barg (1994), Light (1994), Light etal.
(1993) and Kilbury et al (1994) on automatic ac-quisition.
And there are at least a dozen different DATRimplementations available, on various platforms and pro-gramming languages.Sin fact, \[TAG commonly distinguishes two sets offeatures at each node (top and bottota), but for simpli-city we shall assume just one set in this paper.fully specified tree with a compatible root label maybe attached) 6.In representing such a tree in DATR, we do twothings.
First, in keeping with the radically lexica-list character of LTAG, we describe the tree structurefrom its (lexical) anchor upwards 7, using a variantof Kilbury's (1990) bottom-up encoding of trees.
Inthis encoding, a tree is described relative to a parti-cular distinguished leaf node (here the anchor node),using binary relations paxent,  le f t  and r ight ,  re-lating the node to the subtrees associated with itsparent, and immediate-left and -right sisters, enco-ded in the same way.
Second, we embed the resultingtree structure (i.e., the node relations and type in-formation) in the feature structure, so that the treerelations ( le f t ,  r ight  and parent )  become features.The obvious analogy here is the use of f i r s t / res tfeatures to encode subcategorisation lists in frame-works like HPSG.Thus the syntactic feature information directly as-sociated with the entry for give relates to the labelfor the v node (for example, the value of its cat  fea-ture is v, the value of type is emchor), while speci-fications of subfeatures of parent  relate to the labelof the vP node.
A simple bottom-up DATR represen-tation for the whole tree (apart from the node typeinformation) follows:Give:<cat> -- v<parent cat> = vp<parent  le f t  ca t> =np<parent parent cat> = s<right cat> =np<right right cat> = p<right right parent cat> = pp<right right right cat> =np.This says that Give is a verb, with vp as its pa-rent, an s as its grandparent and an NP to the leftof its parent.
It also has an NP to its right, and atree rooted in a P to the right of that, with a PPparent and NP right sister.
The implied bottom-uptree structure is shown graphically in figure 2.
Herethe nodes are laid out just as in figure 1, but rela-ted via parent ,  l e f t  and r ight  links, rather thanthe more usual (implicitly ordered) daughter links.Notice in particular that the r ight  link from theobject noun-phrase node points to the prepositionnode, not its phrasal parent - this whole subtree isitself encoded bottom-up.
Nevertheless, the full treestructure is completely and accurately representedby this encoding.s LTAG's other tree-building operation is adjunetion,which allows a tree-fragment to be spliced into the bodyof a tree.
However, we only need to concern ourselveshere with the representat ion of the trees involved, notwith the substitution/adjunction distinction.rThe tree in figure 1 has more than one anchor - insuch cases it is generally easy to decide which anchor isthe most appropriate root for the tree (here, the verbanchor).78np ?sarentvpl e f t /parent" npr ight  ~r ight  kPPParentnpr ightFigure 2: Bottom-up encoding for GiveOnce we adopt this representational strategy, wri-ting an LTAG lexicon in DATR becomes imilar towriting any other type of lexicalist grammar's  le-xicon in an inheritance-based LKRL.
In HPSG, forexample, the subcategorisation frames are coded aslists of categories, whilst in LTAG they are coded astrees.
But, in both cases, the problem is one of con-cisely describing feature structures associated withlexical entries and relationships between lexical ent-ries.
The same kinds of generalization arise and thesame techniques are applicable.
Of course, the pre-sence of complete trees and the fully lexicalized ap-proach provide scope for capturing generalizationslexically that are not available to approaches thatonly identify parent and sibling nodes, say, in thelexical entries.3 Encoding lexical entriesFollowing conventional models of lexicon organisa-tion, we would expect Give to have a minimal syn-tactic specification itself, since syntactically it is acompletely regular ditransitive verb.
In fact noneof the information introduced so far is specific toGive.
So rather than providing a completely expli-cit DATR definition for Give, as we did above, a moreplausible account uses an inheritance hierarchy defi-ning abstract intransitive, transitive and ditransitiveverbs to support Give (among others), as shown infigure 3.This basic organisational structure can be expres-sed as the following DATR fragmentS:8To gain the intuitive sense of this fragment, reada line such as <> --= VERB as "inherit everything fromthe definition of VERB", and a line such as <parent> ==PPTREE:<> as "inherit the parent subtree from the de-finition of PPTREE'.
Inheritance in DATR is always bydefault - locally defined feature specifications take prio-rity over inherited ones.VERBD ie  VERB+NPEat  VEKB+NP+PP VERB+NP+NPGive SpareFigure 3: The principal exical hierarchyVERB:<> -- TREENODE<cat> == v<type> == anchor<parent> =s VPTREE:<>.VERB+NP:<> == VERB<right> == NPCOMP:<>.VERB+NP+PP:<> -= VERB+NP<right r ight> == PTKEE:<><right right root> == to.VERB+NP+NP:<> == VEBB+NP<right r ight> == NPCOMP:<>.Die:<> == VERB<root> == die.Eat:<> == VEKB+NP<root> == eat.Give:<> == VERB+NP+PP<root> == give.Spare:<> == VERB+NP+NP<root> == spare.Ignoring for the moment the references toTREENODE,  VPTREE,  NPCOMP and  PTREE (which weshall define shortly), we see that VERB defines basicfeatures for all verb entries (and can be used directlyfor intransitives such as Die), VERB+NP inherits ~omVERB butadds  an NP complement to the right ofthe verb (for transitives), VEKB+NP+PP inherits ~omVERB+NP but adds a further PP complement and so79on.
Entries for regular verb lexemes are then mi-nimal - syntactically they just inherit everyth ingfrom the abstract definitions.This DATR fragment is incomplete, because it neg-lects to define the internal structure of the TREEtlODEand the various subtree nodes in the lexical hierar-chy.
Each such node is a description of an LTAG treeat some degree of abstraction 9.
The following DATRstatements complete the fragment, by providing de-finitions for this internal structure:TREENODE :<> == under<type> == in terna l .STREE:<> == TREENODE<cat> == s.VPTREE:<> == TREENODE<cat> ==vp<parent> == STREE:<><le f t> == NPCOMP:<>.NPCOMP:<> == TREENODE<cat> - -  np<type> == subst i tu t ion .PPTREE:<> == TREENODE<cat> == pp.PTREE:<> == TREENODE<cat> I= p<type> == anchor<parent> == PPTREE:<>Here, TREENODE represents an abstract node in anLTAG tree and provides a (default) type of internal.Notice that VERB is itself a TREENODE (but with thenondefault ype anchor),  and the other definitionshere define the remaining tree nodes that arise inour small lexicon: VPTREE is the node for VERB's pa-rent, STREE for VEKB's grandparent, NPCOMP definesthe structure needed for NP complement substitutionnodes, etc.
1?Taken together, these definitions provide a speci-fication for Give just as we had it before, but withthe addition of type  and root  features.
They alsosupport some other verbs too, and it should be clearthat the basic technique extends readily to a widerange of other verbs and other parts of speech.
Also,although the trees we have described are all in i t ia l9Even the lexeme nodes are abstract - individualword forms might be represented by further more specificnodes attached below the lexemes in the hierarchy.1?Our example makes much use'of multiple inheritance(thus, for example, VPTREE inherits from TREENODE,STREE and NPCOMP) but a/l such multiple inheritance isorthogonal in DATR: no path can inherit from more thanone node.trees (in LTAG terminology), we can describe auxi -l i a ry  trees, which include a leaf node of type footjust as easily.
A simple example is provided by thefollowing definition for auxiliary verbs:AUXVERB :<> == TREENODE<cat> --= V<type> == anchor<parent  cat> == vp<r ight  cut> == vp<right type> == foot .4 Lex ica l  ru lesHaving established a basic structure for our LTAGlexicon, we now turn our attention towards captu-ring other kinds of relationship among trees.
Wenoted above that lexical entries are actually associa-ted with t ree  fami l ies ,  and that these group to-gether trees that are related to each other.
Thus inthe same family as a standard ditransitive verb, wemight find the full passive, the agentless passive, thedative alternation, the various relative clauses, andso forth.
It is clear that these families correspondclosely to the outputs of transformations or metaru-les in other frameworks, but the XTAG system cur-rently has no formal component for describing therelationships among families nor mechanisms for ge-nerating them.
And so far we have said nothingabout them either - we have only characterized sin-gle trees.However, LTAG's large domain of locality meansthat all such relationships can be viewed as directlylexical, and ~hus expressible by lexical rules.
In factwe can go further than this: because we have em-bedded the domain of these lexical rules, namely theLTAG tree structures, within the feature structures,we can view such lexical rules as covariation cons-traints within feature structures, in much the sameway that the covariation of, say, syntactic and mor-phological form is treated.
In particular, we can usethe mechanisms that DATR already provides for fea-ture covariation, rather than having to invoke in ad-dition some special purpose lexical rule machinery.We consider six construction types found in theXTAG grammar:  passive, dative, subject-auxiliaryinversion, wh-questions, relative clauses and topica-lisation.
Our basic approach to each of these is thesame.
Lexical rules are specified by defining a deri-ved output  tree structure in terms of an input  treestructure, where each of these structures is a set offeature specifications of the sort defined above.
Eachlexical rule has a name, and the input and outputtree structures for rule foo are referenced by pre-fixing feature paths of the sort given above with<input  foo .
.> or <output  foo .
.>.
So for ex-ample, the category of the parent tree node of theoutput of the passive rule might be referenced as<output pass ive  parent  cat>.
We define a verygeneral default, stating that the output  is the same80as the input ,  so that lexical relationships need onlyconcern themselves with components they modify.This approach to formulating lexical rules in DAIRis quite general and in no way restricted to/TAG: itcan be readily adapted for application in the contextof any feature-based lexicalist grammar formalism.Using this approach, the dative lexical rule can begiven a minimalist implementation by the additionof the following single line to VERB+NP+PP, definedabove.VERB+NP+PP :<output dative right right> == NPCOMP:<>.This causes the second complement to a ditran-sitive verb in the dative alternation to be an NP,rather than a PP as in the unmodified case.
Subject-auxiliary inversion can be achieved similarly by justspecifying the output tree structure without refe-rence to the input structure (note the addition hereof a form feature specifying verb form):AUXVERB :<output  aux inv  fo rm> == f in i te - inv<output  aux inv  parent  cat> == s<output auxinv r ight  cat> == s.Passive is slightly more complex, in that it has tomodify the given input  tree structure rather thansimply overwriting part of it.
The definitions for pas-sive occur at the VERB+NP node, since by default, anytransitive or subclass of transitive has a passive form.Individual transitive verbs, or whole subclasses, canoverride this default, leaving their passive tree struc-ture undefined if required.
For agentless passives,the necessary additions to the VERB+NP node are asfollowsn:VERB+NP :<output passive form> == passive<output passive right> =="<input passive right r ight>".Here, the first line stipulates the form of the verbin the output tree to be passive, while the second lineredefines the complement structure: the output ofpassive has as its first complement the second com-plement of its input, thereby discarding the firstcomplement of its input.
Since complements aredaisy-chained, all the others move up too.Wh-questions, relative clauses and topicalisationare slightly different, in that the application of thelexical rule causes structure to be added to the topof the tree (above the s node).
Although these con-structions involve unbounded dependencies, the un-boundedness is taken care of by the \[TAG adjunctionmechanism: for lexical purposes the dependency islocal.
Since the relevant lexical rules can apply tosentences that contain any kind of verb, they needto be stated at the VERB node.
Thus, for exam-ple, topicalisation and wh-questions can be definedas follows:11Oversimplifying slightly, the double quotes in"<input passive right right>" mean that that DATRpath will not be evaluated locally (i.e., at the VERB+NPnode), but rather at the relevant lexeme node (e.g., Eator Give).VERB :<output topic parent parent parent cat><output topic parent "parent left cat> ==np<output topic parent parent left form>== normal<output whq> == "<output topic>"<output whq parent parent left form> == vh.Here an additional NP and s are attached abovethe original s node to create a topicalised struc-ture.
The wh-rule inherits from the topicalisationrule, changing just one thing: the form of the newNP is marked as wh, rather than as normal.
In thefull fragment 12, the NP added by these rules is alsosyntactically cross-referenced to a specific NP mar-ked as null in the input  tree.
However, space doesnot permit presentation or discussion of the DATRcode that achieves this here.5 App ly ing  lex ica l  ru lesAs explained above, each lexical rule is defined tooperate on its own notion of an input  and produceits own output .
In order for the rules to have an ef-fect, the various input  and output  paths have to belinked together using inheritance, creating a chain ofinheritances between the base, that is, the canonicaldefinitions we introduced in section 3, and sur facetree structures of the lexical entry.
For example, to'apply'  the dative rule to our Give definition, wecould construct a definition such as this:Give-dat :<> ffi= Give<input dative> == <><surface> == <output dative>.Values for paths prefixed with surface inheritfrom the output of the dative rule.
The input ofthe dative rule inherits from the base (unprefixed)case, which inherits from Give.
The dative rule de-finition (just the oneline introduced above, plus thedefault that output inherits from input) thus media-tes between q ive  and the surface of G ive-dat .
Thischain can be extended by inserting additional in-heritance specifications (such as passive).
Note thatsur face  defaults to the base case, so all entries havea sur face  defined.However, in our full fragment, additional supportis provided to achieve and constrain this rule chai-ning.
Word definitions include boolean features in-dicating which rules to apply, and the presence ofthese features trigger inheritance between appro-priate input  and output  paths and the base andsur face  specifications at the ends of the chain.
Forexample, Wordl is an alternative way of specifyingthe dative alternant of Give, but results in inhe-ritance linking equivalent to that found in G ive -databove:12The full version of this DAIR fragment includes allthe components discussed above in a single coherent, butslightly more complex account.
It is available on requestfrom the authors.81Wordl :<> == Give<al t  dat ive> == true.More interestingly, Nord2 properly describes a wh-question based on the agentless passive of the dativeof Give.Word2 :<> == Give<alt whq> == true<al t  dat ive> == true<alt pass ive> == true.<parent left form> =-  nu l lNotice here the final line of Nord2 which specifiesthe location of the 'extracted' NP (the subject, in thiscase), by marking it as null.
As noted above, the fullversion of the whq lexical rule uses this to specify across-reference r lationship between the wh-NP andthe null NP.We can, if we wish, encode constraints on the app-licability of rules in the mapping from boolean flagsto actual inheritance specifications.
Thus, for exam-ple, whq, te l ,  and top ic  are mutually exclusive.If  such constraints are violated, then no value forsur face  gets defined.
Thus Word3 improperly att-empts topicalisation in addition to wh-question for-mation, and, as a result, will fail to define a sur facetree structure at all:Word3 :<> == Give<alt whq> m= t rue<a l t  top ic> == t rue<alt dat ive> -~, t rue<alt  pass ive> -= t rue<parent left form> == nul l .This approach to lexical rules allows them to bespecified at the appropriate point in the lexicM hier-archy, but overridden or modified in subclasses orlexemes as appropriate.
It also allows default gene-ralisation over the lexical rules themselves, and con-trol over their application.
The last section showedhow the whq lexical rule could be built by a single mi-nor addition to that for topicalisation.
However, it isworth noting that, in common with other DATR spe-cifications, the lexical rules presented here are ru leins tances  which can only be applied once to anygiven lexeme - multiple application could be sup-ported, by making multiple instances inherit fromsome common rule specification, but in our currenttreatment such instances would require different rulenames.6 Compar i son  w i th  re la ted  workAs noted above, Vijay-Shanker & Schabes (1992)have also proposed an inheritance-based approachto this problem.
They use monotonic inheritance tobuild up partial descriptions of trees: each descrip-tion is a finite set of dominance, immediate domi-nance and linear precedence statements about treenodes in a tree description language developed byRogers & Vijay-Shanker (1992), and category infor-mation is located in the node labels.This differs from our approach in a number ofways.
First, our use of nonmonotonic inheritanceallows us to manipulate total instead of partial de-scriptions of trees.
The abstract verb class in theVijay-Shanker & Schabes account subsumes both in-transitive and transitive verb classes but is not iden-tical to either - a minimal-satisfying-model step isrequired to map partial tree descriptions into actualtrees.
In our analysis, VERB is the intransitive verbclass, with complements specifically marked as un-defined: thus VERB : <r ight> == under is inheritedfrom TREENODE and VERB+NP just overrides this com-plement specification to add an NP complement.
Se-cond, we describe trees using only local tree relations(between adjacent nodes in the tree), while Vijay-Shanker &5 Schabes also use a nonlocal dominancerelation.Both these properties are crucial to our embed-ding of the tree structure in the feature structure.We want the category information at each tree nodeto be partial in the conventional sense, so that inactual use such categories can be extended (by uni-fication or whatever).
So the feature structures thatwe associate with lexical entries must be viewed aspartial.
But we do not  want the tree structure tobe extendible in the same way: we do not want anintransitive verb to be applicable in a transitive con-text, by unifying in a complement NP.
So the treestructures we define must be total descriptions 13.And of course, our use of only local relations al-lows a direct mapping from tree structure to featurepath, which would not be possible at all if nonlocalrelations were present.So while these differences may seem small, they al-low us to take this significant representational step -significant because it is the tree structure mbeddingthat allows us to view lexical rules as feature cova-riation constraints.
The result is that while Vijay-Shanker & Schabes use a tree description language,a category description language and a further for-malism for lexical rules, we can capture everythingin one framework all of whose components (non-monotonicity, covariation constraint handling, etc.
)have already been independently motivated for otheraspects of lexical description 14.Becket's recent work (1993; 1994) is also directedat exactly the problem we address in the presentpaper.
Like him, we have employed an inheritancehierarchy.
And, like him, we have employed a set oflexical rules (corresponding to his metarules).
Thekey differences between our account and his are (i)13Note that simplified fragment presented here doesnot get this right.
It makes all feature specifications totaldescriptions.
To correct this we would need to changeTREENODE so that only the values of <right>, <left> and<parent> default to under.14As in the work cited in footnote 3, above.82that we have been able to use an existing lexicalknowledge representation language, rather than de-signing a formal system that is specific to \[TAG, and(ii) that we have expressed our lexical rules in ex-actly the same language as that we have used todefine the hierarchy, rather than invoking two quitedifferent formal systems.Becket's sharp distinction between his metarulesand his hierarchy gives rise to some problems thatour approach avoids.
Firstly, he notes that his meta-rules are subject to lexical exceptions and proposesto deal with these by stating "for each entry in the(syntactic) lexicon .. which metarules are applica-ble for this entry" (1993,126).
We have no need tocarry over this use of (recta)rule features ince, inour account, lexical rules are not distinct from anyother kind of property in the inheritance hierarchy.They can be stated at the most inclusive relevantnode and can then be overridden at the exceptionaldescendant nodes.
Nothing specific needs to be saidabout the nonexceptional nodes.Secondly, his metarules may themselves be moreor less similar to each other and he suggests(1994,11) that these similarities could be capturedif the metarules were also to be organized in a hier-archy.
However, our approach allows us to deal withany such similarities in the main lexical hierarchyitself 15 rather than by setting up a separate hierar-chical component just for metarules (which appearsto be what Becket has in mind).Thirdly, as he himself notes (1993,128), becausehis metarules map from elementary trees that are inthe inheritance hierarchy to elementary trees thatare outside it, most of the elementary trees actuallyused are not directly connected to the hierarchy (alt-hough their derived status with respect o it can bereconstructed).
Our approach keeps all elementarytrees, whether or not they have been partly definedby a lexical rule, entirely within the lexical hierarchy.In fact, Becker himself considers the possibilityof capturing all the significant generalizations byusing just one of the two mechanisms that he pro-poses: "one might want to reconsider the usage ofone mechanism for phenomena in both dimensions"(1993,135).
But, as he goes on to point out, his exi-sting type of inheritance network is not up to takingon the task performed by his metarules because theformer is monotonic whilst his metarules are not.However, he does suggest a way in which the hierar-chy could be completely replaced by metarules butargues against adopting it (1993,136).As will be apparent from the earlier sections ofthis paper, we believe that Becker's insights aboutthe organization of an \['lAG lexicon can be betterexpressed if the metarule component is replaced bylSAs illustrated by the way in which the whq lexicalrule inherits from that for topicalisation i the examplegiven above.an encoding of (largely equivalent) lexical rules thatare an integral part of a nonmonotonic nheritancehierarchy that stands as a description of all the ele-mentary trees.AcknowledgementsA precursor of th'is paper was presented at the Sep-tember 1994 TAG+ Workshop in Paris.
We thankthe referees for that event and the ACL-95 refereesfor a number of helpful comments.
We are also gra-teful to Aravind Joshi, Bill Keller, Owen RambowK.
Vijay-Shanker and The XTAG Group.
This rese-arch was partly supported by grants to Evans fromSERC/EPSt~C (UK) and to Gazdar from ESRC(UK).ReferencesAnne Abeille, Kathleen Bishop, Sharon Cote, & YvesSchabes.
1990.
A lexicalized tree adjoining grammarfor english.
Technical Report MS-CIS-90-24, Depart-ment of Computer & Information Science, Univ.
ofPennsylvania.Francois Andry, Norman Fraser, Scott McGlashan, Si-mon Thornton, & Nick Youd.
1992.
Making DATRwork for speech: lexicon compilation in SUNDIA.Comput.
Ling., 18(3):245-267.Petra Barg.
1994.
Automatic acquisition of datr theo-ries from observations.
Theories des lexicons: Arbei-ten des sonderforschungsbereichs 282, Heinrich-HeineUniv.
of Duesseldorf, Duesseldorf.Tilman Becker.
1993.
HyTAG: A new type of Tree Ad-joining Grammar for hybrid syntactic representationof free word order languages.
Ph.D. thesis, Univ.
desSaarlandes.Tflman Becker.
1994.
Patterns in metarules.
In Pro-ceedings of the Third International Workshop on TreeAdjoining Grammars, 9-11.Doris Bleiching.
1992.
Prosodisches wissen in lexicon.
InG.
Goerz, ed., KONVENS-92, 59-68.
Springer-Verlag.Doris Bleiching.
1994.
Integration yon morphophono-logic und prosodie in ein hierarchisches lexicon.
InH.
Trost, ed., Proceedings ofKONVENS-9.t, 32-41.Ted Briscoe, Valeria de Paiva, & Ann Copestake.
1993.Inheritance, Defaults, ?J the Lexicon.
CUP.Dunstan Brown & Andrew Hippisley.
1994.
Conflict inrussian genitive plural assignment: A solution repre-sented in DATR.
J. of Slavic Linguistics, 2(1):48-76.Lynne Cahill & Roger Evans.
1990.
An application ofDATR: the TIC lexicon.
In ECAI-90, 120-125.Lynne Cahill.
1990.
Syllable-based morphology.
InCOLING-90, volume 3, 48-53.Lynne Cahill.
1993.
Morphonology in the lexicon.
InEA CL-93, 37-96.Greville Corbett & Norman Fraser.
1993.
Network mor-phology: a DATR account of Russian nominal inflec-tion.
J. of Linguistics, 29:113-142.83Walter Daelemans & Gerald Gazdar, eds.
1992.
Specialissues on inheritance.
Gomput.
Ling., 18(2 & 3).Christy Doran, Dania Egedi, Beth Ann Hockey, & B. Sri-nivas.
1994a.
Status of the XTAG system.
In Pro-ceedings of the Third International Workshop on TreeAdjoining Grammars, 20-23.Christy Doran, Dania Egedi, Beth Ann Hockey, B. Sri-nivas, & Martin Zaldel.
1994b.
XTAG system - -  awide coverage grammar for english.
In COLING-94,922-928.Markus Duds & Gunter Gebhardi.
1994.
DUTR - aDATR-PATR interface formalism.
In H. Trost, ed.,Proceedings o\] KONVENS.9~, 411-414.Roger Evans & Gerald Gazdar.
1989a.
Inference inDATR.
In EACL.89, 66-71.Roger Evans & Gerald Gazdar.
1989b.
The semanticsof DATR.
In AISB-89, 79-87.Daniel P. Flickinger.
1987.
Le~ical Rules in the Hierar-chical Lexicon.
Ph.D. thesis, Stanford Univ.Norman Fraser & Greville Corbett.
in press.
Gender,animacy, & declensional class assignment: a unifiedaccount for russian.
In Geert Booij & Jaap van Marie,ed., Yearbook o\[ Morphology 1994.
Kluwer, Dordrecht.Dafydd Gibbon.
1992.
ILEX: a linguistic approach tocomputational lexica.
In Ursula Klenk, ed., Com-putatio Linguae: Aulsa("tze zur algorithmischen u dquantitativen Analyse der Sprache (Zeitsehrilt lu("rDialektologie und Linguistik, Beihe\[t 73), 32-53.
FranzSteiner Veflag, Stuttgart.A.
K. Joshi, L. S. Levy, & M. Takahashi.
1975.
Treeadjunct grarnmaxs.
J. Comput.
Syst.
Sci., 10(1):136-163.James Kilbury, Petra \[Barg\] Naerger, & Ingrid Renz.1991.
DATR as a lexical component for PATR.
InEACL-91, 137-142.James Kilbury, Petra Barg, ~: Ingrid Renz.
1994.
Simu-lation lexiealischen erwerbs.
In Christopher HabelGert Rickheit Sascha W. Felix, ed, Kognitive Lingui-stik: Repraesentation u d Prozesse, 251-271.
West-deutscher Verlag, Opladen.James Kilbury.
1990.
Encoding constituent structure infeature structures.
Unpublished manuscript, Univ.
ofDuesseldorf, Duesseldorf.Adam Kilgarriff & Gerald Ga~dar.
1995.
Polysemousrelations.
In Frank Palmer, ed., Grammar ~ meaning:essays in honour o\] Sir John Lyons, 1-25.
CUP.Adam Kilgarriff.
1993.
Inheriting verb alternations.
InEACL-93, 213-221.Hagen Langer.
1994.
Reverse queries in DATR.
InCOLING-94, 1089-1095.Marc Light, Sabine Reinhard, & Marie Boyle-Hinrichs.1993.
INSYST: an automatic inserter system for hier-archical lexica.
In EACL-93, page 471.Marc Light.
1994.
Classification in feature-based defaultinheritance hierarchies.
In H. Trost, ed., Proceedingso\[ KONVENS-94, 220-229.Sabine Reinhard & Dafydd Gibbon.
1991.
Prosodic in-heritance & morphological generalisations.
In EACL-91, 131-136.James Rogers & K. Vijay-Shanker.
1992.
Reasoning.with descriptions of trees.
In ACL-92, 72-80.K.
Vijay-Shanker & Yves Schabes.
1992.
Structuresharing in lexicalized tree-adjoining rammar.
InCOLING-92, 205-211.The XTAG Research Group.
1995.
A lexicalized tree ad-joining grammar for English.
Technical Report IRCSReport 95-03, The Institute for Research in CognitiveScience, Univ.
of Pennsylvania.84
