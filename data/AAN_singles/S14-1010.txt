Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 75?80,Dublin, Ireland, August 23-24 2014.Text Summarization through Entailment-based Minimum Vertex CoverAnand Gupta1, Manpreet Kaur2, Adarsh Singh2, Aseem Goel2, Shachar Mirkin31Dept.
of Information Technology, NSIT, New Delhi, India2Dept.
of Computer Engineering, NSIT, New Delhi, India3Xerox Research Centre Europe, Meylan, FranceAbstractSentence Connectivity is a textual charac-teristic that may be incorporated intelli-gently for the selection of sentences of awell meaning summary.
However, the ex-isting summarization methods do not uti-lize its potential fully.
The present pa-per introduces a novel method for single-document text summarization.
It posesthe text summarization task as an opti-mization problem, and attempts to solveit using Weighted Minimum Vertex Cover(WMVC), a graph-based algorithm.
Tex-tual entailment, an established indicator ofsemantic relationships between text units,is used to measure sentence connectivityand construct the graph on which WMVCoperates.
Experiments on a standard sum-marization dataset show that the suggestedalgorithm outperforms related methods.1 IntroductionIn the present age of digital revolution with pro-liferating numbers of internet-connected devices,we are facing an exponential rise in the volumeof available information.
Users are constantly fac-ing the problem of deciding what to read and whatto skip.
Text summarization provides a practicalsolution to this problem, causing a resurgence inresearch in this field.Given a topic of interest, a standard search of-ten yields a large number of documents.
Many ofthem are not of the user?s interest.
Rather than go-ing through the entire result-set, the reader mayread a gist of a document, produced via summa-rization tools, and then decide whether to fullyread the document or not, thus saving a substan-tial amount of time.
According to Jones (2007),a summary can be defined as ?a reductive trans-formation of source text to summary text throughcontent reduction by selection and/or generaliza-tion on what is important in the source?.
Summa-rization based on content reduction by selection isreferred to as extraction (identifying and includ-ing the important sentences in the final summary),whereas a summary involving content reductionby generalization is called abstraction (reproduc-ing the most informative content in a new way).The present paper focuses on extraction-basedsingle-document summarization.
We formulatethe task as a graph-based optimization problem,where vertices represent the sentences and edgesthe connections between sentences.
Textual en-tailment (Giampiccolo et al., 2007) is employed toestimate the degree of connectivity between sen-tences, and subsequently to assign a weight to eachvertex of the graph.
Then, the Weighted Mini-mum Vertex Cover, a classical graph algorithm,is used to find the minimal set of vertices (that is?
sentences) that forms a cover.
The idea is thatsuch cover of well-connected vertices would cor-respond to a cover of the salient content of the doc-ument.The rest of the paper is organized as follows: InSection 2, we discuss related work and describethe WMVC algorithm.
In Section 3, we proposea novel summarization method, and in Section 4,experiments and results are presented.
Finally, inSection 5, we conclude and outline future researchdirections.2 BackgroundExtractive text summarization is the task of iden-tifying those text segments which provide impor-tant information about the gist of the document?
the salient units of the text.
In (Marcu, 2008),salient units are determined as the ones that con-tain frequently-used words, contain words that arewithin titles and headings, are located at the begin-ning or at the end of sections, contain key phrasesand are the most highly connected to other parts75of the text.
In this work we focus on the last ofthe above criteria, connectivity, to find highly con-nected sentences in a document.
Such sentencesoften contain information that is found in othersentences, and are therefore natural candidates tobe included in the summary.2.1 Related WorkThe connectivity between sentences has been pre-viously exploited for extraction-based summariza-tion.
Salton et al.
(1997) generate intra-documentlinks between passages of a document using auto-matic hypertext link generation algorithms.
Maniand Bloedorn (1997) use the number of sharedwords, phrases and co-references to measure con-nectedness among sentences.
In (Barzilay and El-hadad, 1999), lexical chains are constructed basedon words relatedness.Textual entailment (TE) was exploited recentlyfor text summarization in order to find the highlyconnected sentences in the document.
Textual en-tailment is an asymmetric relation between twotext fragments specifying whether one fragmentcan be inferred from the other.
Tatar et al.
(2008)have proposed a method called Logic Text Tiling(LTT), which uses TE for sentence scoring thatis equal to the number of entailed sentences andto form text segments comprising of highly con-nected sentences.
Another method called Ana-log Textual Entailment and Spectral Clustering(ATESC), suggested in (Gupta et al., 2012), alsouses TE for sentence scoring, using analog scores.We use a graph-based algorithm to produce thesummary.
Graph-based ranking algorithms havebeen employed for text summarization in the past,with similar representation to ours.
Vertices rep-resent text units (words, phrases or sentences) andan edge between two vertices represent any kindof relationship between two text units.
Scores areassigned to the vertices using some relevant crite-ria to select the vertices with the highest scores.In (Mihalcea and Tarau, 2004), content overlapbetween sentences is used to add edges betweentwo vertices and Page Rank (Page et al., 1999) isused for scoring the vertices.
Erkan and Radev(2004) use inter-sentence cosine similarity basedon word overlap and tf-idf weighting to identifyrelations between sentences.
In our paper, we useTE to compute connectivity between nodes of thegraph and apply the weighted minimum vertexcover (WMVC) algorithm on the graph to selectthe sentences for the summary.2.2 Weighted MVCWMVC is a combinatorial optimization problemlisted within the classical NP-complete problems(Garey and Johnson, 1979; Cormen et al., 2001).Over the years, it has caught the attention of manyresearchers, due to its NP-completeness, and alsobecause its formulation complies with many realworld problems.Weighted Minimum Vertex Cover Given aweighted graph G = (V,E,w), such that w isa positive weight (cost) function on the vertices,w : V ?
R, a weighted minimum vertex cover ofG is a subset of the vertices, C ?
V such that forevery edge (u, v) ?
E either u ?
C or v ?
C(or both), and the total sum of the weights is min-imized.C = argminC??v?
C?w(v) (1)3 Weighted MVC for text summarizationWe formulate the text summarization task as aWMVC problem.
The input document to be sum-marized is represented as a weighted graph G =(V,E,w), where each of v ?
V corresponds to asentence in the document; an edge (u, v) ?
E ex-ists if either u entails v or v entails u with a valueat least as high as an empirically-set threshold.
Aweight w is then assigned to each sentence basedon (negated) TE values (see Section 3.2 for furtherdetails).
WMVC returns a cover C which is a sub-set of the sentences with a minimum total weight,corresponding to the best connected sentences inthe document.
The cover is our output ?
the sum-mary of the input document.Our proposed method, shown in Figure 1, con-sists of the following main steps.1.
Intra-sentence textual entailment score com-putation2.
Entailment-based connectivity scoring3.
Entailment connectivity graph construction4.
Application of WMVC to the graphWe elaborate on each of these steps in the fol-lowing sections.3.1 Computing entailment scoresGiven a document d for which summary is to begenerated, we represent d as an array of sentences76Id SentenceS1A representative of the African National Congress said Saturday the South African government may release black nationalist leader Nelson Mandelaas early as Tuesday.S2?There are very strong rumors in South Africa today that on Nov. 15 Nelson Mandela will be released,?
said Yusef Saloojee, chief representative inCanada for the ANC, which is fighting to end white-minority rule in South Africa.S3Mandela the 70-year-old leader of the ANC jailed 27 years ago, was sentenced to life in prison for conspiring to overthrow the South Africangovernment.S4He was transferred from prison to a hospital in August for treatment of tuberculosis.S5Since then, it has been widely rumoured Mandela will be released by Christmas in a move to win strong international support for the South Africangovernment.S6?It will be a victory for the people of South Africa and indeed a victory for the whole of Africa,?
Saloojee told an audience at the University ofToronto.S7A South African government source last week indicated recent rumours of Mandela?s impending release were orchestrated by members of theanti-apartheid movement to pressure the government into taking some action.S8And a prominent anti-apartheid activist in South Africa said there has been ?no indication (Mandela) would pe released today or in the near future.
?S9Apartheid is South Africa?s policy of racial separation.Summary ?There are very strong rumors in South Africa today that on Nov.15 Nelson Mandela will pe released,?
said Yusef Saloojee, chief representativein Canada for the ANC, which is fighting to end white-minority rule in South Africa.
He was transferred from prison to a hospital in August fortreatment of tuberculosis.
A South African government source last week indicated recent rumours of Mandela?s impending release were orchestratedby members of the anti-apartheid movement to pressure the government into taking some action.
Apartheid is South Africa?s policy of racialseparation.Table 1: The sentence array of article AP881113-0007 of cluster do106 in the DUC?02 dataset.Figure 1: Outline of the proposed method.D1?N.
An example article is shown in Table 1.We use this article to demonstrate the steps of ouralgorithm.Then, we compute a TE score between everypossible pair of sentences in D using a textual en-tailment tool.
TE scores for all the pairs are storedin a sentence entailment matrix, SEN?N.
An en-try SE[i, j] in the matrix represents the extent bywhich sentence i entails sentence j.
The sentenceentailment matrix produced for our example doc-ument is shown in Table 2.S1S2S3S4S5S6S7S8S9S1- 0 0 0.04 0 0 0.001 0.02 0.02S20.02 - 0.01 0.04 0.06 0.01 0 0.01 0.04S30 0 - 0.09 0 0 0 0 0.04S40 0 0 - 0 0 0 0 0.01S50 0 0 0.04 - 0 0.01 0.01 0.04S60 0 0 0.04 0 - 0 0 0.02S70 0 0 0.04 0.06 0 - 0.02 0.27S80 0 0 0.04 0 0 0.01 - 0.02S90 0 0 0.04 0 0 0 0 -Table 2: The sentence entailment matrix of the ex-ample article.Id ConnScore Id ConnScoreS10.08 S60.06S20.19 S70.39S30.13 S80.07S40.01 S90.04S50.1Table 3: Connectivity Scores of the sentences ofarticle AP881113-0007.3.2 Connectivity scoresOur assumption is that entailment between sen-tences indicates connectivity, that ?
as mentionedabove ?
is an indicator of sentence salience.
Morespecifically, salience of a sentence is determinedby the degree by which it entails other sentencesin the document.
We thus use the sentence entail-ment matrix to compute a connectivity score foreach sentence by summing the entailment scoresof the sentence with respect to the rest of the sen-tences in the document, and denote this sum asConnScore.
Formally, ConnScore for sentencei is computed as follows.ConnScore[i] =?i 6= jSE [i, j] (2)Applying it to each sentence in the document,we obtain the ConnScore1?Nvector.
The sen-tence connectivity scores corresponding to Table 2are shown in Table 3.3.3 Entailment connectivity graphconstructionThe more a sentence is connected, the higher itsconnectivity score.
To adapt the scores to theWMVC algorithm, that searches for a minimal so-lution, we convert the scores into positive weights77in inverted order:w[i] = ?ConnScore[i] + Z (3)w[i] is the score that is assigned to the vertex ofsentence i; Z is a large constant, meant to keepthe scores positive.
In this paper, Z has been as-signed value = 100.
Now, the better a sentence isconnected, the lower its weight.Given the weights, we construct an undi-rected weighted entailment connectivity graph,G(V,E,w), for the document d. V consists ofvertices for the document?s sentences, and E areedges that correspond to the entailment relationsbetween the sentences.
w is the weight explainedabove.
We create an edge between two vertices asexplained below.
Suppose that Siand Sjare twosentences in d, with entailment scores SE[i, j] andSE[j, i] between them.
We set a threshold ?
forthe entailment scores as the mean of all entailmentvalues in the matrix SE.
We add an edge (i, j) toG if SE[i, j] > ?
OR SE[j, i] > ?
, i.e.
if at leastone of them is as high as the threshold.Figure 2 shows the connectivity graph con-structed for the example in Table 1.Figure 2: The Entailment connectivity graph of theconsidered example with associated Score of eachnode shown.3.4 Applying WMVCFinally, we apply the weighted minimum vertexcover algorithm to find the minimal vertex cover,which would be the document?s summary.
Weuse integer linear programming (ILP) for find-ing a minimum cover.
This algorithm is a 2-approximation for the problem, meaning it is anefficient (polynomial-time) algorithm, guaranteedto find a solution that is no more than 2 times big-ger than the optimal solution.1The algorithm?s1We have used an implementation of ILP for WMVC inMATLAB, grMinVerCover.input is G = (V,E,w), a weighted graph whereeach vertex vi?
V (1 ?
i ?
n) has weight wi.
Itsoutput is a minimal vertex cover C of G, contain-ing a subset of the vertices V .
We then list thesesentences as our summary, according to their orig-inal order in the document.After applying WMVC to the graph in Fig-ure 2, the cover C returned by the algorithm is{S2, S4, S7, S9} (highlighted in Figure 2).Whenever a summary is required, a word-limiton the summary is specified.
We find the thresholdwhich results with a cover that matches the wordlimit through binary search.4 Experiments and results4.1 Experimental settingsWe have conducted experiments on the single-document summarization task of the DUC 2002dataset2, using a random sample that contains 60news articles picked from each of the 60 clus-ters available in the dataset.
The target sum-mary length limit has been set to 100 words.
Weused version 2.1.1 of BIUTEE (Stern and Da-gan, 2012), a transformation-based TE system tocompute textual entailment score between pairs ofsentences.3BIUTEE was trained with 600 text-hypothesis pairs of the RTE-5 dataset (Bentivogliet al., 2009).4.1.1 BaselinesWe have compared our method?s performancewith the following re-implemented methods:1.
Sentence selection with tf-idf: In this base-line, sentences are ranked based on the sumof the tf-idf scores of all the words exceptstopwords they contain, where idf figures arecomputed from the dataset of 60 documents.Top ranking sentences are added to the sum-mary one by one, until the word limit isreached.2.
LTT: (see Section 2)3.
ATESC : (see Section 2)4.1.2 Evaluation metricsWe have evaluated the method?s performance us-ing ROUGE (Lin, 2004).
ROUGE measures the2http://www-nlpir.nist.gov/projects/duc/data/2002_data.html3Available at: http://www.cs.biu.ac.il/?nlp/downloads/biutee.78Method P (%) R (%) F1(%)TF-IDF 13.3 17.6 15.1LTT 39.9 34.6 37.1ATESC 37.7 32.5 34.9WMVC 39.8 38.8 39.2Table 4: ROUGE-1 results.Method P (%) R (%) F1(%)TF-IDF 7.4 9.6 8.4LTT 18.4 15.2 16.6ATESC 16.3 11.7 13.6WMVC 16.7 16.8 16.8Table 5: ROUGE-2 results.quality of an automatically-generated summary bycomparing it to a ?gold-standard?, typically a hu-man generated summary.
ROUGE-n measures n-gram precision and recall of a candidate summarywith respect to a set of reference summaries.
Wecompare the system-generated summary with tworeference summaries for each article in the dataset,and show the results for ROUGE-1, ROUGE-2 andROUGE-SU4 that allows skips within n-grams.These metrics were shown to perform well forsingle document text summarization, especiallyfor short summaries.
Specifically, Lin and Hovy(2003) showed that ROUGE-1 achieves high cor-relation with human judgments.44.2 ResultsThe results for ROUGE-1, ROUGE-2 andROUGE-SU4 are shown in Tables 4, 5 and 6, re-spectively.
For each, we show the precision (P),recall (R) and F1scores.
Boldface marks the high-est score in each table.
As shown in the tables,our method achieves the best score for each of thethree metrics.4.3 AnalysisThe entailment connectivity graph generated con-veys information about the connectivity of sen-tences in the document, an important parameterfor indicating the salience of a sentences.The purpose of the WMVC is therefore to finda subset of the sentences that are well-connectedand cover all the content of all the sentences.
Notethat merely selecting the sentences on the basisof a greedy approach, that picks the those sen-tences with the highest connectivity score, doesnot ensure that all edges of the graph are cov-4See (Lin, 2004) for formal definitions of these metrics.Method P (%) R (%) F1(%)TF-IDF 2.2 4.2 2.9LTT 16 11.8 13.6ATESC 15.5 11.1 12.9WMVC 14.1 14.2 14.2Table 6: ROUGE-SU4 results.ered, i.e.
it does not ensure that all the infor-mation is covered in the summary.
In Figure 3,we illustrate the difference between WMVC (left)and a greedy algorithm (right) over our exampledocument.
The vertices selected by each algo-rithm are highlighted.
The selected set by WMVC,{S2, S4, S7, S9}, covers all the edges in the graph.In contrast, using the greedy algorithm, the subsetof vertices selected on the basis of highest scoresis {S2, S3, S7, S8}.
There, several edges are notcovered (e.g.
(S1?
S9)).It is therefore much more in sync with the sum-marization goal of finding a subset of sentencesthat conveys the important information of the doc-ument in a compressed manner.S1 S2 S4S6S7 S9S5S3S8Weighted  Minimum Vertex Cover    Greedy   vertex  selectionS1 S2 S4S6S7 S9S5S3S8Figure 3: Minimum Vertex Cover vs. Greedy se-lection of sentences.5 Conclusions and future workThe paper presents a novel method for single-document extractive summarization.
We formu-late the summarization task as an optimizationproblem and employ the weighted minimum ver-tex cover algorithm on a graph based on textual en-tailment relations between sentences.
Our methodhas outperformed previous methods that employedTE for summarization as well as a frequency-based baseline.
For future work, we wish to ap-ply our algorithm on smaller segments of the sen-tences, using partial textual entailment Levy et al.
(2013), where we may obtain more reliable en-tailment measurements, and to apply the same ap-proach for multi-document summarization.79ReferencesRegina Barzilay and Michael Elhadad.
1999.
Usinglexical chains for text summanzauon.
In In Inder-jeet Mani and Mark T. Maybury, editors, Advancesin Automatic Text Summarization, pages 111?121,The MIT Press, 1999.Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, DaniloGiampiccolo, and Bernardo Magnini.
2009.
Thefifth pascal recognizing textual entailment chal-lenge.
In Proceedings of Text Analysis Conference,pages 14?24, Gaithersburg, Maryland USA.Thomas H. Cormen, Charles E. Leiserson, Ronald L.Rivest, and Clifford Stein.
2001.
Introduction toAlgorithms.
McGraw-Hill, New York, 2nd edition.Gunes Erkan and Dragomir R. Radev.
2004.
Lexrank:Graph-based lexical centrality as salience in textsummarization.
Journal of Artificial IntelligneceResearch(JAIR), 22(1):457?479.Michael R. Garey and David S. Johnson.
1979.
Com-puters and Intractability: A Guide to the Theory ofNP-Completeness.
FREEMAN, New York.Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,and Bill Dolan.
2007.
The third pascal recognizingtextual entailment challenge.
In Proceedings of theAssociation for Computational Linguistics, ACL?07,pages 1?9, Prague, Czech Republic.Anand Gupta, Manpreet Kaur, Arjun Singh, AshishSachdeva, and Shruti Bhati.
2012.
Analog textualentailment and spectral clustering (atesc) based sum-marization.
In Lecture Notes in Computer Science,Springer, pages 101?110, New Delhi, India.Karen Spark Jones.
2007.
Automatic summarizing:The state of the art.
Information Processing andManagement, 43:1449?1481.Omer Levy, Torsten Zesch, Ido Dagan, and IrynaGurevych.
2013.
Recognizing partial textual entail-ment.
In Proceedings of the Association for Compu-tational Linguistics, pages 17?23, Sofia, Bulgaria.Chin-Yew Lin and Eduard Hovy.
2003.
Auto-matic evaluation of summaries using n-gram co-occurrence statistics.
In Proceedings of the 2003Conference of the North American Chapter of theAssociation for Computational Linguistics on Hu-man Language Technology-Volume 1, pages 71?78,Edmonta, Canada, 27 May- June 1.Chin-Yew Lin.
2004.
Rouge: A package for auto-matic evaluation of summaries.
In Proceedings ofthe Workshop on Text Summarization Branches Out,pages 25?26, Barcelona, Spain.Inderjeet Mani and Eric Bloedorn.
1997.
Multi-document summarization by graph search andmatching.
In Proceedings of the Fourteenth Na-tional Conference on Articial Intelligence (AAAI-97), American Association for Articial Intelligence,pages 622?628, Providence, Rhode Island.Daniel Marcu.
2008.
From discourse structure to textsummaries.
In Proceedings of the ACL/EACL ?97,Workshop on Intelligent Scalable Text Summariza-tion, pages 82?88, Madrid, Spain.Rada Mihalcea and Paul Tarau.
2004.
Textrank:Bringing order into texts.
In Proceedings ofEMNLP, volume 4(4), page 275, Barcelona, Spain.Lawrence Page, Sergey Brin, Rajeev Motwani, andTerry Winograd.
1999.
The pagerank citation rank-ing: Bringing order to the web.
Technical Report.Gerard Salton, Amit Singhal, Mandar Mitra, and ChrisBuckley.
1997.
Automatic text structuring and sum-marization.
Information Processing and Manage-ment, 33:193?207.Asher Stern and Ido Dagan.
2012.
BIUTEE: A mod-ular open-source system for recognizing textual en-tailment.
In Proceedings of the ACL 2012 SystemDemonstrations, pages 73?78, Jeju, Korea.Doina Tatar, Emma Tamaianu Morita, Andreea Mihis,and Dana Lupsa.
2008.
Summarization by logicseg-mentation and text entailment.
In Conference onIntelligent Text Processing and Computational Lin-guistics (CICLing 08), pages 15?26, Haifa, Israel.80
