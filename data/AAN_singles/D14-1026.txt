Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 207?213,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsA Human Judgment Corpus and a Metric for Arabic MT EvaluationHouda Bouamor, Hanan Alshikhabobakr, Behrang Mohit and Kemal OflazerCarnegie Mellon University in Qatar{hbouamor,halshikh,behrang,ko}@cmu.eduAbstractWe present a human judgments datasetand an adapted metric for evaluation ofArabic machine translation.
Our medium-scale dataset is the first of its kind for Ara-bic with high annotation quality.
We usethe dataset to adapt the BLEU score forArabic.
Our score (AL-BLEU) providespartial credits for stem and morphologi-cal matchings of hypothesis and referencewords.
We evaluate BLEU, METEOR andAL-BLEU on our human judgments cor-pus and show that AL-BLEU has the high-est correlation with human judgments.
Weare releasing the dataset and software tothe research community.1 IntroductionEvaluation of Machine Translation (MT) contin-ues to be a challenging research problem.
Thereis an ongoing effort in finding simple and scal-able metrics with rich linguistic analysis.
A widerange of metrics have been proposed and evaluatedmostly for European target languages (Callison-Burch et al., 2011; Mach?a?cek and Bojar, 2013).These metrics are usually evaluated based on theircorrelation with human judgments on a set of MToutput.
While there has been growing interest inbuilding systems for translating into Arabic, theevaluation of Arabic MT is still an under-studiedproblem.
Standard MT metrics such as BLEU (Pa-pineni et al., 2002) or TER (Snover et al., 2006)have been widely used for evaluating Arabic MT(El Kholy and Habash, 2012).
These metrics usestrict word and phrase matching between the MToutput and reference translations.
For morpholog-ically rich target languages such as Arabic, suchcriteria are too simplistic and inadequate.
In thispaper, we present: (a) the first human judgmentdataset for Arabic MT (b) the Arabic LanguageBLEU (AL-BLEU), an extension of the BLEUscore for Arabic MT evaluation.Our annotated dataset is composed of the outputof six MT systems with texts from a diverse set oftopics.
A group of ten native Arabic speakers an-notated this corpus with high-levels of inter- andintra-annotator agreements.
Our AL-BLEU met-ric uses a rich set of morphological, syntactic andlexical features to extend the evaluation beyondthe exact matching.
We conduct different exper-iments on the newly built dataset and demonstratethat AL-BLEU shows a stronger average correla-tion with human judgments than the BLEU andMETEOR scores.
Our dataset and our AL-BLEUmetric provide useful testbeds for further researchon Arabic MT and its evaluation.12 Related WorkSeveral studies on MT evaluation have pointed outthe inadequacy of the standard n-gram based eval-uation metrics for various languages (Callison-Burch et al., 2006).
For morphologically complexlanguages and those without word delimiters, sev-eral studies have attempted to improve upon themand suggest more reliable metrics that correlatebetter with human judgments (Denoual and Lep-age, 2005; Homola et al., 2009).A common approach to the problem of mor-phologically complex words is to integrate somelinguistic knowledge in the metric.
ME-TEOR (Denkowski and Lavie, 2011), TER-Plus (Snover et al., 2010) incorporate limited lin-guistic resources.
Popovi?c and Ney (2009) showedthat n-gram based evaluation metrics calculated onPOS sequences correlate well with human judg-ments, and recently designed and evaluated MPF,a BLEU-style metric based on morphemes andPOS tags (Popovi?c, 2011).
In the same direc-1The dataset and the software are available at:http://nlp.qatar.cmu.edu/resources/AL-BLEU207tion, Chen and Kuhn (2011) proposed AMBER,a modified version of BLEU incorporating re-call, extra penalties, and light linguistic knowl-edge about English morphology.
Liu et al.
(2010)propose TESLA-M, a variant of a metric basedon n-gram matching that utilizes light-weight lin-guistic analysis including lemmatization, POS tag-ging, and WordNet synonym relations.
This met-ric was then extended to TESLA-B to modelphrase synonyms by exploiting bilingual phrasetables (Dahlmeier et al., 2011).
Tantug et al.
(2008) presented BLEU+, a tool that implementsvarious extension to BLEU computation to allowfor a better evaluation of the translation perfor-mance for Turkish.To the best of our knowledge the only humanjudgment dataset for Arabic MT is the small cor-pus which was used to tune parameters of the ME-TEOR metric for Arabic (Denkowski and Lavie,2011).
Due to the shortage of Arabic human judg-ment dataset, studies on the performance of eval-uation metrics have been constrained and limited.A relevant effort in this area is the upper-bound es-timation of BLEU and METEOR scores for Ara-bic MT output (El Kholy and Habash, 2011).
Aspart of its extensive functionality, the AMEANAsystem provides the upper-bound estimate by anexhaustive matching of morphological and lexicalfeatures between the hypothesis and the referencetranslations.
Our use of morphological and lex-ical features overlaps with the AMEANA frame-work.
However, we extend our partial matchingto a supervised tuning framework for estimatingthe value of partial credits.
Moreover, our humanjudgment dataset allows us to validate our frame-work with a large-scale gold-standard data.3 Human judgment datasetWe describe here our procedure for compiling adiverse Arabic MT dataset and annotating it withhuman judgments.3.1 Data and systemsWe annotate a corpus composed of three datasets:(1) the standard English-Arabic NIST 2005 cor-pus, commonly used for MT evaluations and com-posed of news stories.
We use the first Englishtranslation as the source and the single corre-sponding Arabic sentence as the reference.
(2) theMEDAR corpus (Maegaard et al., 2010) that con-sists of texts related to the climate change withfour Arabic reference translations.
We only usethe first reference in this study.
(3) a small datasetof Wikipedia articles (WIKI) to extend our cor-pus and metric evaluation to topics beyond thecommonly-used news topics.
This sub-corpusconsists of our in-house Arabic translations ofseven English Wikipedia articles.
The articles are:Earl Francis Lloyd, Western Europe, Citizenship,Marcus Garvey, Middle Age translation, Acadian,NBA.
The English articles which do not exist inthe Arabic Wikipedia were manually translated bya bilingual linguist.Table 1 gives an overview of these sub-corporacharacteristics.NIST MEDAR WIKI# of Documents 100 4 7# of Sentences 1056 509 327Table 1: Statistics on the datasets.We use six state-of-the-art English-to-ArabicMT systems.
These include four research-orientedphrase-based systems with various morphologicaland syntactic features and different Arabic tok-enization schemes and also two commercial off-the-shelf systems.3.2 Annotation of human judgmentsIn order conduct a manual evaluation of the sixMT systems, we formulated it as a ranking prob-lem.
We adapt the framework used in the WMT2011 shared task for evaluating MT metrics onEuropean language pairs (Callison-Burch et al.,2011) for Arabic MT.
We gather human rankingjudgments by asking ten annotators (each nativespeaker of Arabic with English as a second lan-guage) to assess the quality of the English-Arabicsystems, by ranking sentences relative to eachother, from the best to the worst (ties are allowed).We use the Appraise toolkit (Federmann, 2012)designed for manual MT evaluation.
The tool dis-plays to the annotator, the source sentence andtranslations produced by various MT systems.
Theannotators received initial training on the tool andthe task with ten sentences.
They were presentedwith a brief guideline indicating the purpose of thetask and the main criteria of MT output evaluation.Each annotator was assigned to 22 rankingtasks.
Each task included ten screens.
Each screeninvolveed ranking translations of ten sentences.
Intotal, we collected 22, 000 rankings for 1892 sen-208tences (22 tasks?10 screens?10 judges).
In eachannotation screen, the annotator was shown thesource-language (English) sentences, as well asfive translations to be ranked.
We did not provideannotators with the reference to avoid any bias inthe annotation process.
Each source sentence waspresented with its direct context.
Rather than at-tempting to get a complete ordering over the sys-tems, we instead relied on random selection and areasonably large sample size to make the compar-isons fair (Callison-Burch et al., 2011).An example of a source sentence and its fivetranslations to be ranked is given in Table 2.3.3 Annotation quality and analysisIn order to ensure the validity of any evaluationsetup, a reasonable of inter- and intra-annotatoragreement rates in ranking should exist.
To mea-sure these agreements, we deliberately reassigned10% of the tasks to second annotators.
More-over, we ensured that 10% of the screens are re-displayed to the same annotator within the sametask.
This procedure allowed us to collect reliablequality control measure for our dataset.
?inter?intraEN-AR 0.57 0.62Average EN-EU 0.41 0.57EN-CZ 0.40 0.54Table 3: Inter- and intra-annotator agreementscores for our annotation compared to the aver-age scores for five English to five European lan-guages and also English-Czech (Callison-Burch etal., 2011).We measured head-to-head pairwise agreementamong annotators using Cohen?s kappa (?)
(Co-hen, 1968), defined as follows:?
=P (A)?
P (E)1?
P (E)where P(A) is the proportion of times annotatorsagree and P(E) is the proportion of agreement bychance.Table 3 gives average values obtained for inter-annotator and intra-annotator agreement and com-pare our results to similar annotation efforts inWMT-13 on different European languages.
Herewe compare against the average agreement for En-glish to five languages and also from English toone morphologically rich language (Czech).4Based on Landis and Koch (1977) ?
interpre-tation, the ?intervalue (57%) and also compar-ing our agreement scores with WMT-13 annota-tions, we believe that we have reached a reliableand consistent annotation quality.4 AL-BLEUDespite its well-known shortcomings (Callison-Burch et al., 2006), BLEU continues to be thede-facto MT evaluation metric.
BLEU uses anexact n-gram matching criterion that is too strictfor a morphologically rich language like Arabic.The system outputs in Table 2 are examples ofhow BLEU heavily penalizes Arabic.
Based onBLEU, the best hypothesis is from Sys5which hasthree unigram and one bigram exact matches withthe reference.
However, the sentence is the 4thranked by annotators.
In contrast, the output ofSys3(ranked 1stby annotators) has only one ex-act match, but several partial matches when mor-phological and lexical information are taken intoconsideration.We propose the Arabic Language BLEU (AL-BLEU) metric which extends BLEU to deal withArabic rich morphology.
We extend the matchingto morphological, syntactic and lexical levels withan optimized partial credit.
AL-BLEU starts withthe exact matching of hypothesis tokens againstthe reference tokens.
Furthermore, it considers thefollowing: (a) morphological and syntactic featurematching, (b) stem matching.
Based on Arabic lin-guistic intuition, we check the matching of a sub-set of 5 morphological features: (i) POS tag, (ii)gender (iii) number (iv) person (v) definiteness.We use the MADA package (Habash et al., 2009)to collect the stem and the morphological featuresof the hypothesis and reference translation.Figure 1 summarizes the function in which weconsider partial matching (m(th, tr)) of a hypoth-esis token (th) and its associated reference token(tr).
Starting with the BLEU criterion, we firstcheck if the hypothesis token is same as the ref-erence one and provide the full credit for it.
Ifthe exact matching fails, we provide partial creditfor matching at the stem and morphological level.The value of the partial credits are the sum ofthe stem weight (ws) and the morphological fea-4We compare against the agreement score for annotationsperformed by WMT researchers which are higher than theWMT annotations on Mechanical Turk.209Source France plans to attend ASEAN emergency summit.Reference .?KPA??@?AJ?B@???P?
?k ?Q?K A?Q?frnsaA tEtzm HDwr qmp AaAlaAsyaAn AaAlTaAr}ipHypothesisSystems RankAnnotBLEU RankBLEUAL-BLEU RankAL?BLEUSys12 0.0047 2 0.4816 1?KPA??@?AJ?B@???P??m?A?Q?
?
?m'?wtxTaT frnsaA lHDwr qmp AaAl?syaAn AaAlTaAr}ipSys23 0.0037 3 0.0840 3?AJ?B@???P??m?A?Q?
?
?m'?wtxTaT frnsaA lHDwr qmp AaAlOasyaAnSys31 0.0043 4 0.0940 2?AJ?C??KPA??@????
@ P??m??
?m'A?Q?frnsaA txTaT lHDwr AaAlqmp AaAlTaAr}ip lalOasyaAnSys45 0.0043 4 0.0604 5?P@???@?AJ?@???P??m?A?Q?
?
?kxTaT frnsaA lHDwr qmp ?syaAn AaAlTwaAri}Sys54 0.0178 1 0.0826 4?P@???
@ ??k?AJ?B@???P?
?m?A?Q?frnsaA lHDwr qmp AaAlaAsyaAn xTaT AaAlTwaAri}Table 2: Example of ranked MT outputs in our gold-standard dataset.
The first two rows specify theEnglish input and the Arabic reference, respectively.
The third row of the table lists the different MTsystem as ranked by annotators, using BLEU scores (column 4) and AL-BLEU (column 6).
The differ-ent translation candidates are given here along with their associated Bucklwalter transliteration.3Thisexample, shows clearly that AL-BLEU correlates better with human decision.m(th, tr) =??????
?1, if th= trws+5?i=1wfiotherwiseFigure 1: Formulation of our partial matching.ture weights (wfi).
Each weight is included inthe partial score, if such matching exist (e.g., stemmatch).
In order to avoid over-crediting, we limitthe range of weights with a set of constraints.Moreover, we use a development set to optimizethe weights towards improvement of correlationwith human judgments, using a hill-climbing al-gorithm (Russell and Norvig, 2009).
Figure 2 il-lustrates these various samples of partial matchinghighlighted in different colors.?????
?????
????
???
???????
????????????
????
?????
?????
???????
??????
?REF:HYP:SRC:    France Plans To Attend ASEAN Emergency SummitFigure 2: An MT example with exact matchings(blue), stem and morphological matching (green),stem only matching (red) and morphological-onlymatching (pink).Following the BLEU-style exact matching andscoring of different n-grams, AL-BLEU updatesthe n-gram scores with the partial credits fromnon-exact matches.
We use a minimum partialcredit for n-grams which have tokens with dif-ferent matching score.
The contribution of apartially-matched n-gram is not 1 (as counted inBLEU), but the minimum value that individual to-kens within the bigram are credited.
For exam-ple, if a bigram is composed of a token with exactmatching and a token with stem matching, this bi-gram receives a credit equal to a unigram with thestem matching (a value less than 1).
While par-tial credits are added for various n-grams, the fi-nal computation of the AL-BLEU is similar to theoriginal BLEU based on the geometric mean of thedifferent matched n-grams.
We follow BLEU inusing a very small smoothing value to avoid zeron-gram counts and zero score.5 Experiments and resultsAn automatic evaluation metric is said to be suc-cessful if it is shown to have high agreement withhuman-performed evaluations (Soricut and Brill,2004).
We use Kendall?s tau ?
(Kendall, 1938),a coefficient to measure the correlation betweenthe system rankings and the human judgments atthe sentence level.
Kendall?s tau ?
is calculated asfollows:?
=# of concordant pairs - # of discordant pairstotal pairswhere a concordant pair indicates two translationsof the same sentence for which the ranks obtainedfrom the manual ranking task and from the corre-sponding metric scores agree (they disagree in adiscordant pair).
The possible values of ?
rangefrom -1 (all pairs are discordant) to 1 (all pairs210Dev TestBLEU 0.3361 0.3162METEOR 0.3331 0.3426AL-BLEUMorph0.3746 0.3535AL-BLEULex0.3732 0.3564AL-BLEU 0.3759 0.3521Table 4: Comparison of the average Kendall?s ?correlation.are concordant).
Thus, an automatic evaluationmetric with a higher ?
value is making predic-tions that are more similar to the human judgmentsthan an automatic evaluation metric with a lower?
.
We calculate the ?
score for each sentence andaverage the scores to reach the corpus-level cor-relation.
We conducted a set of experiments tocompare the correlation of AL-BLEU against thestate-of-the art MT evaluation metrics.
For this weuse a subset of 900 sentences extracted from thedataset described in Section 3.1.
As mentionedabove, the stem and morphological features in AL-BLEU are parameterized each by weights whichare used to calculate the partial credits.
We op-timize the value of each weight towards correla-tion with human judgment by hill climbing with100 random restarts using a development set of600 sentences.
The 300 remaining sentences (100from each corpus) are kept for testing.
The de-velopment and test sets are composed of equalportions of sentences from the three sub-corpora(NIST, MEDAR, WIKI).As baselines, we measured the correlation ofBLEU and METEOR with human judgments col-lected for each sentence.
We did not observea strong correlation with the Arabic-tuned ME-TEOR.
We conducted our experiments on the stan-dard METEOR which was a stronger baseline thanits Arabic version.
In order to avoid the zero n-gram counts and artificially low BLEU scores, weuse a smoothed version of BLEU.
We follow Liuand Gildea (2005) to add a small value to both thematched n-grams and the total number of n-grams(epsilon value of 10?3).
In order to reach an op-timal ordering of partial matches, we conducted aset of experiments in which we compared differ-ent orders between the morphological and lexicalmatchings to settle with the final order which waspresented in Figure 1.Table 4 shows a comparison of the average cor-relation with human judgments for BLEU, ME-TEOR and AL-BLEU.
AL-BLEU shows a strongimprovement against BLEU and a competitive im-provement against METEOR both on the test anddevelopment sets.
The example in Table 2 showsa sample case of such improvement.
In the ex-ample, the sentence ranked the highest by the an-notator has only two exact matching with the ref-erence translation (which results in a low BLEUscore).
The stem and morphological matching ofAL-BLEU, gives a score and ranking much closerto human judgments.6 ConclusionWe presented AL-BLEU, our adaptation of BLEUfor the evaluation of machine translation into Ara-bic.
The metric uses morphological, syntactic andlexical matching to go beyond exact token match-ing.
We also presented our annotated corpus ofhuman ranking judgments for evaluation of Ara-bic MT.
The size and diversity of the topics inthe corpus, along with its relatively high annota-tion quality (measured by IAA scores) makes ita useful resource for future research on ArabicMT.
Moreover, the strong performance of our AL-BLEU metric is a positive indicator for future ex-ploration of richer linguistic information in evalu-ation of Arabic MT.7 AcknowledgementsWe thank Michael Denkowski, Ahmed El Kholy,Francisco Guzman, Nizar Habash, Alon Lavie,Austin Matthews, Preslav Nakov for their com-ments and help in creation of our dataset.
Wealso thank our team of annotators from CMU-Qatar.
This publication was made possible bygrants YSREP-1-018-1-004 and NPRP-09-1140-1-177 from the Qatar National Research Fund (amember of the Qatar Foundation).
The statementsmade herein are solely the responsibility of the au-thors.ReferencesChris Callison-Burch, Miles Osborne, and PhilippKoehn.
2006.
Re-evaluating the Role of BLEU inMachine Translation Research.
In Proceedings ofthe 11th Conference of the European Chapter of theAssociation for Computational Linguistics, Trento,Italy.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Omar Zaidan.
2011.
Findings of the 2011Workshop on Statistical Machine Translation.
In211Proceedings of the Sixth Workshop on StatisticalMachine Translation, Edinburgh, Scotland.Boxing Chen and Roland Kuhn.
2011.
AMBER: AModified BLEU, Enhanced Ranking Metric.
In Pro-ceedings of the Sixth Workshop on Statistical Ma-chine Translation, pages 71?77, Edinburgh, Scot-land.Jacob Cohen.
1968.
Weighted Kappa: Nominal ScaleAgreement Provision for Scaled Disagreement orPartial Credit.
Psychological bulletin, 70(4):213.Daniel Dahlmeier, Chang Liu, and Hwee Tou Ng.2011.
TESLA at WMT 2011: Translation Eval-uation and Tunable Metric.
In Proceedings of theSixth Workshop on Statistical Machine Translation,pages 78?84, Edinburgh, Scotland, July.
Associationfor Computational Linguistics.Michael Denkowski and Alon Lavie.
2011.
Meteor1.3: Automatic Metric for Reliable Optimizationand Evaluation of Machine Translation Systems.
InProceedings of the EMNLP 2011 Workshop on Sta-tistical Machine Translation, Edinburgh, UK.Etienne Denoual and Yves Lepage.
2005.
BLEU inCharacters: Towards Automatic MT Evaluation inLanguages Without Word Delimiters.
In Proceed-ings of the Second International Joint Conference onNatural Language Processing, Jeju Island, Republicof Korea.Ahmed El Kholy and Nizar Habash.
2011.
Auto-matic Error Analysis for Morphologically Rich Lan-guages.
In Proceedings of the MT Summit XIII,pages 225?232, Xiamen, China.Ahmed El Kholy and Nizar Habash.
2012.
Ortho-graphic and Morphological Processing for English-Arabic Statistical Machine Translation.
MachineTranslation, 26(1):25?45.Christian Federmann.
2012.
Appraise: an Open-Source Toolkit for Manual Evaluation of MT Out-put.
The Prague Bulletin of Mathematical Linguis-tics, 98(1):25?35.N.
Habash, O. Rambow, and R. Roth.
2009.
Mada+Tokan: A Toolkit for Arabic Tokenization, Diacriti-zation, Morphological Disambiguation, POS Tag-ging, Stemming and Lemmatization.
In Proceed-ings of the Second International Conference on Ara-bic Language Resources and Tools (MEDAR), Cairo,Egypt.Petr Homola, Vladislav Kubo?n, and Pavel Pecina.2009.
A Simple Automatic MT Evaluation Metric.In Proceedings of the Fourth Workshop on StatisticalMachine Translation, pages 33?36, Athens, Greece,March.
Association for Computational Linguistics.Maurice G Kendall.
1938.
A New Measure of RankCorrelation.
Biometrika.J Richard Landis and Gary G Koch.
1977.
The Mea-surement of Observer Agreement for CategoricalData.
Biometrics, 33(1):159?174.Ding Liu and Daniel Gildea.
2005.
Syntactic Featuresfor Evaluation of Machine Translation.
In Proceed-ings of the ACL Workshop on Intrinsic and Extrin-sic Evaluation Measures for Machine Translationand/or Summarization, pages 25?32.Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng.2010.
TESLA: Translation Evaluation of Sentenceswith Linear-Programming-Based Analysis.
In Pro-ceedings of the Joint Fifth Workshop on StatisticalMachine Translation and Metrics (MATR), pages354?359.Matou?s Mach?a?cek and Ond?rej Bojar.
2013.
Resultsof the WMT13 Metrics Shared Task.
In Proceed-ings of the Eighth Workshop on Statistical MachineTranslation, pages 45?51, Sofia, Bulgaria.Bente Maegaard, Mohamed Attia, Khalid Choukri,Olivier Hamon, Steven Krauwer, and MustafaYaseen.
2010.
Cooperation for Arabic LanguageResources and Tools?The MEDAR Project.
In Pro-ceedings of LREC, Valetta, Malta.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A Method for Auto-matic Evaluation of Machine Translation.
In Pro-ceedings of the 40th Annual Meeting on Associa-tion for Computational Linguistics, pages 311?318,Philadelphia, Pennsylvania.Maja Popovi?c and Hermann Ney.
2009.
Syntax-oriented Evaluation Measures for Machine Trans-lation Output.
In Proceedings of the Fourth Work-shop on Statistical Machine Translation, pages 29?32, Athens, Greece.Maja Popovi?c.
2011.
Morphemes and POS Tags forn-gram Based Evaluation Metrics.
In Proceedingsof the Sixth Workshop on Statistical Machine Trans-lation, pages 104?107, Edinburgh, Scotland.Stuart Russell and Peter Norvig.
2009.
Artificial Intel-ligence: A Modern Approach.
Prentice Hall Engle-wood Cliffs.Matthew Snover, Bonnie J. Dorr, Richard Schwartz,Linnea Micciulla, and John Makhoul.
2006.
AStudy of Translation Edit Rate with Targeted Hu-man Annotation.
In Proceedings of AMTA, Boston,USA.Matthew Snover, Nitin Madnani, Bonnie J. Dorr, andRichard Schwartz.
2010.
TER-Plus: Paraphrase,Semantic, and Alignment Enhancements to Transla-tion Edit Rate.
Machine Translation, 23(2-3).Radu Soricut and Eric Brill.
2004.
A Unified Frame-work For Automatic Evaluation Using 4-Gram Co-occurrence Statistics.
In Proceedings of the 42ndMeeting of the Association for Computational Lin-guistics (ACL?04), Main Volume, pages 613?620,Barcelona, Spain, July.212C?uneyd Tantug, Kemal Oflazer, and Ilknur Durgar El-Kahlout.
2008.
BLEU+: a Tool for Fine-GrainedBLEU Computation.
In Proceedings of the 6thedition of the Language Resources and EvaluationConference, Marrakech, Morocco.213
