Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 84?95,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsAccurate Parsing with Compact Tree-Substitution Grammars: Double-DOPFederico Sangati and Willem ZuidemaInstitute for Logic, Language and ComputationUniversity of AmsterdamScience Park 904, 1098 XH Amsterdam, The Netherlands{f.sangati,zuidema}@uva.nlAbstractWe present a novel approach to Data-OrientedParsing (DOP).
Like other DOP models, ourparser utilizes syntactic fragments of arbitrarysize from a treebank to analyze new sentences,but, crucially, it uses only those which areencountered at least twice.
This criterion al-lows us to work with a relatively small butrepresentative set of fragments, which can beemployed as the symbolic backbone of sev-eral probabilistic generative models.
For pars-ing we define a transform-backtransform ap-proach that allows us to use standard PCFGtechnology, making our results easily replica-ble.
According to standard Parseval metrics,our best model is on par with many state-of-the-art parsers, while offering some comple-mentary benefits: a simple generative proba-bility model, and an explicit representation ofthe larger units of grammar.1 IntroductionData-oriented Parsing (DOP) is an approach towide-coverage parsing based on assigning structuresto new sentences using fragments of variable sizeextracted from a treebank.
It was first proposed byScha in 1990 and formalized by Bod (1992), andpreceded many developments in statistical parsing(e.g., the ?treebank grammars?
of Charniak 1997)and linguistic theory (e.g., the current popularityof ?constructions?, Jackendoff 2002).
A rich lit-erature on DOP has emerged since, yielding state-of-the-art results on the Penn treebank benchmarktest (Bod, 2001; Bansal and Klein, 2010) and in-spiring developments in related frameworks includ-ing tree kernels (Collins and Duffy, 2002), reranking(Charniak and Johnson, 2005) and Bayesian adaptorand fragment grammars (e.g., Johnson et al, 2007;O?Donnell et al, 2009; Cohn et al, 2010).
By for-malizing the idea of using large fragments of earlierlanguage experience to analyze new sentences, DOPcaptures an important property of language cogni-tion that has shaped natural language.
It thereforecomplements approaches that have focused on prop-erties like lexicalization or incrementality, and mightbring supplementary strengths in other NLP tasks.Early versions of DOP (e.g., Bod et al, 2003)aimed at extracting all subtrees of all trees in thetreebank.
The total number of constructions, how-ever, is prohibitively large for non-trivial treebanks:it grows exponentially with the length of the sen-tences, yielding the astronomically large number ofapproximately 1048 for section 2-21 of the PennWSJ corpus.
These models thus rely on a big sampleof fragments, which inevitably includes a substan-tial portion of overspecialized constructions.
LaterDOP models have used the Goodman transforma-tion (Goodman, 1996, 2003) to obtain a compactrepresentation of all fragments in the treebank (Bod,2003; Bansal and Klein, 2010).
In this case thegrammatical constructions are no longer explicitlyrepresented, and substantial engineering effort isneeded to optimally tune the models and make themefficient.In this paper we present a novel DOP model(Double-DOP) in which we extract a restricted yetrepresentative subset of fragments: those recurringat least twice in the treebank.
The explicit represen-tation of the fragments allows us to derive simple84ways of estimating probabilistic models on top of thesymbolic grammar.
This and other implementationchoices aim at making the methodology transparentand easily replicable.
The accuracy of Double-DOPis well within the range of state-of-the-art parserscurrently used in other NLP-tasks, while offering theadditional benefits of a simple generative probabilitymodel and an explicit representation of grammaticalconstructions.The contributions of this paper are summarized asfollows: (i) we describe an efficient tree-kernel algo-rithm which allows us to extract all recurring frag-ments, reducing the set of potential elementary unitsfrom the astronomical 1048 to around 106.
(ii) Weimplement and compare different DOP estimationtechniques to induce a probability model (PTSG)on top of the extracted symbolic grammar.
(iii)We present a simple transformation of the extractedfragments into CFG-rules that allows us to use off-the-shelf PCFG parsing and inference.
(iv) We in-tegrate Double-DOP with recent state-splitting ap-proaches (Petrov et al, 2006), yielding an even moreaccurate parser and a better understanding of the re-lation between DOP and state-splitting.The rest of the paper is structured as follows.
Insection 2 we describe the symbolic backbone of thegrammar formalism that we will use for parsing.In section 3 we illustrate the probabilistic exten-sion of the grammar, including our transformationof PTSGs to PCFGs that allows us to use a standardPCFG parser, and a different transform that allowsus to use a standard implementation of the inside-outside algorithm.
In section 4 we present the ex-perimental setup and the results.2 The symbolic backboneThe basic idea behind DOP is to allow arbitrarilylarge fragments from a treebank to be the elemen-tary units of production of the grammar.
Fragmentscan be combined through substitution to obtain thephrase-structure tree of a new sentence.
Figure 1shows an example of a complete syntactic tree ob-tained by combining three elementary fragments.
Asin previous work, two fragments fi and fj can becombined (fi ?
fj) only if the leftmost substitutionsite X?
in fi has the same label as the root node offj ; in this case the resulting tree will correspond tofi with fj replacing X .
The DOP formalism is dis-cussed in detail in e.g., Bod et al (2003).SNP?
VPVBDworeNP?
?NPDTTheNNPFreeNNPFrench?NPJJblackNNarmNNSbands?SNPDTTheNNPFreeNNPFrenchVPVBDworeNPJJblackNNarmNNSbandsFigure 1: An example of a derivation of a complete syn-tactic structure (below) obtained combining three ele-mentary fragments (above) by means of the substitutionoperation ?.
Substitution sites are marked with ?.2.1 Finding Recurring FragmentsThe first step to build a DOP model is to define itssymbolic grammar, i.e.
the set of elementary frag-ments in the model.
In the current work we explic-itly extract a subset of fragments from the trainingtreebank.
To limit the fragment set size, we use asimple but heretofore unexplored constraint: we ex-tract only those fragments that occur two or moretimes in the treebank1.
Extracting this particularset of fragments is not trivial, though: a naive ap-proach that filters a complete table of fragments to-gether with their frequencies fails because that set, ina reasonably sized treebank, is astronomically large.Instead, we use a dynamic programming algorithmbased on tree-kernel techniques (Collins and Duffy,2001; Moschitti, 2006; Sangati et al, 2010).The algorithm iterates over every pair of trees in1More precisely we extract only the largest shared fragmentsfor all pairs of trees in the treebank.
All subtrees of these ex-tracted fragments necessarily also occur at least twice, but theyare only explicitly represented in our extracted set if they hap-pen to form a largest shared fragment from another pair of trees.Hence, if a large tree occurs twice in the treebank the algorithmwill extract from this pair only the full tree as a fragment andnot all its (exponentially many) subtrees.85SNPPRPIVPVBPsaySBARSNPPRPtheyVPVBPareADJPJJready..SNPNNSAnalystsVPVBPsaySBARSNPNNPUSAirVPVBZhasNPJJgreatNNpromise..S NP PRP VP VBP SBAR S NP PRP VP VBP ADJP JJ .S ?
?NP ?
?NNSVP ?
?VBP ?
?SBAR ?S ?
?NP ?
?NNPVP ?VBZNPJJ ?NN.
??
?Figure 2: Left: example of two trees sharing a single maximum fragment, circled in the two trees.
Right: the chartM which is used in the dynamic algorithm to extract all maximum fragments shared between the two trees.
Thehighlighted cells in the chart are the ones which contribute to extract the shared fragment.
The marked cells are thosefor which the corresponding nodes in the two tree have equivalent labels but differ in their lists of child nodes.the treebank to look for common fragments.
Fig-ure 2 shows an example of a pair of trees ??,??
be-ing compared.
The algorithm builds a chartM withone column for every indexed non-terminal node ?iin ?, and one row for every indexed non-terminalnode ?j in ?.
Each cellM?i, j?
identifies a set of in-dices corresponding to the largest fragment in com-mon between the two trees starting from ?i and ?j .This set is empty if ?i and ?j differ in their labels,or they don?t have the same list of child nodes.
Oth-erwise (if both the labels and the lists of childrenmatch) the set is computed recursively as follows:M?i, j?
= {?i} ???
?c={1,2,...,|ch(?
)|}M?ch(?i, c), ch(?j , c)???
(1)where ch(?)
returns the indices of ?
?s children, andch(?, c) the index of its cth child.After filling the chart, the algorithm extracts theset of recurring fragments, and stores them in a ta-ble to keep track of their counts.
This is done byconverting back each fragment implicitly defined inevery cell-set2, and filtering out those that are prop-erly contained in others.In a second pass over the treebank, exact countsare obtained for each fragment in the extracted set.2A cell-set containing a single index corresponds to the frag-ment including the node with that index together with all itschildren.Parse trees in the training corpus are not necessarilycovered entirely by recurring fragments; to ensurecoverage, we also include in the symbolic backboneof our Double-DOP model all PCFG-productionsnot included in the set of extracted fragments.2.2 Comparison with previous DOP workExplicit grammars The number of recurring frag-ments in our symbolic grammar, extracted fromthe training sections of the Penn WSJ treebank3, isaround 1 million, and thus is significantly lower thanprevious work extracting explicit fragments (e.g.,Bod, 2001, used more than 5 million fragments upto depth 14).When looking at the extracted fragments we askif we could have predicted which fragments occurtwice or more.
Figure 3 attempts to tackle this ques-tion by reporting some statistics on the extractedfragments.
The majority of fragments are rathersmall with a limited number of words or substitutionsites in the frontier.
Yet, there is a significant por-tion of fragments, in the tail of the distribution, withmore than 10 words or substitution sites.
Since thespace of all fragments with such characteristics isenormously large, selecting big recurring fragmentsusing random sampling technique is like finding aneedle in a haystack.
Hence, random sampling pro-cesses (like Bod, 2001), will tend to represent fre-3This is after the treebank has been preprocessed.
See alsosection 4.SNPPRPIVPVBPsaySBARSNPPRPtheyVPVBPareADJPJJready..SNPNNSAnalystsVPVBPsaySBARSNPNNPUSAirVPVBZhasNPJJgreatNNpromise..S NP PRP VP VBP SBAR S NP PRP VP VBP ADJP JJ .S ?
?NP ?
?NNSVP ?
?VBP ?
?SBAR ?S ?
?NP ?
?NNPVP ?VBZNPJJ ?NN.
??
?Figure 2: Left: example of two trees sharing a single maximum fragment, circled in the two trees.
Right: the chartM which is used in the dynamic algorithm to extract all maximum fragments shared between the two trees.
Thehighlighted cells in the chart are the ones which contribute to extract the shared fragment.
The marked cells are thosefor which the corresponding nodes in the two tree have equivalent labels but differ in their lists of child nodes.the treebank to look for common fragments.
Fig-ure 2 shows an example of a pair of trees ??,??
be-ing compared.
The algorithm builds a chartM withone column for every indexed non-terminal node ?iin ?, and one row for every indexed non-terminalnode ?j in ?.
Each cellM?i, j?
identifies a set of in-dices corresponding to the largest fragment in com-mon between the two trees starting from ?i and ?j .This set is empty if ?i and ?j differ in their labels,or they don?t have the same list of child nodes.
Oth-erwise (if both the labels and the lists of childrenmatch) the set is computed recursively as follows:M?i, j?
= {?i} ???
?c={1,2,...,|ch(?
)|}M?ch(?i, c), ch(?j , c)???
(1)where ch(?)
returns the indices of ?
?s children, andch(?, c) the index of its cth child.After filling the chart, the algorithm extracts theset of recurring fragments, and stores them in a ta-ble to keep track of their counts.
This is done byconverting back each fragment implicitly defined inevery cell-set2, and filtering out those that are prop-erly contained in others.In a second pass over the treebank, exact countsare obtained for each fragment in the extracted set.2A cell-set containing a single index corresponds to the frag-ment including the node with that index together with all itschildren.Parse trees in the training corpus are not necessarilycovered entirely by recurring fragments; to ensurecoverage, we also include in the symbolic backboneof our Double-DOP model all PCFG-productionsnot included in the set of extracted fragments.2.2 Comparison with previous DOP workExplicit grammars The number of recurring frag-ments in our symbolic grammar, extracted fromthe training sections of the Penn WSJ treebank3, isaround 1 million, and thus is significantly lower thanprevious work extracting explicit fragments (e.g.,Bod, 2001, used more than 5 million fragments upto depth 14).When looking at the extracted fragments we askif we could have predicted which fragments occurtwice or more.
Figure 3 attempts to tackle this ques-tion by reporting some statistics on the extractedfragments.
The majority of fragments are rathersmall with a limited number of words or substitutionsites in the frontier.
Yet, there is a significant por-tion of fragments, in the tail of the distribution, withmore than 10 words or substitution sites.
Since thespace of all fragments with such characteristics isenormously large, selecting big recurring fragmentsusing rando sa pling technique is like finding aneedle in a haystack.
Hence, rando sa pling pro-cesses (like Bod, 2001), ill tend to represent fre-3This is after the treebank has been preprocessed.
See alsosection 4.S NP PRP VP VBP SBAR S NP PRP VP VBP ADJP JJ .S ?
?NP ?
?NNSVP ?
?VBP ?
?SBAR ?S ?
?NP ?
?NNPVP ?VBZNPJJ ?NN.
?
?Figure 2: Left: t trees sharing a single maximum fragment, ircled in the two trees.
Rig t: t e chartM which is us i t ic algorithm to extract all maximum fragments shared b tween the two trees.
Thehighlighted cells in the chart are the ones which contribute to extract the shared fragment.
The marked cells are thosefor which the corresponding nodes in the two tree have equivalent labels but differ in their lists of child nodes.the treebank to look for common fragments.
Fig-ure 2 shows an example of a pair of trees ?
?, ??
be-i g c mpared.
The algorithm builds a chartM withone column fo every ind xed non-terminal node ?iin ?, and one row for every ndexed non-terminalnode ?j in ?.
Each cellM?i, j?
identifies a set of in-dices corresponding to the l gest agment in com-mon between the two trees starting from ?i and ?j .This set is empty if ?i and ?j differ i th ir labels,or they don?t have the same list of child nodes.
Oth-erwise (if both the labels and the lists of childrenmatch) the set is computed recursively as follows:M?i, j?
= {?i} ???
?c={1,2,...,|ch(?
)|}M?ch(?i, c), ch(?j , c)???
(1)where ch(?)
returns the indices of ?
?s children, andch(?, c) the index of its c child.After filling the chart, the algorithm extracts theset of recurring fragments, and stores them in a ta-ble to keep track of their counts.
This is done byconverting back each fragment implicitly defined inevery cell-set2, and filtering out those that are prop-erly contained in others.In a second pass over the treebank, exact countsare obtained for each fragment in the extracted set.2A cell-set containing a single index corresponds to the frag-ment including the node with that index together with all itschildren.Parse trees in the training corpus are not necessarilycovered entirely by recurring fragments; to ensurec verage, we also include in the symb lic backboneof o r Double-DOP model all PCFG-productionsnot included in the set of extracted fragments.2.2 Comparison with previous DOP workExplicit g am ars The numb r of recurring frag-ments in our symbolic grammar, extracted fromthe training sections of the Penn WSJ treebank3, isaround 1 million, and thus is significantly lower thanprevious work extracti g explicit fragments (e.g.,Bod, 2001, used more than 5 million fragments upto d pth 14).When looking at th extra ted fragments we askif we could have predicted which fragments occurtwice or more.
Figure 3 attempts to tackle this ques-tion by reporting some statistics on the extractedfragments.
The majority of fragments are rathersmall with a limited number of words or substitutionsites in the frontier.
Yet, there is a significant por-tion of fragments, in the tail of the distribution, withmore than 10 words or substitution sites.
Since thespace of all fragments with such characteristics isenormously large, selecting big recurring fragmentsusing random sampling technique is like finding aneedle in a haystack.
Hence, random sampling pro-cesses (like Bod, 2001), will tend to represent fre-3This is after the treebank has been preprocessed.
See alsosection 4.86quent recurring constructions such as from NP toNP or whether S or not, together with infrequentoverspecialized fragments like from Houston to NP,while missing large generic constructions such aseverything you always wanted to know about NP butwere afraid to ask.
These large constructions areexcluded completely by models that only allow ele-mentary trees up to a certain depth (typically 4 or 5)into the symbolic grammar (Zollmann and Sima?an,2005; Zuidema, 2007; Borensztajn et al, 2009), oronly elementary trees with exactly one lexical an-chor (Sangati and Zuidema, 2009).100101102103104105106  01020304050Number of FragmentsDepth/ Words / SubstitutionSitesDepth WordsSubstitution SitesFigure 3: Distribution of the recurring fragments typesaccording to several features: depth, number of words,and number of substitution sites.
Their correspondingcurves peak at 4 (depth), 1 (words), and 4 (substitutionsites).Implicit grammars Goodman (1996, 2003) de-fined a transformation for some versions of DOP toan equivalent PCFG-based model, with the numberof rules extracted from each parse tree linear in thesize of the trees.
This transform, representing largerfragments only implicitly, is used in most recentDOP parsers (e.g., Bod, 2003; Bansal and Klein,2010).
Bod has promoted the Goodman transform asthe solution to the computational challenges of DOP(e.g., Bod, 2003); it?s important to realize, how-ever, that the resulting grammars are still very large:WSJ sections 2-21 yield about 2.5 million rules inthe basic version of Goodman?s transform.
More-over, the transformed grammars differ from untrans-formed DOP grammars in that larger fragments areno longer explicitly represented.
Rather, informa-tion about their frequency is distributed over manyCFG-rules: if a construction occurs n times and con-tains m context-free productions, Goodman?s trans-form uses the weights of 7nm +m rules to encodethis fact.
Thus, the information that the idiomaticfragment (PP (IN ?out?)
(PP (IN ?of?)
(NP (NN?town?)))))
occurs 3 times in WSJ sections 2-21, isdistributed over 132 rules.
This way, an attractivefeature of DOP, viz.
the explicit representation ofthe ?productive units?
of language, is lost4.In addition, grammars that implicitly encode allfragments found in a treebank are strongly biased toover-represent big constructions: the great majorityof the entire set of fragments belongs in fact to thelargest tree in the treebank5.
DOP models relying onGoodman?s transform, need therefore to counteractthis tendency.
Bansal and Klein (2010), for instance,rely on a sophisticated tuning technique to correctlyadjust the weights of the rules in the grammar.
Inour Double-DOP approach, instead, the number offragments extracted from each tree varies much less(it ranges between 4 and 1,759).
This comparison isshown in figure 4.3 The probabilistic modelLike CFG grammars, our symbolic model producesextremely many parse trees for a given test sentence.We therefore need to disambiguate between the pos-sible parses by means of a probability model that as-signs probabilities to fragments, and defines a properdistribution over the set of possible full parse trees.For every nonterminal X in the treebank we have:?f?FXp(f) = 1 (2)where FX is the set of fragments in our sym-bolic grammar rooted in X .
A derivation d =f1, f2, .
.
.
, fn of t is a sequence of the fragments thatthrough left-most substitution produces t. The prob-ability of a derivation is computed as the product of4Bansal and Klein (2010) address this issue for contigu-ous constructions by extending the Goodman transform witha ?Packed Graph Encoding?
for fragments that ?bottom out interminals?.
However, constructions with variable slots, such aswhether S or not, are left unchanged.5In fact, the number of extracted fragments increase expo-nentially with the size of the tree.8751010210310510101020105001?1042?1043?1044?104Number of fragmentsRankof tree fromtrainsetRecurringfragmentsAll fragmentsFigure 4: Number of fragments extracted from each treein sections 2-21 of the WSJ treebank, when consideringall-fragments (dotted line) and recurring-fragments (solidline).
Trees on the x-axis are ranked according to thenumber of fragments.
Note the double logarithmic scaleon the y-axis.the probability of each of its fragments.P (d) =?f?dp(f) (3)In section 3.2 we describe ways of obtaining dif-ferent probability distributions over the fragments inour grammar.
In the following section we assume agiven probabilistic model, and illustrate how to usestandard PCFG parsing.3.1 ParsingIt is possible to define a simple transform of ourprobabilistic fragment grammar, such that off-the-shelf parsers can be used.
In order to performthe PTSG/PCFG conversion, every fragment in ourgrammar must be mapped to a CFG rule which willkeep the same probability as the original fragment.The corresponding rule will have as the left handside the root of the fragment and as the right handside its yield, i.e., a sequence of terminals and non-terminals (substitution sites).It might occur that several fragments are mappedto the same CFG rule6.
These are interesting casesof syntactic ambiguity as shown in figure 5.
In orderto resolve this problem we need to map each am-biguous fragment to two unique CFG rules chained6In our binarized treebank we have 31,465 fragments typesthat are ambiguous in this sense.by a unique artificial node, as shown at the bottomof the same figure.
To the first CFG rule in the chainwe assign the probability of the fragment, while thesecond will receive probability 1, so the productgives back the original probability.
The ambiguousand unambiguous PTSG/PCFG mappings need to bestored in a table, in order to convert back the com-pressed CFG derivations to the original PTSG modelafter parsing.Such a transformed PCFG will generate the samederivations as the original PTSG grammar with iden-tical probabilities.
In our experiment we use a stan-dard PCFG parser to produce a list of k-best Viterbiderivations.
These, in turn, will be used to maximizepossible objectives as described in section 3.3.VPVBD NPNPDT NNPPIN?with?NPVPVBD NPDT NNPPIN?with?NPm mVPNODE@7276VPNODE@7277NODE@7276VBD DT NN ?with?
NPNODE@7277VBD DT NN ?with?
NPFigure 5: Above: example of 2 ambiguous fragmentsmapping to the same CFG rule VP ?
VBD DT NN?with?
NP.
The first fragment occurs 5 times in the train-ing treebank, (e.g.
in the sentence was an executive witha manufacturing concern) while the second fragment oc-curs 4 times (e.g.
in the sentence began this campaignwith such high hopes).
Below: the two pairs of CFG rulesthat are used to map the two fragments to separate CFGderivations.3.2 Inducing probability distributionsRelative Frequency Estimate (RFE) The sim-plest way to assign probabilities to fragments is tomake them proportional to their counts7 in the train-ing set.
When enforcing equation 2, that gives the7We refer to the counts of each fragment as returned by ourextraction algorithm in section 2.1.88Relative Frequency Estimate (RFE):pRFE(f) =count(f)?f ?
?Froot(f) count(f ?
)(4)Unlike RFE for PCFGs, however, the RFE forPTSGs has no clear probabilistic interpretation.
Inparticular, it does not yield the maximum likelihoodsolution, and when used as an estimator for an all-fragments grammar, it is strongly biased since it as-signs the great majority of the probability mass tobig fragments (Johnson, 2002).
As illustrated in fig-ure 4 this bias is much weaker when restricting theset of fragments with our approach.
Although thisdoes not solve all theoretical issues, it makes RFE areasonable first choice again.Equal Weights Estimate (EWE) Various otherways of choosing the weights of a DOP grammarhave been worked out.
The best empirical resultshave been reported by Bod (2003) with the EWEproposed by Goodman (2003).
Goodman defined itfor grammars in the Goodman transform, but for ex-plicit grammars it becomes:wEWE(f) =?t?TBcount(f, t)|{f ?
?
t}| (5)pEWE(f) =wEWE(f)?f ?
?Froot(f) wEWE(f ?
)(6)where the first sum is over all parse trees t in the tree-bank (TB), count(f, t) gives the number of timesfragment f occurs in t, and |{f ?
?
t}| is the totalnumber of subtrees of t that were included in thesymbolic grammar.Maximum Likelihood (ML) For reestimation,we can aim at maximizing the likelihood (ML) ofthe treebank.
For this, it turns out that we can de-fine another transformation of our PTSG, such thatwe can apply standard Inside-Outside algorithm forPCFGs (Lari and Young, 1990).
The original ver-sion of IO is defined over string rewriting PCFGs,and maximizes the likelihood of the training set con-sisting of plain sentences.
Reestimation shifts prob-ability mass between alternative parse trees for asentence.
In contrast, our grammars consist of frag-ments of various size, and our training set consistsof parse trees.
Reestimation here shifts probabilitymass between alternative derivations for a parse tree.Our transformation approach is illustrated with anexample in figure 6.
In step (b) the fragments inthe grammar as well as the original parse trees inthe treebank are ?flattened?
into bracket notation.
Instep (c) each fragment is transformed into a CFGrule in the transformed meta-grammar, whose right-hand side is constituted by the bracket notation ofthe fragment.
Each substitution site X?
is raised toa meta-nonterminal X ?, and all other symbols, in-cluding parentheses, become meta-terminals.
Theleft-hand side of the rule is constituted by the origi-nal root symbol R of the fragment raised to a meta-nonterminal R?.The resulting PCFG generates trees in bracket no-tation, and we can run an of-the-shelf inside-outsidealgorithm by presenting it parse trees from the traincorpus in bracket notation8.
In the experiments thatwe report below we used the RFE from section 3, togenerate the initial weights for the grammar.(a)SA?
By?Ax =SAxBy(b) ( S A?
( B y ) ) ?
( A x ) = ( S ( A x ) ( B y ) )(c) S??
( S A?
( B y ) ) ?
A??
( A x ) =S?
( S A?
( A x )( B y ) )(d) ( S ( A x ) ( B y ) )Figure 6: Rule and tree transforms that turn PTSG rees-timation into PCFG reestimation; (a) a derivation of thesentence x y through successive substitutions of elemen-tary trees from a PTSG; (b) the same elementary treesand resulting parse tree in bracket notation; (c) an equiva-lent derivation with the meta-grammar, where the originalsubstitution sites reappear as meta-nonterminals (markedwith a prime) and all other symbols as meta-terminals;(d) the yield of the derivation in c.8However, the results with inside-outside reported in this pa-per were obtained with an earlier version of our code that usesan equivalent but special-purpose implementation.893.3 Maximizing ObjectivesMPD The easiest objective in parsing, is to se-lect the most probable derivation (MPD), obtainedby maximizing equation 3.MPP A DOP grammar can often generate thesame parse tree t through different derivationsD(t) = d1, d2, .
.
.
dm.
The probability of t is there-fore obtained by summing the probabilities of all itspossible derivations.P (t) =?d?D(t)p(d) =?d?D(t)?f?dp(f) (7)An intuitive objective for a parser is to select, fora given sentence, the parse tree with highest proba-bility according to equation 7, i.e., the most probableparse (MPP): unfortunately, identifying the MPP iscomputationally intractable (Sima?an, 1996).
How-ever, we can approximate the MPP by deriving a listof k-best derivations, summing up the probabilitiesof those resulting in the same parse tree, and selectthe tree with maximum probability.MCP, MRS Following Goodman (1998), Sima?an(1999, 2003), and others, we also consider otherobjectives, in particular, the max constituent parse(MCP), and the max rule sum (MRS).MCP maximizes a weighted average of the ex-pected labeled recall L/NC and (approximated) la-beled precision L/NG under the given posterior dis-tribution, where L is the number of correctly labeledconstituents, NC the number of constituents in thecorrect tree, and NG the number of constituents inthe guessed tree.
Recall is easy to maximize sincethe estimated NC is constant.
L/NC can be in factmaximized in:t?
= argmaxt?lc?tP (lc) (8)where lc ranges over all labeled constituents in tand P (lc) is the marginalized probability of all thederivation trees in the grammar yielding the sentenceunder consideration which contains lc.Precision, instead, is harder because the denom-inator NG depends on the chosen guessed tree.Goodman (1998) proposes to look at another metricwhich is strongly correlated with precision, which isthe mistake rate (NG?L)/NC that we want to min-imize.
We combine recall with mistake rate throughlinear interpolation:t?
= argmaxtE( LNC?
?NG ?
LNC) (9)= argmaxt?lc?tP (lc)?
?(1?
P (lc)) (10)where 10 is obtained from 9 assuming NC constant,and the optimal level for ?
has to be evaluated em-pirically.Unlike MPP, the MCP can be calculated effi-ciently using dynamic programming techniques overthe parse forest.
However, in line with the aims ofthis paper to produce an easily reproducible imple-mentation of DOP, we developed an accurate ap-proximation of the MCP using a list of k-best deriva-tions, such as those that can be obtained with an off-the-shelf PCFG parser.We do so by building a standard CYK chart,where every cell corresponds to a specific span inthe test sentence.
We store in each cell the proba-bility of seeing every label in the grammar yieldingthe corresponding span, by marginalizing the prob-abilities of all the parse trees in the obtained k-bestderivations that contains that label covering the samespan.
We then compute the Viterbi-best parse maxi-mizing equation 10.We implement max rule sum (MRS) in a similarway, but do not only keep track of labels in everycell, but of each CFG rule that span the specific yield(see also Sima?an, 1999, 2003).
We haven?t im-plemented the max rule product (MRP) where pos-teriors are multiplied instead of added (Petrov andKlein, 2007; Bansal and Klein, 2010).4 Experimental SetupIn order to build and test our Double-DOP model9,we employ the Penn WSJ Treebank (Marcus et al,1993).
We use sections 2-21 for training, section 24for development and section 23 for testing.Treebank binarization We start with some pre-processing of the treebank, following standard prac-9The software produced for running our model is publiclyavailable and included in the supplementary material to this pa-per.
To the best of our knowledge this is the first DOP softwarereleased that can be used to parse the WSJ PTB.90SNP|SNP|S@NNP|NPDT|NPTheNNP|NPFreeNNP|NPFrenchVP|SVBD|VPworeNP|VPNP|VP@NN|NPJJ|NPblackNN|NParmNNS|NPbandsFigure 7: The binarized version of the tree in figure 1,with H=1 and P=1.tice in WSJ parsing.
We remove traces and func-tional tags.
We apply a left binarization of the train-ing treebank as in Matsuzaki et al (2005) and Kleinand Manning (2003), setting the horizontal historyH=1 and the parent labeling P=1.
This means thatwhen a node has more than 2 children, the ith child(for i ?
3) is conditioned on child i ?
1.
Moreoverthe labels of all non-lexical nodes are enriched withthe labels of their parent node.
Figure 7 shows thebinarized version of the tree structure in figure 1.Unknownwords We replace words appearing lessthan 5 times in the training data by one of 50 un-known word categories based on the presence of lex-ical features as implemented in Petrov (2009).
Insome of the experiments we also perform a smooth-ing over the lexical elements assigning low counts( = 0.01) to open-class ?words, PoS-tags?
pairs notencountered in the training corpus10.Fragment extraction We extract the symbolicgrammar and fragment frequencies from this prepro-cessed treebank as explained in section 2.
This isthe the most time-consuming step (around 160 CPUhours11).In the extracted grammar we have in total1,029,342 recurring fragments and 17,768 unseenCFG rules.
We test several probability distributionsover the fragments (section 3.2) and various maxi-mization objectives (section 3.3).10A PoS-tag is an open class if it rewrites to at least 50 differ-ent words in the training corpus.
A word is an open class wordif it has been seen only with open-class PoS-tags.11Although our code could still be optimized further, it doesalready allow for running the job on M CPUs in parallel, reduc-ing the time required by a factor M (10 hours with 16-CPUs).86.086.587.087.588.000.51 1.151.52F1 / Recall / Precision (%)?Max Const.
ParseMaxRuleSumMaxProbable ParseMaxProbable DerivationPrecision (MCP)F1 score (MCP)Recall (MCP)Figure 8: Double-DOP results on the development sec-tion (?
40) with different maximizing objectives.Parsing We convert our PTSG into a PCFG (sec-tion 3.1) and use Bitpar12 for parsing.
For approx-imating MPP and other objectives we marginalizeprobabilities from the 1,000 best derivations.4.1 ResultsWe start by presenting in figure 8 the results we ob-tain on the development set (section 24).
Here wecompare the maximizing objectives presented in sec-tion 3.3, using RFE to obtain the probability distri-bution over the fragments.
We conclude that, em-pirically, MCP for ?
= 1.15, is the best choice tomaximize F1, followed by MRS, MPP, and MPD.We also compare the various estimators presentedin section 3.2, on the same development set, keep-ing MCP with ?
= 1.15 as the maximizing objec-tive.
We find that RFE is the best estimator (87.2F113) followed by EWE (86.8) and ML (86.6).
Ourbest results with ML are obtained when removingfragments occurring less than 6 times (apart fromCFG-rules) and when stopping at the second iter-ation.
This filtering is done in order to limit thenumber of big fragments in the grammar.
It is wellknown that IO for DOP tends to assign most of theprobability mass to big fragments, quickly overfit-ting the training data.
It is surprising that EWE andML perform worse than RFE, in contrast to earlierfindings (Bod, 2003).12http://www.ims.uni-stuttgart.de/tcl/SOFTWARE/BitPar.html13We computed F1 scores with EvalB (http://nlp.cs.nyu.edu/evalb/) using parameter file new.prm.91808182838485868788  110 2050100104105106107F1Number of fragmentsFragment frequency thresholdF1Double-DOP grammarsizeNumber ofPCFGrulesFigure 9: Performance (on the development set) and sizeof Double-DOP when considering only fragments whoseoccurring frequency in the training treebank is above aspecific threshold (x-axis).
In all cases, all PCFG-rulesare included in the grammars.
For instance, at the right-hand side of the plot a grammar is evaluated which in-cluded only 6754 fragments with a frequency > 100 aswell as 39227 PCFG rules.We also investigate how a further restriction onthe set of extracted fragments influences the perfor-mance of our model.
In figure 9 we illustrate theperformance of Double-DOP when restricting thegrammar to fragments having frequencies greaterthan 1, 2, .
.
.
, 100.
We can notice a rather sharpdecrease in performance as the grammar becomesmore and more compact.Next, we present some results on various Double-DOP grammars extracted from the same trainingtreebank after refining it using the Berkeley state-splitting model14 (Petrov et al, 2006; Petrov andKlein, 2007).
In total we have 6 increasingly refinedversions of the treebank, corresponding to the 6 cy-cles of the Berkeley model.
We observe in figure 10that our grammar is able to benefit from the statesplits for the first four levels of refinement, reachingthe maximum score at cycle 4, where we improveover our base model.
For the last two data points, thetreebank gets too refined, and using Double-DOPmodel on top of it, no longer improves accuracy.We have also compared our best Double-DOP14We use the Berkeley grammar labeler following the basesettings for the WSJ: trees are right-binarized, H=0, andP=0.
Berkeley parser package is available at http://code.google.com/p/berkeleyparser/74767880828486889092123456F1Berkeley grammar/treebankrefinementlevelBerkeley MRPBerkeley MPDDouble-DOPDouble-DOP LexsmoothFigure 10: Comparison on section 24 between the per-formance of Double-DOP (using RFE and MCP with?
= 1.15, H=0, P=0) and Berkeley parser on differentstages of refinement of the treebank/grammar.base model and the Berkeley parser on per-categoryperformance.
Here we observe an interesting trend:the Berkeley parser outperforms Double-DOP onvery frequent categories, while Double-DOP per-forms better on infrequent ones.
A detailed com-parison is included in table 1.Finally, in table 2 we present our results on thetest set (section 23).
Our best model (according tothe best settings on the development set) performsslightly worse than the one by Bansal and Klein(2010) when trained on the original corpus, but out-performs it (and the version of their model withadditional refinements) when trained on the refinedversion, in particular for the exact match score.5 ConclusionsWe have described Double-DOP, a novel DOP ap-proach for parsing, which uses all constructions re-curring at least twice in a treebank.
This method-ology is driven by the linguistic intuition that con-structions included in the grammar should prove tobe reusable in a representative corpus.The extracted set of fragments is significantlysmaller than in previous approaches.
Moreover con-structions are explicitly represented, which makesthem potentially good candidates as semantic ortranslation units to be used in other applications.Despite earlier reported excellent results withDOP parsers, they are almost never used in other92Category % F1 F1label in gold Berkeley Double-DOPNP 41.42 91.4 89.5VP 20.46 90.6 88.6S 13.38 90.7 87.6PP 12.82 85.5 84.1SBAR 3.47 86.0 82.1ADVP 3.36 82.4 81.0ADJP 2.32 68.0 67.3QP 0.98 82.8 84.6WHNP 0.88 94.5 92.0WHADVP 0.33 92.8 91.9PRN 0.32 83.0 77.9NX 0.29 9.50 7.70SINV 0.28 90.3 88.1SQ 0.14 82.1 79.3FRAG 0.10 26.4 34.3SBARQ 0.09 84.2 88.2X 0.06 72.0 83.3NAC 0.06 54.6 88.0WHPP 0.06 91.7 44.4CONJP 0.04 55.6 66.7LST 0.03 61.5 33.3UCP 0.03 30.8 50.0INTJ 0.02 44.4 57.1Table 1: Comparison of the performance (per-categoryF1 score) on the development set between the Berkeleyparser and the best Double-DOP model.NLP tasks: where other successful parsers often fea-ture as components of machine translation, semanticrole labeling, question-answering or speech recogni-tion systems, DOP is conspicuously absent in theseneighboring fields (but for a possible application ofclosely related formalisms see, e.g., Yamangil andShieber, 2010).
The reasons for this are many, butmost important are probably the computational inef-ficiency of many instances of the approach, the lackof downloadable software and the difficulties withreplicating some of the key results.In this paper we have addressed all three obsta-cles: our efficient algorithm for identifying the re-current fragments in a treebank runs in polynomialtime.
The transformation to PCFGs that we defineallows us to use a standard PCFG parser, while re-taining the benefit of explicitly representing largerfragments.
A different transform also allows us torun the popular inside-outside algorithm.
AlthoughIO results are slightly worse than with the naiverelative frequency estimate, it is important to es-tablish that the standard method for dealing withlatent information (i.e., the derivations of a givenparse) is not the best choice in this case.
We expectthat other re-estimation methods, for instance Vari-test (?
40) test (all)Parsing Model F1 EX F1 EXPCFG BaselinePCFG (H=1, P=1) 77.6 17.2 76.5 15.9PCFG (H=1, P=1) Lex smooth.
78.5 17.2 77.4 16.0FRAGMENT-BASED PARSERSZuidema (2007)* 83.8 26.9 - -Cohn et al (2010) MRS 85.4 27.2 84.7 25.8Post and Gildea (2009) 82.6 - - -Bansal and Klein (2010) MCP 88.5 33.0 87.6 30.8Bansal and Klein (2010) MCP 88.7 33.8 88.1 31.7+ Additional RefinementTHIS PAPERDouble-DOP 87.7 33.1 86.8 31.0Double-DOP Lex smooth.
87.9 33.7 87.0 31.5Double-DOP-Sp 88.8 35.9 88.2 33.8Double-DOP-Sp Lex smooth.
89.7 38.3 89.1 36.1REFINEMENT-BASED PARSERSCollins (1999) 88.6 - 88.2 -Petrov and Klein (2007) 90.6 39.1 90.1 37.1Table 2: Summary of the results of different parserson the test set (sec 23).
Double-DOP experiments useRFE, MCP with ?
= 1.15, H=1, P=1; those on state-splitting (Double-DOP-Sp) use Berkeley cycle 4, H=0,P=0.
Results from Petrov and Klein (2007) already in-clude smoothing which is performed similarly to oursmoothing technique (see section 4).
(* Results on a de-velopment set, with sentences up to length 20.
)ational Bayesian techniques, could be formulated inthe same manner.Finally, the availability of our programs, as wellas the third party software that we use, also ad-dresses the replicability issue.
Where some re-searchers in the field have been skeptical of the DOPapproach to parsing, we believe that our independentdevelopment of a DOP parser adds credibility to theidea that an approach that uses very many large sub-trees, can lead to very accurate parsers.AcknowledgmentsWe gratefully acknowledge funding by theNetherlands Organization for Scientific Research(NWO): FS is funded through a Vici-grant ?Inte-grating Cognition?
(277.70.006) to Rens Bod, andWZ through a Veni-grant ?Discovering Grammar?(639.021.612).
We also thank Rens Bod, GideonBorensztajn, Jos de Bruin, Andreas van Cranen-burgh, Phong Le, Remko Scha, Khalil Sima?an andthe anonymous reviewers for very useful comments.93ReferencesMohit Bansal and Dan Klein.
2010.
Simple, accu-rate parsing with an all-fragments grammar.
InProceedings of the 48th Annual Meeting of theACL, pages 1098?1107.
Association for Compu-tational Linguistics, Uppsala, Sweden.Rens Bod.
1992.
A computational model of lan-guage performance: Data oriented parsing.
InProceedings COLING?92 (Nantes, France), pages855?859.
Association for Computational Linguis-tics, Morristown, NJ.Rens Bod.
2001.
What is the minimal set of frag-ments that achieves maximal parse accuracy?
InProceedings of the ACL.
Morgan Kaufmann, SanFrancisco, CA.Rens Bod.
2003.
An efficient implementation of anew DOP model.
In Proceedings of the tenth con-ference on European chapter of the Associationfor Computational Linguistics - Volume 1, EACL?03, pages 19?26.
Association for ComputationalLinguistics, Morristown, NJ, USA.Rens Bod, Khalil Sima?an, and Remko Scha.
2003.Data-Oriented Parsing.
University of ChicagoPress, Chicago, IL, USA.Gideon Borensztajn, Willem Zuidema, and RensBod.
2009.
Children?s Grammars Grow MoreAbstract with Age?Evidence from an AutomaticProcedure for Identifying the Productive Units ofLanguage.
Topics in Cognitive Science, 1(1):175?188.Eugene Charniak.
1997.
Statistical parsing with acontext-free grammar and word statistics.
In Pro-ceedings of the Fourteenth National Conferenceon Artificial Intelligence, pages 598?603.
AAAIPress/MIT Press.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminativereranking.
In Proc.
43nd Meeting of Associationfor Computational Linguistics (ACL 2005).Trevor Cohn, Phil Blunsom, and Sharon Goldwa-ter.
2010.
Inducing tree-substitution grammars.Journal of Machine Learning Research, 11:3053?3096.Michael Collins and Nigel Duffy.
2001.
Convolu-tion Kernels for Natural Language.
In Thomas G.Dietterich, Suzanna Becker, and Zoubin Ghahra-mani, editors, NIPS, pages 625?632.
MIT Press.Michael Collins and Nigel Duffy.
2002.
New rank-ing algorithms for parsing and tagging: Kernelsover discrete structures, and the voted percep-tron.
In Proceedings of 40th Annual Meeting ofthe ACL, pages 263?270.
Association for Compu-tational Linguistics, Philadelphia, Pennsylvania,USA.Michael J. Collins.
1999.
Head-Driven StatisticalModels for Natural Language Parsing.
Ph.D. the-sis, University of Pennsylvania.Joshua Goodman.
1996.
Efficient algorithms forparsing the DOP model.
In Proceedings ofthe Conference on Empirical Methods in NaturalLanguage Processing, pages 143?152.Joshua Goodman.
2003.
Efficient parsing of DOPwith PCFG-reductions.
In Bod et al (2003).Joshua T. Goodman.
1998.
Parsing inside-out.Ph.D.
thesis, Harvard University, Cambridge,MA, USA.Ray Jackendoff.
2002.
Foundations of Language.Oxford University Press, Oxford, UK.Mark Johnson.
2002.
The dop estimation method isbiased and inconsistent.
Computational Linguis-tics, 28:71?76.Mark Johnson, Thomas L. Griffiths, and SharonGoldwater.
2007.
Adaptor grammars: A frame-work for specifying compositional nonparametricbayesian models.
In Advances in Neural Informa-tion Processing Systems, volume 16, pages 641?648.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In ACL ?03: Pro-ceedings of the 41st Annual Meeting on ACL,pages 423?430.
Association for ComputationalLinguistics, Morristown, NJ, USA.K.
Lari and S. J.
Young.
1990.
The estimationof stochastic context-free grammars using theinside-outside algorithm.
Computer Speech andLanguage, 4:35?56.Mitchell P. Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a Large An-notated Corpus of English: The Penn Treebank.Computational Linguistics, 19(2):313?330.94Takuya Matsuzaki, Yusuke Miyao, and Jun?ichiTsujii.
2005.
Probabilistic cfg with latent anno-tations.
In ACL ?05: Proceedings of the 43rdAnnual Meeting on ACL, pages 75?82.
Associa-tion for Computational Linguistics, Morristown,NJ, USA.Alessandro Moschitti.
2006.
Efficient ConvolutionKernels for Dependency and Constituent Syntac-tic Trees.
In ECML, pages 318?329.
MachineLearning: ECML 2006, 17th European Confer-ence on Machine Learning, Proceedings, Berlin,Germany.Timothy J. O?Donnell, Noah D. Goodman, andJoshua B. Tenenbaum.
2009.
Fragment Gram-mars: Exploring Computation and Reuse in Lan-guage.
Technical Report MIT-CSAIL-TR-2009-013, MIT.Slav Petrov.
2009.
Coarse-to-Fine Natural Lan-guage Processing.
Ph.D. thesis, University ofCalifornia at Bekeley, Berkeley, CA, USA.Slav Petrov, Leon Barrett, Romain Thibaux, andDan Klein.
2006.
Learning accurate, compact,and interpretable tree annotation.
In ACL-44:Proceedings of the 21st International Conferenceon Computational Linguistics and the 44th an-nual meeting of the ACL, pages 433?440.
Associ-ation for Computational Linguistics, Morristown,NJ, USA.Slav Petrov and Dan Klein.
2007.
Improved infer-ence for unlexicalized parsing.
In Human Lan-guage Technologies 2007: The Conference of theNorth American Chapter of the ACL; Proceedingsof the Main Conference, pages 404?411.
Asso-ciation for Computational Linguistics, Rochester,New York.Matt Post and Daniel Gildea.
2009.
Bayesian learn-ing of a tree substitution grammar.
In Proceed-ings of the ACL-IJCNLP 2009 Conference ShortPapers, pages 45?48.
Association for Computa-tional Linguistics, Suntec, Singapore.Federico Sangati and Willem Zuidema.
2009.
Unsu-pervised Methods for Head Assignments.
In Pro-ceedings of the 12th Conference of the EuropeanChapter of the ACL (EACL 2009), pages 701?709.
Association for Computational Linguistics,Athens, Greece.Federico Sangati, Willem Zuidema, and Rens Bod.2010.
Efficiently extract recurring tree fragmentsfrom large treebanks.
In Proceedings of theSeventh conference on International LanguageResources and Evaluation (LREC?10).
EuropeanLanguage Resources Association (ELRA), Val-letta, Malta.Remko Scha.
1990.
Taaltheorie en taaltechnolo-gie: competence en performance.
In Q.
A. M.de Kort and G. L. J. Leerdam, editors, Com-putertoepassingen in de Neerlandistiek, LVVN-jaarboek, pages 7?22.
Landelijke Vereniging vanNeerlandici, Almere.
[Language theory andlanguage technology: Competence and Perfor-mance] in Dutch.Khalil Sima?an.
1996.
Computational complexityof probabilistic disambiguation by means of tree-grammars.
In Proceedings of the 16th conferenceon Computational linguistics, pages 1175?1180.Association for Computational Linguistics, Mor-ristown, NJ, USA.Khalil Sima?an.
1999.
Learning Efficient Disam-biguation.
Ph.D. thesis, Utrecht University andUniversity of Amsterdam.Khalil Sima?an.
2003.
On maximizing metrics forsyntactic disambiguation.
In Proceedings of theInternational Workshop on Parsing Technologies(IWPT?03).Elif Yamangil and Stuart M. Shieber.
2010.Bayesian synchronous tree-substitution grammarinduction and its application to sentence compres-sion.
In Proceedings of the 48th Annual Meetingof the ACL, ACL ?10, pages 937?947.
Associa-tion for Computational Linguistics, Stroudsburg,PA, USA.Andreas Zollmann and Khalil Sima?an.
2005.A consistent and efficient estimator for data-oriented parsing.
Journal of Automata, Lan-guages and Combinatorics, 10(2/3):367?388.Willem Zuidema.
2007.
Parsimonious Data-Oriented Parsing.
In Proceedings of the 2007Joint Conference on Empirical Methods in Nat-ural Language Processing and ComputationalNatural Language Learning (EMNLP-CoNLL),pages 551?560.
Association for ComputationalLinguistics, Prague, Czech Republic.95
