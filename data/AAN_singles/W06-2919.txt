Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),pages 141?148, New York City, June 2006. c?2006 Association for Computational LinguisticsA Context Pattern Induction Method for Named Entity ExtractionPartha Pratim TalukdarCIS DepartmentUniversity of PennsylvaniaPhiladelphia, PA 19104partha@cis.upenn.eduThorsten BrantsGoogle, Inc.1600 Amphitheatre Pkwy.Mountain View, CA 94043brants@google.comMark Liberman Fernando PereiraCIS DepartmentUniversity of PennsylvaniaPhiladelphia, PA 19104{myl,pereira}@cis.upenn.eduAbstractWe present a novel context pattern in-duction method for information extrac-tion, specifically named entity extraction.Using this method, we extended severalclasses of seed entity lists into much largerhigh-precision lists.
Using token member-ship in these extended lists as additionalfeatures, we improved the accuracy of aconditional random field-based named en-tity tagger.
In contrast, features derivedfrom the seed lists decreased extractor ac-curacy.1 IntroductionPartial entity lists and massive amounts of unla-beled data are becoming available with the growthof the Web as well as the increased availability ofspecialized corpora and entity lists.
For example,the primary public resource for biomedical research,MEDLINE, contains over 13 million entries and isgrowing at an accelerating rate.
Combined withthese large corpora, the recent availability of entitylists in those domains has opened up interesting op-portunities and challenges.
Such lists are never com-plete and suffer from sampling biases, but we wouldlike to exploit them, in combination with large un-labeled corpora, to speed up the creation of infor-mation extraction systems for different domains andlanguages.
In this paper, we concentrate on explor-ing utility of such resources for named entity extrac-tion.Currently available entity lists contain a smallfraction of named entities, but there are orders ofmagnitude more present in the unlabeled data1.
Inthis paper, we test the following hypotheses:i.
Starting with a few seed entities, it is possibleto induce high-precision context patterns by ex-ploiting entity context redundancy.ii.
New entity instances of the same category canbe extracted from unlabeled data with the in-duced patterns to create high-precision exten-sions of the seed lists.iii.
Features derived from token membership in theextended lists improve the accuracy of learnednamed-entity taggers.Previous approaches to context pattern induc-tion were described by Riloff and Jones (1999),Agichtein and Gravano (2000), Thelen and Riloff(2002), Lin et al (2003), and Etzioni et al (2005),among others.
The main advance in the presentmethod is the combination of grammatical inductionand statistical techniques to create high-precisionpatterns.The paper is organized as follows.
Section 2 de-scribes our pattern induction algorithm.
Section 3shows how to extend seed sets with entities extractedby the patterns from unlabeled data.
Section 4 givesexperimental results, and Section 5 compares ourmethod with previous work.1For example, based on approximate matching, there is anoverlap of only 22 organizations between the 2403 organiza-tions present in CoNLL-2003 shared task training data and theFortune-500 list.1412 Context Pattern InductionThe overall method for inducing entity context pat-terns and extending entity lists is as follows:1.
Let E = seed set, T = text corpus.2.
Find the contexts C of entities in E in the cor-pus T (Section 2.1).3.
Select trigger words from C (Section 2.2).4.
For each trigger word, induce a pattern automa-ton (Section 2.3).5.
Use induced patterns P to extract more entitiesE?
(Section 3).6.
Rank P and E?
(Section 3.1).7.
If needed, add high scoring entities in E?
to Eand return to step 2.
Otherwise, terminate withpatterns P and extended entity list E ?
E?
asresults.2.1 Extracting ContextStarting with the seed list, we first find occurrencesof seed entities in the unlabeled data.
For each suchoccurrence, we extract a fixed number W (contextwindow size) of tokens immediately preceding andimmediately following the matched entity.
As weare only interested in modeling the context here, wereplace all entity tokens by the single token -ENT-.This token now represents a slot in which an entitycan occur.
Examples of extracted entity contexts areshown in Table 1.
In the work presented in this pa-pers, seeds are entity instances (e.g.
Google is a seedfor organization category).increased expression of -ENT- in vad micethe expression of -ENT- mrna was greaterexpression of the -ENT- gene in mouseTable 1: Extracted contexts of known genes withW = 3.The set of extracted contexts is denoted by C .
Thenext step is to automatically induce high-precisionpatterns containing the token -ENT- from such ex-tracted contexts.2.2 Trigger Word SelectionTo induce patterns, we need to determine their starts.It is reasonable to assume that some tokens are morespecific to particular entity classes than others.
Forexample, in the examples shown above, expressioncan be one such word for gene names.
Wheneverone comes across such a token in text, the proba-bility of finding an entity (of the corresponding en-tity class) in its vicinity is high.
We call such start-ing tokens trigger words.
Trigger words mark thebeginning of a pattern.
It is important to note thatsimply selecting the first token of extracted contextsmay not be a good way to select trigger words.
Insuch a scheme, we would have to vary W to searchfor useful pattern starts.
Instead of that brute-forcetechnique, we propose an automatic way of select-ing trigger words.
A good set of trigger words isvery important for the quality of induced patterns.Ideally, we want a trigger word to satisfy the follow-ing:?
It is frequent in the set C of extracted contexts.?
It is specific to entities of interest and therebyto extracted contexts.We use a term-weighting method to rank candi-date trigger words from entity contexts.
IDF (In-verse Document Frequency) was used in our experi-ments but any other suitable term-weighting schememay work comparably.
The IDF weight fw for aword w occurring in a corpus is given by:fw = log( Nnw)where N is the total number of documents in thecorpus and nw is the total number of documents con-taining w. Now, for each context segment c ?
C , weselect a dominating word dc given bydc = argmaxw?c fwThere is exactly one dominating word for eachc ?
C .
All dominating words for contexts in C formmultiset M .
Let mw be the multiplicity of the dom-inating word w in M .
We sort M by decreasing mwand select the top n tokens from this list as potentialtrigger words.142Selection criteria based on dominating word fre-quency work better than criteria based on simpleterm weight because high term weight words maybe rare in the extracted contexts, but would still bemisleadingly selected for pattern induction.
This canbe avoided by using instead the frequency of domi-nating words within contexts, as we did here.2.3 Automata InductionRather than using individual contexts directly, wesummarize them into automata that contain the mostsignificant regularities of the contexts sharing agiven trigger word.
This construction allows us todetermine the relative importance of different con-text features using a variant of the forward-backwardalgorithm from HMMs.2.3.1 Initial InductionFor each trigger word, we list the contexts start-ing with the word.
For example, with ?expression?as the trigger word, the contexts in Table 1 are re-duced to those in Table 2.
Since ?expression?
is aleft-context trigger word, only one token to the rightof -ENT- is retained.
Here, the predictive contextlies to the left of the slot -ENT- and a single to-ken is retained on the right to mark the slot?s rightboundary.
To model predictive right contexts, the to-ken string can be reversed and the same techniquesas here applied on the reversed string.2expression of -ENT- inexpression of -ENT- mrnaexpression of the -ENT- geneTable 2: Context segments corresponding to triggerword ?expression?.Similar contexts are prepared for each triggerword.
The context set for each trigger word is thensummarized by a pattern automaton with transitionsthat match the trigger word and also the wildcard-ENT- .
We expect such automata to model the po-sition in context of the entity slot and help us extractmore entities of the same class with high precision.2Experiments reported in this paper use predictive left con-text only.101112ofofofthethea......aFigure 1: Fragment of a 1-reversible automatonWe use a simple form of grammar induction tolearn the pattern automata.
Grammar induction tech-niques have been previously explored for informa-tion extraction (IE) and related tasks.
For instance,Freitag (1997) used grammatical inference to im-prove precision in IE tasks.Context segments are short and typically do notinvolve recursive structures.
Therefore, we chose touse 1-reversible automata to represent sets of con-texts.
An automaton A is k-reversible iff (1) A isdeterministic and (2) Ar is deterministic with k to-kens of lookahead, where Ar is the automaton ob-tained by reversing the transitions of A. Wrapper in-duction using k-reversible grammar is discussed byChidlovskii (2000).In the 1-reversible automaton induced for eachtrigger word, all transitions labeled by a given tokengo to the same state, which is identified with thattoken.
Figure 1 shows a fragment of a 1-reversibleautomaton.
Solan et al (2005) describe a similar au-tomaton construction, but they allow multiple transi-tions between states to distinguish among sentences.Each transition e = (v,w) in a 1-reversible au-tomaton A corresponds to a bigram vw in the con-texts used to create A.
We thus assign each transitionthe probabilityP (w|v) = C(v,w)?w?C(v,w?
)where C(v,w) is the number of occurrences of thebigram vw in contexts for W .
With this construc-tion, we ensure words will be credited in proportionto their frequency in contexts.
The automaton mayovergenerate, but that potentially helps generaliza-tion.1432.3.2 PruningThe initially induced automata need to be prunedto remove transitions with weak evidence so as toincrease match precision.The simplest pruning method is to set a countthreshold c below which transitions are removed.However, this is a poor method.
Consider state 10 inthe automaton of Figure 2, with c = 20.
Transitions(10, 11) and (10, 12) will be pruned.
C(10, 12)  cbut C(10, 11) just falls short of c. However, fromthe transition counts, it looks like the sequence ?the-ENT-?
is very common.
In such a case, it is notdesirable to prune (10, 11).
Using a local thresholdmay lead to overpruning.We would like instead to keep transitions that areused in relatively many probable paths through theautomaton.
The probability of path p is P (p) =?
(v,w)?p P (w|v).
Then the posterior probability ofedge (v,w) isP (v,w) =?
(v,w)?p P (p)?p P (p),which can be efficiently computed by the forward-backward algorithm (Rabiner, 1989).
We can nowremove transitions leaving state v whose posteriorprobability is lower than pv = k(maxw P (v,w)),where 0 < k ?
1 controls the degree of pruning,with higher k forcing more pruning.
All induced andpruned automata are trimmed to remove unreachablestates.101112ofofofthethean(98)13aan... (40)... (7)(5)(80)(18)(40)(20)(20)(20)(2)-ENT-Figure 2: Automaton to be pruned at state 10.
Tran-sition counts are shown in parenthesis.3 Automata as ExtractorEach automaton induced using the method describedin Sections 2.3-2.3.2 represents high-precision pat-terns that start with a given trigger word.
By scan-ning unlabeled data using these patterns, we can ex-tract text segments which can be substituted for theslot token -ENT-.
For example, assume that the in-duced pattern is ?analyst at -ENT- and?
and thatthe scanned text is ?He is an analyst at the Univer-sity of California and ...?.
By scanning this text us-ing the pattern mentioned above, we can figure outthat the text ?the University of California?
can sub-stitute for ?-ENT-?.
This extracted segment is acandidate extracted entity.
We now need to decidewhether we should retain all tokens inside a candi-date extraction or purge some tokens, such as ?the?in the example.One way to handle this problem is to build alanguage model of content tokens and retain onlythe maximum likelihood token sequence.
However,in the current work, the following heuristic whichworked well in practice is used.
Each token in theextracted text segment is labeled either keep (K) ordroppable (D).
By default, a token is labeled K. Atoken is labeled D if it satisfies one of the droppablecriteria.
In the experiments reported in this paper,droppable criteria were whether the token is presentin a stopword list, whether it is non-capitalized, orwhether it is a number.Once tokens in a candidate extraction are labeledusing the above heuristic, the longest token sequencecorresponding to the regular expression K[D K]?K isretained and is considered a final extraction.
If thereis only one K token, that token is retained as the fi-nal extraction.
In the example above, the tokens arelabeled ?the/D University/K of/D California/K?, andthe extracted entity will be ?University of Califor-nia?.To handle run-away extractions, we can set adomain-dependent hard limit on the number of to-kens which can be matched with ?-ENT-?.
Thisstems from the intuition that useful extractions arenot very long.
For example, it is rare that a personname longer than five tokens.3.1 Ranking Patterns and EntitiesUsing the method described above, patterns andthe entities extracted by them from unlabeled dataare paired.
But both patterns and extractions varyin quality, so we need a method for ranking both.Hence, we need to rank both patterns and entities.This is difficult given that there we have no nega-144tive labeled data.
Seed entities are the only positiveinstances that are available.Related previous work tried to address this prob-lem.
Agichtein and Gravano (2000) seek to extractrelations, so their pattern evaluation strategy consid-ers one of the attributes of an extracted tuple as akey.
They judge the tuple as a positive or a negativematch for the pattern depending on whether there areother extracted values associated with the same key.Unfortunately, this method is not applicable to entityextraction.The pattern evaluation mechanism used here issimilar in spirit to those of Etzioni et al (2005) andLin et al (2003).
With seeds for multiple classesavailable, we consider seed instances of one classas negative instances for the other classes.
A pat-tern is penalized if it extracts entities which belongto the seed lists of the other classes.
Let pos(p) andneg(p) be respectively the number of distinct pos-itive and negative seeds extracted by pattern p. Incontrast to previous work mentioned above, we donot combine pos(p) and neg(p) to calculate a singleaccuracy value.
Instead, we discard all patterns pwith positive neg(p) value, as well as patterns whosetotal positive seed (distinct) extraction count is lessthan certain threshold ?pattern.
This scoring is veryconservative.
There are several motivations for sucha conservative scoring.
First, we are more interestedin precision than recall.
We believe that with mas-sive corpora, large number of entity instances canbe extracted anyway.
High accuracy extractions al-low us to reliably (without any human evaluation)use extracted entities in subsequent tasks success-fully (see Section 4.3).
Second, in the absence ofsophisticated pattern evaluation schemes (which weare investigating ?
Section 6), we feel it is best toheavily penalize any pattern that extracts even a sin-gle negative instance.Let G be the set of patterns which are retainedby the filtering scheme described above.
Also, letI(e, p) be an indicator function which takes value 1when entity e is extracted by pattern p and 0 other-wise.
The score of e, S(e), is given byS(e) = ?p?GI(e, p)This whole process can be iterated by includ-ing extracted entities whose score is greater than orequal to a certain threshold ?entity to the seed list.4 Experimental ResultsFor the experiments described below, we used 18billion tokens (31 million documents) of news dataas the source of unlabeled data.
We experimentedwith 500 and 1000 trigger words.
The results pre-sented were obtained after a single iteration of theContext Pattern Induction algorithm (Section 2).4.1 English LOC, ORG and PERFor this experiment, we used as seed sets subsets ofthe entity lists provided with CoNLL-2003 sharedtask data.3 Only multi-token entries were includedin the seed lists of respective categories (location(LOC), person (PER) & organization (ORG) in thiscase).
This was done to partially avoid incorrectcontext extraction.
For example, if the seed entity is?California?, then the same string present in ?Uni-versity of California?
can be incorrectly consideredas an instance of LOC.
A stoplist was used for drop-ping tokens from candidate extractions, as describedin Section 3.
Examples of top ranking induced pat-terns and extracted entities are shown in Table 9.Seed list sizes and experimental results are shownin Table 3.
The precision numbers shown in Table 3were obtained by manually evaluating 100 randomlyselected instances from each of the extended lists.Category SeedSizePatternsUsedExtendedSizePrecisionLOC 379 29 3001 70%ORG 1597 276 33369 85%PER 3616 265 86265 88%Table 3: Results of LOC, ORG & PER entity list ex-tension experiment with ?pattern = 10 set manually.The overlap4 between the induced ORG list andthe Fortune-500 list has 357 organization names,which is significantly higher than the seed list over-lap of 22 (see Section 1).
This shows that we havebeen able to improve coverage considerably.4.2 Watch Brand NameA total of 17 watch brand names were used asseeds.
In addition to the pattern scoring scheme3A few locally available entities in each category were alsoadded.
These seeds are available upon request from the authors.4Using same matching criteria as in Section 1.145of Section 3.1, only patterns containing sequence?watch?
were finally retained.
Entities extractedwith ?entity = 2 are shown in Table 5.
Extractionprecision is 85.7%.Corum, Longines, Lorus, Movado, Accutron, Au-demars Piguet, Cartier, Chopard, Franck Muller,IWC, Jaeger-LeCoultre, A. Lange & Sohne, PatekPhilippe, Rolex, Ulysse, Nardin, Vacheron Con-stantinTable 4: Watch brand name seeds.Rolex Fossil SwatchCartier Tag Heuer Super BowlSwiss Chanel SPOTMovado Tiffany SekondaSeiko TechnoMarine RolexesGucci Franck Muller Harry WinstonPatek Philippe Versace Hampton SpiritPiaget Raymond Weil Girard PerregauxOmega Guess Frank MuellerCitizen Croton David YurmanArmani Audemars Piguet ChopardDVD DVDs ChineseBreitling Montres Rolex ArmitronTourneau CD NFLTable 5: Extended list of watch brand names aftersingle iteration of pattern induction algorithm.This experiment is interesting for several reasons.First, it shows that the method presented in this pa-per is effective even with small number of seed in-stances.
From this we conclude that the unambigu-ous nature of seed instances is much more importantthan the size of the seed list.
Second, no negativeinformation was used during pattern ranking in thisexperiment.
This suggests that for relatively unam-biguous categories, it is possible to successfully rankpatterns using positive instances only.4.3 Extended Lists as Features in a TaggerSupervised models normally outperform unsuper-vised models in extraction tasks.
The downside ofsupervised learning is expensive training data.
Onthe other hand, massive amounts of unlabeled dataare readily available.
The goal of semi-supervisedlearning to combine the best of both worlds.
Recentresearch have shown that improvements in super-vised taggers are possible by including features de-rived from unlabeled data (Miller et al, 2004; Liang,2005; Ando and Zhang, 2005).
Similarly, automati-cally generated entity lists can be used as additionalfeatures in a supervised tagger.System F1 (Precision, Recall)Florian et al (2003),best single, no list89.94 (91.37, 88.56)Zhang and Johnson(2003), no list90.26 (91.00, 89.53)CRF baseline, no list 89.52 (90.39, 88.66)Table 6: Baseline comparison on 4 categories (LOC,ORG, PER, MISC) on Test-a dataset.For this experiment, we started with a conditionalrandom field (CRF) (Lafferty et al, 2001) taggerwith a competitive baseline (Table 6).
The base-line tagger was trained5 on the full CoNLL-2003shared task data.
We experimented with the LOC,ORG and PER lists that were automatically gener-ated in Section 4.1.
In Table 7, we show the accuracyof the tagger for the entity types for which we hadinduced lists.
The test conditions are just baselinefeatures with no list membership, baseline plus seedlist membership features, and baseline plus inducedlist membership features.
For completeness, we alsoshow in Table 8 accuracy on the full CoNLL task(four entity types) without lists, with seed list only,and with the three induced lists.
The seed lists (Sec-tion 4.1) were prepared from training data itself andhence with increasing training data size, the modeloverfitted as it became completely reliant on theseseed lists.
From Tables 7 & 8 we see that incor-poration of token membership in the extended listsas additional membership features led to improve-ments across categories and at all sizes of trainingdata.
This also shows that the extended lists are ofgood quality, since the tagger is able to extract usefulevidence from them.Relatively small sizes of training data pose inter-esting learning situation and is the case with practi-cal applications.
It is encouraging to observe that thelist features lead to significant improvements in suchcases.
Also, as can be seen from Table 7 & 8, theselists are effective even with mature taggers trainedon large amounts of labeled data.5Standard orthographic information, such as character n-grams, capitalization, tokens in immediate context, chunk tags,and POS were used as features.146Training Data Test-a Test-b(Tokens) No List Seed List Unsup.
List No List Seed List Unsup.
List9268 68.16 70.91 72.82 60.30 63.83 65.5623385 78.36 79.21 81.36 71.44 72.16 75.3246816 82.08 80.79 83.84 76.44 75.36 79.6492921 85.34 83.03 87.18 81.32 78.56 83.05203621 89.71 84.50 91.01 84.03 78.07 85.70Table 7: CRF tagger F-measure on LOC, ORG, PER extraction.Training Data Test-a Test-b(Tokens) No List Seed List Unsup.
List No List Seed List Unsup.
List9229 68.27 70.93 72.26 61.03 64.52 65.60204657 89.52 84.30 90.48 83.17 77.20 84.52Table 8: CRF tagger F-measure on LOC, ORG, PER and MISC extraction.5 Related WorkThe method presented in this paper is similar inmany respects to some of the previous work oncontext pattern induction (Riloff and Jones, 1999;Agichtein and Gravano, 2000; Lin et al, 2003; Et-zioni et al, 2005), but there are important differ-ences.
Agichtein and Gravano (2000) focus on rela-tion extraction while we are interested in entity ex-traction.
Moreover, Agichtein and Gravano (2000)depend on an entity tagger to initially tag unlabeleddata whereas we do not have such requirement.
Thepattern learning methods of Riloff and Jones (1999)and the generic extraction patterns of Etzioni et al(2005) use language-specific information (for exam-ple, chunks).
In contrast, the method presented hereis language independent.
For instance, the Englishpattern induction system presented here was appliedon German data without any change.
Also, in thecurrent method, induced automata compactly repre-sent all induced patterns.
The patterns induced byRiloff and Jones (1999) extract NPs and that deter-mines the number of tokens to include in a singleextraction.
We avoid using such language dependentchunk information as the patterns in our case includeright6 boundary tokens thus explicitly specifying theslot in which an entity can occur.
Another interest-ing deviation here from previous work on contextpattern induction is the fact that on top of extending6In case of predictive left context.seed lists at high precision, we have successfully in-cluded membership in these automatically generatedlexicons as features in a high quality named entitytagger improving its performance.6 ConclusionWe have presented a novel language-independentcontext pattern induction method.
Starting with afew seed examples, the method induces in an unsu-pervised way context patterns and extends the seedlist by extracting more instances of the same cat-egory at fairly high precision from unlabeled data.We were able to improve a CRF-based high qualitynamed entity tagger by using membership in theseautomatically generated lists as additional features.Pattern and entity ranking methods need furtherinvestigation.
Thorough comparison with previ-ously proposed methods also needs to be carried out.Also, it will be interesting to see whether the fea-tures generated in this paper complement some ofthe other methods (Miller et al, 2004; Liang, 2005;Ando and Zhang, 2005) that also generate featuresfrom unlabeled data.7 AcknowledgementsWe thank the three anonymous reviewers as well asWojciech Skut, Vrishali Wagle, Louis Monier, andPeter Norvig for valuable suggestions.
This work issupported in part by NSF grant EIA-0205448.147Induced LOC Patternstroops in -ENT-toCup qualifier against -ENT-insouthern -ENT-townwar - torn -ENT-.countries including -ENT-.Bangladesh and -ENT-,England in -ENT-inwest of -ENT-andplane crashed in -ENT-.Cup qualifier against -ENT-,Extracted LOC EntitiesUSUnited StatesJapanSouth AfricaChinaPakistanFranceMexicoIsraelPacificInduced PER Patternscompatriot -ENT-.compatriot -ENT-inRep.
-ENT-,Actor -ENT-isSir -ENT-,Actor -ENT-,Tiger Woods , -ENT-andmovie starring -ENT-.compatriot -ENT-andmovie starring -ENT-andExtracted PER EntitiesTiger WoodsAndre AgassiLleyton HewittErnie ElsSerena WilliamsAndy RoddickRetief GoosenVijay SinghJennifer CapriatiRoger FedererInduced ORG Patternsanalyst at -ENT-.companies such as -ENT-.analyst with -ENT-inseries against the -ENT-tonightToday ?s Schaeffer ?s Option Activity Watch features -ENT-(Cardinals and -ENT-,sweep of the -ENT-withjoint venture with -ENT-(rivals -ENT-Inc.Friday night ?s game against -ENT-.Extracted ORG EntitiesBoston Red SoxSt.
Louis CardinalsChicago CubsFlorida MarlinsMontreal ExposSan Francisco GiantsRed SoxCleveland IndiansChicago White SoxAtlanta BravesTable 9: Top ranking LOC, PER, ORG induced pattern and extracted entity examples.ReferencesEugene Agichtein and Luis Gravano.
2000.
Snowball:Extracting relations from large plain-text collections.In Proceedings of the Fifth ACM International Con-ference on Digital Libraries.Rie Ando and Tong Zhang.
2005.
A high-performancesemi-supervised learning method for text chunking.
InProceedings of ACL-2005.
Ann Arbor, USA.Boris Chidlovskii.
2000.
Wrapper generation by k-reversible grammar induction.
ECAI Workshop onMachine Learning for Information Extraction.Oren Etzioni, Michael Cafarella, Doug Downey, Ana-Maria Popescu, Tal Shaked, Stephen Soderland,Daniel S. Weld, and Alexander Yates.
2005.
Unsuper-vised named-entity extraction from the web - an exper-imental study.
Artificial Intelligence Journal.Radu Florian, Abe Ittycheriah, Hongyan Jing, and TongZhang.
2003.
Named entity recognition through clas-sifier combination.
In Proceedings of CoNLL-2003.Dayne Freitag.
1997.
Using grammatical inference toimprove precision in information extraction.
In ICML-97 Workshop on Automata Induction, Grammatical In-ference, and Language Acquisition, Nashville.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: Probabilistic modelsfor segmenting and labeling sequence data.
In Proc.ICML 2001.Percy Liang.
2005.
Semi-supervised learning for naturallanguage.
MEng.
Thesis, MIT.Winston Lin, Roman Yangarber, and Ralph Grishman.2003.
Bootstrapped learning of semantic classes frompositive and negative examples.
In Proceedings ofICML-2003 Workshop on The Continuum from La-beled to Unlabeled Data.Scott Miller, Jethran Guinness, and Alex Zamanian.2004.
Name tagging with word clusters and discrimi-native training.
In Proceedings of HLT-NAACL 2004.L.
R. Rabiner.
1989.
A tutorial on hidden markov mod-els and selected applications in speech recognition.
InProc.
of IEEE, 77, 257?286.Ellen Riloff and Rosie Jones.
1999.
Learning Dictio-naries for Information Extraction by Multi-level Boot-strapping.
In Proceedings of the Sixteenth NationalConference on Artificial Intelligence.Zach Solan, David Horn, Eytan Ruppin, and ShimonEdelman.
2005.
Unsupervised learning of natural lan-guages.
In Proceedings of National Academy of Sci-iences.
102:11629-11634.Michael Thelen and Ellen Riloff.
2002.
A bootstrappingmethod for learning semantic lexicons using extractionpattern contexts.
In Proceedings of EMNLP 2002.Tong Zhang and David Johnson.
2003.
A robust riskminimization based named entity recognition system.In Proceedings of CoNLL-2003.148
