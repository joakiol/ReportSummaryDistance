Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 9?17,Sydney, July 2006. c?2006 Association for Computational LinguisticsMulti-Domain Spoken Dialogue Systemwith Extensibility and Robustness against Speech Recognition ErrorsKazunori Komatani Naoyuki Kanda Mikio Nakano?Kazuhiro Nakadai?
Hiroshi Tsujino?
Tetsuya Ogata Hiroshi G. OkunoKyoto University, Yoshida-Hommachi, Sakyo, Kyoto 606-8501, Japan{komatani,ogata,okuno}@i.kyoto-u.ac.jp?
Honda Research Institute Japan Co., Ltd., 8-1 Honcho, Wako, Saitama 351-0188, Japan{nakano,nakadai,tsujino}@jp.honda-ri.comAbstractWe developed a multi-domain spoken dia-logue system that can handle user requestsacross multiple domains.
Such systemsneed to satisfy two requirements: extensi-bility and robustness against speech recog-nition errors.
Extensibility is required toallow for the modification and additionof domains independent of other domains.Robustness against speech recognition er-rors is required because such errors areinevitable in speech recognition.
How-ever, the systems should still behave ap-propriately, even when their inputs are er-roneous.
Our system was constructed onan extensible architecture and is equippedwith a robust and extensible domain selec-tion method.
Domain selection was basedon three choices: (I) the previous domain,(II) the domain in which the speech recog-nition result can be accepted with the high-est recognition score, and (III) other do-mains.
With the third choice we newlyintroduced, our system can prevent dia-logues from continuously being stuck inan erroneous domain.
Our experimentalresults, obtained with 10 subjects, showedthat our method reduced the domain selec-tion errors by 18.3%, compared to a con-ventional method.1 IntroductionMany spoken dialogue systems have been devel-oped for various domains, including: flight reser-vations (Levin et al, 2000; Potamianos and Kuo,2000; San-Segundo et al, 2000), train travel in-formation (Lamel et al, 1999), and bus informa-tion (Komatani et al, 2005b; Raux and Eskenazi,2004).
Since these systems only handle a sin-gle domain, users must be aware of the limita-tions of these domains, which were defined bythe system developer.
To handle various domainsthrough a single interface, we have developed amulti-domain spoken dialogue system, which iscomposed of several single-domain systems.
Thesystem can handle complicated tasks that containrequests across several domains.Multi-domain spoken dialogue systems need tosatisfy the following two requirements: (1) exten-sibility and (2) robustness against speech recog-nition errors.
Many such systems have been de-veloped on the basis of a master-slave architec-ture, which is composed of a single master moduleand several domain experts handling each domain.This architecture has the advantage that each do-main expert can be independently developed, bymodifying existing experts or adding new expertsinto the system.
In this architecture, the mastermodule needs to select a domain expert to whichresponse generation and dialogue management forthe user?s utterance are committed.
Hereafter, wewill refer to this selecting process domain selec-tion.The second requirement is robustness againstspeech recognition errors, which are inevitable insystems that use speech recognition.
Therefore,these systems must robustly select domains evenwhen the input may be incorrect due to speechrecognition errors.We present an architecture for a multi-domainspoken dialogue system that incorporates a newdomain selection method that is both extensi-ble and robust against speech recognition errors.Since our system is based on extensible architec-ture similar to that developed by O?Neill (O?Neillet al, 2004), we can add and modify the domain9Speech recognitionDomain selectionUtterance generationSpeech synthesisUserutteranceSystemresponseUser SystemCentral module Expert for domain AmethodLanguage understandingDialogue state updateDialogue managementDialogue statesvariableExpert for domain BmethodvariableExpert for domain BmethodvariableExpert for domain BmethodLanguage understandingDialogue state updateDialogue managementDialogue statesvariableFigure 1: Distributed-type architecture for multi-domain spoken dialogue systemsexperts easily.
In order to maintain robustness,domain selection takes into consideration vari-ous features concerning context and situations ofthe dialogues.
We also designed a new selectionframework that satisfies the extensibility issue byabstracting the transitions between the current andnext domains.
Specifically, our system selects thenext domain based on: (I) the previous domain,(II) the domain in which the speech recognitionresult can be accepted with the highest recognitionscore, and (III) other domains.
Conventional meth-ods cannot select the correct domain when neitherthe previous domain nor the speech recognition re-sults for a current utterance are correct.
To over-come this drawback, we defined another choice as(III) that enables the system to detect an erroneoussituation and thus prevent the dialogue from con-tinuing to be incorrect.
We modeled this frame-work as a classification problem using machinelearning, and showed it is effective by perform-ing an experimental evaluation of 2,205 utterancescollected from 10 subjects.2 Architecture used for Multi-DomainSpoken Dialogue SystemsIn multi-domain spoken dialogue systems, the sys-tem design is more complicated than in single do-main systems.
When the designed systems areclosely related to each other, a modification in acertain domain may affect the whole system.
Thistype of a design makes it difficult to modify ex-isting domains or to add new domains.
Therefore,a distributed-type architecture has been previouslyproposed (Lin et al, 2001), which enables systemdevelopers to design each domain independently.In this architecture, the system is composed oftwo kinds of components: a part that can be de-signed independently of all other domains, and apart in which relations among domains should beconsidered.
By minimizing the latter component,a system developer can design each domain semi-independently, which enables domains to be eas-ily added or modified.
Many existing systems arebased on this architecture (Lin et al, 2001; O?Neillet al, 2004; Pakucs, 2003; Nakano et al, 2005).Thus, we adopted the distributed-type architec-ture (Nakano et al, 2005).
Our system is roughlycomposed of two parts, as shown in Figure 1: sev-eral experts that control dialogues in each domain,and a central module that controls each expert.When a user speaks to the system, the central mod-ule drives a speech recognizer, and then passesthe result to each domain expert.
Each expert,which controls its own domains, executes a lan-guage understanding module, updates its dialoguestates based on the speech recognition result, andreturns the information required for domain selec-tion1.
Based on the information obtained fromthe experts, the central module selects an appro-priate domain for giving the response.
An expertthen takes charge of the selected domain and deter-mines the next dialogue act based on its dialoguestate.
The central module generates a responsebased on the dialogue act obtained from the expert,and outputs the synthesized speech to the user.Communications between the central module andeach expert are realized using method-calls in thecentral module.
Each expert is required to haveseveral methods, such as utterance understandingor response selection, to be considered an expert1Dialogue states in a domain that are not selected duringdomain selection are returned to their previous states.10in this architecture.As was previously described, the central mod-ule is not concerned with processing the speechrecognition results; instead, the central moduleleaves this task to each expert.
Therefore, it isimportant that the central module selects an ex-pert that is committed to the process of the speechrecognition result.
Furthermore, information usedduring domain selection should also be domainindependent, because this allows easier domainmodification and addition, which is, after all, themain advantage of distributed-type architecture.3 Extensible and Robust DomainSelectionDomain selection in the central module shouldalso be performedwithin an extensible framework,and also should be robust against speech recogni-tion errors.In many conventional methods, domain selec-tion is based on estimating the most likely do-mains based on the speech recognition results.Since these methods are heavily dependent onthe performance of the speech recognizers, theyare not robust because the systems will fail whena speech recognizer fails.
To behave robustlyagainst speech recognition errors, the success ofspeech recognition and of domain selection shouldbe treated separately.
Furthermore, in some con-ventional methods, accurate language models arerequired to construct the domain selection partsbefore new domains are added to a multi-domainsystem.
This means that they are not extensible.When selecting a domain, other studies haveused the information on the domain in which a pre-vious response was made.
Lin et al (2001) gavepreference to the domain selected in the previousturn by adding a certain score as an award whencomparing the N-best candidates of the speechrecognition for each domain.
Lane and Kawa-hara (2005) also assigned a similar preference inthe classification with Support Vector Machine(SVM).
A system described in (O?Neill et al,2004) does not change its domain until its sub-taskis completed, which is a constraint similar to keep-ing dialogue in one domain.
Since these methodsassume that the previous domain is most likely thecorrect domain, it is expected that these methodskeep a system in the domain despite errors dueto speech recognition problems.
Thus, should do-main selection be erroneous, the damage due to theSame domain asprevious responseDomain havingthe highest score inspeech recognizerUser utterancePrevious turn Current turn(I)(II)(III)????????????
?Other domainsexcept (I), (II)SelecteddomainFigure 2: Overview of domain selectionerror is compounded, as the system assumes thatthe previous domain is always correct.
Therefore,we solve this problem by considering features thatrepresent the confidence of the previously selecteddomain.We define domain selection as being based onthe following 3-class categorization: (I) the previ-ous domain, (II) the domain in which the speechrecognition results can be accepted with the high-est recognition score, which is different from theprevious domain, and (III) other domains.
Figure2 depicts the three choices.
This framework in-cludes the conventional methods as choices (I) and(II).
Furthermore, it considers the possibility thatthe current interpretations may be wrong, whichis represented as choice (III).
This framework alsohas extensibility for adding new domains, since ittreats domain selection not by detecting each do-main directly, but by defining only a relative re-lationship between the previous and current do-mains.Since our framework separates speech recogni-tion results and domain selection, it can keep di-alogues in the correct domain even when speechrecognition results are wrong.
This situation isrepresented as choice (I).
An example is shownin Figure 3.
Here, the user?s first utterance (U1)is about the restaurant domain.
Although the sec-ond utterance (U2) is also about the restaurant do-main, an incorrect interpretation for the restaurantdomain is obtained because the utterance containsan out-of-vocabulary word and is incorrectly rec-ognized.
Although a response for utterance U2should ideally be in the restaurant domain, the sys-tem control shifts to the temple sightseeing infor-mation domain, in which an interpretation is ob-tained based on the speech recognition result.
This11?
?U1: Tell me bars in Kawaramachi area.
(domain: restaurant)S1: Searching for bars in Kawaramachi area.30 items found.U2: I want Tamanohikari (name of liquor).
(domain: restaurant)Tamanohikari is out-of-vocabulary word, andmisrecognized as Tamba-bashi (name of place).
(domain: temple)S2 (bad): Searching spots near Tamba-bashi.
10 itemsfound.
(domain: temple)S2 (good): I do not understand what you said.
Do youhave any other preferences?
(domain: restaurant)?
?Figure 3: Example in which choice (I) is appropri-ate in spite of speech recognition erroris shown as utterance S2 (bad).
In such cases, ourframework is capable of behaving appropriately.This is shown as S2 (good), which is made byselecting choice (I).
Accepting erroneous recogni-tion results is more harmful than rejecting correctones for the following reasons: 1) a user needs tosolve the misunderstanding as a result of the falseacceptance, and 2) an erroneous utterance affectsthe interpretation of the utterances following it.Furthermore, we define choice (III), which de-tects the cases where normal dialogue manage-ment is not suitable, in which case the centralmodule selects an expert based on either the pre-vious domain or the domain based on the speechrecognition results.
The situation corresponds toa succession of recognition errors.
However, thisproblem is more difficult to solve than merely de-tecting a simple succession of the errors becausethe system needs to distinguish between speechrecognition errors and domain selection errors inorder to generate appropriate next utterances.
Fig-ure 4 shows an example of such a situation.
Here,the user?s utterances U1 and U2 are about the tem-ple domain, but a speech recognition error oc-curred in U2, and system control shifts to the hoteldomain.
The user again says (U3), but this resultsin the same recognition error.
In this case, a do-main that should ideally be selected is neither thedomain in the previous turn nor the domain deter-mined based on the speech recognition results.
Ifthis situation can be detected, the system should beable to generate an appropriate response, like S3(good), and prevent inappropriate responses based?
?U1: Tell me the address of Horin-ji (temple name).
(domain: temple)S1: The address of Horin-ji is ...U2: Then, what is the fee for Horin-ji?
(domain: temple)misrecognized as ?the fee of Holiday Inn?.
(domain: hotel)S2: The fee of Holiday Inn is ...U3: The fee of Horin-ji.
(domain: temple)again misrecognized as ?the fee of Holiday Inn?.
(domain: hotel)S3 (bad): The fee of Holiday Inn is ...S3 (good): Are you asking about hotel information?U4: No.S4: Would you like to return to the temple informationservice??
?Figure 4: Example in which choice (III) should beselectedon an incorrect domain determination.
It is pos-sible for the system to restart from two utterancesbefore (U1), after asking a confirmatory question(S4) about whether to return to it or not.
After that,repetition of similar errors can also be avoided ifthe system prohibits transition to the hotel domain.4 Domain Selection using DialogueHistoryWe constructed a classifier that selects the appro-priate domains using various features, includingdialogue histories.
The selected domain candi-dates are based on: (I) the previous domain, (II)the domain in which the speech recognition resultscan be accepted with the highest recognition score,or (III) other domains.
Here, we describe the fea-tures present in our domain selection method.In order to not spoil the system?s extensibility,an advantage of the distributed-type architecture,the features used in the domain selection shouldnot depend on the specific domains.
We categorizethe features used into three categories listed below:?
Features representing the confidence withwhich the previous domain can be consideredcorrect (Table 1)?
Features about a user?s speech recognition re-sult (Table 2)12Table 1: Features representing confidence in pre-vious domainP1: number of affirmatives after entering the domainP2: number of negations after entering the domainP3: whether tasks have been completed in the domain(whether to enter ?requesting detailed information?in database search task)P4: whether the domain appeared beforeP5: number of changed slots after entering the domainP6: number of turns after entering the domainP7: ratio of changed slots (= P5/P6)P8: ratio of user?s negative answers (= P2/(P1 + P2))P9: ratio of user?s negative answers in the domain (=P2/P6)P10: states in tasksTable 2: Features of speech recognition resultsR1: best posteriori probability of the N-best candidatesinterpreted in the previous domainR2: best posteriori probability for the speech recogni-tion result interpreted in the domain, that is the do-main with the highest scoreR3: average of word?s confidence scores for the bestcandidate of speech recognition results in the do-main, that is, the domain with the highest scoreR4: difference of acoustic scores between candidatesselected as (I) and (II)R5: ratio of averages of words?
confidence scores be-tween candidates selected as (I) and (II)?
Features representing the situation after do-main selection (Table 3)We can take into account the possibility that acurrent estimated domain might be erroneous, byusing features representing the confidence in theprevious domain.
Each feature from P1 to P9 isdefined to represent the determination of whetheran estimated domain is reliable or not.
Specifi-cally, if there are many affirmative responses froma user or many changes of slot values during in-teractions in the domain, we regard the current do-main as reliable.
Conversely, the domain is notreliable if there are many negative answers from auser after entering the domain.We also adopted the feature P10 to representthe state of the task, because the likelihood thata domain is changed depends on the state of thetask.
We classified the tasks that we treat into twocategories using the following classifications firstmade by Araki et al (1999).
For a task catego-rized as a ?slot-filling type?, we defined the di-alogue states as one of the following two types:?not completed?, if not all of the requisite slotshave been filled; and ?completed?, if all of theTable 3: Features representing situations after do-main selectionC1: dialogue state after the domain selection after se-lecting previous domainC2: whether the interpretation of the user?s utterance isnegative in previous domainC3: number of changed slots after selecting previousdomainC4: dialogue state after selecting the domain with thehighest speech recognition scoreC5: whether the interpretation of the user?s utteranceis negative in the domain with the highest speechrecognition scoreC6: number of changed slots after selecting the domainwith the highest speech recognition scoreC7: number of common slots (name of place, here)changed after selecting the domain with the high-est speech recognition scoreC8: whether the domain with the highest speech recog-nition score has appeared beforerequisite slots have been filled.
For a task catego-rized as a ?database search type?, we defined thedialogue states as one of the following two types:?specifying query conditions?
and ?requesting de-tailed information?, which were defined in (Ko-matani et al, 2005a).The features which represent the user?s speechrecognition result are listed in Table 2 and corre-spond to those used in conventional studies.
R1considers the N-best candidates of speech recogni-tion results that can be interpreted in the previousdomain.
R2 and R3 represent information about adomain with the highest speech recognition score.R4 and R5 represent the comparisons between theabove-mentioned two groups.The features that characterize the situations af-ter domain selection correspond to the informationeach expert returns to the central module after un-derstanding the speech recognition results.
Theseare listed in Table 3.
Features listed from C1 toC3 represent a situation in which the previous do-main (choice (I)) is selected.
Those listed fromC4 to C8 represent a situation in which a domainwith the highest recognition score (choice (II)) isselected.Note that these features listed here have sur-vived after feature selection.
A feature survivesif the performance in the domain classification isdegradedwhen it is removed from a feature set oneby one.
We had prepared 32 features for the initialset.13Table 4: Specifications of each domainName of Class of # of vocab.
# ofdomain task in ASR slotsrestaurant database search 1,562 10hotel database search 741 9temple database search 1,573 4weather slot filling 87 3bus slot filling 1,621 3total - 7,373 -5 Experimental Evaluation5.1 ImplementationWe implemented a Japanese multi-domain spokendialogue system with five domain experts: restau-rant, hotel, temple, weather, and bus.
Specifica-tions of each expert are listed in Table 4.
If thereis any overlapping slot between the vocabulariesof the domains, our architecture can treat it as acommon slot, whose value is shared among thedomains when interacting with the user.
In oursystem, place names are treated as a common slot.We adopted Julian as the grammar-basedspeech recognizer (Kawahara et al, 2004).
Thegrammar rules for the speech recognizer can beautomatically generated from those used in thelanguage understanding modules in each domain.As a phonetic model, we adopted a 3000-statesPTM triphone model (Kawahara et al, 2004).5.2 Collecting Dialogue DataWe collected dialogue data using a baseline sys-tem from 10 subjects.
First, the subjects used thesystem by following a sample scenario, to get ac-customed to the timing to speak.
They, then, usedthe system by following three scenarios, where atleast three domains were mentioned, but neitheran actual temple name nor domain was explicitlymentioned.
One of the scenarios is shown in Fig-ure 5.
Domain selection in the baseline systemwas performed on the basis of the baseline methodthat will be mentioned in Section 5.4, in which ?was set to 40 after preliminary experiments.In the experiments, we obtained 2,205 utter-ances (221 per subject, 74 per dialogue).
Theaccuracy of the speech recognition was 63.3%,which was rather low.
This was because the sub-jects tended to repeat similar utterances even aftermisrecognition occurred due to out-of-grammar orout-of-vocabulary utterances.
Another reason wasthat the dialogues for subjects with worse speechrecognition results got longer, which resulted in anincrease in the total number of misrecognition.?
?Tomorrow or the day after, you are planning a sightsee-ing tour of Kyoto.
Please find a shrine you want to visitin the Arashiyama area, and determine, after consider-ing the weather, on which day you will visit the shrine.Please, ask for a temperature on the day of travel.
Alsofind out how to go to the shrine, whether you can take abus from the Kyoto station to there, when the shrine isclosing, and what the entrance fee is.?
?Figure 5: Example of scenarios5.3 Construction of the Domain ClassifierWe used the data containing 2,205 utterances col-lected using the baseline system, to construct a do-main classifier.
We used C5.0 (Quinlan, 1993) asa classifier.
The features used were described inSection 4.
Reference labels were given by handfor each utterance based on the domains the sys-tem had selected and transcriptions of the user?sutterances, as follows2.Label (I): When the correct domain for a user?sutterance is the same as the domain in whichthe previous system?s response was made.Label (II): Except for case (I), when the correctdomain for a user?s utterance is the domainin which a speech recognition result in the N-best candidates with the highest score can beinterpreted.Label (III): Domains other than (I) and (II).5.4 Evaluation of Domain SelectionWe compared the performance of our domain se-lection with that of the baseline method describedbelow.Baseline method: A domain having an interpre-tation with the highest score in the N-bestcandidates of the speech recognition was se-lected, after adding ?
for the acoustic likeli-hood of the speech recognizer if the domainwas the same as the previous one.
We calcu-lated the accuracies of domain selections forvarious ?.2Although only one of the authors assigned the labels,they could be easily assigned without ambiguity, since thelabels were automatically defined as previously described.Thus, the annotator only needs to judge whether a user?s re-quest was about the same domain as the previous system?s re-sponse or whether it was about a domain in the speech recog-nition result.1401002003004005006007008009000 10 20 30 40 50 60?#oferrorsindomainselectiontotaldomain in previous utt.domain with highest scoreother domainFigure 6: Accuracy of domain selection in thebaseline methodOur method: A domain was selected based onour method.
The performance was calculatedwith a 10-fold cross validation, that is, onetenth of the 2,205 utterances were used as testdata, and the remainder was used as trainingdata.
The process was repeated 10 times, andthe average of the accuracies was computed.Accuracies for domain selection were calculatedper utterance.
When there were several domainsthat had the same score after domain selection, onedomain was randomly selected among them as anoutput.Figure 6 shows the number of errors for do-main selection in the baseline method, categorizedby their reference labels as ?
changed.
As ?
in-creases, so does the system desire to keep the pre-vious domain.
A condition where ?
= 0 cor-responds to a method in which domains are se-lected based only on the speech recognition re-sults, which implies that there are no constraintson keeping the current domain.
As we can seein Figure 6, the number of errors whose refer-ence labels are ?a domain in the previous response(choice (I))?
decreases as ?
gets larger.
This is be-cause incorrect domain transitions due to speechrecognition errors were suppressed by the con-straint to keep the domains.
Conversely, we cansee an increase in errors whose labels are ?a do-main with the highest speech recognition score(choice (II))?.
This is because there is too muchincentive for keeping the previous domain.
Thesmallest number of errors was 634 when ?
= 35,and the error rate of domain selection was 28.8%(= 634/2205).
There were 371 errors whose refer-ence labels were neither ?a domain in the previousresponse?
nor ?a domain with the highest speechrecognition score?, which cannot be detected evenwhen ?
is changed based on conventional frame-works.We also calculated the classification accuracy ofour method.
Table 5 shows the results as a con-fusion matrix.
The left hand figure denotes thenumber of outputs in the baseline method, whilethe right hand figure denotes the number of out-puts in our method.
Correct outputs are in thediagonal cells, while the domain selection errorsare in the off diagonal cells.
Total accuracy in-creased by 5.3%, from 71.2% to 76.5%, and thenumber of errors in domain selection was reducedfrom 634 to 518, so the error reduction rate was18.3% (= 116/634).
There was no output in thebaseline method for ?other domains (III)?, which isin the third column, because conventional frame-works have not taken this choice into considera-tion.
Our method was able to detect this kind oferror in 157 of 371 utterances, which allows usto prevent further errors from continuing.
More-over, accuracies for (I) and (II) did not get worse.Precision for (I) improved from 0.77 to 0.83, andthe F-measure for (I) also improved from 0.83 to0.86.
Although recall for (II) got worse, its preci-sion improved from 0.52 to 0.62, and consequentlythe F-measure for (II) improved slightly from 0.61to 0.62.
These results show that our method candetect choice (III), which was newly introduced,without degrading the existing classification accu-racies.The features that follow played an importantrole in the decision tree.
The features that repre-sent confidence in the previous domain appearedin the upper part of the tree, including ?the num-ber of affirmatives after entering the domain (P1)?,?the ratio of user?s negative answers in the do-main (P9)?, ?the number of turns after entering thedomain (P6)?, and ?the number of changed slotsbased on the user?s utterances after entering thedomain (P5)?.
These were also ?whether a domainwith the highest score has appeared before (C8)?and ?whether an interpretation of a current user?sutterance is negative (C2)?.6 ConclusionWe constructed a multi-domain spoken dialoguesystem using an extensible framework.
Domainselection in conventional studies is based on ei-ther the domain based on the speech recognition15Table 5: Confusion matrix in domain selection (baseline / our method)reference label \ output in previous response (I) with highest score (II) others (III) # total label (recall)in previous response (I) 1289 / 1291 162 / 85 0 / 75 1451 (0.89 / 0.89)with highest score (II) 84 / 99 299?
/ 256?
0 / 28 383 (0.74 / 0.62)others (III) 293 / 172 78 / 42 0 / 157 371 ( 0 / 0.42)total 1666 / 1562 539 / 383 0 / 260 2205(precision) (0.77) / (0.83) (0.52) / (0.62) ( - ) / (0.60) (0.712 / 0.765)?
: These include 17 errors because of random selection when there were several domains having the same highest scores.results or the previous domain.
However, we no-ticed that these conventional frameworks cannotcope with situations where neither of these do-mains is correct.
Detection of such situationscan prevent dialogues from staying in the incor-rect domain, which allows our domain selectionmethod to be robust against speech recognition er-rors.
Furthermore, our domain selection methodis also extensible.
Our method does not select thedomains directly, but, by categorizing them intothree classes, it can cope with an increase or de-crease in the number of domains.
Based on the re-sults of an experimental evaluation using 10 sub-jects, our method was able to reduce domain se-lection errors by 18.3% compared to a baselinemethod.
This means our system is robust againstspeech recognition errors.There are still some issues that could makeour system more robust, and this is included infuture work.
For example, in this study, weadopted a grammar-based speech recognizer toconstruct each domain expert easily.
However,other speech recognition methods could be used,such as a statistical language model.
As well,multiple speech recognizers employing differentdomain-dependent grammars could be run in par-allel.
Thus, we need to investigate how to integratethese approaches into our framework, without de-stroying the extensibility.ReferencesMasahiro Araki, Kazunori Komatani, Taishi Hirata,and Shuji Doshita.
1999.
A dialogue library fortask-oriented spoken dialogue systems.
In Proc.IJCAI Workshop on Knowledge and Reasoning inPractical Dialogue Systems, pages 1?7.Tatsuya Kawahara, Akinobu Lee, Kazuya Takeda, Kat-sunobu Itou, and Kiyohiro Shikano.
2004.
Re-cent progress of open-source LVCSR engine Juliusand japanese model repository.
In Proc.
Int?l Conf.Spoken Language Processing (ICSLP), pages 3069?3072.Kazunori Komatani, Naoyuki Kanda, Tetsuya Ogata,and Hiroshi G. Okuno.
2005a.
Contextualconstraints based on dialogue models in databasesearch task for spoken dialogue systems.
In Proc.European Conf.
Speech Commun.
& Tech.
(EU-ROSPEECH), pages 877?880, Sep.Kazunori Komatani, Shinichi Ueno, Tatsuya Kawa-hara, and Hiroshi G. Okuno.
2005b.
User model-ing in spoken dialogue systems to generate flexibleguidance.
User Modeling and User-Adapted Inter-action, 15(1):169?183.Lori Lamel, Sophie Rosset, Jean-Luc Gauvain, andSamir Bennacef.
1999.
The LIMSI ARISE sys-tem for train travel information.
In IEEE Int?l Conf.Acoust., Speech & Signal Processing (ICASSP),pages 501?504, Phoenix, AZ.Ian R. Lane and Tatsuya Kawahara.
2005.
Utteranceverification incorporating in-domain confidence anddiscourse coherence measures.
In Proc.
EuropeanConf.
Speech Commun.
& Tech.
(EUROSPEECH),pages 421?424.E.
Levin, S. Narayanan, R. Pieraccini, K. Biatov,E.
Bocchieri, G. Di Fabbrizio, W. Eckert, S. Lee,A.
Pokrovsky,M.
Rahim, P. Ruscitti, andM.Walker.2000.
The AT&T-DARPA communicator mixed-initiative spoken dialogue system.
In Proc.
Int?lConf.
Spoken Language Processing (ICSLP).Bor-shen Lin, Hsin-min Wang, and Lin-shan Lee.2001.
A distributed agent architecture for intelli-gent multi-domain spoken dialogue systems.
IEICETrans.
on Information and Systems, E84-D(9):1217?1230, Sept.Mikio Nakano, Yuji Hasegawa, Kazuhiro Nakadai,Takahiro Nakamura, Johane Takeuchi, ToyotakaTorii, Hiroshi Tsujino, Naoyuki Kanda, and Hi-roshi G. Okuno.
2005.
A two-layer model for be-havior and dialogue planning in conversational ser-vice robots.
In 2005 IEEE/RSJ International Con-ference on Intelligent Robots and Systems (IROS),pages 1542?1548.Ian O?Neill, Philip Hanna, Xingkun Liu, and MichaelMcTear.
2004.
Cross domain dialogue modelling:An object-based approach.
In Proc.
Int?l Conf.
Spo-ken Language Processing (ICSLP).Botond Pakucs.
2003.
Towards dynamic multi-domain dialogue processing.
In Proc.
European16Conf.
Speech Commun.
& Tech.
(EUROSPEECH),pages 741?744.Alexandros Potamianos and Hong-Kwang J. Kuo.2000.
Statistical recursive finite state machine pars-ing for speech understanding.
In Proc.
Int?l Conf.Spoken Language Processing (ICSLP), volume 3,pages 510?513.J.
Ross Quinlan.
1993.
C4.5: Programs for Ma-chine Learning.
Morgan Kaufmann, San Mateo,CA.
http://www.rulequest.com/see5-info.html.Antoine Raux and Maxine Eskenazi.
2004.
Non-native users in the let?s go!!
spoken dialogue sys-tem: Dealing with linguistic mismatch.
In Proc.
ofHLT/NAACL.Ruben San-Segundo, Bryan Pellom, Wayne Ward, andJose M. Pardo.
2000.
Confidence measures for di-alogue management in the CU communicator sys-tem.
In IEEE Int?l Conf.
Acoust., Speech & SignalProcessing (ICASSP).17
