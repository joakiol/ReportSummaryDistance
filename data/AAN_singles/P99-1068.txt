Mining the Web for Bilingual TextPh i l ip  Resn ik*Dept.
of Linguistics/Institute for Advanced Computer StudiesUniversity of Maryland, College Park, MD 20742resnik@umiacs, umd.
eduAbst ractSTRAND (Resnik, 1998) is a language-independent system for automatic discoveryof text in parallel translation on the WorldWide Web.
This paper extends the prelim-inary STRAND results by adding automaticlanguage identification, scaling up by ordersof magnitude, and formally evaluating perfor-mance.
The most recent end-product is an au-tomatically acquired parallel corpus comprising2491 English-French document pairs, approxi-mately 1.5 million words per language.1 In t roduct ionText in parallel translation is a valuable re-source in natural language processing.
Sta-tistical methods in machine translation (e.g.
(Brown et al, 1990)) typically rely on largequantities of bilingual text aligned at the doc-ument or sentence level, and a number ofapproaches in the burgeoning field of cross-language information retrieval exploit parallelcorpora either in place of or in addition to map-pings between languages based on informationfrom bilingual dictionaries (Davis and Dunning,1995; Landauer and Littman, 1990; Hull andOard, 1997; Oard, 1997).
Despite the utility ofsuch data, however, sources of bilingual text aresubject to such limitations as licensing restric-tions, usage fees, restricted omains or genres,and dated text (such as 1980's Canadian poli-tics); or such sources imply may not exist for* This work was supported by Department of De-fense contract MDA90496C1250, DARPA/ITO Con-tract N66001-97-C-8540, and a research grant from SunMicrosystems Laboratories.
The author gratefully ac-knowledges the comments of the anonymous reviewers,helpful discussions with Dan Melamed and Doug Oard,and the assistance of Jeff Allen in the French-Englishexperimental evaluation.language pairs of interest.Although the majority of Web content is inEnglish, it also shows great promise as a sourceof multilingual content.
Using figures fromthe Babel survey of multilinguality on the Web(htZp :/ /www.
i soc .
o rg / ) ,  it is possible to esti-mate that as of June, 1997, there were on the or-der of 63000 primarily non-English Web servers,ranging over 14 languages.
Moreover, a follow-up investigation of the non-English servers ug-gests that nearly a third contain some usefulcross-language data, such as parallel English onthe page or links to parallel English pages - -the follow-up also found pages in five languagesnot identified by the Babel study (Catalan, Chi-nese, Hungarian, Icelandic, and Arabic; MichaelLittman, personal communication).
Given thecontinued explosive increase in the size of theWeb, the trend toward business organizationsthat cross national boundaries, and high levelsof competition for consumers in a global mar-ketplace, it seems impossible not to view mul-tilingual content on the Web as an expandingresource.
Moreover, it is a dynamic resource,changing in content as the world changes.
Forexample, Diekema et al, in a presentation atthe1998 TREC-7 conference (Voorhees and Har-man, 1998), observed that the performance oftheir cross-language information retrieval washurt by lexical gaps such as Bosnia/Bosnie-this illustrates a highly topical missing pair intheir static lexical resource (which was based onWordNet 1.5).
And Gey et al, also at TREC-7,observed that in doing cross-language r trievalusing commercial machine translation systems,gaps in the lexicon (their example was acupunc-ture/Akupunktur) could make the difference be-tween precision of 0.08 and precision of 0.83 onindividual queries.ttesnik (1998) presented an algorithm called527Candidate PairGenerationCmdidat~ PairEvaluafio~(s t ructura l )i i ' Candidate pak t i ia, Filtel/ng i, OanSuage d=pen&nO 1 i I1_ _ _ ~ _  _lFigure 1: The STRAND architectureSTRA N D (S t ructura l  T rans la t ion  Recognition fo rAcquiring Natural Data) designed to explorethe Web as a source of parallel text, demon-strating its potential with a small-scale valu-ation based on the author's judgments.
Afterbriefly reviewing the STRAND architecture andpreliminary results (Section 2), this paper goesbeyond that preliminary work in two significantways.
First, the framework is extended to in-clude a filtering stage that uses automatic lan-guage identification to eliminate an importantclass of false positives: documents that appearstructurally to be parallel translations but are infact not in the languages of interest.
The systemis then run on a somewhat larger scale and eval-uated formally for English and Spanish usingmeasures of agreement with independent humanjudges, precision, and recall (Section 3).
Sec-ond, the algorithm is scaled up more seriously togenerate large numbers of parallel documents,this time for English and French, and again sub-jected to formal evaluation (Section 4).
Theconcrete nd result reported here is an automat-ically acquired English-French parallel corpusof Web documents comprising 2491 documentpairs, approximately 1.5 million words per lan-guage (without markup), containing little or nonoise.2 STRAND Pre l iminar iesThis section is a brief summary of the STRANDsystem and previously reported preliminary re-sults (Resnik, 1998).The STRAND architecture is organized as apipeline, beginning with a candidate generationstage that (over-)generates candidate pairs ofdocuments that might be parallel translations.
(See Figure 1.)
The first implementation f thegeneration stage used a query to the Altavistasearch engine to generate pages that could beviewed as "parents" of pages in parM\]el transla-tion, by asking for pages containing one portionof anchor text (the readable material in a hy-perlink) containing the string "English" withina fixed distance of another anchor text contain-ing the string "Spanish".
(The matching pro-cess was case-insensitive.)
This generated manygood pairs of pages, such as those pointed to byhyperlinks reading Click here for English ver-sion and Click here for Spanish version, as wellas many bad pairs, such as university pages con-taining links to English Literature in close prox-imity to Spanish Literature.The candidate generation stage is followedby a candidate valuation stage that representsthe core of the approach, filtering out bad can-didates from the set of generated page pairs.It employs a structural recognition algorithmexploiting the fact that Web pages in paralleltranslation are invariably very similar in theway they are structured - -  hence the 's' inSTRAND.
For example, see Figure 2.The structural recognition algorithm firstruns both documents through a transducerthat reduces each to a linear sequence oftokens corresponding to HTML markupelements, interspersed with tokens repre-senting undifferentiated "chunks" of text.For example, the transducer would replacethe HTML source text <TITLE>hCL'99Conference Home Page</TITLE> with thethree tokens \[BEGIN: TITLE\], \[Chunk: 24\], and\[END:TITLE\].
The number inside the chunktoken is the length of the text chunk, notcounting whitespace; from this point on onlythe length of the text chunks is used, andtherefore the structural filtering algorithm iscompletely language independent.Given the transducer's output for each doc-ument, the structural filtering stage aligns thetwo streams of tokens by applying a standard,widely available dynamic programming algo-rithm for finding an optimal alignment betweentwo linear sequences.
1 This alignment matchesidentical markup tokens to each other as muchas possible, identifies runs of unmatched tokensthat appear to exist only in one sequence butnot the other, and marks pairs of non-identicaltokens that were forced to be matched to eachother in order to obtain the best alignment pos-1 Known to many programmers as d i f f .528High l ights  Best Practices ofSeminar on Self-Regulationre$,ulla~ !mo~.
AJ medm,te~ fm rile sw m,  Zm~ Bro ,~ DirecSc~ Gr.aera\].
Ccm*m'ael PSodu~re.~a~t mima= att~lmtive mm d* l i~  (ASD) m ~  atmh u ,~lut~at7 ?~d~a a~in du.~T ~lf-nv*mq~nL He ~ thai ?
for~b,~m~n|  ~ A~\[~ v,ua~l d e ~  inch topi~ uwl~ck ASD= pm,~d= tl~ ram1 =pprop*u~ mecl=m~= * d wire ~ m= ~udk=l~ m=d w~din=.Vdmm*r~ C=I~"A voluuuu7 code iJ ?
,~  ~4 ~aadardized ~t~at~ -- ~ cxpl~:ifly ~ ?4 ?
I~isla~ve~gut~orT ~gin'~ -* dc=iloed to ipB=oc~ ~**~, cc~Uol = ~ ?
L~e b~i~ o( ~ who agre=dTreamry Board $ c~'*J.sr~, "t~imiam so~=u'  rll6at to ~eguha~ They ,im#y c~IT~ the pm~ie,p~m?altetamln ~ bell I r?|tda~ed by the g~enm~"~f~h~ to o~emen~ aed e~e mS~t~e f~, nSu l=k~ Wht~ ~ ~des b~e ?
eemb~ ~a?~'=laSo, indudi~:?
.,1~ p ,~tm *o be ,k~tepett ~?
q~ddy eum h*~:?
the l~= c ~ i ~  ?~,d m pre~ Id  pm ie #=;S~mm,en*?
'@ Fails saillants des praflques exemplalresS~mina l re  sin" I 'autor (=glemen rat ionLe v~k=di  25 oc~m~ 1996, 40 ~ u  d= mg~u d?
IA n~gl?~nu~m l  ~ ~ ~nduL.?
~ I~prafiqo~ ?~aku cn ~ 4?
r~| l?~ ~ vi=~l i ald??
I~ odor= ~ ~ famillalis~ iv=Zaue Bmw~ dn?~ gL~ruL Du'ccxi~ dc~ b i~ ~ ou~tvzmati~, a ~v~ La e~mcc ?n ~Lulamque ~a l t  p rod~mm m &~zl~mt ~ La d i~Lf~ d~s nw~.s de pt~sLau~ des s ~  qmtraltax~t d= divm ~jets.
~ lu  m/:caai=~  ~ ~?
r~e t~t ~ i?
?= I~ I~~prew~ ~mi  gl~ I?i probl~ae~ ~v~l  pu  chacua.c~l~ ,d~Ud~t~ i~ l~lil latif m t~Ic~sa l r?
- ~ paur ia f l t ,?~,  f ~ ,  o~m34~ = ~va\ ]~~ m  d?
= ?p~i ,tea oat ~.
Ib ='$1imin~l p~.
?
p~rsui',i M. Bd= Gl~h~, ualy~epn ~ap~.
Affsi~?
~gle~mair=,  = ~ aM C~i l  du T~sm, 5= rue& aM S~ven~nt  doAu ~nt  o~ I= n!gtcmcmask~ fair I ' ob~ d '~ e ~  ~ du pabli?, le= S ~ n u  i I'L, ch?ll??
i l l  ~t t== d '~t= b pm~lh~ de ~ qul fraRumt I~ i u i t i~v= de t~ikmmtmkm:?
h f= i l l t  ~ l~ iu  a~ IJm$1~lle iLs peuvuq ~,e m~llft4u= Cu fcm~.~= d~ ~uB~ d i n ,Figure 2: Structural similarity in parallel translations on the Websible.
2 At this point, if there were too manyunmatched tokens, the candidate pair is takento be prima facie unacceptable and immediatelyfiltered out.Otherwise, the algorithm extracts from thealignment hose pairs of chunk tokens that werematched to each other in order to obtain thebest alignments.
3 It then computes the corre-lation between the lengths of these non-markuptext chunks.
As is well known, there is a re-\]\]ably linear relationship in the lengths of texttranslations - -  small pieces of source text trans-late to smaJl pieces of target text, medium tomedium, and large to large.
Therefore we canapply a standard statistical hypothesis test, andif p < .05 we can conclude that the lengths arereliably correlated and accept the page pair aslikely to be translations of each other.
Other-wise, this candidate page pair is filtered out.
42An anonymous reviewer observes that d i f f  has nopreference for aligning chunks of similar lengths, whichin some cases might lead to a poor alignment when agood one exists.
This could result in a failure to identifytrue translations and is worth investigating further.3Chunk tokens with exactly equal lengths are ex-cluded; see (Resnik, 1998) for reasons and other detailsof the algorithm.4The level of significance (p < .05) was the ini-tial selection during algorithm development, and neverchanged.
This, the unmatched-tokens threshold forprima/aeie rejection due to mismatches (20~0), and themaximum distance between hyperlinks in the genera-In the preliminary evaluation, I generated atest set containing 90 English-Spanish candi-date pairs, using the candidate generation stageas just described?
I evaluated these candi-dates by hand, identifying 24 as true translationpairs.
5 Of these 24, STRAND identified 15 as truetranslation pairs, for a recall of 62.5%.
Perhapsmore important, it only generated 2 additionaltranslation pairs incorrectly, for a precision of15/17 = s8.2%.3 Add ing  Language Ident i f i ca t ionIn the original STRAND architecture, addi-tional filtering stages were envisaged as pos-sible (see Figure 1), including such language-dependent processes as automatic languageidentification and content-based comparison ofstructually aligned document segments usingcognate matching or existing bilingual dictio-naries.
Such stages were initially avoided inorder to keep the system simple, lightweight,and independent of linguistic resources?
How-tion stage (10 lines), are parameters of the algorithmthat were determined during development using a smallamount of arbitrarily selected French-English data down-loaded from the Web.
These values work well in prac-tice and have not been varied systematically; their valueswere fixed in advance of the preliminary evaluation andhave not been changed since.?
The complete test set and my judgmentsfor this preliminary evaluation can be found athttp ://umiacs.
umd?
edu/~resnik/amt a98/.529.
.
.
.
.
.
.
.
.
.
.
"-.%', .
.
.
.
.~"~-'~.
"2  .~?
~u~ / v .
.B .~,~ I s~.~c .~ I o,~,~o I~ .~1~lea~ ~ =~ ~ mmy oL ~ bo~J  me~ free a t .
re  6~m ~ ~ ,  ~ ~ J  ad,f~0~J dayJdltpJltt b?
fstac, tt?l la in yt, ur ~=Ii~,=%~ = r~ l =  tk:l lvct7 I=  LIPS OYELNIg I l l r  iato fiatSptt~l 1 ~  ba~.
Wt ~ig o~a~ ~ou ~ith tat dfiptfitg ~ (bared ~ uilka).Ykwlt~ PW'cbu~o,  ?~'t ~ .
lo, ~ c~.,,,Its rmt*Figure 3: Structurally similar pages that are not translationsever, in conducting an error analysis for the pre-liminary evaluation~ and further exploring thecharacteristics of parallel Web pages, it becameevident that such processing would be impor-tant in addressing one large class of potentialfalse positives.
Figure 3 illustrates: it showstwo documents that are generated by lookingfor "parent" pages containing hyperlinks to En-glish and Spanish, which pass the structural fil-ter with flying colors.
The problem is poten-tially acute if the generation stage happens toyield up many pairs of pages that come from on-line catalogues or other Web sites having largenumbers of pages with a conventional structure.There is, of course, an obvious solution thatwill handle most such cases: making sure thatthe two pages are actually written in the lan-guages they are supposed to be written in.
Inorder to filter out candidate page pairs thatfail this test, statistical language identificationbased on character n-grams was added to thesystem (Dunning, 1994).
Although this doesintroduce a need for language-specific trainingdata for the two languages under consideration,it is a very mild form of language dependence:Dunning and others have shown that whenclassifying strings on the order of hundreds orthousands of characters, which is typical of thenon-markup text in Web pages, it is possibleto discriminate languages with accuracy in thehigh 90% range for many or most language pairsgiven as little as 50k characters per language astraining material.For the language filtering stage of STRAND,the following criterion was adopted: given twodocuments dl and d2 that are supposed to bein languages L1 and L2, keep the documentpair iff Pr(Ll ldl) > Pr(L21dl) and Pr(/21d2) >Pr(Llld2).
For English and Spanish, this trans-lates as a simple requirement that the "English"page look more like English than Spanish, andthat the "Spanish" page look more like Spanishthan English.
Language identification is per-formed on the plain-text versions of the pages.Character 5-gram models for languages underconsideration are constructed using 100k char-acters of training data from the European Cor-pus Initiative (ECI), available from the Linguis-tic Data Consortium (LDC).In a formal evaluation, STRAND with the newlanguage identification stage was run for Englishand Spanish, starting from the top 1000 hitsyielded up by Altavista in the candidate gen-eration stage, leading to a set of 913 candidatepairs.
A test set of 179 items was generated forannotation by human judges, containing:?
All the pairs marked GOOD (i.e.
transla-tions) by STRAND (61); these are the pairsthat passed both the structural and lan-guage identification filter.?
All the pairs filtered out via language idea-530tification (73)?
A random sample of the pairs filtered outstructurally (45)It was impractical to manually evaluate all pairsfiltered out structurally, owing to the time re-quired for judgments and the desire for two in-dependent judgments per pair in order to assessinter-judge reliability.The two judges were both native speakers ofSpanish with high proficiency in English, nei-ther previously familiar with the project.
Theyworked independently, using a Web browser toaccess test pairs in a fashion that allowed themto view pairs side by side.
The judges weretold they were helping to evaluate a system thatidentifies pages on the Web that are translationsof each other, and were instructed to make de-cisions according to the following criterion:Is this pair of pages intended to showthe same material to two differentusers, one a reader of English and theother a reader of Spanish?The phrasing of the criterion required some con-sideration, since in previous experience with hu-man judges and translations I have found thatjudges are frequently unhappy with the qual-ity of the translations they are looking at.
Forpresent purposes it was required neither thatthe document pair represent a perfect transla-tion (whatever that might be), nor even nec-essarily a good one: STR,AND was being testednot on its ability to determine translation qual-ity, which might or might not be a criterion forinclusion in a parallel corpus, but rather its abil-ity to facilitate the task of locating page pairsthat one might reasonably include in a corpusundifferentiated by quality (or potentially post-filtered manually).The judges were permitted three responses:?
Yes: translations of each other?
No: not translations of each other?
Unable to tellWhen computing evaluation measures, pagepairs classified in the third category by a hu-man judge, for whatever eason, were excludedfrom consideration.Comparison N Pr(Agree)J1, J2: 106 0.85 0.70J1, STRAND: 165 0.91 0.79J2, STRAND: 113 0.81 0.61J1 f3 J2, STRAND: 90 0.91 0.82Table 1: English-Spanish evaluationTable 1 shows agreement measures betweenthe two judges, between STRAND and eachindividual judge, and the agreement betweenSTRAND and the intersection of the two judges'annotations - -  that is, STRAND evaluatedagainst only those cases where the two judgesagreed, which are therefore the items we can re-gard with the highest confidence.
The table alsoshows Cohen's to, an agreement measure thatcorrects for chance agreement (Carletta, 1996);the most important ?
value in the table is thevalue of 0.7 for the two human judges, whichcan be interpreted as sufficiently high to indi-cate that the task is reasonably well defined.
(As a rule of thumb, classification tasks with< 0.6 are generally thought of as suspect inthis regard.)
The value of N is the number ofpairs that were included, after excluding thosefor which the human judgement in the compar-ison was undecided.Since the cases where the two judges agreedcan be considered the most reliable, these wereused as the basis for the computation of recalland precision.
For this reason, and becausethe human-judged set included only a sampleof the full set evaluated by STRAND, it was nec-essary to extrapolate from the judged (by bothjudges) set to the full set in order to computerecall/precision figures; hence these figures arereported as estimates.
Precision is estimatedas the proportion of pages judged GOOD bySTRAND that were also judged to be good (i.e.
"yes") by both judges - -  this figure is 92.1%Recall is estimated as the number of pairs thatshould have been judged GOOD by STRAND(i.e.
that recieved a "yes" from both judges)that STRAND indeed marked GOOD - -  this fig-ure is 47.3%.These results can be read as saying that of ev-ery 10 document pairs included by STRAND ina parallel corpus acquired fully automaticallyfrom the Web, fewer than 1 pair on average wasincluded in error.
Equivalently, one could saythat the resulting corpus contains only about5318% noise.
Moreover, at least for the confidentlyjudged cases, STRAND is in agreement with thecombined human judgment more often than thehuman judges agree with each other.
The recallfigure indicates that for every true translationpair it accepts, STRAND must also incorrectly re-ject a true translation pair.
Alternatively, thiscan be interpreted as saying that the filteringprocess has the system identifying about halfof the pairs it could in principle have foundgiven the candidates produced by the genera-tion stage.
Error analysis suggests that recallcould be increased (at a possible cost to pre-cision) by making structural filtering more in-telligent; for example, ignoring some types ofmarkup (such as italics) when computing align-ments.
However, I presume that if the numberM of translation pairs on the Web is large, thenhalf of M is also large.
Therefore I focus on in-creasing the total yield by attempting to bringthe number of generated candidate pairs closerto M, as described in the next section.4 Scaling Up Candidate GenerationThe preliminary experiments and the new ex-periment reported in the previous ection madeuse of the Altavista search engine to locate "par-ent" pages, pointing off to multiple languageversions of the same text.
However, the samebasic mechanism is easily extended to locate"sibling" pages: cases where the page in onelanguage contains a link directly to the trans-lated page in the other language.
Explorationof the Web suggests that parent pages and sib-ling pages cover the major relationships betweenparallel translations on the Web.
Some siteswith bilingual text are arranged according to athird principle: they contain a completely sep-arate monolingual sub-tree for each language,with only the single top-level home page point-ing off to the root page of single-language v r-sion of the site.
As a first step in increasingthe number of generated candidate page pairs,STRAND was extended to permit both parentand sibling search criteria.
Relating monolin-gual sub-trees is an issue for future work.In principle, using Altavista queries forthe candidate generation stage should enableSTRAND to locate every page pair in the A1-tavista index that meets the search criteria.This likely to be an upper bound on the can-Comparison N Pr(Agree)J1, J2: 267 0.98 0.95J1, STRAND: 273 0.84 0.65J2, STRAND: 315 0.85 0.63J1 N J2, STRAND: 261 0.86 0.68Table 2: English-French evaluationdidates that can be obtained without buildinga Web crawler dedicated to the task, since oneof Altavista's distinguishing features is the sizeof its index.
In practice, however, the user inter-face for Altavista appears to limit the numberof hits returned to about the first 1000.
It waspossible to break this barrier by using a featureof Altavista's "Advanced Search": including arange of dates in a query's selection criteria.Having already redesigned the STRAND gener-ation component to permit multiple queries (inorder to allow search for both parent and siblingpages), each query in the query set was trans-formed into a set of mutually exclusive queriesbased on a one-day range; for example, one ver-sion of a query would restrict he result to pageslast updated on 30 November 1998, the next 29November 1998, and so forth.Although the solution granularity was notperfect - -  searches for some days still bumpedup against he 1000-hit maximum - -  use of bothparent and sibling queries with date-range re-stricted queries increased the productivity ofthe candidate generation component by an or-der of magnitude.
The scaled-up system wasrun for English-French document pairs in lateNovember, 1998, and the generation componentproduced 16763 candidate page pairs (with du-plicates removed), an 18-fold increase over theprevious experiment.
After eliminating 3153page pairs that were either exact duplicatesor irretrievable, STRAND'S structural filteringremoved 9820 candidate page pairs, and thelanguage identification component removed an-other 414.
The remaining pairs identified asGOOD - -  i.e.
those that STRAND consideredto be parallel translations - -  comprise a paral-lel corpus of 3376 document pairs.A formal evaluation, conducted in the samefashion as the previous experiment, yields theagreement data in Table 2.
Using the caseswhere the two human judgments agree asground truth, precision of the system is esti-mated at 79.5%, and recall at 70.3%.532Comparison N Pr(Agree) i?J1, J2: 267 0.98 0.95J1, STRAND: 273 0.88 0.70J2, STRAND: 315 0.88 0.69J1 N J2, STRAND: 261 0.90 0.75Table 3: English-French evaluation with stricterlanguage ID criterionA look at STRAND'S errors quickly identifiesthe major source of error as a shortcoming ofthe language identification module: its implicitassumption that every document is either in En-glish or in French.
This assumption was vi-olated by a set of candidates in the test set,all from the same site, that pair Dutch pageswith French.
The language identification cri-terion adopted in the previous section requiresonly that the Dutch pages look more like En-glish than like French, which in most cases istrue.
This problem is easily resolved by train-ing the existing language identification compo-nent with a wider range of languages, and thenadopting a stricter filtering criterion requiringthat Pr(Englishldl ) > Pr(Lldl ) for every lan-guage L in that range, and that d2 meet thecorresponding requirement for French.
6 Doingso leads to the results in Table 3.This translates into an estimated 100% pre-cision against 64.1% recall, with a yield of 2491documents, approximately 1.5 million words perlanguage as counted after removal of HTMLmarkup.
That is, with a reasonable thoughadmittedly post-hoc revision of the languageidentification criterion, comparison with humansubjects suggests the acquired corpus is non-trivial and essentially noise free, and moreover,that the system excludes only a third of thepages that should have been kept.
Naturallythis will need to be verified in a new evaluationon fresh data.SLanguage ID across a wide range of languages isnot.
difficult to obtain.
E.g.
see the 13-language setof the freely available CMU stochastic language iden-tifier (http://www.cs.cmu.edu/,,~dougb/ident.html),the 18-language set of the Sun Language ID Engine(ht tp: / /www.sunlabs.com /research /ila/ demo /index.html ),or the 31-language set of the XRCE LanguageIdentifier (http://www.rxrc.xerox.com/research/mltt/Tools/guesser.html).
Here I used the language IDmethod of the previous section trained with profilesof Danish, Dutch, English, French, German, Italian,Norwegian, Portuguese, Spanish, and Swedish.5 Conc lus ionsThis paper places acquisition of parallel textfrom the Web on solid empirical footing, mak-ing a number of contributions that go beyondthe preliminary study.
The system has beenextended with automated language identifica-tion, and scaled up to the point where a non-trivial parallel corpus of English and French canbe produced completely automatically from theWorld Wide Web.
In the process, it was discov-ered that the most lightweight use of languageidentification, restricted to just the the languagepair of interest, needed to be revised in favor of astrategy that includes identification over a widerange of languages.
Rigorous evaluation usinghuman judges suggests that the technique pro-duces an extremely clean corpus - -  noise esti-mated at between 0 and 8% - -  even without hu-man intervention, requiring no more resourcesper language than a relatively small sample oftext used to train automatic language identifi-cation.Two directions for future work are appar-ent.
First, experiments need to be done usinglanguages that are less common on the Web.Likely first pairs to try include English-Korean,English-Italian, and English-Greek.
Inspectionof Web sites - -  those with bilingual text identi-fied by STRAND and those without - -  suggeststhat the strategy of using Altavista to generatecandidate pairs could be improved upon signifi-cantly by adding a true Web crawler to "mine"sites where bilingual text is known to be avail-able, e.g.
sites uncovered by a first pass of thesystem using the Altavista engine.
I would con-jecture that for English-French there is an orderof magnitude more bilingual text on the Webthan that uncovered in this early stage of re-search.A second natural direction is the applica-tion of Web-based parallel text in applicationssuch as lexical acquisition and cross-languageinformation retrieval - -  especially since a side-effect of the core STRAND algorithm is aligned"chunks", i.e.
non-markup segments found tocorrespond to each other based on alignmentof the markup.
Preliminary experiments usingeven small amounts of these data suggest hatstandard techniques, uch as cross-language lex-ical association, can uncover useful data.533ReferencesP.
Brown, J. Cocke, S. Della Pietra, V. DellaPietra, F. Jelinek, R. Mercer, and P. Roossin.1990.
A statistical approach to ma-chine translation.
Computational Linguistics,16(2):79-85.Jean Carletta.
1996.
Assessing agreementon classification tasks: the Kappa statis-tic.
Computational Linguistics, 22(2):249-254, June.Mark Davis and Ted Dunning.
1995.
A TRECevaluation of query translation methods formulti-lingual text retrieval.
In Fourth TextRetrieval Conference (TREC-4).
NIST.Ted Dunning.
1994.
Statistical identification oflanguage.
Computing Research LaboratoryTechnical Memo MCCS 94-273, New MexicoState University, Las Cruces, New Mexico.David A.
Hull and Douglas W. Oard.
1997.Symposium on cross-language text andspeech retrieval.
Technical Report SS-97-04,American Association for Artificial Intelli-gence, Menlo Park, CA, March.Thomas K. Landauer and Michael L. Littman.1990.
Fully automatic ross-language docu-ment retrieval using latent semantic ndexing.In Proceedings of the Sixth Annual Confer-ence of the UW Centre for the New OxfordEnglish Dictionary and Text Research, pagespages 31-38, UW Centre for the New OEDand Text Research, Waterloo, Ontario, Octo-ber.Douglas W. Oar& 1997.
Cross-language t xtretrieval research in the USA.
In ThirdDELOS Workshop.
European Research Con-sortium for Informatics and MathematicsMarch.Philip Resnik.
1998.
Parallel strands: A pre-liminary investigation i to mining the web forbilingual text.
In Proceedings of the ThirdConference of the Association for MachineTranslation in the Americas, AMTA-98, inLecture Notes in Artificial Intelligence, 1529,Langhorne, PA, October 28-31.E.
M. Voorhees and D. K. Harman.
1998.The seventh Text REtrieval Conference(TREC-7).
NIST special publication,Galthersburg, Maryland, November 9-11.http ://trec.
nist.
gov/pubs, html.534
