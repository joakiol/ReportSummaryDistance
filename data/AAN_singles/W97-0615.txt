Filtering Errors and Repairing Linguistic Anomaliesfor Spoken Dialogue SystemsDavid Roussel* and Ariane Halber tThomson-CSFLaboratoire Central de Recherches, F-91404 Orsay Cedex, Franceemaih (roussel,ariane}@thomson-lcr.frAbst rac tOur work addresses the integration ofspeech recognition and language processingfor whole spoken dialogue systems.To filter ill-recognized words, we designan on-line computing of word confidencescores based on the recognizer output hy-pothesis.
To infer as much informationas possible from the retained sequence ofwords, we propose a bottom-up syntactico-semantic robust parsing relying on a lexi-calized tree grammar and on integrated re-pairing strategies.1 In t roduct ionSpoken dialogue systems enable people to interactwith computers using speech.
However, a key chal-lenge for such interfaces i to couple successfully au-tomatic speech recognition (ASR) and natural lan-guage processing modules (NLP) given their limits.Several collaboration modalities between ASR andNLP have been investigated.
On the one hand,the speech recognition task can benefit from linguis-tic decision to uncover the correct utterance, see(Rayner et al, 1994) among others.
On the otherhand, NLP components can be robust with respectto recognition errors.
The straightforward approachis to be robust by focusing only on informative words(Lamel et al, 1995; Meteer and Rohlicek, 1994).
Bynature, it misses some existing information in thesentence and it can be misled in case of errors oninformative words.
A more controlled robustness iexpected with a complete linguistic analysis (Young,1994; Hanrieder and GSrz, 1995; Dowding et al,1994).
In a practical application, a dialogue module*with Lab.
CLIPS IMAG, Grenobletwith Dept.
Signal, ENST Pariscan then handle interactive recovery, as illustratedby (Suhm, Myers, and Waibel, 1996).The current work attempts to repair misrecogni-tions by mobilising available acoustic ues and byusing linguistic abstraction and syntactico-semanticpredictions.
We present a filtering method and arepairing parsing strategy which fit in a completesystem architecture.An advantage ofour approach is the use of a coremodule that is independent from any application.Another advantage, for real applications, is to beaware of the expected performances of the ASR sys-tems.
Indeed, there are obstacles that prevent ASRsystems to be fully reliable.
In particular, the de-coding algorithms enforce models which do not ex-ploit all linguistic knowledge, mainly due to com-putational complexity.
This hinders somehow thedecoding so that the right solution is sometimes justnot available.2 Sys tem arch i tec tureThe system architecture consists in a speech recog-nizer, a word confidence scoring module, a robustparsing module and higher modules -around a di-alogue module (Normand, Pernel, and Bacconnet,1997).The modules of the system articulate in a comple-mentary way.
The scoring module goal is to provideword acoustic onfidence scores to help the robustparser in its task.
The parsing module takes thebest recognition hypothesis.
It attempts to repairrecognition errors and transmits a semantic repre-sentation of the sentence to the dialogue module.It relies on a lexicalized tree grammar and on inte-grated repairing rules.
They make use of the knowl-edge embedded in the lexical grammar and of can-didates present in the N-best hypothesis.
We havestudied its capacities to detect and predict missingelements and to select syntactically and semanticallywell-formed sentences.
The robust parser needs con-74fidence scoring module to point out inserted and sub-stituted elements.The words identified as inserted or as substitutedare marked but the decision is laid upon the robustparsing or subsequent linguistic processes.
More-over, falsely rejected words can give rise to deletionrepairing procedures.
The robust parsing strategyapplies syntactic and semantic well-formedness con-straints.
It derives the meaning of the sentence outof available elements and furthermore predicts themissing elements required to meet the constraints.Whatever the case, initially well-formed sentence ornot, the parsing produces a usable analysis for thehigher layers to perform the final interpretation orto trigger a repairing dialogue.3 Word  Er rors  F i l te r ingInserted and substituted elements are a major prob-lem as they are a source of misunderstanding.
Ifnot treated early on in a spoken dialogue system,they weaken the dialogue interaction, caught be-tween running the risk of confusing the user withirrelevant interactions or annoying the user withrepetitive confirmation checks.As parsing is not always able to reject ill-recognized sentences, especially when they remainwell-formed, cross-checking is required betweenacoustic and linguistic information.
Our methodis to isolate errors according to a scoring criterionand then transmit o the parsing suspected elementswith the alternative acoustic candidates.
They canbe reactivated by the parsing if necessary, to achievea complete analysis.3.1 Scor ing  MethodA way to get a scoring criterion is to attribute arecognition confidence score to each word in the bestsentence hypothesis.A confidence score relates to the word beingrightly recognized and not only to the word beingacoustically close to an acoustic reference.
It nor-mally depends on the recognizer behaviour, the lan-guage to be recognized, and the application?
Forexample (Rivlin, 1995) sees it as a normalisation ofthe phonemes acoustic scores and derives an exactestimation from a recognition corpus.
We proposehere a simple on-line computing of the word confi-dence score.
It is not an exact measure but it hasminimal knowledge requirements.
The scoring re-lies on the observation of concurrent hypothesis ofthe recognizer and their associated acoustic scores.We have tested it with the N-best sentence hypothe-sis but lattice and word graph could be investigatedfurther.An initial score for each word in the best sen-tence is taken either from the word acoustic scoreor from the sentence score, distributed uniformly onthe words.
The score we have used here is the globalsentence acoustic score.
This initial word score is re-evaluated on the basis of concordances between thedifferent recognition hypothesis.
The major param-eter for score estimation is the alignment betweenthe word in the best hypothesis and the words inthe  other hypothesis.
In our case this alignment isachieved by a dynamic programming method 1For each N-best, an alignment value is definedfrom the words alignment.
It disfavours especiallythe recidivist occurrences of a word candidate.
Letwi be the i th word in the best hypothesis, the align-ment value at rank n is:when wi is aligned with itself-1  when wi is not alignedAln(wi) = - r  when wi is aligned for the r thtime with a given word(1)The re-evaluation of a word score will derive fromthis word alignment value.Each N-best gives rise to a re-evaluation of thecurrent word score.
This re-evaluation decomposesinto two factors, a re-scoring potential V and a re-scoring amplitude AS.
Let Sn(wi) be the score ofthe word wi having observed N-best hypothesis upto rank n:= + (2)Where Vn(wi) is the potential for rescoring theword wi according to hypothesis Hn - the sentencehypothesis at rank n and ASh is the rescoring am-plitude at rank n.The first factor of the re-evaluation is the po-tential, defined in equation 3.
It is based on thealignments and indicates the type of increase or de-crease that a word deserves.
A context effect is intro-duced in the potential in the form of penalties andbonus which are proportional to the direct neigh-bouts alignment values (see equation 4), so that:V=(wi) = Aln(wi) + ~ 6Aln(wj, wi) (3)cr+Aln(wj) if Al,(wj) > 06Aln(wj,wi) = a-Aln(wj) if Al,(wj) < 0 (4)1As no additional phonetic or temporal information? "
S is used to do the alignment, here might be seldom caseof bad alignment.
The problem should not arise withlattice or word graph as they keep temporal information?75Where Al,(wi)  is the alignment value of word wibetween the first-best hypothesis H1 and the N-besthypothesis H, .
~Aln(wj, wi) is the context effect ofword wj on word wi (equation 4).
Practically this iseither a positive contribution if wj is well alignedor a negative contribution if wj is badly aligned.We consider context effect only from the immediateneighbours.The second factor of the re-evaluation is the am-plitude (cf.
equation 2).
The amplitude is the samefor every word at a given rank.
It is based on then th hypothesis score and the rank so that the ampli-tude decreases with the rank and with the relativescore difference between H1 and H~.
It expresses therescoring power of hypothesis Hn and is calculatediteratively as:ASr, = ASh_i(1 - S(H~) - S (H~)IS(H~)I - ~) (5)Where # is a linear slope that ensures a minimaldecrease.
S(H, )  is the global acoustic score of thehypothesis H .
.The scoring stops in the case of the amplitudereaching zero.
Fig 1 and 2 show evolution of theword score across N-best re-evaluation.3.2 F i l te r ing  app l i ca t ionOnce the word confidence scores are available, thefiltering still needs a threshold to point out would-be errors.
It is set on-line as the maximum scorethat different ypical cases of words to be eliminatedcould reach.
It is computed in the same time asword confidence scores.
We consider the worst casescore of several empirical cases independent from thetwo recognizer we tested.
One of those cases is aword that would be not-aligned 80% of the time andalways surrounded by aligned neighbours.When the suspect words have been spotted, it re-mains to be decided whether they are substitutionsor insertions.
We distinguish them thanks to seg-mental cues and to local word variations betweencompetitive hypothesis.
Practically, the alignmentspreviously calculated are scanned ; if the two bor-dering neighbours of a word w are once adjacent andwell aligned in an hypothesis, w is marked as an in-sertion.3.3 Eva luat ionWe have tested the word scoring module, with the in-corporated filtering, on errors produced by two exist-ing ASR systems from SRI and Cambridge Univer-sity.
The former, Nuance Communication recognizersystem is constrained by a Context Free Grammar.uCtered:  DO YOU HAVE SOME RED ARMCHAIRSHI : DO YOU HAVE TWO RED COMPUTERSH2: DO YOU HAVE TWO RED ARMCHAIRSH3: DO YOU HAVE THOSE RED COMPUTERSH4: DO YOU HAVE THE RED COMPUTERSH5: DO YOU HAVE THOSE RED ARMCHAIRSH6: DO YOU HAVE THE RED ARMCHAIRSH7: DO YOU HAVE SOME RED COMPUTERSTable 1: N-best hypothesis for the sentence "do youhave some red armchairs"do you have ~xne red amlcha~'s200 , , , ++\] rank 2 .
.
.
.
rank 3 .
.
.
.
.
.. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~ rank  4 - -150  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
- , .
.
~+.
rank  6 .
.
.
.
.. .
.
.
.
.
.
.
.
.
....2.-....::::.
:1~ : .
.
.
.
/~  .....................................,{,,~, /,.v "...:'>\h., ,," .
/ /  ' , )',,\.,,/\ /do ~ haveltWO red computerswoa:l In the fket b~tFigure 1: word scores across N-best ranks for thebest hypothesis "do you have two red computers"The latter, Abbot, uses an n-gram model (backed offtrigram model) 2The application domain is taken from the COVENz project (Normand and Tromp, 1996) , describedon http://chinon.thomson-csf.fr/coven/.
COVEN(COllaborative Virtual ENvironments) addressesthe technical and design-level requirements ofVirtual-based multi-participant collaborative activ-ities in professional and citizen-oriented domains.Among the grounding testbed applications, an in-terior design application is being developed, whichprovides the background of the work described inthis article.
A typical interior design scenario dealswith composition of pieces of furniture, equipmentand decoration in an office room by several partici-~The training corpus for the trigram was generatedartificially by the context free grammar of the first recog-nizer mentioned.
15% of the testset is out of the NuanceContext Free Grammar.
The sampling rate of acousticmodels are 8 kHz for Nuance and 16 kHz for Abbot.The Nuance communication recognizer system exploitsphonemes in context.
Abbot uses a neural network tomodel standard phonemes.3COVEN is a European project of the ACTS Pro-gramme (Advanced Communications Technologies andServices).76pants, within the limits of a common budget.
Ele-ments of the design are taken from a set of possiblefurniture, equipment and decoration objects, withvariable attributes in value domains.
The user mayask information to the system which provides guid-ance for the user decision.The evaluation results of the speech recognizersare given with others results in table 5.
Here aretwo examples of scoring and filtering.
Figure 1shows the evolution across seven N-best of an ill-recognized sentence score profile.
At the end, thetwo ill-recognized words (some and armchairs) areidentified as errors, they are classified as substitu-tions according to their type of alignment in the dif-ferent N-best.
The recognition hypothesis are dis-played in table 1 (the recognizer is Nuance).In the second example table 2 (from Abbot), theword is is inserted, but not in all N-best hypothesis.The confidence scores succeed in pointing is as ill-recognized, the alignment considerations will thenclassify it as an insertion.uttered: CAN YOU GIVE ME THE BUDGETHI: CAN YOU GIVE ME IS A BUDGETH2: CAM YOU GIVE ME IS THE BUDGETH3: CAN YOU GIVE ME A BUDGETH4: CAN YOU GIVE ME IT BUDGETH5: CAN YOU GIVE IT THE BUDGETH6: CAN YOU GIVE ME THE BUDGETH7: CAN YOU GIVE ME THESE BUDGETTable 2: N-best hypothesis for the sentence "can yougive me the budget"ited performances for the filtering taken alone andwe suspect hat even with future improvements, itwill remain limited.
A better filtering can only beachieved if it is informed by other knowledge sources.Performances of filtering, when coupled with the ro-bust parsing, are indeed much more satisfactory.4 Repa i r ing  Pars ing  St ra tegyThe aim of the robust parser presented here is tobuild a semantic representation needed by higherlayers of the system while faced with possible ill-formed sentences.
The parsing itself is led by a Lex-icalized Tree Grammar (Schabes, Abeill~, and Joshi,1988).
It relies on a set of elementary trees (de-fined in the lexicon) which have at least one termi-nal symbol on its frontier, called the anchor.
Treescan be combined through two simple operations :substitution 4 and furcation (de Smedt and Kempen,1990).
Those operations are theoretically equivalentto Tree Adjoining Grammar operations.
However anoriginal property of our Lexicaiized Tree Grammaris to integrate a set of semantic operations whichlay down additional constraints.
The parser han-dles semantic features, attached to the trees, andpropagates them according to specific rules (Rous-sel, 1996).
The result is a semantic representationbuilt synchronously to the syntactical tree.3020100-10-20-30-40-50i80 ?-70 I Ican you gh~eyOU give me the buget................................................. : ' ,~ ,~, .~ - -  /....................................................... ' ~"~ 32 :::: 1 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~"~,', ~ rankrank45 -----?
~.~,~, rank 8 .
.
.
.
.. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.,,?., //,,, ".,./;i ime is a bt~IgeaW~'d In lhe flrs~ bestFigure 2: word scores across N-best ranks for thebest hypothesis "can you give me is a budget"First evaluation of the filtering hints that it maybe a good guidance but not a sufficient criterion:some parameter settings, such as the threshold, re-main problematic.
Table 5 displays rather lim-Figure 3: elementary trees and attached semanticfeatures for the sentence "give me more informationabout the company"In figures 3 and 4, the heads of the trees are stan-dard syntactic categories, the star symbol on the4It should be borne in mind that the term substitutionwhen speaking of Tree Grammars has nothing to do withthe term substitution that refers to a recognition error77right or left of the head indicates an auxiliary treethat will combine with a compatible tree ; a X* headsymbol indicates a tree which combines with a nodeof category X on its right, a *X node combines witha node X on its left.
Nodes X0, X1, or more gener-ally Xn, are substitution sites, they are awaiting atree whose head symbol is X.
Substitution sites bearsyntactic and semantic constraints on their possi-ble substitutors.
Here, the semantic onstraints aremade visible in the node symbol (e.g.
N0-PERSONmeans the substitutor of this node must be of cate-gory N -noun-  and must possess a semantic feature:PERSON).The parsing reveals, through linguistic anomalies,errors that wouldn't be spotted efficiently by acous-tic criteria.
The linguistic context allows to enrich?
and complete the analysis in case of an error, eitherdetected uring the parsing as a linguistic anomalyor signalled previously from confidence scores.Actually, the robust parsing strategy articulatesaround a single parser, which is used iteratively ac-cording to the anomalies encountered.
Three passescan each provide analysis when anomalies are de-tected -for correct sentences, the first pass is suffi-cient.
Each pass will in turn modify the result of theprevious pass and hand it back to the parser.In the first pass, lexical items are first matchedwith their corresponding elementary tree in the lex-icon.
Concurrent trees for one item give rise to par-allel concurrent branches of parsing, but they aretaken into account in a local chart parsing.For example the verb want is associated in theCOVEN s lexicon with two entries, one for the in-finitive construction and one for the transitive con-struction.
As preposition to exists in the lexicon, asentence in which the words want and to appear callstwo lexicon matching, thus two parsing branches.Figure 4 displays the trees involved.
The parserwill select the right matching along the syntactico-semantic operations thanks to expectations of sub-stitution sites.The first pass includes a first feature of robust-ness since unreliable words signalled by the filteringas probable substitutions are represented by an au-tomatically generated "joker" tree.
A joker tree isan overspecified tree that cumulates emantic fea-tures from different candidates whose elementarytree share the same structure 6.
Several alternativejoker trees are generated when word candidates be-long to different categories.
Initially all semanticfeatures in an overspecified joker tree are marked5cf.
section 3.28joker trees are similar to elementary tree.
They canalso be defined manually to fit identified casesPNQ-PBtS(~,I kaNT NI-ENI'IIYISPTO ~-EXTITY\[ :ACTION = :REQUEST \]:L.QC = :D.STPAYI~IT TO PC-TEXS?-I/fC :ACTIO~I = :REQLI~TFigure 4: concurrent elementary trees and attachedsemantic features for the words want toas uncertain, not to confuse the higher levels, then,during the parsing the semantic features mobilisedfor the tree operations are relieved from their uncer-tain status.
To avoid a heavy combinatorial search,directly operations to combine two adjacent jokersare not attempted.Figure 5: analysis of "give me is a budget", recoveryfrom a substitutionConcerning insertions, the parser checks whethera local analysis is possible without a word suspectedto be inserted, if so, the decision is made to eliminatethe word, if not, the word is considered as substitu-tion, and processed as described above.
This is notan absolute criterion, in particular optional wordsfalsely considered to be insertions by the filteringare not recovered.The repairing capacities at this stage apply for in-stance to the case mentioned table 2.
In sentence"can you give me is a budget", the word a is markedas a substitution (cf.
3.2).
It triggers the genera-78tion of joker trees, the candidates a, the, this, theseare represented by a single joker tree while it, inthe 4 ~h best hypothesis, involves a different jokertree - i t  is in fact its own tree, but with semanticfeatures marked as uncertain.
The branch of pars-ing containing this joker is eliminated on syntacticgrounds, whereas the first branch of parsing turnsinto a complete analysis (figure 5).
The word iswhich is marked as a possible insertion is confirmedin its status and definitely eliminated.The second pass aims at recovering from would-bedeleted words by re-inserting expected co-occurringwords.
We use knowledge about co-occurrencesimplicitly described in some elementary trees: el-ementary trees defined for more than one anchorare now being selected even if all their anchors arenot present in the recognized sentence.
It is how-ever checked whether the anchors appear in givencompetitive recognition hypothesis at compatiblepositions 7.
In the following example in table 3 therecognizer (here, Abbot) has recognized the sentencewhom is this chair are too light instead of the actualutterance whom is this chair chosen by.uttered: WHOM IS THIS CHAIR CHOSEN BYHI: WHON IS THIS CHAIR ARE TOO LIGHTH2: WHOM IS THIS CHAIR TO AN BYH3: WHOM IS THIS CHAIR TO AN WALL IH4: WHOM IS THIS CHAIR CHOSEN ITHS: WHOM IS THIS CHAIR TO AN WALL MINDH6: WHOM IS THIS CHAIR TO AN WALL MYHT: WHOM IS THIS CHAIR TO AN BY ATable 3: N-best hypothesis for the sentence "whomis this chair chosen by"The sequence are ~oo light is spotted by the fil-tering as a probable substitution.
At pass one, theparser doesn't succeed in putting together the ele-mentary trees which span the whole sentence.Now, in pass two it is observed that in the surepart of the sentence whom is this chair, two wordswhom and be are the  beginning of several multi-anchor elementary trees.
The aligned candidateswith the sequence are too light allow to select onlyone multi-anchor tree WHOM-BE-N1-CHOSEN-BY.
Thisprovides a complete analysis.The second pass enables a lexical recovery.
Theknowledge exploited here about dependencies be-tween words at arbitrary distance can operate par-ticularly efficiently with an n-gram driven recog-nizer.
Indeed, the co-occurrences captured by ann-gram model suffer from a limited scope and anadjacency condition.~The position is figured out from the hypothesis align-ment, see section 3.1Figure 6: analysis of "whom is this chair chosen by";the origninal sentence is recoveredThe third pass differs from previous passes ; in-stead of initiating the recovery from the lexical el-ements at hand, it summons predictions from thegrammatical expectations.This pass is meant to detect he other errors andcomplete the analysis with underspecified elements.Each anomaly revealed by the parsing has thetrees around it examined to determine whether itis possible to restore a local well-formedness by in-serting a tree.Patterns of anomaly that fits in this case are de-fined in a compact way thanks to the general treetypes used in the grammar.
There are about twentypatterns, each of them is made to insert the requiredtree, in the form of an underspecified joker tree.
Thistype of joker tree has a full syntactic structure butundefined semantic features: some semantic featurescan be added along the syntactico semantic opera-tions.The third pass can chose to ignore joker trees in-troduced in the first pass.
This allows to correctirrelevant matching of joker in the first pass.
Thisoccurs when two words are substituted for a singleword, or when an insertion is classified as a substi-tution.ut tered :  CAN YOU GIVE ME HOP.E INFORMATION ABOUT TItE COMPANYSl : CAN YOU GIVE ME MORE INFORMATION THE COMPANYH2: CAN YOU GIVE ME MORE INFORMATION BY THE COMPANYH3: CAN YOU GIVE ME NORE INFORMATION THAT COMPANYH4: CAN YOU GIVE ME MORE INFORMATION ABOUT SECRETARYH5: CAN YOU GIVE ME MORE INFORMATION THE OVER COMPANYH6: CAN YOU GIVE ME MORE INFORMATION BOW TO THE COMPANYH7: CAN YOU GIVE ME MORE INFORMATION ONE THE COMPANYTable 4: N-best hypothesis for the sentence "can yougive me more information about the company"Example table 4 stands for a typical omission re-covery.
The word about was deleted so that neitherof the first passes can span the entire sentence.
Thethird pass succeeds in inferring an analysis by in-serting a generic prepositional tree that meets thesyntactic and semantic expectations (see figure 7).Yet the recovery lets the information introduced by79WellRecognizedSentencesIllRecognizedSentencescorrect preservingwrong filtering weaklyrecovered by the robust parsingCorrect filteringpartial filteringwrong filtering, sentencerightly rejected by the parsingwrong filtering, sentencefalsely analysed as well formedwrong filtering, sentenceanalysed through the robust parsingN,,ance Abbot32 % 62 %27,5% \[ 6,5%11% 8,5 %8,5% 5%0% 5%17 % 6,5 %4% 6,5%i00 % i00 % {lCorrectInterpretationPotentially CorrectInterpretationRejectionFalseInterpretationTable 5: results on filtering and subsequent repairing strategyFigure 7: analysis of "give me more information thecompany", recovery from an omission1 st pass 1 st to 2 nd pass 1 st to 3 ra pass\] 32,5 ms. 44 ms. 113 ms.Table 6: comparison of average cpu time requiredfor different parsing optionsthe preposition undefined.
However a look at com-patible aligned words in the N-best hypothesis caninstanciate the joker once an analysis is found.5 Evaluat ionThe parser has been tested on a 200 wordsapplication s .
The robust parsing runs in real time onan SGI Indigos2 Impact (R4400 250 MHz).
Table 6shows the processing performances for each parsingpass.Results on the repairing capacities according tothe filtering behaviour are presented in table 5.
"Weakly recovered" means that all the informa-tion is present in the semantic representation, butpart of it may be marked as uncertain with otherparasite information (see figure 5 for an example).
"Potentially correct interpretation" means that avalid semantic representation has been reached withSThe application task and the recognizer systems aredescribed section 3.3.some biased information.
This bias might be ignoredor detected by the higher level modules.
The lasttwo lines of the table distinguish between two kindsof wrongly filtered sentence: the first appear well-formed to the parser -there is no way to recover fromthose-, the second contain anomalies detected by theparser -there might be some way to repair or rejectthose ones.
It can be observed that the approachis basically non-destructive toward well-recognizedsentences.
There is a theoretical case that would re-sult in a loss of information: the false rejection ofan optional word.
But it didn't show up.
For ill-recognized sentences, at least 27% are fully recov-ered, for Nuance as well as for Abbot (this concernsline 3 of table 5).
In both cases too, a little less than50% appear difficult to recover, given the currentfiltering (last two lines of the table).806 Conc lus ionThe results enlighten the repairing capacities of acouple filtering module/robust parsing module.
Inaddition this couple presents some original desirablefeatures that we intend to push further.
First, al-though the parser belongs to the family of robustparsers -since it can process ill-formed sentence- itis still able to reject a subset of ill-formed sentences,which may be produced by a recognizer.
Second,thanks to the lexical recovery from word candidatesin the N-best hypothesis, the spoken input can bedecoded further.The scoring module can be seen as achieving notso much a filtering than a narrowing of the searchspace of recognition candidates.
However, the ap-proach has limitations: the parser cannot handle alarge number of candidates so that the number ofN-best must be limited and hence the correct candi-dates sometimes missed.Moreover, spurious hypothesis generated alongthe passes are still hard to eliminate.
This sug-gests the need for cross-checking with other knowl-edge sources, like statistical cues derived from textcorpora or from recognition errors corpora.To sum up, our work described an integration ofspeech recognition and language processing which isindependent from a given recognition system.
Thebasic idea was to make use of available acoustic in-formation in order to point out a limited set of wordsto suspect --especially inserted words- and to exploitthe potential of linguistic knowledge in order to re-pair the best sentence hypothesis.
It can serve as abasis for many more developments.Re ferencesde Smedt, K and G. Kempen.
1990.
Segment gram-mar : a formalism for incremental generation.
InC. Paris et al, editor, Natural anguage generationand computational linguistics.
Dodrecht, Kluwer.Dowding, J., R. Moore, F. Andry, and D. Moran.1994.
Interleaving syntax and semantics in an ef-ficient bottom-up arser.
In A CL '94.Hanrieder, G. and G. GSrz.
1995.
Robust parsing ofspoken dialogue using contextual knowledge andrecognition probabilities.
In ESCA Tutorial andResearch Workshop on Spoken Dialogue Systems,Denmark.Lamel, L., S.K.
Bennacef, H. Bonneau-Maynard,S.
Rosset, and J.L.
Gauvaln.
1995.
Recent de-velopments in spoken language systems for infor-mation retrieval.
In VIGSO'95, Denmark.Meteer, M. and R. Rohlicek.
1994.
Integrated tech-niques for phrase xtraction from speech.
In Hu-man Language Technology Workshop, pages 228-233.Normand, V., D. Pernel, and B. Bacconnet.
1997.Speech-based multimodal interaction in virtualenvironments: Research at the Thomson-CSF cor-porate research laboraties.
PRESENCE: Teleop-erators and Virtual Environments.
to appear aslab-review.Normand, V. and J. Tromp.
1996.
CollaborativeVirtual Environments : the COVEN project.
InFIVE'96, Pisa, December.Rayner, M., D. Carter, V. Digalakis, and P. Price.1994.
Combining knowledge sources to reorder N-Best speech ypothesis lists.
In Human LanguageTechnology Workshop, pages 217-221.Rivlin, Z.
1995.
Confidence measure for acousticlikelihood scores.
In Eurospeech'95.Roussel, D. 1996.
A lexicalized tree grammar withmorphological component for spoken languageprocessing : in french.
In Colloque Reprdsenta-tion et Outils pour les Bases Lexicales, Grenoble,November.Schabes, Y., A. Abeill@, and A. Joshi.
1988.
Pars-ing strategies with lexicalized grammars : Treeadjoining grammar.
In COLING'88, Budapest,pages 578-583.Suhm, B., B. Myers, and A. Walbel.
1996.
Inter-active recovery from speech recognition errors inspeech user interface.
In ICSLP'96, pages 865-868.Young, S.R.
1994.
Spoken dialog systems: Basic ap-proach and overview.
In NCAI'9~, Seattle, pages116-121.81
