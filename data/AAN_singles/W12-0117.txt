Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 129?138,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsUsing Sense-labeled Discourse Connectivesfor Statistical Machine TranslationThomas Meyer and Andrei Popescu-BelisIdiap Research InstituteRue Marconi 19, 1920 Martigny, Switzerland{thomas.meyer, andrei.popescu-belis}@idiap.chAbstractThis article shows how the automatic dis-ambiguation of discourse connectives canimprove Statistical Machine Translation(SMT) from English to French.
Connec-tives are firstly disambiguated in terms ofthe discourse relation they signal betweensegments.
Several classifiers trained usingsyntactic and semantic features reach state-of-the-art performance, with F1 scores of0.6 to 0.8 over thirteen ambiguous Englishconnectives.
Labeled connectives are thenused into SMT systems either by mod-ifying their phrase table, or by trainingthem on labeled corpora.
The best modi-fied SMT systems improve the translationof connectives without degrading BLEUscores.
A threshold-based SMT system us-ing only high-confidence labels improvesBLEU scores by 0.2?0.4 points.1 IntroductionCurrent approaches to Statistical Machine Trans-lation (SMT) have difficulties in modeling long-range dependencies between words, includingthose that are due to discourse-level phenomena.Among these, discourse connectives are wordsthat signal rhetorical relations between clauses orsentences.
Their translation often depends on theexact relation signaled in context, a feature thatcurrent SMT systems were not designed to cap-ture, hence their frequent mistranslations of con-nectives (see Section 2 below).In this paper, we present a series of experimentsthat aim to use, in SMT systems, data with au-tomatically labeled discourse connectives.
Sec-tion 3 first presents the data sets used in our ex-periments.
We designed classifiers that attempt toassign sense labels to ambiguous discourse con-nectives, and their scores compare favorably withthe state-of-the-art for this task, as shown in Sec-tion 4.
In particular, we consider WordNet rela-tions and temporal expressions as well as candi-date translations of connectives as additional fea-tures (Section 4.2).However, our main goal is not the disambigua-tion of connectives per se, but the use of the labelsassigned to connectives as additional input to anSMT system.
To the best of our knowledge, ourexperiments are the first attempts to combine con-nective disambiguation and SMT.
Three solutionsto this combination are compared in Section 5:modifying phrase tables, and training on data la-beled manually, or automatically, with senses ofconnectives.
We further show that a modifiedSMT system is best used when the confidence fora given label is high (Section 6).
The paper con-cludes with a comparison to related work (Sec-tion 7) and an outline of future work (Section 8).2 Discourse Connectives in TranslationDiscourse connectives such as although, however,since or while form a functional category oflexical items that are frequently used to markcoherence or discourse relations such as expla-nation, synchrony or contrast between units oftext or discourse.
For example, in the Europarlcorpus from years 199x (Koehn, 2005), thefollowing nine lexical items, which are often(though not always) discourse connectives, areamong the 400 most frequent tokens over atotal of 12,846,003 (in parentheses, rank andnumber of occurrences): after (244th/6485),although (375th/4062), however (110th/12,857),indeed (334th/4486), rather (316th/4688),129since (190th/8263), still (168th/9195), while(390th/3938), yet (331st/4532) ?
see also (Car-toni et al, 2011).
Discourse connectives canbe difficult to translate, because many of themcan signal different relations between clauses indifferent contexts.
Moreover, if a wrong connec-tive is used in translation, then a text becomesincoherent, as in the two examples below, takenfrom Europarl and translated (EN/FR) withMoses (Koehn et al, 2007) trained on the entirecorpus:1.
EN: This tax, though [contrast], does not comewithout its problems.FR-SMT: *Cette taxe, me?me si [concession],ne se pre?sente pas sans ses proble`mes.2.
EN: Finally, and in conclusion, Mr President,with the expiry of the ECSC Treaty, theregulations will have to be reviewed since[causal] I think that the aid system will haveto continue beyond 2002 .
.
.FR-SMT: *Enfin, et en conclusion, Monsieurle pre?sident, a` l?expiration du traite?
CECA,la re?glementation devra e?tre revu depuis que[temporal] je pense que le syste`me d?aidesdevront continuer au-dela` de 2002 .
.
.In the first example, the connective generatedby SMT (me?me si, literally ?even if?)
signals aconcession and not a contrast, for which the con-nective mais should have been used (as in the ref-erence).
In the second example, the connectivedepuis que (literally ?from the time?)
generatedby SMT expresses a temporal relation and not acausal one, which should have been conveyed e.g.by the French car.Such examples suggest that the disambiguationof connectives prior to translation could help SMTsystems to generate a correct connective in the tar-get language.
Of course, depending on the lan-guage pair, some ambiguities can be carried overfrom the source to the target language, so theyneed not be solved.
Still, improving the over-all translation of discourse connectives should in-crease the overall coherence of MT output, with apotential large impact on perceived quality.3 Data Used in Our ExperimentsFor both tasks, the disambiguation of connectivesand SMT, different training and testing data setsare available.
This section shows how we madeuse of these resources and how we augmentedthem by manual and automated annotation of thesenses of discourse connectives.3.1 Data for the Disambiguation ofDiscourse ConnectivesOne of the most important resources for discourseconnectives in English is the Penn DiscourseTreebank (Prasad et al, 2008).
The PDTB pro-vides a discourse-layer annotation over the WallStreet Journal Corpus (WSJ) and the Penn Tree-bank syntactic annotation.
The discourse anno-tation consists of manually annotated senses forabout 100 types of explicit connectives, for im-plicit ones, and their clause spans.
For the en-tire WSJ corpus of about 1,000,000 tokens thereare 18,459 instances of annotated explicit connec-tives.
The senses that discourse connectives cansignal are organized in a hierarchy with 4 toplevelsenses, followed by 16 subtypes on the secondlevel and 23 detailed subsenses on the third level.Studies making use of the PDTB to build classi-fiers usually split the WSJ corpus into Sections02?21 for training and Section 23 for testing (aswe did for our disambiguation experiments, seeSection 4).From the PDTB, we extracted the 13 most fre-quent and most ambiguous connectives: after, al-though, however, indeed, meanwhile, neverthe-less, nonetheless, rather, since, still, then, while,and yet.
This set shows in particular that connec-tives signaling contrastive or temporal senses arethe most ambiguous ones, hence they are also po-tentially difficult to translate, as this ambiguity isoften not preserved across languages (Danlos andRoze, 2011).
We used the senses from the sec-ond PDTB hierarchy level (as the third level is toofine-grained for EN/FR translation) and generatedthe training and testing sets listed with statistics inTable 1 (Section 4).In principle, classifiers trained on PDTB datacan be applied directly to label connectives overthe English side of the Europarl corpus (Koehn,2005) used for training and testing SMT.
How-ever, to control the difference in register fromnewswire texts to formal political speech, and toallow for future studies of other languages, wealso performed manual annotation (Cartoni et al,2011) of five connectives over the Europarl corpus(although, even though, since, though and while).130The manual annotation was performed on sub-sets of Europarl v5 (years 199x) for the first fewhundred occurrences of each connective.
Insteadof a potentially difficult and costly annotation ofsenses, as in the PDTB, we performed translationspotting, asking annotators to highlight the trans-lation of each of the five connectives in the Frenchside of the corpus.
From the list of all observedtranslations one can then cluster the necessarysense labels, as some target language connectivesclearly signal only one sense or, in cases whereambiguity is preserved, one can group the equallyambiguous connectives under one composite la-bel.
For example, while is sometimes translatedto the French discourse connectives tandis que oralors que which both preserve the ambiguity ofwhile signaling a temporal or contrastive sense.With this method we built the data sets listed withstatistics in Table 2 below (Section 4).3.2 Data for Statistical Machine TranslationThe translation data for our SMT experiments hasbeen often used in other MT research work and isfreely distributed for the shared tasks of the Work-shop on Machine Translation (WMT)1.For training our SMT systems, the EN/FR Eu-roparl corpus v5 was used in three ways to inte-grate data with labeled discourse connectives intoSMT: no changes (for MT phrase table modifica-tions), integration of manually annotated data andintegration of automatically labeled data.
Thesemethods are described below in Section 5 ?
here,we gather descriptions of the corresponding data.a: Modification of the phrase table: Europarl(346,803 sentences), labeling the translationmodel after training.b: Integration of manual annotation: Europarl(346,803 sentences), minus all 8,901 sen-tences containing one of the above 5 connec-tive types, plus 1,147 sentences with manu-ally sense-labeled connectives.c: Integration of automated annotation: Europarl?
years 199x (58,673 sentences), all occur-rences of the 13 PDTB subset connectivetypes have been labeled by classifiers (in6,961 sentences).For Minimum Error Rate tuning (MERT) (Och,2003) of the SMT systems, we used the 20091statmt.org/wmt10/translation-task.htmlNews Commentary (NC) EN/FR development setwith the following modifications:d: Phrase table: NC 2009 (2,051 sentences), nomodifications.e: Manual annotation: NC 2009 (2,051 sen-tences), minus all 123 sentences containingone of the above 5 connective types, plus 102sentences with manually sense-labeled con-nectives.f: Automated annotation: NC 2009 (2,051 sen-tences), all occurrences of the 13 PDTB sub-set connective types have been labeled byclassifiers (in 340 sentences).For testing our modified SMT systems, threetest sets were extracted in the following way:g: 35 sentences from NC 2007, with 7 occur-rences for each of the 5 connective typesabove, manually labeled.h: 62 sentences from NC 2007 and 2006 with oc-currences for the 13 PDTB connective types,automatically labeled with classifiers.i: 10,311 sentences from the EN/FR UN corpus,all occurrences of the five Europarl connec-tive types, automatically labeled with classi-fiers.These test sets might appear small compared tothe amount of data normally used for SMT systemtesting.
In our system evaluation however, apartfrom automated scoring, we also had to performmanual counts of improved translations, which iswhy we could not evaluate more than a hundredsentences (Section 5).
When counting manuallyfor test set (i), it was downsampled to the sameamount of 35 and 62 sentences as for sets (g)and (h), by extracting the first occurrences of eachconnective.In all experiments, we use the Moses Phrase-based SMT decoder (Koehn et al, 2007) and a 5-gram language model built over the entire Frenchpart of the Europarl corpus v5.4 Automatically DisambiguatingDiscourse Connectives4.1 Classifier PT: Trained on PDTB DataA first classifier (?PT?)
for ambiguous discourseconnectives and their senses was built by usingthe PDTB subset of 13 ambiguous connectives astraining material.
For each connective we built a131Connective Number of occurrences and senses F1 ScoresTraining set: total and per sense Test set: total and per sense PT PT+after 507 456 As, 51 As/Ca 25 22 As, 3 As/Ca 0.66 1.00although 267 135 Cs, 118 Ct, 14 Cp 16 9 Ct, 7 Cs 0.60 0.66however 176 121 Ct, 32 Cs, 23 Cp 14 13 Ct, 1 Cs 0.33 1.00indeed 69 37 Cd, 24 R, 3 Ca, 3 E, 2 I *2 2 R *0.50 *0.50meanwhile 117 66 Cj/S, 16 Cd, 16 S, 14Ct/S, 5 Ct10 5 S, 5 Ct/S 0.32 0.53nevertheless 26 15 Ct, 11 Cs 6 4 Cs, 2 Ct 0.44 0.66nonetheless 12 7 Cs, 3 Ct, 2 Cp *1 1 Cs *1.00 *1.00rather 10 6 R, 2 Al, 1 Ca, 1 Ct *1 1 Al *0.00 *0.00since 166 75 As, 83 Ca, 8 As/Ca 9 4 As, 3 Ca, 2As/Ca0.78 0.78still 114 56 Cs, 51 Ct, 7 Cp 13 9 Ct, 4 Cs 0.60 0.66then 145 136 As, 6 Cd, 3 As/Ca 6 5 As, 1 Cd 0.83 1.00while 631 317 Ct, 140 S, 79 Cs, 41Ct/S, 36 Cd, 18 Cp37 19 Ct, 10 S, 4 Cs,4 Ct/S0.93 0.96yet 80 46 Ct, 25 Cs, 9 Cp *2 2 Ct *0.5 *1.00Total 2,320 ?
142 ?
0.57 0.75Table 1: Performance of MaxEnt connective sense classifiers: Classifier PT (initial feature set) and ClassifierPT+ (with candidate translation features) for 13 temporal and contrastive connectives in the PDTB.
The senselabels are coded as follows.
Al: alternative, As: asynchronous, Ca: cause, Cd: condition, Cj: conjunction, Cp:comparison, Cs: concession, Ct: contrast, E: expansion, I: instantiation, R: restatement, S: synchrony.
In somecases marked with ?
*?, the test sets are too small to provide meaningful scores.specialized classifier, by using the Stanford Max-imum Entropy classifier package (Manning andKlein, 2003).
Maximum Entropy is known to han-dle discrete features well and has been appliedsuccessfully to connective disambiguation before(see Section 7).An initial set of features can directly be ob-tained from the PDTB (and must hence be con-sidered as oracle features): the (capitalized) con-nective token, its POS tag, first word of clause 1,last word of clause 1, first word of clause 2 (theone containing the explicit connective), last wordof clause 2, POS tag of the first word of clause 2,type of first word of clause 2, parent syntacticalcategories of the connective, punctuation patternof the sentences.
Apart from these standard fea-tures in discourse connective disambiguation weused WordNet (Miller, 1995) to compute lexicalsimilarity scores with the lesk metric (Baner-jee and Pedersen, 2002) for all the possible com-binations of nouns, verbs and adjectives in thetwo clauses, as well as antonyms found for theseword groups.
In addition, we used features thatare likely to help detecting temporal relations andwere obtained from the Tarsqi Toolkit (Verhagenand Pustejovsky, 2008), which annotates Englishsentences automatically with the TimeML anno-tation language for temporal expressions.
For ex-ample, in the sentence The crimes may appearsmall, but the prices can be huge (PDTB Sec-tion 2, WSJ file 0290), for example, our featureswould indicate the antonyms small vs. huge thatsignal the contrast, along with a temporal order-ing of the event appear before the event can.We report the classifier performances as F1scores for each connective (weighting precisionand recall equally) in Table 1, testing on Section23 of the PDTB.
This sense classifier will be re-ferred to as Classifier PT in the rest of the paper,in particular when used for the SMT experiments.4.2 Classifier PT+: With CandidateTranslations as FeaturesIn an attempt to improve Classifier PT, we addeda new type of feature, resulting in Classifier PT+.Namely, we used candidate translations of dis-course connectives from a baseline SMT system(not adapted to connectives).
To find these values,a Moses baseline decoder was used to translate thePDTB data, which was then word-aligned (En-132Connective Number of occurrences and senses F1Size of training set: total and per sense Test set: total and per sense Scorealthough 173 155 Cs, 18 Ct 10 5 Cs, 5 Ct 0.67even though 179 165 Cs, 14 Ct 10 5 Cs, 5 Ct 1.00since 413 274 S, 131 Ca, 8 S/Ca 10 5 Ca, 3 S, 2 S/Ca 0.80though 150 80 Cs, 70 Ct 10 5 Cs, 5 Ct 1.00while 280 130 Cs, 41 Ct, 89 S/Ct, 13 S/Ca, 7 S 14 4 Cs, 2 Ct, 2 S/Ct, 2S/Ca, 4 S0.64Total 1,195 ?
54 ?
0.82Table 2: Performance of a MaxEnt connective sense classifier (Classifier EU) for 5 connectives in the Europarlcorpus.
The sense labels are coded as follows.
Cs: Concession, Ct: Contrast, S: Synchrony, Ca: Cause.glish source with target French) by using GIZA++(Och and Ney, 2003).
In this alignment, wesearched for the translation equivalents of the 13PDTB connectives by using a hand-crafted dic-tionary of possible French translations.
When thetranslation candidate is not ambiguous ?
e.g.
bienque as a translation for while clearly signals a con-cession ?
its specific sense label was added as thevalue of an additional feature.
In some cases,however, the values of the features are not de-termined (and are set to NONE): either when theSMT system or GIZA++ failed in translating oraligning a connective, or when the target connec-tive was just as ambiguous as the source one (e.g.while translated as tandis que, which can be la-beled both temporal or contrast).
Overall, thisprocedure led to an accuracy gain of ClassifierPT+ with respect to Classifier PT of about 0.1 to0.6 F1 score for some of the connectives, as canbe seen in the last column of Table 1.4.3 Classifier EU: Trained on Europarl DataAs explained in Section 3.1, we performed man-ual annotation of connective senses in Europarlas well, to provide labeled instances directly inthe data used for SMT training and to account forthe register change.
For the Europarl data sets,we built a new MaxEnt classifier (called Classi-fier EU) using the same feature set as ClassifierPT.
However, all features were this time extractedautomatically (no oracle).
In particular, we usedCharniak and Johnson?s (2005) parser to then ex-tract the syntactic features.
In Table 2, we re-port the results of Classifier EU, again in termsof F1 scores.
For all three classifiers, PT, PT+and EU, the F1 scores are in a range of 0.6 and0.8, thus comparing favorably to the state-of-the-art for discourse connective disambiguation withdetailed senses (Section 7).
Classifier EU alsocompares favorably to PT and PT+, as seen for in-stance for since (0.80 vs. 0.78) or although (0.67vs.
0.60?0.66).5 Use of Labeled Connectives for SMTIn this section, we report on experiments thatstudy the effect of discourse connective labelingon SMT.
The experiments differ with respect tothe method used for taking advantage of the la-bels, but also with respect to the data sets and thesense classifiers that are used.5.1 Evaluation Metrics for MTThe variation in MT quality can be estimated inseveral ways.
On the one hand, we use the BLEUmetric (Papineni et al, 2002) with one referencetranslation as is most often done in current SMTresearch2.
To improve confidence in the BLEUscores, especially when test sets are small, wealso compute BLEU scores using bootstrappingof data sets (Zhang and Vogel, 2010); the testsets are re-sampled a thousand times and the av-erage BLEU score is computed from individualsample scores.
The BLEU approach is not likely,however, to be sensitive enough to the small dif-ferences due to the correction of discourse con-nectives (less than one word per sentence).
Wetherefore additionally resort to a manual evalua-tion metric, referred to as ?Connectives, whichcounts the occurrences of connectives that are bet-ter translated by our modified systems comparedto the baseline ones.2The scores are generated by the NIST MTeval scriptversion 11b, available from www.itl.nist.gov/iad/mig/tools/.133MT system N. Connectives in MT test data ?Conn.
(%) BLEU scoresOcc.
Types Labeling + = ?
Standard BootstrapModified phrase table 1 35 5 manual 29 51 20 39.92 40.542 10,311 5 Cl.
EU 34 46 20 22.13 23.63Trained on manual 3 35 5 manual 32 57 11 41.58 42.38annotations 4 10,311 5 Cl.
EU 26 66 8 22.43 24.00Trained on automatic 5 62 13 Cl.
PT 16 60 24 14.88 15.96annotations (Cl.
PT) 6 10,311 5 Cl.
EU 16 66 18 19.78 21.17Trained on automatic 7 62 13 Cl.
PT+ 11 70 19 15.67 16.73annotations (Cl.
PT+) 8 10,311 5 Cl.
EU 18 68 14 20.14 21.55Table 3: MT systems dealing with manually and automatically (PT, PT+, EU) sense-labeled connectives: BLEUscores (including bootstrapped ones) and variation in the translation of individual connectives (?Connectives,as a percentage).
The description of each condition and the baseline BLEU scores are in the text of the article.5.2 Phrase Table ModificationA first way of using labeled connectives is tomodify the phrase table of an SMT system previ-ously trained/tuned on data sets (a)/(d) from Sec-tion 3.2, in order to force it to translate each spe-cific sense of a discourse connective (as indicatedby its label) with an acceptable equivalent se-lected among those learned from the training data.Of course, this only handles cases when connec-tives are translated by explicit lexical items (typ-ically, target connectives) and not by more com-plex grammatical constructs.The phrase table modification is done as fol-lows.
Based on a small dictionary of the five con-nective types of Table 2, their acceptable Frenchequivalents and the possible senses, the initialphrase table is searched for phrases containing aconnective and each occurrence is inspected tofind out which sense is reflected in the transla-tion.
If the sense is non-ambiguous, then the ta-ble entry is modified to include the label, and theprobability score is set to 1 in order to maximizethe chance that the respective translation is foundduring decoding.
For instance, for every phrasetable entry where while is translated as alors que,this corresponds to a contrastive use and while ischanged into while CONTRAST.
Or, for the en-tries where while is translated as bien que, thelexical entry is changed into while CONCESSION.However, when the source entry is as ambiguousas the target one, no modification is made.
Thismeans that during decoding (testing) with labeledsentences, these entries will never be used.The results of the SMT system are shown inexperiments 1 and 2 in Table 3, respectively test-ing over data set (g) (7 manually annotated sen-tences for each of the 5 connectives) and overset (i), in which the 5 connectives were automat-ically labeled with Classifier EU.
In the first test,the translations of 29% of the connectives are im-proved by the modified system, while 20% aredegraded and 51% remain unchanged ?
thus re-flecting an overall 10% improvement in the trans-lations of connectives (?Connectives).
How-ever, for this test set, the BLEU score is about 3points below the baseline SMT system that usedthe same phrase table without modification of la-bels and scores (not shown in Table 3).
In exper-iment 2, however, the BLEU score of the modi-fied system is in the same range as the baselineone (22.13 vs. 22.76).
As for ?Connectives,as it was not possible to score manually all the10,311 connectives, we sampled 35 sentences andfound that 34% of the connectives are improved,20% are degraded and 46% remain unchanged,again reflecting an improvement in the translationof connectives.
This shows that piping automaticlabeling and SMT with a modified phrase tabledoes not degrade the overall BLEU score, whileincreasing ?Connectives.5.3 Training on Tagged CorporaWe explored a more principled way to integrateexternal labels into SMT, by using labeled data(manually or automatically) for training, so thatthe system directly learns a modified phrase tablewhich allows the translation of labeled data (auto-matically) when testing.1345.3.1 Manual Gold AnnotationWe report first two experiments using the man-ual gold annotation for the five connective typesover Europarl excerpts, used for training.
Whenused also for testing (experiment 3 in Table 3),this can be seen as an oracle experiment, measur-ing the translation improvement when connectivesense labeling is perfect.
However, in experiment4, the SMT system uses the output of an auto-matic labeler.
For training/tuning we used datasets (b)/(e), Section 3.2.In experiment 3, for test set (g), 32% of theconnectives were translated better by the modi-fied system, 57% remained the same, and 11%were degraded.
In experiment 4, over a 35 sen-tence sample of the bigger test set (i), 26% wereimproved, 66% remained the same, and only8% were degraded.
The baseline SMT system(not shown in Table 3) was built with the sameamounts of unlabeled training and tuning data.Overall, the BLEU scores of our modified systemsare similar to the baseline ones, though still lower?
41.58 vs. 42.77 for experiment 3, and 22.43vs.
22.76 for experiment 4, also confirmed by thebootstrapped scores.Another comparison shows that the systemtrained on manual annotations (exp.
4) outper-forms the system using a modified phrase ta-ble (exp.
2) in terms of BLEU scores (22.43 vs.22.13) and bootstrapped ones (24.00 vs. 23.63).5.3.2 Automated AnnotationWe evaluated an SMT system trained on datathat was automatically labeled using the classi-fiers in Section 4.
This method provides a largeamount of imperfect training data, and uses nomanual annotations at all, except for the initialtraining of the classifiers.
For these experiments(5 and 6 in Table 3), the BLEU scores as wellas the manual counts of improved connectives arelower than in the preceding experiments because,overall, less training/tuning data was used ?
about15% of Europarl, data sets (c) and (f) in Sec-tion 3.2.
The baseline system was built over thesame amount of data, with no labels.Testing here was performed over the slightlybigger test set (h) with 62 sentences (13 connec-tive types).
The occurrences were tagged withClassifier PT prior to translation (exp.
5).
Com-pared to the baseline system, the translations of16% of the connectives were improved, while60% remained the same and 24% were degraded.In experiment 6, the 10,311 UN occurrences for 5connective types were first tagged with ClassifierEU.
Evaluated on a sample of 62 sentences, 16%of the connectives were improved, while 66% re-mained the same and 18% were degraded.
De-spite less training data, in terms of BLEU, the dif-ference to the respective baseline system (scoresnot shown in Table 3) is similar in both experi-mental settings: 19.78 vs. 20.11 for experiment6 (automated annotation), compared to 22.43 vs.22.76 for experiment 4 (manual annotation).Finally, we carried out two experiments (7and 8) with Classifier PT+, which uses as addi-tional features the translation candidates and has ahigher accuracy than PT (Section 4.2).
As a result,the translation of connectives (?Connectives) isindeed improved compared (respectively) to ex-periments 5 and 6, as it appears from lines 7?8of Table 3.
Also, the BLEU scores of the corre-sponding SMT systems are increased in experi-ments 7 vs. 5 and in 8 vs. 6, and are now equalto the baseline ones (for experiment 8: 20.14 vs.20.11, or, bootstrapped, 21.55 vs. 21.55).The results of experiments 7/8 vs. 5/6 in-dicate that improved classifiers for connec-tives also improve SMT output as measured by?Connectives, with BLEU remaining fairlyconstant, and therefore are worth investigatingin more depth in the future.
When compar-ing manual (experiments 3/4) vs. automated an-notation (experiments 5/6/7/8) and their use inSMT, the differences in the scores (BLEU and?Connectives) highlight a trade-off: manuallyannotated data used for training leads to betterscores, but noisier and larger training data that isannotated automatically is an acceptable solutionwhen manual annotations are not available.6 Classifier Confidence ScoresAs shown with the above experiments, the accu-racy of the connective classifiers influences SMTquality.
We therefore hypothesize that an SMTsystem dealing with labeled connectives wouldbest be used when the confidence of the classi-fier is high, while a generic SMT system could beused for lower confidence values.We experimented with the confidence scores ofClassifier EU, which assigns a score between 0and 1 to each of its decisions on the connectives?labels.
(All processing is automatic in these ex-135(a) although (b) sinceFigure 1: Use of a combined system (COMB) that directs the input sentences either to a system trained on a sense-labeled corpus (TTC) or to a baseline one (BASE), depending on the confidence of the connective classifier.
Thex-axis shows the threshold above which TTC is used ?
BASE being used below it ?
and the y-axis shows theBLEU scores of COMB with respect to TTC and BASE.
Figure (a) is for although and (b) for since.periments, and the evaluation is done solely interms of BLEU).
We defined a threshold-basedprocedure to combine SMT systems: if the con-fidence for a sense label is above a certain thresh-old, then the sentence is translated by an SMTsystem trained on labeled data from experiment4 (or ?tagged corpus?, hence noted TTC), and if itis below the threshold, it is sent to a baseline sys-tem (noted BASE).
The resulting BLEU scores ofthe combined system (COMB) obtained for vari-ous threshold values are shown in Figure 1 for twoconnectives.Firstly, we considered all the 1,572 sentencesfrom the UN corpus which contained the connec-tive although, labeled either as contrast or con-cession.
We show BLEU scores of the COMBsystem for several thresholds in the interval of ob-served confidence scores, along with the scores ofBASE and TTC, in Figure 1(a).
The results showthat the scores of COMB increase with the valueof the threshold, and that for at least one valueof the threshold (0.95) COMB outperforms bothTTC and BASE by 0.20 BLEU points.To confirm this finding with another connec-tive, we took the first 1,572 sentences containingthe connective since from the UN corpus.
TheBLEU scores for COMB are shown for the rangeof observed confidence values (0.4?1.0) in Fig-ure 1(b).
For several values of the threshold,COMB outperforms both BASE and TTC, in par-ticular for 0.85, with a difference of 0.39 BLEUpoints.The significance of the observed improvementwas tested as follows.
For each of the two con-nectives, we split the test sets of 1,572 sentenceseach in five folds, and compared for each fold thescores of COMB for the best performing thresh-old (0.95 or 0.85) with the highest of BASE orTTC (i.e.
BASE for although and TTC for since).We performed a paired t-test to compute the sig-nificance of the difference, and found p = 0.12 foralthough.
This value, although slightly above theconventional boundary of 0.1, shows that the fivepairs of scores reflect a significant difference inquality.
Similarly, when performing a t-test forsince, the difference in scores is found significantat the 0.01 level (p = 0.005).
Of course, COMBis always significantly better than the lower ofBASE or TTC (p < 0.05).
In the future, the sys-tem combination will be tested for all connectives,and the respective values of the thresholds will beset on tuning, not on test data.7 Related WorkDiscourse parsing (Marcu, 2000) has proven tobe a difficult task, even when complex models(CRFs, SVMs) are used (Wellner, 2009; Her-nault et al, 2010).
The performance of discourseparsers is in a range of 0.4 to 0.6 F1 score.136With the release of the PDTB, recent researchfocused on the disambiguation of discourse con-nectives as a task in its own right.
For the disam-biguation of explicit connectives, the state-of-the-art performance for labeling all types of connec-tives in English is quite high.
In the PDTB data,the disambiguation of discourse vs. non-discourseuses of connectives reaches 97% accuracy (Lin etal., 2010).
The labeling of the four main sensesfrom the PDTB sense hierarchy (temporal, contin-gency, comparison, expansion) reaches 94% ac-curacy (Pitler and Nenkova, 2009) ?
however, thebaseline accuracy is already around 85% when us-ing only the connective token as a feature.
Vari-ous methods for classification and feature analy-sis have been proposed (Wellner et al, 2006; El-well and Baldridge, 2008).
Other studies havefocused on the analysis of highly ambiguous dis-course connectives only.
Miltsakaki et al (2005)report classification results for the connectivessince, while and when.
Using a Maximum En-tropy classifier, they reach 75.5% accuracy forsince, 71.8% for while and 61.6% for when.
Asthe PDTB was not completed at that time, the datasets and labels are not exactly identical to the onesthat we used above (see Section 4).The disambiguation of senses signaled by dis-course connectives can be seen as a word sensedisambiguation (WSD) problem for functionalwords (as opposed to WSD for content words,which is more frequently studied).
The integra-tion of WSD into SMT has especially been stud-ied by Carpuat and Wu (2007), who used thetranslation candidates output by a baseline SMTsystem as word sense labels.
This is similar toour use of translation candidates as an additionalfeature for classification in Section 4.2.
Then,the output of several classifiers based on linguis-tic features was weighed against the translationcandidates output by the baseline SMT system.With this procedure, their WSD+SMT system im-proved the BLEU scores by 0.4?0.5 for the En-glish/Chinese pair.Chang et al (2009) use a LogLinear classi-fier with linguistic features in order to disam-biguate the Chinese particle ?DE?
that has five dif-ferent context-dependent uses (modifier, preposi-tion, relative clause etc.).
When the classifier isused to annotate the particle prior to SMT, theoutput of the translation system improves by upto 1.49 BLEU score for phrase-based Chinese toEnglish translation.
Ma et al (2011) use a Maxi-mum Entropy model to POS tag English colloca-tional particles (e.g.
come down/by, turn against,inform of ) more specifically than a usual POS tag-ger does (where only one label is given to all par-ticles).
The authors claim the usefulness of sucha particle tagger for English/Chinese translation,but do not show its actual integration into an MTsystem.These approaches, as well as ours, show thatintegrating discourse information into SMT ispromising and deserves future examination.
Thedisambiguation of word senses, including func-tion words, can improve SMT output when thesenses are annotated in a pre-processing step thatuses classifiers based on linguistic features atthe semantic and discourse levels, which are notavailable to a state-of-the-art SMT systems.8 Conclusion and Future WorkThis paper has presented methods and results forthe disambiguation of temporal and contrastivediscourse connectives using MaxEnt classifierswith syntactic and semantic features, in Englishtexts, in terms of senses intended to help SMT.These classifiers have been used to perform exper-iments with connective-annotated data applied toEN/FR SMT systems.
The results have shown animprovement in the translation of connectives forfully automatic systems trained on either hand-labeled or automatically-labeled data.
Moreover,BLEU scores were significantly improved by 0.2?0.4 when such systems were only used for con-nectives that had been disambiguated with highconfidence.In future work we plan to improve the senseclassifiers using additional features, to improvetheir integration with SMT, and to unify our datasets through additional manual annotations overEuroparl.
The applicability of the method to otherlanguages will also be demonstrated experimen-tally.AcknowledgmentsWe are grateful for the funding of thiswork to the Swiss National Science Foun-dation (SNSF), under the COMTIS Sin-ergia Project n. CRSI22 127510 (seewww.idiap.ch/comtis/).137ReferencesSatanjeev Banerjee and Ted Pedersen.
2002.
AnAdapted Lesk Algorithm for Word Sense Disam-biguation Using WordNet.
In Alexander Gel-bukh, editor, Computational Linguistics and Intelli-gent Text Processing, LNCS 2276, pages 117?171.Springer, Berlin/Heidelberg.Marine Carpuat and Dekai Wu.
2007.
Improving Sta-tistical Machine Translation using Word Sense Dis-ambiguation.
Proc.
of EMNLP-CoNLL, pages 61?72, Prague.Bruno Cartoni, Sandrine Zufferey, Thomas Meyer, andAndrei Popescu-Belis.
2011.
How Comparableare Parallel Corpora?
Measuring the Distribution ofGeneral Vocabulary and Connectives.
Proc.
of the4th Workshop on Building and Using ComparableCorpora (BUCC), pages 78?86, Portland, OR.Pi-Chuan Chang, Dan Jurafsky, and Christopher D.Manning.
2009.
Disambiguating ?DE?
for Chinese-English Machine Translation.
Proc.
of the FourthWorkshop on Statistical Machine Translation atEACL-2009, Athens.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best Parsing and MaxEnt DiscriminativeReranking.
Proc.
of the 43rd Annual Meeting of theACL, pages 173?180, Ann Arbor, MI.Laurence Danlos and Charlotte Roze.
2011.
Traduc-tion (Automatique) des Connecteurs de Discours.Actes 18e Confe?rence sur le Traitement Automa-tique des Langues Naturelles (TALN), Montpellier.Robert Elwell and Jason Baldridge.
2008.
DiscourseConnective Argument Identification with Connec-tive Specific Rankers.
Proc.
of the 2nd IEEEInternational Conference on Semantic Computing(ICSC), pages 198?205, Santa Clara, CA.Hugo Hernault, Helmut Prendinger, David A. duVerle,and Mitsuru Ishizuka.
2010.
HILDA: A DiscourseParser using Support Vector Machine classification.Dialogue and Discourse, 3(1):1?33.Philipp Koehn, et al 2007.
Moses: Open SourceToolkit for Statistical Machine Translation.
Proc.of 45th Annual Meeting of the ACL, DemonstrationSession, pages 177?180, Prague.Philipp Koehn.
2005.
Europarl: A Parallel Corpus forStatistical Machine Translation.
Proc.
of MT Sum-mit X, pages 79?86, Phuket.Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2010.A PDTB-styled End-to-end Discourse Parser.
Tech-nical Report TRB8/10, School of Computing, Na-tional University of Singapore.Jianjun Ma, Degen Huang, Haixia Liu, and WenfengSheng.
2011.
POS Tagging of English Particlesfor Machine Translation.
Proc.
of MT Summit XIII,pages 57?63, Xiamen.Christopher Manning and Dan Klein.
2003.
Opti-mization, MaxEnt Models, and Conditional Estima-tion without Magic.
Tutorial at HLT-NAACL and41st ACL conferences, Edmonton, AB and Sapporo.Daniel Marcu.
2000.
The Theory and Practice ofDiscourse Parsing and Summarization.
A BradfordBook.
The MIT Press, Cambridge, MA.George A. Miller.
1995.
WordNet: A LexicalDatabase for English.
Communications of the ACM,38(11):39?41.Eleni Miltsakaki, Nikhil Dinesh, Rashmi Prasad, Ar-avind Joshi, and Bonnie Webber.
2005.
Exper-iments on Sense Annotations and Sense Disam-biguation of Discourse Connectives.
Proc.
of the4th Workshop on Treebanks and Linguistic Theories(TLT), Barcelona.Franz Josef Och and Hermann Ney.
2003.
A System-atic Comparison of Various Statistical AlignmentModels.
Computational Linguistics, 29(1):19?51.Franz Josef Och.
2003.
Minimum Error Rate Train-ing in Statistical Machine Translation.
Proc.
of the41st Annual Meeting of the ACL, pages 160?167,Sapporo.Kishore Papineni, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2002.
BLEU: A method for Au-tomatic Evaluation of Machine Translation.
Proc.of 40th Annual Meeting of the ACL, pages 311?318,Philadelphia, PA.Emily Pitler and Ani Nenkova.
2009.
Using Syntaxto Disambiguate Explicit Discourse Connectives inText.
Proc.
of the 47th Annual Meeting of the ACLand the 4th International Joint Conference of theAFNLP (ACL-IJCNLP), Short Papers, pages 13?16,Singapore.Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-sakaki, Livio Robaldo, Aravind Joshi, and Bon-nie Webber.
2008.
The Penn Discourse Treebank2.0.
Proc.
of the 6th International Conference onLanguage Resources and Evaluation (LREC), pages2961?2968, Marrakech.Marc Verhagen and James Pustejovsky.
2008.
Tem-poral Processing with the TARSQI Toolkit.
Proc.of the 22nd International Conference on Com-putational Linguistics (COLING), Demonstrations,pages 189?192, Manchester, UK.Ben Wellner, James Pustejovsky, Catherine Havasi,Roser Sauri, and Anna Rumshisky.
2006.
Classi-fication of Discourse Coherence Relations: An Ex-ploratory Study using Multiple Knowledge Sources.Proc.
of the 7th SIGdial Meeting on Discourse andDialog, pages 117?125, Sydney.Ben Wellner.
2009.
Sequence Models and RankingMethods for Discourse Parsing.
PhD thesis, Bran-deis University, Waltham, MA.Ying Zhang and Stefan Vogel.
2010.
SignificanceTests of Automatic Machine Translation EvaluationMetrics.
Machine Translation, 24(1):51?65.138
