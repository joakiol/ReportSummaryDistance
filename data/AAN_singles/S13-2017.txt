Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 98?102, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsMELODI: Semantic Similarity of Words and Compositional Phrasesusing Latent Vector WeightingTim Van de CruysIRIT, CNRStim.vandecruys@irit.frStergos AfantenosIRIT, Toulouse Universitystergos.afantenos@irit.frPhilippe MullerIRIT, Toulouse Universityphilippe.muller@irit.frAbstractIn this paper we present our system for theSemEval 2013 Task 5a on semantic similar-ity of words and compositional phrases.
Oursystem uses a dependency-based vector spacemodel, in combination with a technique calledlatent vector weighting.
The system computesthe similarity between a particular noun in-stance and the head noun of a particular nounphrase, which was weighted according to thesemantics of the modifier.
The system is en-tirely unsupervised; one single parameter, thesimilarity threshold, was tuned using the train-ing data.1 IntroductionIn the course of the last two decades, vector spacemodels have gained considerable momentum for se-mantic processing.
Initially, these models only dealtwith individual words, ignoring the context in whichthese words appear.
More recently, two different butrelated approaches emerged that take into accountthe interaction between different words within a par-ticular context.
The first approach aims at building ajoint, compositional representation for larger unitsbeyond the individual word level (e.g., the com-posed, semantic representation of the noun phrasecrispy chips).
The second approach, different but re-lated to the first one, computes the specific meaningof a word within a particular context (e.g.
the mean-ing of the noun bank in the context of the adjectivebankrupt).In this paper, we describe our system for the Sem-Eval 2013 Task 5a: semantic similarity of words andcompositional phrases ?
which follows the latter ap-proach.
Our system uses a dependency-based vectorspace model, in combination with a technique calledlatent vector weighting (Van de Cruys et al 2011).The system computes the similarity between a par-ticular noun instance and the head noun of a par-ticular noun phrase, which was weighted accordingto the semantics of the modifier.
The system is en-tirely unsupervised; one single parameter, the simi-larity threshold, was tuned using the training data.2 Related workIn recent years, a number of methods have been de-veloped that try to capture the compositional mean-ing of units beyond the individual word level withina distributional framework.
One of the first ap-proaches to tackle compositional phenomena in asystematic way is Mitchell and Lapata?s (2008) ap-proach.
They explore a number of different mod-els for vector composition, of which vector addition(the sum of each feature) and vector multiplication(the elementwise multiplication of each feature) arethe most important.
Baroni and Zamparelli (2010)present a method for the composition of adjectivesand nouns.
In their model, an adjective is a linearfunction of one vector (the noun vector) to anothervector (the vector for the adjective-noun pair).
Thelinear transformation for a particular adjective is rep-resented by a matrix, and is learned automaticallyfrom a corpus, using partial least-squares regression.Coecke et al(2010) present an abstract theoreticalframework in which a sentence vector is a functionof the Kronecker product of its word vectors, whichallows for greater interaction between the different98word features.
And Socher et al(2012) present amodel for compositionality based on recursive neu-ral networks.Closely related to the work on compositionalityis research on the computation of word meaning incontext.
Erk and Pado?
(2008, 2009) make use ofselectional preferences to express the meaning ofa word in context.
And Dinu and Lapata (2010)propose a probabilistic framework that models themeaning of words as a probability distribution overlatent factors.
This allows them to model contex-tualized meaning as a change in the original sensedistribution.Our work takes the latter approach of computingword meaning in context, and is described in detailbelow.3 MethodologyOur method uses latent vector weighting (Van deCruys et al 2011) in order to compute a se-mantic representation for the meaning of a wordwithin a particular context.
The method reliesupon a factorization model in which words, togetherwith their window-based context features and theirdependency-based context features, are linked to la-tent dimensions.
The factorization model allows usto determine which dimensions are important for aparticular context, and adapt the dependency-basedfeature vector of the word accordingly.
The mod-ified feature vector is then compared to the targetnoun feature vector with the cosine similarity func-tion.This following sections describe our model inmore detail.
In section 3.1, we describe non-negative matrix factorization ?
the factorizationtechnique that our model uses.
Section 3.2 describesour way of combining dependency-based contextfeatures and window-based context features withinthe same factorization model.
Section 3.3, then, de-scribes our method of computing the meaning of aword within a particular context.3.1 Non-negative Matrix FactorizationOur latent model uses a factorization techniquecalled non-negative matrix factorization (Lee andSeung, 2000) in order to find latent dimensions.
Thekey idea is that a non-negative matrix A is factorizedinto two other non-negative matrices, W and HAi?
j ?Wi?kHk?
j (1)where k is much smaller than i, j so that both in-stances and features are expressed in terms of a fewcomponents.
Non-negative matrix factorization en-forces the constraint that all three matrices must benon-negative, so all elements must be greater than orequal to zero.Using the minimization of the Kullback-Leiblerdivergence as an objective function, we want to findthe matrices W and H for which the divergencebetween A and WH (the multiplication of W andH) is the smallest.
This factorization is carriedout through the iterative application of update rules.Matrices W and H are randomly initialized, and therules in 2 and 3 are iteratively applied ?
alternatingbetween them.
In each iteration, each vector is ade-quately normalized, so that all dimension values sumto 1.Ha?
?Ha?
?i WiaAi?(WH)i?
?k Wka(2)Wia?Wia??
Ha?Ai?(WH)i?
?v Hav(3)3.2 Combining syntax and context wordsUsing an extension of non-negative matrix fac-torization (Van de Cruys, 2008), it is possibleto jointly induce latent factors for three differentmodes: nouns, their window-based context words,and their dependency-based context features.
Theintuition is that the window-based context wordsinform us about broad, topical similarity, whereasthe dependency-based features get at a tighter,synonym-like similarity.
As input to the algo-rithm, two matrices are constructed that capture thepairwise co-occurrence frequencies for the differentmodes.
The first matrix contains co-occurrence fre-quencies of words cross-classified by dependency-based features, and the second matrix contains co-occurrence frequencies of words cross-classified bywords that appear in the word?s context window.NMF is then applied to the two matrices, and theseparate factorizations are interleaved (i.e.
matrixW, which contains the nouns by latent dimensions,99is shared between both factorizations).
A graphicalrepresentation of the interleaved factorization algo-rithm is given in figure 1.
The numbered arrows in-dicate the sequence of the updates.= W=UIVKIAnouns xdependenciesBnouns xcontext wordsIHK U3214 GK VFigure 1: A graphical representation of the interleavedNMFWhen the factorization is finished, the three dif-ferent modes (words, window-based context wordsand dependency-based context features) are all rep-resented according to a limited number of latent fac-tors.The factorization that comes out of the NMFmodel can be interpreted probabilistically (Gaussierand Goutte, 2005; Ding et al 2008).
More specifi-cally, we can transform the factorization into a stan-dard latent variable model of the formp(wi,d j) =K?z=1p(z)p(wi|z)p(d j|z) (4)by introducing two K?K diagonal scaling matricesX and Y, such that Xkk = ?i Wik and Ykk = ?
j Hk j.The factorization WH can then be rewritten asWH = (WX?1X)(YY?1H)= (WX?1)(XY)(Y?1H)(5)such that WX?1 represents p(wi|z), (Y?1H)T rep-resents p(d j|z), and XY represents p(z).
UsingBayes?
theorem, it is now straightforward to deter-mine p(z|d j).p(z|d j) =p(d j|z)p(z)p(d j)(6)3.3 Meaning in Context3.3.1 OverviewUsing the results of the factorization model de-scribed above, we can now adapt a word?s featurevector according to the context in which it appears.Intuitively, the context of the word (in our case,the dependency-based context feature that acts as anadjectival modifier to the head noun) pinpoint theimportant semantic dimensions of the particular in-stance, creating a probability distribution over latentfactors.
The required probability vector, p(z|d j), isyielded by our factorization model.
This probabil-ity distribution over latent factors can be interpretedas a semantic fingerprint of the passage in which thetarget word appears.
Using this fingerprint, we cannow determine a new probability distribution overdependency features given the context.p(d|d j) = p(z|d j)p(d|z) (7)The last step is to weight the original probabilityvector of the word according to the probability vec-tor of the dependency features given the word?s con-text, by taking the pointwise multiplication of prob-ability vectors p(d|wi) and p(d|d j).p(d|wi,d j) = p(d|wi) ?
p(d|d j) (8)Note that this final step is a crucial one in ourapproach.
We do not just build a model based onlatent factors, but we use the latent factors to de-termine which of the features in the original wordvector are the salient ones given a particular context.This allows us to compute an accurate adaptation ofthe original word vector in context.3.3.2 ExampleLet us exemplify the procedure with an example.Say we want to compute the distributionally similarwords to the noun instrument within the phrases (1)and (2), taken from the task?s test set:(1) musical instrument(2) optical instrumentFirst, we extract the context feature for both in-stances, in this case C1 = {musicalad j} for phrase(1), and C2 = {opticalad j} for phrase (2).
Next, we100look up p(z|C1) and p(z|C2) ?
the probability distri-butions over latent factors given the context ?
whichare yielded by our factorization model.
Using theseprobability distributions over latent factors, we cannow determine the probability of each dependencyfeature given the different contexts ?
p(d|C1) andp(d|C2) (equation 7).The former step yields a general probability dis-tribution over dependency features that tells us howlikely a particular dependency feature is given thecontext that our target word appears in.
Our last stepis now to weight the original probability vector ofthe target word (the aggregate of dependency-basedcontext features over all contexts of the target word)according to the new distribution given the contextin which the target word appears (equation 8).We can now return to our original matrix A andcompute the top similar words for the two adaptedvectors of instrument given the different contexts,which yields the results presented below.1.
instrumentN , C1: percussion, flute, violin,melody, harp2.
instrumentN , C2: sensor, detector, amplifier,device, microscope3.4 Implementational detailsOur model has been trained on the UKWaC cor-pus (Baroni et al 2009).
The corpus has beenpart of speech tagged and lemmatized with Stan-ford Part-Of-Speech Tagger (Toutanova and Man-ning, 2000; Toutanova et al 2003), and parsed withMaltParser (Nivre et al 2006) trained on sections2-21 of the Wall Street Journal section of the PennTreebank extended with about 4000 questions fromthe QuestionBank1, so that dependency triples couldbe extracted.The matrices needed for our interleaved NMF fac-torization are extracted from the corpus.
Our modelwas built using 5K nouns, 80K dependency relations,and 2K context words2 (excluding stop words) withhighest frequency in the training set, which yieldsmatrices of 5K nouns ?
80K dependency relations,and 5K nouns ?
2K context words.1http://maltparser.org/mco/english_parser/engmalt.html2We used a fairly large, paragraph-like window of four sen-tences.model accuracy precision recall F1dist .69 .83 .48 .61lvw .75 .84 .61 .71Table 1: Results of the distributional model (dist) and la-tent vector weighting model (lvw) on the SemEval task5aThe interleaved NMF model was carried out usingK = 600 (the number of factorized dimensions in themodel), and applying 100 iterations.
The interleavedNMF algorithm was implemented in Matlab; the pre-processing scripts and scripts for vector computationin context were written in Python.The model is entirely unsupervised.
The only pa-rameter to set, the cosine similarity threshold ?
, isinduced from the training set.
We set ?
= .049.4 ResultsTable 1 shows the evaluation results of the simpledistributional model (which only takes into accountthe head noun) and our model that uses latent vectorweighting.
The results indicate that our model basedon latent vector weighting performs quite a bit bet-ter than a standard dependency-based distributionalmodel.
The lvw model attains an accuracy of .75 ?a 6% improvement over the distributional model ?and an F-measure of .71 ?
a 10% improvement overthe distributional model.5 ConclusionIn this paper we presented an entirely unsuper-vised system for the assessment of the similarity ofwords and compositional phrases.
Our system uses adependency-based vector space model, in combina-tion with latent vector weighting.
The system com-putes the similarity between a particular noun in-stance and the head noun of a particular noun phrase,which was weighted according to the semantics ofthe modifier.
Using our system yields a substantialimprovement over a simple dependency-based dis-tributional model, which only takes the head nouninto account.101ReferencesMarco Baroni and Roberto Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: Representingadjective-noun constructions in semantic space.
InProceedings of the 2010 Conference on Empiri-cal Methods in Natural Language Processing, pages1183?1193, Cambridge, MA, October.
Association forComputational Linguistics.Marco Baroni, Silvia Bernardini, Adriano Ferraresi, andEros Zanchetta.
2009.
The wacky wide web: Acollection of very large linguistically processed web-crawled corpora.
Language Resources and Evalua-tion, 43(3):209?226.Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark.2010.
Mathematical foundations for a compositionaldistributed model of meaning.
Lambek Festschrift,Linguistic Analysis, vol.
36, 36.Chris Ding, Tao Li, and Wei Peng.
2008.
On the equiv-alence between non-negative matrix factorization andprobabilistic latent semantic indexing.
ComputationalStatistics & Data Analysis, 52(8):3913?3927.Georgiana Dinu and Mirella Lapata.
2010.
Measuringdistributional similarity in context.
In Proceedings ofthe 2010 Conference on Empirical Methods in NaturalLanguage Processing, pages 1162?1172, Cambridge,MA, October.Katrin Erk and Sebastian Pado?.
2008.
A structuredvector space model for word meaning in context.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing, pages 897?906,Waikiki, Hawaii, USA.Katrin Erk and Sebastian Pado?.
2009.
Paraphrase as-sessment in structured vector space: Exploring param-eters and datasets.
In Proceedings of the Workshop onGeometrical Models of Natural Language Semantics,pages 57?65, Athens, Greece.Eric Gaussier and Cyril Goutte.
2005.
Relation betweenPLSA and NMF and implications.
In Proceedings ofthe 28th annual international ACM SIGIR conferenceon Research and development in information retrieval,pages 601?602, Salvador, Brazil.Daniel D. Lee and H. Sebastian Seung.
2000.
Algo-rithms for non-negative matrix factorization.
In Ad-vances in Neural Information Processing Systems 13,pages 556?562.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
proceedings of ACL-08: HLT, pages 236?244.Joakim Nivre, Johan Hall, and Jens Nilsson.
2006.
Malt-parser: A data-driven parser-generator for dependencyparsing.
In Proceedings of LREC-2006, pages 2216?2219.Richard Socher, Brody Huval, Christopher D. Manning,and Andrew Y. Ng.
2012.
Semantic compositional-ity through recursive matrix-vector spaces.
In Pro-ceedings of the 2012 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning, pages 1201?1211, Jeju Island, Korea, July.
Association for Com-putational Linguistics.Kristina Toutanova and Christopher D. Manning.
2000.Enriching the knowledge sources used in a maximumentropy part-of-speech tagger.
In Proceedings of theJoint SIGDAT Conference on Empirical Methods inNatural Language Processing and Very Large Corpora(EMNLP/VLC-2000), pages 63?70.Kristina Toutanova, Dan Klein, Christopher Manning,and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In Pro-ceedings of HLT-NAACL 2003, pages 252?259.Tim Van de Cruys, Thierry Poibeau, and Anna Korho-nen.
2011.
Latent vector weighting for word meaningin context.
In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Processing,pages 1012?1022, Edinburgh, Scotland, UK., July.
As-sociation for Computational Linguistics.Tim Van de Cruys.
2008.
Using three way data for wordsense discrimination.
In Proceedings of the 22nd In-ternational Conference on Computational Linguistics(Coling 2008), pages 929?936, Manchester.102
