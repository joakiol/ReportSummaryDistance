Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 805?814,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsTransferring User Interests Across Websites with Unstructured Text forCold-Start RecommendationYu-Yang Huang and Shou-De LinDepartment of Computer Science and Information Engineering,National Taiwan University, Taipei, Taiwan{r02922050,sdlin}@csie.ntu.edu.twAbstractIn this work, we investigate the possibilityof cross-website transfer learning for tacklingthe cold-start problem.
To address the cold-start issues commonly present in a collabora-tive ltering (CF) system, most existing cross-domain CF models require auxiliary ratingdata from another domain; nevertheless, un-der the cross-website scenario, such data is of-ten unobtainable.
Therefore, we propose thenearest-neighbor transfer matrix factorization(NT-MF) model, where a topic model is ap-plied to the unstructured user-generated con-tent in the source domain, and the similaritybetween users in the latent topic space is uti-lized to guide the target-domain CF model.Specically, the latent factors of the nearest-neighbors are regarded as a set of pseudo ob-servations, which can be used to estimate theunknown parameters in the model.
Improve-ment over previous methods, especially for thecold-start users, is demonstrated with experi-ments on a real-world cross-website dataset.1 IntroductionWhile collaborative ltering (CF) approaches areone of the most successful methods for building rec-ommender systems, their performance deterioratesdramatically under cold-start situations.
That is, lowprediction accuracy is observed for users/items withvery few ratings.
Content-based recommender sys-tems may also suffer from the cold-start problem.For instance, content-based nearest-neighbor mod-els (Pazzani and Billsus, 2007) might not be as ef-fective if some users contain too few information togenerate a meaningful set of neighbors.Two types of solutions have been proposed to ad-dress the cold-start problem.
The rst is to cre-ate hybrid recommendation models that impose acontent-based model on a CF model to enrich theinformation for users/items with sparse rating pro-les (Burke, 2002; Burke, 2007).
The second is totransfer the information from auxiliary domains asa remedy to the cold-start individuals (Deng et al,2015).
This paper aims at bringing a marriage be-tween these two types of strategies.Although transfer learning gradually gains pop-ularity in handling the cold-start issue (Roy et al,2012), most of them assume a homogeneous modelwhere observations in both domains are of the sametype.
That is, to transfer knowledge to a rating-based/text-based recommender system, the sourcesystem must also be rating-based/text-based.
Someearlier works even require the ratings from both do-mains to be in the same format (Li et al, 2009),or assume specic structured text, such as user-provided tags (Shi et al, 2011; Deng et al, 2015).
Inthis work, by contrast, no source-domain ratings areavailable and unstructured user-generated content istreated as the auxiliary data.
We propose a hetero-geneous transfer learning framework to utilize un-structured auxiliary text for a better target-domainCF model.As there is no single service satisfying all so-cial needs, users nowadays hold multiple accountsacross many websites.
Furthermore, the accountlinking mechanism is often available on these web-sites.
This allows a precise mapping between theaccounts of the same user to be built.
One major ap-plication of our approach is to improve the recom-805mendation quality in the target service using auxil-iary data obtained from another seemingly irrelevantservice.For instance, consider a new user on YouTube.The initial recommended videos for this user islikely to be irrelevant as there is very few infor-mation available.
However, with the account link-ing mechanism, YouTube accounts can be linked toTwitter accounts with a simple click.
Our goal is toutilize the content generated by this user on Twitter,despite the possibility that the content is irrelevantto their preference on video browsing, to produce abetter video recommendation list on YouTube.Seemingly intuitive, there exist some difcultiesin such cross-website transfer learning approach.The biggest challenge lies in the fact that most usersdo not use multiple services (e.g.
social media sites)for the same purpose.
Usually a user registers formultiple services because each of them serves itsown purpose.
As a result, we cannot assume the ex-istence of direct mentions about target-domain itemsin the source-domain text data.
For example, a regu-lar YouTube viewer does not necessarily tweet aboutthe videos he/she has viewed.
Thus simple meth-ods such as keyword matching are likely to fail.The same reasoning also implies that, when transfer-ring knowledge across websites or services, the as-sumption of a shared rating format or structured textis overly optimistic.
Even websites aiming for thesame purpose often violate this assumption, let alnewebsites of different types.
Therefore, we expectthat the source and target services contain heteroge-neous information (e.g.
content vs. rating).
In ourmodel, we make a less strong assumption: regard-less of the type of information available in each do-main, the users that are similar in one domain shouldhave similar taste in the other domain.
Thus, insteadof directly transfer the content material from sourceto target domain, we transfer the similarity betweenusers, and use it as a guide to improve the CF modelin the target domain.2 LDA-MF ModelWe rst introduce an intuitive model to realize theabove-mentioned idea, and point out several in-trinsic weaknesses making it unsuitable for cross-website transfer learning.Here we rely on the probabilistic matrix factoriza-tion (PMF) model as our target-domain CF model.In the PMF model, each user latent factor is mod-eled (a priori) by a zero-mean Gaussian.
To incor-porate source-domain information into the target-domain PMF model, for each user i, a topic vector?i is extracted from source-domain text content andassigned as the prior mean of this user's PMF latentfactor, that is,ui ?
N (?i, ?
?1U I), (1)where ?U is the precision parameter and I is theidentity matrix.
Different from the original PMFmodel, prior distributions of different user latent fac-tors are no longer identical.
For users having simi-lar source-domain topic vectors, their latent factorsare expected to be close in the target-domain latentspace.
Such property allows the similarity betweenusers to be transferred from source domain to thetarget domain.With the latent Dirichlet alocation (LDA) (Bleiet al, 2003) topic model being used, the graphicalmodel is depicted in Figure 1.
This model is sim-ilar in structure to the recently proposed collabora-tive topic regression (CTR) (Wang and Blei, 2011)model.
The main difference is that, instead of mod-eling description about items, now user-generatedcontent from the source domain is modeled in ourproblem.
We call this model the LDA-MF model.rijui vj?U ?Vzw?i??
?NMKFigure 1: The LDA-MF model.Although LDA-MF indeed incorporates knowl-edge from the source domain, it has certain weak-nesses which need to be addressed.
The most sig-nicant drawback is that the dimensionalities of theLDA topic vector ?i and the PMF user latent fac-tor ui are required to be equal.
These two variablesare of very different nature.
One is extracted fromtext data in the source domain to model the topics806of the user-generated content, and the other is gen-erated from the rating data in the target domain tomodel the latent interests of users.
It is an overlystrong assumption to assume the optimal dimension-alities for LDA and PMF are equal.
In practice, ifwe choose the dimensionality to optimize the pre-dictive power of PMF (e.g.
by cross-validation onthe rating data), the LDA model is likely to yieldsub-optimal results and vice versa.
The experimentsthat will be shown later conrm this concern.
Fur-thermore, since the two variables are modeling dif-ferent types of observations coming from differentwebsites, the underlying meanings of the latent di-mensions are unlikely to be identical.
By treatingthe LDA topic vector as the prior mean of the PMFuser latent factor, the latent dimensions are forced tobe one-to-one aligned, which is again a strong as-sumption.
Finally, the topic vectors are drawn fromthe Dirichlet distribution which has a bounded (andpositive) support S, while the latent factors in PMFare unbounded Gaussian random vectors.
If the op-timal solution of ui is far from S, the performanceof the model could be affected, particularly in thecold-start situation where data is sparse and the priorplays an important role.3 Nearest-Neighbor Transfer MF ModelTo alleviate the drawbacks of the LDA-MF model,here we propose nearest-neighbor transfer matrixfactorization (NT-MF) model to transfer user inter-ests across websites.
The entire framework is de-picted in Figure 2.We begin by describing the scenario in which ourmodel operates.
First, there is a rating-based rec-ommender system (i.e.
PMF) in the target domain,which suffers from the cold-start problem.
The tar-get domain might or might not contain content in-formation.
For example, in the video recommenda-tion task, we can use the titles of all rated videos ofa user to generate content information in the targetdomain.
Such information is not available for thecold-start users since they have not rated any videos.However, in the source domain there are some con-tent information available for these users.
This canbe, say, the content of a user's tweets.
As previouslymentioned, this type of auxiliary text data is imme-diately available when a user connects the accountsFigure 2: The entire system.from two domains.
Therefore, we assume this aux-iliary text data is available for all users.3.1 Model OutlineNext, we describe the high level concept of ourmodel.
As described previously, we have observedthat the hypotheses encoded by the LDA-MF modelis too strong as the PMF latent factor is enforcedto inherit certain mathematical properties from theLDA topic vector.
Here we loosen the constraint toonly enforce that users should have similar distri-butions over the target-domain PMF latent factorsif there is a high similarity between their source-domain topic vectors.It is a reasonable hypothesis since our objectiveis to make the target-domain rating matrix factorizein a way that is consistent with the knowledge ex-tracted from source-domain text.
After all, the fac-torization of incomplete matrix is not unique, andthere is no reason that the latent factor should matchthe topic factor of the user.
In fact, our hypothesisimplies a different distribution over the PMF latentfactor for each user, i.e.
ui ?
N (?i,?i), where(?i,?i) are unknown parameters, and are (possibly)different for each user.To estimate the unknown parameters in a dis-tribution, normally we need a set of observations,(u(1)i , u(2)i , .
.
.
).
However, the parameters now be-long to a distribution over a latent variable, which isnon-trivial to estimate since we have no observationsabout the user latent factor.
An exhaustive search807over the parameter space is obviously intractable.Even if we treat the entire model as a hierarchicalmodel and learn the parameters indirectly from rat-ing data, the cold-start problem immediately comesin and forbids us from learning a representative dis-tribution for users.We propose the idea of using the latent factors ofthe nearest-neighbors to estimate the unknown pa-rameters in the distribution for a user.
That is, thelatent factors of the nearest-neighbors, {ul}l?kNN(i),are regarded as a set of pseudo observations to re-place the unavailable data, (u(1)i , u(2)i , .
.
.
).
How-ever, the denition of ?closeness?
is not based ontarget-domain rating data, but computed by the topicvectors obtained from the content in the source do-main (and the target domain, if available).
Ourmodel is thus not hampered by the cold-start prob-lem.Note that, in addition to the set of k-nearest-neighbors kNN(i), we also have the correspondingsimilarity scores sim(i, l) between each neighborl and user i.
The similarity scores along with thelist of nearest-neighbors are transferred to the tar-get domain to form a set of weighted samples, D,which can be used to estimate the unknown parame-ters (?i,?i), i.e.,D ?
{{ul}l?kNN(i)wl = sim(i, l).
(2)The main purpose of assigning a sample weightwl to each of the pseudo observations ul is that bydoing so, users with a higher source-domain simi-larity to user i will have a larger impact on the esti-mation of the target-domain parameters (?i,?i).
Inother words, with this model specication, the simi-larity between users is transferred across domains.3.2 Inference in NT-MF ModelTo perform inference in our model, we adopt themaximum a posteriori (MAP) strategy and alter-nately update the user and item latent factors (i.e.by block coordinate ascent), similar to some previ-ous solutions (Salakhutdinov andMnih, 2007; Wangand Blei, 2011).To solve for the optimal user latent factor ui,we need to rst estimate the unknown parameters(?i,?i).
Therefore, in our coordinate ascent algo-rithm, different from the original PMF model, weupdate the user latent factors one by one.
That is,all user latent factors are regarded as xed constantsexcept for the one, ui, to be updated.
By doing so,for each user i, a set of pseudo observations aboutui (Eq.
2) is available.
Using these pseudo obser-vations, the unknown parameters (?i,?i) can thenbe estimated with standard techniques such as maxi-mum likelihood estimation (MLE).
After an estima-tor of (?i,?i) is obtained, we can analytically solvefor the MAP solution of the user latent factor ui.Then, we move on to the next user, and the coor-dinate ascent procedure continues.
These two steps,namely the estimation of unknown parameters andthe updating of the latent factors, are repeated untilconvergence.One advantage of this procedure is that the listof nearest-neighbors and the similarities in Eq.
2need not be recomputed during inference, avoidingexpensive recomputation of pairwise similarities.
Itis also noticeable that, different from other transfer-based approaches, rating information and structuredtext from the source domain are not required in thisprocedure of model optimization.
This further addsa level of exibility to our framework for transfer-ring user interests across websites.3.3 Case Study: Inferring Unknown MeanTo clarify the previous discussions, we present asimple but detailed case-study on how an NT-MFmodel and its optimization procedure can be de-rived.
The latent factor ui for each user is assumedto be generated from a multivariate normal distribu-tion with unknown mean ?i and a known precisionparameter ?U , which is shared among the users.The generative process proceeds as follows:1.
For each user i, draw user latent factor ui ?N (?i, ?
?1U I).2.
For each item j, draw item latent factor vj ?N (0, ?
?1V I).3.
For each observed user-to-item pair (i, j), drawthe rating rij ?
N (uTi vj , ?
?10 ),where ?0 is the precision parameter of the rat-ings, and ?U , ?V are the precision parameter of the808users and items, respectively.
We use the notationN (x|?,?)
to denote the Gaussian pdf with mean ?and covariance ?.The model is optimized by maximizing the pos-terior likelihood of the latent variables (an additiveterm is omitted),L = ?
?02M?i=1N?j=1?ij(rij ?
uTi vj)2?
?U2M?i=1(ui ?
?i)T (ui ?
?i)?
?V2N?j=1vTj vj ,(3)where ?ij is an indicator variable which is equal to1 if item j is rated by user i, and 0 otherwise.To solve the MAP problem, we need to rst es-timate the unknown parameters in the distribution,which in this case is the mean vector ?i.
Thelikelihood function over the pseudo observations,{ul}l?kNN(i), is dened as,p(D|?i, ?U ) =?l?kNN(i)N (ul|?i, ?
?1U I).
(4)By taking derivative of Eq.
4 with respect to ?iand set it to zero, we obtain,?l?kNN(i)(ul ?
?i) = 0, (5)which implies that the MLE of ?i is the samplemean.
However, since we are dealing with a set ofweighted samples, the sample mean is replaced bythe weighted average (the weights wl are assumedto add up to one):?i =?l?kNN(i)wlul.
(6)Our model yields an intuitive result: to estimatethe mean vector ?i of ui, we can simply take theweighted average of the latent factors ul from thenearest-neighbors as an estimator, where the weightsare the similarity scores between the textual prolesof user i and its neighbors.Given ?i, we can now maximize Eq.
3 with re-spect to ui and vj .
By taking derivative of Eq.
3with respect to ui and vj and set it to zero, we obtainthe update equations,?
?N?j=1?ijvjvTj +?U?0I?
?ui =N?j=1?ijrijvj +?U?0?i(7)( M?i=1?ijuiuTi +?V?0I)vj =M?i=1?ijrijui.
(8)Now with Eq.
6 to Eq.
8 at hand, we can itera-tively solve for ?i, ui and vj for all users and itemsuntil the model converges.It can be seen from this case-study that NT-MFeliminates the three major drawbacks of the previ-ously mentioned LDA-MF model.
First, the topicvectors and the user latent factors are not requiredto have equal dimensionalities, which allows for theoptimal dimensionality to be chosen in both models.Second, the mean vector, that is, the kNN weightedaverage in Eq.
6, is a linear combination of a set ofuser latent factors; as a result, the latent dimensionsof ui and ?i are naturally aligned.
Third, the meanvector ?i has the same support as the user latent fac-tor ui, avoiding the risk of prior misspecication incold-start situations.4 ExperimentWe use YouTube video recommendation to test theusefulness of NT-MF under the cold-start scenario.The NT-MF model used in this section follows theoptimization procedure derived in Section 3.3.4.1 Dataset and StatisticsTo construct a dataset containing both the users' rat-ing history and textual information, we begin withthe user prole pages on Google+.
A large propor-tion of Google+ users provide links to their prolepages from other social network services (e.g.
Twit-ter).
More importantly, if a user owns a YouTubeaccount, a link to the user's YouTube channel willbe automatically added to his Google+ prole.
Thismakes a fully aligned dataset available.
Users' Twit-ter accounts are obtained via their Google+ prolepage, and the concatenation of tweets is regarded asthe auxiliary text data.
It has been shown that byconcatenating the tweets, more representative user809topic vectors can be obtained (Hong and Davison,2010).
We refer to this text data as the Twitter cor-pus.Videos in a user's ?liked?
or ?favorite?
playlistsare considered to have a rating rij = 1.
Othervideos are assigned rij = 0.
In other words, weare dealing with a one-class collaborative ltering(OCCF) problem (Pan et al, 2008).
We adopt thesame strategy as in (Wang and Blei, 2011) to dealwith OCCF.
First, all ratings are assumed to be ob-served, i.e.
?ij = 1 for all user-item pairs.
Next,a condence parameter cij is introduced to reducethe inuence of the huge number of zeroes duringmodel optimization.
The condence parameter takesplace of the original rating precision parameter ?0and is dened in (Wang and Blei, 2011) as cij =a if rij = 1 and cij = b otherwise (a > b > 0).
Allthe derivations in the previous sections follow intu-itively.The titles of the liked videos are concatenated andtreated as the text data in the target domain (whichwe refer to as the YouTube corpus).
As for the vo-cabulary, stopwords are rst removed, and then 5000words are selected from the YouTube corpus basedon their TF-IDF scores (Blei and Lafferty, 2009).On average, each user's Twitter text data contains5149 words and 1193 distinct terms, and each user'sYouTube text data contains 158 words and 116 dis-tinct terms.
These statistics are in accordance withour assumption that text data in the source domain isabundant comparing to that in the target domain.To validate the prediction result, each user has atleast 10 liked videos.
Videos with less than 5 likesare removed from the dataset.
After data cleansing,there are 7328 users and 18691 videos in the dataset.The maximum number of likes received by a video is98, and the average is 19.1.
Among all videos, 92%of them are liked by less than 40 users.
The max-imum number of likes given by a user is 908, andthe average is 48.8.
Among all users, 89% of themhave liked less than 100 videos.
The sparsity (ratioof zeroes to the total number of entries) of the ratingmatrix is 99.74%, which illustrates the difculty ofthis recommendation task.4.2 Evaluation and ScenarioWe choose the area under ROC curve (AUC) as theevaluation metric.
AUC is often used to comparemodels when there is severe class imbalance, whichis the case in our OCCF problem since we regardall zeroes as observed.
All reported results are theaverage of 5 random data splits.Similar to the experiments performed in (Wangand Blei, 2011), we test the performance of eachmodel under two different scenarios.
The rst one isthe task of in-matrix prediction.
In this task, the likesreceived by each video are partitioned into three sets,namely the training, validation and testing sets.
Theratio of data partition is 3:1:1.
There are no cold-start users for the in-matrix prediction.The second task is the out-of-matrix prediction,where the users are partitioned into three sets withthe same 3:1:1 ratio.
To make the two tasks compa-rable, we randomly split the data until the number ofobservations in each of the three sets is closed to thatof the in-matrix task.
Users in the testing set are allcold-start users.
The only data we have when mak-ing prediction on the cold-start users is the auxiliarytext data.4.3 Baseline Methods?
LDA:We run linear regression on the LDA fea-tures to predict the ratings.
This model servesas a content-based baseline.?
UKNN: The user-kNN algorithm (Herlockeret al, 1999) based on LDA features is imple-mented.
This model serves as a neighborhood-based baseline.?
PMF: PMF (Salakhutdinov and Mnih, 2007) isa classic and widely-used CF model.
It usesonly the rating information, and thus is not ca-pable of performing the out-of-matrix task.?
LDA-MF: This model is implemented as hasbeen described in Section 2.
It is similar toCTR (Wang and Blei, 2011) in structure.
Sincethe optimization of the full model convergesbadly, we pre-train the LDA part of the model,and x the topic vector when optimizing thePMF part.All hyperparameters are tuned on the validationset.
Due to efciency and storage considerations,for UKNN and NT-MF, the k-nearest-neighbors arecomputed approximately with the FLANN library81050 2000.800.810.820.830.840.85(0.769)(0.762)KAUCLDA UKNN PMF LDA-MF NT-MF50 2000.800.810.820.830.840.85(0.749)(0.757)(0.716)(0.718)KAUCLDA UKNN PMF LDA-MF NT-MF50 2000.800.810.820.830.840.85(0.759)(0.776)(0.791)(0.791)KAUCLDA UKNN PMF LDA-MF NT-MF(a) YouTube corpus (b) Twitter corpus (c) YouTube + Twitter corpusFigure 3: In-matrix AUC using different corpus.
For methods signicantly worse than others, we cut off the plot and put the AUCvalues on top of the bars.
NT-MF is signicantly better than the baselines in all plots, according to a paired t-test (p < 0.05).
(Muja and Lowe, 2014).
The symmetric Kullback-Leibler divergence is chosen to be the distance met-ric between topic vectors.
For all baseline methods,we use K to denote the dimensionality of the latentvariables.
However, when discussing about NT-MF,since the number of topics can be different from thenumber of user latent factors, we use T to denote theformer andK to denote the latter to avoid confusion.4.4 In-Matrix PredictionIn this section, the in-matrix prediction is discussed.First, we test the model's general performance ondifferent corpora.
Normally, the optimal numberof topics will not be the same for different cor-pora.
Since the LDA model performs the best withK = 50 on the YouTube corpus and K = 200 onthe Twitter corpus, we report the results when K isset to these two numbers.Figure 3(a) shows the results when no source-domain information is available and thus no trans-fer learning is performed.
That is, all models areprovided only with the YouTube ratings and theYouTube corpus.
Because the YouTube corpus isscarce, the LDA model results in lower AUC whenmore topics are used, signifying overtting.
Thesame reason also leads to limited improvement ofLDA-MF over PMF.
Using neighborhood informa-tion alone, UKNN performs poorly.
On the otherhand, as a model bringing neighborhood informationinto PMF, NT-MF outperforms all baselines signi-cantly.
The above analysis shows that, although us-ing either content (LDA) or neighborhood (UKNN)information alone is insufcient to generate goodpredictions, they can effectively improve the factor-ization of the rating matrix if used correctly.To demonstrate the advantage of transfer learning,we study the scenario where only source-domaintext and target-domain ratings are available.
That is,the YouTube corpus in the previous analysis is re-placed with the Twitter corpus.
The result is shownin Figure 3(b).
Comparing to Figure 3(a), we cansee that although the Twitter corpus is larger thanthe YouTube corpus, it leads to a worse performancefor LDA and UKNN.
Content information from thenoisy Twitter corpus alone is not sufcient to capturethe rating behavior of users.
However, by integrat-ing the content information and rating history, bothLDA-MF and NT-MF benet from a larger corpus.In the following analyses, we use data from bothwebsites.
For LDA, PMF and LDA-MF, we mergethe two corpora by summing up the word counts.For UKNN and NT-MF, however, there is a moreelegant way to combine the knowledge from differ-ent websites.
First, we compute user similarity sep-arately from the two corpora.
Then, the two sets ofsimilarity scores are weighted and averaged.
Finally,the nearest-neighbors are computed based on this setof newly generated similarity scores.
By applyingthis strategy to NT-MF, not only can ?i and ui dif-fer in dimensionality, but also the optimal number oftopics can be used for different corpora.
RegardlessofK, we use T = 50 for YouTube and T = 200 forTwitter in our NT-MF model.
The result is shown inFigure 3(c).
By comparing it with Figure 3(b), wecan see that the AUC of NT-MF increases while thatof LDA-MF remains unchanged.
UKNN also bene-ts from this strategy.
These facts show that, insteadof merging the two corpora directly, our strategy ofaveraging the similarities is more advantageous.81120 40 60 80 100 120 140 160 180 2000.760.770.780.790.800.810.820.830.840.85# of observed ratingsAUCLDA UKNN PMF LDA-MF NT-MF20 40 60 80 100 120 140 160 180 2000.010.020.030.040.050.060.07# of observed ratingsAUCdifferenceLDAUKNNPMFLDA-MF(a) (b)Figure 4: (a) Cumulative in-matrix AUC.
Each point (x, y) in the gure means that the model gives an averaged AUC of y amongall users who have less than or equal to x observed ratings.
(b) Difference in cumulative in-matrix AUC between NT-MF andbaseline methods.Next, as a preliminary investigation of the perfor-mance on cold-start users, in Figure 4(a), we plotthe cumulative AUC with respect to the total num-ber of observed ratings.
NT-MF outperforms othermethods in terms of cumulative AUC regardless ofthe number of observed ratings.
The advantage ofNT-MF over the baseline methods is even greateras the number of observed ratings decreases (exceptfor LDA).
To make it clear, we plot the differencein AUC between NT-MF and the baseline methodsin Figure 4(b).
This phenomenon sheds light on theadvantage of NT-MF under cold-start scenario.4.5 Out-of-Matrix PredictionIn this section, we discuss the out-of-matrix predic-tion.
Users in the testing set are all completely cold-start users.
That is, we are only provided the Twit-ter corpus when making prediction for these users.Therefore, our previous strategy of averaging thesimilarities only applies to users in the training set.For this study we adopt the strategy of merging thetwo corpus instead of averaging the similarities.
Thenumber of topics T = 150 is chosen for NT-MFwithrespect to the validation AUC.The result is presented in Figure 5.
We plot theAUC against the dimensionality of the latent vari-ables K. It can be observed that NT-MF beats allbaseline methods regardless of K. Comparing toFigure 3, the out-of-matrix AUC is much lower, sig-nifying the difculty of cold-start recommendation.Under the cold-start scenario, the latent factor10 20 50 100 150 2000.670.680.690.700.710.720.730.740.750.760.77KAUCLDAUKNNLDA-MFNT-MFFigure 5: Out-of-matrix AUC.
NT-MF is signicantly betterthan the baselines, according to a paired t-test (p < 0.05).used in the prediction phase is taken to be the priormean for the MF-based models.
For LDA-MF theprior mean is the topic vector ?i, while for NT-MF itis the weighted average ?i given by Eq.
6.Since ?i is used in place of ui in the LDA-MFmodel when generating predictions, the curves ofLDA and LDA-MF look very similar.
A pairedt-test (p < 0.05) shows no statistically signicantdifference between these two methods when K =10 (p = 0.48) and K = 20 (p = 0.09).
Despite thefact that ui = ?i is xed for the cold-start users inthe LDA-MF model, as K becomes larger, the itemlatent factors can carry more information in the rat-ing data, which results in a higher AUC than LDA.However, since the dimensionalities of the LDA partand PMF part must match, the inference procedureof LDA-MF becomes very slow whenK is large.
To812make a better use of the available data, the compu-tational efciency must be sacriced.On the other hand, note that NT-MF achieves thehighest AUC when K = 50.
In fact, not only doesNT-MF beat all baseline methods under differentKvalues, it also outperforms the best LDA-MF model(K = 200) with fewer latent factors (K = 20).
Un-like LDA-MF, the latent factors of the cold-startusers are not xed in NT-MF.
Therefore, NT-MF canrepresent the information in a more concise way.
Inthis case, NT-MF is better than LDA-MF in terms ofboth execution speed and predictive power.10 20 50 100 150 2000.720.730.740.750.76KAUCT = 10 T = 20 T = 50 T = 100T = 150 T = 200Figure 6: Performance of NT-MF based on out-of-matrix AUCfor different values ofK and T .In Figure 6 we investigate the effect of differentvalues of K and T .
For each curve, we can see thatthe performance is about the same forK ?
50.
Thisis in accordance with the observation that NT-MFdoes not need as many latent factors as LDA-MF toachieve the same level of performance.
Also, whileincreasing the number of topics T improves the per-formance in general, increasing T from 150 to 200gives no signicant improvement.
The most impor-tant observation is that the highest AUC is achievedwhenK = 50 and T = 150.
In other words, the op-timal number of topics is different from that of userlatent factors.
This further justies the advantage ofNT-MF against previous methods.5 Related WorkAlthough not directly aiming to solve the problemwe have proposed, there exists some models of sim-ilar structure or adopt similar ideas.As previously mentioned, LDA-MF is similar instructure to CTR.
Collaborative topic Poisson fac-torization (CTPF) (Gopalan et al, 2014) combinesthe ideas of CTR and Poisson factorization (Gopalanet al, 2013) for a better performance.
We have alsotried CTPF on our dataset; nevertheless, there is nosignicant improvement over LDA-MF.Recently, the neighborhood-aware probabilisticmatrix factorization (NHPMF) model is proposed(Wu et al, 2012) as a method to combine kNN andPMF.
It is originally proposed to leverage taggingdata for improving PMF.
This model can also beapplied to our problem if we use the Twitter cor-pus in place of the unavailable tagging data.
How-ever, in the NHPMFmodel, the mean parameters arenot treated as constants when the user latent factorsare updated.
As a result, an extra term appears inthe gradient formula, which leads to an O(k2) timecomplexity, with k being the number of nearest-neighbors considered.
On the other hand, the com-putation of the weighted average (i.e.
Eq.
6) takesO(k) time complexity.
We have implemented NH-PMF for comparison.
As we increase k, NHPMFbecomes signicantly slower than NT-MF, while itsperformance is no better than NT-MF on our dataset.6 ConclusionIn this work, we propose NT-MF, a cross-websitetransfer learning model which integrates content,neighborhood and rating information to alleviate thecold-start problem.
A signicant improvement overprevious methods is demonstrated on a real-worldcross-website dataset.
The improvement is evenmore signicant under the cold-start scenario.So far we use the LDA topic vector to representa user.
As future work, different aspects of text canbe taken into account to generate a more comprehen-sive user model.
For example, writing styles or opin-ion mining may provide different insights on userbehavior.
Another possible extension is to apply ouridea to more realistic settings such as large-scale andonline recommender systems.AcknowledgmentsThis material is based upon work supported by Mi-crosoft Research Asia (MSRA) under award numberFY16-RES-THEME-013 and by Taiwan Ministry ofScience and Technology (MOST) under grant num-ber 103-2221-E-002-104-MY2.813ReferencesDavidM.
Blei and John D. Lafferty.
2009.
Topic models.In Text Mining: Theory and Applications.
Taylor andFrancis.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alocation.
J. Mach.
Learn.Res., 3:993?1022, March.Robin Burke.
2002.
Hybrid recommender systems:Survey and experiments.
User Modeling and User-Adapted Interaction, 12(4):331?370, November.Robin Burke.
2007.
The adaptive web.
chapter Hy-brid Web Recommender Systems, pages 377?408.Springer-Verlag, Berlin, Heidelberg.Zhengyu Deng, Ming Yan, Jitao Sang, and ChangshengXu.
2015.
Twitter is faster: Personalized time-aware video recommendation from twitter to youtube.ACM Trans.
Multimedia Comput.
Commun.
Appl.,11(2):31:1?31:23, January.Prem Gopalan, Jake M. Hofman, and David M. Blei.2013.
Scalable recommendation with poisson factor-ization.
CoRR, abs/1311.1704.Prem Gopalan, Laurent Charlin, and David M. Blei.2014.
Content-based recommendations with pois-son factorization.
In Z. Ghahramani, M. Welling,C.
Cortes, N. D. Lawrence, and K. Q. Weinberger, ed-itors, Advances in Neural Information Processing Sys-tems 27, pages 3176?3184.
Curran Associates, Inc.Jonathan L. Herlocker, Joseph A. Konstan, Al Borchers,and John Riedl.
1999.
An algorithmic frameworkfor performing collaborative ltering.
In Proceedingsof the 22Nd Annual International ACM SIGIR Con-ference on Research and Development in InformationRetrieval, SIGIR '99, pages 230?237, New York, NY,USA.
ACM.Liangjie Hong and Brian D. Davison.
2010.
Empiricalstudy of topic modeling in twitter.
In Proceedings ofthe First Workshop on Social Media Analytics, SOMA'10, pages 80?88, New York, NY, USA.
ACM.Bin Li, Qiang Yang, and Xiangyang Xue.
2009.
Canmovies and books collaborate?
: Cross-domain collab-orative ltering for sparsity reduction.
In Proceedingsof the 21st International Joint Conference on ArticalIntelligence, IJCAI'09, pages 2052?2057, San Fran-cisco, CA, USA.
Morgan Kaufmann Publishers Inc.Marius Muja and David G. Lowe.
2014.
Scalable nearestneighbor algorithms for high dimensional data.
Pat-tern Analysis and Machine Intelligence, IEEE Trans-actions on, 36.Rong Pan, Yunhong Zhou, Bin Cao, Nathan Nan Liu, Ra-jan M. Lukose, Martin Scholz, and Qiang Yang.
2008.One-class collaborative ltering.
In Proceedings ofthe 8th IEEE International Conference on Data Min-ing (ICDM 2008), December 15-19, 2008, Pisa, Italy,pages 502?511.Michael J. Pazzani and Daniel Billsus.
2007.
Theadaptive web.
chapter Content-based Recommenda-tion Systems, pages 325?341.
Springer-Verlag, Berlin,Heidelberg.Suman Deb Roy, TaoMei, Wenjun Zeng, and Shipeng Li.2012.
Socialtransfer: Cross-domain transfer learningfrom social streams for media applications.
In Pro-ceedings of the 20th ACM International Conference onMultimedia, MM '12, pages 649?658, New York, NY,USA.
ACM.Ruslan Salakhutdinov and Andriy Mnih.
2007.
Proba-bilistic matrix factorization.
In Advances in Neural In-formation Processing Systems 20, Proceedings of theTwenty-First Annual Conference on Neural Informa-tion Processing Systems, Vancouver, British Columbia,Canada, December 3-6, 2007, pages 1257?1264.Yue Shi, Martha Larson, and Alan Hanjalic.
2011.
Tagsas bridges between domains: Improving recommenda-tion with tag-induced cross-domain collaborative l-tering.
In Proceedings of the 19th International Con-ference on User Modeling, Adaption, and Personaliza-tion, UMAP'11, pages 305?316, Berlin, Heidelberg.Springer-Verlag.Chong Wang and David M. Blei.
2011.
Collaborativetopic modeling for recommending scientic articles.In Proceedings of the 17th ACM SIGKDD Interna-tional Conference on Knowledge Discovery and DataMining, KDD '11, pages 448?456, New York, NY,USA.
ACM.Le Wu, Enhong Chen, Qi Liu, Linli Xu, TengfeiBao, and Lei Zhang.
2012.
Leveraging taggingfor neighborhood-aware probabilistic matrix factoriza-tion.
In Proceedings of the 21st ACM InternationalConference on Information and Knowledge Manage-ment, CIKM '12, pages 1854?1858, New York, NY,USA.
ACM.814
