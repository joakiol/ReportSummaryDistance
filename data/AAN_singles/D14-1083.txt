Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 751?762,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsWhy are You Taking this Stance?Identifying and Classifying Reasons in Ideological DebatesKazi Saidul Hasan and Vincent NgHuman Language Technology Research InstituteUniversity of Texas at DallasRichardson, TX 75083-0688{saidul,vince}@hlt.utdallas.eduAbstractRecent years have seen a surge of interestin stance classification in online debates.Oftentimes, however, it is important to de-termine not only the stance expressed byan author in her debate posts, but also thereasons behind her supporting or oppos-ing the issue under debate.
We thereforeexamine the new task of reason classifi-cation in this paper.
Given the close in-terplay between stance classification andreason classification, we design computa-tional models for examining how automat-ically computed stance information can beprofitably exploited for reason classifica-tion.
Experiments on our reason-annotatedcorpus of ideological debate posts fromfour domains demonstrate that sophisti-cated models of stances and reasons canindeed yield more accurate reason andstance classification results than their sim-pler counterparts.1 IntroductionIn recent years, researchers have begun exploringnew opinion mining tasks.
One such task is debatestance classification (SC): given a post written fora two-sided topic discussed in an online debate fo-rum, determine which of the two sides (i.e., for oragainst) its author is taking (Agrawal et al., 2003;Thomas et al., 2006; Bansal et al., 2008; Soma-sundaran and Wiebe, 2009; Burfoot et al., 2011;Hasan and Ng, 2013b).
For example, the author ofthe post shown in Figure 1 is pro-abortion.Oftentimes, however, it is important to deter-mine not only the author?s stance expressed in herdebate posts, but also the reasons why she supportsor opposes the issue under debate.
Intuitively,given a debate topic such as ?Should abortion bebanned??
or ?Do you support Obamacare?
?, it[I feel that abortion should remain legal, or rather, parentsshould have the power to make the decision themselves andnot face any legal hindrance of any form.
]1Let us take alook from the social perspective.
[If parents cannot affordto provide for the child, or if the family is facing financialconstraints, it is understandable that abortion can remain asone of the options.
]2Reason 1: Woman?s right to abortReason 2: Unwanted babies are threat to their parents?
fu-tureFigure 1: A sample post on abortion annotatedwith reasons.should not be difficult for us to come up with a setof reasons people typically use to back up theirstances.
Given a set of reasons associated witheach stance in an online debate, the goal of post-level reason classification is to identify those rea-son(s) an author uses to back up her stance in herdebate post.
A more challenging version of thistask is sentence-level reason classification, wherethe goal is to identify not only the reason(s) an au-thor uses in her post, but also the sentence(s) inthe post that the author uses to describe each ofher reasons.
For example, the author of the postshown in Figure 1 mentions two reasons why shesupports abortion, namely it?s a woman?s right toabort and unwanted babies are threat to their par-ents?
future, which are mentioned in the first andthird sentences in the post respectively.Our goal in this paper is to examine post- andsentence-level reason classification (RC) in ideo-logical debates.
Many online debaters use emo-tional languages, which may involve sarcasm andinsults, to express their points, thereby makingRC and SC in ideological debates potentially morechallenging than that in other debate settings suchas congressional debates and company-internaldiscussions (Walker et al., 2012).Besides examining the new task of RC in ide-ological debates, we believe that our work makesthree contributions.
First, we propose to addresspost-level RC by means of sentence-level RC by751(1) determining the reason(s) associated with eachof its sentences (if any), and then (2) taking theunion of the set of reasons associated with all of itssentences to be the set of reasons associated withthe post.
We hypothesize that this sentence-basedapproach, which exploits a training set in whicheach sentence in a post is labeled with its reason,would achieve better performance than a multi-label text classification approach to post-level RC,which learns to determine the subset of reasons apost contains directly from a training set in whicheach post is labeled with the corresponding set ofreasons.
In other words, we hypothesize that wecould achieve better results for post-level RC bylearning from sentence-level than from post-levelreason annotations, as sentence-level reason anno-tations can enable a learning algorithm to accu-rately attribute an annotated reason to a particularportion of a post.Second, we propose stance-supported RC sys-tems, hypothesizing that automatically computedstance information can be profitably exploited forRC.
Since we are exploiting automatically com-puted (and thus potentially noisy) stance informa-tion, we hypothesize that the effectiveness of suchinformation would depend in part on the way it isexploited in RC systems.
As a result, we introducea set of stance-supported models for RC, start-ing with simple pipeline models and then mov-ing on to joint models with increasing sophisti-cation.
Note that exploiting stance informationby no means guarantees that RC performance willimprove, as an incorrect determination of stancecould lead to an incorrect identification of rea-sons.
Hence, one of our goals is to examine how tomodel stances and reasons so that RC can benefitfrom stance information.Finally, since progress on RC is hindered in partby the lack of an annotated corpus, we make ourreason-annotated dataset publicly available.1Toour knowledge, this will be the first publicly avail-able corpus for sentence- and post-level RC.2 Corpus and AnnotationWe collected debate posts from four populardomains, Abortion (ABO), Gay Rights (GAY),Obama (OBA), and Marijuana (MAR), from anonline debate forum2.
All debates are two-sided,1http://www.hlt.utdallas.edu/?saidul/stance/2http://www.createdebate.com/so each post receives one of two stance labels, foror against, depending on whether the author ofthe post supports or opposes abortion, gay rights,Obama, or the legalization of marijuana respec-tively.
A post?s stance label is given by its author.Note that each post belongs to a thread, whichis a tree with one or more nodes such that (1) eachnode corresponds to a debate post, and (2) a postyiis the parent of another post yjif yjis a replyto yi.
Given a thread, we generate post sequences,each of which is a path from the root of the threadto one of its leaves.
Hence, a post sequence is anordered set of posts such that each post is a replyto its immediately preceding post in the sequence.Table 2a shows the statistics of the four stance-labeled datasets.While the debate posts contain the stance labelsgiven by their authors, they are not annotated withreasons.
As part of our study of RC, we annotateeach post with the reasons it gives for its stance.Our annotation procedure is composed of threesteps.
First, two human annotators independentlyexamined each post and identified the reasons au-thors present to support their stances (i.e., for andagainst) in each domain.
Second, they discussedand agreed on the reasons identified for each do-main.
Third, they independently annotated the textof each post with reason labels from the post?s do-main.
To do this, they labeled each sentence of apost with the set of reasons the author expressed inthat sentence.
Any sentence that does not belongto any reason class was assigned the NONE class.After the annotators completed the aforemen-tioned steps, they were asked to collapse all thereason classes that occur in less than 2% of thesentences annotated with non-NONE classes intothe OTHER class.
In other words, all the sentencesthat were originally annotated with one of theseinfrequent reason classes will now be labeled asOTHER.
Our decision to merge infrequent classesis motivated by two observations.
First, from apractical point of view, infrequent reasons do notcarry much weight.
Second, from a modeling per-spective, it is often not worth increasing modelcomplexity by handling infrequent classes.
Theresulting set of reason classes for each domain isshown in Table 1.A closer examination of the resulting annota-tions reveals that approximately 3% of the sen-tences received multiple reason labels.
Again, toavoid the complexity of modeling multi-labeled752Domain Stance Reason classesABOfor [F1] Abortion is a woman?s right (26%); [F2] Rape victims need it to be legal (7%); [F3] A fetusis not human (38%); [F4] Mother?s life in danger (5%); [F5] Unwanted babies are ill-treated byparents (8%); [F6] Birth control fails at times (3%); [F7] Abortion is not murder (3%); [F8] Motheris not healthy/financially solvent (4%); [F9] Others (6%)against [A1] Put baby up for adoption (9%); [A2] Abortion kills a life (29%); [A3] An unborn baby is ahuman and has the right to live (40%); [A4] Be willing to have the baby if you have sex (14%);[A5] Abortion is harmful for women (5%); [A6] Others (3%)GAYfor [F1] Gay marriage is like any other marriage (14%); [F2] Gay people should have the same rightsas straight people (36%); [F3] Gay parents can adopt and ensure a happy life for a baby (10%); [F4]People are born gay (18%); [F5] Religion should not be used against gay rights (11%); [F6] Others(11%)against [A1] Religion does not permit gay marriages (18%); [A2] Gay marriages are not normal/againstnature (39%); [A3] Gay parents can not raise kids properly (11%); [A4] Gay people have problemsand create social issues (16%); [A5] Others (16%)OBAfor [F1] Fixed the economy (21%); [F2] Ending the wars (7%); [F3] Better than the republican candi-dates (25%); [F4] Makes good decisions/policies (8%); [F5] Has qualities of a good leader (14%);[F6] Ensured better healthcare (8%); [F7] Executed effective foreign policies (6%); [F8] Createdmore jobs (4%); [F9] Others (7%)against [A1] Destroyed our economy (26%); [A2] Wars are still on (11%); [A3] Unemployment rate is high(5%); [A4] Healthcare bill is a failure(9%); [A5] Poor decision-maker (7%); [A6] We have betterrepublicans than Obama (5%); [A7] Not eligible as a leader (20%); [A8] Ineffective foreign policies(4%); [A9] Others (13%)MARfor [F1] Not addictive (23%); [F2] Used as a medicine (11%); [F3] Legalized marijuana can be con-trolled and regulated by the government (33%); [F4] Prohibition violates human rights (15%); [F5]Does not cause any damage to our bodies (6%); [F6] Others (12%)against [A1] Damages our bodies (23%); [A2] Responsible for brain damage (22%); [A3] If legalized,people will use marijuana and other drugs more (12%); [A4] Causes crime (9%); [A5] Highlyaddictive (17%); [A6] Others (17%)Table 1: Reason classes and their percentages in the corresponding stance for each domain.sentences given their rarity, we asked each annota-tor to pick the reason that was highlighted the mostin each multi-labeled sentence.Inter-annotator agreement scores at the sentencelevel and the post level, expressed in terms of Co-hen?s Kappa (Carletta, 1996), are shown in Ta-ble 2b.
Given that the majority of sentences werelabeled as NONE, we avoid inflating agreement bynot considering the sentences labeled with NONEby both annotators when computing Kappa.
As wecan see, we achieved substantial post-level agree-ment and high sentence-level agreement.The major source of inter-annotator disagree-ment for all four datasets stems from the fact thatin many cases, the annotators, while agreeing onthe reason class, differ on how long the text spanfor a reason should be.
This hurts sentence-levelagreement but not post-level agreement, since thelatter only concerns whether a reason was men-tioned in a post, and explains why the sentence-level agreement scores are lower than the corre-sponding post-level scores.
Minor sources of dis-agreement arise from the facts that (1) the anno-tators selected different reason labels for some ofthe multi-labeled sentences, and (2) they tend todisagree in some cases where authors use sarcasmABO GAY OBA MARStance-labeled posts1741 1376 985 626for posts (%)54.9 63.4 53.9 69.5Average post sequence length4.1 4.0 2.6 2.5(a) Statistics of stance-labeled postsABO GAY OBA MARReason-labeled posts463 561 447 432% of sentences w/ reason tags20.4 29.8 34.4 43.7Kappa (sentence)0.66 0.63 0.61 0.67Kappa (post)0.82 0.80 0.78 0.83(b) Statistics of reason-labeled postsTable 2: Stance and reason annotation statistics.to present a reason.
Each case of disagreement isresolved through discussion among the annotators.3 Baseline RC SystemOur baseline system uses a maximum entropy(MaxEnt) classifier to determine whether a reasonis expressed in a post and/or its sentence(s).
Wecreate one training instance for each sentence ineach post in the training set, using the reason labelas its class label.
We represent each instance usingfive types of features, as described below.N-gram features.
We encode each unigram andbigram collected from the training sentences as a753binary feature indicating the n-gram?s presence orabsence in a given sentence.Dependency-based features.
To capture theinter-word relationships that n-grams may not,we employ the dependency-based features previ-ously used for stance classification in Anand etal.
(2011).
These features have three variants.
Inthe first variant, the pair of arguments involved ineach dependency relation extracted by a depen-dency parser is used as a feature.
The second vari-ant is the same as the first except that the head (i.e.,the first argument in a relation) is replaced by itspart-of-speech tag.
The features in the third vari-ant, the topic-opinion features, are created by re-placing each sentiment-bearing word in features ofthe first two types with its corresponding polaritylabel (i.e., + or ?
).Frame-semantic features.
While dependency-based features capture the syntactic dependencies,frame-semantic features encode the semantic rep-resentation of the concepts in a sentence.
Fol-lowing our previous work on stance classification(Hasan and Ng, 2013c), we employ three typesof features computed based on the frame-semanticparse of each sentence in a post obtained from SE-MAFOR (Das et al., 2010).
Frame-word interac-tion features encode whether two words appear indifferent elements of the same frame.
Hence, eachframe-word interaction feature consists of (1) thename of the frame f from which it is created, and(2) an unordered word pair in which the words aretaken from two frame elements of f .
A frame-pairfeature is represented as a word pair correspondingto the names of two frames and encodes whetherthe target word of the first frame appears withinan element of the second frame.
Finally, frame n-gram features are a variant of word n-grams.
Foreach word n-gram in the sentence, a frame n-gramfeature is created by replacing one or more wordsin the word n-gram with the name of the frame orthe frame element in which the word appears.
Adetailed description of these features can be foundin Hasan and Ng (2013c).Quotation features.
We employ two quotationfeatures.
IsQuote is a binary feature that indicateswhether a sentence is a quote or not (i.e., whetherit appeared in its parent post in the post sequence).Note that if an instance is a quote from a previ-ous post, it is unlikely that it represents a reasonthe author is presenting to support her argument.Instead, the author may have quoted this beforestating her counter-argument.
FollowsQuote is abinary feature that indicates whether a sentencefollows a sentence for which the IsQuote featurevalue is true.
Intuitively, a sentence following aquote is likely to present a counter-argument.Positional feature.
We split each post into fourparts (such that each part contains roughly thesame number of sentences) and create one posi-tional feature that encodes which part of the postcontains a given sentence.
This feature is moti-vated by our observations on the training data that(1) reasons are more likely to appear in the secondhalf of a post and (2) on average more than one-third of the reasons appear in the last quarter of apost.After training, we can apply the resulting RCsystem to classify the test instances, which aregenerated in the same way as the training in-stances.
Once the sentences of a test post are clas-sified, we simply assume its post-level reason la-bels to be the set of reason labels assigned by theclassifier to its sentences.4 Stance-Supported RC SystemsIn this section, we propose a set of systems forRC.
Unlike the baseline RC system, these RCsystems are stance-supported, enabling us to ex-plore how different ways of modeling automati-cally computed stances and reasons can improveRC classification.
Below we present our systemsin increasing order of modeling sophistication.4.1 Pipeline SystemsWe examine two pipeline systems, P1 and P2.Given a set of test posts, both systems first de-termine the stance of each post and then apply astance-specific reason classifier to each of them.More specifically, both P1 and P2 employ twostance-specific reason classifiers: one is trained onall the posts labeled as for and the other is trainedon all the posts labeled as against.
Each stance-specific reason classifier is trained using MaxEnton the same feature set as that of the Baseline RCsystem.
It computes for a particular stance s theprobability P (r|s, t), where r is a reason label andt is a sentence in a test post p.P1 and P2 differ only with respect to the SCmodel used to stance-label each post.
In P1, thestance s of a post p is determined by applying to pa stance classifier that computes P (s|p).
To trainthe classifier, we employ MaxEnt.
Each train-754ing instance corresponds to a training post andis represented by all but the quotation and posi-tional features used to train the Baseline RC sys-tem, since these two feature types are sentence-based rather than post-based.
After training, theresulting classifier can be used to stance-label apost independently of the other posts.In P2, on the other hand, we recast SC as a se-quence labeling task.
In other words, we train aSC model that assumes as input a post sequenceand outputs a stance sequence, with one stance la-bel for each post in the input post sequence.
Thischoice is motivated by an observation we madepreviously (Hasan and Ng, 2013a): since each postin a sequence is a reply to the preceding post, wecould exploit their dependencies by determiningtheir stance labels together.3As our sequence learner, we employ a maxi-mum entropy Markov model (MEMM) (McCal-lum et al., 2000).
Given an input post sequencePS= (p1, p2, .
.
.
, pn), the MEMM finds the mostprobable stance sequence S = (s1, s2, .
.
.
, sn) bycomputing P (S|PS), where:P (S|PS) =n?k=1P (sk|sk?1, pk) (1)This probability can be computed efficiently viadynamic programming (DP), using a modified ver-sion of the Viterbi algorithm (Viterbi, 1967).There is a caveat, however.
Recall that the postsequences are generated from a thread.
Since atest post may appear in more than one sequence,different occurrences of it may be assigned differ-ent stance labels by the MEMM.
To determine thefinal stance label for the post, we average the prob-abilities assigned to the for stance over all its oc-currences; if the average is ?
0.5, then its finallabel is for; otherwise, its label is against.4.2 System based on Joint InferenceOne weakness of the pipeline systems is that errorsmay propagate from the SC system to the RC sys-tem.
If the stance of a post is incorrectly labeled,its reasons will also be incorrectly labeled.To avoid this problem, we employ joint infer-ence.
Specifically, we first train a SC system and3While we could similarly recast the problem of assigningreasons to the sentences in a post as a sequence learning task,we did not pursue this idea further because preliminary ex-periments indicated that sequence learning for RC was inef-fective: there is little, if any, dependency between the reasonlabels in consecutive sentences.a RC system independently of each other.
We em-ploy the Baseline as our RC system, since this isthe only RC system that is not stance-specific.
Forthe SC system, we employ P2.Since the SC system and the RC system aretrained independently of each other, their outputsmay not be consistent.
For instance, an inconsis-tency arises if a post is labeled as for but one ormore of its reasons are associated with the oppos-ing stance.
In fact, an inconsistency can arise inthe output of the RC system alone: reasons associ-ated with both stances may be assigned by the RCsystems to different sentences of a given post.To enforce consistency, we apply integer lin-ear programming (ILP) (Roth and Yih, 2004).
Weformulate one ILP program for each debate post.Each ILP program contains two post-stance vari-ables (xforand xagainst) and |T | ?
|LR| reasonvariables (i.e., one indicator variable zt,rfor eachreason class r and each sentence t), where |T | isthe number of sentences in the post and |LR| is thenumber of reason labels.
Our objective is to maxi-mize the linear combination of these variables andtheir corresponding probabilities assigned by theirrespective classifiers (see (2) below) subject to twotypes of constraints, the integrity constraints andthe post-reason constraints.
The integrity con-straints ensure that each post is assigned exactlyone stance and each sentence in a post is assignedexactly one reason class (see the two equality con-straints in (3)).
The post-reason constraints ensureconsistency between the predictions made by theSC and the RC systems.
Specifically, (1) if thereis at least one reason supporting the for stance, thepost must be assigned a for label; and (2) a forpost must have at least one for reason.
These con-straints are defined for the against label as well(see the constraints in (4)).Maximize:?s?LSasxs+1|T ||T |?t=1?r?LRbt,rzt,r(2)subject to:?s?LSxs= 1, ?
t?r?LRzt,r= 1,xs?
{0, 1}, zt,r?
{0, 1}(3)?
t xs?
zt,r,|T |?t=1zt,r?
xs(4)755Note that (1) asand bt,rare two sets of probabil-ities assigned by the SC and RC systems respec-tively; (2) LSand LRdenote the set of stancelabels and reason labels respectively; and (3) thefraction1|T |ensures that both classifiers are con-tributing equally to the objective function.4.3 Systems based on Joint LearningAnother way to avoid the error propagation prob-lem in pipeline systems is to perform joint learn-ing.
In joint learning, the two tasks, SC and RC,are learned jointly.
Below we propose three jointmodels in increasing level of sophistication.J1 is a joint model that, given a test post p, findsthe stance label s and the reason label for each ofthe sentences that together maximize the probabil-ity P (Rp, s|p), where Rp= (r1, r2, .
.
.
, rn) is thesequence of reason labels with ri(1 ?
i ?
n)being the reason label assigned to ti, the i-th sen-tence in p. Using Chain Rule,P (Rp, s|p) = P (s|p)P (Rp|s, p)= P (s|p)n?i=1P (ri|s, ti)(5)Hence, P (Rp, s|p) can be computed by usingthe stance-specific RC classifier and the SC classi-fier employed in P1.The second joint model, J2, is the same asJ1, except that we recast SC as a sequence la-beling task.
As before, we employ MEMM tolearn how to predict stance sequences.
Given apost sequence PS= (p1, p2, .
.
.
, pn), J2 finds thestance sequence S = (s1, s2, .
.
.
, sn) and rea-sons R = (R1, R2, .
.
.
, Rn) that jointly maximizeP (R,S|PS).
Note that Riis the sequence of rea-son labels assigned to the sentences in post i.The R and S that jointly maximize P (R,S|PS)can be found efficiently via DP, using a modifiedversion of the Viterbi algorithm.
Unlike in P2, inJ2 the decoding process is slightly more compli-cated because we have to take into account Ri.
Be-low we show the recursive definitions used to com-pute the entries in the DP table, where vk(h) is the(k,h)-th entry of the table; P (h|p) is provided bythe MaxEnt stance classifier used in P1; P (h|j, p)is provided by the MEMM stance classifier usedin P2; P (rmaxi|h, ti) is provided by the stance-specific reason classifier used in the pipeline sys-tems; and rmaxiis the reason label for sentence tithat has the highest probability according to thereason classifier.Base case:v1(h) = P (h|p)n?i=1P (rmaxi|h, ti) (6)Recursive definition:vk(h) = maxjvk?1(j)P (h|j, p)n?i=1P (rmaxi|h, ti)(7)To motivate our third joint model, J3, we makethe following observation.
Recall that a post ina post sequence is a reply to its preceding post.An inspection of the training data reveals that inmany cases, a reply is a rebuttal to the preced-ing post, where an author attempts to argue whythe points or reasons raised in the preceding postare wrong and then provides her reasons for theopposing stance.
Motivated by this observation,we hypothesize that the reasons mentioned in thepreceding post could be useful for predicting thereasons in the current post.
However, none of themodels we have presented so far makes use of thereasons predicted for the preceding post.This motivates the design of J3, which we buildon top of J2.
Specifically, to incorporate the reasonlabels predicted for the preceding post in a post se-quence, we augment the feature set of the stance-specific reason classifiers with a set of reason fea-tures, with one binary feature for each reason.
Thevalue of a reason feature is 1 if and only if the cor-responding reason is predicted to be present in thepreceding post.
Hence, in J3, we can apply thesame DP equations we used in J2 except that theset of features used by the reason classifier is aug-mented with the reason features.5 EvaluationWhile our primary goal is to evaluate the RC sys-tems introduced in the previous section, we arealso interested in whether SC performance can im-prove when SC is jointly modeled with RC.
Morespecifically, our evaluation is driven by the follow-ing question: will RC performance and SC perfor-mance improve as we employ more sophisticatedmethods for modeling reasons and stances?
Be-fore showing the results, we describe the metricsfor evaluating RC and SC systems.756SystemABO GAY OBA MARStanceReasonStanceReasonStanceReasonStanceReasonSentence Post Sentence Post Sentence Post Sentence PostBaseline ?
32.7 45.0 ?
23.3 40.5 ?
19.5 31.5 ?
28.7 44.2P1 62.8 34.5 46.3 63.4 24.5 43.2 61.0 20.3 33.5 67.2 30.5 47.3P2 65.1 36.1 47.7 64.2 26.6 45.5 63.8 21.1 34.4 68.5 32.9 48.8ILP 65.2 36.5 48.4 64.6 28.0 46.7 63.6 22.8 35.0 68.8 33.1 48.9J1 62.5 36.0 47.6 64.0 26.7 45.6 61.2 23.1 35.7 67.8 33.3 49.2J2 65.9 37.9 50.6 65.3 29.6 48.5 63.5 24.5 37.1 68.7 34.5 50.5J3 66.3 39.5 52.3 65.7 31.4 49.8 64.0 25.1 38.0 69.0 35.1 51.1Table 3: SC accuracies and RC F-scores for our five-fold cross-validation experiments.5.1 Experimental SetupWe express SC results in terms of accuracy (i.e.,the percentage of test posts labeled with the cor-rect stance) and RC results in terms of F-scoremicro-averaged over all reason classes except theNONE class.
For each RC system, we report itssentence-level RC score and post-level RC score,which are computed over sentences and posts re-spectively.
As mentioned at the end of Section 3,the set of post-level reason labels of a given postis automatically obtained by taking the union ofthe set of reason labels assigned to each of its sen-tences.
Hence, a reason classifier will be rewardedas long as it can predict, for any sentence in a testpost, a reason label that the annotators assigned tosome sentence in the same post.We obtain these scores via five-fold cross-validation experiments.
During fold partition, allposts that are in the same post sequence are as-signed to the same fold.
All reason and stanceclassifiers are domain-specific, meaning that eachof them is trained on sentences/posts from ex-actly one domain and is applied to classify sen-tences/posts from the same domain.
We use theStanford maximum entropy classifier4for classifi-cation and solve ILP programs using lpsolve5.5.2 Results and DiscussionResults are shown in Table 3.
Each row corre-sponds to one of our seven RC systems, showingits SC accuracy as well as its sentence- and post-level RC F-scores for each domain.Let us begin by discussing the RC results.
First,P1 and P2 significantly beat the Baseline on all4http://nlp.stanford.edu/software/classifier.shtml5http://sourceforge.net/projects/lpsolve/four domains by an average of 1.4 and 3.1 pointsat the sentence level and by an average of 2.3 and3.8 points at the post level respectively.6These re-sults show that stance information can indeed beprofitably used for RC even if it is incorporatedinto RC systems in a simple manner.
Second,improving SC through sequence learning can im-prove RC: the systems in which SC is recast as se-quence labeling (P2 and J2) perform significantlybetter than the corresponding systems that do not(P1 and J1).
Third, ILP significantly beats P2 ontwo domains (ABO and GAY) and achieves thesame level of performance as P2 on the remain-ing domains.
These results suggest that joint infer-ence is no worse (and sometimes even better) thanpipeline learning as far as exploiting stance infor-mation for RC is concerned.
Fourth, the systemstrained via joint learning (J1 and J2) beat their cor-responding pipeline counterparts (P1 and P2) onall four domains, significantly so by an average of2.3 and 2.5 points at the sentence level and by anaverage of 2.0 and 2.6 points at the post level re-spectively, suggesting that joint learning is indeeda better way to incorporate stance information thanpipeline learning.
Finally, J3, the joint system thatexploits reasons predicted for the previous post,significantly beats J2, the system on which it isbuilt, by 1.6 and 1.8 points at the sentence leveland by 1.7 and 1.3 points at the post level for ABOand GAY respectively.
It also yields small, statis-tically insignificant, improvements (0.6 points atthe sentence level and 0.6?0.9 points at the postlevel) for the remaining two domains.
These re-sults suggest that the reasons predicted for the pre-vious post indeed provide useful information forpredicting the current post?s reasons.Overall, these results are consistent with our hy-6All significance tests are paired t-tests (p < 0.05).757pothesis that the usefulness of stance informationdepends in part on the way it is exploited, andthat RC performance increases as we employ moresophisticated methods for modeling reasons andstances.
Our best system, J3, significantly beatsthe Baseline by an average of 6.7 and 7.5 points atthe sentence and post levels respectively.As mentioned earlier, a secondary goal of thiswork is to determine whether joint modeling canimprove SC as well.
For that reason, we com-pare the performances of the best pipeline model(P2) and the best joint model (J3) on each domain.We find that in terms of SC accuracy, J3 is signifi-cantly better than P2 on ABO and GAY, and yieldsslightly, though insignificantly, better performanceon the remaining two domains.
In other words, ourresults suggest that joint modeling of SC and RChas a positive impact on SC performance on alldomains, and the impact can sometimes be largeenough to yield significantly better results.5.3 Further ComparisonWe hypothesized in the introduction that thesentence-based approach to post-level RC wouldyield better performance than the multi-label textclassification approach.
In Section 5.2, we pre-sented results of the sentence-based approach toRC.
So, to test this hypothesis, we next evaluatethe multi-label text classification approach to RC.Recall that the multi-label text classification ap-proach assumes the following setup.
Given a setof training posts where each post is multi-labeledwith the set of reasons it contains, the goal is totrain a system to determine the set of reasons atest post contains.
Hence, unlike in the sentence-based approach, in this approach no sentence-levelreason annotations are exploited during training.We implement this approach by recasting multi-label text classification as n binary text classifica-tion tasks, where n is the number of reason classesfor a domain.
In the binary classification task forpredicting reason i, we train a binary classifier ciusing the one-versus-all training scheme.
Specif-ically, to train ci, we create one training instancefor each post p in the training set, labeling it aspositive if and only if p contains reason i. Notethat if i is a minority reason, the class distributionof the resulting training set will be highly skewedtowards the negatives, which will in turn cause theresulting MaxEnt classifier to be biased towardspredicting a test instance as negative.To address this problem, we adjust the classifi-cation thresholds associated with the binary classi-fiers.
Recall that a test instance is classified as pos-itive by a binary classifier if and only if its prob-ability of belonging to the positive class is abovethe classification threshold used.
Hence, adjustingthe threshold amounts to adjusting the number oftest instances classified as positive, thus address-ing the bias problem mentioned above.
Specifi-cally, we adjust the thresholds of the classifiers asfollows.
We train the binary classifiers to optimizethe overall F-score by jointly tuning their classifi-cation thresholds on 25% of the training data re-served for development purposes.
Since comput-ing the exact solution to this optimization prob-lem is computationally expensive, we employ a lo-cal search algorithm that changes the value of onethreshold at a time to optimize F-score while keep-ing the remaining thresholds fixed.
During testing,classifier ciwill classify a test instance as positiveif its probability of belonging to the positive classis above the corresponding threshold.We apply this multi-label text classification ap-proach to obtain post-level RC scores for the Base-line, P1 and P2.
Note that since P1 and P2 arepipeline systems, the binary classifiers they use topredict a test post?s reasons depend on the post?spredicted stance.
Specifically, if a test post is pre-dicted to have a positive (negative) stance, thenonly the reason classifiers associated with the pos-itive (negative) stance will be used to predict thereasons it contains.
On the other hand, this ap-proach cannot be used in combination with ILP orthe joint models to produce post-level RC scores:they all require a reason classifier trained onreason-annotated sentences, which are not avail-able in the multi-label text classification approach.Post-level RC results of the Baseline and thetwo pipeline systems, P1 and P2, obtained via thismulti-label text classification approach are shownin Table 4.
These scores are significantly lowerthan the corresponding scores in Table 3 by 3.2,2.9, and 3.1 points for the Baseline, P1, and P2 re-spectively, when averaged over the four domains.They confirm our hypothesis that the sentence-based approach to post-level RC is indeed betterthan its multi-class text classification counterpart.5.4 Error AnalysisTo get a better understanding of our best-performing RC system (J3), we examine its major758System ABO GAY OBA MARBaseline 39.8 37.9 30.1 40.8P1 41.5 41.0 31.7 44.7P2 43.3 42.6 32.0 46.3Table 4: Post-level RC F-scores obtained via themulti-class text classification approach.sources of error in this subsection.For the four domains, 75?83% of the errorscan be attributed to the system?s inability to de-cide whether a sentence describes a reason or not.Specifically, in 51?54% of the erroneous cases, areason sentence is misclassified as NONE.
On theother hand, 23?30% of the cases are concernedwith assigning a reason label to a NONE sentence.The remaining 17?25% of the errors concern mis-labeling a reason sentence with the wrong reason.A closer examination of the errors reveals thatthey resulted primarily from (1) the lack of accessto background knowledge, (2) the failure to pro-cess complex discourse structures, and (3) the fail-ure to process sarcastic statements and rhetoricalquestions.
We present two examples for each ofthese three major sources of error from the ABOand OBA domains in Table 5.
In each example,we show their predicted (P) and gold (G) labels.Lack of access to background knowledge.Consider the first example for ABO in Table 5.Our system misclassifies this sentence in part be-cause it lacks the background knowledge that ?ge-netic code?
is one of the characteristics of life anda fetus having it means a fetus has life (A3).
Sim-ilarly, the system cannot determine the reason forthe first OBA example without the knowledge that?deficit spending?
is a term related to the econ-omy and that increasing it is bad (A1).
We believesome of these relations can be extracted from lex-ical knowledge bases such as YAGO2 (Suchaneket al., 2007), Freebase (Bollacker et al., 2008), andBabelNet (Navigli and Ponzetto, 2012).Failure to process complex discourse struc-tures.
Our system misclassifies the second ex-ample for ABO in part because the first part ofthe sentence (i.e., Sure, the fetus has the potentialto one day be a person) expresses a meaning thatis completely inverted by the second part.
Suchcomplex discourse structures often lead to classi-fication errors even for sentences whose interpre-tation requires no background knowledge.
We be-lieve that this problem can be addressed in partby a better understanding of the structure of a dis-course, particularly the relation between two dis-course segments, using a discourse parser.Failure to process sarcastic statements andrhetorical questions.
Owing to the nature ofour dataset (i.e., debate posts), many errors arisefrom sentences containing sarcasm and/or rhetori-cal questions.
This is especially a problem in longpost sequences, where authors frequently restatetheir opponents?
positions, sometimes ironically.A first step towards handling these errors wouldtherefore be to identify sentences containing sar-casm and/or rhetorical questions.6 Related WorkIn this section, we discuss related work in the areasof document-level RC, argument recognition, tex-tual entailment in online debates, argumentationmining, and sentiment analysis.Document-level reason classification.
Persingand Ng (2009) apply a multi-label text classifi-cation approach to document-level RC of aviationsafety incident reports.
Given a set of pre-definedreasons, their RC system seeks to identify the rea-sons that can explain why the incident describedin a given report occurred.
Their work is dif-ferent from ours in at least two respects.
First,while our posts occur in post sequences (whichcan be profitably exploited in RC, for example, asin J3), their incident reports were written indepen-dently of each other.
Second, they do not performsentence-level RC, as the lack of sentence-levelreason annotations in their dataset prevented themfrom training a sentence-level reason classifier.Argument recognition.
Boltuz?ic?
and?Snajder(2014) propose a multi-class classification taskcalled argument recognition in online discussions.Given a post and a reason for a particular domain,the task is to predict the extent to which the au-thor of the post supports or opposes the reasonas measured on a five-point ordinal scale rang-ing from ?explicitly supports?
to ?explicitly op-poses?.
Hence, unlike RC, argument recognitionfocuses on the magnitude rather than the exis-tence of a post-reason relationship.
In addition,Boltuz?ic?
and?Snajder focus on post-level (ratherthan sentence-level) classification and employ per-fect (rather than predicted) stance information.Textual entailment in online debates.
Giventhe title of a debate and a post written in re-sponse to it, this task seeks to detect arguments in759DomainBackground knowledge Complex discourse structure Sarcasm/rhetorical questionsExample P G Example P G Example P GABO Science does agree thatthe fetus has an individ-ual genetic code and fitsinto the biological defi-nition of life.NONE A3 Sure, the fetus has thepotential to one day bea person, but right nowit is not.NONE F3 So are there enoughhomes for 50,000,000babies?F9 F5OBA Democrats haveincreased deficit spend-ing by 2 trillion dollarsover 2 years.NONE A1 Bush raised the debtby two billion for thewars, Obama has out-spent that in a week.A2 A1 I agree, Bush put us indebt for the next 100years, so we can blameObama forever.NONE F3Table 5: Examples of the major sources of error.
P and G stand for predicted tag and gold tag respectively.the post that entail or contradict the title (Cabrioand Villata, 2012).
Hence, this task is concernedwith identifying text segments that correspond torationales without a predefined set of rationales,whereas RC is concerned with both identifyingtext segments and classifying them based on agiven set of reasons.Argumentation mining.
The goal of this task isto extract the argumentative structure of a docu-ment.
Researchers have proposed approaches tomine the structure of scientific papers (Teufel andMoens, 2000; Teufel, 2001), product reviews (Vil-lalba and Saint-Dizier, 2012; Wyner et al., 2012),newspaper articles (Feng and Hirst, 2011), and le-gal documents (Bru?ninghaus and Ashley, 2005;Wyner et al., 2010; Palau and Moens, 2011; Ash-ley and Walker, 2013).
A major difference be-tween this task and RC is that the argument typesrefer to generic structural cues, textual patternsetc., whereas our reason classes refer to the spe-cific reasons an author may mention to support herstance in a domain.
For instance, in the case ofa scientific article, the argument types correspondto general background, description of the paper?sor some other papers?
approach, objective, con-trastive and/or comparative comments, etc.
(Teufeland Moens, 2000).
The argument types for legaldocuments refer to legal factors which are eitherpro-plaintiff or pro-defendant (Bru?ninghaus andAshley, 2005).
For instance, for trade secret lawcases, factors such as Waiver-of-Confidentialityand Disclosure-in-Public-Forum refer to certainfacts strengthening the claim of one of the sidesparticipating in a case.Sentiment analysis.
RC resembles certain tasksin sentiment analysis.
One such task is pro and conreason classification in reviews (Kim and Hovy,2006), where sentences containing opinions aswell as reasons justifying the opinions are to beextracted and classified as PRO, CON, or NONE.Hence, this task focuses on categorizing sentencesinto coarse-grained, high-level groups (e.g., PROvs.
CON, POSITIVE vs.
NEGATIVE), but does notattempt to subcategorize the PRO and CON classesinto fine-grained reason classes, unlike RC.
Some-what similar to the PRO and CON sentence classifi-cation task is the task of determining the relevanceof a sentence in a review for polarity classifica-tion.
Zaidan et al.
(2007) coined the term ratio-nale to refer to any subjective textual content thatcontains evidence supporting the author?s opinionor stance.
These rationales, however, may not al-ways contain reasons.
For instance, a sentence thatmentions that the author likes a product is a ra-tionale, but it does not contain any reason for herliking it.
Methods have been proposed for auto-matically identifying rationales (e.g., Yessenalinaet al.
(2010), Trivedi and Eisenstein (2013)) anddistinguishing subjective from objective materialsin a review (e.g., Pang and Lee (2004), Wiebe andRiloff (2005), McDonald et al.
(2007), Zhao et al.(2008)).
Note that in all these attempts, the endgoal is not to classify sentences, but to employthe results of sentence classification to improve ahigher-level task, such as sentiment classification.7 ConclusionWe examined the new task of reason classification.We exploited stance information for reason classi-fication, proposing systems of varying complexityfor modeling stances and reasons.
Experiments onour reason-annotated corpus of ideological debateposts from four domains demonstrate that sophis-ticated models of stances and reasons can indeedyield more accurate reason and stance classifica-tion results than their simpler counterparts.
Nev-ertheless, reason classification remains a challeng-ing task: the best post-level F-scores are in the low50s.
By making our corpus publicly available, wehope to stimulate further research on this task.760AcknowledgmentsWe thank the three anonymous reviewers for theirdetailed and insightful comments on an earlierdraft of this paper.
This work was supported inpart by NSFGrants IIS-1147644 and IIS-1219142.Any opinions, findings, conclusions or recommen-dations expressed in this paper are those of the au-thors and do not necessarily reflect the views or of-ficial policies, either expressed or implied, of NSF.ReferencesRakesh Agrawal, Sridhar Rajagopalan, RamakrishnanSrikant, and Yirong Xu.
2003.
Mining newsgroupsusing networks arising from social behavior.
In Pro-ceedings of the 12th International Conference onWorld Wide Web, pages 529?535.Pranav Anand, Marilyn Walker, Rob Abbott, Jean E.Fox Tree, Robeson Bowmani, and Michael Minor.2011.
Cats rule and dogs drool!
: Classifying stancein online debate.
In Proceedings of the 2nd Work-shop on Computational Approaches to Subjectivityand Sentiment Analysis, pages 1?9.Kevin D. Ashley and Vern R. Walker.
2013.
From in-formation retrieval (IR) to argument retrieval (AR)for legal cases: Report on a baseline study.
In Pro-ceedings of the 26th InternationalConference on Le-gal Knowledge and Information System, pages 29?38.Mohit Bansal, Claire Cardie, and Lillian Lee.
2008.The power of negative thinking: Exploiting labeldisagreement in the min-cut classification frame-work.
In COLING 2008: Companion volume:Posters, pages 15?18.Kurt Bollacker, Colin Evans, Praveen Paritosh, TimSturge, and Jamie Taylor.
2008.
Freebase: A col-laboratively created graph database for structuringhuman knowledge.
In Proceedings of the 2008 ACMSIGMOD International Conference on Managementof Data, pages 1247?1250.Filip Boltuz?ic?
and Jan?Snajder.
2014.
Back up yourstance: Recognizing arguments in online discus-sions.
In Proceedings of the First Workshop on Ar-gumentation Mining, pages 49?58.Stefanie Bru?ninghaus and Kevin D. Ashley.
2005.Reasoning with textual cases.
In Proceedings of the6th International Conference on Case-Based Rea-soning, pages 137?151.Clinton Burfoot, Steven Bird, and Timothy Baldwin.2011.
Collective classification of congressionalfloor-debate transcripts.
In Proceedings of the 49thAnnual Meeting of the Association for Computa-tional Linguistics: Human Language Technologies,pages 1506?1515.Elena Cabrio and Serena Villata.
2012.
Natural lan-guage arguments: A combined approach.
In Pro-ceedings of the 20th European Conference on Artifi-cial Intelligence, pages 205?210.Jean Carletta.
1996.
Assessing agreement on classi-fication tasks: The Kappa statistic.
ComputationalLinguistics, 22(2):249?254.Dipanjan Das, Nathan Schneider, Desai Chen, andNoah A. Smith.
2010.
SEMAFOR 1.0: Aprobabilistic frame-semantic parser.
Technical re-port, Carnegie Mellon University Technical ReportCMU-LTI-10-001.Vanessa Wei Feng and Graeme Hirst.
2011.
Classi-fying arguments by scheme.
In Proceedings of the49th Annual Meeting of the Association for Com-putational Linguistics: Human Language Technolo-gies, pages 987?996.Kazi Saidul Hasan and Vincent Ng.
2013a.
Extra-linguistic constraints on stance recognition in ide-ological debates.
In Proceedings of the 51st AnnualMeeting of the Association for Computational Lin-guistics (Volume 2: Short Papers), pages 816?821.Kazi Saidul Hasan and Vincent Ng.
2013b.
Frame se-mantics for stance classification.
In Proceedings ofthe Seventeenth Conference on Computational Nat-ural Language Learning, pages 124?132.Kazi Saidul Hasan and Vincent Ng.
2013c.
Stanceclassification of ideological debates: Data, mod-els, features, and constraints.
In Proceedings ofthe Sixth International Joint Conference on NaturalLanguage Processing, pages 1348?1356.Soo-Min Kim and Eduard Hovy.
2006.
Automaticidentification of pro and con reasons in online re-views.
In Proceedings of the COLING/ACL MainConference Poster Sessions, pages 483?490.Andrew McCallum, Dayne Freitag, and FernandoPereira.
2000.
Maximum entropy Markov mod-els for information extraction and segmentation.
InProceedings of the 17th International Conference onMachine Learning, pages 591?598.Ryan McDonald, Kerry Hannan, Tyler Neylon, MikeWells, and Jeff Reynar.
2007.
Structured models forfine-to-coarse sentiment analysis.
In Proceedings ofthe 45th Annual Meeting of the Association of Com-putational Linguistics, pages 432?439.Roberto Navigli and Simone Paolo Ponzetto.
2012.BabelNet: The automatic construction, evaluationand application of a wide-coverage multilingual se-mantic network.
Artificial Intelligence, 193:217?250.Raquel Mochales Palau and Marie-Francine Moens.2011.
Argumentationmining.
Artificial Intelligenceand Law, 19(1):1?22.761Bo Pang and Lillian Lee.
2004.
A sentimental educa-tion: Sentiment analysis using subjectivity summa-rization based on minimum cuts.
In Proceedings ofthe 42nd Meeting of the Association for Computa-tional Linguistics, pages 271?278.Isaac Persing and Vincent Ng.
2009.
Semi-supervisedcause identification from aviation safety reports.
InProceedings of the Joint Conference of the 47th An-nual Meeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processingof the AFNLP, pages 843?851.Dan Roth and Wen-tau Yih.
2004.
A linear program-ming formulation for global inference in natural lan-guage tasks.
In Proceedings of the Eighth Confer-ence on ComputationalNatural Language Learning,pages 1?8.Swapna Somasundaran and Janyce Wiebe.
2009.
Rec-ognizing stances in online debates.
In Proceed-ings of the Joint Conference of the 47th AnnualMeeting of the ACL and the 4th International JointConference on Natural Language Processing of theAFNLP, pages 226?234.Fabian M. Suchanek, Gjergji Kasneci, and GerhardWeikum.
2007.
YAGO: A core of semantic knowl-edge.
In Proceedings of the 16th InternationalWorld Wide Web Conference, pages 697?706.Simone Teufel and Marc Moens.
2000.
What?s yoursand what?s mine: Determining intellectual attribu-tion in scientific text.
In Proceedings of the 2000Joint SIGDAT Conference on Empirical Methods inNatural Language Processing and Very Large Cor-pora, pages 9?17.Simone Teufel.
2001.
Task-based evaluation of sum-mary quality: Describing relationships between sci-entific papers.
In Proceedings of the NAACL Work-shop on Automatic Summarization, pages 12?21.Matt Thomas, Bo Pang, and Lillian Lee.
2006.
Get outthe vote: Determining support or opposition fromcongressional floor-debate transcripts.
In Proceed-ings of the 2006 Conference on Empirical Methodsin Natural Language Processing, pages 327?335.Rakshit Trivedi and Jacob Eisenstein.
2013.
Dis-course connectors for latent subjectivity in sentimentanalysis.
In Proceedings of the 2013 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, pages 808?813.Maria Paz Garcia Villalba and Patrick Saint-Dizier.2012.
Some facets of argument mining for opinionanalysis.
In Proceedings of the Fourth InternationalConference on Computational Models of Argument,pages 23?34.Andrew J. Viterbi.
1967.
Error bounds for convolu-tional codes and an asymptotically optimum decod-ing algorithm.
IEEE Transactions on InformationTheory, 13(2):260?269.MarilynWalker, Pranav Anand, Rob Abbott, and RickyGrant.
2012.
Stance classification using dialogicproperties of persuasion.
In Proceedings of the 2012Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 592?596.Janyce Wiebe and Ellen Riloff.
2005.
Creating subjec-tive and objective sentence classifiers from unanno-tated texts.
In Proceedings of the 6th InternationalConference on Computational Linguistics and Intel-ligent Text Processing, pages 486?497.AdamWyner, Raquel Mochales-Palau, Marie-FrancineMoens, and David Milward.
2010.
Approaches totext mining arguments from legal cases.
In EnricoFrancesconi, Simonetta Montemagni, Wim Peters,and Daniela Tiscornia, editors, Semantic Processingof Legal Texts: Where the Language of Law Meetsthe Law of Language, pages 60?79.
Springer-Verlag.Adam Wyner, Jodi Schneider, Katie Atkinson, andTrevor J. M. Bench-Capon.
2012.
Semi-automatedargumentative analysis of online product reviews.
InProceedings of the Fourth International Conferenceon Computational Models of Argument, pages 43?50.Ainur Yessenalina, Yejin Choi, and Claire Cardie.2010.
Automatically generating annotator rationalesto improve sentiment classification.
In Proceedingsof the ACL 2010 Conference Short Papers, pages336?341.Omar Zaidan, Jason Eisner, and Christine Piatko.2007.
Using ?annotator rationales?
to improve ma-chine learning for text categorization.
In HumanLanguage Technologies 2007: The Conference ofthe North American Chapter of the Association forComputational Linguistics; Proceedings of the MainConference, pages 260?267.Jun Zhao, Kang Liu, and Gen Wang.
2008.
Adding re-dundant features for crfs-based sentence sentimentclassification.
In Proceedings of the 2008 Confer-ence on Empirical Methods in Natural LanguageProcessing, pages 117?126.762
