Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 98?106,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsEmotional Perception of Fairy Tales:Achieving Agreement in Emotion Annotation of TextEkaterina P. Volkova1,2, Betty J. Mohler2, Detmar Meurers1, Dale Gerdemann1, Heinrich H. Bu?lthoff21 Universita?t Tu?bingen, Seminar fu?r Sprachwissenschaft19 Wilchelmstr., Tu?bingen, 72074, Germany2 Max Planck Institute for Biological Cybernetics38 Spemannstr., Tu?bingen, 72076, GermanyAbstractEmotion analysis (EA) is a rapidly developingarea in computational linguistics.
An EAsystem can be extremely useful in fields suchas information retrieval and emotion-drivencomputer animation.
For most EA systems,the number of emotion classes is very limitedand the text units the classes are assignedto are discrete and predefined.
The questionwe address in this paper is whether the setof emotion categories can be enriched andwhether the units to which the categoriesare assigned can be more flexibly defined.We present an experiment showing how anannotation task can be set up so that untrainedparticipants can perform emotion analysiswith high agreement even when not restrictedto a predetermined annotation unit and usinga rich set of emotion categories.
As such itsets the stage for the development of morecomplex EA systems which are closer to theactual human emotional perception of text.1 IntroductionAs a first step towards developing an emotionanalysis (EA) system simulating human emotionalperception of text, it is important to research thenature of the emotion analysis performed by humansand examine whether they can reliably performthe task.
To investigate these issues, we conductedan experiment to find out the strategies peopleuse to annotate selected folk fairy tale texts foremotions.
The participants had to choose from a setof fifteen emotion categories, a significantly largerset than typically used in EA, and assign them to anunrestricted range of text.To explore whether human annotators can reliablyperform a task, inter-annotator agreement (IAA)(Artstein and Poesio, 2008) is the relevant measure.This measure can be calculated between every twoindividual annotations in order to find pairs or eventeams of annotators whose strategies seem to beconsistent and coherent enough so that they can beused further as the gold-standard annotation suitedto train a machine learning approach for automaticEA analysis.
A resulting EA system, capable ofsimulating human emotional perception of text,would be useful for information retrieval and manyother fields.There are two main aspects of the resulting anno-tations to be researched.
First, how consistently canpeople perceive and locate the emotional aspect offairy tale texts?
Second, how do they express theirperception of text by means of annotation strategies?In the next sections, we address these questions andprovide details of an experiment we conducted toempirically advance our understanding of the issues.2 Motivation and Aimed ApplicationMost existing EA systems are implemented for andused in specific predefined areas.
The applicationfield could be anything from extracting appraisalexpressions (Whitelaw et al, 2005) to opinionmining of customer feedback (Lee et al, 2008).In our case, the intended application of the EAsystem predominantly is emotion enhancement ofhuman-computer interaction, especially in virtualor augmented reality.
Emotion enhancement of98computer animation, especially when it deals withspoken or written text, is primarily done throughmanual annotation of text, even if a rich databaseof perceptually guided animations for behavioralscripts compilation is available (Cunningham andWallraven, 2009).
The resulting system of ourproject is meant to be a bridge between unprocessedinput text (generated or provided) and visual andauditory information, coming from the virtualcharacter, like generated speech, facial expressionsand body language.
In this way a virtual characterwould be able to simulate emotional perception andproduction of text in story telling scenarios.3 Related WorkAlthough EA is often referred to as a developingfield, the amount of work carried out during the lastdecades is phenomenal.
This section is not meant asa full overview of the related research as that scopeis too great for the length of this paper.
To contextu-alize the research presented in this paper we focus onthe projects that inspired us and fostered the ideas.The work done by Alm (Alm and Sproat, 2005;Alm et al, 2005; Alm, 2008) is close to ourproject in its sprit and goals.
Alm, (2008) aims atimplementing affective text-to-speech system forstorytelling scenarios.
An EA system, detectingsentences with emotions expressed in written textis a crucial element for achieving this goal.
Theannotated corpus was composed of three sets ofchildren?s stories written by Beatrix Potter, H. C.Andersen, and the Brothers Grimm.Like Liu et al (2003), Alm (2008) uses sev-eral emotional categories, while most research inautomatic EA works with pure polarities.
The setof emotion categories used is essentially the list ofbasic emotions (Ekman, 1993), which has a justifiedpreference for negative emotion categories.
Ek-mann?s list of basic emotions was extended by Alm,since the emotion of surprise is validly taken as am-bivalent and was thus split into positive surprise andnegative surprise.
The EA system described in Almet al (2005) is machine learning based, where theEA problem is defined as multi-class classificationproblem, with sentences as classification units.Liu et al (2003) have combined an emotionlexicon and handcrafted rules, which allowed themto create affect models and thus form a representa-tion of the emotional affinity of a sentence.
Theirannotation scheme is also sentence-based.
TheEA system was tested on short user-composed textemails describing emotionally colored events.In the research on recognizing contextual polaritydone by Wilson et al (2009) a rich prior-polaritylexicon and dependency parsing technique wereemployed to detect and analyze subjectivity onphrasal level, taking into account all the power ofcontext, captured through such features as negation,polarity modification and polarity shifters.
Thework presents auspicious results of high accuracyscores for classification between neutrality andpolarized private states and between negative andpositive subjective phrases.
A detailed accountof several ML algorithms performance tests isdiscussed in thought-provoking manner.
This workencouraged us to build a lexicon of subjective cluesand use sentence structure information for futurefeature extraction and ML architecture training.Another thought-provoking work by Polanyj(2006) shows the influence of the context on subjec-tive clues.
This is relevant to our project since weare collecting lexicons of subjective clues and themechanisms of contextual influence may prove tobe of value for future automatic EA system training.Bethard et at.
(2004) provide valuable informa-tion about corpus annotation for EA means and giveaccounts on the performance of various existing MLalgorithms.
They provide excellent analysis of au-tomatic extraction of opinion proposition and theirholders.
For feature extraction, the authors employsuch well-known resources as WordNet (Miller etal., 1990), PropBank (Kingsbury et al, 2002) andFrameNet (Baker et al, 1998).
Several types ofclassification tasks involve evaluation on the levelof documents.
For example, detecting subjectivesentences, expressions, and other opinionated itemsin documents representing certain press categories(Wiebe et al, 2004) and measuring strength ofsubjective clauses (Wilson et al, 2004).
All theseand many more helped us to decide upon our ownstrategies, provided many examples of corpus col-lection and annotation, feature extraction and MLtechniques usage in ways specific for the EA task.994 Experimental SetupHaving established the research context, we nowturn to the questions we investigate in this paper:the use of an enriched category set and the flexibleannotation units, and their influence on annotationquality.
We describe the experiment we conductedand its main results.
Each participant performedseveral tasks for each session.
The first task alwayswas a cognitive task on emotion categories takenoutside the fairy tales context.
The results are dis-cussed in Sections 4.1 and 4.2.
The next assignmentdiscussed in Section 4.3 was to annotate a list ofwords for their inherent polarities.
The third taskwas to read the text out loud to the experimenter.This allowed the participant to feel immersed intothe story telling scenario and also get used to thetext of the story they were about to annotate forthe full set of emotion categories.
The annotationprocess is described in Section 4.4.
The last exercisewas to read the full fairy tale text out loud again,with the difference that this time their voice andface were recorded by means of a microphone and acamera.
The potential importance of the extra datasources like speech melody and facial expressionsare further discussed in Section 8 as future work.Ten German native speakers voluntarily partic-ipated in the experiment.
The participants weredivided into two groups and each participant workedon five of the eight texts.
The fairy tale sets for eachgroup overlapped in two texts, which allowed us toachieve a high number of individual annotations in ashort amount of time and compare the performanceof people working on different sets of texts (seeTable 1).
Each participant annotated their texts infive sessions, dealing with only one text per session.The fatigue effect was avoided as no annotator hadmore than one session a day.4.1 Determining Emotion CategoriesFirst, we needed to define the set of emotions tobe used in the experiment.
Based on the currentemotion theories from comparative literature andcognitive psychology (Ekman, 1993; Auracher,2007; Fontaine et al, 2007), we compiled a set offifteen emotion categories: seven positive, sevennegative, and neutral (see Table 2).
We chose anequal number of negative and positive emotions,User Fairy Tale IDJG D R BR FH DS BM SJA1 ?
?
?
?
?A2 ?
?
?
?
?A3 ?
?
?
?
?A4 ?
?
?
?
?A5 ?
?
?
?
?A6 ?
?
?
?
?A7 ?
?
?
?
?A8 ?
?
?
?
?A9 ?
?
?
?
?A10 ?
?
?
?
?Table 1: Annotation SetsPositive NegativeEntspannung (relief) Unruhe (disturbance)Freude (joy) Trauer (sadness)Hoffnung (hope) Verzweiflung ( despair)Interesse (interest) Ekel (disgust)Mitgefu?hl (compassion) Hass (hatred)U?berraschung (surprise) Angst (fear)Zustimmung (approval) A?rger (anger)Table 2: Emotion Categories Used in the Experimentsince in our experiment the main focus is on thefreedom and equality of choice of emotion cate-gories.
We aimed at the set to be comprehensive andwe also expected the participants to be able to detecteach of the emotions in the text as well as expressthem through speech melody and facial expressions.The polarity of each category was determinedexperimentally.
Participants were asked to decideon the underlying polarity of each emotion categoryand then to evaluate each emotion on an intensityscale [1:5], ?5?
marking extreme polarization, ?1?being close to neutral.
All participants were in fullagreement concerning the underlying polarity ofthe emotions in the set, while the numerical valuesvaried.
It is important to note, that the categoryU?berraschung (surprise) was stably estimated aspositive.
In English the word surprise is reportedto be ambivalent (Alm and Sproat, 2005), but wefound that in German its most common translationis clearly positive.4.2 Emotion Categories ClusteringIn the second part of the experiment we asked partic-ipants to organize the fifteen emotions into clusters.Each cluster was to represent a situation in which100Cluster Polarity{relief, hope, joy} positive{joy, surprise} positive{joy, approval} positive{approval, interest} positive{disgust, anger, hatred} negative{fear, despair, disturbance} negative{fear, disturbance, sadness} negative{sadness, compassion} mixedTable 3: Emotion Clustersseveral emotions were equally likely to co-occur,e.g.
a situation formulated by a participant as ?Whena friend gives me a nicely wrapped birthday presentand I am about to open it.?
was reported to involvesuch emotions as joy, interest and surprise.
Onaverage, each participant has formed 5 clusters with3?4 items per cluster.
The clusters were encoded assets on unordered pairs of items.
Pairs were filteredout if they were indicated by fewer than seven par-ticipants.
As the result, the following eight clusterswere obtained (see Table 3).
For most clusters, thecategories composing them share one polarity.
The{sadness, compassion} cluster is the only exception.It is important to note that the clusters weredetermined through this cognitive task, indepen-dently of the annotations.
Since the annotatorsagree well on clustering the emotions, employingthis information captures conceptual agreementbetween individual annotations even if the specificemotion categories for the same stretch of text donot coincide.
However, we intend to keep the fullset of emotions for the future corpus expansions.4.3 Word list AnnotationFor each text, we compiled its word list by taking theset of words contained in the text, normalizing eachword to its lemma and filtering the set for most com-mon German stop words (function words, pronouns,auxiliaries).
Like full story texts, word lists weredivided into two annotation sets.
At each session,before seeing the full text of the fairy tale, the partic-ipant was to annotate each item of the correspondingword list for its inherent polarity.
All the words weretaken out their contexts and were neutral by default.The annotator?s task was to label only those wordsthat had the potential to change the polarity of thecontext in which they could occur.
We purposefullyGerman Title English Title Abbr.Arme Junge im Grab Poor Boy in Grave JGBremer Stadtmusikanten Bremen Musicians BMDornro?schen Little Briar-Rose BREselein Donkey DFrau Holle Mother Hulda FHHeilige Joseph im Walde St. Joseph in Forest SJHund und Sperling Dog and Sparrow DSRa?tsel Riddle RTable 4: Stories Used (the titles are shortened)did not limit the task to the words occurring in alltexts in order to be able to investigate the stabilityof participants?
decisions.
Every annotator workedwith five word lists, one for each fairy tale text.
Thetotal number of unique items for the first annotationset was 893 words and 823 words long for thesecond set; 267 and 236 words correspondinglyoccurred in more than one word list.
These wordscould potentially be marked with different polaritycategories, but in fact only about 15% of thosewords (4% from the total number of items on eachof the word lists) were ?unstable?, namely, labeledwith different polarities by the same annotator.
Thelabels received in these cases were either {positive,neutral} or {negative, neutral}.
These words werefurther ?stabilized?
by either choosing the mostfrequent label or the neutral label if the unstableword had received only two label instances.
Theresults show that such annotation tasks could beused further for subjective clues lexicon collection.4.4 Text AnnotationFor the third and main part of the experiment, weselected eight Grimm?s fairy tales, each 1200 ?
1400words long and written in Standard German (seeTable 4).
The texts were chosen based on theirgenre, for in spite of the depth of all the hiddenand open references to human psyche and nationaltraditions that were shown in works of (von Franz,1996; Propp and Dundes, 1977), folk fairy talesare relatively uncomplicated in the plot-line andthe characters?
personalities.
Due to this relativesimplicity of the content, we expect the participants?emotional reactions to folk fairy tale texts to be morecoherent than to other texts of fiction literature.The task for the participants was to locate andmark stretches of text where an emotion was to be101conveyed through the speech melody and/or facialexpressions if the participant was to read the textout loud.
To make the annotation process and itsfurther analysis time-efficient and convenient forboth, annotators and experimenters, a simple toolwas developed.
We created the Manual EmotionAnnotation Tool (MEAT) which allows the userto annotate text for emotion by selecting stretchesof text and labeling it with one of fifteen emotioncategories.
The application also has a special modefor word list annotation, where only the threepolarity categories are available: positive, negativeand neutral.
The user can always undo their labelsor change them until they are satisfied with theannotation and can submit the results.
The mainpart of the experiment resulted in fifty individualannotations which produced 150 annotation pairs.5 Analyzing Inter-annotator AgreementFor each of the 150 pairs (two texts annotatedby ten annotators, six texts annotated by fiveannotators), the IAA rate was calculated.
However,the calculation of IAA is not as straightforwardin this situation as it might seem.
In many typesof corpus annotation, e.g., in POS tagging, thereare previously identified discrete elements.
In thisexperiment we intentionally have no predefinedunits, even if this makes the IAA calculation moredifficult.
Consider the following examples:(1) A1: ?.
.
.
[the evil wolf]X ate the girl?A2: ?.
.
.
the [evil wolf ate the girl]X?
(2) A1: ?.
.
.
[the evil wolf]X ate the girl?A2: ?.
.
.
[the evil wolf]Y ate the girl?
(3) A1: ?.
.
.
[the evil wolf]X ate the girl?A2: ?.
.
.
the evil wolf ate [the girl]X?
(4) A1: ?.
.
.
[the evil wolf]X ate [the girl]Z?A2: ?.
.
.
[the evil wolf ate the girl]X?In example (1) both annotators marked certainstretches of text with the same category X, but theannotations do not completely coincide, there isonly an overlap.
This situation is similar to that insyntactic annotation, where one needs to distinguishbetween bracketing and labeling of the constituentand measures such as Parseval (Carroll et al, 2002)have been much debated.Both annotators in example (1) recognize evilwolf as marked for X and thus this example shouldbe counted towards agreement, while examples (2)and (3) should not.
A second type of evaluationarises if the emotion clusters are taken into account.According to this evaluation type, example (2) iscounted towards agreement if the categories X andY belong to the same cluster.Example (4) provides an illustration of how IAAis accounted for in a more complex case.
AnnotatorA1 has marked two stretches of text with twodifferent emotion categories, while annotator A2has united both stretches under the same emotioncategory.
Both annotators agree that the evil wolf ismarked for X, but disagree on the emotion categoryfor the girl.
In order to avoid the crossing bracketsproblem (Carroll et al, 2002), we treat the evilwolf ate as agreement, and the girl as disagree-ment.
Although ate was left unmarked by one ofthe annotators, it is counted towards agreementbecause it is next to a stretch of text on which bothannotators agree.
Stretches of text the annotatorsagree or disagree upon also receive weight values:the higher the number of words that belong to openword classes in a stretch, the higher its weight.The general calculation formulae for the IAAmeasure are taken from (Artstein and Poesio, 2008):?
=Ao ?Ae1?AeAo =1i?i?IargiAe =1I2?k?Knc1knc2kAo is the observed agreement, Ae is the expectedagreement, I is the number of annotation items, Kis the set of all categories used by both annotators,nck is the number of items assigned by annotator cto category k.6 Analyzing Annotation StrategiesAnalysis of IAA, presented in Section 5 can answerthe first question we aim to investigate: How consis-tently do people perceive and locate the emotionalaspect of fairy tale texts?
The second issue nec-essary for investigation is the annotation strategiespeople use to express their emotional perceptionof text.
In our experiment conditions, the resultingstrategies can be investigated via three aspects:a) length of user-defined flexible units b) emotional1020%?1%?2%?3%?4%?5%?6%?7%?8%?9%?10%?1?
3?
5?
7?
9?
11?
13?
15?
17?
19?
21?
23?
25?
27?
29?Unit?length?Frequency?(%)?Unit?Length?
(in?word?tokens)?Figure 1: Annotator Defined Unit Length Ratingcomposition of fairy tales c) emotional flow of thefairy tales.
In this section we give a brief account ofour findings concerning the given aspects.The participants were always free to select textstretches of the length they considered to be appro-priate for a specific emotional category label.
Theonly guideline they received was to mark the entirestretch of text which, according to their judgement,was marked by the chosen emotion category and,if read without the surrounding context, wouldstill allow one to clearly perceive the appliedemotion category label.
As Figure 1 shows, themost frequent unit length consists of four to sevenword tokens, which corresponds to short phrases,e.g., a verb phrase with a noun phrase argument.We consider the findings to be encouraging, sincethis observation could be used favorably for theautomatic EA system training.Emotional composition of a fairy tale helps to re-veal the overall character of the text and establishif the story is abundant with various emotions or isoverloaded with only a few.
For our overall researchgoal, we would prefer the former kind of stories,since they would build a rich training corpus.
Fig-ures 2 and 3 give an overview on the average sharesvarious emotion categories hold over the eight texts.It is important to note that 65%?
75% of the text wasleft neutral.
The results show that most stories arerich in positive rather than negative emotions, withtwo exceptions we would like to elaborate upon.
Thestories The Poor Boy in the Grave and The Dogand the Sparrow belonged to different annotationsets and thus no annotator dealt with both stories.These texts were selected partially for their potential0%?5%?10%?15%?20%?25%?JG?
DS?
BR?
BT?
SJ?
D?
FH?
R?approval?compassion?hope?interest?joy?relief?surprise?Figure 2: Distribution of Positive Emotion Categories in Texts0%?5%?10%?15%?20%?25%?JG?
DS?
BR?
BT?
SJ?
D?
FH?
R?anger?despair?disgust?disturbance?fear?hatered?sadness?Figure 3: Distribution of Negative Emotion Categories in Textsovercharge with negative emotions.
The hypothesisproved to be true, since the annotators have labeledon average 20% of text with negative emotions, likehatred and sadness.
The only positive emotion cate-gory salient for the The Poor Boy in the Grave storyis compassion, which is also mostly triggered by sadevents happening to a positive character.The emotional flow in the fairy tales is illustratedby the graph presented in Figure 4.
In order to buildit, we used the numerical evaluations obtained inthe first part of the experiment and described insection 4.1.
For each fairy tale text, each word tokenwas mapped to the absolute value of the averagenumerical evaluation of its emotional categoriesassigned by all participants.
The word tokens alsoreceived its relative position in the text, where thefirst word was at position 0.0 and the last at 1.0.Thus, the emotional trajectories of all texts werecorrelated despite the fact that their actual lengthsdiffered.
The polynomial fit graph, taken over thusacquired emotional flow common for all fairy taletexts has a wave-shaped form and is similar to the1030 .
0 0 .
2 0 .
4 0 .
6 0 .
8 1 .
00 .
00 .
20 .
40 .
60 .
81 .
01 .
21 .
41 .
6S t o r y  p r o g r e s s  [ r u ]Emotional response [ru]Figure 4: Emotional Trajectory over all Storiesemotional trajectory reported by Alm and Sproat(2005).
The emotional charge increases and fallssteeply in the beginning of the fairy tale, then cyclesthough rise and fall phases (which do not exceedin their intensity the average rate of 0.6) and thenascents steeply at the end of the story.
We agree withthe explanation of such a trajectory, given by Proppand Dundes (1977) and also elaborated by Alm andSproat (2005) ?
the first emotional intensity peakin the story line corresponds to the rising action,after the main characters have been introduced andthe plot develops through a usually unexpectedevent.
At the end of the story the intensity is high-est, regardless whether the denouement is a happyending or a tragedy.
The fact that the fairy tale textswe chose for the experiment are relatively short isprobably responsible for the steep peak of intensityin the very beginning of the story ?
the stories aretoo short to include a proper exposition.
However,we need to investigate further how much of this is aproperty of texts themselves and how much ?
theperception (and thus annotation) of emotions.7 ResultsThe IAA scores were calculated using the emotionclusters information, for according to the results,participants would often stably use different emo-tions from same clusters at the same stretch of text.Four out of ten participants, two from eachgroup (marked gray in Table 1), had very low IAAscores (?
< 0.40 average per participant), a highproportion of unmarked text, and they used fewemotion categories ( < 7 categories average perparticipant), so for the evaluation part their data wasdiscarded.
The final IAA evaluation was calculatedon all the annotation pairs obtained from the sixremaining participants (marked black in table 1),whose average agreement score in the original setof participants was originally higher than 0.50.
Thetotal number of annotation pairs amounted to 48:two texts annotated by all the six annotators, sixtexts annotated by three annotators for each of thetwo annotation sets.According to the interpretation of ?
by (Landisand Koch, 1977), the annotator agreement was mod-erate on average (0.53), and some pairs approachedthe almost perfect IAA rate (0.83).
The IAA rates,calculated on the full set of fifteen emotions, with-out taking the emotion clusters into consideration,gave a moderate IAA rate on average (0.34) andreached substantial level (0.62) at maximum.
The?
rates are considerably high for the hard task andare comparable with the results presented in (Almand Sproat, 2005).
The word lists have a somewhatlower ?
IAA (0.45 on average, 0.72 at maximum),which is due to the low number of categories andthe heavy bias towards the neutral category.
Theobserved agreement on word lists is considerablyhigh: 0.81 on average, reaching 0.91 at maximum.While our approach may seem very similar tothe one of Alm (2005), there are some importantdifferences.
We gave the participants the freedom ofusing flexible annotation units, which allowed theannotators to define the source of emotion more pre-cisely and mark several emotions in one sentence.
Infact, in 39% of all annotated sentences represented amixture of the neutral category and ?polarized?
cat-egories, 20% of which included more than one ?po-larized?
categories.
Another difference is the rich setof emotion categories, with equal number of positiveand negative items.
The results show that people cansuccessfully use the large set to express their emo-tional perception of text (e.g., see Figures 3 and 2).Other important findings include the fact thatshort phrases are the naturally preferred annotationunit among our participants and that the emotionaltrajectory of a general story line corresponds to theone proposed by Propp and Dundes (1977).1048 Future Work8.1 Corpus ExpansionIn the near future, we will expand the collectionsof annotated text in order to compile a substantiallylarge training corpus.
We plan to work furtherwith three annotators that have formed a naturalteam, since their group has always attained thehighest annotation scores for their annotation set,exceeding the highest scores in the other annotationset.
The task defined for the three annotators issimilar to the experiment described in the paper,with several differences.
For the corpus expansionwe chose 85 stories by the Grimm Brothers 1400?
4500 tokens long.
We expect that longer textshave more potential space for an emotionally richplot.
Each text will be annotated by two people,the third annotator will tie-break disagreements bychoosing the most appropriate of the conflictingcategories, similar to the method described by (Almand Sproat, 2005).
It is also probable that a basicannotation unit will be defined and imposed on theannotators, for, as the studies discussed in Section 6show, short phrases are a language unit most oftennaturally chosen by annotators.Each of the annotators will also work with a sin-gle word list, compiled from all texts and filtered forthe most common stop-words.
Each of the words onthe word list should be annotated with its inherentpolarity (positive, negative or neutral).
Since eachword on the list is free of its context, the listsprovide valuable information about the word and itscontext interaction in full texts, which can be furtherused for machine learning architecture training.We also plan to keep the fifteen emotion cat-egories and their clustering, since it gives theannotator more freedom of expression and simulta-neously allows the researches to find the commoncognitive ground behind the labels if they varywithin one cluster8.2 Feature Extraction and Machine LearningArchitecture TrainingWhen the corpus is large enough, the relevantfeatures will be extracted automatically by meansof existing NLP tools, followed by training a ma-chine learning architecture, most probably TiMBL(Daelemans et al, 2004), to map textual units tothe emotion categories.
It is yet to be determinedwhich features to use, one compulsory parameteris that all the features should be available throughautomatic processing tools.
This is crucial, sincethe resulting EA system has to be fully automatedwith no manual work involved.8.3 Extra Information Sources and theirPotential ContributionWe also plan to collect data from other informationsources, like video and audio recordings, by invitingamateur actors for story-telling sessions.
This willallow emotion retrieval from the speech melody,facial expressions and body language.
The manualannotation and the extra data sources can be alignedby means of Text and Speech Aligner (Rapp, 1995),which allows to track correspondences betweenthem.
This alignment would most certainly ben-efit the facial and body animation of the virtualcharacters, since there is no clear understandingof time correlation between emotions labeled inwritten text and the ones expressed through speechand facial clues in a story telling scenario.
An EAsystem could also be perfected through a carefulanalysis of recorded speech and video of storytelling sessions ?
regular recurrence of subjectivityof certain contexts will be even more significantif the transmission of the emotions from the storyteller to the listener via mentioned informationsources is successful.9 ConclusionsIn this paper, we reported on an experiment inves-tigating the inter-annotator agreement levels whichcan be achieved by untrained human annotators per-forming emotion analysis of variable units of text.While EA is a very difficult task, our experimentshows that even untrained annotators can have highagreement rates, even given considerable freedomin expressing their emotional perception of text.
Tothe best of our knowledge, this is the first attempt atemotion analysis that operates on flexible, annotatordefined units and uses a relatively rich inventory ofemotion categories.
We consider the resulting IAArates to be high enough to accept the annotationsas suitable for gold-standard corpus compilation inthe frame of this research.
As such, we view thiswork as the first step towards the development of amore complex EA system, which aims to simulatethe actual human emotional perception of text.105ReferencesC.O.
Alm and R. Sproat.
2005.
Emotional sequencingand development in fairy tales.
In Proceedings of theFirst International Conference on Affective Computingand Intelligent Interaction (ACII05).
Springer.C.O.
Alm, D. Roth, and R. Sproat.
2005.
Emotions fromtext: Machine learning for text-based emotion predic-tion.
In Proceedings of HLT/EMNLP, volume 2005.C.O.
Alm.
2008.
Affect in Text and Speech.lrc.cornell.edu.R.
Artstein and M. Poesio.
2008.
Inter-coder agreementfor computational linguistics.
Computational Linguis-tics, 34(4):555?596.Jan Auracher.
2007.
... wie auf den allma?chtigen Schlageiner magischen Rute.
Psychophysiologische Messun-gen zur Textwirkung.
Ars poetica ; 3.
Dt.
Wiss.-Verl.C.F.
Baker, C.J.
Fillmore, and J.B. Lowe.
1998.The berkeley framenet project.
In Proceedings ofthe 17th international conference on Computationallinguistics-Volume 1, pages 86?90.
Association forComputational Linguistics Morristown, NJ, USA.Steven Bethard, Hong Yu, Ashley Thornton, VasileiosHatzivassiloglou, and Dan Jurafsky.
2004.
Automaticextraction of opinion propositions and their holders.
In2004 AAAI Spring Symposium on Exploring Attitudeand Affect in Text, page 2224.J.
Carroll, A. Frank, D. Lin, D. Prescher, and H. Uszkor-eit.
2002.
Beyond Parseval-Towards improved evalua-tion measures for parsing systems.
In Workshop at the3rd International Conference on Language Resourcesand Evaluation LREC-02., Las Palmas.D.
W. Cunningham and C. Wallraven.
2009.
Dynamicinformation for the recognition of conversational ex-pressions.
Journal of Vision, 9(13:7):1?17, 12.Walter Daelemans, Jakub Zavrel, Ko van der Sloot, andAntal van den Bosch.
2004.
Timbl: Tilburg memorybased learner, version 5.1, reference guide.
ilk techni-cal report 04-02.
Technical report.P.
Ekman.
1993.
Facial Expression and Emotion.
Amer-ican Psychologist, 48(4):384?392.JR Fontaine, KR Scherer, EB Roesch, and PC Ellsworth.2007.
The world of emotions is not two-dimensional.Psychological science: a journal of the American Psy-chological Society/APS, 18(12):1050.P.
Kingsbury, M. Palmer, and M. Marcus.
2002.
Addingsemantic annotation to the Penn Treebank.
In Pro-ceedings of the Human Language Technology Confer-ence, pages 252?256.
Citeseer.J.R.
Landis and G.G.
Koch.
1977.
The measurement ofobserver agreement for categorical data.
Biometrics,33(1):159?174.D.
Lee, O.R.
Jeong, and S. Lee.
2008.
Opinion min-ing of customer feedback data on the web.
In Pro-ceedings of the 2nd international conference on Ubiq-uitous information management and communication,page 230235, New York, New York, USA.
ACM.Hugo Liu, Henry Lieberman, and Ted Selker.
2003.A model of textual affect sensing using real-worldknowledge.
In Proceedings of the 8th internationalconference on Intelligent user interfaces - IUI ?03,page 125, New York, New York, USA.
ACM Press.G.A.
Miller, R. Beckwith, C. Fellbaum, D. Gross, andK.J.
Miller.
1990.
Introduction to Wordnet: An on-line lexical database*.
International Journal of lexi-cography, 3(4):235.L.
Polanyi and A. Zaenen.
2006.
Contextual valenceshifters.
Computing Attitude and Affect in Text: The-ory and Applications, page 110.V.I.A.
Propp and A. Dundes.
1977.
Morphology of theFolktale.
University of Texas Press.S.
Rapp.
1995.
Automatic phonemic transcription andlinguistic annotation from known text with HiddenMarkov Models.
In Proceedings of ELSNET GoesEast and IMACS Workshop.
Citeseer.M.L.
von Franz.
1996.
The interpretation of fairy tales.Shambhala Publications.C.
Whitelaw, N. Garg, and S. Argamon.
2005.
Using ap-praisal groups for sentiment analysis.
In Proceedingsof the 14th ACM international conference on Informa-tion and knowledge management, page 631.
ACM.J.
Wiebe, T. Wilson, R. Bruce, M. Bell, and M. Martin.2004.
Learning subjective language.
Computationallinguistics, 30(3):277?308.T.
Wilson, J. Wiebe, and R. Hwa.
2004.
Just how madare you?
Finding strong and weak opinion clauses.
InProceedings of the National Conference on ArtificialIntelligence, pages 761?769.
Menlo Park, CA; Cam-bridge, MA; London; AAAI Press; MIT Press; 1999.T.
Wilson, J. Wiebe, and P. Hoffmann.
2009.
Recogniz-ing Contextual Polarity: an exploration of features forphrase-level sentiment analysis.
Computational Lin-guistics, 35(3):399433, September.106
