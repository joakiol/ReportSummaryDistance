Hopfield Models as Nondeterministic Finite-State MachinesMarc F.J. DrossacrsComputer Science Department,University of Twente,P.O Box 217, 7500 AE Enschede,The Netherlands,email: mdrssrs@cs.utwente.nl.AbstractTbe use of neural networks for integrated linguisticanalysis may be profitable.
This paper presents thefirst results of our research on that subject: a Hop-field model for syntactical analysis.
We construct aneural network as an implementation f a boundedpush-down automaton, which can accept context-freelanguages with limited center-embedding.
The net-work's behavior can be predicted a priori, so the pre-sented theory can be tested.
The operation of thenetwork as an implementation f the acceptor is prov-ably correct.
Furthermore we found a solution to theproblem of spurious states in Hopfield models: weuse them as dynamically constructed representationsof sets of states of the implemented acceptor.
Theso-called neural-network aceeptor we propose, is fastbut large.1 In t roduct ionNeural networks may be well suited for integrated lin-guistic analysis, as Waltz and Pollack \[10\] indicate.An integrated linguistic analysis is a parallel compo-sition of several analyses, such as syntactical, seman-tical, and pragmatic analysis.
When integrated, theseanalyses constrain each other interactively, and maythus suppress a combinatoric explosion of sentencestructure and meaning representations.This paper presents the first results of our researchinto the use of neural networks for integrated linguis-tic analysis: a Hopfield model for syntactical nalysis.Syntactical analysis in the context of integration withother analyses boils down to the decision whether asentence is an element of a language.
A parse tree issuperfluous here as an intermediary representation,since it will not be finished before the complete in-tegrated analysis is.
This fact allows us to deal withthe problem of a restricted length of the sentences aneural par .
.
.
.
n handle, see e.g.
\[51,\[7\], a problemthat could not be elegantly solved, see e.g.
\[6\],\[3\].In this paper we propose a formal model that rec-ognizes syntactically correct sentences (section 2), atlopfield model onto which we want to map this for-mar model (section 3), the parameters that makes thenetwork operate as intended (section 4), and a wayto map the formal model onto the Hopfield model,including a correctness result for the latter (section5).
The theoretically predicted behavior of the so~obtained network has been verified, and a simple ex-ample provides the taste of it (section 6).
We alasconsider complexity aspects of the model (section 7).Section 8 consists of concluding remarks.2 A Bounded Push-Down Au-tomatonAlthough it is not an t.~tablished fact, it is as-stoned here that natural languages are context-free,and consequently that sentences in a natural lan-guage can be recognized, by a push-down atttoma-ton (PDA).
ilowever, we are not interested in mod-eling the competence of natural anguage users, butin modeling their performance.
The human perfor-mance in natural language use is also characterizedby a very limited degree of center-embedding.
Interms of PDAs this means that there is n bound onthe,number of items on the stack of a PI)A for anatural anguage.
A bounded push-down automatonM = (Q,Y~,I',6, qs, Zo, F) is a PDA that has an up-per limit k E ~ on the number of items on its stack,i.e.
H ~< k for every instantaneous description (ID)(q, w, a) of M. The set of stack states of this PDA isdelined to be: QST =- {a I (qo,w, Zs) P*~t (q,e,?~)}.QsT is finite: IQsT\[ <_ (IFI) ~, therefore we may de-fine a nondeterministic finite~state aeceptor (NDA)M ~ that has QST ms its set of states.The class of PDAs of which we would like to mapbounded versions onto NDAs is constrained, amongothers to the class of v-free PDAs.
By this constraintwe anticipate the situation that grammars are storedin a neural network by self-organization.
In that sit-nation a neural network will store e-productions onlyif examples of applications of e-productions are re-peatedly presented to it.
This requires e to have arepresentation i  the machine, in which case it failsto accommodate its definition.Another restriction we would like to introduce is togrammars in 2-standard form with a minimal num-ber of quadratic productions: productions of the formACrEs DE COL~G-92, NANTES, 23-28 AOOT 1992 I I 3 PROC.
OF COLING-92, NA~rEs, AIJG.
23-28, 1992A ~ bCD where b is a terminal and C and D arevariables.
Such a grammar can be seen as a min-imal extension of a right-linear grammar.
Withinsuch grammars,  quadratic productions provide for thecenter-embedding.
Since such grammars have a min-imal number of quadratic productions, acceptance bya PDA defined for such grammars requires a minimaluse of (stack) memory, and titus generates a mini-real Qs'r.
To maintain this minimal use of memory arestriction to one-state PDAs that accept by emptystack is also required: when a PDA is mapped ontoan NDA, the information concerning its states is h)st,unless it was stored on the stack.An e-free, one-state PDA that simulates a context-free grammar in 2-standard form with a minimalnumber of quadratic productions (and that acceptsby empty stack) satisfies all our criteria.
For everysuch PDA we can define an NDA, for which we canprove \[4\] that it accepts the same language ms thePDA does.Def in i t ion  2.1Let M = ({*},Z,I',6,*,Zo,{~) be an e-free boundedPDA with hound k. An NDA defined for M is anNDA M'  = (Q', Y2', 6', Q'o, f " )  such that:Q' = QST, as defined above;E' = E;6' : QsT x E ~ 2 osT is defined by:~'(~l, ~)= {~ I (*,~,~)~M (,, w,~)}Q~ : {z0}; andF'  = {el (empty stack).Theorem 2.2 (correctness of the NDA)Let M be an e.free one-state PDA with bound k, ifM'  is an NDA a.~ defined in definition 2.1, then Maccepts a string by empty stack tf and only if M'  ac-cepts it by accepting state.In as far as a natural language is context-free, weclaim that there is an instance of our aeeeptor thatrecognizes it.3 An  Input -Dr iven  Sequenc ingHopf ie ld  Mode lIn this section a noiseless Hopfield model is proposedthat is tailored to implement NDAs on.
The model isbased on the associative memory described by Buh-mann et al \[2\] and the theory of delayed synap~sfrom \[8\].
We chose the Itopfield model because ofits analytical transparency, and its capability of se-quence traversing, which agrees well with the sequen-tial nature of language use at a pbenomenologica\[level.
The Hopfield model proposed is a memoryfor temporal transitions extended with externa|-inputsynapses.
Figure 1 shows the architecture involved.Ill this network only those neurons are active uponwhich a combined local field operates that transcendsthe threshold.
The activity generated by such a lo~cal field is the active overlap of the temporal image ofpast activity provided by so-called temporal synapses,and the image of input external activity provided byso-called input synapses.
By the temporal synapses,this activity will later generate another (subthresh-old) temporal image, so network activity may be con-sidered a transition mechanism that brings the net-work from one temporal image to another.
Activeoverlaps are unique with high probability if the ac-tivity patterns are chosen at random and representlow nrean network activity.
This uniqueness makestile selectivity of the network very plausible: if anexternal activity pattern is presented that does notmatch the current temporal image, then there willnot he activity of any significance; tile input is notrecognized.When an NDA is mapped onto this network,pairs of NDA-state q and input-symbol x, such that6(q, x) y?
{~, are mapped onto activity patterns.
Tem-poral relations in the network then serve to imple-ment NDA transitions.
Note that single NDA tran-sitions arc mapped onto single network transitions.This results in complex representations of the NDAstates and the input symbols.
An NDA state is rapresented by all activity patterns that represent a paircontaining that state, and input patterns are rep-resented by a component-wise OR over all activitypatterns containing that input symbol.
A conse-quence is that mixed temporal images, the subthresh-old analogue of mixture states, are a very natural phe-nomenon in this network, because tile temporal imageof an active overlap comprises at least all activity pat-terns representing a successor state.
But this is notall.
Also the network will act as if it implements thedeterministic equivalent of the NDA, i.e.
it will traceall paths through state space the input allows for,concurrently.
The representations of the states of thisdeterministic finite-state automaton (FSA) are dy-namically constructed along the way; they are mixedtemporal images.
The concept of a "dynamically con-structed representation" is borrowed from Touretzky\[9\], who, by the way, argued that they could not existin the current generation of neural networks, such msltopfield models.A time cycle of the network can be described asfollows:1~ The network is allowed to evolve into a stableactivity pattern that is the active overlap of atemporal image of past activity, and the inputimage of external input for a pe~'iod tr (= relax-ation time), when an external activity pattern ispresented to the network;2.
After some time the network reaches a state ofstable activity and starts to construct a new tem-poral image.
It is allowed to do this for a periodt ,  (= active time)',3.
Then the input is removed, and the networkevolves towards inactivity.
This takes againabout a period t~;Acids DE COLING-92, NANTES, 23-28 AOt~T 1992 1 1 4 PRO(:.
OF COL\]NG-92, NANTI~S, AUG. 23-28.
19924.
Not before a period ta (= delay time) has passed,a new input arrives.
The new temporal image isforwarded by tile slow synapses during a periodta +iv, starting when td ends.
The slow synapseshave forgotten the old temporal image while thenetwork was in its td.The synapses modeled in the network collect theincoming activity over a period ~ + tr, and emit thetime average over again a period ta + tr after hav-ing waited a period ta + t~.
In the network thisis modeled by computing a time average over priorneuronal activity, and then multiply it by the synop-tic efficacy.
Tile time average ranges over a period(2t~ + la + 31~)/N - (l, + I~)/N.
The first argu-ment is the total time span in the network, coveringtwo active periods and an intervening delay time, in-cluding their transition times.
The second argumentis tile current period of network is activity, activitythat cannot directly interfere with the network's dy-na\[ l l iCS.More formally the network can be described as fol-lows:5'i (5 {0,1}, where i= 1 .
.
.
.
,N,l 1 i fh~( t )>U. '
) ' i (t+l) : 0 if h i ( t )< U,h~(t) : h~(t)+hl(t) ,Nj= lthe temporal transition term,t Pu al l  - a)N jz~=l"" - a)(~?~' - a),with J .
= 0,~(t )  ~ r O- 0 L ~ S j ( t -  t')w(t')dt', where{ 7~- i fO<t<rw(t) = 0 otherwise ,h~(t) = x~(s,'(t) - a),the external input term,The Si are neuronal variables (5",'.
is a neuron in an-other network), hi is the total input on Si, U isa threshold value which is equal for all Si, Jij isthe synaptic efficacy of the synapse connecting S i toSi, and A is the relative magnitude of the synapses.The average at time ~ is expressed by ~/(~), wherer =- (2t.
+ ta + 3tr)/N and ~ -~ (ta + t~)/N.
Thefunction w(t) determines over which period activityis averaged.
The input synapses are nonzero only incase i = j.
These synapses carry a negative groundsignal -A 'a ,  which is equivalent to an extra thresholdgenerated by the input synapses.
The activity pat-terns {~'} ({~"} ~ (~\ ] ,~  .
.
.
.
.
~N) ) are statist,tallyindependent, and satisfy the same probability distri-bution as the patterns in the model of Buhmann etal.
\[2\]:.
.
.
.
.
.
- (1 -  a )b(~) ,  wherel if x :~ 0~(x) ~ 0 otherwise.If a ?
?
the pattern is biased.
For N -~ co,I /N  ~N=I ~' --* a.
The updating p .
.
.
.
is a MonteCarlo random walk.D II~ IFigure 1: 7'he model for N = 3.
Usually HopJieldmodels consist of very many neurons.
The arced ar-rows denote temporal synapses.
The straiyht alT'owsdenote input synapses.4 Est imat ion  o f  ParametersA number of system parameters need to be related inorder to make the model work correctly.Timing; is fairly important ill this network.
Tiletime the network is active (to) should not exceed tiledelay time t~.
If it does then ta+lr > ta+tr, and sinceno average is computed over a period ta + tr back intime, not the fldl time average of the previous activityneed to be computed, consequently we choose ta < ta.The choice for a transition time tr depends on tileprobability with which one requires the network toget in the next stable activity state.
This subject willbe dealt with in section 7.In the estimates of A t and the storage capacity be-low, an expression of the temporal transition termin terms of the overlap parameter m o is used, whichwill be introduced here first.
The overlap parame-ter rnP(t) measures the overlap of a network state{S} ~_ ($1,$2,.. .
,SN) r at time t with stored pat-tern {~P}, and is delined by:N 1 ~(~'  - .
)s ' , ( t ) .
mo(t) = ~-N =Im" E i-a, I - a\].
The expression for the temporaltransition term is:Nh~(t) = ~slj~j(1).j= lAcaqis OE COLING-92, NANTES, 23-28 ao~r 1992 1 1 5 PROC.
Of COLING-92, N^NTgs, AUG. 23-28, 1992Assuming that N --* co while p remains fixed this is,after expansion of the Jq and ignoring infinitesimalterms, approximated by:At p Nh:(t) - a (1 -a )N  Z(~+l -a )  Z (~f -a ) ; j ( t )t~=l j=lt I?A' ~,-.tf?+ , = (1 - a) ,=z'-~l'" - a)t~l"(t), wherer - O ~o??
m"(t)  ~ ~ m"(t -  t ')w(t')dt' .If the temporal image is {~?~} then h~ is about (Nco):h~(t) = ~, (~+l  _a ) ( r _  O) w(t)dt.0If a number of patterns in a mixture state havethe same successor, that pattern may be activated.To prevent his A ~ will be chosen such that the slowsynapses do not induce activity in the network au-tonomously, not even if all the neurons in the networkare active.
On average, the activity in the networkis given by the parameter a.
The total activity in anetwork is a quantity x such that z = 1/a, so whatwe require is that xh~ < U, i.e.
that:a~(~+~ - . )
( r  - 0) w( t )d t  < U.aOThe interesting case is (i "+1 = 1.
Since the integralis at most O/(r - O) which is the strongest conditionon the left side, the left expression can be writtenas )d(1 - a)/a.
It was earlier demanded that onlya combined local field can transcend the threshold,which implies that external input .~e(1 - a) < U, sowe can take A ~ < A~a safely.
This is small because ais small.Next a value for the threshold that optimizes tor-age capacity is estimated by signal-to-noise ratio anal-ysis, following \[1\] and \[2\], for N,p  --* oo.
Temporaleffects are neglected because they effect signal andnoise equally.
It is also assumed that external inputis present, so that the critical noise effects can bestudied.
In this model the external input synapsesdo not add noise, they do not contain any informa-tion apart  from the magnitude of the incoming signal.Now suppose the system is in state {S} = {~}.
Thesignal is that part of the input that excites the currentpattern:s = A'(~, ~-a) .The noise is the part of the input that excites otherpatterns.
It can be seen as a random variable withzero mean and it is estimated by its variance: ~t v'~-awhere a = p/N .
We want that given the right inputh i > U, if both the temporal and the external inputexcite Si, and that hi < U if the temporal input doesnot excite Si.
This gives signal-to-noise ratios:Pt = ( ,V+At ) (1 -a ) -U ,  andU + ,Va - ,~'(1 - a)PO =,v vCg~Recall is optimal in case Po = Pl which is true for athreshold:Uor,, = At (1 -a )+,V(?-a) .Substituted in either P0 or Pt it results in Pore =l This result is the same as obtained by Buh-mann et al \[2\], and they found a storage capacityct c .~ - (a inu)  -1 where ac = prna,:/N.
The stor-age capacity is large for small a, so a will be chosena << 0.5.
A last remark concerns an initial input forthe network.
In case there has been no input for thenetwork for some time, it does not contain a temporalimage anymore, and consequently has to be restarted.This can be done by preceding a sequence by an extrastrong first input, a kind of warning signal of magni-tude e.g.
A" + At.
This input creates a memory ofprevious activity and serves as a starting point forthe temporal sequences stored in the network.5 Neura l -Network  AcceptorsIn this section it is shown how NDAs from definition2.1 can be mapped onto networks as described in sec-tions 3 and 4.
Such networks can only be used forcyclic recognition runs.
Where "cyclic" indicates thatboth the initial and the accepting state of an NDAare mapped onto the same network state.
If this werenot done, the accepting state is not assigned an ac-tivity vector, since no transition departs from it, seedefinition 5.2 below.
Cyclic recognition in its turncan only be correctly done for grammars that gen-erate end-of-sentence markers.
Any grammar can beextended to do this.An NDA is related to a network by a parameterlist.Def in i t ion  5.1Let M = (Q, ,6, Qo, F )  be an NDA.
A parame-ter list defined .for an NDA M is a list of the form(a, A ~, A t , N, p, prnaz, ta, td, tr, U), where:1. a 6 \[0,1\] c ~t;2. t~ < td, with ta,td 6 ~I;3.
A ~ 6 ~+ where ~+ = {x }x E Ax  > 0};4.
0< A t < ,Va;5. p = ~q.q'eQ I {x 6 ~Clq' e ~f(q,x)} I xI {Y 6 ~, 16(q',y) # 0} I;6.
Pmax >~ P;7.
N > ( -a lna)p  ..... ;Ac-t~ DE COLING-92, NANTES, 23-28 AO6"r 1992 1 1 6 PROC.
OF COLING-92, NANTES, AUG. 23-28.
19928.
G: see section 7;9.
U=M(1-a)+M(?-a) .Note that there arc an infinite number of parameterlists for each NDA.The mapping of an NDA onto a network starts bymapping basic entities of the NDA onto activity pat-terns.
Such a pattern is called a code.Def in i t ion  5.2Let M = (Q,E,ti, Qo, F) he an NDA, let t" :(a,A e,M, N,p, pma,,ta,ta,tr,U) be a parameter  listdefined according to definition 5.1.
The coding \[unc-tion c is given by:c : QxE~{0,1}  N,such that for q E Q, and x E :E:1. if 6(q, x) :/: 0:c(q,x) = {~},where ~i is chosen at random from {0, 1} withprobability distributionr ' (~)  = a6(~ - I )  + ( t  - a )~(~, ) ,  a .d2.
undefined otherwise.The set of codes is then partitioned: into sets of ac-tivity patterns corresponding to NDA states, and intosets of patterns corresponding to input symbols.Def in i t ion  5.3Let M = (Q,E,6, Qs, I") Ire an NDA, let P =(a, M, A t, N, p, PmaJ:, In, td, tr, U) be a parameter listdefined according to definition 5.1.The set Pq of activity patteT~s for q E Q is:G = {c(%z) l x e ~}.The set P:: of activity patterns for" x E E is:P, = {c(q,x) I q E Q}.Then a network transition is defined ms a matr ix op-erator specified by the network's torage prescription,and related to NDA transitions using the previouslydefined partit ion of the set of codes.Def in i t ion  5.4Let M = (Q,~,6, Qo, F) bc an NDA, let P =(a, M, A t , N, p, Pma*, ta, t d, G, U) be a parameter listdefned according to definition 5.1. and let a be anN-dimensional vector, with each component a. Theset Tr of network transitions is:At tTr  = {d(?,q,x) \[ d(q',q,x) = a(1 - a)NE (c(q',y) - a)(c(q,x) -- a) T Ac(q',y)eP u,qt E ~(q, x)},where each j t  is an N x N matrix.This suffices to define a neuralonetwork acceptor.Def in i t ion  5.5Let M = (Q,E,6, Qo, I") be an NDA, let P =(a, A*, A*, N, p, Pmax, to, td, tr, U) be a parameter  listdefined according to definition 5.1.
A neural-networkacceptor (NNA) defined for an NDA M that  takes itsparameters from P is a quadruple H = (T, f ,  U, S),where:1. tim topology T, a list of neurons per layer, is:(N) ;2. the activation fimction f is given by: Si =1 if E~J / j .~ j+A' (S~-a)>U(} if E j Je j~ J+A' ( 'd~-a)<-U '3.
the update procedure is a Monte Carlo randomwalk.4.
the synaptic coefficients are given by:'It =: E j l  (q,.q.~) C Tr, andje .
: A~I where I is the identity matrix.In order to construct activity patterns that can servexs external input x for the network a component-wiseOR operation is performed over the set P~ as definedin definition 5.3.Def in i t ion  5.67he OR operation over a set t?
of activity patternsis specified by:1. oa({~v},  {U}) = {OR(~/ ' ,~)}  ;~.
oit ({?
}, .
.
.
,  W'}) :~o f t .
(K '} ,  ( ) l t ({~q .
.
.
.
.
{~" })) ; ~uda OR(P~) = OR({~ ~} .
.
.
.
.
{C'}) if?
e~ = {W} .
.
.
.
.
{~"}},At last a formal definition can be given of a temporalimage a.s the set of all activity patterns for whichthere is a network input that makes such an activitypattern the network's next qua.st-stable state.Def in i t ion  5.7Let M = (Q,E ,6 ,  Qc, F) I)e an NDA, let P =(a, M, M, N, p, p , .
, , ,  t,,  ta, it,  U) be a parameter  listdefined according to definition 5.1, and let H =(T, f, U,S) be an NNA defined according to defini-tion 5.5 that takes its parameters from P. A temporalimage is a set:{c(q, x) \[  input OIt(P~) for H impliesm ?
(q'~) = 1 -- a},a set Pq, is a temporal image of a quasi-stable stale{S} = c(q, x) of H if and only if J~q,,q,,:) is a transitionof H.Now that we have a neural-uetwork acceptor, we triayalso Wahl to use it to judge the legality of stringsagainst a given grammar  with it.ACTE's DE COLlNG-92, NANTES, 23-28 AO~I' 1992 1 1 7 I'ROC.
OV COLING-92, NANTES, AUCl.
23-28, 1992Def in i t ion  5.8Let M = (Q,E,6, Qo, F) be an NDA, let P --(a, M, A t, N, p, Pm~x, is, Gt, G, U) be a parameter listdefined according to definition 5.1, let H =(T, f, U, S) be an NNA defined according to defini-tion 5.5 that takes its parameters from P, and letai E E, q E Q0, and q' E F. H is said to accept astring w = al " " " an if and only if H evolves througha series of temporal images that ends at Pq, if startedat Pq, if OR(P~) , .
.
.
,  OR(Pa,)  appears as externalinput for the network.Next the correctness of an NNA is to be proven.Since an NNA is essentially a stochastic machine, thisraises some extra problems.
What we propose is tolet various network parameters approach values forwhich the NNA has only exact properties, and provethat the uetwork is correct in the limit.
Those "vari-ous network parameters" are defined below.Def in i t ion  5.9Let M : (Q,E,6, Qo, F) he an NDA, let P =(a, M, M, N, p, P,na,,, ta, t,t~ t,., U) be a parameter listdefined according to definition 5.1.
A list of largeparameters i a parameter list P such that:1. a =_ c l /N  where cl << N is a constant;2.
~ ~ c~N/cl, where c2 is a small constant;3.
0 < M < A~a,i.e.
0< M < c2;4.
Pm,~" -- -{a lna) - l  N;5.
N ~oo;6. t~lN ~oo.The following lemma states that for neural-networkacceptors that take their parameters from a list oflarge parameters both the probability that the net-work reaches the next stable state within relaxationtime, and the probability that only the patterns thatare temporally related to the previous activity pat-tern will become active, tend to unity.
Essentiallyit means that such networks operate exactly as pre-scribed by the synapses.
Such networks are intrinsi-cally correct.Lemma 5.10 intrinsical correctnessLet M = (Q,~2,8, Qo, F) be an NDA, let P =(a, A', ~t, N, p, pm,::, ta, ta, tr, U) be a parameter listdefined according to definitions 5.1 and 5.9, and letH = (T, f ,  U, S) be an NNA defined according to def-inition 5.5 that takes its parameters from P, then His such that:1. for all neurons Si in H, P(SI is selected) ~ 1during network evolution; and2.
/or all actwity patterns {c} ~ Upu,  y ~ QuZ,if l~ 7 ?
", then P(~,~ = ~ = 1) ~ O, wherei= l , .
.
.
,N .Then the correctness of an NNA follows.Theorem 5.11 (correctness of the NNA)Let M = (Q,Z,6,  Q0, F) be an NDA, let P =(a, A', A t, N, p, p,,o~, t,,, ta, G, U) be a parameter listdefined according to definitions 5.1 and 5.9, let H =(71, f ,  U, S) be an NNA defined according to definition5.5 that takes its parameters from P, and let w E E +,then the probability that M accepts a string w if  andonly if H accepts w, tends to unity.The proof of the theorem is given in \[4\].6 Simulat ion Resul tsAs an'example we constructed au NNA that acceptsthe language generated by a grammar with produc-tions:S '  ~ theBE,B ~ naanSV \[ womanSV \[ babySV \[mauV J womanV I bahyV,S ~ theB,E~t ,V ~ saw I cried I comforted.It takes its parameters from the llst:(0.05,1.5,0.07,800,64,6.68N,5,5,5,1.46).
It was testedwith tile sentence ".
tile baby the woman comfortedcried ."
The preceding full stop is a first input thatawakens tile network.
The graph below shows thetime evolution of the network.c(BS,wmm)cS~VR.~by)?Cave)c(SVVE,~)c~VVE,~)c(V V V r~co~roma)c(VVVR~d)~(VVV~w),~v V l~xaD/~~vvE,~w)c(S'.~)0 i 2 ~ d ~ 6 7INPUTSPlot of the system dynamics.7 Complexity AspectsIf a neural-network acceptor h~.s to process a sequenceof n input patterns, it (worst ease) first has to con-struct its initial temporal image, when awakened byan initial input (that is not considered a part of thesequence), and then has to build n further tempo-ral images.
The time required to process a string ofAC'rE.S DE COLING-92, NaNTEs, 23-28 Aor~r 1992 1 I 8 PgOC.
OF COLING-92, NAWrES.
AUG. 23-28, 1992length u as a function of the length of the input se-quence is thus (T -- O)(n + 1).
The constant r a l~depends oti t,.
which is chosen to let the network sat-isfy a certain probability that it reaches the next statein relaxation.
This probability is given by (1 - .~/)owhere B = tr/N.
The time complexity of the neural-network acceptor is O(n).The upper limit on the number p of stored tem-poral relations between single activity patterns is\[ Q 1:~ x I ~3 \[2.
The number of neurons in a net-work is then cx { Q \]e ?
\] E 17, where e depends onthe storage capacity and the chosen (low) probabil-ity that selection errors occur.
The randomly chosenactivity patterns overlap, so if a large number of pat-terns is active they may constitute, by overlap, otherunselected activity patterns that will create tlreir owncausal consequences.
This is called a selection er=rot.
The probability that this can happen can beestinrated by l'~,.,,,~(n) -~.
1 - 1'(,'?, = 0), where thelatter is:( - l  - 2np ' 1.-2, , i )~In this expressi .
.
.
.
P ---- ( .
.
.
.
.
, ) /v  wh .
.
.
.
-_- (~  ), ,: isthe nmnber of activity patterns tored in the network,and m is the number of patterns that were supposedto be present in the mixture state.
The probabilityq = 1 - p, and ,1 :_~ (aN) is the number of patternsthat can bc constructed fi'om the aetiw~ neurons inthe mix.
S,, is the mnnber of wrongly selected activ-ity patterns for a given n. l),rror(n) decreases withincrea~ing N if the other parameters remain tixed.The space complexity of the network, exprc&sed asthe nnmber of neurons, and as a function of the num-ber of NI)A states is O(\[ Q 17).
This is large becauseQ = Qs'r <1 F I ~ for some PDA M. tlowever thingsconld have been worse.
Not using mixed temporalimages to represent FSA states would necessitate theuse of a mnnber of temporal images of order 2 IQ'I~,So compared to a more conventional use of lloptieldmodels, this approach yields a redaction of the spacecomplexity of the network.8 Conc lus ionsWe proposed an receptor for all context-free lan-guages with limited center-embedding, and a suitablevariant of the Ilopfield mode\].
The formal model wasimplemented on the lloptield model, and a correct=uess theorem for the latter was given.
Simulation re-stilts provided initial corroboration ofonr theory.
Theobtamed neural-network eceptor is fast but large.Continuation of this research in the near fntureconsists of the design of an adaptive variant of thismode\[, one that learns a grammar from examples inan unsupervised fashion.Acknowledgements1 am very grateful for the indispensable support ofMannes Poel and Anton Nijholt in making this paper.Special thanks go out to Bert llelthuis who wrotetile very beautiful simulation program with which thetheory was corroborated.References\[1} 1).J.
Amit.
Modcltag Brain FunctiOn.
CambridgeUniversity Press, (~ambridge, 1989.\[2\] J. i luhmann, R. Divko, and K. Schulten.
Asso-ciative memory with high information content.Physical Review A, 39(5):2689-2692, 1989.\[3\] E. Charniak and E. Santos.
A connection|stcontext-free parser which is not context-free, butthen it is not really connection|st either.
In Pro-eeedinys 91h Ann.
Couf.
o\]' the Cognitive ScienceSociety, pages 70 77, 1987.\[4\] M.F.J.
l)rossaers.
Neural-network aceeptors.
"l~chuical Report Memoranda lnformatica 92-36,University of Twente, Enschede, 1992.\[5\] M.A.
Fanty.
Context=free parsing in connection-|st networks.
Technical Report TR 174, Univer-sity of Rocbester, Rochester, NY, 1985.\[6\] 11.
Nakagawa nd T. Mort.
A parser based onconnection|st n odel.
In Proceedings oJ: COLING'88, pages 454--458, 1988.\[7\] B. Sehnan and G. llirst.
Parsing as an energyminimization problem, in E. Davis, editor, Ge-netic Algorithms and Simulated Annealing, Re-search Notes in Artzficial ntelligence, pages 141-154, Los Altos, Calif., 1987.
Morgan KaufinanPublishers.\[8\] II.
Sompoiinsky and 1.
Kanter.
Temporal a.s.so-elation in asymmetric neural networks.
PhysicalReview Letters, 57(22):2861-2864, 1986.\[9\] I).S.
Touretzky.
Connection|sin and composi-tional semantics.
Technical Report CMU-CS-89-147, Carnegie Mellon University, Pittsburgh,1989.\[1(\]\] I).L.
Waltz and J.B. Pollack.
Massively parallelparsing: A strongly interactive model of natu-ral language interpretation.
Cognitive Science,pages 51- 74, 1985.ACTES DECOL1NG-92, NANri!s, 23-28 Aofn" 1992 1 1 9 I>ROC.
OF COLING-92, NANIT~S, AUG. 23-28, 1992
