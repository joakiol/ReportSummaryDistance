Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition, pages 81?88,Prague, Czech Republic, June 2007 c?2007 Association for Computational LinguisticsThe Benefits of Errors:Learning an OT Grammar with a Structured Candidate SetTama?s Biro?ACLC, Universiteit van AmsterdamSpuistraat 210Amsterdam, The Netherlandst.s.biro@uva.nlAbstractWe compare three recent proposals addinga topology to OT: McCarthy?s PersistentOT, Smolensky?s ICS and B??ro?
?s SA-OT.
Totest their learnability, constraint rankings arelearnt from SA-OT?s output.
The errors inthe output, being more than mere noise, fol-low from the topology.
Thus, the learner hasto reconstructs her competence having ac-cess only to the teacher?s performance.1 Introduction: topology and OTThe year 2006 witnessed the publication of sev-eral novel approaches within Optimality Theory(OT) (Prince and Smolensky, 1993 aka 2004) intro-ducing some sort of neighbourhood structure (topol-ogy, geometry) on the candidate set.
This idea hasbeen already present since the beginnings of OT butits potentialities had never been really developed un-til recently.
The present paper examines the learn-ability of such an enriched OT architecture.Traditional Optimality Theory?s GEN functiongenerates a huge candidate set from the underlyingform (UF) and then EVAL finds the candidate w thatoptimises the Harmony function H(w) on this unre-stricted candidate set.
H(w) is derived from the vi-olation marks assigned by a ranked set of constraintsto w. The surface form SF corresponding to UF isthe (globally) optimal element of GEN(UF):SF(UF) = argoptw?GEN(UF)H(w) (1)Yet, already Prince and Smolensky(1993/2004:94-95) mention the possibility ofrestricting GEN, creating an alternative closer tostandard derivations.
Based the iterative syllabifi-cation in Imdlawn Tashlhiyt Berber, they suggest:?some general procedure (Do-?)
is allowed tomake a certain single modification to the input,producing the candidate set of all possible outcomesof such modification.?
The outputs of Do-?
are?neighbours?
of its input, so Do-?
defines a topol-ogy.
Subsequently, EVAL finds the most harmonicelement of this restricted candidate set, which thenserves again as the input of Do-?.
Repeating thisprocedure again and again produces a sequence ofneighbouring candidates with increasing Harmony,which converges toward the surface form.Calling Do-?
a restricted GEN, as opposed to thefreedom of analysis offered by the traditional GEN,McCarthy (2006) develops this idea into the Per-sistent OT architecture (aka.
harmonic serialism,cf.
references in McCarthy 2006).
He demonstrateson concrete examples how repeating the GEN ?EVAL ?
GEN ?
EVAL ?...
cycle until reach-ing some local optimum will produce a more restric-tive language typology that conforms rather well toobservation.
Importantly for our topic, learnabil-ity, he claims that Persistent OT ?can impose stricterranking requirements than classic OT because of theneed to ensure harmonic improvement in the inter-mediate forms as well as the ultimate output?.In two very different approaches, both based onthe traditional concept of GEN, Smolensky?s Inte-grated Connectionist/Symbolic (ICS) Cognitive Ar-chitecture (Smolensky and Legendre, 2006) andthe strictly symbolic Simulated Annealing for Op-timality Theory Algorithm (SA-OT) proposed by81B??ro?
(2005a; 2005b; 2006a), use simulated anneal-ing to find the best candidate w in equation (1).Simulated annealing performs a random walk on thesearch space, moving to a similar (neighbouring) el-ement in each step.
Hence, it requires a topology onthe search space.
In SA-OT this topology is directlyintroduced on the candidate set, based on a linguis-tically motivated symbolic representation.
At thesame time, connectionist OT makes small changes inthe state of the network; so, to the extent that statescorrespond to candidates, we obtain again a neigh-bourhood relation on the candidate set.Whoever introduces a neighbourhood structure(or a restricted GEN) also introduces local optima:candidates more harmonic than all their neighbours,independently of whether they are globally opti-mal.
Importantly, each proposal is prone to bestuck in local optima.
McCarthy?s model repeats thegeneration-evaluation cycle as long as the first localoptimum is not reached; whereas simulated anneal-ing is a heuristic optimisation algorithm that some-times fails to find the global optimum and returnsanother local optimum.
How do these proposals in-fluence the OT ?philosophy?
?For McCarthy, the first local optimum reachedfrom UF is the grammatical form (the surface formpredicted by the linguistic competence model), sohe rejects equation (1).
Yet, Smolensky and B?
?ro?keep the basic idea of OT as in (1), and B??ro?
(2005b;2006a) shows the errors made by simulated anneal-ing can mimic performance errors (such as stressshift in fast speech).
So mainstream OptimalityTheory remains the model of linguistic competence,whereas its cognitively motivated, though imperfectimplementation with simulated annealing becomesa model of linguistic performance.
Or, as B??ro?
putsit, a model of the dynamic language production pro-cess in the brain.
(See also Smolensky and Legen-dre (2006), vol.
1, pp.
227-229.
)In the present paper we test the learnability of anOT grammar enriched with a neighbourhood struc-ture.
To be more precise, we focus on the latter ap-proaches: how can a learner acquire a grammar, thatis, the constraint hierarchy defining the Harmonyfunction H(w), if the learning data are produced bya performance model prone to make errors?
What isthe consequence of seeing errors not simply as merenoise, but as the result of a specific mechanism?2 Walking in the candidate setFirst, we introduce the production algorithms (sec-tion 2) and a toy grammar (section 3), before we canrun the learning algorithms (section 4).Equation (1) defines Optimality Theory as an op-timisation problem, but finding the optimal candi-date can be NP-hard (Eisner, 1997).
Past solutions?chart parsing (Tesar and Smolensky, 2000; Kuhn,2000) and finite state OT (see Biro (2006b) for anoverview)?require conditions met by several, butnot by all linguistic models.
They are also ?too per-fect?, not leaving room for performance errors andcomputationally too demanding, hence cognitivelynot plausible.
Alternative approaches are heuris-tic optimization techniques: genetic algorithms andsimulated annealing.These heuristic algorithms do not always find the(globally) optimal candidate, but are simple and stillefficient because they exploit the structure of thecandidate set.
This structure is realized by a neigh-bourhood relation: for each candidate w there existsa set Neighbours(w), the set of the neighboursof w. It is often supposed that neighbours differonly minimally, whatever this means.
The neigh-bourhood relation is usually symmetric, irreflexiveand results in a connected structure (any two candi-dates are connected by a finite chain of neighbours).The topology (neighbourhood structure) opensthe possibility to a (random) walk on the candi-date set: a series w0, w1, w2, ..., wL such that forall 0 ?
i < L, candidate wi+1 is wi or a neigh-bour of wi.
(Candidate w0 will be called winit, andwL will be wfinal, henceforth.)
Genetic algorithmsstart with a random population of winit?s, and em-ploy OT?s EVAL function to reach a population ofwfinal?s dominated by the (globally) optimal candi-date(s) (Turkel, 1994).
In what follows, however,we focus on algorithms using a single walk only.The simplest algorithm, gradient descent, comesin two flavours.
The version on Fig.
1 defines wi+1as the best element of set {wi}?Neighbours(wi).It runs as long as wi+1 differs from wi, and is deter-ministic for each winit.
Prince and Smolensky?s andMcCarthy?s serial evaluation does exactly this: winitis the underlying form, Do-?
(the restricted GEN)creates the set {w} ?Neighbours(w), and EVALfinds its best element.82ALGORITHM Gradient Descent: OT with restricted GENw := w_init;repeatw_prev := w;w := most_harmonic_element( {w_prev} U Neighbours(w_prev) );until w = w_prevreturn w # w is an approximation to the optimal solutionFigure 1: Gradient Descent: iterated Optimality Theory with a restricted GEN (Do-?
).ALGORITHM Randomized Gradient Descentw := w_init ;repeatRandomly select w?
from the set Neighbours(w);if (w?
not less harmonic than w) then w := w?
;until stopping condition = truereturn w # w is an approximation to the optimal solutionFigure 2: Randomized Gradient DescentThe second version of gradient descent isstochastic (Figure 2).
In step i, a ran-dom w?
?
Neighbours(wi) is chosen us-ing some pre-defined probability distribution onNeighbours(wi) (often a constant function).
Ifneighbour w?
is not worse than wi, then the next el-ement wi+1 of the random walk will be w?
; other-wise, wi+1 is wi.
The stopping condition requiresthe number of iterations reach some value, or theaverage improvement of the target function in thelast few steps drop below a threshold.
The output iswfinal, a local optimum if the walk is long enough.Simulated annealing (Fig.
3) plays with this sec-ond theme to increase the chance of finding theglobal optimum and avoid unwanted local optima.The idea is the same, but if w?
is worse than wi, thenthere is still a chance to move to w?.
The transitionprobability of moving to w?
depends on the targetfunction E at wi and w?, and on ?temperature?
T :P (wi ?
w?|T ) = exp(?E(w?)?E(wi)T).
Using arandom r, we move to w?
iff r < P (wi ?
w?|T ).Temperature T is gradually decreased following thecooling schedule.
Initially the system easily climbslarger hills, but later it can only descend valleys.
Im-portantly, the probability wfinal is globally optimalconverges to 1 as the number of iterations grows.But the target function is not real-valued in Op-timality Theory, so how can we calculate the tran-sition probability?
ICS (Smolensky and Legendre,2006) approximates OT?s harmony function with areal-valued target function, while B??ro?
(2006a) in-troduces a novel algorithm (SA-OT, Figure 4) toguarantee the principle of strict domination in theconstraint ranking.
The latter stays on the purelysymbolic level familiar to the linguist, but does notalways display the convergence property of tradi-tional simulated annealing.Temperature in the SA-OT Algorithm is a pair(K, t) with t > 0, and is diminished in two, em-bedded loops.
Similarly, the difference in the targetfunction (Harmony) is not a single real number but apair (C, d).
Here C is the fatal constraint, the high-est ranked constraint by which wi and w?
behave dif-ferently, while d is the difference of the violations ofthis constraint.
(For H(wi) = H(w?)
let the differ-ence be (0, 0).)
Each constraint is assigned a real-valued rank (most often an integer; we shall call ita K-value) such that a higher ranked constraint hasa higher K-value than a lower ranked constraint (hi-erarchies are fully ranked).
The K-value of the fatalconstraint corresponds to the first component of thetemperature, and the second component of the dif-ference in the target function corresponds to the sec-ond component of the temperature.
The transitionprobability from wi to its neighbour w?
is 1 if w?
isnot less harmonic than wi; otherwise, the originallyexponential transition probability becomesP(wi ?
w?| (K, t))=????
?1 if K-value of C< Ke?
dt if K-value of C= K0 if K-value of C> K83ALGORITHM Simulated Annealingw := w_init ; T := T_max ;repeatCHOOSE random w?
in Neighbours(w);Delta := E(w?)
- E(w);if ( Delta < 0 ) then w := w?
;else # move to w?
with transition probability P(Delta;T) = exp(-Delta/T):generate random r uniformly in range (0,1);if ( r < exp(-Delta / T) ) then w := w?
;T := alpha(T); # decrease T according to some cooling scheduleuntil stopping condition = truereturn w # w is an approximation to the minimal solutionFigure 3: Minimizing a real-valued energy function E(w) with simulated annealing.Again, wi+1 is w?
if the random number r generatedbetween 0 and 1 is less than this transition proba-bility; otherwise wi+1 = wi.
B??ro?
(2006a, Chapt.2-3) argues that this definition fits best the underly-ing idea behind both OT and simulated annealing.In the next part of the paper we focus on SA-OT,and return to the other algorithms afterwards only.3 A string grammarTo experiment with, we now introduce an abstractgrammar that mimics real phonological ones.Let the set of candidates generated by GEN forany input be {0, 1, ..., P ?
1}L, the set of strings oflength L over an alphabet of P phonemes.
We shalluse L = P = 4.
Candidate w?
is a neighbour ofcandidate w if and only if a single minimal oper-ation (a basic step) transforms w into w?.
A min-imal operation naturally fitting the structure of thecandidates is to change one phoneme only.
In or-der to obtain a more interesting search space and inorder to meet some general principles?the neigh-bourhood relation should be symmetric, yielding aconnected graph but be minimal?a basic step canonly change the value of a phoneme by 1 modulo P .For instance, in the L = P = 4 case, neighbours of0123 are among others 1123, 3123, 0133 and 0120,but not 1223, 2123 or 0323.
If the four phonemes arerepresented as a pair of binary features (0 = [??
],1 = [+?
], 2 = [++] and 3 = [?+]), then this basicstep alters exactly one feature.We also need constraints.
Constraint No-n countsthe occurrences of phoneme n (0 ?
n < P )in the candidate (i.e., assigns one violation markper phoneme n).
Constraint No-initial-n punishesphoneme n word initially only, whereas No-final-ndoes the same word finally.
Two more constraintssum up the number of dissimilar and similar pairs ofadjacent phonemes.
Let w(i) be the ith phoneme instring w, and let [b] = 1 if b is true and [b] = 0 if b isfalse; then we have 3P + 2 markedness constraints:No-n: non(w) = ?L?1i=0 [w(i) = n]No-initial-n: nin(w) = [w(0) = n]No-final-n: nfn(w) = [w(L?1) = n]Assimilate: ass(w) = ?L?2i=0 [w(i) 6= w(i+1)]Dissimilate: dis(w) =?L?2i=0 [w(i) = w(i+1)]Grammars also include faithfulness constraintspunishing divergences from a reference string ?,usually the input.
Ours sums up the distance of thephonemes in w from the corresponding ones in ?:FAITH?
(w) =?L?1i=0 d(?
(i), w(i))where d(a, b) = min((a ?
b) mod P, (b ?
a)mod P )) is the minimal number of basic steps trans-forming phoneme a into b.
In our case, faithfulnessis also the number of differing binary features.To illustrate SA-OT, we shall use grammar H:H: no0 ?
ass ?
Faith?=0000 ?
ni1 ?ni0 ?
ni2 ?
ni3 ?
nf0 ?
nf1 ?
nf2 ?nf3 ?
no3 ?
no2 ?
no1 ?
disA quick check proves that the global optimumis candidate 3333, but there are many other localoptima: 1111, 2222, 3311, 1333, etc.
Table 1shows the frequencies of the outputs as a functionof t step, all other parameters kept unchanged.Several characteristics of SA-OT can be observed.For high t step, the thirteen local optima ({1, 3}4and 2222) are all produced, but as the number of84ALGORITHM Simulated Annealing for Optimality Theoryw := w_init ;for K = K_max to K_min step K_stepfor t = t_max to t_min step t_stepCHOOSE random w?
in Neighbours(w);COMPARE w?
to w: C := fatal constraintd := C(w?)
- C(w);if d <= 0 then w := w?
;else w := w?
with transition probabilityP(C,d;K,t) = 1 , if K-value(C) < K= exp(-d/t) , if K-value(C) = K= 0 , if K-value(C) > Kend-forend-forreturn w # w is an approximation to the optimal solutionFigure 4: The Simulated Annealing for Optimality Theory Algorithm (SA-OT).iterations increases (parameter t step drops), theprobability of finding the globally optimal candidategrows.
In many grammars (e.g., ni1 and ni3 movedto between no0 and ass in H), the global optimumis the only output for small t step values.
Yet, Halso yields irregular forms: 1111 and 2222 are notglobally optimal but their frequencies grow togetherwith the frequency of 3333.4 Learning grammar from performanceTo summarise, given a grammar, that is, a constrainthierarchy, the SA-OT Algorithm produces perfor-mance forms, including the grammatical one (theglobal optimum), but possibly also irregular formsand performance errors.
The exact distribution de-pends on the parameters of the algorithm, whichare not part of the grammar, but related to external(physical, biological, pragmatic or sociolinguistic)factors, for instance, to speech rate.Our task of learning a grammar can be formulatedthus: given the output distribution of SA-OT basedon the target OT hierarchy (the target grammar),the learner seeks a hierarchy that produces a simi-lar performance distribution using the same SA-OTAlgorithm.
(See Yang (2002) on grammar learningas parameter setting in general.)
Without any infor-mation on grammaticality, her goal is not to mimiccompetence, not to find a hierarchy with the sameglobal optima.
The grammar learnt can diverge fromthe target hierarchy, as long as their performance iscomparable (see also Apoussidou (2007), p. 203).For instance, if ni1 and ni3 change place in gram-mar H, the grammaticality of 1111 and 3333 are re-versed, but the performance stays the same.
This re-sembles two native speakers whose divergent gram-mars are revealed only when they judge differentlyforms otherwise produced by both.We suppose that the learner employs the sameSA-OT parameter setting.
The acquisition of theparameters is deferred to future work, because thistask is not part of language acquisition but of socialacculturation: given a grammar, how can one learnwhich situation requires what speed rate or whatlevel of care in production?
Consequently, fine-tuning the output frequencies, which can be doneby fine-tuning the parameters (such as t step) andnot the grammar, is not our goal here.
But languagelearners do not seem to do it, either.Learning algorithms in Optimality Theory belongto two families: off-line and on-line algorithms.
Off-line algorithms, the prototype of which is Recur-sive Constraint Demotion (RCD) (Tesar, 1995; Tesarand Smolensky, 2000), first collect the data and thenattempt to build a hierarchy consistent with them.On-line algorithms, such as Error Driven ConstraintDemotion (ECDC) (Tesar, 1995; Tesar and Smolen-sky, 2000) and Gradual Learning Algorithm (GLA)(Boersma, 1997; Boersma and Hayes, 2001), startwith an initial hierarchy and gradually alter it basedon discrepancies between the learning data and thedata produced by the learner?s current hierarchy.Since infants gather statistical data on theirmother tongue-to-be already in pre-linguistic stages(Saffran et al, 1996; Gervain et al, submitted), anoff-line algorithm created our initial grammar.
Then,on-line learning refined it, modelling child language85output t step = 1 t step = 0.1 t step = 0.01 t step = 0.0013333 0.1174 ?
0.0016 0.2074 ?
0.0108 0.2715 ?
0.0077 0.3107 ?
0.00321111 0.1163 ?
0.0021 0.2184 ?
0.0067 0.2821 ?
0.0058 0.3068 ?
0.00582222 0.1153 ?
0.0024 0.2993 ?
0.0092 0.3787 ?
0.0045 0.3602 ?
0.00911133 0.0453 ?
0.0018 0.0485 ?
0.0038 0.0328 ?
0.0006 0.0105 ?
0.00143311 0.0436 ?
0.0035 0.0474 ?
0.0054 0.0344 ?
0.0021 0.0114 ?
0.0016others 0.5608 0.1776 < 0.0002 ?Table 1: Outputs of SA-OT for hierarchy H. ?Others?
are twelve forms, each with a frequency between 2%and 8% for t step = 1, and lower than 4.5% for t step = 0.1.
(Forms produced in 8% of the cases att step = 1 are not produced if t step = 0.01!)
An experiment consisted of running 4096 simulationsand counting relative frequencies; each cell contains the mean and standard deviation of three experiments.development.
(Although on-line algorithms requirevirtual production only, not necessarily uttered incommunication, we suppose the two go together.
)We defer for future work issues as parsing hiddenstructures, learning underlying forms and biases forranking markedness above faithfulness.4.1 Learning SA-OTWe first implemented Recursive Constraint Demo-tion with SA-OT.
To begin with, RCD creates a win-ner/loser table, in which rows correspond to pairs(w, l) such that winner w is a learning datum, andloser l is less harmonic than w. Column winnermarks contains the constraints that are more severelyviolated by the winner than by the loser, and vice-versa for column loser marks.
Subsequently, RCDbuilds the hierarchy from top.
It repeatedly collectsthe constraints not yet ranked that do not occur aswinner marks.
If no such constraint exists, then thelearning data are inconsistent.
These constraints arethen added to the next stratum of the hierarchy in arandom order, while the rows in the table containingthem as loser marks are deleted (because these rowshave been accounted for by the hierarchy).Given the complexity of the learning data pro-duced by SA-OT, it is an advantage of RCD thatit recognises inconsistent data.
But how to collectthe winner-loser pairs for the table?
The learner hasno information concerning the grammaticality of thelearning data, and only knows that the forms pro-duced are local optima for the target (unknown) hi-erarchy and the universal (hence, known) topology.Thus, we constructed the winner-loser table from allpairs (w, l) such that w was an observed form, andl was a neighbour of w. To avoid the noise presentin real-life data, we considered only w?s with a fre-quency higher than?N , where N was the numberof learning data.
Applying then RCD resulted in ahierarchy that produced the observed local optima?and most often also many others, depending on therandom constraint ranking in a stratum.
These un-wanted local optima suggest a new explanation ofsome ?child speech forms?.Therefore, more information is necessary to findthe target hierarchy.
As learners do not use nega-tive evidence (Pinker, 1984), we did not try to re-move extra local optima directly.
Yet, the learners docollect statistical information.
Accordingly, we en-riched the winner/loser table with pairs (w, l) suchthat w was a form observed significantly more fre-quently than l; l?s were observed forms and the extralocal optima.
(A difference in frequency was signifi-cant if it was higher than?N .)
The assumption thatfrequency reflects harmony is based on the heuris-tics of SA-OT, but is far not always true.
So RCDrecognised this new table often to be inconsistent.Enriching the table could also be done gradually,adding a new pair only if enough errors have sup-ported it (Error-Selective Learning, Tessier (2007).The pair is then removed if it proves inconsistentwith stronger pairs (pairs supported by more errors,or pairs of observed forms and their neighbours).Yet, we instead turned to real on-line algorithms,namely to Boersma?s Gradual Learning Algorithm(GLA) (Boersma, 1997).
(Error Driven ConstraintDemotion is not robust, and gets stuck for incon-sistent data.)
Similarly to Error-Selective Learn-ing, GLA accumulates gradually the arguments for86reranking two constraints.
The GLA Algorithm as-signs a real-valued rank r to each constraint, so thata higher ranked constraint has a higher r. Then, ineach learning step the learning datum (the winner)is compared to the output produced by the learner?sactual hierarchy (the loser).
Every constraint?s rankis decreased by a small value (the plasticity) if thewinner violates it more than the loser, and it is in-creased by the same value if the loser has more vi-olations than the winner.
Often?still, not always(Pater, 2005)?these small steps accumulate to con-verge towards the correct constraint ranking.When producing an output (the winner) for thetarget hierarchy and another one (the loser) for thelearner?s hierarchy, Boersma uses Stochastic OT(Boersma, 1997).
But one can also employ tradi-tional OT evaluation, whereas we used SA-OT witht step = 0.1.
The learner?s actual hierarchy inGLA is stored by the real-valued ranks r. So thefatal constraint in the core of SA-OT (Fig.
4) isthe constraint that has the highest r among the con-straints assigning different violations to w and w?.
(A random one of them, if more constraints have thesame r-values, but this is very rare.).
The K-valueswere the floor of the r-values.
(Note the possibil-ity of more constraints having the same K-value.
)The r-values could also be directly the K-values; butsince parameters K max, K min and K step are in-tegers, this would cause the temperature not enterthe domains of the constraints, which would skip animportant part of simulated annealing.Similarly to Stochastic OT, our model also dis-played different convergence properties of GLA.Quite often, GLA reranked its initial hierarchy (theoutput of RCD) into a hierarchy yielding the sameor a similar output distribution to that produced bythe target hierarchy.
The simulated child?s perfor-mance converged towards the parent?s performance,and ?child speech forms?
were dropped gradually.In other cases, however, the GLA algorithmturned the performance worse.
The reason for thatmight be more than the fact that GLA does not al-ways converge.
Increasing or decreasing the con-straints?
rank by a plasticity in GLA is done in or-der to make the winners gradually better and thelosers worse.
But in SA-OT the learner?s hierarchycan produce a form that is indeed more harmonic(but not a local optimum) for the target ranking thanthe learning datum; then the constraint promotionsand demotions miss the point.
Moreover, unlikein Stochastic OT, these misguided moves might bemore frequent than the opposite moves.Still, the system performed well with our gram-mar H. Although the initial grammars returned byRCD included local optima (?child speech forms?,e.g., 0000), learning with GLA brought the learner?sperformance most often closer to the teacher?s.
Still,final hierarchies could be very diverse, with differentglobal optima and frequency distributions.In another experiment the initial ranking was thetarget hierarchy.
Then, 13 runs returned the targetdistribution with some small changes in the hierar-chy; in five cases the frequencies changed slightly,but twice the distribution became qualitatively dif-ferent (e.g., 2222 not appearing).4.2 Learning in other architecturesLearning in the ICS architecture involves similarproblems to those encountered with SA-OT.
Thelearner is faced again with performance forms thatare local optima and not always better than unat-tested forms.
The learning differs exclusively as aconsequence of the connectionist implementation.In McCarthy?s Persistent OT, the learner onlyknows that the observed form is a local optimum,i.
e., it is better than all its neighbours.
Then, she hasto find a path backwards, from the surface form tothe underlying form, such that in each step the can-didate closer to the SF is better than all other neigh-bours of the candidate closer to the UF.
Hence, theproblem is more complex, but it results in a similarwinner/loser table of locally close candidates.5 Conclusion and future workWe have tested the learnability of an OT grammarenriched with a neighbourhood structure.
The learn-ing data were produced by a performance model(viz., SA-OT), so the learner only had access to theteacher?s performance.
But by knowing the mecha-nism distorting production, she still could learn thetarget competence more or less.
(Minor differencesin competence are possible, as long as the perfor-mance is very similar.)
She made use of the struc-ture (the topology) of the candidate set, but also ofthe observed error patterns.
Future work may exploit87the fact that different parameter settings of SA-OTyield different distributions.Not correctly reconstructed grammars often leadto different grammaticality judgements, but also toquantitative differences in the performance distribu-tion, despite the qualitative similarity.
This fact canexplain diachronic changes and why some grammarsare evolutionarily more stable than others.Inaccurate reconstruction, as opposed to exactlearning, is similar to what Dan Sperber and oth-ers said about symbolic-cultural systems: ?The tacitknowledge of a participant in a symbolic-culturalsystem is neither taught nor learned by rote.
Rathereach new participant [...] reconstructs the ruleswhich govern the symbolic-cultural system in ques-tion.
These reconstructions may differ considerably,depending upon such factors as the personal his-tory of the individual in question.
Consequently, theproducts of each individual?s symbolic mechanismare idiosyncratic to some extent.?
(Lawson and Mc-Cauley, 1990, p. 68, italics are original).
This obser-vation has been used to argue that cultural learningis different from language learning; now we turn thetable and claim that acquiring a language is indeedsimilar in this respect to learning a culture.ReferencesDiana Apoussidou.
2007.
The Learnability of MetricalPhonology.
Ph.D. thesis, University of Amsterdam.Tama?s B??ro?.
2005a.
How to define Simulated Annealingfor Optimality Theory?
In Proc.
10th FG and 9thMoL, Edinburgh.
Also ROA-8971.Tama?s B??ro?.
2005b.
When the hothead speaks: Sim-ulated Annealing Optimality Theory for Dutch fastspeech.
In C. Cremers et al, editor, Proc.
of the 15thCLIN, pages 13?28, Leiden.
Also ROA-898.Tama?s B??ro?.
2006a.
Finding the Right Words: Imple-menting Optimality Theory with Simulated Annealing.Ph.D.
thesis, University of Groningen.
ROA-896.Tama?s B??ro?.
2006b.
Squeezing the infinite into the fi-nite.
In A. Yli-Jyr et al, editor, Finite-State Methodsand Natural Language Processing, FSMNLP 2005,Helsinki, LNAI-4002, pages 21?31.
Springer.Paul Boersma and Bruce Hayes.
2001.
Empirical tests ofthe Gradual Learning Algorithm.
Linguistic Inquiry,32:45?86.
Also: ROA-348.1ROA: Rutgers Optimality Archive at http://roa.rutgers.eduPaul Boersma.
1997.
How we learn variation, option-ality, and probability.
Proceedings of the Institute ofPhonetic Sciences, Amsterdam (IFA), 21:43?58.Jason Eisner.
1997.
Efficient generation in primitive op-timality theory.
In Proc.
of the 35th Annual Meeting ofthe Association for Computational Linguistics and 8thEACL, pages 313?320, Madrid.Judit Gervain, Marina Nespor, Reiko Mazuka, RyotaHorie, and Jacques Mehler.
submitted.
Bootstrappingword order in prelexical infants: a japanese-italiancross-linguistic study.
Cognitive Psychology.Jonas Kuhn.
2000.
Processing optimality-theoretic syn-tax by interleaved chart parsing and generation.
InProc.ACL-38, Hongkong, pages 360?367.E.
Thomas Lawson and Robert N. McCauley.
1990.
Re-thinking Religion: Connecting Cognition and Culture.Cambridge University Press, Cambridge, UK.John J. McCarthy.
2006.
Restraint of analysis.
InE.
Bakovic?
et al, editor, Wondering at the Natural Fe-cundity of Things: Essays in Honor of A.
Prince, pages195?219.
U. of California, Santa Cruz.
ROA-844.Joe Pater.
2005.
Non-convergence in the GLA and vari-ation in the CDA.
ms., ROA-780.Steven Pinker.
1984.
Language Learnability & Lan-guage Development.
Harvard UP, Cambridge, Mass.Alan Prince and Paul Smolensky.
1993 aka 2004.Optimality Theory: Constraint Interaction in Gen-erative Grammar.
Blackwell, Malden, MA, etc.Also: RuCCS-TR-2, 1993; ROA Version: 537-0802,http://roa.rutgers.edu, 2002.Jenny R. Saffran, Richard N. Aslin, and Elissa L. New-port.
1996.
Statistical learning by 8-month-old in-fants.
Science, 274(5294):1926?1928.Paul Smolensky and Ge?raldine Legendre.
2006.The Harmonic Mind: From Neural Computation toOptimality-Theoretic Grammar.
MIT P., Cambridge.Bruce Tesar and Paul Smolensky.
2000.
Learnability inOptimality Theory.
MIT Press, Cambridge, MA.Bruce Tesar.
1995.
Computational Optimality Theory.Ph.D.
thesis, University of Colorado.
Also: ROA-90.Anne-Michelle Tessier.
2007.
Biases and Stages inPhonological Acquisition.
Ph.D. thesis, University ofMassachusetts Amherst.
Also: ROA-883.Bill Turkel.
1994.
The acquisition of Optimality Theo-retic systems.
m.s., ROA-11.Charles D. Yang.
2002.
Knowledge and Learning in Nat-ural Language.
Oxford U. P., Oxford?New York.88
