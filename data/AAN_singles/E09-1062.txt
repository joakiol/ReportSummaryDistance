Proceedings of the 12th Conference of the European Chapter of the ACL, pages 541?548,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsPerformance Confidence Estimation for Automatic SummarizationAnnie LouisUniversity of Pennsylvanialannie@seas.upenn.eduAni NenkovaUniversity of Pennsylvanianenkova@seas.upenn.eduAbstractWe address the task of automatically pre-dicting if summarization system perfor-mance will be good or bad based on fea-tures derived directly from either single- ormulti-document inputs.
Our labelled cor-pus for the task is composed of data fromlarge scale evaluations completed over thespan of several years.
The variation of databetween years allows for a comprehensiveanalysis of the robustness of features, butposes a challenge for building a combinedcorpus which can be used for training andtesting.
Still, we find that the problem canbe mitigated by appropriately normalizingfor differences within each year.
We ex-amine different formulations of the classi-fication task which considerably influenceperformance.
The best results are 84%prediction accuracy for single- and 74%for multi-document summarization.1 IntroductionThe input to a summarization system significantlyaffects the quality of the summary that can be pro-duced for it, by either a person or an automaticmethod.
Some inputs are difficult and summariesproduced by any approach will tend to be poor,while other inputs are easy and systems will ex-hibit good performance.
User satisfaction with thesummaries can be improved, for example by auto-matically flagging summaries for which a systemexpects to perform poorly.
In such cases the usercan ignore the summary and avoid the frustrationof reading poor quality text.
(Brandow et al, 1995) describes an intelligentsummarizer system that could identify documentswhich would be difficult to summarize based onstructural properties.
Documents containing ques-tion/answer sessions, speeches, tables and embed-ded lists were identified based on patterns andthese features were used to determine whether anacceptable summary can be produced.
If not, theinputs were flagged as unsuitable for automaticsummarization.
In our work, we provide deeperinsight into how other characteristics of the textitself and properties of document clusters can beused to identify difficult inputs.The task of predicting the confidence in systemperformance for a given input is in fact relevant notonly for summarization, but in general for all ap-plications aimed at facilitating information access.In question answering for example, a system maybe configured not to answer questions for whichthe confidence of producing a correct answer islow, and in this way increase the overall accuracyof the system whenever it does produce an answer(Brill et al, 2002; Dredze and Czuba, 2007).Similarly in machine translation, some sen-tences might contain difficult to translate phrases,that is, portions of the input are likely to leadto garbled output if automatic translation is at-tempted.
Automatically identifying such phraseshas the potential of improving MT as shown byan oracle study (Mohit and Hwa, 2007).
More re-cent work (Birch et al, 2008) has shown that prop-erties of reordering, source and target languagecomplexity and relatedness can be used to pre-dict translation quality.
In information retrieval,the problem of predicting system performance hasgenerated considerable interest and has led to no-tably good results (Cronen-Townsend et al, 2002;Yom-Tov et al, 2005; Carmel et al, 2006).5412 Task definitionIn summarization, researchers have recognizedthat some inputs might be more successfully han-dled by a particular subsystem (McKeown et al,2001), but little work has been done to qualify thegeneral characteristics of inputs that lead to subop-timal performance of systems.
Only recently theissue has drawn attention: (Nenkova and Louis,2008) present an initial analysis of the factors thatinfluence system performance in content selection.This study was based on results from the Doc-ument Understanding Conference (DUC) evalua-tions (Over et al, 2007) of multi-document sum-marization of news.
They showed that input, sys-tem identity and length of the target summary wereall significant factors affecting summary quality.Longer summaries were consistently better thanshorter ones for the same input, so improvementscan be easy in applications where varying targetsize is possible.
Indeed, varying summary size isdesirable in many situations (Kaisser et al, 2008).The most predictive factor of summary qualitywas input identity, prompting a closer investiga-tion of input properties that are indicative of dete-rioration in performance.
For example, summariesof articles describing different opinions about anissue or of articles describing multiple distinctevents of the same type were of overall poor qual-ity, while summaries of more focused inputs, deal-ing with descriptions of a single event, subject orperson (biographical), were on average better.A number of features were defined, capturingaspects of how focused on a single topic a giveninput is.
Analysis of the predictive power of thefeatures was done using only one year of DUCevaluations.
Data from later evaluations was usedto train and test a logistic regression classifier forprediction of expected system performance.
Thetask could be performed with accuracy of 61.45%,significantly above chance levels.The results also indicated that special care needsto be taken when pooling data from different eval-uations into a single dataset.
Feature selection per-formed on data from one year was not useful forprediction on data from other years, and actuallyled to worse performance than using all features.Moreover, directly indicating which evaluation thedata came from was the most predictive featurewhen testing on data from more than one year.In the work described here, we show how theapproach for predicting performance confidencecan be improved considerably by paying specialattention to the way data from different years iscombined, as well as by adopting alternative taskformulations (pairwise comparisons of inputs in-stead of binary class prediction), and utilizingmore representative examples for good and badperformance.
We also extend the analysis to sin-gle document summarization, for which predict-ing system performance turns out to be much moreaccurate than for multi-document summarization.We address three key questions.What features are predictive of performance ona given input?
In Section 4, we discuss fourclasses of features capturing properties of the in-put, related to input size, information-theoreticproperties of the distribution of words in the input,presence of descriptive (topic) words and similar-ity between the documents in multi-document in-puts.
Rather than using a single year of evaluationsfor the analysis, we report correlation with ex-pected system performance for all years and tasks,showing that in fact the power of these featuresvaries considerably across years (Section 5).How to combine data from different years?
Theavailable data spans several years of summariza-tion evaluations.
Between years, systems change,as well as number of systems and average inputdifficulty.
All of these changes impact system per-formance and make data from different years dif-ficult to analyze when taken together.
Still, onewould want to combine all of the available eval-uations in order to have more data for developingmachine learning models.
In Section 6 we demon-strate that this indeed can be achieved, by normal-izing within each year by the highest observed per-formance and only then combining the data.How to define input difficulty?
There are severalpossible definitions of ?input difficulty?
or ?goodperformance?.
All the data can be split in twobinary classes of ?good?
and ?bad?
performancerespectively, or only representative examples inwhich there is a clear difference in performancecan be used.
In Section 7 we show that these alter-natives can dramatically influence prediction ac-curacy: using representative examples improvesaccuracy by more than 10%.
Formulating the taskas ranking of two inputs, predicting which one ismore difficult, also turns out to be helpful, offeringmore data even within the same year of evaluation.5423 DataWe use the data from single- and multi-documentevaluations performed as part of the DocumentUnderstanding Conferences (Over et al, 2007)from 2001 to 2004.1 Generic multi-documentsummarization was evaluated in all of these years,single document summaries were evaluated onlyin 2001 and 2002.
We use the 100-word sum-maries from both tasks.In the years 2002-2004, systems were eval-uated respectively on 59, 37 and 100 (50for generic summarization and 50 biographical)multi-document inputs.
There were 149 inputs forsingle document summarization in 2001 and 283inputs in 2002.
Combining the datasets from thedifferent years yields a collection of 432 observa-tions for single-document summarization, and 196for multi-document summarization.Input difficulty, or equivalently expected con-fidence of system performance, was defined em-pirically, based on actual content selection evalua-tions of system summaries.
More specifically, ex-pected performance for each input was defined asthe average coverage score of all participating sys-tems evaluated on that input.
In this way, the per-formance confidence is not specific to any givensystem, but instead reflects what can be expectedfrom automatic summarizers in general.The coverage score was manually computed byNIST evaluators.
It measures content selection byestimating overlap between a human model and asystem summary.
The scale for the coverage scorewas different in 2001 compared to other years: 0to 4 scale, switching to a 0 to 1 scale later.4 FeaturesFor our experiments we use the features proposed,motivated and described in detail by (Nenkova andLouis, 2008).
Four broad classes of easily com-putable features were used to capture aspects ofthe input predictive of system performance.Input size-related Number of sentences in theinput, number of tokens, vocabulary size, percent-age of words used only once, type-token ratio.Information-theoretic measures Entropy ofthe input word distribution and KL divergence be-tween the input and a large document collection.1Evaluations from later years did not include generic sum-marization, but introduced new tasks such as topic-focusedand update summarization.Log-likelihood ratio for words in the inputNumber of topic signature words (Lin and Hovy,2000; Conroy et al, 2006) and percentage of sig-nature words in the vocabulary.Document similarity in the input set Thesefeatures apply to multi-document summarizationonly.
Pairwise similarity of documents within aninput were computed using tf.idf weighted vectorrepresentations of the documents, either using allwords or using only topic signature words.
In bothsettings, minimum, maximum and average cosinesimilarity was computed, resulting in six similar-ity features.Multi-document summaries from DUC 2001were used for feature selection.
The 29 sets forthat year were divided according to the averagecoverage score of the evaluated systems.
Sets withcoverage below the average were deemed to be theones that will elicit poor performance and the restwere considered examples of sets for which sys-tems perform well.
T-tests were used to select fea-tures that were significantly different between thetwo classes.
Six features were selected: vocabu-lary size, entropy, KL divergence, percentage oftopic signatures in the vocabulary, and average co-sine and topic signature similarity.5 Correlations with performanceThe Pearson correlations between features of theinput and average system performance for eachyear is shown in Tables 1 and 2 for multi- andsingle-document summarization respectively.
Thelast two columns show correlations for the com-bined data from different evaluation years.
Forthe last column in both tables, the scores in eachyear were first normalized by the highest score thatyear.
Features that were significantly correlatedwith expected performance at confidence level of0.95 are marked with (*).
Overall, better perfor-mance is associated with smaller inputs, lower en-tropy, higher KL divergence and more signatureterms, as well as with higher document similarityfor multi-document summarization.Several important observations can be madefrom the correlation numbers in the two tables.Cross-year variation There is a large variation inthe strength of correlation between performanceand various features.
For example, KL diver-gence is significantly correlated with performancefor most years, with correlation of 0.4618 for thegeneric summaries in 2004, but the correlation was543features 2001 2002 2003 2004G 2004B All(UN) All(N)tokens -0.2813 -0.2235 -0.3834* -0.4286* -0.1596 -0.2415* -0.2610*sentences -0.2511 -0.1906 -0.3474* -0.4197* -0.1489 -0.2311* -0.2753*vocabulary -0.3611* -0.3026* -0.3257* -0.4286* -0.2239 -0.2568* -0.3171*per-once -0.0026 -0.0375 0.1925 0.2687 0.2081 0.2175* 0.1813*type/token -0.0276 -0.0160 0.1324 0.0389 -0.1537 -0.0327 -0.0993entropy -0.4256* -0.2936* -0.1865 -0.3776* -0.1954 -0.2283* -0.2761*KL divergence 0.3663* 0.1809 0.3220* 0.4618* 0.2359 0.2296* 0.2879*avg cosine 0.2244 0.2351 0.1409 0.1635 0.2602 0.1894* 0.2483*min cosine 0.0308 0.2085 -0.5330* -0.1766 0.1839 -0.0337 -0.0494max cosine 0.1337 0.0305 0.2499 0.1044 -0.0882 0.0918 0.1982*num sign -0.1880 -0.0773 -0.1799 -0.0149 0.1412 -0.0248 0.0084% sign.
terms 0.3277 0.1645 0.1429 0.3174* 0.3071* 0.1952* 0.2609*avg topic 0.2860 0.3678* 0.0826 0.0321 0.1215 0.1745* 0.2021*min topic 0.0414 0.0673 -0.0167 -0.0025 -0.0405 -0.0177 -0.0469max topic 0.2416 0.0489 0.1815 0.0134 0.0965 0.1252 0.2082*Table 1: Correlations between input features and average system performance for multi-document inputsof DUC 2001-2003, 2004G (generic task), 2004B (biographical task), All data (2002-2004) - UNnor-malized and Normalized coverage scores.
P-values smaller than 0.05 are marked by *.not significant (0.1809) for 2002 data.
Similarly,the average similarity of topic signature vectors issignificant in 2002, but has correlations close tozero in the following two years.
This shows thatno feature exhibits robust predictive power, espe-cially when there are relatively few datapoints.
Inlight of this finding, developing additional featuresand combining data to obtain a larger collection ofsamples are important for future progress.Normalization Because of the variation from yearto year, normalizing performance scores is benefi-cial and leads to higher correlation for almost allfeatures.
On average, correlations increase by 0.05for all features.
Two of the features, maximum co-sine similarity and max topic word similarity, be-come significant only in the normalized data.
Aswe will see in the next section, prediction accu-racy is also considerably improved when scoresare normalized before pooling the data from dif-ferent years together.Single- vs. multi-document task The correla-tions between performance and input features arehigher in single-document summarization than inmulti-document.
For example, in the normalizeddata KL divergence has correlation of 0.28 formulti-document summarization but 0.40 for sin-gle document.
The number of signature termsis highly correlated with performance in single-document summarization (-0.25) but there is prac-tically no correlation for multi-document sum-maries.
Consequently, we can expect that theperformance prediction will be more accurate forsingle-document summarization.features 2001 2002 All(N)tokens -0.3784* -0.2434* -0.3819*sentences -0.3999* -0.2262* -0.3705*vocabulary -0.4410* -0.2706* -0.4196*per-once -0.0718 0.0087 0.0496type/token 0.1006 0.0952 0.1785entropy -0.5326* -0.2329* -0.3789*KL divergence 0.5332* 0.2676* 0.4035*num sign -0.2212* -0.1127 -0.2519*% sign 0.3278* 0.1573* 0.2042*Table 2: Correlations between input features andaverage system performance for single doc.
inputsof DUC?01, ?02, All (?01+?02) N-normalized.
P-values smaller than 0.05 are marked by *.6 Classification experimentsIn this section we explore how the alternative taskformulations influence success of predicting sys-tem performance.
Obviously, the two classes ofinterest for the prediction will be ?good perfor-mance?
and ?poor performance?.
But separat-ing the real valued coverage scores for inputs intothese two classes can be done in different ways.All the data can be used and the definition of?good?
or ?bad?
can be determined in relation tothe average performance on all inputs.
Or only thebest and worst sets can be used as representativeexamples.
We explore the consequences of adopt-ing either of these options.For the first set of experiments, we divide allinputs based on the mean value of the average sys-tem scores as in (Nenkova and Louis, 2008).
Allmulti-document results reported in this paper arebased on the use of the six significant features dis-cussed in Section 4.
DUC 2002, 2003 and 2004data was used for 10-fold cross validation.
We ex-544perimented with three classifiers available in R?logistic regression (LogR), decision tree (DTree)and support vector machines (SVM).
SVM anddecision tree classifiers are libraries under CRANpackages e1071 and rpart.2 Since our develop-ment set was very small (only 29 inputs), we didnot perform any parameter tuning.There is nearly equal number of inputs on eitherside of the average system performance and therandom baseline performance in this case wouldgive 50% accuracy.6.1 Multi-document taskThe classification accuracy for the multi-document inputs is reported in Table 3.
Thepartitioning into classes was done based onthe average performance (87 easy sets and 109difficult sets).As expected, normalization considerably im-proves results.
The absolute largest improvementof 10% is for the logistic regression classifier.
Forthis classifier, prediction accuracy for the non-normalized data is 54% while for the normalizeddata, it is 64%.
Logistic regression gives the bestoverall classification accuracy on the normalizeddata compared to SVM classifier that does best onthe unnormalized data (56% accuracy).
Normal-ization also improves precision and recall for theSVM and logistic regression classifiers.The differences in accuracies obtained by theclassifiers is also noticable and we discuss thesefurther in Section 7.6.2 Single document taskWe now turn to the task of predicting summa-rization performance for single document inputs.As we saw in section 5, the features are strongerpredictors for summarization performance in thesingle-document task.
In addition, there is moredata from evaluations of single document summa-rizers.
Stronger features and more training datacan both help achieve higher prediction accura-cies.
In this section, we separate out the two fac-tors and demonstrate that indeed the features aremuch more predictive for single document sum-marization than for multidocument.In order to understand the effect of having moretraining data, we did not divide the single doc-ument inputs into a separate development set touse for feature selection.
Instead, all the features2http://cran.r-project.org/web/packages/classifier accuracy P R FDTree 66.744 66.846 67.382 67.113LogR 67.907 67.089 69.806 68.421SVM 69.069 66.277 80.317 72.625Table 4: Single document input classification Pre-cision (P), Recall (R),and F score (F) for difficultinputs on DUC?01 and ?02 (total 432 examples)divided into 2 classes based on the average cover-age score (217 difficult and 215 easy inputs).discussed in Section 4 except the six cosine andtopic signature similarity measures are used.
Thecoverage score ranges in DUC 2001 and 2002 aredifferent.
They are normalized by the maximumscore within the year, then combined and parti-tioned in two classes with respect to the averagecoverage score.
In this way, the 432 observationsare split into almost equal halves, 215 good perfor-mance examples and 217 bad performance.
Table4 shows the accuracy, precision and recall of theclassifiers on single-document inputs.From the results in Table 4 it is evident thatall three classifiers achieve accuracies higher thanthose for multi-document summarization.
The im-provement is largest for decision tree classifica-tion, nearly 15%.
The SVM classifier has the high-est accuracy for single document summarizationinputs, (69%), which is 7% absolute improvementover the performance of the SVM classifier forthe multi-document task.
The smallest improve-ment of 4% is for the logistic regression classi-fier which is the one with highest accuracy for themulti-document taskImproved accuracy could be attributed to thefact that almost double the amount of data is avail-able for the single-document summarization ex-periments.
To test if this was the main reason forimprovement, we repeated the single-documentexperiments using a random sample of 196 inputs,the same amount of data as for the multi-documentcase.
Even with reduced data, single-documentinputs are more easily classifiable as difficult oreasy compared to multi-document, as shown in Ta-bles 3 and 5.
The SVM classifier is still the bestfor single-document summarization and its accu-racy is the same with reduced data as with alldata.
With less data, the performance of the lo-gistic regression and decision tree classifiers de-grades more and is closer to the numbers for multi-document inputs.545Classifier N/UN Acc Pdiff Rdiff Peasy Reasy Fdiff FeasyDTree UN 51.579 56.580 56.999 46.790 45.591 55.383 44.199N 52.105 56.474 57.786 46.909 45.440 55.709 44.298LogR UN 54.211 56.877 71.273 50.135 34.074 62.145 39.159N 63.684 63.974 79.536 63.714 45.980 69.815 51.652SVM UN 55.789 57.416 73.943 50.206 32.753 63.784 38.407N 62.632 61.905 81.714 61.286 38.829 69.873 47.063Table 3: Multi-document input classification results on UNnormalized and Normalized data from DUC2002 to 2004.
Both Normalized and UNormalized data contain 109 difficult and 87 easy inputs.
Sincethe split is not balanced, the accuracy of classification as well as the Precision (P), Recall (R) and F score(F) are reported for both classes of easy and diff(icult) inputs.classifier accuracy P R FDTree 53.684 54.613 53.662 51.661LogR 61.579 63.335 60.400 60.155SVM 69.474 66.339 85.835 73.551Table 5: Single-document-input classification Pre-cision (P), Recall (R), and F score (F) for difficultinputs on a random sample of 196 observations (99difficult/97 easy) from DUC?01 and ?02.7 Learning with representative examplesIn the experiments in the previous section, we usedthe average coverage score to split inputs into twoclasses of expected performance.
Poor perfor-mance was assigned to the inputs for which theaverage system coverage score was lower than theaverage for all inputs.
Good performance was as-signed to those with higher than average cover-age score.
The best results for this formulationof the prediction task is 64% accuracy for multi-document classification (logistic regression classi-fier; 196 datapoints) and 69% for single-document(SVM classifier; 432 and 196 datapoints).However, inputs with coverage scores close tothe average may not be representative of eitherclass.
Moreover, inputs for which performancewas very similar would end up in different classes.We can refine the dataset by using only those ob-servations that are highly representative of the cat-egory they belong to, removing inputs for whichsystem performance was close to the average.
Itis desirable to be able to classify mediocre inputsas a separate category.
Further studies are neces-sary to come up with better categorization of in-puts rather than two strict classes of difficult andeasy.
For now, we examine the strength of our fea-tures in distinguishing the extreme types by train-ing and testing only on inputs that are representa-tive of these classes.We test this hypothesis by starting with 196multi-document inputs and performing the 10-foldcross validation using only 80%, 60% and 50%of the data, incrementally throwing away obser-vations around the mean.
For example, the 80%model was learnt on 156 observations, taking theextreme 78 observations on each side into the dif-ficult and easy categories.
For the single documentcase, we performed the same tests starting witha random sample of 196 observations as 100%data.3 All classifiers were trained and tested onthe same division of folds during cross validationand compared using a paired t-test to determinethe significance of differences if any.
Results areshown in Table 6.
In parentheses after the accu-racy of a given classifier, we indicate the classifiersthat are significantly better than it.Classifiers trained and tested using only repre-sentative examples perform more reliably.
TheSVM classifier is the best one for the single-document setting and in most cases significantlyoutperforms logistic regression and decision treeclassifiers on accuracy and recall.
In the multi-document setting, SVM provides better overall re-call than logistic regression.
However, with re-spect to accuracy, SVM and logistic regressionclassifiers are indistinguishable.
The decision treeclassifier performs worse.For multi-document classification, the F scoredrops initially when data is reduced to only 80%.But when using only half of the data, accuracyof prediction reaches 74%, amounting to 10% ab-solute improvement compared to the scenario inwhich all available data is used.
In the single-document case, accuracy for the SVM classifierincreases consistently, reaching accuracy of 84%.8 Pairwise ranking approachThe task we addressed in previous sections was toclassify inputs into ones for which we expect good3We use the same amount of data as is available for multi-document so that the results can be directly comparable.546Single document classification Multi-document classificationData CL Acc P R F Acc P R F100%DTree 53.684 (S) 54.613 53.662 (S) 51.661 52.105 (S,L) 56.474 57.786 (S,L) 55.709LogR 61.579 (S) 63.335 60.400 (S) 60.155 63.684 63.974 79.536 69.815SVM 69.474 66.339 85.835 73.551 62.632 61.905 81.714 69.87380%DTree 62.000 (S) 62.917 (S) 67.089 (S) 62.969 53.333 57.517 55.004 (S) 51.817LogR 68.000 68.829 69.324 (S) 67.686 58.667 60.401 59.298 (S) 57.988SVM 71.333 70.009 86.551 75.577 62.000 61.492 71.075 63.90560%DTree 68.182 (S) 72.750 60.607 (S) 64.025 57.273 (S) 63.000 58.262 (S) 54.882LogR 70.909 73.381 69.250 69.861 67.273 68.357 70.167 65.973SVM 76.364 73.365 82.857 76.959 66.364 68.619 75.738 67.72650%DTree 70.000 (S) 69.238 67.905 (S) 66.299 65.000 60.381 (L) 70.809 64.479LogR 76.000 (S) 76.083 72.500 (S) 72.919 74.000 72.905 70.381 (S) 70.965SVM 84.000 83.476 89.000 84.379 72.000 67.667 79.143 71.963Table 6: Performance of multiple classifiers on extreme observations from single and multi-documentdata (100% data = 196 data points in both cases divided into 2 classes on the basis of average covergescore).
Reported precision (P), recall (R) and F score (F) are for difficult inputs.
Experiments on ex-tremes use equal number of examples from each class - baseline performance is 50%.
Systems whoseperformance is significantly better than the specified numbers are shown in brackets (S-SVM, D-DecisionTree, L-Logistic Regression).performance and ones for which poor system per-formance is expected.
In this section, we evaluatea different approach to input difficulty classifica-tion.
Given a pair of inputs, can we identify theone on which systems will perform better?
Thisranking task is easier than requiring a strict deci-sion on whether performance will be good or not.Ranking approaches are widely used in textplanning and sentence ordering (Walker et al,2001; Karamanis, 2003) to select the text with beststructure among a set of possible candidates.
Un-der the summarization framework, (Barzilay andLapata, 2008) ranked different summaries for thesame input according to their coherence.
Simi-larly, ranking alternative document clusters on thesame topic to choose the best input will prove anadded advantage to summarizer systems.
Whensummarization is used as part of an informationaccess interface, the clustering of related docu-ments that form the input to a system is doneautomatically.
Currently, the clustering of docu-ments is completely independent of the need forsubsequent summarization of the resulting clus-ters.
Techniques for predicting summarizer per-formance can be used to inform clustering so thatthe clusters most suitable for summarization canbe chosen.
Also, when sample inputs for whichsummaries were deemed to be good are available,these can be used as a standard with which newinputs can be compared.For the pairwise comparison task, the featuresare the difference in feature values between thetwo inputs A and B that form a pair.
The dif-ference in average system scores of inputs A andB in the pair is used to determine the input forwhich performance was better.
Every pair couldgive two training examples, one positive and onenegative depending on the direction in which thedifferences are computed.
We choose one exam-ple from every pair, maintaining an equal numberof positive and negative instances.The idea of using representative examples canbe applied for the pairwise formulation of the taskas well?the larger the difference in system perfor-mance is, the better example the pair represents.Very small score differences are not as indicativeof performance on one input being better than theother.
Hence the experiments were duplicated on80%, 60% and 40% of the data where the retainedexamples were the ones with biggest differencebetween the system performance on the two sets(as indicated by the average coverage score).
Therange of score differences in each year are indi-cated in the Table 7.All scores are normalized by the maximumscore within the year.
Therefore the smallest andlargest possible differences are 0 and 1 respec-tively.
The entries corresponding to the years2002, 2003 and 2004 show the SVM classificationresults when inputs were paired only with thosewithin the same year.
Next inputs of all years werepaired with no restrictions.
We report the classifi-cation accuracies on a random sample of these ex-amples equal in size to the number of datapointsin the 2004 examples.Using only representative examples leads to547Amt Data Min score diff Points Acc.All2002 0.00028 1710 65.792003 0.00037 666 73.942004 0.00023 4948 70.712002-2004 0.00005 4948 68.8580%2002 0.05037 1368 68.392003 0.08771 532 78.872004 0.05226 3958 73.362002-2004 0.02376 3958 70.6860%2002 0.10518 1026 73.042003 0.17431 400 82.502004 0.11244 2968 77.412002-2004 0.04844 2968 71.3940%2002 0.16662 684 76.032003 0.27083 266 87.312004 0.18258 1980 79.342002-2004 0.07489 1980 74.95Maximum score difference 2002 (0.8768), 2003 (0.8969),2004 (0.8482), 2002-2004 (0.8768)Table 7: Accuracy of SVM classification of mul-tidocument input pairs.
When inputs are pairedirrespective of year (2002-2004), datapoints equalin number to that in 2004 were chosen at random.consistently better results than using all the data.The best classification accuracy is 76%, 87% and79% for comparisons within the same year and74% for comparisons across years.
It is importantto observe that when inputs are compared with-out any regard to the year, the classifier perfor-mance is worse than when both inputs in the pairare taken from the same evaluation year, present-ing additional evidence of the cross-year variationdiscussed in Section 5.
A possible explanationis that system improvements in later years mightcause better scores to be obtained on inputs whichwere difficult previously.9 ConclusionsWe presented a study of predicting expected sum-marization performance on a given input.
Wedemonstrated that prediction of summarizationsystem performance can be done with high ac-curacy.
Normalization and use of representativeexamples of difficult and easy inputs both provebeneficial for the task.
We also find that per-formance predictions for single-document sum-marization can be done more accurately than formulti-document summarization.
The best classi-fier for single-document classification are SVMs,and the best for multi-document?logistic regres-sion and SVM.
We also record good predictionperformance on pairwise comparisons which canprove useful in a variety of situations.ReferencesR.
Barzilay and M. Lapata.
2008.
Modeling local co-herence: An entity-based approach.
CL, 34(1):1?34.A.
Birch, M. Osborne, and P. Koehn.
2008.
Predictingsuccess in machine translation.
In Proceedings ofEMNLP, pages 745?754.R.
Brandow, K. Mitze, and L. F. Rau.
1995.
Automaticcondensation of electronic publications by sentenceselection.
Inf.
Process.
Manage., 31(5):675?685.E.
Brill, S. Dumais, and M. Banko.
2002.
An analysisof the askmsr question-answering system.
In Pro-ceedings of EMNLP.D.
Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.2006.
What makes a query difficult?
In Proceed-ings of SIGIR, pages 390?397.J.
Conroy, J. Schlesinger, and D. O?Leary.
2006.Topic-focused multi-document summarization usingan approximate oracle score.
In Proceedings ofACL.S.
Cronen-Townsend, Y. Zhou, and W. B. Croft.
2002.Predicting query performance.
In Proceedings of SI-GIR, pages 299?306.M.
Dredze and K. Czuba.
2007.
Learning to admityou?re wrong: Statistical tools for evaluating webqa.
In NIPS Workshop on Machine Learning for WebSearch.M.
Kaisser, M. A. Hearst, and J.
B. Lowe.
2008.
Im-proving search results quality by customizing sum-mary lengths.
In Proceedings of ACL: HLT, pages701?709.N.
Karamanis.
2003.
Entity Coherence for DescriptiveText Structuring.
Ph.D. thesis, University of Edin-burgh.C.
Lin and E. Hovy.
2000.
The automated acquisitionof topic signatures for text summarization.
In Pro-ceedings of COLING, pages 495?501.K.
McKeown, R. Barzilay, D. Evans, V. Hatzivas-siloglou, B. Schiffman, and S. Teufel.
2001.Columbia multi-document summarization: Ap-proach and evaluation.
In Proceedings of DUC.B.
Mohit and R. Hwa.
2007.
Localization of difficult-to-translate phrases.
In Proceedings of ACL Work-shop on Statistical Machine Translations.A.
Nenkova and A. Louis.
2008.
Can you summa-rize this?
identifying correlates of input difficultyfor multi-document summarization.
In Proceedingsof ACL: HLT, pages 825?833.P.
Over, H. Dang, and D. Harman.
2007.
Duc in con-text.
Inf.
Process.
Manage., 43(6):1506?1520.M.
Walker, O. Rambow, and M. Rogati.
2001.
Spot:a trainable sentence planner.
In Proceedings ofNAACL, pages 1?8.E.
Yom-Tov, S. Fine, D. Carmel, and A. Darlow.2005.
Learning to estimate query difficulty: includ-ing applications to missing content detection anddistributed information retrieval.
In Proceedings ofSIGIR, pages 512?519.548
