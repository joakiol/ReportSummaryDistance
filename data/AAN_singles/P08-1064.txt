Proceedings of ACL-08: HLT, pages 559?567,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsA Tree Sequence Alignment-based Tree-to-Tree Translation ModelMin Zhang1  Hongfei Jiang2  Aiti Aw1  Haizhou Li1  Chew Lim Tan3 and Sheng Li21Institute for Infocomm Research 2Harbin Institute of Technology 3National University of Singaporemzhang@i2r.a-star.edu.sg hfjiang@mtlab.hit.edu.cn tancl@comp.nus.edu.sgaaiti@i2r.a-star.edu.sg lisheng@hit.edu.cnhli@i2r.a-star.edu.sgAbstractThis paper presents a translation model that isbased on tree sequence alignment, where a treesequence refers to a single sequence of sub-trees that covers a phrase.
The model leverageson the strengths of both phrase-based and lin-guistically syntax-based method.
It automati-cally learns aligned tree sequence pairs withmapping probabilities from word-aligned bi-parsed parallel texts.
Compared with previousmodels, it not only captures non-syntacticphrases and discontinuous phrases with lin-guistically structured features, but also sup-ports multi-level structure reordering of treetypology with larger span.
This gives ourmodel stronger expressive power than other re-ported models.
Experimental results on theNIST MT-2005 Chinese-English translationtask show that our method statistically signifi-cantly outperforms the baseline systems.1 IntroductionPhrase-based modeling method (Koehn et al,2003; Och and Ney, 2004a) is a simple, but power-ful mechanism to machine translation since it canmodel local reorderings and translations of multi-word expressions well.
However, it cannot handlelong-distance reorderings properly and does notexploit discontinuous phrases and linguisticallysyntactic structure features (Quirk and Menezes,2006).
Recently, many syntax-based models havebeen proposed to address the above deficiencies(Wu, 1997; Chiang, 2005; Eisner, 2003; Ding andPalmer, 2005; Quirk et al 2005; Cowan et al,2006; Zhang et al, 2007; Bod, 2007; Yamada andKnight, 2001; Liu et al, 2006; Liu et al, 2007;Gildea, 2003; Poutsma, 2000; Hearne and Way,2003).
Although good progress has been reported,the fundamental issues in applying linguistic syn-tax to SMT, such as non-isomorphic tree align-ment, structure reordering and non-syntactic phrasemodeling, are still worth well studying.In this paper, we propose a tree-to-tree transla-tion model that is based on tree sequence align-ment.
It is designed to combine the strengths ofphrase-based and syntax-based methods.
The pro-posed model adopts tree sequence 1  as the basictranslation unit and utilizes tree sequence align-ments to model the translation process.
Therefore,it not only describes non-syntactic phrases withsyntactic structure information, but also supportsmulti-level tree structure reordering in larger span.These give our model much more expressivepower and flexibility than those previous models.Experiment results on the NIST MT-2005 Chinese-English translation task show that our method sig-nificantly outperforms Moses (Koehn et al, 2007),a state-of-the-art phrase-based SMT system, andother linguistically syntax-based methods, such asSCFG-based and STSG-based methods (Zhang etal., 2007).
In addition, our study further demon-strates that 1) structure reordering rules in ourmodel are very useful for performance improve-ment while discontinuous phrase rules have lesscontribution and 2) tree sequence rules are able tomodel non-syntactic phrases with syntactic struc-ture information, and thus contribute much to theperformance improvement, but those rules consist-ing of more than three sub-trees have almost nocontribution.The rest of this paper is organized as follows:Section 2 reviews previous work.
Section 3 elabo-1 A tree sequence refers to an ordered sub-tree sequence thatcovers a phrase or a consecutive tree fragment in a parse tree.It is the same as the concept ?forest?
used in Liu et al(2007).559rates the modelling process while Sections 4 and 5discuss the training and decoding algorithms.
Theexperimental results are reported in Section 6.
Fi-nally, we conclude our work in Section 7.2 Related WorkMany techniques on linguistically syntax-basedSMT have been proposed in literature.
Yamadaand Knight (2001) use noisy-channel model totransfer a target parse tree into a source sentence.Eisner (2003) studies how to learn non-isomorphictree-to-tree/string mappings using a STSG.
Dingand Palmer (2005) propose a syntax-based transla-tion model based on a probabilistic synchronousdependency insertion grammar.
Quirk et al (2005)propose a dependency treelet-based translationmodel.
Cowan et al (2006) propose a feature-based discriminative model for target languagesyntactic structures prediction, given a sourceparse tree.
Huang et al (2006) study a TSG-basedtree-to-string alignment model.
Liu et al (2006)propose a tree-to-string model.
Zhang et al(2007b) present a STSG-based tree-to-tree transla-tion model.
Bod (2007) reports that the unsuper-vised STSG-based translation model performsmuch better than the supervised one.
The motiva-tion behind all these work is to exploit linguistical-ly syntactic structure features to model thetranslation process.
However, most of them fail toutilize non-syntactic phrases well that are provenuseful in the phrase-based methods (Koehn et al,2003).The formally syntax-based model for SMT wasfirst advocated by Wu (1997).
Xiong et al (2006)propose a MaxEnt-based reordering model forBTG (Wu, 1997) while Setiawan et al (2007) pro-pose a function word-based reordering model forBTG.
Chiang (2005)?s hierarchal phrase-basedmodel achieves significant performance improve-ment.
However, no further significant improve-ment is achieved when the model is made sensitiveto syntactic structures by adding a constituent fea-ture (Chiang, 2005).In the last two years, many research efforts weredevoted to integrating the strengths of phrase-based and syntax-based methods.
In the following,we review four representatives of them.1) Hassan et al (2007) integrate supertags (akind of lexicalized syntactic description) into thetarget side of translation model and language mod-el under the phrase-based translation framework,resulting in good performance improvement.
How-ever, neither source side syntactic knowledge norreordering model is further explored.2) Galley et al (2006) handle non-syntacticphrasal translations by traversing the tree upwardsuntil a node that subsumes the phrase is reached.This solution requires larger applicability contexts(Marcu et al, 2006).
However, phrases are utilizedindependently in the phrase-based method withoutdepending on any contexts.3) Addressing the issues in Galley et al (2006),Marcu et al (2006) create an xRS rule headed by apseudo, non-syntactic non-terminal symbol thatsubsumes the phrase and its corresponding multi-headed syntactic structure; and one sibling xRSrule that explains how the pseudo symbol can becombined with other genuine non-terminals foracquiring the genuine parse trees.
The name of thepseudo non-terminal is designed to reflect the fullrealization of the corresponding rule.
The problemin this method is that it neglects alignment consis-tency in creating sibling rules and the naming me-chanism faces challenges in describing morecomplicated phenomena (Liu et al, 2007).4) Liu et al (2006) treat all bilingual phrases aslexicalized tree-to-string rules, including thosenon-syntactic phrases in training corpus.
Althoughthe solution shows effective empirically, it onlyutilizes the source side syntactic phrases of the in-put parse tree during decoding.
Furthermore, thetranslation probabilities of the bilingual phrasesand other tree-to-string rules are not compatiblesince they are estimated independently, thus hav-ing different parameter spaces.
To address theabove problems, Liu et al (2007) propose to useforest-to-string rules to enhance the expressivepower of their tree-to-string model.
As is inherentin a tree-to-string framework, Liu et al?s methoddefines a kind of auxiliary rules to integrate forest-to-string rules into tree-to-string models.
One prob-lem of this method is that the auxiliary rules arenot described by probabilities since they are con-structed during decoding, rather than learned fromthe training corpus.
So, to balance the usage of dif-ferent kinds of rules, they use a very simple featurecounting the number of auxiliary rules used in aderivation for penalizing the use of forest-to-stringand auxiliary rules.In this paper, an alternative solution is presentedto combine the strengths of phrase-based and syn-5601( )IT e1( )JT fAFigure 1: A word-aligned parse tree pairs of a Chi-nese sentence and its English translationFigure 2: Two Examples of tree sequencesFigure 3: Two examples of translation rulestax-based methods.
Unlike previous work, our so-lution neither requires larger applicability contexts(Galley et al, 2006), nor depends on pseudo nodes(Marcu et al, 2006) or auxiliary rules (Liu et al,2007).
We go beyond the single sub-tree mappingmodel to propose a tree sequence alignment-basedtranslation model.
To the best of our knowledge,this is the first attempt to empirically explore thetree sequence alignment based model in SMT.3 Tree Sequence Alignment Model3.1 Tree Sequence Translation RuleThe leaf nodes of a sub-tree in a tree sequence canbe either non-terminal symbols (grammar tags) orterminal symbols (lexical words).
Given a pair ofsource and target parse trees 1( )JT f and 1( )IT e  inFig.
1, Fig.
2 illustrates two examples of tree se-quences derived from the two parse trees.
A treesequence translation rule r  is a pair of aligned treesequences r =< 21( )jjTS f , 21( )iiTS e , A%  >, where:z 21( )jjTS f is a source tree sequence, coveringthe span [ 1 2,j j ] in 1( )JT f , andz 21( )iiTS e is a target one, covering the span[ 1 2,i i ] in 1( )IT e , andz A% are the alignments between leaf nodes oftwo tree sequences, satisfying the followingcondition: 1 2 1 2( , ) :i j A i i i j j j?
?
?
?
?
?
?% .Fig.
3 shows two rules extracted from the tree pairshown in Fig.
1, where r1 is a tree-to-tree rule andr2 is a tree sequence-to-tree sequence rule.
Ob-viously, tree sequence rules are more powerfulthan phrases or tree rules as they can capture allphrases (including both syntactic and non-syntacticphrases) with syntactic structure information andallow any tree node operations in a longer span.We expect that these properties can well addressthe issues of non-isomorphic structure alignments,structure reordering, non-syntactic phrases anddiscontinuous phrases translations.3.2 Tree Sequence Translation ModelGiven the source and target sentences 1Jf and 1Ieand their parse trees 1( )JT f and 1( )IT e , the treesequence-to-tree sequence translation model isformulated as:1 11 11 1 1 1 1 1( ), ( )1 1( ), ( )1 1 11 1 1 1( | ) ( , ( ), ( ) | )( ( ( ) | )( ( ) | ( ), )( | ( ), ( ), ))J IJ II J I I J JT f T eJ JT f T eI J JI I J Jr rrrrP e f P e T e T f fP T f fP T e T f fP e T e T f f==????
(1)In our implementation, we have:5611) 1 1( ( ) | ) 1J JrP T f f ?
since we only use the bestsource and target parse tree pairs in training.2) 1 1 1 1( | ( ), ( ), ) 1I I J JrP e T e T f f ?
since we justoutput the leaf nodes of 1( )IT e to generate 1Ieregardless of source side information.Since 1( )JT f contains the information of 1Jf ,now we have:1 1 1 1 11 1( | ) ( ( ) | ( ), )( ( ) | ( ))I J I J JI Jr rrP e f P T e T f fP T e T f==(2)By Eq.
(2), translation becomes a tree structuremapping issue.
We model it using our tree se-quence-based translation rules.
Given the sourceparse tree 1( )JT f , there are multiple derivationsthat could lead to the same target tree 1( )IT e , themapping probability 1 1( ( ) | ( ))I JrP T e T f is obtainedby summing over the probabilities of all deriva-tions.
The probability of each derivation?
is givenas the product of the probabilities of all the rules( )ip r  used in the derivation (here we assume thata rule is applied independently in a derivation).2 21 11 1 1 1( | ) ( ( ) | ( ))= ( : ( ), ( ), )iI J I Ji ji i jrr rP e f P T e T fp r TS e TS f A?
?
?=< >??
%    (3)Eq.
(3) formulates the tree sequence alignment-based translation model.
Figs.
1 and 3 show howthe proposed model works.
First, the source sen-tence is parsed into a source parse tree.
Next, thesource parse tree is detached into two source treesequences (the left hand side of rules in Fig.
3).Then the two rules in Fig.
3 are used to map thetwo source tree sequences to two target tree se-quences, which are then combined to generate atarget parse tree.
Finally, a target translation isyielded from the target tree.Our model is implemented under log-linearframework (Och and Ney, 2002).
We use sevenbasic features that are analogous to the commonlyused features in phrase-based systems (Koehn,2004): 1) bidirectional rule mapping probabilities;2) bidirectional lexical rule translation probabilities;3) the target language model; 4) the number ofrules used and 5) the number of target words.
Inaddition, we define two new features: 1) the num-ber of lexical words in a rule to control the model?spreference for lexicalized rules over un-lexicalizedrules and 2) the average tree depth in a rule to bal-ance the usage of hierarchical rules and flat rules.Note that we do not distinguish between larger (tal-ler) and shorter source side tree sequences, i.e.
welet these rules compete directly with each other.4 Rule ExtractionRules are extracted from word-aligned, bi-parsedsentence pairs 1 1( ), ( ),J IT f T e A< > , which areclassified into two categories:z initial rule, if all leaf nodes of the rule areterminals (i.e.
lexical word), andz abstract rule, otherwise, i.e.
at least one leafnode is a non-terminal (POS or phrase tag).Given an initial rule 2 21 1( ), ( ),j ij iTS f TS e A< >% ,its sub initial rule is defined as a triple4 43 3?
( ), ( ),j ij iTS f TS e A< >  if and only if:z 4 43 3?
( ), ( ),j ij iTS f TS e A< > is an initial rule.z 3 4 3 4( , ) :i j A i i i j j j?
?
?
?
?
?
?% , i.e.A?
A?
%z 43( )jjTS f is a sub-graph of 21( )jjTS f while43( )iiTS e  is a sub-graph of 21( )iiTS e .Rules are extracted in two steps:1) Extracting initial rules first.2) Extracting abstract rules from extracted ini-tial rules with the help of sub initial rules.It is straightforward to extract initial rules.
Wefirst generate all fully lexicalized source and targettree sequences using a dynamic programming algo-rithm and then iterate over all generated source andtarget tree sequence pairs 2 21 1( ), ( )j ij iTS f TS e< > .
Ifthe condition ?
( , )i j?
1 2 1 2:A i i i j j j?
?
?
?
?
?
?is satisfied, the triple 2 21 1( ), ( ),j ij iTS f TS e A< >% isan initial rule, where A%  are alignments betweenleaf nodes of 21( )jjTS f  and 21( )iiTS e .
We then de-rive abstract rules from initial rules by removingone or more of its sub initial rules.
The abstractrule extraction algorithm presented next is imple-mented using dynamic programming.
Due to spacelimitation, we skip the details here.
In order to con-trol the number of rules, we set three constraintsfor both finally extracted initial and abstract rules:1) The depth of a tree in a rule is not greater562than h .2) The number of non-terminals as leaf nodes isnot greater than c .3) The tree number in a rule is not greater than d.In addition, we limit initial rules to have at mostseven lexical words as leaf nodes on either side.However, in order to extract long-distance reorder-ing rules, we also generate those initial rules withmore than seven lexical words for abstract rulesextraction only (not used in decoding).
This makesour abstract rules more powerful in handlingglobal structure reordering.
Moreover, by configur-ing these parameters we can implement othertranslation models easily: 1) STSG-based modelwhen 1d = ; 2) SCFG-based model when 1d =and 2h = ; 3) phrase-based translation model only(no reordering model) when 0c =  and 1h = .Algorithm 1: abstract rules extractionInput: initial rule set inirOutput: abstract rule set absr1: for each i inir r?
, do2:    put all sub initial rules of ir  into a set subiniir3:    for each subset subiniir?
?
do4:          if there are spans overlapping betweenany two rules in the subset ?
then5:                    continue   //go to line 36:           end if7:           generate an abstract rule by removingthe portions covered by ?
from ir  andco-indexing the pairs of non-terminalsthat rooting the removed source andtarget parts8:           add them into the abstract rule set absr9:     end do10: end do5 DecodingGiven 1( )JT f , the decoder is to find the best deri-vation ?
that generates < 1( )JT f , 1( )IT e >.111 1,?
arg max ( ( ) | ( ))arg max ( )IIiI Jeie rre P T e T fp r?
??=?
?
(4)Algorithm 2: Tree Sequence-based DecoderInput: 1( )JT f   Output: 1( )IT eData structures:1 2[ , ]h j j    To store translations to a span 1 2[ , ]j j1: for s = 0 to J -1 do      // s: span length2:     for 1j = 1 to J - s , 2j = 1j + s  do3:          for each rule r spanning 1 2[ , ]j j  do4:               if r  is an initial rule then5:                    insert r into 1 2[ , ]h j j6:               else7:      generate new translations fromr by replacing non-terminal leafnodes of r with their correspond-ing spans?
translations that are al-ready translated in previous steps8:      insert them into 1 2[ , ]h j j9:  end if10: end for11: end for12: end for13: output the hypothesis with the highest scorein [1, ]h J  as the final best translationThe decoder is a span-based beam search to-gether with a function for mapping the source deri-vations to the target ones.
Algorithm 2 illustratesthe decoding algorithm.
It translates each span ite-ratively from small one to large one (lines 1-2).This strategy can guarantee that when translatingthe current span, all spans smaller than the currentone have already been translated before if they aretranslatable (line 7).
When translating a span, if theusable rule is an initial rule, then the tree sequenceon the target side of the rule is a candidate transla-tion (lines 4-5).
Otherwise, we replace the non-terminal leaf nodes of the current abstract rulewith their corresponding spans?
translations thatare already translated in previous steps (line 7).
Tospeed up the decoder, we use several thresholds tolimit search beams for each span:1) ?
, the maximal number of rules used2) ?
, the minimal log probability of rules3) ?
, the maximal number of translations yieldIt is worth noting that the decoder does not forcea complete target parse tree to be generated.
If norules can be used to generate a complete targetparse tree, the decoder just outputs whatever have563been translated so far monotonically as one hy-pothesis.6 Experiments6.1 Experimental SettingsWe conducted Chinese-to-English translation ex-periments.
We trained the translation model on theFBIS corpus (7.2M+9.2M words) and trained a 4-gram language model on the Xinhua portion of theEnglish Gigaword corpus (181M words) using theSRILM Toolkits (Stolcke, 2002) with modifiedKneser-Ney smoothing.
We used sentences withless than 50 characters from the NIST MT-2002test set as our development set and the NIST MT-2005 test set as our test set.
We used the Stanfordparser (Klein and Manning, 2003) to parse bilin-gual sentences on the training set and Chinese sen-tences on the development and test sets.
Theevaluation metric is case-sensitive BLEU-4 (Papi-neni et al, 2002).
We used GIZA++ (Och and Ney,2004) and the heuristics ?grow-diag-final?
to gen-erate m-to-n word alignments.
For the MER train-ing (Och, 2003), we modified Koehn?s MERtrainer (Koehn, 2004) for our tree sequence-basedsystem.
For significance test, we used Zhang et alsimplementation (Zhang et al 2004).We set three baseline systems: Moses (Koehn etal., 2007), and SCFG-based and STSG-based tree-to-tree translation models (Zhang et al, 2007).
ForMoses, we used its default settings.
For theSCFG/STSG and our proposed model, we used thesame settings except for the parameters d and h( 1d = and 2h = for the SCFG; 1d = and 6h = forthe STSG; 4d =  and 6h = for our model).
Weoptimized these parameters on the training and de-velopment sets: c =3, ?
=20, ?
=-100 and ?
=100.6.2 Experimental ResultsWe carried out a number of experiments to ex-amine the proposed tree sequence alignment-basedtranslation model.
In this subsection, we first re-port the rule distributions and compare our modelwith the three baseline systems.
Then we study themodel?s expressive ability by comparing the con-tributions made by different kinds of rules, includ-ing strict tree sequence rules, non-syntactic phraserules, structure reordering rules and discontinuousphrase rules2.
Finally, we investigate the impact ofmaximal sub-tree number and sub-tree depth in ourmodel.
All of the following discussions are held onthe training and test data.RuleInitial Rules  Abstract RulesL P U TotalBP 322,965 0 0  322,965TR 443,010 144,459 24,871  612,340TSR 225,570 103,932 714  330,216Table 1: # of rules used in the testing ( 4d = , h =  6)(BP: bilingual phrase (used in Moses), TR: tree rule (on-ly 1 tree), TSR: tree sequence rule (> 1 tree), L: fullylexicalized, P: partially lexicalized, U: unlexicalized)Table 1 reports the statistics of rules used in theexperiments.
It shows that:1) We verify that the BPs are fully covered bythe initial rules (i.e.
lexicalized rules), in which thelexicalized TSRs model all non-syntactic phrasepairs with rich syntactic information.
In addition,we find that the number of initial rules is greaterthan that of bilingual phrases.
This is because onebilingual phrase can be covered by more than oneinitial rule which having different sub-tree struc-tures.2) Abstract rules generalize initial rules to un-seen data and with structure reordering ability.
Thenumber of the abstract rule is far less than that ofthe initial rules.
This is because leaf nodes of anabstract rule can be non-terminals that canrepresent any sub-trees using the non-terminals asroots.Fig.
4 compares the performance of differentmodels.
It illustrates that:1) Our tree sequence-based model significantlyoutperforms (p < 0.01) previous phrase-based andlinguistically syntax-based methods.
This empirical-ly verifies the effect of the proposed method.2) Both our method and STSG outperform Mos-es significantly.
Our method also clearly outper-forms STSG.
These results suggest that:z The linguistically motivated structure featuresare very useful for SMT, which can be cap-2 To be precise, we examine the contributions of strict treesequence rules and single tree rules separately in this section.Therefore, unless specified, the term ?tree sequence rules?used in this section only refers to the strict tree sequence rules,which must contain at least two sub-trees on the source side.564tured by the two syntax-based models throughtree node operations.z Our model is much more effective in utilizinglinguistic structures than STSG since it usestree sequence as basic translation unit.
Thisallows our model not only to handle structurereordering by tree node operations in a largerspan, but also to capture non-syntactic phras-es, which circumvents previous syntacticconstraints, thus giving our model more ex-pressive power.3) The linguistically motivated SCFG showsmuch lower performance.
This is largely becauseSCFG only allows sibling nodes reordering and failsto utilize both non-syntactic phrases and those syn-tactic phrases that cannot be covered by a singleCFG rule.
It thereby suggests that SCFG is lesseffective in modelling parse tree structure transferbetween Chinese and English when using PennTreebank style linguistic grammar and under word-alignment constraints.
However, formal SCFGshow much better performance in the formally syn-tax-based translation framework (Chiang, 2005).This is because the formal syntax is learned fromphrases directly without relying on any linguistictheory (Chiang, 2005).
As a result, it is more ro-bust to the issue of non-syntactic phrase usage andnon-isomorphic structure alignment.24.7126.0723.8622.7221.522.523.524.525.526.5SCFG Moses STSG OursBLEU(%)Figure 4: Performance comparison of different methodsRuleTypeTR(STSG)TR+TSR_LTR+TSR_L+TSR_PTR+TSRBLEU(%) 24.71 25.72 25.93 26.07Table 2: Contributions of TSRs (see Table 1 for the de-finitions of the abbreviations used in this table)Table 2 measures the contributions of differentkinds of tree sequence rules.
It suggests that:1) All the three kinds of TSRs contribute to theperformance improvement and their combinationfurther improves the performance.
It suggests thatthey are complementary to each other since thelexicalized TSRs are used to model non-syntacticphrases while the other two kinds of TSRs can ge-neralize the lexicalized rules to unseen phrases.2)  The lexicalized TSRs make the major con-tribution since they can capture non-syntacticphrases with syntactic structure features.Rule Type BLEU (%)TR+TSR 26.07(TR+TSR) w/o SRR 24.62(TR+TSR) w/o DPR 25.78Table 3: Effect of Structure Reordering Rules (SRR:refers to the structure reordering rules that have at leasttwo non-terminal leaf nodes with inverted order in thesource and target sides, which are usually not capturedby phrase-based models.
Note that the reordering be-tween lexical words and non-terminal leaf nodes is notconsidered here) and Discontinuous Phrase Rules (DPR:refers to these rules having at least one non-terminalleaf node between two lexicalized leaf nodes) in ourtree sequence-based model ( 4d =  and 6h = )Rule Type # of rules # of rules overlapped(Intersection)SRR 68,217 18,379 (26.9%)DPR 57,244 18,379 (32.1%)Table 4: numbers of SRR and DPR rulesTable 3 shows the contributions of SRR andDPR.
It clearly indicates that SRRs are very effec-tive in reordering structures, which improve per-formance by 1.45 (26.07-24.62) BLEU score.However, DPRs have less impact on performancein our tree sequence-based model.
This seems incontradiction to the previous observations3 in lite-rature.
However, it is not surprising simply be-cause we use tree sequences as the basic translationunits.
Thereby, our model can capture all phrases.In this sense, our model behaves like a phrase-based model, less sensitive to discontinuous phras-3 Wellington et al (2006) reports that discontinuities are veryuseful for translational equivalence analysis using binary-branching structures under word alignment and parse treeconstraints while they are almost of no use if under wordalignment constraints only.
Bod (2007) finds that discontinuesphrase rules make significant performance improvement inlinguistically STSG-based SMT models.565es (Wellington et al, 2006).
Our additional expe-riments also verify that discontinuous phrase rulesare complementary to syntactic phrase rules (Bod,2007) while non-syntactic phrase rules may com-promise the contribution of discontinuous phraserules.
Table 4 reports the numbers of these twokinds of rules.
It shows that around 30% rules areshared by the two kinds of rule sets.
These over-lapped rules contain at least two non-terminal leafnodes plus two terminal leaf nodes, which impliesthat longer rules do not affect performance toomuch.22.0725.2826.1425.94 26.02 26.0721.522.523.524.525.526.51 2 3 4 5 6BLEU(%)Figure 5: Accuracy changing with different max-imal tree depths ( h = 1 to 6 when 4d = )22.7224.7126.0526.03 26.0725.7425.2925.2825.2624.7821.522.523.524.525.526.51 2 3 4 5BLEU(%)Figure 6: Accuracy changing with the different maximalnumber of trees in a tree sequence ( d =1 to 5), the upperline is for 6h =  while the lower line is for 2h = .Fig.
5 studies the impact when setting differentmaximal tree depth ( h ) in a rule on the perfor-mance.
It demonstrates that:1) Significant performance improvement isachieved when the value of h  is increased from 1to 2.
This can be easily explained by the fact thatwhen h = 1, only monotonic search is conducted,while h = 2 allows non-terminals to be leaf nodes,thus introducing preliminary structure features tothe search and allowing non-monotonic search.2) Internal structures and large span (due to hincreasing) are also useful as attested by the gainof 0.86 (26.14-25.28) Blue score when the value ofh  increases from 2 to 4.Fig.
6 studies the impact on performance by set-ting different maximal tree number (d) in a rule.
Itfurther indicates that:1) Tree sequence rules (d >1) are useful andeven more helpful if we limit the tree depth to nomore than two (lower line, h=2).
However, treesequence rules consisting of more than three sub-trees have almost no contribution to the perform-ance improvement.
This is mainly due to datasparseness issue when d >3.2) Even if only two-layer sub-trees (lower line)are allowed, our method still outperforms STSGand Moses when d>1.
This further validates theeffectiveness of our design philosophy of usingmulti-sub-trees as basic translation unit in SMT.7 Conclusions and Future WorkIn this paper, we present a tree sequence align-ment-based translation model to combine thestrengths of phrase-based and syntax-based me-thods.
The experimental results on the NIST MT-2005 Chinese-English translation task demonstratethe effectiveness of the proposed model.
Our studyalso finds that in our model the tree sequence rulesare very useful since they can model non-syntacticphrases and reorderings with rich linguistic struc-ture features while discontinuous phrases and treesequence rules with more than three sub-trees haveless impact on performance.There are many interesting research topics onthe tree sequence-based translation model worthexploring in the future.
The current method ex-tracts large amount of rules.
Many of them are re-dundant, which make decoding very slow.
Thus,effective rule optimization and pruning algorithmsare highly desirable.
Ideally, a linguistically andempirically motivated theory can be worked out,suggesting what kinds of rules should be extractedgiven an input phrase pair.
For example, mostfunction words and headwords can be kept in ab-stract rules as features.
In addition, word align-ment is a hard constraint in our rule extraction.
Wewill study direct structure alignments to reduce theimpact of word alignment errors.
We are also in-terested in comparing our method with the forest-to-string model (Liu et al, 2007).
Finally, wewould also like to study unsupervised learning-based bilingual parsing for SMT.566ReferencesRens Bod.
2007.
Unsupervised Syntax-Based MachineTranslation: The Contribution of DiscontinuousPhrases.
MT-Summmit-07.
51-56.David Chiang.
2005.
A hierarchical phrase-based mod-el for SMT.
ACL-05.
263-270.Brooke Cowan, Ivona Kucerova and Michael Collins.2006.
A discriminative model for tree-to-tree transla-tion.
EMNLP-06.
232-241.Yuan Ding and Martha Palmer.
2005.
Machine transla-tion using probabilistic synchronous dependency in-sertion grammars.
ACL-05.
541-548.Jason Eisner.
2003.
Learning non-isomorphic tree map-pings for MT.
ACL-03 (companion volume).Michel Galley, Mark Hopkins, Kevin Knight and DanielMarcu.
2004.
What?s in a translation rule?
HLT-NAACL-04.Michel Galley, J. Graehl, K. Knight, D. Marcu, S. De-Neefe, W. Wang and I. Thayer.
2006.
Scalable Infe-rence and Training of Context-Rich SyntacticTranslation Models.
COLING-ACL-06.
961-968Daniel Gildea.
2003.
Loosely Tree-Based Alignment forMachine Translation.
ACL-03.
80-87.Jonathan Graehl and Kevin Knight.
2004.
Training treetransducers.
HLT-NAACL-2004.
105-112.Mary Hearne and Andy Way.
2003.
Seeing the wood forthe trees: data-oriented translation.
MT Summit IX,165-172.Liang Huang, Kevin Knight and Aravind Joshi.
2006.Statistical Syntax-Directed Translation with Ex-tended Domain of Locality.
AMTA-06 (poster).Dan Klein and Christopher D. Manning.
2003.
AccurateUnlexicalized Parsing.
ACL-03.
423-430.Philipp Koehn, F. J. Och and D. Marcu.
2003.
Statistic-al phrase-based translation.
HLT-NAACL-03.
127-133.Philipp Koehn.
2004.
Pharaoh: a beam search decoderfor phrase-based statistical machine translationmodels.
AMTA-04, 115-124Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, Ri-chard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin and Evan Herbst.
2007.
Moses: OpenSource Toolkit for Statistical Machine Translation.ACL-07 (poster) 77-180.Yang Liu, Qun Liu and Shouxun Lin.
2006.
Tree-to-String Alignment Template for Statistical MachineTranslation.
COLING-ACL-06.
609-616.Yang Liu, Yun Huang, Qun Liu and Shouxun Lin.2007.
Forest-to-String Statistical Translation Rules.ACL-07.
704-711.Daniel Marcu, W. Wang, A. Echihabi and K. Knight.2006.
SPMT: Statistical Machine Translation withSyntactified Target Language Phrases.
EMNLP-06.44-52.I.
Dan Melamed.
2004.
Statistical machine translationby parsing.
ACL-04.
653-660.Franz J. Och and Hermann Ney.
2002.
Discriminativetraining and maximum entropy models for statisticalmachine translation.
ACL-02.
295-302.Franz J. Och.
2003.
Minimum error rate training instatistical machine translation.
ACL-03.
160-167.Franz J. Och and Hermann Ney.
2004a.
The alignmenttemplate approach to statistical machine translation.Computational Linguistics, 30(4):417-449.Kishore Papineni, Salim Roukos, ToddWard and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
ACL-02.
311-318.Arjen Poutsma.
2000.
Data-oriented translation.COLING-2000.
635-641Chris Quirk and Arul Menezes.
2006.
Do we needphrases?
Challenging the conventional wisdom inSMT.
COLING-ACL-06.
9-16.Chris Quirk, Arul Menezes and Colin Cherry.
2005.Dependency treelet translation: Syntactically in-formed phrasal SMT.
ACL-05.
271-279.Stefan Riezler and John T. Maxwell III.
2006.
Gram-matical Machine Translation.
HLT-NAACL-06.248-255.Hendra Setiawan, Min-Yen Kan and Haizhou Li.
2007.Ordering Phrases with Function Words.
ACL-7.712-719.Andreas Stolcke.
2002.
SRILM - an extensible languagemodeling toolkit.
ICSLP-02.
901-904.Benjamin Wellington, Sonjia Waxmonsky and I. DanMelamed.
2006.
Empirical Lower Bounds on theComplexity of Translational Equivalence.
COLING-ACL-06.
977-984.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377-403.Deyi Xiong, Qun Liu and Shouxun Lin.
2006.
Maxi-mum Entropy Based Phrase Reordering Model forSMT.
COLING-ACL-06.
521?
528.Kenji Yamada and Kevin Knight.
2001.
A syntax-basedstatistical translation model.
ACL-01.
523-530.Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, ShengLi and Chew Lim Tan.
2007.
A Tree-to-Tree Align-ment-based Model for Statistical Machine Transla-tion.
MT-Summit-07.
535-542.Ying Zhang.
Stephan Vogel.
Alex Waibel.
2004.
Inter-preting BLEU/NIST scores: How much improvementdo we need to have a better system?
LREC-04.
2051-2054.567
