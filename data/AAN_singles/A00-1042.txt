Evaluation of Automatically Identified Index Termsfor Browsing Electronic Documents INina Wacholder, Judith L. Klavans and David K. EvansColumbia UniversityDepartment of Computer Science andCenter for Research on Information Access1.
AbstractWe present an evaluation of domain-independent atural anguage tools for use inthe identification of significant concepts indocuments.
Using qualitative evaluation, wecompare three shallow processing methods forextracting index terms, i.e., terms that can beused to model the content of documents.
Wefocus on two criteria: quality and coverage.
Interms of quality alone, our results show thattechnical term (TT) extraction \[Justeson andKatz 1995\] receives the highest rating.
How-ever, in terms of a combined quality and cover-age metric, the Head Sorting (HS) method,described in \[Wacholder 1998\], outperformsboth other methods, keyword (KW) and TT.2.
IntroductionIn this paper, we consider the problem of howto evaluate the automatic identification of indexterms that have been derived without recourseto lexicons or to other kinds of domain-specificinformation.
By index terms, we mean naturallanguage xpressions that constitute a meaning-ful representation f a document for humans.The premise of this research is that if signifi-cant topics coherently represent information ina document, hese topics can be used as indexterms that approximate the content of individ-ual documents in large collections of electronicdocuments.We compare three shallow processingmethods for identifying index terms:?
Keywords (KW) are terms identified bycounting frequency of stemmed words in adocument;Technical terms (TT) are noun phrases(NPs) or subparts of NPs repeated morethan twice in a document \[Justeson andKatz 1995\];Head sorted terms (HS) are identified bya method in which simplex noun phrases(as defined below) are sorted by head andthen ranked in decreasing order of fre-quency \[Wacholder 1998\].The three methods that we evaluated are do-main-independent i  that they use statisticaland/or linguistic properties that apply to anynatural anguage document in any field.
Thesemethods are also corpus-independent, i  thatthe ranking of terms for an individual documentis not dependent on properties of the corpus.2.1 Overview of methods and resultsSubjects were drawn from two groups:professionals and students.
Professionals in-cluded librarians and publishing professionalsfamiliar with both manual and automatic textindexing.
Students included undergraduate andgraduate students with a variety of academicinterests.To assess terms, we used a standardqualitative ranking technique.
We presentedsubjects with an article and a list of termsidentified by one of the three methods.
Subjectswere asked to answer the following generalquestion: "Would this term be useful in anelectronic index for this article?"
Terms wererated on a scale of 1 to 5, where 1 indicates ahigh quality term that should definitely be in-cluded in the index and 5 indicates a junk termthat definitely should not be included.
For ex-1 This research was partly funded by NSF IRI 97-12069, "Automatic identification ofsignificant topics in do-main independent full text documents" and NSF IRI 97-53054, "Computationally tractable methods for docu-ment analysis".
_'tO~ample, the phrase court-approved affirmativeaction plans received an average rating of 1from the professionals, meaning that it wasranked as useful for the article; the KW af-firmative received an average rating of 3.75,meaning that it was less useful; and the KWaction received an average ranking of 4.5,meaning that it was not useful.The goal of our research is to determinewhich method, or combination of methods,provides the best results.
We measure resultsin terms of two criteria: quality and coverage.By quality, we mean that evaluators rankedterms high on the 1 to 5 scale from highest olowest.
By coverage, we mean the thoroughnesswith which the terms cover the significant op-ics in the document.
Our methodology permitsus to measure both criteria, as shown in Figure4.Our results from both the professionals andstudents how that TTs are superior with re-spect to quality; however, there are only asmall number of TTs per document, so they donot provide adequate coverage in that they arenot fully representative of the document as awhole.
In contrast, KWs provide good cover-age but relatively poor quality in that KWs arevague, and not well filtered.
SNPs, which havebeen sorted using HS and filtered, provide abetter balance of quality and coverage.From our study, we draw the followingconclusions:?
The KW approach identifies some usefulindex terms, but they are mixed in with alarge number of low-ranked terms.?
The TT approach identifies high qualityterms, but with low coverage, i.e., rela-tively few indexing terms.?
The HS approach achieves a balance be-tween quality and coverage.3.
Domain-independent metrics for identi-fying significant opicsIn order to identify significant opics ina document, a significance measure is needed,i.e., a method for determining which conceptsin the document are relatively important for agiven task.
The need to determine the impor-tance of a particular concept within a documentis motivated by a range of applications, in-cluding information retrieval \[Salton 1989\],303automatic determination of authorship\[Mosteller and Wallace 1963\], similarity met-rics for cross-document clustering \[Hatzivas-siloglou et al 1999\], automatic indexing\[Hodges et al 1996\] and input to summariza-tion \[Paice 1990\].For example, one of the earlier appli-cations using frequency for identifying signifi-cant topics in a document was proposed by\[Luhn 1958\] for use in creating automatic ab-stracts.
For each document, a list of stop-listed stems was created, and ranked by fre-quency; the most frequent keywords were usedto identify significant sentences in the originaldocument.
Luhn's premise was that emphasis,as indicated by repetition of words and collo-cation is an indicator of significance.
Namely,"the more often certain words are found in eachother's company within a sentence, the moresignificance may be attributed to each of thesewords."
This basic observation, although re-fined extensively by later summarization tech-niques (as reviewed in \[Paice 1990\]), relies onthe capability of identifying significant con-cepts.The standard IR technique known astf*idf \[Salton 1989\] seeks to identify docu-ments relevant o a particular query by relativ-izing keyword frequency in a document ascompared to frequency in a corpus.
Thismethod can be used to locate at least some im-portant concepts in full text.
Although it hasbeen effective for information retrieval, forother applications, such as human-oriented in-dexing, this technique is impractical.
Ambigu-ity of stems (trad might refer to trader ortradition) and of isolated words (state might bea political entity or a mode of being) meansthat lists of keywords have not usually beenused to represent the content of a document tohuman beings.
Furthermore, humans have adifficult time processing stems and parts ofwords out of phrasal context.The technical term (TT) method, an-other technique for identification of significantterms in text that can be used as index termswas introduced by \[Justeson and Katz 1995\],who developed an algorithm for identifyingrepeated multi-word phrases such as centralprocessing unit in the computer domain orword sense in the lexical semantic domain.This algorithm identifies candidate TTs in acorpus by locating NPs consisting of nouns,adjectives, and sometimes prepositionalphrases.
TTs are defined as those NPs, or theirsubparts, which occur above some frequencythreshold in a corpus.
However, as \[Boguraevand Kennedy 1998\] observe, the TT techniquemay not characterize the full content of docu-ments.
Indeed, even in a technical document,TTs do not provide adequate coverage of theNPs in a document that contribute to its con-tent, especially since TTs are by definitionmulti-word.
A truly domain-general methodshould apply to both technical and non-technical documents.
The relevant differencebetween technical and non-technical documentsis that in technical documents, many of thetopics which are significant to the document asa whole may be also TTs.\[Wacholder 1998\] proposed themethod of Head Sorting for identifying signifi-cant topics that can be used to represent asource document.
HS also uses a frequencymeasure to provide an approximation of topicsignificance.
However, instead of counting fre-quency of stems or repetition of word se-quences, this method counts frequency of arelatively easily identified grammatical e ement,heads of simplex noun phrases (SNPs).
Forcommon NPs (NPs whose head is a commonnoun), an SNP is a maximal NP that includespremodifiers uch as determiners and posses-sives but not post-nominal constituents such asprepositions or relativizers.
For example, thewell-known book is an SNP but the well-knownbook on asteroids includes two SNPs, well-known book and asteroids.
For proper names,an SNP is a name that refers to a single entity.For example, Museum of the City of New York,the name of an organization, is an SNP eventhough the organizational name incorporates acity name.
Others, such as \[Church 1988\],have discussed a similar concept, sometimescalled simple or base NPs.The HS approach is based on the as-sumption that nominal elements can be used toconvey the gist of a document.
SNPs, whichare semantically and syntactically coherent,appear to be at a good level of detail for con-tent representation f the document.
'304SNPs are identified by a system \[Evans1998; Evans et al 2000\] which sequentiallyparses text that has been tagged with part ofspeech using a finite state machine.
Next, thecomplete list of SNPs identified in a documentis sorted by the head of the phrase, which, atleast for English-language common SNPs, isalmost always the last word.
The intuitive justi-fication for sorting SNPs by head is based onthe fundamental linguistic distinction betweenhead and modifier: in general, a head makes agreater contribution to the syntax and seman-tics of a phrase than does a modifier.
This lin-guistic insight can be extended to the documentlevel.
If, as a practical matter, it is necessary torank the contribution to a whole documentmade by the sequence of words constituting anNP, the head should be ranked more highlythan other words in the phrase.
This distinctionis important in linguistic theory; for example,\[Jackendoff 1977\] discusses the relationship ofheads and modifiers in phrase structure.
It isalso important in NLP, where, for example,\[Strzalkowski 1997\] and \[Evans and Zhai1996\] have used the distinction between headsand modifiers to add query terms to informa-tion retrieval systems.Powerful corpus processing techniqueshave been developed to measure deviance froman average occurrence or co-occurrence in thecorpus.
In this paper we chose to evaluatemethods that depend only on document-internaldata, independent of corpus, domain or genre.We therefore did not use, for example, tf*idf,the purely statistical technique that is the usedby most information retrieval systems, or\[Smadja 1993\], a hybrid statistical and sym-bolic technique for identifying collocations.4.
Experimental MethodTo evaluate techniques, we performed a quali-tative user evaluation i  which the terms identi-fied by each method were compared forusefulness as index terms.4.1 SubjectsWe performed our study with librari-ans, publishing professionals and undergradu-ate and graduate students at our university.
29subjects participated in the study: 7 librariansand publishing professionals and 22 students.4.2 DataFor this experiment, we selected threearticles from the 1990 Wall Street Journalcontained in the Tipster collection of docu-ments.
The articles were about 500 words inlength.To compare methods, each article wasprocessed three times: 1) with SMART toidentify stemmed keywords \[Salton 1989\]; 2)with an implementation of the TT algorithmbased on \[Justeson and Katz 1995\]; and 3) withour implementation of the HS method.
Outputfor one article is shown in Appendix A. Figure1 shows the articles selected, their length inwords and the number of index terms fromeach method for each article presented to thesubjects.DOC words KW TT HS415-0109 509 63 4 49516-0043 594 51 9 54517-0062 514 52 8 57Figure 1: Word and term count, by type, perarticleThe number of TTs is much lower than thenumber of KWs or HSs.
This presented us witha problem: on the one hand, we were concernedabout preserving the integrity of the threemethods, each of which has their own logic,and at the same time, we were concerned topresent lists that were balanced relative to eachother.
Toward this end, we made several deci-sions about presentation of the data:1.
Threshold: So that no bias would be un-intentionally introduced, we presentedsubjects with all terms output by eachmethod, up to a specified cut-off poin-However, using lists of equal length foreach method would have necessitated itheromitting HSs and KWs or changing thedefinition of TTs.
Therefore we made thefollowing decisions:?
For TTs, we included all identifiedterms;?
For HSs, we included all terms whosehead occurred more than once in thedocument;305..?
For KWs, we included all terms in or-der of decreasing frequency, up to thepoint where we observed diminishingquality and where the number of KWsapproximated the number of HSs.Order: For the KW and TT approach,order is not significant.
However, for theHS approach, the grouping together ofphrases with common heads is, we claim,one of the advantages of the method.
Wetherefore alphabetized the KWs and TTs instandard left to right order and alphabet-ized the HSs by head, e.g., trust accountprecedes money market fund.Morphological expansion: The KW ap-proach identifies stems which represent aset of one or more morphological variantsof the stem.
Since in some cases the stemis not an English word, we expanded eachstem to include the morphological variantsthat actually occurred in the article.
Forexample, for the stem reject, we listed re-jected and rejecting but did not list rejects,which did not occur in the article.4.3 Presentation to subjectsEach subject was presented with three articles.For one article, the subject received a headsorted list of HSs; for another article, the sub-ject received a list of technical terms, and forthe third article, the subject saw a list of key-words.
No time limit was placed on the task.5.
ResultsOur results for the three types of terms, bydocument, are shown in Figure 2.
Although weasked subjects to rate three articles, some vol-unteers rated only two.
All results were in-cluded.Doc900405-0109900516-0043 3.73900517-0062 2.983.27 Avg of AvgsFigure 2: Averageindex termsAvgKWrating3.08Avg AvgTT HSrating rating1.45 2.712.19 2.711.7 3.251.79 2.89ratings of 3 types of5.1 QualityFor the three lists of index terms, TTs receivedthe highest ratings for all three documents--anaverage of 1.79 on the scale of 1 to 5, with 1being the best rating.
HS came in second, withan average of 2.89, and KW came in last withan average of 3.27.
It should be noted that av-eraging the average conceals the fact that thenumber of TTs is much lower than the othertwo types of terms, as shown in Figure 1.Figure 3 (included before Appendix A)shows cumulative rankings of terms by method.The X axis represents ratings awarded by sub-jects.
The Y axis reflects the percentage ofterms receiving a given rank or better.
All dataseries must reach 100% since every term hasbeen assigned a rating by the evaluators.
Atany given data point, a larger value indicatesthat a larger percentage of that series' data hasthat particular ating or better.
For example,100% of the TTs have a rating of 3 or better;while only about 30% of the terms of the low-est-scoring KW document received a score of 3or better.
In two out of the three documents,HS terms fall between TTs and KWs.5.2 CoverageThe graph in Figure 3 shows resultsfor quality, not coverage.
In contrast, Figure 4,which shows the total number of terms rated ator below specified rankings, allows us to meas-ure quality and coverage.
(1 is the highest rat-ing; 5 is the lowest.)
This figure shows that theHS method identifies more high quality termsthan the TT method oes.~ od HSNumber of terms rankedat or better than2 3 4 527 75 124 16641 96 132 16015 21 21 21Figure 4: Running total of terms identified ator below a specified rankTT clearly identifies the highest quality terms:100% of TTs receive a rating of 2 or better.However, only 8 TTs received a rating of 2 orbetter (38% of the total), while 41 HSs re-306ceived a rating of 2 or better (26% of the total).This indicates that the TT method misses manyhigh quality terms.
KW, the least discriminat-ing method in terms of quality, also providesbetter coverage than does TT.This result is consistent with our observa-tion that TT identifies the highest quality terms,but there are very few of them: an average of 7per 500 words compared to over 50 for HS andKW.
Therefore there is a need for additionalhigh quality terms.
The list of HSs received ahigher average rating than did the list of KWs,as shown in Figure 2.
This is consistent withour expectation that phrases containing morecontent-bearing modifiers would be perceivedas more useful index terms than would singleword phrases consisting only of heads.5.3 Ranking variabilityThe difference in the average ratings forthe list of KWs and the list of head-sortedSNPs was less than expected.
The small differ-ence in average ratings for the HS list and theKW list can be explained, at least in part, bytwo factors: 1) Differences among profession-als and students in inter-subject agreement andreliability; 2) A discrepancy in the rating ofsingle word terms across term types.22 students and 7 professionals par-ticipated in the study.
Figure 5 shows differ-ences in the ratings of professionals and ofstudents.KWHSTTProfessionals Students2.64 3.302.3 3.031.49 2.1Figure 5: Average ratings, by term type, ofprofessionals and studentsWhen variation in the scores for terms was cal-culated using standard eviation, the standarddeviation for the professionals was 0.78, whilefor the students it was 1.02.
Because of therelatively low number of professionals, thestandard deviation was calculated only overterms that were rated by more than one profes-sional.
A review of the students' results showedthat they appeared not to be as careful as theprofessionals.
For example, the phrase 'WallStreet Journal' was included on the HS list onlybecause it is specified as the document source.However, four of the eight students assignedthis term a high rating (1 or 2); this is puzzlingbecause the document is about asbestos-relateddisease.
The other four students assigned a 4or 5 to 'Wall Street Journal', as we expected.But the average score for this term was 3, dueto the anomalous ratings.
We therefore havemore confidence in the reliability of the profes-sional ratings, even though there are relativelyfew of them.We examined some of the differences inrating for term types.
Single word index termsare rated more highly by professionals whenthey appear in the context of other single wordindex terms, but are downrated in the contextof phrasal expansions that make the meaning ofthe one-word term more specific.
The KW listand HS list overlap when the SNP consists onlyof a single word (the head) or only of a headmodified by determiners.
When the same wordappears in both lists in identical form, the tokenin the KW list tends to receive a better atingthan the token does when it appears in the HSlist, where it is often followed by expansions ofthe head.
For example, the word exposure re-ceived an average rating of 2.2 when it ap-peared on the KW list, but a rating of only 2.75on the HS list.
However, the more specificphrase racial quotas, which immediately fol-lowed quota on the HS list received a rating of1.To better understand these differences, weselected 40 multi-word phrases and examinedthe average score that the phrase received in theTT and HS lists, and compared it to the aver-age ratings that individual words received inthe KW list.
We found that in about half of thecases (21 of 40), the phrase as a whole and theindividual words in the phrase received similarscores, as in Example 1 in Figure 6.
In justover one-fourth of the cases (12 of 40), thephrase scored well, but scores from the indi-vidual words were rated from good to poor, asin Example 2.
In about one-eighth of the cases(6 of 40), the phrase scored well, but the indi-vidual words scored poorly, as in Example 3.Finally, in only one case, shown in Example 4of Figure 6, the phrase scored poorly but theindividual words scored well.307PhraseSupreme Court(1.5)reverse discrimi-I nation(1)lymph systememploymentdecisions(2.75)Word 1Supreme(1)reverse(3.25)lymph(1)employ-ment(1.25)Word 2Court(1.25)discrimi-nation(3.25)system(5)decisions(1.25)Figure 6: Comparison of scores of phrasesand single wordsThis shows that single words in isolation arejudged differently than the same word whenpresented in the context of a larger phrase.These results have important implications inthe design of indexing tools.6.
ConclusionOur results show that the head sortingtechnique outperforms two other indexingmethods, technical terms and keywords, asmeasured by balance of quality and coverage.We have performed a qualitative valuation ofthree techniques for identifying significantterms in a document, driven by an indexingtask.
Such an applicati;on can be used to createa profile or thumbnail of a document by pre-senting to users a set of terms which can beconsidered to be a representation f the contentof the document.
We have used human judgesto evaluate the effectiveness of each method.This research is a contribution to the overallevaluation of computational linguistic tools interms of their usefulness for human-orientedcomputational applications.8.
ReferencesBoguraev, Branimir and Kennedy, Christopher(1998) "Applications of term identificationterminology: domain description and contentcharacterisation", Natural Language Engi-neering 1(1): 1-28.Church, Kenneth Ward (1988) "A stochastic partsprogram and noun phrase parser for unre-stricted text", in Proceedings of the SecondConference on Applied Natural LanguageProcessing, pp.
136-143.Evans, David A. and Chengxiang Zhai (1996)"Noun-phrase analysis in unrestricted text forinformation retrieval", Proceedings of the34th Annual Meeting of the Association forComputational Linguistics, pp.
17-24.24-27June 1996, University of California, SantaCruz, California, Morgan Kaufmann Pub-lishers.Evans, David K. (1998) LinklT Documentation,Columbia University Department ofCom-puter Science Report.Evans, David K., Klavans, Judith, and Wacholder,Nina (2000) "Document processing withLinklT", RIAO Conference, Paris, France, toappear.Hatzivassiloglou, Vasileios, Judith L. Klavans andEleazar Eskin (1999) "Detecting text simi-larity over short passages: exploring linguis-tic feature combinations via machinelearning", Proceedings of the EMNLP/VLC-99 Joint SIGDAT Conference on EmpiricalMethods in NLP and Very Large Corpora,June 21-22, 1999, University of Maryland,College Park, MD.Hedges, Julia, Shiyun Yie, Ray Reighart and LoisBoggess (1996) "An automated system thatassists in the generation ofdocument in-dexes", Natural Language Engineering2(2): 137-160.Jackendoff, Ray (1977) X-bar Syntax: A Study ofPhrase Structure, MIT Press, Cambridge,MA.Justeson, John S. and Slava M. Katz (1995)"Technical terminology: some linguisticproperties and an algorithm for identificationin text", Natural Language Engineering1(1):9-27.Luhn, Hans P. (1958) "The automatic creation ofliterature abstracts", IBM Journal, 159-165.Mosteller, Frederick and David L. Wallace (1963)"Inference in an authorship problem", Jour-nal of the American Statistical Association58(302):275-309.
Available athttp://www.jstor.org/.Paice, Chris D. (1990) "Constructing literatureabstracts by computer: techniques and pros-pects".
Information Processing & Manage-ment 26(1): 171-186.Salton, Gerald (1989) Automatic Text Processing:The Transformation, Analysis and Retrievalof lnformation by Computer.
Addison-Wesley, Reading, MA.Smadja, Frank (1993) "Retrieving collocationsfrom text", Computational Linguistics19(1):143-177.Strzalkowski, Thomas (1997) "Building effectivequeries in natural language information re-trieval", Proceedings of the ANLP, ACL,Washington, DC., pp.299-306.Wacholder, Nina (1998) "Simplex NPS sorted byhead: a method for identifying significanttopics within a document", Proceedings ofthe Workshop on the Computational Treat-ment of Nominals, pp.70-79.
COLING-ACL'98, Montreal, Canada, August 16, 1998.Figure 3: Cumulative ranking of terms, by method0.90.8~ 0.7i.
0.6 0.50.4~ o.a0.2?~ 0.100.5 1 1.5 2 2.5 3Rating3.5 4 4.5 5308Appendix A: Terms identified in WSJ900405-0109HSsamendmentsHatch amendmentother amendmentsattemptsbiasjob biasintentional biasbillcommitteeSenate labor CommitteecourtSupreme Courtco-workersdecisionsSupreme Court decisionsemployment decisionsDemocratsdiscriminationreverse discriminationemployeeswomen employeesemployersgroupscivil-rights groupsconservative policygroupsOrrin Hatchhealthdiscriminatory impactJob-Bias Measurebasic employment anti-discrimination law1866 civil-rights lawlawsuitslawmakerslegislationcomprehensive legislationmore modest measureminority/minoritiespanelplanscourt-approved affirmativeaction plansdiscriminatory seniority planspracticesemployment practicesquotasracial quotasfight/rightsequal rightsyearKeywordsactionaddress/addressingadopt/adoptedaffirmativeagreeaimedalleged/allegingamendapprovedattempt/attemptsbiasbillBushchallengecircumstancescivilclearscommitteecourt/CourtdecisionDemocratsdiscriminationemployment/employers/employeesforce/Forcegive/givingGOPgroupsHatchhealth~ghimpactjobjustifylabor/Laborlawlawmakerslawsuitslegislative/legislationmakemeasureminority/minoritiesMr.overturningpanelplanspolicypracticesquotasracialrejected/rejectingreverserightsrules/rulingsafetySen./Sens.SenateshownstreetSupreme; vote/voted309womenworkersyearTechnical termsdiscriminatory impactemployment practiceSenator HatchSupreme Court
