The Computation of Word Associations:Comparing Syntagmatic and Paradigmatic ApproachesReinhard RappUniversity of Mainz, FASKD-76711 Germersheim, Germanyrapp@mail.fask.uni-mainz.deAbstractIt is shown that basic language processes suchas the production of free word associations andthe generation of synonyms can be simulatedusing statistical models that analyze the distri-bution of words in large text corpora.
Accord-ing to the law of association by contiguity, theacquisition of word associations can be ex-plained by Hebbian learning.
The free word as-sociations as produced by subjects on presenta-tion of single stimulus words can thus be pre-dicted by applying first-order statistics to thefrequencies of word co-occurrences as observedin texts.
The generation of synonyms can alsobe conducted on co-occurrence data but re-quires second-order statistics.
The reason is thatsynonyms rarely occur together but appear insimilar lexical neighborhoods.
Both approachesare systematically compared and are validatedon empirical data.
It turns out that for bothtasks the performance of the statistical system iscomparable to the performance of human sub-jects.1 IntroductionAccording to Ferdinand de Saussure (1916), thereare two fundamental types of relations betweenwords that he believes correspond to basic opera-tions of our brain: syntagmatic and paradigmaticassociations.
There is a syntagmatic relation be-tween two words if they co-occur in spoken orwritten language more frequently than expectedfrom chance and if they have different grammaticalroles in the sentences in which they occur.
Typicalexamples are the word pairs coffee ?
drink, sun ?hot, or teacher ?
school.
The relation between twowords is paradigmatic if the two words can sub-stitute for one another in a sentence without affect-ing the grammaticality or acceptability of the sen-tence.
Typical examples are synonyms or antonymslike quick ?
fast, or eat ?
drink.
Normally, wordswith a paradigmatic relation are the same part ofspeech, whereas words with a syntagmatic relationcan but need not be the same part of speech.In this paper we want to show that the two typesof relations as defined by de Saussure are reflectedin the statistical distribution of words in large cor-pora.
We present algorithms that automaticallyretrieve words with either the syntagmatic or theparadigmatic type of relationship from corpora andperform a quantitative evaluation of our results.2 Paradigmatic AssociationsParadigmatic associations are words with high se-mantic similarity.
According to Ruge (1992), thesemantic similarity of two words can be computedby determining the agreement of their lexicalneighborhoods.
For example, the semantic similarityof the words red and blue can be derived from thefact that they both frequently co-occur with wordslike color, flower, dress, car, dark, bright, beauti-ful, and so forth.
If for each word in a corpus a co-occurrence vector is determined whose entries arethe co-occurrences with all other words in the cor-pus, then the semantic similarities between wordscan be computed by conducting simple vectorcomparisons.
To determine the words most similarto a given word, its co-occurrence vector is com-pared to the co-occurrence vectors of all otherwords using one of the standard similarity measures,for example, the cosine coefficient.
Those wordsthat obtain the best values are considered to be mostsimilar.
Practical implementations of algorithmsbased on this principle have led to excellent resultsas documented in papers by Ruge (1992), Grefen-stette (1994), Agarwal (1995), Landauer & Dumais(1997), Sch?tze (1997), and Lin (1998).2.1 Human DataIn this section we relate the results of our version ofsuch an algorithm to similarity estimates obtainedby human subjects.
Fortunately, we did not need toconduct our own experiment to obtain the human?ssimilarity estimates.
Instead, such data was kindlyprovided by Thomas K. Landauer, who had taken itfrom the synonym portion of the Test of English asa Foreign Language (TOEFL).
Originally, the datacame, along with normative data, from the Educa-tional Testing Service (Landauer & Dumais 1997).The TOEFL is an obligatory test for foreign stu-dents who would like to study at an American orEnglish university.The data comprises 80 test items.
Each itemconsists of a problem word in testing parlance andfour alternative words, from which the test taker isasked to choose that with the most similar meaningto the problem word.
For example, given the testsentence ?Both boats and trains are used fortransporting the materials?
and the four alternativewords planes, ships, canoes, and railroads, thesubject would be expected to choose the word ships,which is the one most similar to boats.2.2 CorpusAs mentioned above, our method of simulating thiskind of behavior is based on regularities in the sta-tistical distribution of words in a corpus.
We choseto use the British National Corpus (BNC), a 100-million-word corpus of written and spoken languagethat was compiled with the intention of providing arepresentative sample of British English.Since this corpus is rather large, to save diskspace and processing time we decided to remove allfunction words from the text.
This was done on thebasis of a list of approximately 200 English functionwords.
We also decided to lemmatize the corpus aswell as the test data.
This not only reduces thesparse-data problem but also significantly reducesthe size of the co-occurrence matrix to be computed.More details on these two steps of corpus pre-processing can be found in Rapp (1999).2.3 Co-occurrence CountingFor counting word co-occurrences, as in most otherstudies a fixed window size is chosen and it is de-termined how often each pair of words occurswithin a text window of this size.
Choosing a win-dow size usually means a trade-off between twoparameters: specificity versus the sparse-data prob-lem.
The smaller the window, the stronger the asso-ciative relation between the words inside the win-dow, but the more severe the sparse data problem(see figure 1 in section 3.2).
In our case, with ?1word, the window size looks rather small.
However,this can be justified since we have reduced the ef-fects of the sparse-data problem by using a largecorpus and by lemmatizing the corpus.
It alsoshould be noted that a window size of ?1 appliedafter elimination of the function words is compa-rable to a window size of ?2 without elimination ofthe function words (assuming that roughly everysecond word is a function word).Based on the window size of ?1, we computed aco-occurrence matrix of about a million words inthe lemmatized BNC.
Although the resulting matrixis extremely large, this was feasible since we used asparse format that does not store zero entries.2.4 Computation of Word SimilaritiesTo determine the words most similar to a givenword, the co-occurrence vector of this word is com-pared to all other vectors in the matrix and thewords are ranked according to the similarity valuesobtained.
It is expected that the most similar wordsare ranked first in the sorted list.For vector comparison, different similaritymeasures can be considered.
Salton & McGill(1983) proposed a number of measures, such as thecosine coefficient, the Jaccard coefficient, and theDice coefficient.
For the computation of relatedterms and synonyms, Ruge (1995) and Landauer &Dumais (1997) used the cosine measure, whereasGrefenstette (1994, p. 48) used a weighted Jaccardmeasure.
We propose here the city-block metric,which computes the similarity between two vectorsX and Y as the sum of the absolute differences ofcorresponding vector positions:?=?=niii YXs1In a number of experiments we compared it to othersimilarity measures, such as the cosine measure, theJaccard measure (standard and binary), the Euclid-ean distance, and the scalar product, and found thatthe city-block metric yielded good results (see Rapp,1999).2.5 ResultsTable 1 shows the top five paradigmatic associa-tions to six stimulus words.
As can be seen from thetable, nearly all words listed are of the same part ofspeech as the stimulus word.
Of course, our defini-tion of the term paradigmatic association as givenin the introduction implies this.
However, the simu-lation system never obtained any information onpart of speech, and so it is nevertheless surprisingthat ?
besides computing term similarities ?
it im-plicitly seems to be able to cluster parts of speech.This observation is consistent with other studies(e.g., Ruge, 1995).blue cold fruit green tobacco whiskeyred hot food red cigarette whiskygreen warm flower blue alcohol brandygrey dry fish white coal champagneyellow drink meat yellow import lemonadewhite cool vegetable grey textile vodkaTable 1: Computed paradigmatic associations.A qualitative inspection of the word lists generatedby the system shows that the results are quitesatisfactory.
Paradigmatic associations like blue red, cold hot, and tobacco cigarette areintuitively plausible.
However, a quantitativeevaluation would be preferable, of course, and forthis reason we did a comparison with the results ofthe human subjects in the TOEFL test.
Rememberthat the human subjects had to choose the wordmost similar to a given stimulus word from a list offour alternatives.In the simulation, we assumed that the systemhad chosen the correct alternative if the correct wordwas ranked highest among the four alternatives.This was the case for 55 of the 80 test items, whichgives us an accuracy of 69%.
This accuracy mayseem low, but it should be taken into account thatthe TOEFL tests the language abilities of prospec-tive university students and therefore is rather diffi-cult.
Actually, the performance of the average hu-man test taker was worse than the performance ofthe system.
The human subjects were only able tosolve 51.6 of the test items correctly, which gives anaccuracy of 64.5%.
Please note that in the TOEFL,average performance (over several types of tests,with the synonym test being just one of them) ad-mits students to most universities.
On the otherhand, by definition, the test takers did not have anative command of English, so the performance ofnative speakers would be expected to be signifi-cantly better.
Another consideration is the fact thatour simulation program was not designed to makeuse of the context of the test word, so it neglectedsome information that may have been useful for thehuman subjects.Nevertheless, the results look encouraging.Given that our method is rather simple, let us nowcompare our results to the results obtained withmore sophisticated methods.
One of the methodsreported in the literature is singular value decompo-sition (SVD); another is shallow parsing.
SVD, asdescribed by Sch?tze (1997) and Landauer & Du-mais (1997), is a method similar to factor analysisor multi-dimensional scaling that allows a signifi-cant reduction of the dimensionality of a matrix withminimum information loss.
Landauer & Dumais(1997) claim that by optimizing the dimensionalityof the target matrix the performance of their wordsimilarity predictions was significantly improved.However, on the TOEFL task mentioned above,after empirically determining the optimal dimen-sionality of their matrix, they report an accuracy of64.4%.
This is somewhat worse than our result of69%, which was achieved without SVD and withoutoptimizing any parameters.
It must be emphasized,however, that the validity of this comparison isquestionable, as many parameters of the two modelsare different, making it unclear which ones are re-sponsible for the difference.
For example, Landauerand Dumais used a smaller corpus (4.7 millionwords), a larger window size (151 words on aver-age), and a different similarity measure (cosinemeasure).
We nevertheless tend to interpret theresults of our comparison as evidence for the viewthat SVD is just another method for smoothing thathas its greatest benefits for sparse data.
However,we do not deny the technical value of the method.The one-time effort of the dimensionality reductionmay be well spent in a practical system because allsubsequent vector comparisons will be speeded upconsiderably with shorter vectors.Let us now compare our results to those ob-tained using shallow parsing, as previously done byGrefenstette (1993).
The view here is that the win-dow-based method may work to some extent, butthat many of the word co-occurrences in a windoware just incidental and add noise to the significantword pairs.
A simple method to reduce this problemcould be to introduce a threshold for the minimumnumber of co-occurrences; a more sophisticatedmethod is the use of a (shallow) parser.
Ruge(1992), who was the first to introduce this method,claims that only head-modifier relations, as knownfrom dependency grammar, should be considered.For example, if we consider the sentence ?Peterdrives the blue car?, then we should not count theco-occurrence of Peter and blue, because blue isneither head nor modifier of Peter.
Ruge developeda shallow parser that is able to determine the head-modifier relations in unrestricted English text with arecall of 85% and a precision of 86% (Ruge, 1995).Using this parser she extracted all head-modifierrelations from the 100 million words of the BritishNational Corpus.
Thus, the resulting co-occurrencematrix only contained the counts of the head-modi-fier relations.
The word similarities were computedfrom this matrix by using the cosine similaritymeasure.
Using this method, Ruge achieved anaccuracy of about 69% in the TOEFL synonymtask, which is equivalent to our results.Again, we need to emphasize that parametersother than the basic methodology could have influ-enced the result, so we need to be cautious with aninterpretation.
However, to us it seems that the viewthat some of the co-occurrences in corpora shouldbe considered as noise is wrong, or else if there issome noise it obviously cancels out over large cor-pora.
It would be interesting to know how a systemperformed that used all co-occurrences except thehead-modifier relations.
We tend to assume thatsuch a system would perform worse, so the parserselected the good candidates.
However, the experi-ment has not been done, so we cannot be sure.Although the shallow parsing could not improvethe results in this case, we nevertheless should pointout its virtues: It improves efficiency since it leads tosparser matrices.
It also seems to be able to separatethe relevant from the irrelevant co-occurrences.Third, it may be useful for determining the type ofrelationship between words (e.g., synonymy, an-tonymy, meronymy, hyponymy, etc., see Berland &Charniak, 1999).
Although this is not within thescope of this paper, it is very relevant for relatedtasks, for example, the automatic generation ofthesauri.3 Syntagmatic AssociationsSyntagmatic associations are words that frequentlyoccur together.
Therefore, an obvious approach toextract them from corpora is to look for word pairswhose co-occurrence is significantly larger thanchance.
To test for significance, the standard chi-square test can be used.
However, Dunning (1993)pointed out that for the purpose of corpus statistics,where the sparseness of data is an important issue, itis better to use the log-likelihood ratio.
It would thenbe assumed that the strongest syntagmatic associa-tion to a word would be that other word that gets thehighest log-likelihood score.Please note that this method is computationallyfar more efficient than the computation of paradig-matic associations.
For the computation of the syn-tagmatic associations to a stimulus word only thevector of this single word has to be considered,whereas for the computation of paradigmatic asso-ciations the vector of the stimulus word has to becompared to the vectors of all other words in thevocabulary.
The computation of syntagmatic asso-ciations is said to be of first-order type, whereas thecomputation of paradigmatic associations is ofsecond-order type.
Algorithms for the computationof first-order associations have been used in lexico-graphy for the extraction of collocations (Smadja,1993) and in cognitive psychology for the simula-tion of associative learning (Wettler & Rapp, 1993).3.1 Association NormsAs we did with the paradigmatic associations, wewould like to compare the results of our simulationto human performance.
However, it is difficult tosay what kind of experiment should be conducted toobtain human data.
As with the paradigmatic asso-ciations, we decided not to conduct our own ex-periment but to use the Edinburgh Associative The-saurus (EAT), a large collection of associationnorms, as compiled by Kiss et al (1973).
Kiss pre-sented lists of stimulus words to human subjects andasked them to write after each word the first wordthat the stimulus word made them think of.
Table 2gives some examples of the associations the subjectscame up with.As can be seen from the table, not all of theassociations given by the subjects seem to be of syn-tagmatic type.
For example, the word pairs blue ?black or cold ?
hot are clearly of paradigmatic type.This observation is of importance and will be dis-cussed later.blue cold fruit green tobacco whiskeysky hot apple grass smoke drinkblack ice juice blue cigarette gingreen warm orange red pipe bottlered water salad yellow poach sodawhite freeze machine field road ScotchTable 2: Some sample associations from the EAT.3.2 ComputationFor the computation of the syntagmatic associationswe used the same corpus as before, namely theBritish National Corpus.
In a preliminary experi-ment we tested if there is a correlation between theoccurrence of a stimulus word in the corpus and theoccurrence of the most frequent associative responseas given by the subjects.
For this purpose, we se-lected 100 stimulus/response pairs and plotted a barchart from the co-occurrence data (see figure 1).
Inthe bar chart, the x-axis corresponds to the distanceof the response word from the stimulus word (meas-ured as the number of words separating them), andthe y-axis corresponds to the occurrence frequencyof the response word in a particular distance fromthe stimulus word.
Please note that for the purposeof plotting this bar chart, function words have beentaken into account.Figure 1: Occurrence frequency H of a response word ina particular distance A from the corresponding stimulusword (averaged over 100 stimulus/response pairs).As can be seen from the figure, the closer we get tothe stimulus word, the more likely it is that we findan occurrence of its strongest associative response.Exceptions are the positions directly neighboring thestimulus word.
Here it is rather unlikely to find theresponse word.
This observation can be explainedby the fact that content words are most often sepa-rated by function words, so that the neighboringpositions are occupied by function words.Now that it has been shown that there is somerelationship between human word associations andword co-occurrences, let us briefly introduce ouralgorithm for extracting word associations fromtexts.
Based on a window size of ?20 words, wefirst compute the co-occurrence vector for a givenstimulus word, thereby eliminating all words with acorpus frequency of less than 101.
We then applythe log-likelihood test to this vector.
According toLawson & Belica1 the log-likelihood ratio can becomputed as follows: Given the word W, for eachco-occurring word S, its window frequency A, itsresidual frequency C in the reference corpus, theresidual window size B and the residual corpus sizeD are stored in a 2 by 2 contingency table.S ?S TotalW A B A+B?W C D C+DTotal A+C B+D NThen the log-likelihood statistics are calculated:))log()()log()()log()()log()(logloglogloglog(2DCDCDBDBCACABABANNDDCCBBAAG++?++?++?++?++++=Finally, the vocabulary is ranked according to de-scending values of G as computed for each word.The word with the highest value is considered to bethe primary associative response.3.3 ResultsIn table 3 a few sample association lists as predictedby our system are listed.
They can be compared tothe human associative responses given in table 2.The valuation of the predictions has to take intoaccount that association norms are conglomerates ofthe answers of different subjects that differ consid-erably from each other.
A satisfactory predictionwould be proven if the difference between the pre-1Handout at GLDV Meeting, Frankfurt/Main 1999.dicted and the observed responses were about equalto the difference between an average subject and therest of the subjects.
This is actually the case.
For 27out of the 100 stimulus words the predicted re-sponse is equal to the observed primary response.This compares to an average of 28 primary re-sponses given by a subject in the EAT.
Otherevaluation measures lead to similar good results(Wettler & Rapp, 1993; Rapp, 1996).blue cold fruit green tobacco whiskeyred hot vegetable red advertising drinkeyes water juice blue smoke Jessesky warm fresh yellow ban bottlewhite weather tree leaves cigarette Irishgreen winter salad colour alcohol pourTable 3: Results with the co-occurrence-based approach.We conclude from this that our method seems to bewell suited to predict the free word associations asproduced by humans.
And as human associationsare not only of syntagmatic but also of paradigmatictype, so does the co-occurrence-based method pre-dict both types of associations rather well.
In theranked lists produced by the system we find a mix-ture of both types of associations.
However, for agiven association there is no indication whether it isof syntagmatic or paradigmatic type.We suggest a simple method to distinguish theparadigmatic from the syntagmatic associations.Remember that the 2nd-order approach described inthe previous section produced paradigmatic asso-ciations only.
So if we simply remove the wordsproduced by the 2nd-order approach from the wordlists obtained by the 1st-order approach, then thisshould give us solely syntagmatic associations.4 Comparison between Syntagmaticand Paradigmatic AssociationsTable 4 compares the top five associations to a fewstimulus words as produced by the 1st-order and the2nd-order approach.
In the list, we have printed inbold those 1st-order associations that are not amongthe top five in the second-order lists.
Further inspec-tions of these words shows that they are all syntag-matic associations.
So the method proposed seemsto work in principle.
However, we have not yet con-ducted a systematic quantitative evaluation.
Con-ducting a systematic evaluation is not trivial, sincethe definitions of the terms syntagmatic and para-digmatic as given in the introduction may not beprecise enough.
Also, for a high recall, the wordlists considered should be much longer than the topfive.
However, the further down we go in the rankedlists, the less typical are the associations.
So it is notclear where to automatically set a threshold.
We didnot further elaborate on this because for ourpractical work this issue was of lesser importance.Although both algorithms are based on word co-occurrences, our impression is that their strengthsand weaknesses are rather different.
So we see agood chance of obtaining an improved generator forassociations by combining the two methods.stimulus 1st-order 2nd-orderblue red redeyes greensky greywhite yellowgreen whitecold hot hotwater warmwarm dryweather drinkwinter coolfruit vegetable foodjuice flowerfresh fishtree meatsalad vegetablegreen red redblue blueyellow whiteleaves yellowcolour greytobacco advertising cigarettesmoke alcoholban coalcigarette importalcohol textilewhiskey drink whiskyJesse brandybottle champagneIrish lemonadepour vodkaTable 4: Comparison between 1st-order and 2nd-orderassociations.5 Discussion and ConclusionWe have described algorithms for the computationof 1st-order and 2nd-order associations.
The resultsobtained have been compared with the answers ofhuman subjects in the free association task and inthe TOEFL synonym test.
It could be shown thatthe performance of our system is comparable to theperformance of the subjects for both tasks.We observed that there seems to be some rela-tionship between the type of computation performed(1st-order versus 2nd-order) and the terms syntag-matic and paradigmatic as coined by de Saussure.Whereas the results of the 2nd-order computationare of paradigmatic type exclusively, those of the1st-order computation are a mixture of both syn-tagmatic and paradigmatic associations.
Removingthe 2nd-order associations from the 1st-order as-sociations leads to solely syntagmatic associations.We believe that the observed relation betweenour statistical models and the intuitions of de Saus-sure are not incidental, and that the striking similar-ity of the simulation results with the human associa-tions also has a deeper reason.
Our explanation forthis is that human associative behavior is governedby the law of association by contiguity, which iswell known from psychology (Wettler, Rapp &Ferber, 1993).
In essence, this means that in the pro-cess of learning or generating associations the hu-man mind seems to conduct operations that areequivalent to co-occurrence counting, to performingsignificance tests, or to computing vector similarities(see also Landauer & Dumais, 1997).
However,further work is required to find out to what extentother language-related tasks can also be explainedstatistically.AcknowledgementsThis research was supported by the DFG.
I wouldlike to thank Manfred Wettler and Gerda Ruge formany inspiring discussions.ReferencesAgarwal, R. (1995).
Semantic Feature Extraction fromTechnical Texts with Limited Human Intervention.Dissertation, Mississippi State University.Berland, M., Charniak, E. (1999).
Finding Parts in VeryLarge Corpora.
In: Proceedings of ACL 1999, Col-lege Park.
57?64.de Saussure, F. (1916/1996).
Cours de linguistique g?-n?rale.
Paris: Payot.Dunning, T. (1993).
Accurate methods for the statisticsof surprise and coincidence.
Computational Linguis-tics, 19(1), 61-74.Grefenstette, G. (1993).
Evaluation techniques forautomatic semantic extraction: comparing syntacticand window based approaches.
In: Proceedings of theWorkshop on Acquisition of Lexical Knowledge fromText, Columbus, Ohio.Grefenstette, G. (1994).
Explorations in Automatic The-saurus Discovery.
Dordrecht: Kluwer.Kiss, G.R., Armstrong, C., Milroy, R., Piper, J.
(1973).An associative thesaurus of English and its com-puter analysis.
In: A. Aitken, R. Beiley and N.Hamilton-Smith (eds.
): The Computer and LiteraryStudies.
Edinburgh: University Press.Landauer, T. K.; Dumais, S. T. (1997).
A solution toPlato?s problem: the latent semantic analysis theory ofacquisition, induction, and representation of know-ledge.
Psychological Review, 104(2), 211?240.Lin, D. (1998).
Automatic Retrieval and Clustering ofSimilar Words.
In: Proceedings of COLING-ACL1998, Montreal, Vol.
2, 768?773.Rapp, R. (1996).
Die Berechnung von Assoziationen.Hildesheim: Olms.Rapp, R. (1999).
Automatic identification of wordtranslation from unrelated English and German cor-pora.
In: Proceedings of ACL 1999, College Park.519?526.Ruge, G. (1992).
Experiments on Linguistically BasedTerm Associations.
Information Processing & Ma-nagement 28(3), 317?332.Ruge, G. (1995).
Wortbedeutung und Termassoziation.Hildesheim: Olms.Salton, G.; McGill, M. (1983).
Introduction to ModernInformation Retrieval.
New York: McGraw-Hill.Sch?tze, H. (1997).
Ambiguity Resolution in LanguageLearning: Computational and Cognitive Models.Stanford: CSLI Publications.Smadja, F. (1993).
Retrieving collocations from text:Xtract.
Computational Linguistics 19(1), 143?177.Wettler, M.; Rapp, R. (1993).
Computation of wordassociations based on the co-occurrences of words inlarge corpora.
In: Proceedings of the 1st Workshop onVery Large Corpora: Columbus, Ohio, 84?93.Wettler, M., Rapp, R., Ferber, R. (1993).
Freie Asso-ziationen und Kontiguit?ten von W?rtern in Texten.Zeitschrift f?r Psychologie, 201, 99?108.
