Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 45?48,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsEnforcing Transitivity in Coreference ResolutionJenny Rose Finkel and Christopher D. ManningDepartment of Computer ScienceStanford UniversityStanford, CA 94305{jrfinkel|manning}@cs.stanford.eduAbstractA desirable quality of a coreference resolutionsystem is the ability to handle transitivity con-straints, such that even if it places high like-lihood on a particular mention being corefer-ent with each of two other mentions, it willalso consider the likelihood of those two men-tions being coreferent when making a final as-signment.
This is exactly the kind of con-straint that integer linear programming (ILP)is ideal for, but, surprisingly, previous workapplying ILP to coreference resolution has notencoded this type of constraint.
We train acoreference classifier over pairs of mentions,and show how to encode this type of constrainton top of the probabilities output from ourpairwise classifier to extract the most probablelegal entity assignments.
We present resultson two commonly used datasets which showthat enforcement of transitive closure consis-tently improves performance, including im-provements of up to 3.6% using the b3 scorer,and up to 16.5% using cluster f-measure.1 IntroductionMuch recent work on coreference resolution, whichis the task of deciding which noun phrases, or men-tions, in a document refer to the same real worldentity, builds on Soon et al (2001).
They built adecision tree classifier to label pairs of mentions ascoreferent or not.
Using their classifier, they wouldbuild up coreference chains, where each mentionwas linked up with the most recent previous men-tion that the classifier labeled as coreferent, if sucha mention existed.
Transitive closure in this modelwas done implicitly.
If John Smith was labeledcoreferent with Smith, and Smith with Jane Smith,then John Smith and Jane Smith were also corefer-ent regardless of the classifier?s evaluation of thatpair.
Much work that followed improved upon thisstrategy, by improving the features (Ng and Cardie,2002b), the type of classifier (Denis and Baldridge,2007), and changing mention links to be to the mostlikely antecedent rather than the most recent posi-tively labeled antecedent (Ng and Cardie, 2002b).This line of work has largely ignored the implicittransitivity of the decisions made, and can result inunintuitive chains such as the Smith chain just de-scribed, where each pairwise decision is sensible,but the final result is not.Ng and Cardie (2002a) and Ng (2004) highlightthe problem of determining whether or not commonnoun phrases are anaphoric.
They use two clas-sifiers, an anaphoricity classifier, which decides ifa mention should have an antecedent and a pair-wise classifier similar those just discussed, whichare combined in a cascaded manner.
More recently,Denis and Baldridge (2007) utilized an integer lin-ear programming (ILP) solver to better combine thedecisions made by these two complementary clas-sifiers, by finding the globally optimal solution ac-cording to both classifiers.
However, when encodingconstraints into their ILP solver, they did not enforcetransitivity.The goal of the present work is simply to showthat transitivity constraints are a useful source ofinformation, which can and should be incorporatedinto an ILP-based coreference system.
For this goal,we put aside the anaphoricity classifier and focuson the pairwise classifier and transitivity constraints.We build a pairwise logistic classifier, trained on allpairs of mentions, and then at test time we use anILP solver equipped with transitivity constraints tofind the most likely legal assignment to the variableswhich represent the pairwise decisions.1 Our re-sults show a significant improvement compared tothe na?
?ve use of the pairwise classifier.Other work on global models of coreference (as1A legal assignment is one which respects transitive closure.45opposed to pairwise models) has included: Luo et al(2004) who used a Bell tree whose leaves representpossible partitionings of the mentions into entitiesand then trained a model for searching the tree; Mc-Callum and Wellner (2004) who defined several con-ditional random field-based models; Ng (2005) whotook a reranking approach; and Culotta et al (2006)who use a probabilistic first-order logic model.2 Coreference ResolutionFor this task we are given a document which is an-notated with a set of mentions, and the goal is tocluster the mentions which refer to the same entity.When describing our model, we build upon the no-tation used by Denis and Baldridge (2007).2.1 Pairwise ClassificationOur baseline systems are based on a logistic classi-fier over pairs of mentions.
The probability of a pairof mentions takes the standard logistic form:P (x?i,j?|mi,mj ; ?)
=(1 + e?f(mi,mj)??
)?1 (1)where mi and mj correspond to mentions i and jrespectively; f(mi,mj) is a feature function over apair of mentions; ?
are the feature weights we wishto learn; and x?i,j?
is a boolean variable which takesvalue 1 if mi and mj are coreferent, and 0 if they arenot.
The log likelihood of a document is the sum ofthe log likelihoods of all pairs of mentions:L(x|m; ?)
=?mi,mj?m2log P (x?i,j?|mi,mj; ?
)(2)where m is the set of mentions in the document, andx is the set of variables representing each pairwisecoreference decision x?i,j?.
Note that this model isdegenerate, because it assigns probability mass tononsensical clusterings.
Specifically, it will allowx?i,j?
= x?j,k?
= 1 while x?i,k?
= 0.Prior work (Soon et al, 2001; Denis andBaldridge, 2007) has generated training data forpairwise classifiers in the following manner.
Foreach mention, work backwards through the preced-ing mentions in the document until you come to atrue coreferent mention.
Create negative examplesfor all intermediate mentions, and a positive exam-ple for the mention and its correct antecedent.
Thisapproach made sense for Soon et al (2001) becausetesting proceeded in a similar manner: for each men-tion, work backwards until you find a previous men-tion which the classifier thinks is coreferent, adda link, and terminate the search.
The COREF-ILPmodel of Denis and Baldridge (2007) took a dif-ferent approach at test time: for each mention theywould work backwards and add a link for all pre-vious mentions which the classifier deemed coref-erent.
This is equivalent to finding the most likelyassignment to each x?i,j?
in Equation 2.
As noted,these assignments may not be a legal clustering be-cause there is no guarantee of transitivity.
The tran-sitive closure happens in an ad-hoc manner afterthis assignment is found: any two mentions linkedthrough other mentions are determined to be coref-erent.
Our SOON-STYLE baseline used the sametraining and testing regimen as Soon et al (2001).Our D&B-STYLE baseline used the same test timemethod as Denis and Baldridge (2007), however attraining time we created data for all mention pairs.2.2 Integer Linear Programming to EnforceTransitivityBecause of the ad-hoc manner in which transitiv-ity is enforced in our baseline systems, we do notnecessarily find the most probable legal clustering.This is exactly the kind of task at which integerlinear programming excels.
We need to first for-mulate the objective function which we wish theILP solver to maximize at test time.2 Let p?i,j?
=log P (x?i,j?|mi,mj ; ?
), which is the log probabil-ity that mi and mj are coreferent according to thepairwise logistic classifier discussed in the previoussection, and let p??i,j?
= log(1 ?
p?i,j?
), be the logprobability that they are not coreferent.
Our objec-tive function is then the log probability of a particu-lar (possibly illegal) variable assignment:max?mi,mj?m2p?i,j?
?x?i,j??
p??i,j?
?
(1?x?i,j?)
(3)We add binary constraints on each of the variables:x?i,j?
?
{0, 1}.
We also add constraints, over eachtriple of mentions, to enforce transitivity:(1 ?
x?i,j?)
+ (1 ?
x?j,k?)
?
(1 ?
x?i,k?)
(4)2Note that there are no changes from the D&B-STYLE base-line system at training time.46This constraint ensures that whenever x?i,j?
=x?j,k?
= 1 it must also be the case that x?i,k?
= 1.3 ExperimentsWe used lp solve3 to solve our ILP optimizationproblems.
We ran experiments on two datasets.
Weused the MUC-6 formal training and test data, aswell as the NWIRE and BNEWS portions of the ACE(Phase 2) corpus.
This corpus had a third portion,NPAPER, but we found that several documents wheretoo long for lp solve to find a solution.4We added named entity (NE) tags to the data us-ing the tagger of Finkel et al (2005).
The ACE datais already annotated with NE tags, so when they con-flicted they overrode the tags output by the tagger.We also added part of speech (POS) tags to the datausing the tagger of Toutanova et al (2003), and usedthe tags to decide if mentions were plural or sin-gular.
The ACE data is labeled with mention type(pronominal, nominal, and name), but the MUC-6 data is not, so the POS and NE tags were usedto infer this information.
Our feature set was sim-ple, and included many features from (Soon et al,2001), including the pronoun, string match, definiteand demonstrative NP, number and gender agree-ment, proper name and appositive features.
We hadadditional features for NE tags, head matching andhead substring matching.3.1 Evaluation MetricsThe MUC scorer (Vilain et al, 1995) is a popularcoreference evaluation metric, but we found it to befatally flawed.
As observed by Luo et al (2004),if all mentions in each document are placed into asingle entity, the results on the MUC-6 formal testset are 100% recall, 78.9% precision, and 88.2%F1 score ?
significantly higher than any publishedsystem.
The b3 scorer (Amit and Baldwin, 1998)was proposed to overcome several shortcomings ofthe MUC scorer.
However, coreference resolutionis a clustering task, and many cluster scorers al-ready exist.
In addition to the MUC and b3 scorers,we also evaluate using cluster f-measure (Ghosh,2003), which is the standard f-measure computedover true/false coreference decisions for pairs of3From http://lpsolve.sourceforge.net/4Integer linear programming is, after all, NP-hard.mentions; the Rand index (Rand, 1971), which ispairwise accuracy of the clustering; and variationof information (Meila, 2003), which utilizes the en-tropy of the clusterings and their mutual information(and for which lower values are better).3.2 ResultsOur results are summarized in Table 1.
We showperformance for both baseline classifiers, as well asour ILP-based classifier, which finds the most prob-able legal assignment to the variables representingcoreference decisions over pairs of mentions.
Forcomparison, we also give the results of the COREF-ILP system of Denis and Baldridge (2007), whichwas also based on a na?
?ve pairwise classifier.
Theyused an ILP solver to find an assignment for the vari-ables, but as they note at the end of Section 5.1, it isequivalent to taking all links for which the classifierreturns a probability ?
0.5, and so the ILP solver isnot really necessary.
We also include their JOINT-ILP numbers, however that system makes use of anadditional anaphoricity classifier.For all three corpora, the ILP model beat bothbaselines for the cluster f-score, Rand index, andvariation of information metrics.
Using the b3 met-ric, the ILP system and the D&B-STYLE baselineperformed about the same on the MUC-6 corpus,though for both ACE corpora, the ILP system wasthe clear winner.
When using the MUC scorer, theILP system always did worse than the D&B-STYLEbaseline.
However, this is precisely because thetransitivity constraints tend to yield smaller clusters(which increase precision while decreasing recall).Remember that going in the opposite direction andsimply putting all mentions in one cluster producesa MUC score which is higher than any in the table,even though this clustering is clearly not useful inapplications.
Hence, we are skeptical of this mea-sure?s utility and provide it primarily for compari-son with previous work.
The improvements fromthe ILP system are most clearly shown on the ACENWIRE corpus, where the b3 f-score improved 3.6%,and the cluster f-score improved 16.5%.4 ConclusionWe showed how to use integer linear program-ming to encode transitivity constraints in a corefer-47MUC SCORER b3 SCORER CLUSTERMODEL P R F1 P R F1 P R F1 RAND VOIMUC-6D&B-STYLE BASELINE 84.8 59.4 69.9 79.7 54.4 64.6 43.8 44.4 44.1 89.9 1.78SOON-STYLE BASELINE 91.5 51.5 65.9 94.4 46.7 62.5 88.2 31.9 46.9 93.5 1.65ILP 89.7 55.1 68.3 90.9 49.7 64.3 74.1 37.1 49.5 93.2 1.65ACE ?
NWIRED&B COREF-ILP 74.8 60.1 66.8 ?
?
?
?D&B JOINT-ILP 75.8 60.8 67.5 ?
?
?
?D&B-STYLE BASELINE 73.3 67.6 70.4 70.1 71.4 70.8 31.1 54.0 39.4 91.7 1.42SOON-STYLE BASELINE 85.3 37.8 52.4 94.1 56.9 70.9 67.7 19.8 30.6 95.5 1.38ILP 78.7 58.5 67.1 86.8 65.2 74.5 76.1 44.2 55.9 96.5 1.09ACE ?
BNEWSD&B COREF-ILP 75.5 62.2 68.2 ?
?
?
?D&B JOINT-ILP 78.0 62.1 69.2 ?
?
?
?D&B-STYLE BASELINE 77.9 51.1 61.7 80.3 64.2 71.4 35.5 33.8 34.6 0.89 1.32SOON-STYLE BASELINE 90.0 43.2 58.3 95.6 58.4 72.5 83.3 21.5 34.1 0.93 1.09ILP 87.8 46.8 61.1 93.5 59.9 73.1 77.5 26.1 39.1 0.93 1.06Table 1: Results on all three datasets with all five scoring metrics.
For VOI a lower number is better.ence classifier which models pairwise decisions overmentions.
We also demonstrated that enforcing suchconstraints at test time can significantly improve per-formance, using a variety of evaluation metrics.AcknowledgmentsThanks to the following members of the StanfordNLP reading group for helpful discussion: SharonGoldwater, Michel Galley, Anna Rafferty.This paper is based on work funded by the Dis-ruptive Technology Office (DTO) Phase III Programfor Advanced Question Answering for Intelligence(AQUAINT).ReferencesB.
Amit and B. Baldwin.
1998.
Algorithms for scoringcoreference chains.
In MUC7.A.
Culotta, M. Wick, and A. McCallum.
2006.
First-order probabilistic models for coreference resolution.In NAACL.P.
Denis and J. Baldridge.
2007.
Joint determination ofanaphoricity and coreference resolution using integerprogramming.
In HLT-NAACL, Rochester, New York.J.
Finkel, T. Grenager, and C. Manning.
2005.
Incorpo-rating non-local information into information extrac-tion systems by Gibbs sampling.
In ACL.J.
Ghosh.
2003.
Scalable clustering methods for datamining.
In N. Ye, editor, Handbook of Data Mining,chapter 10, pages 247?277.
Lawrence Erlbaum Assoc.X.
Luo, A. Ittycheriah, H. Jing, N. Kambhatla, andS.
Roukos.
2004.
A mention-synchronous corefer-ence resolution algorithm based on the Bell tree.
InACL.A.
McCallum and B. Wellner.
2004.
Conditional modelsof identity uncertainty with application to noun coref-erence.
In NIPS.M.
Meila.
2003.
Comparing clusterings by the variationof information.
In COLT.V.
Ng and C. Cardie.
2002a.
Identifying anaphoric andnon-anaphoric noun phrases to improve coreferenceresolution.
In COLING.V.
Ng and C. Cardie.
2002b.
Improving machine learn-ing approaches to coreference resolution.
In ACL.V.
Ng.
2004.
Learning noun phrase anaphoricity to im-prove coreference resolution: issues in representationand optimization.
In ACL.V.
Ng.
2005.
Machine learning for coreference resolu-tion: From local classification to global ranking.
InACL.W.
M. Rand.
1971.
Objective criteria for the evaluationof clustering methods.
In Journal of the American Sta-tistical Association, 66, pages 846?850.W.
Soon, H. Ng, and D. Lim.
2001.
A machine learningapproach to coreference resolution of noun phrases.
InComputational Linguistics, 27(4).K.
Toutanova, D. Klein, and C. Manning.
2003.
Feature-rich part-of-speech tagging with a cyclic dependencynetwork.
In HLT-NAACL 2003.M.
Vilain, J. Burger, J. Aberdeen, D. Connolly, andL.
Hirschman.
1995.
A model-theoretic coreferencescoring scheme.
In MUC6.48
