"No Better, but no Worse, than People"David D. McDonaldUniversity of Massachusetts at Amherst1.
Generation versus UnderstandingNatural anguage understanding and natural language generation could employ the sameknowledge of language.
1 They could even represent their knowledge in the same way,provided that it was a nonprocedural encoding.
2 However, the two processes that draw on theknowledge cannot be the same because of the radical differences in information flow: Decision-making is a radically different kind of process than hypothesis maintainance.
Understandingproceeds from a sequentially scanned text to content and intentions; generation does just theopposite.
Understanding processes must cope with ambiguity and underspecification,problems that do not arise in generation (i.e.
an audience receives more information fromsituationally controlled inferences than from the literal text).
Generators on the other hand muston some basis choose from an oversupply of syntactic and lexical mechanisms all the whileremaining consistent with the constraints imposed by grammaticality, linear order, and stylisticconvention--a classic planning problem that now invites careful solutions closely tuned to thespecial demands of natural language.Neither process is particularly more heuristic in its judgements han the other.
If generationappears more algorithmic, it is because of the weakness of the present models of intentionality,situation, lexical sources, and especially audience reactions.
People have no assurance that heir1 Presently they don't--generation uses more.
Generation demands knowledge of the conventions andheuristics of language use, but understanding systems today do not attempt to recover any such assessmentsof why speakers ay what they do in the particular manner that they do.
They don't have to--the programsthey are presently working for wouldn't appreciate he information if they did.2 My own candidate for the neutral, shared encoding would be a catalogue of all the minimal elementarysurface structure trees of the language, plus the rules that govern how they can be combined, e.g.
a TreeAdjoining Grammar.
Paired with each tree fragment would be a mapping function associating it with thesituations in which its use was appropriate for the individual speaker.
Besides my own use of TAGs(McDonald & Pustejovsky, 1985), this framework is a reasonable description of at least he Phran and Phredsystems at Berkeley (Jacobs, 1985), and Doug Appelt's (1985) use of functional unification grammar.221choice of what to say will be effective; when programs have richer models they won't be certaineither.
For all but the most mundain tasks, the complexity of the circumstances will precludetimed procedures.
Instead, our programs will have to do what we appear to: make their choicesheuristically, anticipating how the rest of the discourse will go if their assumptions are correct,and being prepared to adjust if it turns out that hey are not.Any conclusion other than that the same knowledge structures underly both understandingand generation would be a drastic philosophic jump from our common view of language as aninterpersonal medium and an interface to thought.
Any difficulties we presently have in makingthe same structures "go both ways" reflects weaknesses in our conceptual designs rather thanfacts about people.
In particular, present arget representations for understanding areimpoverished: inverting them leads to badly underspecified messages since they contain noinformation about deliberately adopted perspectives orconnotated information (such as newnessor value judgements), and very little about what the original speaker's goals were.2.
Generation is special.To do good work in generation one is forced to come to grips with problems that othertasks are today able to ignore.
Three examples: One cannot work on generation and ignoresyntax.
One cannot avoid accounting for the control of variation in linguistic form throughappeals to synonomy or cannonical form.
One cannot passively accept the semanticrepresentations of one's colleagues' knowledge-based reasoning systems without firstdetermining that they are notationally and epistemologically able to support he distinctions thatlanguage makes.It is not clear to me that these are the sorts of issues that will draw the otherwise reluctantlinguist o consider AI, but they without question draw the AI people who work on them tolinguistics.
Proficiency in the technicalities of syntax and morphology is obligatory ingeneration research.
More importantly, generation people must have a linguist's kill at arguingthe consequences of alternative theoretical decisions: Working as we do from empirically222unestablished models of intention and knowedge out to text, we have to justify our designsusing indirect evidence and comparative r asoning, something that linguists are well trained for.This difficulty is in other espects a great advantage (one that linguists in my experiencewell appreciate) when we are working on today's cutting edge problems in computationallinguistics, such as the structure of discourse or the signaling of intended inferences and theirrelationship to underlying knowledge of the world and social behavior.
Our established toolssuch as example-driven comparative analysis do not fare well on these problems because of theenormous number of factors that contribute to them.
Descriptive theories of underlying abstractstructure are unsatisfying because the abstractions are slippery to evaluate.What they need is the synthetic approach provided by generation.
The generation processconverts abstract structures into concrete texts whose properties we can evaluate mpirically.Theories of discourse now can stand or fall on whether they lead to effective conversations,theories of inferencing on whether texts based on them do evoke the intended conclusions.3.
Two non-prob lemsThe possibility of a program somehow generating things that no human could understand isa red herring.3 People say things all the time that other people don't understand, yet we don'tthink anything unusual is happening.
Usually the audience fails to make an expected inferencerather than misunderstand some literal part of the utterance, a problem that can happen quiteeasily when the speaker misjudges what the audience already knows, or the speaker thinks thatthey share some judgement orcontext when they do not.
Another source of the problem comesfrom the speaker thinking that a certain turn of phrase should signal a certain inference but theaudience is opaque to that signal.3 Since programs wouldn't talk to us if they didn't need to communicate, saying things to us that we don'tunderstand would just be failing to achieve their own goals.
Perhaps they might choose to talk this way toeach other (though why should they, since given any commonality in their internal designs, telepathy wouldbe much simpler and more satisfying), but if we give them any sensitivity otheir audience's reactions (andhow could communication be effective without i ) they will quickly realize that we're missing the point ofmost of what hey're saying to us and change their techniques.223The very same mistake could be made by a program--we cannot program them to besuperhumanly aware of their audience.
The only protection is incorporating into languageinterfaces the same kind of sensitivity to later audience reactions that we have ourselves.
Weknow what the effect of following our inferences should be on our audiences, and we can sensewhen they have missed our intent.
We especially know how to feed back a communicationsfailure onto our own generation strategies so that we will make different choices the next timewe need to get across a similar idea.
We should make our machines able to do the same.The problem of how best to match a system's input and output language abilities is likely toturn out to be a red herring as well, one that will go away naturally as soon as ourunderstanding systems become as syntactically and lexically competent as our generators.
4 Theproblem is that presently if the generator produces a more sophisticated construction than theunderstander can parse or uses a word that it does not know, then the human user, mimicingwhat the generator has done, will be frustrated when he turns out not to be understood.If this were the only difficulty, then it could be solved by straightforward softwareengineering: consistency tools would force one to drop items from the generator's repertoirethat the understander did not know.
Unfortunately the problem goes deeper than that.
Themismatch is not the issue, since people's abilities do not match either: we all can understandmarkedly more than we would ever say.
The real problem for a non-research interface is--directqueries for literal information aside--that machine understanding abilities are so far below thehuman level that any facile, inference motivating output from the generator is going to suggestto the user that the system will understand things that it cannot.Because of this, I personally would never include language input in a non-researchinterface today.
Interactive graphics and menu facilities do not suffer from the ambiguity andscope of inferencing problems faced by language, and give a realistic picture of what a systemis actually able to comprehend.
Interfaces based on a "graphics in, graphics and speech out"4 It is trivial to specify a linguistically complex phrase and have a generator utter it by rote.
Such canned ortemplate-based text is often the best route to take in a practical interface.
If the programmer is sure that thesituation warrants the phrase then it can safely be used, even though there may be no explicit model withinthe system from which the phrase could have been deliberately composed.224paradigm have not been given enough study by the language and communications researchcommunity, and are likely to be a much better match to the deliberative and intentional bilitiesof the programs we can experiment with today.4.
Controling Decision-makingThere are volumes to be said on how one could or should control for syntactic and lexicalchoice--this is the primary question that any computational theory of generation answers.Rather than attempt to summarize my position in the little space that remains, let me point outtwo issues that I believe distinguish much of the work presently going on; for a largerdiscussion of these see McDonald, Vaughan, and Pustejovsky (1987).The first issue is whether one attempts to make psychological c aims with the form andoperation of the generator.
This is the more demanding road to take.
It may also turn out to bethe only one that provides for continuous extension and elaboration.
Language, like vision,may be so tied up with the nature of the human mind and its computational properties that nodesign that goes against those properties will ever be more than a special purpose hack.
Makingclaims with a computational process requires one to take exceptional discipline in designing theoperations and representations it will use.
Much of the explanatory load will be taken on by therestrictions on the mechanism's behavior, and these can be easily diluted by the kinds ofprogramming conveniences that make a generator easier to engineer.
Adopting a psychologicalpoint of view can thus retard efforts to make a generator more competent.The second issue is how much generation knowledge is to be found in the non-linguistic,"underlying program" in whose service the generator is operating.
The more that we take to bethere, the greater the burder we place on our knowledge-based system colleagues to make surethat it is included; however our theories may have very good reasons for requiring it.
Thisknowledge might be the direct encoding of rhetorically relevant structural relations: How deeplydo we believe that the notions of "compare and contrast" are to be found in the mind or shouldbe found in a program?
It might be of lexical identities: Are the conceptual primitives of theunderlying program fine-grained and closely matched to real words, or large-grained and225abstract?
It might also be in the modularity of the underlying system's information: Is itpropositional nd easily mapped onto kernel clauses and noun phrases, or does it have somedrastically different organization?Generation research today has the lion's share of the important computational linguisticsproblems.
As more and more people work in it, it will quickly become the cutting edge, forcingextensions on understanding and knowledge representation if they are to match it as a source ofinsight into the nature of language and thought in the human mind.
There is no appropriate goalfor generation research short of matching human performance, part of which entails coming tounderstand the limits on that performance.
We don't really know how good people are at usinglanguage; our experiments with mechanical speakers may someday tell us.5.
Referenceso Appelt, Doug (1985) Planning English Sentences, Cambridge University Press.Jacobs, Paul (1985) "PHRED: A generator for natural anguage interfaces", BerkeleyComputer Science Department TR 85/198.McDonald, David & Pustejovsky, James (1985) "TAGs as a Grammatical Formalism forGeneration", proc.
ACL-85, Chicago, 1985, 94-103., ,  & Vaughan, Marie (1987) "Factors Contributing to Efficiency in NaturalLanguage Generation", in Kempen (ed) Papers from the Third InternationalWorkshop on Language Generation, Martinus Nijhoff Press (Kluwer), TheNetherlands.226
