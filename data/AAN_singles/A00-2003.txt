A Probabilistic Genre-Independent Model of PronominalizationMichael StrubeEuropean Media Laboratory GmbHVilla BoschSchlol3-Wolfsbrunnenweg 3369118 Heidelberg, GermanyMichael.
St rube@eml .vil la-bosch.
deMaria WoltersInst.
f. Kommunikationsforschung u. PhonetikUniversitiit BonnPoppelsdorfer Allee 4753115 Bonn, Germanywolters@ikp.uni -bonn.deAbstractOur aim in this paper is to identify genre-independent factors that influence the decision topronominalize.
Results based on the annotation oftwelve texts from four genres show that only a fewfactors have a strong influence on pronominaliza-tion across genres, i.e.
distance from last mention,agreement, and form of the antecedent.
Finally, wedescribe aprobabilistic model of pronominalizationderived from our data.1 IntroductionGenerating adequate referring expressions i an ac-tive research topic in Natural Language Generation.Adequate referring expressions are those that en-able the user to quickly and unambiguously identifythe discourse ntity that the expression co-specifieswith.
In this paper, we concentrate on an importantaspect of that question, which has received less at-tention than the question of anaphora resolution indiscourse interpretation, i.e., when is it feasible topronominalize?Our aim is to identify the central factors that in-fluence pronominalization across genres.
Section 2motivates and presents the factors that were investi-gated in this study: distance from last mention, par-allelism, ambiguity, syntactic function, agreement,sortal class, syntactic function of the antecedent andform of the antecedent.
Our analyses are based ona corpus of twelve texts from four different genreswith a total of more than 24,000 words and 7126referring expressions (Section 3).
The results ofthe statistical analyses are summarized in Section4.
There are strong statistical associations betweeneach of the factors and pronominalization.
Onlywhen we combine them into a probabilistic modelwe can identify those factors whose contributionis really important, i.e.
distance from last mention,agreement, and to a certain degree form of the an-tecedent.
Since these factors can be annotated tel-atively cheaply, we conclude that it is possible todevelop reasonable statistical pronominalization al-gorithms.2 Factors in Pronoun Generation2.1 Previous WorkLately, a number of researchers have done corpus-based work on NP generation and pronoun resolu-tion, and a number of studies have found differencesin the frequency of both personal and demonstrativepronouns across genres.
However, none of thesestudies compares the influence of different factorson pronoun generation across genres.Recently, Poesio et al (1999) have described acorpus-based approach to statistical NP generation.While they ask the same question as previous re-searchers (e.g.
Dale (1992)), their methods differfrom traditional work on NP generation.
Poesioet al (1999) use two kinds of factors: (1) factorsrelated to the NP under consideration such as agree-ment information, semantic factors, and discoursefactors, and (2) factors related to the antecedent,such as animacy, clause type, thematic role, proxim-ity, etc.
Poesio et al (1999) report hat they were notable to annotate many of these factors reliably.
Onthe basis of these annotations, they constructed de-cision trees for predicting surface forms of referringexpressions based on these factors - with good re-sults: all 28 personal pronouns in their corpus weregenerated correctly.
Unfortunately, they do not eval-uate the contribution of each of these factors, so wedo not know which ones are important.Work on corpus-based approaches to anaphoraresolution is more numerous.
Ge et al (1998)describe a supervised probabilistic pronoun resolu-tion algorithm which is based on complete syntac-tic information.
The factors they use include dis-tance from last mention, syntactic function and con-text, agreement information, animacy of the refer-ent, a simplified notion of selectional restrictions,18AgreeSynClassSynAnteFormAnteDistDist4ParAmbigAgreement in person, gender, and numberSyntactic functionSortal Class (cf.
Tab.
2)Syntactic function of antecedent.
"F" for first mention, "N" for deadendForm of antecedent (pers.
pron., poss.pron., def.
NP, indef.
NP, proper name)Distance to last mention in unitsDist reduced to 4 values (deadend,Dist=0, Dist= 1, Dist>=2)Parallelism (Syn=SynAnte)Number of competing discourse ntitiesTable 1: Overview of factorsand the length of the coreference chain.
Cardie &Wagstaff (1999) describe an unsupervised algorithmfor noun phrase coreference resolution.
Their fac-tors are taken from Ge et al (1998), with two excep-tions.
First, they replace complete syntactic infor-mation with information about NP bracketing.
Sec-ond, they use the sortal class of the referent whichthey determine on the basis of WordNet (Fellbaum,1998).There has been no comparison between corpus-based approaches for anaphora resolution and moretraditional algorithms based on focusing (Sidner,1983) or centering (Grosz et al, 1995) except forAzzam et al (1998).
However, their comparisonis flawed by evaluating a syntax-based focus algo-rithm on the basis of insufficient syntactic informa-tion.
For pronoun generation, the original centeringmodel (Grosz et al, 1995) provides a rule which issupposed to decide whether a referring expressionhas to be realized as a pronoun.
However, this ruleapplies only to the referring expression which is thebackward-looking center (Cb) of the current utter-ance.
With respect o all other referring expressionin this utterance centering is underspecified.Yeh & Mellish (1997) propose a set of hand-crafted rules for the generation of anaphora (zeroand personal pronouns, full NPs) in Chinese.
How-ever, the factors which appear to be important intheir evaluation are similar to factors describedby authors mentioned above: distance, syntacticconstraints on zero pronouns, discourse structure,salience and animacy of discourse ntities.2.2 Our FactorsThe factors we investigate in this paper only rely onannotations of NPs and their co-specification rela-tions.
We did not add any discourse structural anno-tation, because (1) the texts are extracts from largertexts which are not available to us, and (2) we havenot yet found a labelling scheme for discourse struc-ture that has an inter-coder reliability comparable tothe MUC coreference annotation scheme.Based on our review of the literature and relevantwork in linguistics (for sortal class, mainly Fraurud(1996) and Fellbaum (1998)), we have chosen thenine factors listed in Table 1.
Methodologically, wedistinguish two kinds of factors:NP-level factors are independent from co-specification relations.
They depend on thesemantics of the discourse entity or on discourseinformation supplied for the NP generation algo-rithm by the NLG system.
Typical examples areNP agreement by gender, number, person and case,the syntactic function of the NP (subject, object,PP adjunct, other), the sortal class of the discourseentity to which an NP refers, discourse structure, ortopicality of the discourse ntities.
In this paper, wefocus on the first three factors, agreement (Agree),syntactic function (Syn), and sortal class (Class).Since we are using syntactically annotated atain the Penn Treebank-II format, the syntactic func-tion of an NP was derived from these annotations.Agreement for gender, number, and person was la-belled by hand.
Since English has almost no nomi-nal case morphemes, case was not annotated.Sortal classes provide information about the dis-course entity that a referring expression evokes oraccesses.
The classes, summarized in Table 2, werederived from EuroWordNet BaseTypes (Vossen,1998) and are defined extensionally on the basisof WordNet synsets.
Their selection was motivatedby two main considerations: all classes hould oc-cur in all genres, and the number of classes houldbe as small as possible in order to avoid problemswith sparse data.
Four classes, State, Event, Action,and Property, cover different types of situations,two cover spatiotemporal characteristics of situa-tions (Loc/Time).
The four remaining classes coverthe two dimensions "concrete vs. abstract (Con-cept)" and "human (Pers) vs. non-human (PhysObj)vs. institutionalised groups of humans (Group)".Since we are only interested in the decisionwhether to employ pronouns rather than full NPsand less in the form of the NP itself, and since ourmethodology is based on corpus annotation, we didnot take into account more formal semantic ate-gories such as kinds vs. individuals.Co-specification-level factors depend on infor-mation about sequences of referring expressions19PersonGroupPhysObjConceptLocTimeEventActionStatePropertyone or more human beingsinstitutionalized group of human beingsphysical objectabstract onceptgeographical locationdate, time spansth.
which takes place in space and timesth.
which is donestate of affairs, feeling .
.
.
.characteristic orattribute of sth.Table 2: Overview of Sortal Classes with roughcharacterizations of relevant synsetswhich co-specify with each other.
Such a sequenceconsists of all referring expressions that evoke or ac-cess the same discourse ntity.
In this paper, we usethe following factors from the literature: distanceto last mention (Dist and Dist4), ambiguity (Am-big), parallelism (Par), form of the antecedent (For-mAnte), and syntactic function of the antecedent(SynAnte).
We also distinguish between discourseentities that are only evoked once, deadend entities,and entities that are accessed repeatedly.Parallelism is defined on the basis of syntacticfunction: a referring expression and its antecedentare parallel if they have the same syntactic function.For calculating distance and ambiguity, we seg-mented the texts into major clause units (MCUs).Each MCU consists of a major clause C plusany subordinate clauses and any coordinated majorclauses whose subject is the same as that of C andwhere that subject has been elided.Dist provides the number of MCUs between thecurrent and the last previous mention of a discourseentity.
When an entity is evoked for the first time,Dist is set to "D".
Dist4 is derived from Dist by as-signing the fixed distance 2 to all referring expres-sions whose antecedent is more than 1 MCU away.Ambiguity is defined as the number of all discourseentities with the same agreement features that occurin the previous unit or in the same unit before thecurrent referring expression.3 DataOur data consisted of twelve (plus two) texts fromthe Brown corpus and the corresponding part-of-speech and syntactic annotations from the PennTreebank (LDC, 1995).
The texts were selectedbecause they contained relatively little or no directspeech; segments of direct speech pose problems forboth pronoun resolution and generation because ofthe change in point of view.
Morpho-syntactic in-formation such as markables, part-of-speech labels,grammatical role labels, and form of referring ex-pression were automatically extracted from the ex-isting Treebank annotations.The texts come from four different genres: Popu-lar Lore (CF), Belles Lettres (CG), Fiction/General(CK), and Fiction/Mystery (CL).
The choice ofgenres was dictated by the availability of detailedTreebank-II parses.
Table 3 shows that the distri-bution of referring expressions differs considerablybetween genres.The texts from the two non-narrative types, CFand CG, contain far more discourse ntities andfar less pronouns than the narrative genres CK andCL.
The high number of pronouns in CK and CLis partly due to the fact that in one text from eachgenre, we have a first person singular narrator.
CKpatterns with CF and CG in the average numberof MCUs; the sentences in the sample from mys-tery fiction are shorter and arguably less complex.CL also has disproportionally few deadend refer-ents.
The high percentage of deadend referents inCK is due to the fact that two of the texts deal withrelationship between two people.
These four dis-course referents account for the 4 longest corefer-ence chains in CK (85, 96, 109, and 127 mentions).Two annotators (the authors, both trained lin-guists), hand-labeled the texts with co-specificationinformation based on the specifications for the Mes-sage Understanding Coreference task (Hirschman& Chinchor (1997); for theoretical reasons, we didnot mark reflexive pronouns and appositives as co-specifying).
The MCUs were labelled by the sec-ond author.
All referring expressions were anno-tated with agreement and sortal class information.Labels were placed using the GUI-based annotationtool REFEREE (DeCristofaro et al, 1999).The annotators developed the Sortal Class anno-tation guidelines on the basis of two training texts.Then, both labellers annotated two texts from eachgenre independently (eight in total).
These eighttexts were used to determine the reliability of thesortal class coding scheme.
Since sortal class an-notation is intrinsically hard, the annotators lookedup the senses of the head noun of each referring NPthat was not a pronoun or a proper name in Word-Net.
Each sense was mapped irectly to one or moreof the ten classes given in Table 2.
The annotatorsthen chose the adequate sense.The reliability of the annotations were measured9111 20GenreCFCGCKCLwords ref.
expr.
entities sequ.. MCUs % pron.
%deadend med.
len.6097 1725 1223 125 304 19.59% (1.8%, 0.3%, 58.3%) 89.78% 36103 1707 1290 120 269 16.17% (9.8%, 1.1%, 4%) 90.70% 26020 1848 1071 113 386 36.15% (19.5%, 1.2%, 56.1%) 89.45% 26018 1846 954 170 477 35.64% (14.0%, 1.5%, 53.6%) 80.09% 4Table 3: Relevant quantitative characteristics of the texts.
Average length: 2020 words, 120 MCUs.
sequ.
:number of sequences of co-specifying referring expressions.
% deadend: percentage of discourse ntitiesmentioned only once.
% pronouns: percentage ofall referring expressions realized as pronouns, in brackets:perc.
of first person singular pronouns, perc.
of second person singular pronouns, perc.
of third personsingular masculine and feminine pronouns, reed.
len.
: median length of sequences ofco-specifying referringexpressionswith Cohen's n (Cohen, 1960; Carletta, 1996).
Co-hen (1960) shows that a n between 0.68 and 0.80 al-lows tentative conclusions, while e; > 0.80 indicatesreliable annotations.
For genres CF (n = 0.83), CK(n = 0.84) and CL (n = 0.83), the sortal class an-notations were indeed reliable, but not for genre CG(n = 0.63).
Nevertheless, overall, the sortal classannotations were reliable (n ---- 0.8).
Problems aremainly due to the abstract classes Concept, Action,Event, State, and Property.
Abstract head nounssometimes have several senses that fit the contextalmost equally well, but that lead to different sor-tal classes.
Another problem is metaphorical usage.This explains the bad results for CG, which featuresmany abstract discourse ntities.4 Towards a ProbabilisticGenre- Independent  ModelIn this section, we investigate owhat extent the fac-tors proposed in section 2.2 influence the decision toprominalize.
For the purpose of the statistical naly-sis, pronominalization is modelled by a feature Pro.For a given referring expression, that feature has thevalue "P" if the referring expression is a personalor a possessive pronoun, else "N".
We model thisvariable with a binomial distribution.
I4.1 How do the Factors AffectPronominalization?First, we examine for all nine factors if there is astatistical ssociation between these factors and Pro.Standard non-parametric tests how a strong associ-ation between all nine factors and Pro.
2 This holds~For all statistical calculations and for the logistic regres-sion analyses reported below, we used R (Ihaka & Gentleman,1996).2We used the KruskaI-Wallis test for the ordinal Ambigvariable and the X2-test for the other, nominal, variables.
Sincefirst mentions and deadends are coded by the character "D" inboth for all referring expressions and for those thatoccur in sequences of co-specifying referring ex-pressions.
All of the tests were significant at thep < 0.001-level, with the exception of Par: for ex-pressions that are part of co-specification sequencesthe effect of that factor is not significant.In the next analysis tep, we determine which ofthe feature values are associated isproportionallyoften with pronouns, and which values tend to beassociated with full NPs.
More specifically, we testfor each feature-value pair if the pronominalizationprobability is significantly higher or lower than thatcomputed over (a) the complete data set, (b) all re-ferring expressions in sequences of co-specifyingreferring expressions, (c) all third person referringexpressions in sequences.
Almost all feature valuesshow highly significant effects for (a) and (b), butsome of these effects vanish in condition (c).
Be-low, we report on associations which are significantat p < 0.001 under all three conditions.Unsurprisingly, there is a strong effect of agree-ment values: NPs referring to the first and secondperson are always pronominalized, and third personmasculine or feminine NPs, which can refer to per-sons, are pronominalized more frequently than thirdperson neuter and third person plural.
Pronouns arestrongly preferred if the distance to the antecedent is0 or 1 MCUs.
Referring expressions are more likelyto be pronominalized in subject position than as aPP adjunct, and referring expressions with adjunctsas antecedents are also pronominalized less oftenthan those with antecedents in subject or object po-sition.
There is a clear preference for pronouns aspossessive determiners, and referring expressionsthat co-specify with an antecedent possessive pro-noun are highly likely to be pronominalised.
Weboth Dist and Dist4, both are treated as a categorical variableby R. For more on these tests, see (Agresti, 1990).
"1,1 21also notice strong genre-independent ffects of par-allelism.
Although at first glance, Ambig appears tohave a significant effect as well, (median ambiguityfor nouns is 3, median ambiguity for pronouns 0),closer inspection reveals that this is mainly due tofirst and second person and third person masculineand feminine pronouns.The sortal classes show a number of interest-ing patterns (cf.
Table 4).
Not only do the classesdiffer in the percentage of deadend entities, thereare also marked differences in pronominalizabil-ity.
There appear to be three groups of sortalclasses: Person/Group, with the lowest rate of dead-end entities and the highest percentage of pro-nouns - not only due to the first and second per-son personal pronouns- ,  Location/PhysObj, withroughly two thirds of all entities not in sequencesand a significantly lower pronominalization rate,and Concept/Action/Event/Property/State/Concept,with over 80% deadend entities.
Within this group,Action, Event, and Concept are pronominalizedmore frequently than State and Property.
Time is theleast frequently pronominalized class.
An impor-tant reason for the difference between Loc and Timemight be that Times are almost always referred backto by temporal adverbs, while locations, especiallytowns and countries, can also be accessed via thirdperson neuter personal pronouns.Interactions between the factors and genre wereexamined by an analysis of deviance run on a fit-ted logistic regression model; significance was cal-culated using the F-test.
All factors except for Parshow strong (p < 0.001) interactions with Genre.In other words, the influence of all factors but paral-lelism on pronominalization is mediated by Genre.There are two main reasons for this effect: first,some genres contain far more first and second per-son personal pronouns, which adds to the weight ofAgree, and second, texts which are about personsand the actions of persons, such as the texts in CKand CL, tend to use more pronouns than texts whichare mainly argumentative or expository.4.2 Which Factors are Important?To separate the important from the unimportant fac-tors, many researchers use decision and regressiontrees, mostly the binary CART variant (Breimanet al, 1984).
We use a different kind of model here,logistic regression, which is especially well suitedfor categorical data analysis (cf.
eg.
Agresti (1990)or Kessler et al (1997)).
In this model, the valueof the binary target variable is predicted by a lin-ear combination of the predictor variables.
Vari-able weights indicate the importance of a variablefor classification: the higher the absolute value ofthe weight, the more important i is.Logistic regression models are not only evaluatedby their performance on training and test data.
Wecould easily construct a perfect model of any train-ing data set with n variables, where n is the size ofthe data set.
But we need models that are small, yetpredict the target values well.
A suitable criterionis the Akaike Information Criterion (AIC, Akaike(1974)), which punishes both models that do not fitthe data well and models that have too many pa-rameters.
The quality of a factor is judged by theamount of variation in the target variable that it ex-plains.
Note that increased prediction accuracy doesnot necessarily mean an increase in the amount ofvariation explained.
As the model itself is a contin-uous approximation of the categorical distinctionsto be modelled, it may occur that the numerical vari-ation in the predictions decreases, but that this de-crease is lost when re-translating numerical predic-tions into categorical ones.The factors for our model were selected based onthe following procedure: We start with a model thatalways predicts the most frequent class.
We then de-termine which factor provides the greatest reductionin the AIC, add that factor to the model and retrain.This step is repeated until all factors have been usedor adding another factor does not yield any signifi-cant improvements anymore.
3This procedure invariably yields the sequenceDist4, Agree, Class, FormAnte, Syn, SynAnte, Am-big, Par, both when training models on the completedata set and when training on a single genre.
Inspec-tion of the AIC values suggests that parallelism isthe least important factor, and does not improve theAIC significantly.
Therefore, we will discard it fromthe outset.
All other factors are maintained in theinitial full model.
This model is purely additive; itdoes not include interactions between factors.
Thisapproach allows us to filter out factors which onlymediate the influence of other factors, but do not ex-ert any significant influence of their own.
Note thatthis probabilistic model only provides a numericaldescription of how its factors affect pronominaliza-tion in our corpus.
As such, it is not equivalent toa theoretical model, but rather provides data for fur-3We excluded Dist from this stepwise procedure, since therelevant information is covered already by Dist4, which fur-thermore has much fewer values.22Class%deadend% pronouns% pron.
(sequences)Act Concept Event Group Loc Pers PhysObj Prop State Time84.1 80.0 88.0 46.1 63.3 17.3 65.5 88.5 87.8 92.96.2 8,5 6.0 28.4 5.7 63.4 10.2 2.5 3.2 0.332.5 29.6 33.3 51.6 15.4 73.8 27.2 21.4 23,7 4.5Table 4: Results for Sortal Classes.
% deadend: percentage of deadend entities; % pronouns: percentpronominalised, % pron.
(sequences: percent pronominalised relative to all occurrences in co-specificationsequences% correctAIC% variationCF CG CK CL all97.1 93.5 93.6 91.5324.7 654.8 786.1 904.083.0 65.4 70.1 65.493.12685.868.7Table 5: Quality of models fitted to each of thegenre-specific corpora (CF, CG, CK, CL) and thecomplete data set (all).
% correct: correctly pre-dicted pronominalization decition, AIC: Akaike In-formation Criterion, % variation: percentage oforiginal variation in the data (as measured by de-viance) accounted for by the modelther theoretical interpretation.Results of a first evaluation of the full modelare summarized in Table 5.
The model can ex-plain more than two thirds of the variation in thecomplete data set and can predict pronominalizationquite well on the data it was fitted on.
The mat-ter becomes more interesting when we examine thegenre-specific results.
Although overall predictionperformance r mains table, the model is obviouslysuited better to some genres than to others.
The bestresults are obtained on CF, the worst on CL (mys-tery fiction).
In the CL texts, MCUs are short, athird of all referring expressions are pronouns, thereis no first person singular narrator, and most para-graphs which mention persons are about the inter-action between two persons.The Relative Importance of Factors.
All val-ues of Dist4 have very strong weights in all mod-els; this is clearly the most important factor.
Thesame goes for Agree, where the first and second per-son are strong signs of pronominalization, and, to alesser degree, masculine and feminine third personsingular.
The most important distinction providedby Class appears to be that between Persons, non-Persons, and Times.
This holds as well when themodel is only trained on third person referring ex-pressions.
For singular referring expressions, Per-sonhood information is reflected in gender, but notfor plural referring expressions.
Another importantinfluence is the form of the antecedent.
The syn-tactic function of the referring expression and of itsantecedent are less important, as is ambiguity.In order to examine the importance of the fac-tors in more detail, we refitted the models on thecomplete data set while omitting one or more of thethree central features Dist4, Agree, and Class.
Theresults are summarized in Table 6.
The most inter-esting finding is that even if we exclude all threefactors, prediction accuracy only drops by 3.2%.This means that the remaining 4 factors also con-tain most of the relevant information, but that thisinformation is coded more "efficiently", so to speak,in the first three.
Speaking of these factors, ques-tions concerning the effect of sortal class remains.Remarkably enough, when sortal class is omitted,accuracy increases by 0.7%.
The increase in A1Ccan be explained by a decrease in the amount ofexplained variation.
A third result is that informa-tion about the form of the antecedent can substitutefor distance information, if that information is miss-ing.
Both variables code the crucial distinctions be-tween expressions that evoke entities and those thataccess evoked entities.
Furthermore, a pronominalantecedent tends to occur at a distance of less than 2MCUs.
The contribution of syntactic function re-mains stable and significant, albeit comparativelyunimportant.Predictive Power: To evaluate the predictivepower of the models computed so far, we determinethe percentage of correctly predicted pronouns andNPs.
The performance of the trained models wascompared to two very simple algorithms:Algorithm A: Always choose the most frequentoption (i.e.
noun).Algorithm B: If the antecedent is in the sameMCU, or if it is in the previous MCU and thereis no ambiguity, choose a pronoun; else choosea noun.Table 7 summarises the results of the compari-son.
To determine the overall predictive power of23excludednoneClassAgreeDist4Dist4 + ClassDist4 + AgreeAgree + ClassDist4 + Agree + ClassAICfit%correct2686 92.62785 93.32984 92.63346 90.23443 90.23597 89.63098 92.63739 89.4~ explained variationDist4 Agree Class PForm Syn PSyn Ambig54.4 21.1 5.7 3.8 2.3 0.5 l.l54.4 21.1 n.a.
4.7 2.8 0.5 1.154.4 n.a.
14.3 6.2 2.7 0.6 1.1n.a.
35.8 6.1 32 3 0.8 0.In.a.
35.8 n.a.
33.7 3.4 0.8 0.1n.a.
n.a.
31.4 35.4 3.1 0.8 0.254.4 n.a.
n.a.
13.11 3.5 0.5 3.6n.a.
n.a.
n.a.
52.62 4 0.7 1.7Table 6: Effect of leaving out any one of the three most important factors on model fit.
italics: significanceis p < 0.0.5, for all other factors, p < 0.005 or better.test data setCF CG CK CL allAlg.
A 80.4 83.8 63.8 65.4 72.8Alg.
B 91.1 93.0 88.6 84.7 89.4Model 96.5 92.2 91.8 90.9 92.6 + 0.02w/oClass 96.8 92.4 91.7 90.7 93.0+ 0.01pothesize that the decrease in performance is mainlydue to the model itself, not to the training data.
Theresults presented in both Table 5 and 7 show thatalthough the model we have found is not quite asgenre-independent as we would want it to be, it pro-vides a reasonable fit to all the genres we examined.Table 7: Results of algorithms vs. models on testdata in % correct prediction if referring expressionis to be pronominalised or not.
Setup for genres:model is trained on three genres, tested on the re-maining onethe model, we used 10-fold cross-validation.
Al-gorithm A always fares worst, while algorithm B,which is based mainly on distance, the strongest fac-tor in the model, performs quite well.
Its overallperformance is 3.2% below that of  the full model,and 3.6% below that of the full model without sor-tal class information.
It even outperforms the mod-els on CG, which has the lowest percentage of Per-sons (12.9% vs. 35% for CF and 43.4% and 43.5%for CL and CK).
For all other genres, the statisticalmodels outperform the simple heuristics.
Excludingsortal class information can boost prediction perfor-mance on unseen data by as much as 0.4% for thecomplete corpus.
The apparent contradiction be-tween this finding and the results reported in theprevious section can be explained if we considerthat not only were some sortal classes comparativelyrare in the data (Property, Event), but that our sortalclass definition may still be too fine-grained.We evaluated the genre-independence of themodel by training on three genres and testing on thefourth.
The results show that the model fares quitewell for genre CF, which is also the genre where theoverall fit was best (see Table 5).
We therefore hy-5 Future  WorkWe have described a probabilistic model of pronom-inalization that is able to correctly predict 93% ofall pronouns in a corpus that consists of twelve textsfrom four different genres.
Since the model was de-rived from a limited corpus and a limited number ofgenres, we cannot guarantee that our results are ap-plicable to all texts without modifications.
But sinceits performance on our sample is consistently above90% correct, we are reasonably confident hat ourmain findings will hold for a wide variety of textsand text types.
In particular, we isolated several fac-tors which are robust predictors of pronominaliza-tion across genres: distance from last mention andagreement, and to a certain extent he form of theantecedent, which appears to be a good substitute ifthe other two factors are not available.
All three fea-tures can be computed on the basis of a chunk parse,a rough morphosyntactic analysis of the resultingNPs, and co-specification sequences.
In computa-tional terms, they are comparatively cheap.
Largecorpora can be annotated relatively quickly with thisinformation, which can then be used for statisticalpronoun generation.The comparatively expensive sortai class anno-tation, on the other hand, was not very importantin the final model; in fact, prediction accuracy de-creased when sortal class was included.
Thereare two main reasons for this: first, the proposedsortal class annotation scheme needs further work,24second, the relationship between sortal class andpronominalization may well be too intricate to bemodelled by the factor Class alone.We set out to find a genre-independent modelof pronominalization.
The model we found per-forms quite well, but genre still considerably affectsits performance.
Where does the remaining, unex-plained variation come from?
The variation mightbe just that - stylistic variation.
It might stem fromone of the traditional factors that we did not takeinto account here, such as thematic role.
However,we suspect that the crucial factor at play here is dis-course structure (McCoy & Strube, 1999).Acknowledgements Work on this paper was be-gun while Michael Strube was a postdoctoral fellowat the Institute for Research in Cognitive Science,University of Pennsylvania, nd Maria Wolters vis-ited the Institute for a week in summer 1999.
Wewould like to thank Kathleen McCoy, Jonathan De-Cristofaro, and the three anonymous reviewers fortheir comments on earlier stages of this work.ReferencesAgresti, Alan (1990).
Categorical Data Analysis.
NewYork, N.Y.: Wiley.Akaike, H. (1974).
A new look at statistical modelidentification.
1EEE Transactions Automatic Control,19:716-722.Azzam, Saliha, Kevin Humphreys & Robert Gaizauskas(1998).
Evaluating a focus-based approach toanaphora resolution.
In Proceedings of the 17 th In-ternational Conference on Computational Linguisticsand 36 th Annual Meeting of the Association for Com-putational Linguistics, Montr6al, Qu6bec, Canada,10--14 August 1998, pp.
74-78.Breiman, Leo, Jerome H. Friedman, Charles J.
Stone &R.A. Olshen (1984).
Classification and RegressionTrees.
Belmont, Cal.
: Wadsworth and Brooks/Cole.Cardie, Claire & Kiri Wagstaff (1999).
Noun phrasecoreference as clustering.
In Proceedings of the 1999SIGDAT Conference on Empirical Methods in NaturalLanguage Processing and Very Large Corpora, Col-lege Park, Md., 21-22 June 1999, pp.
82-89.Carletta, Jean (1996).
Assessing agreement on classifi-cation tasks: The kappa statistic.
Computational Lin-guistics, 22(2):249-254.Cohen, Jacob (1960).
A coefficient of agreement fornominal scales.
Educational and Psychological Mea-surement, 20:37--46.Dale, Robert (1992).
Generating Referring Expressions:Constructing Descriptions in a Domain of Objectsand Processes.
Cambridge, Mass.
: MIT Press.DeCristofaro, Jonathan, Michael Strube & Kathleen EMcCoy (1999).
Building a tool for annotating ref-erence in discourse.
In ACL '99 Workshop on theRelationship between Discourse~Dialogue Structureand Reference, University of Maryland, Maryland, 21June, 1999, pp.
54-62.Fellbaum, Christiane (Ed.)
(1998).
WordNet: An Elec-tronic Lexical Database.
Cambridge, Mass.
: MITPress.Fraurud, Kari (1996).
Cognitive ontology and NP form.In T. Fretheim & J. Gundel (Eds.
), Reference andReferent Accessibility, pp.
65-87.
Amsterdam, TheNetherlands: Benjamins.Ge, Niyu, John Hale & Eugene Charniak (1998).
A sta-tistical approach to anaphora resolution.
In Proceed-ings of the Sixth Workshop on Very Large Corpora,Montr6al, Canada, pp.
161-170.Grosz, Barbara J., Aravind K. Joshi & Scott Weinstein(1995).
Centering: A framework for modeling the lo-cal coherence of discourse.
Computational Linguis-tics, 21 (2):203-225.Hirschman, Lynette & Nancy Chinchor (1997).
MUC-7 Coreference Task Definition, http://www.muc.
sais.
com/proceed ings / .Ihaka, Ross & Ross Gentleman (1996).
R: A languagefor data analysis and graphics.
Journal of Computa-tional and Graphical Statistics, 5:299-314.Kessler, Brett, Geoffrey Nunberg & Hinrich Schiitze(1997).
Automatic detection of text genre.
In Proceed-ings of the 35 th Annual Meeting of the Associationfor Computational Linguistics and of the 8 th Confer-ence of the European Chapter of the Association forComputational Linguistics, Madrid, Spain, 7-12 July1997, pp.
32-38.LDC (1995).
Penn Treebank-H. Linguistic Data Consor-tium.
University of Pennsylvania, Philadelphia, Penn.McCoy, Kathleen F. & Michael Strube (1999).
Gener-ating anaphoric expressions: Pronoun or definite de-scription?
In ACL '99 Workshop on the Relationshipbetween Discourse/Dialogue Structure and Reference,University of Maryland, Maryland, 21 June, 1999, pp.63-71.Poesio, Massimo, Renate Henschel, Janet Hitzeman &Rodger Kibble (1999).
Statistical NP generation: Afirst report.
In R. Kibble & K. van Deemter (Eds.
),Proceedings of the Workshop on The Generation ofNominal Expressions, 11th European Summer Schoolon Logic, Language, and Information, Utrecht, 9-13August 1999.Sidner, Candace L. (1983).
Focusing in the compre-hension of definite anaphora.
In M. Brady & R.C.Berwick (Eds.
), Computational Models of Discourse,pp.
267-330.
Cambridge, Mass.
: MIT Press.Vossen, Piek (Ed.)
(1998).
EuroWordNet: A MultilingualDatabase with Lexical Semantic Networks.
Dordrecht,The Netherlands: Kluwer.Yeh, Ching-Long & Chris Mellish (1997).
An empiri-cal study on the generation of anaphora in Chinese.Computational Linguistics, 23( !
): 169-190.25
