Regularized Least-Squares Classification for Word SenseDisambiguationMarius PopescuDepartment of Computer Science, University of BucharestStr.
Academiei 1470109 Bucharest,Romania,mpopescu@phobos.cs.unibuc.roAbstractThe paper describes RLSC-LIN and RLSC-COMB systems which participated in theSenseval-3 English lexical sample task.
Thesesystems are based on Regularized Least-SquaresClassification (RLSC) learning method.
Wedescribe the reasons of choosing this method,how we applied it to word sense disambigua-tion, what results we obtained on Senseval-1, Senseval-2 and Senseval-3 data and discusssome possible improvements.1 IntroductionWord sense disambiguation can be viewed asa classification problem and one way to ob-tain a classifier is by machine learning methods.Unfortunately, there is no single one universalgood learning procedure.
The No Free LunchTheorem assures us that we can not design agood learning algorithm without any assump-tions about the structure of the problem.
So,we start by trying to find out what are the par-ticular characteristics of the learning problemposed by the word sense disambiguation.In our opinion, one of the most importantparticularities of the word sense disambiguationlearning problem, seems to be the dimensional-ity problem, more specifically the fact that thenumber of features is much greater than thenumber of training examples.
This is clearlytrue about data in Senseval-1, Senseval-2 andSenseval-3.
One can argue that this happensbecause of the small number of training exam-ples in these data sets, but we think that thisis an intrinsic propriety of learning task in thecase of word sense disambiguation.In word sense disambiguation one importantknowledge source is the words that co-occur (inlocal or broad context) with the word that hadto be disambiguated, and every different wordthat appears in the training examples will be-come a feature.
Increasing the number of train-ing examples will increase also the number ofdifferent words that appear in the training ex-amples, and so will increase the number of fea-tures.
Obviously, the rate of growth will not bethe same, but we consider that for any reason-able number of training examples (reasonable asthe possibility of obtaining these training exam-ples and as the capacity of processing, learningfrom these examples) the dimension of the fea-ture space will be greater.Actually, the high dimensionality of the fea-ture space with respect to the number of exam-ples is a general scenario of learning in the caseof Natural Language Processing tasks and wordsense disambiguation is one of these examples.In such situations, when the dimension ofthe feature space is greater than the numberof training examples, the potential for over-fitting is huge and some form of regulariza-tion is needed.
This is the reason why wechose to use Regularized Least-Squares Classifi-cation (RLSC) (Rifkin, 2002; Poggio and Smale,2003), a method of learning based on kernelsand Tikhonov regularization.In the next section we explain what sourceof information we used and how this informa-tion is transformed into features.
In section 3we briefly describe the RLSC learning algorithmand in section 4, how we applied this algorithmfor word sense disambiguation and what resultswe have obtained.
Finally, in section 5, we dis-cuss some possible improvements.2 Knowledge Sources and FeatureSpaceWe follow the common practice (Yarowsky,1993; Florian and Yarowsky, 2002; Lee and Ng,2002) to represent the training instances as fea-ture vectors.
This features are derived from var-ious knowledge sources.
We used the followingknowledge sources:?
Local information:?
the word form of words that appearAssociation for Computational Linguisticsfor the Semantic Analysis of Text, Barcelona, Spain, July 2004SENSEVAL-3: Third International Workshop on the Evaluation of Systemsnear the target word in a window ofsize 3?
the part-of-speech (POS) tags that ap-pear near the target word in a windowof size 3?
the lexical form of the target word?
the POS tag of the target word?
Broad context information:?
the lemmas of all words that appearin the provided context of target word(stop words are removed)In the case of broad context we use thebag-of-words representation with two weightingschema.
Binary weighting for RLSC-LIN andterm frequency weighting1 for RLSC-COMB.For stemming we used Porter stem-mer (Porter, 1980) and for tagging we usedBrill tagger (Brill, 1995).3 RLSCRLSC (Rifkin, 2002; Poggio and Smale, 2003)is a learning method that obtains solutions forbinary classification problems via Tikhonov reg-ularization in a Reproducing Kernel HilbertSpace using the square loss.Let S = (x1, y1), .
.
.
, (xn, yn) be a trainingsample with xi ?
Rd and yi ?
{?1, 1} for all i.The hypothesis space H of RLSC is the set offunctions f : Rd ?
R of the form:f(x) =n?i=1cik(x,xi)with ci ?
R for all i and k : Rd ?
Rd ?
Ra kernel function (a symmetric positive definitefunction) that measures the similarity betweentwo instances.RLSC tries to find a function from this hy-pothesis space that simultaneously has smallempirical error and small norm in Reproduc-ing Kernel Hilbert Space generated by kernel k.The resulting minimization problem is:minf?H1nn?i=1(yi ?
f(xi))2 + ?
?f?2KIn spite of the complex mathematical toolsused, the resulted learning algorithm is a very1We didn?t use any kind of smoothing.
The weight ofa term is simply the number of time the term appears inthe context divided by the length of the context.simple one (for details of how this algorithm isderived see Rifkin, 2002):?
From the training set S =(x1, y1), .
.
.
, (xn, yn) construct thekernel matrix KK = (kij)1?i,j?n kij = k(xi,xj)?
Compute the vector of coefficients c =(c1, .
.
.
, cn)?
by solving the system of lin-ear equations:(K + n?I)c = yc = (K + n?I)?1ywhere y = (y1, .
.
.
, yn)?
and I is the iden-tity matrix of dimension n?
Form the classifier:f(x) =n?i=1cik(x,xi)The sign(f(x)) will be interpreted as the pre-dicted label (?1 or +1) to be assigned to in-stance x, and the magnitude |f(x)| as the con-fidence in this prediction.4 Applying RLSC to Word SenseDisambiguationTo apply the RLSC learning method we musttake care about some details.First, RLSC produces a binary classifier andword sense disambiguation is a multi-class clas-sification problem.
There are a lot of ap-proaches for combining binary classifiers tosolve multi-class problems.
We used one-vs-allscheme.
We trained a different binary classi-fier for each sense.
For a word with m senseswe train m different binary classifiers, each onebeing trained to distinguish the examples in asingle class from the examples in all remainingclasses.
When a new example had to be classi-fied, the m classifiers are run, and the classifierwith the highest confidence, which outputs thelargest (most positive) value, is chosen.
If morethan one such classifiers exists, than from thesenses output by these classifiers we chose theone that appears most frequently in the train-ing set.
One advantage of one-vs-all combiningscheme is the fact that it exploits the confidence(real value) of classifiers produced by RLSC.
Formore arguments in favor of one-vs-all see (Rifkinand Klautau, 2004).Second, RLSC needs a kernel function.Preliminary experiments with Senseval-1 andSenseval-2 data show us that the best perfor-mance is obtained by linear kernel.
This obser-vation agrees with the Lee and Ng results (Leeand Ng, 2002), that in the case of SVM alsohave obtained the best performance with linearkernel.
One-vs-all combining scheme requirescomparison of confidences output by differentclassifiers, and for an unbiased comparison thereal values produced by classifiers correspond-ing to different senses of the target word mustbe on the same scale.
To achieve this goal weneed a normalized version of linear kernel.Our first system RLSC-LIN used the follow-ing kernel:k(x,y) = < x,y >?x?
?y?where x and y are two instances (feature vec-tors), < ?, ?
> is the dot product on Rd and ?
?
?is the L2 norm on Rd.In the case of RLSC-LIN we used a binaryweighting scheme for coding broad context.
Inthe RLSC-COMB we tried to obtain more in-formation from broad context and we used aterm frequency weighting scheme.
Now, the fea-ture vectors will have apart form 0 two kind ofvalues: 1 for features that encode local informa-tion and much small values (of order of 10?2)for features encoding broad context.
A simplelinear kernel will not work in this case becauseits value will be dominated by the similarity oflocal contexts.
To solve this problem we splitthe kernel in two parts:k(x,y) = 12kl(x,y) +12kb(x,y)where kl is a linear normalized kernel that usesonly the components of the feature vectors thatencode local information (and have 0/1 values)and kb is a normalized kernel that uses only thecomponents of the feature vectors that encodebroad context.The last detail concerning application ofRLSC is the value of regularization parameter?.
Experimenting on Senseval-1 and Senseval-2 data sets we establish that small values of ?achieve best performance.
In all reported re-sults we used ?
= 10?9.The results2 of RLSC-LIN and RLSC-2The coarse-grained score on Senseval-3 for bothRLSC-LIN and RLSC-COMB was 0.784COMB on Senseval-1, Senseval-2 and Senseval-3 data are summarized in Table 1.RLSC-LIN RLSC-COMBSenseval-1 0.772 0.775Senseval-2 0.652 0.656Senseval-3 0.718 0.722Table 1: Fine-grained score forRLSC-LIN andRLSC-COMB on Senseval data setsBecause RLSC has many points in commonwith the well-known Support Vector Machine(SVM), we list in Table 2 for comparison theresults obtained by SVM with the same kernels.SVM-LIN SVM-COMBSenseval-1 0.771 0.773Senseval-2 0.644 0.642Senseval-3 0.714 0.708Table 2: Fine-grained score for SVM-LIN andSVM-COMB on Senseval data setsThe results are competitive with the state ofthe art results reported until now.
For exam-ple the best two results reported until now onSenseval-2 data are 0.654 (Lee and Ng, 2002)obtained with SVM and 0.665 (Florian andYarowsky, 2002) obtained by classifiers combi-nation.The results are especially good if we take intoaccount the fact that our systems do not usesyntactic information3 while the others do.
Leeand Ng (Lee and Ng, 2002) report a fine-grainedscore for SVM of only 0.648 if they do not usesyntactic knowledge source.These results encourage us to participatewith RLSC-LIN and RLSC-COMB to theSenseval-3 competition.5 Possible ImprovementsFirst evident improvement is to incorporatesyntactic information as knowledge source intoour systems.It is quite possible to substantially improvethe results of RLSC-COMB using a combina-tion of more adequate kernels (each kernel in thecombination being adequate to the source of in-formation represented by the part of the feature3It takes too long to adapt to our systems a parser(to prepare the data for parsing, parse it with a freestatistical parser and extract useful features from theparser output)vector that the kernel uses).
For example, wecan use a combination of a linear kernel for localinformation a string kernel (Lodhi et al, 2002)for broad context and a tree kernel (Collins andDuffy, 2002) for syntactic relations.Also, instead of using an equal weight for eachkernel in the combination we can use weights4that reflect the importance for disambiguationof knowledge source that the kernel uses, or wecan establish the weight of each kernel experi-mentally by kernel-target algnment (Cristianiniet al, 2002).ReferencesEric Brill.
1995.
Transformation-based error-driven learning and natural language process-ing: A case study in part of speech tagging.Computational Linguistics, 21(4):543?565.M.
Collins and N. Duffy.
2002.
Convolutionkernels for natural language.
In T. G. Di-etterich, S. Becker, and Z. Ghahramani, ed-itors, Advances in Neural Information Pro-cessing Systems 14, pages 625?632, Cam-bridge, MA.
MIT Press.N.
Cristianini, J. Shawe-Taylor, A. Elisseeff,and J. Kandola.
2002.
On kernel-targetalignment.
In T. G. Dietterich, S. Becker, andZ.
Ghahramani, editors, Advances in Neu-ral Information Processing Systems 14, pages367?373, Cambridge, MA.
MIT Press.Radu Florian and David Yarowsky.
2002.
Mod-eling consensus: Classifier combination forword sense disambiguation.
In Proceedings ofEMNLP?02, pages 25?32, Philadelphia, PA,USA.Yoong Lee and Hwee Ng.
2002.
An empiricalevaluation of knowledge sources and learn-ing algorithms for word sense disambiguation.In Proceedings of EMNLP?02, pages 41?48,Philadelphia, PA, USA.Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello Cristianini, and Chris Watkins.2002.
Text classification using string ker-nels.
Journal of Machine Learning Research,2(February):419?444.Tomaso Poggio and Steve Smale.
2003.
Themathematics of learning: Dealing with data.Notices of the American Mathematical Soci-ety (AMS), 50(5):537?544.Martin Porter.
1980.
An algorithm for suffixstripping.
Program, 14(3):130?137.Ryan Rifkin and Aldebaro Klautau.
2004.
In4The weights must sum to onedefense of one-vs-all classification.
Journal ofMachine Learning Research, 5(January):101?141.Ryan Rifkin.
2002.
Everything Old Is NewAgain: A Fresh Look at Historical Approachesto Machine Learning.
Ph.D. thesis, Mas-sachusetts Institute of Technology.David Yarowsky.
1993.
One sense per colloca-tion.
In ARPA Human Language TechnologyWorkshop, pages 266?271, Princeton, USA.
