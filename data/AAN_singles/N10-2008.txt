Proceedings of the NAACL HLT 2010: Demonstration Session, pages 29?32,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsSerious Game Environments for Language and Culture EducationAlicia Sagae, W. Lewis Johnson, and Rebecca RowAlelo, Inc.12910 Culver Boulevard, Suite JLos Angeles, CA 90066, USA{asagae, ljhonson, rrow}@alelo.comAbstractIn this demonstration we will present technol-ogies that enable learners to engage in spokenconversations in foreign languages, integrat-ing intelligent tutoring and serious game ca-pabilities into a package that helps learnersquickly acquire communication skills.
Con-versational AI technologies based on theSAIBA framework for dialog modeling arerealized in this 3-D game environment.
Par-ticipants will be introduced to tools for author-ing dialogs in this framework, and will havean opportunity to experience learning withAlelo products, including the OperationalLanguage and Culture Training System(OLCTS).1 IntroductionAlelo's language and culture education environ-ments, including The Tactical Language and Cul-ture Training System (TLCTS) and the OperationalLanguage and Culture Training System (OLCTS),are AI-enhanced learning platforms that helplearners quickly acquire communication skills inforeign languages and cultures.
They have beendeveloped by Alelo, Inc. based on a prototype de-veloped at the University of Southern California(USC).These environments utilize an integrated combi-nation of intelligent tutoring system and seriousgame technologies.
Trainees work through a seriesof interactive lessons and exercises, called the SkillBuilder, focusing on mission-oriented communica-tion skills.
The lessons make extensive use of au-tomated speech recognition focused on learnerlanguage, and provide learners with feedback ontheir performance.
Cultural notes describing cus-toms and nonverbal gestures are integrated into theSkill Builder lessons.
Trainees apply their skills inan interactive Arcade Game, where they use spo-ken commands in the target language to navigate atown grid, and in a Mission Game, where they par-ticipate in real-time dialog with simulated localpeople in order to accomplish their mission.2 Systems that Impact LearnersFive TLCTS/OLCTS training courses have beendeveloped so far: Tactical IraqiTM, focusing on Ira-qi Arabic, Tactical PashtoTM and Tactical DariTMfocusing on the predominant dialects spoken inAfghanistan, Tactical FrenchTM for Sahel Africa,and Operational IndonesianTM.
TLCTS courses arecomplete training courses, providing all of thetraining materials needed to conduct basic trainingin foreign language and culture.
For example, Tac-tical IraqiTM includes eighteen Mission Gamescenes, ten Arcade Game levels, and sixty-threeSkill Builder scenes comprising over 2000 lessonpages.
Additional scenes and lessons are underdevelopment.While the platform imposes no limit on contentsize, the material developed so far or these systemstypically covers 80-120 hours of training.
In-gamereference materials, including glossaries, summa-ries of lesson content, and grammar notes, are29available both as part of the training package andas is a support Web site.
Manuals, comprising tu-torials and training guidelines, help with initialorientation, training management, and trouble-shooting.
The OLCTS versions of these coursesinclude supplementary exercises delivered onhandheld devices and on the web, giving trainees arange of platforms for "train-anywhere" access.TLCTS rapidly transitioned into widespread use.Computer labs for training with TLCTS courseshave been established in numerous locations in theUSA and around the world.
An estimated twenty-five thousand US military users have trained withthe system, and consistently rate it highly.
It hasalso been made available to service members inallied military forces.Although the Tactical Language and Cultureconcept was originally developed under militaryfunding, the approach can be applied quite general-ly to language and culture learning.
The key is thatthe courses are task-oriented: the learner has a taskto carry out, the Skill Builder helps the learner toacquire the skills necessary to carry out the task,and the Mission Game gives the learner an oppor-tunity to practice the task in compelling simulatedsettings.3 Conversational Agent TechnologiesSimulated dialogs are executed by the virtual hu-man architecture described in (Johnson & Valente,2008).
The architecture adopts a variant of theSAIBA framework (Vilhjalmsson & Marsella,2005), which separates intent planning (the choiceof what to communicate) from production of be-lievable behavior (how to realize the communica-tion).
An overview of the social simulationprocess is given in Figure 1.3.1 Rule-Driven BehaviorVirtual human behavior is generated by a series ofcomponents that include explicit models of speechand language (for natural language understandingand generation) as well as behavior-mapping rulesthat implicitly reflect the subject-matter expertiseof the rule authors.
These rules generally occur atthe level of communicative acts (Traum & Hin-kelman, 1992).
A simple example of such a rule,expressed in natural language, is shown below:IF the learner says that your home is beautiful,THEN reply that it is quite plain(1)UtteranceAutomatedSpeechRecognizerInterpretationRulesEnvironmentalContextCulturalContextDialogContextIntent PlanningAgentBehaviorInterpretationCommunicativeAct <FML>BehaviorGenerationTranslationRulesBehaviorSpecification<BML>BehaviorRealizationCommunicativeAct <FML>Input ProcessingConversationCachePersonality ModelLanguage ModelCulture ModelWorld Model(physical/social)Agent ModelsInputProcessingSocial SimulationModuleGameEngineSimulationManagementAgentAgentSocial Simulation Manager (API)PlayerPlayer Speech+- GestureFigure 1.
Dialog simulation architecture in Alelo language and culture training systems303.2 Collaborative AuthoringRules like (1) are created by a content developmentteam with expertise in linguistics and cultural anth-ropology.
This work is supported by a set of web-based collaborative authoring tools, called Konaand TIDE.
Kona is used to create lesson contentfor the Skill Builder, while TIDE is a graphicaleditor used to encode dialog rules as transitions ina Finite State Machine.Kona gives authors access to a database of les-son content, specified in XML format.
The authorscan selectively lock and edit lessons in the data-base, and view and edit different fields in the spe-cification of each page in the lesson.
The authorcan edit the written descriptions of the image onthe page, the cultural notes, and the enabling learn-ing objectives (ELOs) covered in the page.
In oth-er views, authors can link in images and soundrecordings, and make notes and comments for oth-er authors to review.
The lesson specifications arethen automatically translated into the internal dataformat used in OLCTS, so that authors can reviewthe lessons as they appear in the training applica-tion.4 The DemonstrationThe demonstration will give participants an oppor-tunity to use OLCTS, and other Alelo interactivelanguage and culture training products, and learnabout their supporting authoring tools.
It is in-tended for people who are interested in gaining anin-depth understanding of AIED (artificial intelli-gence in education) technology for serious games,and the development tools used to create them.The demo will be conducted by a presenter, whowill give live demonstrations of the software, andan assistant presenter who will coach the partici-pants in the use of the game and supporting author-ing tools.4.1 OverviewFirst, the participants will get a hands-on intro-duction to one of the Operational Language andCulture courses.
Under supervision of a presenter,Figure 2.
Screen shot of a Mission Game dialog in Operational DariTM31the participants will learn to say a few phrases inthe Skill Builder and use the phrases that they havelearned in the Mission Game.
This portion can betailored on the fly to the interests of participants,and can take from 5 to 30 minutes to complete.Depending on time and interest, participantsmay also have an opportunity to work with anOLCTS course in more depth.
They can be calledupon to learn some basic communication skills inDari and apply them in the Mission Game.
Thiswill give participants a firsthand understanding ofhow each component of OLCTS supports learning,how the components support each other, and howartificial intelligence technology is applied in thelearning experience.Finally, the presenter will demo some of the au-thoring tools used to create OLCTS content.
Theparticipants will propose modifications or exten-sions to an existing OLCTS course.
The presenterwill use the authoring tools in real time to make themodifications, following the recommendations ofthe participants.4.2 Example: Engaging in a Dialog in Opera-tional DariTMFor a video summary of the demonstration, pleasevisit http://www.alelo.com/movie_tlt-6min.html.The user experience in the Mission Game is oneengaging component of this demonstration.
Anexample script for a Mission Game interaction inAlelo's Operational DariTM course is given in thefollowing sections.A sample of a Mission Game screen is shown inFigure 2.
The player controls the figure in the cen-ter-left.
At this point in the demonstration, theplayer has received a briefing that describes acommunication task that he or she should accom-plish in this exercise.
To complete the task, theplayer must engage the virtual human, or non-player character (NPC) shown on the right.Organizing rebuilding operations is one exampleof such a task.
The NPC is a host-national charac-ter in Afghanistan.
The player should check on thestatus of their shared plan for rebuilding and giveconstructive feedback.
This type of communica-tion task can require finesse and delicacy on thepart of the player in order to be culturally appro-priate.
It draws on the learner's understanding andskill with face-saving, a prominent feature of manycultures worldwide.The learner must initiate the conversation byspeaking into a headset-mounted microphone.
Heor she clicks on the microphone icon, shown inFigure 3, speaks, then clicks on the icon again toindicate the end of the turn.Figure 2.
Push the microphone button to speak during adialog, push again to stop.Recognized player speech is posted to a dialog his-tory window that appears near the top of the virtualscene, as shown in Figure 1.
The NPC respondsusing spoken output, creating a realistic and engag-ing practice environment.
During the dialog, theplayer may view hints that display key phrases inDari.
Once the player has discussed all of the hostnational's training mistakes, the dialog ends in suc-cess.ReferencesH.
Vilhjalmsson and S. Marsella.
"Social PerformanceFramework", in Proceedings of the AAAI Workshopon Modular Construction of Human-Like Intelli-gence.
2005.W.
L. Johnson and A. Valente.
?Tactical Language andCulture Training Systems: Using Artificial Intelli-gence to Teach Foreign Languages and Cultures?, inProceedings of IAAI 2008.
March 2008.David R. Traum and Elizabeth A. Hinkelman.
"Conver-sation Acts in Task-Oriented Spoken Dialogue", inComputational Intelligence, 8(3):575--599, 1992.32
