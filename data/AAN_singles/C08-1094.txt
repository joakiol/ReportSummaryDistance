Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 745?752Manchester, August 2008Classifying chart cells for quadratic complexity context-free inferenceBrian Roark and Kristy HollingsheadCenter for Spoken Language UnderstandingOregon Health & Science University, Beaverton, Oregon, 97006 USA{roark,hollingk}@cslu.ogi.eduAbstractIn this paper, we consider classifying wordpositions by whether or not they can eitherstart or end multi-word constituents.
Thisprovides a mechanism for ?closing?
chartcells during context-free inference, whichis demonstrated to improve efficiency andaccuracy when used to constrain the well-known Charniak parser.
Additionally, wepresent a method for ?closing?
a sufficientnumber of chart cells to ensure quadraticworst-case complexity of context-free in-ference.
Empirical results show that thisO(n2) bound can be achieved without im-pacting parsing accuracy.1 IntroductionWhile there have been great advances in the statis-tical modeling of hierarchical syntactic structure inthe past 15 years, exact inference with such mod-els remains very costly, so that most rich syntac-tic modeling approaches involve heavy pruning,pipelining or both.
Pipeline systems make use ofsimpler models with more efficient inference to re-duce the search space of the full model.
For ex-ample, the well-known Ratnaparkhi (1999) parserused a POS-tagger and a finite-state NP chunker asinitial stages of a multi-stage Maximum Entropyparser.
The Charniak (2000) parser uses a simplePCFG to prune the chart for a richer model; andCharniak and Johnson (2005) added a discrimina-tively trained reranker to the end of that pipeline.Recent results making use of finite-state chun-kers early in a syntactic parsing pipeline haveshown both an efficiency (Glaysher andMoldovan,2006) and an accuracy (Hollingshead and Roark,2007) benefit to the use of such constraints in aparsing system.
Glaysher and Moldovan (2006)demonstrated an efficiency gain by explicitly dis-allowing entries in chart cells that would result inconstituents that cross chunk boundaries.
Holling-shead and Roark (2007) demonstrated that highprecision constraints on early stages of the Char-niak and Johnson (2005) pipeline?in the form ofbase phrase constraints derived either from a chun-ker or from later stages of an earlier iteration of thec?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.same pipeline?achieved significant accuracy im-provements, by moving the pipeline search awayfrom unlikely areas of the search space.
Bothof these approaches (as with Ratnaparkhi earlier)achieve their improvements by ruling out parts ofthe search space for downstream processes, and thegain can either be realized in efficiency (same ac-curacy, less time) or accuracy (same time, greateraccuracy).
Parts of the search space are ruled outprecisely when they are inconsistent with the gen-erally reliable output of the chunker, i.e., the con-straints are a by-product of chunking.In this paper, we consider building classifiersthat more directly address the problem of ?closing?chart cells to entries, rather than extracting this in-formation from taggers or chunkers built for a dif-ferent purpose.
We build two classifiers, which tageach word in the sequence with a binary class la-bel.
The first classifier decides if the word can be-gin a constituent of span greater than one word; thesecond classifier decides if the word can end a con-stituent of span greater than 1.
Given a chart cell(i, j) with start word wiand end word wj, wherej>i, that cell can be ?closed?
to entries if the firstclassifier decides thatwicannot be the first word ofa multi-word constituent or if the second classifierdecides that wjcannot be the last word in a multi-word constituent.
In such a way, we can optimizeclassifiers specifically for the task of constrain-ing chart parsers.
Note that such classifier outputwould be relatively straightforward to incorporateinto most existing context-free constituent parsers.We demonstrate the baseline accuracies of suchclassifiers, and their impact when the constraintsare placed on the Charniak and Johnson (2005)parsing pipeline.
Various ways of using classifieroutput are investigated, including one method forguaranteeing quadratic complexity of the context-free parser.
A proof of the quadratic complexity isincluded, along with a detailed performance evalu-ation when constraining the Charniak parser to beworst-case quadratic.2 BackgroundDynamic programming for context-free inferencegenerally makes use of a chart structure, as shownin Fig.
1.
Each cell in the chart represents a pos-sible constituent spanning a substring, which isidentified by the indices of the first and last wordsof the substring.
Thus, the cell identified with745i, ji, j?1 i+1, ji, j?2 i+1, j?1 i+2, ji, j?3 i+1, j?2 i+2, j?1 i+3, ji, j?4 i+1, j?3 i+2, j?2 i+3, j?1 i+4, jFigure 1: Fragment of a chart structure.
Each cell is indexedwith start and end word indices.i, j will contain possible constituents spanning thesubstring wi.
.
.
wj.
Context-free inference has cu-bic complexity in the length of the string n, dueto the O(n2) chart cells and O(n) possible childconfigurations at each cell.
For example, the CYKalgorithm, which assumes a grammar in ChomskyNormal Form (hence exactly 2 non-terminal chil-dren for each constituent of span greater than 1),must consider the O(n) possible midpoints for thetwo children of constituents at each cell.In a parsing pipeline, some decisions about thehidden structure are made at an earlier stage.
Forexample, base phrase chunking involves identify-ing a span as a base phrase of some category,often NP.
A base phrase constituent has no chil-dren other than pre-terminal POS-tags, which allhave a single terminal child, i.e., there is no in-ternal structure in the base phrase involving non-POS non-terminals.
This has a number of implica-tions for the context-free parser.
First, there is noneed to build internal structure within the identi-fied base phrase constituent.
Second, constituentswhich cross brackets with the base phrase can-not be part of the final tree structure.
This sec-ond constraint on possible trees can be thoughtof as a constraint on chart cells, as pointed outin Glaysher and Moldovan (2006): no multi-wordspanning constituent can begin at a word fallingwithin a base-phrase chunk, other than the firstword of that chunk.
Similarly, no multi-word span-ning constituent can end at a word falling within abase-phrase chunk, other than the last word of thatchunk.
These constraints rule out many possiblestructures that the full context-free parser wouldhave to otherwise consider.These start and end constraints can be extractedfrom the output of the chunker, but the chunker isnot trained to optimize the accuracy (or the pre-cision) of these particular constraints, rather typi-cally to optimize chunking accuracy.
Further, theseconstraints can apply even for words which falloutside of typical chunks.
For example, in En-glish, verbs and prepositions tend to occur beforetheir arguments, hence are often unlikely to endconstituents, despite not being inside a typicallydefined base phrase.
If we can build a classifierspecifically for this task (determining whether aStrings in corpus 39832Word tokens in corpus 950028Tokens neither first nor last in string 870399Word tokens in S1439558 50.5%Word tokens in E1646855 74.3%Table 1: Statistics on word classes from sections 2-21 of thePenn Wall St. Journal Treebankword can start or end a multi-word constituent),we can more directly optimize the classifier for usewithin a pipeline.3 Starting and ending constituentsTo better understand the particular task that wepropose, and its likely utility, we first look at thedistribution of classes and our ability to build sim-ple classifiers to predict these classes.
First, letus introduce notation.
Given a string of n wordsw1.
.
.
wn, we will say that a word wi(1<i<n) isin the class S>1if there is a constituent spanningwi.
.
.
wjfor some j>i; and wi?
S1otherwise.Similarly, we will say that a word wj(1<j<n) isin the class E>1if there is a constituent spanningwi.
.
.
wjfor some i<j; and wj?
E1otherwise.These are two separate binary classification tasks.Note that the first word w1and the last wordwnare unambiguous in terms of whether they startor end constituents of length greater than 1.
Thefirst word w1must start a constituent spanning thewhole string, and the last word wnmust end thatsame constituent.
The first word w1cannot end aconstituent of length greater than 1; similarly, thelast word wncannot start a constituent of lengthgreater than 1.
Hence our classifier evaluationomits those two word positions, leading to n?2classifications for a string of length n.Table 1 shows statistics from sections 2-21 ofthe Penn WSJ Treebank (Marcus et al, 1993).From the nearly 1 million words in approximately40 thousand sentences, just over 870 thousand areneither the first nor the last word in the string,hence possible members of the sets S1or E1, i.e.,not beginning a multi-word constituent (S1) or notending a multi-word constituent (E1).
Of these,over half (50.5%) do not begin multi-word con-stituents, and nearly three quarters (74.3%) do notend multi-word constituents.
This high latter per-centage reflects English right-branching structure.How well can we perform these binary classifi-cation tasks, using simple (linear complexity) clas-sifiers?
To investigate this question, we used sec-tions 2-21 of the Penn WSJ Treebank as trainingdata, section 00 as heldout, and section 24 as de-velopment.
Word classes are straightforwardly ex-tracted from the treebank trees, by measuring thespan of constituents starting and ending at eachword position.
We trained log linear models withthe perceptron algorithm (Collins, 2002) using fea-746Markov orderClassification Task 0 1 2S1(no multi-word constituent start) 96.7 96.9 96.9E1(no multi-word constituent end) 97.3 97.3 97.3Table 2: Classification accuracy on development set for bi-nary classes S1and E1, for various Markov orders.tures similar to those used for NP chunking in Shaand Pereira (2003), including surrounding POS-tags (provided by a separately trained log linearPOS-tagger) and surrounding words, up to 2 be-fore and 2 after the current word position.Table 2 presents classification accuracy on thedevelopment set for both of these classificationtasks.
We trained models with Markov order 0(each word classified independently), order 1 (fea-tures with class pairs) and order 2 (features withclass triples).
This did not change performancefor the E1classification, but Markov order 1 wasslightly (but significantly) better than order 0 forS1classification.
Hence, from this point forward,all classification will be Markov order 1.We can see from these results that simple classi-fication approaches yield very high classificationaccuracy.
The question now becomes, how canclassifier output be used to constrain a context-freeparser, and what is the impact on parser perfor-mance of using such a classifier in the pipeline.4 Closing chart cellsBefore moving on to an empirical investigation ofconstraining context-free parsers with the methodswe propose, we first need to take a fairly detailedlook at representations internal to these parsers.
Inparticular, while we can rule out multi-word con-stituents with particular start and end positions,there may be intermediate or incomplete structureswithin the parser that should not be ruled out atthese same start and end positions.
Hence the no-tion of ?closing?
a chart cell is slightly more com-plicated than it may seem initially.Consider the chart representation in Fig.
1.
Sup-pose that wiis in class S1and wjis in class E1,for i<j.
We can ?close?
all cells (i, k) such thati<k and all cells (l, j) such that l<j, based onthe fact that multi-word constituents cannot beginwith word wiand cannot end with wj.
A closedcell will not take complete entries, and, dependingon the constraint used to close the cell, will haverestrictions on incomplete entries.
To make thismore explicit, let us precisely define complete andincomplete entries.Context-free inference using dynamic program-ming over a chart structure builds longer-span con-stituents by combining smaller span constituents,guided by rules in a context-free grammar.
Acontext-free grammar G = (V, T, S?, P ) consistsof: a set of non-terminal symbols V , including aspecial start symbol S?
; a set of terminal symbolsT ; and a set of rule productions P of the formA ?
?
for A ?
V and ?
?
(V ?
T )?, i.e.,a single non-terminal on the left-hand side of therule production, and a sequence of 0 or more ter-minals or non-terminals on the right-hand side ofthe rule.
If we have a rule production A ?
B Cin P , a completed B entry in chart cell (i, j) anda completed C entry in chart cell (j, k), then wecan place a completed A entry in chart cell (i, k),typically with some indication that the A was builtfrom the B and C entries.
Such a chart cell entryis sometimes called an ?edge?.The issue with incomplete edges arises whenthere are rule productions in P with more than twochildren on the right-hand side of the rule.
Ratherthan trying to combine an arbitrarily large num-ber of smaller cell entries, a more efficient ap-proach, which exploits shared structure betweenrules, is to only perform pairwise combination,and store incomplete edges to represent combina-tions that require further combination to achievea complete edge.
This can either be performedin advance, e.g., by factoring a grammar to be inChomsky Normal Form, as required by the CYKalgorithm (Cocke and Schwartz, 1970; Younger,1967; Kasami, 1965), resulting in ?incomplete?non-terminals created by the factorization; or in-complete edges can be represented through so-called dotted rules, as with the Earley (1970) al-gorithm, in which factorization is essentially per-formed on the fly.
For example, if we have a ruleproduction A ?
B C D in P , a completed B en-try in chart cell (i, j) and a completed C entry inchart cell (j, k), then we can place an incompleteedgeA ?
B C ?D in chart cell (i, k).
The dot sig-nifies the division between what has already beencombined (to the left of the dot), and what remainsto be combined.1Then, if we have an incompleteedge A ?
B C ?D in chart cell (i, k) and a com-plete D in cell (k, l), we can place a completed Aentry in chart cell (i, l).If a chart cell (i, j) has been ?closed?
due toconstraints limiting multi-word constituents withthat span ?
either wi?
S1or wj?
E1(and i<j) ?then it is clear that ?complete?
edges should not beentered in the cell, since these represent preciselythe multi-word constituents that are being ruledout.
How about incomplete edges?
To the extentthat an incomplete edge can be extended to a validcomplete edge, it should be allowed.
There aretwo cases.
If wi?
S1, then under the assumptionthat incomplete edges are extended from left-to-right (see footnote 1), the incomplete edge should1Without loss of generality, we will assume that edges areextended from left-to-right.747Parsing accuracy % of CellsParsing constraints LR LP F ClosedNone (baseline) 88.6 89.2 88.9 ?S1positions 87.6 89.1 88.3 44.6E1positions 87.4 88.5 87.9 66.4Both S1and E186.5 88.6 87.4 80.3Table 3: Charniak parsing accuracy on section 24 under var-ious constraint conditions, using word labels extracted usingMarkov order 1 model.be discarded, because any completed edges thatcould result from extending that incomplete edgewould have the same start position, i.e., the chartcell would be (i, k) for some k>i, which is closedto the completed edge.
However, if wi6?
S1, thenwj?
E1.
A complete edge achieved by extendingthe incomplete edge will end at wkfor k>j, andcell (i, k) may be open, hence the incomplete edgeshould be allowed in cell (i, j).
See ?6 for limita-tions on how such incomplete edges arise in closedcells, which has consequences for the worst-casecomplexity under certain conditions.5 Constraining the Charniak parser5.1 Parser overview and constraint methodsThe Charniak (2000) parser is a multi-stage,agenda-driven, edge-based parser, that can be con-strained by precluding edges from being placed onthe agenda.
Here we will briefly describe the over-all architecture of that parser, and our method forconstraining its search.The first stage of the Charniak parser uses anagenda and a simple PCFG to build a sparse chart,which is used in later stages with the full model.We will focus on this first stage, since it is herethat we will be constraining the parser.
The edgeson the agenda and in the chart are dotted rules, asdescribed in ?4.
When edges are created, they arepushed onto the agenda.
Edges that are poppedfrom the agenda are placed in the chart, and thencombined with other chart entries to create newedges that are pushed onto the agenda.
When acomplete edge spanning the whole string is placedin the chart, at least one full solution exists in thechart.
After this happens, the parser continuesadding edges to the chart and agenda until reachingsome parameterized target number of additionaledges in the chart, at which point the next stageof the pipeline receives the chart as input and anyedges remaining on the agenda are discarded.We constrain the first stage of the Charniakparser as follows.
Using classifiers, a subset ofword positions are assigned to class S1, and a sub-set are assigned to class E1.
(Words can be as-signed to both.)
When an edge is created for cell(i, j), where i < j, it is not placed on the agendaif either of the following two conditions hold: 1)wi?
S1; or 2) the edge is complete and wj?
E1.0.5 0.6 0.7 0.8 0.9 10.950.960.970.980.991RecallPrecisionStart classificationEnd classificationFigure 2: Precision/recall tradeoff of S1and E1tags on thedevelopment set.Of course, the output of our classifier is not per-fect, hence imposing these constraints will some-times rule out the true parse, and parser accuracymay degrade.
Furthermore, because of the agenda-based heuristic search, the efficiency of search maynot be impacted as straightforwardly as one mightexpect for an exact inference algorithm.
For thesereasons, we have performed extensive empiricaltrials under a variety of conditions to try to clearlyunderstand the best practices for using these sortsof constraints for this sort of parser.5.2 Experimental trialsWe begin by simply taking the output of theMarkov order 1 taggers, whose accuracies are re-ported in Table 2, and using word positions labeledas S1or E1to ?close?
cells in the Charniak parser,as described above.
Table 3 presents parser accu-racy on the development set (section 24) under fourconditions: the unconstrained baseline; using justS1words to close cells; using just E1word posi-tions to close cells; and using both S1and E1po-sitions to close cells.
As can be seen from theseresults, all of these trials result in a decrease inaccuracy from the baseline, with larger decreasesassociated with higher percentages of closed cells.These results indicate that, despite the relativelyhigh accuracy of classification, the precision of ourclassifier in producing the S1and E1tags is toolow.
To remedy this, we traded some recall for pre-cision as follows.
We used the forward-backwardalgorithm with our Markov order 1 tagging modelto assign a conditional probability at each word po-sition of the tags S1and E1given the string.
Ateach word position wifor 1<i<n, we took the loglikelihood ratio of tag S1as follows:LLR(wi?
S1) = logP(wi?
S1| w1.
.
.
wn)P(wi6?
S1| w1.
.
.
wn)(1)and the same for tag E1.
A default classificationthreshold is to label S1or E1if the above log like-lihood is greater than zero, i.e., if the S1tag is morelikely than not.
To improve the precision, we canmove this threshold to some greater value.7480 0.2 0.4 0.6 0.8 187.58888.58989.5Fraction of constraints preservedCharniakparser F?measureStart position constraintsEnd position constraintsBaseline performanceFigure 3: Charniak parser F-measure at various operatingpoints of the fraction c of total constraints kept.Each word position in a string was ranked withrespect to these log likelihood ratios for eachtag.2If the total number of words wiwithLLR(wi?
S1) > 0 is k, then we defined multi-ple operating points by setting the threshold suchthat ck words remained above threshold, for someconstant c between 0 and 1.
Fig.
2 shows the pre-cision/recall tradeoff at these operating points forboth S1and E1tags.
Note that for both tags, wecan achieve over 99% precision with recall above70%, and for theE1tag (a more frequent class thanS1) that level of precision is achieved with recallgreater than 90%.Constraints were derived at each of these oper-ating points and used within the Charniak parsingpipeline.
Fig.
3 shows the F-measure parsing per-formance using either S1or E1constraints at vari-ous values of c for preserving ck of the original kconstraints.
As can be seen from that graph, withimproved precision both types of constraints haveoperating points that achieve accuracy improve-ments over the baseline parser on the dev set underdefault parser settings.This accuracy improvement is similar to resultsobtained in Hollingshead and Roark (2007), wherebase phrase constraints from a finite-state chun-ker were used to achieve improved parse accuracy.Their explanation for the accuracy improvement,which seems to apply in this case as well, is thatthe first stage of the Charniak parser is still pass-ing the same number of edges in the chart to thesecond stage, but that the edges now come frommore promising parts of the search space, i.e., theparser does a better job of exploring good parts ofthe search space.
Hence the constraints seem to bedoing what they should do, which is constrain thesearch without unduly excluding good solutions.Note that these results are all achieved withthe default parsing parameterizations, so that ac-curacy gains are achieved, but not necessarily ef-ficiency gains.
The Charniak parser allows for2Perceptron weights were interpreted in the log domainand conditionally normalized appropriately.0 200 400 600 800 1000 12008687888990Seconds to parse development setF?measureparseaccuracyConstrained parserUnconstrained parserFigure 4: Speed/accuracy tradeoff for both the uncon-strained Charniak parser and when constrained with high pre-cision start/end constraints.narrow search parameterizations, whereby feweredges are added to the chart in the initial stage.Given the improved search using these constraints,high accuracy may be achieved at far narrowersearch parameterizations than the default setting ofthe parser.
To look at potential efficiency gainsto be had from these constraints, we chose themost constrained operating points for both startand end constraints that do not hurt accuracy rel-ative to the baseline parser (c = 0.7 for S1andc = 0.8 for E1) and used both kinds of constraintsin the parser.
We then ran the Charniak parser withvarying search parameters, to observe performancewhen search is narrower than the default.
Fig.
4presents F-measure accuracy for both constrainedand unconstrained parser configurations at varioussearch parameterizations.
The times for the con-strained parser configurations include the approx-imately 20 seconds required for POS-tagging andword-boundary classification of the dev set.These results demonstrate a sharper knee of thecurve for the constrained runs, with parser accu-racy that is above that achieved by the uncon-strained parser under the default search parameter-ization, even after a nearly 5 times speedup.5.3 Analysis of constraints on 1-best parsesThere are two ways in which the constraints couldbe improving parser performance: by helping theparser to find higher probability parses that it wasformerly losing because of search errors; or bynot allowing the parser to select high probabilityparses that violate the constraints.
To get a senseof whether the constraints on the parser are sim-ply fixing search errors or are imposing constraintson the model itself, we examined the 1-best parsesfrom both constrained and unconstrained scenar-ios.
First, we calculated the geometric mean ofthe 1-best parse probabilities under both scenarios,which were (in logs) ?207.99 for unconstrainedand ?208.09 for constrained.
Thus, the con-strained 1-best parses had very slightly less proba-bility than the unconstrained parses, indicating thatthe constraints were not simply fixing search er-749rors, but also eliminated some MAP parses.To get a sense of how often search errorswere corrected versus ruling out of MAP parses,we compared the constrained and unconstrainedparses at each string, and tallied when the uncon-strained parse probabilities were greater (or less)than the constrained parse probabilities, as well aswhen they were equal.
At the default search pa-rameterization (210), 84.8 percent of the stringshad the same parses; in 9.2 percent of the cases theunconstrained parses had higher probability; andin 5.9 percent of the cases the constrained parseshad higher probability.
The narrower search pa-rameterization at the knee of the curve in Fig.
4 hadsimilar results: 84.6 percent were the same; in 8.6percent of the cases the unconstrained probabilitywas higher; and in 6.8 percent of the cases the con-strained probability was higher.
Hence, when the1-best parse differs, the parse found via constraintshas a higher probability in approximately 40 per-cent of the cases.6 O(n2) complexity context-free parsingUsing sufficient S1and E1constraints of the sortwe have been investigating, we can achieve worst-case quadratic (instead of cubic) complexity.
Aproof, based on the CYK algorithm, is given inAppendix A, but we can make the key points here.First, cubic complexity of context-free inferenceis due to O(n2) chart cells and O(n) possiblechild configurations per cell.
If we ?close?
all butO(n) cells, the ?open?
cells will be processed withworst-case quadratic complexity (O(n) cells withO(n) possible child configurations per cell).
If wecan show that the remaining O(n2) ?closed?
cellseach can be processed within constant time, thenthe overall complexity is quadratic.
The proof inAppendix A shows that this is the case if closing acell is such that: when a cell (i, j) is closed, theneither all cells (i, k) for k>i are closed or all cells(k, j) for k<j are closed.
These conditions areachieved when we select sets S1and E1and closecells accordingly.Just as we were able to order word position loglikelihood scores for classes S1and E1to im-prove precision in the previous section, here wewill order them so that we can continue select-ing positions until we have guaranteed less thansome threshold of ?open?
cells.
If the thresholdis linear in the length of the string, we will beable to parse the string with worst-case quadraticcomplexity, as shown in Appendix A.
We will setour threshold to kn for some constant k (in ourexperiments, k ranges from 2 to 10).
Table 4presents the percentage of cells closed, class (S1and E1) precision and parser accuracy when thenumber of ?open?
cells is bounded to be less thanOpen % cells Class Parse accuracycells closed Prec LR LP Fall ?
?
88.6 89.2 88.910n 39.1 99.9 88.6 89.2 88.98n 50.4 99.9 88.6 89.2 88.96n 62.8 99.9 88.6 89.2 88.94n 75.7 99.8 88.8 89.4 89.12n 88.8 99.8 88.8 89.5 89.1Table 4: Varying constant k for kn ?open?
cells, yieldingO(n2) parsing complexity guaranteesthe threshold.
These results clearly demonstratethat such constraints can be placed on real context-free parsing problems without significant impact toaccuracy?in fact, with small improvements.We were quite surprised by these trials, fully ex-pecting these limits to negatively impact accuracy.The likely explanation is that the existing Char-niak search strategy itself is bounding processingin such a way that the additional constraints placedon the process do not interfere with standard pro-cessing.
Note that our approach closes a higherpercentage of cells in longer strings, which theCharniak pipeline already more severely prunesthan shorter strings.
Further, this approach appearsto be relying very heavily on E1constraints, hencehas very high precision of classification.While the Charniak parser may not be the idealframework within which to illustrate these worst-case complexity improvements, the lack of impair-ment to the parser provides strong evidence thatother parsers could make use of the resulting chartsto achieve significant efficiency gains.7 Conclusion & Future WorkIn this paper, we have presented a very simple ap-proach to constraining context-free chart parsingpipelines that has several nice properties.
First,it is based on a simple classification task thatcan achieve very high accuracy using very sim-ple models.
Second, the classifier output canbe straightforwardly used to constrain any chart-based context-free parser.
Finally, we have shown(in Appendix A) that ?closing?
sufficient cellswith these techniques leads to quadratic worst-casecomplexity bounds.
Our empirical results with theCharniak parser demonstrated that our classifierswere sufficiently accurate to allow for such boundsto be placed on the parser without hurting parsingaccuracy.Future work in this direction will involve tryingdifferent methods for defining effective operatingpoints, such as more heavily constraining longerstrings, in an attempt to further improve the searchin the Charniak parser.
We would also like to in-vestigate performance when using other chart pars-ing strategies, such as when using cell pruning in-stead of an agenda.750CYK(w1.
.
.
wn, G = (V, T, S?, P, ?))
 PCFG G must be in CNF1 for t = 1 to n do  scan in words/POS-tags (span=1)2 for j = 1 to |V | do3 ?j(t, t)?
P(Aj?
wt)4 for s = 2 to n do  all spans > 15 for t = 1 to n?s+1 do6 e?
t+s?1  end word position for this span7 for i = 1 to |V | do8?i(t, e)?
argmaxt<m?e?argmaxj,kP(Ai?
AjAk) ?j(t,m?
1) ?k(m, e)?9?i(t, e)?
maxt<m?e?maxj,kP(Ai?
AjAk) ?j(t,m?
1) ?k(m, e)?Figure 5: Pseudocode of a basic CYK algorithm for PCFG in Chomsky Normal Form (CNF).ReferencesCharniak, E. and M. Johnson.
2005.
Coarse-to-fine n-bestparsing and MaxEnt discriminative reranking.
In Proceed-ings of the 43rd Annual Meeting of the Association forComputational Linguistics (ACL), pages 173?180.Charniak, E. 2000.
A maximum-entropy-inspired parser.
InProceedings of the 1st Conference of the North AmericanChapter of the Association for Computational Linguistics,pages 132?139.Cocke, J. and J.T.
Schwartz.
1970.
Programming languagesand their compilers: Preliminary notes.
Technical report,Courant Institute of Mathematical Sciences, NYU.Collins, M.J. 2002.
Discriminative training methods for hid-den Markov models: Theory and experiments with per-ceptron algorithms.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Processing(EMNLP), pages 1?8.Earley, J.
1970.
An efficient context-free parsing algorithm.Communications of the ACM, 6(8):451?455.Glaysher, E. and D. Moldovan.
2006.
Speeding up full syn-tactic parsing by leveraging partial parsing decisions.
InProceedings of the COLING/ACL 2006 Main ConferencePoster Sessions, pages 295?300.Hollingshead, K. and B. Roark.
2007.
Pipeline iteration.
InProceedings of the 45th Annual Meeting of the Associationfor Computational Linguistics (ACL), pages 952?959.Kasami, T. 1965.
An efficient recognition and syntaxanalysis algorithm for context-free languages.
Technicalreport, AFCRL-65-758, Air Force Cambridge ResearchLab., Bedford, MA.Marcus, M.P., M.A.
Marcinkiewicz, and B. Santorini.
1993.Building a large annotated corpus of English: The Penntreebank.
Computational Linguistics, 19:313?330.Ratnaparkhi, A.
1999.
Learning to parse natural languagewith maximum entropy models.
Machine Learning, 34(1-3):151?175.Sha, F. and F. Pereira.
2003.
Shallow parsing with con-ditional random fields.
In Proceedings of HLT-NAACL,pages 134?141.Younger, D.H. 1967.
Recognition and parsing of context-free languages in time n3.
Information and Control,10(2):189?208.Appendix A Proof of quadraticcomplexity parsing with constraintsFor this proof, we will use the well-known CYKparsing algorithm, which makes use of grammarsin Chomsky Normal Form (CNF).
To achieveCNF, among other things, rules with more than 2children on the right-hand side must be factoredinto multiple binary rules.
To do this, compos-ite non-terminals are created in the factorizations,which represent incomplete constituents, i.e., thoseedges that require further combination to be madecomplete.3For example, if we have a rule pro-duction A ?
B C D in the context-free grammarG, then a new composite non-terminal would becreated, e.g., B-C, and two binary rules would re-place the previous ternary rule: A ?
B-C D andB-C ?
B C. The B-C non-terminal representspart of a rule expansion that needs to be combinedwith something else to produce a complete non-terminal from the original set of non-terminals.Let V?be the set of non-terminals that are cre-ated through factorization, which hence representincomplete edges.Fig.
5 shows pseudocode of a basic CYK algo-rithm for use with a probabilistic CFG in CNF,G = (V, T, S?, P, ?).
The function ?
maps fromrules in P to probabilities.
Lines 1-3 of the algo-rithm in Fig.
5 initialize the span 1 cells.
Lines 4-9are where the cubic complexity comes in: O(n)loops in line 4, each of which include O(n) loopsin line 5, each of which requires finding a max-imum over O(n) midpoints m in lines 8-9.
Foreach non-terminal Ai?
V at each cell (t, e), thealgorithm stores a backpointer ?i(t, e) in line 8, forefficiently extracting the maximum likelihood so-lution at the end of inference; and maximum prob-abilities ?i(t, e) in line 9, for use in the dynamicprogram.Given a set of word positions in the classes S1and E1, as defined in the main part of this paper,we can designate all cells (i, j) in the chart whereeither wi?
S1or wj?
E1to be ?closed?.
Chartcells that are not closed will be called ?open?.
Thetotal number of cells in the chart is (n2+ n)/2,and if we set a threshold on the maximum numberof open cells to be kn, the number of closed cellsmust be at least (n2+n)/2?kn.
Given an orderingof words (see ?6 for one approach), we can addwords to these sets one word at a time and close the3As before, we assume that edges are extended from left-to-right, which requires a left-factorization of the grammar.751QUADCYK(w1.
.
.
wn, G = (V, T, S?, P, ?
), V?, S1, E1)  PCFG G must be in CNF1 for t = 1 to n do  scan in words/POS-tags (span=1)2 for j = 1 to |V | do3 ?j(t, t)?
P(Aj?
wt)4 for s = 2 to n do  all spans > 15 for t = 1 to n?s+1 do6 e?
t+s?1  end word position for this span7 if wt?
S1CONTINUE  start position t ?closed?8 else if we?
E1 end position e ?closed?9 for i = 1 to |V | do10 if Ai6?
V?CONTINUE  only ?incomplete?
factored non-terminals (V?
)11?i(t, e)?
argmaxj,kP(Ai?
AjAk) ?j(t, e?
1) ?k(e, e)12?i(t, e)?
maxj,kP(Ai?
AjAk) ?j(t, e?
1) ?k(e, e)13 else  chart cell (t, e) ?open?14 for i = 1 to |V | do15?i(t, e)?
argmaxt<m?e?argmaxj,kP(Ai?
AjAk) ?j(t,m?
1) ?k(m, e)?16?i(t, e)?
maxt<m?e?maxj,kP(Ai?
AjAk) ?j(t,m?
1) ?k(m, e)?Figure 6: Pseudocode of a modified CYK algorithm, with quadratic worst case complexity with O(n) ?open?
cells.
Inaddition to string and grammar, it requires specification of factored non-terminal set V?and position constraints (S1, E1).related cells, until the requisite number of closuresare achieved.
Then the resulting sets of S1wordpositions andE1word positions can be provided tothe parsing algorithm, in addition to the grammarG and the set of factored non-terminals V?.Fig.
6 shows pseudocode of a modified CYK al-gorithm that takes into account S1and E1wordclasses.
Lines 1-6 of the algorithm in Fig.
6 areidentical to those in the algorithm in Fig.
5.
Atline 7, we have identified the chart cell being pro-cessed, which is (t, e).
If wt?
S1then the cell iscompletely closed, and there is nothing to do.
Oth-erwise, if we?
E1(lines 8-12), then factored non-terminals from V?can be created in that cell byfinding legal combinations of children categories.If neither of these conditions hold, then the cell isopen (lines 13-16) and processing occurs as in thestandard CYK algorithm (lines 14-16 of the algo-rithm in Fig.
6 are identical to lines 7-9 in Fig.
5).If the number of ?open?
cells is less than kn forsome constant k, then we can prove that the algo-rithm in Fig.
6 is O(n2) when given a left-factoredgrammar in CNF.
A key part of the proof rests ontwo lemmas:Lemma 1: Let V?be the set of composite non-terminals created when left-factoring a CFGto be in CNF, as described earlier.
Then, forany production Ai?
AjAkin the grammar,Ak6?
V?.Proof: With left-factoring, any k-ary productionA ?
A1.
.
.
Ak?1Akresults in new non-terminalsthat concatenate the first k ?
1 non-terminals onthe right-hand side.
These factored non-terminalsare always the leftmost child in the new produc-tion, hence no second child in the resulting CNFgrammar can be a factored non-terminal.2Lemma 2: For a cell (t, e) in the chart, ifwe?
E1, then the only possible midpoint mfor creating an entry in the cell is e.Proof: Placing an entry in cell (t, e) requires a ruleAi?
AjAk, an Ajentry in cell (t,m?1) and anAkentry in cell (m, e).
Suppose there is an Aken-try in cell (m, e) for m < e. Recall that we?
E1,hence the cell (m, e) is closed to non-terminals notin V?.
By Lemma 1, Ak6?
V?, therefore the cell(m, e) is closed to Akentries.
This is a contradic-tion.
Therefore, the lemma is proved.2Theorem: Let O be the set of cells (t, e) suchthat wt6?
S1and we6?
E1(?open?
cells).If |O| < kn for some constant k, where n isthe length of the string, then the algorithm inFig.
6 has worst case complexity O(n2).Proof: Lines 4 and 5 of the algorithm in Fig.
6loop throughO(n2) cells (t, e), for which there arethree cases: wt?
S1(line 7 of Fig.
6); we?
E1(lines 8-12); and (t, e) ?
O (lines 13-16).Case 1: wt?
S1.
No further work to be done.Case 2: we?
E1.
There is a constant amount ofwork to be done, for the reason that there is onlyone possible midpoint m for binary children com-binations (namely e, as proved in Lemma 2), henceno need to perform the maximization over O(n)midpoints.Case 3: (t, e) ?
O.
As with standard CYK pro-cessing, there are O(n) possible midpoints m overwhich to maximize, hence O(n) work required.Only O(n) cells fall in case 3, hence the to-tal amount of work associated with the cells in Ois O(n2).
There are O(n2) cells associated withcases 1 and 2, each of which has a total amountof work bounded by a constant, hence the totalamount of work associated with the cells not inO is also O(n2).
Therefore the overall worst-casecomplexity of the algorithm under these conditionsis O(n2).
2752
