Hierarchical Non-Emitting Markov ModelsEr ic  Sven  R is tad  and  Rober t  G .
ThomasDepar tment  of Computer  SciencePr inceton  Un ivers i tyP r inceton ,  N J  08544-2087{ristad, rgt )?cs.
princeton, eduAbst ractWe describe a simple variant of the inter-polated Markov model with non-emittingstate transitions and prove that it is strictlymore powerful than any Markov model.Empirical results demonstrate that thenon-emitting model outperforms the inter-polated model on the Brown corpus andon the Wall Street Journal under a widerange of experimental conditions.
The non-emitting model is also much less prone toovertraining.1 I n t roduct ionThe Markov model has long been the core technol-ogy of statistical anguage modeling.
Many othermodels have been proposed, but none has offered abetter combination of predictive performance, com-putational efficiency, and ease of implementation.Here we add hierarchical non-emitting state tran-sitions to the Markov model.
Although the statesin our model remain Markovian, the model itselfis no longer Markovian because it can representunbounded dependencies in the state distribution.Consequently, the non-emitting Markov model isstrictly more powerful than any Markov model, in-cluding the context model (Rissanen, 1983; Rissa-nen, 1986), the backoff model (Cleary and Witten,1984; Katz, 1987), and the interpolated Markovmodel (Jelinek and Mercer, 1980; MacKay and Peto,1994).More importantly, the non-emitting model consis-tently outperforms the interpolated Markov modelon natural language texts, under a wide range ofexperimental conditions.
We believe that the su-perior performance of the non-emitting model isdue to its ability to better model conditional inde-pendence.
Thus, the non-emitting model is betterable to represent both conditional independence andlong-distance dependence, ie., it is simply a betterstatistical model.
The non-emitting model is alsonearly as computationally eff?cient and easy to im-plement as the interpolated model.The remainder of our article consists of four sec-tions.
In section 2, we review the interpolatedMarkov model and briefly demonstrate hat all inter-polated models are equivalent to some basic Markovmodel of the same model order.
Next, we introducethe hierarchical non-emitting Markov model in sec-tion 3, and prove that even a lowly second ordernon-emitting model is strictly more powerful thanany basic Markov model, of any model order.
Insection 4, we report empirical results for the inter-polated model and the non-emitting model on theBrown corpus and Wall Street Journal.
Finally, insection 5 we conjecture that the empirical success ofthe non-emitting model is due to its ability to bet-ter model a point of apparent independence, such asmay occur at a sentence boundary.Our notation is as follows.
Let A be a finite alpha-bet of distinct symbols, \[A\[ = k, and let z T 6 A Tdenote an arbitrary string of length T over the al-phabet A.
Then z~ denotes the substring of z T thatbegins at position i and ends at position j.
For con-venience, we abbreviate the unit length substring z~as zi and the length t prefix of z T as z*.2 BackgroundHere we review the basic Markov model and the in-terpolated Markov model, and establish their equiv-alence.A basic Markov model ?
= (A,n,6,)  consists ofan alphabet A, a model order n, n > 0, and thestate transition probabilities 6, : A n x A ---* \[0, 1\].With probability 6,(y\[zn), a Markov model in thestate z '~ will emit the symbol y and transition to thestate z '~y.
Therefore, the probability Prn(ZtlX t-1 , ?
)assigned by an order n basic Markov model ?
to asymbol z '  in the history z t-1 depends only on thelast n symbols of the history.?
,'~ I ,T t - l \  pm(z, lz ' - l ,?
)=~.~ ,I , - .
J  (1)An interpolated Markov model ?
= (A,n,A,6)consists of a finite alphabet A, a maximal model or-der n, the state transition probabilities 6 = 60 .. .
6,,6i : A i x A ~ \[0, 1\], and the state-conditional inter-polation parameters A = A0... An, Ai : A i ---* \[0, 1\].381The probability assigned by an interpolated modelis a linear combination of the probabilities assignedby all the lower order Markov models.p0(yl ', ? )
=+(1 - Ai(zi))p?
(ylz~, ?)
(2)where )q(z i) = 0 for i > n, and and thereforep~(z, lzt-1, ?)
,-7 = p?(ztlzt_,~,?
), ie., the predictiondepends only on the last n symbols of the history.In the interpolated model, the interpolation pa-rameters mooth the conditional probabilities esti-mated from longer histories with those estimatedfrom shorter histories (:lelinek and Mercer, 1980).Longer histories upport stronger predictions, whileshorter histories have more accurate statistics.
In-terpolating the predictions from histories of differentlengths results in more accurate predictions than canbe obtained from any fixed history length.A quick glance at the form of (2) and (1) re-veals the fundamental simplicity of the interpolatedMarkov model.
Every interpolated model ?
is equiv-alent to some basic Markov model ?'
(temma 2.1),and every basic Markov model ?
is equivalent osome interpolated context model ?'
(lemma 2.2).Lemma 2.1V?
3qJ' VZ T E A* ~m(ZTI?
',T) : pe(zTI?,T)\]Proof .
We may convert the interpolated model ?into a basic model ?'
of the same model order n,simply by setting 6"(ylz n) equal to pc(y\[z n, ?)
forall states z n E A n and symbols y 6 A.
\[\]Lemma 2.2V?
~?t vzT 6 A* \[pc(zTI?
',T) = pm(xT\]?,T)\]Proof .
Every basic model is equivalent to an inter-polated model whose interpolation values are unityfor states of order n. \[\]The lemmas uffice to establish the following the-orem.Theorem 1 The class of interpolated Markov mod-els is equivalent to the class of basic Markov models.Proof .
By lemmas 2.1 and 2.2. f"lA similar argument applies to the backoff model.Every backoff model can be converted into an equiv-alent basic model, and every basic model is a backoffmodel.3 Non-Emi t t ing  Markov  Mode lsA hierarchical non-emitting Markov model ?
=(A,n, A,5) consists of an alphabet A, a maximalmodel order n, the state transition probabilities,5 = 5o...6n, 6i : A i x A ~ \[0,1\], and the non-emitting state transition probabilities A = A0 .
.
.
An,hi : A i ---* \[0, 1\].
With probability 1 - Ai(zi), a non-emitting model will transition from the state z i tothe state z~ without emitting a symbol.
With proba-bility A/(z')~i (Y\[Z i), a .non-emitting model will tran-sition from the state z* to the state z 'y  and emit thesymbol y.Therefore, the probability pe(yJ \[z i, ?)
assigned toa string yJ in the history x i by a non-emitting model?
has the recursive form (3),=+(1  - ?
)(3)where Ai(z i) = 0 for i > n and A0(e) = 1.
Note that,unlike the basic Markov model, p~(zt lzt - l ,?)
#t--1 pe(ztlzt_n, ?)
because the state distribution of thenon-emitting model depends on the prefix zi-n:This simple fact will allow us to establish that thereexists a non-emitting model that is not equivalent toany basic model.Lemma 3.1 states that there exists a non-emittingmodel ?
that cannot be converted into an equivalentbasic model of any order.
There will always be astring z T that distinguishes the non-emitting model?
from any given basic model ?'
because the non-emitting model can encode unbounded ependenciesin its state distribution.Lemma 3.13?
V?'
3z T E A* \[p,(zTI?,T) # pm(zT\[?
',T)\]Proof .
The idea of the proof is that our non-emitting model will encode the first symbol Zl ofthe string z T in its state distribution, for an un-bounded distance.
This will allow it to predict thelast symbol ZT using its knowledge of the first sym-bol zl.
The basic model will only be able predict thelast symbol ZT using the preceding n symbols, andtherefore when T is greater than n, we can arrangefor p, (zT l?
,T)  to differ from any p,~(zT\[?
',T), sim-ply by our choice of zl.The smallest non-emitting model capable of ex-hibiting the required behavior has order 2.
Thenon-emitting transition probabilities A and the in-terior of the string z T-1 will be chosen so that thenon-emitting model is either in an order 2 state oran order 0 state, with no way to transition from oneto the other.
The first symbol zl will determinewhether the non-emitting model goes to the order 2state or stays in the order 0 state.
No matter whatprobability the basic model assigns to the final sym-bol ZT, the non-emitting model can assign a differentprobability by the appropriate choice of Zl, 6O(ZT),andConsider the second order non-emitting modelover a binary alphabet with )~(0) = 1, A(1) = 0, andA(ll) = 1 on strings in AI 'A .
When zl = 0, then x2will be predicted using the 1st order model 61(x21xl),and all subsequent zt will be predicted by the secondorder model 62(ztlxtt_-~).
When zl = 0, then all sub-sequent z, will be predicted by the 0th order modelt -1  ~5o(xt).
Thus for all t > p, pc(x~\[x ~-x) ?
p~(t\[xt_v)for any fixed p, and no basic model is equivalent tothis simple non-emitting model.
\[\]It is obvious that every basic model is also a non-emitting model, with the appropriate choice of non-382emitting transition probabilities.Lemma 3.2V?
3~' V2: T E A* \[pe(xTJ?
',T) = prn(zTl?,T)\]These lemmas suffice to establish the followingtheorem.Theorem 2 The class of non-emitting Markovmodels is strictly more powerful than the class of ba-sic Markov models, because it is able to represent alarger class of probability distributions on strings.Proof .
By lemmas 3.1 and 3.2. r-ISince interpolated models and backoff models areequivalent o basic Markov models, we have asa corollary that non-emitting Markov models arestrictly more powerful than interpolated models andbackoff models as well.
Note that non-emittingMarkov models are considerably less powerful thanthe full class of stochastic finite state automaton(SFSA) because their states are Markovian.
Non-emitting models are also less powerful than the full:class of hidden Markov models.Algorithms to evaluate the probability of a stringaccording to a non-emitting model, and to opti-mize the non-emitting state transitions on a train-ing corpus are provided in related work (Ristad andThomas, 1997).4 Empi r i ca l  Resu l tsThe ultimate measure of a statistical model is itspredictive performance in the domain of interest.To take the true measure of non-emitting modelsfor natural language texts, we evaluate their per-formance as character models on the Brown corpus(Francis and Kucera, 1982) and as word models onthe Wall Street Journal.
Our results show that thenon-emitting Markov model consistently gives bet.terpredictions than the traditional interpolated Markovmodel under equivalent experimental conditions: Inall cases we compare non-emitting and interpolatedmodels of identical model orders, with the samenumber of parameters.
Note that the non-emittingbigram and the interpolated bigram are equivalent.Corpus Size Alphabet BlocksBrown 6,004,032 90 21WSJ 1989 6,219,350 20,293 22WSJ 1987-89 42,373,513 20,092 152All ,~ values were initialized uniformly to 0.5 andthen optimized using deleted estimation on the first90% of each corpus (Jelinek and Mercer, 1980).DEr.ET~D-ESTIMATIoN(B,?)1.
Until convergence2.
Initialize A+,,~- to zero;3.
For each block Bi in B4.
Initialize 6 using B - Bi;5.
EXPECTATION-STEP( Bi ,?,~ +,~- );6.
MAXIMIZATION-STEP(~b,~+ ,)~- );7.Initialize ~ using B;Here ,~+ (zi) accumulates the expectations ofemit-ting a, symbol from state z i while )~-(zi) accumu-lates the expectations of transitioning to the statez~ without emitting a symbol.The remaining 10% percent of each corpus wasused to evaluate model performance.
No parametertying was performed.14.1 Brown CorpusOur first set of experiments were with charactermodels on the Brown corpus.
The Brown cor-pus is an eclectic collection of English prose, con-taining 6,004,032 characters partitioned into 500files.
Deleted estimation used 21 blocks.
Re-sults are reported as per-character test messageentropies (bits/char), - L log  2p(yvjv).
The non-t lemitting model outperforms the interpolated modelfor all nontrivial model orders, particularly for largerm.odel orders.
The non-emitting model is consider-ably less prone to overtraining.
After 10 EM itera-tions, the order 9 non-emitting model scores 2.0085bits/char while the order 9 interpolated model scores2.3338 bits/char after 10 EM iterations.Bto~,m Comus3.B .
.
.
.
.
.
.
N<~..e~,nlng I do~k Be~ EM Itorltio~1 -e--- 6 ~1~ Inta~t~lno Model: ~ i I  EM hemtio~ ~-, 3.
Not~emJflJn Mod~l: 10th~Mlte/itlon .
--"Interpo4ate~ Model: lOtPI EM neritk)41 -m-- I \3"4 f ~ .
~2 J2.~~ - ~ .
.
:  .......
:-..---.
: .....2t i i i s a i 1.8 2 3 4 5 6 7 8~ol  On~rFigure 1: Test message ntropies as a function ofmodel order on the Brown corpus.4.2 WSJ  1989The second set of exPeriments was on the 1989Wall Street Journal corpus, which contains 6,219,350words.
Our vocabulary consisted of the 20,293words that occurred at least 10 times in the en-tire WSJ 1989 corpus.
All out-of-vocabulary words1 In forthcoming work, we compare the performance ofthe interpolated and non-emitting models on the Browncorpus and Wall Street Journal with ten different pa-rameter tying schemes.
Our experiments confirm thatsome parameter tying schemes improve model perfor-mance, although only slightly.
The non-emitting modelconsistently outperformed the interpolated model on allthe corpora for all the parameter tying schemes that weevaluated.383WS..I 1987-'89160 were mapped to a unique OOV symbol.
Deletedestimation used 22 blocks.
Following standard prac-tice in the speech recognition community, resultsare reported as per-word test message perplexities,p(yVlv)-?.
Again, the non-emitting model outper-forms the interpolated Markov model for all nontriv-ial model orders.WSJ 1989 .-- , , ,Norl-emc~ng Model: But  EM It or=tk~Intsrp~ated Model: ~ EM I~er~ion ~- -170160150140 *,~,30 'k11o "*~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Ioo i i " L i ,,1 2 Model30;,der 4Figure 2: Test message perplexities as a function ofmodel order on WSJ 1989.4.3 WSJ  1987-89The third set of experiments was on the 1987-89 WallStreet Journal corpus, which contains 42,373,513words.
Our vocabulary consisted of the 20,092 wordsthat occurred at least 63 times in the entire WSJ1987-89 corpus.
Again, all out-of-vocabulary wordswere mapped to a unique OOV symbol.
Deleted es-timation used 152 blocks.
Results are reported astest message perplexities.
As with the WS3 1989corpus, the non-emitting model outperforms the in-terpolated model for all nontrivial model orders.5 Conc lus ionThe power of the non-emitting model comes fromits ability to represent additional information in itsstate distribution.
In the proof of lemma 3.1 above,we used the state distribution to represent a long dis-tance dependency.
We conjecture, however, that theempirical success of the non-emitting model is dueto its ability to remember to ignore (ie., to forget) amisleading history at a point of apparent indepen-dence.A point of apparent independence occurs whenwe have adequate statistics for two strings z n-1 andyn but not yet for their concatenation z,,- lyn.
Inthe most extreme case, the frequencies of z n-1 andyn are high, but the frequency of even the medialbigram zn-lyl is low.
In such a situation, we wouldlike to ignore the entire history z n-1 when predictingy'~, because all di(yjlxn-l~ -1) will be close to zeroxJJ;SO1401201101009080Non-4mitting Modot: Be=t EM #erat)o41Lnterpolatod Moflel: Best EM Itorlt~on ~-Figure 3: Test message perplexities as a function ofmodel order on WSJ 1987-89.for i < n. To simplify the example, we assume that6(y j lz~- l~ -1) = 0 for j _> 1 and i < n.In such a situation, the interpolated model mustrepeatedly transition past some suffix of the historyz ~-1 for each of the next n -1  predictions, and so thetotal probability assigned to pc(y nle) by the interpo-lated model is a product of n(n - 1)/2 probabilities.po(y~ I ~"-~ )"-~ ))\] = \[i=~l(1-A(x~ *-1 P(Y~I~)n- -1  \]...(1 - a(~_~yi~-l))p(yn l  ~-~)F,,-I r ' .
-- i  \]:" \[k~=li~= (1--A(X'~-ly~-I)) Pc(Yn'~)(4)In contrast, the non-emitting model will imme-diately transition to the empty context in order topredict the first symbol Yl, and then it need neveragain transition past any suffix of x n-\].
Conse-quently, the total probability assigned to pe(yn\[e)by the non-emitting model is a product of only n -  1probabilities.n--1 \]Given the same state transition probabilities, notethat (4) must be considerably ess than (5) becauseprobabilities lie in \[0, 1\].
Thus, we believe that theempirical success of the non-emitting model comesfrom its ability to effectively ignore a misleading his-tory rather than from its ability to remember distantevents.384Finally, we note the use of hierarchical non-emitting transitions i a general technique that maybe employed in any time series model, including con-text models and backoff models.AcknowledgmentsBoth authors are partially supported by YoungInvestigator Award IRI-0258517 to Eric Ristad fromthe National Science Foundation.Re ferencesLalit R. Bahl, Peter F. Brown, Peter V. de Souza,Robert L. Mercer, and David Nahamoo.
1991.
Afast algorithm for deleted interpolation.
In Proc.EUROSPEECH '91, pages 1209-1212, Genoa.J.G.
Cleary and I.H.
Witten.
1984.
Data com-pression using adaptive coding and partial stringmatching.
IEEE Trans.
Comm., COM-32(4):396-402.W.
Nelson Francis and Henry Kucera.
1982.
Fre-quency analysis of English usage: lexicon andgrammar.
Houghton Mifflin, Boston.Fred Jelinek and Robert L. Mercer.
1980.
Inter-polated estimation of Markov source parametersfrom sparse data.
In Edzard S. Gelsema andLaveen N. Kanal, editors, Pattern Recognition inPractice, pages 381-397, Amsterdam, May 21-23.North Holland.Slava Katz.
1987.
Estimation of probabilities fromsparse data for the language model component ofa speech recognizer.
IEEE Trans.
ASSP, 35:400-401.David J.C. MacKay and Linda C. Bauman Peto.1994.
A hierarchical Dirichlet language model.Natural Language Engineering, 1(1).Jorma Rissanen.
1983.
A universal data compres-sion system.
IEEE Trans.
Information Theory,IT-29(5):656-664.Jorma Rissanen.
1986.
Complexity of strings in theclass of Markov sources.
IEEE Trans.
InformationTheory, IT-32(4):526-532.Eric Sven Ristad and Robert G. Thomas.
1997.
Hi-erarchical non-emitting Markov models.
Techni-cal Report CS-TR-544-96, Department of Com-puter Science, Princeton University, Princeton,NJ, March.Frans M. J. Willems, Yuri M. Shtarkov, andTjalling J. Tjalkens.
1995.
The context-treeweighting method: basic properties.
IEEE Trans.Inf.
Theory, 41(3):653-664.385
