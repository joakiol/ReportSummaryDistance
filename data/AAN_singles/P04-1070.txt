An alternative method of training probabilistic LR parsersMark-Jan NederhofFaculty of ArtsUniversity of GroningenP.O.
Box 716NL-9700 AS GroningenThe Netherlandsmarkjan@let.rug.nlGiorgio SattaDept.
of Information EngineeringUniversity of Paduavia Gradenigo, 6/AI-35131 PadovaItalysatta@dei.unipd.itAbstractWe discuss existing approaches to train LR parsers,which have been used for statistical resolution ofstructural ambiguity.
These approaches are non-optimal, in the sense that a collection of probabilitydistributions cannot be obtained.
In particular, someprobability distributions expressible in terms of acontext-free grammar cannot be expressed in termsof the LR parser constructed from that grammar,under the restrictions of the existing approaches totraining of LR parsers.
We present an alternativeway of training that is provably optimal, and that al-lows all probability distributions expressible in thecontext-free grammar to be carried over to the LRparser.
We also demonstrate empirically that thiskind of training can be effectively applied on a largetreebank.1 IntroductionThe LR parsing strategy was originally devisedfor programming languages (Sippu and Soisalon-Soininen, 1990), but has been used in a wide rangeof other areas as well, such as for natural languageprocessing (Lavie and Tomita, 1993; Briscoe andCarroll, 1993; Ruland, 2000).
The main differencebetween the application to programming languagesand the application to natural languages is that inthe latter case the parsers should be nondetermin-istic, in order to deal with ambiguous context-freegrammars (CFGs).
Nondeterminism can be han-dled in a number of ways, but the most efficientis tabulation, which allows processing in polyno-mial time.
Tabular LR parsing is known from thework by (Tomita, 1986), but can also be achievedby the generic tabulation technique due to (Lang,1974; Billot and Lang, 1989), which assumes an in-put pushdown transducer (PDT).
In this context, theLR parsing strategy can be seen as a particular map-ping from context-free grammars to PDTs.The acronym ?LR?
stands for ?Left-to-right pro-cessing of the input, producing a Right-most deriva-tion (in reverse)?.
When we construct a PDTA froma CFG G by the LR parsing strategy and apply it onan input sentence, then the set of output strings ofArepresents the set of all right-most derivations that Gallows for that sentence.
Such an output string enu-merates the rules (or labels that identify the rulesuniquely) that occur in the corresponding right-mostderivation, in reversed order.If LR parsers do not use lookahead to decide be-tween alternative transitions, they are called LR(0)parsers.
More generally, if LR parsers look ahead ksymbols, they are called LR(k) parsers; some sim-plified LR parsing models that use lookahead arecalled SLR(k) and LALR(k) parsing (Sippu andSoisalon-Soininen, 1990).
In order to simplify thediscussion, we abstain from using lookahead in thisarticle, and ?LR parsing?
can further be read as?LR(0) parsing?.
We would like to point out how-ever that our observations carry over to LR parsingwith lookahead.The theory of probabilistic pushdown automata(Santos, 1972) can be easily applied to LR parsing.A probability is then assigned to each transition, bya function that we will call the probability functionpA, and the probability of an accepting computa-tion of A is the product of the probabilities of theapplied transitions.
As each accepting computationproduces a right-most derivation as output string, aprobabilistic LR parser defines a probability distri-bution on the set of parses, and thereby also a prob-ability distribution on the set of sentences generatedby grammar G. Disambiguation of an ambiguoussentence can be achieved on the basis of a compari-son between the probabilities assigned to the respec-tive parses by the probabilistic LR model.The probability function can be obtained on thebasis of a treebank, as proposed by (Briscoe andCarroll, 1993) (see also (Su et al, 1991)).
Themodel by (Briscoe and Carroll, 1993) however in-corporated a mistake involving lookahead, whichwas corrected by (Inui et al, 2000).
As we will notdiscuss lookahead here, this matter does not play asignificant role in the current study.
Noteworthy isthat (Sornlertlamvanich et al, 1999) showed empir-ically that an LR parser may be more accurate thanthe original CFG, if both are trained on the basisof the same treebank.
In other words, the resultingprobability function pA on transitions of the PDTallows better disambiguation than the correspond-ing function pG on rules of the original grammar.A plausible explanation of this is that stack sym-bols of an LR parser encode some amount of leftcontext, i.e.
information on rules applied earlier, sothat the probability function on transitions may en-code dependencies between rules that cannot be en-coded in terms of the original CFG extended withrule probabilities.
The explicit use of left con-text in probabilistic context-free models was inves-tigated by e.g.
(Chitrao and Grishman, 1990; John-son, 1998), who also demonstrated that this maysignificantly improve accuracy.
Note that the prob-ability distributions of language may be beyond thereach of a given context-free grammar, as pointedout by e.g.
(Collins, 2001).
Therefore, the use of leftcontext, and the resulting increase in the number ofparameters of the model, may narrow the gap be-tween the given grammar and ill-understood mech-anisms underlying actual language.One important assumption that is made by(Briscoe and Carroll, 1993) and (Inui et al, 2000)is that trained probabilistic LR parsers should beproper, i.e.
if several transitions are applicable fora given stack, then the sum of probabilities as-signed to those transitions by probability functionpA should be 1.
This assumption may be moti-vated by pragmatic considerations, as such a propermodel is easy to train by relative frequency estima-tion: count the number of times a transition is ap-plied with respect to a treebank, and divide it bythe number of times the relevant stack symbol (orpair of stack symbols) occurs at the top of the stack.Let us call the resulting probability function prfe .This function is provably optimal in the sense thatthe likelihood it assigns to the training corpus ismaximal among all probability functions pA that areproper in the above sense.However, properness restricts the space of prob-ability distributions that a PDT allows.
This meansthat a (consistent) probability function pA may ex-ist that is not proper and that assigns a higher like-lihood to the training corpus than prfe does.
(By?consistent?
we mean that the probabilities of allstrings that are accepted sum to 1.)
It may evenbe the case that a (proper and consistent) probabil-ity function pG on the rules of the input grammar Gexists that assigns a higher likelihood to the corpusthan prfe , and therefore it is not guaranteed that LRparsers allow better probability estimates than theCFGs from which they were constructed, if we con-strain probability functions pA to be proper.
In thisrespect, LR parsing differs from at least one otherwell-known parsing strategy, viz.
left-corner pars-ing.
See (Nederhof and Satta, 2004) for a discus-sion of a property that is shared by left-corner pars-ing but not by LR parsing, and which explains theabove difference.As main contribution of this paper we establishthat this restriction on expressible probability dis-tributions can be dispensed with, without losing theability to perform training by relative frequency es-timation.
What comes in place of properness isreverse-properness, which can be seen as proper-ness of the reversed pushdown automaton that pro-cesses input from right to left instead of from left toright, interpreting the transitions of A backwards.As we will show, reverse-properness does not re-strict the space of probability distributions express-ible by an LR automaton.
More precisely, assumesome probability distribution on the set of deriva-tions is specified by a probability function pA ontransitions of PDT A that realizes the LR strat-egy for a given grammar G. Then the same prob-ability distribution can be specified by an alterna-tive such function p?A that is reverse-proper.
In ad-dition, for each probability distribution on deriva-tions expressible by a probability function pG for G,there is a reverse-proper probability function pA forA that expresses the same probability distribution.Thereby we ensure that LR parsers become at leastas powerful as the original CFGs in terms of allow-able probability distributions.This article is organized as follows.
In Sec-tion 2 we outline our formalization of LR pars-ing as a construction of PDTs from CFGs, makingsome superficial changes with respect to standardformulations.
Properness and reverse-propernessare discussed in Section 3, where we will showthat reverse-properness does not restrict the spaceof probability distributions.
Section 4 reports on ex-periments, and Section 5 concludes this article.2 LR parsingAs LR parsing has been extensively treated in exist-ing literature, we merely recapitulate the main defi-nitions here.
For more explanation, the reader is re-ferred to standard literature such as (Harrison, 1978;Sippu and Soisalon-Soininen, 1990).An LR parser is constructed on the basis of a CFGthat is augmented with an additional rule S?
?` S,where S is the former start symbol, and the newnonterminal S?
becomes the start symbol of theaugmented grammar.
The new terminal ` acts asan imaginary start-of-sentence marker.
We denotethe set of terminals by ?
and the set of nontermi-nals by N .
We assume each rule has a unique labelr.As explained before, we construct LR parsers aspushdown transducers.
The main stack symbolsof these automata are sets of dotted rules, whichconsist of rules from the augmented grammar witha distinguished position in the right-hand side in-dicated by a dot ???.
The initial stack symbol ispinit = {S?
?
` ?
S}.We define the closure of a set p of dotted rules asthe smallest set closure(p) such that:1. p ?
closure(p); and2.
for (B ?
?
?
A?)
?
closure(p) and A ??
a rule in the grammar, also (A ?
?
?)
?closure(p).We define the operation goto on a set p of dottedrules and a grammar symbol X ?
?
?N as:goto(p,X) = {A?
?X ?
?
|(A?
?
?
X?)
?
closure(p)}The set of LR states is the smallest set such that:1. pinit is an LR state; and2.
if p is an LR state and goto(p,X) = q 6= ?, forsome X ?
?
?N , then q is an LR state.We will assume that PDTs consist of three typesof transitions, of the form P a,b7?
P Q (a push tran-sition), of the form P a,b7?
Q (a swap transition), andof the form P Q a,b7?
R (a pop transition).
Here P , Qand R are stack symbols, a is one input terminal oris the empty string ?, and b is one output terminal oris the empty string ?.
In our notation, stacks growfrom left to right, so that P a,b7?
P Q means that Q ispushed on top of P .
We do not have internal statesnext to stack symbols.For the PDT that implements the LR strategy, thestack symbols are the LR states, plus symbols of theform [p;X], where p is an LR state andX is a gram-mar symbol, and symbols of the form (p,A,m),where p is an LR state, A is the left-hand side ofsome rule, and m is the length of some prefix of theright-hand side of that rule.
More explanation onthese additional stack symbols will be given below.The stack symbols and transitions are simultane-ously defined in Figure 1.
The final stack symbolis pfinal = (pinit , S?, 0).
This means that an inputa1 ?
?
?
an is accepted if and only if it is entirely readby a sequence of transitions that take the stack con-sisting only of pinit to the stack consisting only ofpfinal .
The computed output consists of the string ofterminals b1 ?
?
?
bn?
from the output components ofthe applied transitions.
For the PDTs that we willuse, this output string will consist of a sequence ofrule labels expressing a right-most derivation of theinput.
On the basis of the original grammar, the cor-responding parse tree can be constructed from suchan output string.There are a few superficial differences with LRparsing as it is commonly found in the literature.The most obvious difference is that we divide re-ductions into ?binary?
steps.
The main reason is thatthis allows tabular interpretation with a time com-plexity cubic in the length of the input.
Otherwise,the time complexity would be O(nm+1), where mis the length of the longest right-hand side of a rulein the CFG.
This observation was made before by(Kipps, 1991), who proposed a solution similar toours, albeit formulated differently.
See also a relatedformulation of tabular LR parsing by (Nederhof andSatta, 1996).To be more specific, instead of one step of thePDT taking stack:?p0p1 ?
?
?
pmimmediately to stack:?p0qwhere (A ?
X1 ?
?
?Xm ?)
?
pm, ?
is a stringof stack symbols and goto(p0, A) = q, we havea number of smaller steps leading to a series ofstacks:?p0p1 ?
?
?
pm?1pm?p0p1 ?
?
?
pm?1(A,m?1)?p0p1 ?
?
?
(A,m?2)...?p0(A, 0)?p0qThere are two additional differences.
First, wewant to avoid steps of the form:?p0(A, 0)?p0qby transitions p0 (A, 0)?,?7?
p0 q, as such transitionscomplicate the generic definition of ?properness?for PDTs, to be discussed in the following section.For this reason, we use stack symbols of the form[p;X] next to p, and split up p0 (A, 0)?,?7?
p0 q intopop [p0;X0] (A, 0)?,?7?
[p0;A] and push [p0;A]?,?7?
[p0;A] q.
This is a harmless modification, which in-creases the number of steps in any computation byat most a factor 2.Secondly, we use stack symbols of the form(p,A,m) instead of (A,m).
This concerns the con-ditions of reverse-properness to be discussed in the?
For LR state p and a ?
?
such that goto(p, a) 6= ?:pa,?7?
[p; a] (1)?
For LR state p and (A?
?)
?
p, where A?
?
has label r:p?,r7?
[p;A] (2)?
For LR state p and (A?
?
?)
?
p, where |?| = m > 0 and A?
?
has label r:p?,r7?
(p,A,m?
1) (3)?
For LR state p and (A?
?
?
X?)
?
p, where |?| = m > 0, such that goto(p,X) = q 6= ?
:[p;X] (q, A,m)?,?7?
(p,A,m?
1) (4)?
For LR state p and (A?
?
X?)
?
p, such that goto(p,X) = q 6= ?
:[p;X] (q, A, 0)?,?7?
[p;A] (5)?
For LR state p and X ?
?
?N such that goto(p,X) = q 6= ?:[p;X]?,?7?
[p;X] q (6)Figure 1: The transitions of a PDT implementing LR(0) parsing.following section.
By this condition, we considerLR parsing as being performed from right to left, sobackwards with regard to the normal processing or-der.
If we were to omit the first components p fromstack symbols (p,A,m), we may obtain ?dead ends?in the computation.
We know that such dead endsmake a (reverse-)proper PDT inconsistent, as proba-bility mass lost in dead ends causes the sum of prob-abilities of all computations to be strictly smallerthan 1.
(See also (Nederhof and Satta, 2004).)
Itis interesting to note that the addition of the compo-nents p to stack symbols (p,A,m) does not increasethe number of transitions, and the nature of LR pars-ing in the normal processing order from left to rightis preserved.With all these changes together, reductionsare implemented by transitions resulting in thefollowing sequence of stacks:??
[p0;X0][p1;X1] ?
?
?
[pm?1;Xm?1]pm??
[p0;X0][p1;X1] ?
?
?
[pm?1;Xm?1](pm, A,m?1)??
[p0;X0][p1;X1] ?
?
?
(pm?1, A,m?2)...??
[p0;X0](p1, A, 0)??[p0;A]??
[p0;A]qPlease note that transitions of the form[p;X] (q, A,m)?,?7?
(p,A,m?
1) may corre-spond to several dotted rules (A ?
?
?
X?)
?
p,with different ?
of length m and different ?.
If wewere to multiply such transitions for different ?
and?, the PDT would become prohibitively large.3 Properness and reverse-propernessIf a PDT is regarded to process input from left toright, starting with a stack consisting only of pinit ,and ending in a stack consisting only of pfinal , thenit seems reasonable to cast this process into a prob-abilistic framework in such a way that the sum ofprobabilities of all choices that are possible at anygiven moment is 1.
This is similar to how the notionof ?properness?
is defined for probabilistic context-free grammars (PCFGs); we say a PCFG is proper iffor each nonterminalA, the probabilities of all ruleswith left-hand side A sum to 1.Properness for PCFGs does not restrict the spaceof probability distributions on the set of parse trees.In other words, if a probability distribution can bedefined by attaching probabilities to rules, then wemay reassign the probabilities such that that PCFGbecomes proper, while preserving the probabilitydistribution.
This even holds if the input grammaris non-tight, meaning that probability mass is lostin ?infinite derivations?
(Sa?nchez and Bened?
?, 1997;Chi and Geman, 1998; Chi, 1999; Nederhof andSatta, 2003).Although CFGs and PDTs are weakly equiva-lent, they behave very differently when they are ex-tended with probabilities.
In particular, there seemsto be no notion similar to PCFG properness thatcan be imposed on all types of PDTs without los-ing generality.
Below we will discuss two con-straints, which we will call properness and reverse-properness.
Neither of these is suitable for all typesof PDTs, but as we will show, the second is moresuitable for probabilistic LR parsing than the first.This is surprising, as only properness has been de-scribed in existing literature on probabilistic PDTs(PPDTs).
In particular, all existing approaches toprobabilistic LR parsing have assumed propernessrather than anything related to reverse-properness.For properness we have to assume that for eachstack symbol P , we either have one or more tran-sitions of the form P a,b7?
P Q or P a,b7?
Q, or oneor more transitions of the form Q P a,b7?
R, but nocombination thereof.
In the first case, propernessdemands that the sum of probabilities of all transi-tions P a,b7?
P Q and P a,b7?
Q is 1, and in the secondcase properness demands that the sum of probabili-ties of all transitions Q P a,b7?
R is 1 for each Q.Note that our assumption above is without lossof generality, as we may introduce swap transitionsP?,?7?
P1 and P?,?7?
P2, where P1 and P2 are newstack symbols, and replace transitions P a,b7?
P Qand P a,b7?
Q by P1a,b7?
P1 Q and P1a,b7?
Q, andreplace transitions Q P a,b7?
R by Q P2a,b7?
R.The notion of properness underlies the normaltraining process for PDTs, as follows.
We assumea corpus of PDT computations.
In these computa-tions, we count the number of occurrences for eachtransition.
For each P we sum the total number ofall occurrences of transitions P a,b7?
P Q or P a,b7?
Q.The probability of, say, a transition P a,b7?
P Q isnow estimated by dividing the number of occur-rences thereof in the corpus by the above total num-ber of occurrences of transitions with P in the left-hand side.
Similarly, for each pair (Q,P ) we sumthe total number of occurrences of all transitions ofthe formQ P a,b7?
R, and thereby estimate the proba-bility of a particular transitionQ P a,b7?
R by relativefrequency estimation.
The resulting PPDT is proper.It has been shown that imposing properness iswithout loss of generality in the case of PDTsconstructed by a wide range of parsing strategies,among which are top-down parsing and left-cornerparsing.
This does not hold for PDTs constructed bythe LR parsing strategy however, and in fact, proper-ness for such automata may reduce the expressivepower in terms of available probability distributionsto strictly less than that offered by the original CFG.This was formally proven by (Nederhof and Satta,2004), after (Ng and Tomita, 1991) and (Wright andWrigley, 1991) had already suggested that creatinga probabilistic LR parser that is equivalent to an in-put PCFG is difficult in general.
The same difficultyfor ELR parsing was suggested by (Tendeau, 1997).For this reason, we investigate a practical alter-native, viz.
reverse-properness.
Now we have to as-sume that for each stack symbol R, we either haveone or more transitions of the form P a,b7?
R orQ Pa,b7?
R, or one or more transitions of the formPa,b7?
P R, but no combination thereof.
In the firstcase, reverse-properness demands that the sum ofprobabilities of all transitions P a,b7?
R or Q P a,b7?
Ris 1, and in the second case reverse-properness de-mands that the sum of probabilities of transitionsPa,b7?
P R is 1 for each P .
Again, our assumptionabove is without loss of generality.In order to apply relative frequency estimation,we now sum the total number of occurrences of tran-sitions P a,b7?
R or Q P a,b7?
R for each R, and wesum the total number of occurrences of transitionsPa,b7?
P R for each pair (P,R).We now prove that reverse-properness does notrestrict the space of probability distributions, bymeans of the construction of a ?cover?
grammarfrom an input CFG, as reported in Figure 2.
Thiscover CFG has almost the same structure as the PDTresulting from Figure 1.
Rules and transitions al-most stand in a one-to-one relation.
The only note-worthy difference is between transitions of type (6)and rules of type (12).
The right-hand sides of thoserules can be ?
because the corresponding transitionsare deterministic if seen from right to left.
Now itbecomes clear why we needed the components p instack symbols of the form (p,A,m).
Without it, onecould obtain an LR state q that does not match theunderlying [p;X] in a reversed computation.We may assume without loss of generality thatrules of type (12) are assigned probability 1, as aprobability other than 1 could be moved to corre-sponding rules of types (10) or (11) where stateq was introduced.
In the same way, we may as-sume that transitions of type (6) are assigned prob-ability 1.
After making these assumptions, we ob-tain a bijection between probability functions pA forthe PDT and probability functions pG for the coverCFG.
As was shown by e.g.
(Chi, 1999) and (Neder-hof and Satta, 2003), properness for CFGs does notrestrict the space of probability distributions, andthereby the same holds for reverse-properness forPDTs that implement the LR parsing strategy.It is now also clear that a reverse-proper LRparser can describe any probability distribution thatthe original CFG can.
The proof is as follows.Given a probability function pG for the input CFG,we define a probability function pA for the LRparser, by letting transitions of types (2) and (3)?
For LR state p and a ?
?
such that goto(p, a) 6= ?
:[p; a]?
p (7)?
For LR state p and (A?
?)
?
p, where A?
?
has label r:[p;A]?
p r (8)?
For LR state p and (A?
?
?)
?
p, where |?| = m > 0 and A?
?
has label r:(p,A,m?
1)?
p r (9)?
For LR state p and (A?
?
?
X?)
?
p, where |?| = m > 0, such that goto(p,X) = q 6= ?:(p,A,m?
1)?
[p;X] (q, A,m) (10)?
For LR state p and (A?
?
X?)
?
p, such that goto(p,X) = q 6= ?:[p;A]?
[p;X] (q, A, 0) (11)?
For LR state q:q ?
?
(12)Figure 2: A grammar that describes the set of computations of the LR(0) parser.
Start symbol is pfinal =(pinit , S?, 0).
Terminals are rule labels.
Generated language consists of right-most derivations in reverse.have probability pG(r), and letting all other transi-tions have probability 1.
This gives us the requiredprobability distribution in terms of a PPDT that isnot reverse-proper in general.
This PPDT can nowbe recast into reverse-proper form, as proven by theabove.4 ExperimentsWe have implemented both the traditional trainingmethod for LR parsing and the novel one, and havecompared their performance, with two concrete ob-jectives:1.
We show that the number of free parametersis significantly larger with the new trainingmethod.
(The number of free parameters isthe number of probabilities of transitions thatcan be freely chosen within the constraints ofproperness or reverse-properness.)2.
The larger number of free parameters does notmake the problem of sparse data any worse,and precision and recall are at least compara-ble to, if not better than, what we would obtainwith the established method.The experiments were performed on the WallStreet Journal (WSJ) corpus, from the Penn Tree-bank, version II.
Training was done on sections 02-21, i.e., first a context-free grammar was derivedfrom the ?stubs?
of the combined trees, taking partsof speech as leaves of the trees, omitting all af-fixes from the nonterminal names, and removing ?-generating subtrees.
Such preprocessing of the WSJcorpus is consistent with earlier attempts to deriveCFGs from that corpus, as e.g.
by (Johnson, 1998).The obtained CFG has 10,035 rules.
The dimen-sions of the LR parser constructed from this gram-mar are given in Table 1.The PDT was then trained on the trees from thesame sections 02-21, to determine the number oftimes that transitions are used.
At first sight it is notclear how to determine this on the basis of the tree-bank, as the structure of LR parsers is very differ-ent from the structure of the grammars from whichthey are constructed.
The solution is to construct asecond PDT from the PDT to be trained, replacingeach transition ?
a,b7?
?
with label r by transition?b,r7?
?.
By this second PDT we parse the tree-bank, encoded as a series of right-most derivationsin reverse.1 For each input string, there is exactlyone parse, of which the output is the list of usedtransitions.
The same method can be used for otherparsing strategies as well, such as left-corner pars-ing, replacing right-most derivations by a suitablealternative representation of parse trees.By the counts of occurrences of transitions, wemay then perform maximum likelihood estimationto obtain probabilities for transitions.
This canbe done under the constraints of properness or ofreverse-properness, as explained in the previoussection.
We have not applied any form of smooth-1We have observed an enormous gain in computational ef-ficiency when we also incorporate the ?shifts?
next to ?reduc-tions?
in these right-most derivations, as this eliminates a con-siderable amount of nondeterminism.total # transitions 8,340,315# push transitions 753,224# swap transitions 589,811# pop transitions 6,997,280Table 1: Dimensions of PDT implementing LRstrategy for CFG derived from WSJ, sect.
02-21.proper rev.-prop.# free parameters 577,650 6,589,716# non-zero probabilities 137,134 137,134labelled precision 0.772 0.777labelled recall 0.747 0.749Table 2: The two methods of training, based onproperness and reverse-properness.ing or back-off, as this could obscure properties in-herent in the difference between the two discussedtraining methods.
(Back-off for probabilistic LRparsing has been proposed by (Ruland, 2000).)
Alltransitions that were not seen during training weregiven probability 0.The results are outlined in Table 2.
Note that thenumber of free parameters in the case of reverse-properness is much larger than in the case of normalproperness.
Despite of this, the number of transi-tions that actually receive non-zero probabilities is(predictably) identical in both cases, viz.
137,134.However, the potential for fine-grained probabilityestimates and for smoothing and parameter-tyingtechniques is clearly greater in the case of reverse-properness.That in both cases the number of non-zero prob-abilities is lower than the total number of parame-ters can be explained as follows.
First, the treebankcontains many rules that occur a small number oftimes.
Secondly, the LR automaton is much largerthan the CFG; in general, the size of an LR automa-ton is bounded by a function that is exponential inthe size of the input CFG.
Therefore, if we use thesame treebank to estimate the probability function,then many transitions are never visited and obtain azero probability.We have applied the two trained LR automataon section 22 of the WSJ corpus, measuring la-belled precision and recall, as done by e.g.
(John-son, 1998).2 We observe that in the case of reverse-properness, precision and recall are slightly better.2We excluded all sentences with more than 30 words how-ever, as some required prohibitive amounts of memory.
Onlyone of the remaining 1441 sentences was not accepted by theparser.The most important conclusion that can be drawnfrom this is that the substantially larger space ofobtainable probability distributions offered by thereverse-properness method does not come at the ex-pense of a degradation of accuracy for large gram-mars such as those derived from the WSJ.
For com-parison, with a standard PCFG we obtain labelledprecision and recall of 0.725 and 0.670, respec-tively.3We would like to stress that our experimentsdid not have as main objective the improvement ofstate-of-the-art parsers, which can certainly not bedone without much additional fine-tuning and theincorporation of some form of lexicalization.
Ourmain objectives concerned the relation between ournewly proposed training method for LR parsers andthe traditional one.5 ConclusionsWe have presented a novel way of assigning proba-bilities to transitions of an LR automaton.
Theoreti-cal analysis and empirical data reveal the following.?
The efficiency of LR parsing remains unaf-fected.
Although a right-to-left order of read-ing input underlies the novel training method,we may continue to apply the parser from leftto right, and benefit from the favourable com-putational properties of LR parsing.?
The available space of probability distributionsis significantly larger than in the case of themethods published before.
In terms of thenumber of free parameters, the difference thatwe found empirically exceeds one order ofmagnitude.
By the same criteria, we can nowguarantee that LR parsers are at least as pow-erful as the CFGs from which they are con-structed.?
Despite the larger number of free parameters,no increase of sparse data problems was ob-served, and in fact there was a small increasein accuracy.AcknowledgementsHelpful comments from John Carroll and anony-mous reviewers are gratefully acknowledged.
Thefirst author is supported by the PIONIER ProjectAlgorithms for Linguistic Processing, funded byNWO (Dutch Organization for Scientific Research).The second author is partially supported by MIURunder project PRIN No.
2003091149 005.3In this case, all 1441 sentences were accepted.ReferencesS.
Billot and B. Lang.
1989.
The structure of sharedforests in ambiguous parsing.
In 27th AnnualMeeting of the Association for ComputationalLinguistics, pages 143?151, Vancouver, BritishColumbia, Canada, June.T.
Briscoe and J. Carroll.
1993.
Generalized prob-abilistic LR parsing of natural language (cor-pora) with unification-based grammars.
Compu-tational Linguistics, 19(1):25?59.Z.
Chi and S. Geman.
1998.
Estimation of prob-abilistic context-free grammars.
ComputationalLinguistics, 24(2):299?305.Z.
Chi.
1999.
Statistical properties of probabilisticcontext-free grammars.
Computational Linguis-tics, 25(1):131?160.M.V.
Chitrao and R. Grishman.
1990.
Statisticalparsing of messages.
In Speech and Natural Lan-guage, Proceedings, pages 263?266, Hidden Val-ley, Pennsylvania, June.M.
Collins.
2001.
Parameter estimation for sta-tistical parsing models: Theory and practice ofdistribution-free methods.
In Proceedings of theSeventh International Workshop on Parsing Tech-nologies, Beijing, China, October.M.A.
Harrison.
1978.
Introduction to Formal Lan-guage Theory.
Addison-Wesley.K.
Inui, V. Sornlertlamvanich, H. Tanaka, andT.
Tokunaga.
2000.
Probabilistic GLR parsing.In H. Bunt and A. Nijholt, editors, Advancesin Probabilistic and other Parsing Technologies,chapter 5, pages 85?104.
Kluwer Academic Pub-lishers.M.
Johnson.
1998.
PCFG models of linguistictree representations.
Computational Linguistics,24(4):613?632.J.R.
Kipps.
1991.
GLR parsing in time O(n3).
InM.
Tomita, editor, Generalized LR Parsing, chap-ter 4, pages 43?59.
Kluwer Academic Publishers.B.
Lang.
1974.
Deterministic techniques for ef-ficient non-deterministic parsers.
In Automata,Languages and Programming, 2nd Colloquium,volume 14 of Lecture Notes in Computer Science,pages 255?269, Saarbru?cken.
Springer-Verlag.A.
Lavie and M. Tomita.
1993.
GLR?
?
an efficientnoise-skipping parsing algorithm for context freegrammars.
In Third International Workshop onParsing Technologies, pages 123?134, Tilburg(The Netherlands) and Durbuy (Belgium), Au-gust.M.-J.
Nederhof and G. Satta.
1996.
Efficient tab-ular LR parsing.
In 34th Annual Meeting of theAssociation for Computational Linguistics, pages239?246, Santa Cruz, California, USA, June.M.-J.
Nederhof and G. Satta.
2003.
Probabilis-tic parsing as intersection.
In 8th InternationalWorkshop on Parsing Technologies, pages 137?148, LORIA, Nancy, France, April.M.-J.
Nederhof and G. Satta.
2004.
Probabilis-tic parsing strategies.
In 42nd Annual Meetingof the Association for Computational Linguistics,Barcelona, Spain, July.S.-K. Ng and M. Tomita.
1991.
Probabilistic LRparsing for general context-free grammars.
InProc.
of the Second International Workshop onParsing Technologies, pages 154?163, Cancun,Mexico, February.T.
Ruland.
2000.
A context-sensitive model forprobabilistic LR parsing of spoken languagewith transformation-based postprocessing.
InThe 18th International Conference on Compu-tational Linguistics, volume 2, pages 677?683,Saarbru?cken, Germany, July?August.J.-A.
Sa?nchez and J.-M.
Bened??.
1997.
Consis-tency of stochastic context-free grammars fromprobabilistic estimation based on growth trans-formations.
IEEE Transactions on Pattern Anal-ysis and Machine Intelligence, 19(9):1052?1055,September.E.S.
Santos.
1972.
Probabilistic grammars and au-tomata.
Information and Control, 21:27?47.S.
Sippu and E. Soisalon-Soininen.
1990.
ParsingTheory, Vol.
II: LR(k) and LL(k) Parsing, vol-ume 20 of EATCS Monographs on TheoreticalComputer Science.
Springer-Verlag.V.
Sornlertlamvanich, K. Inui, H. Tanaka, T. Toku-naga, and T. Takezawa.
1999.
Empirical sup-port for new probabilistic generalized LR pars-ing.
Journal of Natural Language Processing,6(3):3?22.K.-Y.
Su, J.-N. Wang, M.-H. Su, and J.-S. Chang.1991.
GLR parsing with scoring.
In M. Tomita,editor, Generalized LR Parsing, chapter 7, pages93?112.
Kluwer Academic Publishers.F.
Tendeau.
1997.
Analyse syntaxique etse?mantique avec e?valuation d?attributs dansun demi-anneau.
Ph.D. thesis, University ofOrle?ans.M.
Tomita.
1986.
Efficient Parsing for NaturalLanguage.
Kluwer Academic Publishers.J.H.
Wright and E.N.
Wrigley.
1991.
GLR pars-ing with probability.
In M. Tomita, editor, Gen-eralized LR Parsing, chapter 8, pages 113?128.Kluwer Academic Publishers.
