Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 1324?1334, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsUnambiguity Regularization for Unsupervised Learning of ProbabilisticGrammarsKewei Tu?Departments of Statistics and Computer ScienceUniversity of California, Los AngelesLos Angeles, CA 90095, USAtukw@ucla.eduVasant HonavarDepartment of Computer ScienceIowa State UniversityAmes, IA 50011, USAhonavar@cs.iastate.eduAbstractWe introduce a novel approach named unam-biguity regularization for unsupervised learn-ing of probabilistic natural language gram-mars.
The approach is based on the observa-tion that natural language is remarkably unam-biguous in the sense that only a tiny portion ofthe large number of possible parses of a nat-ural language sentence are syntactically valid.We incorporate an inductive bias into gram-mar learning in favor of grammars that leadto unambiguous parses on natural languagesentences.
The resulting family of algorithmsincludes the expectation-maximization algo-rithm (EM) and its variant, Viterbi EM, as wellas a so-called softmax-EM algorithm.
Thesoftmax-EM algorithm can be implementedwith a simple and computationally efficientextension to standard EM.
In our experimentsof unsupervised dependency grammar learn-ing, we show that unambiguity regularizationis beneficial to learning, and in combinationwith annealing (of the regularization strength)and sparsity priors it leads to improvementover the current state of the art.1 IntroductionMachine learning offers a potentially powerful ap-proach to learning probabilistic grammars from data.Because of the high cost of manual sentence anno-tation, there is substantial interest in unsupervisedgrammar learning, i.e., the induction of a grammarfrom a corpus of unannotated sentences.
The sim-plest such approaches attempt to maximize the like-?Part of the work was done while at Iowa State University.lihood of the grammar given the training data, typi-cally using expectation-maximization (EM) (Baker,1979; Lari and Young, 1990; Klein and Manning,2004).
More recent approaches incorporate addi-tional prior information of the target grammar intolearning.
For example, Kurihara and Sato (2004)used Dirichlet priors over rule probabilities to obtainsmoothed estimates of the probabilities.
Johnson etal.
(2007) used Dirichlet priors with hyperparame-ters set to values less than 1 to encourage sparsityof grammar rules.
Finkel et al(2007) and Liang etal.
(2007) proposed to use the hierarchical Dirichletprocess prior to bias learning towards concise gram-mars without the need to pre-specify the number ofnonterminals.
Cohen et al(2008) and Cohen andSmith (2009) employed the logistic normal prior tomodel the correlations between grammar symbols.Gillenwater et al(2010) incorporated a sparsity biason grammar rules into learning by means of poste-rior regularization.More recently, Spitkovsky et al(2010) and Poonand Domingos (2011) observed that the use ofViterbi EM (also called hard EM) in place of stan-dard EM can lead to significantly improved resultsin unsupervised learning of probabilistic grammarsfrom natural language and image data respectively,even if no prior information is used.
This finding issurprising because Viterbi EM is a degenerate caseof standard EM and is therefore generally consid-ered to be less effective in locating the optimumof the objective function.
Spitkovsky et al(2010)speculated that the observed advantage of ViterbiEM over standard EM is due to standard EM reserv-ing too much probability mass to spurious parses in1324the E-step.
However, it is still unclear as to whyViterbi EM can avoid this problem.Against this background, we propose the use ofa novel type of prior information for unsupervisedlearning of probabilistic natural language grammars,namely the syntactic unambiguity of natural lan-guage.
Although it is often possible to correctlyparse a natural language sentence in more than oneway, natural language is remarkably unambiguousin the sense that the number of plausible parses of anatural language sentence is rather small in compar-ison with the total number of possible parses.
Thus,we incorporate into learning an inductive bias in fa-vor of grammars that lead to unambiguous parses onnatural language sentences, by using the posteriorregularization framework (Ganchev et al 2010).We name this approach unambiguity regularization.The resulting family of algorithms includes standardEM and Viterbi EM, as well as an algorithm thatfalls between standard EM and Viterbi EM whichwe call softmax-EM.
The softmax-EM algorithmcan be implemented with a simple and computation-ally efficient extension to standard EM.
The fact thatViterbi EM is a special case of our approach alsogives an explanation of the advantage of Viterbi EMobserved in previous work: it is because Viterbi EMimplicitly utilizes unambiguity regularization.
Inour experiments of unsupervised dependency gram-mar learning, we show that unambiguity regulariza-tion is beneficial to learning, and in combinationwith annealing (of the regularization strength) andsparsity priors it leads to improvement over the cur-rent state of the art.It should be noted that our approach is closelyrelated to the deterministic annealing (DA) tech-nique studied in the optimization literature (Rose,1998).
However, DA has a very different motiva-tion than ours and differs from our approach in a fewimportant algorithmic details, as will be discussedin section 5.
When applied to unsupervised gram-mar learning, DA has been shown to lead to worseparsing accuracy than standard EM (Smith and Eis-ner, 2004); in contrast, we show that our approachleads to significantly higher parsing accuracy thanstandard EM in unsupervised dependency grammarlearning.The rest of the paper is organized as follows.
Sec-tion 2 analyzes the degree of unambiguity of naturallanguage grammars.
Section 3 introduces the unam-biguity regularization approach and shows that stan-dard EM, Viterbi EM and softmax-EM are its specialcases.
We show the experimental results in section4, discuss related work in section 5 and conclude thepaper in section 6.2 The (Un)ambiguity of Natural LanguageGrammarsA grammar is said to be ambiguous on a sentence ifthe sentence can be parsed in more than one way bythe grammar.
It is widely acknowledged that natu-ral language grammars are ambiguous on a signifi-cant proportion of natural language sentences.
Forexample, Manning and Schu?tze (1999) show that asentence randomly chosen from theWall Street Jour-nal ?
?The post office will hold out discounts andservice concessions as incentives?
?
has at leastfive plausible syntactic parses.
When we parse thissentence using the Berkeley parser (Petrov et al2006), one of the state-of-the-art English languageparsers, we find many alternative parses in additionto the parses shown in (Manning and Schu?tze, 1999).Indeed, with a probabilistic context-free grammarof only 26 nonterminals (as used in the Berke-ley parser), the estimated total number of possibleparses1 of the example sentence is 2 ?
1037.
How-ever, upon closer examination, we find that amongthis very large number of possible parses, only a fewhave significant probabilities.
Figure 1 shows theprobabilities of the 100 best parses of the examplesentence.
We can see that most of the parses haveprobabilities that are negligible compared with theprobability of the best parse (i.e., the parse with thelargest probability).
Quantitatively, we find that theprobabilities of the parses decrease roughly expo-nentially as we go from the best parse to the lesslikely parses.
We confirmed this observation by ex-amining the parses of many other natural languagesentences obtained using the Berkeley parser.
Thisobservation suggests that natural language gram-mars are indeed remarkably unambiguous on natu-ral language sentences, in the sense that for a typical1Given a sentence of lengthm and a complete Chomsky nor-mal form grammar with n nonterminals, the number of all pos-sible parses is Cm?1?n2m?1, where Cm?1 is the (m?
1)-thCatalan number.
This number is further increased if there areunary rules between nonterminals in the grammar.13250 20 40 60 80 10000.050.10.150.20.25100 Best ParsesProbabilityFigure 1: The probabilities of the 100 best parses of theexample sentence.natural language sentence, the probability mass ofthe parses is concentrated to a tiny portion of all pos-sible parses.
This is not surprising in light of the factthat the main purpose of natural language is commu-nication and in the course of language evolution theselection pressure for more efficient communicationwould favor unambiguous languages.To highlight the unambiguity of natural languagegrammars, here we compare the parse probabilitiesshown in Figure 1 with the parse probabilities pro-duced by two other probabilistic context-free gram-mars.
In figure 2(a) we show the probabilities of the100 best parses of the example sentence producedby a random grammar.
The random grammar has asimilar number of nonterminals as in the Berkeleyparser, and its grammar rule probabilities are sam-pled from a uniform distribution and then normal-ized.
It can be seen that unlike the natural languagegrammar, the random grammar produces a very uni-form probability distribution over parses.
Figure2(b) shows the probabilities of the 100 best parsesof the example sentence produced by a maximum-likelihood grammar learned from the unannotatedWall Street Journal corpus of the Penn Treebank us-ing the EM algorithm.
An exponential decrease canbe observed in the probabilities, but the probabil-ity mass is still much less concentrated than in thecase of the natural language grammar.
Again, weconfirmed this observation by repeating the exper-iments on many other natural language sentences.This suggests that both the random grammar and themaximum-likelihood grammar are far more ambigu-ous on natural language sentences than true naturallanguage grammars.3 Learning with UnambiguityRegularizationMotivated by the preceding observation, we want toincorporate into learning an inductive bias in favorof grammars that are unambiguous on natural lan-guage sentences.
First of all, we need a precise defi-nition of the ambiguity of a grammar on a sentence.Assume a grammar with a fixed set of grammar rulesand let ?
be the rule probabilities.
Let x represent asentence and let z represent the parse of x.
One natu-ral measurement of the ambiguity is the informationentropy of z conditioned on x and ?
:H(z|x, ?)
= ??zp?
(z|x) log p?
(z|x)The lower the entropy is, the less ambiguous thegrammar is on sentence x.
When the entropyreaches 0, the grammar is strictly unambiguous onsentence x, i.e., sentence x has a unique parse ac-cording to the grammar.Now we need to modify the objective functionof grammar learning to favor low ambiguity of thelearned grammar in parsing natural langauge sen-tences.
One approach is to use a prior distribu-tion that favors grammars with low ambiguity onthe sentences that they generate.
Since the likeli-hood term in the objective function would ensurethat the learned grammar will have high probabilityof generating natural language sentences, combin-ing the likelihood and the prior would lead to lowambiguity of the learned grammar on natural lan-guage sentences.
Unfortunately, adding this priorto the objective function makes learning intractable.Hence, here we adopt an alternative approach usingthe posterior regularization framework (Ganchev etal., 2010).
Posterior regularization biases learningin favor of solutions with desired behavior by con-straining the model posteriors on the unlabeled data.In our case, we use the constraint that the probabilitydistributions on the parses of the training sentencesgiven the learned grammar must have low entropy,which is equivalent to requiring the learned grammarto have low ambiguity on the training sentences.LetX = {x1, x2, .
.
.
, xn} denote the set of train-ing sentences, Z = {z1, z2, .
.
.
, zn} denote the set13260 20 40 60 80 1000123456x 10?24100 Best ParsesProbability(a)0 20 40 60 80 10000.511.522.53 x 10?3100 Best ParsesProbability(b)Figure 2: The probabilities of the 100 best parses of the example sentence produced by (a) a random grammar and (b)a maximum-likelihood grammar learned by the EM algorithm.of parses of the training sentences, and ?
denote therule probabilities of the grammar.
We use the slack-penalized version of the posterior regularization ob-jective function:J(?)
= log p(?|X)?minq,?(KL(q(Z)||p?
(Z|X)) + ??i?i)s.t.
?i,H(zi) = ?
?ziq(zi) log q(zi) ?
?iwhere ?
is a nonnegative constant that controls thestrength of the regularization term; q is an auxil-iary distribution such that q(Z) =?i q(zi).
Thefirst term in the objective function is the log poste-rior probability of the grammar parameters given thetraining corpus, and the second term minimizes theKL-divergence between the auxiliary distribution qand the posterior distribution on Z while constrainsq to have low entropy.
We can incorporate the con-straint into the objective function, so we getJ(?)
= log p(?|X)?minq(KL(q(Z)||p?
(Z|X)) + ?
?iH(zi))To optimize this objective function, we can per-form coordinate ascent on a two-variable function:F (?, q) = log p(?|X)?(KL(q(Z)||p?
(Z|X)) + ?
?iH(zi))There are two steps in each coordinate ascent itera-tion.
In the first step, we fix q and optimize ?.
It canbe shown that??
= argmax?F (?, q)= argmax?Eq[log(p?(X,Z)p(?
))]This is equivalent to theM-step in the EM algorithm.The second step fixes ?
and optimizes q.q?
= argmaxqF (?, q)= argminq(KL(q(Z)||p?
(Z|X)) + ?
?iH(zi))It is different from the E-step of the EM algorithmin that it contains an additional regularization term?
?i H(zi).
Ganchev et al(2010) propose to usethe projected subgradient method to solve this op-timization problem in the general case of posteriorregularization.
In our case, however, it is possible toobtain an analytical solution as shown below.First, note that the optimization objective of thisstep can be rewritten as the sum over functions ofindividual training sentences.KL(q(Z)||p?
(Z|X)) + ?
?iH(zi) =?ifi(q)wherefi(q) = KL(q(zi)||p?
(zi|xi)) + ?H(zi)=?zi(q(zi) logq(zi)1??p?
(zi|xi))1327So we can optimize fi(q) for each training sentencexi.
The optimum of fi(q) depends on the value ofthe constant ?.Case 1: ?
= 0.fi(q) contains only the KL-divergence term, so thesecond step in the coordinate ascent iteration be-comes the standard E-step of the EM algorithm.q?
(zi) = p?
(zi|xi)Case 2: 0 < ?
< 1.The space of valid assignments of the distributionq(zi) is a unit (m?1)-simplex, where m is the num-ber of valid parses of sentence xi.
Denote this spaceby ?.Theorem 1. fi(q) is strictly convex on the unit sim-plex ?
when 0 < ?
< 1.Proof Sketch.
Define g(x) = x log x, where g(0) isdefined to be 0.
For any t ?
(0, 1), for any twopoints q1 and q2 in the unit simplex ?, we can showthattfi(q1) + (1?
t)fi(q2)?
fi(tq1 + (1?
t)q2)= (1?
?
)?zi[tg(q1(zi)) + (1?
t)g(q2(zi))?
g(tq1(zi) + (1?
t)q2(zi))]It is easy to prove that g(x) is strictly convex on theinterval [0, 1].
Because ?zi, 0 ?
q1(zi), q2(zi) ?
1,we havetg(q1(zi)) + (1?
t)g(q2(zi))> g(tq1(zi) + (1?
t)q2(zi))Because 1?
?
> 0, we havetfi(q1) + (1?
t)fi(q2)?
fi(tq1 + (1?
t)q2) > 0By applying the Lagrange multiplier, we get thestationary point of fi(q) on the unit simplex ?:q?
(zi) = ?ip?(zi|xi)11??
(1)where ?i is the normalization factor?i =1?zi p?(zi|xi)11?
?Because fi(q) is strictly convex on the unit simplex?, this stationary point is the global minimum.
Notethat because 11??
> 1, q?
(zi) can be seen as the re-sult of applying a variant of the softmax function top?(zi|xi).
To compute q?, note that p?
(zi|xi) is theproduct of a set of grammar rule probabilities, so wecan raise all the rule probabilities of the grammar tothe power of 11??
and then run the normal E-step ofthe EM algorithm.
The normalization of q?
is in-cluded in the normal E-step.With q?, the objective function becomesF (?, q?)
= (1?
?
)?ilog?zip(zi, xi|?)11?
?+ log p(?)?
log p(X)The first term is proportional to the log ?likelihood?of the corpus computed with the exponentiated ruleprobabilities.
So we can use the parsing algorithm toefficiently compute the value of the objective func-tion (on the training corpus or on a separate devel-opment set) to determine when the coordinate ascentiteration shall be terminated.Case 3: ?
= 1We need to minimizefi(q) = ?
?zi(q(zi) log p?
(zi|xi))Because log p?
(zi|xi) ?
0 for any zi, the minimumof fi(q) is reached atq?
(zi) ={1 if zi = argmaxzi p?
(zi|xi)0 otherwiseCase 4: ?
> 1Theorem 2. fi(q) is strictly concave on the unit sim-plex ?
when ?
> 1.The proof is the same as that of theorem 1, ex-cept that 1 ?
?
is now negative which reverses thedirection of the last inequality in the proof.Theorem 3.
The minimum of fi(q) is attained at avertex of the unit simplex ?.Proof.
Assume the minimum of fi(q) is attainedat q?
that is not a vertex of the unit simplex ?,so there are at least two assignments of zi, say z1and z2, such that q?
(z1) and q?
(z2) are nonzero.1328Let q?
be the same distribution as q?
except thatq?
(z1) = 0 and q?
(z2) = q?
(z1) + q?(z2).
Let q?
?be the same distribution as q?
except that q??
(z1) =q?
(z1) + q?
(z2) and q??
(z2) = 0.
Obviously, both q?and q??
are in the unit simplex ?
and q?
?= q??.
Lett = q?(z2)q?(z1)+q?
(z2) , and obviously we have 0 < t < 1.So we get q?
= tq?
+ (1 ?
t)q??.
According to The-orem 2, fi(q) is strictly concave on the unit simplex?, so we have fi(q?)
> tfi(q?)
+ (1 ?
t)fi(q??
).Without loss of generality, suppose fi(q?)
?
fi(q??
).So we have tfi(q?)
+ (1 ?
t)fi(q??)
?
fi(q??)
andtherefore fi(q?)
> fi(q??
), which means fi(q) doesnot attain the minimum at q?.
This contradicts theassumption.Now we need to find out at which of the verticesof the unit simplex ?
is the minimum of fi(q) at-tained.
At the vertex where the probability mass isconcentrated at the assignment z, the value of fi(q)is ?
log p?(z|xi).
So the minimum is attained atq?
(zi) ={1 if zi = argmaxzi p?
(zi|xi)0 otherwiseIt can be seen that the minimum in the case of?
> 1 is attained at the same point as in the case of?
= 1, at which all the probability mass is assignedto the best parse of the sentence.
So q?
can be com-puted using the E-step of the Viterbi EM algorithm.Denote the best parse by z?i .
With q?, the objectivefunction becomesF (?, q?)
=?ilog p(z?i , xi|?
)+ log p(?)?
log p(X)The first term is the sum of the log probabilities ofthe best parses of the corpus.
So again we can usethe parsing algorithm to efficiently compute it to de-cide when to terminate the iterative algorithm.SummaryOur unambiguity regularization approach is an ex-tension of the EM algorithm.
The behavior of ourapproach is controlled by the value of the nonneg-ative parameter ?.
A larger value of ?
correspondsto a stronger bias in favor of an unambiguous gram-mar.
When ?
= 0, our approach reduces to the stan-dard EM algorithm.
When ?
?
1, our approachreduces to the Viterbi EM algorithm, which consid-ers only the best parses of the training sentences inthe E-step.
When 0 < ?
< 1, our approach fallsbetween standard EM and Viterbi EM: it applies asoftmax function (Eq.1) to the distributions of parsesof the training sentences in the E-step.
The softmaxfunction can be computed by simply exponentiatingthe grammar rule probabilities before the standardE-step, which does not increase the time complexityof the E-step.
We refer to the algorithm in the caseof 0 < ?
< 1 as the softmax-EM algorithm.3.1 Annealing the Strength of RegularizationIn unsupervised learning of probabilistic grammars,the initial grammar is typically very ambiguous(e.g., a random grammar).
So we need to set ?
to avalue that is large enough to induce unambiguity.
Onthe other hand, natural language grammars do con-tain some degree of ambiguity, so if the value of ?is too large, then the learned grammar might be ex-cessively unambiguous and thus not a good modelof natural languages.
Hence, it is unclear how tochoose an optimal value of ?.One way to avoid choosing a fixed value of ?
isto anneal its value.
We start learning with a largevalue of ?
(e.g., ?
= 1) to strongly push the learneraway from the highly ambiguous initial grammar;then we gradually reduce the value of ?, possiblyending with ?
= 0, to avoid inducing excessive un-ambiguity in the learned grammar.
Note that if thevalue of ?
is annealed to 0, then our approach can beseen as providing an unambiguous initialization forstandard EM.3.2 Unambiguity Regularization withMean-field Variational InferenceVariational inference approximates the posterior ofthe model given the data.
It typically leads to moreaccurate predictions than the maximum a posteriori(MAP) estimation.
In addition, for certain types ofprior distributions (e.g., a Dirichlet prior with hy-perparameters set to values less than 1), variationalinference is able to find a solution when MAP esti-mation fails.
Here we incorporate unambiguity reg-ularization into mean-field variational inference.The objective function with unambiguity regular-1329ization for mean-field variational inference is:F (q(?
), q(Z)) = log p(X)?(KL(q(?
)q(Z)||p(?,Z|X)) + ?
?iH(zi))where ?i,H(zi) = ?
?ziq(zi) log q(zi)We can perform coordinate ascent that alternatelyoptimizes q(?)
and q(Z).
Since the regularizationterm does not contain q(?
), the optimization of q(?
)is exactly the same as in the standard mean-fieldvariational inference.
To optimize q(Z), we haveq?
(Z) =argminq(Z)(KL(q(Z)||p?
(X,Z)) + ?
?iH(zi))where p?
(X,Z) is defined aslog p?
(X,Z) = Eq(?
)[log p(?,Z,X)] + constNow we can follow a derivation similar to that in thesetting of MAP estimation with unambiguity regu-larization, and we can obtain a similar result but withp?
(zi|xi) replaced with p?
(xi, zi) in each of the fourcases.Note that if Dirichlet priors are used over gram-mar rule probabilities ?, then p?
(xi, zi) can be rep-resented as the product of a set of weights inmean-field variational inference (Kurihara and Sato,2004).
Therefore in order to compute q?
(zi), when0 < ?
< 1, we simply need to raise all the weightsto the power of 11??
before running the normal stepof computing q?
(zi) in standard mean-field varia-tional inference; and when ?
?
1, we can simplyuse the weights to find the best parse of the trainingsentence and assign probability 1 to it.4 ExperimentsWe tested the effectiveness of unambiguity regular-ization in unsupervised learning of a type of depen-dency grammar called the dependency model withvalence (DMV) (Klein and Manning, 2004).
Wereport the results on the Wall Street Journal cor-pus (with section 2-21 for training and section 23for testing) in section 4.1?4.3, and the results onthe corpora of eight additional languages in sectionTesting AccuracyValue of ?
?
10 ?
20 All0 (standard EM) 46.2 39.7 34.90.25 53.7 44.7 40.30.5 51.9 42.9 38.80.75 51.6 43.1 38.81 (Viterbi EM) 58.3 45.2 39.4Table 1: The dependency accuracies of grammars learnedby our approach with different values of ?.4.4.
On each corpus, we trained the learner on thegold-standard part-of-speech tags of the sentencesof length ?
10 with punctuation stripped off.
Westarted our algorithm with the informed initializationproposed in (Klein and Manning, 2004), and termi-nated the algorithm when the increase in the valueof the objective function fell below a threshold of0.001%.
To evaluate a learned grammar, we used thegrammar to parse the testing corpus and computedthe dependency accuracy which is the percentage ofthe dependencies that are correctly matched betweenthe parses generated by the grammar and the goldstandard parses.
We report the dependency accu-racy on subsets of the testing corpus correspondingto sentences of length ?
10, length ?
20, and theentire testing corpus.4.1 Results with Different Values of ?We compared the performance of our approach withfive different values of the parameter ?
: 0 (i.e., stan-dard EM), 0.25, 0.5, 0.75, 1 (i.e., Viterbi EM).
Table1 shows the experimental results.
It can be seen thatlearning with unambiguity regularization (i.e., with?
> 0) consistently outperforms learning withoutunambiguity regularization (i.e., ?
= 0).
The gram-mar learned by Viterbi EM has significantly higherdependency accuracy in parsing short sentences.
Wespeculate that this is because short sentences are lessambiguous and therefore a strong unambiguity regu-larization is especially helpful in learning the gram-matical structures of short sentences.
On the testingsentences of all lengths, ?
= 0.25 achieves the bestdependency accuracy, which suggests that control-ling the strength of unambiguity regularization cancontribute to improved performance.1330Testing Accuracy?
10 ?
20 AllDMV ModelUR-Annealing 63.6 53.1 47.9UR-Annealing&Prior 66.6 57.7 52.3PR-S (Gillenwater et al 2010) 62.1 53.8 49.1SLN TieV&N (Cohen and Smith, 2009) 61.3 47.4 41.4LN Families (Cohen et al 2008) 59.3 45.1 39.0Extended ModelsUR-Annealing on E-DMV(2,2) 71.4 62.4 57.0UR-Annealing on E-DMV(3,3) 71.2 61.5 56.0L-EVG (Headden et al 2009) 68.8 - -LexTSG-DMV (Blunsom and Cohn, 2010) 67.7 - 55.7Table 2: The dependency accuracies of grammars learnedby our approach (denoted by ?UR?)
with annealing andprior, compared with previous published results.4.2 Results with Annealing and PriorWe annealed the value of ?
from 1 to 0 when run-ning our approach.
We reduced the value of ?
ata constant speed such that it reaches 0 at iteration100.
The results of this experiment (shown as ?UR-Annealing?
in Table 2) suggest that annealing thevalue of ?
not only helps circumvent the problem ofchoosing an optimal value of ?, but may also lead tosubstantial improvements over the results of learn-ing using any fixed value of ?.Dirichlet priors with the hyperparameter ?
set to avalue less than 1 are often used to induce parametersparsity.
We added Dirichlet priors over grammarrule probabilities and ran the variational inferenceversion of our approach.
The value of ?
was set to0.25 as suggested by previous work (Cohen et al2008; Gillenwater et al 2010).
When tested withdifferent values of ?, adding Dirichlet priors with?
= 0.25 consistently boosted the dependency ac-curacy of the learned grammar by 1?2%.
When thevalue of ?
was annealed during variational inferencewith Dirichlet priors, the dependency accuracy wasfurther improved (shown as ?UR-Annealing&Prior?in Table 2).The first part of Table 2 also compares our re-sults with the best results that have been published inthe literature for unsupervised learning of the DMVmodel (with different priors or regularizations thanours).
It can be seen that our best result (unambigu-ity regularization with annealing and prior) clearlyoutperforms previous results.
Furthermore, we ex-pect our approach to be more computationally ef-ficient than the other approaches, because our ap-proach only inserts an additional parameter expo-nentiation step into each iteration of standard EM orvariational inference, in contrast to the other threeapproaches all of which involve additional gradientdescent optimization steps in each iteration.4.3 Results on Extended ModelsIt has been pointed out that the DMV model is verysimplistic and cannot capture many linguistic phe-nomena; therefore a few extensions of DMV havebeen proposed, which achieve significant improve-ment over DMV in unsupervised grammar learn-ing (Headden et al 2009; Blunsom and Cohn,2010).
We examined the effect of unambiguity reg-ularization on E-DMV, an extension of DMV (withtwo different settings: (2,2) and (3,3)) (Headden etal., 2009; Gillenwater et al 2010).
As shown inthe second part of Table 2, unambiguity regular-ization with annealing on E-DMV achieves betterdependency accuracies than the state-of-the-art ap-proaches to unsupervised parsing with extended de-pendency models.
Addition of Dirichlet priors, how-ever, did not further improve the accuracies in thissetting.
Note that E-DMV is an unlexicalized ex-tension of DMV that is relatively simple.
We spec-ulate that the performance of unambiguity regular-ization can be further improved if applied to moreadvanced models like LexTSG-DMV (Blunsom andCohn, 2010).4.4 Results on More LanguagesWe examined the effect of unambiguity regulariza-tion with the DMV model on the corpora of eightadditional languages2.
The experimental results ofall the nine languages are summarized in Table 3.
Itcan be seen that learning with unambiguity regular-ization (i.e., with ?
> 0) outperforms learning with-out unambiguity regularization (i.e., ?
= 0) on eightout of the nine languages, but the optimal value of?
is very different across languages.
Annealing thevalue of ?
from 1 to 0 does not always lead to fur-ther improvement over using the optimal value of ?2The corpora are from the PASCAL Challenge onGrammar Induction (http://wiki.cs.ox.ac.uk/InducingLinguisticStructure/SharedTask).1331for each language, but on average it has better per-formance than using any fixed value of ?
and henceis useful when the optimal value of ?
is hard to iden-tify.5 Related WorkDeterministic annealing (DA) (Rose, 1998; Smithand Eisner, 2004) also extends the standard EM al-gorithm by exponentiating the posterior probabili-ties of the hidden variables in the E-step.
However,the goal of DA is to improve the optimization of anon-concave objective function, which is achievedby setting the exponent in the E-step to a value closeto 0, so that the distribution of the hidden variablesbecomes nearly uniform and the objective functionbecomes almost concave and therefore easy to opti-mize; this exponent is then gradually increased to1 to optimize the original objective function.
Incontrast, the goal of unambiguity regularization isto bias learning in favor of unambiguous grammars,which is achieved by setting the exponent in the E-step (i.e., 11??
in Eq.1) to a value larger than 1, sothat the distribution of the hidden variables becomesless uniform (i.e., parses become less ambiguous); inour annealing approach, the exponent is initializedto a very large value (positive infinity in our experi-ment) to push the learner away from the ambiguousinitial grammar, and then gradually decreased to 1 toavoid inducing excessive unambiguity in the learnedgrammar.
The empirical results of Smith and Eisner(2004) show that DA resulted in lower parsing ac-curacy compared with standard EM in unsupervisedconstituent parsing; and a ?skew?
posterior term hadto be inserted into the E-step formulation of DA toboost its accuracy over that of standard EM.
In con-trast, the results of our experiments show that unam-biguity regularization leads to significantly higherparsing accuracy than standard EM.Unambiguity regularization is also related tothe minimum entropy regularization framework forsemi-supervised learning (Grandvalet and Bengio,2005; Smith and Eisner, 2007), which tries to min-imize the entropy of the class label or hidden vari-ables on unlabeled data in addition to maximizingthe likelihood of labeled data.
However, entropyregularization is either motivated by the theoreti-cal result that unlabeled data samples are informa-tive when classes are well separated (Grandvalet andBengio, 2005), or derived from the expected condi-tional log-likelihood (Smith and Eisner, 2007).
Incontrast, our approach is motivated by the observedunambiguity of natural language grammars.
Oneimplication of this difference is that if our approachis applied to semi-supervised learning, the regular-ization term would be applied to labeled sentencesas well (by ignoring the labels) because the targetgrammar shall be unambiguous on all the trainingsentences.The sparsity bias, which favors a grammar withfewer grammar rules, has been widely used in un-supervised grammar learning (Chen, 1995; Johnsonet al 2007; Gillenwater et al 2010).
Although amore sparse grammar is often less ambiguous, ingeneral that is not always the case.
We have shownthat unambiguity regularization could lead to betterperformance than approaches utilizing the sparsitybias, and that the two types of biases can be appliedtogether for further improvement in the learning per-formance.6 ConclusionWe have introduced unambiguity regularization, anovel approach to unsupervised learning of proba-bilistic natural language grammars.
It is based onthe observation that natural language grammars areremarkably unambiguous in the sense that in parsingnatural language sentences they tend to concentratethe probability mass to a tiny portion of all possi-ble parses.
By using posterior regularization, weincorporate an inductive bias into learning in favorof grammars that are unambiguous on natural lan-guage sentences.
The resulting family of algorithmsincludes standard EM and Viterbi EM, as well asthe softmax-EM algorithm which falls between stan-dard EM and Viterbi EM.
The softmax-EM algo-rithm can be implemented by adding a simple pa-rameter exponentiation step into standard EM.
Inour experiments of unsupervised dependency gram-mar learning, we show that unambiguity regulariza-tion is beneficial to learning, and by incorporatingregularization strength annealing and sparsity priorsour approach outperforms the current state-of-the-art grammar learning algorithms.
For future work,we plan to combine unambiguity regularization with1332Arabic Basque Czech Danish Dutch English Portuguese Slovene Swedish?
= 0 (standard EM) 27.4 32.1 27.8 35.6 29.4 34.9 23.7 30.6 31.9?
= 0.25 30.6 39.3 27.2 35.2 30.9 40.3 27.7 23.8 42.0?
= 0.5 32.6 40.6 33.0 37.4 32.7 38.8 27.5 15.3 29.3?
= 0.75 31.6 41.8 16.1 36.0 35.1 38.8 26.2 15.1 32.7?
= 1 (Viterbi EM) 29.6 39.8 28.6 33.6 28.0 39.4 27.3 14.6 37.2UR-Annealing 26.7 41.6 39.3 34.1 43.1 47.8 26.4 16.4 46.0Table 3: The dependency accuracies (on sentences of all lengths in the testing corpus) of grammars learned by ourapproach from the corpora of the following languages: Arabic (Hajic?
et al 2004), Basque (Aduriz et al 2003), Czech(Hajic?
et al 2000), Danish (Buch-Kromann et al 2007), Dutch (Beek et al 2002), English, Portuguese (Afonso etal., 2002), Slovene (Erjavec et al 2010), Swedish (Nivre et al 2006).other types of priors and regularizations for unsu-pervised grammar learning, to apply it to more ad-vanced grammar models, and to explore alternativeformulations of unambiguity regularization.AcknowledgementThe work of Kewei Tu was supported at Iowa StateUniversity in part by a research assistantship fromthe Iowa State University Center for ComputationalIntelligence, Learning, and Discovery, and at Uni-versity of California, Los Angeles by the DARPAgrant FA 8650-11-1-7149.
The work of VasantHonavar was supported by the National ScienceFoundation, while working at the Foundation.
Anyopinion, finding, and conclusions contained in thisarticle are those of the authors and do not necessar-ily reflect the views of the National Science Founda-tion.ReferencesI.
Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,A.
Diaz de Ilarraza, A. Garmendia, , and M. Oronoz.2003.
Construction of a basque dependency treebank.In Proc.
of the 2nd Workshop on Treebanks and Lin-guistic Theories (TLT).Susana Afonso, Eckhard Bick, Renato Haber, and DianaSantos.
2002.
?floresta sinta?(c)tica?
: a treebank forPortuguese.
In Proceedings of the 3rd Intern.
Conf.
onLanguage Resources and Evaluation (LREC), pages1968?1703.J.
K. Baker.
1979.
Trainable grammars for speech recog-nition.
In Speech Communication Papers for the 97thMeeting of the Acoustical Society of America.Van Der Beek, G. Bouma, R. Malouf, G. Van Noord, andRijksuniversiteit Groningen.
2002.
The alpino depen-dency treebank.
In In Computational Linguistics in theNetherlands (CLIN, pages 1686?1691.Phil Blunsom and Trevor Cohn.
2010.
Unsupervisedinduction of tree substitution grammars for depen-dency parsing.
In Proceedings of the 2010 Conferenceon Empirical Methods in Natural Language Process-ing, EMNLP ?10, pages 1204?1213, Stroudsburg, PA,USA.
Association for Computational Linguistics.Matthias Buch-Kromann, Ju?rgen Wedekind, , andJakob Elming.
2007.
The copenhagen danish-english dependency treebank v. 2.0. http://www.buch-kromann.dk/matthias/cdt2.0/.Stanley F. Chen.
1995.
Bayesian grammar induction forlanguage modeling.
In Proceedings of the 33rd annualmeeting on Association for Computational Linguistics.Shay B. Cohen and Noah A. Smith.
2009.
Shared logis-tic normal distributions for soft parameter tying in un-supervised grammar induction.
In HLT-NAACL, pages74?82.Shay B. Cohen, Kevin Gimpel, and Noah A. Smith.2008.
Logistic normal priors for unsupervised prob-abilistic grammar induction.
In NIPS, pages 321?328.Tomaz Erjavec, Darja Fiser, Simon Krek, and NinaLedinek.
2010.
The jos linguistically tagged corpusof slovene.
In LREC.Jenny Rose Finkel, Trond Grenager, and Christopher D.Manning.
2007.
The infinite tree.
In Proceedings ofthe 45th Annual Meeting of the Association of Compu-tational Linguistics, pages 272?279.
Association forComputational Linguistics, June.Kuzman Ganchev, Joa?o Grac?a, Jennifer Gillenwater, andBen Taskar.
2010.
Posterior regularization for struc-tured latent variable models.
Journal of MachineLearning Research, 11:2001?2049.Jennifer Gillenwater, Kuzman Ganchev, Joa?o Grac?a, Fer-nando Pereira, and Ben Taskar.
2010.
Sparsity in de-pendency grammar induction.
In ACL ?10: Proceed-ings of the ACL 2010 Conference Short Papers, pages194?199, Morristown, NJ, USA.
Association for Com-putational Linguistics.Yves Grandvalet and Yoshua Bengio.
2005.
Semi-supervised learning by entropy minimization.
In1333Lawrence K. Saul, Yair Weiss, and Le?on Bottou, ed-itors, Advances in Neural Information Processing Sys-tems 17, pages 529?536.
MIT Press, Cambridge, MA.Jan Hajic?, Alena Bo?hmova?, Eva Hajic?ova?, and BarboraVidova?-Hladka?.
2000.
The Prague DependencyTreebank: A Three-Level Annotation Scenario.
InA.
Abeille?, editor, Treebanks: Building and UsingParsed Corpora, pages 103?127.
Amsterdam:Kluwer.Jan Hajic?, Otakar Smrz?, Petr Zema?nek, Jan S?naidauf, andEmanuel Bes?ka.
2004.
Prague arabic dependencytreebank: Development in data and tools.
In In Proc.of the NEMLAR Intern.
Conf.
on Arabic Language Re-sources and Tools, pages 110?117.William P. Headden, III, Mark Johnson, and David Mc-Closky.
2009.
Improving unsupervised dependencyparsing with richer contexts and smoothing.
In HLT-NAACL, pages 101?109.Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-ter.
2007.
Bayesian inference for pcfgs via markovchain monte carlo.
In HLT-NAACL, pages 139?146.Dan Klein and Christopher D. Manning.
2004.
Corpus-based induction of syntactic structure: Models of de-pendency and constituency.
In Proceedings of ACL.Kenichi Kurihara and Taisuke Sato.
2004.
An appli-cation of the variational Bayesian approach to prob-abilistic contextfree grammars.
In IJCNLP-04 Work-shop beyond shallow analyses.K.
Lari and S. Young.
1990.
The estimation of stochas-tic context-free grammars using the inside-outside al-gorithm.
Computer Speech and Language, 4:35?36.Percy Liang, Slav Petrov, Michael I. Jordan, and DanKlein.
2007.
The infinite pcfg using hierarchi-cal Dirichlet processes.
In Proceedings of EMNLP-CoNLL, pages 688?697.Christopher D. Manning and Hinrich Schu?tze.
1999.Foundations of statistical natural language process-ing.
MIT Press, Cambridge, MA, USA.Joakim Nivre, Jens Nilsson, and Johan Hall.
2006.
Tal-banken05: A Swedish Treebank with Phrase Struc-ture and Dependency Annotation.
In Proceedings ofthe fifth international conference on Language Re-sources and Evaluation (LREC2006), May 24-26,2006, Genoa, Italy, pages 1392?1395.
European Lan-guage Resource Association, Paris.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In ACL-44: Proceedings ofthe 21st International Conference on ComputationalLinguistics and the 44th annual meeting of the Associ-ation for Computational Linguistics, pages 433?440,Morristown, NJ, USA.
Association for ComputationalLinguistics.Hoifung Poon and Pedro Domingos.
2011.
Sum-productnetworks : A new deep architecture.
In Proceedingsof the Twenty-Seventh Conference on Uncertainty inArtificial Intelligence (UAI).Kenneth Rose.
1998.
Deterministic annealing for clus-tering, compression, classification, regression, and re-lated optimization problems.
In Proceedings of theIEEE, pages 2210?2239.Noah A. Smith and Jason Eisner.
2004.
Annealingtechniques for unsupervised statistical language learn-ing.
In Proceedings of the 42nd Annual Meeting onAssociation for Computational Linguistics, ACL ?04,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.David A. Smith and Jason Eisner.
2007.
Bootstrappingfeature-rich dependency parsers with entropic priors.In Proceedings of the 2007 Joint Conference on Em-pirical Methods in Natural Language Processing andComputational Natural Language Learning (EMNLP-CoNLL), pages 667?677, Prague, June.Valentin I. Spitkovsky, Hiyan Alshawi, Daniel Jurafsky,and Christopher D. Manning.
2010.
Viterbi trainingimproves unsupervised dependency parsing.
In Pro-ceedings of the Fourteenth Conference on Computa-tional Natural Language Learning, CoNLL ?10, pages9?17, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.1334
