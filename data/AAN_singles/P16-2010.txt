Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 59?65,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsJoint Word Segmentation and Phonetic Category InductionMicha ElsnerDept.
of LinguisticsThe Ohio State Universitymelsner0@gmailStephanie AntetomasoDept.
of LinguisticsThe Ohio State Universityantetomaso.2@osu.eduNaomi H. FeldmanDept.
of Linguistics and UMIACSUniversity of Marylandnhf@umd.eduAbstractWe describe a model which jointly per-forms word segmentation and inducesvowel categories from formant values.Vowel induction performance improvesslightly over a baseline model which doesnot segment; segmentation performance de-creases slightly from a baseline using en-tirely symbolic input.
Our high joint perfor-mance in this idealized setting implies thatproblems in unsupervised speech recogni-tion reflect the phonetic variability of realspeech sounds in context.1 IntroductionIn learning to speak their native language, a de-veloping infant must acquire two related pieces ofinformation: a set of lexical items (along with thecontexts in which they are likely to occur), and aset of phonetic categories.
For instance, an English-learning infant must learn that [i] and [I] are differ-ent segments, differentiating between words likebeat and bit, while for a Spanish-learning infant,[i] and [I]-like tokens represent realizations of thesame category.
It is clear that these two tasks areintimately related, and that models of languageacquisition must solve both together?
but how?This problem has inspired much recent work inlow-resource speech recognition (Lee et al, 2015;Lee and Glass, 2012; Jansen and Church, 2011;Varadarajan et al, 2008), with impressive results.Nonetheless, many of these researchers concludethat their systems learn too many phonetic cate-gories, a problem they attribute to the presenceof contextual variants (allophones) of the differ-ent sounds.
For instance, the [a] in dog is likelylonger than the [a] in dock (Ladefoged and John-son, 2010), but this difference is not phonologicallymeaningful in English?
it cannot differentiate anypair of words on its own.
Many unsupervised sys-tems are claimed to erroneously learn these kindsof differences as categorical ones.Here, we attempt to model the problem in a morecontrolled setting by extending work in cognitivemodeling of language acquisition.
We present asystem which jointly acquires vowel categories andlexical items from a mixed symbolic/acoustic rep-resentation of the input.
As is traditional in cogni-tive models of vowel acquisition, it uses a single-point formant representation of the vowel acous-tics, and is tested on a simulated corpus in whichvowel acoustics are unaffected by context.
We findthat, under these circumstances, vowel categoriesand lexical items can be learned jointly with rel-atively little decrease in accuracy from learningeither alone.
Thus, our results support the hypothe-sis that the more realistic problem is hard becauseof contextual variability.
As a secondary point, weshow that the results reflect problems with localminima in the popular framework of hierarchicalBayesian modeling.2 Related workThis work aims to induce both a set of phoneticvowel categories and a lexical representation fromunlabeled data.
It extends the closely related modelof Feldman et al (2013a), which performs the sametask, but with known word boundaries; this re-quirement is a significant limitation on the model?scognitive plausibility.
Our model infers a latentword segmentation.
Another extension, Frank et al(2014), uses semantic information to disambiguatewords, but still with known word boundaries.A few models learn a lexicon while categoriz-ing all sounds, instead of just vowels.
Lee et al(2015) and Lee and Glass (2012) use hierarchicalBayesian models to induce word and subword units.These models are mathematically very similar to59our own, differing primarily using more complexacoustic representations and inducing categoriesfor all sounds instead of just vowels.
Jansen andChurch (2011) learns whole-word Markov models,then clusters their states into phone-like units us-ing a spectral algorithm.
Their system still learnsmultiple allophonic categories for most sounds.In the segmentation literature, several previoussystems learn lexical items from variable input (El-sner et al, 2013; Daland and Pierrehumbert, 2011;Rytting et al, 2010; Neubig et al, 2010; Fleck,2008).
However, these models use pre-processedrepresentations of the acoustics (phonetic transcrip-tion or posterior probabilities from a phone rec-ognizer) rather than inducing an acoustic categorystructure directly.
Elsner et al (2013) and Neubiget al (2010) use Bayesian models and samplingschemes similar to those presented here.Acquisition models like Elsner et al (2013),Ryt-ting et al (2010) and Fleck (2008) are designed tohandle phonological variability.
In particular, theyare designed to cope with words which have mul-tiple transcribed pronunciations ([wan] and [want]for ?want?
); this kind of alternation can insert ordelete whole segments, or change a vowel soundfrom one perceptual category to another.
Such vari-ability is common in spoken English (Pitt et al,2005) and presents a challenge for speech recogni-tion (McAllaster et al, 1998).In contrast, the system presented here modelsphonetic variability within a single category.
Ituses an untranscribed, continuous-valued represen-tation for vowel sounds, so that different tokenswithin a single category may differ from one an-other.
But it does so within an idealized datasetwhich lacks phonological variants.
Moreover, al-though the phonetic input to the system is variable,the variation is not predictable; tokens within thecategory differ at random, independently from theirenvironment.Several other models also learn phonetic cat-egories from continuous input, either from realor idealized datasets, without learning a lexicon.Varadarajan et al (2008) learn subword units by in-crementally splitting an HMM model of the data tomaximize likelihood.
Badino et al (2014) performk-means clustering on the acoustic representationlearned by an autoencoder.
Cognitive models usingformant values as input are common, many usingmixture of Gaussians (Vallabha et al, 2007; deBoer and Kuhl, 2003).
Because they lack a lexicon,these models have particular difficulty distinguish-ing meaningful from allophonic variability.3 Dataset and modelOur dataset replicates the previous idealized set-ting for vowel category induction in cognitivemodeling, but in a corpus of unsegmented utter-ances rather than a wordlist.
We adapt a stan-dard word segmentation corpus of child-directedspeech (Brent, 1999), which consists of 8000 utter-ances from Bernstein-Ratner (1987), orthographi-cally transcribed and then phonetically transcribedusing a pronunciation dictionary.We add simulated acoustics (without contextualvariation) to each vowel in the Brent corpus.
Fol-lowing previous cognitive models of category in-duction (Feldman et al, 2013b), we use the voweldataset given by Hillenbrand et al (1995), whichgives formants for English vowels read in the con-text h d. We estimate a multivariate Gaussian dis-tribution for each vowel, and, whenever a monoph-thongal vowel occurs in the Brent corpus, we re-place it with a pair of formants (f1, f2) drawn fromthe appropriate Gaussian.
The ARPABET diph-thongs ?oy, aw, ay, em, en?, and all the consonants,retain their discrete values.
The first three wordsof the dataset, orthographically ?you want to?, arerendered: y[380.53 1251.69] w[811.88 1431.96]nt[532.91 1094.14].3.1 ModelOur model merges the Feldman et al (2013a) vowelcategory learner with the Elsner et al (2013) noisy-channel framework for word segmentation, whichis in turn based on the segmentation model of Gold-water et al (2009).
In generative terms, it definesa sequential process for sampling a dataset.
Theobservations will be surface strings S, which aredivided into (latent) words Xi=1:n. We denote thej-th character of word i as Sij.
When Sijis avowel, the observed value is a real-valued formantpair (f1, f2); when it is a consonant, it is observeddirectly.1.
Draw a distribution over vowel categories,piv?
DP (?v)2.
Sample parameters for each category,?v,?v?
NIW (?0,?, ?)3.
Draw a distribution over word strings, G0?DP (?0, CV (piv, pc, pstop)4.
Draw bigram transition distributions, Gx?DP (?1, G0)605.
Sample word sequences, Xi?
GXi?16.
Realize each vowel token in the surface string,Sij?
Normal(?Xij,?Xij)The initial prior over word forms,CV (piv, pc, pstop) is the following: sample aword length ?
1 from Geom(pstop); for eachcharacter in the word, choose to sample a con-sonant with probability pcor a vowel otherwise;sample all consonants uniformally, and all vowelsaccording to the (possibly-infinite) probabilityvector piv.1In practice, we integrate out piv,yielding a Chinese restaurant process in which thedistribution over vowels in a new word depend onthose used in already-seen words.
Vowels whichoccur in many word types are more likely to recur(Goldwater et al, 2006; Teh et al, 2006).The hyperparameters for the model are ?0and?1(which control the size of the unigram andbigram vocabularies), ?v(which weakly affectsthe number of vowel categories), ?0, n, ?
and ?
(which affect the average location and dispersionof vowel categories in formant space), and pcandpstop(which weakly affect the length and composi-tion of words).
We set ?0and ?1to their optimalvalues for word segmentation (3000 and 100 (Gold-water et al, 2009)) and ?vto .001.
In practice, novalue of ?vwe tried would produce a useful num-ber of vowels and so we fix the maximum numberof vowels (non-probabilistically) to nv; we explorea variety of values of this parameter below.
Themean vector for the vowel category parameters isset to [500, 1500] and the inverse precision matrixto 500I , biasing vowel categories to be near thecenter of the vowel space and have variances on theorder of hundreds of hertz.
We set the prior degreesof freedom ?
to 2.001.
Since ?
can be interpretedas a pseudocount determining the prior strength,this means the prior influence is relatively weak forreasonably-sized vowel categories.
We set pc= .5and pstop= .5; based on Goldwater et al (2009),we do not expect these parameters to be influential.These hyperparameter values were mostly takenfrom previous work.
The vowel inverse precisionand degrees of freedom differ from those in Feld-man et al (2013a), since our approach requiresus to sample from the prior, but the uninformativeprior used there was too poor a fit for the data.We chose a variance with units on the order of theoverall data variance, but did not tune it.1Feldman et al (2013a) assumes a more complex distribu-tion over consonants, while Goldwater et al (2009) assumesuniformity over all sounds.3.2 InferenceWe conduct inference by Gibbs sampling, includ-ing three sampling moves: block sampling of theanalyses of a single utterance, table label relabelingof a lexical item (Johnson and Goldwater, 2009)and resampling of the vowel category parameters?vand ?v.
We run 1000 iterations of utteranceresampling, with table relabeling every 10 itera-tions.2Following previous work, we integrateout the mixing weight distributions G0, G1andpiv, resulting in Chinese restaurant process distribu-tions for unigrams, bigrams and vowel categoriesin the lexicon (Teh et al, 2006).
Unlike Feldman etal.
(2013a) and many other variants of the InfiniteMixture of Gaussians (Rasmussen, 1999), we donot integrate out ?vand ?v, since this would cre-ate long-distance dependencies between differenttokens of the same vowel category within an utter-ance and thus complicate the implementation of awhole-utterance block sampling scheme.To block sample the analyses of a single utter-ance, we use beam sampling (Van Gael et al, 2008;Huggins and Wood, 2014), an auxiliary-variablesampling scheme in which we encode the modelas an (infeasibly large) finite-state transducer, thensample cutoff variables which restrict our algorithmto a finite subset of the transducer and sample atrajectory within it.
We then use a Metropolis-Hastings acceptance test to correct for the discrep-ancy between our finite-state encoding and the ac-tual model probability caused by repetitions of alexical item within the same utterance.Specifically, for each vowel sij, we sample acutoff cij?
U [0, P (sij|Xij)].
This cutoff indi-cates the least probable category assignment wewill permit for the surface symbol sij.
This cutoffconstrains us to consider only a finite number ofvowels at each point; if there are not enough, wecan instantiate unseen vowels by sampling their ?and ?
from the prior.
We then construct the latticeof possible word segmentations in which sijis al-lowed to correspond to any vowel in any lexicalentry, as long as all the consonants match up andthe vowel assignment density P (sij|xij) is greaterthan the cutoff.
We then propose a new trajectoryby sampling from this lattice.
See Mochihashi et al2Annealing is applied linearly, with inverse temperaturescaling from .1 to 1 for 800 iterations, then linearly from 1.0 to2.0 to encourage a MAP solution.
The Gaussian densities foracoustic token emissions are annealed to inverse temperature.3, to keep them comparable to the LM probabilities (Bahl etal., 1980).61(2009) for details of the finite-state construction.As in Feldman et al (2013a), we use a table rela-beling move (Johnson and Goldwater, 2009) whichchanges the word type for a single table in the uni-gram Chinese restaurant process by changing oneof the vowels.
This recategorizes a large number oftokens which share the same type (though not nec-essarily all, since there may be multiple unigramtables for the same word type).
The implementa-tion is tricky because of the bigram dependenciesbetween adjacent words, some of which may betokens of the same lexical item.
Nonetheless, thismove is necessary because token-level samplinghas insufficient mobility to change the represen-tation of a whole word type: if the sampler hasincorrectly assigned many tokens to the non-wordhAv, moving any single token to the correct h?vwill raise the transducer probability but also catas-trophically lower the lexical probability by creatinga singleton lexical item.Finally, because ?vand ?vare explicitly repre-sented rather than integrated out, their values mustbe resampled given the set of formant values as-sociated with each vowel cluster.
The use of aconjugate (Normal-Inverse Wishart) prior makesthis simple, applying equations 250-254 in Murphy(2007).4 ResultsDespite using multiple block moves, mobility isa severe issue for the sampler; the inference pro-cedure fails to merge together redundant vowelcategories even when doing so would raise the pos-terior probability significantly.
We demonstratethis by running the sampler with various numbersof vowel categories nv.
Posterior probabilities peakaround the true value of 12, but models with extracategories always use the entire set.With nvset to 11 or 12 categories, quantitativeperformance is relatively good, although segmen-tation is not as good as the Goldwater et al (2009)segmenter without any acoustics.
In fact, the sys-tem slightly outperforms the Feldman et al (2013a)lexical-distributional model with gold-standard seg-mentation.
Results are shown in Table 1.Word tokens are correctly segmented (bothboundaries correct) with an F-score of 67%3(ver-sus 74% in (Goldwater et al, 2009).
Individualboundaries are detected with an F-score of 82%3The joint model scores are averaged over two samplerruns.System Seg P R F Vow P R FGoldwater 76 72 74 - - -Feldman - - - - - 76joint, nv=12 64 69 67 87 80 83joint, nv=11 65 70 67 85 84 85Table 1: Segmentation and vowel clustering scores.versus 87%.
We also evaluate the lexical items,checking whether words are correctly grouped aswell as segmented (for example, whether tokens of?is?
and ?as?
are separated).
Feldman et al (2013a)evaluates the lexicon by computing a pairwise F-score on tokens (positive class: clustered together).Under this metric, their highest lexicon score forEnglish words is 93%.
We compute this metricon the subset of words for which the segmenta-tion system performs correctly (it is not clear howto count ?misses?
and ?false alarms?
for tokenswhich were mis-segmented).
On this subset, thismetric scores our system with nv= 12 at 91%,which indicates that we correctly identify most ofthe correctly segmented items.We evaluate our phonetic clustering by comput-ing the same pairwise F-score on pairs of vowel to-kens.
Our score is 83%; the Feldman et al (2013a)model scores 76%.
We conjecture that the improve-ment results from the use of bigram context in-formation to disambiguate between homophones.Confusion between vowels (attached as supplemen-tal material) is mostly reasonable.
We find cross-clusters for ah,ao, ey,ih, and uh,uw.
The model?ssuccessful learning of the vowel categories demon-strates that the high performance of cognitive mod-els in this domain is not due solely to their access togold-standard word boundaries (see also Martin etal.
(2013)).
We believe that the idealized acousticvalues (sampled from stationary Gaussians reflect-ing laboratory production) are critical in allowingthese models to outperform those which use naturalspeech.Though solving the two tasks together is harderthan tackling either alone, these results nonethe-less demonstrate comparable performance to othermodels which have to cope with variability whilesegmenting.
Fleck (2008) reports only 44% seg-mentation scores on transcribed English text in-cluding phonological variability; the noisy channelmodel of Elsner et al (2013) yields a segmentationtoken score of 67%.4Besides generic task difficulty, we attribute the4Word segmentation scores from Lee et al (2015), learningdirectly on acoustics, range between 16 and 20.62low scores to the model?s inability to mix, whichprevents it from merging similar vowel classes.
Be-cause table relabeling does not merge tables in theCRP hierarchy, even if it replaces an uncommonword with a more common one, the configurationalprobability does not change.
Thus the model?s spar-sity preference cannot encourage such moves.
Theprior on vowel categories, DP (pv), does encour-age changes which reduce the number of lexicaltypes using a rare vowel, but relabeling a table canrearrange at most a single sample from this priordistribution and is easily outweighed by the likeli-hood.A hand analysis of one sampler run in which /I/was split into two categories showed clear mixingproblems.
Many common words, such as ?it?
and?this?, appeared as duplicate lexical entries (e.g.
[I1t] and [I2t]).
These presumably captured somechance variation within the category, but not anactual linguistic feature.We suspect that this mobility problem is also alikely issue with models like Lee and Glass (2012)which use deep Bayesian hierarchies and relativelylocal inference moves.
Since the problem occurseven in this idealized setting, we expect it to exacer-bate the problems caused by contextual variabilityin more realistic experiments.Some errors did result from the joint nature ofthe task itself.
We looked for reanalyses involv-ing both a mis-segmentation and a vowel categorymistake.
For instance, the model is capable of mis-analyzing the word ?milk?
as ?me?
followed by thephonotactically implausible sequence ?lk?.
Mis-takes like these, in which the misanalysis creates aword, are relatively rare as a proportion of the total.The most common words created are ?say?, ?and?,?shoe?, ?it?
and ?a?.
More commonly, misanaly-ses of this type segment out single vowels or non-words like [luk], [eN], and [mO].
Some such errorscould be corrected by incorporating phonotacticsinto the model (Johnson and Goldwater, 2009).
Ingeneral, the error patterns are neither particularlyinterpretable nor cognitively very plausible.
Thisstands in contrast to the effects on word boundarydetection found in a model of phonological varia-tion (Elsner et al, 2013).5 ConclusionThe main result of our work is that joint word seg-mentation and vowel clustering is possible, withrelatively high effectiveness, by merging modelsknown to be successful in each setting indepen-dently.
The finding that success of this kind ispossible in an idealized setting reinforces an ar-gument made in previous work: that much of thedifficulty in category acquisition is due to contex-tual variation.Both phonological and phonetic variability prob-ably contribute to the difficulty of the real task.Phonological processes such as reduction cre-ate variant versions of words, splitting real lexi-cal items and creating misleading minimal pairs.Phonetic processes like coarticulation and com-pensatory lengthening create predictible variationwithin a category, encouraging the model to splitthe category into allophones.
In future work, wehope to quantify the contributions of these sourcesof error and work to address them explicitly withinthe same model.AcknowledgementsThis research was funded by NSF grants 1422987and 1421695.
We are grateful for the advice ofthree anonymous reviewers, and to Sharon Gold-water for distributing the baseline DPSeg system.ReferencesLeonardo Badino, Claudia Canevari, Luciano Fadiga,and Giorgio Metta.
2014.
An auto-encoderbased approach to unsupervised learning of subwordunits.
In Acoustics, Speech and Signal Processing(ICASSP), 2014 IEEE International Conference on,pages 7634?7638.
IEEE.Lalit Bahl, Raimo Bakis, Frederick Jelinek, and RobertMercer.
1980.
Language-model/acoustic-channel-model balance mechanism.
Technical disclosurebulletin Vol.
23, No.
7b, IBM, December.Nan Bernstein-Ratner.
1987.
The phonology of parent-child speech.
In K. Nelson and A. van Kleeck,editors, Children?s Language, volume 6.
Erlbaum,Hillsdale, NJ.Michael R. Brent.
1999.
An efficient, probabilisticallysound algorithm for segmentation and word discov-ery.
Machine Learning, 34:71?105, February.Robert Daland and Janet B. Pierrehumbert.
2011.Learning diphone-based segmentation.
CognitiveScience, 35(1):119?155.Bart de Boer and Patricia Kuhl.
2003.
Investigatingthe role of infant-directed speech with a computermodel.
Acoustic Research Letters On-Line, 4:129?134.63Micha Elsner, Sharon Goldwater, Naomi Feldman, andFrank Wood.
2013.
A joint learning model of wordsegmentation, lexical acquisition, and phonetic vari-ability.
In Proceedings of the 2013 Conference onEmpirical Methods in Natural Language Processing,pages 42?54, Seattle, Washington, USA, October.Association for Computational Linguistics.Naomi H. Feldman, Thomas L. Griffiths, Sharon Gold-water, and James L. Morgan.
2013a.
A role for thedeveloping lexicon in phonetic category acquisition.Psychological Review, 4:751?778.Naomi H. Feldman, Emily B. Myers, Katherine S.White, Thomas L. Griffiths, and James L. Mor-gan.
2013b.
Word-level information influencesphonetic learning in adults and infants.
Cognition,127(3):427?438.Margaret M. Fleck.
2008.
Lexicalized phonotac-tic word segmentation.
In Proceedings of ACL-08:HLT, pages 130?138, Columbus, Ohio, June.
Asso-ciation for Computational Linguistics.Stella Frank, Naomi Feldman, and Sharon Goldwater.2014.
Weak semantic context helps phonetic learn-ing in a model of infant language acquisition.
InACL (1), pages 1073?1083.Sharon Goldwater, Thomas L. Griffiths, and MarkJohnson.
2006.
Contextual dependencies in un-supervised word segmentation.
In Proceedings ofthe 21st International Conference on ComputationalLinguistics and 44th Annual Meeting of the Associa-tion for Computational Linguistics, pages 673?680,Sydney, Australia, July.
Association for Computa-tional Linguistics.Sharon Goldwater, Thomas L. Griffiths, and MarkJohnson.
2009.
A Bayesian framework for wordsegmentation: Exploring the effects of context.
Cog-nition, 112(1):21?54.James Hillenbrand, Laura A. Getty, Michael J. Clark,and Kimberlee Wheeler.
1995.
Acoustic character-istics of American English vowels.
The Journal ofthe Acoustical society of America, 97:3099.Jonathan Huggins and Frank Wood.
2014.
Infi-nite structured hidden semi-Markov models.
arXivpreprint arXiv:1407.0044, June.Aren Jansen and Kenneth Church.
2011.
Towards un-supervised training of speaker independent acousticmodels.
In INTERSPEECH, pages 1693?1692.Mark Johnson and Sharon Goldwater.
2009.
Improv-ing nonparametric Bayesian inference: Experimentson unsupervised word segmentation with adaptorgrammars.
In Proceedings of Human LanguageTechnologies: The 2009 Annual Conference of theNorth American Chapter of the Association for Com-putational Linguistics, Boulder, Colorado.Peter Ladefoged and Keith Johnson.
2010.
A course inphonetics.
Wadsworth Publishing.Chia-ying Lee and James Glass.
2012.
A nonparamet-ric Bayesian approach to acoustic model discovery.In Proceedings of the 50th Annual Meeting of theAssociation for Computational Linguistics (Volume1: Long Papers), pages 40?49, Jeju Island, Korea,July.
Association for Computational Linguistics.Chia-ying Lee, Timothy J O?Donnell, and James Glass.2015.
Unsupervised lexicon discovery from acous-tic input.
Transactions of the Association for Com-putational Linguistics, 3:389?403.Andrew Martin, Sharon Peperkamp, and EmmanuelDupoux.
2013.
Learning phonemes with a proto-lexicon.
Cognitive Science, 37:103?124.Don McAllaster, Lawrence Gillick, Francesco Scat-tone, and Michael Newman.
1998.
Fabricating con-versational speech data with acoustic models: a pro-gram to examine model-data mismatch.
In ICSLP.Daichi Mochihashi, Takeshi Yamada, and NaonoriUeda.
2009.
Bayesian unsupervised word segmen-tation with nested pitman-yor language modeling.In Proceedings of the Joint Conference of the 47thAnnual Meeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processingof the AFNLP, pages 100?108, Suntec, Singapore,August.
Association for Computational Linguistics.Kevin Murphy.
2007.
Conjugate Bayesian analysis ofthe gaussian distribution.
Technical report, Univer-sity of British Columbia.Graham Neubig, Masato Mimura, Shinsuke Mori, andTatsuya Kawahara.
2010.
Learning a languagemodel from continuous speech.
In 11th Annual Con-ference of the International Speech CommunicationAssociation (InterSpeech 2010), pages 1053?1056,Makuhari, Japan, 9.Mark A. Pitt, Keith Johnson, Elizabeth Hume, ScottKiesling, and William Raymond.
2005.
The Buck-eye corpus of conversational speech: labeling con-ventions and a test of transcriber reliability.
SpeechCommunication, 45(1):89?95.Carl Edward Rasmussen.
1999.
The infinite Gaussianmixture model.
In NIPS, volume 12, pages 554?560.Anton Rytting, Chris Brew, and Eric Fosler-Lussier.2010.
Segmenting words from natural speech: sub-segmental variation in segmental cues.
Journal ofChild Language, 37(3):513?543.Y.
W. Teh, M. I. Jordan, M. J. Beal, and D. M.Blei.
2006.
Hierarchical Dirichlet processes.Journal of the American Statistical Association,101(476):1566?1581.Gautam K. Vallabha, James L. McClelland, FerranPons, Janet F. Werker, and Shigeaki Amano.
2007.Unsupervised learning of vowel categories frominfant-directed speech.
Proceedings of the NationalAcademy of Sciences, 104(33):13273?13278.64Jurgen Van Gael, Yunus Saatci, Yee Whye Teh, andZoubin Ghahramani.
2008.
Beam sampling for theinfinite Hidden Markov model.
In Proceedings ofthe 25th International Conference on Machine learn-ing, ICML ?08, pages 1088?1095, New York, NY,USA.
ACM.Balakrishnan Varadarajan, Sanjeev Khudanpur, andEmmanuel Dupoux.
2008.
Unsupervised learningof acoustic sub-word units.
In Proceedings of the As-sociation for Computational Linguistics: Short Pa-pers, pages 165?168.65
