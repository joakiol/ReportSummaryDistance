Proceedings of the NAACL-HLT 2007 Doctoral Consortium, pages 17?20,Rochester, April 2007. c?2007 Association for Computational LinguisticsAnalysis of Summarization Evaluation ExperimentsMarie-Jos?e GouletCIRAL, Department of LinguisticsLaval University, Quebec CityG1K 7P4, Canadamarie-josee.goulet.1@ulaval.caAbstractThe goals of my dissertation are: 1) to pro-pose a French terminology for the presen-tation of evaluation results of automaticsummaries, 2) to identify and describeexperimental variables in evaluations ofautomatic summaries, 3) to highlight themost common tendencies, inconsistenciesand methodological problems in summa-rization evaluation experiments, and 4)to make recommendations for the presen-tation of evaluation results of automaticsummaries.
In this paper, I focus on thesecond objective, i.e.
identifying and de-scribing variables in summarization eval-uation experiments.1 IntroductionThe general subject of my dissertation is summa-rization evaluation.
As stated in my thesis proposal,my work aims at four goals: 1) proposing a Frenchterminology for the presentation of evaluation re-sults of automatic summaries, 2) identifying and de-scribing experimental variables in evaluations of au-tomatic summaries, 3) highlighting the most com-mon tendencies, inconsistencies and methodologicalproblems in summarization evaluations, and 4) mak-ing recommendations for the presentation of evalua-tion results of automatic summaries.
In this paper, Iwill focus on the second objective.My ultimate goal is to provide the francophonescientific community with guidelines for the evalua-tion of automatic summaries of French texts.
Evalu-ation campaigns for NLP applications already existin France, the EVALDA project1.
However, no cam-paign has yet been launched for French automaticsummaries, like Document Understanding Confer-ences for English texts or Text Summarization Chal-lenge for Japanese texts.
I hope that such a campaignwill begin in the near future and that my thesis workmay then serve as a guide for its design.2 Completed WorkI collected 22 scientific papers about summarizationevaluation, published between 1961 and 2005.
Eachpaper has been the subject of an in-depth analysis,where every detail regarding the evaluation has beencarefully noted, yielding a quasi-monstrous amountof experimental variables.
These variables havebeen classified into four categories: 1) informationabout source texts, 2) information about automaticsummaries being evaluated, 3) information aboutother summaries used in the evaluation process, and4) information about evaluation methods and crite-ria.
At the current stage of my research work, thefirst three types of variables have been analyzed andwill be presented here.2.1 Variables about source textsFour types of information about source textsemerged from the analysis: 1) the number of sourcetexts, 2) the length, 3) the type of text, and 4) the lan-guage.
First, the number of source texts is an indica-tor of the significance of the evaluation.
In my study,1http://www.elda.org/rubrique25.html17all the evaluations used less than 100 source texts,except for Mani and Bloedorn (1999) (300 sourcetexts), Brandow et al (1995) (250 source texts), Ku-piec et al (1995) (188 source texts) and Teufel andMoens (1999) (123 source texts).Secondly, regarding source text length, it is ex-pressed in different ways from one evaluation to an-other.
For example, Edmundson (1969) gives thenumber of words, Klavans et al (1998) give thenumber of sentences and Minel et al (1997) givethe number of pages.
In some papers, the lengthof the shortest and of the longest text is provided(Marcu, 1999) while in others it is the average num-ber of words, sentences or pages that is given (Teufeland Moens, 1999).
Obviously, it would be wise tostandardize the way source texts length is given inevaluation experiments.In my corpora, there are three main types ofsource texts: 1) scientific papers, 2) technical re-ports, and 3) newspapers.
Also, Minel et al (1997)used book extracts and memos, and Farzindar andLapalme (2005) used judgments of the Canadianfederal court.
All evaluations used only one type ofsource texts, except for Kupiec et al (1995) and forMinel et al (1997).Finally, the majority of the evaluations used Eng-lish texts.
Some authors used French texts (Minel etal., 1997; Ch?ar et al, 2004), Korean texts (Myaengand Jang, 1999) or Japanese texts (Nanba and Oku-mura, 2000).2.2 Variables about automatic summariesbeing evaluatedIn this section, I describe variables about automaticsummaries being evaluated.
The variables have beenclassified into six categories: 1) the total numberof automatic summaries evaluated, 2) the numberof automatic summaries produced per source text,3) if they are multiple document summaries, 4) thelength, 5) if they are extracts or abstracts, and 6)their purpose.First, concerning the total number of automaticsummaries, Brandow et al (1995), Mani and Bloe-dorn (1999), Kupiec et al (1995), Salton et al(1997) and Teufel and Moens (1999) evaluated re-spectively 750, 300, 188, 150 and 123 automaticsummaries.
All the other studies for which this in-formation is given evaluated less than 100 automaticsummaries.
It may appear redundant to give thenumber of source texts and the number of automaticsummaries in an evaluation, but sometimes morethan one automatic summary per source text mayhave been produced.
This is the case in Brandowet al (1995) and Barzilay and Elhadad (1999) whereautomatic summaries of different lengths have beenevaluated.Automatic summaries can either be producedfrom one text or more than one text.
In my cor-pora, only Mani and Bloedorn (1999) and Ch?ar etal.
(2004) evaluated multiple document summaries.As for source texts, automatic summary length isexpressed in different ways from one evaluation toanother.
Moreover, it is not always expressed in thesame way than source text length, which is inconsis-tent.On a different note, most experiments evaluatedextracts, except for Maybury (1999) and Saggionand Lapalme (2002) who evaluated abstracts, re-flecting the predominance of systems producing ex-tracts in the domain of summarization.
Extracts aresummaries produced by extracting the most impor-tant segments from texts while abstracts are the re-sult of a comprehension process and text generation.Most extracts evaluated are composed of sentences,except for Salton et al (1997) and Ch?ar et al (2004)where they are respectively composed of paragraphsand passages.
The type of automatic summaries iscrucial information because it normally influencesthe choice of the evaluation method and criteria.
In-deed, we do not evaluate extracts and abstracts in thesame way since they are not produced in the sameway.
Also, their purposes generally differ, which canalso influence the choice of the evaluation methodand criteria.Last, some papers contain the specific purpose ofautomatic summaries, not only if they are indica-tive or informative, which is interesting because itcan sometimes explain the choice of the evaluationmethod.
Only 9 experiments out of 22 give this in-formation in my corpora.2.3 Variables about other summaries used inthe evaluation processOne of the most common evaluation methods con-sists of comparing automatic summaries with othersummaries.
During my analysis, I identified seven18types of information about these other summaries:1) the total number of other summaries, 2) the typeof summaries, 3) the length, 4) the total number ofhuman summarizers, 5) the number of human sum-marizers per source text, 6) the instructions given tothe human summarizers, and 7) the human summa-rizers?
profile.The number of other summaries does not neces-sarily correspond to the number of automatic sum-maries evaluated, depending on many factors: theuse of other summaries of different types or differentlengths, the number of persons producing the othersummaries, the number of other systems producingthe other summaries, and so on.There are two general types of summaries usedfor comparison with the automatic summaries be-ing evaluated.
First, gold standard summaries (ortarget summaries) can be author summaries, pro-fessional summaries or summaries produced specif-ically for the evaluation.
Second, baseline sum-maries are generally produced by extracting randomsentences from source texts or produced by anothersystem.In my corpora, gold standard summaries are of-ten produced specifically for the evaluation.
In mostcases, they are produced by manually extracting themost important passages, sentences or paragraphs,allowing automatic comparison between automaticsummaries and gold standard summaries.On the other hand, many evaluations used base-line summaries.
For example, Barzilay and Elhadad(1999) used summaries produced by Word AutoSum-marize, Hovy and Lin (1999) used summaries pro-duced by automatically extracting random sentencesfrom source texts.
In Brandow et al (1995), Kupiecet al (1995) and Teufel and Moens (1999), baselinesummaries were produced by automatically extract-ing sentences at the beginning of the texts, and inMyaeng and Jang (1999) by extracting the first fivesentences of the conclusion.Logically, the length of the summaries used forthe comparison should be equivalent to the length ofthe automatic summaries being evaluated.
If auto-matic summaries of different lengths are evaluated,there should be corresponding baselines and/or goldstandard summaries for each length, unless the goalof the evaluation is to determine if the length plays arole in the quality of automatic summaries.Many of the evaluations analyzed do not indicatethe number of human summarizers participating inthe production of gold standard summaries.
A few ofthem specify the total number of persons involved,but not the number for each source text.
This is animportant variable because summarizing, either byextracting or abstracting, is a subjective task.
Themore people involved in the summarization of onetext, the more we can consider the final summaryto be reliable.
From the pieces of information I wasable to gather, the number of summarizers per sourcetext ranges from 1 to 13 in my corpora.In analyzing the evaluations of my corpora, I re-alized that some authors gave clear instructions tothe human summarizers, for example Edmundson(1969).
In other cases, authors asked the summariz-ers to extract the most ?important?
sentences.
Theterm ?important?
includes other terms like represen-tative, informative, relevant, and eligible.
It is rarelymentioned however if those words were explained tothe summarizers.I also noticed that some evaluations used peoplecoming from different backgrounds, for example inSalton et al (1997), while others used more homo-geneous groups, for example in Barzilay and El-hadad (1999) and Kupiec et al (1995).3 Future DirectionsIn the next couple of months, I plan to analyze evalu-ation methods identified in my corpora, for examplecomparing automatic summaries with gold standardor baseline summaries, and asking judges to givetheir opinion on the quality of automatic summaries.I will also describe evaluation criteria used to as-sess the quality of the automatic summaries, for ex-ample informativeness and readability.
Next, I willmake recommendations for the presentation of sum-marization evaluation results, based on the knowl-edge acquired from my analysis of 22 scientific pa-pers, and from previous evaluation campaigns.4 ConclusionIn this paper, I described variables about sourcetexts, about automatic summaries being evaluatedand about other summaries used in summarizationevaluation experiments.
These variables provideimportant information for the understanding of the19evaluation results presented in a scientific paper.
Myanalysis is based on 22 scientific papers on summa-rization evaluation, which is to my knowledge thelargest study on the variables found in evaluation ex-periments.
This constitutes a notable contribution inthe domain of summarization.
In another paper (inFrench) to appear, I propose a French terminologyfor the presentation of evaluation results in the do-main of summarization, which is also a major con-tribution.To conclude, the analysis presented in this pa-per gave an overview of summarization evaluationhabits since 1961.
Also, it showed that there isno common agreement as to how evaluation resultsshould be presented in a scientific paper about auto-matic summaries.AcknowledgementsI would like to thank the SSHRC and the FQRSCfor granting me doctoral scholarships.
I would alsolike to thank Jo?l Bourgeoys, Neil Cruickshank,Lorraine Couture and the anonymous reviewer fortheir useful comments.ReferencesR.
Barzilay and M. Elhadad.
1999.
Using lexical chainsfor text summarization.
In I. Mani and M. T. May-bury, editors, Advances in Automatic Text Summariza-tion, pages 111?121, Cambridge, Massachusetts.
MITPress.R.
Brandow, K. Mitze, and L. Rau.
1995.
Auto-matic condensation of electronic publications by sen-tence selection.
Information Processing Management,31(5):675?685.S.
L. Ch?ar, O. Ferret, and C. Fluhr.
2004.
Filtrage pourla construction de r?sum?s multidocuments guid?epar un profil.
Traitement automatique des langues,45(1):65?93.H.
P. Edmundson.
1969.
New methods in automatic ab-stracting.
Journal of the Association for ComputingMachinery, 16(2):264?285.A.
Farzindar and G. Lapalme.
2005.
Production automa-tique de r?sum?
de textes juridiques : ?valuation dequalit?
et d?acceptabilit?.
In TALN, pages 183?192,Dourdan.E.
Hovy and C.-Y.
Lin.
1999.
Automated text sum-marization in SUMMARIST.
In I. Mani and M. T.Maybury, editors, Advances in Automatic Text Sum-marization, pages 81?94, Cambridge, Massachusetts.MIT Press.J.
L. Klavans, K. R. McKeown, M.-Y.
Kan, and S. Lee.1998.
Resources for the evaluation of summarizationtechniques.
In Antonio Zampolli, editor, LREC, pages899?902, Granada, Spain.J.
Kupiec, J. Pedersen, and F. Chen.
1995.
A trainabledocument summarizer.
In SIGIR, pages 68?73, Seat-tle.I.
Mani and E. Bloedorn.
1999.
Summarizing simi-larities and differences among related documents.
InI.
Mani and M. T. Maybury, editors, Advances in Au-tomatic Text Summarization, pages 357?379, Cam-bridge, Massachusetts.
MIT Press.D.
Marcu.
1999.
Discourse trees are good indicatorsof importance in text.
In I. Mani and M. T. May-bury, editors, Advances in Automatic Text Summariza-tion, pages 123?136, Cambridge, Massachusetts.
MITPress.M.
Maybury.
1999.
Generating summaries from eventdata.
In I. Mani and M. T. Maybury, editors, Ad-vances in Automatic Text Summarization, pages 265?281, Cambridge, Massachusetts.
MIT Press.J.-L. Minel, S. Nugier, and G. Piat.
1997.
How to ap-preciate the quality of automatic text summarization?Examples of FAN and MLUCE protocols and their re-sults on SERAPHIN.
In EACL, pages 25?31, Madrid.S.
H. Myaeng and D.-H. Jang.
1999.
Developmentand evaluation of a statistically-based document sum-marization system.
In I. Mani and M. T. Maybury,editors, Advances in Automatic Text Summarization,pages 61?70, Cambridge, Massachusetts.
MIT Press.H.
Nanba and M. Okumura.
2000.
Producing morereadable extracts by revising them.
In 18th Inter-national Conference on Computational Linguistics,pages 1071?1075, Saarbrucker.H.
Saggion and G. Lapalme.
2002.
Generat-ing indicative-informative summaries with SumUM.Computational Linguistics, 28(4):497?526.G.
Salton, A. Singhal, M. Mitra, and C. Buckley.
1997.Automatic text structuring and summarization.
Infor-mation Processing and Management, 33(2):193?207.S.
Teufel and M. Moens.
1999.
Argumentative clas-sification of extracted sentences as a first step to-wards flexible abstracting.
In I. Mani and M. T. May-bury, editors, Advances in Automatic Text Summariza-tion, pages 155?171, Cambridge, Massachusetts.
MITPress.20
