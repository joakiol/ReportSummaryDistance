BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 46?53,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsRecognizing Speculative Language in Biomedical Research Articles:A Linguistically Motivated PerspectiveHalil Kilicoglu and Sabine BerglerDepartment of Computer Science and Software EngineeringConcordia UniversityMontreal, Quebec, Canada{h_kilico, bergler}@cse.concordia.caAbstractWe explore a linguistically motivated ap-proach to the problem of recognizingspeculative language (?hedging?)
in bio-medical research articles.
We describe amethod, which draws on prior linguisticwork as well as existing lexical resourcesand extends them by introducing syntacticpatterns and a simple weighting scheme toestimate the speculation level of the sen-tences.
We show that speculative languagecan be recognized successfully with suchan approach, discuss some shortcomings ofthe method and point out future researchpossibilities.1 IntroductionScience involves making hypotheses, experiment-ing, and reasoning to reach conclusions, which areoften tentative and provisional.
Scientific writing,particularly in biomedical research articles, reflectsthis, as it is rich in speculative statements, alsoknown as hedges.
Most text processing systemsignore hedging and focus on factual language (as-sertions).
Although assertions, sometimes mere co-occurrence of terms, are the focus of most infor-mation extraction and text mining applications,identifying hedged text is crucial, because hedgingalters, in some cases even reverses, factual state-ments.
For instance, the italicized fragment in ex-ample (1) below implies a factual statement whileexample (2) contains two hedging cues (indicateand might), which render the factual propositionspeculative:(1) Each empty cell indicates that the corre-sponding TPase query was not used at the par-ticular stage of PSI-BLAST analysis.
(2) These experiments indicated that the roXgenes might function as nuclear entry sites forthe assembly of the MSL proteins on the Xchromosome.These examples not only illustrate the phe-nomenon of hedging in the biomedical literature,they also highlight some of the difficulties in rec-ognizing hedges.
The word indicate plays a differ-ent role in each example, acting as a hedging cueonly in the second.In recent years, there has been increasing inter-est in the speculative aspect of biomedical lan-guage (Light et al, 2004, Wilbur et al, 2006,Medlock and Briscoe, 2007).
In general, thesestudies focus on issues regarding annotatingspeculation and approach the problem of recog-nizing speculation as a text classification problem,using the well-known ?bag of words?
method(Light et al 2004, Medlock and Briscoe, 2007) orsimple substring matching (Light et al, 2004).While both approaches perform reasonably well,they do not take into account the more complexand strategic ways hedging can occur in biomedi-cal research articles.
In example (3), hedging isachieved with a combination of referring to ex-perimental results (We ... show that ?
indicating)and the prepositional phrase to our knowledge:(3) We further show that D-mib is specificallyrequired for Ser endocytosis and signalingduring wing development indicating for thefirst time to our knowledge that endocytosisregulates Ser signaling.In this paper, we extend previous work throughlinguistically motivated techniques.
In particular,we pay special attention to syntactic structures.
We46address lexical hedges by drawing on a set of lexi-cal hedging cues and expanding and refining it in asemi-automatic manner to acquire a hedging dic-tionary.
To capture more complex strategic hedges,we determine syntactic patterns that commonly actas hedging indicators by analyzing a publiclyavailable hedge classification dataset.
Further-more, recognizing that ?not all hedges are createdequal?, we use a weighting scheme, which alsotakes into consideration the strengthening or weak-ening effect of certain syntactic structures on lexi-cal hedging cues.
Our results demonstrate thatlinguistic knowledge can be used effectively toenhance the understanding of speculative language.2 Related WorkThe term hedging was first used in linguistic con-text by Lakoff (1972).
He proposed that naturallanguage sentences can be true or false to someextent, contrary to the dominant truth-conditionalsemantics paradigm of the era.
He was mainlyconcerned with how words and phrases, such asmainly and rather, make sentences fuzzier or lessfuzzy.Hyland (1998) provides one of the most com-prehensive accounts of hedging in scientific arti-cles in the linguistics literature.
He views hedgesas polypragmatic devices with an array of purposessuch as weakening the force of statement, ex-pressing deference to the reader and signaling un-certainty.
He proposes a fuzzy model, in which hecategorizes scientific hedges by their pragmaticpurpose, such as reliability hedges and reader-oriented hedges.
He also identifies the principalsyntactic realization devices for different types ofhedges, including epistemic verbs (verbs indicatingthe speaker?s mode of knowing), adverbs and mo-dal auxiliaries and presents the most frequentlyused members of these types based on analysis of amolecular biology article corpus.Palmer (1986) identifies epistemic modality,which expresses the speaker?s degree of commit-ment to the truth of proposition and is closelylinked to hedging.
He identifies three types ofepistemic modality: ?speculatives?
express uncer-tainty, ?deductives?
indicate an inference from ob-servable evidence, and ?assumptives?
indicateinference from what is generally known.
He fo-cuses mainly on the use of modal verbs in ex-pressing various types of epistemic modality.In their investigation of event recognition innews text, Saur?
et al (2006) address event modal-ity at the lexical and syntactic level by means ofSLINKs (subordination links), some of which(?modal?, ?evidential?)
indicate hedges.
They usecorpus-induced lexical knowledge from TimeBank(Pustejovsky et al (2003)), standard linguisticpredicate classifications, and rely on a finite-statesyntactic module to identify subordinated eventsbased on the subcategorization properties of thesubordinating event.DiMarco and Mercer (2004) study the intendedcommunicative purpose (dispute, confirmation, useof materials, tools, etc.)
of citations in scientifictext and show that hedging is used more frequentlyin citation contexts.In the medical field, Friedman et al (1994) dis-cuss uncertainty in radiology reports and theirnatural language processing system assigns one offive levels of certainty to extracted findings.Light et al (2004) explore issues with annotat-ing speculative language in biomedicine and out-line potential applications.
They manually annotatea corpus of approximately 2,000 sentences fromMEDLINE abstracts.
Each sentence is annotated asbeing definite, low speculative and highly specula-tive.
They experiment with simple substringmatching and a SVM classifier, which uses singlewords as features.
They obtain slightly better accu-racy with simple substring matching suggestingthat more sophisticated linguistic knowledge mayplay a significant role in identification of specula-tive language.
It is also worth noting that bothtechniques yield better accuracy over full abstractsthan on the last two sentences of abstracts, inwhich speculative language is found to be moreprevalent.Medlock and Briscoe (2007) extend Light etal.
?s (2004) work, taking full-text articles into con-sideration and applying a weakly supervisedlearning model, which also uses single words asfeatures, to classify sentences as simply specula-tive or non-speculative.
They manually annotate atest set and employ a probabilistic model fortraining set acquisition using suggest and likely asseed words.
They use Light et al?s substringmatching as the baseline and improve to a re-call/precision break-even point (BEP) of 0.76, us-ing a SVM committee-based model from 0.60recall/precision BEP of the baseline.
They note thattheir learning models are unsuccessful in identify-47ing assertive statements of knowledge paucity,generally marked syntactically rather than lexi-cally.Wilbur et al (2006) suggest that factual infor-mation mining is not sufficient and present an an-notation scheme, in which they identify fivequalitative dimensions that characterize scientificsentences: focus (generic, scientific, methodology),evidence (E0-E3), certainty (0-3), polarity (posi-tive, negative) and trend (+,-).
Certainty and evi-dence dimensions, in particular, are interesting interms of hedging.
They present this annotationscheme as the basis for a corpus that will be usedto automatically classify biomedical text.Discussion of hedging in Hyland (1998) pro-vides the basic linguistic underpinnings of thestudy presented here.
Our goals are similar to thoseoutlined in the work of Light et al (2004) andMedlock and Briscoe (2007); however, we proposethat a more linguistically oriented approach notonly could enhance recognizing speculation, butwould also bring us closer to characterizing thesemantics of speculative language.
Some of thework discussed above (in particular, Saur?
et al(2006) and Wilbur et al (2006)) will be relevant inthat regard.3 MethodsTo develop an automatic method to identifyspeculative sentences, we first compiled a set ofcore lexical surface realizations of hedging drawnfrom Hyland (1998).
Next, we augmented this setby analyzing a corpus of 521 sentences, 213 ofwhich are speculative, and also noted certain syn-tactic structures used for hedging.
Furthermore, weidentified lexical cues and syntactic patterns thatstrongly suggest non-speculative contexts (?un-hedgers?).
We then expanded and manually refinedthe set of lexical hedging and ?unhedging?
cuesusing WordNet (Fellbaum, 1998) and the UMLSSPECIALIST Lexicon (McCray et al, 1994).Next, we quantified the strength of the hedgingcues and patterns through corpus analysis.
Finally,to recognize the syntactic patterns, we used theStanford Lexicalized Parser (Klein and Manning,2003) and its dependency parse representation(deMarneffe et al, 2006).
We use weights assignedto hedging cues to compute an overall hedgingscore for each sentence.To evaluate the effectiveness of our method, weused basic information retrieval evaluation metrics:precision, recall, accuracy and F1score.
In addi-tion, we measure the recall/precision break-evenpoint (BEP), which indicates the point at whichprecision and recall are equal, to provide a com-parison to results previously reported.
As baseline,we use the substring matching method, describedin Light et al (2004) in addition to another sub-string matching method, which uses terms rankedin top 15 in Medlock and Briscoe (2007).
Tomeasure the statistical significance of differencesbetween the performances of baseline and oursystem, we used the binomial sign test.4 Data SetIn our experiments, we use the publicly availablehedge classification dataset1, reported in Medlockand Briscoe (2007).
This dataset consists of amanually annotated test set of 1537 sentences (380speculative) extracted from six full-text articles onDrosophila melanogaster (fruit-fly) and a trainingset of 13,964 sentences (6423 speculative) auto-matically induced using a probabilistic acquisitionmodel.
A pool of 300,000 sentences randomly se-lected from an archive of 5579 full-text articlesforms the basis for training data acquisition anddrives their weakly supervised hedge classificationapproach.While this probabilistic model for training dataacquisition is suitable for the type of weakly su-pervised learning approach they describe, we findthat it may not be suitable as a fair data sample,since the speculative instances overemphasizecertain hedging cues used as seed terms (suggest,likely).
On the other hand, the manually annotatedtest set is valuable for our purposes.
To train oursystem, we (the first author) manually annotated aseparate training set of 521 sentences (213 specu-lative) from the pool, using the annotation guide-lines provided.
Despite being admittedly small, thetraining set seems to provide a good sample, as thedistribution of surface realization features (epis-temic verbs (32%), adverbs (26%), adjectives(19%), modal verbs (%21)) correspond roughly tothat presented in Hyland (1998).5 Core Surface Realizations of Hedging1http://www.benmedlock.co.uk/hedgeclassif.html48Hyland (1998) provides the most comprehensiveaccount of surface realizations of hedging in sci-entific articles, categorizing them into two classes:lexical and non-lexical features.
Lexical featuresinclude modal auxiliaries (may and might being thestrongest indicators), epistemic verbs, adjectives,adverbs and nouns.
Some common examples ofthese feature types are given in Table 1.Feature Type ExamplesModal auxiliaries may, might, could, would,shouldEpistemic judgmentverbssuggest, indicate, specu-late, believe, assumeEpistemic evidentialverbsappear, seemEpistemic deductiveverbsconclude, infer, deduceEpistemic adjectives likely, probable, possibleEpistemic adverbs probably, possibly, per-haps, generallyEpistemic nouns possibility, suggestionTable 1.
Lexical surface features of hedgingNon-lexical hedges usually include referenceto limiting experimental conditions, reference to amodel or theory or admission to a lack of knowl-edge.
Their surface realizations typically go be-yond words and even phrases.
An example is givenin sentence (4), with hedging cues italicized.
(4) Whereas much attention has focused on eluci-dating basic mechanisms governing axon de-velopment, relatively little is known about thegenetic programs required for the establish-ment of dendrite arborization patterns that arehallmarks of distinct neuronal types.While lexical features can arguably be exploitedeffectively by machine learning approaches, auto-matic identification of non-lexical hedges auto-matically seems to require syntactic and, in somecases, semantic analysis of the text.Our first step was to expand on the core lexicalsurface realizations identified by Hyland (1998).6 Expansion of Lexical Hedging CuesEpistemic verbs, adjectives, adverbs and nounsprovide the bulk of the hedging cues.
Althoughepistemic features are commonly referred to andanalyzed in the linguistics literature and variouswidely used lexicons exist that classify differentpart-of-speech (e.g., VerbNet (Kipper Schuler,2005) for verb classes), we are unaware of anysuch comprehensive classification based on epis-temological status of the words.
We explore in-ducing such a lexicon from the core lexicalexamples identified in Hyland (1998) (a total of 63hedging cues) and expanding it semi-automaticallyusing two lexicons: WordNet (Fellbaum, 1998)and UMLS SPECIALIST Lexicon (McCray,1994).We first extracted synonyms for each epistemicterm in our list using WordNet synsets.
We thenremoved those synonyms that did not occur in ourpool of sentences, since they are likely to be veryuncommon words in scientific articles.
Expandingepistemic verbs is somewhat more involved thanexpanding other epistemic types, as they tend tohave more synsets, indicating a greater degree ofword sense ambiguity (assume has 9 synsets).Based on the observation that an epistemic verbtaking a clausal complement marked with that is avery strong indication of hedging, we only consid-ered verb senses which subcategorize for a thatcomplement.
Expansion via WordNet resulted in66 additional lexical features.Next, we considered the case of nominaliza-tions.
Again, based on corpus analysis, we notedthat nominalizations of epistemic verbs and adjec-tives are a common and effective means of hedgingin molecular biology articles.
The UMLSSPECIALIST Lexicon provides syntactic informa-tion, including nominalizations, for biomedical aswell as general English terms.
We extracted thenominalizations of words in our expanded diction-ary of epistemic verbs and adjectives from UMLSSPECIALIST Lexicon and discarded those that donot occur in our pool of sentences, resulting in anadditional 48 terms.
Additional 5 lexical hedgingcues (e.g., tend, support) were identified via man-ual corpus analysis and further expanded using themethodology described above.An interesting class of cues are terms expressingstrong certainty (?unhedgers?).
Used within thescope of negation, these terms suggest hedging,while in the absence of negation they strongly sug-gest a non-speculative context.
Examples of theseinclude verbs indicating certainty, such as know,demonstrate, prove and show, and adjectives, suchas clear.
These features were also added to thedictionary and used together with other surface49cues to recognize speculative sentences.
Thehedging dictionary contains a total of 190 features.7 Quantifying Hedging StrengthIt is clear that not all hedging devices are equallystrong and that the choice of hedging device affectsthe strength of the speculation.
However, deter-mining the strength of a hedging device is nottrivial.
The fuzzy pragmatic model proposed byHyland (1998) employs general descriptive termssuch as ?strong?
and ?weak?
when discussing par-ticular cases of hedging and avoids the need forprecise quantification.
Light et al (2004) reportlow inter-annotator agreement in distinguishinglow speculative sentences from highly speculativeones.
From a computational perspective, it wouldbe useful to quantify hedging strength to determinethe confidence of the author in his or her proposi-tion.As a first step in accommodating noticeable dif-ferences in strengths of hedging features, we as-signed weights (1 to 5, 1 representing the lowesthedging strength and 5 the highest) to all hedgingfeatures in our dictionary.
Core features were as-signed weights based on the discussion in Hyland(1998).
For instance, he identifies modal auxilia-ries, may and might, as the prototypical hedgingdevices, and they were given weights of 5.
On theother hand, modal auxiliaries commonly used innon-epistemic contexts (would, could) were as-signed a lower weight of 3.
Though not as strongas may and might, core epistemic verbs and ad-verbs are generally good hedging cues and there-fore were assigned weights of 4.
Core epistemicadjectives and nouns often co-occur with othersyntactic features to act as strong hedging cues andwere assigned weights of 3.
Terms added to thedictionary via expansion were assigned a weightone less than their seed terms.
For instance, thenominalization supposition has weight 2, since it isexpanded from the verb suppose (weight 3), whichis further expanded from its synonym speculate(weight 4), a core epistemic verb.
The reduction inweights of certain hedging cues reflects their pe-ripheral nature in hedging.Hyland (1998) notes that writers tend to com-bine hedges (?harmonic combinations?)
and sug-gests the possibility of constructing scales ofcertainty and tentativeness from these combina-tions.
In a similar vein, we accumulate the weightsof the hedging features found in a sentence andassign an overall hedging score to each sentence.8 The Role of SyntaxCorpus analysis shows that various syntactic de-vices play a prominent role in hedging, both ashedging cues and for strengthening or weakeningeffects.
For instance, while some epistemic verbsdo not act as hedging cues (or may be weak hedg-ing cues) when used alone, together with a thatcomplement or an infinitival clause, they are goodindicators of hedging.
A good example is appear,which often occurs in molecular biology articleswith its ?come into sight?
meaning (5) and be-comes a good hedging cue when it takes an infini-tival complement (6):(5) The linearity of the ommatidial arrangementwas disrupted and numerous gaps appearedbetween ommatidia arrow.
(6) In these data a substantial fraction of both si-lent and replacement DNA mutations appear toaffect fitness.On the other hand, as discussed above, wordsexpressing strong certainty (?unhedgers?)
are goodindicators of hedging when negated, and stronglynon-speculative otherwise.We examined the training set and identified themost salient syntactic patterns that play a role inhedging.
A syntactic pattern, or lack thereof, af-fects the overall score assigned to a hedging cue; astrengthening syntactic pattern will increase theoverall score contributed by the cue, while a weak-ening pattern will decrease it.
For instance, in sen-tence (5) above, the absence of the infinitivalcomplement will reduce the score contribution ofappear by 1, resulting in a score of 3 instead of 4.On the other hand, that appear takes an infinitivalclause in example (6) will increase the score con-tribution of appear by 1.
All score contributions ofa sentence add up to its hedging score.A purely syntactic case is that of whether (if).Despite being a conjunction, it seems to act as ahedging cue when it introduces a clausal comple-ment regardless of existence of any other hedgingcue from the hedging dictionary.
The basic syntac-tic patterns we identified and implemented andtheir effect on the overall hedging score are givenin Table 2.50To obtain the syntactic structures of sentences,we used the statistical Stanford Lexicalized Parser(Klein and Manning, 2003), which provides a fullparse tree, in addition to part-of-speech taggingbased on the Penn Treebank tagset.
A particularlyuseful feature of the Stanford Lexicalized Parser istyped dependency parses extracted from phrasestructure parses (deMarneffe, et al (2006)).
Weuse these typed dependency parses to identifyclausal complements, infinitival clauses and nega-tion.
For instance, the following two dependencyrelations indicate a clausal complement markedwith that and identify the second syntactic patternin Table 2.ccomp(<EPISTEMIC VERB>,<VB>)complm(<VB>,that)In these relations, ccomp stands for clausalcomplement with internal subject and complmstands for complementizer.
VB indicates any verb.Syntactic Pattern Effecton Score+1+2<EPISTEMIC VERB> to(inf) VB<EPISTEMIC VERB> that(comp) VBOtherwise-1+2<EPISTEMIC NOUN> followed bythat(comp)Otherwise-1not <UNHEDGING VERB> +1no| not <UNHEDGING NOUN> +2no| not immediately followed by<UNHEDGING ADVERB>+1no| not immediately followed by<UNHEDGING ADJECTIVE>+1whether| if in a clausal complementcontext3Table 2.
Syntactic patterns and their effect on the over-all hedging score.9 BaselineFor our experiments, we used two baselines.
First,we used the substring matching method reported inLight et al (2004), which labels sentences con-taining one of more of the following as specula-tive: suggest, potential, likely, may, at least, inpart, possibl, further investigation, unlikely, puta-tive, insights, point toward, promise and propose(Baseline1).
Secondly, we used the top 15 rankedterm features determined using P(spec|xj) in train-ing and classification models (at smoothing pa-rameter?
?=5) reported in Medlock and Briscoe(2007): suggest, likely, may, might, seems, Taken,suggests, probably, Together, suggesting, possibly,suggested, findings, observations, Given.
Our sec-ond baseline uses the substring matching methodwith these features (Baseline2).10 ResultsThe evaluation results obtained using the baselinemethods are given in Table 3.Method Precision Recall Accuracy F1scoreBaseline1 0.79 0.40 0.82 0.53Baseline2 0.95 0.43 0.85 0.60Table 3.
Baseline evaluation results.The evaluation results obtained from our systemby varying the overall hedging score and using itas threshold are given in Table 4.
It is worth notingthat the highest overall hedging score we obtainedwas 16; however, we do not show the results forevery possible threshold here for brevity.HedgingScoreThresholdPrecision Recall Accuracy F1score1 0.68 0.95 0.88 0.792 0.75 0.94 0.91 0.833 0.85 0.86 0.93 0.854 0.91 0.71 0.91 0.805 0.92 0.63 0.89 0.756 0.97 0.40 0.85 0.577 1 0.19 0.79 0.33Table 4.
Evaluation results from our system.As seen from Table 3 and Table 4, our resultsshow improvement over both baseline methods interms of accuracy and F1score.
Increasing thethreshold (thereby requiring more or strongerhedging devices to qualify a sentence as specula-tive) improves the precision while lowering therecall.
The best accuracy and F1score are achievedat threshold t=3.
At this threshold, the differencesbetween the results obtained with our method andbaseline methods are statistically significant at0.01 level (p < 0.01).51Method Recall/Precision BEPBaseline1 0.60Baseline2 0.76Our system 0.85Table 5.
Recall / precision break-even point (BEP) re-sultsWith the threshold providing the best accuracyand F1score, precision and recall are roughly thesame (0.85), indicating a recall/precision BEP ofapproximately 0.85, also an improvement over0.76 achieved with a weakly supervised classifier(Medlock and Briscoe, 2007).
Recall/precisionBEP scores are given in Table 5.11 DiscussionOur results confirm that writers of scientific arti-cles employ basic, predictable hedging strategies tosoften their claims or to indicate uncertainty anddemonstrate that these strategies can be capturedusing a combination of lexical and syntacticmeans.
Furthermore, the results indicate thathedging cues can be gainfully weighted to providea rough measure of tentativeness or uncertainty.For instance, a sentence with the highest overallhedging score is given below:(7) In one study, Liquid facets was proposed totarget Dl to an endocytic recycling compart-ment suggesting that recycling of Dl may berequired for signaling.On the other hand, hedging is not strong in thefollowing sentence, which is assigned an overallhedging score of 2:(8) There is no apparent need for cytochrome crelease in C. elegans since CED-4 does not re-quire it to activate CED-3.Below, we discuss some of the common errortypes we encountered.
Our discussion is based onevaluation at hedging score threshold of 0, whereexistence of a hedging cue is sufficient to label asentence speculative.Most of the false negatives produced by thesystem are due to syntactic patterns not addressedby our method.
For instance, negation of ?unhedg-ers?
was used as a syntactic pattern; the patternwas able to recognize know as an ?unhedger?
inthe following sentence, but not the negative quanti-fier (l i t t le), labeling the sentence as non-speculative.
(9) Little was known however about the specificrole of the roX RNAs during the formation ofthe DCC.In fact, Hyland (1998) notes ?negation in scien-tific research articles shows a preference for nega-tive quantifiers (few, little) and lexical negation(rarely, overlook).?
However, we have not en-countered this pattern while analyzing the trainingset and have not addressed it.
Nevertheless, ourapproach lends itself to incremental developmentand adding such a pattern to our rulebase is rela-tively simple.Another type of false negative is caused by cer-tain derivational forms of epistemic words.
In thefollowing example, the adjective suggestive is notrecognized as a hedging trigger, even though itsbase form suggest is an epistemic verb.
(10) Phenotypic differences are suggestive ofdistinct functions for some of these genes inregulating dendrite arborization.It seems that more sophisticated lexicon expan-sion rules can be employed to handle such cases.For example, WordNet?s ?derivationally relatedform?
feature may be used as the basis of theseexpansion rules.Regarding false positives, most of them are dueto word sense ambiguity concerning hedging cues.For instance, the modal auxiliary could  is fre-quently used as a past tense form of can in scien-tific articles to express the role of enablingconditions and external constraints on the occur-rence of the proposition rather than uncertainty ortentativeness regarding the proposition.
Currently,our system is unable to recognize such cases.
Anexample is given below:(10) Also we could not find any RAG-like se-quences in the recently sequenced sea urchinlancelet hydra and sea anemone genomes,which encode RAG-like sequences.The context around the hedging cue seems toplay a role in these cases.
First person plural pro-noun (we) and/or reference to objective enablingconditions seem to be a common characteristicamong false positive cases of could.In other cases, such as appear, in the absence ofstrengthening syntactic cues (to, that), we lowerthe hedging score; however, depending on thethreshold, this may not be sufficient to render thesentence non-speculative.
Rather than loweringthe score equally for all epistemic verbs, a more52appropriate approach would be to consider verbsenses separately (e.g., appear should be effec-tively unhedged without a strengthening cue, whilesuggest should only be weakened).Another type of false positives concern ?weak?hedging cues, such as epistemic deductive verbs(conclude, estimate) as well as adverbs (essen-tially, usually) and nominalizations (implication,assumption).We have also seen a few instances, which seemspeculative on the surface, but were labeled non-speculative.
An example is given below:(11) Caspases can also be activated with the aidof Apaf-1, which in turn appears to be regu-lated by cytochrome c and dATP.12 Conclusion and Future WorkIn this paper, we present preliminary experimentswe conducted in recognizing speculative sentences.We draw on previous linguistic work and extend itvia semi-automatic methods of lexical acquisition.Using a corpus specifically annotated for specula-tion, we demonstrate that our linguistically ori-ented approach improves on the previouslyreported results.Our next goal is to extend our work using alarger, more comprehensive corpus.
This will al-low us to identify other commonly used hedgingstrategies and refine and expand the hedging dic-tionary.
We also aim to refine the weightingscheme in a more principled way.While recognizing that a sentence is speculativeis useful in and of itself, it seems more interestingand clearly much more challenging to identifyspeculative sentence fragments and the proposi-tions that are being hedged.
In the future, we willmove in this direction with the goal of character-izing the semantics of speculative language.AcknowledgementsWe would like to thank Thomas C. Rindflesch forhis suggestions and comments on the first draft ofthis paper.ReferencesdeMarneffe, M. C., MacCartney B., Manning C.D.2006.
Generating Typed Dependency Parses fromPhrase Structure Parses.
In Proc of 5th InternationalConference on Language Resources and Evaluation,pp.
449-54.DiMarco C. and Mercer R.E.
2004.
Hedging in Scien-tific Articles as a Means of Classifying Citations.
InExploring Attitude and Affect in Text: Theories andApplications AAAI-EAAT 2004. pp.50-4.Fellbaum, C. 1998.
WordNet: An Electronic LexicalDatabase.
MIT Press, Cambridge, MA.Friedman C., Alderson P., Austin J., Cimino J.J., John-son S.B.
1994.
A general natural-language text proc-essor for clinical radiology.
Journal of the AmericanMedical Informatics Association, 1(2): 161-74.Hyland K. 1998.
Hedging in Scientific Research Arti-cles.
John Benjamins B.V., Amsterdam, Netherlands.Kipper Schuler, K. 2005.
VerbNet: A broad-coverage,comprehensive verb lexicon.
PhD thesis, Universityof Pennsylvania.Klein D. and Manning C. D. 2003.
Accurate unlexical-ized parsing.
In Proc of 41st Meeting of the Associa-tion for Computational Linguistics.
pp.
423-30.Lakoff  G. 1972.
Hedges: A Study in Meaning Criteriaand the Logic of Fuzzy Concepts.
Chicago Linguis-tics Society Papers, 8, pp.183-228.Light M., Qiu X.Y., Srinivasan P. 2004.
The Languageof Bioscience: Facts, Speculations, and Statements inbetween.
In BioLINK 2004: Linking Biological Lit-erature, Ontologies and Databases, pp.
17-24.McCray A. T., Srinivasan S., Browne A. C. 1994.
Lexi-cal methods for managing variation in biomedicalterminologies.
In Proc of 18th Annual Symposium onComputer Applications  in  Medical Care, pp.
235-9.Medlock B. and Briscoe T. 2007.
Weakly SupervisedLearning for Hedge Classification in Scientific Lit-erature.
In Proc of 45thMeeting of the Association forComputational Linguistics.
pp.992-9.Palmer F.R.
1986.
Mood and Modality.
CambridgeUniversity Press, Cambridge, UK.Pustejovsky J., Hanks P., Saur?
R., See A., GaizauskasR., Setzer A., Radev D., Sundheim B., Day D.
FerroL., Lazo M. 2003.
The TimeBank Corpus.
In Proc ofCorpus Linguistics.
pp.
647-56.Saur?
R., Verhagen M., Pustejovsky J.
2006.
SlinkET: apartial modal parser for events.
In Proc of 5thInter-national Conference on Language Resources andEvaluation.Wilbur W.J., Rzhetsky A., Shatkay H. 2006.
New Di-rections in Biomedical Text Annotations: Defini-tions, Guidelines and Corpus Construction.
BMCBioinformatics, 7:356.53
