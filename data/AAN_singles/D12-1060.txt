Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 655?665, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsDo Neighbours Help?
An Exploration of Graph-based Algorithmsfor Cross-domain Sentiment ClassificationNatalia PonomarevaStatistical Cybermetrics Research group,University of Wolverhampton, UKnata.ponomareva@wlv.ac.ukMike ThelwallStatistical Cybermetrics Research group,University of Wolverhampton, UKm.thelwall@wlv.ac.ukAbstractThis paper presents a comparative studyof graph-based approaches for cross-domainsentiment classification.
In particular, thepaper analyses two existing methods: anoptimisation problem and a ranking algorithm.We compare these graph-based methods witheach other and with the other state-of-the-art approaches and conclude that graphdomain representations offer a competitivesolution to the domain adaptation problem.Analysis of the best parameters for graph-based algorithms reveals that there are nooptimal values valid for all domain pairsand that these values are dependent on thecharacteristics of corresponding domains.1 IntroductionThe sentiment classification (SC) is an active areaof research concerned automatic identification ofsentiment strength or valence of texts.
SC ofproduct reviews is commercially important andwidely researched but it typically needs to beoptimised separately for each type of product (i.e.domain).
When domain-specific data are absentor insufficient the researchers usually seek solutionin semi-supervised, unsupervised or cross-domainapproaches.
In this paper, we focus on cross-domainmethods in order to take advantage of the hugeamount of annotated sentiment data available on theInternet.
Our aim is to find out to what extent it ispossible to learn sentiment phenomena from thesedata and transfer them to new domains rather thaninduce them from scratch for each new domain.Previous research has shown that models trained onone data usually give much worse results on another,especially when both data sets belong to completelydifferent domains.
This is largely because thesentiment words and their valences depend a loton the domain where they are expressed.
Thefirst problem concerns the words that can conveyopposite sentiments with respect to the context ordomain.
For example, a word ?ridiculous?
inbook reviews may express a negative meaning whentalking about a book content, however for reviewson electronics this word can bear a positive meaningwhen talking about prices.
Another and morecommon problem is related to sentiment words thatare specific for each domain.
For instance, wordslike ?boring?, ?inspiring?, ?engaging?
are verycommon in book reviews but it is almost impossibleto find them in reviews on electronics.
At the sametime, the electronics domain can contain words like?defective?, ?refund?, ?return?, ?customer service?,which are very unusual for book reviews.Several cross-domain approaches have beensuggested recently to solve the problem of accuracyloss in cross-domain sentiment classification,namely Structural Correspondence Learning (SCL)(Blitzer et al2007), the graph-based approach(Wu et al2009) and Spectral Feature Alignment(SFA) (Pan et al2010).
In this paper, we exploregraph-based algorithms which refer to a group oftechniques that model data as a graph of documents.This data representation takes into account not onlydocument contents but also document connectivitywhich is modeled as document sentiment similarityrather than content similarity.
Our interest in graph655algorithms is two-fold.
First, graph-based domainrepresentations can benefit from two independentsources of information: scores given by a machinelearning technique which indicate the probabilityof a document to belong to a sentiment class andsimilarity relations between documents.
Second,unlike other suggested methods, this approach canbe easily adapted to multiple classes, which makesit possible to classify documents using finer-grainedsentiment scales.Different graph-based algorithms have beenapplied to several SA tasks (Pang and Lee, 2005;Goldberg and Zhu, 2006; Wu et al2009), butno comparison has been made to find the mostappropriate one for SC.
Moreover, in the frameworkof the domain adaption task, we come across theproblem of choosing the best set of parameters,which, as we further demonstrate, depends onthe characteristics of a corresponding domainpair.
Unfortunately, no study has investigated thisproblem.
(Pang and Lee, 2005; Goldberg andZhu, 2006) exploited the graph-based approach fora semi-supervised task and experimented with databelonging to one domain and, therefore did notcome across this issue.
The work of (Wu et al2009) lacks any discussion about the choice ofthe parameter values; the authors set some valuesequal for all domains without mentioning how theyobtained these numbers.The present research brings several contributions.First, we compare two graph-based algorithmsin cross-domain SC settings: the algorithmexploited in (Goldberg and Zhu, 2006), whichseeks document sentiments as an output of anoptimisation problem (OPTIM) and the algorithmadopted by (Wu et al2009), that uses rankingto assign sentiment scores (RANK).
Second,as document similarity is a crucial factor forsatisfactory performance of graph-based algorithms,we suggest and evaluate various sentiment similaritymeasures.
Sentiment similarity is different fromtopic similarity as it compares documents withrespect to the sentiment they convey rather thantheir topic.
Finally, we discover the dependencyof algorithm parameter values on domain propertiesand, subsequently, the impossibility to find universalparameter values suitable for all domain pairs.We discuss a possible strategy for choosing thebest set of parameters based on our previousstudy (Ponomareva and Thelwall, 2012), wherewe introduced two domain characteristics: domainsimilarity and domain complexity and demonstratedtheir strong correlation with cross-domain accuracyloss.The rest of the paper is structured as follows.In Section 2 we give a short overview of relatedworks on cross-domain SC.
Section 3 describes andcompares the OPTIM and RANK algorithms.
InSection 4 we discuss an issue of document similarityand select document representation that correlatesbest with document sentiments.
Experimentalresults are described in Section 5 followed by adiscussion on the strategy for choosing the bestparameter values of the algorithms (Section 6).Finally, in Section 7 we summarise our contributionsand discuss further research.2 Related workCross-domain sentiment analysis has receivedconsiderable attention during the last five yearsand, since then, several approaches to tackle thisproblem have emerged.
The most straightforwardapproach is to use an ensemble of classifiers astested in several works (Aue and Gamon, 2005; Liand Zong, 2008).
It is a well-explored technique inmachine learning concerned with training classifierson domains where annotated data are availableand then, combining them in ensembles for theclassification of target data.
Aue and Gamon (2005)studied several possibilities to combine data fromdomains with known annotations and came up withthe conclusion that an ensemble of classifiers ina meta-classifier gives higher performance than asimple merge of all features.Structural Correspondence Learning (SCL)(Blitzer et al2007) is another domain transferapproach, which was also tested on parts of speech(PoS) tagging (Blitzer et al2006).
Its underlyingidea is to find correspondences between featuresfrom source and target domains through modelingtheir correlations with pivot features.
Pivot featuresare features occurring frequently in both domains,which, at the same time, serve as good predictorsof document classes, like the general sentimentwords ?excellent?
and ?awful?.
The extraction656of pivot features was made on the basis of theirfrequency in source and target corpora and theirmutual information with positive and negativesource labels.
The correlations between the pivotfeatures and all other features were modeled usinga supervised learning of linear pivot predictors topredict occurrences of each pivot in both domains.The proposed approach was tested on review datafrom 4 domains (books, DVDs, kitchen appliancesand electronics) and demonstrated a significantgain of accuracy for most domain pairs comparedto the baseline.
However, for a few domains theperformance degraded due to feature misalignment:the narrowness of the source domain and diversityof the target domain created false projections offeatures in the target domain.
The authors proposedto correct this misalignment with a small amount ofannotated in-domain data.Spectral Feature Alignment (SFA), introduced byPan et al2010), holds the same idea as SCL,i.e., an alignment of source and target featuresthrough their co-occurrences with general sentimentwords.
But instead of learning representations ofpivots in source and target domains the authorsused spectral clustering to align domain-specific anddomain-independent words into a set of feature-clusters.
The constructed clusters were then used forthe representation of all data examples and trainingthe sentiment classifier.
This new solution yields asignificant improvement on cross-domain accuracycompared with SCL for almost all domain pairs.The method suggested by Bollegala et al2011)also relies on word co-occurrences.
In particular,the authors presented a method for automaticconstruction of a sentiment-sensitive thesauruswhere each lexical element (either unigram orbigram) is connected to a list of related lexicalelements which most frequently appear in thecontext expressing the same sentiment.
Thisthesaurus is then used on the training step toexpand feature vectors with related elements toovercome the feature mismatch problem.
Themethod was tested on the same data set as SCLand SFA but unlike previous works the authorsused a combination of domains to create sentiment-sensitive thesauri and to train the cross-domainclassifier.
They compare the accuracy of theirapproach with an average accuracy over the resultswith the same target domain given by SCL andSFA, and concluded that their method surpasses allexisting approaches.
However, we think that sucha comparison is not optimal.
Indeed, using theapproach described in (Ponomareva and Thelwall,2012) we can choose the most appropriate datafor training our classifier rather than averaging theresults given by all data sets.
Therefore, instead ofaverage accuracies, the best accuracies with respectto the same target domain should be compared.
Thiscomparison leads to opposite conclusions, namelythat SCL and SFA significantly outperform thesentiment-sensitive thesaurus-based method.Unlike the approaches mentioned above,graph-based algorithms exploit relations betweendocuments for finding the correct document scores.We describe them in more details in the next section.3 Graph-based algorithmsIn this section we present and compare 2 graph-based algorithms which use similar graph structuresbut completely different methods to infer nodescores.
The RANK algorithm (Wu et al2009) is based on node ranking, while OPTIM(Goldberg and Zhu, 2006) determines solution ofgraph optimisation problem.
Initially OPTIM wasapplied for the rating-inference problem in a semi-supervised setting.
This study, for the first time,analyses its behaviour for cross-domain SC andcompares its performance with a similar approach.3.1 OPTIM algorithmThe OPTIM algorithm represents graph-basedlearning as described in (Zhu et al2003).
Let usintroduce the following notation:?
G = (V,E) is an undirected graph with 2nnodes V and weighted edges E.?
L stands for labeled data (source domain data)and U for unlabeled data (target domain data).?
xi is a graph node which refers to a document,f(xi) is a true label of a document which issupposed to be unknown even for annotateddocuments, allowing for noisy labels.
Eachxi ?
L is connected to yi which representsa given rating of a document.
The edgeweight between x ?
i and yi is a large number657Figure 1: Graph models for the OPTIM (A) and RANK (B) algorithmsM introducing the hard constraints betweenlabeled documents and their ratings.
Each xi ?U is connected to y?i that stands for predictedrating of a document.
The edge weight betweenxi and y?i is equal to 1.?
Each unlabeled document xi is connected to itsk nearest labeled documents kNNL(i) (sourcedomain neighbours).
The weight between xiand xj ?
kNNL(i) is measured by a givensimilarity w and denoted a ?
wij .?
Each unlabeled document xi is connected toits k?
nearest unlabeled documents k?NNU (i)(target domain neighbours).
The weightbetween xi and xj ?
k?NNU (i) is denoted byb ?
wij .Figure 1A illustrates the graph structuredescribed.
The algorithm is based on the assumptionthat the rating function f(x) is smooth with respectto the graph, so there are no harsh jumps ofsentiment between nearest neighbours.
To satisfythe smoothness condition sentiment variabilitybetween the closest nodes should be minimised.Another requirement is to minimise the differencebetween each initial node rating and its final value,although in the case of unlabeled nodes this isoptional.
Taking into consideration the conditionsmentioned the sentiment-inference problem can beformulated as an optimisation problem:L(f) =?i?LM(f(xi)?
yi)2 +?i?U(f(xi)?
y?i)2+?i?U?j?kNNL(i)awij(f(xi)?
f(xj))2+?i?U?j?k?NNU (i)bwij(f(xi)?
f(xj))2 ?
min (1)After the substitutions ?
= ak + bk?
and ?
= ba thefinal optimisation problem can be written as:L(f) =?i?LM(f(xi)?
yi)2 +?i?U[(f(xi)?
y?i)2+?k + ?k?(?j?kNNL(i)wij(f(xi)?
f(xj))2+?j?k?NNU (i)?wij(f(xi)?
f(xj))2)]?
min (2)where ?
defines the relative weight betweenlabeled and unlabeled neighbours, while ?
controlsthe weight of the graph-based solution with respectto the primarily obtained supervised sentimentscores.658The minimum-loss function which gives thesolution of the optimisation problem can be foundby setting the gradient to zero.
For more details onthe problem solution see (Goldberg and Zhu, 2006).3.2 RANK algorithmThe RANK algorithm has a similar graph structure(Figure 1B): nodes represent labeled and unlabeleddocuments and there is a parameter (in this case?)
that controls the relative importance of labeleddata over unlabeled data and is an analogueof ?
in OPTIM.
The weight of edges betweendifferent nodes is also measured by documentsimilarity.
However, there are no edges betweennodes and their initial sentiments because RANKis an iterative algorithm and each iteration givesnew scores to unlabeled nodes while labelednodes remain constant.
More precisely, on eachiteration sentiment scores of unlabeled documentsare updated on the basis of the weighted sum ofsentiment scores of the nearest labeled neighboursand the nearest unlabeled neighbours.
The processstops when convergence is achieved, i.e.
thedifference in sentiment scores is less than apredefined tolerance.Using the same notation as for OPTIM we canformulate the iterative procedure in the followingway:fk(xi) =?j?kNNL(i)?wijf(xj)+?j?k?NNU (i)(1?
?
)wijfk?1xj) (3)where fk(xi) is the node sentiment score on thek-th iteration.
Document scores are normalisedafter each iteration to ensure convergence (Wu etal., 2009).
It is worth noting that initially theauthors did not consider having a different numberof neighbours for the source and target domains.Analysing differences in the graph structures andassumptions of both models we can say that theyare almost identical.
Even the smoothness conditionholds for the RANK algorithm as the score of anode is an averaged sum of the neighbours.
Theonly principal difference concerns the requirementof closeness of initial and final sentiment scores forOPTIM.
This condition gives more control on thestability of the algorithm performance.4 Measure of document similarityA good measure of document similarity is a keyfactor for the successful performance of graph-basedalgorithms.
In this section we propose and evaluateseveral measures of document similarity based ondifferent vector representations and the cosine ofdocument vectors.Following (Goldberg and Zhu, 2006) and (Pangand Lee, 2005) we consider 2 types of documentrepresentations:- feature-based: this involves weighteddocument features.
The question here concerns thefeatures to be selected.
When machine learningis employed the answer is straightforward: themost discriminative features are the best ones forour task.
However, we assume that we do notknow anything about the domain when measuringsentiment similarity and, thus, we should establishthe appropriate set of features only relying on ourprior knowledge about sentiment words.
Accordingto previous studies, adjectives, verbs and adverbs aregood indicators of sentiment (Pang and Lee, 2008),therefore, we keep only unigrams and bigrams thatcontain these PoS.
We test two feature weights - tfidfand idf (Ftfidf and Fidf in Table1 respectively).
Theevident drawback of such a vector representationconcerns the discarding of nouns, which in manycases also bear sentiments.
To overcome this issuewe introduce a new measure that uses sentimentdictionaries to add nouns expressing sentiments(Fidf+SOCAL).- lexicon-based: uses sentiment dictionaries toassign scores to lexical elements of two types: wordsor sentences.
The dimension of the correspondingdocument vector representation conforms with thegranularity of the sentiment scale.
For example,in case of binary sentiment scales, a documentvector consists of two dimensions, where firstcomponent corresponds to the percentage of positivewords (sentences) and the second component -to the percentage of negative words (sentences).To assign sentiment scores to lexical elementswe exploit different sentiment resources, namely659domain Ftfidf Fidf Fidf+SOCAL W2 W10 S2BO 0.61 0.62 0.64 0.49 0.50 0.44DV 0.61 0.61 0.64 0.56 0.56 0.51EL 0.62 0.66 0.68 0.47 0.49 0.46KI 0.65 0.67 0.68 0.51 0.54 0.53Table 1: Correlation for various similarity measures with sentiment scores of documents across different domains.SentiWordNet (Esuli and Sebastiani, 2006), SO-CAL (Taboada et al2010) and SentiStrength(Thelwall et al2012).
The scores of sentencesare averaged by the number of their positive andnegative words.
Preliminary experiments show a bigadvantage of SO-CAL-dictionaries comparing withother resources.
SentiWordNet demonstrates quitean unsatisfactory performance, while SentiStrength,being very precise, has an insufficient scope and,therefore, finds no sentiment in a substantial numberof documents.The best document representation is selectedon the basis of its correlation with the sentimentscores of documents.
To compute correlationsfor feature-based measures, we take 1000 featureswith highest average tfidf weights.
Table 1 givesthe results of a comparison for two documentrepresentations and their different settings.
HereW2 and S2 stand for word-based and sentence-based representations of dimension 2 and W10 -for word-based representation of dimension 10.All use SO-CAL-dictionaries to assign scores towords or sentences.
Feature-based representationsdemonstrate significantly better correlations withdocument sentiments although for some domains,like DV, the lexical element-based representationproduces a similar result.
Integration of SO-CAL-dictionaries gives insignificant contribution into theoverall correlation, which maybe due to the limitednumber of features participated in the analysis.In our further experiments we use both Fidf andFidf+SOCAL document representations.5 Experimental resultsOur data comprises Amazon product reviews on 4topics: books (BO), electronics (EL), kitchen (KI)and DVDs (DV), initially collected and describedby Blitzer et al2007).
Reviews are rated usinga binary scale, 1-2 star reviews are considered asnegative and 4-5 star reviews as positive.
The datawithin each domain are balanced: they contain 1000positive and 1000 negative reviews.First, we compute a baseline for each domainpair by training a Support Vector Machines (SVMs)classifier using one domain as training data andanother as test data.
We choose SVMs as ourmain learning technique because they have provedto be the best supervised algorithm for SC (Pangand Lee, 2008).
In particular, we use the LIBSVMlibrary (Chang and Lin, 2011) and a linear kernelfunction to train the classifier.
For the featureset we experiment with different features andfeature weights and conclude that unigrams andbigrams weighted with binary values yield the bestperformance.Figure 2: Baseline accuracy for cross-domain SC.
(x-axis - source domains, y-axis - target domains).Figure 2 presents an isoline image of cross-domain accuracies for all domain pairs.1 Productson the x-axis represent source domains and products1We should point out that in the images the shading betweenpoints is not intended to suggest interpolation but is used tohighlight the overall pattern.
Of course the pattern depends on adomain order on the axes, therefore, similar domains are placedtogether to make the regions with high and low accuraciesevident.660on the y-axis represent target domains.
The isolinesimage of the baseline accuracy delivers a goodrepresentation of domain relations.
In particular, wecan observe two regions with the highest accuracy(EL-KI, KI-EL) and (BO-DV, DV-BO) and tworegions with a big performance drop (EL-BO, EL-DV, KI-BO, KI-DV) and (BO-EL, BO-KI, DV-EL, DV-KI).
As shown in our previous study(Ponomareva and Thelwall, 2012) the first tworegions conform with the most similar domain pairsBO, DV and EL, KI.OPTIM and RANK require the setting of severalparameters: (k, k?, ?, ?)
for OPTIM and (k, k?, ?
)for RANK.
As it is computationally expensive toiterate over all possible values of parameters we firstrun the algorithms on a small matrix of parametersand then apply the gradient descent method whichtakes the values with highest accuracy as its startingpoints.
We execute both algorithms with differentsimilarity measures, Fidf and Fidf+SOCAL.
InTable 2 OPTIM and RANK run with Fidf , whileOPTIM+SOCAL and RANK+SOCAL run withFidf+SOCAL.
We give the best accuracies achievedby these algorithms for each domain pair.
Unlikethe correlations, the accuracies increase significantlywith the integration of SO-CAL-dictionaries, theaverage improvement is about 3% for RANK and1.5% for OPTIM.
In general, RANK consistentlyoutperforms OPTIM for all domain pairs, OPTIMshows competitive performance only for the pairsof similar domains BO-DV, KI-EL and EL-KI.We should also point out that OPTIM is moretime-consuming as it requires expensive matrixoperations.
Due to these advantages of the RANKalgorithm, we mostly focus on its analysis in the restof the paper.It is interesting to examine the performance ofRANK on the basis of the 3D isolines image (Figure3B).
The isolines stretch from left to right indicatingthat accuracy is almost independent of the sourcedomain.
Such behaviour for RANK suggests apositive answer to our question stated in the title:even if domains are quite different, neighbours fromthe same domain will fix these discrepancies.
Thisproperty is definitely a big advantage of the RANKalgorithm in the context of the cross-domain task asit minimises the importance of the source domain.Obviously more experiments with different datamust be accomplished to prove this conclusion witha higher level of confidence.We also compare graph-based algorithms withother state-of-the-art approaches, such as SCL andSFA (Table 2, Figure 3).
The best results in Table 2are highlighted and if the difference is statisticallysignificant with ?
= 0.05 the correspondingaccuracy is underlined.
Note that we comparegraph-based approaches against the others but noteach other, therefore, if the result given by RANK isunderlined it means that it is statistically significantonly in comparison with SCL and SFA and not withOPTIM.
According to Table 2, RANK surpassesSCL for almost all domain pairs with an averagedifference equal to 2%.
Interestingly, withoutusing SO-CAL-dictionaries RANK loses to bothSCL and SFA for almost all domain pairs.
Theadvantage of RANK over SFA is disputable asthere is not much consistency about when onealgorithm outperforms another, except that SFA isbetter overall for close domains.
However Figure3 suggests an interesting finding: that for domainswith different complexities swapping source andtarget alchanges the method that produces thebest performance.
A comparison of RANK and SCLon the Chinese texts given by (Wu et al2009)shows the same phenomenon.
It seems that RANKworks better when the target domain is simpler,maybe because it can benefit more from in-domainneighbours of the less rich and ambiguous domain.In the future, we plan to increase the impact oflexically different but reliably labeled source databy implementing the SFA algorithm and measuringdocument similarity between feature clusters ratherthan separate features.6 Strategy for choosing optimalparametersThe results of the RANK and OPTIM algorithmspresented in the previous section represent thehighest accuracies obtained after running gradientdescent method.
Table 3 lists the best parametervalues of the RANK algorithm over several domainpairs.
Our attempt to establish some universal valuesvalid for all domain pairs was not successful as thechoice of the parameters depends upon the domainproperties.
Of course, in real life situations we do661source-target baseline OPTIM RANK OPTIM+ RANK+ SCL SFASOCAL SOCALBO-EL 70.0 74.0 77.2 74.4 79.8 77.5 72.5BO-DV 76.5 78.6 77.4 79.9 79.8 75.8 81.4BO-KI 69.5 74.6 78.6 77.3 82.8 78.9 78.8DV-BO 74.4 78.8 78.9 80.5 82.1 79.7 77.5DV-EL 67.2 73.6 78.8 74.4 80.9 74.1 76.7DV-KI 70.2 75.6 80.4 77.3 83.2 81.4 80.8EL-BO 65.5 67.8 69.9 69.5 73.6 75.4 75.7EL-DV 71.3 74.2 72.6 75.6 77.0 76.2 77.2EL-KI 81.6 83.6 83.2 85.7 85.3 85.9 86.8KI-BO 64.7 68.4 70.9 69.7 74.8 68.6 74.8KI-DV 70.1 72.3 72.4 73.4 78.4 76.9 77.0KI-EL 79.7 82.6 81.9 83.7 83.7 86.8 85.1average 71.7 75.3 76.9 76.8 80.1 78.1 78.7Table 2: Comparison of different cross-domain algorithmsFigure 3: Accuracy obtained with different cross-domain algorithms over various domains: A) OPTIM, B) RANK,C) SCL, D) SFA.
(x-axis - source domains, y-axis - target domains).662parameter BO-EL BO-DV BO-KI EL-BO EL-DV EL-KI?
0.34 0.78 0.30 0.50 0.55 0.9k 50 100 25 75 50 200k?
220 50 40 100 150 10Table 3: Best number of labeled and unlabeled neighbours for the RANK algorithm over various domain pairssource- similarity complexity ?target varianceBO-EL 1.23 -1.93 0.34BO-DV 1.75 0.06 0.76BO-KI 1.17 -1.26 0.48DV-BO 1.75 -0.06 0.75DV-EL 1.22 -1.99 0.52DV-KI 1.18 -1.32 0.44EL-BO 1.23 1.93 0.62EL-DV 1.22 1.99 0.68EL-KI 1.87 0.67 0.75KI-BO 1.17 1.26 0.64KI-DV 1.18 1.32 0.54KI-EL 1.87 -0.67 0.76Table 4: Similarity, complexity variance and ?
averagedover the best results (confidence level of 95%) of theRANK algorithm.
The values are given on variousdomain pairsnot have a knowledge of the parameter values whichproduce the best performance and, therefore, itwould be useful to elaborate a strategy for choosingthe optimal values with respect to a correspondingdomain pair.
In our previous work (Ponomarevaand Thelwall, 2012) we introduced two domaincharacteristics: domain similarity and domaincomplexity variance and proved their impact intothe cross-domain accuracy loss.
Domain similarityand complexity are independent properties of adomain pair as the former measures similarity ofdata distributions for frequent words, while the lattercompares the tails of distributions.
In Ponomarevaand Thelwall (2012), we tested various metricsto estimate these domain characteristics.
As aresult, inversed ?2 was proved to be the bestmeasure of domain similarity as it gave the highestcorrelation with the cross-domain accuracy drop.The percentage of rare words (words that occurless than 3 times) was found to be the closestapproximation to domain complexity as it showedthe highest correlation with the in-domain accuracydrop.It is naturally to assume that if domain similarityand complexity are responsible for the cross-domainaccuracy loss, they might influence on the parametervalues of domain adaptation algorithms.
This isproved to be true for the ?
parameter, whose valuesaveraged over the top results of the RANK algorithmare listed in Table 4.
We use the confidenceinterval of 95% to select the top values of ?.Table 4 shows that ?
is the lowest for dissimilardomains with a simpler target (negative values ofdomain complexity variance), which means that theRANK algorithm benefits the most from unlabeledbut simpler data.
?
grows to values close to 0.6for dissimilar domains with more complex target(positive values of domain complexity variance),which shows that the impact of simpler source data,though different from target, increases.
Finally ?reaches its maximum for similar domains with thesame level of complexity.
Unfortunately, due tocomparable amount of data for each domain, nocases of similar domains with different complexityare observed.
We plan to study these particular casesin the future.High dependency of ?
on both domaincharacteristics is proved numerically.
Thecorrelation between ?
and domain similarity andcomplexity reaches 0.91, and decreases drasticallywhen one of these characteristics is ignored.Concerning the optimal number of labeled andunlabeled neighbours, no regularity is evident(Table 3).
In our opinion, that is an effectof choosing the neighbours on the basis of thequantitative threshold.
Nevertheless, differentdomains have distinct pairwise document similaritydistributions.
Figure 4 demonstrates similaritydistributions for BO, EL and DV inside and acrossdomains.
Therefore, taking into account onlythe quantitative threshold we ignore discrepancies663Figure 4: Pairwise document similarity distributions inside domains (A) and across domains (B)in graph connectivities inside and across domainsand may bring ?bad?
neighbours to participate indecision-making.
In our further research we planto explore the idea of a qualitative threshold, whichchooses neighbours according to their similarity anduses the same similarity levels for in-domain andcross-domain graphs.7 Conclusions and future workThis paper has studied the performance of twograph-based algorithms, OPTIM and RANK whenapplied to cross-domain sentiment classification.Comparison on their performance on the same datahas revealed that, in spite of the similar graphstructures, RANK consistently produces betterresults than OPTIM.
We also have compared thegraph-based algorithms with other cross-domainmethods, including SCL and SFA, and concludedthat RANK considerably outperforms SCL andobtains better results than SFA for half of thecases.
Given that we consider only the bestaccuracies obtained with RANK, such comparisonis not completely fair but it shows the potential ofthe RANK algorithm once the strategy for choosingits optimal parameters is established.
In this paper,we also discuss some ideas about how to inferoptimal parameter values for the algorithms on thebasis of domain characteristics.
In particular, thestrong correlation for ?
with domain similarity andcomplexity has been observed.
Unfortunately weare not able to find any regularity in the numberof source and target domain neighbours, which wethink is the result of the qualitative approach toselecting the closest neighbours.As a result of this research we have identifiedthe following future directions.
First, we planto improve the RANK performance by choosingthe number of neighbours on the basis of thedocument similarity threshold which we set equalfor both in-domain and cross-domain neighbours.We expect that this modification will diminish thenumber of ?bad?
neighbours and allow us to reveal adependency of similarity threshold on some domainproperties.
Another research direction will focus onthe integration of SFA into the similarity measureto overcome the problem of lexical discrepancy inthe source and target domains.
Finally, as all ourconclusions have been drawn on a data set of 12domain pairs, we plan to increase a number ofdomains to verify our findings on larger data sets.AcknowledgmentsThis work was supported by a European Uniongrant by the 7th Framework Programme, Theme 3:Science of complex systems for socially intelligentICT.
It is part of the CyberEmotions project(contract 231323).ReferencesAnthony Aue and Michael Gamon.
2005.
Customizingsentiment classifiers to new domains: A case study.
InProceedings of Recent Advances in Natural LanguageProcessing (RANLP ?05).John Blitzer, Ryan McDonald, and FernandoPereira.
2006.
Domain adaptation with structural664correspondence learning.
In Proceedings of the2006 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP ?06), pages 120?128.John Blitzer, Mark Dredze, and Fernando Pereira.
2007.Biographies, bollywood, boom-boxes and blenders:Domain adaptation for sentiment classification.
InProceedings of the 45th Annual Meeting of theAssociation of Computational Linguistics (ACL ?07),pages 440?447.Danushka Bollegala, David Weir, and John Carroll.2011.
Using multiple sources to construct a sentimentsensitive thesaurus for cross-domain sentimentclassification.
In Proceedings of the 49th AnnualMeeting of the Association for ComputationalLinguistics (ACL ?11), pages 132?141.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIBSVM:A library for support vector machines.
ACMTransactions on Intelligent Systems and Technology,2:27:1?27:27.
Software available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.Andrea Esuli and Fabrizio Sebastiani.
2006.Sentiwordnet: A publicly available lexical resource foropinion mining.
In Proceedings of the 5th Conferenceon Language Resources and Evaluation (LREC 06),pages 417?422.Andrew B. Goldberg and Xiaojin Zhu.
2006.
Seeingstars when there aren?t many stars: graph-basedsemi-supervised learning for sentiment categorization.In Proceedings of the First Workshop on GraphBased Methods for Natural Language Processing(TextGraphs ?06), pages 45?52.Shoushan Li and Chengqing Zong.
2008.
Multi-domainsentiment classification.
In Proceedings of ACL-08:HLT, Short Papers, pages 257?260.Sinno Jialin Pan, Xiaochuan Niz, Jian-Tao Sunz, QiangYangy, and Zheng Chen.
2010.
Cross-domainsentiment classification via spectral feature alignment.In Proceedings of International World Wide WebConference (WWW ?10).Bo Pang and Lillian Lee.
2005.
Seeing stars: exploitingclass relationships for sentiment categorization withrespect to rating scales.
In Proceedings of the 43rdAnnual Meeting on Association for ComputationalLinguistics (ACL ?05), pages 115?124.Bo Pang and Lillian Lee.
2008.
Opinion miningand sentiment analysis.
Foundations and Trends inInformation Retrieval, 2(1-2):1?135.Natalia Ponomareva and Mike Thelwall.
2012.Bibliographies or blenders: Which resource is best forcross-domain sentiment analysis?
In Proceedings ofthe 13th Conference on Intelligent Text Processing andComputational Linguistics (CICLing ?12).M.
Taboada, J. Brooke, M. Tofiloski, K. Voll, andM.
Stede.
2010.
Lexicon-based methods forsentiment analysis.
Computational Linguistics,37(2):267?307.M.
Thelwall, K. Buckley, and G. Paltoglou.
2012.Sentiment strength detection for the social web.Journal of the American Society for InformationScience and Technology, 63(1):163?173.Qiong Wu, Songbo Tan, and Xueqi Cheng.
2009.
Graphranking for sentiment transfer.
In Proceedings of theACL-IJCNLP 2009 Conference Short Papers, pages317?320.Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty.2003.
Semi-supervised learning using gaussian fieldsand harmonic functions.
In Proceedings of the20th International Conference on Machine Learning(ICML ?03), pages 912?919.665
