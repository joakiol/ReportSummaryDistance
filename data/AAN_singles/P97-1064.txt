A Structured Language ModelCipr ian  Che lbaThe  Johns  Hopk ins  Univers i tyCLSP ,  Bar ton  Hal l  3203400 N. Char les  Street ,  Ba l t imore ,  MD-21218che lba@j  hu.
eduAbst ractThe paper presents a language model thatdevelops yntactic structure and uses it toextract meaningful information from theword history, thus enabling the use oflong distance dependencies.
The model as-signs probability to every joint sequenceof words-binary-parse-structure with head-word annotation.
The model, its proba-bilistic parametrization, and a set of ex-periments meant to evaluate its predictivepower are presented.the  dog  I heard  yesterday  barkedFigure 1: Partial parse' ?
"~.
(  ~ I h_{-=*l  ) ~_{-I \[ h_O~ w_l ... w..p ........ w q.. .w~r w_l r+l l  ...w_k w_lk+l} ..... w_n </s>Figure 2: A word-parse k-prefix1 In t roduct ionThe main goal of the proposed project is to developa language model(LM) that uses syntactic structure.The principles that guided this propo?al were:?
the model will develop syntactic knowledge as abuilt-in feature; it will assign a probability to everyjoint sequence of words-binary-parse-structure;?
the model should operate in a left-to-right man-ner so that it would be possible to decode word lat-tices provided by an automatic speech recognizer.The model consists of two modules: a next wordpredictor which makes use of syntactic structure asdeveloped by a parser.
The operations of these twomodules are intertwined.2 The  Bas ic  Idea  and  Termino logyConsider predicting the word barked in the sen-tence:the dog I heard yesterday barked again.A 3-gram approach would predict barked from(heard, yesterday)  whereas it is clear that thepredictor should use the word dog which is out-side the reach of even 4-grams.
Our assumptionis that what enables us to make a good predic-tion of barked is the syntactic structure in thepast.
The correct partial parse of the word his-tory when predicting barked is shown in Figure 1.The word dog is called the headword of the con-stituent ( the (dog ( .
.
. )
)) and dog is an exposedheadword when predicting barked - -  topmost head-word in the largest constituent that contains it.
Thesyntactic structure in the past filters out irrelevantwords and points to the important ones, thus en-abling the use of long distance information whenpredicting the next word.
Our model will assign aprobability P(W, T) to every sentence W with ev-ery possible binary branching parse T and everypossible headword annotation for every constituentof T. Let W be a sentence of length I words towhich we have prepended <s> and appended </s>so that wo =<s> and wl+l =</s>.
Let Wk be theword k-prefix w0.. .
wk of the sentence and WkT~the word-parse k-prefix.
To stress this point, aword-parse k-prefix contains only those binary treeswhose span is completely included in the word k-prefix, excluding wo =<s>.
Single words can be re-garded as root-only trees.
Figure 2 shows a word-parse k-prefix; h_0 .. h_{-m} are the exposed head-words.
A complete parse - -  Figure 3 - -  is any bi-nary parse of the wl .. .
wi </s> sequence with therestriction that </s> is the only allowed headword.498~D<s> w_l  .
.
.
.
.
.
w_l  </s>Figure 3: Complete parseNote that (wl .
.
.w i )  needn't be a constituent, butfor the parses where it is, there is no restriction onwhich of its words is the headword.The model will operate by means of two modules:?
PREDICTOR predicts the next word wk+l giventhe word-parse k-prefix and then passes control tothe PARSER;?
PARSER grows the already existing binarybranching structure by repeatedly generating thetransitions ad jo in - le f t  or ad jo in - r ight  until itpasses control to the PREDICTOR by taking a nu l ltransition.The operations performed by the PARSER en-sure that all possible binary branching parses withall possible headword assignments for the w~... wkword sequence can be generated.
They are illus-trated by Figures 4-6.
The following algorithm de-scribes how the model generates a word sequencewith a complete parse (see Figures 3-6 for notation):Transition t; // a PARSER transitiongenerate  <s> ;do{predict next_word; //PREDICTORdo{ //PARSERif(T_{-l} != <s> )if(h_0 == </s>) t = adjoin-right;else  t = {ad jo in -{ le f t , r ight} ,  null};else I; = null;}while(t != null)}while(!
(h_0 == </s> &E T_{-1} == <s>))t = adjoin-right; // adjoin <s>; DONEIt is easy to see that any given word sequence with apossible parse and headword annotation isgeneratedby a unique sequence of model actions.3 P robab i l i s t i c  Mode lThe probability P(W, T) can be broken into:1+1 p P(W,T)  = l-L=1\[ (wk/Wk-lTk-1)"~\]~21 P ( tk l wk, Wk- , Tk-1, t~ .
.
.
t~_l) \] where:?
Wk-lTk-1 is the word-parse (k - 1)-prefix?
wk is the word predicted by PP~EDICTOR?
Nk - 1 is the number of adjoin operations thePARSER executes before passing control to thePREDICTOR (the N~-th operation at position k isthe nu l l  transition); N~ is a function of Th_{-2  } h_{- I  } h_OFigure 4: Before an adjoin operationh.~( -z  ) - -  h_ ( -2 )  h .
_o .
h .
_ ( -  x )Figure 5: Result of adjoin-lefth '_{* t  ) .h_ (o2)  h*_O -- n_Oh_  .. .
.
.
.
.
.
.
.
.Figure 6: Result of adjoin-right?
t~ denotes the i-th PARSER operation carriedout at position k in the word string;t k E {adjoin-left,adjoin-right},i < Nk ,=null, i = NkOur model is based on two probabilities:P(wk/Wk-lTk-1) (1)P(t~/Wk, Wk-lTk-1, t~.. .
t~_l) (2)As can be seen (wk, Wk-lTk-1, tk k .
.
.
t i _ l )  is oneof the Nk word-parse k-prefixes of WkTk, i = 1, Nkat position k in the sentence.To ensure a proper probabilistic model we haveto make sure that (1) and (2) are well defined con-ditional probabilities and that the model halts withprobability one.
A few provisions need to be taken:?
P(nul l /WkTk) = 1, if T_{-1} == <s> ensuresthat <s> is adjoined in the last step of the parsingprocess;?
P(adjo in-r ight/WkTk) = 1, if h_0 == </s>ensures that the headword of a complete parse is<Is>;?
3~ > Os.t.
P(wk=</s>/Wk-lT~-l)  >_ e, VWk-lTk-1ensures that the model halts with probability one.3.1 The  f i rs t  mode lThe first term (1) can be reduced to an n-gram LM,P(w~/W~-lTk-1) = P(wk/W~- l .
.
.
Wk-n+l).A simple alternative to this degenerate approachwould be to build a model which predicts the nextword based on the preceding p-1 exposed headwordsand n-1 words in the history, thus making the fol-lowing equivalence classification:\[WkTk\] = {h_O .
.
h_{ -p+2}, iUk - l .
.Wk-n+ 1 }.499The approach is similar to the trigger LM(Lau93),the difference being that in the present work triggersare identified using the syntactic structure.3.2 The second modelModel (2) assigns probability to different binaryparses of the word k-prefix by chaining the ele-mentary operations described above.
The workingsof the PARSER are very similar to those of Spat-ter (Jelinek94).
It can be brought o the full powerof Spatter by changing the action of the adjoinoperation so that it takes into account he termi-nal/nonterminal l bels of the constituent proposedby adjoin and it also predicts the nonterminal la-bel of the newly created constituent; PREDICTORwill now predict he next word along with its POStag.
The best equivalence classification ofthe WkTkword-parse k-prefix is yet to be determined.
TheCollins parser (Collins96) shows that dependency-grammar-like bigram constraints may be the mostadequate, so the equivalence classification \[WkTk\]should contain at least (h_0, h_{-1}}.4 P re l iminary  Exper imentsAssuming that the correct partial parse is a func-tion of the word prefix, it makes ense to comparethe word level perplexity(PP) of a standard n-gramLM with that of the P(wk/Wk-ITk-1) model.
Wedeveloped and evaluated four LMs:?
2 bigram LMs P(wk/Wk-lTk-1) = P(Wk/Wk-1)referred to as W and w, respectively; wk-1 is the pre-vious (word, POStag) pair;?
2 P(wk/Wk-ITk--1) = P(wjho)  models, re-ferred to as H and h, respectively; h0 is the previousexposed (headword, POS/non-term tag) pair; theparses used in this model were those assigned man-ually in the Penn Treebank (Marcus95) after under-going headword percolation and binarization.All four LMs predict a word wk and they wereimplemented using the Maximum Entropy Model-ing Toolkit 1 (Ristad97).
The constraint templatesin the {W,H} models were:4 <= <*>_<*> <7>; P- <= <7>_<*> <7>;2 <= <?>_<7> <?>; 8 <= <*>_<?> <7>;and in the {w,h} models they were:4 <= <*>_<*> <7>; 2 <= <7>_<*> <7>;<.> denotes a don't care position, <7>_<7> a (word,tag) pair; for example, 4 <= <7>_<*> <7> will trig-ger on all ((word, any tag), predicted-word) pairsthat occur more than 3 times in the training data.The sentence boundary isnot included in the PP cal-culation.
Table 1 shows the PP results along withI ftp://ftp.cs.princeton.edu/pub/packages/memtthe number of parameters for each of the 4 modelsdescribed.H LM PP \[ parara H LM PP param IH 312 206540 h 410 102437Table 1: Perplexity results5 AcknowledgementsThe author thanks to Frederick Jelinek, SanjeevKhudanpur, Eric Ristad and all the other membersof the Dependency Modeling Group (Stolcke97),WS96 DoD Workshop at the Johns Hopkins Uni-versity.Re ferencesMichael John Collins.
1996.
A new statistical parserbased on bigram lexical dependencies.
In Pro-ceedings of the 3~th Annual Meeting of the As-sociation for Computational Linguistics, 184-191,Santa Cruz, CA.Frederick Jelinek.
1997.
Information extraction fromspeech and text - -  course notes.
The Johns Hop-kins University, Baltimore, MD.Frederick Jelinek, John Lafferty, David M. Mager-man, Robert Mercer, Adwait Ratnaparkhi, SalimRoukos.
1994.
Decision Tree Parsing using a Hid-den Derivational Model.
In Proceedings of theHuman Language Technology Workshop, 272-277.ARPA.Raymond Lau, Ronald Rosenfeld, and SalimRoukos.
1993.
Trigger-based language models: amaximum entropy approach.
In Proceedings of theIEEE Conference on Acoustics, Speech, and Sig-nal Processing, volume 2, 45-48, Minneapolis.Mitchell P. Marcus, Beatrice Santorini, Mary AnnMarcinkiewicz.
1995.
Building a large annotatedcorpus of English: the Penn Treebank.
Computa-tional Linguistics, 19(2):313-330.Eric Sven Ristad.
1997.
Maximum entropy model-ing toolkit.
Technical report, Department ofCom-puter Science, Princeton University, Princeton,N J, January 1997, v. 1.4 Beta.Andreas Stolcke, Ciprian Chelba, David Engle,Frederick Jelinek, Victor Jimenez, Sanjeev Khu-danpur, Lidia Mangu, Harry Printz, Eric SvenRistad, Roni Rosenfeld, Dekai Wu.
1997.
Struc-ture and Performance ofa Dependency LanguageModel.
In Proceedings of Eurospeech'97, PJaodes,Greece.
To appear.500
