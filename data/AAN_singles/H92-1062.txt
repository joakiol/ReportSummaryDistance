SYNTACTIC/SEMANTIC COUPLING IN THE BBN DELPHISYSTEMRobert Bobrow, Robert Ingria, David StallardBBN Systems and Technologies10 Moulton StreetCambridge, MA 02138ABSTRACTWe have recently made significant changes to the BBN DELPHIsyntactic and semantic analysis component.
These goal of thesechanges was to maintain the fight coupling between syntax and se-mantics characteristic ofearlier versions of DELPHI, while makingit possible for the system to provide useful semantic interpreta-tions of input for which complete syntactic analysis is impossi-ble.
Semantic interpretation is viewed as a process operating ona sequence of messages characterizing local grammatical relationsamong phrases, rather than as a recursive tree walk over a globaUycomplete and coherent parse tree.
The combination of incrementalsemantic interpretation and statistical control of the parsing pro-cess makes it feasible to reconstruct local grammatical relationswith substantial ccuracy, even when a global parse cannot be ob-tained.
Grammatical relations provide the interface between syn-tactic processing and semantic interpretation, and standard globalparsing is viewed as merely one way to obtain evidence for the ex-istence of such grammatical relations in an input string.
This focuson grammatical relations leads to substantial simplification of bothgrammar and semantic rules, and will facilitate our ultimate aimof acquiring syntactic, semantic and lexical knowledge by largelyautomatic means.1.
THE PROBLEMThere are two long standing problems of computational lin-guistics for systems which do syntactic processing beforesemantic processing.
First, they are limited by the cover-age of their grammar.
Second, "syntactically ill-formed"word sequences, which represent a noticeable fraction ofthe utterances in any natural setting (e.g.
spontaneouslyspoken input or unedited text) cause many failures.
Otherarchitectures have their own problems.
Systems that dependprimarily on semantic processing tend to be uncomfortablydomain-dependent, working best in highly constrained prob-lem domains.
"Semantic grammar" systems often do notcapture a wide range of syntactic variations with the samemeaning, while "frame based" systems typically allow forill-formedness and syntactic variation by forcing all inputinto the procrustean bed of a highly limited task model.3112.
DELPHI'S APROACHOur goal in the DELPHI system has to develop techniquesthat allow general, task independent, syntactic knowledge tobe used to the fullest extent possible, making it feasible toencode semantic knowledge in the simplest (and thus mostlearnable) form, without sacrificing enerality.
Classical ap-proaches to this ideal fail completely when presented with"syntactically ill-formed" input, whether that ill-formednessis due to the system's incomplete representation f syntac-tic regularities, or to genuine disfluencies on the part of thespeaker/writer.
We have been continually making progresstoward a balanced approach that allows us to take advan-tage of syntactic onstraints wherever possible, while allow-ing the system to interpret inputs for which no grammaticalparse can be found.The differences between our approach and more standardsyntactically oriented approaches are subtle.
At first glance,our grammar and our parser do not look radically differentthan thosed used in other syntactic analysis approaches (withthe exception of the scheduling algorithm mentioned below).We started with a relatively standard context-free parsingalgorithm, applied to what is for the most part a straightfor-ward unification-based grammar.
The largest modificationto the parser was its conversion to an agenda-based chart-pax.
er, with scheduling depending on measured statisticallikelihood of grammatical rules \[1\].
This enhanced effi-ciency significantly, by allowing us to generate parses in a"best first" order, but did not change the syntactic overageof our system.All versions of DELPHI for the last several years have inte-grated semantic processing with parsing.
This ensures thatall syntactic structures placed in the chart are semanticallycoherent, further educing the search space for the best parse.In the early versions of DELPHI, each syntactic rule had anassociated semantic rule which had to be sucessfuUy appliedbefore the syntactically hypothesized constituent would beaccepted in the chart.
Because of the large number of syn-tactic rules needed to have a broad coverage grammar, thenumber of semantic rules was quite large, and the represen-tation for lexical semantics was quite complex.2.1 Parsing as TransductionThe biggest change in DELPHI came as we started to look atthe parser, not as a device for constructing syntactic trees,but as an information txansducer that makes it possible tosimplit 3, and generalize the roles for semantic interpretation.The purpose of syntactic analysis in this view is to makeinformation encoded in ordering and constituency as readilyavailable as the information encoded in the lexicai items,and to map syntactic paraphrases to informarionally equiv-alent structures.
The actual interface between parsing andsemantics i  a dynamic process tructured as a cascade (asin Woods notion of cascaded ATNs \[4\]), with parsing andsemantic interpretation acting as coroutines.
The input tothe semantic interpreter is a sequence of messages, each re-questing the "binding" of some constituent to a head.
Thesemantic interpreter does not perform any sort of recursivetree-walk over the syntactic structure produced by the parser,and is in fact immune to many details of the tree structure.This view of a grammar as a transducer between input stringsand semantic representation made it possible for us to sub-stanriaily restructure the grammar in such a way as to bothdecrease the number of rules and increase its coverage.
Theoriginal DELPHI grammar contained 1143 rules.
The re-structured grammar has only 453 rules.
This overall numberperhaps underestimates the impact of the change in point ofview, because it includes rules for various specialized sub-grammars uch as numbers, latitudes and longitudes andclock times, which were not revised.
The number of VPrules (excluding conjunction and modal rules) dropped from83 to 15, while the coverage of VP phenomena increased.2.2 The "Piece Parts" MetaphorIn general, while certain orderings of modifiers seemstrongly constrained by grammar (determiner and relativeclause for NPs, subject and object for clauses, indirect ob-ject and object for VPs), other orderings eem to be moreweakly determined (the "arguments" of a verb, such as theorigin and destination phrases of a verb of motion, usu-ally occur before more general verbal adjuncts like rime-modifiers), and can be over-ridden by factors such as suchas "heaviness".
Thus, most attachments can be modelled bysimple binary adjunction.
Since the exact topology of theparse tree could be modified without materially affecting thetransduction operation, we opted to generate complex recur-sive structures primarily by left and fight adjunction, usingrules of the general form(X .
.
.)
=> (X -LEFT-MOD .
.
.)
(X .
..)and(X ...) => (X ...) (X -R IGHT-MOD ...)312When the structures produced by such rules are written in abracketed notation the resulting items look like notMng somuch as onions!
( ( ( ( (x .
.
.
)???
)(X -R IGHT-MOD .
.
. )
)  .
.
. )
.
.
.
)The critical issue for a transducer, however, is the infor-marion flow from syntax to semantics, and at this level thelayers of the onion disappear, with each adjunct being "log-ically attached" to the "head" of the constituent.In effect, we have factored a number of constructions thatwere previously treated as units into "piece parts" that canbe combined together in various ways, subject o semanticwell-formedness.
Verb subcategorization s aprime exampleof one such area \[2\].
Rather than using subcategorizationfeatures to name sets of categories that appear together ascomplements, we have defined approximately 15 verb phraserules that list the possible constituents that may appear ascomplements to a verb.
These may embed witMn each otherfreely, so long as the results are semantically interpretableby the head.We have adopted this approach throughout he grammar.For example, the complements and adjuncts that may ap-pear within a noun phrase are introduced by recursive NPrules, similar to the VP rules we have discussed here.
Thisrecursive scheme allows the piece part rules of the grammarto be combined together in novel ways, governed by thelexical semantics of individual words.
The grammar writerdoes not need to foresee all possible combinations, as before.2.3 ExamplesFor example, in an earlier version of the grammar, in orderfor a verb, like "fly", to take two prepositional phrase com-plements, the following rule was required (for the purposesof exposition we suppress complex unification structures) :(VP ... :SUBJ  :WFF) =>(V :WORD ...(D ITRANSPREP :PREP  :PREP1:PP2) ...)(PP :PREP  ... :PPi)(PP :PREP1 ...
:PP2)?..
:PP iThe word "fly" contained the feature (DITRANSPREP(FROMPREP) (TOPREP) ...) in its lexical entry to constrainthe prepositions to be "from" and "to".
In order for "fly"to take the prepositions in the opposite order, as well, eitherthe lexical entry for "fly" would have contained the addi-tional subcategorization e try (DITRANSPREP (TOPREP)(FROMPREP) ...) with the values of the two prepositionarguments reversed, or we would have needed to add thefollowing "lexical redundancy rule" to the grammar:(VP ... :SUBJ  :WFF) =>(V :WORD ...(D ITRANSPREP :PREP :PREP1 ...:PP2) ...)(PP :PREP1 ... :PP2)(PP :PREP ... :PPi):PPiwhich automatically inverts the order of :PREP and :PREP1for any verb taking two prepositional phrases.
To allow"fly" to take a "to" phrase without a "from" phrase, as oftenoccurs, we would need both another subcategorization spec-ification in the lexical entry for "fly", and another rule in thegrammar, allowing a verb to take a single PP complement.In the current grammar, all PP complements are handled byone single rule:(VP :AGR ...) =>:HEAD (VP :AGR ...):PP -COMP (PP :PREP ...)This rule allows a verb to take a single prepositional phrasecomplement, or, by recursion, an indefinite number of oth-ers, consistent with its semantics.
The lexical informationparticular to individual verbs governs the number of prepo-sitions that the verb takes, their optionality or obligatoriness,and their semantic interpretation.
Special purpose rules fordifferent numbers and orders of PPs are not required.3.
GRAMMATICAL  RELAT IONSThe chief effort over the last year has been to codify thisnotion of logical attachment, simplifying the set of suchattachments o highlight the common underlying substruc-ture of grammatical paraphrases.
To this end we re-orientedour grammar around the notion of "grammatical relation".Grammatical relations include the familar ones of deep-structure subject and object, as well as other elations.
Theserelations may be seen as the end result of making the in-formation encoded in ordering and constituency explicitlyavailable.
(In languages with freer word order this informa-tion is often encoded in morphological ffixes or pre and postpositions.)
From the point of view of a syntactic-semantictransducer, the key point of any grammatical relation is thatit licenses (one of) a small number of semantic relations be-tween the ("meanings" of) the related constituents.
Some-times the grammatical relation constrains the semantic rela-tion in ways that cannot be predicted from the semantics ofthe constituents alone (given "John", "Mary" and "kissed",only the grammatical relations or prior world knowledge de-termine who gave and who received).
Other times the gram-matical relation simply licenses the one plausible semanticrelation (given "John", "ate" and "hamburger", if there isa relation, it is the hamburger that is most likely to havebeen consumed--but in the sentence "John ate the fries butrejected the hamburger" our knowledge of the destiny of thehamburger is mediated by its lack of a grammatical relationto "ate").Grammatical relations are incorporated into the grammar bygiving each element of the right hand side of a grammar rulea grammatical relation as a label.
Some typical rules are, inschematic form:(NP ... ) =>: t read (NP .
.
.
):PP -COMP (PP :PREP .
.
.
)(N -BAR ...) =>:PRE-NOM (N ...): HZAD (N-BAR .
.
.
)The first indicates that an NP may have an NP as a headand a PP as an adjunct, with the grammatical relation :pp-comp holding between them (the actual operation of bindinga :pp-comp splits it into a number of sub-relations based onthe preposition, but that can be safely ignored here).
Thesecond indicates that the head need not occur as the first con-stituent on the right side.
All that is required is that one ofthe right-hand elements is labeled as the "head" of the rule,and it is the source of information about the initial semanticand syntactic "binding state".
This binding state controlswhether or not the other elements of the right-hand side can"bind" to the head via the relation that labels them.
Seman-tics is associated with grammatical relations, not with par-ticular grammar rules (as are Montague-grammar and mostunification-based semantics systems).3.1"Binding rules" - the Semantics of Gram-matical RelationsThe implementation of the use of such binding states inthe transduction of grammatical relations to semantic struc-ture is facilitated by the procedural elements we have in-troduced into our unification grammar formalism.
In earlyversions of DELPHI, grammar ules contained only unifi-cation expressions for grammatical constituents.
Later ver-sions added "logical nodes"---expressions which looked likeconstituents, but which were satisfied by deductions in aunification-based axiom set.
These logical nodes were usedto add various constraints to the grammar, much as in adefinite clause grammar.
An analysis of the time spent in313parsing showed that substantial time was spent in such log-ical computations, and it became clear that more efficientdata structures and procedural techniques could be used toimplement many such computations \[3\].
The current versionof the system uses these embedded procedural mechanismsto manipulate specialized ata structures that efficently rep-resent he binding state of a constituent, o determine ff aproposed grammatical relation leads to a consistent bind-ing state, and if so what the semantic implications of thatbinding are.A separate system of "binding rules" for each grammaticalrelation licenses the binding of a constituent to a head viathat relation by specifying the semantic implications of bind-ing.
These rules generally specify aspects that must be trueof the semantic structure of the head and bound constituentin order for the binding to take place, and may also specifycertain syntactic requirements.
They may take into accountthe existence of previous bindings to the head, allowing cer-tain semantic roles (such as time specification) to be frilledmultiply, while other semantic roles may be restricted tohaving just one ffiller.As adjuncts are added to a structure the binding list is ex-tended.
As layers are added to the onion, a simple linearlist of bindings is maintained representing the head and itsgrammatical relation to each of the constituents added witheach layer of the onion.
Semantic binding rules are used toverify the local semantic plausibility of a structure, i.e.
thesemantic plausibility of each proposed grammatical relation.The next phase of semantic interpretation takes place whenthe onion is complete, i.e.
when a constituent X is insertedas other than the head of a larger constituent.
This situationprovides evidence that the outermost layer of the onion hasbeen reached, and that no more adjuncts are to be added.At this time it is possible to evaluate semantic rules thatcheck for completeness and produce an "interpretation" ofthe constituent.
These completion rules operate directly onthe binding list, not on the recursive left or right branchingtree structure produced by direct application of the grammar.The actual tree structure is at this level immaterial, havingbeen replaced by the flattened binding list representation frelational structure.4.
ROBUSTNESS BASED ON STAT IST ICSAND SEMANTICSSimply having a transduction system with semantics basedon grammatical relations does not deal with the issue of ro-bustness - the ability to make sense of an input even if it can-not be assigned a well-formed syntactic tree.
The difficultywith standard syntactic techniques i that local syntactic ev-idence is not enough to accurately determine grammaticalrelations.
A NP (e.g.
"John") followed by a verb (e.g.
"flew") may be the subject of that verb (e.g.
"John flew to314Boston") or may be unrelated (e.g.
"The man I introduced toJohn flew to Boston").
The standard way of getting aroundthis is to attempt to find a globally consistent set of gram-matical relation labels (i.e.
a global parsed and make useof the fact that the existence of a global parse containing agiven relation is stronger evidence for that relation than lo-cal structure (although syntactic ambiguity makes even suchglobal structures suspect).
This is indeed the best approachif all you have available is a syntactic grammar.The strategy we use in DELPHI is based on the existence oftwo other sources of information.
In the first place we havesemantic onstraints that can be applied incrementally, sothat we can check each proposed grammatical relation for se-mantic coherence in the context of other assumed grammat-ical structures.
Additionally, we have statistical informationon the likelihood of various word senses, grammatical rules,and grammatical-semantic transductiuns.
Thus we can notonly rule out many locally possible grammatical relations onthe basis of semantic incoherence, we can rank alternativelocal structures on the basis of empirically measured statis-tics.
The net result is that even in the absence of a singleglobal parse, we can be reasonably sure of the local gram-matical relations and semantic ontent of various fragments(we can even give numerical estimates of the likelihood ofeach such structure).4.1 Control structureThe DELPHI system attempts to obtain a complete parse ofits input, using its agenda-based best-first parsing algorithm.If  it is unable to do this it uses the parser in a fragment-production mode, producing the most probable structure foran initial segment of the input, then restarting the parser ina top down mode on the first element of the unparsed stringwhose lexical category provides a reasonable anchor for top-down prediction.
This process is repeated until the entireinput is spanned with fragments.
Experiments have shownthat the combination of statistical evaluation and semanticconstraints lets this procedure produce ahighly useful chunk-ing of the input for interpretation by other non-syntacticallydriven strategies.
Further details are given in the accompa-nying paper on the DELPHI fall-back processing strategies.5.
ADVANTAGES OF  THIS  APPROACHThe separation of syntactic grammar ules from semanticbinding and completion rules has important consequencesfor processing.
First, it enables the notion of grammaticalrelation to be separated from the notion of tree structure, andthus greatly facilitates fragment parsing.
Second, while itallows syntax and semantics to be strongly coupled in termsof processing (parsing and semantic interpretation) it allowsthem to be essentially decoupled in terms of notation.
Thismakes the grammar and the semantics considerably easierto modify and maintain.We believe, however, that in the long term the most im-portant advantage is that this view leads us to a new kindof language model, in which knowledge can be much moreeasily extracted through automatic training.
We view therole of the grammar as codifying the way that tree structureprovides evidence for grammatical relations.
Thus the rule(NP .
.
. )
=>:HEAD (~ .
.
.
):PP-COMP (PP :PREP .
.
.
)says that a noun phrase followed by a prepositional phraseprovides evidence for the relation PP-COMP between thePP and NP head.The separation between rules types will allow us for thefirst time to consider the effect of grammatical relations onmeaning, independently of the way that evidence for theserelations is produced by the parser.
One effect of this is tomake it possible to use a hypothesized semantic interpreta-tion of a set of tree fragments to generate a new syntacticrule.Thus, in normal operation, the primary evidence for a gram-matical relation is the result of actually parsing part of aninput.
However, since grammatical relations between con-stituents entail semantic relations, if we can make an esti-mate of the likelihood of certain semantic relations based ondomain knowledge, pragmatics, and task models, etc., it isin principle possible to use abductive reasoning to suggestlikely grammatical relations, and thereby propose new gram-mar mles.
In effect, grammatical relations form an abstractlevel of representation that greatly simplifies the interactionof syntactic and semantic processing.ACKNOWLEDGEMENTSThe work reported here was supported by the Advanced Re-search Projects Agency and was monitored by the Officeof Naval Research under Contract No.
N00014-89-C-0008.The views and conclusions contained in this document arethose of the authors and should not be interpreted as neces-sarily representing the official policies, either expressed orimplied, of the Defense Advanced Research Projects Agencyor the United States Government.REFERENCES1.
Bobrow, R. "Statistical Agenda Parsing", in Speech and Nat-ural Language: Proceedings of a Workshop Held at PacificGrove, California, February 19-22, 1991, Morgan KaufmannPublishers, Inc., San Mateo, California, pp.
222-224.2.
Bobrow, R., R. Ingria, and D. Stallard) (1991) "The MappingUnit Approach to Subcategorization", in Speech and Natural315Language: Proceedings of a Workshop Held at Pacific Grove,California, February 19-22, 1991, Morgan Katffrnann Pub-lishers, Inc., San Mateo, California, pp.
185--189.3.
Bobrow, R. and L. Ramshaw (1990) "On Deftly IntroducingProcedural Elements into Unification Parsing", in Speech andNatural Language: Proceedings of a Workshop Held at Hid-den Valley, Pennsylvania, June 24-27, 1990, Morgan Kauf-mann Publishers, Inc., San Mateo, California, pp.
237-240.4.
Woods, W. (1980) "Cascaded ATN Grammars", inAmericanJournal of Computational Linguistics, January-March 1980,vol 6, no.
1, Association for Computational Linguistics, p1-12.
