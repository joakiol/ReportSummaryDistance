An Analysis of the AskMSR Question-Answering SystemEric Brill, Susan Dumais and Michele BankoMicrosoft ResearchOne Microsoft WayRedmond, Wa.
98052{brill,sdumais,mbanko}@microsoft.comAbstractWe describe the architecture of theAskMSR question answering system andsystematically evaluate contributions ofdifferent system components to accuracy.The system differs from most questionanswering systems in its dependency ondata redundancy rather than sophisticatedlinguistic analyses of either questions orcandidate answers.
Because a wrong an-swer is often worse than no answer, wealso explore strategies for predictingwhen the question answering system islikely to give an incorrect answer.1 IntroductionQuestion answering has recently received attentionfrom the information retrieval, information extrac-tion, machine learning, and natural language proc-essing communities (AAAI, 2002; ACL-ECL,2002; Voorhees and Harman, 2000, 2001).
Thegoal of a question answering system is to retrieveanswers to questions rather than full documents orbest-matching passages, as most information re-trieval systems currently do.
The TREC QuestionAnswering Track, which has motivated much ofthe recent work in the field, focuses on fact-based,short-answer questions such as ?Who killed Abra-ham Lincoln??
or ?How tall is Mount Everest?
?In this paper we describe our approach to shortanswer tasks like these, although the techniques wepropose are more broadly applicable.Most question answering systems use a va-riety of linguistic resources to help in understand-ing the user?s query and matching sections indocuments.
The most common linguistic resourcesinclude: part-of-speech tagging, parsing, namedentity extraction, semantic relations, dictionaries,WordNet, etc.
(e.g., Abney et al, 2000; Chen et al2000; Harabagiu et al, 2000; Hovy et al, 2000;Pasca et al, 2001; Prager et al, 2000).
We choseinstead to focus on the Web as a gigantic data re-pository with tremendous redundancy that can beexploited for question answering.
We view ourapproach as complimentary to more linguistic ap-proaches, but have chosen to see how far we canget initially by focusing on data per se as a keyresource available to drive our system design.
Re-cently, other researchers have also looked to theweb as a resource for question answering (Buch-holtz, 2001; Clarke et al, 2001; Kwok et al,2001).
These systems typically perform complexparsing and entity extraction for both queries andbest matching Web pages, and maintain localcaches of pages or term weights.
Our approach isdistinguished from these in its simplicity and effi-ciency in the use of the Web as a large data re-source.Automatic QA from a single, small infor-mation source is extremely challenging, since thereis likely to be only one answer in the source to anyuser?s question.
Given a source, such as theTREC corpus, that contains only a relatively smallnumber of formulations of answers to a query, wemay be faced with the difficult task of mappingquestions to answers by way of uncovering com-plex lexical, syntactic, or semantic relationshipsbetween question string and answer string.
Theneed for anaphor resolution and synonymy, thepresence of alternate syntactic formulations andindirect answers all make answer finding a poten-tially challenging task.
However, the greater theAssociation for Computational Linguistics.Language Processing (EMNLP), Philadelphia, July 2002, pp.
257-264.Proceedings of the Conference on Empirical Methods in NaturalQuestion Rewrite Query <Search Engine>Collect Summaries,Mine N-gramsFilter N-GramsTile N-Grams N-Best AnswersWhere is the LouvreMuseum located?
?+the Louvre Museum +is located?
?+the Louvre Museum +is +in?
?+the Louvre Museum +is near?
?+the Louvre Museum +is?Louvre AND Museum AND nearin Paris France 59%museums          12%hostels              10%Figure 1.
System Architectureanswer redundancy in the source data collection,the more likely it is that we can find an answer thatoccurs in a simple relation to the question.
There-fore, the less likely it is that we will need to solvethe aforementioned difficulties facing natural lan-guage processing systems.In this paper, we describe the architecture ofthe AskMSR Question Answering System andevaluate contributions of different system compo-nents to accuracy.
Because a wrong answer isoften worse than no answer, we also explorestrategies for predicting when the question answer-ing system is likely to give an incorrect answer.2 System ArchitectureAs shown in Figure 1, the architecture of our sys-tem can be described by four main steps: query-reformulation, n-gram mining, filtering, and n-gram tiling.
In the remainder of this section, wewill briefly describe these components.
A moredetailed description can be found in [Brill et al,2001].2.1 Query ReformulationGiven a question, the system generates a numberof weighted rewrite strings which are likely sub-strings of declarative answers to the question.
Forexample, ?When was the paper clip invented??
isrewritten as ?The paper clip was invented?.
Wethen look through the collection of documents insearch of such patterns.
Since many of these stringrewrites will result in no matching documents, wealso produce less precise rewrites that have a muchgreater chance of finding matches.
For each query,we generate a rewrite which is a backoff to a sim-ple ANDing of all of the non-stop words in thequery.The rewrites generated by our system aresimple string-based manipulations.
We do not usea parser or part-of-speech tagger for query refor-mulation, but do use a lexicon for a small percent-age of rewrites, in order to determine the possibleparts-of-speech of a word as well as its morpho-logical variants.
Although we created the rewriterules and associated weights manually for the cur-rent system, it may be possible to learn query-to-answer reformulations and their weights (e.g.,Agichtein et al, 2001; Radev et al, 2001).2.2 N-Gram MiningOnce the set of query reformulations has been gen-erated, each rewrite is formulated as a search en-gine query and sent to a search engine from whichpage summaries are collected and analyzed.
Fromthe page summaries returned by the search engine,n-grams are collected as possible answers to thequestion.
For reasons of efficiency, we use onlythe page summaries returned by the engine and notthe full-text of the corresponding web page.The returned summaries contain the queryterms, usually with a few words of surroundingcontext.
The summary text is processed in accor-dance with the patterns specified by the rewrites.Unigrams, bigrams and trigrams are extracted andsubsequently scored according to the weight of thequery rewrite that retrieved it.
These scores aresummed across all summaries containing the n-gram (which is the opposite of the usual inversedocument frequency component of docu-ment/passage ranking schemes).
We do not countfrequency of occurrence within a summary (theusual tf component in ranking schemes).
Thus, thefinal score for an n-gram is based on the weightsassociated with the rewrite rules that generated itand the number of unique summaries in which itoccurred.2.3 N-Gram FilteringNext, the n-grams are filtered and reweighted ac-cording to how well each candidate matches theexpected answer-type, as specified by a handful ofhandwritten filters.
The system uses filtering inthe following manner.
First, the query is analyzedand assigned one of seven question types, such aswho-question, what-question, or how-many-question.
Based on the query type that has beenassigned, the system determines what collection offilters to apply to the set of potential answers foundduring the collection of n-grams.
The candidate n-grams are analyzed for features relevant to the fil-ters, and then rescored according to the presence ofsuch information.A collection of 15 simple filters were devel-oped based on human knowledge about questiontypes and the domain from which their answers canbe drawn.
These filters used surface string fea-tures, such as capitalization or the presence of dig-its, and consisted of handcrafted regular expressionpatterns.2.4 N-Gram TilingFinally, we applied an answer tiling algorithm,which both merges similar answers and assembleslonger answers from overlapping smaller answerfragments.
For example, "A B C" and "B C D" istiled into "A B C D." The algorithm proceedsgreedily from the top-scoring candidate - all sub-sequent candidates (up to a certain cutoff) arechecked to see if they can be tiled with the currentcandidate answer.
If so, the higher scoring candi-date is replaced with the longer tiled n-gram, andthe lower scoring candidate is removed.
The algo-rithm stops only when no n-grams can be furthertiled.3 ExperimentsFor experimental evaluations we used the first 500TREC-9 queries (201-700) (Voorhees and Harman,2000).
We used the patterns provided by NIST forautomatic scoring.
A few patterns were slightlymodified to accommodate the fact that some of theanswer strings returned using the Web were notavailable for judging in TREC-9.
We did this in avery conservative manner allowing for more spe-cific correct answers (e.g., Edward J. Smith vs.Edward Smith) but not more general ones (e.g.,Smith vs. Edward Smith), and also allowing forsimple substitutions (e.g., 9 months vs. ninemonths).
There also are substantial time differ-ences between the Web and TREC databases (e.g.,the correct answer to Who is the president of Bo-livia?
changes over time), but we did not modifythe answer key to accommodate these time differ-ences, because it would make comparison withearlier TREC results impossible.
These changesinfluence the absolute scores somewhat but do notchange relative performance, which is our focushere.All runs are completely automatic, startingwith queries and generating a ranked list of 5 can-didate answers.
For the experiments reported inthis paper we used Google as a backend because itprovides query-relevant summaries that make ourn-gram mining efficient.
Candidate answers are amaximum of 50 bytes long, and typically muchshorter than that.
We report the Mean ReciprocalRank (MRR) of the first correct answer, the Num-ber of Questions Correctly Answered (NAns), andthe proportion of Questions Correctly Answered(%Ans).3.1 Basic System PerformanceUsing our current system with default settings weobtain a MRR of 0.507 and answers 61% of thequeries correctly (Baseline, Table 1).
The averageanswer length was 12 bytes, so the system is re-turning short answers, not passages.
Although itis impossible to compare our results precisely withTREC-9 groups, this is very good performance andwould place us near the top of 50-byte runs forTREC-9.3.2 Contributions of ComponentsTable 1 summarizes the contributions of the differ-ent system components to this overall perform-ance.
We report summary statistics as well aspercent change in performance when componentsare removed (%Drop MRR).Query Rewrites:As described earlier, queries are transformed tosuccessively less precise formats, with a finalbackoff to simply ANDing all the non-stop queryterms.
More precise queries have higher weightsassociated with them, so n-grams found in theseresponses are given priority.
If we set al the re-write weights to be equal, MRR drops from 0.507to 0.489, a drop of 3.6%.
Another way of lookingat the importance of the query rewrites is to exam-ine performance where the only rewrite the systemuses is the backoff AND query.
Here the drop ismore substantial, down to 0.450 which represents adrop of 11.2%.Query rewrites are one way in which wecapitalize on the tremendous redundancy of dataon the web ?
that is, the occurrence of multiplelinguistic formulations of the same answers in-creases the chances of being able to find an answerthat occurs within the context of a simple patternmatch with the query.
Our simple rewrites helpcompared to doing just AND matching.
Soubbotinand Soubbotin (2001) have used more specificregular expression matching to good advantage andwe could certainly incorporate some of those ideasas well.MRR NAns %Ans%DropMRRBaseline 0.507 307 61.4% 0.0%Query Rewrite:Same Weight All Rewrites 0.489 298 59.6% 3.6%AND-only query 0.450 281 56.2% 11.2%Filter N-Gram:Base, NoFiltering 0.416 268 53.6% 17.9%AND, NoFiltering 0.338 226 45.2% 33.3%Tile N-Gram:Base, NoTiling 0.435 277 55.4% 14.2%AND, NoTiling 0.397 251 50.2% 21.7%Combinations:Base, NoTiling NoFiltering 0.319 233 46.6% 37.1%AND, NoTiling NoFiltering 0.266 191 38.2% 47.5%Table 1.
Componential analysis of the AskMSR QA system.N-Gram Filtering:Unigrams, bigrams and trigrams are extracted fromthe (up to) 100 best-matching summaries for eachrewrite, and scored according the weight of thequery rewrite that retrieved them.
The score as-signed to an n-gram is a weighted sum across thesummaries containing the n-grams, where theweights are those associated with the rewrite thatretrieved a particular summary.
The best-scoringn-grams are then filtered according to seven querytypes.
For example the filter for the query Howmany dogs pull a sled in the Iditarod?
prefers anumber, so candidate n-grams  like dog race, run,Alaskan, dog racing, many mush move down thelist and pool of 16 dogs (which is a correct answer)moves up.
Removing the filters decreases MRRby 17.9% relative to baseline (down to 0.416).
Oursimple n-gram filtering is the most important indi-vidual component of the system.N-Gram Tiling:Finally, n-grams are tiled to create longer answerstrings.
This is done in a simple greedy statisticalmanner from the top of the list down.
Not doingthis tiling decreases performance by 14.2% relativeto baseline (down to 0.435).
The advantagesgained from tiling are two-fold.
First, with tilingsubstrings do not take up several answer slots, sothe three answer candidates: San, Francisco, andSan Francisco, are conflated into the single answercandidate: San Francisco.
In addition, longer an-swers can never be found with only trigrams, e.g.,light amplification by stimulted emission of radia-tion can only be returned by tiling these shorter n-grams into a longer string.Combinations of Components:Not surprisingly, removing all of our major com-ponents except the n-gram accumulation (weightedsum of occurrences of unigrams, bigrams and tri-grams) results in substantially worse performancethan our full system, giving an MRR of 0.266, adecrease of 47.5%.
The simplest entirely statisti-cal system with no linguistic knowledge or proc-essing employed, would use only AND queries, dono filtering, but do statistical tiling.
This systemuses redundancy only in summing n-gram countsacross summaries.
This system has MRR 0.338,which is a 33% drop from the best version of oursystem, with all components enabled.
Note, how-ever, that even with absolutely no linguistic proc-essing, the performance attained is still very rea-sonable performance on an absolute scale, and infact only one TREC-9 50-byte run achieved higheraccuracy than this.To summarize, we find that all of our process-ing components contribute to the overall accuracyof the question-answering system.
The preciseweights assigned to different query rewrites seemsrelatively unimportant, but the rewrites themselvesdo contribute considerably to overall accuracy.N-gram tiling turns out to be extremely effective,serving in a sense as a ?poor man?s named-entityrecognizer?.
Because of the effectiveness of ourtiling algorithm over large amounts of data, we donot need to use any named entity recognition com-ponents.
The component that identifies what filtersto apply over the harvested n-grams, along with theactual regular expression filters themselves, con-tributes the most to overall performance.4 Component ProblemsAbove we described how components contributedto improving the performance of the system.
Inthis section we look at what components errors areattributed to.
In Table 2, we show the distributionof error causes, looking at those questions forwhich the system returned no correct answer in thetop five hypotheses.Problem % of ErrorsUnits 23Time 20Assembly 16Correct 14Beyond Paradigm 12Number Retrieval 5Unknown Problem 5Synonymy 2Filters  2Table 2.
Error AttributionThe biggest error comes from not knowingwhat units are likely to be in an answer given aquestion (e.g.
How fast can a Corvette go ?
xxxmph).
Interestingly, 34% of our errors (Time andCorrect) are not really errors, but are due to timeproblems or cases where the answer returned istruly correct but not present in the TREC-9 answerkey.
16% of the failures come from the inability ofour n-gram tiling algorithm to build up the fullstring necessary to provide a correct answer.Number retrieval problems come from the factthat we cannot query the search engine for a num-ber without specifying the number.
For example, agood rewrite for the query How many islands doesFiji have would be ?
Fiji has <NUM> islands ?,but we are unable to give this type of query to thesearch engine.
Only 12% of the failures we clas-sify as being truly outside of the system?s currentparadigm, rather than something that is either al-ready correct or fixable with minor system en-hancements.5 Knowing When We Don?t KnowTypically, when deploying a question answeringsystem, there is some cost associated with return-ing incorrect answers to a user.
Therefore, it isimportant that a QA system has some idea as tohow likely an answer is to be correct, so it canchoose not to answer rather than answer incor-rectly.
In the TREC QA track, there is no distinc-tion made in scoring between returning a wronganswer to a question for which an answer existsand returning no answer.
However, to deploy areal system, we need the capability of making atrade-off between precision and recall, allowingthe system not to answer a subset of questions, inhopes of attaining high accuracy for the questionswhich it does answer.Most question-answering systems usehand-tuned weights that are often combined in anad-hoc fashion into a final score for an answer hy-pothesis (Harabagiu et al, 2000; Hovy et al, 2000;Prager et al, 2000; Soubbotin & Soubbotin, 2001;Brill et.
al., 2001).
Is it still possible to induce auseful precision-recall (ROC) curve when the sys-tem is not outputting meaningful probabilities foranswers?
We have explored this issue within theAskMSR question-answering system.Ideally, we would like to be able to deter-mine the likelihood of answering correctly solelyfrom an analysis of the question.
If we can deter-mine we are unlikely to answer a question cor-rectly, then we need not expend the time, cpucycles and network traffic necessary to try to an-swer that question.We built a decision tree to try to predictwhether the system will answer correctly, based ona set of features extracted from the question string:word unigrams and bigrams, sentence length(QLEN), the number of capitalized words in thesentence, the number of stop words in the sentence(NUMSTOP), the ratio of the number of nonstopwords to stop words, and the length of longestword (LONGWORD).
We use a decision tree be-cause we also wanted to use this as a diagnostictool to indicate what question types we need to putfurther developmental efforts into.
The decisiontree built from these features is shown in Figure 2.The first split of the tree asks if the word ?How?appears in the question.
Indeed, the system per-forms worst on ?How?
question types.
We do beston short ?Who?
questions with a large number ofstop words.Figure 2.
Learning When We Don't Know -- Us-ing Only Features from QueryWe can induce an ROC curve from thisdecision tree by sorting the leaf nodes from thehighest probability of being correct to the lowest.Then we can gain precision at the expense of recallby not answering questions in the leaf nodes thathave the highest probability of error.
The result ofdoing this can be seen in Figures 3 and 4, the linelabeled ?Question Features?.
The decision treewas trained on Trec 9 data.
Figure 3 shows theresults when applied to the same training data, andFigure 4 shows the results when testing on Trec 10data.
As we can see, the decision tree overfits thetraining data and does not generalize sufficiently togive useful results on the Trec 10 (test) data.Next, we explored how well answer cor-rectness correlates with answer score in our sys-tem.
As discussed above, the final score assignedto an answer candidate is a somewhat ad-hoc scorebased upon the number of retrieved passages the n-gram occurs in, the weight of the rewrite used toretrieve each passage, what filters apply to the n-gram, and the effects of merging n-grams in an-swer tiling.
In Table 3, we show the correlationcoefficient calculated between whether a correctanswer appears in the top 5 answers output by thesystem and (a) the score of the system?s firstranked answer and (b) the score of the first rankedanswer minus the score of the second ranked an-swer.
A correlation coefficient of 1 indicatesstrong positive association, whereas a correlationof ?1 indicates strong negative association.
We seethat there is indeed a correlation between thescores output by the system and the answer accu-racy, with the correlation being tighter when justconsidering the score of the first answer.CorrelationCoefficientScore #1 .363Score #1 ?
Score #2 .270Table 3 .
Do answer scores correlate with correct-ness?Because a number of answers returned byour system are correct but scored wrong accordingto the TREC answer key because of time mis-matches, we also looked at the correlation, limitingourselves to Trec 9 questions that were not time-sensitive.
Using this subset of questions, the corre-lation coefficient between whether a correct an-swer appears in the system?s top five answers, andthe score of the #1 answer, increases from .363 to.401.
In Figure 3 and 4, we show the ROC curveinduced by deciding when not to answer a questionbased on the score of the first ranked answer (theline labeled ?score of #1 answer?).
Note that thescore of the top ranked answer is a significantlybetter predictor of accuracy than what we attain byconsidering features of the question string, andgives consistent results across two data sets.Finally, we looked into whether other at-tributes were indicative of the likelihood of answercorrectness.
For every question, a set of snippets isgathered.
Some of these snippets come from ANDqueries and others come from more refined exactstring match rewrites.
In Table 4, we show MRRas a function of the number of non-AND snippetsretrieved.
For instance, when all of the snippetscome from AND queries, the resulting MRR wasfound to be only .238.
For questions with 100 to400 snippets retrieved from exact string match re-writes, the MRR was .628.NumQ MRR0 91 0.2381 to 10 80 0.40511 to 100 153 0.612100 to 400 175 0.628NumNon-ANDPassagesTable 4 .
Accuracy vs.
Number of Passages Re-trieved From Non-AND RewritesWe built a decision tree to predict whethera correct answer appears in the top 5 answers,based on all of the question-derived features de-scribed earlier, the score of the number one rank-ing answer, as well as a number of additionalfeatures describing the state of the system in proc-essing a particular query.
Some of these featuresinclude: the total number of matching passagesretrieved, the number of non-AND matching pas-sages retrieved, whether a filter applied, and theweight of the best rewrite rule for which matchingpassages were found.
We show the resulting deci-sion tree in Figure 5, and resulting ROC curve con-structed from this decision tree, in Figure 3 and 4(the line labeled ?All Features?).
In this case, thedecision tree does give a useful ROC curve on thetest data (Figure 4), but does not outperform thesimple technique of using the ad hoc score of thebest answer returned by the system.
Still, the deci-sion tree has proved to be a useful diagnostic inhelping us understand the weaknesses of our sys-tem.ROC Curve for QA0.650.70.750.80.850.90.9510 0.5 1RecallPrecisionAll FeaturesScore of #1AnswerQuestionFeaturesFigure 3.
Three different precision/recall trade-offs, trained on Trec 9 and tested on Trec 9.Trec 10 ROC Curve0.50.550.60.650.70.750.80.850.90.9510 0.5 1RecallPrecisionScore of #1AnswerQuestionFeaturesAll FeaturesFigure 4.
Three different precision/recall trade-offs, trained on Trec 9 and tested on Trec 10.6 ConclusionsWe have presented a novel approach to question-answering and carefully analyzed the contributionsof each major system component, as well as ana-lyzing what factors account for the majority of er-rors made by the AskMSR question answeringsystem.
In addition, we have demonstrated anapproach to learning when the system is likely toanswer a question incorrectly, allowing us to reachany desired rate of accuracy by not answeringsome portion of questions.
We are currently ex-ploring whether these techniques can be extendedbeyond short answer QA to more complex cases ofinformation access.Figure 5.
Learning When We Don't Know -- Us-ing All FeaturesReferencesAAAI Spring Symposium Series Mining answers fromtext and knowledge bases (2002).S.
Abney, M. Collins and A. Singhal (2000).
Answerextraction.
In Proceedings of ANLP 2000.ACL-EACL Workshop on Open-domain question an-swering.
(2002).E.
Agichtein, S. Lawrence and L. Gravano (2001).Learning search engine specific query transforma-tions for question answering.
In Proceedings ofWWW10.E.
Brill, J. Lin, M. Banko, S. Dumais and A. Ng (2001).Data-intensive question answering.
In Proceedingsof the Tenth Text Retrieval Conference (TREC 2001).S.
Buchholz (2001).
Using grammatical relations, an-swer frequencies and the World Wide Web for TRECquestion answering.
To appear in Proceedings ofthe Tenth Text REtrieval Conference (TREC 2001).J.
Chen, A. R. Diekema, M. D. Taffet, N. McCracken,N.
E. Ozgencil, O. Yilmazel, E. D. Liddy (2001).Question answering: CNLP at the TREC-10 questionanswering track.
To appear in Proceedings of theTenth Text REtrieval Conference (TREC 2001).C.
Clarke, G. Cormack and T. Lyman (2001).
Exploit-ing redundancy in question answering.
In Proceed-ings of SIGIR?2001.C.
Clarke, G. Cormack and T. Lynam (2001).
Webreinforced question answering.
To appear in Pro-ceedings of the Tenth Text REtrieval Conference(TREC 2001).S.
Harabagiu, D. Moldovan, M. Pasca, R. Mihalcea, M.Surdeanu, R. Bunescu, R. Girju, V. Rus and P.Morarescu (2000).
FALCON: Boosting knowledgefor question answering.
In Proceedings of the NinthText REtrieval Conference (TREC-9).E.
Hovy, L. Gerber, U. Hermjakob, M. Junk and C. Lin(2000).
Question answering in Webclopedia.
InProceedings of the Ninth Text REtrieval Conference(TREC-9).E.
Hovy, U. Hermjakob and C. Lin (2001).
The use ofexternal knowledge in factoid QA.
To appear inProceedings of the Tenth Text REtrieval Conference(TREC 2001).C.
Kwok, O. Etzioni and D. Weld (2001).
Scaling ques-tion answering to the Web.
In Proceedings ofWWW?10.M.
A. Pasca and S. M. Harabagiu (2001).
High per-formance question/answering.
In Proceedings ofSIGIR?2001.J.
Prager, E. Brown, A. Coden and D. Radev (2000).Question answering by predictive annotation.
InProceedings of SIGIR?2000.D.
R. Radev, H. Qi, Z. Zheng, S. Blair-Goldensohn, Z.Zhang, W. Fan and J. Prager (2001).
Mining the webfor answers to natural language questions.
In ACMCIKM 2001: Tenth International Conference on In-formation and Knowledge Management.M.
M. Soubbotin and S. M. Soubbotin (2001).
Patternsand potential answer expressions as clues to the rightanswers.
To appear in Proceedings of the Tenth TextREtrieval Conference (TREC 2001).E.
Voorhees and D. Harman, Eds.
(2000).
Proceedingsof the Ninth Text REtrieval Conference (TREC-9).E.
Voorhees and D. Harman, Eds.
(2001).
Proceedingsof the Tenth Text REtrieval Conference (TREC2001).
