Session 8: Statistical Language ModelingMitchell Marcus, ChairDepar tment  of Computer  and In format ion  Sc ienceUn ivers i ty  of Pennsy lvan iaPh i lade lph ia ,  PA 19104-63891.
In t roduct ionOver the past several years, the successful applicationof statistical techniques in natural language processinghas penetrated further and further into written languagetechnology, proceding with time from the periphery ofwritten language processing into deeper and deeper as-pects of language processing.
At the periphery of naturallanguage understanding, Hidden Markov Models werefirst applied over ten years ago to the problem of deter-mining part of speech (POS).
HMM POS taggers haveyielded quite good results for many tasks (96%+ correct,on a per word basis), and have been widely used in writ-ten language systems for the last several years.
A littlecloser in from the periphery, extensions to probabilis-tic context free parsing (PCFG) methods have greatlyincreased the accuracy of probabilistic parsing methodswithin the last several years; these methods conditionthe probabilities of standard CFG rules on aspects ofextended lingustic context.
Just within the last yearor two, we have begun to see the first applications ofstatistical methods to the problem of word sense deter-mination and lexical semantics.
It is worthy of note thatthe first presentation of a majority of these techniqueshas been within this series of Workshops ponsored byARPA.It is a measure of how fast this field is progressing thata majority of papers in this session, six, are on lexicalsemantics, an area where the effective application of sta-tistical techniques would have been unthinkable only afew years ago.
One other paper addresses the questionof how a POS tagger can be built using very limitedamounts of training data, another presents a method forfinding word associations and two others address variousaspects of statistical parsing.2.
Part of Speech TaggingThe first paper in this session, by Matsukawa, Miller andWeischedel, describes a cascade of several components,sandwiching a novel algorithm between the output of anexisting black-box segmentation and POS labelling sys-tern for Japanese, JUMAN, and the POST HMM POStagger.
The middle algorithm uses what the authors callexample-based correction to change some of JUMAN'sinitial word segmentation and to add alternative POStags from which POST can then make a final selection.
(Japanese text is printed without spaces; determiningwhere one word stops and another starts is a crucialproblem in Japanese text processing.)
The example-based correction method, closely related to a methodpresented by Brill at this workshop last year, uses a verysmall amount of training data to learn a set of symbolictransformation rules which augment or change the out-put of JUMAN in particular deterministic contexts.3.
Gra lnrnar  Induct ion  and  Probab i l i s t i cPars ingMost current methods for probabilistic parsing either es-timate grammar ule probabilities directly from an an-notated corpus or else use Baker's Inside/Outside algo-rithm (often in combination with some annotation) toestimate the parameters from an unannotated corpus.The 2/0 algorithm, however, maximizes the wrong ob-jective function for purposes of recovering the expectedgrammatical structure for a given sentence; the 2/0 al-gorithm finds the model that maximizes the likelihoodof the observed sentence strings without reference to thegrammatical structure assigned to that string by the es-timated gramnaar.
Often, however, probabilistic parsingis used to derive a tree structure for use with a semanticanalysis component based upon syntax directed transla-tion; for this translation to work effectively, the detailsof the parse tree must be appropriate for tree-based se-mantic composition techniques.
Current techniques arealso inapplicable to the recently developed class of chunkparsers, parsers which use finite-state techniques to parsethe non-recursive structures of the language, and thenuse another technique, usually related to dependencyparsing, to connect hese chunks together.
Two papersin this session can be viewed as addressing one or both ofthese issues.
The paper by Abney presents a new mea-sure for evaluating parser performance tied directly to225grammatical structure, and suggests ways in which sucha measure can be used for chunk parsing.
Brill presentsa new technique for parsing which extends the symbolicPOS tagger he presented last year.
Surprisingly, thissimple technique performs as well as the best recent re-sults using the I /O algorithm, using a very simple tech-nique to learn less than two hundred purely symbolicrules which deterministically parse new input.4.
Lex ica l  semant ics :  Sense  c lassdeterminat ionThe remaining papers in this session address three sep-arate areas of lexical semantics.
The first is sense classdetermination, determining, for example, whether a par-ticular use of the word "newspaper" refers to the physi-cal entity that sits by your front door in the morning, orthe corporate ntity that publishes it; whether a partic-ular use of "line" means a product line, a queue, a lineof text, a fishing line, etc.
Several papers in this ses-sion address the question of how well automatic statisti-cal techniques can discriminate between alternative wordsenses, and how much information such techniques mustuse.
The paper by Leacock, Miller and Voorhees teststhree different echniques for sense class determination:Bayesian decision theory, neural networks, and contentvectors.
These experiments show that the three tech-niques are statistically indistinguishable, ach resolvingbetween three different uses of "line" with an accuracyof about 76%, and between six different uses with anaccuracy of about 73%.
These techniques use an ex-tended context of about, 100 words around the targetword; Yarowsky's paper presents a new technique whichuses only five words on either side of the target word, butcan provide roughly comparable results by itself.
Thisnew method might well be combined with one of theseearlier techniques to provide improved performance overeither technique individually.5.
Lex ica l  semant ics :  ad jec t iva l  sca lesA second area of lexical semnantics focuses on the seman-tics of adjectives that determine linguistic scales.
Forexample, one set of adjectives lie on the linguistic scalefrom hot through warm and cool to cold, while anotherset lies on the scale that goes fi'om huge through bigto little to tiny.
Many adjectives can be characteriz-ing as picking out a point or range on some such scale.These scales play a role in human language understand-ing because of a phenomenon called scalar implicature,which underlies the fact that if someone asks if Tokyois a big city, much better than replying "yes" is to say,"Well, no; it's actually quite huge".
By the law of scalarimplicature, one cannot felicitously assent to an asser-tion about a midpoint on a scale even if it is logicallytrue, if an assertion about an extremum is also logi-cally true.
McKeown and Hatzivassiloglou take a firststep toward using statistical techniques to automaticallydetermine where adjectives fall along such scales by pre-senting a method which automatically clusters adjectivesinto groups which are closely related to such scales.6.
Lex ica l  semant ics :  Se lec t iona lRest r i c t ionsAnother key aspect of lexical semantics i the determina-tion of the selectional constraints of verbs; determiningfor each sense of any given verb what kinds of entitiescan serve as the subject for a given verb sense, and whatkinds of entities can serve as objects.
For example, forone meaning of open, the thing opened is most likely tobe an entrance; for another meaning, a mouth; for an-other, a container; fbr another, a discourse.
One keybarrier to determining such selectional constraints auto-maritally is a serious problem with sparse data; in a largecorpus, a given verb is likely to occur with any particu-lar noun as object in only a handful of instances.
Twopapers in this session automatically derive selectional re-strictions, each with a different solution to this partic-ular form of the sparse data problem.
The paper byResnik utilizes an information theoretic technique to au-tomatically determine such selectional restrictions; thisinformation is then used to resolve a number of syntac-tic anabiguities that any parser must deal with.
Resnikuses the noun is-a network within Miller's WordNet toprovide sufficiently large classes to obtain reliable results.Grishman and Sterling attack the problem of sparse databy using co-occurance smoothing on a set.
of fully auto-matically generated selectional constraints.In one last paper in lexical semantics, Matsukawapresents a new naethod of determining word associationsin Japanese text.
Such word associations are useful indealing with parsing ambiguities and should also proveuseful for Japanese word segmentation.226
