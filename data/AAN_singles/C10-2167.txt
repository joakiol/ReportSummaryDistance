Coling 2010: Poster Volume, pages 1462?1470,Beijing, August 2010Extracting and Ranking Product Featuresin Opinion DocumentsLei  ZhangDepartment of Computer ScienceUniversity of Illinois at Chicagolzhang3@cs.uic.eduSuk Hwan LimHewlett-Packard Labssuk-hwan.lim@hp.comBing LiuDepartment of Computer ScienceUniversity of Illinois at Chicagoliub@cs.uic.eduEamonn O?Brien-StrainHewlett-Packard Labseob@hpl.hp.comAbstractAn important task of opinion mining isto extract people?s opinions on featuresof an entity.
For example, the sentence,?I love the GPS function of MotorolaDroid?
expresses a positive opinion onthe ?GPS function?
of the Motorolaphone.
?GPS function?
is the feature.This paper focuses on mining features.Double propagation is a state-of-the-arttechnique for solving the problem.
Itworks well for medium-size corpora.However, for large and small corpora, itcan result in low precision and low re-call.
To deal with these two problems,two improvements based on part-wholeand ?no?
patterns are introduced to in-crease the recall.
Then feature ranking isapplied to the extracted feature candi-dates to improve the precision of thetop-ranked candidates.
We rank featurecandidates by feature importance whichis determined by two factors: feature re-levance and feature frequency.
Theproblem is formulated as a bipartitegraph and the well-known web pageranking algorithm HITS is used to findimportant features and rank them high.Experiments on diverse real-life datasetsshow promising results.1 IntroductionIn recent years, opinion mining or sentimentanalysis (Liu, 2010; Pang and Lee, 2008) hasbeen an active research area in NLP.
One task isto extract people?s opinions expressed onfeatures of entities (Hu and Liu, 2004).
Forexample, the sentence, ?The picture of thiscamera is amazing?, expresses a positiveopinion on the picture of the camera.
?picture?is the feature.
How to extract features from acorpus is an important problem.
There areseveral studies on feature extraction (e.g., Huand Liu, 2004, Popescu and Etzioni, 2005,Kobayashi et al, 2007, Scaffidi et al, 2007,Stoyanov and Cardie.
2008, Wong et al, 2008,Qiu et al, 2009).
However, this problem is farfrom being solved.Double Propagation (Qiu et al, 2009) is astate-of-the-art unsupervised technique forsolving the problem.
It mainly extracts nounfeatures, and works well for medium-sizecorpora.
But for large corpora, this method canintroduce a great deal of noise (low precision),and for small corpora, it can miss importantfeatures.
To deal with these two problems, wepropose a new feature mining method, whichenhances that in (Qiu et al, 2009).
Firstly, twoimprovements based on part-whole patterns and?no?
patterns are introduced to increase recall.Part-whole or meronymy is an importantsemantic relation in NLP, which indicates thatone or more objects are parts of another object.1462For example, the phrase ?the engine of the car?contains the part-whole relation that ?engine?
ispart of ?car?.
This relation is very useful forfeature extraction, because if we know oneobject is part of a product class, this objectshould be a feature.
?no?
pattern is anotherextraction pattern.
Its basic form is the word?no?
followed by a noun/noun phrase, forinstance, ?no noise?.
People often express theirshort comments or opinions on features usingthis pattern.
Both types of patterns can help findfeatures missed by double propagation.
As forthe low precision problem, we present a featureranking approach to tackle it.
We rank featurecandidates based on their importance whichconsists of two factors: feature relevance andfeature frequency.
The basic idea of featureimportance ranking is that if a feature candidateis correct and frequently mentioned in a corpus,it should be ranked high; otherwise it should beranked low in the final result.
Feature frequencyis the occurrence frequency of a feature in acorpus, which is easy to obtain.
However,assessing feature relevance is challenging.
Wemodel the problem as a bipartite graph and usethe well-known web page ranking algorithmHITS (Kleinberg, 1999) to find importantfeatures and rank them high.
Our experimentalresults show superior performances.
In practicalapplications, we believe that ranking is alsoimportant for feature mining because rankingcan help users to discover important featuresfrom the extracted hundreds of fine-grainedcandidate features efficiently.2 Related workHu and Liu (2004) proposed a technique basedon association rule mining to extract productfeatures.
The main idea is that people often usethe same words when they comment on thesame product features.
Then frequent itemsetsof nouns in reviews are likely to be product fea-tures while the infrequent ones are less likely tobe product features.
This work also introducedthe idea of using opinion words to find addi-tional (often infrequent) features.Popescu and Etzioni (2005) investigated thesame problem.
Their algorithm requires that theproduct class is known.
The algorithm deter-mines whether a noun/noun phrase is a featureby computing the pointwise mutual information(PMI) score between the phrase and class-specific discriminators, e.g., ?of xx?, ?xx has?,?xx comes with?, etc., where xx is a productclass.
This work first used part-whole patternsfor feature mining, but it finds part-whole basedfeatures by searching the Web.
Querying theWeb is time-consuming.
In our method, we usepredefined part-whole relation patterns to ex-tract features in a domain corpus.
These patternsare domain-independent and fairly accurate.Following the initial work in (Hu and Liu2004), several researchers have further exploredthe idea of using opinion words in product fea-ture mining.
A dependency based method wasproposed in (Zhuang et al, 2006) for a moviereview analysis application.
Qiu et al (2009)proposed a double propagation method, whichexploits certain syntactic relations of opinionwords and features, and propagates throughboth opinion words and features iteratively.
Theextraction rules are designed based on differentrelations between opinion words and features,and among opinion words and features them-selves.
Dependency grammar was adopted todescribe these relations.
In (Wang and Wang,2008), another bootstrapping method was pro-posed.
In (Kobayashi et al 2007), a pattern min-ing method was used.
The patterns are relationsbetween feature and opinion pairs (they call as-pect-evaluation pairs).
The patterns are minedfrom a large corpus using pattern mining.
Statis-tics from the corpus are used to determine theconfidence scores of the extraction.In general information extraction, there aretwo approaches: rule-based and statistical.
Earlyextraction systems are mainly based on rules(e.g., Riloff, 1993).
In statistical methods, themost popular models are Hidden Markov Mod-els (HMM) (Rabiner, 1989), Maximum EntropyModels (ME) (Chieu et al, 2002) and Condi-tional Random Fields (CRF) (Lafferty et al,2001).
CRF has been shown to be the most ef-fective method.
It was used in (Stoyanov et al,2008).
However, a limitation of CRF is that itonly captures local patterns rather than longrange patterns.
It has been shown in (Qiu et al,2009) that many feature and opinion word pairshave long range dependencies.
Experimentalresults in (Qiu et al, 2009) indicate that CRFdoes not perform well.Other related works on feature extractionmainly use topic modeling to capture topics in1463reviews (Mei et al, 2007).
In (Su et al, 2008),the authors also proposed a clustering basedmethod with mutual reinforcement to identifyfeatures.
However, topic modeling or clusteringis only able to find some general/rough features,and has difficulty in finding fine-grained or pre-cise features, which is more related to informa-tion extraction.3 The Proposed MethodAs discussed in the introduction section, ourproposed method deals with the problems ofdouble propagation.
So let us give a short ex-planation why double propagation can causeproblems in large or small corpora.Double propagation assumes that features arenouns/noun phrases and opinion words are ad-jectives.
It is shown that opinion words areusually associated with features in some ways.Thus, opinion words can be recognized by iden-tified features, and features can be identified byknown opinion words.
The extracted opinionwords and features are utilized to identify newopinion words and new features, which are usedagain to extract more opinion words and fea-tures.
This propagation or bootstrapping processends when no more opinion words or featurescan be found.
The biggest advantage of the me-thod is that it requires no additional resourcesexcept an initial seed opinion lexicon, which isreadily available (Wilson et al, 2005, Ding etal., 2008).
Thus it is domain independent andunsupervised, avoiding laborious and time-consuming work of labeling data for supervisedlearning methods.
It works well for medium?size corpora.
But for large corpora, this methodmay extract many nouns/noun phrases whichare not features.
The precision of the methodthus drops.
The reason is that during propaga-tion, adjectives which are not opinionated willbe extracted as opinion words, e.g., ?entire?
and?current?.
These adjectives are not opinionwords but they can modify many kinds ofnouns/noun phrases, thus leading to extractingwrong features.
Iteratively, more and morenoises may be introduced during the process.The other problem is that for certain domains,some important features do not have opinionwords modifying them.
For example, in reviewsof mattresses, a reviewer may say ?There is avalley on my mattress?, which implies a nega-tive opinion because ?valley?
is undesirable fora mattress.
Obviously, ?valley?
is a feature, but?valley?
may not be described by any opinionadjective, especially for a small corpus.
Doublepropagation is not applicable in this situation.To deal with the problem, we propose a novelmethod to mine features, which consists of twosteps: feature extraction and feature ranking.For feature extraction, we still adopt the doublepropagation idea to populate feature candidates.But two improvements based on part-whole re-lation patterns and a ?no?
pattern are made tofind features which double propagation cannotfind.
They can solve part of the recall problem.For feature ranking, we rank feature candidatesby feature importance.A part-whole pattern indicates one object ispart of another object.
For the previous example?There is a valley on my mattress?, we can findthat it contains a part-whole relation between?valley?
and ?mattress?.
?valley?
belongs to?mattress?, which is indicated by the preposi-tion ?on?.
Note that ?valley?
is not actually apart of mattress, but an effect on the mattress.
Itis called a pseudo part-whole relation.
For sim-plicity, we will not distinguish it from an actualpart-whole relation because for our feature min-ing task, they have little difference.
In this case,?noun1 on noun2?
is a good indicative patternwhich implies noun1 is part of noun2.
So if weknow ?mattress?
is a class concept, we can inferthat ?valley?
is a feature for ?mattress?.
Thereare many phrase or sentence patternsrepresenting this type of semantic relationwhich was studied in (Girju et al 2006).
Besidepart-whole patterns, ?no?
pattern is another im-portant and specific feature indicator in opiniondocuments.
We introduce these patterns in de-tail in Sections 3.2 and 3.3.Now let us deal with the first problem: noise.With opinion words, part-whole and ?no?
pat-terns, we have three feature indicators at hands,but all of them are ambiguous, which meansthat they are not hard rules.
We will inevitablyextract wrong features (also called noises) byusing them.
Pruning noises from feature candi-dates is a hard task.
Instead, we propose a newangle for solving this problem: feature ranking.The basic idea is that we rank the extracted fea-ture candidates by feature importance.
If a fea-ture candidate is correct and important, it shouldbe ranked high.
For unimportant feature or1464noise, it should be ranked low in the final result.Ranking is also very useful in practice.
In alarge corpus, we may extract hundreds of fine-grained features.
But the user often only caresabout those important ones, which should beranked high.
We identified two major factorsaffecting the feature importance: one is featurerelevance and the other is feature frequency.Feature relevance: it describes how possiblea feature candidate is a correct feature.
We findthat there are three strong clues to indicate fea-ture relevance in a corpus.
The first clue is thata correct feature is often modified by multipleopinion words (adjectives or adverbs).
For ex-ample, in the mattress domain, ?delivery?
ismodified by ?quick?
?cumbersome?
and ?time-ly?.
It shows that reviewers put emphasis on theword ?delivery?.
Thus we can infer that ?deli-very?
is a possible feature.
The second clue isthat a feature could be extracted by multiplepart-whole patterns.
For example, in the cardomain, if we find following two phrases, ?theengine of the car?
and ?the car has a big en-gine?, we can infer that ?engine?
is a feature forcar, because both phrases contain part-wholerelations to indicate ?engine?
is a part of ?car?.The third clue is the combination of opinionword modification, part-whole pattern extrac-tion and ?no?
pattern extraction.
That is, if afeature candidate is not only modified by opi-nion words but also extracted by part-whole or?no?
patterns, we can infer that it is a featurewith high confidence.
For example, for sentence?there is a bad hole in the mattress?, it stronglyindicates that ?hole?
is a feature for a mattressbecause it is modified by opinion word ?bad?and also in the part-whole pattern.
What ismore, we find that there is a mutual enforce-ment relation between opinion words, part-whole and ?no?
patterns, and features.
If an ad-jective modifies many correct features, it ishighly possible to be a good opinion word.
Si-milarly, if a feature candidate can be extractedby many opinion words, part-whole patterns, or?no?
pattern, it is also highly likely to be a cor-rect feature.
This indicates that the Web pageranking algorithm HITS is applicable.Feature frequency: This is another importantfactor affecting feature ranking.
Feature fre-quency has been considered in (Hu and Liu,2004; Blair-Goldensohn et al, 2008).
We con-sider a feature f1 to be more important than fea-ture f2 if f1 appears more frequently than f2 inopinion documents.
In practice, it is desirable torank those frequent features higher than infre-quent features.
The reason is that missing a fre-quently mentioned feature in opinion mining isbad, but missing a rare feature is not a big issue.Combining the above factors, we propose anew feature mining method.
Experiments showgood results on diverse real-life datasets.3.1 Double PropagationAs we described above, double propagation isbased on the observation that there are naturalrelations between opinion words and featuresdue to the fact that opinion words are often usedto modify features.
Furthermore, it is observedthat opinion words and features themselves haverelations in opinionated expressions too (Qiu etal., 2009).
These relations can be identified viaa dependency parser (Lin, 1998) based on thedependency grammar.
The identification of therelations is the key to feature extraction.Dependency grammar: It describes the de-pendency relations between words in a sentence.After parsed by a dependency parser, words in asentence are linked to each other by a certainrelation.
For a sentence, ?The camera has agood lens?, ?good?
is the opinion word and?lens?
is the feature of camera.
After parsing,we can find that ?good?
depends on ?lens?
withrelation mod.
Here mod means that ?good?
isthe adjunct modifier for ?lens?.
In some cases,an opinion word and a feature are not directlydependent, but they directly depend on a sameword.
For example, from the sentence ?The lensis nice?, we can find that both feature ?lens?
andopinion word ?nice?
depend on the verb ?is?with the relation s and pred respectively.
Here smeans that ?lens?
is the surface subject of ?is?while pred means that ?nice?
is the predicate ofthe ?is?
clause.In (Qiu et al, 2009), it defines two categoriesof dependency relations to summarize all typesof dependency relations between two words,which are illustrated in Figure 1.
Arrows areused to represent dependencies.Direct relations: It represents that one worddepends on the other word directly or they bothdepend on a third word directly, shown in (a)and (b) of Figure 1.
In (a), B depends on A di-rectly, and in (b) they both directly depend on D.Indirect relation: It represents that one word1465depends on the other word through other wordsor they both depend on a third word indirectly.For example, in (c) of Figure 1, B depends on Athrough D; in (d) of Figure 1, A depends on Dthrough I1 while B depends on D through I2.
Forsome complicated situations, there can be morethan one I1 or I2.Fig.1 Different relations between A and BParsing indirect relations is error-prone forWeb corpora.
Thus we only use direct relationto extract opinion words and feature candidatesin our application.
For detailed extraction rules,please refer to the paper (Qiu et al, 2009).3.2 Part-whole relationAs we discussed above, a part-whole relation isa good indicator for features if the class conceptword (the ?whole?
part) is known.
For example,the compound nominal ?car hood?
contains thepart-whole relation.
If we know ?car?
is theclass concept word, then we can infer that?hood?
is a feature for car.
Part-whole patternsoccur frequently in text and are expressed by avariety of lexico-syntactic structures (Girju etal, 2006; Popescu and Etzioni, 2005).
There aretwo types of lexico-syntactic structures convey-ing part-whole relations: unambiguous structureand ambiguous structure.
The unambiguousstructure clearly indicates a part-whole relation.For example, for sentences ?the camera consistsof lens, body and power cord.?
and ?the bedwas made of wood?.
In these cases, the detec-tion of the patterns leads to the discovery of realpart-whole relations.
We can easily find featuresof the camera and the bed.
Unfortunately, thiskind of patterns is not very frequent in a corpus.However, there are many ambiguous expres-sions that are explicit but convey part-wholerelations only in some contexts.
For example,for two phrases ?valley on the mattress?
and?toy on the mattress?, ?valley?
is a part of ?mat-tress?
whereas ?toy?
is not a part of ?mattress?.Our idea is to use both the unambiguous andambiguous patterns.
Although ambiguous pat-terns may bring some noise, we can rank themlow in the ranking procedure.
The followingtwo kinds of patterns are what we have utilizedfor feature extraction.3.2.1 Phrase patternIn this case, the part-whole relation exists in aphrase.NP + Prep + CP:  noun/noun phrase (NP)contains the part word and the class conceptphrase (CP) contains the whole word.
They areconnected by the preposition word (Prep).
Forexample, ?battery of the camera?
is an instanceof this pattern where NP (battery) is the partnoun and CP (camera) is the whole noun.
Forour application, we only use three specific pre-positions: ?of?, ?in?
and ?on?.CP + with + NP:   likewise, CP is the classconcept phrase, and NP is the noun/noun phrase.They are connected by the word ?with?.
HereNP is likely to be a feature.
For example, in aphrase, ?mattress with a cover?, ?cover?
is afeature for mattress.NP CP or CP NP: noun/noun phase (NP)and class concept phrase (CP) forms a com-pound word.
For example, ?mattress pad?.
Here?pad?
is a feature of ?mattress?.3.2.2 Sentence patternIn these patterns, the part-whole relation is indi-cated in a sentence.
The patterns contain specif-ic verbs.
The part word and the whole word canbe found inside noun phrases or prepositionalphrases which contain specific prepositions.
Weutilize the following patterns in our application.
?CP Verb NP?
:  CP is the class conceptphrase that contains the whole word, NP is thenoun phrase that contains the part word and theverb is restricted and specific.
For example, in asentence, ?the phone has a big screen?, we caninfer that ?screen?
is a feature for ?phone?,which is a class concept.
In sentence patterns,verbs play an important role.
We use indicativeverbs to find part-whole relations in a sentence,A DA BBBADADI1BI2(a) (b)(c) (d)1466i.e., ?has?, ?have?
?include?
?contain?
?consist?,?comprise?
and so on (Girju et al 2006).It is worth mentioning that in order to usepart-whole relations, the class concept word fora corpus is needed, which is fairly easy to findbecause the noun with the most frequent occur-rences in a corpus is always the class conceptword based on our experiments.3.3 ?no?
PatternBesides opinion word and part-whole relation,?no?
pattern is also an important pattern indicat-ing features in a corpus.
Here ?no?
representsword no.
The basic form of the pattern is ?no?word followed by noun/noun phrase.
This sim-ple pattern actually is very useful to feature ex-traction.
It is a specific pattern for product re-views and forum posts.
People often expresstheir comments or opinions on features by thisshort pattern.
For example, in a mattress domain,people always say that ?no noise?
and ?no in-dentation?.
Here ?noise?
and ?indentation?
areall features for the mattress.
We discover thatthis pattern is frequently used in corpora and avery good indicator for features with a fairlyhigh precision.
But we have to take care of thesome fixed ?no?
expression, like ?no problem?
?no offense?.
In these cases, ?problem?
and ?of-fense?
should not be regarded as features.
Wehave a list of such words, which are manuallycompiled.3.4 Bipartite Graph and HITS AlgorithmHyperlink-induced topic search (HITS) is a linkanalysis algorithm that rates Web pages.
Asdiscussed in the introduction section, we canapply the HITS algorithm to compute featurerelevance for ranking.Before illustrating how HITS can be appliedto our scenario, let us first give a briefintroduction to HITS.
Given a broad searchquery q, HITS sends the query to a searchengine system, and then collects k (k = 200 inthe original paper) highest ranked pages, whichare assumed to be highly relevant to the searchquery.
This set is called the root set R; then itgrows R by including any page pointed to apage in R, then forms a base set S. HITS thenworks on the pages in S. It assigns every page inS an authority score and a hub score.
Let thenumber of pages to be studied be n. We use G =(V, E) to denote the (directed) link graph of S. Vis the set of pages (or nodes) and E is the set ofdirected edges (or links).
We use L to denote theadjacency matrix of the graph.???
?
 ???????
??
?
??????????
?(1)Let the authority score of the page i be A(i), andthe hub score of page i be H(i).
The mutual rein-forcing relationship of the two scores isrepresented as follows:????
?
?
???????????
(2)????
?
?
???????????
(3)We can write them in a matrix form.
We use Ato denote the column vector with all the authori-ty scores, A = (A(1), A(2), ?, A(n))T, and use Hto denote the column vector with all the hubscores, H = (H(1), H(2), ?, H(n))T,?
?4)                         ???)?
?
5)                            ??
)To solve the problem, the widely used methodis power iteration, which starts with some ran-dom values for the vectors, e.g., A0 = H0 = (1, 1,1, ?1,).
It then continues to compute iterativelyuntil the algorithm converges.From the formulas, we can see that the author-ity score estimates the importance of the contentof the page, and the hub score estimates the val-ues of its links to other pages.
An authorityscore is computed as the sum of the scaled hubscores that point to that page.
A hub score is thesum of the scaled authority scores of the pagesit points to.
The key idea of HITS is that a goodhub points to many good authorities and a goodauthority is pointed by many good hubs.
Thus,authorities and hubs have a mutual reinforce-ment relationship.For our scenario, we have three strong cluesfor features in a corpus: opinion words, part-whole patterns, and the ?no?
pattern.
Althoughall these three clues are not hard rules, thereexist mutual enforcement relations betweenthem.
If an adjective modify many features, it ishighly likely to be a good opinion word.
If afeature candidate is modified by many opinionwords, it is likely to be a genuine feature.
Thesame goes with part-whole patterns, the ?no?pattern, or the combination for these three clues.This kind of mutual enforcement relation can benaturally modeled in the HITS framework.1467Applying the HITS algorithm: Based on thekey idea of HITS algorithm and feature indica-tors, we can apply the HITS algorithm to obtainthe feature relevance ranking.
Features act asauthorities and feature indicators act as hubs.Different from the general HITS algorithm, fea-tures only have authority scores and feature in-dicators only have hub scores in our case.
Theyform a directed bipartite graph, which is illu-strated in Figure 2.
We can run the HITS algo-rithm on this bipartite graph.
The basic idea isthat if a feature candidate has a high authorityscore, it must be a highly-relevant feature.
If afeature indicator has a high hub score, it must bea good feature indicator.Fig.
2 Relations between feature indicators andfeatures3.5 Feature RankingAlthough the HITS algorithm can rank featuresby feature relevance, the final ranking is notonly determined by relevance.
As we discussedbefore, feature frequency is another importantfactor affecting the final ranking.
It is highlydesirable to rank those correct and frequentfeatures at top because they are more importantthan the infrequent ones in opinion mining (oreven other applications).
With this in mind, weput everything together to present the finalalgorithm that we use.
We use two steps:Step 1:  Compute feature score using HITSwithout considering frequency.
Initially, we usethree feature indicators to populate featurecandidates, which form a directed bipartitegraph.
Each feature candidate acts as anauthority node in the graph; each featureindicator acts as a hub node.
For node s in thegraph, we let ??
be the hub score and ??
be theauthority score.
Then, we initialize ??
and ??
to1 for all nodes in the graph.
We update thescores of ??
and ??
until they converge usingpower iteration.
Finally, we normalize ??
andcompute the score S for a feature.Step 2: The final score function consideringthe feature frequency is given in Equation (6).?
?
????????????????
(6)where ???????
is the frequency count ofture?, and S(f) is the authority score of the can-didate feature f. The idea is to push the frequentcandidate features up by multiplying the log offrequency.
Log is taken in order to reduce theeffect of big frequency count numbers.4 ExperimentsThis section evaluates the proposed method.
Wefirst describe the data sets, evaluation metricsand then the experimental results.
We also com-pare our method with the double propagationmethod given in (Qiu et al, 2009).4.1 Data SetsWe used four diverse data sets to evaluate ourtechniques.
They were obtained from a com-mercial company that provides opinion miningservices.
Table 1 shows the domains (based ontheir names) and the number of sentences ineach data set (?Sent.?
means the sentence).
Thedata in ?Cars?
and ?Mattress?
are product re-views extracted from some online review sites.?Phone?
and ?LCD?
are forum discussion postsextracted from some online forum sites.
Wesplit each review/post into sentences and thesentences are POS-tagged using the Brill?s tag-ger (Brill, 1995).
The tagged sentences are theinput to our system.Data  Sets Cars Mattress Phone LCD# of Sent.
2223 13233 15168 1783Table 1.
Experimental data sets4.2 Evaluation MetricsBesides precision and recall, we adopt the pre-cision@N metric for experimental evaluation(Liu, 2006).
It gives the percentage of correctfeatures that are among the top N feature candi-dates in a ranked list.
We compare our method?sresults with those of double propagation whichranks extracted candidates only by occurrencefrequency.4.3 Experimental ResultsWe first compare our results with double propa-Feature Indicators                      Features1468gation on recall and precision for different cor-pus sizes.
The results are presented in Tables 2,3, and 4 for the four data sets.
They show theprecision and recall of 1000, 2000, and 3000sentences from these data sets.
We did not trymore sentences because manually checking therecall and precision becomes prohibitive.
Notethat there are less than 3000 sentences for ?Cars?and ?LCD?
data sets.
Thus, the columns for?Cars?
and ?LCD?
are empty in Table 4.
In theTables, ?DP?
represents the double propagationmethod; ?Ours?
represents our proposed method;?Pr?
represents precision, and ?Re?
representsrecall.Cars Mattress Phone LCDPr Re Pr Re Pr Re Pr ReDP 0.79 0.55 0.79 0.54 0.69 0.23 0.68 0.43Ours 0.78 0.56 0.77 0.64 0.68 0.44 0.66 0.55Table 2.
Results of 1000 sentencesCars Mattress Phone LCDPr Re Pr Re Pr Re Pr ReDP 0.70 0.65 0.70 0.58 0.67 0.42 0.64 0.52Ours 0.66 0.69 0.70 0.66 0.70 0.50 0.62 0.56Table 3.
Results of 2000 sentencesCars Mattress Phone LCDPr Re Pr ReDP 0.65 0.59 0.64 0.48Ours 0.66 0.67 0.62 0.51Table 4.
Results of 3000 sentencesFrom the tables, we can see that for corpora inall domains, our method outperforms doublepropagation on recall with only a small loss inprecision.
In data sets for ?Phone?
and ?Mat-tress?, the precisions are even better.
We alsofind that with the increase of the data size, therecall gap between the two methods becomessmaller gradually and the precisions of both me-thods also drop.
However, in this case, featureranking plays an important role in discoveringimportant features.Ranking comparison between the two me-thods is shown in Tables 5, 6, and 7, which givethe precisions of top 50, 100 and 200 resultsrespectively.
Note that the experiments reportedin these tables were run on the whole data sets.There were no more results for the ?LCD?
databeyond top 200 as there were only a limitednumber of features discussed in the data.
So thecolumn for ?LCD?
in Table 7 is empty.
We rankthe extracted feature candidates based on fre-quency for the double propagation method (DP).Using occurrence frequency is the natural wayto rank features.
The more frequent a featureoccurs in a corpus, the more important it is.However, frequency-based ranking assumes theextracted candidates are correct features.
Thetables show that our proposed method (Ours)outperforms double propagation considerably.The reason is that some highly-frequent featurecandidates extracted by double propagation arenot correct features.
Our method considers thefeature relevance as an important factor.
So itproduces much better rankings.Cars Mattress Phone LCDDP 0.84 0.81 0.64 0.68Ours 0.94 0.90 0.76 0.76Table 5.
Precision at top 50Cars Mattress Phone LCDDP      0.82      0.80      0.65      0.68Ours      0.88      0.85      0.75      0.73Table 6.
Precision at top 100Cars Mattress Phone LCDDP      0.75      0.71      0.70Ours      0.80      0.79      0.76Table 7.
Precision at top 2005 ConclusionFeature extraction for entities is an importanttask for opinion mining.
The paper proposed anew method to deal with the problems of thestate-of-the-art double propagation method forfeature extraction.
It first uses part-whole and?no?
patterns to increase recall.
It then ranks theextracted feature candidates by feature impor-tance, which is determined by two factors: fea-ture relevance and feature frequency.
The Webpage ranking algorithm HITS was applying tocompute feature relevance.
Experimental resultsusing diverse real-life datasets show promisingresults.
In our future work, apart from improv-ing the current methods, we also plan to studythe problem of extracting features that are verbsor verb phrases.AcknowledgementThis work was funded by a HP Labs InnovationResearch Program Award (CW165044).1469ReferencesBlair-Goldensohn, Sasha., Kerry, Hannan., Ryan,McDonald., Tyler, Neylon., George A. Reis, Jeff,Reyna.
2008.
Building Sentiment Summarizer forLocal Service Reviews In Proceedings of theWorkshop of NLPIX .
WWW, 2008Brill, Eric.
1995.
Transformation-Based Error-Driven Learning and Natural LanguageProcessing: a case study in part of speech tagging.Computational Linguistics, 1995.Chieu, Hai Leong and Hwee-Tou Ng.
2002.
NameEntity Recognition: a Maximum Entropy Ap-proach Using Global Information.
In Proceedingsof the 6th Workshop on Very Large Corpora,2002.Ding, Xiaowen., Bing Liu and Philip S. Yu.
2008.
AHolistic Lexicon-Based Approach to OpinionMining  In Proceedings of WSDM 2008.Girju, Roxana., Adriana Badulescu and Dan Moldo-van.
2006.
?Automatic Discovery of Part-WholeRelations?
Computational Linguistics ,32(1):83-135 2006Hu, Mingqin and Bing Liu.
2004.
Mining and Sum-marizing Customer Reviews.
In Proceedings ofKDD 2004Kleinberg, Jon.
1999.
?Authoritative sources inhyperlinked environment?
Journal of the ACM 46(5): 604-632 1999Kobayashi, Nozomi., Kentaro Inui and Yuji Matsu-moto.
2007 Extracting Aspect-Evaluation and As-pect-of Relations in Opinion Mining.
In Proceed-ings of EMNLP, 2007.Lafferty, John., Andrew McCallum and FernandoPereira.
2001 Conditional Random Fields: Proba-bilistic Models for Segmenting and Labeling Se-quence Data.
In Proceedings of ICML, 2001.Lin, Dekang.
1998.
Dependency-based evaluation ofMINIPAR.
In Proceedings of the Workshop onEvaluation of Parsing System at ICLRE 1998.Liu, Bing.
2006.
Web Data Mining: ExploringHyperlinks, contents and usage data.
Springer,2006.Liu, Bing.
2010.
Sentiment analysis and subjectivity.Handbook of Natural Language Processing,second edition, 2010.Mei, Qiaozhu, Ling Xu, Matthew Wondra, Hang Suand ChengXiang Zhai.
2007.
Topic SentimentMixture: Modeling Facets and Opinions in Web-logs.
In Proceedings of WWW, pages 171?180,2007.Pang, Bo., Lillian Lee.
2008.
Opinion Mining andSentiment Analysis.
Foundations and Trends inInformation Retrieval  pp.
1-135 2008Pantel, Patrick., Eric Crestan, Arkady Borkovsky,Ana-Maria  Popescu, Vishunu Vyas.
2009.
Web-Scale Distributional Similarity and Entity Set Ex-pansion.
In Proceedings of.
EMNLP, 2009Popescu, Ana-Maria and Oren, Etzioni.
2005.
Ex-tracting product features and opinions from re-views.
In Proceedings of EMNLP, 2005.Qiu, Guang., Bing, Liu., Jiajun Bu and Chun Chen.2009.
Expanding Domain Sentiment Lexiconthrough Double Propagation.
In Proceedings ofIJCAI 2009.Rabiner, Lawrenence.
1989.
A Tutorial on HiddenMarkov Models and Selected Applications inSpeech Recognition.
In Proceedings of the IEEE,77(2), 1989.Riloff, Ellen.
1993.
Automatically Constructing aDictionary for Information Extraction Tasks.
InProceedings of AAAI 1993.Scaffidi, Christopher., Kevin Bierhoff, Eric Chang,Mikhael Felker, Herman Ng and Chun Jin.
2007.Red opal: Product-feature Scoring from Reviews.In Proceedings of EC 2007Stoyanov, Veselin and Claire Cardie.
2008.
TopicIdentification for Fine-grained Opinion Analysis.In Proceedings of COLING 2008Su, Qi., Xinying Xu., Honglei Guo, Zhili Guo, XianWu, Xiaoxun Zhang, Bin Swen and Zhong Su.2008.
Hidden Sentiment Association in ChineseWeb Opinion Mining.
In Proceedings of WWW2008.Wang, Bo., Houfeng Wang.
2008.
Bootstrappingboth Product Features and Opinion Words fromChinese Customer Reviews with Cross-InducingIn Proceedings of IJCNLP 2008Wilson, Theresa., Janyce Wiebe and Paul Hoffmann.2005.
Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis.
In Proceedings ofHLT/EMNLP 2005Wong, Tak-Lam., Wai Lam and Tik-Sun Wong.2008.
An Unsupervised Framework for Extractingand Normalizing Product Attributes from MultipleWeb Sites In Proceedings of SIGIR 2008Zhuang, Li., Feng Jing, Xiao-yan Zhu.
2006.
MovieReview Mining and Summarization.
In Proceed-ings of CIKM 20061470
