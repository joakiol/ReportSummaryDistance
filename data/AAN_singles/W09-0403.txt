Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 33?36,Athens, Greece, 30 March ?
31 March 2009. c?2009 Association for Computational LinguisticsA Simple Automatic MT Evaluation MetricPetr HomolaCharles UniversityPrague, Czech RepublicVladislav Kubon?Charles UniversityPrague, Czech Republic{homola|vk|pecina}@ufal.mff.cuni.czPavel PecinaCharles UniversityPrague, Czech RepublicAbstractThis paper describes a simple evaluationmetric for MT which attempts to overcomethe well-known deficits of the standardBLEU metric from a slightly different an-gle.
It employes Levenshtein?s edit dis-tance for establishing alignment betweenthe MT output and the reference transla-tion in order to reflect the morphologicalproperties of highly inflected languages.
Italso incorporates a very simple measureexpressing the differences in the word or-der.
The paper also includes evaluation onthe data from the previous SMT workshopfor several language pairs.1 IntroductionThe problem of finding a reliable machine trans-lation metrics corresponding with a human judg-ment has recently returned to the centre of atten-tion.
After a brief period following the introduc-tion of generally accepted and widely used met-rics, BLEU (Papineni et al, 2002) and NIST (Dod-dington, 2002), when it seemed that this persistentproblem has finally been solved, the researchersactive in the field of machine translation (MT)started to express their worries that although thesemetrics are simple, fast and able to provide con-sistent results for a particular system during its de-velopment, they are not sufficiently reliable for thecomparison of different systems or different lan-guage pairs.The results of the NIST evaluation in 2005(Le and Przybocki, 2005) have also strengthenedthe suspicion that the correlation between humanjudgment and the BLEU and NIST measures is notas strong as it was widely believed.
Both mea-sures seem to favor the MT output created by sys-tems based on n-gram architecture, they are un-able to take into account certain factors which arevery important for the human judges of translationquality.The article (Callison-Burch et al, 2006) thor-oughly discusses the deficits of the BLEU andsimilar metrics.
The authors claim that the existingautomatic metrics, including some of the new andseemingly more reliable ones as e.g.
Meteor (cf.
(Banerjee and Lavie, 2005)) ?.
.
.
they are all quiterough measures of translation similarity, and haveinexact models of allowable variation in transla-tion.?
This claim is supported by a construction oftranslation variations which have identical BLEUscore, but which are very different for a humanjudge.
The authors identify three prominent fac-tors which contribute to the inadequacy of BLEU ?the failure to deal with synonyms and paraphrases,no penalties for missing content, and the crudenessof the brevity penalty.Let us add some more factors based on our ex-periments with languages typologically differentthan English, Arabic or Chinese, which are prob-ably the languages most frequently used in recentshared-task MT evaluations.
The highly inflectedlanguages and languages with a higher degree ofword-order freedom may provide additional ex-amples of sentences in which relatively small al-terations of correct word forms may have a direeffect on the BLEU score while the sentence stillremains understandable and acceptable for humanevaluators.The effect of rich inflection has been observedfor example in (Ty?novsky?, 2007), where the au-thor mentions the fact that the BLEU score usedfor measuring the improvements in his experimen-tal Czech-German EBMT system penalized heav-ily all subtle errors in Czech morphology arisingfrom an out-of-context combined partial transla-tions taken from different examples.The problem of the insensitivity of BLEU to thevariations of the order of n-grams identified in ref-erence translations has already been mentioned in33the paper (Callison-Burch et al, 2006).
The au-thors showed examples where changing a goodword order into an unacceptable one did not af-fect the BLEU score.
We may add a different ex-ample documenting the phenomenon that a pairof syntactically correct Czech sentences with thesame word forms, differing only in the word orderwhose n-gram score for n = 2, 3, and 4 differsgreatly.
Let us take one of the sentences from the2008 SMT workshop and its reference translation:When Caligula appointed his horse to the Sen-ate, the horse at least did not have blood on itshoofs.
?
Kdyz?
Caligula zvolil do sena?tu sve?hokone?, neme?l jeho ku?n?
aspon?
na kopytech krev.If we modify the Czech reference sentence intoKdyz?
sve?ho kone?
do sena?tu zvolil Caligula, jehoku?n?
aspon?
neme?l na kopytech krev., we destroy 8out of 15 bigrams, 11 out of 14 trigrams and 12out of 13 quadrigrams while we still have sentencewith almost identical meaning and probably verysimilar human evaluation.
The BLEU score of themodified sentence is, however, lower than it wouldbe for the identical copy of the reference transla-tion.2 The description of the proposed metricThere is one aspect of the problem of a MTquality metric which tends to be overlooked butwhich is very important from the practical pointof view.
This aspect concerns the expected diffi-culties when post-editing the MT output.
It is veryimportant for everybody who really wants to usethe MT output and who faces the decision whetherit is better to post-edit the MT output or whether anew translation made by human translators wouldbe faster and more efficient way towards the de-sired quality.
It is no wonder that such a met-ric is mentioned only in connection with systemswhich really aim at practical exploitation, not witha majority of experimental MT system which willhardly ever reach the stage of industrial exploita-tion.We have described one example of such practi-cally oriented metric in (Hajic?
et al, 2003).
Themetric exploits the matching algorithm of TradosTranslator?s Workbench for obtaining the percent-age of differences between the MT output and thereference translation (created by post-editing theMT output).
The advantage of this measure is itsclose connection to the real world of human trans-lating by means of translation memory, the disad-vantage concerns the use of a proprietary match-ing algorithm which has not been made public andwhich requires the actual use of the Trados soft-ware.Nevertheless, the matching algorithm of Tradosgives results which to a great extent correspondto a much simpler traditional metric, to the Lev-enshtein?s edit distance.
The use of this metricmay help to refine a very strict treatment of word-form differences by BLEU.
A similar approach atthe level of unigram matching has been used bythe well-known METEOR metric (Agarwal andLavie, 2008), which proved its qualities during theprevious MT evaluation task in 2008 (Callison-Burch et al, 2008).
Meteor uses Porter stemmeras one step in the word alignment algorithm.
Italso relies on synonymy relations in WordNet.When designing our metric, we have decided tofollow two general strategies ?
to use as simplemeans as possible and to avoid using any languagedependent tools or resources.
Levenshtein metric(or its modification for word-level edit distance)therefore seemed to be the best candidate for sev-eral aspects of the proposed measure.The first aspect we have decided to include wasthe inflection.
The edit distance has one advan-tage over the language independent stemmer ?
itcan uniformly handle the differences regardless oftheir position in the string.
The stemmer will prob-ably face certain problems with changes inside thestem as e.g.
in the Czech equivalent of the wordhouse in different cases du?m (nom.sg) ?
domu(gen., dat.
or loc.
sg.)
or German Mann in differ-ent numbers der Mann (sg.)
?
die Ma?nner (pl.
),while the edit distance will treat them uniformlywith the variation of prefixes, suffixes and infixes.As mentioned above, we have also intended toaim at the treatment of the free word order in ourmetric.
However this seems to be one of the ma-jor flaws of the BLEU score, it turned out that theword order is extremely difficult if we stick to theuse of simple and language independent means.
Ifwe take Czech as an example of a language withrelatively high degree of word-order freedom, wecan still find certain restrictions (e.g.
the sentence-second position of clitics, their mutual order, theadjectives typically, but not always preceding thenouns they depend upon etc.)
which will defi-nitely influence the human judgment of the accept-ability of a particular sentence.
These restrictionsare language dependent (for example Polish, the34language very closely related to Czech, has dif-ferent rules for congruent attributes, the adjectivesstand much more often to the right of the govern-ing noun) and they are also very difficult to capturealgorithmically.
If the MT output is compared toa single reference translation only, there is, in fact,no way how the metric could account for the pos-sible correct variations of the word order withoutexploiting very deep language dependent informa-tion.
If there are more reference translations, it ispossible that they will provide the natural varia-tions of the word order, but it, in fact, means thatif we want to stick to the above mentioned require-ments, we have to give up the hope that our metricwill capture this important phenomenon.2.1 Word alignment algorithmIn order to capture the word form variationscaused by the inflection, we have decided to em-ploy the following alignment algorithm at the levelof individual word forms.
Let us use the follow-ing notation: Let the reference translation R be asequence of words ri, where i ?< 1, .
.
.
, n >.Let the MT output T be a sequence of words tj,where j ?< 1, .
.
.
,m >.
Let us also set a thresh-old of similarity s ?< 0, 1 >.
(s roughly ex-presses how different the forms of a lemma maybe.
The idea behind this criterion is that a mistakein one morphological category (reflected mostlyby a different ending of the corresponding wordform) is not as serious as a completely differentlexeme.
This holds especially for morphologicallyrich languages that can have tens or even hun-dreds of distinct word forms for a single lemma.
)Starting from t1, let us find for each tj the bestri for i ?< 1, .
.
.
, n > such that the edit dis-tance dj from tj to ri normalized by the lengthof tj is minimal and at the same time dj < s.If the ri is already aligned to some tk, k < jand the edit distance dk > dj , then align tj tori and re-calculate the alignment for tk to its sec-ond best candidate, otherwise take the second bestcandidate rl conforming with the above mentionedconditions and align it to tj .
As a result of thisprocess, we get the alignment score ATR from Tto R. ATR =?
(1?di)m (for i ?< 1, .
.
.
, n >)where di = 1 for those word forms ti which arenot aligned to any of the word forms rj from R.Then we calculate the alignment score ART usingthe same algorithm and aligning the words from Rto T. The similarity score S equals the minimumfrom ATR and ART .
The way how the similar-ity score S is constructed ensures that the scoretakes into account a difference in length betweenT and R, therefore it is not necessary to includeany brevity penalty into the metric.2.2 A structural metricIn order to express word-order difference betweenthe MT output and the reference translation wehave designed a structural part of the metric.
Itis based on an algorithm similar to one of the stan-dard sorting methods, an insert sort.
The refer-ence translation R represents the desired word or-der and the algorithm counts the number of op-erations necessary for obtaining the correct wordorder from the word order of the MT output T byinserting the words ti to their desired positions rj(ti is aligned to rj).
If a particular word ti is notaligned to any rj , a penalty of 1 is added to thenumber of operations.2.3 A combination of both metricsThe overall score is computed as a weighted aver-age of both metrics mentioned above.
Let L be thelexical similarity score and M the structural scorebased on a word mapping.
Then then overall scoreS can be obtained as follows:S = aL+ bMThe coefficients a and b must sum up to one.They allow to capture the difference in the degreeof word-order freedom among target languages.The coefficient b should be set lower for the tar-get languages with more free word-order.
Becauseboth then partial measures L andM have values inthe interval < 0, 1 >, the value of S will also fallinto this interval.3 The experimentWe have performed a test of the proposed met-ric using the data from the last year?s SMT work-shop.1 The parameters a, b, and s have been set tothe same value for all evaluated language pairs, nolanguage dependent alterations were tested in thisexperiment:Parameter Values 0.15a 0.9b 0.11The data are available at http://www.statmt.org/wmt08.35The values for the parameters have been set upempirically with special attention being paid toCzech, the only language with really rich inflec-tion among the languages being tested.We have performed sentence-level and system-level evaluation using the Spearman?s rank corre-lation coefficient which is defined as follows:?
= 1?6?d2in(n2 ?
1)where di = xi?yi is the difference between theranks of corresponding values Xi and Yi and n isthe number of values in each data set.The following scores express the correlation ofour automatic metric and the human judgementsfor the language pairs English-Czech and English-German.
The sentence-level correlation ?sent isthe average of Spearman?s ?
across all sentences.Language pair Metric ?sent ?sysEnglish-Czech proposed 0.20 0.50English-Czech BLEU 0.21 0.50English-German proposed 0.91 0.37English-German BLEU 0.90 0.203.1 ConclusionsThe metric presented in this paper attempts tocombine some of the important factors whichseem to be neglected by some generally acceptedMT evaluation metrics.
Inspired by the fact thathuman judges tend to accept incorrect word-formsof corectly translated lemmas, it employs a simi-larity measure relaxing the requirements on iden-tity (or similarity) of matching word forms in theMT output and the reference translation.
At thesame time, it also incorporates a penalty for dif-ferent length of the MT output and the referencetranslation.
The second component of the metrictackles the problem of incorrect word-order.
Theconstants used in the metric allow to set the weightof its two components with regard to the target lan-guage properties.The experiments performed on the data fromthe previous shared evaluation task are promising.They indicate that the first component of the met-ric succesfully replaces the strict unigram mea-sure used in BLEU while the second componentmay require certain alteration in order to achieve ahigher correlation with human judgement.AcknowledgmentsThe presented research has been supported by thegrant No.
1ET100300517 of the GAAV C?R andby Ministry of Education of the Czech Republic,project MSM 0021620838.ReferencesAbhaya Agarwal and Alon Lavie.
2008.
Meteor,M-BLEU and M-TER: Evaluation metrics for highcorrelation with human rankings of machine trans-lation output.
In Proceedings of the Third Work-shop on Statistical Machine Translation, pages 115-118.
Columbus, Ohio, Association for Computa-tional Linguistics.Satanjeev Banerjee and Alon Lavie.
2005.
Meteor: Anautomatic metric for MT evaluation with improvedcorrelation with human judgments..
In Workshopon Intrinsic and Extrinsic Evaluation Measures forMT and/or Summarization, Ann Arbor, Michigan.Chris Callison-Burch, Miles Osborne, Philipp Koehn.2006.
Re-evaluating the Role of BLEU in Ma-chine Translation Research..
In Proceedings of theEACL?06, Trento, Italy.Chris Callison-Burch, Cameron Fordyce, PhilippKoehn, Christof Monz, Josh Schroeder.
2008.Further Meta-Evaluation of Machine Translation..In Proceedings of the Third Workshop on Statisti-cal Machine Translation, pages 70-106, Columbus,Ohio.
Association for Computational Linguistics.George Doddington.
2002.
Automatic evaluationof machine translation quality using n-gram co-occurrence statistics.
In Proceedings of the secondinternational conference on Human Language Tech-nology Research, San Diego, California,USAJan Hajic?, Petr Homola, Vladislav Kubon?.
2003.
ASimple Multilingual Machine Translation System..In Proceedings of the MT Summit IX, New Orleans,USA.Kishore Papineni, Salim Roukos, ToddWard, and Wei-Jing Zhu.
2002.
Bleu: A method for automatic eval-uation of machine translation..
In Proceedings ofACL 2002.Audrey Le and Mark Przybocki.
2005.
NIST 2005machine translation evaluation official results.. Of-ficial release of automatic evaluation scores for allsubmissions.Miroslav Ty?novsky?.
2007.
Exploitation of Linguis-tic Information in EBMT.. Master thesis at CharlesUniversity in Prague, Faculty of Mathematics andPhysics.36
