Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 710?720,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsUsing Conceptual Class Attributes to Characterize Social Media UsersShane Bergsma and Benjamin Van DurmeDepartment of Computer Science and Human Language Technology Center of ExcellenceJohns Hopkins UniversityBaltimore, MD 21218, USAAbstractWe describe a novel approach for automat-ically predicting the hidden demographicproperties of social media users.
Buildingon prior work in common-sense knowl-edge acquisition from third-person text,we first learn the distinguishing attributesof certain classes of people.
For exam-ple, we learn that people in the Femaleclass tend to have maiden names and en-gagement rings.
We then show that thisknowledge can be used in the analysis offirst-person communication; knowledge ofdistinguishing attributes allows us to bothclassify users and to bootstrap new train-ing examples.
Our novel approach enablessubstantial improvements on the widely-studied task of user gender prediction, ob-taining a 20% relative error reduction overthe current state-of-the-art.1 IntroductionThere has been growing interest in characteriz-ing social media users based on the content theygenerate; that is, automatically labeling users withdemographic categories such as age and gender(Burger and Henderson, 2006; Schler et al, 2006;Rao et al, 2010; Mukherjee and Liu, 2010; Pen-nacchiotti and Popescu, 2011; Burger et al, 2011;Van Durme, 2012).
Automatic user character-ization has applications in targeted advertisingand personalization, and could also lead to finer-grained assessment of public opinion (O?Connoret al, 2010) and health (Paul and Dredze, 2011).Consider the following tweet and suppose wewish to predict the user?s gender:Dirac was one of my boyhood heroes.I?m glad I met him once.
RT Paul Diracimage by artist Eric Handy: http:...State-of-the-art approaches cast this problem as aclassification task and train classifiers using super-vised learning (Section 2).
The features of theclassifier are indicators of specific words in theuser-generated text.
While a human would as-sume that someone with boyhood heroes is male,a standard classifier has no way of exploiting suchknowledge unless the phrase occurs in trainingdata.
We present an algorithm that improves usercharacterization by collecting and exploiting suchcommon-sense knowledge.Our work is inspired by algorithms that pro-cesses large text corpora in order to discover theattributes of semantic classes, e.g.
(Berland andCharniak, 1999; Schubert, 2002; Almuhareb andPoesio, 2004; Tokunaga et al, 2005; Girju et al,2006; Pas?ca and Van Durme, 2008; Alfonseca etal., 2010).
We learn the distinguishing attributesof different demographic groups (Section 3), andthen automatically assign users to these groupswhenever they refer to a distinguishing attribute intheir writings (Section 4).
Our approach obviatesthe need for expensive annotation efforts, and al-lows us to rapidly bootstrap training data for newclassification tasks.We validate our approach by advancing thestate-of-the-art on the most well-studied user clas-sification task: predicting user gender (Section 5).Our bootstrapped system, trained purely fromautomatically-annotated Twitter data, significantlyreduces error over a state-of-the-art system trainedon thousands of gold-standard training examples.2 Supervised User CharacterizationThe current state-of-the-art in user characteriza-tion is to use supervised classifiers trained on an-notated data.
For each instance to be classified, theoutput is a decision about a distinct demographicproperty, such as Male/Female or Over/Under-18.A variety of classification algorithms have beenemployed, including SVMs (Rao et al, 2010), de-710cision trees (Pennacchiotti and Popescu, 2011), lo-gistic regression (Van Durme, 2012), and the Win-now algorithm (Burger et al, 2011).Content Features: BoW Prior classifiers use aset of features encoding the presence of specificwords in the user-generated text.
We call thesefeatures BoW features as they encode the stan-dard Bag-of-Words representation which has beenhighly effective in text categorization and informa-tion retrieval (Sebastiani, 2002).User-Profile Features: Usr Some researchershave explored features for user-profile meta-information in addition to user content.
This mayinclude the user?s communication behavior andnetwork of contacts (Rao et al, 2010), their fullname (Burger et al, 2011) and whether they pro-vide a profile picture (Pennacchiotti and Popescu,2011).
We focus on the case where we onlyhave access to the user?s screen-name (a.k.a.
user-name).
Using a combination of content and user-name features ?represents a use case common tomany different social media sites, such as chatrooms and news article comment streams?
(Burgeret al, 2011).
We refer to features derived from ausername as Usr features in our experiments.3 Learning Class AttributesWe aim to improve the automated classificationof users into various demographic categories bylearning and applying the distinguishing attributesof those categories, e.g.
that males have boyhoodheroes.
Our approach builds on lexical-semanticresearch on the topic of class-attribute extraction.In this research, the objective is to discover vari-ous attributes or parts of classes of entities.
Forexample, Berland and Charniak (1999) learn thatthe class car has parts such as headlight, wind-shield, dashboard, etc.
Berland and Charniak ex-tract these attributes by mining a corpus for fillersof patterns such as ?car?s X?
or ?X of a car?.
Notetheir patterns explicitly include the class itself(car).
Another approach is to use patterns that arebased on instances (i.e.
hyponyms or sub-classes)of the class.
For example, Pas?ca and Van Durme(2007) learn the attributes of the class car via pat-terns involving instances of cars, e.g.
ChevroletCorvette?s X and X of a Honda Civic.
For these ap-proaches, lists of instances are typically collectedfrom publicly-available resources such as Word-Net or Wikipedia (Pas?ca and Van Durme, 2007;Van Durme et al, 2008), acquired automaticallyfrom corpora (Pas?ca and Van Durme, 2008; Al-fonseca et al, 2010), or simply specified by hand(Schubert, 2002).Creation of Instance Lists We use an instance-based approach; our instances are derived fromcollections of common nouns that are associatedwith roles and occupations of people.
For thegender task that we study in our experiments, weacquire class instances by filtering the dataset ofnouns and their genders created by Bergsma andLin (2006).
This dataset indicates how often anoun is referenced by a male, female, neutral orplural pronoun.
We extract prevalent commonnouns for males and females by selecting onlythose nouns that (a) occur more than 200 timesin the dataset, (b) mostly occur with male or fe-male pronouns, and (c) occur as lower-case moreoften than upper-case in a web-scale N-gram cor-pus (Lin et al, 2010).
We then classify a noun asMale (resp.
Female) if the noun is indicated tooccur with male (resp.
female) pronouns at least85% of the time.
Since the gender data is noisy,we also quickly pruned by hand any instances thatwere malformed or obviously incorrectly assignedby our automatic process.
This results in 652 in-stances in total.
Table 1 provides some examples.Male: bouncer, altar boy, army officer, dictator,assailant, cameraman, drifter, chauffeur, bad guyFemale: young lady, lesbian, ballerina, waitress,granny, chairwoman, heiress, soprano, socialiteTable 1: Example instances used for extraction ofclass attributes for the gender classification taskAttribute Extraction We next collect and rankattributes for each class.
We first look for fillers ofattribute-patterns involving each of the instances.Let I represent an instance of one of our classes.We find fillers of the single high-precision pattern:{word=I ,tag=NN}| {z }instance{word=?s}| {z }?s[{word=.
*}* {tag=N.
*}]| {z }attribute(E.g.
dictator ?s [former mistress]).
The expres-sion ?tag=NN?
means that I must be tagged asa noun.
The expression in square brackets is thefiller, i.e.
the extracted attribute, A.
The notation?{word=.
*}* tag=N.*?
means that A can be anysequence of tokens ending in a noun.
We use an711equivalent pattern when I is multi-token.
The out-put of this process is a set of (I ,A) pairs.In attribute extraction, typically one mustchoose between the precise results of rich pat-terns (involving punctuation and parts-of-speech)applied to small corpora (Berland and Charniak,1999) and the high-coverage results of superficialpatterns applied to web-scale data, e.g.
via theGoogle API (Almuhareb and Poesio, 2004).
Weobtain the best of both worlds by matching ourprecise pattern against a version of the Google N-gram Corpus that includes the part-of-speech tagdistributions for every N-gram (Lin et al, 2010).We found that applying this pattern to web-scaledata is effective in extracting useful attributes.
Weacquired around 20,000 attributes in total.Finding Distinguishing Attributes Unlikeprior work, we aim to find distinguishing proper-ties of each class; that is, the kinds of propertiesthat uniquely distinguish a particular category.Prior work has mostly focused on finding ?rel-evant?
attributes (Alfonseca et al, 2010) or?correct?
parts (Berland and Charniak, 1999).
Aleg is a relevant and correct part of both a male anda female (and many other living and inanimateobjects), but it does not help us distinguish malesfrom females in social media.
We therefore rankour attributes for each class by their strength ofassociation with instances of that specific class.1To calculate the association, we first disregardthe count of each (I ,A) pair and consider eachunique pair to be a single probabilistic event.We then convert the (I ,A) pairs to corresponding(C,A) pairs by replacing I with the correspondingclass, C. We then calculate the pointwise mutualinformation (Church and Hanks, 1990) betweeneach C and A over the set of events:PMI(C,A) = log p(C,A)p(C)p(A) (1)If the PMI>0, the observed probability of a classand attribute co-occurring is greater than the prob-ability of co-occurrence that we would expect if Cand A were independently distributed.
For eachclass, we rank the attributes by their PMI scores.1Reisinger and Pas?ca (2009) considered the related prob-lem of finding the most appropriate class for each attribute;they take an existing ontology of concepts (WordNet) as aclass hierarchy and use a Bayesian approach to decide ?thecorrect level of abstraction for each attribute.
?Filtering Attributes We experimented with twodifferent methods to select a final set of distin-guishing attributes for each class: (1) we useda threshold to select the top-ranked attributes foreach class, and (2) we manually filtered the at-tributes.
For the gender classification task, wemanually filtered the entire set of attributes to se-lect around 1000 attributes that were judged to bediscriminative (two thirds of which are female).This filtering took one annotator only a few hoursto complete.
Because this process was so trivial,we did not invest in developing annotation guide-lines or measuring inter-annotator agreement.
Wemake these filter attributes available online as anattachment to this article, available through theACL Anthology.Ultimately, we discovered that manual filter-ing was necessary to avoid certain pathologicalcases in our Twitter data.
For example, our PMIscoring finds homepage to be strongly associatedwith males.
In our gold-standard gender data(Section 5), however, every user has a home-page [by dataset construction]; we might there-fore incorrectly classify every user as Male.
Weagree with Richardson et al (1998) that ?auto-matic procedures ... provide the only credibleprospect for acquiring world knowledge on thescale needed to support common-sense reasoning?but ?hand vetting?
might be needed to ensure ?ac-curacy and consistency in production level sys-tems.?
Since our approach requires manual in-volvement in the filtering of the attribute list, onemight argue that one should simply manually enu-merate the most relevant attributes directly.
How-ever, the manual generation of conceptual featuresby a single researcher results in substantial vari-ability both across and within participants (McRaeet al, 2005).
Psychologists therefore generatesuch lists by pooling the responses across manyparticipants: future work may compare our ?auto-matically generate, manually prune?
approach tosoliciting attributes via crowdsourcing.2Table 2 gives examples of our extracted at-2One can also view the work of manually filtering at-tributes as a kind of ?feature labeling.?
There is evidencefrom Zaidan et al (2007) that a few hours of feature labelingcan be more productive than annotating new training exam-ples.
In fact, since Zaidan et al (2007) label features at thetoken level (e.g., in our case one would highlight ?handbag?in a given tweet), while we label features at the type level(e.g., deciding whether to mark the word ?handbag?
as fem-inine in general), our process is likely even more efficient.Future work may also wish to consider this connection to so-called ?annotator rationales?
more deeply.712Male: wife, widow, wives, ex-girlfriend, erec-tion, testicles, wet dream, bride, buddies, ex-wife, first-wife, penis, death sentence, manhoodFemale: vagina, womb, maiden name, dresses,clitoris, wedding dress, uterus, shawl, necklace,ex-husband, ex-boyfriend, dowry, nightgownTable 2: Example attributes for gender classes, indescending order of class-association scoretributes.
Our approach captures many multi-tokenattributes; these are often distinguishing eventhough the head noun is ambiguous (e.g.
nameis ambiguous, maiden name is not).
Our attributesalso go beyond the traditional meronyms that werethe target of earlier work.
As we discuss furtherin Related Work (Section 7), previous researchershave worried about a proper definition of parts orattributes and relied on human judgments for eval-uation (Berland and Charniak, 1999; Girju et al,2006; Van Durme et al, 2008).
For us, whethera property such as dowry should be consideredan ?attribute?
of the class Female is immaterial;we echo Almuhareb and Poesio (2004) who (on adifferent task) noted that ?while the notion of ?at-tribute?
is not completely clear... our results sug-gest that trying to identify attributes is beneficial.
?4 Applying Class AttributesTo classify users using the extracted attributes, welook for cases where users refer to such attributesin their first-person writings.
We performed a pre-liminary analysis of a two-week sample of tweetsfrom the TREC Tweets2011 Corpus.3 We foundthat users most often reveal their attributes in thepossessive construction, ?my X?
where X is an at-tribute, quality or event that they possess (in a lin-guistic sense).
For example, we found over 1000tweets with the phrase ?my wife.?
In contrast, ?Ihave a wife?
occurs only 5 times.4We therefore assign a user to a demographiccategory as follows: We first part-of-speech tagour data using CRFTagger (Phan, 2006) and thenlook for ?my X?
patterns where X is a sequenceof tokens terminating in a noun, analogous to our3http://trec.nist.gov/data/tweets/ This corpus was de-veloped for the TREC Microblog track (Soboroff et al, 2012).4Note that ?I am a man?
occurs only 20 times.
Usersalso reveal their names in ?my name is X?
patterns in severalhundred tweets, but this is small compared to cases of self-distinguishing attributes.
Exploiting these alternative pat-terns could nevertheless be a possible future direction.attribute-extraction pattern (Section 3).5 When auser uses such a ?my X?
construction, we matchthe filler X against our attribute lists for eachclass.
If the filler is on a list, we call it a self-distinguishing attribute of a user.
We then applyour knowledge of the self-distinguishing attributeand its corresponding class in one of the followingthree ways:(1) ARules: Using Attribute-Based Rules toOverride a Classifier When human-annotateddata is available for training and testing a su-pervised classifier, we refer to it as gold stan-dard data.
Our first technique provides a sim-ple way to use our identified self-distinguishingattributes in conjunction with a classifier trainedon gold-standard data.
If the user has any self-distinguishing attributes, we assign the user to thecorresponding class; otherwise, we trust the outputof the classifier.
(2) Bootstrapped: Automatic Labeling of Train-ing Examples Even without gold standard train-ing data, we can use our self-distinguishing at-tributes to automatically bootstrap annotations.We collect a large pool of unlabeled users and theirtweets, and we apply the ARules described aboveto label those users that have self-distinguishingattributes.
Once an example is auto-annotated,we delete the self-distinguishing attributes fromthe user?s content.
This prevents the subsequentlearning algorithm from trivially learning the ruleswith which we auto-annotated the data.
Next, theauto-annotated examples are used as training datafor a supervised system.6 Finally, when applyingthe Bootstrapped classifiers, we can still apply theARules as a post-process (although in practice thismade little difference in our final results).
(3) BootStacked: Gold Standard and Boot-strapped Combination Although we show thatan accurate classifier can be trained using auto-annotated Bootstrapped data alone, we also testwhether we can combine this data with any gold-standard training examples to achieve even betterperformance.
We use the following simple but5While we used an ?off the shelf?
POS tagger in thiswork, we note that taggers optimized specifically for socialmedia are now available and would likely have resulted inhigher tagging accuracy (e.g.
Owoputi et al (2013)).6Note that while our target gender task presents mutually-exclusive output classes, we can still train classifiers for othercategories without clear opposites (e.g.
for labeling usersas Parents or Doctors) by using the 1-class classificationparadigm (Koppel and Schler, 2004).713effective method for combining data from thesetwo sources, inspired by prior techniques used inthe domain adaptation literature (Daume?
III andMarcu, 2006).
We first use the trained Boot-strapped system to make predictions on the entireset of gold standard data (gold train, development,and test sets).
We then use these predictions asfeatures in a classifier trained on the gold standarddata.
We refer to this system as the BootStackedsystem in our evaluation.5 Twitter Gender PredictionTo test the use of self-distinguishing attributesin user classification, we apply our methods tothe task of gender classification on Twitter.
Thisis an important and intensely-studied task withinacademia and industry.
Furthermore, for this taskit is possible to semi-automatically acquire largeamounts of ground truth (Burger et al, 2011).We can therefore benchmark our approach againststate-of-the-art supervised systems trained withplentiful gold-standard data, giving us an idea ofhow well our Bootstrapped system might compareto theoretically top-performing systems on othertasks, domains, and social media platforms wheresuch gold-standard training data is not available.Gold Data Our data is derived from the corpuscreated by Burger et al (2011).
Burger et al ob-served that many Twitter users link their Twitterprofile to homepages on popular blogging web-sites.
Since ?many of these [sites] have well-structured profile pages [where users] must se-lect gender and other attributes from dropdownmenus,?
they were able to link these attributes tothe Twitter users.
Using this process, they createda large multi-lingual corpus of Twitter users andgenders.We filter non-English tweets from this corpususing the LID system of Bergsma et al (2012)and also tweets containing URLs (since many ofthese are spam) and re-tweets.
We then filter userswith <40 tweets and randomly divide the remain-ing users into 2282 training, 1140 development,and 1141 test examples.Classifier Set-up We train logistic-regressionclassifiers on this gold standard data via the LI-BLINEAR package (Fan et al, 2008).
We optimizethe classifier?s regularization parameter on devel-opment data and report final results on the held-out test examples.
We also report the results ofour new attribute-based strategies (Section 4) onthe test data.
We report accuracy: the percentageof examples labeled correctly.Our classifiers use both BoW and Usr features(Section 2).
To increase the generality of ourBoW features, we preprocess the text by lower-casing and converting all digits to special ?#?
sym-bols.
We then create real-valued features thatencode the log-count of each word in the input.While Burger et al (2011) found ?no apprecia-ble difference in performance?
when using eitherbinary presence/absence features or encoding thefrequency of the word, we found real-valued fea-tures worked better in development experiments.For the Usr features, we add special beginning andending characters to the username, and then createfeatures for all character n-grams of length two-to-four in the modified username string.
We in-clude n-gram features with the original capitaliza-tion pattern and separate features with the n-gramslower-cased.Unlabeled Data For Bootstrapped training, wealso use a pool of unlabeled Twitter data.
Thispool comprises the union of 2.2 billion tweetsfrom 05/2009 to 10/2010 (O?Connor et al, 2010),1.9 billion tweets collected from 07/2011 to11/2012, and 80 million tweets collected from thefollowers of 10-thousand location and language-specific Twitter feeds.
We filter this corpus asabove, except we do not put any restrictions on thenumber of tweets needed per user.
We also filterany users that overlap with our gold standard data.Bootstrapping Analysis We apply our Boot-strapped auto-annotation strategy to this unlabeleddata, yielding 789,285 auto-annotated examplesof users and their tweets.
The decisions of ourbootstrapping process reflect the true gender dis-tribution; the auto-annotated data is 60.5% Fe-male, remarkably close to the 60.9% proportionin our gold standard test set.
Figure 1 shows thata wide range of self-distinguishing attributes areused in the auto-annotation process.
This is impor-tant because if only a few attributes are used (e.g.wife/husband or penis/vagina), we might system-atically miss a segment of users (e.g.
young peoplethat don?t have husbands or wives, or people thatdon?t frequently talk about their genitalia).
Thus awide range of common-sense knowledge is usefulfor bootstrapping, which is one reason why auto-matic approaches are needed to acquire it.714050000100000150000200000engagement ring ?Note: showing only first 10% of attributes used boyfriend ?hubby ?bra ?
future wife ?natural hair ?jewelry ?bride ?
beard?due date ?wife ?husband ?tux ?purse ?Figure 1: Frequency with which attributes are used to auto-annotate examples in the bootstrapping ap-proach.
The plot identifies some attributes and their corresponding class (labeled via gender symbol).Majority-class baseline 60.9Supervised on 100 examples 72.0Supervised on 2282 examples 84.0Supervised on 100 examples + ARules 74.7Supervised on 2282 examples + ARules 84.7Bootstrapped 86.0BootStacked 87.2Table 3: Classification accuracy (%) on gold stan-dard test data for user gender prediction on Twitter6 ResultsOur main classification results are presented in Ta-ble 3.
The majority-class baseline for this taskis to always choose Female; this achieves an ac-curacy of 60.9%.
A standard classifier trainedon 100 gold-standard training examples improvesover this baseline, to 72.0%, while one with 2282training examples achieves 84.0%.
This latter re-sult represents the current state-of-the-art: a clas-sifier trained on thousands of gold standard exam-ples, making use of both Usr and BoW features.Our performance compares favourably to Burgeret al (2011), who achieved 81.4% using the samefeatures, but on a very different subset of the data(also including tweets in other languages).7Applying the ARules as a post-process signifi-cantly improves performance in both cases (Mc-Nemar?s, p<0.05).
It is also possible to use theARules as a stand-alone system rather than as apost-process, however the coverage is low: we finda distinguishing attribute in 18.3% of the 695 Fe-male instances in the test data, and make the cor-7Note that it is possible to achieve even higher perfor-mance on gender classification in social media if you havefurther information about a user, such as their full first andlast name (Burger et al, 2011; Bergsma et al, 2013).rect decision in 96.9% of these cases.
We find adistinguishing attribute in 11.4% of the 446 Maleinstances, with 86.3% correct decisions.The Bootstrapped system substantially im-proves over the state-of-the-art, achieving 86% ac-curacy and doing so without using any gold stan-dard training data.
This is important because hav-ing thousands of gold standard annotations for ev-ery possible user characterization task, in everydomain and social media platform, is not realis-tic.
Combining the bootstrapped classifier withthe gold standard annotations in the BootStackedmodel results in further gains in performance.8These results provide strong validation for boththe inherent utility of class-attributes knowledge inuser characterization and the effectiveness of ourspecific strategies for exploiting such knowledge.Figure 2 shows the learning curve of the Boot-strapped classifier.
Performance rises consistentlyacross all the auto-annotated training data; thisis encouraging because there is theoretically noreason not to vastly increase the amount of auto-annotated data by collecting an even larger col-lection of tweets.
Finally, note that most of thegains of the Bootstrapped system appear to derivefrom the tweet content itself, i.e.
the BoW fea-tures.
However, the Usr features are also helpfulat most training sizes.We provide some of the top-ranked features ofthe Bootstrapped system in Table 4.
We see thata variety of other common-sense knowledge islearned by the system (e.g., the association be-tween males and urinals, boxers, fatherhood, etc.
),as well as stylistic clues (e.g.
Female users usingbetcha and xox in their writing).
The username8We observed no further gains in accuracy when applyingthe ARules as a post-process on top of these systems.71560657075808590100  1000  10000  100000  1e+06AccuracyNumber of auto-annotated training pts.BoW+UsrBoWUsrFigure 2: Learning curve for Bootstrappedlogistic-regression classifier, with automatically-labeled data, for different feature classes.features capture reasonable associations betweengender classes and particular names (such as mike,tony, omar, etc.)
and also between gender classesand common nouns (such as guy, dad, sir, etc.
).7 Related WorkUser Characterization The field of sociolin-guistics has long been concerned with how variousmorphological, phonological and stylistic aspectsof language can vary with a person?s age, gender,social class, etc.
(Fischer, 1968; Labov, 1972).This early work therefore had an emphasis on ana-lyzing the form of language, as opposed to its con-tent.
This emphasis continued into early machinelearning approaches, which predicted author prop-erties based on the usage of function words, parts-of-speech, punctuation (Koppel et al, 2002) andspelling/grammatical errors (Koppel et al, 2005).Recently, researchers have focused less on thesociolinguistic implications and more on the tasksthemselves, naturally leading to classifiers withfeature representations capturing content in ad-dition to style (Schler et al, 2006; Garera andYarowsky, 2009; Mukherjee and Liu, 2010).
Ourwork represents a logical next step for content-based classification, a step partly suggested bySchler et al (2006) who noted that ?those whoare interested in automatically profiling bloggersfor commercial purposes would be well served byconsidering additional features - which we delib-erately ignore in this study - such as author self-identification.
?Male BoW features: wife, wifey, sucked, shave,boner, boxers, missus, installed, manly, in-laws,brah, urinal, kickoff, golf, comics, ubuntu, homo,nhl, jedi, fatherhood, nigga, movember, algebraMale Usr features: boy, mike, ben, guy, mr, dad,jr, kid, tony, dog, lord, sir, omar, dude, man, bigFemale BoW features: hubby, hubs, jewelry,sewing, mascara, fabulous, bf, softball, betcha,motherhood, perky, cozy, zumba, xox, cuddled,belieber, bridesmaid, anorexic, jammies, padFemale Usr features: mrs, mom, jen, lady, wife,mary, joy, mama, pink, kim, diva, elle, woma, msTable 4: Examples of highly-weighted BoW (con-tent) and Usr (username) features (in descendingorder of weight) in the Bootstrapped system forpredicting user gender in Twitter.Many recent papers have analyzed the lan-guage of social media users, along dimensionssuch as ethnicity (Eisenstein et al, 2011; Rao etal., 2011; Pennacchiotti and Popescu, 2011; Finket al, 2012) time zone (Kiciman, 2010), polit-ical orientation (Rao et al, 2010; Pennacchiottiand Popescu, 2011) and gender (Rao et al, 2010;Burger et al, 2011; Van Durme, 2012).Class-Attribute Extraction The idea of usingsimple patterns to extract useful semantic relationsgoes back to Hearst (1992) who focused on hy-ponyms.
Hearst reports that she ?tried applyingthis technique to meronymy (i.e., the part/wholerelation), but without great success.?
Berland andCharniak (1999) did have success using Hearst-style patterns for part-whole detection, which theyattribute to their ?very large corpus and the use ofmore refined statistical measures for ranking theoutput.?
Girju et al (2006) devised a supervisedclassification scheme for part/whole relation dis-covery that integrates the evidence from multiplepatterns.
These efforts focused exclusively on themeronymy relation as used in WordNet (Miller etal., 1990).
Indeed, Berland and Charniak (1999)attempted to filter out attributes that were regardedas qualities (like driveability) rather than parts(like steering wheels) by removing words end-ing with the suffixes -ness, -ing, and -ity.
In ourwork, such qualities are not filtered and are ulti-mately valuable in classification; for example, theattributes peak fertility and loveliness are highly716associated with females.As subsequent research became more focusedon applications, looser definitions of class at-tributes were adopted.
Almuhareb and Poesio(2004) automatically mined class attributes that in-clude parts, qualities, and those with an ?agen-tive?
or ?telic?
role with the class.
Their ex-tended set of attributes was shown to enable animproved representation of nouns for the purposeof clustering these nouns into semantic concepts.Tokunaga et al (2005) define attributes as prop-erties that can serve as focus words in questionsabout a target class; e.g.
director is an attributeof a movie since one might ask, ?Who is the di-rector of this movie??
Another line of researchhas been motivated by the observation that muchof Internet search consists of people looking forvalues of various class attributes (Bellare et al,2007; Pas?ca and Van Durme, 2007; Pas?ca and VanDurme, 2008; Alfonseca et al, 2010).
By knowingthe attributes of different classes, search enginescan better recognize that queries such as ?altitudeguadalajara?
or ?population guadalajara?
are seek-ing values for a particular city?s ?altitude?
and?population?
attributes (Pas?ca and Van Durme,2007).
Finally, note that Van Durme et al (2008)compared instance-based and class-based patternsfor broad-definition attribute extraction, and foundboth to be effective.Of course, text-mining with custom-designedpatterns is not the only way to extract class-attribute information.
Experts can manually spec-ify the attributes of entities, as in the WordNetproject (Miller et al, 1990).
Others have auto-matically extracted attribute relations from dictio-nary definitions (Richardson et al, 1998), struc-tured online sources such as Wikipedia infoboxes,(Wu and Weld, 2007) and large-scale collectionsof high-quality tabular web data (Cafarella et al,2008).
Attribute extraction has also been viewedas a sub-component or special case of the infor-mation obtained by general-purpose knowledgeextractors (Schubert, 2002; Pantel and Pennac-chiotti, 2006).NLP Applications of Common-Sense Knowl-edge The kind of information derived fromclass-attribute extraction is sometimes referred toas a type of common-sense knowledge.
The needfor computer programs to represent common-sense knowledge has been recognized since thework of McCarthy (1959).
Lenat et al (1990)defines common sense as ?human consensus re-ality knowledge: the facts and concepts that youand I know and which we each assume the otherknows.
?While we are the first to exploit common-sense knowledge in user characterization, com-mon sense has been applied to a range of otherproblems in natural language processing.
In manyways WordNet can be regarded as a collection ofcommon-sense relationships.
WordNet has beenapplied in a myriad of NLP applications, includ-ing in seminal works on semantic-role labeling(Gildea and Jurafsky, 2002), coreference resolu-tion (Soon et al, 2001) and spelling correction(Budanitsky and Hirst, 2006).
Also, many ap-proaches to the task of sentiment analysis ?be-gin with a large lexicon of words marked withtheir prior polarity?
(Wilson et al, 2009).
Likeour class-attribute associations, the common-senseknowledge that the word cool is positive whileunethical is negative can be learned from asso-ciations in web-scale data (Turney, 2002).
Wemight also view information about synonyms orconceptually-similar words as a kind of common-sense knowledge.
In this perspective, our workis related to recent work that has extracteddistributionally-similar words from web-scale dataand applied this knowledge in tasks such asnamed-entity recognition (Lin and Wu, 2009) anddependency parsing (Ta?ckstro?m et al, 2012).8 ConclusionWe have proposed, developed and successfullyevaluated a novel approach to user characteriza-tion based on exploiting knowledge of user classattributes.
The knowledge is obtained using a newalgorithm that discovers distinguishing attributesof particular classes.
Our approach to discoveringdistinguishing attributes represents a significantnew direction for research in class-attribute extrac-tion, and provides a valuable bridge between thefields of user characterization and lexical knowl-edge extraction.We presented three effective techniques forleveraging this knowledge within the frameworkof supervised user characterization: rule-basedpost-processing, a learning-by-bootstrapping ap-proach, and a stacking approach that integrates thepredictions of the bootstrapped system into a sys-tem trained on annotated gold-standard trainingdata.
All techniques lead to significant improve-717ments over state-of-the-art supervised systems onthe task of Twitter gender classification.While our technique has advanced the state-of-the-art on this important task, our approach mayprove even more useful on other tasks where train-ing on thousands of gold-standard examples is noteven an option.
Currently we are exploring theprediction of finer-grained user roles, such as stu-dent, waitress, parent, and so forth, based on ex-tensions to the process laid out here.ReferencesEnrique Alfonseca, Marius Pas?ca, and EnriqueRobledo-Arnuncio.
2010.
Acquisition of instanceattributes via labeled and related instances.
In Proc.SIGIR, pages 58?65.Abdulrahman Almuhareb and Massimo Poesio.
2004.Attribute-based and value-based clustering: Anevaluation.
In Proc.
EMNLP, pages 158?165.Kedar Bellare, Partha P. Talukdar, Giridhar Kumaran,Fernando Pereira, Mark Liberman, Andrew McCal-lum, and Mark Dredze.
2007.
Lightly-SupervisedAttribute Extraction.
In NIPS Workshop on MachineLearning for Web Search.Shane Bergsma and Dekang Lin.
2006.
Bootstrappingpath-based pronoun resolution.
In Proc.
Coling-ACL, pages 33?40.Shane Bergsma, Paul McNamee, Mossaab Bagdouri,Clayton Fink, and Theresa Wilson.
2012.
Languageidentification for creating language-specific Twittercollections.
In Proceedings of the Second Workshopon Language in Social Media, pages 65?74.Shane Bergsma, Mark Dredze, Benjamin VanDurme, Theresa Wilson, and David Yarowsky.2013.
Broadly improving user classification viacommunication-based name and location clusteringon twitter.
In Proc.
NAACL.Matthew Berland and Eugene Charniak.
1999.
Find-ing parts in very large corpora.
In Proc.
ACL, pages57?64.Alexander Budanitsky and Graeme Hirst.
2006.
Eval-uating WordNet-based measures of lexical semanticrelatedness.
Computational Linguistics, 32(1):13?47.John D. Burger and John C. Henderson.
2006.
Anexploration of observable features related to bloggerage.
In Proc.
AAAI Spring Symposium: Computa-tional Approaches to Analyzing Weblogs, pages 15?20.John D. Burger, John Henderson, George Kim, andGuido Zarrella.
2011.
Discriminating gender onTwitter.
In Proc.
EMNLP, pages 1301?1309.Michael J. Cafarella, Alon Y. Halevy, Daisy Zhe Wang,Eugene Wu, and Yang Zhang.
2008.
WebTables:exploring the power of tables on the web.
Proc.PVLDB, 1(1):538?549.Kenneth W. Church and Patrick Hanks.
1990.
Wordassociation norms, mutual information, and lexicog-raphy.
Computational Linguistics, 16(1).Hal Daume?
III and Daniel Marcu.
2006.
Domain adap-tation for statistical classifiers.
Journal of ArtificialIntelligence Research, 26.Jacob Eisenstein, Noah A. Smith, and Eric P. Xing.2011.
Discovering sociolinguistic associations withstructured sparsity.
In Proc.
ACL, pages 1365?1374.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
LIBLINEAR:A library for large linear classification.
J. Mach.Learn.
Res., 9:1871?1874.Clayton Fink, Jonathon Kopecky, Nathan Bos, andMax Thomas.
2012.
Mapping the Twitterverse inthe developing world: An analysis of social mediause in Nigeria.
In Proc.
International Conference onSocial Computing, Behavioral Modeling, and Pre-diction, pages 164?171.John L. Fischer.
1968.
Social influences on the choiceof a linguistic variant.
Word, 14:47?56.Nikesh Garera and David Yarowsky.
2009.
Modelinglatent biographic attributes in conversational genres.In Proc.
ACL-IJCNLP, pages 710?718.Daniel Gildea and Daniel Jurafsky.
2002.
Automaticlabeling of semantic roles.
Computational Linguis-tics, 28:245?288.Roxana Girju, Adriana Badulescu, and Dan Moldovan.2006.
Automatic discovery of part-whole relations.Computational Linguistics, 32(1):83?135.Marti A. Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In Proc.
Coling,pages 539?545.Emre Kiciman.
2010.
Language differences and meta-data features on Twitter.
In Proc.
SIGIR 2010 WebN-gram Workshop, pages 47?51.Moshe Koppel and Jonathan Schler.
2004.
Authorshipverification as a one-class classification problem.
InProc.
ICML, pages 489?495.Moshe Koppel, Shlomo Argamon, and Anat RachelShimoni.
2002.
Automatically categorizing writ-ten texts by author gender.
Literary and LinguisticComputing, 17(4):401?412.Moshe Koppel, Jonathan Schler, and Kfir Zigdon.2005.
Determining an author?s native language bymining a text for errors.
In Proc.
KDD, pages 624?628.718William Labov.
1972.
Sociolinguistic Patterns.
Uni-versity of Pennsylvania Press.Douglas B. Lenat, R. V. Guha, Karen Pittman, Dex-ter Pratt, and Mary Shepherd.
1990.
CYC: towardprograms with common sense.
Commun.
ACM,33(8):30?49.Dekang Lin and Xiaoyun Wu.
2009.
Phrase clusteringfor discriminative learning.
In Proc.
ACL-IJCNLP,pages 1030?1038.Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,David Yarowsky, Shane Bergsma, Kailash Patil,Emily Pitler, Rachel Lathbury, Vikram Rao, KapilDalwani, and Sushant Narsale.
2010.
New tools forweb-scale N-grams.
In Proc.
LREC, pages 2221?2227.John McCarthy.
1959.
Programs with common sense.In Proc.
Teddington Conference on the Mechaniza-tion of Thought Processes, pages 75?91.
London:Her Majesty?s Stationery Office.Ken McRae, George S. Cree, Mark S. Seidenberg, andChris McNorgan.
2005.
Semantic feature pro-duction norms for a large set of living and nonliv-ing things.
Behavior Research Methods, 37(4):547?559.George A. Miller, Richard Beckwith, Christiane Fell-baum, Derek Gross, and Katherine J. Miller.1990.
Introduction to WordNet: an on-line lexicaldatabase.
International Journal of Lexicography,3(4).Arjun Mukherjee and Bing Liu.
2010.
Improving gen-der classification of blog authors.
In Proc.
EMNLP,pages 207?217.Brendan O?Connor, Ramnath Balasubramanyan,Bryan R. Routledge, and Noah A. Smith.
2010.From tweets to polls: Linking text sentiment topublic opinion time series.
In Proc.
ICWSM, pages122?129.Olutobi Owoputi, Brendan O?Connor, Chris Dyer,Kevin Gimpel, Nathan Schneider, and Noah A.Smith.
2013.
Improved part-of-speech tagging foronline conversational text with word clusters.
InProc.
of NAACL.Patrick Pantel and Marco Pennacchiotti.
2006.Espresso: leveraging generic patterns for automati-cally harvesting semantic relations.
In Proc.
Coling-ACL, pages 113?120.Marius Pas?ca and Benjamin Van Durme.
2007.
Whatyou seek is what you get: extraction of class at-tributes from query logs.
In Proc.
IJCAI, pages2832?2837.Marius Pas?ca and Benjamin Van Durme.
2008.Weakly-supervised acquisition of open-domainclasses and class attributes from web documents andquery logs.
In Proc.
ACL-08: HLT, pages 19?27.Michael Paul and Mark Dredze.
2011.
You are whatyou tweet: Analyzing Twitter for public health.
InProc.
ICWSM, pages 265?272.Marco Pennacchiotti and Ana-Maria Popescu.
2011.A machine learning approach to Twitter user classi-fication.
In Proc.
ICWSM, pages 281?288.Xuan-Hieu Phan.
2006.
CRFTagger: CRF EnglishPOS Tagger.
crftagger.sourceforge.net.Delip Rao, David Yarowsky, Abhishek Shreevats, andManaswi Gupta.
2010.
Classifying latent user at-tributes in Twitter.
In Proc.
International Work-shop on Search and Mining User-Generated Con-tents, pages 37?44.Delip Rao, Michael Paul, Clay Fink, David Yarowsky,Timothy Oates, and Glen Coppersmith.
2011.
Hi-erarchical bayesian models for latent attribute detec-tion in social media.
In Proc.
ICWSM, pages 598?601.Joseph Reisinger and Marius Pas?ca.
2009.
Latentvariable models of concept-attribute attachment.
InProc.
ACL-IJCNLP, pages 620?628.Stephen D. Richardson, William B. Dolan, and LucyVanderwende.
1998.
MindNet: Acquiring andstructuring semantic information from text.
In Proc.ACL-Coling, pages 1098?1102.Jonathan Schler, Moshe Koppel, Shlomo Argamon,and James W. Pennebaker.
2006.
Effects of age andgender on blogging.
In Proc.
AAAI Spring Sympo-sium: Computational Approaches to Analyzing We-blogs, pages 199?205.Lenhart Schubert.
2002.
Can we derive general worldknowledge from texts?
In Proc.
HLT, pages 84?87.Fabrizio Sebastiani.
2002.
Machine learning in au-tomated text categorization.
ACM Comput.
Surv.,34:1?47.Ian Soboroff, Dean McCullough, Jimmy Lin, CraigMacdonald, Iadh Ounis, and Richard McCreadie.2012.
Evaluating real-time search over tweets.
InProc.
ICWSM.Wee Meng Soon, Hwee Tou Ng, and DanielChung Yong Lim.
2001.
A machine learning ap-proach to coreference resolution of noun phrases.Computational Linguistics, 27(4).Oscar Ta?ckstro?m, Ryan McDonald, and Jakob Uszko-reit.
2012.
Cross-lingual word clusters for directtransfer of linguistic structure.
In Proc.
NAACL-HLT, pages 477?487.Kosuke Tokunaga, Jun?ichi Kazama, and Kentaro Tori-sawa.
2005.
Automatic discovery of attribute wordsfrom web documents.
In Proc.
IJCNLP, pages 106?118.719Peter D. Turney.
2002.
Thumbs up or thumbs down?Semantic orientation applied to unsupervised classi-fication of reviews.
In Proc.
ACL, pages 417?424.Benjamin Van Durme, Ting Qian, and Lenhart Schu-bert.
2008.
Class-driven attribute extraction.
InProc.
Coling, pages 921?928.Benjamin Van Durme.
2012.
Streaming analysis ofdiscourse participants.
In Proc.
EMNLP-CoNLL,pages 48?58.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2009.
Recognizing contextual polarity: An explo-ration of features for phrase-level sentiment analy-sis.
Computational Linguistics., 35(3):399?433.Fei Wu and Daniel S. Weld.
2007.
Autonomously se-mantifying Wikipedia.
In Proc.
CIKM, pages 41?50.Omar Zaidan, Jason Eisner, and Christine Piatko.2007.
Using ?annotator rationales?
to improve ma-chine learning for text categorization.
In Proc.NAACL-HLT.720
