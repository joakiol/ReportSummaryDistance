Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 209?216Manchester, August 2008Syntactic Reordering Integrated with Phrase-based SMTJakob ElmingComputational LinguisticsCopenhagen Business Schooljel.isv@cbs.dkAbstractWe present a novel approach to wordreordering which successfully integratessyntactic structural knowledge withphrase-based SMT.
This is done by con-structing a lattice of alternatives basedon automatically learned probabilisticsyntactic rules.
In decoding, the alter-natives are scored based on the outputword order, not the order of the input.Unlike previous approaches, this makes itpossible to successfully integrate syntacticreordering with phrase-based SMT.
Onan English-Danish task, we achieve anabsolute improvement in translation qual-ity of 1.1 % BLEU.
Manual evaluationsupports the claim that the present ap-proach is significantly superior to previousapproaches.1 IntroductionThe emergence of phrase-based statistical machinetranslation (PSMT) (Koehn et al, 2003) has beenone of the major developments in statistical ap-proaches to translation.
Allowing translation ofword sequences (phrases) instead of single wordsprovides SMT with a robustness in word selectionand local word reordering.PSMT has two means of reordering the words.Either a phrase pair has been learned where the tar-get word order differs from the source (phrase in-ternal reordering), or distance penalized orderingsof target phrases are attempted in decoding (phrasec?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.external reordering).
The first solution is strong,the second is weak.The second solution is necessary for reorderingswithin a previously unseen sequence or over dis-tances greater than the maximal phrase length.
Inthis case, the system in essence relies on the tar-get side language model to get the correct wordorder.
The choice is made without knowing whatthe source is.
Basically, it is a bias against phraseexternal reordering.It seems clear that reordering often depends onhigher level linguistic information, which is ab-sent from PSMT.
In recent work, there has beensome progress towards integrating syntactic infor-mation with the statistical approach to reorder-ing.
In works such as (Xia and McCord, 2004;Collins et al, 2005; Wang et al, 2007; Habash,2007), reordering decisions are done ?determinis-tically?, thus placing these decisions outside theactual PSMT system by learning to translate froma reordered source language.
(Crego and Mari?no,2007; Zhang et al, 2007; Li et al, 2007) are morein the spirit of PSMT, in that multiple reorderingsare presented to the PSMT system as (possiblyweighted) options.Still, there remains a basic conflict between thesyntactic reordering rules and the PSMT system:one that is most likely due to the discrepancy be-tween the translation units (phrases) and units ofthe linguistic rules, as (Zhang et al, 2007) pointout.In this paper, we proceed in the spirit of the non-deterministic approaches by providing the decoderwith multiple source reorderings.
But instead ofscoring the input word order, we score the order ofthe output.
By doing this, we avoid the integrationproblems of previous approaches.It should be noted that even though the experi-209ments are conducted within a source reordering ap-proach, this scoring is also compatible with otherapproach.
We will, however, not look further intothis possiblity in the present paper.In addition, we automatically learn reorderingrules based on several levels of linguistic informa-tion fromword form to subordination and syntacticstructure to produce reordering rules that are notrestricted to operations on syntactic tree structurenodes.In the next section, we discuss and contrast re-lated work.
Section 3 describes aspects of Englishand Danish structure that are relevant to reorder-ing.
Section 4 describes the automatic inductionof reordering rules and its integration in PSMT.
Insection 5, we describe the SMT system used in theexperiments.
Section 6 evaluates and discusses thepresent approach.2 Related WorkWhile several recent authors have achieved posi-tive results, it has been difficult to integrate syn-tactic information while retaining the strengths ofthe statistical approach.Several approaches do deterministic reordering.These do not integrate the reordering in the PSMTsystem; instead they place it outside the systemby first reordering the source language, and thenhaving a PSMT system translate from reorderedsource language to target language.
(Collins et al,2005; Wang et al, 2007) do this using manuallycreated rules, and (Xia and McCord, 2004) and(Habash, 2007) use automatically extracted rules.All use rules extracted from syntactic parses.As mentioned by (Al-Onaizan and Papineni,2006), it can be problematic that these determin-istic choices are beyond the scope of optimizationand cannot be undone by the decoder.
That is,there is no way to make up for bad informationin later translation steps.Another approach is non-deterministic.
Thisprovides the decoder with both the original andthe reordered source sentence.
(Crego and Mari?no,2007) operate within Ngram-based SMT.
Theymake use of syntactic structure to reorder the in-put into a word lattice.
Since the paths are notweighted, the lattice merely narrows down the sizeof the search space.
The decoder is not given rea-son to trust one path (reordering) over another.
(Zhang et al, 2007) assign weights to the pathsof their input word lattice.
Instead of hierarchicallinguistic structure, they use reordering rules basedon POS and syntactic chunks, and train the systemwith both original and reordered source word orderon a restricted data set (<500K words).
Their sys-tem does not out-perform a standard PSMT sys-tem.
As they themselves point out, a reason forthis might be that their reordering approach is notfully integrated with PSMT.
This is one of the mainproblems addressed in the present work.
(Li et al, 2007) use weighted n-best lists as in-put for the decoder.
They use rules based on asyntactic parse, allowing children of a tree nodeto swap place.
This is excessively restrictive.
Forexample, a common reordering in English-Danishtranslation has the subject change place with thefinite verb.
Since the verb is often embedded ina VP containing additional words that should notbe moved, such rules cannot be captured by localreordering on tree nodes.In many cases, the exact same word order that isobtained through a source sentence reordering, isalso accessible through a phrase internal reorder-ing.
A negative consequence of source order (SO)scoring as done by (Zhang et al, 2007) and (Liet al, 2007) is that they bias against the valuablephrase internal reorderings by only promoting thesource sentence reordering.
As described in sec-tion 4.3, we solve this problem by reordering theinput string, but scoring the output string, thusallowing the strengths of PSMT to co-exist withrule-based reordering.3 Language comparisonThe two languages examined in this investigation,English and Danish, are very similar from a struc-tural point of view.
A word alignment will most of-ten display an almost one-to-one correlation.
In thehand-aligned data, only 39% of the sentences con-tain reorderings (following the notion of reorder-ing as defined in 4.1).
On average, a sentence con-tains 0.66 reorderings.One of the main differences between Englishand Danish word order is that Danish is a verb-second language: the finite verb of a declarativemain clause must always be the second constituent.Since this is not the case for English, a reorderingrule should move the subject of an English sen-tence to the right of the finite verb, if the first po-sition is filled by something other than the subject.This is exemplified by (1) (examples are annotatedwith English gloss and translation), where ?they?210t7?
?
?
?
?
?
t6?
?
 ?
?
?
?t5?
 ?
?
?
?
?t4?
?
?
?
 ?
?t3?
?
?
?
?
 ?t2?
?
?
 ?
?
?t1 ?
?
?
?
?
?s1s2s3s4s5s6s7Table 1: Reordering exampleshould move to the right of ?come?
to get the Dan-ish word order as seen in the gloss.
(1)[nunowkommercomedethey ]?here they come?Another difference is that Danish sentence adver-bials in a subordinate clause move to the left ofthe finite verb.
This is illustrated in example (2).This example also shows the difficulty for a PSMTsystem.
Since the trigram ?han kan ikke?
is fre-quent in Danish main clauses, and ?han ikke kan?is frequent in subordinate clauses, we need infor-mation on subordination to get the correct wordorder.
This information can be obtained from theconjunction ?that?.
A trigram PSMT system wouldnot be able to handle the reordering in (2), since?that?
is beyond the scope of ?not?.
(2)[hanhesigersaysatthathanheikkenotkancansesee ]?he says that he can not see?In the main clause, on the other hand, Danishprefers the sentence adverbial to appear to the rightof the finite verb.
Therefore, if the English adver-bial appears to the left of the finite verb in a mainclause, it should move right as in example (3).
(3)[hunshes?asawaldrigneverskibetthe ship ]?she never saw the ship?4 Reordering rules4.1 Definition of reorderingIn this experiment, reordering is defined as twoword sequences exchanging positions.
Thesetwo sequences are restricted by the followingconditions:?
Parallel consecutive: They have to make upconsecutive sequences of words, and each hasto align to a consecutive sequence of words.?
Maximal: They have to be the longest possi-ble consecutive sequences changing place.?
Adjacent: They have to appear next to eachother on both source and target side.The sequences are not restricted in length, mak-ing both short and long distance reordering possi-ble.
Furthermore, they need not be phrases in thesense that they appear as an entry in the phrase ta-ble.Table 1 illustrates reordering in a word align-ment matrix.
The table contains reorderings be-tween the light grey sequences (s32and s64)1andthe dark grey sequences (s55and s66).
On the otherhand, the sequences s33and s54are e.g.
not consid-ered reordered, since neither are maximal, and s54is not consecutive on the target side.4.2 Rule inductionIn section 3, we pointed out that subordination isvery important for word order differences betweenEnglish and Danish.
In addition, the sentence po-sition of constituents plays a role.
All this infor-mation is present in a syntactic sentence parse.
Asubordinate clause is defined as inside an SBARconstituent; otherwise it is a main clause.
The con-stituent position can be extracted from the sentencestart tag and the following syntactic phrases.
POSand word form are also included to allow for morespecific/lexicalized rules.Besides including this information for the candi-date reordering sequences (left sequence (LS) andright sequence (RS)), we also include it for the setof possible left (LC) and right (RC) contexts ofthese.
The span of the contexts varies from a singleword to all the way to the sentence border.
Table2 contains an example of the information availableto the learning algorithm.
In the example, LS andRS should change place, since the first position isoccupied by something other than the subject in amain clause.In order to minimize the training data, wordand POS sequences are limited to 4 words, andphrase structure (PS) sequences are limited to 3constituents.
In addition, an entry is only used ifat least one of these three levels is not too long for1Notation: syxmeans the consecutive source sequence cov-ering words x to y.211Level LC LS RS RCWORD <s> today , || today , || , he was driving home || home .
|| home .
< /s>POS <S> NN , || NN , || , PRP AUX VBG NN || NN .
|| NN .
< /S>PS <S> NP , || NP , || , NP AUX VBG ADVP || ADVP .
|| ADVP .
< /S>SUBORD main main main mainTable 2: Example of experience for learning.
Possible contexts separated by ||.both LS and RS, and too long contexts are not in-cluded in the set.
This does not constrain the pos-sible length of a reordering, since a PS sequence oflength 1 can cover an entire sentence.In order to extract rules from the annotateddata, we use a rule-based classifier, Ripper (Cohen,1996).
The motivation for using Ripper is that it al-lows features to be sets of strings, which fits wellwith our representation of the context, and it pro-duces easily readable rules that allow better under-standing of the decisions being made.
In section6.2, extracted rules are exemplified and analyzed.The probabilities of the rules are estimated usingMaximum Likelihood Estimation based on the in-formation supplied by Ripper on the performanceof the individual rules on the training data.
Theselogarithmic probabilities are easily integratable inthe log-linear PSMT model as an additional pa-rameter by simple addition.The rules are extracted from the hand-aligned,Copenhagen Danish-English Dependency Tree-bank (Buch-Kromann et al, 2007).
5478 sentencesfrom the news paper domain containing 111,805English words and 100,185 Danish words.
TheEnglish side is parsed using a state-of-the-art sta-tistical English parser (Charniak, 2000).4.3 Integrating rule-based reordering inPSMTThe integration of the rule-based reordering in ourPSMT system is carried out in two separate stages:1.
Reorder the source sentence to assimilate theword order of the target language.2.
Score the target word order according to therelevant rules.Stage 1) is done in a non-deterministic fashion bygenerating a word lattice as input in the spirit ofe.g.
(Zens et al, 2002; Crego and Mari?no, 2007;Zhang et al, 2007).
This way, the system has boththe original word order, and the reorderings pre-dicted by the rule set.
The different paths of theword lattice are merely given as equal suggestionsto the decoder.
They are in no way individuallyweighted.Separating stage 2) from stage 1) is motivatedby the fact that reordering can have two distinctorigins.
They can occur because of stage 1), i.e.the lattice reordering of the original English wordorder (phrase external reordering), and they canoccur inside a single phrase (phrase internal re-ordering).
We are, however, interested in doingphrase-independent, word reordering.
We want topromote rule-predicted reorderings, regardless ofwhether they owe their existence to a syntactic ruleor a phrase table entry.This is accomplished by letting the actual scor-ing of the reordering focus on the target string.
Thedecoder is informed of where a rule has predicteda reordering, how much it costs to do the reorder-ing, and how much it costs to avoid it.
This isthen checked for each hypothezised target stringby keeping track of what source position target or-der (SPTO) it corresponds to.The SPTO is a representation of which sourceposition the word in each target position originatesfrom.
Putting it differently, the hypotheses containtwo parallel strings; a target word string and itsSPTO string.
In order to access this information,each phrase table entry is annotated with its inter-nal word alignment, which is available as an in-termediate product from phrase table creation.
If aphrase pair has multiple word alignments, the mostfrequent is chosen.Table 3 exemplifies the SPTO scoring.
Thesource sentence is ?today he was late?, and a rulehas predicted that word 3 and 4 should changeplace.
When the decoder has covered the first fourinput words, two of the hypotheses might be H1and H2.
At this point, it becomes apparent that H2contains the desired SPTO (namely ?4 3?
), and itget assigned the reordering cost.
H1 does not con-tain the rule-suggested SPTO (in stead, the wordsare in the order ?3 4?
), and it gets the violation cost.Both these scorings are performed in a phrase-212Source sentence: today1,2he3was4late5Rule: 3 4 ?
4 3Hypothesis Target string SPTOH1 idag han var 1 3 4H2 idag var han 1 4 3Table 3: Example of SPTO scoring during decod-ing at source word 4.independent manner.
The decoder assigns the re-ordering cost to H2 without knowing whether thereordering is internal (due to a phrase table entry)or external (due to a syntactic rule).Phrase internal reorderings at other points of thesentence, i.e.
points that are not covered by a rule,are not judged by the reordering model.
Our ruleextraction does not learn every possible reorderingbetween the two languages, but only the most gen-eral ones.
If no rule has an opinion at a certainpoint in a sentence, the decoder is free to chose thetranslation it prefers without reordering cost.Separating the scoring from the source languagereordering also has the advantage that the SPTOscoring in essence is compatible with other ap-proaches such as a traditional PSMT system.
Wewill, however, not examine this possibility furtherin the present paper.5 The PSMT systemThe baseline is the PSMT system used for the2006 NAACL SMT workshop (Koehn and Monz,2006) with phrase length 3 and a trigram languagemodel (Stolcke, 2002).
The system was trainedon the English and Danish part of the Europarlcorpus version 3 (Koehn, 2005).
Fourth quarterof 2000 was removed in order to use the com-mon test set of 11369 sentences (330,082 Englishwords and 309,942 Danish words with one ref-erence) for testing.
In addition, fourth quarterof 2001 was removed for development purposes.Of these, 10194 were used for various analysispurposes, thereby keeping the test data perfectlyunseen.
500 sentences were taken from the de-velopment set for tuning the decoder parameters.This was done using the Downhill Simplex algo-rithm.
In total, 1,137,088 sentences containing31,376,034 English words and 29,571,518 Danishwords were left for training the phrase table andlanguage model.The decoder used for the baseline systemis Pharaoh (Koehn, 2004) with its distance-Figure 1: Example word lattice.System Dev Test Swap SubsetBaseline 0.262 0.252 0.234no scoring 0.267 0.256 0.241SO scoring 0.268 0.258 0.244SPTO scoring 0.268 0.258 0.245Table 4: BLEU scores for different scoring meth-ods.penalizing reordering model.
For the experiments,we use our own decoder which ?
except for thereordering model ?
uses the same knowledgesources as Pharaoh, i.e.
bidirectional phrase trans-lation model and lexical weighting model, phraseand word penalty, and target language model.
Itsbehavior is comparable to Pharaoh when doingmonotone decoding.The search algorithm of our decoder is similarto the RG graph decoder of (Zens et al, 2002).
Itexpects a word lattice as input.
Figure 1 shows theword lattice for the example in table 3.Since the input format defines all possible wordorders, a simple monotone search is sufficient.
Us-ing a language model of order n, for each hy-pothezised target string ending in the same n-1-gram, we only have to extend the highest scoringhypothesis.
None of the others can possibly out-perform this one later on.
This is because the max-imal context evaluating a phrase extending this hy-pothesis, is the history (n-1-gram) of the first wordof that phrase.
The decoder is not able to look anyfurther back at the preceeding string.6 Evaluation6.1 Results and discussionThe SPTO reordering approach is evaluated on the11369 sentences of the common test set.
Resultsare listed in table 4 along with results on the de-velopment set.
We also report on the swap subset.These are the 3853 sentences where the approachactually motivated reorderings in the test set, in-ternal or external.
The remaining 7516 sentenceswere not influenced by the SPTO reordering ap-proach.213System BLEU Avr.
Human ratingBaseline 0.234 3.00 (2.56)no scoring 0.240 3.00 (2.74)SO scoring 0.239 3.00 (2.62)SPTO scoring 0.244 2.00 (2.08)Table 5: Evaluation on the set where SO and SPTOproduce different translations.
Average human rat-ings are medians with means in parenthesis, lowerscores are better, 1 is the best score.We report on 1) the baseline PSMT system, 2) asystem provided with a rule reordered word latticebut no scoring, 3) the same system but with an SOscoring in the spirit of (Zhang et al, 2007; Li et al,2007), and finally 4) the same system but with theSPTO scoring.The SPTO approach gets an increase over thebaseline PSMT system of 0.6 % BLEU.
The swapsubset, however, shows that the extracted rules aresomewhat restricted, only resulting in swap in13of the sentences.
The relevant set, i.e.
the setwhere the present approach actually differs fromthe baseline, is therefore the swap subset.
Thisway, we concentrate on the actual focus of the pa-per, namely the syntactically motivated SPTO re-ordering.
Here we achieve an increase in perfor-mance of 1.1 % BLEU.Comparing to the other scoring approaches doesnot show much improvement.
A possible explana-tion is that the rules do not apply very often, incombination with the fact that the SO and SPTOscoring mechanisms most often behave alike.
Thedifference in SO and SPTO scoring only leads toa difference in translation in 10% of the sentenceswhere reordering is done.
This set is interesting,since it provides a focus on the difference betweenthe SO and the SPTO approaches.
In table 5, weevaluate on this set.The BLEU scores on the entire set indicate thatSPTO is a superior scoring method.
To backthis observation, the 100 first sentences are man-ually evaluated by two native speakers of Danish.
(Callison-Burch et al, 2007) show that rankingsentences gives higher inter-annotator agreementthan scoring adequacy and fluency.
We thereforeemploy this evaluation method, asking the evalua-tors to rank sentences from the four systems giventhe input sentence.
Ties are allowed.
The an-notators had reasonable inter-annotator agreement(?
= 0.523, P (A) = 0.69, P (E) = 0.35).
Table 5Decoder choice SO SPTOPhrase internal reordering 401 1538Phrase external reordering 3846 2849Reject reordering 1468 1328Table 6: The choices made based on the SO andSPTO scoring for the 5715 reorderings proposedby the rules for the test data.shows the average ratings of the systems.
Thisclearly shows the SPTO scoring to be significantlysuperior to the other methods (p < 0.05).Most of the cases (55) where SPTO outperformsSO are cases where SPTO knows that a phrase paircontains the desired reordering, but SO does not.Therefore, SO has to use an external reorderingwhich brings poorer translation than the internalreordering, because the words are translated indi-vidually rather than by a single phrase (37 cases),or it has to reject the desired reordering (18 cases),which also hurts translation, since it does not getthe correct word order.Table 6 shows the effect of SO and SPTO scor-ing in decoding.
Most noticeable is that the SOscoring is strongly biased against phrase inter-nal reorderings; SPTO uses nearly four times asmany phrase internal reorderings as SO.
In addi-tion, SPTO is a little less likely to reject a rule pro-posed reordering.6.2 Rule analysisThe rule induction resulted in a rule set containing27 rules.
Of these, 22 concerned different waysof identifying contexts where a reordering shouldoccur due to the verb second nature of Danish.
4rules had to do with adverbials in main and in sub-ordinate clauses, and the remaining rule expressedthat currency is written after the amount in Dan-ish, while it is the other way around in English.Since the training data however only includes Dan-ish Crowns, the rule was lexicalized to ?DKK?.Table 7 shows a few of the most frequently usedrules.
The first three rules deal with the verbsecond phenomenon.
The only difference amongthese is the left context.
Either it is a prepositionalphrase, a subordinate clause or an adverbial.
Theseare three ways that the algorithm has learned toidentify the verb second phenomenon conditions.Rule 3 is interesting in that it is lexicalized.
In thelearning data, the Danish correspondent to ?how-ever?
is most often not topicalized, and the subject214No LC LS RS RC1 PS: <S> PP , PS: NP POS: FV2 PS: SBAR , PS: NP POS: FV3 PS: ADVP , PS: NP POS: FV!
WORD:however ,4 PS: FV POS: RB PS: VPSUB: sub5 PS: <S> NP PS: ADVP POS: FVSUB: mainTable 7: Example rules and their application statis-tics.is therefore not forced from the initial position.
Asa consequence, the rule states that it should onlyapply, if ?however?
is not included in the left con-text of the reordering.Rule 4 handles the placement of adverbials in asubordinate clause.
Since the right context is sub-ordinate and a verb phrase, the current sequencesmust also be subordinate.
In contrast, the fifth ruledeals with adverbials in a main clause, since theleft context noun phrase is in a main clause.A problem with the hand-aligned data used forrule-induction is that it is out of domain comparedto the Europarl data used to train the SMT system.The hand-aligned data is news paper texts, and Eu-roparl is transcribed spoken language from the Eu-ropean Parliament.
Due to its spoken nature, Eu-roparl contains frequent sentence-initial forms ofaddress.
That is, left adjacent elements that are notintegrated parts of the sentence as in example (4).This is not straightforward, because on the sur-face these look a lot like topicalized constructions,as in example (5).
In topicalized constructions, itis an integrated part of the sentence that is movedto the front in order to affect the flow of discourseinformation.
This difference is crucial for the re-ordering rules, since ?i?
and ?have?
should reorderin (5), but not in (4), in order to get Danish wordorder.
(4) mr president , i have three points .
(5) as president , i have three points .When translating the development set, it becameclear that many constructions like (4) were re-ordered.
Since these constructions were notpresent in the hand-aligned data, the learning algo-rithm did not have the data to learn this difference.We therefore included a manual, lexicalized rulestating that if the left context contained one of a setof titles (mr, mrs, ms, madam, gentlemen), the re-ordering should not take place.
Since the learningincludes word form information, this is a rule thatthe learning algorithm is able to learn.
To a greatextent, the rule eliminates the problem.The above examples also illustrate that local re-ordering (in this case as local as two neighbor-ing words) can be a problem for PSMT, sinceeven though the reordering is local, the informa-tion about whether to reorder or not is not neces-sarily local.6.3 Reordering analysisIn this section, we will show and discuss a fewexamples of the reorderings made by the SPTOapproach.
Table 8 contain two translations takenfrom the test set.In translation 1), the subject (bold) is correctlymoved to the right of the finite verb (italics), whichthe baseline system fails to do.
Moving the finiteverb away from the infinite verb ?feature?, how-ever, leads to incorrect agreement between these.While the baseline correctly retains the infiniteform (?st?a?
), the language model forces another fi-nite form (the past tense ?stod?)
in the SPTO re-ordering approach.Translation 2) illustrates the handling of adver-bials.
The first reordering is in a main clause,therefore, the adverbial is moved to the right of thefinite verb.
The second reordering occurs in a sub-ordinate clause, and the adverbial is moved to theleft of the finite verb.
Neither of these are handledsuccessfully by the baseline system.In this case, the reordering leads to better wordselection.
The English ?aims to?
corresponds tothe Danish ?sigter mod?, which the SPTO approachgets correct.
However, the baseline system trans-lates ?to?
to its much more common translation?at?, because ?to?
is separated from ?aims?
by theadverbial ?principally?.7 Conclusion and Future PlansWe have described a novel approach to word re-ordering in SMT, which successfully integratessyntactically motivated reordering in phrase-basedSMT.
This is achieved by reordering the inputstring, but scoring on the output string.
As op-posed to previous approaches, this neither biasesagainst phrase internal nor external reorderings.We achieve an absolute improvement in translationquality of 1.1 % BLEU.
A result that is supportedby manual evaluation, which shows that the SPTO2151 S based on this viewpoint , every small port and every ferry port which handlesa great deal of tourist traffic should feature on the european list .B baseret p?a dette synspunkt , ethvert lille havn og alle f?rgehavnen somh?andterer en stor turist trafik skal st?a p?a den europ?iske liste .P baseret p?a dette synspunkt , skal alle de sm?a havne , og alle f?rgehavnensom behandler mange af turister trafik stod p?a den europ?iske liste .2 S the rapporteur generally welcomes the proposals in the commission white paper on thissubject but is apprehensive of the possible implications of the reform , which aimsprincipally to decentralise the implementation of competition rules .B ordf?reren generelt bifalder forslagene i kommissionens hvidbog om dette emne , men erbekymret for de mulige konsekvenser af den reform , som sigter hovedsagelig atdecentralisere gennemf?relsen af konkurrencereglerne .P ordf?reren bifalder generelt forslagene i kommissionens hvidbog om dette emne , men erbekymret for de mulige konsekvenser af den reform , som is?r sigter mod atdecentralisere gennemf?relsen af konkurrencereglerne .Table 8: Examples of reorderings.
S is source, B is baseline, and P is the SPTO approach.
The elementsthat have been reordered in the P sentence are marked alike in all sentences.
The text in bold has changedplace with the text in italics.approach is significantly superior to previous ap-proaches.In the future, we plan to apply this approachto English-Arabic translation.
We expect greatergains, due to the higher need for reordering be-tween these less-related languages.
We also wantto examine the relation between word alignmentmethod and the extracted rules and the relationshipbetween reordering and word selection.
Finally, alimitation of the current experiments is that theyonly allow rule-based external reorderings.
Sincethe SPTO scoring is not tied to a source reorderingapproach, we want to examine the effect of simplyadding it as an additional parameter to the base-line PSMT system.
This way, all external reorder-ings are made possible, but only the rule-supportedones get promoted.ReferencesAl-Onaizan, Y. and K. Papineni.
2006.
Distortion modelsfor statistical machine translation.
In Proceedings of 44thACL.Buch-Kromann, M., J. Wedekind, and J. Elming.
2007.
TheCopenhagen Danish-English Dependency Treebank v. 2.0.http://www.isv.cbs.dk/?mbk/cdt2.0.Callison-Burch, C., C. Fordyce, P. Koehn, C. Monz, andJ.
Schroeder.
2007.
(Meta-) evaluation of machine transla-tion.
In Proceedings of ACL-2007 Workshop on StatisticalMachine Translation.Charniak, E. 2000.
A maximum-entropy-inspired parser.
InProceedings of the 1st NAACL.Cohen, W. 1996.
Learning trees and rules with set-valuedfeatures.
In Proceedings of the 14th AAAI.Collins, M., P. Koehn, and I. Kucerova.
2005.
Clause restruc-turing for statistical machine translation.
In Proceedings ofthe 43rd ACL.Crego, J. M. and J.
B. Mari?no.
2007.
Syntax-enhanced n-gram-based smt.
In Proceedings of the 11th MT Summit.Habash, N. 2007.
Syntactic preprocessing for statistical ma-chine translation.
In Proceedings of the 11th MT Summit.Koehn, P. and C. Monz.
2006.
Manual and automatic evalu-ation of machine translation between european languages.In Proceedings on the WSMT.Koehn, P., F. J. Och, and D. Marcu.
2003.
Statistical phrase-based translation.
In Proceedings of NAACL.Koehn, P. 2004.
Pharaoh: a beam search decoder for phrase-based statistical machine translation models.
In Proceed-ings of AMTA.Koehn, P. 2005.
Europarl: A parallel corpus for statisticalmachine translation.
In Proceedings of MT Summit.Li, C., M. Li, D. Zhang, M. Li, M. Zhou, and Y. Guan.
2007.A probabilistic approach to syntax-based reordering forstatistical machine translation.
In Proceedings of the 45thACL.Stolcke, A.
2002.
Srilm ?
an extensible language modelingtoolkit.
In Proceedings of the International Conference onSpoken Language Processing.Wang, C., M. Collins, and P. Koehn.
2007.
Chinese syntacticreordering for statistical machine translation.
In Proceed-ings of EMNLP-CoNLL.Xia, F. and M. McCord.
2004.
Improving a statistical mt sys-tem with automatically learned rewrite patterns.
In Pro-ceedings of Coling.Zens, R., F. J. Och, and H. Ney.
2002.
Phrase-based statisticalmachine translation.
In Jarke, M., J. Koehler, and G. Lake-meyer, editors, KI - 2002: Advances in Artificial Intel-ligence.
25.
Annual German Conference on AI.
SpringerVerlag.Zhang, Y., R. Zens, and H. Ney.
2007.
Improved chunk-levelreordering for statistical machine translation.
In Proceed-ings of the IWSLT.216
