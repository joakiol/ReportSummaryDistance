Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1660?1670,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsWho caught a cold?
?
Identifying the subject of a symptomShin Kanouchi?, Mamoru Komachi?, Naoaki Okazaki?,Eiji Aramaki?, and Hiroshi Ishikawa?
?Tokyo Metropolitan University, {kanouchi-shin at ed., komachi at, ishikawh at}tmu.ac.jp?Tohoku University, okazaki at ecei.tohoku.ac.jp?Kyoto University, eiji.aramaki at gmail.comAbstractThe development and proliferation of so-cial media services has led to the emer-gence of new approaches for surveying thepopulation and addressing social issues.One popular application of social mediadata is health surveillance, e.g., predictingthe outbreak of an epidemic by recogniz-ing diseases and symptoms from text mes-sages posted on social media platforms.
Inthis paper, we propose a novel task thatis crucial and generic from the viewpointof health surveillance: estimating a sub-ject (carrier) of a disease or symptommen-tioned in a Japanese tweet.
By designingan annotation guideline for labeling thesubject of a disease/symptom in a tweet,we perform annotations on an existing cor-pus for public surveillance.
In addition,we present a supervised approach for pre-dicting the subject of a disease/symptom.The results of our experiments demon-strate the impact of subject identificationon the effective detection of an episode ofa disease/symptom.
Moreover, the resultssuggest that our task is independent of thetype of disease/symptom.1 IntroductionSocial media services, including Twitter and Face-book, provide opportunities for individuals toshare their experiences, thoughts, and opinions.The wide use of social media services has ledto the emergence of new approaches for survey-ing the population and addressing social issues.One popular application of social media data isflu surveillance, i.e., predicting the outbreak of in-fluenza epidemics by detecting mentions of flu in-fections on social media platforms (Culotta, 2010;Lampos and Cristianini, 2010; Aramaki et al,2011; Paul and Dredze, 2011; Signorini et al,2011; Collier, 2012; Dredze et al, 2013; Gesualdoet al, 2013; Stoov?
and Pedrana, 2014).Previous studies mainly relied on shallow tex-tual clues in Twitter posts in order to predict thenumber of flu infections, e.g., the number of oc-currences of specific keywords (such as ?flu?
or?influenza?)
on Twitter.
However, such a simpleapproach can lead to incorrect predictions.
Bro-niatowski et al (2013) argued that media atten-tion increases chatter, i.e., the number of tweetsthat mention the flu without the poster being ac-tually infected.
Examples include, ?I don?t wishthe flu on anyone?
and ?A Harry Potter actor hos-pitalised after severe flu-like syndromes.?
Lazeret al (2014) reported large errors in Google FluTrends (Carneiro and Mylonakis, 2009) on the ba-sis of a comparison with the proportion of doctorvisits for influenza-like illnesses.Lamb et al (2013) aimed to improve the ac-curacy of detecting mentions of flu infections.Their method trains a binary classifier to distin-guish tweets reporting flu infections from thoseexpressing concern or awareness about the flu,e.g., ?Starting to get worried about swine flu.?
Ac-cordingly, they reported encouraging results (e.g.,better correlations with CDC trends), but their ap-proach requires supervision data and a lexicon(word class features) specially designed for the flu.Moreover, even though this method is a reason-able choice for improving the accuracy, it is notreadily applicable to other types of diseases (e.g.,dengue fever) and symptoms (e.g., runny nose),which are also important for public health (Velardiet al, 2014).In this paper, we propose a more generalizedtask setting for public surveillance.
In otherwords, our objective is to estimate the subject(carrier) of a disease or symptom mentioned in aJapanese tweet.
More specifically, we are inter-ested in determining who has a disease/symptom1660(if any) in order to examine whether the poster suf-fers from the disease or symptom.
For example,given the sentence ?I caught a cold,?
we wouldpredict that the first person (?I,?
i.e., the poster)is the subject (carrier) of the cold.
On the otherhand, we can ignore the sentence, ?The TV pre-senter caught a cold?
only if we predict that thesubject of the cold is the third person, who is at adifferent location from the poster.Although the task setting is simple and intuitive,we identify several key challenges in this study.1.
Novel task setting.
The task of identifyingthe subject of a disease/symptom is similarto predicate-argument structure (PAS) anal-ysis for nominal predicates (Meyers et al,2004; Sasano et al, 2004; Komachi et al,2007; Gerber and Chai, 2010).
However,these studies do not treat diseases (e.g., ?in-fluenza?)
and symptoms (e.g., ?headache?)
asnominal predicates.
To the best of our knowl-edge, this task has not been explored in natu-ral language processing (NLP) thus far.2.
Identifying whether the subject has a dis-ease/symptom.
Besides the work on PASanalysis for nominal predicates, the most rel-evant work is PAS analysis for verb predi-cates.
However, our task is not as simple aspredicting the subject of the verb governinga disease/symptom-related noun.
For exam-ple, the subject of the verb ?beat?
is the firstperson ?I?
in the sentence ?I beat the flu,?
butthis does not imply that the poster has the flu.At the same time, we can use a variety ofexpressions for indicating an infection, e.g.,?I?m still sick!!
This flu is just incredible...,?
?I can feel the flu bug in me,?
and ?I testedpositive for the flu.?3.
Omitted subjects.
We often come acrosstweets with omitted subjects, e.g., ?Downwith the flu feel?
and ?Thanks the flu forstriking in hard this week?
even in Englishtweets.
Because the first person is omittedfrequently, it is important to predict omittedsubjects from the viewpoint of the applica-tion (public surveillance).In this paper, we present an approach for iden-tifying the subjects of various types of diseasesand symptoms.
The contributions of this paper arethree-fold.1.
In order to explore a novel and general tasksetting, we design an annotation guideline forlabeling a subject of a disease/symptom in atweet, and we deliver annotations in an exist-ing corpus for public surveillance.
Further,we propose a method for predicting the sub-ject of a disease/symptom by using the anno-tated corpus.2.
The experimental results show that the taskof identifying subjects is independent of thetype of diseases/symptom.
We verify thepossibility of transferring supervision data todifferent targets of diseases and symptoms.In other words, we verify that it is possi-ble to utilize the supervision data for a par-ticular disease/symptom to improve the ac-curacy of predicting subjects of another dis-ease/symptom.3.
In addition, the experimental results demon-strate the impact of identifying subjects onimproving the accuracy of the downstreamapplication (identification of an episode of adisease/symptom).The remainder of this paper is organized as fol-lows.
Section 2 describes the corpus used in thisstudy as well as our annotation work for identify-ing subjects of diseases and symptoms.
Section3.1 presents our method for predicting subjects onthe basis of the annotated corpus.
Sections 3.2and 3.3 report the performance of the proposedmethod.
Section 3.4 describes the contributionsof this study toward identifying episodes of dis-eases and symptoms.
Section 4 reviews some re-lated studies.
Finally, Section 5 summarizes ourfindings and concludes the paper with a brief dis-cussion on the scope for future work.2 Corpus2.1 Target corpusWe used a Japanese corpus for public surveil-lance of diseases and symptoms (Aramaki et al,2011).
The corpus targets seven types of dis-eases and symptoms: cold, cough, headache, chill,runny nose, fever, and sore throat.
Tweets con-taining keywords for each disease/symptom werecollected using the Twitter Search API: for exam-ple, tweets about sore throat were collected usingthe query ?
(sore OR pain) AND throat?.
Further,1661Figure 1: Examples of annotations of subject labels.Subject label Definition ExampleFIRSTPERSON The subject of the disease/symptom is the posterof the tweet.I wish I have fever or some-thing so that I don?t have togo to school.NEARBYPERSON The subject of the disease/symptom is a personwhom the poster can directly see or hear.my sister continues to havea high fever...FARAWAYPERSON The subject of the disease/symptom is a personwho is at a different location from the poster.
@***** does sour stuffgive you a headache?NONHUMAN The subject of the disease/symptom is not a per-son.
Alternatively, the sentence does not describea disease/symptom but a phenomenon or event re-lated to the disease/symptom.My room is so chill.
But Ilike it.NONE The subject of the disease/symptom does not ex-ist.
Alternatively, the sentence does not mentionan occurrence of a disease/symptom.I hate buyin cold medicinecuz I never know whichone to buyTable 1: Definitions of subject labels and example tweets.the corpus consists of 1,000 tweets for each dis-ease/symptom besides cold, and 5,000 tweets forcold.
The corpus was collected through wholeyears 2007-2008.
This period was not in theA/H1N1 flu pandemic season.An instance in this corpus consists of a tweettext (in Japanese) and a binary label (episode la-bel, hereafter) indicating whether someone nearthe poster has the target disease/symptom1.
A pos-itive episode indicates an occurrence of the dis-ease/symptom.
In this study, we disregarded in-stances of sore throat in the experiments becausemost such instances were positive episodes2.1This label is positive if someone mentioned in the tweetis in the same prefecture as the poster.
This is because the cor-pus was designed to survey the spread of a disease/symptomin every prefecture.2In Japanese tweets, sore throat or throat pain mostly de-scribes the health condition of the poster.2.2 Annotating subjectsIn this study, we annotated the subjects of diseasesand symptoms in the corpus described in Section2.1.
Specifically, we annotated the subjects in 500tweets for each disease/symptom (except for sorethroat).
Thus, our corpus includes a total of 3,000tweets in which the subjects of diseases and symp-toms are annotated.Figure 1 shows examples of annotations inthis study.
Episode labels, tweet texts, and dis-ease/symptom keywords were annotated by Ara-maki et al (2011) in the corpus.We annotated the subject labels of the dis-eases/symptoms in each tweet and identified thosewho had the target disease/symptom.
The sub-ject labels indicate those who have the correspond-ing disease/symptom; they are described in detail1662Label FIRSTPERSON NEARBYPERSON FARAWAYPERSON NONHUMAN NONE Total# tweets 2,153 129 201 40 401 2,924# explicit subjects 70 (3.3%) 112 (86.8%) 175 (87.1%) 38 (95.0%) 0 (0.0%) 395# positive episodes 1,833 99 2 0 16 1,950# negative episodes 320 30 199 40 385 974Positive ratio 85.1% 76.7% 1.0% 0.0% 4.0% 66.7%Table 2: Associations between subject labels and positive/negative episodes of diseases and symptoms.herein.In addition to the subject labels, we annotatedthe text span that indicates a subject.
However, thesubjects of diseases/symptoms are often omitted intweet texts.
Example 3 in Figure 1 shows a casein which the subject is omitted.
The informationas to whether the subject is omitted is useful foranalyzing the difficulty in predicting the subjectof a disease/symptom.Table 1 lists the definitions of the subject la-bels with tweeted examples.
Because it is impor-tant to distinguish the primary information (infor-mation that is observed and experienced by theposter) from the secondary information (informa-tion that is broadcasted by the media) for the ap-plication of public surveillance, we introduced fivelabels: FIRSTPERSON, NEARBYPERSON, FAR-AWAYPERSON, NONHUMAN, and NONE.FIRSTPERSON is assigned when the subject ofthe disease/symptom is the poster of the tweet.When annotating this label, we ignore the modal-ity or factuality of the event of acquiring the dis-ease/symptom.
For example, the example tweetcorresponding to FIRSTPERSON in Table 1 doesnot state that the poster has a fever but only thatthe poster has a desire to have a fever.
Althoughsuch tweets may be inappropriate for identifyinga disease/symptom, this study focuses on identify-ing the possessive relation between a subject anda disease/symptom.
The concept underlying thisdecision is to divide the task of public surveillanceinto several sub-tasks that are sufficiently general-ized for use in other NLP applications.
Therefore,the task of analyzing the modality lies beyond ofscope of this study (Kitagawa et al, ).
We applythe same criterion to the labels NEARBYPERSON,FARAWAYPERSON, and NONHUMAN.NEARBYPERSON is assigned when the subjectof the disease/symptom is a person whom theposter can directly see or hear.
In the original cor-pus (Aramaki et al, 2011), a tweet is labeled aspositive if the person having a disease/symptom isin the same prefecture as the poster.
However, it isextremely difficult for annotators to judge from atweet whether the person mentioned in the tweetis in the same prefecture as the poster.
Never-theless, we would like to determine from a tweetwhether the poster can directly see or hear a pa-tient.
For these reasons, we introduced the labelNEARBYPERSON in this study.FARAWAYPERSON applies to all cases in whichthe subject is a human, but not classified as FIRST-PERSON or NEARBYPERSON.
This category fre-quently includes tweeted replies, as in the case ofthe example corresponding to FARAWAYPERSONin Table 1.
We assign FARAWAYPERSON to suchsentences because we are unsure whether the sub-ject of the symptom is a person whom the postercan physically see or hear.NONHUMAN applies to cases in which the sub-ject is not a human but an object or a concept.
Forexample, a sentence with the phrase ?My room isso chill?
is annotated with this label.NONE indicates that the sentence does not men-tion a target disease or symptom even though itincludes a keyword for the disease/symptom.In order to investigate the inter-annotator agree-ment, we sampled 100 tweets of cold at random,and examined the Cohen?s ?
statistic by two an-notators.
The ?
statistic is 0.83, indicating a highlevel agreement (Carletta, 1996).Table 2 reports the distribution of subject la-bels in the corpus annotated in this study.
Whenthe subject of a disease/symptom is FIRSTPER-SON, only 3.3% of the tweets have explicit tex-tual clues for the first person3.
In other words,when the subject of a disease/symptom is FIRST-PERSON, we rarely find textual clues in tweets.
Incontrast, there is a greater likelihood of finding ex-plicit clues for NEARBYPERSON, FARAWAYPER-SON, and NONHUMAN subjects.Table 2 also lists the probability of positiveepisodes given a subject label, i.e., the posi-tive ratio.
The likelihood of a positive episode3This ratio may appear to be extremely low, but it is verycommon to omit first person pronouns in Japanese sentences.1663is extremely high when the subject label of adisease/symptom is FIRSTPERSON (85.1%) orNEARBYPERSON (76.7%).
In contrast, FAR-AWAYPERSON, NONHUMAN, and NONE sub-jects represent negative episodes (less than 5.0%).These facts suggest that identifying subject labelscan improve the accuracy of predicting patient la-bels for diseases and symptoms.3 Experiment3.1 Subject classifierWe built a classifier to predict a subject label fora disease/symptom mentioned in a sentence by us-ing the corpus described in the previous section.In our experiment, we merged training instanceshaving the label NONHUMAN with those havingthe label NONE because the number of NONHU-MAN instances was small and we did not need todistinguish the label NONHUMAN from the labelNONE in the final episode detection task.
Thus,the classifier was trained to choose a subject la-bel from among FIRSTPERSON, NEARBYPER-SON, FARAWAYPERSON, and NONE.
We dis-carded instances in which multiple diseases orsymptoms are mentioned in a tweet as well asthose in which multiple subjects are associatedwith a disease/symptom in a tweet.
In addition,we removed text spans corresponding to retweets,replies, and URLs; the existence of these spanswas retained for firing features.
We trained an L2-regularized logistic regression model using Clas-sias 1.14.
The following features were used.Bag-of-Words (BoW).
Nine words included be-fore and after a disease/symptom keyword.
Wesplit a Japanese sentence into a sequence of wordsusing a Japanese morphological analyzer, MeCab(ver.0.98) with IPADic (ver.2.7.0)5.Disease/symptom word (Keyword).
The sur-face form of the disease/symptom keyword (e.g.?cold?
and ?headache?).2,3-gram.
Character-based bigrams and tri-grams before and after the disease/symptom key-word within a window of six letters.URL.
A boolean feature indicating whether thetweet includes a URL.4http://www.chokkan.org/software/classias/5http://taku910.github.io/mecab/Feature Micro F1 Macro F1BoW (baseline) 77.2 42.2BoW + Keyword 81.9 53.6BoW + 2,3-gram 79.1 46.1BoW + URL 77.3 42.7BoW + RT & reply 80.0 47.1BoW + NearWord 77.6 46.8BoW + FarWord 77.3 42.7BoW + Title word 77.1 42.7BoW + Tweet length 77.4 43.3BoW + Is-head 77.6 43.5All features 84.0 61.8Table 3: Performance of the subject classifier.RT & reply.
Boolean features indicatingwhether the tweet is a reply or a retweet.Word list for NEARBYPERSON (NearWord).A boolean feature indicating whether the tweetcontains a word that is included in the lexicon forNEARBYPERSON.
We manually collected wordsthat may refer to a person who is near the poster,e.g., ?girlfriend,?
?sister,?
and ?staff.?
The Near-Word list includes 97 words.Word list for FARAWAYPERSON (FarWord).A boolean feature indicating whether the tweetcontains a word that is included in the lexicon forFARAWAYPERSON.
Similarly to the NearWordlist, we manually collected 50 words (e.g., ?in-fant?)
for compiling this list.Title word.
A boolean feature indicatingwhether the tweet contains a title word accom-panied by a proper noun.
The list of title wordsincludes expressions such as ????
and ????
(roughly corresponding to ?Ms?
and ?Mr?)
thatdescribe the title of a person.Tweet length.
Three types of boolean featuresthat fire when the tweet has less than 11 words, 11to 30 words, and more than 30 words, respectively.Is-head.
A boolean feature indicating whetherthe word following a disease/symptom keywordis a noun.
In Japanese, when the word follow-ing a disease/symptom keyword is a noun, the dis-ease/symptom keyword is unlikely to be the headof the noun phrase.1664Correct/predicted label FIRSTPERSON NEARBY.
FARAWAY.
NONE TotalFIRSTPERSON 2,084 (?15) 6 (+1) 25 (+21) 38 (?7) 2,153NEARBYPERSON 80 (?20) 41 (+29) 4 (?5) 4 (?4) 129FARAWAYPERSON 88 (?49) 8 (+2) 89 (+46) 16 (+1) 201NONE 174 (?158) 2 (+1) 10 (+4) 255 (+153) 441Total predictions 2,426 (?237) 57 (+33) 128 (+66) 313 (+137) 2,924Table 4: Confusion matrix between predicted and correct subject labels.3.2 Evaluation of the subject classifierTable 3 reports the performance of the subjectclassifier measured via five-fold cross validation.We used 3,000 tweets corresponding to six typesof diseases and symptoms for this experiment.
TheBag-of-Words (BoW) feature achieved micro andmacro F1 scores of 77.2 and 42.2, respectively.When all the features were used, the performancewas boosted, i.e., micro and macro F1 scores of84.0 and 61.8 were achieved.
Features such as dis-ease/symptom keywords, retweet & reply, and thelexicon for NEARBYPERSON were particularly ef-fective in improving the performance.The surface form of the disease/symptom key-word was found to be the most effective featurein this task, the reasons for which are discussed inSection 3.3.A retweet or reply tweet provides evidencethat the poster has interacted with another person.Such meta-linguistic features may facilitate se-mantic and discourse analysis in web texts.
How-ever, this feature is mainly limited to tweets.The lexicon for NEARBYPERSON provided animprovement of 4.6 points in terms of the macroF1 score.
This is because (i) around 90% ofthe subjects for NEARBYPERSON were explicitlystated in the tweets and (ii) the vocabulary of peo-ple near the poster was limited.Table 4 shows the confusion matrix between thecorrect labels and the predicted labels.
The diag-onal elements (in bold face) represent the numberof correct predictions.
The figures in parenthesesdenote the number of instances for which the base-line feature set made incorrect predictions, but thefull feature set made correct predictions.
For ex-ample, the classifier predicted NEARBYPERSONsubjects 48 times; 34 out of 48 predictions werecorrect.
The full feature set increased the numberof correct predictions by 22.From the diagonal elements (in bold face), wecan confirm that the number of correct predictionsincreased significantly from the baseline case, ex-cept for FIRSTPERSON.
One of the reasons forthe improved accuracy of NONE prediction is theimbalanced label ratio of each disease/symptom.NONE accounts for 14% of the entire corpus, butonly 5% of the runny nose corpus.
On the otherhand, NONE accounts for more than 30% of thechill corpus.
The disease/symptom keyword fea-ture adjusts the ratio of the subject labels foreach disease/symptom, and the accuracy of sub-ject identification is improved.As compared to the baseline case, the number ofFIRSTPERSON cases that were predicted as FAR-AWAYPERSON increased.
Such errors may be at-tributed to the reply feature.
According to ourannotation scheme, FARAWAYPERSON containsmany reply tweets.
Because the reply & retweetfeatures make the second-largest contribution inour experiment, the subject classifier tends to out-put FARAWAYPERSON if the tweet is a reply.Table 5 summarizes the subject classification re-sults comparing the case in which the subject ofa disease/symptom exists in the tweet with thatin which the subject does not exist.
The pre-diction of FIRSTPERSON is not affected by thepresence of the subject because FIRSTPERSONsubjects are often omitted (especially in Japanesetweets).
The prediction of NEARBYPERSON andFARAWAYPERSON is difficult if the subject is notstated explicitly.
In contrast, it is easy to correctlypredict NONE even though the subject is not ex-pressed explicitly.
This is because it is not easy tocapture a variety of human-related subjects usingBag-of-Words, N-gram, or other simple featuresused in this experiment.3.3 Dependency on diseases/symptomsThe experiments described in Section 3.2 usetraining instances for all types of diseases andsymptoms.
However, each disease/symptom mayhave a set of special expressions for describingthe state of an episode.
For example, even though?catch a cold?
is a common expression, we cannot1665Subject FIRSTPERSON NEARBYPERSON FARAWAYPERSON NONE# Explicit 66/69 (95.7%) 40/112 (35.7%) 79/174 (45.4%) 1/26 (3.8%)# Omitted 2,018/2,084 (96.8%) 1/17 (5.9%) 10/27 (37.0%) 254/415 (61.2%)# Total 2,084/2,153 (96.8%) 41/129 (31.8%) 89/201 (44.3%) 255/441 (57.8%)Table 5: Subject classification results comparing explicit subjects with omitted subjects.Figure 2: F1 scores for predicting subjects of coldwith different types and sizes of training data.say ?catch a fever?
by combining the verb ?catch?and the disease ?fever.?
The corpus developed inSection 2.2 can be considered as the supervisiondata for weighting linguistic patterns that connectdiseases/symptoms with their subjects.
This view-point raises another question: how strongly doesthe subject classifier depend on specific diseasesand symptoms?In order to answer this question, we compare theperformance of recognizing subjects of cold whenusing the training instances for all types of dis-eases and symptoms with that when using only thetraining instances for the target disease/symptom.Figure 2 shows the macro F1 scores with all train-ing instances (dotted line) and with only coldtraining instances (solid line)6.In this case, training with cold instances is nat-urally more efficient than training with other typesof diseases/symptoms.
When trained with 400 in-stances only for cold, the classifier achieved anF1 score of 45.2.
Moreover, we confirmed thatadding training instances for other types of dis-eases/symptoms improved the F1 score: the max-6For the solid line, we used 500 instances of ?cold?
as atest set, and we plotted the learning curve by increasing thenumber of training instances for other diseases/symptoms.For the dotted line, we fixed 100 instances for a test set, andwe plotted the learning curve by increasing the number oftraining instances (100, 200, 300, and 400).Figure 3: Overall structure of the system.imum F1 score was 54.6 with 2,900 instances.These results indicate the possibility of buildinga subject classifier that is independent of specificdiseases/symptoms but applicable to a variety ofdiseases/symptoms.
We observed a similar ten-dency for other types of diseases/symptoms.3.4 Contributions to the episode classifierThe ultimate objective of this study is to detectoutbreaks of epidemics by recognizing diseasesand symptoms.
In order to demonstrate the contri-butions of this study, we built an episode classifierthat judges whether the poster or a person close tothe poster suffers from a target disease/symptom.Figure 3 shows the overall structure of the system.Given a tweet, the system predicts the subject la-bel for a disease/symptom, and integrates the pre-dicted subject label as a feature for the episodeclassifier.
In addition to the features used in Ara-maki et al (2011), we included binary features,each of which corresponds to a subject label pre-dicted by the proposed method.
We trained an L2-regularized logistic regression model using Clas-sias 1.1.Table 6 summarizes the performance of theepisode classifier with different settings: withoutsubject labels (baseline), with predicted subject la-1666Setting Cold Cough Headache Chill Runny nose Fever Macro F1Baseline (BL) 84.4 88.5 90.8 75.9 89.2 78.1 84.5BL + predicted subjects 85.0 88.3 90.7 81.4 89.4 80.2 85.8BL + gold-standard subjects 87.7 92.6 93.5 88.5 91.4 88.6 90.4Table 6: Performance of the episode classifier.bels , and with gold-standard subject labels.
Wemeasured the F1 scores via five-fold cross vali-dation7.
Further, we confirmed the contributionof subject label prediction, which achieved an im-provement of 1.3 points over the baseline method(85.8 vs. 84.5).
When using the gold-standardsubject labels, the episode classifier achieved animprovement of 5.9 points.
These results highlightthe importance of recognizing a subject who has adisease/symptom using the episode classifier.Considering the F1 score for each dis-ease/symptom, we observed the largest improve-ment for chill.
This is because the Japanese wordfor ?chill?
has another meaning a cold air mass.When the word ?chill?
stands for a cold air massin a tweet, the subject for ?chill?
is NONE.
There-fore, the episode classifier can disambiguate themeaning of ?chill?
on the basis of the subject la-bels.
Similarly, the subject labels improved theperformance for ?fever?.In contrast, the subject labels did not improvethe performance for headache and runny nose con-siderably.
This is because the subjects for thesesymptoms are mostly FIRSTPERSON, as we sel-dom mention the symptoms of another person insuch cases.
In other words, the episode classi-fier can predict a positive label for these symptomswithout knowing the subjects of these symptoms.4 Related Work4.1 Twitter and NLPNLP researchers have addressed two major direc-tions for Twitter: adapting existing NLP technolo-gies to noisy texts and extracting useful knowl-edge from Twitter.
The former includes improvingthe accuracy of part-of-speech tagging (Gimpel etal., 2011) and named entity recognition (Plank etal., 2014), as well as normalizing ill-formed wordsinto canonical forms (Han and Baldwin, 2011;Chrupa?a, 2014).
Even though we did not incor-7For the ?predicted?
setting, first, we predicted the subjectlabels in a similar manner to five-fold cross validation, and weused the predicted labels as features for the episode classifier.porate the findings of these studies, they could bebeneficial to our work in the future.The latter has led to the development of sev-eral interesting applications besides health surveil-lance.
These include prediction of future rev-enue (Asur and Huberman, 2010) and stock mar-ket trends (Si et al, 2013), mining of public opin-ion (O?Connor et al, 2010), event extraction andsummarization (Sakaki et al, 2010; Thelwall etal., 2011; Marchetti-Bowick and Chambers, 2012;Shen et al, 2013; Li et al, 2014a), user profil-ing (Bergsma et al, 2013; Han et al, 2013; Liet al, 2014b; Zhou et al, 2014), disaster man-agement (Varga et al, 2013), and extraction ofcommon-sense knowledge (Williams and Katz,2012).
Our work can directly contribute to theseapplications, e.g., sentiment analysis, user profil-ing, event extraction, and disaster management.4.2 Semantic analysis for nounsOur work can be considered as a semantic anal-ysis that identifies an argument (subject) for adisease/symptom-related noun.
NomBank (Mey-ers et al, 2004) provides annotations of noun ar-guments in a similar manner to PropBank (Palmeret al, 2005), which provides annotations of verbs.In NomBank, nominal predicates and their argu-ments are identified: for example, ARG0 (typi-cally, subject or agent) is ?customer?
and ARG1(typically, objects, patients, themes) is ?issue?
forthe nominal predicate ?complaints?
in the sen-tence ?There have been no customer complaintsabout that issue.?
Gerber and Chai (2010) im-proved the coverage of NomBank by handling im-plicit arguments.
Some studies have addressed thetask of identifying implicit and omitted argumentsfor nominal predicates in Japanese (Komachi etal., 2007; Sasano et al, 2008).Our work shares a similar goal with the above-mentioned studies, i.e., identifying an implicitARG0 for a disease and symptom.
However, thesestudies do not regard a disease/symptom as a nom-inal predicate because they consider verb nom-inalizations as nominal predicates.
In addition,1667they use a corpus that consists of newswire text,the writing style and word usage of which differconsiderably from those of tweets.
For these rea-sons, we proposed a novel task setting for identi-fying subjects of diseases and symptoms, and webuilt an annotated corpus for developing the sub-ject classifier and analyzing the challenges of thistask.5 ConclusionIn this paper, we presented a novel approach tothe identification of subjects of various types ofdiseases and symptoms.
First, we constructedan annotated corpus based on an existing cor-pus for public surveillance.
Then, we traineda classifier for predicting the subject of a dis-ease/symptom.
The results of our experimentsshowed that the task of identifying the subjectsis independent of the type of disease/symptom.In addition, the results demonstrated the contribu-tions of our work toward identifying an episode ofa disease/symptom from a tweet.In the future, we plan to consider a greater vari-ety of diseases and symptoms in order to developapplications for public health, e.g., monitoring themental condition of individuals.
Thus, we can notonly improve the accuracy of subject identificationbut also enhance the generality of this task.AcknowledgmentsThis study was partly supported by Japan Sci-ence and Technology Agency (JST).
We are grate-ful to the anonymous referees for their construc-tive reviews.
We are also grateful to TakayukiSato and Yasunobu Asakura for their annotationefforts.
This study was inspired by Project NextNLP8, a workshop for error analysis on variousNLP tasks.
We appreciate Takenobu Tokunaga,Satoshi Sekine, and Kentaro Inui for their helpfulcomments.ReferencesEiji Aramaki, Sachiko Maskawa, and Mizuki Morita.2011.
Twitter catches the flu: Detecting influenzaepidemics using twitter.
In Proceedings of the 2011Conference on Empirical Methods in Natural Lan-guage Processing, pages 1568?1576.Sitaram Asur and Bernardo A. Huberman.
2010.
Pre-dicting the future with social media.
In Proceedings8https://sites.google.com/site/projectnextnlp/english-pageof the 2010 IEEE/WIC/ACM International Confer-ence on Web Intelligence and Intelligent Agent Tech-nology - Volume 01, WI-IAT ?10, pages 492?499,Washington, DC, USA.
IEEE Computer Society.Shane Bergsma, Mark Dredze, Benjamin Van Durme,Theresa Wilson, and David Yarowsky.
2013.Broadly improving user classification viacommunication-based name and location clus-tering on Twitter.
In Proceedings of the 2013Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 1010?1019.David Broniatowski, Michael J. Paul, and MarkDredze.
2013.
National and local influenza surveil-lance through Twitter: An analysis of the 2012-2013influenza epidemic.
PLoS ONE, 8(12):e83672.Jean Carletta.
1996.
Assessing agreement on classi-fication tasks: the kappa statistic.
Computationallinguistics, 22(2):249?254.Herman Anthony Carneiro and Eleftherios Mylonakis.2009.
Google trends: a web-based tool for real-timesurveillance of disease outbreaks.
Clinical Infec-tious Diseases, 49(10):1557?1564.Grzegorz Chrupa?a.
2014.
Normalizing tweets withedit scripts and recurrent neural embeddings.
InProceedings of the 52nd Annual Meeting of the As-sociation for Computational Linguistics (Volume 2:Short Papers), pages 680?686.Nigel Collier.
2012.
Uncovering text mining: a surveyof current work on web-based epidemic intelligence.Global Public Health: An International Journal forResearch, Policy and Practice, 7(7):731?749.Aron Culotta.
2010.
Towards detecting influenzaepidemics by analyzing Twitter messages.
In Pro-ceedings of the Workshop on Social Media Analytics(SOMA), pages 115?122.Mark Dredze, Michael J. Paul, Shane Bergsma, andHieu Tran.
2013.
Carmen: A Twitter geolocationsystem with applications to public health.
In Pro-ceedings of the AAAI Workshop on Expanding theBoundaries of Health Informatics Using AI (HIAI),pages 20?24.Matthew Gerber and Joyce Y. Chai.
2010.
BeyondNomBank: A study of implicit arguments for nom-inal predicates.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Lin-guistics, pages 1583?1592.Francesco Gesualdo, Giovanni Stilo, Eleonora Agri-cola, Michaela V. Gonfiantini, Elisabetta Pan-dolfi, Paola Velardi, and Alberto E. Tozzi.2013.
Influenza-like illness surveillance on Twit-ter through automated learning of na?ve language.PLoS One, 8(12):e82489.1668Kevin Gimpel, Nathan Schneider, Brendan O?Connor,Dipanjan Das, Daniel Mills, Jacob Eisenstein,Michael Heilman, Dani Yogatama, Jeffrey Flanigan,and Noah A. Smith.
2011.
Part-of-speech taggingfor Twitter: Annotation, features, and experiments.In Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 42?47.Bo Han and Timothy Baldwin.
2011.
Lexical normali-sation of short text messages: Makn sens a #twitter.In Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 368?378.Bo Han, Paul Cook, and Timothy Baldwin.
2013.
Astacking-based approach to Twitter user geolocationprediction.
In Proceedings of the 51st Annual Meet-ing of the Association for Computational Linguis-tics: System Demonstrations, pages 7?12.Yoshiaki Kitagawa, Mamoru Komachi, Eiji Aramaki,Naoaki Okazaki, and Hiroshi Ishikawa.
Diseaseevent detection based on deep modality analysis.In Proceedings of the 53rd Annual Meeting of theAssociation for Computational Linguistics and the7th International Joint Conference on Natural Lan-guage Processing of the Asian Federation of NaturalLanguage Processing (ACL-IJCNLP) 2015 StudentResearch Workshop.Mamoru Komachi, Ryu Iida, Kentaro Inui, and YujiMatsumoto.
2007.
Learning based argument struc-ture analysis of event-nouns in Japanese.
In Pro-ceedings of the Conference of the Pacific Associationfor Computational Linguistics (PACLING), pages120?128.Alex Lamb, Michael J. Paul, and Mark Dredze.
2013.Separating fact from fear: Tracking flu infections onTwitter.
In Proceedings of the 2013 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, pages 789?795.Vasileios Lampos and Nello Cristianini.
2010.
Track-ing the flu pandemic by monitoring the social web.In 2nd IAPR Workshop on Cognitive InformationProcessing (CIP 2010), pages 411?416.David Lazer, Ryan Kennedy, Gary King, and Alessan-dro Vespignani.
2014.
The parable of Googleflu: Traps in big data analysis.
Science,343(6176):1203?1205.Jiwei Li, Alan Ritter, Claire Cardie, and Eduard Hovy.2014a.
Major life event extraction from Twitterbased on congratulations/condolences speech acts.In Proceedings of the 2014 Conference on Em-pirical Methods in Natural Language Processing(EMNLP), pages 1997?2007.Jiwei Li, Alan Ritter, and Eduard Hovy.
2014b.Weakly supervised user profile extraction from Twit-ter.
In Proceedings of the 52nd Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 165?174.Micol Marchetti-Bowick and Nathanael Chambers.2012.
Learning for microblogs with distant supervi-sion: Political forecasting with Twitter.
In Proceed-ings of the 13th Conference of the European Chap-ter of the Association for Computational Linguistics,pages 603?612.Adam Meyers, Ruth Reeves, Catherine Macleod,Rachel Szekely, Veronika Zielinska, Brian Young,and Ralph Grishman.
2004.
The NomBankProject: An interim report.
In Proceedings of theNAACL/HLT Workshop on Frontiers in Corpus An-notation, pages 24?31.Brendan O?Connor, Ramnath Balasubramanyan,Bryan R. Routledge, , and Noah A. Smith.
2010.From tweets to polls: Linking text sentiment topublic opinion time series.
In Proceedings of theFourth International AAAI Conference on Weblogsand Social Media (ICWSM), pages 122?129.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The Proposition Bank: An annotated cor-pus of semantic roles.
Computational Linguistics,31(1):71?106.Michael J. Paul and Mark Dredze.
2011.
You are whatyou tweet: Analyzing Twitter for public health.
InProceedings of the Fifth International AAAI Confer-ence on Weblogs and Social Media (ICWSM), pages265?272.Barbara Plank, Dirk Hovy, Ryan McDonald, and An-ders S?gaard.
2014.
Adapting taggers to Twit-ter with not-so-distant supervision.
In Proceedingsof COLING 2014, the 25th International Confer-ence on Computational Linguistics: Technical Pa-pers, pages 1783?1792.Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.2010.
Earthquake shakes Twitter users: real-timeevent detection by social sensors.
In Proceedingsof the 19th international conference on World WideWeb (WWW), pages 851?860.Ryohei Sasano, Daisuke Kawahara, and Sadao Kuro-hashi.
2004.
Automatic construction of nominalcase frames and its application to indirect anaphoraresolution.
In Proceedings of the 20th internationalconference on Computational Linguistics, pages1201?1207.Ryohei Sasano, Daisuke Kawahara, and Sadao Kuro-hashi.
2008.
A fully-lexicalized probabilistic modelfor Japanese zero anaphora resolution.
In Proceed-ings of the 22nd International Conference on Com-putational Linguistics-Volume 1, pages 769?776.Chao Shen, Fei Liu, Fuliang Weng, and Tao Li.
2013.A participant-based approach for event summariza-tion using Twitter streams.
In Proceedings of the2013 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 1152?1162.1669Jianfeng Si, Arjun Mukherjee, Bing Liu, Qing Li,Huayi Li, and Xiaotie Deng.
2013.
Exploiting topicbased Twitter sentiment for stock prediction.
In Pro-ceedings of the 51st Annual Meeting of the Associa-tion for Computational Linguistics (Volume 2: ShortPapers), pages 24?29.Alessio Signorini, Alberto Maria Segre, and Philip M.Polgreen.
2011.
The use of Twitter to track levels ofdisease activity and public concern in the U.S. dur-ing the influenza A H1N1 pandemic.
PLoS ONE,6(5):e19467.Mark A. Stoov?
and Alisa E. Pedrana.
2014.
Makingthe most of a brave new world: Opportunities andconsiderations for using Twitter as a public healthmonitoring tool.
Preventive Medicine, 63:109?111.Mike Thelwall, Kevan Buckley, and Georgios Pal-toglou.
2011.
Sentiment in Twitter events.
Journalof the American Society for Information Science andTechnology, 62(2):406?418.Istv?n Varga, Motoki Sano, Kentaro Torisawa, ChikaraHashimoto, Kiyonori Ohtake, Takao Kawai, Jong-Hoon Oh, and Stijn De Saeger.
2013.
Aid isout there: Looking for help from tweets during alarge scale disaster.
In Proceedings of the 51st An-nual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 1619?1629.Paola Velardi, Giovanni Stilo, Alberto E. Tozzi, andFrancesco Gesualdo.
2014.
Twitter mining for fine-grained syndromic surveillance.
Artificial Intelli-gence in Medicine, 61(3):153?163.Jennifer Williams and Graham Katz.
2012.
Extractingand modeling durations for habits and events fromTwitter.
In Proceedings of the 50th Annual Meet-ing of the Association for Computational Linguistics(Volume 2: Short Papers), pages 223?227.Deyu Zhou, Liangyu Chen, and Yulan He.
2014.
Asimple bayesian modelling approach to event extrac-tion from Twitter.
In Proceedings of the 52nd An-nual Meeting of the Association for ComputationalLinguistics (Volume 2: Short Papers), pages 700?705.1670
