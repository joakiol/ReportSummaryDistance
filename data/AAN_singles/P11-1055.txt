Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 541?550,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsKnowledge-Based Weak Supervision for Information Extractionof Overlapping RelationsRaphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, Daniel S. WeldComputer Science & EngineeringUniversity of WashingtonSeattle, WA 98195, USA{raphaelh,clzhang,xiaoling,lsz,weld}@cs.washington.eduAbstractInformation extraction (IE) holds the promiseof generating a large-scale knowledgebase from the Web?s natural language text.Knowledge-based weak supervision, usingstructured data to heuristically label a trainingcorpus, works towards this goal by enablingthe automated learning of a potentiallyunbounded number of relation extractors.Recently, researchers have developed multi-instance learning algorithms to combat thenoisy training data that can come fromheuristic labeling, but their models assumerelations are disjoint ?
for example theycannot extract the pair Founded(Jobs,Apple) and CEO-of(Jobs, Apple).This paper presents a novel approach formulti-instance learning with overlapping re-lations that combines a sentence-level extrac-tion model with a simple, corpus-level compo-nent for aggregating the individual facts.
Weapply our model to learn extractors for NYTimes text using weak supervision from Free-base.
Experiments show that the approachruns quickly and yields surprising gains inaccuracy, at both the aggregate and sentencelevel.1 IntroductionInformation-extraction (IE), the process of generat-ing relational data from natural-language text, con-tinues to gain attention.
Many researchers dream ofcreating a large repository of high-quality extractedtuples, arguing that such a knowledge base couldbenefit many important tasks such as question an-swering and summarization.
Most approaches to IEuse supervised learning of relation-specific exam-ples, which can achieve high precision and recall.Unfortunately, however, fully supervised methodsare limited by the availability of training data and areunlikely to scale to the thousands of relations foundon the Web.A more promising approach, often called ?weak?or ?distant?
supervision, creates its own trainingdata by heuristically matching the contents of adatabase to corresponding text (Craven and Kum-lien, 1999).
For example, suppose that r(e1, e2) =Founded(Jobs,Apple) is a ground tuple in thedatabase and s =?Steve Jobs founded Apple, Inc.?is a sentence containing synonyms for both e1 =Jobs and e2 = Apple, then s may be a naturallanguage expression of the fact that r(e1, e2) holdsand could be a useful training example.While weak supervision works well when the tex-tual corpus is tightly aligned to the database con-tents (e.g., matching Wikipedia infoboxes to as-sociated articles (Hoffmann et al, 2010)), Riedelet al (2010) observe that the heuristic leads tonoisy data and poor extraction performance whenthe method is applied more broadly (e.g., matchingFreebase records to NY Times articles).
To fixthis problem they cast weak supervision as a form ofmulti-instance learning, assuming only that at leastone of the sentences containing e1 and e2 are ex-pressing r(e1, e2), and their method yields a sub-stantial improvement in extraction performance.However, Riedel et al?s model (like that ofprevious systems (Mintz et al, 2009)) assumesthat relations do not overlap ?
there cannotexist two facts r(e1, e2) and q(e1, e2) that areboth true for any pair of entities, e1 and e2.Unfortunately, this assumption is often violated;541for example both Founded(Jobs, Apple) andCEO-of(Jobs, Apple) are clearly true.
In-deed, 18.3% of the weak supervision facts in Free-base that match sentences in the NY Times 2007 cor-pus have overlapping relations.This paper presents MULTIR, a novel model ofweak supervision that makes the following contri-butions:?
MULTIR introduces a probabilistic, graphicalmodel of multi-instance learning which handlesoverlapping relations.?
MULTIR also produces accurate sentence-levelpredictions, decoding individual sentences aswell as making corpus-level extractions.?
MULTIR is computationally tractable.
Inferencereduces to weighted set cover, for which it usesa greedy approximation with worst case runningtime O(|R| ?
|S|) where R is the set of possi-ble relations and S is largest set of sentences forany entity pair.
In practice, MULTIR runs veryquickly.?
We present experiments showing that MULTIRoutperforms a reimplementation of Riedelet al (2010)?s approach on both aggregate (cor-pus as a whole) and sentential extractions.Additional experiments characterize aspects ofMULTIR?s performance.2 Weak Supervision from a DatabaseGiven a corpus of text, we seek to extract facts aboutentities, such as the company Apple or the cityBoston.
A ground fact (or relation instance), isan expression r(e) where r is a relation name, forexample Founded or CEO-of, and e = e1, .
.
.
, enis a list of entities.An entity mention is a contiguous sequence of tex-tual tokens denoting an entity.
In this paper we as-sume that there is an oracle which can identify allentity mentions in a corpus, but the oracle doesn?tnormalize or disambiguate these mentions.
We useei ?
E to denote both an entity and its name (i.e.,the tokens in its mention).A relation mention is a sequence of text (in-cluding one or more entity mentions) which statesthat some ground fact r(e) is true.
For example,?Steve Ballmer, CEO of Microsoft, spoke recentlyat CES.?
contains three entity mentions as well as arelation mention for CEO-of(Steve Ballmer,Microsoft).
In this paper we restrict our atten-tion to binary relations.
Furthermore, we assumethat both entity mentions appear as noun phrases ina single sentence.The task of aggregate extraction takes two inputs,?, a set of sentences comprising the corpus, and anextraction model; as output it should produce a setof ground facts, I , such that each fact r(e) ?
I isexpressed somewhere in the corpus.Sentential extraction takes the same input andlikewise produces I , but in addition it also producesa function, ?
: I ?
P(?
), which identifies, foreach r(e) ?
I , the set of sentences in ?
that containa mention describing r(e).
In general, the corpus-level extraction problem is easier, since it need onlymake aggregate predictions, perhaps using corpus-wide statistics.
In contrast, sentence-level extrac-tion must justify each extraction with every sentencewhich expresses the fact.The knowledge-based weakly supervised learningproblem takes as input (1) ?, a training corpus, (2)E, a set of entities mentioned in that corpus, (3) R,a set of relation names, and (4), ?, a set of groundfacts of relations in R. As output the learner pro-duces an extraction model.3 Modeling Overlapping RelationsWe define an undirected graphical model that al-lows joint reasoning about aggregate (corpus-level)and sentence-level extraction decisions.
Figure 1(a)shows the model in plate form.3.1 Random VariablesThere exists a connected component for each pair ofentities e = (e1, e2) ?
E ?
E that models all ofthe extraction decisions for this pair.
There is oneBoolean output variable Y r for each relation namer ?
R, which represents whether the ground factr(e) is true.
Including this set of binary randomvariables enables our model to extract overlappingrelations.Let S(e1,e2) ?
?
be the set of sentences whichcontain mentions of both of the entities.
For eachsentence xi ?
S(e1,e2) there exists a latent variableZi which ranges over the relation names r ?
R and,542E ?
E?RS??
(a)Steve Jobs was founderof Apple.Steve Jobs, Steve Wozniak andRonald Wayne founded Apple.Steve Jobs is CEO ofApple.founder ??
founder none0?????
?1 0 0??
??.........?????????
????????
?
?????????
(b)Figure 1: (a) Network structure depicted as plate model and (b) an example network instantiation for the pair of entitiesSteve Jobs, Apple.importantly, also the distinct value none.
Zi shouldbe assigned a value r ?
R only when xi expressesthe ground fact r(e), thereby modeling sentence-level extraction.Figure 1(b) shows an example instantiation of themodel with four relation names and three sentences.3.2 A Joint, Conditional Extraction ModelWe use a conditional probability model that definesa joint distribution over all of the extraction randomvariables defined above.
The model is undirectedand includes repeated factors for making sentencelevel predictions as well as globals factors for ag-gregating these choices.For each entity pair e = (e1, e2), define x tobe a vector concatenating the individual sentencesxi ?
S(e1,e2), Y to be vector of binary Yr randomvariables, one for each r ?
R, and Z to be the vec-tor of Zi variables, one for each sentence xi.
Ourconditional extraction model is defined as follows:p(Y = y,Z = z|x; ?
)def=1Zx?r?join(yr, z)?i?extract(zi, xi)where the parameter vector ?
is used, below, to de-fine the factor ?extract.The factors ?join are deterministic OR operators?join(yr, z)def={1 if yr = true ?
?i : zi = r0 otherwisewhich are included to ensure that the ground factr(e) is predicted at the aggregate level for the as-signment Y r = yr only if at least one of the sen-tence level assignments Zi = zi signals a mentionof r(e).The extraction factors ?extract are given by?extract(zi, xi)def= exp??
?j?j?j(zi, xi)?
?where the features ?j are sensitive to the relationname assigned to extraction variable zi, if any, andcues from the sentence xi.
We will make use of theMintz et al (2009) sentence-level features in the ex-peiments, as described in Section 7.3.3 DiscussionThis model was designed to provide a joint approachwhere extraction decisions are almost entirely drivenby sentence-level reasoning.
However, defining theY r random variables and tying them to the sentence-level variables, Zi, provides a direct method formodeling weak supervision.
We can simply train themodel so that the Y variables match the facts in thedatabase, treating the Zi as hidden variables that cantake any value, as long as they produce the correctaggregate predictions.This approach is related to the multi-instancelearning approach of Riedel et al (2010), in thatboth models include sentence-level and aggregaterandom variables.
However, their sentence levelvariables are binary and they only have a single ag-gregate variable that takes values r ?
R ?
{none},thereby ruling out overlapping relations.
Addition-ally, their aggregate decisions make use of Mintz-style aggregate features (Mintz et al, 2009), that col-lect evidence from multiple sentences, while we use543Inputs:(1) ?, a set of sentences,(2)E, a set of entities mentioned in the sentences,(3) R, a set of relation names, and(4) ?, a database of atomic facts of the formr(e1, e2) for r ?
R and ei ?
E.Definitions:We define the training set {(xi,yi)|i = 1 .
.
.
n},where i is an index corresponding to a particu-lar entity pair (ej , ek) in ?, xi contains all ofthe sentences in ?
with mentions of this pair, andyi = relVector(ej , ek).Computation:initialize parameter vector ??
0for t = 1...T dofor i = 1...n do(y?, z?)?
arg maxy,z p(y, z|xi; ?
)if y?
6= yi thenz?
?
arg maxz p(z|xi,yi; ?)??
?
+ ?
(xi, z?)?
?
(xi, z?
)end ifend forend forReturn ?Figure 2: The MULTIR Learning Algorithmonly the deterministic OR nodes.
Perhaps surpris-ing, we are still able to improve performance at boththe sentential and aggregate extraction tasks.4 LearningWe now present a multi-instance learning algo-rithm for our weak-supervision model that treats thesentence-level extraction random variables Zi as la-tent, and uses facts from a database (e.g., Freebase)as supervision for the aggregate-level variables Y r.As input we have (1) ?, a set of sentences, (2)E, a set of entities mentioned in the sentences, (3)R, a set of relation names, and (4) ?, a databaseof atomic facts of the form r(e1, e2) for r ?
R andei ?
E. Since we are using weak learning, the Y rvariables in Y are not directly observed, but can beapproximated from the database ?.
We use a proce-dure, relVector(e1, e2) to return a bit vector whosejth bit is one if rj(e1, e2) ?
?.
The vector does nothave a bit for the special none relation; if there is norelation between the two entities, all bits are zero.Finally, we can now define the training set to bepairs {(xi,yi)|i = 1 .
.
.
n}, where i is an indexcorresponding to a particular entity pair (ej , ek), xicontains all of the sentences with mentions of thispair, and yi = relVector(ej , ek).Given this form of supervision, we would like tofind the setting for ?
with the highest likelihood:O(?)
=?ip(yi|xi; ?)
=?i?zp(yi, z|xi; ?
)However, this objective would be difficult to op-timize exactly, and algorithms for doing so wouldbe unlikely to scale to data sets of the size we con-sider.
Instead, we make two approximations, de-scribed below, leading to a Perceptron-style addi-tive (Collins, 2002) parameter update scheme whichhas been modified to reason about hidden variables,similar in style to the approaches of (Liang et al,2006; Zettlemoyer and Collins, 2007), but adaptedfor our specific model.
This approximate algorithmis computationally efficient and, as we will see,works well in practice.Our first modification is to do online learninginstead of optimizing the full objective.
Define thefeature sums ?
(x, z) =?j ?
(xj , zj) which rangeover the sentences, as indexed by j.
Now, we candefine an update based on the gradient of the locallog likelihood for example i:?
logOi(?)?
?j= Ep(z|xi,yi;?
)[?j(xi, z)]?Ep(y,z|xi;?
)[?j(xi, z)]where the deterministic OR ?join factors ensure thatthe first expectation assigns positive probability onlyto assignments that produce the labeled facts yi butthat the second considers all valid sets of extractions.Of course, these expectations themselves, espe-cially the second one, would be difficult to com-pute exactly.
Our second modification is to doa Viterbi approximation, by replacing the expecta-tions with maximizations.
Specifically, we computethe most likely sentence extractions for the labelfacts arg maxz p(z|xi,yi; ?)
and the most likely ex-traction for the input, without regard to the labels,arg maxy,z p(y, z|xi; ?).
We then compute the fea-tures for these assignments and do a simple additiveupdate.
The final algorithm is detailed in Figure 2.5445 InferenceTo support learning, as described above, we needto compute assignments arg maxz p(z|x,y; ?)
andarg maxy,z p(y, z|x; ?).
In this section, we describealgorithms for both cases that use the deterministicOR nodes to simplify the required computations.Predicting the most likely joint extractionarg maxy,z p(y, z|x; ?)
can be done efficientlygiven the structure of our model.
In particular, wenote that the factors ?join represent deterministic de-pendencies between Z and Y, which when satisfieddo not affect the probability of the solution.
It is thussufficient to independently compute an assignmentfor each sentence-level extraction variable Zi, ignor-ing the deterministic dependencies.
The optimal set-ting for the aggregate variables Y is then simply theassignment that is consistent with these extractions.The time complexity is O(|R| ?
|S|).Predicting sentence level extractions given weaksupervision facts, arg maxz p(z|x,y; ?
), is morechallenging.
We start by computing extractionscores ?extract(xi, zi) for each possible extraction as-signment Zi = zi at each sentence xi ?
S, andstoring the values in a dynamic programming table.Next, we must find the most likely assignment z thatrespects our output variables y.
It turns out thatthis problem is a variant of the weighted, edge-coverproblem, for which there exist polynomial time op-timal solutions.Let G = (E ,V = VS ?
Vy) be a completeweighted bipartite graph with one node vSi ?
VS foreach sentence xi ?
S and one node vyr ?
Vy for eachrelation r ?
R where yr = 1.
The edge weights aregiven by c((vSi , vyr ))def= ?extract(xi, zi).
Our goal isto select a subset of the edges which maximizes thesum of their weights, subject to each node vSi ?
VSbeing incident to exactly one edge, and each nodevyr ?
Vy being incident to at least one edge.Exact Solution An exact solution can be obtainedby first computing the maximum weighted bipartitematching, and adding edges to nodes which are notincident to an edge.
This can be computed in timeO(|V|(|E| + |V| log |V|)), which we can rewrite asO((|R|+ |S|)(|R||S|+ (|R|+ |S|) log(|R|+ |S|))).Approximate Solution An approximate solutioncan be obtained by iterating over the nodes in Vy,????????
????????????
? ?
? ???????????
????
????????????
???????
??Figure 3: Inference of arg maxz p(Z = z|x,y) requiressolving a weighted, edge-cover problem.and each time adding the highest weight incidentedge whose addition doesn?t violate a constraint.The running time is O(|R||S|).
This greedy searchguarantees each fact is extracted at least once andallows any additional extractions that increase theoverall probability of the assignment.
Given thecomputational advantage, we use it in all of the ex-perimental evaluations.6 Experimental SetupWe follow the approach of Riedel et al (2010) forgenerating weak supervision data, computing fea-tures, and evaluating aggregate extraction.
We alsointroduce new metrics for measuring sentential ex-traction performance, both relation-independent andrelation-specific.6.1 Data GenerationWe used the same data sets as Riedel et al (2010)for weak supervision.
The data was first tagged withthe Stanford NER system (Finkel et al, 2005) andthen entity mentions were found by collecting eachcontinuous phrase where words were tagged iden-tically (i.e., as a person, location, or organization).Finally, these phrases were matched to the names ofFreebase entities.Given the set of matches, define ?
to be set of NYTimes sentences with two matched phrases, E to bethe set of Freebase entities which were mentioned inone or more sentences, ?
to be the set of Freebasefacts whose arguments, e1 and e2 were mentioned ina sentence in ?, and R to be set of relations namesused in the facts of ?.
These sets define the weaksupervision data.6.2 Features and InitializationWe use the set of sentence-level features describedby Riedel et al (2010), which were originally de-545veloped by Mintz et al (2009).
These include in-dicators for various lexical, part of speech, namedentity, and dependency tree path properties of entitymentions in specific sentences, as computed with theMalt dependency parser (Nivre and Nilsson, 2004)and OpenNLP POS tagger1.
However, unlike theprevious work, we did not make use of any featuresthat explicitly aggregate these properties across mul-tiple mention instances.The MULTIR algorithm has a single parameter T ,the number of training iterations, that must be spec-ified manually.
We used T = 50 iterations, whichperformed best in development experiments.6.3 Evaluation MetricsEvaluation is challenging, since only a small per-centage (approximately 3%) of sentences matchfacts in Freebase, and the number of matches ishighly unbalanced across relations, as we will seein more detail later.
We use the following metrics.Aggregate Extraction Let ?e be the set of ex-tracted relations for any of the systems; we com-pute aggregate precision and recall by comparing?e with ?.
This metric is easily computed but un-derestimates extraction accuracy because Freebaseis incomplete and some true relations in ?e will bemarked wrong.Sentential Extraction Let Se be the sentenceswhere some system extracted a relation and SF bethe sentences that match the arguments of a fact in?.
We manually compute sentential extraction ac-curacy by sampling a set of 1000 sentences fromSe ?
SF and manually labeling the correct extrac-tion decision, either a relation r ?
R or none.
Wethen report precision and recall for each system onthis set of sampled sentences.
These results providea good approximation to the true precision but canoverestimate the actual recall, since we did not man-ually check the much larger set of sentences whereno approach predicted extractions.6.4 Precision / Recall CurvesTo compute precision / recall curves for the tasks,we ranked the MULTIR extractions as follows.
Forsentence-level evaluations, we ordered according to1http://opennlp.sourceforge.net/RecallPrecision0.00 0.05 0.10 0.15 0.20 0.25 0.300.00.20.40.60.81.0SOLORRiedel et al, 2010MULTIRFigure 4: Aggregate extraction precision / recall curvesfor Riedel et al (2010), a reimplementation of that ap-proach (SOLOR), and our algorithm (MULTIR).the extraction factor score ?extract(zi, xi).
For aggre-gate comparisons, we set the score for an extractionY r = true to be the max of the extraction factorscores for the sentences where r was extracted.7 ExperimentsTo evaluate our algorithm, we first compare it to anexisting approach for using multi-instance learningwith weak supervision (Riedel et al, 2010), usingthe same data and features.
We report both aggregateextraction and sentential extraction results.
We theninvestigate relation-specific performance of our sys-tem.
Finally, we report running time comparisons.7.1 Aggregate ExtractionFigure 4 shows approximate precision / recall curvesfor three systems computed with aggregate metrics(Section 6.3) that test how closely the extractionsmatch the facts in Freebase.
The systems include theoriginal results reported by Riedel et al (2010) aswell as our new model (MULTIR).
We also comparewith SOLOR, a reimplementation of their algorithm,which we built in Factorie (McCallum et al, 2009),and will use later to evaluate sentential extraction.MULTIR achieves competitive or higher preci-sion over all ranges of recall, with the exceptionof the very low recall range of approximately 0-1%.
It also significantly extends the highest recallachieved, from 20% to 25%, with little loss in preci-sion.
To investigate the low precision in the 0-1% re-call range, we manually checked the ten highest con-546RecallPrecision0.0 0.1 0.2 0.3 0.4 0.5 0.60.00.20.40.60.81.0SOLORMULTIRFigure 5: Sentential extraction precision / recall curvesfor MULTIR and SOLOR.fidence extractions produced by MULTIR that weremarked wrong.
We found that all ten were true factsthat were simply missing from Freebase.
A manualevaluation, as we perform next for sentential extrac-tion, would remove this dip.7.2 Sentential ExtractionAlthough their model includes variables to modelsentential extraction, Riedel et al (2010) did not re-port sentence level performance.
To generate theprecision / recall curve we used the joint model as-signment score for each of the sentences that con-tributed to the aggregate extraction decision.Figure 4 shows approximate precision / recallcurves for MULTIR and SOLOR computed againstmanually generated sentence labels, as defined inSection 6.3.
MULTIR achieves significantly higherrecall with a consistently high level of precision.
Atthe highest recall point, MULTIR reaches 72.4% pre-cision and 51.9% recall, for an F1 score of 60.5%.7.3 Relation-Specific PerformanceSince the data contains an unbalanced number of in-stances of each relation, we also report precision andrecall for each of the ten most frequent relations.
LetSMr be the sentences where MULTIR extracted aninstance of relation r ?
R, and let SFr be the sen-tences that match the arguments of a fact about re-lation r in ?.
For each r, we sample 100 sentencesfrom both SMr and SFr and manually check accu-racy.
To estimate precision P?r we compute the ratioof true relation mentions in SMr , and to estimate re-call R?r we take the ratio of true relation mentions inSFr which are returned by our system.Table 1 presents this approximate precision andrecall for MULTIR on each of the relations, alongwith statistics we computed to measure the qual-ity of the weak supervision.
Precision is high forthe majority of relations but recall is consistentlylower.
We also see that the Freebase matches arehighly skewed in quantity and can be low quality forsome relations, with very few of them actually cor-responding to true extractions.
The approach gener-ally performs best on the relations with a sufficientlylarge number of true matches, in many cases evenachieving precision that outperforms the accuracy ofthe heuristic matches, at reasonable recall levels.7.4 Overlapping RelationsTable 1 also highlights some of the effects of learn-ing with overlapping relations.
For example, in thedata, almost all of the matches for the administra-tive divisions relation overlap with the contains re-lation, because they both model relationships for apair of locations.
Since, in general, sentences aremuch more likely to describe a contains relation, thisoverlap leads to a situation were almost none of theadministrate division matches are true ones, and wecannot accurately learn an extractor.
However, wecan still learn to accurately extract the contains rela-tion, despite the distracting matches.
Similarly, theplace of birth and place of death relations tend tooverlap, since it is often the case that people are bornand die in the same city.
In both cases, the precisionoutperforms the labeling accuracy and the recall isrelatively high.To measure the impact of modeling overlappingrelations, we also evaluated a simple, restrictedbaseline.
Instead of labeling each entity pair withthe set of all true Freebase facts, we created a datasetwhere each true relation was used to create a dif-ferent training example.
Training MULTIR on thisdata simulates effects of conflicting supervision thatcan come from not modeling overlaps.
On averageacross relations, precision increases 12 points but re-call drops 26 points, for an overall reduction in F1score from 60.5% to 40.3%.7.5 Running TimeOne final advantage of our model is the mod-est running time.
Our implementation of the547RelationFreebase Matches MULTIR#sents % true P?
R?/business/person/company 302 89.0 100.0 25.8/people/person/place lived 450 60.0 80.0 6.7/location/location/contains 2793 51.0 100.0 56.0/business/company/founders 95 48.4 71.4 10.9/people/person/nationality 723 41.0 85.7 15.0/location/neighborhood/neighborhood of 68 39.7 100.0 11.1/people/person/children 30 80.0 100.0 8.3/people/deceased person/place of death 68 22.1 100.0 20.0/people/person/place of birth 162 12.0 100.0 33.0/location/country/administrative divisions 424 0.2 N/A 0.0Table 1: Estimated precision and recall by relation, as well as the number of matched sentences (#sents) and accuracy(% true) of matches between sentences and facts in Freebase.Riedel et al (2010) approach required approxi-mately 6 hours to train on NY Times 05-06 and 4hours to test on the NY Times 07, each without pre-processing.
Although they do sampling for infer-ence, the global aggregation variables require rea-soning about an exponentially large (in the numberof sentences) sample space.In contrast, our approach required approximatelyone minute to train and less than one second to test,on the same data.
This advantage comes from thedecomposition that is possible with the determinis-tic OR aggregation variables.
For test, we simplyconsider each sentence in isolation and during train-ing our approximation to the weighted assignmentproblem is linear in the number of sentences.7.6 DiscussionThe sentential extraction results demonstrates theadvantages of learning a model that is primarilydriven by sentence-level features.
Although previ-ous approaches have used more sophisticated fea-tures for aggregating the evidence from individualsentences, we demonstrate that aggregating strongsentence-level evidence with a simple deterministicOR that models overlapping relations is more effec-tive, and also enables training of a sentence extractorthat runs with no aggregate information.While the Riedel et al approach does include amodel of which sentences express relations, it makessignificant use of aggregate features that are primar-ily designed to do entity-level relation predictionsand has a less detailed model of extractions at theindividual sentence level.
Perhaps surprisingly, ourmodel is able to do better at both the sentential andaggregate levels.8 Related WorkSupervised-learning approaches to IE were intro-duced in (Soderland et al, 1995) and are too nu-merous to summarize here.
While they offer highprecision and recall, these methods are unlikely toscale to the thousands of relations found in text onthe Web.
Open IE systems, which perform self-supervised learning of relation-independent extrac-tors (e.g., Preemptive IE (Shinyama and Sekine,2006), TEXTRUNNER (Banko et al, 2007; Bankoand Etzioni, 2008) and WOE (Wu and Weld, 2010))can scale to millions of documents, but don?t outputcanonicalized relations.8.1 Weak SupervisionWeak supervision (also known as distant- or self su-pervision) refers to a broad class of methods, butwe focus on the increasingly-popular idea of usinga store of structured data to heuristicaly label a tex-tual corpus.
Craven and Kumlien (1999) introducedthe idea by matching the Yeast Protein Database(YPD) to the abstracts of papers in PubMed andtraining a naive-Bayes extractor.
Bellare and Mc-Callum (2007) used a database of BibTex recordsto train a CRF extractor on 12 bibliographic rela-tions.
The KYLIN system aplied weak supervisionto learn relations from Wikipedia, treating infoboxesas the associated database (Wu and Weld, 2007);Wu et al (2008) extended the system to use smooth-ing over an automatically generated infobox taxon-548omy.
Mintz et al (2009) used Freebase facts to train100 relational extractors on Wikipedia.
Hoffmannet al (2010) describe a system similar to KYLIN,but which dynamically generates lexicons in orderto handle sparse data, learning over 5000 Infoboxrelations with an average F1 score of 61%.
Yaoet al (2010) perform weak supervision, while usingselectional preference constraints to a jointly reasonabout entity types.The NELL system (Carlson et al, 2010) can alsobe viewed as performing weak supervision.
Its ini-tial knowledge consists of a selectional preferenceconstraint and 20 ground fact seeds.
NELL thenmatches entity pairs from the seeds to a Web cor-pus, but instead of learning a probabilistic model,it bootstraps a set of extraction patterns using semi-supervised methods for multitask learning.8.2 Multi-Instance LearningMulti-instance learning was introduced in order tocombat the problem of ambiguously-labeled train-ing data when predicting the activity of differ-ent drugs (Dietterich et al, 1997).
Bunescu andMooney (2007) connect weak supervision withmulti-instance learning and extend their relationalextraction kernel to this context.Riedel et al (2010), combine weak supervisionand multi-instance learning in a more sophisticatedmanner, training a graphical model, which assumesonly that at least one of the matches between thearguments of a Freebase fact and sentences in thecorpus is a true relational mention.
Our model maybe seen as an extension of theirs, since both modelsinclude sentence-level and aggregate random vari-ables.
However, Riedel et al have only a single ag-gregate variable that takes values r ?
R ?
{none},thereby ruling out overlapping relations.
We havediscussed the comparison in more detail throughoutthe paper, including in the model formulation sec-tion and experiments.9 ConclusionWe argue that weak supervision is promising methodfor scaling information extraction to the level whereit can handle the myriad, different relations on theWeb.
By using the contents of a database to heuris-tically label a training corpus, we may be able toautomatically learn a nearly unbounded number ofrelational extractors.
Since the processs of match-ing database tuples to sentences is inherently heuris-tic, researchers have proposed multi-instance learn-ing algorithms as a means for coping with the result-ing noisy data.
Unfortunately, previous approachesassume that all relations are disjoint ?
for exam-ple they cannot extract the pair Founded(Jobs,Apple) and CEO-of(Jobs, Apple), becausetwo relations are not allowed to have the same argu-ments.This paper presents a novel approach for multi-instance learning with overlapping relations thatcombines a sentence-level extraction model with asimple, corpus-level component for aggregating theindividual facts.
We apply our model to learn extrac-tors for NY Times text using weak supervision fromFreebase.
Experiments show improvements for bothsentential and aggregate (corpus level) extraction,and demonstrate that the approach is computation-ally efficient.Our early progress suggests many interesting di-rections.
By joining two or more Freebase tables,we can generate many more matches and learn morerelations.
We also wish to refine our model in orderto improve precision.
For example, we would liketo add type reasoning about entities and selectionalpreference constraints for relations.
Finally, we arealso interested in applying the overall learning ap-proaches to other tasks that could be modeled withweak supervision, such as coreference and namedentity classification.The source code of our system, its out-put, and all data annotations are available athttp://cs.uw.edu/homes/raphaelh/mr.AcknowledgmentsWe thank Sebastian Riedel and Limin Yao for shar-ing their data and providing valuable advice.
Thismaterial is based upon work supported by a WRF /TJ Cable Professorship, a gift from Google and bythe Air Force Research Laboratory (AFRL) underprime contract no.
FA8750-09-C-0181.
Any opin-ions, findings, and conclusion or recommendationsexpressed in this material are those of the author(s)and do not necessarily reflect the view of the AirForce Research Laboratory (AFRL).549ReferencesMichele Banko and Oren Etzioni.
2008.
The tradeoffsbetween open and traditional relation extraction.
InProceedings of 46th Annual Meeting of the Associ-ation for Computational Linguistics (ACL-08), pages28?36.Michele Banko, Michael J. Cafarella, Stephen Soderland,Matthew Broadhead, and Oren Etzioni.
2007.
Openinformation extraction from the web.
In Proceedingsof the 20th International Joint Conference on ArtificialIntelligence (IJCAI-07), pages 2670?2676.Kedar Bellare and Andrew McCallum.
2007.
Learn-ing extractors from unlabeled text using relevantdatabases.
In Sixth International Workshop on Infor-mation Integration on the Web.Razvan Bunescu and Raymond Mooney.
2007.
Learningto extract relations from the web using minimal super-vision.
In Proceedings of the 45th Annual Meeting ofthe Association for Computational Linguistics (ACL-07).Andrew Carlson, Justin Betteridge, Bryan Kisiel, BurrSettles, Estevam R. Hruschka Jr., and Tom M.Mitchell.
2010.
Toward an architecture for never-ending language learning.
In Proceedings of the AAAIConference on Artificial Intelligence (AAAI-10).Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and experi-ments with perceptron algorithms.
In Proceedings ofthe 2002 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP-2002).Mark Craven and Johan Kumlien.
1999.
Constructingbiological knowledge bases by extracting informationfrom text sources.
In Proceedings of the Seventh Inter-national Conference on Intelligent Systems for Molec-ular Biology, pages 77?86.Thomas G. Dietterich, Richard H. Lathrop, and Toma?sLozano-Pe?rez.
1997.
Solving the multiple instanceproblem with axis-parallel rectangles.
Artificial Intel-ligence, 89:31?71, January.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by gibbs sam-pling.
In Proceedings of the 43rd Annual Meeting ofthe Association for Computational Linguistics (ACL-05), pages 363?370.Raphael Hoffmann, Congle Zhang, and Daniel S. Weld.2010.
Learning 5000 relational extractors.
In Pro-ceedings of the 48th Annual Meeting of the Associ-ation for Computational Linguistics (ACL-10), pages286?295.Percy Liang, A.
Bouchard-Co?te?, Dan Klein, and BenTaskar.
2006.
An end-to-end discriminative approachto machine translation.
In International Conference onComputational Linguistics and Association for Com-putational Linguistics (COLING/ACL).Andrew McCallum, Karl Schultz, and Sameer Singh.2009.
Factorie: Probabilistic programming via imper-atively defined factor graphs.
In Neural InformationProcessing Systems Conference (NIPS).Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-sky.
2009.
Distant supervision for relation extrac-tion without labeled data.
In Proceedings of the 47thAnnual Meeting of the Association for ComputationalLinguistics (ACL-2009), pages 1003?1011.Joakim Nivre and Jens Nilsson.
2004.
Memory-baseddependency parsing.
In Proceedings of the Conferenceon Natural Language Learning (CoNLL-04), pages49?56.Sebastian Riedel, Limin Yao, and Andrew McCallum.2010.
Modeling relations and their mentions withoutlabeled text.
In Proceedings of the Sixteenth Euro-pean Conference on Machine Learning (ECML-2010),pages 148?163.Yusuke Shinyama and Satoshi Sekine.
2006.
Preemptiveinformation extraction using unrestricted relation dis-covery.
In Proceedings of the Human Language Tech-nology Conference of the North American Chapterof the Association for Computation Linguistics (HLT-NAACL-06).Stephen Soderland, David Fisher, Jonathan Aseltine, andWendy G. Lehnert.
1995.
Crystal: Inducing a concep-tual dictionary.
In Proceedings of the Fourteenth In-ternational Joint Conference on Artificial Intelligence(IJCAI-1995), pages 1314?1321.Fei Wu and Daniel S. Weld.
2007.
Autonomously se-mantifying wikipedia.
In Proceedings of the Inter-national Conference on Information and KnowledgeManagement (CIKM-2007), pages 41?50.Fei Wu and Daniel S. Weld.
2008.
Automatically refin-ing the wikipedia infobox ontology.
In Proceedings ofthe 17th International Conference on World Wide Web(WWW-2008), pages 635?644.Fei Wu and Daniel S. Weld.
2010.
Open informationextraction using wikipedia.
In The Annual Meeting ofthe Association for Computational Linguistics (ACL-2010), pages 118?127.Limin Yao, Sebastian Riedel, and Andrew McCallum.2010.
Collective cross-document relation extractionwithout labelled data.
In Proceedings of the 2010 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP-2010), pages 1013?1023.Luke Zettlemoyer and Michael Collins.
2007.
Onlinelearning of relaxed CCG grammars for parsing to log-ical form.
In Proceedings of the Joint Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning(EMNLP-CoNLL-2007).550
