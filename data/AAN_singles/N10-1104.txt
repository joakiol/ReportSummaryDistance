Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 701?704,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsA Hybrid Morphologically Decomposed Factored Language Models forArabic LVCSRAmr El-Desoky, Ralf Schlu?ter, Hermann NeyLehrstuhl fu?r Informatik 6 ?
Computer Science DepartmentRWTH Aachen University, D-52056 Aachen, Germany{desoky,schluter,ney}@cs.rwth-aachen.deAbstractIn this work, we try a hybrid methodology forlanguage modeling where both morphologicaldecomposition and factored language model-ing (FLM) are exploited to deal with the com-plex morphology of Arabic language.
At theend, we are able to obtain from 3.5% to 7.0%relative reduction in word error rate (WER)with respect to a traditional full-words sys-tem, and from 1.0% to 2.0% relative WER re-duction with respect to a non-factored decom-posed system.1 IntroductionArabic language is characterized by a complex mor-phological structure where different kinds of pre-fixes and suffixes are appended to the word stemsproducing a very large number of inflectional forms.This leads to poor language model (LM) probabil-ity estimates, and thus high LM perplexities (PPLs)causing problems in large vocabulary continuousspeech recognition (LVCSR).
One successful ap-proach to deal with this problem is to consider LMsincluding morphologically decomposed words.
An-other approach is to use the factored language mod-els (FLMs) which are powerful models that com-bine multiple sources of information and efficientlyintegrate them via a complex backoff mechanism(Bilmes and Kirchhoff, 2003).Morphological decomposition is successfullyused for Arabic LMs in several previous works.Some are based on linguistic knowledge, and oth-ers are based on unsupervised methods.
Some of thelinguistic methods are based on the Buckwalter Ara-bic Morphological Analyzer (BAMA) like in (Lamelet al, 2008).
Alternatively, in our previous work(El-Desoky et al, 2009), we use the MorphologicalAnalyzer and Disambiguator for Arabic (MADA)(Habash and Rambow, 2007).
On the other side,most of the unsupervised methods are based on theminimum description length principle (MDL) like in(Creutz et al, 2007).Another type of models is the FLM, in whichwords are viewed as vectors of K factors, so thatwt := {f1:Kt }.
A factor could be any feature of theword such as morphological class, stem, root or evena semantic feature.
An FLM is a model over factors,i.e., p(f1:Kt |f1:Kt?1 , f1:Kt?2 , ..., f1:Kt?n+1), which could bereformed as a product of probabilities of the formp(f |f1, f2, ..., fN ).
The main idea of the model is tobackoff to other factors when some word n-gram isnot observed in the training data, thus improving theprobability estimates.In this work we try to combine the strengthsof morphological decomposition and factored lan-guage modeling.
Therefore, language models withfactored morphemes are used.
For this purpose, theLM training data are processed such that full-wordsare decomposed into prefix-stem-suffix format withdifferent added features.
We compare our approachwith the standard full-word, decomposed word, andfactored full-word n-gram approaches.2 Factorization and DecompositionWe use MADA 2.0 in order to perform morphologi-cal analysis and attach a complete set of morpholog-ical tags to Arabic words in context.
From those tags701we derive three different features.
Moreover, we de-rive a fourth feature based on the root of the wordgenerated by ?Sebawai?
(Darwish, 2002).
The listof features is:?
?W?
(Word): word surface form.?
?L?
(Lexeme): word lexeme.?
?M?
(Morph): morphological description.?
?P?
(Pattern): word after subtracting root.The LM training corpora are processed so thatwords are replaced by the factored representation asrequired by SRILM-FLM extensions (Kirchhoff etal., 2008).
Then, word decomposition is performedbased on MADA as described in our previous publi-cation (El-Desoky et al, 2009).3 FLM topologiesIn order to obtain a good performance via FLMs, weneed to optimize the FLM parameters: the combi-nation of the conditioning factors, backoff path, andsmoothing options.
For this purpose, we use a Ge-netic Algorithm based FLM optimization tool (GA-FLM) developed by Kirchhoff (2006) which seeks tominimize the PPL over some held-out text.
Further-more, we apply some manual optimization to finetune the FLM parameters.
For memory limitations,we only use factors up to 2 previous time slots (tri-gram like models).
Finally, we come up with a setof competing FLMs with rather close PPLs.
In Ta-ble 1, we record the PPLs measured for some held-out text.
The first column gives the combination ofthe parent factors.
So that, FLM1 corresponds tothe model P (Wt|Wt?1,Wt?2), which is the FLMequivalent of the standard tri-gram LM (our base-line model), while FLM2 & FLM3 correspond tothe model P (Wt|Wt?1,Mt?1, Lt?1, Pt?1,Wt?2),however FLM4 & FLM5 correspond to the modelP (Wt|Wt?1,Mt?1, Lt?1,Wt?2,Mt?2, Lt?2).
The?gtmin?
refers to the count threshold that is suffi-cient to have a language model hit at some node ofthe the backoff graph (for exact topologies, contactthe first author).
From Table 1, comparing PPLs(non-normalized) across factored and non-factoredLMs, we see that using more factors other than thenormal word could help decreasing the PPL.
This istrue for all the used types of vocabulary units.vocabularyFLMx parent factors FW PD FD1: W1 W2 (baseline) 302.6 284.1 82.7W1 M1 L1 P1 W22: gtmin = 1 306.2 296.9 83.23: gtmin = 2-4 290.9 279.1 79.8W1 M1 L1 W2 M2 L24: gtmin = 1 300.2 291.1 83.65: gtmin = 2-4 294.5 283.7 81.1Table 1: perplexities of the FLMs using vocabularies:(FW: 70k full-words; PD: partially decomposed with 20kful-words + 50k morphemes; FD: 70k fully decomposed).FLMx parent factors WER [%]1: W1 W2 (baseline) 20.4W1 M1 L1 P1 W22: gtmin = 1 20.23: gtmin = 2-4 20.4W1 M1 L1 W2 M2 L24: gtmin = 1 19.95: gtmin = 2-4 20.3Table 2: WERs using FLMs based on 70k full-words.In order to select the best FLM topology, we runa simple one pass recognition for a small internaldev corpus derived from GALE data sets, consistsof 40 minutes of audio data recorded during Januaryto March 2007.
The acoustic models are within-word tri-phone models trained using 1100h of au-dio material.
The basic acoustic models are trainedbased on Maximum Likelihood (ML) method.
Then,a discriminative training based on Minimum PhoneError (MPE) criterion is performed to enhance themodels.
A 70k full-words lexicon is used.
TheFLM training data consists of 206 Million runningfull-words.
A standard bi-gram LM based on full-words is used to generate N-best lists, then N-bestlist rescoring is performed using the different FLMtopologies shown in Table 1.
We start by N = 1000-best down to 3-best sentences.
Using N = 10 alwaysgives the best results.
The recognition WERs arerecorded in Table 2.
The least WER is obtained withFLM4.
We note that the best FLM does not corre-spond to the least PPL.
This is because a higher ?gt-min?
value causes more backoff in cases of insuffi-cient data leading to better estimates.
Therefore, weselect FLM4 for the coming experiments.7024 Experimental SetupOur recognition system is close to the one describedin section 3.
However, we use within and across-word models at different recognition passes.
In ad-dition, we use 70k or 256k lexicon of full-words orpartially decomposed words.
Alternatively, we eval-uate the results on the GALE 2007 development andevaluation sets (dev07: 2.5h; eval07: 4h).
Our rec-ognizer works in 3 passes.
In the first pass, within-word acoustic models are used with no adaptation,along with a standard bi-gram LM to generate lat-tices, followed by a standard tri-gram or 4-gram LMrescoring of lattices.
The second pass does the same,but it uses across-word models with ConstrainedMaximum Likelihood Linear Regression (CMLLR)adaptation.
Then, a third pass with additional Max-imum Likelihood Linear Regression (MLLR) adap-tation is performed, using a standard bi-gram LM togenerate lattices or N-best lists.
Then, one of the fol-lowing is performed: 1) lattice rescoring using stan-dard tri-gram or 4-gram LM, 2) N-best list rescoringusing FLMs based on full-words, partially or fullydecomposed words.5 ExperimentsIn this section, we record our recognition resultsfor: 1) systems based on full-words, and 2) systemsbased on decomposed words.
Also, we introduceadditional results for larger lexicon sizes.5.1 Systems Based on Full-wordsIn this section, we present the WERs of our recogni-tion systems based on full-words.
Where, during thesearch, we use a lexicon of 70K full-words.
In thefirst 2 passes, we use a standard bi-gram LM to gen-erate lattices, followed by a standard tri-gram LMrescoring of lattices.
However, in the third pass, wegenerate both lattices and N-best lists based on thesame bi-gram LM.
The final lattices and N-best listsare rescored using different LMs as shown in Table3.
In case we perform N-best list rescoring with aFLM, the N-best lists are processed to produce fac-tored representation, followed by partial or full de-composition as previously described in section 2.It is clear from Table 3 that the least WER isachieved when using N-best list rescoring using afull-words based FLM.
This gives an absolute im-LM rescoring (3rd pass) Dev07 [%]tri-gram lattice resc.
(baseline) 16.54-gram lattice resc.
16.3N-best FLM resc.
:+ FW (original N-best) 15.7+ PD (decomposed N-best) 15.8+ FD (decomposed N-best) 16.0Table 3: WERs for 70k full-words systems.LM rescoring (3rd pass) Dev07 [%]tri-gram lattice resc.
(baseline) 14.74-gram lattice resc.
14.5N-best FLM resc.
:+ FW (re-joint N-best) 14.6+ PD (original N-best) 14.3+ FD (decomposed N-best) 14.4Table 4: WERs for 70k partially decomposed systems(20k full-words + 50k morphemes).provement of 0.8% (about 4.8% relative) comparedto the standard tri-gram lattice rescoring.
On theother hand, we have 0.6% absolute improvement(about 3.7% relative) compared to the standard 4-gram lattice rescoring.
Decomposition does not helpin this case.
This is because the original N-best listsare generated in full-words format, whose decom-position might not lead to better LM scores.
For thisreason, we expect that it is better to start with a de-composed LM for lattice and N-best generation.5.2 Systems Based on Decomposed WordsThis section introduces the WERs of our systemsbased on decomposed words.
We use a similar setupas in section 5.1.
However, we use a lexicon and abi-gram LM based on a 70k partially decomposedwords (20k full-words + 50k morphemes).
Table 4presents the results.
As expected, we get the bestWER when using N-best list rescoring with a FLMbased on partially decomposed words.
An absoluteimprovement of 0.4% (2.7% relative) is achievedcompared to the new baseline.
Compared to the oldbaseline of Table 3, we get an absolute improvementof 2.2% (13.3% relative).5.3 Larger Lexicon SizesNow, we increase the size of our lexicon to 256kpartially decomposed words (20k full-words + 236k703Dev07 Eval07System [%] [%]traditional full-words 14.9 16.5partially decomposed+ 4-gram lat.
resc.
(baseline) 14.2 16.1+ N-best FLM resc.
:+ FW (re-joint N-best) 14.1 -+ PD (original N-best) 13.9 15.9+ FD (decomposed N-best) 14.0 -Table 5: WERs for 256k full-words, and partially decom-posed systems (20k full-words + 236k morphemes).70k vocabularies 256k vocabulariesCorpus FW PD FD FW PD FDDev07 3.65 1.33 0.75 1.36 0.51 0.24Eval07 4.82 1.94 1.13 1.85 0.64 0.41Table 6: OOVs [%] of the used vocabularies.morphemes).
In addition, we use a standard 4-gram LM for rescoring the bi-gram lattices in thefirst 2 passes.
To complete our comparisons, werecord the WERs using traditional 256k full-wordslexicon, standard bi-gram search, and standard 4-gram LM for lattice rescoring, with no decomposi-tion or factorization.
In Table 5, we see that the im-provement persists for the larger lexicon.
Comparedto the new baseline, the 256k decomposed systemachieves WER reductions of [dev07: 0.3% absolute(2.1% relative); eval07: 0.2% absolute (1.2% rela-tive)] when using N-best list rescoring with a FLMbased on partially decomposed words.
Moreover, itimproves over the traditional full-words by [dev07:1.0% absolute (6.7% relative); eval07: 0.6% abso-lute (3.6% relative)].
The out-of-vocabulary rates(OOVs) are given in Table 6.
It is worth noting thatusing fully decomposed lexicons as well as higherorder LMs could not improve WERs, this we previ-ously proved in (El-Desoky et al, 2009).6 ConclusionsWe have introduced a hybrid approach to Ara-bic language modeling.
Our approach combinesthe strengths of both morphological decompositionand factored language modeling.
Thus, we haveused language models with factored morphemes.We have compared our approach to traditional ap-proaches like: standard full-word n-grams, standarddecomposed n-grams, and full-word based factoredlanguage models.
Finally, we could achieve someimprovements over all the traditional approaches.Nevertheless, we have only considered the use offactored language models in the rescoring phase.AcknowledgmentsThis material is based upon work supported by theDARPA under Contract No.
HR0011-06-C-0023.Any opinions, findings and conclusions expressed inthis material are those of the authors and do not nec-essarily reflect the views of DARPA.ReferencesJ.
Bilmes and K. Kirchhoff.
2003.
Factored languagemodels and generalized parallel backoff.
In Proc.
Hu-man Language Technology Conf.
of the North Ameri-can Chapter of the ACL, volume 2, pages 4 ?
6, Ed-monton, Canada, May.M.
Creutz, T. Hirsimki, M. Kurimo, A. Puurula,J.
Pylkknen, V. Siivola, M. Varjokallio, E. Arisoy,M.
Saraclar, and A. Stolcke.
2007.
Morph-basedspeech recognition and modeling of out-of-vocabularywords across languages.
ACM Transactions on Speechand Language Processing, 5(1), December.K.
Darwish.
2002.
Building a shallow Arabic morpho-logical analyzer in one day.
In ACL workshop on Com-putational approaches to semitic languages, Philadel-phia, PA, USA, July.A.
El-Desoky, C. Gollan, D. Rybach, R. Schlu?ter, andH.
Ney.
2009.
Investigating the use of morphologicaldecomposition and diacritization for improving ArabicLVCSR.
In Interspeech, pages 2679 ?
2682, Brighton,UK, September.N.
Habash and O. Rambow.
2007.
Arabic diacritiza-tion through full morphological tagging.
In Proc.
Hu-man Language Technology Conf.
of the North Ameri-can Chapter of the ACL, volume Companion, pages 53?
56, Rochester, NY, USA, April.K.
Kirchhoff, D. Vergyri, J. Bilmes, K. Duh, and A. Stol-cke.
2006.
Morphology-based language modeling forconversational Arabic speech recognition.
ComputerSpeech and Language, 20(4):589 ?
608, October.K.
Kirchhoff, J. Bilmes, and K. Duh.
2008.
Factoredlanguage model tutorial.
Technical report, Departmentof Electrical Engineering, University of Washington,Seattle, Washington, USA, February.L.
Lamel, A. Messaoudi, and J.L Gauvain.
2008.
Investi-gating morphological decomposition for transcriptionof Arabic broadcast news and broadcast conversationdata.
In Interspeech, volume 1, pages 1429 ?
1432,Brisbane, Australia, September.704
