BBN Real-Time Speech Recognition DemonstrationsSteve Austin, Rusty Bobrow, Dan Ellard, Robert Ingria, John Makhoul,Long Nguyen, Pat Peterson, Paul Placeway, Richard SchwartzBBN Systems and TechnologiesCambridge MA 02138Typically, real-time speech recognition - if achieved at all- is accomplished either by greatly simplifying the process-ing to be done, or by the use of special-purpose hardware.Each of these approaches has obvious problems.
The for-mer results in a substantial loss in accuracy, while the latteroften results in obsolete hardware being developed at greatexpense and delay.Starting in 1990 \[1\] \[2\] we have taken a different approachbased on modifying the algorithms to provide increasedspeed without loss in accuracy.
Our goal has been to usecommercially available off-the-shelf (COTS) hardware toperform speech recognition.
Initially, this meant using work-stations with powerful but standard signal processing boardsacting as accelerators.
However, even these signal process-ing boards have two significant disadvantages:1.
They often cost as much as the workstation they areplugged into..
The interface between each board and workstation iscomplicated, and always different for each combinationof workstation and board.To make speech recognition available to a broad base ofusers at an affordable cost, we have eliminated these disad-vantages by developing algorithms that are able to operatein real-time on COTS workstations without requiring addi-tional add-on hardware and without decreasing recognitionspeed and accuracy.
An additional advantage is that we areable to benefit from the improvements in workstation priceand performance, with very minimal porting effort.
TheBBN RUBY TM system, a robust commercialization f theBYBLOS TM speech recognition technology, is the result ofthis development effort.
At the workshop, we demonstratedtwo example systems that employ the RUBY speech recog-nition system.Both demonstrations run on Silicon Graphics workstations(Personal IRIS 4D/35 and Indigo), which contain a built-in programmable A/D-D/A.
The signal processing and vec-tor quantization, which runs in a separate process fromthe recognition search, communicates with the recognitionsearch via network sockets.
We have reduced the computa-tion required for this front end processing to the point whereit requires little enough of the CPU so that there is enoughleft over to perform the more expensive search in real time.Since accuracy is our primary concern, we have verifiedthat this signal processing results in the same accuracy asour previous ignal processing software.1.
REAL-TIME ATIS SYSTEMThe ATIS demonstration i tegrated BBN's DELPHI natu-ral language understanding system with the RUBY speechrecognition component.
RUBY is used as a black-box, con-trolled entirely through an application programmers inter-face (API).
The natural anguage component is our currentresearch system, which runs as a separate process.
Bothprocesses run on the same processor, although not at thesame time.
The NL processing is performed strictly afterthe speech recognition, since competing for the same pro-cessor could not make it faster.
(If two separate processorsare available, the processing can be overlapped as describedin \[1\].
)The speech recognition component performs three separatesteps.
First, it uses a forward Viterbi-like computation tofindthe 1-Best speech answer in real time as the user is speak-ing.
Very shortly after the user stops speaking, it displaysthe 1-best answer.
Then, it performs a backwards N-Bestpass to find the N-Best hypotheses.
Finally, we rescore achof the text hypotheses using a higher-order n-gram statsti-cal class grammar and reorder the hypotheses accordingly.In this application, we use a trigram model; this resconngcomputation requires very minimal processing.
(Note thatat this time we have omitted the acoustic rescoring stage inwhich we could rescore with between-word triphone modelsand semi-continuous HMM models.)
After that, the N-Bestanswers are sent to DELPHI which searches the N-Best an-swers for an interpretable s ntence and then finds the answerto the question.Our goal in this effort was to produce a system that was asclose to real-time as possible.
However, it is of little interestto us to demonstrate r al-time at the expense of accuracy.We have verified that the speech system, when operating250in real-time, degrades only marginally from the very highperformance figures reported in the evaluation.
In particular,the word recognition error rate degrades from 9.6% to 11.7%- a 20% increase in error rate.Normally, the system displays the 1-Best answer within ahalf second of detection of the end of speech.
This is fastenough that it feels instantaeous.
(This speed is in markedcontrast with the other near real-time demonstrations shownat the workshop, which all required from two to five timesreal time, resulting in a delay that was at least equal to thelength of the utterance - usually several seconds.)
Nextit performs the N-Best recognition, and then interprets theanswer.
The N-Best recognition usually runs in less than1 second, since it is sped up immensely by the forwardpass.
The time required for the interpretation depends onhow many of the speech answers must be considered, andon how complicated a retrieval results.
In most cases, thisphase requires only another second or two.To operate the demonstration system, the user clicks on the"Push to Talk" window at the top of the screen.
The statuswill change from "ready" to "listening".
As soon as theuser begins speaking, the status will change to "beginning ofspeech".
When (s)he stops speaking, it will change to "endof speech".
The system briefly displays its status as "First-Best" and "N-Best" while it completes these phases of therecognition.
Finally, the system will "Interpret" the query,which includes all parsing, semantic interpretation, discoursemodeling, and data retrieval ~om the actual database.The answer displayed in the speech window first containsthe answer from the 1-Best pass, then the top-choice of theN-Best, and finally the sentence chosen by DELPHI.
The N-Best hypotheses are displayed at the bottom of the screen forinformation only.
Then, the answer to the query is displayedunder the recognized sentence.
If the answer to be displayedis larger than will fit in the window, it can be scrolled.A history of the previous four sentences are shown in awindow that can scroll all the way back through the previousquestions.
If the user wishes to review any of the previousanswers in more detail, they may mouse on the arrow tothe right of the question, which brings back a copy of thequestion and answer in a separate window that may be placedand sized as desired, and then used for reference as long asneeded.To the right of the main display, we also display the dis-course state, which consists of the set of constraints thatwere used to answer the query.
In this way, the user canverify how much of the previous context was actually used toanswer the question.
As each successive query is interpreted,the system may add new constraints, modify old ones, orcompletely reset the context.
The user may also reset thediscourse state by speaking either of the commands, "BEGINSCENARIO", "END SCENARIO", or "NEW SCENARIO".2.
REAL-T IME SPEECH RECOGNIT IONFOR A IR -TRAFF IC  CONTROLAPPL ICAT IONSWe also demonstrated RUBY configured for air-traffic on-trol (ATC) applications.
This system is notable for itsvery high speed, accuracy, robustness, and reliability, allnecessary qualities for ATC applications requiring human-machine interaction.
Such applications include training sys-tems, where the trainee controller interacts with a simu-lated world, and operational environments, where a con-troller's interaction with a pilot could automatically generatea database retrieval request for flight plan information.In this demonstration, the system extracts the aircraft flightidentification from an utterance as soon as that informationhas been spoken.
For example, if the controller says, "Deltathree fifty seven descend and maintain 2000", the flight in-formation could be captured for display or other uses by thetime the controller has said the first syllable of "descend".To achieve this immediate response, the speech recognitiondetects when the controller has completed the flight identi-fier and is speaking the rest of the utterance.
This requiresa different process than is usually used for speech recog-nition.
Normally we wait until the end of the sentence todetermine the most likely word string for the complete utter-ance.
For this application, the system stops the recognitionprocess as soon as it determines that it is most likely to havethe complete flight information.Another unique capability that is demonstrated here is thecapability to reject the flight ID if it is not in a specificclosed set.
Again, this is done by explicitly modeling thelikelihood that the user has spoken a flight ID other than theset that is expected at any given time.
Further developmentof these systems is continuing.REFERENCES1.
Austin, S., P. Peterson, P. Placeway, R. Schwartz, J. Vander-grift, "Toward a Real-Time Spoken Language System UsingCommercial Hardware", Proceedings of the DARPA Speechand Natural Language Workshop, Morgan Kaufmann Pub-lishers, Inc., Jun.
1990, pp.
72-77.2.
Schwartz, R., S. Austin, "Efficient, High-Performance Algo-rithms for N-Best Search", Proceedings of the DARPA Speechand Natural Language Workshop, Morgan Kaufmann Publish-ers, Inc., Jun.
1990, pp.
6-11.251
