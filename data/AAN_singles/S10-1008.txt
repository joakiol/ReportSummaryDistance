Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 45?50,Uppsala, Sweden, 15-16 July 2010.c?2010 Association for Computational LinguisticsSemEval-2010 Task 10:Linking Events and Their Participants in DiscourseJosef Ruppenhofer and Caroline SporlederComputational LinguisticsSaarland University{josefr,csporled}@coli.uni-sb.deRoser MoranteCNTSUniversity of AntwerpRoser.Morante@ua.ac.beCollin BakerICSIBerkeley, CA 94704collin@icsi.berkeley.eduMartha PalmerDepartment of LinguisticsUniversity of Colorado at Bouldermartha.palmer@colorado.eduAbstractWe describe the SemEval-2010 sharedtask on ?Linking Events and Their Partic-ipants in Discourse?.
This task is an ex-tension to the classical semantic role label-ing task.
While semantic role labeling istraditionally viewed as a sentence-internaltask, local semantic argument structuresclearly interact with each other in a largercontext, e.g., by sharing references to spe-cific discourse entities or events.
In theshared task we looked at one particular as-pect of cross-sentence links between ar-gument structures, namely linking locallyuninstantiated roles to their co-referentsin the wider discourse context (if suchco-referents exist).
This task is poten-tially beneficial for a number of NLP ap-plications, such as information extraction,question answering or text summarization.1 IntroductionSemantic role labeling (SRL) has been defined asa sentence-level natural-language processing taskin which semantic roles are assigned to the syntac-tic arguments of a predicate (Gildea and Jurafsky,2002).
Semantic roles describe the function of theparticipants in an event.
Identifying the seman-tic roles of the predicates in a text allows knowingwho did what to whom when where how, etc.However, semantic role labeling as it is cur-rently defined misses a lot of information due tothe fact that it is viewed as a sentence-internaltask.
Hence, relations between different local se-mantic argument structures are disregarded.
Thisview of SRL as a sentence-internal task is partlydue to the fact that large-scale manual annotationprojects such as FrameNet1and PropBank2typ-ically present their annotations lexicographicallyby lemma rather than by source text.It is clear that there is an interplay between lo-cal argument structure and the surrounding dis-course (Fillmore, 1977).
In early work, Palmer etal.
(1986) discussed filling null complements fromcontext by using knowledge about individual pred-icates and tendencies of referential chaining acrosssentences.
But so far there have been few attemptsto find links between argument structures acrossclause and sentence boundaries explicitly on thebasis of semantic relations between the predicatesinvolved.
Two notable exceptions are Fillmore andBaker (2001) and Burchardt et al (2005).
Fillmoreand Baker (2001) analyse a short newspaper arti-cle and discuss how frame semantics could benefitdiscourse processing but without making concretesuggestions of how to model this.
Burchardt et al(2005) provide a detailed analysis of the links be-tween the local semantic argument structures in ashort text; however their system is not fully imple-mented either.With the shared task, we aimed to make a firststep towards taking SRL beyond the domain ofindividual sentences by linking local semantic ar-gument structures to the wider discourse context.The task addresses the problem of finding fillersfor roles which are neither instantiated as directdependents of our target predicates nor displacedthrough long-distance dependency or coinstantia-tion constructions.
Often a referent for an unin-stantiated role can be found in the wider context,i.e.
in preceding or following sentences.
An ex-ample is given in (1), where the CHARGES role1http://framenet.icsi.berkeley.edu/2http://verbs.colorado.edu/?mpalmer/projects/ace.html45(ARG2 in PropBank) of cleared is left empty butcan be linked to murder in the previous sentence.
(1) In a lengthy court case the defendant wastried for murder.
In the end, he wascleared.Another very rich example is provided by (2),where, for instance, the experiencer and the ob-ject of jealousy are not overtly expressed as depen-dents of the noun jealousy but can be inferred to beWatson and the speaker, Holmes, respectively.
(2) Watson won?t allow that I know anythingof art but that is mere jealousy because ourviews upon the subject differ.This paper is organized as follows.
In Section 2we define how the concept of Null Instantiationis understood in the task.
Section 3 describes thetasks to be performed, and Section 4, how theyare evaluated.
Section 5 presents the participantsystems, and Section 6, their results.
Finally, inSection 7, we put forward some conclusions.2 Null InstantiationsThe theory of null complementation used here isthe one adopted by FrameNet, which derives fromthe work of Fillmore (1986).3Briefly, omissionsof core arguments of predicates are categorizedalong two dimensions, the licensor and the in-terpretation they receive.
The idea of a licensorrefers to the fact that either a particular lexical itemor a particular grammatical construction must bepresent for the omission of a frame element (FE)to occur.
For instance, the omission of the agent in(3) is licensed by the passive construction.
(3) No doubt, mistakes were made0Protagonist.The omission is a constructional omission be-cause it can apply to any predicate with an appro-priate semantics that allows it to combine with thepassive construction.
On the other hand, the omis-sion in (4) is lexically specific: the verb arrive al-lows the Goal to be unspecified but the verb reach,also a member of the Arriving frame, does not.
(4) We arrived 0Goalat 8pm.3Palmer et al?s (1986) treatment of uninstantiated ?essen-tial roles?
is very similar (see also Palmer (1990)).The above two examples also illustrate the sec-ond major dimension of variation.
Whereas, in (3)the protagonist making the mistake is only existen-tially bound within the discourse (instance of in-definite null instantiation, INI), the Goal locationin (4) is an entity that must be accessible to speakerand hearer from the discourse or its context (def-inite null instantiation, DNI).
Finally, note thatthe licensing construction or lexical item fully andreliably determines the interpretation.
Whereasmissing by-phrases have always an indefinite in-terpretation, whenever arrive omits the Goal lexi-cally, the Goal has to be interpreted as definite, asit is in (4).The import of this classification to the task hereis that we will concentrate on cases of DNI, bethey licensed lexically or constructionally.3 Description of the Task3.1 TasksWe originally intended to offer the participants achoice of two different tasks: a full task, in whichthe test set was only annotated with gold stan-dard word senses (i.e., frames) for the target wordsand the participants had to perform role recogni-tion/labeling and null instantiation linking, and aNI only task, in which the test set was alreadyannotated with gold standard semantic argumentstructures and the participants only had to recog-nize definite null instantiations and find links toantecedents in the wider context (NI linking).However, it turned out that the basic semanticrole labeling task was already quite challengingfor our data set.
Previous shared tasks have shownthat frame-semantic SRL of running text is a hardproblem (Baker et al, 2007), partly due to the factthat running text is bound to contain many framesfor which no or little annotated training data areavailable.
In our case the difficulty was increasedbecause our data came from a new genre and do-main (i.e., crime fiction, see Section 3.2).
Hence,we decided to add standard SRL, i.e., role recogni-tion and labeling, as a third task (SRL only).
Thistask did not involve NI linking.3.2 DataThe participants were allowed to make use of a va-riety of data sources.
We provided a training setannotated with semantic argument structure andnull instantiation information.
The annotationswere originally made using FrameNet-style and46later mapped semi-automatically to PropBank an-notations, so that participants could choose whichframework they wanted to work in.
The data for-mats we used were TIGER/SALSA XML (Erkand Pad?o, 2004) (FrameNet-style) and a modifiedCoNLL-format (PropBank-style).
As it turnedout, all participants chose to work on FrameNet-style annotations, so we will not describe the Prop-Bank annotation in this paper (see Ruppenhofer etal.
(2009) for more details).FrameNet-style annotation of full text is ex-tremely time-consuming.
Since we also had to an-notate null instantiations and co-reference chains(for evaluation purposes, see Section 4), we couldonly make available a limited amount of data.Hence, we allowed participants to make use of ad-ditional data, in particular the FrameNet and Prop-Bank releases.4We envisaged that the participantswould want to use these additional data sets totrain SRL systems for the full task and to learnsomething about typical fillers for different rolesin order to solve the NI linking task.
The anno-tated data sets we made available were meant toprovide additional information, e.g., about the typ-ical distance between an NI and its filler and abouthow to distinguish DNIs and INIs.We annotated texts from two of Arthur ConanDoyle?s fiction works.
The text that served astraining data was taken from ?The Adventure ofWisteria Lodge?.
Of this lengthy, two-part storywe annotated the second part, titled ?The Tiger ofSan Pedro?.
The test set was made up of the lasttwo chapters of ?The Hound of the Baskervilles?.We chose fiction rather than news because we be-lieve that fiction texts with a linear narrative gen-erally contain more context-resolvable NIs.
Theyalso tend to be longer and have a simpler structurethan news texts, which typically revisit the samefacts repeatedly at different levels of detail (in theso-called ?inverted pyramid?
structure) and whichmix event reports with commentary and evalua-tion, thus sequencing material that is understoodas running in parallel.
Fiction texts should lendthemselves more readily to a first attempt at inte-grating discourse structure into semantic role la-beling.
We chose Conan Doyle?s work becausemost of his books are not subject to copyright any-more, which allows us to freely release the anno-tated data.
Note, however, that this choice of data4For FrameNet we provided an intermediate release,FrameNet 1.4 alpha, which contained more frames and lexi-cal units than release 1.3.means that our texts come from a different domainand genre than many of the examples in FrameNetand PropBank as well as making use of a some-what older variety of English.5Table 1 provides basic statistics of the data sets.The training data had 3.1 frames per sentence andthe test data 3.2, which is lower than the 8.8 framesper sentence in the test data of the 2007 SemEvaltask on Frame Semantic Structure Extraction.6Wethink this is mainly the result of switching to a do-main different from the bulk of what FrameNethas made available in the way of full-text anno-tation.
In doing so, we encountered many newframes and lexical units for which we could notourselves create the necessary frames and pro-vide lexicographic annotations.
The statistics alsoshow that null-instantiation is relatively common:in the training data, about 18.7% of all FEs areomitted, and in the test set, about 18.4%.
Of theDNIs, 80.9% had an antecedent in the trainingdata, and 74.2% in the test data.To ensure a high quality of the annotations, bothdata sets were annotated by more than one personand then adjudicated.
The training set was an-notated independently by two experienced anno-tators and then adjudicated by the same two peo-ple.
The test set was annotated by three annota-tors and then adjudicated by the two experiencedannotators.
Throughout the annotation and adju-dication process, we discussed difficult cases andalso maintained a wiki.
Additionally, we created asoftware tool that checked the consistency of ourannotations against the frame, frame element andFE-relation specifications of FrameNet and alertedannotators to problems with their annotations.
Theaverage agreement (F-score) for frame assignmentfor pairs of annotators on the two chapters in thetest set ranges from 0.7385 to 0.7870.
The agree-ment of individual annotators with the adjudicatedgold standard ranges from 0.666 to 0.798.
Giventhat the gold standard for the two chapters features228 and 229 different frame types, respectively,this level of agreement seems quite good.5While PropBank provides annotations for the Penn Tree-bank and is thus news-based, the lexicographic annotationsin FrameNet are extracted from the BNC, a balanced cor-pus.
The FrameNet full-text annotations, however, only coverthree domains: news, travel guides, and nuclear proliferationreports.6The statistics in Table 1 and all our discussion of thedata includes only instances of semantic frames and ignoresthe instances of the Coreference, Support, and Relativizationframes, which we labeled on the data as auxiliary informa-tion.47data set sentences tokens frame inst.
frame types overt FEs DNIs (resolved) INIstrain 438 7,941 1,370 317 2,526 303 (245) 277test 525 9,131 1,703 452 3,141 349 (259) 361Table 1: Statistics for the provided data setsFor the annotation of NIs and their links to thesurrounding discourse we created new guidelinesas this was a novel annotation task.
We adoptedideas from the annotation of co-reference informa-tion, linking locally unrealized roles to all men-tions of the referents in the surrounding discourse,where available.
We marked only identity rela-tions but not part-whole or bridging relations be-tween referents.
The set of unrealized roles un-der consideration includes only the core argumentsbut not adjuncts (peripheral or extra-thematic rolesin FrameNet?s terminology).
Possible antecedentsare not restricted to noun phrases but include allconstituents that can be (local) role fillers forsome predicate plus complete sentences (whichcan sometimes fill roles such as MESSAGE).4 EvaluationAs noted above, we allowed participants to ad-dress three different tasks: SRL only, NI only,full task.
For role recognition and labeling weused a standard evaluation set-up, i.e., accuracy forrole labeling and precision, recall, F-Score for rolerecognition.The NI linkings were evaluated slightly differ-ently.
In the gold standard, we identified refer-ents for null instantiations in the discourse con-text.
In some cases, more than one referent mightbe appropriate, e.g., because the omitted argumentrefers to an entity that is mentioned multiple timesin the context.
In this case, a system is given creditif the NI is linked to any of these expressions.
Toachieve this we create equivalence sets for the ref-erents of NIs (by annotating coreference chains).If the NI is linked to any item in the equivalenceset, the link is counted as a true positive.
We canthen define NI linking precision as the numberof all true positive links divided by the number oflinks made by a system, and NI linking recall asthe number of true positive links divided by thenumber of links between an NI and its equivalenceset in the gold standard.
NI linking F-Score isthen the harmonic mean between NI linking preci-sion and recall.Since it may sometimes be difficult to deter-mine the correct extent of the filler of an NI, wescore an automatic annotation as correct if it in-cludes the head of the gold standard filler in thepredicted filler.
However, in order to not favor sys-tems which link NIs to very large spans of text tomaximize the likelihood of linking to a correct ref-erent, we introduce a second evaluation measure,which computes the overlap (Dice coefficient) be-tween the words in the predicted filler (P) of an NIand the words in the gold standard one (G):NI linking overlap =2|P ?G||P | + |G|(5)Example (6) illustrates this point.
The verbwon in the second sentence evokes the Fin-ish competition frame whose COMPETITION roleis omitted.
From the context it is clear that thecompetition role is semantically filled by their firstTV debate (head: debate) and last night?s debate(head: debate) in the previous sentences.
Thesetwo expressions form the equivalence set for theCOMPETITION role in the last sentence.
Any sys-tem that would predict a linkage to a filler thatcovers the head of either of these two expressionswould score a true positive for this NI.
However,a system that linked to last night?s debate wouldhave an NI linking overlap of 1 (i.e., 2*3/(3+3))while a system linking the whole second sentenceLast night?s debate was eagerly anticipated to theNI would have an overlap of 0.67 (i.e., 2*3/(6+3))(6) US presidential rivals Republican JohnMcCain and Democrat Barack Obamahave yesterday evening attacked eachother over foreign policy and the econ-omy, in [their first TV debate]Competition.
[Last night?s debate]Competitionwas ea-gerly anticipated.
Two national flashpolls suggest that [Obama]CompetitorwonFinish competition0Competition.5 Participating SystemsWhile a fair number of people expressed an inter-est in the task and 26 groups or individuals down-loaded the data sets, only three groups submitted48results for evaluation.
Feedback from the teamsthat downloaded the data suggests that this wasdue to coinciding deadlines and to the difficultyand novelty of the task.
Only the SEMAFORgroup addressed the full task, using a pipeline ofargument recognition followed by NI identifica-tion and resolution.
Two groups (GETARUNS++and SEMAFOR) tackled the NI only task, andalso two groups, the SRL only task (CLR and SE-MAFOR7).All participating systems were built upon ex-isting systems for semantic processing whichwere modified for the task.
Two of the groups,GETARUNS++ and CLR, employed relativelydeep semantic processing, while the third, SE-MAFOR, employed a shallower probabilistic sys-tem.
Different approaches were taken for NI link-ing.
The SEMAFOR group modeled NI linking asa variant of role recognition and labeling by ex-tending the set of potential arguments beyond thelocally available arguments to also include nounphrases from the previous sentence.
The systemthen uses, among other information, distributionalsemantic similarity between the heads of potentialarguments and role fillers in the training data.
TheGETARUNS++ group applied an existing systemfor deep semantic processing, anaphora resolutionand recognition of textual entailment, to the task.The system analyzes the sentences and assigns itsown set of labels, which are subsequently mappedto frame semantic categories.
For more details ofthe participating systems please consult the sepa-rate system papers.6 Results and Analysis6.1 SRL TaskArgument Recognition LabelPrec.
Rec.
F1 Acc.SHA 0.6332 0.3884 0.4812 0.3471SEM 0.6528 0.4674 0.5448 0.4184CLR 0.6702 0.1121 0.1921 0.1093Table 2: Shalmaneser (SHA), SEMAFOR (SEM)and CLR performance on the SRL task (acrossboth chapters)The results on the SRL task are shown in Table2.
To get a better sense of how good the perfor-mance of the submitted systems was on this task,7For SEMAFOR, this was the first step of their pipeline.we applied the Shalmaneser statistical semanticparser (Erk and Pad?o, 2006) to our test data andreport the results.
Note, however, that we used aShalmaneser trained only on FrameNet version 1.3which is different from the version 1.4 alpha thatwas used in the task, so its results are lower thanwhat can be expected with release 1.4 alpha.We observe that although the SEMAFOR andthe CLR systems score a higher precision thanShalmaneser for argument recognition, the SE-MAFOR system scores considerably higher recallthan Shalmaneser, whereas the CLR system scoresa much lower recall.6.2 NI TaskTackling the resolution of NIs proved to be a dif-ficult problem due to a variety of factors.
First,the NI sub-task was completely new and involvesseveral steps of linguistic processing.
It also isinherently difficult in that a given FE is not al-ways omitted with the same interpretation.
Forinstance, the Content FE of the Awareness frameevoked by know is interpreted as indefinite inthe blog headline More babbling about what itmeans to know but as definite in a discourselike Don?t tell me you didn?t know!.
Second,prior to this SemEval task there was no full-texttraining data available that contained annotationswith all the kinds of information that is relevantto the task, namely overt FEs, null-instantiatedFEs, resolutions of null-instantiations, and coref-erence.
Third, the data we used also representeda switch to a new domain compared to existingFrameNet full-text annotation, which comes fromnewspapers, travel guides, and the nuclear pro-liferation domain.
Our most frequent frame wasObservable bodyparts, whereas it is Weapons inFrameNet full-text.
Fourth, it was not well un-derstood at the beginning of the task that, in cer-tain cases, FrameNet?s null-instantiation annota-tions for a given FE cannot be treated in isolationof the annotations of other FEs.
Specifically, null-instantiation annotations interact with the set of re-lations between core FEs that FrameNet uses in itsanalyses.
As an example, consider the CoreSet re-lation, which specifies that from a set of core FEsat least one must be instantiated overtly, thoughmore of them can be.
As long as one of the FEsin the set is expressed overtly, null-instantiation isnot annotated for the other FEs in the set.
Forinstance, in the Statement frame, the two FEs49Topic and Message are in one CoreSet and thetwo FEs Speaker and Medium are in another.
Ifa frame instance occurs with an overt Speaker andan overt Topic, the Medium and Message FEs arenot marked as null-instantiated.
Automatic sys-tems that treat each core FE separately, may pro-pose DNI annotations for Medium and Message,resulting in false positives.Therefore, we think that the evaluation that weinitially defined was too demanding for a noveltask.
It would have been better to give sepa-rate scores for 1) ability to recognize when a coreFE has to be treated as null-instantiated; 2) abil-ity to distinguish INI and DNI; and 3) ability tofind antecedents.
The systems did have to tacklethese steps anyway and an analysis of the sys-tem output shows that they did so with differentsuccess.
The two chapters of our test data con-tained a total of 710 null instantiations, of which349 were DNI and 361 INI.
The SEMAFOR sys-tem recognized 63.4% (450/710) of the cases ofNI, while the GETARUNS++ system found only8.0% (57/710).
The distinction between DNI andINI proved very difficult, too.
Of the NIs thatthe SEMAFOR system correctly identified, 54.7%(246/450) received the correct interpretation type(DNI or INI).
For GETARUNS++, the percentageis higher at 64.2% (35/57), but also based on fewerproposed classifications.
A simple majority-classbaseline gives a 50.8% accuracy.
Interestingly, theSEMAFOR system labeled many more INIs thanDNIs, thus often misclassifying DNIs as INI.
TheGETARUNS++ system applied both labels aboutequally often.7 ConclusionIn this paper we described the SemEval-2010shared task on ?Linking Events and Their Partic-ipants in Discourse?.
The task is novel, in that ittackles a semantic cross-clausal phenomenon thathas not been treated before in a task, namely, link-ing locally uninstantiated roles to their coreferentsat the text level.
In that sense the task representsa first step towards taking SRL beyond the sen-tence level.
A new corpus of fiction texts has beenannotated for the task with several types of seman-tic information: semantic argument structure, co-reference chains and NIs.
The results scored bythe systems in the NI task and the feedback fromparticipant teams shows that the task was more dif-ficult than initially estimated and that the evalua-tion should have focused on more specific aspectsof the NI phenomenon, rather than on the com-pleteness of the task.
Future work will focus onmodeling the task taking this into account.AcknowledgementsJosef Ruppenhofer and Caroline Sporleder are supportedby the German Research Foundation DFG (under grant PI154/9-3 and the Cluster of Excellence Multimodal Comput-ing and Interaction (MMCI), respectively).
Roser Morante?sresearch is funded by the GOA project BIOGRAPH of theUniversity of Antwerp.
We would like to thank Jinho Choi,Markus Dr?ager, Lisa Fuchs, Philip John Gorinski, RussellLee-Goldman, Ines Rehbein, and Corinna Schorr for theirhelp with preparing the data and/or implementing softwarefor the task.
Thanks also to the SemEval-2010 Chairs KatrinErk and Carlo Strapparava for their support during the taskorganization period.ReferencesC.
Baker, M. Ellsworth, K. Erk.
2007.
SemEval-2007Task 19: Frame semantic structure extraction.
InProceedings of SemEval-07.A.
Burchardt, A. Frank, M. Pinkal.
2005.
Building textmeaning representations from contextually relatedframes ?
A case study.
In Proceedings of IWCS-6.K.
Erk, S. Pad?o.
2004.
A powerful and versatile XMLformat for representing role-semantic annotation.
InProceedings of LREC-2004.K.
Erk, S. Pad?o.
2006.
Shalmaneser - a flexible tool-box for semantic role assignment.
In Proceedings ofLREC-06.C.
Fillmore, C. Baker.
2001.
Frame semantics for textunderstanding.
In Proc.
of the NAACL-01 Workshopon WordNet and Other Lexical Resources.C.
Fillmore.
1977.
Scenes-and-frames semantics, lin-guistic structures processing.
In A. Zampolli, ed.,Fundamental Studies in Computer Science, No.
59,55?88.
North Holland Publishing.C.
Fillmore.
1986.
Pragmatically controlled zeroanaphora.
In Proceedings of the Twelfth AnnualMeeting of the Berkeley Liguistics Society.D.
Gildea, D. Jurafsky.
2002.
Automatic label-ing of semantic roles.
Computational Linguistics,28(3):245?288.M.
Palmer, D. Dahl, R. Passonneau, L. Hirschman,M.
Linebarger, J. Dowding.
1986.
Recovering im-plicit information.
In Proceedings of ACL-1986.M.
Palmer.
1990.
Semantic Processing for Finite Do-mains.
CUP, Cambridge, England.J.
Ruppenhofer, C. Sporleder, R. Morante, C. Baker,M.
Palmer.
2009.
Semeval-2010 task 10: Linkingevents and their participants in discourse.
In TheNAACL-HLT 2009 Workshop on Semantic Evalua-tions: Recent Achievements and Future Directions(SEW-09).50
