Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 590?595,Dublin, Ireland, August 23-24, 2014.SU-FMI: System Description for SemEval-2014 Task 9on Sentiment Analysis in TwitterBoris Velichkov?, Borislav Kapukaranov?, Ivan Grozev?, Jeni Karanesheva?, Todor Mihaylov?,Yasen Kiprov?, Georgi Georgiev?
?, Ivan Koychev?
?, Preslav Nakov?
?AbstractWe describe the submission of the teamof the Sofia University to SemEval-2014Task 9 on Sentiment Analysis in Twit-ter.
We participated in subtask B, wherethe participating systems had to predictwhether a Twitter message expresses pos-itive, negative, or neutral sentiment.
Wetrained an SVM classifier with a linearkernel using a variety of features.
Weused publicly available resources only, andthus our results should be easily replicable.Overall, our system is ranked 20th out of50 submissions (by 44 teams) based on theaverage of the three 2014 evaluation datascores, with an F1-score of 63.62 on gen-eral tweets, 48.37 on sarcastic tweets, and68.24 on LiveJournal messages.1 IntroductionWe describe the submission of the team of theSofia University, Faculty of Mathematics and In-formatics (SU-FMI) to SemEval-2014 Task 9 onSentiment Analysis in Twitter (Rosenthal et al.,2014).
?Sofia University, bobby.velichkov@gmail.com?Sofia University, b.kapukaranov@gmail.com?Sofia University, iigrozev@gmail.com?Sofia University, j.karanesheva@gmail.com?Sofia University, tbmihailov@gmail.com?Sofia University, yasen.kiprov@gmail.com?
?Ontotext, g.d.georgiev@gmail.com?
?Sofia University, koychev@fmi.uni-sofia.bg?
?Qatar Computing Research Institute,pnakov@qf.org.qaThis work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/This SemEval challenge had two subtasks:?
subtask A (term-level) asks to predict the sen-timent of a phrase inside a tweet;?
subtask B (message-level) asks to predict theoverall sentiment of a tweet message.In both subtasks, the sentiment can be positive,negative, or neutral.
Here are some examples:?
positive: Gas by my house hit $3.39!!!!
I?mgoing to Chapel Hill on Sat.
:)?
neutral: New York Giants: Game-by-GamePredictions for the 2nd Half of the Seasonhttp://t.co/yK9VTjcs?
negative: Why the hell does Selma haveschool tomorrow but Parlier clovis & othersdon?t??
negative (sarcastic): @MetroNorth wall towall people on the platform at South Nor-walk waiting for the 8:08.
Thanks for the Sat.Sched.
Great senseBelow we first describe our preprocessing, fea-tures and classifier in Section 2.
Then, we discussour experiments, results and analysis in Section 3.Finally, we conclude with possible directions forfuture work in Section 4.2 MethodOur approach is inspired by the highest scoringteam in 2013, NRC Canada (Mohammad et al.,2013).
We reused many of their resources.1Our system consists of two main submodules,(i) feature extraction in the framework of GATE(Cunningham et al., 2011), and (ii) machine learn-ing using SVM with linear kernels as implementedin LIBLINEAR2(Fan et al., 2008).1http://www.umiacs.umd.edu/?saif/WebPages/Abstracts/NRC-SentimentAnalysis.htm2http://www.csie.ntu.edu.tw/?cjlin/liblinear/5902.1 PreprocessingWe integrated a pipeline of various resources fortweet analysis that are already available in GATE(Bontcheva et al., 2013) such as a Twitter tok-enizer, a sentence splitter, a hashtag tokenizer, aTwitter POS tagger, a morphological analyzer, andthe Snowball3stemmer.We further implemented in GATE some shal-low text processing components in order to handlenegation contexts, emoticons, elongated words,all-caps words and punctuation.
We also addedcomponents to find words and phrases containedin sentiment lexicons, as well as to annotate wordswith word cluster IDs using the lexicon built atCMU,4which uses the Brown clusters (Brown etal., 1992) as implemented5by (Liang, 2005).2.2 Features2.2.1 Sentiment lexicon featuresWe used several preexisting lexicons, both manu-ally designed and automatically generated:?
Minqing Hu and Bing Liu opinion lexicon(Hu and Liu, 2004): 4,783 positive and 2,006negative terms;?
MPQA Subjectivity Cues Lexicon (Wilson etal., 2005): 8,222 terms;?
Macquarie Semantic Orientation Lexicon(MSOL) (Mohammad et al., 2009): 30,458positive and 45,942 negative terms;?
NRC Emotion Lexicon (Mohammad et al.,2013): 14,181 terms with specified emotion.For each lexicon, we find in the tweet the termsthat are listed in it, and then we calculate the fol-lowing features:?
Negative terms count;?
Positive terms count;?
Positive negated terms count;?
Positive/negative terms count ratio;?
Sentiment of the last token;?
Overall sentiment terms count.3http://snowball.tartarus.org/4http://www.ark.cs.cmu.edu/TweetNLP/cluster_viewer.html5http://github.com/percyliang/brown-clusterWe further used the following lexicons:?
NRC Hashtag Sentiment Lexicon: list ofwords and their associations with positiveand negative sentiment (Mohammad et al.,2013): 54,129 unigrams, 316,531 bigrams,480,010 pairs, and 78 high-quality positiveand negative hashtag terms;?
Sentiment140 Lexicon: list of words with as-sociations to positive and negative sentiments(Mohammad et al., 2013): 62,468 unigrams,677,698 bigrams, 480,010 pairs;?
Stanford Sentiment Treebank: contains239,231 evaluated words and phrases.
If aword or a phrase was found in the tweet, wetook the given sentiment label.For the NRC Hashtag Sentiment Lexicon andthe Sentiment140 Lexicon, we calculated the fol-lowing features for unigrams, bigrams and pairs:?
Sum of positive terms?
sentiment;?
Sum of negative terms?
sentiment;?
Sum of the sentiment for all terms in thetweet;?
Sum of negated positive terms?
sentiment;?
Negative/positive terms ratio;?
Max positive sentiment;?
Min negative sentiment;?
Max sentiment of a term.We used different features for the two lexicongroups because their contents differ.
The first fourlexicons provide a discrete sentiment value foreach word.
In contrast, the following two lexiconsoffer numeric sentiment scores, which allows fordifferent feature types such as sums and min/maxscores.Finally, we manually built a new lexicon withall emoticons we could find, where we assigned toeach emoticon a positive or a negative label.
Wethen calculated four features: number of positiveand negative emoticons in the tweet, and whetherthe last token is a positive or a negative emoticon.5912.2.2 Tweet-level featuresWe use the following tweet-level features:?
All caps: the number of words with all char-acters in upper case;?
Hashtags: the number ot hashtags in thetweet;?
Elongated words: the number of words withcharacter repetitions.2.2.3 Term-level featuresWe used the following term-level features:?
Word n-grams: presence or absence of 1-grams, 2-grams, 3-grams, 4-grams, and 5-grams.
We add an NGRAM prefix to each n-gram.
Unfortunately, the n-grams increasethe feature space greatly and contribute tohigher sparseness.
They also slow downtraining dramatically.
That is why our finalsubmission only includes 1-grams.?
Character n-grams: presence or absence ofone, two, three, four and five-character pre-fixes and suffixes of all words.
We add a PREor SUF prefix to each character n-gram.?
Negations: the number of negated contexts.We define a negated context as a segmentof a tweet that starts with a negation word(e.g., no, shouldnt) from our custom gazetteerand ends with one of the punctuation marks:,, ., :, ;, !, ?.
A negated context affects then-gram and the lexicon features: we add aNEG suffix to each word following the nega-tion word, e.g., perfect becomes perfect NEG.?
Punctuation: the number of contiguous se-quences of exclamation marks, of questionmarks, of either exclamation or questionmarks, and of both exclamation and questionmarks.
Also, whether the last token containsan exclamation or a question mark (excludingURLs).?
Stemmer: the stem of each word, excludingURLs.
We add a STEM prefix to each stem.?
Lemmatizer: the lemma of each word, ex-cluding URLs.
We add a LEMMA prefix toeach lemma.
We use the built-in GATE Mor-phological analyser as our lemmatizer.?
Word and word bigram clusters: wordclusters have been shown to improve the per-formance of supervised NLP models (Turianet al., 2010).
We use the word clusters builtby CMU?s NLP toolkit, which were producedover a collection of 56 million English tweets(Owoputi et al., 2012) and built using thePercy Liang?s HMM-based implementation6of Brown clustering (Liang, 2005; Brown etal., 1992), which group the words into 1,000hierarchical clusters.
We use two featuresbased on these clusters:?
presence/absence of a word in a wordcluster;?
presence/absence of a bigram in a bi-gram cluster.?
POS tagging: Social media are generallyhard to process using standard NLP tools,which are typically developed with newswiretext in mind.
Such standard tools are nota good fit for Twitter messages, which aretoo brief, contain typos and special word-forms.
Thus, we used a specialized POStagger, TwitIE, which is available in GATE(Bontcheva et al., 2013), and which we in-tegrated in our pipeline.
It provides (i) atokenizer specifically trained to handle smi-lies, user names, URLs, etc., (ii) a normal-izer to correct slang and misspellings, and(iii) a POS tagger that uses the Penn Treebanktagset, but is optimized for tweets.
Using theTwitIE toolkit, we performed POS taggingand we extracted all POS tag types that wecan find in the tweet together with their fre-quencies as features.2.3 ClassifierFor classification, we used the above features anda support vector machine (SVM) classifier as im-plemented in LIBLINEAR.
This is a very scal-able implementation of SVM that does not supportkernels, and is suitable for classification on largedatasets with a large number of features.
This isparticularly useful for text classification, where thenumber of features is very large, which means thatthe data is likely to be linearly separable, and thususing kernels is not really necessary.
We scaled theSVM input and we used L2-regularization duringtraining.6https://github.com/percyliang/brown-cluster5923 Experiments, Results, Analysis3.1 Experimental setupAt development time, we trained on train-2013,tuned the C value of SVM on dev-2013, and eval-uated on test-2013 (Nakov et al., 2013).
For oursubmission, we trained on train-2013+dev-2013,and we evaluated on the 2014 test dataset pro-vided by the organizers.
This dataset contains twoparts and a total of five datasets: (a) progress test(the Twitter and SMS test datasets for 2013), and(b) new test datasets (from Twitter, from Twitterwith sarcasm, and from LiveJournal).
We usedC=0.012, which was best on development.3.2 Official resultsDue to our very late entering in the competition,we have only managed to perform a small num-ber of experiments, and we only participated insubtask B.
We were ranked 20th out of 50 sub-missions; our official results are shown in Table 1.The numbers after our score are the delta to thebest solution.
We have also included a rankingamong 2014 participant systems on the 2013 datasets, released by the organizers.Data Category F1-score (best) Rankingtweets2014 63.62 (6.23) 23sarcasm2014 48.34 (9.82) 19LiveJournal2014 68.23 (6.60) 21tweets2013 60.96 (9.79) 29SMS2013 61.67 (8.61) 162014 mean 60.07 (7.55) 20Table 1: Our submitted system for subtask B.3.3 AnalysisTables 2 and 3 analyze the impact of the individualfeatures.
They show the F1-scores and the losswhen a feature or a group of features is removed;we show the impact on all test datasets, both from2013 and from 2014.
The exception here is the all+ ngrams row, which contains our scores if we hadused the n-grams feature group.The features are sorted by their impact onthe Twitter2014 test set.
We can see that thethree most important feature groups are POS tags,word/bigram clusters, and lexicons.We can further see that although the overall lex-icon feature group is beneficial, some of the lex-icons actually hurt the 2014 score and we wouldhave been better off without them.These are the Sentiment140 lexicon, the Stan-ford Sentiment Treebank and the NRC Emotionlexicon.
The highest gain we get is from the lex-icons of Minqing Hu and Bing Liu.
It must benoted that using lexicons with good results ap-parently depends on the context, e.g., the Senti-ment140 lexicon seems to be helping a lot withthe LiveJournal test dataset, but it hurts the Sar-casm score by a sizeable margin.Another interesting observation is that eventhough including the n-gram feature group is per-forming notably better on the Twitter2013 testdataset, it actually worsens performance on all2014 test sets.
Had we included it in our results,we would have scored lower.The negation context feature brings little in re-gards to regular tweets or LiveJournal text, but itheavily improves our score on the Sarcasm tweets.It is unclear why our results differ so much fromthose of the NRC-Canada team in 2013 since ourfeatures are quite similar.
We attribute the differ-ence to the fact that some of the lexicons we useactually hurt our score as we mentioned above.Another difference could be that last year?s NRCsystem uses n-grams, which we have disabled asthey lowered our scores.
Last but not least, therecould be bugs lurking in our feature representationthat additionally lower our results.3.4 Post-submission improvementsFirst, we did more extensive experiments to val-idate our classifier?s C value.
We found that thebest value for C is actually 0.08 instead of ouroriginal proposal 0.012.Then, we experimented further with our lexi-con features and we removed the following ones,which resulted in significant improvement overour submitted version:?
Sentiment of the last token for NRC Emotion,MSOL, MPQA, and Bing Liu lexicons;?
Max term positive, negative and sentimentscores for unigrams of Sentiment140 andNRC Sentiment lexicons;?
Max term positive, negative and sentimentscores for bigrams of Sentiment140 and NRCSentiment lexicons;?
Max term positive, negative and sentimentscores for hashtags of Sentiment140 andNRC Sentiment lexicons.593Feature Diff SMS2013 SMS2013 delta Twitter2013 Twitter2013 deltasubmitted features 61.67 60.96no POS tags 54.73 -6.94 52.32 -8.64no word clusters 58.06 -3.61 55.44 -5.52all lex removed 59.94 -1.73 58.35 -2.61no Hu-Liu lex 60.56 -1.11 60.10 -0.86all + ngrams 61.37 -0.30 62.22 1.26no NRC #lex 61.35 -0.32 60.66 -0.30no MSOL lex 61.88 0.21 61.35 0.39no Stanford lex 61.84 0.17 61.02 0.06no negation cntx 61.94 0.27 60.88 -0.08no encodings 61.74 0.07 60.92 -0.04no NRC emo lex 61.67 0.00 60.96 0.00no Sent140 lex 61.61 -0.06 60.32 -0.64Table 2: Ablation experiments on the 2013 test sets.Feature Diff LiveJournal LJ delta Twitter Twitter delta Sarcasm Sarcasm deltasubmitted features 68.23 63.62 48.34no POS tags 62.28 -5.95 59.00 -4.62 43.70 -4.64no word clusters 65.08 -3.15 59.82 -3.80 43.96 -4.38all lex removed 66.16 -2.07 60.73 -2.89 49.59 1.25no Hu-Liu lex 66.44 -1.79 62.15 -1.47 46.72 -1.62all + ngrams 67.79 -0.44 62.96 -0.66 47.82 -0.52no NRC #lex 66.81 -1.42 63.25 -0.37 47.54 -0.80no MSOL lex 68.50 0.27 63.54 -0.08 48.34 0.00no Stanford lex 67.86 -0.37 63.70 0.08 48.34 0.00no negation cntx 68.09 -0.14 63.62 0.00 46.37 -1.97no encodings 68.23 0.00 63.64 0.02 47.54 -0.80no NRC emo lex 68.24 0.01 63.62 0.00 48.34 0.00no Sent140 lex 67.32 -0.91 63.94 0.32 49.47 1.13Table 3: Ablation experiments on the 2014 test sets.The improved scores are shown in Table 4, withthe submitted and the best system results.Test Set New F1 Old F1 Besttweets2014 66.23 63.62 69.85sarcasm2014 50.00 48.34 58.16LiveJournal2014 69.41 68.24 74.84tweets2013 63.08 60.96 70.75SMS2013 62.28 61.67 70.282014 mean 62.20 60.07 67.62Table 4: Our post-submission results.4 Conclusion and Future WorkWe have described the system built by the team ofSU-FMI for SemEval-2014 task 9.
Due to our lateentering in the competition, we were only ranked20th out of 50 submissions (from 44 teams).We have made some interesting observationsabout the impact of the different features.
Amongthe best-performing feature groups were POS-tagcounts, word cluster presence and bigrams, theHu-Liu lexicon and the NRC Hashtag Sentimentlexicon.
These had the most sustainable perfor-mance over the 2013 and the 2014 test datasets.Others we did not use, seemingly more contextdependent, seem to have been more suited for the2013 test sets like the n-grams feature group.Even though we made some improvements af-ter submitting our initial version, we feel there ismore to gain and optimize.
There seem to be sev-eral low-hanging fruits based on our experimentsdata, which could add few points to our F1-scores.Going forward, our goal is to extend our experi-ments with more feature sub- and super-sets and toturn our classifier into a state-of-the-art performer.594AcknowledgmentsThis work is partially supported by the FP7-ICT Strategic Targeted Research Project PHEME(No.
611233), and by the European Social Fundthrough the Human Resource Development Oper-ational Programme under contract BG051PO001-3.3.06-0052 (2012/2014).ReferencesKalina Bontcheva, Leon Derczynski, Adam Funk,Mark Greenwood, Diana Maynard, and NirajAswani.
2013.
TwitIE: An open-source infor-mation extraction pipeline for microblog text.
InProceedings of the International Conference on Re-cent Advances in Natural Language Processing,RANLP ?13, pages 83?90, Hissar, Bulgaria.Peter Brown, Peter deSouza, Robert Mercer, Vin-cent Della Pietra, and Jenifer Lai.
1992.
Class-based n-gram models of natural language.
Compu-tational Linguistics, 18:467?479.Hamish Cunningham, Diana Maynard, and KalinaBontcheva.
2011.
Text Processing with GATE.Gateway Press CA.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
LIBLINEAR:A library for large linear classification.
J. Mach.Learn.
Res., 9:1871?1874.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of the TenthACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining, KDD ?04, pages168?177, New York, NY, USA.Percy Liang.
2005.
Semi-supervised learning for nat-ural language.
Master?s thesis, Massachusetts Insti-tute of Technology.Saif Mohammad, Cody Dunne, and Bonnie Dorr.2009.
Generating high-coverage semantic orienta-tion lexicons from overtly marked words and a the-saurus.
In Proceedings of the Conference on Empir-ical Methods in Natural Language Processing: Vol-ume 2, EMNLP ?09, pages 599?608, Singapore.Saif Mohammad, Svetlana Kiritchenko, and XiaodanZhu.
2013.
NRC-Canada: Building the state-of-the-art in sentiment analysis of tweets.
In Proceedingsof the Seventh International Workshop on SemanticEvaluation Exercises, SemEval ?13, Atlanta, Geor-gia, USA.Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,Veselin Stoyanov, Alan Ritter, and Theresa Wilson.2013.
SemEval-2013 task 2: Sentiment analysis inTwitter.
In Second Joint Conference on Lexical andComputational Semantics (*SEM), Volume 2: Pro-ceedings of the Seventh International Workshop onSemantic Evaluation, SemEval ?13, pages 312?320,Atlanta, Georgia, USA.Olutobi Owoputi, Brendan O?Connor, Chris Dyer,Kevin Gimpel, and Nathan Schneider.
2012.
Part-of-speech tagging for Twitter: Word clusters andother advances.
Technical Report CMU-ML-12-107, Carnegie Mellon University.Sara Rosenthal, Alan Ritter, Veselin Stoyanov, andPreslav Nakov.
2014.
SemEval-2014 task 9: Sen-timent analysis in Twitter.
In Proceedings of theEighth International Workshop on Semantic Evalu-ation, SemEval ?14, Dublin, Ireland.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: A simple and general methodfor semi-supervised learning.
In Proceedings of the48th Annual Meeting of the Association for Compu-tational Linguistics, ACL ?10, pages 384?394, Upp-sala, Sweden.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of theConference on Human Language Technology andEmpirical Methods in Natural Language Process-ing, HLT ?05, pages 347?354, Vancouver, BritishColumbia, Canada.595
