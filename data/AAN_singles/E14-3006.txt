Proceedings of the Student Research Workshop at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 45?55,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsResolving Coreferent and Associative Noun Phrases in Scientific TextIna R?osigerInstitute for Natural Language ProcessingUniversity of Stuttgart, GermanyPfaffenwaldring 5b, 70569 Stuttgartroesigia@ims.uni-stuttgart.deSimone TeufelComputer LaboratoryUniversity of Cambridge, UK15 JJ Thomson Avenue, Cambridge CB3 0FDsht25@cl.cam.ac.ukAbstractWe present a study of information sta-tus in scientific text as well as ongoingwork on the resolution of coreferent andassociative anaphora in two different sci-entific disciplines, namely computationallinguistics and genetics.
We present an an-notated corpus of over 8000 definite de-scriptions in scientific articles.
To adapt astate-of-the-art coreference resolver to thenew domain, we develop features aimed atmodelling technical terminology and inte-grate these into the coreference resolver.Our results indicate that this integration,combined with domain-dependent train-ing data, can outperform the performanceof an out-of-the-box coreference resolver.For the (much harder) task of resolving as-sociative anaphora, our preliminary resultsshow the need for and the effect of seman-tic features.1 IntroductionResolving anaphoric relations automatically re-quires annotated data for training and testing.Anaphora and coreference resolution systemshave been tested and evaluated on different genres,mainly news articles and dialogue.
However, forscientific text, annotated data are scarce and coref-erence resolution systems are lacking (Sch?afer etal., 2012).
We present a study of anaphora in sci-entific literature and show the difficulties that arisewhen resolving coreferent and associative entitiesin two different scientific disciplines, namely com-putational linguistics and genetics.Coreference resolution in scientific articles is con-sidered difficult due to the high proportion of def-inite descriptions (Watson et al., 2003), whichtypically require domain knowledge to be re-solved.
The more complex nature of the texts isalso reflected in the heavy use of abstract entitiessuch as results or variables, while easy-to-resolvenamed entities are less frequently used.
We testan existing, state-of-the-art coreference resolutiontool on scientific text, a domain on which it hasnot been trained, and adapt it to this new do-main.
We also address the resolution of asso-ciative anaphora (Clark, 1975; Prince, 1981), arelated phenomenon, which is also called bridg-ing anaphora.
The interpretation of an associativeanaphor is based on the associated antecedent, butthe two are not coreferent.
Examples 1 and 2 showtwo science-specific cases of associative anaphorafrom our data.
(1) Xe-Ar was found to be in a layered structurewith Ar on the surface1.
(2) We base our experiments on the Penn tree-bank.
The corpus size is ...The resolution of associative links is important be-cause it can help in tasks which use the conceptof textual coherence, e.g.
Barzilay and Lapata(2008)?s entity grid or Hearst (1994)?s text seg-mentation.
They might also be of use in higher-level text understanding tasks such as textual en-tailment (Mirkin et al., 2010) or summarisationbased on argument overlap (Kintsch and van Dijk,1978; Fang and Teufel, 2014).Gasperin (2009) showed that biological texts dif-fer considerably from other text genres, such asnews text or dialogue.
In this respect, our resultsconfirm that the proportion between non-referringand referring entities in scientific text differs fromthat reported for other genres.
The same holds forthe type and relative number of linguistic expres-sions used for reference.
To address this issue, wedecided to investigate information status (Nissimet al., 2004) of noun phrases.
Information statustells us whether a noun phrase refers to an already1Anaphors are typed in bold face, their antecedents shownin italics.45known entity, or whether it can be treated as non-referring.
Since no corpus of full-text scientific ar-ticles annotated with both information status andanaphoric relations was available, we had to cre-ate and annotate our own corpus.
The main con-tributions of this work are (i) a new informationstatus-based annotation scheme and an annotatedcorpus of scientific articles, (ii) a study of infor-mation status in scientific text that compares thedistribution of the different categories in scientifictext with the distribution in news text, as well asbetween the two scientific disciplines, (iii) exper-iments on the resolution of coreferent anaphora:we devise domain adaptation for science and showhow this improves an out-of-the-box coreferenceresolver, and (iv) experiments on the resolution ofassociative anaphora with a coreference resolverthat is adapted to this new notion of ?reference?by including semantic features.
To the best of ourknowledge, this is the first work on anaphora res-olution in multi-discipline, full-text scientific pa-pers that also deals with associative anaphora.2 Related WorkNoun phrase coreference resolution is the task ofdetermining which noun phrases (NPs) in a text ordialogue refer to the same real-world entities (Ng,2010).
Resolving anaphora in scientific text hasonly recently gained interest in the research com-munity and focuses mostly on the biomedical do-main (Gasperin, 2009; Batista-Navarro and Ana-niadou, 2011; Cohen et al., 2010).
Some work hasbeen done for other disciplines, such as compu-tational linguistics.
Sch?afer et al.
(2012) presenta large corpus of 266 full-text computational lin-guistics papers from the ACL Anthology, anno-tated with coreference links.
The CoNLL sharedtask 2012 on modelling multilingual unrestrictedcoreference in OntoNotes (Pradhan et al., 2012)produced several state-of-the-art coreference sys-tems (Fernandes et al., 2012; Bj?orkelund andFarkas, 2012; Chen and Ng, 2012) trained on newstext and dialogue, as provided in the OntoNotescorpus (Hovy et al., 2006).
Other state-of-the-artsystems, such as Raghunathan et al.
(2010) andBerkeley?s Coreference Resolution System (Dur-rett and Klein, 2013), also treat coreference asa task on news text and dialogue.
We base ourexperiments on the IMS coreference resolver byBj?orkelund and Farkas (2012), one of the best pub-licly available English coreference systems.
Theresolver uses the decision of a cluster-based de-coding algorithm, i.e.
one that decides whethertwo mentions are placed in the same or in differentclusters, or whether they should be considered sin-gletons.
Their novel idea is that the decision of thisalgorithm is encoded as a feature and fed to a pair-wise classifier, which makes decisions about pairsof mentions rather than clusters.
This stacked ap-proach overcomes problems of previous systemsthat are based on the isolated pairwise decision.The features used are mostly taken from previouswork on coreference resolution and encode a va-riety of information, i.e, surface forms and theirPOS tags, subcategorisation frames and paths inthe syntax tree as well as the semantic distance be-tween the surface forms (e.g.
edit distance).However, none of this work is concerned withassociative anaphora.
Hou et al.
(2013) presenta corpus of news text annotated with associativelinks that are not limited with respect to semanticrelations between anaphor and antecedent.
Theirexperiments focus on antecedent selection only,assuming that the recognition of associative enti-ties has already been performed.
Information sta-tus has been investigated extensively in differentgenres such as news text, e.g.
in Markert et al.(2012).
Poesio and Vieira (1998) performed an in-formation status-based corpus study on news text,defining the following categories: coreferential,bridging, larger situation, unfamiliar and doubt.To the best of our knowledge, there is currentlyno study on information status in scientific text.In this paper, we propose a classification schemefor scientific text that is derived from Riester etal.
(2010) and Poesio and Vieira (1998).
We in-vestigate the differences between news text andscientific text by analysing the distribution of in-formation status categories.
We hypothesise thatthe proportion of associative anaphora in scientifictext is higher than in news text, making it neces-sary to resolve them in some form.
Our exper-iments on the resolution of coreferent anaphoraconcern the domain-adaptation of a coreferenceresolver to this new domain and examine the effectof domain-dependent training data and featuresaimed at capturing technical terminology.
We alsopresent an unusual setup where we assume that anexisting coreference resolver can also be used toidentify associative links.
We integrate semanticfeatures in the hope of detecting cases where do-main knowledge is required to establish the rela-tion between the anaphor and the antecedent.46Category ExampleCOREFERENCE LINKS GIVEN (SPECIFIC) We present the following experiment.
It deals with ...GIVEN (GENERIC) We use the Jaccard similarity coefficient in our experiments.The Jaccard similarity coefficient is useful for ...ASSOCIATIVE LINKS ASSOCIATIVE Xe-Ar was found to be in a layered structure with Aron the surface .ASSOCIATIVEThe structure of the protein ...(SELF-CONTAINING)DESCRIPTION The fact that the accuracy improves ...Categories UNUSED Noam Chomsky introduced the notion of ...without links DEICTIC This experiment deals with ...PREDICATIVE Pepsin, the enzyme, ...IDIOM On the one hand ... on the other hand ...DOUBTTable 1: Categories in our classification scheme3 Corpus CreationWe manually annotated a small scientific corpusto provide a training and test corpus for our exper-iments, using the annotation tool Slate (Kaplan etal., 2012).3.1 Annotation SchemeTwo types of reference are annotated, namelyCOREFERENCE and ASSOCIATIVE LINKS.COREFERENCE LINKS are annotated for all typesof nominal phrases; such links are annotatedbetween enitites that refer to the same referentin the real world.
ASSOCIATIVE LINKS andinformation status categories are only annotatedfor definite noun phrases.
In our scheme, ASSO-CIATIVE LINKS are only annotated when there isa clear relation between the two entities.
As wedo not pre-define possible associative relations,this definition is vague, but it is necessary to keepthe task as general as possible.
Additionally,we distinguish the following nine categories,as shown in Table 12: The category GIVENcomprises coreferent entities that refer back to analready introduced entity.
If a coreference linkis detected, the referring expression is markedas GIVEN and the link with its referent NP isannotated.
The obligatory attribute GENERIC tellsus whether the given entity has a generic or aspecific reading.
ASSOCIATIVE refers to entitiesthat are not coreferent but whose interpretationis based on a previously introduced entity.
Atypical relation between the two noun phrases ismeronymy, but as mentioned above we do notpre-define a set of allowed semantic relations.2The entity being classified is typed in bold face, referringexpressions are marked by a box and the referent is shown initalics.The category ASSOCIATIVE (SELF-CONTAINING)comprises cases where we identify an associativerelation between the head noun phrase and themodifier.
ASSOCIATIVE SELF-CONTAININGentities are annotated without a link between thetwo parts.
In scientific text, an entity is consideredDEICTIC if it points to an object that is connectedto the current text.
Therefore, we include allentities that refer to the current paper (or aspectsthereof) in this category.
Entities that have notbeen mentioned before and are not related to anyother entity in the text, but can be interpretedbecause they are part of the common knowledgeof the writer and the reader are covered by thecategory UNUSED.
DESCRIPTION is annotatedfor entities that are self-explanatory and typicallyoccur in particular syntactic patterns such asNP complements or relative clauses.
Idiomaticexpressions or metaphoric use are covered inthe category IDIOM.
Predicative expressions,including appositions, are annotated as PRED-ICATIVE.
Finally, the category DOUBT is usedwhen the text or the antecedent is unclear.
Notethat NEW, a category that has been part of mostprevious classification schemes of informationstatus, is not present as this information status istypically observed in indefinite noun phrases.
Aswe deal exclusively with definite noun phrases3,we do not include this category in our scheme.In contrast to Poesio and Vieira?s scheme, ourscontains the additional categories PREDICATIVE,ASSOCIATIVE SELF-CONTAINING, DEICTIC andIDIOM.3With the exception of coreferring anaphoric expressions,as previously discussed.47GEN CLSentences 1834 1637Words 43691 38794Def.
descriptions 3800 4247Table 2: Properties of the annotated two subcor-pora, genetics (GEN) and computational linguis-tics (CL)GEN CLCoreference links 1976 2043Associative links 328 324Given 1977 2064Associative 315 280Associative (sc) 290 272Description 810 1215Unused 286 286Deictic 28 54Predicative 9 19Idiom 9 34Doubt 39 22Table 3: Distribution of information status cate-gories and links in the two disciplines, in absolutenumbers3.2 Resulting CorpusOur annotated corpus contains 16 full-text sci-entific papers, 8 papers for each of the twodisciplines.
The computational linguistics (CL)papers cover various topics ranging from dialoguesystems to machine translation; the genetics(GEN) papers deal mostly with the topic of shortinterfering RNAs, but focus on different aspects ofit.
In total, the annotated computational linguisticspapers contain 1637 sentences, 38,794 words and4247 annotated definite descriptions while theannotated genetics papers contain 1834 sentences,43,691 words and 3800 definite descriptions; thetwo domain subcorpora are thus fairly comparablein size.
See Table 2 for corpus statistics andTable 3 for the distribution of categories and links.It is well-known that there are large differencesin reference phenomena between scientific textand other domains (Gasperin, 2009).
In scientifictext, it is assumed that the reader has a relativelyhigh level of background.
We would expect thisgeneral property of scientific text to have an im-pact on the distribution of categories with respectto information status.Table 4 compares the two scientific disciplines inour study with each other.
We note that the propor-tion of entities classified as DESCRIPTION in theCL papers is considerably higher than in the GENpapers.
The proportions of the other categories aresimilar, though the proportion of GIVEN, ASSO-CIATIVE and UNUSED entities is slightly higher inthe GEN articles.Table 4 also compares the distribution of cat-egories in news text (Poesio and Vieira, 1998;P&V) with that of ours (as far as they arealignable, using our names for categories).
Notethat on a conceptual level, these categories areequivalent, but there are some differences with re-spect to the annotation guidelines.The most apparent difference is the proportion ofUNUSED entities (6-7 % in science, 23 % in newstext) which might be due to the prevalence ofnamed entities in news text.
Compared to the dis-tribution of categories in news text, the proportionof GIVEN entities is about 4-8 % higher in scien-tific text.
The proportion of ASSOCIATIVE enti-ties4is twice as high in the scientific domain com-pared to news text.
UNUSED entities have a dis-tinctly lower proportion, with about 7%.
As ourguidelines limit deictic references to only thosethat refer to (parts of) the current paper, we geta slightly lower proportion than the 2 % in newstext, reported by Poesio and Vieira (1998) in anearlier experiment, where no such limitation waspresent.Category GEN CL P&VGiven 52.03 48.60 44.00Associative 8.29 6.59 8.50Associative (sc) 7.63 6.40 ?Description 21.31 28.61 21.30Unused 7.53 6.73 23.50Deictic 0.74 1.27 ?Predicative 0.24 0.45 ?Idiom 0.24 0.80 (2.00)Doubt 1.03 0.52 2.60Table 4: Distribution of information status cate-gories in different domains, in percentIt has been shown in similar annotation exper-iments on information status, with similarly fine-grained schemes (Markert et al., 2012; Riester etal., 2010), that it is possible to achieve annotationwith marginally to highly reliable inter-annotatoragreement.
In our experiments, only one per-son (the first author) performed the annotation,so that we cannot compute any agreement mea-surements.
We are currently performing an inter-annotator study with two additional annotators sothat we can better judge human agreement and usethe annotations as a reliable gold standard.4The union of categories ASSOCIATIVE and ASSOCIA-TIVE SELF-CONTAINING.484 Adapting a Coreference Resolver tothe Scientific DomainTo show the difficulties that a coreference resolverfaces in the scientific domain, we ran, out-of-the-box, a coreference system (Bj?orkelund and Farkas,2012), that has not been trained on scientific text,on our corpus and perform an error analysis.
Inparticular, we are curious about which of the sys-tem?s errors are domain-dependent.
This analysismotivates a set of terminological features that areincorporated and tested in Section 6.4.1 Error AnalysisDomain-dependent errors.
The lack of seman-tic, domain-dependent knowledge results in thesystem?s failure to identify coreferent expressions,e.g.
those expressed as synonyms.
This type oferror can be prevented by implementing domain-dependent knowledge.
In Example 3, we wouldlike to generate a link between treebank and cor-pus as these terms are used as synonyms.
Thesame is true for protein-forming molecules andamino acids in Example 4.
(3) Experiments were performed with the cleanpart of the treebank.
The corpus consists of1 million words.
(4) Amino acids are organic compounds madefrom amine (-NH2) and carboxylic acid (-COOH) functional groups.
The protein-forming molecules ...Another common error is that the coreferenceresolver links all occurrences of demonstrativescience-specific expressions such as this paperor this approach to each other, even if they areseveral paragraphs apart.
In most cases, thesedemonstrative expressions do not corefer, butrefer to an approach or a paper recently describedor cited.
This type of error is particularly frequentin the computational linguistics domain andmight be reduced by a feature that captures thispeculiarity.
A special case occurs when authorsre-use clauses of the abstract in the introduction.The coreference resolver then interprets ratherlarge spans as coreferent which are not annotatedin the gold standard.
Yet a different kind of erroris based on the fact that the coreference resolverhas been trained on OntoNotes, i.e.
mostly onnon-written text.
Thus, the classifier has not seencertain phenomena and, for example, links alloccurrences of e.g.
into one equivalence class asit is interpreted as a named entity.General errors.
Some errors are general er-rors of coreference resolvers in the sense that theyhave very little to do with domain dependence,such as choosing the wrong antecedent or link-ing non-referential occurrences of it (see Exam-ples 5 and 6).
(5) This approach allows the processes of build-ing referring expressions and identifyingtheir referents.
(6) The issue of how to design sirnas that pro-duce high efficacy is the focus of a lot of cur-rent research.
Since it was discovered that ...4.2 Terminological FeaturesThis section deals with the design of possibleterminological features for our experiments thatare aimed at capturing some form of domainknowledge.
We create these using the informationin 1000 computational linguistics and 1000genetics papers that are not part of our scientificcorpus.Non-coreferring bias list.
Our first featureconcentrates on nouns which have a low proba-bility to be coreferring (i.e.
category GIVEN) ifthey appear as the head of noun phrase.
We as-sume that the normal case of coreference betweendefinite noun phrases is that of a concept intro-duced as an indefinite NP and later referred to asa definite NP, and compile a list of lexemes thatdo not follow this pattern.
NPs with those lexemesshould be more likely to be of category UNUSED orDESCRIPTION.
We find the lexemes by recordinghead nouns of definite NPs which are not observedin a prior indefinite NP in the same document (lo-cal list) or the entire document collection (globallist).
We create two lists of such head words forevery discipline.
The lexemes are arranged in de-creasing order of their frequency so that we canuse both their presence or non-presence on the listand their rank on the list as potential features.As can be seen in Table 5, the presence, the be-ginning and the literature are definite descriptionsthat are always used without having been intro-duced to the discourse.
These terms are either partof domain knowledge (the hearer, the reader) orpart of the general scientific terminology (the lit-erature).
In the local list we see expressions thatcan be used without having been introduced, but49may in some contexts occur in the indefinite formas well, e.g.
the word or the sentence.CL GEN(a) global (b) local (a) global (b) localpresence number manuscript databeginning word respect regionliterature sentence prediction genehearer training monograph casereader user notion speciesTable 5: Top five terms of local and global non-coreferring bias listsCollocation list.
One of our hypotheses is thatthe NPs occurring in verb-object collocations aretypically not part of any coreference chain.
To testthis, we use our collection of 2000 scientific pa-pers to extract domain-specific verb-object collo-cations.
We assume that for some collocations,this tendency is stronger (make use, take place)than for others that could potentially be corefer-ring (see figure, apply rule).
The collocations havebeen identified with a term extraction tool (Gojunet al., 2012).
Every collocation that occurs at leasttwice in the data is present on the list.
Table 6shows the most frequent terms.make + use take + placegive + rise silence + activityderive + form refashion + planparse + sentence predict + sirnasort + feature match + predicatesee + figure use + informationsilence + efficiency follow + transfectionembed + sentence apply + rulefocus + algorithm stack + symbolTable 6: Most frequent occurring collocation can-didates in scientific text (unsorted)Argumentation nouns, work nouns andidioms.
As mentioned in Section 4.1, thebaseline classifier often links demonstrative,science-specific entities, even if they are severalparagraphs apart.
To prevent this, we combine adistance measure with a set of 182 argumentationand work nouns taken from Teufel (2010), such asachievement, claim or experiment.
We also createa small list of idioms as they are never part of acoreference chain.5 Adapting a Coreference Resolver forAssociative Links in ScienceWe now turn to the much harder task of resolvingassociative anaphora.5.1 Types of Associative AnaphoraTo illustrate the different types of associativeanaphora, we here show a few examples, mostlytaken from the genetics papers.
The anaphorsare shown in bold face, the antecedents in italics.Many associative anaphors include noun phraseswith the same head.
In most of these cases, theanaphor contains a different modifier than the an-tecedent, such as(8) the negative strain ... the positive strain;(9) three categories ... the first category;(10) siRNAs ... the most effective siRNAs.We assume that these associative relations canbe identified with a coreference resolver withoutadding additional features.
Other cases are muchharder to identify automatically, such as thosewhere semantic knowledge is required to interpretthe relation between the entities:(11) the classifier ... the training data;(12) this database ... the large dataset.In other cases, the nominal phrase in the an-tecedent tends to be derivationally related to thehead word in the anaphor, as in(13) the spotty distribution ...the spots;(14) competitor ... the competitive effect.There are also a number of special cases, such as(15) the one interference process ... the other in-terference process.We hypothesise that the integration of semanticfeatures discussed in the previous section enablesthe resolver to cover more than just those casesthat are based on the similarity of word forms.5.2 Semantic FeaturesIt is apparent that the recognition and correctresolution of associative anaphora requires se-mantic knowledge.
Therefore, we adapt thecoreference resolver by extending the WordNetfeature, one of the features implemented in theIMS resolver, to capture more than just synonyms.50We use the following WordNet relations: Hyper-nymy (macromolecule ?
protein), hyponymy(nucleoprotein ?
protein), meronymy (surface?
structure), substance meronymy (aminoacid ?
protein), topic member (acute, chronic?medicine) and topic ( periodic table?
chemistry).WordNet?s coverage in the scientific domain issurprisingly good: 75,91 % of all common nounsin the GEN papers and 88,12 % in the CL papersare listed in WordNet.
Terms that are not cov-ered are, for example, abbreviations of differenttypes of ribonucleic acid in genetics or specialistterms like tagging, subdialogue or SVM in com-putational linguistics.6 ExperimentsWe now compare the performance of an out-of-the-box coreference system with the resolvertrained on our annotated scientific corpus (Sec-tion 6.2).
We also show the effect of adding ad-ditional features aimed at capturing technical ter-minology.
In the experiments on the resolution ofassociative anaphora (Section 6.3), we test the hy-pothesis that the coreference resolver is able to ad-just to the new notion of reference and show theeffect of semantic features.6.1 Experimental SetupWe perform our experiments using the IMS coref-erence resolver as a state-of-the-art coreferenceresolution system (Bj?orkelund and Farkas, 2012)5.The algorithm and the features included have notbeen changed except where otherwise stated.
Weuse the OntoNotes datasets from the CoNLL 2011shared task6(Pradhan et al., 2012; Hovy et al.,2006), only for training the out-of-the-box sys-tem.
We also use WordNet version 3.0 as pro-vided in the 2012 shared task7as well as JAWS,the Java API for WordNet searching8.
Perfor-mance is reported on our annotated corpus, us-ing 8-fold cross-validation and the official CoNLLscorer (version 5).5See: www.ims.uni-stuttgart.de/forschung/ressourcen/werkzeuge/IMSCoref.htmlWe follow their strategy to use the AMP decoder as thefirst decoder and the PCF decoder, a pairwise decoder, as asecond.
The probability threshold is set to 0.5 for the firstand 0.65 for the second decoder.6http://conll.cemantix.org/2011/data.html7http://conll.cemantix.org/2012/data.html8http://lyle.smu.edu/?tspell/jaws/6.2 Resolving Coreferent ReferencesIMS coreference resolver unchanged.
To beable to judge the performance of an existing coref-erence resolver on scientific text, we first re-port performance without making any changes tothe resolver whatsoever, using different trainingdata.
The BASELINE version is trained on theOntoNotes dataset from the CoNLL 2011 sharedtask.
In the SCIENTIFIC version, we only use ourannotated scientific papers.
MIXED contains theentire OntoNotes dataset as well as the scientificpapers, leading to a larger training corpus whichcompensates for the rather small size of the scien-tific corpus9.
Table 7 shows the average CoNLLscores10of the two subdomains genetics and com-putational linguistics.Training Set GEN CL GEN+CLBaseline 35.30 40.30 37.80Scientific 44.94 42.41 43.68Mixed 47.92 47.44 47.68Table 7: Resolving coreferent references:CoNLL metric scores for different training setsThe BASELINE achieves relatively low resultsin comparison to the score of 61.24 that wasreported in the shared task (Bj?orkelund andFarkas, 2012).
Even though our scientific corpusis only 7% the size of the OntoNotes dataset, itinceases performance of the BASELINE systemby 15,6%.
The SCIENTIFIC version outperformsthe BASELINE version for all of the GEN papersand for 6 out of 8 CL papers.
MIXED, the versionthat combines the scientific corpus with the entireOntoNotes dataset, proves to work best (47.92 forGEN and 47.44 for CL).
In THE BASELINE ver-sion, the performance on the CL papers is betterthan on the GEN papers.
Interestingly, this is nottrue for the SCIENTIFIC version, where the per-formance on the GEN papers is better.
However,as the main result here, we can see that trainingon scientific text was successful.
The increase inscore in both the SCIENTIFIC and the MIXED ver-sion over BASELINE is statistically significant119We also experimented with a balanced version, whichcontains an equal amount of sentences from the OntoNotescorpus and our scientific corpus.
The results are not reportedhere as this version performed worse.10The CoNLL score is the arithmetic mean of MUC, B3and CEAFE.11We compute significance using the Wilcoxon signed ranktest (Siegel and Castellan, 1988) at the 0.01 level unless oth-erwise stated.51(+9.64 and +12.62 in the GEN papers, +2.11 and+7.14 in the CL papers, absolute in CoNLL score).IMS coreference resolver, adapted to the do-main.
We show the results from the expansion ofthe feature set in Table 8.
Each of the single fea-tures is added to the version in the line above thecurrent version.
Compared to the MIXED version,adding the features to the resolver results in anincrease in performance for both of the scientificdisciplines.
However, when adding the colloca-tion feature to the version including the bias lists,the argumentation nouns as well as idioms, perfor-mance drops slightly.
This might indicate the needfor a revised collocation list where those nouns arefiltered out that could potentially be coreferring,e.g.
see figure.
For the best version of the CLpapers, the increase in CoNLL score, comparedwith the MIXED version, is +1.08; for the GEN pa-pers it is slightly less, namely +0.22.
This increasein score is promising, but the data is too small toshow significance.GEN CL GEN+CLMixed 47.92 47.44 47.68+ Bias Lists 48.04 47.79 47.94+ Arg.
Nouns and Idioms 48.14 48.52 48.33+ Collocations 48.03 48.12 48.08Table 8: Resolving coreferent references:CoNLL scores for the extended feature setsHowever, compared with the BASELINE ver-sion, the final version (marked bold) performs sig-nificantly better and outperforms the out-of-the-box run by 36.47 % absolute on the CoNLL met-ric for the GEN papers and by 20.40 % for the CLpapers.
The results also show that, in our experi-ments, the effect of using domain-specific trainingmaterial is larger than the effect of adding termi-nological features.6.3 Resolving Associative ReferencesIMS coreference resolver unchanged.
Asassociative references are not annotated in theOntoNotes dataset, the only possible baseline wecan use is the system trained on the scientific cor-pus.
Average CoNLL scores were 33.52 for GENand 32.86 for CL (33.14 overall).
As expected,the performance on associative anaphora is worsethan on coreferent anaphora.
We have not madeany changes to the resolver, so it is interesting tosee that the resolver is indeed able to adjust tothe new notion of reference and manages to linkassociative references.We found that the resolver generally identifiesvery few associative references and so the mostcommon error of the system is that it fails torecognise associative relations, particularly ifthe computed edit distance, one of the standardfeatures in the coreference resolver, is very high.The easiest associative relations to detect arethose which have similar surface forms.
Forexample, the coreference resolver correctly linksRNAI and RNAI genes, the sense strand andthe anti-sense strand or siRNAs and efficacioussiRNAs.
However, for most of the associativereferences, the lack of easily identifiable surfacemarkers makes the task difficult.
Ironically, thesystem also falsely classifies many coreferencelinks as associative, although it has this time ofcourse been trained only on associative references.This is not surprising, given that the tasks areso similar that we are able to use a coreferenceresolver for the associative task in the first place.IMS coreference resolver using semantic fea-tures.
Table 9 gives the results of the extendedfeature set that includes the semantic features de-scribed in Section 5.2.
Each of the respective se-mantic features shown in the table is added to theversion in the line above the current version.It can be seen that the different WordNet re-lations have different effects on the two scien-tific disciplines.
For the genetics papers, the in-clusion of synonyms, hyponyms and hypernymsresults in the highest increase in performance(+2.02).
For the computational linguistics pa-pers, the inclusion of synonyms, hyponyms, top-ics and meronyms obtains the best performance(+1.19).
As the effect of the features is discipline-dependent, we create two separate final featuresets for the two disciplines.
The GEN version con-tains synonyms, hyponyms and hypernyms whilethe CL version contains synonyms, hyponyms,topics and meronyms.
The highest increase in per-formance for the CL feature set (and the one re-sulting in the final system) was achieved by drop-ping topic members and hypernyms.
In the finalCL system, the increase in performance comparedto the baseline version is +1.35.
Both final ver-sions significantly outperform the baseline.When comparing the output of the extendedsystem to the baseline system, it can be seen that52GEN CL GEN+CLBaseline 33.52 32.86 33.19+ Synonyms 33.95 32.87 33.41+ Hyponyms 34.04 32.94 33.49+ Hypernyms 35.54 31.35 33.45+ Topic members 34.61 30.61 32.61+ Topics 34.09 32.88 33.49+ Meronyms 33.70 34.05 33.88+ Substance meronyms 33.57 32.40 32.99Final version 35.54 34.21 34.88(domain-dependent)Table 9: Resolving associative references:CoNLL metric scores for the extended feature setsthe resolver now links many more mentions (5.7times more in the GEN papers, 3.8 times morein the CL papers).
The reason why this does notlead to an even larger increase in performance liesin the large number of false positives.
However,when looking at the data it becomes apparent thatthe newly created links are mostly links that poten-tially could have been annotated during the anno-tation, but are not part of the gold standard becausethe associative antecedent is not absolutely neces-sary in order to interpret the anaphor or becausethe entity has been linked to a different entitywhere the associative relation is stronger.
The ex-istence of more-or-less acceptable alternative as-sociative links casts some doubt on using a goldstandard as the sole evaluation criterion.
An alter-native would be to ask humans for a rating of thesensibility of the links determined by the system.7 Conclusion and Future WorkWe have presented a study of information statusin two scientific disciplines as well as preliminaryexperiments on the resolution of both coreferentand associative anaphora in these disciplines.
Ourresults show a marked difference in the distribu-tions of information status categories between sci-entific and news text.
Our corpus of 16 full-textscholarly papers annotated with information sta-tus and anaphoric links, which we plan to releasesoon, contains over 8000 annotated definite nounphrases.
We demonstrate that the integration ofdomain-dependent terminological features, com-bined with domain-dependent training data, out-performs the unadjusted IMS system (Bj?orkelundand Farkas, 2012) by 36.47 % absolute on theCoNLL metric for the genetics papers and by20.40 % absolute for the computational linguisticspapers.
The effect of domain-dependent trainingmaterial was stronger than the integration of ter-minological features.
As far as the resolution ofassociative anaphora is concerned, we have shownthat it is generally possible to adapt a corefer-ence resolver to this task, and we have achievedan improvement in performance using novel se-mantic features.
We are currently performingan inter-annotator study with two additional an-notators, which will also lead to a better under-standing of the relative difficulty of the categories.Furthermore, we plan to convert the coreference-annotated ACL papers by Sch?afer et al.
(2012) intoCoNLL format and use them for training the coref-erence resolver.
As we have annotated our corpuswith information status, it might also be interest-ing to train a classifier on the information statuscategories and use its predictions to improve theperformance on anaphora resolution tasks.
To doso, we will create a separate corpus for testing,annotated solely with coreference and associativelinks.AcknowledgementsWe would like to thank Jonas Kuhn, Ulrich Heidand Arndt Riester for their valuable commentsas well as the anonymous reviewers for their in-sightful remarks.
We gratefully acknowledge fi-nancial support from the Deutsche Forschungsge-meinschaft (DFG) in the framework of project B3and D7 of the SFB 732.
This research is par-tially supported by the Intelligence Advanced Re-search Projects Activity (IARPA) via Departmentof Interior National Business Center (DoI/NBC)contract number D11PC20153.
The U.S. Gov-ernment is authorized to reproduce and distributereprints for Governmental purposes notwithstand-ing any copyright annotation thereon.
Disclaimer:The views and conclusions contained herein arethose of the authors and should not be interpretedas necessarily representing the official policiesor endorsements, either expressed or implied, ofIARPA, DoI/NBC, or the U.S. Government.ReferencesRegina Barzilay and Mirella Lapata.
2008.
Modelinglocal coherence: An entity-based approach.
Compu-tational Linguistics, 34(1):1?34.Riza T. Batista-Navarro and Sophia Ananiadou.
2011.Building a coreference-annotated corpus from thedomain of biochemistry.
In Proceedings of BioNLP2011 Workshop, pages 83?91.
Association for Com-putational Linguistics.53Anders Bj?orkelund and Rich?ard Farkas.
2012.
Data-driven multilingual coreference resolution using re-solver stacking.
In Joint Conference on EMNLP andCoNLL-Shared Task, pages 49?55.
Association forComputational Linguistics.Chen Chen and Vincent Ng.
2012.
Combining thebest of two worlds: A hybrid approach to multilin-gual coreference resolution.
In Joint Conference onEMNLP and CoNLL - Shared Task, pages 56?63,Jeju Island, Korea, July.
Association for Computa-tional Linguistics.Herbert H. Clark.
1975.
Bridging.
In Proceedingsof the 1975 workshop on Theoretical issues in nat-ural language processing, pages 169?174.
Associa-tion for Computational Linguistics.Kevin B. Cohen, Arrick Lanfranchi, William Cor-vey, William A. Baumgartner Jr, Christophe Roeder,Philip V. Ogren, Martha Palmer, and LawrenceHunter.
2010.
Annotation of all coreference inbiomedical text: Guideline selection and adaptation.In Proceedings of BioTxtM 2010: 2nd workshopon building and evaluating resources for biomedicaltext mining, pages 37?41.Greg Durrett and Dan Klein.
2013.
Easy victories anduphill battles in coreference resolution.
In Proceed-ings of the Conference on Empirical Methods in Nat-ural Language Processing, Seattle, Washington, Oc-tober.
Association for Computational Linguistics.Yimai Fang and Simone Teufel.
2014.
A summariserbased on human memory limitations and lexicalcompetition.
In Proceedings of the EACL.
Associ-ation for Computational Linguistics.
(to appear).Eraldo Fernandes, C?
?cero dos Santos, and Ruy Milidi?u.2012.
Latent structure perceptron with feature in-duction for unrestricted coreference resolution.
InJoint Conference on EMNLP and CoNLL - SharedTask, pages 41?48, Jeju Island, Korea, July.
Associ-ation for Computational Linguistics.Caroline V. Gasperin.
2009.
Statistical anaphoraresolution in biomedical texts.
Technical Re-port UCAM-CL-TR-764, University of Cambridge,Computer Laboratory, December.Anita Gojun, Ulrich Heid, Bernd Weissbach, CarolaLoth, and Insa Mingers.
2012.
Adapting and evalu-ating a generic term extraction tool.
In Proceedingsof the 8th international conference on Language Re-sources and Evaluation (LREC), pages 651?656.Marti A. Hearst.
1994.
Multi-paragraph segmentationof expository text.
In Proceedings of the 32nd An-nual Meeting of the Association for ComputationalLinguistics, pages 9?16.Yufang Hou, Katja Markert, and Michael Strube.
2013.Global inference for bridging anaphora resolution.In Proceedings of NAACL-HLT, pages 907?917.Eduard Hovy, Mitchell Marcus, Martha Palmer,Lance Ramshaw, and Ralph Weischedel.
2006.OntoNotes: the 90% solution.
In Proceedings ofthe human language technology conference of theNAACL, Companion Volume: Short Papers, pages57?60.
Association for Computational Linguistics.Dain Kaplan, Ryu Iida, Kikuko Nishina, and TakenobuTokunaga.
2012.
Slate ?
a tool for creating andmaintaining annotated corpora.
Journal for Lan-guage Technology and Computational Linguistics,pages 89?101.Walter Kintsch and Teun A. van Dijk.
1978.
Toward amodel of text comprehension and production.
Psy-chological Review, 85(5):363?394.Katja Markert, Yufang Hou, and Michael Strube.
2012.Collective classification for fine-grained informationstatus.
In Proceedings of the 50th Annual Meetingof the Association for Computational Linguistics:Long Papers-Volume 1, pages 795?804.
Associationfor Computational Linguistics.Shachar Mirkin, Ido Dagan, and Sebastian Pad?o.
2010.Assessing the role of discourse references in entail-ment inference.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Lin-guistics, ACL 2010, pages 1209?1219.
Associationfor Computational Linguistics.Vincent Ng.
2010.
Supervised noun phrase coref-erence research: The first fifteen years.
In Pro-ceedings of the 48th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 1396?1411.
Association for Computational Linguistics.Malvina Nissim, Shipra Dingare, Jean Carletta, andMark Steedman.
2004.
An annotation scheme forinformation status in dialogue.
LREC 2004.Massimo Poesio and Renata Vieira.
1998.
A corpus-based investigation of definite description use.
Com-putational linguistics, 24(2):183?216.Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,Olga Uryupina, and Yuchen Zhang.
2012.
CoNLL-2012 shared task: Modeling multilingual unre-stricted coreference in OntoNotes.
In Proceedingsof the Joint Conference on EMNLP and CoNLL:Shared Task, pages 1?40.Ellen F. Prince.
1981.
Toward a taxonomy of given-new information.
In Radical Pragmatics, pages223?55.
Academic Press.Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-garajan, Nathanael Chambers, Mihai Surdeanu, DanJurafsky, and Christopher Manning.
2010.
A multi-pass sieve for coreference resolution.
In Proceed-ings of EMNLP 2010.Arndt Riester, David Lorenz, and Nina Seemann.2010.
A recursive annotation scheme for referentialinformation status.
In Proceedings of the Seventh In-ternational Conference on Language Resources andEvaluation (LREC), pages 717?722.54Ulrich Sch?afer, Christian Spurk, and J?org Steffen.2012.
A fully coreference-annotated corpus ofscholarly papers from the acl anthology.
In Proceed-ings of the 24th International Conference on Com-putational Linguistics.
International Conference onComputational Linguistics (COLING-2012), De-cember 10-14, Mumbai, India, pages 1059?1070.Sidney Siegel and N. John Jr. Castellan.
1988.
Non-parametric Statistics for the Behavioral Sciences.McGraw-Hill, Berkeley, CA, 2nd edition.Simone Teufel.
2010.
The Structure of Scientific Arti-cles: Applications to Citation Indexing and Summa-rization.
CSLI Publications.Rebecca Watson, Judita Preiss, and Ted Briscoe.2003.
The contribution of domain-independentrobust pronominal anaphora resolution to open-domain question-answering.
In Proceedings of theSymposium on Reference Resolution and its Appli-cations to Question Answering and Summarization.Venice, Italy June, pages 23?25.55
