Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1967?1972,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsRepresenting Verbs with Rich Contexts: an Evaluation on Verb SimilarityEmmanuele ChersoniAix-Marseille Universityemmanuelechersoni@gmail.comEnrico SantusThe Hong Kong Polytechnic Universityesantus@gmail.comAlessandro LenciUniversity of Pisaalessandro.lenci@unipi.itPhilippe BlacheAix-Marseille Universityphilippe.blache@univ-amu.frChu-Ren HuangThe Hong Kong Polytechnic Universitychuren.huang@polyu.edu.hkAbstractSeveral studies on sentence processing sug-gest that the mental lexicon keeps track of themutual expectations between words.
CurrentDSMs, however, represent context words asseparate features, thereby loosing importantinformation for word expectations, such asword interrelations.
In this paper, we presenta DSM that addresses this issue by definingverb contexts as joint syntactic dependencies.We test our representation in a verb similaritytask on two datasets, showing that joint con-texts achieve performances comparable to sin-gle dependencies or even better.
Moreover,they are able to overcome the data sparsityproblem of joint feature spaces, in spite of thelimited size of our training corpus.1 IntroductionDistributional Semantic Models (DSMs) rely on theDistributional Hypothesis (Harris, 1954; Sahlgren,2008), stating that words occurring in similar con-texts have similar meanings.
On such theoreticalgrounds, word co-occurrences extracted from cor-pora are used to build semantic representations in theform of vectors, which have become very popular inthe NLP community.
Proximity between word vec-tors is taken as an index of meaning similarity, andvector cosine is generally adopted to measure suchproximity, even though other measures have beenproposed (Weeds et al, 2004; Santus et al, 2016).Most of DSMs adopt a bag-of-words approach,that is they turn a text span (i.e., a word window or aparsed sentence) into a set of words and they regis-ter separately the co-occurrence of each word with agiven target.
The problem with this approach is thatvaluable information concerning word interrelationsin a context gets lost, because words co-occurringwith a target are treated as independent features.This is why works like Ruiz-Casado et al (2005),Agirre et al (2009) and Melamud et al (2014) pro-posed to introduce richer contexts in distributionalspaces, by using entire word windows as features.These richer contexts proved to be helpful to seman-tically represent verbs, which are characterized byhighly context-sensitive meanings, and complex ar-gument structures.
In fact, two verbs may share in-dependent words as features despite being very dis-similar from the semantic point of view.
For instancekill and heal share the same object nouns in The doc-tor healed the patient and the The poison killed thepatient, but are highly different if we consider theirjoint dependencies as a single context.
Nonetheless,richer contexts like these suffer from data sparsity,therefore requiring either larger corpora or complexsmoothing processes.In this paper, we propose a syntactically savvy no-tion of joint contexts.
To test our representation,we implement several DSMs and we evaluate themin a verb similarity task on two datasets.
The re-sults show that, even using a relatively small corpus,our syntactic joint contexts are robust with respect to1967data sparseness and perform similarly or better thansingle dependencies in a wider range of parametersettings.The paper is organized as follows.
In Section2, we provide psycholinguistic and computationalbackground for this research, describing recent mod-els based on word windows.
In Section 3, we de-scribe our reinterpretation of joint contexts with syn-tactic dependencies.
Evaluation settings and resultsare presented in Section 4.2 Related WorkA number of studies in sentence processing sug-gests that verbs activate expectations on their typ-ical argument nouns and vice versa (McRae et al,1998; McRae et al, 2005) and nouns do the samewith other nouns occurring as co-arguments in thesame events (Hare et al, 2009; Bicknell et al,2010).
Experimental subjects seem to exploit a richevent knowledge to activate or inhibit dynamicallythe representations of the potential arguments.
Thisphenomenon, generally referred to as thematic fit(McRae et al, 1998), supports the idea of a mentallexicon arranged as a web of mutual expectations.Some past works in computational linguistics(Baroni and Lenci, 2010; Lenci, 2011; Sayeed andDemberg, 2014; Greenberg et al, 2015) modeledthematic fit estimations by means of dependency-based or of thematic roles-based DSMs.
However,these semantic spaces are built similarly to tradi-tional DSMs as they split verb arguments into sepa-rate vector dimensions.
By using syntactic-semanticlinks, they encode the relation between an event andeach of its participants, but they do not encode di-rectly the relation between participants co-occurringin the same event.Another trend of studies in the NLP communityaimed at the introduction of richer contextual fea-tures in DSMs, mostly based on word windows.
Thefirst example was the composite-feature model byRuiz-Casado et al (2005), who extracted word win-dows through a Web Search engine.
A compositefeature for the target word watches is Alicia always____ romantic movies, extracted from the sentence Iheard that Alicia always watches romantic movieswith Antony (the placeholder represents the targetposition).
Thanks to this approach, Ruiz-Casado andcolleagues achieved 82.50 in the TOEFL synonymdetection test, outperforming Latent Semantic Anal-ysis (LSA; see Landauer et al (1998)) and severalother methods.Agirre et al (2009) adopted an analogous ap-proach, relying on a huge learning corpus (1.6 Ter-aword) to build composite-feature vectors.
Theirmodel outperformed a traditional DSM on the sim-ilarity subset of the WordSim-353 test set (Finkel-stein et al, 2001).Melamud et al (2014) introduced a probabilisticsimilarity scheme for modeling the so-called jointcontext.
By making use of the Kneser-Ney languagemodel (Kneser and Ney, 1995) and of a probabilis-tic distributional measure, they were able to over-come data sparsity, outperforming a wide variety ofDSMs on two similarity tasks, evaluated on Verb-Sim (Yang and Powers, 2006) and on a set of 1,000verbs extracted from WordNet (Fellbaum, 1998).On the basis of their results, the authors claimed thatcomposite-feature models are particularly advanta-geous for measuring verb similarity.3 Syntactic joint contextsA joint context, as defined in Melamud et al (2014),is a word window of order n around a target word.The target is replaced by a placeholder, and the valueof the feature for a word w is the probability of wto fill the placeholder position.
Assuming n=3, aword like love would be represented by a collectionof contexts such as the new students ____ the schoolcampus.
Such representation introduces data sparse-ness, which has been addressed by previous studieseither by adopting huge corpora or by relying on n-gram language models to approximate the probabil-ities of long sequences of words.However, features based on word windows do notguarantee to include all the most salient event par-ticipants.
Moreover, they could include unrelatedwords, also differentiating contexts describing thesame event (e.g.
consider Luis ____ the red ball andLuis ____ the blue ball).For these reasons, we introduce the notion of syn-tactic joint contexts, further abstracting from linearword windows by using dependencies.
Each featureof the word vector, in our view, should correspond toa typical verb-argument combination, as an approx-1968imation to our knowledge about typical event par-ticipants.
In the present study, we are focusing onverbs because verb meaning is highly context sen-sitive and include information about complex argu-ment configurations.
Therefore, verb representationshould benefit more from the introduction of jointfeatures (Melamud et al, 2014).The procedure for defining of our representationsis the following:?
we extract a list of verb-argument dependenciesfrom a parsed corpus, and for each target verbwe extract all the direct dependencies from thesentence of occurrence.
For instance, in Fi-nally, the dictator acknowledged his failure, wewill have: target = ?acknowledge-v?
; subject =?dictator-n?
; and object = ?failure-n?.?
for each sentence, we generate a joint contextfeature by joining all the dependencies for thegrammatical relations of interest.
From the ex-ample above, we would generate the featuredictator-n.subj+____+failure-n.obj.For our experiments, the grammatical relationsthat we used are subject, object and complement,where complement is a generic relation grouping to-gether all dependencies introduced by a preposition.Our distributional representation for a target wordis a vector of syntatic joint contexts.
For instance,the word vector for the verb to begin would includefeatures like {jury-n.subj+____+deliberation-n.obj,operation-n.subj+____+on-i_thursday-n.comp,recruit-n.subj+____+training-n.obj+on-i_street-n.comp ...}.
The value of each joint feature will bethe frequency of occurrence of the target verb withthe corresponding argument combination, possiblyweighted by some statistical association measure.4 Evaluation4.1 Corpus and DSMsWe trained our DSMs on the RCV1 corpus, whichcontains approximately 150 million words (Lewis etal., 2004).
The corpus was tagged with the taggerdescribed in Dell?Orletta (2009) and dependency-parsed with DeSR (Attardi et al, 2009).
RCV1was chosen for two reasons: i) to show that ourjoint context-based representation can deal with datasparseness even with a training corpus of limitedsize; ii) to allow a comparison with the results re-ported by Melamud et al (2014).All DSMs adopt Positive Pointwise Mutual Infor-mation (PPMI; Church and Hanks (1990)) as a con-text weighting scheme and vary according to threemain parameters: i) type of contexts; ii) number ofdimensions; iii) application of Singular Value De-composition (SVD; see Landauer et al (1998)).For what concerns the first parameter, we devel-oped three types of DSMs: a) traditional bag-of-words DSMs, where the features are content wordsco-occurring with the target in a window of width2; b) dependency-based DSMs, where the featuresare words in a direct dependency relation with thetarget; c) joint context-based DSMs, using the jointfeatures described in the previous section.
The sec-ond parameter refers instead to the number of con-texts that have been used as vector dimensions.
Sev-eral values were explored (i.e.
10K, 50K and 100K),selecting the contexts according to their frequency.Finally, the third parameter concerns the applicationof SVD to reduce the matrix.
We report only theresults for a number k of latent dimensions rangingfrom 200 to 400, since the performance drops sig-nificantly out of this interval.4.2 Similarity MeasuresAs a similarity measure, we used vector cosine,which is by far the most popular in the existing lit-erature (Turney et al, 2010).
Melamud et al (2014)have proposed the Probabilistic Distributional Simi-larity (PDS), based on the intuition that two words,w1 and w2, are similar if they are likely to occur ineach other?s contexts.
PDS assigns a high similarityscore when both p(w1| contexts of w2) and p(w2|contexts of w1) are high.
We tried to test variationsof this measure with our representation, but we werenot able to achieve satisfying results.
Therefore, wereport here only the scores with the cosine.4.3 DatasetsThe DSMs are evaluated on two test sets: Verb-Sim (Yang and Powers, 2006) and the verb subsetof SimLex-999 (Hill et al, 2015).
The former in-cludes 130 verb pairs, while the latter includes 222verb pairs.1969Both datasets are annotated with similarity judge-ments, so we measured the Spearman correlation be-tween them and the scores assigned by the model.The VerbSim dataset alows for comparison withMelamud et al (2014), since they also evaluatedtheir model on this test set, achieving a Spearmancorrelation score of 0.616 and outperforming all thebaseline methods.The verb subset of SimLex-999, at the best ofour knowledge, has never been used as a benchmarkdataset for verb similarity.
The SimLex dataset isknown for being quite challenging: as reported byHill et al (2015), the average performances of simi-larity models on this dataset are much lower than onalternative benchmarks like WordSim (Finkelstein etal., 2001) and MEN (Bruni et al, 2014).We exclude from the evaluation datasets all thetarget words occurring less than 100 times in ourcorpus.
Consequently, we cover 107 pairs in theVerbSim dataset (82.3, the same of Melamud et al(2014)) and 214 pairs in the SimLex verbs dataset(96.3).4.4 ResultsTable 1 reports the Spearman correlation scores forthe vector cosine on our DSMs.
At a glance, wecan notice the discrepancy between the results ob-tained in the two datasets, as SimLex verbs confirmsto be very difficult to model.
We can also recog-nize a trend related to the number of contexts, asthe performance tends to improve when more con-texts are taken into account (with some exceptions).Single dependencies and joint contexts perform verysimilarly, and no one has a clear edge on the other.Both of them outperform the bag-of-words modelon the VerbSim dataset by a nice margin, whereasthe scores of all the model types are pretty much thesame on SimLex verbs.
Finally, it is noteworthy thatthe score obtained on VerbSim by the joint contextmodel with 100K dimensions goes very close to theresult reported by Melamud et al (2014) (0.616).Table 2 and Table 3 report the results of the mod-els with SVD reduction.
Independently of the num-ber of dimensions k, the joint contexts almost alwaysoutperform the other model types.
Overall, the per-formance of the joint contexts seems to be more sta-ble across several parameter configurations, whereasbag-of-words and single dependencies are subject tobigger drops.
Exceptions can be noticed only forthe VerbSim dataset, and only with a low numberof dimensions.
Finally, the correlation coefficientsfor the two datasets seem to follow different trends,as the models with a higher number of contexts per-form better on SimLex verbs, while the opposite istrue for the VerbSim dataset.On the VerbSim dataset, both single dependenciesand joint contexts have again a clear advantage overbag-of-words representations Although they achievea similar performance with 10K contexts, the corre-lation scores of the former decrease more quicklyas the number of contexts increases, while the latterare more stable.
Moreover, joint contexts are able tooutperform single dependencies.On SimLex verbs, all the models are very close and?
differently from the previous dataset ?
the higher-dimensional DSMs are the better performing ones.Though differences are not statistically significant,joint context are able to achieve top scores over theother models.1More in general, the best results are obtained withSVD reduction and k=200.
The joint context-basedDSM with 10K dimensions and k = 200 achieves0.65, which is above the result of Melamud et al(2014), although the difference between the two cor-relation scores is not significant.
As for SimLexverbs, the best result (0.283) is obtained by the jointcontext DSM with 100K dimensions and k = 200.Model VerbSim SimLex verbsBag-of-Words-10K 0.385 0.085Single - 10k 0.561 0.090Joint - 10k 0.568 0.105Bag-of-Words-50K 0.478 0.095Single - 50k 0.592 0.115Joint - 50k 0.592 0.105Bag-of-Words-100K 0.488 0.114Single - 100k 0.587 0.132Joint - 100k 0.607 0.114Table 1: Spearman correlation scores for VerbSim and for theverb subset of SimLex-999.
Each model is identified by the typeand by the number of features of the semantic space.1p-values computed with Fisher?s r-to-z transformationcomparing correlation coefficients between the joint context-DSMs and the other models on the same parameter settings.1970Model k = 200 k = 300 k = 400Bag-of-Words-10K 0.457 0.445 0.483Single - 10k 0.623 0.647 0.641Joint - 10k 0.650 0.636 0.635Bag-of-Words-50K 0.44 0.453 0.407Single - 50k 0.492 0.486 0.534Joint - 50k 0.571 0.591 0.613Bag-of-Words-100K 0.335 0.324 0.322Single - 100k 0.431 0.413 0.456Joint - 100k 0.495 0.518 0.507Table 2: Spearman correlation scores for VerbSim, after theapplication of SVD with different values of k.Model k = 200 k = 300 k = 400Bag-of-Words-10K 0.127 0.113 0.111Single - 10k 0.168 0.172 0.165Joint - 10k 0.190 0.177 0.181Bag-of-Words-50K 0.196 0.191 0.21Single - 50k 0.218 0.228 0.222Joint - 50k 0.256 0.250 0.227Bag-of-Words-100K 0.222 0.18 0.16Single - 100k 0.225 0.218 0.199Joint - 100k 0.283 0.256 0.222Table 3: Spearman correlation scores for the verb subset ofSimLex-999, after the application of SVD with different valuesof k.4.5 ConclusionsIn this paper, we have presented our proposal for anew type of vector representation based on joint fea-tures, which should emulate more closely the gen-eral knowledge about event participants that seemsto be the organizing principle of our mental lexicon.A core issue of previous studies was the data sparse-ness challenge, and we coped with it by means of amore abstract, syntactic notion of joint context.The models using joint dependencies were ableat least to perform comparably to traditional,dependency-based DSMs.
In our experiments, theyeven achieved the best correlation scores across sev-eral parameter settings, especially after the applica-tion of SVD.
We want to emphasize that previousworks such as Agirre et al (2009) already showedthat large word windows can have a higher discrimi-native power than indipendent features, but they didit by using a huge training corpus.
In our study, jointcontext-based representations derived from a smallcorpus such as RCV1 are already showing competi-tive performances.
This result strengthens our beliefthat dependencies are a possible solution for the datasparsity problem of joint feature spaces.We also believe that verb similarity might not bethe best task to show the usefulness of joint con-texts for semantic representation.
The main goal ofthe present paper was to show that joint contextsare a viable option to exploit the full potential ofdistributional information.
Our successful tests onverb similarity prove that syntactic joint contexts donot suffer of data sparsity and are also able to beatother types of representations based on independentword features.
Moreover, syntactic joint contexts aremuch simpler and more competitive with respect towindow-based ones.The good performance in the verb similarity taskmotivates us to further test syntactic joint contextson a larger range of tasks, such as word sense dis-ambiguation, textual entailment and classification ofsemantic relations, so that they can unleash their fullpotential.
Moreover, our proposal opens interest-ing perspectives for computational psycholinguis-tics, especially for modeling those semantic phe-nomena that are inherently related to the activationof event knowledge (e.g.
thematic fit).AcknowledgmentsThis paper is partially supported by HK PhD Fellow-ship Scheme, under PF12-13656.
Emmanuele Cher-soni?s research is funded by a grant of the UniversityFoundation A*MIDEX.ReferencesEneko Agirre, Enrique Alfonseca, Keith Hall, JanaKravalova, Marius Pas?ca, and Aitor Soroa.
2009.
Astudy on similarity and relatedness using distributionaland wordnet-based approaches.
In Proceedings of the2009 conference of the NAACL-HLT, pages 19?27.
As-sociation for Computational Linguistics.Giuseppe Attardi, Felice Dell?Orletta, Maria Simi, andJoseph Turian.
2009.
Accurate dependency parsingwith a stacked multilayer perceptron.
In Proceedingsof EVALITA, 9.Marco Baroni and Alessandro Lenci.
2010.
Distribu-tional memory: A general framework for corpus-basedsemantics.
Computational Linguistics, 36(4):673?721.Klinton Bicknell, Jeffrey L Elman, Mary Hare, KenMcRae, and Marta Kutas.
2010.
Effects of event1971knowledge in processing verbal arguments.
Journalof Memory and Language, 63(4):489?505.Elia Bruni, Nam-Khanh Tran, and Marco Baroni.
2014.Multimodal distributional semantics.
J. Artif.
Intell.Res.
(JAIR), 49(1-47).Kenneth Ward Church and Patrick Hanks.
1990.
Wordassociation norms, mutual information, and lexicogra-phy.
Computational linguistics, 16(1):22?29.Felice Dell?Orletta.
2009.
Ensemble system for part-of-speech tagging.
In Proceedings of EVALITA, 9.Christiane Fellbaum.
1998.
WordNet.
Wiley Online Li-brary.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and EytanRuppin.
2001.
Placing search in context: The con-cept revisited.
In Proceedings of the 10th internationalconference on World Wide Web, pages 406?414.
ACM.Clayton Greenberg, Asad Sayeed, and Vera Demberg.2015.
Improving unsupervised vector-space thematicfit evaluation via role-filler prototype clustering.
InProceedings of the 2015 conference of the NAACL-HLT, Denver, USA.Mary Hare, Michael Jones, Caroline Thomson, SarahKelly, and Ken McRae.
2009.
Activating event knowl-edge.
Cognition, 111(2):151?167.Zellig S Harris.
1954.
Distributional structure.
Word,10(2-3):146?162.Felix Hill, Roi Reichart, and Anna Korhonen.
2015.Simlex-999: Evaluating semantic models with (gen-uine) similarity estimation.
Computational Linguis-tics.Reinhard Kneser and Hermann Ney.
1995.
Improvedbacking-off for m-gram language modeling.
In Acous-tics, Speech, and Signal Processing, 1995.
ICASSP-95., 1995 International Conference on, volume 1,pages 181?184.
IEEE.Thomas K Landauer, Peter W Foltz, and Darrell Laham.1998.
An introduction to latent semantic analysis.Discourse processes, 25(2-3):259?284.Alessandro Lenci.
2011.
Composing and updatingverb argument expectations: A distributional semanticmodel.
In Proceedings of the 2nd Workshop on Cog-nitive Modeling and Computational Linguistics, pages58?66.
Association for Computational Linguistics.David D Lewis, Yiming Yang, Tony G Rose, and Fan Li.2004.
Rcv1: A new benchmark collection for text cat-egorization research.
The Journal of Machine Learn-ing Research, 5:361?397.Ken McRae, Michael J Spivey-Knowlton, and Michael KTanenhaus.
1998.
Modeling the influence of the-matic fit (and other constraints) in on-line sentencecomprehension.
Journal of Memory and Language,38(3):283?312.Ken McRae, Mary Hare, Jeffrey L Elman, and Todd Fer-retti.
2005.
A basis for generating expectancies forverbs from nouns.
Memory & Cognition, 33(7):1174?1184.Oren Melamud, Ido Dagan, Jacob Goldberger, IdanSzpektor, and Deniz Yuret.
2014.
Probabilistic mod-eling of joint-context in distributional similarity.
InCoNLL, pages 181?190.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient estimation of word representa-tions in vector space.
arXiv preprint arXiv:1301.3781.Maria Ruiz-Casado, Enrique Alfonseca, and PabloCastells.
2005.
Using context-window overlappingin synonym discovery and ontology extension.
In Pro-ceedings of RANLP, pages 1?7.Magnus Sahlgren.
2008.
The distributional hypothesis.Italian Journal of Linguistics, 20(1):33?54.Enrico Santus, Emmanuele Chersoni, Alessandro Lenci,Chu-Ren Huang, and Philippe Blache.
2016.
TestingAPSyn against Vector Cosine on Similarity Estima-tion.
In Proceedings of the Pacific Asia Conference onLanguage, Information and Computing (PACLIC 30).Asad Sayeed and Vera Demberg.
2014.
Combining un-supervised syntactic and semantic models of thematicfit.
In Proceedings of the first Italian Conference onComputational Linguistics (CLiC-it 2014).Peter D Turney, Patrick Pantel, et al 2010.
From fre-quency to meaning: Vector space models of semantics.Journal of artificial intelligence research, 37(1):141?188.Julie Weeds, David Weir, and Diana McCarthy.
2004.Characterising measures of lexical distributional simi-larity.
In Proceedings of the 20th international confer-ence on Computational Linguistics, page 1015.
Asso-ciation for Computational Linguistics.Dongqiang Yang and David MW Powers.
2006.
Verbsimilarity on the taxonomy of WordNet.
Masaryk Uni-versity.1972
