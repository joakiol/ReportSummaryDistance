Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 9?16,The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational LinguisticsComparing Local and Sequential Models forStatistical Incremental Natural Language UnderstandingSilvan Heintze, Timo Baumann, David SchlangenDepartment of LinguisticsUniversity of Potsdam, Germanyfirstname.lastname@uni-potsdam.deAbstractIncremental natural language understand-ing is the task of assigning semantic rep-resentations to successively larger prefixesof utterances.
We compare two types ofstatistical models for this task: a) localmodels, which predict a single class foran input; and b), sequential models, whichalign a sequence of classes to a sequenceof input tokens.
We show that, with somemodifications, the first type of model canbe improved and made to approximate theoutput of the second, even though the lat-ter is more informative.
We show on twodifferent data sets that both types of modelachieve comparable performance (signifi-cantly better than a baseline), with the firsttype requiring simpler training data.
Re-sults for the first type of model have beenreported in the literature; we show that forour kind of data our more sophisticatedvariant of the model performs better.1 IntroductionImagine being at a dinner, when your friend Bertsays ?My friend, can you pass me the salt overthere, please??.
It is quite likely that you get theidea that something is wanted of you fairly earlyinto the utterance, and understand what exactly itis that is wanted even before the utterance is over.This is possible only because you form an un-derstanding of the meaning of the utterance evenbefore it is complete; an understanding whichyou refine?and possibly revise?as the utterancegoes on.
You understand the utterance incremen-tally.
This is something that is out of reach formost current dialogue systems, which process ut-terances non-incrementally, en bloc (cf.
(Skantzeand Schlangen, 2009), inter alia).Enabling incremental processing in dialoguesystems poses many challenges (Allen et al,2001; Schlangen and Skantze, 2009); we focushere on the sub-problem of modelling incrementalunderstanding?a precondition for enabling trulyinteractive behaviour.
More specifically, we lookat statistical methods for learning mappings be-tween (possibly partial) utterances and meaningrepresentations.
We distinguish between two typesof understanding, which were sketched in the firstparagraph above: a) forming a partial understand-ing, and b) predicting a complete understanding.Recently, some results have been published onb), predicting utterance meanings, (Sagae et al,2009; Schlangen et al, 2009).
We investigatehere how well this predictive approach works intwo other domains, and how a simple extension oftechniques (ensembles of slot-specific classifiersvs.
one frame-specific one) can improve perfor-mance.
To our knowledge, task a), computing par-tial meanings, has so far only been tackled withsymbolic methods (e.g., (Milward and Cooper,1994; Aist et al, 2006; Atterer and Schlangen,2009));1 we present here some first results on ap-proaching it with statistical models.Plan of the paper: First, we discuss relevant pre-vious work.
We then define the task of incrementalnatural language understanding and its two vari-ants in more detail, also looking at how modelscan be evaluated.
Finally, we present and discussthe results of our experiments, and close with aconclusion and some discussion of future work.2 Related WorkStatistical natural language understanding is an ac-tive research area, and many sophisticated mod-els for this task have recently been published, bethat generative models (e.g., in (He and Young,2005)), which learn a joint distribution over in-1We explicitly refer to computation of incremental inter-pretations here; there is of course a large body of work onstatistical incremental parsing (e.g., (Stolcke, 1995; Roark,2001)).9(Mairesse et al, 2009) 94.50(He and Young, 2005) 90.3(Zettlemoyer and Collins, 2007) 95.9(Meza et al, 2008) 91.56Table 1: Recent published f-scores for non-incremental statistical NLU, on the ATIS corpusput, output and possibly hidden variables; or, morerecently, discriminative models (e.g., (Mairesse etal., 2009)) that directly learn a mapping betweeninput and output.
Much of this work uses the ATIScorpus (Dahl et al, 1994) as data and hence is di-rectly comparable.
In Table 1, we list the resultsachieved by this work; we will later situate our re-sults relative to this.That work, however, only looks at mappings be-tween complete utterances and semantic represen-tations, whereas we are interested in the process ofmapping semantic representations to successivelylarger utterance fragments.
More closely relatedthen is (Sagae et al, 2009; DeVault et al, 2009),where a maximum entropy model is trained formapping utterance fragments to semantic frames.
(Sagae et al, 2009) make the observation that of-ten the quality of the prediction does not increaseanymore towards the end of the utterance; that is,the meaning of the utterance can be predicted be-fore it is complete.In (Schlangen et al, 2009), we presented amodel that predicts incrementally a specific as-pect of the meaning of a certain type of utterance,namely the intended referent of a referring expres-sion; the similarity here is that the output is of thesame type regardless of whether the input utter-ance is complete or not.
(DeVault et al, 2009) discuss how such ?mindreading?
can be used interactionally in a dialoguesystem, e.g.
for completing the user?s utteranceas an indication of the system?s grounding state.While these are interesting uses, the approach issomewhat limited by the fact that it is incrementalonly on the input side, while the output does notreflect how ?complete?
(or not) the input is.
Wewill compare this kind of incremental processingin the next section with one where the output isincremental as well, and we will then present re-sults from our own experiments with both kinds ofincrementality in statistical NLU.3 Task, Evaluation, and Data Sets3.1 The TaskWe have said that the task of incremental naturallanguage understanding consists in the assignmentof semantic representations to progressively morecomplete prefixes of utterances.
This descriptioncan be specified along several aspects, and thisyields different versions of the task, appropriatefor different uses.
One question is what the as-signed representations are, the other is what ex-actly they are assigned to.
We investigate thesequestions here abstractly, before we discuss the in-stantiations in the next sections.Let?s start by looking at the types of representa-tions that are typically assigned to full utterances.A type often used in dialogue systems is the frame,an attribute value matrix.
(The attributes are heretypically called slots.)
These frames are normallytyped, that is, there are restrictions on which slotscan (and must) occur together in one frame.
Theframes are normally assigned to the utterance as awhole and not to individual words.In an incremental setting, where the inputpotentially consists of an incomplete utterance,choosing this type of representation and style ofassignment turns the task into one of prediction ofthe utterance meaning.
What we want our modelto deliver is a guess of what the meaning of the ut-terance is going to be, even if we have only seena prefix of the utterance so far; we will call this?whole-frame output?
below.2Another popular representation of semantics inapplied systems uses semantic tags, i.e., markersof semantic role that are attached to individualparts of the utterance.
Such a style of assignmentis inherently ?more incremental?, as it provides away to assign meanings that represent only whathas indeed been said so far, and does not make as-sumptions about what will be said.
The semanticrepresentation of the prefix simply contains all andonly the tags assigned to the words in the prefix;this will be called ?aligned output?
below.
To ourknowledge, the potential of this type of represen-tation (and the models that create them) for incre-mental processing has not yet been explored; wepresent our first results below.Finally, there is a hybrid form of representationand assignment.
If we allow the output frames to?grow?
as more input comes in (hence possibly vi-olating the typing of the frames as they are ex-pected for full utterances), we get a form of rep-resentation with a notion of ?partial semantics?
(as2In (Schlangen and Skantze, 2009), this type of incremen-tal processing is called ?input incremental?, as only the inputis incrementally enriched, while the output is always of thesame type (but may increase in quality).10only that is represented for which there is evidencein what has already been seen), but without directassociation of parts of the representation and partsof the utterance or utterance prefix.3.2 EvaluationWhole-Frame Output A straightforward met-ric is Correctness, which can take the values 1(output is exactly as expected) or 0 (output is notexactly as expected).
Processing a test corpus inthis way, we get one number for each utteranceprefix, and, averaging this number, one measure-ment for the whole corpus.This can give us a first indication of the gen-eral quality of the model, but because it weighsthe results for prefixes of all lengths equally, itcannot tell us much about how well the incremen-tal processing worked.
In actual applications, wepresumably do not expect the model to be correctfrom the very first word on, but do expect it to getbetter the longer the available utterance prefix be-comes.
To capture this, we define two more met-rics: first occurrence (FO), as the position (relativeto the eventual length of the full utterance) wherethe response was correct first; and final decision(FD) as the position from which on the responsestayed correct (which consequently can only bemeasured if indeed the response stays correct).3The difference between FO and FD then tells ussomething about the stability of hypotheses of themodel.In some applications, we may indeed only beable to do further processing with fully correct?or at least correctly typed?frames; in which casecorrectness and FO/FD on frames are appropriatemetrics.
However, sometimes even frames that areonly partially correct can be of use, for example ifspecific system reactions can be tied to individualslots.
To give us more insight about the quality of amodel in such cases, we need a metric that is finer-grained than binary correctness.
Following (Sagaeet al, 2009), we can conceptualise our task as oneof retrieval of slot/value pairs, and use precisionand recall (and, as their combination, f-score) asmetrics.
As we will see, it will be informative toplot the development of this score over the courseof processing the utterance.For these kinds of evaluations, we need as agold standard only one annotation per utterance,3These metrics of course can only be computed post-hoc,as during processing we do not know how long the utteranceis going to be.namely the final frame.Aligned Output As sequence alignments havemore structure?there is a linear order between thetags, and there is exactly one tag per input token?correctness is a more fine-grained, and hence moreinformative, metric here; we define it as the pro-portion of tags that are correct in a sequence.
Wecan also use precision and recall here, looking ateach position in the sequence individually: Hasthe tag been recalled (true positive), or has some-thing else been predicted instead (false negative,and false positive)?
Lastly, we can also recon-struct frames from the tag sequences, where se-quences of the same tag are interpreted as seg-menting off the slot value.
(And hence, what wasseveral points for being right or wrong, one foreach tag, becomes one, being either the correctslot value or not.
We will discuss these differenceswhen we show evaluations of aligned output.
)For this type of evaluation, we need gold-standard information of the same kind, that is, weneed aligned tag sequences.
This information ispotentially more costly to create than the one fi-nal semantic representation needed for the whole-frame setting.Hybrid Output As we will see below, the hy-brid form of output (?growing?
frames) is pro-duced by ensembles of local classifiers, with oneclassifier for each possible slot.
How this outputcan be evaluated depends on what type of informa-tion is available.
If we only have the final frame,we can calculate f-score (in the hope that preci-sion will be better than for the whole-frame clas-sifier, as such a classifier ensemble can focus onpredicting slots/value pairs for which there is di-rect evidence); if we do have sequence informa-tion, we can convert it to growing frames and eval-uate against that.3.3 The Data SetsATIS As our first dataset, we use the ATIS airtravel information data (Dahl et al, 1994), as pre-processed by (Meza et al, 2008) and (He andYoung, 2005).
That is, we have available for eachutterance a semantic frame as in (1), and also atag sequence that aligns semantic concepts (sameas the slot names) and words.
One feature to notehere about the ATIS representations is that the slotvalues / semantic atoms are just the words in theutterance.
That is, the word itself is its own se-mantic representation, and no additional abstrac-11tion is performed.
In this domain, this is likely un-problematic, as there aren?t many different ways(that are to be expected in this domain) to refer toa given city or a day of the week, for example.
(1) ?What flights are there arriving in Chicago after11pm?????
?GOAL = FLIGHTTOLOC.CITY NAME = ChicagoARRIVE TIME.TIME RELATIVE = afterARRIVE TIME.TIME = 11pm???
?In our experiments, we use the ATIS trainingset which contains 4481 utterances, between 1and 46 words in length (average 11.46; sd 4.34).The vocabulary consists of 897 distinct words.There are 3159 distinct frames, 2594 (or 58% ofall frames) of which occur only once.
Which ofthe 96 possible slots occur in a given frame isdistributed very unevenly; there are some veryfrequent slots (like FROMLOC.CITYNAMEor DEPART DATE.DAY NAME) andsome very rare or even unique ones (e.g.,ARRIVE DATE.TODAY RELATIVE, orTIME ZONE).Pentomino The second corpus we use is of ut-terances in a domain that we have used in muchprevious work (e.g., (Schlangen et al, 2009;Atterer and Schlangen, 2009; Ferna?ndez andSchlangen, 2007)), namely, instructions for ma-nipulating puzzle pieces to form shapes.
The par-ticular version we use here was collected in aWizard-of-Oz study, where the goal was to instructthe computer to pick up, delete, rotate or mirrorpuzzle tiles on a rectangular board, and drop themon another one.
The user utterances were anno-tated with semantic frames and also aligned withtag sequences.
We use here a frame representationwhere the slot value is a part of the utterance (asin ATIS), an example is shown in (2).
(The cor-pus is in German; the example is translated herefor presentation.)
We show the full frame here,with all possible slots; unused slots are filled with?empty?.
Note that this representation is some-what less directly usable in this domain than forATIS; in a practical system, we?d need some fur-ther module (rule-based or statistical) that mapssuch partial strings to their denotations, as thismapping is less obvious here than in the travel do-main.
(2) ?Pick up the W-shaped piece in the upper right cor-ner???????
?action = ?pick up?tile = ?the W-shaped piecein the upper right corner?field = emptyrotpar = emptymirpar = empty??????
?The corpus contains 1563 utterances, averagelength 5.42 words (sd 2.35), with a vocabulary of222 distinct words.
There are 964 distinct frames,with 775 unique frames.In both datasets we use transcribed utterancesand not ASR output, and hence our results presentan upper bound on real-world performance.4 Local Models: Support Vector MachinesIn this section we report the results of our exper-iments with local classifiers, i.e.
models which,given an input, predict one out of a set of classes asan answer.
Such models are very naturally suitedto the prediction task, where the semantics of thefull utterance is treated as its class, which is to bepredicted on the basis of what possibly is only aprefix of that utterance.
We will also look at asimple modification, however, which enables suchmodels to do something that is closer to the task ofcomputing partial meanings.4.1 Experimental SetupFor our experiments with local models, we usedthe implementations of support vector machinesprovided by the WEKA toolkit (Witten and Frank,2005); as baseline we use a simple majority classpredictor.4We used the standard WEKA tools to convertthe utterance strings into word vectors.
Trainingwas always done with the full utterance, but test-ing was done on prefixes of utterances; i.e., a sen-tence with 5 words would be one instance in train-ing, but in a testing fold it would contribute 5 in-stances, one with one word, one with two words,and so on.5 Because of this special way of testingthe classifiers, and also because of the modifica-4We tried other classifiers (C4.5, logistic regression, naiveBayes) as well, and found comparable performance on a de-velopment set.
However, because of the high time costs(some models needed > 40 hours for training and testing onmodern multi-CPU servers) we do not systematically com-pare performance and instead focus on SVMs.
In any case,our interest here is not in comparing classification algorithms,but rather in exploring approaches to the novel problem ofstatistical incremental NLU.5On a development set, we tried training on utterance pre-fixes, but that degraded performance, presumably due to in-crease in ambiguous training instances (same beginnings ofwhat ultimately are very different utterances).12tions described below, we had to provide our ownmethods for cross-validation and evaluation.
Forthe larger ATIS data set, we used 10 folds in crossvalidation, and for the Pentomino dataset 20 folds.4.2 ResultsTo situate our results, we begin by looking atthe performance of the models that predict a fullframe, when given a full utterance; this is thenormal, ?non-incremental?
statistical NLU task.6(3)classf.
metric ATIS Pentomaj correctness 1.07 1.79maj f-score 35.98 16.15SVM correctness 16.21 38.77SVM f-score 68.17 63.23We see that the results for ATIS are considerablylower than the state of the art in statistical NLU(Table 1).
This need not concern us too muchhere, as we are mostly interested in the dynam-ics of the incremental process, but it indicates thatthere is room for improvement with more sophisti-cated models and feature design.
(We will discussan example of an improved model shortly.)
Wealso see a difference between the corpora reflectedin these results: being exactly right (good correct-ness) seems to be harder on the ATIS corpus, whilebeing somewhat right (good f-score) seems to beharder on the pento corpus; this is probably due tothe different sizes of the search space of possibleframe types (large for ATIS, small for pento).What we are really interested in, however, is theperformance when given only a prefix of an ut-terance, and how this develops over the course ofprocessing successively larger prefixes.
We caninvestigate this with Figure 1.
First, look at thesolid lines.
The black line shows the average f-score at various prefix lengths (in 10% steps) forthe ATIS data, the grey line for the pento corpus.We see that both lines show a relatively steady in-cline, meaning that the f-score continues to im-prove when more of the utterance is seen.
This isinteresting to note, as both (DeVault et al, 2009)and (Atterer et al, 2009) found that in their data,all that is to be known can often be found some-what before the end of the utterance.
That thisdoes not work so well here is most likely due tothe difference in domain and the resulting utter-ances.
Utterances giving details about travel plans6The results for ATIS are based on half of the overallATIS data, as cross-validating the model on all data took pro-hibitively long, presumably due to the large number of uniqueframes / classes.2 4 6 8 100.00.20.40.60.8percentiles into utterancef?scorel llllllll ll all utterancesshort utterancesnormal utteranceslong utteranceslllllllll lFigure 1: F-Score by Length of Prefixare likely to present many important details, andsome of them late into the utterance; cf.
(1) above.The data from (DeVault et al, 2009) seems to bemore conversational in nature, and, more impor-tantly, presumable the expressible goals are lessclosely related to each other and hence can be readoff of shorter prefixes.As presented so far, the results are not veryhelpful for practical applications of incrementalNLU.
One thing one would like to know in a prac-tical situation is how much the prediction of themodel can be trusted for a given partial utterance.We would like to read this off graphs like thosein the Figure?but of course, normally we cannotknow what percentage of an utterance we have al-ready seen!
Can we trust this averaged curve if wedo not know what length the incoming utterancewill have?To investigate this question, we have binned thetest utterances into three classes, according to theirlength: ?normal?, for utterances that are of aver-age length?
half a standard deviation, and ?short?for all that are shorter, and ?long?
for all that arelonger.
The f-score curves for these classes areshown with the non-solid lines in Figure 1.
Wesee that for ATIS there is not much variation com-pared to averaging over all utterances, and more-over, that the ?normal?
class very closely followsthe general curve.
On the pento data, the modelseems to be comparably better for short utterances.In a practical application, one could go withthe assumption that the incoming utterance is go-ing to be of normal length, and use the ?normal?13curve for guidance; or one could devise an ad-ditional classifier that predicts the length-class ofthe incoming utterance, or more generally predictswhether a frame can already be trusted (DeVault etal., 2009).
We leave this for future work.As we have seen, the models that treat the se-mantic frame simply as a class label do not fareparticularly well.
This is perhaps not that surpris-ing; as discussed above, in our corpora there aren?tthat many utterances with exact the same frame.Perhaps it would help to break up the task, andtrain individual classifiers for each slot?7 Thisidea can be illustrated with (2) above.
There we al-ready included ?unused?
slots in the frame; if wenow train classifiers for each slot, allowing themto predict ?empty?
in cases where a slot is unused,we can in theory reconstruct any frame from theensemble of classifiers.
To cover the pento data,the ensemble is small (there are 5 frames); it isconsiderably larger for ATIS, where there are somany distinct slots.Again we begin by looking at the performancefor full utterances (i.e., at 100% utterance length),but this time for constructing the frame from thereply of the classifier ensemble:(4)classf.
metric ATIS Pentomaj correctness 0.16 0maj f-score 33.18 20.24SVM correctness 52.69 50.48SVM f-score 86.79 73.15We see that this approach leads to an impressiveimprovement on the ATIS data (83.64 f-score in-stead of 68.17), whereas the improvement on thepento data is more modest (73.15 / 63.23).Figure 2 shows the incremental development ofthe f-scores for the reconstructed frame.
We seea similar shape in the curves; again a relativelysteady incline for ATIS and a more dramatic shapefor pento, and again some differences in behaviourfor the different length classes of utterances.
How-ever, by just looking at the reconstructed frame,we are ignoring valuable information that the slot-classifier approach gives us.
In some applications,we may already be able to do something usefulwith partial information; e.g., in the ATIS domain,we could look up an airport as soon as a FROM-LOC becomes known.
Hence, we?d want morefine-grained information, not just about when wecan trust the whole frame, but rather about when7A comparable approach is used for the non-incrementalcase for example by (Mairesse et al, 2009).2 4 6 8 100.00.20.40.60.8percentiles into utterancef?scorel llllllllll all utterancesshort utterancesnormal utteranceslong utterancesllll lllll lFigure 2: F-Score by Length of Prefix; Slot Clas-sifierswe can trust individual predicted slot values.
(Andso we move from the prediction task to the partialrepresentations task.
)To explore this, we look at First Occurrence andFinal Decision for some selected slots in Table 2.For some slots, the first occurrence (FO) of thecorrect value comes fairly early into the utterance(e.g., for the name of the airline it?s at ca.
60%,for the departure city at ca.
63%, both with rela-tively high standard deviation, though) while oth-ers are found the first time rather late (goal cityat 81%).
This conforms well with intuitions abouthow such information would be presented in an ut-terance (?I?d like to fly on Lufthansa from Berlinto Tokyo?
).We also see that the predictions are fairly stable:the number of cases where the slot value stays cor-rect until the end is almost the same as that whereit is correct at least once (FD applicable vs. FOapl), and the average position is almost the same.In other words, the classifiers seem to go fairlyreliably from ?empty?
(no value) to the correctvalue, and then seem to stay there.
The overheadof unnecessary edits (EO) is fairly low for all slotsshown in the table.
(Ideally, EO is 0, meaning thatthere is no change except the one from ?empty?
tocorrect value.)
All this is good news, as it meansthat a later module in a dialogue system can oftenbegin to work with the partial results as soon asa slot-classifier makes a non-empty prediction.
Inan actual application, how trustworthy the individ-ual classifiers are would then be read off statistics14slot name avg FO stdDev apl avg FD stdDev apl avg EO stdDev aplAIRLINE NAME 0.5914 0.2690 506 0.5909 0.2698 501 0.5180 0.5843 527DEPART TIME.PERIOD OF DAY 0.7878 0.2506 530 0.7992 0.2476 507 0.2055 0.5558 579FLIGHT DAYS 0.4279 0.2660 37 0.4279 0.2660 37 0.0000 0.0000 37FROMLOC.CITY NAME 0.6345 0.1692 3633 0.6368 0.1692 3554 0.1044 0.4526 3718ROUND TRIP 0.5366 0.2140 287 0.5366 0.2140 287 0.0104 0.1015 289TOLOC.CITY NAME 0.8149 0.1860 3462 0.8162 0.1856 3441 0.2348 0.5723 3628frames 0.9745 0.0811 2382 0.9765 0.0773 2361 0.7963 1.1936 4481Table 2: FO/FD/EO for some selected slots; averaged over utterances of all lengthslike these, given a corpus from the domain.To conclude this section, we have shown thatclassifiers that predict a complete frame based onutterance prefixes have a somewhat hard task here(harder, it seems, than in the corpus used in (Sagaeet al, 2009), where they achieve an f-score of 87on transcribed utterances), and the prediction re-sults improve steadily throughout the whole utter-ance, rather than reaching their best value beforeits end.
When the task is ?spread?
over severalclassifiers, with each one responsible for only oneslot, performance improves drastically, and also,the results become much more ?incremental?.
Wenow turn to models that by design are more incre-mental in this sense.5 Sequential Models: ConditionalRandom Fields5.1 Experimental SetupWe use Conditional Random Fields (Lafferty etal., 2001) as our representative of the class of se-quential models, as implemented in CRF++.8 Weuse a simple template file that creates featuresbased on a left context of three words.Even though sequential models have the poten-tial to be truly incremental (in the sense that theycould produce a new output when fed a new in-crement, rather than needing to process the wholeprefix again), CRF++ is targeted at tagging appli-cations, and expects full sequences.
We hence testin the same way as the SVMs from the previoussection, by computing a new tag sequence for eachprefix.
Training again is done only on full utter-ances / tag sequences.We compare the CRF results against two base-lines.
The simplest consists of just always choos-ing the most frequent tag, which is ?O?
(for other,marking material that does not contribute directlyto the relevant meaning of the utterance, suchas ?please?
in ?I?d like to return on Monday,please.?).
The other baseline tags each word with8http://crfpp.sourceforge.net/2 4 6 8 100.50.60.70.80.91.0percentiles into utterancef?scorel l l l l l l l l ll all utterancesshort utterancesnormal utteranceslong utterancesl l l l l l l l lFigure 3: F-Score by Length of PrefixATIS Corr.
Tag F-Score Frame F-ScoreCRF 93.38 82.56 76.10Maj 85.14 60.86 48.08O 63.43 00.31 00.31Pento Corr.
Tag F-Score Frame F-ScoreCRF 89.19 88.95 76.94Maj 80.20 80.13 65.94O 5.90 0.19 0.19Table 3: Results of CRF modelsits most frequent training data tag.5.2 ResultsWe again begin by looking at the limiting case, theresults for full utterances (i.e., at the 100%mark).Table 3 show three sets of results for each cor-pus.
Correctness looks at the proportion of tagsin a sequence that were correct.
This measure isdriven up by correct recognition of the dummytag ?o?
; as we can see, this is quite frequentlycorrect in ATIS, which drives up the ?always useO?-baseline.
Tag F-Score values the importanttags higher; we see here, though, that the majoritybaseline (each word tagged with its most frequenttag) is surprisingly good.
It is solidly beaten forthe ATIS data, though.
On the pento data, withits much smaller tagset (5 as opposed to 95), thisbaseline comes very high, but still the learner isable to get some improvement.
The last metricevaluates reconstructed frames.
It is stricter, be-cause it offers less potential to be right (a sequenceof the same tag will be translated into one slotvalue, turning several opportunities to be right into15only one).The incremental dynamics looks quite differenthere.
Since the task is not one of prediction, wedo not expect to get better with more information;rather, we start at an optimal point (when nothingis said, nothing can be wrong), and hope that wedo not amass too many errors along the way.
Fig-ure 3 confirms this, showing that the classifier isbetter able to keep the quality for the pento datathan for the ATIS data.
Also, there is not muchvariation depending on the length of the utterance.6 ConclusionsWe have shown how sequential and local statisticalmodels can be used for two variants of the incre-mental NLU task: prediction, based on incompleteinformation, and assignment of partial representa-tions to partial input.
We have shown that break-ing up the prediction task by using an ensembleof classifiers improves performance, and creates ahybrid task that sits between prediction and incre-mental interpretation.While the objective quality as measured by ourmetrics is quite good, what remains to be shown ishow such models can be integrated into a dialoguesystem, and how what they offer can be turned intoimprovements on interactivity.
This is what we areturning to next.Acknowledgements Funded by ENP grant from DFG.ReferencesG.S.
Aist, J. Allen, E. Campana, L. Galescu, C.A.Gomez Gallo, S. Stoness, M. Swift, and M Tanenhaus.2006.
Software architectures for incremental understand-ing of human speech.
In Proceedings of the Interna-tional Conference on Spoken Language Processing (IC-SLP), Pittsburgh, PA, USA, September.James Allen, George Ferguson, and Amanda Stent.
2001.An architecture for more realistic conversational systems.In Proceedings of the conference on intelligent user inter-faces, Santa Fe, USA, June.Michaela Atterer and David Schlangen.
2009.
RUBISC ?a robust unification-based incremental semantic chunker.In Proceedings of the 2nd International Workshop on Se-mantic Representation of Spoken Language (SRSL 2009),Athens, Greece, March.Michaela Atterer, Timo Baumann, and David Schlangen.2009.
No sooner said than done?
testing incrementality ofsemantic interpretations of spontaneous speech.
In Pro-ceedings of Interspeech 2009, Brighton, UK, September.Deborah A. Dahl, Madeleine Bates, Michael Brown, WilliamFisher, Kate Hunicke-Smith, David Pallett, Christine Pao,Alexander Rudnicky, and Elizabeth Shriberg.
1994.
Ex-panding the scope of the atis task: the atis-3 corpus.
InProceedings of the workshop on Human Language Tech-nology, pages 43?48, Plainsboro, NJ, USA.David DeVault, Kenji Sagae, and David Traum.
2009.
Cani finish?
learning when to respond to incremental inter-pretation results in interactive dialogue.
In Proceedingsof the 10th Annual SIGDIAL Meeting on Discourse andDialogue (SIGDIAL?09), London, UK, September.Raquel Ferna?ndez and David Schlangen.
2007.
Referringunder restricted interactivity conditions.
In Simon Keizer,Harry Bunt, and Tim Paek, editors, Proceedings of the8th SIGdial Workshop on Discourse and Dialogue, pages136?139, Antwerp, Belgium, September.Yulan He and Steve Young.
2005.
Semantic processing us-ing the hidden vector state model.
Computer Speech andLanguage, 19(1):85?106.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Conditionalrandom fields: Probabilistic models for segmenting andlabeling sequence data.
In Proc.
of ICML, pages 282?289.F.
Mairesse, M. Gasic, F. Jurcicek, S. Keizer, B. Thomson,K.
Yu, and S. Young.
2009.
Spoken language understand-ing from unaligned data using discriminative classificationmodels.
In Proceedings of the 2009 IEEE InternationalConference on Acoustics, Speech and Signal Processing,Taipei, Taiwan, April.Ivan Meza, Sebastian Riedel, and Oliver Lemon.
2008.
Ac-curate statistical spoken language understanding from lim-ited development resources.
In In Proceedings of ICASSP.David Milward and Robin Cooper.
1994.
Incremental in-terpretation: Applications, theory, and relationships to dy-namic semantics.
In Proceedings of COLING 1994, pages748?754, Kyoto, Japan, August.Brian Roark.
2001.
Robust Probabilistic Predictive Syntac-tic Processing: Motivations, Models, and Applications.Ph.D.
thesis, Department of Cognitive and Linguistic Sci-ences, Brown University.Kenji Sagae, Gwen Christian, David DeVault, and DavidTraum.
2009.
Towards natural language understand-ing of partial speech recognition results in dialogue sys-tems.
In Short paper proceedings of the North Ameri-can chapter of the Association for Computational Linguis-tics - Human Language Technologies conference (NAACL-HLT?09), Boulder, Colorado, USA, June.David Schlangen and Gabriel Skantze.
2009.
A general, ab-stract model of incremental dialogue processing.
In Pro-ceedings of the 12th Conference of the European Chapterof the Association for Computational Linguistics (EACL2009), pages 710?718, Athens, Greece, March.David Schlangen, Timo Baumann, and Michaela Atterer.2009.
Incremental reference resolution: The task, met-rics for evaluation, and a bayesian filtering model that issensitive to disfluencies.
In Proceedings of SIGdial 2009,the 10th Annual SIGDIAL Meeting on Discourse and Di-alogue, London, UK, September.Gabriel Skantze and David Schlangen.
2009.
Incrementaldialogue processing in a micro-domain.
In Proceedingsof the 12th Conference of the European Chapter of theAssociation for Computational Linguistics (EACL 2009),pages 745?753, Athens, Greece, March.Andreas Stolcke.
1995.
An efficient probabilistic context-free parsing algorithm that computes prefix probabilities.Computational Linguistics, 21(2):165?201.Ian H. Witten and Eibe Frank.
2005.
Data Mining: Practi-cal machine learning tools and techniques.
Morgan Kauf-mann, San Francisco, USA, 2nd edition.Luke S. Zettlemoyer and Michael Collins.
2007.
Onlinelearning of relaxed ccg grammars for parsing to logicalform.
In Proceedings of EMNLP-CoNLL.16
