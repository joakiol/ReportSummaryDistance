First Joint Conference on Lexical and Computational Semantics (*SEM), pages 54?58,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsDetecting Text Reuse with Modified and Weighted N-gramsRao Muhammad Adeel Nawab?, Mark Stevenson?
and Paul Clough?
?Department of Computer Science and ?iSchoolUniversity of Sheffield, UK.
{r.nawab@dcs, m.stevenson@dcs, p.d.clough@} .shef.ac.ukAbstractText reuse is common in many scenarios anddocuments are often based, at least in part, onexisting documents.
This paper reports an ap-proach to detecting text reuse which identifiesnot only documents which have been reusedverbatim but is also designed to identify casesof reuse when the original has been rewrit-ten.
The approach identifies reuse by compar-ing word n-grams in documents and modifiesthese (by substituting words with synonymsand deleting words) to identify when text hasbeen altered.
The approach is applied to a cor-pus of newspaper stories and found to outper-form a previously reported method.1 IntroductionText reuse is the process of creating new docu-ment(s) using text from existing document(s).
Textreuse is standard practice in some situations, such asjournalism.
Applications of automatic detection oftext reuse include the removal of (near-)duplicatesfrom search results (Hoad and Zobel, 2003; Seo andCroft, 2008), identification of text reuse in journal-ism (Clough et al, 2002) and identification of pla-giarism (Potthast et al, 2011).Text reuse is more difficult to detect when theoriginal text has been altered.
We propose an ap-proach to the identification of text reuse which isintended to identify reuse in such cases.
The ap-proach is based on comparison of word n-grams, apopular approach to detecting text reuse.
However,we also account for synonym replacement and worddeletion, two common text editing operations (Bell,1991).
The relative importance of n-grams is ac-counted for using probabilities obtained from a lan-guage model.
We show that making use of modifiedn-grams and their probabilities improves identifica-tion of text reuse in an existing journalism corpusand outperforms a previously reported approach.2 Related WorkApproaches for identifying text reuse based onword-level comparison (such as the SCAM copy de-tection system (Shivakumar and Molina, 1995)) tendto identify topical similarity between a pair of doc-uments, whereas methods based on sentence-levelcomparison (e.g.
the COPS copy detection sys-tem (Brin et al, 1995)) are unable to identify whentext has been reused if only a single word has beenchanged in a sentence.Comparison of word and character n-grams hasproven to be an effective method for detecting textreuse (Clough et al, 2002; Ceden?o et al, 2009; Chiuet al, 2010).
For example, Ceden?o et al (2009)showed that comparison of word bigrams and tri-grams are an effective method for detecting reuse injournalistic text.
Clough et al (2002) also appliedn-gram overlap to identify reuse of journalistic text,combining it with other approaches such as sentencealignment and string matching algorithms.
Chiu etal.
(2010) compared n-grams to identify duplicateand reused documents on the web.
Analysis of wordn-grams has also proved to be an effective methodfor detecting plagiarism, another form of text reuse(Lane et al, 2006).However, a limitation of n-gram overlap approachis that it fails to identify reuse when the original54text has been altered.
To overcome this problem wepropose using modified n-grams, which have beenaltered by deleting or substituting words in the n-gram.
The modified n-grams are intended to im-prove matching with the original document.3 Determining Text Reuse with N-gramOverlap3.1 N-grams Overlap (NG)Following Clough et al (2002), the asymmetric con-tainment measure (eqn 1) was used to quantify thedegree of text within a document (A) that is likely tohave been reused in another document (B).scoren(A,B) =?ngram?Bcount(ngram,A)?ngram?Bcount(ngram,B)(1)where count(ngram,A) is the number of timesngram appears in document A.
A score of 1 meansthat document B is contained in document A and ascore of 0 that none of the n-grams in B occur in A.3.2 Modified N-gramsN-gram overlap has been shown to be useful formeasuring text reuse as derived texts typically sharelonger n-grams (?
3 words).
However, the approachbreaks down when an original document has beenaltered.
To counter this problem we applied vari-ous techniques for modifying n-grams that allow forword deletions (Deletions) and word substitutions(WordNet and Paraphrases), two common text edit-ing operations.Deletions (Del) Assume that w1, w2, ...wn is ann-gram.
Then a set of modified n-grams can be cre-ated by removing one of the w2 ... wn?1.
The firstand last words in the n-gram are not removed sincethey will also be generated as shorter n-grams.
Ann-gram will generate n ?
2 deleted n-grams and nodeleted n-grams will be generated for unigrams andbigrams.Substitutions Further n-grams can be created bysubstituting one of the words in an n-gram with oneof its synonyms from WordNet (WN).
For wordswith multiple senses we use synonyms from allsenses.
Modified n-grams are created by substitut-ing one of the words in the n-gram with one of itssynonyms from WordNet.Similarly to the WordNet approach, n-grams canbe created by substituting one of the words with anequivalent term from a paraphrase lexicon, whichwe refer to as Paraphrases (Para).
A paraphraselexicon was generated automatically (Burch, 2008)and ten lexical equivalents (the default setting) pro-duced for each word.
Modified n-grams were cre-ated by substituting one of the words in the n-gramwith one of the lexical equivalents.3.3 Comparing Modified N-gramsThe modified n-grams are applied in the text reusescore by generating modified n-grams for the docu-ment that is suspected to contain reused text.
Thesen-grams are then compared with the original docu-ment to determine the overlap.
However, the tech-niques in Section 3.2 generate a large number ofmodified n-grams which means that the numberof n-grams that overlap with document A can begreater than the total number of n-grams in B, lead-ing to similarity scores greater than 1.
To avoid thisthe n-gram overlap counts are constrained in a simi-lar way that they are clipped in BLEU and ROUGE(Papineni et al, 2002; Lin, 2004).For each n-gram in B, a set of modified n-grams,mod(ngram), is created.1 The count for an in-dividual n-gram in B, exp count(ngram,B), canbe computed as the number of times any n-gram inmod(ngram) occurs in A, see equation 2.?ngram?
?mod(ngram)count(ngram?, A) (2)However, the contribution of this count to the textreuse score has to be bounded to ensure that the com-bined count of the modified n-grams appearing inA does not exceed the number of times the origi-nal n-gram occurs in B. Consequently the text reusescore, scoren(A,B), is computed using equation 3.?ngram?Bmin(exp count(ngram,A), count(ngram,B))?ngram?Bcount(ngram,B)(3)3.4 Weighting N-gramsProbabilities of each n-gram, obtained using a lan-guage model, are used to increase the importance of1This is the set of n-grams that could have been created bymodifing an n-gram in B and includes the original n-gram itself.55rare n-grams and decrease the contribution of com-mon ones.
N-gram probabilities are computed us-ing the SRILM language modelling toolkit (Stolcke,2002).
The score for each n-gram is computed asits Information Content (Cover and Thomas, 1991),ie.
?log(P ).
When the language model (LM) isapplied the scores associated with each n-gram areused instead of counts in equations 2 and 3.4 Experiments4.1 METER CorpusThe METER corpus (Gaizauskas et al, 2001) con-tains 771 Press Association (PA) articles, some ofwhich were used as source(s) for 945 news storiespublished by nine British newspapers.These 945 documents are classified as Wholly De-rived (WD), Partially Derived (PD) and Non De-rived (ND).
WD means that the newspaper articleis likely derived entirely from the PA source text;PD reflects the situation where some of the newspa-per article is derived from the PA source text; newsstories likely to be written independently of the PAsource fall into the category of ND.
In our experi-ments, the 768 stories from court and law reportingwere used (WD=285, PD=300, ND=183) to allowcomparison with Clough et al (2002).
To provide acollection to investigate binary classification we ag-gregated the WD and PD cases to form a Derived set.Each document was pre-processed by converting tolower case and removing all punctuation marks.4.2 Determining ReuseThe text reuse task aims to distinguish between lev-els of text reuse, i.e.
WD, PD and ND.
Two versionsof a classification task were used: binary classifica-tion distinguishes between Derived (i.e.
WD ?
PD)and ND documents, and ternary classification distin-guishes all three levels of reuse.A Naive Bayes classifier (Weka version 3.6.1) and10-fold cross validation were used for the experi-ments.
Containment similarity scores between allPA source texts and news articles on the same storywere computed for word uni-grams, bi-grams, tri-grams, four-grams and five-grams.
These five simi-larity scores were used as features.
Performance wasmeasured using precision, recall and F1 measureswith the macro-average reported across all classes.The language model (Section 3.4) was trained us-ing 806,791 news articles from the Reuters Corpus(Rose et al, 2002).
A high proportion of the newsstories selected were related to the topics of enter-tainment and legal reports to reflect the subjects ofthe new articles in the METER corpus.5 Results and AnalysisTables 1 and 2 show the results of the binaryand ternary classification experiments respectively.?NG?
refers to the comparison of n-grams in eachdocument (Section 3.1), while ?Del?, ?WN?
and?Para?
refer to the modified n-grams created us-ing deletions, WordNet and paraphrases respectively(Section 3.2).
The prefix ?LM?
(e.g.
?LM-NG?)
in-dicates that the n-grams are weighted using the lan-guage model probability scores (Section 3.4).For the binary classification task (Table 1) it canbe observed that including modified n-grams im-proves performance.
This improvement is observedwhen each of the three types of modified n-gramsis applied individually, with a greater increase beingobserved for the n-grams created using the WordNetand paraphrase approaches.
Further improvement isobserved when different types of modified n-gramsare combined with the best performance obtainedwhen all three types are used.
All improvementsover the baseline approach (NG) are statisticallysignificant (Wilcoxon signed-rank test, p < 0.05).These results demonstrate that the various types ofmodified n-grams all contribute to identifying whentext is being reused since they capture different typesof rewrite operations.In addition, performance consistently improveswhen n-grams are weighted using language modelscores.
The improvement is significant for all typesof n-grams.
This demonstrates that the informationprovided by the language model is useful in deter-mining the relative importance of n-grams.Several of the results are higher than those re-ported by Clough et al (2002) (F1=0.763), despitethe fact their approach supplements n-gram overlapwith additional techniques such as sentence align-ment and string search algorithms.Results of the ternary classification task areshown in Table 2.
Results show a similar patternto those observed for the binary classification task56Approach P R F1NG 0.836 0.706 0.732LM-NG 0.846 0.722 0.746Del 0.851 0.745 0.767LM-Del 0.858 0.765 0.785WN 0.876 0.801 0.817LM-WN 0.879 0.810 0.825Para 0.884 0.821 0.834LM-Para 0.888 0.831 0.843Del+WN 0.889 0.835 0.847LM-Del+WN 0.884 0.848 0.855Del+Para 0.892 0.841 0.853LM-Del+Para 0.896 0.849 0.860WN+Para 0.894 0.848 0.858LM-WN+Para 0.896 0.865 0.871Del+WN+Para 0.897 0.856 0.865LM-Del+WN+Para 0.903 0.876 0.882(Clough et al, 2002) ?
?
0.763Table 1: Results for binary classificationand the best result is also obtained when all threetypes of modified n-grams are included and n-gramsare weighted with probability scores.
Once againweighting n-grams with language model scores im-proves results for all types of n-gram and this im-provement is significant.
Results for several types ofn-gram are also better than those reported by Cloughet al (2002) (F1=0.664).Results for all approaches are lower for theternary classification.
This is because the binaryclassification task involves distinguishing betweentwo classes of documents which are relatively dis-tinct (derived and non-derived) while the ternarytask divides the derived class into two (WD and PD)which are more difficult to separate (see Table 3showing confusion matrix for the approach whichgave best results for ternary classification).6 ConclusionThis paper describes an approach to the analysis oftext reuse which is based on comparison of n-grams.This approach is augmented by modifying the n-grams in various ways and weighting them withprobabilities derived from a language model.
Evalu-ation is carried out on a standard data set containingexamples of reused journalistic texts.
Making use ofApproach P R F1NG 0.596 0.557 0.551LM-NG 0.615 0.579 0.574Del 0.612 0.584 0.579LM-Del 0.633 0.611 0.606WN 0.644 0.636 0.631LM-WN 0.649 0.640 0.635Para 0.662 0.653 0.647LM-Para 0.669 0.659 0.654Del+WN 0.655 0.649 0.643LM-Del+WN 0.668 0.656 0.650Del+Para 0.665 0.658 0.652LM-Del+Para 0.661 0.662 0.655WN+Para 0.668 0.661 0.655LM-WN+Para 0.680 0.675 0.668Del+WN+Para 0.669 0.666 0.660LM-Del+WN+Para 0.688 0.689 0.683(Clough et al, 2002) ?
?
0.664Table 2: Results for ternary classificationClassified as WD PD NDWD 139 94 14PD 57 206 54ND 1 13 191Table 3: Confusion matrix when ?LM-Del+WN+Para?approach used for ternary classificationmodified n-grams with appropriate weights is foundto improve performance when detecting text reuseand the approach described here outperforms an ex-isting approach.
In future we plan to experimentwith other methods for modifying n-grams and alsoto apply this approach to other types of text reuse.AcknowledgmentsThis work was funded by the COMSATS Instituteof Information Technology, Islamabad, Pakistan un-der the Faculty Development Program (FDP) and aGoogle Research Award.ReferencesAlberto B. Ceden?o, Paolo Rosso, and Jose M. Bened2009.
Reducing the Plagiarism Detection SearchSpace on the basis of the Kullback-Leibler DistanceProceedings of CICLing-09, 523?534.57Allan Bell 1991.
The Language of News Media.
Black-well.Andreas Stolcke.
2002.
SRILM - An Extensible Lan-guage Modeling Toolkit.
In Proceedings of the Inter-national Conference on Spoken Language Processing,901?904.Chin-Yew Lin.
2004.
Rouge: A Package for AutomaticEvaluation of Summaries.
In Proceedings of the ACL-04 Workshop, 74?81.Chris Callison-Burch.
2008.
Syntactic Constraints onParaphrases Extracted from Parallel Corpora.
In Pro-ceedings of EMNLP?08, 196?205.Jangwon Seo and W. Bruce Croft.
2008.
Local TextReuse Detection.
In Proceedings of SIGIR?08, 571?578.
In Proceedings of the 31st Annual InternationalACM SIGIR Conference on Research and Develop-ment in Information Retrieval, 571?578.Kishore Papineni, Salim Roukos, Todd Ward, and WeiJ.
Zhu.
2002.
Bleu: A Method for Automatic Eval-uation of Machine Translation.
In Proceedings ofACL?02, 311?318.Martin Potthast, Andreas Eiselt, Alberto Barro?n-Ceden?o,Benno Stein and Paolo Rosso.
2011.
Overview of the3rd International Competition on Plagiarism Detec-tion.
Notebook Papers of CLEF 11 Labs and Work-shops.Narayanan Shivakumar and Hector G. Molina.
1995.SCAM: A Copy Detection Mechanism for Digital Doc-uments.
Proceedings of the 2nd Annual Conferenceon the Theory and Practice of Digital Libraries, Texas,USA.Paul Clough, Robert Gaizauskas, Scott S.L.
Piao, andYorick Wilks.
2002.
Measuring Text Reuse.
In Pro-ceedings of ACL?02, Philadelphia, USA, 152?159.Peter C. R. Lane, Caroline M. Lyon, and James A. Mal-colm.
2006.
Demonstration of the Ferret plagiarismdetector.
Proceedings of the 2nd International Plagia-rism Conference, Newcastle, UK.Robert Gaizauskas, Jonathan Foster, Yorick Wilks, JohnArundel, Paul Clough, and Scott S.L.
Piao.
2001.
TheMETER Corpus: A Corpus for Analysing JournalisticText Reuse.
In Proceedings of the Corpus LinguisticsConference, 214?223.Sergey Brin, James Davis and Hector G. Molina.
1995.Copy Detection Mechanisms for Digital Documents.Proceedings ACM SIGMOD?95, 398?409.Stanford Chiu, Ibrahim Uysal, Bruce W. Croft.
2010.Evaluating text reuse discovery on the web.
In Pro-ceedings of the third symposium on Information inter-action in context, 299?304.Thomas M. Cover, Joy A. Thomas.
1991.
Elements ofInformation Theory.
Wiley, New York, USA.Timothy C. Hoad and Justin Zobel.
2003.
Methodsfor Identifying Versioned and Plagiarized Documents.Journal of the American Society for Information Sci-ence and Technology, 54(3):203?215.Tony Rose, Mark Stevenson, Miles Whitehead.
2002.The Reuters Corpus Volume 1 - from Yesterday?s newsto tomorr ow?s language resources.
In Proceedings ofthe Third International Conference on Language Re-sources and Evaluation (LREC-02), 827?832.58
