THE MULTIL INGUAL ENTITY TASK (MET) OVERVIEWRoberta MerchantMary Ellen OkurowskiDepartment of Defense9800 Savage RoadFt.
Meade, MD 20755-6000Nancy ChinchorScience Applications International Corporation10260 Campus Pt.
Dr. M/S A2-FSan Diego, CA 92121In November, 1996, the Message UnderstandingConference-6 (MUC-6) evaluation of named entityidentification demonstrated that systems are approach-ing human performance onEnglish language t xts \[10\].Informal and anonymous, the MET provided a newopportunity to assess progress on the same task in Span-ish, Japanese, and Chinese.
Preliminary results indicatethat MET systems in all three languages performedcomparably to those of the MUC-6 evaluatien inEnglish.Based upon the Named Entity Task Guidelines \[ 11\],the task was to locate and tag with SGML named entityexpressions (people, organizations, and locations), timeexpressions (time and date), and numeric expressions(percentage and money) in Spanish texts from AgenceFrance Presse, in Japanese texts from Kyodo newswire,or in Chinese texts from Xinhua newswkel.
Across lan-guages the keywords "press conference" retrieved a richsubcorpus of texts, covering awide spectrum of topics.Frequency and types of expressions vary in the threelanguage sets \[2\] \[8\] \[9\].
The original task guidelineswere modified so that he core guidelines were languageindependent with language specific rules appended.The schedule was quite abbreviated.
In the fall,Government language teams retrieved training and testtexts with multilingual software for the Fast Data Finder(FDF), refined the MUC-6 guidelines, and manuallytagged 100 training texts using the SRA Named EntityTool.
In January, the training texts were released alongwith 200 sample unannotated training texts to the partic-ipating sites.
A dry run was held in late March and earlyApril and in late April the official test on 100 texts was.
The language t xts were supplied by theLinguistic Data Consortium (LDC) at theUniversity of Pennsylvania.performed anonymously.
SAIC created language ver-sions of the scoring program and provided technicalsupport throughout.Both commercial and academic groups partici-pated.
Two groups, New Mexico State University/Com-puting Research Lab (NMSU/CRL) and Mitre Corp.elected to participate in all languages, SRA in Spanishand Japanese, BBN in Spanish (with FinCen) and Chi-nese, and SRI, NEC/Uuiversity of Sheffield, and NITData in Japanese.
Prior experience with the languagesvaried across groups, from new starts in January to thosewith censiderable d velopment history in multilingualtext processing.The MET results have been quite instructive from anumber of different angles.
First of all, multilingualnamed entity extraction is a technology that is clearlyready for application as the score ranges indicate inTable 1.
Second, the informal anonymous natureLanguage High Range Low RangeSpanish 93.04 83.40Japanese 92.12 70.79Chinese 84.51 72.21Table 1: MET Resultsappeared to encourage experimentation which is evi-denced in the technical discussion of the snmmary sitepapers \[1\]\[6\]\[12\].
Third, system architectures haveevolved toward increasing language portability \[ 1\]\[3\]\[4\]\[5\]\[7\], and, fourth, new acquisition techniques are accel-erating development \[1\]\[4\]\[5\].
Fifth, resource sharingcontinues to play an important role in fostering technol-445ogy development.
For example, two of the three sites inChinese shared a word segmentor developed by NMSU/CRL\[1\]\[4\].An additional contribution of MEr was the basehn-ing of human performance (Table 2).
Dry run test datacreated by the language teams were analyzed to obtainconsistency and accuracy scores as well as timing on thetask.
Analysts averaged eight minutes per article forannotation, including review and correction.
Analysisrevealed that inter-analyst variation on the task is quitelow and that analysts performed this task accurately.This contrasts ignificantly with human performancedata on a more complex information extraction task inMUC-5 \[13\].
When human baseline data are juxtaposedwith the system scores, it is clear that the systems areapproaching human accuracy with a much higher speed,offering further support for readiness for application.Language ConsistencySpanish 92.92Japanese 95Chinese 94.32AccuracyHigh LowRange Range91.42 88.6298 9798 95.94Table 2: Inter-analyst ResultsThe scores in Tables 1 and 2 are the F-Measuresobtained by the scoring software.
The F-Measure is usedto compute a single score in which recall and precisionhave equal weight in computation.
Recall, a measure ofcompleteness, i  the number that the system got correctout of all of those that it could possibly have gotten cor-rect; and precision, ameasure of accuracy, is the numberof those that it got correct out of the number that it pro-vided answers for.The F-Measures in Table 1 were produced by theautomated scoring program.
The program compares thehuman-generated answer key and the system-generatedresponses to produce ascore report for each system.
Thelow and high F-Measure scores from the formal test heldin late April represent the current performance of thesystems in this experimental evaluation.The scoring software performs two processes: map-ping and scoring.
After parsing the incoming answerkey and system response, it determines what piece ofinformation in the response should be scored against eachpiece of information in the key.
This process of alignmentis called mapping and relies on the text being overlappingat least in part and, in cases where more than one mappingpossibility exists, the software optimizes over the F-Mea-sure for that piece of information.
The scoring results arethen tallied and reported.The F-Measures in Table 2 for the human perfor-mance baseline were also preduced by the automated scor-ing program.
The consistency scores are the F-Measuresresulting from comparing the two analysts' answer keys.The accuracy results are the F-Measures obtained by com-paring each analyst's answer key against he final answerkey.
The measures are reported anonymously asa high anda low score.In terms of the evaluation methodology, a number oflessons were learned from this experimental evaluation.The first was that the scoring software development effortwould be improved by requesting realistic data from par-ticipants as early as possible for software testing instead ofwaiting until the dry run.
An analysis of the order in whichthe data was provided, the timing of the distribution of thedata, and the reliability of that data suggest that the resultsreported here are really the "floor" of what the technologyis currently capable of rather than the "ceiling.
"Given thatthe systems are performing so close to human pelrfor-mance, it will be necessary to perform significance testingin the future.
This testing will include human-generatedresponses in the test.The Multilingual Entity Task section of this volume isa collection of papers that review the evaluation task andthe participating systems.
This overview paper is followedby three papers, discussing the task by language.
Papersfrom each of the sites then briefly provide technicaldescriptions of their systems and participation in MET.References\[1\] Aberdeen, John, John Burger, David Day, LynetteHirschman, David palmer, Patricia Robinson, and MarcV'dain.
"MITRE: Description of the ALEMBIC System asUsed In MET" in this volume\[2\] Anderson, Don A.
"The Multilingual Entity TaskA Descriptive Analysis of Enamex in Spanish" in this vol-ume.\[3\] Aone, Chinatsu.
NameTag TM Japanese and Span-ish Systems as Used for MET" in this volume.\[4\] Ayuso, Damaris, Daniel Bikel, Tasha Hall,Peterson, Ralph Weischedel, Patrick Jost.
"Approaches inMET (Multilingual Entity Task) in this volume.446[5] Cowie, Jim.
"CRL's Approach to MET" in thisvolume.
[6] Edguchi, Yoshio and Tsuyoshi KitanL "NITData: Description of the Erie System Used in MUC-6"in this volume.
[7] Kameyama, Megumi.
"MET Name Recognitionwith Japanese FASTUS" in this volume.
[8] Keenan, Thomas.
"An Interpretive Data Analy-sis of Chinese Named Entity Subtypes" inthis volume.
[9]Maiorano, Steven and Terry Wilson, "Mulfilin-gual Entity Task: Japanese Results" in this volume.
[10] Sundheim, Beth.
Proceedings of the MessageUnderstanding Conference-6 (MUC-6), Morgan Kauf-mann Publishers, Inc.: San Francisco, 1996.
[11] Sundheim, Beth.
"Guidelines for NamedEntity Task:' Version 2.3, 1995.
[12] Takomoto, Yoshikazu, Takahiro Wakao,I-Iiroshi Yamada, Robert Gaizauskas, Yorick Wilks,"NEC Corporation and University of Sheftield: Descrip-tion of NEC/Sheflield System Used for MET Japanese"in this volume.
[13] Will, Craig.
"Comparing Human and MachinePerformance for Natural Language Information Extrac-tion: Results from the TIPSTER Text Evaluation - IDA"in Proceedings TIPSTER Text Program ~Phase D, Mor-gan Kaufmann Publishers, Inc.: San Francisco, pp.
179 -193, 1993.447
