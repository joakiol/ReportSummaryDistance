Proceedings of the Workshop on Evaluation Metrics and System Comparison for Automatic Summarization, pages 19?27,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsMachine Translation for Multilingual Summary Content EvaluationJosef Steinberger and Marco TurchiJoint Research Centre,European Commission,Via E. Fermi 2749,21027 Ispra (VA), Italy[name].
[surname]@jrc.ec.europa.euAbstractThe multilingual summarization pilot task atTAC?11 opened a lot of problems we are fac-ing when we try to evaluate summary qual-ity in different languages.
The additional lan-guage dimension greatly increases annotationcosts.
For the TAC pilot task English arti-cles were first translated to other 6 languages,model summaries were written and submit-ted system summaries were evaluated.
Westart with the discussion whether ROUGE canproduce system rankings similar to those re-ceived from manual summary scoring by mea-suring their correlation.
We study then threeways of projecting summaries to a differentlanguage: projection through sentence align-ment in the case of parallel corpora, sim-ple summary translation and summarizing ma-chine translated articles.
Building such sum-maries gives opportunity to run additional ex-periments and reinforce the evaluation.
Later,we investigate whether an evaluation based onmachine translated models can perform closeto an evaluation based on original models.1 IntroductionEvaluation of automatically produced summaries indifferent languages is a challenging problem for thesummarization community, because human effortsare multiplied to create model summaries for eachlanguage.
Unavailability of parallel corpora suitablefor news summarization adds even another annota-tion load because documents need to be translated toother languages.
At the last TAC?11 campaign, sixresearch groups spent a lot of work on creating eval-uation resources in seven languages (Giannakopou-los et al, 2012).
Thus compared to the monolingualevaluation, which requires writing model summariesand evaluating outputs of each system by hand, inthe multilingual setting we need to obtain transla-tions of all documents into the target language, writemodel summaries and evaluate the peer summariesfor all the languages.In the last fifteen years, research on MachineTranslation (MT) has made great strides allowinghuman beings to understand documents written invarious languages.
Nowadays, on-line services suchas Google Translate and Bing Translator1 can trans-late text into more than 50 languages showing thatMT is not a pipe-dream.In this paper we investigate how machine trans-lation can be plugged in to evaluate quality of sum-marization systems, which would reduce annotationefforts.
We also discuss projecting summaries to dif-ferent languages with the aim to reinforce the evalu-ation procedure (e.g.
obtaining additional peers forcomparison in different language or studying theirlanguage-independence).This paper is structured as follows: after dis-cussing the related work in section 2, we give ashort overview of the TAC?11 multilingual pilot task(section 3).
We compare average model and systemmanual scores and we also study ROUGE correla-tion to the manual scores.
We run our experimentson a subset of languages of the TAC multilingualtask corpus (English, French and Czech).
Section4 introduces our translation system.
We mention its1http://translate.google.com/ and http://www.microsofttranslator.com/19translation quality for language pairs used later inthis study.
Then we move on to the problem of pro-jecting summaries to different languages in section5.
We discuss three approaches: projecting sum-mary through sentence alignment in a parallel cor-pus, translating a summary, and summarizing trans-lated source texts.
Then, we try to answer the ques-tion whether using translated models produces sim-ilar system rankings as when using original models(section 6), accompanied by a discussion of discrim-inative power difference and cross-language modelcomparison.2 Related workAttempts of using machine translation in differentnatural language processing tasks have not beenpopular due to poor quality of translated texts, butrecent advance in Machine Translation has mo-tivated such attempts.
In Information Retrieval,Savoy and Dolamic (2009) proposed a comparisonbetween Web searches using monolingual and trans-lated queries.
On average, the results show a limiteddrop in performance, around 15% when translatedqueries are used.In cross-language document summarization, Wanet al (2010) and Boudin et al (2010) combined theMT quality score with the informativeness score ofeach sentence to automatically produce summary ina target language.
In Wan et al (2010), each sen-tence of the source document is ranked according toboth scores, the summary is extracted and then theselected sentences translated to the target language.Differently, in Boudin et al (2010), sentences arefirst translated, then ranked and selected.
Both ap-proaches enhance the readability of the generatedsummaries without degrading their content.Automatic evaluation of summaries has beenwidely investigated in the past.
In the task ofcross-lingual summarization evaluation Saggion etal.
(2002) proposed different metrics to assess thecontent quality of a summary.
Evaluation of sum-maries without the use of models has been intro-duced by Saggion et al (2010).
They showed thatsubstituting models by full document in the com-putation of the Jensen-Shannon divergence measurecan produce reliable rankings.
Yeloglu et al (2011)concluded that the pyramid method partially re-flects the manual inspection of the summaries andROUGE can only be used when there is a manuallycreated summary.
A method, and related resources,which allows saving precious annotation time andthat makes the evaluation results across languagesdirectly comparable was introduced by Turchi etal.
(2010).
This approach relies on parallel data andit is based on the manual selection of the most im-portant sentences in a cluster of documents from asentence-aligned parallel corpus, and by projectingthe sentence selection to various target languages.Our work addresses the same problem of reducingannotation time and generating models, but from adifferent prospective.
Instead of using parallel dataand annotation projection or full documents, we in-vestigate the use of machine translation at differentlevel of summary evaluation.
While the aproach ofTurchi et al (2010) is focussed on sentence selectionevaluation our strategy can also evaluate generativesummaries, because it works on summary level.3 TAC?11 Multilingual PilotThe Multilingual task of TAC?11 (Giannakopouloset al, 2012) aimed to evaluate the application of(partially or fully) language-independent summa-rization algorithms on a variety of languages.
Thetask was to generate a representative summary (250words) of a set of 10 related news articles.The task included 7 languages (English, Czech,French, Hebrew, Hindi, Greek and Arabic).
Anno-tation of each language sub-corpus was performedby a different group.
English articles were manu-ally translated to the target languages, 3 model sum-maries were written for each topic.8 groups (systems) participated in the task, how-ever, not all systems produced summaries for all lan-guages.
In addition there were 2 baselines: Cen-troid Baseline ?
the start of the centroid article andGA Topline ?
summary based on genetic algorithmusing model summary information, which shouldserve as an upper bound.Human annotators scored each summary, bothmodels and peers, on the 5-to-1 scale (5 = the best, 1= the worst) ?
human grades.
The score correspondsto the overall responsiveness of the main TAC task ?equal weight of content and readability.
22In this article we focus on raw human grades.
The task20English French Czech average English French Czech averageManual grades Manual gradesaverage model 4.06 4.03 4.73 4.27 4.06 4.03 4.73 4.27average peer 2.73 2.18 2.56 2.50 2.73 2.18 2.56 2.50ROUGE-2 ROUGE-SU4average model .194 .222 .206 .207 .235 .255 .237 .242average peer .139 .167 .182 .163 .183 .207 .211 .200correlation to manual grading ?
peers and models not stemmedpeers only .574 .427 .444 .482 .487 .362 .519 .456(p-value) (< .1)models & peers .735 .702 .484 .640 .729 .703 .549 .660(p-value) (< .01) (< .02) (< .02) (< .02)correlation to manual grading ?
peers and models stemmedPeers only .573 .445 .500 .506 .484 .336 .563 .461(p-value) (< .1)models & peers .744 .711 .520 .658 .723 .700 .636 .686(p-value) (< .01) (< .01) (< .02) (< .02) (< .1)Table 1: Average ROUGE-2 and ROUGE-SU4 scores for models and peers, and their correlation to the manualevaluation (grades).
We report levels of significance (p) for two-tailed test.
Cells with missing p-values denote non-significant correlations (p > .1).3.1 Manual EvaluationWhen we look at the manually assigned grades wesee that there is a clear gap between human and au-tomatic summaries (see the first two rows in table1).
While the average grade for models were alwaysover 4, peers were graded lower by 33% for Englishand by 54% for French and Czech.
However, therewere 5 systems for English and 1 system for Frenchwhich were not significantly worse than at least onemodel.3.2 ROUGEThe first question is: can an automatic metric rankthe systems similarly as manual evaluation?
Thiswould be very useful when we test different config-urations of our systems, in which case manual scor-ing is almost impossible.
Another question is: canthe metric distinguish well the gap between mod-els and peers?
ROUGE is widely used because ofits simplicity and its high correlation with manuallyassigned content quality scores on overall systemrankings, although per-case correlation is lower.We investigated how the two most commonROUGE scores (ROUGE-2 and ROUGE-SU4) cor-overview paper (Giannakopoulos et al, 2012) discusses, in ad-dition, scaling down the grades of shorter summaries to avoidassigning better grades to shorter summaries.relate with human grades.
Although using n-gramswith n greater than 1 gives limited possibility toreflect readability in the scores when compared toreference summaries, ROUGE is considered mainlyas a content evaluation metric.
Thus we cannotexpect a perfect correlation because half of thegrade assigned by humans reflects readability issues.ROUGE could not also evaluate properly the base-lines.
The centroid baseline contains a continuoustext (the start of an article) and it thus gets highergrades by humans because of its good readability,but from the ROUGE point of view the baseline isweak.
On the other hand, the topline used informa-tion from models and it is naturally more similar tothem when evaluated by ROUGE.
Its low readabil-ity ranked it lower in the case of human evaluation.Because of these problems we include in the correla-tion figures only the submitted systems, neither thebaseline nor the topline.Table 1 compares average model and peerROUGE scores for the three analyzed languages.
Itadds two correlations3 to human grades: for mod-els+systems and for systems only.
The first caseshould answer the question whether the automaticmetric can distinguish between human and auto-matic summaries.
The second settings could show3We used the classical Pearson correlation.21whether the automatic metric accurately evaluatesthe quality of automatic summaries.
To ensure a faircomparison of models and non-models, each modelsummary is evaluated against two other models, andeach non-model summary is evaluated three times,each time against a different couple of models, andthese three scores are averaged out (the jackknif-ing procedure).4 The difference of the model andsystem ROUGE scores is significant, although it isnot that distinctive as in the case of human grades.The distinction results in higher correlations whenwe include models than in the more difficult systemsonly case.
This is shown by both correlation figuresand their confidence.
The only significant correla-tion for the systems only case was for English andROUGE-2.
Other correlations did not cross the 90%confidence level.
If we run ROUGE for morpholog-ically rich languages (e.g.
Czech), stemming playsmore important role than in the case of English.
Inthe case of French, which stands in between, wefound positive effect of stemming only for ROUGE-2.
ROUGE-2 vs. ROUGE-SU4: for English andFrench we see better correlation with ROUGE-2 butthe free word ordering in Czech makes ROUGE-SU4 correlate better.4 In-house TranslatorOur translation service (Turchi et al, 2012) isbased on the most popular class of Statistical Ma-chine Translation systems (SMT): the Phrase-Basedmodel (Koehn et al, 2003).
It is an extension ofthe noisy channel model introduced by Brown etal.
(1993), and uses phrases rather than words.
Asource sentence f is segmented into a sequence ofI phrases f I = {f1, f2, .
.
.
fI} and the same isdone for the target sentence e, where the notion ofphrase is not related to any grammatical assumption;a phrase is an n-gram.
The best translation ebest off is obtained by:ebest = arg maxe p(e|f) = arg maxe p(f |e)pLM (e)4In our experiments we used the same ROUGE settings as atTAC.
The summaries were truncated to 250 words.
For Englishwe used the Porter stemmer included in the ROUGE package,for Czech the aggressive version from http://members.unine.ch/jacques.savoy/clef/index.html andfor French http://jcs.mobile-utopia.com/jcs/19941\_FrenchStemmer.java.= arg maxeI?i=1?(fi|ei)?
?d(ai ?
bi?1)?d|e|?i=1pLM(ei|e1 .
.
.
ei?1)?LMwhere ?
(fi|ei) is the probability of translating aphrase ei into a phrase fi.
d(ai ?
bi?1) is thedistance-based reordering model that drives the sys-tem to penalize significant word reordering duringtranslation, while allowing some flexibility.
In thereordering model, ai denotes the start position ofthe source phrase that is translated into the ith tar-get phrase, and bi?1 denotes the end position ofthe source phrase translated into the (i ?
1)th targetphrase.
pLM (ei|e1 .
.
.
ei?1) is the language modelprobability that is based on the Markov?s chain as-sumption.
It assigns a higher probability to flu-ent/grammatical sentences.
?
?, ?LM and ?d areused to give a different weight to each element.
Formore details see (Koehn et al, 2003).
In this workwe use the open-source toolkit Moses (Koehn et al,2007).Furthermore, our system takes advantage of alarge in-house database of multi-lingual named andgeographical entities.
Each entity is identified in thesource language and its translation is suggested tothe SMT system.
This solution avoids the wrongtranslation of those words which are part of a namedentity and also common words in the source lan-guage, (e.g.
?Bruno Le Maire?
which can bewrongly translated to ?Bruno Mayor?
), and enlargesthe source language coverage.We built four models covering the following lan-guage pairs: En-Fr, En-Cz, Fr-En and Cz-En.
Totrain them we use the freely available corpora: Eu-roparl (Koehn, 2005), JRC-Acquis (Steinberger etal., 2006), CzEng0.9 (Bojar and ?Zabokrtsky?, 2009),Opus (Tiedemann, 2009), DGT-TM5 and News Cor-pus (Callison-Burch et al, 2010), which resultsin more than 4 million sentence pairs for eachmodel.
Our system was tested on the News test set(Callison-Burch et al, 2010) released by the orga-nizers of the 2010 Workshop on Statistical MachineTranslation.
Performance was evaluated using theBleu score (Papineni et al, 2002): En-Fr 0.23, En-Cz 0.14, Fr-En 0.26 and Cz-En 0.22.
The Czech5http://langtech.jrc.it/DGT-TM.html22language is clearly more challenging than French forthe SMT system, this is due to the rich morphologyand the partial free word order.
These aspects aremore evident when we translate to Czech, for whichwe have poor results.5 Mapping Peers to Other LanguagesWhen we want to generate a summary of a set of ar-ticles in a different language we have different pos-sibilities.
The first case is when we have articles inthe target language and we run our summarizer onthem.
This was done in the Multilingual TAC task.If we have parallel corpora we can take advantage ofprojecting a sentence-extractive summary from onelanguage to another (see Section 5.1).If we do not have the target language articles wecan apply machine translation to get them and runthe summarizer on them (see Section 5.3).
If wemiss a crucial resource for running the summarizerfor the target language we can simply translate thesummaries (see Section 5.2).In the case of the TAC Multilingual scenario theseprojections can also give us summaries for all lan-guages from the systems which were applied onlyon some languages.5.1 Aligned SummariesHaving a sentence-aligned (parallel) corpus givesaccess to additional experiments.
Because the cur-rent trend is still on the side of pure sentence extrac-tion we can investigate whether the systems selectthe same sentences across the languages.
While cre-ating the TAC corpus each research group translatedthe English articles into their language, thus the re-sulting corpus was close to be parallel.
However,sentences are not always aligned one-to-one becausea translator may decide, for stylistic or other reasons,to split a sentence into two or to combine two sen-tences into one.
Translations and original texts arenever perfect, so that it is also possible that the trans-lator accidentally omits or adds some information,or even a whole sentence.
For these reasons, align-ers such as Vanilla6, which implements the Gale andChurch algorithm (Gale and Church, 1994), typi-cally also allow two-to-one, one-to-two, zero-to-oneand one-to-zero sentence alignments.
Alignments6http://nl.ijs.si/telri/Vanilla/other than one-to-one thus present a challenge forthe method of aligning two text, in particular one-to-two and two-to-one alignments.
We used Vanillato align Czech and English article sentences, but be-cause of high error rate we corrected the alignmentby hand.The English summaries were then aligned toCzech (and the opposite direction as well) accord-ing to the following approach.
Sentences in a sourcelanguage system summary were split.
For each sen-tence we found the most similar sentence in thesource language articles based on 3-gram overlap.The alignment information was used to select sen-tences for the target language summary.
Some sim-plification rules were applied: if the most similarsentence found in the source articles was alignedwith more sentences in the target language articles,all the projected sentences were selected (one-to-twoalignment); if the sentence to be projected coveredonly a part of sentences aligned with one target lan-guage sentence, the target language sentence was se-lected (two-to-one alignment).The 4th row in table 2 shows average peerROUGE scores of aligned summaries.7 When com-paring the scores to the peers in original language(3rd row) we notice that the average peer score isslightly better in the case of English (cz?en projec-tion) and significantly worse for Czech (en?cz pro-jection) indicating that Czech summaries were moresimilar to English models than English summariesto Czech models.Having the alignment we can study the overlapof the same sentences selected by a summarizer indifferent languages.
The peer average for the en-cz language pair was 31%, meaning that only a bitless than one third of sentences was selected both toEnglish and Czech summaries by the same system.The percentage differed a lot from a summarizer toanother one, from 13% to 57%.
This number can beseen as an indicator of summarizer?s language inde-pendence.However, the system rankings of aligned sum-maries did not correlate well with human grades.There are many inaccuracies in the alignment sum-mary creation process.
At first, finding the sentence7Models are usually not sentence-extractive and thus align-ing them would not make much sense.23ROUGE-2 ROUGE-SU4fr?en cz?en en?fr en?cz avg.
fr?en cz?en en?fr en?cz avg.average ROUGE scoresorig.
model .194 .194 .222 .206 .207 .235 .235 .255 .237 .242transl.
model .128 .162 .187 .123 .150 .184 .217 .190 .160 .188orig.
peer .139 .139 .167 .182 .163 .183 .183 .207 .211 .200aligned peer .148 .146 .147 .175 .140 .180transl.
peer .100 .119 .128 .102 .112 .155 .174 .179 .140 .162correlation to source language manual grading for translated summariespeers only .411 .483 .746 .456 .524 .233 .577 .754 .571 .534(p-value) (< .05) (< .05)models & peers .622 .717 .835 .586 .690 .581 .777 .839 .620 .704(p-value) (< .05) (< .05) (< .01) (< .1) (< .05) (< .02) (< .01) (< .05)correlation to target language manual grading for translated summariespeers only .685 .708 .555 .163 .528 .516 .754 .529 .267 .517(p-value) (< .1)Table 2: ROUGE results of translated summaries, evaluated against target language models (e.g., cz?en againstEnglish models).in the source data that was probably extracted isstrongly dependent on the sentence splitting eachsummarizer used.
At second, alignment relationsdifferent from one-to-one results in selecting con-tent with different length compared to the originalsummary.
And since ROUGE measures recall, andtruncates the summaries, it introduces another inac-curacy.
There were also relations one-to-zero (sen-tences not translated to the target language).
In thatcase no content was added to the target summary.5.2 Translated SummariesThe simplest way to obtain a summary in a differentlanguage is to apply machine translation software onsummaries.
Here we investigate (table 2) whethermachine translation errors affect the system orderby correlation to human grades again.
In this casewe have two reference human grade sets: one forthe source language (from which we translate) andone for the target language (to which we translate).Since there were different models for each languagewe can include models only in computing the corre-lation against source language manual grading.At first, we can see that ROUGE scores are af-fected by the translation errors.
Average modelROUGE-2 score went down by 28% and averagepeer ROUGE-2 by 31%.
ROUGE-SU4 seems to bemore robust to deal with the translation errors: mod-els went down by 21%, peers by 19%.
The gap be-tween models and peers is still distinguishable, sys-tem ranking correlation to human grades holds sim-ilar levels although less statistically significant cor-relations can be seen.
Clearly, quality of the trans-lator affects these results because our worst transla-tor (en?cz) produced the worst summaries.
Cor-relation to the source language manual grades in-dicates how the ranking of the summarizers is af-fected (changed) by translation errors.
For exam-ple it compares ranking for English based on man-ual grades with ranking computed on the same sum-maries translated from English to French.
The sec-ond scenario (correlation to target language scores)shows how similar is the ranking of summarizersbased on translated summaries with the target lan-guage ranking based on original summaries.
If weomit translation inaccuracies, low correlation in thelatter case indicates qualitatively different output ofparticipating peers (e.g.
en and cz summaries).5.3 Summarizing Translated ArticlesTo complete the figure we tested the configurationin which we first translate the full articles to thetarget language and then apply a summarizer.
Aswe have at disposal an implementation of system3 from the TAC multilingual task we used it on 4translated document sets (en?cz, cz?en, fr?en,en?fr).
This system was the best according to hu-man grades in all three discussed languages.24method ROUGE-2 ROUGE-SU4en .177 .209cz ?
en alignment .200 .235cz ?
en translation .142 .194en from (cz ?
en source translation) .132 .181fr ?
en translation .120 .172en from (fr ?
en source translation) .129 .185fr .214 .241en ?
fr translation .167 .212fr from (en ?
fr source translation) .156 .202cz .204 .225en ?
cz alignment .176 .196en ?
cz translation .115 .150cz from (en ?
cz source translation) .138 .178Table 3: ROUGE results of different variants of summaries produced by system 3.
The first line shows the ROUGEscores of the original English summaries submitted by system 3.
The second line gives average scores of the cz?enaligned summaries (see Section 5.1), in the 3rd and 5th lines there are figures of cz?en and fr?en translated sum-maries, and 4th and 6th lines show scores when the summarizer was applied on translated source texts (cz?en andfr?en).
Similarly, lines further down show performance for French and Czech.The system is based on the latent semantic anal-ysis framework originally proposed by Gong andLiu (2002) and later improved by J. Steinbergerand Jez?ek (2004).
It first builds a term-by-sentencematrix from the source articles, then applies Singu-lar Value Decomposition (SVD) and finally uses theresulting matrices to identify and extract the mostsalient sentences.
SVD finds the latent (orthogonal)dimensions, which in simple terms correspond to thedifferent topics discussed in the source (for detailssee (Steinberger et al, 2011)).Table 3 shows all results of summaries generatedby the summarizer.
The first part compares Englishsummaries.
We see that when projecting the sum-mary through alignment from Czech, see Section5.1, a better summary was obtained.
When usingtranslation the summaries are always significantlyworse compared to original (TAC) summaries, withthe lowest performing en?cz translation.
It is in-teresting that in the case of this low-performingtranslator it was significantly better to translate thesource articles and to use the summarizer afterwards.The advantage of this configuration is that the coreof the summarizer (LSA) treats all terms the sameway, thus even English terms that were not trans-lated work well for sentence selection.
On the otherhand, when translating the summary ROUGE willnot match the English terms in Czech models.6 Using Translated ModelsWith growing number of languages the annotationeffort rises (manual creation of model summaries).Now we investigate whether we can produce modelsin one pivot language (e.g., English) and translatethem automatically to all other languages.
The factthat in the TAC corpus we have manual summariesfor each language gives us opportunity to reinforcethe evaluation by translating all model summariesto a common language and thus obtaining a largernumber of models.
This way we can also evaluatesimilarity among models coming from different lan-guages and it lowers the annotators?
subjectivity.6.1 Evaluation Against Translated ModelsTable 4 shows ROUGE figures when peers wereevaluated against translated models.
We discuss alsothe case when English peer summaries (and mod-els as well) are evaluated against both French andCzech models translated to English.
We can seeagain lower ROUGE scores caused by translation er-rors, however, there is more or less the same gapbetween peers and models and the correlation holdssimilar levels as when using the original target lan-guage models.
Exceptions are using English modelstranslated to French and Czech models translated toEnglish in combination with the systems only cor-relation.
If we used both French and Czech mod-25ROUGE-2 ROUGE-SU4peers from en fr cz avg.
en fr cz avg.models tr.
from fr cz fr / cz en en fr cz fr / cz en enaverage model .144 .167 .155 .165 .144 .155 .207 .221 .206 .215 .190 .208average peer .110 .111 .104 .135 .125 .117 .170 .162 .153 .186 .172 .169correlation to target language manual gradingpeers only .639 .238 .424 .267 .541 .422 .525 .136 .339 .100 .624 .345(p-value) < .1models & peers .818 .717 .782 .614 .520 .690 .785 .692 .759 .559 .651 .793(p-value) < .01 < .02 < .01 < .05 < .01 < .02 < .01 < .1 < .1Table 4: ROUGE results of using translated model summaries, which evaluate both peer and model summaries in theparticular language.els translated to English, higher correlation of En-glish peers with translated French models was av-eraged out by lower correlation with Czech models.And because the TAC Multilingual task contained 7languages the experiment can be extended to usingtranslated models from 6 languages.
However, ourresults rather indicate that using the best translator isbetter choice.Given the small scale of the experiment we cannotdraw strong conclusions on discriminative power8when using translated models.
However, our ex-periments indicate that by using translated sum-maries we are partly loosing discriminative power(i.e.
ROUGE finds fewer significant differences be-tween systems).6.2 Comparing Models Across LanguagesBy translating both Czech and French models toEnglish we could compare all models against eachother.
For each topic we had 9 models: 3 originalEnglish models, 3 translated from French and 3 fromCzech.
In this case we reached slightly better cor-relations for the models+systems case: ROUGE-2:.790, ROUGE-SU4: .762.
It was mainly because ofthe fact that this time also models only rankings fromROUGE correlated with human grades (ROUGE-2:.475, ROUGE-SU4: .445).
When we used only En-glish models, the models ranking did not correlate atall (?
-0.1).
Basically, one English model was lesssimilar to the other two, but it did not mean that itwas worse which was shown by adding models from8Discriminative power measures how successful the auto-matic measure is in finding the same significant differences be-tween systems as manual evaluation.other languages.
If we do not have enough referencesummaries this could be a way to lower subjectivityin the evaluation process.7 ConclusionIn this paper we discuss the synergy between ma-chine translation and multilingual summarizationevaluation.
We show how MT can be used to obtainboth peer and model evaluation data.Summarization evaluation mostly aims to achievetwo main goals a) to identify the absolute perfor-mance of each system and b) to rank all the sys-tems according to their performances.
Our resultsshow that the use of translated summaries or mod-els does not alter much the overall system ranking.It maintains a fair correlation with the source lan-guage ranking although without statistical signifi-cance in most of the systems only cases given thelimited data set.
A drop in ROUGE score is evident,and it strongly depends on the translation perfor-mance.
The use of aligned summaries, which lim-its the drop, requires high quality parallel data andalignments, which are not always available and havea significant cost to be created.The study leaves many opened questions: Whatis the required translation quality which would letus substitute target language models?
Are transla-tion errors averaged out when using translated mod-els from more languages?
Can we add a new lan-guage to the TAC multilingual corpus just by usingMT having in mind lower quality (?
lower scores)and being able to quantify the drop?
Experiment-ing with a larger evaluation set could try to find theanswers.26ReferencesO.
Bojar and Z.
?Zabokrtsky?.
2009.
CzEng0.9: Large Par-allel Treebank with Rich Annotation.
Prague Bulletinof Mathematical Linguistics, 92. in print.F.
Boudin, S. Huet, J.M.
Torres-Moreno, and J.M.
Torres-Moreno.
2010.
A graph-based approach to cross-language multi-document summarization.
Researchjournal on Computer science and computer engineer-ing with applications (Polibits), 43:113?118.P.F.
Brown, V.J.D.
Pietra, S.A.D.
Pietra, and R.L.
Mercer.1993.
The mathematics of statistical machine trans-lation: Parameter estimation.
Computational linguis-tics, 19(2):263?311.C.
Callison-Burch, P. Koehn, C. Monz, K. Peterson,M.
Przybocki, and O.F.
Zaidan.
2010.
Findings ofthe 2010 joint workshop on statistical machine trans-lation and metrics for machine translation.
In Proceed-ings of the Joint Fifth Workshop on Statistical MachineTranslation and MetricsMATR, pages 17?53.
Associa-tion for Computational Linguistics.W.A.
Gale and K.W.
Church.
1994.
A program for align-ing sentences in bilingual corpora.
Computational lin-guistics, 19.G.
Giannakopoulos, M. El-Haj, B. Favre, M. Litvak,J.
Steinberger, and V. Varma.
2012.
Tac 2011 multil-ing pilot overview.
In Proceedings of TAC?11.
NIST.Y.
Gong and X. Liu.
2002.
Generic text summarizationusing relevance measure and latent semantic analysis.In Proceedings of ACM SIGIR, New Orleans, US.P.
Koehn, F.J. Och, and D. Marcu.
2003.
Statisti-cal phrase-based translation.
In Proceedings of the2003 Conference of the North American Chapter of theAssociation for Computational Linguistics on HumanLanguage Technology-Volume 1, pages 48?54.
Asso-ciation for Computational Linguistics.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, et al 2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proceed-ings of the 45th Annual Meeting of the ACL on Inter-active Poster and Demonstration Sessions, pages 177?180.
Association for Computational Linguistics.P.
Koehn.
2005.
Europarl: A parallel corpus for statis-tical machine translation.
In Proceedings of the MTsummit, volume 5.K.
Papineni, S. Roukos, T. Ward, and W.J.
Zhu.
2002.Bleu: a method for automatic evaluation of machinetranslation.
In Proceedings of the 40th annual meet-ing on association for computational linguistics, pages311?318.
Association for Computational Linguistics.H.
Saggion, D. Radev, S. Teufel, W. Lam, and S.M.Strassel.
2002.
Developing infrastructure for the eval-uation of single and multi-document summarizationsystems in a cross-lingual environment.
In Proceed-ings of LREC 2002, pages 747?754.H.
Saggion, J.M.
Torres-Moreno, I. Cunha, and E. San-Juan.
2010.
Multilingual summarization evaluationwithout human models.
In Proceedings of the 23rd In-ternational Conference on Computational Linguistics:Posters, pages 1059?1067.
Association for Computa-tional Linguistics.J.
Savoy and L. Dolamic.
2009.
How effective isgoogle?s translation service in search?
Communica-tions of the ACM, 52(10):139?143.J.
Steinberger and K. Jez?ek.
2004.
Text summarizationand singular value decomposition.
In Proceedings ofthe 3rd ADVIS conference, Izmir, Turkey.R.
Steinberger, B. Pouliquen, A. Widiger, C. Ignat, T. Er-javec, D. Tufis, and D. Varga.
2006.
The jrc-acquis:A multilingual aligned parallel corpus with 20+ lan-guages.
Arxiv preprint cs/0609058.J.
Steinberger, M. Kabadjov, R. Steinberger, H. Tanev,M.
Turchi, and V. Zavarella.
2011.
Jrcs participationat tac 2011: Guided and multilingual summarizationtasks.
In Proceedings of the Text Analysis Conference(TAC).J.
Tiedemann.
2009.
News from opus-a collection ofmultilingual parallel corpora with tools and interfaces.In Proceedings of the Recent Advances in Natural Lan-guage Processing Conference, volume 5, pages 237?248.
John Benjamins Amsterdam.M.
Turchi, J. Steinberger, M. Kabadjov, and R. Stein-berger.
2010.
Using parallel corpora for multilin-gual (multi-document) summarisation evaluation.
InProceedings of the Multilingual and Multimodal Infor-mation Access Evaluation Conference, pages 52?63.Springer.M.
Turchi, M. Atkinson, A. Wilcox, B. Crawley,S.
Bucci, R. Steinberger, and E. Van der Goot.
2012.Onts:optima news translation system.
In Proceedingsof EACL 2012, page 25.X.
Wan, H. Li, and J. Xiao.
2010.
Cross-languagedocument summarization based on machine transla-tion quality prediction.
In Proceedings of the 48thAnnual Meeting of the Association for ComputationalLinguistics, pages 917?926.
Association for Computa-tional Linguistics.O.
Yeloglu, E. Milios, and N. Zincir-Heywood.
2011.Multi-document summarization of scientific corpora.In Proceedings of the 2011 ACM Symposium on Ap-plied Computing, pages 252?258.
ACM.27
