Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 549?553, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsSZTE-NLP: Sentiment Detection on Twitter MessagesViktor Hangya, Ga?bor Berend, Richa?rd FarkasUniversity of SzegedDepartment of Informaticshangyav@gmail.com, {berendg,rfarkas}@inf.u-szeged.huAbstractIn this paper we introduce our contributionto the SemEval-2013 Task 2 on ?SentimentAnalysis in Twitter?.
We participated in ?taskB?, where the objective was to build mod-els which classify tweets into three classes(positive, negative or neutral) by their con-tents.
To solve this problem we basically fol-lowed the supervised learning approach andproposed several domain (i.e.
microblog) spe-cific improvements including text preprocess-ing and feature engineering.
Beyond the su-pervised setting we also introduce some earlyresults employing a huge, automatically anno-tated tweet dataset.1 IntroductionIn the past few years, the popularity of social me-dia has increased.
Many studies have been made inthe area (Jansen et al 2009; O?Connor et al 2010;Bifet and Frank, 2010; Sang and Bos, 2012).
Peo-ple post messages on a variety of topics, for exampleproducts, political issues, etc.
Thus a big amount ofuser generated data is created day-by-day.
The man-ual processing of this data is impossible, thereforeautomatic procedures are needed.In this paper we introduce an approach which isable to assign sentiment labels to Twitter messages.More precisely, it classifies tweets into positive, neg-ative or neutral polarity classes.
The system partici-pated in the SemEval-2013 Task 2: Sentiment Anal-ysis in Twitter, Task?B Message Polarity Classifica-tion (Wilson et al 2013).
In our approach we useda unigram based supervised model because it hasbeen shown that it works well on short messages liketweets (Jiang et al 2011; Barbosa and Feng, 2010;Agarwal et al 2011; Liu, 2010).
We reduced thesize of the dictionary by normalizing the messagesand by stop word filtering.
We also explored novelfeatures which gave us information on the polarity ofa tweet, for example we made use of the acronymsin messages.In the ?constrained?
track of Task?B we used thegiven training and development data only.
For the?unconstrained?
track we downloaded tweets usingthe Twitter Streaming API1 and automatically anno-tated them.
We present some preliminary results onexploiting this huge dataset for training our classi-fier.2 ApproachAt the beginning of our experiments we used aunigram-based supervised model.
Later on, we re-alized that the size of our dictionary is huge, soin the normalization phase we tried to reduce thenumber of words in it.
We investigated novel fea-tures which contain information on the polarity ofthe messages.
Using these features we were able toimprove the precision of our classifier.
For imple-mentation we used the MALLET toolkit, which is aJava-based package for natural language processing(McCallum, 2002).2.1 NormalizationOne reason for the unusually big dictionary size isthat it contains one word in many forms, for exam-1https://dev.twitter.com/docs/streaming-apis/streams/public549ple in upper and lower case, in a misspelled form,with character repetition, etc.
On the other hand, itcontained numerous special annotations which aretypical for blogging, such as Twitter-specific anno-tations, URL?s, smileys, etc.
Keeping these in mindwe made the following normalization steps:?
First, in order to get rid of the multiple formsof a single word we converted them into lowercase form then we stemmed them.
For this pur-pose we used the Porter Stemming Algorithm.?
We replaced the @ and # Twitter-specific tagswith the [USER] and [TAG] notations, respec-tively.
Besides we converted every URL in themessages to the [URL] notation.?
Smileys in messages play an important rolein polarity classification.
For this reason wegrouped them into positive and negative smi-ley classes.
We considered :), :-),: ), :D, =), ;),; ), (: and :(, :-(, : (, ):, ) : smileys as positiveand negative, respectively.?
Since numbers do not contain information re-garding a message polarity, we converted themas well to the [NUMBER] form.
In ad-dition, we replaced the question and excla-mation marks with the [QUESTION MARK]and [EXCLAMATION MARK] notations.
Af-ter this we removed the unnecessary char-acters ?"#$%&()*+,./:;<=>\?
{}?, withthe exception that we removed the ?
characteronly if a word started or ended with it.?
In the case of words which contained characterrepetitions ?
more precisely those which con-tained the same character at least three timesin a row ?, we reduced the length of this se-quence to three.
For instance, in the caseof the word yeeeeahhhhhhh we got the formyeeeahhh.
This way we unified these charac-ter repetitions, but we did not loose this extrainformation.?
Finally we made a stop word filtering in orderto get rid of the undesired words.
To identifythese words we did not use a stop word dictio-nary, rather we filtered out those words whichappeared too frequently in the training corpus.We have chosen this method because we wouldlike to automatically detect those words whichare not relevant in the classification.Before the normalization step, the dictionary con-tained approximately 41, 000 words.
After the aboveintroduced steps we managed to reduce the size ofthe dictionary to 15, 000 words.2.2 FeaturesAfter normalizing Twitter messages, we searchedfor special features which characterize the polarityof the tweets.
One such feature is the polarity ofeach word in a message.
To determine the polarityof a word, we used the SentiWordNet sentiment lex-icon (Baccianella et al 2010).
In this lexicon, a pos-itive, negative and an objective real value belong toeach word, which describes the polarity of the givenword.
We consider a word as positive if the relatedpositive value is greater than 0.3, we consider it asnegative if the related negative value is greater than0.2 and we consider it as objective if the related ob-jective value is greater than 0.8.
The threshold of theobjective value is high because most words are ob-jective in this lexicon.
After calculating the polarityof each word we created three new features for eachtweet which are the number of positive, negative andobjective words, respectively.
We also checked if anegation word precedes a positive or negative wordand if so we inverted its polarity.We also tried to group acronyms by their polarity.For this purpose we used an acronym lexicon whichcan be found on the www.internetslang.comwebsite.
For each acronym we used the polarity ofeach word in the acronym?s description and we de-termined the polarity of the acronym by calculat-ing the rate of positive and negative words in thedescription.
This way we created two new fea-tures which are the number of positive and negativeacronyms in a given message.Our intuition was that people like to use characterrepetitions in their words for expressing their happi-ness or sadness.
Besides normalizing these tokens(see Section 2.1), we created a new feature as well,which represents the number of this kind of wordsin a tweet.Beyond character repetitions people like to writewords or a part of the text in upper case in order to550call the reader?s attention.
Because of this we cre-ated another feature which is the number of uppercase words in the given text.3 Collected DataIn order to achieve an appropriate precision with su-pervised methods we need a big amount of trainingdata.
Creating this database manually is a hard andtime-consuming task.
In many cases it is hard evenfor humans to determine the polarity of a message,for instance:After a whole 5 hours away from work, Iget to go back again, I?m so lucky!In the above tweet we cannot decide precisely thepolarity because the writer can be serious or just sar-castic.In order to increase the size of the training datawe acquired additional tweets, which we used inthe unconstrained run for Task?B.
We created an ap-plication which downloads tweets using the TwitterStreaming API.
The API supports language filter-ing, which was used to get rid of non-English mes-sages.
Our manual investigations of the downloadedtweets revealed, however, that this filter allows a bigamount of non-English tweets, which is probablydue to the fact that some Twitter users did not settheir language.
We used Twitter4J2 API (which isa Java library for the Twitter API) for downloadingthese tweets.
We automatically annotated the down-loaded tweets using the Twitter Sentiment3 web ap-plication, similar to Barbosa and Feng (2010) butwe used only one annotator.
This web applicationalso supports language detection, but after this extrafiltration, our dataset still contained a considerableamount of non-English messages.
After 16 hoursof data collection we got 350, 000 annotated tweets,where the distribution of neutral, positive and neg-ative classes was approximately 60%, 20%, 20%,respectively.
For further testing purposes we havechosen 10, 000 tweets from each class.4 ResultsWe report results on the two official test sets of theshared task.
The ?twitter?
test set consists of 3, 8132http://twitter4j.org3http://www.sentiment140.comtweets while the ?sms?
set consists of 2, 094 smsmessages.
We evaluated both test databases in twoways, in the so-called constrained run we only usedthe official training database, while in the uncon-strained run we also used a part of the additionaldata, which was mentioned in the 3 section.
Theofficial training database contained 4, 028 positive,1, 655 negative and 3, 821 neutral tweets while forthe unconstrained run we used an additional 10, 000tweets from each class.
This way in each phase wegot four kinds of runs, which were evaluated withthe Na?
?ve Bayes and Maximum Entropy classifiers.In Table 1 the evaluation of the unigram-basedmodel with the Na?
?ve Bayes learner can be seen.The table contains the F-scores for the positive, neg-ative and neutral labels for each of the four runs.The avg column contains the average F-score for thepositive and negative labels, which was the officialevaluation metric for SemEval-2013 Task 2.
We gotthe best scores for the neutral label whilst the worstscores are obtained for the negative label, which isdue to the fact that there were much less negativeinstances in the training database.
It can be seenthat the F-scores for the unconstrained run are betterboth for the tweet and sms test databases.
For theunigram-based model the F-scores are higher whenwe used the Maximum Entropy model (see Table 2).pos neg neut avgtwitter-constrained 0.59 0.09 0.65 0.34twitter-unconstrained 0.60 0.17 0.65 0.38sms-constrained 0.46 0.16 0.63 0.31sms-unconstrained 0.47 0.38 0.53 0.42Table 1: Unigram-based model, Na?
?ve Bayes learnerpos neg neut avgtwitter-constrained 0.60 0.33 0.67 0.46twitter-unconstrained 0.60 0.40 0.66 0.50sms-constrained 0.47 0.31 0.69 0.39sms-unconstrained 0.52 0.47 0.66 0.49Table 2: Unigram-based model, Maximum EntropylearnerIn Tables 3 and 4 the evaluation results can beseen for the normalized model.
The normalization551step increased the precision for both learning al-gorithms and the Maximum Entropy learner is stillbetter than Na?
?ve Bayes.
Besides this we noticedthat for both learners in the case of the tweet testdatabase, the unconstrained run had lower scoresthan the constrained whilst in the case of the smstest database this phenomenon did not appear.pos neg neut avgtwitter-constrained 0.65 0.32 0.67 0.48twitter-unconstrained 0.62 0.21 0.63 0.41sms-constrained 0.56 0.27 0.72 0.41sms-unconstrained 0.52 0.35 0.66 0.43Table 3: Normalized model, Na?
?ve Bayes learnerpos neg neut avgtwitter-constrained 0.66 0.40 0.68 0.53twitter-unconstrained 0.61 0.42 0.64 0.51sms-constrained 0.61 0.38 0.77 0.49sms-unconstrained 0.57 0.47 0.72 0.52Table 4: Normalized model, Maximum EntropylearnerThe evaluation results of the feature-based modelcan be seen in Tables 5 and 6.
In the case of theNa?
?ve Bayes learner, the features did not increase theF-scores, only for the sms-unconstrained run.
Forthe other runs the achieved scores decreased.
In thecase of the Maximum Entropy learner the featuresincreased the F-scores, slightly for the constrainedruns and a bit more for the unconstrained runs.From this analysis we can conclude that the nor-malization of the messages yielded a considerableincrease in the F-scores.
We discussed above thatthis step also significantly reduced the size of thedictionary.
The features increased the precision too,especially for the unconstrained run.
This meansthat these features represent properties which areuseful for those training data which are not from thesame corpus as the test messages.
We compared twomachine learning algorithms and from the results weconcluded that the Maximum Entropy learner per-forms better than the Na?
?ve Bayes on this task.
Ourexperiments also showed that the external, automat-ically labeled training database helped only in theclassification of sms messages.
This is due to thefact that the smses and our external database arefrom a different distribution than the official tweetdatabase.pos neg neut avgtwitter-constrained 0.65 0.32 0.67 0.48twitter-unconstrained 0.62 0.17 0.79 0.39sms-constrained 0.56 0.38 0.74 0.47sms-unconstrained 0.54 0.29 0.70 0.41Table 5: Feature-based model, Na?
?ve Bayes learnerpos neg neut avgtwitter-constrained 0.66 0.41 0.69 0.54twitter-unconstrained 0.63 0.43 0.65 0.53sms-constrained 0.62 0.39 0.79 0.50sms-unconstrained 0.61 0.49 0.75 0.55Table 6: Feature-based model, Maximum Entropylearner5 Conclusions and Future WorkRecently, sentiment analysis on Twitter messageshas gained a lot of attention due to the huge amountof Twitter users and their tweets.
In this paper we ex-amined different methods for classifying Twitter andsms messages.
We proposed special features whichcharacterize the polarity of the messages and weconcluded that due to the informality (slang, spellingmistakes, etc.)
of the messages it is crucial to nor-malize them properly.In the future, we plan to investigate the utility ofrelations between Twitter users and between theirtweets and we are interested in topic-dependent sen-timent analysis.AcknowledgmentsThis work was supported in part by the Euro-pean Union and the European Social Fund throughproject FuturICT.hu (grant no.
: TA?MOP-4.2.2.C-11/1/KONV-2012-0013).ReferencesApoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow,and Rebecca Passonneau.
2011.
Sentiment Analysis552of Twitter Data.
In Proceedings of the Workshop onLanguage in Social Media (LSM 2011), pages 30?38,June.Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-tiani.
2010.
SentiWordNet 3.0: An Enhanced Lex-ical Resource for Sentiment Analysis and OpinionMining.
In Nicoletta Calzolari (Conference Chair),Khalid Choukri, Bente Maegaard, Joseph Mariani,Jan Odijk, Stelios Piperidis, Mike Rosner, and DanielTapias, editors, Proceedings of the Seventh Interna-tional Conference on Language Resources and Evalu-ation (LREC?10), Valletta, Malta, May.
European Lan-guage Resources Association (ELRA).Luciano Barbosa and Junlan Feng.
2010.
Robust Sen-timent Detection on Twitter from Biased and NoisyData.
In Poster volume, Coling 2010, pages 36?44,August.Albert Bifet and Eibe Frank.
2010.
Sentiment Knowl-edge Discovery in Twitter Streaming Data.Bernard J. Jansen, Mimi Zhang, Kate Sobel, and AbdurChowdury.
2009.
Twitter Power: Tweets as ElectronicWord of Mouth.
In Journal of the American societyfor information science and technology, pages 2169?2188.Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and TiejunZhao.
2011.
Target-dependent Twitter SentimentClassification.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Linguis-tics, pages 151?160, June.Bing Liu.
2010.
Sentiment Analysis and Subjectivity.
InN.
Indurkhya and F. J. Damerau, editors, Handbook ofNatural Language Processing.Andrew Kachites McCallum.
2002.
Mal-let: A machine learning for language toolkit.http://mallet.cs.umass.edu.Brendan O?Connor, Ramnath Balasubramanyan,Bryan R. Routledge, and Noah A. Smith.
2010.From Tweets to Polls: Linking Text Sentiment toPublic Opinion Time Series.
In Proceedings of theInternational AAAI Conference on Weblogs and SocialMedia, May.Erik Tjong Kim Sang and Johan Bos.
2012.
Predictingthe 2011 Dutch Senate Election Results with Twitter.In Proceedings of the 13th Conference of the EuropeanChapter of the Association for Computational Linguis-tics, pages 53?60, April.Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, SaraRosenthal, Veselin Stoyanov, and Alan Ritter.
2013.SemEval-2013 Task 2: Sentiment Analysis in Twitter.In Proceedings of the International Workshop on Se-mantic Evaluation, SemEval ?13, June.553
