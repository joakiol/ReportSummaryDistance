Proceedings of the 8th Workshop on Asian Language Resources, pages 30?37,Beijing, China, 21-22 August 2010. c?2010 Asian Federation for Natural Language ProcessingAugmenting a Bilingual Lexicon with Informationfor Word Translation DisambiguationTakashi TsunakawaFaculty of InformaticsShizuoka Universitytuna@inf.shizuoka.ac.jpHiroyuki KajiFaculty of InformaticsShizuoka Universitykaji@inf.shizuoka.ac.jpAbstractWe describe a method for augmentinga bilingual lexicon with additional in-formation for selecting an appropriatetranslation word.
For each word in thesource language, we calculate a corre-lation matrix of its association wordsversus its translation candidates.
Weestimate the degree of correlation byusing comparable corpora based onthese assumptions: ?parallel word as-sociations?
and ?one sense per wordassociation.?
In our word translationdisambiguation experiment, the resultsshow that our method achieved 42%recall and 49% precision for Japa-nese-English newspaper texts, and 45%recall and 76% precision for Chi-nese-Japanese technical documents.1 IntroductionThe bilingual lexicon, or bilingual dictionary,is a fundamental linguistic resource for multi-lingual natural language processing (NLP).
Foreach word, multiword, or expression in thesource language, the bilingual lexicon providestranslation candidates representing the originalmeaning in the target language.Selecting the right words for translation is aserious problem in almost all of multilingualNLP.
One word in the source language almostalways has two or more translation candidatesin the target language by looking up them inthe bilingual lexicon.
Because each translationcandidate has a distinct meaning and property,we must be careful in selecting the appropriatetranslation candidate that has the same sense asthe word inputted.
This task is often calledword translation disambiguation.In this paper, we describe a method for add-ing information for word translation disam-biguation into the bilingual lexicon.
Compara-ble corpora can be used to determine whichword associations suggest which translationsof the word (Kaji and Morimoto, 2002).
First,we extract word associations in each languagecorpus and align them by using a bilingual dic-tionary.
Then, we construct a word correlationmatrix for each word in the source language.This correlation matrix works as informationfor word translation disambiguation.We carried out word translation experimentson two settings: English-to-Japanese and Chi-nese-to-Japanese.
In the experiments, we testedDice/Jaccard coefficients, pointwise mutualinformation, log-likelihood ratio, and Student?st-score as the association measures for extract-ing word associations.2 Constructing word correlation ma-trices for word translation disam-biguation2.1 Outline of our methodIn this section, we describe the method forcalculating a word correlation matrix for eachword in the source language.
The correlationmatrix for a word f consists of its associationwords and its translation candidates.
Amongthe translation candidates, we choose the mostacceptable one that is strongly suggested by itsassociation words occurring around f.We use two assumptions for this framework:(i)  Parallel word associations:Translations of words associated witheach other in a language are also asso-ciated with each other in another language30(Rapp, 1995).
For example, two Englishwords ?tank?
and ?soldier?
are associatedwith each other and their Japanese trans-lations ???
(sensha)?
and ???
(hei-shi)?
are also associated with each other.
(ii)  One sense per word association:A polysemous word exhibits only onesense of a word per word association(Yarowsky, 1993).
For example, a poly-semous word ?tank?
exhibits the ?militaryvehicle?
sense of a word when it is asso-ciated with ?soldier,?
while it exhibits the?container for liquid or gas?
sense when itis associated with ?gasoline.
?Under these assumptions, we determine whichof the words associated with an input wordsuggests which of its translations by aligningword associations by using a bilingual dictio-nary.
Consider the associated English words(tank, soldier) and their Japanese translations(??
(sensha), ??
(heishi)).
When wetranslate the word ?tank?
into Japanese, theassociated word ?soldier?
helps us to translateit into ???
(sensha)?, not to translate it into????
(tanku)?
which means ?a storagetank.
?This naive method seems to suffer from thefollowing difficulties: A disparity in topical coverage betweentwo corpora in two languages A shortage in the bilingual dictionary The existence of polysemous associatedwords that cannot determine the correctsense of the input wordFor these difficulties, we use the tendency thatthe two words associated with a third word arelikely to suggest the same sense of the thirdword when they are also associated with eachother.
For example, consider an English asso-ciated word pair (tank, troop).
The word ?troop?cannot distinguish the different meanings be-cause it can co-occur with the word ?tank?
inboth senses of the word.
The third word ?sol-dier,?
which is associated with both ?tank?
and?troop,?
can suggest the translation ???(sensha).
?The overview of our method is shown inFigure 1.
We first extract associated word pairsin the source and target languages from com-parable corpora.
Using a bilingual dictionary,we obtain alignments of these word associa-tions.
Then, we iteratively calculate a correla-tion matrix for each word in the source lan-guage.
Finally, we select the translation withthe highest correlation from the translationcandidates of the input word and theco-occurring words.For each input word in the source language,we calculate correlation values between theirtranslation candidates and their associationwords.
The algorithm is shown in Figure 2.In Algorithm 1, the initialization of correla-tion values is based on word associations,where D is a set of word pairs in the bilingualdictionary, and Af and Ae are the sets of asso-ciated word pairs.
First, we retain associatedwords f?
(i) when its translation e?
exists andwhen e?
is associated with e. In the iteration,the correlation values of associated words f?
(i)that suggest e(j) increase relatively by usingassociation scores ((), ) andFigure 1.
Overview of our method.31((), ).
In our experiments, we set thenumber of iterations Nr to 10.2.2 Alternative association measures forextracting word associationsWe extract co-occurring word pairs and calcu-late their association scores.
In this paper, wefocus on some frequently used metrics forfinding word associations based on their oc-currence/co-occurrence frequencies.Suppose that words x and y frequentlyco-occur.
Let n1 and n2 be the occurrence fre-quencies of x and y respectively, and let m bethe frequency that x and y co-occur between wcontent words.
The parameter w is a windowsize that adjusts the range of co-occurrences.Let N and M be the sum of occur-rences/co-occurrences of all words/word pairs,respectively.
The frequencies are summarizedin Table 1.The word association scores (, ) are de-fined as follows: Dice coefficient (Smadja, 1993)Dice(, ) =2+(1) Jaccard coefficient (Smadja et al, 1996)Jaccard(, ) =+ 	  (2) Pointwise mutual information (pMI)(Church and Hanks, 1990)pMI(, ) = log/( )(	  )(3) Log-likelihood ratio (LLR) (Dunning,1993)LLR(, )= 2logL(,, )+ logL(	  , , )logL(,, )logL(	  , , ); (4)logL(, 	, ) = log  + (	  ) log(1  ), (5)=,  =,  =(6) Student?s t-score (TScore) (Church et al,1991)TScore(, ) =(7)We calculate association scores for all pairsof words when their occurrence frequenciesare not less than a threshold Tf and when theirxoccurx notoccurTotalyoccurm n2 ?
m n2y notoccurn1 ?
m M ?
n1?
n2 + mN ?
n2Total n1 N ?
n1 NTable 1.
Contingency matrix of occurrencefrequencies.Figure 2.
Algorithm for calculating correlationmatrices.
? ((), )  (, ())max ((), )   (, ())(, ) =!1 (": (, ) # $, (, ) # %)0 (otherwise)&'Algorithm 1:Input:f: an input wordf?
(1), ?, f?
(I): associated words of fe(1), ?, e(J): translation candidatesof fNr: number of iterationsA bilingual lexiconWord association scores  for bothlanguagesOutput:Cf = [Cf (f?
(i), e(j))]: a correlation ma-trix for f1: if  *(), () - 0 then2:   *(), ()  .3456(7),8(9)< 346(7),8()>3: else4:   *(), ()  .
05: end6:  .
07: while i < Nr8:    .
 + 19:   *(), ()  .
((), )10: end(?
: = ?
{**|(,**)#@4,(*(7),**)#@4})32threshold Tc.We handle word pairs whose associationscores are not less than a predefined value TA;some of the thresholds were evaluated in ourexperiment.
The associated word pair sets Afand Ae in Algorithm 1 includes only word pairswhose scores are not less than TA in the sourceand target language, respectively.3 Word Translation DisambiguationConsider that a translator changes a word in aninput sentence.
Usually, two or more transla-tion candidates are enumerated in the bilingualdictionary for a word.
The translator shouldselect a translation word that is grammatically/syntactically correct, semantically equivalentto the input, and pragmatically appropriate.We assume that the translation word e for aninput word f tends to be selected if words oc-curring around f are strongly correlated with e.Using the correlation matrices, we select e as atranslation if the associated word f?
occursaround f and the score Cf(f?,e) is large.
In addi-tion, we take distance between f and f?
intoaccount.We define the score of the translation worde(f0) for an input word f0 as follows.
Consideran input word f0 that occurs in the context of??
f-2 f-1 f0 f1 f2 ?.?
The score for a trans-lation word (A) for an input word f0 isdefined as where p is the relative position ofthe words surrounding f0, B 5C, (A)< isthe value of the correlation matrix for f0, and EFigure 3.
An example of an input word ?home?is the window size for word translation disam-biguation.A simple example is shown in Figure 3.
Theword ?home?
in this context means the senseof ?home base?
used in baseball games, not?house?
or ?hometown.?
The surroundingwords such as ?games,?
?bases,?
and ?runs?can be clues for indicating the correct sense ofthe word.
By using the correlation matrix (Ta-ble 2) and formula (8), we calculate a score foreach translation candidate and select the besttranslation with the largest score.
In this case,Score(??
(honrui)) = 0.1134 was the bestscore and the correct translation was selected.4 Experiment4.1 Experimental settingsWe carried out word translation experimentson two different settings.
In the first experi-ment (Experiment A), we used large-scalecomparable corpora from English and Japanesenewspaper texts.
The second experiment (Ex-periment B) was targeted at translating tech-nical terms by using Chinese and Japanesedomain-specific corpora.We used the following linguistic resourcesfor the experiments: Experiment A<home> ?(kuni)[country]??
(honrui)[home base]?(ie)[house]??
(jitaku)[my home]??(katei)[homeplace]??
(shisetsu)[facilities]base 0.009907 0.017649 0.005495 0.006117 0.005186 0.005597game 0.043507 0.048358 0.025145 0.028208 0.019987 0.023014Kansas 0.010514 0.003786 0.004280 0.007307 0.004320 0.005459run 0.023468 0.042035 0.014430 0.015765 0.012061 0.012986season 0.044855 0.050952 0.025406 0.028506 0.020716 0.023631Table 2.
Word correlation matrix for a word ?home,?as information for word translation disambiguationScore(A)= F1G|H|B 5C, (A)< ,K|C|KN(8)A career .284 hitter, Beltran batted .267in the regular season, split betweenKansas City and Houston, but camealive in the playoffs.
He hit .435 in 12postseason games, with six stolen bases,eight home runs and 14 runs batted in.33 Training comparable corpora The New York Times texts fromEnglish Gigaword CorpusFourth Edition (LDC2009T13):1.6 billion words The Mainichi Shimbun Corpus(2000-2005): 195 million words Test corpus A part of The New York Times(January 2005): 157 paragraphs,1,420 input words Experiment B1 Training comparable corpus In-house Chinese-Japanese pa-rallel corpus in the environmentdomain: 53,027 sentence pairs1 Test corpus A part of the training data: 1,443sentences, 668 input words Experiment B2 Training comparable corpus In-house Chinese-Japanese pa-rallel corpus in the medical do-main: 123,175 sentence pairs Test corpus A part of the training data: 940sentences, 3,582 input words Dictionaries Japanese-English bilingual dictiona-ries: Total 333,656 term pairs EDR Electronic Dictionary Eijiro, Third Edition EDICT (Breen, 1995) Chinese-English bilingual dictionary Chinese-English TranslationLexicon Version 3.0 (LDC2002L27): 54,170 term pairs Wanfang Data Chinese-EnglishScience/Technology BilingualDictionary: 525,259 term pairsFor the Chinese-Japanese translation, we gen-erated a Chinese-Japanese bilingual dictionaryby merging Chinese-English and Japa-nese-English dictionaries.
The Chi-nese-Japanese bilingual dictionary includes1 We could prepare only parallel corpora for Chi-nese-Japanese language pair as training corpora.For our experiments, we assumed them as compa-rable corpora and did not use the correspondence ofsentence pairs.every Chinese-Japanese term pair (tC, tJ) when(tC, tE) and (tJ, tE) were present in the dictiona-ries for one or more English terms tE.
Thismerged dictionary contains about two millionterm pairs.
While these Chinese-Japanese termpairs include wrong translations, it was not aserious problem in our experiments becausewrong translations were excluded in the pro-cedure of our method.We applied morphological analysis andpart-of-speech tagging by using TreeTagger(Schmid, 1994) for English, JUMAN for Jap-anese, and mma (Kruengkrai et al, 2009) forChinese, respectively.In the test corpus, we manually annotatedreference translations for each target word.
2Experiment A:The parameters we used were as follows:Tf = 100 (Japanese), Tf = 1000 (English),Tc = 4, w = 30, E = 30.Experiment B1/B2:Tf = 100, Tc = 4, w = 10, E = 25.Some of the parameters were empirically ad-justed.In the experiments, the matrices could beobtained for 9103 English words (A), 674Chinese words (B1) and 1258 Chinese words(B2), respectively.
In average one word had3.24 (A), 1.15 (B1) and 1.51 (B2) translationcandidates34.2 Results of English-Japanese wordtranslationby using the best setting.
Table 2is the resulted matrix for the word ?home?
inthe Experiment A.Table 3 shows the results of Experiment A. Weclassified the translation results for 1,420target English words into four categories: True,False, R, and M. When the translation wasoutput, the result was True if the output isincluded in the reference translations, and itwas False otherwise.
The result was R when allthe associated words in the correlation matrix2 We prepared multiple references for several tar-get words.
The average numbers of reference trans-lations for an input word are 1.84 (A), 1.50 (B1),and 1.48 (B2), respectively.3 From each matrix, we cut off the columns withtranslations that do not have the best scores for anyassociated words, because such translations arenever selected.34did not occur around the input word.
The resultwas M when no correlation matrix existed forthe input word.
We did not select a translationoutput in these cases.
The recall and precisionare shown in the parentheses below. Recall = (True) / (Number of input words) Precision = (True) / (True + False)Among the settings, we obtained the bestresults, 42% recall and 49% precision, whenwe used the Jaccard coefficient for associationscores and TA = 0, which means all pairs weretaken into consideration.
Among other settings,the Dice coefficient achieved a comparableperformance with Jaccard.4.3 Results of Chinese-Japanese wordtranslationTables 4 and 5 show the results of ExperimentB.
In each domain, we tested only the settingson Dice, pMI, and LLR with TA = 0.In the environmental domain, the pointwisemutual information score achieved the bestperformance, 45% recall and 76% precision.However, the Dice coefficient gave the bestrecall (55%) for the medical domain.
This re-sult indicates that Experiment B1/B2 hadhigher precision and more words without thecorrelation matrix than Experiment A had.4.4 DiscussionAs a result, we could generate bilingual lex-icons with word translation disambiguationinformation for 9103 English words and 1932Chinese words.
Although the number of wordsmight be augmented by changing the settings,the size does not seem to be sufficient as bi-lingual dictionaries.
The availability of largeroutput should be investigated.The experimental results show that our me-thod selected correct translations for at leasthalf of the input words if a correlation matrixexisted and if the associated words co-occur.Among all input words, at least 40% of theinput words can be translated.
The bilingualdictionaries included 24.4, 38.6, and 52.0translation candidates for one input word inExperiment A, B1, and B2, respectively.
Whenwe select the most frequent word, the preci-sions were 7%, 1%, and 1%, respectively.Meanwhile, the average numbers of translationTable 3.
Results of English-Japanese wordtranslation (A)Table 4.
Results of Chinese-Japanese wordtranslation for environmental domain (B1).Table 5.
Results of Chinese-Japanese wordtranslation for medical domain (B2).candidates in the correlation matrices for oneinput word are shown in Table 6.
These indi-cate that our method effectively removed noisyScore TA True False R MDice 0 588 627 90 115(41%/48%)0.001 586 619 94 121(41%/49%)0.01 479 507 243 191(34%/49%)Jacc-ard0 594 621 90 115(42%/49%)0.001 584 609 105 122(41%/49%)0.01 348 378 374 320(25%/48%)pMI 0 292 309 704 115(21%/49%)1 293 308 703 116(21%/49%)LLR 10 530 747 28 115(37%/42%)100 529 744 32 115(37%/42%)T-Score1 486 793 26 115(34%/38%)4 489 787 26 118(34%/38%)Score TA True False R MDice 0 1984 895 82 621(55%/69%)pMI 0 1886 804 271 621(53%/70%)LLR 0 1652 1246 63 621(46%/57%)Score TA True False R MDice 0 277 124 14 253(41%/69%)pMI 0 303 95 17 253(45%/76%)LLR 0 269 131 15 253(40%/67%)35translations from the Chinese-Japanese dictio-nary merged Japanese-English and Chi-nese-English dictionaries, and that the associa-tion scores contributed word translation dis-ambiguation.Among the settings, the Jaccard/Dice coef-ficients were proven to be effective, althoughpointwise mutual information (pMI) was alsoeffective for technical domains and the Chi-nese-Japanese language pair.
Because the Jac-card/Dice coefficients were originally used formeasuring the proximity of sets, these mightbe effective for collecting related words byusing the similarity of kinds of co-occurringwords.
However, pMI tends to emphasizelow-frequency words as associated words.
Theconsequence of this tendency might be thatlow-frequency associated words do not appeararound the input word in the newspaper text.4In most metrics for the association score, thelowest threshold value TA achieved the bestperformance.
This result indicates that thecut-off of associated words by some thresholdswas not effective, although it requires moretime and memory space to obtain correlationmatrices without cut-off.
How to optimize oth-er parameters in our method remains unsolved.More words without the correlation matrixwere present in Experiment B1/B2 than in Ex-periment A because the input word was often atechnical term that was not in the bilingual dic-tionary.
The better recall and precision of Ex-periment B1/B2 came from several reasons,including difference of test sets and languagepairs.
In addition, it might have an impact onthis result that the fact that word translationdisambiguation of technical terms is easierthan word translation disambiguation of com-mon words.We handled only nouns as input words andassociated words in this study.
Consideringonly the co-occurrence in a fixed windowwould be insufficient to apply this method tothe translation of verbs and other parts ofspeech.
In future work, we will consider syn-4 We limited the maximum number of associationwords for one word to 400 in descending order oftheir association scores because of restriction ofcomputational resources.
In future work, we mayalleviate the drawback of pMI by enlarging or de-leting this limitation.Score TA Exp.A Exp.B1 Exp.B2Dice 0 1.83 0.61 0.91pMI 0 1.27 0.50 0.79LLR 10/0 1.34 1.48 2.75Table 6.
Average numbers of translation can-didates in the correlation matrices for one inputword.tactic co-occurrence, which is obtained byconducting dependency parsing of the resultsof a sentence.
The correlation between asso-ciated words and translation candidates alsoneeds to be re-examined.
Similarly, we willhandle verbs as associated words to the inputnouns by using syntactic co-occurrence.5 Related WorkStatistical machine translation (Brown et al,1990) automatically acquires knowledge forword translation disambiguation from parallelcorpora.
Word translation disambiguation isbased on probabilities calculated from theword alignment, phrase pair extraction, and thelanguage model.
However, much broad con-text/domain information is not considered.Carpuat and Wu (2007) proposed con-text-dependent phrasal translation lexicons byintroducing context-dependent features intostatistical machine translation.Unsupervised methods using dictionariesand corpora were proposed for monolingualWSD (Ide and Veronis, 1998).
They usedgrammatical information includingparts-of-speech, syntactically related words,and co-occurring words as the clues for theWSD.
Our method uses a part of the clues forbilingual WSD and word translation disam-biguation.Li and Li (2002) constructed a classifier forword translation disambiguation by using abilingual dictionary with bootstrapping tech-niques.
We also conducted recursive calcula-tion by dealing with the bilingual dictionary asthe seeds of the iteration.Vickrey et al (2005) introduced a context asa feature for a statistical MT system and theygenerated word-level translations.
How to in-troduce the word-level translation disambigua-tion into sentence-level translation is a consi-derable problem.366 ConclusionIn this paper, we described a method for add-ing information for word translation disam-biguation into the bilingual lexicon, by consi-dering the associated words that co-occur withthe input word.
We based our method on thefollowing two assumptions: ?parallel wordassociations?
and ?one sense per word associa-tion.?
We aligned word associations by using abilingual dictionary, and constructed a correla-tion matrix for each word in the source lan-guage for word translation disambiguation.Experiments showed that our method was ap-plicable for both common newspaper text anddomain-specific text and for two languagepairs.
The Jaccard/Dice coefficients wereproven to be more effective than the other me-trics as word association scores.
Future workincludes extending our method to handle verbsas input words by introducing syntacticco-occurrence.
The comparisons with otherdisambiguation methods and machine transla-tion systems would strengthen the effective-ness of our method.
We consider also evalua-tions on real NLP tasks including machinetranslation.AcknowledgmentsThis work was partially supported by Japa-nese/Chinese Machine Translation Project inSpecial Coordination Funds for PromotingScience and Technology (MEXT, Japan).ReferencesBrown, Peter F., John Cocke, Stephen A. DellaPietra, Vincent J. Della Pietra, Fredrick Jelinek,John D. Lafferty, Robert L. Mercer, and Paul S.Roossin.
1990.
A statistical approach to machinetranslation.
Computational Linguistics,16(2):79-85.Carpuat, Marine and Dekai Wu.
2007.
Con-text-dependent phrasal translation lexicons forstatistical machine translation.
In Proc.
of Ma-chine Translation Summit XI, pages 73-80.Church, Kenneth W., William Gale, Patrick Hanksand Donald Hindle.
1991.
Using statistics inlexical analysis.
Lexical Acquisition: UsingOn-line Resources to Build a Lexicon, pages115-164.Church, Kenneth W. and Patrick Hanks.
1990.
Wordassociation norms, mutual information, and lex-icography.
Computational Linguistics,16(1):22-29.Dunning, Ted.
1993.
Accurate methods for the sta-tistics of surprise and coincidence.
Computa-tional Linguistics, 19(1):61-74.Ide, Nancy and Jean Veronis.
1998.
Introduction tothe special issue on word sense disambiguation:the state of the art.
Computational Linguistics,24(1):1-40.Kaji, Hiroyuki and Yasutsugu Morimoto.
2002.Unsupervised word sense disambiguation usingbilingual comparable corpora.
In Proc.
of the19th International Conference on ComputationalLinguistics, pages 411-417.Kruengkrai, Canasai, Kiyotaka Uchimoto, Jun?ichiKazama, Yiou Wang, Kentaro Torisawa, andHitoshi Isahara.
2009.
An error-drivenword-character hybrid model for joint Chineseword segmentation and POS tagging.
In Proc.
ofthe Joint Conference of the 47th Annual Meetingof the ACL and the 4th International Joint Con-ference on Natural Language Processing of theAFNLP, pages 513-521.Li, Cong and Hang Li.
2002.
Word translationdisambiguation using bilingual bootstrapping.
InProc.
of the 40th Annual Meeting of Associationfor Computational Linguistics, pages 343-351.Rapp, Reinhard.
1995.
Identifying word transla-tions in non-parallel texts.
In Proc.
of the 33rdAnnual Meeting of the Association for Computa-tional Linguistics, pages 320-322.Schmid, Helmut.
1994.
Probabilistic part-of-speechtagging using decision trees.
In Proc.
of the 1stInternational Conference on New Methods inNatural Language Processing.Smadja, Frank.
1993.
Retrieving collocations fromtext: Xtract.
Computational Linguistics,19(1):143-177.Smadja, Frank, Kathleen R. McKeown and Vasi-leios Hatzivassiloglou.
1996.
Translating collo-cations or bilingual lexicons: A statistical ap-proach.
Computational Linguistics, 22(1):3-38.Vickrey, David, Luke Biewald, Marc Teyssier, andDaphne Koller.
2005.
Word-sense disambigua-tion for machine translation.
In Proc.
of theConference on HLT/EMNLP, pages 771-778.Yarowsky, David.
1993.
One sense per collocation.In Proc.
of ARPA Human Language TechnologyWorkshop, pages 266-271.37
