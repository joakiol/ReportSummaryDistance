Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 349?352,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsGeneralizing Hierarchical Phrase-based Translationusing Rules with Adjacent NonterminalsHendra Setiawan and Philip ResnikUMIACS Laboratory for Computational Linguistics and Information ProcessingUniversity of Maryland, College Park, MD 20742, USAhendra, resnik @umd.eduAbstractHierarchical phrase-based translation (Hiero,(Chiang, 2005)) provides an attractive frame-work within which both short- and long-distance reorderings can be addressed consis-tently and efciently.
However, Hiero is gen-erally implemented with a constraint prevent-ing the creation of rules with adjacent nonter-minals, because such rules introduce compu-tational and modeling challenges.
We intro-duce methods to address these challenges, anddemonstrate that rules with adjacent nontermi-nals can improve Hiero's generalization powerand lead to signicant performance gains inChinese-English translation.1 IntroductionHierarchical phrase-based translation (Hiero, (Chi-ang, 2005)) has proven to be a very useful com-promise between syntactically informed and purelycorpus-driven translation.
By automatically learn-ing synchronous grammar rules from parallel text,Hiero captures short- and long-distance reorderingsconsistently and efciently.
However, implementa-tions of Hiero generally forbid adjacent nonterminalsymbols on the source side of hierarchical rules, apractice we will refer to as the non-adjacent nonter-minals constraint.
The main argument against suchrules is that they cause the system to produce multi-ple derivations that all lead to the same translation ?a form of redundancy known as spurious ambiguity.Spurious ambiguity can lead to drastic reductions indecoding efciency, and the obvious solutions, suchas reducing beam width, erode translation quality.In Section 2, we argue that the non-adjacent non-terminals constraints severely limits Hiero's gener-alization power, limiting its coverage of importantreordering phenomena.
In Section 3, we discussthe challenges that arise in relaxing this constraint.In Section 4 we introduce new methods to addressthose challenges, and Section 5 validates the ap-proach empirically.Improving Hiero via variations on rule prun-ing and ltering is well explored, e.g., (Chiang,2005; Chiang et al, 2008; Zollmann and Venugopal,2006), to name just a few.
These proposals dif-fer from each other mainly in the specic linguis-tic knowledge being used, and on which side theconstraints are applied.
In contrast, we complementprevious work by showing that adding rules to Hierocan provide benets if done judiciously.2 Judicious Use of Adjacent NonterminalsOur motivations largely followMenezes and Quirk's(2007) discussion of reorderings and generalization.As a specic example, we will use a Chinese to En-glish verb phrase (VP) translation (Fig.
1), whichrepresents one of the most prominent phrase con-structions in Chinese.
Here the construction of theChinese VP involves joining a prepositional phrase(PP) and a smaller verbal phrase (VP-A), with thepreposition at the beginning as a PP marker.
In thetranslation, the VP-A precedes the PP, a shift frompre-verbal PP in Chinese to post-verbal in English.?\???
??
?rank 10th at Eastern division????????
?PPPPPPPPPPPPPPP NP VP-APPVP??
HH??
HHHHHHFigure 1: A Chinese-English verb phrase translation349Hiero can correctly translate the example if itlearns any of the following rules from training data:X???
X1 ??
?, rank 10th at X1?
(1)X??
??\??
X1, X1 at Eastern div.?
(2)X?
?X1 ?\??
X2, X2 X1 Eastern div.?
(3)However, in practice, data sparsity makes the chanceof learning these rules rather slim.
For instance,learning Rule 1 depends on training data containinginstances of the shift with identical wording for theVP-A, which belongs to an open word class.If Hiero fails to learn any of the above rules, itwill apply the ?glue rules?
S ?
?S X1, S X1?
andS ?
?X, X?.
But these glue rules clearly can-not model the VP-A's movement.
In failing to learnRules 1-3, Hiero has no choice but to translate VP-Ain a monotone order.On the other hand, consider the following ruleswith adjacent nonterminals on the source side (or XXrules, for brevity):X???
X1X2, X2 at X1?
(4)X??X1X2??
?, rank 10th X1X2?
(5)X?
?X1X2, X2X1?
(6)Note that although XX rules 4-6 can potentially in-crease the chance of modeling the pre-verbal to post-verbal shift, not all of them are benecial to learn.For instance, Rule 5 models the word order shift butintroduces spurious ambiguity, since the nontermi-nals are translated in monotone order.
Rule 6, whichresembles the inverted rule of the Inversion Trans-duction Grammar (Wu, 1997), is highly ambigu-ous because its application has no lexical grounding.Rule 4 avoids both problems, and is also easier tolearn, since it is lexically anchored by a preposition,?
(at), which we can expect to appear frequently intraining.
These observations will motivate us to fo-cus on rules that model non-monotone reordering ofphrases surrounding a lexical item on the target side.3 Addressing XX Rule ChallengesThe rst challenge created by introducing XX rulesis computational: relaxing the constraint signi-cantly increases the grammar size.
Motivated byour earlier discussion, we address this by permittingonly rules that model non-monotone reordering, i.e.those rules whose nonterminals are projected intothe target language in a different word order, leavingmonotone mappings to be handled by the glue rulesas previously.
This choice helps keep the searchspace more manageable, and also avoids spuriousambiguity.
In addition, we disallow rules in whichnonterminals are adjacent on both the source and tar-get sides, by imposing the non adjacent nonterminalconstraint on the target side whenever the constraintis relaxed on the source side.
This forces any non-monotone reorderings to always be grounded in lex-ical evidence.
We refer to the permitted subset ofXX rules as XX-nonmono rules.The second challenge involves modeling: intro-ducing XX rules places them in competition withthe existing glue rules.
In particular, these two kindsof rules try to model the same phenomena, namelythe translations of phrases that appear next to eachother.
However, they differ in terms of the featuresassociated with the rules.
XX rules will be asso-ciated with the same features as any other hierar-chical rules, since they are all learned via an iden-tical training method.
In contrast, glue rules areintroduced into the grammar in an ad hoc manner,and the only feature associated with them is a ?gluepenalty?.
These distinct feature sets makes directcomparison of scores unreliable.
As a result the de-coder may simply prefer to always select glue rulesbecause they are associated with fewer features re-sulting in adjacent phrases always being translatedin a monotone order.
To address this issue, we in-troduce a new model, which we call the target-sidefunction words orientation-based model, or simplyPorit , which evaluates the application of the twokinds of rules on the same context, i.e.
for our ex-ample, it is the function word?
(at).4 Target-side Function WordsOrientation-based ModelThe Porit model is motivated by the function wordsreordering hypothesis (Setiawan et al, 2007), whichsuggests that function words encode essential infor-mation about the (re)ordering of their neighboringphrases.
In contrast to Setiawan et al (2007), wholooked at neighboring contexts for function wordson the source side, we focus here on modeling theinuence of function words on neighboring phrases350on the target side.
We argue that this focus better tsour purpose, since the phrases that we want to modelare the function words' neighbors on the target side,as illustrated in Fig.
1.To develop this idea, we rst dene an orit func-tion that takes a source function word as a refer-ence point, along with its neighboring phrase on thetarget side.
The orit function outputs one of thefollowing orientation values (Nagata et al, 2006):Monotone-Adjacent (MA); Reverse-Adjacent (RA);Monotone-Gap (MG); and Reverse-Gap (RG).
TheMonotone/Reverse distinction indicates whether thesource order follows the target order.
The Ad-jacent/Gap distinction indicates whether the twophrases are adjacent or separated by an interveningphrase on the source side.
For example, in Fig.
1,the value of orit for right neighbor Eastern divisionwith respect to function word?
(at) is MA, since itscorresponding source phrase ?\??
is adjacentto?
(at) and their order is preserved on the Englishside.
The value for left neighbor rank 10th with re-spect to?
(at) is RG, since ??
? is separatedfrom ?
(at) and their order is reversed on the En-glish side.More formally, we dene Porit(orit(Y,X)|Y ),where orit(Y,X) ?
{MA,RA,MG,RG} is the ori-entation of a target phrase X with a source functionword Y as the reference point.1We estimate the orientation model us-ing maximum likelihood, which involvescounting and normalizing events of interest:(Y, o = orit(Y,X)).
Specically, we estimatePorit(o|Y ) = C(Y, o)/C(Y, ?).
Collecting trainingcounts C(Y, o) involves several steps.
First, werun GIZA++ on the training bitext and apply the?grow-diag-nal?
heuristic over the training datato produce a bi-directional word alignment.
Then,we enumerate all occurrences of Y and determineorit(Y,X).
To ensure uniqueness, we enforcethat neighbor X be the longest possible phrasethat satises the consistency constraint (Och andNey, 2004).
Determining orit(Y,X) can then bedone in a straightforward manner by looking at themonotonicity (monotone or reverse) and adjacency(adjacent or gap) between Y 's and X .1In fact, separate models are developed for left and rightneighbors, although for clarity we suppress this distinctionthroughout.MT06 MT08baseline 30.58 23.59+itg 29.82 23.21+XX 30.10 22.86+XX-nonmono 30.96 24.07+orit 30.19 23.69+XX-nonmono+orit 31.49 24.73Table 1: Experimental results where better than baselineresults are italicized, and statistically signicant better(p < 0.01) are in bold.5 ExperimentsWe evaluated the generalization of Hiero to includeXX rules on a Chinese-to-English translation task.We treat the N = 128 most frequent words inthe corpus as function words, an approximation thathas worked well in the past and minimized depen-dence on language-specic resources (Setiawan etal., 2007).
We report BLEU r4n4 and assess signi-cance using the standard bootstrapping approach.We trained on the NIST MT06 Eval corpus ex-cluding the UN data (approximately 900K sentencepairs), segmenting Chinese using the Harbin seg-menter (Zhao et al, 2001).
Our 5-gram languagemodel with modied Kneser-Ney smoothing wastrained on the English side of our training data plusportions of the Gigaword v2 English corpus.
Weoptimized the feature weights using minimum er-ror rate training, using the NIST MT03 test set asthe development set.
We report the results on theNIST 2006 evaluation test (MT06) and the NIST2008 evaluation test (MT08).Table 1 reports experiments in an incrementalfashion, starting from the baseline model (the orig-inal Hiero), then adding different sets of rules, andnally adding the orientation-based model.
In ourrst experiments, we investigated the introductionof three different sets of XX rules.
First (+itg),we simply add the ITG's inverted rule (Rule 6) tothe baseline system in an ad-hoc manner, similar tothe glue rules.
This hurts performance consistentlyacross MT06 and MT08 sets, which we suspect isa result of ITG rule applications often aggravatingsearch error.
Second (+XX), we permitted generalXX rules.
This results in a grammar size increase of25-26%, ltering out rules irrelevant for the test set,351and leads to a signicant performance drop, againperhaps attributable to search error.
When we in-spected the rules, we observe that the majority ofthese rules involve spurious word insertions.
Third(+XX-nonmono), we introduced only XX-nonmonorules; this produced only a 5% additional rules, andyielded a marginal but consistent gain.In a second experiment (+orit), we introducedthe target-side function words orientation-basedmodel.
Note that this experiment is orthogonal to therst set, since we introduce no additional rules.
Re-sults are mixed, worse for MT06 but better (with sig-nicance) for MT08.
Here, we suspect the model'spotential has not been fully realized, since Hieroonly considers monotone reordering in unseen cases.Finally, we combine both the XX-nonmono rulesand the Porit model (+XX-nonmono+orit).
Thecombination produces a signicant, consistent gainacross all test sets.
This result suggests that the ori-entation model contributes more strongly in unseencases when Hiero also considers non-monotone re-ordering.
We interpret this result as a validationof our hypothesis that carefully relaxing the non-adjacent constraint improves translation.6 Discussion and Future WorkTo our knowledge, the work reported here is therst to relax the non-adjacent nonterminals con-straint in hierarchical phrase-based models.
The re-sults conrm that judiciously adding rules to a Hierogrammar, adjusting the modeling accordingly, canachieve signicant gains.Although we found that XX-nonmono rules per-formed better than general XX rules, we believe thelatter may nonetheless prove useful.
Manually in-specting our system's output, we nd that the outputis often shorter than the references, and the missingwords often correspond to function words that aremodeled by those rules.
Using XX rules to modellegitimate word insertions is a topic for future work.AcknowledgmentsThe authors gratefully acknowledge partial supportfrom the GALE program of the Defense AdvancedResearch Projects Agency, Contract No.
HR0011-06-2-001.
Any opinions, ndings, conclusions orrecommendations expressed in this paper are thoseof the authors and do not necessarily reect theviews of the sponsors.ReferencesDavid Chiang, Yuval Marton, and Philip Resnik.
2008.Online large-margin training of syntactic and struc-tural translation features.
In Proceedings of the 2008Conference on Empirical Methods in Natural Lan-guage Processing, pages 224?233, Honolulu, Hawaii,October.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofthe 43rd Annual Meeting of the Association for Com-putational Linguistics (ACL'05), pages 263?270, AnnArbor, Michigan, June.
Association for ComputationalLinguistics.Arul Menezes and Chris Quirk.
2007.
Using dependencyorder templates to improve generality in translation.In Proceedings of the Second Workshop on StatisticalMachine Translation, pages 1?8, Prague, Czech Re-public, June.
Association for Computational Linguis-tics.Masaaki Nagata, Kuniko Saito, Kazuhide Yamamoto,and Kazuteru Ohashi.
2006.
A clustered global phrasereordering model for statistical machine translation.In Proceedings of the 21st International Conferenceon Computational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguistics,pages 713?720, Sydney, Australia, July.
Associationfor Computational Linguistics.Franz Josef Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine transla-tion.
Computational Linguistics, 30(4):417?449.Hendra Setiawan, Min-Yen Kan, and Haizhou Li.
2007.Ordering phrases with function words.
In Proceed-ings of the 45th Annual Meeting of the Associationof Computational Linguistics, pages 712?719, Prague,Czech Republic, June.
Association for ComputationalLinguistics.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?404, Sep.Tiejun Zhao, Yajuan Lv, Jianmin Yao, Hao Yu, MuyunYang, and Fang Liu.
2001.
Increasing accuracy ofchinese segmentation with strategy of multi-step pro-cessing.
Journal of Chinese Information Processing(Chinese Version), 1:13?18.Andreas Zollmann and Ashish Venugopal.
2006.
Syntaxaugmented machine translation via chart parsing.
InProceedings on the Workshop on Statistical MachineTranslation, pages 138?141, New York City, June.
As-sociation for Computational Linguistics.352
