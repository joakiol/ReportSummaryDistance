Parsing with Dependency Relationsand Robust ParsingJacques Courtin, Damien GenthialCLIPS - IMAG CAMPUSBP 5338040 GRENOBLE CEDEX 9Phone: +33 476 51 49 15E-Mail :  Jacques.Courtin@imag.fr, Damien.Genthial@imag.frAbst rac tAfter a short recall of our view of dependencygrammars, we present wo dependency parsers.The first uses dependency relations to have amore concise expression of dependency rulesand to get efficiency in parsing.
The seconduses typed feature structures to add some se-mantic knowledge on dependency trees andparses in a more robust left to right manner.1.
IntroductionOur team has been working with dependencygrammars for more than twenty-five years(Courtin 73).
Two dependency parsers built byour team are presented in this paper.
The firstone uses the notion of dependency relations inorder to implement dependency grammars effi-ciently; it is described in the first part of the text.The second one was built with the followingobjectives: adding the use of some semanticknowledge in the process of syntactic parsingand obtaining a robust parser (second part ofthe text).2.
Parsing with dependency re lat ionsThe linguistic model we use for dependency isinspired by the Tesni&e model (Tesni~re 59),which we will recall shortly in order to defineprecisely our terminology.2.1.
The linguistic modelRelationship between words is the fundamentalconcept associated with dependency structures(DS).
Given two words of the language, a rela-tion is established between them, defining adominated word (or dependent) and a domi-nating word (or governor).
This relation can berepresented by an arc between two nodes, whereeach node is labelled by a word.
The arc de-scends from the governor to the dependent.Example: the dependency structure for the sen-tence (, we present two parsers ,~:/ presentwe ~ parsersWe can also use a linear notation with bracketsand write: (we) present ((two) parsers).But the graphical representation is more read-able and shows clearly the hierarchy between thegovernor and its dependents, which of course,can also have dependents.Dependency grammarsA dependency grammar (formalism used by(Hays 64)) on a vocabulary V is made of:?
a family of parts Ci of V such that the unionof Ci is equal to V.?
a set of rules, each having one of the twofollowing forms:i) * (X)ii) X ( XI ... Xi * Xi+l ... Xn )Ci are word classes or lexico-syntactic categoriesand are denoted by their name (Determiner,Noun, Adjective,...).
Xi in the rules above arecategory names.The star shows the place of the governor rela-tively to its dependents, so in a type ii) rule,X l...Xi are left dependents of the X governorand Xi+l ... Xn are its right dependents.When n = 0, the rule is written X(*) and is aterminating rule; type i)rules are initial rules.Grammar example:We use the following categories: Determiner (D),Noun (N), Adjective (A), Verb (V).?
(V)V(N, *,N)N(D,*) N(D,A, *) N(A, *)D(*) V(*) A(*) N(*)95V={drinks,  eats) D={the, a}N={dog, cat, cup, milk)A=tblack, white, hot}With this grammar one can build the structure:d r inks  /cat :  ~mi lkhot:GenerationDependency grammars are generative, workingwith the following generating rules:a) choose a type i) rule (which determines themain governor),b) choose and apply type ii) rules until we ob-tain a complete structure, entirely made ofterminating rules.With the example grammar above, we can makethe following derivation (which matches thesentence: ~ the black cat drinks hot milk ,0:* (V)* (V(N, *,N) )* (V(N(D,A,*) ,*,N) )* (V(N(D,A,*), *,N(A,*) ) )* (V(*) (N(D,A,*),*,N(A,*))  );ivi;.i cNc*tN(*) (A(*),*)))Remark:For a given governor, the dependency grammarmust contain as many rules as there are possibleconfigurations of dependents below this gover-nor.
For example, if we want nominal phraseswith at least a noun, an optional determiner and0,1 or 2 adjectives before the noun, we will havethe grammar:N(*) N(A,*) N(A,A,*)N(D, *) N(D,A, *) N(D,A,A, *)The formalism proposed below shows a betterway to describe the same things.2.2.
Dependency  re la t ionsThe method used in the PILAF !
system (Courtin77) to build dependency structures is a directanalysis: we transform the input word chain in adependency tree by using a form of depend-ency grammar and no intermediate structure.But the algorithm does not directly use Tesni~reIprocfdures Interactives Linguistiques Appliqufes auFran~ais (Interactive Linguistic Procedures Applied toFrench)96type dependency grammars because, as we seenbefore, these grammars impose a combinatorialdescription of all the possible configurations ofdependents for a given governor.
To overcomethis drawback, we introduce dependency rela-tions between two lexico-syntactic categories.Example:To say that N governs the g we simply writeN -> JtDependency Relations (DR) must not only codethe relation itself but also:?
the relative positions of the dependent andthe governor: is it a left dependent or a rightone ??
the relative positions of all dependents of agiven governor.Example:We want to describe the sentence ,~ The black catdrinks hot milk ,~ which gives the sequence ofcategories:DANVANand the dependency tree given above.Dependency relations must stipulate that a nouncan appear on the left or on the fight below averb and that below a noun, the determiner pre-cedes the adjective.
So we attach to each relationan vector of integers (either positive or negative)and we write:GOUV-> DEP := (x I ..... Xn),which says that we can have 0,1...n dependentsof the DEP category below the governor of theGOV category.The integers are presented in ascending order,showing the relative position of DEP belowGORY.
For any given governor, the integer valuesalso determine the relative positions of all itsdifferent possible dependents.Example:N -> A := (-14, -15)N -> D := (-16)V -> N := (-20, + 20)Positive integers concern right dependents andnegative integers left dependents.
The integer ofthe second relation stipulate that the determiner,if any, will be placed before the adjectives, be-cause -16 is less than -15 and -14.
From the firstrelation we can see that no word can be placed,below the noun, between the two adjectives(there is no integer between - 15 and -14).These relations can be drawn as the followingtrees:/v\N N D A A-20 +20 -16 -15 -14An important thing to be noted is that each inte-ger position gives the possibility for a dependentto be present at that position, but never imposesthat presence.So the three relations above are equivalent to thefollowing dependency grammar:N(A,*) N(A,A,*)N(D,*) N(D,A,*) N(D,A,A, *)V(N,*) V(*,N) V(N,*,N)N(*) A(*) D(*) V(*)It can be noted that these relations are in somesense similar to disjunctive forms of Sleator'slink grammars (Sleator and Temperley 91).2.3.
Pars ing a lgor i thmThis algorithm supposes that the morphologicalstep is finished and that it has produced the se-quence of lexico-syntactic categories for theinput sentence, ach word corresponding to onecategory - or several if the word is ambiguous.So the parser's inputs are:?
the sequence XI...Xn of categories computedby the morphological parser;?
the set of dependency relations and the asso-ciated integer vectors.We add to the Xi sequence the pseudo categorySI~'T=X0 which will help in determining thepossible governor of the sentence (to initiate theparsing process).
If, for example, possible maingovernors of a sentence are coordination con-junction (el and verb, we will have the relations:SENT -> V := (+i)SENT-> C := (+I)As we can have only one governor for a sen-tence, these two relations are mutually exclusive.This is expressed by the value of the integer: +1,which is the same for the two possible depend-ents of SENT.In order to build the dependency tree (or trees)associated with the given sequence of categories,the parser first initializes the square array offigure 1.As (Sleator And Temperley 91), we only wantprojective structures (or planar structures), i.e.trees which can be traversed by a left to rightinfix algorithm to find the original linear orderof the sentence.
The motivations for this limita-tion to projective structures are the following:97it is important to be able to retrieve, from thetree, the original linear form of the sentence;this limitation leads to greater parsing effi-ciency: for each governor, the search for itsdependents will be made in two separatespaces: a left and a right space.OOV Xo Xx Xa XNDEP SENTgo ~ POl Pc~ PortSENTix ,  P~o ?
P~ P~xa ~ ~z~ o ~znPij: set of integers determined by the relationsXj -> X i.PiF 0Figure l : Square array fo r a sentenceSo for a given governor Xj, all its left depend-ents must have an index i < j (the index ordermatches the word order in the sentence).
Thesame is true for right dependents, with index k >j.
So we can remove from the top-right triangleof the array all positive numbers and from thebottom-left riangle all negative ones.
We thenhave the two properties:?
1< i,j < n, i > j, if Pij ~ 0 thenVpe  P i j ,wehave p>O.?
1< i, j < n, i <j,  ifPij ~ 0 thenVp?
Pij, we have p<O.After having initialized the array and removeduseless parts of it, the parser builds, with a de-scendant and recursive algorithm, all depend-ency structures compatible with the array:a) For each possible governor of the sentence(SENT column):b) build all left sub-trees and all right sub-trees (rocursively);c) build the final structures by mergingthe partial fight and left ones.One can say that we catch the SENT categoryand ,~ pull ~, the structures out of the array.
Thealgorithm succeeds if at least one ~ pulled ~,structure contains all the words of the inputsentence.With real sentences, of course, we have lexicalambiguities or structural ambiguities.
In bothcases, the algorithm is non-deterministic andbuilds all possible solutions by blind combinato-rial enumeration.Dependency relations, associated with the algo-rithm described above, constitute a grammaticalmodel with very few constraints.
We can quicklystate that the parser will succeed on more sen-tences than the language sentences.
This featurecan be viewed as an advantage in the frameworkof a man-machine communication system,where the essential quality of an utterance is tobe interpretable, ven if it is not syntacticallycorrect: .C lose file),, for example, is un-grammatical but we can interpret it and executethe associated command.On the contrary, this lack of constraints is pe-nalizing efficiency: the algorithm will build a lotof incorrect structures because we can not statefor example, that a given governor must have atleast one dependent at that position, that a givenrelation only apply in a given context ....These limits and the necessary addition of somesemantic knowledge in the syntactic parsingprocess lead us to design the new method fordependency tree construction presented in thesecond part.2.4.
Conc lus ionDespite its relatively limited power of expres-sion, this parser builds dependency structuresextremely quickly (,<instantaneously)> on apersonal computer) as long as the input sentenceis not too long and not too ambiguous (saywhen the number of produced trees is less than20).The parser has been put to use in a system fordetection and correction of syntactic errors(Strube de Lima 90).
The main purpose was tocheck the numerous concordancy rules for gen-der, number and person in written French sen-tences.
For this type of application, it was ofcourse essential for the parser not to take intoaccount morphological properties of wordswhile building dependency structures.By its lack of constraints and its high practicalefficiency, this algorithm could be used in ap-plications for man-machine interfaces whereexchanges are short and language often ap-proximative.3.
Robust  Pars ingThe use of the preceding parser in a system fordetection and correction of syntactic errors inFrench has raised the following problems:?
even for a simple task such as detection andcorrection of agreement errors in writtentexts, you need a powerful parsing mecha-nism able to determine, for example, the an-tecedent of a relative pronoun;?
a system for error correction can not rely onthe correctness of the inputs in order to builda structure which is essential to make a mini-mal work.
So you have to improve theknowledge of the system, i.e.
in our case, toadd some semantic information on words inorder to determine more precisely the rela-tions between them;?
the syntactic parser of a such system mustalso be robust and produce an output even ifthe input is completely ill-formed.These problems lead us to define a new depend-ency parser which will be able to manipulatesome semantic information and which will beerror resistant.
This work results in a prototypecalled CADET 2 of a dependency tree transducer,which we will describe in the following sections.3.1.
A l anguage for  wr i t ing depend-ency grammarsWe have attempted to design a language for thedescription of dependency structures retainingthe precision of Tesni~re's grammars, but moreappropriate for automatic treatment.
Our basicidea is that the governor-dependent relationshould not be expressed for two categories ingeneral, but for two words which are instancia-tions of these categories in a given sentence.
Wetherefore think it is necessary, when describing agovernor-dependent relation, to indicate thecontext in which the relation is valid.To build dependency structures, we must be ableto determine, for any two words, caracterized bytheir lexical category: determiner, noun, verb .
.
.
.
.which one governs the other.
More generally,given two dependency trees, we must know howto merge them into a unique tree.Example:N-  V ----~ "VD N N ND(D)N, V(N) -> ((D)N)V(N)We have defined a language based on rewritingrules; each rule applies to a dependency forest2 Constructeur d'Arbres de DEpendances Tyl~s (TypedDependency Trees Builder).98and produces a dependency tree.
A set of suchrules constitutes a dependency grammar whichcan be applied to a sentence by means of aninterpreter.
This interpreter is in fact a tree-transducer driven by the rules.Example of a simple rule: (the "-"  beginscomments)N_V \[ -- Name of the rule(I:{N), C0, SF:{pv))2:{V)) -- Forest=>( ( i, $F ) 2 ) \]--  Resulting treeThis rule applies to any forest which includes asequence of an N and a V, whose left dependentsare only preverbal particles pv.
It builds a newtree where the N is added as a dependent of theV.The advantage of these rules, compared to sim-ple binary relations, is that it is possible to ex-press the context of each category whichappears.
It is thus possible to restrict a governorto one or two dependents only, or to forbidmore than one occurrence of a given category ....One can also define linked pairs of binary rela-tions, as for coordination conjunctions (C):N_C\ [(I:{N), 2:{C), 3:(N))=>( ( 1 ) 2 ( 3 ))  \]On the other hand, they present he drawback ofthe primitive dependency grammars: there mustbe a rule for almost every pair of lexical catego-ries (LC).
To avoid this problem, we have cho-sen to use a hierarchy of LCs instead of theusual linear set of LCs (Genthial & al.
90).
Thishierarchy is a set, partially ordered by the i s -arelation (figure 2).We can, in this manner, express very generalrules like the two given above (NV and N_C)or more specific ones like:aux_~pas \[(i: {xbe; xhave), 2: {pastp})=>( (1 )2) \ ]By means ofis-a ((cnoun, pnoun}, N) andis-a ( {xbe, xhave, verb, pastp), V)relations, the N_V rule for instance may be ap-plied to all the following pairs of categories:(cnoun, xbe) (pnoun, xbe)(cnoun, xhave) (pnoun, xhave)(cnoun, verb) (pnoun, verb)(cnoun, pastp) (pnoun, pastp)We can thus define a set of basic categorieswhich describe words in a very specific way, anduse these categories fo r  lexical indexing.
Thecategories can then be grouped in ,~ meta-categories ~according to the structures we wantto build.
Finally, we can write the rules whicheffectively build these structures.We can also write grammars in an incrementalfashion, starting with the highest categories (e.g.N, V, A, C, P) then testing the rules on our cor-pus, progressively adding more precise rules forthe lowest categories to treat specific phenom-ena.So, by using this method, we can avoid the usualcompromise between a very fine set of LCs(which multiplies morphological ambiguitiesand syntactic rules) and a very general set(which multiplies yntactic ambiguities).
We alsoobtain a fairly robust syntactic parsing: all un-known words are given the most general cate-gory (CLS), to which any rule can apply, thusan unknown word does not stop the parsingprocess.Similar type hierarchies have already been usedin work on language semantics to represent hetaxonomy of semantic types.
We shall thereforeuse the same formalism for the representation ofsyntactic and semantic knowledge (see ?3.3).N V A/on ocnoun ~xha~vVerb pastp adjWe use the following abbreviations: cnoun and pnounfor common and proper nouns, xbe and xhave for theauxiliaries be and have, pastp for past participle, adj foradjective, P for preposition and C for coordinationconjunction.Figure 2: Example of hierarchy3.2.
Building dependency s t ructuresGiven a set of rewriting rules, the tree transducerproceeds by a left to right scanning of the inputtext.
Each time a word is recognized by themorphological parser, it is transmitted to thesyntactic module which includes it in the currentstate of the analysis.
As the data manipulated bythe tree transducer must be trees or forests, eachword is transformed in a one node tree, wherethe root bears the information associated to theword.In order to manage multiple interpretations ofthe same word or of the same sentence, thetransducer maintains a list of forests where each99t, is of,he.entente.These forests, which are the current state of theanalysis, are called stacks because each time anew word is recognized, a one node tree isI pushed on each forest and the parsing always _ _resumes on the top of each forest.Given a list of stacks, the transducer applies eachI applicable rule to the top of all stacks and eachtime a rule applies, a new stack is produced andadded at the end o f  the list.
Doing so, the trans-ducer will also apply the rules to the new stacksI produced, cyclically.
If more than one rule ap-plies to a particular stack, more than one stackwill be produced, but if at least one rule appliesto a stack, this stack will be removed from the DI list.Example: (adapted from French)We consider only four categories: D,A, N, V (forI determiner, adjective, noun and verb) and wegive the following very simple rules:D_N \[(I:{D}, 2:{N}) => ((I) 2) \] N ,Nl A_N \[(I:{A), 2:{N}) => ((i) 2) \] D"N_A \[(I:{N}, 2:{A)) => (i (2)) \] DN_V 2:(V)) => 2) \] / /N  /N  ,4V_N \[(I:{V), 2:(N}) => (I (2)) \] D A D An Figure 3 shows the evolution of the list of stacksduring the parsing of the French nominal Figure 3: Stacks evolutionphrase: ,~ la belle ferme ,, which is ambiguousand leads to the following sequence of catego- Our example gives three correct structures:I ties: - (la)belle(ferme) the firm beauty((la)belle)ferme the beauty closesDIA~N~ (la,belle)ferme the beautiful farmI The algorithm is guaranted to stop because we &l LV JWe first introduce the word a la ,~ as a one node have added a constraint: rewriting rules aretree bearing the D category.
As no rule can ap- written in such a way that the length of a stackmust reduce each time a rule is applied to it.
A i ply to this tree, we then introduce the word,, belle ,, which is ambiguous.
The ambiguity detailed discussion of termination and angives two forests which are described on list (1).
evaluation of the algorithm can be found inThe D N rule applies to this list and gives list (2).
(Genthial 91).N Introducing the word ferme leads to list 3.3. h ie rarch ies  (3), Type  (( ))on which we detail rule application.
So the rule We have chosen to represent knowledge aboutA_N applied to the second stack of the list pro- words and trees with a unique formalism: ~P-N duces a new forest (or stack) which is appendedto the list.
When the transducer ends with the terms (Ait-Kaci 84).
~P-terms are typed featuresoriginal list, it finds the new produced stacks and structures which permit the description of typesproceeds with them, applying grammar rules.
(in the sense of classical programming lan-guages uch as Pascal), i.e.
sets of values.
I The D_N rule will then be applied to the newproduced forest (D, (A)N).
The process stops Example:when the transducer reaches the end of the list UL(lex => "eats';and, after removing the stacks where a rule has cat  => verb ;applied, we obtain list (4).
sub j  => UL(sem => S:AN'DIA'I~) ;A correct interpretation (according to a given obj  => UL(s~-'ta => O:EATABI~)  ;grammar) of the input sentence can be found in sere => /2qGEST(agent => S;each stack which contains exactly one tree: this pat ient  => O))A tree is a dependency structure of the sentence.100The use of reference tags like s or 0 allowsstructure sharing, so W-terms are not trees butgraphs.Simple types are defined in the signature whichis a set partially ordered by the i s -a  relation.This order is extended to W-terms by the uniqueoperation used to manipulate them: unification.The unification of two simple types is defined asthe set of lower bounds of these two types (inthe i s -a  relation).
Unification allows implicitinheritance of properties, and can be efficientlyimplemented (Ai't-Kaci & al.
89).In our parser, a W-term is attached to each nodeof a tree and to transduction rules we haveadded expressions which enable us to test andmodify those ~l'-terms.
We can thus simultane-ously build a syntactic structure (dependencytree) and a semantic structure (W-term, whichalso contains morphological and syntacticalinformation), and which is built by unification(see also (Hellwig 86) on the use of unificationfor dependency parsing).Example of rules and application:We have two words:UL( lex  => "dog" ;cat  => cnoun;sere => CANINE)UL  (lex => "eats" ;cat  => verb;subj => UL(sem => S :ANIMATE) ;obj => UL(sem => O:EATABLE) ;sem => INGEST(agent  => S;pat ient  => O))and the rule:sub jec t  \[ (i: {N}, 2:{V})/Un i f  (i, 2. subj) / -- Cond i t ions=>( (1 )2) ;ASS IGN (2. subj, i) ; \] -- Ac t ionsThe root of the resulting tree is decorated by:UL( lex  => "eats" ;cat  => verb;subj => UL( lex  => "dog';ca t  => cnoun;sere => S :CANINE) ;obj => UL(sem => O:EATABLE) ;sem => INGEST(agent  => S;pat ient  => O))3.4.
Conc lus ionThe use o f  a category hierarchy simplifies thewriting of the rules and introduces a way ofmanipulating unknown words which is not partof the mechanisms of the system but which isintegrated in the objects it manipulates.
We canthen write rules without thinking about ill-formedness (i.e.
it is not necessary to make therules tolerant because the tolerance is implicit inthe system).More generally, the use of unification in con-junction with dependency parsing allow to buildsyntactic structures efficiently while having thepFossibility to make very fine descriptions with-terms.ReferencesHassan Ait-Kaci (1984).
A Lattice.Theoretic Approachto Computation Based on a Calculus of Partially-Ordered Type Structures.
Ph.D., University of Penn-sylvania 1984.Hassan Air Kaci et al (1989).
Efficient implementa-tion of Lattice Operations.
ACM Transactions onProgramming Languages and Systems 11:1, pp.
116-146.Jacques Courtin (1973).
Un analyseur syntaxique inter-actif pour la communication heroine.machine.
IntlConference of Computational Linguistics, Pise, Italy,August 1973, Vol.
I.Jacques Courtin (1977).
Algorithmes pour le traitementinteractif des langues naturelles.
Th~se d'~tat, GrenobleI, Octobre 1977.Damien Genthial, Jacques Courtin et Irene Kowarski(1990).
Contribution of a Category Hierarchy to theRobustness of Syntactic Parsing.
13th CoLing, Hel-sinki, Finland, August 1990, Vol.
2, pp 139-144.Damien Genthial (1991 ).
Contribution ~ la constructiond'un syst~me robuste d'analyse du franfais.
Th~se, Uni-versit~ Joseph Fourier, 10janvier 1991.D.
Hays (1964).
Dependency theory : a formalism cowlsome observations.
Language 40, pp.
511-525.Peter Heliwig (1986).
Dependency Unification Gram-mar.
1 lth CoLing, Bonn, FRG, August 1986, pp 195-198.Daniel Sleator et Davy Temperley (1991).
ParsingEnglish with a Link Grammar.
Technical Report CMU-CS-91-196, School of Computer Science, Pittsburgh,October 1991.V~xa Lucia Strube de Lima (1990).
Contributionr~tude du traitement des erreurs au niveau lexico-syntaxique darts un texte ~crit en franfais.
Th~se, Uni-versit~ Joseph Fourier, Mars 1990.Lucien Tesni~:re (1959).
Eldments de syntaxe ~tu-rale.
Klincksiek, Paris101
