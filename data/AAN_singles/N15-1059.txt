Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 567?577,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsNASARI: a Novel Approach to a Semantically-AwareRepresentation of ItemsJos?e Camacho-Collados, Mohammad Taher Pilehvar and Roberto NavigliDepartment of Computer ScienceSapienza University of Rome{collados,pilehvar,navigli}@di.uniroma1.itAbstractThe semantic representation of individualword senses and concepts is of fundamentalimportance to several applications in Natu-ral Language Processing.
To date, conceptmodeling techniques have in the main basedtheir representation either on lexicographic re-sources, such as WordNet, or on encyclope-dic resources, such as Wikipedia.
We pro-pose a vector representation technique thatcombines the complementary knowledge ofboth these types of resource.
Thanks to itsuse of explicit semantics combined with anovel cluster-based dimensionality reductionand an effective weighting scheme, our repre-sentation attains state-of-the-art performanceon multiple datasets in two standard bench-marks: word similarity and sense clustering.We are releasing our vector representations athttp://lcl.uniroma1.it/nasari/.1 IntroductionObtaining accurate semantic representations of indi-vidual word senses or concepts is vital for severalapplications in Natural Language Processing (NLP)such as, for example, Word Sense Disambigua-tion (Navigli, 2009; Navigli, 2012), Entity Linking(Bunescu and Pas?ca, 2006; Rao et al, 2013), seman-tic similarity (Budanitsky and Hirst, 2006), Informa-tion Extraction (Banko et al, 2007), and resourcelinking and integration (Pilehvar and Navigli, 2014).One prominent semantic representation approach isthe distributional semantic model, which representslexical items as vectors in a semantic space.
Theweights in these vectors were traditionally computedon the basis of co-occurrence statistics (Salton et al,1975; Turney and Pantel, 2010; Dinu and Lapata,2010; Lappin and Fox, 2014), whereas for the morerecent generation of distributional models weightcomputation is viewed as a context prediction prob-lem, often to be solved by using neural networks(Collobert and Weston, 2008; Turian et al, 2010;Mikolov et al, 2013).
Unfortunately, unless theyare provided with large amounts of sense-annotateddata these corpus-based techniques cannot capturepolysemy in their representations, as they conflatedifferent meanings of a word into a single vector.Therefore, most sense modeling techniques tend tobase their computation on the knowledge obtainedfrom various lexical resources.
However, these tech-niques mainly utilize the knowledge derived fromeither WordNet (Banerjee and Pedersen, 2002; Bu-danitsky and Hirst, 2006; Pilehvar et al, 2013) orWikipedia (Medelyan et al, 2009; Mihalcea, 2007;Dandala et al, 2013; Gabrilovich and Markovitch,2007; Strube and Ponzetto, 2006), which are, re-spectively, the most widely-used lexicographic andencyclopedic resources in lexical semantics (Hovyet al, 2013).
This restriction to a single resourcebrings about two main limitations: (1) the sensemodeling does not benefit from the complementaryknowledge of different resources, and (2) the ob-tained representations are resource-specific and can-not be used across settings.In this paper we put forward a novel concept rep-resentation technique, called NASARI, which ex-ploits the knowledge available in both types of re-source in order to obtain effective representations ofarbitrary concepts.
The contributions of this paperare threefold.
First, we propose a novel technique567for rich semantic representation of arbitrary Word-Net synsets or Wikipedia pages.
Second, we provideimprovements over the conventional tf-idf weight-ing scheme by applying lexical specificity (Lafon,1980), a statistical measure mainly used for termextraction, to the task of computing vector weightsin a vector representation.
Third, we propose asemantically-aware dimensionality reduction tech-nique that transforms a lexical item?s representationfrom a semantic space of words to one of Word-Net synsets, simultaneously providing an implicitdisambiguation and a distribution smoothing.
Wedemonstrate that our representation achieves state-of-the-art performance on two different tasks: (1)word similarity on multiple standard datasets: MC-30, RG-65, and WordSim-353 similarity, and (2)Wikipedia sense clustering, in which our unsuper-vised system surpasses the performance of a state-of-the-art supervised technique that exploits knowl-edge available in Wikipedia in several languages.2 Semantic Representation of ConceptsLexical resources and concepts.
The gist of ourapproach lies in its combination of knowledge fromtwo different lexical resources: (1) the expert-based lexicographic WordNet, whose basic con-stituents are synsets, i.e., concepts expressed bysets of synonymous words (Miller et al, 1990),and (2) the collaboratively-constructed encyclopedicWikipedia, whose articles can be considered as indi-vidual concepts.
Throughout the paper, by a conceptwe mean a tuple b = (p, s) where p is a Wikipediapage and s is the corresponding WordNet synset.As a bridge between the two resources we use thesynset-to-article mappings provided by BabelNet1(Navigli and Ponzetto, 2012), a high coverage mul-tilingual encyclopedic dictionary and semantic net-work that merges, among other resources, Wikipediaand WordNet.
Note that the concept b can also con-tain a Wikipedia page or a WordNet synset only, if amapping is not provided by BabelNet.Semantic representation: NASARI.
Our conceptmodeling approach consists of two phases.
First,for a given concept, we collect a set of relevantWikipedia pages by leveraging the structural infor-mation in Wikipedia and WordNet (Section 2.1).1http://www.babelnet.org/Figure 1: The process of obtaining contextual informa-tion for a WordNet synset or a Wikipedia article.Then, we analyze the obtained contextual informa-tion and construct two vector representations of theconcept (Section 2.2).2.1 Collecting contextual informationFigure 1 illustrates the process of obtaining a set ofrelevant Wikipedia pages Tbas contextual informa-tion for a given concept b = (p, s).
Let Lpbe theset containing p and all the Wikipedia pages hav-ing an outgoing link to p, and Rsbe the set con-sisting of s and all other synsets that are in its di-rect neighbourhood.
We further enrich Rsby in-cluding the coordinate synsets of s and the relatedsynsets from its disambiguated gloss2.
Let B be afunction mapping each WordNet synset s?to its cor-responding Wikipedia page p, if such mapping ex-ists in BabelNet, and to the empty set otherwise.Hence, B(Rs) = ?s??RsB(s?).
Then, our con-textual information is the set of Wikipedia pagesTb= Lp?
B(Rs).
In the case either p or s is notpresent in the concept b, we take the contextual in-formation as Tb= B(Rs) or Tb= Lp, respectively.2.2 Vector constructionBy processing the collected contextual informationTb, NASARI represents the concept b as two vec-tors in two semantic spaces: (1) word-based and (2)synset-based.
LetWbbe the bag of words of all theWikipedia pages in Tbafter lemmatization and stop-2http://wordnet.princeton.edu/glosstag.shtml568word removal.
We use lexical specificity in order toextract the most representative words (Section 2.2.1)and synsets (Section 2.2.2) ofWb.Lexical specificity.
Lexical specificity (Lafon,1980) is a statistical measure that has been usedin a wide range of NLP applications, such as tex-tual data analysis (Lebart et al, 1998), term ex-traction (Drouin, 2003), and domain disambigua-tion (Camacho Collados et al, 2014).
However, toour knowledge, it has never heretofore been usedto calculate weights in a vector-based representa-tion (Turney and Pantel, 2010).
Lexical specificityis based on the hypergeometric distribution overword frequencies.
This statistical measure is partic-ularly suitable for extracting an accurate set of rep-resentative terms for a given subcorpus of a refer-ence corpus (Lafon, 1980).
Unlike the conventionalterm frequency-inverse document frequency weight-ing scheme (Jones, 1972, tf-idf ), lexical specificityis not especially sensitive to different text lengths.Assume a reference corpus of T words and a t-words subcorpus of that corpus.
The goal is to finda set of terms that are peculiar to the subcorpus,but not to the whole reference corpus.
Formally,given a word w that occurs f and k times in thecorpus and subcorpus, respectively, positive speci-ficity computes the relevance of w to the subcorpusas P (X ?
k) if k ?ftT, where X is a random vari-able following a hypergeometric distribution withparameters f , t and T , andftTis the expected valueof X .
In our setting we are only interested in the pos-itive specificity, i.e., the set of most relevant wordsappearing in the contextual information.
We ap-ply the standard procedure of applying log10andthen inverting the sign of the specificity probabili-ties in order to re-scale them to the real line, whichis more easily interpretable (Drouin, 2003; Cama-cho Collados et al, 2014).
We only retain wordswith specificity greater than two, which is equal to?log10(0.01).
This threshold leads to a set of repre-sentative words that are relevant to the context witha confidence of at least 99%, i.e., P (X ?
k) ?
0.01(Billami et al, 2014).2.2.1 Word-based representationThis word-based representation models the con-cept b in a conventional semantic space whose di-mensions are individual words.
We leverage lexicalspecificity to compute a weighted set of most repre-sentative words forWbwith respect to the referencecorpus, i.e., the whole Wikipedia.
As an example,the obtained word-based vector for the edge of wa-ter sense of shore has water, ocean, lake, beach andsea among its most relevant dimensions.2.2.2 Synset-based representationGiven that the amount of contextual informa-tion gathered for a concept can be small, the re-sulting word-based vector can be sparse and as aconsequence prone to noise, especially in the caseof less frequent concepts.
Therefore, we put for-ward a method that tackles the issue, providing richsemantically-aware representations.
To this end,we group - and thereby generalize - similar dimen-sions in the obtained word-based vector, to producea smaller vector in which dimensions are WordNetsynsets and weights are computed on the basis of thecombined information of the individual words in thegroup.
The generalization procedure can be summa-rized in two main steps.First, for each word w in Wb, we obtain fromWordNet the set Hwof all the direct hypernymsof all the synsets containing w. For each synseth ?
Hwwe check whether there exists anotherword w?from the contextual information that is ahyponym of h, i.e., h ?
Hw?
Hw?.
When suchis the case, letting Yhbe the set of all words in thehyponym synsets of h, we combine w, w?and allthe other words in Yhinto a single dimension repre-sented by their common hypernym h. Thus for ourearlier example, the three words ocean, lake, and seaare grouped into a single dimension represented bytheir hypernym, i.e., the synset containing the bodyof water sense of water (water2nin WordNet 3.0)3.Then, we compute the weight associated with thenew dimension by calculating the lexical specificityof the word cluster.
Formally, we calculate the lex-ical specificity of h by setting the parameters k andf as the total number of times the words in Yhoccurin Wband the whole Wikipedia, respectively.
Thevalues of t and T remain unchanged.Our generalization procedure is similar to the di-mensionality reduction that is performed using sin-gular value decomposition in Latent Semantic Anal-ysis (Landauer and Dumais, 1997, LSA).
However,3We denote the ithsense of word w with POS p as wip.569LSA is not applicable to our setting because, dueto the usage of lexical specificity, our vectors arerelatively small in size and different vectors usuallyhave few overlapping dimensions.
Moreover, unlikeLSA, in which the size-reduced vectors have opaqueconflations of multiple words as their dimensions,our new semantic space has human- and machine-readable synsets as its dimensions.
Our general-ization procedure produces three advantages: (1) itmaps the vectors from a word-based semantic spaceto a lower-dimensional space of synsets, (2) whilemerging multiple words into a single synset an im-plicit disambiguation of context words takes place,providing better means for sense distinction, and(3) the dimensionality reduction tackles the potentialnoise and sparsity, resulting in smoother vectors.3 NASARI for Semantic SimilaritySo far we have explained how NASARI constructstwo types of representations, i.e., word-based andsynset-based, for arbitrary WordNet synsets andWikipedia pages.
In this section we provide amethod that leverages NASARI representations foreffective measurement of concept and word simi-larity.
Semantic similarity between a pair of lexi-cal items (e.g., words or concepts) lies at the coreof many applications in NLP and hence it has re-ceived a considerable amount of research interest,leading to a wide range of semantic similarity mea-sures (Mohammad and Hirst, 2012).3.1 Concept similarityGiven a pair of concepts, we first use the proceduredescribed in Section 2 to obtain for each conceptthe two corresponding vector representations, i.e.,word-based and synset-based.
For each representa-tion type, we then compute the similarity of the twoconcepts by comparing their corresponding vectors.This results in two similarity scores, one for eachrepresentation type.
The final similarity is computedas the average of the two similarity scores.
We useWeighted Overlap for comparing vectors.Weighted Overlap.
Proposed by Pilehvar et al(2013), Weighted Overlap (WO) first sorts the el-ements of each vector viand then harmonicallyAlgorithm 1 NASARI-based word similarityInput: words w1and w2Output: Sim, similarity score1: for each synonym set H ?
S2: if w1?
H & w2?
H then3: return Sim = 14: for each word wi?
{w1, w2}5: Cwi?
?, set of concepts associated with wi6: if wi?
WordNet & winot Named Entity then7: for each sense s ?
WordNet senses (wi)8: Cwi?
Cwi?
{s}9: else10: for each page p ?
piped links (wi)11: Cwi?
Cwi?
{p}12: Vi?
?, set of representations for concepts in Cwi13: for each concept c ?
Cwi14: vwrd?NASARI word-based rep. of c15: vsyn?NASARI synset-based rep. of c16: v ?
(vwrd, vsyn)17: Vi?
Vi?
{v}18: Sim?maxv?V1,v?
?V2WO(vwrd,v?wrd)+WO(vsyn,v?syn)219: return Simweights the overlaps between the two vectors:WO(v1, v2) =?q?O(r1q+ r2q)?1?|O|i=1(2i)?1(1)where O is the set of overlapping dimensions be-tween the two vectors and rjqis the rank of dimen-sion q in the vector vj.
Given that our vectors aresignificantly smaller than those in the original set-ting of WO, the overlaps are also generally smallerin size.
Hence, we apply a square root operationto the computed value in order to obtain a moreuniformly-distributed range of scores across the sim-ilarity scale, i.e., [0, 1].
In our experiments we showthe advantage we gain by using WO in comparisonto the conventional cosine measure.3.2 Word similarityAlgorithm 1 shows the procedure we devised formeasuring semantic similarity between two words.There are three main steps:1.
Given a pair of words w1and w2the algorithmfirst checks whether they are synonymous ac-cording to our synonym set collection S. InSection 3.2.1, we explain how we obtain thisset.
If the words are defined as synonyms in S,570the algorithm returns the maximum similarityscore of one (lines 1-3).2.
If the words are not defined as synonyms, weproceed by obtaining, for each word wi, its setof possible senses (Cwi, lines 5-11).
We accord-ingly obtain the set of their respective NASARIvector representations (Vi, lines 13-17), two(word-based and synset-based) for each con-cept in Cwi.
Section 3.2.2 describes the conceptextraction process.3.
Finally, the algorithm returns the similarityscore Sim (line 19), calculated as the similarityof the closest senses of w1and w2.
In our de-fault setting, we linearly combine our two vec-tor representations by averaging them (line 18).3.2.1 Wiktionary synonyms SWiktionary is a rich collaboratively-constructedlexical resource that provides a considerable amountof multilingual lexical information for a large num-ber of words.
We use this resource in order to ob-tain sets of synonymous words S. To this end, wefirst extract all the pre-specified synonymy relationsin the English Wiktionary.
This results in 17K setswith an average size of 2.8 synonyms.In order to enrich the set we introduce a methodthat exploits the multilinguality of Wiktionary toextract synonymous words.
Our approach utilizestranslations of words in other languages as bridgesbetween synonymous words in English.
Specifi-cally, for each sense s of word w in Wiktionary, wefirst get al the available translations.
Assume thatthe sense s of w translates into the word tlin lan-guage l. If there is another word sense s?of anotherword w?in Wiktionary that is also translated to tlinlanguage l, we hypothesize that w and w?are syn-onyms.
In order to avoid ambiguity, as tlwe onlyconsider words that are monosemous according tolanguage l.This procedure results in around 9K additionalsynonymous sets with an average size of 2.1.
For in-stance, the Finnish noun ammatti, which is monose-mous according to Wiktionary, links seven Englishwords into a single set of synonyms: career, busi-ness, profession, occupation, trade, calling, and vo-cation.
The final synonym set collection S contains25K sets, each having, on average, 2.6 words.3.2.2 Concept extractionIf the two input words w1and w2are not found inthe same synonym set in S , we proceed by obtain-ing their sets of senses Cw1and Cw2, respectively.Depending on the type of wi, we use two differentresources for obtaining Cwi: the WordNet sense in-ventory and Wikipedia.WordNet words.
When the word wiis defined inthe WordNet sense inventory and is not a namedentity (line 6 in Algorithm 1), we set Cwias allthe WordNet synsets that contain wi, i.e., Cwi={synset s ?
WordNet : wi?
s}.
We use Stan-ford Named Entity Recognizer (Finkel et al, 2005)in our experiments.WordNet OOV and named entities.
For namedentities and words that do not exist in WordNet?svocabulary (OOV) we construct the set Cwiby ex-ploiting Wikipedia?s piped links (line 10 in Algo-rithm 1).
To this end, we take as elements of Cwithe Wikipedia pages of the hyperlinks which havewias their surface form, i.e., piped-links (wi).
If|Cwi| > 5, we prune Cwito its top-5 pages in termsof their number of ingoing links.
Our choice ofWikipedia as a source for named entities is due toits higher coverage in comparison to WordNet.4 ExperimentsWe evaluated NASARI on two different tasks thatrequire the computation of semantic similarity be-tween words or concepts: word similarity (Section4.1) and sense clustering (Section 4.2).4.1 Word similarity4.1.1 DatasetsWe took as benchmark for our word similarityexperiments three standard datasets that are widelyused in the literature: RG-65 (Rubenstein and Good-enough, 1965), MC-30 (Miller and Charles, 1991),and WordSim-353 (Finkelstein et al, 2002; Agirreet al, 2009).
WordSim-353 originally conflatedsimilarity and relatedness, leading to high similar-ity scores for pairs such as computer-keyboard de-spite the dissimilarity in their meanings.
To correctthe conflation, Agirre et al (2009) partitioned thedataset into two subsets: relatedness and similarity.Given that our similarity measure is targeted at se-mantic similarity, we took the similarity subset of571WordSim-353 (WS-Sim) as test bed for our evalua-tions.
The subset comprises 203 word pairs.4.1.2 Experimental setupIn this task, we assess the performance of dif-ferent systems in terms of Pearson correlation.
Wecompare our system against six similarity measuresthat have reported best performance on the threedatasets.
Lin (Lin, 1998) and ADW (Pilehvar et al,2013) are WordNet-based approaches that leveragethe structural information of WordNet for the com-putation of semantic similarity.
Most similar to ourwork are Explicit Semantic Analysis (Gabrilovichand Markovitch, 2007, ESA), which represents aword in a high-dimensional space of Wikipedia arti-cles, and Salient Semantic Analysis (Hassan and Mi-halcea, 2011, SSA), which leverages the linking ofconcepts within Wikipedia articles for generating se-mantic profiles of words.
Word2Vec (Mikolov et al,2013) and PMI-SVD are the best predictive and co-occurrence models obtained by Baroni et al (2014)on a 2.8 billion-token corpus that also includes theEnglish Wikipedia.4Word2Vec is based on neu-ral network context prediction models (Mikolov etal., 2013), whereas PMI-SVD is a traditional co-occurrence based vector wherein weights are cal-culated by means of Pointwise Mutual Information(PMI) and the vector?s dimension is reduced to 500by singular value decomposition (SVD).
We use theDKProSimilarity (B?ar et al, 2013) implementationof Lin and ESA in order to evaluate these measureson the WS-Sim dataset.4.1.3 ResultsTable 1 shows the Pearson correlation of the dif-ferent similarity measures on the three datasets con-sidered.
NASARI proves to be highly reliable onthe task of word similarity, providing state-of-the-art performance on RG-65 and MC-30, and compet-itive results on WS-Sim.
Importantly, the improve-ment we attain over measures that utilize as theirknowledge base either WordNet (i.e., ADW, Lin) orWikipedia (i.e., ESA and SSA) shows that our usageof the complementary information of the two typesof resource has been helpful.
We note that our Wik-tionary module detects four additional synonymouspairs (i.e., similarity = 1.0) in MC-30 (13%), eight inRG-65 (12%), and thirteen in WS-Sim (6%) that are4clic.cimec.unitn.it/composes/semantic-vectors.htmlMeasure RG-65 MC-30 WS-SimNASARI 0.91 0.91 0.74SSA 0.86 0.88 NAWord2Vec 0.840.83?0.76?Lin 0.83 0.82 0.66ADW 0.81 0.79 0.63PMI-SVD 0.740.76?0.68?ESA 0.72 0.74 0.45Table 1: Pearson correlation of different similarity mea-sures on RG-65, MC-30, and WordSim-353 similarity(WS-Sim) datasets.
Results for Lin and ESA on RG-65and MC-30 are taken from (Hassan and Mihalcea, 2011).We show the best performance obtained by Baroni et al(2014) out of 48 configurations specifically tested on RG-65 (highlighted by ) and across different datasets includ-ing WS-Sim (highlighted by ?
).not defined as synonyms in WordNet.
We also ob-tain competitive results according to the Spearmancorrelation (a setting in which the absolute similarityscores do not play a role and it is solely their rankingthat matters) on all the three datasets: MC-30 (0.89),RG-65 (0.88), and WS-Sim (0.73).WS-Sim is the only dataset on which we do notreport state-of-the-art performance.
An analysis ofthe outputs of our system on the WS-Sim dataset re-vealed that there are pairs in this subset of WordSim-353 that are not assigned proper scores accordingto the similarity scale.
Hill et al (2014) had previ-ously pointed out this deficiency of WS-Sim, mainlydue to its original relatedness-based scoring scale.For instance, word pairs that are barely related (e.g.,street-children) or antonyms (e.g., profit-loss andsmart-stupid) are assigned relatively high similar-ity values (respectively, 4.9 for the former and 7.3and 5.8 for the latter case, in the 0-10 scale).
Inall these cases our system produces more appropri-ate judgements according to the similarity scale.
Onthe other hand, there are highly similar pairs in thedataset with relatively low gold scores.
Examples in-clude school-center5and term-life6with the respec-tive gold similarity scores of 3.4 and 4.5, whereas5School and center have a pair of highly similar senses inWordNet 3.0: center3n: ?a building dedicated to a particularactivity?
and school2n: ?a building where young people receiveeducation.
?6Term and life are in coordinate synsets (with time period astheir common hypernym) in WordNet 3.0.572Measure System type 500-pair SemEvalNASARI unsupervised 84.6% 88.4%Dan-mono supervised 77.4% 83.5%Dan-multi supervised 84.4% 85.5%Baseline - 71.4% 82.5%Table 2: Accuracy of different systems on two manually-annotated English datasets for sense clustering inWikipedia.
Dan-mono and Dan-multi are the monolin-gual and multilingual systems of Dandala et al (2013).NASARI computes their similarities as 8.4 and 9.6.4.2 Sense clusteringOur second set of experiments focuses on sense clus-tering of the Wikipedia sense inventory.
Wikipediacan be considered as a sense inventory wherein thedifferent meanings of a word are denoted by the arti-cles listed in its disambiguation page (Mihalcea andCsomai, 2007).
Given the high granularity of thisinventory, clustering of senses can be highly bene-ficial to tasks that take this encyclopedic resourceas their sense inventory (Hovy et al, 2013), such asWikipedia-based Word Sense Disambiguation (Mi-halcea, 2007; Dandala et al, 2013).4.2.1 DatasetsFor the sense clustering task, we take as ourbenchmark the two datasets created by Dandala etal.
(2013).
In these datasets, clustering has beenviewed as a binary classification problem in whichall possible pairings of senses of a word are anno-tated whether they ought to be clustered or not.
Thefirst dataset contains 500 pairs, 357 of which areset to clustered and the remaining 143 to not clus-tered.
The second dataset, referred to as the Se-mEval dataset, is based on a set of highly ambigu-ous words taken from SemEval evaluations (Mihal-cea, 2007) and consists of 925 pairs, 162 of whichare positively labeled, i.e., clustered.4.2.2 Experimental setupIn this task we use the procedure explained in Sec-tion 3.1 for measuring the similarity of concepts.
Apair of pages is set to belong to the same clusterif their similarity exceeds the middle point in oursimilarity scale, i.e., 0.5 in the scale of [0, 1].
Wecompare our results with the state-of-the-art systemsof Dandala et al (2013) that perform clustering byexploiting the structure and content of an Englishpage (monolingual variant), or several pages in dif-ferent languages (multilingual variant that uses En-glish, German, Spanish and Italian pages).
Thesesystems are essentially multi-feature Support Vec-tor Machine classifiers that use an automatically-labeled dataset for their training.4.2.3 ResultsTable 2 lists the results of NASARI as well as thestate-of-the-art systems of Dandala et al (2013).
Wealso report the results for a baseline system that setsall pairs as not clustered.
As can be seen from the ta-ble, our system proves to be highly robust and com-petitive by outperforming, in an unsupervised set-ting, the supervised monolingual and multilingualsystems of Dandala et al (2013) on both datasets.As regards the F1, we obtain 72.0% and 64.2% onthe 500-pair and SemEval datasets, respectively, ameasure that is not reported by Dandala et al (2013).4.3 AnalysisRecall from Section 2 that our system has two vec-tor representations, for each of which we computevectors based on lexical specificity.
We also men-tioned in Section 3 that we opt for Weighted Overlapas our vector comparison method.
In order to ana-lyze the impact of each of these elements, we carriedout a series of experiments with the conventionallogarithmically-scaled tf-idf weighting scheme andthe cosine vector comparison technique.
For a wordw, we calculate the tf-idf by taking tf as the fre-quency of w in the corresponding contextual infor-mation, and idf = log(|D|/|{p ?
D : w ?
p}|),where D is the set of all pages in Wikipedia.Table 3 shows the performance of the NASARI-based similarity system and its individual vectorrepresentations for different weight computationschemes, i.e., lexical specificity and tf-idf, and fordifferent vector comparison techniques, i.e., cosineand WO, on word similarity and sense clusteringdatasets.
As can be seen from the Table, the perfor-mance of the word-based representation consistentlyimproves on both tasks when combined with the ad-ditional information from the synset-based vectors,demonstrating that the sense distinctions offered bythe generalization process have been beneficial.Between the two vector comparison methods,WO proves to better suit our specificity-based vec-573VectorrepresentationWeightingschemeVectorcomparisonWord similarity Sense clusteringMC-30 RG-65 WS-Sim 500-pair SemEvalCombinedspecificityWO?0.91?0.91?0.74?84.6%?88.4%cosine 0.88 0.89 0.75 76.2% 83.6%tf-idfWO 0.85 0.87 0.73 60.4% 67.8%cosine 0.79 0.84 0.70 81.4% 86.1%Word-basedspecificityWO 0.90 0.91 0.73 82.0% 85.0%cosine 0.86 0.88 0.72 73.2% 83.4%tf-idfWO 0.86 0.87 0.72 78.4% 82.6%cosine 0.83 0.87 0.71 79.2% 84.4%Synset-basedspecificityWO 0.91 0.90 0.75 78.8% 83.8%cosine 0.90 0.88 0.75 79.8% 85.0%tf-idfWO 0.86 0.85 0.73 37.2% 41.1%cosine 0.71 0.80 0.66 79.4% 85.0%Word-based specificity WO?0.86?0.87?0.71?80.0%?85.1%Table 3: Performance of NASARI and its individual vector representations for different weight computation schemes,i.e., lexical specificity and tf-idf, and for different vector comparison techniques, i.e., cosine and Weighted Overlap(WO), in terms of Pearson correlation (word similarity) and accuracy (sense clustering).
The scores highlighted by ?are the ones obtained using our default NASARI setting, and the ones highlighted by ?
correspond to the setting of oursystem using Wikipedia as its only knowledge source.tors by outperforming cosine in most cases.
Thereason behind the lower performance of WO for thesynset-based vectors on the task of sense clusteringcan be explained by the nature of the correspondingdatasets.
Since the synset-based vectors and theiroverlapping dimensions are small, their cosine sim-ilarity scores also tend to be relatively low, unlikeWO whose range of values is not affected by thenumber of overlapping dimensions.
Given that inthe experiments the threshold is fixed to the mid-dle point of the scale (cf.
Section 4.2.2), gener-ally low similarity values lead to a high-precision,low-recall system, which is rewarded by higher ac-curacy performance in datasets in which a large por-tion of instances are negative.
In fact, for the synset-based vector representation weighted using speci-ficity, the F1 performance of the cosine is signifi-cantly lower than WO.
On the SemEval dataset theF1 performance of WO is 60.1%, whereas cosine at-tains 37.1%.
Similarly, on the 500-pair dataset, WOleads cosine by 16.8%: 68.5% vs. 51.7%.As far as the weighting scheme is concerned, lex-ical specificity outperforms tf-idf on both tasks, ir-respective of the vector comparison technique andrepresentation.
We attribute the better performanceof lexical specificity to the probabilistic nature ofweights in its vectors.
The tf-idf weighting scheme,in contrast, suffers from insensitivity to the relativesize of the contextual information.
Thus, subse-quently, specificity-based vectors provide the advan-tage of accurately reducing the vectors?
dimension,unlike the tf-idf scheme in which the size-insensitiveweights are not comparable across vectors.
As a re-sult, the specificity-based vectors are substantiallysmaller in size, bringing about better space utiliza-tion and faster running time.
In our experiments thevectors obtained by using lexical specificity were,on average, almost nine times (2505 vs. 21825) andfour times (335 vs. 1311) smaller than the tf-idf -based vectors for the word-based and synset-basedvector representations, respectively.We were also interested in verifying the advantagegained by combining the complementary knowledgeof Wikipedia and WordNet.
To this end, we carriedout an experiment in which NASARI uses Wikipediaas its only knowledge source (i.e., without usingWordNet).
The last row in the Table (highlightedby ?)
shows the results for this setting.
Note thatsince WordNet is not used in this setting, we areconstrained to the word-based vector representationonly.
The results show that the combination of thetypes of resource leads to a consistent performance574improvement across tasks and datasets, with the av-erage improvement being 5%.5 Related WorkGiven that in this work we focused mainly on simi-larity for the evaluation of our semantic representa-tion, in addition to concept representation, we alsobriefly discuss related works for semantic similarity.Concept representation.
Distributional semanticmodels are usually the first choice for represent-ing textual items such as words or sentences (Tur-ney and Pantel, 2010).
These models have attractedconsiderable research interest, resulting in variousco-occurrence based representations (Salton et al,1975; Evert, 2005; Pado and Lapata, 2007; Erkand Pad?o, 2008) or predictive models (Collobert andWeston, 2008; Turian et al, 2010; Mikolov et al,2013; Baroni et al, 2014).
Although there havebeen approaches proposed in the literature for learn-ing sense-specific embeddings (Weston et al, 2013;Huang et al, 2012; Neelakantan et al, 2014), theircoverage is limited only to those senses that are cov-ered in the underlying corpus.
Moreover, the ob-tained sense representations are usually not linkedto any sense inventory, and therefore such linkinghas to be carried out, either manually, or with thehelp of sense-annotated data.
Hence, unless theyare provided with large amounts of sense-annotateddata, these techniques cannot furnish an effectiverepresentation of word senses in an existing standardsense inventory.Consequently, most sense modeling techniqueshave based their representation on the knowledgederived from resources such as WordNet (Mihal-cea and Moldovan, 1999; Agirre and Lopez, 2003;Agirre and de Lacalle, 2004; Pilehvar et al, 2013),or Wikipedia (Gabrilovich and Markovitch, 2007;Mihalcea, 2007).
None of these techniques, how-ever, combine knowledge from multiple types ofresource, making their representations resource-specific and also prone to sparsity.
In contrast,our method is based on the complementary knowl-edge of two different resources and their interlink-ing, leading to richer semantic representations thatare also applicable across resources.
Most similarto our combination of complementary knowledge isthe work of Franco-Salvador et al (2014) for cross-lingual document retrieval.Concept similarity.
Concept similarity tech-niques are mainly limited to the knowledge thattheir underlying lexical resources provide.
Forinstance, methods designed for measuring semanticsimilarity of WordNet synsets (Banerjee and Peder-sen, 2002; Budanitsky and Hirst, 2006; Pilehvar etal., 2013) usually leverage lexicographic or struc-tural information in this lexical resource.
Similarly,Wikipedia-based approaches (Hassan and Mihalcea,2011; Strube and Ponzetto, 2006; Milne and Witten,2008) do not usually benefit from the expert-basedlexico-semantic knowledge provided in WordNet.In contrast, our approach combines knowledge fromboth resources, providing two advantages: (1) moreeffective measurement of similarity based on richsemantic representations, and (2) the possibility ofmeasuring cross-resource semantic similarity, i.e.,between Wikipedia pages and WordNet synsets.6 ConclusionsIn this paper we presented a novel semanticapproach, called NASARI, for effective vectorrepresentation of arbitrary WordNet synsets andWikipedia pages.
The strength of our approachlies in its combination of complementary knowl-edge from different types of resource, while at thesame time it also benefits from an effective vec-tor representation with two novel features: lexi-cal specificity for the calculation of vector weightsand a semantically-aware dimensionality reduc-tion.
NASARI attains state-of-the-art performanceon multiple standard benchmarks in word similarityas well as Wikipedia sense clustering.
We releasethe representations obtained for all the Wikipediapages and WordNet synsets in http://lcl.uniroma1.it/nasari/.
As future work we plan to integrate NASARIinto BabelNet and apply our representation to a mul-tilingual setting, enabling the comparison of pairs ofconcepts across languages.
We also intend to useour approach on the task of multilingual Word SenseDisambiguation.AcknowledgmentsThe authors gratefully acknowledgethe support of the ERC Starting GrantMultiJEDI No.
259234.575ReferencesEneko Agirre and Oier Lopez de Lacalle.
2004.
Pub-licly available topic signatures for all WordNet nom-inal senses.
In Proceedings of LREC, pages 1123?1126, Lisbon, Portugal.Eneko Agirre and Oier Lopez.
2003.
Clustering Word-Net word senses.
In Proceedings of RANLP, pages121?130, Borovets, Bulgaria.Eneko Agirre, Enrique Alfonseca, Keith Hall, JanaKravalova, Marius Pas?ca, and Aitor Soroa.
2009.
Astudy on similarity and relatedness using distributionaland WordNet-based approaches.
In Proceedings ofNAACL-HLT, pages 19?27, Boulder, Colorado.Satanjeev Banerjee and Ted Pedersen.
2002.
An adaptedLesk algorithm for Word Sense Disambiguation usingWordNet.
In Proceedings of CICLing, pages 136?145,Mexico City, Mexico.Michele Banko, Michael J. Cafarella, Stephen Soderland,Matt Broadhead, and Oren Etzioni.
2007.
Open infor-mation extraction from the Web.
In Proceedings ofIJCAI, pages 2670?2676, New York, NY, USA.Daniel B?ar, Torsten Zesch, and Iryna Gurevych.
2013.DKPro similarity: An open source framework for textsimilarity.
In Proceedings of ACL: System Demonstra-tions, pages 121?126, Sofia, Bulgaria, August.Marco Baroni, Georgiana Dinu, and Germ?an Kruszewski.2014.
Don?t count, predict!
a systematic compari-son of context-counting vs. context-predicting seman-tic vectors.
In Proceedings of ACL, pages 238?247,Baltimore, Maryland.Mokhtar-Boumeyden Billami, Jos?e Camacho-Collados,Evelyne Jacquey, and Laurence Kister.
2014.
Seman-tic annotation and terminology validation in full scien-tific articles in social sciences and humanities (annota-tion s?emantique et validation terminologique en texteint?egral en shs) [in french].
In Proceedings of TALN2014, pages 363?376, Marseille, France.Alexander Budanitsky and Graeme Hirst.
2006.
Eval-uating WordNet-based measures of Lexical SemanticRelatedness.
Computational Linguistics, 32(1):13?47.Razvan Bunescu and Marius Pas?ca.
2006.
Using ency-clopedic knowledge for named entity disambiguation.In Proceedings of EACL, pages 9?16, Trento, Italy.Jose Camacho Collados, Mokhtar Billami, EvelyneJacquey, and Laurence Kister.
2014.
Approche statis-tique pour le filtrage terminologique des occurrencesde candidats termes en texte intgral.
In Journ?eesinternationales d?Analyse statistique des Donn?eesTextuelles, pages 121?133, Paris, France.Ronan Collobert and Jason Weston.
2008.
A unified ar-chitecture for natural language processing: Deep neu-ral networks with multitask learning.
In Proceedingsof ICML, pages 160?167, Helsinki, Finland.Bharath Dandala, Chris Hokamp, Rada Mihalcea, andRazvan C. Bunescu.
2013.
Sense clustering usingWikipedia.
In Proceedings of RANLP, pages 164?171,Hissar, Bulgaria.Georgiana Dinu and Mirella Lapata.
2010.
Measuringdistributional similarity in context.
In Proceedings ofEMNLP, pages 1162?1172, Massachusetts, USA.Patrick Drouin.
2003.
Term extraction using non-technical corpora as a point of leverage.
Terminology,9(1):99?115.Katrin Erk and Sebastian Pad?o.
2008.
A structured vec-tor space model for word meaning in context.
In Pro-ceedings of EMNLP, pages 897?906, Edinburgh, UK.Stefan Evert.
2005.
The statistics of word cooccur-rences: word pairs and collocations.
Ph.D. thesis,Universitt Stuttgart.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informationinto information extraction systems by gibbs sampling.In Proceedings of ACL, pages 363?370, Ann Arbor,Michigan.Lev Finkelstein, Gabrilovich Evgeniy, Matias Yossi,Rivlin Ehud, Solan Zach, Wolfman Gadi, and RuppinEytan.
2002.
Placing search in context: The conceptrevisited.
ACM Transactions on Information Systems,20(1):116?131.Marc Franco-Salvador, Paolo Rosso, and Roberto Nav-igli.
2014.
A knowledge-based representation forcross-language document retrieval and categorization.In Proceedings of the 14thConference on Europeanchapter of the Association for Computational Linguis-tics (EACL), Gothenburg, Sweden.Evgeniy Gabrilovich and Shaul Markovitch.
2007.
Com-puting semantic relatedness using Wikipedia-basedexplicit semantic analysis.
In Proceedings of IJCAI,pages 1606?1611, Hyderabad, India.Samer Hassan and Rada Mihalcea.
2011.
Semantic relat-edness using salient semantic analysis.
In Proceedingsof AAAI, pages 884,889, San Francisco, USA.Felix Hill, Roi Reichart, and Anna Korhonen.
2014.Simlex-999: Evaluating semantic models with (gen-uine) similarity estimation.
arXiv:1408.3456.Eduard H. Hovy, Roberto Navigli, and Simone PaoloPonzetto.
2013.
Collaboratively built semi-structuredcontent and Artificial Intelligence: The story so far.Artificial Intelligence, 194:2?27.Eric H. Huang, Richard Socher, Christopher D. Manning,and Andrew Y. Ng.
2012.
Improving word representa-tions via global context and multiple word prototypes.In Proceedings of ACL, pages 873?882, Jeju Island,Korea.Karen Sp?arck Jones.
1972.
A statistical interpretation ofterm specificity and its application in retrieval.
Jour-nal of Documentation, 28:11?21.576Pierre Lafon.
1980.
Sur la variabilit?e de la fr?equence desformes dans un corpus.
Mots, 1:127?165.Thomas K. Landauer and Susan T. Dumais.
1997.
A so-lution to Plato?s problem: The latent semantic analysistheory of acquisition, induction, and representation ofknowledge.
Psychological Review, 104(2):211.Shalom Lappin and Chris Fox, editors.
2014.
Handbookof Contemporary Semantics ?
second edition.
Wiley-Blackwell, Malden, MA.Ludovic Lebart, Andre Salem, and Lisette Berry.
1998.Exploring textual data.
Kluwer Academic Publishers.Dekang Lin.
1998.
An information-theoretic definitionof similarity.
In Proceedings of ICML, pages 296?304,San Francisco, CA.Olena Medelyan, David Milne, Catherine Legg, andIan H. Witten.
2009.
Mining meaning fromWikipedia.
International Journal of Human-Computer Studies, 67(9):716?754.Rada Mihalcea and Andras Csomai.
2007.
Wikify!Linking documents to encyclopedic knowledge.
InProceedings of ACM on Information and Knowledgemanagement, pages 233?242, Lisbon, Portugal.Rada Mihalcea and Dan Moldovan.
1999.
An automaticmethod for generating sense tagged corpora.
In Pro-ceedings of AAAI, pages 461?466, Orlando, Florida,USA.Rada Mihalcea.
2007.
Using Wikipedia for auto-matic Word Sense Disambiguation.
In Proceedings ofNAACL-HLT-07, pages 196?203, Rochester, NY.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient estimation of word represen-tations in vector space.
In Workshop at InternationalConference on Learning Representations, Scottsdale,Arizona.George A. Miller and Walter G. Charles.
1991.
Contex-tual correlates of semantic similarity.
Language andCognitive Processes, 6(1):1?28.George A. Miller, R.T. Beckwith, Christiane D. Fell-baum, D. Gross, and K. Miller.
1990.
WordNet: anonline lexical database.
International Journal of Lexi-cography, 3(4):235?244.David Milne and Ian H. Witten.
2008.
An effective, low-cost measure of semantic relatedness obtained fromWikipedia links.
In Proceedings of the Workshopon Wikipedia and Artificial Intelligence: An EvolvingSynergy at AAAI-08, pages 25?30, Chicago, IL.Saif Mohammad and Graeme Hirst.
2012.
Distributionalmeasures of semantic distance: A survey.
CoRR,abs/1203.1858.Roberto Navigli and Simone Paolo Ponzetto.
2012.
Ba-belNet: The automatic construction, evaluation andapplication of a wide-coverage multilingual semanticnetwork.
Artificial Intelligence, 193:217?250.Roberto Navigli.
2009.
Word Sense Disambiguation: Asurvey.
ACM Computing Surveys, 41(2):1?69.Roberto Navigli.
2012.
A quick tour of Word SenseDisambiguation, induction and related approaches.
InSOFSEM 2012: Theory and practice of computer sci-ence, pages 115?129.
Springer.Arvind Neelakantan, Jeevan Shankar, Alexandre Pas-sos, and Andrew McCallum.
2014.
Efficient non-parametric estimation of multiple embeddings perword in vector space.
In Proceedings of EMNLP,pages 1059?1069, Doha, Qatar.Sebastian Pado and Mirella Lapata.
2007.
Dependency-based construction of semantic space models.
Compu-tational Linguistics, 33(2):161?199.Mohammad Taher Pilehvar and Roberto Navigli.
2014.A robust approach to aligning heterogeneous lexicalresources.
In Proceedings of ACL, pages 468?478,Baltimore, USA.Mohammad Taher Pilehvar, David Jurgens, and RobertoNavigli.
2013.
Align, Disambiguate and Walk: aUnified Approach for Measuring Semantic Similarity.In Proceedings of ACL, pages 1341?1351, Sofia, Bul-garia.Delip Rao, Paul McNamee, and Mark Dredze.
2013.
En-tity linking: Finding extracted entities in a knowledgebase.
In Multi-source, Multilingual Information Ex-traction and Summarization, pages 93?115.Herbert Rubenstein and John B. Goodenough.
1965.Contextual correlates of synonymy.
Communicationsof the ACM, 8(10):627?633.Gerard Salton, A. Wong, and C. S. Yang.
1975.
A vectorspace model for automatic indexing.
Communicationsof the ACM, 18(11):613?620.Michael Strube and Simone Paolo Ponzetto.
2006.WikiRelate!
computing semantic relatedness usingwikipedia.
In Proceedings of the 21st National Con-ference on Artificial Intelligence, pages 1419?1424,Boston, Massachusetts.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: A simple and general methodfor semi-supervised learning.
In Proceedings of ACL,pages 384?394, Uppsala, Sweden.Peter D. Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of semantics.Journal of Artificial Intelligence Research, 37:141?188.Jason Weston, Antoine Bordes, Oksana Yakhnenko, andNicolas Usunier.
2013.
Connecting language andknowledge bases with embedding models for relationextraction.
In Proceedings of EMNLP, pages 1366?1371, Seattle, Washington, USA.577
