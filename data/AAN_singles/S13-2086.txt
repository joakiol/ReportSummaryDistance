Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 520?524, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsCodeX: Combining an SVM Classifier and Character N-gram LanguageModels for Sentiment Analysis on Twitter TextQi Han, Junfei Guo and Hinrich Schu?tzeInstitute for Nature Language ProcessingUniversity of StuttgartStuttgart, Germany{hanqi, guojf}@ims.uni-stuttgart.deAbstractThis paper briefly reports our system for theSemEval-2013 Task 2: sentiment analysis inTwitter.
We first used an SVM classifier witha wide range of features, including bag ofword features (unigram, bigram), POS fea-tures, stylistic features, readability scores andother statistics of the tweet being analyzed,domain names, abbreviations, emoticons inthe Twitter text.
Then we investigated the ef-fectiveness of these features.
We also usedcharacter n-gram language models to addressthe problem of high lexical variation in Twit-ter text and combined the two approaches toobtain the final results.
Our system is robustand achieves good performance on the Twittertest data as well as the SMS test data.1 IntroductionThe challenge of the SemEval-2013 Task 2 (TaskB) is the ?Message Polarity Classification?
(Wilsonet al 2013).
Specifically, the task was to classifywhether a given message has positive, negative orneutral sentiment; for messages conveying both pos-itive and negative sentiment, whichever is strongershould be chosen.In recent years, text messaging and microblog-ging such as tweeting has gained its popularity.Since these short messages are often used not onlyto discuss facts but also to share opinions and sen-timents, sentiment analysis on this type of data haslately become interesting.
However, some featuresof this type of data make natural language process-ing challenging.
For example, the messages are usu-ally short and the language used can be very in-formal, with misspellings, creative spellings, slang,URLs and special abbreviations.
Some research hasalready been done attempting to address these prob-lems, to enable sentiment analysis on this type ofdata, in particular on Twitter data, and even to usethe outcome of sentiment analysis to make predic-tions (Jansen et al 2009; Barbosa and Feng, 2010;Bifet and Frank, 2010; Davidov et al 2010; Jiang etal., 2011; Pak and Paroubek, 2010; Saif et al 2012;Tumasjan et al 2010).As the research mentioned above, our system useda machine learning based approach for sentimentanalysis.
Our system combines results from an SVMclassifier using a wide range of features as well asvotes derived from character n-gram language mod-els to do the final prediction.The rest of this paper is organized as follows.
Sec-tion 2 describes the features used for the SVM clas-sifier.
Section 3 describes how the votes from char-acter n-gram language models were derived.
Section4 describes the details of our method.
And finallysection 5 presents the results.2 FeaturesWe pre-processed the tweets as follows: i) tok-enized the tweets using a tokenizer suitable for Twit-ter data, which, for example, recognize emoticonsand hashtags; ii) replaced all URLs with the tokentwitterurl; iii) replaced all Twitter usernames withthe token @twitterusername; iv) converted all to-kens into lower case; v) replaced all sequences ofrepeated characters by three characters, for example,convert gooooood to goood, this way we recognize520the emphasized usage of the word; vi) expanded ab-breviations with a dictionary,1 which we will referto as noslang dictionary; vii) appended neg to allwords from one position before a negation word tothe next punctuation mark.We represented each given tweet using 6 featurefamilies:?
Lexical features (UG, BG): Number of timeseach unigram appears in the tweet (UG); num-ber of times each bigram appears in the tweet(BG).?
POS features (POS U, POS B): Number oftimes each POS appears in the tweet divided bynumber of tokens of that tweet (POS U); num-ber of times each POS bigram appears in thetweet (POS B).
To tag the tweet we used theark-twitter-nlp tagger.2?
Statistical features (STAT): Various readabil-ity scores (ARI, Flesch Reading Ease, RIX,LIX, Coleman Liau Index, SMOG Index, Gun-ning Fog Index, Flesch-Kincaid Grade Level)of the tweet; some simple statistics of the tweet(average count of words per sentence, complexword count, syllable count, sentence count,word count, char count).
We calculated thestatistics and scores after pre-processing stepvi).
We then normalized these scores so thatthey had mean 0 and standard deviation 1.?
Stylistic features (STY): Number of timesan emoticon appears in the tweet, number ofwords which are written in all capital let-ters, number of words containing charactersrepeated consecutively more than three times,number of words containing characters re-peated consecutively more than four times.
Wecalculated these features after pre-processingstep i).
We used the binarized and the logarith-mically scaled version of these features.?
Abbreviation features (ABB): For every termin the noslang dictionary, we checked whetherit was present in the tweet or not and used thisas a feature.1http://www.noslang.com2http://www.ark.cs.cmu.edu/TweetNLP/?
URL features (URL): We expanded the URLsin the Twitter text and collected all the domainnames which the URLs in the training set pointto, and used them as binary features.Feature sets UG, BG, POS U, POS B are com-mon features for sentiment analysis (Pang et al2002).
Remus (2011) showed that incorporat-ing readability measures as features can improvesentence-level subjectivity classification.
Stylisticfeatures have also been used in sentiment analysis onTwitter data (Go et al 2010).
Some abbreviationsexpress sentiment which is not apparent from wordlevel.
For example lolwtime, which means laugh-ing out loud with tears in my eyes, expresses positivesentiment overall, but this does not follow directly atthe sentiment of individual words, so the feature setABB might be helpful.
Finally, we conjecture that atweet including an URL pointing to youtube.comis more likely to be subjective than a tweet includingan URL pointing to a news website.3 Integrating votes from language modelsbased on character n-gramsLanguage Models can be used for text classificationtasks.
Since the goal of the SemEval-2013 Task 2(Task B) is to classify each tweet into one of thethree classes: positive, negative or neutral, a lan-guage model approach can be used.Emoticon-smoothed language models have beenused to do Twitter sentiment analysis (Liu et al2012).
The language models used there were basedon words.
However, there is evidence (Aisopos etal., 2012; Raaijmakers and Kraaij, 2008) showingthat super-word character n-gram features can bequite effective for sentiment analysis on short infor-mal data.
This is because noise and mis-spellingstend to have smaller impact on substring patternsthan on word patterns.
Our system used languagemodels based on character n-grams to improve theperformance of sentiment analysis on tweets.For every tweet we constructed 3 sequences ofcharacter-trigrams and 4 sequences of character-four-grams.
For instance, the tweet "HelloWorld!"
would have 7 corresponding substringrepresentations:<s><s>H ell o W orl d!</s>,<s>He llo Wo rld !</s></s>,521Hel lo Wor ld!,<s><s><s>H ello Wor ld!</s><s><s>He llo Worl d!</s></s>,<s>Hel lo W orld !</s></s></s>,Hell o Wo rld!where <s> means start of a sentence, </s> meansend of a sentence, means whitespace.
Using thecorresponding sequences of character-trigrams fromall positive tweets in training set we trained a lan-guage model LM+3 .
To train the language modelwe used Chen and Goodman?s modified Kneser-Neydiscounting for N-grams from the SRILM toolkit(Stolcke, 2002).
Given a new sequence of character-trigrams derived from a positive tweet, it shouldgive a lower perplexity value than a language modeltrained on sequences of character-trigrams fromnegative tweets.In this way we obtained 6 language models:LM?3 from character-trigram sequences of neg-ative tweets, LMN3 from character-trigram se-quences of neutral tweets, LM+3 from character-trigram sequences of positive tweets, LM?4 fromcharacter-four-grams sequences of negative tweets,LMN4 from character-four-gram sequences of neu-tral tweets, LM+4 from character-four-gram se-quences of positive tweets.For every new tweet, we first obtain the 7 corre-sponding substring representations.
Then for eachsubstring representation, we calculate 3 votes fromthe language models.
For instance, for a sequenceof character-trigrams, we first calculate three per-plexity valuesP?3 , PN3 , P+3 using language modelsLM?3 , LMN3 , LM+3 then produce votes accordingto the following discretization function:vote(LMxn , LMyn) ={1 if P xn ?
Pyn ;?1 else.where n ?
{3, 4} is the length of the character n-gram, x, y ?
{?,+, N} are class labels and P xn , Pynare the corresponding perplexity values.
In this waywe obtain 21 votes for every tweet.
However, in thefinal classification, every sentence got 42 votes, ofwhich 21 were derived from bigram language mod-els of the substrings and 21 were from trigram lan-guage models of these substrings.Feature Sets AccuracyUG,BG,POS U,POS B,STAT,STY,ABB,URL 0.692BG,POS U,POS B,STAT,STY,ABB,URL 0.641POS U,POS B,STAT,STY,ABB,URL 0.579POS U,STAT,STY,ABB,URL 0.564STAT,STY,ABB,URL 0.524STY,ABB,URL 0.474STY,URL 0.454URL 0.441Table 1: Cross validation average accuracy with differ-ent feature sets.
we started with all 8 feature sets andremoved feature sets one by one, where we always firstremoved the feature set that resulted in the biggest dropin accuracy.4 MethodsIn this section we describe the methods used by oursystem.Firstly, we did feature selection on all the featuresdescribed in Section 2.
Using Mutual Information(Shannon and Weaver, 1949) and 10-fold cross vali-dation we chose the top 13,500 features.
Using thesefeatures we trained an SVM classifier with the train-ing data.
As the implementation of the SVM classi-fier we used liblinear (Fan et al 2008).
The SVMclassifier was then used to produce initial predictionsfor messages in the development set, the Twitter testset and the SMS test set.Then, we represented every message in the devel-opment set, the Twitter test set and the SMS testset using the 42 votes we described in Section 3together with the predictions of the SVM classifierwe described above.
Using the Bagging algorithmfrom the WEKA machine learning toolkit (Hall etal., 2009) and the development set data, we traineda new classifier and used this classifier for the finalprediction on Twitter test data and SMS test data.5 Results5.1 Feature analysisTo study the effectiveness of different features, westarted with all 8 feature sets and removed featuresets one by one, where we always first removed thefeature set that resulted in the biggest drop in accu-racy.
We did 10 fold cross validation on training set522Feature Sets AccuracyPOS U,POS B,STAT,STY,ABB,URL 0.579POS B,STY,ABB,URL 0.571POS U,STY,ABB,URL 0.557STAT,STY,ABB,URL 0.524STY,ABB,URL 0.474Table 2: Cross validation average accuracy with furthercombination of feature sets.Accuracy F1 (pos, neg)Majority Baseline 0.4123 0.2919SVM Classifier 0.6612 0.5414SVM + LM Votes 0.6457 0.5384Table 3: Overall accuracy and average F1 score for posi-tive and negative classes on Twitter test data.and used average accuracy as a metric.As we can see from Table 1, lexical features werethe most important features ?
they counted for morethan 0.11 loss of accuracy when removed from thefeatures.
POS features and statistical features werealso important, POS bigrams more so than POS uni-grams.
Stylistic, abbreviation and URL features, onthe contrary, seem to be only of moderate useful-ness.To further investigate the relationship between thefeature sets POS U, POS B and STAT, we did addi-tional experiments.
From Table 2, we can see thatremoving all three feature sets caused a decreasein accuracy to 0.47, including just one feature setPOS B, POS U or STAT resulted in accuracy above0.57, 0.55 and 0.52 respectively.
This shows thatall three feature sets were quite effective and POS Bwas most useful.
However, adding all of the threefeature sets only caused an increase in accuracy to0.579, which suggests that they were highly corre-lated.Accuracy F1 (pos, neg)Majority Baseline 0.2350 0.1902SVM Classifier 0.6504 0.5811SVM + LM Votes 0.6418 0.5670Table 4: Overall accuracy and average F1 score for posi-tive and negative classes on SMS test data.5.2 Effectiveness of language model featuresTo evaluate the effectiveness of features derivedfrom language models of character n-grams, wecompared the performance of our SVM classifierand that of the classifier combining the SVM clas-sifier results and language model features.3 We per-formed our experiments on both of the Twitter testdata and the SMS test data.
The results in Table 3and Table 4 suggested that in our current setup, lan-guage model features were not very helpful.Table 3 and Table 4 also show that our systemimproved the performance greatly compared to Ma-jority baseline system,4.
Compared with other par-ticipants in the SemEval-2013 Task 2, our systemachieved average performance on Twitter test data.However, it has been the ninth best out of all 48 sys-tems for the performance on SMS test data.
Thisshows that our system can be easily adapted to dif-ferent contexts without a big drop in performance.One reason for that might be that we did not use anysentiment lexicon developed specifically for Twitterdata and we used high level features like the statisti-cal features and POS features for our classification.6 ConclusionThis paper briefly reports our system designed forthe SemEval-2013 Task 2: sentiment analysis inTwitter.
We first used an SVM classifier with a widerange of features.
We found that simple statisticsof the tweets, for example word count or readabil-ity scores, can help in sentiment analysis on Twittertext.We then used character n-gram language mod-els to address the problem of high lexical variationin Twitter text and combined the two approachesto obtain the final results.
Although in our currentsetup, features derived from character n-gram lan-guage models do not perform very well, they maybenefit from a larger training data set.AcknowledgmentsThis work was funded by DFG projects SFB 732.We would like to thank our colleagues at IMS.3We accidentally used feature set POS B two times in ourrepresentation, but it didn?t change the results significantly.4To be consistent with the evaluation metric, we chose themajority class of positive and negative classes.523ReferencesFotis Aisopos, George Papadakis, Konstantinos Tserpes,and Theodora Varvarigou.
2012.
Content vs. contextfor sentiment analysis: a comparative analysis overmicroblogs.
In Proceedings of the 23rd ACM confer-ence on Hypertext and social media, HT ?12, pages187?96, New York, NY, USA.
ACM.Luciano Barbosa and Junlan Feng.
2010.
Robust senti-ment detection on twitter from biased and noisy data.In Proceedings of the 23rd International Conferenceon Computational Linguistics: Posters, COLING ?10,pages 36?44, Stroudsburg, PA, USA.
Association forComputational Linguistics.Albert Bifet and Eibe Frank.
2010.
Sentiment knowl-edge discovery in twitter streaming data.
In Proceed-ings of the 13th international conference on Discov-ery science, DS?10, pages 1?15, Berlin, Heidelberg.Springer-Verlag.Dmitry Davidov, Oren Tsur, and Ari Rappoport.
2010.Enhanced sentiment learning using twitter hashtagsand smileys.
In Proceedings of the 23rd InternationalConference on Computational Linguistics: Posters,COLING ?10, pages 241?249, Stroudsburg, PA, USA.Association for Computational Linguistics.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-RuiWang, and Chih-Jen Lin.
2008.
LIBLINEAR: a li-brary for large linear classification.
J. Mach.
Learn.Res., 9:1871?1874, June.Alec Go, Richa Bhayani, and L. Huang.
2010.
Exploit-ing the unique characteristics of tweets for sentimentanalysis.
Technical report, Technical Report, StanfordUniversity.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA data mining software: an update.SIGKDD Explor.
Newsl., 11(1):10?18, November.Bernard J. Jansen, Mimi Zhang, Kate Sobel, and AbdurChowdury.
2009.
Twitter power: Tweets as elec-tronic word of mouth.
J.
Am.
Soc.
Inf.
Sci.
Technol.,60(11):2169?2188, November.Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and TiejunZhao.
2011.
Target-dependent twitter sentiment clas-sification.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistics:Human Language Technologies - Volume 1, HLT ?11,pages 151?160, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Kun-Lin Liu, Wu-Jun Li, and Minyi Guo.
2012.
Emoti-con smoothed language models for twitter sentimentanalysis.
In Twenty-Sixth AAAI Conference on Artifi-cial Intelligence.A.
Pak and P. Paroubek.
2010.
Twitter as a corpus forsentiment analysis and opinion mining.
LREC.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
sentiment classification using ma-chine learning techniques.
In Proceedings of EMNLP,pages 79?86.Stephan Raaijmakers and Wessel Kraaij.
2008.
A shal-low approach to subjectivity classification.
Proceed-ings of ICWSM, pages 216?217.Robert Remus.
2011.
Improving sentence-level subjec-tivity classification through readability measurement.May.
Proceedings of the 18th Nordic Conference ofComputational Linguistics NODALIDA 2011.Hassan Saif, Yulan He, and Harith Alani.
2012.
Seman-tic sentiment analysis of twitter.
In Philippe Cudr-Mauroux, Jeff Heflin, Evren Sirin, Tania Tudorache,Jrme Euzenat, Manfred Hauswirth, Josiane XavierParreira, Jim Hendler, Guus Schreiber, Abraham Bern-stein, and Eva Blomqvist, editors, The Semantic WebISWC 2012, number 7649 in Lecture Notes in Com-puter Science, pages 508?524.
Springer Berlin Heidel-berg, January.Claude E. Shannon and Warren Weaver.
1949.
Mathe-matical Theory of Communication.
University of Illi-nois Press.Andreas Stolcke.
2002.
SRILMAn extensible languagemodeling toolkit.
In In Proceedings of the 7th Inter-national Conference on Spoken Language Processing(ICSLP 2002, page 901904.Andranik Tumasjan, Timm O. Sprenger, Philipp G. Sand-ner, and Isabell M. Welpe.
2010.
Predicting electionswith twitter: What 140 characters reveal about politi-cal sentiment.
Proceedings of the Fourth InternationalAAAI Conference on Weblogs and Social Media.Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, SaraRosenthal, Veselin Stoyanov, and Alan Ritter.
2013.SemEval-2013 task 2: Sentiment analysis in twitter.In Proceedings of the International Workshop on Se-mantic Evaluation, SemEval ?13, June.524
