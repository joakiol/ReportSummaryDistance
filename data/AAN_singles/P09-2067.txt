Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 265?268,Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLPAutomatic Story Segmentation using a Bayesian Decision Frameworkfor Statistical Models of Lexical Chain FeaturesWai-Kit Lo Wenying Xiong Helen MengThe Chinese University The Chinese University The Chinese Universityof Hong Kong, of Hong Kong, of Hong Kong,Hong Kong, China Hong Kong, China Hong Kong, Chinawklo@se.cuhk.edu.hk wyxiong@se.cuhk.edu.hk hmmeng@se.cuhk.edu.hkAbstractThis paper presents a Bayesian decisionframework that performs automatic storysegmentation based on statistical model-ing of one or more lexical chain features.Automatic story segmentation aims to lo-cate the instances in time where a storyends and another begins.
A lexical chainis formed by linking coherent lexicalitems chronologically.
A story boundaryis often associated with a significantnumber of lexical chains ending before it,starting after it, as well as a low count ofchains continuing through it.
We devise aBayesian framework to capture such be-havior, using the lexical chain features ofstart, continuation and end.
In the scoringcriteria, lexical chain starts/ends aremodeled statistically with the Weibulland uniform distributions at story boun-daries and non-boundaries respectively.The normal distribution is used for lexi-cal chain continuations.
Full combinationof all lexical chain features gave the bestperformance (F1=0.6356).
We found thatmodeling chain continuations contributessignificantly towards segmentation per-formance.1 IntroductionAutomatic story segmentation is an importantprecursor in processing audio or video streams inlarge information repositories.
Very often, thesecontinuous streams of data do not come withboundaries that segment them into semanticallycoherent units, or stories.
The story unit isneeded for a wide range of spoken language in-formation retrieval tasks, such as topic tracking,clustering, indexing and retrieval.
To performautomatic story segmentation, there are threecategories of cues available: lexical cues fromtranscriptions, prosodic cues from the audiostream and video cues such as anchor face andcolor histograms.
Among the three types of cues,lexical cues are the most generic since they canwork on text and multimedia sources.
Previousapproaches include TextTiling (Hearst 1997) thatmonitors changes in sentence similarity, use ofcue phrases (Reynar 1999) and Hidden MarkovModels (Yamron 1998).
In addition, the ap-proach based on lexical chaining captures thecontent coherence by linking coherent lexicalitems (Morris and Hirst 1991, Hirst and St-Onge1998).
Stokes (2004) discovers boundaries bychaining up terms and locating instances of timewhere the count of chain starts and ends (boun-dary strength) achieves local maxima.
Chan et al(2007) enhanced this approach through statisticalmodeling of lexical chain starts and ends.
Wefurther extend this approach in two aspects: 1) aBayesian decision framework is used; 2) chaincontinuations straddling across boundaries aretaken into consideration and statistically modeled.2 Experimental SetupExperiments are conducted using data from theTDT-2 Voice of America Mandarin broadcast.In particular, we only use the data from the longprograms (40 programs, 1458 stories in total),each of which is about one hour in duration.
Theaverage number of words per story is 297.
Thenews programs are further divided chronologi-cally into training (for parameter estimation ofthe statistical models), development (for tuningdecision thresholds) and test (for performanceevaluation) sets, as shown in Figure 1.
Automaticspeech recognition (ASR) outputs that are pro-vided in the TDT-2 corpus are used for lexicalchain formation.265The story segmentation task in this work is todecide whether a hypothesized utterance boun-dary (provided in the TDT-2 data based on thespeech recognition result) is a story boundary.Segmentation performance is evaluated using theF1-measure.20 hour 10 hour 10 hourFeb.20th,1998 Mar.4th,1998 Mar.17th,1998 Apr.4th,1998Training Set Development Set Test Set697 stories 385 stories 376 storiesFigure 1: Organization of the long programs in TDT-2VOA Mandarin for our experiments.3 ApproachOur approach considers utterance boundaries thatare labeled in the TDT-2 corpus and classifiesthem either as a story boundary or non-boundary.We form lexical chains from the TDT-2 ASRoutputs by linking repeated words.
Since wordsmay also repeat across different stories, we limitthe maximum distance between consecutivewords within the lexical chain.
This limit is op-timized according to the approach in (Chan et al2007) based on the training data.
The optimalvalue is found to be 130.9sec for long programs.We make use of three lexical chain features:chain starts, continuations and ends.
At the be-ginning of a story, new words are introducedmore frequently and hence we observe many lex-ical chain starts.
There is also tendency of manylexical chains ending before a story ends.
As aresult, there is a higher density of chain starts andends in the proximity of a story boundary.
Fur-thermore, there tends to be fewer chains strad-dling across a story boundary.
Based on thesecharacteristics of lexical chains, we devise a sta-tistical framework for story segmentation bymodeling the distribution of these lexical chainfeatures near the story boundaries.3.1 Story Segmentation based on a SingleLexical Chain FeatureGiven an utterance boundary with the lexicalchain feature, X, we compare the conditionalprobabilities of observing a boundary, B, or non-boundary, B , as<> )|()|( XBPXBP .
(1)where X is a single chain feature, which may bethe chain start (S), chain continuation (C), orchain end (E).By applying the Bayes?
theorem, this can berewritten as a likelihood ratio test,BxXPBXP ?
)|()|( <>(2)for which the decision thresholdis )(/)( BPBPx =?
, dependent on the a prioriprobability of observing boundary or a non-boundary.3.2 Story Segmentation based on CombinedChain FeaturesWhen multiple features are used in combination,we formulate the problem as),,|(),,|( CESBPCESBP <> .
(3)By assuming that the chain features are condi-tionally independent of one another (i.e.,P(S,C,E|B) = P(S|B) P(C|B) P(E|B)), the formu-lation can be rewritten as a likelihood ratio test<> SECBCPBEPBSPBCPBEPBSP ?)|()|()|()|()|()|(.
(4)4 Modeling of Lexical Chain Features4.1 Chain starts and endsWe follow (Chan et al 2007) to model the lexi-cal chain starts and ends at a story boundary witha statistical distribution.
We apply a windowaround the candidate boundaries (same windowsize for both chain starts and ends) in our work.Chain features falling outside the window areexcluded from the model.
Figure 2 shows thedistribution when a window size of 20 seconds isused.
This is the optimal window size whenchain start and end features are combined.0-2-4-6-8-10-12-14-16-18-20 2 4 6 8 10 12 14 16 18 201020304050Offset from story boundary in secondNumber of lexical chain featuresFitted Weibull dist.
forlexical chain endsFrequency of lexicalchain featuresFitted Weibull dist.
forlexical chain startsxFigure 2: Distribution of chain starts and ends atknown story boundaries.
The Weibull distribution isused to model these distributions.We also assume that the probability of seeinga lexical chain start / end at a particular instanceis independent of the starts / ends of other chains.As a result, the probability of seeing a sequenceof chain starts at a story boundary is given by theproduct of a sequence of Weibull distributions?=?????????????
?=skiNitki etkBSP11)|( ???
, (5)266where S is the sequence of time with chain starts(S=[t1, t2, ?
ti, ?
tNs]), ks is the shape, ?s is thescale for the fitted Weibull distribution for chainstarts, Ns is the number of chain starts.
The sameformulation is similarly applied to chain ends.Figure 3 shows the frequency of raw featurepoints for lexical chain starts and ends near utter-ance boundaries that are non-story boundaries.Since there is no obvious distribution pattern forthese lexical chain features near a non-storyboundary, we model these characteristics with auniform distribution.2 4 6 8 10 12 14 160.020.040.060.080-2-4-6-8-10-12-14-160.1Relative frequency of chain starts / endsOffset from utterance boundary in seconds(non-story boundaries only)Lexical chain starts / endsFitted uniform dist.
forlexical chain startsxFitted uniform dist.
forlexical chain endsFigure 3: Distribution of chain starts and ends at ut-terance boundaries that are non-story boundaries.4.2 Chain continuationsFigure 4 shows the distributions of chain contin-uations near story boundary and non-story boun-dary.
As one may expect, there are fewer lexicalchains that straddle across a story boundary (thecurve of )|( BCP ) when compared to a non-storyboundary (the curve of )|( BCP ).
Based on theobservations, we model the probability of occur-rence of lexical chains straddling across a givenstory boundary or non-story boundary by a nor-mal distribution.00.020.040.060.080.10.120.140.16Probability0 5 10 15 20 25Number of chain continuations straddling across anutterance boundaryStory boundary, )|( BCPNon-story boundary, )|( BCPRelative frequency of lexical chaincontinuation at an utterance boundaryxFitted distribution at story boundaryFitted distribution at non-story boundaryProbabilityFigure 4: Distributions of chain continuations at storyboundaries and non-story boundaries.5 Story Segmentation based on Combi-nation of Lexical Chain FeaturesWe trained the parameters of the Weibull distri-bution for lexical chain starts and ends at storyboundaries, the uniform distribution for lexicalchain start / end at non-story boundary, and thenormal distribution for lexical chain continua-tions.
Instead of directly using a threshold asshown in Equation (2), we optimize on the para-meter n, which is the optimal number of top scor-ing utterance boundaries that are classified asstory boundaries in the development set.5.1 Using Bayesian decision frameworkWe compare the performance of the Bayesiandecision framework to the use of likelihood onlyP(X|B) as shown in Figure 5.
The results demon-strate consistent improvement in F1-measurewhen using the Bayesian decision framework.00.20.40.6F1-measure)|( BSP )|( BEP)|()|(BSPBSP)|()|(BEPBEPF1-measureFigure 5: Story segmentation performance in F1-measure when using single lexical chain features.5.2 Modeling multiple features jointly00.20.40.60.8F1-measure(a) (b) (c) (d) (e) (f) (g) (h))|()|((c)BEPBEP)|()|((d)BCPBCP)|()|()|()|((e)BEPBSPBEPBSP)|()|()|()|((f)BCPBSPBCPBSP)|()|()|()|((g)BCPBEPBCPBEP)|()|()|()|()|()|((h)BCPBEPBSPBCPBEPBSP)|()|((b)BSPBSP]2007[),(core (a) ChanESSF1-measureFigure 6: Results of F1-measure comparing the seg-mentation results using different statistical models oflexical chain features.We further compare the performance of variousscoring methods including single and combinedlexical chain features.
The baseline result is ob-tained using a scoring function based on the like-lihoods of seeing a chain start or end at a storyboundary (Chan et al 2007) which is denoted asScore(S, E).
Performance from other methodsbased on the same dataset can be referenced fromChan et al 2007 and will not be repeated here.The best story segmentation performance isachieved by combining all lexical chain featureswhich achieves an F1-measure of 0.6356.
Allimprovements have been verified to be statisti-cally significant (?=0.05).
By comparing the re-sults of (e) to (h), (c) to (g), and (b) to (f), we cansee that lexical chain continuation feature contri-butes significantly and consistently towards storysegmentation performance.2675.3 AnalysisUtterance boundary(occurs at 664 second in document VOM19980317_0900_1000,which is not a story boundary)time5 10-5-1011 chain continuations:W1[??
], W2[??
], W3[??
], W4[???
], W5[??],W6[??
], W7[??
], W8[??
], W9[??
], W10[??
], W11[??
]15-15W 15[??
]W 16[??
]W 17[???
]W 18[??
]W 19[??
]W 20[??
]W 21[??
]W 12[??
]W 13[??
]W 14[??
]ts1 ts2 ts3 ts4 ts5 ts6 ts7te1te2te3Figure 7: Lexical chain starts, ends and continuationsin the proximity of a non-story boundary.
Wi[xxxx]denotes the i-th Chinese word ?xxxx?.Figure 7 shows an utterance boundary that is anon-story boundary.
There is a high concentra-tion of chain starts and ends near the boundarywhich leads to a misclassification if we onlycombine chain starts and ends for segmentation.However, there are also a large number of chaincontinuations across the utterance boundary,which implies that a story boundary is less likely.The full combination gives the correct decision.Utterance boundary(occurs at 2014 second in documentVOM19980319_0900_1000, which is a story boundary)time10 201020ts1 ts3te4te5te6 te1te2te3 ts26 chain continuations:W1[???
], W2[??
], W3[???],W4[??
], W5[?
?, W6[??
]W 13[?????
]W 14[????
]W 15[??
]W 12[??
]W 11[??
]W 10[???
]W 9[??
]W 8[??
]W 7[??
]Figure 8: Lexical chain starts, ends and continuationsin the proximity of a story boundary.Figure 8 shows another example where an ut-terance boundary is misclassified as a non-storyboundary when only the combination of lexicalchain starts and ends are used.
Incorporation ofthe chain continuation feature helps rectify theclassification.From these two examples, we can see that theincorporation of chain continuation in our storysegmentation framework can complement thefeatures of chain starts and ends.
In both exam-ples above, the number of chain continuationsplays a crucial role in correct identification of astory boundary.6 ConclusionsWe have presented a Bayesian decision frame-work that performs automatic story segmentationbased on statistical modeling of one or more lex-ical chain features, including lexical chain starts,continuations and ends.
Experimentation showsthat the Bayesian decision framework is superiorto the use of likelihoods for segmentation.
Wealso experimented with a variety of scoring crite-ria, involving likelihood ratio tests of a singlefeature (i.e.
lexical chain starts, continuations orends), their pair-wise combinations, as well asthe full combination of all three features.
Lexicalchain starts/ends are modeled statistically withthe Weibull and normal distributions for storyboundaries and non-boundaries.
The normal dis-tribution is used for lexical chain continuations.Full combination of all lexical chain featuresgave the best performance (F1=0.6356).
Model-ing chain continuations contribute significantlytowards segmentation performance.AcknowledgmentsThis work is affiliated with the CUHK MoE-Microsoft Key Laboratory of Human-centric Compu-ting and Interface Technologies.
We would also liketo thank Professor Mari Ostendorf for suggesting theuse of continuing chains and Mr. Kelvin Chan forproviding information about his previous work.ReferencesChan, S. K. et al 2007.
?Modeling the Statistical Be-haviour of Lexical Chains to Capture Word Cohe-siveness for Automatic Story Segmentation?, Proc.of INTERSPEECH-2007.Hearst, M. A.
1997.
?TextTiling: Segmenting Textinto Multiparagraph Subtopic Passages?, Computa-tional Linguistics, 23(1), pp.
33?64.Hirst, G. and St-Onge, D. 1998.
?Lexical chains asrepresentations of context for the detection andcorrection of malapropisms?, WordNet: An Elec-tronic Lexical Database, pp.
305?332.Morris, J. and Hirst, G. 1991.
?Lexical cohesion com-puted by thesaural relations as an indicator of thestructure of text?, Computational Linguistics,17(1), pp.
21?48.Reynar, J.C. 1999, ?Statistical models for topic seg-mentation?, Proc.
37th annual meeting of the ACL,pp.
357?364.Stokes, N. 2004.
Applications of Lexical CohesionAnalysis in the Topic Detection and Tracking Do-main, PhD thesis, University College Dublin.Yamron, J.P. et al 1998, ?A hidden Markov modelapproach to text segmentation and event tracking?,Proc.
ICASSP 1998, pp.
333?336.268
