Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing, ACL-IJCNLP 2009, pages 50?53,Suntec, Singapore, 7 August 2009.c?2009 ACL and AFNLPMeasuring semantic relatedness with vector space models and randomwalksAmac?
Herda?gdelenCenter for Mind/Brain SciencesUniversity of Trentoamac@herdagdelen.comKatrin ErkLinguistics DepartmentUniversity of Texas at Austinkatrin.erk@mail.utexas.eduMarco BaroniCenter for Mind/Brain SciencesUniversity of Trentomarco.baroni@unitn.itAbstractBoth vector space models and graph ran-domwalk models can be used to determinesimilarity between concepts.
Noting thatvectors can be regarded as local views ofa graph, we directly compare vector spacemodels and graph random walk models onstandard tasks of predicting human simi-larity ratings, concept categorization, andsemantic priming, varying the size of thedataset from which vector space and graphare extracted.1 IntroductionVector space models, representing word mean-ings as points in high-dimensional space, havebeen used in a variety of semantic relatednesstasks (Sahlgren, 2006; Pad?o and Lapata, 2007).Graphs are another way of representing relationsbetween linguistic entities, and they have beenused to capture semantic relatedness by using bothcorpus-based evidence and the graph structure ofWordNet and Wikipedia (Pedersen et al, 2004;Widdows and Dorow, 2002; Minkov and Cohen,2008).
We study the relationship between vec-tor space models and graph random walk mod-els by embedding vector space models in graphs.The flexibility offered by graph randomwalk mod-els allows us to compare the vector space-basedsimilarity measures to extended notions of relat-edness and similarity.
In particular, a randomwalk model can be viewed as smoothing directsimilarity between two vectors using second-orderand even higher-order vectors.
This view leadsto the second focal point of this paper: We in-vestigate whether random walk models can sim-ulate the smoothing effects obtained by methodslike Singular Value Decomposition (SVD).
To an-swer this question, we compute models on reduced(downsampled) versions of our dataset and evalu-ate the robustness of random walk models, a clas-sic vector-based model, and SVD-based modelsagainst data sparseness.2 Model definition and implementationWe use directed graphs with weighted edges, G =(V,E,w) where V is a set of nodes, E = V ?
Vis a set of edges and w : E ?
R is the weight-ing function on edges.
For simplicity, we assumethat G is fully connected, edges with zero weightscan be considered as non-existing in the graph.
Onthese graphs, we perform random walks with aninitial probability distribution q over the nodes (a1 ?
|V | vector).
We then follow edges with prob-ability proportional to their weights, so that theprobability of walking from node v1to node v2is w(v1, v2)/?vw(v1, v).
A fixed length randomwalk ends after a predetermined number of steps.In flexible walks, there is a constant probability ?of stopping at each step.
Thus, walk length fol-lows a geometric distribution with parameter ?,the probability of a walk of length k is ?(1??
)k?1and the expected walk length is 1/?.
For example,a flexible walk with ?
= 1/2 will produce 1-step,2-step, and higher-step walks while the expectedaverage length is 2.Relating vectors and graphs.
Corpus co-occurrence (e1, e2, a12) of two entities e1and e2that co-occur with (potentially transformed) counta12can be represented in either a vector or a graph.In a vector, it corresponds to a dimension value ofa12for the dimension e2of entity e1.
In a graph,it corresponds to two nodes labeled e1and e2con-nected by an edge with weight a12.Similarity measures.
Let R(q) = p denote aspecific random walk process which transforms an50initial probability distribution q to a final prob-ability distribution p over the nodes.
We writeq(m) for the probability assigned to the node munder q.
If the initial distribution q concentratesall probability on a single node n, i.e., q(n) = 1and q(x) = 0 for all nodes x 6= n, we writepr(n ?
m) for the probability p(m) of endingup at node m.The simplest way of measuring relatednessthrough random walks is to consider the probabil-ity p(m) of a single node m as an endpoint for awalk starting with start probability distribution q,that is, p = R(q).
We call this a direct, one-direction measure of relatedness between q andm.
Direct, one-direction measures are typicallyasymmetric.
In case all start probability is con-centrated on a single node n, we can also considerdirect, two-direction measures, which will be acombination of pr(m ?
n) and pr(n ?
m).
Thepoint of using two-direction measures is that thesecan be made symmetric, which is an advantagewhen we are modeling undirected semantic sim-ilarity.
In the experiments below we focus on theaverage of the two probabilities.In addition to direct measures, we will use in-direct measures, in which we compute the relat-edness of endpoint probability distributions p1=R(q1) and p2= R(q2).
As endpoint distribu-tions can be viewed both as probability distribu-tions and as vectors, we used three indirect mea-sures: 1) Jensen/Shannon divergence, a symmet-ric variant of the Kullback/Leibler divergence be-tween probability distributions.
2) cosine similar-ity, and 3) dot product.
Dot product is a naturalchoice in a graph setting because we can view it asthe probability of a pair of walks, one starting at anode determined by q1and the other starting at anode governed by q2, ending at the same node.Discussion.
Direct and indirect relatedness mea-sures together with variation in walk length give usa simple, powerful and flexible way to capture dif-ferent kinds of similarity (with traditional vector-based approach as a special case).
Longer walksor flexible walks will capture higher order effectsthat may help coping with data sparseness, similarto the use of second-order vectors.
Dimensionalityreduction techniques like Singular Value Decom-position (SVD) also capture these higher-order ef-fects, and it has been argued that that makes themmore resistant against sparseness (Sch?utze, 1997).To our knowledge, no systematic comparison ofSVD and classical vector-based methods has beendone on different corpus sizes.
In our experiments,we will compare the performance of SVD andflexible-walk smoothing at different corpus sizesand for a variety of tasks.Implementation: We extract tuples from the 2-billion word ukWaC corpus,1dependency-parsedwith MINIPAR.2Following Pad?o and Lapata(2007), we only consider co-occurrences wheretwo target words are connected by certain de-pendency paths, namely: the top 30 most fre-quent preposition-mediated noun-to-noun paths(soldier+with+gun), the top 50 transitive-verb-mediated noun-to-noun paths (soldier+use+gun),the top 30 direct or preposition-mediated verb-noun paths (kill+obj+victim, kill+in+school), andthe modifying and predicative adjective-to-nounpaths.
Pairs (w1, w2) that account for 0.01%or less of the marginal frequency of w1weretrimmed.
The resulting tuple list, with raw countsconverted to mutual information scores, containsabout 25 million tuples.To test how well graph-based and alternativemethods ?scale down?
to smaller corpora, we sam-pled random subsets of tuples corresponding to0.1%, 1%, 10%, and 100% of the full list.
To putthings into perspective, the full list was extractedfrom a corpus of about 2 billion words; so, the10% list is on the order of magnitude of the BNC,and the 0.1% list is on the order of magnitude ofthe Brown corpus.
From each of the 4 resultingdatasets, we built one graph and two vector spacemodels: one space with full dimensionality, andone space reduced to 300 dimensions using singu-lar value decomposition.3 ExperimentsFirst, we report the results for all tasks obtained onthe full data-set and then proceed with the compar-ison of different models on differing graph sizesto see the robustness of the models against datasparseness.Human similarity ratings: We use the datasetof Rubenstein and Goodenough (1965), consist-ing of averages of subject similarity ratings for65 noun pairs.
We use the Pearson?s coefficientbetween estimates and human judgments as ourperformance measure.
The results obtained for1http://wacky.sslmit.unibo.it2http://www.cs.ualberta.ca/?lindek/minipar.htm51Direct (average) Vector (cosine) Indirect (dot product) Previous0.5 1 2 svd vector 0.5 1 2RG 0.409 0.326 0.571 0.798 0.689 0.634 0.673 0.400 BL: 0.70CLW: 0.849AAMP Purity 0.480 0.418 0.669 0.701 0.704 0.664 0.667 0.612 AP: 0.709RS: 0.791Hodgsonsynonym 2, 563 1.289 5, 408??10.015?
?6, 623?
?5, 462?
?5, 954?
?5, 537?
?coord 4, 275?
?3, 969?
?6, 319??11.157?
?7, 593?
?8, 466?
?8, 477?
?4, 854?
?antonym 2, 853?
2, 237 5, 319?
?7, 724?
?5, 455?
?4, 589?
?4, 859?
?6, 810?
?conass 9, 209??10.016?
?5, 889?
?9, 299?
?6, 950?
?5, 993?
?5, 455?
?4, 994?
?supersub 4, 038?
?4, 113?
?6, 773??10.422?
?7, 901?
?6, 792?
?7, 165?
?4, 828?
?phrasacc 4, 577?
?4, 718?
?2, 911?3, 532?3, 023?3, 506?3, 612?1.038Table 1: All datasets.
* (**) indicates significance level p < 0.01 (p < 0.001).
BL: (Baroni and Lenci,2009), CLW: (Chen et al, 2006), AP: (Almuhareb, 2006), RS: (Rothenh?ausler and Sch?utze, 2009)0.1% 1% 10%cos svd cos vector dot 2 cos svd cos vector dot 2 cos svd cos vector dot 2RG 0.219 0.244 0.669 0.676 0.700 1.159 0.911 0.829 1.068AAMP 0.379 0.339 0.366 0.723 0.622 0.634 0.923 0.886 0.948Synonym 0.369 0.464 0.610 0.493 0.590 0.833 0.857 0.770 1.081Antonym 0.449 0.493 0.231 0.768 0.585 0.730 1.044 0.849 0.977Conass 0.187 0.260 0.261 0.451 0.498 0.942 0.857 0.704 1.062Coord 0.282 0.362 0.456 0.527 0.570 1.050 0.927 0.810 1.187Phrasacc 0.268 0.132 0.761 0.849 0.610 1.215 0.920 0.868 1.049Supersub 0.313 0.353 0.285 0.645 0.601 1.029 0.936 0.752 1.060Table 2: Each cell contains the ratio of the performance of the corresponding model for the correspondingdownsampling ratio to the performance of the same model on the full graph.
The higher ratio means theless deterioration due to data sparseness.the full graph are in Table 1, line 1.
The SVDmodel clearly outperforms the pure-vector basedapproach and the graph-based approaches.
Its per-formance is above that of previous models trainedon the same corpus (Baroni and Lenci, 2009).
Thebest model that we report is based on web searchengine results (Chen et al, 2006).
Among thegraph-based random walk models, flexible walkwith parameter 0.5 and fixed 1-step walk with in-direct relatedness measures using dot product sim-ilarity achieve the highest performance.Concept categorization: Almuhareb (2006) pro-posed a set of 402 nouns to be categorized into21 classes of both concrete (animals, fruit.
.
. )
andabstract (feelings, times.
.
. )
concepts.
Our resultson this clustering task are given in Table 1 (line2).
The difference between SVD and pure-vectormodels is negligible and they both obtain the bestperformance in terms of both cluster entropy (notshown in the table) and purity.
Both models?
per-formances are comparable with the previously re-ported studies, and above that of random walks.Semantic priming: The next dataset comesfrom Hodgson (1991) and it is of interest sinceit requires capturing different forms of seman-tic relatedness between prime-target pairs: syn-onyms (synonym), coordinates (coord), antonyms(antonym), free association pairs (conass), super-and subordinate pairs (supersub) and phrasal as-sociates (phrasacc).
Following previous simula-tions of this data-set (Pad?o and Lapata, 2007), wemeasure the similarity of each related target-primepair, and we compare it to the average similar-ity of the target to all the other primes instanti-ating the same relation, treating the latter quan-tity as our surrogate of an unrelated target-primepair.
We report results in terms of differences be-tween unrelated and related pairs, normalized tot-scores, marking significance according to two-tailed paired t-tests for the relevant degrees of free-dom.
Even though the SVD-based and pure-vectormodels are among the top achievers in general, wesee that in different tasks different random walkmodels achieve comparable or even better perfor-mances.
In particular, for phrasal associates andconceptual associates, the best results are obtainedby random walks based on direct measures.3.1 Robustness against data sparsenessSo far, we reported only the results obtained onthe full graph.
However, in order to see the re-sponse of the models to using smaller corpora52we ran another set of experiments on artificiallydown-sampled graphs as explained above.
In thiscase, we are not interested in the absolute perfor-mance of the models per se but the relative per-formance.
Thus, for ease of comparison we fixedeach model?s performance on the full graph to 1for each task and linearly scaled its performanceon smaller graphs.
For example saying that theSVD-based model achieves a score of 0.911 on10% graph for the Rubenstein and Goodenoughdataset means that the ratio of the performanceof SVD-based model on 10% graph to the per-formance of the same model on the full graph is0.911.
The results are given in Table 2, where theonly random walk model we report is dot 2, i.e., a2-step random walk coupled with the dot product-based indirect measure.
This is by far the randomwalk model most robust to downsampling.
In the10% graph, we see that on all tasks but one, dot 2is the model least affected by the data reduction.On the contrary, down-sampling has a positive ef-fect on this model because on 6 tasks, it actuallyperforms better than it does on the full graph!
Thesame behavior is also observed on the 1% graph- as an example, for phrasal associates relations,dot 2 performance increases by a factor of around1.2 when we use one hundredth of the graph in-stead of the full one.
For the smallest graph weused, 0.1%, still dot 2 provides the highest relativeperformance in 5 out of the 8 tasks.4 ConclusionWe compared graph-based random walk modelsand vector models.
For this purpose, we showedhow corpus co-occurrences could be representedboth as a graph and a vector, and we identifiedtwo different ways to calculate relatedness basedon the outcomes of random walks, by direct andindirect measures.
The experiments carried outon 8 different tasks by using the full graph re-vealed that SVD-based model performs very wellacross all types of semantic relatedness.
How-ever, there is also evidence that -depending onthe particular relation- some random walk modelscan achieve results as good as or even better thanthose of SVD-based models.
Our second ques-tion was whether the random walk models wouldbe able to simulate the smoothing effects obtainedby SVD.
While answering this question, we alsocarried out a systematic comparison of plain andSVD-based models on different tasks with differ-ent sizes of data.
One interesting result is that anSVD-based model is not necessarily more robustto data sparseness than the plain vector model.The more interesting result is that a 2-step ran-dom walk model, based on indirect measures withdot product, consistently outperforms both SVD-based and plain vector models in terms of relativeperformance, thus it is able to achieve compara-ble results on very small datasets.
Actually, theimprovement on absolute performance measuresof this random walk by making the dataset evensmaller calls for future research.ReferencesA.
Almuhareb.
2006.
Attributes in lexical acquisition.Dissertation, University of Essex.M.
Baroni and A. Lenci.
2009.
One distributionalmemory, many semantic spaces.
In Proceedings ofGEMS, Athens, Greece.Hsin-Hsi Chen, Ming-Shun Lin, and Yu-Chuan Wei.2006.
Novel association measures using web searchwith double checking.
In Proceedings of ACL, pages1009?16.J.
Hodgson.
1991.
Informational constraints on pre-lexical priming.
Language and Cognitive Processes,6:169?205.Einat Minkov and William W. Cohen.
2008.
Learn-ing graph walk based similarity measures for parsedtext.
In Proceedings of EMNLP?08.S.
Pad?o and M. Lapata.
2007.
Dependency-based con-struction of semantic space models.
ComputationalLinguistics, 33(2):161?199.T.
Pedersen, S. Patwardhan, and J. Michelizzi.
2004.Wordnet::similarity: Measuring the relatedness ofconcepts.
In Proceedings of NAACL, pages 38?41.Klaus Rothenh?ausler and Hinrich Sch?utze.
2009.Unsupervised classification with dependency basedword spaces.
In Proceedings of GEMS, pages 17?24.H.
Rubenstein and J.B. Goodenough.
1965.
Contex-tual correlates of synonymy.
Communications of theACM, 8(10):627?633.M.
Sahlgren.
2006.
The Word-Space Model.
Disserta-tion, Stockholm University.H.
Sch?utze.
1997.
Ambiguity Resolution in NaturalLanguage Learning.
CSLI, Stanford.Dominic Widdows and Beate Dorow.
2002.
Agraph model for unsupervised lexical acquisition.In 19th International Conference on ComputationalLinguistics, pages 1093?1099.53
