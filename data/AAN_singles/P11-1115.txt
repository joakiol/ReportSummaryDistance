Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1148?1158,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsKnowledge Base Population:  Successful Approaches and ChallengesHeng Ji Ralph GrishmanComputer Science Department Computer Science DepartmentQueens College and Graduate CenterCity University of New YorkNew York UniversityNew York, NY 11367, USA New York, NY 10003, USAhengji@cs.qc.cuny.edu grishman@cs.nyu.eduAbstractIn this paper we give an overview of theKnowledge Base Population (KBP) track atthe 2010 Text Analysis Conference.
The maingoal of KBP is to promote research in discov-ering facts about entities and augmenting aknowledge base (KB) with these facts.
This isdone through two tasks, Entity Linking ?
link-ing names in context to entities in the KB ?and Slot Filling ?
adding information about anentity to the KB.
A large source collection ofnewswire and web documents is providedfrom which systems are to discover informa-tion.
Attributes (?slots?)
derived fromWikipedia infoboxes are used to create thereference KB.
In this paper we provide anoverview of the techniques which can serve asa basis for a good KBP system, lay out theremaining challenges by comparison with tra-ditional Information Extraction (IE) and Ques-tion Answering (QA) tasks, and provide somesuggestions to address these challenges.1 IntroductionTraditional information extraction (IE) evaluations,such as the Message Understanding Conferences(MUC) and Automatic Content Extraction (ACE),assess the ability to extract information from indi-vidual documents in isolation.
In practice, how-ever, we may need to gather information about aperson or organization that is scattered among thedocuments of a large collection.
This requires theability to identify the relevant documents and tointegrate facts, possibly redundant, possibly com-plementary, possibly in conflict, coming fromthese documents.
Furthermore, we may want to usethe extracted information to augment an existingdata base.
This requires the ability to link indi-viduals mentioned in a document, and informationabout these individuals, to entries in the data base.On the other hand, traditional Question Answering(QA) evaluations made limited efforts at disam-biguating entities in queries (e.g.
Pizzato et al,2006), and limited use of relation/event extractionin answer search (e.g.
McNamee et al, 2008).The Knowledge Base Population (KBP) sharedtask, conducted as part of the NIST Text AnalysisConference, aims to address and evaluate thesecapabilities, and bridge the IE and QA communi-ties to promote research in discovering facts aboutentities and expanding a knowledge base withthese facts.
KBP is done through two separate sub-tasks, Entity Linking and Slot Filling; in 2010, 23teams submitted results for one or both sub-tasks.A variety of approaches have been proposed toaddress both tasks with considerable success; nev-ertheless, there are many aspects of the task thatremain unclear.
What are the fundamental tech-niques used to achieve reasonable performance?What is the impact of each novel method?
Whattypes of problems are represented in the currentKBP paradigm compared to traditional IE and QA?In which way have the current testbeds and evalua-tion methodology affected our perception of thetask difficulty?
Have we reached a performanceceiling with current state of the art techniques?What are the remaining challenges and what arethe possible ways to address these challenges?
Inthis paper we aim to answer some of these ques-tions based on our detailed analysis of evaluationresults.11482 Task Definition and Evaluation MetricsThis section will summarize the tasks conducted atKBP 2010.
The overall goal of KBP is to auto-matically identify salient and novel entities, linkthem to corresponding Knowledge Base (KB) en-tries (if the linkage exists), then discover attributesabout the entities, and finally expand the KB withany new attributes.In the Entity Linking task, given a person (PER),organization (ORG) or geo-political entity (GPE, alocation with a government) query that consists ofa name string and a background document contain-ing that name string, the system is required to pro-vide the ID of the KB entry to which the namerefers; or NIL if there is no such KB entry.
Thebackground document, drawn from the KBP cor-pus, serves to disambiguate ambiguous namestrings.In selecting among the KB entries, a systemcould make use of the Wikipedia text associatedwith each entry as well as the structured fields ofeach entry.
In addition, there was an optional taskwhere the system could only make use of the struc-tured fields; this was intended to be representativeof applications where no backing text was avail-able.
Each site could submit up to three runs withdifferent parameters.The goal of Slot Filling is to collect from the cor-pus information regarding certain attributes of anentity, which may be a person or some type of or-ganization.
Each query in the Slot Filling task con-sists of the name of the entity, its type (person ororganization), a background document containingthe name (again, to disambiguate the query in casethere are multiple entities with the same name), itsnode ID (if the entity appears in the knowledgebase), and the attributes which need not be filled.Attributes are excluded if they are already filled inthe reference data base and can only take on a sin-gle value.
Along with each slot fill, the systemmust provide the ID of a document which supportsthe correctness of this fill.
If the corpus does notprovide any information for a given attribute, thesystem should generate a NIL response (and nodocument ID).
KBP2010 defined 26 types of at-tributes for persons (such as the age, birthplace,spouse, children, job title, and employing organiza-tion) and 16 types of attributes for organizations(such as the top employees, the founder, the yearfounded, the headquarters location, and subsidiar-ies).
Some of these attributes are specified as onlytaking a single value (e.g., birthplace), while somecan take multiple values (e.g., top employees).The reference KB includes hundreds of thousandsof entities based on articles from an October 2008dump of English Wikipedia which includes818,741 nodes.
The source collection includes1,286,609 newswire documents, 490,596 webdocuments and hundreds of transcribed spokendocuments.To score Entity Linking, we take each query andcheck whether the KB node ID (or NIL) returnedby a system is correct or not.
Then we computethe Micro-averaged Accuracy, computed across allqueries.To score Slot Filling, we first pool all the systemresponses (as is done for information retrievalevaluations) together with a set of manually-prepared slot fills.
These responses are then as-sessed by hand.
Equivalent answers (such as ?BillClinton?
and ?William Jefferson Clinton?)
aregrouped into equivalence classes.
Each systemresponse is rated as correct, wrong, or redundant (aresponse which is equivalent to another responsefor the same slot or an entry already in the knowl-edge base).
Given these judgments, we countCorrect = total number of non-NIL system outputslots judged correctSystem = total number of non-NIL system outputslotsReference = number of single-valued slots with acorrect non-NIL response +number of equivalence classes for all list-valued slotsRecall = Correct / ReferencePrecision = Correct / SystemF-Measure = (2 ?
Recall ?
Precision) / (Recall +Precision)3 Entity Linking: What WorksIn Entity Linking, we saw a general improvementin performance over last year?s results ?
the topsystem achieved 85.78% micro-averaged accuracy.When measured against a benchmark based on in-ter-annotator agreement, two systems?
perform-ance approached and one system exceeded thebenchmark on person entities.3.1 A General ArchitectureA typical entity linking system architecture is de-picted in Figure 1.1149Figure 1.
General Entity LinkingSystem ArchitectureIt includes three steps: (1) query expansion ?
ex-pand the query into a richer set of forms usingWikipedia structure mining or coreference resolu-tion in the background document.
(2) candidategeneration ?
finding all possible KB entries that aquery might link to; (3) candidate ranking ?
rankthe probabilities of all candidates and NIL answer.Table 1 summarizes the systems which ex-ploited different approaches at each step.
In thefollowing subsections we will highlight the newand effective techniques used in entity linking.3.2 Wikipedia Structure MiningWikipedia articles are peppered with structuredinformation and hyperlinks to other (on average25) articles (Medelyan et al, 2009).
Such informa-tion provides additional sources for entity linking:(1).
Query Expansion: For example, WebTLab(Fernandez et al, 2010) used Wikipedia link struc-ture (source, anchors, redirects and disambigua-tion) to extend the KB and compute entity co-occurrence estimates.
Many other teams includingCUNY and Siel used redirect pages and disam-biguation pages for query expansion.
The Siel teamalso exploited bold texts from first paragraphs be-cause they often contain nicknames, alias namesand full names.Methods  System Examples SystemRankingRangeWikipedia Hyperlink Mining  CUNY (Chen et al, 2010), NUSchime (Zhang et al,2010), Siel (Bysani et al, 2010), SMU-SIS (Gottipati etal., 2010), USFD (Yu et al, 2010), WebTLab team (Fer-nandez et al, 2010)[2, 15]QueryExpansionSource document coreferenceresolutionCUNY (Chen et al, 2010) 9Document semantic analysisand context modelingARPANI (Thomas et al, 2010), CUNY (Chen et al,2010), LCC (Lehmann et al, 2010)[1,14] CandidateGenerationIR CUNY (Chen et al, 2010), Budapestacad (Nemeskey etal., 2010), USFD (Yu et al, 2010)[9, 16]Unsupervised SimilarityComputation (e.g.
VSM)CUNY (Chen et al, 2010), SMU-SIS (Gottipati et al,2010), USFD (Yu et al, 2010)[9, 14]SupervisedClassificationLCC (Lehmann et al, 2010), NUSchime (Zhang et al,2010), Stanford-UBC (Chang et al, 2010),  HLTCOE(McNamee, 2010), UC3M (Pablo-Sanchez et al, 2010)[1, 10]Rule-based LCC (Lehmann et al, 2010), BuptPris (Gao et al, 2010) [1, 8]Global Graph-based Ranking CMCRC (Radford et al, 2010) 3CandidateRankingIR Budapestacad (Nemeskey et al, 2010) 16Table 1.
Entity Linking Method ComparisonQueryQuery ExpansionWikihyperlinkminingSource docCoreferenceResolutionKB Node Candidate GenerationKB Node Candidate RankingWiki KB+Textsunsupervisedsimilaritycomputationsupervisedclassifica-tionIRAnswerIRDocument semantic analysisGraph-based1150(2).
Candidate Ranking: Stanford-UBC usedWikipedia hyperlinks (clarification, disambigua-tion, title) for query re-mapping, and encoded lexi-cal and part-of-speech features  from Wikipediaarticles containing hyperlinks to the queries to traina supervised classifier; they reported a significantimprovement on micro-averaged accuracy, from74.85% to 82.15%.
In fact, when the mined attrib-utes become rich enough, they can be used as anexpanded query and sent into an information re-trieval engine in order to obtain the relevant sourcedocuments.
Budapestacad team (Nemeskey et al,2010) adopted this strategy.3.3 Ranking Approach ComparisonThe ranking approaches exploited in the KBP2010entity linking systems can be generally categorizedinto four types:(1).
Unsupervised or weakly-supervised learning,in which annotated data is minimally used to tunethresholds and parameters.
The similarity measureis largely based on the unlabeled contexts.(2).
Supervised learning, in which a pair of entityand KB node is modeled as an instance for classi-fication.
Such a classifier can be learned from theannotated training data based on many differentfeatures.(3).
Graph-based ranking, in which context entitiesare taken into account in order to reach a globaloptimized solution together with the query entity.(4).
IR (Information Retrieval) approach, in whichthe entire background source document is consid-ered as a single query to retrieve the most relevantWikipedia article.The first question we will investigate is howmuch higher performance can be achieved by us-ing supervised learning?
Among the 16 entity link-ing systems which participated in the regularevaluation, LCC (Lehmann et al, 2010), HLTCOE(McNamee, 2010), Stanford-UBC (Chang et al,2010), NUSchime (Zhang et al, 2010) and UC3M(Pablo-Sanchez et al, 2010) have explicitly usedsupervised classification based on many lexicaland name tagging features, and most of them areranked in top 6 in the evaluation.
Therefore we canconclude that supervised learning normally leads toa reasonably good performance.
However, a high-performing entity linking system can also be im-plemented in an unsupervised fashion by exploit-ing effective characteristics and algorithms, as wewill discuss in the next sections.3.4 Semantic Relation FeaturesAlmost all entity linking systems have used seman-tic relations as features (e.g.
BuptPris (Gao et al,2010), CUNY (Chen et al, 2010) and HLTCOE).The semantic features used in the BuptPris systeminclude name tagging, infoboxes, synonyms, vari-ants and abbreviations.
In the CUNY system, thesemantic features are automatically extracted fromtheir slot filling system.
The results are summa-rized in Table 2, showing the gains over a baselinesystem (using only Wikipedia title features in thecase of BuptPris, using tf-idf weighted word fea-tures for CUNY).
As we can see, except for personentities in the BuptPris system, all types of entitieshave obtained significant improvement by usingsemantic features in entity linking.System Using Se-manticFeaturesPER ORG GPE OverallNo 83.89 59.47 33.38 58.93 BuptPrisYes 79.09 74.13 66.62 73.29No 84.55 63.07 57.54 59.91 CUNYYes 92.81 65.73 84.10 69.29Table 2.
Impact of Semantic Features on EntityLinking (Micro-Averaged Accuracy %)3.5 Context InferenceIn the current setting of KBP, a set of target enti-ties is provided to each system in order to simplifythe task and its evaluation, because it?s not feasibleto require a system to generate answers for all pos-sible entities in the entire source collection.
How-ever, ideally a fully-automatic KBP system shouldbe able to automatically discover novel entities(?queries?)
which have no KB entry or few slotfills in the KB, extract their attributes, and conductglobal reasoning over these attributes in order togenerate the final output.
At the very least, due tothe semantic coherence principle (McNamara,2001), the information of an entity depends on theinformation of other entities.
For example, theWebTLab team and the CMCRC team extracted allentities in the context of a given query, and disam-biguated all entities at the same time using a Pag-eRank-like algorithm (Page et al, 1998) or aGraph-based Re-ranking algorithm.
The SMU-SISteam (Gottipati and Jiang, 2010) re-formulatedqueries using contexts.
The LCC team modeled1151contexts using Wikipedia page concepts, and com-puted linkability scores iteratively.
Consistent im-provements were reported by the WebTLab system(from 63.64% to 66.58%).4 Entity Linking: Remaining Challenges4.1 Comparison with Traditional Cross-document Coreference ResolutionPart of the entity linking task can be modeled as across-document entity resolution problem whichincludes two principal challenges: the same entitycan be referred to by more than one name stringand the same name string can refer to more thanone entity.
The research on cross-document entitycoreference resolution can be traced back to theWeb People Search task (Artiles et al, 2007) andACE2008 (e.g.
Baron and Freedman, 2008).Compared to WePS and ACE, KBP requires link-ing an entity mention in a source document to aknowledge base with or without Wikipedia arti-cles.
Therefore sometimes the linking decisionsheavily rely on entity profile comparison withWikipedia infoboxes.
In addition, KBP introducedGPE entity disambiguation.
In source documents,especially in web data, usually few explicit attrib-utes about GPE entities are provided, so an entitylinking system also needs to conduct externalknowledge discovery from background relateddocuments or hyperlink mining.4.2 Analysis of Difficult QueriesThere are 2250 queries in the Entity Linkingevaluation; for 58 of them at most 5 (out of the 46)system runs produced correct answers.
Most ofthese queries have corresponding KB entries.
For19 queries all 46 systems produced different resultsfrom the answer key.
Interestingly, the systemswhich perform well on the difficult queries are notnecessarily those achieved top overall performance?
they were ranked 13rd, 6th, 5th, 12nd, 10th, and 16threspectively for overall queries.
11 queries arehighly ambiguous city names which can exist inmany states or countries (e.g.
?Chester?
), or referto person or organization entities.
From these mostdifficult queries we observed the following chal-lenges and possible solutions.?
Require deep understanding of context enti-ties for GPE queriesIn a document where the query entity is not a cen-tral topic, the author often assumes that the readershave enough background knowledge (?anchor?
lo-cation from the news release information, worldknowledge or related documents) about these enti-ties.
For 6 queries, a system would need to inter-pret or extract attributes for their context entities.For example, in the following passage:?There are also photos of Jake on IHJ inBrentwood, still looking somber?in order to identify that the query ?Brentwood?
islocated in California, a system will need to under-stand that ?IHJ?
is ?I heart Jake community?
andthat the ?Jake?
referred to lives in Los Angeles, ofwhich Brentwood is a part.In the following example, a system is required tocapture the knowledge that ?Chinese Christianman?
normally appears in ?China?
or there is a?Mission School?
in ?Canton, China?
in order tolink the query ?Canton?
to the correct KB entry.This is a very difficult query also because the morecommon way of spelling ?Canton?
in China is?Guangdong?.
?and was from a Mission School in Canton, ?but for the energetic efforts of this Chinese Chris-tian man and the Refuge Matron??
Require external hyperlink analysisSome queries require a system to conduct detailedanalysis on the hyperlinks in the source documentor the Wikipedia document.
For example, in thesource document ?
?Filed under: Falcons<http://sports.aol.com/fanhouse/category/atlanta-falcons/>?, a system will need to analyze thedocument which this hyperlink refers to.
Suchcases might require new query reformulation andcross-document aggregation techniques, which areboth beyond traditional entity disambiguationparadigms.1152?
Require Entity Salience RankingSome of these queries represent salient entities andso using web popularity rank (e.g.
ranking/hitcounts of Wikipedia pages from search engine) canyield correct answers in most cases (Bysani et al,2010; Dredze et al, 2010).
In fact we found that ana?ve candidate ranking approach based on webpopularity alone can achieve 71% micro-averagedaccuracy, which is better than 24 system runs inKBP2010.Since the web information is used as a black box(including query expansion and query log analysis)which changes over time, it?s more difficult to du-plicate research results.
However, gazetteers withentities ranked by salience or major entitiesmarked are worth encoding as additional features.For example, in the following passages:... Tritschler brothers competed in gymnastics at the1904 Games in St Louis 104 years ago?
and ?A char-tered airliner carrying Democratic White House hope-ful Barack Obama was forced to make an unscheduledlanding on Monday in St. Louis after its flight crewdetected mechanical problems?although there is little background information todecide where the query ?St Louis?
is located, a sys-tem can rely on such a major city list to generatethe correct linking.
Similarly, if a system knowsthat ?Georgia Institute of Technology?
has highersalience than ?Georgian Technical University?, itcan correctly link a query ?Georgia Tech?
in mostcases.5 Slot Filling: What Works5.1 A General ArchitectureThe slot-filling task is a hybrid of traditional IE (afixed set of relations) and QA (responding to aquery, generating a unified response from a largecollection).
Most participants met this challengethrough a hybrid system which combined aspectsof QA (passage retrieval) and IE (answer extrac-tion).
A few used off-the-shelf QA, either bypass-ing question analysis or (if QA was used as a?black box?)
creating a set of questions corre-sponding to each slot.The basic system structure (Figure 2) involvedthree phases:  document/passage retrieval (retriev-ing passages involving the queried entity), answerextraction (getting specific answers from the re-trieved passages), and answer combination (merg-ing and selecting among the answers extracted).The solutions adopted for answer extraction re-flected the range of current IE methods as well asQA answer extraction techniques (see Table 3).Most systems used one main pipeline, whileCUNY and BuptPris adopted a hybrid approach ofcombining multiple approaches.One particular challenge for KBP, in compari-son with earlier IE tasks, was the paucity of train-ing data.
The official training data, linked tospecific text from specific documents, consisted ofresponses to 100 queries; the participants jointlyprepared responses to another 50.
So traditionalsupervised learning, based directly on the trainingdata, would provide limited coverage.
Coveragecould be improved by using the training data asseeds for a bootstrapping procedure.Figure 2.
General Slot Filling System ArchitectureIE(Distant Learning/Bootstrapping)QuerySourceCollectionIR Document  LevelIR, QA Sentence/Passage LevelPatternAnswerLevel ClassifierQATrainingData/ExternalKBRulesAnswersQueryExpansionKnowledgeBaseRedundancyRemoval1153Methods System ExamplesDistant Learning (largeseed, one iteration)CUNY (Chen et al, 2010)PatternLearning Bootstrapping (smallseed, multiple iterations)NYU (Grishman and Min, 2010)Distant Supervision Budapestacad (Nemeskey et al, 2010), lsv (Chrupala et al,2010), Stanford (Surdeanu et al, 2010), UBC (Intxaurrondoet al, 2010)TrainedIESupervisedClassifierTrained from KBP train-ing data and other re-lated tasksBuptPris (Gao et al, 2010), CUNY (Chen et al, 2010), IBM(Castelli et al, 2010), ICL (Song et al, 2010),  LCC(Lehmann et al, 2010), lsv (Chrupala et al, 2010), Siel(Bysani et al, 2010)QA CUNY (Chen et al, 2010), iirg (Byrne and Dunnion, 2010)Hand-coded Heuristic Rules BuptPris (Gao et al, 2010), USFD (Yu et al, 2010)Table 3.
Slot Filling Answer Extraction Method ComparisonOn the other hand, there were a lot of 'facts' avail-able ?
pairs of entities bearing a relationship corre-sponding closely to the KBP relations ?
in the formof filled Wikipedia infoboxes.
These could beused for various forms of indirect or distant learn-ing, where instances in a large corpus of such pairsare taken as (positive) training instances.
How-ever, such instances are noisy ?
if a pair of entitiesparticipates in more than one relation, the foundinstance may not be an example of the intendedrelation ?
and so some filtering of the instances orresulting patterns may be needed.
Several sitesused such distant supervision to acquire patterns ortrain classifiers, in some cases combined with di-rect supervision using the training data (Chrupalaet al, 2010).Several groups used and extended existing rela-tion extraction systems, and then mapped the re-sults into KBP slots.
Mapping the ACE relationsand events by themselves provided limited cover-age  (34% of slot fills in the training data), but washelpful when combined with other sources (e.g.CUNY).
Groups with more extensive existing ex-traction systems could primarily build on these(e.g.
LCC, IBM).For example, IBM (Castelli et al, 2010) ex-tended their mention detection component to cover36 entity types which include many non-ACEtypes; and added new relation types between enti-ties and event anchors.
LCC and CUNY appliedactive learning techniques to cover non-ACE typesof entities, such as ?origin?, ?religion?, ?title?,?charge?, ?web-site?
and ?cause-of-death?, andeffectively develop lexicons to filter spurious an-swers.Top systems also benefited from customizing andtightly integrating their recently enhanced extrac-tion techniques into KBP.
For example, IBM,NYU (Grishman and Min, 2010) and CUNY ex-ploited entity coreference in pattern learning andreasoning.
It is also notable that traditional extrac-tion components trained from newswire data sufferfrom noise in web data.
In order to address thisproblem, IBM applied their new robust mentiondetection techniques for noisy inputs (Florian et al,2010); CUNY developed a component to recoverstructured forms such as tables in web data auto-matically and filter spurious answers.5.2 Use of External Knowledge BaseMany instance-centered knowledge bases that haveharvested Wikipedia are proliferating on the se-mantic web.
The most well known are probablythe Wikipedia derived resources, including DBpe-dia (Auer 2007), Freebase (Bollacker 2008) andYAGO (Suchanek et al, 2007) and Linked OpenData (http://data.nytimes.com/).
The main motiva-tion of the KBP program is to automatically distillinformation from news and web unstructured datainstead of manually constructed knowledge bases,but these existing knowledge bases can provide alarge number of seed tuples to bootstrap slot fillingor guide distant learning.Such resources can also be used in a more directway.
For example, CUNY exploited Freebase andLCC exploited DBpedia as fact validation in slotfilling.
However, most of these resources aremanually created from single data modalities andonly cover well-known entities.
For example,while Freebase contains 116 million instances of11547,300 relations for 9 million entities, it only covers48% of the slot types and 5% of the slot answers inKBP2010 evaluation data.
Therefore, both CUNYand LCC observed limited gains from the answervalidation approach from Freebase.
Both systemsgained about 1% improvement in recall with aslight loss in precision.5.3 Cross-Slot and Cross-Query ReasoningSlot Filling can also benefit from extracting re-vertible queries from the context of any targetquery, and conducting global ranking or reasoningto refine the results.
CUNY and IBM developedrecursive reasoning components to refine extrac-tion results.
For a given query, if there are no otherrelated answer candidates available, they built "re-vertible?
queries in the contexts, similar to (Prageret al, 2006), to enrich the inference process itera-tively.
For example, if a is extracted as the answerfor org:subsidiaries of the query q,  we can con-sider a as a new revertible query and verify that aorg:parents answer of a is q.
Both systems signifi-cantly benefited from recursive reasoning (CUNYF-measure on training data was enhanced from33.57% to 35.29% and IBM F-measure was en-hanced from 26% to 34.83%).6 Slot Filling: Remaining ChallengesSlot filling remains a very challenging task; onlyone system exceeded 30% F-measure on the 2010evaluation.
During the 2010 evaluation data anno-tation/adjudication process, an initial answer keyannotation was created by a manual search of thecorpus (resulting in 797 instances), and then anindependent adjudication pass was applied to as-sess these annotations together with pooled systemresponses.
The Precision, Recall and F-measure forthe initial human annotation are only about 70%,54% and 61% respectively.
While we believe theannotation consistency can be improved, in part byrefinement of the annotation guidelines, this doesplace a limit on system performance.Most of the shortfall in system performance re-flects inadequacies in the answer extraction stage,reflecting limitations in the current state-of-the-artin information extraction.
An analysis of the 2010training data shows that cross-sentence coreferenceand some types of inference are critical to slot fill-ing.
In only 60.4% of the cases do the entity nameand slot fill appear together in the same sentence,so a system which processes sentences in isolationis severely limited in its performance.
22.8% ofthe cases require cross-sentence (identity) corefer-ence; 15% require some cross-sentence inferenceand 1.8% require cross-slot inference.
The infer-ences include:?
Non-identity coreference: in the following pas-sage: ?Lahoud is married to an Armenian and thecouple have three children.
Eldest son Emile EmileLahoud was a member of parliament between 2000and 2005.?
the semantic relation between ?chil-dren?
and ?son?
needs to be exploited in orderto generate ?Emile Emile Lahoud?
as theper:children of the query entity ?Lahoud?;?
Cross-slot inference based on revertible que-ries, propagation links or even world knowl-edge to capture some of the most challengingcases.
In the KBP slot filling task, slots are of-ten dependent on each other, so we can im-prove the results by improving the ?coherence?of the story (i.e.
consistency among all gener-ated answers (query profiles)).
In the followingexample:?People Magazine has confirmed that actress JuliaRoberts has given birth to her third child a boynamed Henry Daniel Moder.
Henry was bornMonday in Los Angeles and weighed 8?
lbs.
Rob-erts, 39, and husband Danny Moder, 38, are al-ready parents to twins Hazel and Phinnaeus whowere born in November 2006.?the following reasoning rules are needed togenerate the answer ?Henry Daniel Moder?
asper:children of ?Danny Moder?
:ChildOf (?Henry Daniel Moder?, ?Julia Roberts?)?
Coreferential (?Julia Roberts?, ?Roberts?)?
SpouseOf (?Roberts?, ?Danny Moder?)
?ChildOf (?Henry Daniel Moder?, ?Danny Moder?
)KBP Slot Filling is similar to ACE Relation Ex-traction, which has been extensively studied for thepast 7 years.
However, the amount of training datais much smaller, forcing sites to adjust their train-ing strategies.
Also, some of the constraints ofACE relation mention extraction ?
notably, thatboth arguments are present in the same sentence ?are not present, making the role of coreference andcross-sentence inference more critical.The role of coreference and inference as limitingfactors, while generally recognized, is emphasized1155by examining the 163 slot values that the humanannotators filled but that none of the systems wereable to get correct.
Many of these difficult casesinvolve a combination of problems, but we esti-mate that at least 25% of the examples involvecoreference which is beyond current system capa-bilities, such as nominal anaphors:?Alexandra Burke is out with the video for her secondsingle ?
taken from the British artist?s debut album?
?a woman charged with running a prostitution ring ?her business, Pamela Martin and Associates?
(underlined phrases are coreferential).While the types of inferences which may be re-quired is open-ended, certain types come up re-peatedly, reflecting the types of slots to be filled:systems would benefit from specialists which areable to reason about times, locations, family rela-tionships, and employment relationships.7 Toward System CombinationThe increasing number of diverse approachesbased on different resources provide new opportu-nities for both entity linking and slot filling tasks tobenefit from system combination.The NUSchime entity linking system trained aSVM based re-scoring model to combine two indi-vidual pipelines.
Only one feature based on confi-dence values from the pipelines was used for re-scoring.
The micro-averaged accuracy was en-hanced from 79.29%/79.07% to 79.38% aftercombination.
We also applied a voting approach onthe top 9 entity linking systems and found that allcombination orders achieved significant gains,with the highest absolute improvement of 4.7% inmicro-averaged accuracy over the top entity link-ing system.The CUNY slot filling system trained a maxi-mum-entropy-based re-ranking model to combinethree individual pipelines, based on various globalfeatures including voting and dependency rela-tions.
Significant gain in F-measure was achieved:from 17.9%, 27.7% and 21.0% (on training data) to34.3% after combination.
When we applied thesame re-ranking approach to the slot filling sys-tems which were ranked from the 2nd to 14th, weachieved 4.3% higher F-score than the best ofthese systems.8 ConclusionCompared to traditional IE and QA tasks, KBP hasraised some interesting and important research is-sues: It places more emphasis on cross-documententity resolution which received limited effort inACE; it forces systems to deal with redundant andconflicting answers across large corpora; it linksthe facts in text to a knowledge base so that NLPand data mining/database communities have a bet-ter chance to collaborate; it provides opportunitiesto develop novel training methods such as distant(and noisy) supervision through Infoboxes (Sur-deanu et al, 2010; Chen et al, 2010).In this paper, we provided detailed analysis of thereasons which have made KBP a more challengingtask, shared our observations and lessons learnedfrom the evaluation, and suggested some possibleresearch directions to address these challengeswhich may be helpful for current and new partici-pants, or IE and QA researchers in general.AcknowledgementsThe first author was supported by the U.S. Army Re-search Laboratory under Cooperative Agreement Num-ber W911NF-09-2-0053, the U.S. NSF CAREERAward under Grant IIS-0953149 and PSC-CUNY Re-search Program.
The views and conclusions containedin this document are those of the authors and should notbe interpreted as representing the official policies, eitherexpressed or implied, of the Army Research Laboratoryor the U.S. Government.
The U.S. Government is au-thorized to reproduce and distribute reprints for Gov-ernment purposes notwithstanding any copyrightnotation hereon.ReferencesJavier Artiles, Julio Gonzalo and Satoshi Sekine.
2007.The SemEval-2007 WePS Evaluation: Establishing abenchmark for the Web People Search Task.
Proc.the 4th International Workshop on Semantic Evalua-tions (Semeval-2007).S.
Auer, C. Bizer, G. Kobilarov, J. Lehmann and Z. Ives.2007.
DBpedia: A nucleus for a web of open data.Proc.
6th International Semantic Web Conference.K.
Balog, L. Azzopardi, M. de Rijke.
2008.
PersonalName Resolution of Web People Search.
Proc.WWW2008 Workshop: NLP Challenges in the Infor-mation Explosion Era (NLPIX 2008).1156Alex Baron and Marjorie Freedman.
2008. Who is Whoand What is What: Experiments in Cross-DocumentCo-Reference.
Proc.
EMNLP 2008.K.
Bollacker, R. Cook, and P. Tufts.
2007.
Freebase: AShared Database of Structured General HumanKnowledge.
Proc.
National Conference on ArtificialIntelligence (Volume 2).Lorna Byrne and John Dunnion.
2010.
UCD IIRG atTAC 2010.
Proc.
TAC 2010 Workshop.Praveen Bysani, Kranthi Reddy, Vijay Bharath Reddy,Sudheer Kovelamudi, Prasad Pingali and VasudevaVarma.
2010.
IIIT Hyderabad in Guided Summariza-tion and Knowledge Base Population.
Proc.
TAC2010 Workshop.Vittorio Castelli, Radu Florian and Ding-jung Han.2010.
Slot Filling through Statistical Processing andInference Rules.
Proc.
TAC 2010 Workshop.Angel X. Chang, Valentin I. Spitkovsky, Eric Yeh,Eneko Agirre and Christopher D. Manning.
2010.Stanford-UBC Entity Linking at TAC-KBP.
Proc.TAC 2010 Workshop.Zheng Chen, Suzanne Tamang, Adam Lee, Xiang Li,Wen-Pin Lin, Matthew Snover, Javier Artiles,Marissa Passantino and Heng Ji.
2010.
CUNY-BLENDER TAC-KBP2010 Entity Linking and SlotFilling System Description.
Proc.
TAC 2010 Work-shop.Grzegorz Chrupala, Saeedeh Momtazi, Michael Wie-gand, Stefan Kazalski, Fang Xu, Benjamin Roth, Al-exandra Balahur, Dietrick Klakow.
SaarlandUniversity Spoken Language Systems at the Slot Fill-ing Task of TAC KBP 2010.
Proc.
TAC 2010 Work-shop.Mark Dredze, Paul McNamee, Delip Rao, Adam Gerberand Tim Finin.
2010.
Entity Disambiguation forKnowledge Base Population.
Proc.
COLING 2010.Norberto Fernandez, Jesus A. Fisteus, Luis Sanchez andEduardo Martin.
2010.
WebTLab: A Cooccurence-based Approach to KBP 2010 Entity-Linking Task.Proc.
TAC 2010 Workshop.Radu Florian, John F. Pitrelli, Salim Roukos and ImedZitouni.
2010.
Improving Mention Detection Robust-ness to Noisy Input.
Proc.
EMNLP2010.Sanyuan Gao, Yichao Cai, Si Li, Zongyu Zhang, JingyiGuan, Yan Li, Hao Zhang, Weiran Xu and Jun Guo.2010.
PRIS at TAC2010 KBP Track.
Proc.
TAC2010 Workshop.Swapna Gottipati and Jing Jiang.
2010.
SMU-SIS atTAC 2010 ?
KBP Track Entity Linking.
Proc.
TAC2010 Workshop.Ralph Grishman and Bonan Min.
2010.
New York Uni-versity KBP 2010 Slot-Filling System.
Proc.
TAC2010 Workshop.Ander Intxaurrondo, Oier Lopez de Lacalle and EnekoAgirre.
2010.
UBC at Slot Filling TAC-KBP2010.Proc.
TAC 2010 Workshop.John Lehmann, Sean Monahan, Luke Nezda, ArnoldJung and Ying Shi.
2010.
LCC Approaches toKnowledge Base Population at TAC 2010.
Proc.TAC 2010 Workshop.Paul McNamee and Hoa Dang.
2009.
Overview of theTAC 2009 Knowledge Base Population Track.
Proc.TAC 2009 Workshop.Paul McNamee, Hoa Trang Dang, Heather Simpson,Patrick Schone and Stephanie M. Strassel.
2010.
AnEvaluation of Technologies for Knowledge BasePopulation.
Proc.
LREC2010.Paul McNamee, Rion Snow, Patrick Schone and JamesMayfield.
2008.
Learning Named Entity Hyponymsfor Question Answering.
Proc.
IJCNLP2008.Paul McNamee.
2010.
HLTCOE Efforts in Entity Link-ing at TAC KBP 2010.
Proc.
TAC 2010 Workshop.Danielle S McNamara.
2001.
Reading both High-coherence and Low-coherence Texts: Effects of TextSequence and Prior Knowledge.
Canadian Journal ofExperimental Psychology.Olena Medelyan, Catherine Legg, David Milne and IanH.
Witten.
2009.
Mining Meaning from Wikipedia.International Journal of Human-Computer Studiesarchive.
Volume 67 , Issue 9.David Nemeskey, Gabor Recski, Attila Zseder and An-dras Kornai.
2010.
BUDAPESTACAD at TAC 2010.Proc.
TAC 2010 Workshop.Cesar de Pablo-Sanchez, Juan Perea and Paloma Marti-nez.
2010.
Combining Similarities with Regressionbased Classifiers for Entity Linking at TAC 2010.Proc.
TAC 2010 Workshop.Lawrence Page, Sergey Brin, Rajeev Motwani andTerry Winograd.
1998.
The PageRank Citation Rank-ing: Bringing Order to the Web.
Proc.
the 7th Interna-tional World Wide Web Conference.Luiz Augusto Pizzato, Diego Molla and Cecile Paris.2006.
Pseudo Relevance Feedback Using Named En-tities for Question Answering.
Proc.
the AustralasianLanguage Technology Workshop 2006.J.
Prager, P. Duboue, and J. Chu-Carroll.
2006.
Improv-ing QA Accuracy by Question Inversion.
Proc.
ACL-COLING 2006.1157Will Radford, Ben Hachey, Joel Nothman, MatthewHonnibal and James R. Curran.
2010.
CMCRC atTAC10: Document-level Entity Linking with Graph-based Re-ranking.
Proc.
TAC 2010 Workshop.Yang Song, Zhengyan He and Houfeng Wang.
2010.ICL_KBP Approaches to Knowledge Base Popula-tion at TAC2010.
Proc.
TAC 2010 Workshop.F.
M. Suchanek, G. Kasneci, and G. Weikum.
2007.Yago: A Core of Semantic Knowledge.
Proc.
16thInternational World Wide Web Conference.Mihai Surdeanu, David McClosky, Julie Tibshirani,John Bauer, Angel X. Chang, Valentin I. Spitkovsky,Christopher D. Manning.
2010.
A Simple DistantSupervision Approach for the TAC-KBP Slot FillingTask.
Proc.
TAC 2010 Workshop.Ani Thomas, Arpana Rawai, M K Kowar, SanjaySharma, Sarang Pitale and Neeraj Kharya.
2010.Bhilai Institute of Technology Durg at TAC 2010:Knowledge Base Population Task Challenge.
Proc.TAC 2010 Workshop.Jingtao Yu, Omkar Mujgond and Rob Gaizauskas.2010.
The University of Sheffield System at TACKBP 2010.
Proc.
TAC 2010 Workshop.Wei Zhang, Yan Chuan Sim, Jian Su and Chew LimTan.
2010.
NUS-I2R: Learning a Combined Systemfor Entity Linking.
Proc.
TAC 2010 Workshop.1158
