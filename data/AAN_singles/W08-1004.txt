Proceedings of the ACL-08: HLT Workshop on Parsing German (PaGe-08), pages 24?32,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsRevisiting the impact of different annotation schemes on PCFG parsing:A grammatical dependency evaluationAdriane BoydDepartment of LinguisticsThe Ohio State University1712 Neil AvenueColumbus, Ohio 43210, USAadriane@ling.osu.eduDetmar MeurersSeminar fu?r SprachwissenschaftUniversita?t Tu?bingenWilhelmstrasse 1972074 Tu?bingen, Germanydm@sfs.uni-tuebingen.deAbstractRecent parsing research has started addressingthe questions a) how parsers trained on differ-ent syntactic resources differ in their perfor-mance and b) how to conduct a meaningfulevaluation of the parsing results across sucha range of syntactic representations.
Two Ger-man treebanks, Negra and Tu?Ba-D/Z, consti-tute an interesting testing ground for such re-search given that the two treebanks make verydifferent representational choices for this lan-guage, which also is of general interest giventhat German is situated between the extremesof fixed and free word order.
We show thatprevious work comparing PCFG parsing withthese two treebanks employed PARSEVALand grammatical function comparisons whichwere skewed by differences between the twocorpus annotation schemes.
Focusing on thegrammatical dependency triples as an essen-tial dimension of comparison, we show thatthe two very distinct corpora result in compa-rable parsing performance.1 IntroductionSyntactically annotated corpora have been producedfor a range of languages and they differ significantlyregarding which language properties are encodedand how they are represented.
Between the two ex-tremes of constituency treebanks for English and de-pendency treebanks for free word order languagessuch as Czech lie languages such as German, forwhich two different treebanks have explored differ-ent options for encoding topology and dependency,Negra (Brants et al, 1999) and Tu?Ba-D/Z (Telljo-hann et al, 2005).Recent research has started addressing the ques-tion of how parsers trained on these different syntac-tic resources differ in their performance.
Such workmust also address the question of how to conduct ameaningful evaluation of the parsing results acrosssuch a range of syntactic representations.
In this pa-per, we show that previous work comparing PCFGparsing for the two German treebanks used represen-tations which cannot adequately be compared usingthe given PARSEVAL measures and that a grammat-ical dependency evaluation is more meaningful thanthe grammatical function evaluation provided.We present the first comparison of Negra andTu?Ba-D/Z using a labeled dependency evaluationbased on the grammatical function labels providedin the corpora.
We show that, in contrast to previ-ous literature, a labeled dependency evaluation es-tablishes that PCFG parsers trained on the two cor-pora give similar parsing performance.
The focus onlabeled dependencies also provides a direct link torecent work on dependency-based evaluation (e.g.,Clark and Curran, 2007) and dependency parsing(e.g., CoNLL shared tasks 2006, 2007).1.1 Previous workThe question of how to evaluate parser output hasnaturally already arisen in earlier work on parsingEnglish.
As discussed by Lin (1995) and others, thePARSEVAL evaluation typically used to analyze theperformance of statistical parsing models has manydrawbacks.
Bracketing evaluation may count a sin-gle error multiple times and does not differentiatebetween errors that significantly affect the interpre-tation of the sentence and those that are less crucial.24It also does not allow for evaluation of particularsyntactic structures or provide meaningful informa-tion about where the parser is failing.
In addition,and most directly relevant for this paper, PARSE-VAL scores are difficult to compare across syntacticannotation schemes (Carroll et al, 2003).At the same time, previous research on PCFGparsing using treebank training data present PAR-SEVAL measures in comparing the parsing per-formance for different languages and annotationschemes, reporting a number of striking differences.For example, Levy and Manning (2003), Ku?bler(2005), and Ku?bler et al (2006) highlight the sig-nificant effect of language properties and annotationschemes for German and Chinese treebanks.
In re-lated work, parser enhancements that provide a sig-nificant performance boost for English, such as headlexicalization, are reported not to provide the samekind of improvement, if any, for German (Dubey andKeller, 2003; Dubey, 2004; Ku?bler et al, 2006).Previous work has compared the similar Negraand Tiger corpora of German to the very differentTu?Ba-D/Z corpus.
Ku?bler et al (2006) comparesthe Negra and Tu?Ba-D/Z corpora of German usinga PARSEVAL evaluation and an evaluation on coregrammatical function labels that is included to ad-dress concerns about the PARSEVAL measure.1 Us-ing the Stanford Parser (Klein and Manning, 2002),which employs a factored PCFG and dependencymodel, they claim that the model trained on Tu?Ba-D/Z consistently outperforms that trained on Ne-gra in PARSEVAL and grammatical function evalu-ations.
Dubey (2004) also includes an evaluation ongrammatical function for statistical models trainedon Negra, but obtains very different results fromKu?bler et al (2006).2In recent related work, Rehbein and van Genabith(2007a) demonstrate using the Tiger and Tu?Ba-D/Z1The evaluation is based only on the grammatical function;it does not identify the dependency pair that it labels.2While the focus of Ku?bler et al (2006) is on comparingparsing results across corpora, Dubey (2004) focuses on im-proving parsing for Negra, including corpus-specific enhance-ments leading to better results.
This difference in focus andadditional differences in experimental setup mean that a fine-grained comparison of the results is inappropriate ?
the rele-vant point here is that the gap between the results (23% for sub-jects, 35% for accusative objects) warrants further attention inthe context of comparing parsing results across corpora.corpora of German that PARSEVAL is inappropri-ate for comparisons of the output of PCFG parserstrained on different treebank annotation schemes be-cause PARSEVAL scores are affected by the ratioof terminal to non-terminal nodes.
A dependency-based evaluation on triples of the form word-POS-head shows better results for the parser trainedon Tiger even though the much lower PARSEVALscores, if meaningful, would predict that the out-put for Tiger is of lower quality.
However, theirdependency-based evaluation does not make useof the grammatical function labels, which are pro-vided in the corpora and closely correspond to therepresentations used in recent work on formalism-independent evaluation of parsers (e.g., Clark andCurran, 2007).3Addressing these issues, we resolve the apparentdiscrepancy between Ku?bler et al (2006) and Dubey(2004) and establish a firm grammatical functioncomparison of Negra and Tu?Ba-D/Z.
We also ex-tend the evaluation to a labeled dependency evalu-ation based on grammatical relations for both cor-pora.
Such an evaluation, which abstracts away fromthe specifics of the annotation schemes, shows that,in contrast to the claims made in Ku?bler et al (2006),the parsing results for PCFG parsers trained on theseheterogeneous corpora are very similar.2 The corpora usedAs motivated in the introduction, the work discussedin this paper is based on two German corpora, Ne-gra and Tu?Ba-D/Z, which differ significantly in thesyntactic representations used ?
thereby offering aninteresting test bed for investigating the influence ofan annotation scheme on the parsers trained.2.1 NegraThe Negra corpus (Brants et al, 1999) consists ofnewspaper text from the Frankfurter Rundschau, aGerman newspaper.
Version 2 of the corpus contains20,602 sentences.
It uses the STTS tag set (Schilleret al, 1995) for part-of-speech annotation.
There are25 non-terminal node labels and 46 edge labels.The syntactic annotation of Negra combines fea-tures from phrase structure grammar and depen-3Their evaluation also introduces an additional level of com-plexity by finding heads heuristically rather than relying on thehead labels present on some elements in each corpus.25dency grammar using a tree-like syntactic structurewith grammatical functions labeled on the edges ofthe tree.
Flat sentence structures are used in manyplaces to avoid attachment ambiguities and non-branching phrases are not used.The annotation scheme emphasizes the use of thetree structure to encode grammatical dependencies,representing a head and all its dependents within alocal tree regardless of whether a dependent is real-ized near its head or not, e.g., because it has beenextraposed or fronted.
Since traditional syntax treesdo not permit the crossing branches needed to li-cense discontinuous constituents, Negra uses a ?syn-tax graph?
data structure to represent the annotation.An example of a syntax graph with a discontinuousconstituent (VP) due to a fronted dative object (NP)is shown in Figure 1.DieserPDATMeinungNNkannVMFINichPPERnurADVvollADJDzustimmenVVINF.$.NK NKNPDA MO HDVPOCHD SB MOSVROOTthis opinion can I only completely agreeFigure 1: Negra tree for ?I can only agree with this opin-ion completely.
?Negra uses flat NP and PP annotation with nomarked heads.
For example, both Dieser and Mein-ung in Figure 1 have the grammatical function label?NK?.
Since unary branching is not used in Negra, abare noun or pronoun argument is not dominated byan NP node, as shown by the pronoun ich above.A verbal head in Negra is always marked with theedge label ?HD?
and its arguments are its sisters inthe local tree.
The subject is always the sister of thefinite verb, which is a daughter of S. If the finite verbis the main verb in the clause, the objects are also itssisters, i.e., the finite verb, subject and objects areall daughters of S. If the main verb is an auxiliarygoverning a non-finite main verb, the non-finite verband its objects and modifiers form a VP where theobjects are sisters of the non-finite verb as in Fig-ure 1.
The VP is then a sister of the finite verb.The finite verb in a German declarative clause ap-pears in the so-called verb-second position, immedi-ately following the fronted constituent.
As a result,the VP in Negra is discontinuous whenever one ofits children has been fronted, as in the common wordorders exemplified in (1a) and (1b).
(1) a. DietheTu?rdoorhathasAnnaAnnawiederagainzugeschlagen.slammed-shut?Anna slammed the door shut again.?b.
WiederagainhathasAnnaAnnadietheTu?rdoorzugeschlagen.slammed-shut?Anna slammed the door shut again.
?The sentence we saw in Figure 1 contains a dis-continuous VP with a fronted dative object (DieserMeinung).
The dative object and a modifier (voll)form a VP with the non-finite verb (zustimmen).2.2 Tu?Ba-D/ZThe Tu?Ba-D/Z corpus, version 2, (Telljohann et al,2005) consists of 22,091 sentences of newspapertext from the German newspaper die tageszeitung.Like Negra, it uses the STTS tag set (Schiller et al,1995) for part-of-speech annotation.
Syntactically ituses 27 non-terminal node labels and 47 edge labels.The syntactic annotation incorporates a topologi-cal field analysis of the German clause (Reis, 1980;Ho?hle, 1986), which segments a sentence into topo-logical units depending on the position of the finiteverb (verb-first, verb-second, verb-last).
In a verb-first and verb-second sentence, the finite verb is theleft bracket (LK), whereas in a verb-last subordinateclause, the subordinating conjunction occupies thatfield.
In all clauses, the non-finite verb cluster formsthe right bracket (VC), and arguments and modifierscan appear in the middle field (MF) between the twobrackets.
Extraposed material is found to the rightof the right bracket, and in a verb-second sentenceone constituent appears in the fronted field (VF) pre-ceding the finite verb.
By specifying constraints onthe elements that can occur in the different fields,the word order in any type of German clause can beconcisely characterized.Each clause in the Tu?Ba-D/Z corpus is dividedinto topological fields at the top level, and each topo-logical field contains phrase-level annotation.
An26example sentence from Tu?Ba-D/Z is shown in Fig-ure 2, where the topological fields VF, LK, MF, andVC are visible under the SIMPX clause node.Daf?rPROPwirdVAFINAndreaNEFischerNEwenigPIATZeitNNhabenVAINF.$.HDPXHDVXFIN- -NX- HDNXHDVXINFOA-MODVFHDLK-EN-ADDOVVCON OAMF- - - -SIMPXVROOTfor it will Andrea Fischer little time haveFigure 2: Tu?Ba-D/Z tree for ?Andrea Fischer will havelittle time for it.
?Edge labels are used to mark heads and gram-matical functions, even though it can be nontrivialto figure out which grammatical function belongsto which head given that heads and their argumentsoften are in separate topological fields.
For exam-ple, in Figure 2 the subject noun chunk (NX) hasthe edge label ON (object - nominative) and the ob-ject noun chunk has the edge label OA (object - ac-cusative); both are realized within the middle field(MF), while the finite verb (VXFIN) marked as HD(head) is in the left sentence bracket (LK).
This is-sue becomes relevant in section 3.4.2, discussing anevaluation based on labeled dependency triples.Where Negra uses discontinuous constituents,Tu?Ba-D/Z uses special edge labels to annotate gram-matical relations which are not locally realized.
Forexample, the fronted prepositional phrase (PX) inFigure 2 has the edge label OA-MOD which needsto be matched with the noun phrase (NX) with labelOA that is found in the MF field.2.3 Comparing Negra and Tu?Ba-D/ZTo give an impression of how the different anno-tation schemes affect the appearance of a typicaltree in the two corpora, Table 1 provides statisticson average sentence length and the number of non-terminals per sentence.Negra Tu?Ba-D/ZNo.
of Sentences 20,602 22,091Terminals/Sentence 17.2 17.3Non-terminals/Sentence 7.0 20.7Table 1: General Characteristics of the CorporaWhile the sentences in Negra and Tu?Ba-D/Z onaverage have the same number of words, the averageTu?Ba-D/Z sentence has nearly three times as manynon-terminal nodes as the average Negra sentence.This difference is mainly due to the extra level oftopological fields annotation and the use of morecontoured structures in many places where Negrauses flatter structures.3 ExperimentsThe goal of the following experiments is a compar-ison of parsing performance across different typesof evaluation metrics for parsers trained on Negra(Ver.
2) and Tu?Ba-D/Z (Ver.
2).3.1 Data PreparationFollowing Ku?bler et al (2006), only sentences withfewer than 35 words were used, which results in20,002 sentences for Negra and 21,365 sentencesfor Tu?Ba-D/Z.
Because punctuation is not attachedwithin the sentence in the corpus annotation, punc-tuation was removed.To be able to train PCFG parsing models, it is nec-essary to convert the syntax graphs encoding treeswith discontinuities in Negra into traditional syntaxtrees.
Around 30% of sentences in Negra contain atleast one discontinuity.
To remove discontinuities,we used the conversion program included with theNegra corpus annotation tools (Brants and Plaehn,2000), the same tool used in Ku?bler et al (2006),which raises non-head elements to a higher tree un-til there are no more discontinuities.
For example,for the discontinuous tree with a fronted object wesaw in Figure 1, the PP containing the fronted NPDieser Meinung is raised to become a daughter ofthe top S node.4Additionally, the edge labels used in both corporaneed to be folded into the node labels to become a4An alternate method that avoids certain problems with thisraising method is discussed in Boyd (2007).27part of context-free grammar rules used by a PCFGparser.
In the Penn Treebank-style versions of thecorpora appropriate for training a PCFG parser, eachedge label is joined with the phrase or POS labelon the phrase or word immediately below it.
Bothcorpora include edge labels above all phrases andwords.
However the flatter structures in Negra resultin 39 different edge labels on words while Tu?Ba-D/Zhas only 5.Unlike Ku?bler et al (2006), which ignored edgelabels on words, we incorporate all edge labelspresent in both corpora.
As a consequence of this,providing a parser with perfect lexical tags wouldalso provide the edge label for that word.
Tu?Ba-D/Zdoes not annotate grammatical functions other thanHD on words, but Negra includes many grammati-cal functions on words.
Including edge labels in theperfect lexical tags would artificially boost the re-sults of a grammatical function evaluation for Negrasince it amounts to providing the correct grammati-cal function for the 38% of arguments in Negra thatare single words.To avoid this problem, we introduced non-branching phrasal nodes into Negra to prevent thecorrect grammatical function label from being pro-vided with the perfect lexical tag in the casesof single-word arguments, which are mostly barenouns and pronouns.
We added phrasal nodes aboveall single-word subject, accusative object, dative ob-ject, and genitive object5 arguments, with the cate-gory of the inserted phrase depending on the POStag on the word.
The introduced phrasal node isgiven the word?s original grammatical function la-bel; the grammatical function label of the word itselfbecomes NK for NPs and HD for APs and VPs.
Intotal, 14,580 nodes were inserted into Negra in thisway.
Tu?Ba-D/Z has non-branching phrases above allsingle-word arguments, so that no such modificationwas needed.63.2 Experimental SetupWe trained unlexicalized PCFG parsing models us-ing LoPar (Schmid, 2000).
Unlexicalized models5Genitive objects are modified for the sake of consistencyamong arguments even though there are too few genitive objectsto provide reliable results in the evaluation.6The addition of edge labels to terminal POS labels resultsin 337 lexical tags for Negra and 91 for Tu?Ba-D/Z.were used to minimize the impact of other corpusdifferences on parsing.
A ten-fold cross validationwas performed for all experiments.73.3 PARSEVAL EvaluationAs a reference point for comparison with previouswork, the PARSEVAL results8 are given in Table 2.Negra Tu?Ba-D/ZUnlabeled Precision 78.69 89.92Unlabeled Recall 82.29 86.48Labeled Precision 64.08 75.36Labeled Recall 67.01 72.47Coverage 97.00 99.90Table 2: PARSEVAL EvaluationThe parser trained on Tu?Ba-D/Z performs muchbetter than the one trained on Negra on all labeledand unlabeled bracketing scores.
As we saw insection 2, Negra and Tu?Ba-D/Z use very differentsyntactic annotation schemes, resulting in over 2.5times as many non-terminals per sentence in Tu?Ba-D/Z as in Negra with the additional unary nodes.As mentioned previously, Rehbein and van Genabith(2007a) showed that PARSEVAL is affected by theratio of terminal to non-terminal nodes, so these re-sults are not expected to indicate the quality of theparses.
The comparison with grammatical functionand dependency evaluations we turn to next show-cases that PARSEVAL does not provide a meaning-ful evaluation metric across annotation schemes.3.4 Dependency EvaluationComplementing the issue of the ratio of terminalsto non-terminals raised in the last section, one canquestion whether counting all brackets in the sen-tence equally, as done by the PARSEVAL metric,provides a good measure of how accurately the ba-sic functor-argument structure of the sentence hasbeen captured in a parse.
Thus, it is useful to per-7Our experimental setup is designed to support a compari-son between Negra and Tu?Ba-D/Z for the three evaluation met-rics and is intended to be comparable to the setup of Ku?bleret al (2006).
For Negra, Dubey (2004) explores a range of pars-ing models and the corpus preparation he uses differs from theone discussed in this paper so that a discussion of his results isbeyond the scope of the corpus comparison in this paper.8Scores were calculated using evalb.28form an evaluation based on the grammatical func-tion labels that are important for determining thefunctor-argument structure of the sentence: subjects,accusative objects, and dative objects.9 The firststep in an evaluation of functor-argument structureis to identify whether an argument bears the correctgrammatical function label.3.4.1 Grammatical Function Label EvaluationKu?bler et al (2006) present the results shown in Ta-ble 3 for the parsing performance of the unlexical-ized model of the Stanford Parser (Klein and Man-ning, 2002).
In this grammatical function label eval-uation, Tu?Ba-D/Z outperforms Negra for subjects,accusative objects, and dative objects based on anevaluation of phrasal arguments.Negra Tu?Ba-D/ZPrec Rec F Prec Rec FSubj 52.50 58.02 55.26 66.82 75.93 72.38Acc 35.14 36.30 35.72 43.84 47.31 45.58Dat 8.38 3.58 5.98 24.46 9.96 17.21Table 3: Grammatical Function Label Evaluation forPhrasal Arguments from Ku?bler et al (2006)Note that this grammatical function label evalua-tion is restricted to labels on phrases; grammaticalfunction labels on words are ignored in training andtesting.
This results in an unbalanced comparisonbetween Negra and Tu?Ba-D/Z since, as discussedin section 2, Tu?Ba-D/Z includes unary-branchingphrases above all single-word arguments whereasNegra does not.
In effect, single-word argumentsin Negra ?
mainly pronouns and bare nouns ?
arenot considered in the evaluation from Ku?bler et al(2006).
The result is thus a comparison of multi-word arguments in Negra to both single- and multi-word arguments in Tu?Ba-D/Z.
Recall from section3.1 that this is not a minor difference: single-wordarguments account for 38% of subjects, accusativeobjects, and dative objects in Negra.As discussed in the data preparation section, Ne-gra was modified for our experiment so as not to9Genitive objects are also annotated in both corpora, butthey are too infrequent to provide meaningful results.
As dis-cussed in Rehbein and van Genabith (2007b), labels such assubject (SB for Negra, ON for Tu?Ba-D/Z) are not necessarilycomparable in all instances, but such cases are infrequent.provide the parser with the grammatical function la-bels for single word phrases as part of the perfecttags provided.
This evaluation handles multiple cat-egories of arguments, not just NPs, so it focusessolely on the grammatical function labels, ignoringthe phrasal categories.
For example, in Negra an NP-OA in a parse is considered a correct accusative ob-ject even if the OA label in the gold standard has thecategory MPN.
The results are shown in Table 4.Negra Tu?Ba-D/ZPrec Rec F Prec Rec FSubj 69.69 69.12 69.42 65.74 72.24 68.99Acc 48.17 50.97 49.57 41.37 46.81 44.09Dat 20.93 15.22 18.08 21.40 11.51 16.46Table 4: Grammatical Function Label EvaluationIn contrast to the results for NP grammatical func-tions of Ku?bler et al (2006) we saw in Table 3, Ne-gra and Tu?Ba-D/Z perform quite similarly overall,with Negra slightly outperforming Tu?Ba-D/Z for alltypes of arguments.These results also form a clear contrast to thePARSEVAL results we saw in Table 2.
Contraryto the finding in Ku?bler et al (2006), the PAR-SEVAL evaluation does not echo the grammaticalfunction label evaluation.
In keeping with the re-sults from Rehbein and van Genabith (2007a), wefind that PARSEVAL is not an adequate predictor ofperformance in an evaluation targeting the functor-argument structure of the sentence for comparisonsbetween PCFG parsers trained on corpora with dif-ferent annotation schemes.3.4.2 Labeled Dependency Triple EvaluationWhile determining the grammatical function of anelement is an important part of determining thefunctor-argument structure of a sentence, the othernecessary component is determining the head ofeach function.
To evaluate whether both the functorand the argument have been correctly found, an eval-uation of labeled dependency triples is needed.
Asin the previous section, we focus on the grammaticalfunction labels for arguments of verbs.
To completea labeled dependency triple for each argument, weadditionally need to locate the lexical verbal head.In Negra, the head is the sister of an argu-ment marked with the function label ?HD?, however29heads are only marked for a subset of the phrase cat-egories: S, VP, AP, and AVP.10 This subset includesthe phrase categories that contain verbs and their ar-guments, S and VP.
In our experiment, the parserfinds the HD grammatical function labels with a veryhigh f-score: 99.5% precision and 96.5% recall.
Ifthe sister with the label HD is a word, then that wordis the lexical head for the purposes of this depen-dency evaluation.
If the sister with the label HD isa phrase, then a recursive search for heads withinthat phrase finds a lexical head.
In 3.2% of cases inthe gold standard, it is not possible to find a lexicalhead for an argument.
Further methods could be ap-plied to find the remaining heads heuristically, butwe avoid the additional parameters this introducesfor this evaluation by ignoring these cases.For Tu?Ba-D/Z, finding the head is not as simplebecause the verbal head and its arguments are in dif-ferent topological fields.
To create a parallel com-parison to Negra, the finite verb from the local clauseis chosen as the head for all subjects.
The (finite ornon-finite) main full verb is designated as the headfor the accusative and dative objects.
It is possibleto automatically find an appropriate head verb for allbut 2.7% of subjects, accusative objects, and dativeobjects.11 As with Negra, only cases where a headverb can be found in the gold standard are consid-ered in the evaluation.As in the grammatical function evaluation in theprevious section, only the grammatical function la-bel, not the phrase category is considered in the eval-uation.
The results for the labeled dependency eval-uation are shown in Table 5.
The parser trained onNegra outperforms the one trained on Tu?Ba-D/Z forall types of arguments.4 Discussion of ResultsComparing PARSEVAL scores for a parser trainedon the Negra and the Tu?Ba-D/Z corpus with a gram-matical function and a labeled dependency evalua-10However, some strings labeled as S and VP do not containa head and thus lack a daughter with a HD function label.11The relative numbers of instances where a lexical head isnot found are comparable for Negra and Tu?Ba-D/Z.
Heads arenot found for approximately 4% of subjects, 1% of accusativeobjects, and 1% of dative objects.
These instances are fre-quently due to elision of the verb in headlines and coordinatedclauses.Negra Tu?Ba-D/ZPrec Rec F Prec Rec FSubj 72.84 69.03 70.93 60.52 65.98 63.25Acc 47.96 48.80 48.38 37.39 40.83 39.11Dat 19.56 14.01 16.79 19.32 10.39 14.85Table 5: Labeled Dependency Evaluationtion, we confirm that the PARSEVAL scores do notcorrelate with the scores in the other two evalua-tions, which given their closeness to the semanticfunctor argument structure make meaningful targetsfor evaluating parsers.Shifting the focus to the grammatical functionevaluation, we showed that a grammatical functionevaluation based on phrasal arguments as providedby Ku?bler et al (2006) is inadequate for compar-ing parsers trained on the Negra and Tu?Ba-D/Z cor-pora.
By introducing non-branching phrase nodesabove single-word arguments in Negra, it is possi-ble to provide a balanced comparison for the gram-matical function label evaluation between Negra andTu?Ba-D/Z on both phrasal and single-word argu-ments.
The models trained on both corpora performvery similarly in the grammatical function evalua-tion, in contrast to the claims in Ku?bler et al (2006).When the grammatical function label evaluationis extended into a labeled dependency evaluation byfinding the verbal head to complete the labeled de-pendency triple, the parser trained on Negra outper-forms that trained on Tu?Ba-D/Z.
The more signifi-cant drop in results for Tu?Ba-D/Z compared to thegrammatical function label evaluation may be dueto the fact that a verbal lexical head in Tu?Ba-D/Z isnot in the same local tree as its dependents, whereasit is in Negra.
The presence of intervening topolog-ical field nodes in Tu?Ba-D/Z may make it difficultfor the parser to consistently identify the elementsof the dependency triple across several subtrees.The Negra corpus annotation scheme makes itsimple to identify the heads of verb arguments, butthe flat NP and PP structures make it difficult to ex-tend a labeled dependency analysis beyond verb ar-guments.
On the other hand, Tu?Ba-D/Z has markedheads in NPs and PPs, but it is not as easy to pairverb arguments with their heads because the verbsare in separate topological fields from their argu-30ments.
For a constituent-based corpus annotationscheme to lend itself to a thorough labeled depen-dency evaluation, heads should be marked clearlyfor all phrase categories and all non-head elementsneed to have marked grammatical functions.The presence of topological field nodes in Tu?Ba-D/Z deserves more discussion in relation to a gram-matical dependency evaluation.
The corpus con-tains two very different types of nodes in its syntac-tic trees: nodes such as NP and PP that correspondto constituents and nodes such as VF (Vorfeld) andMF (Mittelfeld) that correspond to word order do-mains.
Constituents such as NP have grammaticalrelations to other elements in the sentence and haveidentifiable heads within them, whereas nodes en-coding word order domains have neither.12 Whileconstituents and word order domains sometimes co-incide, such as the Vorfeld normally consisting of asingle constituent, this is not the general case.
Forexample, the Mittelfeld often contains multiple con-stituents which each stand in different grammaticalrelations to the verb(s) in the left and right sentencebrackets (LK and VC).Returning to the issue of finding dependencies be-tween constituents, the intervening word order do-main nodes can make it non-trivial to determinethese relations in Tu?Ba-D/Z.
For example, word or-der domain nodes will always intervene between averb and its arguments.
In order to have all gram-matical dependencies directly encoded in the tree-bank, it would be preferable for corpus annotationschemes to ensure that a homogeneous constituencyrepresentation can be easily obtained.5 Future WorkAn evaluation on arguments of verbs is just a firststep in working towards a more complete labeleddependency evaluation.
Because Negra and Tu?Ba-D/Z do not have parallel uses of many grammaticalfunction labels beyond arguments of verbs, a moredetailed evaluation on more types of dependency re-lations will require a complex dependency conver-sion method to provide comparable results.12While the focus in this work is on unlexicalized parsing,this also calls into question the effect of head lexicalization fora corpus that contains elements that by their nature are not thetypes of elements that have heads.Since previous work on head-lexicalized pars-ing models for German has focused on PARSEVALevaluations, it would also be useful to perform a la-beled dependency evaluation to determine what ef-fect head lexicalization has on particular construc-tions for the parsers.
Because of the concerns dis-cussed in the previous section and the difference inwhich types of clauses have marked heads in Negraand Tu?Ba-D/Z, the effect of head lexicalization onthe parsing results may differ for the two corpora.6 ConclusionAddressing the general question of how to compareparsing results for different annotation schemes, werevisited the comparison of PCFG parsing results forthe Negra and Tu?Ba-D/Z corpora.
We show thatthese different annotation schemes lead to very sig-nificant differences in PARSEVAL scores for un-lexicalized PCFG parsing models, but grammaticalfunction label and labeled dependency evaluationsfor arguments of verbs show that this difference doesnot carry over to measures which are relevant to thesemantic functor-argument structure.
In contrast toKu?bler et al (2006) a grammatical function evalua-tion on subjects, accusative objects, and dative ob-jects establishes that Negra and Tu?Ba-D/Z performsimilarly when all types of words and phrases ap-pearing as arguments are taken into consideration.
Alabeled dependency evaluation based on grammati-cal relations, which links this work to current workon formalism-independent parser evaluation (e.g.,Clark and Curran, 2007), shows that the parsing per-formance for Negra and Tu?Ba-D/Z is comparable.ReferencesAdriane Boyd, 2007.
Discontinuity Revisited: An Im-proved Conversion to Context-Free Representations.In Proceedings of the Linguistic Annotation Workshop(LAW).
Prague, Czech Republic.Thorsten Brants and Oliver Plaehn, 2000.
InteractiveCorpus Annotation.
In Proceedings of the Second In-ternational Conference on Language Resources andEvaluation (LREC).
Athens, Greece.Thorsten Brants, Wojciech Skut and Hans Uszkoreit,1999.
Syntactic Annotation of a German NewspaperCorpus.
In Proceedings of the ATALA Treebank Work-shop.
Paris, France.John Carroll, Guido Minnen and Ted Briscoe, 2003.Parser evaluation: using a grammatical relation anno-tation scheme.
In A. Abeille?
(ed.
), Treebanks: Build-ing and Using Parsed Corpora, Kluwer, Dordrecht.31Stephen Clark and James Curran, 2007.
Formalism-Independent Parser Evaluation with CCG and Dep-Bank.
In Proceedings of the 45th Annual Meeting ofthe Association of Computational Linguistics (ACL).Prague, Czech Republic.Amit Dubey, 2004.
Statistical Parsing for German: Mod-eling Syntactic Properties and Annotation Differences.Ph.D.
thesis, Universita?t des Saarlandes.Amit Dubey and Frank Keller, 2003.
Probabilistic Pars-ing Using Sister-Head Dependencies.
In Proceedingsof the 41st Annual Meeting of the Association for Com-putational Linguistics (ACL).
Sapporo, Japan.Tilman Ho?hle, 1986.
Der Begriff ?Mittelfeld?, An-merkungen u?ber die Theorie der topologischen Felder.In Akten des Siebten Internationalen Germanistenkon-gresses 1985.
Go?ttingen, Germany.Dan Klein and Christopher D. Manning, 2002.
Fast ExactInference with a Factored Model for Natural LanguageParsing.
In Advances in Neural Information Process-ing Systems 15 (NIPS).
Vancouver, British Columbia,Canada.Sandra Ku?bler, 2005.
How do treebank annotationschemes influence parsing results?
Or how not to com-pare apples and oranges.
In Proceedings of the Con-ference on Recent Advances in Natural Language Pro-cessing (RANLP).
Borovets, Bulgaria.Sandra Ku?bler, Erhard W. Hinrichs and Wolfgang Maier,2006.
Is it really that difficult to parse German?
InProceedings of the Conference on Empirical Methodsin Natural Language Processing (EMNLP).
Sydney,Australia.Roger Levy and Christopher Manning, 2003.
Is it harderto parse Chinese, or the Chinese Treebank?
In Pro-ceedings of the 41st Annual Meeting of the Associationfor Computational Linguistics (ACL).Dekang Lin, 1995.
A Dependency-based Method forEvaluating Broad-Coverage Parsers.
In Proceedingsof the International Joint Conference on Artificial In-telligence (IJCAI).
Montreal, Quebec, Canada.Ines Rehbein and Josef van Genabith, 2007a.
TreebankAnnotation Schemes and Parser Evaluation for Ger-man.
In Proceedings of the Joint Conference on Em-pirical Methods in Natural Language Processing andComputational Natural Language Learning.
Prague,Czech Republic.Ines Rehbein and Josef van Genabith, 2007b.
Why is it sodifficult to compare treebanks?
TIGER and Tu?Ba-D/Zrevisited.
In Proceedings of the Workshop on Tree-banks and Linguistic Theories (TLT).
Bergen, Norway.Marga Reis, 1980.
On Justifying Topological Frames:?Positional Field?
and the Order of Nonverbal Con-stituents in German.
Documentation et Rechercheen Linguistique Allemande Contemporaine Vincennes(DRLAV), 22/23.Anne Schiller, Simone Teufel and Christine Thielen,1995.
Guidelines fu?r das Tagging deutscher Textcor-pora mit STTS.
Technical report, Universita?t Stuttgart,Universita?t Tu?bingen, Germany.Helmut Schmid, 2000.
LoPar: Design and Implemen-tation.
Arbeitspapiere des Sonderforschungsbereiches340 No.
149, Universita?t Stuttgart.Heike Telljohann, ErhardW.
Hinrichs, Sandra Ku?bler andHeike Zinsmeister, 2005.
Stylebook for the Tu?bingenTreebank of Written German (Tu?Ba-D/Z).
Technicalreport, Seminar fu?r Sprachwissenschaft, Universita?tTu?bingen, Germany.32
