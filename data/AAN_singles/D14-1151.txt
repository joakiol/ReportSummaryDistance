Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1447?1452,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsUsing Mined Coreference Chains as a Resource for a Semantic TaskHeike Adel and Hinrich Sch?utzeCenter for Information and Language ProcessingUniversity of MunichGermanyheike.adel@cis.lmu.deAbstractWe propose to use coreference chains ex-tracted from a large corpus as a resourcefor semantic tasks.
We extract three mil-lion coreference chains and train wordembeddings on them.
Then, we com-pare these embeddings to word vectors de-rived from raw text data and show thatcoreference-based word embeddings im-prove F1on the task of antonym classifi-cation by up to .09.1 IntroductionAfter more than a decade of work on coreferenceresolution, coreference resolution systems havereached a certain level of maturity (e.g., Recasenset al.
(2010)).
While accuracy is far from perfectand many phenomena such as bridging still posedifficult research problems, the quality of the out-put of these systems is high enough to be usefulfor many applications.In this paper, we propose to run coreference res-olution systems on large corpora, to collect thecoreference chains found and to use them as a re-source for solving semantic tasks.
This amountsto using mined coreference chains as an automat-ically compiled resource similar to the way cooc-currence statistics, dependency pairs and alignedparallel corpora are used in many applications inNLP.
Coreference chains have interesting comple-mentary properties compared to these other re-sources.
For example, it is difficult to distinguishtrue semantic similarity (e.g., ?cows?
?
?cattle?
)from mere associational relatedness (e.g., ?cows??
?milk?)
based on cooccurrence statistics.
In con-trast, coreference chains should be able to makethat distinction since only ?cows?
and ?cattle?
canoccur in the same coreference chain, not ?cows?and ?milk?.As a proof of concept we compile a resourceof mined coreference chains from the Gigawordcorpus and apply it to the task of identifyingantonyms.
We induce distributed representationsfor words based on (i) cooccurrence statistics and(ii) mined coreference chains and show that a com-bination of both outperforms cooccurrence statis-tics on antonym identification.In summary, we make two contributions.
First,we propose to use coreference chains mined fromlarge corpora as a resource in NLP and publish thefirst such resource.
Second, in a proof of conceptstudy, we show that they can be used to solve a se-mantic task ?
antonym identification ?
better thanis possible with existing resources.We focus on the task of finding antonyms in thispaper since antonyms usually are distributionallysimilar but semantically dissimilar words.
Hence,it is often not possible to distinguish them fromsynonyms with distributional models only.
In con-trast, we expect that the coreference-based repre-sentations can provide useful complementary in-formation to this task.
In general, coreference-based similarity can however be used as an addi-tional feature for any task that distributional simi-larity is useful for.
Thus, our coreference resourcecan be applied to a variety of NLP tasks, e.g.
find-ing alternative names for entities (in a way similarto Wikipedia anchors) for tasks in the context ofknowledge base population.The remainder of the paper is organized as fol-lows.
In Section 2, we describe how we createword embeddings and how our antonym classi-fier works.
The word embeddings are then eval-uated qualitatively, quantitatively and for the taskof antonym detection (Section 3).
Section 4 dis-cusses related work and Section 5 concludes.2 System description2.1 Coreference-based embeddingsStandard word embeddings derived from text datamay not be able to distinguish between semantic1447text-based coref.-basedhis my, their, her, your, our he, him, himself, zechariah, ancestorwoman man, girl, believer, pharisee, guy girl, prostitute, lupita, betsy, lehiaTable 1: Nearest neighbors of ?his?
/ ?woman?
for text-based & coreference-based embeddingsassociation and true synonymy.
As a result, syn-onyms and antonyms may be mapped to similarword vectors (Yih et al., 2012).
For many NLPtasks, however, information about true synonymyor antonymy may be important.In this paper, we develop two different wordembeddings: embeddings calculated on raw textdata and embeddings derived from automaticallyextracted coreference chains.
For the calcula-tion of the vector representations, the word2vectoolkit1by Mikolov et al.
(2013) is applied.
Weuse the skip-gram model for our experiments be-cause its results for semantic similarity are betteraccording to Mikolov et al.
(2013).
We train afirst model on a subset of English Gigaword data.2In the following sections, we call the resultingembeddings text-based.
To improve the seman-tic similarities of the vectors, we prepare anothertraining text consisting of coreference chains.
Weuse CoreNLP (Lee et al., 2011) to extract coref-erence chains from the Gigaword corpus.
Thenwe build a skip-gram model on these coreferencechains.
The extracted coreference chains are pro-vided as an additional resource to this paper3.
Al-though they have been developed using only apublicly available toolkit, we expect this resourceto be helpful for other researchers since the pro-cess to extract the coreference chains of such alarge text corpus takes several weeks on multi-coremachines.
In total, we extracted 3.1M coreferencechains.
2.7M of them consist of at least two differ-ent markables.
The median (mean) length of thechains is 3 (4.0) and the median (mean) length ofa markable is 1 (2.7).
To train word embeddings,the markables of each coreference chain are con-catenated to one text line.
These lines are used asinput sentences for word2vec.
We refer to the re-sulting embeddings as coreference-based.2.2 Antonym detectionIn the following experiments, we use word em-beddings to discriminate antonyms from non-antonyms.
We formalize this as a supervised clas-1https://code.google.com/p/word2vec2LDC2012T21, Agence France-Presse 20103https://code.google.com/p/cisternsification task and apply SVMs (Chang and Lin,2011).The following features are used to represent apair of two words w and v:1. cosine similarity of the text-based embed-dings of w and v;2. inverse rank of v in the nearest text-basedneighbors of w;3. cosine similarity of the coreference-basedembeddings of w and v;4. inverse rank of v in the nearest coreference-based neighbors of w;5. difference of (1) and (3);6. difference of (2) and (4).We experiment with three different subsets ofthese features: text-based (1 and 2), coreference-based (3 and 4) and all features.3 Experiments and results3.1 Qualitative analysis of word vectorsTable 1 lists the five nearest neighbors based oncosine similarity of text-based and coreference-based word vectors for ?his?
and ?woman?.We see that the two types of embeddings cap-ture different notions of similarity.
Unlike the text-based neighbors, the coreference-based neighborshave the same gender.
The text-based neighborsare mutually substitutable words, but substitutionseems to change the meaning more than for thecoreference-based neighbors.In Figure 1, we illustrate the vectors for someantonyms (connected by lines).For reducing the dimensionality of the vectorspace to 2D, we applied the t-SNE toolkit4.
It usesstochastic neighbor embedding with a Student?st-distribution to map high dimensional vectorsinto a lower dimensional space (Van der Maatenand Hinton, 2008).
The Figure shows that thecoreference-based word embeddings are able to4http://homepage.tudelft.nl/19j49/t-SNE.html14481.5 1.0 0.5 0.0 0.5 1.01.41.21.00.80.60.40.20.00.2willingnessinnocenceguiltliteracyunwillingnesstoughnessilliteracyfrailty1.5 1.0 0.5 0.0 0.5 1.01.41.21.00.80.60.40.20.00.2willingnessinnocenceguiltunwillingnessfrailtytoughness literacyilliteracyFigure 1: 2D-positions of words in the text-based (top) and coreference-based embeddings (bottom)enlarge the distance between antonyms (especiallyfor guilt vs. innocence and toughness vs. frailty)compared to text-based word vectors.3.2 Quantitative analysis of word vectorsTo verify that coreference-based embeddings bet-ter represent semantic components relevant tocoreference, we split our coreference resource intotwo parts (about 85% and 15% of the data), trainedembeddings on the first part and computed the co-sine similarity ?
both text-based and coreference-based ?
for each pair of words occurring in thesame coreference chain in the second part.
Thestatistics in Table 2 confirm that coreference-basedvectors have higher similarity within chains thantext-based vectors.3.3 Experimental setupWe formalize antonym detection as a binary classi-fication task.
Given a target word w and one of itsnearest neighbors v, the classifier decides whetherv is an antonym of w. Our data set is a set of pairs,each consisting of a target word w and a candi-date v. For all word types of our vocabulary, wesearch for antonyms using the online dictionaryMerriam Webster.5The resulting list is providedas an additional resource6.
It contains 6225 wordswith antonyms.
Positive training examples are col-lected by checking if the 500 nearest text-basedneighbors of w contain one of the antonyms listedby Webster.
Negative training examples are cre-ated by replacing the antonym with a random wordfrom the 500 nearest neighbors that is not listed as5http://www.merriam-webster.com6https://code.google.com/p/cisternan antonym.
By selecting both the positive andthe negative examples from the nearest neighborsof the word vectors, we intend to develop a taskwhich is hard to solve: The classifier has to findthe small portion of semantically dissimilar words(i.e., antonyms) among distributionally very simi-lar words.
The total number of positive and nega-tive examples is 2337 each.
The data are split intotraining (80%), development (10%) and test (10%)sets.In initial experiments, we found only a smalldifference in antonym classification performancebetween text-based and coreference-based fea-tures.
When analyzing the errors, we realized thatour rationale for using coreference-based embed-dings only applies to nouns, not to other parts ofspeech.
This will be discussed in detail below.
Wetherefore run our experiments in two modes: allword classification (all pairs are considered) andnoun classification (only pairs are considered forwhich the target word is a noun).
We use the Stan-ford part-of-speech tagger (Toutanova et al., 2003)to determine whether a word is a noun or not.Our classifier is a radial basis function (rbf) sup-port vector machine (SVM).
The rbf kernel per-formed better than a linear kernel in initial exper-iments.
The SVM parameters C and ?
are opti-mized on the development set.
The representationof target-candidate pairs consists of the featuresdescribed in Section 2.3.4 Experimental results and discussionWe perform the experiments with the three differ-ent feature sets described in Section 2: text-based,coreference-based and all features.
Table 3 shows1449all word classification noun classificationdevelopment set test set development set test setfeature set P R F1P R F1P R F1P R F1text-based .83 .66 .74 .74 .55 .63 .91 .61 .73 .74 .51 .60coreference-based .67 .42 .51 .65 .43 .52 .86 .47 .61 .77 .45 .57text+coref .79 .65 .72 .75 .58 .66 .88 .70 .78 .79 .61 .69Table 3: Results for different feature sets.
Best result in each column in bold.minimum maximum mediantext-based vectors -0.350 0.998 0.156coref.-based vectors -0.318 0.999 0.161Table 2: Cosine similarity of words in the samecoreference chainresults for development and test sets.For all word classification, coreference-basedfeatures do not improve performance on the de-velopment set (e.g., F1is .74 for text-based vs .72for text+coref).
On the test set, however, the com-bination of all features (text+coref) has better per-formance than text-based alone: .66 vs .63.For noun classification, using coreference-based features in addition to text-based featuresimproves results on development set (F1is .78 vs.73) and test set (.69 vs .60).These results show that mined coreferencechains are a useful resource and provide infor-mation that is complementary to other methods.Even though adding coreference-based embed-dings improves performance on antonym classi-fication, the experiments also show that usingonly coreference-based embeddings is almost al-ways worse than using only text-based embed-dings.
This is not surprising given that the amountof training data for the word embeddings is differ-ent in the two cases.
Coreference chains provideonly a small subset of the word-word relations thatare given to the word2vec skip-gram model whenapplied to raw text.
If the sizes of the training datasets were similar in the two cases, we would ex-pect performance to be comparable.In the beginning, our hypothesis was that coref-erence information should be helpful for antonymclassification in general.
When we performed anerror analysis for our initial results, we realizedthat this hypothesis only holds for nouns.
Othertypes of words cooccurring in coreference chainsare not more likely to be synonyms than wordscooccurring in text windows.
Two contexts thatillustrate this point are ?bright sides, but also dif-ficult and dark ones?
and ?a series of black andwhite shots?
(elements of coreference chains initalics).
Thus, adjectives with opposite meaningscan cooccur in coreference chains just as they cancooccur in window-based contexts.
For nouns, itis much less likely that the same coreference chainwill contain both a noun and its antonym since ?by definition ?
markables in a coreference chainrefer to the same identical entity.4 Related workTraditionally, words have been represented byvectors of the size of the vocabulary with a one atthe word index and zeros otherwise (one-hot vec-tors).
However, this approach cannot handle un-known words (Turian et al., 2010) and similari-ties among words cannot be represented (Mikolovet al., 2013).
Therefore, distributed word repre-sentations (embeddings) become more and morepopular.
They are low-dimensional, real-valuedvectors.
Mikolov et al.
(2013) have publishedword2vec, a toolkit that provides different possi-bilities to estimate word embeddings (cbow modeland skip-gram model).
They show that the re-sulting word vectors capture semantic and syntac-tic relationships of words.
Baroni et al.
(2014)show that word embeddings are able to outper-form count based word vectors on a variety ofNLP tasks.
Recently, Levy and Goldberg (2014)have generalized the skip-gram model to includenot only linear but arbitrary contexts like contextsderived from dependency parse trees.
Andreas andKlein (2014) investigate the amount of additionalinformation continuous word embeddings couldadd to a constituency parser and find that mostof their information is redundant to what can belearned from labeled parse trees.
In (Yih et al.,2012), the vector space representation of words ismodified so that high positive similarities are as-signed to synonyms and high negative similaritiesto antonyms.
For this, latent semantic analysis isapplied to a matrix of thesaurus entries.
The val-1450ues representing antonyms are negated.There has been a great deal of work on apply-ing the vector space model and cosine similarityto find synonyms or antonyms.
Hagiwara et al.
(2006) represent each word as a vector with cooc-currence frequencies of words and contexts as el-ements, normalized by the inverse document fre-quency.
The authors investigate three types of con-textual information (dependency, sentence cooc-currence and proximity) and find that a combi-nation of them leads to the most stable results.Schulte im Walde and K?oper (2013) build a vectorspace model on lexico-syntactic patterns and ap-ply a Rocchio classifier to distinguish synonymsfrom antonyms, among other tasks.
Van der Plasand Tiedemann (2006) use automatically alignedtranslations of the same text in different languagesto build context vectors.
Based on these vectors,they detect synonyms.In contrast, there are also studies using linguis-tic knowledge from external resources: Senellartand Blondel (2008) propose a method for syn-onym detection based on graph similarity in agraph generated using the definitions of a mono-lingual dictionary.
Harabagiu et al.
(2006) rec-ognize antonymy by generating antonymy chainsbased on WordNet relations.
Mohammad et al.
(2008) look for the word with the highest degree ofantonymy to a given target word among five candi-dates.
For this task, they use thesaurus informationand the similarity of the contexts of two contrast-ing words.
Lin et al.
(2003) use Hearst patternsto distiguish synonyms from antonyms.
Work byTurney (2008) is similar except that the patternsare learned.Except for the publicly available coreferenceresolution system, our approach does not need ex-ternal resources such as dictionaries or bilingualcorpora and no human labor is required.
Thus,it can be easily applied to any corpus in any lan-guage as long as there exists a coreference resolu-tion system in this language.
The pattern-basedapproach (Lin et al., 2003; Turney, 2008) dis-cussed above also needs few resources.
In contrastto our work, it relies on patterns and might there-fore restrict the number of recognizable synonymsand antonyms to those appearing in the context ofthe pre-defined patterns.
On the other hand, pat-terns could explicitely distinguish contexts typicalfor synonyms from contexts for antonyms.
Hence,we plan to combine our coreference-based methodwith pattern-based methods in the future.5 ConclusionIn this paper, we showed that mined corefer-ence chains can be used for creating word em-beddings that capture a type of semantic sim-ilarity that is different from the one capturedby standard text-based embeddings.
We showedthat coreference-based embeddings improve per-formance of antonym classification by .09 F1compared to using only text-based embeddings.We achieved precision values of up to .79, recallvalues of up to .61 and F1scores of up to .69.AcknowledgmentsThis work was supported by DFG (grant SCHU2246/4-2).ReferencesJacob Andreas and Dan Klein.
2014.
How much doword embeddings encode about syntax?
In ACL,pages 822?827.Marco Baroni, Georgiana Dinu, and Germ?anKruszewski.
2014.
Don?t count, predict!
asystematic comparison of context-counting vs.context-predicting semantic vectors.
In ACL, pages238?247.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIB-SVM: a library for support vector machines.
ACMTransactions on Intelligent Systems and Technology(TIST), 2(3):27.Masato Hagiwara, Yasuhiro Ogawa, and KatsuhikoToyama.
2006.
Selection of effective contextualinformation for automatic synonym acquisition.
InCOLING/ACL, pages 353?360.Sanda Harabagiu, Andrew Hickl, and Finley Lacatusu.2006.
Negation, contrast and contradiction in textprocessing.
In AAAI, volume 6, pages 755?762.Heeyoung Lee, Yves Peirsman, Angel Chang,Nathanael Chambers, Mihai Surdeanu, and Dan Ju-rafsky.
2011.
Stanford?s multi-pass sieve corefer-ence resolution system at the CoNLL-2011 sharedtask.
In CoNLL: Shared Task, pages 28?34.Omer Levy and Yoav Goldberg.
2014.
Dependency-based word embeddings.
In ACL, pages 302?308.Dekang Lin, Shaojun Zhao, Lijuan Qin, and MingZhou.
2003.
Identifying synonyms among distribu-tionally similar words.
In IJCAI, pages 1492?1493.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient estimation of word represen-tations in vector space.
Workshop at ICLR.1451Saif Mohammad, Bonnie Dorr, and Graeme Hirst.2008.
Computing word-pair antonymy.
In EMNLP,pages 982?991.Marta Recasens, Llu?
?s M`arquez, Emili Sapena,M Ant`onia Mart?
?, Mariona Taul?e, V?eronique Hoste,Massimo Poesio, and Yannick Versley.
2010.Semeval-2010 task 1: Coreference resolution inmultiple languages.
In 5th International Workshopon Semantic Evaluation, pages 1?8.Sabine Schulte im Walde and Maximilian K?oper.
2013.Pattern-based distinction of paradigmatic relationsfor German nouns, verbs, adjectives.
In LanguageProcessing and Knowledge in the Web, pages 184?198.
Springer.Pierre Senellart and Vincent D Blondel.
2008.
Auto-matic discovery of similar words.
In Survey of TextMining II, pages 25?44.
Springer.Kristina Toutanova, Dan Klein, Christopher Manning,and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In NAACL-HLT, pages 252?259.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general methodfor semi-supervised learning.
In ACL, pages 384?394.Peter D. Turney.
2008.
A uniform approach to analo-gies, synonyms, antonyms, and associations.
InCOLING, pages 905?912.Laurens Van der Maaten and Geoffrey Hinton.
2008.Visualizing data using t-SNE.
Journal of MachineLearning Research, 9(11):2579?2605.Lonneke Van der Plas and J?org Tiedemann.
2006.Finding synonyms using automatic word alignmentand measures of distributional similarity.
In COL-ING/ACL, pages 866?873.Wen-tau Yih, Geoffrey Zweig, and John C Platt.2012.
Polarity inducing latent semantic analysis.
InEMNLP/CoNLL, pages 1212?1222.1452
