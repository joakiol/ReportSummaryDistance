Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 526?535,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsJoint Inference of Named Entity Recognition and Normalization for TweetsXiaohua Liu ?
?, Ming Zhou ?, Furu Wei ?, Zhongyang Fu ?, Xiangyang Zhou ?
?School of Computer Science and TechnologyHarbin Institute of Technology, Harbin, 150001, China?Department of Computer Science and EngineeringShanghai Jiao Tong University, Shanghai, 200240, China?School of Computer Science and TechnologyShandong University, Jinan, 250100, China?Microsoft Research AsiaBeijing, 100190, China?
{xiaoliu, fuwei, mingzhou}@microsoft.com?
zhongyang.fu@gmail.com ?
v-xzho@microsoft.comAbstractTweets represent a critical source of fresh in-formation, in which named entities occur fre-quently with rich variations.
We study theproblem of named entity normalization (NEN)for tweets.
Two main challenges are the er-rors propagated from named entity recogni-tion (NER) and the dearth of information ina single tweet.
We propose a novel graphi-cal model to simultaneously conduct NER andNEN on multiple tweets to address these chal-lenges.
Particularly, our model introduces abinary random variable for each pair of wordswith the same lemma across similar tweets,whose value indicates whether the two relatedwords are mentions of the same entity.
Weevaluate our method on a manually annotateddata set, and show that our method outper-forms the baseline that handles these two tasksseparately, boosting the F1 from 80.2% to83.6% for NER, and the Accuracy from 79.4%to 82.6% for NEN, respectively.1 IntroductionTweets, short messages of less than 140 charactersshared through the Twitter service 1, have becomean important source of fresh information.
As a re-sult, the task of named entity recognition (NER)for tweets, which aims to identify mentions of rigiddesignators from tweets belonging to named-entitytypes such as persons, organizations and locations(2007), has attracted increasing research interest.For example, Ritter et al (2011) develop a sys-tem that exploits a CRF model to segment named1http://www.twitter.comentities and then uses a distantly supervised ap-proach based on LabeledLDA to classify named en-tities.
Liu et al (2011) combine a classifier basedon the k-nearest neighbors algorithm with a CRF-based model to leverage cross tweets information,and adopt the semi-supervised learning to leverageunlabeled tweets.However, named entity normalization (NEN) fortweets, which transforms named entities mentionedin tweets to their unambiguous canonical forms, hasnot been well studied.
Owing to the informal natureof tweets, there are rich variations of named enti-ties in them.
According to our investigation on thedata set provided by Liu et al (2011), every namedentity in tweets has an average of 3.3 variations 2.As an illustrative example, we show ?Anneke Gron-loh?, which may occur as ?Mw.,Gronloh?, ?AnnekeKronloh?
or ?Mevrouw G?.
We thus propose NENfor tweets, which plays an important role in entityretrieval, trend detection, and event and entity track-ing.
For example, Khalid et al (2008) show thateven a simple normalization method leads to im-provements of early precision, for both documentand passage retrieval, and better normalization re-sults in better retrieval performance.Traditionally, NEN is regarded as a septated task,which takes the output of NER as its input (Li et al,2002; Cohen, 2005; Jijkoun et al, 2008; Dai et al,2011).
One limitation of this cascaded approach isthat errors propagate from NER to NEN and there isno feedback from NEN to NER.
As demonstrated byKhalid et al (2008), most NEN errors are caused2This data set consists of 12,245 randomly sampled tweetswithin five days.526by recognition errors.
Another challenge of NENis the dearth of information in a single tweet, dueto the short and noise-prone nature of tweets.
Re-portedly, the accuracy of a baseline NEN systembased on Wikipedia drops considerably from 94%on edited news to 77% on news comments, a kind ofuser generated content (UGC) with similar style totweets (Jijkoun et al, 2008).We propose jointly conducting NER and NENon multiple tweets using a graphical model, toaddress these challenges.
Intuitively, improvingthe performance of NER boosts the performanceof NEN.
For example, consider the following twotweets: ??
?
?Alex?s jokes.
Justin?s smartness.
Max?srandomnes?
?
?
?
and ??
?
?Alex Russo was like thebest character on Disney Channel?
?
?
?.
Identify-ing ?Alex?
and ?Alex Russo?
as PERSON will en-courage NEN systems to normalize ?Alex?
into?Alex Russo?.
On the other hand, NEN can guideNER.
For instance, consider the following twotweets: ??
?
?
she knew Burger King when he was aPrince!?
?
?
?
and ??
?
?
I?m craving all sorts of food:mcdonalds, burger king, pizza, chinese?
?
?
?.
Sup-pose the NEN system believes that ?burger king?cannot be mapped to ?Burger King?
since these twotweets are not similar in content.
This will help NERto assign them different types of labels.
Our methodoptimizes these two tasks simultaneously by en-abling them to interact with each other.
This largelydifferentiates our method from existing work.Furthermore, considering multiple tweets simul-taneously allows us to exploit the redundancy intweets, as suggested by Liu et al (2011).
For exam-ple, consider the following two tweets: ??
?
?BobbyShaw you don?t invite the wind?
?
?
?
and ??
?
?
I ownyah !
Loool bobby shaw?
?
?
?.
Recognizing ?BobbyShaw?
in the first tweet as a PERSON is easy owingto its capitalization and the following word ?you?,which in turn helps to identify ?bobby shaw?
in thesecond tweet as a PERSON.We adopt a factor graph as our graphical model,which is constructed in the following manner.
Wefirst introduce a random variable for each word inevery tweet, which represents the BILOU (Begin-ning, the Inside and the Last tokens of multi-tokenentities as well as Unit-length entities) label of thecorresponding word.
Then we add a factor to con-nect two neighboring variables, forming a conven-tional linear chain CRFs.
Hereafter, we use tm todenote the mth tweet ,tim and yim to denote the ithword of of tm and itsBILOU label, respectively, andf im to denote the factor related to yi?1m and yim.
Next,for each word pair with the same lemma, denoted bytim and tjn, we introduce a binary random variable,denoted by zijmn, whose value indicates whether timand tjn belong to two mentions of the same entity.
Fi-nally, for any zijmn we add a factor, denoted by f ijmn,to connect yim, yjn and zijmn.
Factors in the samegroup ({f ijmn} or {f im}) share the same set of fea-ture templates.
Figure 1 illustrates an example ofour factor graph for two tweets.Figure 1: A factor graph that jointly conducts NER andNEN on multiple tweets.
Blue and green circles rep-resent NE type (y-serials) and normalization variables(z-serials), respectively; filled circles indicate observedrandom variables; blue rectangles represent the factorsconnecting neighboring y-serial variables while red rect-angles stand for the factors connecting distant y-serialand z-serial variables.It is worth noting that our factor graph is differ-ent from the skip-chain CRFs (Galley, 2006) in thesense that any skip-chain factor of our model con-sists not only of two NE type variables (yim and yjn),which is the case for skip-chain CRFs, but also a nor-malization variable (zijmn).
It is these normalizationvariables that enable us to conduct NER and NENjointly.We manually add normalization information tothe data set shared by Liu et al (2011), to eval-uate our method.
Experimental results show thatour method achieves 83.6% F1 for NER and 82.6%Accuracy for NEN, outperforming the baseline with80.2%F1 for NER and 79.4% Accuracy for NEN.We summarize our contributions as follows.1.
We introduce the task of NEN for tweets, andpropose jointly conducting NER and NEN for527multiple tweets using a factor graph, whichleverages redundancy in tweets to make up forthe dearth of information in a single tweet andallows these two tasks to inform each other.2.
We evaluate our method on a human annotateddata set, and show that our method comparesfavorably with the baseline, achieving betterperformance in both tasks.Our paper is organized as follows.
In the next sec-tion, we introduce related work.
In Section 3 and 4,we formally define the task and present our method.In Section 5, we evaluate our method.
And finallywe conclude our work in Section 6.2 Related WorkRelated work can be divided into two categories:NER and NEN.2.1 NERNER has been well studied and its solutions can bedivided into three categories: 1) Rule-based (Krupkaand Hausman, 1998); 2) machine learning based(Finkel and Manning, 2009; Singh et al, 2010); and3) hybrid methods (Jansche and Abney, 2002).
Ow-ing to the availability of annotated corpora, such asACE05, Enron (Minkov et al, 2005) and CoNLL03(Tjong Kim Sang and De Meulder, 2003), datadriven methods are now dominant.Current studies of NER mainly focus on formaltext such as news articles (Mccallum and Li, 2003;Etzioni et al, 2005).
A representative work is thatof Ratinov and Roth (2009), in which they system-atically study the challenges of NER, compare sev-eral solutions, and show some interesting findings.For example, they show that the BILOU encodingscheme significantly outperforms the BIO schema(Beginning, the Inside and Outside of a chunk).A handful of work on other genres of texts exists.For example, Yoshida and Tsujii build a biomedi-cal NER system (2007) using lexical features, or-thographic features, semantic features and syntacticfeatures, such as part-of-speech (POS) and shallowparsing; Downey et al (2007) employ capitaliza-tion cues and n-gram statistics to locate names of avariety of classes in web text; Wang (2009) intro-duces NER to clinical notes.
A linear CRF modelis trained on a manually annotated data set, whichachieves an F1 of 81.48% on the test data set; Chiti-cariu et al (2010) design and implement a high-level language NERL which simplifies the processof building, understanding, and customizing com-plex rule-based named-entity annotators for differ-ent domains.Recently, NER for Tweets attracts growing inter-est.
Finin et al (2010) use Amazons Mechani-cal Turk service 3 and CrowdFlower 4 to annotatenamed entities in tweets and train a CRF model toevaluate the effectiveness of human labeling.
Rit-ter et al (2011) re-build the NLP pipeline fortweets beginning with POS tagging, through chunk-ing, to NER, which first exploits a CRF model tosegment named entities and then uses a distantly su-pervised approach based on LabeledLDA to clas-sify named entities.
Unlike this work, our work de-tects the boundary and type of a named entity si-multaneously using sequential labeling techniques.Liu et al (2011) combine a classifier based onthe k-nearest neighbors algorithm with a CRF-basedmodel to leverage cross tweets information, andadopt the semi-supervised learning to leverage un-labeled tweets.
Our method leverages redundancein similar tweets, using a factor graph rather than atwo-stage labeling strategy.
One advantage of ourmethod is that local and global information can in-teract with each other.2.2 NENThere is a large body of studies into normalizingvarious types of entities for formally written texts.For instance, Cohen (2005) normalizes gene/proteinnames using dictionaries automatically extractedfrom gene databases; Magdy et al (2007) addresscross-document Arabic name normalization using amachine learning approach, a dictionary of personnames and frequency information for names in acollection; Cucerzan (2007) demostrates a large-scale system for the recognition and semantic dis-ambiguation of named entities based on informa-tion extracted from a large encyclopedic collectionand Web search results; Dai et al (2011) employa Markov logic network to model interweaved con-3https://www.mturk.com/mturk/4http://crowdflower.com/528straints in a setting of gene mention normalization.Jijkoun et al (2008) study NEN for UGC.
Theyreport that the accuracy of a baseline NEN systembased on Wikipedia drops considerably from 94%on edited news to 77% on UGC.
They identify threemain error sources, i.e., entity recognition errors,multiple ways of referring to the same entity and am-biguous references, and exploit hand-crafted rules toimprove the baseline NEN system.We introduce the task of NEN for tweets, a newgenre of texts with rich entity variations.
In contrastto existing NEN systems, which take the output ofNER systems as their input, our method conductsNER and NEN at the same time, allowing them toreinforce each other, as demonstrated by the experi-mental results.3 Task DefinitionA tweet is a short text message with no more than140 characters.
Here is an example of a tweet: ?my-craftingworld: #Win Microsoft Office 2010 Homeand Student #Contest from @office http://bit.ly/ ?
?
?
?, where ?mycraftingworld?
is the name of the userwho published this tweet.
Words beginning with?#?
like ??#Win?
are hash tags; words startingwith ?@?
like ?@office?
represent user names; and?http://bit.ly/?
is a shortened link.Given a set of tweets, e.g., tweets within some pe-riod or related to some query, our task is: 1) To rec-ognize each mention of entities of predefined typesfor each tweet; and 2) to restore each entity mentioninto its unambiguous canonical form.
Following Liuet al (2011), we focus on four types of entities, i.e.,PERSON, ORGANIZATION, PRODUCT, and LO-CATION, and constrain our scope to English tweets.Note that the NEN sub-task can be transformed asfollows.
Given each pair of entity mentions, decidewhether they denote the same entity.
Once this isachieved, we can link all the mentions of the sameentity, and choose a representative mention, e.g., thelongest mention, as their canonical form.As an illustrative example, consider the followingthree tweets: ??
?
?Gaga?s Christmas dinner with herfamily.
Awwwwn?
?
?
?, ??
?
?Lady Gaaaaga with herfamily on Christmas?
?
?
?
and ??
?
?Buying a maga-zine just because Lady Gaga?s on the cover?
?
?
?.
Itis expected that ?Gaga?, ?Lady Gaaaaga?
and ?LadyGaga?
are all labeled as PERSON, and can be re-stored as ?Lady Gaga?.4 Our MethodIn contrast to existing work, our method jointlyconducts NER and NEN for multiple tweets.
Wefirst give an overview of our method, then detail itsmodel and features.4.1 OverviewGiven a set of tweets as input, our method recog-nizes predefined types of named entities and for eachentity outputs its unambiguous canonical form.To resolve NER, we assign a label to eachword in a tweet, indicating both the boundaryand entity type.
Following Ratinov and Roth(2009), we use the BILOU schema.
For ex-ample, consider the tweet ??
?
?without you islike an iphone without apps; Lady gaga with-out her telephone?
?
?
?, the labeled sequence us-ing the BILOU schema is: ??
?
?withoutO youOisO likeO anO iphoneU?PRODUCT withoutO appsO;LadyB?PERSON gagaL?PERSON withoutO herOtelephoneO?
?
?
?
, where ?iphoneU?PRODUCT ?
indi-cates that ?iphone?
is a product name of unit length;?LadyB?PERSON?
means ?Lady?
is the beginningof a person name while ?gagaL?PERSON?
suggeststhat ?gaga?
is the last token of a person name.To resolve NEN, we assign a binary value labelzijmn to each pair of words tim and tjn which share thesame lemma.
zijmn = 1 or -1, indicating whether timand tjn belong to two mentions of the same entity 5.For example, consider the three tweets presented inSection 3.
?Gaga11?
6 and ?Gaga13?
will be assigneda ?1?
label, since they are part of two mentions of thesame entity ?Lady Gaga?
; similarly, ?Lady12?
and?Lady13?
are connected with a ?1?
label.
Note thatthere are no NEN labels for pairs like ?her11?
and?her12?
or ?with11 and ?with12?, since words like ?her?and ?with?
are stop words.With NE type and normalization labels obtained,we judge two mentions, denoted by ti1??
?ikm and5Stop words have no normalization labels.
The stop wordsare mainly from http://www.textfixer.com/resources/common-english-words.txt.6We use wim to denote word w?s ith appearance in the mthtweet.
For example, ?Gaga11?
denotes the first occurance of?Gaga?
in the first tweet.529tj1??
?jln , respectively, refer to the same entity if andonly if: 1) The two mentions share the same entitytype; 2) ti1??
?ikm is a sub-string of tj1??
?jln or vise versa;and 3) zijmn = 1, i = i1, ?
?
?
, ik and j = j1, ?
?
?
, jl,if zijmn exists.
Still take the three tweets presentedin Section 3 for example.
Suppose ?Gaga11?
and?Lady Gaga13?
are labeled as PERSON, and thereis only one related NE normalization label, whichis associated with ??Gaga11?
and ?Gaga13?
and has 1as its value.
We then consider that these two men-tions can be normalized into the same entity; in asimilar way, we can align ?Lady12 Gaaaaga?
with?Lady13 Gaga?.
Combining these pieces informa-tion together, we can infer that ?
?Gaga11?, ?Lady12Gaaaaga?
and ?Lady13 Gaga?
are three mentions ofthe same entity.
Finally, we can select ?Lady13 Gaga?as the representative, and output ?Lady Gaga?
astheir canonical form.
We choose the mention withthe maximum number of words as the representa-tive.
In case of a tie, we prefer the mention with anWikipedia entry 7.The central problem with our method is infer-ring all the NE type (y-serial) and normalization(z-serial) variables.
To achieve this, we constructa factor graph according to the input tweets, whichcan evaluate the probability of every possible assign-ment of y-serials and z-serials, by checking thecharacteristics of the assignment.
Each character-istic is called a feature.
In this way, we can selectthe assignment with the highest probability.
Nextwe will introduce our model in detail, including itstraining and inference procedure and features.4.2 ModelWe adopt a factor graph as our model.
One advan-tage of our model is that it allows y-serials andz-serials variables to interact with each other tojointly optimize NER and NEN.Given a set of tweets T = {tm}Nm=1, we can builda factor graph G = (Y,Z, F,E), where: Y and Zdenote y-serials and z-serials variables, respec-tively; F represents factor vertices, consisting of{f im} and {fijmn}, f im = f im(yi?1m , yim) and fijmn =f ijmn(yim, yjn, zijmn); E stands for edges, which de-pends on F , and consists of edges between yi?1m andyim, and those between yim,yjn and f ijmn.7If it still ends up as a draw, we will randomly choose onefrom the best.G = (Y, Z, F,E) defines a probability distribu-tion according to Formula 1.lnP (Y, Z|G, T ) ?
?m,iln f im(yi?1m , yim)+?m,n,i,j?ijmn ?
ln f ijmn(yim, yjn, zijmn)(1)where ?ijmn = 1 if and only if tim and tjn have thesame lemma and are not stop words, otherwise zero.A factor factorizes according to a set of features, sothat:ln f im(yi?1m , yim) =?k?
(1)k ?
(1)k (yi?1m , yim)ln f ijmn(yim, yjn, zijmn) =?k?
(2)k ?
(2)k (yim, yjn, zijmn)(2){?
(1)k }K1k=1 and {?
(2)k }K2k=1 are two feature sets.
?
={?
(1)k }K1k=1?{?
(2)k }K2k=1 is called the feature weightset or parameter set of G. Each feature has a realvalue as its weight.Training ?
is learnt from annotated tweets T , bymaximizing the data likelihood, i.e.,??
= argmax?lnP (Y,Z|?, T ) (3)To solve this optimization problem, we first calcu-late its gradient:?
lnP (Y, Z|T ; ?)??1k=?m,i?
(1)k (yi?1m , yim)?
?m,i?yi?1m ,yimp(yi?1m , yim|T ; ?)?
(1)k (yi?1m , yim)(4)?
lnP (Y, Z|T ; ?)?
?2k=?m,n,i,j?ijmn ?
?
(2)k (yim, yjn, zijmn)?
?m,n,i,j?ijmn?yim,yjn,zijmnp(yim, yjn, zijmn|T ; ?)??
(2)k (yim, yjn, zijmn)(5)Here, the two marginal probabilitiesp(yi?1m , yim|T ; ?)
and p(yim, yjn, zijmn|T ; ?)
arecomputed using loopy belief propagation (Murphyet al, 1999).
Once we have computed the gradient,??
can be worked out by standard techniques suchas steepest descent, conjugate gradient and the530limited-memory BFGS algorithm (L-BFGS).
Wechoose L-BFGS because it is particularly well suitedfor optimization problems with a large number ofvariables.Inference Supposing the parameters ?
have beenset to ?
?, the inference problem is: Given a setof testing tweets T , output the most probableassignment of Y and Z, i.e.,(Y, Z)?
= argmax(Y,Z)lnP (Y,Z|?
?, T ) (6)We adopt the max-product algorithm to solve thisinference problem.
The max-product algorithm isnearly identical to the loopy belief propagation al-gorithm, with the sums replaced by maxima in thedefinitions.
Note that in both the training and test-ing stage, the factor graph is constructed in the sameway as described in Section 1.Efficiency We take several actions to improve ourmodel?s efficiency.
Firstly, we manually compile acomprehensive named entity dictionary from vari-ous sources including Wikipedia, Freebase 8, newsarticles and the gazetteers shared by Ratinov andRoth (2009).
In total this dictionary contains 350million entries 9.
By looking up this dictionary 10,we generate the possible BILOU labels, denoted byY im hereafter, for each word tim.
For instance, con-sider ??
?
?Good Morning new11 york11?
?
?
?.
Suppose?New York City?
and ?New York Times?
are inour dictionary, then ?new11 york11?
is the matchedstring with two corresponding entities.
As a re-sult, ?B-LOCATION?
and ?B-ORGANIZATION?will be added to Ynew11 , and ?I-LOCATION?
and?I-ORGANIZATION?
will be added to Yyork11 .
IfY im ?= ?, we enforce the constraint for training andtesting that yim ?
Y im , to reduce the search space.Secondly, in the testing phase, we introduce threerules related to zijmn: 1) zijmm = 1, which says twowords sharing the same lemma in the same tweetdenote the same entity; 2) set zijmn to 1, if the sim-ilarity between tm and tn is above a threshold (0.8in our work), or tm and tn share one hash tag; and3)zmnij = ?1, if the similarity between tm andtn is below a threshold (0.3 in work).
To compute8http://freebase.com/view/military9One phrase refereing to L entities has L entries.10We use case-insensitive leftmost longest match.the similarity, each tweet is represented as a bag-of-words vector with the stop words removed, and thecosine similarity is adopted, as defined in Formula7.
These rules pre-label a significant part of z-serialvariables (accounting for 22.5%), with an accuracyof 93.5%.sim(tm, tn) =t?m ?
t?n|?tm||?tn|(7)Note that in our experiments, these measures reducethe training and testing time by 36.2% and 62.8%,respectively, while no obvious performance drop isobserved.4.3 FeaturesA feature in {?
(1)k }K1k=1 involves a pair of neighbor-ing NE-type labels, i.e., yi?1m and yim, while a fea-ture in {?
(2)k }K2k=1 concerns a pair of distant NE-typelabels and its associated normalization label, i.e.,yim,yjn and zijmn.
Details are given below.4.3.1 Feature Set One: {?
(1)k }K1k=1We adopts features similar to Wang (2009), andRatinov and Roth (2009), i.e., orthographic features,lexical features and gazetteer-related features.
Thesefeatures are defined on the observation.
Combiningthem with yi?1m and yim constitutes {?
(1)k }K1k=1.Orthographic features: Whether tim is capitalizedor upper case; whether it is alphanumeric or containsany slashes; wether it is a stop word; word prefixesand suffixes.Lexical features: Lemma of tim, ti?1m and ti+1m ,respectively; whether tim is an out-of-vocabulary(OOV) word 11; POS of tim, ti?1m and ti+1m , respec-tively; whether tim is a hash tag, a link, or a useraccount.Gazetteer-related features: Whether Y im is empty;the dominating label/entity type in Y im.
Which oneis dominant is decided by majority voting of the en-tities in our dictionary.
In case of a tie, we randomlychoose one from the best.4.3.2 Feature Set Two: {?
(2)k }K2k=1Similarly, we define orthographic, lexical featuresand gazetteer-related features on the observation, yim11We first conduct a simple dictionary-lookup based normal-ization with the incorrect/correct word pair list provided by Hanet al (2011) to correct common ill-formed words.
Then we callan online dictionary service to judge whether a word is OOV.531and yjn; and then we combine these features withzijmn, forming {?
(2)k }K2k=1.Orthographic features: Whether tim / tjn is capital-ized or upper case; whether tim / tjn is alphanumericor contains any slashes; prefixes and suffixes of tim.Lexical features: Lemma of tim; whether tim isOOV; whether tim / ti+1m / ti?1m and tjn / tj+1n / tj?1nhave the same POS; whether yim and yjn have thesame label/entity type.Gazetteer-related features: Whether Y im?Y jn /Y i+1m?Y j+1n / Y i?1m?Y j?1n is empty; whether thedominating label/entity type in Y im is the same asthat in Y jn .5 ExperimentsWe manually annotate a data set to evaluate ourmethod.
We show that our method outperforms thebaseline, a cascaded system that conducts NER andNEN individually.5.1 Data PreparationWe use the data set provided by Liu et al (2011),which consists of 12,245 tweets with four types ofentities annotated: PERSON, LOCATION, ORGA-NIZATION and PRODUCT.
We enrich this data setby adding entity normalization information.
Twoannotators 12 are involved.
For any entity mention,two annotators independently annotate its canonicalform.
The inter-rater agreement measured by kappais 0.72.
Any inconsistent case is discussed by thetwo annotators till a consensus is reached.
2, 245tweets are used for development, and the remainderare used for 5-fold cross validation.5.2 Evaluation MetricsWe adopt the widely-used Precision, Recall and F1to measure the performance of NER for a partic-ular type of entity, and the average Precision, Re-call and F1 to measure the overall performance ofNER (Liu et al, 2011; Ritter et al, 2011).
As forNEN, we adopt the widely-used Accuracy, i.e., towhat percentage the outputted canonical forms arecorrect (Jijkoun et al, 2008; Cucerzan, 2007; Li etal., 2002).12Two native English speakers.5.3 BaselineWe develop a cascaded system as the baseline,which conducts NER and NEN sequentially.
ItsNER module, denoted by SBR, is based on the state-of-the-art method introduced by Liu et al (2011);and its NEN model , denoted by SBN , followsthe NEN system for user-generated news commentsproposed by Jijkoun et al (2008), which useshandcrafted rules to improve a typical NEN systemthat normalizes surface forms to Wikipedia page ti-tles.
We use the POS tagger developed by Ritter etal.
(2011) to extract POS related features, and theOpenNLP toolkit to get lemma related features.5.4 ResultsTables 1- 2 show the overall performance of thebaseline and ours (denoted by SRN ).
It can beseen that, our method yields a significantly higherF1 (with p < 0.01) than SBR, and a moderate im-provement of accuracy as compared with SBN (withp < 0.05).
As a case study, we show that our systemsuccessfully identified ?jaxon11?
as a PERSON in thetweet ??
?
?
come to see jaxon11 someday?
?
?
?, whichis mistakenly labeled as a LOCATION by SBR.This is largely owing to the fact that our systemaligns ?jaxon11?
with ?Jaxson12?
in the tweet ??
?
?
Ilove Jaxson12,Hes like my little brother?
?
?
?, in which?Jaxson12?
is identified as a PERSON.
As a result,this encourages our system to consider ?jaxon11?
asa PERSON.
We also find cases where our systemworks but SBN fails.
For example, ?Goldman11?in the tweet ??
?
?Goldman sees massive upside riskin oil prices?
?
?
?
is normalized into ?Albert Gold-man?
by SBR, because it is mistakenly identified asa PERSON by SBS ; in contrast, our system recog-nizes ?Goldman12 Sachs?
as an ORGANIZATION,and successfully links ?Goldman12?
to ?Goldman11?,resulting that ?Goldman11?
is identified as an ORGA-NIZATION and normalized into ?Goldman Sachs?.Table 3 reports the NER performance of ourmethod for each entity type, from which we see thatour system consistently yields better F1 on all entitytypes than SBR.
We also see that our system booststhe F1 for ORGANIZATION most significantly, re-flecting the fact that a large number of organizationsthat are incorrectly labeled as PERSON by SBR, arenow correctly recognized by our method.532System Pre Rec F1SRN 84.7 82.5 83.6SBR 81.6 78.8 80.2Table 1: Overall performance (%) of NER.System AccuracySRN 82.6SBN 79.4Table 2: Overall Accuracy (%) of NEN .System PER PRO LOC ORGSRN 84.2 80.5 82.1 85.2SBR 83.9 78.7 81.3 79.8Table 3: F1 (%) of NER on different entity types.Features NER (F1) NEN (Accuracy)Fo 59.2 61.3Fo + Fl 65.8 68.7Fo + Fg 80.1 77.2Fo + Fl + Fg 83.6 82.6Table 4: Overall F1 (%) of NER and Accuracy (%) ofNEN with different feature sets.Table 4 shows the overall performance of ourmethod with various feature set combinations,where Fo, Fl and Fg denote the orthographic fea-tures, the lexical features, and the gazetteer-relatedfeatures, respectively.
From Table 4 we see thatgazetteer-related features significantly boost the F1for NER and Accuracy for NEN, suggesting the im-portance of external knowledge for this task.5.5 DiscussionOne main error source for NER and NEN, whichaccounts for more than half of all the errors, isslang expressions and informal abbreviations.
Forinstance, our method recognizes ?California11?
inthe tweet ??
?
?And Now, He Lives All The Way InCalifornia11?
?
?
?
as a LOCATION, however, it mis-takenly identifies ?Cali12?
in the tweet ??
?
?
i loveCali so much?
?
?
?
as a PERSON.
One reason is oursystem does not generate any z-serial variable for?California11?
and ?Cali12?
since they have differ-ent lemmas.
A more complicated case is ?BS11?
inthe tweet ??
?
?
I, bobby shaw, am gonna put BS11 oneverything?
?
?
?, in which ?BS11?
is the abbreviationof ?bobby shaw?.
Our method fails to recognize?BS11?
as an entity.
There are two possible ways tofix these errors: 1) Extending the scope of z-serialvariables to each word pairs with a common prefix;and 2) developing advanced normalization compo-nents to restore such slang expressions and informalabbreviations into their canonical forms.Our method does not directly exploit Wikipediafor NEN.
This explains the cases where our systemcorrectly links multiple entity mentions but fails togenerate canonical forms.
Take the following twotweets for example: ??
?
?
nitip link win711 sp1?
?
?
?and ??
?
?Hit the 3TB wall on SRT installing freshWin712?
?
?
?.
Our system recognizes ?win711?
and?Win712?
as two mentions of the same product, butcannot output their canonical forms ?Windows 7?.One possible solution is to exploit Wikipedia tocompile a dictionary consisting of entities and theirvariations.6 Conclusions and Future workWe study the task of NEN for tweets, a new genreof texts that are short and prone to noise.
Two chal-lenges of this task are the dearth of information ina single tweet and errors propagated from the NERcomponent.
We propose jointly conducting NERand NEN for multiple tweets using a factor graph, toaddress these challenges.
One unique characteristicof our model is that a NE normalization variable isintroduced to indicate whether a word pair belongsto the mentions of the same entity.
We evaluate ourmethod on a manually annotated data set.
Experi-mental results show our method yields better F1 forNER and Accuracy for NEN than the state-of-the-artbaseline that conducts two tasks sequentially.In the future, we plan to explore two directions toimprove our method.
First, we are going to developadvanced tweet normalization technologies to re-solve slang expressions and informal abbreviations.Second, we are interested in incorporating knowl-edge mined from Wikipedia into our factor graph.AcknowledgmentsWe thank Yunbo Cao, Dongdong Zhang, and Mu Lifor helpful discussions, and the anonymous review-ers for their valuable comments.533ReferencesLaura Chiticariu, Rajasekar Krishnamurthy, YunyaoLi, Frederick Reiss, and Shivakumar Vaithyanathan.2010.
Domain adaptation of rule-based annotatorsfor named-entity recognition tasks.
In EMNLP, pages1002?1012.Aaron Cohen.
2005.
Unsupervised gene/protein namedentity normalization using automatically extracted dic-tionaries.
In Proceedings of the ACL-ISMB Work-shop on Linking Biological Literature, Ontologies andDatabases: Mining Biological Semantics, pages 17?24, Detroit, June.
Association for Computational Lin-guistics.Silviu Cucerzan.
2007.
Large-scale named entity disam-biguation based on wikipedia data.
In In Proc.
2007Joint Conference on EMNLP and CNLL, pages 708?716.Hong-Jie Dai, Richard Tzong-Han Tsai, and Wen-LianHsu.
2011.
Entity disambiguation using a markov-logic network.
In Proceedings of 5th InternationalJoint Conference on Natural Language Processing,pages 846?855, Chiang Mai, Thailand, November.Asian Federation of Natural Language Processing.Doug Downey, Matthew Broadhead, and Oren Etzioni.2007.
Locating Complex Named Entities in Web Text.In IJCAI.Oren Etzioni, Michael Cafarella, Doug Downey, Ana-Maria Popescu, Tal Shaked, Stephen Soderland,Daniel S. Weld, and Alexander Yates.
2005.
Unsu-pervised named-entity extraction from the web: an ex-perimental study.
Artif.
Intell., 165(1):91?134.Tim Finin, Will Murnane, Anand Karandikar, NicholasKeller, Justin Martineau, and Mark Dredze.
2010.Annotating named entities in twitter data with crowd-sourcing.
In CSLDAMT, pages 80?88.Jenny Rose Finkel and Christopher D. Manning.
2009.Nested named entity recognition.
In EMNLP, pages141?150.Michel Galley.
2006.
A skip-chain conditional randomfield for ranking meeting utterances by importance.In Association for Computational Linguistics, pages364?372.Bo Han and Timothy Baldwin.
2011.
Lexical normalisa-tion of short text messages: Makn sens a #twitter.
InACL HLT.Martin Jansche and Steven P. Abney.
2002.
Informa-tion extraction from voicemail transcripts.
In EMNLP,pages 320?327.Valentin Jijkoun, Mahboob Alam Khalid, Maarten Marx,and Maarten de Rijke.
2008.
Named entity normal-ization in user generated content.
In Proceedings ofthe second workshop on Analytics for noisy unstruc-tured text data, AND ?08, pages 23?30, New York,NY, USA.
ACM.Mahboob Khalid, Valentin Jijkoun, and Maarten de Ri-jke.
2008.
The impact of named entity normaliza-tion on information retrieval for question answering.In Craig Macdonald, Iadh Ounis, Vassilis Plachouras,Ian Ruthven, and Ryen White, editors, Advances in In-formation Retrieval, volume 4956 of Lecture Notes inComputer Science, pages 705?710.
Springer Berlin /Heidelberg.George R. Krupka and Kevin Hausman.
1998.
Isoquest:Description of the netowlTM extractor system as usedin muc-7.
In MUC-7.Huifeng Li, Rohini K. Srihari, Cheng Niu, and Wei Li.2002.
Location normalization for information extrac-tion.
In COLING.Xiaohua Liu, Shaodian Zhang, Furu Wei, and MingZhou.
2011.
Recognizing named entities in tweets.In ACL.Walid Magdy, Kareem Darwish, Ossama Emam, andHany Hassan.
2007.
Arabic cross-document personname normalization.
In In CASL Workshop 07, pages25?32.Andrew Mccallum and Wei Li.
2003.
Early resultsfor named entity recognition with conditional randomfields, feature induction and web-enhanced lexicons.In HLT-NAACL, pages 188?191.Einat Minkov, Richard C. Wang, and William W. Cohen.2005.
Extracting personal names from email: apply-ing named entity recognition to informal text.
In HLT,pages 443?450.Kevin P. Murphy, Yair Weiss, and Michael I. Jordan.1999.
Loopy belief propagation for approximate in-ference: An empirical study.
In In Proceedings of Un-certainty in AI, pages 467?475.David Nadeau and Satoshi Sekine.
2007.
A survey ofnamed entity recognition and classification.
Linguisti-cae Investigationes, 30:3?26.Lev Ratinov and Dan Roth.
2009.
Design challengesand misconceptions in named entity recognition.
InCoNLL, pages 147?155.Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.2011.
Named entity recognition in tweets: An ex-perimental study.
In Proceedings of the 2011 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 1524?1534, Edinburgh, Scotland, UK.,July.
Association for Computational Linguistics.Sameer Singh, Dustin Hillard, and Chris Leggetter.
2010.Minimally-supervised extraction of entities from textadvertisements.
In HLT-NAACL, pages 73?81.Erik F. Tjong Kim Sang and Fien De Meulder.
2003.
In-troduction to the CoNLL-2003 shared task: language-independent named entity recognition.
In HLT-NAACL, pages 142?147.534Yefeng Wang.
2009.
Annotating and recognising namedentities in clinical notes.
In ACL-IJCNLP, pages 18?26.Kazuhiro Yoshida and Jun?ichi Tsujii.
2007.
Rerankingfor biomedical named-entity recognition.
In BioNLP,pages 209?216.535
