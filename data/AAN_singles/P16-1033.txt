Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 344?354,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsActive Learning for Dependency Parsing with Partial AnnotationZhenghua Li?, Min Zhang?
?, Yue Zhang?, Zhanyi Liu?,Wenliang Chen?, Hua Wu?, Haifeng Wang??
Soochow University, Suzhou, China{zhli13,minzhang,wlchen}@suda.edu.cn, zhangyue1107@qq.com?
Baidu Inc., Beijing, China{liuzhanyi,wu hua,wanghaifeng}@baidu.comAbstractDifferent from traditional active learningbased on sentence-wise full annotation(FA), this paper proposes activelearning with dependency-wise partialannotation (PA) as a finer-grained unit fordependency parsing.
At each iteration,we select a few most uncertain wordsfrom an unlabeled data pool, manuallyannotate their syntactic heads, and add thepartial trees into labeled data for parserretraining.
Compared with sentence-wiseFA, dependency-wise PA gives us moreflexibility in task selection and avoidswasting time on annotating trivial tasksin a sentence.
Our work makes thefollowing contributions.
First, we arethe first to apply a probabilistic model toactive learning for dependency parsing,which can 1) provide tree probabilitiesand dependency marginal probabilitiesas principled uncertainty metrics, and2) directly learn parameters from PAbased on a forest-based training objective.Second, we propose and compare severaluncertainty metrics through simulationexperiments on both Chinese and English.Finally, we conduct human annotationexperiments to compare FA and PA onreal annotation time and quality.1 IntroductionDuring the past decade, supervised dependencyparsing has gained extensive progress in boostingparsing performance on canonical texts, especiallyon texts from domains or genres similar to exist-ing manually labeled treebanks (Koo and Collins,2010; Zhang and Nivre, 2011).
However, the?Correspondence author.$0I1saw2Sarah3with4a5telescope6Figure 1: A partially annotated sentence, whereonly the heads of ?saw?
and ?with?
are decided.upsurge of web data (e.g., tweets, blogs, andproduct comments) imposes great challenges toexisting parsing techniques.
Meanwhile, previousresearch on out-of-domain dependency parsinggains little success (Dredze et al, 2007; Petrovand McDonald, 2012).
A more feasible way foropen-domain parsing is to manually annotate acertain amount of texts from the target domain orgenre.
Recently, several small-scale treebanks onweb texts have been built for study and evaluation(Foster et al, 2011; Petrov and McDonald, 2012;Kong et al, 2014; Wang et al, 2014).Meanwhile, active learning (AL) aims to reduceannotation effort by choosing and manually an-notating unlabeled instances that are most valu-able for training statistical models (Olsson, 2009).Traditionally, AL utilizes full annotation (FA) forparsing (Tang et al, 2002; Hwa, 2004; Lynn et al,2012), where a whole syntactic tree is annotatedfor a given sentence at a time.
However, ascommented by Mejer and Crammer (2012), theannotation process is complex, slow, and proneto mistakes when FA is required.
Particularly,annotators waste a lot of effort on labeling trivialdependencies which can be well handled by cur-rent statistical models (Flannery and Mori, 2015).Recently, researchers report promising resultswith AL based on partial annotation (PA) for de-pendency parsing (Sassano and Kurohashi, 2010;Mirroshandel and Nasr, 2011; Majidi and Crane,2013; Flannery and Mori, 2015).
They find344that smaller units rather than sentences providemore flexibility in choosing potentially informa-tive structures to annotate.Beyond previous work, this paper endeavors tomore thoroughly study this issue, and has madesubstantial progress from the following perspec-tives.
(1) This is the first work that applies a state-of-the-art probabilistic parsing model to ALfor dependency parsing.
The CRF-baseddependency parser on the one hand allowsus to use probabilities of trees or marginalprobabilities of single dependencies for un-certainty measurement, and on the other handcan directly learn parameters from partiallyannotated trees.
Using probabilistic modelsmay be ubiquitous in AL for relatively sim-pler tasks like classification and sequence la-beling, but is definitely novel for dependencyparsing which is dominated by linear modelswith perceptron-like training.
(2) Based on the CRF-based parser, we makesystematic comparison among several uncer-tainty metrics for both FA and PA. Simulationexperiments show that compared with usingFA, AL with PA can greatly reduce annota-tion effort in terms of dependency number by62.2% on Chinese and by 74.2% on English.
(3) We build a visualized annotation platformand conduct human annotation experimentsto compare FA and PA on real annotationtime and quality, where we obtain severalinteresting observations and conclusions.All codes, along with the data from humanannotation experiments, are released at http://hlt.suda.edu.cn/?zhli for future re-search study.2 Probabilistic Dependency ParsingGiven an input sentence x = w1...wn, the goal ofdependency parsing is to build a directed depen-dency tree d = {h ?
m : 0 ?
h ?
n, 1 ?m ?
n}, where |d| = n and h ?
m representsa dependency from a head word h to a modifierword m. Figure 1 depicts a partial tree containingtwo dependencies.11In this work, we follow many previous works to focuson unlabeled dependency parsing (constructing the skeletondependency structure).
However, the proposed techniquesIn this work, we for the first time apply a proba-bilistic CRF-based parsing model to AL for depen-dency parsing.
We adopt the second-order graph-based model of McDonald and Pereira (2006),which casts the problem as finding an optimal treefrom a fully-connect directed graph and factors thescore of a dependency tree into scores of pairs ofsibling dependencies.d?= argmaxd?Y(x)Score(x,d;w)Score(x,d;w) =?
(h,s,m):h?s?d,h?m?dw ?
f(x, h, s,m)(1)where s and m are adjacent siblings both modify-ing h; f(x, h, s,m) are the corresponding featurevector; w is the feature weight vector; Y(x) isthe set of all legal trees for x according to thedependency grammar in hand; d?is the 1-bestparse tree which can be gained efficiently via adynamic programming algorithm (Eisner, 2000).We use the state-of-the-art feature set listed inBohnet (2010).Under the log-linear CRF-based model, theprobability of a dependency tree is:p(d|x;w) =eScore(x,d;w)?d??Y(x)eScore(x,d?
;w)(2)Ma and Zhao (2015) give a very detailed andthorough introduction to CRFs for dependencyparsing.2.1 Learning from FAUnder the supervised learning scenario, a labeledtraining data D = {(xi,di)}Ni=1is provided tolearn w. The objective is to maximize the loglikelihood of D:L(D;w) =?Ni=1log p(di|xi;w) (3)which can be solved by standard gradient descentalgorithms.
In this work, we adopt stochastic gra-dient descent (SGD) with L2-norm regularizationfor all CRF-based parsing models.2explored in this paper can be easily extended to the case oflabeled dependency parsing.2We borrow the implementation of SGD inCRFsuite (http://www.chokkan.org/software/crfsuite/), and use 100 sentences for a batch.3452.2 Marginal Probability of DependenciesMarcheggiani and Arti`eres (2014) shows thatmarginal probabilities of local labels can beused as an effective uncertain metric for ALfor sequence labeling problems.
In the case ofdependency parsing, the marginal probability of adependency is the sum of probabilities of all legaltrees that contain the dependency.p(h?
m|x;w) =?d?Y(x):h?m?dp(d|x;w) (4)Intuitively, marginal probability is a more princi-pled metric for measuring reliability of a depen-dency since it considers all legal parses in thesearch space, compared to previous methods basedon scores of local classifiers (Sassano and Kuro-hashi, 2010; Flannery and Mori, 2015) or votesof n-best parses (Mirroshandel and Nasr, 2011).Moreover, Li et al (2014) find strong correlationbetween marginal probability and correctness of adependency in cross-lingual syntax projection.3 Active Learning for DependencyParsingThis work adopts the standard pool-based ALframework (Lewis and Gale, 1994; McCallum andNigam, 1998).
Initially, we have a small set oflabeled seed data L, and a large-scale unlabeleddata pool U .
Then the procedure works as follows.
(1) Train a new parser on the current L.(2) Parse all sentences in U , and select a set ofthe most informative tasks U?
(3) Manually annotate: U??
L?
(4) Expand labeled data: L ?
L??
LThe above steps loop for many iterations until apredefined stopping criterion is met.The key challenge for AL is how to measure theinformativeness of structures in concern.
Follow-ing previous work on AL for dependency parsing,we make a simplifying assumption that if thecurrent model is most uncertain about an output(sub)structure, the structure is most informative interms of boosting model performance.3.1 Sentence-wise FASentence-wise FA selects K most uncertain sen-tences in Step (2), and annotates their whole treestructures in Step (3).
In the following, we de-scribe several uncertainty metrics and investigatetheir practical effects through experiments.
Givenan unlabeled sentence x = w1...wn, we use d?to denote the 1-best parse tree produced by thecurrent model as in Eq.
(1).
For brevity, we omitthe feature weight vector w in the equations.Normalized tree score.
Following previousworks that use scores of local classifiersfor uncertainty measurement (Sassano andKurohashi, 2010; Flannery and Mori, 2015), weuse Score(x,d?)
to measure the uncertainty of x,assuming that the model is more uncertain aboutx if d?gets a smaller score.
However, we find thatdirectly using Score(x,d?)
always selects veryshort sentences due to the definition in Eq.
(1).Thus we normalize the score with the sentencelength n as follows.3Confi(x) =Score(x,d?
)n1.5(5)Normalized tree probability.
The CRF-basedparser allows us, for the first time in AL for de-pendency parsing, to directly use tree probabilitiesfor uncertainty measurement.
Unlike previousapproximate methods based on k-best parses (Mir-roshandel and Nasr, 2011), tree probabilities glob-ally consider all parse trees in the search space,and thus are intuitively more consistent and properfor measuring the reliability of a tree.
Our initialassumption is that the model is more uncertainabout x if d?gets a smaller probability.
However,we find that directly using p(d?|x) would selectvery long sentences because the solution spacegrows exponentially with sentence length.
We findthat the normalization strategy below works well.4Confi(x) =n?p(d?|x) (6)Averaged marginal probability.
As discussedin Section 2.2, the marginal probability of a de-pendency directly reflects its reliability, and thuscan be regarded as another global measurementbesides tree probabilities.In fact, we find that theeffect of sentence length is naturally handled withthe following metric.5Confi(x) =?h?m?d?p(h?
m|x)n(7)3We have also tried replacing n1.5with n (still prefershort sentences) and n2(bias to long sentences).4We have also tried p(d?|x)?f(n), where f(n) = lognor f(n) =?n, but both work badly.5We have also triedn??h?m?d?p(h?
m|x), leadingto slightly inferior results.3463.2 Single Dependency-wise PAAL with single dependency-wise PA selects Mmost uncertain words from U in Step (2), and an-notates the heads of the selected words in Step (3).After annotation, the newly annotated sentenceswith partial trees L?are added into L. Differentfrom the case of sentence-wise FA, L?are also putback to U , so that new tasks can be further chosenfrom them.Marcheggiani and Arti`eres (2014) make sys-tematic comparison among a dozen uncertaintymetrics for AL with PA for several sequencelabeling tasks.
We borrow three effective metricsaccording to their results.Marginal probability max.
Suppose h0=argmaxhp(h?
i|x) is the most likely head fori.
The intuition is that the lower p(h0?
i) is, themore uncertain the model is on deciding the headof the token i.Confi(x, i) = p(h0?
i|x) (8)Marginal probability gap.
Suppose h1=argmaxh?=h0p(h?
i|x) is the second most likelyhead for i.
The intuition is that the smaller theprobability gap is, the more uncertain the model isabout i.Confi(x, i) = p(h0?
i|x) ?
p(h1?
i|x) (9)Marginal probability entropy.
This metricconsiders the entropy of all possible heads for i.The assumption is that the smaller the negativeentropy is, the more uncertain the model is abouti.Confi(x, i) =?hp(h?
i|x) log p(h?
i|x)(10)3.3 Batch Dependency-wise PAIn the framework of single dependency-wise PA,we assume that the selection and annotation ofdependencies in the same sentence are strictlyindependent.
In other words, annotators may beasked to annotate the head of one selected word af-ter reading and understanding a whole (sometimespartial) sentence, and may be asked to annotateanother selected word in the same sentence in nextAL iteration.
Obviously, frequently switchingsentences incurs great waste of cognitive effort,$0I1saw2Sarah3with4a5telescope6Figure 2: An example parse forest converted fromthe partial tree in Figure 1.and annotating one dependency can certainly helpdecide another dependency in practice.Inspired by the work of Flannery and Mori(2015), we propose AL with batch dependency-wise PA, which is a compromise betweensentence-wise FA and single dependency-wisePA.
In Step 2, AL with batch dependency-wisePA selects K most uncertain sentences from U ,and also determines r% most uncertain wordsfrom each sentence at the same time.
In Step3, annotators are asked to label the heads ofthe selected words in the selected sentences.We propose and experiment with the followingthree strategies based on experimental results ofsentence-wise FA and single dependency-wisePA.Averaged marginal probability & gap.First, select K sentences from U using averagedmarginal probability.
Second, select r% wordsusing marginal probability gap for each selectedsentence.Marginal probability gap.
First, for eachsentence in U , select r% most uncertain wordsaccording to marginal probability gap.
Second,select K sentences from U using the averagedmarginal probability gap of the selected r% wordsin a sentence as the uncertainty metric.Averaged marginal probability.
This strategyis the same with the above strategy, except itmeasures the uncertainty of a word i accordingto the marginal probability of the dependencypointing to i in d?, i.e., p(j ?
i|x), where j ?i ?
d?.3.4 Learning from PAA major challenge for AL with PA is how to learnfrom partially labeled sentences, as depicted inFigure 1.
Li et al (2014) show that a probabilisticCRF-based parser can naturally and effectivelylearn from PA.
The basic idea is converting apartial tree into a forest as shown in Figure 2,347and using the forest as the gold-standard referenceduring training, also known as ambiguous labeling(Riezler et al, 2002; T?ackstr?om et al, 2013).For each remaining word without head, weadd all dependencies linking to it as long as thenew dependency does not violate the existingdependencies.
We denote the resulting forest asF j, whose probability is naturally the sum ofprobabilities of each tree d in F .p(F|x;w) =?d?Fp(d|x;w)=?d?FeScore(x,d;w)?d??Y(x)eScore(x,d?
;w)(11)Suppose the partially labeled training data isD = {(xi,Fi)}Ni=1.
Then its log likelihood is:L(D;w) =?Ni=1log p(Fi|xi;w) (12)T?ackstr?om et al (2013) show that the partialderivative of the L(D;w) with regard to w (a.k.athe gradient) in both Equation (3) and (12) can beefficiently solved with the classic Inside-Outsidealgorithm.64 Simulation ExperimentsWe use Chinese Penn Treebank 5.1 (CTB) forChinese and Penn Treebank (PTB) for English.For both datasets, we follow the standard datasplit, and convert original bracketed structures intodependency structures using Penn2Malt with itsdefault head-finding rules.
To be more realis-tic, we use automatic part-of-speech (POS) tagsproduced by a state-of-the-art CRF-based tagger(94.1% on CTB-test, and 97.2% on PTB-test, n-fold jackknifing on training data), since POS tagsencode much syntactic annotation.
Because ALexperiments need to train many parsing models,we throw out all training sentences longer than 50to speed up our experiments.
Table 1 shows thedata statistics.Following previous practice on AL with PA(Sassano and Kurohashi, 2010; Flannery andMori, 2015), we adopt the following AL settingsfor both Chinese and English .
The first 500training sentences are used as the seed labeleddata L. In the case of FA, K = 500 new sentences6This work focuses on projective dependency parsing.Please refer to Koo et al (2007), McDonald and Satta (2007),and Smith and Smith (2007) for building a probabilistic non-projective parser.Train Dev TestChinese#Sentences 14,304 803 1,910#Tokens 318,408 20,454 50,319English#Sentences 39,115 1,700 2,416#Tokens 908,154 40,117 56,684Table 1: Data statistics.are selected and annotated at each iteration.
Inthe case of single dependency-wise PA, we selectand annotate M = 10, 000 dependencies, whichroughly correspond to 500 sentences consideringthat the averaged sentence length is about 22.3 inCTB-train and 23.2 in PTB-train.
In the case ofbatch dependency-wise PA, we set K = 500, andr = 20% for Chinese and r = 10% for English,considering that the parser trained on all dataachieves about 80% and 90% accuracies.We measure parsing performance using thestandard unlabeled attachment score (UAS)including punctuation marks.
Please note that wealways treat punctuation marks as ordinary wordswhen selecting annotation tasks and calculatingUAS, in order to make fair comparison betweenFA and PA.74.1 FA vs.
Single Dependency-wise PAFirst, we make comparison on the performance ofAL with FA and with single dependency-wise PA.Results on Chinese are shown in Figure 3.Following previous work, we use the number ofannotated dependencies (x-axis) as the annotationcost in order to fairly compare FA and PA. We useFA with random selection as a baseline.
We alsodraw the accuracy of the CRF-based parser trainedon all training data, which can be regarded as theupper bound.For FA, the curve of the normalized tree scoreintertwines with that of random selection.
Mean-while, the performance of normalized tree prob-ability is very close to that of averaged marginalprobability, and both are clearly superior to thebaseline with random selection.For PA, the difference among the three uncer-tainty metrics is small.
The marginal probabilitygap clearly outperforms the other two metrics be-fore 50, 000 annotated dependencies, and remains7Alternatively, we can exclude punctuation marks for taskselection in AL with PA. Then, to be fair, we have to discardall dependencies pointing to punctuation marks in the case ofFA.
This makes the experiment setting more complicated.3487273747576777879800  50000  100000  150000  200000  250000  300000UAS(%)Number of Annotated DependenciesParser trained on all dataPA (single): marginal probability gapPA (single): marginal probability maxPA (single): marginal probability entropyFA: averaged marginal probabilityFA: normalized tree probabilityFA: normalized tree scoreFA: random selectionFigure 3: FA vs. PA on CTB-dev.8485868788899091920  50000  100000  150000  200000  250000  300000UAS(%)Number of Annotated DependenciesParser trained on all dataPA (single): marginal probability gapPA (single): marginal probability maxPA (single): marginal probability entropyFA: averaged marginal probabilityFA: normalized tree probabilityFA: normalized tree scoreFA: random selectionFigure 4: FA vs. PA on PTB-dev.very competitive at all other points.
The marginalprobability max achieves best peak UAS, and evenoutperforms the parser trained on all data, whichcan be explained by small disturbance duringcomplex model training.
The marginal probabilityentropy, although being the most complex metricamong the three, seems inferior all the time.It is clear that using PA can greatly reduceannotation effort compared with using FA in termsof annotated dependencies.Results on English are shown in Figure 4.
Theoverall findings are similar to those in Figure 3, ex-cept that the distinction among different methodsis more clear.
For FA, normalized tree scoreis consistently better than the random baseline.Normalized tree probability always outperformsnormalized tree score.
Averaged marginal proba-bility performs best, except being slightly inferiorto normalized tree probability in earlier stages.For PA, it is consistent that marginal probabilitygap is better than marginal probability max, andmarginal probability entropy is the worst.In summary, based on the results on the de-72737475767778798010000  20000  30000  40000  50000  60000  70000UAS(%)Number of Annotated DependenciesParser trained on all dataPA (single): marginal probability gapPA (batch 20%): marginal probability gapPA (batch 20%): averaged marginal probabilityPA (batch 20%): averaged marginal probability & gapFigure 5: Single vs. batch dependency-wise PA onCTB-dev.8485868788899091929310000  20000  30000  40000  50000  60000  70000UAS(%)Number of Annotated DependenciesParser trained on all dataPA (single): marginal probability gapPA (batch 10%): marginal probability gapPA (batch 10%): averaged marginal probabilityPA (batch 10%): averaged marginal probability & gapPA (batch 20%): marginal probability gapFigure 6: Single vs. batch dependency-wise PA onPTB-dev.velopment data in Figure 3 and 4, the best ALmethod with PA only needs about80,000318,408= 25%annotated dependencies on Chinese, and about90,000908,154= 10% on English, to reach the same per-formance with parsers trained on all data.
More-over, the PA methods converges much faster thanthe FA ones, since for the same x-axis number,much more sentences (with partial trees) are usedas training data for AL with PA than FA.4.2 Single vs. Batch Dependency-wise PAThen we make comparison on AL with singledependency-wise PA and with the more practicalbatch dependency-wise PA.Results on Chinese are shown in Figure 5.
Wecan see that the three strategies achieve very sim-ilar performance and are also very close to singledependency-wise PA. AL with batch dependency-wise PA even achieves higher accuracy before20, 000 annotated dependencies, which should becaused by the smaller active learning steps (about3492, 000 dependencies at each iteration, contrasting10, 000 for single dependency-wise PA).
When thetraining data runs out at about 7, 300 dependen-cies, AL with batch dependency-wise PA only lagsbehind with single dependency-wise PA by about0.3%, which we suppose can be reduced if largertraining data is available.Results on English are shown in Figure 6,and are very similar to those on Chinese.
Onetiny difference is that the marginal probabilitygap is slightly worse that the other two metrics.The three uncertainty metrics have very similaraccuracy curves, which are also very close to thecurve of single dependency-wise PA.
In addition,we also try r = 20% and find that results areinferior to r = 10%, indicating that the extra 10%annotation tasks are less valuable and contributive.4.3 Main Results on Test DataTable 2 shows the results on test data.
We compareour CRF-based parser with ZPar v6.08, a state-of-the-art transition-based dependency parser (Zhangand Nivre, 2011).
We train ZPar with defaultparameter settings for 50 iterations, and choosethe model that performs best on dev data.
Wecan see that when trained on all data, our CRF-based parser outperforms ZPar on both Chineseand English.To compare FA and PA, we report the numberof annotated dependencies needed under each ALstrategy to achieve an accuracy lower by about 1%than the parser trained on all data.9FA (best) refers to FA with averaged marginalprobability, and it needs187,123?149,051187,123= 20.3%less annotated dependencies than FA with ran-dom selection on Chinese, and395,199?197,907395,199=50.0% less on English.PA (single) with marginal probability gapneeds149,051?50,958149,051= 65.8% less annotateddependencies than FA (best) on Chinese, and197,907?61,448197,907= 69.0% less on English.PA (batch) with marginal probability gap needsslightly more annotation than PA (single) on Chi-nese but slightly less annotation on English, andcan reduce the amount of annotated dependenciesby149,051?56,389149,051= 62.2% over FA (best) on Chi-8http://people.sutd.edu.sg/?yue_zhang/doc/9The gap 1% is chosen based on the curves ondevelopment data (Figure 3 and 4) with the following twoconsiderations: 1) larger gap may lead to wrong impressionthat AL is weak; 2) smaller gap (e.g., 0.5%) cannot bereached for the worst AL method (FA: random).Chinese English#Dep labeled UAS #Dep labeled UASZPar 318,408 77.97 908,154 91.45This parser 318,408 78.36 908,154 91.66FA (random) 187,123 77.43 395,199 90.67FA (best) 149,051 77.32 197,907 90.66PA (single) 50,958 77.22 61,448 90.72PA (batch) 56,389 77.38 51,016 90.70Table 2: Results on test data.nese and by197,907?51,016197,907= 74.2% on English.5 Human Annotation ExperimentsSo far, we measure annotation effort in termsof the number of annotated dependencies andassume that it takes the same amount of timeto annotate different words, which is obviouslyunrealistic.
To understand whether active learningbased on PA can really reduce annotation timeover based on FA in practice, we build a webbrowser based annotation system,10and conducthuman annotation experiments on Chinese.In this part, we use CTB 7.0 which is a newerand larger version and covers more genres, andadopt the newly proposed Stanford dependencies(de Marneffe and Manning, 2008; Chang et al,2009) which are more understandable for anno-tators.11Since manual syntactic annotation isvery difficult and time-consuming, we only keepsentences with length [10, 20] in order to bettermeasure annotation time by focusing on sentencesof reasonable length, which leave us 12, 912 train-ing sentences under the official data split.
Then,we use a random half of training sentences totrain a CRF-based parser, and select 20% mostuncertain words with marginal probability gap foreach sentence of the left half.We employ 6 postgraduate students as our an-notators who are at different levels of familiarityin syntactic annotation.
Before annotation, theannotators are trained for about two hours byintroducing the basic concepts, guidelines, and il-lustrating examples.
Then, they are asked to prac-tice on the annotation system for about anothertwo hours.
Finally, all annotators are required to10http://hlt-service.suda.edu.cn/syn-dep-batch.
Please try.11We use Stanford Parser 3.4 (2014-06-16) for constituent-to-dependency structure conversion.350Time: Sec/Dep Annotation accuracyFA PA FA (on 20%) PA (diff)Annotator #1 4.0 7.9 84.65 (73.41) 75.28 (+1.87)Annotator #2 7.5 16.0 78.90 (72.22) 62.18 (-10.04)Annotator #3 10.0 22.2 69.75 (59.77) 56.91 (-2.86)Annotator #4 5.1 8.7 66.75 (49.19) 61.77 (+12.58)Annotator #5 7.0 17.3 65.47 (48.50) 48.39 (-0.11)Annotator #6 7.0 10.6 58.05 (43.28) 48.37 (+5.09)Overall 6.7 13.6 70.36 (57.28) 59.06 (+1.78)Table 3: Statistics of human annotation.formally annotate the same 100 sentences.
Thesystem is programed that each sentence has 3FA submissions and 3 PA submissions.
Duringformal annotation, the annotators are not allowedto discuss with each other or look up any guide-line or documents, which may incur unnecessaryinaccuracy in timing.
Instead, the annotatorscan only decide the syntactic structures based onthe basic knowledge of dependency grammar andone?s understanding of the sentence structure.
Theannotation process lasts for about 5 hours.
Onaverage, each annotator completes 50 sentenceswith FA (763 dependencies) and 50 sentences withPA (178 dependencies).Table 3 lists the results in descending order ofan annotator?s experience in syntactic annotation.The first two columns compare the time needed forannotating a dependency in seconds.
On average,annotating a dependency in PA takes about twiceas much time as in FA, which is reasonable con-sidering the words to be annotated in PA may bemore difficult for annotators while the annotationof some tasks in FA may be very trivial and easy.Combined with the results in Table 2, we may inferthat to achieve 77.3% accuracy on CTB-test, ALwith FA requires 149, 051 ?
6.7 = 998, 641.7seconds of annotation, whereas AL with batchdependency-wise PA needs 56, 389 ?
13.6 =766, 890.4 seconds.
Thus, we may roughly saythat AL with PA can reduce annotation time overFA by998,641.7?766,890.4998,641.7= 23.2%.We also report annotation accuracy accordingto the gold-standard Stanford dependencies con-verted from bracketed structures.12Overall, theaccuracy of FA is 70.36?59.06 = 11.30% higher12An anonymous reviewer commented that the directcomparison between an annotator?s performance on PA andFA based on accuracy may be misleading since the FA andPA sentences for one annotator are mutually exclusive.than that of PA, which should be due to the trivialtasks in FA.
To be more fair, we compare theaccuracies of FA and PA on the same 20% selecteddifficult words, and find that annotators exhibitdifferent responses to the switch.
Annotator #4achieve 12.58% higher accuracy when under PAthan under FA.
The reason may be that under PA,annotators can be more focused and therefore per-form better on the few selected tasks.
In contrast,some annotators may perform better under FA.For example, annotation accuracy of annotator #2increases by 10.04% when switching from PA toFA, which may be due to that FA allows annotatorsto spend more time on the same sentence and gainhelp from annotating easier tasks.
Overall, we findthat the accuracy of PA is 59.06?
57.28 = 1.78%higher than that of FA, indicating that PA actuallycan improve annotation quality.6 Related WorkRecently, AL with PA attracts much attention insentence-wise natural language processing suchas sequence labeling and parsing.
For sequencelabeling, Marcheggiani and Arti`eres (2014) sys-tematically compare a dozen uncertainty metricsin token-wise AL with PA (without comparisonwith FA), whereas Settles and Craven (2008) in-vestigate different uncertainty metrics in AL withFA.
Li et al (2012) propose to only annotate themost uncertain word boundaries in a sentence forChinese word segmentation and show promisingresults on both simulation and human annotationexperiments.
All above works are based on CRFsand make extensive use of sequence probabilitiesand token marginal probability.In parsing community, Sassano and Kurohashi(2010) select bunsetsu (similar to phrases) pairswith smallest scores from a local classifier, andlet annotators decide whether the pair composesa dependency.
They convert partially annotatedinstances into local dependency/non-dependencyclassification instances to help a simple shift-reduce parser.
Mirroshandel and Nasr (2011)select most uncertain words based on votes of n-best parsers, and convert partial trees into full treesby letting a baseline parser perform constraineddecoding in order to preserve partial annotation.Under a different query-by-committee AL frame-work, Majidi and Crane (2013) select most uncer-tain words using a committee of diverse parsers,and convert partial trees into full trees by letting351the parsers of committee to decide the heads ofremaining tokens.
Based on a first-order (point-wise) Japanese parser, Flannery and Mori (2015)use scores of a local classifier for task selection,and treat PA as dependency/non-dependency in-stances (Flannery et al, 2011).
Different fromabove works, this work adopts a state-of-the-artprobabilistic dependency parser, uses more prin-cipled tree probabilities and dependency marginalprobabilities for uncertainty measurement, andlearns from PA based on a forest-based trainingobjective which is more theoretically sound.Most previous works on AL with PA only con-duct simulation experiments.
Flannery and Mori(2015) perform human annotation to measure trueannotation time.
A single annotator is employedto annotate for two hours alternating FA and PA(33% batch) every fifteen minutes.
Beyond theirinitial expectation, they find that the annotationtime per dependency is nearly the same for FA andPA (different from our findings) and gives a fewinteresting explanations.Under a non-AL framework, Mejer and Cram-mer (2012) propose an interesting light feedbackscheme for dependency parsing by letting annota-tors decide the better one from top-2 parse treesproduced by the current parsing model.Hwa (1999) pioneers the idea of using PAto reduce manual labeling effort for constituentgrammar induction.
She uses a variant Inside-Outside re-estimation algorithm (Pereira and Sch-abes, 1992) to induce a grammar from PA. Clarkand Curran (2006) propose to train a Combina-torial Categorial Grammar parser using partiallylabeled data only containing predicate-argumentdependencies.
Tsuboi et al (2008) extend CRF-based sequence labeling models to learn fromincomplete annotations, which is the same withMarcheggiani and Arti`eres (2014).
Li et al (2014)propose a CRF-based dependency parser that canlearn from partial tree projected from source-language structures in the cross-lingual parsingscenario.
Mielens et al (2015) propose to imputemissing dependencies based on Gibbs sampling inorder to enable traditional parsers to learn frompartial trees.7 ConclusionsThis paper for the first time applies a state-of-the-art probabilistic model to AL with PA fordependency parsing.
It is shown that the CRF-based parser can on the one hand provide treeprobabilities and dependency marginal probabili-ties as principled uncertainty metrics and on theother hand elegantly learn from partially annotateddata.
We have proposed and compared several un-certainty metrics through simulation experiments,and show that AL with PA can greatly reducethe amount of annotated dependencies by 62.2%on Chinese 74.2% on English.
Finally, we con-duct human annotation experiments on Chinese tocompare PA and FA on real annotation time andquality.
We find that annotating a dependency inPA takes about 2 times long as in FA.
This sug-gests that AL with PA can reduce annotation timeby 23.2% over with FA on Chinese.
Moreover,the results also indicate that annotators tend toperform better under PA than FA.For future work, we would like to advance thisstudy in the following directions.
The first idea isto combine uncertainty and representativeness formeasuring informativeness of annotation targets inconcern.
Intuitively, it would be more profitableto annotate instances that are both difficult forthe current model and representative in capturingcommon language phenomena.
Second, we so farassume that the selected tasks are equally difficultand take the same amount of effort for humanannotators.
However, it is more reasonable thathuman are good at resolving some ambiguities butbad at others.
Our plan is to study which syntacticstructures are more suitable for human annotation,and balance informativeness of a candidate taskand its suitability for human annotation.
Finally,one anonymous reviewer comments that we mayuse automatically projected trees (Rasooli andCollins, 2015; Guo et al, 2015; Ma and Xia, 2014)as the initial seed labeled data, which is cheap andinteresting.AcknowledgmentsThe authors would like to thank the anonymousreviewers for the helpful comments.
We also thankJunhui Li and Chunyu Kit for reading our paperand giving many good suggestions.
Particularly,Zhenghua is very grateful to many of his students:Fangli Lu, Qiuyi Yan, and Yue Zhang build the an-notation system; Jiayuan Chao, Wei Chen, ZiweiFan, Die Hu, Qingrong Xia, and Yue Zhang partic-ipate in data annotation.
This work was supportedby National Natural Science Foundation of China(Grant No.
61502325, 61525205, 61572338).352ReferencesBernd Bohnet.
2010.
Top accuracy and fastdependency parsing is not a contradiction.
InProceedings of COLING, pages 89?97.Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, andChristopher D. Manning.
2009.
Discriminativereordering with Chinese grammatical relationsfeatures.
In Proceedings of the Third Workshopon Syntax and Structure in Statistical Translation(SSST-3) at NAACL HLT 2009, pages 51?59.Stephen Clark and James Curran.
2006.
Partialtraining for a lexicalized-grammar parser.
InProceedings of the Human Language TechnologyConference of the NAACL, pages 144?151.Marie-Catherine de Marneffe and Christopher D.Manning.
2008.
The Stanford typed dependenciesrepresentation.
In Coling 2008: Proceedings of theworkshop on Cross-Framework and Cross-DomainParser Evaluation, pages 1?8.Mark Dredze, John Blitzer, Partha Pratim Talukdar,Kuzman Ganchev, Jo?ao Graca, and FernandoPereira.
2007.
Frustratingly hard domain adaptationfor dependency parsing.
In Proceedings of theCoNLL Shared Task Session of EMNLP-CoNLL2007.Jason Eisner.
2000.
Bilexical grammars and theircubic-time parsing algorithms.
In Advances inProbabilistic and Other Parsing Technologies, pages29?62.Daniel Flannery and Shinsuke Mori.
2015.
Combiningactive learning and partial annotation for domainadaptation of a japanese dependency parser.
InProceedings of the 14th International Conference onParsing Technologies, pages 11?19.Daniel Flannery, Yusuke Miayo, Graham Neubig, andShinsuke Mori.
2011.
Training dependency parsersfrom partially annotated corpora.
In Proceedings ofIJCNLP, pages 776?784.Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,Joseph Le Roux, Joakim Nivre, Deirdre Hogan, andJosef van Genabith.
2011.
From news to comment:Resources and benchmarks for parsing the languageof web 2.0.
In Proceedings of IJCNLP, pages 893?901.Jiang Guo, Wanxiang Che, David Yarowsky, HaifengWang, and Ting Liu.
2015.
Cross-lingual depen-dency parsing based on distributed representations.In Proceedings of ACL, pages 1234?1244.Rebecca Hwa.
1999.
Supervised grammar inductionusing training data with limited constituent informa-tion.
In Proceedings of ACL, pages 73?79.Rebecca Hwa.
2004.
Sample selection for statisticalparsing.
Computional Linguistics, 30(3):253?276.Lingpeng Kong, Nathan Schneider, SwabhaSwayamdipta, Archna Bhatia, Chris Dyer, andNoah A. Smith.
2014.
A dependency parserfor tweets.
In Proceedings of EMNLP, pages1001?1012.Terry Koo and Michael Collins.
2010.
Efficient third-order dependency parsers.
In ACL, pages 1?11.Terry Koo, Amir Globerson, Xavier Carreras, andMichael Collins.
2007.
Structured predictionmodels via the matrix-tree theorem.
In Proceedingsof EMNLP-CoNLL, pages 141?150.David D. Lewis and William A. Gale.
1994.
Asequential algorithm for training text classifiers.
InProceedings of the 17th Annual International ACMSIGIR Conference on Research and Development inInformation Retrieval, pages 3?12.Shoushan Li, Guodong Zhou, and Chu-Ren Huang.2012.
Active learning for Chinese word segmen-tation.
In Proceedings of COLING 2012: Posters,pages 683?692.Zhenghua Li, Min Zhang, and Wenliang Chen.
2014.Soft cross-lingual syntax projection for dependencyparsing.
In COLING, pages 783?793.Teresa Lynn, Jennifer Foster, Mark Dras, and Elaine U?1Dhonnchadha.
2012.
Active learning and the irishtreebank.
In Proceedings of ALTA.Xuezhe Ma and Fei Xia.
2014.
Unsuperviseddependency parsing with transferring distributionvia parallel guidance and entropy regularization.
InProceedings of ACL, pages 1337?1348.Xuezhe Ma and Hai Zhao.
2015.
Probabilistic mod-els for high-order projective dependency parsing.Arxiv, abs/1502.04174.Saeed Majidi and Gregory Crane.
2013.
Activelearning for dependency parsing by a committee ofparsers.
In Proceedings of IWPT, pages 98?105.Diego Marcheggiani and Thierry Arti`eres.
2014.An experimental comparison of active learningstrategies for partially labeled sequences.
InProceedings of EMNLP, pages 898?906.Andrew McCallum and Kamal Nigam.
1998.Employing EM and pool-based active learning fortext classification.
In Proceedings of ICML, pages350?358.Ryan McDonald and Fernando Pereira.
2006.Online learning of approximate dependency parsingalgorithms.
In Proceedings of EACL, pages 81?88.Ryan McDonald and Giorgio Satta.
2007.
On the com-plexity of non-projective data-driven dependencyparsing.
In Proceedings of the Tenth InternationalConference on Parsing Technologies, pages 121?132.353Avihai Mejer and Koby Crammer.
2012.
Trainingdependency parser using light feedback.
InProceedings of NAACL.Jason Mielens, Liang Sun, and Jason Baldridge.
2015.Parse imputation for dependency annotations.
InProceedings of ACL-IJCNLP, pages 1385?1394.Seyed Abolghasem Mirroshandel and Alexis Nasr.2011.
Active learning for dependency parsingusing partially annotated sentences.
In Proceedingsof the 12th International Conference on ParsingTechnologies, pages 140?149.Fredrik Olsson.
2009.
A literature survey of activemachine learning in the context of natural languageprocessing.
Technical report, Swedish Institute ofComputer Science.Fernando Pereira and Yves Schabes.
1992.
Inside-outside reestimation from partially bracketed cor-pora.
In Proceedings of the Workshop on Speechand Natural Language (HLT), pages 122?127.Slav Petrov and Ryan McDonald.
2012.
Overview ofthe 2012 shared task on parsing the web.
In Notesof the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL).Mohammad Sadegh Rasooli and Michael Collins.2015.
Density-driven cross-lingual transfer ofdependency parsers.
In Proceedings of EMNLP,pages 328?338.Stefan Riezler, Tracy H. King, Ronald M. Kaplan,Richard Crouch, John T. III Maxwell, and MarkJohnson.
2002.
Parsing the wall street journal usinga lexical-functional grammar and discriminativeestimation techniques.
In Proceedings of 40th An-nual Meeting of the Association for ComputationalLinguistics, pages 271?278.Manabu Sassano and Sadao Kurohashi.
2010.
Usingsmaller constituents rather than sentences in activelearning for japanese dependency parsing.
InProceedings of ACL, pages 356?365.Burr Settles and Mark Craven.
2008.
An analysisof active learning strategies for sequence labelingtasks.
In Proceedings of EMNLP, pages 1070?1079.David A. Smith and Noah A. Smith.
2007.Probabilistic models of nonprojective dependencytrees.
In Proceedings of EMNLP-CoNLL, pages132?140.Oscar T?ackstr?om, Ryan McDonald, and Joakim Nivre.2013.
Target language adaptation of discriminativetransfer parsers.
In Proceedings of NAACL, pages1061?1071.Min Tang, Xiaoqiang Luo, and Salim Roukos.
2002.Active learning for statistical natural languageparsing.
In Proceedings of ACL, pages 120?127.Yuta Tsuboi, Hisashi Kashima, Hiroki Oda, ShinsukeMori, and Yuji Matsumoto.
2008.
Training condi-tional random fields using incomplete annotations.In Proceedings of COLING, pages 897?904.William Yang Wang, Lingpeng Kong, KathrynMazaitis, and William W Cohen.
2014.
Depen-dency parsing for weibo: An efficient probabilisticlogic programming approach.
In Proceedings ofEMNLP, pages 1152?1158.Yue Zhang and Joakim Nivre.
2011.
Transition-baseddependency parsing with rich non-local features.
InProceedings of ACL, pages 188?193.354
