Learning Non-Isomorphic Tree Mappings for Machine TranslationJason Eisner, Computer Science Dept., Johns Hopkins Univ.
<jason@cs.jhu.edu>AbstractOften one may wish to learn a tree-to-tree mapping, training iton unaligned pairs of trees, or on a mixture of trees and strings.Unlike previous statistical formalisms (limited to isomorphictrees), synchronous TSG allows local distortion of the tree topol-ogy.
We reformulate it to permit dependency trees, and sketchEM/Viterbi algorithms for alignment, training, and decoding.1 Introduction: Tree-to-Tree MappingsStatistical machine translation systems are trained onpairs of sentences that are mutual translations.
For exam-ple, (beaucoup d?enfants donnent un baiser a` Sam, kidskiss Sam quite often).
This translation is somewhat free,as is common in naturally occurring data.
The first sen-tence is literally Lots of?children give a kiss to Sam.This short paper outlines ?natural?
formalisms and al-gorithms for training on pairs of trees.
Our methods workon either dependency trees (as shown) or phrase-structuretrees.
Note that the depicted trees are not isomorphic.akissbaiserdonnentSam oftenquitebeaucoup un Samd?enfantskidsOur main concern is to develop models that can alignand learn from these tree pairs despite the ?mismatches?in tree structure.
Many ?mismatches?
are characteristicof a language pair: e.g., preposition insertion (of ?
),multiword locutions (kiss ?
give a kiss to; misinform?
wrongly inform), and head-swapping (float down ?descend by floating).
Such systematic mismatches shouldbe learned by the model, and used during translation.It is even helpful to learn mismatches that merely tendto arise during free translation.
Knowing that beaucoupd?
is often deleted will help in aligning the rest of the tree.When would learned tree-to-tree mappings be useful?Obviously, in MT, when one has parsers for both thesource and target language.
Systems for ?deep?
anal-ysis and generation might wish to learn mappings be-tween deep and surface trees (Bo?hmova?
et al, 2001)or between syntax and semantics (Shieber and Schabes,1990).
Systems for summarization or paraphrase couldalso be trained on tree pairs (Knight and Marcu, 2000).Non-NLP applications might include comparing student-written programs to one another or to the correct solution.Our methods can naturally extend to train on pairs offorests (including packed forests obtained by chart pars-ing).
The correct tree is presumed to be an element ofthe forest.
This makes it possible to train even when thecorrect parse is not fully known, or not known at all.2 A Natural Proposal: Synchronous TSGWe make the quite natural proposal of using a syn-chronous tree substitution grammar (STSG).
An STSGis a collection of (ordered) pairs of aligned elementarytrees.
These may be combined into a derived pair oftrees.
Both the elementary tree pairs and the operation tocombine them will be formalized in later sections.As an example, the tree pair shown in the introductionmight have been derived by ?vertically?
assembling the6 elementary tree pairs below.
The _ symbol denotesa frontier node of an elementary tree, which must bereplaced by the circled root of another elementary tree.If two frontier nodes are linked by a dashed line labeledwith the state X , then they must be replaced by two rootsthat are also linked by a dashed line labeled with X .akissnull (0,Adv)StartunbaiserNPdonnentNPNPbeaucoupNPd?
(0,Adv)nullnull often(0,Adv)(0,Adv)null quiteenfants kidsNPSam SamNPThe elementary trees represent idiomatic translation?chunks.?
The frontier nodes represent unfilled roles inthe chunks, and the states are effectively nonterminalsthat specify the type of filler that is required.
Thus, don-nent un baiser a` (?give a kiss to?)
corresponds to kiss,with the French subject matched to the English subject,and the French indirect object matched to the Englishdirect object.
The states could be more refined thanthose shown above: the state for the subject, for exam-ple, should probably be not NP but a pair (Npl, NP3s).STSG is simply a version of synchronous tree-adjoining grammar or STAG (Shieber and Schabes, 1990)that lacks the adjunction operation.
(It is also equivalentto top-down tree transducers.)
What, then, is new here?First, we know of no previous attempt to learn the?chunk-to-chunk?
mappings.
That is, we do not know attraining time how the tree pair of section 1 was derived,or even what it was derived from.
Our approach is toreconstruct all possible derivations, using dynamic pro-gramming to decompose the tree pair into aligned pairsof elementary trees in all possible ways.
This producesa packed forest of derivations, some more probable thanothers.
We use an efficient inside-outside algorithm todo Expectation-Maximization, reestimating the model bytraining on all derivations in proportion to their probabil-ities.
The runtime is quite low when the training trees arefully specified and elementary trees are bounded in size.1Second, it is not a priori obvious that one can reason-ably use STSG instead of the slower but more powerfulSTAG.
TSG can be parsed as fast as CFG.
But withoutan adjunction operation,2, one cannot break the trainingtrees into linguistically minimal units.
An elementarytree pair A = (elle est finalement partie, finally she left)cannot be further decomposed into B = (elle est partie,she left) and C = (finalement, finally).
This appears tomiss a generalization.
Our perspective is that the gener-alization should be picked up by the statistical model thatdefines the probability of elementary tree pairs.
p(A) canbe defined using mainly the same parameters that definep(B) and p(C), with the result that p(A) ?
p(B) ?
p(C).The balance between the STSG and the statistical modelis summarized in the last paragraph of this paper.Third, our version of the STSG formalism is moreflexible than previous versions.
We carefully address thecase of empty trees, which are needed to handle free-translation ?mismatches.?
In the example, an STSG can-not replace beaucoup d?
(?lots of?)
in the NP by quiteoften in the VP; instead it must delete the former and in-sert the latter.
Thus we have the alignments (beaucoupd?, ) and (, quite often).
These require innovations.
Thetree-internal deletion of beaucoup d?
is handled by anempty elementary tree in which the root is itself a fron-tier node.
(The subject frontier node of kiss is replacedwith this frontier node, which is then replaced with kids.
)The tree-peripheral insertion of quite often requires anEnglish frontier node that is paired with a French null.We also formulate STSGs flexibly enough that they canhandle both phrase-structure trees and dependency trees.The latter are small and simple (Alshawi et al, 2000):tree nodes are words, and there need be no other structureto recover or align.
Selectional preferences and other in-teractions can be accommodated by enriching the states.Any STSG has a weakly equivalent SCFG that gen-erates the same string pairs.
So STSG (unlike STAG)has no real advantage for modeling string pairs.3 ButSTSGs can generate a wider variety of tree pairs, e.g.,non-isomorphic ones.
So when actual trees are providedfor training, STSG can be more flexible in aligning them.1Goodman (2002) presents efficient TSG parsing with un-bounded elementary trees.
Unfortunately, that clever methoddoes not permit arbitrary models of elementary tree probabili-ties, nor does it appear to generalize to our synchronous case.
(It would need exponentially many nonterminals to keep trackof an matching of unboundedly many frontier nodes.
)2Or a sister-adjunction operation, for dependency trees.3However, the binary-branching SCFGs used by Wu (1997)and Alshawi et al (2000) are strictly less powerful than STSG.3 Past WorkMost statistical MT derives from IBM-style models(Brown et al, 1993), which ignore syntax and allow ar-bitrary word-to-word translation.
Hence they are able toalign any sentence pair, however mismatched.
However,they have a tendency to translate long sentences into wordsalad.
Their alignment and translation accuracy improveswhen they are forced to translate shallow phrases as con-tiguous, potentially idiomatic units (Och et al, 1999).Several researchers have tried putting ?more syntax?into translation models: like us, they use statistical ver-sions of synchronous grammars, which generate sourceand target sentences in parallel and so describe their cor-respondence.4 This approach offers four features absentfrom IBM-style models: (1) a recursive phrase-basedtranslation, (2) a syntax-based language model, (3) theability to condition a word?s translation on the translationof syntactically related words, and (4) polynomial-timeoptimal alignment and decoding (Knight, 1999).Previous work in statistical synchronous grammarshas been limited to forms of synchronous context-freegrammar (Wu, 1997; Alshawi et al, 2000; Yamada andKnight, 2001).
This means that a sentence and its trans-lation must have isomorphic syntax trees, although theymay have different numbers of surface words if nullwords  are allowed in one or both languages.
This rigid-ity does not fully describe real data.The one exception is the synchronous DOP approachof (Poutsma, 2000), which obtains an STSG by decom-posing aligned training trees in all possible ways (and us-ing ?naive?
count-based probability estimates).
However,we would like to estimate a model from unaligned data.4 A Probabilistic TSG FormalismFor expository reasons (and to fill a gap in the literature),first we formally present non-synchronous TSG.
Let Q bea set of states.
Let L be a set of labels that may decoratenodes or edges.
Node labels might be words or nontermi-nals.
Edge labels might include grammatical roles suchas Subject.
In many trees, each node?s children have anorder, recorded in labels on the node?s outgoing edges.An elementary tree is a a tuple ?V, V i, E, `, q, s?where V is a set of nodes; V i ?
V is the set of internalnodes, and we write V f = V ?V i for the set of frontiernodes; E ?
V i ?
V is a set of directed edges (thus allfrontier nodes are leaves).
The graph ?V,E?
must be con-nected and acyclic, and there must be exactly one noder ?
V (the root) that has no incoming edges.
The func-tion ` : (V i ?E) ?
L labels each internal node or edge;q ?
Q is the root state, and s : V f ?
Q assigns a fron-tier state to each frontier node (perhaps including r).4The joint probability model can be formulated, if desired,as a language model times a channel model.A TSG is a set of elementary trees.
The generationprocess builds up a derived tree T that has the same formas an elementary tree, and for which V f = ?.
Initially,T is chosen to be any elementary tree whose root stateT.q = Start.
As long as T has any frontier nodes, T.V f ,the process expands each frontier node d ?
T.V f by sub-stituting at d an elementary tree t whose root state, t.q,equals d?s frontier state, T.s(d).
This operation replacesT with ?T.V ?
t.V ?
{d}, T.V i?
t.V i, T.E??
t.E, T.`?t.`, T.q, T.s ?
t.s ?
{d, t.q}?.
Note that a function is re-garded here as a set of ?input, output?
pairs.
T.E?
is aversion of T.E in which d has been been replaced by t.r.A probabilistic TSG also includes a function p(t | q),which, for each state q, gives a conditional probabilitydistribution over the elementary trees t with root state q.The generation process uses this distribution to randomlychoose which tree t to substitute at a frontier node of Thaving state q.
The initial value of T is chosen from p(t |Start).
Thus, the probability of a given derivation is aproduct of p(t | q) terms, one per chosen elementary tree.There is a natural analogy between (probabilistic)TSGs and (probabilistic) CFGs.
An elementary tree twith root state q and frontier states q1 .
.
.
qk (for k ?
0) isanalogous to a CFG rule q ?
t q1 .
.
.
qk.
(By including tas a terminal symbol in this rule, we ensure that distinctelementary trees t with the same states correspond to dis-tinct rules.)
Indeed, an equivalent definition of the gener-ation process first generates a derivation tree from thisderivation CFG, and then combines its terminal nodes t(which are elementary trees) into the derived tree T .5 Tree Parsing Algorithms for TSGGiven a a grammar G and a derived tree T , we may be in-terested in constructing the forest of T ?s possible deriva-tion trees (as defined above).
We call this tree parsing,as it finds ways of decomposing T into elementary trees.Given a node c ?
T.v, we would like to find all thepotential elementary subtrees t of T whose root t.r couldhave contributed c during the derivation of T .
Such anelementary tree is said to fit c, in the sense that it is iso-morphic to some subgraph of T rooted at c.The following procedure finds an elementary tree t thatfits c. Freely choose a connected subgraph U of T suchthat U is rooted at c (or is empty).
Let t.V i be the vertexset of U .
Let t.E be the set of outgoing edges from nodesin t.V i to their children, that is, t.E = T.E ?
(t.V i ?T.V ).
Let t.` be the restriction of T.` to t.V i ?
t.E, thatis, t.` = T.` ?
((t.V i ?
t.E) ?
L).
Let t.V be the setof nodes mentioned in t.E, or put t.V = {c} if t.V i =t.E = ?.
Finally, choose t.q freely from Q, and chooses : t.V f ?
Q to associate states with the frontier nodesof t; the free choice is because the nodes of the derivedtree T do not specify the states used during the derivation.How many elementary trees can we find that fit c?
Letus impose an upper bound k on |t.V i| and hence on |U |.Then in an m-ary tree T , the above procedure considers atmost mk?1m?1 connected subgraphs U of order ?
k rootedat c. For dependency grammars, limiting to m ?
6 andk = 3 is quite reasonable, leaving at most 43 subgraphsU rooted at each node c, of which the biggest containonly c, a child c?
of c, and a child or sibling of c?.
Thesewill constitute the internal nodes of t, and their remainingchildren will be t?s frontier nodes.However, for each of these 43 subgraphs, we mustjointly hypothesize states for all frontier nodes and theroot node.
For |Q| > 1, there are exponentially manyways to do this.
To avoid having exponentially many hy-potheses, one may restrict the form of possible elemen-tary trees so that the possible states of each node of tcan be determined somehow from the labels on the corre-sponding nodes in T .
As a simple but useful example, anode labeled NP might be required to have state NP.
Richlabels on the derived tree essentially provide supervisionas to what the states must have been during the derivation.The tree parsing algorithm resembles bottom-up chartparsing under the derivation CFG.
But the input is a treerather than a string, and the chart is indexed by nodes ofthe input tree rather than spans of the input string:51. for each node c of T , in bottom-up order2.
for each q ?
Q, let ?c(q) = 03. for each elementary tree t that fits c4.
increment ?c(t.q) by p(t | t.q) ?
?d?t.V f ?d(t.s(d))The ?
values are inside probabilities.
After running thealgorithm, if r is the root of T , then ?r(Start) is the prob-ability that the grammar generates T .p(t | q) in line 4 may be found by hash lookup if thegrammar is stored explicitly, or else by some probabilisticmodel that analyzes the structure, labels, and states of theelementary tree t to compute its probability.One can mechanically transform this algorithm tocompute outside probabilities, the Viterbi parse, the parseforest, and other quantities (Goodman, 1999).
One canalso apply agenda-based parsing strategies.For a fixed grammar, the runtime and space are onlyO(n) for a tree of n nodes.
The grammar constant is thenumber of possible fits to a node c of a fixed tree.
Asnoted above, there usually not many of these (unless thestates are uncertain) and they are simple to enumerate.As discussed above, an inside-outside algorithm maybe used to compute the expected number of times eachelementary tree t appeared in the derivation of T .
That isthe E step of the EM algorithm.
In the M step, these ex-pected counts (collected over a corpus of trees) are usedto reestimate the parameters ~?
of p(t | q).
One alternatesE and M steps till p(corpus | ~?)
?p(~?)
converges to a localmaximum.
The prior p(~?)
can discourage overfitting.5We gloss over the standard difficulty that the derivationCFG may contain a unary rule cycle.
For us, such a cycle isa problem only when it arises solely from single-node trees.6 Extending to Synchronous TSGWe are now prepared to discuss the synchronous case.A synchronous TSG consists of a set of elementary treepairs.
An elementary tree pair t is a tuple ?t1, t2, q,m, s?.Here t1 and t2 are elementary trees without state la-bels: we write tj = ?Vj , V ij , Ej , `j?.
q ?
Q is theroot state as before.
m ?
V f1 ?
Vf2 is a matchingbetween t1?s and t2?s frontier nodes,6.
Let m?
denotem ?
{(d1,null) : d1 is unmatched in m} ?
{(null, d2) :d2 is unmatched in m}.
Finally, s : m?
?
Q assigns astate to each frontier node pair or unpaired frontier node.In the figure of section 2, donnent un baiser a` has 2frontier nodes and kiss has 3, yielding 13 possible match-ings.
Note that least one English node must remain un-matched; it still generates a full subtree, aligned with null.As before, a derived tree pair T has the same form asan elementary tree pair.
The generation process is similarto before.
As long as T.m?
6= ?, the process expands somenode pair (d1, d2) ?
T.m?.
It chooses an elementary treepair t such that t.q = T.s(d1, d2).
Then for each j = 1, 2,it substitutes tj at dj if non-null.
(If dj is null, then t.qmust guarantee that tj is the special null tree.
)In the probabilistic case, we have a distribution p(t | q)just as before, but this time t is an elementary tree pair.Several natural algorithms are now available to us:?
Training.
Given an unaligned tree pair (T1, T2), wecan again find the forest of all possible derivations, withexpected inside-outside counts of the elementary treepairs.
This allows EM training of the p(t | q) model.The algorithm is almost as before.
The outer loop iter-ates bottom-up over nodes c1 of T1; an inner loop iter-ates bottom-up over c2 of T2.
Inside probabilities (forexample) now have the form ?c1,c2(q).
Although thisbrings the complexity up to O(n2), the real complica-tion is that there can be many fits to (c1, c2).
There arestill not too many elementary trees t1 and t2 rooted at c1and c2; but each (t1, t2) pair may be used in many ele-mentary tree pairs t, since there are exponentially manymatchings of their frontier nodes.
Fortunately, mostpairs of frontier nodes have low ?
values that indicatethat their subtrees cannot be aligned well; pairing suchnodes in a matching would result in poor global proba-bility.
This observation can be used to prune the spaceof matchings greatly.?
1-best Alignment (if desired).
This is just like train-ing, except that we use the Viterbi algorithm to find thesingle best derivation of the input tree pair.
This deriva-tion can be regarded as the optimal syntactic alignment.76A matching between A and B is a 1-to-1 correspondencebetween a subset of A and a subset of B.7As free-translation post-processing, one could try to matchpairs of stray subtrees that could have aligned well, according tothe chart, but were forced to align with null for global reasons.?
Decoding.
We create a forest of possible synchronousderivations (cf.
(Langkilde, 2000)).
We chart-parse T1as much as in section 5, but fitting the left side of anelementary tree pair to each node.
Roughly speaking:1. for c1 = null and then c1 ?
T1.V , in bottom-up order2.
for each q ?
Q, let ?c1(q) = ??3.
for each probable t = (t1, t2, q,m, s) whose t1 fits c14.
max p(t | q) ??(d1,d2)?m?
?d1(s(d1, d2)) into ?c1(q)We then extract the max-probability synchronousderivation and return the T2 that it derives.
This algo-rithm is essentially alignment to an unknown tree T2;we do not loop over its nodes c2, but choose t2 freely.7 Status of the ImplementationWe have sketched an EM algorithm to learn the probabil-ities of elementary tree pairs by training on pairs of fulltrees, and a Viterbi decoder to find optimal translations.We developed and implemented these methods at the2002 CLSP Summer Workshop at Johns Hopkins Univer-sity, as part of a team effort (led by Jan Hajic?)
to translatedependency trees from surface Czech, to deep Czech, todeep English, to surface English.
For the within-languagetranslations, it sufficed to use a simplistic, fixed model ofp(t | q) that relied entirely on morpheme identity.Team members are now developing real, trainablemodels of p(t | q), such as log-linear models on meaning-ful features of the tree pair t. Cross-language translationresults await the plugging-in of these interesting models.The algorithms we have presented serve only to ?shrink?the modeling, training and decoding problems from fulltrees to bounded, but still complex, elementary trees.H.
Alshawi, S. Bangalore, and S. Douglas.
2000.
Learningdependency translation models as collections of finite statehead transducers.
Computational Linguistics, 26(1):45?60.A.
Bo?hmova?, J.
Hajic?, E.
Hajic?ova?, and B. Hladka?.
2001.
ThePrague dependency treebank.
In A.
Abeille?, ed., Treebanks:Building & Using Syntactically Annotated Corpora.
Kluwer.Joshua Goodman.
1999.
Semiring parsing.
ComputationalLinguistics, 25(4):573?605, December.Joshua Goodman.
2002.
Efficient parsing of DOP with PCFG-reductions.
In Rens Bod, Khalil Sima?an, and Remko Scha,editors, Data Oriented Parsing.
CSLI.Kevin Knight and Daniel Marcu.
2000.
Statistics-basedsummarization?step 1: Sentence compression.
Proc.
AAAI.Kevin Knight.
1999.
Decoding complexity in word-replace-ment translation models.
Computational Linguistics, 25(4).Irene Langkilde.
2000.
Forest-based statistical sentence gener-ation.
In Proceedings of NAACL.F.
Och, C. Tillmann, and H. Ney.
1999.
Improved alignmentmodels for statistical machine translation.
Proc.
of EMNLP.A.
Poutsma.
2000.
Data-oriented translation.
Proc.
COLING.Stuart Shieber and Yves Schabes.
1990.
Synchronous tree ad-joining grammars.
In Proc.
of COLING.Dekai Wu.
1997.
Stochastic inversion transduction grammarsand bilingual parsing of parallel corpora.
Comp.
Ling., 23(3).Kenji Yamada and Kevin Knight.
2001.
A syntax-based statis-tical translation model.
In Proceedings of ACL.This work was supported by ONR grant N00014-01-1-0685,?Improving Statistical Models Via Text Analyzers Trainedfrom Parallel Corpora.?
The views expressed are the author?s.
