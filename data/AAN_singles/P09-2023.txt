Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 89?92,Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLPPredicting Barge-in Utterance Errors by usingImplicitly Supervised ASR Accuracy and Barge-in Rate per UserKazunori KomataniGraduate School of InformaticsKyoto UniversityYoshida, Sakyo, Kyoto 606-8501, Japankomatani@i.kyoto-u.ac.jpAlexander I. RudnickyComputer Science DepartmentCarnegie Mellon UniversityPittsburgh, PA 15213, U.S.A.air@cs.cmu.eduAbstractModeling of individual users is a promis-ing way of improving the performance ofspoken dialogue systems deployed for thegeneral public and utilized repeatedly.
Wedefine ?implicitly-supervised?
ASR accu-racy per user on the basis of responsesfollowing the system?s explicit confirma-tions.
We combine the estimated ASR ac-curacy with the user?s barge-in rate, whichrepresents how well the user is accus-tomed to using the system, to predict in-terpretation errors in barge-in utterances.Experimental results showed that the es-timated ASR accuracy improved predic-tion performance.
Since this ASR accu-racy and the barge-in rate are obtainableat runtime, they improve prediction perfor-mance without the need for manual label-ing.1 IntroductionThe automatic speech recognition (ASR) resultis the most important input information for spo-ken dialogue systems, and therefore, its errors arecritical problems.
Many researchers have tackledthis problem by developing ASR confidence mea-sures based on utterance-level information anddialogue-level information (Litman et al, 1999;Walker et al, 2000).
Especially in systems de-ployed for the general public such as those of (Ko-matani et al, 2005) and (Raux et al, 2006), thesystems need to correctly detect interpretation er-rors caused by various utterances made by vari-ous kinds of users including novices.
Furthermore,since some users access such systems repeatedly(Komatani et al, 2007), error detection by usingindividual user models would be a promising wayof improving performance.In another aspect in dialogue systems, cer-tain dialogue patterns indicate that ASR resultsin certain positions are reliable.
For exam-ple, Sudoh and Nakano (2005) proposed ?post-dialogue confidence scoring?
in which ASR re-sults corresponding to the user?s intention upondialogue completion are assumed to be correctand are used for confidence scoring.
Bohus andRudnicky (2007) proposed ?implicitly-supervisedlearning?
in which users?
responses following thesystem?s explicit confirmations are used for confi-dence scoring.
If ASR results can be regarded asreliable after the dialogue, machine learning algo-rithms can use such ASR results as teacher signals.This approach enables the system to improve itsperformance without any manual labeling or tran-scription, a task which requires much time and la-bor when spoken dialogue systems are developed.We focus on users?
affirmative and negative re-sponses to the system?s explicit confirmations asin (Bohus and Rudnicky, 2007) and estimate theuser?s ASR accuracy on the basis of his or her his-tory of responses.
The estimated ASR accuracy iscombined with the user?s barge-in rate to predictthe interpretation error in the current barge-in ut-terance.
Because the estimated ASR accuracy andthe barge-in rate per user are obtainable at runtime,it is possible to improve prediction performancewithout any manual transcription or labeling.2 Implicitly Supervised Estimation ofASR Accuracy2.1 Predicting Errors in Barge-in UtteranceWe aim to predict interpretation errors in barge-in utterances at runtime.
These errors are causedby ASR errors, and barge-in utterances are moreprone to be misrecognized.
A user study con-ducted by Rose and Kim (2003) revealed that thereare many more disfluencies when users barge-incompared with when users wait until the systemprompt ends.
It is difficult to select the erroneousutterances to be rejected by using a classifier that89distinguishes speech from noise on the basis of theGaussian Mixture Model (Lee et al, 2004); suchdisfluencies and resulting utterance fragments areparts of human speech.Barge-in utterances are, therefore, more diffi-cult to recognize correctly, especially when noviceusers barge-in.
To detect their interpretation er-rors, other features should be incorporated insteadof speech signals or ASR results.
We predictedthe interpretation errors in barge-in utterances onthe basis of each user?s barge-in rate (Komatani etal., 2008).
This rate intuitively corresponds to howwell users are accustomed to using the system, es-pecially to its barge-in function.Furthermore, we utilize a user?s ASR accuracyin his or her history of all utterances includingbarge-ins.
The ASR accuracy also indicates theuser?s habituation.
However, it has been shownthat the user?s ASR accuracy and barge-in ratedo not improve simultaneously (Komatani et al,2007).
In fact, some expert users have low barge-in rates.
We thus can predict whether a barge-inutterance will be correctly interpreted or not byintegrating the user?s current ASR accuracy andbarge-in rate.2.2 Estimating ASR Accuracy by usingImplicitly Supervised LabelsTo perform runtime prediction, we use informa-tion derived from the dialogue patterns to estimatethe user?s ASR accuracy.
We estimate the accu-racy on the basis of the user?s history of responsesfollowing the system?s explicit confirmations suchas ?Leaving from Kyoto Station.
Is that correct?
?Specifically, we assume that the ASR resultsof affirmative or negative responses following ex-plicit confirmations are correct and that the userutterances corresponding to the content of the af-firmative responses are also correct.
We furtherassume that the remaining utterances are incorrectbecause users do not often respond with ?no?
forexplicit confirmations containing incorrect contentand instead repeat their original utterances.
Con-sequently, we regard that the ASR results of thefollowing utterances are correct: (1) affirmativeresponses and their immediately preceding utter-ances and (2) negative responses.
Accordingly, allother utterances are incorrect.
We thus calculatethe user?s estimated ASR accuracy by using theuser?s utterance history, as follows:(Estimated ASR accuracy)=2?
(#affirmatives) + (#negatives)(#all utterances)(1)2.3 Predicting Errors by Using Barge-in Rateand ASR AccuracyWe predict the errors in barge-in utterances by us-ing a logistic regression function:P =11 + exp(?
(a1x1+ a2x2+ b)).Its inputs x1and x2are the barge-in rate until thecurrent utterance and ASR accuracy until the pre-vious utterance.
To account for temporal changesin barge-in rates, we set a window when calculat-ing them (Komatani et al, 2008).
That is, whenthe window width is N , the rates are calculated byusing only the last N utterances, and the previousutterances are discarded.
When the window widthexceeds the total number of utterances by the user,the barge-in rates are calculated by using all theuser?s utterances.
Thus, when the width exceeds2,838, the maximum number of utterances madeby one user in our data, the barge-in rates equalthe average rates of all previous utterances by theuser.We calculate the estimated ASR accuracy everytime a user makes an affirmative or negative re-sponse.
When the user makes other utterances, wetake the estimated accuracy when the last affirma-tive/negative response is made to be the accuracyof those utterances.3 Experimental Evaluation3.1 Target DataWe used data collected by the Kyoto City Bus In-formation System (Komatani et al, 2005).
Thissystem locates a bus that a user wants to ride andtells the user how long it will be before the busarrives.
The system was accessible to the publicby telephone.
It used the safest strategy to preventerroneous responses, that is, to make explicit con-firmations for all ASR results.We used 27,519 utterances after removing callswhose phone numbers were not recorded andthose the system developer called for debugging.From that number, there were 7,193 barge-in ut-terances, i.e., utterances that a user starts speakingduring a system prompt.
The phone numbers ofthe calls were recorded, and we assumed that each90Table 1: ASR accuracy by response typeCorrect Incorrect Total (Acc.
)Affirmative 9,055 246 9,301 (97.4%)Negative 2,006 289 2,295 (87.4%)Other 8,914 7,009 15,923 (57.9%)Total 19,975 7,544 27,519 (72.6%)00.10.20.30.40.50.60.70.80.910  0.2  0.4  0.6  0.8  1EstimatedASRAccuracy.Transcription-based ASR AccuracyFigure 1: Correlation between transcription-basedand estimated ASR accuracynumber corresponded to one individual.
Most ofthe numbers were those of mobile phones, whichare usually not shared, so the assumption seemsreasonable.Each utterance was transcribed and its interpre-tation result, correct or not, was given manually.We assumed that an interpretation result for anutterance was correct if all content words in itstranscription were correctly included in the result.The result was regarded as an error if any contentwords were missed or misrecognized.3.2 Verifying Implicitly Supervised LabelsWe confirmed our assumption that the ASR re-sults of affirmative or negative responses follow-ing explicit confirmations are correct.
We clas-sified the user utterances into affirmatives, nega-tives, and other, and calculated the ASR accuracies(precision rates) as shown in Table 1.
Affirmativesinclude hai (?yes?
), soudesu (?that?s right?
), OK,etc; and negatives include iie (?no?
), chigaimasu(?I don?t agree?
), dame (?No good?
), etc.
The ta-ble indicates that the ASR accuracies of affirma-tives and negatives were high.
One of the reasonsfor the high accuracy was that these utterances aremuch shorter than other content words, so theywere not confused with other content words.
An-other reason was that the system often gave helpmessages such as ?Please answer yes or no.
?We then analyzed the correlation between thetranscription-based ASR accuracy and the esti-5560657075801  10  100  1000  10000PredictionAcc.Window widthbarge-in rate onlycorrect ASR acc.
+ barge-in rateestimated ASR acc.
+ barge-in rateFigure 2: Prediction accuracy with various win-dow widthsmated ASR accuracy based on Equation 1.
Weplotted the two ASR accuracies in Figure 1 for26,231 utterances made after at least one affir-mative/negative response by the user.
The corre-lation coefficient between them was 0.806.
Al-though the assumption that all ASR results of af-firmative/negative responses are correct might bestrong, the estimated ASR accuracy had a highcorrelation with the transcription-based ASR ac-curacy.3.3 Prediction using Implicitly SupervisedLabelsWe measured the prediction accuracy for 7,193barge-in utterances under several conditions.
Wedid not set windows when calculating the ASR ac-curacies and thus used all previous utterances ofthe user, because the windows did not improveprediction accuracy.
One of the reasons for thislack of improvement is that the ASR accuraciesdid not change as significantly as the barge-in ratesbecause the accuracies of frequent users convergedearlier (Komatani et al, 2007).We first confirmed the effect of thetranscription-based (?correct?, hereafter) ASRaccuracy.
As shown in Figure 2 and Table 2,the prediction accuracy improved by using theASR accuracy in addition to the barge-in rate.The best prediction accuracy (78.6%) was whenthe window width of the barge-in rate was 100,and the accuracy converged when the width was30.
The prediction accuracy was 72.7% whenonly the ?correct?
ASR accuracy was used, andthe prediction accuracy was 71.8% when onlythe barge-in rate was used.
Thus, the predictionaccuracy was better when both inputs were usedrather than when either input was used.
This91Table 2: Best prediction accuracies for each con-dition and window width wConditions (Used inputs) Prediction acc.
(%)barge-in rate 71.8 (w=30)correct ASR acc.
72.7+ barge-in rate 78.6 (w=100)estimated ASR acc.
59.4+ barge-in rate 74.3 (w=30)fact indicates that both the barge-in rate andASR accuracy have different information andcontribute to the prediction accuracy.Next, we analyzed the prediction accuracy afterreplacing the correct ASR accuracy with the esti-mated one described in Section 2.2.
The best ac-curacy (74.3%) was when the window width was30.
This accuracy was higher than that of usingonly barge-in rates.
Hence, the estimated ASR ac-curacy without manual labeling is effective in pre-dicting the errors in barge-in utterances at runtime.4 ConclusionWe proposed a method to estimate the errors inbarge-in utterances by using a novel dialogue-levelfeature obtainable at runtime.
This method doesnot require supervised manual labeling.
The esti-mated ASR accuracy based on the user?s utterancehistory was dependable in predicting the errors inthe current utterance.
We thus showed that ASRaccuracy can be estimated in an implicitly super-vised manner.The information obtained by our method can beused for confidence scoring.
Thus, our future workwill include integrating the proposed features withbottom-up information such as acoustic-score-based confidence measures.
Additionally, we sim-ply assumed in this study that all affirmative andnegative responses following the explicit confir-mation are correct.
By modeling this assumptionmore precisely, prediction accuracy will improve.Finally, we identified individuals on the basis oftheir telephone numbers.
If we utilize user identi-fication techniques to account for situations whenno speaker information is available beforehand,this method can be applied to systems other thantelephone-based ones, e.g., to human-robot inter-action.AcknowledgmentsWe are grateful to Prof. Tatsuya Kawahara of Ky-oto University who led the project of the KyotoCity Bus Information System.ReferencesDan Bohus and Alexander Rudnicky.
2007.
Implicitly-supervised learning in spoken language interfaces: an ap-plication to the confidence annotation problem.
In Proc.SIGdial Workshop on Discourse and Dialogue, pages256?264.Kazunori Komatani, Shinichi Ueno, Tatsuya Kawahara, andHiroshi G. Okuno.
2005.
User modeling in spoken dia-logue systems to generate flexible guidance.
User Model-ing and User-Adapted Interaction, 15(1):169?183.Kazunori Komatani, Tatsuya Kawahara, and Hiroshi G.Okuno.
2007.
Analyzing temporal transition of real user?sbehaviors in a spoken dialogue system.
In Proc.
INTER-SPEECH, pages 142?145.Kazunori Komatani, Tatsuya Kawahara, and Hiroshi G.Okuno.
2008.
Predicting ASR errors by exploiting barge-in rate of individual users for spoken dialogue systems.
InProc.
INTERSPEECH, pages 183?186.Akinobu Lee, Keisuke Nakamura, Ryuichi Nisimura, HiroshiSaruwatari, and Kiyohiro Shikano.
2004.
Noice robustreal world spoken dialogue system using GMM based re-jection of unintended inputs.
In Proc.
Int?l Conf.
SpokenLanguage Processing (ICSLP), pages 173?176.Diane J. Litman, Marilyn A. Walker, and Michael S. Kearns.1999.
Automatic detection of poor speech recognition atthe dialogue level.
In Proc.
Annual Meeting of the Asso-ciation for Computational Linguistics (ACL), pages 309?316.Antoine Raux, Dan Bohus, Brian Langner, Alan W. Black,and Maxine Eskenazi.
2006.
Doing research on a de-ployed spoken dialogue system: One year of Let?s Go!experience.
In Proc.
INTERSPEECH.Richard C. Rose and Hong Kook Kim.
2003.
A hy-brid barge-in procedure for more reliable turn-taking inhuman-machine dialog systems.
In Proc.
IEEE Auto-matic Speech Recognition and Understanding Workshop(ASRU), pages 198?203.Katsuhito Sudoh and Mikio Nanano.
2005.
Post-dialogueconfidence scoring for unsupervised statistical languagemodel training.
Speech Communication, 45:387?400.Marilyn Walker, Irene Langkilde, Jerry Wright, Allen Gorin,and Diane Litman.
2000.
Learning to predict problematicsituations in a spoken dialogue system: Experiments withHow May I Help You?
In Proc.
North American Chapterof Association for Computational Linguistics (NAACL),pages 210?217.92
