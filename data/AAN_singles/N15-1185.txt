Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1616?1626,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational Linguistics?You?re Mr. Lebowski, I?m the Dude?
:Inducing Address Term Formality in Signed Social NetworksVinodh KrishnanCollege of ComputingGeorgia Institute of TechnologyAtlanta, GA 30308krishnan.vinodh@gmail.comJacob EisensteinSchool of Interactive ComputingGeorgia Institute of TechnologyAtlanta, GA 30308jacobe@gatech.eduAbstractWe present an unsupervised model for induc-ing signed social networks from the contentexchanged across network edges.
Inferencein this model solves three problems simulta-neously: (1) identifying the sign of each edge;(2) characterizing the distribution over contentfor each edge type; (3) estimating weights fortriadic features that map to theoretical mod-els such as structural balance.
We apply thismodel to the problem of inducing the socialfunction of address terms, such as Madame,comrade, and dude.
On a dataset of moviescripts, our system obtains a coherent cluster-ing of address terms, while at the same timemaking intuitively plausible judgments of theformality of social relations in each film.
Asan additional contribution, we provide a boot-strapping technique for identifying and tag-ging address terms in dialogue.11 IntroductionOne of the core communicative functions of lan-guage is to modulate and reproduce social dynam-ics, such as friendship, familiarity, formality, andpower (Hymes, 1972).
However, large-scale em-pirical work on understanding this communicativefunction has been stymied by a lack of labeled data:it is not clear what to annotate, let alne whetherand how such annotations can be produced reliably.Computational linguistics has made great progressin modeling language?s informational dimension,1Code and data for this paper is available at https://github.com/vinodhkris/signed-social.but ?
with a few notable exceptions ?
computa-tion has had little to contribute to our understandingof language?s social dimension.Yet there is a rich theoretical literature on socialstructures and dynamics.
In this paper, we focuson one such structure: signed social networks, inwhich edges between individuals are annotated withinformation about the nature of the relationship.
Forexample, the individuals in a dyad may be friendsor foes; they may be on formal or informal terms;or they may be in an asymmetric power relation-ship.
Several theories characterize signed social net-works: in structural balance theory, edge signs in-dicate friendship and enmity, with some triads ofsigned edges being stable, and others being unsta-ble (Cartwright and Harary, 1956); conversely, instatus theory (Leskovec et al, 2010b), edges indicatestatus differentials, and triads should obey transitiv-ity.
But these theoretical models can only be appliedwhen the sign of each social network connection isknown, and they do not answer the sociolinguisticquestion of how the sign of a social tie relates to thelanguage that is exchanged across it.We present a unified statistical model that incor-porates both network structure and linguistic con-tent.
The model connects signed social networkswith address terms (Brown and Ford, 1961), whichinclude names, titles, and ?placeholder names,?
suchas dude.
The choice of address terms is an indica-tor of the level of formality between the two parties:for example, in contemporary North American En-glish, a formal relationship is signaled by the useof titles such as Ms and Mr, while an informal re-lationship is signaled by the use of first names and1616placeholder names.
These tendencies can be cap-tured with a multinomial distribution over addressterms, conditioned on the nature of the relationship.However, the linguistic signal is not the only indi-cator of formality: network structural properties canalso come into play.
For example, if two individ-uals share a mutual friend, with which both are oninformal terms, then they too are more likely to havean informal relationship.
With a log-linear prior dis-tribution over network structures, it is possible to in-corporate such triadic features, which relate to struc-tural balance and status theory.Given a dataset of unlabeled network structuresand linguistic content, inference in this model simul-taneously induces three quantities of interest:?
a clustering of network edges into types;?
a probabilistic model of the address terms thatare used across each edge type, thus revealingthe social meaning of these address terms;?
weights for triadic features of signed networks,which can then be compared with the predic-tions of existing social theories.Such inferences can be viewed as a form of so-ciolinguistic structure induction, permitting socialmeanings to be drawn from linguistic data.
In addi-tion to the model and the associated inference pro-cedure, we also present an approach for inducinga lexicon of address terms, and for tagging themin dialogues.
We apply this procedure to a datasetof movie scripts (Danescu-Niculescu-Mizil and Lee,2011).
Quantitative evaluation against human rat-ings shows that the induced clusters of address termscorrespond to intuitive perceptions of formality, andthat the network structural features improve pre-dictive likelihood over a purely text-based model.Qualitative evaluation shows that the model makesreasonable predictions of the level of formality ofsocial network ties in well-known movies.We first describe our model for linking networkstructure and linguistic content in general terms, asit can be used for many types of linguistic con-tent and edge labels.
Next we describe a procedurewhich semi-automatically induces a lexicon of ad-dress terms, and then automatically labels them intext.
We then describe the application of this proce-dure to a dataset of movie dialogues, including quan-titative and qualitative evaluations.2 Joint model of signed social networksand textual contentWe now present a probabilistic model for linkingnetwork structure with content exchanged over thenetwork.
In this section, the model is presented ingeneral terms, so that it can be applied to any type ofevent counts, with any form of discrete edge labels.The application of the model to forms of address isdescribed in Sections 4 and 5.We observe a dataset of undirected graphs G(t)={i, j}, with a total ordering on nodes such that i < jin all edges.
For each edge ?i, j?, we observe di-rected content vectors xi?jand xi?j, which mayrepresent counts of words or other discrete events,such as up-votes and down-votes for comments ina forum thread.
We hypothesize a latent edge labelyij?
Y , so that xi?jand xi?jare conditioned onyij.
In this paper we focus on binary labels (e.g.,Y = {+,?
}), but the approach generalizes to largerfinite discrete sets, such as directed binary labels(e.g., Y = {++,+?,?+,??})
and comparativestatus labels (e.g., Y = {<,>,?
}).We model the likelihood of the observations con-ditioned on the edge labels as multinomial,xi?j| yij?Multinomial(?
?yij) (1)xi?j| yij?Multinomial(??yij).
(2)Parameter tying can be employed to handle spe-cial cases.
For example, if the edge labels are undi-rected, then we add the constraint ?
?y= ?
?y,?y.If the edge labels reflect relative status, then wewould instead add the constraints (?
?<= ??>),(?
?>= ?
?<), and (??
?= ???
).The distribution over edge labelings P (y) is mod-eled in a log-linear framework, with features that canconsider network structure and signed triads:P (y;G,?,?)
=1Z(?,?;G)?
exp??i,j?
?G?>f(yij, i, j, G)?
exp??i,j,k?
?T (G)?yij,yjk,yik, (3)1617where T (G) is the set of triads in the graph G.The first term of Equation 3 represents a normal-izing constant.
The second term includes weights?, which apply to network features f(yij, i, j, G).This can include features like the number of mu-tual friends between nodes i and j, or any numberof more elaborate structural features (Liben-Nowelland Kleinberg, 2007).
For example, the featureweights ?
could ensure that the edge label Yij= +is especially likely when nodes i and j have manymutual friends in G. However, these features cannotconsider any edge labels besides yij.In the third line of Equation 3, each weight?yij,yjk,yikcorresponds to a signed triad type, invari-ant to rotation.
In a binary signed network, struc-tural balance theory would suggest positive weightsfor ?+++(all friends) and ?+??
(two friends anda mutual enemy), and negative weights for ?++?
(two enemies and a mutual friend) and ????(allenemies).
In contrast, a status-based network theorywould penalize non-transitive triads such as ?>><.Thus, in an unsupervised model, we can examine theweights to learn about the semantics of the inducededge types, and to see which theory best describesthe signed network configurations that follow fromthe linguistic signal.
This is a natural next step fromprior work that computes the frequency of triads inexplicitly-labeled signed social networks (Leskovecet al, 2010b).3 Inference and estimationOur goal is to estimate the parameters ?, ?, and ?,given observations of network structures G(t)andlinguistic content x(t), for t ?
{1, .
.
.
, T}.
Elidingthe sum over instances t, we seek to maximize thevariational lower bound on the expected likelihood,LQ=EQ[logP (y,x;?,?, G)]?
EQ[logQ(y)]=EQ[logP (x | y;?)]
+ EQ[logP (y;G,?,?)]?
EQ[logQ(y)].
(4)The first and third terms factor across edges,EQ[logP (x | y;?)]
=??i,j??G?y??Yqij(y?
)x>i?jlog ??y?+qij(y?
)x>i?jlog ?
?y?EQ[logQ(y)] =??i,j??G?y??Yqij(y?)
log q(y?
).The expected log-prior EQ[logP (y)] is com-puted from the prior distribution defined in Equa-tion 3, and therefore involves triads of edge labels,EQ[logP (y;?,?)]
= ?
logZ(?,?;G)+??i,j??G?y?qij(y?
)?>f(y?, i, j, G)+??i,j,k?
?T (G)?y,y?,y??qij(y)qjk(y?)qik(y??)?y,y?,y?
?.We can reach a local maximum of thevariational bound by applying expectation-maximization (Dempster et al, 1977), iteratingbetween updates to Q(y), and updates to theparameters ?,?,?.
This procedure is summarizedin Table 1, and described in more detail below.3.1 E-stepIn the E-step, we sequentially update each qij, takingthe derivative of Equation 4:?LQ?qij(y)= logP (xi?j| Yij= y;??
)+ logP (xi?j| Yij= y;??
)+ EQ(y?
(ij))[logP (y | Yij= y;?,?)]?
log qij(y)?
1.
(5)After adding a Lagrange multiplier to ensure that?yqij(y) = 1, we obtain a closed-form solutionfor each qij(y).
These iterative updates to qijcanbe viewed as a form of mean field inference (Wain-wright and Jordan, 2008).3.2 M-stepIn the general case, the maximum expected likeli-hood solution for the content parameter ?
is givenby the expected counts,??y???i,j??Gqij(y)xi?j(6)??y???i,j??Gqij(y)xi?j.
(7)As noted above, we are often interested in specialcases that require parameter tying, such as ??y=??y,?y.
This can be handled by simply computingexpected counts across the tied parameters.16181.
Initialize Q(Y(t)) for each t ?
{1 .
.
.
T}2.
Iterate until convergence:E-step update each qijin closed form, basedon Equation 5.M-step: content Update ?
in closed formfrom Equations 6 and 7.M-step: structure Update ?,?, and c by ap-plying L-BFGS to the noise-contrastiveestimation objective in Equation 8.Table 1: Expectation-maximization estimation procedureObtaining estimates for ?
and ?
is more challeng-ing, as it would seem to involve computing the par-tition function Z(?,?
;G), which sums over all pos-sible labeling of each network G(t).
The number ofsuch labelings is exponential in the number of edgesin the network.
West et al (2014) show that for anobjective function involving features on triads anddyads, it is NP-hard to find even the single optimallabeling.We therefore apply noise-contrastive estimation(NCE; Gutmann and Hyv?arinen, 2012), whichtransforms the problem of estimating the densityP (y) into a classification problem: distinguishingthe observed graph labelings y(t)from randomly-generated ?noise?
labelings?y(t)?
Pn, where Pnis a noise distribution.
NCE introduces an addi-tional parameter c for the partition function, so thatlogP (y;?,?, c) = logP0(y;?,?
)+c, with P0(y)representing the unnormalized probability of y. Wecan then obtain the NCE objective by writingD = 1for the case that y is drawn from the data distribu-tion and D = 0 for the case that y is drawn from thenoise distribution,JNCE(?,?, c)=?tlogP (D = 1 | y(t);?,?, c)?
logP (D = 0 |?y(t);?,?, c), (8)where we draw exactly one noise instance?y for eachtrue labeling y(t).Because we are working in an unsupervised set-ting, we do not observe y(t), so we cannot directlycompute the log probability in Equation 8.
Instead,we compute the expectations of the relevant logprobabilities, under the distribution Q(y),EQ[logP0(y;?,?)]
=??i,j?
?G?yqij(y)?>f(y, i, j,G)+?k:?i,j,k?
?T (G)?y,y?,y??qij(y)qjk(y?)qik(y??)?y,y?,y??.
(9)We define the noise distribution Pnby samplingedge labels yijfrom their empirical distribution un-der Q(y).
The expectation Eq[logPn(y)] is there-fore simply the negative entropy of this empiricaldistribution, multiplied by the number of edges inG.We then plug in these expected log-probabilities tothe noise-contrastive estimation objective function,and take derivatives with respect to the parameters?, ?, and c. In each iteration of the M-step, weoptimize these parameters using L-BFGS (Liu andNocedal, 1989).4 Identifying address terms in dialogueThe model described in the previous sections is ap-plied in a study of the social meaning of addressterms ?
terms for addressing individual people ?which include:Names such as Barack, Barack Hussein Obama.Titles such as Ms., Dr., Private, Reverend.
Titlescan be used for address either by preceding aname (e.g., Colonel Kurtz), or in isolation (e.g.,Yes, Colonel.
).Placeholder names such as dude (Kiesling, 2004),bro, brother, sweetie, cousin, and asshole.These terms can be used for address only in iso-lation (for example, in the address cousin Sue,the term cousin would be considered a title).Because address terms connote varying levels offormality and familiarity, they play a critical rolein establishing and maintaining social relationships.However, we find no prior work on automaticallyidentifying address terms in dialogue transcripts.There are several subtasks: (1) distinguishing ad-dresses from mentions of other individuals, (2) iden-tifying a lexicon of titles, which either precede nameaddresses or can be used in isolation, (3) identifying1619Text: I ?m not Mr. Lebowski ; you ?re Mr. Lebowski .POS: PRP VBP RB NNP NNP : PRP VBP NNP NNP .Address: O O O B-ADDR L-ADDR O O O B-ADDR L-ADDR OFigure 1: Automatic re-annotation of dialogue data for address term sequencesFeature DescriptionLexical The word to be tagged, and itstwo predecessors and successors,wi?2:i+2.POS The part-of-speech of the token tobe tagged, and the POS tags of itstwo predecessors and successors.Case The case (lower, upper, or title) ofthe word to be tagged, and its twopredessors and successors.ConstituencyparseFirst non-NNP ancestor node ofthe word wiin the constituentparse tree, and all leaf node sib-lings in the tree.DependencyparseAll dependency relations involv-ing wi.Location Distance of wifrom the start andthe end of the sentence or turn.Punctuation All punctuation symbols occur-ring before and after wi.Second personpronounAll forms of the second personpronoun within the sentence.Table 2: Features used to identify address spansa lexicon of placeholder names, which can only beused in isolation.
We now present a tagging-basedapproach for performing each of these subtasks.We build an automatically-labeled dataset fromthe corpus of movie dialogues provided by Danescu-Niculescu-Mizil and Lee (2011); see Section 6 formore details.
This dataset gives the identity ofthe speaker and addressee of each line of dialogue.These identities constitute a minimal form of manualannotation, but in many settings, such as social me-dia dialogues, they could be obtained automatically.We augment this data by obtaining the first (given)and last (family) names of each character, which wemine from the website rottentomatoes.com.Next, we apply the CoreNLP part-of-speech tag-ger (Manning et al, 2014) to identify sequences ofthe NNP tag, which indicates a proper noun in thePenn Treebank Tagset (Marcus et al, 1993).
Foreach NNP tag sequence that contains the name of theaddressee, we label it as an address, using BILOUnotation (Ratinov and Roth, 2009): Beginning,Inside, and Last term of address segments; Outsideand Unit-length sequences.
An example of this tag-ging scheme is shown in Figure 1.Next, we train a classifier (Support Vector Ma-chine with a linear kernel) on this automatically la-beled data, using the features shown in Table 2.
Forsimplicity, we do not perform structured prediction,which might offer further improvements in accuracy.This classifier provides an initial, partial solutionto the first problem, distinguishing second-personaddresses from references to other individuals (forname references only).
On heldout data, the clas-sifier?s macro-averaged F-measure is 83%, and itsmicro-averaged F-measure is 98.7%.
Class-by-classbreakdowns are shown in Table 3.4.1 Address term lexiconsTo our surprise, we were unable to find manually-labeled lexicons for either titles or placeholdernames.
We therefore employ a semi-automated ap-proach to construct address term lexicons, bootstrap-ping from the address term tagger to build candidatelists, which we then manually filter.Titles To induce a lexicon of titles, we considerterms that are frequently labeled with the tag B-ADDR across a variety of dialogues, performing abinomial test to obtain a list of terms whose fre-quency of being labeled as B-ADDR is significantlyhigher than chance.
Of these 34 candidate terms, wemanually filter out 17, which are mainly commonfirst names, such as John; such names are frequentlylabeled as B-ADDR across movies.
After this man-ual filtering, we obtain the following titles: agent,aunt, captain, colonel, commander, cousin, deputy,detective, dr, herr, inspector, judge, lord, master,mayor, miss, mister, miz, monsieur, mr, mrs, ms, pro-fessor, queen, reverend, sergeant, uncle.1620Placeholder names To induce a lexicon of place-holder names, we remove the CURRENT-WORD fea-ture from the model, and re-run the tagger on alldialogue data.
We then focus on terms which arefrequently labeled U-ADDR, indicating that theyare the sole token in the address (e.g., I?m/O per-fectly/O calm/O, dude/U-ADDR.)
We again per-form a binomial test to obtain a list of terms whosefrequency of being labeled U-ADDR is significantlyhigher than chance.
We manually filter out 41 termsfrom a list of 96 possible placeholder terms obtainedin the previous step.
Most terms eliminated wereplural forms of placeholder names, such as fellasand dudes; these are indeed address terms, but be-cause they are plural, they cannot refer to a singleindividual, as required by our model.
Other falsepositives were fillers, such as uh and um, whichwere ocassionally labeled as I-ADDR by our tag-ger.
After manual filtering, we obtain the followingplaceholder names: asshole, babe, baby, boss, boy,bro, bud, buddy, cocksucker, convict, cousin, cow-boy, cunt, dad, darling, dear, detective, doll, dude,dummy, father, fella, gal, ho, hon, honey, kid, lad,lady, lover, ma, madam, madame, man, mate, mis-ter, mon, moron, motherfucker, pal, papa, partner,peanut, pet, pilgrim, pop, president, punk, shithead,sir, sire, son, sonny, sport, sucker, sugar, sweetheart,sweetie, tiger.4.2 Address term tokensWhen constructing the content vectors xi?jandxi?j, we run the address span tagger describedabove, and include counts for the following types ofaddress spans:?
the bare first name, last name, and completename of individual j;?
any element in the title lexicon if labeled as B-ADDR by the tagger;?
any element in the title or placeholder lexicon,if labeled as U-ADDR by the tagger.5 Address terms in a model of formalityAddress terms play a key role in setting the formalityof a social interaction.
However, understanding thisrole is challenging.
While some address terms, likeMs and Sir, are frequent, there is a long tail of rareClass F-measureTotal InstancesI-ADDR 0.58 53B-ADDR 0.800 483U-ADDR 0.987 1864L-ADDR 0.813 535O-ADDR 0.993 35975Table 3: Breakdown of f-measure and number of in-stances by class in the test set.terms whose meaning is more difficult to ascertainfrom data, such as admiral, dude, and player.
More-over, the precise social meaning of address terms canbe context-dependent: for example, the term com-rade may be formal in some contexts, but jokinglyinformal in others.Both problems can be ameliorated by adding so-cial network structure.
We treat Y = V as indicatingformality and Y = T as indicating informality.
(Thenotation invokes the concept of T/V systems frompoliteness theory (Brown, 1987), where T refers tothe informal Latin second-person pronoun tu, and Vrefers to the formal second-person pronoun vos.
)While formality relations are clearly asymmetricin many settings, for simplicity we assume symmet-ric relations: each pair of individuals is either on for-mal or informal terms with each other.
We thereforeadd the constraints that ?
?V= ?
?Vand ?
?T= ?
?T.In this model, we have a soft expectation that triadswill obey transitivity: for example, if i and j havean informal relationship, and j and k have an in-formal relationship, then i and k are more likely tohave an informal relationship.
After rotation, thereare four possible triads, TTT, TTV, TVV, and VVV.The weights estimated for these triads will indicatewhether our prior expectations are validated.
Wealso consider a single pairwise feature template, ametric from Adamic and Adar (2003) that sums overthe mutual friends of i and j, assigning more weightto mutual friends who themselves have a small num-ber of friends:AA(i, j) =?k??(i)?k??
(j)1log #|?
(k)|, (10)where ?
(i) is the set of friends of node i.
(Wealso tried simply counting the number of mu-tual friends, but the Adamic-Adar metric performs1621slightly better.)
This feature appears in the vectorf(yij, i, j, G), as defined in Equation 3.6 Application to movie dialoguesWe apply the ideas in this paper to a datasetof movie dialogues (Danescu-Niculescu-Mizil andLee, 2011), including roughly 300,000 conversa-tional turns between 10,000 pairs of characters in617 movies.
This dataset is chosen because it notonly provides the script of each movie, but also indi-cates which characters are in dialogue in each line.We evaluate on quantitative measures of predictivelikelihood (a token-level evaluation) and coherenceof the induced address term clusters (a type-levelevaluation).
In addition, we describe in detail theinferred signed social networks on two films.We evaluate the effects of three groups of fea-tures: address terms, mutual friends (using theAdamic-Adar metric), and triads.
We include ad-dress terms in all evaluations, and test whether thenetwork features improve performance.
Ablatingboth network features is equivalent to clusteringdyads by the counts of address terms, but all eval-uations were performed by ablating components ofthe full model.
We also tried ablating the text fea-tures, clustering edges using only the mutual friendsand triad features, but we found that the resultingclusters were incoherent, with no discernible rela-tionship to the address terms.6.1 Predictive log-likelihoodTo compute the predictive log-likelihood of the ad-dress terms, we hold out a randomly-selected 10%of films.
On these films, we use the first 50%of address terms to estimate the dyad-label beliefsqij(y).
We then evaluate the expected log-likelihoodof the second 50% of address terms, computed as?yqij(y)?nlogP (xn| ?y) for each dyad.
Thisis comparable to standard techniques for computingthe held-out log-likelihood of topic models (Wallachet al, 2009).As shown in Table 4, the full model substantiallyoutperforms the ablated alternatives.
This indicatesthat the signed triad features contribute meaningfulinformation towards the understanding of addressterms in dialogue.AddresstermsMutualfriendsSignedtriadsLog-likelihoodX -2133.28X X -2018.21X X -1884.02X X X -1582.43Table 4: Predictive log-likelihoods.V-cluster T-clustersir FIRSTNAMEmr+LASTNAME manmr+FIRSTNAME babymr honeymiss+LASTNAME darlingson sweetheartmister+FIRSTNAME buddymrs sweetiemrs+LASTNAME honFIRSTNAME+LASTNAME dudeTable 5: The ten strongest address terms for each cluster,sorted by likelihood ratio.6.2 Cluster coherenceNext, we consider the model inferences that re-sult when applying the EM procedure to the entiredataset.
Table 5 presents the top address terms foreach cluster, according to likelihood ratio.
The clus-ter shown on the left emphasizes full names, titles,and formal address, while the cluster on the right in-cludes the given name and informal address termssuch as man, baby, and dude.
We therefore use thelabels ?V-cluster?
and ?T-cluster?, referring to theformal and informal clusters, respectively.We perform a quantitative evaluation of this clus-tering through an intrusion task (Chang et al, 2009).Specifically, we show individual raters three terms,selected so that two terms are from the same clus-ter, and the third term is from the other cluster; wethen ask them to identify which term is least likethe other two.
Five raters were each given a list offorty triples, with the order randomized.
Of the fortytriples, twenty were from our full model, and twentywere from a text-only clustering model.
The ratersagreed with our full model in 73% percent of cases,and agreed with the text-only model in 52% percent1622ttt+1.23vtt-1.05vvt-6.48 vvv+3.73Figure 2: Estimated triad feature weightsof cases.
By Fisher?s exact test, this difference isstatistically significant at p < 0.01.
Both results aresignificantly greater than chance agreement (33%)by a binomial test, p < 0.001.6.3 Network feature weightsFigure 2 shows the feature weights for each of thefour possible triads.
Triads with homogeneous signsare preferred, particularly TTT (all informal); het-erogeneous triads are dispreferred, particularly TTV,which is when two individuals have a formal rela-tionship despite having a mutual informal tie.
Lessdispreferred is TVV, when a pair of friends have aninformal relationship despite both having a formalrelationship with a third person; consider, for exam-ple, the situation of two students and their professor.In addition, the informal sign is preferred when thedyad has a high score on the Adamic-Adar metric,and dispreferred otherwise.
This coheres with theintuition that highly-embedded edges are likely tobe informal, with many shared friends.6.4 Qualitative resultsAnalysis of individual movies suggests that the in-duced tie signs are meaningful and coherent.
Forexample, the film ?Star Wars?
is a space opera, inwhich the protagonists Luke, Han, and Leia attemptto defeat an evil empire led by Darth Vader.
The in-duced signed social network is shown in Figure 3.The V-edges seem reasonable: C-3PO is a roboticservant, and Blue Leader is Luke?s military com-mander (BLUE LEADER: Forget it, son.
LUKE: Yes,sir, but I can get him...).
In contrast, the characterpairs with T-edges all have informal relationships:the lesser-known character Biggs is Luke?s more ex-perienced friend (BIGGS: That?s no battle, kid).The animated film ?South Park: Bigger, Longer& Uncut?
centers on three children: Stan, Cartman,and Kyle; it also involves their parents, teachers, andfriends, as well as a number of political and religiousfigures.
The induced social network is shown in Fig-BENBLUELEADERHANLUKEC-3POLEIABIGGSFigure 3: Induced signed social network from the filmStar Wars.
Blue solid edges are in the V-cluster, reddashed edges are in the T-cluster.ure 4.
The children and their associates mostly haveT-edges, except for the edge to Gregory, a Britishcharacter with few speaking turns.
This part of thenetwork also has a higher clustering coefficient, asthe main characters share friends such as Chef andThe Mole.
The left side of the diagram centers onKyle?s mother, who has more formal relationshipswith a variety of authority figures.7 Related workRecent work has explored the application of signedsocial network models to social media.
Leskovecet al (2010b) find three social media datasets fromwhich they are able to identify edge polarity; this en-ables them to compare the frequency of signed triadsagainst baseline expectations, and to build a classi-fier to predict edge labels (Leskovec et al, 2010a).However, in many of the most popular social mediaplatforms, such as Twitter and Facebook, there is nometadata describing edge labels.
We are also inter-ested in new applications of signed social networkanalysis to datasets outside the realm of social me-dia, such as literary texts (Moretti, 2005; Elson et al,2010; Agarwal et al, 2013) and movie scripts, but insuch corpora, edge labels are not easily available.In many datasets, it is possible to obtain the tex-tual content exchanged between members of the net-work, and this content can provide a signal for net-work structure.
For example, Hassan et al (2012)characterize the sign of each network edge in terms1623CARTMANKENNYWENDYMR.GARRISONBIGTHINGPHILLIPCANADIANMINISTEROFMOVIESPRINCIPALVICTORIACARTMAN'SMOMKYLE CHEFSADDAMHUSSEINSATANIKEMR.MACKEYSTANTERRANCEKYLE'SMOMGREGORYPRESIDENTTHEMOLEFigure 4: Induced signed social network from the filmSouth Park: Bigger, Longer & Uncut.
Blue solid edgesare in the V-cluster, red dashed edges are in the T-cluster.of the sentiment expressed across it, finding thatthe resulting networks cohere with the predictionsof structural balance theory; similar results are ob-tained by West et al (2014), who are thereby ableto predict the signs of unlabeled ties.
Both papersleverage the relatively mature technology of senti-ment analysis, and are restricted to edge labels thatreflect sentiment.
The unsupervised approach pre-sented here could in principle be applied to lexiconsof sentiment terms, rather than address terms, but weleave this for future work.The issue of address formality in English was con-sidered by Faruqui and Pad?o (2011), who show thatannotators can label the formality of the second per-son pronoun with agreement of 70%.
They use theseannotations to train a supervised classifier, obtain-ing comparable accuracy.
If no labeled data is avail-able, annotations can be projected from languageswhere the T/V distinction is marked in the mor-phology of the second person pronoun, such as Ger-man (Faruqui and Pad?o, 2012).
Our work shows thatit is possible to detect formality without labeled dataor parallel text, by leveraging regularities across net-work structures; however, this requires the assump-tion that the level of formality for a pair of individu-als is constant over time.
The combination of ourunsupervised approach with annotation projectionmight yield models that attain higher performancewhile capturing change in formality over time.More broadly, a number of recent papers haveproposed to detect various types of social relation-ships from linguistic content.
Of particular interestare power relationships, which can be induced fromn-gram features (Bramsen et al, 2011; Prabhakaranet al, 2012) and from coordination, where one par-ticipant?s linguistic style is asymmetrically affectedby the other (Danescu-Niculescu-Mizil et al, 2012).Danescu-Niculescu-Mizil et al (2013) describe anapproach to recognizing politeness in text, lexicaland syntactic features motivated by politeness the-ory.
Anand et al (2011) detect ?rebuttals?
in argu-mentative dialogues, and Hasan and Ng (2013) em-ploy extra-linguistic structural features to improvethe detection of stances in such debates.
In all ofthese cases, labeled data is used to train supervisedmodel; our work shows that social structural regu-larities are powerful enough to support accurate in-duction of social relationships (and their linguisticcorrelates) without labeled data.8 ConclusionThis paper represents a step towards unifying the-oretical models of signed social network structureswith linguistic accounts of the expression of socialrelationships in dialogue.
By fusing these two phe-nomena into a joint probabilistic model, we can in-duce edge types with robust linguistic signatures andcoherent structural properties.
We demonstrate theeffectiveness of this approach on movie dialogues,where it induces symmetric T/V networks and theirlinguistic signatures without supervision.
Futurework should evaluate the capability of this approachto induce asymmetric signed networks, the utilityof partial or distant supervision, and applications tonon-fictional dialogues.AcknowledgmentsWe thank the reviewers for their detailed feedback.The paper benefitted from conversations with Cris-tian Danescu-Niculescu-Mizil, Chris Dyer, JohanUgander, and Bob West.
This research was sup-ported by an award from the Air Force Office of Sci-entific Research, and by Google, through a FocusedResearch Award for Computational Journalism.1624ReferencesLada A Adamic and Eytan Adar.
2003.
Friends andneighbors on the web.
Social networks, 25(3):211?230.Apoorv Agarwal, Anup Kotalwar, and Owen Rambow.2013.
Automatic extraction of social networks fromliterary text: A case study on alice in wonderland.
Inthe Proceedings of the 6th International Joint Confer-ence on Natural Language Processing (IJCNLP 2013).Pranav Anand, Marilyn Walker, Rob Abbott, Jean E.Fox Tree, Robeson Bowmani, and Michael Minor.2011.
Cats rule and dogs drool!
: Classifying stancein online debate.
In Proceedings of the 2nd Workshopon Computational Approaches to Subjectivity and Sen-timent Analysis (WASSA 2.011), pages 1?9, Portland,Oregon, June.
Association for Computational Linguis-tics.Philip Bramsen, Martha Escobar-Molano, Ami Patel, andRafael Alonso.
2011.
Extracting social power rela-tionships from natural language.
In Proceedings ofthe Association for Computational Linguistics (ACL),pages 773?782, Portland, OR.Roger Brown and Marguerite Ford.
1961.
Address inamerican english.
The Journal of Abnormal and So-cial Psychology, 62(2):375.Penelope Brown.
1987.
Politeness: Some universalsin language usage, volume 4.
Cambridge UniversityPress.Dorwin Cartwright and Frank Harary.
1956.
Structuralbalance: a generalization of heider?s theory.
Psycho-logical review, 63(5):277.Jonathan Chang, Sean Gerrish, Chong Wang, Jordan LBoyd-graber, and David M Blei.
2009.
Reading tealeaves: How humans interpret topic models.
In NeuralInformation Processing Systems (NIPS), pages 288?296.Cristian Danescu-Niculescu-Mizil and Lillian Lee.
2011.Chameleons in imagined conversations: A new ap-proach to understanding coordination of linguisticstyle in dialogs.
In Proceedings of the ACL Work-shop on Cognitive Modeling and Computational Lin-guistics.Cristian Danescu-Niculescu-Mizil, Lillian Lee, Bo Pang,and Jon Kleinberg.
2012.
Echoes of power: Lan-guage effects and power differences in social interac-tion.
In Proceedings of the Conference on World-WideWeb (WWW), pages 699?708, Lyon, France.Cristian Danescu-Niculescu-Mizil, Moritz Sudhof, DanJurafsky, Jure Leskovec, and Christopher Potts.
2013.A computational approach to politeness with applica-tion to social factors.
In Proceedings of the Associa-tion for Computational Linguistics (ACL), pages 250?259, Sophia, Bulgaria.Arthur P Dempster, Nan M Laird, and Donald B Rubin.1977.
Maximum likelihood from incomplete data viathe em algorithm.
Journal of the Royal Statistical So-ciety.
Series B (Methodological), pages 1?38.David K Elson, Nicholas Dames, and Kathleen R McK-eown.
2010.
Extracting social networks from literaryfiction.
In Proceedings of the Association for Compu-tational Linguistics (ACL), pages 138?147, Uppsala,Sweden.Manaal Faruqui and Sebastian Pad?o.
2011.
?I ThouThee, Thou Traitor?
: Predicting Formal vs. Infor-mal Address in English Literature.
In Proceedings ofthe Association for Computational Linguistics (ACL),pages 467?472, Portland, OR.Manaal Faruqui and Sebastian Pad?o.
2012.
Towardsa model of formal and informal address in english.In Proceedings of the European Chapter of the Asso-ciation for Computational Linguistics (EACL), pages623?633.Michael U Gutmann and Aapo Hyv?arinen.
2012.Noise-contrastive estimation of unnormalized statisti-cal models, with applications to natural image statis-tics.
The Journal of Machine Learning Research,13(1):307?361.Kazi Saidul Hasan and Vincent Ng.
2013.
Extra-linguistic constraints on stance recognition in ideo-logical debates.
In Proceedings of the Associationfor Computational Linguistics (ACL), pages 816?821,Sophia, Bulgaria.Ahmed Hassan, Amjad Abu-Jbara, and Dragomir Radev.2012.
Extracting signed social networks from text.In Workshop Proceedings of TextGraphs-7 on Graph-based Methods for Natural Language Processing,pages 6?14.
Association for Computational Linguis-tics.Dell Hymes.
1972.
On communicative competence.
So-ciolinguistics, pages 269?293.Scott F Kiesling.
2004.
Dude.
American Speech,79(3):281?305.Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg.2010a.
Predicting positive and negative links in onlinesocial networks.
In Proceedings of the Conference onWorld-Wide Web (WWW), pages 641?650.Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg.2010b.
Signed networks in social media.
In Proceed-ings of Human Factors in Computing Systems (CHI),pages 1361?1370.David Liben-Nowell and Jon Kleinberg.
2007.
The link-prediction problem for social networks.
Journal of theAmerican society for information science and technol-ogy, 58(7):1019?1031.Dong C Liu and Jorge Nocedal.
1989.
On the limitedmemory BFGS method for large scale optimization.Mathematical programming, 45(1-3):503?528.1625Christopher D. Manning, Mihai Surdeanu, John Bauer,Jenny Finkel, Steven J. Bethard, and David McClosky.2014.
The Stanford CoreNLP natural language pro-cessing toolkit.
In Proceedings of 52nd Annual Meet-ing of the Association for Computational Linguistics:System Demonstrations, pages 55?60.Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beat-rice Santorini.
1993.
Building a large annotated cor-pus of English: The Penn Treebank.
ComputationalLinguistics, 19(2):313?330.Franco Moretti.
2005.
Graphs, maps, trees: abstractmodels for a literary history.
Verso.Vinodkumar Prabhakaran, Owen Rambow, and MonaDiab.
2012.
Predicting overt display of power in writ-ten dialogs.
In Proceedings of the North AmericanChapter of the Association for Computational Linguis-tics (NAACL), pages 518?522.Lev Ratinov and Dan Roth.
2009.
Design challengesand misconceptions in named entity recognition.
InProceedings of the Thirteenth Conference on Compu-tational Natural Language Learning, pages 147?155.Association for Computational Linguistics.Martin J Wainwright and Michael I Jordan.
2008.
Graph-ical models, exponential families, and variational in-ference.
Foundations and TrendsR?
in Machine Learn-ing, 1(1-2):1?305.Hanna M Wallach, Iain Murray, Ruslan Salakhutdi-nov, and David Mimno.
2009.
Evaluation meth-ods for topic models.
In Proceedings of the Inter-national Conference on Machine Learning (ICML),pages 1105?1112.Robert West, Hristo Paskov, Jure Leskovec, and Christo-pher Potts.
2014.
Exploiting social network struc-ture for person-to-person sentiment analysis.
Transac-tions of the Association for Computational Linguistics,2:297?310.1626
