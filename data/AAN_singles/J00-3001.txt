Extracting the Lowest-Frequency Words:Pitfalls and PossibilitiesMarc  Weeber*University of GroningenR.
Hara ld  Baayen*Max Planck Institute forPsycholinguisticsRe in  Vos tUniversity of Groningen, University ofMaastrichtIn a medical information extraction system, we use common word association techniques toextract side-effect-related t rms.
Many of these terms have a frequency of less than five.
Standardword-association-based applications disregard the lowest-frequency words, and hence disregarduseful information.
We therefore devised an extraction system for the full word frequency range.This system computes the significance of association by the log-likelihood ratio and Fisher's exacttest.
The output of the system shows a recurrent, corpus-independent pa tern in both recall andthe number of significant words.
We will explain these patterns by the statistical behavior of thelowest-frequency words.
We used Dutch verb-particle combinations a a second and independentcollocation extraction application to illustrate the generality of the observed phenomena.
We willconclude that a) word-association-based extraction systems can be enhanced by also consideringthe lowest-frequency words, b) significance levels should not be fixed but adjusted for the optimalwindow size, c) hapax legomena, words occurring only once, should be disregarded a priori inthe statistical analysis, and d) the distribution of the targets to extract should be considered incombination with the extraction method.1.
IntroductionThe research reported here arose from an attempt o determine the conditions underwhich optimal recall and precision are obtained for the extraction of terms related toside effects of drugs in medical abstracts.
We used the standard technique of defining awindow around a seed term, side-effect in our case, and selected as potentially relevantterms those words that appeared more often in these windows than expected underchance conditions.Our original question concerned the extent o which recall and precision are in-fluenced by the size of the window.
It turns out, however, that a preliminary questionneeds to be answered first, namely, how to gauge the significance of the large effect ofthe lowest-frequency words on recall, precision, and the number of words extractedas potentially relevant erms.
* Groningen University Institute for Drug Exploration, Department ofSocial Pharmacy andPharmacoepidemiology, Ant.
Deusinglaan 1,9713 AV Groningen, The Netherlands.
E-mail:marc@farm.rug.nlt Faculty of Health Sciences, Department ofHealth Ethics and Philosophy, P.O.
Box 616, 6200 MDMaastricht, The Netherlands.
E-mail: rein.vos@zw.unimaas.nl:~ Max Planck Institute for Psycholinguistics, P.O.
Box 310, 6500 AH Nijmegen.
E-mail: baayen@mpi.nl(~) 2000 Association for Computational LinguisticsComputational Linguistics Volume 26, Number 3o.iZ150 -100500 I \ ] l r l l , , , .
, ,  .
.
.
.
.
.
.5 10 15 200.15Z 0.10Z 0.055 10 15 20Frequency Class Frequency Class(a) (b)Figure 1Frequency distribution of medical expert word types.
Panel (a) shows the number ofside-effect-related word types as judged by a medical expert (Nexpert) as a function of thefirst 23 frequency classes.
Panel (b) shows the proportion of expert ypes/total corpus types(Ntotal) for the first 23 frequency classes.
The horizontal dashed line indicates the meanproportion of 0.0619.It is common practice in information retrieval to discard the lowest-frequencywords a priori as nonsignificant (Rijsbergen 1979).
In Smadja's collocation algorithmXtract, the lowest-frequency words are effectively discarded as well (Smadja 1993).Church and Hanks (1990) use mutual information to identify collocations, a methodthey claim is reasonably effective for words with a frequency of not less than five.A frequency threshold of five seems quite low.
Unfortunately, even this lowerfrequency threshold of five is too high for the extraction of side-effect-related rmsfrom our medical abstracts.
To see this, consider the left panel of Figure 1, whichplots the number of side-effect-related words in our corpus of abstracts as judgedby a medical expert, as a function of word-frequency lass.
The side-effect-relatedwords with a frequency of less than five account for 295 of a total of 432 expertwords (68.3%).
The right panel of Figure 1 shows that the first 23 word-frequencyclasses are characterized by, on average, the same proportion of side-effect-relatedwords.
The a priori assumption of Rijsbergen (1979) that the lowest-frequency wordsare nonsignificant is not warranted for our data, and, we suspect not for many otherdata sets as well.The recent literature has seen some discussion of the appropriate statistical meth-ods for analyzing the contingency tables that contain the counts of how a word isdistributed inside and outside the windows around a seed term.
Dunning (1993) hascalled attention to the log-likelihood ratio, G 2, as appropriate for the analysis of suchcontingency tables, especially when such contingency tables concern very low fre-quency words.
Pedersen (1996) and Pedersen, Kayaalp, and Bruce (1996) follow upDunning's uggestion that Fisher's exact est might be even more appropriate for suchcontingency tables.We have therefore investigated for the full range of word frequencies whetherthere is an optimal window size with respect o recall and the number of significantwords extracted using both the log-likelihood ratio and Fisher's exact test.
In Sec-tion 2, we will show that indeed there seems to be an optimal window size for bothstatistical tests.
However, a recurrent pattern of local optima calls this conclusion intoquestion.
Upon closer inspection, this recurrent pattern appears at fixed ratios of thenumber of words inside the window to the number of words outside the window(complement).302Weeber, Vos, and Baayen Extracting the Lowest-Frequency WordsIn Section 3, we will relate the recurrent patterns of local optima at fixed window-complement ratios (henceforth W/C-ratios) to the distributions ofthe lowest-frequencywords over window and complement.
We will call attention to the critical effect of thechoice of W/C-ratios on the significance of the lowest-frequency words.As the improvement in the extraction of side-effect terms from medical abstracts,as gauged by the F-measure, which combines recall and precision (Rijsbergen 1979),is small, we also applied the same approach to the extraction of Dutch verb-particlecombinations from a newspaper corpus.
In Section 4, we report substantially betterresults for this more lexical extraction task, which is subject o the same statisticalbehavior of the lowest-frequency words.In the last section, we will discuss the consequences of our findings for the op-timization of word-based extraction systems and collocation research with respect othe lowest-frequency words.2.
An Optimal Window Size for Medical Abstracts?The MEDLINE bibliographic database contains a large number of abstracts of scien-tific journal papers discussing medical and drug-related research.
Typically, abstractsdiscussing medical drugs mention the side effects of these drugs briefly.
Informationon side effects is potentially relevant for finding new applications for existing drugs(Rikken and Vos 1995).
We are therefore interested in any terms related to the sideeffects of drugs.Before proceeding, it may be useful to clarify the way in which the present re-search differs from standard research on collocations.
In the latter kind of research,there is no a priori knowledge of which combinations of words are true collocations.Moreover, the most salient collocations generally are found at the top of a list rankedaccording to measures for surprise or association, such as G 2 or mutual information(Manning and Sch~itze 1999).
The large numbers of word combinations with signifi-cant but low values for these measures are often of less interest.
Low-frequency wordsare predominant among these kinds of collocations.
In our research, we likewise findmany low-frequency terms for side effects with low ranks in medical abstracts.
Therelatively well-known side effects that are mentioned frequently can be captured byexamining the top ranks in the lists of extracted words.
At the same time, the rarelymentioned side-effect terms are no less important, and in post marketing surveillancethe extraction of such side-effect terms may be crucial for the acceptance or rejectionof new medicines.Is reliable automatic extraction of both low- and high-frequency side-effect termsfrom MEDLINE abstracts feasible?
To answer this question, we explored the efficacyof a standard collocation-based term extraction method that extracts those words thatappear more frequently in the immediate neighborhood of a given seed term thanmight be expected under chance conditions.We compiled two corpora on the side effects of the cardiovascular d ugs captopriland enalapril from MEDLINE abstracts.
The first corpus contains all abstracts mention-ing captopril and the word side.
The second corpus contains all abstracts mentioningcaptopril and at least one of the compounds side-effect, side effect, side-effects, and sideeffects.
Thus, the second corpus is a subset of the first.
The first corpus is comprisedof 118,675 tokens and 7,678 types; the second corpus 103,603 tokens and 6,582 types.A medical expert marked 432 of the latter word types as side-effect-related rms.
Theleft panel of Figure 1 summarizes the head of the frequency distribution of these termsin the larger corpus.
Note that most side-effect-related rms have a frequency lower303Computational Linguistics Volume 26, Number 3Table 1General 2x2 contingency table.
A = frequency of the target in the windowcorpus, B = frequency of the target in the complement corpus, W = totalnumber of words in the window, C = total number of words in thecomplement.
Corpus size N = W + C.window complementfrequency of target A Bsum frequency of other words W - A C - BW CA+BW+C-A-BW+Cthan five.
What we need, then, is an extraction method that is sensitive enough toselect such very low frequency terms.In the collocation-based method studied here, the neighborhood of a given seedterm is defined in terms of a window around the seed term.
We constructed windowsaround all seed terms in the corpus, leading to a window corpus and a complementcorpus.
The window corpus contains all words that appear within a given windowsize of the seed term.
For instance, with a window size of 10, any word appearingfrom five words before the seed to five words after the seed as well as the seed itself isincluded in the window corpus.
The word tokens not in the window corpus comprisethe complement corpus.
Any type in the window corpus is a potential side-effect-related term.
For any such target type, we tabulate its distribution in window andcomplement corpora in a contingency table like Table 1.Given W and C, we need to know whether the frequency of the target in thewindow corpus, A, is high enough to warrant extraction.
Typically, given the marginalB and distribution of the contingency table, a target is extracted for which wA--~A > ~-2-~,for which the tabulated istribution is nonhomogeneous according to tests such as G 2and Fisher's exact test for a given cMevel.In this approach, the window size is a crucial variable.
At small window sizes,many potentially relevant erms fail to appear in the window corpus.
However, atlarge window sizes, many irrelevant words are found in the window corpus and maybe extracted spuriously.To see to what extent window size may affect the results of the extraction proce-dure, consider the solid lines in panels (a) and (b) of Figure 2.
The left panel shows theresults for recall when we use the log-likelihood ratio, G 2, the right panel the resultsfor Fisher's exact test.
We define recall as the proport ion of the number of side-effectwords extracted and the total number of side-effect words available in the window.For both statistical tests, recall seems to be optimal at window size 2.
However,at this window size, the number of words extracted is very small.
This can be seen inpanels (c) and (d).
Considered jointly, panels (a) and (c) suggest an optimal windowsize of 24 for our larger corpus (corpus 1), as recall is still high, and the number ofsignificant words is maximal.
When Fisher's exact est is used instead of G 2, panels (b)and (d) suggest 42 as the optimal size.The dashed lines in panels (a) to (d) show the corresponding results for our smallercorpus (corpus 2).
Unsurprisingly, the general pattern for this subcorpus is quite sim-ilar, although the drops in recall and the number of significant words, Nsig, occur atsomewhat smaller window sizes.Interestingly, we can synchronize the curves for both corpora by plotting recall andthe number of significant items, Nsig, against the window-complement ratio (W/C).This is shown in panels (e) and (f).
These panels suggest not an optimal window size304Weeber, Vos, and Baayen Extracting the Lowest-Frequency Words04 f 0.3 i~ 0.2 ~ i0 20 40 6(J 80 100 12024 86Window Size(a)6?
?r A'~ 4?
?t / ',il _ .
~ ~  ~ \[.
, /  i .
.
.
.
.
.0 20 40 60 80 100 12024 86Window Size(c)4oo\[ ,200I,03 i }0.2o.
h0 20 40 60 80 100 1206 42 82Window Size(b)3oo r ,~o0 20 40 60 80 100 1206 42 82Window Size(d)300 IiilI  ..... "10.0 0.2 0.4 0.6 0.8 0.0 0.2 0.4 0.6 0.80.17 0.62 0.05 0.29 0.58w/c w/c(e) (t)Figure 2Results of the word extraction procedure (a = 0.05).
Solid line = corpus 1, dashed line =corpus 2.
Panel (a) shows the log-likelihood, G2, recall results as a function of the windowsize.
Panel (b) shows recall values for Fisher's exact est.
Panel (c) shows the total number ofsignificant words (Nsig) as a function of the window size for G 2.
Panel (d) shows the same as(c) but for Fisher's exact test.
Panel (e), G 2, and (f), Fisher's exact est, also show the totalnumber of significant words, but as a function of the W/C-ratio; the ratio of the number ofwords in the window corpus to the number of words in the complement corpus.but an optimal W/C-ratio (0.17 for G 2 and 0.29 for Fisher's exact test).
Although wenow seem to have shown that recall and Nsig depend on the choice of w indow size,the sudden drops in recall and Nsig and the reoccurrence of such drops at variousW/C-ratios is a source of worry, not only for G 2 results, but also for the results basedon Fisher's exact test.
A further source of worry is the fact that the two tests divergeconsiderably with respect o the optimal W/C-ratio.3.
Contingency Tables and the Lowest-Frequency WordsBefore we can have any confidence in the optimality of a given W/C-ratio, we shouldunderstand why the saw-tooth-shaped patterns of Nsig arise.
Both the log-likelihoodratio (G 2) and Fisher's exact test compute the significance of contingency tables similarto Table 1.
So why is it that the left panels in Figure 2 differ from the right panels?G 2 has a 2-distribution as N --* cx~.
This convergence is not guaranteed for lowexpected frequencies and sparse tables, which renders use of G 2 problematic for ourlowest-frequency words in that it may  suggest words to be more remarkable than they305Computational Linguistics Volume 26, Number 3Table 2Contingency tables for hapax legomena, dis legomena, nd tris legomena.W = number of words in window corpus; C = number of words incomplement corpus.
Total corpus size: N = W + C.(a): 1 0 (b): 2 0 (c): 1 1W-1 C W-2  C W-1  C-1(d): 3 0 (e): 2 1 (f): 1 2W-3  C W-2  C-1  W-1  C-2really are.
Fisher's exact test, on the other hand, does not use an approximation to aprobability distribution but computes the exact hypergeometric distribution given themarginal totals of the contingency table.
While Fisher's exact test is suitable for theanalysis of sparse tables, it is inherently conservative because it regards the marginaltotals not as stochastic variables but as fixed boundary conditions.
Consequently, thistest is likely to reject words that are in fact remarkably distributed in the contingencytable.
The difference in behavior of the two tests is clearly visible in panels (c) and (d)of Figure 2: the number of significant words (Nsig) according to G 2 is roughly twiceas large as that according to Fisher's exact test.When a hapax legomenon 1, a word with frequency 1, occurs in the window corpus,we use contingency table (a) as shown in Table 2.
For dis legomena, words with afrequency of 2, that appear at least once in the window corpus, we obtain the twocontingency tables (b) and (c).
The interesting contingency tables for tris legomena retables (d) to (f).
These six tables are relevant for 63.8% of the side-effect-related termsas judged by our medical expert.How do changes in the W/C-ratio affect G 2 and Fisher's exact test, when appliedto contingency tables (a) to (f)?
In other words, how does the choice of the windowsize affect whether a low-frequency word is judged to be a significant erm, for fixedA and B (e.g., A = 1 and B = 0 for a hapax legomenon)?First, consider contingency tables with B = 0, for instance tables (a), (b), and (d).For small A, (A ~ W, C), it is easily seen (see the appendix) that the critical W/C-ratiobased on the log-likelihood ratio is:W 1C ~/eX/2 - 1'(1)with X the X 2 value corresponding to a given s-level with 1 degree of freedom.
ForA = 1 and c~ -- 0.05, X = 3.84, the critical W/C-ratio equals 0.1718.
This is exactlythe W/C-ratio in panel (e) in Figure 2 at which the first and largest drop in the num-ber of significant words occurs.
Up to this ratio, any hapax legomenon appearing inthe window corpus is judged to be a significant erm.
For W/C > 0.1718, no hapaxlegomenon will be extracted.Fisher's exact test is far more conservative.
For this test, the critical W/C-ratio is1 The term hapax legomenon (literally 'read once') goes back to classical studies and was originally usedto refer to the words used once only in the works of a given author, e.g., Homer.
By analogy, dislegomenon and tris legomenon have come into use to refer to words occurring only twice or threetimes.306Weeber, Vos, and Baayen Extracting the Lowest-Frequency WordsTable 3Critical W/C-ratios where sparse and skewed contingency tables losesignificance.
Equations 1 and 2 provide the ratios for the B = 0 cases.
Theother ratios are obtained by simulations.distribution G 2 FisherA-B cz = 0.05 c~ = 0.01 c~ -- 0.05 c~ -- 0.011 - 0 0.1718 0.0375 0.0526 0.01011 - 1 0.0400 0.0092 0.0260 0.00502 - 0 0.6204 0.2348 0.2880 0.11111 - 2 0.0232 0.0053 0.0172 0.00332 - 1 0.1917 0.0824 0.1565 0.06263 - 0 1.1155 0.4938 0.5833 0.2746hapax legomenadis legomenatris legomena(see the appendix for details):wC 1-  ?/-P' (2)where P is the s-level.
For A -- 1 and P = 0.05, the critical W/C-ratio for a hapaxlegomenon equals 0.0526.
In panel (f) of Figure 2, we observe the first drop in thenumber of significant words at precisely this W/C-ratio.
For very small W/C-ratios,any hapax legomenon in the window corpus is also judged to be significant accordingto Fisher's exact test.
Compared to G 2, Fisher's exact test rejects hapax legomena ssignificant at much smaller W/C-ratios.
Note that when W/C -- 0.05/0.95 = 0.0526,i.e., when the window corpus is exactly 1/20 of the total corpus, the probability that ahapax legomenon appears in the window corpus equals 0.05.
Our conclusion is that,with the W/C-ratio as the only determinant of significance, the windowing method isnot powerful enough to distinguish between relevant and irrelevant hapax legomena.In other words, hapax legomena should be removed from consideration a priori.For dis legomena that appear exclusively in the window corpus, the critical ratiosare 0.6204 for G 2, corresponding to the second major drop in panel (e) of Figure 2,and 0.2880 for Fisher's exact test, corresponding to the severe drop following themaximum of Nsig in panel (f).
The third major drop in this panel corresponds to thecritical W/C-ratio for tris legomena occurring three times in the window corpus.For contingency tables with B > 0; A > B; A, B <~ W, C, critical W/C-ratios are noteasy to capture analytically.
We therefore carried out a simulation study for W + C =100,000.
For fixed A and B and a given s-level, we calculated the critical W/C-ratioby iterative approximation.
Results are summarized in Table 3.When we highlight these critical ratios in Figure 2 by means of vertical dashedlines, we obtain Figure 3.
Panels (a) to (d) correspond to the curves for corpus 2 in thefirst four panels of Figure 2.
For the log-likelihood ratio, we observe that both the majorand minor drops in recall and the number of significant words (Nsig) occur at the W/C-ratios where different distributions of the lowest-frequency words lose significance.
ForFisher's exact test, we observe exactly the same pattern.
Panels (e) and (f) show thenumber of significant words for a pseudorandomized version of corpus 2 where weused the same tokens but randomized the order of their appearance.
Although thenumber of significant words is lower, the saw-tooth-shaped pattern with the suddendrops at fixed ratios reemerges.We conclude that W and C are the prime determinants of both recall and thenumber of significant words.
At first sight, Fisher's test is clearly preferable to the307Computational Linguistics Volume 26, Number 31-1 1-0 2-1 3-1 2-0 0.41- ~ ~= Fi i~03 ~ ii i~ 0.2 ,0.0 0.2 0.4 0.6 0.8W/C(a)1-1 1-0 2-1 3-1 2-0600 I42?o?o10.0 0.2 0.4 0.6 0.8W/C(c)1-1 1-0 2-1 3-1 2-0400 f300\[Z 200\[100\[1-0 2-1 2-0~ 0.3\[0.20.13-1 3-00.0 0.2 0.4 0.6 0.8W/C(b)1-0 2-1 2:0 3-1 3-00.0 0.2 0.4 0.6 0.8W/C(d)1-0150\[Z "~ 100\[50\[,0.02-1 2-0 3-1 3-00.0 0.2 0.4 0.6 0.8 0.2 0.4 0.6 0.8W/C W/C(e) (0F igure  3Results of word extraction procedure (a = 0.05) with A-B distributions.
Panels (a),log-likelihood ratio, G 2, and (b), Fisher's exact est, show the recall results of the extractionprocedure for corpus 2.
Panels (c) and (d) show the total number of significant words (Nsig),again for G 2 and Fisher's exact est, respectively (see also Figure 2).
Panels (e) and (f) show theresults for a randomized corpus for G 2 and Fisher's exact est.
The numbers above the panelsindicate the A-B distribution of the contingency tables in Table 2.log-likelihood ratio because the extreme saw-tooth-shaped pattern is substantially re-duced.
However, the use of Fisher's exact est does not eliminate the effect of the choiceof window and complement size on the number of significant words and recall.
Atspecific W/C-ratios, nonnegligible numbers of words with the lowest frequency of oc-currence suddenly lose significance.
Moreover, in our discussion thus far, we have nottaken extraction precision into account nor the trade-off between precision and recall.For the assessment of overall extraction results, we turn to the F-measure (Rijsbergen1979), a measure that assigns equal weights to precision (P) and recall (R):2PRF= P+R"  (3)Figure 4 plots precision, recall, and F as a function of the W/C-ratio.
The commontrade-off between recall and precision is clearly present for the smaller window sizes,with the F-measure providing a kind of average.Thus far, we have applied a common collocation extraction technique to a semanticassociation task.
Actual extraction performance is low: F is maximally 0.17.
To gauge308Weeber, Vos, and Baayen Extracting the Lowest-Frequency Words0.4 I .
.
.
.
.
.
.
.
(}.3ko.2 ~'i'{7. , i : ,  " " /  .
.
.
.
~- "  q , _  1 i(}.
(7 0.2 {}.4 (}.6 - 0.8 I .
{}{7.17 {7,62u2(}.30.2{7.1{}.
(7 0.2 0.4 0.60.
{}5 0 .29 0 .58i t,i i kO.8 I.Ow/c  w/cFigure 4F, recall, and precision as a function of the W/C-ratio.
Recall (R, dashed line), F (solid line),and precision (P, dotted line) using G 2 (left panel) and Fisher's exact est (right panel) for oursecond corpus plotted as a function of the W/C-ratio.whether better results can be obtained with the present echniques, we examined theextraction of Dutch verb-particle combinations.4.
Extracting Verb-Particle CombinationsIn English, the particle of verb-particle combinations always follows the verb, as inshe rang him up.
In Dutch, the particle can occur either before or after the verb.
Whenit occurs before the verb, it is separated from the verb by te ('to') and/or  one or moreauxiliary verbs.
Extracting such particle-verb combinations i relatively straightfor-ward.
However, when the particle follows the verb, it may be separated from the verbby many constituents of arbitrary complexity: Hij zegt de belangrijke afspraak met de pro-grammeur voor vanmiddag af ('he says the important meeting with the programmer forthis afternoon off'; i.e., he cancels the meeting).
How well does our present approachlend itself to the extraction of verb-particle combinations with the particle af ('off')when the particle follows the verb?We investigated this question by studying verb-particle combinations with af froma Dutch newspaper corpus of about 4.5 million word tokens.
We extracted by hand allsentences from the corpus that contain af (3,802 sentences, 97,903 tokens) and singledout those sentences in which af belongs to a verb-particle combination in which theverb occurs to the left of the particle (2,202 sentences with 42,825 tokens).
The targetsto extract from the 2,202 sentences are 436 different verb inflections, of which 276 havea frequency of less than five.
Just as the judgments of a medical expert were used inthe preceding extraction task to provide a frame of reference for the evaluation ofprecision and recall, the present lexical extraction task has as its frame of reference the2,202 sentences that we judged to contain a verb followed at some point to the rightby a particle.
How many of the 436 different verb inflections can we extract with ourwindowing technique, and what is the trade-off between recall and precision?To answer this question, we defined windows to the left of the seed term af in therange of positions \[-12, -1\].
We calculated the W/C-ratio for each window size.
Foreach word in all windows, we calculated its significance according to G 2 and Fisher'sexact test.
Using the 436 target verb inflections as a frame of reference, we computedprecision, recall, and F. Panel (a) of Figure 5 plots F as a function of the W/C-ratio.F reaches a maximum F of 0.31 at W/C = 0.59 for G 2 (the solid line in the figure)and a maximum of 0.27 at W/C = 0.50 for Fisher's exact test (the dashed line).
These309Computational Linguistics Volume 26, Number 303\[ / -{}2 / / /O.
_~}2 -{}14 {}.6 {}.8 I.
{}Z400200/ / J  J J\0.2 0.4 0.6 {}T8 I.{}W/C(a}W/{."(b){}.30.2{}.
1f .~ , .
J/7/__  L t .
.
.
.
J .0,2 0.4 0.64{10~0Z2001}.8 1,11L .
.
.
.
.
?
_ _ _  ?
L _ ?
{}.2 0.4 0,6 0.8 I .
{}W/C  W/C(c) (d)Figure 5Extraction results for the af corpus.
Panel (a) shows F for G 2 (solid line) and Fisher's exact est(dashed line) as a function of the W/C-ratio.
Panel (b) displays the number of significantwords (Nsig) according to both tests.
Panel (c) shows F for G 2 at c~ = 0.05 (solid line) andFisher's exact est at c~ = 0.1 (dotted line).
Panel (d) shows Nsig for G 2 at c~ = 0.05 and forFisher's exact est at c~ -- 0.1.results compare favorably with the maximum F of 0.17 obtained for the extraction ofside-effect terms from medical abstracts.Panel (b) of Figure 5 shows the by-this-time familiar saw-tooth-shaped pattern ofthe number of significant word types as function of the W/C-ratio.
We observe againthat Fisher's exact test is more conservative, and in the extraction task, less successful,than G 2.
However, by opting for a more liberal c~-level we can compensate for theconservatism of Fisher's exact test and obtain an F profile that is indistinguishablefrom that of G 2 as shown in panel (c) for ~ -- 0.1.
Panel (d) returns to the number ofsignificant erms (Nsig) when Fisher's exact test is used with c~ = 0.1.
Note that theoptimal W/C-ratio according to F for G 2 (0.59) still leads to a higher Nsig than theoptimal W/C-ratio (0.83) for Fisher's exact test with c~ -- 0.1.
However, in the case ofFisher's exact test, the precision is much higher than when G 2 is used.
These resultssuggest hat the choice of G 2 or Fisher's exact test should be guided by the desiredtrade-off between precision and recall.5.
D iscuss ionThe question that originally motivated the present research concerned the determina-tion of the optimal window size for the extraction of side-effect-related words.
Most310Weeber, Vos, and Baayen Extracting the Lowest-Frequency Wordswords that are judged by a medical expert o be related to side effects have frequenciesof use that are so low that they fall below the frequency thresholds generally usedin standard information extraction techniques.
Is it nevertheless possible to single outsuch low-frequency terms through optimal window size estimation, especially sincethe log-likelihood ratio and Fisher's exact est have recently been advanced as suitabletechniques even for the analysis of the lowest-frequency ranges?Manipulation of the window size revealed a saw-tooth-shaped pattern in the num-ber of significant words (Nsig) that depends not on the window size itself but onthe W/C-ratio.
This saw-tooth-shaped pattern arises most prominently when the log-likelihood ratio is used to extract significant words, but it is also clearly visible whenFisher's exact test is used.
This pattern is due to the way in which these tests eval-uate surprise as a function of the window size for the lowest-frequency words.
Weargue that hapax legomena should be disregarded a priori, while for low-frequencywords with frequency greater than 1, only the most extreme distributions over win-dow and complement are reliable in that we are confident that these terms are reallyrelated to the seed.
For dis and tris legomena, for instance, all occurrences should ineffect be concentrated in the window.
Only then are we confident that there is truly arelationship between the seed and the target.With these restrictions, the optimum W/C-ratio for our side-effect data is justsmaller than 0.2880, using Fisher's exact test, which amounts to an optimal win-dow size of 36.
Of the 295 terms with a frequency of 4 or less that a medical expertjudged to be side-effect-related rms, we capture 14, which amounts to 4.8%.
Whenwe exclude the hapax legomena s impossible to extract reliably a priori, we capture14/122 = 11.5%.
Although the gain in number of significant low-frequency items issmall, the success for the low-frequency items is still reasonable when compared to thecorresponding success rate of 26/137 = 19.0% for the items with a frequency of 5 ormore.
These results uggest that the windowing technique is far from optimal for theextraction of side-effect terms from medical abstracts, irrespective of the frequenciesof these terms.The windowing technique applied to the extraction of Dutch verb-particle com-binations led to more encouraging results.
Choosing 0.4625 as the optimal W/C-ratiofor the af data, which amounts to accepting dis legomena with a 2-0 distribution, andusing a = 0.1 with Fisher's exact est, we obtain an optimal window size of 5.
Withthis window, we extract 42 of the 139 lowest-frequency words in the 2 to 4 range, i.e.,30.2%.
This compares favorably to the success rate of 60/170 = 35.2% for verbs witha frequency greater than 4.
When we use G 2 instead of Fisher's exact test to obtainimproved recall at the cost of lesser precision, we extract 58/139 = 41.7% of the lowest-frequency words in the 2 to 4 range and 64/170 = 37.6% of the higher-frequency words(optimum W/C-ratio 0.6204, corresponding window size of 7).
For this more lexicalextraction task, extraction success rates are comparable for the lower-frequency andthe higher-frequency words.
Neglecting the extraction of the lower-frequency wordsa priori would have led to the loss of nearly half of the words currently extracted.The difference in the results between the two extraction tasks, side effects in medi-cal abstracts and verb-af combinations in a newspaper corpus, is due to the differencein the distributions of the targets around the seed terms.
Concentrating on the lowest-frequency word tokens, the left panel of Figure 6 shows their distribution for theside-effect corpus.
The right panel shows the corresponding distribution for the afcorpus.
The side-effect terms reveal a wide scatter around the seed at position 0.
Bycontrast, verbs predominantly cluster close to the left of af.
Apparently, the distancebetween the verb and the particle is more constrained than the distance between side-effect terms and the seed term.
The optimal window size of 7 (position -7) for G 2311Computational Linguistics Volume 26, Number 310i k-300 -200 -100 0 100 2004020-40 -20Position Position(a) (b), ll,,,,,it,, .... ,,20Figure 6Frequency distribution of words occurring two to four times.
Panel (a) shows for the sideeffect corpus how the expert words with a frequency of 2, 3, and 4 are distributed around theseed term.
Panel (b) shows this distribution for the af corpus.obtained above ties in with the distribution of the lowest-frequency words: 68% of alllowest-frequency tokens are in this window.
For the side-effect corpus, only 31% ofall low-frequency tokens are in the optimal window of 36 for Fisher's exact test.
Thissuggests that the optimal window size must be ascertained on the basis of the distri-bution of targets around the seed, on the one hand, and by optimizing the statistics,on the other hand.As an illustration of how the statistics can be optimized, we return to the af data.When we look at the distribution of the lowest-frequency words in Figure 6, an optimalwindow size of 8 to the left suggests itself.
This translates into a W/C-ratio of 0.6689.Given that we want to retain dis legomena with a 2-0 distribution, we proceed tocompute the corresponding significance levels for both G 2 and Fisher's exact test byEquations 1 and 2.
The critical X 2 value for G 2 equals 3.65, the critical P for Fisher'sexact test is 0.161.
The extraction results for both tests as measured by F are 0.31 and0.33, respectively.
This procedure allows us to extract 64/139 = 46.0% of the low-frequency words and 66/170 -~ 38.8% of the high-frequency words using G 2, and64/139 = 46.0% and 79/170 = 46.7%, respectively, using Fisher's exact test.
Note thatthis technique is optimal for the extraction of the lowest-frequency words, leading toidentical performance for G 2 and Fisher's exact test for these words.
For the higher-frequency words, Fisher's exact test leads to a slightly better recall with the sameprecision scores (0.31 for both tests).While we have observed reasonable results with both G 2 and Fisher's exact est, wehave not yet discussed how these results compare to the results that can be obtainedwith a technique commonly used in corpus linguistics based on the mutual information(MI) measure (Church and Hanks 1990):I(x,y) --- log 2 P(x,y) (4) P(x)P(y)In (4), y is the seed term and x a potential target word.
A high MI score for a giventarget word suggests an association between this target and the seed term.
Or perhapsmore precisely, a low MI score suggests a dissociation between target and seed word(Manning and Sch/itze 1999).
To compute recall, precision, and F, we require a cut-offvalue.
As there is no theoretically motivated cut-off value, we vary it systematically.Panel (a) of Figure 7 plots the results for the af corpus.
The x-axis represents the MI312Weeber, Vos, and Baayen Extracting the Lowest-Frequency Wordsf -~kg o(a)/ - -  < ~0.I 0 .2~-"7~__7  _ .q,-" ",~', ...
., 04.
Slgl~l/l~ nc (Z3 e/eLe ~9- ~?~\C(b)Figure 7Extraction results (F) for the af corpus for mutual information and Fisher's exact est.
Panel (a)shows the F score as a function of both W/C-ratio and mutual information cut-off value.Panel (b) shows F as a function of W/C-ratio and the significance l vel c~ used with Fisher'sexact est.cut-off value, the y-axis the W/C-ratio, and the z-axis the F value.
Note that F is ratherindifferent to variation in window size and MI cut-off value.
It varies between 0 (atthe right-hand edge) to 0.17, with most values around 0.15 (the plateau in the figure).Interestingly, the highest possible MI cut-off point equals 4.27: the right-hand edge ofthe plateau.
In fact, 4.27 is the maximum MI score for this corpus size (42,825) andthe frequency of the seed term af (2,206), irrespective of the frequency of the targetword, reached when all occurrences of the target word are concentrated in the window(see the appendix for details).
Consequently, any hapax legomenon appearing in thewindow will automatically be assigned the maximum value of MI, along with targetwords with the most extreme W/C distributions (Window-Complement: 2-0, 3-0, 4-0,etc.).
This has the unfortunate consequence that, with regard to their MI score, trulyremarkably distributed target words become indistinguishable from the statisticallyunremarkable hapax legomena.Panel (b) of Figure 7 displays the corresponding results when we use Fisher's exacttest rather than MI.
Instead of varying the MI cut-off value, we vary the significancelevel a.
Note that the resulting F scores tend to be roughly twice as high as thoseobtained with MI-based extraction.
As there are a number of very similar local maxima,the choice of window size and significance l vel should be based on the desired trade-off between precision and recall given the general distribution of the target wordsaround the seed term.
2We conclude that, at least for the present word extraction task,Fisher's exact est compares favorably to mutual information (as does G2).All the analyses presented thus far are conditional analyses, in the sense that wecompiled new corpora from the database of abstracts and from the newspaper corpuscontaining only relevant abstracts (containing the drug names captopril and enalaprilas well as the term side-effect) and relevant sentences (containing the particle af andits verb to its left), respectively.
The size of the complement was always determinedwith respect o these new conditional corpora, and not with respect o all MEDLINE2 Note that we manipulate the a-levels in the same way as the MI cut-off values.
In the presenttechnique, the a-level is a parameter that we vary to optimize extraction results for a training data set.Our use of a should be carefully distinguished from the function of preset a-levels when testing thesignificance of observed ifferences in experimentally obtained data.313Computational Linguistics Volume 26, Number 3Table 4General and specific 2 x 2 contingency tables for low-frequency words.Table (a) provides the general notation of the counts in a 2 x 2contingency table.
In table (b), A = frequency of rare words (1, 2, 3 .
.
.
.
),W = number of words in window, C = number of words in complement.Corpus size N = W + C.(a): 1/11 /'/12 /'/1+/'/21 /'/22 /'/2+//+1 //+2 //+q-(b): AW-AW0 AC W+C-AC W+Cabstracts or to the complete newspaper corpus.
This raises the question of whetherbetter results might have been obtained if the complete data sets had been used.
Inprinciple, more data might imply more power.
At the same time, more data also entailsthe risk of more noise.
At least for our af data, enlarging the complement leads to worseperformance.
When we allow any sentence that contains af in our analyses, F decreasesfrom 0.31 to 0.23 for G 2.
When we base the analyses on the complete newspaper corpus,F reduces further to 0.19.
The reason for this decrease in performance is probably due tothe W/C-ratio being very low for all practical window sizes, i.e., at the very left part ofthe saw-tooth-shaped pattern characterizing Nsig as a function of W/C.
Consequently,any low-frequency word is singled out as a significant item whenever it occurs at leastonce in the window.
Given the Zipfian structure of word-frequency distributions, agreat many spurious low-frequency words are extracted.As mentioned in the introduction, the received wisdom is that the windowingmethod is unreliable for events with a frequency of less than 5.
By means of ananalysis of the behavior of statistical tests for 2 x 2 contingency tables with sparsedata, a method for optimizing the use of these tests has been developed.
We hopethat this technique will prove to be useful for domains in which the extraction oflow-probability events is crucial.AppendixLog-Likelihood RatioFor the general contingency table, table (a) in Table 4, the log-likelihood ratio is definedby (Agresti 1990):G 2 = 2 ~_, ~ nijin(nij/mq),i jwhere rhq = ni+n+j/n++.
When we use the specific contingency table for hapax legom-ena, table (b) in Table 4, we obtain for a specific G 2 of X the formula:W+C (W-A) (W+C)  W+CX/2 = A ln~+(W-A) ln  W(W+C-A)  +CInw+C-A"= In (W-  A) w-A - InW w + In(W q- C) w+C - In(W q- C - A) w+C-A,= In (W-  A)W-A(w q- C) w+CwW(w q- C - A) w+C-A '(W - A)W-A(w q- C) w+C eX/2wW(w + C - A) w+c-A "314Weeber, Vos, and Baayen Extracting the Lowest-Frequency WordsWe rewrite the latter equation to:eX/2w w(W-A) w-AeX/2wW(w - -  A) A(W-A)  w(W + C) w+c(W + C - A) w+c-A'(w + c)W(w + c)c(w + c - A?
(W + C - A)W(W + C - A) cBecause W >> A and therefore W + C >> A, we rewrite the formula above as follows:eX/2 W w W A (w + c)W(w + c)C(w + c) Aw w (w + c)W(w + c)  ceX/2w A = (W ?
C) A,~ W  = W+ C.So that the ratio is:W 1C ~ - 1 'When N > 10,000, the error of this equation is smaller than 0.001.Fisher's Exact TestWith Fisher's exact est, the observed marginal totals are used to compute the hyper-geometric distribution, which is defined for the general 2 x 2 table, table (a) of Table 4,as (Agresti 1990):?/2+ ) (n '+) (n+ , rill - -  F / l ln+lThe probability of every possible table with given marginal totals adds to 1.
We useFisher's exact est that sums the hypergeometric probabilities of all tables with prob-abilities less than or equal to the observed table.
With B -- 0, table (b) in Table 4 is theonly table we are interested in so that the probability P for this contingency table is:PA C-AW-A  )(w + c - A)!
(w + c)~ 'W!C!W~(W + C - A)~(w - A)~(W + C)~"W(W- 1)... (W-A  + 1)(W-A)!
(W + C - A)!
(w-a)~ (w+ c ) (w+ c -  1 ) .
.
.
(w+ c -A  + 1)(w+ c -A) !315Computational Linguistics Volume 26, Number 3Because A = 1,2 ,3 , .
.
.
,  W >> A and therefore W + C >> A, we allow ourselves toformulate W!
= wA(w - A)!
and (W + C)!
= (W + c)A(w + C - A)!.
We thereforerewrite Fisher's exact test as follows:The W/C-ratio is then:wAw (w +P =(W + c )Aw!
(w + C)!
'WA(W + C)W -W+C"wC 1-  ~Y-P"When N > 20,000, the error of this equation is smaller than 0.001.Practical Issues Using Fisher's Exact Test.
We used a network algorithm to computeFisher's exact est (Mehta and Patel 1986; Clarkson, Fan, and Joe 1993).
This algorithmis computationally intensive, but since many words have the same table, only a fewtables have to be computed and their results can be cached.
It takes an average of 50seconds to compute one window size in a 100,000 word corpus on a Pentium 133MHz,48MB Linux machine.Source code for the algorithm can be found at: http://www, acm.
org/pubs/citations/j ournals/toms/1986-12-2/p154-meht a/Mutual InformationGiven the definition of Mutual Information (Church and Hanks 1990),I(x,y) = log 2 P(x,y)P(x)P(y)"we consider the distribution of a window word according to the contingency table (a)in Table 4.
P(x) is the relative frequency of the target word, P(y) is the relative frequencyof the seed term, and P(x,y) is the frequency of the target word in the window.
Interms of the contingency table, we have:/'/11I(x,y) = log 2 n++//1+ S 'f /++ 7"/++where S is the frequency of the seed.
Substituting nn = nl+ - n12, we find that/11+ - -  F/12I(x,y) = log 2 n++ nl+ S '/ /++ / /++1= log 2 n++//1+ S 'n++(nl+ -- nu) " n++316Weeber, Vos, and Baayen Extracting the Lowest-Frequency Words= log2(n++) - log2(S) - log2(nl+) + log2(nl+ - n12).For a given corpus and extraction task, corpus size (n++) and the frequency of theseed term S are fixed, so that we can writeI(x,y) = C - log2(nl+) + log2(nl+ - n12).As n12 K nl+, I(x,y) reaches its maximum value (C) when n12 = 0, i.e., when allinstances of the target word are in the window, irrespective of the frequency of thetarget.AcknowledgmentsWe are indebted to three anonymousreviewers whose criticisms have led tosubstantial improvements.
This study wasfinancially supported by the Dutch NationalResearch Council NWO (PIONIER grant tothe third author).ReferencesAgresti, Alan.
1990.
Categorical Data Analysis.John Wiley & Sons, New York.Church, Kenneth W. and Patrick Hanks.1990.
Word association norms, mutualinformation, and lexicography.Computational Linguistics, 16(1):22-29.Clarkson, Douglas B., Yuan-An Fan, andHarry Joe.
1993.
A remark on algorithm643: FEXACT: An algorithm forperforming Fisher's exact est in r x ccontingency tables.
ACM Transactions onMathematical Software, 19(4):484-488.Dunning, Ted.
1993.
Accurate methods forthe statistics of surprise and coincidence.Computational Linguistics, 19(1):61-74.Manning, Christopher D. and HinrichSchiitze, 1999.
Foundations of StatisticalNatural Language Processing, chapter 5,Collocations.
The MIT Press, Cambridge,MA.Mehta, Cyrus R. and Nitin R. Patel.
1986.Algorithm 643.
FEXACT: A fortransubroutine for Fisher's exact est onunordered r x c contingency tables.
ACMTransactions on Mathematical Software,12(2):154-161.Pedersen, Ted.
1996.
Fishing for exactness.In Proceedings ofthe South-Central SASUsers Group Conference, pages 188-200,Austin, TX.Pedersen, Ted, Mehmet Kayaalp, andRebecca Bruce.
1996.
Significant lexicalrelationships.
In Proceedings ofthe 13thNational Conference on Artificial Intelligence.AAAI Press/The MIT Press, Menlo Park,CA, pages 455-460.Rijsbergen, Cornelis J. van.
1979.
InformationRetrieval.
Second edition.
Butterworths,London.Rikken, Floor and Rein Vos.
1995.
Howadverse drug reactions can play a role ininnovative drug research.
Pharmacy Worldand Science, 17(6):195-200.Smadja, Frank.
1993.
Retrieving collocationsfrom text: Xtract.
Computational Linguistics,19(1):143-177.317
