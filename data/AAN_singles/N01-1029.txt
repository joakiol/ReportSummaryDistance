A Structured Language Model based on Context-Sensitive ProbabilisticLeft-Corner ParsingDong Hoon Van Uytsel?donghoon@esat.kuleuven.ac.beFilip Van Aelten?filip.van.aelten@lhs.beDirk Van Compernolle?compi@esat.kuleuven.ac.be?Katholieke Universiteit Leuven, ESAT, Belgium?Lernout & Hauspie, BelgiumAbstractRecent contributions to statistical language model-ing for speech recognition have shown that prob-abilistically parsing a partial word sequence aidsthe prediction of the next word, leading to ?struc-tured?
language models that have the potential tooutperform n-grams.
Existing approaches to struc-tured language modeling construct nodes in the par-tial parse tree after all of the underlying words havebeen predicted.
This paper presents a different ap-proach, based on probabilistic left-corner grammar(PLCG) parsing, that extends a partial parse bothfrom the bottom up and from the top down, lead-ing to a more focused and more accurate, thoughsomewhat less robust, search of the parse space.
Atthe core of our new structured language model is afast context-sensitive and lexicalized PLCG parsingalgorithm that uses dynamic programming.
Prelim-inary perplexity and word-accuracy results appearto be competitive with previous ones, while speed isincreased.1 Structured language modelingIn its current incarnation, (unconstrained) speechrecognition relies on a left-to-right language modelL , which estimates the occurrence of a next wordw j given a sequence of preceding words c j = w j?10(the context):1L(w j |c j) = p?
(w j |c j ).L is called a language model (LM).Obviously the context space is huge and evenin very large training corpora most contexts neveroccur, which prohibits a reliable probability esti-mation.
Therefore the context space needs to bemapped to a much smaller space, such that onlythe essential information is retained.
In spite of its1As a shorthand, wba denotes a sequence wawa+1 .
.
.
wb ifb ?
a, else it is the empty sequence.simplicity the trigram LM, that reduces c j to w j?1j?2,is hard to improve on and still the main languagemodel component in state-of-the-art speech recog-nition systems.
It is therefore commonly used as abaseline in the evaluation of other models, includingthe one described in this paper.Structured language models (SLM) introduceparsing into language modeling by alternating be-tween predicting the next word using features ofpartial parses of the context and extending the par-tial parses to cover the next word.
Following thisapproach, Chelba and Jelinek (2000) obtained aSLM that slightly improves on a trigram modelboth in perplexity and recognition performance.The Chelba-Jelinek SLM is, to our knowledge, thefirst left-to-right LM using parsing techniques thatis successfully applied to large vocabulary speechrecognition.
It is built on top of a lexicalized prob-abilistic shift-reduce parser that predicts the nextword from the headwords (?exposed?
heads) andcategories of the last two predicted isolated con-stituents of the context.
Then the predicted wordbecomes the last isolated constituent and the lasttwo constituents are repeatedly recombined until theparser decides to stop.A dynamic programming (DP) version ofChelba?s parser, inspired on the CYK chart parser,was proposed in (Jelinek and Chelba, 1999).
Ourimplementation is roughly quadratic in the lengthof the sentence, but not significantly faster thanChelba?s non-DP parser.
It scored somewhat lowerin perplexity before reestimation (presumably byavoiding search errors), but remained roughly atthe same level after full inside-outside reestimation(Van Aelten and Hogenhout, 2000).An obvious weakness of the Chelba-Jelinek SLMis the bottom-up behavior of the parser: it createsisolated constituents and only afterwards is it able tocheck whether a constituent fits into a higher struc-ture.
Van Uytsel (2000) developed a top-down al-ternative along similar lines but based on a lexical-ized and context-sensitive DP version of an efficientEarley parser (Stolcke, 1995; Jelinek and Lafferty,1991).
The Earley-based SLM performed worsethan the Chelba-Jelinek SLM, mostly due to the factthat the rule production probabilities cannot be con-ditioned on the underlying lexical information, thusproducing a lot of wrong parses.The weaknesses of our Earley SLM have ledus to consider probabilistic left-corner grammar(PLCG) parsing (Manning and Carpenter, 1997),which follows a mixed bottom-up and top-down ap-proach.
Its potential to enhance parsing efficiencyhas been recognized by Roark and Johnson (2000),who simulated a left-corner parser with a top-downbest-first parser applying a left-corner-transformedPCFG grammar.
For the language model describedin this paper, however, we implemented a DP ver-sion of a native left-corner parser using a left-cornertreebank grammar (containing projection rules in-stead of production rules).
The efficiency of our im-plementation further allowed to enrich the historyannotation of the parser states and to apply a lexi-calized grammar.The following section contains a brief review ofManning?s PLCG parser.
Section 3 describes how itwas adapted to our SLM framework: we introducelexicalization and context-sensitivity, present a DPalgorithm using a chart of parser states and finallywe define a language model based on the adaptedPLCG parser.
At the end of the same section we ex-plain how the initial language model can be trainedon additional plain text through a variant of inside-outside reestimation.
In section 4 we evaluate a fewPLCG-based SLMs obtained from the Penn Tree-bank and BLLIP WSJ Corpus.
We present test setperplexity measurements and word accuracy after n-best list rescoring to assess their viability for speechrecognition.2 Classic PLCG parsingThe parameters of a PLCG are called projectionprobabilities.
They are of the formp(Z ?
X ?|X, G),to be read as ?given a completed constituent X dom-inated by a goal category G, the probability thatthere is a Z that has X as its first daughter and ?as its next daughters?.
A PLCG contains essentiallythe same rules as a probabilistic context-free gram-mar (PCFG), but the latter conditions the rule prob-abilities on the mother category Z (production prob-abilities).
In both cases the joint probability of theentire parse tree and the parsed sentence is the prod-uct of the production resp.
projection probabilitiesof the local trees it consists of.While PCFG parsing proceeds from the top downor from the bottom up, PLCG naturally leads to aparsing scheme that is a mixture of both.
The ad-vantages of this are made clear in the subsectionsbelow.
Formally, a PLCG parser has three elemen-tary operations:?
SHIFT: given that an unexpanded constituentG starts from position i , shift the next word wiwith probability ps(wi |G) (G is called the goalcategory);?
PROJECT: given a complete constituent X ,dominated by a goal category G, starting in po-sition i and ending in j , predict a mother con-stituent Z starting in position i and completedup till position j , and zero or more unexpandedsister constituents ?
starting in j with probabil-ity pp(Z ?
X ?|X, G);?
ATTACH: given a complete constituent X dom-inated by a goal category G, identify the firstas the latter with probability pa(X, G).3 Extending the PLCG framework3.1 Synchronous chart parsing with PLCGIn this subsection we present the basic parsing al-gorithm and its data structures and operations.
Inthe subsections that follow, we will introduce lexi-calization and context-sensitivity by extending thisframework.The PLCG parsing process is interpreted as asearch through a network of states, a compact re-presentation of the search space.
The network nodescorrespond to states and the arcs to operations (an-notated with transition probabilities).
A (partial)parse corresponds to a (partial) path through the net-work.
The joint probability of a partial parse and thecovered part of the sentence is equal to the partialpath probability, i.e.
the product of the probabilitiesof the transitions in the path.3.1.1 PLCG statesWe write a state q asq = (G; Z ?
i X ?
j?
;?, ?)
(1)where G is the goal category, Z is the category of aconstituent from position i complete up till positionj , X is the first daughter category, ?
denotes the re-maining unresolved daughters of Z , and ?
and ?
areforward and inner probabilities defined below.
Thewildcard ?
symbolizes zero or more resolved daugh-ter categories: we make abstraction of the identitiesof resolved daughters (except the first one), becausefurther parser moves do not depend on them.
If ?
isempty, q is called a complete state, otherwise q is agoal state.3.1.2 Forward and inner probabilityGiven a state q as defined in (1).
We define its for-ward probability ?
= ?
(q) as the sum of the prob-abilities of the paths ending in q, starting in the ini-tial state and generating w j?10 .
As a consequence,?
(q) = p(w j?10 , q) (joint probability).The inner probability ?
= ?
(q) is the sum of theprobabilities of the paths generating w j?1i , endingin q and starting with a SHIFT of wi .
As a conse-quence, ?
(q) = p(w j?1i , q).Note that the forward and inner probabilities ofthe final state should be identical and equal to p(S).3.1.3 Parser operationsIn this paragraph we reformulate the classic PLCGparser operations in terms of transitions betweenstates.
We hereby specify update formulas for for-ward and inner probabilities.Shift The SHIFT operation starts from a goal stateq = (G; Z ?
i X ?
j Y ?
;?, ?)
(2)and shifts the next word w at position j of the inputby updating q ?
or generating a new state q ?
where2q ?
= (Y ;W?
j w ?
j+1;?
?+= ?p, ?
?
= p) (3)with transition probabilityp = ps(w|Y ).
(4)If q ?
already lives in the chart, only its forward prob-ability is updated.
The given update formula is jus-tified by the relation?
(q ?)
=?q?sq ??
(q)p(q ?
q ?
)where the sum is over all SHIFT transitions from q toq ?
and p(q ?
q ?)
denotes the transition probabilityfrom q to q ?.
Computing ?
(q ?)
is a trivial case of thedefinition.2The C-like shorthand notation ?
?+= ?p means that ??
isset to ?p if there was no q ?
in the chart yet, otherwise ??
isincremented with ?p.Projection From a complete state, two transitionsare possible: ATTACH to a goal state with a prob-ability pa or PROJECT with a probability 1 ?
pa.PROJECT starts from a complete stateq = (G; Z ?
i X ?
j ;?, ?)
(5)and generates or updates a stateq ?
= (G; T ?
i Z ?
j?;?
?+= ?p, ?
?+= ?p) (6)with transition probabilityp = pp(T, ?|Z , G) ?
(1?
pa(Z , G)).
(7)Again, the forward probability is computed recur-sively as a sum of products.
Now ?
?
needs to beaccumulated, too: the constituent Z in general maybe resolved with more than one different X , whicheach time adds to ?
?.Note that a mother constituent inherits G fromher first daughter (left-corner).Attachment Given a complete state q as in (5)where G = Z and some goal state q ??
in the par-tial path leading to qq ??
= (G ??
; T ?
h U ?
i Z ?;??
?, ?
??)
(8)then the ATTACH operation is a transition from q toq ?
withq ?
= (G ??
; T ?
h U ?
j?;?
?+= ????p/?
?
?, ?
?+= ?p)(9)and transition probabilityp = pa(Z , G) ?
?
??.
(10)Why can ??
not be updated from ?, similarly to (3)and (6)?
The reason is that ATTACH makes use ofnon-local constraints: the transition from q to q ?
isonly possible if a matching goal state q ??
occurred ina path leading to q.
Therefore computing ?
as in (3)and (6) would include all paths that generate q ?, alsothose that do not contain q ??.
Instead, the update of??
in (9) combines all paths leading to q ??
with thepaths starting from q ??
and ending in q.
The updateof ?
?
follows an analogous reasoning.3.1.4 Chart representationThe parser produces a set of states that can be conve-niently organized in a staircase-shaped chart similarto the one used by the CYK parser.
In the chart cellwith coordinates (i, j) we store all the states startingin i and completed up till position j .3.1.5 Synchronous parsing algorithmFollowing (Chelba, 2000), we represent a sentenceby a sequence of word identities starting with asentence-begin token ?s?, that is used in the con-text but not predicted, followed by a sentence-endtoken ?/s?, that is predicted by the model.
We arecollecting the sentence proper together with ?/s?
un-der a node labeled TOP?, and the TOP?
node togetherwith ?s?
under a TOP node.
The parser starts fromthe initial stateqI = (TOP;TOP/?s?
?
?1 SB/?s?
?
0TOP?
; 1, 1).
(11)After processing the sentence S = wN?10 and pro-vided a full parse was found, the final stateqF = (TOP;TOP/?s?
?
?1 SB/?s?
?
N ; p(S), p(S))(12)is found in cell (?1, N).Now we are ready to formulate the parsing algo-rithm.
Note that we treat an ATTACH operation as aspecial PROJECT, as explained in Sec.
4.1.1 for j ?
0, 1 to N2 for i ?
j ?
1, j ?
2 to ?13 foreach complete state q in cell (i, j)4 foreach proj in projections(q)5 if goal(q) = cat(q) and proj = ?attach?6 for h?
i ?
1, i ?
2 to ?17 foreach goal state m in cell (h, i)matching q8 q ??
ATTACH(q, m)9 add q ?
to cell (h, j)10 else11 q ??
PROJECT(q)12 add q ?
to cell (i, j)13 if q ?
is complete, recursively add furtherprojections/attachments14 if j = N15 break16 for i ?
?1, 0 to j ?
117 foreach goal state q in cell (i, j)18 q ??
SHIFT(q, w j )19 add q ?
to cell ( j, j + 1)3.2 Lexicalization and context-sensitivityProbably the most important shortcoming ofPCFG?s is the assumption of context-free rule prob-abilities, i.e.
the probability distribution over pos-sible righthand sides given a lefthand side is inde-pendent from the function or position of the left-hand side.
This assumption is quite wrong.
Forinstance, in the Penn Treebank an NP in subjectposition produces a personal pronoun in 13.7% ofthe cases, while in object position it only does so in2.1% of the cases (Manning and Carpenter, 1997).Furthermore, findings from corpus-based linguis-tic studies and developments in functional gram-mar indicate that the lexical realization of a con-text, besides its syntactic analysis, strongly influ-ences patterns of syntactic preference.
Today?s bestautomatic parsers are made substantially more ef-ficient and accurate by applying lexicalized gram-mar (Manning and Schu?tze, 1999).3.2.1 Context-sensitive and lexicalized statesIn our work we did not attempt to find semantic gen-eralizations (such as casting a verb form to its infini-tive form or finding semantic attributes); our simple(but probably suboptimal) approach, borrowed from(Magerman, 1994; Collins, 1996; Chelba, 2000), isto percolate words upward in the parse tree in theform in which they appear in the sentence.
In ourexperiments, we opted to hardcode the head posi-tions as part of the projection rules.3 The nodes ofthe resulting partial parse trees thus are annotatedwith a category label (the CAT feature) and a lexicallabel (the WORD feature).The notation (1) of a state is now replaced withq = (G, L1, L2; Z/z ?
i X/x ?
j?
;?, ?)
(13)where z is the WORD of the mother (possiblyempty), x is the WORD of the first daughter (notempty), and the extended context contains?
G = CAT of a goal state qg;?
L1 = (CAT, WORD) of the state q1 projectingqg;?
L2 = (CAT, WORD) of the state q2 projecting agoal state dominating q1.If the grammar only contains unary and binaryrules, L1 and L2 correspond with Chelba?s conceptof exposed heads ?
which was in fact the idea be-hind the definition above.
The mixed bottom-up andtop-down parsing order of PLCG allows to condi-tion q on a goal constituent G higher up in the par-tial tree containing q; this turns out to significantlyimprove efficiency with respect to Jelinek?s bottom-up chart parser.3Inserting a probabilistic head percolation model, as in(Chelba, 2000), may be an alternative.3.2.2 Extended parser operationsIn this section, we extend the parser operations ofSec.
3.1.3 to handle context-sensitive and lexical-ized states.
The forward and inner probability up-date formulas remain formally the same and are notrepeated here.The SHIFT operation q ?s q ?
is a transition fromq to q ?
with probability p whereq = (G, L1, L2; Z/z ?
i X/x ?
j Y ?
;?, ?)
(2?
)q ?
= (Y, X/x, L1;W/w?
j W/w ?
j+1;?
?, ?
?)(3?
)p = ps(w j |q).
(4?
)The PROJECT operation q ?p q ?
is a transitionfrom q to q ?
with probability p whereq = (G, L1, L2; Z/z ?
i X/x ?
j ;?, ?)
(5?
)q ?
= (G, L1, L2; T/t ?
i Z/z ?
j?;?
?, ?
?)
(6?
)p = pp(T, ?|q) ?
(1?
pa(q)) (7?
)If Z is in head position, t = z; otherwise t is leftunspecified.The ATTACH operation q ?a q ?
is a transitionfrom q to q ?
given q ??
with a probability p whereq ??
= (G, L1, L2; Z/z ?
h X/x ?
iY?;??
?, ?
??)(8?
)q = (Y, X/x, L1;Y/y ?
i T/t ?
j ;?, ?
)q ?
= (G, L1, L2; Z/z ??
h X/x ?
j?;?
?, ?
?)
(9?
)p = pa(q) ?
?
??
(10?
)If Y is in head position, z ?
= y; otherwise, z ?
= z.3.3 PLCG-based language modelA language model (LM) is a word sequence pre-dictor (or an estimator of word sequence probabili-ties).
Following common practice in language mod-eling for speech recognition, we predict words in asentence from left to right4 with probabilities of theform p(w j |w j?10 ).
Suppose the parser has workedits way through w j?10 and is about to make w j -SHIFT transitions.
Then we can writep(w j |w j?10 ) =?q?
  jp(w j |q)p(q|w j?10 ).
(14)4Since this allows the language model to be applied in earlystages of the search.where  j is the set of goal states in position j .
Thefactor p(w j |q) is given by the transition probabilityassociated with the SHIFT operation.5On the other hand, note that?q?
  j?
(q) =?q?
 j?
(q) = p(w j?10 ) (15)where  j is the set of states in position j thatresulted from SHIFT operations.
The first equa-tion holds because there are only PROJECT and AT-TACH transitions between the elements of  j and j , since the sum of outgoing transitions from eachstate in that region equals 1 and therefore the totalprobability mass is preserved.
By inserting (15) into(14) we obtainp(w j |w j?10 ) =?q?
  j p(w j |q)?(q)?q?
  j ?(q).
(16)3.4 Model reestimationThe pp, ps and pa submodels can be rees-timated with iterative expectation-maximization,which needs the computation of frequency expec-tations.
For this purpose we define the outer prob-ability of a state q, written as ?
(q), as the sum ofprobabilities of precisely that part of the paths thatis not included in the inner probability of q. Theouter probability of a complete state is analogous toBaker?s (1979) definition of an outside probability.The outer probabilities are computed in the re-verse direction starting from qF, provided that a listof backward references were stored with each state(?
(q ?)
?
?
?, ?
(q ??)
?
?
??):6?
?
(qF) = 1.?
Reverse ATTACH (cfr.
(8?, 9?, 10?
)): ?+= ?
?
pand ?
?
?+= ?
??p/?
??.
These formulas are madeclear in Fig.
1.?
Reverse PROJECT (cfr.
(5?, 6?, 7?
)): ?+= ?
?p.?
A reverse SHIFT is not necessary, but could beused as a computational check.5Consequently the computation of LM probabilities re-quires almost no extra work.
A model p(w j |q) used in (14)different from ps(w j |q) used by the parser may be chosen how-ever.6Care has to be taken that an outer probability is completebefore it propagates to other items.
A topological sort couldserve this purpose.qIpqos aq ?
?sqaq ?
qF  ?
??
   ?
??
  ???
  ?
   ?
  ?
  ?
?
   ?
?
  ??
Figure 1: Relations between inner and outer probabili-ties along a single path at attachment of q to q ??
resultinginto q ?.Now the expected frequency of a transition o ?
{s, p, a} from q to q ?
in a full parse of S isE[Freq(q ?o q ?|S)] =?all pathsPr(path|S)Freq(q ?o q ?|path).
(17)Since all full parses terminate in qF, the final state,?
(qF) = ?
(qF) = Pr(S).
Therefore (17) is com-putable asE[Freq(q ?o q ?|S)] ={1?(qF)?(q?)?
(q ?)
if o = s,1?(qF)?
(q)p(q?o q?)?
(q ?)
else.
(18)The expected frequencies required for the reesti-mation of the conditional distributions are then ob-tained by summing (18) over the state attributesfrom which the required distribution is independent.4 Empirical evaluation4.1 ModelingWe have trained two sets of models.
The first setwas trained on sections 0?20 of the Penn Treebank(PTB) (Marcus et al, 1995) using sections 21?22for development decisions and tested on sections23?24.
The second set was trained on the BLLIPWSJ Corpus (BWC), which is a machine-parsed(Charniak, 2000) version of (a selection of) theACL/DCI corpus, very similar to the selection madefor the WSJ0/1 CSR corpus.
As the training set,we used the BWC minus the WSJ0/1 ?dfiles?
and?efiles?
intended for CSR development and evalua-tion testing.The PTB devset was used for fixing submodel pa-rameterizations and software debugging, while per-plexities are measured on the PTB testset.
TheBWC trainset was used in rescoring N-best listsin order to assess the models?
potential in speechrecognition.
Both the PTB and BWC underwentthe following preprocessing steps: (a) A vocabu-lary was fixed as the 10k (PTB) resp.
30k (BWC)most frequent words; out-of-vocabulary words werereplaced by ?unk?.
Numbers in Arabic digits werereplaced by one token ?N?.
(b) Punctuation was re-moved.
(c) All characters were converted to lower-case.
(d) All parse trees were binarized in much thesame way as detailed in (Chelba, 2000, pp.
12?17);non-terminal unary productions were eliminated bycollapsing two nodes connected by a unary branchto one node annotated with a combined label.
Thisstep allowed a simple implementation and compar-ison of results with related publications.
We dis-tinguished 1891 different projections, 143 differentnon-terminal categories and 41 different parts-of-speech.
(e) All constituents were annotated with alexical head using deterministic rules by Magerman(1994).The training then proceded by decomposing allparse trees into sequences of SHIFT, PROJECT andATTACH transitions.
The submodels were finallyestimated from smoothed relative counts of transi-tions using standard language modeling techniques:Good-Turing back-off (Katz, 1987) and deleted in-terpolation (Jelinek, 1997).Shift submodelThe SHIFT submodel implements (4?).
Finding agood parameterization entails fixing the featuresthat should explicitly appear in the context and inwhich order, so that all information-bearing ele-ments are incorporated, with limited data fragmen-tation.
This is not a straightforward task.
We wentthrough an iterative process of intuitively guessingwhich feature should be added or removed fromthe context or changing the order, building a corre-sponding model and evaluating its conditional per-plexity (CPPL) against the devset.
The CPPL ofa SHIFT submodel is its perplexity measured ona test set consisting of (context, word to be pre-dicted) pairs (i.e.
the SHIFT transitions according toa certain parameterization) extracted from the cor-rect parse trees of a parsed test corpus.
In otherwords, the CPPL is an underbound of the PPL inthat it would be the PPL from an ideal parser.
We fi-nally concluded that the parameterization (notationbeing consistent with (2?
))ps(w|Y, x, L1.WORD), (19)where the conditioning sequence is ordered frommost to least significant, is optimal for our purposesin the given experimental conditions.
The CPPL ofTable 1: Word trigram (baseline) and PTB model per-plexities.model GT DI(a) word trigram 190 193(b) PLCG-based LM 185 187(c) linear interpolation: .6(a) + .4(b) 159 166this model on the PTB devset is 48, which displaysthe great potential of a correct syntactic partial parseto predict the next word.Project/attach submodelThe ATTACH submodel can be incorporated intothe PROJECT submodel by treating the attachmentas a special kind of projection.
This approachwas systematically applied since it sped up pars-ing.
Having the possibility to choose different pa-rameterizations in separate PROJECT and ATTACHsubmodels did not lower perplexity and increasedexecution time.
Therefore, we always used com-bined PROJECT/ATTACH submodels in further ex-periments.The PROJECT/ATTACH submodel implements (7?
)and (10?).
The process of finding an appropriateparameterization used to build the SHIFT submodelwas also applied here.
Finally we concluded thatthe parameterization (notation being consistent with(5?
))pp(T, ?|Z , G, z) (20)is optimal for our purposes in the given experimen-tal conditions.4.2 Evaluation of PTB modelsTable 1 lists test set perplexities (excluding OOVsand unparsed parts of sentences) of Good-Turingsmoothed back-off models (GT) and deleted-interpolation smoothed (DI) models trained on thePTB trainset and tested on the PTB testset.
We ob-served similar results with both smoothing meth-ods.
As a baseline, word trigram (a) was trainedand tested on the same material.
The PPL obtainedwith the PLCG-based LM (b), using parametriza-tions (19) and (20), is not much lower than the base-line PPL.7 Interpolation (c) with the baseline how-ever yields a relative PPL reduction of 14 to 16%with respect to the baseline.7Using parametrizations pp(T, ?|z, G, L1.CAT) for projec-tion from W-items and pp(T, ?|G, Z, X, z) for other projec-tions, we recently obtained a PPL of 178 (and 155 when inter-polated).
This result is left out from the discussion in order tokeep it clear and complete.Table 2: WER results (%) after 100-best list rescoringon the DARPA WSJ Nov ?92 evaluation test set, non-verbalized punctuation.
The models are smoothed withGood-Turing back-off (WER results in column GT) ordeleted interpolation (DI).rescoring model GT DI(a) DARPA word trigram 10.44(b) BWC word trigram 11.31 11.08(c) BWC Chelba-Jelinek SLM 10.86(d) (a) and (c) combined 9.82(e) (b) and (c) combined 10.60(f) BWC PLCG-based SLM 11.45 11.48(g) (a) and (e) combined 9.85 9.87(h) (b) and (e) combined 10.38 10.58(i) Best possible 4.46 4.46Parse accuracy is around 79% for both labeledprecision and recall on section 23 of PTB (exclud-ing unparsed sentences, about 4% of all sentences).In comparison, with our own implementation ofChelba-Jelinek, we measured a labeled precisionand recall of 57% and 75% on the same input.
Theseresults seem fairly low compared to other recentwork on large-scale parsing, but may be partly dueto the left-to-right restriction of our language mod-els,8 which for instance prohibits word-lookahead.Moreover, while we measured accuracy against abinarized version of PTB, the original parses arerather flat, which may allow higher accuracies.4.3 Evaluation of BWC-modelsThe main target application of our research intoLM is speech recognition.
We performed N-bestlist rescoring experiments on the DARPA WSJ Nov?92 evaluation test set, non-verbalized punctuation.The N-best lists were obtained from the L&H VoiceXpress v4 speech recognizer using the standard tri-gram model included in the test suite (20k open vo-cabulary, no punctuation).In Table 2 we report word-recognition error rates(WER) after rescoring using Chelba-Jelinek andPLCG-based models.
Both DI and GT smooth-ing methods yielded very comparable results.
Dueto technical limitations, all the models except thebaseline trigram were trimmed by ignoring highest-order events that occurred only once.The best PLCG-based SLM trained on the BWCtrain set (f) performs worse than the official wordtrigram (a).
However, since the BWC does not com-pletely cover the complete WSJ0 LM train material8Not to be confused with left-to-right parsing.and slightly differs in tokenization, it is more fairto compare with the performance of a word trigramtrained on the BWC train set (b).
Results (g) and(h) show that the PLCG-based SLM lowers WERwith 4% relative when used in combination with thebaseline models.
A comparable result was obtainedwith the Chelba-Jelinek SLM (results (d) and (e)).5 Conclusion and future workThe PLCG-based SLM exposes a slight loss of ro-bustness in the reduced recognition rate when itis used as a stand-alone rescoring LM.
Combinedwith a word trigram LM however, perplexity andWER reductions with respect to a word 3-grambaseline seem similar to those obtained with theChelba-Jelinek SLM and those previously reportedby Chelba (2000).
On the other hand, the PLCG-based SLM is significantly faster and obtains ahigher parsing accuracy.In the future we plan to evaluate full EM reesti-mation of the models on the trainset using the for-mulas given in this paper.AcknowledgementsThe authors wish to thank Paul Vozila for discussingintermediate results and for providing the authorswith the 100-best lists used for sentence rescoring.The authors are also indebted to Saskia Janssens andKristin Daneels for their help with some of the ex-periments.This research is supported by the Institute forthe promotion of Innovation by Science and Tech-nology in Flanders (IWT-Flanders), contract no.000286.ReferencesJames K. Baker.
1979.
Trainable grammars forspeech recognition.
In Jared J. Wolf and Den-nis H. Klatt, editors, Speech Communication Pa-pers for the 97th Meeting of the Acoustical Soci-ety of America, pages 547?550.
The MIT Press,Cambridge, MA.Eugene Charniak.
2000.
A maximum-entropy in-spired parser.
In Proc.
of the NAACL, pages 132?139.Ciprian Chelba.
2000.
Exploiting Syntactic Struc-ture for Natural Language Modeling.
Ph.D. the-sis, Johns Hopkins University.Michael J. Collins.
1996.
A new statistical parserbased on bigram lexical dependencies.
In Proc.of the 34th Annual Meeting of the ACL, pages184?191.Frederick Jelinek and Ciprian Chelba.
1999.Putting language into language modeling.
InProc.
of Eurospeech ?99, volume I, pages KN?1?6.Frederik Jelinek and John Lafferty.
1991.
Compu-tation of the probability of initial substring gener-ation by stochastic context-free grammars.
Com-putational Linguistics, 17(3):315?323.Frederick Jelinek.
1997.
Statistical Methods forSpeech Recognition.
The MIT Press, Cambridge,MA.Slava M. Katz.
1987.
Estimation of probabili-ties from sparse data for the language modelcomponent of a speech recognizer.
IEEE Trans.on Acoustics, Speech and Signal Processing,35:400?401.David M. Magerman.
1994.
Natural LanguageParsing as Statistical Pattern Recognition.
Ph.D.thesis, Stanford University.Christopher D. Manning and Bob Carpenter.
1997.Probabilistic parsing using left corner languagemodels.
In Proc.
of the Fifth International Work-shop on Parsing Technologies, pages 147?158.Christopher D. Manning and Hinrich Schu?tze.1999.
Foundations of Statistical Natural Lan-guage Processing.
The MIT Press, Cambridge,MA.Mitchell P. Marcus, Beatrice Santorini, andMary Ann Marcinkiewicz.
1995.
Building alarge annotated corpus of English: the Penn Tree-bank.
Computational Linguistics, 19(2):313?330.Brian Roark and Mark Johnson.
2000.
Efficientprobabilistic top-down and left-corner parsing.In Proc.
of the 37th Annual Meeting of the ACL,pages 421?428.Andreas Stolcke.
1995.
An efficient probabilis-tic context-free parsing algorithm that computesprefix probabilities.
Computational Linguistics,21(2):165?201.Filip Van Aelten and Marc Hogenhout.
2000.Inside-outside reestimation of Chelba-Jelinekmodels.
Internal Report L&H?SR?00?027,Lernout & Hauspie, Wemmel, Belgium.Dong Hoon Van Uytsel.
2000.
Earley-inspiredparsing language model: Background and pre-liminaries.
Internal Report PSI-SPCH-00-1,K.U.Leuven, ESAT, Heverlee, Belgium.
