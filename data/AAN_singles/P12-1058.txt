Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 554?562,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsSentence Dependency Tagging in Online Question Answering ForumsZhonghua Qu and Yang LiuThe University of Texas at Dallas{qzh,yangl@hlt.utdallas.edu}AbstractOnline forums are becoming a popular re-source in the state of the art question answer-ing (QA) systems.
Because of its nature as anonline community, it contains more updatedknowledge than other places.
However, go-ing through tedious and redundant posts tolook for answers could be very time consum-ing.
Most prior work focused on extractingonly question answering sentences from userconversations.
In this paper, we introduce thetask of sentence dependency tagging.
Findingdependency structure can not only help findanswer quickly but also allow users to traceback how the answer is concluded throughuser conversations.
We use linear-chain con-ditional random fields (CRF) for sentence typetagging, and a 2D CRF to label the depen-dency relation between sentences.
Our ex-perimental results show that our proposed ap-proach performs well for sentence dependencytagging.
This dependency information canbenefit other tasks such as thread ranking andanswer summarization in online forums.1 IntroductionAutomatic Question Answering (QA) systems relyheavily on good sources of data that contain ques-tions and answers.
Question answering forums, suchas technical support forums, are places where usersfind answers through conversations.
Because oftheir nature as online communities, question answer-ing forums provide more updated answers to newproblems.
For example, when the latest release ofLinux has a bug, we can expect to find solutionsin forums first.
However, unlike other structuredknowledge bases, often it is not straightforward toextract information such as questions and answers inonline forums because such information spreads inthe conversations among multiple users in a thread.A lot of previous work has focused on extract-ing the question and answer sentences from forumthreads.
However, there is much richer informationin forum conversations, and simply knowing a sen-tence is a question or answer is not enough.
Forexample, in technical support forums, often it takesseveral iterations of asking and clarifications to de-scribe the question.
The same happens to answers.Usually several candidate answers are provided, andnot all answers are useful.
In this case users?
feed-back is needed to judge the correctness of answers.Figure 1 shows an example thread in a technicalsupport forum.
Each sentence is labeled with its type(a detailed description of sentence types is providedTable 1).
We can see from the example that ques-tions and answers are not expressed in a single sen-tence or a single post.
Only identifying question andanswering sentences from the thread is not enoughfor automatic question answering.
For this example,in order to get the complete question, we would needto know that sentence S3 is a question that inquiresfor more details about the problem asked earlier, in-stead of stating its own question.
Also, sentence S5should not be included in the correct answer sinceit is not a working solution, which is indicated by anegative feedback in sentence S6.
The correct solu-tion should be sentence S7, because of a user?s posi-tive confirmation S9.
We define that there is a depen-dency between a pair of sentences if one sentence554A: [S1:M-GRET] Hi everyone.
[S2:P-STAT] Ihave recently purchased USB flash and I am havingtrouble renaming it, please help me.B: [S3:A-INQU] What is the size and brand of thisflash?A: [S4:Q-CLRF] It is a 4GB SanDisk flash.B: [S5:A-SOLU] Install gparted, select flash driveand rename.A: [S6:M-NEGA] I got to the Right click onpartition and the label option was there but grayedout.B: [S7:A-SOLU] Sorry again, I meant to right clickthe partition and select Unmount and then selectChange name while in gparted.A: [S8:C-GRAT] Thank you so much.
[S9:M-POST] I now have an ?Epic USB?
You Rock!Figure 1: Example of a Question Answering Thread inUbuntu Support Forumexists as a result of another sentence.
For example,question context sentences exist because of the ques-tion itself; an answering sentence exists because ofa question; or a feedback sentence exists because ofan answer.
The sentence dependency structure ofthis example dialog is shown in Figure 2.S1: M-GRETS2: P-STATS3: A-INQU S4:Q-CLRFS5:A-SOLU S6:M-NEGA S7:A-SOLUS8:C-GRATS9:M-POSTFigure 2: Dependency Structure of the Above ExampleThis example shows that in order to extract in-formation from QA forums accurately, we need tounderstand the sentence dependency structure of aQA thread.
Towards this goal, in this paper, we de-fine two tasks: labeling the types for sentences, andfinding the dependency relations between sentences.For the first task of sentence type labeling, we de-fine a rich set of categories representing the purposeof the sentences.
We use linear-chain conditionalrandom fields (CRF) to take advantage of manylong-distance and non-local features.
The secondtask is to identify relations between sentences.
Mostprevious work only focused on finding the answer-question relationship between sentences.
However,other relations can also be useful for information ex-traction from online threads, such as user?s feed-backs on the answers, problem detail inquiry andquestion clarifications.
In this study, we use twoapproaches for labeling of dependency relation be-tween sentences.
First each sentence is consideredas a source, and we run a linear-chain CRF to la-bel whether each of the other sentences is its tar-get.
Because multiple runs of separate linear-chainCRFs ignore the dependency between source sen-tences, the second approach we propose is to use a2D CRF that models all pair relationships jointly.The data we used was collected from Ubuntuforum general help section.
Our experimental re-sults show that our proposed sentence type taggingmethod works very well, even for the minority cate-gories, and that using 2D CRF further improves per-formance over linear-chain CRFs for identifying de-pendency relation between sentences.The paper is organized as follows.
In the follow-ing section, we discuss related work on finding ques-tions and answers in online environment as well assome dialog act tagging techniques.
In Section 3, weintroduce the use of CRFs for sentence type and de-pendency tagging.
Section 4 describes data collec-tion, annotation, and some analysis.
In Section 5, weshow that our approach achieves promising resultsin thread sentence dependency tagging.
Finally weconclude the paper and suggest some possible futureextensions.2 Related WorkThere is a lot of useful knowledge in the user gener-ated content such as forums.
This knowledge sourcecould substantially help automatic question answer-ing systems.
There has been some previous workfocusing on the extraction of question and corre-sponding answer pairs in online forums.
In (Dinget al, 2008), a two-pass approach was used to findrelevant solutions for a given question, and a skip-chain CRF was adopted to model long range de-555pendency between sentences.
A graph propagationmethod was used in (Cong et al, 2008) to rankrelevant answers to questions.
An approach usingemail structure to detect and summarize question an-swer pairs was introduced in (Shrestha and Mck-eown, 2004).
These studies focused primarily onfinding questions and answers in an online envi-ronment.
In this paper, in order to provide a bet-ter foundation for question answer detection in on-line forums, we investigate tagging sentences with amuch richer set of categories, as well as identifyingtheir dependency relationships.
The sentence typeswe use are similar to dialog acts (DA), but definedspecifically for question answering forums.
Work of(Clark and Popescu-Belis, 2004) defined a reusablemulti-level tagset that can be mapped from conversa-tional speech corpora such as the ICSI meeting data.However, it is hard to reuse any available corpus orDA tagset because our task is different, and also on-line forum has a different style from speech data.Automatic DA tagging has been studied a lot previ-ously.
For example, in (Stolcke et al, 2000), HiddenMarkov Models (HMMs) were used for DA tagging;in (Ji and Bilmes, 2005), different types of graphicalmodels were explored.Our study is different in several aspects: we areusing forum domains, unlike most work of DA tag-ging on conversational speech; we use CRFs for sen-tence type tagging; and more importantly, we alsopropose to use different CRFs for sentence relationdetection.
Unlike the pair-wise sentence analysisproposed in (Boyer et al, 2009) in which HMMwas used to model the dialog structure, our model ismore flexible and does not require related sentencesto be adjacent.3 Thread Structure TaggingAs described earlier, we decompose the structureanalysis of QA threads into two tasks, first deter-mine the sentence type, and then identify relatedsentences.
This section provides details for eachtask.3.1 Sentence Type TaggingIn human conversations, especially speech conver-sations, DAs have been used to represent the pur-pose or intention of a sentence.
Different sets ofDAs have been adopted in various studies, rangingfrom very coarse categories to fine grained ones.
Inthis study, we define 13 fine grained sentence types(corresponding to 4 coarse categories) tailored to ourdomain of QA forum threads.
Table 1 shows the cat-egories and their description.
Some tags such as P-STAT and A-SOLU are more important in that userstry to state a problem and provide solutions accord-ingly.
These are the typical ones used in previouswork on question answering.
Our set includes otheruseful tags.
For example, C-NEGA and C-POSI canevaluate how good an answer is.
Even though C-GRAT does not provide any direct feedback on thesolutions, existence of such a tag often strongly im-plies a positive feedback to an answer.
These sen-tence types can be grouped into 4 coarse categories,as shown in Table 1.Types Category DescriptionProblemsP-STAT question of problemP-CONT problem contextP-CLRF problem clarificationAnswersA-SOLU solution sentenceA-EXPL explanation on solutionsA-INQU inquire problem detailsConfirm.C-GRAT gratitudeC-NEGA negative feedbackC-POSI positive feedbackMisc.M-QCOM question commentM-ACOM comment on the answerM-GRET greeting and politenessM-OFF off-topic sentencesTable 1: Sentence Types for QA ThreadsTo automatically label sentences in a thread withtheir types, we adopt a sequence labeling approach,specifically linear-chain conditional random fields(CRFs), which have shown good performance inmany other tasks (Lafferty, 2001).
Intuitively thereis a strong dependency between adjacent sentences.For example, in our data set, 45% sentences follow-ing a greeting sentence (M-GRET) are question re-lated sentences; 53% sentences following a questioninquiry sentence (Q-INQ) are solution related sen-tences.
The following describes our modeling ap-proaches and features used for sentence type tag-ging.5563.1.1 Linear-chain Conditional Random FieldLinear-chain CRFs is a type of undirected graphi-cal models.
Distribution of a set of variables in undi-rected graphical models can be written asp(x, y) =1Z?A?A(xA, yA) (1)Z is the normalization constant to guarantee validprobability distributions.
CRFs is a special caseof undirected graphical model in which ?
are log-linear functions:?A(xA, yA) = exp{?k?AkfAk(xA, yA)}(2)?A is a real value parameter vector for featurefunction set fA.
In the sequence labeling task, fea-ture functions across the sequence are often tied to-gether.
In other words, feature functions at differentlocations of the sequence share the same parametervector ?.Figure 3: Graphical Structure of Linear-chain CRFs.Linear-chain CRF is a special case of the generalCRFs.
In linear-chain CRF, cliques only involve twoadjacent variables in the sequence.
Figure 3 showsthe graphical structure of a linear-chain CRF.
In ourcase of sentence tagging, cliques only contain twoadjacent sentences.
Given the observation x, theprobability of label sequence y is as follows:p(y|x) =1Z|y|?i=1?e(x, y, i)|y|?j=0?v(x, y, j) (3)?e(x, y, i) = exp{?k?ekfek(yi?1, yi, x, i)}(4)?v(x, y, j) = exp{?k?vkfvk(yj , x, j)}(5)where feature templates fek and fvk correspond toedge features and node features respectively.Feature DescriptionCosine similarity with previous sentence.Quote segment within two adjacent sentences?Code segment within two adjacent sentences?Does this sentence belong to author?s post?Is it the first sentence in a post?Post author participated thread before?Does the sentence contain any negative words?Does the sentence contain any URL?Does the sentence contain any positive words?Does the sentence contain any question mark?Length of the sentence.Presence of verb.Presence of adjective.Sentence perplexity based on a background LM.Bag of word features.Table 2: Features Used in Sentence Type Tagging.3.1.2 Sentence Type Tagging FeaturesWe used various types of feature functions in sen-tence type tagging.
Table 2 shows the complete listof features we used.
Edge features are closely re-lated to the transition between sentences.
Here weuse the cosine similarity between sentences, whereeach sentence is represented as a vector of words,with term weight calculated using TD-IDF (term fre-quency times inverse document frequency).
Highsimilarity between adjacent sentences suggests sim-ilar or related types.
For node features, we exploredifferent sources of information about the sentence.For example, the presence of a question mark indi-cates that a sentence may be a question or inquiry.Similarly, we include other cues, such as positiveor negative words, verb and adjective words.
Sincetechnical forums tend to contain many system out-puts, we include the perplexity of the sentence as afeature which is calculated based on a backgroundlanguage model (LM) learned from common En-glish documents.
We also use bag-of-word featuresas in many other text categorization tasks.Furthermore, we add features to represent postlevel information to account for the structure of QAthreads, for example, whether or not a sentence be-longs to the author?s post, or if a sentence is the be-ginning sentence of a post.5573.2 Sentence Dependency TaggingKnowing only the sentence types without their de-pendency relations is not enough for question an-swering tasks.
For example, correct labeling of ananswer without knowing which question it actuallyrefers to is problematic; not knowing which answera positive or negative feedback refers to will not behelpful at all.
In this section we describe how sen-tence dependency information is determined.
Notethat sentence dependency relations might not be aone-to-one relation.
A many-to-many relation is alsopossible.
Take question answer relation as an ex-ample.
There could be potentially many answersspreading in many sentences, all depending on thesame question.
Also, it is very likely that a questionis expressed in multiple sentences too.Dependency relationship could happen betweenmany different types of sentences, for example, an-swer(s) to question(s), problem clarification to ques-tion inquiry, feedback to solutions, etc.
Instead ofdeveloping models for each dependency type, wetreat them uniformly as dependency relations be-tween sentences.
Hence, for every two sentences,it becomes a binary classification problem, i.e.,whether or not there exists a dependency relationbetween them.
For a pair of sentences, we call thedepending sentence the source sentence, and the de-pended sentence the target sentence.
As describedearlier, one source sentence can potentially dependon many different target sentences, and one targetsentence can also correspond to multiple sources.The sentence dependency task is formally definedas, given a set of sentences St of a thread, find thedependency relation {(s, t)|s ?
St, t ?
St}, where sis the source sentence and t is the target sentence thats depends on.We propose two methods to find the dependencyrelationship.
In the first approach, for each sourcesentence, we run a labeling procedure to find the de-pendent sentences.
From the data, we found given asource sentence, there is strong dependency betweenadjacent target sentences.
If one sentence is a tar-get sentence of the source, often the next sentenceis a target sentence too.
In order to take advantageof such adjacent sentence dependency, we use thelinear-chain CRFs for the sequence labeling.
Fea-tures used in sentence dependency labeling are listedin Table 3.
Note that a lot of the node features usedhere are relative to the source sentence since the taskhere is to determine if the two sentences are related.For a thread of N sentences, we need to perform Nruns of CRF labeling, one for each sentence (as thesource sentence) in order to label the target sentencecorresponding to this source sentence.Feature Description* Cosine similarity with previous sentence.
* Is adjacent sentence of the same type?
* Pair of types of the adjacent target sentences.Pair of types of the source and target sentence.Is target in the same post as the source?Do target and source belong to the same author?Cosine similarity between target and source sentence.Does target sentence happen before source?Post distance between source and target sentence.
* indicates an edge featureTable 3: Features Used in Sentence Dependency LabelingThe linear-chain CRFs can represent the depen-dency between adjacent target sentences quite well.However they cannot model the dependency be-tween adjacent source sentences, because labelingis done for each source sentence individually.
Tomodel the dependency between both the source sen-tences and the target sentences, we propose to use2D CRFs for sentence relation labeling.
2D CRFsare used in many applications considering two di-mension dependencies such as object recognitions(Quattoni et al, 2004) and web information extrac-tion (Zhu et al, 2005).
The graphical structure ofa 2D CRF is shown in Figure 4.
Unlike one di-mensional sequence labeling, a node in 2D environ-ment is dependent on both x-axis neighbors and y-axis neighbors.
In the sentence relation task, thesource and target pair is a 2D relation in which itslabel depends on labels of both its adjacent sourceand its adjacent target sentence.
As shown in Fig-ure 4, looking from x-axis is the sequence of targetsentences with a fixed source sentence, and from y-axis is the sequence of source sentences with a fixedtarget sentence.
This model allows us to model allthe sentence relationships jointly.
2D CRFs contain3 templates of features: node template, x-axis edgetemplate, and y-axis edge template.
We use the sameedge features and node features as in linear-chainCRFs for node features and y-axis edge features in5582D CRFs.
For the x-axis edge features, we use thesame feature functions as for y-axis, except that nowthey represent the relation between adjacent sourcesentences.y i y i + 1 .
.
.. .
.Xy 0 0 .
.
.. .
.. .
.
.
.
.
.
.
.y 10y 0 1 y 11 XSo u r c eTargetFigure 4: Graphical Structure of 2D CRF for SentenceRelation Labeling.In a thread containing N sentences, we wouldhave a 2D CRF containing N2 nodes in a N ?
Ngrid.
Exact inference in such a graph is intractable.In this paper we use loopy belief propagation algo-rithm for the inference.
Loopy belief propagation isa message passing algorithm for graph inference.
Itcalculates the marginal distribution for each node inthe graph.
The result is exact in some graph struc-tures (e.g., linear-chain CRFs), and often convergesto a good approximation for general graphs.4 DataWe used data from ubuntu community forum gen-eral help section for the experiments and evalua-tion.
This is a technical support section that providesanswers to the latest problems in Ubuntu Linux.Among all the threads that we have crawled, we se-lected 200 threads for this initial study.
They con-tain between 2?
10 posts and at least 2 participants.Sentences inside each thread are segmented usingApache OpenNLP tools.
In total, there are 706 postsand 3,483 sentences.
On average, each thread con-tains 3.53 posts, and each post contains around 4.93sentences.
Two annotators were recruited to anno-tate the sentence type and the dependency relationbetween sentences.
Annotators are both computerscience department undergraduate students.
Theyare provided with detailed explanation of the anno-tation standard.
The distribution of sentence typesin the annotated data is shown in Table 4, along withinter-annotator Kappa statistics calculated using 20common threads annotated by both annotators.
Wecan see that the majority of the sentences are aboutproblem descriptions and solutions.
In general, theagreement between the two annotators is quite good.General Type Category Percentage KappaProblemsP-STAT 12.37 0.88P-CONT 37.30 0.77P-CLRF 1.01 0.98AnswersA-SOLU 9.94 0.89A-EXPL 11.60 0.89A-INQU 1.38 0.99ConfirmationC-GRAT 5.06 0.98C-NEGA 1.98 0.96C-POSI 1.84 0.96MiscellaneousM-QCOM 1.98 0.93M-ACOM 1.47 0.96M-GRET 1.01 0.96M-OFF 7.92 0.96Table 4: Distribution and Inter-annotator Agreement ofSentence Types in DataThere are in total 1, 751 dependency relationsidentified by the annotators among those tagged sen-tences.
Note that we are only dealing with intra-thread sentence dependency, that is, no dependencyamong sentences in different threads is labeled.Considering all the possible sentence pairs in eachthread, the labeled dependency relations represent asmall percentage.
The most common dependencyis problem description to problem question.
Thisshows that users tend to provide many details ofthe problem.
This is especially true in technical fo-rums.
Seeing questions without their context wouldbe confusing and hard to solve.
The relation of an-swering solutions and question dependency is alsovery common, as expected.
The third common re-lation is the feedback dependency.
Even though thenumber of feedback sentences is small in the dataset, it plays a vital role to determine the quality ofanswers.
The main reason for the small number isthat, unlike problem descriptions, much fewer sen-tences are needed to give feedbacks.5 ExperimentIn the experiment, we randomly split annotatedthreads into three disjoint sets, and run a three-foldcross validation.
Within each fold, first sentencetypes are labeled using linear-chain CRFs, then the559resulting sentence type tagging is used in the sec-ond pass to determine dependency relations.
Forpart-of-speech (POS) tagging of the sentences, weused Stanford POS Tagger (Toutanova and Man-ning, 2000).
All the graphical inference and estima-tions are done using MALLET package (McCallum,2002).In this paper, we evaluate the results using stan-dard precision and recall.
In the sentence type tag-ging task, we calculate precision, recall, and F1score for each individual tag.
For the dependencytagging task, a pair identified by the system is cor-rect only if the exact pair appears in the reference an-notation.
Precision and recall scores are calculatedaccordingly.5.1 Sentence Type Tagging ResultsThe results of sentence type tagging using linear-chain CRFs are shown in Table 5.
For a comparison,we include results using a basic first-order HMMmodel.
Because HMM is a generative model, weuse only bag of word features in the generative pro-cess.
The observation probability is the probabil-ity of the sentence generated by a unigram languagemodel, trained for different sentence types.
Sincefor some applications, fine grained categories maynot be needed, for example, in the case of findingquestions and answers in a thread, we also includein the table the tagging results when only the gen-eral categories are used in both training and testing.We can see from the table that using CRFsachieves significantly better performance thanHMMs for most categories, except greeting and off-topic types.
This is mainly because of the advantageof CRFs, allowing the incorporation of rich discrimi-native features.
For the two major types of problemsand answers, in general, our system shows very goodperformance.
Even for minority types like feed-backs, it also performs reasonably well.
When usingcoarse types, the performance on average is bettercompared to the finer grained categories, mainly be-cause of the fewer classes in the classification task.Using the fine grained categories, we found that thesystem is able to tell the difference between ?prob-lem statement?
(P-STAT) and ?problem context?
(P-CONT).
Note that in our task, a problem statement isnot necessarily a question sentence.
Instead it couldbe any sentence that expresses the need for a solu-Linear-chain CRF First-order HMM13 Fine Grained TypesTag Prec.
/ Rec.
F1 Prec.
/ Rec.
F1M-GRET 0.45 / 0.58 0.51 0.73 / 0.57 0.64P-STAT 0.79 / 0.72 0.75 0.35 / 0.34 0.35P-CONT 0.80 / 0.74 0.77 0.58 / 0.18 0.27A-INQU 0.37 / 0.48 0.42 0.11 / 0.25 0.15A-SOLU 0.78 / 0.64 0.71 0.27 / 0.29 0.28A-EXPL 0.4 / 0.76 0.53 0.24 / 0.19 0.21M-POST 0.5 / 0.41 0.45 0.04 / 0.1 0.05C-GRAT 0.43 / 0.53 0.48 0.01 / 0.25 0.02M-NEGA 0.67 / 0.5 0.57 0.09 / 0.31 0.14M-OFF 0.11 / 0.23 0.15 0.20 / 0.23 0.21P-CLRF 0.15 / 0.33 0.21 0.10 / 0.12 0.11M-ACOM 0.27 / 0.38 0.32 0.09 / 0.1 0.09M-QCOM 0.34 / 0.32 0.33 0.08 / 0.23 0.114 General TypesTag Prec.
/ Rec.
F1 Prec.
/ Rec.
F1Problem 0.85 / 0.76 0.80 0.73 / 0.27 0.39Answers 0.65 / 0.72 0.68 0.45 / 0.36 0.40Confirm.
0.80 / 0.74 0.77 0.06 / 0.26 0.10Misc.
0.43 / 0.61 0.51 0.04 / 0.36 0.08Table 5: Sentence Type Tagging Performance UsingCRFs and HMM.tion.We also performed some analysis of the featuresusing the feature weights in the trained CRF mod-els.
We find that some post level information is rela-tively important.
For example, the feature represent-ing whether the sentence is before a ?code?
segmenthas a high weight for problem description classifica-tion.
This is because in linux support forum, peopleusually put some machine output after their problemdescription.
We also notice that the weights for verbwords are usually high.
This is intuitive since the?verb?
of a sentence can often determine its purpose.5.2 Sentence Dependency Tagging ResultsTable 6 shows the results using linear-chain CRFs(L-CRF) and 2D CRFs for sentence dependency tag-ging.
We use different settings in our experiments.For the categories of sentence types, we evaluate us-ing both the fine grained (13 types) and the coarsecategories (4 types).
Furthermore, we examine twoways to obtain the sentence types.
First, we use theoutput from automatic sentence type tagging.
In thesecond one, we use the sentence type informationfrom the human annotated data in order to avoid theerror propagation from automatic sentence type la-560beling.
This gives an oracle upper bound for thesecond pass performance.Using Oracle Sentence TypeSetup Precision Recall F113 typesL-CRF 0.973 0.453 0.6182D-CRF 0.985 0.532 0.6914 generalL-CRF 0.941 0.124 0.2182D-CRF 0.956 0.145 0.252Using System Sentence TypeSetup Precision Recall F113 typesL-CRF 0.943 0.362 0.5232D-CRF 0.973 0.394 0.5614 generalL-CRF 0.939 0.101 0.1822D-CRF 0.942 0.127 0.223Table 6: Sentence Dependency Tagging PerformanceFrom the results we can see that 2D CRFs out-perform linear-chain CRFs for all the conditions.This shows that by modeling the 2D dependency insource and target sentences, system performance isimproved.
For the sentence types, when using auto-matic sentence type tagging systems, there is a per-formance drop.
The performance gap between us-ing the reference and automatic sentence types sug-gests that there is still room for improvement frombetter sentence type tagging.
Regarding the cate-gories used for the sentence types, we observe thatthey have an impact on dependence tagging perfor-mance.
When using general categories, the perfor-mance is far behind that using the fine grained types.This is because some important information is lostwhen grouping categories.
For example, a depen-dency relation can be: ?A-EXPL?
(explanation forsolutions) depends on ?A-SOLU?
(solutions); how-ever, when using coarse categories, both are mappedto ?Solution?, and having one ?Solution?
dependingon another ?Solution?
is not very intuitive and hardto model properly.
This shows that detailed cate-gory information is very important for dependencytagging even though the tagging accuracy from thefirst pass is far from perfect.Currently our system does not put constraints onthe sentence types for which dependencies exist.
Inthe system output we find that sometimes there areobvious dependency errors, such as a positive feed-back depending on a negative feedback.
We mayimprove our models by taking into account differentsentence types and dependency relations.6 ConclusionIn this paper, we investigated sentence dependencytagging of question and answer (QA) threads in on-line forums.
We define the thread tagging task as atwo-step process.
In the first step, sentence typesare labeled.
We defined 13 sentence types in or-der to capture rich information of sentences to bene-fit question answering systems.
Linear chain CRFis used for sentence type tagging.
In the secondstep, we label actual dependency between sentences.First, we propose to use a linear-chain CRF to labelpossible target sentences for each source sentence.Then we improve the model to consider the depen-dency between sentences along two dimensions us-ing a 2D CRF.
Our experiments show promisingperformance in both tasks.
This provides a goodpre-processing step towards automatic question an-swering.
In the future, we plan to explore usingconstrained CRF for more accurate dependency tag-ging.
We will also use the result from this work inother tasks such as answer quality ranking and an-swer summarization.7 AcknowledgmentThis work is supported by DARPA under ContractNo.
HR0011-12-C-0016 and NSF No.
0845484.Any opinions expressed in this material are those ofthe authors and do not necessarily reflect the viewsof DARPA or NSF.ReferencesKristy Elizabeth Boyer, Robert Phillips, Eun Young Ha,Michael D. Wallis, Mladen A. Vouk, and James C.Lester.
2009.
Modeling dialogue structure with ad-jacency pair analysis and hidden markov models.
InProc.
NAACL-Short, pages 49?52.Alexander Clark and Andrei Popescu-Belis.
2004.Multi-level dialogue act tags.
In Proc.
SIGDIAL,pages 163?170.Gao Cong, Long Wang, Chinyew Lin, Youngin Song, andYueheng Sun.
2008.
Finding question-answer pairsfrom online forums.
In Proc.
SIGIR, pages 467?474.Shilin Ding, Gao Cong, Chinyew Lin, and Xiaoyan Zhu.2008.
Using conditional random fields to extract con-texts and answers of questions from online forums.
InProc.
ACL-HLT.Gang Ji and J Bilmes.
2005.
Dialog Act Tagging UsingGraphical Models.
In Proc.
ICASSP.561John Lafferty.
2001.
Conditional random fields: Proba-bilistic models for segmenting and labeling sequencedata.
In Proc.
ICML, pages 282?289.Andrew Kachites McCallum.
2002.
Mal-let: A machine learning for language toolkit.http://mallet.cs.umass.edu.Ariadna Quattoni, Michael Collins, and Trevor Darrell.2004.
Conditional random fields for object recogni-tion.
In Proc.
NIPS, pages 1097?1104.Lokesh Shrestha and Kathleen Mckeown.
2004.
Detec-tion of question-answer pairs in email conversations.In Proc.
Coling, pages 889?895.Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-beth Shriberg, Rebecca Bates, Daniel Jurafsky, PaulTaylor, Rachel Martin, Carol Van Ess-Dykema, andMarie Meteer.
2000.
Dialogue act modeling forautomatic tagging and recognition of conversationalspeech.
Computational Linguistics, 26:339?373.Kristina Toutanova and Christopher D. Manning.
2000.Enriching the knowledge sources used in a maximumentropy part-of-speech tagger.
In Proc.
EMNLP/VLC,pages 63?70.Jun Zhu, Zaiqing Nie, Ji R. Wen, Bo Zhang, and Wei Y.Ma.
2005.
2D Conditional Random Fields for Webinformation extraction.
In Proc.
ICML, pages 1044?1051, New York, NY, USA.562
