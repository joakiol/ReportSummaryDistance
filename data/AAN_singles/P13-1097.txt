Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 983?992,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsProbabilistic Sense Sentiment Similarity through Hidden EmotionsMitra Mohtarami1, Man Lan2, and Chew Lim Tan11Department of Computer Science, National University of Singapore;2Department of Computer Science, East China Normal University{mitra,tancl}@comp.nus.edu.sg;mlan@cs.ecnu.edu.cnAbstractSentiment Similarity of word pairs reflects thedistance between the words regarding theirunderlying sentiments.
This paper aims to in-fer the sentiment similarity between wordpairs with respect to their senses.
To achievethis aim, we propose a probabilistic emotion-based approach that is built on a hidden emo-tional model.
The model aims to predict a vec-tor of basic human emotions for each sense ofthe words.
The resultant emotional vectors arethen employed to infer the sentiment similarityof word pairs.
We apply the proposed ap-proach to address two main NLP tasks, name-ly, Indirect yes/no Question Answer Pairs in-ference and Sentiment Orientation prediction.Extensive experiments demonstrate the effec-tiveness of the proposed approach.1 IntroductionSentiment similarity reflects the distance be-tween words based on their underlying senti-ments.
Semantic similarity measures such as La-tent Semantic Analysis (LSA) (Landauer et al,1998) can effectively capture the similarity be-tween semantically related words like "car" and"automobile", but they are less effective in relat-ing words with similar sentiment orientation like"excellent" and "superior".
For example, the fol-lowing relations show the semantic similaritybetween some sentiment words computed byLSA::		, 	 = 0.40		 < 	,  = 0.46	 < 		,   = 0.65Clearly, the sentiment similarity between theabove words should be in the reversed order.
Infact, the sentiment intensity in "excellent" iscloser to "superior" than "good".
Furthermore,sentiment similarity between "good" and "bad"should be 0.In this paper, we propose a probabilistic ap-proach to detect the sentiment similarity ofwords regarding their senses and underlying sen-timents.
For this purpose, we propose to modelthe hidden emotions of word senses.
We showthat our approach effectively outperforms thesemantic similarity measures in two NLP tasks:Indirect yes/no Question Answer Pairs (IQAPs)Inference and Sentiment Orientation (SO) pre-diction that are described as follows:In IQAPs, answers do not explicitly containthe yes or no keywords, but rather provide con-text information to infer the yes or no answer(e.g.
Q: Was she the best one on that old show?A: She was simply funny).
Clearly, the sentimentwords in IQAPs are the pivots to infer the yes orno answers.
We show that sentiment similaritybetween such words (e.g., here the adjectivesbest and Funny) can be used effectively to inferthe answers.The second application (SO prediction) aims todetermine the sentiment orientation of individualwords.
Previous research utilized the semanticrelations between words obtained from WordNet(Hassan and Radev, 2010) and semantic similari-ty measures (e.g.
Turney and Littman, 2003) forthis purpose.
In this paper, we show that senti-ment similarity between word pairs can be effec-tively utilized to compute SO of words.The contributions of this paper are follows:?
We propose an effective approach to predictthe sentiment similarity between word pairsthrough hidden emotions at the sense level,?
We show the utility of sentiment similarityprediction in IQAP inference and SO predic-tion tasks, and?
Our hidden emotional model can infer the typeand number of hidden emotions in a corpus.9832 Sentiment Similarity through HiddenEmotionsAs we discussed above, semantic similaritymeasures are less effective to infer sentimentsimilarity between word pairs.
In addition, dif-ferent senses of sentiment words carry differenthuman emotions.
In fact, a sentiment word canbe represented as a vector of emotions with in-tensity values from "very weak" to "very strong".For example, Table 1 shows several sentimentwords and their corresponding emotion vectorsbased the following set of emotions: e = [anger,disgust, sadness, fear, guilt, interest, joy, shame,surprise].
For example, "deceive" has 0.4 and 0.5intensity values with respect to the emotions"disgust" and "sadness" with an overall -0.9 (i.e.-0.4-0.5) value for sentiment orientation(Neviarouskaya et al, 2007; Neviarouskaya etal., 2009).Word Emotional Vector SOe = [anger, disgust, sadness, fear, guilt, interest, joy, shame, surprise]Rude ['0.2', '0.4',0,0,0,0,0,0,0] -0.6doleful [0, 0, '0.4',0,0,0,0,0,0] -0.4smashed [0,0, '0.8', '0.6',0,0,0,0,0] -1.4shamefully [0,0,0,0,0,0,0, '0.7',0] -0.7deceive [0, '0.4', '0.5',0,0,0,0,0,0] -0.9Table  1.
Sample of emotional vectorsThe difficulty of the sentiment similarity predic-tion task is evident when terms carry differenttypes of emotions.
For instance, all the words inTable 1 have negative sentiment orientation, but,they carry different emotions with different emo-tion vectors.
For example, "rude" reflects theemotions "anger" and "disgust", while the word"doleful" only reflects the emotion "sadness".
Assuch, the word "doleful" is closer to the words"smashed" and "deceive" involving the emotion"sadness" than others.
We show that emotionvectors of the words can be effectively utilized topredict the sentiment similarity between them.Previous research shows little agreement aboutthe number and types of the basic emotions(Ortony and Turner 1990; Izard 1971).
Thus, weassume that the number and types of basic emo-tions are hidden and not pre-defined and proposea Probabilistic Sense Sentiment Similarity(PSSS) approach to extract the hidden emotionsof word senses to infer their sentiment similarity.3 Hidden Emotional ModelOnline review portals provide rating mechanisms(in terms of stars, e.g.
5- or 10-star rating) to al-Figure 1.The structure of PSSS modellow users to attach ratings to their reviews.
Arating indicates the summarized opinion of a userwho ranks a product or service based on his feel-ings.
There are various feelings and emotionsbehind such ratings with respect to the content ofthe reviews.Figure 1 shows the intermediate layer of hid-den emotions behind the ratings (sentiments)assigned to the documents (reviews) containingthe words.
This Figure indicates the generalstructure of our PSSS model.
It shows that hid-den emotions (ei) link the rating (rj) and the doc-uments (dk).
In this Section, we aim to employratings and the relations among ratings, docu-ments, and words to extract the hidden emotions.Figure 2 illustrates a simple graphical modelusing plate representation of Figure 1.
As Figures2 shows, the rating r from a set of ratings R={r1,?,rp} is assigned to a hidden emotion setE={e1,?,ek}.
A document d from a set of docu-ments D= {d1,?,dN} with vocabulary set W={w1,?,wM} is associated with the hidden emotionset.The model presented in Figure 2(a) has beenexplored in (Mohtarami et al, 2013) and is calledSeries Hidden Emotional Model (SHEM).
Thisrepresentation assumes that the word w is de-pendent to d and independent to e (we refer tothis Assumption as A1).
However, in reality, aword w can inherit properties (e.g., emotions)(b): Bridged modelFigure 1. he structure of PSSS odel(a): Series modelFigure 2.
Hidden emotional model984from the document d that contains w. Thus, wecan assume that w is implicitly dependant on e.To account for this, we present Bridged HiddenEmotional Model (BHEM) shown in Figure 2(b).Our assumption, A2, in the BHEM model is asfollows: w is dependent to both d and e.Considering Figure 1, we represent the entiretext collection as a set of (w,d,r) in which eachobservation (w,d,r) is associated with a set ofunobserved emotions.
If we assume that the ob-served tuples are independently generated, thewhole data set is generated based on the jointprobability of the observation tuples (w,d,r) asthe follows (Mohtarami et al, 2013):" =	###$%, , &',(,)'()=	###$%, , &',(&(,) 									1'()where, P(w,d,r) is the joint probability of the tu-ple (w,d,r), and n(w,d,r) is the frequency of w indocument d of rating r (note that n(w,d) is theterm frequency of w in d and n(d,r) is one if r isassigned to d, and 0 otherwise).
The joint proba-bility for the BHEM is defined as follows con-sidering hidden emotion e:- regarding class probability of the hidden emotion eto be assigned to the observation (w,d,r):$%, ,  = 	+$%, , |	$	-==	+$%, |	$|	$	-- regarding assumption A2 and Bayes' Rule:=	+$%|, 	$, 	$|	-- using Bayes' Rule:=	+$, 	|%$%$|	-- regarding A2 and conditional independency:=	+$|%$	|%$%$|	-= $|%+$%|	$	$|																																							2-In the bridged model, the joint probability doesnot depend on the probability P(d|e) and theprobabilities P(w|e), P(e) and P(r|e) are un-known, while in the SHEM model explained in(Mohtarami et al, 2013), the joint probabilitydoes not depend on P(w|e), and probabilitiesP(d|e), P(e), and P(r|e) are unknown.We employ Maximum Likelihood approach tolearn the probabilities and infer the possible hid-den emotions.
The log-likelihood of the wholedata set D in Equation (1) can be defined as fol-lows: = 	+++%, , log$%, , 														3'()Replacing P(w,d,r) by the values computed us-ing the bridged model in Equation (2) results in:= 	+++%, , log[$|%+$%|	$	$|	-]'()4The above optimization problems are hard tocompute due to the log of sum.
Thus, Expecta-tion-maximization (EM) is usually employed.EM consists of two following steps:1.
E-step: Calculates posterior probabilities forhidden emotions given the words, documentsand ratings, and2.
M-step: Updates unknown probabilities (suchas P(w|e) etc) using the posterior probabilitiesin the E-step.The steps of EM can be computed for BHEMmodel.
EM of the model employs assumptionsA2 and Bayes Rule and is defined as follows:E-step:$	|%, ,  = $|	$	$%|	?
$|	$	$%|	- 																												5M-step:$|	 = ?
?%, , $e|%, , '(?
?
?%, ,  $e|%, , '()=	 ?%, $e|%, , '?
?%, $e|%, , ') 																														6$%|	 = ?
?%, , $e|%, , ()?
?
?%, , $e|%, , ()'=	 ?%, $e|%, , )?
?%, $e|%, , )' 																													7$	 = ?
?
?%, , $e|%, , '()?
?
?
?%,, $e|%, , ')(8= 	 ?
?%,  $e|%, , ')?
?
?%,  $e|%, , ')8 																								8Note that in Equation (5), the probabilityP(e|w,d,r) does not depend on the document d.Also, in Equations (6)-(8) we remove the de-pendency on document d using the followingEquation:+%, ,  =%, (9where n(w,r) is the occurrence of w in all thedocuments in the rating r.The EM steps computed by the bridged modeldo not depend on the variable document d, anddiscard d from the model.
The reason is that wbypasses d to directly associate with the hiddenemotion e in Figure 2(b).985Similar to BHEM, the EM steps for SHEM canbe computed by considering assumptions A1 andBayes Rule as follows (Mohtarami et al, 2013):E-step:$	|%, ,  = $|	$	$|	?
$|	$	$|	- 																											10M-step:$|	 = ?
?%, , $e|%, , '(?
?
?%, ,  $e|%, , '() 										11$|	 = ?
?%, , $e|%, , ')?
?
?%, ,  $e|%, , ')( 										12$	 = ?
?
?%, ,  $e|%, , '()?
?
?
?%, , $e|%, , ')(8 							13Finally, we construct the emotional vectors us-ing the algorithm presented in Table 2.
The algo-rithm employs document-rating, term-documentand term-rating matrices to infer the unknownprobabilities.
This algorithm can be used withboth bridged or series models.
Our goal is to in-fer the emotional vector for each word w that canbe obtained by the probability P(w|e).
Note that,this probability can be simply computed for theSHEM model using P(d|e) as follows:$%|	 =+$%|$|	(143.1 Enriching Hidden Emotional ModelsWe enrich our emotional model by employingthe requirement that the emotional vectors of twosynonym words w1 and w2 should be similar.
Forthis purpose, we utilize the semantic similaritybetween each two words and create an enrichedmatrix.
Equation (15) shows how we computethis matrix.
To compute the semantic similaritybetween word senses, we utilize their synsets asfollows:%;%< = $=>%;|>%<?= 1|>%;|	 +1|>%<| + $=%;|%<?|@A&'B|C|@A&'D|E15where, syn(w) is the synset of w. Let count(wi,wj) be the co-occurrence of the wi and wj, and letcount(wj) be the total word count.
The probabil-ity of wi given wj will then be P(wi|wj) =count(wi, wj)/ count(wj).
In addition, note thatemploying the synset of the words help to obtaindifferent emotional vectors for each sense of aword.The resultant enriched matrix W?W is multi-plied to the inputs of our hidden model (matricesW?D	or	W?R.
Note that this takes into accountInput:Series Model: Document-Rate D?R, Term-DocumentW?DBridged Model: Term-Rate W?ROutput: Emotional vectors {e1, e2, ?,ek} for wAlgorithm:1.
Enriching hidden emotional model:Series Model: Update Term-Document W?DBridged Model: Update Term-Rate W?R2.
Initialize unknown probabilities:Series Model: Initialize P(d|e), P(r|e), and P(e), ran-domlyBridged Model: Initialize P(w|e), P(r|e), and P(e)3. while L  has not converged to a pre-specified value do4.
E-step;Series Model: estimate the value of P(e|w,d,r) inEquation 10Bridged Model: estimate the value of P(e|w,d,r) inEquation 55.
M-step;Series Model: estimate the values of P(r|e), P(d|e),and P(e) in Equations 11-13, respectivelyBridged Model: estimate the values of P(r|e), P(w|e),and P(e) in Equations 6-8, respectively6.
end while7.
If series hidden emotional model is used then8.
Infer word emotional vector: estimate P(w|e) inEquation 14.9.
End ifTable  2.
Constructing emotional vectors via P(w|e)the senses of the words as well.
The learning stepof EM is done using the updated inputs.
In thiscase, the correlated words can inherit the proper-ties of each other.
For example, if wi does notoccur in a document or rating involving anotherword (i.e., wj), the word wi can still be indirectlyassociated with the document or rating throughthe word wj.
However, the distribution of theopinion words in documents and ratings is notuniform.
This may decrease the effectiveness ofthe enriched matrix.The nonuniform distribution of opinion wordshas been also reported by Amiri et al (2012)who showed that the positive words are frequent-ly used in negative reviews.
We also observedthe same pattern in the development dataset.
Fig-ure 3 shows the overall occurrence of some posi-tive and negative seeds in various ratings.
Asshown, in spite of the negative words, the posi-tive words may frequently occur in both positiveand negative documents.
Such distribution of986Figure 3.
Nonuniform distribution of opinion wordspositive words can mislead the enriched model.To address this issue, we measure the confi-dence of an opinion word in the enriched matrixas follows.KL' = M[NO'P ?
"O'P ?
NO'R ?
"O'R]NO'P ?
"O'P + NO'R ?
"O'R  16where, NO'P (NO'R) is the frequency of w in theratings 1 to 4 (7 to 10), and "O'P ("O'R) is thetotal number of documents with rating 1 to 4 (7to 10) that contain w. The confidence value of wvaries from 0 to 1, and it increases if:?
There is a large difference between the occur-rences of w in positive and negative ratings.?
There is a large number of reviews involvingw in the relative ratings.To improve the efficiency of enriched matrix,the columns corresponding to each word in thematrix are multiplied by its confidence value.4 Predicting Sentiment SimilarityWe utilize the approach proposed in (Mohtaramiet al, 2013) to compute the sentiment similaritybetween two words.
This approach compares theemotional vector of the given words.
Let X and Ybe the emotional vectors of two words.
Equation(17) computes their correlation:V, W = ?
V; ?
VXW; ?
WX&;YZ?
1[\ 																																17where,is number of emotional categories, V,] WXand [ , \  are the mean and standard deviationvalues of ^  and _  respectively.
V, W = ?1indicates that the two vectors are completely dis-similar, and V, W = 1 indicates that the vec-tors have perfect similarity.The approach makes use of a thresholdingmechanism to estimate the proper correlationvalue to find sentimentally similar words.
Forthis, as in Mohtarami et al (2013) we utilized theantonyms of the words.
We consider two words,Input:`: The adjective in the question of given IQAP.
: The adjective in the answer of given IQAP.Output: answer ?
{>	,,  }Algorithm:1. if ` or  are missing from our corpus then2.
answer=Uncertain;3. else if  `,  < 0 then4.
answer=No;5.        else if `,  > 0 then6.
answer=yes;Figure 4.
Sentiment similarity for IQAP inference%; and %< as similar in sentiment iff they satisfyboth of the following conditions:1.
=%; ,%<?
> =%;,~%<?,2.
=%; ,%<?
> =~%;,%<?where, ~%;  is antonym of %; , and =%; , %<?is obtained from Equation (17).
Finally, we com-pute the sentiment similarity (SS) as follows:=%; ,%<?
==%; ,%<?
?fg=%; ,~%<?, =~%;,%<?h			18Equation (18) enforces two sentimentally simi-lar words to have weak correlation to the anto-nym of each others.
A positive value of SS(.,.
)indicates the words are sentimentally similar anda negative value shows that they are dissimilar.5 ApplicationsWe explain our approach in utilizing sentimentsimilarity between words to perform IQAP infer-ence and SO prediction tasks respectively.In IQAPs, we employ the sentiment similaritybetween the adjectives in questions and answersto interpret the indirect answers.
Figure 4 showsthe algorithm for this purpose.
SS(.,.)
indicatessentiment similarity computed by Equation (18).A positive SS means the words are sentimentallysimilar and thus the answer is yes.
However,negative SS leads to a no response.In SO-prediction task, we aim to computemore accurate SO using our sentiment similaritymethod.
Turney and Littman (2003) proposed amethod in which the SO of a word is calculatedbased on its semantic similarity with seven posi-tive words minus its similarity with seven nega-tive words as shown in Figure 5.
As the similari-ty function, A(.,.
), they employed point-wise mu-tual information (PMI) to compute the similaritybetween the words.
Here, we utilize the sameapproach, but instead of PMI we use our SS(.,.
)measure as the similarity function.987Input: $%: seven words with positive SO i%: seven words with negative SO .
, .
: similarity function, and %: a given word withunknown SOOutput: sentiment orientation of wAlgorithm:1.
$ = j_% =+ %, %?
+ %,%&'l)(m	n'l)(@o'l)(m	p'l)(@Figure 5.
SO based on the similarity function A(.,.
)6 Evaluation and Results6.1 Data and SettingsWe used the review dataset employed by Maas etal.
(2011) as the development dataset that con-tains movie reviews with star rating from onestar (most negative) to 10 stars (most positive).We exclude the ratings 5 and 6 that are moreneutral.
We used this dataset to compute all theinput matrices in Table 2 as well as the enrichedmatrix.
The development dataset contains 50kmovie reviews and 90k vocabulary.We also used two datasets for the evaluationpurpose: the MPQA (Wilson et al, 2005) andIQAPs (Marneffe et al, 2010) datasets.
TheMPQA dataset is used for SO prediction experi-ments, while the IQAP dataset is used for theIQAP experiments.
We ignored the neutralwords in MPQA dataset and used the remaining4k opinion words.
Also, the IQAPs dataset(Marneffe et al, 2010) contains 125 IQAPs andtheir corresponding yes or no labels as theground truth.6.2 Experimental ResultsTo evaluate our PSSS model, we perform exper-iments on the SO prediction and IQAPs infer-ence tasks.
Here, we consider six emotions forboth bridged and series models.
We study theeffect of emotion numbers in Section 7.1.
Also,we set a threshold of 0.3 for the confidence valuein Equation (16), i.e.
we set the confidence val-ues smaller than the threshold to 0.
We explainthe effect of this parameter in Section 7.3.Evaluation of SO PredictionWe evaluate the performance of our PSSS mod-els in the SO prediction task using the algorithmexplained in Figure 5 by setting our PSSS assimilarity function (A).
The results on SO predic-tion are presented in Table 3.
The first and se-Method Precision Recall F1PMI 56.20 56.36 55.01ER 65.68 65.68 63.27PSSS-SHEM 68.51 69.19 67.96PSSS-BHEM 69.39 70.07 68.68Table 3.
Performance on SO prediction taskcond rows present the results of our baselines,PMI (Turney and Littman, 2003) and ExpectedRating (ER) (Potts, 2011) of words respectively.PMI extracts the semantic similarity betweenwords using their co-occurrences.
As Table 3shows, it leads to poor performance.
This ismainly due to the relatively small size of the de-velopment dataset which affects the quality ofthe co-occurrence information used by the PMI.ER computes the expected rating of a wordbased on the distribution of the word across rat-ing categories.
The value of ER indicates the SOof the word.
As shown in the two last rows of thetable, the results of PSSS approach are higherthan PMI and ER.
The reason is that PSSS isbased on the combination between sentimentspace (through using ratings, and matrices W?Rin BHEM, D?R in SHEM) and semantic space(through the input W?D in SHEM and enrichedmatrix W?W in both hidden models).
However,the PMI employs only the semantic space (i.e.,the co-occurrence of the words) and ER uses oc-currence of the words in rating categories.Furthermore, the PSSS model achieves higherperformance with BHEM rather than SHEM.This is because the emotional vectors of thewords are directly computed from the EM stepsof BHEM.
However, the emotional vectors ofSHEM are computed after finishing the EM stepsusing Equation (14).
This causes the SHEMmodel to estimate the number and type of thehidden emotions with a lower performance ascompared to BHEM, although the performancesof SHEM and BHEM are comparable as ex-plained in Section 7.1.Evaluation of IQAPs InferenceTo apply our PSSS on IQAPs inference task, weuse it as the sentiment similarity measure in thealgorithm explained in Figure 4.
The results arepresented in Table 4.
The first and second rowsare baselines.
The first row is the result obtainedby Marneffe et al (2010) approach.
This ap-proach is based on the similarity between the SOof the adjectives in question and answer.
Thesecond row of Table 4 show the results of using apopular semantic similarity measure, PMI, as thesentiment similarity (SS) measure in Figure 4.988Method Prec.
Rec.
F1Marneffe et al (2010) 60.00 60.00 60.00PMI 60.61 58.70 59.64PSSS-SHEM  62.55 61.75 61.71PSSS-BHEM (w/o WSD) 65.90 66.11 63.74SS-BHEM (with WSD) 66.95 67.15 65.66Table 4.
Performance on IQAP inference taskThe result shows that PMI is less effective tocapture the sentiment similarity.Our PSSS approach directly infers yes or noresponses using SS between the adjectives anddoes not require computing SO of the adjectives.In Table 4, PSSS-SHEM and PSSS-BHEM indi-cate the results when we use our PSSS withSHEM and BHEM respectively.
Table 4 showsthe effectiveness of our sentiment similaritymeasure.
Both models improve the performanceover the baselines, while the bridged model leadsto higher performance than the series model.Furthermore, we employ Word Sense Disam-biguation (WSD) to disambiguate the adjectivesin the question and its corresponding answer.
Forexample, Q: ?
Is that true?
A: This is extraor-dinary and preposterous.
In the answer, the cor-rect sense of the extraordinary is unusual and assuch answer no can be correctly inferred.
In thetable, (w/o WSD) is based on the first sense (mostcommon sense) of the words, whereas (withWSD) utilizes the real sense of the words.
AsTable 4 shows, WSD increases the performance.WSD could have higher effect, if more IQAPscontain adjectives with senses different from thefirst sense.7 Analysis and Discussions7.1 Number and Types of EmotionsIn our PSSS approach, there is no limitation onthe number and types of emotions as we assumedemotions are hidden.
In this Section, we performexperiments to predict the number and type ofhidden emotions.Figure 6 and 7 show the results of the hiddenmodels (SHEM and BHEM) on SO predictionand IQAPs inference tasks respectively with dif-ferent number of emotions.
As the Figures show,in both tasks, SHEM achieved high performanc-es with 11 emotions.
However, BHEM achievedhigh performances with six emotions.
Now, thequestion is which emotion number should beconsidered?
To answer this question, we furtherstudy the results as follows.First, for SHEM, there is no significant differ-ence between the performances with six and 11emotions in the SO prediction task.
This is theFigure 6.
Performance of BHEM and SHEM on SOprediction through different #of emotionsFigure 7.
Performance of BHEM and SHEM onIQAPs inference through different #of emotionssame for BHEM.
Also, the performances ofSHEM on the IQAP inference task with six and11 emotions are comparable.
However, there is asignificant difference between the performancesof BHEM in six and 11 emotions.
So, we consid-er the dimension in which both hidden emotionalmodels present a reasonable performance overboth tasks.
This dimension is six here.Second, as shown in the Figures 6 and 7, incontrast to BHEM, the performance of SHEMdoes not considerably change with differentnumber of emotions over both tasks.
This is be-cause, in SHEM, the emotional vectors of thewords are derived from the emotional vectors ofthe documents after the EM steps, see Equation(14).
However, in BHEM, the emotional vectorsare directly obtained from the EM steps.
Thus,the bridged model is more sensitive than seriesmodel to the number of emotions.
This couldindicate that the bridged model is more accuratethan the series model to estimate the number ofemotions.Therefore, based on the above discussion, theestimated number of emotions is six in our de-velopment dataset.
This number may vary usingdifferent development datasets.In addition to the number of emotions, theirtypes can also be interpreted using our approach.To achieve this aim, we sort the words based ontheir probability values, P(w|e), with respect to989Figure 8.
Effect of synonyms & antonyms in SO pre-diction task with different emotion numbers in BHEMEmotion#1 Emotion#2 Emotion#3excellent (1)magnificently(1)blessed (1)sublime (1)affirmation (1)tremendous (2)unimpressive (1)humorlessly (1)paltry (1)humiliating (1)uncreative (1)lackluster (1)disreputable (1)villian (1)onslaught (1)ugly (1)old (1)disrupt (1)Table 5.
Sample words in three emotionseach emotion.
Then, the type of the emotions canbe interpreted by observing the top k words ineach emotion.
For example, Table 5 shows thetop 6 words for three out of six emotions ob-tained for BHEM.
The numbers in parenthesesshow the sense of the words.
The correspondingemotions for these categories can be interpretedas "wonderful", "boring" and "disreputable", re-spectively.We also observed that, in SHEM with elevenemotion numbers, some of the emotion catego-ries have similar top k words such that they canbe merged to represent the same emotion.
Thus,it indicates that the BHEM is better than SHEMto estimates the number of emotions than SHEM.7.2 Effect of Synsets and AntonymsWe show the important effect of synsets and an-tonyms in computing the sentiment similarity ofwords.
For this purpose, we repeat the experi-ment for SO prediction by computing sentimentsimilarity of word pairs with and without usingsynonyms and antonyms.
Figure 8 shows theresults of obtained from BHEM.
As the Figureshown, the highest performance can be achievedwhen synonyms and antonyms are used, whilethe lowest performance is obtained without usingthem.
Note that, when the synonyms are notused, the entries of enriched matrix are computedusing P(wi|wj) instead of P(syn(wi)|syn(wj)) in theEquation (15).
Also, when the antonyms are notused, the Max(,) in Equation (18) is 0 and SS iscomputed using only correlation between words.The results show that synonyms can improvethe performance.
As Figure 8 shows, the twoFigure 9.
Effect of confidence values in SO predictionwith different emotion numbers in BHEMhighest performances are obtained when we usesynonyms and the two lowest performances areachieved when we don't use synonyms.
This isindicates that the synsets of the words can im-prove the quality of the enriched matrix.
The re-sults also show that the antonyms can improvethe result (compare WOSynWAnt withWOSynWOAnt).
However, synonyms lead togreater improvement than antonyms (compareWSynWOAnt with WOSynWAnt).7.3 Effect of Confidence ValueIn Section 3.1, we defined a confidence value foreach word to improve the quality of the enrichedmatrix.
To illustrate the utility of the confidencevalue, we repeat the experiment for SO predic-tion by BHEM using all the words appears inenriched matrix with different confidencethresholds.
The results are shown in Figure 9,"w/o confidence" shows the results when wedon?t use the confidence values, while "with con-fidence" shows the results when use the confi-dence values.
Also, "confidence>x" indicates theresults when we set al the confidence valuesmaller than x to 0.
The thresholding helps toeliminate the effect of low confident words.As Figure 9 shows, "w/o confidence" leads tothe lowest performance, while "with confidence"improves the performance with different numberof emotions.
The thresholding is also effective.For example, a threshold like 0.3 or 0.4 improvesthe performance.
However, if a large value (e.g.,0.6) is selected as threshold, the performancedecreases.
This is because a large threshold fil-ters a large number of words from enriched mod-el that decreases the effect of the enriched ma-trix.7.4 Convergence AnalysisThe PSSS approach is based on the EM algo-rithm for the BHEM (or SHEM) presented inTable 2.
This algorithm performs a predefined990number of iterations or until convergence.
Tostudy the convergence of the algorithm, we re-peat our experiments for SO prediction andIQAPs inference tasks using BHEM with differ-ent number of iterations.
Figure 10 shows thatafter the first 15 iterations the performance doesnot change dramatically and is nearly constantwhen more than 30 iterations are performed.
Thisshows that our algorithm will converge in lessthan 30 iterations for BHEM.
We observed thesame pattern in SHEM.7.5 Bridged Vs. Series ModelThe bridged and series models are both based onthe hidden emotions that were developed to pre-dict the sense sentiment similarity.
Althoughtheir best results on the SO prediction and IQAPsinference tasks are comparable, they have somesignificant differences as follows:?
BHEM is considerably faster than SHEM.
Thereason is that, the input matrix of BHEM (i.e.,W?R) is significantly smaller than the inputmatrix of SHEM (i.e., W?D).?
In BHEM, the emotional vectors are directlycomputed from the EM steps.
However, theemotional vector of a word in SHEM is com-puted using the emotional vectors of the doc-uments containing the word.
This adds noisesto the emotional vectors of the words.?
BHEM gives more accurate estimation overtype and number of emotions versus SHEM.The reason is explained in Section 7.1.8 Related WorksSentiment similarity has not received enoughattention to date.
Most previous works employedsemantic similarity of word pairs to address SOprediction and IQAP inference tasks.
Turney andLittman (2003) proposed to compute pair-wisedmutual information (PMI) between a target wordand a set of seed positive and negative words toinfer the SO of the target word.
They also uti-lized Latent Semantic Analysis (LSA) (Landaueret al, 1998) as another semantic similarity meas-ure.
However, both PMI and LSA are semanticsimilarity measure.
Similarly, Hassan and Radev(2010) presented a graph-based method for pre-dicting SO of words.
They constructed a lexicalgraph where nodes are words and edges connecttwo words with semantic similarity obtainedfrom Wordnet (Fellbaum 1998).
They propagat-ed the SO of a set of seeds through this graph.However, such approaches did not take into ac-count the sentiment similarity between words.Figure 10.
Convergence of BHEMIn IQAPs, Marneffe et al (2010) inferred theyes/no answers using SO of the adjectives.
If SOof the adjectives have different signs, then theanswer conveys no, and Otherwise, if the abso-lute value of SO for the adjective in question issmaller than the absolute value of the adjective inanswer, then the answer conveys yes, and other-wise no.
In Mohtarami et al (2012), we used twosemantic similarity measures (PMI and LSA) forthe IQAP inference task.
We showed that meas-uring the sentiment similarities between the ad-jectives in question and answer leads to higherperformance as compared to semantic similaritymeasures.In Mohtarami et al (2012), we proposed anapproach to predict the sentiment similarity ofwords using their emotional vectors.
We as-sumed that the type and number of emotions arepre-defined and our approach was based on thisassumption.
However, in previous research, thereis little agreement about the number and types ofbasic emotions.
Furthermore, the emotions indifferent dataset can be varied.
We relaxed thisassumption in Mohtarami et al, (2013) by con-sidering the emotions as hidden and presented ahidden emotional model called SHEM.
This pa-per also consider the emotions as hidden and pre-sents another hidden emotional model calledBHEM that gives more accurate estimation ofthe numbers and types of the hidden emotions.9 ConclusionWe propose a probabilistic approach to infer thesentiment similarity between word senses withrespect to automatically learned hidden emo-tions.
We propose to utilize the correlations be-tween reviews, ratings, and words to learn thehidden emotions.
We show the effectiveness ofour method in two NLP tasks.
Experiments showthat our sentiment similarity models lead to ef-fective emotional vector construction and signif-icantly outperform semantic similarity measuresfor the two NLP task.991ReferencesHadi Amiri and Tat S. Chua.
2012.
Mining Slangand Urban Opinion Words and Phrases fromcQA Services: An Optimization Approach.Proceedings of the fifth ACM international confer-ence on Web search and data mining (WSDM).
Pp.193-202.Christiane Fellbaum.
1998.
WordNet: An Electron-ic Lexical Database.
Cambridge, MA: MITPress.Ahmed Hassan and Dragomir Radev.
2010.
Identify-ing Text Polarity Using Random Walks.
Pro-ceeding in the Association for Computational Lin-guistics (ACL).
Pp: 395?403.Aminul Islam and Diana Inkpen.
2008.
Semantic textsimilarity using corpus-based word similarityand string similarity.
ACM Transactions onKnowledge Discovery from Data (TKDD).Carroll E. Izard.
1971.
The face of emotion.
NewYork: Appleton-Century-Crofts.Soo M. Kim and Eduard Hovy.
2004.
Determiningthe sentiment of opinions.
Proceeding of theConference on Computational Linguistics(COLING).
Pp: 1367?1373.Thomas K. Landauer, Peter W. Foltz, and DarrellLaham.
1998.
Introduction to Latent SemanticAnalysis.
Discourse Processes.
Pp: 259-284.Andrew L. Maas, Raymond E. Daly, Peter T. Pham,Dan Huang, Andrew Y. Ng, and Christopher Potts.2011.
Learning Word Vectors for SentimentAnalysis.
Proceeding in the Association for Com-putational Linguistics (ACL).
Pp:142-150.Marie-Catherine D. Marneffe, Christopher D. Man-ning, and Christopher Potts.
2010.
"Was it good?It was provocative."
Learning the meaning ofscalar adjectives.
Proceeding in the Associationfor Computational Linguistics (ACL).
Pp: 167?176.Mitra Mohtarami, Hadi Amiri, Man Lan, Thanh P.Tran, and Chew L. Tan.
2012.
Sense SentimentSimilarity: An Analysis.
Proceeding of the Con-ference on Artificial Intelligence (AAAI).Mitra Mohtarami, Man Lan, and Chew L. Tan.
2013.From Semantic to Emotional Space in Proba-bilistic Sense Sentiment Analysis.
Proceeding ofthe Conference on Artificial Intelligence (AAAI).Alena Neviarouskaya, Helmut Prendinger, andMitsuru Ishizuka.
2007.
Textual Affect Sensingfor Sociable and Expressive Online Communi-cation.
Proceedings of the conference on AffectiveComputing and Intelligent Interaction (ACII).
Pp:218-229.Alena Neviarouskaya, Helmut Prendinger, andMitsuru Ishizuka.
2009.
SentiFul: Generating aReliable Lexicon for Sentiment Analysis.
Pro-ceeding of the conference on Affective Computingand Intelligent Interaction (ACII).
Pp: 363-368.Andrew Ortony and Terence J. Turner.
1990.
What'sBasic About Basic Emotions.
American Psycho-logical Association.
97(3), 315-331.Christopher Potts, C. 2011.
On the negativity ofnegation.
In Nan Li and David Lutz, eds., Pro-ceedings of Semantics and Linguistic Theory 20,636-659.Peter D. Turney and Michael L. Littman.
2003.Measuring Praise and Criticism: Inference ofSemantic Orientation from Association.
ACMTransactions on Information Systems, 21(4), 315?346.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity inphrase-level sentiment analysis.
Proceeding inHLT-EMNLP.
Pp: 347?354.992
