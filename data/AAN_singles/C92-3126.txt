A COMPUTATIONAL MODEL OF LANGUAGEDATA ORIENTED PARSINGRENS BOlt*Department of Computational I JnguisticsUniversity of AmsterdmnSpuistraat 1341012 VII AmsterdamThe Netherlandsrens@alf.let.uva.nlPERFORMANCE:Abstract1)ata Oriented Parsing (IX)P) is a model where noabstract rules, but language xt~riences in the ti3ru~ of all,'malyzed COlpUS, constitute the basis for langnageprocessing.
Analyzing a new input means that thesystem attempts to find tile most probable way toreconstruct the input out of frugments that alr"c~y existill the corpus.
Disambiguation occurs as a side-effect.DOP can be implemented by using colivelllional parsingstrategies.In~oducfionThis paper tommlizes the model for natural Imlgnageintroduced m \[Sclm 199o\].
Since that article is writtenin Dutch, we will translate Some parts of it more or lessliterally in this introduction.
According to Scba, thecurrent radition of language processing systems is basedon linguistically motivated competence models ofnatural Imlguages.
"llte problems that these systems lulliato, suggest file necessity of a more perfommnceoriented model of language processing, that takes intoaccount the statistical properties of real language use.qllerefore Scha proposes a system ritat makes use of anannotated corpus.
AnMyzing a new input means that thesystem attempts to find the most probable way toreconstruct the input out of fragments that already existin the corpus.The problems with competence grammars thatare mentioned in Scha's aiticle, include the explosion ofambiguities, the fact tilat Itunmn judgemeats ongrammaticality are not stable, that competence granunarsdo not account for language h~alge, alld that no existingrule-based grammar gives a descriptively 'adequatecharacterization of an actual language.
According toScha, tile deveh,pment of a fornml gnatunar fur naturallatlguage gets more difficult ,as tire grammar gets larger.When the number of phenotnena one has already takeainto account gets larger, the number of iareractions thatmust be considered when ,me tries to introduce allaccount of a new pllenomenon grows accordingly.As to tile problem of ,'mtbiguity, it has turnedout that as soon as a formal gratmnar clmracterizes anon-trivial part of a natural anguage, almost every inputsentence of reasonable length gets ml re\]manageablylarge number of different structural analyses (and* The author wishes to thank his colleagues at the Departmentof Computational Linguistics of the Ilaiversity of Amsterdamfor many fruitful discussions, and, in particular, Remko Scha,Martin van den Berg, Kwee Tjoe l,iong and Frodenk Somsenfor valuable comments on earlier w~'rsions of this paper.semantical interpretations).
I "lids is problenmtic sincemost of these interpretations ~re not perceived aslVossible by a hunmn language user, while there are nosystematic reasons 111 exclude tileln on syutactic orsematltic grounds.
Often it is just a ntatter of relativeimplausibility: tile only reason why a certainiarerpmtarion of a sentence is not perceived, is thataanther interprctatilm is much more plausible.Competence and Performance'tale lhnriations of the current language procossingsysterus are not suprising: riley are the directconsequence of rile fact that these systems implementChart\]sky's notion of a coutpetence grmnmar.
Theformal grilnuuars that constitute the subject-nmtter oftheoretieal linguistics, aim at characterizing theclnnpetencc of tile langnage user.
But the preferenceslanguage users have m the case of ambiguous entences,are paradigm instances of perfonatmce phenomena.In order to build effective lauguage processingsystems we nmst intplement performanec-grammars,rather than competence gratumars, qlaese performancegranmuus houM not only contain information on thestructural possibilities of file general I~mgnage system,but also on details of actual language use in a languageconmmnity, and of tile language experiences of anindividual, which cause this individual to have certainexpectations on what kinds of uUerances are going tooccur, and what slractures and interpretations theseutterances are going to have.Therc is all alternative linguistic tradition tluathas always focused on the concrete details of actuallanguage use: file statistical tradition.
In this approach,syntactic structure is usually ignored; only 'superficial'stalistical properties of a large coqms are described: fileprobability that a certain word is followed by a certainother word, the probability that a certain sequence oftwo words is followed by a ce~ml word, etc.
(Markov-cludns, see e.g.
\[Bahl 1983\]).
This approach busperforumd succesfully ill certain practical tasks, such ,asselecting the most probable sentence from the outputs ofa speech recognition coruptment.
It will be clear that thisapproach is not suitable for mmly other tasks, becauseno uotion of syntactic structme is used.
Aud there arestatistical dependencies within the sentences of a corpus,that cam extend over all arbitrarily long sequence ofwords; this is ignored by file Markov-approach.
Thechallenge is now to develop a theory of languageprocesslag that does justice to tile statistieM ,as well asto tile structural aspects of langange.1 In \[Martin 19791 it is reported that their t~ser generated 455different lxuses for tile sentence "lAst the sales of productsproduced in 1973 with the products produced in 1972".ACRES DE COLING-92, NANTES, 23-28 no(tr 1992 8 5 5 PROC.
OV COLING-92.
NAN rES, AUG. 23-28, 1992The Synthesis of Syntax and StatisticsThe idea that a synthesis between syntactic andstatistical approaches could be useful has incidentallybeen proposed before, but has not been worked out verywell so far.
The only technical elaboration of this ideathat exists at the moment, the notion of a probabilisdcgtamnmr, is of a rather simplistic nature.
A probabilisticgrammar is simply a juxtaposition of the mostfundamental syntactic notion and the most fundamentalstatistical notion: it is an "old-fashioned" context freegrammar, that describes syntactic structures by means ofa set of abstract rewrite rules that are now provided withprobabilities that correspond to the application-probabilities of the rules (see e.g.
\[Jeliuek 1990\]).As long as a probabilistic grammar onlyassigns probabilities to individual rewrite rules, thegrammar cannot account for all statistical properties of alanguage corpus.
It is, for instance, not possible toindicate how the probability of syntactic structures orlexical items depends on their syntacticflexical context.As a consequence of this, it is not possible to recognizefrequent phrases and figures of speech as such - adisappointing property, for one would prefer that suchphrases and figures of speech would get a high priorityin the ranking of the possible syntactic analyses of asentence.
Some improvements can be made by applyingthe Markov-approach to rewrite rules, as is found in thework of \[Salomaa 1969\] and \[Magerman 1991\].Nevertheless, any approach which ties probabilities torewrite rules will never be able to acconunodate allstatistical dependencies.
Optimal statistical estimationscan only be achieved if tile statistics are applied todifferent kinds of units than rewrite rules.
It isinteresting to note that also in the field of theoreticallinguistics tile necessity to use other kinds of structuralunits has been put forward.
The clearest articulation ofthis idea is found in the work of \[Fillmore 1988\].From a linguistic point of view thatemphasizes the syntactic complexities caused byidiomatic and semi-idiomatic expressions, Fillmore etal.
arrive at the proposal to describe language not bymeans of a set of rewrite rules, but by meaus of a set ofconstructions.
A construction is a tree-strncture: afragment of a constituent-structure that can comprisemore than one level.
This tree is labeled with syntactic,semantic and pragnmtic ategories and feature-values.Lexical items can be specified as part of a construction.Constructions can be idiomatic in nature: the meaningof a larger constituent can be specified without beingconstructed front the meanings of its sub-constituents.Fillmore's ideas still show the influence of thetradition of formal grammars: the constructions areschemata, and the combinatorics of putting theconstructions together looks very much like a contextfree gramnmr.
But the way in which Filhnoregeneralizes the notion of grmnmar resolves the problemswe found in the current statistical grammars: if aconstrnction-granunar is combined with statisticalnotions it is perhaps possible to represent all statisticalinformation.
This is one of the central ideas behind ourapproach.A New Approach: Data OrientedParsingThe starting-point of our approach is the idea indicatedabove, that when a human language user analyzessentences, there is a strong preference for the recognitionof sentences, constituents and patterns that occurredbefore in the experience of the language user.
There is astatistical component in language processing that prefersmore frequent structures and interpretations to lessfrequently perceived alternatives.The information we ideally would like to usein order to model the language performance uf a naturallanguage user, comprises therefore an enumeration f alllexical items and syntactic/semantic structures everexperieaced by the language user, with their frequency ofoccurrence.
In practice this means: a very large corpus ofsentences with their syntactic analyses and semanticinterpretatious.
Every senteace comprises a large numberof constructions: not only the whole sentence and all itsconstituents, but also the patterns that can be abstractedfrom the analyzed sentence by introducing 'free variables'for lexical elements or complex constituents.Parsing then does not happen by applyinggrammatical rules to rite input sentence, but byconstructing an optinml analogy between the inputsentence and as many corpus sentences ,as possible.Sometimes the system shall need to abstract away frommost of the properties of the trees in the corpus, andsometimes a part of tile input is found literally in thecorpus, and can be treated as one unit in the parsingprocess.
Thus the system tries to combine constructionsfrom the corpus so as to reconstruct the input sentenceas 'well' as possible.
~llte preferred parse out of all parsesof the input sentence is obtained by maximizing fileconditional probability of a parse given the sentence.Finally, the preferred parse is added to the corpus,bringing it into a new 'state'.To illustrate the basic idea, consider thefollowing extremely simple exmnple.
Assume that thewhole corpus consists of only the following two trees:SA M:' VPA Wa V NP .~.
ppw' i'/x - i l i lcoting'92 P Pr opec~d Pr in JudyI I Iin Nantes coling'90Then the input sentence who opened cohng '92 in Nantesin July can be analyzed as an S by combining thefollowing constructions from file corpus:+ +NP VP Pr Pr PPV1?
PP who coling~2 P Pr A A  !
jV NP P Pr i los1 I Iopened in JulyACRES DE COLING-92?
NANTES, 23-28 AOL'r 1992 8 5 6 I)ROC.
(71: COLING-92, NANTES.
AUG. 23-28, 1992The ModelIn order to come to fomml definitions of p,'u'se andprefettedparse we tirst specify some basic notions.LabelsWe distinguish between file set of lexical l,lbels L andthe ~t  of non-lexical labels IV.
Lexical labels representwords.
Non-lexical l',fl~els represent syi~tactic and/orsemantic mid/or i)ragnlalie infonnatiou, depending onfile kind of corpns being used.
We write J~ for l , u l~SUingGiven a set of hlbcls ~,  a string is all u-tuple ofelements of ~:  (LI,...,L n) ~ ~u.
All input string is mlnquple of elements of L: (l,t,._,Ln) ~ I, n. ACollckttellatio\[l ~ Gill big defined OI( sllil(gS US usual:(;l,...,b),~(c,...,ll) = (a,...,b,c,...,d).TreeGiven a set of labels J~, the set of trees is defined as tilesnmllest set Tree sucl~ that(1) i f I ,~ ,  then (l,,O)~Tree(2) i f  L6"~, tl,..,,tneTi'ce., then (l,,(ll,...,tn))eT~eeFor a set of trees 77cc over a ~t  of labels ~, we define afunction root.
~/i-ee-9~ mid a tuuction le;tves: ~l?ee~L nbyfor n_>O, root((L,(tl,...,tn))) = I,rot n>O, le,~ves((L,(tt,...,l~t))) ~ l?,'lves(tl)*...
~le~lves(tn)torn--O, leaves((L,O)) = (L)CorpusA corpus C is a multiset of trees, ill file ~nse  that allytree can  occur zero, nile or more times.
'File lt~tves ofevery tree in a corpus is ml element of Ln: it consfimtesthe string of wo(ds of which that tree is the amdysis thatseemed most appropriate for understanding tile striug illthe context in which it was uttered.Construction8Ill order to define the Constowtions of a tree, we needtwo additional notions: Subffees and l~tttems,Snbtrees((L,(tl,...,t~))) =n\[(L,(tl,..,t~))} u (~ Snhtrees?ll))i=~Pattems((L,(t 1,..., In))) ={(L,O) 1 ty {(l,(ul,...,no.)
) / Vi~11,,l: nid~attenls(ti)lConstructions(T) = {t / 3beSubtrecs(1): teP,'tttenls(u)}We Slulll use tile lbllowing notation for a constnlction ofa tree in a corpus: tee =tier ~nc()" tc(.imstmctionsO0.Example: consider tree T. qhe trees T 1 and T 2 m~conslnletions of T, while '\['3 is not.T S TI  "T T VP PP VP PPI v  , I / xop~wwwi N ~ Ju~/vi a poT 3~N,~vp pp/pCompos i t ionIf t and u are trees, such Ilmt tile le\[tmost non-lcxic;llleMof t is equal to the mot of n, then tou is the tree thatresults from substituting this leaf in t by tree u. Thei)mtial function o:'l~eexTree-47ivc is called ~mlposJtion.We will write (toU)ov ;Ls touov, and ill general(..((tloQ)o(~)o..)otn as tl~t2o(~o...otn.Exmuple:v t~ vp Np vpT VP pp t~a:l VP PPN Prtr0 he IPalp~eTree 7' is a par~ of input slring s with respect o C, iffleaves(7) = s and there m'e constructions tl,-.,tn e (~,such that 1' = tto...ot n. A tuple (fl,...,t n) of suchconstructions i  said to generate par~ To f  s. Note thatdifferent tuples of constructions Gm generate the .,vanteparse.
The set of par~s of s will( respect to C,P,'use(s,C), is given byI','use(S,C) =(1 eTive / lcaves(T)=s A 3tl,...,t .
e C" ?-t lo.
.
:tn\]"File set of tuples of C(nlstructions that generate a parse7; "lbples(F,C), is given byluplcs('L(O = \[(tl,...,t~p / tl,...,t n ~" C A tlo...otn=T }ProbabilityAll input string can have several parses and every suchparse can be generated by ~veral  different c()mbinations()f COllstruclious lrOlll tile corpus.
What we are interestedin, is, given an input string s, tile probability thatarbiffury conlbinations of COllSIxuctions fro((I tile colpusgenerate a celtain prose 25 of s. Thus we are interested illtile colldJtkmal prolXlbility of a pm'se 1)given s, with asprobability space tile set of constructions of O'ees in thecorpus.l,et '/~ be a parse of iupet string s, and supl~)setimt 15 can exhaustively be generated by k tuples ofconstructions: 1iqges(15,C) = ((tl l,..,thn),(t21,..,12n2) .
.
.
.
.
(tkh..,tknt)}.
Thell 7) occurs ill"(tll,...,tlnl) or (t21,...,ten 2) or .... or (Ikl,,.,tknk)occur, aud (thl,...,tlmt) (~culs iff thl and th2 and ....ACrl!s ol.
: COLING-92.
NAN rEs.
23-28 AOt~:f 1992 8 5 7 l)mlc.
OF COI,ING-92, NANTES, AU?}.
23-28.
I992and t/mh Occur (hall,k\]).
Thus the probability of Ti isgiven byP(T i) = P( (t l l r%.r3t lm) u .... ~ (tglc3...r~tknvJk ~pIn shortened form: P(Ti) = P (u  (el tlxl)p=l q=lTile events tpq are no__L mutually exclusive, sinceconslructions can overlap, and can include otherconstructions.
The formula for tim joint probability ofevents E i is given by:n nP(,'3 E i) = 11 l'OSilEi_l.,.h'l)i=l i=lTile formula for the probability of combination of eventsE i (that are not independent) is given by (see e.g.
\[Harris1966\]):kP(L/ Ei) = X P(Ei) - X l'(ldi1~L'i2) + X P(h'it,'~Ei2~Ei3)i=l i i1<i2 i1<i2.~i3- .... +/- P (E I~E2~ ... c~lS k)We will use Bayes' decomposition formula toderive the conditional probability of "1) given s. Let 7/~and Tj be parses of s; the conditional probability of T igiven s, is illen given by:P(Ti)P(sFI" i) P(r)P(srl~)V(7)ts)  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.P(s) z~j P(Tj)P(slTj)Since P(slTj) is 1 for all j, we may writeP(T)P(Tils ) .
.
.
.
.
.
.
.
.
.
.~ p<rj)A parse 1)of  s with nmxinml conditionalprobability P(Tils) is called a preferred parse of s.ImplementationSeveral different implementations of DOP are possible.In \[Scholtes 1992\] a neural net implementation f DOPis proposed, ltere we will show that conventional rule-based parsing strategies can be applied tn DOP, byconverting constructions into rules.
A construction Canbe seen as a production rule, where the lefthand-side ofthe rule is constituted by the root of rite constructionand the righthand-side is constituted by the leaves of theconstruction.
The only exmt condition is that of everysuch rule its corresponding construction should beremembered in order to generate a parse-tree for the inputstring (by composing the constructions that correspondto the rules ilmt are applied).
For a construction t, thecorresponding production rule is given byroot(t) ~ leaves(OIn order to calculate the pteterredparse of an input stringby maximizing the conditional probability, all parseswith all possible tuples of constructions must begenerated, which becomes highly inefficient.
Often weare not interested in all parses of an alnbiguous inputstring, neither in their exact probabilities, but only inwhich parse is the preferred parse.
Thus we would liketo have a strategy fllat estimates file top of theprobability hierarchy of parses.
"llais call be achieved byusing Monte Carlo techniques (see e.g.
\[Hammersley1964\]): we estimate the preferred parse by taking randomsamples frotn the space of possibilities.
This will giveus a more effective approach dian exhaustivelycalculating the probabilities.DiscussionAlthough DOP has not yet been tested thoroughly 2, wecall already predict sonic of its capabilities.
In DOP, theprobability of a parse depends on all tuples ofcoustructious that generate that parse.
~lhe more differentways in which a parse can be generated, the lligher itsprobability.
This implies that a parse which can (also)be generated by relatively large constructions is favouredover a parse which can only be generated by relativelysmall constructions.
This means that prepositiotmlplu'ase attxichments arid figures of speech can beprocessed adequately by I)OP.As 1o the problem of hmguage acquisition, thisntight seem problematic for DOP: with all "alreadyanalyzed corpus, only adult language behaviour can besimulated.
The problem of language acquisition is ittour perspective the problem of the acquisition of aninitial corpus, in which non-linguistic input andpragmatics should play na important role.An additional remark should be devoted here toformal granlmars and disambiguation.
Much work hasbeen done to extend rule-based granunars withselectional restrictions such that the explosion ofambiguities is constrabled considerably, llowever, torepresent semantic and pragmatic onstraints i a veryexpensive task.
No one has ever succeeded in doing soexcept in relatively small grammars.
Furthermore, abasic question renmins as to whether it is possible toformally etlcode all of die syntactic, semantic alldpragmatic infomlation needed for disambiguation.
InDOP, the additional infornmtion that one can draw froma corpus of hand-marked structural annotations i thatone can by-pass the necessity for modelling worldknowledge, since this will autonmtically enter into thedisarnbiguation of structures by Imnd.
Extractingconstructions from these structures, and combining themin the most probable way, taking into account allpossible statistical dependencies between them,preserves this world knowledge in the best possibleway.In conclusion, it may be interesting to note thatour idea of using past lallguage xperiences instead ofrules, has much in cormnon with Stich's ideas aboutlanguage (\[Stich 1971\]).
lu Stich's view, judgements ofgralnmaticality are not determined by applying aprecompiled set of gratmuar rules, but rather have thecharacter of a perceptual judgement on the question towhat extent rite judged sentonce 'lotiks like' thesentences the language user has in his head as examplesof granlmaticality.
The cot)crete language xperiences offile past of a language user determine how a newutterance is processed; there is no evidence for fileassumption that past language experiences aregeneralized into a consistent heory that defines the2 Corpora that will be used to lust DOP, mcude tile ToscaCorpus, built at the University of Nijmugen, and possibly thePenn Trcebm~k, built at the Umversity of Pennsylvania.AcrEs Dr: COLING-92.
NANTES.
23-28 AOt',q" 1992 8 S 8 PROC.
OF COLING-92, NANTES, Aunt.
23-28.
1992grammaticality and the structure of new utterancesunivocally.References\[Bahl 1983\]: Bahl, L., Jelinek, F. and Mercer, R., 'AMaximum Likelihood Approach to Continuous SpeechRecognition', in: /EEE Transactions on Pattern Analysisand Machine Intelligence, Vol.
PAMI-5, No.2.\[Fillmore 1988\] Fillmore, C., Kay, P. mid O'Connor,M., 'Regularity and idiomaticity in grammaticalconstructions: the ease of let alne', L,'mguage 64, p.501-538.\[Hanmlersley 1964\]:  Hauunersley, J.M.
andtlandscomb, D.C., Monte C~lo Methods, Chapnumand Hall, London.\[Hams 1966\]: Harris, 11, lbeory of Probability,Addison-Wesley, Reading (Mass).\[Jelinek 1990\]: Jelinek, F., l.afferty, J.D.
and Mercer,R.I,., B~ic Methods of  Probabilistic Context FreeGranmuws, Yorktown tleights: IBM RC 16374(#72684).\[Magerman 1991\]: Magemmn, D. and Marcus, M.,'Pearl: A Probabilistic Chart P~u'ser',in: Proceedings ofthe European Chapter of the ACL'91, Berlin.\[Martin 1979\]: M,'min, W.A., Preliminary analysis of abre.adth-tirst parsing algorithin: Theoretical ,andexperimental results (Technical Report No.
TR-261).MIT LCS.\[Salomaa 1969\]: Salomaa, A., 'Probabilistic andweighted grmnmars', in: lnfomJation and control 15, p.529-544,\[Scha 1990\]: Scha, R., 'Language Theory and LanguageTechnology; Competence and Perfomumce' (in Dutch),in: Q.A.M.
de Kort & G.L.J.
I,eerdam (eds.
),Computertoepassingen in de Ncerlandistiek, Almere:Landelijke Vereniging van Neerlandici.
(LVVN-jaarboek)\[Scholtes 1992\]: Scholtes, J. C. and Bloembergen, S.,'The Design of a Neural Data-Oriented Parsing 0DOP)System', Proceedings of  the Intonational JointConference on Neural Networks 1992, Baltimore.\[Stich 1971\]: Stich, S.P., 'What every speaker knows',in: Philosophical Review 80, p.476-496.ACRES DE COLING-92, NANTES, 23-28 AOt~l" 1992 8 5 9 PROC.
OF COLING-92, NANTES, AUG. 23-28, 1992
