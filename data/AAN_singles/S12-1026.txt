First Joint Conference on Lexical and Computational Semantics (*SEM), pages 180?188,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsUnsupervised Induction of a Syntax-Semantics LexiconUsing Iterative RefinementHagen Fu?rstenauCCLSColumbia UniversityNew York, NY, USAhagen@ccls.columbia.eduOwen RambowCCLSColumbia UniversityNew York, NY, USArambow@ccls.columbia.eduAbstractWe present a method for learning syntax-semantics mappings for verbs from unanno-tated corpora.
We learn linkings, i.e., map-pings from the syntactic arguments and ad-juncts of a verb to its semantic roles.
By learn-ing such linkings, we do not need to model in-dividual semantic roles independently of oneanother, and we can exploit the relation be-tween different mappings for the same verb,or between mappings for different verbs.
Wepresent an evaluation on a standard test set forsemantic role labeling.1 IntroductionA verb can have several ways of mapping its seman-tic arguments to syntax (?diathesis alternations?
):(1) a.
We increased the response rate with SHK.b.
SHK increased the response rate.c.
The response rate increased.The subject of increase can be the agent (1a), the in-strument (1b), or the theme (what is being increased)(1c).
Other verbs that show this pattern includebreak or melt.Much theoretical and lexicographic (descriptive)work has been devoted to determining how verbsmap their lexical predicate-argument structure tosyntactic arguments (Burzio, 1986; Levin, 1993).The last decades have seen a surge in activity onthe computational front, spurred in part by efforts toannotate large corpora for lexical semantics (Bakeret al, 1998; Palmer et al, 2005).
Initially, we haveseen computational efforts devoted to finding classesof verbs that share similar syntax-semantics map-pings from annotated and unannotated corpora (La-pata and Brew, 1999; Merlo and Stevenson, 2001).More recently, there has been an explosion of inter-est in semantic role labeling (with too many recentpublications to cite).In this paper, we explore learning syntax-semantics mappings for verbs from unannotated cor-pora.
We are specifically interested in learning link-ings.
A linking is a mapping for one verb from itssyntactic arguments and adjuncts to all of its se-mantic roles, so that individual semantic roles arenot modeled independently of one another and sothat we can exploit the relation between differentmappings for the same verb (as in (1) above), orbetween mappings for different verbs.
We there-fore follow Grenager and Manning (2006) in treat-ing linkings as first-class objects; however, we dif-fer from their work in two important respects.
First,we use semantic clustering of head words of argu-ments in an approach that resembles topic modeling,rather than directly modeling the subcategorizationof verbs with a distribution over words.
Second andmost importantly, we do not make any assumptionsabout the linkings, as do Grenager and Manning(2006).
They list a small set of rules from whichthey derive all linkings possible in their model; incontrast, we are able to learn any linking observedin the data.
Therefore, our approach is language-independent.
Grenager and Manning (2006) claimthat their rules represent ?a weak form of UniversalGrammar?, but their rules lack such common linkingoperations as the addition of an accusative reflex-ive for the unaccusative (Romance) or case mark-ing (many languages), and they include a specific(English) preposition.
We have no objection to us-ing linguistic knowledge, but we do not feel that wehave the empirical basis as of now to provide a setof Universal Grammar rules relevant for our task.180A complete syntax-semantics lexicon describeshow lexemes syntactically realize their semantic ar-guments, and provides selectional preferences onthese dependents.
Though rich lexical resources ex-ist (such as the PropBank rolesets, the FrameNet lex-icon, or VerbNet, which relates and extends thesesources), none of them is complete, not even for En-glish, on which most of the efforts have focused.However, if a complete syntax-semantics lexicondid exist, it would be an extremely useful resource:the task of shallow semantic parsing (semantic ar-gument detection and semantic role labeling) couldbe reduced to determining the best analysis accord-ing to this lexicon.
In fact, the learning model wepresent in this paper is itself a semantic role labelingmodel, since we can simply apply it to the data wewant to label semantically.This paper is a step towards the unsupervised in-duction of a complete syntax-semantics lexicon.
Wepresent a unified procedure for associating verbswith linkings and for associating the discovered se-mantic roles with selectional preferences.
As input,we assume a syntactic representation scheme and aparser which can produce syntactic representationsof unseen sentences in the chosen scheme reason-ably well, as well as unlabeled text.
We do not as-sume a specific theory of lexical semantics, nor aspecific set of semantic roles.
We induce a set oflinkings, which are mappings from semantic rolesymbols to syntactic functions.
We also induce alexicon, which associates a verb lemma with a dis-tribution over the linkings, and which associates thesematic role symbols with verb-specific selectionalpreferences (which are distributions over distribu-tions of words).
We evaluate on the task of semanticrole labeling using PropBank (Palmer et al, 2005)as a gold standard.We focus on semantic arguments, as they are de-fined specifically for each verb and thus have verb-specific mappings to syntactic arguments, whichmay further be subject to diathesis alternations.
Incontrast, semantic adjuncts (modifiers) apply (inprinciple) to all verbs, and do not participate indiathesis alternations.
For this reason, the Prop-Bank lexicon includes arguments but not adjunctsin its framesets.
The method we present in this pa-per is designed to find verb-specific arguments, andwe therefore take the results on semantic arguments(Argn) as our primary result.
On these, we achieve a20% F-measure error reduction over a high syntac-tic baseline (which maps each syntactic relation to asingle semantic argument).2 Related WorkAs mentioned above, our approach is most similarto that of Grenager and Manning (2006).
However,since their model uses hand-crafted rules, they areable to predict and evaluate against actual PropBankrole labels, whereas our approach has to be evaluatedin terms of clustering quality.The problem of unsupervised semantic role la-beling has recently attracted some attention (Langand Lapata, 2011a; Lang and Lapata, 2011b; Titovand Klementiev, 2012).
While the present papershares the general aim of inducing semantic roleclusters in an unsupervised way, it differs in treat-ing syntax-semantics linkings explicitly and model-ing predicate-specific distributions over them.Abend et al (2009) address the problem of un-supervised argument recognition, which we do notaddress in the present paper.
For the purpose ofbuilding a complete unsupervised semantic parser,a method such as theirs would be complementary toour work.3 ModelIn this section, we decribe a model that generatesarguments for a given predicate instance.
Specifi-cally, this generative model describes the probabilityof a given set of argument head words and associatedsyntactic functions in terms of underlying semanticroles, which are modelled as latent variables.
Thesemantic role labeling task is therefore framed as theinduction of these latent variables from the observeddata, which we assume to be preprocessed by a syn-tactic parser.The basic idea of our approach is to explicitlymodel linkings between the syntactic realizationsand the underlying semantic roles of the argumentsin a predicate-argument structure.
Since our modelof argument classification is completely unsuper-vised, we cannot assign familiar semantic role labelslike Agent or Instrument, but rather aim at inducingrole clusters, i.e., clusters of argument instances thatshare a semantic role.
For example, each of the three181instances of response rate in (1) should be assignedto the same cluster.
We assume a fixed maximumnumber R of semantic roles per predicate and for-mulate argument classification as the task of assign-ing each argument in a predicate-argument struc-ture to one of the numbered roles 1, .
.
.
, R. Suchan assignment can therefore be represented by anR-tuple, where each role position is either filledby one of the arguments or empty (denoted as ).We represent each argument by its head word andits syntactic function, i.e., the path of syntactic de-pendency relations leading to it from the predicate.In our example (1a), a possible assignment of ar-guments to semantic roles could therefore be rep-resented by a head word tuple (we, rate, ,SHK)and a corresponding tuple of syntactic functions(nsubj, dobj, , prep with), where for the sake of theexample we have chosen R = 4 and the third se-mantic role slot is empty.
Note that this orderedR-tuple thus represents a semantic labeling of theunordered set of arguments, which our model takesas input.
While in the case of a single predicate-argument structure the assignment of arguments toarbitrary semantic role numbers does not provideadditional information, its value lies in the con-sistent assignment of arguments to specific rolesacross instances of the same predicate.
For exam-ple, to be consistent with the assignment above, (1b)would have to be represented by (, rate, ,SHK)and (, dobj, , nsubj).To formulate a generative model of argument tu-ples, we separately consider the tuple of argumenthead words and the tuple of syntactic functions.
Thefollowing two subsections will address each of thesein turn.3.1 Selectional PreferencesThe probability of an argument in a certain semanticrole depends strongly on the selectional preferencesof the predicate with respect to this role.
In the con-text of our model, we therefore need to describe theprobability P (wr|p, r) of an argument head wordwrdepending on the predicate p and the role r. Insteadof directly modeling predicate- and role-specific dis-tributions over head words, however, we model se-lectional preferences as distributions ?p,r(c) over se-mantic word classes c = 1, .
.
.
, C (with C being afixed model parameter), each of which is in turn as-sociated with a distribution ?c(wr) over the vocab-ulary.
They are thus similar to topics in semantictopic models.
An advantage of this approach is thatsemantic word classes can be shared among differentpredicates, which facilitates their inference.
Techni-cally, the introduction of semantic word classes canbe seen as a factorization of the probability of theargument head P (wr|p, r) =?Cc=1 ?p,r(c)?c(wr).3.2 LinkingsAnother important factor for the assignment of ar-guments to semantic roles are their syntactic func-tions.
While in the preceding subsection we consid-ered selectional preferences for each semantic roleseparately (assuming their independence), the inter-dependence between syntactic functions is crucialand cannot be ignored: The assignment of an ar-gument does not depend solely on its own syntacticfunction, but on the whole subcategorization frameof the predicate-argument structure.
We thereforehave to model the probability of the whole tupley = (y1, .
.
.
, yR) of syntactic functions.We assume that for each predicate there is a rela-tively small number of ways in which it realizes itsarguments syntactically, i.e., in which semantic rolesare linked to syntactic functions.
These may corre-spond to alternations like those shown in (1).
Insteadof directly modeling the predicate-specific probabil-ity P (y|p), we consider predicate-specific distribu-tions ?p(l) over linkings l = (x1, .
.
.
, xR).
Such alinking then gives rise to the tuple y = (y1, .
.
.
, yR)by way of probability distributions P (yr|xr) =?xr(yr).
This allows us to keep the number of possi-ble linkings l per predicate relatively small (by set-ting ?p(l) = 0 for most l), and generate a wide vari-ety of syntactic function tuples y from them.3.3 Structure of the ModelFigure 1 presents our linking model.
For eachpredicate-argument structure in the corpus, it con-tains observable variables for the predicate p and theunordered set s of arguments, and further shows la-tent variables for the linking l and (for each role r)the semantic word class c, the head word w, and thesyntactic function y.The distributions ?p,r(c) and ?c(w) are drawnfrom Dirichlet priors with symmetric parameters ?and ?, respectively.
In the case of the linking dis-182wR Nplcys?
??
?Figure 1: Representation of our linking model as aBayesian network.
The nodes p and s are observed foreach of the N predicate-argument structures in the cor-pus.
The latent variables c, w, l, and y are inferred fromthe data along with their distributions ?, ?, ?, and ?.tribution ?p(l), we are faced with an exponentiallylarge space of possible linkings (considering a setG of syntactic functions, there are (|G| + 1)R pos-sible linkings).
This is both computationally prob-lematic and counter-intuitive.
We therefore maintaina global list L of permissible linkings and enforce?p(l) = 0 for all l /?
L. On the set L we then draw?p(l) from a Dirichlet prior with symmetric param-eter ?.
In Section 3.5, we will describe how the link-ing list L is iteratively induced from the data.We introduced the distribution ?x to allow for in-cidental changes when generating the tuple of syn-tactic functions out of the linking.
If this pro-cess were allowed to arbitrarily change any syntacticfunction in the linking, the linkings would be too un-constrained and not reflect the syntactic functions inthe corpus.
We therefore parameterize ?x in sucha way that the only allowed modifications are theaddition or removal of syntactic functions from thelinking, but no change from one syntactic functionto another.
We attain this by parameterizing ?x asfollows:?x(y) =???????????
? if x = y = 1?
?|G| if x =  and y ?
G1?
?x if x ?
G and y = ?x if x = y ?
G0 elseHere, G again denotes the set of all syntactic func-tions.
The parameter ? is drawn from a uniformprior on the interval [0.0, 1.0] and the |G| parame-ters ?x for x ?
G have uniform priors on [0.5, 1.0].This has the effect that no syntactic function canchange into another, that a syntactic function isnever more probable to disappear than to stay, andthat all syntactic functions are added with the sameprobability.
This last property will be important forthe iterative refinement process described in Sec-tion 3.5.3.4 TrainingIn this subsection, we describe how we train themodel described so far, assuming that we are givena fixed linking list L. The following subsection willaddress the problem of infering this list.
In Sec-tion 3.6, we will then describe how we apply thetrained model to infer semantic role assignments forgiven predicate-argument structures.To train the linking model, we apply a Gibbs sam-pling procedure to the latent variables shown in Fig-ure 1.
In each sampling iteration, we first samplethe values of the latent variables of each predicate-argument structure based on the current distribu-tions, and then the latent distributions based oncounts obtained over the corpus.
For each predicate-argument structure, we begin with a blocked sam-pling step, simultaneously drawing values for w andy, while summing out c. This gives usP (w, y|p, l, s) ?R?r=1?xr(yr)C?c=1?p,r(c)?c(wr)where we have omitted the factor P (s|w, y), whichis uniform as long as we assume that w and y in-deed represent permutations of the argument set s.To sample efficiently from this distribution, we pre-compute the inner sum (as a tensor contraction or,equivalently, R matrix multiplications).
We thenenumerate all permutations of the argument set andcompute their probabilities, defaulting to an approx-imative beam search procedure in cases where thespace of permutations is too large.Next, the linking l is sampled according toP (l|p, y) ?
P (l|p)P (y|l) = ?p(l)R?r=1?xr(yr)Since the space L of possible linkings is small, com-pletely enumerating the values of this distribution is183not a problem.After sampling the latent variables w, y, and l foreach corpus instance, we go on to apply Gibbs sam-pling to the latent distributions.
For example, for ?pwe obtainP (?p|p1, l1, .
.
.
, pN , lN ) ?
P (?p)N?i=1P (li|pi)?
Dir(?
)(?p) ?
?l?L[?p(l)]np(l) = Dir(~np + ?
)(?p)Here np(l) is the number of corpus instances withpredicate p and latent linking l, and ~np is the vectorof these counts for a fixed p, indexed by l. Hence,?p is drawn from the Dirichlet distribution parame-terized by this vector, smoothed in each componentby ?.In the same way, the sampling distributions for?p,r and ?c are determined as Dir(~np,r + ?)
andDir(~nc + ?
), where each ~np,r is a vector of counts1indexed by word classes c and each ~nc is a vectorof counts indexed by head words wr.
Similarly,we draw the parameter ? in the parameterizationof ?x from Beta(n(, ) + 1,?x?G n(, x) + 1)and approximate ?x by drawing ?x fromBeta (n(x, x) + 1, n(x, ) + 1) and redrawingit uniformly from [0.5, 1.0], if it is smaller than 0.5.In this context, n(x, y) refers to the number of timesthe syntactic relation x is turned into y, countedover all corpus instances and semantic roles.To test for convergence of the sampling process,we monitor the log-likelihood of the data.
For eachpredicate-argument structure with predicate pi andargument set si, we haveP (pi, si) ?
?lP (l|pi)P (si|l) ?
P (si|li)=?w,yP (w, y, si|li) =?w,y?siP (w, y|li) =: LiThe approximation is rather crude (replacing an ex-pected value by a single sample from P (l|pi)), butwe expect the errors to mostly cancel out over theinstances of the corpus.
The last sum ranges over allpairs (w, y) that represent permutations of the argu-ment set s, and this can be computed as a by-product1Since we do not sample c, we use pseudo-counts based onP (cr|p, r, wr) for each instance.of the sampling process of w and y.
We then com-pute L := log?Ni=1 Li =?Ni=1 logLi, and termi-nate the sampling process if L does not increase bymore than 0.1% over 5 iterations.3.5 Iterative Refinement of Possible LinkingsIn Section 3.3, we have addressed the problem ofthe exponentially large space of possible linkings byintroducing a subset L ?
GR from which linkingsmay be drawn.
We now need to clarify how this sub-set is determined.
In contrast to Grenager and Man-ning (2006), we do not want to use any linguisticintuitions or manual rules to specify this subset, butrather automatically infer it from the data, so that themodel stays agnostic to the language and paradigmof semantic roles.
We therefore adopt a strategy ofiterative refinement.We start with a very small set that only containsthe trivial linking (, .
.
.
, ) and one linking for eachof the R most frequent syntactic functions, placingthe most frequent one in the first slot, the second onein the second slot etc.
We then run Gibbs sampling.When it has converged in terms of log-likelihood,we add some new linkings to L. These new link-ings are inferred by inspecting the action of the stepfrom l to y in the generative model.
Here, a syntac-tic function may be added to or deleted from a link-ing.
If a particular syntactic function is frequentlyadded to some linking, then a corresponding linking,i.e., one featuring this syntactic function and thus notrequiring such a modification, seems to be missingfrom the set L. We therefore count for each link-ing l how often it is either reduced by the deletion ofany syntactic function or expanded by the additionof a syntactic function.
We then rank these modifi-cations in descending order and for each of them de-termine the semantic role slot in which the modifica-tion (deletion or addition) occured most frequently.By applying the modification to this slot, each of thelinkings gives rise to a new one.
We add the first a ofthose, skipping new linkings if they are duplicates ofthose we already have in the linking set.
We iteratethis procedure, alternating between Gibbs samplingto convergence and the addition of a new linkings.3.6 InferenceTo predict semantic roles for a given predicate andargument set, we maximize P (l, w, y|p, s).
If the184space of permutations is too large for exhaustiveenumeration, we apply a similar beam search pro-cedure as the one employed in training to approxi-mately maximize P (w, y|p, s, l) for each value of l.For efficiency, we do not marginalize over l. Thishas the potential of reducing prediction quality, aswe do not predict the most likely role assignment,but rather the most likely combination of role assign-ment and latent linking.In all experiments we averaged over 10 consec-utive samples of the latent distributions, at the endof the sampling process (i.e., when convergence hasbeen reached).4 Experimental SetupWe train and evaluate our linking model on the dataset produced for the CoNLL-08 Shared Task onJoint Parsing of Syntactic and Semantic Dependen-cies (Surdeanu et al, 2008), which is based on thePropBank corpus (Palmer et al, 2005).
This dataset includes part-of-speech tags, lemmatized tokens,and syntactic dependencies, which have been con-verted from the manual syntactic annotation of theunderlying Penn Treebank (Marcus et al, 1993).4.1 Data SetAs input to our model, we decided not to use the syn-tactic representation in the CoNLL-08 data set, butinstead to rely on Stanford Dependencies (de Marn-effe et al, 2006), which seem to facilitate seman-tic analysis.
We thus used the Stanford Parser2 toconvert the underlying phrase structure trees of thePenn Tree Bank into Stanford Dependencies.
In theresulting dependency analyses, the syntactic headword of a semantic role may differ from the syntactichead according to the provided syntax.
We thereforemapped the semantic role annotation onto the Stan-ford Dependency trees by identifying the tree nodethat covers the same set of tokens as the one markedin the CoNLL-08 data set.The focus of the present work is on the linkingbehavior and classification of semantic argumentsand not their identification.
The latter is a substan-tially different task, and likely to be best addressedby other approaches, such as that of (Abend et al,2version 1.6.8, available at http://nlp.stanford.edu/software/lex-parser.shtml2009).
We therefore use gold standard informationof the CoNLL-08 data set for identifying argumentsets as input to our model.
The task of our model isthen to classify these arguments into semantic roles.We train our model on a corpus consisting of thetraining and the test part of the CoNLL-08 data set,which is permissible since as a unsupervised systemour model does not make any use of the annotatedargument labels for training.
We test the model per-formance against the gold argument classification onthe test part.
For development purposes (both de-signing the model and tuning the parameters as de-scribed in Section 4.4), we train on the training anddevelopment part and test on the development part.4.2 Evaluation MeasuresAs explained above, our model does not predict spe-cific role labels, such as those annotated in Prop-Bank, but rather aims at clustering like argumentinstances together.
Since the (numbered) labels ofthese clusters are arbitrary, we cannot evaluate thepredictions of our model against the PropBank goldannotation directly.
We follow Lang and Lapata(2011b) in measuring the quality of our clusteringin terms of cluster purity and collocation instead.Cluster purity is a measure of the degree to whichthe predicted clusters meet the goal of containingonly instances with the same gold standard class la-bel.
Given predicted clusters C1, .
.
.
, CnC and goldclusters G1, .
.
.
, GnG over a set of n argument in-stances, it is defined asPu =1nnC?i=1maxj=1,...,nG|Ci ?Gj |Similarly, cluster collocation measures how well theclustering meets the goal of clustering all gold in-stances with the same label into a single predictedcluster, formally:Co =1nnG?j=1maxi=1,...,nC|Ci ?Gj |We determine purity and collocation separately foreach predicate type and then compute their micro-average, i.e., weighting each score by the number ofargument instances of this precidate.
Just as preci-sion and recall, purity and collocation stand in trade-off.
In the next section, we therefore report theirF1 score, i.e., their harmonic mean 2?Pu?CoPu+Co .1854.3 Syntactic BaselineWe compare the performance of our model with asimple syntactic baseline that assumes that semanticroles are identical with syntactic functions.
We fol-low Lang and Lapata (2011b) in clustering argumentinstances of each predicate by their syntactic func-tions.
We do not restrict the number of clusters perpredicate.
In contrast, Lang and Lapata (2011b) re-strict the number of clusters to 21, which is the num-ber of clusters their system generates.
We found thatthis reduces the baseline by 0.1% F1-score (Argn onthe development set, c.f.
Table 1).
If we reduce thenumber of clusters in the baseline to the number ofclusters in our system (7), the baseline is reduced byanother 0.8% F1-score.
These lower baselines aredue to lower purity values.
In general, we find that asmaller number of clusters results in lower F1 mea-sure for the baseline; the reported baseline thereforeis the strictest possible.4.4 Parameters and TuningFor all experiments, we fixed the number of seman-tic roles at R = 7.
This is the maximum size of theargument set over all instances of the data set andthus the lower limit for R. If R was set to a highervalue, the model would be able to account for thepossibility of a larger number of roles, out of whichnever more than 7 are expressed simultaneously.
Weleave such investigation to future work.
We set thesymmetric parameters for the Dirichlet distributionsto ?
= 1.0, ?
= 0.1, and ?
= 1.0.
This correspondsto uninformative uniform priors for ?p,r and ?p, anda prior encouraging a sparse lexical distribution ?c,similar as in topic models such as LDA (Blei et al,2003).The number C of word classes, the number a ofadditional linkings in each refinement of the linkingset L, and the number k of refinement steps weretuned on the development set.
We first fixed a = 10and trained models for C = 10, 20, .
.
.
, 100, per-forming 50 refinement steps.
The best F1 score wasobtained withC = 10 after k = 20 refinements (i.e.,with 200 linkings).
Next, we fixed these two param-eters and trained models for a = 5, 10, 15, 20, 25.Here, we confirmed an optimal value of a = 10.5 ResultsIn this section, we give quantitative results, compar-ing our system to the syntactic baseline in terms ofcluster purity and collocation, and a qualitative dis-cussion of some phenomena observed in the perfor-mance of the model.5.1 Quantitative ResultsTable 1 shows the results of applying our models tothe CoNLL-08 test with the parameter values tunedin Section 4.4.
For comparison, we also show re-sults on the development set.
The table is dividedinto three parts, one only considering semantic ar-guments (Argn), one considering adjuncts (ArgM),and one aggregating results over both kinds of Prop-Bank roles (Arg*).
It can be seen that our modelconsistently outperforms the syntactic baseline interms of collocation (by 10% on Argn, 3% on ArgM,and 8.2% overall).
In terms of purity, however, itfalls short of the baseline.
As mentioned above,there is a trade-off between purity and collocation.Compared to our model, which we run with a totalof 7 semantic role slots, the baseline predicts a largenumber of small argument clusters for each predi-cate, whereas our model tends to group argumentstogether based on selectional preferences.In terms of F1 score, our model outperforms thebaseline by 3.6% on Argn, which translates into arelative error reduction by 20%.
On adjuncts, onthe other hand, our model falls short of the base-line by almost 10% F1 score.
This indicates thatour approach based on explicit representations oflinkings is most suited to the classification of argu-ments rather than adjuncts, which do not participatein diathesis alternations and do therefore not profitas much from our explicit induction of linkings.5.2 Qualitative ObservationsAmong the verbs with at least 10 test instances, in-clude shows the largest gain in F1 score over thebaseline.
In the test corpus, we find an interestingpair of sentences for this predicate:(2) a. Mr. Herscu proceeded to launch an ambi-tious, but ill-fated, $1 billion acquisitionbinge that included Bonwit Teller and B.Altman & Co. [...]186Argn ArgM Arg*Test Set Pu Co F1 Pu Co F1 Pu Co F1Syntactic Baseline 90.6 75.4 82.3 87.0 73.3 79.6 88.0 74.9 80.9Linking Model 86.4 85.4 85.9 64.4 76.3 69.8 74.5 83.1 78.6Development Set Pu Co F1 Pu Co F1 Pu Co F1Syntactic Baseline 91.5 73.9 81.8 88.7 78.6 83.3 89.2 75.1 81.5Linking Model 85.6 84.4 85.0 67.7 79.9 73.3 75.2 83.2 79.0Table 1: Purity (Pu), collocation (Co), and F1 scores of our model and the syntactic baseline in percent.
Performanceon arguments (Argn), adjuncts (ArgM), and overall results (Arg*) are shown separately.b.
Not included in the bid are Bonwit Teller orB.
Altman & Co. [...]The first of these two sentences is generated from thelinking (nsubj, dobj, , , , , -rcmod), which doesnot need to be modified in any way to account for thesubject that (coreferent with the head of the pred-icate in the modifying relative clause, binge) andthe direct object Teller (head of the phrase BonwitTeller and B. Altman & Co.).
These are assignedto the first and second role slots, respectively.
Thesecond sentence, on the other hand, is generated outof the linking (prep in, nsubjpass, , , , , ).
Here,the passive subject Teller is assigned to the secondrole slot (which we may interpret as the Includee),while the first semantic role (the Includer) is labeledon bid, which is realized in a prepositional phraseheaded by the preposition in.
Note that this alter-nation is not the general passive alternation though,which would have led to Teller is not included by thebid.
Instead, the model learned a specific alternationpattern for the predicate include.But even where a specific linking has not beenlearned, the model can often still infer a correct la-beling by virtue of its selectional preference com-ponent.
In our corpus, the predicate give occursmostly with a direct and an indirect object as inCNN recently gave most employees raises of asmuch as 15%.
The model therefore learns a link-ing (nsubj, dobj, , , , , iobj), but fails to learn thatthe Beneficient role can also be expressed with thepreposition to as in(3) [...] only 25% give $2,500 or more to charityeach year.However, when applying our model to this sentence,it nonetheless assigns charity to the last role slot (thesame one previously occupied by the indirect ob-ject).
This is due to the fact that charity is a goodfit for the selectional preference of this role slot ofthe predicate give.6 ConclusionsWe have presented a novel generative model ofpredicate-argument structures that incorporates se-lectional preferences of argument heads and explic-itly describes linkings between semantic roles andsyntactic functions.
The model iteratively inducesa lexicon of possible linkings from unlabeled data.The trained model can be used to cluster given ar-gument instances according to their semantic roles,outperforming a competitive syntactic baseline.The approach is independent of any particular lan-guage or paradigm of semantic roles.
However, inits present form the model assumes that each predi-cate has its own set of semantic roles.
In formalismssuch as Frame Semantics (Baker et al, 1998), se-mantic roles generalize across semantically similarpredicates belonging to the same frame.
A naturalextension of our approach would therefore consist inmodeling predicate groups that share semantic rolesand selectional preferences.Acknowledgments.
This work was supported by the In-telligence Advanced Research Projects Activity (IARPA) viaDepartment of Interior National Business Center (DoI/NBC)contract number D11PC20153.
The U.S. Government is autho-rized to reproduce and distribute reprints for Governmental pur-poses notwithstanding any copyright annotation thereon.
Dis-claimer: The views and conclusions contained herein are thoseof the authors and should not be interpreted as necessarily rep-resenting the official policies or endorsements, either expressedor implied, of IARPA, DoI/NBC, or the U.S. Government.187ReferencesOmri Abend, Roi Reichart, and Ari Rappoport.
2009.Unsupervised argument identification for semanticrole labeling.
In Proceedings of the Joint Conferenceof the 47th Annual Meeting of the ACL and the 4thInternational Joint Conference on Natural LanguageProcessing of the AFNLP, pages 28?36, Singapore.Collin F. Baker, J. Fillmore, and John B. Lowe.
1998.The Berkeley FrameNet project.
In 36th Meetingof the Association for Computational Linguistics and17th International Conference on Computational Lin-guistics (COLING-ACL?98), pages 86?90, Montre?al.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alocation.
Journal of MachineLearning Research, 3:993?1022.Luigi Burzio.
1986.
Italian Syntax: A Government-Binding Approach.
Reidel, Dordrecht.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typed de-pendency parses from phrase structure parses.
In Pro-ceedings of LREC 2006.Trond Grenager and Christopher D. Manning.
2006.Unsupervised discovery of a statistical verb lexicon.In Proceedings of the 2006 Conference on EmpiricalMethods in Natural Language Processing, pages 1?8,Sydney, Australia.Joel Lang and Mirella Lapata.
2011a.
Unsupervised se-mantic role induction via split-merge clustering.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, pages 1117?1126, Portland, Ore-gon, USA.Joel Lang and Mirella Lapata.
2011b.
Unsupervised se-mantic role induction with graph partitioning.
In Pro-ceedings of the 2011 Conference on Empirical Meth-ods in Natural Language Processing, pages 1320?1331, Edinburgh, Scotland, UK.Maria Lapata and Chris Brew.
1999.
Using subcatego-rization to resolve verb class ambiguity.
In In Proceed-ings of Joint SIGDAT Conference on Empirical Meth-ods in Natural Language Processing and Very LargeCorpora, pages 266?-274, College Park, MD.Beth Levin.
1993.
English Verb Classes and Alterna-tions: A Preliminary Investigation.
The University ofChicago Press.Mitchell M. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a Large AnnotatedCorpus of English: The Penn Treebank.
Computa-tional Linguistics, 19.2:313?330, June.Paola Merlo and Suzanne Stevenson.
2001.
Automaticverb classification based on statistical distributions ofargument structure.
Computational Linguistics, 27(3).Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The Proposition Bank: An annotated corpus ofsemantic roles.
Computational Linguistics, 31(1):71?106.Mihai Surdeanu, Richard Johansson, Adam Meyers,Llu?
?s Ma`rquez, and Joakim Nivre.
2008.
The conll2008 shared task on joint parsing of syntactic and se-mantic dependencies.
In CoNLL 2008: Proceedingsof the Twelfth Conference on Computational Natu-ral Language Learning, pages 159?177, Manchester,England.Ivan Titov and Alexandre Klementiev.
2012.
A bayesianapproach to unsupervised semantic role induction.
InProceedings of the Conference of the European Chap-ter of the Association for Computational Linguistics,Avignon, France, April.188
