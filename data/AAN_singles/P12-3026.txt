Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 151?156,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsUWN: A Large Multilingual Lexical Knowledge BaseGerard de MeloICSI Berkeleydemelo@icsi.berkeley.eduGerhard WeikumMax Planck Institute for Informaticsweikum@mpi-inf.mpg.deAbstractWe present UWN, a large multilingual lexi-cal knowledge base that describes the mean-ings and relationships of words in over 200languages.
This paper explains how link pre-diction, information integration and taxonomyinduction methods have been used to buildUWN based on WordNet and extend it withmillions of named entities from Wikipedia.We additionally introduce extensions to coverlexical relationships, frame-semantic knowl-edge, and language data.
An online interfaceprovides human access to the data, while asoftware API enables applications to look upover 16 million words and names.1 IntroductionSemantic knowledge about words and named enti-ties is a fundamental building block both in vari-ous forms of language technology as well as in end-user applications.
Examples of the latter includeword processor thesauri, online dictionaries, ques-tion answering, and mobile services.
Finding se-mantically related words is vital for query expan-sion in information retrieval (Gong et al, 2005),database schema matching (Madhavan et al, 2001),sentiment analysis (Godbole et al, 2007), and ontol-ogy mapping (Jean-Mary and Kabuka, 2008).
Fur-ther uses of lexical knowledge include data cleaning(Kedad and M?tais, 2002), visual object recognition(Marsza?ek and Schmid, 2007), and biomedical dataanalysis (Rubin and others, 2006).Many of these applications have used English-language resources like WordNet (Fellbaum, 1998).However, a more multilingual resource equippedwith an easy-to-use API would not only enable us toperform all of the aforementioned tasks in additionallanguages, but also to explore cross-lingual applica-tions like cross-lingual IR (Etzioni et al, 2007) andmachine translation (Chatterjee et al, 2005).This paper describes a new API that makes lexicalknowledge about millions of items in over 200 lan-guages available to applications, and a correspond-ing online user interface for users to explore the data.We first describe link prediction techniques used tocreate the multilingual core of the knowledge basewith word sense information (Section 2).
We thenoutline techniques used to incorporate named enti-ties and specialized concepts (Section 3) and othertypes of knowledge (Section 4).
Finally, we describehow the information is made accessible via a user in-terface (Section 5) and a software API (Section 6).2 The UWN CoreUWN (de Melo and Weikum, 2009) is based onWordNet (Fellbaum, 1998), the most popular lexi-cal knowledge base for the English language.
Word-Net enumerates the senses of a word, providing ashort description text (gloss) and synonyms for eachmeaning.
Additionally, it describes relationships be-tween senses, e.g.
via the hyponymy/hypernymy re-lation that holds when one term like ?publication?
isa generalization of another term like ?journal?.This model can be generalized by allowing wordsin multiple languages to be associated with a mean-ing (without, of course, demanding every meaningbe lexicalized in every language).
In order to ac-complish this at a large scale, we automatically link151terms in different languages to the meanings alreadydefined in WordNet.
This transforms WordNet intoa multilingual lexical knowledge base that coversnot only English terms but hundreds of thousandsof terms from many different languages.Unfortunately, a straightforward translation runsinto major difficulties because of homonyms andsynonyms.
For example, a word like ?bat?
has 10senses in the English WordNet, but a German trans-lation like ?Fledermaus?
(the animal) only applies toa small subset of those senses (cf.
Figure 1).
Thischallenge can be approached by disambiguating us-ing machine learning techniques.Figure 1: Word sense ambiguityKnowledge Extraction An initial input knowl-edge base graph G0 is constructed by ex-tracting information from existing wordnets,translation dictionaries including Wiktionary(http://www.wiktionary.org), multilingual thesauriand ontologies, and parallel corpora.
Additionalheuristics are applied to increase the density of thegraph and merge near-duplicate statements.Link Prediction A sequence of knowledge graphsGi are iteratively derived by assessing paths froma new term x to an existing WordNet sense z viasome English translation y covered by WordNet.
Forinstance, the German ?Fledermaus?
has ?bat?
as atranslation and hence initially is tentatively linked toall senses of ?bat?
with a confidence of 0.
In eachiteration, the confidence values are then updated toreflect how likely it seems that those links are cor-rect.
The confidences are predicted using RBF-kernel SVM models that are learnt from a trainingset of labelled links between non-English words andsenses.
The feature space is constructed using a se-ries of graph-based statistical scores that representproperties of the previous graph Gi?1 and addition-ally make use of measures of semantic relatednessand corpus frequencies.
The most salient featuresxi(x, z) are of the form:?y??(x,Gi?1)?
(x, y) sim?x(y, z) (1)?y??(x,Gi?1)?
(x, y) sim?x(y, z)sim?x(y, z) + dissimx(y, z)(2)The formulae consider the out-neighbourhood y ??
(x,Gi?1) of x, i.e.
its translations, and then ob-serve how strongly each y is tied to z.
The functionsim?
computes the maximal similarity between anysense of y and the current sense z.
The dissim func-tion computes the sum of dissimilarities betweensenses of y and z, essentially quantifying how manyalternatives there are to z.
Additional weightingfunctions ?, ?
are used to bias scores towards sensesthat have an acceptable part-of-speech and sensesthat are more frequent in the SemCor corpus.Relying on multiple iterations allows us to drawon multilingual evidence for greater precision andrecall.
For instance, after linking the German ?Fled-ermaus?
to the animal sense of ?bat?, we may be ableto infer the same for the Turkish translation ?yarasa?.Results We have successfully applied these tech-niques to automatically create UWN, a large-scalemultilingual wordnet.
Evaluating random samplesof term-sense links, we find (with Wilson-score in-tervals at ?
= 0.05) that for French the preci-sion is 89.2% ?
3.4% (311 samples), for German85.9% ?
3.8% (321 samples), and for MandarinChinese 90.5% ?
3.3% (300 samples).
The over-all number of new term-sense links is 1,595,763, for822,212 terms in over 200 languages.
These figurescan be grown further if the input is extended by tap-ping on additional sources of translations.3 MENTA: Named Entities andSpecialized ConceptsThe UWN Core is extended by incorporating largeamounts of named entities and language- anddomain-specific concepts from Wikipedia (de Meloand Weikum, 2010a).
In the process, we also obtain152human-readable glosses in many languages, links toimages, and other valuable information.
These ad-ditions are not simply added as a separate knowl-edge base, but fully connected and integrated withthe core.
In particular, we create a mapping betweenWikipedia and WordNet in order to merge equiva-lent entries and we use taxonomy construction meth-ods in order to attach all new named entities to theirmost likely classes, e.g.
?Haight-Ashbury?
is linkedto a WordNet sense of the word ?neighborhood?.Information Integration Supervised link predic-tion, similar to the method presented in Section 2, isused in order to attach Wikipedia articles to semanti-cally equivalent WordNet entries, while also exploit-ing gloss similarity as an additional feature.
Addi-tionally, we connect articles from different multilin-gual Wikipedia editions via their cross-lingual inter-wiki links, as well as categories with equivalent ar-ticles and article redirects with redirect targets.We then consider connected components of di-rectly or transitively linked items.
In the ideal case,such a connected component consists of a numberof items all describing the same concept or entity, in-cluding articles from different versions of Wikipediaand perhaps also categories or WordNet senses.Unfortunately, in many cases one obtains con-nected components that are unlikely to be correct,because multiple articles from the same Wikipediaedition or multiple incompatible WordNet senses areincluded in the same component.
This can be dueto incorrect links produced by the supervised linkprediction, but often even the original links fromWikipedia are not consistent.In order to obtain more consistent connected com-ponents, we use combinatorial optimization meth-ods to delete certain links.
In particular, for eachconnected component to be analysed, an IntegerLinear Program formalizes the objective of mini-mizing the costs for deleted edges and the costs forignoring soft constraints.
The basic aim is that ofdeleting as few edges as possible while simultane-ously ensuring that the graph becomes as consistentas possible.
In some cases, there is overwhelmingevidence indicating that two slightly different arti-cles should be grouped together, while in other casesthere might be little evidence for the correctness ofan edge and so it can easily be deleted with low cost.While obtaining an exact solution is NP-hard andAPX-hard, we can solve the corresponding LinearProgram using a fast LP solver like CPLEX and sub-sequently apply region growing techniques to obtaina solution with a logarithmic approximation guaran-tee (de Melo and Weikum, 2010b).The clean connected components resulting fromthis process can then be merged to form aggregateentities.
For instance, given WordNet?s standardsense for ?fog?, water vapor, we can check whichother items are in the connected component andtransfer all information to the WordNet entry.
Byextracting snippets of text from the beginning ofWikipedia articles, we can add new gloss descrip-tions for fog in Arabic, Asturian, Bengali, and manyother languages.
We can also attach pictures show-ing fog to the WordNet word sense.Taxonomy Induction The above process con-nects articles to their counterparts in WordNet.
Inthe next step, we ensure that articles without any di-rect counterpart are linked to WordNet as well, bymeans of taxonomic hypernymy/instance links (deMelo and Weikum, 2010a).We generate individual hypotheses about likelyparents of entities.
For instance, articles are con-nected to their Wikipedia categories (if these are notassessed to be mere topic descriptors) and categoriesare linked to parent categories, etc.
In order to linkcategories to possible parent hypernyms in Word-Net, we adapt the approach proposed for YAGO(Suchanek et al, 2007) of determining the headwordof the category name and disambiguating it.Since we are dealing with a multilingual scenariothat draws on articles from different multilingualWikipedia editions that all need to be connected toWordNet, we apply an algorithm that jointly looksat an entity and all of its parent candidates (not justfrom an individual article, but all articles in the sameconnected component) as well as superordinate par-ent candidates (parents of parents, etc.
), as depictedin Figure 2.
We then construct a Markov chain basedon this graph of parents that also incorporates thepossibility of random jumps from any parent backto the current entity under consideration.
The sta-tionary probability of this Markov chain, which canbe obtained using random walk methods, providesus a ranking of the most likely parents.153Figure 2: Noisy initial edges (left) and cleaned, integrated output (right), shown in a simplified formFigure 3: UWN with named entitiesResults Overall, we obtain a knowledge base with5.4 million concepts or entities and 16.7 millionwords or names associated with them from over200 languages.
Over 2 million named entities comeonly from non-English Wikipedia editions, but theirtaxonomic links to WordNet still have an accuracyaround 90%.
An example excerpt is shown in Fig-ure 3, with named entities connected to higher-levelclasses in UWN, all with multilingual labels.4 Other ExtensionsWord Relationships Another plugin providesword relationships and properties mined from Wik-tionary.
These include derivational and etymologi-cal word relationships (e.g.
that ?grotesque?
comesfrom the Italian ?grotta?
: grotto, artificial cave), al-ternative spellings (e.g.
?encyclop?dia?
for ?en-cyclopedia?
), common misspellings (e.g.
?minis-cule?
for ?minuscule?
), pronunciation information(e.g.
how to pronounce ?nuclear?
), and so on.Frame-Semantic Knowledge Frame semantics isa cognitively motivated theory that describes wordsin terms of the cognitive frames or scenarios thatthey evoke and the corresponding participants in-volved in them.
For a given frame, FrameNetprovides definitions, involved participants, associ-ated words, and relationships.
For instance, theCommerce_goods-transfer frame normallyinvolves a seller and a buyer, among other things,and different words like ?buy?
and ?sell?
can be cho-sen to describe the same event.Such detailed knowledge about scenarios islargely complementary in nature to the sense re-lationships that WordNet provides.
For instance,WordNet emphasizes the opposite meaning of thewords ?happy?
and ?unhappy?, while frame seman-tics instead emphasizes the cognitive relatedness ofwords like ?happy?, ?unhappy?, ?astonished?, and?amusement?, and explains that typical participantsinclude an experiencer who experiences the emo-tions and external stimuli that evoke them.
Therehave been individual systems that made use of bothforms of knowledge (Shi and Mihalcea, 2005; Cop-pola and others, 2009), but due to their very differentnature, there is currently no simple way to accom-plish this feat.
Our system addresses this by seam-lessly integrating frame semantic knowledge into thesystem.
We draw on FrameNet (Baker et al, 1998),the most well-known computational instantiation offrame semantics.
While the FrameNet project isgenerally well-known, its use in practical applica-154tions has been limited due to the lack of easy-to-useAPIs and because FrameNet alne does not cover asmany words as WordNet.
Our API simultaneouslyprovides access to both sources.Language information For a given language, thisextension provides information such as relevantwriting systems, geographical regions, identifica-tion codes, and names in many different languages.These are all integrated into WordNet?s hypernymhierarchy, i.e.
from language families like the Siniticlanguages one may move down to macrolanguageslike Chinese, and then to more specific forms likeMandarin Chinese, dialect groups like Ji-Lu Man-darin, or even dialects of particular cities.The information is obtained from ISO standards,the Unicode CLDR as well as Wikipedia and thenintegrated with WordNet using the information in-tegration strategies described above (de Melo andWeikum, 2008).
Additionally, information aboutwriting systems is taken from the Unicode CLDRand information about individual characters is ob-tained from the Unicode, Unihan, and Hanzi Datadatabases.
For instance, the Chinese character ??
?is connected to its radical component ???
and to itspronunciation component ??
?.5 Integrated Query Interface and WikiWe have developed an online interface that providesaccess to our data to interested researchers (yago-knowledge.org/uwn/ ), as shown in Figure 4.Interactive online interfaces offer new ways of in-teracting with lexical knowledge that are not possi-ble with traditional print dictionaries.
For example,a user wishing to find a Spanish word for the conceptof persuading someone not to believe somethingmight look up the word ?persuasion?
and then navi-gate to its antonym ?dissuasion?
to find the Spanishtranslation.
A non-native speaker of English lookingup the word ?tercel?
might find it helpful to see pic-tures available for the related terms ?hawk?
or ?fal-con?
?
a Google Image search for ?tercel?
merely de-livers images of Toyota Tercel cars.While there have been other multilingual inter-faces to WordNet-style lexical knowledge in the past(Pianta et al, 2002; Atserias and others, 2004), theseprovide less than 10 languages as of 2012.
The mostsimilar resource is BabelNet (Navigli and Ponzetto,2010), which contains multilingual synsets but doesnot connect named entities from Wikipedia to themin a multilingual taxonomy.Figure 4: Part of Online Interface6 Integrated APIOur goal is to make the knowledge that we have de-rived available for use in applications.
To this end,we have developed a fully downloadable API thatcan easily be used in several different programminglanguages.
While there are many existing APIs forWordNet and other lexical resources (e.g.
(Judea etal., 2011; Gurevych and others, 2012)), these don?tprovide a comparable degree of integrated multilin-gual and taxonomic information.Interface The API can be used by initializing anaccessor object and possibly specifying the list ofplugins to be loaded.
Depending on the particularapplication, one may choose only Princeton Word-Net and the UWN Core, or one may want to in-clude named entities from Wikipedia and frame-semantic knowledge derived from FrameNet, for in-stance.
The accessor provides a simple graph-basedlookup API as well as some convenience methodsfor common types of queries.An additional higher-level API module imple-ments several measures of semantic relatedness.
Italso provides a simple word sense disambiguationmethod that, given a tokenized text with part-of-155speech and lemma annotations, selects likely wordsenses by choosing the senses (with matching part-of-speech) that are most similar to words in the con-text.
Note that these modules go beyond existingAPIs because they operate on words in many differ-ent languages and semantic similarity can even beassessed across languages.Data Structures Under the hood, each plugin re-lies on a disk-based associative array to store theknowledge base as a labelled multi-graph.
The out-going labelled edges of an entity are saved on disk ina serialized form, including relation names and rela-tion weights.
An index structure allows determiningthe position of such records on disk.Internally, this index structure is implemented asa linearly-probed hash table that is also stored ex-ternally.
Note that such a structure is very efficientin this scenario, because the index is used as a read-only data store by the API.
Once an index has beencreated, write operations are no longer performed,so B+ trees and similar disk-based balanced tree in-dices commonly used in relational database manage-ment systems are not needed.
The advantage is thatthis enables faster lookups, because retrieval opera-tions normally require only two disk reads per plu-gin, one to access a block in the index table, andanother to access a block of actual data.7 ConclusionUWN is an important new multilingual lexical re-source that is now freely available to the community.It has been constructed using sophisticated knowl-edge extraction, link prediction, information integra-tion, and taxonomy induction methods.
Apart froman online querying and browsing interface, we havealso implemented an API that facilitates the use ofthe knowledge base in applications.ReferencesJordi Atserias et al 2004.
The MEANING multilingualcentral repository.
In Proc.
GWC 2004.Collin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet project.
In Proc.COLING-ACL 1998.Niladri Chatterjee, Shailly Goyal, and Anjali Naithani.2005.
Resolving pattern ambiguity for English toHindi machine translation using WordNet.
In Proc.Workshop Translation Techn.
at RANLP 2005.Bonaventura Coppola et al 2009.
Frame detection overthe Semantic Web.
In Proc.
ESWC.Gerard de Melo and Gerhard Weikum.
2008.
Languageas a foundation of the Semantic Web.
In Proc.
ISWC.Gerard de Melo and Gerhard Weikum.
2009.
Towardsa universal wordnet by learning from combined evi-dence.
In Proc.
CIKM 2009.Gerard de Melo and Gerhard Weikum.
2010a.
MENTA:Inducing multilingual taxonomies from Wikipedia.
InProc.
CIKM 2010.Gerard de Melo and Gerhard Weikum.
2010b.
Untan-gling the cross-lingual link structure of Wikipedia.
InProc.
ACL 2010.Oren Etzioni, Kobi Reiter, Stephen Soderland, and Mar-cus Sammer.
2007.
Lexical translation with applica-tion to image search on the Web.
In Proc.
MT Summit.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
The MIT Press.Namrata Godbole, Manjunath Srinivasaiah, and StevenSkiena.
2007.
Large-scale sentiment analysis for newsand blogs.
In Proc.
ICWSM.Zhiguo Gong, Chan Wa Cheang, and Leong Hou U.2005.
Web query expansion by WordNet.
In Proc.DEXA 2005.Iryna Gurevych et al 2012.
Uby: A large-scale uni-fied lexical-semantic resource based on LMF.
In Proc.EACL 2012.Yves R. Jean-Mary and Mansur R. Kabuka.
2008.
AS-MOV: Results for OAEI 2008.
In Proc.
OM 2008.Alex Judea, Vivi Nastase, and Michael Strube.
2011.WikiNetTk ?
A tool kit for embedding world knowl-edge in NLP applications.
In Proc.
IJCNLP 2011.Zoubida Kedad and Elisabeth M?tais.
2002.
Ontology-based data cleaning.
In Proc.
NLDB 2002.Jayant Madhavan, P. Bernstein, and E. Rahm.
2001.Generic schema matching with Cupid.
In Proc.
VLDB.Marcin Marsza?ek and C. Schmid.
2007.
Semantic hier-archies for visual object recognition.
In Proc.
CVPR.Roberto Navigli and Simone Paolo Ponzetto.
2010.
Ba-belNet: Building a very large multilingual semanticnetwork.
In Proc.
ACL 2010.Emanuele Pianta, Luisa Bentivogli, and Christian Gi-rardi.
2002.
MultiWordNet: Developing an alignedmultilingual database.
In Proc.
GWC.Daniel L. Rubin et al 2006.
National Center for Biomed-ical Ontology.
OMICS, 10(2):185?98.Lei Shi and Rada Mihalcea.
2005.
Putting the pieces to-gether: Combining FrameNet, VerbNet, and WordNetfor robust semantic parsing.
In Proc.
CICLing.Fabian M. Suchanek, Gjergji Kasneci, and GerhardWeikum.
2007.
YAGO: A core of semantic knowl-edge.
In Proc.
WWW 2007.156
