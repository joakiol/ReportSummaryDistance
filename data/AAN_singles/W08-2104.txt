CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 25?32Manchester, August 2008Linguistic features in data-driven dependency parsingLilja ?vrelidNLP-unit, Dept.
of SwedishUniversity of GothenburgSwedenlilja.ovrelid@svenska.gu.seAbstractThis article investigates the effect of a setof linguistically motivated features on ar-gument disambiguation in data-driven de-pendency parsing of Swedish.
We presentresults from experiments with gold stan-dard features, such as animacy, definite-ness and finiteness, as well as correspond-ing experiments where these features havebeen acquired automatically and showsignificant improvements both in overallparse results and in the analysis of specificargument relations, such as subjects, ob-jects and predicatives.1 IntroductionData-driven dependency parsing has recently re-ceived extensive attention in the parsing commu-nity and impressive results have been obtained fora range of languages (Nivre et al, 2007).
Evenwith high overall parsing accuracy, however, data-driven parsers often make errors in the assign-ment of argument relations such as subject andobject and the exact influence of data-derived fea-tures on the parsing accuracy for specific linguisticconstructions is still relatively poorly understood.There are a number of studies that investigate theinfluence of different features or representationalchoices on overall parsing accuracy, (Bod, 1998;Klein and Manning, 2003).
There are also attemptsat a more fine-grained analysis of accuracy, target-ing specific linguistic constructions or grammati-cal functions (Carroll and Briscoe, 2002; Ku?blerand Prokic?, 2006; McDonald and Nivre, 2007).c?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.But there are few studies that combine the two per-spectives and try to tease apart the influence of dif-ferent features on the analysis of specific construc-tions, let alne motivated by a thorough linguisticanalysis.In this paper, we investigate the influence of aset of linguistically motivated features on parse re-sults for Swedish, and in particular on the analysisof argument relations such as subjects, objects andsubject predicatives.
Motivated by an error anal-ysis of the best performing parser for Swedish inthe CoNLL-X shared task, we extend the featuremodel employed by the parser with a set of lin-guistically motivated features and go on to showhow these features may be acquired automatically.We then present results from corresponding parseexperiments with automatic features.The rest of the paper is structured as follows.
Insection 2 we present relevant properties of Swedishmorphosyntax, as well as the treebank and parseremployed in the experiments.
Section 3 presentsan error analysis of the baseline parser and we goon to motivate a set of linguistic features in sec-tion 4, which are employed in a set of experimentswith gold standard features, discussed in section5.
Section 6 presents the automatic acquisition ofthese features, with a particular focus on animacyclassification and in section 7 we report parse ex-periments with automatic features.2 Parsing SwedishBefore we turn to a description of the treebankand the parser used in the experiments, we want topoint to a few grammatical properties of Swedishthat will be important in the following:Verb second (V2) Swedish is, like the majority ofGermanic languages a V2-language; the fi-nite verb always resides in second position in25declarative main clauses.Word order variation Pretty much any con-stituent may occupy the sentence-initial po-sition, but subjects are most common.Limited case marking Nouns are only inflectedfor genitive case.
Personal pronouns dis-tinguish nominative and accusative case, butdemonstratives and quantifying pronouns arecase ambiguous (like nouns).2.1 Treebank: Talbanken05Talbanken05 is a Swedish treebank converted todependency format, containing both written andspoken language (Nivre et al, 2006a).1 For eachtoken, Talbanken05 contains information on wordform, part of speech, head and dependency rela-tion, as well as various morphosyntactic and/orlexical semantic features.
The nature of this ad-ditional information varies depending on part ofspeech:NOUN: definiteness, animacy, case (?/GEN)PRO: animacy, case (?/ACC)VERB: tense, voice (?/PA)2.2 Parser: MaltParserWe use the freely available MaltParser,2 whichis a language-independent system for data-drivendependency parsing.
MaltParser is based ona deterministic parsing strategy, first proposedby Nivre (2003), in combination with treebank-induced classifiers for predicting the next parsingaction.
Classifiers can be trained using any ma-chine learning approach, but the best results haveso far been obtained with support vector machines,using LIBSVM (Chang and Lin, 2001).
Malt-Parser has a wide range of parameters that need tobe optimized when parsing a new language.
Asour baseline, we use the settings optimized forSwedish in the CoNLL-X shared task (Nivre et al,2006b), where this parser was the best perform-ing parser for Swedish.
The only parameter thatwill be varied in the later experiments is the fea-ture model used for the prediction of the next pars-ing action.
Hence, we need to describe the featuremodel in a little more detail.MaltParser uses two main data structures, astack (S) and an input queue (I), and builds a de-pendency graph (G) incrementally in a single left-1The written sections of the treebank consist of profes-sional prose and student essays and amount to 197,123 run-ning tokens, spread over 11,431 sentences.2http://w3.msi.vxu.se/users/nivre/research/MaltParser.htmlFORM POS DEP FEATSS:top + + + +S:top+1 +I:next + + +I:next?1 + +I:next+1 + + +I:next+2 +G: head of top + +G: left dep of top +G: right dep of top +G: left dep of next + + +G: left dep of head of top +G: left sibling of right dep of top +G: right sibling of left dep of top + +G: right sibling of left dep of next + +Table 1: Baseline and extended (FEATS) featuremodel for Swedish; S: stack, I: input, G: graph;?n = n positions to the left(?)
or right (+)to-right pass over the input.
The decision thatneeds to be made at any point during this deriva-tion is (a) whether to add a dependency arc (withsome label) between the token on top of the stack(top) and the next token in the input queue (next),and (b) whether to pop top from the stack or pushnext onto the stack.
The features fed to the classi-fier for making these decisions naturally focus onattributes of top, next and neighbouring tokens inS, I or G. In the baseline feature model, these at-tributes are limited to the word form (FORM), partof speech (POS), and dependency relation (DEP) ofa given token, but in later experiments we will addother linguistic features (FEATS).
The baseline fea-ture model is depicted as a matrix in Table 1, whererows denote tokens in the parser configuration (de-fined relative to S, I and G) and columns denoteattributes.
Each cell containing a + corresponds toa feature of the model.3 Baseline and Error AnalysisThe written part of Talbanken05 was parsed em-ploying the baseline feature model detailed above,using 10-fold cross validation for training and test-ing.
The overall result for unlabeled and labeleddependency accuracy is 89.87 and 84.92 respec-tively.3Error analysis shows that the overall most fre-quent errors in terms of dependency relations in-volve either various adverbial relations, due to PP-attachment ambiguities and a large number of ad-3Note that these results are slightly better than the officialCoNLL-X shared task scores (89.50/84.58), which were ob-tained using a single training-test split, not cross-validation.Note also that, in both cases, the parser input contained goldstandard part-of-speech tags.26Gold Sys before after TotalSS OO 103 (23.1%) 343 (76.9%) 446 (100%)OO SS 103 (33.3%) 206 (66.7%) 309 (100%)Table 2: Position relative to verb for confused sub-jects and objectsverbial labels, or the argument relations, such assubjects, direct objects, formal subjects and sub-ject predicatives.
In particular, confusion of argu-ment relations are among the most frequent errortypes with respect to dependency assignment.4Swedish exhibits some ambiguities in word or-der and morphology which follow from the proper-ties discussed above.
We will exemplify these fac-tors through an analysis of the errors where sub-jects are assigned object status (SS OO) and viceversa (OO SS).
The confusion of subjects and ob-jects follows from lack of sufficient formal disam-biguation, i.e., simple clues such as word order,part-of-speech and word form do not clearly indi-cate syntactic function.With respect to word order, subjects and objectsmay both precede or follow their verbal head.
Sub-jects, however, are more likely to occur prever-bally (77%), whereas objects typically occupy apostverbal position (94%).
We would therefore ex-pect postverbal subjects and preverbal objects to bemore dominant among the errors than in the tree-bank as a whole (23% and 6% respectively).
Table2 shows a breakdown of the errors for confusedsubjects and objects and their position with respectto the verbal head.
We find that postverbal subjects(after) are in clear majority among the subjects er-roneously assigned the object relation.
Due to theV2 property of Swedish, the subject must residein the position directly following the finite verbwhenever another constituent occupies the prever-bal position, as in (1) where a direct object residessentence-initially:(1) Sammasameerfarenhetexperiencegjordemadeengelsma?nnenenglishmen-DEF?The same experience, the Englishmen had?For the confused objects we find a larger propor-tion of preverbal elements than for subjects, which4We define argument relations as dependency relationswhich obtain between a verb and a dependent which issubcategorized for and/or thematically entailed by the verb.Note that arguments are not distinguished structurally fromnon-arguments, like adverbials, in dependency grammar, butthrough dependency label.is the mirror image of the normal distribution ofsyntactic functions among preverbal elements.
AsTable 2 shows, the proportion of preverbal ele-ments among the subject-assigned objects (33.3%)is notably higher than in the corpus as a whole,where preverbal objects account for a miniscule6% of all objects.In addition to the word order variation dis-cussed above, Swedish also has limited morpho-logical marking of syntactic function.
Nouns aremarked only for genitive case and only pronounsare marked for accusative case.
There is also syn-cretism in the pronominal paradigm where the pro-noun is invariant for case, e.g.
det, den ?it?, in-gen/inga ?no?, and may, in fact, also function asa determiner.
This means that, with respect toword form, only the set of unambiguous pronounsclearly indicate syntactic function.
In the errors,we find that nouns and functionally ambiguouspronouns dominate the errors where subjects andobjects are confused, accounting for 84.5% of theSS OO and 93.5% of the OO SS errors.The initial error analysis shows that the confu-sion of argument relations constitutes a frequentand consistent error during parsing.
Ambiguitiesin word order and morphological marking consti-tute a complicating factor and we find cases thatdeviate from the most frequent word order pat-terns and are not formally disambiguated by part-of-speech information.
It is clear that we in orderto resolve these ambiguities have to examine fea-tures beyond syntactic category and linear word or-der.4 Linguistic features for argumentdisambiguationArgument relations tend to differ along several lin-guistic dimensions.
These differences are foundas statistical tendencies, rather than absolute re-quirements on syntactic structure.
The propertyof animacy, a referential property of nominal el-ements, has been argued to play a role in argumentrealization in a range of languages see de Swartet.al.
(2008) for an overview.
It is closely cor-related with the semantic property of agentivity,hence subjects will tend to be referentially animatemore often than objects.
Another property whichmay differentiate between the argument functionsis the property of definiteness, which can be linkedwith a notion of givenness, (Weber and Mu?ller,2004).
This is reflected in the choice of refer-ring expression for the various argument types in27Talbanken05 ?
subjects are more often pronominal(49.2%), whereas objects and subject predicativesare typically realized by an indefinite noun (67.6%and 89.6%, respectively).
As mentioned in section2, there are categorical constraints which are char-acteristic for Swedish morphosyntax.
Even if themorphological marking of arguments in Scandina-vian is not extensive or unambiguous, case maydistinguish arguments.
Only subjects may followa finite verb and precede a non-finite verb and onlycomplements may follow a non-finite verb.
Infor-mation on tense or the related finiteness is there-fore something that one might assume to be ben-eficial for argument analysis.
Another property ofthe verb which clearly influences the assignmentof core argument functions is the voice of the verb,i.e., whether it is passive or active.55 Experiments with gold standardfeaturesWe perform a set of experiments with an extendedfeature model and added, gold standard informa-tion on animacy, definiteness, case, finiteness andvoice, where the features were employed individu-ally as well as in combination.5.1 Experimental methodologyAll parsing experiments are performed using 10-fold cross-validation for training and testing onthe entire written part of Talbanken05.
The fea-ture model used throughout is the extended fea-ture model depicted in Table 1, including all fourcolumns.6 Hence, what is varied in the exper-iments is only the information contained in theFEATS features (animacy, definiteness, etc.
), whilethe tokens for which these features are defined re-mains constant.
Overall parsing accuracy will bereported using the standard metrics of labeled at-tachment score (LAS) and unlabeled attachmentscore (UAS).7 Statistical significance is checkedusing Dan Bikel?s randomized parsing evaluation5We experimented with the use of tense as well as finite-ness, a binary feature which was obtained by a mapping fromtense to finite/non-finite.
Finiteness gave significantly betterresults (p<.03) and was therefore employed in the following,see (?vrelid, 2008b) for details.6Preliminary experiments showed that it was better to tieFEATS features to the same tokens as FORM features (ratherthan POS or DEP features).
Backward selection from thismodel was tried for several different instantiations of FEATSbut with no significant improvement.7LAS and UAS report the percentage of tokens that are as-signed the correct head with (labeled) or without (unlabeled)the correct dependency label, calculated using eval.pl with de-fault settings (http://nextens.uvt.nl/?conll/software.html)comparator.8 Since the main focus of this article ison the disambiguation of grammatical functions,we report accuracy for specific dependency rela-tions, measured as a balanced F-score.5.2 ResultsThe overall results for these experiments are pre-sented in table 3, along with p-scores.
The exper-iments show that each feature individually causesa significant improvement in terms of overall la-beled accuracy as well as performance for argu-ment relations.
Error analysis comparing the base-line parser (NoFeats) with new parsers trained withindividual features reveal the influence of thesefeatures on argument disambiguation.
We findthat animacy influences the disambiguation of sub-jects from objects, objects from indirect objectsas well as the general distinction of argumentsfrom non-arguments.
Definiteness has a notableeffect on the disambiguation of subjects and sub-ject predicatives.
Information on morphologicalcase shows a clear effect in distinguishing betweenarguments and non-arguments, and in particular,in distinguishing nominal modifiers with genitivecase.
The added verbal features, finiteness andvoice, have a positive effect on the verbal depen-dency relations, as well as an overall effect on theassignment of the SS and OO argument relations.Information on voice also benefits the relation ex-pressing the demoted agent (AG) in passive con-structions, headed by the preposition av ?by?, as inEnglish.The ADCV experiment which combines infor-mation on animacy, definiteness, case and verbalfeatures shows a cumulative effect of the addedfeatures with results which differ significantlyfrom the baseline, as well as from each of the in-dividual experiments (p<.0001).
We observe clearimprovements for the analysis of all argument re-lations, as shown by the third column in table 4which presents F-scores for the various argumentrelations.6 Acquiring featuresA possible objection to the general applicabilityof the results presented above is that the addedinformation consists of gold standard annotationfrom a treebank.
However, the morphosyntacticfeatures examined here (definiteness, case, tense,voice) represent standard output from most part-of-speech taggers.
In the following we will also8http://www.cis.upenn.edu/?dbikel/software.html28UAS LAS p-valueNoFeats 89.87 84.92 ?Anim 89.93 85.10 p<.0002Def 89.87 85.02 p<.02Case 89.99 85.13 p<.0001Verb 90.24 85.38 p<.0001ADC 90.13 85.35 p<.0001ADCV 90.40 85.68 p<.0001Table 3: Overall results in gold standard ex-periments expressed as unlabeled and labeledattachment scores.NoFeats Gold AutoSS subject 90.25 91.80 91.32OO object 84.53 86.27 86.10SP subj.pred.
84.82 85.87 85.80AG pass.
agent 73.56 81.34 81.02ES logical subj.
71.82 73.44 72.60FO formal obj.
56.68 65.64 65.38VO obj.
small clause 72.10 83.40 83.12VS subj.
small clause 58.75 65.56 68.75FS formal subj.
71.31 72.10 71.31IO indir.
obj.
76.14 77.76 76.29Table 4: F-scores for argument relations withcombined features (ADCV).Feature ApplicationDefiniteness POS-taggerCase POS-taggerAnimacy - NN Animacy classifierAnimacy - PN Named Entity TaggerAnimacy - PO Majority classTense (finiteness), voice POS-taggerTable 5: Overview of applications employed forautomatic feature acquisition.show that the property of animacy can be fairlyrobustly acquired for common nouns by meansof distributional features from an automaticallyparsed corpus.Table 5 shows an overview of the applicationsemployed for the automatic acquisition of our lin-guistic features.
For part-of-speech tagging, weemploy MaltTagger ?
a HMM part-of-speech tag-ger for Swedish (Hall, 2003).
The POS-tagger dis-tinguishes tense and voice for verbs, nominativeand accusative case for pronouns, as well as defi-niteness and genitive case for nouns.6.1 AnimacyThe feature of animacy is clearly the most chal-lenging feature to acquire automatically.
Recallthat Talbanken05 distinguishes animacy for allnominal constituents.
In the following we describethe automatic acquisition of animacy informationfor common nouns, proper nouns and pronouns.Common nouns Table 6 presents an overviewof the animacy data for common nouns in Tal-banken05.
It is clear that the data is highly skewedClass Types Tokens coveredAnimate 644 6010Inanimate 6910 34822Total 7554 40832Table 6: The animacy data set from Talbanken05;number of noun lemmas (Types) and tokens ineach class.towards the non-person class, which accounts for91.5% of the data instances.
Due to the small sizeof the treebank we classify common noun lem-mas based on their morphosyntactic distributionin a considerably larger corpus.
For the animacyclassification of common nouns, we construct ageneral feature space for animacy classification,which makes use of distributional data regardingsyntactic properties of the noun, as well as variousmorphological properties.
The syntactic and mor-phological features in the general feature space arepresented below:Syntactic features A feature for each dependencyrelation with nominal potential: (transitive)subject (SUBJ), object (OBJ), prepositionalcomplement (PA), root (ROOT)9, apposition(APP), conjunct (CC), determiner (DET), pred-icative (PRD), complement of comparativesubjunction (UK).
We also include a featurefor the complement of a genitive modifier, theso-called ?possessee?, (GENHD).Morphological features A feature for each mor-9Nominal elements may be assigned the root relation insentence fragments which do not include a finite verb.29phological distinction relevant for a noun:gender (NEU/UTR), number (SIN/PLU), defi-niteness (DEF/IND), case (NOM/GEN).
Also,the part-of-speech tags distinguish dates(DAT) and quantifying nouns (SET), e.g.
del,rad ?part, row?, so these are also included asfeatures.For extraction of distributional data for the Tal-banken05 nouns we make use of the Swedish Pa-role corpus of 21.5M tokens.10 To facilitate featureextraction, we part-of-speech tag the corpus andparse it with MaltParser, which assigns a depen-dency analysis.11 For classification, we make useof the Tilburg Memory-Based Learner (TiMBL)(Daelemans et al, 2004).12 and optimize theTiMBL parameters on a subset of the full dataset.13We obtain results for animacy classification ofnoun lemmas, ranging from 97.3% accuracy to94.0% depending on the sparsity of the data.
Withan absolute frequency threshold of 10, we obtainan accuracy of 95.4%, which constitutes a 50%reduction of error rate over a majority baseline.We find that classification of the inanimate class isquite stable throughout the experiments, whereasthe classification of the minority class of animatenouns suffers from sparse data.
We obtain a F-score of 71.8% F-score for the animate class and97.5% for the inanimate class with a threshold of10.
The common nouns in Talbanken05 are classi-fied for animacy following a leave-one-out trainingand testing scheme where each of the n nouns inTalbanken05 are classified with a classifier trainedon n ?
1 instances.
This ensures that the trainingand test instances are disjoint at all times.
More-over, the fact that the distributional data is takenfrom a separate data set ensures non-circularity10Parole is available at http://spraakbanken.gu.se11For part-of-speech tagging, we employ the MaltTagger ?a HMM part-of-speech tagger for Swedish (Hall, 2003).
Forparsing, we employ MaltParser with a pretrained model forSwedish, which has been trained on the tags output by thetagger.
It makes use of a smaller set of dependency relationsthan those found in Talbanken05.12TiMBL is freely available athttp://ilk.uvt.nl/software.html13For parameter optimization we employ theparamsearch tool, supplied with TiMBL, seehttp://ilk.uvt.nl/software.html.
Paramsearch implementsa hill climbing search for the optimal settings on iterativelylarger parts of the supplied data.
We performed parameteroptimization on 20% of the total >0 data set, where webalanced the data with respect to frequency.
The resultingsettings are k = 11, GainRatio feature weighting and InverseLinear (IL) class voting weights.since we are not basing the classification on goldstandard parses.Proper nouns In the task of named entity recog-nition (NER), proper nouns are classified accord-ing to a set of semantic categories.
For the annota-tion of proper nouns, we make use of a named en-tity tagger for Swedish (Kokkinakis, 2004), whichis a rule-based tagger based on finite-state rules,supplied with name lists, so-called ?gazetteers?.The tagger distinguishes the category ?Person?
forhuman referring proper nouns and we extract in-formation on this category.Pronouns A subset of the personal pronouns inScandinavian, as in English, clearly distinguishtheir referent with regard to animacy, e.g.
han,det ?he, it?.
There is, however, a quite large groupof third person plural pronouns which are ambigu-ous with regards to the animacy of their referent,e.g., de, dem, deras ?they, them, theirs?.
Pronom-inal reference resolution is a complex task whichwe will not attempt to solve in the present context.The pronominal part-of-speech tags from the part-of-speech tagger distinguish number and genderand in the animacy classification of the personalpronouns we classify based on these tags only.
Weemploy a simple heuristic where the pronominaltags which had more than 85% human instances inthe gold standard are annotated as human.14 Thepronouns which are ambiguous with respect to an-imacy are not annotated as animate.In table 7 we see an overview of the accuracyof the acquired features, i.e., the percentage ofcorrect instances out of all instances.
Note thatwe adhere to the general annotation strategy inTalbanken05, where each dimension (definiteness,case etc.)
contains a null category ?, which ex-presses the lack of a certain property.
The acqui-sition of the morphological features (definiteness,case, finiteness and voice) are very reliable, withaccuracies from 96.9% for voice to 98.5% for thecase feature.It is not surprising that we observe the largestdiscrepancies from the gold standard annotationin the automatic animacy annotation.
In general,the annotation of animate nominals exhibits a de-cent precision (95.7) and a lower recall (61.3).
Theautomatic classification of human common nouns14A manual classification of the individual pronoun lem-mas was also considered.
However, the treebank has a total of324 different pronoun forms, hence we opted for a heuristicclassification of the part-of-speech tags instead.30Dimension Features Instances Correct AccuracyDefiniteness DD, ?
40832 40010 98.0Case GG, AA, ?
68313 67289 98.5AnimacyNNPNPOHH, ?
68313 61295 89.7AnimacyNNHH, ?
40832 37952 92.9AnimacyPNHH, ?
2078 1902 91.5AnimacyPOHH, ?
25403 21441 84.4Finiteness FV, ?
30767 30035 97.6Voice PA, ?
30767 29805 96.9Table 7: Accuracy for automatically acquired linguistic features.Gold AutomaticUAS LAS UAS LAS p-valueNoFeats 89.87 84.92 89.87 84.92 ?Def 89.87 85.02 89.88 85.03 p<0.01Case 89.99 85.13 89.95 85.11 p<.0001Verb 90.24 85.38 90.12 85.26 p<.0001Anim 89.93 85.10 89.86 85.01 p<.03ADC 90.13 85.35 90.01 85.21 p<.0001ADCV 90.40 85.68 90.27 85.54 p<.0001Table 8: Overall results in experiments with auto-matic features compared to gold standard features.
(AnimacyNN) also has a quite high precision(94.2) in combination with a lower recall (55.5).The named-entity recognizer (AnimacyPN) showsmore balanced results with a precision of 97.8 anda recall of 85.2 and the heuristic classification ofthe pronominal part-of-speech tags (AnimacyPO)gives us high precision (96.3) combined with lowerrecall (62.0) for the animate class.7 Experiments with acquired featuresThe experimental methodology is identical to theone described in 5.1 above, the only difference be-ing that the linguistic features are acquired auto-matically, rather than being gold standard.
In orderto enable a direct comparison with the results fromthe earlier experiments, we employ the gold stan-dard part-of-speech tags, as before.
This meansthat the set for which the various linguistic featuresare defined is identical, whereas the feature valuesmay differ.Table 8 presents the overall results with auto-matic features, compared to the gold standard re-sults and p-scores for the difference of the auto-matic results from the NoFeats baseline.
As ex-pected, we find that the effect of the automatic fea-tures is generally lower than their gold standardcounterparts.
However, all automatic features im-prove significantly on the NoFeats baseline.
In theerror analysis we find the same tendencies in termsof improvement for specific dependency relations.The morphological argument features from thePOS-tagger are reliable, as we saw above, andwe observe almost identical results to the goldstandard results.
The addition of informationon definiteness causes a significant improvement(p<.01), and so does the addition of informationon case (p<.0001).
The addition of the automat-ically acquired animacy information results in asmaller, but significant improvement of overall re-sults even though the annotation is less reliable(p<.03).
An interesting result is that the automat-ically acquired information on animacy for com-mon nouns actually has a significantly better effectthan the gold standard counterparts due to captur-ing distributional tendencies (?vrelid, 2008a).
Asin the gold standard experiments, we find that thefeatures which have the most notable effect on per-formance are the verbal features (p<.0001).In parallel with the results achieved with thecombination of gold standard features, we observeimprovement of overall results compared to thebaseline (p<.0001) and each of the individual fea-tures when we combine the features of the argu-ments (ADC; p<.01) and the argument and ver-bal features (ADCV; p<.0001).
Column 4 in Ta-ble 4 shows an overview of performance for theargument relations, compared to the gold standardexperiments.
We find overall somewhat lower re-sults in the experiment with automatic features, butfind the same tendencies with the automatically ac-quired features.318 ConclusionAn error analysis of the best performing data-driven dependency parser for Swedish revealedconsistent errors in dependency assignment,namely the confusion of argument functions.
Weestablished a set of features expressing distinguish-ing semantic and structural properties of argu-ments such as animacy, definiteness and finitenessand performed a set of experiments with gold stan-dard features taken from a treebank of Swedish.The experiments showed that each feature individ-ually caused an improvement in terms of overall la-beled accuracy and performance for the argumentrelations.
We furthermore found that the resultsmay largely be replicated with automatic featuresand a generic part-of-speech tagger.
The featureswere acquired automatically employing a part-of-speech tagger, a named-entity recognizer and ananimacy classifier of common noun lemmas em-ploying morphosyntactic distributional features.
Aset of corresponding experiments with automaticfeatures gave significant improvement from the ad-dition of individual features and a cumulative ef-fect of the same features in combination.
In partic-ular, we show that the very same tendencies in im-provement for specific argument relations such assubjects, objects and predicatives may be obtainedusing automatically acquired features.Properties of the Scandinavian languages con-nected with errors in argument assignment are notisolated phenomena.
A range of other languagesexhibit similar properties, for instance, Italian ex-hibits word order variation, little case, syncretismin agreement morphology, as well as pro-drop;German exhibits a larger degree of word ordervariation in combination with quite a bit of syn-cretism in case morphology; Dutch has word ordervariation, little case and syncretism in agreementmorphology.
These are all examples of other lan-guages for which the results described here are rel-evant.ReferencesBod, Rens.
1998.
Beyond Grammar: An experience-basedtheory of language.
CSLI Publications, Stanford, CA.Carroll, John and Edward Briscoe.
2002.
High precision ex-traction of grammatical relations.
In Proceedings of the19th International Conference on Computational Linguis-tics (COLING), pages 134?140.Chang, Chih-Chung and Chih-Jen Lin.
2001.
LIBSVM: Alibrary for support vector machines.
Software available athttp://www.csie.ntu.edu.tw/?cjlin/libsvm.Daelemans, Walter, Jakub Zavrel, Ko Van der Sloot, and An-tal Van den Bosch.
2004.
TiMBL: Tilburg Memory BasedLearner, version 5.1, Reference Guide.
Technical report,ILK Technical Report Series 04-02.de Swart, Peter, Monique Lamers, and Sander Lestrade.2008.
Animacy, argument structure and argument encod-ing: Introduction to the special issue on animacy.
Lingua,118(2):131?140.Hall, Johan.
2003.
A probabilistic part-of-speech taggerwith suffix probabilities.
Master?s thesis, Va?xjo?
Univer-sity, Sweden.Klein, Dan and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
In Proceedings of the 41st AnnualMeeting of the Association for Computational Linguistics(ACL), pages 423?430.Kokkinakis, Dimitrios.
2004.
Reducing the effect of nameexplosion.
In Proceedings of the LREC Workshop: Be-yond Named Entity Recognition, Semantic labelling forNLP tasks.Ku?bler, Sandra and Jelena Prokic?.
2006.
Why is German de-pendency parsing more reliable than constituent parsing?In Proceedings of the Fifth Workshop on Treebanks andLinguistic Theories (TLT), pages 7?18.McDonald, Ryan and Joakim Nivre.
2007.
Characterizingthe errors of data-driven dependency parsing.
In Proceed-ings of the Eleventh Conference on Computational NaturalLanguage Learning (CoNLL), pages 122?131.Nivre, Joakim, Jens Nilsson, and Johan Hall.
2006a.
Tal-banken05: A Swedish treebank with phrase structure anddependency annotation.
In Proceedings of the fifth Inter-national Conference on Language Resources and Evalua-tion (LREC), pages 1392?1395.Nivre, Joakim, Jens Nilsson, Johan Hall, Gu?ls?en Eryig?it, andSvetoslav Marinov.
2006b.
Labeled pseudo-projectivedependency parsing with Support Vector Machines.
InProceedings of the Conference on Computational NaturalLanguage Learning (CoNLL).Nivre, Joakim, Johan Hall, Sandra Ku?bler, Ryan McDon-ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.2007.
CoNLL 2007 Shared Task on Dependency Pars-ing.
In Proceedings of the CoNLL Shared Task Session ofEMNLP-CoNLL 2007, pages 915?932.
?vrelid, Lilja.
2008a.
Argument Differentiation.
Soft con-straints and data-driven models.
Ph.D. thesis, Universityof Gothenburg.
?vrelid, Lilja.
2008b.
Finite matters: Verbal features in data-driven parsing of Swedish.
In Proceedings of the Interna-tional Conference on NLP, GoTAL 2008.Weber, Andrea and Karin Mu?ller.
2004.
Word order varia-tion in German main clauses: A corpus analysis.
In Pro-ceedings of the 20th International Conference on Compu-tational Linguistics, pages 71?77.32
