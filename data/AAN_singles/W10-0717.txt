Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 108?113,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsUsing Mechanical Turk to Annotate Lexicons forLess Commonly Used LanguagesAnn Irvine and Alexandre KlementievComputer Science DepartmentJohns Hopkins UniversityBaltimore, MD 21218{anni,aklement}@jhu.eduAbstractIn this work we present results from usingAmazon?s Mechanical Turk (MTurk) to an-notate translation lexicons between Englishand a large set of less commonly used lan-guages.
We generate candidate translations for100 English words in each of 42 foreign lan-guages using Wikipedia and a lexicon induc-tion framework.
We evaluate the MTurk an-notations by using positive and negative con-trol candidate translations.
Additionally, weevaluate the annotations by adding pairs to ourseed dictionaries, providing a feedback loopinto the induction system.
MTurk workers aremore successful in annotating some languagesthan others and are not evenly distributedaround the world or among the world?s lan-guages.
However, in general, we find thatMTurk is a valuable resource for gatheringcheap and simple annotations for most of thelanguages that we explored, and these anno-tations provide useful feedback in building alarger, more accurate lexicon.1 IntroductionIn this work, we make use of several free and cheapresources to create high quality lexicons for lesscommonly used languages.
First, we take advan-tage of small existing dictionaries and freely avail-able Wikipedia monolingual data to induce addi-tional lexical translation pairs.
Then, we pay Me-chanical Turk workers a small amount to check andcorrect our system output.
We can then use the up-dated lexicons to inform another iteration of lexiconinduction, gather a second set of MTurk annotations,and so on.Here, we provide results of one iteration of MTurkannotation.
We discuss the feasibility of usingMTurk for annotating translation lexicons betweenEnglish and 42 less commonly used languages.
Ourprimary goal is to enlarge and enrich the small,noisy bilingual dictionaries that we have for eachlanguage.
Our secondary goal is to study the qualityof annotations that we can expect to obtain for ourset of low resource languages.
We evaluate the anno-tations both alone and as feedback into our lexiconinduction system.2 Inducing Translation CandidatesVarious linguistic and corpus cues are helpful for re-lating word translations across a pair of languages.A plethora of prior work has exploited orthographic,topic, and contextual similarity, to name a few(Rapp, 1999; Fung and Yee, 1998; Koehn andKnight, 2000; Mimno et al, 2009; Schafer andYarowsky, 2002; Haghighi et al, 2008; Garera etal., 2008).
In this work, our aim is to induce trans-lation candidates for further MTurk annotation for alarge number of language pairs with varying degreesof relatedness and resource availability.
Therefore,we opt for a simple and language agnostic approachof using contextual information to score translationsand discover a set of candidates for further anno-tation.
Table 1 shows our 42 languages of interestand the number of Wikipedia articles with interlin-gual links to their English counterparts.
The ideais that tokens which tend to appear in the contextof a given type in one language should be similarto contextual tokens of its translation in the otherlanguage.
Each word can thus be represented as a108Tigrinya 36 Punjabi 401Kyrgyz 492 Somali 585Nepali 1293 Tibetan 1358Uighur 1814 Maltese 1896Turkmen 3137 Kazakh 3470Mongolian 4009 Tatar 4180Kurdish 5059 Uzbek 5875Kapampangan 6827 Urdu 7674Irish 9859 Azeri 12568Tamil 13470 Albanian 13714Afrikaans 14315 Hindi 14824Bangla 16026 Tagalog 17757Latvian 22737 Bosnian 23144Welsh 25292 Latin 31195Basque 38594 Thai 40182Farsi 58651 Bulgarian 68446Serbian 71018 Indonesian 73962Slovak 76421 Korean 84385Turkish 86277 Ukrainan 91022Romanian 97351 Russian 295944Spanish 371130 Polish 438053Table 1: Our 42 languages of interest and the number ofWikipedia pages for each that have interlanguage linkswith English.vector of contextual word indices.
Following Rapp(1999), we use a small seed dictionary to project1the contextual vector of a source word into the tar-get language, and score its overlap with contextualvectors of candidate translations, see Figure 1.
Topscoring target language words obtained in this man-ner are used as candidate translations for MTurk an-notation.
While longer lists will increase the chanceof including correct translations and their morpho-logical variants, they require more effort on the partof annotators.
To strike a reasonable balance, we ex-tracted relatively short candidate lists, but allowedMTurk users to type their own translations as well.3 Mechanical Turk TaskFollowing previous work on posting NLP tasks onMTurk (Snow et al, 2008; Callison-Burch, 2009),we use the service to gather annotations for proposedbilingual lexicon entries.
For 32 of our 42 languagesof interest, we were able to induce lexical translation1A simple string match is used for projection.
While weexpect that more sophisticated approaches (e.g.
exploiting mor-phological analyses) are likely to help, we cannot assume thatsuch linguistic resources are available for our languages.e(1)e(2)e(3)e(K-1)e(K)e(i)???f(1)f(2)f(3)f(N-1)f(N)???dict.project???????
?f(1)f(2)f(N)compareFigure 1: Lexicon induction using contextual informa-tion.
First, contextual vectors are projected using smalldictionaries and then they are compared with the targetlanguage candidates.candidates and post them on MTurk for annotation.We do not have dictionaries for the remaining ten,so, for those languages, we simply posted a set of100 English words and asked workers for manualtranslations.
We had three distinct workers translateeach word.For the 32 languages for which we proposedtranslation candidates, we divided our set of 100English words into sets of ten English words to becompleted within a single HIT.
MTurk defines HIT(Human Intelligence Task) as a self-contained unitof work that requesters can post and pay workers asmall fee for completing.
We requested that threeMTurk workers complete each of the ten HITs foreach language.
For each English word within a HIT,we posted ten candidate translations in the foreignlanguage and asked users to check the boxes besideany and all of the words that were translations of theEnglish word.
We paid workers $0.10 for complet-ing each HIT.
If our seed dictionary included an en-try for a given English word, we included that in thecandidate list as a positive control.
Additionally, weincluded a random word in the foreign language asa negative control.
The remaining eight or nine can-didate translations were proposed by our inductionsystem.
We randomized the order in which the can-didates appeared to workers and presented the wordsas images rather than text to discourage copying andpasting into online translation systems.In addition to gathering annotations on candidate109!
"#$%&'"$()#&*(%()+&,-.%"$%&/%0$+(%"&1%2)#-"$%& 3-")&4%(5$%&6%"78%#)+9&/9$8$::$")+&;(9)<&Figure 2: Distribution of MTurk workers around theworldtranslations, we gathered the following informationin each HIT:?
Manual translations of each English word, es-pecially for the cases where none of our pro-posed candidate translations were accurate?
Geographical locations via IP addresses?
How the HIT was completed: knowledge of thelanguages, paper dictionary, online dictionary?
Whether the workers were native speakers ofeach language (English and foreign), and forhow many years they have spoken each4 ResultsFigure 2 shows the percent of HITs that were com-pleted in different countries.
More than 60% of HITswere completed by workers in India, more than halfof which were completed in the single city of Chen-nai.
Another 18% were completed in the UnitedStates, and roughly 2% were completed in Romania,Pakistan, Macedonia, Latvia, Bangladesh, and thePhilippines.
Of all annotations, 54% reported thatthe worker used knowledge of the two languages,while 28% and 18% reported using paper and onlinedictionaries, respectively, to complete the HITs.Ninety-three MTurk workers completed at leastone of our HITs, and 53 completed at least two.The average number of HITs completed per workerwas 12.
One worker completed HITs for 17 differ-ent languages, and nine workers completed HITs inmore than three languages.
Of the ten prolific work-ers, one was located in the United States, one in theUnited Kingdom, and eight in India.
Because weposted each HIT three times, the minimum numberof workers per language was three.
Exactly threeworkers completed all ten HITs posted in the fol-lowing languages: Kurdish, Maltese, Tatar, Kapam-pangan, Uzbek, and Latvian.
We found that the av-erage number of workers per language was 5.2.
Tendistinct workers (identified with MTurk worker IDs)completed Tamil HITs, and nine worked on the FarsiHITs.4.1 Completion TimeFigure 3 shows the time that it took for our HITsfor 37 languages to be completed on MTurk.
TheHITs for the following languages were posted for aweek and were never completed: Tigrinya, Uighur,Tibetan, Kyrgyz, and Kazakh.
All five of the un-completed HIT sets required typing annotations, amore time consuming task than checking transla-tion candidates.
Not surprisingly, languages withmany speakers (Hindi, Spanish, and Russian) andlanguages spoken in and near India (Hindi, Tamil,Urdu) were completed very quickly.
The languagesfor which we posted a manual translation only HITare marked with a * in Figure 3.
The HIT type doesnot seem to have affected the completion time.4.2 Annotation QualityLexicon Check Agreement.
Figure 4 shows thepercent of positive control candidate translationsthat were checked by the majority of workers (atleast two of three).
The highest amounts of agree-ment with the controls were for Spanish and Polish,which indicates that those workers completed theHITs more accurately than the workers who com-pleted, for example, the Tatar and Thai HITs.
How-ever, as already mentioned, the seed dictionaries arevery noisy, so this finding may be confounded bydiscrepancies in the quality of our dictionaries.
Thenoisy dictionaries also explain why agreement withthe positive controls is, in general, relatively low.We also looked at the degree to which workersagreed upon negative controls.
The average per-cent agreement between the (majority of) workersand the negative controls over all 32 languages isonly 0.21%.
The highest amount of agreement withnegative controls is for Kapampangan and Turkmen(1.28% and 1.26%, respectively).
These are two of1100?20?40?60?80?100?120?140?160?Hindi*?Spanish?Russian?Tamil?Urdu*?Turkish?Romanian?Ukrainan*?Latvian?
Polish?Albanian?Afrikaans?Bulgarian?
Farsi?Indonesian?Welsh?Slovak?Tagalog?Maltese?Serbian?Uzbek?
Thai?Bosnian?Korean?Nepali*?Irish?Mongolian*?Azeri?Punjabi?Bangla?
Tatar?Kapampangan?Kurdish?La?
?Turkmen?Somali?Basque?Hours?Time?to?complete?HITs?Time?to?first?hit?Figure 3: Number of hours HITs posted on MTurk before completion; division of the time between posting and thecompletion of one HIT and the time between the completion of the first and last HIT shown.
HITs that required lexicaltranslation only (not checking candidate translations) are marked with an *.the languages for which there was little agreementwith the positive controls, substantiating our claimthat those HITs were completed less accurately thanfor other languages.Manual Translation Agreement.
For each En-glish word, we encouraged workers to manually pro-vide one or more translations into the foreign lan-guage.
Figure 5 shows the percent of English wordsfor which the MTurk workers provided and agreedupon at least one manual translation.
We definedagreement as exact string match between at leasttwo of three workers, which is a conservative mea-sure, especially for morphologically rich languages.As shown, there was a large amount of agreementamong the manual translations for Ukrainian, Farsi,Thai, and Korean.
The MTurk workers did not pro-vide any manual translations at all for the followinglanguages: Somali, Kurdish, Turkmen, Uzbek, Ka-pampangan, and Tatar.It?s easy to speculate that, despite discouragingthe use of online dictionaries and translation systemsby presenting text as images, users reached this highlevel of agreement for manual translations by usingthe same online translation systems.
However, wesearched for 20 of the 57 English words for whichthe workers agreed upon a manually entered Russiantranslation in Google translate, and we found that theRussian translation was the top Google translationfor only 11 of the 20 English words.
Six of the Rus-sian words did not appear at all in the list of trans-lations for the given English word.
Thus, we con-clude that, at least for some of our languages of in-terest, MTurk workers did provide accurate, human-generated lexical translations.4.3 Using MTurk Annotations in InductionTo further test the usefulness of MTurk generatedbilingual lexicons, we supplemented our dictionar-ies for each of the 37 languages for which we gath-ered MTurk annotations with translation pairs thatworkers agreed were good (both chosen from thecandidate set and manually translated).
We com-pared seed dictionaries of size 200 with those sup-plemented with, on average, 69 translation pairs.
Wefound an average relative increase in accuracy ofour output candidate set (evaluated against completeavailable dictionaries) of 53%.
This improvement isfurther evidence that we are able to gather high qual-ity translations from MTurk, which can assist thelexicon induction process.
Additionally, this showsthat we could iteratively produce lexical translationcandidates and have MTurk workers annotate them,supplementing the induction dictionaries over manyiterations.
This framework would allow us to gener-1110?5?10?15?20?25?Spanish?Polish?Bosnian?Romanian?
Irish?Bulgarian?Tagalog?Russian?Afrikaans?Welsh?Tamil?Maltese?Turkish?Latvian?Punjabi?Indonesian?Azeri?Slovak?La??Albanian?Uzbek?Basque?Serbian?Turkmen?Korean?Bangla?Somali?Kurdish?Farsi?Kapampangan?Tatar?
Thai?Percent?agreement?on??
?posi?ve?controls?Figure 4: Percent of positive control candidate translations for which two or three workers checked as accurate.0?10?20?30?40?50?60?70?80?90?100?Ukrainan*?Farsi?
Thai?Korean?Irish?Maltese?Albanian?Slovak?Latvian?Turkish?Serbian?Welsh?Afrikaans?Indonesian?Hindi*?Polish?Russian?Urdu*?Tagalog?Romanian?Mongolian*?Spanish?Bulgarian?Bosnian?Azeri?Nepali*?La??
Tamil?Punjabi?Bangla?Basque?Somali?Kurdish?Turkmen?Uzbek?Kapampangan?Tatar?Percent?of?English?words?with?agreed?upon?
?manual?transla?ons?Figure 5: Percent of 100 English words for which at least two of three MTurk workers provided at least one matchingmanual translation; HITs that required lexical translation only (not checking candidate translations) are marked withan *.ate very large and high quality dictionaries startingwith a very small set of seed translation pairs.5 ConclusionThe goal of this work was to use Amazon?s Mechan-ical Turk to collect and evaluate the quality of trans-lation lexicons for a large set of low resource lan-guages.
In order to make the annotation task easierand maximize the amount of annotation given ourbudget and time constraints, we used contextual sim-ilarity along with small bilingual dictionaries to ex-tract a set of translation candidates for MTurk anno-tation.
For ten of our languages without dictionaries,we asked workers to type translations directly.
Wewere able to get complete annotations of both typesquickly for 37 of our languages.
The other five lan-guages required annotations of the latter type, whichmay explain why they remained unfinished.We used annotator agreement with positive andnegative controls to assess the quality of generatedlexicons and provide an indication of the relativedifficulty of obtaining high quality annotations foreach language.
Not surprisingly, annotation agree-ment tends to be low for those languages which areespecially low resource, as measured by the num-ber of Wikipedia pages.
Because there are relativelyfew native speakers of these languages in the on-line community, those HITs were likely completedby non-native speakers.
Finally, we demonstratedthat augmenting small seed dictionaries with the ob-tained lexicons substantially impacts contextual lex-icon induction with an average relative gain of 53%in accuracy across languages.In sum, we found that the iterative approach of au-112tomatically generating noisy annotation and askingMTurk users to correct it to be an effective means ofobtaining supervision.
Our manual annotation tasksare simple and annotation can be obtained quicklyfor a large number of low resource languages.ReferencesChris Callison-Burch.
2009.
Fast, cheap, and creative:Evaluating translation quality using amazons mechan-ical turk.
In Proceedings of EMNLP.Pascale Fung and Lo Yuen Yee.
1998.
An IR approachfor translating new words from nonparallel, compara-ble texts.
In Proceedings of ACL, pages 414?420.Nikesh Garera, Chris Callison-Burch, and DavidYarowsky.
2008.
Improving translation lexicon induc-tion from monolingual corpora via dependency con-texts and part-of-speech equivalences.
In Proceedingsof CoNLL.Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,and Dan Klein.
2008.
Learning bilingual lexiconsfrom monolingual corpora.
In Proceedings of ACL,pages 771?779.Philipp Koehn and Kevin Knight.
2000.
Estimating wordtranslation probabilities from unrelated monolingualcorpora using the EM algorithm.
In Proceedings ofAAAI.David Mimno, Hanna Wallach, Jason Naradowsky, DavidSmith, and Andrew McCallum.
2009.
Polylingualtopic models.
In Proceedings of EMNLP.Reinhard Rapp.
1999.
Automatic identification of wordtranslations from unrelated English and German cor-pora.
In Proceedings of ACL, pages 519?526.Charles Schafer and David Yarowsky.
2002.
Induc-ing translation lexicons via diverse similarity mea-sures and bridge languages.
In Proceedings of CoNLL,pages 146?152.Rion Snow, Brendan OConnor, Daniel Jurafsky, and An-drew Y. Ng.
2008.
Cheap and fast - but is it good?evaluating non-expert annotations for natural languagetasks.
In Proceedings of EMNLP.113
