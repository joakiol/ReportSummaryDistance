The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 95?104,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsPredicting Learner Levels for Online Exercises of HebrewMarkus Dickinson, Sandra Ku?bler, Anthony MeyerIndiana UniversityBloomington, IN, USA{md7,skuebler,antmeyer}@indiana.eduAbstractWe develop a system for predicting the level oflanguage learners, using only a small amountof targeted language data.
In particular, wefocus on learners of Hebrew and predict levelbased on restricted placement exam exercises.As with many language teaching situations, amajor problem is data sparsity, which we ac-count for in our feature selection, learning al-gorithm, and in the setup.
Specifically, we de-fine a two-phase classification process, isolat-ing individual errors and linguistic construc-tions which are then aggregated into a secondphase; such a two-step process allows for easyintegration of other exercises and features inthe future.
The aggregation of informationalso allows us to smooth over sparse features.1 Introduction and MotivationSeveral strands of research in intelligent computer-assisted language learning (ICALL) focus on deter-mining learner ability (Attali and Burstein, 2006;Yannakoudakis et al, 2011).
One of the tasks, de-tecting errors in a range of languages and for a rangeof types of errors, is becoming an increasingly popu-lar topic (Rozovskaya and Roth, 2011; Tetreault andChodorow, 2008); see, for example, the recent HOO(Helping Our Own) Challenge for Automated Writ-ing Assistance (Dale and Kilgarriff, 2011).
Onlyrarely has there been work on detecting errors inmore morphologically-complex languages (Dickin-son et al, 2011).In our work, we extend the task to predicting thelearner?s level based on the errors, focusing on He-brew.
Our system is targeted to be used in a uni-versity setting where incoming students need to beplaced into the appropriate language level?i.e., theappropriate course?based on their proficiency inthe language.
Such a level prediction system for He-brew faces a number of challenges: 1) unclear cor-respondence between errors and levels, 2) missingNLP resources, and, most critically, 3) data sparsity.Placing learners into levels is generally done bya human, based on a written exam (e.g.
(Fulcher,1997)).
To model the decision process automati-cally, we need to understand how the types of er-rors, as well as their frequencies, correspond tolearner levels.
There is only little work investigat-ing this correspondence formally (see (Hawkins andFilipovic?, 2010; Alexopoulou et al, 2010) for dis-cussion) and only on error-annotated English learnercorpora.
For this reason, we follow a data-drivenapproach to learn the correspondence between er-rors and levels, based on exercises from writtenplacement exams.
Although the exact exercises willvary across languages and language programs, themethodology is widely applicable, as developinga small set of exercises requires minimal effort?effort already largely expended for paper exams.Currently, we focus on an exercise in which thelearner has to order a set of words into a grammat-ical sentence.
Our goal is to move towards freerlanguage production and to analyze language pro-ficiency through more variables, but, in the interestof practicality, we start in a more restricted way.For lesser-resourced languages, there is generallylittle data and few NLP resources available.
For He-brew, for example, we must create our own pool of95learner data, and while NLP tools and resources ex-ist (Goldberg and Elhadad, 2011; Yona and Wintner,2008; Itai and Wintner, 2008), they are not adaptedfor dealing with potentially ill-formed learner pro-ductions.
For this reason, we are performing linguis-tic analysis on the gold standard answers to obtainoptimal linguistic analyses.
Then, the system alignsthe learner answer to the gold standard answer anddetermines the types of deviations.Since Hebrew is a less commonly taught language(LCTL), we have few placement exams from whichto learn correspondences.
Compounding the datasparsity problem is that each piece of data is com-plex: if a learner produces an erroneous answer,there are potentially a number of ways to analyze it(cf.
e.g.
(Dickinson, 2011)).
An error could feature,for instance, a letter inserted in an irregular verbstem, or between two nouns; any of these proper-ties may be relevant to describing the error (cf.
howerrors are described in different taxonomies, e.g.(D?
?az-Negrillo and Ferna?ndez-Dom?
?nguez, 2006;Boyd, 2010)).
Specific error types are unlikely torecur, making sparsity even more of a concern.
Wethus need to develop methods which generalize well,finding the most useful aspects of the data.Our system is an online system to be used at theHebrew Language Program at our university.
Thesystem is intended to semi-automatically place in-coming students into the appropriate Hebrew course,i.e., level.
As with many exams, the main purpose isto ?reduce the number of students who attend an oralinterview?
(Fulcher, 1997).2 The DataExercise type We focus on a scrambled sentenceexercise, in which learners are given a set of well-formed words and must put them into the correct or-der.
For example, given (1), they must produce oneof the correct choices in (2).
This gives them theopportunity to practice skills in syntactic ordering.1(1) barC beph dibrw hybrit ieral la tmid(2) a. lanottmidalwaysdibrwspokebephin-the-languagehybritthe-HebrewbarCin-land-ofieralIsrael..1We follow the transliteration scheme of the Hebrew Tree-bank (Sima?an et al, 2001).
?They did not always speak in the He-brew language in the land of Israel.?b.
barC ieral la dibrw tmid beph hybrit .c.
la tmid dibrw barC ieral beph hybrit .
(3) barC ieral la tmid dibrw beph hybriM .Although the lexical choice is restricted?in thatlearners are to select from a set of words?learnersmust write the words.
Thus, in addition to syntacticerrors, there is possible variation in word form, as in(3), where hybrit is misspelled as hybriM.This exercise was chosen because: a) it has beenused on Hebrew placement exams for many years;and b) the amount of expected answers is con-strained.
Starting here also allows us to focus lesson the NLP preprocessing and more on designinga machine learning set-up to analyze proficiency.It is important to note that the proficiency level isdetermined by experts looking at the whole exam,whereas we are currently predicting the proficiencylevel on the basis of a single exercise.Placement exams The data for training and test-ing is pooled from previous placement exams at ouruniversity.
Students who intend to take Hebrew havein past years been given written placement exams,covering a range of question types, including scram-bled sentences.
The learners are grouped into thefirst to the sixth semester, or they test out.
We areusing the following levels: the first four semesters(100, 150, 200, 250), and anything above (300+).We use a small set of data?38 learners covering128 sentences across 11 exercises?all the data thatis available.
While this is very small, it is indicativeof the type of situation we expect for resource-poorlanguages, and it underscores the need to developmethods appropriate for data-scarce situations.
(Manual) annotation For each of the 11 uniqueexercises, we annotate an ordered list of correct an-swers, ranked from best to worst.
Since Hebrew pos-sesses free word order, there are between 1 and 10correct answers per exercise, with an average of 3.4gold standard answers.
The sentences have between8 and 15 words, with an average of 9.7 words per ex-ercise.
This annotation concerns only the gold stan-dard answers.
It requires minimal effort and needsto be performed only once per exercise.96T09: SURFACE qnwSEGMENTATION (VB-BR3V qnw)PRE_PARTICLES -MAIN_WORD:INDICES 0,1,2,TAG VB-BR3VBINYAN PAALINFL_PREFIX -STEM 0,1,ROOT 0,1,h,INFL_SUFFIX 2,PRO_SUFFIX -Figure 1: An example annotated word for qnw (?bought?
),token T09 in one particular exerciseTo annotate, we note that all the correct answersshare the same set of words, varying in word or-der and not in morphological properties.
Thus,we store word orders separately from morphologi-cal annotation, annotating morphology once for allpossible word orders.
An example of morpholog-ical annotation is given in fig.
1 for the verb qnw(?bought?).
Segmentation information is providedby referring to indices (e.g., STEM 0,1), while TAGand BINYAN provide morphosyntactic properties.Since the annotation is on controlled, correct data,i.e., not potentially malformed learner data, we canexplore automatically annotating exercises in the fu-ture, as we expect relatively high accuracy.3 System overviewThe overall system architecture is given in fig.
2; theindividual modules are described below.
In brief,we align a learner sentence with the gold standard;use three specialized classifiers to classify individ-ual phenomena; and then combine the informationfrom these classifiers into an overall classifier for thelearner level.
This means the classification is per-formed in two phases: the first phase looks at indi-vidual phenomena (i.e., errors and other properties);the second phase aggregates all phenomena of onelearner over all exercises and makes a final decision.4 Feature extractionTo categorize learners into levels, we first need to ex-tract relevant information from each sentence.
Thatis, we need to perform a linguistic and/or error anal-ysis on each sentence, which can be used for classi-Learnersentence(L)AlignmentGoldstandardanswers(G1 .
.
.
G2)FeatureextractionIntertokenerrorsIntratokenerrorsGlobalfeaturesIntra-ClassifierInter-ClassifierGlobal-ClassifierClassifiedintravectorsClassifiedintervectorsClassifiedglobalvectorsLearnerclassifierL?
GiFigure 2: Overall system architecture (boxes = systemcomponents, circles = data)fication (sec.
5).
Although we extract features forclassification, this analysis could also be used forother purposes, such as providing feedback.4.1 Phenomena of interestWe extract features capturing individual phenom-ena.
These can be at the level of individual words,bigrams of words, or anything up to a whole sen-tence; and they may represent errors or correctly-produced language.
Importantly, at this stage, eachphenomenon is treated uniquely and is not combinedor aggregated until the second phase (see sec.
5).While features can be based on individual phe-nomena of any type, we base our extracted featureslargely upon learner errors.
Errors have been shownto have a significant impact on predicting learnerlevel (Yannakoudakis et al, 2011).
To detect errors,we align the learner sentence with a gold standardand extract the features.
Although we focus on er-rors, we model some correct language (sec.
4.3.3).4.2 Token alignmentWith a listing of correct answers, we align thelearner sentence to the answer which matches best:97We iterate over the correct answers and align learnertokens with correct tokens, based on the cost of map-ping one to the other.
The aligned sentence is theone with the lowest overall cost.
The cost between asource token ws and target token wt accounts for:1.
Levenshtein distance between ws & wt (Lev)2. similiarity between ws & wt (longest commonsubsequence (LCSq) & substring (LCSt))3. displacement between ws & wt (Displ)This method is reminiscent of alignment ap-proaches in paraphrasing (e.g.
(Grigonyte` et al,2010)), but note that our problem is more restrictedin that we have the same number of words, and inmost cases identical words.
We use different dis-tance and similarity metrics, to ensure robustnessacross different kinds of errors.
The third metric isthe least important, as learners can shift tokens farfrom their original slot, and thus it is given a lowweight.
The only reason to use it at all is to distin-guish cases where more than one target word is astrong possibility, favoring the closer one.The formula for the cost between source and tar-get words ws and wt is given in (4), where the dis-tance metrics are averaged and normalized by thelength of the target word wt.
This length is also usedto convert the similarity measures into distances, asin (5).
We non-exhaustively tested different weightdistributions on about half the data, and our final setis given in (6), where slightly less weight is givenfor the longest common substring and only a minoramount for the displacement score.
(4) cost(ws, wt) = ?1Displ(ws, wt) +?2Lev(ws,wt)+?3dLCSq(ws,wt)+?4dLCSt(ws,wt)3?len(wt)(5) dLCS(ws, wt) = len(wt)?
LCS(ws, wt)(6) ?1 = 0.05; ?2 = 1.0; ?3 = 1.0; ?4 = 0.7In calculating Levenshtein distance, we hand-created a small table of weights for insertions, dele-tions, and substitutions, to reflect likely modifica-tions in Hebrew.
All weights can be tweaked in thefuture, but we have observed good results thus far.The total alignment is the one which minimizesthe total cost (7).
A is an alignment between thelearner sentence s and a given correct sentence t.Alignments to NULL have a cost of 0.6, so thatwords with high costs can instead align to nothing.
(7) align = argminA?
(ws,wt)?A cost(ws, wt)4.3 Extracted featuresWe extract three different types of features; as thesehave different feature sets, we correspondingly havethree different classifiers, as detailed in sec.
5.1.They are followed by a fourth classifier that talliesup the results of these three classifiers.4.3.1 Intra-token featuresBased on the token alignments, it is straightfor-ward to calculate differences within the tokens andthus to determine values for many features (e.g., adeleted letter in a prefix).
We calculate such intra-token feature vectors for each word-internal error.For instance, consider the learner attempt (8b) forthe target in (8a).
We find in the learner answer twointra-token errors: one in hmtnwt (cf.
hmtnh), wherethe fem.pl.
suffix -wt has been substituted for thefem.sg.
ending -h, and another in hnw (cf.
qnw),where h has been substituted for q.
These two errorsyield the feature vectors presented as example casesin table 1.
(8) a.
haMQhNthey.FEMeilmwpaidhrbhmuchksPmoneybebilforhmtnh1the-giftehNwhich-they.FEMqnw2bought??
?Did they pay much money for the giftthat they bought??b.
haM hN eilmw hrbh ksP bebil hmtnwt1ehN hnw2 ?Features 1 and 11 in table 1 are the POS tags of themorphemes preceding and following the erroneousmorpheme, respectively.
The POS tag of the mor-pheme containing the error is given by feature 2, andits person, gender, and number by feature 3.
The re-maining features describe the error itself (f. 6?8), aswell as its word-internal context, i.e., both its left (f.4?5) and right (f. 9?10) contexts.The context features refer to individual characterslots, which may or may not be occupied by actualcharacters.
For example, since the error in hmtnwtis word-final, its two right-context slots are empty,hence the ?#?
symbol for both features 9 and 10.The feature values for these character slots aregenerally not literal characters, but rather abstract la-bels representing various categories, most of which98Features hnw hmtnwt1.
Preceding POS PRP H2.
Current POS VB NN3.
Per.Gen.Num.
3cp -fs4.
Left Context (2) # R25.
Left Context (1) # R36.
Source String h wt7.
Target String q h8.
Anomaly h?q wt?h9.
Right Context (1) R2 #10.
Right Context (2) INFL-SFX #11.
Following POS yyQM RELTable 1: Intra-token feature categoriesare morphological in nature.
In hmtnwt, for exam-ple, the two left-context characters t and n are thesecond and third radicals of the root, hence the fea-ture values R2 and R3, respectively.4.3.2 Inter-token featuresThe inter-token features encode anomalies whosescope is not confined to a particular token.
Suchanomalies include token displacements and missingtokens.
We again use the Levenshtein algorithm todetect inter-token anomalies, but we disable the sub-stitution operation here so that we can link up corre-sponding deletions and insertions to yield ?shifts.
?For example, suppose the target is A B C D, andthe learner has D A B C. Without substitutions, theminimal cost edit sequence is to delete D from thebeginning of the learner?s input and insert D at theend.
Merging the two operations results in a D shift.The learner sentence in (9b) shows two inter-token anomalies with respect to the target in (9a).First, the learner has transposed the two tokens insequence 1, namely the verb dibrw (?speak-PAST?
)and the adverb tmid (?always?).
Second, sequence 2(the PP beph hybrit, ?in the Hebrew language?)
hasbeen shifted from its position in the target sentence.
(9) a. barCin-land-ofieralIsraellanotdibrw1speak-PASTtmid1alwaysbeph2in-the-languagehybrit2the-Hebrew..b. barC ieral beph2 hybrit2 la tmid1dibrw1 .Table 2 presents the inter-token feature vectorsfor the two anomalies in (9b).
After Anomaly,Features Seq.
1 Seq.
21.
Anomaly TRNS SHFT2.
Sequence Label RB?VP PP3.
Head Per.Gen.Num.
3cp ---4.
Head POS.
(Binyan) VB.PIEL IN5.
Sequence-Initial POS VB IN6.
Sequence-Final POS RB JJ7.
Left POS (Learner) RB NNP8.
Right POS (Learner) IN RB9.
Left POS (Target) RB RB10.
Right POS (Target) IN yyDOT11.
Sequence Length 2 212.
Normalized Error Cost 0.625 0.25013.
Sent-Level@Rank 200@2 200@2Table 2: Inter-token feature categoriesthe next three features provide approximations ofphrasal properties, e.g., the phrasal category andhead, based on a few syntactically-motivated heuris-tics.
Sequence Label identifies the lexical or phrasalcategory of the shifted token/token-sequence (e.g.,PP).
Note that sequence labels for transpositions arespecial cases consisting of two category labels sep-arated by an arrow.
Head Per.Gen.Num and HeadPOS.
(Binyan) represent the morphosyntactic prop-erties of the sequence?s (approximate) head word,namely its person, gender, and number, and its POStag.
If the head is a verb, the POS tag is followed bythe verb?s binyan (i.e., verb class), as in VB.PIEL.The cost feature, Normalized Error Cost, is com-puted as follows: for missing, extra, and transposedsequences, the cost is simply the sequence lengthdivided by the sentence length.
For shifts, the se-quence length and the shift distance are summedand then divided by the sentence length.
Sent-Level@Rank indicates both the difficulty level of theexercise and the word-order rank of target sentenceto which the learner sentence has been matched.4.3.3 Global featuresIn addition to errors, we also look at global fea-tures capturing global trends in a sentence, in orderto integrate information about the learner?s overallperformance on a sentence.
For example, we notethe percentage of target POS bigrams present in thelearner sentence (POS recall).
Table 3 presents theglobal features.
The two example feature vectors arethose for the sentences (8b) and (9b) above.99Features Ex.
(8b) Ex.
(9b)1.
POS Bigram Recall 2.000 1.2732.
LCSeq Ratio 2.000 1.2503.
LCStr Ratio 1.200 0.5004.
Relaxed LCStr Ratio 2.000 0.5005.
Intra-token Error Count 1.500 0.0006.
Inter-token Error Count 0.000 1.5007.
Intra-token Net Cost 1.875 0.0008.
Norm.
Aggregate Displ.
0.000 0.4229.
Sentence Level 200 200Table 3: Global feature categoriesExcept for feature 9 (Sentence Level), every fea-ture in table 3 is multiplied by a weight derived fromthe sentence level.
These weights serve either to pe-nalize or compensate for a sentence?s difficulty, de-pending on the feature type.
Because features 1?4 are ?positive?
measures, they are multiplied bya factor proportional to the sentence level, namelyl = 1. .
.
4, whose values correspond directly to thelevels 150?300+, respectively.
Features 5?8, in con-trast, are ?negative?
measures, so they are multipliedby a factor inversely proportional to l, namely 5?l4 .Among the positive features, LCSeq looks for thelongest common subsequence between the learnersentence and the target, while LCStr Ratio and Re-laxed LCStr Ratio both look for longest commonsubstrings.
However, Relaxed LCStr Ratio allowsfor token-internal anomalies (as long as the token it-self is present) while LCStr Ratio does not.As for the negative features, the two Error Countfeatures simply tally up the errors of each typepresent in the sentence.
The Intra-token Net Costsums over the token-internal Levenshtein distancesbetween corresponding learner and target tokens.Normalized Aggregate Displacement is the sum ofinsertions and deletions carried out during inter-token alignment, normalized by sentence length.5 Two-phase classificationTo combine the features for individual phenomena,we run a two-phase classifier.
In the first phase, weclassify each feature vector for each phenomenoninto a level.
In the second phase, we aggregate overthis output to classify the overall learner level.We use two-phase classification in order to: 1)modularize each individual phenomenon, mean-ing that new phenomena are more easily incorpo-rated into future models; 2) better capture sparsely-represented phenomena, by aggregating over them;and 3) easily integrate other exercise types simplyby having more specialized phase 1 classifiers andby then integrating the results into phase 2.One potential drawback of two-phase classifica-tion is that of not having gold standard annotation ofphase 1 levels or even knowing for sure whether in-dividual phenomena can be classified into consistentand useful categories.
That is, even if a 200-levellearner makes an error, that error is not necessarily a200-level error.
We discuss this next.5.1 Classifying individual phenomenaWith our three sets of features (sec.
4), we set upthree classifiers.
Depending upon the type, the ap-propriate classifier is used to categorize each phase1 vector.
For classification, every phase 1 vector isassigned a single learner level.
However, this as-sumes that each error indicates a unique level, whichis not always true.
A substitution of i for w, for ex-ample, may largely be made by 250-level (interme-diate) learners, but also by learners of other levels.One approach is to thus view each phenomenon asmapping to a set of levels, and for a new vector, clas-sification predicts the set of appropriate levels, andtheir likelihood.
Another approach to overcome thefact that each uniquely-classified phenomenon canbe indicative of many levels is to rely on phase 2to aggregate over different phenomena.
The advan-tage of the first approach is that it makes no assump-tions about individual phenomena being indicativeof a single level, but the disadvantage is that onemay start to add confusion for phase 2 by includ-ing less relevant levels, especially when using littletraining data.
The second approach counteracts thisconfusion by selecting the most prototypical levelfor an individual phenomenon (cf.
criterial featuresin (Hawkins and Buttery, 2010)), giving less noiseto phase 2.
We may lose important non-best levelinformation, but as we show in sec.
6, with a rangeof classifications from phase 1, the second phase canlearn the proper learner level.In either case, from the perspective of training,an individual phenomenon can be seen, in terms oflevel, as the set of learners who produced such a phe-nomenon.
We thus approximate the level of each100Feature type Feature type1.
100-level classes 7.
Intra-token error sum2.
150-level classes 8.
Inter-token error sum3.
200-level classes 9.
Sentences attempted4.
250-level classes 10.
250-level attempts5.
300-level classes 11.
300-level attempts6.
Composite errorTable 4: Feature categories for learner level predictionphenomenon by using the level of the learner fromthe gold standard training data.
This allows us not tomake a theoretical classification of phenomena (asopposed to taxonomically labeling phenomena).5.2 Predicting learner levelWe aggregate the information from phase 1 classifi-cation to classify overall learner levels.
We assumethat the set of all individual phenomena and theirquantities (e.g., proportion of phenomena classifiedas 200-level in phase 1) characterize a learner?s level(Hawkins and Buttery, 2010).
The feature typesare given in table 4.
Features 1?6 are discussed insec.
6.1; features 7?8 are (normalized) sums; and therest record the number of sentences attempted, bro-ken down by intended level of the sentence.
Lower-level attempts are not included, as they are the samevalues for nearly all students.
When we incorporateother exercise types in the future, additional featurescan be added?and the current features modified?to fold in information from those exercise types.An example To take an example, one of our(300+) learners attempts four sentences, giving foursets of global features, and makes four errors, fora total of eight phase 1 individual phenomena.
Onephenomenon is automatically classified as 100-level,one as 150, four as 200, one as 250, and one as 300+.Taking the 1-best phase 1 output (see section 6.3),the phase 2 vector in this case is as in (10a), corre-sponding directly to the features in table 4.
(10) a.
0.25, 0.25, 1.00, 0.25, 0.25, 2.00, 0.50,0.50, 4, 1, 0b.
0.25, 0.00, 1.00, 0.25, 0.00, 1.625, 0.00,0.50, 4, 1, 0In training, we find a 300+-level learner with avery similar vector, namely that of (10b).
Dependingupon the exact experimental set-up (e.g., k2 = 1,see section 6.3), then, this vector helps the system tocorrectly classify our learner as 300+.6 Evaluation6.1 Details of the experimentsWe use TiMBL (Daelemans et al, 2010; Daelemanset al, 1999), a memory-based learner (MBL), forboth phases.
We use TiMBL because MBL has beenshown to work well with small data sets (Banko andBrill, 2001); allows for the use of both text-basedand numeric features; and does not suffer from afragmented class space.
We mostly use the defaultsettings of TiMBL?the IB1 learning algorithm andoverlap comparison metric between instances?andexperiment with different values of k.For prediction of phenomenon level (phase 1) andlearner level (phase 2), the system is trained on datafrom placement exams previously collected in a He-brew language program, as described in sec.
2.
Withonly 38 learners, we use leave-one-out testing, train-ing on the data from the 37 other learners in orderto run a model on each learner?s sentences.
All ofphase 1 is completed (i.e., automatically analyzed)before training the phase 2 models.
As a baseline,we use the majority class (level 150); choosing thisfor all learners gives an accuracy of 34.2% (13/38).2Phase 1 probability distributions BecauseTiMBL retrieves all neighbor with the k nearestdistances rather than the k nearest neighbors, wecan use the number of neighbors in phase 1 to adjustthe values of, e.g., 150-level classes.
For example,the output from phase 1 for two different vectorsmight be as in (11).
Both have a distribution of 23150-level and 13 200-level; however, in one case,this is based on 6 neighbors, whereas for the other,there are 12 neighbors within the nearest distance.
(11) a) 150:4, 200:2 b) 150:8, 200:4With more data, we may have more confidencein the prediction of the second case.
The classesfeatures (fx) of table 4 are thus calculated as in(12), multiplying counts of each class (c(x)) by theirprobabilities (p(x)).2We are aware that the baseline is not very strong, but theonly alternative would be to use a classifier since we observedno direct correlation between level and number of errors.101k Intra Inter Global Overall1 28.1% 38.6% 34.4% 34.7%3 34.2% 44.6% 44.6% 41.9%5 34.2% 37.1% 36.7% 36.3%Table 5: Phase 1 accuracies(12) fx =?phase1c(x)p(x)The Composite error feature combines all classesfeatures into one score, inversely weighing them bylevel, so that more low-level errors give a high value.6.2 Predicting phenomena levelsWe first evaluate phase 1 accuracy, as in table 5.
Us-ing k = 3 gives the best phase 1 result, 41.9%.
Weevaluate with respect to the single-best class, i.e.,the level of the learner of interest.
Accuracy is thepercentage of correctly-classified instances out of allinstances.
We assume an instance is classified cor-rectly if its class corresponds to the learner level.Accuracy is rather low, at 41.9%.
However, wemust bear in mind that we cannot expect 100% accu-racy, given that individual phenomena do not clearlybelong to a single level.
Intra-token classification islowest, likely due to greater issues of sparsity: ran-dom typos are unlikely to occur more than once.6.3 Predicting learner levelFor the second phase, we use different settings forphase 1 instances.
The results are shown in table 6.The overall best results are reached using single-bestclassification for phase 1 and k = 1 for phase 2, giv-ing an accuracy of 60.5%.
Note that the best resultdoes not use the best performing setting for phase 1but rather the one with the lowest performance forphase 1.
This shows clearly that optimizing the twophases individually is not feasible.
We obtain thesame accuracy using k = 5 for both phases.Since we are interested in how these two settingsdiffer, we extract confusion matrices for them; theyare shown in table 7.
The matrices show that the in-herent smoothing via the k nearest neighbors leadsto a good performance for lower levels, to the ex-clusion of levels higher than 200.
The higher lev-els are also the least frequent: the k1 = 5/k2 = 5case shows a bias towards the overall distribution oflevels, whereas the 1-best/k2 = 1 setting is morePhase 11-best k1 = 1 k1 = 3 k1 = 5Phase2 Max 42.1 47.4 57.9 42.1k2 = 1 60.5 57.9 36.8 39.5k2 = 3 42.1 44.7 44.7 42.1k2 = 5 39.5 42.1 44.7 60.5Table 6: Phase 2 accuracies for different phase 1 settingsSystem1-best 100 150 200 250 300+ Acc.Gold100 6 1 6/7150 2 7 3 1 7/13200 2 7 1 1 7/11250 1 1 0/2300+ 1 1 3 3/5k=5 100 150 200 250 300+ Acc.Gold100 5 2 5/7150 2 9 2 9/13200 2 9 9/11250 2 0/2300+ 1 4 0/5Table 7: Classification confusion matriceslikely to guess neighboring classes.
In order to betteraccount for incorrect classifications which are closeto the correct answer (e.g., 250 for 200), we alsocalculated weighted kappa for all the results in ta-ble 6.
Based on kappa, the best result is based onthe setting k1 = 1/k2 = 1 (0.647), followed by1-best/k2 = 1 (0.639).
The weighted kappa fork1 = 5/k2 = 5 is significantly lower (0.503).We are also interested in whether we need sucha complex system: phase 1 can outputs a distribu-tion of senses (k1 = n), or we can use the singlebest class as input to phase 2 (1-best).
In a differentvein, phase 2 is a machine learner (k2 = n) trainedon phase 1 classified data, but could be simplifiedto take the maximum phase 1 class (Max).
The re-sults in table 6 show that using the single-best resultfrom phase 1 in combination with k2 = 1 providesthe best results, indicating that phase 2 can properlyaggregate over individual phenomena (see sec.
5.1).However, for all other phase 2 settings, adding thedistribution over phase 1 results increases accuracy.Using the maximum class rather than the machinelearner in phase 2 generally works best in combina-102tion with more nearest neighbors in phase 1, provid-ing a type of smoothing.
However, using the maxi-mum has an overall detrimental effect.While the results may not be robust enough to de-ploy, they are high, given that this is only one type ofexercise, and we have used a very small set of train-ing data.
When performing the error analysis, wefound one student who had attempted only half ofthe sentences?generally a sign of a low level?whowas put into level 300.
We assume this student per-formed better on other exercises in the exam.
Giventhis picture, it is not surprising that our system con-sistently groups this student into a lower level.6.4 Ablation studiesWe are particularly interested in how the differentphases interact, 1) because one major way to expandthe system is to add different exercises and incor-porate them into the second phase, and 2) becausethe results in table 6 show a strong interdependencebetween phases.
We thus performed a set of exper-iments to gauge the effect of different types of fea-tures.
By running ablation studies?i.e., removingone or more sets of features (cf.
e.g.
(Yannakoudakiset al, 2011))?we can determine their relative impor-tance and usefulness.
We run phase 2 (k = 1) usingdifferent combinations of phase 1 classifiers (1-best)as input.
The results are presented in table 8.Intra Inter Global Acc.Y Y Y 60.5%Y Y N 47.4%Y N N 42.1%N Y Y 42.1%N Y N 42.1%Y N Y 36.8%N N Y 34.2%Table 8: Ablation studies, evaluating on phase 2 accuracyPerhaps unsurprisingly, the combination of allfeature types results in the highest results of 60.5%.Also, using only one type of features results in thelowest performance, with the global features beingthe least informative set, on par with the baseline of34.2%.
If we use only two feature sets, removingthe global features results in the least deterioration.Since these features do not directly model errors butrather global sentence trends, this is to be expected.However, leaving out inter-token features results inthe second-lowest results (36.8%), thus showing thatthis set is extremely important?again not surprisinggiven that we are working with an exercise designedto test word order skills.7 Summary and OutlookWe have developed a system for predicting the levelof Hebrew language learners, using only a smallamount of targeted language data.
We have pre-dicted level based on a single placement exam exer-cise, finding a surprising degree of accuracy despitemissing much of the information normally used onsuch exams.
We accounted for the problem of datasparsity by breaking the problem into a two-phaseclassification and through our choice of learning al-gorithm.
The classification process isolates individ-ual errors and linguistic constructions which are thenaggregated into a second phase; such a two-step pro-cess allows for easy integration of other exercisesand features in the future.
The aggregation of infor-mation allows us to smooth over sparse features.In the immediate future, we are integrating otherexercises, to improve the overall accuracy of levelprediction (i.e., the second phase) and make auto-matic testing more valid (cf.
e.g.
(Fulcher, 1997)),while at the same time incorporating more linguisticprocessing for more complex input.
For example,with question formation exercises, no closed set ofcorrect answers exists, and one must use parse treedistance to delineate features.
With multiple exer-cises, we have plans to test the system with incomingstudents to the Hebrew program at our university.AcknowledgmentsWe would like to thank Ayelet Weiss for helpthroughout, as well as Chris Riley and Amber Smith.Part of this work was funded by the IU Jewish Stud-ies Center, as well as by the Institute for Digital Artsand Humanities and the Data to Insight Center at IU.We also thank Stephanie Dickinson from the IndianaStatistical Consulting Center (ISCC) for analysis as-sistance and the three anonymous reviewers for theircomments.103ReferencesDora Alexopoulou, Helen Yannakoudakis, and TedBriscoe.
2010.
From discriminative features to learnergrammars: a data driven approach to learner corpora.Talk given at Second Language Research Forum, Uni-versity of Maryland, October 2010.Yigal Attali and Jill Burstein.
2006.
Automated essayscoring with e-rater v.2.
Journal of Technology, Learn-ing, and Assessment, 4(3), February.Michele Banko and Eric Brill.
2001.
Mitigating thepaucity-of-data problem: Exploring the effect of train-ing corpus size on classifier performance for naturallanguage processing.
In Proceedings of HLT 2001,First International Conference on Human LanguageTechnology Research, pages 253?257, San Diego, CA.Adriane Boyd.
2010.
EAGLE: an error-annotated cor-pus of beginning learner German.
In Proceedings ofLREC-10, Valetta, Malta.Walter Daelemans, Antal van den Bosch, and Jakub Za-vrel.
1999.
Forgetting exceptions is harmful in lan-guage learning.
Machine Learning, 34:11?41.Walter Daelemans, Jakub Zavrel, Ko van der Sloot, andAntal van den Bosch.
2010.
Timbl: Tilburg memorybased learner, version 6.3, reference guide.
Technicalreport, ILK Research Group.
Technical Report Seriesno.
10-01.Robert Dale and Adam Kilgarriff.
2011.
Helping ourown: The HOO 2011 pilot shared task.
In Proceedingsof the Generation Challenges Session at the 13th Eu-ropean Workshop on Natural Language Generation,pages 242?249, Nancy, France, September.Ana D?
?az-Negrillo and Jesu?s Ferna?ndez-Dom??nguez.2006.
Error tagging systems for learner corpora.Spanish Journal of Applied Linguistics (RESLA),19:83?102.Markus Dickinson, Ross Israel, and Sun-Hee Lee.
2011.Developing methodology for Korean particle error de-tection.
In Proceedings of the Sixth Workshop on In-novative Use of NLP for Building Educational Appli-cations, pages 81?86, Portland, OR, June.Markus Dickinson.
2011.
On morphological analysis forlearner language, focusing on Russian.
Research onLanguage and Computation, 8(4):273?298.Glenn Fulcher.
1997.
An English language placementtest: issues in reliability and validity.
Language Test-ing, 14(2):113?138.Yoav Goldberg and Michael Elhadad.
2011.
Joint He-brew segmentation and parsing using a PCFGLA lat-tice parser.
In Proceedings of ACL-HLT, pages 704?709, Portland, OR, June.Gintare` Grigonyte`, Joa?o Paulo Cordeiro, Gae?l Dias, Ru-men Moraliyski, and Pavel Brazdil.
2010.
Para-phrase alignment for synonym evidence discovery.In Proceedings of the 23rd International Conferenceon Computational Linguistics (COLING), pages 403?411, Beijing, China.John A. Hawkins and Paula Buttery.
2010.
Criterial fea-tures in learner corpora: Theory and illustrations.
En-glish Profile Journal, 1(1):1?23.John A. Hawkins and Luna Filipovic?.
2010.
CriterialFeatures in L2 English: Specifying the Reference Lev-els of the Common European Framework.
CambridgeUniversity Press.Alon Itai and Shuly Wintner.
2008.
Language resourcesfor Hebrew.
Language Resources and Evaluation,42(1):75?98.Alla Rozovskaya and Dan Roth.
2011.
Algorithm selec-tion and model adaptation for ESL correction tasks.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, pages 924?933, Portland, OR,June.Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altman,and N. Nativ.
2001.
Building a tree-bank of Mod-ern Hebrew text.
Traitment Automatique des Langues,42(2).Joel Tetreault and Martin Chodorow.
2008.
The ups anddowns of preposition error detection in ESL writing.In Proceedings of COLING-08, Manchester.Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.2011.
A new dataset and method for automaticallygrading ESOL texts.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages180?189, Portland, OR, June.Shlomo Yona and Shuly Wintner.
2008.
A finite-statemorphological grammar of Hebrew.
Natural Lan-guage Engineering, 14(2):173?190.104
