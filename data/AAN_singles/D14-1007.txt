Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 57?67,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsPolicy Learning for Domain Selection in an Extensible Multi-domainSpoken Dialogue SystemZhuoran WangMathematical & Computer SciencesHeriot-Watt UniversityEdinburgh, UKzhuoran.wang@hw.ac.ukHongliang Chen, Guanchun WangHao Tian, Hua Wu?, Haifeng WangBaidu Inc., Beijing, P. R. ChinaSurnameForename@baidu.com?wu hua@baidu.comAbstractThis paper proposes a Markov DecisionProcess and reinforcement learning basedapproach for domain selection in a multi-domain Spoken Dialogue System built ona distributed architecture.
In the proposedframework, the domain selection prob-lem is treated as sequential planning in-stead of classification, such that confir-mation and clarification interaction mech-anisms are supported.
In addition, it isshown that by using a model parameter ty-ing trick, the extensibility of the systemcan be preserved, where dialogue com-ponents in new domains can be easilyplugged in, without re-training the domainselection policy.
The experimental resultsbased on human subjects suggest that theproposed model marginally outperforms anon-trivial baseline.1 IntroductionDue to growing demand for natural human-machine interaction, over the last decade Spo-ken Dialogue Systems (SDS) have been increas-ingly deployed in various commercial applicationsranging from traditional call centre automation(e.g.
AT&T ?Lets Go!?
bus information sys-tem (Williams et al., 2010)) to mobile personalassistants and knowledge navigators (e.g.
Ap-ple?s SiriR?, Google NowTM, Microsoft Cortana,etc.)
or voice interaction for smart household ap-pliance control (e.g.
Samsung Evolution Kit forSmart TVs).
Furthermore, latest progress in open-vocabulary Automatic Speech Recognition (ASR)is pushing SDS from traditional single-domain in-formation systems towards more complex multi-domain speech applications, of which typical ex-amples are those voice assistant mobile applica-tions.Recent advances in SDS have shown that sta-tistical approaches to dialogue management canresult in marginal improvement in both the nat-uralness and the task success rate for domain-specific dialogues (Lemon and Pietquin, 2012;Young et al., 2013).
State-of-the-art statisticalSDS treat the dialogue problem as a sequentialdecision making process, and employ establishedplanning models, such as Markov Decision Pro-cesses (MDPs) (Singh et al., 2002) or Partially Ob-servable Markov Decision Processes (POMDPs)(Thomson and Young, 2010; Young et al., 2010;Williams and Young, 2007), in conjunction withreinforcement learning techniques (Jur?c??
?cek et al.,2011; Jur?c??
?cek et al., 2012; Ga?si?c et al., 2013a)to seek optimal dialogue policies that maximiselong-term expected (discounted) rewards and arerobust to ASR errors.However, to the best of our knowledge, most ofthe existing multi-domain SDS in public use arerule-based (e.g.
(Gruber et al., 2012; Mirkovicand Cavedon, 2006)).
The application of statisticalmodels in multi-domain dialogue systems is stillpreliminary.
Komatani et al.
(2006) and Nakanoet al.
(2011) utilised a distributed architecture (Linet al., 1999) to integrate expert dialogue systems indifferent domains into a unified framework, wherea central controller trained as a data-driven clas-sifier selects a domain expert at each turn to ad-dress user?s query.
Alternatively, Hakkani-T?ur etal.
(2012) adopted the well-known InformationState mechanism (Traum and Larsson, 2003) toconstruct a multi-domain SDS and proposed a dis-criminative classification model for more accuratestate updates.
More recently, Ga?si?c et al.
(2013b)proposed that by a simple expansion of the kernelfunction in Gaussian Process (GP) reinforcementlearning (Engel et al., 2005; Ga?si?c et al., 2013a),one can adapt pre-trained dialogue policies to han-dle unseen slots for SDS in extended domains.In this paper, we use a voice assistant applica-57User Interface ManagerASRUser IntentionIdentier Central ControllerSLU NLGDomain Expert(Travel Info)SLU NLGDomain Expert(Restaurant Search)SLU NLGDomain Expert(Movie Search)SLU NLGDomain Expert(etc.
)WebSearchWeatherReportQAetc.Out-of-domainServicesServiceRankerMobile DevicesFlight TicketBookingTrain TicketBookingHotelBookingspeechtexttext, clicksquery,intention label,condenceTTS Web PageRendering etc.Figure 1: The distributed architecture of the voice assistant system (a simplified illustration).tion (similar to Apple?s Siri but in Chinese lan-guage) as an example to demonstrate a novelMDP-based approach for central interaction man-agement in a complex multi-domain dialogue sys-tem.
The voice assistant employs a distributed ar-chitecture similar to (Lin et al., 1999; Komatani etal., 2006; Nakano et al., 2011), and handles mixedinteractions of multi-turn dialogues across differ-ent domains and single-turn queries powered bya collection of information access services (suchas web search, Question Answering (QA), etc.
).In our system, the dialogues in each domain aremanaged by an individual domain expert SDS, andthe single-turn services are used to handle thoseso-called out-of-domain requests.
We use fea-turised representations to summarise the currentdialogue states in each domain (see Section 3 formore details), and let the central controller (theMDP model) choose one of the following systemactions at each turn: (1) addressing user?s querybased on a domain expert, (2) treating it as anout-of-domain request, (3) asking user to confirmwhether he/she wants to continue a domain ex-pert?s dialogue or to switch to out-of-domain ser-vices, and (4) clarifying user?s intention betweentwo domains.
The Gaussian Process TemporalDifference (GPTD) algorithm (Engel et al., 2005;Ga?si?c et al., 2013a) is adopted here for policy op-timisation based on human subjects, where a pa-rameter tying trick is applied to preserve the ex-tensibility of the system, such that new domainexperts (dialogue systems) can be flexibly pluggedin without the need of re-training the central con-troller.Comparing to the previous classification-basedmethods (Komatani et al., 2006; Nakano et al.,2011), the proposed approach not only has theadvantage of action selection in consideration oflong-term rewards, it can also yield more robustpolicies that allow clarifications and confirmationsto mitigate ASR and Spoken Language Under-standing (SLU) errors.
Our human evaluation re-sults show that the proposed system with a trainedMDP policy achieves significantly better natural-ness in domain switching tasks than a non-trivialbaseline with a hand-crafted policy.The remainder of this paper is organised asfollows.
Section 2 defines the terminology usedthroughout the paper.
Section 3 briefly overviewsthe distributed architecture of our system.
TheMDP model and the policy optimisation algorithmare introduced in Section 4 and Section 5, respec-tively.
After this, experimental settings and eval-uation results are described in Section 6.
Finally,we discuss some possible improvements in Sec-tion 7 and conclude ourselves in Section 8.2 TerminologyA voice assistant application provides a unifiedspeech interface to a collection of individual infor-mation access systems.
It aims to collect and sat-isfy user requests in an interactive manner, where58different types of interactions can be involved.Here we focus ourselves on two interaction scenar-ios, i.e.
task-oriented (multi-turn) dialogues andsingle-turn queries.According to user intentions, the dialogue inter-actions in our voice assistant system can further becategorised into different domains, of which eachis handled by a separate dialogue manager, namelya domain expert.
Example domains include travelinformation, restaurant search, etc.
In addition,some domains in our system can be further de-composed into sub-domains, e.g.
the travel in-formation domain consists of three sub-domains:flight ticket booking, train ticket booking and hotelreservation.
We use an integrated domain expert toaddress queries in all its sub-domains, so that rel-evant information can be shared across those sub-domains to allow intelligent induction in the dia-logue flow.For convenience of future reference, we callthose single-turn information access systems out-of-domain services or simply services for short.The services integrated in our system include websearch, semantic search, QA, system command ex-ecution, weather report, chat-bot, and many more.3 System ArchitectureThe voice assistant system introduced in this pa-per is built on a distributed architecture (Lin et al.,1999), as shown in Figure 1, where the dialogueflow is processed as follows.
Firstly, a user?s query(either an ASR utterance or directly typed in text)is passed to a user intention identifier, which la-bels the raw query with a list of intention hypothe-ses with confidence scores.
Here an intention labelcould be either a domain name or a service name.After this, the central controller distributes the rawquery together with its intention labels and confi-dence scores to all the domain experts and the ser-vice modules, which will attempt to process thequery and return their results to the central con-troller.The domain experts in the current implementa-tion of our system are all rule-based SDS follow-ing the RavenClaw framework proposed in (Bo-hus and Rudnicky, 2009).
When receiving a query,a domain expert will use its own SLU module toparse the utterance or text input and try to updateits dialogue state in consideration of both the SLUoutput and the intention labels.
If the dialoguestate in the domain expert can be updated giventhe query, it will return its output, internal ses-sion record and a confidence score to the centralcontroller, where the output can be either a natu-ral language utterance realised by its Natural Lan-guage Generation (NLG) module or a set of datarecords obtained from its database (if a databasesearch operation is triggered), or both.
If the do-main expert cannot update its state using the cur-rent query, it will just return an empty result witha low confidence score.
Similar procedures ap-ply to those out-of-domain services as well, butthere are no session records or confidence scoresreturned.
Finally, given all the returned informa-tion, the central controller chooses, according toits policy, the module (either a domain expert or aservice) whose results will be provided to the user.When the central controller decides to pass adomain expert?s output to the user, we regard thedomain expert as being activated.
Also note here,the updated state of a domain expert in a turn willnot be physically stored, unless the domain expertis activated in that turn.
This is a necessary mech-anism to prevent an inactive domain expert beingmisled by ambiguous queries in other domains.In addition, we use a well-engineered priorityranker to rank the services based on the num-bers of results they returned as well as some priorknowledge about the quality of their data sources.When the central controller decides to show userthe results from an out-of-domain service, it willchoose the top one from the ranked list.4 MDP Modelling of the Central ControlProcessThe main focus of this paper is to seek a policy forrobustly switching the control flow among thosedomain experts and services (the service ranker inpractice) during a dialogue, where the user mayhave multiple or compound goals (e.g.
booking aflight ticket, booking a restaurant in the destina-tion city and checking the weather report of thedeparture or destination city).In order to make the system robust to ASR er-rors or ambiguous queries, the central controllershould also have basic dialogue abilities for confir-mation and clarification purposes.
Here we definethe confirmation as an action of asking whether auser wants to continue the dialogue in a certain do-main.
If the system receives a negative response atthis point, it will switch to out-of-domain services.On the other hand, the clarification action is de-59fined between domains, in which case, the systemwill explicitly ask the user to choose between twodomain candidates before continuing the dialogue.Due to the confirmation and clarification mech-anisms defined above, the central controller be-comes a sequential decision maker that must takethe overall smoothness of the dialogue into ac-count.
Therefore, we propose an MDP-based ap-proach for learning an optimal central control pol-icy in this section.The potential state space of our MDP is huge,which in principle consists of the combinations ofall possible situations of the domain experts andthe out-of-domain services, therefore function ap-proximation techniques must be employed to en-able tractable computations.
However, when de-veloping such a complex application as the voiceassistant here, one also needs to take the extensi-bility of the system into account, so that new do-main experts can be easily integrated into the sys-tem without major re-training or re-engineering ofthe existing components.
Essentially, it requiresthe state featurisation and the central control pol-icy learnt here to be independent of the number ofdomain experts.
In Section 4.3, we show that sucha property can be achieved by a parameter tyingtrick in the definition of the MDP.4.1 MDP PreliminariesLet PXdenote the set of probability distributionsover a set X .
An MDP is defined as a five tuple?S,A, T,R, ?
?, where the components are definedas follows.
S and A are the sets of system statesand actions, respectively.
T : S ?
A ?
PSis thetransition function, and T (s?|s, a) defines the con-ditional probability of the system transiting fromstate s ?
S to state s??
S after taking actiona ?
A. R : S ?
A ?
PRis the reward functionwith R(s, a) specifying the distribution of the im-mediate rewards for the system taking action a atstate s. In addition, 0 ?
?
?
1 is the discountfactor on the summed sequence of rewards.A finite-horizon MDP operates as follows.
Thesystem occupies a state s and takes an action a,which then will make it transit to a next state s?
?T (?|s, a) and receive a reward r ?
R(s, a).
Thisprocess repeats until a terminal state is reached.For a given policy pi : S ?
A, the valuefunction Vpiis defined to be the expected cumula-tive reward, as Vpi(s0) = E[?nt=0?trt|st,pi(st)],where s0is the starting state and n is the plan-ning horizon.
The aim of policy optimisation isto seek an optimal policy pi?that maximises thevalue function.
If T and R are given, in conjunc-tion with a Q-function, the optimal value V?canbe expressed by recursive equations as Q(s, a) =R(s, a) + ??s?
?ST (s?|s, a)V?(s?)
and V?
(s) =maxa?AQ(s, a) (here we assume R(s, a) is de-terministic), which can be solved by dynamic pro-gramming (Bellman, 1957).
For problems withunknown T or R, such as dialogue systems, theQ-values are usually estimated via reinforcementlearning (Sutton and Barto, 1998).4.2 Problem DefinitionLet D denote the set of the domain experts in ourvoice assistant system, and sdbe the current di-alogue state of domain expert d ?
D at a certaintimestamp.
We also define soas an abstract state todescribe the current status of those out-of-domainservices.
Then mathematically we can representthe central control process as an MDP, where itsstate s is a joint set of the states of all the domainexperts and the services, as s = {sd}d?D?
{so}.Four types of system actions are defined as fol-lows.?
present(d): presenting the output of do-main expert d to user;?
present ood(null): presenting the re-sults of the top-ranked out-of-domain servicegiven by the service ranker;?
confirm(d): confirming whether userwants to continue with domain expert d (orto switch to out-of-domain services);?
clarify(d,d?
): asking user to clarifyhis/her intention between domains d and d?.For convenience of notations, we use a(x) todenote a system action of our MDP, where a ?
{present,present ood,confirm,clarify},x ?
{d,null, (d, d?)}d,d?
?D,d6=d?, x = nullonly applies to present ood, and x = (d, d?
)only applies to clarify actions.4.3 Function ApproximationFunction approximation is a commonly used tech-nique to estimate the Q-values when the statespace of the MDP is huge.
Concretely, in our case,we assume that:Q(s, a(x)) = f(?
(s, a(x)); ?)
(1)60where ?
: S ?
A ?
RKis a feature functionthat maps a state-action pair to an K-dimensionalfeature vector, and f : RK?
R is a function of?
(s, a(x)) parameterised by ?.
A frequent choiceof f is the linear function, as:Q(s, a(x)) = ?>?
(s, a(x)) (2)After this, the policy optimisation problem be-comes learning the parameter ?
to approximate theQ-values based on example dialogue trajectories.However, a crucial problem with the standardformulation in Eq.
(2) is that the feature function?
is defined over the entire state and action spaces.In this case, when a new domain expert is inte-grated into the system, both the state space and theaction space will be changed, therefore one willhave to re-define the feature function and conse-quentially re-train the model.
In order to achievean extensible system, we make some simplifica-tion assumptions and decompose the feature func-tion as follows.
Firstly, we let:?
(s, a(x)) = ?a(sx) (3)=???????
?pr(sd) if a(x) =present(d)?ood(so) if a(x) =present ood()?cf(sd) if a(x) =confirm(d)?cl(sd, sd?)
if a(x) =clarify(d,d?
)where the feature function is reduced to only de-pend on the state of the action?s operand, insteadof the entire system state.
Then, we make those ac-tions a(x) that have a same action type (a) but op-erate different domain experts (x) share the sameparameter, i.e.
:Q(s, a(x)) = ?>a?a(sx) (4)This decomposition and parameter tying trick pre-serves the extensibility of the system, because both?>aand ?aare independent of x, when there is anew domain expert?d, we can directly substituteits state s?dinto Eq.
(3) and (4) to compute its cor-responding Q-values.4.4 FeaturesBased on the problem formulation in Eq.
(3) and(4), we shall only select high-level summary fea-tures to sketch the dialogue state and dialogue his-tory of each domain expert, which must be ap-plicable to all domain experts, regardless of theirdomain-specific characteristics or implementationdifferences.
Suppose that the dialogue states of the# Feature Range1the number of unfilledrequired slots of a domainexpert{0, .
.
.
,M}2the number of filled requiredslots of a domain expert{0, .
.
.
,M}3the number of filled optionalslots of a domain expert{0, .
.
.
, L}4whether a domain expert hasexecuted a database search{0, 1}5the confidence scorereturned by a domain expert[0, 1.2]6the total number of turns thata domain expert has beenactivated during a dialogueZ+7e?tawhere tadenotes therelative turn of a domainexpert being last activated,or 0 if not applicable[0, 1]8e?tcwhere tcdenotes therelative turn of a domainexpert being last confirmed,or 0 if not applicable[0, 1]9the summed confidencescore from the user intentionidentifier of a query beingfor out-of-domain services[0, 1.2N ]Table 1: A list of all features used in our model.M and L respectively denote the maximum num-bers of required and optional slots for the domainexperts.
N is the maximum number of hypothesesthat the intention identifier can return.
Z+standsfor the non-negative integer set.domain experts can be represented as slot-valuepairs1, and for each domain there are required slotsand optional slots, where all required slots mustbe filled before the domain expert can execute adatabase search operation.
The features investi-gated in the proposed framework are listed in Ta-ble 1.Detailed featurisation in Eq.
(3) is explainedas follows.
For ?pr, we choose the first 8 fea-tures plus a bias dimension that is always set to1This is a rather general assumption.
Informally speak-ing, for most task-oriented SDS, one can extract a slot-valuerepresentation from their dialogue models, of which exam-ples include the RavenClaw architecture (Bohus and Rud-nicky, 2009), the Information State dialogue engine (Traumand Larsson, 2003), MDP-SDS (Singh et al., 2002) orPOMDP-SDS (Thomson and Young, 2010; Young et al.,2010; Williams and Young, 2007).61?1.
Whilst, feature #9 plus a bias is used to de-fine ?ood.
All the features are used in ?cf, as todo a confirmation, one needs to consider the jointsituation in and out of the domain.
Finally, thefeature function for a clarification action betweentwo domains d and d?is defined as ?cl(sd, sd?)
=exp{?|?pr(sd) ?
?pr(sd?
)|}, where we use | ?
|to denote the element-wise absolute of a vectoroperand.
The intuition here is that the more dis-tinguishable the (featurised) states of two domainexperts are, the less we tend to clarify them.For those domain experts that have multiplesub-domains with different numbers of requiredand optional slots, the feature extraction procedureonly applies to the latest active sub-domain.In addition, note that, the confidence scores pro-vided by the user intention identifier are only usedas features for out-of-domain services.
This is be-cause in the current version of our system, the con-fidence estimation of the intention identifier fordomain-dependent dialogue queries is less reliabledue to the lack of context information.
In contrast,the confidence scores returned by the domain ex-perts will be more informative at this point.5 Policy Learning with GPTDIn traditional statistical SDS, dialogue policies areusually trained using reinforcement learning basedon simulated dialogue trajectories (Schatzmannet al., 2007; Keizer et al., 2010; Thomson andYoung, 2010; Young et al., 2010).
Although theevaluation of the simulators themselves could bean arguable issue, there are various advantages,e.g.
hundreds of thousands of data examples canbe easily generated for training and initial policyevaluation purposes, and different reinforcementlearning models can be compared without incur-ring notable extra costs.However, for more complex multi-domain SDS,particularly a voice assistant application like oursthat aims at handling very complicated (ideallyopen-domain) dialogue scenarios, it would be dif-ficult to develop a proper simulator that can rea-sonably mimic real human behaviours.
There-fore, in this work, we learn the central controlpolicy directly with human subjects, for whichthe following properties of the learning algorithmare required.
Firstly and most importantly, thelearner must be sample-efficient as the data collec-tion procedure is costly.
Secondly, the algorithmshould support batch reinforcement learning.
Thisis because when using function approximation, thelearning process may not strictly converge, and thequality of the sequence of generated policies tendsto oscillate after a certain number of improvingsteps at the beginning (Bertsekas and Tsitsiklis,1996).
If online reinforcement learning is used,we will be unable to evaluate the generated policyafter each update, and hence will not know whichpolicy to keep for the final evaluation.
Therefore,we do a batch policy update and iterate the learn-ing process for a number of batches, such that thedata collection phase in a new iteration yields anevaluation of the policy obtained from the previ-ous iteration at the same time.To fulfill the above two requirements, the Gaus-sian Process Temporal Difference (GPTD) algo-rithm (Engel et al., 2005) is a proper choice, due toits sample efficiency (Fard et al., 2011) and batchlearning ability (Engel et al., 2005), as well as itsprevious success in dialogue policy learning withhuman subjects (Ga?si?c et al., 2013a).
Note that,GPTD can also admit recursive (online) compu-tations, but here we focus ourselves on the batchversion.A Gaussian Process (GP) is a generative modelof Bayesian inference that can be used for func-tion regression, and has the superiority of obtain-ing good posterior estimates with just a few obser-vations (Rasmussen and Williams, 2006).
GPTDmodels the Q-function as a zero mean GP whichdefines correlations in different parts of the fea-turised state and action spaces through a kernelfunction ?, as:Q(s, a(x)) ?
GP(0, ?
((sx, a), (sx, a))) (5)Given a sequence of t state-action pairs Xt=[(s0, a0(x0)), .
.
.
, (st, at(xt))] from a collectionof dialogues and their corresponding immedi-ate rewards rt= [r0, .
.
.
, rt], the posterior ofQ(s, a(x)) for an arbitrary new state-action pair(s, a(x)) can be computed as:Q(s, a(x))|Xt,rt?
N(?Q(s, a(x)), cov (s, a(x)))(6)?Q(s, a(x)) = kt(sx, a)>H>tG?1trt(7)cov (s, a(x)) = ?
((sx, a), (sx, a))?
kt(sx, a)>H>tG?1tHtkt(sx, a) (8)Gt= HtKtH>t+ ?2HtH>t(9)62Ht=????
?1 ??
?
?
?
0 00 1 ?
?
?
0 0...............0 ?
?
?
0 1 ???????
(10)where Ktis the Gram matrix with elementsKt(i, j) = ?
((sixi, ai), (sjxj, aj)), kt(sx, a) =[?
((sixi, ai), (sx, a))]ti=0is a vector, and ?
is ahyperparameter specifying the diagonal covari-ance values of the zero-mean Gaussian noise.
Inaddition, we use cov (s, a(x)) to denote (for short)the self-covariance cov (s, a(x), s, a(x)).In our case, as different feature functions ?aaredefined for different action types, the kernel func-tion is defined to be:?
((sx, a), (s?x?, a?))
= [[a = a?
]]?a(sx, s?x?)
(11)where [[?]]
is an indicator function and ?ais the ker-nel function defined corresponding to the featurefunction ?a.Given a state, a most straightforward policy isto select the action that corresponds to the max-imum mean Q-value estimated by the GP.
How-ever, since the objective is to learn the Q-functionassociated with the optimal policy by interactingdirectly with users, the policy must exhibit someform of stochastic behaviour in order to explorealternatives during the process of learning.
In thiswork, the strategy employed for the exploration-exploitation trade-off is that, during exploration,actions are chosen according to the variance ofthe GP estimate for the Q-function, and duringexploitation, actions are chosen according to themean.
That is:pi(s) ={arg maxa(x)?Q(s, a(x)) : w.p.
1?
arg maxa(x)cov (s, a(x)) : w.p.
(12)where 0 <  < 1 is a pre-defined exploration rate,and will be exponentially reduced at each batchiteration during our learning process.Note that, in practice, not all the actions arevalid at every possible state.
For example, if a do-main expert d has never been activated during adialogue and can neither process the user?s currentquery, the actions with an operand d will be re-garded as invalid at this state.
When executing thepolicy, we only consider those valid actions for agiven state.Score Interpretation5The domain selections are totallycorrect, and the entire dialogue flowis fluent.4The domain selections are totallycorrect, but the dialogue flow isslightly redundant.3There are accidental domainselections errors, or the dialogueflow is perceptually redundant.2There are frequent domain selectionserrors, or the dialogue flow isintolerably redundant.1Most domain selections areincorrect, or the dialogue isincompletable.Table 2: The scoring standard in our experiments.6 Experimental Results6.1 TrainingWe use the batch version of GPTD as describedin Section 5 to learn the central control policywith human subjects.
There are three domain ex-perts available in our current system, but duringthe training only two domains are used, which arethe travel information domain and the restaurantsearch domain.
We reserve a movie search domainfor evaluating the generalisation property of thelearnt policy (see Section 6.2).
The learning pro-cess started from a hand-crafted policy.
Then 15experienced users2volunteered to contribute dia-logue examples with multiple or compound goals(see Figure 4 for an instance), from whom wecollected around 50?70 dialogues per day for 5days3.
After each dialogue, the users were askedto score the system from 5 to 1 according to a scor-ing standard shown in Table 2.
The scores weretaken as the (delayed) rewards to train the GPTDmodel, where we set the rewards for intermediateturns to 0.
The working policy was updated dailybased on the data obtained up to that day.
Thedata collected on the first day was used for pre-liminary experiments to choose the hyperparame-2Overall user satisfactions may rely on various aspects ofthe entire system, e.g.
the data source quality of the services,the performance of each domain expert, etc.
It will be diffi-cult to make non-experienced users to score the central con-troller isolatedly.3Not all the users participated the experiments everyday.There were 311 valid dialogues received in total, with an av-erage length of 9 turns.632?3?4?5?Figure 2: Average scores and standard deviationsduring policy iteration.0.7?0.72?0.74?0.76?0.78?0.8?0.82?0.84?0.86?0.88?0.9?Figure 3: Domain selection accuracies during pol-icy iteration.ters of the model, such as the kernel function, thekernel parameters (if applicable), and ?
and ?
inthe GPTD model.
We initially experimented withlinear, polynomial and Gaussian kernels, with dif-ferent configurations of ?
and ?
values, as wellas kernel parameter values.
It was found thatthe linear kernel in combination with ?
= 5 and?
= 0.99 works more appropriate than the othersettings.
This configuration was then fixed for therest iterations.The learning process was iterated for 4 days af-ter the first one.
On each day, we computed themean and standard deviation of the user scoresas an evaluation of the policy learnt on the pre-vious day.
The learning curve is illustrated in Fig-ure 2.
Note here, as we were actually executing astochastic policy according to Eq.
(12), to calcu-late the values in Figure 2 we ignored those dia-logues that contain any actions selected due to theexploration.
We also manually labelled the cor-rectness of domain selection at every turn of thedialogues.
The domain selection accuracies of theobtained policy sequence are shown in Figure 3,where similarly, those exploration actions as wellPolicyScenarioBaseline GPTDp-value(i) 4.5?0.8 4.2?0.8 0.387(ii) 3.4?0.9 4.2?0.8 0.018(iii) 4.1?1.0 4.3?1.0 0.0821(iv) 3.9?1.1 4.5?0.8 0.0440Table 3: Paired comparison experiments betweenthe system with a trained GPTD policy and therule-based baseline.as the clarification and confirmation actions wereexcluded from the calculations.
Although the do-main selection accuracy is not the target that ourlearning algorithm aims to optimise, it reflects thequality of the learnt policies from a different angleof view.It can be found in Figure 2 that the best policywas obtained in the third iteration, and after thatthe policy quality oscillated.
The same finding isindicated in Figure 3 as well, when we use the do-main selection accuracy as the evaluation metric.Therefore, we kept the policy corresponding to thepeak point of the learning curve for the compari-son experiments below.6.2 Comparison ExperimentsWe conducted paired comparison experiments infour scenarios to compare between the systemwith the GPTD-learnt central control policy and anon-trivial baseline.
The baseline is a publicly de-ployed version of the voice assistant application.The central control policy of the baseline system ishandcrafted, which has a separate list of semanticmatching rules for each domain to enable domainswitching.The first two scenarios aim to test the perfor-mance of the two systems on (i) switching betweena domain expert and out-of-domain services, and(ii) switching between two domain experts, whereonly the two training domains (travel informationand restaurant search) were considered.
Scenar-ios (iii) and (iv) are similar to scenarios (i) and (ii)respectively, but at this time, the users were re-quired to carry out the tests surrounding the moviesearch domain (which is addressed by a new do-main expert not used in the training phase).
Therewere 13 users who participated this experiment.In each scenario, every user was required to testthe two systems with an identical goal and similarqueries.
After each test, the users were asked to64score the two systems separately according to thescoring standard in Table 2.The average scores received by the two systemsare shown in Table 3, where we also compute thestatistical significance (the p-values) of the resultsbased on paired t-tests.
It can be found that thelearnt policy works significantly better than therule-based policy in scenarios (ii) and (iv), but inscenarios (i) and (iii) the differences between twosystems are statistically insignificant.
Moreover,the learnt policy preserves the extensibility of theentire system as expected, of which strong evi-dences are given by the results in scenarios (iii)and (iv).6.3 Policy AnalysisTo better understand the policy learnt by theGPTD model, we look into the obtained weightvectors, as shown in Table 4.
It can be found thatconfidence score (#5) is an informative feature forall the system actions, while the relative turn of adomain being last activated (#7) is an additionalstrong evidence for a confirmation decision.
Inaddition, the similarity between the dialogue com-pletion status (#1 & #2) of two ambiguous domainexperts and the relative turns of them being lastconfirmed (#8) tend to be extra dominating fea-tures for clarification decisions, besides the close-ness of the confidence scores returned by the twodomain experts.A less noticeable but important phenomenon isobserved for feature #6, i.e.
the total number ofactive turns of a domain expert during a dialogue.Concretely, feature #6 has a small negative effecton presentation actions but a small positive con-tribution to confirmation actions.
Such weightscould correspond to the discount factor?s penaltyto long dialogues in the value function.
How-ever, it implies an unexpected effect in extremecases, which we explain in detail as follows.
Al-though the absolute weights for feature #6 are tinyfor both presentation and confirmation actions, thefeature value will grow linearly during a dialogue.Therefore, when a dialogue in a certain domainlast rather long, there tend to be very frequent con-firmations.
A possible solution to this problemcould be either ignoring feature #6 or twisting it tosome nonlinear function, such that its value stopsincreasing at a certain threshold point.
In addition,to cover sufficient amount of those ?extreme?
ex-amples in the training phase could also be an alter-Feature Weights#present confirm clarify1 0.09 0.02 0.60presentood2 0.20 0.29 0.533 0.18 0.29 0.164 -0.10 0.16 0.255 0.75 0.57 0.546 -0.02 0.11 0.137 0.25 1.19 0.368 -0.22 -0.19 0.699 ?
0.20 ?
0.47Bias -1.79 ?
?
-2.42Table 4: Feature weights learnt by GPTD.
See Ta-ble 1 for the meanings of the features.native solution, as our current training set containsvery few examples that exhibit extraordinary longdomain persistence.7 Further DiscussionsThe proposed approach is a rather general frame-work to learn extensible central control policiesfor multi-domain SDS based on distributed archi-tectures.
It does not rely on any internal represen-tations of those individual domain experts, as longas a unified featurisation of their dialogue statescan be achieved.However, from the entire system point of view,the current implementation is still preliminary.Particularly, the confirmation and clarificationmechanisms are isolated, for which the surface re-alisations sometimes may sound stiff.
This phe-nomenon explains one of the reasons that makethe proposed system slightly less preferred by theusers than the baseline in scenario (i), when theinteraction flows are relative simple.
A possi-ble improvement here could be associating theconfirmation and clarification actions in the cen-tral controller to the error handling mechanismswithin each domain expert, and letting domain ex-perts generate their own utterances on receiving aconfirmation/clarification request from the centralcontroller.Online reinforcement learning with real usercases will be another undoubted direction of fur-ther improvement of our system.
The key chal-lenge here is to automatically estimate user?s satis-factions, which will be transformed to the rewardsfor the reinforcement learners.
Strong feedbackssuch as clicks or actual order placements can be65collected.
But to regress user?s true satisfaction,other environment features must also be taken intoaccount.
Practical solutions are still an open issueat this stage, and are left to our future work.8 ConclusionIn this paper, we introduce an MDP frameworkfor learning domain selection policies in a com-plex multi-domain SDS.
Standard problem for-mulation is modified with tied model parameters,so that the entire system is extensible and newdomain experts can be easily integrated withoutre-training the policy.
This expectation is con-firmed by empirical experiments with human sub-jects, where the proposed system marginally beatsa non-trivial baseline and demonstrates proper ex-tensibility.
Several possible improvements are dis-cussed, which will be the central arc of our futureresearch.AcknowledgementsThe research in this paper is supported by China?s973 Programme no.
2014CB340505.
The first au-thor is partially funded by the EC FP7 programmeunder grant agreement no.
287615 (PARLANCE)and a SICSA PECE grant.
The authors would alsolike to thank Qiaoqiao She, Duo Cai and the HCI-APP group at Baidu for volunteering to participatein the human subject experiments.ReferencesRichard Ernest Bellman.
1957.
Dynamic Program-ming.
Princeton University Press, Princeton, NJ.Dimitri P. Bertsekas and John N. Tsitsiklis.
1996.Neuro-Dynamic Programming.
Athena Scientific,Belmont, MA.Dan Bohus and Alexander I. Rudnicky.
2009.
TheRavenClaw dialog management framework: Archi-tecture and systems.
Computer Speech and Lan-guage, 23(3):332?361.Yaakov Engel, Shie Mannor, and Ron Meir.
2005.
Re-inforcement learning with Gaussian processes.
InProceedings of the 22nd International Conferenceon Machine Learning (ICML), pages 201?208.Mahdi Milani Fard, Joelle Pineau, and CsabaSzepesv?ari.
2011.
PAC-Bayesian policy evaluationfor reinforcement learning.
In Proceedings of the27th Conference on Uncertainty in Artificial Intelli-gence (UAI), pages 195?202.USER/ASR:      ??????
(Flight ticket to Qingdao.
)SYSTEM: ?????????????????????
???????
(Okay, flight ticket to Qingdao.
When are you        departing, and where are you departing from?)
[Travel Info.
Domain Expert]USER/ASR:  ????
(I?m leaving tomorrow.
)SYSTEM: ???????????????????????
(Okay, flight ticket to Qingdao, leaving         tomorrow.
Where are you departing from?)
[Travel Info.
Domain Expert]USER/ASR:  ??
(Beijing)SYSTEM: [Results Displayed     - Travel Info.
Domain Expert]USER:    ??????????
(Are there any good Sichuan cuisines in Qingdao?
)ASR:       ???????????
(ungrammatical utterance, partially with     ?delicious?
and ?Sichuan food?.
)SYSTEM: ?????????
(Do you want to continue with flight ticket booking?)
[Confirmation - Central Controller]USER:      ??
(No)SYSTEM: [Web Search Service: ?????????????
]USER/ASR:  ?????
(I?m going to Qingdao tomorrow.
)SYSTEM: [Results Displayed     - Travel Info.
Domain Expert]USER/ASR:  ?????????
(Restaurants near Qingdao Airport)SYSTEM: ????????
(I found the following restaurants for you.)
[Results Displayed     - Restaurant Search Domain Expert]USER/ASR:  ?????????
(Will it rain in Qingdao tomorrow?
)SYSTEM: ?????????16??20????5-6?
(Qingdao is rainy tomorrow.
The temperature is 16 to 20         degree centigrade with moderate east wind.)
[Results Displayed     - Weather Report Service]Figure 4: An example dialogue containing multi-ple user goals.
The icons stand for graphical userinterface based nonverbal outputs.Milica Ga?si?c, Catherine Breslin, Matthew Henderson,Dongho Kim, Martin Szummer, Blaise Thomson,Pirros Tsiakoulis, and Steve Young.
2013a.
On-line policy optimisation of Bayesian spoken dia-logue systems via human interaction.
In Proceed-ings of the IEEE International Conference on Acous-tics, Speech and Signal Processing (ICASSP), pages8367?8371.Milica Ga?si?c, Catherine Breslin, Matthew Hender-son, Dongho Kim, Martin Szummer, Blaise Thom-son, Pirros Tsiakoulis, and Steve Young.
2013b.POMDP-based dialogue manager adaptation to ex-tended domains.
In Proceedings of the 14th annualSIGdial Meeting on Discourse and Dialogue, pages214?222.Thomas Robert Gruber, Adam John Cheyer, Dag66Kittlaus, Didier Rene Guzzoni, Christopher DeanBrigham, Richard Donald Giuli, Marcello Bastea-Forte, and Harry Joseph Saddler.
2012.
Intelligentautomated assistant.
United States Patent No.
US20120245944 A1.Dilek Z. Hakkani-T?ur, Gokhan T?ur, Larry P. Heck,Ashley Fidler, and Asli C?elikyilmaz.
2012.
A dis-criminative classification-based approach to infor-mation state updates for a multi-domain dialog sys-tem.
In Proceedings of the 13th Annual Conferenceof the International Speech Communication Associ-ation (INTERSPEECH).Filip Jur?c??
?cek, Blaise Thomson, and Steve Young.2011.
Natural actor and belief critic: Reinforcementalgorithm for learning parameters of dialogue sys-tems modelled as POMDPs.
ACM Transactions onSpeech and Language Processing, 7(3):6:1?6:25.Filip Jur?c??
?cek, Blaise Thomson, and Steve Young.2012.
Reinforcement learning for parameter esti-mation in statistical spoken dialogue systems.
Com-puter Speech & Language, 26(3):168?192.Simon Keizer, Milica Ga?si?c, Filip Jur?c??
?cek, Franc?oisMairesse, Blaise Thomson, Kai Yu, and SteveYoung.
2010.
Parameter estimation for agenda-based user simulation.
In Proceedings of the 11thannual SIGdial Meeting on Discourse and Dialogue,pages 116?123.Kazunori Komatani, Naoyuki Kanda, Mikio Nakano,Kazuhiro Nakadai, Hiroshi Tsujino, Tetsuya Ogata,and Hiroshi G. Okuno.
2006.
Multi-domain spo-ken dialogue system with extensibility and robust-ness against speech recognition errors.
In Proceed-ings of the 7th SIGdial Workshop on Discourse andDialogue, pages 9?17.Oliver Lemon and Olivier Pietquin, editors.
2012.Data-Driven Methods for Adaptive Spoken Dia-logue Systems: Computational Learning for Conver-sational Interfaces.
Springer.Bor-shen Lin, Hsin-min Wang, and Lin-Shan Lee.1999.
A distributed architecture for cooperativespoken dialogue agents with coherent dialogue stateand history.
In Proceedings of the IEEE AutomaticSpeech Recognition and Understanding Workshop(ASRU).Danilo Mirkovic and Lawrence Cavedon.
2006.
Di-alogue management using scripts.
United StatesPatent No.
US 20060271351 A1.Mikio Nakano, Shun Sato, Kazunori Komatani, KyokoMatsuyama, Kotaro Funakoshi, and Hiroshi G.Okuno.
2011.
A two-stage domain selection frame-work for extensible multi-domain spoken dialoguesystems.
In Proceedings of the 12th annual SIGdialMeeting on Discourse and Dialogue, pages 18?29.Carl Edward Rasmussen and Christopher K. I.Williams, editors.
2006.
Gaussian Processes forMachine Learning.
MIT Press.Jost Schatzmann, Blaise Thomson, Karl Weilhammer,Hui Ye, and Steve Young.
2007.
Agenda-baseduser simulation for bootstrapping a POMDP dia-logue system.
In Proceedings of Human LanguageTechnologies 2007: The Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics; Companion Volume, Short Pa-pers, pages 149?152.Satinder Singh, Diane Litman, Michael Kearns, andMarilyn Walker.
2002.
Optimizing dialogue man-agement with reinforcement learning: Experimentswith the NJFun system.
Journal of Artificial Intelli-gence Research, 16(1):105?133.Richard S. Sutton and Andrew G. Barto.
1998.
Re-inforcement Learning: An Introduction.
MIT Press,Cambridge, MA.Blaise Thomson and Steve Young.
2010.
Bayesianupdate of dialogue state: A POMDP framework forspoken dialogue systems.
Computer Speech andLanguage, 24(4):562?588.David R. Traum and Staffan Larsson.
2003.
The In-formation State approach to dialogue management.In Jan van Kuppevelt and Ronnie W. Smith, editors,Current and New Directions in Discourse and Dia-logue, pages 325?353.
Springer.Jason D. Williams and Steve Young.
2007.
Partiallyobservable Markov decision processes for spokendialog systems.
Computer Speech and Language,21(2):393?422.Jason D. Williams, Iker Arizmendi, and AlistairConkie.
2010.
Demonstration of AT&T ?Let?s Go?
:A production-grade statistical spoken dialog system.In Proceedings of the 3rd IEEE Workshop on SpokenLanguage Technology (SLT).Steve Young, Milica Ga?si?c, Simon Keizer, Franc?oisMairesse, Jost Schatzmann, Blaise Thomson, andKai Yu.
2010.
The Hidden Information State model:a practical framework for POMDP-based spoken di-alogue management.
Computer Speech and Lan-guage, 24(2):150?174.Steve Young, Milica Ga?si?c, Blaise Thomson, and Ja-son D. Williams.
2013.
POMDP-based statisticalspoken dialogue systems: a review.
Proceedings ofthe IEEE, PP(99):1?20.67
