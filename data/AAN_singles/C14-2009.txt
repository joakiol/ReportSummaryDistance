Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations,pages 39?43, Dublin, Ireland, August 23-29 2014.Lightweight Client-Side Chinese/JapaneseMorphological Analyzer Based on Online LearningMasato Hagiwara Satoshi SekineRakuten Institute of Technology, New York215 Park Avenue South, New York, NY{masato.hagiwara, satoshi.b.sekine}@mail.rakuten.comAbstractAs mobile devices and Web applications become popular, lightweight, client-side languageanalysis is more important than ever.
We propose Rakuten MA, a Chinese/Japanesemorphological analyzer written in JavaScript.
It employs an online learning algorithmSCW, which enables client-side model update and domain adaptation.
We have achieveda compact model size (5MB) while maintaining the state-of-the-art performance, viatechniques such as feature hashing, FOBOS, and feature quantization.1 IntroductionWord segmentation (WS) and part-of-speech (PoS) tagging, often jointly called morphologicalanalysis (MA), are the essential component for processing Chinese and Japanese, where wordsare not explicitly separated by whitespaces.
There have been many word segmentater and PoStaggers proposed in both Chinese and Japanese, such as Stanford Segmenter (Tseng et al.,2005), zpar (Zhang and Clark, 2011), MeCab (Kudo et al., 2004), JUMAN (Kurohashi andNagao, 1994), to name a few.
Most of them are intended for server-side use and provide limitedcapability to extend or re-train models.
However, as mobile devices such as smartphones andtablets become popular, there is a growing need for client based, lightweight language analysis,and a growing number of applications are built upon lightweight languages such as HTML, CSS,and JavaScript.
Techniques such as domain adaptation and model extension are also becomingmore important than ever.In this paper, we present Rakuten MA, a morphological analyzer entirely written in JavaScriptbased on online learning.
We will be releasing the software as open source before the COLING2014 conference at https://github.com/rakuten-nlp/rakutenma, under Apache License, ver-sion 2.0.
It relies on general, character-based sequential tagging, which is applicable to anylanguages and tasks which can be processed in a character-by-character basis, including WS andPoS tagging for Chinese and Japanese.
Notable features include:1.
JavaScript based ?
Rakuten MA works as a JavaScript library, the de facto ?lingua franca?of the Web.
It works on popular Web browsers as well as node.js, which enables a wide rangeof adoption such as smartphones and Web browser extensions.
Note that TinySegmenter1is also entirely written in JavaScript, but it does not support model re-training or anylanguages other than Japanese.
It doesn?t output PoS tags, either.2.
Compact ?
JavaScript-based analyzers pose a difficult technical challenge, that is, thecompactness of the model.
Modern language analyzers often rely on a large number offeatures and/or dictionary entries, which is impractical on JavaScript runtime environments.In order to address this issue, Rakuten MA implements some notable techniques.
First, thefeatures are character-based and don?t rely on dictionaries.
Therefore, while it is inherentlyincapable of dealing with words which are longer than features can capture, it may be robustThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers andproceedings footer are added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1http://chasen.org/~taku/software/TinySegmenter/39S-N-nc S-P-k!!
"!B-V-c E-V-c#!
$!S-P-sj%!tagscharacters x1:!
c1:C x2:" c2:H x3:# c3:C x4:$ c4:H x5:% c5:H?Figure 1: Character-based tagging modelFeature Descriptionxi?2, xi?1, xi, xi+1, xi+2char.
unigramsxi?2xi?1, xi?1xi, xixi+1, xi+1xi+2char.
bigramsci?2, ci?1, ci, ci+1, ci+2type unigramsci?2ci?1, ci?1ci, cici+1, ci+1ci+2type bigramsTable 1: Feature Templates Used for Taggingxiand ciare the character and the character type at ia.aEach feature template is instantiated and concate-nated with possible tags.
Character type bigram featureswere only used for JA.
In CN, we built a character typedictionary, where character types are simply all the possi-ble tags in the training corpus for a particular character.to unknown words compared with purely dictionary-based systems.
Second, it employstechniques such as feature hashing (Weinberger et al., 2009), FOBOS (Duchi and Singer,2009), and feature quantization, to make the model compact while maintaining the samelevel of analysis performance.3.
Online Learning ?
it employs a modern online learning algorithm called SCW (Wang et al.,2012), and the model can be incrementally updated by feeding new instances.
This enablesusers to update the model if errors are found, without even leaving the Web browser ornode.js.
Domain adaptation is also straightforward.
Note that MeCab (Kudo et al., 2004),also supports model re-training using a small re-training corpus.
However, the training isinherently a batch, iterative algorithm (CRF) thus it is hard to predict when it finishes.2 Analysis Model and Compact Model RepresentationBase Model Rakuten MA employs the standard character-based sequential tagging model.It assigns combination of position tags2 and PoS tags to each character (Figure 1).
The optimaltag sequence y?
for an input string x is inferred based on the features ?
(y) and the weight vectorw as y?
= arg maxy?Y (x)w ?
?
(y), where Y (x) denotes all the possible tag sequences for x, viastandard Viterbi decoding.
Table 1 shows the feature template sets.For training, we used soft confidence weighted (SCW) (Wang et al., 2012).
SCW is an onlinelearning scheme based on Confidence Weighted (CW), which maintains ?confidence?
of eachparameter as variance ?
in order to better control the updates.
Since SCW itself is a generalclassification model, we employed the structured prediction model (Collins, 2002) for WS.The code snippet in Figure 2 shows typical usage of Rakuten MA in an interactive way.
Linesstarting with ?//?
and ?>?
are comments and user input, and the next lines are returned results.Notice that the analysis of?????????
?President Barak Obama?
get better as the modelobserves more instances.
The analyzer can only segment it into individual characters when themodel is empty ((1) in the code), whereas WS is partially correct after observing the first 10sentences of the corpus ((2) in the code).
After directly providing the gold standard, the result(3) becomes perfect.We used and compared the following three techniques for compact model representations:Feature Hashing (Weinberger et al., 2009) applies hashing functions h which turn an arbi-trary feature ?i(y) into a bounded integer value v, i.e., v = h(?i(y)) ?
R, where 0 ?
v < 2N ,(N = hash size).
This technique is especially useful for online learning, where a large, growingnumber of features such as character/word n-grams could be observed on the fly, which themodel would otherwise need to keep track of using flexible data structures such as trie, whichcould make training slower as the model observes more training instances.
The negative effect ofhash collisions to the performance is negligible because most collisions are between rare features.2As for the position tags, we employed the SBIEO scheme, where S stands for a single character word, BIEfor beginning, middle, and end of a word, respectively, and O for other positions.40// initialize with empty model> var r = new RakutenMA({});// (1) first attempt, failed with separate chars.> r.tokenize("?????????
").toString()"?,,?,,?,,?,,?,,?,,?,,?,,?,"// train with first 10 sentences in a corpus> for (var i = 0; i < 10; i ++)> r.train_one( rcorpus[i] );// the model is no longer empty> r.modelObject {mu: Object, sigma: Object}// (2) second attempt -> getting closer> r.tokenize("?????????").toString()"???,N-nc,??
?,N-pn,?,,?,,?,Q-n"// retrain with an answer// return object suggests there was an update> r.train_one([["???","N-np"],...
["???","N-np"],["???
","N-nc"]]);Object {ans: Array[3], sys: Array[5], updated: true}// (3) third attempt> r.tokenize("?????????").toString()"???,N-np,???,N-np,??
?,N-nc"Figure 2: Rakuten MA usage exampleChinese Prec.
Rec.
F Japanese Prec.
Rec.
FStanford Parser 97.37 93.54 95.42 MeCab+UniDic 99.15 99.61 99.38zpar 91.18 92.36 91.77 JUMAN **88.55 **83.06 **85.72Rakuten MA 92.61 92.64 92.62 KyTea *80.57 *85.02 *82.73TinySegmenter *86.93 *85.19 *86.05Rakuten MA 96.76 97.30 97.03Table 2: Segmentation Performance Comparison with Different Systems* These systems use different WS criteria and their performance is shown simply for reference.
** Wepostprocessed JUMAN?s WS result so that the WS criteria are closer to Rakuten MA?s.Forward-Backward Splitting (FOBOS) (Duchi and Singer, 2009) is a framework to intro-duce regularization to online learning algorithms.
For each training instance, it runs uncon-strained parameter update of the original algorithm as the first phase, then solves an instan-taneous optimization problem to minimize a regularization term while keeping the parameterclose to the first phrase.
Specifically, letting wt+12,jthe j-th parameter after the first phrase ofiteration t and ?
the regularization coefficient, parameter update of FOBOS with L1 regulariza-tion is done by: wt+1,j= sign(wt+12,j) [???wt+12,j????
?]+.
The strength of regularization can beadjusted by the coefficient ?.
In combining SCW and FOBOS, we retained the confidence value?
of SCW unchanged.Feature Quantization simply multiplies float numbers (e.g., 0.0165725659236262) by M(e.g., 1,000) and round it to obtain a short integer (e.g., 16).
The multiple M determines thestrength of quantization, i.e., the larger the finer grained, but the larger model size.3 ExperimentsWe used CTB 7.0 (Xue et al., 2005) for Chinese (CN), and BCCWJ (Maekawa, 2008) forJapanese (JA), with 50,805 and 60,374 sentences, respectively.
We used the top two levels ofBCCWJ?s PoS tag hierarchy (38 unique tags) and all the CTB PoS tags (38 unique tags).
Theaverage decoding time was 250 millisecond per sentence on Intel Xeon 2.13GHz, measured onnode.js.
We used precision (Prec.
), recall (Rec.
), and F-value (F) of WS as the evaluation metrics,averaged over 5-fold cross validation.
We ignored the PoS tags in the evaluation because theyare especially difficult to compare across different systems with different PoS tag hierarchies.Comparison with Other Analyzers First, we compare the performance of Rakuten MAwith other word segmenters.
In CN, we compared with Stanford Segmenter (Tseng et al., 2005)and zpar (Zhang and Clark, 2011).
In JA, we compared with MeCab (Kudo et al., 2004),JUMAN (Kurohashi and Nagao, 1994), KyTea (Neubig et al., 2011), and TinySegmenter.Table 2 shows the result.
Note that, some of the systems (e.g., Stanford Parser for CN andMeCab+UniDic for JA) use the same corpus as the training data and their performance isunfairly high.
Also, other systems such as JUMAN and KyTea employ different WS criteria,and their performance is unfairly low, although JUMAN?s WS result was postprocessed so that41!
"#$%"#$&"#$'"#$($ )$ *$ +$ ,$ !$ %$ &$ '$ ("$!
"#$%#&'()"*+',)-*./&0"#*-./01$2/01$3$Figure 3: Domain Adaptation Result for CN!
"#$%"#$&"#$'""#$'$ ($ )$ *$ +$ ,$ !$ %$ &$ '"$!
"#$%#&'()"*+',)-*./&0"#*-./01$2/01$3$Figure 4: Domain Adaptation Result for JA!
"#"""$%&'($%&')$%&'*$%&'+$,&'-"./($,&*-"./($,&'-"./)$,&*-"./)$,&'-"./*$012$0'2$"-(*$"-3"$"-3*$"-4"$"-4*$*""$ *#"""$!"#$%&'"()*&"+),"-./"56789:;8$<86=-$>67?:;@$<A5AB$<86=-$>67?:;@C<A5AB$DE6;=-$<86=-$>67?
:;@CDE6;=-$<A5ABCDE6;=-$F99$Figure 5: Model Comparisonit gives a better idea how it compares with Rakuten MA.
We can see that Rakuten MA canachieve WS performance comparable with the state-of-the-art even without using dictionaries.Domain Adaptation Second, we tested Rakuten MA?s domain adaptation ability.
We chosee-commerce as the target domain, since it is a rich source of out-of-vocabulary words and posesa challenge to analyzers trained on newswire text.
We sampled product titles and descriptionsfrom Rakuten Taiwan3 (for CN, 2,786 sentences) and Rakuten Ichiba4 (for JA, 13,268 sentences).These collections were then annotated by human native speakers in each language, following thetagging guidelines of CTB (for CN) and BCCWJ (for JA).We divided the corpus into 5 folds, then used four of them for re-training and one for testing.The re-training data is divided into ?batches,?
consisting of mutually exclusive 50 sentences,which were fed to the pre-trained model on CTB (for CN) and BCCWJ (for JA) one by one.Figure 3 and 4 show the results.
The performance quickly levels off after five batches for JA,which gives an approximated number of re-training instances needed (200 to 300) for adaptation.Note that adaptation took longer on CN, which may be attributed to the fact that Chinese WSitself is a harder problem, and to the disparity between CTB (mainly news articles in mainlandChina) and the adaptation corpus (e-commerce text in Taiwan).3http://www.rakuten.com.tw/.
Note that Rakuten Taiwan is written in Taiwanese traditional Chinese.
Itwas converted to simplified Chinese by using Wikipedia?s traditional-simplified conversion table http://svn.wikimedia.org/viewvc/mediawiki/trunk/phase3/includes/ZhConversion.php.
Still, having large Taiwan spe-cific vocabulary poses additional challenges for domain adaptation.4http://www.rakuten.co.jp/42Compact Model Representation Third, we consider the three techniques, and investigatehow these techniques affect the trade-off between WS performance and the model size, which ismeasured by the feature trie byte size in raw JSON format5.Figure 5 shows the scatter plot of F-value vs model size in KB, since we are rather interestedin the trade-off between the model size and the performance.
The baseline is the raw modelwithout any techniques mentioned above.
Notice that the figure?s x-axis is in log scale, and theupper left corner in the figure corresponds to better trade-off (higher performance with smallermodel size).
We can observe that all the three techniques can reduce the model size to someextent while keeping the performance at the same level.
In fact, these three techniques areindependent from each other and can be freely combined to achieve better trade-off.
If we limitstrictly the same level of performance compared to the baseline, feature hashing (17bit hashsize) with quantization (Point (1) in the figure) seems to achieve the best trade-off, slightlyoutperforming the baseline (F = 0.9457 vs F = 0.9455 of the baseline) with the model size ofas little as one fourth (5.2MB vs 20.6MB of the baseline).
It is somewhat surprising to see thatfeature quantization, which is a very simple method, achieves relatively good performance-sizetrade-off (Point (2) in the figure).4 ConclusionIn this paper, we proposed Rakuten MA, a lightweight, client-side morphological analyzerentirely written in JavaScript.
It supports online learning based on the SCW algorithm, whichenables quick domain adaptation, as shown in the experiments.
We successfully achieved acompact model size of as little as 5MB while maintaining the state-of-the-art performance, usingfeature hashing, FOBOS, and feature quantization.
We are planning to achieve even smallermodel size by adopting succinct data structure such as wavelet tree (Grossi et al., 2003).AcknowledgementsThe authors thank Satoko Marumoto, Keiji Shinzato, Keita Yaegashi, and Soh Masuko fortheir contribution to this project.ReferencesMichael Collins.
2002.
Discriminative training methods for hidden Markov models: theory and experiments withperceptron algorithms.
In Proc.
of the EMNLP 2002, pages 1?8.John Duchi and Yoram Singer.
2009.
Efficient online and batch learning using forward backward splitting.Journal of Machine Learning Research, 10:2899?2934.Roberto Grossi, Ankur Gupta, and Jeffrey Scott Vitter.
2003.
High-order entropy-compressed text indexes.
InProf.
of SODA 2003, pages 841?850.Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004.
Applying conditional random fields to Japanesemorphological analysis.
In Proceedings of EMNLP, pages 230?237.Sadao Kurohashi and Makoto Nagao.
1994.
Improvements of Japanese morphological analyzer JUMAN.
InProceedings of the International Workshop on Sharable Natural Language Resources, pages 22?38.Kikuo Maekawa.
2008.
Compilation of the Kotonoha-BCCWJ corpus (in Japanese).
Nihongo no kenkyu (Studiesin Japanese), 4(1):82?95.Graham Neubig, Yosuke Nakata, and Shinsuke Mori.
2011.
Pointwise prediction for robust, adaptable japanesemorphological analysis.
In Proceedings of ACL-HLT, pages 529?533.Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel Jurafsky, and Christopher Manning.
2005.
A conditionalrandom field word segmenter.
In Fourth SIGHAN Workshop on Chinese Language Processing.Jialei Wang, Peilin Zhao, and Steven C. Hoi.
2012.
Exact soft confidence-weighted learning.
In Proc.
of ICML2012, pages 121?128.Kilian Weinberger, Anirban Dasgupta, Josh Attenberg, John Langford, and Alex Smola.
2009.
Feature hashingfor large scale multitask learning.
In Proc.
of ICML 2009, pages 1113?1120.Naiwen Xue, Fei Xia, Fu-dong Chiou, and Marta Palmer.
2005.
The penn Chinese treebank: Phrase structureannotation of a large corpus.
Natural Language Engineering, 11(2):207?238.Yue Zhang and Stephen Clark.
2011.
Syntactic processing using the generalized perceptron and beam search.Computational Linguistics, 37(1):105?151.5We used one fifth of the Japanese corpus BCCWJ for this experiment.
Parameter ?
of FOBOS was variedover {1.0?
10?7, 5.0?
10?7, 1.0?
10?6, 5.0?
10?6, 1.0?
10?5}.
The hash size of feature hashing was varied over14, 15, 16, 176.
The multiple of feature quantization M is set to M = 1000.43
