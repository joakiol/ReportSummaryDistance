Probabilistic Parsing and Psychological PlausibilityThors ten  Brants  and  Mat thew CrockerSaarland University, COlnl)U|;al;ional LinguisticsD-6G041 Saarbriicken, Germany{brants ,  crocker}@coi?,  un?-sb ,  deAbst ractGiven the recent evidence for prot)abilisticmechanisms in models of hmnan aml)iguity res-olution, this paper investigates the plausibil-ity of exl)loiting current wide-coverage, 1)rob-al)ilistic 1)arsing techniques to model hmnanlinguistic t)ert'orman(:e. In l)arl.i(:ulm ', we in-vestigate the, t)crforlnance of stan(tar(l stoclms-tic parsers when they arc revis(;(l to el)crateincrementally, and with reduced nlenlory re-sources.
We t)resent techniques for rankingand filtering mlMyses, together with exl)erimen-tal results.
Our results confirm that stochas-tic parsers which a(lhere to these 1)sy('hologi-cally lnotivated constraints achieve goo(l l)er-f()rman(:e. Memory cast t)e reduce(t (lown to1% ((:Oml)are(l to exhausitve search) without re-ducing recall an(l 1)rox:ision.
A(lditionally, thes(;models exhil)it substamtially faster l)ertbrmance.FinMly, we ~rgue that this generM result is likelyto hold for more sophisticated, nnd i)sycholin-guistically plausil)le, probal)ilistic parsing mod-els.1 I n t roduct ionLanguage engineering and coml)ut~tional psy-cholinguistics are often viewed as (list|net re-search progrmnmes: engineering sohttions aimat practical methods which ('an achieve good1)erformance, typically paying little attentionto linguistic or cognitive modelling.
Comlm-tational i)sycholing,fistics, on the other hand,is often focussed on detailed mo(lelling of hu-man lmhaviour tbr a relatively small numberof well-studied constructions.
In this paper wesuggest hat, broadly, the human sentence pro-cessing mechanism (HSPM) and current statis-ti(:al parsing technology can be viewed as havingsimilar ol)jectives: to optimally (i.e.
ral)idly andaccurately) understand l;he text and utl;erancesthey encounter.Our aim is to show that large scale t)robabilis-tic t)arsers, when subjected to basic cognitiveconstraints, can still achieve high levels of pars-ing accuracy.
If successful, this will contributeto a t)lausil)h; explanation of the fact th~tt I)(;() -\])lc, in general, are also extremely accurate androl)llS(;.
Sllch a 1'o81111; Wollld also strellgthclt ex-isting results showing that related l)robal)ilisticlne('hanisms can exl)lain specific psycholinguis-tic phenomena.To investigate this issue, we construct a stan-dard 'l)aseline' stochastic parser, which mir-rors t;he pertbrmance of a similar systems (e.g.
(,lohnson, 1998)).
We then consider an incre-re(total version of th(', parser, and (;v~,htat(; timetf'c(:ts of several l)rol)al)ilistic filtering strate-gies which m'e us(,(l to 1)rune the l)arser's earchspace, and ther(;l)y r('(lu('(', memory load.rio &,,-;sess th(; generMity of oltr resnll;s formore Sol)histi(;ate(t prot)al)ilistic models, we alsoconduct experiments using a model in whichparent-node intbrmation is encoded on the(laughters.
This increase in contextual informa-tion has t)(;(;11 shown 1;o improve t)erforlnance(.Johnson, 1998), and the model is also shownto be rolmst to the inerementality and memoryconstraints investigated here.We present the results of parsing pertbr-mance ext)eriments , showing the accuracy ofthese systems with respect to l)oth a parsedcorpus and the 1)aseline parser.
Our experi-ments suggest hat a strictly incremental model,in which memory resources are substantiallyreduced through filtering, can achieve l)reci-sion and recall which equals that of 'unre-stricted' systems.
Furthermore, implementa-tion of these restrictions leads to substantiallyfaster 1)(;rtbrmance.
In (:onchlsion, we arguethat such 1)road-coverage probabilistic parsing111models provide a valuable framework tbr ex-plaining the human capacity to rapidly, accu-rately, and robustly understand "garden va-riety" language.
This lends further supt)ortto psycholinguistic a counts which posit proba-bilistic ambiguity resolution mechanisms to ex-plain "garden path" phenomena.It is important to reiterate that our intentionhere is only to investigate the performance ofprobabilistic parsers under psycholinguisticallymotivated constraints.
We do not argue for thepsychological plausibility of SCFG parsers (orthe parent-encoded variant) per se.
Our inves-tigation of these models was motivated ratherby our desire to obtain a generalizable resultfor these simple and well-understood models,since obtaining similar results for more sophisti-cated models (e.g.
(Collins, 1996; Ratnaparkhi,199711 might have been attributed to specialproperties of these models.
Rather, the currentresult should be taken as support br the poten-tial scaleability and performance ofprobabilisticI)sychological models uch as those proposed by(aurafsky, 1996) and (Crocker and Brants, toappear).2 Psycholinguistic Mot ivat ionTheories of human sentence processing havelargely been shaped by the study of pathologiesin tnnnan language processing behaviour.
Mostpsycholinguistic models seek to explain the d{f-ficulty people have in comprehending structuresthat are ambiguous or memory-intensive (see(Crocker, 1999) for a recent overview).
Whileoften insightflfl, this approach diverts attentionfrom the fact that people are in fact extremelyaccnrate and effective in understanding thevast majority of their "linguistic experience".This observation, combined with the mountingpsycholinguistic evidence for statistically-basedmechanisms, leads us to investigate the merit ofexploiting robust, broad coverage, probabilistieparsing systems as models of hmnan linguisticpertbrmance.The view that hmnan language processingcan be viewed as an optimally adapted sys-tem, within a probabilistic fl'amework, is ad-vanced by (Chater et al, 19981, while (Juraf-sky, 19961 has proposed a specific probabilis-tic parsing model of human sentence process-ing.
In work on human lexical category dis-ambiguation, (Crocker and Corley, to appear),have demonstrated that a standard (iimrmnen-tal) HMM-based part-of-speech tagger mod-els the finding from a range of psycholinguis-tic experiments.
In related research, (Crockerand Brants, 19991 present evidence that anincremental stochastic parser based oll Cas-caded Markov Models (Brants, 1999) can ac-count tbr a range of experimentally observedlocal ambiguity preferences.
These includeNP/S complement ambiguities, reduced relativeclauses, noun-verb category ambiguities, and'that'-ambiguities (where 'that' can be either acomplementizer or a determiner) (Crocker andBrants, to appear).Crucially, however, there are differences be-tween the classes of mechanisms which are psy-chologically plausible, and those which prevailin current language technology.
We suggest thattwo of the most important differences concernincrcmentality~ and memory 7vso'urces.
There isoverwhehning experimental evidence that peo-ple construct connected (i.e.
semantically in-terpretable) analyses for each initial substringof an utterance, as it is encountered.
That is,processing takes place incrementally, from leftto right, on a word by word basis.Secondly, it is universally accecpted that peo-ple can at most consider a relatively smallnumber of competing analyses (indeed, somewould argue that number is one, i.e.
process-ing is strictly serial).
In contrast, many exist-ing stochastic parsers are "unrestricted", in thatthey are optinfised tbr accuracy, and ignore sucht)sychologically motivated constraints.
Thus theappropriateness of nsing broad-coverage proba-bilistic parsers to model the high level of hu-man performance is contingent upon being ableto maintain these levels of accuracy when theconstraints of" incrementality and resource limi-rations are imposed.3 Incremental StochasticContext-Free ParsingThe fbllowing assumes that the reader is fa-miliar with stochastic context-free grammars(SCFG) and stochastic chart-parsing tech-niques.
A good introduction can be found, e.g.,in (Manning and Schfitze, 19991.
We use stan-dard abbreviations for terminial nodes, 11051-terminal nodes, rules and probabilities.112This t)tq)er invcsl;igates tochastic (;onl;(;xl;-fl'ee parsing l)ascd on ~ grmmmu" (;hat is (tcrivc(lfrom a trcel)ank, start ing with 1)art-ofsl)eechta,gs as t(;rlninals.
The gl:;~nllnt~r is (lcriv(;d l)y(:olle(:ting M1 rul('.s X -+ c~ th;tt oc(:ur in the tr(',(;-bank mM (;heir ffe(lU(m(:i('~s f .
The l)l'()l);tl)ilil;yof a rule is set to.f(x l ' ( x  - (:l)E .f(xfl\],br ~ descril)l;ion of treebank grammars see(Charniak, 1.996).
The gr~mmmr does not coii-ta.in c-rules, oth(:rwis(: th(:r(: is no restrictionoll the rules.
In particular, w(: do not r(:quir('C homsky-NormM-Form.In addit ion to the rult:s tha(; corr(:st)ond(;o sl;rucl;ur(:s in th(: corpus, w(: a.dd ;~ newst~u:l; sylnl)ol ROOT to l;h(; grnmmar and rulesROOT -~ X for all non-t;(;rminals X togel;lwxwith l)rol)al)iliti('s (h:):iv(:d l'roln th(: root n()(t(:sin th(: tort)us I.For t)m:sing th(:se gr~unmn)'s, w(: r(:ly uponn stan(tard l)oLi;onl-U t) (:ha.rl,-t)arsing t(:(:hniqu(:with n modification for in(:rcmental parsing, i.
(:.,tbt" each word, all edges nr(: proc(:ss(:d and l)ossi-b\]y 1)run(:d 1)(:ti)r(: \])ro(:e(:(ling to the next word.Th(: outlilm of th(: Mgorithm is as follows.A (:hart; (:ntry 1~ (:onsists of a sl;;u:I, aim (:n(l 1)o-s it ion i ;rod j ,  a (tott(:d rul(: X ~ (~:.
'7, tim insi(t(:l)rol)nl)ility fl(Xi,.j) thud; X g(:n(:ra.tx:s l;ll(: t(:rmi-hal string from t)osi(:ion i to .7, mM informationM)out th(: most l)robat)\](: ilL~i(t(' stru(:i;ur(:.
1t7 th(:dot of th(: dotte(t ruh: is nt th(' r ightmost i)osi-tion, the corresl)ondillg (:(lg(: is an inactive edg(:.If the (tot is at mty other 1)osition, il; is mt ,,ctivc,edge.
Imu:l;ivo, e(tgcs repr(',scnt re('ogniz(',d hypo-(:heti(:a,1 constituents, whil(; a(:tiv(; (;(lg(',s r(;1)r(;-s(:nt 1)r(:lixes of hyl)ol;heticM (:()llsi;it;ll(:lll;s.Th(: i th t(:rminal nod(: I,i l;lla, t; (:nt(:rs th(: (:hartgencra, tcs an inactive edge for l;\]m span (i - 1, i).Ba, sed on this, n(;w active mid inactive (;(lges aregenerated according to the stan(t~tr(t algorithm.Sine(: we are ilfl;(:r(:stcd in th(: most i)robM)lepars(:, the chart can be minimized in th(: tbl-lowing way whik: sti\]l 1)crfi)rming an ('xhaustiv(:search.
If" ther(: is mor(: l;hm~ one (:(lg(~ that  cov-ers a span ( i , j )  having (;h(', sa, me non-t(:rminMsymbol on th(; lefIAmnd side of th(: (to(,(x:(l rule,1The ROOT node is used int;ernally fl)r parsing; it isneither emitted nor count,ed for recall and l)recision.only the one with the highest inside prol)M)ilityis k(;1)t ill tit(; (:\]mrt.
The others cmmot con-trilmt(; to th(; most i)rol)M)le 1)nrse..For an ina('tiv(: edge si)aiming i to j and rei)-rcs(mting the rule X --> y1 .
.
.yq~ the insidel)robM)ility/31 is set to\[dil = 1"(x -+ H (2)/=\]wher(: il and jl mm'k the start and end t)ostitionof Yl having i = il nnd j = Jr.
The insid(:prol)M)ility tbr an active cdg(: fiA with the dotafter th(: kth syml)ol of th(: right-hmM side issol, tokI I  <r ' t I t  il,jl} (3)l-dW(: (lo not use the t)rol)M)i\]ity of th(: rule a.t thispoint.
This allows us to ('oral)in(: a.ll (:(Ig(:s with(;h(: sam(: st)m~ and th(: dot al; th(: sam(: 1)ositionbut with (liiI'er(:uI; symbols on the l(,ft-hmM side.Jntrodu(:ing a distinguish(:(1 M't-hand sid(: onlyfor in~mtiv(: (lg('s significantly r(:du(;(:s th(: nun>b(:r of a(:(;iv(: (:dg(:s in the (:hm't.
This goes onest, e t) furth(:r than lint)licitly right-1)inarizing th(:grmmnar; not only suilix(:s of right-hmM si(h:sare join(:(t, but also l;hc ('orr(:sponding l(:fi;-handsid(:s.d Memory  Rest r i c t ions\?
(: inv(:stig~rt(: th(: (dimin~I;ion (pruning) ()fedges from th(: ('hnrt in our in('r(:nl(:n|;a| \])re's-ing sch(:m(:.
Aft(:r processing a word and b(:fi))'(:1)roc(:cding to the n(:xt word during incremental1)re:sing, low rnnk(,d edges ~r(: removed.
This is(:(luivM(:lfl; t;() imposing m(:mory rcsia'ictions onthe t)ro(:('ssing system.The, original algor ithm k('ei)s on(; edge in th(:(:hart fi)r each (:oml)ination of span (start andcn(l position) ~md non-tcrmimd symbol (for in-active edges) or r ight-hand side l)r(:fixcs of (lot;-te(t rules (for active edges).
With 1)tinting, werestric(; the mmfl)cr of edges allowed per span.The limit~tion (:an b(: cxi)resscd in two ways:1.
Va'riable bcam,.
Sch:ct a threshold 0 > 1.Edg(: c. is removed, ill its 1)rol)ability is p~:,I;lm 1)csl; l)rol)M)ility fi)r the span is Pl, andv,; < pl_.
(~l)01132.
Fixed beam.
Select a maximum number ofedges per span m. An edge e is removed, ifits prot)ability is not in the first m highestprobabilities tbr edges with the same span.We pertbrmed exl)eriments using both typesof beauls.
Fixed beams yielded consistently bet-ter results than variable beams when t)lottingchart size vs. F-score.
Thereibre, the followingresults are reported tbr fixed t)eams.We, compare and rank edges covering thesame span only, and we rank active and inactiveedges separately.
This is in contrast to (Char-niak et al, 1998) who rank all edges.
Theyuse nornmlization in order to account tbr dif-ferent spans since in general, edges for longerspans involve more nmltiplications of t)robabil -ities, yielding lower probabilities.
Charniak etal.
's normalization value is calculated by a dil-ferent probability model than the inside proba-bilities of the edges.
So, in addition to the nor-malization for different span lengths, they needa normalizatio11 constant hat accounts tbr thedifferent probability models.This investigation is based on a much simt)lerranking tbrmula.
We use what can be describedas the unigram probability of a non-terminalnode, i.e., the a priori prot)ability of the co lresl)onding non-ternlinal symbol(s) times theinside t)robat)ility.
Thus, fi~r an inactive edge(i, j, X --> (~,/31(Xi,j)}, we use the l)rob~fl)ilityPm(X i , j )  = P (X)  .
P ( tg .
.
.
t j _ I IX )  (5)=for ranking.
This is the prol)ability of the nodeand its yield being present in a parse.
Thehigher this value, |;lie better is this node.
flI isthe inside probability for inactive edges as givenin eqnation 2, P(X)  is the a priori probabilitytbr non-terminal X, (as estimated from the fre-quency in the training COrlmS) and Pm is theprobability of the edge tbr the non-terminal Xspanning positions i to j that is used tbr rank-ing.For an active edge { i , j ,X  --~ y1 .
.
.
yk .yk+l  ym,  y )  k ? "
~ , ,~ )) " ' "  ~A( i l , j l  (the (tot is aI"ter the kth symbol ofllSe:the right-hand side) we(7)= P(Y I .
.
.Yk ) .
f lA (E I~, :h .
.
.Y i~ , jk )  (9)p (y l  ,,, yk)  can be read ()If the corpus.
It isthe a priori probability that the right-hand sideof a production has the prefix y1 ... y/c, whichis estilnated byf (y l  .
.
.
yt~ is prefix) 00)Nwhere N is the total number of productions inthe corpus 2, i = ij, j = j/~ and flA is the insideprobability of the pretix.5 Exper iments5.1 DataWe use sections 2 - 21 of the Wall Street Jour-lYecl)ank (Marcus el; al., nal part of' the Penn ~ "1993) to generate a treebank grammar.
Traces,flmctional tags and other tag extensions that donot mark syntactic ategory are removed beforetraining 3.
No other modifications are made.
Fortesting, we use the \] 578 sentences of length 40or less of section 22.
The input to the parser isthe sequence of i)art-ofspeech tags.5.2 Eva luat ionFor evaluation, we use the parsewfi measuresand report labeld F-score (the harmolfiC meanof labeled recall and labeled precision).
R.eport-ing the F-score makes ore" results comt)aral)le tothose of other previous experinmnts using thesame data sets.
As a n leasure  of  the an lountof work done by the parser, we report the sizeof the chart.
The mnnl)er of active and imm-rive edges that enter the chart is given tbr theexhaustive search, not cored;lug those hypothet-ical edges theft are replaced or rejected becausethere is an alternative dge with higher t)roba-t)ility 4.
For t)runed search, we give |:tie percent-age of edges required.5.3 F ixed BeamFor our experiments, we define the beam by amaximunl number of edges per span.
Beamsfor active and inactive edges are set separately.The Imams run from 2 to 12, and we test all2Here, we use proper prefixes, i.e., all prefixes notincluding the last element.aAs an example, PP-TMP=3 is replaced 173, PP.4The size of the chart is corot)arable to the "numberof edges popped" as given in (Chanfiak et al, 1998).114i7877(D8 7o cJ~75(1)74 z$73727179Resu l ts  w i th  Or ig ina l  and  Parent  Encod ingActive: 8/ ma?.:l'~v?
", "~I ;t(:l ix'(" (i/ j  j~  activ(',: 3C" ' ' ; "  F l l ldCt  1V( 12active: 9inactive,: 2active: 3 /1.0 1.2ina(:tiv(',: 6 ilmctive: 8a(:l.ive: d .
a('tivo,: 7I I 1 \[ \] i I I i - -\] .d 1.6 1.8 2.0 2.2 2.d 2.6 2.8 3.0 % (:hart; sizeFigure 1: \]!
}xt)erimental results tbr increJnelfl;al parsing and t)rmfing.
The figm:e shows the percent-age of edges relative to (',xhaustiv(; s(;ar(:h mid l;h(', F-s(:()re a(:hieved with this chart size.
Exhaustivesearch yiehled 71.21% fin" th(; original en(:o(ting and 7!
).28% for the I)arent (m(:o(ting.
l/.c, sull;s in thegrey ar(;as are equiwflent with a (:()nli(l('n(:('~ (tegr(',e of (~ =: 0.99.12\] comlfi\]~ati(ms of the, s(~ lmmus for ac:i;ivc andillactiw~ edges, l~ach setting results in a lm.ri;ic -ulm" average size of l;he chart and an F-score,which arc tel)erred ill (;he following se(:l;ioll.5.4  Exper imenta l  Resu l tsThe results of our 121 tes(; Hills with (tifl'erentsettings for active and in;u:tivc \])(~a.ms m'e givenin figure 1.
The (tittgranl shows ch~trt sizes vs.labeled F-scores.
It sorts char|; sizes across d i fferent sel;l;ings of the beams.
If several beamsett;ings result in equiwdenfi chart sizes, the di-agram cent;tins the one yielding th(', highes|, F-SCOI ' (LThe 111~ill tinding is thai: we can r('xlu(:e thesize of the chart to l)el;ween 1% and 3% ofthe size required fi)r exhaustive s(,ar(:h withoutaffecting the results.
Only very small 1)camsd(;grad(' t)ertbrmance 5.
The eiti;ct occurs forboth models despite the simple ranking formub~.This significantly reduces memory r(,quirements'~Givc, n the' amount of test data (26,322 non-terminalnod(!s), results within a rang(' of around 0.7% arc cquiv-al(mt with a (:onfidcnc(; degr(',(, of (~ = 99%.
(given as size of the chart) and increases l)m'singqmed.i1 t Exhaustive search yields an I -Score of71.21 % when using the original Petal %'eel)ankcn(:odh~g.
()nly around 1% the edges are re-(tuir('.d to yield e.(tuiwdcnt resul(;s with incrcm(,.n-tal processing and printing after each word isadded to the chart;.
This result is, among othersettings, obtained by a tixcd beam of 2 for in-active edges and 3 tin" active e(lges ri1,br the parmtt encoding, exhaustive searchyields an l,-Scorc of 79.28%.
Only 1)etween 2mM 3% of the edges are required to yMd anequiwflcnt result with incremental t)l'OCcSSillgand pruning.
As an cXmnl)le, the point at size= 3.0% F-score = 79.1% is generated by thebeam setting of 12 for imml;ive and 9 tbr activeedges.
The parent encoding yields around 8%higher F-scores but it also imposes a higher ab-solute and relative memory load on t;he process.The higher (hw'ee of par~dlelism in l;he inactive(;Using variable Imams, wc would nccd \].95% of the\[:hart entries 1;o achieve an (Kl l l ivalenI ;  F - scor (x115chart stems from the parent hytmthesis in eachnode.
In terms of pure node categories, the av-erage number of parallel nodes at this point is3.5 7 .Exhaustive search for the base encoding needsin average 140,000 edges per sentence, tbr tileparent encoding 200,000 edges; equivalent re-sults for the base encoding can be achieved witharound 1% of these edges, equivalent results tbrthe parent encoding need between 2 and 3%.The lower mmlber of edges significantly in-creases parsing speed.
Using exhaustive searchtbr the base model, the parser processes 3.0 to-kens per second (measured on a Pentium III500; no serious efforts of optimization have goneinto the parser).
With a chart size of 1%, speedis 630 tokens/second.
This is a factor of 210without decreasing accuracy.
Sl)eed for the par-ent model is 0.5 tokens/second (exhaustive) and111 tokens/seconds (3.0% chart size), yieldingan improvement by factor 220.6 Related WorkProbably mostly related to the work reportedhere are (Charniak et al, 1998) and (Roark andJohnson, 1999).
Both report on significantlyimproved parsing efl:iciency by selecting onlysubset of edges tbr processing.
There are threemain differences to our at)t)roach.
One is thatthey use a ranking fbr best-first search whilewe immediately prune hypotheses.
They needto store a large number edges because it is notknown in advance how maw of the edges will beused until a parse is found.
Tile second differ-ence is that we proceed strictly incrementallywithout look-ahead.
(Chanfiak et al, 1998)use a non-incremental procedure, (Roark andJohnson, 1999) use a look-ahead of one word.Thirdly, we use a much simpler ranking tbnnula.Additionally, (Chanfiak et al, 1998) and(Roark and Johnson, 1999) do not use theoriginal Penntree encoding tbr the context-fl'eestructures.
Betbre training and parsing, theychange/remove some of the productions and in-troduce new part-of-speech tags tbr auxiliaries.The exact effect of these modifications is un-known, and it is unclear if these affect compa-7For the active chart, lmralellism cannot be given fordifferent nodes types since active edges are introducedfbr right-hand side prefixes, collapsing all possible left-hand sides.rability to our results.Tile heavy restrictions in our method (imme-diate pruning, no look-ahead, very simple rank-ing formula) have consequences on the accuracy.Using right context and sorting instead of prun-ing yields roughly 2% higher results (comparedto our base encodingS).
But our work showsthat even with these massive restrictions, thechart size can be reduced to 1% without a de-crease in accuracy when compared to exhaustivesearch.7 ConclusionsA central challenge in computational psycholin-guistics is to explaiu how it is that people areso accurate and robust in processing language.Given the substantial psycholinguistic evidencetbr statistical cognitive mechanisms, our objec-tive in this paper was to assess the plausibilityof using wide-coverage probabilistic parsers tomodel lmman linguistic performance.
In par-ticular, we set out to investigate the effects ofimposing incremental processing and significantmemory limitations on such parsers.The central finding of our experiments i thatincremental parsing with massive (97% - 99%)pruning of the search space does not impairthe accuracy of stochastic ontext-free parsers.This basic finding was rotmst across differentsettings of the beams and tbr the original PennTreebank encoding as well as the parent encod-ing.
We did however, observe significantly re-duced memory and time requirements when us-ing combined active/inactive dge filtering.
Toour knowledge, this is the first investigation ontree-bank grammars that systematically variesthe beam tbr pruning.Our ainl in this paper is not to challengestate-of-the-art parsing accuracy results.
Forour experiments we used a purely context-ti'eestochastic parser combined with a very sim-ple pruning scheme based on simple "unigram"probabilities, and no use of right context.
Wedo, however suggest hat our result should ap-ply to richer, more sophistacted probabilisticSComparison of results is not straight-forward since(Roark and Johnson, 1999) report accuracies only tbrthose sentences for which a parse tree was generated (be-tween 93 and 98% of the sentences), while our parser(except for very small Imams) generates parses for vir-tually all sentences, hence we report; accuracies for allsentences.116models, e.g.
when adding word st~tistics to themodel (Charni~d?, 1997).We thereibre conclude theft wide-covcr~ge,prol)~fl)ilistic pnrsers do not suffer impaired a('-curacy when subject to strict cognii;iv(~ meXnOl'ylimitntions mM incremental processing.
Fm'-thermore, parse times are sut)stm~ti~fily reduced.This sltggt',sts that it; m~y lie fruit;tiff to tlur,sllCthe use of these models within ?
',onlt)utationall)sycholinguistics, where it: is necessary to ex-plain not Olfly the relatively r~tr(; 'pathologies' ofthe hmmm parser, but also its mor(; fl'e(tuentlyol)scrved ~u:(:ur~my ~(1 rol)llSiilless.ReferencesThorst;en \]h'mfl;s. 1999.
Cascadt;d Mm'kov mod-els.
In P'rocecdings Vf 9th, Cm@~'t'.'m:e.
ofthe EuTvpea'n Chapter of the Association ,fro"Com.p'atatiou, al Linguistics EA 6'\])-99, B(;rg(;n,Norway.Eugene Charni~k, Sharon (\]ohlwater, and Mnrk,Johnson.
1998. ltMge-b~sed lmst-tirst (:hartpro'sing.
In l~'rocec.dings of l, hc.
Si:cl, h. l,Vor/,:-shop on l/cry LaT~\](: Corpora (WVLC-9S),Montreal, K~ma(la.Eugene Ch~rni~fl?.
1996.
'15:ee-bank grmmm~rs.In P'rocecding,~ of t,h,c Th, irtec'nth, NationalCm@rc'.nce on A'rt'ti/icial lntdlig(:,m:~:, l)a.g(,,s1031 1036, Menlo Pnrk: AAA\] Press/M1Tl)i.ess.\]!htgen('.
Chm:nia.k.
1997.
Sl;a.i;isti(:al \]mrs-ing wit;h ~t context-fr(:(~ gl:;41111llVtl' 2.1~11(1 \voF(|statistics.
In P~'occ.cdings qf the \],b,a'rt,(:enthNational Co~@'r('.nce o'n A'rt~ificial Intelli-gence, pagc.s 1031 1036, Menlo Park: AAAIPress/MIT Press.Nicholas Chafer, Matthew Crock(;l', ~md MartinPickcring.
1998.
The rational analysis of in-quiry: The case.
for parsign.
In Charter andO~ksfor(1, editors, Ratio'hal Models o/" Cog'ni-tion.
Oxford University Press.Michael Collins.
1!196.
A new st~tistical l)arscrb~tse.d on l)igr~un lexical depend(;neies, inProceedings of ACL-96, Sa, llta, Cruz, CA,USA.Matthew Crocker and Thorsten Br~mts.
1999.Incremental probabilisti(: lnodels of lmmanlinguistic perform;race.
In The 5th Cm@r-cnce on Arc:hitcctu~v.s a'nd Mcch, anism.,s forLa'nguagc Processing, Edi~flmrgh, U.K.Matthew Crocker and Thorst;en Brmfl;s. to ~t)-l)car.
Wide cover~ge l)rol)~flfilistic sentenceprocessing.
Journal of Psych, oling'aistic Re-search,, November 2000.M~tthew Cro('kex mM Steil'an Corley.
to ~l)-peru:.
Modulm" nrchitectures and statisticMmcchnnisms: The case.
frolli lexical categorydisnmbiguntion.
In Merlo and Stevenson, ed-itors, The Lczical Basis of Sentence Process-in9.
John Bcnjamins.1VI~tthew Crocker.
1999.
Mech~misms for scn-|;ellce, tn'oeessing.
In Garrod and Pieker-ing, editors, Language Proc~ssing.
Psychology\])ross, London, UK.Mm'k ,Johnson.
1998.
PCFG models of linguis-tic t;rec \]'el)rcse\]fl;~tions.
Com, p,utational Lin-g'aistic.~, 24(4):613 632.\])~mi(;l .\]m:at~ky.
\]996.
A t)robabilistic n|o(M oflexi(:~tl nnd syntactic a(:(:ess and (lisambigua-tion.
Cognitive Science, 20:137 194.Christot)her Mmming mid Him'ieh S(:lfiil;ze_1999.
l,b,a.ndatiou, s of Statistical Natural Lan-9'uag(: P'roct'.s,si'ng.
MKI' Press, Cmnl)ridge,Mass~Lc:husetts.Mit(:hell IVlarmts, \]{eatrice S~mtorini, andMary Ann M~rcinkiewicz.
1993.
Buildinga lm:ge mmotated corl)us of English: TheP(mn Treet)ank.
Computational Linguistics,|!
)(2):313 330.A(twait l/.~ttnat)~trkhi.
11!)!)7.
A \]inem" ol)servedtime stnl;isI;ic;d t)m;ser based on m~tximmn en-tropy models.
In \])'rocc.c:ding.s of the Co',:fcr-?
:'m:c o'n Empirical Methods in Nat'a'ral La'n-g'uafle P'lvccssing \]'?MNLP-gZ Providence,11\].\]h'inn Ronrk ~md Mm:k Johnson.
1!199.
Efficientt)rol)al)ilisl, ic tot)-(lown ;rod left-(:orner pars-illg.
hi \])'l'occcdi~,.
(l,s of the.
,~7111, A~l, t,'ttal Mcc.t-i'ng of the A.ssociation for Cou~,p'atation Lin-g'aistic.~ A CL- 99, M~rybmd.117
