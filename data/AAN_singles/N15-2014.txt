Proceedings of NAACL-HLT 2015 Student Research Workshop (SRW), pages 103?109,Denver, Colorado, June 1, 2015.c?2015 Association for Computational LinguisticsDetecting Translation Direction: A Cross-Domain StudySauleh EetemadiMichigan State University, East Lansing, MIMicrosoft Research, Redmond, WAsaulehe@microsoft.comKristina ToutanovaMicrosoft ResearchRedmond, WAkristout@microsoft.comAbstractParallel corpora are constructed by taking adocument authored in one language and trans-lating it into another language.
However,the information about the authored and trans-lated sides of the corpus is usually not pre-served.
When available, this information canbe used to improve statistical machine trans-lation.
Existing statistical methods for trans-lation direction detection have low accuracywhen applied to the realistic out-of-domainsetting, especially when the input texts areshort.
Our contributions in this work are three-fold: 1) We develop a multi-corpus paral-lel dataset with translation direction labels atthe sentence level, 2) we perform a compara-tive evaluation of previously introduced fea-tures for translation direction detection in across-domain setting and 3) we generalize apreviously introduced type of features to out-perform the best previously proposed featuresin detecting translation direction and achieve0.80 precision with 0.85 recall.1 IntroductionTranslated text differs from authored text (Baker,1993).
The main differences are simplification, ex-plicitation, normalization and interference (Volan-sky et al, 2013).
Statistical classifiers have beentrained to detect Translationese1.
Volansky et al(2013) state two motivations for automatic detectionof Translationese: empirical validation of Trans-lationese linguistic theories and improving statis-tical machine translation (Kurokawa et al, 2009).1Translated text is often referred to as ?Translationese?
(Volansky et al, 2013).Most of the prior work focus on in-domain Trans-lationese detection (Baroni and Bernardini, 2006;Kurokawa et al, 2009).
That is, the training andtest set come from the same, usually narrow, do-main.
Cross-domain Translationese detection servesthe two stated motivations better than in-domain de-tection.
First, automatic classification validates lin-guistic theories only if it works independent of thedomain.
Otherwise, the classifier could performwell by memorizing lexical terms unique to a spe-cific domain without using any linguistically mean-ingful generalizations.
Second, a Translationeseclassifier can improve statistical machine translationin two ways: 1) By labeling the parallel trainingdata with translation direction2; 2) By labeling in-put sentences to a decoder at translation time anduse matching models.
The accuracy of the classifieris the main factor determining its impact on statis-tical machine translation.
Most parallel or mono-lingual training data sources do not contain transla-tion direction meta-data.
Also, the input sentencesat translation time can be from any domain.
There-fore, a cross-domain setting for translation directiondetection is more appropriate for improving statisti-cal machine translation as well.
We develop a cross-domain training and test data set and compare someof the linguistically motivated features from priorwork (Kurokawa et al, 2009; Volansky et al, 2013)in this setting.
In addition, we introduce a new bilin-gual feature that outperforms all prior work in both2Detection of translation direction refers to classifying atext block pair (A andB) asAwas translated toB or vice versa.In contrast, Translationese detection usually refers to classify-ing a single block of text as ?Translationese?
versus ?Original?.103in-domain and cross-domain settings.Our work also differs from many prior works byfocusing on sentence level, rather than block levelclassification.
Although Kurokawa et al (2009)compare sentence level versus block level detectionaccuracy, most other research focuses on block leveldetection (Baroni and Bernardini, 2006; Volanskyet al, 2013).
Sentence level classification servesthe stated motivations above better than block levelclassification.
For empirical validation of linguis-tic theories, features that are detectable at the sen-tence level are more linguistically meaningful thanblock level statistics.
Sentence level detection is alsomore appropriate for labeling decoder input as wellas some statistical machine translation training data.In the rest of the paper, we first review prior workon sentence level and cross-domain translation di-rection detection.
In Section 3 we motivate the se-lection of features used in this study.
Next, we de-scribe our cross-domain data set and the classifica-tion algorithm we use to build and evaluate modelsgiven a set of features.
Experimental results are pre-sented in Section 5.2.2 Related WorkVolansky et al (2013) provide a comprehensivelist of monolingual features used for Translationesedetection.
These features include POS n-grams,character n-grams, function word frequency, punc-tuation frequency, mean word length, mean sentencelength, word n-grams and type/token ratio.
We areaware of only one prior work that presented a cross-domain evaluation.
Koppel and Ordan (2011) use alogistic regression classifier with function word un-igram frequencies to achieve 92.7% accuracy withten fold cross validation on the EuroParl (Koehn,2005) corpus and 86.3% on the IHT corpus.
How-ever testing the EuroParl trained classifier on theIHT corpus yields an accuracy of 64.8% (and theaccuracy is 58.8% when the classifier is trained onIHT and tested on EuroParl).
The classifiers in thisstudy are trained and tested on text blocks of approx-imately 1500 tokens, and there is no comparativeevaluation of models using different feature sets.We are also aware of two prior works that in-vestigate Translationese detection accuracy at thesentence level.
First Kurokawa et al(2009) usethe Hansard English-French corpus for their ex-Label DescriptionENG.LEX English word n-gramsFRA.LEX French word n-gramsENG.POS English POS Tag n-gramsFRA.POS French POS Tag n-grmasENG.BC English Brown Cluster n-gramsFRA.BC French Brown Cluster n-gramsPOS.MTU POS MTU n-gramsBC.MTU Brown Cluster MTU n-gramsTable 1: Classification features and their labels.periments.
For sentence level translation direc-tion detection they reach F-score of 77% usingword n-grams and stay slightly below 70% F-scorewith POS n-grams using an SVM classifier.
Sec-ond, Eetemadi and Toutanova (2014) leverage wordalignment information by extracting POS tag mini-mal translation units (MTUs) (Quirk and Menezes,2006) along with an online linear classifier trainedon the Hansard English-French corpus to achieve70.95% detection accuracy at the sentence level.3 Feature SetsThe goal of our study is to compare novel and pre-viously introduced features in a cross-domain set-ting.
Due to the volume of experiments requiredfor comparison, for an initial study, we select a lim-ited number of feature sets for comparison.
Priorworks claim POS n-gram features capture linguis-tic phenomena of translation and should generalizeacross domains (Kurokawa et al, 2009; Eetemadiand Toutanova, 2014).
We chose source and tar-get POS n-gram features for n = 1 .
.
.
5 to test thisclaim.
Another feature we have chosen is from thework of Eetemadi and Toutanova (2014) where theyachieve higher accuracy by introducing POS MTU3n-gram features.POS MTUs incorporate source and target side in-formation in addition to word alignment.
Prior workhas also claimed lexical features such as word n-grams do not generalize across domains due to cor-pus specific vocabulary (Volansky et al, 2013).
Wetest this hypothesis using source and target word n-gram features.
Using n-grams of length 1 through 5we run 45 (nine data matrix entries times n-gramlengths of five) experiments for each feature setmentioned above.In addition to the features mentioned above, we3Minimal Translation Units (Quirk and Menezes, 2006)104Corpus Authored Language Translation Language Training Sentences Test SentencesEuroParl English French 62k 6kEuroParl French English 43k 4kHansard English French 1,697k 169kHansard French English 567k 56kHansard-Committees English French 2,930k 292kHansard-Committees French English 636k 63kTable 2: Cross-Domain Data Setsmake a small modification to the feature used to ob-tain the best previously reported sentence level per-formance (Eetemadi and Toutanova, 2014) to derivea new type of features.
POS MTU n-gram fea-tures are the most linguistically informed featuresamongst prior work.
We introduce Brown cluster(Brown et al, 1992) MTUs instead.
Our use ofBrown clusters is inspired by recent success on theiruse in statistical machine translation systems (Bhatiaet al, 2014; Durrani et al, 2014).
Finally, we alsoinclude source and target Brown cluster n-grams asa comparison point to better understand their effec-tiveness compared to POS n-grams and their contri-bution to the effectiveness of Brown cluster MTUs.Given these 8 feature types summarized in Table1, n-gram lengths of up to 5 and the 3 ?
3 data ma-trix explained in the next section, we run 360 exper-iments for this cross-domain study.4 Data, Preprocessing and FeatureExtractionWe chose the English-French language pair forour cross-domain experiments based on prior workand availability of labeled data.
Existing sentence-parallel datasets used for training machine trans-lation systems, do not normally contain gold-standard translation direction information, and addi-tional processing is necessary to compile a datasetwith such information (labels).
Kurokawa et al(2009) extract translation direction information fromthe English-French Hansard parallel dataset usingspeaker language tags.
We use this dataset, and treatthe two sections ?main parliamentary proceedings?and ?committee hearings?
as two different corpora.These two corpora have slightly different domains,although they share many common topics as well.We additionally choose a third corpus, whose do-main is more distinct from these two, from the Eu-roParl English-French corpus.
Islam and Mehler(2012) provided a customized version of Europarlwith translation direction labels, but this dataset onlycontains sentences that were authored in Englishand translated to French, and does not contain ex-amples for which the original language of author-ing was French.
We thus prepare a new datasetfrom EuroParl and will make it publicly availablefor use.
The original unprocessed version of Eu-roParl (Koehn, 2005) contains speaker language tags(original language of authoring) for the French andEnglish sides of the parallel corpus.
We filter out in-consistencies in the corpus.
First, we filter out sec-tions where the language tag is missing from one orboth sides.
We also filter out sections with conflict-ing language tags.
Parallel sections with differentnumber of sentences are also discarded to maintainsentence alignment.
This leaves us with three datasets (two Hansard and one EuroParl) with transla-tion direction information available, and which con-tain sentences authored in both languages.
We holdout 10% of each data set for testing and use the restfor training.
Our 3?3 corpus data matrix consists ofall nine combinations of training on one corpus andtesting on another (Table 2).4.1 PreprocessingFirst, we clean all data sets using the following sim-ple techniques.?
Sentences with low alphanumeric density arediscarded.?
A character n-gram based language detectiontool is used to identify the language of eachsentence.
We discard sentences with a detectedlanguage other than their label.?
We discard sentences with invalid unicodecharacters or control characters.?
Sentences longer than 2000 characters are ex-cluded.Next, an HMM word alignment model (Vogel etal., 1996) trained on the WMT English-French cor-pus (Bojar et al, 2013) word-aligns sentence pairs.105Figure 1: POS Tagged and Brown Cluster Aligned Sentence PairsWe discard sentence pairs where the word alignmentfails.
We use the Stanford POS tagger (Toutanovaand Manning, 2000) for English and French to tagall sentence pairs.
A copy of the alignment file withwords replaced with their POS tags is also gener-ated.
French and English Brown clusters are trainedseparately on the French and English sides of theWMT English-French corpus (Bojar et al, 2013).The produced models assign cluster IDs to words ineach sentence pair.
We create a copy of the align-ment file with cluster IDs instead of words as well.4.2 Feature ExtractionThe classifier of our choice (Section 5) extracts n-gram features with n specified as an option.
Inpreparation for classifier training and testing, featureextraction only needs to produce the unigram fea-tures while preserving the order (n-grams of higherlength are automatically extracted by the classifier).POS, word, and Brown cluster n-gram features aregenerated by using the respective representation forsequences of tokens in the sentences.
For POS andBrown cluster MTU features, the sequence of MTUsis defined as the left-to-right in source order se-quence (due to reordering, the exact enumeration or-der of MTUs matters).
For example, for the sen-tence pair in Figure 1, the sequence of Brown clus-ter MTUs is: 73?
(390,68), 208?24, 7689?3111,7321?1890, 2?16.5 ExperimentsWe chose the Vowpal Wabbit (Langford et al,2007) (VW) online linear classifier since it is fast,scalable and it has special (bag of words and n-gramgeneration) options for text classification.
We foundthat VW was comparable in accuracy to a batch lo-gistic regression classifier.
For training and test-ing the classifier, we created balanced datasets withthe same number of training examples in both di-rections.
This was achieved by randomly removingsentence pairs from the English to French directionuntil it matches the French to English direction.
Forexample, 636k sentence pairs are randomly chosenfrom the 2,930k sentence pairs in English to FrenchHansard-Committees corpus to match the number ofexamples in the French to English direction.5.1 Evaluation MethodWe are interested in comparing the performance ofvarious feature sets in translation direction detec-tion.
Performance evaluation of different classifi-cation features objectively is challenging in the ab-sence of a downstream task.
Specifically, dependingon the preferred balance between precision and re-call, different features can be superior.
Ideally anROC graph (Fawcett, 2006) visualizes the tradeoffbetween precision and recall and can serve as an ob-jective comparison between different classificationfeature sets.
However, it is not practical to presentROC graphs for 360 experiments.
Hence, we resortto the Area Under the ROC graph (AUC) measure asa good measure to provide an objective comparison.Theoretically, the area under the curve can be inter-preted as the probability that the classifier scores arandom negative example higher than a random pos-itive example (Fawcett, 2006).
As a point of refer-ence, we also provide F-scores for experimental set-tings that are comparable to the prior work reviewedin Section 2.5.2 ResultsFigure 2 presents AUC points for all experiments.Rows and columns are labeled with corpus namesfor training and test data sets respectively.
For ex-ample, the graph on the third row and first columncorresponds to training on the Hansard-Committeescorpus and testing on EuroParl.
Within each graphwe compare the AUC performance of different fea-1065055606570758085901 2 3 4 5HANSARD-COMMITTES5055606570758085901 2 3 4 51 2 3 4 5N-GRAM COUNTBC.MTU ENU.BC ENU.LEX ENU.POS FRA.BC FRA.LEX FRA.POS POS.MTU5055606570758085901 2 3 4 5HANSARD5055606570758085901 2 3 4 55055606570758085901 2 3 4 55055606570758085901 2 3 4 5EUROPARLEUROPARL5055606570758085901 2 3 4 5HANSARD5055606570758085901 2 3 4 5HANSARD-COMMITTEESAUCTEST CORPUSTRAININGCORPUSFigure 2: Comparing area under the ROC curve for the translation direction detection task when training and testingon different corpora using each of the eight feature sets.
See Table 1 for experiment label description.tures with n-gram lengths of 1 through 5.Graphs on the diagonal correspond to in-domaindetection and demonstrate higher performance com-pared to off diagonal graphs.
This confirms the ba-sic assumption that cross-domain translation direc-tion detection is a more difficult task.
The over-all performance is also higher when trained on theHansard corpus and tested on Hansard-Commiteeand vice versa.
This is because the Hansard cor-pus is more similar to the Hansard-Committees cor-pus compared to the EuroParl corpus.
It is also ob-servable that the variation in performance of dif-ferent features diminishes as the training and testcorpora become more dissimilar.
For instance, thisphenomenon can be observed on the second row ofgraphs where the features are most spread out whentested on the Hansard corpus.
They are less spreadout when tested on the Hansard-Committees corpus,and compressed together when tested on the Eu-roParl corpus.
The same phenomenon can be ob-served for classifiers trained on other corpora.For different feature types, different n-gram or-der of the features is best, depending on the featuregranularity.
To make it easier to observe patternsin the performance of different feature types, Figure3 shows the performance for each feature type andeach train-test corpus combination as a single point,by using the best n-gram order for that feature/datacombination.
Each of the 9 train/test data combina-tions is shown as a curve over feature types.We can see that MTU features (which look at bothlanguages at the same time) outperform individualsource or target features (POS or Brown cluster) forall datasets.
Brown clusters are unsupervised andcan provide different levels of granularity.
On theother hand, POS tags usually provide a fixed gran-ularity and require lexicons or labeled data to train.We see that Brown clusters outperform correspond-ing POS tags across data settings.
As an example,when training and testing on the Hansard corpusFRA.BC outperforms FRA.POS by close to 20 AUCpoints.Lexical features outperform monolingual POSand Brown cluster features in most settings althoughtheir advantages diminish as the training and testcorpus become more dissimilar.
This is somewhatcontrary to prior claims that lexical features will notgeneralize well across domains ?
we see that lexicalfeatures do capture important generalizations acrossdomains and models that use only POS tag featureshave lower performance, both in and out-of-domain.Figure 4 shows the rank of each feature amongst10750556065707580859095BC.MTU FRA.LEX ENU.LEX ENU.BC FRA.BC POS.MTU ENU.POS FRA.POSMAX AUCFEATURESEU-EU EU-H EU-HC HC-EU HC-H HC-HC H-EU H-H H- HCCross-DomainFigure 3: Translation detection performance matrix fortraining and testing on three different corpora - We ranexperiments for n-grams of up to length five for eachfeature (See Table 1 for feature label descriptions).
Un-like Figure 2 where we report AUC values for all n-gramlengths, in this graph we only present the highest AUCnumber for each feature.
Each marker type indicates atraining and test set combination.
The format of experi-ment labels in the legend is [TrainingSet]-[TestSet] andEU: EuroParl, H: Hansard, HC: Hansard Committees.For example, EU-HC means training on EuroParl corpusand testing on Hansard Committees corpus.all 8 different features for each entry in the cross-corpus data matrix (Similar to Figure 3 the highestperforming n-gram length has been chosen for eachfeature).
Brown cluster MTUs outperform all otherfeatures with rank one in all dataset combinations.Source and target POS tag features are the lowestperforming features in 8 out of 9 data set combina-tions.
The POS.MTU has its lowest ranks (7 and8) when it is trained on the EuroParl corpus and itshighest ranks (2 and 3) when trained on the Hansard-Committees corpus.
High number of features inPOS.MTU requires a large data set for training.
Thevariation in performance for POS.MTU can be ex-plained by the significant difference in training datasize between EuroParl and Hansard-Committees.Finally, while FRA.LEX and ENG.LEX are mostlyin rank 2 and 3 (after BC.MTU) they have their low-est ranks (6 and 4) in cross-corpus settings (HC-EUand HC-H).Finally, we report precision and recall numbersto enable comparison between our experiments andprevious work reported in Section 2.
When train-012345678H - H EU - EU HC - HC H - H C HC - H H - EU EU - HC EU - H H C - EUMAX AUC RANKTRAINING AND TEST CORPUS COMBINATIONSBC.MTU FRA.LEX ENU.LEX ENU.BC FRA.BC POS.MTU ENU.POS FRA.POSCross -DomainFigure 4: Translation direction detection AUC perfor-mance rank for each training and test set combination.For corpus combination abbreviations see description ofFigure 3.
For feature label descriptions see Table 1.ing and testing on the Hansard corpus, BC.MTUachieves 0.80 precision with 0.85 recall.
In compari-son, ENG.POS achieves 0.65 precision with 0.64 re-call and POS.MTU achieves 0.73 precision and 0.74recall.
These are the highest performance of eachfeature with n-grams of up to length 5.6 Conclusion and Future WorkFrom among eight studied sets of features, Browncluster MTUs were the most effective at identify-ing translation direction at the sentence level.
Theywere superior in both in-domain and cross-domainsettings.
Although English-Lexical features did notperform as well as Brown cluster MTUs, they per-formed better than most other methods.
In futurework, we plan to investigate lexical MTUs and toconsider feature sets containing any subset of theeight or more basic feature types we have consid-ered here.
With these experiments we hope to gainfurther insight into the performance of feature setsin in out out-of-domain settings and to improve thestate-of-the-art in realistic translation direction de-tection tasks.
Additionally, we plan to use this clas-sifier to extend the work of Twitto-Shmuel (2013) bybuilding a more accurate and larger parallel corpuslabeled for translation direction to further improveSMT quality.108ReferencesMona Baker.
1993.
Corpus linguistics and translationstudies: Implications and applications.
Text and tech-nology: in honour of John Sinclair, 233:250.Marco Baroni and Silvia Bernardini.
2006.
A newapproach to the study of translationese: Machine-learning the difference between original and trans-lated text.
Literary and Linguistic Computing,21(3):259?274.Austin Matthews Waleed Ammar Archna Bhatia, We-ston Feely, Greg Hanneman Eva Schlinger SwabhaSwayamdipta, Yulia Tsvetkov, and Alon Lavie ChrisDyer.
2014.
The cmu machine translation systems atwmt 2014.
ACL 2014, page 142.Ondrej Bojar, Christian Buck, Chris Callison-Burch,Christian Federmann, Barry Haddow, Philipp Koehn,Christof Monz, Matt Post, Radu Soricut, and LuciaSpecia.
2013.
Findings of the 2013 workshop onstatistical machine translation.
In Proceedings of theEighth Workshop on Statistical Machine Translation,pages 1?44.Peter F Brown, Peter V Desouza, Robert L Mercer, Vin-cent J Della Pietra, and Jenifer C Lai.
1992.
Class-based n-gram models of natural language.
Computa-tional linguistics, 18(4):467?479.Nadir Durrani, Philipp Koehn, Helmut Schmid, andAlexander Fraser.
2014.
Investigating the usefulnessof generalized word representations in smt.
In Pro-ceedings of the 25th Annual Conference on Computa-tional Linguistics (COLING), Dublin, Ireland, pages421?432.Sauleh Eetemadi and Kristina Toutanova.
2014.
Asym-metric features of human generated translation.
InProceedings of the 2014 Conference on EmpiricalMethods in Natural Language Processing (EMNLP),pages 159?164.
Association for Computational Lin-guistics.Tom Fawcett.
2006.
An introduction to roc analysis.Pattern recognition letters, 27(8):861?874.Zahurul Islam and Alexander Mehler.
2012.
Customiza-tion of the europarl corpus for translation studies.
InLREC, page 2505?2510.Philipp Koehn.
2005.
Europarl: A Parallel Corpusfor Statistical Machine Translation.
In ConferenceProceedings: the tenth Machine Translation Summit,pages 79?86, Phuket, Thailand.
AAMT, AAMT.Moshe Koppel and Noam Ordan.
2011.
Translationeseand its dialects.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Linguis-tics: Human Language Technologies-Volume 1, page1318?1326.
Association for Computational Linguis-tics.David Kurokawa, Cyril Goutte, and Pierre Isabelle.2009.
Automatic detection of translated text andits impact on machine translation.
Proceedings.
MTSummit XII, The twelfth Machine Translation Sum-mit International Association for Machine Translationhosted by the Association for Machine Translation inthe Americas.J Langford, L Li, and A Strehl, 2007.
Vowpal wabbitonline learning project.Chris Quirk and Arul Menezes.
2006.
Do we needphrases?
: Challenging the conventional wisdom in sta-tistical machine translation.
In Proceedings of theMain Conference on Human Language TechnologyConference of the North American Chapter of the As-sociation of Computational Linguistics, HLT-NAACL?06, pages 9?16, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Kristina Toutanova and Christopher D. Manning.
2000.Enriching the knowledge sources used in a maximumentropy part-of-speech tagger.
In Proceedings of the2000 Joint SIGDAT Conference on Empirical Methodsin Natural Language Processing and Very Large Cor-pora: Held in Conjunction with the 38th Annual Meet-ing of the Association for Computational Linguistics- Volume 13, EMNLP ?00, pages 63?70, Stroudsburg,PA, USA.
Association for Computational Linguistics.Naama Twitto-Shmuel.
2013.
Improving Statistical Ma-chine Translation by Automatic Identification of Trans-lationese.
Ph.D. thesis, University of Haifa.Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
Hmm-based word alignment in statistical trans-lation.
In Proceedings of the 16th conference on Com-putational linguistics-Volume 2, pages 836?841.
Asso-ciation for Computational Linguistics.Vered Volansky, Noam Ordan, and Shuly Wintner.
2013.On the features of translationese.
Literary and Lin-guistic Computing, page 31.109
