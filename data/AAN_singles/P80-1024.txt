Pars ingw.
A. MartinLaboratory for Computcr ScienceMassachusetts Institute of TechnologyCambridge, Massachu.~tts 02139\[.ooking at the Proceedings of last year's Annual Meeting, one sccs that thesession most closely parallcling this one was entitled Language Structure andPar~ing.
\[n avcry nice prescnu~fion, Martin Kay was able to unite the papers ofthat scssion uudcr a single theme.
As hc stated it.
"Illcre has been a shift of emphasis away from highly~tmctured systems of complex rules as the principalrepository of infi~mmtion about the syntax of alanguage towards a view in which the responsibilityis distributed among the Icxicoo.
semantic parts ofthe linguistic description, aod a cognitive or strategiccomponent.
Concomitantly, interest has shiRedfrom al!lorithms for syntactic analysis andgeneration, in which the central stnlctorc and theexact seqtlencc of events are paramount, to systemsiu which a heavier burden is carried by the datastl ucture and in wilich the order of,:vents is a m,.~terof strategy.\['his ycar.
the papers of the session represent a greater diversity of rescan:hdirections.
The paper by Hayes.
and thc paper by Wilcnsky and Aren~ arc bothexamples of what Kay had in mind.
but the paper I)y Church, with rcgard to thequestion of algorithms, is quite the opposite.
He {tolds that once the full rangeuf constraints dcscribing pc~plc's processing behavior has been captul'ed, thebest parsing strategies will be rather straightforwarcL and easily cxplaincd asalgorithms.Perilaps the seven papers in this year's ession can best be introduecd by brieflyciting ~mc of the achievcmcqts and problems reported in the works theyrefcrence,In thc late i960"s Woods tweeds70\] capped an cfTort by several people todcvch)p NI'N parsing.
'lllis well known technique applies a smdghtforward topdown, left CO right` dcpth fic~t pat.~ing algorithm to a syntactic grammar.I-:~pccialiy in the compiled fi)rm produced by Ilorton \[Bnrton76~,\].
the parserwas able to produce the first parse in good time.
but without ~manticconstraints, numcroos yn~ictic analyses could be and ~,mctimcs were fou.nd,especially in scntenccs with conjunctions.
A strength of the system was theATN grammar, which can be dc~ribcd as a sct of context frec production ruleswhose right hand sides arc finite statc machincs and who.~ U'ansition arcs havebccn augmented with functions able to read and set registers, and also able toblock a transition on their an:.
Many people have found this a convenientfonnulism in which m develop grammars of Engtish.The Woods ATN parser was a great success and attempts were made to exploitit (a) as a modc\[ of human processing and (b) as a tool for writing grammars.
Atthe same time it was recognized to havc limimdoos.
It wasn't olerant of errors,and it couldn't handle unknown words or constructions (there were n~'tnysyntactic constmcdons which it didn't know).
In addidon, the questionanswering system fed by the parser had a weak notion of word and phrase.~emantics and it was not always able to handle quantificrs properly.
It is notctcar thcs?
components could have supported a stronger interaction withsyntactic parsing, had Woods chosen to a~cmpt i .On the success ide.
Kaplan \[Kaplan72\] was inspired to claim that the ATNparser provided a good model tbr some aspects of human processing.
Someaspects which might bc modeled are:Linnuistic PhenomenonPrefcrred readings ofAmbiguous SentencesGarden ~th SentencesPerceived ComplexityDifferencesCenter Embedding PoundsA'rN Comnntadonal MechanismOrdcred Trying ofAlternative ArcsBack-trackingHold List CostingCounting Total TransitionsNone\[n one study, most pcople got the a) reading of 1).
One can try to explain desl) Thcy told the girl that Bill liked the story.la) They told the girl \[that \[Bill iked the scoryJs \].lb) Th~ told \[the girl that Bill likedlN P thestory.by ordering the arcs leaving the state where the head noun of'an NP has been~'ccpccd: a Ix)p am (tcrminuting the NP) is tried before an an: accepting amodifying relative clause.
\]-h)wcver, Ricil \[Rich75\] puims out that dfis an:ordering solution would seem to have diltlculdcs with 2).
This sentence is oftennut peracived2) They told the girl that Bill liked that hewould be at the loath;all game.as requiring backup, yet if the arcs an: ordered as for I), it does require backup.There is no doubt that whatever is going on.
the awareness of backup in 3) is somuch stronger than in 2) that it seems like a different phenomcnoo.
To resolvethis,3) The horse raced past the b,'u'n fell.one could claim that perceived backup is some fimction of' the length of theactual b~kup, or maybe of the degree of commiunent to the original path(althoogh it isnt clear what this would mean in ATN terms).In this session.
Ferrari and Stock will turn the are ordering game around anddescribe, for actual tex~ the probability that a given arc is the correct exit an:from a node.
given the an: by wiuch the parser arrived at the node.
\[t will beintcr~ting to look at their distributions.
\[n the speech project at IBM War, souLaboratories \[Baker75\] it was discovered some time ago that, for a given text, thesyntactic class era word could be predicted correctly over 90% of the umo givenonly the syntactic lass of the preceding word` Interestingly, the correctness of'predictions fell off less than 10% whcn only the current word w~ used.
Onewonders if this same level of skewncss holds across texts, or (what we will hear)for the continuation of phrases.
These results should be helpful in discussingthe whole issue of arc orderiog"Implicit in any al~ ordering strategy is the assumption that not all parses of asentence will be fi)und.
Having the "best" path, the parscr will stop wben it getsan acceptable analysis.
Arc ordering helps find that "best' path.
Marcus\[Man:us7g\], agreed with the idea of following only a best path, but he claimedthat the reason there is no pe~eived backup in 2) is that the human parser isable to look ahead a few constituents iostead of just one s~ate and oneeoilstitucnt in making a u'ansition.
He claims this makes a more accurate modelof human garden path behavior, but it doesn't address the issue of unlimitedstuck depth.
Here, Church will describe a parser similar in design co Marcus',except that it conserves memory.
This allows Church to address psychologicalfacLS not addrc~qed by either Marcus or the ATN models.
Church claims thatexploiting stack size constraints will incn:ase the cimnces of building a good bestpath parser.91Besides psychological modeling, thcre is also an interest m using thc ATNft)nnalism for writing and teaching rammars.
Paramount here is e:;planation,both of the grammar and its appiicatinn to a particu!ar sentence.
The papcr byKchler and Woods reports on this.
Weischcdcl picks a particular problem,responding to an input which the ATN can't handle.
He a~,'xiatcs a list ofdiagnostic couditions and actions with each state.
When no pur.xc is found, theparser finds tile last store on the path which progressed the thnhcst d)rongh theinput string and executes its diagnostic conditions and actions.
When a parseruses ,rely syutactic onstraints" one cxpects it to find a lut of parses.
UsuuJly thenumber of parses grows marc than tincarly with sentence length.
Thus, for a~tirly COmlflete grammar and moderate to king sentences, one would expectthat the cast of no parses (handled by Wei.%hedcl) would be rare in comparisonwith the oilier two cases (not handled) where file set of parses doesn't includethe correct one, or where the grammar has been mistakenly, written to allowundesired pa!~s" Success of the above eflol'ts to folinw only the best pathwould clearly be relevant here.
No doubt Wcischcdel's proeedure can help finda lot of bugs if die t~t examples are chosen with a little care.
Ihtt there is sdllinteresting work to be done on grammar and parser explanation, andWeisehcdcl is onc of those who intends to explore itThe remaining three papers tem from three separate traditions which reject thestrict syntactic ATN formalism, each for its own reasons.
They are:i) Semantic Grammars -- the Davidson andKaplan paperii) Scmantic Structure Driven Parsing -Wilcnsky and Arens paperiii) Multiple knowledge Source Parsing -- HayespaperEach of these systems claims some advantage over the more widely known andaccepted ATN.The somandc grammar parser can be viewed as a variation of the ATN whichattempts to cope with the ATN's lack of semantics.
Kapian's work builds onwork stancd by Burton \[Burton76b\] and picked up by Hcndrix et al\[ltendrtx78J.
The semantic grammar parser uses semanuc in.
;tcad of syntacticarc categories.
"l'his collapses yntax and semantics into a single structure.When an ATN parsing strategy is used the result is actuall7 ~ flexible than asyntactic ATN, but it is faster because syntactic possibilities are elin'*in;tted bythe semantics of the domain.
"Ilm strategy is justified m terms of  thepcrfum'*ancc of actual running systems.
Kaplan also calls on a speed criteria insuggest,og (hat when an unkuown word is cncountcred the system assomc allpossibilities which will let parsing prncccd.
Theo if more than one possibilityleads to a successful parse, the system should attempt to rt,~olve the word fi.trthcrby file search or user query.As Kaplan points nut.
d)is trick is not limited to semantic grammars, but only tosystems having enough constraints.
It would hc interesting to know hOW w(:.
itwoutd work for systems using Oshcrson's \[Oshcrson78\] prcdicahility criterion.instead of troth for their scmanocs.
Oshcrson distinguishes between "greenidea", which he says is silly and "marricd bachelor" which he say~ is just raise.Hc ilotes that "idea is oat green" is no better, but "bac\[~ehlr is not married" isfine.
Prcdicability is a looser constrain* than Kaplan uses, aud if it would still becuough to limit database search this wo.
"l bc intcrcv;ng, because prcdicabilityis easier to implement across abroad domain.Wilen~ky is a former stu,:tent of Schank's and thus COlt'*us ffom a tradition whichemphastzes sentatmcs over syutax.
He ~s right in emphasizing Ore importanceof phrase scmantics.
The grammarians Quirk aud Grcenhaum \[Quirk731 poiutout tile syntactic ,ll'*d semantic importaucc of verb phrases over verbs.- inhngutstms, lhesnan \[Ih'csnang0l is developing a theory of  Icxical phrases which92accounts" by lcxical relatkms between constituents (if a phrase, for many of thephenomena explained by the old transfomtational grammar.
}:or example.given4) There were reported to have been lionssighted.a typical ATN parser would attempt by register manipulations to make "lions"the suhject.
Using a phrase approach, "there be lions sighted" can be taken asmeaning "exist lions sighted."
wl)erc "lions" is an object and "sighted" an objectcomplement "There" is related to the "be" m "been" by a series ofrelationships between the argumentS of semantic structures.
Wilensky appearsto have suppressed syntax into his semantic component, and so it will beinrct~ting to sec how he handles the traditional syntactic phenomcna of 4), likepassive and verb forms.Finalb, the paper by Hayes shows the influence of the speech recognitionprojects where bad input gave the Woods A'rN great dimcnlty.
Text input ismuch better than speech input.
However, examination of actual input\[Malhotra75\] does show sentences like:5) What would have profits have been?Fortunately, these cases are rare.
Much more likely is clipsis and the omissionof syntax when the semantics are clear.
For example, the missing commas in6) Give ratios of manufacturing costs to salesfor plants 1 2 3 and 4 for 72 and 73.Examples like these show that errors and omissions are not random phenomenaand that there can be something to the study of errors and how to deal withdiem.In summary, it can be seen ~at while much progress has been made inconsmtcting u~bic parsers, the basic i~ues, such as the division of syntax.semantics" and pragmatics both in representation and in urdcr uf processing, arestill up for grabs.
'l'be problem has plenty of structure, so there is good fun tobe had.References\[Ikukcr751\[llresnang0\]\[Burton76aj\[Burmn76bl\[Hcndrix73\]Baker.
J.K. "Stochastic Modeling forAutomatic Speech Understanding," SneechRceoeuition."
lnvi\[~ Pap~r~ ~ ~ IEEESvmnosiurTL Reddy, D.R.
(E'kt.
), \]975.Bresnan.
Joan.
"Polyadicity: Part I of aTheory of l.exical Rules andRcpreseflmtions," MI'\[" Department ofLinguistics (January 1980).Burton.
Richard R. and Woods, William A.
"A Compiling System fnr AugmentedTransition Networks," COLING 76.Burton.
Richard R. "Semantic Grammar: AnEngineering Technique \[or ConstructingNatural I~mguage Undcr~tanding Systems,"BBN Report 3453, Bolt.
Beranek, andNewman, Boston, Ma.
(December D76).Hendrix, Gary G.  Sacerdoa, E.D.,Sagalowicz.
D.. and Slocum.
J.
"l)cveloping aNatural I.anguage Interface to Complexl')ata," ACM l"rans, ~ Dqf.ahase Systems.
vo\[.3, no.
2 (June 1978).
pp.
105-147.\[Kaplan72\]\[Malhotra751\[Marcus7Sl\[O~erson7Sl\[Quirk731IRich751\[Woods70lKaplan, Ronald M. "Augmented TransitionNetworks as Psychological Models ofSentence Comprehension," ArtificialIntcllieenee, 3 (October 1972).
pp.
77-100.Malhotra.
Ashok.
"l)esign Critcria for aKnowlcdgc-Based English Language Systemfor Management: An ExperimentalAnalysis," MIT/LCS/rR- 1.46, MIT,Laboratory for Computer Science,Cambridge.
Ma.
(February 1975).Marcus` Mitchell.
"A Theory of SyntacticRecognition for Natural l.'mguages," Ph.D.thesis.
MIT Dept.
of Electrical Engineeringand Computer Science, Cambridge, Ma.
(tobe published by MrT Press).Oshcrson, Danicl N. "Three Conditions onConceptual Naturalness."
Cognition, 6 (197g),pp.
263-289.Quirk.
R. and Greenbaum.
S. A ConciseGrammar o~Ctmiemnorarv F.nnlisll, HarcourtBrace Jovanovich.
New York (L973).Rich, Charles.
"On the Psychological Realityof Augmented Transition Network Models ofSentence Cumprehension," unpublishedpaper, MIT Artilicial Intelligence I.aboratory,Cambridge, Ma.
(July \[97S).Woods.
William A.
"Transition NetworkGrammars for Natural Language Analysis"CACM 13.
10 (October 1970), pp.
591-602.93
