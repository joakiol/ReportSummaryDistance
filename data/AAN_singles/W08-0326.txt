Proceedings of the Third Workshop on Statistical Machine Translation, pages 171?174,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsMATREX: the DCU MT System for WMT 2008John Tinsley, Yanjun Ma, Sylwia Ozdowska, Andy WayNational Centre for Language TechnologyDublin City UniversityDublin 9, Ireland{jtinsley, yma, sozdowska, away}@computing.dcu.ieAbstractIn this paper, we give a description of the ma-chine translation system developed at DCUthat was used for our participation in the eval-uation campaign of the Third Workshop onStatistical Machine Translation at ACL 2008.We describe the modular design of our data-driven MT system with particular focus onthe components used in this participation.
Wealso describe some of the significant moduleswhich were unused in this task.We participated in the EuroParl task for thefollowing translation directions: Spanish?English and French?English, in which we em-ployed our hybrid EBMT-SMT architecture totranslate.
We also participated in the Czech?English News and News Commentary taskswhich represented a previously untested lan-guage pair for our system.
We report resultson the provided development and test sets.1 IntroductionIn this paper, we present the Data-Driven MT sys-tems developed at DCU, MATREX (Machine Trans-lation using Examples).
This system is a hybrid sys-tem which exploits EBMT and SMT techniques tobuild a combined translation model.We participated in both the French?English andSpanish?English EuroParl tasks.
In these two tasks,we monolingually chunk both source and targetsides of the dataset using a marker-based chunker(Gough and Way, 2004).
We then align these chunksusing a dynamic programming, edit-distance-stylealgorithm and combine them with phrase-basedSMT-style chunks into a single translation model.We also participated in the Czech?English NewsCommentary and News tasks.
This language pairrepresents a new challenge for our system and pro-vides a good test of its flexibility.The remainder of this paper is organised as fol-lows: Section 2 details the various components ofour system, in particular the chunking and chunkalignment strategies used for the shared task.
In Sec-tion 3, we outline the complete system setup for theshared task, and in Section 4 we give some resultsand discussion thereof.2 The MATREX SystemThe MATREX system is a modular hybrid data-driven MT system, built following established De-sign Patterns, which exploits aspects of both theEBMT and SMT paradigms.
It consists of a num-ber of extendible and re-implementable modules, themost significant of which are:?
Word Alignment Module: outputs a set of wordalignments given a parallel corpus,?
Chunking Module: outputs a set of chunksgiven an input corpus,?
Chunk Alignment Module: outputs alignedchunk pairs given source and target chunks ex-tracted from comparable corpora,?
Decoder: returns optimal translation given aset of aligned sentence, chunk/phrase and wordpairs.In some cases, these modules may comprisewrappers around pre-existing software.
For exam-ple, our system configuration for the shared taskincorporates a wrapper around GIZA++ (Och andNey, 2003) for word alignment and a wrapperaround Moses (Koehn et al, 2007) for decoding.
It171should be noted, however, that the complete systemis not limited to using only these specific modulechoices.
The following subsections describe thosemodules unique to our system.2.1 Marker-Based ChunkingThe chunking module used for the shared task isbased on the Marker Hypothesis, a psycholinguisticconstraint which posits that all languages are markedfor surface syntax by a specific closed set of lex-emes or morphemes which signify context.
Using aset of closed-class (or ?marker?)
words for a particu-lar language, such as determiners, prepositions, con-junctions and pronouns, sentences are segmentedinto chunks.
A chunk is created at each new occur-rence of a marker word with the restriction that eachchunk must contain at least one content (or non-marker) word.
An example of this chunking strategyfor English and Spanish is given in Figure 1.2.2 Chunk AlignmentIn order to align the chunks obtained by the chunk-ing procedures described in Section 2.1, we makeuse of an ?edit-distance-style?
dynamic program-ming alignment algorithm.In the following, a denotes an alignment betweena target sequence e consisting of I chunks and asource sequence f consisting of J chunks.
Giventhese sequences of chunks, we are looking for themost likely alignment a?:a?
= argmaxaP(a|e, f) = argmaxaP(a, e|f).We first consider alignments such as those ob-tained by an edit-distance algorithm, i.e.a = (t1, s1)(t2, s2) .
.
.
(tn, sn),with ?k ?
J1, nK, tk ?
J0, IK and sk ?
J0, JK, and?k < k?
:tk ?
tk?
or tk?
= 0,sk ?
sk?
or sk?
= 0,where tk = 0 (resp.
sk = 0) denotes a non-alignedtarget (resp.
source) chunk.We then assume the following model:P(a, e|f) = ?kP(tk, sk, e|f) = ?kP(etk |fsk),where P(e0|fj) (resp.
P(ei|f0)) denotes an ?inser-tion?
(resp.
?deletion?)
probability.Assuming that the parameters P(etk |fsk) areknown, the most likely alignment is computed bya simple dynamic-programming algorithm.1Instead of using an Expectation-Maximization al-gorithm to estimate these parameters, as commonlydone when performing word alignment (Brownet al, 1993; Och and Ney, 2003), we directly com-pute these parameters by relying on the informationcontained within the chunks.
The conditional prob-ability P(etk |fsk) can be computed in several ways.In our experiments, we have considered three mainsources of knowledge: (i) word-to-word translationprobabilities, (ii) word-to-word cognates, and (iii)chunk labels.
These sources of knowledge are com-bined in a log-linear framework.
The weights ofthe log-linear model are not optimised; we experi-mented with different sets of parameters and did notfind any significant difference as long as the weightsstay in the interval [0.5 ?
1.5].
Outside this inter-val, the quality of the model decreases.
More detailsabout the combination of knowledge sources can befound in (Stroppa and Way, 2006).2.3 Unused ModulesThere are numerous other features available in oursystem which, due to time constraints, were not ex-ploited for the purposes of the shared task.
Theyinclude:?
Word packing (Ma et al, 2007): a bilinguallymotivated packing of words that changes thebasic unit of the alignment process in order tosimplify word alignment.?
Supertagging (Hassan et al, 2007b): incorpo-rating lexical syntactic descriptions, in the formof supertags, to the language model and targetside of the translation model in order to betterinform decoding.?
Source-context features (Stroppa et al, 2007):use memory-based classification to incorporatecontext-informed features on the source side ofthe translation model.?
Treebank-based phrase extraction (Tinsleyet al, 2007): extract word and phrase align-ments based on linguistically informed sub-sentential alignment of the parallel data.1This algorithm is actually a classical edit-distance al-gorithm in which distances are replaced by opposite-log-conditional probabilities.172English: [I voted] [in favour] [of the strategy presented] [by the council] [concerning relations] [withMediterranean countries]Spanish: [He votado] [a favor] [de la estrategia presentada] [por el consejo] [relativa las relaciones][con los pa?
?ses mediterrane?os]Figure 1: English and Spanish Marker-Based chunkingFilter criteria es?en fr?en cz?enInitial Total 1258778 1288074 1096941Blank Lines 5632 4200 2Length 6794 8361 2922Fertility 120 82 1672Final Total 1246234 1275432 1092345Table 1: Summary of pre-processing on training data.3 Shared Task SetupThe following section describes the system setupusing the Spanish?English and French?English Eu-roParl, and Czech?English CzEng training data.3.1 Pre-processingFor all tasks we initially tokenised the data (Czechdata was already tokenised) and removed blanklines.
We then filtered out sentence pairs based onlength (>100 words) and fertility (9:1 word ratio).Finally we lowercased the data.
Details of this pre-processing are given in Table 1.3.2 System ConfigurationAs mentioned in Section 2, our word alignmentmodule employs a wrapper around GIZA++.We built a 5-gram language model based the tar-get side of the training data.
This was done usingthe SRI Language Modelling toolkit (Stolcke, 2002)employing linear interpolation and modified Kneser-Ney discounting (Chen and Goodman, 1996).Our phrase-table comprised a combination ofmarker-based chunk pairs2, extracted as describedin Sections 2.1 and 2.2, and word-alignment-basedphrase pairs extracted using the ?grow-diag-final?method of Koehn et al (2003), with a maximumphrase length of 7 words.
Phrase translation proba-bilities were estimated by relative frequency over allphrase pairs and were combined with other features,2This module was omitted from the Czech?English systemas we have yet to verify whether marker-based chunking is ap-propriate for Czech.System BLEU (-EBMT) BLEU (+EBMT)es?en 0.3283 0.3287fr?en 0.2768 0.2770cz?en 0.2235 -Table 2: Summary of results on developments sets de-vtest2006 for EuroParl tasks and nc-test2007 for cz?entasks.System BLEU (-EBMT) BLEU (+EBMT)es?en 0.3274 0.3285fr?en 0.3163 0.3174cz?en (news) 0.1458 -cz?en (nc) 0.2217 -Table 3: Summary of results on 2008 test data.such as a reordering model, in a log-linear combina-tion of functions.We tuned our system on the development set de-vtest2006 for the EuroParl tasks and on nc-test2007for Czech?English, using minimum error-rate train-ing (Och, 2003) to optimise BLEU score.Finally, we carried out decoding using a wrapperaround the Moses decoder.3.3 Post-processingCase restoration was carried out by training the sys-tem outlined above - without the EBMT chunk ex-traction - to translate from the lowercased versionof the applicable target language training data to thetruecased version.
We have previously shown thisapproach to be very effective for both case and punc-tuation restoration (Hassan et al, 2007a).
The trans-lations were then detokenised.4 ResultsThe system output is evaluated with respect toBLEU score.
Results on the development sets andtest sets for each task are given in Tables 2 and 3respectively, where ?-EBMT?
indicates that EBMTchunk modules were not used, and ?+EBMT?
indi-cates that they were used.1734.1 DiscussionThose configurations which incorporated the EBMTchunks improved slightly over those which did not.Groves (2007) has shown previously that combin-ing EBMT and SMT translation models can lead toconsiderable improvement over the baseline systemsfrom which they are derived.
The results achievedhere lead us to believe that on such a large scalethere may be a more effective way to incorporate theEBMT chunks.Previous work has shown the EBMT chunks tohave higher precision than their SMT counterparts,but they lack sufficient recall when used in isola-tion (Groves, 2007).
We believe that increasing theirinfluence in the translation model may lead to im-proved translation accuracy.
One experiment to thiseffect would be to add the EBMT chunks as a sep-arate phrase table in the log-linear model and allowthe decoder to chose when to use them.Finally, we intend to exploit the unused modulesof the system in future experiments to investigatetheir effects on the tasks presented here.AcknowledgmentsThis work is supported by Science Foundation Ireland(grant nos.
05/RF/CMS064 and OS/IN/1732).
Thanksalso to the reviewers for their insightful comments andsuggestions.ReferencesBrown, P. F., Pietra, S. A. D., Pietra, V. J. D., and Mercer,R.
L. (1993).
The mathematics of statistical machinetranslation: Parameter estimation.
Computational Lin-guistics, 19(2):263?311.Chen, S. F. and Goodman, J.
(1996).
An Empirical Studyof Smoothing Techniques for Language Modeling.
InProceedings of the Thirty-Fourth Annual Meeting ofthe Association for Computational Linguistics, pages310?318, San Francisco, CA.Gough, N. and Way, A.
(2004).
Robust Large-ScaleEBMT with Marker-Based Segmentation.
In Proceed-ings of the 10th International Conference on Theoreti-cal and Methodological Issues in Machine Translation(TMI-04), pages 95?104, Baltimore, MD.Groves, D. (2007).
Hybrid Data-Driven Models of Ma-chine Translation.
PhD thesis, Dublin City University,Dublin, Ireland.Hassan, H., Ma, Y., and Way, A.
(2007a).
MATREX: theDCU Machine Translation System for IWSLT 2007.
InProceedings of the International Workshop on SpokenLanguage Translation, pages 69?75, Trento, Italy.Hassan, H., Sima?an, K., and Way, A.
(2007b).
Su-pertagged Phrase-based Statistical Machine Transla-tion.
In Proceedings of the 45th Annual Meeting of theAssociation for Computational Linguistics (ACL?07),pages 288?295, Prague, Czech Republic.Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Fed-erico, M., Bertoldi, N., Cowan, B., Shen, W., Moran,C., Zens, R., Dyer, C., Bojar, O., Constantin, A., andHerbst, E. (2007).
Moses: Open Source Toolkit forStatistical Machine Translation.
In Annual Meeting ofthe Association for Computational Linguistics (ACL),demonstration session, pages 177?180, Prague, CzechRepublic.Koehn, P., Och, F. J., and Marcu, D. (2003).
Statisti-cal Phrase-Based Translation.
In Proceedings of the2003 Conference of the North American Chapter of theAssociation for Computational Linguistics on HumanLanguage Technology (NAACL ?03), pages 48?54, Ed-monton, Canada.Ma, Y., Stroppa, N., and Way, A.
(2007).
Boostrap-ping Word Alignment via Word Packing.
In Proceed-ings of the 45th Annual Meeting of the Association forComputational Linguistics (ACL?07), pages 304?311,Prague, Czech Republic.Och, F. (2003).
Minimum error rate training in statisticalmachine translation.
In Proceedings of the 41st AnnualMeeting of the Association for Computational Linguis-tics (ACL), pages 160?167, Sapporo, Japan., Sapporo,Japan.Och, F. J. and Ney, H. (2003).
A Systematic Comparisonof Various Statistical Alignment Models.
Computa-tional Linguistics, 29(1):19?51.Stolcke, A.
(2002).
SRILM - An Extensible LanguageModeling Toolkit.
In Proceedings of the Interna-tional Conference Spoken Language Processing, Den-ver, CO.Stroppa, N., van den Bosch, A., and Way, A.
(2007).Exploiting Source Similarity for SMT using Context-Informed Features.
In Proceedings of the 11th Interna-tional Conference on Theoretical and MethodologicalIssues in Machine Translation (TMI-07), pages 231?240, Sko?vde, Sweden.Stroppa, N. and Way, A.
(2006).
MaTrEx: the DCU ma-chine translation system for IWSLT 2006.
In Proceed-ings of the International Workshop on Spoken Lan-guage Translation, pages 31?36, Kyoto, Japan.Tinsley, J., Hearne, M., and Way, A.
(2007).
ExploitingParallel Treebanks to Improve Phrase-Based Statisti-cal Machine Translation.
In Proceedings of the SixthInternational Workshop on Treebanks and LinguisticTheories (TLT-07), pages 175?187, Bergen, Norway.174
