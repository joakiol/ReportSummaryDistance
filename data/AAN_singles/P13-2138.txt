Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 790?796,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsGeneralizing Image Captions for Image-Text Parallel CorpusPolina Kuznetsova, Vicente Ordonez, Alexander Berg,Tamara Berg and Yejin ChoiDepartment of Computer ScienceStony Brook UniversityStony Brook, NY 11794-4400{pkuznetsova,vordonezroma,aberg,tlberg,ychoi}@cs.stonybrook.eduAbstractThe ever growing amount of web imagesand their associated texts offers new op-portunities for integrative models bridgingnatural language processing and computervision.
However, the potential benefits ofsuch data are yet to be fully realized dueto the complexity and noise in the align-ment between image content and text.
Weaddress this challenge with contributionsin two folds: first, we introduce the newtask of image caption generalization, for-mulated as visually-guided sentence com-pression, and present an efficient algo-rithm based on dynamic beam search withdependency-based constraints.
Second,we release a new large-scale corpus with1 million image-caption pairs achievingtighter content alignment between imagesand text.
Evaluation results show the in-trinsic quality of the generalized captionsand the extrinsic utility of the new image-text parallel corpus with respect to a con-crete application of image caption transfer.1 IntroductionThe vast number of online images with accom-panying text raises hope for drawing synergisticconnections between human language technolo-gies and computer vision.
However, subtleties andcomplexity in the relationship between image con-tent and text make exploiting paired visual-textualdata an open and interesting problem.Some recent work has approached the prob-lem of composing natural language descriptionsfor images by using computer vision to retrieveimages with similar content and then transferring?A house beingpulled by a boat.?
?I saw her in the lightof her reading lampand sneaked back toher door with thecamera.?
?Sections of thebridge sitting in theDyer Constructionyard south ofCabelas Driver.
?Circumstantialinformation that is notvisually presentVisually relevant,but with overlyextraneous detailsVisually truthful,but for an uncommonsituationFigure 1: Examples of captions that are not readilyapplicable to other visually similar images.text from the retrieved samples to the query im-age (e.g.
Farhadi et al(2010), Ordonez et al(2011), Kuznetsova et al(2012)).
Other work(e.g.
Feng and Lapata (2010a), Feng and Lapata(2010b)) uses computer vision to bias summariza-tion of text associated with images to produce de-scriptions.
All of these approaches rely on ex-isting text that describes visual content, but manytimes existing image descriptions contain signifi-cant amounts of extraneous, non-visual, or other-wise non-desirable content.
The goal of this paperis to develop techniques to automatically clean upvisually descriptive text to make it more directlyusable for applications exploiting the connectionbetween images and language.As a concrete example, consider the first imagein Figure 1.
This caption was written by the photoowner and therefore contains information relatedto the context of when and where the photo wastaken.
Objects such as ?lamp?, ?door?, ?camera?are not visually present in the photo.
The secondimage shows a similar but somewhat different is-sue.
Its caption describes visible objects such as?bridge?
and ?yard?, but ?Cabelas Driver?
areoverly specific and not visually detectable.
The790Dependency Constraints with Examples Additional Dependency ConstraintsConstraints Sentence Dependencyadvcl*(?)
Taken when it was running... taken?running acomp*(?
), advmod(?
), agent*(?
), attr(?)amod(?)
A wooden chair in the living room chair?
wooden auxpass(?
), cc*(?),complm(?
), cop*(?)aux(?)
This crazy dog was jumping... jumping?was csubj*/csubjpass*(?),expl(?
), mark*(?)ccomp*(?)
I believe a bear was in the box... believe?was infmod*(?
), mwe(?
), nsubj*/nsubjpass*(?)prep(?)
A view from the balcony view?from npadvmod(?
), nn(?
), conj*(?
), num*(?)det(?)
A cozy street cafe... cafe?A number(?
), parataxis(?),?dobj*(?)
A curious cow surveys the road... surveys?road partmod*(?
), pcomp*(?
), purpcl*(?)iobj*(?)
...rock gives the water the color gives?water possessive(?
), preconj*(?
), predet*(?)neg(?)
Not a cloud in the sky... cloud?Not prt(?
), quantmod(?
), rcmod(?
), ref(?)pobj*(?)
This branch was on the ground... on?ground rel*(?
), tmod*(?
), xcomp*(?
), xsubj(?
)Table 1: Dependency-based Constraintstext of the third image, ?A house being pulled by aboat?, pertains directly to the visual content of theimage, but is unlikely to be useful for tasks such ascaption transfer because the depiction is unusual.1This phenomenon of information gap between thevisual content of the images and their correspond-ing narratives has been studied closely by Dodgeet al(2012).The content misalignment between images andtext limits the extent to which visual detectorscan learn meaningful mappings between imagesand text.
To tackle this challenge, we introducethe new task of image caption generalization thatrewrites captions to be more visually relevant andmore readily applicable to other visually similarimages.
Our end goal is to convert noisy image-text pairs in the wild (Ordonez et al 2011) intopairs with tighter content alignment, resulting innew simplified captions over 1 million images.Evaluation results show both the intrinsic qualityof the generalized captions and the extrinsic util-ity of the new image-text parallel corpus.
The newparallel corpus will be made publicly available.22 Sentence Generalization as ConstraintOptimizationCasting the generalization task as visually-guidedsentence compression with lightweight revisions,we formulate a constraint optimization problemthat aims to maximize content selection and lo-cal linguistic fluency while satisfying constraintsdriven from dependency parse trees.
Dependency-based constraints guide the generalized caption1Open domain computer vision remains to be an openproblem, and it would be difficult to reliably distinguish pic-tures of subtle visual differences, e.g., pictures of ?a waterfront house with a docked boat?
from those of ?a floatinghouse pulled by a boat?.2Available at http://www.cs.stonybrook.edu/?ychoi/imgcaption/to be grammatically valid (e.g., keeping articlesin place, preventing dangling modifiers) while re-maining semantically compatible with respect to agiven image-text pair (e.g., preserving predicate-argument relations).
More formally, we maximizethe following objective function:F (y;x) = ?
(y;x, v) + ?
(y;x)subject to C(y;x, v)where x = {xi} is the input caption (a sentence),v is the accompanying image, y = {yi} is theoutput sentence, ?
(y;x, v) is the content selectionscore, ?
(y;x) is the linguistic fluency score, andC(y;x, v) is the set of hard constraints.
Let l(yi)be the index of the word in x that is selected as thei?th word in the output y so that xl(yi) = yi.
Then,we factorize ?(?)
and ?(?)
as:?
(y;x, v) =?i?
(yi, x, v) =?i?
(xl(yi), v)?
(y;x) =?i?
(yi, ..., yi?K)=?i?
(xl(yi), ..., xl(yi?K))where K is the size of local context.Content Selection ?
Visual Estimates:The computer vision system used consists of 7404visual classifiers for recognizing leaf level Word-Net synsets (Fellbaum, 1998).
Each classifier istrained using labeled images from the ImageNetdataset (Deng et al 2009) ?
an image databaseof over 14 million hand labeled images orga-nized according to the WordNet hierarchy.
Imagesimilarity is represented using a Spatial PyramidMatch Kernel (SPM) (Lazebnik et al 2006) withLocality-constrained Linear Coding (Wang et al2010) on shape based SIFT features (Lowe, 2004).791(a) (b)0 1 2 3 4 5 6 7 80200400600800# of sentences (in thousands)0 1 2 3 404008001200# of sentences (in thousands)Figure 2: Number of sentences (y-axis) for eachaverage (x-axis in (a)) and maximum (x-axis in(b)) number of words with future dependenciesModels are linear SVMs followed by a sigmoid toproduce probability for each node.3Content Selection ?
Salient Topics:We consider Tf.Idf driven scores to favor salienttopics, as those are more likely to generalizeacross many different images.
Additionally, weassign a very low content selection score (??)
forproper nouns and numbers and a very high score(larger then maximum idf or visual score) for the2k most frequent words in our corpus.Local Linguistic Fluency:We model linguistic fluency with 3-gram condi-tional probabilities:?
(xl(yi), xl(yi?1), xl(yi?2)) (1)= p(xl(yi)|xl(yi?2), xl(yi?1))We experiment with two different ngram statis-tics, one extracted from the Google Web 1T cor-pus (Brants and Franz., 2006), and the other com-puted from the 1M image-caption corpus (Or-donez et al 2011).Dependency-driven Constraints:Table 1 defines the list of dependencies usedas constraints driven from the typed dependen-cies (de Marneffe and Manning, 2009; de Marn-effe et al 2006).
The direction of arrows indi-cate the direction of inclusion requirements.
Forexample, dep(X ??
Y ), denotes that ?X?
mustbe included whenever ?Y ?
is included.
Similarly,dep(X ??
Y ) denotes that ?X?
and ?Y ?
musteither be included together or eliminated together.We determine the uni- or bi-directionality of theseconstraints by manually examining a few examplesentences corresponding to each of these typed de-pendencies.
Note that some dependencies such asdet(??)
would hold regardless of the particular3Code was provided by Deng et al(2012).Method-1 (M1) v.s.
Method-2 (M2) M1 winsover M2SALIENCY ORIG 76.34%VISUAL ORIG 81.75%VISUAL SALIENCY 72.48%VISUAL VISUAL W/O CONSTR 83.76%VISUAL NGRAM-ONLY 90.20%VISUAL HUMAN 19.00%Table 2: Forced Choice Evaluation (LM Corpus =Google)lexical items, while others, e.g., dobj(??)
mayor may not be necessary depending on the context.Those dependencies that we determine as largelycontext dependent are marked with * in Table 1.One could consider enforcing all dependencyconstraints in Table 1 as hard constraints so thatthe compressed sentence must not violate any ofthose directed dependency constraints.
Doing sowould lead to overly conservative compressionwith least compression ratio however.
Therefore,we relax those that are largely context dependentas soft constraints (marked in Table 1 with *) byintroducing a constant penalty term in the objec-tive function.
Alternatively, the dependency basedconstraints can be learned statistically from thetraining corpus of paired original and compressedsentences.
Since we do not have such in-domaintraining data at this time, we leave this explorationas future research.Dynamic Programming with Dynamic Beam:The constraint optimization we formulated corre-sponds to an NP-hard problem.
In our work, hardconstraints are based only on typed dependencies,and we find that long range dependencies occur in-frequently in actual image descriptions, as plottedin Figure 2.
With this insight, we opt for decodingbased on dynamic programming with dynamicallyadjusted beam.4 Alternatively, one can find an ap-proximate solution using Integer Linear Program-ming (e.g., Clarke and Lapata (2006), Clarke andLapata (2007), Martins and Smith (2009)).3 EvaluationSince there is no existing benchmark data for im-age caption generalization, we crowdsource evalu-ation using Amazon Mechanical Turk (AMT).
Weempirically compare the following options:4The required beam size at each step depends on howmany words have dependency constraints involving any wordfollowing the current one ?
beam size is at most 2p, where pis the max number of words dependent on any future words.792Big elm tree overthe house is notheir anymore.?
Tree over the house.Abandonnedhouses in theforest.?
Houses in theforest.A woman paints a tree inbloom near the duck pondin the Boston PublicGarden, April 15, 2006.?
A tree in bloom .Pillbox in fieldbehind a pubcar park.?
Pub car.Flowering tree inmixed forest atWakehurst.?
Flowering treein forest.The insulbrick matchesthe yard.
This is outsideof medina ohio near thetonka truck house.?
The yard.
This isoutside the house.Query Image Retrieved ImagesFigure 3: Example Image Caption TransferMethod LM strict matching semantic matchingCorpus BLEU P R F BLEU P R FORIG N/A 0.063 0.064 0.139 0.080 0.215 0.220 0.508 0.276SALIENCY Image Corpus 0.060 0.074 0.077 0.068 0.302 0.411 0.399 0.356VISUAL Image Corpus 0.060 0.075 0.075 0.068 0.305 0.422 0.397 0.360SALIENCY Google Corpus 0.064 0.070 0.101 0.074 0.286 0.337 0.459 0.340VISUAL Google Corpus 0.065 0.071 0.098 0.075 0.296 0.354 0.457 0.350Table 3: Image Description Transfer: performance in BLEU and F1 with strict & semantic matching.?
ORIG: original uncompressed captions?
HUMAN: compressed by humans (See ?
3.2)?
SALIENCY: linguistic fluency + saliency-basedcontent selection + dependency constraints?
VISUAL: linguistic fluency + visually-guidedcontent selection + dependency constraints?
x W/O CONSTR: method xwithout dependencyconstraints?
NGRAM-ONLY: linguistic fluency only3.1 Intrinsic Evaluation: Forced ChoiceTurkers are provided with an image and two cap-tions (produced by different methods) and areasked to select a better one, i.e., the most relevantand plausible caption that contains the least extra-neous information.
Results are shown in Table 2.We observe that VISUAL (full model with visuallyguided content selection) performs the best, beingselected over SALIENCY (content-selection with-out visual information) in 72.48% cases, and evenover the original image caption in 81.75% cases.This forced-selection experiment between VI-SUAL and ORIG demonstrates the degree of noiseprevalent in the image captions in the wild.
Ofcourse, if compared against human-compressedcaptions, the automatic captions are preferredmuch less frequently ?
in 19% of the cases.
Inthose 19% cases when automatic captions are pre-ferred over human-compressed ones, it is some-times that humans did not fully remove informa-tion that is not visually present or verifiable, andother times humans overly compressed.
To ver-ify the utility of dependency-based constraints,we also compare two variations of VISUAL, withand without dependency-based constraints.
As ex-pected, the algorithm with constraints is preferredin the majority of cases.3.2 Extrinsic Evaluation: Image-basedCaption RetrievalWe evaluate the usefulness of our new image-textparallel corpus for automatic generation of imagedescriptions.
Here the task is to produce, for aquery image, a relevant description, i.e., a visu-ally descriptive caption.
Following Ordonez et al(2011), we produce a caption for a query imageby finding top k most similar images within the1M image-text corpus (Ordonez et al 2011) andthen transferring their captions to the query im-age.
To compute evaluation measures, we take theaverage scores of BLEU(1) and F-score (unigram-based with respect to content-words) over k = 5candidate captions.Image similarity is computed using two global(whole) image descriptors.
The first is the GISTfeature (Oliva and Torralba, 2001), an image de-scriptor related to perceptual characteristics ofscenes ?
naturalness, roughness, openness, etc.The second descriptor is also a global image de-scriptor, computed by resizing the image into a?tiny image?
(Torralba et al 2008), which is ef-fective in matching the structure and overall colorof images.
To find visually relevant images, wecompute the similarity of the query image to im-793Huge wall of glassat the ConferenceCentre inYohohama  Japan.?
Wall of glassMy footprint in asand box?
A sand boxJames the cat isdreaming of runningin a wide greenvalley?
Running ina valley (notrelevant)This little boy was socute.
He was flying hisspiderman kite all byhimself on top of MaxPatch?
This little boy was socute.
He was flying(semantically odd)A view of the post officebuilding in Manila fromthe other side of thePasig River?
A view of the postoffice building fromthe sideCell phone shot ofa hat stall in theNortheast Market,Baltimore, MD.?
Cell phone shot.
(visually notverifiable)Figure 4: Good (left three, in blue) and bad examples (right three, in red) of generalized captionsages in the whole dataset using an unweighted sumof gist similarity and tiny image similarity.Gold standard (human compressed) captions areobtained using AMT for 1K images.
The resultsare shown in Table 3.
Strict matching gives creditonly to identical words between the gold-standardcaption and the automatically produced caption.However, words in the original caption of thequery image (and its compressed caption) do notoverlap exactly with words in the retrieved cap-tions, even when they are semantically very close,which makes it hard to see improvements evenwhen the captions of the new corpus are more gen-eral and transferable over other images.
Therefore,we also report scores based on semantic matching,which gives partial credits to word pairs based ontheir lexical similarity.5 The best performing ap-proach with semantic matching is VISUAL (withLM = Image corpus), improving BLEU, Precision,F-score substantially over those of ORIG, demon-strating the extrinsic utility of our newly gener-ated image-text parallel corpus in comparison tothe original database.
Figure 3 shows an exampleof caption transfer.4 Related WorkSeveral recent studies presented approaches toautomatic caption generation for images (e.g.,Farhadi et al(2010), Feng and Lapata (2010a),Feng and Lapata (2010b), Yang et al(2011),Kulkarni et al(2011), Li et al(2011), Kuznetsovaet al(2012)).
The end goal of our work differs inthat we aim to revise original image captions into5We take Wu-Palmer Similarity as similarity mea-sure (Wu and Palmer, 1994).
When computing BLEU withsemantic matching, we look for the match with the highestsimilarity score among words that have not been matched be-fore.
Any word matched once (even with a partial credit) willbe removed from consideration when matching next words.descriptions that are more general and align moreclosely to the visual image content.In comparison to prior work on sentence com-pression, our approach falls somewhere betweenunsupervised to distant-supervised approach (e.g.,Turner and Charniak (2005), Filippova and Strube(2008)) in that there is not an in-domain train-ing corpus to learn generalization patterns directly.Future work includes exploring more direct su-pervision from human edited sample generaliza-tion (e.g., Knight and Marcu (2000), McDonald(2006)) Galley and McKeown (2007), Zhu et al(2010)), and the inclusion of edits beyond dele-tion, e.g., substitutions, as has been explored bye.g., Cohn and Lapata (2008), Cordeiro et al(2009), Napoles et al(2011).5 ConclusionWe have introduced the task of image caption gen-eralization as a means to reduce noise in the paral-lel corpus of images and text.
Intrinsic and extrin-sic evaluations confirm that the captions in the re-sulting corpus align better with the image contents(are often preferred over the original captions bypeople), and can be practically more useful withrespect to a concrete application.AcknowledgmentsThis research was supported in part by the StonyBrook University Office of the Vice President forResearch.
Additionally, Tamara Berg is supportedby NSF #1054133 and NSF #1161876.
We thankreviewers for many insightful comments and sug-gestions.794ReferencesThorsten Brants and Alex Franz.
2006.
Web 1t 5-gramversion 1.
In Linguistic Data Consortium.James Clarke and Mirella Lapata.
2006.
Constraint-based sentence compression: An integer program-ming approach.
In Proceedings of the COL-ING/ACL 2006 Main Conference Poster Sessions,pages 144?151, Sydney, Australia, July.
Associationfor Computational Linguistics.James Clarke and Mirella Lapata.
2007.
Modellingcompression with discourse constraints.
In Pro-ceedings of the 2007 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP-CoNLL), pages 1?11, Prague, Czech Republic, June.Association for Computational Linguistics.Trevor Cohn and Mirella Lapata.
2008.
Sentencecompression beyond word deletion.
In Proceedingsof the 22nd International Conference on Compu-tational Linguistics (Coling 2008), pages 137?144,Manchester, UK, August.
Coling 2008 OrganizingCommittee.Joao Cordeiro, Gael Dias, and Pavel Brazdil.
2009.Unsupervised induction of sentence compressionrules.
In Proceedings of the 2009 Workshopon Language Generation and Summarisation (UC-NLG+Sum 2009), pages 15?22, Suntec, Singapore,August.
Association for Computational Linguistics.Marie-Catherine de Marneffe and Christopher D. Man-ning.
2009.
Stanford typed dependencies manual.Marie-Catherine de Marneffe, Bill Maccartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.In Language Resources and Evaluation Conference2006.Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,and Li Fei-Fei.
2009.
ImageNet: A Large-Scale Hi-erarchical Image Database.
In Conference on Com-puter Vision and Pattern Recognition.Jia Deng, Jonathan Krause, Alexander C. Berg, andL.
Fei-Fei.
2012.
Hedging your bets: Optimizingaccuracy-specificity trade-offs in large scale visualrecognition.
In Conference on Computer Vision andPattern Recognition.Jesse Dodge, Amit Goyal, Xufeng Han, Alyssa Men-sch, Margaret Mitchell, Karl Stratos, Kota Yam-aguchi, Yejin Choi, Hal Daume III, Alex Berg, andTamara Berg.
2012.
Detecting visual text.
In Pro-ceedings of the 2012 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages762?772, Montre?al, Canada, June.
Association forComputational Linguistics.Ali Farhadi, Mohsen Hejrati, Mohammad AminSadeghi, Peter Young1, Cyrus Rashtchian, JuliaHockenmaier, and David Forsyth.
2010.
Every pic-ture tells a story: generating sentences for images.In European Conference on Computer Vision.Christiane D. Fellbaum, editor.
1998.
WordNet: anelectronic lexical database.
MIT Press.Yansong Feng and Mirella Lapata.
2010a.
How manywords is a picture worth?
automatic caption genera-tion for news images.
In Association for Computa-tional Linguistics.Yansong Feng and Mirella Lapata.
2010b.
Topic mod-els for image annotation and text illustration.
In Hu-man Language Technologies.Katja Filippova and Michael Strube.
2008.
Depen-dency tree based sentence compression.
In Proceed-ings of the Fifth International Natural LanguageGeneration Conference, INLG ?08, pages 25?32,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Michel Galley and Kathleen McKeown.
2007.
Lex-icalized Markov grammars for sentence compres-sion.
In Human Language Technologies 2007:The Conference of the North American Chapter ofthe Association for Computational Linguistics; Pro-ceedings of the Main Conference, pages 180?187,Rochester, New York, April.
Association for Com-putational Linguistics.Kevin Knight and Daniel Marcu.
2000.
Statistics-based summarization - step one: Sentence compres-sion.
In AAAI/IAAI, pages 703?710.Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Sim-ing Li, Yejin Choi, Alexander C Berg, and Tamara LBerg.
2011.
Babytalk: Understanding and gener-ating simple image descriptions.
In Conference onComputer Vision and Pattern Recognition.Polina Kuznetsova, Vicente Ordonez, Alexander Berg,Tamara Berg, and Yejin Choi.
2012.
Collective gen-eration of natural image descriptions.
In Proceed-ings of the 50th Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Pa-pers), pages 359?368, Jeju Island, Korea, July.
As-sociation for Computational Linguistics.Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce.2006.
Beyond bags of features: Spatial pyramidmatching.
In Conference on Computer Vision andPattern Recognition, June.Siming Li, Girish Kulkarni, Tamara L. Berg, Alexan-der C. Berg, and Yejin Choi.
2011.
Composingsimple image descriptions using web-scale n-grams.In Proceedings of the Fifteenth Conference on Com-putational Natural Language Learning, pages 220?228, Portland, Oregon, USA, June.
Association forComputational Linguistics.David G. Lowe.
2004.
Distinctive image features fromscale-invariant keypoints.
Int.
J. Comput.
Vision,60:91?110, November.795Andre Martins and Noah A. Smith.
2009.
Summariza-tion with a joint model for sentence extraction andcompression.
In Proceedings of the Workshop onInteger Linear Programming for Natural LanguageProcessing, pages 1?9, Boulder, Colorado, June.
As-sociation for Computational Linguistics.Ryan T. McDonald.
2006.
Discriminative sentencecompression with soft syntactic evidence.
In EACL2006, 11st Conference of the European Chapter ofthe Association for Computational Linguistics, Pro-ceedings of the Conference, April 3-7, 2006, Trento,Italy.
The Association for Computer Linguistics.Courtney Napoles, Chris Callison-Burch, Juri Ganitke-vitch, and Benjamin Van Durme.
2011.
Paraphras-tic sentence compression with a character-basedmetric: Tightening without deletion.
In Proceed-ings of the Workshop on Monolingual Text-To-TextGeneration, pages 84?90, Portland, Oregon, June.Association for Computational Linguistics.Aude Oliva and Antonio Torralba.
2001.
Modeling theshape of the scene: a holistic representation of thespatial envelope.
International Journal of ComputerVision.Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.2011.
Im2text: Describing images using 1 millioncaptioned photographs.
In Neural Information Pro-cessing Systems (NIPS).Antonio Torralba, Rob Fergus, and William T. Free-man.
2008.
80 million tiny images: a large datasetfor non-parametric object and scene recognition.Pattern Analysis and Machine Intelligence, 30.Jenine Turner and Eugene Charniak.
2005.
Super-vised and unsupervised learning for sentence com-pression.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguis-tics (ACL?05), pages 290?297, Ann Arbor, Michi-gan, June.
Association for Computational Linguis-tics.Jinjun Wang, Jianchao Yang, Kai Yu, Fengjun Lv,T.
Huang, and Yihong Gong.
2010.
Locality-constrained linear coding for image classification.In Conference on Computer Vision and PatternRecognition (CVPR).Zhibiao Wu and Martha Palmer.
1994.
Verbs seman-tics and lexical selection.
In Proceedings of the 32ndannual meeting on Association for ComputationalLinguistics, ACL ?94, pages 133?138, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Yezhou Yang, Ching Teo, Hal Daume III, and YiannisAloimonos.
2011.
Corpus-guided sentence genera-tion of natural images.
In Proceedings of the 2011Conference on Empirical Methods in Natural Lan-guage Processing, pages 444?454, Edinburgh, Scot-land, UK., July.
Association for Computational Lin-guistics.Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.2010.
A monolingual tree-based translation modelfor sentence simplification.
In Proceedings of the23rd International Conference on ComputationalLinguistics (Coling 2010), pages 1353?1361, Bei-jing, China, August.
Coling 2010 Organizing Com-mittee.796
