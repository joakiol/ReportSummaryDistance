Proceedings of NAACL-HLT 2013, pages 556?562,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsEmbracing Ambiguity: A Comparison of Annotation Methodologies forCrowdsourcing Word Sense LabelsDavid JurgensDepartment of Computer ScienceUniversity of California, Los Angelesjurgens@cs.ucla.eduAbstractWord sense disambiguation aims to identifywhich meaning of a word is present in a givenusage.
Gathering word sense annotations is alaborious and difficult task.
Several methodshave been proposed to gather sense annota-tions using large numbers of untrained anno-tators, with mixed results.
We propose threenew annotation methodologies for gatheringword senses where untrained annotators areallowed to use multiple labels and weight thesenses.
Our findings show that given the ap-propriate annotation task, untrained workerscan obtain at least as high agreement as anno-tators in a controlled setting, and in aggregategenerate equally as good of a sense labeling.1 IntroductionWord sense annotation is regarded as one of the mostdifficult annotation tasks (Artstein and Poesio, 2008)and building manually-annotated corpora with high-quality sense labels is often a time- and resource-consuming task.
As a result, nearly all sense-taggedcorpora in wide-spread use are created using trainedannotators (Hovy et al 2006; Passonneau et al2010), which results in a knowledge acquisition bot-tleneck for training systems that require sense labels(Gale et al 1992).
In other NLP areas, this bot-tleneck has been addressed through gathering anno-tations using many untrained workers on platformssuch as Amazon Mechanical Turk (MTurk), a taskcommonly referred to as crowdsourcing.
Recently,several works have proposed gathering sense anno-tations using crowdsourcing (Snow et al 2008; Bie-mann and Nygaard, 2010; Passonneau et al 2012b;Rumshisky et al 2012).
However, these meth-ods produce sense labels that are different from thecommonly used sense inventories such as WordNet(Fellbaum, 1998) or OntoNotes (Hovy et al 2006).Furthermore, while Passonneau et al(2012b) diduse WordNet sense labels, they found the qualitywas well below that of trained experts.We revisit the task of crowdsourcing word senseannotations, focusing on two key aspects: (1) theannotation methodology itself, and (2) the restric-tion to single sense assignment.
First, the choice insense inventory plays an important role in gatheringhigh-quality annotations; fine-grained inventoriessuch as WordNet often contain several related sensesfor polysemous words, which untrained annotatorsfind difficult to correctly apply in a given context(Chugur et al 2002; McCarthy, 2006; Palmer etal., 2007; Rumshisky and Batiukova, 2008; Brownet al 2010).
However, many agreement studieshave restricted annotators to using a single sense,which can significantly lower inter-annotator agree-ment (IAA) in the presence of ambiguous or poly-semous usages; indeed, multiple studies have shownthat when allowed, annotators readily assign multi-ple senses to a single usage (Ve?ronis, 1998; Mur-ray and Green, 2004; Erk et al 2009; Passonneauet al 2012b).
Therefore, we focus on annotationmethodologies that enable workers to use as manylabels as they feel appropriate, asking the question:if allowed to make labeling ambiguity explicit, willannotators agree?
Furthermore, we adopt the goalof Erk et al(2009), which enabled annotators toweight each sense by its applicability to the givencontext, thereby quantifying the ambiguity.556This paper provides the following contributions.First, we demonstrate that the choice in annotationsetup can significantly improve IAA and that the la-bels of untrained workers follow consistent patternsthat enable creating high quality labeling from theiraggregate.
Second, we find that the sense labelingfrom crowdsourcing matches performance with an-notators in a controlled setting.2 Related WorkGiven the potential utility of a sense-labeled corpus,multiple studies have examined how to efficientlygather high quality sense annotations.
Snow et al(2008) had MTurk workers, referred to as Turkers,disambiguate uses of ?president.?
While they re-ported extremely high IAA (0.952), their analysiswas only performed on a single word.Biemann and Nygaard (2010) and Biemann(2012) construct a sense-labeled corpus by concur-rently constructing the sense inventory itself.
Turk-ers used a lexical substitution task to identify validsubstitutions of a target word.
The contexts for theresulting substitutions were clustered based on theirword overlap and the resulting clusters were labeledas senses.
Biemann and Nygaard (2010) showed thatthe number of sense definitions for a word in theirinventory was correlated with the number in Word-Net, often with their inventory having fewer sensesby combining related meanings and omitting raremeanings.Hong and Baker (2011) evaluated multiple anno-tation strategies for gathering FrameNet sense anno-tations, ultimately yielding high (>90%) accuracyfor most terms after filtering.
They highlight am-biguous and polysemous usages as a notable sourceof errors, which the present work directly addresses.In the most related work, Passonneau et al(2012b) had Turkers annotate contexts using one ormore senses, with the requirement that a worker la-bels all contexts.
While they found that agreementbetween all workers was low, their annotations couldbe combined using the GLAD model (Whitehill etal., 2000) to obtain good performance, though notas good as trained annotators.3 Annotation MethodologiesWe consider three methodologies for gatheringsense labels: (1) the methodology of Erk et al(2009) for gathering weighted labels, (2) a multi-stage strategy that uses both binary and Likert rat-ings, and (3) MaxDiff, a paired choice format.Likert Ratings Likert rating scales provide themost direct way of gathering weighted sense labels;Turkers are presented with all senses of a word andthen asked to rate each on a numeric scale.
We adoptthe annotation guidelines of Erk et al(2009) whichused a five-point scale, ranging from 1 to 5, indicat-ing the sense does not apply or that it matches thecontextual usage exactly, respectively.Select and Rate Recent efforts in crowdsourc-ing have proposed multi-stage processes for accom-plishing complex tasks, where efforts by one groupof workers are used to create new subtasks for otherworkers to complete (Bernstein et al 2010; Kitturet al 2011; Kulkarni et al 2012).
We propose atwo-stage strategy that aims to reduce the complex-ity of the annotation task, referred to as Select andRate (S+R).
First, Turkers are presented with all thesenses and asked to make a binary choice of whichsenses apply.
Second, a Likert rating task is createdfor only those senses whose selection frequency isabove a threshold, thereby concentrating worker fo-cus on a potentially smaller set of senses.Our motivation for S+R is two-fold.
First, thesense definitions of certain words may be unclearor misinterpreted by a minority of the Turkers, whothen systematically rate inapplicable senses as appli-cable.
The Select task can potentially remove suchnoise and therefore improve both IAA and ratingquality in the subsequent Rate task.
Second, whilethe present study analyzes words with 4?8 senses,we are ultimately interested in annotating highlypolysemous words with tens of senses, which couldpresent a significant cognitive burden for an annota-tor to rate concurrently.
Here, the Select stage canpotentially reduce the number of senses presented,leading to less cognitive burden in the Rate stage.Furthermore, as a pragmatic benefit, removing in-applicable senses reduces the visual space requiredfor displaying the questions on the MTurk platform,which can improve annotation throughput.MaxDiff MaxDiff is an alternative to scale-basedratings in which Turkers are presented with a onlysubset of all of a word?s senses and then asked to se-lect (1) the sense option that best matches the mean-557add.v ask.v win.v argument.n interest.n paper.n different.a important.aErk et al(2009) IAA 0.470 0.354 0.072 0.497 0.320 0.403 0.212 0.466MTurk Likert IAA 0.336 0.212 0.129 0.250 0.209 0.522 0.030 0.240MTurk Select 0.309 0.127 0.179 0.192 0.164 0.449 0.024 0.111MTurk Rate 0.204 0.076 0.026 0.005 0.081 0.108 0.005 0.116MTurk MaxDiff 0.493 0.353 0.295 - 0.349 0.391 0.220 0.511Likert Mode 0.500 0.369 0.083 0.445 0.388 0.518 0.124 0.516S+R Median 0.473 0.394 0.149 0.497 0.390 0.497 0.103 0.416MTurk MaxDiff 0.508 0.412 0.184 - 0.408 0.496 0.115 0.501Sampled Baseline 0.238 0.178 0.042 0.254 0.162 0.205 0.100 0.221Random Baseline 0.239 0.186 0.045 0.249 0.269 0.200 0.110 0.269Table 1: IAA per word (top) and IAA between aggregate labelings and the GWS annotators (bottom)ing in the example context and (2) the sense optionthat least matches (Louviere, 1991).
In our setting,we presented three options at a time for words withfewer than seven senses, and four options for thosewith seven senses.
For a single context, multiplesubsets of the senses are presented and then their rel-ative ranking is used to produce the numeric rating.The final applicability ratings were produced usinga modification of the counting procedure of Orme(2009).
First, all sense ratings are computed as thenumber of times the sense was rated best minus thenumber of times rated least.
Second, all negatively-rated senses are assigned score of 1, and all posi-tively ratings are normalized to be (1, 5].4 ExperimentsFor measuring the difference in methodologies, wepropose three experiments based on different anal-yses of comparing Turker and non-Turker annota-tions on the same dataset, the latter of which we re-fer to as the reference labeling.
First, we measurethe ability of the Turkers individually by evaluat-ing their IAA with the reference labeling.
Second,many studies using crowdsourcing combine the re-sults into a single answer, thereby leveraging thewisdom of the crowds (Surowiecki, 2005) to smoothover inconsistencies in the data.
Therefore, in thesecond experiment, we evaluate different methodsof combining Turker responses into a single senselabeling, referred to as an aggregate labeling, andcomparing that with the reference labeling.
Third,we measure the replicability of the Turker annota-tions (Kilgarriff, 1999) using a sampling methodol-ogy.
Two equally-sized sets of Turker annotationsare created by randomly sampling without replace-ment from the full set of annotations for each item.IAA is calculated between the aggregate labelingscomputed from each set.
This sampling is repeated50 times and we report the mean IAA as a measureof the expected degree of replicability when anno-tating using different groups of Turkers.For the reference sense labeling, we use a subsetof the GWS dataset of Erk et al(2009), where threeannotators rated 50 instances each for eight words.For clarity, we refer to these individuals as the GWSannotators.
Given a word usage in a sentence, GWSannotators rated the applicability of all WordNet 3.0senses using the same Likert scale as described inSection 3.
Contexts were drawn evenly from theSemCor (Miller et al 1993) and SENSEVAL-3 lex-ical substitution (Mihalcea et al 2004) corpora.GWS annotators were apt to use multiple senses,with nearly all instances having multiple labels.For each annotation task, Turkers were presentedwith an identical set of annotation guidelines, fol-lowed by methodology-specific instructions.1 To in-crease the familiarity with the task, four instanceswere shown per task, with all instances using thesame target word.
Unlike Passonneau et al(2012b),we did not require a Turker to annotate all contextsfor a single word; however many Turkers did com-plete the majority of instances.
Both the Likert, Se-lect, and Rate tasks used ten Turkers each.
Senseswere passed from Select to Rate if they received at1Full guidelines are available at http://cs.ucla.edu/?jurgens/sense-annotation/558least three votes.
For MaxDiff, we gathered at least3n annotations per context where n is the number ofsenses of the target word, ensuring that each senseappeared at least once.
Due to resource limitations,we omitted the evaluation of argument.n for MaxD-iff.
Following the recommendation of Kosinski et al(2012), Turkers were paid $0.05USD for each Lik-ert, Select, and Rate task.
For MaxDiff, due to theirshorter nature and comparably high volume, Turkerswere paid $0.03USD per task.To ensure fluency in English as well as reduce thepotential for low-quality results, we prefaced eachtask with a simple test question that asked the Turkerto pick out a definition of the target word from a listof four options.
The incorrect options were selectedso that they would be nonsensical for anyone famil-iar with the target word.
Additionally, we rejectedall Turker responses where more than one optionwas missing a rating.
In the case of missing ratings,we infer a rating of 1.
Approximately 20-30% of thesubmissions were rejected by these criteria, under-scoring the importance of filtering.For measuring IAA, we selected Krippendorff?s?
(Krippendorff, 1980; Artstein and Poesio, 2008),which is an agreement coefficient that handles miss-ing data, as well as different levels of measurement,e.g., nominal data (Select and MaxDiff) and intervaldata (Likert and Rate).2 Krippendorff?s ?
adjusts forchance, ranging between [?1, 1] for nominal dataand (?1, 1] for interval data, where 1 indicates per-fect agreement and -1 indicates systematic disagree-ment; random labels would have an expected ?
ofzero.
We treat each sense and instance combinationas a separate item to rate.5 ResultsThe results of the first experiment appear in the topof Table 1.
Two important aspects emerge.
First, theword itself plays a significant role in IAA.
ThoughErk et al(2009) reported a pair-wise IAA of theGWS annotators between 0.466 and 0.506 usingSpearman?s ?, the IAA varies considerably betweenwords for both Turkers and GWS annotators whenmeasured using Krippendorff?s ?.Second, the choice of annotation methodology2We note that although the ratings are technically given onan ordinal scale (ranks), we use the interval scale to allow com-parison with rational ratings from the aggregate solutions.significantly impacts IAA.
While both the Likert andS+R tasks have lower IAA than the GWS annota-tors do, the MaxDiff annotators achieve higher IAAfor almost all words.
We hypothesize that compar-ing senses for applicability is an easier task for theuntrained worker, rather than having to construct amental scale of what constitutes the applicability ofeach sense.
Surprisingly, the binary Select task hasa lower IAA than the more complex the Likert task.An analysis of the duration of median task comple-tion times for the Likert and Select tasks showed lit-tle difference (with the exception of paper.n, whichwas on average 50 second faster for Likert ratings),suggesting that both tasks are equally as cognitivelydemanding.
In addition, the Rate task has the lowestIAA, despite its similarity to the Likert task.
An in-spection of the annotations shows that the full ratingscale was used, so the low value is not due to Turk-ers always using the same rating, which would yieldan IAA near chance.In the second experiment, we created a aggregatesense labeling and compared its IAA with the GWSannotators, shown in Table 1 (bottom).
For scale-based ratings, we considered three arithmetic oper-ations for selecting the final rating: mode, median,and mean.
We found that the mode yielded the high-est average IAA for the Likert ratings and median forS+R; however, the differences in IAA using each op-eration were often small.
We compare the IAA withGWS annotators against two baselines: one gener-ated by sampling from the GWS annotators?
ratingdistribution, and a second generated by uniformlysampling in [1, 5].
By comparison, the aggregate la-belings have a much larger IAA than the baselines,which is often at least as high as the IAA amongstthe GWS annotators themselves, indicating that theTurkers in aggregate are capable of producing equiv-alent ratings.
Of the three annotation methodolo-gies, MaxDiff provides the highest IAA both withinits annotators and with its aggregate key.
Surpris-ingly, neither the Likert or S+R aggregate labelingappears better than the other.Based on the second experiment, we measuredthe average IAA across all words between the ag-gregate Likert and MaxDiff solutions, which was0.472.
However, this IAA is significantly affected bythe annotations for win.v and different.a, which hadthe lowest IAA among Turkers (Table 1) and there-559Corpus Sense Inventory IAA MeasurementSensEval-1(Kilgarriff and Rosenzweig, 2000)HECTOR 0.950 Replicability experiment(Kilgarriff, 1999)OntoNotes (Hovy et al 2006) OntoNotes ?
0.90?
Pair-wise agreementSALSA (Burchardt et al 2006) FrameNet 0.86 Percentage agreementSensEval-2 Lexical Sample(Kilgarriff, 2002)WordNet 1.7 0.853, 0.710, 0.673?
Adjudicated AgreementGWS with MaxDiff Replicability WordNet 3.0 0.815 Krippendorff?s ?SemCor (Fellbaum et al 1998) WordNet 1.6 0.786, 0.57?
Percentage agreementSensEval-3 (Snyder and Palmer, 2004) WordNet 1.7 0.725 Percentage agreementMASC (Passonneau et al 2012a) WordNet 3.1 -0.02 to 0.88/ Krippendorff?s ?with MASI (Passonneau et al 2006)MASC, single phase reportedin Passonneau et al(2010)WordNet 3.1 0.515 Krippendorff?s ?GWS with Likert Replicability WordNet 3.0 0.409 Krippendorff?s ?GWS with Erk et al(2009) annotators WordNet 3.0 0.349 Krippendorff?s ??
Not all words achieved this agreement.?
Kilgarriff (2002) uses a multi-stage agreement procedure where two annotators rate each item, and in the case of disagree-ment, a third annotator is added.
If the third annotator agrees with either of the first two, the instance is marked as a caseof agreement.
However, the unadjudicated agreement for the dataset was 67.3 measured using pair-wise agreement.
Are-annotation by Palmer et al(2004) produced a similar pair-wise agreement of 71.0.?
Tou et al(1999) perform a re-annotation test of the same data using student annotators, finding substantially lower agreement Excludes agreement for argument.n, which was not annotated/ IAA ranges for 37 words; no corpus-wide IAA is provided.Table 2: IAA for sense-annotated corporafore produce noisy aggregate solutions.
When win.vand different.a are excluded, the agreement betweenaggregate Likert and MaxDiff solutions is 0.649.While this IAA is still moderate, it suggests thatTurkers can still produce similar annotations evenwhen using different annotation methodologies.For the third experiment, replicability is reportedas the average IAA between the sampled aggregatelabelings for all annotated words.
Table 2 shows thisIAA for Likert and MaxDiff methodologies in com-parison to other sense annotation studies.
Krippen-dorff (2004) recommends that an ?
of 0.8 is nec-essary to claim high-quality agreement, which isachieved by the MaxDiff methodology.
In contrast,the average IAA between sampled Likert ratings issignificantly lower, though the methodology doesachieve an ?
of 0.812 for paper.n.
However, whenthe two words with the lowest IAA, win.v and differ-ent.a, are excluded, the average ?
increases to 0.880for MaxDiff and 0.649 for Likert.
Overall, these re-sults suggest that MaxDiff can generate highly repli-cable annotations with agreement on par with that ofother high-quality sense-labeled corpora.
Further-more, the Likert methodology may in aggregate stillproduce moderately replicable annotations in somecases.6 Conclusion and Future WorkWord sense disambiguation is a difficult task, bothfor humans and algorithms, with an important bot-tleneck in acquiring large sense annotated corpora.As a potential solution, we proposed three annota-tion methodologies for crowdsourcing sense labels.Importantly, we relax the single sense assignmentrestriction in order to let annotators explicitly noteambiguity through weighted sense ratings.
Our find-ings reveal that moderate IAA can be obtained usingMaxDiff ratings, with IAA surpassing that of anno-tators in a controlled setting.
Furthermore, our find-ings showed marked differences in rating difficultyper word, even in the weighted rating setting.
Infuture work, we will investigate what factors influ-ence annotation difficulty in order to improve IAAto what is considered expert levels, drawing fromexisting work analyzing difficulty in the single labelsetting (Murray and Green, 2004; Passonneau et al2009; Cinkova?
et al 2012).560ReferencesR.
Artstein and M. Poesio.
2008.
Inter-coder agreementfor computational linguistics.
Computational Linguis-tics, 34(4):555?596.Michael S. Bernstein, Ggreg Little, Robert C. Miller,Bjo?n Hartmann, Mark S. Ackerman, David R. Karger,David Crowell, and Katrina Panovich.
2010.
Soylent:a word processor with a crowd inside.
In Proceedingsof UIST, pages 313?322.
ACM.Chris Biemann and Valerie Nygaard.
2010.
Crowdsourc-ing WordNet.
In The 5th International Conference ofthe Global WordNet Association (GWC-2010).Chris Biemann.
2012.
Turk Bootstrap Word Sense In-ventory 2.0: A Large-Scale Resource for Lexical Sub-stitution.
In Proceedings of LREC.Susan Windisch Brown, Travis Rood, and Martha Palmer.2010.
Number or nuance: Which factors restrict reli-able word sense annotation?
In Proceedings of LREC.Aljoscha Burchardt, Katrin Erk, Anette Frank, AndreaKowalski, Sebastian Pado?, and Manfred Pinkal.
2006.The SALSA corpus: a German corpus resource forlexical semantics.
In Proceedings of LREC.Irina Chugur, Julio Gonzalo, and Felisa Verdejo.
2002.Polysemy and sense proximity in the senseval-2 testsuite.
In Proceedings of the SIGLEX/SENSEVALWorkshop on Word Sense Disambiguation: RecentSuccesses and Future Directions, pages 32?39.
ACL.Silvie Cinkova?, Martin Holub, and Vincent Kr??.
2012.Managing uncertainty in semantic tagging.
In Pro-ceedings of EACL, pages 840?850.
ACL.Katrin Erk, Diana McCarthy, and Nicholas Gaylord.2009.
Investigations on word senses and word usages.In Proceedings of ACL, pages 10?18.
ACL.Christiane Fellbaum, Jaochim Grabowski, and Shari Lan-des.
1998.
Performance and confidence in a seman-tic annotation task.
WordNet: An electronic lexicaldatabase, pages 217?237.Christine Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
The MIT Press.William A. Gale, Kenneth W. Church, and DavidYarowsky.
1992.
A method for disambiguating wordsenses in a large corpus.
Computers and the Humani-ties, 26(5):415?439.J.
Hong and C.F.
Baker.
2011.
How Good is the Crowdat ?real?
WSD?
In Proceedings of the Fifth LinguisticAnnotation Workshop (LAW V), pages 30?37.
ACL.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
OntoNotes:the 90% solution.
In Proceedings of NAACL, pages57?60.
ACL.A.
Kilgarriff and J. Rosenzweig.
2000.
Framework andresults for english senseval.
Computers and the Hu-manities, 34(1):15?48.Adam Kilgarriff.
1999.
95% replicability for manualword sense tagging.
In Proceedings of EACL, pages277?278.
ACL.Adam Kilgarriff.
2002.
English lexical sample task de-scription.
In Senseval-2: Proceedings of the 2nd In-ternational Workshop on Evaluating Word Sense Dis-ambiguation Systems.A.
Kittur, B. Smus, S. Khamkar, and R.E.
Kraut.
2011.Crowdforge: Crowdsourcing complex work.
In Pro-ceedings of UIST, pages 43?52.
ACM.M.
Kosinski, Y. Bachrach, G. Kasneci, J. Van-Gael, andT.
Graepel.
2012.
Crowd IQ: Measuring the intelli-gence of crowdsourcing platforms.
In ACM Web Sci-ences.
ACM.Klaus Krippendorff.
1980.
Content Analysis: An Intro-duction to Its Methodology.
Sage, Beverly Hills, CA.Klaus Krippendorff.
2004.
Content Analysis: An In-troduction to Its Methodology.
Sage, Thousand Oaks,CA, second edition.A.
Kulkarni, M. Can, and B. Hartmann.
2012.
Collabo-ratively crowdsourcing workflows with turkomatic.
InProceedings of CSCW, pages 1003?1012.
ACM.J.
J. Louviere.
1991.
Best-Worst Scaling: A Model forthe Largest Difference Judgments.
Technical report,University of Alberta.
Working Paper.Diana McCarthy.
2006.
Relating WordNet senses forword sense disambiguation.
In Proceedings of theACL Workshop on Making Sense of Sense: BringingPsycholinguistics and Computational Linguistics To-gether, pages 17?24.Rada Mihalcea, Timothy Chklovski, and Adam Kilgar-riff.
2004.
The Senseval-3 English lexical sampletask.
In Senseval-3: Third International Workshop onthe Evaluation of Systems for the Semantic Analysis ofText, pages 25?28.
ACL.George A. Miller, Claudia Leacock, Randee Tengi, andRoss T. Bunker.
1993.
A semantic concordance.
InProceedings of HLT, pages 303?308.
ACL.G.
Craig Murray and Rebecca Green.
2004.
Lexicalknowledge and human disagreement on a WSD task.Computer Speech & Language, 18(3):209?222.Bryan Orme.
2009.
MaxDiff Analysis: Simple Count-ing, Individual-Level Logit, and HB.
Sawtooth Soft-ware.Martha Palmer, Olga Babko-Malaya, and Hoa TrangDang.
2004.
Different sense granularities for differ-ent applications.
In Proceedings of the Second Work-shop on Scalable Natural Language UnderstandingSystems.
ACL.Martha Palmer, Hoa Trang Dang, and Christiane Fell-baum.
2007.
Making fine-grained and coarse-grainedsense distinctions, both manually and automatically.Natural Language Engineering, 13(02):137?163.561Rebecca Passonneau, Nizar Habash, and Owen Rambow.2006.
Inter-annotator agreement on a multilingualsemantic annotation task.
In Proceedings of LREC,pages 1951?1956.Rebecca J. Passonneau, Ansaf Salleb-Aouissi, and NancyIde.
2009.
Making sense of word sense variation.
InProceedings of the NAACL HLT Workshop on Seman-tic Evaluations: Recent Achievements and Future Di-rections.Rebecca J. Passonneau, Ansaf Salleb-Aoussi, VikasBhardwaj, and Nancy Ide.
2010.
Word sense anno-tation of polysemous words by multiple annotators.
InProceedings of LREC.Rebecca J Passonneau, Collin Baker, Christiane Fell-baum, and Nancy Ide.
2012a.
The MASC word sensesentence corpus.
In Proceedings of LREC.Rebecca J. Passonneau, Vikas Bhardwaj, Ansaf Salleb-Aouissi, and Nancy Ide.
2012b.
Multiplicity andword sense: evaluating and learning from multiply la-beled word sense annotations.
Language Resourcesand Evaluation, 46(2):209?252.Anna Rumshisky and Olga Batiukova.
2008.
Polysemyin verbs: systematic relations between senses and theireffect on annotation.
In Proceedings of the Workshopon Human Judgements in Computational Linguistics,pages 33?41.
ACL.Anna Rumshisky, Nick Botchan, Sophie Kushkuley, andJames Pustejovsky.
2012.
Word Sense Inventories byNon-experts.
In Procoeedings of LREC.Rion Snow, Brendan O?Connor, Dan Jurafsky, and An-drew Y. Ng.
2008.
Cheap and fastbut is it good?
: Eval-uating non-expert annotations for natural languagetasks.
In Proceedings of EMNLP, pages 254?263.ACL.Benjamin Snyder and Martha Palmer.
2004.
The en-glish all-words task.
In Senseval-3: Third Interna-tional Workshop on the Evaluation of Systems for theSemantic Analysis of Text, pages 41?43.James Surowiecki.
2005.
The wisdom of crowds.
An-chor.Ng Hwee Tou, Chung Yong Lim, and Shou King Foo.1999.
A Case Study on Inter-Annotator Agreementfor Word Sense Disambiguation.
In Proceedings ofthe ACL SIGLEX Workshop on Standardizing LexicalResources.Jean Ve?ronis.
1998.
A study of polysemy judgments andinter-annotator agreement.
In Program and advancedpapers of the Senseval workshop, pages 2?4.Jacob Whitehill, Paul Ruvolo, Tingfan Wu, JacobBergsma, and Javier Movellan.
2000.
Whose voteshould count more: Optimal integration of labels fromlabelers of unknown expertise.
In Proceedings ofNIPS.562
