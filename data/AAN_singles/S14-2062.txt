Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 366?369,Dublin, Ireland, August 23-24, 2014.JOINT_FORCES:Unite Competing Sentiment Classifiers with Random ForestOliver D?rr, Fatih Uzdilli, and Mark CieliebakZurich University of Applied SciencesWinterthur, Switzerland{dueo, uzdi, ciel}@zhaw.chAbstractIn this paper, we describe how we cre-ated a meta-classifier to detect the mes-sage-level sentiment of tweets.
We par-ticipated in SemEval-2014 Task 9B bycombining the results of several exist-ing classifiers using a random forest.The results of 5 other teams from thecompetition as well as from 7 general-purpose commercial classifiers wereused to train the algorithm.
This way,we were able to get a boost of up to3.24 F1 score points.1 IntroductionThe interest in sentiment analysis grows as pub-licly available text content grows.
As one of themost used social media platforms, Twitter pro-vides its users a unique way of expressing them-selves.
Thus, sentiment analysis of tweets hasbecome a hot research topic among academiaand industry.In this paper, we describe our approach of com-bining multiple sentiment classifiers into a meta-classifier.
The introduced system participated inSemEval-2014 Task 9: ?Sentiment Analysis inTwitter, Subtask?B Message Polarity Classifica-tion?
(Rosenthal et al., 2014).
The goal was toclassify a tweet on the message level using thethree classes positive, negative, and neutral.
Theperformance is measured using the macro-averaged F1 score of the positive and negativeclasses which is simply named ?F1 score?throughout the paper.
An almost identical taskwas already run in 2013 (Nakov et al., 2013).The tweets for training and development wereonly provided as tweet ids.
A fraction (10-15%)of the tweets was no longer available on twitter,which makes the results of the competition notfully comparable.
For testing, in addition to lastyear?s data (tweets and SMS) new tweets anddata from a surprise domain (LiveJournal) wereprovided.
An overview of the provided data isshown in Table 1.Using additional manually labelled data fortraining the algorithm was not allowed for a?constrained?
submission.
Submissions usingadditional data for training were marked as ?un-constrained?.Dataset Total Pos Neg NeuTraining (Tweets) 8224 3058 1210 3956Dev (Tweets) 1417 494 286 637Test: Twitter2013 3813 1572 601 1640Test: SMS2013 2093 492 394 1207Test: Twitter2014 1853 982 202 669Test: Twitter?14Sarcasm 86 33 40 13Test: LiveJournal2014 1142 427 304 411Table 1: Number of Documents we were able todownload for Training, Development and Test-ing.Our System.
The results of 5 other teams fromthe competition as well as from 7 general-purpose commercial classifiers were used to trainour algorithm.
Scientific subsystems were s_gez(Gezici et al., 2013), s_jag (Jaggi et al., 2014),s_mar (Marchand et al., 2013), s_fil (Filho andPardo, 2013), s_gun (G?nther and Furrer, 2013).They are all ?constrained?
and machine learning-based, some with hybrid rule-based approaches.Commercial subsystems were provided byThis work is licensed under a Creative Commons Attribution4.0 International Licence.
Page numbers and proceedingsfooter are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/366Lymbix (c_lym), MLAnalyzer1 (c_mla), Seman-tria (c_sem), Sentigem (c_snt), Syttle (c_sky),Text-Processing.com (c_txp), and Webknox(c_web).
Subsystems c_txp and c_web are ma-chine learning-based, c_sky is rule-based, andm_mla is a mix (other tools unknown).
All sub-systems were designed to handle tweets and fur-ther text types.Our submission included a subset of all classi-fiers including unconstrained ones, leading to anunconstrained submission.
The 2014 winningteam obtained an F1 score of 70.96 on the Twit-ter2014 test set.
Our approach was ranked on the12th place out of the 50 participating submis-sions, with an F1 score of 66.79.
Our furtherrankings were 12th on the LiveJournal data, 12thon the SMS data, 12th on Twitter-2013, and 26thon Twitter Sarcasm.Improvement.
Although our meta-classifierdid not reach a top position in the competition,we were able to beat even the best single subsys-tem it was based on for almost all test sets (ex-cept sarcasm).
In previous research we showedthat same behaviour on different systems anddata sets (Cieliebak et al., 2014).
This shows thatalso other systems from the competition, evenbest ones, probably can be improved using ourapproach.2 ApproachMeta-Classifier.
A meta-classifier is an ap-proach to predict a classification given the indi-vidual results of other classifiers by combiningthem.
A robust classifier, which can naturallyhandle categorical input such as sentiments bydesign, is the random forest classifier (Breiman,2001).
The algorithm uses the outputs of individ-ual classifiers as features and the labels on thetraining data as input for training.
Afterwards, inthe test phase, the random forest makes predic-tions using the outputs of the same individualclassifiers.
We use the random forest implemen-tation of the R-package "randomForest" and treatthe three votes (negative, neutral, positive) ascategorical input.Training Data.
To build a meta-classifier, first,one has to train all the subsystems with a dataset.Second, the meta-classifier has to be trainedbased on the output of the subsystems with a dif-ferent dataset than the one used for training the1 mashape.com/mlanalyzer/ml-analyzersubsystems.
We decided to take the natural splitof the data provided by the organizers (see Table1).
For the scientific subsystems we used theTraining set to train on; for training the randomforest classifier we used the Dev set.
The com-mercial systems were used "as-is", in particular,we did not train them on any of the provided datasets.
Table 2 shows the performance of the indi-vidual subsystems on the different data sets.IDDevSMS2013Twitter2013Twitter2014Sarcasm2014LiveJournal2014s_gez 32.22 31.23 30.77 28.57 51.57 50.83s_jag 61.47 56.17 60.21 62.73 44.26 63.91s_mar 28.95 22.94 26.68 22.86 31.01 24.47s_fil 52.88 49.94 55.61 55.08 38.22 56.41s_gun 63.93 61.51 65.33 65.09 48.80 68.91c_lym 48.38 44.40 48.68 54.17 34.87 58.71c_mla 49.79 46.41 50.17 47.74 43.16 59.02c_sma 55.89 52.26 56.15 53.51 49.33 56.53c_sky 56.30 52.04 54.67 56.28 40.60 54.61c_txp 43.69 46.47 41.15 44.00 59.74 56.57c_web 47.44 41.64 45.21 48.83 45.25 53.45c_snt 56.86 58.42 62.17 58.35 36.08 65.74Table 2: F1 scores of the individual systems.Bold shows the best commercial or scientificsystem per data set; grey cells indicates the over-all maximum.3 ExperimentsThere exist three obvious selections of subsys-tems for our meta-classifier: all subsystems, onlyscientific subsystems, and only commercial sub-systems (called All_Subsystems, All_Scientific,and All_Commercial, respectively).
Table 3shows performance of these selections of subsys-tems on the data sets.
For comparison, the tableshows also the performance of the overall bestindividual subsystem in the first row.
It turns outthat All_Subsystems is almost always better thanthe best individual subsystem, while the othertwo meta-classifiers are inferior.Testing All Subsets.
We performed a systematicevaluation on how the performance depends onthe choice of a particular selection of individualsubsystems.
This resembles feature selection,which is a common task in machine learning, and367As a general trend we see that the performanceincreases with the number of classifiers; howev-er, there exist certain subsets which perform bet-ter than using all available classifiers.Best Subset Selection.
In Figure 1, we markedfor each number of subsystems the highest OOB-F1-Score on the Dev set by a diamond.
In addi-tion, the subset with the overall highest OOB-F1-Score, consisting of 7 classifiers, is displayed asa filled diamond.Figure 1: Box Plot showing the F1 scores (out-of-bag) for all subsets on the Dev set.
Diamondsmark the best combination of classifiers for thecorresponding number.We also evaluated the performance of these?best?
subsets on other unseen test data.
In Fig-ure 2, we show the results of the test set Twit-ter2014.
The scores for the very subsets markedin Figure 1 are displayed in the same way here.Figure 2: F1 scores of all subsets on the Twit-ter2014 test set.For comparison, we marked the performanceof the system with all classifiers by a straightline.
We find that all subsets that are ?best?
onthe Dev set perform very well on the Twit-ter2014 set.
In fact, some even beat the systemwith all classifiers.
Similar behaviour can be ob-served for Twitter2013 and LiveJournal2014 (da-ta not shown), while All_Subsets yields signifi-cantly superior results on SMS2013 (see Figure3).
No conclusive observation is possible forSarcasm2014 (data not shown).To elucidate on the question whether to use asubset with the highest OOB-F1 on the Dev set(called Max_OOB_Subset) or to use all availableclassifiers, we show in Table 3 the performanceof these systems on all test sets in rows 2 and 5,respectively.
Since All_Systems is in 2 out of 5cases the best classifier, and?Max_OOB_Subset?
in 3 out of 5 cases, a deci-sive answer cannot be drawn.
However, we findDev(OOB)SMS2013 Twitter2013 Twitter2014 Twitter2014SarcasmLiveJournal2014Best Individual 63.93 61.51 65.33 65.09 48.80 68.91All_Subsystems 63.54 64.22 67.03 67.70 46.37 71.11All_Scientific 64.52 60.42 64.54 64.99 43.35 67.86All_Commercial 62.11 58.34 60.70 63.86 44.85 65.57Max_OOB_Subset 68.27 63.02 67.49 68.33 45.40 71.43Our Submission 65.00 62.20 66.61 66.79 45.40 70.02Table 3: Performance (in F1 score) of meta-classifiers with different subsystems.
The subset used inour submission is composed of s_gez, s_jag, s_mar, s_fil, s_gun, c_sma, c_sky, c_snt.?Max_OOB_Subset?
is composed of s_jag, s_mar, s_gun, c_lym, c_sma, c_sky, c_txp.
Bold showsbest result per data set.
The first row shows results of the best individual subsystem.368that All_Systems generalizes better to foreigntypes of data, while Max_OOB_Subset performswell on similar data (in this case, tweets).Figure 3: F1 score of all subsets on the SMS2013test set.4 ConclusionWe have shown that a meta-classifier approachusing random forest can beat the performance ofthe individual sentiment classifiers it is based on.Typically, the more subsystems are used, the bet-ter the performance.
However, there exist selec-tions of only few subsystems that perform com-parable to using all subsystems.
In fact, a goodselection strategy is to select the subset whichhas maximum out-of-bag F1 score on the trainingdata.
This subset performs slightly better thanAll_Systems on similar data sets, and only slight-ly worse on new types of data.
Advantage of thissubset is that it requires less classifiers (7 insteadof 12 in our case), which reduces the cost(runtime or license fees) of the meta-classifier.5 AcknowledgementsWe would like to thank all system providers forgiving us the opportunity to use their systems forthis evaluation, and especially Tobias G?ntherand Martin Jaggi for carefully reading the manu-script.ReferencesLeo Breiman.
2001.
Random Forests.
Machine Learn-ing 45(1), 5-32.Mark Cieliebak, Oliver D?rr, Fatih Uzdilli.
2014.
Me-ta-Classifiers Easily Improve Commercial Senti-ment Detection Tools.
In Proceedings of the 9thedition of the Language Resources and EvaluationConference (LREC), pages 3100-3104, May 26-31,2014, Reykjavik, Iceland.Pedro P. Balage Filho, Thiago A. S. Pardo.
2013.NILC USP: A Hybrid System for Sentiment Anal-ysis in Twitter Messages.
In Proceedings of the In-ternational Workshop on Semantic Evaluation(SemEval-2013), pages 568-572, June 14-15, 2013,Atlanta, Georgia, USA.Gizem Gezici, Rahim Dehkharghani, Berrin Yani-koglu, Dilek Tapucu, Yucel Saygin.
2013.
SU-Sentilab: A Classification System for SentimentAnalysis in Twitter.
In Proceedings of the Interna-tional Workshop on Semantic Evaluation(SemEval-2013), pages 471-477, June 14-15, 2013,Atlanta, Georgia, USA.Tobias G?nther, Lenz Furrer.
2013.
GU-MLT-LT:Sentiment Analysis of Short Messages using Lin-guistic Features and Stochastic Gradient Descent.In Proceedings of the International Workshop onSemantic Evaluation (SemEval-2013), pages 328-332, June 14-15, 2013, Atlanta, Georgia, USA.Martin Jaggi, Fatih Uzdilli, and Mark Cieliebak.2014.
Swiss-Chocolate: Sentiment Detection usingSparse SVMs and Part-Of-Speech n-Grams.
InProceedings of the International Workshop on Se-mantic Evaluation (SemEval-2014), August 23-24,2014, Dublin, Ireland.Morgane Marchand, Alexandru Ginsca, Romaric Be-san?on, Olivier Mesnard.
2013.
[LVIC-LIMSI]:Using Syntactic Features and Multi-polarity Wordsfor Sentiment Analysis in Twitter.
In Proceedingsof the International Workshop on Semantic Evalua-tion (SemEval-2013), pages 418-424, June 14-15,2013, Atlanta, Georgia, USA.Sara Rosenthal, Preslav Nakov, Alan Ritter, andVeselin Stoyanov.
2014.
SemEval-2014 Task 9:Sentiment Analysis in Twitter.
In Proceedings ofthe International Workshop on Semantic Evalua-tion (SemEval-2014), August 23-24, 2014, Dublin,Ireland.Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,Veselin Stoyanov, Alan Ritter, Theresa Wilson.2013.
SemEval-2013 Task 2: Sentiment Analysisin Twitter.
In Proceedings of the InternationalWorkshop on Semantic Evaluation (SemEval-2013), pages 312-320, June 14-15, 2013, Atlanta,Georgia, USA.369
