Planning Referential Actsfor Animated Presentation AgentsE l i sabeth  AndrE ,  Thomas  R is tGerman Research Center for Artificial Intell igence (DFKI)Stuhlsatzenhausweg 3, D-66123 Saarbrficken, GermanyEmail: { andre,f ist  } @dfki.uni-sb.deAbst rac tComputer-based presentation systems en-able the realization of effective and dy-namic presentation styles that incorporatemultiple media.
In particular, they allowfor the emulation of conversational stylesknown from personal human-human com-munication.
In this paper, we argue thatlife-like characters are an effective meansof encoding references to world objects ina presentation.
We present a two-phase ap-proach which first generates high-level ref-erential acts and then transforms them intofine-grained animation sequences.?
effectively establish cross-references betweenpresentation parts which are conveyed by differ-ent media possibly being displayed in differentwindows;?
enable new forms of deixis by personalizing thesystem as a situated presenter.For illustration, let's have a look at two examplepresentations taken from the PPP system (Person-alized Plan-Based Presenter, (RAM97)).
In Fig.
1, apointing esture is combined with a graphical anno-tation technique using a kind of magnifying lass.1 In t roduct ionA number of researchers have developed algorithmsin order to discriminate r ferents from alternativesvia linguistic means (cf.
(RD92)).
When movingfrom language discourse to a multimedia discourse,referring expressions may be composed of severalconstituents in different media.
Each constituentconveys ome discriminating attributes which in sumallow for a proper identification of the referent.
How-ever, to ensure that a composed referring expressionis intelligible, the system has to establish cohesivelinks between the single parts (cf.
(AR94)).In this paper, we argue that life-like charactersare particularly suitable for accomplishing referringtasks.
For example, a life-like character can:?
draw the viewer's attention to graphical objectrepresentations by pointing with body parts,and additional devices uch as a pointing stick.?
make use of facial displays and head movementsas an additional means of disambiguating dis-course references,Figure 1: Referring to Objects Using a MagnifyingGlassThe Persona provides an overview of interesting sitesin the Saarland county by uttering their names andpointing to their location on a map.
In addition,Persona annotates the map with a picture of eachsite before the user's eyes.
The advantage of thismethod over static annotations i  that the systemcan influence the temporal order in which the userprocesses an illustration.
Furthermore, space prob-lems are avoided since the illustration of the corre-sponding building disappears again after it has been68 E. Andrd and T. RistFigure 2: Establishing Cross-Media Referencesdescribed.
The example also demonstrates how fa-cial displays and head movements help to restrict hevisual focus.
By having the Persona look into the di-rection of the target object, the user's attention isdirected to the target object.Whereas in the last example, the pointing act ofthe Persona referred to a single graphical object,the scenario in Fig.
2 illustrates how cross-medialinks can be effectively built up between several il-lustrations.
In this example, the Persona informsthe user where the DFKI building is located.
It ut-ters: "DFKI is located in Saarbriicken" and uses twopointing sticks to refer to two graphical depictionsof DFKI on maps with different granularity.As shown above, life-like characters facilitate thedisambiguation of referring expressions.
On theother hand, a number of additional dependencieshave to be handled since a referring act involves notonly the coordination f document parts in differentmedia, but also the coordination of locomotion, ges-tures and facial displays.
To accomplish these tasks,we have chosen a two-phase approach which involvesthe following steps:(1) the creation of a script that specifies the tempo-ral behavior of the constituents of a referentialact, such as speaking and pointing(2) the context-sensitive conversion of these con-stituents into animation sequences2 Representation of the Mul t imediaDiscourseA few researchers have already addressed referringacts executed by life-like characters in a virtual 3Denvironment (cf.
(CPB+94; LVTC97)).
In this case,the character may refer to virtual objects in thesame way as a human will do in a real environmentwith direct access to the objects.
A different situa-tion occurs when a character interacts with objectsvia their presentations as in the example scenariosabove.
Here, we have to explicitly distinguish be-tween domain objects and document objects.
First,there may be more than one representative for oneand the same world object in a presentation.
For ex-ample, in Fig.
2, DFKI is represented by a schematicdrawing and a colored polygon.
Furthermore, itmakes a difference whether a system refers to fea-tures of an object in the domain or in the presen-tation since these features may conflict with eachother.
To enable r ferences to objects in a presenta-tion, we have to explicitly represent how the systemhas encoded information.
For instance, to generatea cross-media reference as in Fig.
2, the system hasto know which images are encodings for DFKI.
In-spired by (Mac86), we use a relation tuple of theform:(Encodes carrier info context-space)to specify the semantic relationship between a pre-sentation means, and the information the means isto convey in a certain context space (cf.
(AR94)).In our approach, the third argument refers to thecontext space to which the encoding relation cor-responds to and not to a graphical anguage as inthe original Mackinlay approach.
This enables usto use one and the same presentation means differ-ently in different context spaces.
For example, thezoom inset in Fig.
1 is used as a graphical encodingof the DFKI building in the current context space,but may serve in another context as a representativebuilding of a certain architectual style.
In addition,we not only specify encoding relations between in-dividual objects, but also specify encoding relationson a generic level (e.g., that the property of being ared polygon on a map encodes the property of beingPlanning Referential Acts for  Animated Presentation Agents 69mmmnmmmmmmmmmmmmmmmm\[\]ma built-up area in the world).Furthermore, we have to explicitly represent thePersona's current state since it influences both thecontents and the form of a referring expression.
Forinstance, the applicability of deictic spatial expres-sions, such as "on my left", depends on the Persona'scurrent position.3 H igh leve l  P lann ing  o f  Re ferent ia lAc tsFollowing a speech-act heoretic perspective, weconsider referring as a goal-directed activity (cf.(AK87)).
The goal underlying a referring expres-sion is to make the user activate appropriate mentalrepresentations i  the sense of picking them out of aset of representations which are already available orwhich have to be built up (e.g., by localizing an ob-ject in a user's visual field).
To plan referential actswhich accomplish such goals, we build upon our pre-vious work on multimedia presentation design (cf.(AR96)).
The main idea behind this approach wasto formalize action sequences for designing presenta-tion scripts as operators of a planning system.
Start-ing from a complex communicative goal, the plannertries to find a presentation strategy which matchesthis goal and generates a refinement-style plan in theform of a directed acyclic graph (DAG).
This planreflects not only the rhetorical structure, but alsothe temporal behavior of a presentation by means ofqualitative and metric constraints.
Qualitative con-straints are represented in an "Allen-style" fashion(cf.
(All83)) which allows for the specification ofthirteen temporal relationships between two namedintervals, e.g.
(Speak1 (During) PointP).
Quantita-tive constraints appear as metric (in)equalities, e.g.
(5 < Duration Point2).
While the top of the pre-sentation plan is a more or less complex presentationgoal (e.g., instructing the user in switching on a de-vice), the lowest level is formed by elementary pro-duction (e.g., to create an illustration or to encode areferring expression) and presentation acts (e.g., todisplay an illustration, to utter a verbal reference orto point to an object).If the presentation planner decides that a referenceto an object should be made, it selects a strategyfor activating a mental representation f this object.These strategies incorporate knowledge concerning:?
the attributes to be selected for referent disam-biguationTo discriminate objects from alternatives, thesystem may refer not only to features of an ob-ject in a scene, but also to features of the pre-sentation model, their interpretation and to theposition of objects within a presentation, seealso (Waz92).?
the determination ofan appropriate media com-binationTo discriminate an object against its alterna-tives through visual attributes, such as shapeor surface, or its location, illustrations are used.Pointing gestures are planned to disambiguateor simplify a referring expression or to establisha coreferential relationship to other documentparts.?
the temporal coordination of the constituents ofa referential actIf a referrring expression is composed of severalconstituents of different media, they have to besynchronized in an appropriate manner.
For in-stance, a pointing gesture should be executedwhile the corresponding verbal part of the re-ferring expression is uttered.After the planning process is completed, the sys-tem builds up a schedule for the presentation whichspecifies the temporal behavior of all production andpresentation acts.
To accomplish this task, the sys-tem first builds up a temporal constraint network bycollecting all temporal constraints on and betweenthe actions.
Some of these constraints are given bythe applied plan operators.
Others result from lin-earization constraints of the natural-langnage gener-ator.For illustration, let's assume the presentationplanner has built up the following speech and point-ing acts:AI :  (S-Speak Persona User (type pushtomodus (def imp tense pres number sg)))A2: (S-Speak Persona User(theagent ( ype individualthediscourserole(type discourserole value hearer)modus(def the ref pro number sg))))A3: (S-Speak Persona User(theobject (type taskobjectthetaskobject(type namedobjectthename S-4theclass(type class value on-off-switch)))))A4: (S-Speak Persona User(thegoal (type destthedest (type destloc value right))))AS: (S-Point PersonaUser image-on-off-switch-1 window-3)At this time decisions concerning word orderingsare not yet made.
The only temporal constraints70 E. Andrg and T. Ristwhich have been set up by the planner are: (AS(During) A3).
That is the Persona has to point toan object while the object's name and type is utteredverbally.The act specifications A1 to A4 are forwarded tothe natural-language neration component wheregrammatical encoding, linearization and inflectiontakes place.
This component generates: "Push theon/off switch to the right".
That is, during text gen-eration we get the following additional constraints:(A1 (meets) A3), (A3 (meets) A~).
1After collecting all constraints, the system de-termines the transitive closure over all qualitativeconstraints and computes numeric ranges over in-terval endpoints and their difference.
Finally, aschedule is built up by resolving all disjunctionsand computing a total temporal order (see (AR96)).Among other things, disjunctions may result fromdifferent correct word orderings, such as "Pressthe on/off switch now."
versus "Now, press theon/off switch."
In this case, the temporal con-straint network would contain the following con-straints: (Or (S-Speak-Now (Meets) S-Speak-Press)(S-Speak-Switch (Meets) S-Speak-Now)), (S-Speak-Press (Meets) S-Speak-Switch), (S-Point (During)S-Speak-Switch).
For these constraints, the systemwould build up the following schedules:Schedule 11: Start S-Speak-Now2: Start S-Speak-Press, End S-Speak-Now3: Start S-Speak-Switch, End S-Speak-Press4: Start S-Point5: End S-Point6: End S-Speak-SwitchSchedule 21: Start S-Speak-Press2: Start S-Speak-Switch, End S-Speak-Press3: Start S-Point4: End S-Point5: Start S-Speak-Now, End S-Speak-Switch6: End S-NowSince it is usually difficult to anticipate at designtime the exact durations of speech acts, the systemjust builds up a partial schedule which reflects theordering of the acts.
This schedule is refined at pre-sentation display time by adding new metric con-straints concerning the duration of speech acts tothe temporal constraint network.4 Context -sens i t ive  Ref inement  o fRe ferent ia l  Ac tsThe presentation scripts generated by the presen-tation planner are forwarded to the Persona Server1Note that we don't get any temporal constraints forA2 since it is not realized on the surface level.which converts them into fine-grained animations.Since the basic actions the Persona has to performdepend on its current state, complex dependencieshave to be considered when creating of animation se-quences.
To choose among different start positionsand courses of pointing gestures (see Fig.
3), weconsider the following criteria:Figure 3: Different Pointing Gestures- the position o/the Persona relative to the targetobject;If the Persona is too far away from the targetobject, it has to walk to it or use a tele-scopepointing stick.
In case the target object is lo-cated behind the Persona, the Persona has toturn around.
To determine the direction of thepointing gesture, the system considers the ori-entation of the vector from the Persona to thetarget object.
For example, if the target objectis located on the right of the Persona's rightfoot, the Persona has to point down and to theright.- the set of adjacent objects and the size of thetarget object;To avoid ambiguities and occlusions, the Per-sona may have to use a pointing stick.
On theother hand, it may point to isolated and largeobjects just with a hand.- the current screen layout;If there are regions which must not be occludedby the Persona, the Persona might not be ableto move closer to the target object and mayhave to use a pointing stick instead.- the expected length of a verbal explanation thataccompanies the pointing gesture;If the Persona intends to provide a longer verbalexplanation, it should move to the target objectand turn to the user (as in the upper row inFig.
3).
In case the verbal explanation is veryshort, the Persona should remain stationary ifpossible.Planning Referential Acts for Animated Presentation Agents 71High-Level I s-point\[t~ t2 IPersona  Act ionsI take-position\[t~ t2\]l I start-point\[t2 t~ I lend-p?int\[t~ t,0 IContext-Sensitive I m?ve't?\[t' t~\]l I r-stick-point\[t2 tJ I,on.,onDecompos i t ion  into , .
, .
.~r ,  , , , ~ \ ~,Un ln ter  r Jp tab le  I , - .u, , ,u,  "2,J I l, \ ~ .
k~.
,~_,~r?
+,  ~ X .Bas ic  Postures / ' ~- I "-' ,u, ,u-,,:, u2 ,3~J I / I r-ste?
(t2, t ' ) l "h  I Ir-stick-expose\[t~,Frames , / ~ / , , $ , , ~ ,f-toroCt  t,) I ',Pix aps) ~ ~  ~ ~ ~ ~ ~ .~Figure 4: Context-Sensitive D composition of a Pointing Gesture- the remaining overall presentation time.While the default strategy is to move the Per-sona towards the target object, time shortagewill make the Persona use a pointing stick in-stead.To support he definition of Persona ctions, wehave defined adeclarative specification language andimplemented a multi-pass compiler that enables theautomated generation of finite-state automata fromthese declarations.
These fine-state automata inturn are translated into efficient machine code (cf.(RAM97)).Fig.
4 shows a context-sensitive decomposition fa pointing act delivered by the presentation plannerinto an animation sequence.
Since in our case theobject he Persona has to point to is too far away, thePersona first has to perform an navigation act beforethe pointing gesture may start.
We associate witheach action a time interval in which the action takesplace.
For example, the act take-position has to beexecuted uring (tl t2).
The same applies to themove-to act, the specialization of take-position.
Theintervals associated with the subactions of move-toare subintervals of (tl t2) and form a sequence.
Thatis the Persona first has to turn to the right during(tl t21), then take some steps during (t21 t22) andfinally turn to the front during (t22 t2).
Note thatthe exact length of all time intervals can only bedetermined at runtime.5 ConclusionIn this paper, we have argued that the use of life-likecharacters in the interface can essentially increasethe effectiveness of referrring expressions.
We havepresented an approach for the automated planningof referring expressions which may involve differentmedia and dedicated body movements of the char-acter.
While content selection and media choice areperformed in a proactive planning phase, the trans-formation of referential cts into fine-grained anima-tion sequences is done reactively taking into accountthe current situation of the character at presentationruntime.The approach presented here provides a goodstarting point for further extensions.
Possible di-rections include:?
Extending the repertoire of pointing gesturesCurrently, the Persona only supports punctualpointing with a hand or a stick.
In the fu-ture, we will investigate additional pointing es-tures, such as encircling and underlining, by ex-ploiting the results from the XTRA project (cf.
(Rei92)).72 E. Andrd and T. Rist?
Spatial deixisThe applicability of spatial prepositions, suchas "on the left", depends on the orientation ofthe space which is either given by the intrinsicorganization of the reference object or the loca-tion of the observer (see e.g.
(Wun85)).
Whilewe assumed in our previous work on the seman-tics of spatial prepositions that the user's loca-tion coincides with the presenter's location (cf.
(Waz92)), we now have to distinguish whetheran object is localized from the user's point ofview or the Persona's point of view as the situ-ated presenter.?
Referring to moving target objectsA still unsolved problem results from the dy-namic nature of online presentations.
Since im-age attributes may change at any time, the vi-sual focus has to be updated continuously whichmay be very time-consuming.
For instance, thePersona is currently not able to point to movingobjects in an animation sequence since there issimply not enough time to determine an object'scoordinates at presentation time.?
Empirical evaluation of the Persona's pointinggesturesWe have argued that the use a life-like char-acter enables the realization of more effectivereferring expressions.
To empirically validatethis hypothesis, we are currently embarking ona study of the user's reference resolution pro-cesses with and without the Persona.AcknowledgmentsThis work has been supported by the BMBF underthe grants ITW 9400 7 and 9701 0.
We would liketo thank Jochen Mfiller for his work on the Personaserver and the overall system integration.Re ferencesD.
Appelt and A. Kronfeld.
A computational modelof referring.
In Proc.
of the I0 th HCAI, pages640--647, Milan, Italy, 1987.J.
F. Allen.
Maintaining Knowledge about Tem-poral Intervals.
Communications of the A CM,26(11):832-843, 1983.E.
Andre and T. Rist.
Referring to World Ob-jects with Text and Pictures.
In Proc.
of the15 th COLING, volume 1, pages 530-534, Kyoto,Japan, 1994.E.
Andr~ and T. Rist.
Coping with temporal con-straints in multimedia presentation planning.
InProc.
off AAAI-96, volume 1, pages 142-147, Port-land, Oregon, 1996.J.
Cassell, C. Pelachaud, N.I.
Badler, M. Steedman,B.
Achorn, T. Becket, B. Douville, S. Prevost, andM.
Stone.
Animated conversation: Rule-basedgeneration of facial expression,gesture and spokenintonation for multiple conversational gents.
InProc.
of Siggraph'94, Orlando, 1994.J.
Lester, J.L.
Voerman, S.G.
Towns, and C.B.
Call-away.
Cosmo: A life-like animated pedagogi-cal agent with deictic believability.
In Proc.
ofthe IJCAI-97 Workshop on Animated InterfaceAgents: Making them Intelligent, Nagoya, 1997.J.
Mackinlay.
Automating the Design of Graphi-cal Presentations ofRelational Information.
ACMTransactions on Graphics, 5(2):110-141, April1986.T.
Rist, E. AndrE, and J. Mfiller.
Adding AnimatedPresentation Agents to the Interface.
In Proceed-ings of the 1997 International Conference on In-telligent User Interfaces, pages 79-86, Orlando,Florida, 1997.E.
Reiter and R. Dale.
A Fast Algorithm for theGeneration of Referring Expressions.
In Proc.of the 14 th COLING, volume 1, pages 232-238,Nantes, France, 1992.N.
Reithinger.
The Performance of an Incremen-tal Generation Component for Multi-Modal Dia-log Contributions.
In R. Dale, E. Hovy, D. RSsner,and O.
Stock, editors, Aspects of Automated Nat-ural Language Generation: Proceedings of the6th International Workshop on Natural LanguageGeneration, pages 263-276.
Springer, Berlin, Hei-delberg, 1992.P.
Wazinski.
Generating Spatial Descriptions forCross-Modal References.
In Proceedings of theThird Conference on Applied Natural LanguageProcessing, pages 56-63, Trento, Italy, 1992.D.
Wunderlich.
Raumkonzepte.
Zur Semantik derlokaien Pr~ipositionen.
In T.T.
Ballmer andR.
Posener, editors, Nach-Chomskysche Linguis-tik, pages 340--351. de Gruyter, Berlin, New York,1985.
