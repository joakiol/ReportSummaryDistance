Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 115?119,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsArguments and Modifiers from the Learner?s PerspectiveLeon BergenMITBrain and Cognitive Sciencebergen@mit.eduEdward GibsonMITBrain and Cognitive Scienceegibson@mit.eduTimothy J. O?DonnellMITBrain and Cognitive Sciencetimod@mit.eduAbstractWe present a model for inducing sen-tential argument structure, which distin-guishes arguments from optional modi-fiers.
We use this model to study whetherrepresenting an argument/modifier distinc-tion helps in learning argument structure,and whether a linguistically-natural argu-ment/modifier distinction can be inducedfrom distributional data alone.
Our resultsprovide evidence for both hypotheses.1 IntroductionA fundamental challenge facing the languagelearner is to determine the content and structureof the stored units in the lexicon.
This problem ismade more difficult by the fact that many lexicalunits have argument structure.
Consider the verbput.
The sentence, John put the socks is incom-plete; when hearing such an utterance, a speakerof English will expect a location to also be speci-fied: John put the socks in the drawer.
Facts suchas these can be captured if the lexical entry for putalso specifies that the verb has three required ar-guments: (i) who is doing the putting (ii) what isbeing put (iii) and the destination of the putting.The problem of acquiring argument structure isfurther complicated by the fact that not all phrasesin a sentence fill an argument role.
Instead, manyare modifiers.
Consider the sentence John put thesocks in the drawer at 5 o?clock.
The phrase at5 o?clock occurs here with the verb put, but it isnot an argument.
Removing this phrase does notchange the core structure of the PUTTING event,nor is the sentence incomplete without this phrase.The distinction between arguments and mod-ifiers has a long history in traditional grammarand is leveraged in many modern theories of syn-tax (Haegeman, 1994; Steedman, 2001; Sag etal., 2003).
Despite the ubiquity of the distinc-SNPJohnVPVputNPthe socksPPin the drawerPPat 5 o?clockSNPJohnVPVputNPthe socksPPin the drawerFigure 1: The VP?s in these sentences only sharestructure if we separate arguments from modifiers.tion in syntax, however, there is a lack of consen-sus on the necessary and sufficient conditions forargumenthood (Schu?tze, 1995; Schu?tze and Gib-son, 1999).
It remains unclear whether the argu-ment/modifier distinction is purely semantic or isalso represented in syntax, whether it is binary orgraded, and what effects argument/modifierhoodhave on the distribution of linguistic forms.In this work, we take a new approach to theseproblems.
We propose that the argument/modifierdistinction is inferred on a phrase?by?phrase basisusing probabilistic inference.
Crucially, allowingthe learner to separate the core argument structureof phrases from peripheral modifier content in-creases the generalizability of argument construc-tions.
For example, the two sentences in Figure 1intuitively share the same argument structures, butthis overlap can only be identified if the preposi-tional phrase, ?at 5 o?clock,?
is treated as a modi-fier.
Thus representing the argument/modifier dis-tinction can help the learner find useful argumentstructures which generalize robustly.Although, like the majority of theorists, weagree that the argument/adjunct distinction is fun-damentally semantic, in this work we focus on itsdistributional correlates.
Does the optionality ofmodifier phrases help the learner acquire lexicalitems with the right argument structure?2 ApproachWe adopt an approach where the lexicon consistsof an inventory of stored tree fragments.
These115tree fragments encode the necessary phrase types(i.e., arguments) that must be present in a struc-ture before it is complete.
In this system, sen-tences are generated by recursive substitution oftree fragments at the frontier argument nodes ofother tree fragments.
This approach extends workon learning probabilistic Tree?Substitution Gram-mars (TSGs) (Post and Gildea, 2009; Cohn et al,2010; O?Donnell, 2011; O?Donnell et al, 2011).1To model modification, we introduce a secondstructure?building operation, adjunction.
Whilesubstitution must be licensed by the existenceof an argument node, adjunction can insert con-stituents into well?formed trees.
Many syntactictheories have made use of an adjunction operationto model modification.
Here, we adopt the variantknown as sister?adjunction (Rambow et al, 1995;Chiang and Bikel, 2002) which can insert a con-stituent as the sister to any node in an existing tree.In order to derive the complete tree for a sen-tence, starting from an S root node, we recursivelysample arguments and modifiers as follows.2 Forevery nonterminal node on the frontier of ourderivation, we sample an elementary tree from ourlexicon to substitute into this node.
As alreadynoted, these elementary trees represent the argu-ment structure of our tree.
Then, for each argu-ment nonterminal on the tree?s interior, we sister?adjoin one or more modifier nodes, which them-selves are built by the same recursive process.Figure 2 illustrates two derivations of thesame tree, one in standard TSG without sister?adjunction, and one in our model.
In the TSGderivation, at top, an elementary tree with four ar-guments ?
including the intuitively optional tem-poral PP ?
is used as the backbone for the deriva-tion.
The four phrases filling these argumentsare then substituted into the elementary tree, asindicated by arrows.
In the bottom derivation,which uses sister?adjunction, an elementary treewith only three arguments is used as the back-bone.
While the right-most temporal PP neededto be an argument of the elementary tree in theTSG derivation, the bottom derivation uses sister?adjunction to insert this PP as a child of the VP.Sister?adjunction therefore allows us to use an ar-1Note that we depart from many discussions of argumentstructure in that we do not require that every stored fragmenthas a head word.
In effect, we allow completely abstractphrasal constructions to also have argument structures.2Our generative model is related to the generative modelfor Tree?Adjoining Grammars proposed in (Chiang, 2000)SNPJohnVPVputNPthe socksPPin the drawerPPat 5 o?clockNPJohnNPthe socksPPin the drawerPPat 5 o?clockSNPJohnVPVputNPthe socksPPin the drawerPPat 5 o?clockNPJohnNPthe socksPPat 5 o?clockPPin the drawerFigure 2: The first part of the figure shows howto derive the tree in TSG, while the second partshows how to use sister-adjunction to derive thesame tree in our model.gument structure that matches the true argumentstructure of the verb ?put.
?This figure illustrates how derivations in ourmodel can have a greater degree of generalizabil-ity than those in a standard TSG.
Sister?adjunctionwill be used to derive children which are not partof the core argument structure, meaning that agreater variety of structures can be derived by acombination of common argument structures andsister-adjoined modifiers.
Importantly, this makesthe learning problem for our model less sparsethan for TSGs; our model can derive the trees in acorpus using fewer types of elementary trees thana TSG.
As a result, the distribution over these ele-mentary trees is easier to estimate.To understand what role modifiers play duringlearning, we will develop a learning model thatcan induce the lexicon and modifier contexts usedby our generative model.3 ModelOur model extends earlier work on inductionof Bayesian TSGs (Post and Gildea, 2009;O?Donnell, 2011; Cohn et al, 2010).
The modeluses a Bayesian non?parametric distribution?thePitman-Yor Process, to place a prior over the lex-icon of elementary trees.
This distribution allowsthe complexity of the lexicon to grow to arbitrarysize with the input, while still enforcing a bias formore compact lexicons.116For each nonterminal c, we define:Gc|ac, bc, PE ?
PYP(ac, bc, PE(?|c)) (1)e|c,Gc ?
Gc, (2)where PE(?|c) is a context free distribution overelementary trees rooted at c, and e is an elementarytree.The context-free distribution over elementarytrees PE(e|c) is defined by:PE(e|c) =?i?I(e)(1?sci)?f?F (e)scf?c????ePc?(?|c?
),(3)where I(e) is the set of internal nodes in e, F (e) isthe set of frontier nodes, ci is the nonterminal cat-egory associated with node i, and sc is the proba-bility that we stop expanding at a node c. For thispaper, the parameters sc are set to 0.5.In addition to defining a distribution over ele-mentary trees, we also define a distribution whichgoverns modification via sister?adjunction.
Tosample a modifier, we first decide whether or notto sister?adjoin into location l in a tree.
Followingthis step, we sample a modifier category (e.g., aPP) conditioned on the location l?s context: its par-ent and left siblings.
Because contexts are sparse,we use a backoff scheme based on hierarchicalDirichlet processes similar to the ngram backoffschemes defined in (Teh, 2006; Goldwater et al,2006).
Let c be a nonterminal node in a tree de-rived by substitution into argument positions.
Thenode c will have n ?
1 children derived by ar-gument substitution: d0, ..., dn.
In order to sister?adjoin between two of these children di, di+1, werecursively sample nonterminals si,1, ..., si,k untilwe hit a STOP symbol:Pa(si,1, ..., si,k, STOP |C0) (4)=k?j=1Pa(si,j |Cj) ?
(1?
PCj (STOP ))?
PCk+1(STOP )where Cj = d1, s1,1, ..., di, si,1, ..., si,j?1, c is thecontext for the j?th modifier between these chil-dren.
The distribution over sister?adjoined non-terminals is defined using a hierarchical Dirichletprocess to implement backoff in a prefix tree overcontexts.
We define the distribution G(ql, ..., q1)over sister?adjoined nonterminals si,j given thecontext ql, ..., q1 by:G(ql, ..., q1) ?
DP(?,G(ql?1, ..., q1)).
(5)The distribution G at the root of the hierarchy isnot conditioned on any prior context.
We define Gby:G ?
DP(?,Multinomial(m)) (6)where m is a vector with entries for each nonter-minal, and where we samplem ?
Dir(1,...,1).To perform inference, we developed a localGibbs sampler which generalizes the one proposedby (Cohn et al, 2010).4 ResultsWe evaluate our model in two ways.
First,we examine whether representing the argu-ment/modifier distinction increases the ability ofthe model to learn highly generalizable elemen-tary trees that can be used as argument structuresacross a variety of sentences.
Second, we askwhether our model is able to induce the correctargument/modifier distinction according to a lin-guistic gold?standard.
We trained our model onsections 2?21 of the WSJ part of the Penn Tree-bank (Marcus et al, 1999).
The model was trainedon the trees in this corpus, without any further an-notations for substitution or modification.To address the first question, we compared thestructure of the grammar learned by our model toa grammar learned by a version of our model with-out sister?adjunction (i.e., a TSG similar to theone used in Cohn et al).
Our model should findmore common structure among the trees in the in-put corpus, and therefore it should learn a set of el-ementary trees which are more complex and morewidely shared across sentences.
We evaluated thishypothesis by analyzing the average complexityof the most probable elementary trees learned bythese models.
As Table 1 shows, our model dis-covers elementary trees that have greater depthand more nodes than those found by the TSG.
Inaddition, our model accounts for a larger portionof the corpus with fewer rules: the top 50, 100, and200 most common elementary trees in our model?slexicon account for a greater portion of the corpusthan the corresponding sets in the TSG.Figure 3 illustrates a representative examplefrom the corpus.
By using sister-adjuntion to sepa-rate the ADVP node from the rest of the sentence?sderivation, our model was able to use a commondepth-3 elementary tree to derive the backbone ofthe sentence.
In contrast, the TSG cannot give thesame derivation, as it needs to include the ADVP117SNPMost of those who left stock fundsADVPsimplyVPVPVBDswitchedPPinto money market fundsPPinto money market fundsVBDswitchedNPMost of those who left stock fundsFigure 3: Part of a derivation found by our model.Model Rank Avg treedepthAvg treesize#TokensModifier 50 1.59 3.42 97282TSG 50 1.38 2.98 88023Modifier 100 1.84 3.98 134205TSG 100 1.58 3.38 116404Modifier 200 1.97 4.27 170524TSG 200 1.77 3.84 146040Table 1: This table shows the average depth andnode count for elementary trees in our model andthe TSG.
The results are shown for the 50, 100,and 200 most frequent types of elementary trees.node in the elementary tree; this wider elementarytree is much less common in the corpus.We next examined whether our model learnedto correctly identify modifiers in the corpus.
Un-fortunately, marking for argument/modifiers in thePenn Treebank is incomplete, and is limited tocertain adverbials, e.g.
locative and temporalPP?s.
To supplement this markup, we made use ofthe corpus of (Kaeshammer and Demberg, 2012).This corpus adds annotations indicating, for eachnode in the Penn Treebank, whether that node isa modifier.
This corpus was compiled by com-bining information from Propbank (Palmer et al,2005) with a set of heuristics, as well as the NP-branching structures proposed in (Vadas and Cur-ran, 2007).
It is important to note that this corpuscan only serve as a rough benchmark for evalua-tion of our model, as the heuristics used in its de-velopment did not always follow the correct lin-guistic analysis; the corpus was originally con-structed for an alternative application in compu-tational linguistics, for which non?linguistically?natural analyses were sometimes convenient.
Ourmodel was trained on this corpus, after it had beenstripped of argument/modifier annotations.We compare our model?s performance to a ran-dom baseline.
Our model constrains every non-terminal to have at least one argument child, andour Gibbs sampler initializes argument/modifierchoices randomly subject to this constraint.
WeModel Precision Recall #Guessed #CorrectRandom 0.27 0.19 298394 82702Modifier 0.62 0.15 108382 67516Table 2: This table shows precision and recall inidentifying modifier nodes in the corpus.therefore calculated the probability that a nodethat was randomly initialized as a modifier was infact a modifier, i.e.
the precision of random ini-tialization.
Next, we looked at the precision ofour model following training.
Table 2 shows thatamong nodes that were labeled as modifiers, 0.27were labeled correctly before training and 0.62were labeled correctly after.
This table also showsthe recall performance for our model decreased by0.04.
Some of this decrease is due to limitations ofthe gold standard; for example, our model learnsto classify infinitives and auxiliary verbs as argu-ments ?
consistent with standard linguistic anal-yses ?
whereas the gold standard classifies theseas modifiers.
Future work will investigate how themetric used for evaluation can be improved.5 SummaryWe have investigated the role of the argu-ment/modifier distinction in learning.
We firstlooked at whether introducing this distinctionhelps in generalizing from an input corpus.Our model, which represents modification usingsister?adjunction, learns a richer lexicon than amodel without modification, and its lexicon pro-vides a more compact representation of the in-put corpus.
We next looked at whether the tra-ditional linguistic classification of arguments andmodifiers can be induced from distributional in-formation.
Without supervision from the correctlabelings of modifiers, our model learned to iden-tify modifiers more accurately than chance.
Thissuggests that although the argument/modifier dis-tinction is traditionally drawn without reference todistributional properties, the distributional corre-lates of this distinction are sufficient to partiallyreconstruct it from a corpus.
Taken together, theseresults suggest that representing the difference be-tween arguments and modifiers may make it easierto acquire a language?s argument structure.AcknowledgmentsWe thank Vera Demberg for providing the goldstandard, and Tom Wasow for helpful comments.118ReferencesDavid Chiang and Daniel Bikel.
2002.
Recoveringlatent information in treebanks.
In Proceedings ofCOLING 2002.David Chiang.
2000.
Staistical parsing with anautomatically?extracted tree adjoining grammar.
InProceedings of the 38th Annual Meeting of the Asso-ciation for Computational Linguistics.
Associationfor Computational Linguistics.Trevor Cohn, Phil Blunsom, and Sharon Goldwater.2010.
Inducing tree?substitution grammars.
Jour-nal of Machine Learning Research, 11:3053?3096.Sharon Goldwater, Thomas L. Griffiths, and MarkJohnson.
2006.
Interpolating between types and to-kens by estimating power?law generators.
In Ad-vances in Neural Information Processing Systems18, Cambridge, Ma.
MIT Press.Liliane Haegeman.
1994.
Government & Binding The-ory.
Blackwell.Mirian Kaeshammer and Vera Demberg.
2012.
Ger-man and English treebanks and lexica for tree?adjoining grammars.
In Proceedings of the Interna-tional Conference on Language Resources and Eval-uation (LREC 2012).Mitchell P. Marcus, Beatrice Santorini, Mary AnnMarcinkiewicz, and Ann Taylor.
1999.
Treebank?3.
Technical report, Linguistic Data Consortium,Philadelphia.Timothy J. O?Donnell, Jesse Snedeker, Joshua B.Tenenbaum, and Noah D. Goodman.
2011.
Pro-ductivity and reuse in language.
In Proceedings ofthe 33rd Annual Conference of the Cognitive ScienceSociety.Timothy J. O?Donnell.
2011.
Productivity and Reusein Language.
Ph.D. thesis, Harvard University.Martha Palmer, P. Kingsbury, and Daniel Gildea.
2005.The proposition bank.
Computational Linguistics,31(1):71?106.Matt Post and Daniel Gildea.
2009.
Bayesian learningof a tree substitution grammar.
In Proceedings of theJoint Conference of the 47th Annual Meeting of theACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP.Owen Rambow, K. Vijay-Shanker, and David Weir.1995.
D?tree grammars.
In Proceedings of the33rd annual meeting of the Association for Compu-tational Linguistics.
Association for ComputationalLinguistics.Ivan A.
Sag, Thomas Wasow, and Emily M. Bender.2003.
Syntactic Theory: A Formal Introduction.CSLI, Stanford, CA, 2 edition.Carson T Schu?tze and Edward Gibson.
1999.
Ar-gumenthood and english prepositional phrase at-tachment.
Journal of Memory and Language,40(3):409?431.Carson T. Schu?tze.
1995.
PP attachment and argu-menthood.
Technical report, Papers on languageprocessing and acquisition, MIT working papers inlinguistics, Cambridge, Ma.Mark Steedman.
2001.
The syntactic process.
TheMIT press.Yee Whye Teh.
2006.
A Bayesian interpretation of in-terpolated Kneser-Ney.
Technical Report TRA2/06,National University of Singapore, School of Com-puting.David Vadas and James Curran.
2007.
Adding nounphrase structure to the penn treebank.
In Proceed-ings of the 45th annual meeting of the Associa-tion for Computational Linguistics.
Association forComputational Linguistics.119
