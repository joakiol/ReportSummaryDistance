Proceedings of the Workshop on Statistical Machine Translation, pages 134?137,New York City, June 2006. c?2006 Association for Computational LinguisticsPORTAGE: with Smoothed Phrase Tablesand Segment Choice ModelsHoward JohnsonNational Research CouncilInstitute for Information TechnologyInteractive Information1200 Montreal RoadOttawa, ON, Canada K1A 0R6Howard.Johnson@cnrc-nrc.gc.caFatiha Sadat, George Foster, Roland Kuhn,Michel Simard, Eric Joanis and Samuel LarkinNational Research CouncilInstitute for Information TechnologyInteractive Language Technologies101 St-Jean-Bosco StreetGatineau, QC, Canada K1A 0R6firstname.lastname@cnrc-nrc.gc.caAbstractImprovements to Portage and its partici-pation in the shared task of NAACL 2006Workshop on Statistical Machine Trans-lation are described.
Promising ideas inphrase table smoothing and global dis-tortion using feature-rich models are dis-cussed as well as numerous improvementsin the software base.1 IntroductionThe statistical machine translation system Portage isparticipating in the NAACL 2006 Workshop on Sta-tistical Machine Translation.
This is a good opportu-nity to do benchmarking against a publicly availabledata set and explore the benefits of a number of re-cently added features.Section 2 describes the changes that have beenmade to Portage in the past year that affect the par-ticipation in the 2006 shared task.
Section 3 outlinesthe methods employed for this task and extensionsof it.
In Section 4 the results are summarized in tab-ular form.
Following these, there is a conclusionssection that highlights what can be gleaned of valuefrom these results.2 PortageBecause this is the second participation of Portage insuch a shared task, a description of the base systemcan be found elsewhere (Sadat et al 2005).
Briefly,Portage is a research vehicle and development pro-totype system exploiting the state-of-the-art in sta-tistical machine translation (SMT).
It uses a custombuilt decoder followed by a rescoring module thatadjusts weights based on a number of features de-fined on the source sentence.
We will devote spaceto discussing changes made since the 2005 sharedtask.2.1 Phrase-Table SmoothingPhrase-based SMT relies on conditional distribu-tions p(s|t) and p(t|s) that are derived from the jointfrequencies c(s, t) of source/target phrase pairs ob-served in an aligned parallel corpus.
Traditionally,relative-frequency estimation is used to derive con-ditional distributions, ie p(s|t) = c(s, t)/?s c(s, t).However, relative-frequency estimation has thewell-known problem of favouring rare events.
Forinstance, any phrase pair whose constituents occuronly once in the corpus will be assigned a probabil-ity of 1, almost certainly higher than the probabili-ties of pairs for which much more evidence exists.During translation, rare pairs can directly competewith overlapping frequent pairs, so overestimatingtheir probabilities can significantly degrade perfor-mance.To address this problem, we implemented twosimple smoothing strategies.
The first is based onthe Good-Turing technique as described in (Churchand Gale, 1991).
This replaces each observed jointfrequency c with cg = (c + 1)nc+1/nc, where ncis the number of distinct pairs with frequency c(smoothed for large c).
It also assigns a total countmass of n1 to unseen pairs, which we distributedin proportion to the frequency of each conditioning134phrase.
The resulting estimates are:pg(s|t) =cg(s, t)?s cg(s, t) + p(t)n1,where p(t) = c(t)/?t c(t).
The estimates forpg(t|s) are analogous.The second strategy is Kneser-Ney smoothing(Kneser and Ney, 1995), using the interpolated vari-ant described in (Chen and Goodman., 1998):1pk(s|t) =c(s, t) ?
D + D n1+(?, t) pk(s)?s c(s, t)where D = n1/(n1 + 2n2), n1+(?, t) is the num-ber of distinct phrases s with which t co-occurs, andpk(s) = n1+(s, ?
)/?s n1+(s, ?
), with n1+(s, ?
)analogous to n1+(?, t).Our approach to phrase-table smoothing contraststo previous work (Zens and Ney, 2004) in whichsmoothed phrase probabilities are constructed fromword-pair probabilities and combined in a log-linearmodel with an unsmoothed phrase-table.
We believethe two approaches are complementary, so a combi-nation of both would be worth exploring in futurework.2.2 Feature-Rich DT-based distortionIn a recent paper (Kuhn et al 2006), we presented anew class of probabilistic ?Segment ChoiceModels?
(SCMs) for distortion in phrase-based systems.
Insome situations, SCMs will assign a better distortionscore to a drastic reordering of the source sentencethan to no reordering; in this, SCMs differ from theconventional penalty-based distortion, which alwaysfavours less rather than more distortion.We developed a particular kind of SCM based ondecision trees (DTs) containing both questions of apositional type (e.g., questions about the distanceof a given phrase from the beginning of the sourcesentence or from the previously translated phrase)and word-based questions (e.g., questions about thepresence or absence of given words in a specifiedphrase).The DTs are grown on a corpus consisting ofsegment-aligned bilingual sentence pairs.
This1As for Good-Turing smoothing, this formula applies onlyto pairs s, t for which c(s, t) > 0, since these are the only onesconsidered by the decoder.segment-aligned corpus is obtained by training aphrase translation model on a large bilingual cor-pus and then using it (in conjunction with a distor-tion penalty) to carry out alignments between thephrases in the source-language sentence and thosein the corresponding target-language sentence in asecond bilingual corpus.
Typically, the first corpus(on which the phrase translation model is trained) isthe same as the second corpus (on which alignmentis carried out).
To avoid overfitting, the alignmentalgorithm is leave-one-out: statistics derived froma particular sentence pair are not used to align thatsentence pair.Note that the experiments reported in (Kuhn etal, 2006) focused on translation of Chinese into En-glish.
The interest of the experiments reported hereonWMT data was to see if the feature-rich DT-baseddistortion model could be useful for MT betweenother language pairs.3 Application to the Shared Task: Methods3.1 Restricted Resource ExerciseThe first exercise that was done is to replicate theconditions of 2005 as closely as possible to see theeffects of one year of research and development.The second exercise was to replicate all three ofthese translation exercises using the 2006 languagemodel, and to do the three exercises of translat-ing out of English into French, Spanish, and Ger-man.
This was our baseline for other studies.
Athird exercise involved modifying the generationof the phrase-table to incorporate our Good-Turingsmoothing.
All six language pairs were re-processedwith these phrase-tables.
The improvement in theresults on the devtest set were compelling.
This be-came the baseline for further work.
A fourth ex-ercise involved replacing penalty-based distortionmodelling with the feature-rich decision-tree baseddistortion modelling described above.
A fifth ex-ercise involved the use of a Kneser-Ney phrase-table smoothing algorithm as an alternative to Good-Turing.For all of these exercises, 1-best results after de-coding were calculated as well as rescoring on 1000-best lists of results using 12 feature functions (13in the case of decision-tree based distortion mod-elling).
The results submitted for the shared task135were the results of the third and fourth exerciseswhere rescoring had been applied.3.2 Open Resource ExerciseOur goal in this exercise was to conduct a com-parative study using additional training data for theFrench-English shared task.
Results of WPT 2005showed an improvement of at least 0.3 BLEU pointwhen exploiting different resources for the French-English pair of languages.
In addition to the trainingresources used in WPT 2005 for the French-Englishtask, i.e.
Europarl and Hansard, we used a bilingualdictionary, Le Grand Dictionnaire Terminologique(GDT) 2 to train translation models and the Englishside of the UN parallel corpus (LDC2004E13) totrain an English language model.
Integrating termi-nological lexicons into a statistical machine transla-tion engine is not a straightforward operation, sincewe cannot expect them to come with attached prob-abilities.
The approach we took consists on view-ing all translation candidates of each source term orphrase as equiprobable (Sadat et al 2006).In total, the data used in this second part of ourcontribution to WMT 2006 is described as follows:(1) A set of 688,031 sentences in French and En-glish extracted from the Europarl parallel corpus (2)A set of 6,056,014 sentences in French and Englishextracted from the Hansard parallel corpus, the offi-cial record of Canada?s parliamentary debates.
(3) Aset of 701,709 sentences in French and English ex-tracted from the bilingual dictionary GDT.
(4) Lan-guage models were trained on the French and En-glish parts of the Europarl and Hansard.
We usedthe provided Europarl corpus while omitting datafrom Q4/2000 (October-December), since it is re-served for development and test data.
(5) An addi-tional English language model was trained on 128million words of the UN Parallel corpus.For the supplied Europarl corpora, we relied onthe existing segmentation and tokenization, exceptfor French, which we manipulated slightly to bringinto line with our existing conventions (e.g., convert-ing l ?
an into l?
an, aujourd ?
hui into aujourd?hui).For the Hansard corpus used to supplement ourFrench-English resources, we used our own align-ment based on Moore?s algorithm, segmentation,2http://www.granddictionnaire.com/and tokenization procedures.
English preprocessingsimply included lower-casing, separating punctua-tion from words and splitting off ?s.4 ResultsThe results are shown in Table 1.
The numbersshown are BLEU scores.
The MC rows correspondto the multi-corpora results described in the open re-source exercise section above.
All other rows arefrom the restricted resource exercise.The devtest results are the scores computed be-fore the shared-task submission and were used todrive the choice of direction of the research.
Thetest results were computed after the shared-task sub-mission and serve for validation of the conclusions.We believe that our use of multiple training cor-pora as well as our re-tokenization for French andan enhanced language model resulted in our overallsuccess in the English-French translation track.
Theresults for the in-domain test data puts our group atthe top of the ranking table drawn by the organizers(first on Adequacy and fluency and third on BLEUscores).5 ConclusionBenchmarking with same language model and pa-rameters as WPT05 reproduces the results with atiny improvement.
The larger language model usedin 2006 for English yields about half a BLEU.
Good-Turing phrase table smoothing yields roughly halfa BLEU point.
Kneser-Ney phrase table smooth-ing yields between a third and half a BLEU pointmore than Good-Turing.
Decision tree based distor-tion yields a small improvement for the devtest setwhen rescoring was not used but failed to show im-provement on the test set.In summary, the results from phrase-tablesmoothing are extremely encouraging.
On the otherhand, the feature-rich decision tree distortion mod-elling requires additional work before it provides agood pay-back.
Fortunately we have some encour-aging avenues under investigation.
Clearly there ismore work needed for both of these areas.AcknowledgementsWe wish to thank Aaron Tikuisis and Denis Yuenfor important contributions to the Portage code base136Table 1: Restricted and open resource resultsfr ??
en es ??
en de ??
en en ??
fr en ??
es en ??
dedevtest: with rescoringWPT05 29.32 29.08 23.21LM-2005 29.30 29.21 23.41LM-2006 29.88 29.54 23.94 30.43 28.81 17.33GT-PTS 30.35 29.84 24.60 30.89 29.54 17.62GT-PTS+DT-dist 30.09 29.44 24.62 31.06 29.46 17.84KN-PTS 30.55 30.12 24.66 31.28 29.90 17.78MC WPT05 29.63MC 30.09 31.30MC+GT-PTS 30.75 31.37devtest: 1-best after decodingLM-2006 28.59 28.45 23.22 29.22 28.30 16.94GT-PTS 29.23 28.91 23.67 30.07 28.86 17.32GT-PTS+DT-dist 29.48 29.07 23.50 30.22 29.46 17.42KN-PTS 29.77 29.76 23.27 30.73 29.62 17.78MC WPT05 28.71MC 29.63 31.01MC+GT-PTS 29.90 31.22test: with rescoringLM-2006 26.64 28.43 21.33 28.06 28.01 15.19GT-PTS 27.19 28.95 21.91 28.60 28.83 15.38GT-PTS+DT-dist 26.84 28.56 21.84 28.56 28.59 15.45KN-PTS 27.40 29.07 21.98 28.96 29.06 15.64MC 26.95 29.12MC+GT-PTS 27.10 29.46test: 1-best after decodingLM-2006 25.35 27.25 20.46 27.20 27.18 14.60GT-PTS 25.95 28.07 21.06 27.85 27.96 15.05GT-PTS+DT-dist 25.86 28.04 20.74 27.85 27.97 14.92KN-PTS 26.83 28.66 21.36 28.62 28.71 15.42MC 26.70 28.74MC+GT-PTS 26.81 29.03and the OQLF (Office Que?be?cois de la LangueFranc?aise) for permission to use the GDT.ReferencesS.
F. Chen and J. T. Goodman.
1998.
An empiricalstudy of smoothing techniques for language modeling.Technical Report TR-10-98, Computer Science Group,Harvard University.K.
Church and W. Gale.
1991.
A comparison of the en-hanced Good-Turing and deleted estimation methodsfor estimating probabilities of English bigrams.
Com-puter speech and language, 5(1):19?54.R.
Kneser and H. Ney.
1995.
Improved backing-off form-gram language modeling.
In Proc.
InternationalConference on Acoustics, Speech, and Signal Process-ing (ICASSP) 1995, pages 181?184, Detroit, Michi-gan.
IEEE.R.
Kuhn, D. Yuen, M. Simard, G. Foster, P. Paul, E. Joa-nis and J. H. Johnson.
2006.
Segment Choice Models:Feature-Rich Models for Global Distortion in Statisti-cal Machine Translation (accepted for publication inHLT-NAACL conference, to be held June 2006).F.
Sadat, J. H. Johnson, A. Agbago, G. Foster, R. Kuhn,J.
Martin and A. Tikuisis.
2005.
PORTAGE: APhrase-based Machine Translation System In Proc.ACL 2005 Workshop on building and using paralleltexts.
Ann Arbor, Michigan.F.
Sadat, G. Foster and R. Kuhn.
2006.
Syste`me de tra-duction automatique statistique combinant diffe?rentesressources.
In Proc.
TALN 2006 (Traitement Automa-tique des Langues Naturelles).
Leuven, Belgium, April10-13, 2006.R.
Zens and H. Ney.
2004.
Improvements in phrase-based statistical machine translation.
In Proc.
HumanLanguage Technology Conference / North AmericanChapter of the ACL, Boston, May.137
