Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 47?57,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsLearning Structured Perceptrons for Coreference Resolutionwith Latent Antecedents and Non-local FeaturesAnders Bj?orkelund and Jonas KuhnInstitute for Natural Language ProcessingUniversity of Stuttgart{anders,jonas}@ims.uni-stuttgart.deAbstractWe investigate different ways of learningstructured perceptron models for coref-erence resolution when using non-localfeatures and beam search.
Our experi-mental results indicate that standard tech-niques such as early updates or Learningas Search Optimization (LaSO) performworse than a greedy baseline that only useslocal features.
By modifying LaSO to de-lay updates until the end of each instancewe obtain significant improvements overthe baseline.
Our model obtains the bestresults to date on recent shared task datafor Arabic, Chinese, and English.1 IntroductionThis paper studies and extends previous work us-ing the structured perceptron (Collins, 2002) forcomplex NLP tasks.
We show that for the task ofcoreference resolution the straightforward combi-nation of beam search and early update (Collinsand Roark, 2004) falls short of more limited fea-ture sets that allow for exact search.
This contrastswith previous work on, e.g., syntactic parsing(Collins and Roark, 2004; Huang, 2008; Zhangand Clark, 2008) and linearization (Bohnet etal., 2011), and even simpler structured predictionproblems, where early updates are not even nec-essary, such as part-of-speech tagging (Collins,2002) and named entity recognition (Ratinov andRoth, 2009).The main reason why early updates underper-form in our setting is that the task is too difficultand that the learning algorithm is not able to profitfrom all training data.
Put another way, early up-dates happen too early, and the learning algorithmrarely reaches the end of the instances as it halts,updates, and moves on to the next instance.An alternative would be to continue decod-ing the same instance after the early updates,which is equivalent to Learning as Search Opti-mization (LaSO; Daum?e III and Marcu (2005b)).The learning task we are tackling is howeverfurther complicated since the target structure isunder-determined by the gold standard annotation.Coreferent mentions in a document are usually an-notated as sets of mentions, where all mentions ina set are coreferent.
We adopt the recently pop-ularized approach of inducing a latent structurewithin these sets (Fernandes et al, 2012; Chang etal., 2013; Durrett and Klein, 2013).
This approachprovides a powerful boost to the performance ofcoreference resolvers, but we find that it does notcombine well with the LaSO learning strategy.
Wetherefore propose a modification to LaSO, whichdelays updates until after each instance.
The com-bination of this modification with non-local fea-tures leads to further improvements in the cluster-ing accuracy, as we show in evaluation results onall languages from the CoNLL 2012 Shared Task ?Arabic, Chinese, and English.
We obtain the bestresults to date on these data sets.12 BackgroundCoreference resolution is the task of grouping re-ferring expressions (or mentions) in a text into dis-joint clusters such that all mentions in a clusterrefer to the same entity.
An example is given inFigure 1 below, where mentions from two clustersare marked with brackets:[Drug Emporium Inc.]a1said [Gary Wilber]b1wasnamed CEO of [this drugstore chain]a2.
[He]b2suc-ceeds his father, Philip T. Wilber, who founded [thecompany]a3and remains chairman.
Robert E. LyonsIII, who headed the [company]a4?s Philadelphia re-gion, was appointed president and chief operating offi-cer, succeeding [Gary Wilber]b3.Figure 1: An excerpt of a document with the men-tions from two clusters marked.1Our system is available at http://www.ims.uni-stuttgart.de/?anders/coref.html47In recent years much work on coreference res-olution has been devoted to increasing the ex-pressivity of the classical mention-pair model, inwhich each coreference classification decision islimited to information about two mentions thatmake up a pair.
This shortcoming has been ad-dressed by entity-mention models, which relate acandidate mention to the full cluster of mentionspredicted to be coreferent so far (for more discus-sion on the model types, see, e.g., (Ng, 2010)).Nevertheless, the two best systems in the lat-est CoNLL Shared Task on coreference resolu-tion (Pradhan et al, 2012) were both variants ofthe mention-pair model.
While the second bestsystem (Bj?orkelund and Farkas, 2012) followedthe widely used baseline of Soon et al (2001), thewinning system (Fernandes et al, 2012) proposedthe use of a tree representation.The tree-based model of Fernandes et al (2012)construes the representation of coreference clus-ters as a rooted tree.
Figure 2 displays an exampletree over the clusters from Figure 1.
Every men-tion corresponds to a node in the tree, and arcs be-tween mentions indicate that they are coreferent.The tree additionally has a dummy root node.
Ev-ery subtree under the root node corresponds to acluster of coreferent mentions.Since coreference training data is typically notannotated with trees, Fernandes et al (2012) pro-posed the use of latent trees that are induced dur-ing the training phase of a coreference resolver.The latent tree provides more meaningful an-tecedents for training.2For instance, the popularpair-wise instance creation method suggested bySoon et al (2001) assumes non-branching trees,where the antecedent of every mention is its lin-ear predecessor (i.e., heb2is the antecedent ofGary Wilberb3).
Comparing the two alternativeantecedents of Gary Wilberb3, the tree in Fig-ure 2 provides a more reliable basis for training acoreference resolver, as the two mentions of GaryWilber are both proper names and have an exactstring match.3 Representation and LearningLetM = {m0,m1, ...,mn} denote the set of men-tions in a document, including the artificial rootmention (denoted by m0).
We assume that the2We follow standard practice and overload the termsanaphor and antecedent to be any type of mention, i.e., namesas well as pronouns.
An antecedent is simply the mention tothe left of the anaphor.Drug Emporium Inc.a1the companya3this drugstore chaina2Gary Wilberb1Heb2Gary Wilberb3rootcompanya4Figure 2: A tree representation of Figure 1.mentions are ordered ascendingly with respect tothe linear order of the document, where the docu-ment root precedes all other mentions.3For eachmention mj, let Ajdenote the set of potential an-tecedents.
That is, the set of all mentions thatprecede mjaccording to the linear order includ-ing the root node, or, Aj= {mi| i < j}.
Fi-nally, let A denote the set of all antecedent sets{A0, A1, ..., An}.In the tree model, each mention corresponds toa node, and an antecedent-anaphor pair ?ai,mi?,where ai?
Ai, corresponds to a directed edge (orarc) pointing from antecedent to anaphor.The score of an arc ?ai,mi?
is defined asthe scalar product between a weight vector wand a feature vector ?(?ai,mi?
), where ?
isa feature extraction function over an arc (thusextracting features from the antecedent and theanaphor).
The score of a coreference tree y ={?a1,m1?, ?a2,m2?, ..., ?an,mn?}
is defined asthe sum of the scores of all the mention pairs:score(?ai,mi?)
= w ?
?(?ai,mi?)
(1)score(y) =??ai,mi??yscore(?ai,mi?
)The objective is to find the output y?
that maxi-mizes the scoring function:y?
= arg maxy?Y(A)score(y) (2)where Y(A) denotes the set of possible trees giventhe antecedent sets A.
By treating the mentions asnodes in a directed graph and assigning scores tothe arcs according to (1), Fernandes et al (2012)solved the search problem using the Chu-Liu-Edmonds (CLE) algorithm (Chu and Liu, 1965;3We impose a total order on mentions.
In case of nestedmentions, the mention that begins first is assumed to precedethe embedded one.
If two mentions begin at the same token,the longer one is taken to precede the shorter one.48Edmonds, 1967), which is a maximum spanningtree algorithm that finds the optimal tree over aconnected directed graph.
CLE, however, has thedrawback that the scores of the arcs must remainfixed and can not change depending on other arcsand it is not clear how to include non-local featuresin a CLE decoder.3.1 Online learningWe find the weight vector w by online learning us-ing a variant of the structured perceptron (Collins,2002).
Specifically, we use the passive-aggressive(PA) algorithm (Crammer et al, 2006), since wefound that this performed slightly better in prelim-inary experiments.4The structured perceptron iterates over train-ing instances ?xi, yi?, where xiare inputs and yiare outputs.
For each instance it uses the currentweight vector w to make a prediction y?igiven theinput xi.
If the prediction is incorrect, the weightvector is updated in favor of the correct structure.Otherwise the weight vector is left untouched.
Inour setting inputs xicorrespond to documents andoutputs yiare trees over mentions in a document.The training data is, however, not annotated withtrees, but only with clusters of mentions.
That is,the yi?s are not defined a priori.3.2 Latent antecedentsIn order to have a tree structure to update against,we use the current weight vector and apply thedecoder to a constrained antecedent set and ob-tain a latent tree over the mentions in a docu-ment, where each mention is assigned a single cor-rect antecedent (Fernandes et al, 2012).
We con-strain the antecedent sets such that only trees thatcorrespond to the correct clustering can be built.Specifically, let?Ajdenote the set of correct an-tecedents for a mention mj, or?Aj={{m0} if mjhas no correct antecedent{ai| COREF(ai,mj), ai?
Aj} otherwisethat is, if mention mjis non-referential or the firstmention of its cluster,?Ajcontains only the docu-ment root.
Otherwise it is the set of all mentionsto the left that belong to the same cluster as mj.Analogously to A, let?A denote the set of con-strained antecedent sets.
The latent tree y?
needed4We also implement the feature mapping function ?
asa hash kernel (Bohnet, 2010) and apply averaging (Collins,2002), though for brevity we omit this from the pseudocode.for updates is then defined to be the optimal treeover Y(?A), subject to the current weight vector:y?
= arg maxy?Y(?A)score(y)The intuition behind the latent tree is that duringonline learning, the weight vector will start favor-ing latent trees that are easier to learn (such as theone in Figure 2).Algorithm 1 PA algorithm with latent treesInput: Training data D, number of iterations TOutput: Weight vector w1: w =?
?02: for t ?
1..T do3: for ?Mi,Ai,?Ai?
?
D do4: y?i= arg maxY(A)score(y) .
Predict5: if ?CORRECT(y?i) then6: y?i= arg maxY(?A)score(y) .
Latent tree7: ?
= ?(y?i)?
?
(y?i)8: ?
=??w+LOSS(y?i)???2.
PA weight9: w = w + ??
.
PA update10: return wAlgorithm 1 shows pseudocode for the learn-ing algorithm, which we will refer to as the base-line learning algorithm.
Instead of looping overpairs ?x, y?
of documents and trees, it loops overtriples ?M,A,?A?
that comprise the set of men-tions M and the two sets of antecedent candidates(line 3).
Moreover, rather than checking that thetree is identical to the latent tree, it only requiresthe tree to correctly encode the gold clustering(line 5).
The update that occurs in lines 7-9 is thepassive-aggressive update.
A loss function LOSSthat quantifies the error in the prediction is usedto compute a scalar ?
that controls how much theweights are moved in each update.
If ?
is set to 1,the update reduces to the standard structured per-ceptron update.
The loss function can be an arbi-trarily complex function that returns a numericalvalue of how bad the prediction is.
In the sim-plest case, Hamming loss can be used, i.e., foreach incorrect arc add 1.
We follow Fernandeset al (2012) and penalize erroneous root attach-ments, i.e., mentions that erroneously get the rootnode as their antecedent, with a loss of 1.5.
For allother arcs we use Hamming loss.4 Incremental SearchWe now show that the search problem in (2) canequivalently be solved by the more intuitive best-first decoder (Ng and Cardie, 2002), rather thanusing the CLE decoder.
The best-first decoder49works incrementally by making a left-to-right passover the mentions, selecting for each mention thehighest scoring antecedent.The key aspect that makes the best-first decoderequivalent to the CLE decoder is that all arcs pointfrom left to right, both in this paper and in the workof Fernandes et al (2012).
We sketch a proof thatthis decoder also returns the highest scoring tree.First, note that this algorithm indeed returns atree.
This can be shown by assuming the opposite,in which case the tree has to have a cycle.
Thenthere must be a mention that has its antecedent tothe right.
Though this is not possible since all arcspoint from left to right.Second, this tree is the highest scoring tree.Again, assume the contrary, i.e., that there is ahigher scoring tree in Y(A).
This implies that forsome mention there is a higher scoring antecedentthan the one selected by the decoder.
This contra-dicts the fact that the best-first decoder selects thehighest scoring antecedent for each mention.55 Introducing Non-local FeaturesSince the best-first decoder makes a left-to-rightpass, it is possible to extract features on the partialstructure on the left.
Such non-local features areable to capture information beyond that of a men-tion and its potential antecedent, e.g., the size ofa partially built cluster, or features extracted fromthe antecedent of the antecedent.When only local features are used, greedysearch (either with CLE or the best-first decoder)suffices to find the highest scoring tree.
That is,greedy search provides an exact solution to equa-tion 2.
Non-local features, however, render the ex-act search problem intractable.
This is becausewith non-local features, locally suboptimal (i.e.,non-greedy) antecedents for some mentions maylead to a higher total score over a whole document.In order to keep some options around duringsearch, we extend the best-first decoder with beamsearch.
Beam search works incrementally bykeeping an agenda of state items.
At each step,all items on the agenda are expanded.
The subsetof size k (the beam size) of the highest scoring ex-pansions are retained and put back into the agendafor the next step.
The feature extraction function ?5In case there are multiple maximum spanning trees, thebest-first decoder will return one of them.
This also holds forthe CLE algorithm.
With proper definitions, the proof can beconstructed to show that both search algorithms return treesbelonging to the set of maximum spanning trees over a graph.is also extended such that it also receives the cur-rent state s as an argument: ?
(?mi,mj?, s).
Thestate encodes the previous decisions and enables ?to extract features from the partial tree on the left.We now outline three different ways of learningthe weight vector w with non-local features.5.1 Early updatesThe beam search decoder can be plugged into thetraining algorithm, replacing the calls to arg max.Since state items leading to the best tree maybe pruned from the agenda before the decoderreaches the end of the document, the introduc-tion of non-local features may cause the decoderto return a non-optimal tree.
This is problem-atic as it might cause updates although the correcttree has a higher score than the predicted one.
Ithas previously been observed (Huang et al, 2012)that substantial gains can be made by applying anearly update strategy (Collins and Roark, 2004):if the correct item is pruned before reaching theend of the document, then stop and update.While beam search and early updates have beensuccessfully applied to other NLP applications,our task differs in two important aspects: First,coreference resolution is a much more difficulttask, which relies on more (world) knowledge thanwhat is available in the training data.
In otherwords, it is unlikely that we can devise a featureset that is informative enough to allow the weightvector to converge towards a solution that lets thelearning algorithm see the entire documents dur-ing training, at least in the situation when no ex-ternal knowledge sources are used.Second, our gold structure is not known butis induced latently, and may vary from iterationto iteration.
With non-local features this is trou-blesome since the best latent tree of a completedocument may not necessarily coincide with thebest partial tree at some intermediate mentionmj,j < n, i.e., a mention before the last in a docu-ment.
We therefore also apply beam search to findthe latent tree to have a partial gold structure forevery mention in a document.Algorithm 2 shows pseudocode for the beamsearch and early update training procedure.
Thealgorithm maintains two parallel agendas, one forgold items and one for predicted items.
At ev-ery mention, both agendas are expanded and thuscover the same set of mentions.
Then the predictedagenda is checked to see if it contains any correct50Algorithm 2 Beam search and early updateInput: Data set D, epochs T , beam size kOutput: weight vector w1: w =?
?02: for t ?
1..T do3: for ?Mi,Ai,?Ai?
?
D do4: AgendaG= {}5: AgendaP= {}6: for j ?
1..n do7: AgendaG= EXPAND(AgendaG,?Aj,mj, k)8: AgendaP= EXPAND(AgendaP, Aj,mj, k)9: if ?CONTAINSCORRECT(AgendaP) then10: y?
= EXTRACTBEST(AgendaG)11: y?
= EXTRACTBEST(AgendaP)12: update .
PA update13: GOTO 3 .
Skip and move to next instance14: y?
= EXTRACTBEST(AgendaP)15: if ?CORRECT(y?)
then16: y?
= EXTRACTBEST(AgendaG)17: update .
PA updateitem.
If there is no correct item in the predictedagenda, search is halted and an update is madeagainst the best item from the gold agenda.
Thealgorithm then moves on to the next document.
Ifthe end of a document is reached, the top scoringpredicted item is checked for correctness.
If it isnot, an update is made against the best gold item.A drawback of early updates is that the remain-der of the document is skipped when an early up-date is applied, effectively discarding some train-ing data.6An alternative strategy that makes bet-ter use of the training data is to apply the max-violation procedure suggested by Huang et al(2012).
However, since our gold trees change fromiteration to iteration, and even inside of a singledocument, it is not entirely clear with respect towhat gold tree the maximum violation should becomputed.
Initial experiments with max-violationupdates indicated that they did not improve muchover early updates, and also had a tendency to onlyconsider a smaller portion of the training data.5.2 LaSOTo make full use of the training data we imple-mented Learning as Search Optimization (LaSO;Daum?e III and Marcu, 2005b).
It is very similarto early updates, but differs in one crucial respect:When an early update is made, search is continuedrather than aborted.
Thus the learning algorithmalways reaches the end of a document, avoidingthe problem that early updates discard parts of thetraining data.6In fact, after 50 iterations about 70% of the mentions inthe training data are still being ignored due to early updates.Correct items are computed the same way aswith early updates, where an agenda of gold itemsis maintained in parallel.
When search is resumedafter an intermediate LaSO update, the predictionagenda is re-seeded with gold items (i.e., itemsthat are all correct).
This is necessary since theupdate influences what the partial gold structurelooks like, and the gold agenda therefore needs tobe recreated from the beginning of the document.Specifically, after each intermediate LaSO update,the gold agenda is expanded repeatedly from thebeginning of the document to the point where theupdate was made, and is then copied over to seedthe prediction agenda.
In terms of pseudocode,this is accomplished by replacing lines 12 and 13in Algorithm 2 with the following:12: update .
PA update13: AgendaG= {}14: for mi?
{m1, ...,mj} .
Recreate gold agenda15: AgendaG= EXPAND(AgendaG,?Ai,mi, k)16: AgendaP= COPY(AgendaG)17: GOTO 6 .
Continue5.3 Delayed LaSO updatesWhen we applied LaSO, we noticed that it per-formed worse than the baseline learning algorithmwhen only using local features.
We believe that thereason is that updates are made in the middle ofdocuments which means that lexical forms of an-tecedents are ?fresh in memory?
of the weight vec-tor.
This results in fewer mistakes during trainingand leads to fewer updates.
While this feedbackmakes it easier during training, such feedback isnot available during test time, and the LaSO learn-ing setting therefore mimics the testing setting toa lesser extent.We also found that LaSO updates change theshape of the latent tree and that the average dis-tance between mentions connected by an arc in-creased.
This problem can also be attributed tohow lexical items are fresh in memory.
Such treestend to deviate from the intuition that the latenttrees are easier to learn.
They also render distance-based features (which are standard practice andgenerally rather useful) less powerful, as distancein sentences or mentions becomes less of a reliableindicator for coreference.To cope with this problem, we devised thedelayed LaSO update, which differs from LaSOonly in the respect that it postpones the actual up-dates until the end of a document.
This is accom-plished by summing the distance vectors ?
at ev-ery point where LaSO would make an update.
At51Algorithm 3 Delayed LaSO updateInput: Data set D, iterations T , beam size kOutput: weight vector w1: w =?
?02: for t ?
1..T do3: for ?Mi,Ai,?Ai?
?
D do4: AgendaG= {}5: AgendaP= {}6: ?acc=?
?07: lossacc= 08: for j ?
1..n do9: AgendaG= EXPAND(AgendaG,?Aj,mj, k)10: AgendaP= EXPAND(AgendaP, Aj,mj, k)11: if ?CONTAINSCORRECT(AgendaP) then12: y?
= EXTRACTBEST(AgendaG)13: y?
= EXTRACTBEST(AgendaP)14: ?acc= ?acc+ ?(y?)?
?(y?
)15: lossacc= lossacc+ LOSS(y?
)16: AgendaP= AgendaG17: y?
= EXTRACTBEST(AgendaP)18: if ?CORRECT(y?)
then19: y?
= EXTRACTBEST(AgendaG)20: ?acc= ?acc+ ?(y?)?
?(y?
)21: lossacc= lossacc+ LOSS(y?
)22: if ?acc6=?
?0 then23: update w.r.t.
?accand lossaccthe end of a document, an update is made with re-spect to the sum of all ??s.
Similarly, a runningsum of the partial loss is maintained within a doc-ument.
Since the PA update only depends on thedistance vector ?
and the loss, it can be appliedwith respect to these sums at the end of the doc-ument.
When only local features are used, thisupdate is equivalent to the updates in the baselinelearning algorithm.
This follows because greedysearch finds the optimal tree when only local fea-tures are used.
Similarly, using only local features,the beam-based best-first decoder will also returnthe optimal tree.
Algorithm 3 shows the pseu-docode for the delayed LaSO learning algorithm.6 FeaturesIn this section we briefly outline the type of fea-tures we use.
The feature sets are customized foreach language.
As a baseline we use the featuresfrom Bj?orkelund and Farkas (2012), who rankedsecond in the 2012 CoNLL shared task and is pub-licly available.
The exact definitions and featuresets that we use are available as part of the down-load package of our system.6.1 Local featuresBasic features that can be extracted on one orboth mentions in a pair include (among oth-ers): Mention type, which is either root, pro-noun, name, or common; Distance features, e.g.,the distance in sentences or mentions; Rule-basedfeatures, e.g., StringMatch or SubStringMatch;Syntax-based features, e.g., category labels orpaths in the syntax tree; Lexical features, e.g., thehead word of a mention or the last word of a men-tion.In order to have a strong local baseline, we ap-plied greedy forward/backward feature selectionon the training data using a large set of local fea-ture templates.
Specifically, the training set ofeach language was split into two parts where 75%was used for training, and 25% for testing.
Featuretemplates were incrementally added or removedin order to optimize the mean of MUC, B3, andCEAFe(i.e., the CoNLL average).6.2 Non-local FeaturesWe experimented with non-local features drawnfrom previous work on entity-mention mod-els (Luo et al, 2004; Rahman and Ng, 2009), how-ever they did not improve performance in prelimi-nary experiments.
The one exception is the size ofa cluster (Culotta et al, 2007).
Additional featureswe use areShape encodes the linear ?shape?
of a cluster interms of mention type.
For instance, the clustersrepresenting Gary Wilber and Drug EmporiumInc.
from the example in Figure 1, would be repre-sented as RNPN and RNCCC, respectively.
WhereR, N, P, and C denote the root node, names, pro-nouns, and common noun phrases, respectively.Local syntactic context is inspired by the EntityGrid (Barzilay and Lapata, 2008), where the ba-sic assumption is that references to an entity fol-low particular syntactic patterns.
For instance, anentity may be introduced as an object in one sen-tence, whereas in subsequent sentences it is re-ferred to in subject position.
Grammatical func-tions are approximated by the path in the syntaxtree from a mention to its closest S node.
The par-tial paths of a mention and its linear predecessor,given the cluster of the current antecedent, informsthe model about the local syntactic context.Cluster start distance denotes the distance inmentions from the beginning of the documentwhere the cluster of the antecedent in considera-tion begins.Additionally, the non-local model also has ac-cess to the basic properties of other mentions inthe partial tree structure, such as head words.
The52non-local features were selected with the samegreedy forward strategy as the local features, start-ing from the optimized local feature sets.7 Experimental SetupWe apply our model to the CoNLL 2012 SharedTask data, which includes a training, develop-ment, and test set split for three languages: Ara-bic, Chinese and English.
We follow the closedtrack setting where systems may only be trainedon the provided training data, with the exceptionof the English gender and number data compiledby Bergsma and Lin (2006).
We use automaticallyextracted mentions using the same mention extrac-tion procedure as Bj?orkelund and Farkas (2012).We evaluate our system using the CoNLL 2012scorer, which computes several coreference met-rics: MUC (Vilain et al, 1995), B3(Bagga andBaldwin, 1998), and CEAFeand CEAFm(Luo,2005).
We also report the CoNLL average (alsoknown as MELA; Denis and Baldridge (2009)),i.e., the arithmetic mean of MUC, B3, and CEAFe.It should be noted that for B3and the CEAF met-rics, multiple ways of handling twinless mentions7have been proposed (Rahman and Ng, 2009; Stoy-anov et al, 2009).
We use the most recent ver-sion of the CoNLL scorer (version 7), which im-plements the original definitions of these metrics.8Our system is evaluated on the version of thedata with automatic preprocessing information(e.g., predicted parse trees).
Unless otherwisestated we use 25 iterations of perceptron trainingand a beam size of 20.
We did not attempt to tuneeither of these parameters.
We experiment withtwo feature sets for each language: the optimizedlocal feature sets (denoted local), and the opti-mized local feature sets extended with non-localfeatures (denoted non-local).8 ResultsLearning strategies.
We begin by looking at thedifferent learning strategies.
Since early updatesdo not always make use of the complete docu-ments during training, it can be expected that itwill require either a very wide beam or more iter-ations to get up to par with the baseline learningalgorithm.
Figure 3 shows the CoNLL average on7i.e., mentions that appear in the prediction but not ingold, or the other way around8Available at http://conll.cemantix.org/2012/software.html5456586062640 10 20 30 40 50CoNLLavg.IterationsBaselineEarly (local), k=20Early (local), k=100Early (non-local), k=20Early (non-local), k=100Figure 3: Comparing early update training withthe baseline training algorithm.the English development set as a function of num-ber of training iterations with two different beamsizes, 20 and 100, over the local and non-local fea-ture sets.
The figure shows that even after 50 itera-tions, early update falls short of the baseline, evenwhen the early update system has access to moreinformative non-local features.9In Figure 4 we compare early update with LaSOand delayed LaSO on the English development set.The left half uses the local feature set, and the rightthe extended non-local feature set.
Recall that withonly local features, delayed LaSO is equivalent tothe baseline learning algorithm.
As before, earlyupdate is considerably worse than other learningstrategies.
We also see that delayed LaSO out-performs LaSO, both with and without non-localfeatures.
Note that plain LaSO with non-local fea-tures only barely outperforms the delayed LaSOwith only local features (i.e., the baseline), whichindicates that only delayed LaSO is able to fullyleverage non-local features.
From these results weconclude that we are better off when the learningalgorithm handles one document at a time, insteadof getting feedback within documents.Local vs. Non-local feature sets.
Table 1 dis-plays the differences in F-measures and CoNLLaverage between the local and non-local systemswhen applied to the development sets for each lan-guage.
All metrics improve when more informa-tive non-local features are added to the local fea-ture set.
Arabic and English show considerableimprovements, and the CoNLL average increases9Although the Early systems still seem to show slight in-creases after 50 iterations, it needs a considerable number ofiterations to catch up with the baseline ?
after 100 iterationsthe best early system is still more than half a point behind thebaseline.535859606162636465Local Non-localCoNLLavg.EarlyLaSODelayed LaSOFigure 4: Comparison of learning algorithms eval-uated on the English development set.MUC B3CEAFmCEAFeCoNLLArabiclocal 47.33 42.51 49.71 46.49 45.44non-local 49.31 43.52 50.96 47.18 46.67Chineselocal 65.84 57.94 62.23 57.05 60.27non-local 66.4 57.99 62.37 57.12 60.5Englishlocal 69.95 58.7 62.91 56.03 61.56non-local 70.74 60.03 65.01 56.8 62.52Table 1: Comparison of local and non-local fea-ture sets on the development sets.about one point.
For Chinese the gains are gen-erally not as pronounced, though the MUC metricgoes up by more than half a point.Final results.
In Table 2 we compare the re-sults of the non-local system (This paper) to thebest results from the CoNLL 2012 Shared Task.10Specifically, this includes Fernandes et al?s (2012)system for Arabic and English (denoted Fernan-des), and Chen and Ng?s (2012) system for Chi-nese (denoted C&N).
For English we also com-pare it to the Berkeley system (Durrett and Klein,2013), which, to our knowledge, is the best pub-licly available system for English coreference res-olution (denoted D&K).
As a general baseline, wealso include Bj?orkelund and Farkas?
(2012) sys-tem (denoted B&F), which was the second bestsystem in the shared task.
For almost all met-rics our system is significantly better than the bestcompetitor.
For a few metrics the best competitoroutperforms our results for either precision or re-call, but in terms of F-measures and the CoNLLaverage our system is the best for all languages.10Thanks to Sameer Pradhan for providing us with the out-puts of the other systems for significance testing.9 Related WorkOn the machine learning side Collins and Roark?s(2004) work on the early update constitutes ourstarting point.
The LaSO framework was intro-duced by Daum?e III and Marcu (2005b), but has,to our knowledge, only been applied to the relatedtask of entity detection and tracking (Daum?e IIIand Marcu, 2005a).
The theoretical motivation forearly updates was only recently explained rigor-ously (Huang et al, 2012).
The delayed LaSOupdate that we propose decomposes the predic-tion task of a complex structure into a number ofsubproblems, each of which guarantee violation,using Huang et al?s (2012) terminology.
We be-lieve this is an interesting novelty, as it leveragesthe complete structures for every training instanceduring every iteration, and expect it to be applica-ble also to other structured prediction tasks.Our approach also resembles imitation learningtechniques such as SEARN (Daum?e III et al, 2009)and DAGGER (Ross et al, 2011), where the searchproblem is reduced to a sequence of classificationsteps that guide the search algorithm through thesearch space.
These frameworks, however, rely onthe notion of an expert policy which provides anoptimal decision at each point during search.
Inour context that would require antecedents for ev-ery mention to be given a priori, rather than usinglatent antecedents as we do.Perceptrons for coreference.
The perceptronhas previously been used to train coreference re-solvers either by casting the problem as a binaryclassification problem that considers pairs of men-tions in isolation (Bengtson and Roth, 2008; Stoy-anov et al, 2009; Chang et al, 2012, inter alia) orin the structured manner, where a clustering for anentire document is predicted in one go (Fernandeset al, 2012).
However, none of these works usenon-local features.
Stoyanov and Eisner (2012)train an Easy-First coreference system with theperceptron to learn a sequence of join operationsbetween arbitrary mentions in a document and ac-cesses non-local features through previous mergeoperations in later stages.
Culotta et al (2007) alsoapply online learning in a first-order logic frame-work that enables non-local features, though usinga greedy search algorithm.Latent antecedents.
The use of latent an-tecedents goes back to the work of Yu andJoachims (2009), although the idea of determining54MUC B3CEAFmCEAFeCoNLLRec Prec F1Rec Prec F1Rec Prec F1Rec Prec F1avg.ArabicB&F 43.9 52.51 47.82 35.7 49.77 41.58 43.8 50.03 46.71 40.45 41.86 41.15 43.51Fernandes 43.63 49.69 46.46 38.39 47.7 42.54 47.6 50.85 49.17 48.16 45.03 46.54 45.18This paper 47.53 53.3 50.25 44.14 49.34 46.6 50.94 55.19 52.98 49.2 49.45 49.33 48.72ChineseB&F 58.72 58.49 58.61 49.17 53.2 51.11 56.68 51.86 54.14 55.36 41.8 47.63 52.45C&N 59.92 64.69 62.21 51.76 60.26 55.69 59.58 60.45 60.02 58.84 51.61 54.99 57.63This paper 62.57 69.39 65.8 53.87 61.64 57.49 58.75 64.76 61.61 54.65 59.33 56.89 60.06EnglishB&F 65.23 70.1 67.58 49.51 60.69 54.47 56.93 59.51 58.19 51.34 49.14 59.21 57.42Fernandes 65.83 75.91 70.51 51.55 65.19 57.58 57.48 65.93 61.42 50.82 57.28 53.86 60.65D&K 66.58 74.94 70.51 53.2 64.56 58.33 59.19 66.23 62.51 52.9 58.06 55.36 61.4This paper 67.46 74.3 70.72 54.96 62.71 58.58 60.33 66.92 63.45 52.27 59.4 55.61 61.63Table 2: Comparison with other systems on the test sets.
Bold numbers indicate significance at thep < 0.05 level between the best and the second best systems (according to the CoNLL average) usinga Wilcoxon signed rank sum test.
We refrain from significance tests on the CoNLL average, as it is anaverage over other F-measures.meaningful antecedents for mentions can be tracedback to Ng and Cardie (2002) who used a rule-based approach.
Latent antecedents have recentlygained popularity and were used by two systems inthe CoNLL 2012 Shared Task, including the win-ning system (Fernandes et al, 2012; Chang et al,2012).
Durrett and Klein (2013) present a corefer-ence resolver with latent antecedents that predictsclusterings over entire documents and fit a log-linear model with a custom task-specific loss func-tion using AdaGrad (Duchi et al, 2011).
Changet al (2013) use a max-margin approach to learna pairwise model and rely on stochastic gradientdescent to circumvent the costly operation of de-coding the entire training set in order to computethe gradients and the latent antecedents.
None ofthe aforementioned works use non-local featuresin their models, however.Entity-mention models.
Entity-mention mod-els that compare a single mention to a (partial)cluster have been studied extensively and severalworks have evaluated non-local entity-level fea-tures (Luo et al, 2004; Yang et al, 2008; Rah-man and Ng, 2009).
Luo et al (2004) also applybeam search at test time, but use a static assign-ment of antecedents and learns log-linear modelusing batch learning.
Moreover, these works al-ter the basic feature definitions from their pair-wise models when introducing entity-level fea-tures.
This contrasts with our work, as ourmention-pair model simply constitutes a specialcase of the non-local system.10 ConclusionWe presented experiments with a coreference re-solver that leverages non-local features to improveits performance.
The application of non-local fea-tures requires the use of an approximate search al-gorithm to keep the problem tractable.
We eval-uated standard perceptron learning techniques forthis setting both using early updates and LaSO.
Wefound that the early update strategy is considerablyworse than a local baseline, as it is unable to ex-ploit all training data.
LaSO resolves this issue bygiving feedback within documents, but still under-performs compared to the baseline as it distorts thechoice of latent antecedents.We introduced a modification to LaSO, whereupdates are delayed until each document is pro-cessed.
In the special case where only local fea-tures are used, this method coincides with stan-dard structured perceptron learning that uses exactsearch.
Moreover, it is also able to profit from non-local features resulting in improved performance.We evaluated our system on all three languagesfrom the CoNLL 2012 Shared Task and presentthe best results to date on these data sets.AcknowledgmentsWe are grateful to the anonymous reviewers aswell as Christian Scheible and Wolfgang Seekerfor comments on earlier versions of this paper.This research has been funded by the DFG viaSFB 732, project D8.55ReferencesAmit Bagga and Breck Baldwin.
1998.
Algorithms forscoring coreference chains.
In In The First Interna-tional Conference on Language Resources and Eval-uation Workshop on Linguistics Coreference, pages563?566.Regina Barzilay and Mirella Lapata.
2008.
Modelinglocal coherence: An entity-based approach.
Compu-tational Linguistics, 34(1):1?34.Eric Bengtson and Dan Roth.
2008.
Understand-ing the value of features for coreference resolution.In Proceedings of the 2008 Conference on Empiri-cal Methods in Natural Language Processing, pages294?303, Honolulu, Hawaii, October.
Associationfor Computational Linguistics.Shane Bergsma and Dekang Lin.
2006.
Bootstrappingpath-based pronoun resolution.
In Proceedings ofthe 21st International Conference on ComputationalLinguistics and 44th Annual Meeting of the Associ-ation for Computational Linguistics, pages 33?40,Sydney, Australia, July.
Association for Computa-tional Linguistics.Anders Bj?orkelund and Rich?ard Farkas.
2012.
Data-driven multilingual coreference resolution using re-solver stacking.
In Joint Conference on EMNLP andCoNLL - Shared Task, pages 49?55, Jeju Island, Ko-rea, July.
Association for Computational Linguistics.Bernd Bohnet, Simon Mille, Beno?
?t Favre, and LeoWanner.
2011.
<stumaba >: From deep represen-tation to surface.
In Proceedings of the GenerationChallenges Session at the 13th European Workshopon Natural Language Generation, pages 232?235,Nancy, France, September.
Association for Compu-tational Linguistics.Bernd Bohnet.
2010.
Top accuracy and fast depen-dency parsing is not a contradiction.
In Proceedingsof the 23rd International Conference on Computa-tional Linguistics (Coling 2010), pages 89?97, Bei-jing, China, August.Kai-Wei Chang, Rajhans Samdani, Alla Rozovskaya,Mark Sammons, and Dan Roth.
2012.
Illinois-coref: The ui system in the conll-2012 shared task.In Joint Conference on EMNLP and CoNLL - SharedTask, pages 113?117, Jeju Island, Korea, July.
Asso-ciation for Computational Linguistics.Kai-Wei Chang, Rajhans Samdani, and Dan Roth.2013.
A constrained latent variable model for coref-erence resolution.
In Proceedings of the 2013 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 601?612, Seattle, Washington,USA, October.
Association for Computational Lin-guistics.Chen Chen and Vincent Ng.
2012.
Combining thebest of two worlds: A hybrid approach to multilin-gual coreference resolution.
In Joint Conference onEMNLP and CoNLL - Shared Task, pages 56?63,Jeju Island, Korea, July.
Association for Computa-tional Linguistics.Yoeng-jin Chu and Tseng-hong Liu.
1965.
On theshortest aborescence of a directed graph.
ScienceSinica, 14:1396?1400.Michael Collins and Brian Roark.
2004.
Incremen-tal parsing with the perceptron algorithm.
In Pro-ceedings of the 42nd Meeting of the Association forComputational Linguistics (ACL?04), Main Volume,pages 111?118, Barcelona, Spain, July.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and experi-ments with perceptron algorithms.
In Proceedingsof the 2002 Conference on Empirical Methods inNatural Language Processing, pages 1?8.
Associ-ation for Computational Linguistics, July.Koby Crammer, Ofer Dekel, Joseph Keshet, ShaiShalev-Shwartz, and Yoram Singer.
2006.
Onlinepassive?aggressive algorithms.
Journal of MachineLearning Reseach, 7:551?585, March.Aron Culotta, Michael Wick, and Andrew McCallum.2007.
First-order probabilistic models for corefer-ence resolution.
In Human Language Technologies2007: The Conference of the North American Chap-ter of the Association for Computational Linguistics;Proceedings of the Main Conference, pages 81?88,Rochester, New York, April.
Association for Com-putational Linguistics.Hal Daum?e III and Daniel Marcu.
2005a.
A large-scale exploration of effective global features for ajoint entity detection and tracking model.
In Pro-ceedings of Human Language Technology Confer-ence and Conference on Empirical Methods in Natu-ral Language Processing, pages 97?104, Vancouver,British Columbia, Canada, October.
Association forComputational Linguistics.Hal Daum?e III and Daniel Marcu.
2005b.
Learningas search optimization: approximate large marginmethods for structured prediction.
In ICML, pages169?176.Hal Daum?e III, John Langford, and Daniel Marcu.2009.
Search-based structured prediction.
MachineLearning, 75(3):297?325.Pascal Denis and Jason Baldridge.
2009.
Global JointModels for Coreference Resolution and Named En-tity Classification.
In Procesamiento del LenguajeNatural 42, pages 87?96, Barcelona: SEPLN.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learningand stochastic optimization.
J. Mach.
Learn.
Res.,12:2121?2159, July.Greg Durrett and Dan Klein.
2013.
Easy victories anduphill battles in coreference resolution.
In Proceed-ings of the 2013 Conference on Empirical Methodsin Natural Language Processing, pages 1971?1982,56Seattle, Washington, USA, October.
Association forComputational Linguistics.Jack Edmonds.
1967.
Optimum branchings.
Jour-nal of Research of the National Bureau of Standards,71(B):233?240.Eraldo Fernandes, C?
?cero dos Santos, and Ruy Milidi?u.2012.
Latent structure perceptron with feature in-duction for unrestricted coreference resolution.
InJoint Conference on EMNLP and CoNLL - SharedTask, pages 41?48, Jeju Island, Korea, July.
Associ-ation for Computational Linguistics.Liang Huang, Suphan Fayong, and Yang Guo.
2012.Structured perceptron with inexact search.
In Pro-ceedings of the 2012 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages142?151, Montr?eal, Canada, June.
Association forComputational Linguistics.Liang Huang.
2008.
Forest reranking: Discrimina-tive parsing with non-local features.
In Proceedingsof ACL-08: HLT, pages 586?594, Columbus, Ohio,June.
Association for Computational Linguistics.Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, NandaKambhatla, and Salim Roukos.
2004.
A mention-synchronous coreference resolution algorithm basedon the bell tree.
In Proceedings of the 42nd Meet-ing of the Association for Computational Linguis-tics, pages 135?142, Barcelona, Spain, July.Xiaoqiang Luo.
2005.
On coreference resolutionperformance metrics.
In Proceedings of HumanLanguage Technology Conference and Conferenceon Empirical Methods in Natural Language Pro-cessing, pages 25?32, Vancouver, British Columbia,Canada, October.
Association for ComputationalLinguistics.Vincent Ng and Claire Cardie.
2002.
Improving ma-chine learning approaches to coreference resolution.In Proceedings of 40th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 104?111, Philadelphia, Pennsylvania, USA, July.
Asso-ciation for Computational Linguistics.Vincent Ng.
2010.
Supervised noun phrase coref-erence research: The first fifteen years.
In Pro-ceedings of the 48th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 1396?1411, Uppsala, Sweden, July.
Association for Com-putational Linguistics.Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,Olga Uryupina, and Yuchen Zhang.
2012.
Conll-2012 shared task: Modeling multilingual unre-stricted coreference in ontonotes.
In Joint Confer-ence on EMNLP and CoNLL - Shared Task, pages1?40, Jeju Island, Korea, July.
Association for Com-putational Linguistics.Altaf Rahman and Vincent Ng.
2009.
Supervised mod-els for coreference resolution.
In Proceedings of the2009 Conference on Empirical Methods in NaturalLanguage Processing, pages 968?977, Singapore,August.
Association for Computational Linguistics.Lev Ratinov and Dan Roth.
2009.
Design chal-lenges and misconceptions in named entity recog-nition.
In Proceedings of the Thirteenth Confer-ence on Computational Natural Language Learning(CoNLL-2009), pages 147?155, Boulder, Colorado,June.
Association for Computational Linguistics.St?ephane Ross, Geoffrey J. Gordon, and J. AndrewBagnell.
2011.
A reduction of imitation learningand structured prediction to no-regret online learn-ing.
In AISTATS, pages 627?635.Wee Meng Soon, Hwee Tou Ng, and DanielChung Yong Lim.
2001.
A machine learning ap-proach to coreference resolution of noun phrases.Computational Linguistics, 27(4):521?544.Veselin Stoyanov and Jason Eisner.
2012.
Easy-firstcoreference resolution.
In Proceedings of COLING2012, pages 2519?2534, Mumbai, India, December.Veselin Stoyanov, Nathan Gilbert, Claire Cardie, andEllen Riloff.
2009.
Conundrums in noun phrasecoreference resolution: Making sense of the state-of-the-art.
In Proceedings of the Joint Conference ofthe 47th Annual Meeting of the ACL and the 4th In-ternational Joint Conference on Natural LanguageProcessing of the AFNLP, pages 656?664, Suntec,Singapore, August.
Association for ComputationalLinguistics.Marc Vilain, John Burger, John Aberdeen, Dennis Con-nolly, and Lynette Hirschman.
1995.
A model the-oretic coreference scoring scheme.
In ProceedingsMUC-6, pages 45?52, Columbia, Maryland.Xiaofeng Yang, Jian Su, Jun Lang, Chew Lim Tan,Ting Liu, and Sheng Li.
2008.
An entity-mention model for coreference resolution with in-ductive logic programming.
In Proceedings of ACL-08: HLT, pages 843?851, Columbus, Ohio, June.Association for Computational Linguistics.Chun-Nam Yu and T. Joachims.
2009.
Learning struc-tural svms with latent variables.
In InternationalConference on Machine Learning (ICML).Yue Zhang and Stephen Clark.
2008.
A tale of twoparsers: Investigating and combining graph-basedand transition-based dependency parsing.
In Pro-ceedings of the 2008 Conference on Empirical Meth-ods in Natural Language Processing, pages 562?571, Honolulu, Hawaii, October.
Association forComputational Linguistics.57
