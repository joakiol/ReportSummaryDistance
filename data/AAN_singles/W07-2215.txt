Proceedings of the 10th Conference on Parsing Technologies, pages 109?120,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsModular and Efficient Top-Down Parsing for Ambiguous Left-RecursiveGrammarsRichard A.
Frost and Rahmatullah HafizSchool of Computer ScienceUniversity of WindsorCanadarfrost@cogeco.caPaul C. CallaghanDepartment of Computer ScienceUniversity of DurhamU.K.P.C.Callaghan@durham.ac.ukAbstractIn functional and logic programming,parsers can be built as modular executablespecifications of grammars, using parsercombinators and definite clause grammarsrespectively.
These techniques are based ontop-down backtracking search.
Commonlyused implementations are inefficient forambiguous languages, cannot accommodateleft-recursive grammars, and require expo-nential space to represent parse trees forhighly ambiguous input.
Memoization isknown to improve efficiency, and work byother researchers has had some success inaccommodating left recursion.
This papercombines aspects of previous approachesand presents a method by which parsers canbe built as modular and efficient executablespecifications of ambiguous grammarscontaining unconstrained left recursion.1 IntroductionTop-down parsers can be built as a set of mutually-recursive processes.
Such implementations are mod-ular in the sense that parsers for terminals and simplenon-terminals can be built and tested first.
Subse-quently, parsers for more complex non-terminals canbe constructed and tested.
Koskimies (1990), andNederhof and Koster (1993) discuss this and otheradvantages of top-down parsing.In functional and logic programming, top-downparsers can be built using parser combinators (e.g.see Hutton 1992 for a discussion of the origins ofparser combinators, and Frost 2006 for a discussionof their use in natural-language processing) and def-inite clause grammars (DCGs) respectively.
For ex-ample, consider the following grammar, in whichs stands for sentence, np for nounphrase, vp forverbphrase, and det for determiner:s ::= np vpnp ::= noun | det nounvp ::= verb npdet ::= ?a?
| ?t?noun ::= ?i?
| ?m?
| ?p?
| ?b?verb ::= ?s?A set of parsers for this grammar can be con-structed in the Haskell functional programming lan-guage as follows, where term, ?orelse?, and?thenS?
are appropriately-defined higher-orderfunctions called parser combinators.
(Note thatbackquotes surround infix functions in Haskell).s = np ?thenS?
vpnp = noun ?orelse?
(det ?thenS?
noun)vp = verb ?thenS?
npdet = term ?a?
?orelse?
term ?t?noun = term ?i?
?orelse?
term ?m??orelse?
term ?p??orelse?
term ?b?verb = term ?s?Note that the parsers are written directly in theprogramming language, in code which is similar instructure to the rules of the grammar.
As such,the implementation can be thought of as an exe-cutable specification with all of the associated ad-vantages.
In particular, this approach facilitatesmodular piecewise construction and testing of com-ponent parsers.
It also allows parsers to be definedto return semantic values directly instead of inter-mediate parse results, and parsers to be parameter-ized in order to accommodate context-sensitive lan-109guages (e.g.
Eijck 2003).
Also, in functional pro-gramming, the type checker can be used to catch er-rors in parsers attributed with semantic actions.Parser combinators and DCGs have been used ex-tensively in applications such as prototyping of com-pilers, and the creation of natural language inter-faces to databases, search engines, and web pages,where complex and varied semantic actions areclosely integrated with syntactic processing.
How-ever, both techniques are based on top-down re-cursive descent search with backtracking.
Com-monly used implementations have exponential com-plexity for ambiguous languages, cannot handle left-recursion, and do not produce compact representa-tions of parse trees.
(Note, a left-recursive grammaris one in which a non-terminal p derives an expan-sion p .. headed with a p either directly or indi-rectly.
Application of a parser for such a grammarresults in infinite descent.)
These shortcomings limitthe use of parser combinators and DCGs especiallyin natural-language processing.The problem of exponential time complexity intop-down parsers constructed as sets of mutually-recursive functions has been solved by Norvig(1991) who uses memotables to achieve polynomialcomplexity.
Norvig?s technique is similar to the useof dynamic programming and state sets in Earley?salgorithm (1970), and tables in the CYK algorithmof Cocke, Younger and Kasami.
The basic idea inNorvig?s approach is that when a parser is appliedto the input, the result is stored in a memotable forsubsequent reuse if the same parser is ever reappliedto the same input.
In the context of parser combina-tors, Norvig?s approach can be implemented using afunction memoize to selectively ?memoize?
parsers.In some applications, the problem of left-recursion can be overcome by transforming thegrammar to a weakly equivalent non-left-recursiveform.
(i.e.
to a grammar which derives the same setof sentences).
Early methods of doing this resultedin grammars that are significantly larger than theoriginal grammars.
This problem of grammar sizehas been solved by Moore (2000) who developed amethod, based on a left-corner grammar transforma-tion, which produces non-left recursive grammarsthat are not much larger than the originals.
How-ever, although converting a grammar to a weakly-equivalent form is appropriate in some applications(such as speech recognition) it is not appropriate inother applications.
According to Aho, Sethi, andUllman (1986) converting a grammar to non-left re-cursive form makes it harder to translate expressionscontaining left-associative operators.
Also, in NLPit is easier to integrate semantic actions with parsingwhen both leftmost and rightmost parses of ambigu-ous input are being generated.
For example, con-sider the first of the following grammar rules:np ::= noun | np conj npconj ::= "and" | "or"noun ::= "jim" | "su" | "ali"and its non-left-recursive weakly equivalent form:np ::= noun np?np?
::= conj np np?
| emptyThe non-left-recursive form loses the leftmostparses generated by the left-recursive form.
Inte-grating semantic actions with the non-left-recursiverule in order to achieve the two correct interpre-tations of input such as ["john", "and", "su","or", "ali"] is significantly harder than with theleft-recursive form.Several researchers have recognized the impor-tance of accommodating left-recursive grammars intop-down parsing, in general and in the context ofparser combinators and DCGs in particular, and haveproposed various solutions.
That work is describedin detail in section 3.In this paper, we integrate Norvig?s techniquewith aspects of existing techniques for dealing withleft recursion.
In particular: a) we make use of thelength of the remaining input as does Kuno (1965),b) we keep a record of how many times each parseris applied to each input position in a way that issimilar to the use of cancellation sets by Neder-hof and Koster (1993), c) we integrate memoizationwith a technique for dealing with left recursion asdoes Johnson (1995), and d) we store ?left-recursioncounts?
in the memotable, and encapsulate the mem-oization process in a programming construct calleda monad, as suggested by Frost and Hafiz (2006).Our method includes a new technique for accom-modating indirect left recursion which ensures cor-rect reuse of stored results created through curtail-ment of left-recursive parsers.
We also modify thememoization process so that the memotable repre-sents the potentially exponential number of parsetrees in a compact polynomial sized form using a110technique derived from the chart parsing methods ofKay (1980) and Tomita (1986).As an example use of our method, consider thefollowing ambiguous left-recursive grammar fromTomita (1985) in which pp stands for prepositionalphrase, and prep for preposition.
This grammar isleft recursive in the rules for s and np.
Experimentalresults using larger grammars are given later.s ::= np vp | s ppnp ::= noun | det noun | np pppp ::= prep npvp ::= verb npdet ::= ?a?
| ?t?noun ::= ?i?
| ?m?
| ?p?
| ?b?verb ::= ?s?prep ::= ?n?
| ?w?The Haskell code below defines a parser for theabove grammar, using our combinators:s = memoize "s" ((np ?thenS?
vp)?orelse?
(s ?thenS?
pp))np = memoize "np" (noun?orelse?
(det ?thenS?
noun)?orelse?
(np ?thenS?
pp))pp = memoize "pp" (prep ?thenS?
np)vp = memoize "vp" (verb ?thenS?
np)det = memoize "det" (term ?a??orelse?
term ?t?
)noun = memoize "noun" (term ?i??orelse?
term ?m??orelse?
term ?p??orelse?
term ?b?
)verb = memoize "verb" (term ?s?
)prep = memoize "prep" (term ?n??orelse?
term ?w?
)The following shows the output when theparser function s is applied to the input string"isamntpwab", representing the sentence ?I saw aman in the park with a bat?.
It is a compact rep-resentation of the parse trees corresponding to theseveral ways in which the whole input can be parsedas a sentence, and the many ways in which subse-quences of it can be parsed as nounphrases etc.
Wediscuss this representation in more detail in subsec-tion 4.4.apply s "isamntpwab" =>"noun"1 ((1,2), [Leaf "i"])4 ((4,5), [Leaf "m"])7 ((7,8), [Leaf "p"])10 ((10,11), [Leaf "b"])"det"3 ((3,4), [Leaf "a"])6 ((6,7), [Leaf "t"])9 ((9,10), [Leaf "a"])"np"1 ((1,2), [SubNode ("noun", (1,2))])3 ((3,5), [Branch [SubNode ("det", (3,4)),SubNode ("noun",(4,5))]])((3,8), [Branch [SubNode ("np", (3,5)),SubNode ("pp", (5,8))]])((3,11),[Branch [SubNode ("np", (3,5)),SubNode ("pp", (5,11))],Branch [SubNode ("np", (3,8)),SubNode ("pp", (8,11))]])6 ((6,8), [Branch [SubNode ("det", (6,7)),SubNode ("noun",(7,8))]])((6,11),[Branch [SubNode ("np", (6,8)),SubNode ("pp", (8,11))]])9 ((9,11),[Branch [SubNode ("det", (9,10)),SubNode ("noun",(10,11))]])"prep"5 ((5,6), [Leaf "n"])8 ((8,9), [Leaf "w"])"pp"8 ((8,11),[Branch [SubNode ("prep",(8,9)),SubNode ("np", (9,11))]])5 ((5,8), [Branch [SubNode ("prep",(5,6)),SubNode ("np", (6,8))]])((5,11),[Branch [SubNode ("prep",(5,6)),SubNode ("np", (6,11))]])"verb"2 ((2,3), [Leaf "s"])"vp"2 ((2,5), [Branch [SubNode ("verb",(2,3)),SubNode ("np", (3,5))]])((2,8), [Branch [SubNode ("verb",(2,3)),SubNode ("np", (3,8))]])((2,11),[Branch [SubNode ("verb",(2,3)),SubNode ("np", (3,11))]])"s"1 ((1,5), [Branch [SubNode ("np", (1,2)),SubNode ("vp", (2,5))]])((1,8), [Branch [SubNode ("np", (1,2)),SubNode ("vp", (2,8))],Branch [SubNode ("s", (1,5)),SubNode ("pp", (5,8))]])((1,11),[Branch [SubNode ("np", (1,2)),SubNode ("vp", (2,11))],Branch [SubNode ("s", (1,5)),SubNode ("pp", (5,11))],Branch [SubNode ("s", (1,8)),SubNode ("pp", (8,11))]]Our method has two disadvantages: a) it hasO(n4) time complexity, for ambiguous grammars,compared with O(n3) for Earley-style parsers (Ear-ley 1970), and b) it requires the length of the inputto be known before parsing can commence.Our method maintains all of the advantages oftop-down parsing and parser combinators discussedearlier.
In addition, our method accommodates ar-bitrary context-free grammars, terminates correctlyand correctly reuses results generated by direct andindirect left recursive rules.
It parses ambiguous lan-guages in polynomial time and creates polynomial-sized representations of parse trees.In many applications the advantages of our ap-proach will outweigh the disadvantages.
In particu-lar, the additional time required for parsing will notbe a major factor in the overall time required whensemantic processing, especially of ambiguous input,is taken into account.111We begin with some background material, show-ing how our approach relates to previous work byothers.
We follow that with a detailed description ofour method.
Sections 5, 6, and 7 contain informalproofs of termination and complexity, and a briefdescription of a Haskell implementation of our al-gorithm.
Complete proofs and the Haskell code areavailable from any of the authors.We tested our implementation on four natural-language grammars from Tomita (1986), and onfour abstract highly-ambiguous grammars.
The re-sults, which are presented in section 8, indicate thatour method is viable for many applications, espe-cially those for which parser combinators and defi-nite clause grammars are particularly well-suited.We present our approach with respect to parsercombinators.
However, our method can also be im-plemented in other languages which support recur-sion and dynamic data structures.2 Top-Down Backtracking RecognitionTop-down recognizers can be implemented as a setof mutually recursive processes which search forparses using a top-down expansion of the gram-mar rules defining non-terminals while looking formatches of terminals with tokens on the input.
To-kens are consumed from left to right.
Backtrack-ing is used to expand all alternative right-hand-sidesof grammar rules in order to identify all possibleparses.
In the following we assume that the inputis a sequence of tokens input, of length l inputthe members of which are accessed through an in-dex j.
Unlike commonly-used implementations ofparser combinators, which produce recognizers thatmanipulate subsequences of the input, we assume,as in Frost and Hafiz (2006), that recognizers arefunctions which take an index j as argument andwhich return a set of indices as result.
Each indexin the result set corresponds to the position at whichthe recognizer successfully finished recognizing asequence of tokens that began at position j .
Anempty result set indicates that the recognizer failedto recognize any sequence beginning at j. Multipleresults are returned for ambiguous input.According to this approach, a recognizer term tfor a terminal t is a function which takes an indexj as input, and if j is greater than l input, the rec-ognizer returns an empty set.
Otherwise, it checksto see if the token at position j in the input corre-sponds to the terminal t. If so, it returns a singletonset containing j + 1, otherwise it returns the emptyset.
For example, a basic recognizer for the termi-nal ?s?
can be defined as follows (note that we use afunctional pseudo code throughout, in order to makethe paper accessible to a wide audience.
We also usea list lookup offset of 1):term_s = term ?s?where term t j= {} , if j > l_input= {j + 1}, if jth element of input = t= {} , otherwiseThe empty recognizer is a function which alwayssucceeds returning its input index in a set:empty j = {j}A recognizer corresponding to a construct p | qin the grammar is built by combining recognizersfor p and q, using the parser combinator ?orelse?.When the composite recognizer is applied to indexj, it applies p to j, then it applies q to j, and subse-quently unites the resulting sets.
:(p ?orelse?
q) j = unite (p j) (q j)e.g, assuming that the input is "ssss", then(empty ?orelse?
term_s) 2 => {2, 3}A composite recognizer corresponding to a se-quence of recognizers p q on the right hand side ofa grammar rule, is built by combining those recog-nizers using the parser combinator ?thenS?.
Whenthe composite recognizer is applied to an index j, itfirst applies p to j, then it applies q to each index inthe set of results returned by p. It returns the unionof these applications of q.
(p ?thenS?
q) j = union (map q (p j))e.g., assuming that the input is "ssss", then(term_s ?thenS?
term_s) 1 => {3}The combinators above can be used to definecomposite mutually-recursive recognizers.
For ex-ample, the grammar sS ::= ?s?
sS sS | emptycan be encoded as follows:sS = (term_s ?thenS?
sS ?thenS?
sS)?orelse?
emptyAssuming that the input is "ssss", the recognizersS returns a set of five results, the first four corre-sponding to proper prefixes of the input being rec-ognized as an sS.
The result 5 corresponds to thecase where the whole input is recognized as an sS.112sS 1 => {1, 2, 3, 4, 5}The method above does not terminate for left-recursive grammars, and has exponential timecomplexity with respect to l input for non-left-recursive grammars.
The complexity is due to thefact that recognizers may be repeatedly applied tothe same index during backtracking induced by theoperator ?orelse?.
We show later how complexitycan be improved, using Norvig?s memoization tech-nique.
We also show, in section 4.4, how the com-binators term, ?orelse?, and ?thenS?
can be re-defined so that the processors create compact repre-sentations of parse trees in the memotable, with noeffect on the form of the executable specification.3 Left Recursion and Top-Down ParsingSeveral researchers have proposed ways in whichleft-recursion and top-down parsing can coexist:1) Kuno (1965) was the first to use the length ofthe input to force termination of left-recursive de-scent in top-down parsing.
The minimal lengths ofthe strings generated by the grammar on the contin-uation stack are added and when their sum exceedsthe length of the remaining input, expansion of thecurrent non-terminal is terminated.
Dynamic pro-gramming in parsing was not known at that time,and Kuno?s method has exponential complexity.2) Shiel (1976) recognized the relationship be-tween top-down parsing and the use of state setsand tables in Earley and SYK parsers and developedan approach in which procedures corresponding tonon-terminals are called with an extra parameter in-dicating how many terminals they should read fromthe input.
When a procedure corresponding to anon-terminal n is applied, the value of this extra pa-rameter is partitioned into smaller values which arepassed to the component procedures on the right ofthe rule defining n. The processor backtracks whena procedure defining a non-terminal is applied withthe same parameter to the same input position.
Themethod terminates for left-recursion but has expo-nential complexity.3) Leermakers (1993) introduced an approachwhich accommodates left-recursion through ?recur-sive ascent?
rather than top-down search.
Althoughachieving polynomial complexity through memoiza-tion, the approach no longer has the modularity andclarity associated with pure top-down parsing.
Leer-makers did not extend his method to produce com-pact representations of trees.4) Nederhof and Koster (1993) introduced ?can-cellation?
parsing in which grammar rules are trans-lated into DCG rules such that each DCG non-terminal is given a ?cancellation set?
as an extraargument.
Each time a new non-terminal is de-rived in the expansion of a rule, this non-terminalis added to the cancellation set and the resulting setis passed on to the next symbol in the expansion.If a non-terminal is derived which is already in theset then the parser backtracks.
This technique pre-vents non-termination, but loses some parses.
Tosolve this, for each non-terminal n, which has a left-recursive alternative 1) a function is added to theparser which places a special token n at the frontof the input to be recognized, 2) a DCG correspond-ing to the rule n ::= n is added to the parser, and3) the new DCG is invoked after the left-recursiveDCG has been called.
The approach accommodatesleft-recursion and maintains modularity.
An exten-sion to it also accommodates hidden left recursionwhich can occur when the grammar contains ruleswith empty right-hand sides.
The shortcoming ofNederhof and Koster?s approach is that it is expo-nential in the worst case and that the resulting codeis less clear as it contains additional production rulesand code to insert the special tokens.5) Lickman (1995) defined a set of parser com-binators which accommodate left recursion.
Themethod is based on an idea by Philip Wadler in anunpublished paper in which he claimed that fixedpoints could be used to accommodate left recursion.Lickman implemented Wadler?s idea and provideda proof of termination.
The method accommodatesleft recursion and maintains modularity and clarityof the code.
However, it has exponential complex-ity, even for recognition.6) Johnson (1995) appears to have been the firstto integrate memoization with a method for dealingwith left recursion in pure top-down parsing.
Thebasic idea is to use the continuation-passing styleof programming (CPS) so that the parser computesmultiple results, for ambiguous cases, incrementally.There appears to have been no attempt to extendJohnson?s approach to create compact representa-tions of parse trees.
One explanation for this could113be that the approach is somewhat convoluted and ex-tending it appears to be very difficult.
In fact, John-son states, in his conclusion, that ?an implemen-tation attempt (to create a compact representation)would probably be very complicated.
?7) Frost and Hafiz (2006) defined a set of parsercombinators which can be used to create polynomialtime recognizers for grammars with direct left recur-sion.
Their method stores left-recursive counts in thememotable and curtails parses when a count exceedsthe length of the remaining input.
Their method doesnot accommodate indirect left recursion, nor does itcreate parse trees.Our new method combines many of the ideas de-veloped by others: as with the approach of Kuno(1965) we use the length of the remaining input tocurtail recursive descent.
Following Shiel (1976),we pass additional information to parsers which isused to curtail recursion.
The information that wepass to parsers is similar to the cancellation setsused by Nederhof and Koster (1993) and includesthe number of times a parser is applied to each inputposition.
However, in our approach this informa-tion is stored in a memotable which is also used toachieve polynomial complexity.
Although Johnson(1995) also integrates a technique for dealing withleft recursion with memoization, our method dif-fers from Johnson?s O(n3) approach in the techniquethat we use to accommodate left recursion.
Also,our approach facilitates the construction of com-pact representations of parse trees whereas John-son?s appears not to.
In the Haskell implementationof our algorithm, we use a functional programmingstructure called a monad to encapsulate the detailsof the parser combinators.
Lickman?s (1995) ap-proach also uses a monad, but for a different pur-pose.
Our algorithm stores ?left-recursion counts?in the memotable as does the approach of Frostand Hafiz (2006).
However, our method accommo-dates indirect left recursion and can be used to createparsers, whereas the method of Frost and Hafiz canonly accommodate direct left recursion and createsrecognizers not parsers.4 The New MethodWe begin by describing how we improve complex-ity of the recognizers defined in section 2.
We thenshow how to accommodate direct and indirect leftrecursion.
We end this section by showing how rec-ognizers can be extended to parsers.4.1 MemoizationAs in Norvig (1991) a memotable is constructed dur-ing recognition.
At first the table is empty.
Duringthe process it is updated with an entry for each rec-ognizer r i that is applied.
The entry consists of a setof pairs, each consisting of an index j at which therecognizer r i has been applied, and a set of resultsof the application of r i to j.The memotable is used as follows: whenever arecognizer r i is about to be applied to an index j,the memotable is checked to see if that recognizerhas ever been applied to that index before.
If so,the results from the memotable are returned.
If not,the recognizer is applied to the input at index j, thememotable is updated, and the results are returned.For non-left-recursive recognizers, this process en-sures that no recognizer is ever applied to the sameindex more than once.The process of memoization is achieved throughthe function memoize which is defined as follows,where the update function stores the result of rec-ognizer application in the table:memoize label r_i j= if lookup label j succeeds,return memotable resultelse apply r_i to j,update table, and return resultsMemoized recognizers, such as the following,have cubic complexity (see later):msS = memoize "msS"((ms ?thenS?
msS?thenS?
msS)?orelse?
empty)ms = memoize "ms" term_s4.2 Accommodating direct left recursionIn order to accommodate direct left recursion, we in-troduce a set of values c ij denoting the number oftimes each recognizer r i has been applied to the in-dex j.
For non-left-recursive recognizers this ?left-rec count?
will be at most one, as the memotablelookup will prevent such recognizers from ever be-ing applied to the same input twice.
However, forleft-recursive recognizers, the left-rec count is in-creased on recursive descent (owing to the fact thatthe memotable is only updated on recursive ascent114after the recognizer has been applied).
Applicationof a recognizer r to an index j is failed whenever theleft-rec count exceeds the number of unconsumedtokens of the input plus 1.
At this point no parse ispossible (other than spurious parses which could oc-cur with circular grammars ?
which we want to re-ject).
As illustration, consider the following branchbeing created during the parse of two remaining to-kens on the input (where N, P and Q are nodes in theparse search space corresponding to non-terminals,and A, B and C to terminals or non-terminals):N/ \N A/ \N B/ \P C/Q/NThe last call of the parser for N should be failedowing to the fact that, irrespective of what A, B, andC are, either they must require at least one input to-ken, otherwise they must rewrite to empty.
If theyall require a token, then the parse cannot succeed.
Ifany of them rewrite to empty, then the grammar iscircular (N is being rewritten to N) and the last callshould be failed in either case.Note that failing a parse when a branch is longerthan the length of the remaining input is incorrect asthis can occur in a correct parse if recognizers arerewritten into other recognizers which do not have?token requirements to the right?.
For example, wecannot fail the parse at P or Q as these could rewriteto empty without indicating circularity.
Also notethat we curtail the recognizer when the left-rec countexceeds the number of unconsumed tokens plus 1.The plus 1 is necessary to accommodate the casewhere the recognizer rewrites to empty on applica-tion to the end of the input.To make use of the left-rec counts, we simplymodify the memoize function to refer to an addi-tional table called ctable which contains the left-rec counts c ij, and to check and increment thesecounters at appropriate points in the computation:if the memotable lookup for the recognizer r i andthe index j produces a result, that result is returned.However, if the memotable does not contain a resultfor that recognizer and that index, c ij is checkedto see if the recognizer should be failed becauseit has descended too far through left-recursion.
Ifso, memoize returns an empty set as result with thememotable unchanged.
Otherwise, the counter c ijis incremented and the recognizer r i is applied to j,and the memotable is updated with the result beforeit is returned.
The function memoize defined below,can now be applied to rules with direct left recursion.memoize label r_i j =if lookup label j succeedsreturn memotable resultselse if c_ij > (l_input)-j+1, return {}else increment c_ij, apply r_i to j,update memotable,and return results4.3 Accommodating indirect left recursionWe begin by illustrating how the method describedabove may return incomplete results for grammarscontaining indirect left recursion.Consider the following grammar, and subset ofthe search space, where the left and right branchesrepresent the expansions of the first two alternateright-hand-sides of the rule for the non terminal S,applied to the same position on the input:S ::= S then ..| Q | P | x SP ::= S then .
/ \Q ::= T S then .. QT ::= P | |S then .. T| |P P| |S then.. S then ..|fail SSuppose that the left branch occurs before theright branch, and that the left branch was failed dueto the left-rec count for S exceeding its limit.
Theresults stored for P on recursive ascent of the leftbranch would be an empty set.
The problem is thatthe later call of P on the right branch should not reusethe empty set of results from the first call of P as theyare incomplete with respect to the position of P onthe right branch (i.e.
if P were to be re-applied to theinput in the context of the right branch, the resultswould not necessarily be an empty set).
This prob-lem is a result of the fact that S caused curtailmentof the results for P as well as for itself.
This problemcan be solved as follows:1151) Pass left-rec contexts down the parse space.
Weneed additional information when storing and con-sidering results for reuse.
We begin by defining the?left-rec-context?
of a node in the parse search spaceas a list of the following type, containing for each in-dex, the left-rec count for each recognizer, includingthe current recognizer, which have been called in thesearch branch leading to that node:[(index,[(recog label,left rec count)])]2) Generate the reasons for curtailment whencomputing results.
For each result we need to knowif the subtrees contributing to it have been curtailedthrough a left-rec limits, and if so, which recogniz-ers, at which indices, caused the curtailment.
A listof (recog label, index) pairs which caused cur-tailment in any of the subtrees is returned with theresult.
?orelse?
and ?thenS?
are modified, accord-ingly, to merge these lists, in addition to merging theresults from subtrees.3) Store results in the memotable together with asubset of the current left-rec context correspondingto those recognizers which caused the curtailment.When a result is to be stored in the memotable fora recognizer P, the list of recognizers which causedcurtailment (if any) in the subtrees contributing tothis result is examined.
For each recognizer S whichcaused curtailment at some index, the current left-rec counter for S at that index (in the left-rec contextfor P) is stored with the result for P. This means thatthe only part of the left-rec context of a node, that isstored with the result for that node, is a list of thoserecognizers and current left-rec counts which had aneffect on curtailing the result.
The limited left-reccontext which is stored with the result is called the?left-rec context of the result?.4) Consider results for reuse.
Whenever a mem-otable result is being considered for reuse, the left-rec-context of that result is compared with the left-rec-context of the current node in the parse search.The result is only reused if, for each recognizer andindex in the left-rec context of the result, the left-rec-count is smaller than or equal to the left-rec-countof that recognizer and index in the current context.This ensures that a result stored for some applicationP of a recognizer at index j is only reused by a sub-sequent application P?
of the same recognizer at thesame position, if the left-rec context for P?
wouldconstrain the result more, or equally as much, as ithad been constrained by the left-rec context for P atj.
If there were no curtailment, the left-rec contextof a result would be empty and that result can bereused anywhere irrespective of the current left-reccontext.4.4 Extending recognizers to parsersInstead of returning a list of indices representingsuccessful end points for recognition, parsers alsoreturn the parse trees.
However, in order that thesetrees be represented in a compact form, they are con-structed with reference to other trees that are storedin the memotable, enabling the explicit sharing ofcommon subtrees, as in Kay?s (1980) and Tomita?s(1986) methods.
The example in section 1 illustratesthe results returned by a parser.Parsers for terminals return a leaf value togetherwith an endpoint, stored in the memotable as illus-trated below, indicating that the terminal "s" wasidentified at position 2 on the input:"verb" 2 ((2,3),[Leaf "s"])The combinator ?thenS?
is extended so thatparsers constructed with it return parse trees whichare represented using reference to their immediatesubtrees.
For example:"np" ......3 ((3,5),[Branch[SubNode("det", (3,4)),SubNode("noun",(4,5))]])This memotable entry shows that a parse tree for anounphrase "np" has been identified, starting at po-sition 3 and finishing at position 5, and which con-sists of two subtrees, corresponding to a determinerand a noun.The combinator ?orelse?
unites results from twoparsers and also groups together trees which havethe same begin and end points.
For example:"np" .........3 ((3,5),[Branch[SubNode("det", (3,4)),SubNode("noun",(4,5))]])((3,8), [Branch[SubNode("np", (3,5)),SubNode("pp", (5,8))]])((3,11),[Branch[SubNode("np", (3,5)),SubNode("pp", (5,11))],Branch[SubNode("np", (3,8)),SubNode("pp", (8,11))]])116which shows that four parses of a nounphrase "np"have been found starting at position 3, two of whichshare the endpoint 11.An important feature is that trees for the samesyntactic category having the same start/end pointsare grouped together and it is the group that is re-ferred to by other trees of which it is a constituent.For example, in the following the parse tree for a"vp" spanning positions 2 to 11 refers to a group ofsubtrees corresponding to the two parses of an "np"both of which span positions 3 to 11:"vp" 2 (["np"],[])((2,5), [Branch[SubNode("verb",(2,3)),SubNode("np", (3,5))]])((2,8), [Branch[SubNode("verb",(2,3)),SubNode("np", (3,8))]])((2,11),[Branch[SubNode("verb",(2,3)),SubNode("np", (3,11))]])5 TerminationThe only source of iteration is in recursive functioncalls.
Therefore, proof of termination is based onthe identification of a measure function which mapsthe arguments of recursive calls to a well-foundedascending sequence of integers.Basic recognizers such as term ?i?
and the rec-ognizer empty have no recursion and clearly termi-nate for finite input.
Other recognizers that are de-fined in terms of these basic recognizers, throughmutual and nested recursion, are applied by thememoize function which takes a recognizer and anindex j as input and which accesses the memotable.An appropriate measure function maps the index andthe set of left?rec values to an integer, which in-creases by at least one for each recursive call.
Thefact that the integer is bounded by conditions im-posed on the maximum value of the index, the max-imum values of the left-rec counters, and the max-imum number of left-rec contexts, establishes ter-mination.
Extending recognizers to parsers doesnot involve any additional recursive calls and conse-quently, the proof also applies to parsers.
A formalproof is available from any of the authors.6 ComplexityThe following is an informal proof.
A formal proofis available from any of the authors.We begin by showing that memoized non-left-recursive and left-recursive recognizers have aworst-case time complexities of O(n3) and O(n4) re-spectively, where n is the number of tokens in theinput.
The proof proceeds as follows: ?orelse?requires O(n) operations to merge the results fromtwo alternate recognizers provided that the indicesare kept in ascending order.
?then?
involves O(n2)operations when applying the second recognizer ina sequence to the results returned by the first rec-ognizer.
(The fact that recognizers can have mul-tiple alternatives involving multiple recognizers insequence increases cost by a factor that depends onthe grammar, but not on the length of the input).
Fornon-left-recursive recognizers, memoize guaranteesthat each recognizer is applied at most once to eachinput position.
It follows that non-left recursive rec-ognizers have O(n3) complexity.
Recognizers withdirect left recursion can be applied to the same inputposition at most n times.
It follows that such recog-nizers have O(n4) complexity.
In the worst case arecognizer with indirect left recursion could be ap-plied to the same input position n * nt times wherent is the number of nonterminals in the grammar.This worst case would occur when every nontermi-nal was involved in the path of indirect recursion forsome nonterminal.
Complexity remains O(n4).The only difference between parsers and recog-nizers is that parsers construct and store parts ofparse trees rather than end points.
We extend thecomplexity analysis of recognizers to that of parsersand show that for grammars in Chomsky NormalForm (CNF) (i.e.
grammars whose right-hand-sideshave at most two symbols, each of which can be ei-ther a terminal or a non-terminal), the complexityof non-left recursive parsers is O(n3) and of left-recursive parsers it is O(n4).
The analysis begins bydefining a ?parse tuple?
consisting of a parser namep, a start/end point pair (s, e), and a list of parsernames and end/point pairs corresponding to the firstlevel of the parse tree returned by p for the sequenceof tokens from s to e. (Note that this corresponds toan entry in the compact representation).
The anal-ysis then considers the effect of manipulating setsof parse tuples, rather than endpoints which are thevalues manipulated by recognizers.
Parsers corre-sponding to grammars in CNF will return, in theworst case, for each start/end point pair (s, e) , (((e -s) + 1) * t2) parse tuples, where t is the number of ter-minals and non-terminals in the grammar.
It follows117that there are O(n) parse tuples for each parser andbegin/endpoint pair.
Each parse tuple correspondsto a bi-partition of the sequence starting at s and fin-ishing at e by two parsers (possibly the same) fromthe set of parsers corresponding to terminals andnon-terminals in the grammar.
It is these parse tu-ples that are manipulated by ?orelse?
and ?thenS?.The only effect on complexity of these operations isto increase the complexity of ?orelse?
from O(n)to O(n2), which is the same as the complexity of?thenS?.
Owing to the fact that the complexity of?thenS?
had the highest degree in the application ofa compound recognizer to an index, increasing thecomplexity of ?orelse?
to the same degree in pars-ing has no effect on the overall complexity of theprocess.The representation of trees in the memotable hasone entry for each parser.
In the worst case, whenthe parser is applied to every index, the entry hasn sub-entries, corresponding to n begin points.
Foreach of these sub-entries there are up to n sub-sub-entries, each corresponding to an end point of theparse.
Each of these sub-entries contains O(n) parsetuples as discussed above.
It follows that the size ofthe compact representation is O(n3).7 ImplementationWe have implemented our method in the pure func-tional programming language Haskell.
We use amonad (Wadler 1995) to implement memoization.Use of a monad allows the memotable to be sys-tematically threaded through the parsers while hid-ing the details of table update and reuse, allowinga clean and simple interface to be presented to theuser.
The complete Haskell code is available fromany of the authors.8 Experimental ResultsIn order to provide evidence of the low-order poly-nomial costs and scalability of our method, we con-ducted a limited evaluation with respect to fourpractical natural-language grammars used by Tomita(Appendix F, 1986) when comparing his algorithmwith Earley?s, and four variants of an abstract highlyambiguous grammar from Aho and Ullman (1972).Our Haskell program was compiled using the Glas-gow Haskell Compiler 6.6 (the code has not yet beentuned to obtain the best performance from this pat-form).
We used a 3GHz/1GB PC in our experiments.8.1 Tomita?s GrammarsThe Tomita grammars used were: G1 (8 rules), G2(40 rules), G3 (220 rules), and G4 (400 rules).
Weused two sets of input: a) the three most-ambiguousinputs from Tomita?s sentence set 1 (Appendix G)of lengths 19, 26, and 26 which we parsed withG3 (as did Tomita), and b) three inputs of lengths4, 10, and 40, with systematically increasingambiguity, chosen from Tomita?s sentence set 2,which he generated automatically using the formula:noun verb det noun (prep det noun)?The results, which are tabulated in figure 1,show our timings and those recorded by Tomita forhis original algorithm and for an improved Earleymethod, using a DEC-20 machine (Tomita 1986,Appendix D).Considered by themselves our timings are lowenough to suggest that our method is feasible foruse in small to medium applications, such as NL in-terfaces to databases or rhythm analysis in poetry.Such applications typically have modest grammars(no more than a few hundred rules) and are not re-quired to parse huge volumes of input.Clearly there can be no direct comparison againstyears-old DEC-20 times, and improved versions ofboth of these algorithms do exist.
However, we pointto some relevant trends in the results.
The increasesin times for our method roughly mirror the increasesshown for Tomita?s algorithm, as grammar complex-ity and/or input size increase.
This suggests that ouralgorithm scales adequately well, and not dissimi-larly to the earlier algorithms.8.2 Highly ambiguous abstract grammarsWe defined four parsers as executable specifica-tions of four variants of a highly-ambiguous gram-mar introduced by Aho and Ullman (1972) whendiscussing ambiguity: an unmemoized non-left?recursive parser s, a memoized version ms, a memo-ized left?recursive version sml, and a left?recursiveversion with all parts memoized.
(This improvesefficiency similarly to converting the grammar toChomsky Normal Form.
):118Input No.
of Our algorithm (complete parsing)-PC Tomitas (complete parsing)-DEC 20 Earleys (recognition only)-DEC 20length Parses G1 G2 G3 G4 G1 G2 G3 G4 G1 G2 G3 G4Input from Tomitas sentence set 1.
Timings are in seconds.19 346 0.02 4.79 7.6626 1,464 0.03 8.66 14.65Input from Tomitas sentence set 2.
Timings are in seconds.22 429 0.00 0.00 0.03 0.05 2.80 6.40 4.74 19.93 2.04 7.87 7.25 42.7531 16,796 0.00 0.02 0.05 0.09 6.14 14.40 10.40 45.28 4.01 14.09 12.06 70.7440 742,900 0.03 0.08 0.11 0.14 11.70 28.15 18.97 90.85 6.75 22.42 19.12 104.91Figure 1: Informal comparison with Tomita/Earley resultss = (term ?a?
?thenS?
s ?thenS?
s)?orelse?
emptysm = memoize "sm"((term ?a?
?thenS?
sm ?thenS?
sm)?orelse?
empty)sml = memoize "sml"((sml ?thenS?
sml?thenS?
term ?a?)?orelse?
empty)smml = memoize "smml"((smml ?thenS?
(memoize "smml_a"(smml ?thenS?
term ?a?)))?orelse?
empty)We chose these four grammars as they are highlyambiguous.
According to Aho and Ullman (1972),s generates over 128 billion complete parses of aninput consisting of 24 ?a??s.
Although the left-recursive grammar does not generate exactly thesame parses, it generates the same number of parses,as it matches a terminal at the end of the rule ratherthan at the start.Input No.
of parses Seconds to generate thelength excluding packed representationpartial parses of full and partial parsess sm sml smml6 132 1.22 0.00 0.00 0.0012 208,012 out of 0.00 0.00 0.02space24 1.29e+12 0.08 0.13 0.0648 1.313e+26 0.83 0.97 0.80Figure 2: Times to compute forest for nThese results show that our method can accom-modate massively-ambiguous input involving thegeneration of large and complex parse forests.
Forexample, the full forest for n=48 contains 1,225choice nodes and 19,600 branch nodes.
Note alsothat the use of more memoization in smml reducesthe cost of left-rec checking.9 Concluding CommentsWe have extended previous work of others on mod-ular parsers constructed as executable specifica-tions of grammars, in order to accommodate am-biguity and left recursion in polynomial time andspace.
We have implemented our method as a set ofparser combinators in the functional programminglanguage Haskell, and have conducted experimentswhich demonstrate the viability of the approach.The results of the experiments suggest that ourmethod is feasible for use in small to medium ap-plications which need parsing of ambiguous gram-mars.
Our method, like other methods which useparser combinators or DCGs, allows parsers to becreated as executable specifications which are ?em-bedded?
in the host programming language.
It isoften claimed that this embedded approach is moreconvenient than indirect methods which involve theuse of separate compiler tools such as yacc, for rea-sons such as support from the host language (includ-ing type checking) and ease of use.
The major ad-vantage of our method is that it increases the typeof grammars that can be accommodated in the em-bedded style, by supporting left recursion and ambi-guity.
This greatly increases what can be done inthis approach to parser construction, and removesthe need for non-expert users to painfully rewriteand debug their grammars to avoid left recursion.We believe such advantages balance well against anyreduction in performance, especially when an appli-cation is being prototyped.The Haskell implementation is in its initial stage.We are in the process of modifying it to improve ef-ficiency, and to make better use of Haskell?s lazyevaluation strategy (e.g.
to return only the first nsuccessful parses of the input).Future work includes proof of correctness, analy-sis with respect to grammar size, testing with largernatural language grammars, and extending the ap-119proach so that language evaluators can be con-structed as modular executable specifications of at-tribute grammars.AcknowledgementsRichard Frost acknowledges the support providedby the Natural Sciences and Engineering ResearchCouncil of Canada in the form of a discovery grant.References1.
Aho, A. V. and Ullman, J. D. (1972) The Theory ofParsing, Translation, and Compiling.
Volume I: Parsing.Prentice-Hall.2.
Aho, A. V., Sethi, R. and Ullman, J. D. (1986) Compil-ers: Principles, Techniques and Tools.
Addison-WesleyLongman Publishing Co.3.
Camarao, C., Figueiredo, L. and Oliveira, R.,H.
(2003)Mimico: A Monadic Combinator Compiler Generator.Journal of the Brazilian Computer Society 9(1).4.
Earley, J.
(1970) An efficient context-free parsing algo-rithm.Comm.
ACM 13(2) 94?102.5.
Eijck, J. van (2003) Parser combinators for extraction.
InPaul Dekker and Robert van Rooy, editors, Proceedingsof the Fourteenth Amsterdam Colloqium ILLC, Univer-sity of Amsterdam.
99?104.6.
Frost, R. A.
(2006) Realization of Natural-Language In-terfaces using Lazy Functional Programming.
ACM Com-put.
Surv.
38(4).7.
Frost, R. A. and Hafiz, R. (2006) A New Top-Down Pars-ing Algorithm to Accommodate Ambiguity and Left Re-cursion in Polynomial Time.
SIGPLAN Notices 42 (5)46?54.8.
Hutton, G. (1992) Higher-order functions for parsing.
J.Functional Programming 2 (3) 323?343.9.
Johnson, M. (1995) Squibs and Discussions: Memo-ization in top-down parsing.
Computational Linguistics21(3) 405?417.10.
Kay, M. (1980) Algorithm schemata and data structures insyntactic processing.
Technical Report CSL-80?12, XE-ROX Palo Alto Research Center.11.
Koskimies, K. (1990) Lazy recursive descent parsing formodular language implementation.
Software Practiceand Experience 20 (8) 749?772.12.
Kuno, S. (1965) The predictive analyzer and a path elim-ination technique.
Comm.
ACM 8(7) 453?462.13.
Leermakers, R. (1993) The Functional Treatment of Pars-ing.
Kluwer Academic Publishers, ISBN0?7923?9376?7.14.
Lickman, P. (1995) Parsing With Fixed Points.
Master?sThesis, University of Cambridge.15.
Moore, R. C. (2000) Removing left recursion fromcontext-free grammars.
In Proceedings, 1st Meetingof the North American Chapter of the Association forComputational Linguistics, Seattle, Washington, ANLP?NAACL 2000.
249?255.16.
Nederhof, M. J. and Koster, C. H. A.
(1993) Top-DownParsing for Left-recursive Grammars.
TechnicalReport93?10 Research Institute for Declarative Systems, De-partment of Informatics, Faculty of Mathematics and In-formatics, Katholieke Universiteit, Nijmegen.17.
Norvig, P. (1991) Techniques for automatic memoisationwith applications to context-free parsing.
ComputationalLinguistics 17(1) 91?98.18.
Shiel, B.
A.
(1976) Observations on context-free pars-ing.
Technical Report TR 12?76, Center for Researchin Computing Technology, Aiken Computational Labo-ratory, Harvard University.19.
Tomita, M. (1986) Efficient Parsing for Natural Lan-guage: A Fast Algorithm for Practical Systems.
KluwerAcademic Publishers, Boston, MA.20.
Wadler, P. (1995) Monads for functional programming,Proceedings of the Baastad Spring School on AdvancedFunctional Programming, ed J. Jeuring and E. Meijer.Springer Verlag LNCS 925.120
