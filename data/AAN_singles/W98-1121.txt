POS Tagging versus Classes in Language Model ingPeter A. HeemanComputer Science and EngineeringOregon Graduate InstitutePO Box 91000 Portland OR 97291heeman@cse, ogi.
eduAbstractLanguage models for speech recognition concen-Irate solely on recognizing the words that were spo-ken.
In this paper, we advocate redefining thespeech recognition problem so that its goal is to findboth the best sequence of words and their POS tags,and thus incorporate POS tagging.
The use of POStags allows more sophisticated generalizations thanare afforded by using a class-based approach.
Fur-thermore, if we want to incorporate speech repairand intonational phrase modeling into the languagemodel, using POS tags rather than classes gives bet-ter performance in this task.1 IntroductionFor recognizing spontaneous speech, the acousticsignal is to weak to narrow down the number ofword candidates.
Hence, speech recognizers em-ploy a language model that prunes out acoustic al-ternatives by taking into account the previous wordsthat were recognized.
In doing this, the speechrecognition problem is viewed as finding the mostlikely word sequence 12?
given the acoustic signal(Jelinek, 1985).if" = argmwaX Pr(WIA ) (1)We can rewrite the above using Bayes' rule.14 r = argmax Pr(AIW) Pr(W) (2)w Pr(A)Since Pr(A) is independent of the choice of W, wesimplify the above as follows.l~V = argr~axPr(AIW)Pr(W) (3)The first term, Pr(AIW), is the acoustic model andthe second term, Pr(W), is the lanffuage model,which assigns aprobability to the sequence of wordsW.
We can rewrite W explicitly as a sequence ofwords W1W2W3... WN, where N is the number ofwords in the sequence.
For expository ease, we usethe notation Wid to refer to the sequence of wordsWi to Wj.
We now use the definition of conditionalprobabilities to rewrite Pr(Wi,N) as follows.NPr(W1,N) = H Pr(Wi\[Wu-1) (4)i= lTo estimate the probability distribution, a train-ing corpus is typically used from which the proba-bilities can be estimated using relative frequencies.Due to sparseness of data, one must define equiv-alence classes amongst the contexts WLi-1, whichcan be done by limiting the context o an n-gramlanguage model (Jelinek, 1985).
One can also mixin smaller size language models when there is notenough data to support he larger context by usingeither interpolated estimation (Jelinek and Mercer,1980) or a backoff approach (Katz, 1987).
A way ofmeasuring the effectiveness of the estimated proba-bility distribution is to measure the perplexity hat itassigns to a test corpus (Bahl et al, 1977).
Perplex-ity is an estimate of how well the language modelis able to predict he next word of a test corpus interms of the number of alternatives that need to beconsidered ateach point.
The perplexity of a test setwl,N is calculated as 2 t't, where H is the entropy,which is defined as follows.N 1 H = l r(w, lwL,-1) (5)i--11.1 Class-based Language ModelsThe choice of equivalence classes for a languagemodel need not be the previous words.
Wordscan be grouped into classes, and these classes canbe used as the basis of the equivalence classes ofthe context rather than the word identities (Jelinek,1985).
Below we give the equation usually used fora class-based trigram model, where the function gmaps each word to its unambiguous class.Pr(W/IWu.a ) ~ Pr(W~lg(W~) ) Pr(g(W~)lg(W~-l)g(W,-z))Using classes has the potential of reducing the prob-lem of sparseness .of data by allowing generaliza-179tions over similar words, as well as reducing the sizeof the language model.To determine the word classes, one can use thealgorithm of Brown et al (1992), which finds theclasses that give high mutual information betweenthe classes of adjacent words.
In other words, foreach bigram ~i3i_1~.13i in a training corpus, choosethe classes uch that the classes for adjacent wordsg(wi-1) and g(wi) lose as little information abouteach other as possible.
Brown et at give a greedy al-gorithm for finding the classes.
They start with eachword in a separate class and iteratively combineclasses that lead to the smallest decrease in mutualinformation between adjacent words.
Kneser andNey (1993) found that a class-based language modelresults in a perplexity improvement for the LOBcorpus from 541 for a word-based bigrarn model to478 for a class-based bigram model.
Interpolatingthe word-based and class-based models resulted inan improvement to439.1.2 POS-Based ModelsOne can also use POS tags, which capture the syn-tactic role of each word, as the basis of the equiv-alence classes (Jelinek, 1985).
Consider the se-quence of words "hello can I help you".
Here,"hello" is being used as an acknowledgment, "can"as a modal verb, 'T' as a pronoun, "help" as an un-tensed verb, and "you" as a pronoun.
To use POStags in language modeling, the typical approach isto sum over all of the POS possibilities.
Below, wegive the derivation based on using trigrams.Pr(Wi.N)- -EPI,NPr(Wt,NP1,N)N= ~ l~IPr(WdW~,,-~P~,) Pr(PdW~,-,P~,.a)P1.N i=1N= ~ IX Pr(W'IP') Pr(PilP, a,,a) (6)PI,N i= IThe above approach for incorporating POS infor-mation into a language model has not been of muchsuccess in improving speech recognition perfor-mance.
Srinivas (1996) reports that suclt a model re-sults in a 24.5% increase in perplexity over a word-based model on the Wall Street Journal; Niesler andWoodland (1996) report an II .3% increase (but a22-fold decrease in the number of parameters ofsuch a model) for the LOB corpus; and Kneser and180Ney (1993) report a 3% increase on the LOB cor-pus.
The POS tags remove too much of the lexicalinformation that is necessary for predicting the nextword.
Only by interpolating it with a word-basedmodel is an improvement seen (Jelinek, 1985).In the rest of the paper, we first describe the an-notations of the Trains corpus.
We next present ourPOS-based language model and contrast its perfor-mance with a class-based model.
We then augmentthese models to account for speech repairs and in-tonational phrase, and show that the POS-based oneperforms better than the class-based one for model-ing speech repairs and intonational phrases.2 The Trains CorpusAs part of the TRAINS project (Allen et al, 1995),a long term research project to build a conversa-tionally proficient planning assistant, we collecteda corpus of problem solving dialogs (Heeman andAllen, 1995).
The dialogs involve two human par-ticipants, one who is playing the role of a user andhas a certain task to accomplish, and another who isplaying the role of a planning assistant.
The collec-tion methodology was designed to make the settingas close to human-computer interaction as possible,but was not a wizard scenario, where one personpretends to be a computer.
Table 1 gives informa-tion about he corpus.DialogsSpeakersTurnsWordsFragmentsDistinct WordsDistinct Words/POSSingleton WordsSingleton Words/POSIntonational PhrasesSpeech Repairs98346163582987568591101252350109472396Table 1: Size of the Trains Corpus2.1 POS AnnotationsOur POS tagset is based on the Penn Treebanktagset (Marcus et al, 1993), but modified to in-clude tags for discourse markers and end-of-turns,and to provide richer syntactic information (Hee-man, 1997).
Table 2 lists our tagset with differ-ences from the Penn tagset marked in bold.
Con-tractions are annotated using 'A' to conjoin the tagfor each part; for instance, "can't" is annotated as'MDARB'.AC Acknowledgement DP Pro-form NNPS Plural proper NounBE Base form of"be" DT Determiner PDT Pre-determinerBED Past tense EX Existential "there" POS PossessiveBEG Present participle HAVE Base form of"have" PPREPpre-prepositionBEN Past participle HAVED Past tense PREP PrepositionBEP Present HAVEP Present PRP Personal pronoun VBDBEZ 3rd person sing.
pres.
HAVEZ 3rd person sing.
pres.
PRP$ Possessive pronoun VBGCC Co-ordinating conjunct JJ Adjective RB Adverb VBNCC.DDiscourse connective J JR Relative Adjective RBR Relative Adverb VBPCD Cardinal number JJS Superlative Adjective RBS Superlative Adverb VBZDO Base form of"do" MD Modal RB..D Discourse adverbial WDTDOD Past tense NN Noun RP Reduced particle WPDOP Present NNS Plural noun SC Subordinating conjunct WRBDOZ 3rd person sing.
present NNP Proper Noun TO To-infinitive WP$Table 2: Part-of-Speech Tags used in the Trains CorpusTURN Turn markerUI-I_D Discourse interjectionUH..FP Filled pauseVB Verb base form (otherthan 'do', 'be', or 'have')Past tensePresent participlePast participlePresent tense3rd person sing.
pres.Wh-determinerWh-pronounWh-adverbProcessive Wh-pronoun2.2 Speech Repair AnnotationsSpeech repairs occur where the speaker goes backand changes or repeats what was just said (Heeman,1997), as illustrated by the following.Example 1 (d92a-2.1 utt29)the one with the bananas I mean that's taking the bananasreparandum et alerationSpeech repairs have three parts (some of which areoptional): the reparandum, which are the words thespeaker wants to replace, an editing term, whichhelps mark the repair, and the alteration, which isthe replacement of the reparandum.
The end of thereparandum is referred to as the interruption point.For annotating speech repairs, we have extendedthe scheme proposed by Bear et al (1992) so thatit better deals with overlapping and ambiguous re-pairs.
Like their scheme, ours allows the annotatorto capture the word correspondences that exist be-tween the reparandum and the alteration.
Below,we illustrate how a speech repair is annotated.
Inthis example, the reparandum is "engine two fromElmi(ra)-", the editing term is "or", and the alter-ation is "engine three from Elmira".
The wordmatches on "engine" and "from" are annotated with'm' and the word replacement of "two" by "three"is annotated with 'r'.Example 2 (d93-15.2 utt42)engine two from Elmi(ra)- or engine three from Elmiraml r2 m3 m4 l"et ml  r2 nO m4ip:mod+2.3 Intonation AnnotationsSpeakers break up their speech into" intonationalphrases.
This segmentation serves asimilar purposeas punctuation does in written speech.
The ToBIannotation scheme (Silverman et al, 1992) involveslabeling the accented words, intermediate phrasesand intonational phrases with high and low accents.Since we are currently only interested in the intona-tional phrase segmentation, we only label the into-national phrase ndings.3 POS-Based Language ModelIn this section, we present an alternative formulationfor using POS tags in a statistical language model.Here, POS tags are viewed as part of the output ofthe speech recognizer, ather than intermediate ob-jects (Heeman and Allen, 1997a; Heeman, 1997).3.1 Redefining the Recognition ProblemTo add POS tags into the language model, we refrainfrom simply summing over all POS sequences asillustrated in Section 1.2.
Instead, we redefine thespeech recognition problem so that it finds the bestword and POS sequence.
Let P be a POS sequencefor the word sequence W. The goal of the speechrecognizer is to now solve the following.12?P = argmaxPr(WP\]A)W,PPr(AIWP) Pr(WP)= arg ma.2?
w~ Pr(A)= argmaxPr(AlWP)Pr(WP) (7)wpThe first term Pr(AIWP) is the acoustic model,which traditionally excludes the category assign-ment.
In fact, the acoustic model can probablybe reasonably approximated by Pr(AIW).
Thesecond term Pr (WP)  is the POS-based languagemodel and this accounts for both the sequence ofwords and the POS assignment for those words.
Werewrite the sequence WP explicitly in terms of theN words and their corresponding POS tags, thusgiving us the sequence W1,NP1,N.
The probabil-ity Pr(Wi,NP1,N) forms the basis for POS taggers,with the exception that POS taggers work from asequence of given words.181As in Equation 4, we rewrite the probabi\]lityPr(W1,NP1,N) as follows using the definition ofconditional probability,Pr( Wx.N Px,N )N= I~\[pr(wiPdWu_lPz,_~)i= IN= HPr(WilWl, i-lPti)Pr(PilWl, i.lPl, i.1) (8)i= lEquation 8 involves two probability distributionsthat need to be estimated.
Previous attempts at us-ing POS tags in a language model as well as POStaggers (i.e.
(Charniak et al, 1993)) simplify theseprobability distributions, as given in Equations 9and 10.
However, to successfully incorporate POSinformation, we need to account for the full richnessof the probability distributions.
Hence, we cannotuse these two assumptions when learning the prob-ability distributions.Pr(WilW~i-lPl, i) ~ Pr(WilPi) ?
(9)Pr(PilWt~-tP~i-1) ~ Pr(PdPl, i-~) (10)3.2 Estimating the ProbabilitiesTo estimate the probability distributions, we followthe approach of Bahl et al (1989) and use a deci-sion tree learning algorithm (Breiman et al, 1984)to partition the context into equivalence classes.
Thealgorithm starts with a single node.
It then finds aquestion to ask about the node in order to partitionthe node into two leaves, each being more informa-tive as to which event occurred than the parent node.Information theoretic metrics, such as minimizingentropy, are used to decide which question to pro-pose.
The proposed question is then verified usingheldout data: if the split does not lead to a decreasein entropy according to the heldout data, the split isrejected and the node is not further explored (Bahlet al, 1989).
This process continues with the newleaves and results in a hierarchical partitioning ofthe context.After growing a tree, the next step is to use thepartitioning of the context induced by the decisiontree to determine the probability estimates.
Usingthe relative frequencies in each node will be biasedtowards the training data that was used in choosingthe questions.
Hence, Bahl et al smooth these prob-abilities with the probabilities of the parent node us-ing interpolated estimation with a second heldoutdataset.Using the decision tree algorithm to estimateprobabilities is attractive since the algorithm canchoose which parts of the context are relevant, andin what order.
Hence, this approach lends itselfmore readily to allowing extra contextual informa-tion to be included, such as both the word identi-fies and POS tags, and even hierarchical clusteringsof them.
If the extra information is not relevant, itwill not be used.
The approach of using decisiontrees will become ven more critical in the next twosections where the probability distributions will beconditioned on even richer context.3.2.1 Simple QuestionsOne important aspects of using a decision tree algo-rithm is the form of the questions that it is allowed toask.
We allow two basic types of information to beused as part of the context: numeric and categorical.For a numeric variable N,  the decision tree searchesfor questions of the form 'is N >= n', where n isa numeric constant.
For a categorical variable C,it searches over questions of the form 'is C E S'where S is a subset of the possible values of C. Wealso allow restricted boolean combinations of ele-mentary questions (Bahl et al, 1989).3.2.2 Questions about POS TagsThe context hat we use for estimating the probabil-ities includes both word identities and POS tags.
Tomake effective use of this information, we need toallow the decision tree algorithm to generalize be-tween words and POS tags that behave similarly.To learn which words behave similarly, Black etaL(1989) and Magerrnan (1994) used the clusteringalgorithm of Brown et al (1992) to build a hierar-chical classification tree.
Figure 1 gives the clas-sification tree that we built for the POS tags.
Thealgorithm starts with each token in a separate classand iteratively finds two classes to merge that re-sults in the smallest lost of information about POSadjacency.
Rather than stopping at a certain numberof classes, one continues until only a single classremains.
However, the order in which classes weremerged gives a hierarchical binary tree with the rootcorresponding tothe entire tagset, each leaf to a sin-gle POS tag, and intermediate nodes to groupings oftags that are statistically similar.
The path from theroot to a tag gives the binary encoding for the tag.The decision tree algorithm can ask which partitiona word belongs to by asking questions about he bi-nary encoding.182Figure 1: Classification Tree for POS Tags3.2.3 Questions about Word IdentitiesFor handling word identities, one could followthe approach used for handling the POS tags(e.g.
(Black et ai,, 1992; Magerman, 1994))andview the POS tags and word identities as two sep-arate sources of information.
Instead, we view theword identifies as a further refinement of the POStags.
We start the clustering algorithm with a sep-arate class for each word and each POS tag that ittakes on and only allow it to merge classes if thePOS tags are the same.
This results in a word clas-sification tree for each POS tag.
Building a wordclassification tree for each POS tag means that thetree will not be polluted by words that are ambigu-ous as to their POS tag, as exemplified by the word"loads", which is used in the Trains corpus as botha third-person present ense verb VBZ and as a plu-ral noun iNNS.
Furthermore, building dtree for eachPOS tag simplifies the task because the hand an-notations of the POS tags resolve a lot of the dif-ficulty that the algorithm would otherwise have tohandle.
This allows effective trees to be built evenwhen only a small amount of data is available.183~ i t  64<: low > 2them 157me 85us 176they 89we 766$Figure 2: Classification Tree for Personal PronounsFigure 2 shows the classification tree for the per-sonal pronouns (PRP).
For reference, we list thenumber of occurrences of each word.
Notice thatthe algorithm distinguished between the subjectivepronouns T ,  'we', and 'they', and the objective pro-nouns 'me', 'us' and 'them'.
The pronouns 'you'and 'it' take both cases and were probably clusteredaccording to their most common usage in the cor-pus.
Although we could have added extra POS tagsto distinguish between these two types of pronouns,it seems that the clustering algorithm can make upfor some of the shortcomings of the POS tagset.
Theclass low is used to group singleton words.3.3 ResultsBefore giving a comparison between our POS-basedmodel and a class-based model, we first describe theexperimental setup and define the perplexity mea-sures that we use to measure the performance.3.3.1 Experimental SetupTo make the best use of our limited data, we useda six-fold cross-validation procedure: each sixth ofthe data was tested using a model built from the re-maining data.
Changes in speaker are marked in theword transcription with the special token <turn>.We treat contractions, uch as "that'll" and "gonna",as separate words, treating them as "that" and "'ll'"for the first example, and "going" and "ta" for thesecond.
1 We also changed all word fragments intothe token <fragment>.Since current speech recognition rates for sponta-neous speech are quite low, we have run the exper-iments on the hand-collected transcripts.
In search-ing for the best sequence of POS tags for the tran-scribed words, we follow the technique proposedby Chow and Schwartz (1989) and only keep asmall number of alternative paths by pruning thelow probability paths after processing each word.3.3.2 Branching PerplexityOur POS-based model is not only predicting thenext word, but its POS tag as well.
To estimateI See Heeman and Darrmat i  (1997)  for  how to treat cont rac -t ions as separate  words  in a speech  recogn izer .
.the branching factor, and thus the size of the searchspace, we use the following formula for the entropy,where di is the POS tag for word wi.1 NH = - ~ ~ log 215r(wi \[wLi_ 1dl, i)tSr(di \[wLi-x dLi-1 )i=13.3.3 Word PerplexityIn order to compare a POS-based model against atraditional language model, we should not penalizethe POS-based model for incorrect POS tags, andhence we should ignore them when defining the per-plexity.
Just as with a traditional model, we base theperplexity measure on Pr(wilw~i-1).
However, forour model, this probability is not estimated.
Hence,we must rewrite it in terms of the probabilities thatwe do estimate.
To do this, our only recourse is tosum over all possible POS sequences.H 1 N ~DtPr(wiDilWLi.lDl, i_l) Pr(Wl, i.lDl, i.1)=" N--i=Lll?gx EDxl--t Pr(w~i-aDLia)3.3.4 Using Richer ContextTable 3 shows the effect of varying the richness ofthe information that the decision tree algorithm isallowed to use in estimating the POS and word prob-abilities.
The second column uses the approxima-tions given in Equation 9 and 10.
The third col-umn gives the results using the full context.
Theresults show that adding the extra context has thebiggest effect on the perplexity measures, decreas-ing the word perplexity from 43.22 to 24.04, a re-duction of 44.4%.
The effect on POS tagging is lesspronounced, but still gives an error rate reduction of3.8%.
Hence, to use POS tags during speech recog-nition, one must use a richer context for estimatingthe probabilities than what is typically used.Context for Wi Di Di-2,iWi~,i-iContext for Di Di-2,i-i Di.,2,i-i Wi.2,i-1POS Errors 1778 1711POS Error Rate 3.04 2.93Word Perplexity 43.22 24.04Branching Perplexity !
47.25 26.35Table 3: Using Richer Context3.3.5 Class-Based Decision-Tree ModelsIn this section, we compare the POS-based modelagainst aclass-based model.
To make the compari-son as focused as possible, we use the same method-ology for estimating the probability distributions aswe used for the POS-based model.
The classes wereobtained from the word clustering algorithm, butstopping once a certain number of classes has beenreached.
Unfortunately, the clustering algorithm ofBrown et al does not have a mechanism to decidean optimal number of word classes (cf.
(Kneser andNey, 1993)).
Hence, to give an optimal evaluationof the class-based approach, we choose the num-ber of classes that gives the best perplexity results,which was 100 classes.
We then built word clas-sification trees, just as we did for the POS-basedapproach, where words from different classes arenot allowed to be merged.
The resulting class-basedmodel achieved a perplexity of 25.24 in compari-son to 24.04 for the POS-based model.
This im-provement is due to two factors.
First, tracking thesyntactic role of each word gives valuable informa-tion for predicting the subsequent words.
Second,the classification trees for the POS-based approach,which the decision tree algorithm uses to determinethe equivalence classes, are of higher quality.
Thisis due to the POS-based classification trees using thehand-annotated POS information, since they takeadvantage of the hand-coded knowledge present inthe POS tags and are not polluted by words that akeon more than one syntactic role.3.3.6 Preliminary Wall Street Journal ResultsFor building a system that partakes in dialogue,read-speech corpora, such as the Wall Street Jour-nal, are not appropriate.
However, to make ourresults more comparable to the literature, we havedone preliminary tests on the Wall Street Journalcorpus in the Penn Treebank, which has POS an-notations.
This corpus has a significantly larger vo-cabulary size (55800 words) than the Trains corpus.Our current algorithm for clustering the words takesspace in proportion to the square of the number ofunique word/POS combinations (minus any that getgrouped into the low occurring class).
More workis needed to handle larger vocabulary sizes.
Us-ing 78,800 words of data, with a vocabulary sizeof 9711, we achieved a perplexity of 250.75 onthe known words in comparison to a trigram word-based backoff model (Katz, 1987) built with theCMU toolkit (Rosenfeld, 1995), which achieved aperplexity of 296.43.
More work is needed to see ifthese results cale up to larger vocabulary and train-ing data sizes.4 Adding Repairs and PhrasingJust as we redefined the speech recognition prob-lem so as to account for POS tagging, we do thesame for modeling intonational phrases and speech184repairs.
We introduce null tokens between each pairof words ~./)i-1 and wi (Heeman and Allen, 1997b),which will be tagged as to the occurrence of theseevents.
The variable T/indicates if word wi-1 endsan intonational phrase (Ti=%), or not (Ti=null).For detecting speech repairs, we have the prob-lem that repairs are often accompanied by an edit-ing term, such as "urn", "uh", "okay", or "well",and these must be identified as such.
Furthermore,an editing term might be composed of a numberof words, such as "let's see" or "uh well".
Hencewe use two tags: an editing term tag Ei and a re-pair tag Ri.
The editing term tag indicates if wistarts an editing term (Ei=Push), if wi continues anediting term (Ei=ET), if wi-t ends an editing term(Ei=Pop), or otherwise (Ei=null).
The repair tagRi indicates whether word wi is the onset of the al-teration of a fresh start (Ri=C), a modification re-pair (Ri=M), or an abridged repair (Ri=A), or thereis no repair (Pa=null).
Note that for repairs withan editing term, the repair is tagged after the extentof the editing term has been determined.
Below wegive an example showing all non-null tone, editingterm and repair tags.Example 3 (d93-18.1 utt47)it takes one Push you ET know Pop M two hours %If a modification repair or fresh start occurs, weneed to determine the extent (or the onset) of thereparandum, which we refer to as correcting thespeech repair.
Often, speech repairs have strongword correspondences between the reparandum andalteration, involving word matches and word re-placements.
Hence, knowing the extent of thereparandum eans that we can use the reparandumto predict he words (and their POS tags) that makeup the alteration.
In our full model, we add threevariables to account for the correction of speech re-pairs (Heeman and Allen, 1997b; Heeman, 1997).We also add an extra variable to account for silencesbetween words.
After a silence has occurred, we canuse the silence to better predict whether an intona-tional boundary or speech repair has just occurred.Below we give the redefinition of the speechrecognition problem (without speech repair correc-tion and silence information).
The speech recog-nition problem is redefined so that its goal is to findthe maximal assignment for the words as well as thePOS, intonational, and repair tags.I2?PREf' = argmax Pr (WPRET IA  )WPRET185Just as we did in Equation 8, we rewrite the above interms of five probability distributions, each of whichneed to be estimated.
The context for each of theprobability distributions includes all of the previouscontext.
In principal, we could give all of this con-text to the decision tree algorithm and let it decidewhat information is relevant in constructing equiva-lence classes of the contexts.
However, the amountof training data is limited (as are the learning tech-niques) and so we need to encode the context inorder to simplify the task of constructing meaning-ful equivalence classes.
Hence we restructure thecontext o take into account he speech repairs andboundary tones (Heeman, 1997).4.1 ResultsWe now contrast the performance ofaugmenting thePOS-based model with speech repair and intona-tional modeling versus augmenting the class-basedmodel.
Just as in Section 3, all results were obtainedusing a six-fold cross-validation procedure from thethe hand-collected transcripts.
We ran these tran-scripts through a word-aligner (Ent, 1994), a speechrecognizer constrained to recognize what was tran-scribed, in order to automatically obtain silencedurations.
In predicting the end of turn marker<turn>, we do not use any silence information.4.1.1 Recall and PrecisionWe report results on identifying speech repairs andintonational phrases in terms of recall, precisionand error rate.
The recall rate is the number of timesthat the algorithm correctly identifies an event overthe total number of times that it actually occurred.The precision rate is the number of times the algo-rithm correctly identifies it over the total number oftimes it identifies it.
The error rate is the numberof errors in identifying an event over the number oftimes that the event occurred.4.1.2 POS Tagging and PerplexityThe first set of experiments, whose results are givenin Table 4, explore how POS tagging and word per-plexity benefit from modeling boundary tones andspeech repairs.
The second column gives the re-suits of the POS-based language model, introducedin Section 3.
The third column adds in speech re-pair detection and correction, boundary tone identi-fication, and makes use of silence information in de-tecting speech repairs and boundary tones.
We seethat this results in a perplexity reduction of 7.0%,and a POS error reduction of 8.
1%.
As we furtherimprove the modeling of the user's utterance, we\[ POS I Full ModelPOS Errors 1711 1572POS Error Rate , 2.93 2.69Word Perplexity i 24.04 22.35Branching Perplexity 126.35 30.26Table 4: POS Tagging and Perplexityexpect o see further improvements in the languagemodel.
Of course, there is a penalty to pay in termsof increased search space size, as the increase in thebranching perplexity shows.4.1.3 Intonational PhrasesIn Table 5, we demonstrate that modeling intona-tional phrases benefits from modeling POS tags.Column two gives the results of augmenting theclass-based model of Section 3.3.5 with intonationalphrase modeling and column three gives the resultsof augmenting the POS-based model.
Contrastingthe results in column two with those in columnthree, we see that using the POS-based model ~ re-sults in a reduction in the error rate of 17.2% overthe class-based model.
Hence, we see that modelingthe POS tags allows much better modeling of into-national phrases than can be achieved with a class-based model.
The fourth column reports the resultsusing the full model, which accounts for interac-tions with speech repairs and the benefit of usingsilence information (Heeman and Allen, 1997b).Class-Based POS-Based FullTones Tones ModelErrors 4859 i 4024 i 3632Error Rate 44.38 i 36.75 i 33.17i 84.76 Recall 74.55!
81.72Precision 79.741 81.55 ~ 82.53Table 5: Detecting Intonational Phrase Boundaries4.1.4 Detecting Speech RepairsIn Table 6, we demonstrate that modeling the de-tection of speech repairs (and editing terms) bene-fits from modeling POS tags.
In the results below,we ignore errors that are the result of improperlyidentifying the type of repair, and hence score a re-pair as correctly detected as long as it'was identi-fied as either an abridged repair, modification re-pair or fresh start.
Column two gives the results ofaugmenting the class-based model of Section 3.3.5with speech repair modeling and column three givesthe results of augmenting the POS-based model.
In186ErrorsError RateRecallPrecisionClass-BasedRepairs124652.0064.9879.27POS-Based I Full 1Repairs \[ Model!1106 839!46.16 35.01168.61 76.7982.28 86.66Table 6: Detecting Speech Repairsterms of overall detection, the POS-based model re-duces the error rate from 52.0% to 46.2%, a reduc-tion of 11.2%.
This shows that speech repair de-tection profits from being able to make use of syn-tactic generalizations, which are not available froma class-based approach.
The final column gives theresults of the full model, which accounts for interac-tions with speech repair correction and intonationalphrasing, and uses silence information.5 Conclus ionIn this paper, we presented a POS-based languagemodel.
Unlike previous approaches that use POStags in language modeling, we redefine the speechrecognition problem so that it includes finding thebest word sequence and best POS tag interpretationfor those words.
Thus this work can be seen as afirst-step towards tightening the integration betweenspeech recognition and natural anguage processing.In order to make use of the POS tags, we usea decision tree algorithm to learn the probabilitydistributions, and a clustering algorithm to buildhierarchical partitionings of the POS tags and theword identities.
Furthermore, we take advantageof the POS tags in building the word classificationtrees and in estimating the word probabilities, whichboth results in better performance and significantlyspeeds up the training procedure.
We find that us-ing the rich context afforded by decision tree resultsin a perplexity reduction of 44.4%.
We also findthat the POS-based model gives a 4.2% reduction inperplexity over a class-based model, also built withthe decision tree and clustering algorithms.
Prelim-inary results on the Wall Street Journal corpus arealso encouraging.
Hence, using a POS-based modelresults in an improved language model as well asaccomplishes the first part of the task in linguisticunderstanding.We also see that using POS tags in the languagemodel aids in the identification of boundary tonesand speech repairs, which we have also incorpo-rated into the model by further edefining the speechrecognition problem.
The POS tags allow these twoprocesses to generalize about he syntactic role thatwords are playing in the utterance rather than usingcrude class-based approaches which does not distin-guish this information.
We also see that modelingthese phenomena improves the POS tagging resultsas well as the word perplexity.6 AcknowledgmentsWe wish to thank Geraldine Damnati.
The researchinvolved in this paper was done while the first authorwas visiting at CNET, France Ttl6com.ReferencesJ.
Allen, L. Schubert, G. Ferguson, P.Heeman,C.
Hwang, T. Kato, M. Light, N. Martin, B. Miller,M.
Poesio, and D. Traum.
1995.
The Trains project:A case study in building a conversational planningagent.
Journal of Experimental nd Theoretical Al,7:7--48.L.
Bahl, J. Baker, E Jelinek, and R. Mercer.
1977.Perplexityma measure of the difficulty of speechrecognition tasks.
In Proceedings of the Meeting ofthe Acoustical Society of America.L.
Bahl, P. Brown, P. deSouza, and R. Mercer.
1989, Atree-based statistical language model for naturallanguage speech recognition.
IEEE Transactions onAcoustics, Speech, and Signal Processing,36(7): 1001-1008.J.
Bear, J. Dowding, and E. Shriberg.
1992.
Integratingmultiple knowledge sources for detection andcorrection of repairs in human-computer dialog.
InProceedings of the 30 th Annual Meeting of theAssociation for Computational Linguistics, pages56--63.E.
Black, E Jelinek, J. Lafferty, D. Magerman,R.
Mercer, and S. Roukos.
1992.
Towardshistory-based grammars: Using richer models forprobabilistic parsing.
In Proceedings of the DARPASpeech and Natural Language Workshop, ages134-139.L.
Breiman, J. Friedman, R.ichard A. Olshen, andC.harles J.
Stone.
1984.
Classification andRegression Trees.
Wadsworth & Brooks.P.
Brown, V. Della Pietra, P. deSouza, J. Lai, andRobert L. Mercer.
1992.
Class-based n-gram modelsof natural language.
Computational Linguistics,18(4):467--479.E.
Charniak, C. Hendrickson, N. Jacobson, andM.
Perkowitz.
1993.
Equations for part-of-speechtagging.
In Proceedings of the National Conferenceon Artificial Intelligence.Y.
Chow and R. Schwartz.
1989.
The n-best algorithm:An efficient procedure for finding top rksentencehypotheses.
In Proceedings of the DARPA Speechand Natural Language Workshop, ages 199-202.Entropic Research Laboratory, Inc., 1994.
AlignerReference Manual.
Version 1.3.P.
Heeman and J. Allen.
1995.
The Trains spokendialog corpus.
CD-ROM, Linguistics DataConsortium.R Heeman and J. Allen.
1997a.
Incorporating POStagging into language modeling.
In Proceedings ofthe European Conference on Speech Communicationand Technology, pages 2767-2770.R Heeman and J. Allen.
1997b.
Intonationalboundaries, peech repairs, and discourse markers:Modeling spoken dialog.
In Proceedings of theAnnual Meeting of the Association for ComputationalLinguistics, pages 254.--.-261.R Heeman and G. Damnati.
1997.
Derivingphrase-based language models.
In IEEE Workshopon Speech Recognition and Understanding, pages41-48.R Heeman.
1997.
Speech repairs, intonationalboundaries and discourse markers: Modelingspeakers' utterances in spoken dialog.
TR 673, Dept.of Computer Science, U. of Rochester.
Doctoraldissertation.F.
Jelinek and R. Mercer.
1980.
Interpolated estimationof markov source paramaters from sparse data.
InProceedings, Workshop on Pattern Recognition iPractice, pages 381-397.F.
Jelinek.
1985.
Self-organized language modeling forspeech recognition.
Technical report, IBMT.J.
Watson Research Center, Continuous SpeechRecognition Group, Yorktown Heights, NY.S.
Katz.
1987.
Estimation of probabilities from sparsedata for the language model component ofa speechrecognizer.
IEEE Transactions on Acoustics, Speech,and Signal Processing, 35(3):400-401.R.
Kneser and H. Ney.
1993.
Improved clusteringtechniques for class-based statistical languagemodelling.
In Proceedings of the EuropeanConference on Speech Communication a dTechnology, pages 973-976.D.
Magerrnan.
1994.
Natural anguage parsing asstatistical pattern recognition.
Doctoral dissertation,Dept.
of Computer Science, Stanford.M.
Marcus, B. Santorini, and M. Marcinkiewicz.
1993.Building a large annotated corpus of English: ThePenn Treebank.
Computational Linguistics,19(2):313-330.T.
Niesler and P. Woodland.
1996.
A variable-lengthcategory-based n-gram language model.
InProceedings of the International Conference onAudio, Speech and Signal Processing, pages164-167.R.
Rosenfeld.
1995.
The CMU statistical languagemodeling toolkit and its use in the 1994 ARPA CSRevaluation.
In Proceedings of the ARPA SpokenLanguage Systems Technology Workshop.K.
Silverman, M. Beckman, J. Pitrelli, M. Ostendorf,C.
Wightman, P.Price, J. Pierrehumbert, andJ.
Hirschberg.
1992.
ToBI: A standard for labellingEnglish prosody.
In Proceedings of the 2ndInternational Conference on Spoken LanguageProcessing, pages 867-870.B.
Srinivas.
1996.
"Almost parsing" techniques forlanguage modeling.
In Proceedings of theInternational Conference on Spoken LanguageProcessing, pages 1169-1172.187
