Proceedings of the 43rd Annual Meeting of the ACL, pages 280?289,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsQARLA:A Framework for the Evaluation of Text Summarization SystemsEnrique Amigo?, Julio Gonzalo, Anselmo Pen?as, Felisa VerdejoDepartamento de Lenguajes y Sistemas Informa?ticosUniversidad Nacional de Educacio?n a Distanciac/Juan del Rosal, 16 - 28040 Madrid - Spain{enrique,julio,anselmo,felisa}@lsi.uned.esAbstractThis paper presents a probabilisticframework, QARLA, for the evaluationof text summarisation systems.
The in-put of the framework is a set of man-ual (reference) summaries, a set of base-line (automatic) summaries and a set ofsimilarity metrics between summaries.It provides i) a measure to evaluate thequality of any set of similarity metrics,ii) a measure to evaluate the quality ofa summary using an optimal set of simi-larity metrics, and iii) a measure to eval-uate whether the set of baseline sum-maries is reliable or may produce biasedresults.Compared to previous approaches, ourframework is able to combine differentmetrics and evaluate the quality of a setof metrics without any a-priori weight-ing of their relative importance.
We pro-vide quantitative evidence about the ef-fectiveness of the approach to improvethe automatic evaluation of text sum-marisation systems by combining sev-eral similarity metrics.1 IntroductionThe quality of an automatic summary can be es-tablished mainly with two approaches:Human assessments: The output of a number ofsummarisation systems is compared by hu-man judges, using some set of evaluationguidelines.Proximity to a gold standard: The best auto-matic summary is the one that is closest tosome reference summary made by humans.Using human assessments has some clear ad-vantages: the results of the evaluation are inter-pretable, and we can trace what a system is do-ing well, and what is doing poorly.
But it alsohas a couple of serious drawbacks: i) different hu-man assessors reach different conclusions, and ii)the outcome of a comparative evaluation exerciseis not directly reusable for new techniques, i.e., asummarisation strategy developed after the com-parative exercise cannot be evaluated without ad-ditional human assessments made from scratch.Proximity to a gold standard, on the other hand,is a criterion that can be automated (see Section 6),with the advantages of i) being objective, and ii)once gold standard summaries are built for a com-parative evaluation of systems, the resulting test-bed can iteratively be used to refine text summari-sation techniques and re-evaluate them automati-cally.This second approach, however, requires solv-ing a number of non-trivial issues.
For instance,(i) How can we know whether an evaluation met-ric is good enough for automatic evaluation?, (ii)different users produce different summaries, all ofthem equally good as gold standards, (iii) if wehave several metrics which test different featuresof a summary, how can we combine them into anoptimal test?, (iv) how do we know if our test bed280Figure 1: Illustration of some of the restrictions on Q,Kis reliable, or the evaluation outcome may changeby adding, for instance, additional gold standards?In this paper, we introduce a probabilisticframework, QARLA, that addresses such issues.Given a set of manual summaries and another setof baseline summaries per task, together with a setof similarity metrics, QARLA provides quantita-tive measures to (i) select and combine the best(independent) metrics (KING measure), (ii) applythe best set of metrics to evaluate automatic sum-maries (QUEEN measure), and (iii) test whetherevaluating with that test-bed is reliable (JACKmeasure).2 Formal constraints on any evaluationframework based on similarity metricsWe are looking for a framework to evaluate au-tomatic summarisation systems objectively usingsimilarity metrics to compare summaries.
The in-put of the framework is:?
A summarisation task (e.g.
topic oriented, in-formative multi-document summarisation ona given domain/corpus).?
A set T of test cases (e.g.
topic/document setpairs for the example above)?
A set of summaries M produced by humans(models), and a set of automatic summariesA (peers), for every test case.?
A set X of similarity metrics to comparesummaries.An evaluation framework should include, atleast:?
A measure QM,X(a) ?
[0, 1] that estimatesthe quality of an automatic summary a, us-ing the similarity metrics in X to comparethe summary with the models in M .
WithQ, we can compare the quality of automaticsummaries.?
A measure KM,A(X) ?
[0, 1] that estimatesthe suitability of a set of similarity metrics Xfor our evaluation purposes.
With K, we canchoose the best similarity metrics.Our main assumption is that all manual sum-maries are equally optimal and, while they arelikely to be different, the best similarity metric isthe one that identifies and uses the features that arecommon to all manual summaries, grouping andseparating them from the automatic summaries.With these assumption in mind, it is useful tothink of some formal restrictions that any evalua-tion framework Q,K must hold.
We will considerthe following ones (see illustrations in Figure 1):(1) Given two automatic summaries a, a?
and asimilarity measure x, if a is more distant to allmanual summaries than a?, then a cannot be better281than a?.
Formally: ?m ?
M.x(a,m) < x(a?,m) ?QM,x(a) ?
QM,x(a?
)(2) A similarity metric x is better when it is ableto group manual summaries more closely, whilekeeping them more distant from automatic sum-maries: (?m,m?
?
M.x(m,m?)
> x?(m,m?)
?
?m ?M,a ?
Ax(a,m) < x?
(a,m)) ?
KM,A(x) > KM,A(x?
)(3) If x is a perfect similarity metric, the quality ofa manual summary cannot be zero: KM,A(x) = 1 ?
?m ?M.QM,x(m) > 0(4) The quality of a similarity metric or a summaryshould not be dependent on scale issues.
In gen-eral, if x?
= f(x) with f being a growing mono-tonic function, then KM,A(x) = KM,A(x?)
andQM,x(a) = QM,x?
(a) .
(5) The quality of a similarity metric shouldnot be sensitive to repeated elements in A, i.e.KM,A?
{a}(x) = KM,A?{a,a}(x).
(6) A random metric x should have KM,A(x) = 0.
(7) A non-informative (constant) metric x shouldhave KM,A(x) = 0.3 QARLA evaluation framework3.1 QUEEN: Estimation of the quality of anautomatic summaryWe are now looking for a function QM,x(a) thatestimates the quality of an automatic summary a ?A, given a set of models M and a similarity metricx.An obvious first attempt would be to computethe average similarity of a to all model summariesin M in a test sample.
But such a measure dependson scale properties: metrics producing larger sim-ilarity values will produce larger Q values; and,depending on the scale properties of x, this cannotbe solved just by scaling the final Q value.A probabilistic measure that solves this problemand satisfies all the stated formal constraints is:QUEENx,M (a) ?
P (x(a,m) ?
x(m?,m??
))which defines the quality of an automatic sum-mary a as the probability over triples of manualsummaries m,m?,m??
that a is closer to a modelthan the other two models to each other.
This mea-sure draws from the way in which some formal re-strictions on Q are stated (by comparing similarityvalues), and is inspired in the QARLA criterionintroduced in (Amigo et al, 2004).Figure 2: Summaries quality in a similarity metricspaceFigure 2 illustrates some of the features of theQUEEN estimation:?
Peers which are very far from the set ofmodels all receive QUEEN = 0.
In otherwords, QUEEN does not distinguish betweenvery poor automatic summarisation strate-gies.
While this feature reduces granularityof the ranking produced by QUEEN, we findit desirable, because in such situations, thevalues returned by a similarity measure areprobably meaningless.?
The value of QUEEN is maximised for thepeers that ?merge?
with the models.
ForQUEEN values between 0.5 and 1, peers areeffectively merged with the models.?
An ideal metric (that puts all models to-gether) would give QUEEN(m) = 1 for allmodels, and QUEEN(a) = 0 for all peersthat are not put together with the models.This is a reasonable boundary condition say-ing that, if we can distinguish between mod-els and peers perfectly, then all peers arepoor emulations of human summarising be-haviour.3.2 Generalisation of QUEEN to metric setsIt is desirable, however, to have the possibility ofevaluating summaries with respect to several met-rics together.
Let us imagine, for instance, thatthe best metric turns out to be a ROUGE (Lin andHovy, 2003a) variant that only considers unigramsto compute similarity.
Now consider a summary282which has almost the same vocabulary as a hu-man summary, but with a random scrambling ofthe words which makes it unreadable.
Even if theunigram measure is the best hint of similarity tohuman performance, in this case it would producea high similarity value, while any measure basedon 2-grams, 3-grams or on any simple syntacticproperty would detect that the summary is useless.The issue is, therefore, how to find informativemetrics, and then how to combine them into an op-timal single quality estimation for automatic sum-maries.
The most immediate way of combiningmetrics is via some weighted linear combination.But our example suggests that this is not the op-timal way: the unigram measure would take thehigher weight, and therefore it would assign a fairamount of credit to a summary that can be stronglyrejected with other criteria.Alternatively, we can assume that a summary isbetter if it is closer to the model summaries ac-cording to all metrics.
We can formalise this ideaby introducing a universal quantifier on the vari-able x in the QUEEN formula.
In other words,QUEENX,M (a) can be defined as the probability,measured over M ?M ?M , that for every metricin X the automatic summary a is closer to a modelthan two models to each other.QUEENX,M (a) ?
P (?x ?
X.x(a,m) ?
x(m?,m??
))We can think of the generalised QUEEN mea-sure as a way of using a set of tests (every simi-larity metric in X) to falsify the hypothesis that agiven summary a is a model.
If, for every compar-ison of similarities between a,m,m?,m?
?, there isat least one test that a does not pass, then a is re-jected as a model.This generalised measure is not affected by thescale properties of every individual metric, i.e.
itdoes not require metric normalisation and it is notaffected by metric weighting.
In addition, it stillsatisfies the properties enumerated for its single-metric counterpart.Of course, the quality ranking provided byQUEEN is meaningless if the similarity metric xdoes not capture the essential features of the mod-els.
Therefore, we need to estimate the quality ofsimilarity metrics in order to use QUEEN effec-tively.3.3 KING: estimation of the quality of asimilarity metricNow we need a measure KM,A(x) that estimatesthe quality of a similarity metric x to evaluateautomatic summaries (peers) by comparison tohuman-produced models.In order to build a suitable K estimation, wewill again start from the hypothesis that the bestmetric is the one that best characterises humansummaries as opposed to automatic summaries.Such a metric should identify human summariesas closer to each other, and more distant to peers(second constraint in Section 2).
By analogy withQUEEN, we can try (for a single metric):KM,A(x) ?
P (x(a,m) < x(m?,m??))
=1?
(QUEENx,M (a))which is the probability that two models arecloser to each other than a third model to a peer,and has smaller values when the average QUEENvalue of peers decreases.
The generalisation of Kto metric sets would be simply:KM,A(X) ?
1?
(QUEENX,M (a)))This measure, however, does not satisfy formalconditions 3 and 5.
Condition 3 is violated be-cause, given a limited set of models, the K mea-sure grows with a large number of metrics in X ,eventually reaching K = 1 (perfect metric set).But in this situation, QUEEN(m) becomes 0 forall models, because there will always exist a met-ric that breaks the universal quantifier conditionover x.We have to look, then, for an alternative for-mulation for K. The best K should minimiseQUEEN(a), but having the quality of the modelsas a reference.
A direct formulation can be:KM,A(X) = P (QUEEN(m) > QUEEN(a))According to this formula, the quality of a met-ric set X is the probability that the quality of a283model is higher than the quality of a peer ac-cording to this metric set.
This formula satisfiesall formal conditions except 5 (KM,A?
{a}(x) =KM,A?
{a,a}(x)), because it is sensitive to repeatedpeers.
If we add a large set of identical (or verysimilar peers), K will be biased towards this set.We can define a suitable K that satisfies condi-tion 5 if we apply a universal quantifier on a. Thisis what we call the KING measure:KINGM,A(X) ?P (?a ?
A.QUEENM,X(m) > QUEENM,X(a))KING is the probability that a model is betterthan any peer in a test sample.
In terms of a qual-ity ranking, it is the probability that a model gets abetter ranking than all peers in a test sample.
Notethat KING satisfies all restrictions because it usesQUEEN as a quality estimation for summaries; ifQUEEN is substituted for a different quality mea-sure, some of the properties might not hold anylonger.Figure 3: Metrics quality representationFigure 3 illustrates the behaviour of the KINGmeasure in boundary conditions.
The left-most figure represents a similarity metric whichmixes models and peers randomly.
Therefore,P (QUEEN(m) > QUEEN(a)) ?
0.5.
As thereare seven automatic summaries, KING = P (?a ?A,QUEEN(m) > QUEEN(a)) ?
0.57 ?
0The rightmost figure represents a metric whichis able to group models and separate them frompeers.
In this case, QUEEN(a) = 0 for all peers,and then KING(x) = 1.3.4 JACK:Reliability of the peers setOnce we detect a difference in quality betweentwo summarisation systems, the question is nowwhether this result is reliable.
Would we get thesame results using a different test set (different ex-amples, different human summarisers (models) ordifferent baseline systems)?The first step is obviously to apply statisticalsignificance tests to the results.
But even if theygive a positive result, it might be insufficient.
Theproblem is that the estimation of the probabilitiesin KING,QUEEN assumes that the sample setsM,A are not biased.
If M,A are biased, the re-sults can be statistically significant and yet un-reliable.
The set of examples and the behaviourof human summarisers (models) should be some-how controlled either for homogeneity (if the in-tended profile of examples and/or users is narrow)or representativity (if it is wide).
But how to knowwhether the set of automatic summaries is repre-sentative and therefore is not penalising certain au-tomatic summarisation strategies?Our goal is, therefore, to have some estimationJACK(X,M,A) of the reliability of the test set tocompute reliable QUEEN,KING measures.
Wecan think of three reasonable criteria for this es-timation:1.
All other things being equal, if the elementsof A are more heterogeneous, we are enhanc-ing the representativeness of A (we have amore diverse set of (independent) automaticsummarization strategies represented), andtherefore the reliability of the results shouldbe higher.
Reversely, if all automatic sum-marisers employ similar strategies, we mayend up with a biased set of peers.2.
All other things being equal, if the elementsof A are closer to the model summaries in M ,the reliability of the results should be higher.3.
Adding items to A should not reduce its reli-ability.A possible formulation for JACK which satis-fies that criteria is:JACK(X,M,A) ?
P (?a, a?
?
A.QUEEN(a) >0 ?
QUEEN(a?)
> 0 ?
?x ?
X.x(a, a?)
?
x(a,m))i.e.
the probability over all model summaries mof finding a couple of automatic summaries a, a?284which are closer to each other than to m accordingto all metrics.This measure satisfies all three constraints: itcan be enlarged by increasing the similarity of thepeers to the models (the x(m,a) factor in the in-equality) or decreasing the similarity between au-tomatic summaries (the x(a, a?)
factor in the in-equality).
Finally, adding elements to A can onlyincrease the chances of finding a pair of automaticsummaries satisfying the condition in JACK.Figure 4: JACK valuesFigure 4 illustrates how JACK works: in theleftmost part of the figure, peers are grouped to-gether and far from the models, giving a low JACKvalue.
In the rightmost part of the figure, peers aredistributed around the set of models, closely sur-rounding them, receiving a high JACK value.4 A Case of StudyIn order to test the behaviour of our evaluationframework, we have applied it to the ISCORPUSdescribed in (Amigo et al, 2004).
The ISCOR-PUS was built to study an Information Synthesistask, where a (large) set of relevant documents hasto be studied to give a brief, well-organised answerto a complex need for information.
This corpuscomprises:?
Eight topics extracted from the CLEF Span-ish Information Retrieval test set, slightly re-worded to move from a document retrievaltask (find documents about hunger strikesin...) into an Information Synthesis task(make a report about major causes of hungerstrikes in...).?
One hundred relevant documents per topictaken from the CLEF EFE 1994 Spanishnewswire collection.?
M : Manual extractive summaries for everytopic made by 9 different users, with a 50-sentence upper limit (half the number of rel-evant documents).?
A: 30 automatic reports for every topic madewith baseline strategies.
The 10 reports withhighest sentence overlap with the manualsummaries were selected as a way to increasethe quality of the baseline set.We have considered the following similaritymetrics:ROUGESim: ROUGE is a standard measureto evaluate summarisation systems based onn-gram recall.
We have used ROUGE-1(only unigrams with lemmatization and stopword removal), which gives good results withstandard summaries (Lin and Hovy, 2003a).ROUGE can be turned into a similarity met-ric ROUGESim simply by considering onlyone model when computing its value.SentencePrecision: Given a reference and a con-trastive summary, the number of fragments ofthe contrastive summary which are also in thereference summary, in relation to the size ofthe reference summary.SentenceRecall: Given a reference and a con-trastive summary, the number of fragments ofthe reference summary which are also in thecontrastive summary, in relation to the size ofthe contrastive summary.DocSim: The number of documents used to selectfragments in both summaries, in relation tothe size of the contrastive summary.VectModelSim: Derived from the Euclidean dis-tance between vectors of relative word fre-quencies representing both summaries.NICOS (key concept overlap): Same as Vect-ModelSim, but using key-concepts (manuallyidentified by the human summarisers afterproducing the summary) instead of all non-empty words.285TruncatedVectModeln: Same as VectModelSim,but using only the n more frequent termsin the reference summary.
We have used10 variants of this measure with n =1, 8, 64, 512.4.1 Quality of Similarity Metric SetsFigure 5 shows the quality (KING values averagedover the eight ISCORPUS topics) of every individ-ual metric.
The rightmost part of the figure alsoshows the quality of two metric sets:?
The first one ({ROUGESim, VectModelSim,TruncVectModel.1}) is the metric set thatmaximises KING, using only similarity met-rics that do not require manual annotation(i.e.
excluding NICOS) or can only be ap-plied to extractive summaries (i.e.
DocSim,SentenceRecall and SentencePrecision).?
The second one ({ TruncVectModel.1, ROU-GESim, DocSim, VectModelSim }) is the bestcombination considering all metrics.The best result of individual metrics is obtainedby ROUGESim (0.39).
All other individual met-rics give scores below 0.31.
Both metric sets, onthe other, are better than ROUGESim alone, con-firming that metric combination is feasible to im-prove system evaluation.
The quality of the bestmetric set (0.47) is 21% better than ROUGESim.4.2 Reliability of the test setThe 30 automatic summaries (baselines) per topicwere built with four different classes of strategies:i) picking up the first sentence from assorted sub-sets of documents, ii) picking up first and secondsentences from assorted documents, iii) pickingup first, second or third sentences from assorteddocuments, and iv) picking up whole documentswith different algorithms to determine which arethe most representative documents.Figure 6 shows the reliability (JACK) of everysubset, and the reliability of the whole set of au-tomatic summaries, computed with the best met-ric set.
Note that the individual subsets are allbelow 0.2, while the reliability of the full set ofpeers goes up to 0.57.
That means that the con-dition in JACK is satisfied for more than half ofthe models.
This value would probably be higherif state-of-the-art summarisation techniques wererepresented in the set of peers.5 Testing the predictive power of theframeworkThe QARLA probabilistic framework is designedto evaluate automatic summarisation systems and,at the same time, similarity metrics conceived aswell to evaluate summarisation systems.
There-fore, testing the validity of the QARLA proposalimplies some kind of meta-meta-evaluation, some-thing which seems difficult to design or even todefine.It is relatively simple, however, to perform somesimple cross-checkings on the ISCORPUS data toverify that the qualitative information describedabove is reasonable.
This is the test we have im-plemented:If we remove a model m from M , and pretend itis the output of an automatic summariser, we canevaluate the peers set A and the new peer m usingM ?
= M\{m} as the new model set.
If the evalu-ation metric is good, the quality of the new peer mshould be superior to all other peers inA.
What wehave to check, then, is whether the average qualityof a human summariser on all test cases (8 topicsin ISCORPUS) is superior to the average qualityof any automatic summariser.
We have 9 humansubjects in the ISCORPUS test bed; therefore, wecan repeat this test nine times.With this criterion, we can compare our qualitymeasure Q with state-of-the-art evaluation mea-sures such as ROUGE variants.
Table 1 showsthe results of applying this test on ROUGE-1, ROUGE-2, ROUGE-3, ROUGE-4 (as state-of-the-art references) and QUEEN(ROUGESim),QUEEN(Best Metric Combination) as representa-tives of the QARLA framework.
Even if the test isvery limited by the number of topics, it confirmsthe potential of the framework, with the highestKING metric combination doubling the perfor-mance of the best ROUGE measure (6/9 versus 3/9correct detections).286Figure 5: Quality of similarity metricsFigure 6: Reliability of ISCORPUS peer setsEvaluation criterion human summarisers ranked firstROUGE-1 3/9ROUGE-2 2/9ROUGE-3 1/9ROUGE-4 1/9QUEEN(ROUGESim) 4/9QUEEN(Best Metric Combination) 6/9Table 1: Results of the test of identifying the manual summariser2876 Related work and discussion6.1 Application of similarity metrics toevaluate summariesBoth in Text Summarisation and Machine Trans-lation, the automatic evaluation of systems con-sists of computing some similarity metric betweenthe system output and a human model summary.Systems are then ranked in order of decreasingsimilarity to the gold standard.
When there aremore than one reference items, similarity is calcu-lated over a pseudo-summary extracted from everymodel.
BLEU (Papineni et al, 2001) and ROUGE(Lin and Hovy, 2003a) are the standard similar-ity metrics used in Machine Translation and TextSummarisation.
Generating a pseudo-summaryfrom every model, the results of a evaluation met-ric might depend on the scale properties of themetric regarding different models; our QUEENmeasure, however, does not depend on scales.Another problem of the direct application of asingle evaluation metric to rank systems is how tocombine different metrics.
The only way to dothis is by designing an algebraic combination ofthe individual metrics into a new combined met-ric, i.e.
by deciding the weight of each individualmetric beforehand.
In our framework, however, itis not necessary to prescribe how similarity met-rics should be combined, not even to know whichones are individually better indicators.6.2 Meta-evaluation of similarity metricsThe question of how to know which similar-ity metric is best to evaluate automatic sum-maries/translations has been addressed by?
comparing the quality of automatic itemswith the quality of manual references (Culyand Riehemann, 2003; Lin and Hovy,2003b).
If the metric does not identify thatthe manual references are better, then it is notgood enough for evaluation purposes.?
measuring the correlation between the valuesgiven by different metrics (Coughlin, 2003).?
measuring the correlation between the rank-ings generated by each metric and rank-ings generated by human assessors.
(JosephP.
Turian and Melamed, 2003; Lin and Hovy,2003a).The methodology which is closest to our frame-work is ORANGE (Lin, 2004), which evaluates asimilarity metric using the average ranks obtainedby reference items within a baseline set.
As inour framework, ORANGE performs an automaticmeta-evaluation, there is no need for human as-sessments, and it does not depend on the scaleproperties of the metric being evaluated (becausechanges of scale preserve rankings).
The OR-ANGE approach is, indeed, closely related to theoriginal QARLA measure introduced in (Amigo etal., 2004).Our KING,QUEEN, JACK framework, how-ever, has a number of advantages over ORANGE:?
It is able to combine different metrics, andevaluate the quality of metric sets, withoutany a-priori weighting of their relative impor-tance.?
It is not sensitive to repeated (or very similar)baseline elements.?
It provides a mechanism, JACK, to checkwhether a set X,M,A of metrics, manualand baseline items is reliable enough to pro-duce a stable evaluation of automatic sum-marisation systems.Probably the most significant improvement overORANGE is the ability of KING,QUEEN, JACKto combine automatically the information of dif-ferent metrics.
We believe that a comprehensiveautomatic evaluation of a summary must neces-sarily capture different aspects of the problem withdifferent metrics, and that the results of every indi-vidual metric should not be combined in any pre-scribed algebraic way (such as a linear weightedcombination).
Our framework satisfies this con-dition.
An advantage of ORANGE, however, isthat it does not require a large number of gold stan-dards to reach stability, as in the case of QARLA.Finally, it is interesting to compare the rankingsproduced by QARLA with the output of humanassessments, even if the philosophy of QARLAis not considering human assessments as the goldstandard for evaluation.
Our initial tests on DUC288Figure 7: KING vs Pearson correlation with manual rankings in DUC for 1024 metrics combinationstest beds are very promising, reaching Pearsoncorrelations of 0.9 and 0.95 between human as-sessments and QUEEN values for DUC 2004 tasks2 and 5 (Over and Yen, 2004), using metric setswith highest KING values.
The figure 7 showshow Pearson correlation grows up with higherKING values for 1024 metric combinations.AcknowledgmentsWe are indebted to Ed Hovy, Donna Harman, PaulOver, Hoa Dang and Chin-Yew Lin for their in-spiring and generous feedback at different stagesin the development of QARLA.
We are also in-debted to NIST for hosting Enrique Amigo?
as avisitor and for providing the DUC test beds.
Thiswork has been partially supported by the Spanishgovernment, project R2D2 (TIC-2003-7180).ReferencesE.
Amigo, V. Peinado, J. Gonzalo, A.
Pen?as, andF.
Verdejo.
2004.
An empirical study of informa-tion synthesis task.
In Proceedings of the 42th An-nual Meeting of the Association for ComputationalLinguistics (ACL), Barcelona, July.Deborah Coughlin.
2003.
Correlating Automated andHuman Assessments of Machine Translation Qual-ity.
In In Proceedings of MT Summit IX, New Or-leans,LA.Christopher Culy and Susanne Riehemann.
2003.
TheLimits of N-Gram Translation Evaluation Metrics.In Proceedings of MT Summit IX, New Orleans,LA.Luke Shen Joseph P. Turian and I. Dan Melamed.2003.
Evaluation of Machine Translation and itsEvaluation.
In In Proceedings of MT Summit IX,New Orleans,LA.C.
Lin and E. H. Hovy.
2003a.
Automatic Evaluationof Summaries Using N-gram Co-ocurrence Statis-tics.
In Proceeding of 2003 Language TechnologyConference (HLT-NAACL 2003).Chin-Yew Lin and Eduard Hovy.
2003b.
The Poten-tial and Limitations of Automatic Sentence Extrac-tion for Summarization.
In Dragomir Radev and Si-mone Teufel, editors, HLT-NAACL 2003 Workshop:Text Summarization (DUC03), Edmonton, Alberta,Canada, May 31 - June 1.
Association for Computa-tional Linguistics.C.
Lin.
2004.
Orange: a Method for Evaluating Au-tomatic Metrics for Machine Translation.
In Pro-ceedings of the 36th Annual Conference on Compu-tational Linguisticsion for Computational Linguis-tics (Coling?04), Geneva, August.P.
Over and J.
Yen.
2004.
An introduction to duc 2004intrinsic evaluation of generic new text summariza-tion systems.
In Proceedings of DUC 2004 Docu-ment Understanding Workshop, Boston.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2001.Bleu: a method for automatic evaluation of machinetranslation.
In Proceedings of the 40th Annual Meet-ing of the Association for Computational Linguistics(ACL), pages 311?318, Philadelphia, jul.289
