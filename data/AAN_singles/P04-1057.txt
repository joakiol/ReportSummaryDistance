Error Mining for Wide-Coverage Grammar EngineeringGertjan van NoordAlfa-informatica University of GroningenPOBox 7169700 AS GroningenThe Netherlandsvannoord@let.rug.nlAbstractParsing systems which rely on hand-coded linguis-tic descriptions can only perform adequately in asfar as these descriptions are correct and complete.The paper describes an error mining technique todiscover problems in hand-coded linguistic descrip-tions for parsing such as grammars and lexicons.
Byanalysing parse results for very large unannotatedcorpora, the technique discovers missing, incorrector incomplete linguistic descriptions.The technique uses the frequency of n-grams ofwords for arbitrary values of n. It is shown how anew combination of suffix arrays and perfect hashfinite automata allows an efficient implementation.1 IntroductionAs we all know, hand-crafted linguistic descriptionssuch as wide-coverage grammars and large scaledictionaries contain mistakes, and are incomplete.In the context of parsing, people often construct setsof example sentences that the system should be ableto parse correctly.
If a sentence cannot be parsed,it is a clear sign that something is wrong.
Thistechnique only works in as far as the problems thatmight occur have been anticipated.
More recently,tree-banks have become available, and we can applythe parser to the sentences of the tree-bank and com-pare the resulting parse trees with the gold standard.Such techniques are limited, however, because tree-banks are relatively small.
This is a serious prob-lem, because the distribution of words is Zipfian(there are very many words that occur very infre-quently), and the same appears to hold for syntacticconstructions.In this paper, an error mining technique is de-scribed which is very effective at automatically dis-covering systematic mistakes in a parser by usingvery large (but unannotated) corpora.
The idea isvery simple.
We run the parser on a large set of sen-tences, and then analyze those sentences the parsercannot parse successfully.
Depending on the na-ture of the parser, we define the notion ?success-ful parse?
in different ways.
In the experimentsdescribed here, we use the Alpino wide-coverageparser for Dutch (Bouma et al, 2001; van der Beeket al, 2002b).
This parser is based on a large con-structionalist HPSG for Dutch as well as a very largeelectronic dictionary (partly derived from CELEX,Parole, and CGN).
The parser is robust in the sensethat it essentially always produces a parse.
If a fullparse is not possible for a given sentence, then theparser returns a (minimal) number of parsed non-overlapping sentence parts.
In the context of thepresent paper, a parse is called successful only if theparser finds an analysis spanning the full sentence.The basic idea is to compare the frequency ofwords and word sequences in sentences that can-not be parsed successfully with the frequency of thesame words and word sequences in unproblematicsentences.
As we illustrate in section 3, this tech-nique obtains very good results if it is applied tolarge sets of sentences.To compute the frequency of word sequences ofarbitrary length for very large corpora, we use a newcombination of suffix arrays and perfect hash finiteautomata.
This implementation is described in sec-tion 4.The error mining technique is able to discoversystematic problems which lead to parsing failure.This includes missing, incomplete and incorrect lex-ical entries and grammar rules.
Problems whichcause the parser to assign complete but incorrectparses cannot be discovered.
Therefore, tree-banksand hand-crafted sets of example sentences remainimportant to discover problems of the latter type.2 A parsability metric for word sequencesThe error mining technique assumes we have avail-able a large corpus of sentences.
Each sentence is asequence of words (of course, words might includetokens such as punctuation marks, etc.).
We runthe parser on all sentences, and we note for whichsentences the parser is successful.
We define theparsability of a word R(w) as the ratio of the num-ber of times the word occurs in a sentence with asuccessful parse (C(w|OK)) and the total numberof sentences that this word occurs in (C(w)):R(w) =C(w|OK)C(w)Thus, if a word only occurs in sentences that can-not be parsed successfully, the parsability of thatword is 0.
On the other hand, if a word only occursin sentences with a successful parse, its parsabil-ity is 1.
If we have no reason to believe that aword is particularly easy or difficult, then we ex-pect its parsability to be equal to the coverage of theparser (the proportion of sentences with a successfulparse).
If its parsability is (much) lower, then thisindicates that something is wrong.
For the experi-ments described below, the coverage of the parserlies between 91% and 95%.
Yet, for many wordswe found parsability values that were much lowerthan that, including quite a number of words withparsability 0.
Below we show some typical exam-ples, and discuss the types of problem that are dis-covered in this way.If a word has a parsability of 0, but its frequencyis very low (say 1 or 2) then this might easily bedue to chance.
We therefore use a frequency cut-off(e.g.
5), and we ignore words which occur less oftenin sentences without a successful parse.In many cases, the parsability of a word dependson its context.
For instance, the Dutch word viais a preposition.
Its parsability in a certain exper-iment was more than 90%.
Yet, the parser wasunable to parse sentences with the phrase via viawhich is an adverbial expression which means viasome complicated route.
For this reason, we gener-alize the parsability of a word to word sequencesin a straightforward way.
We write C(wi .
.
.
wj)for the number of sentences in which the sequencewi .
.
.
wj occurs.
Furthermore, C(wi .
.
.
wj |OK),is the number of sentences with a successful parsewhich contain the sequence wi .
.
.
wj .
The parsabil-ity of a sequence is defined as:R(wi .
.
.
wj) =C(wi .
.
.
wj |OK)C(wi .
.
.
wj)If a word sequence wi .
.
.
wj has a low parsabil-ity, then this might be because it is part of a dif-ficult phrase.
It might also be that part of the se-quence is the culprit.
In order that we focus onthe relevant sequence, we consider a longer se-quence wh .
.
.
wi .
.
.
wj .
.
.
wk only if its parsabil-ity is lower than the parsability of each of its sub-strings:R(wh .
.
.
wi .
.
.
wj .
.
.
wk) < R(wi .
.
.
wj)This is computed efficiently by considering theparsability of sequences in order of length (shortersequences before longer ones).We construct a parsability table, which is a list ofn-grams sorted with respect to parsability.
An n-gram is included in the parsability table, provided:?
its frequency in problematic parses is largerthan the frequency cut-off?
its parsability is lower than the parsability ofall of its sub-stringsThe claim in this paper is that a parsability tableprovides a wealth of information about systematicproblems in the grammar and lexicon, which is oth-erwise hard to obtain.3 Experiments and results3.1 First experimentData.
For our experiments, we used the TwenteNieuws Corpus, version pre-release 0.1.1 This cor-pus contains among others a large collection ofnews articles from various Dutch newspapers in theperiod 1994-2001.
In addition, we used all newsarticles from the Volkskrant 1997 (available on CD-ROM).
In order that this material can be parsed rel-atively quickly, we discarded all sentences of morethan 20 words.
Furthermore, a time-out per sen-tence of twenty CPU-seconds was enforced.
TheAlpino parser normally exploits a part-of-speech tagfilter for efficient parsing (Prins and van Noord,2003) which was switched off, to ensure that theresults were not influenced by mistakes due to thisfilter.
In table 1 we list some basic quantitative factsabout this material.We exploited a cluster of Linux PCs for parsing.If only a single PC had been available, it would havetaken in the order of 100 CPU days, to construct thematerial described in table 1.These experiments were performed in the autumnof 2002, with the Alpino parser available then.
Be-low, we report on more recent experiments with thelatest version of the Alpino parser, which has beenimproved quite a lot on the basis of the results of theexperiments described here.Results.
For the data described above, we com-puted the parsability table, using a frequency cut-off of 5.
In figure 1 the frequencies of parsabilityscores in the parsability table are presented.
Fromthe figure, it is immediately clear that the relativelyhigh number of word sequences with a parsability of(almost) zero cannot be due to chance.
Indeed, the1http://wwwhome.cs.utwente.nl/?druid/TwNC/TwNC-main.htmlnewspaper sents coverage %NRC 1994 582K 91.2NRC 1995 588K 91.5Volkskrant 1997 596K 91.6AD 2000 631K 91.5PAROOL 2001 529K 91.3total 2,927K 91.4Table 1: Overview of corpus material; first experi-ment (Autumn 2002).ParsabilityFrequency0.0 0.2 0.4 0.6 0.8 1.00500015000Figure 1: Histogram of the frequencies of parsabil-ity scores occurring in parsability table.
Frequencycut-off=5; first experiment (Autumn 2002).parsability table starts with word sequences whichconstitute systematic problems for the parser.
Inquite a lot of cases, these word sequences origi-nate from particular types of newspaper text withidiosyncratic syntax, such as announcements of newbooks, movies, events, television programs etc.
; aswell as checkers, bridge and chess diagrams.
An-other category consists of (parts of) English, Frenchand German phrases.We also find frequent spelling mistakes such asde de where only a single de (the definite article)is expected, and heben for hebben (to have), inden-tiek for identiek (identical), koninging for koningin(queen), etc.
Other examples include wordt ik (be-comes I), vindt ik (finds I), vind hij (find he) etc.We now describe a number of categories of ex-amples which have been used to improve the parser.Tokenization.
A number of n-grams with lowparsability scores point towards systematic mistakesduring tokenization.
Here are a number of exam-ples:22The @ symbol indicates a sentence boundary.R C n-gram0.00 1884 @ .
@ .0.00 385 @ !
@ !0.00 22 ?s advocaat ?s lawyer0.11 8 H. ?s H. ?s0.00 98 @ , roept @ , yells0.00 20 @ , schreeuwt @ , screams0.00 469 @ , vraagt @ , asksThe first and second n-gram indicate sentenceswhich start with a full stop or an exclamation mark,due to a mistake in the tokenizer.
The third andfourth n-grams indicate a problem the tokenizer hadwith a sequence of a single capital letter with a dot,followed by the genitive marker.
The grammar as-sumes that the genitive marking is attached to theproper name.
Such phrases occur frequently in re-ports on criminals, which are indicated in news pa-per only with their initials.
Another systematic mis-take is reflected by the last n-grams.
In reportedspeech such as(1) JeYoubentaregek!,crazy!,roeptyellsFranca.Franca.Franca yells: You are crazy!the tokenizer mistakenly introduced a sentenceboundary between the exclamation mark and thecomma.
On the basis of examples such as these,the tokenizer has been improved.Mistakes in the lexicon.
Another reason an n-gram receives a low parsability score is a mistakein the lexicon.
The following table lists two typicalexamples:R C n-gram0.27 18 de kaft the cover0.30 7 heeft opgetreden has performedIn Dutch, there is a distinction between neuter andnon-neuter common nouns.
The definite article decombines with non-neuter nouns, whereas neuternouns select het.
The common noun kaft, for exam-ple, combines with the definite article de.
However,according to the dictionary, it is a neuter commonnoun (and thus would be expected to combine onlywith the definite article het).
Many similar errorswere discovered.Another syntactic distinction that is listed in thedictionary is the distinction between verbs whichtake the auxiliary hebben (to have) to construct aperfect tense clause vs. those that take the auxiliaryzijn (to be).
Some verbs allow both possibilities.The last example illustrates an error in the dictio-nary with respect to this syntactic feature.Incomplete lexical descriptions.
The majority ofproblems that the parsability scores indicate reflectincomplete lexical entries.
A number of examplesis provided in the following table:R C n-gram0.00 11 begunstigden favoured (N/V)0.23 10 zich eraan dat self there-on that0.08 12 aan te klikken on to click0.08 12 doodzonde dat mortal sin that0.15 11 zwarts black?s0.00 16 dupe van victim of0.00 13 het Turks .
the TurkishThe word begunstigden is ambiguous between onthe one hand the past tense of the verb begunstigen(to favour) and on the other hand the plural nominal-ization begunstigden (beneficiaries).
The dictionarycontained only the first reading.The sequence zich eraan dat illustrates a missingvalency frame for verbs such as ergeren (to irritate).In Dutch, verbs which take a prepositional comple-ment sometimes also allow the object of the prepo-sitional complement to be realized by a subordinate(finite or infinite) clause.
In that case, the preposi-tional complement is R-pronominalized.
Examples:(2) a. HijHeergertis-irritatedzichselfaanonzijnhisaanwezigheidpresenceHe is irritated by his presenceb.
HijHeergertis-irritatedzichselfertherenietnotaanondatthat.
.
.. .
.He is not irritated by the fact that .
.
.The sequence aan te klikken is an example of averb-particle combination which is not licensed inthe dictionary.
This is a relatively new verb whichis used for click in the context of buttons and hyper-links.The sequence doodzonde dat illustrates a syn-tactic construction where a copula combines witha predicative complement and a sentential subject,if that predicative complement is of the appropriatetype.
This type is specified in the dictionary, but wasmissing in the case of doodzonde.
Example:(3) HetItisisdoodzondemortal-sindatthathijheslaaptsleepsThat he is sleeping is a pityThe word zwarts should have been analyzed as agenitive noun, as in (typically sentences about chessor checkers):(4) HijHekeeklookednaaratzwartsblack?storenrookwhereas the dictionary only assigned the inflectedadjectival reading.The sequence dupe van illustrates an example ofan R-pronominalization of a PP modifier.
This isgenerally not possible, except for (quite a large)number of contexts which are determined by theverb and the object:(5) a. HijHeisisdethedupevictimvanofjouwyourvergissingmistakeHe has to suffer for your mistakeb.
HijHeisisdaartherenunowdethedupevictimvanofHe has to suffer for itThe word Turks can be both an adjective (Turkish)or a noun the Turkish language.
The dictionary con-tained only the first reading.Very many other examples of incomplete lexicalentries were found.Frozen expressions with idiosyncratic syntax.Dutch has many frozen expressions and idioms witharchaic inflection and/or word order which breaksthe parser.
Examples include:R C n-gram0.00 13 dan schaadt het then harms it0.00 13 @ God zij @ God be[I]0.22 25 God zij God be[I]0.00 19 Het zij zo It be[I] so0.45 12 goeden huize good house[I]0.09 11 berge mountain[I]0.00 10 hele gedwaald whole[I] dwelled0.00 14 te weegThe sequence dan schaadt het is part of the id-iom Baat het niet, dan schaadt het niet (meaning: itmight be unsure whether something is helpful, butin any case it won?t do any harm).
The sequenceGod zij is part of a number of archaic formulas suchas God zij dank (Thank God).
In such examples,the form zij is the (archaic) subjunctive form of theDutch verb zijn (to be).
The sequence Het zij zo isanother fixed formula (English: So be it), contain-ing the same subjunctive.
The phrase van goedenhuize (of good family) is a frozen expression witharchaic inflection.
The word berge exhibits archaicinflection on the word berg (mountain), which onlyoccurs in the idiomatic expression de haren rijzenmij te berge (my hair rises to the mountain) whichexpresses a great deal of surprise.
The n-gram helegedwaald only occurs in the idiom Beter ten halvegekeerd dan ten hele gedwaald: it is better to turnhalfway, then to go all the way in the wrong direc-tion.
Many other (parts of) idiomatic expressionswere found in the parsability table.The sequence te weeg only occurs as part of thephrasal verb te weeg brengen (to cause).Incomplete grammatical descriptions.
Al-though the technique strictly operates at the levelof words and word sequences, it is capable ofindicating grammatical constructions that are nottreated, or not properly treated, in the grammar.R C n-gram0.06 34 Wij Nederlanders We Dutch0.08 23 Geeft niet Matters not0.00 15 de alles the everything0.10 17 Het laten The letting0.00 10 tenzij .
unless .The sequence Wij Nederlanders constitutes an ex-ample of a pronoun modified by means of an appo-sition (not allowed in the grammar) as in(6) WijWeNederlandersDutcheteneatvaakoftenaardappelspotatoesWe, the Dutch, often eat potatoesThe sequence Geeft niet illustrates the syntac-tic phenomenon of topic-drop (not treated in thegrammar): verb initial sentences in which the topic(typically the subject) is not spelled out.
The se-quence de alles occurs with present participles (usedas prenominal modifiers) such as overheersende asin de alles overheersende paniek (literally: the alldominating panic, i.e., the panic that dominated ev-erything).
The grammar did not allow prenominalmodifiers to select an NP complement.
The se-quence Het laten often occurs in nominalizationswith multiple verbs.
These were not treated in thegrammar.
Example:(7) HetThelatenlettingzienseevanofproblemenproblemsShowing problemsThe word sequence tenzij .
is due to sentences inwhich a subordinate coordinator occurs without acomplement clause:(8) GijThouzultshalltnietnotdoden,kill,tenzij.unless.A large number of n-grams also indicate ellipticalstructures, not treated in that version of the gram-mar.
Another fairly large source of errors are ir-regular named entities (Gil y Gil, Osama bin Laden.
.
.
).newspaper # sentences coverage %NRC 1994 552,833 95.0Volkskrant 1997 569,314 95,2AD 2000 662,380 95,7Trouw 1999 406,339 95,5Volkskrant 2001 782,645 95,1Table 2: Overview of corpus material used for theexperiments; second experiment (January 2004).3.2 Later experimentMany of the errors and omissions that were foundon the basis of the parsability table have been cor-rected.
As can be seen in table 2, the coverageobtained by the improved parser increased substan-tially.
In this experiment, we also measured the cov-erage on additional sets of sentences (all sentencesfrom the Trouw 1999 and Volkskrant 2001 news-paper, available in the TwNC corpus).
The resultsshow that coverage is similar on these unseen test-sets.Obviously, coverage only indicates how often theparser found a full parse, but it does not indicatewhether that parse actually was the correct parse.For this reason, we also closely monitored the per-formance of the parser on the Alpino tree-bank3(van der Beek et al, 2002a), both in terms of parsingaccuracy and in terms of average number of parsesper sentence.
The average number of parses in-creased, which is to be expected if the grammar andlexicon are extended.
Accuracy has been steadilyincreasing on the Alpino tree-bank.
Accuracy isdefined as the proportion of correct named depen-dency relations of the first parse returned by Alpino.Alpino employs a maximum entropy disambigua-tion component; the first parse is the most promisingparse according to this statistical model.
The maxi-mum entropy disambiguation component of Alpinoassigns a score S(x) to each parse x:S(x) =?i?ifi(x) (1)where fi(x) is the frequency of a particular feature iin parse x and ?i is the corresponding weight of thatfeature.
The probability of a parse x for sentence wis then defined as follows, where Y (w) are all theparses of w:p(x|w) =exp (S(x))?y?Y (w) exp (S(y))(2)The disambiguation component is described in de-tail in Malouf and van Noord (2004).3http://www.let.rug.nl/?vannoord/trees/Time (days)Accuracy0 50 100 150 200 250 300 35084.585.586.5Figure 2: Development of Accuracy of the Alpinoparser on the Alpino Tree-bankFigure 2 displays the accuracy from May 2003-May 2004.
During this period many of the prob-lems described earlier were solved, but other partsof the system were improved too (in particular, thedisambiguation component was improved consider-ably).
The point of the graph is that apparently theincrease in coverage has not been obtained at thecost of decreasing accuracy.4 A note on the implementationThe most demanding part of the implementationconsists of the computation of the frequency of n-grams.
If the corpus is large, or n increases, simpletechniques break down.
For example, an approachin which a hash data-structure is used to maintainthe counts of each n-gram, and which incrementsthe counts of each n-gram that is encountered, re-quires excessive amounts of memory for large nand/or for large corpora.
On the other hand, if amore compact data-structure is used, speed becomesan issue.
Church (1995) shows that suffix arrayscan be used for efficiently computing the frequencyof n-grams, in particular for larger n. If the cor-pus size increases, the memory required for the suf-fix array may become problematic.
We propose anew combination of suffix arrays with perfect hashfinite automata, which reduces typical memory re-quirements by a factor of five, in combination witha modest increase in processing efficiency.4.1 Suffix arraysSuffix arrays (Manber and Myers, 1990; Yamamotoand Church, 2001) are a simple, but useful data-structure for various text-processing tasks.
A corpusis a sequence of characters.
A suffix array s is an ar-ray consisting of all suffixes of the corpus, sorted al-phabetically.
For example, if the corpus is the stringabba, the suffix array is ?a,abba,ba,bba?.Rather than writing out each suffix, we use integersi to refer to the suffix starting at position i in thecorpus.
Thus, in this case the suffix array consistsof the integers ?3, 0, 2, 1?.It is straightforward to compute the suffix array.For a corpus of k + 1 characters, we initialize thesuffix array by the integers 0 .
.
.
k. The suffix ar-ray is sorted, using a specialized comparison rou-tine which takes integers i and j, and alphabeticallycompares the strings starting at i and j in the cor-pus.4Once we have the suffix array, it is simple to com-pute the frequency of n-grams.
Suppose we are in-terested in the frequency of all n-grams for n = 10.We simply iterate over the elements of the suffix ar-ray: for each element, we print the first ten wordsof the corresponding suffix.
This gives us all oc-currences of all 10-grams in the corpus, sorted al-phabetically.
We now count each 10-gram, e.g.
bypiping the result to the Unix uniq -c command.4.2 Perfect hash finite automataSuffix arrays can be used more efficiently to com-pute frequencies of n-grams for larger n, withthe help of an additional data-structure, known asthe perfect hash finite automaton (Lucchiesi andKowaltowski, 1993; Roche, 1995; Revuz, 1991).The perfect hash automaton for an alphabeticallysorted finite set of words w0 .
.
.
wn is a weightedminimal deterministic finite automaton which mapswi ?
i for each w0?i?n.
We call i the word codeof wi.
An example is given in figure 3.Note that perfect hash automata implement an or-der preserving, minimal perfect hash function.
Thefunction is minimal, in the sense that n keys aremapped into the range 0 .
.
.
n ?
1, and the functionis order preserving, in the sense that the alphabeticorder of words is reflected in the numeric order ofword codes.4.3 Suffix arrays with wordsIn the approach of Church (1995), the corpus isa sequence of characters (represented by integersreflecting the alphabetic order).
A more space-efficient approach takes the corpus as a sequence ofwords, represented by word codes reflecting the al-phabetic order.To compute frequencies of n-grams for larger n,we first compute the perfect hash finite automatonfor all words which occur in the corpus,5 and map4The suffix sort algorithm of Peter M. McIlroy and M.Douglas McIlroy is used, available as http://www.cs.dartmouth.edu/?doug/ssort.c; This algorithm is ro-bust against long repeated substrings in the corpus.5We use an implementation by Jan Daciuk freely avail-able from http://www.eti.pg.gda.pl/?jandac/fsa.html.d::1cr::5s::7e::1rg::1c kou::2cs::1lottkccoFigure 3: Example of a perfect hash finite automa-ton for the words clock, dock, dog, duck, dust, rock,rocker, stock.
Summing the weights along an ac-cepting path in the automaton yields the rank of theword in alphabetic ordering.the corpus to a sequence of integers, by mappingeach word to its word code.
Suffix array construc-tion then proceeds on the basis of word codes, ratherthan character codes.This approach has several advantages.
The rep-resentation of both the corpus and the suffix arrayis more compact.
If the average word length is k,then the corresponding arrays are k times smaller(but we need some additional space for the perfecthash automaton).
In Dutch, the average word lengthk is about 5, and we obtained space savings in thatorder.If the suffix array is shorter, sorting should befaster too (but we need some additional time to com-pute the perfect hash automaton).
In our experience,sorting is about twice as fast for word codes.4.4 Computing parsability tableTo compute parsability scores, we assume there aretwo corpora cm and ca, where the first is a sub-corpus of the second.
cm contains all sentencesfor which parsing was not successful.
ca containsall sentences overall.
For both corpora, we com-pute the frequency of all n-grams for all n; n-gramswith a frequency below a specified frequency cut-off are ignored.
Note that we need not impose ana priori maximum value for n; since there is a fre-quency cut-off, for some n there simply aren?t anysequences which occur more frequently than thiscut-off.
The two n-gram frequency files are orga-nized in such a way that shorter n-grams precedelonger n-grams.The two frequency files are then combined asfollows.
Since the frequency file corresponding tocm is (much) smaller than the file correspondingto ca, we read the first file into memory (into ahash data structure).
We then iteratively read ann-gram frequency from the second file, and com-pute the parsability of that n-gram.
In doing so,we keep track of the parsability scores assigned toprevious (hence shorter) n-grams, in order to en-sure that larger n-grams are only reported in casethe parsability scores decrease.
The final step con-sists in sorting all remaining n-grams with respectto their parsability.To give an idea of the practicality of the ap-proach, consider the following data for one of theexperiments described above.
For a corpus of2,927,016 sentences (38,846,604 words, 209Mb),it takes about 150 seconds to construct the per-fect hash automaton (mostly sorting).
The automa-ton is about 5Mb in size, to represent 677,488 dis-tinct words.
To compute the suffix array and fre-quencies of all n-grams (cut-off=5), about 15 min-utes of CPU-time are required.
Maximum runtimememory requirements are about 400Mb.
The re-sult contains frequencies for 1,641,608 distinct n-grams.
Constructing the parsability scores on thebasis of the n-gram files only takes 10 secondsCPU-time, resulting in parsability scores for 64,998n-grams (since there are much fewer n-grams whichactually occur in problematic sentences).
The ex-periment was performed on a Intel Pentium III,1266MHz machine running Linux.
The software isfreely available from http://www.let.rug.nl/?vannoord/software.html.5 DiscussionAn error mining technique has been presentedwhich is very helpful in identifying problems inhand-coded grammars and lexicons for parsing.
Animportant ingredient of the technique consists of thecomputation of the frequency of n-grams of wordsfor arbitrary values of n. It was shown how a newcombination of suffix arrays and perfect hash fi-nite automata allows an efficient implementation.A number of potential improvements can be envi-sioned.In the definition of R(w), the absolute frequencyof w is ignored.
Yet, if w is very frequent, R(w)is more reliable than if w is not frequent.
There-fore, as an alternative, we also experimented witha set-up in which an exact binomial test is appliedto compute a confidence interval for R(w).
Resultscan then be ordered with respect to the maximum ofthese confidence intervals.
This procedure seemedto improve results somewhat, but is computation-ally much more expensive.
For the first experimentdescribed above, this alternative set-up results in aparsability table of 42K word tuples, whereas theoriginal method produces a table of 65K word tu-ples.R C n-gram0.00 8 Beter ten0.20 12 ten halve0.15 11 halve gekeerd0.00 8 gekeerd dan0.09 10 dan ten hele0.69 15 dan ten0.17 10 ten hele0.00 10 hele gedwaald0.00 8 gedwaald .0.20 10 gedwaaldTable 3: Multiple n-grams indicating same errorThe parsability table only contains longer n-grams if these have a lower parsability than the cor-responding shorter n-grams.
Although this heuristicappears to be useful, it is still possible that a singleproblem is reflected multiple times in the parsabil-ity table.
For longer problematic sequences, theparsability table typically contains partially over-lapping parts of that sequence.
This phenomenonis illustrated in table 3 for the idiom Beter tenhalve gekeerd dan ten hele gedwaald discussed ear-lier.
This suggests that it would be useful to con-sider other heuristics to eliminate such redundancy,perhaps by considering statistical feature selectionmethods.The definition used in this paper to identify a suc-cessful parse is a rather crude one.
Given that gram-mars of the type assumed here typically assign verymany analyses to a given sentence, it is often thecase that a specific problem in the grammar or lex-icon rules out the intended parse for a given sen-tence, but alternative (wrong) parses are still pos-sible.
What appears to be required is a (statistical)model which is capable of judging the plausibilityof a parse.
We investigated whether the maximumentropy score S(x) (equation 1) can be used to indi-cate parse plausibility.
In this set-up, we considereda parse successful only if S(x) of the best parse isabove a certain threshold.
However, the resultingparsability table did not appear to indicate problem-atic word sequences, but rather word sequences typ-ically found in elliptical sentences were returned.Apparently, the grammatical rules used for ellip-sis are heavily punished by the maximum entropymodel in order that these rules are used only if otherrules are not applicable.AcknowledgmentsThis research was supported by the PIONIERproject Algorithms for Linguistic Processing fundedby NWO.ReferencesGosse Bouma, Gertjan van Noord, and Robert Mal-ouf.
2001.
Wide coverage computational anal-ysis of Dutch.
In W. Daelemans, K. Sima?an,J.
Veenstra, and J. Zavrel, editors, ComputationalLinguistics in the Netherlands 2000.Kenneth Ward Church.
1995.
Ngrams.
ACL 1995,MIT Cambridge MA, June 16.
ACL Tutorial.Claudio Lucchiesi and Tomasz Kowaltowski.
1993.Applications of finite automata representing largevocabularies.
Software Practice and Experience,23(1):15?30, Jan.Robert Malouf and Gertjan van Noord.
2004.
Widecoverage parsing with stochastic attribute valuegrammars.
In Beyond shallow analyses.
For-malisms and statistical modeling for deep anal-ysis, Sanya City, Hainan, China.
IJCNLP-04Workshop.Udi Manber and Gene Myers.
1990.
Suf-fix arrays: A new method for on-line stringsearching.
In Proceedings of the First An-nual AC-SIAM Symposium on Discrete Algo-rithms, pages 319?327.
http://manber.com/publications.html.Robbert Prins and Gertjan van Noord.
2003.
Re-inforcing parser preferences through tagging.Traitement Automatique des Langues, 44(3):121?139.
in press.Dominique Revuz.
1991.
Dictionnaires et lexiques:me?thodes et alorithmes.
Ph.D. thesis, InstitutBlaise Pascal, Paris, France.
LITP 91.44.Emmanuel Roche.
1995.
Finite-state tools for lan-guage processing.
ACL 1995, MIT CambridgeMA, June 16.
ACL Tutorial.Leonoor van der Beek, Gosse Bouma, Robert Mal-ouf, and Gertjan van Noord.
2002a.
The Alpinodependency treebank.
In Marie?t Theune, AntonNijholt, and Hendri Hondorp, editors, Computa-tional Linguistics in the Netherlands 2001.
Se-lected Papers from the Twelfth CLIN Meeting,pages 8?22.
Rodopi.Leonoor van der Beek, Gosse Bouma, and Gertjanvan Noord.
2002b.
Een brede computationelegrammatica voor het Nederlands.
NederlandseTaalkunde, 7(4):353?374.
in Dutch.Mikio Yamamoto and Kenneth W. Church.
2001.Using suffix arrays to compute term frequencyand document frequency for all substrings in acorpus.
Computational Linguistics, 27(1):1?30.
