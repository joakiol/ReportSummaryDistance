Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 173?176,Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLPFinding Hedges by Chasing Weasels: Hedge Detection UsingWikipedia Tags and Shallow Linguistic FeaturesViola Ganter and Michael StrubeEML Research gGmbHHeidelberg, Germanyhttp://www.eml-research.de/nlpAbstractWe investigate the automatic detection ofsentences containing linguistic hedges us-ing corpus statistics and syntactic pat-terns.
We take Wikipedia as an alreadyannotated corpus using its tagged weaselwords which mark sentences and phrasesas non-factual.
We evaluate the quality ofWikipedia as training data for hedge detec-tion, as well as shallow linguistic features.1 IntroductionWhile most research in natural language process-ing is dealing with identifying, extracting and clas-sifying facts, recent years have seen a surge in re-search on sentiment and subjectivity (see Pang &Lee (2008) for an overview).
However, even opin-ions have to be backed up by facts to be effectiveas arguments.
Distinguishing facts from fiction re-quires to detect subtle variations in the use of lin-guistic devices such as linguistic hedges which in-dicate that speakers do not back up their opinionswith facts (Lakoff, 1973; Hyland, 1998).Many NLP applications could benefit fromidentifying linguistic hedges, e.g.
question an-swering systems (Riloff et al, 2003), informationextraction from biomedical documents (Medlock& Briscoe, 2007; Szarvas, 2008), and deceptiondetection (Bachenko et al, 2008).While NLP research on classifying linguistichedges has been restricted to analysing biomedi-cal documents, the above (incomplete) list of ap-plications suggests that domain- and language-independent approaches for hedge detection needto be developed.
We investigate Wikipedia as asource of training data for hedge classification.
Weadopt Wikipedia?s notion of weasel words whichwe argue to be closely related to hedges and pri-vate states.
Many Wikipedia articles contain a spe-cific weasel tag, so that Wikipedia can be viewedas a readily annotated corpus.
Based on this data,we have built a system to detect sentences thatcontain linguistic hedges.
We compare a base-line relying on word frequency measures with onecombining word frequency with shallow linguisticfeatures.2 Related WorkResearch on hedge detection in NLP has been fo-cused almost exclusively on the biomedical do-main.
Light et al (2004) present a study on an-notating hedges in biomedical documents.
Theyshow that the phenomenon can be annotated ten-tatively reliably by non-domain experts when us-ing a two-way distinction.
They also perform firstexperiments on automatic classification.Medlock & Briscoe (2007) develop a weaklysupervised system for hedge classification in avery narrow subdomain in the life sciences.
Theystart with a small set of seed examples knownto indicate hedging.
Then they iterate and ac-quire more training seeds without much manualintervention (step 2 in their seed generation pro-cedure indicates that there is some manual inter-vention).
Their best system results in a 0.76 pre-cision/recall break-even-point (BEP).
While Med-lock & Briscoe use words as features, Szarvas(2008) extends their work to n-grams.
He also ap-plies his method to (slightly) out of domain dataand observes a considerable drop in performance.3 Weasel WordsWikipedia editors are advised to avoid weaselwords, because they ?offer an opinion without re-ally backing it up, and .
.
.
are really used to ex-press a non-neutral point of view.
?1 Examplesfor weasel words as given by the style guide-1http://en.wikipedia.org/wiki/Wikipedia:Guide_to_writing_better_articles173lines2 are: ?Some people say .
.
.
?, ?I think .
.
.
?,?Clearly .
.
.
?, ?.
.
.
is widely regarded as .
.
.
?,?It has been said/suggested/noticed .
.
.
?, ?It maybe that .
.
.
?
We argue that this notion is sim-ilar to linguistic hedging, which is defined byHyland (1998) as ?.
.
.
any linguistic means usedto indicate either a) a lack of complete com-mitment to the truth value of an accompany-ing proposition, or b) a desire not to expressthat commitment categorically.?
The Wikipediastyle guidelines instruct editors to, if they noticeweasel words, insert a {{weasel-inline}} ora {{weasel-word}} tag (both of which we willhereafter refer to as weasel tag) to mark sentencesor phrases for improvement, e.g.
(1) Others argue {{weasel-inline}} thatthe news media are simply cateringto public demand.
(2) ...therefore America is viewed bysome {{weasel-inline}} technologyplanners as falling further behindEurope ...4 Data and AnnotationWeasel tags indicate that an article needs to be im-proved, i.e., they are intended to be removed afterthe objectionable sentence has been edited.
Thisimplies that weasel tags are short lived, very sparseand that ?
because weasels may not have beendiscovered yet ?
not all occurrences of linguistichedges are tagged.
Therefore we collected not onebut several Wikipedia dumps3 from the years 2006to 2008.
We extracted only those articles that con-tained the string {{weasel.
Out of these articles,we extracted 168,923 unique sentences containing437 weasel tags.We use the dump completed on July 14, 2008as development test data.
Since weasel tags arevery sparse, any measure of precision would havebeen overwhelmed by false positives.
Thus wecreated a balanced test set.
We chose one random,non-tagged sentence per tagged sentence, result-ing (after removing corrupt data) in a set of 500sentences.
We removed formatting, comments andlinks to references from all dumps.
As testing datawe use the dump completed on March 6, 2009.It comprises 70,437 sentences taken from articlescontaining the string {{weasel with 328 weasel2http://en.wikipedia.org/wiki/Wikipedia:Avoid_weasel_words3http://download.wikipedia.org/S M CK 0.45 0.71 0.6S 0.78 0.6M 0.8Table 1: Pairwise inter-annotator agreementtags.
Again, we created a balanced set of 500 sen-tences.As the number of weasel tags is very low con-sidering the number of sentences in the Wikipediadumps, we still expected there to be a much highernumber of potential weasel words which had notyet been tagged leading to false positives.
There-fore, we also annotated a small sample manu-ally.
One of the authors, two linguists and onecomputer scientist annotated 100 sentences each,50 of which were the same for all annotators toenable measuring agreement.
The annotators la-beled the data independently and following anno-tation guidelines which were mainly adopted fromthe Wikipedia style guide with only small adjust-ments to match our pre-processed data.
We thenused Cohen?s Kappa (?)
to determine the levelof agreement (Carletta, 1996).
Table 4 shows theagreement between each possible pair of annota-tors.
The overall inter-annotator agreement was?
= 0.65, which is similar to what Light et al(2004) report but worse than Medlock & Briscoe?s(2007) results.
As Gold standard we merged allfour annotations sets.
From the 50 overlapping in-stances, we removed those where less than threeannotators had agreed on one category, resultingin a set of 246 sentences for evaluation.5 Method5.1 Words Preceding Weasel TagsWe investigate the five words occurring right be-fore each weasel tag in the corpus (but within thesame sentence), assuming that weasel phrases con-tain at most five words and weasel tags are mostlyinserted behind weasel words or phrases.Each word within these 5-grams receives an in-dividual score, based a) on the relative frequencyof this word in weasel contexts and the corpus ingeneral and b) on the average distance the wordhas to a weasel tag, if found in a weasel context.We assume that a word is an indicator for a weaselif it occurs close before a weasel tag.
The finalscoring function for each word in the training set174is thus:Score(w) = RelF (w) + AvgDist(w) (1)withRelF (w) =W (w)log2(C(w))(2)andAvgDist(w) =W (w)?W (w)j=0dist(w,weaseltagj)(3)W (w) denotes the number of times word w oc-curred in the context of a weasel tag, whereasC(w) denotes the total number of times w oc-curred in the corpus.
The basic idea of the RelFscore is to give those words a high score, which oc-cur frequently in the context of a weasel tag.
How-ever, due to the sparseness of tagged instances,words that occur with a very high frequency in thecorpus automatically receive a lower score thanlow-frequent words.
We use the logarithmic func-tion to diminish this effect.In equation 3, for each weasel context j,dist(w,weaseltagj) denotes the distance of wordw to the weasel tag in j.
A word that always ap-pears directly before the weasel tag will receivean AvgDist value of 1, a word that always ap-pears five words before the weasel tag will receivean AvgDist value of 15.
The score for each wordis stored in a list, based on which we derive theclassifier (words preceding weasel (wpw)): Eachsentence S is classified byS ?
weasel if wpw(S) > ?
(4)where ?
is an arbitrary threshold used to controlthe precision/recall balance and wpw(S) is thesum of scores over all words in S, normalized bythe hyperbolic tangent:wpw(S) = tanh|S|?i=0Score(wi) (5)with |S| = the number of words in the sentence.5.2 Adding shallow linguistic featuresA great number of the weasel words in Wikipediacan be divided into three categories:1.
Numerically underspecified subjects (?Somepeople?, ?Experts?, ?Many?)2.
Passive constructions (?It is believed?, ?It isconsidered?)3.
Adverbs (?Often?, ?Probably?
)We POS-tagged the test data with the TnT tagger(Brants, 2000) and developed finite state automatato detect such constellations.
We combine thesesyntactic patterns with the word-scoring functionfrom above.
If a pattern is found, only the headof the pattern (i.e., adverbs, main verbs for passivepatterns, nouns and quantifiers for numerically un-derspecified subjects) is assigned a score.
Thescoring function adding syntactic patterns (asp)for each sentence is:asp(S) = tanhheadsS?i=0Score(wi) (6)where headsS= the number of pattern headsfound in sentence S.6 Results and DiscussionBoth, the classifier based on words precedingweasel (wpw) and the one based on added syntac-tic patterns (asp) perform comparably well on thedevelopment test data.
wpw reaches a 0.69 preci-sion/recall break-even-point (BEP) with a thresh-old of ?
= 0.99, while asp reaches a 0.70 BEP witha threshold of ?
= 0.76.Applied to the test data these thresholds yield anF-Score of 0.70 for wpw (prec.
= 0.55/rec.
= 0.98)and an F-score of 0.68 (prec.
= 0.69/rec.
= 0.68)for asp (Table 2 shows results at a few fixed thresh-olds allowing for a better comparison).
This indi-cates that the syntactic patterns do not contributeto the regeneration of weasel tags.
Word frequencyand distance to the weasel tag are sufficient.The decreasing precision of both approacheswhen trained on more tagged sentences (i.e., com-puted with a higher threshold) might be caused bythe great number of unannotated weasel words.
In-deed, an investigation of the sentences scored withthe added syntactic patterns showed that manyhigh-ranked sentences were weasels which hadnot been tagged.
A disadvantage of the weaseltag is its short life span.
The weasel tag marks aphrase that needs to be edited, thus, once a weaselword has been detected and tagged, it is likely toget removed soon.
The number of tagged sen-tences is much smaller than the actual number ofweasel words.
This leads to a great number offalse positives.175?
.60 .70 .76 .80 .90 .98balanced setwpw .68 .68 .68 .69 .69 .70asp .67 .68 .68 .68 .61 .59manual annot.wpw - .59 - - - .59asp .68 .69 .69 .69 .70 .65Table 2: F-scores at different thresholds (bold atthe precision/recall break-even-points determinedon the development data)The difference between wpw and asp becomesmore distinct when the manually annotated dataform the test set.
Here asp outperforms wpw bya large margin, though this is also due to the factthat wpw performs rather poorly.
asp reaches anF-score of 0.69 (prec.
= 0.61/rec.
= 0.78), whilewpw reaches only an F-Score of 0.59 (prec.
= 0.42/rec.
= 1).
This suggests that the added syntacticpatterns indeed manage to detect weasels that havenot yet been tagged.When humans annotate the data they not onlytake specific words into account but the wholesentence, and this is why the syntactic patternsachieve better results when tested on those data.The word frequency measure derived from theweasel tags is not sufficient to cover this more in-telligible notion of hedging.
If one is to be re-stricted to words, it would be better to fall backto the weakly supervised approaches by Medlock& Briscoe (2007) and Szarvas (2008).
These ap-proaches could go beyond the original annotationand learn further hedging indicators.
However,these approaches are, as argued by Szarvas (2008)quite domain-dependent, while our approach cov-ers the entire Wikipedia and thus as many domainsas are in Wikipedia.7 ConclusionsWe have described a hedge detection system basedon word frequency measures and syntactic pat-terns.
The main idea is to use Wikipedia as a read-ily annotated corpus by relying on its weasel tag.The experiments show that the syntactic patternswork better when using a broader notion of hedg-ing tested on manual annotations.
When evalu-ating on Wikipedia weasel tags itself, word fre-quency and distance to the tag is sufficient.Our approach takes a much broader domain intoaccount than previous work.
It can also easily beapplied to different languages as the weasel tag ex-ists in more than 20 different language versions ofWikipedia.
For a narrow domain, we suggest tostart with our approach for deriving a seed set ofhedging indicators and then to use a weakly super-vised approach.Though our classifiers were trained on datafrom multiple Wikipedia dumps, there were onlya few hundred training instances available.
Thetransient nature of the weasel tag suggests touse the Wikipedia edit history for future work,since the edits faithfully record all occurrences ofweasel tags.Acknowledgments.
This work has been par-tially funded by the European Union under theproject Judicial Management by Digital LibrariesSemantics (JUMAS FP7-214306) and by theKlaus Tschira Foundation, Heidelberg, Germany.ReferencesBachenko, Joan, Eileen Fitzpatrick & Michael Schonwet-ter (2008).
Verification and implementation of language-based deception indicators in civil and criminal narratives.In Proceedings of the 22nd International Conference onComputational Linguistics, Manchester, U.K., 18?22 Au-gust 2008, pp.
41?48.Brants, Thorsten (2000).
TnT ?
A statistical Part-of-Speechtagger.
In Proceedings of the 6th Conference on AppliedNatural Language Processing, Seattle, Wash., 29 April ?4 May 2000, pp.
224?231.Carletta, Jean (1996).
Assessing agreement on classifica-tion tasks: The kappa statistic.
Computational Linguistics,22(2):249?254.Hyland, Ken (1998).
Hedging in scientific research articles.Amsterdam, The Netherlands: John Benjamins.Lakoff, George (1973).
Hedges: A study in meaning criteriaand the logic of fuzzy concepts.
Journal of PhilosophicalLogic, 2:458?508.Light, Marc, Xin Ying Qiu & Padmini Srinivasan (2004).
Thelanguage of Bioscience: Facts, speculations, and state-ments in between.
In Proceedings of the HLT-NAACL2004 Workshop: Biolink 2004, Linking Biological Liter-ature, Ontologies and Databases, Boston, Mass., 6 May2004, pp.
17?24.Medlock, Ben & Ted Briscoe (2007).
Weakly supervisedlearning for hedge classification in scientific literature.
InProceedings of the 45th Annual Meeting of the Associationfor Computational Linguistics, Prague, Czech Republic,23?30 June 2007, pp.
992?999.Pang, Bo & Lillian Lee (2008).
Opinion mining and sen-timent analysis.
Foundations and Trends in InformationRetrieval, 2(1-2):1?135.Riloff, Ellen, Janyce Wiebe & Theresa Wilson (2003).
Learn-ing subjective nouns using extraction pattern bootstrap-ping.
In Proceedings of the 7th Conference on Compu-tational Natural Language Learning, Edmonton, Alberta,Canada, 31 May ?
1 June 2003, pp.
25?32.Szarvas, Gyo?rgy (2008).
Hedge classification in biomedicaltexts with a weakly supervised selection of keywords.
InProceedings of the 46th Annual Meeting of the Associationfor Computational Linguistics: Human Language Tech-nologies, Columbus, Ohio, 15?20 June 2008, pp.
281?289.176
