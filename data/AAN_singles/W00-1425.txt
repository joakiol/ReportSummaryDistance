Capturing the Interaction between Aggregation and TextPlanning in Two Generation SystemsHua Cheng and Chris Mel l i shDivision of Informatics, University of Edinburgh80 South Bridge, Ed inburgh EH1 1HN, UKhuac, chrism@dai, ed.
ac.
ukAbst ractIn natural language generation, different gener-ation tasks often interact with each other in acomplex way.
We think that how to resolve thecomplex interactions inside and between tasksis more important to the generation of a co-herent text than how to model each individualfactor.
This paper focuses on the interaction be-tween aggregation and text planning, and triesto explore what preferences exist among the fea-tures considered by the two tasks.
The prefer-ences are implemented in two generation sys-tems, namely ILEX-TS and a text planner us-ing a Genetic Algorithm.
The evaluation em-phasises the second implementation and showsthat capturing these preferences properly canlead to coherent ext.1 D iscourse  coherence  andaggregat ionhi NLG, theories based on domain-independentrhetorical relations, in particular, RhetoricalStructure Theory (Mann and Thompson, 1987),are often used in text planning, whose taskis to select the relevant information to be ex-pressed and organise it into a hierarchical struc-ture which captures certain discourse prefer-ences such as preferences for the use of rhetori-cal relations.In the theory of discourse structure developedby Grosz and Sidner (1986), each discourse seg-ment exhibits two types of coherence: local co-herence among utterances inside the segment,and global coherence between this segment andother discourse segments.
Discourse segmentsare connected by either a dominaTzce relation ora satisfaction-precedence relation.There has been an effort to synthesise tiletwo accounts of discourse structure.
X loser andMoore (1996) argue that the two theories haveconsiderable common ground, which lies in thecorrespondence between the notion of domi-nance and nuclearity.
It is possible to mapbetween Grosz and Sidner's linguistic structureand RST text structure, and relation-based co-herence and global coherence capture similardiscourse properties.Oberlander et al (1999) propose a dis-tinction between two types of discourse coher-ence: proposition-based coherence, which ex-ists between text spans connected by RST re-lations except for object-attribute laboration,and entity-based coherence, which exists be-tween spans of text in virtue of shared entities.entity-based coherence captures the coherenceamong adjacent propositions, which resembleslocal coherence in Grosz and Sidner's theory.To generate a coherent ext, the text planningprocess must try to achieve both local (entity-based) and global (relation-based) coherence.Since the task of aggregation is to combine sinl-ple representations together to form a complexone, which in the mean time leads to a shortertext as a whole, aggregation could affect the or-dering of text plans and the length of the wholetext..
Therefore, it is closely related to tile taskof maintaining both types of coherence.
Herewe treat embedding as a type of aggregation.There is no consensus as to where aggregationshould happen or how it is related to other gen-eration processes (Wilkinson, 1995; Reape andMellish, 1999).
In many NLG systems, aggre-gation is a post planning process whose prefer-ences are only partially taken into account bythe text planner.1.1 Aggregat ion  and local coherenceIn a structured text plan produced by the textplanner, local coherence is normally maintainedthrough the ordering of the selected facts, where186certain types of center transition (e.g.
cen-ter continuation) :are preferred :over:others (eig,.
-;center shifting) (Centering Theory (Grosz et al,1995)).
Aggregation may affect text planningby taking away facts from a sequence featuringpreferred center movements for embedding orsubordination.
As a result, the preferred cen-ter transitions in the original sequences couldbe cut off.
For example, comparing the firsttwo descriptions of.a necklace in Figure 1, 2 isless coherent than 1 because of the shifting fromthe description of the necklace to that of the de-signer, which is a side effect of embedding.Since the centers of sentences are normallyNPs and embedding adds non-restrictive com-ponents into an NP, it could affect the way a Cbis realised (e.g.
preventing it from being a pro-noun).
As pointed out in (Grosz et al, 1995),different realisations (e.g.
pronoun vs. definitedescription) are not equivalent with respect otheir effect on coherence.
Therefore, embeddingcould influence local coherence by forcing a dif-ferent realisation from that preferred by Center-ing Theory.
There is an obvious need to balancethe consideration for local coherence and stylis-tic preferences.1.2 Aggregat ion  and  global coherenceDifferent types of aggregation eed to be com-patible among themselves, in particular, embed-ding and semantic parataxis and hypotaxis.
Us-ing the abstraction of RST, semantic parataxisconcerns facts related by explicit multi-nuclearsemantic relations (e.g.
sequence and contrast)or by implicit connections like parallel commonparts.
If two facts have at least two identi-cal parallel components, we say that a conjunctor disjunct relation exists between them, andthese relations are multi-nuclear relations.
Se-mantic hypotaxis concerns facts connected bynucleus-satellite r lations (e.g.
cause).
Seman-tic parataxis and hypotaxis feature in relation-based coherence and they depend on the textplanner to put the related facts next to eachother in order to perform a combination.
(Cheng, 1998) describes interactions thatneed to be taken into account in aggrega-tion.
Firstly, complex embedded componentslike non-restrictive clauses may interrupt tilesemantic onnection or syntactic similarity be-tween a set of clauses.
Secondly, the possibilitiesof other types of aggregation should be consid-ered for both the main fact and the fact to be-embedded .
uring .
:embedding .decision.
maki ng...And thirdly, performing parataxis inside a hy-potaxis could convey wrong information.We argue that the effect of aggregation is notlimited to the particular NP or sentence whereaggregation happens, but to the coherence ofthe text as a whole.
The complex interactionsdemand the features of aggregation to be eval-uated .together with other coherence~ featuresand aggregation to be planned as a part of textstructuring.
This requires better coordinationbetween aggregation and other generation tasksas well as among different ypes of aggregationthan is present in current NLG systems.In this paper, we describe how to capture theabove interactions as preferences among relatedfeatures, and the implementation f the prefer-ences in two very different generation architec-tures to produce descriptions of museum objectson display.2 P re ferences  among coherencefeaturesWe claim that it is the relative preferencesamong features rather than the absolute magni-tude of each individual one that play the crucialrole in the production of a coherent text.
In thissection we discuss the preferences among fea-tures related to text planning, based on whichthose for embedding can be introduced.2.1 P re ferences  for global coherenceA semantic relation other than conjunct or dis-junet is preferred to be used whenever possiblebecause it usually conveys interesting informa-tion about domain objects and leads to a coher-ent text span.
If a conjunct relation shares a factwith a semantic relation, the conjunct shouldbe suppressed.
For example, in 3 of Figure 1.apart from other relations, there is an amplifica-tion relation signalled by indeed and a conjunctbetween the last two propositions.
Comparedwith 3, 4 is less preferred because it misses tileamplification and the center transition from thenecklace to an Arts and Crafts style jewel is notso smooth, whereas 3 expresses the amplifica-tion explicitly and the conjunct implicitly.However, a semantic relation can only be usedif the knowledge assumed to be shared by thehearer is introduced in the previous discourse(Mellish et al.
1998a).
\Ve assume the strategy1871.
This necklace is in the Arts and Crafts style.
Arts and Crafts style jewels usually have an elaboratedesign.
They tend to have floral motifs.
For instance, this necklace has floral motifs.
It was designedby Jessie King.
King was Scottish.
She once lived in London.2.
This necklace, which was designed by Jessie King, is in the Arts and Crafts style.
Arts andCrafts style jewels usually have an elaborate design.
They tend to have floral motifs.
For instance,this necklace has floral motifs.
King was Scottish.
She once lived in London.3.
The necklace is in the Arts and Crafts style.
It is set with jewels in that it features cabuchonstones.
Indeed, an Arts and Crafts style jewel usually uses cabuchon stones.
It usually uses ovalstones.4.
The necklace is in the Arts and Crafts style.
It is set.
with jewels in that it features cabuchonstones.
An Arts and Crafts style jewel usually uses cabuchon stones and oval stones.Figure 1: Aggregation examplesof (Mellish et al, 1998a) which uses a joint re-lation to connect every two text spans that donot have a semantic relation other than object-attribute elaboration and conjunct/disjunct inbetween.
Although joint is not preferred whenother relations are present, it is better thanmissing presuppositions or embedding a con-junct relation inside a semantic relation.
There-fore, we have the following heuristics, where"A>B" means that A is preferred over B.Heur i s t i c  1 Preferences among features forglobal coherence:a semantic relation > Conjunct/Disjunct >Joint > presuppositions not metJoint > Conjunct inside a semantic relation2.2 Pre ferences  for local coherenceOne way to achieve local coherence is to con-trol center transitions among utterances.
InCentering Theory, Rule 2 specifies preferencesamong center movement in a locally coherentdiscourse segment: sequences of continuationare preferred over sequences of retaining; whichare then preferred over sequences of shifting.Brennan et el.
(1987) also describe typicaldiscourse topic movements in terms of centertransitions between pairs of utterances.
Theyargue that the order of coherence among thetransitions is continuing > retaining > smoothshifting > abrupt shifting.
Instead of claimingthat these are the best models, we use themsimply as an example of linguistic models beingused for evaluating features of text planning.A type of center transition that appears fre-quently in descriptive text is that the descrit)-tion starts with an object, but shifts to associ-ated objects or perspectives of that object.
Thisis a type of abrupt shifting, but it is appropriateas long as the objects are highly associated tothe original object (Schank, 1977).
This phe-nomenon is handled in the system of (Grosz,1977), where subparts of an object are includedinto a focus space as the implicit foci when theobject itself is to be included.We call this center movement an associateshifting, where the center moves from a trig-ger entity to a closely associated entity.
Ourinformal observation from museum descriptionsshows that associate shifting is preferred by hu-man writers to all other types of center move-ments except for continuation.
There are twotypes of associate shifting: where the triggeris in the previous utterance or two entities intwo adjacent utterances have the same trigger.There is no preference between them.Heuristic 2 summarises the above preferences.We admit that these are strict heuristics andthat human texts are sometimes more flexible.Heur i s t i c  2 Preferences among center transi-tions:Continuation > Associate shifting > RetaiTI-ing > Smooth shifting > Abrupt shifting2.3 Pre ferences  for both  types  ofcoherenceTwo propositions can be connected in differ-ent ways, e.g.
through a semmxtic relation or asmooth center transition only.
Since a semanticrelation is always preferred, we have the follow-ing heuristic:Heur i s t i c  3 Preferences among semantic rela-tions and center transitions:a semantic relation > Joint ?
Continuation1882.4 P re ferences  for embedd ing  Good embedding > Normal embedding >We distinguish between.a.-good,.rwrmal,and-bad Jo int  > Bad embedding .
.
.
.
.
=:--..~ .
:-- ~ .--:.
: ........embedding based on the features it bears.
We do Continuation + Smooth shifting + Joint >not claim that the set of features is complete.In a different context, more criteria might haveto be considered.A good embedding is one satisfying all the fol-lowing conditions:1.
The referring part is an indefinite, a demon-strative or a bridging description (as de-fined in (Poesio et al, 1997)).2.
The embedded part can be realised as anadjective or a prepositional phrase (Scottand de Souza, 1990).3.
In the resulting text, the embedded partdoes not lie between text spans connectedby semantic parataxis and hypotaxis.4.
There is an available syntactic slot to holdthe embedded part.A good embedding is highly preferred andshould be performed whenever possible.
A nor-mal embedding is one satisfying condition 1, 3and 4 and the embedded part is a relative clausewhich provides additional information aboutthe referent.
Bad embeddings are all those left,for example, if there is no available syntacticslot for the embedded part.Since semantic parataxis has a higher prioritythan embedding (Cheng, 1998), a good embed-ding should be less preferred than using a con-junct relation, but it should be preferred over acenter continuation for it to happen.To decide the interaction between an embed-ding and a center transition, we use the first twoexamples in Figure 1 again.
The only differencebetween I and 2 is the position of the sentence"This necklace was de.signed by Jessie King",which can be represented in terms of features oflocal coherence and embedding as follows:the last three sentences in 1: Joint + Contin-uation + Joint + Smooth shiftingthe last two sentences plus embedding in 2:Joint + Abrupt shifting + Normal embedding1 is preferred over 2 because the center inovesmore smoothly in 1.
The heuristics derived fromthe above discussions are summarised below:Heur i s t i c  4 Preferences among features forembedding and center transition:Abrupt shifting + Normal embeddingGood embedding > Continuation + JointConjunct > Good embeddingThe '+' symbol can be interpreted in differentways, depending on how the features are usedin an NLG system.
In a traditional system, itmeans the coexistence of two features.
In a sys-tem using numbers for planning, it can have thesame meaning as the arithmetic symbol.3 Captur ing  the  pre ferences  in I LEXThe architecture of text planning has a greateffect on aggregation possibilities.
In object de-scriptive text generation, there lacks a centraloverriding communicative goal which could bedecomposed in a structured way into subgoals.The main goal is to provide interesting infor-mation about the target object.
There are gen-erally only a small number of relations, mainlyobject-attribute elaboration and joint.
For such agenre, a domain-dependent bottom-up lanner(Marcu, 1997) or opportunistic planner (Mel-lish et al, 1998b) suits better than a domain-independent top-down planner.
In these archi-tectures, aggregation is important o text plan-ning because it changes the order in which infor-mation is expressed.
The first implementationwe will describe is based on ILEX (Oberlanderet al, 1998).ILEX is an adaptive hypertext generationsystem, providing natural anguage descriptionsfor museum objects.
The bottom-up text plan-ning is fulfilled in two steps: a content selectionprocedure, where a set of fact nodes with highrelevance is selected from the Content Potential(following a search algorithm), and a contentstructuring procedure, where selected facts arereorganised to form entity-chains (based on thetheory of entity-based coherence), which repre-sent a coherent ext arrangement.To make it possible for the ILEX planner totake into account aggregation, we use a revisedversion of Meteer's Text Structure (Meteer,1992; Panaget, 1997) as the intermediate l vel ofrepresentation between text planning and sen-tence rcalisation to provkte abstract syntacticconstraints to the planning.
We call this sys-tem ILEX-TS (ILEX based on Text Structure).189In ILEX-TS, abstract referring expression de-termination and.aggxegation are performed ur - .
.
:ing text structuring.
For each fact whose TextStructure is being built, if an NP in the fact cantake modifiers, the embedding process will finda list of elaboration facts to the referent andmake embedding decisions based on the con-straints imposed by the NP form.
The decisionsinclude what to embed and what syntactic formthe embedded part should use.Heuristic 1, 2 and 3 are followed naturally ~by the ILEX text planner, which calculates thebest RS tree and puts facts connected by theimaginary conjunct relation next to each other.It tries to feature center continuations as oftenas possible.
When it needs to shift topic, it usesa smooth shifting.ILEX-TS has a set of embedding rules, wherethose rules featuring good embedding are al-ways used first, then a rule featuring a normalembedding.
Bad embedding is not allowed atall.
To coordinate different types of aggrega-tion, the algorithm checks parataxis and hy-potaxis possibilities for each nucleus fact andthe fact to be embedded before it applies anembedding rule.
These realise most of Heuris-tic 4 (except for the second set).
However, be-cause the various factors are optimised in order(with no backtracking), there is no guaranteethat the best overall text will be found.
In addi-tion, complex interactions between aggregationand center transition cannot be easily captured.4 Text  p lann ing  us ing  a GAAlthough most heuristics can be followed inILEX-TS, some interactions are missing, for ex-ample, 9 of Figure 1 will probably be generated.For better coordination, we adopt the text plan-ner based on a Genetic Algorithm (GA) as de-scribed in (Mellish et al, 1998a).
The task is.given a set of facts and a set of relations betweenfacts, to produce a legal rhetoricalstrncture treeusing all the facts and some relations.A fact is represented in terms of a subject,a verb and a complement (as well as a uniqueidentifier).
A relation is represented in terms ofthe relation name, the two facts that are con-nected t) 3" the relation and a list of preconditionfacts which need to have been mentioned beforethe relation can be used i.1As this is an experimental system, the ability of theA genetic algorithm is suitable for such aproblem.because,:the..numher-.of.-possihle-com-binations is huge and the search space is notperfectly smooth and unimodal (there can bemany good combinations).
Also the generationtask does not require a global optimum to befound.
What we need is a combination that iscoherent enough for people to understand.
(Mellish et al, 1998a) summarises the geneticalgorithm roughly as follows:1.
Enumerate a set of random initial se-quences by loosely following sequences offacts where consecutive facts mention thesame entity.2.
Evaluate sequences by evaluating therhetorical structure trees they give rise to.3.
Perform mutation and crossover on the se-quences.4.
Stop after a given number of iterations, andreturn the tree for the "best" sequence.The advantage of this approach is that it pro-vides a mechanism to integrate planning factorsin the evaluation function and search for thebest combinations of them.
So it is an excellentframework for experimenting with the interac-tion between aggregation and text planning.In the algorithm, the RS trees are right-branching and are almost deterministically builtfrom sequences of facts.
Given two sequences,crossover inserts a random segment from onesequence in a random position in the other toproduce two new sequences.
Mutation selectsa random segment of a sequence and moves itinto a random position in the same sequence.To explore the whole space of aggregation.we decide not to perform aggregation on struc-tured facts or on adjacent facts in a linear se-quence because they might restrict the possibil-ities and even miss out good candidates.
In-stead, we define a third operator called embed-ding mutation.
Suppose we have a sequence\[U1,U2,...,Ui,...,U.\], where we call each elementof the sequence a unit, which can be either a factor a list of facts or units with no depth limit.For a list, we call its very first fact the main fact,system is limited in all aspects.
It does not have a realrealisation component,  so the parts we are less interestedin are realised by canned phrases for readability.190Features /FactorsSemant ic  re la t ionsa jo inta conjunct or dis juncta relat ion other  than  jo int ,  con junct  or  d is juncta con junct  ,inside o ther  semant ic  re la t ionsa precondi t ion  not  sat isf iedFocus  movesa cont inu ingan assoc iate  sh i f t inga smooth shiftingresuming  a previous focusEmbedd inga good embeddinga normal embedd inga bad embeddingOtherstopic not mentioned in the first sentenceVa lues  ( ra ters )1 \] 2 ..-20 -4610 1121 69-50 -63-30 -6120 716 114 -36 -436 33 0-30 -64-10 -12Table 1: Two different raters satisfying the same constraints/ x I % / \i i  ~ - - ~ t  !
x x i I xI xI I ~ x i I/ I  x l_ m, -h-- .
.
.
.
.
X .
.
.
.
.
.
g .
.
.
.
-i~ .
.
.
.
.
.
.
T .
.
.
.
.
.
.
i; .
.
.
.
.
.
.
~ .......... ,oFigure 2: Scores for four museum descriptive textsinto which the remaining facts in the list are tobe embedded.
The embedding mutat ion ran-domly selects a unit Ui from the sequence andan entity in its main fact.
It then collects allthe units mentioning this entity and randomlychooses one Uk.
The list containing these twounits \[Ui,Uk\] represents a random embeddingand will be treated as a single unit in later op-erations.
It takes the.
position of Ui to producea new sequence \[U~,U2,...,\[Ui,Uk\],...,U,\] and allrepetit ions outside \[Ui,U~:\] are removed.
Thissequence is then evaluated and ordered in thepopulat ion.The probabil it ies of apl)lying the three opera-tots are: 65% for crossow'r.
30% for embeddingmutat ion and 5% for normal umtation.
This  isbecause the first two are more likely to producesequences bearing desired properties by e i thercombining the good bits of two sequences orperforming a reasonable amount of embedding,whereas normal mutat ion is entirely random 2.5 Jus t i fy ing  the  GA eva luat ionfunct ionThe  linguistic theories discussed in Section 2only give evidence in qualitative terms.
For aGA-based planner to work, we have to come upwith actual numbers that can be used to evalu-2The values for crossover and mutation rate used mour algorithm are fairly standard.191The smal l  por tab le  throne f rom the t ime o f  the  Qianlong Emperor  1736-95 is mad e .
.
.
.of ~acquer~d`wo~d~.wit~T.de~rati~n-in~-g~d`~and.red~It-was-use~-in-the-private.apartrn~r~`~ ..... ".........of the Imperial Palaces.
The cover f rom the  re ign o f  J iaquing,  1796-1820 is woven in yellowsilk, wh ich  is the  imper ia l  colour o f  the Q ing  Dynasty ,1644-1911.
It would have coveredthe throne when not in use.The design on the seat is a imper ia l  f ive c lawed dragon in a circular medal l ion.
On theinside of the arm pieces are small shelves.
Precious possessions can be placed in small shelvesand can be studied as an aid to contemplation.Figure 3: A generated text scored the h ighest ,  w i th  the  embedded parts  highlightedScore2 Score3 Score4 Score5 Score6Score1 .9567 .9337 .9631 .9419 .9515Score2 .9435 .8819 .9280 .9185Score3 .8650 .8462 .9574Score4 .9503 .8940Score5 .8486Table 2: Correlations between six ratersate an RS tree.
Mellish et al (1998a) presentsome scores for evaluating the basic features of atree, but they make it clear that the scores arethere for descriptive purpose, not for makingany serious claim about the best way of evalu-ating RS trees.The methodology we adopted was that wetook the existing evaluation function and ex-tended it to take into account features for localcoherence, embedding and semantic paratmxis.This resulted in rater 1 in Table 1, which sat-isfied all the heuristics mentioned in Section 2.We manually broke down four human writtenmuseum descriptions into individual facts andrelations and reconstructed sequences of factswith the same orderings and aggregations a  inthe original texts.
We then used our evaluationflmction to score the RS trees built from thesesequences.
In the mean time.
we ran the GAalgorithm for 5000 iterations on the facts andrelations for 10 times.
The results are shown inFigure 2, where the four line styles correspondto the four texts.
The jagged lines represent-thescores of the machine generated texts and thestraight lines represent the scores for the corre-sponding human texts.All human texts were scored among the high-est and machine generated texts can get scoresvery close to human ones sometimes.
Since thehuman texts were written and revised bv mu-seum experts, they can be treated as "'nearlybest  texts".
The figure shows that the evalu-ation function based on our heuristics can findgood and correct combinations.
The reason fora relatively bad text being generated sometimesmight be that really bad sequences were pro-duced at the beginning.
This could be improvedby using certain heuristics to get better initialsequences.
Also when the number of facts be-comes larger, more iterations are needed to getreadable texts.
Figure 3 gives a text generatedusing rater 1.To justify our claim that it is the preferencesamong generation factors that decide the coher-ence of a text, we fed the preferences into a con-straint based program.
If a feature can take arange of values, the program randomly selectsa number in that range.
A number of raterscompatible with the constraints were generatedand one of them is given in Table 1 as rater 2.We then generated all possible combinations, in-cluding embedding, of seven facts from a humantext and used six randomly produced raters toscore each of them.The .qualities .of the generated texts are nor-real distributed according to all raters.
Theraters distinguish between good and bad textsand they classify the majority of texts as ofmoderate quality and only very small percent-ages as very good or very bad texts.
The be-haviours of the raters are very similar as thehistograms are of roughly the same shape.192To see to what extent he six raters agree witheach other, we calculated the  Pearson correla-tion coefficient between them, which is shownin Table 2.
We can claim from the table thatfor this data, the scores from the six raters cor-relate, and we have a fairly good chance to be-lieve that the six raters, randomly produced ina sense, agree with each other on evaluating thetext and they measure basically the same thing.Daniel Marcu.
1997.
From local to global coherence:.
A_ bottom=up.approach' to.
text:planning.. 'In Pro-ceedings of the Fourteenth National Conference onArtificial Intelligence, pages 629-635, Providence,Rhode Island.Chris Mellish, Alistair Knott, Jon Oberlander,and Mick O'Donnell.
1998a.
Experiments usingstochastic search for text planning.
In Proceed-ings of the 9th International Workshop on NaturalLanguage Generation, Ontario, Canada.6 Conc lus ions  and  fu ture  work.
.
.
.
.
Chris:MeUish,: Mick O'_Donnell,:.Jon Oberlander, andAlistair Knott.
1998b.
An architecture for op-This paper describes an experiment with thepreferences among features concerning aggrega-tion and text planning, in particular, we presentan mechanism for how relevant features can bescored to contribute together to the planning ofa coherent text.
The statistical results partiallyjustify our claim that it is the preferences amonggeneration features that decide the coherence ofa text.Our experiment could be extended in manyways, for example, validating the evaluationfunction through empirical analysis of humanassessments of the generated texts, and us-ing more texts to test the correlation betweenraters.
The architecture based on the GeneticAlgorithm can also be used for testing interac-tions between or within other text generationmodules.Re ferencesSusan Brennan, Marilyn Walker Friedman, and CarlPollard.
1987.
A centering apporach to pronouns.In Proceedings of the 25th Annual Meeting of theAssociation for Computational Linguistics, pages155-162, Stanford, CA.Hua Cheng.
1998.
Embedding new information i toreferring expressions.
In Proceedings ofCOLING-A CL '98, pages 1478-1480, Montreal, Canada.Barbara Grosz and Candace Sidner.
1986.
Atten-tions, intentions and the structure of discourse.Computational Linguistics, 12:175-204.Barbara Grosz, Aravind Joshi, and Scott Weinstein.1995.
Centering: A framework for modelling thelocal coherence of discourse.
Computational Lin-guistics, 21(2):203-226.Barbara Grosz.
1977.
T-he representation a d use offocus in dialogue understanding.
Technical report151, SRI International.William Mann and Sandra Thompson.
198.7.Rhetorical structure theory: A theory of text or-ganization.
Technical Report ISI/RR-87-190, In-formation Sciences Institute, University of South-ern California.portunistic text generation.
In Proceedings of the9th International Workshop on Natural LanguageGeneration, Ontario, Canada.Marie Meteer.
1992.
Expressibility and The Prob-lem of Efficient Text Planning.
Communicationin Artificial Intelligence.
Pinter Publishers Lim-ited, London.Megan Moser and Johanna Moore.
1996.
Toward asynthesis of two accounts of discourse structure.Computational Linguistics, 22(3):409-419.Jon Oberlander, Mick O'Donnell, Ali Knott, andChris Mellish.
1998.
Conversation i  the mu-seum: Experiments in dynamic hypermedia withthe intelligent labelling explorer.
New Review ofHypermedia nd Multimedia, 4:11-32.Jon Oberlander, Alistair Knott, Mick O'Donnell,and Chris Mellish.
1999.
Beyond elaboration:Generating descriptive texts containing it-clefts.In T Sanders, J Schilperoord, and W Spooren,editors, Text Representation: Linguistic and Psy-cholinguistic Aspects.
Benjamins, Amsterdam.Franck Panaget.
1997.
Micro-planning: A uni-fied representation f lexical and grammatical re-sources.
In Proceeding of the 6th European Work-shop on Natural Language Generation, pages 97-106.Massimo Poesio, Renata Vieira, and Simone Teufel.1997.
Resolving bridging references in unre-stricted text.
Research paper hcrc-rp87, Centrefor Cognitive Science, University of Edinburgh.Michael Reape and Chris Mellish.
1999.
Just whatis aggregation anyway?
In Proceedings of the 7thEuropean Workshop on Natural Language Gener-ation, pages 20-29, Toulouse, France.Roger Schank.
1977.
Rules and topics in conversa-tion.
Cognitive Science, 1(1):421-441.Donia Scott and Clarisse Sieckenius de Souza.
1990.Getting the-message across in rst-based text gen-eration.
In R. Dale, C. Mellish, and M. Zock, edi-tors, Current Research in Natural Language Gen-eration, pages 47-73.
Academic Press.John Wilkinson.
1995.
Aggregation i Natural Lan-guage Generation: Another Look.
Technical re-port, Computer Science Department, Universityof \Vnterloo.193
