Solving Logic Puzzles: From Robust Processing to PreciseSemanticsIddo Lev,?
Bill MacCartney,?
Christopher D.
Manning,??
and Roger Levy??
Department of Computer Science ?
Department of LinguisticsStanford University Stanford UniversityStanford, CA 94305-9040, USA Stanford, CA 94305-2150, USA{iddolev|wcmac|manning}@cs.stanford.edu rog@stanford.eduAbstractThis paper presents intial work on a system thatbridges from robust, broad-coverage natural lan-guage processing to precise semantics and auto-mated reasoning, focusing on solving logic puzzlesdrawn from sources such as the Law School Admis-sion Test (LSAT) and the analytic section of theGraduate Record Exam (GRE).
We highlight keychallenges, and discuss the representations and per-formance of the prototype system.1 IntroductionTraditional approaches to natural language un-derstanding (Woods, 1973; Warren and Pereira,1982; Alshawi, 1992) provided a good accountof mapping from surface forms to semantic rep-resentations, when confined to a very limitedvocabulary, syntax, and world model, and re-sulting low levels of syntactic/semantic ambi-guity.
It is, however, difficult to scale thesemethods to unrestricted, general-domain natu-ral language input because of the overwhelmingproblems of grammar coverage, unknown words,unresolvable ambiguities, and incomplete do-main knowledge.
Recent work in NLP hasconsequently focused on more robust, broad-coverage techniques, but with the effect ofoverall shallower levels of processing.
Thus,state-of-the-art work on probabilistic parsing(e.g., (Collins, 1999)) provides a good solutionto robust, broad coverage parsing with auto-matic and frequently successful ambiguity reso-lution, but has largely ignored issues of semanticinterpretation.
The field of Question Answering(Pasca and Harabagiu, 2001; Moldovan et al,2003) focuses on simple-fact queries.
And so-called semantic parsing (Gildea and Jurafsky,2002) provides as end output only a flat clas-sification of semantic arguments of predicates,ignoring much of the semantic content, such asquantifiers.A major research question that remains unan-swered is whether there are methods for get-ting from a robust ?parse-anything?
statisti-cal parser to a semantic representation preciseenough for knowledge representation and auto-mated reasoning, without falling afoul of thesame problems that stymied the broad appli-cation of traditional approaches.
This paperpresents initial work on a system that addressesthis question.
The chosen task is solving logicpuzzles of the sort found in the Law School Ad-mission Test (LSAT) and the old analytic sec-tion of the Graduate Record Exam (GRE) (seeFigure 1 for a typical example).
The system in-tegrates statistical parsing, ?on-the-fly?
combi-natorial synthesis of semantic forms, scope- andreference-resolution, and precise semantic repre-sentations that support the inference requiredfor solving the puzzles.
Our work comple-ments research in semantic parsing and TREC-style Question Answering by emphasizing com-plex yet robust inference over general-domainNL texts given relatively minimal lexical andknowledge-base resources.1.1 Why Logic Puzzles?Logic puzzles have a number of attractive char-acteristics as a target domain for research plac-ing a premium on precise inference.First, whereas for humans the language un-derstanding part of logic puzzles is trivial butthe reasoning is difficult, for computers it isclearly the reverse.
It is straightforward for acomputer to solve a formalized puzzle, so theresearch effort is on the NLP parts rather thana difficult back-end AI problem.
Moreover, onlya small core of world knowledge (prominently,temporal and spatial entailments) is typicallycrucial to solving the task.Second, the texts employ everyday language:there are no domain-restrictions on syntacticand semantic constructions, and the situationsdescribed by the texts are diverse.Third, and most crucial, answers to puzzlequestions never explicitly appear in the text andPreamble: Six sculptures ?
C, D, E, F, G, and H?
are to be exhibited in rooms 1, 2, and 3 of an artgallery.
The exhibition conforms to the followingconditions:(1) Sculptures C and E may not be exhibited inthe same room.
(2) Sculptures D and G must be exhibited in thesame room.
(3) If sculptures E and F are exhibited in the sameroom, no other sculpture may be exhibited in thatroom.
(4) At least one sculpture must be exhibited in eachroom, and no more than three sculptures may beexhibited in any room.Question 1: If sculpture D is exhibited in room3 and sculptures E and F are exhibited in room 1,which of the following may be true?
(A) Sculpture C is exhibited in room 1.
(B) No more than 2 sculptures are exhibited inroom 3.
(C) Sculptures F and H are exhibited in the sameroom.
(D) Three sculptures are exhibited in room 2.
(E) Sculpture G is exhibited in room 2.Question 2: If sculptures C and G are exhibitedin room 1, which of the following may NOT be acomplete list of the sculpture(s) exhibited in room2?
(A) Sculpture D (B) Sculptures E and H (C).
.
.Adapted from (Weber, 1999).Figure 1: Example of a Puzzle Textmust be logically inferred from it, so there isvery little opportunity to use existing superficialanalysis methods of information-extraction andquestion-answering as a substitute for deep un-derstanding.
A prerequisite for successful infer-ence is precise understanding of semantic phe-nomena like modals and quantifiers, in contrastwith much current NLP work that just ignoressuch items.
We believe that representationswith a well-defined model-theoretic semanticsare required.Finally, the task has a clear evaluation metricbecause the puzzle texts are designed to yieldexactly one correct answer to each multiple-choice question.
Moreover, the domain is an-other example of ?found test material?
in thesense of (Hirschman et al, 1999): puzzle textswere developed with a goal independent of theevaluation of natural language processing sys-tems, and so provide a more realistic evaluationframework than specially-designed tests such asTREC QA.While our current system is not a real worldapplication, we believe that the methods beingdeveloped could be used in applications such asa computerized office assistant that must under-stand requests such as: ?Put each file contain-ing a task description in a different directory.
?2 System OverviewThis section explains the languages we use torepresent the content of a puzzle.
Computingthe representations from a text is a complex pro-cess with several stages, as shown in Figure 2.Most of the stages are independent of the puz-zles domain.
Section 3 reviews the main chal-lenges in this process, and later sections outlinethe various processing stages.
More details ofsome of these stages can be found at (StanfordNLP Group, 2004).2.1 First-Order Logic (FOL)An obvious way of solving logic puzzles is touse off-the-shelf FOL reasoners, such as theo-rem provers and model builders.
Although mostGRE logic puzzles can also be cast as constraint-satisfaction problems (CSPs), FOL representa-tions are more general and more broadly ap-plicable to other domains, and they are closerto the natural language semantics.
GRE logicpuzzles have finite small domains, so it is prac-ticable to use FOL reasoners.The ultimate representation of the content ofa puzzle is therefore written in FOL.
For ex-ample, the representation for the first part ofconstraint (4) in Figure 1 is: ?x.room(x) ??y.sculpture(y)?
exhibit(y, x).
(The treatmentof the modal ?must?
is explained in ?9.2).2.2 Semantic Logic (SL)Representing the meaning of natural languagetexts in FOL is not straightforward becausehuman languages employ events, plural enti-ties, modal operations, and complex numericexpressions.
We therefore use an intermedi-ate representation, written in Semantic Logic(SL), which is intended to be a general-purposesemantic representation language.
SL extendsFOL with event and group variables, the modaloperators ?
(necessarily) and ?
(possibly), andGeneralized Quantifiers (Barwise and Cooper,statisticalparsercombinatorialsemanticsscoperesolutionreferenceresolutionpluralitydisambig.lex.sem.info gapsfillertext parse trees URs DL formulas SL formulasanswer reasoningmoduletoFOLFOL formulasspecific tologic puzzlesgeneralFigure 2: System Overview1981) Q(type, var, restrictor, body), where typecan be ?, ?, at-least(n), etc.
To continue the ex-ample, the intermediate representation for theconstraint is:?Q(?, x1, room(x1), Q(?1, x2, sculpture(x2),?e.exhibit(e) ?
subj(e, x2) ?
in(e, x1)))2.3 Non-determinismAlthough logic puzzles are carefully designedto reduce ambiguities to ensure that thereis exactly one correct answer per question,there are still many ambiguities in the analy-sis, such as multiple possibilities for syntacticstructures, pronominal reference, and quantifierscope.
Each module ranks possible output rep-resentations; in the event that a later stage re-veals an earlier choice to be wrong (it may beinconsistent with the rest of the puzzle, or leadto a non-unique correct answer to a question),the system backtracks and chooses the next-bestoutput representation for the earlier stage.3 Challenges3.1 Combinatorial SemanticsThe challenge of combinatorial semantics is tobe able to assign exactly one semantic repre-sentation to each word and sub-phrase regard-less of its surrounding context, and to combinethese representations in a systematic way untilthe representation for the entire sentence is ob-tained.
There are many linguistic constructionsin the puzzles whose compositional analysis isdifficult, such as a large variety of noun-phrasestructures (e.g., ?Every sculpture must be ex-hibited in a different room?)
and ellipses (e.g.,?Brian saw a taller man than Carl [did]?
).3.2 Scope AmbiguitiesA sentence has a scope ambiguity when quan-tifiers and other operators in the sentence canhave more than one relative scope.
E.g., in con-straint (4) of Figure 1, ?each room?
outscopes?at least one sculpture?, but in other contexts,the reverse scoping is possible.
The challengeis to find, out of all the possible scopings, theappropriate one, to understand the text as thewriter intended.3.3 Reference ResolutionThe puzzle texts contain a wide variety ofanaphoric expressions, including pronouns, defi-nite descriptions, and anaphoric adjectives.
Thechallenge is to identify the possible antecedentsthat these expressions refer to, and to selectthe correct ones.
The problem is complicatedby the fact that anaphoric expressions interactwith quantifiers and may not refer to any par-ticular context element.
E.g., the anaphoric ex-pressions in ?Sculptures C and E are exhibitedin the same room?
and in ?Each man saw a dif-ferent woman?
interact with sets ({C,E} andthe set of all men, respectively).3.4 Plurality DisambiguationSentences that include plural entities are po-tentially ambiguous between different readings:distributive, collective, cumulative, and combi-nations of these.
For example, sentence 1 inFigure 1 says (among other things) that eachof the six sculptures is displayed in one of thethree rooms ?
the group of sculptures and thegroup of rooms behave differently here.
Plu-rality is a thorny topic which interacts in com-plex ways with other semantic issues, includingquantification and reference.3.5 Lexical SemanticsThe meaning of open-category words is oftenirrelevant to solving a puzzle.
For example,the meaning of ?exhibited?, ?sculpture?, and?room?
can be ignored because it is enough tounderstand that the first is a binary relationthat holds between elements of groups describedby the second and third words.1 This observa-1The meanings are still important for the implicitknowledge that a sculpture cannot be exhibited in morethan one room.
However, such knowledge can beguessed, as explained in ?8.tion provides the potential for a general systemthat solves logic puzzles.Of course, in many cases, the particularmeaning of open-category words and other ex-pressions is crucial to the solution.
An exampleis provided in question 2 of Figure 1: the sys-tem has to understand what ?a complete list?means.
Therefore, to finalize the meaning com-puted for a sentence, such expressions should beexpanded to their explicit meaning.
Althoughthere are many such cases and their analysis isdifficult, we anticipate that it will be possible todevelop a relatively compact library of criticalpuzzle text expressions.
We may also be ableto use existing resources such as WordNet andFrameNet.3.6 Information GapsNatural language texts invariably assume someknowledge implicitly.
E.g., Figure 1 does not ex-plicitly specify that a sculpture may not be ex-hibited in more than one room at the same time.Humans know this implicit information, but acomputer reasoning from texts must be givenit explicitly.
Filling these information gaps isa serious challenge; representation and acquisi-tion of the necessary background knowledge arevery hard AI problems.
Fortunately, the puz-zles domain allows us to tackle this issue, asexplained in ?8.3.7 Presuppositions and ImplicaturesIn addition to its semantic meaning, a naturallanguage text conveys two other kinds of con-tent.Presuppositions are pieces of information as-sumed in a sentence.
Anaphoric expressionsbear presuppositions about the existence of en-tities in the context; the answer choice ?Sculp-tures C and E?
conveys the meaning {C,E},but has the presupposition sculpture(C) ?sculpture(E); and a question of the form A ?B, such as question 1 in Figure 1, presupposesthat A is consistent with the preamble.Implicatures are pieces of information sug-gested by the very fact of saying, or not say-ing, something.
Two maxims of (Grice, 1989)dictate that each sentence should be both con-sistent and informative (i.e.
not entailed) withrespect to its predecessors.
Another maxim dic-tates saying as much as required, and hence thesentence ?No more than three sculptures may beexhibited in any room?
carries the implicaturethat in some possible solution, three sculpturesare indeed exhibited in the same room.Systematic calculation of presuppositions andimplicatures has been given less attention inNLP and is less understood than the calcula-tion of meaning.
Yet computing and verifyingthem can provide valuable hints to the systemwhether it understood the meaning of the textcorrectly.4 Morpho-Syntactic AnalysisWhile traditional hand-built grammars often in-clude a rich semantics, we have found theircoverage inadequate for the logic puzzles task.For example, the English Resource Grammar(Copestake and Flickinger, 2000) fails to parseany of the sentences in Figure 1 for lack of cover-age of some words and of several different syn-tactic structures; and parsable simplified ver-sions of the text produce dozens of unrankedparse trees.
For this reason, we use a broad-coverage statistical parser (Klein and Manning,2003) trained on the Penn Treebank.
In addi-tion to robustness, treebank-trained statisticalparsers have the benefit of extensive researchon accurate ambiguity resolution.
Qualitatively,we have found that the output of the parser onlogic puzzles is quite good (see ?10).
After pars-ing, each word in the resulting parse trees isconverted to base form by a stemmer.A few tree-transformation rules are appliedon the parse trees to make them more conve-nient for combinatorial semantics.
Most of themare general, e.g.
imposing a binary branchingstructure on verb phrases, and grouping expres-sions like ?more than?.
A few of them correctsome parsing errors, such as nouns marked asnames and vice-versa.
There is growing aware-ness in the probabilistic parsing literature thatmismatches between training and test set genrecan degrade parse accuracy, and that smallamounts of correct-genre data can be more im-portant than large amounts of wrong-genre data(Gildea, 2001); we have found corroborating ev-idence in misparsings of noun phrases commonin puzzle texts, such as ?Sculptures C and E?,which do not appear in the Wall Street Journalcorpus.
Depending on the severity of this prob-lem, we may hand-annotate a small amount ofpuzzle texts to include in parser training data.5 Combinatorial SemanticsWork in NLP has shifted from hand-built gram-mars that need to cover explicitly every sen-tence structure and that break down on unex-pected inputs to more robust statistical parsing.However, grammars that involve precise seman-tics are still largely hand-built (e.g.
(Carpenter,1998; Copestake and Flickinger, 2000)).
We aimat extending the robustness trend to the seman-tics.
We start with the compositional semanticsframework of (Blackburn and Bos, 2000; Bos,2001) and modify it to achieve greater robust-ness and coverage.2One difference is that our lexicon is keptvery small and includes only a few words withspecial semantic entries (like pronouns, con-nectives, and numbers).
Open-category wordscome with their part-of-speech information inthe parse trees (e.g.
(NN dog)), so their seman-tics can be obtained using generic semantic tem-plates (but cf.
?3.5).In classic rule-to-rule systems of semanticslike (Blackburn and Bos, 2000), each syntacticrule has a separate semantic combination rule,and so the system completely fails on unseensyntactic structures.
The main distinguishinggoal of our approach is to develop a more robustprocess that does not need to explicitly specifyhow to cover every bit of every sentence.
Thesystem incorporates a few initial ideas in thisdirection.First, role and argument-structure informa-tion for verbs is expensive to obtain and unre-liable anyway in natural texts.
So to deal withverbs and VPs robustly, their semantics in oursystem exports only an event variable ratherthan variables for the subject, the direct object,etc.
VP modifiers (such as PPs and ADVPs)combine to the VP by being applied on the ex-ported event variable.
NP modifiers (includingthe sentence subject) are combined to the eventvariable through generic roles: subj, np1, np2,etc.
The resulting generic representations aresuitable in the puzzles domain because usuallyonly the relation between objects is importantand not their particular roles in the relation.This is true for other tasks as well, includingsome broad-coverage question answering.All NPs are analyzed as generalized quanti-fiers, but a robust compositional analysis forthe internal semantics of NPs remains a seriouschallenge.
For example, the NP ?three rooms?should be analyzed as Q(num(3), x, room(x), ..),but the word ?three?
by itself does not con-tribute the quantifier ?
compare with ?at leastthree rooms?
Q(?3, x, room(x), ..).
Yet anothercase is ?the three rooms?
(which presupposes2Our system uses a reimplementation in Lisp ratherthan their Prolog code.a group g such that g ?
room ?
|g| = 3).
Thesystem currently handles a number of NP struc-tures by scanning the NP left-to-right to iden-tify important elements.
This may make it eas-ier than a strictly compositional analysis to ex-tend the coverage to additional cases.All other cases are handled by a flexible com-bination process.
In case of a single child, itssemantics is copied to its parent.
With morechildren, all combinations of applying the se-mantics of one child to its siblings are tried,until an application does not raise a type er-ror (variables are typed to support type check-ing).
This makes it easier to extend the coverageto new grammatical constructs, because usuallyonly the lexical entry needs to be specified, andthe combination process takes care to apply itcorrectly in the parse tree.6 Scope ResolutionOne way of dealing with scope ambiguities is byusing underspecified representations (URs).
AUR is a meta-language construct, describing aset of object-language formulas.3 It describesthe pieces shared by these formulas, but possi-bly underspecifies how they combine with eachother.
A UR can then be resolved to the specificreadings it implicitly describes.We use an extension of Hole Semantics(Blackburn and Bos, 2000)4 for expressing URsand calculating them from parse trees (modulothe modifications in ?5).
There are several ad-vantages to this approach.
First, it supportsthe calculation of just one UR per sentence ina combinatorial process that visits each node ofthe parse tree once.
This contrasts with ap-proaches such as Categorial Grammars (Car-penter, 1998), which produce explicitly all thescopings by using type raising rules for differentcombinations of scope, and require scanning theentire parse tree once per scoping.Second, the framework supports the expres-sion of scoping constraints between differentparts of the final formula.
Thus it is possibleto express hierarchical relations that must existbetween certain quantifiers, avoiding the prob-lems of naive approaches such as Cooper stor-age (Cooper, 1983).
The expression of scopingconstraints is not limited to quantifiers and isapplicable to all other operators as well.
More-over, it is possible to express scope islands by3In our case, DL formulas ?
see footnote 6.4The approach is similar to MRS (Copestake et al,2003).constraining all the parts of a subformula to beoutscoped by a particular node.Another advantage is that URs support ef-ficient elimination of logically-equivalent read-ings.
Enumerating all scopings and usinga theorem-prover to determine logical equiva-lences requires O(n2) comparisons for n scop-ings.
Instead, filtering methods (Chaves, 2003)can add tests to the UR-resolution process,disallowing certain combinations of operators.Thus, only one ordering of identical quantifiersis allowed, so ?A man saw a woman?
yieldsonly one of its two equivalent scopings.
We alsofilter ??
and ??
combinations, allowing onlythe equivalent ??
and ??.
However, numericquantifiers are not filtered (the two scopings of?Three boys saw three films?
are not equiva-lent).
Such filtering can result in substantialspeed-ups for sentences with a few quantifiers(see (Chaves, 2003) for some numbers).Finally, our true goal is determining the cor-rect relative scoping in context rather than enu-merating all possibilities.
We are developinga probabilistic scope resolution module thatlearns from hand-labeled training examples topredict the most probable scoping, using fea-tures such as the quantifiers?
categories andtheir positions and grammatical roles in the sen-tence.57 Reference ResolutionSL is not convenient for representing directlythe meaning of referring expressions because (asin FOL) the extent of a quantifier in a formulacannot be extended easily to span variables insubsequent formulas.
We therefore use Dis-course Logic (DL), which is SL extended withDRSes and ?-expressions as in (Blackburn andBos, 2000) (which is based on Discourse Repre-sentation Theory (Kamp and Reyle, 1993) andits recent extensions for dealing with presuppo-sitions).6 This approach (like other dynamic se-mantics approaches) supports the introductionof entities that can later be referred back to,and explains when indefinite NPs should be in-5E.g.
there is a strong preference for ?each?
to takewide scope, a moderate preference for the first quantifierin a sentence to take wide scope, and a weak preferencefor a quantifier of the grammatical subject to take widescope.6Thus, the URs calculated from parse trees are ac-tually URs of DL formulas.
The scope resolution phaseresolves the URs to explicit DL formulas, and the ref-erence resolution phase converts these formulas to SLformulas.terpreted as existential or universal quantifiers(such as in the antecedent of conditionals).
Thereference resolution framework from (Blackburnand Bos, 2000) provides a basis for finding allpossible resolutions, but does not specify whichone to choose.
We are working on a probabilis-tic reference-resolution module, which will pickfrom the legal resolutions the most probable onebased on features such as: distance, gender, syn-tactic place and constraints, etc.8 Filling Information GapsTo find a unique answer to every question of apuzzle, background information is required be-yond the literal meaning of the text.
In Ques-tion 1 of Figure 1, for example, without the con-straint that a sculpture may not be exhibited inmultiple rooms, answers B, D and E are all cor-rect.
Human readers deduce this implicit con-straint from their knowledge that sculptures arephysical objects, rooms are locations, and phys-ical objects can have only one location at anygiven time.
In principle, such information couldbe derived from ontologies.
Existing ontologies,however, have limited coverage, so we also planto leverage information about expected puzzlestructures.Most puzzles we collected are formaliz-able as constraints on possible tuples of ob-jects.
The crucial information includes: (a)the object classes; (b) the constants nam-ing the objects; and (c) the relations used tolink objects, together with their arguments?classes.
For the sculptures puzzle, this infor-mation is: (a) the classes are sculpture androom; (b) the constants are C,D,E, F,G,H forsculpture and 1, 2, 3 for room; (c) the relationis exhibit(sculpture, room).
This information isobtainable from the parse trees and SL formu-las.Within this framework, implicit world knowl-edge can often be recast as mathematical prop-erties of relations.
The unique location con-straint on sculptures, for example, is equivalentto constraining the mapping from sculptures torooms to be injective (one-to-one); other casesexist of constraining mappings to be surjective(onto) and/or total.
Such properties can be ob-tained from various sources, including cardinal-ity of object classes, pure lexical semantics, andeven through a systematic search for sets of im-plicit constraints that, in combination with theexplicitly stated constraints, yield exactly oneanswer per question.
Figure 3 shows the num-only object classes23?6 = 262, 144 modelsexplicit constraints2,916 modelsimplicit constraints36 = 729 modelsexplicit and implicit constraints78 modelsFigure 3: Effect of explicit and implicit con-straints on constraining the number of possiblemodelsber of possible models for the sculptures puzzleas affected by explicit and implicit constraintsin the preamble.9 Solving the Puzzle9.1 Expanding the answer choicesThe body of a logic puzzle question contains a(unique) wh-term (typically ?which of the fol-lowing?
), a modality (such as ?must be true?
or?could be true?
), and (possibly) an added condi-tion.
Each answer choice is expanded by substi-tuting its SL form for the wh-term in the ques-tion body.
For example, the expansion for an-swer choice (A) of question 1 in Figure 1 wouldbe the SL form corresponding to: ?If sculptureD is exhibited .
.
.
, then [Sculpture C is exhibitedin room 1 ] must be true?.9.2 Translating SL to FOLTo translate an SL representation to pure FOL,we eliminate event variables by replacing an SLform ?e.P (e)?R1(e, t1)?
..?Rn(e, tn) with theFOL form P (t1, .., tn).
An ordering is imposedon role names to guarantee that arguments arealways used in the same order in relations.
Nu-meric quantifiers are encoded in FOL in the ob-vious way, e.g., Q(?2, x, ?, ?)
is translated to?x1?x2.
x1 6= x2?
(???)[x1/x]?
(???
)[x2/x].Each expanded answer choice contains onemodal operator.
Modals are moved outwardof negation as usual, and outward of condition-als by changing A ?
?B to ?
(A ?
B) andA ?
?B to ?(A?B).
A modal operator in theoutermost scope can then be interpreted as adirective to the reasoning module to test eitherentailment (?)
or consistency (?)
between thepreamble and the expanded answer choice.9.3 Using FOL reasonersThere are two reasons for using both theo-rem provers and model builders.
First, theyare complementary reasoners: while a theoremprover is designed to demonstrate the incon-sistency of a set of FOL formulas, and so canfind the correct answer to ?must be true?
ques-tions through proof by contradiction, a modelbuilder is designed to find a satisfying model,and is thus suited to finding the correct an-swer to ?could be true?
questions.7 Second, areasoner may take a very long time to halt onsome queries, but the complementary reasonermay still be used to answer the query in thecontext of a multiple-choice question througha process of elimination.
Thus, if the modelbuilder is able to show that the negations of fourchoices are consistent with the preamble (indi-cating they are not entailed), then it can beconcluded that the remaining choice is entailedby the preamble, even if the theorem prover hasnot yet found a proof.We use the Otter 3.3 theorem prover andthe MACE 2.2 model builder (McCune, 1998).8The reasoning module forks parallel sub-processes, two per answer choice (one for Otter,one for MACE).
If a reasoner succeeds for an an-swer choice, the choice is marked as correct orincorrect, and the dual sub-process is killed.
Ifall answer-choices but one are marked incorrect,the remaining choice is marked correct even ifits sub-processes did not yet terminate.10 ProgressUsing the sculptures puzzle (a set of four ques-tions partly shown in Figure 1) as an initial testcase, we have built a prototype end-to-end sys-tem.
In its present state, the system analyzesand solves correctly all questions in this puzzle,except that there is still no understanding of thephrase ?complete list?
in question 2.
The back-end reasoning module is finished and works forany puzzle formalized in FOL+modals.
Theprobabilistic scope resolution module, trainedon 259 two-quantifier sentences extracted from122 puzzles and tested on 46 unseen sentences,attains an accuracy of about 94% over an 82%linear-order baseline.
A preliminary evaluationon another unseen puzzle shows that on 60%of the sentences, the parser?s output is accurateenough to support the subsequent computationof the semantics, and we expect this to be betterafter it is trained on puzzle texts.
However, the7GRE puzzles always deal with finite domains, so amodel builder is guaranteed to halt on consistent sets offormulas.8An advantage of using Otter and MACE is that theyare designed to work together, and use the same inputsyntax.system as a whole worked end-to-end on onlyone of the unseen sentences in that puzzle; keylosses come from unhandled semantic phenom-ena (e.g.
?only?, ?except?, ellipses), unhandledlexical semantics of words that must be under-stood (e.g.
?complete list?
), and unhandled im-plicit constraint types that need to be filled.11 Conclusion and Further WorkThe key open problem is identifying sufficientlyrobust and general methods for building precisesemantic representations, rather than requiringhand-built translation rules for a seemingly end-less list of special phenomena.
Immediate fu-ture work will include extending and generaliz-ing the system?s coverage of syntax-to-semanticsmappings, incorporating classifiers for suggest-ing likely coreference resolutions and operatorscopings, and developing methods for calculat-ing presuppositions and inferences.
This workmay be sufficient to give good coverage of theproblem domain, or we may need to developnew more robust models of syntactic to seman-tic transductions.AcknowledgementsThanks to Kristina Toutanova for useful discus-sions.This work was supported in part by the AdvancedResearch and Development Activity (ARDA)?sAdvanced Question Answering for Intelligence(AQUAINT) Program; in part by the Departmentof the Navy under grant no.
N000140010660, a Mul-tidisciplinary University Research Initiative on Nat-ural Language Interaction with Intelligent TutoringSystems; and in part by Department of Defenseaward no.
NBCH-D-03-0010(1) ?A Person-CentricEnduring, Personalized, Cognitive Assistant.
?ReferencesHiyan Alshawi, editor.
1992.
The Core LanguageEngine.
MIT Press.J.
Barwise and R. Cooper.
1981.
Generalized quan-tifiers and natural language.
Linguistics and Phi-losophy, 4:159?219.Patrick Blackburn and Johan Bos.
2000.
Rep-resentation and Inference for Natural Language:A First Course in Computational Semantics.http://www.comsem.org/.Johan Bos.
2001.
Doris 2001: Underspecification,resolution and inference for discourse representa-tion structures.
In Blackburn and Kohlhase, edi-tors, ICoS-3.
Inference in Compuational Seman-tics.
Workshop Proceedings.Bob Carpenter.
1998.
Type-Logical Semantics.
MITPress.Rui P. Chaves.
2003.
Non-redundant scope disam-biguation in underspecified semantics.
In Balderten Cate, editor, Proc.
of the 8th ESSLLI StudentSession, pages 47?58.Michael Collins.
1999.
Head-Driven Statistical Mod-els for Natural Language Parsing.
Ph.D. thesis,University of Pennsylvania.Robin Cooper.
1983.
Quantification and SyntacticTheory.
Reidel, Dordrecht.A.
Copestake and D. Flickinger.
2000.
An open-source grammar development environment andbroad-coverage english grammar using HPSG.
InProceedings of LREC.A.
Copestake, D. Flickinger, C. Pol-lard, and I.
Sag.
2003.
Minimal re-cursion semantics: an introduction.http://lingo.stanford.edu/sag/publications.html.D.
Gildea and D. Jurafsky.
2002.
Automatic label-ing of semantic roles.
Computational Linguistics,28(3):245?288.Daniel Gildea.
2001.
Corpus variation and parserperformance.
In Proceedings of EMNLP, pages167?202.Paul H. Grice.
1989.
Studies in the way of words.Harvard University Press.Lynette Hirschman, Marc Light, Eric Breck, andJohn D. Burger.
1999.
Deep Read: A readingcomprehension system.
In Proc.
of the 37th An-nual Meeting of the ACL, pages 325?332.Hans Kamp and Uwe Reyle.
1993.
From Discourseto Logic.
Kluwer, Dordrecht.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proc.
of the 41stAnnual Meeting of the ACL, pages 423?430.W.
McCune.
1998.
Automatic proofs and counterex-amples for some ortholattice identities.
Informa-tion Processing Letters, 65:285?291.Dan I. Moldovan, Christine Clark, Sanda M.Harabagiu, and Steven J. Maiorano.
2003.
CO-GEX: A logic prover for question answering.
InProc.
of HLT/NAACL, pages 87?93.Marius Pasca and Sanda M. Harabagiu.
2001.
Highperformance question/answering.
In Proc.
of SI-GIR, pages 366?374.Stanford NLP Group.
2004.
Project website.http://nlp.stanford.edu/nlkr/.David Warren and Fernando Pereira.
1982.
An effi-cient easily adaptable system for interpreting nat-ural language queries.
Computational Linguistics,8(3-4):110?122.Karl Weber.
1999.
The Unofficial Guide to the GRETest.
ARCO Publishing, 2000 edition.W.
A.
Woods.
1973.
Progress in natural languageunderstanding: An application to lunar geology.In AFIPS Conference Proceedings, volume 42,pages 441?450.
