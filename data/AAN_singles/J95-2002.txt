An Efficient Probabilistic Context-FreeParsing Algorithm that Computes PrefixProbabilitiesAndreas  Stolcke*University of California at BerkeleyandInternational Computer Science InstituteWe describe an extension of Earley's parser for stochastic context-free grammars that computes thefollowing quantities given a stochastic context-free grammar and an input string: a) probabilitiesof successive prefixes being generated by the grammar; b) probabilities ofsubstrings being gen-erated by the nonterminals, including the entire string being enerated by the grammar; c) mostlikely (Viterbi) parse of the string; d) posterior expected number of applications of each grammarproduction, as required for reestimating rule probabilities.
Probabilities (a) and (b) are computedincrementally in a single left-to-right pass over the input.
Our algorithm compares favorably tostandard bottom-up arsing methods for SCFGs in that it works efficiently on sparse grammarsby making use of Earley's top-down control structure.
It can process any context-free rule formatwithout conversion to some normal form, and combines computations for (a) through (d) in asingle algorithm.
Finally, the algorithm has simple extensions for processing partially bracketedinputs, and for finding partial parses and their likelihoods on ungrammatical inputs.1.
IntroductionContext-free grammars are widely used as models of natural language syntax.
Intheir probabilistic version, which defines a language as a probability distribution overstrings, they have been used in a variety of applications: for the selection of parsesfor ambiguous inputs (Fujisaki et al 1991); to guide the rule choice efficiently duringparsing (Jones and Eisner 1992); to compute island probabilities for non-linear parsing(Corazza et al 1991).
In speech recognition, probabilistic ontext-free grammars playa central role in integrating low-level word models with higher-level language mod-els (Ney 1992), as well as in non-finite-state acoustic and phonotactic modeling (Lariand Young 1991).
In some work, context-free grammars are combined with scoringfunctions that are not strictly probabilistic (Nakagawa 1987), or they are used withcontext-sensitive and/or semantic probabilities (Magerman and Marcus 1991; Mager-man and Weir 1992; Jones and Eisner 1992; Briscoe and Carroll 1993).Although clearly not a perfect model of natural anguage, stochastic ontext-freegrammars (SCFGs) are superior to nonprobabilistic CFGs, with probability theory pro-viding a sound theoretical basis for ranking and pruning of parses, as well as forintegration with models for nonsyntactic aspects of language.
All of the applicationslisted above involve (or could potentially make use of) one or more of the following* Speech Technology and Research Laboratory, SRI International, 333 Ravenswood Ave., Menlo Park,CA 94025.
E-mail: stolcke@speech.sri.com.?
1995 Association for Computational LinguisticsComputational Linguistics Volume 21, Number 2standard tasks, compiled by Jelinek and Lafferty (1991).
1..3..What is the probability that a given string x is generated by a grammarG?What is the single most likely parse (or derivation) for x?What is the probability that x occurs as a prefix of some string generatedby G (the prefix probability of x)?How should the parameters (e.g., rule probabilities) in G be chosen tomaximize the probability over a training set of strings?The algorithm described in this article can compute solutions to all four of theseproblems in a single framework, with a number of additional advantages over previ-ously presented isolated solutions.Most probabilistic parsers are based on a generalization of bottom-up chart pars-ing, such as the CYK algorithm.
Partial parses are assembled just as in nonprobabilisticparsing (modulo possible pruning based on probabilities), while substring probabili-ties (also known as "inside" probabilities) can be computed in a straightforward way.Thus, the CYK chart parser underlies the standard solutions to problems (1) and (4)(Baker 1979), as well as (2) (Jelinek 1985).
While the Jelinek and Lafferty (1991) solu-tion to problem (3) is not a direct extension of CYK parsing, the authors neverthelesspresent heir algorithm in terms of its similarities to the computation of inside proba-bilities.In our algorithm, computations for tasks (1) and (3) proceed incrementally, as theparser scans its input from left to right; in particular, prefix probabilities are availableas soon as the prefix has been seen, and are updated incrementally as it is extended.Tasks (2) and (4) require one more (reverse) pass over the chart constructed from theinput.Incremental, left-to-right computation of prefix probabilities i particularly impor-tant since that is a necessary condition for using SCFGs as a replacement for finite-statelanguage models in many applications, uch a speech decoding.
As pointed out by Je-linek and Lafferty (1991), knowing probabilities P (Xo... xi) for arbitrary prefixes Xo...  xienables probabilistic prediction of possible follow-words Xi+l, as P(xi+l I xo.
.
.x i )  =P(Xo...xixi+I)/P(xo...xi).
These conditional probabilities can then be used as wordtransition probabilities in a Viterbi-style decoder or to incrementally compute the costfunction for a stack decoder (Bahl, Jelinek, and Mercer 1983).Another application in which prefix probabilities play a central role is the extrac-tion of n-gram probabilities from SCFGs (Stolcke and Segal 1994).
Here, too, efficientincremental computation saves time, since the work for common prefix strings can beshared.The key to most of the features of our algorithm is that it is based on the top-down parsing method for nonprobabilistic CFGs developed by Earley (1970).
Earley'salgorithm is appealing because it runs with best-known complexity on a number ofspecial classes of grammars.
In particular, Earley parsing is more efficient than thebottom-up methods in cases where top-down prediction can rule out potential parsesof substrings.
The worst-case computational expense of the algorithm (either for thecomplete input, or incrementally for each new word) is as good as that of the other1 Their paper phrases these problem in terms of context-free probabilistic grammars, but they generalizein obvious ways to other classes of models.166Andreas Stolcke Efficient Probabilistic Context-Free Parsingknown specialized algorithms, but can be substantially better on well-known grammarclasses.Earley's parser (and hence ours) also deals with any context-free rule format ina seamless way, without requiring conversions to Chomsky Normal Form (CNF), asis often assumed.
Another advantage is that our probabilistic Earley parser has beenextended to take advantage of partially bracketed input, and to return partial parseson ungrammatical input.
The latter extension removes one of the common objectionsagainst op-down, predictive (as opposed to bottom-up) parsing approaches (Mager-man and Weir 1992).2.
OverviewThe remainder of the article proceeds as follows.
Section 3 briefly reviews the workingsof an Earley parser without regard to probabilities.
Section 4 describes how the parserneeds to be extended to compute sentence and prefix probabilities.
Section 5 deals withfurther modifications for solving the Viterbi and training tasks, for processing partiallybracketed inputs, and for finding partial parses.
Section 6 discusses miscellaneousissues and relates our work to the literature on the subject.
In Section 7 we summarizeand draw some conclusions.To get an overall idea of probabilistic Earley parsing it should be sufficient to readSections 3, 4.2, and 4.4.
Section 4.5 deals with a crucial technicality, and later sectionsmostly fill in details and add optional features.We assume the reader is familiar with the basics of context-free grammar the-ory, such as given in Aho and Ullman (1972, Chapter 2).
Some prior familiarity withprobabilistic ontext-free grammars will also be helpful.
Jelinek, Lafferty, and Mercer(1992) provide a tutorial introduction covering the standard algorithms for the fourtasks mentioned in the introduction.Notation.
The input string is denoted by x. Ix\[ is the length of x.
Individual inputsymbols are identified by indices starting at 0: x0,xl .
.
.
.
.
Xlxl_ 1.
The input alphabetis denoted by E. Substrings are identified by beginning and end positions Xi...j. Thevariables i,j,k are reserved for integers referring to positions in input strings.
Latincapital etters X, Y, Z denote nonterminal symbols.
Latin lowercase letters a, b,... areused for terminal symbols.
Strings of mixed nonterminal nd terminal symbols arewritten using lowercase Greek letters ,~, #, v. The empty string is denoted by e.3.
Earley ParsingAn Earley parser is essentially a generator that builds left-most derivations of strings,using a given set of context-free productions.
The parsing functionality arises becausethe generator keeps track of all possible derivations that are consistent with the inputstring up to a certain point.
As more and more of the input is revealed, the set ofpossible derivations (each of which corresponds to a parse) can either expand as newchoices are introduced, or shrink as a result of resolved ambiguities.
In describing theparser it is thus appropriate and convenient to use generation terminology.The parser keeps a set of states for each position in the input, describing allpending derivations, a These state sets together form the Earley chart.
A state is of the2 Earley states are also known as items in LR parsing; see Aho and Ullman (1972, Section 5.2) andSection 6.2.167Computational Linguistics Volume 21, Number 2fo rmi :  k X --* ,~.#,where X is a nonterminal of the grammar, ,~ and # are strings of nonterminals and/orterminals, and i and k are indices into the input string.
States are derived from pro-ductions in the grammar.
The above state is derived from a corresponding productionX--* ~#with the following semantics:?
The current position in the input is i, i.e., Xo.
.
.x i -1  have been processedSO far.
3 The states describing the parser state at position i are collectivelycalled state set i.
Note that there is one more state set than inputsymbols: set 0 describes the parser state before any input is processed,while set ixl contains the states after all input symbols have beenprocessed.?
Nonterminal X was expanded starting at position k in the input, i.e., Xgenerates some substring starting at position k.?
The expansion of X proceeded using the production X ~ ~#, and hasexpanded the right-hand side (RHS) ,~# up to the position indicated bythe dot.
The dot thus refers to the current position i.A state with the dot to the right of the entire RHS is called a complete state, sinceit indicates that the left-hand side (LHS) nonterminal has been fully expanded.Our description of Earley parsing omits an optional feature of Earley states, thelookahead string.
Earley's algorithm allows for an adjustable amount of lookaheadduring parsing, in order to process LR(k) grammars deterministically (and obtain thesame computational complexity as specialized LR(k) parsers where possible).
The ad-dition of lookahead is orthogonal to our extension to probabilistic grammars, o wewill not include it here.The operation of the parser is defined in terms of three operations that consult hecurrent set of states and the current input symbol, and add new states to the chart.
Thisis strongly suggestive of state transitions in finite-state models of language, parsing,etc.
This analogy will be explored further in the probabilistic formulation later on.The three types of transitions operate as follows.Prediction.
For each statei :  kX ~ &.Y#,where Y is a nonterminal nywhere in the RHS, and for all rules Y --* L, expanding Y,add statesi : iY  ~ .7/.A state produced by prediction is called a predicted state.
Each prediction correspondsto a potential expansion of a nonterminal in a left-most derivation.3 This index is implicit in Earley (1970).
We include it here for clarity.168Andreas Stolcke Efficient Probabilistic Context-Free ParsingScanning.
For each statei: kX ~ )~.a#,where a is a terminal symbol that matches the current input xi, add the statei+1:  kX ~ )~a.#(move the dot over the current symbol).
A state produced by scanning is called ascanned state.
Scanning ensures that the terminals produced in a derivation matchthe input string.Completion.
For each complete statei: jy" '+ ~.and each state in set j, j < i, that has Y to the right of the dot,j : kX --~ .~.Y#,add the statei: kX--* ,~Y.#(move the dot over the current nonterminal).
A state produced by completion is calleda completed state.
4Each completion corresponds to the end of a nonterminal expan-sion started by a matching prediction step.For each input symbol and corresponding state set, an Earley parser performsall three operations exhaustively, i.e., until no new states are generated.
One crucialinsight into the working of the algorithm is that, although both prediction and com-pletion feed themselves, there are only a finite number of states that can possiblybe produced.
Therefore recursive prediction and completion at each position have toterminate ventually, and the parser can proceed to the next input via scanning.To complete the description we need only specify the initial and final states.
Theparser starts out with0 : 0 ~ .S,where S is the sentence nonterminal (note the empty left-hand side).
After processingthe last symbol, the parser verifies that1: 0 ---~ S.has been produced (among possibly others), where I is the length of the input x.
If atany intermediate stage a state set remains empty (because no states from the previousstage permit scanning), the parse can be aborted because an impossible prefix has beendetected.States with empty LHS such as those above are useful in other contexts, as willbe shown in Section 5.4.
We will refer to them collectively as dummy states.
Dummystates enter the chart only as a result of initialization, as opposed to being derivedfrom grammar productions.4 Note the difference between "complete" and "completed" states: complete states (those with the dot tothe right of the entire RHS) are the result of a completion or scanning step, but completion alsoproduces tates that are not yet complete.169Computational Linguistics Volume 21, Number 2Table 1(a) Example grammar for a tiny fragment of English.
(b) Earley parser processing the sentencea circle touches a triangle.
(a)(b)S --* NPVP Det --~ aNP -* Det N N -* circle \[ square \[ triangleVP ~ VT NP VT ~ touchesVP --* VIPP VI --* isPP --* P NP P --* above \] belowa circle touches  a square0 ----~ .Spredicted0S ~ .NP  VP0NP --~ .Det N0Det --~ .ascanned scanned scanned scanned scannedoDet ~ a.
1N ---* circle.
2VT ---* touches .
3Det ~ a.
4N --4 t r iangle .completed completed completed completed completedoNP --~ Det .N  oNP --~ Det  N.  2VP ~ VT .NP  3NP ~ Det .N  4NP ~ Det  N.predicted oS --~ NP .VP  predicted predicted 3VP --4 VT NP .1N -*  .circle predicted 3NP --* .Det N 5N ~ .circle oS --* NP  VP.1N --~ .
square  2VP --* .VT NP  3Det --* .a 4N --~ .
square  0 --* S.1N --* .
t r iangle  2VP --~ .VI PP  4 N --* .
t r iangle2VT ~ .
touches2VI ~ .isState set 0 1 2 3 4 5It is easy to see that Earley parser operations are correct, in the sense that eachchain of transitions (predictions, scanning steps, completions) corresponds to a pos-sible (partial) derivation.
Intuitively, it is also true that a parser that performs thesetransitions exhaustively is complete, i.e., it finds all possible derivations.
Formal proofsof these properties are given in the literature; e.g., Aho and Ullman (1972).
The rela-tionship between Earley transitions and derivations will be stated more formally inthe next section.The parse trees for sentences can be reconstructed from the chart contents.
We willillustrate this in Section 5 when discussing Viterbi parses.Table 1 shows a simple grammar and a trace of Earley parser operation on asample sentence.Earley's parser can deal with any type of context-free rule format, even withnull or c-productions, i.e., those that replace a nonterminal with the empty string.Such productions do, however, require special attention, and make the algorithm andits description more complicated than otherwise necessary.
In the following sectionswe assume that no null productions have to be dealt with, and then summarize thenecessary changes in Section 4.7.
One might choose to simply preprocess the grammarto eliminate null productions, a process which is also described.4.
Probabilistic Earley Parsing4.1 Stochastic Context-Free GrammarsA stochastic ontext-free grammar (SCFG) extends the standard context-free formalismby adding probabilities to each production:\[p\],170Andreas Stolcke Efficient Probabilistic Context-Free Parsingwhere the rule probability p is usually written as P(X --+ ~).
This notation to someextent hides the fact that p is a conditional probability, of production X --+ ,~ beingchosen, given that X is up for expansion.
The probabilities of all rules with the samenonterminal X on the LHS must therefore sum to unity.
Context-freeness in a prob-abilistic setting translates into conditional independence of rule choices.
As a result,complete derivations have joint probabilities that are simply the products of the ruleprobabilities involved.The probabilities of interest mentioned in Section 1 can now be defined formally.Definition 1The following quantities are defined relative to a SCFG G, a nonterminal X, and astring x over the alphabet y~ of G.a) The probability of a (partial) derivation/71 ~/72 ~ ""/Tk is inductivelydefined by1)2)P(/71) = 1P(/71 ~ "" ~ /Tk) = P(X --* ,X)P(u2 ~.
.
.
~/Tk),b)c)d)where /71,/72 .
.
.
.
,/Tk are strings of terminals and nonterminals, X ~ A is aproduction of G, and u2 is derived from/71 by replacing one occurrenceof X with &.The string probabil ity P(X =g x) (of x given X) is the sum of theprobabilities of all left-most derivations X => ... => x producing xfrom X. sThe sentence probability P(S ~ x) (of x given G) is the stringprobability given the start symbol S of G. By definition, this is also theprobability P(x I G) assigned to x by the grammar G.The prefix probabil ity P(S g>L X) (of X given G) is the sum of theprobabilities of all sentence strings having x as a prefix,P(S x) = p(s xy)yEY~*(In particular, P(S ~L e) = 1).In the following, we assume that the probabilities in a SCFG are proper and con-sistent as defined in Booth and Thompson (1973), and that the grammar contains nouseless nonterminals (ones that can never appear in a derivation).
These restrictionsensure that all nonterminals define probability measures over strings; i.e., P(X ~ x) isa proper distribution over x for all X.
Formal definitions of these conditions are givenin Appendix A.5 In a left-most derivation each step replaces the nonterminal furthest o the left in the partiallyexpanded string.
The order of expansion is actually irrelevant for this definition, because of themultiplicative combination of production probabilities.
We restrict summation to left-most derivationsto avoid counting duplicates, and because left-most derivations will play an important role later.171Computational Linguistics Volume 21, Number 24.2 Earley Paths and Their ProbabilitiesIn order to define the probabilities associated with parser operation on a SCFG, weneed the concept of a path, or partial derivation, executed by the Earley parser.Definition 2a)b)c)d)e)An (unconstrained) Earley path, or simply path, is a sequence of Earleystates linked by prediction, scanning, or completion.
For the purpose ofthis definition, we allow scanning to operate in "generation mode," i.e.,all states with terminals to the right of the dot can be scanned, not justthose matching the input.
(For completed states, the predecessor state isdefined to be the complete state from the same state set contributing tothe completion.
)A path is said to be constrained by, or to generate a string x if theterminals immediately to the left of the dot in all scanned states, insequence, form the string x.A path is complete if the last state on it matches the first, except hat thedot has moved to the end of the RHS.We say that a path starts with nonterminal X if the first state on it is apredicted state with X on the LHS.The length of a path is defined as the number of scanned states on it.Note that the definition of path length is somewhat counterintuitive, but is moti-vated by the fact that only scanned states correspond irectly to input symbols.
Thusthe length of a path is always the same as the length of the input string it generates.A constrained path starting with the initial state contains a sequence of statesfrom state set 0 derived by repeated prediction, followed by a single state from set 1produced by scanning the first symbol, followed by a sequence of states produced bycompletion, followed by a sequence of predicted states, followed by a state scanningthe second symbol, and so on.
The significance of Earley paths is that they are in aone-to-one correspondence with left-most derivations.
This will allow us to talk aboutprobabilities of derivations, trings, and prefixes in terms of the actions performed byEarley's parser.
From now on, we will use "derivation" to imply a left-most derivation.Lemma 1a)b)An Earley parser generates statei: kX--+ A.#,if and only if there is a partial derivationS ~ Xo...k_lXly =~ Xo...k_l/~#I/ G Xo...k_lXk...i_l#l/deriving a prefix Xo...i-1 of the input.There is a one-to-one mapping between partial derivations and Earleypaths, such that each production X --~ ~, applied in a derivationcorresponds to a predicted Earley state X --~ .L,.172Andreas Stolcke Efficient Probabilistic Context-Free Parsing(a) is the invariant underlying the correctness and completeness of Earley's algo-rithm; it can be proved by induction on the length of a derivation (Aho and Ullman1972, Theorem 4.9).
The slightly stronger form (b) follows from (a) and the way pos-sible prediction steps are defined.Since we have established that paths correspond to derivations, it is convenientto associate derivation probabilities directly with paths.
The uniqueness condition (b)above, which is irrelevant to the correctness of a standard Earley parser, justifies (prob-abilistic) counting of paths in lieu of derivations.Definit ion 3The probability P(7 ~) of a path ~ is the product of the probabilities of all rules usedin the predicted states occurring in ~v.Lemma 2a) For all paths 7 ~ starting with a nonterminal X, P(7 ~) gives the probabilityof the (partial) derivation represented by ~v.
In particular, the stringprobability P(X ~ x) is the sum of the probabilities of all paths startingwith X that are complete and constrained by x.b) The sentence probability P(S ~ x) is the sum of the probabilities of allcomplete paths starting with the initial state, constrained by x.c) The prefix probability P(S ~L X) is the sum of the probabilities of allpaths 7 ~ starting with the initial state, constrained by x, that end in ascanned state.Note that when summing over all paths "starting with the initial state," summa-tion is actually over all paths starting with S, by definition of the initial state 0 --* .S.
(a) follows directly from our definitions of derivation probability, string probability,path probability, and the one-to-one correspondence b tween paths and derivationsestablished by Lemma 1.
(b) follows from (a) by using S as the start nonterminal.
Toobtain the prefix probability in (c), we need to sum the probabilities of all completederivations that generate x as a prefix.
The constrained paths ending in scanned statesrepresent exactly the beginnings of all such derivations.
Since the grammar is assumedto be consistent and without useless nonterminals, all partial derivations can be com-pleted with probability one.
Hence the sum over the constrained incomplete paths isthe sought-after sum over all complete derivations generating the prefix.4.3 Forward and Inner ProbabilitiesSince string and prefix probabilities are the result of summing derivation probabilities,the goal is to compute these sums efficiently by taking advantage of the Earley controlstructure.
This can be accomplished by attaching two probabilistic quantities to eachEarley state, as follows.
The terminology is derived from analogous or similar quan-tities commonly used in the literature on Hidden Markov Models (HMMs) (Rabinerand Juang 1986) and in Baker (1979).Definit ion 4The following definitions are relative to an implied input string x.a) The forward probabil ity Oq(kX--4 A.\[d,) is the sum of the probabilities ofall constrained paths of length i that end in state kX ---* .~.#.173Computational Linguistics Volume 21, Number 2b) The inner probability ~i(k x ---+ /~.\]1,) is the sum of the probabilities of allpaths of length i - k that start in state k : kX -* .
)~# and end ini : kX --* A.#, and generate the input symbols Xk... xi->It helps to interpret these quantities in terms of an unconstrained Earley parser thatoperates as a generator emitting--rather than recognizing--strings.
Instead of trackingall possible derivations, the generator traces along a single Earley path randomlydetermined by always choosing among prediction steps according to the associatedrule probabilities.
Notice that the scanning and completion steps are deterministic oncethe rules have been chosen.Intuitively, the forward probability Oq(kX "-+ ,,~.~) is the probability of an Earleygenerator producing the prefix of the input up to position i - 1 while passing throughstate kX --* ~.# at position i.
However, due to left-recursion i productions the samestate may appear several times on a path, and each occurrence is counted towardthe total ~i.
Thus, ~i is really the expected number of occurrences of the given statein state set i.
Having said that, we will refer to o~ simply as a probability, both forthe sake of brevity, and to keep the analogy to the HMM terminology of which thisis a generalization.
6 Note that for scanned states, ~ is always a probability, since bydefinition a scanned state can occur only once along a path.The inner probabilities, on the other hand, represent the probability of generatinga substring of the input from a given nonterminal, using a particular production.Inner probabilities are thus conditional on the presence of a given nonterminal X withexpansion starting at position k, unlike the forward probabilities, which include thegeneration history starting with the initial state.
The inner probabilities as defined herecorrespond closely to the quantities of the same name in Baker (1979).
The sum of "yof all states with a given LHS X is exactly Baker's inner probability for X.The following is essentially a restatement of Lemma 2 in terms of forward andinner probabilities.
It shows how to obtain the sentence and string probabilities weare interested in, provided that forward and inner probabilities can be computed ef-fectively.Lemma 3The following assumes an Earley chart constructed by the parser on an input string xwith Ixl = l.a) Provided that S :GL Xo...k-lXV is a possible left-most derivation of thegrammar (for some v), the probability that a nonterminal X generates thesubstring Xk... xi-1 can be computed as the sumP(x xki-1) = Z  i(kxi:k X---~ )~.
(sum of inner probabilities over all complete states with LHS X and startindex k).6 The same technical complication was noticed by Wright (1990) in the computation of probabilistic LRparser tables.
The relation to LR parsing will be discussed in Section 6.2.
Incidentally, a similarinterpretation of forward "probabilities" is required for HMMs with non-emitting states.174Andreas Stolcke Efficient Probabilistic Context-Free Parsingb)c)In particular, the string probability P(S G x) can be computed as 7P(s ~ x) = "rl(0 ~ s.)= ~t(0  ~ s .
)The prefix probability P(S ~a x), with Ixl = 1, can be computed asP(S ~L X) = ~ Oq(kX ---+ .~Xl_l.#)k X - -~ ,,~X l _ l .
\]~(sum of forward probabilities over all scanned states).The restriction in (a) that X be preceded by a possible prefix is necessary, sincethe Earley parser at position i will only pursue derivations that are consistent withthe input up to position i.
This constitutes the main distinguishing feature of Earleyparsing compared to the strict bottom-up computation used in the standard insideprobability computation (Baker 1979).
There, inside probabilities for all positions andnonterminals are computed, regardless of possible prefixes.4.4 Computing Forward and Inner ProbabilitiesForward and inner probabilities not only subsume the prefix and string probabilities,they are also straightforward to compute during a run of Earley's algorithm.
In fact, ifit weren't for left-recursive and unit productions their computation would be trivial.For the purpose of exposition we will therefore ignore the technical complicationsintroduced by these productions for a moment, and then return to them once theoverall picture has become clear.During a run of the parser both forward and inner probabilities will be attachedto each state, and updated incrementally asnew states are created through one of thethree types of transitions.
Both probabilities are set to unity for the initial state 0 --* .S.This is consistent with the interpretation that the initial state is derived from a dummyproduction ~ S for which no alternatives xist.Parsing then proceeds as usual, with the probabilistic computations detailed below.The probabilities associated with new states will be computed as sums of variouscombinations ofold probabilities.
As new states are generated by prediction, scanning,and completion, certain probabilities have to be accumulated, corresponding to themultiple paths leading to a state.
That is, if the same state is generated multipletimes, the previous probability associated with it has to be incremented by the newcontribution just computed.
States and probability contributions can be generated inany order, as long as the summation for one state is finished before its probabilityenters into the computation of some successor state.
Appendix B.2 suggests a way toimplement this incremental summation.Notation.
A few intuitive abbreviations are used from here on to describe Earleytransitions succinctly.
(1) To avoid unwieldy y\]~ notation we adopt the following con-vention.
The expression x += y means that x is computed incrementally asa sum ofvarious y terms, which are computed in some order and accumulated to finally yieldthe value of x.
8 (2) Transitions are denoted by ~,  with predecessor states on the left7 The definitions of forward and inner probabilities coincide for the final state.8 This notation suggests a simple implementation, being obviously borrowed from the programminglanguage C.175Computational Linguistics Volume 21, Number 2and successor states on the right.
(3) The forward and inner probabilities of states arenotated in brackets after each state, e.g.,i: kX --+ ,~.Y# \[a, 7\]is shorthand for a = a i (kX  --+ A.Y#), 7 = "fi(k X -+ ,~.Y#).Prediction (probabilistic).i: kX--+ A.Y# \[a, 7\] ~ i: iY--+.,  \[a',7'\]for all productions Y ~ u.
The new probabilities can be computed asa' += a.P(Y - -+u)7' = P(Y--+ u)Note that only the forward probability is accumulated; 7 is not used in this step.Rationale.
a' is the sum of all path probabilities leading up to kX --* )~.Y#, timesthe probability of choosing production Y --+ u.
The value "7' is just a special case of thedefinition.Scanning (probabilistic).i : k x --+ )~.a# \[a, 7\] ~ i + 1 : k x --* )~a.lt \[a', 7'\]for all states with terminal a matching input at position i.
Then"7' = 7Rationale.
Scanning does not involve any new choices, since the terminal was al-ready selected as part of the production during prediction.
9Completion (probabilistic).i: j Y  --+ u.
\ [a ' ,7" \ ]j :  kX--+,k.Yit \[a, 71 f ~ i: kX--+,kY.it \[a',7'\]Thena / += a.
'7" (11)7' += '7"7" (12)Note that ~" is not used.Rationale.
To update the old forward/ inner probabilities a and "7 to cd and "7',respectively, the probabilities of all paths expanding Y --+ t, have to be factored in.These are exactly the paths summarized by the inner probability "7".9 In different parsing scenarios the scanning step may well modify probabilities.
For example, if theinput symbols themselves have attached likelihoods, these can be integrated by multiplying them ontoa and "/when a symbol is scanned.
That way it is possible to perform efficient Earley parsing withintegrated joint probability computation directly on weighted lattices describing ambiguous inputs.176Andreas Stolcke Efficient Probabilistic Context-Free Parsing4.5 Coping with RecursionThe standard Earley algorithm, together with the probability computations describedin the previous ection, would be sufficient if it weren't for the problem of recursionin the prediction and completion steps.The nonprobabilistic Earley algorithm can stop recursing as soon as all predic-tions/completions yield states already contained in the current state set.
For the com-putation of probabilities, however, this would mean truncating the probabilities re-sulting from the repeated summing of contributions.4.5.1 Prediction loops.
As an example, consider the following simple left-recursiveSCFG.s - .aS ~ Sb \[q\],where q = 1 - p. Nonprobabilistically, the prediction loop at position 0 would stopafter producing the states0 --* .S0S --* .aoS --+ .Sb.This would leave the forward probabilities ata0(0S~.a)  = pao(oS-*  .Sb) = q,corresponding to just two out of an infinity of possible paths.
The correct forwardprobabilities are obtained as a sum of infinitely many terms, accounting for all possiblepaths of length 1.ao(oS --~ .a)ao(oS -~ .Sb )= p+qp+q2p+ .
.
.
.
p(1 _ q)-I = 1= q+q2+q3+ .
.
.
.
q (1-q) - I  =p- lqIn these sums each p corresponds toa choice of the first production, each q to a choiceof the second production.
If we didn't care about finite computation the resultinggeometric series could be computed by letting the prediction loop (and hence thesummation) continue indefinitely.Fortunately, all repeated prediction steps, including those due to left-recursionin the productions, can be collapsed into a single, modified prediction step, and thecorresponding sums computed in closed form.
For this purpose we need a probabilisticversion of the well-known parsing concept of a left comer, which is also at the heartof the prefix probability algorithm of Jelinek and Lafferty (1991).Definition 5The following definitions are relative to a given SCFG G.a) Two nonterminals X and Y are said to be in a left-comer relationX --*L Y iff there exists a production for X that has a RHS starting with Y,X--* YA.177Computational Linguistics Volume 21, Number 2b) The probabilistic left-corner relation 1?
PL = PL(G) is the matrix ofprobabilities P(X --+L Y), defined as the total probability of choosing aproduction for X that has Y as a left corner:P(X--*L Y)= ~ P(X ~ YA).X--*YAcGc)d)The relation X GL Y is defined as the reflexive, transitive closure ofX ~L Y, i.e., X ~ Y iff X = Y or there is a nonterminal Z such thatX --*L Z and Z GL Y.The probabilistic reflexive, transitive left-corner relation RL = RL(G) isa matrix of probability sums R(X :GL Y).
Each R(X ~L Y) is defined as aseriese(x Y) e(x = Y)+ P(x Y)4- y'~ P(X -+L Z1)P(Z1  --+L Y)z1+ ~_, P(X ---~L Z1)P(Z1 ---+L Za)P(Z2 --+L Y)Z1 ,Z24 - .
.
.Alternatively, RL is defined by the recurrence relationR(X ~L Y) = 6(X, Y) + Z e(x  -~L Z)R(Z ~,~ Y),zwhere we use the delta function, defined as 6(X, Y) = 1 if X = Y, and6(X, Y) = 0 if X # Y.The recurrence for RL can be conveniently written in matrix notationRL = 1 4- PLRL,from which the closed-form solution is derived:RL = (I -- PL) -1.An existence proof for RL is given in Appendix A.
Appendix B.3.1 shows how to speedup the computation of RL by inverting only a reduced version of the matrix I - PL.The significance of the matrix RL for the Earley algorithm is that its elements arethe sums of the probabilities of the potentially infinitely many prediction paths leadingfrom a state kX --+ A.Z# to a predicted state iY --~ .~', via any number of intermediatestates.RL can be computed once for each grammar, and used for table-lookup in thefollowing, modified prediction step.10 If a probabilistic relation R is replaced by its set-theoretic version R I, i.e., (x,y) E R' iff R(x,y) ~ 0,then the closure operations used here reduce to their traditional discrete counterparts; hence the choiceof terminology.178Andreas Stolcke Efficient Probabilistic Context-Free ParsingPrediction (probabilistic, transitive).i: kX --+ A.Z# \[c~,'7\] ~ i: iW ~ .11 \[oJ, "7'\]for all productions Y --+ u such that R(Z GL Y) is nonzero.
Thenc~' += c~.R(Z dr  Y)P(Y---* u) (13)"7' = P(Y  ~ . )
(14)The new R(Z GL Y) factor in the updated forward probability accounts for thesum of all path probabilities linking Z to Y.
For Z = Y this covers the case of a singlestep of prediction; R(Y  ~L Y) _> 1 always, since RL is defined as a reflexive closure.4.5.2 Completion loops.
As in prediction, the completion step in the Earley algorithmmay imply an infinite summation, and could lead to an infinite loop if computednaively.
However, only unit productions 11 can give rise to cyclic completions.The problem is best explained by studying an example.
Consider the grammarS --ca \[p\]S T \[q\]T --.
S \[1\],where q = 1 - p. Presented with the input a (the only string the grammar generates),after one cycle of prediction, the Earley chart contains the following states.0:0 ~ .S ~=1,  '7=10:0S --* .T o~ = p- lq,  '7=q0:0T --+ .S c~ -~ p- lq,  '7--1O : oS --* .a o~ ~ p - tp= l, 7=P.The p-1 factors are a result of the left-corner sum 1 + q + q2 q_ .
.
.
.
(1 - q)-l.After scanning oS --* .a, completion without truncation would enter an infiniteloop.
First 0T ~ .S is completed, yielding a complete state 0T --* S., which allows 0S ---*.T to be completed, leading to another complete state for S, etc.
The nonprobabilisticEarley parser can just stop here, but as in prediction, this would lead to truncatedprobabilities.
The sum of probabilities that needs to be computed to arrive at the correctresult contains infinitely many terms, one for each possible loop through the T --+ Sproduction.
Each such loop adds a factor of q to the forward and inner probabilities.The summations for all completed states turn out as1:0S  ---+ c/.I :0T  --+ S.1:0 --+ S.I :0S ~ T.c~=1, '7=pc~ = p- lq(p + pq + pq2 +.
.
. )
= p- lq,  "7 ---- P + Pq + pq2 + .
.
.
.
1oL = p + pq + pq2 + .
.
.
.
1, ,7 = p + pq + pq2 + .
.
.
.
1c~=p- lq (p+pq+pq2 +.
.
.
)=p- lq ,  "7=q(p+pq+pq2 +.
.
.
)=q11 Unit productions are also called "chain productions" or "single productions" in the literature.179Computational Linguistics Volume 21, Number 2The approach taken here to compute exact probabilities in cyclic completions ismostly analogous to that for left-recursive predictions.
The main difference is that unitproductions, rather than left-corners, form the underlying transitive relation.
Beforeproceeding we can convince ourselves that this is indeed the only case we have toworry about.Lemma 4LetklX1 ~ -~1X2.
===~ k2X2 --+ /~2X3 ?
==~ " ' "  ~ kcXc --+/~cXc+l.be a completion cycle, i.e., kl = k?, X1 = Xc, ~1 --- )~c, X2 = Xc+I.
Then it must be thecase that ~1 = /~2 .
.
.
.
.
"~c = C, i.e., all productions involved are unit productionsX1 ~ X2,.
.
.
,  Xc ~ Xc+l.ProofFor all completion chains it is true that the start indices of the states are monotonicallyincreasing, kl ~ k2 ~ ... (a state can only complete an expansion that started atthe same or a previous position).
From kl ~- kc, it follows that kl = k2 .
.
.
.
.
kc.Because the current position (dot) also refers to the same input index in all states,all nonterminals X~,X2,...,Xc have been expanded into the same substring of theinput between kl and the current position.
By assumption the grammar contains nononterminals that generate ?,12 therefore we must have )~1 : ~2 .
.
.
.
.
)~c = e, q.e.d.\[\]We now formally define the relation between onterminals mediated by unit pro-ductions, analogous to the left-corner relation.Definition 6The following definitions are relative to a given SCFG G.a)b)c)d)Two nonterminals X and Y are said to be in a unit-production relationX ~ Y iff there exists a production for X that has Y as its RHS.The probabil istic unit-production relation Pu = Pu(G) is the matrix ofprobabilities P(X --+ Y).The relation X ~ Y is defined as the reflexive, transitive closure ofX ~ Y, i.e., X G Y iff X = Y or there is a nonterminal Z such that X --+ Zand Z ~ Y.The probabil istic reflexive, transitive unit-production relationRu = Ru(G) is the matrix of probability sums R(X ~ Y).
Each R(X ~ Y)is defined as a seriese(x Y) P(x  = Y)+ P(X --* Y)q- ~~ P(X --.4 Zl)P(Zl --4 Y)z112 Even w i th  nu l l  product ions ,  these wou ld  not  be used  for Ear ley transi t ions;  see Section 4.7.180Andreas Stolcke Efficient Probabilistic Context-Free Parsingq- ~ P(X --+ Z1)P(Z 1 ~ Z2)P(Z  2 -..+ Y)ZI,Z2~- .
, .~(x, Y) + ~P(x  -, z)R(z ~ Y).ZAs before, a matrix inversion can compute the relation Ru in closed form:Ru -- (I - Pu) -1.The existence of Ru is shown in Appendix A.The modified completion loop in the probabilistic Earley parser can now use theRu matrix to collapse all unit completions into a single step.
Note that we still haveto do iterative completion on non-unit productions.Completion (probabilistic, transitive).i: jY---* u.
\[ct',7"\] }j: kX--+ A.Z# \[oe, 7\] ~ i: kX--+ AZ.# \[o/,7'\]for all Y, Z such that R(Z ~ Y) is nonzero, and Y + t, is not a unit production (lu\] > 1or u c g).
Then~' += ~.7"e(zg  r)-~' += -r. 7"e(z ~, Y)4.6 An ExampleConsider the grammars --, a \[p\]s - ,  ss  \[q\]where q = 1 - p. This highly ambiguous grammar generates strings of any number ofa's, using all possible binary parse trees over the given number of terminals.
The statesinvolved in parsing the string aaa are listed in Table 2, along with their forward andinner probabilities.
The example illustrates how the parser deals with left-recursionand the merging of alternative sub-parses during completion.Since the grammar has only a single nonterminal, the left-corner matrix PL hasrank 1:PL = M-Its transitive closure isRL = ( I -  PL) -1 ~- ~\]-1 ~--- ~/9-1\].Consequently, the example trace shows the factor p-1 being introduced into the for-ward probability terms in the prediction steps.The sample string can be parsed as either (a(aa)) or ((aa)a), each parse having aprobability of p3q2.
The total string probability is thus 2p3q 2, the computed c~ and 7values for the final state.
The oe values for the scanned states in sets 1, 2, and 3 arethe prefix probabilities for a, aa, and aaa, respectively: P(S GL a) = 1, P(S :GL aa) = q,P(S :?L aaa) = (1 + p)q2.181Computat ional  Linguistics Volume 21, Number  2Tab le  2Earley chart as constructed ur ing the parse of aaa with the grammar in (a).
The two columnsto the right in (b) list the forward and inner probabilities, respectively, for each state.
In both c~and 3' columns, the ?
separates old factors from new ones (as per equations 11, 12 and 13).Addit ion indicates mult iple derivations of the same state.
(a)(b)S ----~ a \[p\]S ~ SS \[q\]State set 00 ~.S  1 1predictedoS ~ .a 1 .
p - lp  = 1 poS ~ .SS 1 ?
p - lq  = p - lq  qState set 1scannedoS --* a. p - lp  = 1 pcompletedoS --* S.S p - lq  .
p = q q .
p = pqpredicted1S ---* .a q.  p - lp  = q p1S ~ .SS q .
p - lq  = p- lq2 qState set 2scanned1S ---~ a. q pcompleted1S ~ S.S p - lq2 ,  p = q2 q.  q = pqoS ~ SS.
q .
p = pq pq .
p = p2qoS --* S.S p - lq  .
p2q = pq2 q .
p2q = p2q20 ~ S. 1 ?
p2q = p2q 1 ?
p2q = p2qpredicted2S ~ .a (q2 q_ pq2) .
p - lp  = (1 + p)q2 p2S ~ .SS (q2 q_ pq2) .
p - lq  = (1 + p-1)q3 qState set 3scanned2S ---* a.
(1 -t- p)q2 pcompleted2S ----+ S.S (1 + p-1)q3, p = (1 + p)q3 q .p  = pq1S ~ SS.
q2 .
p = pq2 pq .
p = p2qIS ---+ S.S p- lq2 .
paq = pq3 q .
p2q = p2q2oS --+ SS.
pq2 .
p + q .
p2q -= 2p2q2 p2q2 .
p + pq .
p2q .~_ 2p3q2oS --* S.S p - lq  .
2p3q2 = 2p2q3 q.
2p3q2 = 2p3q30 ---* S. 1 ?
2p3q 2 = 2p3q 2 1 ?
2p3q 2 = 2p3q 2182Andreas Stolcke Efficient Probabilistic Context-Free Parsing4.7 Null ProductionsNull productions X --* c introduce some complications into the relatively straight-forward parser operation described so far, some of which are due specifically to theprobabilistic aspects of parsing.
This section summarizes the necessary modificationsto process null productions correctly, using the previous description as a baseline.
Ourtreatment of null productions follows the (nonprobabilistic) formulation of Graham,Harrison, and Ruzzo (1980), rather than the original one in Earley (1970).4.7.1 Computing c-expansion probabilities.
The main problem with null productionsis that they allow multiple prediction-completion cycles in between scanning steps(since null productions do not have to be matched against one or more input symbols).Our strategy will be to collapse all predictions and completions due to chains of nullproductions into the regular prediction and completion steps, not unlike the wayrecursive predictions/completions were handled in Section 4.5.A prerequisite for this approach is to precompute, for all nonterminals X, the prob-ability that X expands to the empty string.
Note that this is another ecursive problem,since X itself may not have a null production, but expand to some nonterminal Y thatdoes.Computation of P(X  :~ c) for all X can be cast as a system of non-linear equations,as follows.
For each X, let ex be an abbreviation for P(X  G c).
For example, let X haveproductionsX ---* c \[Pl\]--* YI Y2 ~92\]---9 W3Y4Y5 ~/93\]The semantics of context-free rules imply that X can only expand to c if all the RHSnonterminals in one of X's productions expand to e. Translating to probabilities, weobtain the equationex -- Pl + p2eyley2 + p3ey3eY4eY5 + "" ?In other words, each production contributes a term in which the rule probability ismultiplied by the product of the e variables corresponding to the RHS nonterminals,unless the RHS contains a terminal (in which case the production contributes nothingto ex because it cannot possibly lead to e).The resulting nonlinear system can be solved by iterative approximation.
Eachvariable ex is initialized to P(X  ~ e), and then repeatedly updated by substituting inthe equation right-hand sides, until the desired level of accuracy is attained.
Conver-gence is guaranteed, since the ex values are monotonically increasing and boundedabove by the true values P(X  ~ e) ( 1.
For grammars without cyclic dependen-cies among e-producing nonterminals, this procedure degenerates to simple backwardsubstitution.
Obviously the system has to be solved only once for each grammar.The probability ex can be seen as the precomputed inner probability of an expan-sion of X to the empty string; i.e., it sums the probabilities of all Earley paths thatderive c from X.
This is the justification for the way these probabilities can be used inmodified prediction and completion steps, described next.4.7.2 Prediction with null productions.
Prediction is mediated by the left-corner re-lation.
For each X occurring to the right of a dot, we generate states for all Y that183Computational Linguistics Volume 21, Number 2are reachable from X by way of the X --*L Y relation.
This reachability criterion hasto be extended in the presence of null productions.
Specifically, if X has a productionX --+ Y1 ...Wi-lYi)~ then Yi is a left corner of X iff Y1, .
.
.
,Y i -1  all have a nonzeroprobability of expanding to e. The contribution of such a production to the left-cornerprobability P(X  --+L Yi) isi--1P(X  --* Y1.. .
Yi-lYi&) I I  eYkk=lThe old prediction procedure can now be modified in two steps.
First, replacethe old PL relation by the one that takes into account null productions, as sketchedabove.
From the resulting PL compute the reflexive transitive closure RL, and use it togenerate predictions as before.Second, when predicting a left corner Y with a production Y --* Y1 .
.
.
Yi-IYi)~, addstates for all dot positions up to the first RHS nonterminal that cannot expand to e,say from X --* .Y1 .
.
.
Yi - I  Yi )~ through X --* Y1 .
.
.
Y i - l .Y i  .X.
We will call this procedure"spontaneous dot shifting."
It accounts precisely for those derivations that expand theRHS prefix Y1 ... Wi-1 without consuming any of the input symbols.The forward and inner probabilities of the states thus created are those of thefirst state X --* .Y1.
.
.
Yi-lYi/~, multiplied by factors that account for the implied e-expansions.
This factor is just the product 1-I~=1 eYk, where j is the dot position.4.7.3 Completion with null productions.
Modification of the completion step followsa similar pattern.
First, the unit production relation has to be extended to allow forunit production chains due to null productions.
A rule X ~ Y1 .
.
.
Y i - lY iY i+l  .
.
.
Yj caneffectively act as a unit production that links X and Yi if all other nonterminals on theRHS can expand to e. Its contribution to the unit production relation P(X  ~ Yi) willthen beP(X  ~ Y1 .
.
.
Y i - lY iY i+ l .
.
.
Yj) I IeYkFrom the resulting revised Pu matrix we compute the closure Ru as usual.The second modification is another instance of spontaneous dot shifting.
Whencompleting a state X --+ )~.Y# and moving the dot to get X ~ )~Y.#, additional stateshave to be added, obtained by moving the dot further over any nonterminals in # thathave nonzero e-expansion probability.
As in prediction, forward and inner probabilitiesare multiplied by the corresponding e-expansion probabilities.4.7.4 Eliminating null productions.
Given these added complications one might con-sider simply eliminating all c-productions in a preprocessing step.
This is mostlystraightforward and analogous to the corresponding procedure for nonprobabilisticCFGs (Aho and Ullman 1972, Algorithm 2.10).
The main difference is the updating ofrule probabilities, for which the e-expansion probabilities are again needed...Delete all null productions, except on the start symbol (in case thegrammar as a whole produces cwith nonzero probability).
Scale theremaining production probabilities to sum to unity.For each original rule X ~ ,~Y# that contains a nonterminal Y such thatY~E:184Andreas Stolcke Efficient Probabilistic Context-Free Parsing(a)(b)(c)Create a variant rule X --* &#Set the rule probability of the new rule to eyP(X --, &Y#).
If therule X ~ ~# already exists, sum the probabilities.Decrement the old rule probability by the same amount.Iterate these steps for all RHS occurrences of a null-able nonterminal.The crucial step in this procedure is the addition of variants of the original produc-tions that simulate the null productions by deleting the corresponding nonterminalsfrom the RHS.
The spontaneous dot shifting described in the previous ections effec-tively performs the same operation on the fly as the rules are used in prediction andcompletion.4.8 Complexity IssuesThe probabilistic extension of Earley's parser preserves the original control structurein most aspects, the major exception being the collapsing of cyclic predictions and unitcompletions, which can only make these steps more efficient.
Therefore the complexityanalysis from Earley (1970) applies, and we only summarize the most important resultshere.The worst-case complexity for Earley's parser is dominated by the completion step,which takes O(/2) for each input position, I being the length of the current prefix.
Thetotal time is therefore O(/3) for an input of length l, which is also the complexity of thestandard Inside/Outside (Baker 1979) and LRI (Jelinek and Lafferty 1991) algorithms.For grammars of bounded ambiguity, the incremental per-word cost reduces to O(l),0(/2) total.
For deterministic CFGs the incremental cost is constant, 0(l) total.
Becauseof the possible start indices each state set can contain 0(l) Earley states, giving O(/2)worst-case space complexity overall.Apart from input length, complexity is also determined by grammar size.
Wewill not try to give a precise characterization in the case of sparse grammars (Ap-pendix B.3 gives some hints on how to implement the algorithm efficiently for suchgrammars).
However, for fully parameterized grammars in CNF we can verify thescaling of the algorithm in terms of the number of nonterminals n, and verify that ithas the same O(n 3) time and space requirements a the Inside/Outside (I/O) and LRIalgorithms.The completion step again dominates the computation, which has to computeprobabilities for at most O(n 3) states.
By organizing summations (11) and (12) so that3'" are first summed by LHS nonterminals, the entire completion operation can beaccomplished in 0(//3).
The one-time cost for the matrix inversions to compute theleft-corner and unit production relation matrices is also O(r/3).5.
ExtensionsThis section discusses extensions tothe Earley algorithm that go beyond simple parsingand the computation of prefix and string probabilities.
These extensions are all quitestraightforward and well supported by the original Earley chart structure, which leadsus to view them as part of a single, unified algorithm for solving the tasks mentionedin the introduction.185Computational Linguistics Volume 21, Number 25.1 Viterbi ParsesDefinition 7A Viterbi parse for a string x, in a grammar G, is a left-most derivation that assignsmaximal probability to x, among all possible derivations for x.Both the definition of Viterbi parse and its computation are straightforward gener-alizations of the corresponding otion for Hidden Markov Models (Rabiner and Juang1986), where one computes the Viterbi path (state sequence) through an HMM.
Pre-cisely the same approach can be used in the Earley parser, using the fact that eachderivation corresponds to a path.The standard computational technique for Viterbi parses is applicable here.
Wher-ever the original parsing procedure sums probabilities that correspond to alternativederivations of a grammatical entity, the summation is replaced by a maximization.Thus, during the forward pass each state must keep track of the maximal path prob-ability leading to it, as well as the predecessor states associated with that maximumprobability path.
Once the final state is reached, the maximum probability parse canbe recovered by tracing back the path of "best" predecessor states.The following modifications to the probabilistic Earley parser implement the for-ward phase of the Viterbi computation.?
Each state computes an additional probability, its Viterbi probability v.?
Viterbi probabilities are propagated in the same way as innerprobabilities, except hat during completion the summation is replacedby maximization: Vi(kX --* ,~Y.#) is the maximum of all productsvi( jW --+ 17.
)Vj(kX --+ ,~.Y#) that contribute to the completed statekX --* )~Y.#.
The same-position predecessor jY  -~ ~,.
associated with themaximum is recorded as the Viterbi path predecessor f kX --* ,~Y.# (theother predecessor state kX --* ,~.Y# can be inferred).?
The completion step uses the original recursion without collapsing unitproduction loops.
Loops are simply avoided, since they can only lower apath's probability.
Collapsing unit production completions has to beavoided to maintain a continuous chain of predecessors for laterbacktracing and parse construction.?
The prediction step does not need to be modified for the Viterbicomputation.Once the final state is reached, a recursive procedure can recover the parse treeassociated with the Viterbi parse.
This procedure takes an Earley state i : kX --* &.#as input and produces the Viterbi parse for the substring between k and i as output.
(If the input state is not complete (# ~ ?
), the result will be a partial parse tree withchildren missing from the root node.
)Viterbi parse (i : kX --* -~.#):1.
If )~ = ?, return a parse tree with root labeled X and no children.186Andreas Stolcke Efficient Probabilistic Context-Free Parsing.
Otherwise, if ~ ends in a terminal a, let A~a = ~, and call this procedurerecursively to obtain the parse treeT = Viterbi-parse(i - 1 : k X i+ ,,V.a~).Adjoin a leaf node labeled a as the right-most child to the root of T andreturn T.Otherwise, if A ends in a nonterminal Y, let A'Y = A.
Find the Viterbipredecessor state jW ---+ t~.
for the current state.
Call this procedurerecursively to computeT = Viterbi-parse(j : kX ~ A'.Y#)as well asT' = Viterbi-parse(i : jY --* u.
)Adjoin T ~ to T as the right-most child at the root, and return T.5.2 Rule Probability EstimationThe rule probabilities in a SCFG can be iteratively estimated using the EM (Expectation-Maximization) algorithm (Dempster et al 1977).
Given a sample corpus D, the esti-mation procedure finds a set of parameters that represent a local maximum of thegrammar likelihood function P(D I G), which is given by the product of the stringprobabilitiesP(D I C) = 1-\[ P(S x),xEDi.e., the samples are assumed to be distributed identically and independently.The two steps of this algorithm can be briefly characterized asfollows.E-step: Compute xpectations for how often each grammar rule is used, given thecorpus D and the current grammar parameters (rule probabilities).M-step: Reset the parameters so as to maximize the likelihood relative to theexpected rule counts found in the E-step.This procedure is iterated until the parameter values (as well as the likelihood) con-verge.
It can be shown that each round in the algorithm produces a likelihood that isat least as high as the previous one; the EM algorithm is therefore guaranteed to findat least a local maximum of the likelihood function.EM is a generalization f the well-known Baum-Welch algorithm for HMM esti-mation (Baum et al 1970); the original formulation for the case of SCFGs is attributableto Baker (1979).
For SCFGs, the E-step involves computing the expected number oftimes each production is applied in generating the training corpus.
After that, the M-step consists of a simple normalization of these counts to yield the new productionprobabilities.In this section we examine the computation of production count expectations re-quired for the E-step.
The crucial notion introduced by Baker (1979) for this purposeis the "outer probability" of a nonterminal, or the joint probability that the nonter-minal is generated with a given prefix and suffix of terminals.
Essentially the samemethod can be used in the Earley framework, after extending the definition of outerprobabilities to apply to arbitrary Earley states.187Computational Linguistics Volume 21, Number 2Definition 8Given a string x, Ix\] = l, the outer probability fli(kX ---* A.#) of an Earley state is thesum of the probabilities of all paths that.2.3.4.5.start with the initial state,generate the prefix Xo.. .
Xk-1,pass through k x ---4 .17#, for some u,generate the suffix xi.
?
?
x1-1 starting with state kX --* u.# ,end in the final state.Outer probabilities complement inner probabilities in that they refer precisely tothose parts of complete paths generating x not covered by the corresponding innerprobability 7i(kX --* A.#).
Therefore, the choice of the production X --* A# is not partof the outer probability associated with a state kX ~ A.#.
In fact, the definition makesno reference to the first part A of the RHS: all states haring the same k, X, and # willhave identical outer probabilities.Intuitively, fli(kX --* A.#) is the probability that an Earley parser operating as astring generator yields the prefix Xo...k-1 and the suffix xi...l_l, while passing throughstate kX --* A.# at position i (which is independent of A).
As was the case for forwardprobabilities, fl is actually an expectation of the number of such states in the path, asunit production cycles can result in multiple occurrences for a single state.
Again, wegloss over this technicality in our terminology.
The name is motivated by the fact thatfl reduces to the "outer probability" of X, as defined in Baker (1979), if the dot is infinal position.5.2.1 Computing expected production counts.
Before going into the details of com-puting outer probabilities, we describe their use in obtaining the expected rule countsneeded for the E-step in grammar estimation.Let c(X --* A \] x) denote the expected number of uses of production X --* A in thederivation of string x. Alternatively, c(X --* A \] x) is the expected number of times thatX --+ ), is used for prediction in a complete Earley path generating x.
Let c(X -~ A \] t 9)be the number of occurrences of predicted states based on production X ~ A along apath 79.c(x  lx) = Z P(791s x)c(X ;, 179)79 derives x1 - F_, p(79, s x)c(xP(S ~ x) 79 derives x_ 1P(S ~ Xo...i_lXl\] ~ x).P(S X) i:iX---+.A79)The last summation is over all predicted states based on production X --* A. Thequantity P(S  Xo...i_lXt, :~ x) is the sum of the probabilities of all paths passingthrough i : iX --* .A.
Inner and outer probabilities have been defined such that thisquantity is obtained precisely as the product of the corresponding of "Yi and fli.
Thus,188Andreas Stolcke Efficient Probabilistic Context-Free Parsingthe expected usage count for a rule can be computed as1c(X - .
~ Ix) -P(S X) i:iX--->.,,k9~(~x ~ .A),~(~x -~ .,~).The sum can be computed after completing both forward and backward passes (orduring the backward pass itself) by scanning the chart for predicted states.5.2.2 Computing outer probabilities.
The outer probabilities are computed by tracingthe complete paths from the final state to the start state, in a single backward pass overthe Earley chart.
Only completion and scanning steps need to be traced back.
Reversescanning leaves outer probabilities unchanged, so the only operation of concern isreverse completion.We describe reverse transitions using the same notation as for their forward coun-terparts, annotating each state with its outer and inner probabilities.Reverse completion.i: jY --~ 1,1.
\[fl",'y"\]i: kX-*  AY.# \[fl,"/\] ~ j :  kX--* A.Y# \[fl',7'\]for all pairs of states jY --+ t,.
and kX --* A.Y# in the chart.
Thenfl' += q / ' .
f lfl" += -y'.flThe inner probability 7 is not used.Rationale.
Relative to fl, fl' is missing the probability of expanding Y, which isfilled in from ~,".
The probability of the surrounding of Y(fl") is the probability of thesurrounding of X(fl), plus the choice of the rule of production for X and the expansionof the partial LHS A, which are together given by ~,'.Note that the computation makes use of the inner probabilities computed in theforward pass.
The particular way in which 3' and fl were defined turns out to beconvenient here, as no reference to the production probabilities themselves needs tobe made in the computation.As in the forward pass, simple reverse completion would not terminate in the pres-ence of cyclic unit productions.
A version that collapses all such chains of productionsis given below.Reverse completion (transitive).i: jY--* ~,.
\[fl",7"\]i: k x --* AZ.# \[fl, 3'\] ~ j kX ~ A.Z# \[fl', 7'\]for all pairs of states jY ---+ v. and kX --* A.Z# in the chart, such that the unit productionrelation R(Z  Y) is nonzero.
Thenfl' += q / ' .
f lfl" += -~'.
flR(Z G Y)The first summation is carried out once for each state j : kX --* MZ#, whereas thesecond summation is applied for each choice of Z, but only if X --* AZ# is not itself aunit production, i.e., A# ~ E.189Computational Linguistics Volume 21, Number 2Rationale.
This increments fl" the equivalent of R(Z ~ Y) times, accounting forthe infinity of surroundings in which Y can occur if it can be derived through cyclicproductions.
Note that the computation of tip is unchanged, since "y" already includesan infinity of cyclically generated subtrees for Y, where appropriate.5.3 Parsing Bracketed InputsThe estimation procedure described above (and EM-based estimators in general) areonly guaranteed to find locally optimal parameter estimates.
Unfortunately, it seemsthat in the case of unconstrained SCFG estimation local maxima present a very realproblem, and make success dependent on chance and initial conditions (Lari andYoung 1990).
Pereira nd Schabes (1992) showed that partially bracketed input samplescan alleviate the problem in certain cases.
The bracketing information constrains theparse of the inputs, and therefore the parameter stimates, teering it clear from someof the suboptimal solutions that could otherwise be found.An Earley parser can be minimally modified to take advantage ofbracketed stringsby invoking itself recursively when a left parenthesis  encountered.
The recursive in-stance of the parser is passed any predicted states at that position, processes the inputup to the matching right parenthesis, and hands complete states back to the invokinginstance.
This technique is efficient, as it never explicitly rejects parses not consistentwith the bracketing.
It is also convenient, as it leaves the basic parser operations,including the left-to-right processing and the probabilistic omputations, unchanged.For example, prefix probabilities conditioned on partial bracketings could be computedeasily this way.Parsing bracketed inputs is described in more detail in Stolcke (1993), where it isalso shown that bracketing ives the expected improved efficiency.
For example, themodified Earley parser processes fully bracketed inputs in linear time.5.4 Robust ParsingIn many applications ungrammatical input has to be dealt with in some way.
Tra-ditionally it has been seen as a drawback of top-down parsing algorithms uch asEarley's that they sacrifice "robustness," i.e., the ability to find partial parses in anungrammatical input, for the efficiency gained from top-down prediction (Magermanand Weir 1992).One approach to the problem is to build robustness into the grammar itself.
In thesimplest case one could add top-level productionsS --* XSwhere X can expand to any nonterminal, including an "unknown word" category.
Thisgrammar will cause the Earley parser to find all partial parses of substrings, effectivelybehaving like a bottom-up arser constructing the chart in left-to-right fashion.
Morerefined variations are possible: the top-level productions could be used to model whichphrasal categories ( entence fragments) can likely follow each other.
This probabilisticinformation can then be used in a pruning version of the Earley parser (Section 6.1)to arrive at a compromise between robust and expectation-driven parsing.An alternative method for making Earley parsing more robust is to modify theparser itself so as to accept arbitrary input and find all or a chosen subset of pos-sible substring parses.
In the case of Earley's parser there is a simple extension to190Andreas Stolcke Efficient Probabilistic Context-Free Parsingaccomplish just that, based on the notion of a wildcard statei : k --~ ~.
?,where the wildcard ?
stands for an arbitrary continuation of the RHS.During prediction, a wildcard to the left of the dot causes the chart to be seededwith dummy states --* .X for each phrasal category X of interest.
Conversely, a minimalmodification to the standard completion step allows the wildcard states to collect allabutting substring parses:i: jY--+ #.
}j :  k ~+ ,'~.
?
~ i :  k --* )~ Y.
?for all Y.
This way each partial parse will be represented by exactly one wildcard statein the final chart position.A detailed account of this technique is given in Stolcke (1993).
One advantageover the grammar-modifying approach is that it can be tailored to use various criteriaat runtime to decide which partial parses to follow.6.
Discussion6.1 Online PruningIn finite-state parsing (especially speech decoding) one often makes use of the forwardprobabilities for prun ing  partial parses before having seen the entire input.
Pruningis formally straightforward in Earley parsers: in each state set, rank states accordingto their ~ values, then remove those states with small probabilities compared to thecurrent best candidate, or simply those whose rank exceeds a given limit.
Notice thiswill not only omit certain parses, but will also underestimate he forward and innerprobabilities of the derivations that remain.
Pruning procedures have to be evaluatedempirically since they invariably sacrifice completeness and, in the case of the Viterbialgorithm, optimality of the result.While Earley-based on-line pruning awaits further study, there is reason to be-lieve the Earley framework has inherent advantages over strategies based only onbottom-up information (including so-called "over-the-top" parsers).
Context-free for-ward probabilities include all available probabilistic information (subject o assump-tions implicit in the SCFG formalism) available from an input prefix, whereas theusual inside probabilities do not take into account he nonterminal prior probabilitiesthat result from the top-down relation to the start state.
Using top-down constraintsdoes not necessarily mean sacrificing robustness, as discussed in Section 5.4.
On thecontrary, by using Earley-style parsing with a set of carefully designed and estimated"fault-tolerant" op-level productions, it should be possible to use probabilities to bet-ter advantage in robust parsing.
This approach is a subject of ongoing work, in thecontext of tight-coupling SCFGs with speech decoders (Jurafsky, Wooters, Segal, Stol-cke, Fosler, Tajchman, and Morgan 1995).6.2 Relation to Probabilistic LR ParsingOne of the major alternative context-free parsing paradigms besides Earley's algo-rithm is LR parsing (Aho and Ullman 1972).
A comparison of the two approaches,both in their probabilistic and nonprobabilistic aspects, is interesting and providesuseful insights.
The following remarks assume familiarity with both approaches.
We191Computational Linguistics Volume 21, Number 2sketch the fundamental relations, as well as the important tradeoffs between the twoframeworks.
13Like an Earley parser, LR parsing uses dotted productions, called items, to keeptrack of the progress of derivations as the input is processed.
The start indices are notpart of LR items: we may therefore use the term "item" to refer to both LR items andEarley states without start indices.
An Earley parser constructs sets of possible itemson the fly, by following all possible partial derivations.
An LR parser, on the otherhand, has access to a complete list of sets of possible items computed beforehand, and atruntime simply follows transitions between these sets.
The item sets are known as the"states" of the LR parser.
~4 A grammar is suitable for LR parsing if these transitions canbe performed eterministically b considering only the next input and the contentsof a shift-reduce stack.
Generalized LR parsing is an extension that allows paralleltracking of multiple state transitions and stack actions by using a graph-structuredstack (Tomita 1986).Probabilistic LR parsing (Wright 1990) is based on LR items augmented withcertain conditional probabilities.
Specifically, the probability p associated with an LRitem X --+ )~./z is, in our terminology, a normalized forward probability:i(x - -+  p=P(S ~L X0...i-1)'where the denominator is the probability of the current prefix, is LR item probabilities,are thus conditioned forward probabilities, and can be used to compute conditionalprobabilities of next words: P(xi I X0-..i-1) is the sum of the p's of all items having xito the right of the dot (extra work is required if the item corresponds to a "reduce"state, i.e., if the dot is in final position).Notice that the definition of p is independent of i as well as the start index ofthe corresponding Earley state.
Therefore, to ensure that item probabilities are correctindependent of input position, item sets would have to be constructed so that theirprobabilities are unique within each set.
However, this may be impossible given thatthe probabilities can take on infinitely many values and in general depend on the his-tory of the parse.
The solution used by Wright (1990) is to collapse items whose prob-abilities are within a small tolerance ~and are otherwise identical.
The same thresholdis used to simplify a number of other technical problems, e.g., left-corner probabilitiesare computed by iterated prediction, until the resulting changes in probabilities aresmaller than e. Subject o these approximations, then, a probabilistic LR parser cancompute prefix probabilities by multiplying successive conditional probabilities forthe words it sees.
16As an alternative to the computation of LR transition probabilities from a givenSCFG, one might instead estimate such probabilities directly from traces of parses13 Like Earley parsers, LR parsers can be built using various amounts of lookahead to make the operationof the parser (more) deterministic, and hence more efficient.
Only the case of zero-lookahead, LR(0), isconsidered here; the correspondence between LR(k) parsers and k-lookahead Earley parsers isdiscussed in the literature (Earley 1970; Aho and Ullman 1972).14 Once again, it is helpful to compare this to a closely related finite-state concept: the states of the LRparser correspond to sets of Earley states, similar to the way the states of a deterministic FSAcorrespond to sets of states of an equivalent nondeterministic FSA under the standard subsetconstruction.15 The identity of this expression with the item probabilities of Wright (1990) can be proved by inductionon the steps performed to compute the p's, as shown in Stolcke (1993).16 It is not clear what the numerical properties of this approximation are, e.g., how the errors willaccumulate over longer parses.192Andreas Stolcke Efficient Probabilistic Context-Free Parsingon a training corpus.
Because of the imprecise relationship between LR probabilitiesand SCFG probabilities, it is not clear if the model thus estimated corresponds to anyparticular SCFG in the usual sense.Briscoe and Carroll (1993) turn this incongruity into an advantage by using theLR parser as a probabilistic model in its own right, and show how LR probabilitiescan be extended to capture non--context-free contingencies.
The problem of capturingmore complex distributional constraints in natural anguage is clearly important, butwell beyond the scope of this article.
We simply remark that it should be possible todefine "interesting" nonstandard probabilities in terms of Earley parser actions so asto better model non-context-free phenomena.Apart from such considerations, the choice between LR methods and Earley pars-ing is a typical space-time tradeoff.
Even though an Earley parser runs with the samelinear time and space complexity as an LR parser on grammars of the appropriate LRclass, the constant factors involved will be much in favor of the LR parser, as almost allthe work has already been compiled into its transition and action table.
However, thesize of LR parser tables can be exponential in the size of the grammar (because of thenumber of potential item subsets).
Furthermore, if the generalized LR method is usedfor dealing with nondeterministic grammars (Tomita 1986) the runtime on arbitraryinputs may also grow exponentially.
The bottom line is that each application's needshave to be evaluated against he pros and cons of both approaches to find the bestsolution.
From a theoretical point of view, the Earley approach as the inherent appealof being the more general (and exact) solution to the computation of the various SCFGprobabilities.6.3 Other Related WorkThe literature on Earley-based probabilistic parsers is sparse, presumably because ofthe precedent set by the Inside/Outside algorithm, which is more naturally formulatedas a bottom-up algorithm.Both Nakagawa (1987) and P~seler (1988) use a nonprobabilistic Earley parser aug-mented with "word match" scoring.
Though not truly probabilistic, these algorithmsare similar to the Viterbi version described here, in that they find a parse that optimizesthe accumulated matching scores (without regard to rule probabilities).
Prediction andcompletion loops do not come into play since no precise inner or forward probabilitiesare computed.Magerman and Marcus (1991) are interested primarily in scoring functions to guidea parser efficiently to the most promising parses.
Earley-style top-down prediction isused only to suggest worthwhile parses, not to compute precise probabilities, whichthey argue would be an inappropriate metric for natural anguage parsing.Casacuberta and Vidal (1988) exhibit an Earley parser that processes weighted (notnecessarily probabilistic) CFGs and performs a computation that is isomorphic to thatof inside probabilities hown here.
Schabes (1991) adds both inner and outer proba-bilities to Earley's algorithm, with the purpose of obtaining a generalized estimationalgorithm for SCFGs.
Both of these approaches are restricted to grammars withoutunbounded ambiguities, which can arise from unit or null productions.Dan Jurafsky (personal communication) wrote an Earley parser for the Berke-ley Restaurant Project (BeRP) speech understanding system that originally computedforward probabilities for restricted grammars (without left-corner or unit productionrecursion).
The parser now uses the method described here to provide exact SCFG pre-fix and next-word probabilities to a tightly coupled speech decoder (Jurafsky, Wooters,Segal, Stolcke, Fosler, Tajchman, and Morgan 1995).193Computational Linguistics Volume 21, Number 2An essential idea in the probabilistic formulation of Earley's algorithm is thecollapsing of recursive predictions and unit completion chains, replacing both withlookups in precomputed matrices.
This idea arises in our formulation out of the needto compute probability sums given as infinite series.
Graham, Harrison, and Ruzzo(1980) use a nonprobabilistic version of the same technique to create a highly opti-mized Earley-like parser for general CFGs that implements prediction and completionby operations on Boolean matrices.
~7The matrix inversion method for dealing with left-recursive prediction is borrowedfrom the LRI algorithm of Jelinek and Lafferty (1991) for computing prefix probabilitiesfor SCFGs in CNF) s We then use that idea a second time to deal with the similarrecursion arising from unit productions in the completion step.
We suspect, but havenot proved, that the Earley computation of forward probabilities when applied to aCNF grammar performs a computation that is isomorphic to that of the LRI algorithm.In any case, we believe that the parser-oriented view afforded by the Earley frameworkmakes for a very intuitive solution to the prefix probability problem, with the addedadvantage that it is not restricted to CNF grammars.Algorithms for probabilistic CFGs can be broadly characterized along several di-mensions.
One such dimension is whether the quantities entered into the parser chartare defined in a bottom-up (CYK) fashion, or whether left-to-right constraints are aninherent part of their definition) 9The probabilistic Earley parser shares the inherentleft-to-right character of the LRI algorithm, and contrasts with the bottom-up I /Oalgorithm.Probabilistic parsing algorithms may also be classified as to whether they are for-mulated for fully parameterized CNF grammars or arbitrary context-free rules (typ-ically taking advantage of grammar sparseness).
In this respect he Earley approachcontrasts with both the CNF-oriented I /O and LRI algorithms.
Another approach toavoiding the CNF constraint is a formulation based on probabilistic Recursive Tran-sition Networks (RTNs) (Kupiec 1992).
The similarity goes further, as both Kupiec'sand our approach is based on state transitions, and dotted productions (Earley states)turn out to be equivalent to RTN states if the RTN is constructed from a CFG.7.
Conc lus ionsWe have presented an Earley-based parser for stochastic ontext-free grammars thatis appealing for its combination of advantages over existing methods.
Earley's controlstructure lets the algorithm run with best-known complexity on a number of gram-mar subclasses, and no worse than standard bottom-up robabilistic hart parsers ongeneral SCFGs and fully parameterized CNF grammars.Unlike bottom-up arsers it also computes accurate prefix probabilities incremen-tally while scanning its input, along with the usual substring (inside) probabilities.
Thechart constructed uring parsing supports both Viterbi parse extraction and Baum-Welch type rule probability estimation by way of a backward pass over the parserchart.
If the input comes with (partial) bracketing to indicate phrase structure, this17 This connection to the GHR algorithm was pointed out by Fernando Pereira.
Exploration of this linkthen led to the extension of our algorithm to handle e-productions, asdescribed in Section 4.7.18 Their method uses the transitive (but not reflexive) closure over the left-corner relation PL, for whichthey chose the symbol QL.
We chose the symbol RL in this article to point to this difference.19 Of course a CYK-style parser can operate left-to-right, right-to-left, or otherwise by reordering thecomputation of chart entries.194Andreas Stolcke Efficient Probabilistic Context-Free Parsinginformation can be easily incorporated to restrict he allowable parses.
A simple ex-tension of the Earley chart allows finding partial parses of ungrammatical input.The computation of probabilities i conceptually simple, and follows directly Ear-ley's parsing framework, while drawing heavily on the analogy to finite-state languagemodels.
It does not require rewriting the grammar into normal form.
Thus, the presentalgorithm fills a gap in the existing array of algorithms for SCFGs, efficiently combin-ing the functionalities and advantages of several previous approaches.Appendix A: Existence of RL and RuIn Section 4.5 we defined the probabilistic left-corner and unit-production matrices RLand Ru, respectively, to collapse recursions in the prediction and completion steps.
Itwas shown how these matrices could be obtained as the result of matrix inversions.In this appendix we give a proof that the existence of these inverses is assured if thegrammar is well-defined in the following three senses.
The terminology used here istaken from Booth and Thompson (1973).Definition 9For an SCFG G over an alphabet G, with start symbol S, we say that 2?a)b)c)G is proper iff for all nonterminals X the rule" probabilities sum to unity,i.e.,~:(X--*~)EGG is consistent iff it defines a probability distribution over finite strings,i.e.,P(S xl = 1,xE~*where P(S ~ x) is induced by the rule probabilities according toDefinition l(a).G has no useless nonterminals iff all nonterminals X appear in at leastone derivation of some string x c G* with nonzero probability, i.e.,P(S :~ AX# =~ x) > O.It is useful to translate consistency into "process" terms.
We can view an SCFG asa stochastic string-rewriting process, in which each step consists of simultaneously re-placing all nonterminals in a sentential form with the right-hand sides of productions,randomly drawn according to the rule probabilities.
Booth and Thompson (1973) showthat the grammar is consistent if and only if the probability that stochastic rewritingof the start symbol S leaves nonterminals remaining after n steps, goes to 0 as n ~ ~.More loosely speaking, rewriting S has to terminate after a finite number of steps withprobability 1, or else the grammar is inconsistent.20 Unfortunately, the terminology used in the literature is not uniform.
For example, Jelinek and Lafferty(1991) use the term "proper" to mean (c), and "well-defined" for (b).
They also state mistakenly that (a)and (c) together are a sufficient condition for (b).
Booth and Thompson (1973) show that one can writea SCFG that satisfies (a) and (c) but generates derivations that do not terminate with probability 1, andgive necessary and sufficient conditions for (b).195Computational Linguistics Volume 21, Number 2We observe that the same property holds not only for S, but for all nontermi-nals, if the grammar has no useless terminals.
If any nonterminal X admitted infinitederivations with nonzero probability, then S itself would have such derivations, inceby assumption X is reachable from S with nonzero probability.To prove the existence of RL and Ru, it is sufficient to show that the correspondinggeometric series converge:RL = I + PL + p2 + .
.
.
.
( I -  PL) -1Ru = I + Pu + p2 + .
.
.
.
(I - Pu) -1.Lemma 5If G is a proper, consistent SCFG without useless nonterminals, then the powers P~of the left-corner relation, and P~/of the unit production relation, converge to zero asH ---+ OO.ProofEntry (X, Y) in the left-corner matrix PL is the probability Of generating Y as theimmediately succeeding left-corner below X.
Similarly, entry (X, Y) in the nth powerP~ is the probability of generating Y as the left-corner of X with n - 1 intermediatenonterminals.
Certainly P~(X, Y) is bounded above by the probability that the entirederivation starting at X terminates after n steps, since a derivation couldn't erminatewithout expanding the left-most symbol to a terminal (as opposed to a nonterminal).But that probability tends to 0 as n -+ oo, and hence so must each entry in P~.For the unit production matrix Pu a similar argument applies, since the length ofa derivation is at least as long as it takes to terminate any initial unit production chain.Lemma 6If G is a proper, consistent SCFG without useless nonterminals, then the series for RLand Rt/as defined above converge to finite, non-negative values.ProofP~ converging to 0 implies that the magnitude of PL'S largest eigenvalue (its spectralradius) is < 1, which in turn implies that the series Y~-~0 P\[ converges (similarly forPt/).
The elements of RL and Ru are non-negative since they are the result of addingand multiplying among the non-negative elements of PL and Pu, respectively.Interestingly, a SCFG may be inconsistent and still have converging left-cornerand/or unit production matrices, i.e., consistency is a stronger constraint.
For examples - .aS SS \[q\]is inconsistent for any choice of q > 1, but the left-corner relation (a single numberin this case) is well defined for all q < 1, namely (1 - q)-I = p-1.
In this case the leftfringe of the derivation is guaranteed to result in a terminal after finitely many steps,but the derivation as a whole may never terminate.196Andreas Stolcke Efficient Probabilistic Context-Free ParsingAppendix B: Implementation NotesThis appendix discusses some of the experiences gained from implementing the prob-abilistic Earley parser.B.1 PredictionBecause of the collapse of transitive predictions, this step can be implemented in a veryefficient and straightforward manner.
As explained in Section 4.5, one has to performa single pass over the current state set, identifying all nonterminals Z occurring to theright of dots, and add states corresponding to all productions Y --* u that are reachablethrough the left-corner relation Z ~L Y- As indicated in equation (13), contributionsto the forward probabilities of new states have to be summed when several paths leadto the same state.
However, the summation i  equation (13) can be optimized if thec~ values for all old states with the same nonterminal Z are summed first, and thenmultiplied by R(Z GL Y).
These quantities are then summed over all nonterminals Z,and the result is once multiplied by the rule probability P(Y ~ u) to give the forwardprobability for the predicted state.B.2 CompletionUnlike prediction, the completion step still involves iteration.
Each complete state de-rived by completion can potentially feed other completions.
An important detail hereis to ensure that all contributions to a state's c~ and ~/are summed before proceedingwith using that state as input to further completion steps.One approach to this problem is to insert complete states into a prioritized queue.The queue orders states by their start indices, highest first.
This is because statescorresponding to later expansions always have to be completed first before they canlead to the completion of expansions earlier on in the derivation.
For each start index,the entries are managed as a first-in, first-out queue, ensuring that the dependencygraph formed by the states is traversed in breadth-first order.The completion pass can now be implemented as follows.
Initially, all completestates from the previous canning step are inserted in the queue.
States are then re-moved from the front of the queue and used to complete other states.
Among the newstates thus produced, complete ones are again added to the queue.
The process iteratesuntil no more states remain in the queue.
Because the computation ofprobabilities al-ready includes chains of unit productions, tates derived from such productions neednot be queued, which also ensures that the iteration terminates.A similar queuing scheme, with the start index order reversed, can be used for thereverse completion step needed in the computation of outer probabilities (Section 5.2).B.3 Efficient Parsing with Large Sparse GrammarsDuring work with a moderate-sized, application-specific natural anguage grammartaken from the BeRP speech system (Jurafsky, Wooters, Tajchman, Segal, Stolcke, Foster,and Morgan 1994) we had an opportunity to optimize our implementation f thealgorithm.
Below we relate some of the lessons learned in the process.B.3.1 Speeding up matrix inversions.
Both prediction and completion steps make useof a matrix R defined as a geometric series derived from a matrix P,R=I+p+p2+ .
.
.
.
( i _p )  -1.197Computational Linguistics Volume 21, Number 2Both P and R are indexed by the nonterminals in the grammar.
The matrix P is de-rived from the SCFG rules and probabilities (either the left-corner relation or the unitproduction relation).For an application using a fixed grammar the time taken by the precomputationof left-corner and unit production matrices may not be crucial, since it occurs off-line.
There are cases, however, when that cost should be minimized, e.g., when ruleprobabilities are iteratively reestimated.Even if the matrix P is sparse, the matrix inversion can be prohibitive for largenumbers of nonterminals n. Empirically, matrices of rank n with a bounded numberp of nonzero entries in each row (i.e., p is independent of n) can be inverted in timeO(n2), whereas a full matrix of size n x n would require time O(n3).In many cases the grammar has a relatively small number of nonterminals thathave productions involving other nonterminals in a left-corner (or the RHS of a unitproduction).
Only those nonterminals can have nonzero contributions to the higherpowers of the matrix P. This fact can be used to substantially reduce the cost of thematrix inversion needed to compute R.Let P' be a subset of the entries of P, namely, only those elements indexed by non-terminals that have a nonempty row in P. For example, for the left-corner computation,P' is obtained from P by deleting all rows and columns indexed by nonterminals thatdo not have productions tarting with nonterminals.
Let I' be the identity matrix overthe same set of nonterminals as P'.
Then R can be computed asR = I+( I+p+p2+.
.
.
)P= I + (I' + P' + p,2 +.
.
. )
,  p= I+( I ' -P ' ) - I , P= I+R' ,P .Here R' is the inverse of I' - P', and * denotes a matrix multiplication in which the leftoperand is first augmented with zero elements to match the dimensions of the rightoperand, P.The speedups obtained with this technique can be substantial.
For a grammarwith 789 nonterminals, of which only 132 have nonterminal productions, the left-corner matrix was computed in 12 seconds (including the final multiply with P andaddition of/) .
Inversion of the full matrix I - P took 4 minutes, 28 seconds.
21B.3.2 Linking and bottom-up filtering.
As discussed in Section 4.8, the worst-caserun-time on fully parameterized CNF grammars is dominated by the completion step.However, this is not necessarily true of sparse grammars.
Our experiments showed thatthe computation is dominated by the generation of Earley states during the predictionsteps.It is therefore worthwhile to minimize the total number of predicted states gen-erated by the parser.
Since predicted states only affect the derivation if they lead tosubsequent scanning, we can use the next input symbol to constrain the relevant pre-dictions.
To this end, we compute the extended left-corner relation RLT, indicatingwhich terminals can appear as left corners of which nonterminals.
RLT is a Boolean21 These figures are not very meaningful for their absolute values.
All measurements were obtained on aSun SPARCstation 2 with a CommonLisp/CLOS implementation f generic sparse matrices that wasnot particularly optimized for this task.198Andreas Stolcke Efficient Probabilistic Context-Free Parsingmatrix with rows indexed by nonterminals and columns indexed by terminals.
It canbe computed as the productRLT = RLPLTwhere PLT has a nonzero entry at (X, a) iff there is a production for nonterminal X thatstarts with terminal a. RL is the old left-corner relation.During the prediction step we can ignore incoming states whose RHS nonterminalfollowing the dot cannot have the current input as a left-corner, and then eliminatefrom the remaining predictions all those whose LHS cannot produce the current inputas a left-corner.
These filtering steps are very fast, as they involve only table lookup.This technique for speeding up Earley prediction is the exact converse of the"linking" method described by Pereira and Shieber (1987, chapter 6) for improvingthe efficiency of bottom-up arsers.
There, the extended left-corner elation is usedfor top-down filtering the bottom-up application of grammar ules.
In our case, we uselinking to provide bottom-up filtering for top-down application of productions.On a test corpus this technique cut the number of generated predictions to al-most one-fourth and speeded up parsing by a factor of 3.3.
The corpus consisted of1,143 sentence with an average length of 4.65 words.
The top-down prediction alonegenerated 991,781 states and parsed at a rate of 590 milliseconds per sentence.
Withbottom-up filtered prediction only 262,287 states were generated, resulting in 180 mil-liseconds per sentence.AcknowledgmentsThanks are due Dan Jurafsky and SteveOmohundro for extensive discussions onthe topics in this paper, and FernandoPereira for helpful advice and pointers.Jerry Feldman, Terry Regier, Jonathan Segal,Kevin Thompson, and the anonymousreviewers provided valuable comments forimproving content and presentation.ReferencesAho, Alfred V., and Ullman, Jeffrey D.(1972).
The Theory of Parsing, Translation,and Compiling, Volume 1: Parsing.Prentice-Hall.Bahl, Lalit R.; Jelinek, Frederick; and Mercer,Robert L. (1983).
"A maximum likelihoodapproach to continuous speechrecognition."
IEEE Transactions on PatternAnalysis and Machine Intelligence, 5(2),179-190.Baker, James K. (1979).
"Trainable grammarsfor speech recognition."
In SpeechCommunication Papers for the 97th Meeting ofthe Acoustical Society of America, edited byJared J. Wolf and Dennis H. Klatt,547-550.Baum, Leonard E.; Petrie, Ted; Soules,George; and Weiss, Norman (1970).
"Amaximization technique occuring in thestatistical nalysis of probabilisticfunctions in Markov chains."
The Annals ofMathematical Statistics, 41(1), 164-171.Booth, Taylor L., and Thompson, Richard A.(1973).
"Applying probability measures toabstract languages."
IEEE Transactions onComputers, C-22(5), 442-450.Briscoe, Ted, and Carroll, John (1993).
"Generalized probabilistic LR parsing ofnatural language (corpora) withunification-based grammars.
"Computational Linguistics, 19(1), 25-59.Casacuberta, E, and Vidal, E. (1988).
"Aparsing algorithm for weighted grammarsand substring recognition."
In Syntacticand Structural Pattern Recognition, VolumeF45, NATO ASI Series, edited by GabrielFerrat6, Theo Pavlidis, Alberto Sanfeliu,and Horst Bunke, 51-67.
Springer Verlag.Corazza, Anna; De Mori, Renato; Gretter,Roberto; and Satta, Giorgio (1991).
"Computation of probabilities for anisland-driven parser."
IEEE Transactions onPattern Analysis and Machine Intelligence,13(9), 936-950.Dempster, A. P.; Laird, N. M.; and Rubin,D.
B.
(1977).
"Maximum likelihood fromincomplete data via the EM algorithm.
"Journal of the Royal Statistical Society, SeriesB, 34, 1-38.Earley, Jay (1970).
"An efficient context-freeparsing algorithm."
Communications of theACM, 6(8), 451-455.Fujisaki, T.; Jelinek, F.; Cocke, J.; Black, E.;and Nishino, T. (1991).
"A probabilisticparsing method for sentencedisambiguation."
In Current Issues inParsing Technology, edited by MasaruTomita, 139-152.
Kluwer Academic199Computational Linguistics Volume 21, Number 2Publishers.Graham, Susan L.; Harrison, Michael A.;and Ruzzo, Walter L. (1980).
"Animproved context-free r cognizer."
ACMTransactions on Programming Languages andSystems, 2(3), 415-462.Jelinek, Frederick (1985).
"Markov sourcemodeling of text generation."
In TheImpact of Processing Techniques onCommunications, Volume E91, NATO ASISeries, edited by J. K. Skwirzynski,569-598.
Nijhoff.Jelinek, Frederick, and Lafferty, John D.(1991).
"Computation of the probability ofinitial substring eneration by stochasticcontext-free grammars."
ComputationalLinguistics, 17(3), 315-323.Jelinek, Frederick; Lafferty, John D.; andMercer, Robert L. (1992).
"Basic methodsof probabilistic context free grammars.
"In Speech Recognition and Understanding.Recent Advances, Trends, and Applications,Volume F75, NATO ASI Series, edited byPietro Laface and Renato De Mori,345-360.
Springer Verlag.Jones, Mark A., and Eisner, Jason M.
(1992).
"A probabilistic parser and itsapplications."
In AAAI Workshop onStatistically~Based NLP Techniques, 20-27.Jurafsky, Daniel; Wooters, Chuck; Segal,Jonathan; Stolcke, Andreas; Fosler, Eric;Tajchman, Gary; and Morgan, Nelson(1995).
"Using a stochastic context-freegrammar as a language model for speechrecognition."
In Proceedings, IEEEConference on Acoustics, Speech and SignalProcessing, 189-192.
Detroit, Michigan.Jurafsky, Daniel; Wooters, Chuck; Tajchman,Gary; Segal, Jonathan; Stolcke, Andreas;Fosler, Eric; and Morgan, Nelson (1994).
"The Berkeley restaurant project."
InProceedings, International Conference onSpoken Language Processing, 4, 2139-2142.Yokohama, Japan.Kupiec, Julian (1992).
"Hidden Markovestimation for unrestricted stochasticcontext-free grammars."
In Proceedings,IEEE Conference on Acoustics, Speech andSignal Processing, 1, 177-180, SanFrancisco, California.Lari, K., and Young, S. J.
(1990).
"Theestimation of stochastic context-freegrammars using the Inside-Outsidealgorithm."
Computer Speech and Language,4, 35-56.Lari, K., and Young, S. J.
(1991).
"Applications of stochastic context-freegrammars using the Inside-Outsidealgorithm."
Computer Speech and Language,5, 237-257.Magerman, David M., and Marcus,Mitchell P. (1991).
"Pearl: A probabilisticchart parser."
In Proceedings, SecondInternational Workshop on ParsingTechnologies.
Cancun, Mexico, 193-199.Magerman, David M., and Weir, Carl (1992).
"Efficiency, robustness and accuracy inPicky chart parsing."
In Proceedings, 30thAnnual Meeting of the Association forComputational Linguistics.
Newark,Delaware, 40-47.Nakagawa, Sei-ichi (1987).
"Spokensentence recognition by time-synchronousparsing algorithm of context-freegrammar."
In Proceedings, IEEE Conferenceon Acoustics, Speech and Signal Processing, 2,829-832.
Dallas, Texas.Ney, Hermann (1992).
"Stochastic grammarsand pattern recognition."
In SpeechRecognition and Understanding.
RecentAdvances, Trends, and Applications, volumeF75 of NATO ASI Series, edited by PietroLaface and Renato De Mori, 319-344.Springer Verlag.P~iseler, Annedore (1988).
"Modification ofEarley's algorithm for speechrecognition/' In Recent Advances in SpeechUnderstanding and Dialog Systems, volumeF46 of NATO ASI Series, edited byH.
Niemann, M. Lang, and G. Sagerer,466-472.
Springer Verlag.Pereira, Fernando C. N., and Schabes, Yves(1992).
"Inside-outside r estimation frompartially bracketed corpora."
InProceedings, 30th Annual Meeting of theAssociation for Computational Linguistics.Newark, Delaware, 128-135.Pereira, Fernando C. N., and Shieber,Stuart M. (1987).
Prolog andNatural-Language Analysis.
CLSI LectureNotes Series, Number 10.
Center for theStudy of Language and Information,Stanford, California.Rabiner, L. R., and Juang, B. H. (1986).
"Anintroduction to hidden Markov models.
"IEEE ASSP Magazine, 3(1), 4-16.Schabes, Yves (1991).
"An inside-outsidealgorithm for estimating the parametersof a hidden stochastic context-freegrammar based on Earley's algorithm.
"Paper presented at the Second Workshopon Mathematics of Language, Tarrytown,New York.Stolcke, Andreas, and Segal, Jonathan(1994).
"Precise n-gram probabilities fromstochastic context-free grammars."
InProceedings, 3 lth Annual Meeting of theAssociation for Computational Linguistics.Las Cruces, New Mexico, 74-79.Stolcke, Andreas (1993).
"An efficientprobabilistic ontext-free parsingalgorithm that computes prefix200Andreas Stolcke Efficient Probabilistic Context-Free Parsingprobabilities."
Technical ReportTR-93-065, International ComputerScience Institute, Berkeley, CA.
Revised1994.Tomita, Masaru (1986).
Efficient Parsing forNatural Language.
Kluwer AcademicPublishers.Wright, J. H. (1990).
"LR parsing ofprobabilistic grammars with inputuncertainty for speech recognition.
"Computer Speech and Language, 4 297-323.201
