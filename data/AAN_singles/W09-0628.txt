Proceedings of the 12th European Workshop on Natural Language Generation, pages 165?173,Athens, Greece, 30 ?
31 March 2009. c?2009 Association for Computational LinguisticsReport on the First NLG Challenge onGenerating Instructions in Virtual Environments (GIVE)Donna ByronNortheastern Universitydbyron@ccs.neu.eduAlexander KollerSaarland Universitykoller@mmci.uni-saarland.deKristina StriegnitzUnion Collegestriegnk@union.eduJustine CassellNorthwestern Universityjustine@northwestern.eduRobert DaleMacquarie UniversityRobert.Dale@mq.edu.auJohanna MooreUniversity of EdinburghJ.Moore@ed.ac.ukJon OberlanderUniversity of EdinburghJ.Oberlander@ed.ac.ukAbstractWe describe the first installment of theChallenge on Generating Instructions inVirtual Environments (GIVE), a newshared task for the NLG community.
Wemotivate the design of the challenge, de-scribe how we carried it out, and discussthe results of the system evaluation.1 IntroductionThis paper reports on the methodology and resultsof the First Challenge on Generating Instructionsin Virtual Environments (GIVE-1), which we ranfrom March 2008 to February 2009.
GIVE is anew shared task for the NLG community.
It pro-vides an end-to-end evaluation methodology forNLG systems that generate instructions which aremeant to help a user solve a treasure-hunt task in avirtual 3D world.
The most innovative aspect froman NLG evaluation perspective is that the NLGsystem and the user are connected over the Inter-net.
This makes it possible to cheaply collect largeamounts of evaluation data.Five NLG systems were evaluated in GIVE-1 over a period of three months from November2008 to February 2009.
During this time, wecollected 1143 games that were played by usersfrom 48 countries.
As far as we know, this makesGIVE-1 the largest evaluation effort in terms ofexperimental subjects ever.
We have evaluated thefive systems both on objective measures (successrate, completion time, etc.)
and subjective mea-sures which were collected by asking the users tofill in a questionnaire.GIVE-1 was intended as a pilot experiment inorder to establish the validity of the evaluationmethodology and understand the challenges in-volved in the instruction-giving task.
We believethat we have achieved these purposes.
At the sametime, we provide evaluation results for the fiveNLG systems which will help their developers im-prove them for participation in a future challenge,GIVE-2.
GIVE-2 will retain the successful aspectsof GIVE-1, while refining the task to emphasizeaspects that we found to be challenging.
We invitethe ENLG community to participate in designingGIVE-2.Plan of the paper.
The paper is structured asfollows.
In Section 2, we will describe and moti-vate the GIVE Challenge.
In Section 3, we willthen describe the evaluation method and infras-tructure for the challenge.
Section 4 reports onthe evaluation results.
Finally, we conclude anddiscuss future work in Section 5.2 The GIVE ChallengeIn the GIVE scenario, subjects try to solve a trea-sure hunt in a virtual 3D world that they have notseen before.
The computer has a complete sym-bolic representation of the virtual world.
The chal-lenge for the NLG system is to generate, in realtime, natural-language instructions that will guidethe users to the successful completion of their task.Users participating in the GIVE evaluationstart the 3D game from our website at www.give-challenge.org.
They then see a 3Dgame window as in Fig.
1, which displays instruc-tions and allows them to move around in the worldand manipulate objects.
The first room is a tuto-rial room where users learn how to interact withthe system; they then enter one of three evaluationworlds, where instructions for solving the treasurehunt are generated by an NLG system.
Users caneither finish a game successfully, lose it by trig-gering an alarm, or cancel the game.
This result isstored in a database for later analysis, along with acomplete log of the game.Complete maps of the game worlds used in theevaluation are shown in Figs.
3?5: In these worlds,players must pick up a trophy, which is in a wallsafe behind a picture.
In order to access the tro-165Figure 1: What the user sees when playing withthe GIVE Challenge.phy, they must first push a button to move the pic-ture to the side, and then push another sequence ofbuttons to open the safe.
One floor tile is alarmed,and players lose the game if they step on this tilewithout deactivating the alarm first.
There are alsoa number of distractor buttons which either donothing when pressed or set off an alarm.
Thesedistractor buttons are intended to make the gameharder and, more importantly, to require appropri-ate reference to objects in the game world.
Finally,game worlds contained a number of objects suchas chairs and flowers that did not bear on the task,but were available for use as landmarks in spatialdescriptions generated by the NLG systems.2.1 Why a new NLG evaluation paradigm?The GIVE Challenge addresses a need for a newevaluation paradigm for natural language gener-ation (NLG).
NLG systems are notoriously hardto evaluate.
On the one hand, simply compar-ing system outputs to a gold standard using auto-matic comparison algorithms has limited value be-cause there can be multiple generated outputs thatare equally good.
Finding metrics that accountfor this variability and produce results consistentwith human judgments and task performance mea-sures is difficult (Belz and Gatt, 2008; Stent etal., 2005; Foster, 2008).
Human assessments ofsystem outputs are preferred, but lab-based eval-uations that allow human subjects to assess eachaspect of the system?s functionality are expensiveand time-consuming, thereby favoring larger labswith adequate resources to conduct human sub-jects studies.
Human assessment studies are alsodifficult to replicate across sites, so system devel-opers that are geographically separated find it dif-ficult to compare different approaches to the sameproblem, which in turn leads to an overall diffi-culty in measuring progress in the field.The GIVE-1 evaluation was conducted via aclient/server architecture which allows any userwith an Internet connection to provide systemevaluation data.
Internet-based studies have beenshown to provide generous amounts of data inother areas of AI (von Ahn and Dabbish, 2004;Orkin and Roy, 2007).
Our implementation allowssmaller teams to develop a system that will partici-pate in the challenge, without taking on the burdenof running the human evaluation experiment, andit provides a direct comparison of all participatingsystems on the same evaluation data.2.2 Why study instruction-giving?Next to the Internet-based data collection method,GIVE also differs from other NLG challenges byits emphasis on generating instructions in a vir-tual environment and in real time.
This focus oninstruction giving is motivated by a growing in-terest in dialogue-based agents for situated taskssuch as navigation and 3D animations.
Due to itsappeal to younger students, the task can also beused as a pedagogical exercise to stimulate interestamong secondary-school students in the researchchallenges found in NLG or Computational Lin-guistics more broadly.Embedding the NLG task in a virtual world en-courages the participating research teams to con-sider communication in a situated setting.
Thismakes the NLG task quite different than in otherNLG challenges.
For example, experiments haveshown that human instruction givers make the in-struction follower move to a different location inorder to use a simpler referring expression (RE)(Stoia et al, 2006).
That is, RE generation be-comes a very different problem than the classi-cal non-situated Dale & Reiter style RE genera-tion, which focuses on generating REs that are sin-gle noun phrases in the context of an unchangingworld.On the other hand, because the virtual environ-ments scenario is so open-ended, it ?
and specif-ically the instruction-giving task ?
can potentiallybe of interest to a wide range of NLG researchers.This is most obvious for research in sentence plan-ning (GRE, aggregation, lexical choice) and real-ization (the real-time nature of the task imposeshigh demands on the system?s efficiency).
But if166extended to two-way dialog, the task can also in-volve issues of prosody generation (i.e., researchon text/concept-to-speech generation), discoursegeneration, and human-robot interaction.
Finally,the game world can be scaled to focus on specificissues in NLG, such as the generation of REs orthe generation of navigation instructions.3 Evaluation Method and LogisticsNow we describe the method we applied to obtainexperimental data, and sketch the software infras-tructure we developed for this purpose.3.1 Software architectureA crucial aspect of the GIVE evaluation methodol-ogy is that it physically separates the user and theNLG system and connects them over the Internet.To achieve this, the GIVE software infrastructureconsists of three components (shown in Fig.
2):1. the client, which displays the 3D world tousers and allows them to interact with it;2. the NLG servers, which generate the natural-language instructions; and3.
the Matchmaker, which establishes connec-tions between clients and NLG servers.These three components run on different ma-chines.
The client is downloaded by users fromour website and run on their local machine; eachNLG server is run on a server at the institutionthat implemented it; and the Matchmaker runs ona central server we provide.
When a user starts theclient, it connects to the Matchmaker and is ran-domly assigned an NLG server and a game world.The client and NLG server then communicate overthe course of one game.
At the end of the game,the client displays a questionnaire to the user, andthe game log and questionnaire data are uploadedto the Matchmaker and stored in a database.
Notethat this division allows the challenge to be con-ducted without making any assumptions about theinternal structure of an NLG system.The GIVE software is implemented in Java andavailable as an open-source Google Code project.For more details about the software, see (Koller etal., 2009).3.2 SubjectsParticipants were recruited using email distribu-tion lists and press releases posted on the internet.Game ClientMatchmakerNLG ServerNLG ServerNLG ServerFigure 2: The GIVE architecture.Collecting data from anonymous users over theInternet presents a variety of issues that a lab-based experiment does not.
An Internet-basedevaluation skews the demographic of the subjectpool toward people who use the Internet, but prob-ably no more so than if recruiting on a collegecampus.
More worrisome is that, without a face-to-face meeting, the researcher has less confidencein the veracity of self-reported demographic datacollected from the subject.
For the purposes ofNLG software, the most important demographicquestion is the subject?s fluency in English.
Play-ers of the GIVE 2009 challenge were asked to self-report their command of English, age, and com-puter experience.
English proficiency did interactwith task completion, which leads us to concludethat users were honest about their level of Englishproficiency.
See section 4.4 below for a discus-sion of this interaction.
All-in-all, we feel that theadvantage gained from the large increase in thesize of the subject pool offsets any disadvantageaccrued from the lack of accurate demographic in-formation.3.3 MaterialsFigs.
3?5 show the layout of the three evaluationworlds.
The worlds were intended to provide vary-ing levels of difficulty for the direction-giving sys-tems and to focus on different aspects of the prob-lem.
World 1 is very similar to the developmentworld that the research teams were given to testtheir system on.
World 2 was intended to focuson object descriptions - the world has only oneroom which is full of objects and buttons, many ofwhich cannot be distinguished by simple descrip-tions.
World 3, on the other hand, puts more em-phasis on navigation directions as the world hasmany interconnected rooms and hallways.The difference between the worlds clearly bearsout in the task completion rates reported below.167plantchairalarmlamptutorial roomcouchsafeFigure 3: World 1lampplantchairalarmtutorial roomsafeFigure 4: World 2plantchairlampsafetutorial roomalarmFigure 5: World 33.4 TimelineAfter the GIVE Challenge was publicized inMarch 2008, eight research teams signed up forparticipation.
We distributed an initial version ofthe GIVE software and a development world tothese teams.
In the end, four teams submittedNLG systems.
These were connected to a cen-tral Matchmaker instance that ran for about threemonths, from 7 November 2008 to 5 February2009.
During this time, we advertised participa-tion in the GIVE Challenge to the public in orderto obtain experimental subjects.3.5 NLG systemsFive NLG systems were evaluated in GIVE-1:1. one system from the University of Texas atAustin (?Austin?
in the graphics below);2. one system from Union College in Schenec-tady, NY (?Union?);3.
one system from the Universidad Com-plutense de Madrid (?Madrid?);4.
two systems from the University of Twente:one serious contribution (?Twente?)
and onemore playful one (?Warm-Cold?
).Of these systems, ?Austin?
can serve as a base-line: It computes a plan consisting of the actionsthe user should take to achieve the goal, and ateach point in the game, it realizes the first stepin this plan as a single instruction.
The ?Warm-Cold?
system generates very vague instructionsthat only tell the user if they are getting closer(?warmer?)
to their next objective or if they aremoving away from it (?colder?).
We included thissystem in the evaluation to verify whether the eval-uation methodology would be able to distinguishsuch an obviously suboptimal instruction-givingstrategy from the others.Detailed descriptions of these systemsas well as each team?s own analysis ofthe evaluation results can be found athttp://www.give-challenge.org/research/give-1.4 ResultsWe now report on the results of GIVE-1.
We startwith some basic demographics; then we discussobjective and subjective evaluation measures.Notice that some of our evaluation measures arein tension with each other: For instance, a systemwhich gives very low-level instructions (?moveforward?
; ?ok, now move forward?
; ?ok, now turnleft?
), such as the ?Austin?
baseline, will lead theuser to completing the task in a minimum numberof steps; but it will require more instructions thana system that aggregates these.
This is intentional,and emphasizes both the pilot experiment char-acter of GIVE-1 and our desire to make GIVE afriendly comparative challenge rather than a com-petition with a clear winner.4.1 DemographicsOver the course of three months, we collected1143 valid games.
A game counted as valid if thegame client didn?t crash, the game wasn?t markedas a test game by the developers, and the playercompleted the tutorial.Of these games, 80.1% were played by malesand 9.9% by females; a further 10% didn?t specifytheir gender.
The players were widely distributedover countries: 37% connected from an IP addressin the US, 33% from an IP address in Germany,and 17% from China; Canada, the UK, and Aus-tria also accounted for more than 2% of the partic-168037,575,0112,5150,0Nov7Dec1Jan1Feb1Feb5# games per dayGermanpress releaseUSpress releaseposted toSIGGEN listcovered byChinese blogFigure 6: Histogram of the connections per day.ipants each, and the remaining 2% of participantsconnected from 42 further countries.
This imbal-ance stems from very successful press releases thatwere issued in Germany and the US and whichwere further picked up by blogs, including onein China.
Nevertheless, over 90% of the partici-pants who answered this question self-rated theirEnglish proficiency as ?good?
or better.
About75% of users connected with a client running onWindows, with the rest split about evenly amongLinux and Mac OS X.The effect of the press releases is also plainlyvisible if we look at the distribution of the validgames over the days from November 7 to Febru-ary 5 (Fig.
6).
There are huge peaks at thevery beginning of the evaluation period, coincid-ing with press releases through Saarland Univer-sity in Germany and Northwestern University inthe US, which were picked up by science and tech-nology blogs on the Web.
The US peak containsa smaller peak of connections from China, whichwere sparked by coverage in a Chinese blog.4.2 Objective measuresWe then extracted objective and subjective mea-surements from the valid games.
The objectivemeasures are summarized in Fig.
7.
For each sys-tem and game world, we measured the percent-age of games which the users completed success-fully.
Furthermore, we counted the numbers of in-structions the system sent to the user, measuredthe time until task completion, and counted thenumber of low-level steps executed by the user(any key press, to either move or manipulate anobject) as well as the number of task-relevant ac-tions (such as pushing a button to open a door).?
task success (Did the player get the trophy?)?
instructions (Number of instructions pro-duced by the NLG system.?)?
steps (Number of all player actions.?)?
actions (Number of object manipulationaction.?)?
second (Time in seconds.?
)?Measured from the end of the tutorial until theend of the game.Figure 7: Objective measurementsAustinMadridTwenteUnionWarm-Coldtasksuccess40% 71% 35% 73% 18%A AB BCinstructions83.2 58.3 121.2 80.3 190.0AB BCDsteps103.6 124.3 160.9 117.5 307.4A AB BCDactions11.2 8.7 14.3 9.0 14.3A ABC Cseconds129.3 174.8 207.0 175.2 312.2AB BCDFigure 8: Objective measures by system.
Tasksuccess is reported as the percentage of suc-cessfully completed games.
The other measuresare reported as the mean number of instruc-tions/steps/actions/seconds, respectively.
Lettersgroup indistinguishable systems; systems thatdon?t share a letter were found to be significantlydifferent with p < 0.05.169To ensure comparability, we only counted success-fully completed games for all these measures, andonly started counting when the user left the tutorialroom.
Crucially, all objective measures were col-lected completely unobtrusively, without requiringany action on the user?s part.Fig.
8 shows the results of these objective mea-sures.
This figure assigns systems to groups A,B, etc.
for each evaluation measure.
Systems ingroup A are better than systems in group B, etc.
;if two systems don?t share the same letter, the dif-ference between these two systems is significantwith p < 0.05.
Significance was tested using a?2-test for task success and ANOVAs for instruc-tions, steps, actions, and seconds.
These were fol-lowed by post-hoc tests (pairwise ?2 and Tukey)to compare the NLG systems pairwise.Overall, there is a top group consisting ofthe Austin, Madrid, and Union systems: WhileMadrid and Union outperform Austin on task suc-cess (with 70 to 80% of successfully completedgames, depending on the world), Austin signifi-cantly outperforms all other systems in terms oftask completion time.
As expected, the Warm-Cold system performs significantly worse than allothers in almost all categories.
This confirms theability of the GIVE evaluation method to distin-guish between systems of very different qualities.4.3 Subjective measuresThe subjective measures, which were obtained byasking the users to fill in a questionnaire after eachgame, are shown in Fig.
9.
Most of the questionswere answered on 5-point Likert scales (?overall?on a 7-point scale); the ?informativity?
and ?tim-ing?
questions had nominal answers.
For eachquestion, the user could choose not to answer.The results of the subjective measurements aresummarized in Fig.
10, in the same format asabove.
We ran ?2-tests for the nominal variablesinformativity and timing, and ANOVAs for thescale data.
Again, we used post-hoc pairwise ?2-and Tukey-tests to compare the NLG systems toeach other one by one.Here there are fewer significant differences be-tween different groups than for the objective mea-sures: For the ?play again?
category, there isno significant difference at all.
Nevertheless,?Austin?
is shown to be particularly good at navi-gation instructions and timing, whereas ?Madrid?outperforms the rest of the field in ?informativ-7-point scale items:overall: What is your overall evaluation of the quality of thedirection-giving system?
(very bad 1 .
.
.
7 very good)5-point scale items:task difficulty: How easy or difficult was the task for you tosolve?
(very difficult 1 2 3 4 5 very easy)goal clarity: How easy was it to understand what you weresupposed to do?
(very difficult 1 2 3 4 5 very easy)play again: Would you want to play this game again?
(noway!
1 2 3 4 5 yes please!
)instruction clarity: How clear were the directions?
(totallyunclear 1 2 3 4 5 very clear)instruction helpfulness: How effective were the directions athelping you complete the task?
(not effective 1 2 3 4 5very effective)choice of words: How easy to understand was the system?schoice of wording in its directions to you?
(totally un-clear 1 2 3 4 5 very clear)referring expressions: How easy was it to pick out which ob-ject in the world the system was referring to?
(very hard1 2 3 4 5 very easy)navigation instructions: How easy was it to navigate to a par-ticular spot, based on the system?s directions?
(veryhard 1 2 3 4 5 very easy)friendliness: How would you rate the friendliness of the sys-tem?
(very unfriendly 1 2 3 4 5 very friendly)Nominal items:informativity: Did you feel the amount of information youwere given was: too little / just right / too muchtiming: Did the directions come ... too early / just at the righttime / too lateFigure 9: Questionnaire itemsity?.
In the overall subjective evaluation, the ear-lier top group of Austin, Madrid, and Union isconfirmed, although the difference between Unionand Twente is not significant.
However, ?Warm-Cold?
again performs significantly worse than allother systems in most measures.
Furthermore, al-though most systems perform similarly on ?infor-mativity?
and ?timing?
in terms of the number ofusers who judged them as ?just right?, there aredifferences in the tendencies: Twente and Uniontend to be overinformative, whereas Austin andWarm-Cold tend to be underinformative; Twenteand Union tend to give their instructions too late,whereas Madrid and Warm-Cold tend to give themtoo early.170AustinMadridTwenteUnionWarm-Coldtaskdifficulty4.3 4.3 4.0 4.3 3.5A A A ABgoal clarity4.0 3.7 3.9 3.7 3.3A A A ABplay again2.8 2.6 2.4 2.9 2.5A A A A Ainstructionclarity4.0 3.6 3.8 3.6 3.0A A AB B BCinstructionhelpfulness3.8 3.9 3.6 3.7 2.9A A A ABinformativity46% 68% 51% 56% 51%AB B B Boverall4.9 4.9 4.3 4.6 3.6A A AB BCchoice ofwords4.2 3.8 4.1 3.7 3.5A AB BC C Creferringexpressions3.4 3.9 3.7 3.7 3.5A A AB B B Bnavigationinstructions4.6 4.0 4.0 3.7 3.2AB B BCtiming78% 62% 60% 62% 49%AB B BC Cfriendliness3.4 3.8 3.1 3.6 3.1A A AB B BFigure 10: Subjective measures by system.
Infor-mativity and timing are reported as the percentageof successfully completed games.
The other mea-sures are reported as the mean rating received bythe players.
Letters group indistinguishable sys-tems; systems that don?t share a letter were foundto be significantly different with p < 0.05.4.4 Further analysisIn addition to the differences between NLG sys-tems, there may be other factors which also influ-ence the outcome of our objective and subjectivemeasures.
We tested the following five factors:evaluation world, gender, age, computer expertise,and English proficiency (as reported by the userson the questionnaire).
We found that there is a sig-nificant difference in task success rate for differentevaluation worlds and between users with differentlevels of English proficiency.The interaction graphs in Figs.
11 and 12 alsosuggest that the NLG systems differ in their ro-bustness with respect to these factors.
?2-teststhat compare the success rate of each system inthe three evaluation worlds show that while theinstructions of Union and Madrid seem to workequally well in all three worlds, the performanceof the other three systems differs dramatically be-tween the different worlds.
Especially World 2was challenging for some systems as it requiredrelational object descriptions, such as the blue but-ton on the left of another blue button.The players?
English skills also affected the sys-tems in different ways.
While Austin, Madrid andWarm Cold don?t manage to lead players with onlybasic English skills to success as often as otherplayers, Union?s and Twente?s success rates do notdepend on the players?
English skills (?2-tests donot find significant differences in success rate be-tween players with different levels of English pro-ficiency for these two systems).
However, if weremove the players with the lowest level of En-glish proficiency, language skills do not have aneffect on the task success rate anymore for any ofthe systems.5 ConclusionIn this document, we have described the first in-stallment of the GIVE Challenge, our experimen-tal methodology, and the results.
Altogether, wecollected 1143 valid games for five NLG systemsover a period of three months.
Given that this wasthe first time we organized the challenge, that itwas meant as a pilot experiment from the begin-ning, and that the number of games was sufficientto get significant differences between systems ona number of measures, we feel that GIVE-1 was asuccess.
We are in the process of preparing sev-eral diagnostic utilities, such as heat maps and atool that lets the system developer replay an indi-171Figure 11: Effect of the evaluation worlds on thesuccess rate of the NLG systems.vidual game, which will help the participants gainfurther insight into their NLG systems.Nevertheless, there are a number of improve-ments we will make to GIVE for future install-ments.
For one thing, the timing of the challengewas not optimal: A number of colleagues wouldhave been interested in participating, but the callfor participation came too late for them to acquirefunding or interest students in time for summerprojects or MSc theses.
Secondly, although thesoftware performed very well in handling thou-sands of user connections, there were still game-invalidating issues with the 3D graphics and thenetworking code that were individually rare, butprobably cost us several hundred games.
Theseshould be fixed for GIVE-2.
At the same time,we are investigating ways in which the networkingand matchmaking core of GIVE can be factoredout into a separate, challenge-independent systemon which other Internet-based challenges can bebuilt.
Among other things, it would be straightfor-ward to use the GIVE platform to connect two hu-man users and observe their dialogue while solv-ing a problem.
Judicious variation of parameters(such as the familiarity of users or the visibility ofan instruction giving avatar) would allow the con-struction of new dialogue corpora along such lines.Finally, GIVE-1 focused on the generation ofnavigation instructions and referring expressions,in a relatively simple world, without giving theFigure 12: Effect of the players?
English skills onthe success rate of the NLG systems.user a chance to talk back.
The high success rateof some systems in this challenge suggests thatwe need to widen the focus for a future GIVE-2 ?
by allowing dialogue, by making the worldmore complex (e.g., allowing continuous ratherthan discrete movements and turns), by making thecommunication multi-modal, etc.
Such extensionswould require only rather limited changes to theGIVE software infrastructure.
We plan to come toa decision about such future directions for GIVEsoon, and are looking forward to many fruitful dis-cussions about this at ENLG.Acknowledgments.
We are grateful to the par-ticipants of the 2007 NSF/SIGGEN Workshop onShared Tasks and Evaluation in NLG and manyother colleagues for fruitful discussions while wewere designing the GIVE Challenge, and to theorganizers of Generation Challenges 2009 andENLG 2009 for their support and the opportunityto present the results at ENLG.
We also thank thefour participating research teams for their contri-butions and their patience while we were workingout bugs in the GIVE software.
The creation ofthe GIVE infrastructure was supported in part bya Small Projects grant from the University of Ed-inburgh.172ReferencesA.
Belz and A. Gatt.
2008.
Intrinsic vs. extrinsic eval-uation measures for referring expression generation.In Proceedings of ACL-08:HLT, Short Papers, pages197?200, Columbus, Ohio.M.
E. Foster.
2008.
Automated metrics that agreewith human judgements on generated output for anembodied conversational agent.
In Proceedings ofINLG 2008, pages 95?103, Salt Fork, OH.A.
Koller, D. Byron, J. Cassell, R. Dale, J. Moore,J.
Oberlander, and K. Striegnitz.
2009.
The soft-ware architecture for the first challenge on generat-ing instructions in virtual environments.
In Proceed-ings of the EACL-09 Demo Session.J.
Orkin and D. Roy.
2007.
The restaurant game:Learning social behavior and language from thou-sands of players online.
Journal of Game Develop-ment, 3(1):39?60.A.
Stent, M. Marge, and M. Singhai.
2005.
Evaluatingevaluation methods for generation in the presence ofvariation.
In Proceedings of CICLing 2005.L.
Stoia, D. M. Shockley, D. K. Byron, and E. Fosler-Lussier.
2006.
Noun phrase generation for situateddialogs.
In Proceedings of INLG, Sydney.L.
von Ahn and L. Dabbish.
2004.
Labeling imageswith a computer game.
In Proceedings of the ACMCHI Conference.173
