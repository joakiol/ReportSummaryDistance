Proceedings of the EACL 2009 Workshop on the Interaction between Linguistics and Computational Linguistics, pages 3?11,Athens, Greece, 30 March, 2009. c?2009 Association for Computational LinguisticsHow the statistical revolution changes (computational) linguisticsMark JohnsonCognitive and Linguistic Sciences and Computer ScienceBrown UniversityMark Johnson@Brown.eduAbstractThis paper discusses some of the ways thatthe ?statistical revolution?
has changedand continues to change the relationshipbetween linguistics and computational lin-guistics.
I claim that it is more useful inparsing to make an open world assumptionabout possible linguistic structures, ratherthan the closed world assumption usu-ally made in grammar-based approaches toparsing, and I sketch two different ways inwhich grammar-based approaches mightbe modified to achieve this.
I also de-scribe some of the ways in which proba-bilistic models are starting to have a sig-nificant impact on psycholinguistics andlanguage acquisition.
In language acqui-sition Bayesian techniques may let us em-pirically evaluate the role of putative uni-versals in universal grammar.1 IntroductionThe workshop organizers asked us to write some-thing controversial to stimulate discussion, andI?ve attempted to do that here.
Usually in my pa-pers I try to stick to facts and claims that I can sup-port, but here I have fearlessly and perhaps fool-ishly gone out on a limb and presented guesses,hunches and opinions.
Take them with a grain ofsalt.
Inspired by Wanamaker?s well-known quoteabout advertising, I expect that half of the ideasI?m proposing here are wrong, but I don?t knowwhich half.
I hope the conference will help mefigure that out.Statistical techniques have revolutionized manyscientific fields in the past two decades, includingcomputational linguistics.
This paper discussesthe impact of this on the relationship betweencomputational linguistics and linguistics.
I?m pre-senting a personal perspective rather than a scien-tific review here, and for this reason I focus on ar-eas I have some experience with.
I begin by dis-cussing how the statistical perspective changed myunderstanding of the relationship between linguis-tic theory, grammars and parsing, and then go onto describe some of the ways that ideas from statis-tics and machine learning are starting to have animpact on linguistics today.Before beginning, I?d like to say somethingabout what I think computational linguistics is.
Iview computational linguistics as having both ascientific and an engineering side.
The engineer-ing side of computational linguistics, often callednatural language processing (NLP), is largely con-cerned with building computational tools that douseful things with language, e.g., machine trans-lation, summarization, question-answering, etc.Like any engineering discipline, natural languageprocessing draws on a variety of different scien-tific disciplines.I think it?s fair to say that in the current stateof the art, natural language processing draws farmore heavily on statistics and machine learningthan it does on linguistic theory.
For example, onemight claim that all an NLP engineer really needsto understand about linguistic theory are (say) theparts of speech (POS).
Assuming this is true (I?mnot sure it is), would it indicate that there is some-thing wrong with either linguistic theory or com-putational linguistics?
I don?t think it does: there?sno reason to expect an engineering solution to uti-lize all the scientific knowledge of a related field.The fact that you can build perfectly good bridgeswith Newtonian mechanics says nothing about thetruth of quantum mechanics.I also believe that there is a scientific field ofcomputational linguistics.
This scientific field ex-ists not just because computers are incredibly use-ful for doing linguistics ?
I expect that comput-ers have revolutionized most fields of science ?but because it makes sense to think of linguis-3tic processes as being essentially computational innature.
If we take computation to be the manip-ulation of symbols in a meaning-respecting way,then it seems reasonable to hypothesize that lan-guage comprehension, production and acquisitionare all computational processes.
Viewed this way,we might expect computational linguistics to in-teract most strongly with those areas of linguis-tics that study linguistic processing, namely psy-cholinguistics and language acquisition.
As I ex-plain in section 3 below, I think we are starting tosee this happen.2 Grammar-based and statistical parsingIn some ways the 1980s were a golden age forcollaboration and cross-fertilization between lin-guistic theory and computational linguistics, es-pecially between syntax and parsing.
Gazdarand colleagues showed that Chomskyian transfor-mations could be supplanted by computationallymuch simpler feature passing mechanisms (Gaz-dar et al, 1985), and this lead to an explosion ofwork on ?unification-based?
grammars (Shieber,1986), including the Lexical-Functional Gram-mars and Head-driven Phrase Structure Grammarsthat are still very actively pursued today.
I?ll callthe work on parsing within this general frameworkthe grammar-based approach in order to contrastit with the statistical approach that doesn?t rely onthese kinds of grammars.
I think the statistical ap-proach has come to dominate computational lin-guistics, and in this section I?ll describe why thishappened.Before beginning I think it?s useful to clarify ourgoals for building parsers.
There are many reasonswhy one might build any computational system?
perhaps it?s a part of a commercial product wehope will make us rich, or perhaps we want to testthe predictions of a certain theory of processing?
and these reasons should dictate how and evenwhether the system is constructed.
I?m assumingin this section that we want to build parsers be-cause we expect the representations they producewill be useful for various other NLP engineeringtasks.
This means that parser design is itself essen-tially an engineering task, i.e., we want a devicethat returns parses that are accurate as possible foras many sentences as possible.I?ll begin by discussing a couple of differ-ences between the approaches that are often men-tioned but I don?t think are really that impor-tant.
The grammar-based approaches are some-times described as producing deeper representa-tions that are closer to meaning.
It certainly istrue that grammar-based analyses typically repre-sent predicate-argument structure and perhaps alsoquantifier scope.
But one can recover predicate-argument structure using statistical methods (seethe work on semantic role labeling and ?Prop-Bank?
parsing (Palmer et al, 2005)), and pre-sumably similar methods could be used to resolvequantifier scope as well.I suspect the main reason why statistical pars-ing has concentrated on more superficial syntac-tic structure (such as phrase structure) is becausethere aren?t many actual applications for the syn-tactic analyses our parsers return.
Given the cur-rent state-of-the-art in knowledge representationand artificial intelligence, even if we could pro-duce completely accurate logical forms in somehigher-order logic, it?s not clear whether we coulddo anything useful with them.
It?s hard to find realapplications that benefit from even syntactic infor-mation, and the information any such applicationsactually use is often fairly superficial.
For exam-ple, some research systems for named entity de-tection and extraction use parsing to identify nounphrases (which are potentially name entities) aswell as the verbs that govern them, but they ignorethe rest of the syntactic structure.
In fact, manyapplications of statistical parsers simply use themas language models, i.e., one parses to obtain theprobability that the parser assigns to the string andthrows away the parses it computes in the process(Jelinek, 2004).
(It seems that such parsing-basedlanguage models are good at preferring strings thatare at least superficially grammatical, e.g., whereeach clause contains one verb phrase, which isuseful in applications such as summarization andmachine translation).Grammar-based approaches are also often de-scribed as more linguistically based, while sta-tistical approaches are viewed as less linguisti-cally informed.
I think this view primarily re-flects the origins of the two approaches: thegrammar-based approach arose from the collab-oration between linguists and computer scientistsin the 1980s mentioned earlier, while the statisti-cal approach has its origins in engineering workin speech recognition in which linguists did notplay a major role.
I also think this view is basi-cally false.
In the grammar-based approaches lin-4guists write the grammars while in statistical ap-proaches linguists annotate the corpora with syn-tactic parses, so linguists play a central role inboth.
(It?s an interesting question as to why cor-pus annotation plus statistical inference seems tobe a more effective way of getting linguistic in-formation into a computer than manually writinga grammar).Rather, I think that computational linguistsworking on statistical parsing need a greater levelof linguistic sensitivity at an informal level thanthose working on grammar-based approaches.In the grammar-based approaches all linguisticknowledge is contained in the grammar, which thecomputational linguist implementing the parsingframework doesn?t actually have to understand.All she has to do is correctly implement an in-ference engine for grammars written in the rel-evant grammar formalism.
By contrast, statisti-cal parsers define the probability of a parse interms of its (statistical) features or properties, anda parser designer needs to choose which featurestheir parser will use, and many of these features re-flect at least an intuitive understanding of linguis-tic dependencies.
For example, statistical parsersfrom Magerman (1995) on use features based onhead-dependent relationships.
(The parsers devel-oped by the Berkeley group are a notable excep-tion (Petrov and Klein, 2007)).
While it?s truethat only a small fraction of our knowledge aboutlinguistic structure winds up expressed by fea-tures in modern statistical parsers, as discussedabove there?s no reason to expect all of our sci-entific knowledge to be relevant to any engineer-ing problem.
And while many of the features usedin statistical parsers don?t correspond to linguis-tic constraints, nobody seriously claims that hu-mans understand language only using linguisticconstraints of the kind expressed in formal gram-mars.
I suspect that many of the features thathave been shown to be useful in statistical parsingencode psycholinguistic markedness preferences(e.g., attachment preferences) and at least someaspects of world knowledge (e.g., that the directobject of ?eat?
is likely to be a food).Moreover, it?s not necessary for a statisticalmodel to exactly replicate a linguistic constraint inorder for it to effectively capture the correspond-ing generalization: all that?s necessary is that thestatistical features ?cover?
the relevant examples.For example, adding a subject-verb agreement fea-ture to the Charniak-Johnson parser (Charniak andJohnson, 2005) has no measurable effect on pars-ing accuracy.
After doing this experiment I re-alized this shouldn?t be surprising: the Charniakparser already conditions each argument?s part-of-speech (POS) on its governor?s POS, and sincePOS tags distinguish singular and plural nouns andverbs, these general head-argument POS featurescapture most cases of subject-verb agreement.Note that I?m not claiming that subject-verbagreement isn?t a real linguistic constraint or thatit doesn?t play an important role in human pars-ing.
I think that the type of input (e.g., treebanks)and the kinds of abilities (e.g., to exactly count theoccurences of many different constructions) avail-able to our machines may be so different to what isavailable to a child that the features that work bestin our parsers need not bear much relationship tothose used by humans.Still, I view the design of the features used instatistical parsers as a fundamentally linguistic is-sue (albeit one with computational consequences,since the search problem in parsing is largely de-termined by the features involved), and I expectthere is still more to learn about which combi-nations of features are most useful for statisti-cal parsing.
My guess is that the features usedin e.g., the Collins (2003) or Charniak (2000)parsers are probably close to optimal for EnglishPenn Treebank parsing (Marcus et al, 1993), butthat other features might improve parsing of otherlanguages or even other English genres.
Un-fortunately changing the features used in theseparsers typically involves significant reprogram-ming, which makes it difficult for linguists to ex-periment with new features.
However, it mightbe possible to develop a kind of statistical pars-ing framework that makes it possible to define newfeatures and integrate them into a statistical parserwithout any programming which would make iteasy to explore novel combinations of statisticalfeatures; see Goodman (1998) for an interestingsuggestion along these lines.From a high-level perspective, the grammar-based approaches and the statistical approachesboth view parsing fundamentally in the same way,namely as a specialized kind of inference problem.These days I view ?parsing as deduction?
(one ofthe slogans touted by the grammar-based crowd)as unnecessarily restrictive; after all, psycholin-guistic research shows that humans are exquisitely5sensitive to distributional information, so whyshouldn?t we let our parsers use that informationas well?
And as Abney (1997) showed, it ismathematically straight-forward to define proba-bility distributions over the representations usedby virtually any theory of grammar (even those ofChomsky?s Minimalism), which means that theo-retically the arsenal of statistical methods for pars-ing and learning can be applied to any grammarjust as well.In the late 1990s I explored these kinds of sta-tistical models for Lexical-Functional Grammar(Bresnan, 1982; Johnson et al, 1999).
The hopewas that statistical features based on LFG?s richerrepresentations (specifically, f -structures) mightresult in better parsing accuracy.
However, thisseems not to be the case.
As mentioned above, Ab-ney?s formulation of probabilistic models makesessentially no demands on what linguistic repre-sentations actually are; all that is required is thatthe statistical features are functions that map eachrepresentation to a real number.
These are used tomap a set of linguistic representations (say, the setof all grammatical analyses) to a set of vectors ofreal numbers.
Then by defining a distribution overthese sets of real-valued vectors we implicitly de-fine a distribution over the corresponding linguis-tic representations.This means that as far as the probabilistic modelis concerned the details of the linguistic represen-tations don?t actually matter, so long as there arethe right number of them and it is possible to com-pute the necessary real-valued vectors from them.For a computational linguist this is actually quitea liberating point of view; we aren?t restrictedto slavishly reproducing textbook linguistic struc-tures, but are free to experiment with alternativerepresentations that might have computational orother advantages.In my case, it turned out that the kinds of fea-tures that were most useful for stochastic LFGparsing could in fact be directly computed fromphrase-structure trees.
The features that involvedf -structure properties could be covered by otherfeatures defined directly on the phrase-structuretrees.
(Some of these phrase-structure featureswere implemented by rather nasty C++ routinesbut that doesn?t matter; Abney-type models makeno assumptions about what the feature functionsare).
This meant that I didn?t actually need thef -structures to define the probability distributionsI was interested in; all I needed were the corre-sponding c-structure or phrase-structure trees.And of course there are many ways of obtain-ing phrase-structure trees.
At the time my col-league Eugene Charniak was developing a statis-tical phrase-structure parser that was more robustand had broader coverage than the LFG parser Iwas working with, and I found I generally gotbetter performance if I used the trees his parserproduced, so that?s what I did.
This leads tothe discriminative re-ranking approach developedby Collins and Koo (2005), in which a statisticalparser trained on a treebank is used to produce aset of candidate parses which are then ?re-ranked?by an Abney-style probabilistic model.I suspect these robustness and coverage prob-lems of grammar-based parsing are symptoms ofa fundamental problem in the standard way thatgrammar-based parsing is understood.
First, Ithink grammar-based approaches face a dilemma:on the one hand the explosion of ambiguity sug-gests that some sentences get too many parses,while the problems of coverage show that somesentences get too few, i.e., zero, parses.
While it?spossible that there is a single grammar that canresolve this dilemma, my point here is that eachof these problems suggests we need to modify thegrammars in exactly the opposite way, i.e., gener-ally tighten the constraints in order to reduce am-biguity, while generally relax the constraints in or-der to allow more parses for sentences that haveno parses at all.Second, I think this dilemma only arises be-cause the grammar-based approach to parsing isfundamentally designed around the goal of dis-tinguishing grammatical from ungrammatical sen-tences.
While I agree with Pullum (2007) thatgrammaticality is and should be central to syntac-tic theory, I suspect it is not helpful to view pars-ing (by machines or humans) as a byproduct ofproving the grammaticality of a sentence.
In mostof the applications I can imagine, what we reallywant from a parser is the parse that reflects its bestguess at the intended interpretation of the input,even if that input is ungrammatical.
For example,given the telegraphese input ?man bites dog?
wewant the parser to tell us that ?man?
is likely to bethe agent of ?bites?
and ?dog?
the patient, and notsimply that the sentence is ungrammatical.These grammars typically distinguish grammat-ical from ungrammatical analyses by explicitly6characterizing the set of grammatical analyses insome way, and then assuming that all other anal-yses are ungrammatical.
Borrowing terminologyfrom logic programming (Lloyd, 1987) we mightcall this a closed-world assumption: any analysisthe grammar does not generate is assumed to beungrammatical.Interestingly, I think that the probabilistic mod-els used statistical parsing generally make anopen-world assumption about linguistic analyses.These probabilistic models prefer certain linguis-tic structures over others, but the smoothing mech-anisms that these methods use ensure that everypossible analysis (and hence every possible string)receives positive probability.
In such an approachthe statistical features identify properties of syn-tactic analyses which make the analysis more orless likely, so the probabilistic model can prefer,disprefer or simply be ambivalent about any par-ticular linguistic feature or construction.I think an open-world assumption is generallypreferable as a model of syntactic parsing in bothhumans and machines.
I think it?s not reason-able to assume that the parser knows all the lex-ical entries and syntactic constructions of the lan-guage it is parsing.
Even if the parser encoun-ters a word or construction it doesn?t understand it,that shouldn?t stop it from interpreting the rest ofthe sentence.
Statistical parsers are considerablymore open-world.
For example, unknown wordsdon?t present any fundamental problem for statis-tical parsers; in the absence of specific lexical in-formation about a word they automatically backoff to generic information about words in general.Does the closed-world assumption inherent inthe standard approach to grammar-based parsingmean we have to abandon it?
I don?t think so; Ican imagine at least two ways in which the con-ventional grammar-based approach might be mod-ified to obtain an open-world parsing model.One possible approach keeps the standardclosed-world conception that grammars generateonly grammatical analyses, but gives up the ideathat parsing is a byproduct of determining thegrammaticality of the input sentence.
Instead, wemight use a noisy channel to map grammaticalanalyses generated by the grammar to the actualinput sentences we have to parse.
Parsing involvesrecovering the grammatical source or underlyingsentence as well as its structure.
Presumably thechannel model would be designed to prefer min-imal distortion, so if the input to be parsed isin fact grammatical then the channel would pre-fer the identity transformation, while if the inputis ungrammatical the channel model would mapit to close grammatical sentences.
For example,if such a parser were given the input ?man bitesdog?
it might decide that the most probable un-derlying sentence is ?a man bites a dog?
and re-turn a parse for that sentence.
Such an approachmight be regarded as a way of formalizing the ideathat ungrammatical sentences are interpreted byanalogy with grammatical ones.
(Charniak and Iproposed a noisy channel model along these linesfor parsing transcribed speech (Johnson and Char-niak, 2004)).Another possible approach involves modifyingour interpretation of the grammar itself.
We couldobtain an open world model by relaxing our inter-pretation of some or all of the constraints in thegrammar.
Instead of viewing them as hard con-straints that define a set of grammatical construc-tions, we reinterpret them as violable, probabilis-tic features.
For example, instead of interpret-ing subject-verb agreement as a hard constraintthat rules out certain syntactic analyses, we rein-terpret it as a soft constraint that penalizes analy-ses in which subject-verb agreement fails.
Insteadof assuming that each verb comes with a fixedset of subcategorization requirements, we mightview subcategorization as preferences for certainkinds of complements, implemented by featuresin an Abney-style statistical model.
Unknownwords come with no subcategorization preferencesof their own, so they would inherit the prior or de-fault preferences.
Formally, I think this is fairlyeasy to achieve: we replace the hard unificationconstraints (e.g., that the subject?s number featureequals the verb?s number feature) with a stochas-tic feature that fires whenever the subject?s numberfeature differs from the verb?s number feature, andrely on the statistical model training procedure toestimate that feature?s weight.Computationally, I suspect that either of theseoptions (or any other option that makes thegrammar-based approaches open world) will re-quire a major rethinking of the parsing process.Notice that both approaches let ambiguity prolif-erate (ambiguity is our friend in the fight againstpoor coverage), so we would need parsing al-gorithms capable of handling massive ambiguity.This is true of most statistical parsing models, so7it is possible that the same approaches that haveproven successful in statistical parsing (e.g., usingprobabilities to guide search, dynamic program-ming, coarse-to-fine) will be useful here as well.3 Statistical models and linguisticsThe previous section focused on syntactic parsing,which is an area in which there?s been a fruitful in-teraction between linguistic theory and computa-tional linguistics over a period of several decades.In this section I want to discuss two other emerg-ing areas in which I expect the interaction be-tween linguistics and computational linguistics tobecome increasingly important: psycholinguisticsand language acquisition.
I think it?s no accidentthat these areas both study processing (rather thanan area of theoretical linguistics such as syntaxor semantics), since I believe that the scientificside of computational linguistics is fundamentallyabout such linguistic processes.Just to be clear: psycholinguistics and languageacquisition are experimental disciplines, and Idon?t expect the average researcher in those fieldsto start doing computational linguistics any timesoon.
However, I do think there are an emergingcadre of young researchers in both fields apply-ing ideas and results from computational linguis-tics in their work and using experimental resultsfrom their field to develop and improve the compu-tational models.
For example, in psycholinguisticsresearchers such as Hale (2006) and Levy (2008)are using probabilistic models of syntactic struc-ture to make predictions about human sentenceprocessing, and Bachrach (2008) is using predic-tions from the Roark (2001) parser to help explainthe patterns of fMRI activation observed duringsentence comprehension.
In the field of languageacquisition computational linguists such as Kleinand Manning (2004) have studied the unsuper-vised acquisition of syntactic structure, while lin-guists such as Boersma and Hayes (2001), Gold-smith (2001), Pater (2008) and Albright and Hayes(2003) are developing probabilistic models of theacquisition of phonology and/or morphology, andFrank et al (2007) experimentally tests the predic-tions of a Bayesian model of lexical acquisition.Since I have more experience with computationalmodels of language acquisition, I?ll concentrate onthis topic for the rest of this section.Much of this work can be viewed under the slo-gan ?structured statistical learning?.
That is, spec-ifying the structures over which the learning algo-rithm generalizes is just as important as specifyingthe learning algorithm itself.
One of the things Ilike about this work is that it gets beyond the naivenature-versus-nurture arguments that characterizesome of the earlier theoretical work on languageacquisition.
Instead, these computational modelsbecome tools for investigating the effect of spe-cific structural assumptions on the acquisition pro-cess.
For example, Goldwater et al (2007) showsthat modeling inter-word dependencies improvesword segmentation, which shows that the linguis-tic context contains information that is potentiallyvery useful for lexical acquisition.I think it?s no accident that much of the com-putational work is concerned with phonology andmorphology.
These fields seem to be closer tothe data and the structures involved seem simplerthan in, say, syntax and semantics.
I suspect thatlinguists working in phonology and morphologyfind it easier to understand and accept probabilisticmodels in large part because of Smolensky?s workon Optimality Theory (Smolensky and Legendre,2005).
Smolensky found a way of introducing op-timization into linguistic theory in a way that lin-guists could understand, and this serves as a veryimportant bridge for them to probabilistic models.As I argued above, it?s important with any com-putational modeling to be clear about exactly whatour computational models are intended to achieve.Perhaps the most straight-forward goal for compu-tational models of language acquisition is to viewthem as specifying the actual computations that ahuman performs when learning a language.
Un-der this conception we expect the computationalmodel to describe the learning trajectory of lan-guage acquisition, e.g., if it takes the algorithmmore iterations to learn one word than another,then we would expect humans to take longer tothat word as well.
Much of the work in compu-tational phonology seems to take this perspective(Boersma and Hayes, 2001).Alternatively, we might view our probabilisticmodels (rather than the computational proceduresthat implementing them) as embodying the scien-tific claims we want to make.
Because these prob-abilistic models are too complex to analyze ana-lytically in general we need a computational pro-cedure to compute the model?s predictions, but thecomputational procedure itself is not claimed tohave any psychological reality.
For example, we8might claim that the grammar a child will learnis the one that is optimal with respect to a cer-tain probabilistic model.
We need an algorithm forcomputing this optimal grammar so we can checkthe probabilistic model?s predictions and to con-vince ourselves we?re not expecting the learner toperform magic, but we might not want to claimthat humans use this algorithm.
To use termi-nology from the grammar-based approaches men-tioned earlier, a probabilistic model is a declara-tive specification of the distribution of certain vari-ables, but it says nothing about how this distribu-tion might actually be calculated.
I think Marr?s?three levels?
capture this difference nicely: thequestion is whether we take our models to be ?al-gorithmic level?
or ?computational level?
descrip-tions of cognitive processes (Marr, 1982).Looking into the future, I?m very excited aboutBayesian approaches to language acquisition, as Ithink they have the potential to let us finally ex-amine deep questions about language acquisitionin a quantitative way.
The Bayesian approach fac-tors learning problems into two pieces: the likeli-hood and the prior.
The likelihood encodes the in-formation obtained from the data, while the priorencodes the information possessed by the learnerbefore learning commences (Pearl, 1988).
In prin-ciple the prior can encode virtually any informa-tion, including information claimed to be part ofuniversal grammar.Bayesian priors can incorporate the propertieslinguists often take to be part of universal gram-mar, such as X ?
theory.
A Bayesian prior canalso express soft markedness preferences as wellas hard constraints.
Moreover, the prior can alsoincorporate preferences that are not specificallylinguistic, such as a preference for shorter gram-mars or smaller lexicons, i.e., the kinds of prefer-ences sometimes expressed by an evaluation met-ric (Chomsky, 1965).The Bayesian framework therefore provides uswith a tool to quantitatively evaluate the impactof different purported linguistic universals on lan-guage acquisition.
For example, we can calcu-late the contribution of, say, hypothetical X ?
the-ory universals on the acquisition of syntax.
TheBayesian framework is flexible enough to also per-mit us to evaluate the contribution of the non-linguistic context on learning (Frank et al, to ap-pear).
Finally, non-parametric Bayesian methodspermit us to learn models with an unbounded num-ber features, perhaps giving us the mathematicaland computational tools to understand the induc-tion of rules and complex structure (Johnson et al,2007).Of course doing this requires developing actualBayesian models of language, and this is not easy.Even though this research is still just beginning,it?s clear that the details of the models have a hugeimpact on how well they work.
It?s not enough to?assume some version of X ?
theory?
; one needs toevaluate specific proposals.
Still, my hope is thatbeing able to evaluate the contributions of specificputative universals may help us measure and un-derstand their contributions (if any) to the learningprocess.4 ConclusionIn this paper I focused on two areas of interactionbetween computational linguistics and linguistictheory.
In the area of parsing I argued that weshould design parsers so they incorporate an open-world assumption about sentences and their lin-guistic structures and sketched two ways in whichgrammar-based approaches might be modified tomake them do this; both of which involve aban-doning the idea that parsing is solely a process ofproving the grammaticality of the input.Then I discussed how probabilistic models arebeing applied in the fields of sentence processingand language acquisition.
Here I believe we?re atthe beginning of a very fruitful period of inter-action between empirical research and computa-tional modeling, with insights and results flowingboth ways.But what does all this mean for mainstreamcomputational linguistics?
Can we expect theo-retical linguistics to play a larger role in compu-tational linguistics in the near future?
If by com-putational linguistics we mean the NLP engineer-ing applications that typically receive the bulk ofthe attention at today?s Computational Linguisticsconferences, I?m not so sure.
While it?s reasonableto expect that better scientific theories of how hu-mans understand language will help us build bettercomputational systems that do the same, I think weshould remember that our machines can do thingsthat no human can (e.g., count all the 5-grams interabytes of data), and so our engineering solu-tions may differ considerably from the algorithmsand procedures used by humans.
But I think it?salso reasonable to hope that the interdisciplinary9work involving statistics, computational models,psycholinguistics and language acquisition that Imentioned in the paper will produce new insightsinto how language is acquired and used.AcknowledgmentsI?d like to thank Eugene Charniak and AntskeFokkens for stimulating discussion and helpfulcomments on an earlier draft.
Of course all opin-ions expressed here are my own.ReferencesSteven Abney.
1997.
Stochastic Attribute-Value Grammars.Computational Linguistics, 23(4):597?617.A.
Albright and B. Hayes.
2003.
Rules vs. analogy inEnglish past tenses: a computational/experimental study.Cognition, 90:118?161.Asaf Bachrach.
2008.
Imaging Neural Correlates of Syn-tactic Complexity in a Naturalistic Context.
Ph.D. thesis,Massachusetts Institute of Technology, Cambridge, Mas-sachusetts.P.
Boersma and B. Hayes.
2001.
Empirical tests of the grad-ual learning algorithm.
Linguistic Inquiry, 32(1):45?86.Joan Bresnan.
1982.
Control and complementation.
In JoanBresnan, editor, The Mental Representation of Grammati-cal Relations, pages 282?390.
The MIT Press, Cambridge,Massachusetts.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-finen-best parsing and MaxEnt discriminative reranking.
InProceedings of the 43rd Annual Meeting of the Associa-tion for Computational Linguistics, pages 173?180, AnnArbor, Michigan, June.
Association for ComputationalLinguistics.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In The Proceedings of the North American Chapterof the Association for Computational Linguistics, pages132?139.Noam Chomsky.
1965.
Aspects of the Theory of Syntax.
TheMIT Press, Cambridge, Massachusetts.Michael Collins and Terry Koo.
2005.
Discriminativereranking for natural language parsing.
ComputationalLinguistics, 31(1):25?70.Michael Collins.
2003.
Head-driven statistical modelsfor natural language parsing.
Computational Linguistics,29(4):589?638.Michael C. Frank, Sharon Goldwater, Vikash Mansinghka,Tom Griffiths, and Joshua Tenenbaum.
2007.
Model-ing human performance on statistical word segmentationtasks.
In Proceedings of the 29th Annual Meeting of theCognitive Science Society.Michael C. Frank, Noah Goodman, and Joshua Tenenbaum.to appear.
Using speakers referential intentions to modelearly cross-situational word learning.
Psychological Sci-ence.Gerald Gazdar, Ewan Klein, Geoffrey Pullum, and Ivan Sag.1985.
Generalized Phrase Structure Grammar.
BasilBlackwell, Oxford.J.
Goldsmith.
2001.
Unsupervised learning of the morphol-ogy of a natural language.
Computational Linguistics,27:153?198.Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson.2007.
Distributional cues to word boundaries: Context isimportant.
In David Bamman, Tatiana Magnitskaia, andColleen Zaller, editors, Proceedings of the 31st AnnualBoston University Conference on Language Development,pages 239?250, Somerville, MA.
Cascadilla Press.J.
Goodman.
1998.
Parsing inside-out.
Ph.D.thesis, Harvard University.
available fromhttp://research.microsoft.com/?joshuago/.John Hale.
2006.
Uncertainty about the rest of the sentence.Cognitive Science, 30:643?672.Fred Jelinek.
2004.
Stochastic analysis of structured lan-guage modeling.
In Mark Johnson, Sanjeev P. Khudan-pur, Mari Ostendorf, and Roni Rosenfeld, editors, Mathe-matical Foundations of Speech and Language Processing,pages 37?72.
Springer, New York.Mark Johnson and Eugene Charniak.
2004.
A TAG-basednoisy channel model of speech repairs.
In Proceedings ofthe 42nd Annual Meeting of the Association for Computa-tional Linguistics, pages 33?39.Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,and Stefan Riezler.
1999.
Estimators for stochastic?unification-based?
grammars.
In The Proceedings of the37th Annual Conference of the Association for Computa-tional Linguistics, pages 535?541, San Francisco.
MorganKaufmann.Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-ter.
2007.
Adaptor Grammars: A framework for speci-fying compositional nonparametric Bayesian models.
InB.
Scho?lkopf, J. Platt, and T. Hoffman, editors, Advancesin Neural Information Processing Systems 19, pages 641?648.
MIT Press, Cambridge, MA.Dan Klein and Chris Manning.
2004.
Corpus-based in-duction of syntactic structure: Models of dependency andconstituency.
In Proceedings of the 42nd Annual Meetingof the Association for Computational Linguistics, pages478?485.Roger Levy.
2008.
Expectation-based syntactic comprehen-sion.
Cognition, 106:1126?1177.John W. Lloyd.
1987.
Foundations of Logic Programming.Springer, Berlin, 2 edition.David M. Magerman.
1995.
Statistical decision-tree mod-els for parsing.
In The Proceedings of the 33rd AnnualMeeting of the Association for Computational Linguistics,pages 276?283, San Francisco.
The Association for Com-putational Linguistics, Morgan Kaufman.Michell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated corpusof English: The Penn Treebank.
Computational Linguis-tics, 19(2):313?330.David Marr.
1982.
Vision.
W.H.
Freeman and Company,New York.10Matha Palmer, Daniel Gildea, and Paul Kingsbury.
2005.The Proposition Bank: An annotated corpus of semanticroles.
Computational Linguistics, 31(1):71?106.Joe Pater.
2008.
Gradual learning and convergence.
Linguis-tic Inquiry, 30(2):334?345.Judea Pearl.
1988.
Probabalistic Reasoning in IntelligentSystems: Networks of Plausible Inference.
Morgan Kauf-mann, San Mateo, California.Slav Petrov and Dan Klein.
2007.
Improved inference forunlexicalized parsing.
In Human Language Technologies2007: The Conference of the North American Chapter ofthe Association for Computational Linguistics; Proceed-ings of the Main Conference, pages 404?411, Rochester,New York, April.
Association for Computational Linguis-tics.Geoffrey K. Pullum.
2007.
Ungrammaticality, rarity, andcorpus use.
Corpus Linguistics and Linguistic Theory,3:33?47.Brian Roark.
2001.
Probabilistic top-down parsing and lan-guage modeling.
Computational Linguistics, 27(2):249?276.Stuart M. Shieber.
1986.
An Introduction to Unification-based Approaches to Grammar.
CSLI Lecture Notes Se-ries.
Chicago University Press, Chicago.Paul Smolensky and Ge?raldine Legendre.
2005.
The Har-monic Mind: From Neural Computation To Optimality-Theoretic Grammar.
The MIT Press.11
