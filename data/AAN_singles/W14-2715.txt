Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 109?117,Baltimore, Maryland USA, 27 June 2014.c?2014 Association for Computational LinguisticsCollective Stance Classification of Posts in Online Debate ForumsDhanya SridharComputer Science Dept.UC Santa Cruzdsridhar@soe.ucsc.eduLise GetoorComputer Science Dept.UC Santa Cruzgetoor@soe.ucsc.eduMarilyn WalkerComputer Science Dept.UC Santa Cruzmaw@soe.ucsc.eduAbstractOnline debate sites are a large source ofinformal and opinion-sharing dialogue oncurrent socio-political issues.
Inferringusers?
stance (PRO or CON) towards dis-cussion topics in domains such as politicsor news is an important problem, and isof utility to researchers, government or-ganizations, and companies.
Predictingusers?
stance supports identification of so-cial and political groups, building of betterrecommender systems, and personaliza-tion of users?
information preferences totheir ideological beliefs.
In this paper, wedevelop a novel collective classificationapproach to stance classification, whichmakes use of both structural and linguis-tic features, and which collectively labelsthe posts?
stance across a network of theusers?
posts.
We identify both linguisticfeatures of the posts and features that cap-ture the underlying relationships betweenposts and users.
We use probabilistic softlogic (PSL) (Bach et al., 2013) to modelpost stance by leveraging both these locallinguistic features as well as the observednetwork structure of the posts to reasonover the dataset.
We evaluate our approachon 4FORUMS (Walker et al., 2012b), a col-lection of discussions from an online de-bate site on issues ranging from gun con-trol to gay marriage.
We show that our col-lective classification model is able to eas-ily incorporate rich, relational informationand outperforms a local model which usesonly linguistic information.1 IntroductionModeling user stance (PRO, CON) in discussiontopics in online social media debate is of inter-est to researchers, corporations and governmentalorganizations alike.
Predicting a user?s stance to-wards a given issue can support the identificationof social or political groups (Gawron et al., 2012;Abu-Jbara et al., 2012; Anand et al., 2011; Qiu etal., 2013; Hasan and Ng, 2013), help develop bet-ter recommendation systems, or tailor users?
infor-mation preferences to their ideologies and beliefs.Stance classification problems consist of a collec-tion of debate-style discussions by authors on dif-ferent controversial, political topics.While these may be spoken as in the Congres-sional Debates corpus (Thomas et al., 2006; Bur-foot, 2008), we focus on forum posts on socialmedia debate sites.
Users on debate sites sharetheir opinions freely, using informal and sociallanguage, providing a rich and much more chal-lenging domain for stance prediction.Social media debate sites contain online discus-sions with posts from various authors, where eachpost is either a response to another post or the rootof the discussion (Anand et al., 2011; Walker etal., 2012a).
Posts are linked to one another by ei-ther rebuttal or agreement links and are labelledfor stance, either PRO or CON, depending on theframing of the issue under discussion.
Each postreflects the stance and sentiment of its author.
Au-thors may participate in multiple discussions in thesame topic, and may discuss multiple topics.
Forexample consider the sample posts from the onlinediscussion forum 4forums.com shown in Fig.1.
Here, we see discussion topics, together withsample quotes and responses, where the responseis a direct reply to the quote text.
The annotationsfor stance were gathered using Amazon?s Mechan-ical Turk service with an interface that allowed an-notators to see complete discussions.
Quotes pro-vide additional context that were used by humanannotators in a separate task for annotating agree-ment and disagreement (Misra and Walker, 2013).Responses can be labeled as either PRO or CON to-ward the topic.
For the example shown in Fig.
1,109Quote Q, Response R Stance TopicQ: I thought I?d start a new thread for those newcomers who don?t want to be shocked by sickminded nazi XXXX.
Anyway...
When are fetuses really alive, and how many fetuses are actuallyaborted (murdered) before that time?R: The heart starts beating 3 weeks after conception, and you can?t live without a beating heart,but me personally, I think that as soon as the miracle starts, (egg and sperm combine) that iswhen life begins.
I know it?s more of a spiritual thing for me instead of a fact.
:)CON AbortionQ2: Most americans support a Federal Marriage Amendment.
Defining Marriage as a unionbetween a man and a woman.
Federal Marriage Amendment.
This is the text of the Amend:Marriage in the United States shall consist only of the union of a man and a woman.
Neitherthis constitution or the constitution of any state, nor state or federal law, shall be construed torequire that marital status or the legal incidents thereof be conferred upon unmarried couples orgroups.R2: Debator, why does it bother you so much that some people are gay?
Its a sexual prefference.People like certain things when they have sex.
Example: A man likes a women with small boobs.Or, a man likes a women with nice legs.
People like the way certain things feel (I?m not givingte example for that one;) ).
So why does it bother people that someone?s sexual prefference isjust a little kinkier than thiers?PRO GayMar-riageFigure 1: Sample Quote/Response Pair from 4forums.com with Mechanical Turk annotations for stance.Both response posts are from the same author.both response posts are from the same author.
Wedescribe the dataset further in Section 4.1.We believe that models of post stance in on-line debate should capture both the content and thecontext of author posts.
By jointly reasoning overboth the content of the post and its relationshipswith other posts in the discussion, we perform col-lective classification, as we further define in Sec-tion 3 (Sen et al., 2008).
Previous work has shownthat collective classification models often performbetter than content-only approaches.
(Burfoot etal., 2011; Hasan and Ng, 2013; Thomas et al.,2006; Bansal et al., 2008; Walker et al., 2012c).Here, we develop a collective classification ap-proach for stance prediction which leverages thesentiment conveyed in a post through its language,and the reply links consisting of agreements or re-buttals between posts in a discussion.
We imple-ment our approach using Probabilistic Soft Logic(PSL) (Bach et al., 2013), a recently introducedtool for collective inference in relational data.
Weevaluate our model on data from the 4FORUMSonline debate site (Walker et al., 2012b).Section 2 first presents an overview of our ap-proach and then in Section 3.1 we describe thePSL framework in more detail.
Section 4 de-scribes the evaluation data and our results show-ing that the PSL model improves prediction of poststance in the 4Forums dataset.
In Section 5 wedescribe related work, and compare with our pro-posed approach.
Section 6 summarizes our ap-proach and results.2 Overview of ApproachGiven a set of topics {t1.
.
.
tn}, where each topicticonsists of a set of discussions {di1.
.
.
dij}, wemodel each discussion dkas a collection of posts{pk0, .
.
.
, pkm}, where each post pkiis mapped toits author ai.A discussion di?
D is a tree of posts, startingwith the initial post pi0.
We distinguish betweenposts that start a new thread (root) and others (non-root).
Each non-root post pijis the response tosome previous post pik, where k < j, and we referto pikas the parent of pij.
For a subset of the posts,pijhas been annotated with a real valued numberin the interval [?5, 5] that denotes whether the postdisagrees or agrees with its parent.
Values ?
0 areconsidered disagreement and values?
1, as agree-ment.
We discard the posts where the annotationsare in the interval (0, 1) since those indicate highannotator uncertainty about agreement.Fig.
2 illustrates an example of three discussiontrees for two topics where author a2participatesin multiple discussions of the same topic and a3and a4participate in multiple topics.
An authordirectly replies with a post to another author?s postand either disagrees or agrees.Each post pijin discussion diis also mapped to{xij1, .
.
.
, xijN} linguistic features as described inSection 3.2.1 as well as yij, the stance label (PRO,CON) towards the discussion topic ti.We say that ajparticipates in topic tiif thereexist any posts pj?
diwith author aj.Using the tree structure and posts that have an-notations for agreement or disagreement, we con-110Figure 2: Example of 3 discussions in (a), (b) and (c).
Dotted lines denote the ?writes?
relation betweenauthors and posts and dashed lines denote the ?disagrees?
relation between posts and between authors.Authors can participate in multiple discussions of the same topic, shown by a2in both (a) and (b).Moreover, authors may post in multiple topics, as shown by a3and a4in both (b) and (c), and mayinteract with the same authors multiple times, as shown again in (b) and (c).sider the network graph G of disagreement andagreement between posts and between authors,where the vertices are posts {p0, .
.
.
, pm} and au-thors {a0, .
.
.
, an}.
A disagreement edge existsfrom post puto pvif pudisagrees with pv.A disagreement edge exists from awto ayif anyof the posts {pw, .
.
.
, px} mapped to awdisagreewith any posts {py, .
.
.
pz}mapped to ay.
We sim-ilarly define agreement edges for both posts andauthors.3 Collective Classification of StanceGiven the discussion structure defined in the pre-vious section, our task is to infer the stance of eachpost.
We make use of both linguistic features andthe relational structure in order to collectively orjointly infer the stance labels.
This corresponds toa collective classification setting (Sen et al., 2008),in which we are given a multi-relational networkand some partially observed labels, and we wishto infer all of the unobserved labels, conditionedon observed attributes and links.
Collective clas-sification refers to the combined classification ofa set of interdependent objects (posts, in our do-main) using information given by both the localfeatures of the objects and the properties of theobjects?
neighbors in a network.
For the stanceclassification problem, we infer stance labels forposts using both the correlation between a post andits linguistic attributes {xij1, .
.
.
, xijN}, and thelabels and attributes of its neighbors in observednetwork graph G. We use PSL, described below,to perform collective classification.3.1 Probabilistic Soft LogicProbabilistic soft logic (PSL) is a framework forprobabilistic modeling and collective reasoning inrelational domains (Kimmig et al., 2012; Bach etal., 2013).
PSL provides a declarative syntax anduses first-order logic to define a templated undi-rected graphical model over continuous randomvariables.
Like other statistical relational learn-ing methods, dependencies in the domain are cap-tured by constructing rules with weights that canbe learned from data.But unlike other statistical relational learningmethods, PSL relaxes boolean truth values foratoms in the domain to soft truth values in the in-terval [0,1].
In this setting, finding the most proba-ble explanation (MPE), a joint assignment of truthvalues to all random variable ground atoms, can bedone efficiently.For example, a typical PSL rule looks like thefollowing:P (A,B) ?Q(B,C)?
R(A,C)where P, Q and R are predicates that representobserved or unobserved attributes in the domain,and A, B, and C are variables.
For example, inour 4FORUMS domain, we consider an observedattribute such as writesPost(A, P) and infer an un-observed attribute (or label) such as isProPost(P,T).
Instantiation of predicates with data is calledgrounding (e.g.
writesPost(A2, P7)), and eachground predicate, often called ground atom, has asoft truth value in the interval [0,1].
To build a PSLmodel for stance classification, we represent posts111isProPost(P, T) ?
writesPost(A, P) ?
isProAuth(A, T)?
isProPost(P, T) ?
writesPost(A, P) ?
?
isProAuth(A, T)agreesPost(P, P2) ?
isProPost(P, T) ?
isProPost(P2, T)agreesPost(P, P2) ??
isProPost(P, T) ?
?
isProPost(P2, T)disagreesPost(P, P2) ?
isProPost(P, T) ?
?
isProPost(P2, T)disagreesPost(P, P2) ??
isProPost(P, T) ?
isProPost(P2, T)agreesAuth(A, A2) ?
isProAuth(A, T) ?
isProAuth(A, T)agreesAuth(A, A2) ??
isProAuth(A, T) ?
?
isProAuth(A2, T)disagreesAuth(A, A2) ?
isProAuth(A, T) ?
?
isProAuth(A2, T)disagreesAuth(A, A2) ??
isProAuth(A, T) ?
isProAuth(A2, T)hasLabelPro(P, T) ?
isProPost(P, T)?
hasLabelPro(P, T) ?
?
isProPost(P, T)Table 1: Rules for PSL model, where P = post, T = Topic, and A = Author.and authors as variables and specify predicates toencode different interactions, such as writes, be-tween them.
Domain knowledge is captured bywriting rules with weights that govern the rela-tive importance of the dependencies between pred-icates.
The groundings of all the rules result inan undirected graphical model that represents thejoint probability distribution of assignments for allunobserved atoms, conditioned on the observedatoms.Triangular norms, which are continuous relax-ations of logical AND and OR, are used to com-bine the atoms in the first-order clauses.
As aresult of the soft truth values and the triangu-lar norms, the underlying probabilistic model isa hinge-loss Markov Random Field (HL-MRF).Inference in HL-MRFs is a convex optimization,which leads to a significant improvement in effi-ciency over discrete probabilistic graphical mod-els.
Thus, PSL offers a very natural interface tocompactly represent stance classification as a col-lective classification problem, along with methodsto reason about our domain.3.2 FeaturesWe extract both linguistic features that capture thecontent of a post and features that capture multiplerelations from our dataset.3.2.1 Linguistic FeaturesTo capture the content of a post, on top of a bag-of-words representation with unigrams and bigrams,we also consider basic lengths, discourse cues,repeated punctuation counts and counts of lex-ical categories based on the Linguistic Inquiryand Word Count tool (LIWC) (Pennebaker et al.,2001).
Basic length features capture the numberof sentences, words, and characters, along withthe average word and sentence lengths for eachpost.
The discourse cues feature captures fre-quency counts for the first few words of the post,which often contain discourse cues.
To capturethe information in repeated punctuation like ?!!?,????
or ??!?
we include the frequency count of thegiven punctuation patterns as a feature of each post(Anand et al., 2011).
LIWC counts capture senti-ment by giving the degree to which the post usescertain categories of subjective language.3.2.2 Relational InformationAs our problem domain contains relations be-tween both authors and posts, for our PSL model,we consider the relations between authors, be-tween posts and between authors and posts.
As de-scribed above, in PSL, we model these relations asfirst-order predicates.
In Section 3.3, we describehow we populate the predicates with observationsfrom our data.Author Information We observe that authorsparticipate in discussions by writing posts.
Fora subset of authors, we have annotations for theirinteractions with other authors as either disagree-ment or agreement, as given by network graphG.
We encode this with the following predi-cates: writesPost(A, P), disagreesAuth(A1, A2),agreesAuth(A1, A2).Post Information Posts are linked to the topicof their given discussion, and to other posts intheir discussion through disagreement or agree-ment.
Additionally, we include a predicate for poststance towards its topic as predicted by a classifier112that only uses linguistic features, as described inSection 3.3, as prior information.
We capture theserelations with the following predicates: hasLabel-Pro(P, T), hasTopic(P, T), disagreesPost(P1, P2),agreesPost(P1, P2).3.2.3 Target attributesOur goal is to 1) predict the stance relation be-tween a post and its topic, namely, PRO or CON and2) predict the stance relation between an authorand a topic.
In our PSL model, our target predi-cates are isProPost(P, T) and isProAuth(A, T).3.3 PSL ModelWe construct our collective stance classificationmodel in PSL using the predicates listed above.For disagreement/agreement annotations in the in-terval [-5, 5], we consider values [-5,0] as evidencefor the disagreesAuth relation and values [1, 5] asevidence for the agreesAuth relation.
We discardobservations with annotations in the interval [0,1]because it indicates a very weak signal of agree-ment, which is already rare on debate sites.
Wepopulate disagreesPost and agreesPost in the sameway as described above.For each relation, we populate the correspond-ing predicate with all the instances that we observein data and we fix the truth value of each observa-tion as 1.
For all such predicates where we observeinstances in the data, we say that the predicate isclosed.
For the relations isPostPro and isAuthProthat we predict through inference, a truth value of1 denotes a PRO stance and a truth value of 0 de-notes a CON stance.
We say that those predicatesare open, and the goal of inference is to jointly as-sign truth values to groundings of those predicates.We use our domain knowledge to describe rulesthat relate these predicates to one another.
We fol-low our intuition that agreement between nodesimplies that they have the same stance, and dis-agreement between nodes implies that they haveopposite stances.
We relate post and author nodesto each other by supposing that if a post is PROtowards its topic, then its author will also be PROtowards that topic.We construct a classifier that takes as input thelinguistic features of the posts and outputs predic-tions for stance label of each post.
We then con-sider the labels predicted by the local classifier asa prior for the inference of the target attributes inour PSL model.
Table 1 shows the rules in ourPSL model.Topic Authors PostsAbortion 385 8114Evolution 325 6186Gun Control 319 3899Gay Marriage 316 7025Death Penalty 170 572Table 2: Overview of topics in 4FORUMSdataset.4 Experimental EvaluationWe first describe the dataset we use for evaluationand then describe our evaluation method and re-sults.4.1 DatasetWe evaluate our proposed approach on discus-sions from https://www.4forums.com, anonline debate site on social and political issues.The dataset is publicly available as part of theInternet Argument Corpus, an annotated collec-tion of 109,533 forum posts (Walker et al., 2012b;Walker et al., 2012c).
On 4FORUMS, a user ini-tiates a discussion by posting a new question orcomment under a topic, or participate in an ongo-ing discussion by replying to any of the posts inthe thread.
The discussions were given to Englishspeaking Mechanical Turk annotators for a num-ber of annotation tasks to get labels for the stancesof discussion participants towards the topic, andscores for each post in a discussion indicatingwhether it is in agreement or disagreement withthe preceding post.The scores for agreement and disagreementwere on a 11 point scale [-5, 5] implemented usinga slider, and annotators were given quote/responsepairs to determine if the response text agreedor disagreed with the quote text.
We use themean score across the 5-7 annotators used in thetask.
A more negative value indicates higherinter-annotator confidence of disagreement, and amore positive value indicates higher confidence ofagreement.
The gold-standard annotation used forthe stance label of each post is given by the ma-jority annotation among 3-8 Mechanical Turk an-notators performed as a separate task, using en-tire discussions to determine the stance of the au-thors in the discussion towards the topic.
We usethe stance of each post?s author to determine thepost?s stance.
For our experiments, we use allposts with annotations for stance, and about 90%of these posts also have annotations for agree-113ment/disagreement.The discussions span many topics, and Table 2gives a summary of the topics we consider in ourexperiments and the distribution of posts acrossthese topics.
Each post in a discussion comes asa quote-response pair, where the quote is the textthat the post is in response to, and the response isthe post text.
We refer to (Walker et al., 2012b) fora full description of the corpus and the annotationprocess.4.2 EvaluationIn order to evaluate our methods, we split thedataset into training and testing sets by randomlyselecting half the authors from each topic and theirposts for the training set and using the remainingauthors and their posts for the test set.
This way,we ensure that no two authors appear in both train-ing and test sets for the same topic, since stanceis topic-dependent.
We create 10 randomly sam-pled train/test splits for evaluation.
Each split con-tains about 18,000 posts.
For each train/test split,we train a linear SVM for each topic, with theL2-regularized-L1-loss SVM implemented in theLibLINEAR package (Fan et al., 2008).
We useonly the linguistic features from the posts, for eachtopic in the training set.
We refer to the baselinemodel which only uses the the output of the SVMas the LOCAL model.
We output the predictionsfrom LOCAL model and get stance labels for postsin both the training and test sets.
We use the pre-dictions as prior information for the true stance la-bel in our PSL model, with the hasLabel predicate.We use the gold standard stance annotation(PRO, CON) for each post as ground truth forweight learning and inference.
A truth value of 1for isPostPro and isAuthPro denotes a PRO stanceand a truth value of 0 denotes a CON stance.
Welearn the weights of our PSL model (initially set to1) for each of our training sets and perform infer-ence on each of the test sets.Table 3 shows averages for F1 score for the pos-itive class (PRO), area under the precision-recallcurve (AUC-PR) for the negative class (CON) andarea under the ROC curve (AUROC) over the 10train/test splits.
For the PSL model, the measuresare computed for joint inference over all topicsin the test sets.
For the per-topic linear SVMs(LOCAL model), we compute the measures indi-vidually for the predictions of each topic in thetest sets and take a weighted average over thetopics.
Our PSL model outperforms the LOCALmodel, with statistically significant improvementsin the F1 score and AUC-PR for the negative class.Moreover, our model completes weight learningand inference on the order of seconds, boasting anadvantage in computational efficiency, while alsomaintaining model interpretability.Table 4 shows the weights learned by the PSLmodel for the rules in one of the train/test splitsof the experiment.
The first two rules relatingpost stance and author stance are weighted moreheavily, in part because the writesPost predicatehas a grounding for each author-post pair and con-tributes to lots of groundings of the rule.
The rulesthat capture the alternating disagreement stancealso have significant weight, while the rules denot-ing agreement both between posts and between au-thors are weighted least heavily since there are farfewer instances of agreement than disagreement.This matches our intuition of political debates.We also explored variations of the PSL modelby removing the first two rules relating post stanceand author stance and found that the weight learn-ing algorithm drove the weights of the otherrules close to 0, worsening the performance.We also removed rules 3-10 that capture agree-ment/disagreement from the model, and found thatthe model performs poorly when disregarding thelinks between nodes entirely.
The PSL modellearns to weight the first and second rule veryhighly, and does worse than when considering theprior alone.
Thus, the combination of the rulesgives the model its advantage, allowing the PSLmodel to make use of a richer structure that hasmultiple types of relations and more information.5 Related WorkOver the last ten years, there has been significantprogress on modeling stance.
Previous work cov-ers three different debate settings: (1) congres-sional debates (Thomas et al., 2006; Bansal etal., 2008; Yessenalina et al., 2010; Balahur et al.,2009); (2) company-internal discussion sites (Mu-rakami and Raymond, 2010; Agrawal et al., 2003);and (3) online social and political public forums(Somasundaran and Wiebe, 2009; Somasundaranand Wiebe, 2010; Wang and Ros?e, 2010; Biranand Rambow, 2011; Walker et al., 2012c; Anandet al., 2011).
Debates in online public forums(e.g.
Fig.
1) differ from debates in congress andon company discussion sites because the posts are114Classifier F1 Score AUC-PR negative class AUROCLOCAL 0.66 ?
0.015 0.44 ?
0.04 0.54 ?
0.02PSL 0.74 ?
0.04 0.511 ?
0.04 0.59 ?
0.05Table 3: Averages and standard deviations for F1 score for the positive class, area under PR curve for thenegative class, and area under ROC curve for post stance over 10 train/test splits.isProPost(P, T) ?
writesPost(A, P) ?
isProAuth(A, T) : 10.2?
isProPost(P, T) ?
writesPost(A, P) ?
?
isProAuth(A, T) : 8.5agreesPost(P, P2) ?
isProPost(P, T) ?
isProPost(P2, T) : 0.003agreesPost(P, P2) ??
isProPost(P, T) ?
?
isProPost(P2, T) : 0.003disagreesPost(P, P2) ?
isProPost(P, T) ?
?
isProPost(P2, T) : 0.06disagreesPost(P, P2) ??
isProPost(P, T) ?
isProPost(P2, T) : 0.11agreesAuth(A, A2) ?
isProAuth(A, T) ?
isProAuth(A, T) : 0.001agreesAuth(A, A2) ??
isProAuth(A, T) ?
?
isProAuth(A2, T) : 0.0disagreesAuth(A, A2) ?
isProAuth(A, T) ?
?
isProAuth(A2, T) : 0.23disagreesAuth(A, A2) ??
isProAuth(A, T) ?
isProAuth(A2, T) : 0.6hasLabelPro(P, T) ?
isProPost(P, T) : 2.2?
hasLabelPro(P, T) ?
?
isProPost(P, T) : 4.8Table 4: Weights learned by the model for the PSL rules in train/test split 2 of experimentsshorter and the language is more informal and so-cial.
We predict that this difference makes it moredifficult to achieve accuracies as high for 4FO-RUMS discussions as can be achieved for the con-gressional debates corpus.Work by (Somasundaran and Wiebe, 2009) onidealogical debates very similar to our own showthat identifying argumentation structure improvesperformance; their best performance is approxi-mately 64% accuracy over all topics.
Research by(Thomas et al., 2006; Bansal et al., 2008; Yesse-nalina et al., 2010; Balahur et al., 2009) classifiesthe speaker?s stance in a corpus of congressionalfloor debates.
This work combines graph-basedand text-classification approaches to achieve 75%accuracy on Congressional debate siding over alltopics.
Other work applies MaxCut to the re-ply structure of company discussion forums (Mal-ouf and Mullen, 2008; Murakami and Raymond,2010; Agrawal et al., 2003).
Murakami & Ray-mond (2010) show that rules for identifying agree-ment, defined on the textual content of the postimprove performance.More recent work has explicitly focused on thebenefits of collective classification in these set-tings (Burfoot et al., 2011; Hasan and Ng, 2013;Walker et al., 2012c), and has shown in eachcase that collective classification improves perfor-mance.
The results reported here are the first toapply the PSL collective classification frameworkto the forums conversations from the IAC corpus(Anand et al., 2011; Walker et al., 2012c).6 Discussion and Future WorkHere, we introduce a novel approach to classifystance of posts from online debate forums with acollective classification framework.
We formallyconstruct a model, using PSL, to capture relationalinformation in the network of authors and postsand our intuition that agreement or disagreementbetween users correlates to their stance towards atopic.
Our initial results are promising, showingthat by incorporating more complex interactionsbetween authors and posts, we gain improvementsover a content-only approach.
Our approach isideally suited to collective inference in social me-dia.
It easily extendable to use additional rela-tional information, and richer behavioral and lin-guistic information.AcknowledgmentsThanks to Pranav Anand for providing us with thestance annotations for the 4forums dataset.
Thiswork is supported by National Science Foundationunder Grant Nos.
IIS1218488, CCF0937094 andCISE-RI 1302668.115ReferencesAmjad Abu-Jbara, Mona Diab, Pradeep Dasigi, andDragomir Radev.
2012.
Subgroup detection in ide-ological discussions.
In Association for Computa-tional Linguistics (ACL), pages 399?409.R.
Agrawal, S. Rajagopalan, R. Srikant, and Y. Xu.2003.
Mining newsgroups using networks arisingfrom social behavior.
In International Conferenceon World Wide Web (WWW), pages 529?535.
ACM.Pranav Anand, Marilyn Walker, Rob Abbott, Jean E.Fox Tree, Robeson Bowmani, and Michael Minor.2011.
Cats Rule and Dogs Drool: ClassifyingStance in Online Debate.
In ACL Workshop on Sen-timent and Subjectivity.Stephen H. Bach, Bert Huang, Ben London, and LiseGetoor.
2013.
Hinge-loss markov random fields:Convex inference for structured prediction.
In Un-certainty in Artificial Intelligence (UAI).A.
Balahur, Z. Kozareva, and A. Montoyo.
2009.
De-termining the polarity and source of opinions ex-pressed in political debates.
Computational Linguis-tics and Intelligent Text Processing, pages 468?480.M.
Bansal, C. Cardie, and L. Lee.
2008.
The powerof negative thinking: Exploiting label disagreementin the min-cut classification framework.
COLING,pages 13?16.O.
Biran and O. Rambow.
2011.
Identifying justifi-cations in written dialogs.
In IEEE InternationalConference on Semantic Computing (ICSC), pages162?168.Clinton Burfoot, Steven Bird, and Timothy Baldwin.2011.
Collective classification of congressionalfloor-debate transcripts.
In Association for Compu-tational Linguistics (ACL), pages 1506?1515.C.
Burfoot.
2008.
Using multiple sources of agree-ment information for sentiment classification of po-litical transcripts.
In Australasian Language Tech-nology Association Workshop, volume 6, pages 11?18.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
LIBLINEAR:A library for large linear classification.
Journal ofMachine Learning Research, 9:1871?1874.J.M.
Gawron, D. Gupta, K. Stephens, M.H.
Tsou,B.
Spitzberg, and L. An.
2012.
Using group mem-bership markers for group identification in web logs.In AAAI Conference on Weblogs and Social Media(ICWSM).Kazi Saidul Hasan and Vincent Ng.
2013.
Stance clas-sification of ideological debates: Data, models, fea-tures, and constraints.
International Joint Confer-ence on Natural Language Processing.Angelika Kimmig, Stephen H. Bach, MatthiasBroecheler, Bert Huang, and Lise Getoor.
2012.A short introduction to probabilistic soft logic.In NIPS Workshop on Probabilistic Programming:Foundations and Applications.R.
Malouf and T. Mullen.
2008.
Taking sides: Userclassification for informal online political discourse.Internet Research, 18(2):177?190.Amita Misra and Marilyn A Walker.
2013.
Topic in-dependent identification of agreement and disagree-ment in social media dialogue.
In Conference of theSpecial Interest Group on Discourse and Dialogue,page 920.A.
Murakami and R. Raymond.
2010.
Support or Op-pose?
Classifying Positions in Online Debates fromReply Activities and Opinion Expressions.
In Inter-national Conference on Computational Linguistics(ACL), pages 869?875.J.
W. Pennebaker, L. E. Francis, and R. J. Booth, 2001.LIWC: Linguistic Inquiry and Word Count.Minghui Qiu, Liu Yang, and Jing Jiang.
2013.
Mod-eling interaction features for debate side clustering.In ACM International Conference on Information &Knowledge Management (CIKM), pages 873?878.Prithviraj Sen, Galileo Mark Namata, Mustafa Bilgic,Lise Getoor, Brian Gallagher, and Tina Eliassi-Rad.2008.
Collective classification in network data.
AIMagazine, 29(3):93?106.S.
Somasundaran and J. Wiebe.
2009.
Recogniz-ing stances in online debates.
In ACL and AFNLP,pages 226?234.S.
Somasundaran and J. Wiebe.
2010.
Recognizingstances in ideological on-line debates.
In NAACLHLT 2010 Workshop on Computational Approachesto Analysis and Generation of Emotion in Text,pages 116?124.M.
Thomas, B. Pang, and L. Lee.
2006.
Get out thevote: Determining support or opposition from Con-gressional floor-debate transcripts.
In Conferenceon Empirical Methods in Natural Language Pro-cessing (EMNLP), pages 327?335.Marilyn Walker, Pranav Anand, Rob Abbott, Jean E.Fox Tree, Craig Martell, and Joseph King.
2012a.That?s your evidence?
: Classifying stance in onlinepolitical debate.
Decision Support Sciences.Marilyn Walker, Pranav Anand, Robert Abbott, andJean E. Fox Tree.
2012b.
A corpus for researchon deliberation and debate.
In Language Resourcesand Evaluation Conference, LREC2012.Marilyn Walker, Pranav Anand, Robert Abbott, andRichard Grant.
2012c.
Stance classification usingdialogic properties of persuasion.
In Meeting of theNorth American Association for Computational Lin-guistics.
NAACL-HLT12.116Y.C.
Wang and C.P.
Ros?e.
2010.
Making conversa-tional structure explicit: identification of initiation-response pairs within online discussions.
In HumanLanguage Technologies: The 2010 Annual Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics, pages 673?676.A.
Yessenalina, Y. Yue, and C. Cardie.
2010.Multi-level structured models for document-levelsentiment classification.
In Conference on Em-pirical Methods in Natural Language Processing(EMNLP), pages 1046?1056.117
