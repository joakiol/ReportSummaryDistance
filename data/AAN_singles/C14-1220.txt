Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 2335?2344, Dublin, Ireland, August 23-29 2014.Relation Classification via Convolutional Deep Neural NetworkDaojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou and Jun ZhaoNational Laboratory of Pattern RecognitionInstitute of Automation, Chinese Academy of Sciences95 Zhongguancun East Road, Beijing 100190, China{djzeng,kliu,swlai,gyzhou,jzhao}@nlpr.ia.ac.cnAbstractThe state-of-the-art methods used for relation classification are primarily based on statistical ma-chine learning, and their performance strongly depends on the quality of the extracted features.The extracted features are often derived from the output of pre-existing natural language process-ing (NLP) systems, which leads to the propagation of the errors in the existing tools and hindersthe performance of these systems.
In this paper, we exploit a convolutional deep neural network(DNN) to extract lexical and sentence level features.
Our method takes all of the word tokens asinput without complicated pre-processing.
First, the word tokens are transformed to vectors bylooking up word embeddings1.
Then, lexical level features are extracted according to the givennouns.
Meanwhile, sentence level features are learned using a convolutional approach.
Thesetwo level features are concatenated to form the final extracted feature vector.
Finally, the fea-tures are fed into a softmax classifier to predict the relationship between two marked nouns.
Theexperimental results demonstrate that our approach significantly outperforms the state-of-the-artmethods.1 IntroductionThe task of relation classification is to predict semantic relations between pairs of nominals and canbe defined as follows: given a sentence S with the annotated pairs of nominals e1and e2, we aimto identify the relations between e1and e2(Hendrickx et al., 2010).
There is considerable interest inautomatic relation classification, both as an end in itself and as an intermediate step in a variety of NLPapplications.The most representative methods for relation classification use supervised paradigm; such methodshave been shown to be effective and yield relatively high performance (Zelenko et al., 2003; Bunescuand Mooney, 2005; Zhou et al., 2005; Mintz et al., 2009).
Supervised approaches are further dividedinto feature-based methods and kernel-based methods.
Feature-based methods use a set of features thatare selected after performing textual analysis.
They convert these features into symbolic IDs, which arethen transformed into a vector using a paradigm that is similar to the bag-of-words model2.
Conversely,kernel-based methods require pre-processed input data in the form of parse trees (such as dependencyparse trees).
These approaches are effective because they leverage a large body of linguistic knowledge.However, the extracted features or elaborately designed kernels are often derived from the output of pre-existing NLP systems, which leads to the propagation of the errors in the existing tools and hinders theperformance of such systems (Bach and Badaskar, 2007).
It is attractive to consider extracting featuresthat are as independent from existing NLP tools as possible.To identify the relations between pairs of nominals, it is necessary to a skillfully combine lexical andsentence level clues from diverse syntactic and semantic structures in a sentence.
For example, in thesentence ?The [fire]e1inside WTC was caused by exploding [fuel]e2?, to identify that fire and fuel are in aThis work is licenced under a Creative Commons Attribution 4.0 International License.Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1A word embedding is a distributed representation for a word.
For example, Collobert et al.
(2011) use a 50-dimensionalvector to represent a word.2http://en.wikipedia.org/wiki/Bag-of-words model2335Cause-Effect relationship, we usually leverage the marked nouns and the meanings of the entire sentence.In this paper, we exploit a convolutional DNN to extract lexical and sentence level features for relationclassification.
Our method takes all of the word tokens as input without complicated pre-processing,such as Part-of-Speech (POS) tagging and syntactic parsing.
First, all the word tokens are transformedinto vectors by looking up word embeddings.
Then, lexical level features are extracted according to thegiven nouns.
Meanwhile, sentence level features are learned using a convolutional approach.
These twolevel features are concatenated to form the final extracted feature vector.
Finally, the features are feedinto a softmax classifier to predict the relationship between two marked nouns.The idea of extracting features for NLP using convolutional DNN was previously explored by Col-lobert et al.
(2011), in the context of POS tagging, chunking (CHUNK), Named Entity Recogni-tion (NER) and Semantic Role Labeling (SRL).
Our work shares similar intuition with that of Collobertet al.
(2011).
In (Collobert et al., 2011), all of the tasks are considered as the sequential labeling prob-lems in which each word in the input sentence is given a tag.
However, our task, ?relation classification?,can be considered a multi-class classification problem, which results in a different objective function.Moreover, relation classification is defined as assigning relation labels to pairs of words.
It is thus nec-essary to specify which pairs of words to which we expect to assign relation labels.
For that purpose, theposition features (PF) are exploited to encode the relative distances to the target noun pairs.
To the bestof our knowledge, this work is the first example of using a convolutional DNN for relation classification.The contributions of this paper can be summarized as follows.?
We explore the feasibility of performing relation classification without complicated NLP pre-processing.
A convolutional DNN is employed to extract lexical and sentence level features.?
To specify pairs of words to which relation labels should be assigned, position features are proposedto encode the relative distances to the target noun pairs in the convolutional DNN.?
We conduct experiments using the SemEval-2010 Task 8 dataset.
The experimental results demon-strate that the proposed position features are critical for relation classification.
The extracted lexicaland sentence level features are effective for relation classification.
Our approach outperforms thestate-of-the-art methods.2 Related WorkRelation classification is one of the most important topics in NLP.
Many approaches have been exploredfor relation classification, including unsupervised relation discovery and supervised classification.
Re-searchers have proposed various features to identify the relations between nominals using different meth-ods.In the unsupervised paradigms, contextual features are used.
Distributional hypothesis theory (Harris,1954) indicates that words that occur in the same context tend to have similar meanings.
Accordingly, it isassumed that the pairs of nominals that occur in similar contexts tend to have similar relations.
Hasegawaet al.
(2004) adopted a hierarchical clustering method to cluster the contexts of nominals and simplyselected the most frequent words in the contexts to represent the relation between the nominals.
Chenet al.
(2005) proposed a novel unsupervised method based on model order selection and discriminativelabel identification to address this problem.In the supervised paradigm, relation classification is considered a multi-classification problem, and re-searchers concentrate on extracting more complex features.
Generally, these methods can be categorizedinto two types: feature-based and kernel-based.
In feature-based methods, a diverse set of strategieshave been exploited to convert the classification clues (such as sequences and parse trees) into featurevectors (Kambhatla, 2004; Suchanek et al., 2006).
Feature-based methods suffer from the problemof selecting a suitable feature set when converting the structured representation into feature vectors.Kernel-based methods provide a natural alternative to exploit rich representations of the input classifica-tion clues, such as syntactic parse trees.
Kernel-based methods allow the use of a large set of featureswithout explicitly extracting the features.
Various kernels, such as the convolution tree kernel (Qian et2336WordRepresentationFeatureExtractionOutput W3xFigure 1: Architecture of the neural network usedfor relation classification.WindowProcessingmax over timesConvolutiontanh W2xW1WFPFSentence levelFeaturesFigure 2: The framework used for extracting sen-tence level features.al., 2008), subsequence kernel (Mooney and Bunescu, 2005) and dependency tree kernel (Bunescu andMooney, 2005), have been proposed to solve the relation classification problem.
However, the methodsmentioned above suffer from a lack of sufficient labeled data for training.
Mintz et al.
(2009) proposeddistant supervision (DS) to address this problem.
The DS method selects sentences that match the factsin a knowledge base as positive examples.
The DS algorithm sometimes faces the problem of wronglabels, which results in noisy labeled data.
To address the shortcoming of DS, Riedel et al.
(2010) andHoffmann et al.
(2011) cast the relaxed DS assumption as multi-instance learning.
Furthermore, Taka-matsu et al.
(2012) noted that the relaxed DS assumption would fail and proposed a novel generativemodel to model the heuristic labeling process in order to reduce the wrong labels.The supervised method has been demonstrated to be effective for relation detection and yields rela-tively high performance.
However, the performance of this method strongly depends on the quality of thedesigned features.
With the recent revival of interest in DNN, many researchers have concentrated on us-ing Deep Learning to learn features.
In NLP, such methods are primarily based on learning a distributedrepresentation for each word, which is also called a word embeddings (Turian et al., 2010).
Socher et al.
(2012) present a novel recursive neural network (RNN) for relation classification that learns vectors inthe syntactic tree path that connects two nominals to determine their semantic relationship.
Hashimotoet al.
(2013) also use an RNN for relation classification; their method allows for the explicit weightingof important phrases for the target task.
As mentioned in Section 1, it is difficult to design high qualityfeatures using the existing NLP tools.
In this paper, we propose a convolutional DNN to extract lexicaland sentence level features for relation classification; our method effectively alleviates the shortcomingsof traditional features.3 Methodology3.1 The Neural Network ArchitectureFigure 1 describes the architecture of the neural network that we use for relation classification.
Thenetwork takes an input sentence and discovers multiple levels of feature extraction, where higher levelsrepresent more abstract aspects of the inputs.
It primarily includes the following three components: WordRepresentation, Feature Extraction and Output.
The system does not need any complicated syntactic orsemantic preprocessing, and the input of the system is a sentence with two marked nouns.
Then, theword tokens are transformed into vectors by looking up word embeddings.
In succession, the lexical andsentence level features are respectively extracted and then directly concatenated to form the final featurevector.
Finally, to compute the confidence of each relation, the feature vector is fed into a softmaxclassifier.
The output of the classifier is a vector, the dimension of which is equal to the number ofpredefined relation types.
The value of each dimension is the confidence score of the correspondingrelation.2337Features RemarkL1 Noun 1L2 Noun 2L3 Left and right tokens of noun 1L4 Left and right tokens of noun 2L5 WordNet hypernyms of nounsTable 1: Lexical level features.3.2 Word RepresentationIn the word representation component, each input word token is transformed into a vector by lookingup word embeddings.
Collobert et al.
(2011) reported that word embeddings learned from significantamounts of unlabeled data are far more satisfactory than the randomly initialized embeddings.
In relationclassification, we should first concentrate on learning discriminative word embeddings, which carry moresyntactic and semantic information, using significant amounts of unlabeled data.
Unfortunately, it usuallytakes a long time to train the word embeddings3.
However, there are many trained word embeddings thatare freely available (Turian et al., 2010).
A comparison of the available word embeddings is beyondthe scope of this paper.
Our experiments directly utilize the trained embeddings provided by Turian etal.
(2010).3.3 Lexical Level FeaturesLexical level features serve as important cues for deciding relations.
The traditional lexical level featuresprimarily include the nouns themselves, the types of the pairs of nominals and word sequences betweenthe entities, the quality of which strongly depends on the results of existing NLP tools.
Alternatively,this paper uses generic word embeddings as the source of base features.
We select the word embeddingsof marked nouns and the context tokens.
Moreover, the WordNet hypernyms4are adopted as MVRNN(Socher et al., 2012).
All of these features are concatenated into our lexical level features vector l. Table1 presents the selected word embeddings that are related to the marked nouns in the sentence.3.4 Sentence Level FeaturesAs mentioned in section 3.2, all of the tokens are represented as word vectors, which have been demon-strated to correlate well with human judgments of word similarity.
Despite their success, single wordvector models are severely limited because they do not capture long distance features and semantic com-positionality, the important quality of natural language that allows humans to understand the meaningsof a longer expression.
In this section, we propose a max-pooled convolutional neural network to offersentence level representation and automatically extract sentence level features.
Figure 2 shows the frame-work for sentence level feature extraction.
In the Window Processing component, each token is furtherrepresented as Word Features (WF) and Position Features (PF) (see section 3.4.1 and 3.4.2).
Then, thevector goes through a convolutional component.
Finally, we obtain the sentence level features through anon-linear transformation.3.4.1 Word FeaturesDistributional hypothesis theory (Harris, 1954) indicates that words that occur in the same context tendto have similar meanings.
To capture this characteristic, the WF combines a word?s vector representationand the vector representations of the words in its context.
Assume that we have the following sequenceof words.S : [People]0have1been2moving3back4into5[downtown]6The marked nouns are associated with a label y that defines the relation type that the marked pair contains.Each word is also associated with an index into the word embeddings.
All of the word tokens of thesentence S are then represented as a list of vectors (x0,x1, ?
?
?
,x6), where xicorresponds to the word3Collobert et al.
(2011) proposed a pairwise ranking approach to train the word embeddings, and the total training time foran English corpus (Wikipedia) was approximately four weeks.4http://sourceforge.net/projects/supersensetag/2338embedding of the i-th word in the sentence.
To use a context size of w, we combine the size w windowsof vectors into a richer feature.
For example, when we take w = 3, the WF of the third word ?moving?in the sentence S is expressed as [x2,x3,x4].
Similarly, considering the whole sentence, the WF can berepresented as follows:{[xs,x0,x1], [x0,x1,x2], ?
?
?
, [x5,x6,xe]}53.4.2 Position FeaturesRelation classification is a very complex task.
Traditionally, structure features (e.g., the shortest depen-dency path between nominals) are used to solve this problem (Bunescu and Mooney, 2005).
Apparently,it is not possible to capture such structure information only through WF.
It is necessary to specify whichinput tokens are the target nouns in the sentence.
For this purpose, PF are proposed for relation classi-fication.
In this paper, the PF is the combination of the relative distances of the current word to w1andw2.
For example, the relative distances of ?moving?
in sentence S to ?people?
and ?downtown?
are 3and -3, respectively.
In our method, the relative distances also are mapped to a vector of dimension de(ahyperparameter); this vector is randomly initialized.
Then, we obtain the distance vectors d1and d2withrespect to the relative distances of the current word to w1and w2, and PF = [d1,d2].
Combining the WFand PF, the word is represented as [WF,PF]T, which is subsequently fed into the convolution componentof the algorithm.3.4.3 ConvolutionWe will see that the word representation approach can capture contextual information through combina-tions of vectors in a window.
However, it only produces local features around each word of the sentence.In relation classification, an input sentence that is marked with target nouns only corresponds to a re-lation type rather than predicting label for each word.
Thus, it might be necessary to utilize all of thelocal features and predict a relation globally.
When using neural network, the convolution approach is anatural method to merge all of the features.
Similar to Collobert et al.
(2011), we first process the outputof Window Processing using a linear transformation.Z = W1X (1)X ?
Rn0?tis the output of the Window Processing task, where n0= w?
n, n (a hyperparameter) is thedimension of feature vector, and t is the token number of the input sentence.
W1?
Rn1?n0, where n1(ahyperparameter) is the size of hidden layer 1, is the linear transformation matrix.
We can see that thefeatures share the same weights across all times, which greatly reduces the number of free parameters tolearn.
After the linear transformation is applied, the output Z ?
Rn1?tis dependent on t. To determinethe most useful feature in the each dimension of the feature vectors, we perform a max operation overtime on Z.mi= maxZ(i, ?)
0 ?
i ?
n1(2)where Z(i, ?)
denote the i-th row of matrix Z.
Finally, we obtain the feature vector m ={m1,m2, ?
?
?
,mn1}, the dimension of which is no longer related to the sentence length.3.4.4 Sentence Level Feature VectorTo learn more complex features, we designed a non-linear layer and selected hyperbolic tanh as theactivation function.
One useful property of tanh is that its derivative can be expressed in terms of thefunction value itself:ddxtanhx = 1?
tanh2x (3)It has the advantage of making it easy to compute the gradient in the backpropagation training procedure.Formally, the non-linear transformation can be written asg = tanh(W2m) (4)5xsand xeare special word embeddings that correspond to the beginning and end of the sentence, respectively.2339W2?
Rn2?n1is the linear transformation matrix, where n2(a hyperparameter) is the size of hiddenlayer 2.
Compared with m ?
Rn1?1, g ?
Rn2?1can be considered higher level features (sentence levelfeatures).3.5 OutputThe automatically learned lexical and sentence level features mentioned above are concatenated into asingle vector f = [l, g].
To compute the confidence of each relation, the feature vector f ?
Rn3?1(n3equals n2plus the dimension of the lexical level features) is fed into a softmax classifier.o = W3f (5)W3?
Rn4?n3is the transformation matrix and o ?
Rn4?1is the final output of the network, where n4is equal to the number of possible relation types for the relation classification system.
Each output canbe then interpreted as the confidence score of the corresponding relation.
This score can be interpretedas a conditional probability by applying a softmax operation (see Section 3.6).3.6 Backpropagation TrainingThe DNN based relation classification method proposed here could be stated as a quintuple ?
=(X,N,W1,W2,W3)6.
In this paper, each input sentence is considered independently.
Given an in-put example s, the network with parameter ?
outputs the vector o, where the i-th component oicontainsthe score for relation i.
To obtain the conditional probability p(i|x, ?
), we apply a softmax operation overall relation types:p(i|x, ?)
=eoin4?k=1eok(6)Given all our (suppose T ) training examples (x(i); y(i)), we can then write down the log likelihood of theparameters as follows:J (?)
=T?i=1log p(y(i)|x(i), ?)
(7)To compute the network parameter ?, we maximize the log likelihood J(?)
using a simple optimizationtechnique called stochastic gradient descent (SGD).
N,W1,W2and W3are randomly initialized andX is initialized using the word embeddings.
Because the parameters are in different layers of the neuralnetwork, we implement the backpropagation algorithm: the differentiation chain rule is applied throughthe network until the word embedding layer is reached by iteratively selecting an example (x, y) andapplying the following update rule.?
?
?
+ ??
log p(y|x, ?)??
(8)4 Dataset and Evaluation MetricsTo evaluate the performance of our proposed method, we use the SemEval-2010 Task 8 dataset (Hen-drickx et al., 2010).
The dataset is freely available7and contains 10,717 annotated examples, including8,000 training instances and 2,717 test instances.
There are 9 relationships (with two directions) andan undirected Other class.
The following are examples of the included relationships: Cause-Effect,Component-Whole and Entity-Origin.
In the official evaluation framework, directionality is taken intoaccount.
A pair is counted as correct if the order of the words in the relationship is correct.
For example,both of the following instances S1and S2have the relationship Component-Whole.S1: The [haft]e1of the [axe]e2is make ?
?
?
?
Component-Whole(e1,e2)S2: This [machine]e1has two [units]e2?
?
?
?
Component-Whole(e2,e1)6N represents the word embeddings of WordNet hypernyms.7http://docs.google.com/View?id=dfvxd49s 36c28v9pmw2340?# Window size1 2 3 4 5 6 7F1727476788082# Hidden layer 10 100 200 300 400 500 600F1727476788082# Hidden layer 20 100 200 300 400 500 600F1727476788082Figure 3: Effect of hyperparameters.However, these two instances cannot be classified into the same category because Component-Whole(e1,e2) and Component-Whole(e2,e1) are different relationships.
Furthermore, the official rank-ing of the participating systems is based on the macro-averaged F1-scores for the nine proper relations(excluding Other).
To compare our results with those obtained in previous studies, we adopt the macro-averaged F1-score and also account for directionality into account in our following experiments8.5 ExperimentsIn this section, we conduct three sets of experiments.
The first is to test several variants via cross-validation to gain some understanding of how the choice of hyperparameters impacts upon the perfor-mance.
In the second set of experiments, we make comparison of the performance among the convolu-tional DNN learned features and various traditional features.
The goal of the third set of experiments isto evaluate the effectiveness of each extracted feature.5.1 Parameter SettingsIn this section, we experimentally study the effects of the three parameters in our proposed method:the window size in the convolutional component w, the number of hidden layer 1, and the number ofhidden layer 2.
Because there is no official development dataset, we tuned the hyperparameters by tryingdifferent architectures via 5-fold cross-validation.In Figure 3, we respectively vary the number of hyper parameters w, n1and n2and compute the F1.We can see that it does not improve the performance when the window size is greater than 3.
Moreover,because the size of our training dataset is limited, the network is prone to overfitting, especially whenusing large hidden layers.
From Figure 3, we can see that the parameters have a limited impact on theresults when increasing the numbers of both hidden layers 1 and 2.
Because the distance dimension haslittle effect on the result (this is not illustrated in Figure 3), we heuristically choose de= 5.
Finally,the word dimension and learning rate are the same as in Collobert et al.
(2011).
Table 2 reports all thehyperparameters used in the following experiments.Hyperparameter Window size Word dim.
Distance dim.
Hidden layer 1 Hidden layer 2 Learning rateValue w = 3 n = 50 de= 5 n1= 200 n2= 100 ?
= 0.01Table 2: Hyperparameters used in our experiments.5.2 Results of Comparison ExperimentsTo obtain the final performance of our automatically learned features, we select seven approaches as com-petitors to be compared with our method in Table 3.
The first five competitors are described in Hendrickxet al.
(2010), all of which use traditional features and employ SVM or MaxEnt as the classifier.
Thesesystems design a series of features and take advantage of a variety of resources (WordNet, ProBank,and FrameNet, for example).
RNN represents recursive neural networks for relation classification, as8The corpus contains a Perl-based automatic evaluation tool.2341Classifier Feature Sets F1SVM POS, stemming, syntactic patterns 60.1SVM word pair, words in between 72.5SVM POS, stemming, syntactic patterns, WordNet 74.8MaxEnt POS, morphological, noun compound, thesauri, Google n-grams, WordNet 77.6SVM POS, prefixes, morphological, WordNet, dependency parse, Levin classed, ProBank,FrameNet, NomLex-Plus, Google n-gram, paraphrases, TextRunner82.2RNN - 74.8POS, NER, WordNet 77.6MVRNN - 79.1POS, NER, WordNet 82.4Proposed word pair, words around word pair, WordNet 82.7Table 3: Classifier, their feature sets and the F1-score for relation classification.proposed by Socher et al.
(2012).
This method learns vectors in the syntactic tree path that connect twonominals to determine their semantic relationship.
The MVRNN model builds a single compositionalsemantics for the minimal constituent, including both nominals as RNN (Socher et al., 2012).
It is almostcertainly too much to expect a single fixed transformation to be able to capture the meaning combinationeffects of all natural language operators.
Thus, MVRNN assigns a matrix to every word and modifies themeanings of other words instead of only considering word embeddings in the recursive procedure.Table 3 illustrates the macro-averaged F1 measure results for these competing methods along with theresources, features and classifier used by each method.
Based on these results, we make the followingobservations:(1) Richer feature sets lead to better performance when using traditional features.
This improvementcan be explained by the need for semantic generalization from training to test data.
The quality oftraditional features relies on human ingenuity and prior NLP knowledge.
It is almost impossible tomanually choose the best feature sets.
(2) RNN and MVRNN contain feature learning procedures; thus, they depend on the syntactic tree usedin the recursive procedures.
Errors in syntactic parsing inhibit the ability of these methods to learnhigh quality features.
RNN cannot achieve a higher performance than the best method that usestraditional features, even when POS, NER and WordNet are added to the training dataset.
Comparedwith RNN, the MVRNN model can capture the meaning combination effectively and achieve a higherperformance.
(3) Our method achieves the best performance among all of the compared methods.
We also performa t-test (p 6 0.05), which indicates that our method significantly outperforms all of the comparedmethods.5.3 The Effect of Learned FeaturesFeature Sets F1Lexical L1 34.7+L2 53.1+L3 59.4+L4 65.9+L5 73.3Sentence WF 69.7+PF 78.9Combination all 82.7Table 4: Score obtained for various sets of features on for the test set.
The bottom portion of the tableshows the best combination of lexical and sentence level features.In our method, the network extract lexical and sentence level features.
The lexical level features pri-marily contain five sets of features (L1 to L5).
We performed ablation tests on the five sets of featuresfrom the lexical part of Table 4 to determine which type of features contributed the most.
The results are2342presented in Table 4, from which we can observe that our learned lexical level features are effective forrelation classification.
The F1-score is improved remarkably when new features are added.
Similarly, weperform experiment on the sentence level features.
The system achieves approximately 9.2% improve-ments when adding PF.
When all of the lexical and sentence level features are combined, we achieve thebest result.6 ConclusionIn this paper, we exploit a convolutional deep neural network (DNN) to extract lexical and sentencelevel features for relation classification.
In the network, position features (PF) are successfully proposedto specify the pairs of nominals to which we expect to assign relation labels.
The system obtains asignificant improvement when PF are added.
The automatically learned features yield excellent resultsand can replace the elaborately designed features that are based on the outputs of existing NLP tools.AcknowledgmentsThis work was sponsored by the National Basic Research Program of China (No.
2014CB340503) andthe National Natural Science Foundation of China (No.
61272332, 61333018, 61202329, 61303180).This work was supported in part by Noah?s Ark Lab of Huawei Tech.
Co. Ltd. We thank the anonymousreviewers for their insightful comments.ReferencesNguyen Bach and Sameer Badaskar.
2007.
A review of relation extraction.
Literature review for Language andStatistics II.Razvan C. Bunescu and Raymond J. Mooney.
2005.
A shortest path dependency kernel for relation extraction.
InProceedings of the conference on Human Language Technology and Empirical Methods in Natural LanguageProcessing, pages 724?731.Jinxiu Chen, Donghong Ji, Chew Lim Tan, and Zhengyu Niu.
2005.
Unsupervised feature selection for relationextraction.
In Proceedings of the International Joint Conference on Natural Language Processing, pages 262?267.Ronan Collobert, Jason Weston, L?eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011.Natural language processing (almost) from scratch.
The Journal of Machine Learning Research, 12:2493?2537.Zellig Harris.
1954.
Distributional structure.
Word, 10(23):146?162.Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman.
2004.
Discovering relations among named entities fromlarge corpora.
In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, pages415?422.Kazuma Hashimoto, Makoto Miwa, Yoshimasa Tsuruoka, and Takashi Chikayama.
2013.
Simple customizationof recursive neural networks for semantic relation classification.
In Proceedings of the 2013 Conference onEmpirical Methods in Natural Language Processing, pages 1372?1376.Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid?O.
S?eaghdha, Sebastian Pad?o, MarcoPennacchiotti, Lorenza Romano, and Stan Szpakowicz.
2010.
Semeval-2010 task 8: Multi-way classificationof semantic relations between pairs of nominals.
In Proceedings of the 5th International Workshop on SemanticEvaluation, SemEval ?10, pages 33?38.Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S. Weld.
2011.
Knowledge-basedweak supervision for information extraction of overlapping relations.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistics: Human Language Technologies - Volume 1, pages 541?550.Nanda Kambhatla.
2004.
Combining lexical, syntactic, and semantic features with maximum entropy models forextracting relations.
In Proceedings of the 42nd Annual Meeting on Association for Computational Linguisticson Interactive poster and demonstration sessions.2343Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.
2009.
Distant supervision for relation extraction withoutlabeled data.
In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4thInternational Joint Conference on Natural Language Processing of the AFNLP: Volume 2, pages 1003?1011.Raymond J Mooney and Razvan C Bunescu.
2005.
Subsequence kernels for relation extraction.
In Advances inneural information processing systems, pages 171?178.Longhua Qian, Guodong Zhou, Fang Kong, Qiaoming Zhu, and Peide Qian.
2008.
Exploiting constituent depen-dencies for tree kernel-based semantic relation extraction.
In Proceedings of the 22nd International Conferenceon Computational Linguistics, pages 697?704.Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010.
Modeling relations and their mentions withoutlabeled text.
In Proceedings of the 2010 European conference on Machine learning and knowledge discoveryin databases: Part III, pages 148?163.Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng.
2012.
Semantic compositionalitythrough recursive matrix-vector spaces.
In Proceedings of the 2012 Joint Conference on Empirical Methods inNatural Language Processing and Computational Natural Language Learning, pages 1201?1211.Fabian M. Suchanek, Georgiana Ifrim, and Gerhard Weikum.
2006.
Combining linguistic and statistical analysisto extract relations from web documents.
In Proceedings of the 12th ACM SIGKDD international conferenceon Knowledge discovery and data mining, pages 712?717.Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.
2012.
Reducing wrong labels in distant supervision forrelation extraction.
In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics:Long Papers - Volume 1, pages 721?729.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.
Word representations: a simple and general method forsemi-supervised learning.
In Proceedings of the 48th Annual Meeting of the Association for ComputationalLinguistics, pages 384?394.Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella.
2003.
Kernel methods for relation extraction.
TheJournal of Machine Learning Research, 3:1083?1106.GuoDong Zhou, Su Jian, Zhang Jie, and Zhang Min.
2005.
Exploring various knowledge in relation extraction.In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 427?434.2344
