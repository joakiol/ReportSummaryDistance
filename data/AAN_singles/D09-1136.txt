Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1308?1317,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPBayesian Learning of Phrasal Tree-to-String TemplatesDing Liu and Daniel GildeaDepartment of Computer ScienceUniversity of RochesterRochester, NY 14627{dliu, gildea}@cs.rochester.eduAbstractWe examine the problem of overcomingnoisy word-level alignments when learn-ing tree-to-string translation rules.
Ourapproach introduces new rules, and re-estimates rule probabilities using EM.
Themajor obstacles to this approach are thevery reasons that word-alignments areused for rule extraction: the huge spaceof possible rules, as well as controllingoverfitting.
By carefully controlling whichportions of the original alignments are re-analyzed, and by using Bayesian infer-ence during re-analysis, we show signifi-cant improvement over the baseline rulesextracted from word-level alignments.1 IntroductionNon-parametric Bayesian methods have been suc-cessfully applied to directly learn phrase pairsfrom a bilingual corpus with little or no depen-dence on word alignments (Blunsom et al, 2008;DeNero et al, 2008).
Because such approaches di-rectly learn a generative model over phrase pairs,they are theoretically preferable to the standardheuristics for extracting the phrase pairs from themany-to-one word-level alignments produced bythe IBM series models (Brown et al, 1993) orthe Hidden Markov Model (HMM) (Vogel et al,1996).
We wish to apply this direct, Bayesian ap-proach to learn better translation rules for syntax-based statistical MT (SSMT), by which we specif-ically refer to MT systems using Tree-to-String(TTS) translation templates derived from syntaxtrees (Liu et al, 2006; Huang et al, 2006; Gal-ley et al, 2006; May and Knight, 2007), as op-posed to formally syntactic systems such as Hi-ero (Chiang, 2007).
The stumbling block pre-venting us from taking this approach is the ex-tremely large space of possible TTS templateswhen no word alignments are given.
Given a sen-tence pair and syntax tree over one side, thereare an exponential number of potential TTS tem-plates and a polynomial number of phrase pairs.In this paper, we explore methods for restrictingthe space of possible TTS templates under con-sideration, while still allowing good templates toemerge directly from the data as much as possible.We find an improvement in translation accuracythrough, first, using constraints to limit the numberof new templates, second, using Bayesian methodsto limit which of these new templates are favoredwhen re-analyzing the training data with EM, and,third, experimenting with different renormaliza-tion techniques for the EM re-analysis.We introduce two constraints to limit the num-ber of TTS templates that we extract directly fromtree/string pairs without using word alignments.The first constraint is to limit direct TTS tem-plate extraction to the part of the corpus whereword alignment tools such as GIZA++ do poorly.There is no reason not to re-use the good align-ments from GIZA++, which holds a very compet-itive baseline performance.
As already mentioned,the noisy alignments from GIZA++ are likelyto cross the boundaries of the tree constituents,which leads to comparatively big TTS templates.We use this fact as a heuristic to roughly distin-guish noisy from good word alignments.1Herewe define big templates as those with more than8 symbols in their right hand sides (RHSs).
Theword alignments in big templates are consideredto be noisy and will be recomposed by extractingsmaller TTS templates.
Another reason to do ex-traction on big templates is that the applicabilityof big templates to new sentences is very limiteddue to their size, and the portion of the trainingdata from which they are extracted is effectivelywasted.
The second constraint, after choosing the1Precisely differentiating the noisy/good word alignmentsis as hard as correctly aligning the words.1308extraction site, is to extract the TTS templates allthe way down to the leaves of the hosting tem-plates.
This constraint limits the number of possi-ble left hand sides (LHSs) to be equal to the num-ber of tree nodes in the hosting templates.
Theentire extraction process can be summarized in 3steps:1.
Compute word alignments using GIZA++,and generate the basic TTS templates.2.
Select big templates from the basic TTS tem-plates in step 1, and extract smaller TTS tem-plates all the way down to the bottom frombig templates, without considering the pre-computed word alignments.3.
Combine TTS templates from step 1 and step2 and estimate their probabilities using Vari-ational Bayes with a Dirichlet Process prior.In step 2, since there are no constraints from thepre-computed word alignments, we have completefreedom in generating all possible TTS templatesto overcome noisy word alignments.
We use vari-ational EM to approximate the inference of ourBayesian model and explore different normaliza-tion methods for the TTS templates.
A two-stagenormalization is proposed by combining LHS-based normalization with normalization based onthe root of the LHS, and is shown to be the bestmodel when used with variational EM.Galley et al (2006) recompose the TTS tem-plates by inserting unaligned target words andcombining small templates into bigger ones.
Therecomposed templates are then re-estimated usingthe EM algorithm described in Graehl and Knight(2004).
This approach also generates TTS tem-plates beyond the precomputed word alignments,but the freedom is only granted over unaligned tar-get words, and most of the pre-computed wordalignments remain unchanged.
Other prior ap-proaches towards improving TTS templates fo-cus on improving the word alignment performanceover the classic models such as IBM series mod-els and Hidden Markov Model (HMM), which donot consider the syntactic structure of the align-ing languages and produce syntax-violating align-ments.
DeNero and Klein (2007) use a syntax-based distance in an HMM word alignment modelto favor syntax-friendly alignments.
Fossum et al(2008) start from the GIZA++ alignment and in-crementally delete bad links based on a discrim-SNPVP?NN?AUXissuehas??????SNPVP?NN?AUXissuehas?????
?Figure 1: 5 small TTS templates are extracted based on thecorrect word alignments (left), but only 1 big TTS template(right) can be extracted when the cross-boundary noisy align-ments are added in.inative model with syntactic features.
This ap-proach can only find a better subset of the GIZA++alignment and requires a parallel corpus with gold-standard word alignment for training the discrim-inative model.
May and Knight (2007) factorizethe word alignment into a set of re-orderings rep-resented by the TTS templates and build a hierar-chical syntax-based word alignment model.
Theproblem is that the TTS templates are generatedby the word alignments from GIZA++, which lim-its the potential of the syntactic re-alignment.
Asshown by these prior approaches, directly improv-ing the word alignment either falls into the frame-work of many-to-one alignment, or is substantiallyconfined by the word alignment it builds upon.The remainder of the paper focuses on theBayesian approach to learning TTS templates andis organized as follows: Section 2 describes theprocedure for generating the candidate TTS tem-plates; Section 3 describes the inference methodsused to learn the TTS templates; Section 4 givesthe empirical results, Section 5 discusses the char-acteristics of the learned TTS templates, and Sec-tion 6 presents the conclusion.2 Extracting Phrasal TTS TemplatesThe Tree-to-String (TTS) template, the most im-portant component of a SSMT system, usuallycontains three parts: a fragment of a syntax treein its left hand side (LHS), a sequence of wordsand variables in its right hand side (RHS), anda probability indicating how likely the templateis to be used in translation.
The RHS of a TTStemplate shows one possible translation and re-ordering of its LHS.
The variables in a TTS tem-plate are further transformed using other TTS tem-plates, and the recursive process continues untilthere are no variables left.
There are two ways1309SNP VPNP1 PP ADJAUXin isNP3ofNP3 ?
NP1 ?
?beautiful213 4Figure 2: Examples of valid and invalid templates extractedfrom a big Template.
Template 1, invalid, doesn?t go all theway down to the bottom.
Template 2 is valid.
Template 3, in-valid, doesn?t have the same set of variables in its LHS/RHS.Template 4, invalid, is not a phrasal TTS template.that TTS templates are commonly used in ma-chine translation.
The first is synchronous pars-ing (Galley et al, 2006; May and Knight, 2007),where TTS templates are used to construct syn-chronous parse trees for an input sentence, andthe translations will be generated once the syn-chronous trees are built up.
The other way isthe TTS transducer (Liu et al, 2006; Huang etal., 2006), where TTS templates are used just astheir name indicates: to transform a source parsetree (or forest) into the proper target string.
Sincesynchronous parsing considers all possible syn-chronous parse trees of the source sentence, it isless constrained than TTS transducers and hencerequires more computational power.
In this paper,we use a TTS transducer to test the performance ofdifferent TTS templates, but our techniques couldalso be applied to SSMT systems based on syn-chronous parsing.2.1 Baseline Approach: TTS TemplatesObeying Word AlignmentTTS templates are commonly generated by de-composing a pair of aligned source syntax treeand target string into smaller pairs of tree frag-ments and target string (i.e., the TTS templates).To keep the number of TTS templates to a manage-able scale, only the non-decomposable TTS tem-plates are generated.
This algorithm is referred toas GHKM (Galley et al, 2004) and is widely usedin SSMT systems (Galley et al, 2006; Liu et al,2006; Huang et al, 2006).
The word alignmentused in GHKM is usually computed independentof the syntactic structure, and as DeNero and Klein(2007) and May and Knight (2007) have noted,Ch-En En-Ch Union Heuristic28.6% 33.0% 45.9% 20.1%Table 1: Percentage of corpus used to generate big templates,based on different word alignments9-12 13-20 ?21Ch-En 18.2% 17.4% 64.4%En-Ch 15.9% 20.7% 63.4%Union 9.8% 15.1% 75.1%Heuristic 24.6% 27.9% 47.5%Table 2: In the selected big templates, the distribution ofwords in the templates of different sizes, which are measuredbased on the number of symbols in their RHSsis not the best for SSMT systems.
In fact, noisyword alignments cause more damage to a SSMTsystem than to a phrase based SMT system, be-cause the TTS templates can only be derived fromtree constituents.
If some noisy alignments happento cross over the boundaries of two constituents,as shown in Figure 2, a much bigger tree frag-ment will be extracted as a TTS template.
Eventhough the big TTS templates still carry the orig-inal alignment information, they have much lesschance of getting matched beyond the syntax treewhere they were extracted, as we show in Sec-tion 4.
In other words, a few cross-boundary noisyalignments could disable a big portion of a trainingsyntax tree, while for a phrase-based SMT system,their effect is limited to the phrases they align.
Asa rough measure of how the training corpus is af-fected by the big templates, we calculated the dis-tribution of target words in big and non-big TTStemplates.
The word alignment is computed usingGIZA++2for the selected 73,597 sentence pairs inthe FBIS corpus in both directions and then com-bined using union and heuristic diagonal growing(Koehn et al, 2003).
Table 1 shows that bigtemplates consume 20.1% to 45.9% of the trainingcorpus depending on different types of word align-ments.
The statistics indicate that a significantportion of the training corpus is simply wasted,if the TTS templates are extracted based on wordalignments from GIZA++.
On the other hand, itshows the potential for improving an SSMT sys-tem if we can efficiently re-use the wasted train-ing corpus.
By further examining the selected bigtemplates, we find that the most common form ofbig templates is a big skeleton template starting2GIZA++ is available athttp://www.fjoch.com/GIZA++.html1310from the root of the source syntax tree, and hav-ing many terminals (words) misaligned in the bot-tom.
Table 2 shows, in the selected big templates,the distribution of words in the templates of differ-ent sizes (measured based on the number of sym-bols in their RHS).
We can see that based on ei-ther type of word alignment, the most commonbig templates are the TTS templates with morethan 20 symbols in their RHSs, which are gen-erally the big skeleton templates.
The advantageof such big skeleton templates is that they usuallyhave good marginal accuracy3and allow accuratesmaller TTS templates to emerge.2.2 Liberating Phrasal TTS Templates FromNoisy Word AlignmentsTo generate better TTS templates, we use a moredirect way than modifying the underlying wordalignment: extract smaller phrasal TTS tem-plates from the big templates without looking attheir pre-computed word alignments.
We definephrasal TTS templates as those with more thanone symbol (word or non-terminal) in their LHS.The reason to consider only phrasal TTS tem-plates is that they are more robust than the word-level TTS templates in addressing the complicatedword alignments involved in big templates, whichare usually not the simple type of one-to-many ormany-to-one.
Abandoning the pre-computed wordalignments in big templates, an extracted smallerTTS template can have many possible RHSs, aslong as the two sides have the same set of vari-ables.
Note that the freedom is only given to thealignments of the words; for the variables in thebig templates, we respect the pre-computed wordalignments.
To keep the extracted smaller TTStemplates to a manageable scale, the following twoconstraints are applied:1.
The LHS of extracted TTS templates shouldgo all the way down to the bottom of the LHSof the big templates.
This constraint ensuresthat at most N LHSs can be extracted fromone big Template, where N is the number oftree nodes in the big Template?s LHS.2.
The number of leaves (including both wordsand variables) in an extracted TTS template?sLHS should not exceed 6.
This constraintlimits the size of the extracted TTS templates.3Here, marginal accuracy means the correctness of theTTS template?s RHS corresponding to its LHS.
( VP ( AUX is ) ( ADJ  beautiful ) )         ??
( PP ( IN of )  NP3 )          NP3  ?
( NP NP1 ( PP ( IN of )  NP3 ) )          NP3  ?
NP1( NP NP1 ( PP ( IN of )  NP3 ) )          NP3  ?
NP1  ?
?Figure 3: All valid templates that can be extracted from theexample in Figure 2.1for all template t doif size(t.rhs) > 8 thenfor all tree node s in t.lhs dosubt = subtree(s, t.lhs);if leaf num(subt) ?
6 thenfor i=1:size(t.rhs) dofor j=i:size(t.rhs) doif valid(subt, i, j) thencreate template(subt, i, j);Figure 4: Algorithm that liberates smaller TTS Templatesfrom big templatesAs we show in Section 4, use of bigger TTStemplates brings very limited performancegain.Figure 2.2 describes the template liberating algo-rithm running in O(NM2), where N denotes thenumber of tree nodes in the LHS of the input bigTemplate andM denotes the length of the RHS.
Inthe algorithm, function valid returns true if thereare the same set of variables in the left/right handside of an extracted TTS template; subtree(x, y)denotes the sub-tree in y which is rooted at x andgoes all the way down to y?s bottom.
Figure 2.1shows valid and invalid TTS templates which canbe extracted from an example hosting TTS tem-plate.
Note that, in order to keep the examplesimple, the hosting TTS template only has 4 sym-bols in its RHS, which does not qualify as a bigtemplate according to our definition.
Figure 2.2shows the complete set of valid TTS templateswhich can be extracted from the example TTStemplate.
The subscripts of the non-terminals areused to differentiate identical non-terminals in dif-ferent positions.
The extraction process blindlyreleases smaller TTS templates from the big tem-plates, among which only a small fraction are cor-rect TTS templates.
Therefore, we need an infer-ence method to raise the weight of the correct tem-plates and decrease the weight of the noisy tem-plates.13113 Estimating TTS Template ProbabilityThe Expectation-Maximization (EM) algorithm(Dempster et al, 1977) can be used to estimatethe TTS templates?
probabilities, given a genera-tive model addressing how a pair of source syn-tax tree and target string is generated.
There aretwo commonly used generative models for syntax-based MT systems, each of which corresponds toa normalization method for the TTS templates.The LHS-based normalization (LHSN) (Liu et al,2006; Huang et al, 2006), corresponds to thegenerative process where the source syntax sub-tree is first generated, and then the target stringis generated given the source syntax subtree.
Theother one is normalization based on the root ofthe LHS (ROOTN) (Galley et al, 2006), corre-sponding to the generative process where, giventhe root of the syntax subtree, the LHS syntax sub-tree and the RHS string are generated simultane-ously.
By omitting the decomposition probabilityin the LHS-based generative model, the two gen-erative models share the same formula for comput-ing the probability of a training instance:Pr(T, S) =?RPr(T, S,R) =?R(?t?RPr(t))where T and S denote the source syntax tree andtarget string respectively, R denotes the decompo-sition of (T, S), and t denotes the TTS template.The expected counts of the TTS templates can thenbe efficiently computed using an inside-outside-like dynamic programming algorithm (May andKnight, 2007).LHSN, as shown by Galley et al (2006), cannotaccurately restore the true conditional probabili-ties of the target sentences given the source sen-tences in the training corpus.
This indicates thatLHSN is not good at predicting unseen sentencesor at translating new sentences.
But this deficiencydoes not affect its ability to estimate the expectedcounts of the TTS templates, because the posteri-ors of the TTS templates only depend on the com-parative probabilities of the different derivationsof a training instance (a pair of tree and string).In fact, as we show in Section 4, LHSN is bet-ter than ROOTN in liberating smaller TTS tem-plates out of the big templates, since it is less bi-ased to the big templates in the EM training.4Be-cause the two normalization methods have their4Based on LHSN, the difference between the probabil-ity of a big Template and the product of the probabilities ofE-step:for all pair of syntax tree T and target string S dofor all TTS Template t doEC(t)+ =PR:t?RPr(T,S,R)?PR?Pr(T,S,R?)?
;Increase ?
;M-step:for all TTS Template t doif it is the last iteration thenPr(t) =EC(t)Pt?:t?.root=t.rootEC(t?
);elsePr(t) =EC(t)Pt?:t?.lhs=t.lhsEC(t?
);Figure 5: EM Algorithm For Estimating TTS Templatesown strength and weakness, both of them are usedin our EM algorithm: LHSN is used in all EMiterations except the last one to compute the ex-pected counts of the TTS templates, and ROOTNis used in the last EM iteration to compute the finalprobabilities of the TTS templates.
This two-stagenormalization method is denoted as MIXN in thispaper.Deterministic Annealing (Rose et al, 1992) isis used in our system to speed up the trainingprocess, similar to Goldwater et al (2006).
Westart from a high temperature and gradually de-crease the temperature to 1; we find that the ini-tial high temperature can also help small templatesto survive the initial iterations.
The complete EMframework is sketched in Figure 3, where ?
is theinverse of the specified temperature, and EC de-notes the expected count.3.1 Bayesian Inference with the DirichletProcess PriorBayesian inference plus the Dirichlet Process (DP)have been shown to effectively prevent MT mod-els from overfitting the training data (DeNero etal., 2008; Blunsom et al, 2008).
A similar ap-proach can be applied here for SSMT by consider-ing each TTS template as a cluster, and using DPto adjust the number of TTS templates accordingto the training data.
Note that even though thereis a size limitation on the liberated phrasal TTStemplates, standard EM will still tend to overfitthe training data by pushing up the probabilities ofthe big templates from the noisy word alignments.The complete generative process, integrating theDP prior and the generative models described inits decomposing TTS templates is much less than the onebased on ROOTN, thus LHSN gives comparably more ex-pected counts to the smaller TTS templates than ROOTN.1312for all TTS Template t doif it is the last iteration thenPr(t) =exp(?(EC(t)+?G0(t)))exp(?((Pt?:t?.root=t.rootEC(t?))+?
));elsePr(t) =exp(?(EC(t)+?G0(t)))exp(?((Pt?:t?.lhs=t.lhsEC(t?))+?
));Figure 6: M-step of the Variational EMSection 3.1, is given below:?r| {?r, Gr0} ?
DP (?r, Gr0)t | ?t.root?
?t.root(T, S) | {SG, {t}, ?}
?
SG({t}, ?
)where G0is a base distribution of the TTS tem-plates, t denotes a TTS template, ?t.rootdenotesthe multinomial distribution over TTS templateswith the same root as t, SG denotes the generativemodel for a pair of tree and string in Section 3.1,and ?
is a free parameter which adjusts the rate atwhich new TTS templates are generated.It is intractable to do exact inference under theBayesian framework, even with a conjugate priorsuch as DP.
Two methods are commonly usedfor approximate inference: Markov chain MonteCarlo (MCMC) (DeNero et al, 2008), and Vari-ational Bayesian (VB) inference (Blunsom et al,2008).
In this paper, the latter approach is used be-cause it requires less running time.
The E-step ofVB is exactly the same as standard EM, and in theM-step the digamma function ?
and the base dis-tributionG0are used to increase the uncertainty ofthe model.
Similar to standard EM, both LHS- androot-based normalizations are used in the M-step,as shown in Figure 3.1.
For the TTS templates,which are also pairs of subtrees and strings, a natu-ral choice ofG0is the generative models describedin Section 3.1.
BecauseG0estimates the probabil-ity of the new TTS templates, the root-based gen-erative model is superior to the LHS-based gener-ative model and used in our approach.3.2 InitializationSince the EM algorithm only converges to a lo-cal minimum, proper initializations are needed toachieve good performance for both standard EMand variational EM.
For the baseline templatesderived from word alignments, the initial countsare set to the raw counts in the training corpus.For the templates blindly extracted from big tem-plates, the raw count of a LHS tree fragment isdistributed among their RHSs based on the like-lihood of the template, computed by combiningfor all big template t dofor all template g extracted from t dog.count = g.lhs.count = 0;for all template g extracted from t dog.count += w in(g)?w out(g, t);g.lhs.count += w in(g)?w out(g, t);for all template g extracted from t dog.init +=g.countg.lhs.count;Figure 7: Compute the initial counts of the liberated TTStemplatesthe word-based inside/outside scores.
The algo-rithm is sketched in Figure 3.2, where the insidescore w in(g) is the product of the IBM Model 1scores in both directions, computed based on thewords in g?s LHS and RHS.
The outside scorew out(g, t) is computed similarly, except that theIBM Model 1 scores are computed based on thewords in the hosting template t?s LHS/RHS ex-cluding the words in g?s LHS/RHS.
The initialprobabilities of the TTS templates are then com-puted by normalizing their initial counts usingLHSN or ROOTN.4 ExperimentsWe train an English-to-Chinese translation sys-tem using the FBIS corpus, where 73,597 sentencepairs are selected as the training data, and 500 sen-tence pairs with no more than 25 words on the Chi-nese side are selected for both the developmentand test data.5Charniak (2000)?s parser, trainedon the Penn Treebank, is used to generate the En-glish syntax trees.
Modified Kneser-Ney trigrammodels are trained using SRILM (Stolcke, 2002)upon the Chinese portion of the training data.
Thetrigram language model, as well as the TTS tem-plates generated based on different methods, areused in the TTS transducer.
The model weightsof the transducer are tuned based on the develop-ment set using a grid-based line search, and thetranslation results are evaluated based on a singleChinese reference6using BLEU-4 (Papineni et al,2002).
Huang et al (2006) used character-basedBLEU as a way of normalizing inconsistent Chi-nese word segmentation, but we avoid this prob-lem as the training, development, and test data arefrom the same source.5The total 74,597 sentence pairs used in experiments arethose in the FBIS corpus whose English part can be parsedusing Charniak (2000)?s parser.6BLEU-4 scores based on a single reference are muchlower than the ones based on multiple references.1313E2C C2E Union Heuristicw/ Big 13.37 12.66 14.55 14.28w/o Big 13.20 12.62 14.53 14.21Table 3: BLEU-4 scores (test set) of systems based onGIZA++ word alignments?
5 ?
6 ?
7 ?
8 ?
?BLEU-4 14.27 14.42 14.43 14.45 14.55Table 4: BLEU-4 scores (test set) of the union alignment, us-ing TTS templates up to a certain size, in terms of the numberof leaves in their LHSs4.1 Baseline SystemsGHKM (Galley et al, 2004) is used to generatethe baseline TTS templates based on the wordalignments computed using GIZA++ and differentcombination methods, including union and the di-agonal growing heuristic (Koehn et al, 2003).
Wealso tried combining alignments from GIZA++based on intersection, but it is worse than bothsingle-direction alignments, due to its low cover-age of training corpus and the incomplete transla-tions it generates.
The baseline translation resultsbased on ROOTN are shown in Table 4.1.
The firsttwo columns in the table show the results of thetwo single direction alignments.
e2c and c2e de-note the many English words to one Chinese wordalignment and the many Chinese words to one En-glish word alignment, respectively.
The two rowsshow the results with and without the big tem-plates, from which we can see that removing thebig templates does not affect performance much;this verifies our postulate that the big templateshave very little chance of being used in the trans-lation.
Table 4.1, using the union alignments asthe representative and measuring a template?s sizeby the number of leaves in its LHS, also demon-strates that using big TTS templates brings verylimited performance gain.The result that the union-based combinationoutperforms either single direction alignments andeven the heuristic-based combination, combinedwith the statistics of the disabled corpus in Sec-tion 2.2, shows that more disabled training cor-pus actually leads to better performance.
This canbe explained by the fact that the union alignmentshave the largest number of noisy alignments gath-ered together in the big templates, and thus havethe least amount of noisy alignments which leadto small and low-quality TTS templates.1717.51818.51919.52020.5211  2  3  4  5  6  7  8  9  101.01.01.01.00.90.80.70.50.30.1iterationtemperature parameter ?MIXN-EMLHSN-VBLHSN-EMROOTN-EMROOTN-VBMIXN-VBFigure 8: BLEU-4 scores (development set) of annealing EMand annealing VB in each iteration.4.2 Learning Phrasal TTS TemplatesTo test our learning methods, we start with theTTS templates generated based on e2c, c2e, andunion alignments using GHKM.
This gives us0.98M baseline templates.
We use the big tem-plates from the union alignments as the basisand extract 10.92M new phrasal TTS templates,which, for convenience, are denoted by NEW-PHR.
Because based on Table 1 and Table 2the union alignment has the greatest number ofalignment links and therefore produces the largestrules, this gives us the greatest flexibility in re-aligning the input sentences.
The baseline TTStemplates as well as NEW-PHR are initialized us-ing the method in Section 3.3 for both annealingEM and annealing VB.
To simplify the experi-ments, the same Dirichlet Process prior is used forall multinomial distributions of the TTS templateswith different roots.
G0in the Dirichlet prior iscomputed based on the 1-level TTS templates se-lected from the baseline TTS templates, so that thebig templates are efficiently penalized.
The train-ing algorithms follow the same annealing sched-ule, where the temperature parameter ?
is initial-ized to 0.1, and gradually increased to 1.We experiment with the two training algo-rithms, annealing EM and annealing VB, with dif-ferent normalization methods.
The experimentalresults based on the development data are shownin Figure 4.2, where the free parameter ?
of an-nealing VB is set to 1, 100, and 100 respec-tively for ROOTN, LHSN, and MIXN.
The re-sults verify that LHSN is worse than ROOTN inpredicting the translations, since MIXN outper-forms LHSN with both annealing EM and VB.ROOTN is on par with MIXN and much better1314Max Likelihood Annealing EM Annealing VBw/o new-phr with new-phr w/o new-phr with new-phr w/o new-phr with new-phrLHSN 14.05 13.16 14.31 15.33 14.82 16.15ROOTN 14.50 13.49 14.90 16.06 14.76 16.12MIXN NA NA 14.82 16.37 14.93 16.84Table 5: BLEU-4 scores (test set) of different systems.Initial Template Final Templatenumber new-phr% number new-phr%ROOTN 11.9M 91.8% 408.0K 21.9%LHSN 11.9M 91.8% 557.2K 29.8%MIXN 11.9M 91.8% 500.5K 27.6%Table 6: The total number of templates and the percentage ofNEW-PHR, in the beginning and end of annealing VBthan LHSN when annealing EM is used; but withannealing VB, it is outperformed by MIXN bya large margin and is even slightly worse thanLHSN.
This indicates that ROOTN is not giv-ing large expected counts to NEW-PHR and leavesvery little space for VB to further improve the re-sults.
For all the normalization methods, anneal-ing VB outperforms annealing EM and maintainsa longer ascending path, showing better control ofoverfitting for the Bayesian models.
Figure 4.2shows the optimized results of the developmentset based on annealing VB with different ?.
Thebest performance is achieved as ?
approaches 1,100, and 100 for ROOTN, LHSN and MIXN re-spectively.
The ?
parameter can be viewed as aweight used to balance the expected counts andthe probabilities from G0.
Thus it is reasonablefor LHSN and MIXN to have bigger optimal ?than ROOTN, since ROOTN gives lower expectedcounts to NEW-PHR than LHSN and MIXN do.To see the contribution of the phrasal templateextraction in the performance gain, MT experi-ments are conducted by turning this componenton and off.
Results on the test set, obtained byusing parameters optimized on the developmentset, are shown in Table 4.2.
The template countsused in the Max-Likelihood training are the sameas the ones used in the initialization of anneal-ing EM and VB.
Results show that for annealingEM and VB, use of NEW-PHR greatly improvesperformance, while for the Max-Likelihood train-ing, use of NEW-PHR hurts performance.
Thisis not surprising, because Max-Likelihood train-ing cannot efficiently filter out the noisy phrasaltemplates introduced in the initial NEW-PHR.
An-other observation is that annealing VB does not al-ways outperform annealing EM.
With NEW-PHR19.82020.220.420.620.8210.1  1  10  100  1000?MIXNROOTNLHSNFigure 9: BLEU-4 scores (development set) of annealing VBwith different ?.turned on, annealing VB shows consistent supe-riority over annealing EM; while without NEW-PHR, it only outperforms annealing EM based onLHSN and MIXN, and the improvement is not asbig as when NEW-PHR is turned on.
This indi-cates that without NEW-PHR, there is less needto use VB to shrink down the size of the tem-plate set.
Table 4.2 shows the statistics of the ini-tial template set including NEW-PHR and the finalTTS template set after annealing VB is conducted,where we can see annealing VB efficiently re-duces NEW-PHR to a relatively small size and re-sults in much more compact systems than the sys-tem based on the baseline templates from GIZA++alignments.
Comparing with the best GIZA++-based system union, our best system, utilizingNEW-PHR and the two-stage template normaliza-tion, demonstrates the strength of annealing VBby an absolute improvement of 2.29% in BLEU-4 score, from 14.55 to 16.84.
This improvementis significant at p < 0.005 based on 2000 itera-tions of paired bootstrap re-sampling of the testset (Koehn, 2004).5 DiscussionOur experimental results are obtained based ona relatively small training corpus, the improvedperformance may be questionable when a largertraining corpus is used.
Someone may wonder ifthe performance gain primarily comes from the1315Many-to-one Alignment( VP ( VB make ) ( NP ( DT a ) ( JJ complete ) ( NN statement ) ) ) ????
( S ( VP VBG ( NP ( DT the ) ( NN mass ) ( NN line ) ) PP ) ) PP VBG ????
( PP ( TO to ) ( NP ( DT the ) ( JJS greatest ) ( NN extent ) ) ) ????
?
( PP ( IN of ) ( NP ( JJ peaceful ) ( NNP coexistence ) ) ) ???
?Many-to-many Alignment( VP ( VBN based ) ( PP ( IN on ) ( NP ( JJ actual ) ( NNS needs ) ) ) ) ?
??
??
( PP ( IN into ) ( NP ( NP ( DT the ) ( NNS hands ) ) PP ) ) ??
?
PP ??
( VP ( VBP exercise ) ( NP ( JJ strict ) ( NN self-discipline ) ) ) ?
?
?
?
( SBAR ( S ( NP ( DT the ) ( VBG aging ) NN ) ( VP ( aux is ) NP ) ) ) NN ?
?
?
?
NP( NP NP1 PP ( , , ) ( VP ( VBN centered ) ( PP ( IN around ) NP2 ) ) ) ?
NP2 ?
??
?
NP1 PPAllowance of Bad Word Segmentation( NP ( NP ( NNP japan ) ( POS 's ) ) ( NNP sdf ) ( NNP navy ) ) ???
?
???
( NP ( PDT all ) ( NP ( NNS people ) ( POS 's ) ) ( NNS organizations ) ) ??
??
?Figure 10: Examples of the learned TTS templatesreduced out of vocabulary (OOV) ratio.
We ex-amined the OOV ratio of the test set with/withoutthe learned TTS templates, and found the differ-ence was very small.
In fact, our method is de-signed to learn the phrasal TTS templates, and ex-plictly avoids lexical pairs.
To further understandthe characteristics of the learned TTS templates,we list some representative templates in Figure 4.2classified in 3 groups.
The group Many-to-oneAlignment and Many-to-many Alignment show theTTS templates based on complicated word align-ments, which are difficult to compute based on theexisting word alignment models.
These templatesdo not have rare English words, whose translationcannot be found outside the big templates.
Thedifficulty lies in the non-literal translation of thesource words, which are unlikely to learnt by solyincreasing the size of the training corpus.
Oneother interesting observation is that our learningmethod is tolerant to noisy Chinese word segmen-tation, as shown in group Allowance of Bad WordSegmentation.6 ConclusionThis paper proposes a Bayesian model for extract-ing the Tree-to-String templates directly from thedata.
By limiting the extraction to the big tem-plates from the pre-computed word alignmentsand applying a set of constraints, we restrict thespace of possible TTS templates under consider-ation, while still allowing new and more accuratetemplates to emerge from the training data.
Theempirical results demonstrate the strength of ourapproach, which outperforms the GIZA++-basedsystems by a large margin.
This encourages amove from word-alignment-based systems to sys-tems based on consistent, end-to-end probabilisticmodeling.
Because our Bayesian model employsa very simple prior, more sophisticated generativemodels provide a possible direction for further ex-perimentation.Acknowledgments This work was supported byNSF grants IIS-0546554 and ITR-0428020.ReferencesPhil Blunsom, Trevor Cohn, and Miles Osborne.
2008.Bayesian synchronous grammar induction.
In Neu-ral Information Processing Systems (NIPS).Peter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, and Robert L. Mercer.
1993.
Themathematics of statistical machine translation: Pa-rameter estimation.
Computational Linguistics,19(2):263?311.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proceedings of NAACL-01, pages132?139.1316David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2).A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum likelihood from incomplete data via theEM algorithm.
Journal of the Royal Statistical Soci-ety, 39(1):1?21.John DeNero and Dan Klein.
2007.
Tailoring wordalignments to syntactic machine translation.
In Pro-ceedings of ACL-07, pages 17?24.John DeNero, Alexandre Bouchard-Cote, and DanKlein.
2008.
Sampling alignment structure undera bayesian translation model.
In EMNLP08.Victoria Fossum, Kevin Knight, and Steven Abney.2008.
Using syntax to improveword alignment pre-cision for syntax-based machine translation.
In Pro-ceedings of the Third Workshop on Statistical Ma-chine Translation, Columbus, Ohio.
ACL.Michel Galley, Mark Hopkins, Kevin Knight, andDaniel Marcu.
2004.
What?s in a translation rule?In Proceedings of NAACL-04, pages 273?280.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Pro-ceedings of COLING/ACL-06, pages 961?968, July.Sharon Goldwater, Thomas L. Griffiths, and MarkJohnson.
2006.
Contextual dependencies in un-supervised word segmentation.
In Proceedings ofthe Human Language Technology Conference/NorthAmerican Chapter of the Association for Computa-tional Linguistics (HLT/NAACL).Jonathan Graehl and Kevin Knight.
2004.
Trainingtree transducers.
In Proceedings of NAACL-04.Liang Huang, Kevin Knight, and Aravind Joshi.
2006.Statistical syntax-directed translation with extendeddomain of locality.
In Proceedings of the 7th Bi-ennial Conference of the Association for MachineTranslation in the Americas (AMTA), Boston, MA.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of NAACL-03, Edmonton, Alberta.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proceedings ofEMNLP, pages 388?395, Barcelona, Spain, July.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machinetranslation.
In Proceedings of COLING/ACL-06,Sydney, Australia, July.J.
May and K. Knight.
2007.
Syntactic re-alignmentmodels for machine translation.
In Proceedings ofEMNLP.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automaticevaluation of machine translation.
In Proceedingsof ACL-02.K.
Rose, E. Gurewitz, and G. C. Fox.
1992.
Vec-tor quantization by deterministic annealing.
IEEETransactions on Information Theory, 38(4):1249?1257, July.Andreas Stolcke.
2002.
Srilm - an extensible languagemodeling toolkit.
In International Conference onSpoken Language Processing, volume 2, pages 901?904.Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
Hmm-based word alignment in statisticaltranslation.
In COLING-96, pages 836?841.1317
