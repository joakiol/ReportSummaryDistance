Coling 2008: Companion volume ?
Posters and Demonstrations, pages 11?14Manchester, August 2008Towards Incremental End-of-Utterance Detection in Dialogue SystemsMichaela Atterer, Timo Baumann, David SchlangenInstitute of LinguisticsUniversity of Potsdam, Germany{atterer,timo,das}@ling?.uni-potsdam.deAbstractWe define the task of incremental or 0-lag utterance segmentation, that is, the taskof segmenting an ongoing speech recog-nition stream into utterance units, andpresent first results.
We use a combinationof hidden event language model, featuresfrom an incremental parser, and acous-tic / prosodic features to train classifiers onreal-world conversational data (from theSwitchboard corpus).
The best classifiersreach an F-score of around 56%, improv-ing over baseline and related work.1 IntroductionUnlike written language, speech?and hence, au-tomatic speech transcription?does not come seg-mented into units.
Current spoken dialogue sys-tems simply wait for the speaker to turn silent tosegment their input.
This necessarily reduces theirresponsiveness, as further processing can onlyeven commence a certain duration after the turnhas ended (Ward et al, 2005).
Moreover, giventhe typically simple domains, such work mostlydoes not deal with the problem of segmenting theturn into utterances, i.e.
does not distinguish be-tween utterance and turn segmentation.
However,as our corpus shows (see below), multi-utteranceturns are the norm in natural dialogues.
The workthat does treat intra-turn utterance segmentationdoes so in an offline context, namely the post-processing of automatic transcripts of recordedspeech such as meeting protocols (Fung et al,c?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.2007), and relies heavily on right-context pause in-formation.In this paper, we define the task of incrementalor 0-lag utterance segmentation, that is, the task ofsegmenting an ongoing speech recognition streaminto utterance units using only left-context infor-mation.1 This work is done in the context of devel-oping an incremental dialogue system architecture(as proposed among others by (Aist et al, 2007)),where, ideally, a considerable part of the analy-sis has already been done while the speaker stillspeaks.
The incremental parser and other compo-nents of such a system need to be reset at turn-internal utterance-boundaries with as little delay aspossible.
Hence it is of vital importance to predictthe end-of-utterance while the last word of a sen-tence is processed (or even earlier).
We investigatetypical features an incremental system can access,such as partial parse trees and parser internal in-formation.
These experiments are a first importantstep towards online endpointing in an incrementalsystem.2 DataWe used section 2 of the Switchboard corpus(Godfrey et al, 1992) for our experiments.
Section3 was used for training the language models andthe parser that we used.
Some of the Switchboarddialogues are of a very low quality.
We excludedthose where transcription notes indicated high rateof problems due to static noise, echo from the otherspeaker or background noise.
As our parser be-came very slow for long sentences, we excludedsentences that were longer than 25 words from the10-lag here refers to the time where feature extractionstarts.
As the modules on which feature extraction is basedrequire processing time themselves, a complete absence ofprediction delay is of course not possible.11analysis (4% of the sentences).
We also excludedback-channel utterances (typically one-word turns)from the corpus.Of the remaining corpus we only used the first100,000 instances to reduce the computationalload for training the classifiers.
80 % of those wereused as a training corpus, and 20 % as a test corpus.For follow-up experiments that investigated turn-initial or turn-internal utterance boundaries only(see below), we used the relevant subsets of thefirst 200,000 instances.3 Feature ExtractionOur features comprise prosodic features, and syn-tactic features.Prosodic features are pitch, logarithmized sig-nal energy and derived features, extracted from theaudio every 10 ms.
In order to track changes overtime, we derive features by windowing over pastvalues of pitch, energy, and energy in voiced re-gions only, with window sizes ranging from 50 msto 5000 ms. We calculate the arithmetic mean andthe range of the values, the mean difference be-tween values within the window and the relativeposition of the minimum and maximum.
We alsoperform a linear regression and use its slope, theMSE of the regression and the error of the regres-sion for the last value in the window.As classification was done word-wise (final vs.non-final word), each word was attributed theprosodic features of the last corresponding 10-ms-frame.For the extraction of syntactic features we usedboth n-gram models and a parser.
The parserwas a modified version of Amit Dubey?s sleepyparser,2 which can produce syntactic structure in-crementally online.
The n-gram model was ahidden event model as typically used in the sen-tence unit detection literature (see e.g.
(Fung etal., 2007)).
For the time being, all features basedon word identities are computed on gold stan-dard transcriptions.
We trained n-gram modelsboth based on words and on words plus POS-information that was incrementally obtained fromthe parser.3 We calculated the log-probability oftrigrams with the last token in the n-gram beinga place-holder for end-of-utterance (i.e.
the prob-2http://homepages.inf.ed.ac.uk/adubey/software/3The models were trained using the SRILM-tools (Stol-cke, 2002) for n = 3 using Chen and Goodman?s modifiedKneser-Ney discounting (Chen and Goodman, 1998).ability of (I,would,end-of-utterance) or (Thank,you,end-of-utterance).
We also calculated log probabili-ties for trigrams such as (I, end-of-utterance-1,end-of-utterance).
Thirdly, the log probability was alsocomputed for a string consisting of 4 word/POS-pairs followed by an end marker.Further syntactic features can be roughly di-vided into two classes: parser-based features,which are related to internal states of the parser,and structure-based features which refer to prop-erties of the syntactic tree.
The former try to cap-ture the expectation of there being more incom-plete edges towards the beginning of a sentencethan towards the end.
We also might expect a rel-ative decrease in the overall number of edges to-wards the end of a sentence.
Therefore we track aselection of numbers referring to the various kindsof edges stored in the chart.
Moreover, we utilizesome of the parser?s information about the best in-termediate edge, and use the category after the dotof this edge as an estimate for the most probablecategory to come next.
Furthermore, we use theforward probability of the best tree as a roundedlog probability.The structure-based features are simple featuressuch as the part-of-speech category of the currentword and the number of the word in the currentutterance, and more complex features that try to(roughly) approximate semantic notions of com-pleteness by counting the number of verbs or num-ber of nominal phrases encountered, as we wouldusually expect a sentence to be incomplete if wehaven?t heard a verb or nominal phrase yet.
Forexample, in sentences of the structure (NP) (VP (VNP)) or (NP) (VP (V NP (N PP))), humans wouldtypically be aware that the last phrase has probablybeen reached during the last noun phrase or prepo-sitional phrase (cf.
(Grosjean, 1983)).
However,the length and internal structure of these phrasescan vary a great deal.
We try to capture some ofthis variation by features referring to the last non-terminal seen, the second-to-last non-terminal seenand the number of words seen since the last non-terminal.
A number of features (the count fea-tures) are simple features that record the numberof words since the turn or utterance started and thetime elapsed since the utterance started.
They arealso subsumed under syntactic features.We also used dialogue act features like the pre-vious dialogue act, and the previous dialogue actof the last speaker.
Those currently come from12the gold standard.
We assume that in a dialoguesystem the system would at least have informationabout its own dialogue acts.4 Experimental SettingsWe tested a number of classifiers as implementedin the Weka toolkit (Witten and Frank, 2005),and found that the JRip-classifier, an implementa-tion of the RIPPER-algorithm (Cohen, 1995), per-formed best.
A number of attribute selection algo-rithms also did not result in a significant change ofperformance.
Therefore, we only report the plainresults by JRip.
We also tested the impact each ofour information sources had on the results.
Theaim was to find out how important parser, part-of-speech information and pitch and energy featuresare, respectively.As turn-internal utterance-ends might be moredifficult to detect than those that coincide withturn-ends, we repeat the experiment with turn-internal utterances only.
Deleting turn-final utter-ances from our initial 200,000-instance corpus re-sulted in 128,686 remaining word instances, 80 %of which were used for training.
For a third ex-periment, where we look at turn-initial utterances,we use again a subset of those 200,000 word-instances.For clarity, we simply use precision/recall forevaluation; see (Liu and Shriberg, 2007) for a dis-cussion of other metrics.
As a baseline we assumenon-existent utterance segmentation, which resultsin a recall of 0 and a precision of 100 %.5 ResultsPrecision Recall Fbaseline 100 0 0all features 73.8 45.0 55.9all syntactic features 74.8 44.0 55.4word/POS n-gram features 73.4 45.8 56.4word n-gram features 66.9 34.7 45.7only count features 59.3 7.7 13.6prosodic features only 49.5 8.3 14.2pitch features 100 0 0energy features 48.2 7.4 12.8Table 1: Results for end-of-utterance classificationfor all utterances.Tables 1 and 2 show the results for the experi-mental settings described above.
Dialogue act fea-tures were included in the syntactic features, butJRip did not use them in its rules eventually.
Ta-ble 1 shows that the overall F-score is best whenn-grams with POS information are used.
Addinga parser, however, increases precision.
Prosodyfeatures in general do not seem to have much ofan influence on end-of-utterance prediction in ourdata, with energy features contributing more thanpitch features.
Table 2 indicates, as expected, thatPrecision Recall Fbaseline 100 0 0all features 71.2 40.3 51.4all syntactic features 72.7 38.2 50.0word/POS n-gram features 70.5 41.1 51.9word n-gram features 70.9 26.4 38.5only count features 60.4 1.0 2.0prosodic features only 41.7 1.7 3.3pitch features 100 0 0energy features 31.6 1.2 2.3Table 2: Results for end-of-utterance classificationfor utterances which are not turn-final.the end of an utterance is harder to predict whenit is not turn-final.
Performance drops comparedto the results shown in Table 1.
Note that someof the performance drop must be attributed to theuse of a different data set.
However, the perfor-mance drop is much more dramatic for the exper-iments where only prosody is used than for thosewhere syntax is used.
We speculate that the countfeatures also loose their impact because one-wordutterances like ?Okay?
are usually turn-initial.The results shown in Tables 1 and 2 can be re-garded as an upper bound for a dialogue system,because our experiments so far work with goldstandard sentence boundaries for creating syntac-tic features (e.g., for resetting our parser).
Strictlyspeaking, they are only realistic for turn-initial ut-terances.
For the remaining 14,737 of our 26,401utterances, we therefore report a lower bound,where we use only features that do not have knowl-edge about the beginning of the sentence (Table 3).No count and parser-based features were used forthis experiment, only n-gram (word-based withoutPOS information) and pitch features.
The Tablealso shows the results for the 11,664 turn-initialutterances, where we use all features.4 We thenderive the overall performance from the fractionsof initial and non-initial utterances.Future work aims at putting together a sys-tem where the parser is restarted using predictionsbased on its own output.4Note that turn-initial utterances can at the same time beturn-final.13Precision Recall Fnon-initial 65.0 28.6 39.7initial 74.5 55.1 63.3overall 70.4 40.3 51.3Table 3: Results for end-of-utterance classificationfor utterances which are not turn-initial (reducedfeature set), and utterances that are turn-initial (fullfeature set) and the derived overall performance.6 Related Work(Fuentes et al, 2007) report F-measures of 84%using prosodic features only, but they use left?right-windows for feature calculation, where ourprocessing is truly incremental and more suitablefor real-time usage in a dialogue system.
More-over, they only seem to use one-utterance turns,which makes the task easier when prosodic fea-tures are used.
In our dialogue corpus (Switch-board, section 2), however, each turn containson average 2.5 utterances, and turn-internal ut-terances also need to be recognized.
(Fung etal., 2007) reach an F-score of 75.3% , but reportthat the best feature was pause-duration?a fea-ture we don?t use because we want to find outhow well we can predict the end of a sentencebefore a pause makes this clear.
Similarly, (Fer-rer et al, 2002) rely largely on pause features.
(Schlangen, 2006) investigates incremental predic-tion of end-of-utterance and end-of-turn for var-ious pause-lengths, and achieves an F-score of35.5% for pause length 0, on which we can im-prove here.7 Discussion and ConclusionWe investigated 0-lag end-of-utterance detectionfor incremental dialogue systems.
In our setup,we aim to recognise the end of an utterance assoon as possible, while the potentially last word isprocessed, without the help of information aboutsubsequent silence.
We investigate a number offeatures an incremental system would be able toaccess, such as information from an incremen-tal parser.
We find that remaining (non-pause)prosodic information is not as helpful as in non-incremental studies, especially for non-turn-finalutterances.
Syntactic information, on the otherhand, increases performance.
Future work aims atmore sophisticated prosodic modelling and at test-ing the impact of using real or simulated speechrecognition output.
We also intend to implementend-of-utterance prediction in the context of a realincremental system we are building.8 AcknowledgementThis work was funded by DFG ENPSCHL845/3-1.ReferencesAist, Gregory, James Allen, Ellen Campana, Carlos Gomez-Gallo, Scott Stoness, Mary Swift, and Michael Tanenhaus.2007.
Incremental understanding in human-computer dia-logue and experimental evidence for advantages over non-incremental methods.
In Proc.
of the 2007 Workshop onthe Semantics and Pragmatics of Dialogue (DECALOG).Chen, S.F.
and J. Goodman.
1998.
An empirical studyof smoothing techniques for language modeling.
Techni-cal report, Center for Research in Computing Technology(Harvard University).Cohen, William W. 1995.
Fast effective rule induction.
InMachine Learning: Proceedings of the Twelfth Interna-tional Conference.Ferrer, L., E. Shriberg, and A. Stolcke.
2002.
Is the speakerdone yet?
Faster and more accurate end-of-utterance de-tection using prosody in human-computer dialog.
In Proc.Intl.
Conf.
on Spoken Language Processing, Denver.Fuentes, Olac, David Vera, and Thamar Solorio.
2007.A filter-based approach to detect end-of-utterances fromprosody in dialog systems.
In Proc.
Human LanguageTechnologies 2007, Rochester, New York.Fung, J., D. Hakkani-Tur, M. Magimai-Doss, E. Shriberg,S.
Cuendet, and N. Mirghafori.
2007.
Prosodic featuresand feature selection for multi-lingual sentence segmenta-tion.
In Proc.
Interspeech, pages 2585?2588, Antwerp.Godfrey, John J., E. C. Holliman, and J. McDaniel.
1992.SWITCHBOARD: Telephone speech corpus for researchand development.
In Proc.
of ICASSP-1992, pages 517?520, San Francisco, USA, March.Grosjean, Franc?ois.
1983.
How long is the sentence?
Pre-diction and prosody in the on-line processing of language.Linguistics, 21:501?529.Liu, Y. and E. Shriberg.
2007.
Comparing evaluation metricsfor sentence boundary detection.
In Proc.
IEEE ICASSP,Honolulu,USA.Schlangen, David.
2006.
From reaction to prediction: Exper-iments with computational models of turn-taking.
In Proc.Interspeech 2006, Panel on Prosody of Dialogue Ac ts andTurn-Taking, Pittsburgh, USA.Stolcke, Andreas.
2002.
SRILM ?
an extensible languagemodeling toolkit.
In Proc.
ICSLP 2002.Ward, Nigel G., Anais G. Rivera, Karen Ward, and David G.Novick.
2005.
Root causes of lost time and user stress ina simple dialog system.
In Proc.
of Interspeech, El Paso,USA.Witten, Ian H. and Eibe Frank.
2005.
Data Mining: PracticalMachine Learning Tools and Techniques.
Morgan Kauf-mann.14
