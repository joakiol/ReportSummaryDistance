BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 10?18,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsExtracting Clinical Relationships from Patient NarrativesAngus Roberts, Robert Gaizauskas, Mark HeppleDepartment of Computer Science, University of Sheffield,Regent Court, 211 Portobello, Sheffield S1 4DP{initial.surname}@dcs.shef.ac.ukAbstractThe Clinical E-Science Framework (CLEF)project has built a system to extract clin-ically significant information from the tex-tual component of medical records, for clin-ical research, evidence-based healthcare andgenotype-meets-phenotype informatics.
Onepart of this system is the identification of rela-tionships between clinically important entitiesin the text.
Typical approaches to relationshipextraction in this domain have used full parses,domain-specific grammars, and large knowl-edge bases encoding domain knowledge.
Inother areas of biomedical NLP, statistical ma-chine learning approaches are now routinelyapplied to relationship extraction.
We reporton the novel application of these statisticaltechniques to clinical relationships.We describe a supervised machine learningsystem, trained with a corpus of oncology nar-ratives hand-annotated with clinically impor-tant relationships.
Various shallow featuresare extracted from these texts, and used totrain statistical classifiers.
We compare thesuitability of these features for clinical re-lationship extraction, how extraction variesbetween inter- and intra-sentential relation-ships, and examine the amount of training dataneeded to learn various relationships.1 IntroductionThe application of Natural Language Processing(NLP) is widespread in biomedicine.
Typically, itis applied to improve access to the ever-burgeoningresearch literature.
Increasingly, biomedical re-searchers need to relate this literature to pheno-typic data: both to populations, and to individ-ual clinical subjects.
The computer applicationsused in biomedical research, including NLP appli-cations, therefore need to support genotype-meets-phenotype informatics and the move towards trans-lational biology.
Such support will undoubtedly in-clude linkage to the information held in individualmedical records: both the structured portion, and theunstructured textual portion.The Clinical E-Science Framework (CLEF)project (Rector et al, 2003) is building a frame-work for the capture, integration and presentation ofthis clinical information, for research and evidence-based health care.
The project?s data resource is arepository of the full clinical records for over 20000cancer patients from the Royal Marsden Hospital,Europe?s largest oncology centre.
These recordscombine structured information, clinical narratives,and free text investigation reports.
CLEF uses infor-mation extraction (IE) technology to make informa-tion from the textual portion of the medical recordavailable for integration with the structured record,and thus available for clinical care and research.
TheCLEF IE system analyses the textual records to ex-tract entities, events and the relationships betweenthem.
These relationships give information that isoften not available in the structured record.
Whywas a drug given?
What were the results of a physi-cal examination?
What problems were not present?We have previously reported entity extraction in theCLEF IE system (Roberts et al, 2008b).
This paperexamines relationship extraction.Extraction of relationships from clinical text isusually carried out as part of a full clinical IE sys-tem.
Several such systems have been described.They generally use a syntactic parse with domain-specific grammar rules.
The Linguistic Stringproject (Sager et al, 1994) used a full syntactic and10clinical sublanguage parse to fill template data struc-tures corresponding to medical statements.
Thesewere mapped to a database model incorporatingmedical facts and the relationships between them.MedLEE (Friedman et al, 1994), and more recentlyBioMedLEE (Lussier et al, 2006) used a semanticlexicon and grammar of domain-specific semanticpatterns.
The patterns encode the possible relation-ships between entities, allowing both entities and therelationships between them to be directly matchedin the text.
Other systems have incorporated large-scale domain-specific knowledge bases.
MEDSYN-DIKATE (Hahn et al, 2002) employed a rich dis-course model of entities and their relationships, builtusing a dependency parse of texts and a descrip-tion logic knowledge base re-engineered from exist-ing terminologies.
MENELAS (Zweigenbaum et al,1995) also used a full parse, a conceptual represen-tation of the text, and a large scale knowledge base.In other applications of biomedical NLP, a sec-ond paradigm has become widespread: the appli-cation of statistical machine learning techniques tofeature-based models of the text.
Such approacheshave typically been applied to journal texts.
Theyhave been used both for entity recognition and ex-traction of various relations, such as protein-proteininteractions (see, for example, Grover et al(2007)).This follows on from the success of these methodsin general NLP (see for example Zhou et al(2005)).Statistical machine learning has also been applied toclinical text, but its use has generally been limitedto entity recognition.
The Mayo Clinic text analysissystem (Pakhomov et al, 2005), for example, uses acombination of dictionary lookup and a Na?
?ve Bayesclassifier to identify entities for information retrievalapplications.
To the best of our knowledge, statisti-cal methods have not been previously applied to ex-traction of clinical relationships from text.This paper describes experiments in the statisticalmachine learning of relationships from a novel texttype: oncology narratives.
The set of relationshipsextracted are considered to be of interest for clinicaland research applications down line of IE, such asquerying to support clinical research.
We apply Sup-port Vector Machine (SVM) classifiers to learn theserelationships.
The classifiers are trained and eval-uated using novel data: a gold standard corpus ofclinical text, hand-annotated with semantic entitiesand relationships.
In order to test the applicabilityof this method to the clinical domain, we train clas-sifiers using a number of comparatively simple textfeatures, and look at the contribution of these fea-tures to system performance.
Clinically interestingrelationships may span several sentences, and so wecompare classifiers trained for both intra- and inter-sentential relationships (spanning one or more sen-tence boundaries).
We also examine the influence oftraining corpus size on performance, as hand anno-tation of training data is the major expense in super-vised machine learning.2 Relationship SchemaRelationship Argument 1 Argument 2has target Investigation LocusIntervention Locushas finding Investigation ConditionInvestigation Resulthas indication Drug or device ConditionIntervention ConditionInvestigation Conditionhas location Condition Locusnegation modifies Negation modifier Conditionlaterality modifies Laterality modifier InterventionLaterality modifier Locussub-location modifies Sub-location modifier LocusTable 1: Relationship types and their argument type con-straints.The CLEF application extracts entities, relation-ships and modifiers from text.
By entity, we meansome real-world thing, event or state referred to inthe text: the drugs that are mentioned, the tests thatwere carried out, etc.
Modifiers are words that qual-ify an entity in some way, referring e.g.
to the lat-erality of an anatomical locus, or the negation of acondition (?no sign of inflammation?).
Entities areconnected to each other and to modifiers by rela-tionships: e.g.
linking a drug entity to the conditionentity for which it is indicated, linking an investiga-tion to its results, or linking a negating phrase to acondition.The entities, modifiers, and relationships are de-scribed by both a formal XML schema, and by aset of detailed definitions.
These were developed bya group of clinical experts through an iterative pro-cess, until acceptable agreement was reached.
Entitytypes are mapped to types from the UMLS seman-tic network (Lindberg et al, 1993), each CLEF en-11tity type covering several UMLS types.
Relationshiptypes are those that were felt necessary to capture theessential clinical dependencies between entities re-ferred to in patient documents, and to support CLEFend user applications.Each relationship type is constrained to exist be-tween limited pairs of entity types.
For example,the has location relationship can only exist be-tween a Condition entity and a Locus entity.Some relationships can exist between multiple typepairs.
The full set of relationships and their argu-ment type constraints are shown in Table 1.
Ex-amples of each relationship are given in Roberts etal (2008a).Some of the relationships considered importantby the clinical experts were not obvious without do-main knowledge.
For example,He is suffering from nausea and severeheadaches.
Dolasteron was prescribed.Without domain knowledge, it is not clear that thereis a has indication relationship between the?Dolasteron?
Drug or device entity and the?nausea?
Condition entity.
As in this example,many of this type of relationship are intra-sentential.A single real-world entity may be referred to sev-eral times in the same text.
Each of these co-referring expressions is a mention of the entity.
Thegold standard includes annotation of co-referencebetween different textual mentions of the same en-tity.
For the work reported in this paper, however,co-reference is not considered.
Each entity is as-sumed to have a single mention.
Relationships be-tween entities can be considered, by extension, asrelationships between the single mentions of thoseentities.
The implications of this are discussed fur-ther below.3 Gold Standard CorpusThe schema and definitions were used to hand-annotate the entities and relationships in 77 oncol-ogy narratives, to provide a gold standard for sys-tem training and evaluation.
Corpora of this sizeare typical in supervised machine learning, and re-flect the expense of hand annotation.
Narrativeswere carefully selected and annotated according toa best practice methodology, as described in Robertset al(2008a).
Narratives were annotated by two in-dependent, clinically trained, annotators, and a con-sensus created by a third.
We will refer to this corpusas C77.Annotators were asked to first mark the mentionsof entities and modifiers, and then to go througheach of these in turn, deciding if any had relation-ships with mentions of other entities.
Although theannotators were marking co-reference between men-tions of the same entity, they were asked to ignorethis with respect to relationship annotation.
Boththe annotation tool that they were using and theirannotation guidelines, enforced the creation of rela-tionships between mentions, and not between enti-ties.
The gold standard is thus analogous to the styleof relationship extraction reported here, in whichwe extract relations between single mention entities,and do not consider co-reference.
Annotators werefurther told that relationships could span multiplesentences, and that it was acceptable to use clini-cal domain knowledge to infer that a relationshipexisted between two mentions.
Counts of all rela-tionships annotated in C77 are shown in Table 2,sub-divided by the number of sentence boundariesspanned by a relationship.4 Relationship ExtractionThe system we have built uses the GATE NLPtoolkit (Cunningham et al, 2002) 1.
The system isshown in Figure 1, and is described below.Narratives are first pre-processed using standardGATE modules.
Narratives were tokenised, sen-tences found with a regular expression-based sen-tence splitter, part-of-speech (POS) tagged, andmorphological roots found for tokens.
Each to-ken was also labelled with a generalised POS tag,the first two characters of the full POS tag.
Thistakes advantage of the Penn Treebank tagset usedby GATE?s POS tagger, in which related POS tagsshare the first two characters.
For example, all sixverb POS tags start with the letters ?VB?.After pre-processing, mentions of entities withinthe text are annotated.
In the experiments reported,we assume perfect entity recognition, as given bythe entities in the human annotated gold standard1We used a development build of GATE 4.0, downloadablefrom http://gate.ac.uk12Sentence boundaries between arguments0 1 2 3 4 5 6 7 8 9 >9 Totalhas finding 265 46 25 7 5 4 3 2 2 2 0 361has indication 139 85 35 32 14 11 6 4 5 5 12 348has location 360 4 1 1 1 1 1 0 0 0 4 373has target 122 14 4 2 2 4 3 1 0 1 0 153laterality modifies 128 0 0 0 0 0 0 0 0 0 0 128negation modifies 100 1 0 0 0 0 0 0 0 0 0 101sub location modifies 76 0 0 0 0 0 0 0 0 0 0 76Total 1190 150 65 42 22 20 13 7 7 8 16 1540Cumulative total 1190 1340 1405 1447 1469 1489 1502 1509 1516 1524 1540Table 2: Count of relationships in 77 gold standard documents.described above.
Our results are therefore higherthan would be expected in a system with automaticentity recognition.
It is useful and usual to fix en-tity recognition in this way, to allow tuning specificto relationship extraction, and to allow the isolationof relation-specific problems.
We accept, however,that ultimately, relation extraction does depend onthe quality of entity recognition.
The relation extrac-tion described here is used as part of an operationalIE system in which clinical entity recognition is per-formed by a combination of lexical lookup and su-pervised machine learning.
We have described ourentity extraction system elsewhere (Roberts et al,2008b).4.1 ClassificationWe treat clinical relationship extraction as a classi-fication task, training classifiers to assign a relation-ship type to an entity pair.
An entity pair is a pairingof entities that may or may not be the arguments ofa relation.
For a given document, we create all pos-sible entity pairs within two constraints.
First, en-tities that are paired must be within n sentences ofeach other.
For all of the work reported here, unlessstated, n ?
1 (crossing 0 or 1 sentence boundaries).Second, we can constrain the entity pairs createdby argument type (Rindflesch and Fiszman, 2003).For example, there is little point in creating an en-tity pair between a Drug or device entity anda Result entity, as no relationships, as specifiedby the schema, exist between entities of these types.Entity pairing is carried out by a GATE componentdeveloped specifically for clinical relationship ex-traction.
In addition to pairing entities according tothe above constraints, this component also assignsfeatures to each pair that characterise its lexical andsyntactic qualities (described further in Section 4.2).Entity pairs correspond to classifier training andtest instances.
In classifier training, if an entitypair corresponds to the arguments of a relationshippresent in the gold standard, then it is assigned aclass of that relationship type.
If it does not corre-spond to such a relation, then it is assigned the classnull.
The classifier builds a model of these entitypair training instances, from their features.
In classi-fier application, entity pairs are created from unseentext, under the above constraints.
The classifier as-signs one of our seven relationship types, or null,to each entity pair.We use Support Vector machines (SVMs) as train-able classifiers, as these have proved to be robust andefficient for a range of NLP tasks, including relationextraction.
We use an SVM implementation devel-oped within our own group, and provided as partof the GATE toolkit.
This is a variant on the orig-inal SVM algorithm, SVM with uneven margins, inwhich classification may be biased towards positivetraining examples.
This is particularly suited to NLPapplications, in which positive training examples areoften rare.
Full details of the classifier are given inLi et al(2005).
We used the implementation ?out ofthe box?, with default parameters as determined inexperiments with other data sets.SVMs are binary classifiers: the multi-class prob-lem of classifying entity pairs must therefore bemapped to a number of binary classification prob-lems.
There are several ways in which a multi-class problem can be recast as binary problems.
Thecommonest are one-against-one in which one classi-fier is trained for every possible pair of classes, andone-against-all in which a classifier is trained fora binary decision between each class and all other13classes, including null, combined.
We have car-ried out extensive experiments (not reported here),with these two strategies, and have found little dif-ference between them for our data.
We have chosento use one-against-all, as it needs fewer classifiers(for an n class problem, it needs n classifiers, as op-posed to (n?1)!2 for one-against-one).The resultant class assignments by multiple bi-nary classifiers must be post-processed to deal withambiguity.
In application to unseen text, it is possi-ble that several classifiers assign different classes toan entity pair (test instance).
To disambiguate thesecases, the output of each one-against-all classifier istransformed into a probability, and the class withthe highest probability is assigned.
Re-casting themulti-class relation problem as a number of binaryproblems, and post-processing to resolve ambigui-ties, is handled by the GATE Learning API.Figure 1: The relationship extraction system.4.2 Features for ClassificationThe SVM classification model is built from lexicaland syntactic features assigned to tokens and en-tity pairs prior to classification.
We use featuresdeveloped in part from those described in Zhou etal (2005) and Wang et al(2006).
These features aresplit into 11 sets, as described in Table 3.The tokN features are POS and surface stringtaken from a window of N tokens on each side ofeach paired entity?s mention.
For N = 6, thisgives 48 features.
The rationale behind these sim-ple features is that there is useful information in thewords surrounding two mentions, that helps deter-mine any relationship between them.
The gentokNfeatures generalise tokN to use morphological rootand generalised POS.
The str features are a setof 14 surface string features, encoding the full sur-face strings of both entity mentions, their heads,their heads combined, the surface strings of the first,last and other tokens between the mentions, andof the two tokens immediately before and after theleftmost and rightmost mentions respectively.
Thepos, root, and genpos feature sets are similarlyconstructed from the POS tags, roots, and gener-alised POS tags of the entity mentions and their sur-rounding tokens.
These four feature sets differ fromtokN and gentokN, in that they provide more fine-grained information about the position of featuresrelative to the paired entity mentions.For the event feature set, the main entitieswere divided into events (Investigation andIntervention) and non-events (all others).
Fea-tures record whether the entity pair consists of twoevents, two non-events, one of each, and whetherthere are any intervening events and non-events.This feature set gives similar information to atype(semantic types of arguments) and inter (inter-vening entities), but at a coarser level of typing.5 EvaluationWe used a standard ten-fold cross validationmethodology and standard evaluation metrics.
Met-rics are defined in terms of true positive, false pos-itive and false negative matches between relation-ships in a system annotated response document anda gold standard key document.
A response relation-ship is a true positive if a relationship of the sametype, and with the exact same arguments, exists inthe key.
Corresponding definitions apply for falsepositive and false negative.
Counts of these matchesare used to calculate standard metrics of Recall (R),Precision (P ) and F1 measure.The metrics do not say how hard relationship ex-traction is.
We therefore provide a comparison withInter Annotator Agreement (IAA) scores from thegold standard.
The IAA score gives the agreementbetween the two independent double annotators.
Itis equivalent to scoring one annotator against theother using the F1 metric.
IAA scores are not di-rectly comparable here, as relationship annotation is14Feature set Size DescriptiontokN 8N Surface string and POS of tokens surrounding the arguments, windowed ?N to +N , N = 6 by defaultgentokN 8N Root and gerenalised POS of tokens surrounding the argument entities, windowed ?N to +N , N = 6 by defaultatype 1 Concatenated semantic type of arguments, in arg1-arg2 orderdir 1 Direction: linear text order of the arguments (is arg1 before arg2, or vice versa?
)dist 2 Distance: absolute number of sentence and paragraph boundaries between argumentsstr 14 Surface string features based on Zhou et al(2005), see text for full descriptionpos 14 POS features, as aboveroot 14 Root features, as abovegenpos 14 Generalised POS features, as aboveinter 11 Intervening mentions: numbers and types of intervening entity mentions between argumentsevent 5 Events: are any of the arguments, or intevening entities, events?allgen 96 All features in root and generalised POS forms, i.e.
gentok6+atype+dir+dist+root+genpos+inter+eventnotok 48 All except tokN features, others in string and POS forms, i.e.
atype+dir+dist+str+pos+inter+eventTable 3: Feature sets used for learning relationships.
The size of a set is the number of features in that set.a slightly different task for the human annotators.The relationship extraction system is given entities,and finds relationships between them.
Human an-notators must find both the entities and the relation-ships.
Therefore, were one human annotator to failto find a particular entity, they could never find rela-tionships with that entity.
The raw IAA score doesnot take this into account: if an annotator fails tofind an entity, then they will also be penalised forall relationships with that entity.
We therefore give aCorrected IAA, CIAA, in which annotators are onlycompared on those relations for which they haveboth found the entities involved.
Both forms of IAAare shown in Table 4.
It is clear that it is hard forannotators to reach agreement on relationships, andthat this is compounded massively by lack of perfectagreement on entities.
Note that the gold standardused in training and evaluation reflects a further con-sensus annotation, to correct this poor agreement.6 Results6.1 Feature SelectionThe first group of experiments reported looks at theperformance of relation extraction with various fea-ture sets.
We followed an additive strategy for fea-ture selection.
Starting with basic features, we addedfurther features one set at a time.
We measured theperformance of the resulting classifier each time weadded a new feature set.
Results are shown in Ta-ble 4.
The initial classifier used a tok6+atypefeature set.
Addition of both dir and dist fea-tures give significant improvements in all metrics, ofaround 10% F1 overall, in each case.
This suggeststhat the linear text order of arguments, and whetherrelations are intra- or inter-sentential is important toclassification.
Addition of the str features also givegood improvement in most metrics, again 10% F1overall.
Addition of part-of-speech information, inthe form of pos features, however, leads to a dropin some metrics, overall F1 dropping by 1%.
Unex-pectedly, POS seems to provide little extra informa-tion above that in surface string.
Errors in POS tag-ging cannot be dismissed, and could be the cause ofthis.
The existence of intervening entities, as codedin feature set inter, provides a small benefit.
Theinclusion of information about events, in the eventfeature set, is less clear-cut.We were interested to see if generalising featurescould improve performance, as this had benefitedour previous work in entity extraction.
We replacedall surface string features with their root form, andPOS features with their generalised POS form.
Thisgave the results shown in column allgen.
Resultsare not clear cut, in some cases better and in someworse than the previous best.
Overall, there is nodifference in F1.
There is a slight increase in over-all recall, and a corresponding drop in precision ?as might be expected.Both the tokN, and the str and pos feature setsprovide surface string and POS information abouttokens surrounding and between relationship argu-ments.
The former gives features from a windowaround each argument.
The latter two give a greateramount of positional information.
Do these two pro-vide enough information on their own, without thewindowed features?
To test this, we removed thetokN features from the full cumulative feature set,from column +event.
Results are given in column15Relation Metric tok6+atype +dir +dist +str +pos +inter +event allgen notok IAA CIAAhas finding P 44 49 58 63 62 64 65 63 63R 39 63 78 80 80 81 81 82 82F1 39 54 66 70 69 71 72 71 71 46 80has indication P 37 23 38 42 40 41 42 37 44R 14 14 46 44 44 47 47 45 47F1 18 16 39 39 38 41 42 38 41 26 50has location P 36 36 50 68 71 72 72 73 73R 28 28 74 79 79 81 81 83 83F1 30 30 58 72 74 76 75 77 76 55 80has target P 9 9 32 63 57 60 62 60 59R 11 11 51 68 67 67 66 68 68F1 9 9 38 64 60 63 63 63 62 42 63laterality modifies P 21 38 73 84 83 84 84 86 86R 9 55 82 89 86 88 88 87 89F1 12 44 76 85 83 84 84 84 85 73 94negation modifies P 19 54 85 81 80 79 79 77 81R 12 82 97 98 93 92 93 93 93F1 13 63 89 88 85 84 85 83 85 66 93sub location modifies P 2 2 55 88 86 86 88 88 87R 1 1 62 94 92 95 95 95 95F1 1 1 56 90 86 89 91 91 90 49 96Overall P 33 38 50 63 62 64 65 64 64R 22 36 70 74 73 75 75 76 76F1 26 37 58 68 67 69 69 69 70 47 75Table 4: Variation in performance by feature set.
Features sets are abbreviated as in Table 3.
For the first sevencolumns, features were added cumulatively to each other.
The next two columns, allgen and notok, are as de-scribed in Table 3.
The final two columns give inter annotator agreement and corrected inter annotator agreement, forcomparison.notok.
There is no clear change in performance,some relationships improving, and some worsening.Overall, there is a 1% improvement in F1.It appears that the bulk of performance is attainedthrough entity type and distance features, with somecontribution from positional surface string informa-tion.
Performance is between 1% and 9% lower thanCIAA for the same relationship, with a best overallF1 of 70%, compared to a CIAA of 75%.6.2 Sentences SpannedTable 2 shows that although most relationships areintra-sentential, 23% are inter-sentential, 10% of allrelationships being between arguments in adjacentsentences.
If we consider a relationship to cross nsentence boundaries, then the classifiers described inthe previous section were all trained on relationshipscrossing n ?
1 sentence boundaries, i.e.
with argu-ments in the same or adjacent sentences.
What effectdoes including more distant relationships have onperformance?
We trained classifiers on only intra-sentential relationships, and on relationships span-ning up to n sentence boundaries, for n ?
{1...5}.We also trained a classifier on relationships with1 ?
n ?
5, comprising 85% of all inter-sententialrelationships.
In each case, the cumulative featureset +event from Table 4 was used.
Results areshown in Table 5.
It is clear from the results thatthe feature sets used do not perform well on inter-sentential relationships.
There is a 6% drop in over-all F1 when including relationships with n = 1 to-gether with n < 1.
Performance continues to drop asmore inter-sentential relationships are included, andis very poor for just inter-sentential relationships.A preliminary error analysis suggests that themore distant relationship arguments are from eachother, the more likely clinical knowledge is requiredto extract the relationship.
This raises additional dif-ficulties for extraction, which the simple features de-scribed here are unable to address.6.3 Size of Training CorpusThe provision of sufficient training data for super-vised learning algorithms is a limitation on their use.We examined the effect of training corpus size onrelationship extraction.
The C77 corpus, compris-16Number of sentence boundaries between argumentsinter- intra- inter- and intra-sentential Corpus sizeRelation Metric 1 ?
n ?
5 n < 1 n ?
1 n ?
2 n ?
3 n ?
4 n ?
5 C25 C50 C77has finding P 24 68 65 62 60 61 61 66 63 65R 18 89 81 79 78 78 77 74 74 81F1 18 76 72 69 67 68 67 67 67 72has indication P 18 49 42 42 36 32 30 22 25 42R 17 59 47 42 42 39 38 30 31 47F1 16 51 42 39 37 34 33 23 25 42has location P 0 74 72 73 72 72 72 72 71 72R 0 83 81 81 81 82 82 76 80 81F1 0 77 75 76 75 76 76 73 74 75has target P 3 64 62 59 60 59 58 65 49 62R 1 75 66 64 62 61 61 60 65 66F1 2 68 63 61 60 60 59 59 54 63laterality modifies P 0 86 84 86 86 86 87 77 78 84R 0 89 88 88 88 87 88 69 68 88F1 0 85 84 85 86 85 86 72 69 84negation modifies P 0 80 79 79 80 80 80 78 79 79R 0 94 93 91 93 93 93 80 93 93F1 0 86 85 84 85 86 85 78 84 85sub location modifies P 0 89 88 88 89 89 89 64 91 88R 0 95 95 95 95 95 95 64 85 95F1 0 91 91 91 91 91 91 64 86 91Overall P 22 69 65 64 62 61 60 62 63 65R 17 83 75 73 71 70 70 65 71 75F1 19 75 69 68 66 65 65 63 66 69Table 5: Variation in performance, by number of sentence boundaries (n), and by training corpus size.ing 77 narratives and used in the previous experi-ments, was subsetted to give corpora of 25 and 50narratives, which will be referred to as C25 and C50respectively.
We trained two further classifiers onthese new corpora.
Again, the cumulative featureset +event from Table 4 was used.
Results areshown in Table 5.
Overall, performance improves astraining corpus size increases (F1 rising from 63%to 69%).
We were struck however, by the fact thatincreasing from 50 to 77 documents has little effecton a few relationships (negation modifies andhas location).
It may well be that the amountof training data required has plateaued for those re-lationships.7 ConclusionWe have shown that it is possible to extract clini-cal relationships from text, using shallow features,and supervised statistical machine learning.
Judg-ing from poor inter annotator agreement, the taskis hard.
Our system achieves a reasonable perfor-mance, with an overall F1 just 5% below a cor-rected inter annotator agreement.
This performanceis reached largely by using features of the text thatencode entity type, distance between arguments, andsome surface string information.
Performance does,however, vary with the number of sentences spannedby the relationships.
Learning inter-sentential rela-tionships does not seem amenable to this approach,and may require the use of domain knowledge.A major concern when using supervised learningalgorithms is the expense and availability of trainingdata.
We have shown that while this concern is jus-tified in some cases, larger training corpora may notimprove performance for all relationships.The technology used has proved scalable.
Thefull CLEF IE system, including automatic entityrecognition, is able to process a document in sub-second time on a commodity workstation.
Wehave used the system to extract 6 million relationsfrom over half a million patient documents, for usein downstream CLEF applications (Roberts et al,2008a).
Our future work on relationship extrac-tion in CLEF includes integration of a dependencyparse into the feature set, further analysis to deter-mine what knowledge may be required to learn inter-sentential relations, and integration of relationshipextraction with a co-reference algorithm.17Availability All of the software described hereis open source and can be downloaded as part ofGATE, with the exception of the entity pairing com-ponent, which will be released shortly.
We are cur-rently preparing a UK research ethics committee ap-plication, requesting permission to release our anno-tated corpus.AcknowledgementsCLEF is funded by the UK Medical Research Coun-cil.
We would like to thank the Royal MarsdenHospital for providing the corpus, and our clinicalpartners in CLEF for assistance in developing theschema, and for gold standard annotation.ReferencesH.
Cunningham, D. Maynard, K. Bontcheva, andV.
Tablan.
2002.
GATE: A framework and graphi-cal development environment for robust NLP tools andapplications.
In Proceedings of the 40th AnniversaryMeeting of the Association for Computational Linguis-tics, pages 168?175, Philadelphia, PA, USA, July.C.
Friedman, P. Alderson, J. Austin, J. Cimino, andS.
Johnson.
1994.
A general natural-language textprocessor for clinical radiology.
Journal of the Amer-ican Medical Informatics Association, 1(2):161?174,March.C.
Grover, B. Haddow, E. Klein, M. Matthews,L.
Nielsen, R. Tobin, and X. Wang.
2007.
Adaptinga relation extraction pipeline for the BioCreAtIvE IItask.
In Proceedings of the BioCreAtIvE II Workshop2007, Madrid, Spain.U.
Hahn, M. Romacker, and S. Schulz.
2002.
MEDSYN-DIKATE ?
a natural language system for the ex-traction of medical information from findings reports.International Journal of Medical Informatics, 67(1?3):63?74, December.Y.
Li, K. Bontcheva, and H. Cunningham.
2005.SVM based learning system for information extrac-tion.
In Deterministic and statistical methods in ma-chine learning: first international workshop, number3635 in Lecture Notes in Computer Science, pages319?339.
Springer.D.
Lindberg, B. Humphreys, and A. McCray.
1993.
TheUnified Medical Language System.
Methods of Infor-mation in Medicine, 32(4):281?291.Y.
Lussier, T. Borlawsky, D. Rappaport, Y. Liu, andC.
Friedman.
2006.
PhenoGO: Assigning phenotypiccontext to Gene Ontology annotations with natural lan-guage processing.
In Biocomputing 2006, Proceed-ings of the Pacific Symposium, pages 64?75, Hawaii,USA, January.S.
Pakhomov, J. Buntrock, and P. Duffy.
2005.
Highthroughput modularized NLP system for clinical text.In Proceedings of the 43rd Annual Meeting of theAssociation for Computational Linguistics (ACL?05),interactive poster and demonstration sessions, pages25?28, Ann Arbor, MI, USA, June.A.
Rector, J. Rogers, A. Taweel, D. Ingram, D. Kalra,J.
Milan, P. Singleton, R. Gaizauskas, M. Hepple,D.
Scott, and R. Power.
2003.
CLEF ?
joining uphealthcare with clinical and post-genomic research.
InProceedings of UK e-Science All Hands Meeting 2003,pages 264?267, Nottingham, UK.T.
Rindflesch and M. Fiszman.
2003.
The interaction ofdomain knowledge and linguistic structure in naturallanguage processing: interpreting hypernymic propo-sitions in biomedical text.
Journal of Biomedical In-formatics, 36(6):462?477.A.
Roberts, R. Gaizauskas, M. Hepple, G. Demetriou,Y.
Guo, A. Setzer, and I. Roberts.
2008a.
Seman-tic annotation of clinical text: The CLEF corpus.
InProceedings of Building and evaluating resources forbiomedical text mining: workshop at LREC 2008,Marrakech, Morocco, May.
In press.A.
Roberts, R. Gaizauskas, M. Hepple, and Y. Guo.2008b.
Combining terminology resources and statis-tical methods for entity recognition: an evaluation.In Proceedings of the Sixth International Conferenceon Language Resources and Evaluation, LREC 2008,Marrakech, Morocco, May.
In press.N.
Sager, M. Lyman, C. Bucknall, N. Nhan, and L. Tick.1994.
Natural language processing and the representa-tion of clinical data.
Journal of the American MedicalInformatics Association, 1(2):142?160, March-April.T.
Wang, Y. Li, K. Bontcheva, H. Cunningham, andJ.
Wang.
2006.
Automatic extraction of hierarchicalrelations from text.
In The Semantic Web: Researchand Applications.
3rd European Semantic Web Con-ference, ESWC 2006, number 4011 in Lecture Notesin Computer Science, pages 215?229.
Springer.G.
Zhou, J. Su, J. Zhang, and M. Zhang.
2005.
Ex-ploring Various Knowledge in Relation Extraction.
InProceedings of the 43rd Annual Meeting of the Asso-ciation for Computational Linguistics (ACL?05), pages427?434, Ann Arbor, MI, USA, June.P.
Zweigenbaum, B. Bachimont, J. Bouaud, J. Charlet,and J-F. Boisvieux.
1995.
A multi-lingual architec-ture for building a normalised conceptual representa-tion from medical language.
In Proceedings of the An-nual Symposium on Computer Applications in MedicalCare, pages 357?361, New York, NY, USA.18
