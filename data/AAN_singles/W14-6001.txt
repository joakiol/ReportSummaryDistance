Proceedings of SADAATL 2014, pages 1?10,Dublin, Ireland, August 24, 2014.Investigating Context Parameters in Technology Term RecognitionBehrang Q. Zadeh and Siegfried Handschuh??
?Insight Centre of Data AnalyticsNational University of Ireland, Galway?Department of Computer Science and MathematicsUniversity of Passau, Germany{behrang.qasemizadeh, siegfried.handschuh}@insight-centre.orgAbstractWe propose and evaluate the task of technology term recognition: a method to extract technol-ogy terms at a synchronic level from a corpus of scientific publications.
The proposed methodis built on the principles of terminology extraction and distributional semantics.
It is realizedas a regression task in a vector space model.
In this method, candidate terms are first extractedfrom text.
Subsequently, using the random indexing technique, the extracted candidate termsare represented as vectors in a Euclidean vector space of reduced dimensionality.
These vectorsare derived from the frequency of co-occurrences of candidate terms and words in windows oftext surrounding candidate terms in the input corpus (context window).
The constructed vectorspace and a set of manually tagged technology terms (reference vectors) in a k-nearest neigh-bours regression framework is then used to identify terms that signify technology concepts.
Weexamine a number of factors that play roles in the performance of the proposed method, i.e.
theconfiguration of context windows, neighborhood size (k) selection, and reference vector size.1 IntroductionTechnology terms and their corresponding concepts are part and parcel of any system that tries to cap-ture competitive technological intelligence (QasemiZadeh, 2010; Newman et al., 2014).
We propose amethod of technology term recognition (TTR) at a synchronic level, i.e., the identification of terms thatcorrespond to technological concepts from a corpus of scientific publications.
TTR can be viewed as akind of automatic term recognition (ATR) task.
The input of ATR is a large collection of documents,i.e.
a domain-specific corpus, and the output is a terminological resource.
The generated terminologicalresource embraces terms that signify a wide spectrum of concepts in domain knowledge represented bythe input corpus.
The extracted terms and their corresponding concepts, however, can be further orga-nized in several categories; each category characterizes a group of ?similar?
concepts (e.g.
technology) indomain knowledge.1TTR, therefore, goes beyond ATR and targets a subset of terms that characterizesthe category of technological concepts in domain knowledge (Figure 1).Establishing a precise definition of technology?and subsequently finding its corresponding terms?isa fundamental problem studied in philosophy of science.
The most simplistic definition of technology,perhaps, can be found in a dictionary.
For example, Oxford dictionary defines technology as the ?appli-cation of scientific knowledge for practical purposes?.
As to our understanding, technology terms signalconcepts that involve processes?a series of actions taken in order to achieve a particular goal?e.g.
asmanifested in practical applications of a research.
Consequently, technology terms should not be con-fused with other categories of terms, e.g.
terms that signify research subjects or problems.2For example,in computational linguistics literature, both ?language resource?
and ?natural language processing?
areThis work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedings footerare added by the organizers.
Licence details: http://creativecommons.org/licenses/by/4.0/1Or, contrariwise, a group of similar concepts can form a category.2Even though, these category of terms are strongly correlated.
For instance, a technology may provide a solution for aresearch problem and can be defined in the scope of a research subject.
Therefore, it is important to note that a researchproblem or a research subject is not a technology.1Valid TermsTechnology TermsCandidate TermsAll Possible TermsFigure 1: A Venn diagram that illustrates relationships between candidate terms, valid terms and tech-nology terms.
ATR targets the identification of valid terms amongst candidate terms.
TTR, however,targets the identification of technology terms amongst candidate terms, i.e.
a subset of valid terms.Text Candidate Term ExtractionScoring and RankingTermhoodUnithoodValid TermsFigure 2: Prevalent architecture of the terminology mining methods.valid terms; however, we only recognize the latter as a valid technology term.3In this example, ?lan-guage resource?
signals artefacts such as lexicons and corpora.
Although the process of creating theseartefacts involves several technologies, we do not consider them?and subsequently the term ?languageresource?
?as technology.In the absence of an analytical answer to the question ?what is technology?
?, we suggest exploiting thecontext of terms in order to identify technology terms among them.
We believe that technology terms tendto appear in similar linguistic contexts.
By extending Harris?s (1954) distributional hypothesis, we claimthat the context of (previously) known technology terms can be modelled and used in order to identifynew unknown technology terms.
We thus take a distributional approach to the problem of technologyterm recognition.
Consequently, we tie the context of terms to their meaning by quantification of theirdistributional similarities.
We employ vector spaces to model such distributional similarities (Turney andPantel, 2010).
Consequently, the proposed method for TTR is realized as a term classification task in avector space model (VSM).The proposed method employs the prevalent mechanism of terminology extraction in the form of atwo-step procedure: candidate term extraction followed by term scoring and ranking (Figure 2).
Can-didate term extraction deals with the term formation and the extraction of term candidates.
We employa linguistic filtering based on part-of-speech (PoS) tag sequences for the extraction of candidate terms.Subsequent to candidate term extraction, a scoring procedure?which can be seen as a semantic weight-ing mechanism?is employed to indicate how likely it is that a candidate term is a technology term.
Assuggested in Figure 2, the scoring procedure in ATR usually combines scores that are known as termhoodand unithood.
Unithood indicates the degree to which a sequence of tokens can be combined to form acomplex term (a lexical unit that is made of more than one token).
Unithood is, thus, a measure of thesyntagmatic relation between the constituents of complex terms: a lexical association measure to identifycollocations.4Termhood, on the other hand, ?is the degree to which a stable lexical unit is related to somedomain-specific concepts?
(Kageura and Umino, 1996).
It characterizes a paradigmatic relation betweenlexical units?either simple (made of one token) or complex terms?and the communicative context thatverbalizes domain-concepts.
In this paper, in order to simplify the evaluation framework, we assume thatthe PoS-based approach to candidate term extraction implicitly characterizes the unithood score.
Thefocus is thus on the termhood measure.We devise a termhood measure to distinguish technology terms from the set of extracted candidateterms.
We assume that the association of a term to a technology concept (i.e.
what termhood determines)is a kind of paradigmatic relation that can be characterized using the syntagmatic relations of the termand its co-occurred, surrounding words in a context window (Figure 3).
Words appeared in context win-3Please note that ?natural language processing?, in its alternative sense, can also signal a research subject as well as aresearch problem.4See Evert (2004) on the application of lexical association measures for the identification of collocations.2Information Extractiontechnologytodiscoverknowledgeintext.?
?
?currentemployingwhenarisethatchallengesdiscuss?
?
?Information Extraction methods for the offline construction ofknowledge ?
?
?differentusingofimpacttheofpicture?
?
?Information Extraction has been stimulatedbytheMessage Understanding ?
?
?oftechnologytheofdevelopmenttheon?
?
?Figure 3: Example of a context window of size 3 that extends around terms: words that are placed inrectangles.
In this example, this context-window is shown for the occurrences of the term ?informationextraction?
in three different sentences.CandidateTermVector Space Construction Similarity to Reference VectorsRandom Indexing k-NN RegressionTechnology-ClassMembershipFigure 4: Method for measuring the association of a candidate term to the category of technology terms.dows are represented by the elements of the standard basis of a vector space.5The frequency of words incontext windows of a candidate term (in the whole corpus) then determines the coordinates of the vectorthat represent the candidate term.
To avoid the curse of dimensionality, the VSMs are constructed at re-duced dimensionality using the random indexing technique .
In this VSM, we characterize the categoryof technology terms using a set of reference terms, i.e.
previously known technology terms.
Conse-quently, the proximity of vectors that represent candidate terms to the vectors that represent referenceterms determines the association of candidate terms to the category of technology terms.
This associationis measured using a k-nearest neighbours (k-nn) regression algorithm.
Figure 4 illustrates the method.In the proposed technique, finding context window?s properties that best characterize technologyterms is the major research concern that should be investigated.
These properties are the size of theco-occurrence region, the position of a term in the context window and the direction in which the neigh-bourhood is extended (see Lenci, 2008).
To find the most discriminative context window, we constructseveral VSMs; each VSM represents a context window of a certain configuration (i.e.
size, direction andthe word order information).
We then examine the discriminative power of context windows by reportingthe performance of the k-nn regression algorithm in these VSMs.
Furthermore, to examine the role of thenumber of reference vectors in the performance of the classification task, we repeat these experimentsusing various numbers of reference vectors.
We report the results of similar evaluation methodology,however, using a k-nn voting algorithm in Zadeh and Handschuh (2014b).In the rest of this paper, we first detail the evaluation framework in Section 2: the employed corpus forthe evaluation in Section 2.1, the construction of vector spaces in Section 2.2, the scoring procedure inSection 2.3 and the evaluation methodology in Section 2.4.
Subsequently, we report the observed resultsin Section 3 and conclude in Section 4.2 Setting the Scene2.1 Evaluation CorpusIn order to evaluate the proposed method, we employ the ACL anthology reference corpus (ACLARC) (Bird et al., 2008) and the ACL reference dataset for terminology extraction and classification(AC RD-TEC) (Zadeh and Handschuh, 2014a).6The ACL ARC has been developed with the aim ofproviding a platform for benchmarking methods of scholarly document processing.
It consists of 10,922articles that were published between 1965 to 2006 in the domain of computational linguistics.
Thesearticles are digitized and enriched with bibliography metadata.
The provided resources in the ACL ARCconsist of three layers: (a) source publications in portable document format (PDF), (b) automaticallyextracted text from the articles and (c) bibliographic metadata and citation network.
Each of the articlesin the collection is assigned to a unique identifier that indicates the source (e.g.
journal or conference)5That is, informally, each dimension of the vector space.6The ACL RD-TEC can be obtained from the European Language Resources Association, catalogue ELRA-T0375.3Type Token Sentence Paragraph Section Publication704,085 36,729,513 1,564,430 510,366 92,935 10,922Table 1: Summary statistics of the dataset derived from automatic processing of the ACL ARC.Total# Length = 1 Length = 2 Length = 3 Length = 4 Length = 5Technology Terms 13,841 759 8,674 3,826 539 43Valid Terms 22,044 1,503 14,148 5,680 659 54Invalid Terms 61,758 15,887 33,474 11,016 1,210 171Total Annotated 83,802 17,390 47,622 16,696 1,869 225Table 2: Summary statistics of the annotated candidate terms.and the date (e.g.
1999, 2006, etc.)
of publication.The ACL RD-TEC is a spin-off of the ACL ARC corpus.
It further enriches the ACL ARC metadatausing automatic and manual annotations.
The ACL RD-TEC employed the SectLabel module of Luonget al.
?s (2010) ParsCit tool for the automatic identification of logical text sections in ACL ARC?s rawtext files.
The resulting segmented text units are cleansed using a set of heuristics; for instance, brokenwords and text segments are joined, footnotes and captions are removed, and sections are organised intoparagraphs.
Text sections are further segmented into PoS-tagged sentences and each linguistically well-defined unit, e.g.
types (i.e.
PoS-tagged and lemmatized words), sentences, paragraphs and (sub)sections,is assigned to a unique identifier.
These text units are stored and presented in inverted index files, in a tab-separated format.
Hence, text units can be easily traced back to the contexts and, eventually, publicationsthat they appeared in.
Table 1 shows the statistics of text segments in the dataset.The ACL RD-TEC consists of manual annotations that can be used for the evaluation of ATR and termclassification tasks.
In its current release, more than 80,000 lexical units7are annotated as either valid orinvalid terms.
For a given lexical form t, if t refers to a significant concept in the computational linguisticsdomain, it is annotated as valid.8Examples of valid terms are ?natural language?
and ?terminology?.
Inaddition, valid terms are classified as those that can signal a technology concept.
Technology termsindicate a method or a process that is employed to accomplish a task; examples of these terms are?parsing?
and ?information retrieval?, and more delicate terms such as ?linear interpolation?.Similar to the valid terms, terms that are annotated as technology terms do not exclusively belongto this class.
For example, ?computational linguistics?
is a lexical form that can be classified as atechnology term, e.g., in ??
?
?
promising area of application of computational linguistics techniques?
?
?
?.However, it can also signal other concepts such as a scientific discipline as well as a community, e.g.,in ??
?
?
theoretical work in computational linguistics?
?
?
?
and ??
?
?
pursued by the computational linguis-tics community ?
?
?
?, respectively.As reported in Zadeh and Handschuh (2014a), the observed agreement between 4 participants in themanual annotation of technology terms from a small set of randomly selected candidate terms is 0.828;the obtained Cohen?s kappa coefficient for inter annotator agreement is 0.627.
Table 2 shows the currentstatistics of the annotated terms.2.2 Construction of Vector Space Models using Random IndexingWe employ random indexing (RI) for the construction of the VSMs (Kanerva et al., 2000).
For a corpusof a relatively small size, the context of terms can be represented and efficiently examined with the helpof conventional vector space construction methods.
The vector space is first constructed and then it maybe followed by a dimensionality reduction technique.
However, as the corpus grows and the numberof elements that are employed for context definition increases, due to the high dimensionality of thevector space (orders of millions), these algorithms may suffer from low computational performance.RI is an approach that alleviates this problem by combining the construction of a vector space andthe dimension reduction process.
RI is based on normal random projection.
It thus guarantees that7A lexical unit is defined as a single token, part of a word, a word or a combination of these.8However, it is not guaranteed that all the occurrences of t in the corpus are valid terms.4the relative Euclidean distance between vectors, as well as their cosine similarity, in the original high-dimensional vector space is preserved in the vector space that is constructed at a reduced dimensionality.The vector space construction is a two-step procedure: construction of (a) index vectors followed by theconstruction of (b) context vectors.In the first step, each context element (i.e.
a PoS-tagged word in a context window) is assigned exactlyto one index vector.
Index vectors are high-dimensional randomly generated vectors, in which most ofthe elements are set to 0 and only a few to 1 and ?1.9Once an index vector is generated and assignedto a context element, this information is stored so that it can be retrieved and used for later analysis.
Inthe second step, the construction of context vectors, each candidate term is assigned to a vector of whichall elements are zero.
This context vector has the same dimension as the index vectors.
For each co-occurrence of a candidate term (represented by the context vector ~vti) and a context element (representedby the index vector ~rwj) in the corpus, the context vector for the candidate term is accumulated bythe index vector of the context element, i.e.
~vti= ~vti+ ~rwj.
The corpus is scanned for all the co-occurrences of candidate terms and context elements and the context vectors are updated to reflect theseco-occurrences.
The result is a VSM that is constructed directly in the reduced dimensionality andrepresents candidate terms using the defined context.For instance, in the example given in Figure 3, the term ?information extraction?
is assigned to acontext vector.
If a context window of size 3 that extends in both directions of candidate terms anddiscards word order information is employed, then each unique word in the rectangles, e.g.
has, current,technology, the, been, etc., is assigned to a randomly generated index vector.
The context vector is thenobtained by the accumulation of these index vectors.
If the word order information is encoded, then theappearance of each word at a certain position in the context window must be assigned to a unique indexvector.
As a result, in the given example in Figure 3, the word ?technology?
in the first sentence (position1 after target term) and the third sentence (position 2 before the target term) is assigned to two differentindex vectors, each uniquely represents the word ?technology?
at these positions.In order to employ random indexing, two parameters must be decided: the dimensionality of theVSM and the number of non-zero elements.
As described in Zadeh and Handschuh (2014c), using theprovided proofs in Li et al.
(2006), it can be verified that the dimensionality of RI-constructed VSMsis determined independently of the number of context elements n (i.e.
the original dimensionality ofthe vector space).
It is, however, determined by the probability and the maximum expected amount ofdistortions in pairwise distances and the number of context vectors in the model (in logarithmic scale).The number of non-zero elements, on the other hand, is decided by the number of context elements andthe sparseness of the VSM at the original high dimension (?)
as O(??n).
Accordingly, in the reportedexperiment in this paper, we set the dimension of RI-constructed VSMs to 1800, which is large enoughto make sure that the distances are preserved in the constructed VSMs.
In our experimental setup usingthe contexts that are described in Section 2.4.1, the estimated non-reduced, original dimension of thevector space is between 700,000 and 7 million;10hence, we set 8 elements of index vectors to ?1.2.3 Term Scoring Method: k-Nearest Neighbours RegressionWe employ a standard k-nn regression method to assign scores to the extracted candidate terms.
In thisframework, context vectors that represent candidate terms are compared to those that represent a set ofreference terms Rs.
Rsconsists of both technology and non-technology terms that are manually taggedprior to the similarity measurement task.
We employ the cosine similarity.
For each candidate termt, terms in Rsare sorted in descending order by their cosine similarity to t. We calculate the sum ofthe similarity of valid technology terms to t in the top k terms of this sorted list and consider it as ameasurement of the technology-term class membership for t. As described later in Section 2.4.2, the9This distribution of zero and?1 elements in index vectors this leads to a Gaussian asymptotic distribution and consequentlya Gaussian random projection matrix (see Zadeh and Handschuh, 2014c, for further explanation).10In the defined contexts for our experiments, the original dimension of the vector space is determined by the number oftypes in the corpus, i.e.
700,000.
This number increases when the word order information is also encoded.
In the reportedexperiment, this number escalates to 7 million for a context window of size 5 that extends around the terms and encode wordorder information).5#TermTotal#TermTechnologyAverage Length f (Sentence) f (Paragraph) f (Section) f (Document)3,490 1,596 2.037 1,696,201 1,264,616 870,574 346,000Table 3: Summary statistics of the reference terms Rs.
f(x) denotes the accumulative frequency ofoccurrences of all the terms in Rsin text segments of type x.neighbourhood size k is defined relative to the size of Rs.As stated earlier, candidate terms are extracted from the ACL ARC corpus using a part-of-speech-based filtering technique.
In this method, any sequence of tokens in the corpus that conforms to one ofthe predefined part-of-speech tag sequences is considered as a term candidate.
By employing 31 differentpatterns of length 1 to 5, we extract 1.3 million candidate terms.11Using the k-nn regression describedabove, the technology-term class membership is calculated for all the candidate terms.2.3.1 Reference Term FormationPrior to the k-nn regression task, we extract all the candidate terms which ended or collocated withthe words ?technology?
and ?technique?
(in their lemmatized form).
Examples of the extracted terms are?unsupervised text categorization?, ?basic estimation?, ?bi-directional bottom-up?
and ?boolean keyword?.These terms are then manually annotated as technology and non-technology terms.
For example, in thelist of terms given above, only ?unsupervised text categorization?
is annotated as a technology term.
Theprocess resulted in a set of reference terms Rsconsisting of 3490 terms of which 1596 are annotatedas technology terms (i.e.
positive examples).
The accumulative frequency of the occurrences of theextracted reference terms in the corpus are given in Table 3.2.4 Evaluation MethodologyIn the reported evaluation framework, the procedure described in Section 2.2 is performed to constructseveral vector spaces of various context configurations, which are described in Section 2.4.1.
The de-scribed procedure for term scoring in Section 2.3 is then employed to assign scores to the extractedcandidate terms in all the constructed vector spaces.
In each experiment, candidate terms are sorted indescending order by their assigned scores.
The proportion of technology terms in the list of the top nterms (we start with 250 terms) is reported for the comparison of the performance of the evaluated con-text configurations.
We further investigate the role of the neighbourhood size selection k as well as thenumber of reference terms Rsin the performance of the scoring task.2.4.1 Evaluated Context ParametersIn the reported evaluation, the terms?
contexts are defined by the frequency of distinct PoS-tagged wordsthat co-occurred with terms in a text window of limited size.
We evaluate context windows that areconfigured with three parameters: direction, size and order.The first parameter distinguishes context windows according to the direction in which they are ex-panded to collect the co-occurrence counts.
The context window of a term is expanded (a) to the left-hand side of the term to count the co-occurrence counts of the term with its preceding words in eachsentence of the corpus, (b) to the right-hand side to collect co-occurrences with the succeeding words or(c) around the term, i.e.
in both left and right directions.
The context windows are also configured bytheir size, i.e.
the extent of terms?
neighbourhood for counting the co-occurrences.
As stated in Sahlgren(2008), an optimum size of a context window can only be found through experiments.
However, he alsosuggests that if the goal is to capture a paradigmatic relation (such as the one proposed here), then narrowcontext windows outperform wide context windows.
As a result, in our experiments we limit the size ofcontext windows w to 1 ?
w ?
5.
For the context windows that expand around terms, we extend thecontext region symmetrically in both directions.
As stated earlier, Figure 3 illustrates a context windowof size 3 that extends around terms.Some research suggests that the sequential order of words expresses information about the grammati-cal behaviour of words and, therefore, the inclusion of this information in a distributional model enhances11The extracted candidate terms are included in the ACL RD-TEC.6the performance.
We investigate the impact of word order information on the performance of the sug-gested task.
Capturing word order information requires distinguishing the location of words in contextwindows.
To attain this goal in the random indexing technique, as stated earlier in Section 2.2, the ap-pearance of the same word in different positions in a context window is recognized by assigning theword to several index vectors, each index vector denotes the appearance of the word in certain position.Alternatively, the order of words (i.e.
the position of a word in a context window) can be captured byshuffling their index vectors via a permutation function (Sahlgren et al., 2008).12In our implementation,a circular shift function serves as the permutation function.
Accordingly, if m is the number of tokensafter/before a target term and a word in a context window, then the index vector of the word is shifted mtimes circularly to the right/left before its accumulation to the target term?s context vector.2.4.2 Evaluated Parameters of k-Nearest Neighbours RegressionIn addition to different configurations of context windows, we investigate the role of two other parame-ters in the performance of the proposed k-nn-based method: the neighbourhood size k and the numberof reference vectors |Rs|.
The performance of k-nn is largely dependent on the value of k: a smallvalue for k leads to over-fitting, while a large neighbourhood estimation may reduce the discriminatorypower of the classifier.
The optimum k is subject to the number of reference vectors and the underlyingprobability distribution of target instances in the vector space.
The underlying probability distribution isunknown and difficult to estimate.
Therefore, the optimal value of k is usually obtained by an experi-mental method (Yang, 1999).
Yang also suggests that the performance of the k-nn algorithm is relativelystable for a large range of k values.
Accordingly, we perform an empirical assessment by inspecting theoutput of the proposed method with respect to various values of k that are defined in relation to |Rs|.
Themain objective of our experiment, however, is to examine whether the best-performing context configu-ration can be distinguished irrespective of the value of k. Accordingly, we report the performance of thescoring procedure when k = bpRsc, for p ?
{0.001, 0.005, 0.01, 0.1, 0.2}.Building reference vectors Rsis laborious; it entails a manual annotation of terms.
It requiresa domain expert to provide a list of representative technology terms (positive examples) and non-technology terms (negative examples) from the corpus that is being analyzed.13As a result, a Rsofsmall size is often more desirable than a large one.
In k-nn, a small Rsis also desirable from thecomputational complexity point of view.
However, using a large set of reference vectors often yieldshigher performance.
As a result, the choice of the number of reference vectors |Rs| is a trade-offbetween efficiency and performance.
We thus compare the performance of the method for values of|Rs| ?
{100, 200, 300, 600, 1100, 1600, 3200, 3490}.
In each experiment, we made sure that the createdRshas a balanced number of positive and negative examples; however, the terms are chosen randomly.3 Observed Evaluation ResultsTable 4 reports the observed results in the first set of experiments.
We start to score all the candidateterms using the complete set of reference vectors, i.e.
|Rs| = 3490.
We perform the experiments for allthe possible configurations of context windows, as described in Section 2.4.
Each of these experimentsare repeated for k = bp|Rs|c, for p ?
{0.001, 0.005, 0.01, 0.1, 0.2}.
In these experiments, therefore,the assessed values of k are {3, 17, 34, 349, 698}.
Table 4a and 4b shows the observed results in theconstructed VSMs when the word order information is excluded and included, respectively.
In bothtables, columns show the observed proportion of technology terms in the top 250 terms in the list ofcandidate terms that are weighted using the proposed method; thus, the closer a number is to 1, thehigher the performance.
We suggest Frantzi et al.
?s (1998) c-value score?a general ATR algorithm?for the baseline measure.
The c-value score of a term is measured by its frequency in the corpus that isnormalized by its length and the frequency of its occurrences in other longer terms as a nested term.
Inour experiment, the proportion of technology terms in the top 250 terms in the list of candidate termsthat are weighted by the c-value score is 0.252.12Assuming that shuffling of index vectors is equivalent to generating a new one.13Depending on the type of classification?regression technique, the negative examples may not be required.7Context Value of p in k = bp|Rs|c = bp ?
3400cType Size 0.001 0.005 0.01 0.10 0.20Left1 0.264 0.592 0.596 0.580 0.5362 0.132 0.688 0.664 0.644 0.5563 0.376 0.688 0.696 0.592 0.5124 0.524 0.692 0.656 0.616 0.5365 0.636 0.660 0.660 0.580 0.504Right1 0.124 0.440 0.512 0.224 0.2842 0.628 0.688 0.288 0.380 0.3603 0.684 0.496 0.440 0.380 0.3804 0.596 0.664 0.340 0.456 0.4125 0.708 0.632 0.672 0.452 0.436Around1 0.440 0.748 0.744 0.608 0.6162 0.716 0.836 0.848 0.764 0.7003 0.760 0.840 0.816 0.696 0.6644 0.160 0.800 0.788 0.672 0.6605 0.144 0.748 0.740 0.660 0.660(a) No Word Order InformationValue of p in k = bp|Rs|c = bp ?
3400c0.001 0.005 0.01 0.10 0.200.264 0.592 0.596 0.580 0.5360.128 0.660 0.644 0.588 0.5240.344 0.676 0.652 0.544 0.4480.400 0.648 0.664 0.524 0.4360.448 0.632 0.640 0.488 0.4080.124 0.440 0.512 0.224 0.2840.696 0.736 0.788 0.316 0.3680.720 0.388 0.688 0.304 0.3280.580 0.376 0.600 0.324 0.3280.768 0.384 0.552 0.328 0.3280.376 0.768 0.804 0.704 0.6240.796 0.836 0.824 0.644 0.6080.760 0.828 0.808 0.636 0.5720.784 0.808 0.776 0.612 0.5400.784 0.808 0.756 0.580 0.516(b) Encoded Word Order InformationTable 4: The observed results from the performed evaluations.
The number columns show the proportionof technology terms in the top 250 terms for various values of k.10?30.1 0.20.40.60.8pTtTcw = 110?30.1 0.20.40.60.8pw = 210?30.1 0.20.40.60.8pw = 310?30.1 0.20.40.60.8pw = 410?30.1 0.20.40.60.8pw = 5Around Left Right Right?Left?Around?Figure 5: The performance of various context configurations over various neighbourhood sizes of k =bp|Rs|c.
|TtTc| denotes the proportion of technology terms in the top 250 terms that are sorted by theirassigned scores.
?denotes context types that encode word order information.
The minimum value of|TtTc| axis is set to 0.252, i.e.
our baseline.
The baseline is the proportion of technology terms in the top250 terms in the list of candidate terms that are weighted using the c-value technique.As shown in Figure 5, weighting terms in the VSMs that are built using context windows that extend inboth directions of candidate terms (i.e.
around candidate terms) outperforms VSMs constructed by othertypes of context windows.
In addition, as can also be verified in Table 4, the results from narrow contextwindows (1 ?
w ?
3) are more desirable than those from wide context windows (w ?
3).
However,in contrast to our previous experiments, in which we employed an unweighted k-nn voting classificationframework (Zadeh and Handschuh, 2014b), encoding word order information in the constructed VSMsdoes not necessarily improve the results.
It is important to note that for different values of k, althoughthe overall performance of the TTR method changes, the relative performance of the employed contextwindows with respect to each other is, nearly, constant.
Therefore, we conclude that the best-performingcontext type, thus VSM, can be decided independently of the value of k: a result similar to that reportedin Zadeh and Handschuh (2014b) for an unweighted k-nn voting classification.3.1 Inspecting the Effect of Reference Vector SizeWe are interested in studying the effect of reference vector size, i.e.
|Rs|, on the overall performanceof the technology term recognition task.
In this set of experiments, we limit our evaluation to the best-performing context window in the previous evaluation task, i.e.
the context window that extends aroundcandidate terms.
We repeat the scoring process for |Rs| ?
{100, 200, 300, 600, 1600, 3490}.
Similar tothe previous set of experiments, we define and express the neighbourhood size (k) with respect to the8the neighbourhood size (k)Value of p in k = bp|Rs|cw k = 1 0.001 0.005 0.01 0.10 0.20|Rs|=1001 0.194 0.194 0.194 0.194 0.458 0.4822 0.228 0.228 0.228 0.228 0.466 0.4903 0.276 0.276 0.276 0.276 0.440 0.4724 0.076 0.076 0.076 0.076 0.470 0.4885 0.016 0.016 0.016 0.016 0.486 0.448|Rs|=3001 0.352 0.352 0.352 0.428 0.538 0.5262 0.390 0.390 0.390 0.492 0.646 0.6383 0.450 0.450 0.450 0.532 0.592 0.5664 0.020 0.020 0.020 0.382 0.584 0.5905 0.034 0.034 0.034 0.004 0.586 0.592|Rs|=16001 0.958 0.958 0.484 0.670 0.590 0.5422 0.958 0.958 0.718 0.710 0.660 0.6123 0.958 0.958 0.674 0.702 0.592 0.5744 0.216 0.216 0.616 0.676 0.590 0.5465 0.502 0.502 0.368 0.570 0.566 0.552the neighbourhood size (k)Value of p in k = bp|Rs|cw k = 1 0.001 0.005 0.01 0.10 0.20|Rs|=2001 0.322 0.322 0.322 0.348 0.516 0.4802 0.304 0.304 0.304 0.414 0.598 0.5703 0.378 0.378 0.378 0.466 0.504 0.4684 0.024 0.024 0.024 0.242 0.510 0.4625 0.040 0.040 0.040 0.012 0.516 0.472|Rs|=6001 0.678 0.678 0.426 0.514 0.556 0.5262 0.632 0.632 0.556 0.568 0.658 0.6083 0.652 0.652 0.534 0.488 0.604 0.5504 0.040 0.040 0.006 0.476 0.522 0.5365 0.088 0.088 0.014 0.234 0.526 0.530|Rs|=34901 0.956 0.362 0.670 0.666 0.546 0.5262 0.956 0.624 0.722 0.734 0.608 0.5863 0.956 0.628 0.716 0.692 0.574 0.5344 0.992 0.118 0.698 0.682 0.550 0.5285 0.992 0.124 0.646 0.654 0.548 0.534Table 5: The observed results, i.e.
the proportion of technology terms in the list of top 500 candidateterms, for various sizes of the reference vectors set (|Rs|) and the neighbourhood size (k) for the contextwindow that extends around the terms; w denotes the size of context window.size of the reference vectors (|Rs|), i.e.
k = bp|Rs|c, for p ?
{0.001, 0.005, 0.01, 0.1, 0.2}.
In addition,we report the results for to the nearest-neighbour algorithm, i.e.
when k = 1.
For |Rs| < 1000 andp = 0.001 that results to k = b0.001|Rs|c = 0, we set k = 1.
In these cases, thus, the reported resultsfor k = bp|Rs|c is equivalent to the results reported for k = 1.Table 5 reports the observed results: the proportion of technology terms in the top 500 terms in thelist of candidate terms.
First, these results suggest that the optimum value of p in k = bp|Rs|c, andthus k, depends on |Rs|.
If |Rs| is small, a larger neighborhood performs better than a smaller neighbor-hood.
Inversely, if |Rs| is large enough, a small neighbourhood shows higher performance than a largeneighborhood.
Second, a small neighbourhood is sensitive to the size of context window w (and perhapsthe presence of noise), specifically when |Rs| is small.
As an example, for |Rs| = 600 and k = 1,if w ?
4, then the performance drops sharply.
Therefore, the performance of large neighbourhoodscan be more stable than the performance of small neighbourhoods.
Lastly, when |Rs| reaches 1600 (acertain threshold), there is no significant increase in the performance of the algorithm.
In this case, thenearest-neighbour algorithm outperforms the k-nn method.
We suggest that the obtained results from thenearest-neighbour method can be used as a heuristic-based strategy for the selection of the number ofvectors in the |R|.
Accordingly, one can stop adding new vectors to Rswhen the obtained results fromthe nearest-neighbour method are above a certain threshold.4 ConclusionIn this paper, we proposed a corpus-based, distributional method for the recognition of technology termsfrom a corpus of scientific publications.
The method is established as a k-nn regression task in a Eu-clidean vector space, in which vectors are compared by their cosine similarity.
This vector space repre-sents the co-occurrence frequencies of candidate terms and words in a context window.
We examined anumber of factors that play roles in the performance of the proposed method.In order to find the most discriminative models, we studied several configurations of the context win-dow: its size, the direction in which it is extended, and the incorporation of the word order information.According to these experiments, context windows that collect co-occurrence frequencies in both sidesof terms, in narrow context, i.e.
the size of 2 or 3 words, outperform other context types.
We observedthat narrow context windows, irrespective of other variables in the model (i.e.
the k and the number ofreference vectors), consistently show a performance higher than other context configurations.
Therefore,we suggest that the best-performing context type can be decided independently of the value of k and9the reference vector size.
We also reported an initial experiment that assessed the effect of the referencevector size on the performance of the system.We performed our evaluations on the ACL ARC corpus.
Apart from the proposed methodology andreported experiments, another outcome of the performed experiment is a relatively large set of annotatedtechnology terms, i.e.
the ACL RD-TEC.
The annotated terms in the ACL RD-TEC can be easily mappedinto the ACL ARC documents, thus, into a chronological order and in a citation network.
As a result,the annotations resulting from the experiments reported in this paper can be used in tasks other thantechnology term recognition, e.g.
citation analysis and technology forecasting.The reported experiment can be extended in several ways.
In this paper, we focused on the extractionof technology terms at the corpus level.
It would be helpful to investigate the best-performing contextconfigurations when the co-occurrences are collected from communicative contexts of other sizes thancorpus, e.g.
at a document level, similar to automatic keyphrase extraction tasks.
It is also interestingto compare the performance of the k-nn instance-based algorithm with other learning techniques suchas support vector machines.
Last but not least, we are interested in re-evaluating the proposed methodusing metrics other than the top n terms look-up in the sorted weighted list of terms.
A comparison ofthe evaluation metrics could be an attractive research avenue.AcknowledgementsWe thank the anonymous reviewers.
This publication has emanated from research conducted with thefinancial support of Science Foundation Ireland under Grant Number SFI/12/RC/2289.ReferencesSteven Bird, Robert Dale, Bonnie Dorr, Bryan Gibson, Mark Joseph, Min-Yen Kan, Dongwon Lee, Brett Powley, DragomirRadev, and Yee Fan Tan.
2008.
The ACL anthology reference corpus: a reference dataset for bibliographic research incomputational linguistics.
In LREC?08.
Marrakech, Morocco.Stefan Evert.
2004.
The Statistics of Word Cooccurrences Word Pairs and Collocations.
Ph.D. thesis, Institut f?ur maschinelleSprachverarbeitung, Universit?at Stuttgart.KaterinaT.
Frantzi, Sophia Ananiadou, and Junichi Tsujii.
1998.
The c-value/nc-value method of automatic recognition formulti-word terms.
In Research and Advanced Technology for Digital Libraries, volume 1513 of LNCS, pages 585?604.Zellig S. Harris.
1954.
Distributional structure.
Word, The Journal of the International Linguistic Association, 10:146?162.Kyo Kageura and Bin Umino.
1996.
Methods of automatic term recognition: A review.
Terminology, 3.2 (1996):259?289.Pentti Kanerva, Jan Kristoferson, and Anders Holst.
2000.
Random indexing of text samples for latent semantic analysis.
InProceedings of the 22nd Annual Conference of the Cognitive Science Society, pages 103?6.
Erlbaum.Alessandro Lenci.
2008.
Distributional semantics in linguistic and cognitive research.
From context to meaning: Distributionalmodels of the lexicon in linguistics and cognitive science, special issue of the Italian Journal of Linguistics, 20/1:1?31.Ping Li, Trevor J. Hastie, and Kenneth W. Church.
2006.
Very sparse random projections.
In Proceedings of the 12th ACMSIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ?06, pages 287?296.
ACM, NY, USA.Minh-Thang Luong, Thuy Dung Nguyen, and Min-Yen Kan. 2010.
Logical structure recovery in scholarly articles with richdocument features.
IJDLS, 1(4):1?23.Nils C. Newman, Alan L. Porter, David Newman, Cherie Courseault Trumbach, and Stephanie D. Bolan.
2014.
Comparingmethods to extract technical content for technological intelligence.
JET-M, 32(0):97 ?
109.Behrang QasemiZadeh.
2010.
Towards technology structure mining from text by linguistics analysis.
DERI technical report,National University of Ireland, Galway.Magnus Sahlgren.
2008.
The distributional hypothesis.
Italian Journal of Linguistics, 20:33?54.Magnus Sahlgren, Anders Holst, and Pentti Kanerva.
2008.
Permutations as a means to encode order in word space.
InV.
Sloutsky, B.
Love, and K. Mcrae, editors, Proceedings of the 30th Annual Conference of the Cognitive Science Society,pages 1300?1305.
Cognitive Science Society, Austin, TX.Peter D. Turney and Patrick Pantel.
2010.
From frequency to meaning: vector space models of semantics.
J. Artif.
Int.
Res.,37(1):141?188.Yiming Yang.
1999.
An evaluation of statistical approaches to text categorization.
Inf.
Retr., 1(1-2):69?90.Behrang Q. Zadeh and Siegfried Handschuh.
2014a.
The ACL RD-TEC: a dataset for benchmarking terminology extractionand classification in computational linguistics.
In Coling 2014 CompuTerm 2014.
Dublin, Ireland.Behrang Q. Zadeh and Siegfried Handschuh.
2014b.
Evaluation of technology term recognition with random indexing.
InLREC?14.
European Language Resources Association (ELRA), Reykjavik, Iceland.Behrang Q. Zadeh and Siegfried Handschuh.
2014c.
Random manhattan indexing.
Database and Expert Systems Applications(DEXA), 25th International Workshop on.
IEEE, Munich, Germany.10
