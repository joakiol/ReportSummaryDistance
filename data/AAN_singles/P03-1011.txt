Loosely Tree-Based Alignment for Machine TranslationDaniel GildeaUniversity of Pennsylvaniadgildea@cis.upenn.eduAbstractWe augment a model of translation basedon re-ordering nodes in syntactic trees inorder to allow alignments not conformingto the original tree structure, while keep-ing computational complexity polynomialin the sentence length.
This is done byadding a new subtree cloning operation toeither tree-to-string or tree-to-tree align-ment algorithms.1 IntroductionSystems for automatic translation between lan-guages have been divided into transfer-based ap-proaches, which rely on interpreting the sourcestring into an abstract semantic representationfrom which text is generated in the target lan-guage, and statistical approaches, pioneered byBrown et al (1990), which estimate parameters fora model of word-to-word correspondences and wordre-orderings directly from large corpora of par-allel bilingual text.
Only recently have hybridapproaches begun to emerge, which apply prob-abilistic models to a structured representation ofthe source text.
Wu (1997) showed that restrict-ing word-level alignments between sentence pairsto observe syntactic bracketing constraints signif-icantly reduces the complexity of the alignmentproblem and allows a polynomial-time solution.Alshawi et al (2000) also induce parallel tree struc-tures from unbracketed parallel text, modeling thegeneration of each node?s children with a finite-statetransducer.
Yamada and Knight (2001) present analgorithm for estimating probabilistic parameters fora similar model which represents translation as a se-quence of re-ordering operations over children ofnodes in a syntactic tree, using automatic parser out-put for the initial tree structures.
The use of explicitsyntactic information for the target language in thismodel has led to excellent translation results (Ya-mada and Knight, 2002), and raises the prospect oftraining a statistical system using syntactic informa-tion for both sides of the parallel corpus.Tree-to-tree alignment techniques such as prob-abilistic tree substitution grammars (Hajic?
et al,2002) can be trained on parse trees from paralleltreebanks.
However, real bitexts generally do notexhibit parse-tree isomorphism, whether because ofsystematic differences between how languages ex-press a concept syntactically (Dorr, 1994), or simplybecause of relatively free translations in the trainingmaterial.In this paper, we introduce ?loosely?
tree-basedalignment techniques to address this problem.
Wepresent analogous extensions for both tree-to-stringand tree-to-tree models that allow alignments notobeying the constraints of the original syntactic tree(or tree pair), although such alignments are dispre-ferred because they incur a cost in probability.
Thisis achieved by introducing a clone operation, whichcopies an entire subtree of the source language syn-tactic structure, moving it anywhere in the targetlanguage sentence.
Careful parameterization of theprobability model allows it to be estimated at no ad-ditional cost in computational complexity.
We ex-pect our relatively unconstrained clone operation toallow for various types of structural divergence byproviding a sort of hybrid between tree-based andunstructured, IBM-style models.We first present the tree-to-string model, followedby the tree-to-tree model, before moving on to align-ment results for a parallel syntactically annotatedKorean-English corpus, measured in terms of align-ment perplexities on held-out test data, and agree-ment with human-annotated word-level alignments.2 The Tree-to-String ModelWe begin by summarizing the model ofYamada and Knight (2001), which can be thoughtof as representing translation as an AlexanderCalder mobile.
If we follow the process of anEnglish sentence?s transformation into French,the English sentence is first given a syntactic treerepresentation by a statistical parser (Collins, 1999).As the first step in the translation process, thechildren of each node in the tree can be re-ordered.For any node with m children, m!
re-orderings arepossible, each of which is assigned a probabilityPorder conditioned on the syntactic categories ofthe parent node and its children.
As the secondstep, French words can be inserted at each nodeof the parse tree.
Insertions are modeled in twosteps, the first predicting whether an insertion tothe left, an insertion to the right, or no insertiontakes place with probability Pins , conditioned onthe syntactic category of the node and that of itsparent.
The second step is the choice of the insertedword Pt(f jNULL), which is predicted withoutany conditioning information.
The final step, aFrench translation of each original English word,at the leaves of the tree, is chosen according to adistribution Pt(f je).
The French word is predictedconditioned only on the English word, and eachEnglish word can generate at most one Frenchword, or can generate a NULL symbol, representingdeletion.
Given the original tree, the re-ordering,insertion, and translation probabilities at each nodeare independent of the choices at any other node.These independence relations are analogous to thoseof a stochastic context-free grammar, and allow forefficient parameter estimation by an inside-outsideExpectation Maximization (EM) algorithm.
Thecomputation of inside probabilities ?, outlinedbelow, considers possible reordering of nodes in theoriginal tree in a bottom-up manner:for all nodes ?iin input tree T dofor all k, l such that 1 < k < l < N dofor all orderings ?
of the children ?1...?mof ?idofor all partitions of span k, l into k1, l1...km, lmdo?
(?i, k, l)+= Porder (?|?i)?mj=1?
(?j, kj, lj)end forend forend forend forThis algorithm has computational complexityO(jT jNm+2), where m is the maximum number ofchildren of any node in the input tree T , and Nthe length of the input string.
By storing partiallycompleted arcs in the chart and interleaving the in-ner two loops, complexity of O(jT jn3m!2m) can beachieved.
Thus, while the algorithm is exponentialin m, the fan-out of the grammar, it is polynomial inthe size of the input string.
Assuming jT j = O(n),the algorithm is O(n4).The model?s efficiency, however, comes at a cost.Not only are many independence assumptions made,but many alignments between source and target sen-tences simply cannot be represented.
As a minimalexample, take the tree:ABX YZOf the six possible re-orderings of the three ter-minals, the two which would involve crossing thebracketing of the original tree (XZY and YZX)are not allowed.
While this constraint gives us away of using syntactic information in translation,it may in many cases be too rigid.
In part to dealwith this problem, Yamada and Knight (2001) flat-ten the trees in a pre-processing step by collapsingnodes with the same lexical head-word.
This allows,for example, an English subject-verb-object (SVO)structure, which is analyzed as having a VP nodespanning the verb and object, to be re-ordered asVSO in a language such as Arabic.
Larger syntacticdivergences between the two trees may require fur-ther relaxation of this constraint, and in practice weexpect such divergences to be frequent.
For exam-ple, a nominal modifier in one language may showup as an adverbial in the other, or, due to choicessuch as which information is represented by a mainverb, the syntactic correspondence between the twoSVPNPNNCNNCKyeo-ulPADePAUNeunVPNPNNCNNCSu-KapPCAeulVPNPNNUMyeochNNXNNXKhyeol-ReXSFSsikVPNPNNCCi-KeupLVVVVVPatEFNCiSVPVPVPVPNPNNCCi-KeupNULLLVVVVVPatNULLEFNCiNULLNPNNUMyeochhowNNXXSFSsikmanyNNXKhyeol-RepairsNPNNCNNCSu-KapglovesPCAeulNULLNPVPLVVVVVPateachEFNCiyouNPNNCCi-KeupissuedNNCPADeinPAUNeunNULLNNCKyeo-ulwinterFigure 1: Original Korean parse tree, above, and transformed tree after reordering of children, subtreecloning (indicated by the arrow), and word translation.
After the insertion operation (not shown), the tree?sEnglish yield is: How many pairs of gloves is each of you issued in winter?sentences may break down completely.2.1 Tree-to-String Clone OperationIn order to provide some flexibility, we modify themodel in order to allow for a copy of a (translated)subtree from the English sentences to occur, withsome cost, at any point in the resulting French sen-tence.
For example, in the case of the input treeABX YZa clone operation making a copy of node 3 as a newchild of B would produce the tree:ABX Z YZThis operation, combined with the deletion of theoriginal node Z, produces the alignment (XZY)that was disallowed by the original tree reorder-ing model.
Figure 1 shows an example from ourKorean-English corpus where the clone operation al-lows the model to handle a case of wh-movement inthe English sentence that could not be realized byany reordering of subtrees of the Korean parse.The probability of adding a clone of original node?ias a child of node ?jis calculated in two steps:first, the choice of whether to insert a clone under?j, with probability Pins(clonej?j), and the choiceof which original node to copy, with probabilityPclone(?ijclone = 1) = Pmakeclone(?i)?kPmakeclone(?k)where Pmakeclone is the probability of an originalnode producing a copy.
In our implementation, forsimplicity, Pins(clone) is a single number, estimatedby the EM algorithm but not conditioned on the par-ent node ?j, and Pmakeclone is a constant, meaningthat the node to be copied is chosen from all thenodes in the original tree with uniform probability.It is important to note that Pmakeclone is not de-pendent on whether a clone of the node in ques-tion has already been made, and thus a node maybe ?reused?
any number of times.
This indepen-dence assumption is crucial to the computationaltractability of the algorithm, as the model can beestimated using the dynamic programming methodabove, keeping counts for the expected number oftimes each node has been cloned, at no increase incomputational complexity.
Without such an assump-tion, the parameter estimation becomes a problemof parsing with crossing dependencies, which is ex-ponential in the length of the input string (Barton,1985).3 The Tree-to-Tree ModelThe tree-to-tree alignment model has tree transfor-mation operations similar to those of the tree-to-string model described above.
However, the trans-formed tree must not only match the surface stringof the target language, but also the tree structure as-signed to the string by the treebank annotators.
In or-der to provide enough flexibility to make this possi-ble, additional tree transformation operations allowa single node in the source tree to produce two nodesin the target tree, or two nodes in the source tree tobe grouped together and produce a single node inthe target tree.
The model can be thought of as asynchronous tree substitution grammar, with proba-bilities parameterized to generate the target tree con-ditioned on the structure of the source tree.The probability P (TbjTa) of transforming thesource tree Tainto target tree Tbis modeled in asequence of steps proceeding from the root of thetarget tree down.
At each level of the tree:1.
At most one of the current node?s children isgrouped with the current node in a single ele-mentary tree, with probability Pelem(taj?a)children(?a)), conditioned on the currentnode ?aand its children (ie the CFG produc-tion expanding ?a).2.
An alignment of the children of the currentelementary tree is chosen, with probabilityPalign(?j?a ) children(ta)).
This alignmentoperation is similar to the re-order operationin the tree-to-string model, with the extensionthat 1) the alignment ?
can include insertionsand deletions of individual children, as nodesin either the source or target may not corre-spond to anything on the other side, and 2) inthe case where two nodes have been groupedinto ta, their children are re-ordered together inone step.In the final step of the process, as in the tree-to-string model, lexical items at the leaves of the treeare translated into the target language according to adistribution Pt(f je).Allowing non-1-to-1 correspondences betweennodes in the two trees is necessary to handle thefact that the depth of corresponding words in thetwo trees often differs.
A further consequence ofallowing elementary trees of size one or two is thatsome reorderings not allowed when reordering thechildren of each individual node separately are nowpossible.
For example, with our simple treeABX YZif nodes A and B are considered as one elementarytree, with probability Pelem(tajA ) BZ), their col-lective children will be reordered with probabilityPalign(f(1, 1)(2, 3)(3, 2)gjA ) XYZ)AX Z Ygiving the desired word ordering XZY.
However,computational complexity as well as data sparsityprevent us from considering arbitrarily large ele-mentary trees, and the number of nodes consideredat once still limits the possible alignments.
For ex-ample, with our maximum of two nodes, no trans-formation of the treeABW XCY Zis capable of generating the alignment WYXZ.In order to generate the complete target tree, onemore step is necessary to choose the structure on thetarget side, specifically whether the elementary treehas one or two nodes, what labels the nodes have,and, if there are two nodes, whether each child at-taches to the first or the second.
Because we areultimately interested in predicting the correct targetstring, regardless of its structure, we do not assignprobabilities to these steps.
The nonterminals on thetarget side are ignored entirely, and while the align-ment algorithm considers possible pairs of nodes aselementary trees on the target side during training,the generative probability model should be thoughtof as only generating single nodes on the target side.Thus, the alignment algorithm is constrained by thebracketing on the target side, but does not generatethe entire target tree structure.While the probability model for tree transforma-tion operates from the top of the tree down, prob-ability estimation for aligning two trees takes placeby iterating through pairs of nodes from each tree inbottom-up order, as sketched below:for all nodes ?ain source tree Tain bottom-up order dofor all elementary trees tarooted in ?adofor all nodes ?bin target tree Tbin bottom-up order dofor all elementary trees tbrooted in ?bdofor all alignments ?
of the children of taand tbdo?
(?a, ?b) +=Pelem(ta|?a)Palign(?|?i)?(i,j)???
(?i, ?j)end forend forend forend forend forThe outer two loops, iterating over nodes in eachtree, require O(jT j2).
Because we restrict our el-ementary trees to include at most one child of theroot node on either side, choosing elementary treesfor a node pair is O(m2), where m refers to the max-imum number of children of a node.
Computing thealignment between the 2m children of the elemen-tary tree on either side requires choosing which sub-set of source nodes to delete, O(22m), which subsetof target nodes to insert (or clone), O(22m), and howto reorder the remaining nodes from source to targettree, O((2m)!).
Thus overall complexity of the algo-rithm is O(jT j2m242m(2m)!
), quadratic in the sizeof the input sentences, but exponential in the fan-outof the grammar.3.1 Tree-to-Tree Clone OperationAllowing m-to-n matching of up to two nodeson either side of the parallel treebank allows forlimited non-isomorphism between the trees, as inHajic?
et al (2002).
However, even given this flexi-bility, requiring alignments to match two input treesrather than one often makes tree-to-tree alignmentmore constrained than tree-to-string alignment.
Forexample, even alignments with no change in wordorder may not be possible if the structures of thetwo trees are radically mismatched.
This leads usto think it may be helpful to allow departures fromTree-to-String Tree-to-Treeelementary tree grouping Pelem(taj?a) children(?a))re-order Porder (?j? )
children(?))
Palign(?j?a ) children(ta))insertion Pins(left, right, nonej?)
?
can include ?insertion?
symbollexical translation Pt(f je) Pt(f je)with cloning Pins(clonej?)
?
can include ?clone?
symbolPmakeclone(?)
Pmakeclone(?
)Table 1: Model parameterizationthe constraints of the parallel bracketing, if it canbe done in without dramatically increasing compu-tational complexity.For this reason, we introduce a clone operation,which allows a copy of a node from the source tree tobe made anywhere in the target tree.
After the cloneoperation takes place, the transformation of sourceinto target tree takes place using the tree decomposi-tion and subtree alignment operations as before.
Thebasic algorithm of the previous section remains un-changed, with the exception that the alignments ?between children of two elementary trees can nowinclude cloned, as well as inserted, nodes on the tar-get side.
Given that ?
specifies a new cloned nodeas a child of ?j, the choice of which node to clone ismade as in the tree-to-string model:Pclone(?ijclone 2 ?)
= Pmakeclone(?i)?kPmakeclone(?k)Because a node from the source tree is cloned withequal probability regardless of whether it has al-ready been ?used?
or not, the probability of a cloneoperation can be computed under the same dynamicprogramming assumptions as the basic tree-to-treemodel.
As with the tree-to-string cloning operation,this independence assumption is essential to keepthe complexity polynomial in the size of the inputsentences.For reference, the parameterization of all fourmodels is summarized in Table 1.4 DataFor our experiments, we used a parallel Korean-English corpus from the military domain (Han et al,2001).
Syntactic trees have been annotated by handfor both the Korean and English sentences; in thispaper we will be using only the Korean trees, mod-eling their transformation into the English text.
Thecorpus contains 5083 sentences, of which we used4982 as training data, holding out 101 sentences forevaluation.
The average Korean sentence length was13 words.
Korean is an agglutinative language, andwords often contain sequences of meaning-bearingsuffixes.
For the purposes of our model, we rep-resented the syntax trees using a fairly aggressivetokenization, breaking multimorphemic words intoseparate leaves of the tree.
This gave an averageof 21 tokens for the Korean sentences.
The aver-age English sentence length was 16.
The maximumnumber of children of a node in the Korean treeswas 23 (this corresponds to a comma-separated listof items).
77% of the Korean trees had no morethan four children at any node, 92% had no morethan five children, and 96% no more than six chil-dren.
The vocabulary size (number of unique types)was 4700 words in English, and 3279 in Korean ?before splitting multi-morphemic words, the Koreanvocabulary size was 10059.
For reasons of compu-tation speed, trees with more than 5 children wereexcluded from the experiments described below.5 ExperimentsWe evaluate our translation models both in termsagreement with human-annotated word-level align-ments between the sentence pairs.
For scoringthe viterbi alignments of each system against gold-standard annotated alignments, we use the alignmenterror rate (AER) of Och and Ney (2000), whichmeasures agreement at the level of pairs of words:1AER = 1 ?
2jA \ GjjAj + jGj1While Och and Ney (2000) differentiate between sure andpossible hand-annotated alignments, our gold standard align-ments come in only one variety.AlignmentError RateIBM Model 1 .37IBM Model 2 .35IBM Model 3 .43Tree-to-String .42Tree-to-String, Clone .36Tree-to-String, Clone Pins = .5 .32Tree-to-Tree .49Tree-to-Tree, Clone .36Table 2: Alignment error rate on Korean-English corpuswhere A is the set of word pairs aligned by the au-tomatic system, and G the set algned in the goldstandard.
We provide a comparison of the tree-basedmodels with the sequence of successively more com-plex models of Brown et al (1993).
Results areshown in Table 2.The error rates shown in Table 2 represent theminimum over training iterations; training wasstopped for each model when error began to in-crease.
IBM Models 1, 2, and 3 refer toBrown et al (1993).
?Tree-to-String?
is the modelof Yamada and Knight (2001), and ?Tree-to-String,Clone?
allows the node cloning operation of Section2.1.
?Tree-to-Tree?
indicates the model of Section 3,while ?Tree-to-Tree, Clone?
adds the node cloningoperation of Section 3.1.
Model 2 is initialized fromthe parameters of Model 1, and Model 3 is initializedfrom Model 2.
The lexical translation probabilitiesPt(f je) for each of our tree-based models are initial-ized from Model 1, and the node re-ordering proba-bilities are initialized uniformly.
Figure 1 shows theviterbi alignment produced by the ?Tree-to-String,Clone?
system on one sentence from our test set.We found better agreement with the human align-ments when fixing Pins(left) in the Tree-to-Stringmodel to a constant rather than letting it be deter-mined through the EM training.
While the modellearned by EM tends to overestimate the total num-ber of aligned word pairs, fixing a higher probabilityfor insertions results in fewer total aligned pairs andtherefore a better trade-off between precision andrecall.
As seen for other tasks (Carroll and Char-niak, 1992; Merialdo, 1994), the likelihood crite-rion used in EM training may not be optimal whenevaluating a system against human labeling.
Theapproach of optimizing a small number of metapa-rameters has been applied to machine translation byOch and Ney (2002).
It is likely that the IBM mod-els could similarly be optimized to minimize align-ment error ?
an open question is whether the opti-mization with respect to alignment error will corre-spond to optimization for translation accuracy.Within the strict EM framework, we foundroughly equivalent performance between the IBMmodels and the two tree-based models when makinguse of the cloning operation.
For both the tree-to-string and tree-to-tree models, the cloning operationimproved results, indicating that adding the flexibil-ity to handle structural divergence is important whenusing syntax-based models.
The improvement wasparticularly significant for the tree-to-tree model, be-cause using syntactic trees on both sides of the trans-lation pair, while desirable as an additional source ofinformation, severely constrains possible alignmentsunless the cloning operation is allowed.The tree-to-tree model has better theoretical com-plexity than the tree-to-string model, being quadraticrather than quartic in sentence length, and we foundthis to be a significant advantage in practice.
Thisimprovement in speed allows longer sentences andmore data to be used in training syntax-based mod-els.
We found that when training on sentences of up60 words, the tree-to-tree alignment was 20 timesfaster than tree-to-string alignment.
For reasons ofspeed, Yamada and Knight (2002) limited trainingto sentences of length 30, and were able to use onlyone fifth of the available Chinese-English parallelcorpus.6 ConclusionOur loosely tree-based alignment techniques allowstatistical models of machine translation to make useof syntactic information while retaining the flexibil-ity to handle cases of non-isomorphic source and tar-get trees.
This is achieved with a clone operation pa-rameterized in such a way that alignment probabili-ties can be computed with no increase in asymptoticcomputational complexity.We present versions of this technique both fortree-to-string models, making use of parse trees forone of the two languages, and tree-to-tree models,which make use of parallel parse trees.
Results interms of alignment error rate indicate that the cloneoperation results in better alignments in both cases.On our Korean-English corpus, we found roughlyequivalent performance for the unstructured IBMmodels, and the both the tree-to-string and tree-to-tree models when using cloning.
To our knowl-edge these are the first results in the literature fortree-to-tree statistical alignment.
While we did notsee a benefit in alignment error from using syntactictrees in both languages, there is a significant practi-cal benefit in computational efficiency.
We remainhopeful that two trees can provide more informationthan one, and feel that extensions to the ?loosely?tree-based approach are likely to demonstrate thisusing larger corpora.Another important question we plan to pursue isthe degree to which these results will be borne outwith larger corpora, and how the models may be re-fined as more training data is available.
As one ex-ample, our tree representation is unlexicalized, butwe expect conditioning the model on more lexicalinformation to improve results, whether this is doneby percolating lexical heads through the existingtrees or by switching to a strict dependency repre-sentation.ReferencesHiyan Alshawi, Srinivas Bangalore, and Shona Douglas.2000.
Learning dependency translation models as col-lections of finite state head transducers.
Computa-tional Linguistics, 26(1):45?60.G.
Edward Barton, Jr. 1985.
On the complexity of ID/LPparsing.
Computational Linguistics, 11(4):205?218.Peter F. Brown, John Cocke, Stephen A. Della Pietra,Vincent J. Della Pietra, Frederick Jelinek, John D. Laf-ferty, Robert L. Mercer, and Paul S. Roossin.
1990.
Astatistical approach to machine translation.
Computa-tional Linguistics, 16(2):79?85, June.Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The mathematicsof statistical machine translation: Parameter estima-tion.
Computational Linguistics, 19(2):263?311.Glenn Carroll and Eugene Charniak.
1992.
Two experi-ments on learning probabilistic dependency grammarsfrom corpora.
In Workshop Notes for Statistically-Based NLP Techniques, pages 1?13.
AAAI.Michael John Collins.
1999.
Head-driven StatisticalModels for Natural Language Parsing.
Ph.D. thesis,University of Pennsylvania, Philadelphia.Bonnie J. Dorr.
1994.
Machine translation divergences:A formal description and proposed solution.
Compu-tational Linguistics, 20(4):597?633.Jan Hajic?, Martin ?Cmejrek, Bonnie Dorr, Yuan Ding, Ja-son Eisner, Daniel Gildea, Terry Koo, Kristen Parton,Gerald Penn, Dragomir Radev, and Owen Rambow.2002.
Natural language generation in the context ofmachine translation.
Technical report, Center for Lan-guage and Speech Processing, Johns Hopkins Univer-sity, Baltimore.
Summer Workshop Final Report.Chung-hye Han, Na-Rae Han, and Eon-Suk Ko.
2001.Bracketing guidelines for Penn Korean treebank.Technical Report IRCS-01-010, IRCS, University ofPennsylvania.Bernard Merialdo.
1994.
Tagging English text witha probabilistic model.
Computational Linguistics,20(2):155?172.Franz Josef Och and Hermann Ney.
2000.
Improvedstatistical alignment models.
In Proceedings of ACL-00, pages 440?447, Hong Kong, October.Franz Josef Och and Hermann Ney.
2002.
Discrimina-tive training and maximum entropy models for statis-tical machine translation.
In Proceedings of ACL-02,Philadelphia, PA.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):3?403.Kenji Yamada and Kevin Knight.
2001.
A syntax-basedstatistical translation model.
In Proceedings of ACL-01, Toulouse, France.Kenji Yamada and Kevin Knight.
2002.
A decoder forsyntax-based statistical MT.
In Proceedings of ACL-02, Philadelphia, PA.
