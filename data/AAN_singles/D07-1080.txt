Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
764?773, Prague, June 2007. c?2007 Association for Computational LinguisticsOnline Large-Margin Training for Statistical Machine TranslationTaro Watanabe Jun Suzuki Hajime Tsukada Hideki IsozakiNTT Communication Science Laboratories2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0237 Japan{taro,jun,tsukada,isozaki}@cslab.kecl.ntt.co.jpAbstractWe achieved a state of the art performancein statistical machine translation by usinga large number of features with an onlinelarge-margin training algorithm.
The mil-lions of parameters were tuned only on asmall development set consisting of less than1K sentences.
Experiments on Arabic-to-English translation indicated that a modeltrained with sparse binary features outper-formed a conventional SMT system with asmall number of features.1 IntroductionThe recent advances in statistical machine transla-tion have been achieved by discriminatively train-ing a small number of real-valued features based ei-ther on (hierarchical) phrase-based translation (Ochand Ney, 2004; Koehn et al, 2003; Chiang, 2005) orsyntax-based translation (Galley et al, 2006).
How-ever, it does not scale well with a large number offeatures of the order of millions.Tillmann and Zhang (2006), Liang et al (2006)and Bangalore et al (2006) introduced sparse binaryfeatures for statistical machine translation trained ona large training corpus.
In this framework, the prob-lem of translation is regarded as a sequential labelingproblem, in the same way as part-of-speech tagging,chunking or shallow parsing.
However, the use of alarge number of features did not provide any signifi-cant improvements over a conventional small featureset.Bangalore et al (2006) trained the lexical choicemodel by using Conditional Random Fields (CRF)realized on a WFST.
Their modeling was reduced toMaximum Entropy Markov Model (MEMM) to han-dle a large number of features which, in turn, facedthe labeling bias problem (Lafferty et al, 2001).Tillmann and Zhang (2006) trained their feature setusing an online discriminative algorithm.
Since thedecoding is still expensive, their online training ap-proach is approximated by enlarging a merged k-best list one-by-one with a 1-best output.
Lianget al (2006) introduced an averaged perceptron al-gorithm, but employed only 1-best translation.
InWatanabe et al (2006a), binary features were trainedonly on a small development set using a variant ofvoted perceptron for reranking k-best translations.Thus, the improvement is merely relative to thebaseline translation system, namely whether or notthere is a good translation in their k-best.We present a method to estimate a large num-ber of parameters ?
of the order of millions ?using an online training algorithm.
Although itwas intuitively considered to be prone to overfit-ting, training on a small development set ?
lessthan 1K sentences ?
was sufficient to achieve im-proved performance.
In this method, each train-ing sentence is decoded and weights are updated atevery iteration (Liang et al, 2006).
When updat-ing model parameters, we employ a memorization-variant of a local updating strategy (Liang et al,2006) in which parameters are optimized towarda set of good translations found in the k-best listacross iterations.
The objective function is an ap-proximated BLEU (Watanabe et al, 2006a) thatscales the loss of a sentence BLEU to a document-wise loss.
The parameters are trained using the764Margin Infused Relaxed Algorithm (MIRA) (Cram-mer et al, 2006).
MIRA is successfully employedin dependency parsing (McDonald et al, 2005) orthe joint-labeling/chunking task (Shimizu and Haas,2006).
Experiments were carried out on an Arabic-to-English translation task, and we achieved signif-icant improvements over conventional minimum er-ror training with a small number of features.This paper is organized as follows: First, Sec-tion 2 introduces the framework of statistical ma-chine translation.
As a baseline SMT system, weuse the hierarchical phrase-based translation withan efficient left-to-right generation (Watanabe et al,2006b) originally proposed by Chiang (2005).
InSection 3, a set of binary sparse features are definedincluding numeric features for our baseline system.Section 4 introduces an online large-margin trainingalgorithm using MIRA with our key components.The experiments are presented in Section 5 followedby discussion in Section 6.2 Statistical Machine TranslationWe use a log-linear approach (Och, 2003) in whicha foreign language sentence f is translated into an-other language, for example English, e, by seeking amaximum solution:e?
= argmaxewT ?
h( f , e) (1)where h( f , e) is a large-dimension feature vector.
wis a weight vector that scales the contribution fromeach feature.
Each feature can take any real value,such as the log of the n-gram language model torepresent fluency, or a lexicon model to capture theword or phrase-wise correspondence.2.1 Hierarchical Phrase-based SMTChiang (2005) introduced the hierarchical phrase-based translation approach, in which non-terminalsare embedded in each phrase.
A translation is gener-ated by hierarchically combining phrases using thenon-terminals.
Such a quasi-syntactic structure cannaturally capture the reordering of phrases that is notdirectly modeled by a conventional phrase-based ap-proach (Koehn et al, 2003).
The non-terminal em-bedded phrases are learned from a bilingual corpuswithout a linguistically motivated syntactic struc-ture.Based on hierarchical phrase-based modeling, weadopted the left-to-right target generation method(Watanabe et al, 2006b).
This method is able togenerate translations efficiently, first, by simplifyingthe grammar so that the target side takes a phrase-prefixed form, namely a target normalized form.Second, a translation is generated in a left-to-rightmanner, similar to the phrase-based approach usingEarley-style top-down parsing on the source side.Coupled with the target normalized form, n-gramlanguage models are efficiently integrated during thesearch even with a higher order of n.2.2 Target Normalized FormIn Chiang (2005), each production rule is restrictedto a rank-2 or binarized form in which each rule con-tains at most two non-terminals.
The target normal-ized form (Watanabe et al, 2006b) further imposesa constraint whereby the target side of the alignedright-hand side is restricted to a Greibach NormalForm like structure:X ??
?, ?b?,??
(2)where X is a non-terminal, ?
is a source side string ofarbitrary terminals and/or non-terminals.
?b?
is a cor-responding target side where ?b is a string of termi-nals, or a phrase, and ?
is a (possibly empty) stringof non-terminals.
?
defines one-to-one mapping be-tween non-terminals in ?
and ?.
The use of phrase?b as a prefix maintains the strength of the phrase-base framework.
A contiguous English side with a(possibly) discontiguous foreign language side pre-serves phrase-bounded local word reordering.
Atthe same time, the target normalized framework stillcombines phrases hierarchically in a restricted man-ner.2.3 Left-to-Right Target GenerationDecoding is performed by parsing on the source sideand by combining the projected target side.
Weapplied an Earley-style top-down parsing approach(Wu and Wong, 1998; Watanabe et al, 2006b; Zoll-mann and Venugopal, 2006).
The basic idea isto perform top-down parsing so that the projectedtarget side is generated in a left-to-right manner.The search is guided with a push-down automaton,which keeps track of the span of uncovered source765word positions.
Combined with the rest-cost esti-mation aggregated in a bottom-up way, our decoderefficiently searches for the most likely translation.The use of a target normalized form further sim-plifies the decoding procedure.
Since the rule formdoes not allow any holes for the target side, the inte-gration with an n-gram language model is straight-forward: the prefixed phrases are simply concate-nated and intersected with n-gram.3 Features3.1 Baseline FeaturesThe hierarchical phrase-based translation systememploys standard numeric value features:?
n-gram language model to capture the fluencyof the target side.?
Hierarchical phrase translation probabilities inboth directions, h(?|?b?)
and h(?b?|?
), estimatedby relative counts, count(?, ?b?).?
Word-based lexically weighted models ofhlex(?|?b?)
and hlex(?b?|?)
using lexical transla-tion models.?
Word-based insertion/deletion penalties thatpenalize through the low probabilities of thelexical translation models (Bender et al, 2004).?
Word/hierarchical-phrase length penalties.?
Backtrack-based penalties inspired by the dis-tortion penalties in phrase-based modeling(Watanabe et al, 2006b).3.2 Sparse FeaturesIn addition to the baseline features, a large numberof binary features are integrated in our MT system.We may use any binary features, such ash( f , e) =????????
?1 English word ?violate?
and Arabicword ?tnthk?
appeared in e and f .0 otherwise.The features are designed by considering the decod-ing efficiency and are based on the word alignmentstructure preserved in hierarchical phrase transla-tion pairs (Zens and Ney, 2006).
When hierarchi-cal phrases are extracted, the word alignment is pre-served.
If multiple word alignments are observedei?1 ei ei+1 ei+2 ei+3 ei+4f j?1 f j f j+1 f j+2 f j+3Figure 1: An example of sparse features for a phrasetranslation.with the same source and target sides, only the fre-quently observed word alignment is kept to reducethe grammar size.3.2.1 Word Pair FeaturesWord pair features reflect the word correspon-dence in a hierarchical phrase.
Figure 1 illustratesan example of sparse features for a phrase trans-lation pair f j, ..., f j+2 and ei, ..., ei+3 1.
From theword alignment encoded in this phrase, we can ex-tract word pair features of (ei, f j+1), (ei+2, f j+2) and(ei+3, f j).The bigrams of word pairs are also used tocapture the contextual dependency.
We assumethat the word pairs follow the target side order-ing.
For instance, we define ((ei?1, f j?1), (ei, f j+1)),((ei, f j+1), (ei+2, f j+2)) and ((ei+2, f j+2), (ei+3, f j)) in-dicated by the arrows in Figure 1.Extracting bigram word pair features followingthe target side ordering implies that the correspond-ing source side is reordered according to the tar-get side.
The reordering of hierarchical phrases isrepresented by using contextually dependent wordpairs across their boundaries, as with the feature((ei?1, f j?1), (ei, f j+1)) in Figure 1.3.2.2 Insertion FeaturesThe above features are insufficient to capture thetranslation because spurious words are sometimesinserted in the target side.
Therefore, insertion fea-tures are integrated in which no word alignment isassociated in the target.
The inserted words are asso-ciated with all the words in the source sentence, suchas (ei+1, f1), ..., (ei+1, fJ) for the non-aligned wordei+1 with the source sentence f J1 in Figure 1.
In the1For simplicity, we show an example of phrase translationpairs, but it is trivial to define the features over hierarchicalphrases.766f j?1f j f j+1f j+2f j+3X 1X 2X 3Figure 2: Example hierarchical features.same way, we will be able to include deletion fea-tures where a non-aligned source word is associatedwith the target sentence.
However, this would lead tocomplex decoding in which all the translated wordsare memorized for each hypothesis, and thus not in-tegrated in our feature set.3.2.3 Target Bigram FeaturesTarget side bigram features are also included todirectly capture the fluency as in the n-gram lan-guage model (Roark et al, 2004).
For instance, bi-gram features of (ei?1, ei), (ei, ei+1), (ei+1, ei+2)... areobserved in Figure 1.3.2.4 Hierarchical FeaturesIn addition to the phrase motivated features, weincluded features inspired by the hierarchical struc-ture.
Figure 2 shows an example of hierarchicalphrases in the source side, consisting of X 1 ?
?f j?1X 2 f j+3?, X 2 ?
?f j f j+1X 3?and X 3 ?
?f j+2?.Hierarchical features capture the dependency ofthe source words in a parent phrase to the sourcewords in child phrases, such as ( f j?1, f j), ( f j?1, f j+1),( f j+3, f j), ( f j+3, f j+1), ( f j, f j+2) and ( f j+1, f j+2) as in-dicated by the arrows in Figure 2.
The hierarchicalfeatures are extracted only for those source wordsthat are aligned with the target side to limit the fea-ture size.3.3 NormalizationIn order to achieve the generalization capability, thefollowing normalized tokens are introduced for eachsurface form:?
Word class or POS.?
4-letter prefix and suffix.
For instance, the wordAlgorithm 1 Online Training AlgorithmTraining data: T = {( f t, et)}Tt=1m-best oracles: O = {}Tt=1i = 01: for n = 1, ..., N do2: for t = 1, ..., T do3: Ct ?
bestk( f t; wi)4: Ot ?
oraclem(Ot ?
Ct; et)5: wi+1 = update wi using Ct w.r.t.
Ot6: i = i + 17: end for8: end for9: return?NTi=1 wiNT?violate?
is normalized to ?viol+?
and ?+late?by taking the prefix and suffix, respectively.?
Digits replaced by a sequence of ?@?.
For ex-ample, the word ?2007/6/27?
is represented as?
@@@@/@/@@?.We consider all possible combination of those to-ken types.
For example, the word pair feature (vi-olate, tnthk) is normalized and expanded to (viol+,tnthk), (viol+, tnth+), (violate, tnth+), etc.
using the4-letter prefix token type.4 Online Large-Margin TrainingAlgorithm 1 is our generic online training algo-rithm.
The algorithm is slightly different from otheronline training algorithms (Tillmann and Zhang,2006; Liang et al, 2006) in that we keep and up-date oracle translations, which is a set of good trans-lations reachable by a decoder according to a met-ric, i.e.
BLEU (Papineni et al, 2002).
In line 3,a k-best list is generated by bestk(?)
using the cur-rent weight vector wi for the training instance of( f t, et).
Each training instance has multiple (or, pos-sibly one) reference translations et for the sourcesentence f t. Using the k-best list, m-best oracletranslations Ot is updated by oraclem(?)
for every it-eration (line 4).
Usually, a decoder cannot generatetranslations that exactly match the reference transla-tions due to its beam search pruning and OOV.
Thus,we cannot always assign scores for each referencetranslation.
Therefore, possible oracle translationsare maintained according to an objective function,767i.e.
BLEU.
Tillmann and Zhang (2006) avoided theproblem by precomputing the oracle translations inadvance.
Liang et al (2006) presented a similar up-dating strategy in which parameters were updatedtoward an oracle translation found in Ct, but ignoredpotentially better translations discovered in the pastiterations.New wi+1 is computed using the k-best list Ct withrespect to the oracle translations Ot (line 5).
After Niterations, the algorithm returns an averaged weightvector to avoid overfitting (line 9).
The key to thisonline training algorithm is the selection of the up-dating scheme in line 5.4.1 Margin Infused Relaxed AlgorithmThe Margin Infused Relaxed Algorithm (MIRA)(Crammer et al, 2006) is an online version of thelarge-margin training algorithm for structured clas-sification (Taskar et al, 2004) that has been suc-cessfully used for dependency parsing (McDonald etal., 2005) and joint-labeling/chunking (Shimizu andHaas, 2006).
The basic idea is to keep the norm ofthe updates to the weight vector as small as possible,considering a margin at least as large as the loss ofthe incorrect classification.Line 5 of the weight vector update procedure inAlgorithm 1 is replaced by the solution of:w?i+1 = argminwi+1||wi+1 ?
wi|| + C?e?,e??
(e?, e?
)subject tosi+1( f t, e?)
?
si+1( f t, e?)
+ ?
(e?, e?)
?
L(e?, e?
; et)?
(e?, e?)
?
0?e?
?
Ot,?e?
?
Ct (3)where si( f t, e) ={wi}T ?
h( f t, e).
?(?)
is a non-negative slack variable and C ?
0 is a constant tocontrol the influence to the objective function.
Alarger C implies larger updates to the weight vec-tor.
L(?)
is a loss function, for instance difference ofBLEU, that measures the difference between e?
ande?
according to the reference translations et.
In thisupdate, a margin is created for each correct and in-correct translation at least as large as the loss of theincorrect translation.
A larger error means a largerdistance between the scores of the correct and incor-rect translations.
Following McDonald et al (2005),only k-best translations are used to form the marginsin order to reduce the number of constraints in Eq.
3.In the translation task, multiple translations are ac-ceptable.
Thus, margins for m-oracle translation arecreated, which amount to m ?
k large-margin con-straints.
In this online training, only active featuresconstrained by Eq.
3 are kept and updated, unlikeoffline training in which all possible features have tobe extracted and selected in advance.The Lagrange dual form of Eq.
3 is:max?(?
)?0 ?12||?e?,e??
(e?, e?
)(h( f t, e?)
?
h( f t, e?))||2+?e?,e??
(e?, e?
)L(e?, e?
; et)??e?,e??
(e?, e?
)(si( f t, e?)
?
si( f t, e?
))subject to?e?,e??
(e?, e?)
?
C (4)with the weight vector update:wi+1 = wi +?e?,e??
(e?, e?
)(h( f t, e?)
?
h( f t, e?
))(5)Equation 4 is solved using a QP-solver, such as a co-ordinate ascent algorithm, by heuristically selecting(e?, e?)
and by updating ?(?)
iteratively:?
(e?, e?)
= max (0, ?
(e?, e?)
+ ?
(e?, e?))
(6)?
(e?, e?)
=L(e?, e?
; et) ?
(si( f t, e?)
?
si( f t, e?
))||h( f t, e?)
?
h( f t, e?
)||2C is used to clip the amount of updates.A single oracle with 1-best translation is analyti-cally solved without a QP-solver and is representedas the following perceptron-like update (Shimizuand Haas, 2006):?
= max???????
?0, min???????
?C,L(e?, e?
; et) ?
(si( f t, e?)
?
si( f t, e?
))||h( f t, e?)
?
h( f t, e?)||2???????????????
?Intuitively, the update amount is controlled by themargin and the loss between the correct and incor-rect translations and by the closeness of two transla-tions in terms of feature vectors.
Indeed, Liang et al(2006) employed an averaged perceptron algorithmin which ?
value was always set to one.
Tillmannand Zhang (2006) used a different update style basedon a convex loss function:?
= ?L(e?, e?
; et) ?max(0, 1 ?
(si( f t, e?)
?
si( f t, e?
)))768Table 1: Experimental results obtained by varying normalized tokens used with surface form.# features 2003 (dev) 2004 2005NIST BLEU [%] NIST BLEU [%] NIST BLEU [%]surface form 492K 11.32 54.11 10.57 49.01 10.77 48.05w/ prefix/suffix 4,204K 12.38 63.87 10.42 48.74 10.58 47.18w/ word class 2,689K 10.87 49.59 10.63 49.55 10.89 48.79w/ digits 576K 11.01 50.72 10.66 49.67 10.84 48.39all token types 13,759K 11.24 52.85 10.66 49.81 10.85 48.41where ?
> 0 is a learning rate for controlling theconvergence.4.2 Approximated BLEUWe used the BLEU score (Papineni et al, 2002) asthe loss function computed by:BLEU(E; E) = exp???????
?1NN?n=1log pn(E, E)?????????
BP(E, E)(7)where pn(?)
is the n-gram precision of hypothesizedtranslations E = {et}Tt=1 given reference translationsE = {et}Tt=1 and BP(?)
?
1 is a brevity penalty.
BLEUis computed for a set of sentences, not for a sin-gle sentence.
Our algorithm requires frequent up-dates on the weight vector, which implies higher costin computing the document-wise BLEU.
Tillmannand Zhang (2006) and Liang et al (2006) solvedthe problem by introducing a sentence-wise BLEU.However, the use of the sentence-wise scoring doesnot translate directly into the document-wise scorebecause of the n-gram precision statistics and thebrevity penalty statistics aggregated for a sentenceset.
Thus, we use an approximated BLEU scorethat basically computes BLEU for a sentence set, butaccumulates the difference for a particular sentence(Watanabe et al, 2006a).The approximated BLEU is computed as follows:Given oracle translations O for T , we maintain thebest oracle translations OT1 ={e?1, ..., e?T}.
The ap-proximated BLEU for a hypothesized translation e?for the training instance ( f t, et) is computed over OT1except for e?t, which is replaced by e?
:BLEU({e?1, ..., e?t?1, e?, e?t+1, ..., e?T }; E)The loss computed by the approximated BLEU mea-sures the document-wise loss of substituting the cor-rect translation e?t into an incorrect translation e?.The score can be regarded as a normalization whichscales a sentence-wise score into a document-wisescore.5 ExperimentsWe employed our online large-margin training pro-cedure for an Arabic-to-English translation task.The training data were extracted from the Ara-bic/English news/UN bilingual corpora supplied byLDC.
The data amount to nearly 3.8M sentences.The Arabic part of the bilingual data is tokenized byisolating Arabic scripts and punctuation marks.
Thedevelopment set comes from the MT2003 Arabic-English NIST evaluation test set consisting of 663sentences in the news domain with four referencetranslations.
The performance is evaluated by thenews domain MT2004/MT2005 test set consistingof 707 and 1,056 sentences, respectively.The hierarchical phrase translation pairs are ex-tracted in a standard way (Chiang, 2005): First,the bilingual data are word alignment annotated byrunning GIZA++ (Och and Ney, 2003) in two di-rections.
Second, the word alignment is refinedby a grow-diag-final heuristic (Koehn et al, 2003).Third, phrase translation pairs are extracted togetherwith hierarchical phrases by considering holes.
Inthe last step, the hierarchical phrases are constrainedso that they follow the target normalized form con-straint.
A 5-gram language model is trained on theEnglish side of the bilingual data combined with theEnglish Gigaword from LDC.First, the use of normalized token types in Sec-tion 3.3 is evaluated in Table 1.
In this setting, allthe structural features in Section 3.2 are used, butdifferentiated by the normalized tokens combinedwith surface forms.
Our online large-margin train-ing algorithm performed 50 iterations constrained769Table 2: Experimental results obtained by incrementally adding structural features.# features 2003 (dev) 2004 2005NIST BLEU [%] NIST BLEU [%] NIST BLEU [%]word pairs 11,042K 11.05 51.63 10.43 48.69 10.73 47.72+ target bigram 11,230K 11.19 53.49 10.40 48.60 10.66 47.47+ insertion 13,489K 11.21 52.20 10.77 50.33 10.93 48.08+ hierarchical 13,759K 11.24 52.85 10.66 49.81 10.85 48.41Table 3: Experimental results for varying k-best and m-oracle translations.# features 2003 (dev) 2004 2005NIST BLEU [%] NIST BLEU [%] NIST BLEU [%]baseline 10.64 46.47 10.83 49.33 10.90 47.031-oracle 1-best 8,735K 11.25 52.63 10.82 50.77 10.93 48.111-oracle 10-best 10,480K 11.24 53.45 10.55 49.10 10.82 48.4910-oracle 1-best 8,416K 10.70 47.63 10.83 48.88 10.76 46.0010-oracle 10-best 13,759K 11.24 52.85 10.66 49.81 10.85 48.41sentence-BLEU 14,587K 11.10 51.17 10.82 49.97 10.86 47.04by 10-oracle and 10-best list.
When decoding, a1000-best list is generated to achieve better oracletranslations.
The training took nearly 1 day using 8cores of Opteron.
The translation quality is eval-uated by case-sensitive NIST (Doddington, 2002)and BLEU (Papineni et al, 2002)2.
The table alsoshows the number of active features in which non-zero values were assigned as weights.
The additionof prefix/suffix tokens greatly increased the numberof active features.
The setting severely overfit to thedevelopment data, and therefore resulted in worseresults in open tests.
The word class3 with surfaceform avoided the overfitting problem.
The digit se-quence normalization provides a similar generaliza-tion capability despite of the moderate increase inthe active feature size.
By including all token types,we achieved better NIST/BLEU scores for the 2004and 2005 test sets.
This set of experiments indi-cates that a token normalization is useful especiallytrained on a small data.Second, we used all the normalized token types,but incrementally added structural features in Ta-ble 2.
Target bigram features account for only thefluency of the target side without considering thesource/target correspondence.
Therefore, the in-2We used the tool available at http://www.nist.gov/speech/tests/mt/3We induced 50 classes each for English and Arabic.clusion of target bigram features clearly overfit tothe development data.
The problem is resolved byadding insertion features which can take into ac-count an agreement with the source side that is notdirectly captured by word pair features.
Hierarchi-cal features are somewhat effective in the 2005 testset by considering the dependency structure of thesource side.Finally, we compared our online training algo-rithm with sparse features with a baseline systemin Table 3.
The baseline hierarchical phrase-basedsystem is trained using standard max-BLEU training(MERT) without sparse features (Och, 2003).
Table3 shows the results obtained by varying the m-oracleand k-best size (k, m = 1, 10) using all structuralfeatures and all token types.
We also experimentedsentence-wise BLEU as an objective function con-strained by 10-oracle and 10-best list.
Even the 1-oracle 1-best configuration achieved significant im-provements over the baseline system.
The use ofa larger k-best list further optimizes to the devel-opment set, but at the cost of degraded translationquality in the 2004 test set.
The larger m-oracle sizeseems to be harmful if coupled with the 1-best list.As indicated by the reduced active feature size, 1-best translation seems to be updated toward worsetranslations in 10-oracles that are ?close?
in termsof features.
We achieved significant improvements770Table 4: Two-fold cross validation experiments.closed test open testNIST BLEU NIST BLEU[%] [%]baseline 10.71 44.79 10.68 44.44online 11.58 53.42 10.90 47.64when the k-best list size was also increased.
Theuse of sentence-wise BLEU as an objective providesalmost no improvement in the 2005 test set, but iscomparable for the 2004 test set.As observed in three experiments, the 2004/2005test sets behaved differently, probably because ofthe domain mismatch.
Thus, we conducted a two-fold cross validation using the 2003/2004/2005 testsets to observe the effect of optimization as shownin Table 44.
The MERT baseline system performedsimilarly both in closed and open tests.
Our on-line large-margin training with 10-oracle and 10-best constraints and the approximated BLEU lossfunction significantly outperformed the baseline sys-tem in the open test.
The development data is almostdoubled in this setting.
The MERT approach seemsto be confused with the slightly larger data and withthe mixed domains from different epochs.6 DiscussionIn this work, the translation model consisting of mil-lions of features are successfully integrated.
In or-der to avoid poor overfitting, features are limited toword-based features, but are designed to reflect thestructures inside hierarchical phrases.
One of thebenefit of MIRA is its flexibility.
We may includeas many constraints as possible, like m-oracle con-straints in our experiments.
Although we describedexperiments on the hierarchical phrase-based trans-lation, the online training algorithm is applicable toany translation systems, such as phrase-based trans-lations and syntax-based translations.Online discriminative training has already beenstudied by Tillmann and Zhang (2006) and Lianget al (2006).
In their approach, training was per-formed on a large corpus using the sparse features ofphrase translation pairs, target n-grams and/or bag-of-word pairs inside phrases.
In Tillmann and Zhang4We split data by document, not by sentence.
(2006), k-best list generation is approximated by astep-by-step one-best merging method that separatesthe decoding and training steps.
The weight vectorupdate scheme is very similar to MIRA but basedon a convex loss function.
Our method directly em-ploys the k-best list generated by the fast decodingmethod (Watanabe et al, 2006b) at every iteration.One of the benefits is that we avoid the rather expen-sive cost of merging the k-best list especially whenhandling millions of features.Liang et al (2006) employed an averaged percep-tron algorithm.
They decoded each training instanceand performed a perceptron update to the weightvector.
An incorrect translation was updated towardan oracle translation found in a k-best list, but dis-carded potentially better translations in the past iter-ations.An experiment has been undertaken using a smalldevelopment set together with sparse features for thereranking of a k-best translation (Watanabe et al,2006a).
They relied on a variant of a voted percep-tron, and achieved significant improvements.
How-ever, their work was limited to reranking, thus theimprovement was relative to the performance of thebaseline system, whether or not there was a goodtranslation in a list.
In our work, the sparse featuresare directly integrated into the DP-based search.The design of the sparse features was inspiredby Zens and Ney (2006).
They exploited theword alignment structure inside the phrase trans-lation pairs for discriminatively training a reorder-ing model in their phrase-based translation.
The re-ordering model simply classifies whether to performmonotone decoding or not.
The trained model istreated as a single feature function integrated in Eq.1.
Our approach differs in that each sparse feature isindividually integrated in Eq.
1.7 ConclusionWe exploited a large number of binary featuresfor statistical machine translation.
The model wastrained on a small development set.
The optimiza-tion was carried out by MIRA, which is an onlineversion of the large-margin training algorithm.
Mil-lions of sparse features are intuitively consideredprone to overfitting, especially when trained on asmall development set.
However, our algorithm with771millions of features achieved very significant im-provements over a conventional method with a smallnumber of features.
This result indicates that wecan easily experiment many alternative features evenwith a small data set, but we believe that our ap-proach can scale well to a larger data set for furtherimproved performance.
Future work involves scal-ing up to larger data and more features.AcknowledgementsWe would like to thank reviewers and our colleaguesfor useful comment and discussion.ReferencesSrinivas Bangalore, Patrick Haffner, and Stephan Kan-thak.
2006.
Sequence classification for machine trans-lation.
In Proc.
of Interspeech 2006, pages 1157?1160, Pittsburgh.Oliver Bender, Richard Zens, Evgeny Matusov, and Her-mann Ney.
2004.
Alignment templates: the RWTHSMT system?.
In Proc.
of IWSLT 2004, pages 79?84,Kyoto, Japan.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proc.
of ACL2005, pages 263?270, Ann Arbor, Michigan, June.Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer.
2006.
Online passive-aggressive algorithms.
Journal of Machine LearningResearch, 7:551?585, March.George Doddington.
2002.
Automatic evaluation of ma-chine translation quality using n-gram co-occurrencestatistics.
In In Proc.
ARPA Workshop on Human Lan-guage Technology.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Proc.of COLING/ACL 2006, pages 961?968, Sydney, Aus-tralia, July.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proc.of NAACL 2003, pages 48?54, Edmonton, Canada.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: Probabilistic modelsfor segmenting and labeling sequence data.
In Proc.18th International Conf.
on Machine Learning, pages282?289.
Morgan Kaufmann, San Francisco, CA.Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, andBen Taskar.
2006.
An end-to-end discriminativeapproach to machine translation.
In Proc.
of COL-ING/ACL 2006, pages 761?768, Sydney, Australia,July.Ryan McDonald, Koby Crammer, and Fernando Pereira.2005.
Online large-margin training of dependencyparsers.
In Proc.
of ACL 2005, pages 91?98, Ann Ar-bor, Michigan, June.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.Franz Josef Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine transla-tion.
Computational Linguistics, 30(4):417?449.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proc.
of ACL 2003,pages 160?167, Sapporo, Japan, July.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proc.
of ACL 2002,pages 311?318, Philadelphia, Pennsylvania.Brian Roark, Murat Saraclar, Michael Collins, and MarkJohnson.
2004.
Discriminative language model-ing with conditional random fields and the percep-tron algorithm.
In Proc.
of ACL 2004, pages 47?54,Barcelona, Spain, July.Nobuyuki Shimizu and Andrew Haas.
2006.
Exact de-coding for jointly labeling and chunking sequences.In Proc.
of the COLING/ACL 2006 Main ConferencePoster Sessions, pages 763?770, Sydney, Australia,July.Ben Taskar, Dan Klein, Mike Collins, Daphne Koller, andChristopher Manning.
2004.
Max-margin parsing.
InProc.
of EMNLP 2004, pages 1?8, Barcelona, Spain,July.Christoph Tillmann and Tong Zhang.
2006.
A discrimi-native global training algorithm for statistical MT.
InProc.
of COLING/ACL 2006, pages 721?728, Sydney,Australia, July.Taro Watanabe, Jun Suzuki, Hajime Tsukada, and HidekiIsozaki.
2006a.
NTT Statistical Machine Translationfor IWSLT 2006.
In Proc.
of IWSLT 2006, pages 95?102, Kyoto, Japan.Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.2006b.
Left-to-right target generation for hierarchi-cal phrase-based translation.
In Proc.
of COLING/ACL2006, pages 777?784, Sydney, Australia, July.772Dekai Wu and Hongsing Wong.
1998.
Machine transla-tion with a stochastic grammatical channel.
In Proc.of COLING 98, pages 1408?1415, Montreal, Quebec,Canada.Richard Zens and Hermann Ney.
2006.
Discriminativereordering models for statistical machine translation.In Proc.
of WSMT 2006, pages 55?63, New York City,June.Andreas Zollmann and Ashish Venugopal.
2006.
Syntaxaugmented machine translation via chart parsing.
InProc.
of WSMT 2006, pages 138?141, New York City,June.773
