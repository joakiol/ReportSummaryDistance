Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 507?514,Sydney, July 2006. c?2006 Association for Computational LinguisticsMildly Non-Projective Dependency StructuresMarco KuhlmannProgramming Systems LabSaarland UniversityGermanykuhlmann@ps.uni-sb.deJoakim NivreV?xj?
University andUppsala UniversitySwedennivre@msi.vxu.seAbstractSyntactic parsing requires a fine balancebetween expressivity and complexity, sothat naturally occurring structures can beaccurately parsed without compromisingefficiency.
In dependency-based parsing,several constraints have been proposed thatrestrict the class of permissible structures,such as projectivity, planarity, multi-pla-narity, well-nestedness, gap degree, andedge degree.
While projectivity is gener-ally taken to be too restrictive for naturallanguage syntax, it is not clear which of theother proposals strikes the best balance be-tween expressivity and complexity.
In thispaper, we review and compare the differentconstraints theoretically, and provide an ex-perimental evaluation using data from twotreebanks, investigating how large a propor-tion of the structures found in the treebanksare permitted under different constraints.The results indicate that a combination ofthe well-nestedness constraint and a para-metric constraint on discontinuity gives avery good fit with the linguistic data.1 IntroductionDependency-based representations have become in-creasingly popular in syntactic parsing, especiallyfor languages that exhibit free or flexible word or-der, such as Czech (Collins et al, 1999), Bulgarian(Marinov and Nivre, 2005), and Turkish (Eryig?itand Oflazer, 2006).
Many practical implementa-tions of dependency parsing are restricted to pro-jective structures, where the projection of a headword has to form a continuous substring of thesentence.
While this constraint guarantees goodparsing complexity, it is well-known that certainsyntactic constructions can only be adequately rep-resented by non-projective dependency structures,where the projection of a head can be discontinu-ous.
This is especially relevant for languages withfree or flexible word order.However, recent results in non-projective depen-dency parsing, especially using data-driven meth-ods, indicate that most non-projective structuresrequired for the analysis of natural language arevery nearly projective, differing only minimallyfrom the best projective approximation (Nivre andNilsson, 2005; Hall and Nov?k, 2005; McDon-ald and Pereira, 2006).
This raises the questionof whether it is possible to characterize a class ofmildly non-projective dependency structures that isrich enough to account for naturally occurring syn-tactic constructions, yet restricted enough to enableefficient parsing.In this paper, we review a number of propos-als for classes of dependency structures that liebetween strictly projective and completely unre-stricted non-projective structures.
These classeshave in common that they can be characterized interms of properties of the dependency structuresthemselves, rather than in terms of grammar for-malisms that generate the structures.
We comparethe proposals from a theoretical point of view, andevaluate a subset of them empirically by testingtheir representational adequacy with respect to twodependency treebanks: the Prague DependencyTreebank (PDT) (Hajic?
et al, 2001), and the DanishDependency Treebank (DDT) (Kromann, 2003).The rest of the paper is structured as follows.In section 2, we provide a formal definition of de-pendency structures as a special kind of directedgraphs, and characterize the notion of projectivity.In section 3, we define and compare five differentconstraints on mildly non-projective dependencystructures that can be found in the literature: pla-narity, multiplanarity, well-nestedness, gap degree,and edge degree.
In section 4, we provide an ex-perimental evaluation of the notions of planarity,well-nestedness, gap degree, and edge degree, by507investigating how large a proportion of the depen-dency structures found in PDT and DDT are al-lowed under the different constraints.
In section 5,we present our conclusions and suggestions for fur-ther research.2 Dependency graphsFor the purposes of this paper, a dependency graphis a directed graph on the set of indices correspond-ing to the tokens of a sentence.
We write ?n?
to referto the set of positive integers up to and including n.Definition 1 A dependency graph for a sentencex D w1; : : : ; wn is a directed graph1G D .V I E/; where V D ?n?
and E  V  V .Throughout this paper, we use standard terminol-ogy and notation from graph theory to talk aboutdependency graphs.
In particular, we refer to theelements of the set V as nodes, and to the elementsof the set E as edges.
We write i !
j to mean thatthere is an edge from the node i to the node j (i.e.,.i; j / 2 E), and i ! j to mean that the node idominates the node j , i.e., that there is a (possiblyempty) path from i to j .
For a given node i , the setof nodes dominated by i is the yield of i .
We usethe notation .i/ to refer to the projection of i : theyield of i , arranged in ascending order.2.1 Dependency forestsMost of the literature on dependency grammar anddependency parsing does not allow arbitrary de-pendency graphs, but imposes certain structuralconstraints on them.
In this paper, we restrict our-selves to dependency graphs that form forests.Definition 2 A dependency forest is a dependencygraph with two additional properties:1. it is acyclic (i.e., if i !
j , then not j ! i);2. each of its nodes has at most one incomingedge (i.e., if i !
j , then there is no node ksuch that k ?
i and k !
j ).Nodes in a forest without an incoming edge arecalled roots.
A dependency forest with exactly oneroot is a dependency tree.Figure 1 shows a dependency forest taken fromPDT.
It has two roots: node 2 (corresponding to thecomplementizer proto) and node 8 (correspondingto the final punctuation mark).1We only consider unlabelled dependency graphs.1 2 3 5 64 7 8Nen?
proto zapot?eb?
uzav?rat nov?
n?jemn?
smlouvy .contractsleasenewsignneededis-not therefore .
?It is therefore not needed to sign new lease contracts.
?Figure 1: Dependency forest for a Czech sentencefrom the Prague Dependency TreebankSome authors extend dependency forests by aspecial root node with position 0, and add an edge.0; i/ for every root node i of the remaining graph(McDonald et al, 2005).
This ensures that the ex-tended graph always is a tree.
Although such adefinition can be useful, we do not follow it here,since it obscures the distinction between projectiv-ity and planarity to be discussed in section 3.2.2 ProjectivityIn contrast to acyclicity and the indegree constraint,both of which impose restrictions on the depen-dency relation as such, the projectivity constraintconcerns the interaction between the dependencyrelation and the positions of the nodes in the sen-tence: it says that the nodes in a subtree of a de-pendency graph must form an interval, where aninterval (with endpoints i and j ) is the set?i; j ?
WD f k 2 V j i  k and k  j g :Definition 3 A dependency graph is projective, ifthe yields of its nodes are intervals.Since projectivity requires each node to dominate acontinuous substring of the sentence, it correspondsto a ban on discontinuous constituents in phrasestructure representations.Projectivity is an interesting constraint on de-pendency structures both from a theoretical anda practical perspective.
Dependency grammarsthat only allow projective structures are closelyrelated to context-free grammars (Gaifman, 1965;Obre?bski and Gralin?ski, 2004); among other things,they have the same (weak) expressivity.
The pro-jectivity constraint also leads to favourable pars-ing complexities: chart-based parsing of projectivedependency grammars can be done in cubic time(Eisner, 1996); hard-wiring projectivity into a de-terministic dependency parser leads to linear-timeparsing in the worst case (Nivre, 2003).5083 Relaxations of projectivityWhile the restriction to projective analyses has anumber of advantages, there is clear evidence thatit cannot be maintained for real-world data (Zeman,2004; Nivre, 2006).
For example, the graph inFigure 1 is non-projective: the yield of the node 1(marked by the dashed rectangles) does not forman interval?the node 2 is ?missing?.
In this sec-tion, we present several proposals for structuralconstraints that relax projectivity, and relate themto each other.3.1 Planarity and multiplanarityThe notion of planarity appears in work on LinkGrammar (Sleator and Temperley, 1993), whereit is traced back to Mel?c?uk (1988).
Informally,a dependency graph is planar, if its edges can bedrawn above the sentence without crossing.
Weemphasize the word above, because planarity asit is understood here does not coincide with thestandard graph-theoretic concept of the same name,where one would be allowed to also use the areabelow the sentence to disentangle the edges.Figure 2a shows a dependency graph that is pla-nar but not projective: while there are no crossingedges, the yield of the node 1 (the set f1; 3g) doesnot form an interval.Using the notation linked.i; j / as an abbrevia-tion for the statement ?there is an edge from i to j ,or vice versa?, we formalize planarity as follows:Definition 4 A dependency graph is planar, if itdoes not contain nodes a; b; c; d such thatlinked.a; c/ ^ linked.b; d/ ^ a < b < c < d :Yli-Jyr?
(2003) proposes multiplanarity as a gen-eralization of planarity suitable for modelling de-pendency analyses, and evaluates it experimentallyusing data from DDT.Definition 5 A dependency graph G D .V I E/ ism-planar, if it can be split into m planar graphsG1 D .V I E1/; : : : ;Gm D .V I Em/such that E D E1]  ]Em.
The planar graphs Giare called planes.As an example of a dependency forest that is 2-planar but not planar, consider the graph depicted inFigure 2b.
In this graph, the edges .1; 4/ and .3; 5/are crossing.
Moving either edge to a separategraph partitions the original graph into two planes.1 2 3(a) 1-planar1 2 3 4 5(b) 2-planarFigure 2: Planarity and multi-planarity3.2 Gap degree and well-nestednessBodirsky et al (2005) present two structural con-straints on dependency graphs that characterizeanalyses corresponding to derivations in Tree Ad-joining Grammar: the gap degree restriction andthe well-nestedness constraint.A gap is a discontinuity in the projection of anode in a dependency graph (Pl?tek et al, 2001).More precisely, let i be the projection of thenode i .
Then a gap is a pair .jk ; jkC1/ of nodesadjacent in i such that jkC1   jk > 1.Definition 6 The gap degree of a node i in a de-pendency graph, gd.i/, is the number of gaps in i .As an example, consider the node labelled i in thedependency graphs in Figure 3.
In Graph 3a, theprojection of i is an interval (.2; 3; 4/), so i has gapdegree 0.
In Graph 3b, i D .2; 3; 6/ contains asingle gap (.3; 6/), so the gap degree of i is 1.
Inthe rightmost graph, the gap degree of i is 2, sincei D .2; 4; 6/ contains two gaps (.2; 4/ and .4; 6/).Definition 7 The gap degree of a dependencygraph G, gd.G/, is the maximum among the gapdegrees of its nodes.Thus, the gap degree of the graphs in Figure 3is 0, 1 and 2, respectively, since the node i has themaximum gap degree in all three cases.The well-nestedness constraint restricts the posi-tioning of disjoint subtrees in a dependency forest.Two subtrees are called disjoint, if neither of theirroots dominates the other.Definition 8 Two subtrees T1;T2 interleave, ifthere are nodes l1; r1 2 T1 and l2; r2 2 T2 suchthat l1 < l2 < r1 < r2.
A dependency graph iswell-nested, if no two of its disjoint subtrees inter-leave.Both Graph 3a and Graph 3b are well-nested.Graph 3c is not well-nested.
To see this, let T1be the subtree rooted at the node labelled i , andlet T2 be the subtree rooted at j .
These subtreesinterleave, as T1 contains the nodes 2 and 4, and T2contains the nodes 3 and 5.509ji1 2 3 5 64(a) gd D 0, ed D 0, wnCji1 2 3 5 64(b) gd D 1, ed D 1, wnCji1 2 3 5 64(c) gd D 2, ed D 1, wn Figure 3: Gap degree, edge degree, and well-nestedness3.3 Edge degreeThe notion of edge degree was introduced by Nivre(2006) in order to allow mildly non-projective struc-tures while maintaining good parsing efficiency indata-driven dependency parsing.2Define the span of an edge .i; j / as the intervalS..i; j // WD ?min.i; j /;max.i; j /?
:Definition 9 Let G D .V I E/ be a dependencyforest, let e D .i; j / be an edge in E, and let Gebe the subgraph of G that is induced by the nodescontained in the span of e. The degree of an edge e 2 E, ed.e/, is thenumber of connected components c in Gesuch that the root of c is not dominated bythe head of e. The edge degree of G, ed.G/, is the maximumamong the degrees of the edges in G.To illustrate the notion of edge degree, we returnto Figure 3.
Graph 3a has edge degree 0: the onlyedge that spans more nodes than its head and its de-pendent is .1; 5/, but the root of the connected com-ponent f2; 3; 4g is dominated by 1.
Both Graph 3band 3c have edge degree 1: the edge .3; 6/ inGraph 3b and the edges .2; 4/, .3; 5/ and .4; 6/ inGraph 3c each span a single connected componentthat is not dominated by the respective head.3.4 Related workApart from proposals for structural constraints re-laxing projectivity, there are dependency frame-works that in principle allow unrestricted graphs,but provide mechanisms to control the actually per-mitted forms of non-projectivity in the grammar.The non-projective dependency grammar of Ka-hane et al (1998) is based on an operation on de-pendency trees called lifting: a ?lift?
of a tree T isthe new tree that is obtained when one replaces one2We use the term edge degree instead of the original simpleterm degree from Nivre (2006) to mark the distinction fromthe notion of gap degree.or more edges .i; k/ in T by edges .j ; k/, wherej ! i .
The exact conditions under which a cer-tain lifting may take place are specified in the rulesof the grammar.
A dependency tree is acceptable,if it can be lifted to form a projective graph.3A similar design is pursued in Topological De-pendency Grammar (Duchier and Debusmann,2001), where a dependency analysis consists oftwo, mutually constraining graphs: the ID graphrepresents information about immediate domi-nance, the LP graph models the topological struc-ture of a sentence.
As a principle of the grammar,the LP graph is required to be a lift of the ID graph;this lifting can be constrained in the lexicon.3.5 DiscussionThe structural conditions we have presented herenaturally fall into two groups: multiplanarity, gapdegree and edge degree are parametric constraintswith an infinite scale of possible values; planarityand well-nestedness come as binary constraints.We discuss these two groups in turn.Parametric constraints With respect to thegraded constraints, we find that multiplanarity isdifferent from both gap degree and edge degreein that it involves a notion of optimization: sinceevery dependency graph is m-planar for some suf-ficiently large m (put each edge onto a separateplane), the interesting question in the context ofmultiplanarity is about the minimal values for mthat occur in real-world data.
But then, one notonly needs to show that a dependency graph can bedecomposed into m planar graphs, but also that thisdecomposition is the one with the smallest numberof planes among all possible decompositions.
Upto now, no tractable algorithm to find the minimaldecomposition has been given, so it is not clear howto evaluate the significance of the concept as such.The evaluation presented by Yli-Jyr?
(2003) makesuse of additional constraints that are sufficient tomake the decomposition unique.3We remark that, without restrictions on the lifting, everynon-projective tree has a projective lift.5101 2 3 5 64(a) gd D 2, ed D 11 2 3 54(b) gd D 1, ed D 2Figure 4: Comparing gap degree and edge degreeThe fundamental difference between gap degreeand edge degree is that the gap degree measures thenumber of discontinuities within a subtree, whilethe edge degree measures the number of interven-ing constituents spanned by a single edge.
Thisdifference is illustrated by the graphs displayed inFigure 4.
Graph 4a has gap degree 2 but edge de-gree 1: the subtree rooted at node 2 (marked bythe solid edges) has two gaps, but each of its edgesonly spans one connected component not domi-nated by 2 (marked by the squares).
In contrast,Graph 4b has gap degree 1 but edge degree 2: thesubtree rooted at node 2 has one gap, but this gapcontains two components not dominated by 2.Nivre (2006) shows experimentally that limitingthe permissible edge degree to 1 or 2 can reduce theaverage parsing time for a deterministic algorithmfrom quadratic to linear, while omitting less than1% of the structures found in DDT and PDT.
Itcan be expected that constraints on the gap degreewould have very similar effects.Binary constraints For the two binary con-straints, we find that well-nestedness subsumesplanarity: a graph that contains interleaving sub-trees cannot be drawn without crossing edges, soevery planar graph must also be well-nested.
To seethat the converse does not hold, consider Graph 3b,which is well-nested, but not planar.Since both planarity and well-nestedness areproper extensions of projectivity, we get the fol-lowing hierarchy for sets of dependency graphs:projective  planar  well-nested  unrestrictedThe planarity constraint appears like a very naturalone at first sight, as it expresses the intuition that?crossing edges are bad?, but still allows a limitedform of non-projectivity.
However, many authorsuse planarity in conjunction with a special repre-sentation of the root node: either as an artificialnode at the sentence boundary, as we mentioned insection 2, or as the target of an infinitely long per-pendicular edge coming ?from the outside?, as inearlier versions of Word Grammar (Hudson, 2003).In these situations, planarity reduces to projectivity,so nothing is gained.Even in cases where planarity is used without aspecial representation of the root node, it remainsa peculiar concept.
When we compare it with thenotion of gaps, for example, we find that, in a planardependency tree, every gap .i; j / must contain theroot node r , in the sense that i < r < j : if the gapwould only contain non-root nodes k, then the twopaths from r to k and from i to j would cross.
Thisparticular property does not seem to be mirrored inany linguistic prediction.In contrast to planarity, well-nestedness is inde-pendent from both gap degree and edge degree inthe sense that for every d > 0, there are both well-nested and non-well-nested dependency graphswith gap degree or edge degree d .
All projective de-pendency graphs (d D 0) are trivially well-nested.Well-nestedness also brings computational bene-fits.
In particular, chart-based parsers for grammarformalisms in which derivations obey the well-nest-edness constraint (such as Tree Adjoining Gram-mar) are not hampered by the ?crossing configu-rations?
to which Satta (1992) attributes the factthat the universal recognition problem of LinearContext-Free Rewriting Systems is NP -complete.4 Experimental evaluationIn this section, we present an experimental eval-uation of planarity, well-nestedness, gap degree,and edge degree, by examining how large a pro-portion of the structures found in two dependencytreebanks are allowed under different constraints.Assuming that the treebank structures are sampledfrom naturally occurring structures in natural lan-guage, this provides an indirect evaluation of thelinguistic adequacy of the different proposals.4.1 Experimental setupThe experiments are based on data from the PragueDependency Treebank (PDT) (Hajic?
et al, 2001)and the Danish Dependency Treebank (DDT) (Kro-mann, 2003).
PDT contains 1.5M words of news-paper text, annotated in three layers according tothe theoretical framework of Functional GenerativeDescription (B?hmov?
et al, 2003).
Our experi-ments concern only the analytical layer, and arebased on the dedicated training section of the tree-bank.
DDT comprises 100k words of text selectedfrom the Danish PAROLE corpus, with annotation511Table 1: Experimental results for DDT and PDTproperty DDT PDTall structures n D 4393 n D 73088gap degree 0 3732 84.95% 56168 76.85%gap degree 1 654 14.89% 16608 22.72%gap degree 2 7 0.16% 307 0.42%gap degree 3 ?
?
4 0.01%gap degree 4 ?
?
1 < 0.01%edge degree 0 3732 84.95% 56168 76.85%edge degree 1 584 13.29% 16585 22.69%edge degree 2 58 1.32% 259 0.35%edge degree 3 17 0.39% 63 0.09%edge degree 4 2 0.05% 10 0.01%edge degree 5 ?
?
2 < 0.01%edge degree 6 ?
?
1 < 0.01%projective 3732 84.95% 56168 76.85%planar 3796 86.41% 60048 82.16%well-nested 4388 99.89% 73010 99.89%non-projective structures only n D 661 n D 16920planar 64 9.68% 3880 22.93%well-nested 656 99.24% 16842 99.54%of primary and secondary dependencies based onDiscontinuous Grammar (Kromann, 2003).
Onlyprimary dependencies are considered in the experi-ments, which are based on the entire treebank.44.2 ResultsThe results of our experiments are given in Table 1.For the binary constraints (planarity, well-nested-ness), we simply report the number and percentageof structures in each data set that satisfy the con-straint.
For the parametric constraints (gap degree,edge degree), we report the number and percentageof structures having degree d (d  0), where de-gree 0 is equivalent (for both gap degree and edgedegree) to projectivity.For DDT, we see that about 15% of all analysesare non-projective.
The minimal degree of non-pro-jectivity required to cover all of the data is 2 in thecase of gap degree and 4 in the case of edge degree.For both measures, the number of structures dropsquickly as the degree increases.
(As an example,only 7 or 0:17% of the analyses in DDT have gap4A total number of 17 analyses in DDT were excludedbecause they either had more than one root node, or violatedthe indegree constraint.
(Both cases are annotation errors.
)degree 2.)
Regarding the binary constraints, wefind that planarity accounts for slightly more thanthe projective structures (86:41% of the data is pla-nar), while almost all structures in DDT (99:89%)meet the well-nestedness constraint.
The differ-ence between the two constraints becomes clearerwhen we base the figures on the set of non-projec-tive structures only: out of these, less than 10% areplanar, while more than 99% are well-nested.For PDT, both the number of non-projectivestructures (around 23%) and the minimal degreesof non-projectivity required to cover the full data(gap degree 4 and edge degree 6) are higher than inDDT.
The proportion of planar analyses is smallerthan in DDT if we base it on the set of all structures(82:16%), but significantly larger when based onthe set of non-projective structures only (22:93%).However, this is still very far from the well-nested-ness constraint, which has almost perfect coverageon both data sets.4.3 DiscussionAs a general result, our experiments confirm previ-ous studies on non-projective dependency parsing(Nivre and Nilsson, 2005; Hall and Nov?k, 2005;512McDonald and Pereira, 2006): The phenomenonof non-projectivity cannot be ignored without alsoignoring a significant portion of real-world data(around 15% for DDT, and 23% for PDT).
At thesame time, already a small step beyond projectivityaccounts for almost all of the structures occurringin these treebanks.More specifically, we find that already an edgedegree restriction of d  1 covers 98:24% of DDTand 99:54% of PDT, while the same restrictionon the gap degree scale achieves a coverage of99:84% (DDT) and 99:57% (PDT).
Together withthe previous evidence that both measures also havecomputational advantages, this provides a strongindication for the usefulness of these constraints inthe context of non-projective dependency parsing.When we compare the two graded constraintsto each other, we find that the gap degree measurepartitions the data into less and larger clusters thanthe edge degree, which may be an advantage in thecontext of using the degree constraints as featuresin a data-driven approach towards parsing.
How-ever, our purely quantitative experiments cannotanswer the question, which of the two measuresyields the more informative clusters.The planarity constraint appears to be of littleuse as a generalization of projectivity: enforcingit excludes more than 75% of the non-projectivedata in PDT, and 90% of the data in DDT.
The rela-tively large difference in coverage between the twotreebanks may at least partially be explained withtheir different annotation schemes for sentence-fi-nal punctuation.
In DDT, sentence-final punctua-tion marks are annotated as dependents of the mainverb of a dependency nexus.
This, as we havediscussed above, places severe restrictions on per-mitted forms of non-projectivity in the remainingsentence, as every discontinuity that includes themain verb must also include the dependent punctu-ation marks.
On the other hand, in PDT, a sentence-final punctuation mark is annotated as a separateroot node with no dependents.
This scheme doesnot restrict the remaining discontinuities at all.In contrast to planarity, the well-nestedness con-straint appears to constitute a very attractive exten-sion of projectivity.
For one thing, the almost per-fect coverage of well-nestedness on DDT and PDT(99:89%) could by no means be expected on purelycombinatorial grounds?only 7% of all possibledependency structures for sentences of length 17(the average sentence length in PDT), and onlyslightly more than 5% of all possible dependencystructures for sentences of length 18 (the averagesentence length in DDT) are well-nested.5 More-over, a cursory inspection of the few problematiccases in DDT indicates that violations of the well-nestedness constraint may, at least in part, be dueto properties of the annotation scheme, such as theanalysis of punctuation in quotations.
However, amore detailed analysis of the data from both tree-banks is needed before any stronger conclusionscan be drawn concerning well-nestedness.5 ConclusionIn this paper, we have reviewed a number of pro-posals for the characterization of mildly non-pro-jective dependency structures, motivated by theneed to find a better balance between expressivityand complexity than that offered by either strictlyprojective or unrestricted non-projective structures.Experimental evaluation based on data from twotreebanks shows, that a combination of the well-nestedness constraint and parametric constraintson discontinuity (formalized either as gap degreeor edge degree) gives a very good fit with the em-pirical linguistic data.
Important goals for futurework are to widen the empirical basis by inves-tigating more languages, and to perform a moredetailed analysis of linguistic phenomena that vio-late certain constraints.
Another important line ofresearch is the integration of these constraints intoparsing algorithms for non-projective dependencystructures, potentially leading to a better trade-offbetween accuracy and efficiency than that obtainedwith existing methods.Acknowledgements We thank three anonymousreviewers of this paper for their comments.
Thework of Marco Kuhlmann is funded by the Collab-orative Research Centre 378 ?Resource-AdaptiveCognitive Processes?
of the Deutsche Forschungs-gemeinschaft.
The work of Joakim Nivre is par-tially supported by the Swedish Research Council.5The number of unrestricted dependency trees on n nodesis given by Sequence A000169, the number of well-nesteddependency trees is given by Sequence A113882 in the On-Line Encyclopedia of Integer Sequences (Sloane, 2006).513ReferencesManuel Bodirsky, Marco Kuhlmann, and MathiasM?hl.
2005.
Well-nested drawings as models ofsyntactic structure.
In Tenth Conference on For-mal Grammar and Ninth Meeting on Mathematicsof Language.Alena B?hmov?, Jan Hajic?, Eva Hajic?ov?, and BarboraHladk?.
2003.
The Prague Dependency Treebank:A three-level annotation scenario.
In Anne Abeill?,editor, Treebanks: Building and Using Parsed Cor-pora, pages 103?127.
Kluwer Academic Publishers.Michael Collins, Jan Hajic?, Eric Brill, Lance Ramshaw,and Christoph Tillmann.
1999.
A statistical parserfor Czech.
In 37th Annual Meeting of the Associ-ation for Computational Linguistics (ACL), pages505?512.Denys Duchier and Ralph Debusmann.
2001.
Topo-logical dependency trees: A constraint-based ac-count of linear precedence.
In 39th Annual Meet-ing of the Association for Computational Linguistics(ACL), pages 180?187.Jason Eisner.
1996.
Three new probabilistic modelsfor dependency parsing: An exploration.
In 16thInternational Conference on Computational Linguis-tics (COLING), pages 340?345.G?lsen Eryig?it and Kemal Oflazer.
2006.
Statisticaldependency parsing of turkish.
In Eleventh Confer-ence of the European Chapter of the Association forComputational Linguistics (EACL).Haim Gaifman.
1965.
Dependency systems andphrase-structure systems.
Information and Control,8:304?337.Jan Hajic?, Barbora Vidova Hladka, Jarmila Panevov?,Eva Hajic?ov?, Petr Sgall, and Petr Pajas.
2001.Prague Dependency Treebank 1.0.
LDC, 2001T10.Keith Hall and Vaclav Nov?k.
2005.
Corrective mod-eling for non-projective dependency parsing.
InNinth International Workshop on Parsing Technolo-gies (IWPT).Richard Hudson.
2003.
An encyclopediaof English grammar and Word Grammar.http://www.phon.ucl.ac.uk/home/dick/enc/intro.htm,January.Sylvain Kahane, Alexis Nasr, and Owen Rambow.1998.
Pseudo-projectivity: A polynomially parsablenon-projective dependency grammar.
In 36th An-nual Meeting of the Association for ComputationalLinguistics and 18th International Conference onComputational Linguistics (COLING-ACL), pages646?652.Matthias Trautner Kromann.
2003.
The Danish De-pendency Treebank and the DTAG treebank tool.
InSecond Workshop on Treebanks and Linguistic The-ories (TLT), pages 217?220.Svetoslav Marinov and Joakim Nivre.
2005.
A data-driven parser for Bulgarian.
In Fourth Workshop onTreebanks and Linguistic Theories (TLT), pages 89?100.Ryan McDonald and Fernando Pereira.
2006.
On-line learning of approximate dependency parsing al-gorithms.
In Eleventh Conference of the EuropeanChapter of the Association for Computational Lin-guistics (EACL).Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic?.
2005.
Non-projective dependency pars-ing using spanning tree algorithms.
In 43rd AnnualMeeting of the Association for Computational Lin-guistics (ACL).Igor Mel?c?uk.
1988.
Dependency Syntax: Theory andPractice.
State University of New York Press, Al-bany, New York, USA.Joakim Nivre and Jens Nilsson.
2005.
Pseudo-projective dependency parsing.
In 43rd AnnualMeeting of the Association for Computational Lin-guistics (ACL), pages 99?106.Joakim Nivre.
2003.
An efficient algorithm for pro-jective dependency parsing.
In Eigth InternationalWorkshop on Parsing Technologies (IWPT), pages149?160.Joakim Nivre.
2006.
Constraints on non-projective de-pendency parsing.
In Eleventh Conference of theEuropean Chapter of the Association for Computa-tional Linguistics (EACL).T.
Obre?bski and F. Gralin?ski.
2004.
Some noteson generative capacity of dependency grammar.
InCOLING 2004 Workshop on Recent Advances in De-pendency Grammar Workshop on Recent Advancesin Dependency Grammar.Martin Pl?tek, Tom??
Holan, and Vladislav Kubon?.2001.
On relax-ability of word order by d-grammars.In Third International Conference on Discrete Math-ematics and Theoretical Computer Science.Giorgio Satta.
1992.
Recognition of linear context-free rewriting systems.
In 30th Meeting of the Asso-ciation for Computational Linguistics (ACL), pages89?95, Newark, Delaware, USA.Daniel Sleator and Davy Temperley.
1993.
ParsingEnglish with a link grammar.
In Third InternationalWorkshop on Parsing Technologies.Neil J.
A. Sloane.
2006.
The on-line encyclopediaof integer sequences.
Published electronically athttp://www.research.att.com/ njas/sequences/.Anssi Yli-Jyr?.
2003.
Multiplanarity ?
a model for de-pendency structures in treebanks.
In Second Work-shop on Treebanks and Linguistic Theories (TLT),pages 189?200.Daniel Zeman.
2004.
Parsing With a Statistical De-pendency Model.
Ph.D. thesis, Charles University,Prague, Czech Republic.514
