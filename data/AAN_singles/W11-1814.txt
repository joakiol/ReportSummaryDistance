Proceedings of BioNLP Shared Task 2011 Workshop, pages 94?101,Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational LinguisticsExtracting Bacteria Biotopes with Semi-supervised Named EntityRecognition and Coreference ResolutionNhung T. H. Nguyen and Yoshimasa TsuruokaSchool of Information ScienceJapan Advanced Institute of Science and Technology1-1 Asahidai, Nomi, Ishikawa 923-1292 Japan{nthnhung,tsuruoka}@jaist.ac.jpAbstractThis paper describes our event extraction sys-tem that participated in the bacteria biotopestask in BioNLP Shared Task 2011.
The sys-tem performs semi-supervised named entityrecognition by leveraging additional informa-tion derived from external resources includinga large amount of raw text.
We also performcoreference resolution to deal with events hav-ing a large textual scope, which may span overseveral sentences (or even paragraphs).
Tocreate the training data for coreference resolu-tion, we have manually annotated the corpuswith coreference links.
The overall F-score ofevent extraction was 33.2 at the official eval-uation of the shared task, but it has been im-proved to 33.8 thanks to the refinement madeafter the submission deadline.1 IntroductionIn this paper, we present a machine learning-basedapproach for bacteria biotopes extraction of theBioNLP Shared Task 2011 (Bossy et al , 2011).The task consists of extracting bacteria localizationevents, namely, mentions of given species and theplace where it lives.
Places related to bacteria lo-calization events range from plant or animal hostsfor pathogenic or symbiotic bacteria to natural envi-ronments like soil or water1.
This task also targetsspecific environments of interest such as medical en-vironments (hospitals, surgery devices, etc.
), pro-cessed food (dairy) and geographical localizations.1https://sites.google.com/site/bionlpst/home/bacteria-biotopesThe task of extracting bacteria biotopes involvestwo steps: Named Entity Recognition (NER) andevent detection.
The current dominant approach toNER problems is to use supervised machine learningmodels such as Maximum Entropy Markov Models(MEMMs), Support Vector Machines (SVMs) andConditional Random Fields (CRFs).
These modelshave been shown to work reasonably well when alarge amount of training data is available (Nadeauand Sekine, 2007).
However, because the anno-tated corpus delivered for this particular subtask inthe shared task is very small (78 documents with1754 sentences), we have decided to use a semi-supervised learning method in our system.
Our NERmodule uses a CRF model with enhanced featurescreated from external resources.
More specifically,we use additional features created from the outputof HMM clustering performed on a large amount ofraw text, and word senses from WordNet for tag-ging.The target events in this shared task are dividedinto two types.
The first is Localization eventswhich relates a bacterium to the place where it lives.The second is PartOf events which denotes an or-gan that belongs to an organism.
As in Bossy etal.
(2010), the largest possible scope of the men-tion of a relation is the whole document, and thusit may span over several sentences (or even para-graphs).
This observation motivated us to performcoreference resolution as a pre-processing step, sothat each event can be recognized within a narrowertextual scope.
There are two common approaches tocoreference resolution: one mainly relies on heuris-tics, and the other employs machine learning.
Some94instances of the heuristics-based approach are de-scribed in (Harabagiu et al, 2001; Markert andNissim, 2005; Yang and Su, 2007), where theyuse lexical and encyclopedic knowledge.
Machinelearning-based methods (Soon and Ng, 2001; Ngand Cardie, 2002; Yang et al , 2003; Luo et al, 2004; Daume and Marcu, 2005) train a classi-fier or search model using a corpus annotated withanaphoric pairs.
In our system, we employ the sim-ple supervised method presented in Soon and Ng(2001).
To create the training data, we have man-ually annotated the corpus with coreference infor-mation about bacteria.Our approach, consequently, has three processes:NER, coreference resolution of bacterium entities,and event extraction.
The latter two processes can beformulated as classification problems.
Coreferenceresolution is to determine the relation between can-didate noun phrases and bacterium entities, and theevent extraction is to detect the relation between twoentities.
It should be noted that our official submis-sion in the shared task was carried out without usinga coreference resolution module, and the system hasbeen improved after the submission deadline.Our contribution in this paper is two-fold.
In themethodology aspect, we use an unsupervised learn-ing method to create additional features for the CRFmodel and perform coreference resolution to narrowthe scope of events.
In the resource aspect, the man-ual annotations for training our coreference resolu-tion module will be made available to the researchcommunity.The remainder of this paper is organized as fol-lowed.
Section 2, 3 and 4 describe details about theimplementation of our system.
Section 5 presentsthe experimental results with some error analysis.Finally, we conclude our approach and discuss fu-ture work in section 6.2 Semi-supervised NERAccording to the task description, the NER taskconsists of detecting the phrases that denote bacte-rial taxon names and localizations which are bro-ken into eight types: Host, HostPart, Geographical,Food, Water, Soil, Medical and Environment.
Inthis work, we use a CRF model to perform NER.CFRs (Lafferty et.
al., 2001) are a sequence model-ing framework that not only has all the advantagesof MEMMs but also solves the label bias problemin a principled way.
This model is suitable for la-beling sequence data, especially for NER.
Based onthis model, our CRF tagger is trained with a stochas-tic gradient descent-based method described in Tsu-ruoka et al (2009), which can produce a compactand accurate model.Due to the small size of the training corpus andthe complexity of their category, the entities cannotbe easily recognized by standard supervised learn-ing.
Therefore, we enhance our learning model byincorporating related information from other exter-nal resources.
On top of the lexical and syntacticfeatures, we use two additional types of information,which are expected to alleviate the data sparsenessproblem.
In summary, we use four types of featuresincluding lexical and syntactic features, word clus-ter and word sense features as the input for the CRFmodel.2.1 Word cluster featuresThe idea of enhancing a supervised learning modelwith word cluster information is not new.
Kamazaet.
al.
(2001) use a hidden Markov model (HMM)to produce word cluster features for their maximumentropy model for part-of-speech tagging.
Koo et al(2008) implement the Brown clustering algorithmto produce additional features for their dependencyparser.
For our NER task, we use an HMM to pro-duce word cluster features for our CRF model.We employed an open source library2 for learn-ing HMMs with the online Expectation Maximiza-tion (EM) algorithm proposed by Liang and Klein(2009).
The online EM algorithm is much more ef-ficient than the standard batch EM algorithm and al-lows us to use a large amount of data.
For each hid-den state, words that are produced by this state withthe highest probability are written.
We use this resultof word clustering as a feature for NER.
The optimalnumber of hidden states is selected by evaluating itseffectiveness on NER using the development set.To prepare the raw text for HMM clustering, wedownloaded 686 documents (consisting of both fulldocuments and abstracts) about bacteria biotopes2http://www-tsujii.is.s.u-tokyo.ac.jp/?hillbig/ohmm.htm95Figure 1: Sample of HMM clustering result.from MicrobeWiki, JGI Genome Portal, Genoscope,2Can bacteria pages at EBI and NCBI GenomeProject (the training corpus is also downloaded fromthese five webpages).
In addition, we use the100,000 latest MEDLINE abstracts containing thestring ?bacteri?
in our clustering.
In total, the rawtext consists of more than 100,000 documents withmore than 2 million sentences.A part of the result of HMM clustering is shownin Figure 1.
According to this result, the word ?Bi-fidobacterium?
belongs to cluster number 9, and itsfeature value is ?Cluster-9?.
The word cluster fea-tures of the other words are extracted in the sameway.2.2 Word sense featuresWe used WordNet to produce additional features onword senses.
Although WordNet3 is a large lexi-cal database, it only comprises words in the generalgenre, to which only the localization entities belong.Since it does not contain the bacterial taxon names,the most important entities in this task, we used an-other dictionary for bacteria names.
The dictionarywas extracted from the genomic BLAST page ofNCBI 4.
To connect these two resources, we simplyplace all entries from the NCBI dictionary under the?bacterium?
sense of WordNet.
Table 1 illustratessome word sense features employed in our model.2.3 Pre-processing for bacteria namesIn biomedical documents, the bacteria taxon namesare written in many forms.
For example, they are3http://wordnet.princeton.edu/4http://www.ncbi.nlm.nih.gov/sutils/genom_table.cgiWord POS Sensechromosome NN bodycolonize VBP socialdetected VBN perceptionfly NN animalgastrointestinal JJ pertinfant NN personlongum FW bacteriummaintaining VBG stativemilk NN foodonion NN plantproterins NNS substanceUSA NNP locationTable 1: Sample of word sense features given by Word-Net and NCBI dictionary.presented in a full name like ?Bacillius cereus?, orin a short form such as ?B.
cereus?, or even in an ab-breviation as ?GSB?
(green sulfur bacteria).
More-over, the bacteria names are often modified withsome common strings such as ?strain?, ?spp.
?, ?sp.?,etc.
?Borrelia hermsii strain DAH?, ?Bradyrhizo-bium sp.
BTAi1?, and ?Spirochaeta spp.?
are ex-amples of this kind.
In order to tackle this prob-lem, we apply a pre-processing step before NER.
Al-though there are many previous studies solving thiskind of problem, in our system, we apply a simplemethod for this step.?
Retrieving the full form of bacteria names.
Weassume that (a) both short form and full formmust occur in the same document; (b) a tokenis considered as an abbreviation if it is writ-ten in upper case and its length is shorter than4 characters.
When a token satisfies condition(b) (which means it is an abbreviation), the pro-cessing retrieves its full form by identifying allsequences containing tokens initialized by itsabbreviated character.
In case of short formlike ?B.
cereus?, the selected sequence must in-clude the right token (which is ?cereus?
in ?B.cereus?).?
Making some common strings transparent.
Asour observation on the training data, there are8 common strings in bacteria names, including?strain?, ?str?, ?str.
?, ?subsp?, ?spp.
?, ?spp?,?sp.
?, ?sp?.
All of these strings will be removedbefore NER and recovered after that.963 Coreference Resolution as BinaryClassificationCoreference resolution is the process of determin-ing whether different nominal phrases are used torefer to the same real world entity or concept.
Ourapproach basically follows the learning method de-scribed in Soon and Ng (2001).
In this approach,we build a binary classifier using the coreferring en-tities in the training corpus.
The classifier takes apair of candidates and returns true if they refer tothe same real world entity and false otherwise.
Inthis paper, we limit our module to detecting the bac-teria?s coreference, and hence the candidates consistof noun phrases (NPs) (starting by a determiner),pronouns, possessive adjective and name of bacte-ria.In addition to producing the candidates, the pre-processing step creates a set of features for eachanaphoric pair.
These features are used by the clas-sifier to determine if two candidates have a corefer-ence relation or not.The following features are extracted from eachcandidate pair.?
Pronoun: 1 if one of the candidates is a pro-noun; 0 otherwise.?
Exact or Partial Match: 1 if the two strings ofthe candidates are identical, 2 if they are partialmatching; 0 otherwise.?
Definite Noun Phrase: 1 if one of the candi-dates is a definite noun phrases; 0 otherwise.?
Demonstrative Noun Phrase: 1 if one of thecandidates is a demonstrative noun phrase; 0otherwise.?
Number Agreement: 1 if both candidates aresingular or plural; 0 otherwise.?
Proper Name: 1 if both candidates are bac-terium entities or proper names; 0 otherwise.?
Character Distance: count the number of thecharacters between two candidates.?
Possessive Adjective: 1 if one of the candidatesis possessive adjective; 0 otherwise.Figure 2: Example of annotating coreference resolution.T16 is a bacterium which is delivered in *.a2 file, T24and T25 are anaphoric expressions.
There are two coref-erence relations of T16 and T24, T16 and T25.?
Exist in Coreference Dictionary: 1 if the candi-date exists in the dictionary extracted from thetraining data; 0 otherwise.
This feature aims toremove noun phrases which are unlikely to berelated to the bacterium entities.The first five features are exactly the same as thosein Soon and Ng (2001), while the others are refinedor added to make it suitable for our specific task.In the testing phase, we used the best-firstclustering as in Ng and Cardie (2002).
Ratherthan performing a right-to-left search from eachanaphoric NP for the first coreferent NP, a right-to-left search for a highly likely antecedent was per-formed.
Hence, the classifier was modified to selectthe antecedent of NP with the coreference likelihoodscore above a threshold.
This threshold was tuned byevaluating it on the development set.3.1 Corpus annotationTo create the training data for coreference resolu-tion, we have manually annotated the corpus basedon the gold-standard named entity annotations deliv-ered by the organizer.
Due to our decision to focuson bacteria names, only the coreference of these en-tities are labeled.
We use a format similar to those ofthe organizer, i.e.
the standoff presentation and text-bound annotations.
The coreference annotation fileconsists of two parts, one part for anaphoric expres-sions and the other for coreference relation.
Figure 2shows an example of a coreference annotation withthe original text.974 Event ExtractionThe bacteria biotopes, as mentioned earlier, are di-vided into two types.
The first type of events,namely localization events, relates a bacterium tothe place where it lives, and has two mandatory ar-guments: a Bacterium type and a localization type.The second type of events, i.e.
PartOf events, de-note an organ that belongs to an organism, and hastwo mandatory arguments of type HostPart and Hostrespectively.
We view this step as determining therelationship between two specific entities.
Becauseof no ambiguity between the two types of event, theevent extraction can be solved as the binary classifi-cation of pairs of entities.
The classifier is trained onthe training data with four types of feature extractedfrom the context between two entities: distance insentences, the number of entities, the nearest left andright verbs.Generating Training Examples.
Given thecoreference information on bacterium entities, thesystem considers all the entities belonging to thecoreference chains as real bacteria and generatesevent instances.
Since about 96% of all annotatedevents occur in the same paragraph, we restrict ourmethod to detecting events within one paragraph.?
Localization Event.
The system creates a rela-tionship between a bacterium and a localizationentity with minimum distance between themby the following priorities:(1) The bacterium precedes the localization en-tity in the same sentence.
(2) The bacterium precedes the localization en-tity in the same paragraph.?
PartOf Event.
All possible relationships be-tween Host and HostPart entities are generatedif they are in the same paragraph.5 Experiments and DiscussionThe training and evaluation data used in these exper-iments are provided by the shared task organizers.The token and syntactic information are extractedfrom the supporting resources (Stenetorp et.
al.
,2011).
More detail, the tokenized text was done byGENIA tools, and the syntactic analyses was cre-ated by the McClosky-Charinak parser (McCloskyExperiment Acc.
Pre.
Re.
F-scoreBaseline 94.28 76.32 35.51 48.47Word cluster 94.46 78.23 39.59 52.57Word sense 94.63 74.15 44.49 55.61All Features 94.70 77.62 45.31 57.22Table 2: Performance of Named Entity Recognition interms of Accuracy, Precision, Recall and F-score withdifferent features on the development set.and Charniak, 2008), trained on the GENIA Tree-bank corpus (Tateisi et al, 2005), which is one of themost accurate parsers for biomedical documents.For both classification of anaphoric pairs in coref-erence resolution and determining relationship oftwo entites, we used the SVMlight library 5, a state-of-the-art classifier, with the linear kernel.In order to find the best parameters and featuresfor our final system, we conducted a series of exper-iments at each step of the approach.5.1 Named Entity RecognitionWe evaluated the impact of additional featues onNER by running four experiments.
The Baseline ex-periment was conducted by using the original CRFtagger, which did not use any additional features de-rived from external resources.
The other three ex-periments were conducted by incrementally addingmore features to the CRF tagger.
Table 2 shows theresults on the development set6.Through these experiments we have realized thatusing the external resources is very effective.
Theword cluster and word sense features are used likea dictionary.
The first one can be considered as thedictionary of specific classes of entity in the samedomain with this task, which mainly supports theprecision, whereas the latter is a general dictionaryboosting the recall.
With regard to F-score, the wordsense features outperform the word cluster features.When we combine all of them, the F-score is im-proved significantly by nearly 9 points.The detailed results of individual classes in Ta-ble 3 show that the Environment entities are thehardest to recognize.
Because of their general char-acteristic, these entities are often confused with Host5http://svmlight.joachims.org/6These scores were generated by using the CoNLL 2000evaluation script.98Class Gold Pre.
Re.
F-scoreBacterium 86 70.00 40.23 51.09Host 78 78.57 56.41 65.67HostPart 44 91.67 50.00 64.71Geographical 8 71.43 62.50 66.67Environment 8 0.00 0.00 0.00Food 0 N/A N/A N/AMedical 2 100.00 50.00 66.67Water 17 100.00 17.65 30.00Soil 1 100.00 100.00 100.00All 244 77.62 45.31 57.22Table 3: Results of NER using all features on the de-velopment set.
The ?Gold?
column shows the numberof entities of that class in the gold-standard corpus.
Thescore of Food entities is not available because there is nopositive instance in the development set.Detection LinkingPrecision 24.18 20.48Recall 91.36 33.71F-score 38.24 25.48Table 4: Result of coreference resolution on the develop-ment set achieved with gold-standard named entity anno-tations.or Water.
In contrast, the Geographical category iseasier than the others if we have gazetteers and ad-ministrative name lists.5.2 Coreference ResolutionWe next evaluated the accuracy of coreference reso-lution for bacterium entities.
The evaluation7 is car-ried out in two steps: evaluation of mention detec-tion, and evaluation of mention linking to producecoreference links.
The exact matching criterion wasused when evaluating the accuracy of the two steps.Table 4 shows the performance of the coreferenceresolution module when taking annotated entites asinput.
As mentioned in section 3, the first step of thismodule considers all NPs beginning with a deter-miner and bacterium entities as candidates.
There-fore, the number of the candidate NPs is vastly largerthan that of the positive ones.
This is the reasonwhy the precision of mention detection is low, whilethe recall is high.
This high recall leads to a largenumber of generated linkings and raises the com-7http://sites.google.com/site/bionlpst/home/protein-gene-coreference-taskExperiment Pre.
Re.
F-scoreNo Coref.
42.11 27.34 33.15With Coref.
43.40 27.64 33.77Table 5: Comparative results of event extraction with andwithout coreference information on the test set.Type of eventNum.
of addition Num.
of ruled outTrue False True FalseLocalization 17 1 6 20PartOf 6 5 1 0Total 29 27Table 6: Contribution of coreference resolution to eventextraction.plexity of linking detection.
In order to obtain moreaccurate results, we had to remove weak linkingswhose classification score is under 0.7 (this is thebest threshold on the development set).
However, asshown in Table 4, the performance of mention link-ing was not satisfactory.5.3 Event ExtractionFinally, we carried out two experiments on the testset to investigate the effect of coreference resolutionon event extraction.
The results shown in Table 5 in-dicate that the contribution of coreference resolutionin this particular experiment is not significant.
Thecoreference information helps the module to add 29more events (23 true and 6 false events) and rule out27 events (20 false and 7 true events) compared withthe experiment with no coreference resolution.
De-tail about this contribution is presented in Table 6.We further analyzed the result of event extractionand found that there exist two kinds of Localizationevents, which we call direct and indirect events.
Thedirect events are the ones that are easily recogniz-able on the surface level of textual expressions.
Thethree Localization events in Figure 3 belong to thistype.
Our module is able to detect most of the di-rect events, especially when we have the coreferenceinformation on bacteria ?
it is straight-forward be-cause the two arguments of the event occur in thesame sentence.
In constrast, the indirect eventsare more complicated.
They appear implicitly in thedocument and we need to infer them through an in-termediate agent.
For example, a bacterium causesa disease, and this disease infects the humans or an-99Figure 3: Example of direct events.
The solid line is theLocalization event, the dash line is the PartOf event.Figure 4: Example of indirect events.
The solid line isthe Localization event, the arrow shows the causative re-lation.imals.
Therefore, it can be considered that the bac-terium locates in the humans or animals.
Figure 4illustrates this case.
In this example, the Bacillusanthracis causes Anthrax, Humans contract the dis-ease (which refers to Anthrax), and the Bacillus an-thracis locates in Humans.
These events are verydifficult to recognize since, in this context, we donot have any information about the disease.
Eventsof this type provide an interesting challenge for bac-teria biotopes extraction.6 Conclusion and Future WorkWe have presented our machine learning-based ap-proach for extracting bacteria biotopes.
The systemis implemented with modules for three tasks: NER,coreference resolution and event extraction.For NER, we used a CRF tagger with four typesof features: lexical and syntactic features, the wordcluster and word sense extracted from the externalresources.
Although we achieved a significant im-provement by employing WordNet and the HMMclustering on raw text, there is still much room forimprovement.
For example, because all extractedknowledge used in this NER module belongs to thegeneral knowlegde, its performance is not as good asour expectation.
We envisage that the performanceof the module will be improved if we can find usefulbiological features.We have attempted to use the information ob-tained from the coreference resolution of bacteria tonarrow the event?s scope.
On the test set, although itdoes not improve the system significantly, the coref-erence information has shown to be useful in eventextraction.
8In this work, we simply used binary classifierswith standard features for both coreference resolu-tion and event detection.
More advanced machinelearning approaches for structured prediction maylead to better performance, but we leave it for futurework.ReferencesRobert Bossy, Claire Nedellec, and Julien Jourde.
2010.Guidelines for Annotation of Bacteria Biotopes.Robert Bossy, Julien Jourde, Philippe Bessie`res, Marteenvan de Guchte, and Claire Ne?dellec.
2011.
BioNLPShared Task 2011 - Bacteria Biotope, In Proceedingsof the BioNLP 2011 Workshop Companion Volume forShared Task.
Portland, Oregon, Association for Com-putational Linguistics.Hal Daume?
III and Daniel Marcu.
2005.
A Large-scaleExploration of Effective Global Features for a JointEntity Detection and Tracking Model.
In Proceedingsof HLT-EMNLP 2005, pp.
97-104.Sanda M. Harabagiu, Razvan C. Bunescu and Steven J.Maiorano.
2001.
Text and Knowlegde Mining for Co-reference Resolution.
In Proceedings of NAACL 2001,pp.
1-8.Jun?ichi Kazama, Yusuke Miyao, and Jun?ichi Tsujii.2001.
A Maximum Entropy Tagger with Unsuper-vised Hidden Markov Models.
In Proceedings of NL-PRS 2001, pp.
333-340.Terry Koo, Xavier Carreras, and Michael Collins.
2008.Simple Semi-supervised Dependency Parsing.
In Pro-ceedings of ACL-08: HLT, pp.
595-603.John Lafferty, Andrew McCallum and Fernando Pereira.2001.
Conditional Random Fields: Probabilistic Mod-els for Segmenting and Labeling Sequence Data.
InProceedings of ICML?01, pp.
282-289.Percy Liang and Dan Klein.
2009.
Online EM for Unsu-pervised Models.
In Proceedings of NAACL 2009, pp.611-619.Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, NandaKambhatla and Salim Roukos.
2004.
AMention-Synchronous Co-reference Resolution Algo-rithm based on the Bell Tree.
In Proceedings of ACL2004, pp.
135-142.Katja Markert and Malvina Nissim.
2005.
ComparingKnowledge Sources for Nominal Anaphora Resolu-tion.
In Computational Linguistics, Volume 31 Issue3, pp.
367-402.8If you are interesting in the annotated corpus used for ourcoreference resolution model, please request us by email.100David McClosky and Eugene Charniak.
2008.
Self-Training for Biomedical Parsing.
Proceedings of theAssociation for Computational Linguistics (ACL 2008,short papers), Columbus, Ohio, pp.
101-104.David Nadeau and Satoshi Sekine.
2007.
A survey ofnamed entity recognition and classification.
Linguisti-cae Investigationes, Volume 30(1), pp.
326.Vincent Ng and Claire Cardie.
2002.
Improving Ma-chine Learning Approach to Co-reference Resolution.In Proceedings of ACL 2002, pp.
104-111.Wee Meng Soon and Hwee Tou Ng.
2001.
A Ma-chine Learning Approach to Co-reference Resolutionof Noun Phrases.
Computational Linguistics 2001,Volume 27 Issue 4, pp.
521-544.Pontus Stenetorp, Goran Topic?, Sampo Pyysalo, TomokoOhta, Jin-Dong Kim, and Jun?ichi Tsujii.
2011.BioNLP Shared Task 2011: Supporting Resources.InProceedings of the BioNLP 2011 Workshop Com-panion Volume for Shared Task, Portland, Oregon, As-sociation for Computational Linguistics.Yuka Tateisi, Akane Yakushiji, Tomoko Ohta and JunichiTsujii.
2005.
Syntax Annotation for the GENIA cor-pus.
In Proceedings of IJCNLP 2005 (Companion vol-ume), pp.
222-227.Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ana-niadou.
2009.
Stochastic Gradient Descent Trainingfor L1-regularized Log-linear Models with CumulativePenalty.
In Proceedings of ACL-IJCNLP, pp.
477-485.Xiaofeng Yang, Guodong Zhou, Jian Su and Chew LimTan.
2003.
Co-reference Resolution using Competi-tion Learning Approach.
In Proceedings of ACL 2003,pp.
176-183.Xiaofeng Yang and Jian Su.
2007.
Coreference Reso-lution Using Semantic Relatedness Information fromAutomatically Discovered Patterns.
In Proceedings ofACL 2007, pp.
528-535.101
