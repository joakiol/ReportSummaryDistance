First Joint Conference on Lexical and Computational Semantics (*SEM), pages 288?293,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsUCM-2: a Rule-Based Approach to Infer the Scope of Negation viaDependency ParsingMiguel Ballesteros, Alberto D?
?az, Virginia Francisco,Pablo Gerva?s, Jorge Carrillo de Albornoz and Laura PlazaNatural Interaction Based on Language GroupComplutense University of MadridSpain{miballes, albertodiaz, virginia}@fdi.ucm.es,pgervas@sip.ucm.es, {jcalbornoz, lplazam}@fdi.ucm.esAbstractUCM-2 infers the words that are affected bynegations by browsing dependency syntacticstructures.
It first makes use of an algo-rithm that detects negation cues, like no, notor nothing, and the words affected by themby traversing Minipar dependency structures.Second, the scope of these negation cues iscomputed by using a post-processing rule-based approach that takes into account the in-formation provided by the first algorithm andsimple linguistic clause boundaries.
An initialversion of the system was developed to handlethe annotations of the Bioscope corpus.
Forthe present version, we have changed, omittedor extended the rules and the lexicon of cues(allowing prefix and suffix negation cues, suchas impossible or meaningless), to make it suit-able for the present task.1 IntroductionOne of the challenges of the *SEM Shared Task(Morante and Blanco, 2012) is to infer and classifythe scope and event associated to negations, givena training and a development corpus based on Co-nan Doyle stories (Morante and Daelemans, 2012).Negation, simple in concept, is a complex but essen-tial phenomenon in any language.
It turns an affir-mative statement into a negative one, changing themeaning completely.
We believe therefore that be-ing able to handle and classify negations we wouldbe able to improve several text mining applications.Previous to this Shared Task, we can find severalsystems that handle the scope of negation in the stateof the art.
This is a complex problem, because it re-quires, first, to find and capture the negation cues,and second, based on either syntactic or semanticrepresentations, to identify the words that are di-rectly (or indirectly) affected by these negation cues.One of the main works that started this trend in natu-ral language processing was published by Morante?steam (2008; 2009), in which they presented a ma-chine learning approach for the biomedical domainevaluating it on the Bioscope corpus.In 2010, a Workshop on Negation and Spec-ulation in Natural Language Processing (Moranteand Sporleder, 2010) was held in Uppsala, Swe-den.
Most of the approaches presented worked inthe biomedical domain, which is the most studied innegation detection.The system presented in this paper is a modifica-tion of the one published in Ballesteros et al (2012).This system was developed in order to replicate (asfar as possible) the annotations given in the Bio-scope corpus (Vincze et al, 2008).
Therefore, forthe one presented in the task we needed to modifymost of the rules to make it able to handle the morecomplex negation structures in the Conan Doyle cor-pus and the new challenges that it represents.
Thepresent paper has the intention of exemplifying theproblems of such a system when the task is changed.Our system presented to the Shared Task is basedon the following properties: it makes use of an algo-rithm that traverses dependency structures, it classi-fies the scope of the negations by using a rule-basedapproach that studies linguistic clause boundariesand the outcomes of the algorithm for traversingdependency structures, it applies naive and simple288solutions to the problem of classifying the negatedevent and it does not use the syntactic annotationprovided in the Conan Doyle corpus (just in an ex-ception for the negated event annotation).In Section 2 we describe the algorithms that wepropose for inferring the scope of negation and themodifications that we needed to make to the previ-ous version.
In Section 3 we discuss the evaluationperformed with the blind test set and developmentset and the error analysis over the development set.Finally, in Section 4 we give our conclusions andsuggestions for future work.2 MethodologyOur system consists of two algorithms: the first oneis capable of inferring words affected by the negativeoperators (cues) by traversing dependency trees andthe second one is capable of annotating sentenceswithin the scope of negations.
This second algo-rithm is the one in which we change the behaviour ina deeper way.
The first one just serves as a consult-ing point in some of the rules of the second one.
Byusing the training set and development set providedto the authors we modified, omitted or changed theold rules when necessary.The first algorithm which traverses a dependencytree searching for negation cues to determine thewords affected by negations, was firstly applied (atan earlier stage) to a very different domain (Balles-teros et al, 2010) obtaining interesting results.
Atthat time, the Minipar parser (Lin, 1998) was se-lected to solve the problem in a simple way with-out needing to carry out several machine learningoptimizations which are well known to be dauntingtasks.
We also selected Minipar because at that mo-ment we only needed unlabelled parsing.Therefore, our system consists of three differentmodules: a static negation cue lexicon, an algorithmthat from a parse given by Minipar and the nega-tion cue lexicon produces a set of words affectedby the negations, and a rule-based system that pro-duces the annotation of the scope of the studied sen-tence.
These components are described in the fol-lowing sections.In order to annotate the sentence as it is done inthe Conan Doyle corpus, we also developed a post-processing system that makes use of the outcomesof the initial system and produces the expected out-put.
Besides this, we also generate a very naive rule-based approach to handle the problem of annotatingthe negated event.It is worth to mention that we did not makeuse of the syntactic annotation provided in the Co-nan Doyle corpus, our input is the plain text sen-tence.
Therefore, the system could work without thecolumns that are included in the annotation, just withthe word forms.
We only make use of the annota-tion when we annotate the negated event, checkingthe part-of-speech tag to ascertain whether the cor-responding word is a verb or not.
The system couldwork without these columns but only the results ofthe negated event would be affected.2.1 Negation Cue LexiconThe lexicon containing the negation cues is static.
Itcan be extended indefinitely but it has the restrictionthat it does not learn and it does not grow automat-ically when applying it to a different domain.
Thelexicon used in the previous system (Ballesteros etal., 2012) was also static but it was very small com-pared to the one employed by the present system,just containing less than 20 different negation cues.Therefore, in addition to the previous lexicon, weanalysed the training set and development sets andextracted 153 different negation cues (plus the onesalready present in the previous system).
We storedthese cues in a file that feeds the system when itstarts.
Table 1 shows a small excerpt of the lexicon.not no neither..norunnecessary unoccupied unpleasantunpractical unsafe unseenunshaven windless withoutTable 1: Excerpt of the lexicon2.2 Affected Wordforms Detection AlgorithmThe algorithm that uses the outcomes of Minipar isthe same employed in (Ballesteros et al, 2012) with-out modifications.
It basically traverses the depen-dency structures and returns for each negation cue aset of words affected by the cue.The algorithm takes into account the way of han-dling main verbs by Minipar, in which these verbs289appear as heads and the auxiliary verbs are depen-dants of them.
Therefore, the system first detects thenodes that contain a word which is a negation cue,and afterwards it does the following:?
If the negation cue is a verb, such as lack, it ismarked as a negation cue.?
If the negation cue is not a verb, the algorithmmarks the main verb (if it exists) that governsthe structure as a negation cue.For the rest of nodes, if a node depends directlyon any of the ones previously marked as negationcue, the system marks it as affected.
The negation isalso propagated until finding leaves, so wordformsthat are not directly related to the cues are detectedtoo.Finally, by using all of the above, the algorithmgenerates a list of words affected by each negationcue.2.3 Scope Classification AlgorithmThis second algorithm is the one that has sufferedthe deepest modifications from the first version.
Theprevious version handled the annotation as it is donein the Bioscope corpus.
The algorithm works as fol-lows:?
The system opens a scope when it finds a newnegation cue detected by the affected word-forms detection algorithm.
In Bioscope, onlythe sentences in passive voice include the sub-ject inside the scope.
However, the ConanDoyle corpus does not contain this exceptionalways including the subject in the scope whenit exists.
Therefore, we modified the decisionthat fires this rule, and we apply the way of an-notating sentences in passive voice for all thenegation cues, either passive or active voicesentences.Therefore, for most of the negation cues thesystem goes backward and opens the scopewhen it finds the subject involved or a markerthat indicates another statement, like a comma.There are some exceptions to this, such asscopes in which the cue is without or nei-ther...nor.
For them the system just opens thescope at the cue.?
The system closes a scope when there are nomore wordforms to be added, i.e.:?
It finds words that indicate another state-ment, such as but or because.?
No more words in the output of the firstalgorithm.?
End of the sentence.?
We also added a new rule that can handle thenegation cues that are prefix or suffix of anotherword, such as meaning-less: if the system findsa cue word like this, it then annotates the suffixor prefix as the cue (such as less) and the rest ofthe word as part of the scope.
Note that the Af-fected Wordforms Detection algorithm detectsthe whole word as a cue word.2.4 Negated Event HandlingIn order to come up with a solution that could pro-vide at least some results in the negated event han-dling, we decided to do the following:?
When the cue word contains a negative prefixor a negative suffix, we annotate the word asthe negated event.?
When the cue word is either not or n?t and thenext word is a verb, according to the part-of-speech annotation of the Conan Doyle corpus,we annotate the verb as the negated event.2.5 Post-Processing StepThe post-processing step basically processes the an-notated sentence with Bioscope style, (we showan example for clarification: <scope>There is<cue>no</cue> problem</scope>).
It tokenizesthe sentences, in which each token is a word or awordform, after that, it does the following:?
If the token contains the string <scope>, thesystem just starts a new scope column reserv-ing three new columns and it puts the word inthe first free ?scope?
column.
Because it meansthat there is a new scope for the present sen-tence.?
If the token is between a <cue> annotation, thesystem puts it in the corresponding free ?cue?column of the scope already opened.290?
If the token is annotated as ?negated event?, thesystem just puts the word in the last column ofthe scope already opened.Note that these three rules are not exclusive andcan be fired for the same token, but in this case theyare fired in the same order as they are presented.3 Results and DiscussionIn this section we first show the evaluation resultsand second the error analysis after studying the re-sults on the development set.3.1 ResultsIn this section we show the results obtained in twodifferent tables: Table 2 shows the results of the sys-tem with the test set, Table 3 shows the results of thesystem with the development set.As we can observe, the results for the develop-ment set are higher than the ones obtained for thetest set.
The reason is simple, we used the develop-ment set (apart from the training set) to modify therules and to make the system able to annotate thesentences of the test set.Note that our system only detects some of thenegation cues (around 72% F1 and 76% F1, respec-tively, for the test and development sets).
We there-fore believe that one of the main drawbacks of thepresent system is the static lexicon of cues.
In theprevious version, due to the simplicity of the task,this was not an issue.
However, it is worth notingthat once the negation is detected the results are notthat bad, we show a high precision in most of thetasks.
But the recall suffers due to the coverage ofthe lexicon.It is also worth noting that for the measure Scopetokens, which takes into account the tokens includedin the scope but not a full scope match, our systemprovides interesting outcomes (around 63% F1 and73% F1, respectively), showing that it is able to an-notate the tokens in a similar way.
We believe thatthis fact evidences that the present system comesfrom a different kind of annotation and a differentdomain, and the extension or modification of such asystem is a complex task.We can also observe that the negated events re-sults are very low (around 17.46% F1 and 22.53%F1, respectively), but this was expected because byusing our two rules we are only covering two casesand moreover, these two cases are not always behav-ing in the same way in the corpora.3.2 Error AnalysisIn this section we analyse the different errors of oursystem with respect to the development set.
This setcontains 787 sentences, of which 144 are negationsentences containing 168 scopes, 173 cues and 122negation events.With respect to the negation cue detection wehave obtained 58 false negatives (fn) and 16 falsepositives (fp).
These results are not directly derivedfrom the static lexicon of cues.
The main problem isrelated with the management of sentences with morethan one scope.
The majority of the errors have beenproduced because in some cases all the cues are as-signed to all the scopes detected in the same sen-tence, generating fp, and in other cases the cues ofthe second and subsequent scopes are ignored, gen-erating fn.
The first case occurs in sentences like(1), no and without are labelled as cues in the twoscopes.
The second case occurs in sentences like(2), where neither the second scope nor the secondcue are labelled.
In sentence (3) un is labelled ascue two times (unbrushed, unshaven) but within thesame scope, generating a fp in the first scope and afn in the second one.?
(1) But no [one can glance at your toilet and at-tire without [seeing that your disturbance datesfrom the moment of your waking ..
?]]?
(2) [You do ]n?t [mean] - .
[you do] n?t [meanthat I am suspected] ?
??
(3) Our client smoothed down [his] un[brushedhair] and felt [his] un[shaven chin].We also found false negatives that occur in multiword negation cues as by no means, no more andrather than.A different kind of false positives is related tomodality cues, dialogue elements and special cases(Morante and Blanco, 2012).
For example, no in (4),not in (5) and save in (6).?
(4) ?
You traced him through the telegram , no[doubt]., ?
said Holmes .291Test set gold system tp fp fn precision (%) recall (%) F1 (%)Cues: 264 235 170 39 94 81.34 64.39 71.88Scopes(cue match): 249 233 96 47 153 67.13 38.55 48.98Scopes(no cue match): 249 233 96 48 152 66.90 38.96 49.24Scope tokens(no cue match): 1805 2096 1222 874 583 58.30 67.70 62.65Negated(no cue match): 173 81 36 42 134 46.15 21.18 29.03Full negation: 264 235 29 39 235 42.65 10.98 17.46Table 2: Test set results.Development gold system tp fp fn precision (%) recall (%) F1 (%)Cues: 173 161 115 16 58 87.79 66.47 75.66Scopes(cue match): 168 160 70 17 98 80.46 41.67 54.90Scopes(no cue match): 168 160 70 17 98 80.46 41.67 54.90Scope tokens(no cue match): 1348 1423 1012 411 336 71.12 75.07 73.04Negated(no cue match): 122 71 35 31 82 53.03 29.91 38.25Full negation: 173 161 24 16 149 60.00 13.87 22.53Table 3: Development set results.?
(5) ?
All you desire is a plain statement , [is it]not ?
?.?
(6) Telegraphic inquiries ... that [Marx knew]nothing [of his customer save that he was agood payer] .We can also find problems with affixal negations,that is, bad separation of the affix and root of theword.
For example, in (7) dissatisfied was erro-neously divided in di- and ssatisfied.
Again, it isderived from the use of a static lexicon.?
(7) He said little about the case, but fromthat little we gathered that [he also was notdis[satisfied] at the course of events].Finally, we could also find cases that may be dueto annotation errors.
For example, incredible is notannotated as negation cue in (8).
The annotation ofthis cue we think is inconsistent, it appears 5 timesin the training corpus, 2 times is labelled as cue, but3 times is not.
According to the context in this sen-tence, incredible means not credible.?
(8) ?Have just had most incredible andgrotesque experience.With respect to the full scope detection, most ofthe problems are due again to the management ofsentences with more than one scope.
We have ob-tained 98 fn and 17 fp.
Most of the problems arerelated with affixal negations, as in (9), in which allthe words are included in the scope, which accord-ing to the gold standard is not correct.?
(9) [Our client looked down with a rueful faceat his own] un[conventional appearance].With respect to the scope tokens detection, theresults are higher, around 73% F1 in scope tokenscompared to 55% in full match scopes.
The reasonis because our system included tokens for the ma-jority of scopes, increasing the recall until 75% butlowering the precision due to the inclusion of morefp.4 Conclusions and Future WorkIn this paper we presented our participation in theSEM-Shared Task, with a modification of a rule-based system that was designed to be used in a dif-ferent domain.
As the main conclusion we could saythat modifying such a system to perform in a differ-ent type of texts is complicated.
However, takinginto account this fact, and the results obtained, weare tempted to say that our system presents compet-itive results.292We believe that the present system has a lot ofroom for improvement: (i) improve the manage-ment of sentences with more than one scope modify-ing the scope classification algorithm and the post-processing step, (ii) replacing the dependency parserwith a state-of-the-art parser in order to get higherperformance, or (iii) proposing a different way ofgetting a reliable lexicon of cues, by using a seman-tic approach that informs if the word has a negativemeaning in the context of the sentence.
Again, thiscould be achieved by using one of the parsers pre-sented in the ConLL 2008 Shared Task (Surdeanu etal., 2008).AcknowledgmentsThis research is funded by the Spanish Ministryof Education and Science (TIN2009-14659-C03-01Project).ReferencesMiguel Ballesteros, Rau?l Mart?
?n, and Bele?n D??az-Agudo.2010.
Jadaweb: A cbr system for cooking recipes.
InProceedings of the Computing Cooking Contest of theInternational Conference of Case-Based Reasoning.Miguel Ballesteros, Virginia Francisco, Alberto D?
?az,Jesu?s Herrera, and Pablo Gerva?s.
2012.
Inferring thescope of negation in biomedical documents.
In Pro-ceedings of the 13th International Conference on Intel-ligent Text Processing and Computational Linguistics(CICLING 2012), New Delhi.
Springer.Dekang Lin.
1998.
Dependency-based evaluation ofMINIPAR.
In Proceedings of the Workshop on theEvaluation of Parsing Systems, Granada.Roser Morante and Eduardo Blanco.
2012.
Sem 2012shared task: Resolving the scope and focus of nega-tion.
In Proceedings of the First Joint Conference onLexical and Computational Semantics (*SEM 2012),Montreal, Canada.Roser Morante and Walter Daelemans.
2009.
A met-alearning approach to processing the scope of nega-tion.
In Proceedings of the Thirteenth Conference onComputational Natural Language Learning, CoNLL?09, pages 21?29, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Roser Morante and Walter Daelemans.
2012.Conandoyle-neg: Annotation of negation in conandoyle stories.
In Proceedings of the Eighth Interna-tional Conference on Language Resources and Evalu-ation (LREC).
Istanbul, Turkey.Roser Morante and Caroline Sporleder, editors.
2010.Proceedings of the Workshop on Negation and Specu-lation in Natural Language Processing, Uppsala, Swe-den.Roser Morante, Anthony Liekens, and Walter Daele-mans.
2008.
Learning the scope of negation inbiomedical texts.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing,EMNLP ?08, pages 715?724, Stroudsburg, PA, USA.Association for Computational Linguistics.Mihai Surdeanu, Richard Johansson, Adam Meyers,Llu?
?s Ma`rquez, and Joakim Nivre.
2008.
The CoNLL-2008 shared task on joint parsing of syntactic and se-mantic dependencies.
In CoNLL 2008: Proceedings ofthe Twelfth Conference on Natural Language Learn-ing, pages 159?177, Manchester, United Kingdom.Veronika Vincze, Gyorgy Szarvas, Richard Farkas, Gy-orgy Mora, and Janos Csirik.
2008.
The Bio-Scope corpus: biomedical texts annotated for uncer-tainty, negation and their scopes.
BMC Bioinformat-ics, 9(Suppl 11):S9+.293
