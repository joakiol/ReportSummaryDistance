Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 122?126,Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational LinguisticsReconciling OntoNotes: Unrestricted Coreference Resolution inOntoNotes with ReconcileVeselin StoyanovCLSPJohns Hopkins UniversityBaltimore, MDUday Babbar and Pracheer Gupta and Claire CardieDepartment of Computer ScienceCornell UniversityIthaca, NYAbstractThis paper describes our entry to the 2011 CoNLLclosed task (Pradhan et al, 2011) on modeling un-restricted coreference in OntoNotes.
Our system isbased on the Reconcile coreference resolution re-search platform.
Reconcile is a general software in-frastructure for the development of learning-basednoun phrase (NP) coreference resolution systems.Our entry for the CoNLL closed task is a configura-tion of Reconcile intended to do well on OntoNotesdata.
This paper describes our configuration of Rec-oncile as well as the changes that we had to imple-ment to integrate with the OntoNotes task definitionand data formats.
We also present and discuss theperformance of our system under different testingconditions on a withheld validation set.1 IntroductionNoun phrase (NP) coreference resolution is one ofthe fundamental tasks of the field of Natural Lan-guage Processing (NLP).
Recently, the creation ofthe OntoNotes corpus (Pradhan et al, 2007) hasprovided researchers with a large standard datacollection with which to create and empiricallycompare coreference resolution systems.Reconcile (Stoyanov et al, 2010b) is a generalcoreference resolution research platform that aimsto abstract the architecture of different learning-based coreference systems and to provide infras-tructure for their quick implementation.
Recon-cile is distributed with several state-of-the art NLPcomponents and a set of optimized feature imple-mentations.
We decided to adapt Reconcile forthe OntoNotes corpus and enter it in the 2011CoNLL shared task with three goals in mind: (i) tocompare the architecture and components of Rec-oncile with other state-of-the-art coreference sys-tems, (ii) to implement and provide the capabil-ity of running Reconcile on the OntoNotes corpus,and, (iii) to provide a baseline for future algorithmimplementations in Reconcile that evaluate on theOntoNotes corpus.Although Reconcile can be easily adapted tonew corpora, doing so requires introducing newcomponents.
More precisely, the system has tobe modified to be consistent with the specific def-inition of the coreference task embodied in theOntoNotes annotation instructions.
Additionally,different corpora use different data formats, so thesystem needs to implement capabilities for dealingwith these new formats.
Finally, Reconcile can beconfigured with different features and componentsto create an instantiation that models well the par-ticular data.In this paper we describe, ReconcileCoNLL,our entry to the 2011 CoNLL shared task based onthe Reconcile research platform.
We begin by de-scribing the general Reconcile architecture (Sec-tion 2), then describe the changes that we incor-porated in order to enable Reconcile to work onOntoNotes data (Sections 3 and 4).
Finally, wedescribe our experimental set up and results fromrunning ReconcileCoNLL under different condi-tions (Section 5).2 Overview of ReconcileIn this section we give a high-level overview of theReconcile platform.
We refer the reader for moredetails to Stoyanov et al (2010a) and Stoyanov etal.
(2010b).
Results from running a Reconcile-based coreference resolution system on differentcorpora can be found in Stoyanov et al (2009).Reconcile was developed to be a coreferenceresolution research platform that allows for quickimplementation of coreference resolution systems.The platform abstracts the major processing steps(components) of current state-of-the-art learning-based coreference resolution systems.
A descrip-tion of the steps and the available components canbe found in the referenced papers.3 The ReconcileCoNLL SystemTo participate in the 2011 CoNLL shared task, weconfigured Reconcile to conform to the OntoNotesgeneral coreference resolution task.
We will usethe name ReconcileCoNLL, to refer to this par-ticular instantiation of the general Reconcile plat-form.
The remainder of this section describe thechanges required to enable ReconcileCoNLL torun (accurately) on OntoNotes data.122ReconcileCoNLL employs the same basicpipelined architecture as Reconcile.
We describethe specific components used in each step.1.
Preprocessing.
Documents in the OntoNotescorpus are manually (or semi-automatically) an-notated with many types of linguistic information.This information includes tokens, part-of-speechtags, and named entity information as well as aconstituent syntactic parse of the text.
For the pur-pose of participating in the shared task, we rely onthese manual annotations, when available.
Thus,we do not run most of the standard Reconcile pre-processing components.
One type of informationnot provided in the OntoNotes corpus is a depen-dency parse.
Several of Reconcile?s features relyon a dependency parse of the text.
Thus, we ranthe Stanford dependency parser (Klein and Man-ning, 2003), which performs a constituent parseand uses rules to convert to a dependency format.1Two additional changes to the preprocessingstep were necessary for running on the OntoNotesdata.
The first is the implementation of compo-nents that can convert data from the OntoNotesformat to the Reconcile internal format.
The sec-ond is adaptation of the Coreference Element (CE)extractor to conform to the OntoNotes definitionof what can constitute a CE.
Our implementationsfor these two tasks are briefly described in Sec-tions 4.1 and 4.2, respectively.2.
Feature generation.
ReconcileCoNLL wasconfigured with 61 features that have proven suc-cessful for coreference resolution on other datasets.
Due to the lack of time we performedno feature engineering or selection specific toOntoNotes.
We used a new component for gener-ating the pairwise CEs that comprise training andtest instances, which we dub SMARTPG (for smartpair generator).
This is described in Section 4.3.3.
Classification.
We train a linear classifier us-ing the averaged perceptron algorithm (Freund andSchapire, 1999).
We use a subset of 750 randomlyselected documents for training, since training onthe entire set required too much memory.2 As aresult, we had ample validation data for tuningthresholds, etc.1A better approach would be to use the rules to create thedependency parse from the manual constituent parse.
We de-cided against this approach due to implementation overhead.2It is easy to address the memory issue in the on-line per-ceptron setting, but in the interest of time we chose to reducethe size of the training data.
Training on the set of 750 docu-ments is done efficiently in memory by allocating 4GB to theJava virtual machine.4.
Clustering.
We use Reconcile?s single-linkclustering algorithm.
In other words, we computethe transitive closure of the positive pairwise pre-dictions.
Note that what constitutes a positive pre-diction depends on a threshold set for the classifierfrom the previous step.
This clustering thresholdis optimized using validation data.
More detailsabout the influence of the validation process canbe found in Section 5.5.
Scoring.
The 2011 CoNLL shared task pro-vides a scorer that computes a set of commonlyused coreference resolution evaluation metrics.We report results using this scorer in Section 5.However, we used the Reconcile-internal versionsof scorers to optimize the threshold.
This wasdone for pragmatic reasons ?
time pressure pre-vented us from incorporating the CoNLL scorer inthe system.
We also report the Reconcile-internalscores in the experiment section.This concludes the high-level description ofthe ReconcileCoNLL system.
Next, we describein more detail the main changes implemented toadapt to the OntoNotes data.4 Adapting to OntoNotesThe first two subsection below describe the twomain tasks that need to be addressed when runningReconcile on a new data set: annotation conver-sion and CE extraction.
The third subsection de-scribes the new Smart CE Pairwise instance gen-erator ?
a general component that can be used forany coreference data set.4.1 Annotation ConversionThere are fundamental differences between the an-notation format used by OntoNotes and that usedinternally by Reconcile.
While OntoNotes relieson token-based representations, Reconcile uses astand-off bytespan annotation.
A significant partof the development of ReconcileCoNLL was de-voted to conversion of the OntoNotes manual to-ken, parse, named-entity and coreference annota-tions.
In general, we prefer the stand-off bytespanformat because it allows the reference text of thedocument to remain unchanged while annotationlayers are added as needed.4.2 Coreference Element ExtractionThe definition of what can constitute an elementparticipating in the coreference relation (i.e., aCoreference Element or CE) depends on the par-ticular dataset.
Optimizing the CE extraction com-123Optimized Thres- B- CEAF MUCMetric hold CubedBCubed 0.4470 0.7112 0.1622 0.6094CEAF 0.4542 0.7054 0.1650 0.6141MUC 0.4578 0.7031 0.1638 0.6148Table 1: Reconcile-internal scores for differentthresholds.
The table lists the best threshold forthe validation data and results using that threshold.Pair Gen. BCubed CEAFe MUCSMARTPG 0.6993 0.1634 0.6126All Pairs 0.6990 0.1603 0.6095Table 3: Influence of different pair generators.ponent for the particular task definition can resultin dramatic improvements in performance.
An ac-curate implementation limits the number of ele-ments that the coreference system needs to con-sider while keeping the recall high.The CE extractor that we implemented forOntoNotes extends the existing Reconcile ACE05CE extractor (ACE05, 2005) via the followingmodifications:Named Entities: We exclude named entities oftype CARDINAL NUMBER, MONEY and NORP,the latter of which captures nationality, religion,political and other entities.Possessives: In the OntoNotes corpus, posses-sives are included as coreference elements, whilein ACE they are not.ReconcileCoNLL ignores the fact that verbs canalso be CEs for the OntoNotes coreference task asthis change would have constituted a significantimplementation effort.Overall, our CE extractor achieves recall of over96%, extracting roughly twice the number of CEsin the answer key (precision is about 50%).
Highrecall is desirable for the CE extractor at the cost ofprecision since the job of the coreference system isto further narrow down the set of anaphoric CEs.4.3 Smart Pair GeneratorLike most current coreference resolution systems,at the heart of Reconcile lies a pairwise classifier.The job of the classifier is to decide whether or nottwo CEs are coreferent or not.
We use the termpair generation to refer to the process of creatingthe CE pairs that the classifier considers.
The moststraightforward way of generating pairs is by enu-merating all possible unique combinations.
Thisapproach has two undesirable properties ?
it re-quires time in the order of O(n2) for a given doc-ument (where n is the number of CEs in the docu-ment) and it produces highly imbalanced data setswith the number of positive instances (i.e., coref-erent CEs) being a small fraction of the number ofnegative instances.
The latter issue has been ad-dressed by a technique named instance generation(Soon et al, 2001): during training, each CE ismatched with the first preceding CE with which itcorefers and all other CEs that reside in betweenthe two.
During testing, a CE is compared to allpreceding CEs until a coreferent CE is found orthe beginning of the document is reached.
Thistechnique reduces class imbalance, but it has thesame worst-case runtime complexity of O(n2).We employ a new type of pair generation thataims to address both the class imbalance andimproves the worst-case runtime.
We will useSMARTPG to refer to this component.
Our pairgenerator relies on linguistic intuitions and isbased on the type of each CE.
For a given CE,we use a rule-based algorithm to guess its type.Based on the type, we restrict the scope of possi-ble antecedents to which the CE can refer in thefollowing way:Proper Name (Named Entity): A proper nameis compared against all proper names in the 20 pre-ceding sentences.
In addition, it is compared to allother CEs in the two preceding sentences.Definite noun phrase: Compared to all CEs inthe six preceding sentences.Common noun phrase: Compared to all CEsin the two preceding sentences.Pronoun: Compared to all CEs in the two pre-ceding sentences unless it is a first person pronoun.First person pronouns are additionally comparedto first person pronouns in the preceding 20 sen-tences.During development, we used SMARTPGon coreference resolution corpora other thanOntoNotes and determined that the pair generatortends to lead to more accurate results.
It also hasruntime linear in the number of CEs in a docu-ment, which leads to a sizable reduction in run-ning time for large documents.
Training files gen-erated by SMARTPG also tend to be more bal-anced.
Finally, by omitting pairs that are un-likely to be coreferent, SMARTPG produces muchsmaller training sets.
This leads to faster learningand allows us to train on more documents.124Optimized Metric Threshold BCubed CEAFe MUC BLANC CEAFm CombinedBCubed 0.4470 0.6651 0.4134 0.6156 0.6581 0.5249 0.5647CEAF 0.4542 0.6886 0.4336 0.6206 0.7012 0.5512 0.5809MUC 0.4578 0.6938 0.4353 0.6215 0.7108 0.5552 0.5835Table 2: CoNLL scores for different thresholds on validation data.CoNLL Official Test Scores BCubed CEAFe MUC BLANC CEAFm CombinedClosed Task 0.6144 0.3588 0.5843 0.6088 0.4608 0.5192Gold Mentions 0.6248 0.3664 0.6154 0.6296 0.4808 0.5355Table 4: Official CoNLL 2011 test scores.
Combined score is the average of MUC, BCubed and CEAFe.5 ExperimentsIn this section we present and discuss the resultsfor ReconcileCoNLLwhen trained and evaluatedon OntoNotes data.
For all experiments, we trainon a set of 750 randomly selected documents fromthe OntoNotes corpus.
We use another 674 ran-domly selected documents for validation.
We re-port scores using the scorers implemented inter-nally in Reconcile as well as the scorers suppliedby the CoNLL shared task.In the rest of the section, we describe our resultswhen controlling two aspects of the system ?
thethreshold of the pairwise CE classifier, which istuned on training data, and the method used forpair generation.
We conclude by presenting theofficial results for the CoNLL shared task.Influence of Classifier Threshold As previ-ously mentioned, the threshold above which thedecision of the classifier is considered positiveprovides us with a knob that controls the preci-sion/recall trade-off.
Reconcile includes a mod-ule that can automatically search for a thresholdvalue that optimizes a particular evaluation met-ric.
Results using three Reconcile-internal scor-ers (BCubed, CEAF, MUC) are shown in Table1.
First, we see that the threshold that optimizesperformance on the validation data also exhibitsthe best results on the test data.
The same doesnot hold when using the CoNLL scorer for test-ing, however: as Table 2 shows, the best resultsfor almost all of the CoNLL scores are achieved atthe threshold that optimizes the Reconcile-internalMUC score.
Note that we did not optimize thresh-olds for the external scorer in the name of sav-ing implementation effort.
Unfortunately, the re-sults that we submitted for the official evaluationswere for the suboptimal threshold that optimizesReconcile-internal BCubed score.Influence of Pair Generation Strategies Next,we evaluate the performance of SMARTPG pairgenerators.
We run the same system set-up asabove substituting the pair generation module.
Re-sults (using the internal scorer), displayed in Table3, show our SMARTPG performs identically to thegenerator producing all pairs, while it runs in timelinear in the number of CEs.Official Scores for the CoNLL 2011 SharedTask Table 4 summarizes the official scores ofReconcileCoNLL on the CoNLL shared task.
Sur-prisingly, the scores are substationally lower thanthe scores on our held-out training set.
So far, wehave no explanation for these differences in perfor-mance.
We also observe that using gold-standardinstead of system-extracted CEs leads to improve-ment in score of about point and a half.The official score places us 8th out of 21 sys-tems on the closed task.
We note that becauseof the threshold optimization mix-up we sufferedabout 2 points in combined score performance.Realistically our system should score around 0.54placing us 5th or 6th on the task.6 ConclusionsIn this paper, we presented ReconcileCoNLL, oursystem for the 2011 CoNLL shared task based onthe Reconcile research platform.
We describedthe overall Reconcile platform, our configurationfor the CoNLL task and the changes that we im-plemented specific to the task.
We presented theresults of an empirical evaluation performed onheld-out training data.
We discovered that resultsfor our system on this data are quite different fromthe official score that our system achieved.AcknowledgmentsThis material is based upon work supported bythe National Science Foundation under Grant #0937060 to the Computing Research Associationfor the CIFellows Project.125ReferencesACE05.
2005.
NIST ACE evaluation website.
Inhttp://www.nist.gov/speech/tests/ace/2005.Yoav Freund and Robert E. Schapire.
1999.
Largemargin classification using the perceptron algorithm.In Machine Learning, pages 277?296.D.
Klein and C. Manning.
2003.
Fast Exact Inferencewith a Factored Model for Natural Language Pars-ing.
In Advances in Neural Information Processing(NIPS 2003).Sameer S. Pradhan, Lance Ramshaw, RalphWeischedel, Jessica MacBride, and Linnea Micci-ulla.
2007.
Unrestricted coreference: Identifyingentities and events in ontonotes.
In Proceedingsof the International Conference on SemanticComputing.Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,Martha Palmer, Ralph Weischedel, and NianwenXue.
2011.
Conll-2011 shared task: Modeling un-restricted coreference in ontonotes.
In Proceedingsof the Fifteenth Conference on Computational Nat-ural Language Learning (CoNLL 2011), Portland,Oregon, June.W.
Soon, H. Ng, and D. Lim.
2001.
A MachineLearning Approach to Coreference of Noun Phrases.Computational Linguistics, 27(4):521?541.V.
Stoyanov, N. Gilbert, C. Cardie, and E. Riloff.
2009.Conundrums in noun phrase coreference resolution:Making sense of the state-of-the-art.
In Proceedingsof ACL/IJCNLP.V.
Stoyanov, C. Cardie, N. Gilbert, E. Riloff, D. But-tler, and D. Hysom.
2010a.
Reconcile: A corefer-ence resolution research platform.
Technical report,Cornell University.Veselin Stoyanov, Claire Cardie, Nathan Gilbert, EllenRiloff, David Buttler, and David Hysom.
2010b.Coreference resolution with reconcile.
In Proceed-ings of the ACL 2010.126
