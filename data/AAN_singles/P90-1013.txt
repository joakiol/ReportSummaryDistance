THE COMPUTATIONAL COMPLEXITY  OFAVOID ING CONVERSATIONAL IMPL ICATURESEhud ReiterfAiken Computation LabHarvard UniversityCambridge, Mass 02138ABSTRACtReferring expressions and other object descriptionsshould be maximal under the Local Brevity, NoUnnecessary Components, and Lexical Preferencepreference rules; otherwise, they may lead hearers toinfer unwanted conversational implicatures.
Thesepreference rules can be incorporated into a polyno-mial time generation algorithm, while some alterna-tive formalizations of conversational impficaturemake the generation task NP-Hard.1.
IntroductionNatural language generation (NLG) systemsshould produce referring expressions and other objectdescriptions that are free of false implicatures, i.e.,that do not cause the user of the system to inferincorrect and unwanted conversational implicatures(Grice 1975).
The following utterances illustratereferring expressions that are and are not free of falseimplicatures:la) "Sit by the table"lb) "Sit by the brown wooden table"In a context where only one table was visible, andthis table was brown and made of wood, utterances(la) and (lb) would both fulfill the referring goal: ahearer who heard either utterance would have notrouble picking out the object being referred to.However, a hearer who heard utterance (lb) wouldprobably assume that it was somehow important hatthe table was brown and made of wood, i.e., that thespeaker was trying to do more than just identify thetable.
If the speaker did not have this intention, andonly wished to tell the hearer where to sit, then thiswould be an incorrect conversational implicature, andcould lead to problems later in the discourse.Accordingly, a speaker who only wished to identifythe table should use utterance (la) in this situation,f Currently at the Depamnem of Artificial Intelligence,University of Edinburgh, 80 South Bridge, Edinburgh EHI1HN, Scotland.
97and avoid utterance (lb).Incorrect conversational implicatures may alsoarise from inappropriate attributive (informational)descriptions.
1 This is illustrated by the followingutterances, which might be used by a salesman whowished to inform a customer of the color, material,and sleeve-length ofa shirt:2a) "I have a red T-shirt"2b) "I have a lightweight red cotton shirt withshort sleeves"Utterances (2a) and (2b) both successfully inform thehearer of the relevant properties of the shirt, assum-ing the hearer has some domain knowledge about T-shirts.
However, if the hearer has this domainknowledge, the use of utterance (2b) mightincorrectly implicate that the object being describedwas not a T-shirt - -  because if it was, the hearerwould reason, then the speaker would have usedutterance (Za).Therefore, in the above situations the speaker,whether a human or a computer NLG system, shoulduse utterances (la) and (2a), and should avoid utter-ances (lb) and (2b); utterances (la) and (2a) are freeof false implicatures, while the utterances (lb) and(2b) are not.
This paper proposes a computationalmodel for determining when an object description isfree of false implicatures.
Briefly, a description isconsidered free of false implicatures if it is maximalunder the Local Brevity, No Unnecessary Com-ponents, and Lexical Preference preference rules.These preference rules were chosen on complexity-theoretic as well as linguistic criteria; descriptionsthat are maximal under these preference rules can befound in polynomial time, while some alternative for-malizations of the free-of-false-implicatures con-straint make the generation task NP-Hard.I The referring/attributive distinction follows Donnellan(1966): a referring expression is intended to identify an objectin the current context, while an attributive description is in-tended to communicate information about an object.This paper only addresses the problem of gen-erating free-of-false-implicatures referring expres-sions, such as utterance (la).
Reiter (1990a,b) usesthe same preference rules to formalize the task ofgenerating free-of- alse-implicatures attributivedescriptions, uch as utterance (2a).2.
Referring Expression ModelThe referring-expression model used in thispaper is a variant of Dale's (1989) model for fulldefinite noun phrase referring expressions.
Dale'smodel is applicable in situations in which the speakerintends to refer to an object that the speaker andhearer are mutually aware of, and the speaker has noother communicative goal besides identifying thereferred-to bject.
2 The model assumes that objectsbelong to a taxonomy class (e.g., Chair) and possessvalues for various attributes (e.g., Color:Brown).
3Referring expressions are represented as aclassification and a set of attribute-value pairs: theclassification is syntactically realized as the headnoun, while the attribute-value pairs are syntacticallyrealized as NP modifiers.
Successful referringexpressions are required to be distinguishing descrip-t/ons, i.e., descriptions that contain a classificationand a set of attributes that are true of the object beingreferred to, but not of any other object in the currentdiscourse context.
4More formally, and using a somewhat differentterminology from Dale, let a component be either aclassification or an attribute-value pair.
Aclassification component will be written class:Class;an attribute-value pair component will be writtenAttribute:Value.
Then, given a target object, denotedTarget, and a set of contrasting objects in the currentdiscourse context, denoted Excluded, a set of com-ponents will represent a successful referring expres-sion (a distinguishing description, in Dale's terminol-2 Appelt (1985) presented a more complex rderring-expression model that covered situations where the hearerwas not already aware of the referred-to object, and that al-lowed the speaker to have more complex communicativegoals.
A similar laalysis to the one presented in this papercould in principle be done for Appelt's model, but it wouldbe substantially more difficult, in part because the model ismore complex, and in pa~t because Appeh did not separatehis 'content detcrminatiou' subsystem frona his planner andhis sudaee-form generator.3 All auributes are assumed to be predicative (Karnp1975).4 Dale also suggested that NLG systems hould choosedistinguishing descril0dons ofminimal cardinality; this is dis-cussed in footnote 7.ogy) if the set, denoted RE, satisfies the followingconstraints:1) Every component in RE applies to Target: thatis, every component in RE is either aclassification that subsumes Target, or anattribute-value pair that Target possesses.2) For every member E of Excluded, there is atleast one component in RE that does not applytoE.Example: the current discourse context con-tains objects A, B, and C (and no other objects), andthese objects have the following classifications andattributes (of which both the speaker and the hearerare aware):A) Table with Material:Wood and Color:Brown.B) Chair with Material:Wood and Color:BrownC) Chair with Material:Wood and Color:BlackIn this context, the referring expressions{class:Table} ("the table") and {class:Table,Material:Wood, Color:Brown} ("the brown woodentable") both successfully refer to object A, becausethey match object A but no other object.
Similarly,the referring expressions {class:Chair,Color:Brown} ("the brown chair") and {class:Chair,Material:Wood, Color:Brown} ("the brown woodenchair") both successfully refer to object B, becausethey match object B, but no other object.
The refer-ring expression {class:Chair} (~the chair"), how-ever, does not successfully refer to object B, becauseit also matches object C.983.
Conversational Implicature3.1.
Grice's Maxims and Their InterpretationGrice (1975) proposed four maxims of conver-sation that speakers needed to obey: Quality, Quan-tity, Relevance, and Manner.
For the task of generat-ing referring expressions as formalized in Section 2,these maxims can be interpreted as follows:Quality: The Quality maxim requires utter-anees to be truthful.
In this context, it requires refer-ring expressions to be factual descriptions of thereferred-to bject.
This condition is already part ofthe definition of a successful referring expression,and does not need to be restated as a conversationalimplicature constraint.Quantity: The Quality maxim requires utter-antes to contain enough information to fulfill thespeaker's communicative goal, but not more informa-tion.
In this context, it requires referring expressionsto contain enough information to enable the hearer toidentify the referred-to object, but not more informa-tion.
Therefore, referring expressions should be suc-cessful (as defined in Section 2), but should not con-rain additional elements that are unnecessary forfulfilling the referring oal.Relevance: The Relevance maxim requiresutterances to be relevant o the discourse.
In thiscontext, where the speaker is assumed just to havethe communicative goal of identifying an object tothe hearer, the maxim prohibits referring expressionsfrom containing elements that do not help distinguishthe target object from other objects in the discoursecontext.
Irrelevant elements are also unnecessaryelements, so the Relevance maxim may be con-sidered to be a special case of the Quantity maxim, atleast for the referring-expression generation task asformalized in Section 2.Manner: The Brevity submaxim of the Mannermaxim requires a speaker to use short utterances ifpossible.
In this context it requires the speaker to usea short referring expression if such a referringexpression exists.
The analysis of the other Mannersubmaxims i left for future work.An additional source of conversational impli-catm'e was proposed by Cruse (1977) and Hirschberg(1985), who hypothesized that.
implicatures mightarise from the failure to use basic-level classes(Rosch 1978) in an utterance.
In this paper, suchimplicatures are generalized by assuming that there isa lexical-preference hierarchy among the lexicalclasses (classes that can be realized with single lexi-cal units) known to the hearer, and that the use of alexical class in an utterance implicates that no pre-ferred lexical class could have been used in its place.In summary, conversational implicature con-siderations require referring expressions to be brief,to not contain unnecessary elements, and to uselexically-preferred classes whenever possible.
Thefollowing requests illustrate how violations of theseprinciples in referring expressions may lead tounwanted conversational implicatares:3a) "Wait for me by the pine.
"({class:Pine})993b) "Wait for me by the tree that has pinecones.
"({class:Tree, Seed-type :Pinecone })3c) "Wait for me by the 50-foot-high pine.
"({class:Pine, Height:50-feet } )3d) ~Wait for me by the sugar pine.
"({ class:Sugar-pine })If there were only two trees in the hearer's immediatesurroundings, a pine and an oak, then all of the aboveutterances would be successful referring expressionsthat enabled the hearer to pick out the object beingreferred to (assuming the hearer could recognizepines and oaks).
In such a situation, however, utter-ance (3b) would violate the brevity principle, andthus would implicate that the tree could not bedescribed as a "pine" (which might lead the hearer toinfer that the tree was not a real pine, but some othertree that happened to have pinecones).
Utterance(3c) would violate the no-unnecessary-elements prin-ciple, and thus would implicate that it was importantthat the tree was 50 feet tall (which might lead thehearer to infer that there was another pine tree in thearea that had a different height).
Utterance (3d)would violate the lexical-preference principle, andthus would implicate that the speaker wished toemphasize that the tree was a sugar pine and notsome other kind of pine (which might lead the hearerto infer that the speaker was trying to impress herwith his botanical knowledge).
A speaker who onlywished to tell the hearer where to wait, and did notwant the hearer to make any of these implicatures,would need to use utterance (3a), and to avoid utter-ances (3b), (3c), and (30).3.2.
Formalizing Conversational ImplicatureThrough Preference RulesThe brevity, no-unnecessary-elements, andlexical-preference principles may be formalized byrequiring a description to be a maximal elementunder a preference function of the set of successfulreferring expressions.
More formally, let D be the setof successful referring expressions, and let >> be apreference function that prefers descriptions that areshort, that do not contain unnecessary elements, andthat use lexically preferred classes.
Then, a referringexpression is considered free of false implicatures ifit is a maximal element of D with respect to >>.
Inother words, a description B in D is free of falseimplicatures if there is no description A in D, suchthat A >> B.
This formalization is similar to the par-tially ordered sets that Hirschberg (1985) used to for-malize scalar implicatures: D and >> together form apartially ordered set, and the assumption is that theuse of an element in D carries the conversationalimplicature that no higher-ranked element in D couldhave been used.The overall preference function >> will bedecomposed into separate preference rules that covereach type of implicature: >>B for brevity, >>u forunnecessary elements, and >>t.
for lexical prefer-euce.
>> is then defined as the disjunction of thesepreference rules, i.e., A >> B if A >>s B, A >>v B,or A >>L B.
The assumption will be made in thispaper that there are no conflicts between preferencerules, i.e., that it is never the case that A is preferredover B by one preference rule, but B is preferred overA by another preference rule.
5 Therefore, >> will bea partial order if >>B, >>v, and >>n are partial ord-ers.3.3.
Computational TractabilityComputational complexity considerations areused in this paper to determine xactly how the no-unnecessary-elements, brevity, and lexical-preference principles hould be formalized as prefer-enee rules.
Sections 4, 5, and 6 examine variouspreference rules that might plausibly be used to for-malize these implicatures, and reject preference rulesthat make the generation task NP-Hard.
This isjustified on the grounds that computer NLG systemsshould not be asked to solve NP-Hard problems.
6Human speakers and hearers are also probably notvery proficient at solving NP-Hard problems, whichsuggests that it is unlikely that NP-Hard preferencerules have been incorporated into language.4.
BrevityGrice's submaxim of brevity states that utter-auces should be kept brief.
Many NLG researchers(e.g., Dale 1989; Appelt 1985: pages 117-118) havesuggested that this means generation systems need toproduce the shortest possible utterance.
This will becalled the Full Brevity preference rule.
Unfor-tunately, it is NP-Hard to find the shortest successfulreferring expression (Section 4.1).
Local Brevity(Section 4.2) is a weaker version of the brevity sub-maxim that can be incorporated into a polynomial-time algorithm for generating successful referringexpressions.5 Section 7.2 discusses this assumption.6 Section 7.1 discusses the computational impact of NP-Hard preference rules.i004.1.
Full BrevityThe Full Brevity preference rule requires thegeneration system to generate the shortest successfulreferring expression.
Formally, A >>FB B iflength(A) < length(B).
The task of finding a maximalelement of >>FB, i.e., of finding the shortest success-ful referring expression, is NP-Hard.
This resultholds for all definitions of length the author hasexamined (number of open-class words, number ofwords, number of characters, number of com-ponents).To prove this, let Target-Components denotethose components (classifications and attribute-valuepairs) of Target that are mutually known by thespeaker and the hearer.
For each Tj in Target-Components, let Rules-Out(Tj) be the members ofExcluded that do not possess Tj (so, the presence ofTj in a referring expression 'rules out' thesemembers).
Then, consider a potential referringexpression, RE = {Ct .
.
.
.
.
C,}.
RE will be a suc-cessful referring expression if and only ifa) Every Ci is in Target-Componentsb) The union of Rules-Out(Ci), for all Ci in RE, isequal to Excluded.For example, if the task was referring to objectB in the example context of Section 2, then Target-Components would be {class:Chair, Material:Wood,Color:Brown}, Excluded would be {A, C}, andRules-Out(class:Chair) = { A }Rules-Out(Material:Wood) = empty setRules-Out(Color:Brown) ={C}Therefore, {class:Chair, Color:Brown} (i.e., "thebrown chair") would be a successful referringexpression for object B in this context.If description length is measured by number ofcomponents, 7 finding the minimal length referringexpression is equivalent to solving a minimum setcover problem, where Excluded is the set beingcovered, and the Rules-Out(Tj) are the covering sets.Unfortunately, finding a minimal set cover is an NP-7 Dale's (1989) minimal distinguishing descriptions are,in the terminology of this paper, successful referring expres-sions that are maximal under Full Brevity when number ofcomponents is used as  the measure of description length.Therefore, finding a minimal distinguishing description is anNP-Hard problem.
The algorithm Dale used was essentiallyequivalent o the greedy heuristic for minimal set cover(Johnson 1974); as such it ran quickly, but did not alwaysfind a tree minimal distinguishing description.Hard problem (Garey and Johnson 1979), and thussolving it is in general computationally intractable(assuming that P ~ NP).Similar proofs will work for the otherdefinitions of length mentioned above.
On an intui-tive level, the basic problem is that finding the shor-test description requires searching for the globalminimum of the length function, and this globalminimum (like many global minima) may be veryexpensive to locate.4.2.
Local BrevityThe Local Brevity preference rule is a weakerinterpretation of Grice's brevity submaxim.
It statesthat it should not be possible to generate a shortersuccessful referring expression by replacing a set ofcomponents by a single new componenL Formally,>>us is the transitive closure of >>us', where A >>us,B if size(components(A)-components(B)) = 1, s andlength(A) < length(B).
The best definition oflength(A) is probably the number of open-classwords in the surface realization of A.Local brevity can be checked by selecting apotential new component, finding all minimal sets ofold components whose combined length is greaterthan the length of the new component, performingthe substitution, and checking if the result is a sue-cessful referring expression.
This can be done inpolynomial time if the number of minimal sets ispolynomial in the length of the description, whichwill happen if (non-zero) upper and lower bounds areplaced on the length of any individual component(e.g., the surface realization of every componentmust use at least one open-class word, but no morethan some fixed number of open-class words).element is defined: detecting unnecessary words inreferring expressions i NP-Hard (Section 5.1), butunnecessary components can always be found inpolynomial time (Section 5.2).5.1.
No Unnecessary WordsThe No Unnecessary Words preference ruleforbids referring expressions from containingunnecessary words.
Formally, A >>ow B if A's sur-face form uses a subset of the words used by B's sur-face form.
There are several variants, such as onlyconsidering open-class words, or requiring the wordsin B to be in the same order as the correspondingwords in A.
All of these variants make the genera-tion problem NP-Hard.The formal proofs are in Reiter (1990b).
Intui-tively, the basic problem is that any preference that isstated solely in terms of surface forms must deal withthe possibility that new parses and semantic interpre-tations may arise when the surface form is modified.This means that the only way a generation systemcan guarantee that an utterance satisfies the NoUnnecessary Words rule is to generate all possiblesubsets of the surface form, and then run each subsetthrough a parser and semantic interpreter to check ifit happens to be a successful referring expression.The number of subsets of the surface form isexponential in the size of the surface form, so thisprocess will take exponential time.To illustrate the 'new parse' problem, considertwo possible referring expressions:4a) "the child holding a pumpkin"4b) "the child holding a slice of pumpkin pie"5.
No Unnecessary ElementsThe Gricean maxims of Quantity andRelevance prohibit utterances from containing ele-ments that are unnecessary for fulfilling the speaker'scommunicative goals.
The undesirability ofunneces-sary elements i further supported by the observationthat humans find pleonasms (Cruse 1986) such as "afemale mother" and "an unmarried bachelor" to beanomalous.
The computational tractability of theno-unnecessary-elements principle depends on how8 This is a set formula, where "-* means set-differenceand "size" means nmnher of members.
The formula requiresA to have exactly one COmlx~ent that is not present in B; Bcan have an ~oitra W number of components hat are notpresent in A.i01If utterances (4a) and (4b) were both successfulreferring expressions (i.e., the child had a pumpkin inone hand, and a slice of pumpkin pie in the other),then (4a) >>ow (4b) under any of the variants men-tioned above.
However, because utterance (4a) has adifferent syntactic structure than utterance (4b), theonly way the generation system could discover that(4a) >>vw (4b) would be by constructing utterance(4b)'s surface form, removing the words "slice,""of," and "pie" from it, and analyzing the reducedsurface form.This problem, of new parses and semanticinterpretations being uncovered by modifications tothe surface form, causes difficulties whenever apreference rule is stated solely in terms of the surfaceform.
Accordingly, such preference rules should beavoided.5.2.
No Unnecessary ComponentsThe No Unnecessary Components preferencerule forbids referring expressions from containingunnecessary components.
Formally, A >>uc B if Auses a a subset of the components u ed by B.Unnecessary components can be found in poly-nomial time by using a simple incremental lgorithmthat just removes each component in turn, and checksif what is left constitutes a successful referringexpression.The key algorithmic difference between NoUnnecessary Components and No UnnecessaryWords is that this simple incremental algorithm willnot work for the No Unnecessary Words preferencerule.
This is because there are cases where removingany single word from an utterance's surface formwifl leave an unsuccessful (or incoherent) referringexpression (e.g., imagine removing just "slice" fromutterance (4b)), but removing several words willuncover a new parse that corresponds toa successfulreferring expression.
In contrast, if B is a successfulreferring expression, and there exists another sue-cessful referring expression A that satisfiescomponents(A) c components(B) (and hence A ispreferred over B under the No Unnecessary Com-ponents preference rule), then it will be the case thatany referring expression C that satisfiescomponents(A) c components(C) c components(B)will also be successful.
This means that the simplealgorithm can always produce A from B by incre-mental steps that remove a single component at atime, because the intermediate descriptions formed inthis process will always be successful referringexpressions.
Therefore, the simple incremental algo-rithm will always find unnecessary components, butmay not always find unnecessary words.6.
Lexlcal PreferenceIf the attribute values and classifications usedin the description are members of a taxonomy, thenthey can be realized at different levels of specificity.For example, the object in the parking lot outside theauthor's window might be called "a vehicle," "amotor vehicle," "a car," "a sports car," or "aPorsche.
"The Lexical Preference rule assumes there is alexical-preference hierarchy among the taxonomy'slexical classes (classes that can be realized with sin-gle lexical units).
The rule states that utterancesshould use preferred lexical classes whenever possi-ble.
Formally, A >>t.
B if for every component in A,that is a component in B that has the same structure,102and the lexieal class used by the A component isequal to or lexically preferred over the lexical classused by the B component.The lexical-preference hierarchy should, atminimum, incorporate he following preferences:i) Lexical class A is preferred over lexical classB if A's realization uses a subset of the open-class words used in B's realization.
For exam-ple, the class with realization `` vehicle" is pre-ferred over the class with realization "motorvehicle.
"ii) Lexical class A is preferred over lexical classB if A is a basic-level class, and B is not.
Forexample, if car was a basic-level c ass, then "acar" would be preferred over ``a vehicle" or ` `aporsche.
"9In some cases these two preferences may conflict;this is discussed inSection 7.2.Utterances that violate either preference (i) orpreference (ii) may implicate unwanted implicatures.Preference rule (ii) has been discussed by Cruse(1977) and Hirschberg (1985).
Preference rule (i)may be considered to be another application of theGricean maxim of quantity, and is illustrated by thefollowing utterances:5a) "Wait for me by my car"5b) "Walt for me by my sports car"If utterances (5a) and (5b) were both successfulreferring expressions (e.g., if the speaker possessedonly one ear), then the use of utterance (5b) wouldimplicate that the speaker wished to emphasize thathis vehicle was a sports car, and not some other kindof car.From an algorithmic point of view, referringexpressions that are maximal under the lexical-preference criteria can be found in polynomial time ifthe following restriction is imposed on the lexical-preference hierarchy:Restriction:If lexical class A is preferred over lexical classB, then A must either subsume B or be sub-sumed by B in the class taxonomy.For example, it is acceptable for car to be preferredover vehicle or Porsche, but it is not acceptable forcar to be preferred over gift (because car neither sub-sumes nor is subsumed by g~ft).If the above reslriction holds, a variant of thesimple incremental gorithm of Section 5.2 may beused to implement lexical preference: the algorithmsimply attempts each replacement that lexical prefer-ence suggests, and checks if this results in a success-ful referring expression.
If the restriction does nothold, then the simple incremental lgorithm may fall,and obeying the Lexical Preference rule is in factN-P-Hard (the formal proof is in Reiter (1990b)).7.
ISSUES7.1.
The Impact of NP-Hard Preference RulesIt is difficult o precisely determine the compu-tational expense of generating referring expressionsthat are maximal under the Full Brevity or NoUnnecessary Words preference rules.
The moststraightforward algorithm that obeys Full Brevity (asimilar analysis can be done for No UnnecessaryWords) simply does an exhaustive search: it firstchecks if any one-component referring expression issuccessful, then checks if any two-component refer-ring expression is successful, and so forth.
Let L bethe number of components in the shortest referringexpression, and let N be the number of componentsthat are potentially useful in a description, i.e., thenumber of members of Target-Components that ruleout at least one member of Excluded.
The straight-forward full-brevity algorithm will then need toexamine the following number of descriptions beforeit finds a successful referring expression:For the problem of generating a referring expressionthat identifies object B in the example contextpresented in Section 2, N is 3 and L is 2, so thestraightforward brevity algorithm will take only 6steps to find the shortest description.
This problem isartificially simple, however, because N, the numberof potential description components, i  so small.
In amore realistic problem, one would expect Target-Components oinclude size, shape, orientation, posi-tion, and probably many other attribute-value pairs aswell, which would mean that N would probably be atleast 10 or 20.
L, the number of attributes in theshortest possible referring expression, is probablyfairly small in most realistic situations, but there arecases where it might be at least 3 or 4 (e.g., considerUthe upside-down blue cup on the second shelf").
203For some example values of L and N in this range,the straightforward brevity algorithm will need toexamine the following number of descriptions:L = 3, N = 10; 175 descriptionsL = 4, N = 20; over 6000 descriptionsL = 5, N = 50; over 2,000,000 descriptionsThe straightfo~vard full-brevity algorithm,then, seems prohibitively expensive in at least somecircumstances.
Because finding the shortest descrip-tion is N-P-Hard, it seems likely (existingcomplexity-theoretic te hniques are too weak toprove such statements) that all algorithms for findingthe shortest description will have similarly bad per-formance in the worst case.
It is possible, however,that there exist algorithms that have acceptable per-formance in almost all 'realistic' cases.
Any suchproposed algorithm, however, should be carefullyanalyzed to determine in what circumstances it willfail to find the shortest description or will takeexponential time to run.7.2.
Conflicts Between Preference RulesThe assumption has been made in this paperthat the preference rules do not conflict, i.e., that it isnever the case that description A is preferred overdescription B by one preference rule, while descrip-tion B is preferred over description A by anotherpreference rule.
This means, in particular, that if lex-ical class LC1 is preferred over lexical class LC2,then LC,'s realization must not contain more open-class words than LC2's realization; otherwise, theLexical Preference and Local Brevity preferencerules may conflict.
1?
This can be supported bypsychological nd linguistic findings that basic-levelclasses are almost always realized with single words(Rosch 1978; Berlin, Breedlove, and Raven 1973).However, there are a few exceptions to this rule, i.e.,there do exist a small number of basic-levelcategories that have realizations that require morethan one open-class word.
For example, Washing-Machine is a basic-level class for some people, and ithas a realization that uses two open-class words.This leads to a conflict of the type mentioned above:basic-level Washing-Machine is preferred over non-10 This assmnes that the Local Brevity pTcfenmcc ruleuses  number of open-class words as its measure of descrip-tic~ length.
If number of comp~cnts or number of lcxicalunits is used as the measure of description length, then LocalBrevity will never conflict with Lcxical Prcfc~-ncc.No other conflicts can occur between the No Unneces-saw Components, Local Brevity, and Lexical Preferencepreference rules.basic-level Appliance, but Washing-Machine's reali-zation contains more open-class words thanAppliance's.The presence of a basic-level class with amulti-word realization can also cause a conflict tooccur between the two lexical-preference principlesgiven in Section 6 (such conflicts are otherwiseimpossible).
For example, Washing-Machine's r ali-zation contains a superset of the open-class wordsused in the realization of Machine, so the basic-levelpreference of Section 6 indicates that Washing-Machine should be lexically preferred over Machine,while the realization-subset preference indicates thatMachine should be lexically preferred overWashing-Machine.
The basic-level preferenceshould take priority in such cases, so Washing-Machine is the true lexicaUy-preferred class in thisexample.7.3.
Generalizability of ResultsFor the task of generating attributive descrip-tions as formalized in Reiter (1990a, 1990b), theLocal Brevity, No Unnecessary Components, andLexieal Preference rules are effective at prohibitingutterances that carry unwanted conversational impli-catures, and also can be incorporated into apolynomial-time g neration algorithm, provided thatsome restrictions are imposed on the underlyingknowledge base.
The effectiveness and tractabilityof these preference rules for other generation tasks isan open problem that requires further investigation.The Full Brevity and No Unnecessary Wordspreference rules are computationally intractable forthe attributive description generation task (Reiter1990b), and it seems likely that they will be intract-able for most other generation tasks as well.
Becauseglobal maxima are usually expensive to locate,finding the shortest acceptable utterance will prob-ably be computationally expensive for most genera-tion tasks.
Because the 'new parse' problem ariseswhenever the preference function is staled solely interms of the surface form, detecting unnecessarywords will also probably be quite expensive in mostsituations.8.
ConclusionReferring expressions and other object descrip-tions need to be brief, to avoid unnecessary elements,and to use lexically preferred classes; otherwise, theymay carry unwanted and incorrect conversationalimplicatures.
These principles can be formalized byrequiring referring expressions to be maximal underthe Local Brevity, No Unnecessary Components, and104Lexical Preference preference rules.
These prefer-ence rules can be incorporated into a polynomial-time algorithm for generating free-of-false-implicatures referring expressions, while some alter-native preference rules (Full Brevity and NoUnnecessary Words) make this generation task NP-Hard.AckmowJedgementsMany thanks to Robert Dale, Joyce Friedman, BarbaraGrosz, Joe Marks, Warren Plath, Candy Sid~er, Jeff Siskind, BillWoods, and the anonymous reviewers for their help and sugges-tions.
This work was partially supported by a National ScienceFoundatiou Graduate Fellowship, an IBM Graduate Fellowship,and a contract from U S WEST Advanced Technologies.
Anyopinions, findings, conclusions, or recommendations are those ofthe author and do not necessarily reflect he views of the NationalScience Fotmdation, IBM, or U S WEST Advanced Technologies.ReferencesAppelt, D. 1985 Planning English Referring Expressions.
Cam-bridge University Press: New York.Berlin, B.; Breedlove, D,; and Raven, P. 1973 General Principlesof Classification and Nomenclature in Folk Biology.
Amer-ican Anthropologist 75:214-242.Cruse, D. 1977 The pragmatics of lexical specificity.
Journal ofLinguistics 13:153-164.Cruse, D. 1986 Lexical Semantics.
Cambridge University Press:New York.Dale, R. 1989 Cooking up Referring Expressious.
In Proceedingsof the 27th Annual Meeting of the Association for Compu-tational Linguistics.Donnellan, K. 1966 Reference and Definite Descriptions.
Philo-sophical Review 75:281-304.Garey, M. and Johnson, D. 1979 Computers and Intractability: aGuide to the Theory of NP-Completeness.
W. H. Freeman:San Francisce.Grice, H. 1975 Logic and conversatiou.
In P. Cole and J.
Morgan(Eds.
), Syntax and Semantics: Vol 3, Speech Acts, pg 43-58.
Academic Press: New York.Hirsehberg, J.
1985 A Theory of Scalar lmplicature.
Report MS-CIS-85-56, LINC LAB 21.
Department of Computer andInformation Science, University of Pennsylvania.Johnson, D. 1974 Approximation algorithms for eomhinatorialproblems.
Journal of Computer and Systems Sciences9:256-178.Kamp, H. (1975) Two Theories about Adjectives.
In E.
Koenan(Ed.)
Formal Semantics of Natural Language, pg 123-155.Cambridge University Press: New York.Reiter, E. 1990a Generating Descriptions that Exploit a User'sDomain Knowledge.
To appear in R. Dale0 C. MeRish, andM.
Zock (F_xls.
), Current Research in Natural LanguageGeneration.
Academic Press: New York.Reiter, E. 1990b Generating Appropriate Natural LanguageObject Descriptions.
Ph.D thesis.
Aiken Computation Lab,Harvard University: Cambridge, Mass.Rosch, E. 1978 Principles of Categorization.
In E. Rosch and B.Lloyd (Eds.
), Cognition and Categorization.
Lawrence Erl-baum: Hillsdale, NL
