Proceedings of the 43rd Annual Meeting of the ACL, pages 507?514,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsDiscriminative Syntactic Language Modeling for Speech RecognitionMichael CollinsMIT CSAILmcollins@csail.mit.eduBrian RoarkOGI/OHSUroark@cslu.ogi.eduMurat SaraclarBogazici Universitymurat.saraclar@boun.edu.trAbstractWe describe a method for discriminativetraining of a language model that makesuse of syntactic features.
We followa reranking approach, where a baselinerecogniser is used to produce 1000-bestoutput for each acoustic input, and a sec-ond ?reranking?
model is then used tochoose an utterance from these 1000-bestlists.
The reranking model makes use ofsyntactic features together with a parame-ter estimation method that is based on theperceptron algorithm.
We describe exper-iments on the Switchboard speech recog-nition task.
The syntactic features providean additional 0.3% reduction in test?seterror rate beyond the model of (Roark etal., 2004a; Roark et al, 2004b) (signifi-cant at p < 0.001), which makes use ofa discriminatively trained n-gram model,giving a total reduction of 1.2% over thebaseline Switchboard system.1 IntroductionThe predominant approach within language model-ing for speech recognition has been to use an n-gram language model, within the ?source-channel?or ?noisy-channel?
paradigm.
The language modelassigns a probability Pl(w) to each string w in thelanguage; the acoustic model assigns a conditionalprobability Pa(a|w) to each pair (a,w) where a is asequence of acoustic vectors, and w is a string.
Fora given acoustic input a, the highest scoring stringunder the model isw?
= arg maxw(?
log Pl(w) + log Pa(a|w)) (1)where ?
> 0 is some value that reflects the rela-tive importance of the language model; ?
is typi-cally chosen by optimization on held-out data.
Inan n-gram language model, a Markov assumptionis made, namely that each word depends only onthe previous (n ?
1) words.
The parameters of thelanguage model are usually estimated from a largequantity of text data.
See (Chen and Goodman,1998) for an overview of estimation techniques forn-gram models.This paper describes a method for incorporatingsyntactic features into the language model, usingdiscriminative parameter estimation techniques.
Webuild on the work in Roark et al (2004a; 2004b),which was summarized and extended in Roark et al(2005).
These papers used discriminative methodsfor n-gram language models.
Our approach reranksthe 1000-best output from the Switchboard recog-nizer of Ljolje et al (2003).1 Each candidate stringw is parsed using the statistical parser of Collins(1999) to give a parse tree T (w).
Information fromthe parse tree is incorporated in the model usinga feature-vector approach: we define ?
(a,w) tobe a d-dimensional feature vector which in princi-ple could track arbitrary features of the string wtogether with the acoustic input a.
In this paperwe restrict ?
(a,w) to only consider the string wand/or the parse tree T (w) for w. For example,?
(a,w) might track counts of context-free rule pro-ductions in T (w), or bigram lexical dependencieswithin T (w).
The optimal string under our newmodel is defined asw?
= arg maxw(?
log Pl(w) + ??
?, ?
(a,w)?+log Pa(a|w)) (2)where the arg max is taken over all strings in the1000-best list, and where ??
?
Rd is a parametervector specifying the ?weight?
for each feature in?
(note that we define ?x, y?
to be the inner, or dot1Note that (Roark et al, 2004a; Roark et al, 2004b) giveresults for an n-gram approach on this data which makes use ofboth lattices and 1000-best lists.
The results on 1000-best listswere very close to results on lattices for this domain, suggestingthat the 1000-best approximation is a reasonable one.507product, between vectors x and y).
For this paper,we train the parameter vector ??
using the perceptronalgorithm (Collins, 2004; Collins, 2002).
The per-ceptron algorithm is a very fast training method, inpractice requiring only a few passes over the train-ing set, allowing for a detailed comparison of a widevariety of feature sets.A number of researchers have described workthat incorporates syntactic language models into aspeech recognizer.
These methods have almost ex-clusively worked within the noisy channel paradigm,where the syntactic language model has the taskof modeling a distribution over strings in the lan-guage, in a very similar way to traditional n-gramlanguage models.
The Structured Language Model(Chelba and Jelinek, 1998; Chelba and Jelinek,2000; Chelba, 2000; Xu et al, 2002; Xu et al, 2003)makes use of an incremental shift-reduce parser toenable the probability of words to be conditioned onk previous c-commanding lexical heads, rather thansimply on the previous k words.
Incremental top-down and left-corner parsing (Roark, 2001a; Roark,2001b) and head-driven parsing (Charniak, 2001)approaches have directly used generative PCFGmodels as language models.
In the work of WenWang and Mary Harper (Wang and Harper, 2002;Wang, 2003; Wang et al, 2004), a constraint depen-dency grammar and a finite-state tagging model de-rived from that grammar were used to exploit syn-tactic dependencies.Our approach differs from previous work in a cou-ple of important respects.
First, through the feature-vector representations ?
(a,w) we can essentiallyincorporate arbitrary sources of information fromthe string or parse tree into the model.
We would ar-gue that our method allows considerably more flexi-bility in terms of the choice of features in the model;in previous work features were incorporated in themodel through modification of the underlying gen-erative parsing or tagging model, and modifying agenerative model is a rather indirect way of chang-ing the features used by a model.
In this respect, ourapproach is similar to that advocated in Rosenfeld etal.
(2001), which used Maximum Entropy modelingto allow for the use of shallow syntactic features forlanguage modeling.A second contrast between our work and previ-ous work, including that of Rosenfeld et al (2001),is in the use of discriminative parameter estimationtechniques.
The criterion we use to optimize the pa-rameter vector ??
is closely related to the end goalin speech recognition, i.e., word error rate.
Previ-ous work (Roark et al, 2004a; Roark et al, 2004b)has shown that discriminative methods within an n-gram approach can lead to significant reductions inWER, in spite of the features being of the same typeas the original language model.
In this paper we ex-tend this approach, by including syntactic featuresthat were not in the baseline speech recognizer.This paper describe experiments using a varietyof syntactic features within this approach.
We testedthe model on the Switchboard (SWB) domain, usingthe recognizer of Ljolje et al (2003).
The discrim-inative approach for n-gram modeling gave a 0.9%reduction in WER on this domain; the syntactic fea-tures we describe give a further 0.3% reduction.In the remainder of this paper, section 2 describesprevious work, including the parameter estimationmethods we use, and section 3 describes the feature-vector representations of parse trees that we used inour experiments.
Section 4 describes experimentsusing the approach.2 Background2.1 Previous WorkTechniques for exploiting stochastic context-freegrammars for language modeling have been ex-plored for more than a decade.
Early approachesincluded algorithms for efficiently calculating stringprefix probabilities (Jelinek and Lafferty, 1991; Stol-cke, 1995) and approaches to exploit such algo-rithms to produce n-gram models (Stolcke and Se-gal, 1994; Jurafsky et al, 1995).
The work of Chelbaand Jelinek (Chelba and Jelinek, 1998; Chelba andJelinek, 2000; Chelba, 2000) involved the use of ashift-reduce parser trained on Penn treebank styleannotations, that maintains a weighted set of parsesas it traverses the string from left-to-right.
Eachword is predicted by each candidate parse in this setat the point when the word is shifted, and the con-ditional probability of the word given the previouswords is taken as the weighted sum of the condi-tional probabilities provided by each parse.
In thisapproach, the probability of a word is conditionedby the top two lexical heads on the stack of the par-508ticular parse.
Enhancements in the feature set andimproved parameter estimation techniques have ex-tended this approach in recent years (Xu et al, 2002;Xu et al, 2003).Roark (2001a; 2001b) pursued a different deriva-tion strategy from Chelba and Jelinek, and used theparse probabilities directly to calculate the stringprobabilities.
This work made use of a left-to-right,top-down, beam-search parser, which exploits richlexico-syntactic features from the left context ofeach derivation to condition derivation move proba-bilities, leading to a very peaked distribution.
Ratherthan normalizing a prediction of the next word overthe beam of candidates, as in Chelba and Jelinek,in this approach the string probability is derived bysimply summing the probabilities of all derivationsfor that string in the beam.Other work on syntactic language modeling in-cludes that of Charniak (2001), which made use ofa non-incremental, head-driven statistical parser toproduce string probabilities.
In the work of WenWang and Mary Harper (Wang and Harper, 2002;Wang, 2003; Wang et al, 2004), a constraint depen-dency grammar and a finite-state tagging model de-rived from that grammar, were used to exploit syn-tactic dependencies.
The processing advantages ofthe finite-state encoding of the model has allowedfor the use of probabilities calculated off-line fromthis model to be used in the first pass of decoding,which has provided additional benefits.
Finally, Ochet al (2004) use a reranking approach with syntacticinformation within a machine translation system.Rosenfeld et al (2001) investigated the use ofsyntactic features in a Maximum Entropy approach.In their paper, they used a shallow parser to anno-tate base constituents, and derived features from se-quences of base constituents.
The features were in-dicator features that were either (1) exact matchesbetween a set or sequence of base constituents withthose annotated on the hypothesis transcription; or(2) tri-tag features from the constituent sequence.The generative model that resulted from their fea-ture set resulted in only a very small improvementin either perplexity or word-error-rate.2.2 Global Linear ModelsWe follow the framework of Collins (2002; 2004),recently applied to language modeling in Roark etal.
(2004a; 2004b).
The model we propose consistsof the following components:?
GEN(a) is a set of candidate strings for anacoustic input a.
In our case, GEN(a) is a set of1000-best strings from a first-pass recognizer.?
T (w) is the parse tree for string w.?
?
(a,w) ?
Rd is a feature-vector representationof an acoustic input a together with a string w.?
??
?
Rd is a parameter vector.?
The output of the recognizer for an input a isdefined asF (a) = argmaxw?GEN(a)??
(a,w), ???
(3)In principle, the feature vector ?
(a,w) could takeinto account any features of the acoustic input a to-gether with the utterance w. In this paper we makea couple of restrictions.
First, we define the first fea-ture to be?1(a,w) = ?
log Pl(w) + log Pa(a|w)where Pl(w) and Pa(a|w) are language and acous-tic model scores from the baseline speech recog-nizer.
In our experiments we kept ?
fixed at thevalue used in the baseline recogniser.
It can thenbe seen that our model is equivalent to the modelin Eq.
2.
Second, we restrict the remaining features?2(a,w) .
.
.
?d(a,w) to be sensitive to the stringw alone.2 In this sense, the scope of this paper islimited to the language modeling problem.
As oneexample, the language modeling features might takeinto account n-grams, for example through defini-tions such as?2(a,w) = Count of the the in wPrevious work (Roark et al, 2004a; Roark et al,2004b) considered features of this type.
In this pa-per, we introduce syntactic features, which may besensitive to the parse tree for w, for example?3(a,w) = Count of S ?
NP VP in T (w)where S ?
NP VP is a context-free rule produc-tion.
Section 3 describes the full set of features usedin the empirical results presented in this paper.2Future work may consider features of the acoustic sequencea together with the string w, allowing the approach to be ap-plied to acoustic modeling.5092.2.1 Parameter EstimationWe now describe how the parameter vector ??
isestimated from a set of training utterances.
Thetraining set consists of examples (ai,wi) for i =1 .
.
.m, where ai is the i?th acoustic input, and wiis the transcription of this input.
We briefly reviewthe two training algorithms described in Roark et al(2004b), the perceptron algorithm and global condi-tional log-linear models (GCLMs).Figure 1 shows the perceptron algorithm.
It is anonline algorithm, which makes several passes overthe training set, updating the parameter vector aftereach training example.
For a full description of thealgorithm, see Collins (2004; 2002).A second parameter estimation method, whichwas used in (Roark et al, 2004b), is to optimizethe log-likelihood under a log-linear model.
Sim-ilar approaches have been described in Johnson etal.
(1999) and Lafferty et al (2001).
The objectivefunction used in optimizing the parameters isL(??)
=?ilog P (si|ai, ??)
?
C?j?2j (4)where P (si|ai, ??)
= e??(ai,si),???
?w?GEN(ai) e??(ai,w),???
.Here, each si is the member of GEN(ai) whichhas lowest WER with respect to the target transcrip-tion wi.
The first term in L(??)
is the log-likelihoodof the training data under a conditional log-linearmodel.
The second term is a regularization termwhich penalizes large parameter values.
C is a con-stant that dictates the relative weighting given to thetwo terms.
The optimal parameters are defined as???
= arg max??L(??
)We refer to these models as global conditional log-linear models (GCLMs).Each of these algorithms has advantages.
A num-ber of results?e.g., in Sha and Pereira (2003) andRoark et al (2004b)?suggest that the GCLM ap-proach leads to slightly higher accuracy than the per-ceptron training method.
However the perceptronconverges very quickly, often in just a few passesover the training set?in comparison GCLM?s cantake tens or hundreds of gradient calculations beforeconvergence.
In addition, the perceptron can be usedas an effective feature selection technique, in thatInput: A parameter specifying the number of iterations overthe training set, T .
A value for the first parameter, ?.
Afeature-vector representation ?
(a,w) ?
Rd.
Training exam-ples (ai,wi) for i = 1 .
.
.
m. An n-best list GEN(ai) for eachtraining utterance.
We take si to be the member of GEN(ai)which has the lowest WER when compared to wi.Initialization: Set ?1 = ?, and ?j = 0 for j =2 .
.
.
d.Algorithm: For t = 1 .
.
.
T, i = 1 .
.
.
m?Calculate yi = arg maxw?GEN(ai) ??
(ai,w), ????
For j = 2 .
.
.m, set ?
?j = ?
?j + ?j(ai, si) ?
?j(ai,yi)Output: Either the final parameters ?
?, or the averaged pa-rameters ?
?avg defined as ?
?avg =?t,i ?
?t,i/mT where ?
?t,i isthe parameter vector after training on the i?th training exampleon the t?th pass through the training data.Figure 1: The perceptron training algorithm.
FollowingRoark et al (2004a), the parameter ?1 is set to be some con-stant ?
that is typically chosen through optimization over thedevelopment set.
Recall that ?1 dictates the weight given to thebaseline recognizer score.at each training example it only increments featuresseen on si or yi, effectively ignoring all other fea-tures seen on members of GEN(ai).
For example,in the experiments in Roark et al (2004a), the per-ceptron converged in around 3 passes over the train-ing set, while picking non-zero values for around 1.4million n-gram features out of a possible 41 millionn-gram features seen in the training set.For the present paper, to get a sense of the relativeeffectiveness of various kinds of syntactic featuresthat can be derived from the output of a parser, weare reporting results using just the perceptron algo-rithm.
This has allowed us to explore more of the po-tential feature space than we would have been ableto do using the more costly GCLM estimation tech-niques.
In future we plan to apply GLCM parameterestimation methods to the task.3 Parse Tree FeaturesWe tagged each candidate transcription with (1)part-of-speech tags, using the tagger documented inCollins (2002); and (2) a full parse tree, using theparser documented in Collins (1999).
The modelsfor both of these were trained on the Switchboard510SNPPRPweVPVBDhelpedNPPRPherVPVBpaintNPDTtheNNhouseFigure 2: An example parse treetreebank, and applied to candidate transcriptions inboth the training and test sets.
Each transcriptionreceived one POS-tag annotation and one parse treeannotation, from which features were extracted.Figure 2 shows a Penn Treebank style parse treethat is of the sort produced by the parser.
Given sucha structure, there is a tremendous amount of flexibil-ity in selecting features.
The first approach that wefollow is to map each parse tree to sequences encod-ing part-of-speech (POS) decisions, and ?shallow?parsing decisions.
Similar representations have beenused by (Rosenfeld et al, 2001; Wang and Harper,2002).
Figure 3 shows the sequential representationsthat we used.
The first simply makes use of the POStags for each word.
The latter representations makeuse of sequences of non-terminals associated withlexical items.
In 3(b), each word in the string is asso-ciated with the beginning or continuation of a shal-low phrase or ?chunk?
in the tree.
We include anynon-terminals above the level of POS tags as poten-tial chunks: a new ?chunk?
(VP, NP, PP etc.)
beginswhenever we see the initial word of the phrase dom-inated by the non-terminal.
In 3(c), we show howPOS tags can be added to these sequences.
The finaltype of sequence mapping, shown in 3(d), makes asimilar use of chunks, but preserves only the head-word seen with each chunk.3From these sequences of categories, various fea-tures can be extracted, to go along with the n-gramfeatures used in the baseline.
These include n-tagfeatures, e.g.
ti?2ti?1ti (where ti represents the3It should be noted that for a very small percentage of hy-potheses, the parser failed to return a full parse tree.
At theend of every shallow tag or category sequence, a special end ofsequence tag/word pair ?</parse> </parse>?
was emit-ted.
In contrast, when a parse failed, the sequence consisted ofsolely ?<noparse> <noparse>?.
(a)we/PRP helped/VBD her/PRP paint/VB the/DThouse/NN(b)we/NPb helped/VPb her/NPb paint/VPb the/NPbhouse/NPc(c)we/PRP-NPb helped/VBD-VPb her/PRP-NPbpaint/VB-VPb the/DT-NPb house/NN-NPc(d)we/NP helped/VP her/NP paint/VP house/NPFigure 3: Sequences derived from a parse tree: (a) POS-tagsequence; (b) Shallow parse tag sequence?the superscripts band c refer to the beginning and continuation of a phrase re-spectively; (c) Shallow parse tag plus POS tag sequence; and(d) Shallow category with lexical head sequencetag in position i); and composite tag/word features,e.g.
tiwi (where wi represents the word in posi-tion i) or, more complicated configurations, such asti?2ti?1wi?1tiwi.
These features can be extractedfrom whatever sort of tag/word sequence we pro-vide for feature extraction, e.g.
POS-tag sequencesor shallow parse tag sequences.One variant that we performed in feature extrac-tion had to do with how speech repairs (identified asEDITED constituents in the Switchboard style parsetrees) and filled pauses or interjections (labeled withthe INTJ label) were dealt with.
In the simplest ver-sion, these are simply treated like other constituentsin the parse tree.
However, these can disrupt whatmay be termed the intended sequence of syntacticcategories in the utterance, so we also tried skippingthese constituents when mapping from the parse treeto shallow parse sequences.The second set of features we employed madeuse of the full parse tree when extracting features.For this paper, we examined several features tem-plates of this type.
First, we considered context-freerule instances, extracted from each local node in thetree.
Second, we considered features based on lex-ical heads within the tree.
Let us first distinguishbetween POS-tags and non-POS non-terminal cate-gories by calling these latter constituents NTs.
Foreach constituent NT in the tree, there is an associ-ated lexical head (HNT) and the POS-tag of that lex-ical head (HPNT).
Two simple features are NT/HNTand NT/HPNT for every NT constituent in the tree.511Feature Examples from figure 2(P,HCP,Ci,{+,-}{1,2},HP,HCi ) (VP,VB,NP,1,paint,house)(S,VP,NP,-1,helped,we)(P,HCP,Ci,{+,-}{1,2},HP,HPCi ) (VP,VB,NP,1,paint,NN)(S,VP,NP,-1,helped,PRP)(P,HCP,Ci,{+,-}{1,2},HPP,HCi ) (VP,VB,NP,1,VB,house)(S,VP,NP,-1,VBD,we)(P,HCP,Ci,{+,-}{1,2},HPP,HPCi ) (VP,VB,NP,1,VB,NN)(S,VP,NP,-1,VBD,PRP)Table 1: Examples of head-to-head features.
The examplesare derived from the tree in figure 2.Using the heads as identified in the parser, examplefeatures from the tree in figure 2 would be S/VBD,S/helped, NP/NN, and NP/house.Beyond these constituent/head features, we canlook at the head-to-head dependencies of the sortused by the parser.
Consider each local tree, con-sisting of a parent node (P), a head child (HCP), andk non-head children (C1 .
.
.
Ck).
For each non-headchild Ci, it is either to the left or right of HCP, and iseither adjacent or non-adjacent to HCP.
We denotethese positional features as an integer, positive if tothe right, negative if to the left, 1 if adjacent, and 2 ifnon-adjacent.
Table 1 shows four head-to-head fea-tures that can be extracted for each non-head childCi.
These features include dependencies betweenpairs of lexical items, between a single lexical itemand the part-of-speech of another item, and betweenpairs of part-of-speech tags in the parse.4 ExperimentsThe experimental set-up we use is very similar tothat of Roark et al (2004a; 2004b), and the exten-sions to that work in Roark et al (2005).
We makeuse of the Rich Transcription 2002 evaluation testset (rt02) as our development set, and use the RichTranscription 2003 Spring evaluation CTS test set(rt03) as test set.
The rt02 set consists of 6081 sen-tences (63804 words) and has three subsets: Switch-board 1, Switchboard 2, Switchboard Cellular.
Thert03 set consists of 9050 sentences (76083 words)and has two subsets: Switchboard and Fisher.The training set consists of 297580 transcribedutterances (3297579 words)4.
For each utterance,4Note that Roark et al (2004a; 2004b; 2005) used 20854 ofthese utterances (249774 words) as held out data.
In this workwe simply use the rt02 test set as held out and development data.a weighted word-lattice was produced, represent-ing alternative transcriptions, from the ASR system.The baseline ASR system that we are comparingagainst then performed a rescoring pass on these firstpass lattices, allowing for better silence modeling,and replaces the trigram language model score witha 6-gram model.
1000-best lists were then extractedfrom these lattices.
For each candidate in the 1000-best lists, we identified the number of edits (inser-tions, deletions or substitutions) for that candidate,relative to the ?target?
transcribed utterance.
The or-acle score for the 1000-best lists was 16.7%.To produce the word-lattices, each training utter-ance was processed by the baseline ASR system.
Ina naive approach, we would simply train the base-line system (i.e., an acoustic model and languagemodel) on the entire training set, and then decodethe training utterances with this system to producelattices.
We would then use these lattices with theperceptron algorithm.
Unfortunately, this approachis likely to produce a set of training lattices that arevery different from test lattices, in that they will havevery low word-error rates, given that the lattice foreach utterance was produced by a model that wastrained on that utterance.
To somewhat control forthis, the training set was partitioned into 28 sets, andbaseline Katz backoff trigram models were built foreach set by including only transcripts from the other27 sets.
Lattices for each utterance were producedwith an acoustic model that had been trained on theentire training set, but with a language model thatwas trained on the 27 data portions that did not in-clude the current utterance.
Since language mod-els are generally far more prone to overtraining thanstandard acoustic models, this goes a long way to-ward making the training conditions similar to test-ing conditions.
Similar procedures were used totrain the parsing and tagging models for the trainingset, since the Switchboard treebank overlaps exten-sively with the ASR training utterances.Table 2 presents the word-error rates on rt02 andrt03 of the baseline ASR system, 1000-best percep-tron and GCLM results from Roark et al (2005)under this condition, and our 1000-best perceptronresults.
Note that our n-best result, using just n-gram features, improves upon the perceptron resultof (Roark et al, 2005) by 0.2 percent, putting uswithin 0.1 percent of their GCLM result for that512WERTrial rt02 rt03ASR system output 37.1 36.4Roark et al (2005) perceptron 36.6 35.7Roark et al (2005) GCLM 36.3 35.4n-gram perceptron 36.4 35.5Table 2: Baseline word-error rates versus Roark et al (2005)rt02Trial WERASR system output 37.1n-gram perceptron 36.4n-gram + POS (1) perceptron 36.1n-gram + POS (1,2) perceptron 36.1n-gram + POS (1,3) perceptron 36.1Table 3: Use of POS-tag sequence derived featurescondition.
(Note that the perceptron?trained n-gramfeatures were trigrams (i.e., n = 3).)
This is due toa larger training set being used in our experiments;we have added data that was used as held-out data in(Roark et al, 2005) to the training set that we use.The first additional features that we experimentedwith were POS-tag sequence derived features.
Letti and wi be the POS tag and word at position i,respectively.
We experimented with the followingthree feature definitions:1.
(ti?2ti?1ti), (ti?1ti), (ti), (tiwi)2.
(ti?2ti?1wi)3.
(ti?2wi?2ti?1wi?1tiwi), (ti?2ti?1wi?1tiwi),(ti?1wi?1tiwi), (ti?1tiwi)Table 3 summarizes the results of these trials onthe held out set.
Using the simple features (num-ber 1 above) yielded an improvement beyond justn-grams, but additional, more complicated featuresfailed to yield additional improvements.Next, we considered features derived from shal-low parsing sequences.
Given the results from thePOS-tag sequence derived features, for any given se-quence, we simply use n-tag and tag/word features(number 1 above).
The first sequence type fromwhich we extracted features was the shallow parsetag sequence (S1), as shown in figure 3(b).
Next,we tried the composite shallow/POS tag sequence(S2), as in figure 3(c).
Finally, we tried extract-ing features from the shallow constituent sequence(S3), as shown in figure 3(d).
When EDITED andrt02Trial WERASR system output 37.1n-gram perceptron 36.4n-gram + POS perceptron 36.1n-gram + POS + S1 perceptron 36.1n-gram + POS + S2 perceptron 36.0n-gram + POS + S3 perceptron 36.0n-gram + POS + S3-E perceptron 36.0n-gram + POS + CF perceptron 36.1n-gram + POS + H2H perceptron 36.0Table 4: Use of shallow parse sequence and full parse derivedfeaturesINTJ nodes are ignored, we refer to this conditionas S3-E. For full-parse feature extraction, we triedcontext-free rule features (CF) and head-to-head fea-tures (H2H), of the kind shown in table 1.
Table 4shows the results of these trials on rt02.Although the single digit precision in the tabledoes not show it, the H2H trial, using features ex-tracted from the full parses along with n-grams andPOS-tag sequence features, was the best performingmodel on the held out data, so we selected it for ap-plication to the rt03 test data.
This yielded 35.2%WER, a reduction of 0.3% absolute over what wasachieved with just n-grams, which is significant atp < 0.001,5 reaching a total reduction of 1.2% overthe baseline recognizer.5 ConclusionThe results presented in this paper are a first step inexamining the potential utility of syntactic featuresfor discriminative language modeling for speechrecognition.
We tried two possible sets of featuresderived from the full annotation, as well as a va-riety of possible feature sets derived from shallowparse and POS tag sequences, the best of whichgave a small but significant improvement beyondwhat was provided by the n-gram features.
Futurework will include a further investigation of parser?derived features.
In addition, we plan to explore thealternative parameter estimation methods describedin (Roark et al, 2004a; Roark et al, 2004b), whichwere shown in this previous work to give further im-provements over the perceptron.5We use the Matched Pair Sentence Segment test for WER,a standard measure of significance, to calculate this p-value.513ReferencesEugene Charniak.
2001.
Immediate-head parsing for languagemodels.
In Proc.
ACL.Ciprian Chelba and Frederick Jelinek.
1998.
Exploiting syntac-tic structure for language modeling.
In Proceedings of the36th Annual Meeting of the Association for ComputationalLinguistics and 17th International Conference on Computa-tional Linguistics, pages 225?231.Ciprian Chelba and Frederick Jelinek.
2000.
Structuredlanguage modeling.
Computer Speech and Language,14(4):283?332.Ciprian Chelba.
2000.
Exploiting Syntactic Structure for Nat-ural Language Modeling.
Ph.D. thesis, The Johns HopkinsUniversity.Stanley Chen and Joshua Goodman.
1998.
An empirical studyof smoothing techniques for language modeling.
TechnicalReport, TR-10-98, Harvard University.Michael J. Collins.
1999.
Head-Driven Statistical Modelsfor Natural Language Parsing.
Ph.D. thesis, University ofPennsylvania.Michael Collins.
2002.
Discriminative training methods forhidden markov models: Theory and experiments with per-ceptron algorithms.
In Proc.
EMNLP, pages 1?8.Michael Collins.
2004.
Parameter estimation for statisticalparsing models: Theory and practice of distribution-freemethods.
In Harry Bunt, John Carroll, and Giorgio Satta,editors, New Developments in Parsing Technology.
KluwerAcademic Publishers, Dordrecht.Frederick Jelinek and John Lafferty.
1991.
Computation ofthe probability of initial substring generation by stochas-tic context-free grammars.
Computational Linguistics,17(3):315?323.Mark Johnson, Stuart Geman, Steven Canon, Zhiyi Chi, andStefan Riezler.
1999.
Estimators for stochastic ?unification-based?
grammars.
In Proc.
ACL, pages 535?541.Daniel Jurafsky, Chuck Wooters, Jonathan Segal, AndreasStolcke, Eric Fosler, Gary Tajchman, and Nelson Morgan.1995.
Using a stochastic context-free grammar as a lan-guage model for speech recognition.
In Proceedings of theIEEE Conference on Acoustics, Speech, and Signal Process-ing, pages 189?192.John Lafferty, Andrew McCallum, and Fernando Pereira.
2001.Conditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proc.
ICML, pages282?289, Williams College, Williamstown, MA, USA.Andrej Ljolje, Enrico Bocchieri, Michael Riley, Brian Roark,Murat Saraclar, and Izhak Shafran.
2003.
The AT&T 1xRTCTS system.
In Rich Transcription Workshop.Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, AnoopSarkar, Kenji Yamada, Alex Fraser, Shankar Kumar, LibinShen, David Smith, Katherine Eng, Viren Jain, Zhen Jin, andDragomir Radev.
2004.
A smorgasbord of features for sta-tistical machine translation.
In Proceedings of HLT-NAACL2004.Brian Roark, Murat Saraclar, and Michael Collins.
2004a.
Cor-rective language modeling for large vocabulary ASR with theperceptron algorithm.
In Proc.
ICASSP, pages 749?752.Brian Roark, Murat Saraclar, Michael Collins, and Mark John-son.
2004b.
Discriminative language modeling with condi-tional random fields and the perceptron algorithm.
In Proc.ACL.Brian Roark, Murat Saraclar, and Michael Collins.
2005.
Dis-criminative n-gram language modeling.
Computer Speechand Language.
submitted.Brian Roark.
2001a.
Probabilistic top-down parsing and lan-guage modeling.
Computational Linguistics, 27(2):249?276.Brian Roark.
2001b.
Robust Probabilistic PredictiveSyntactic Processing.
Ph.D. thesis, Brown University.http://arXiv.org/abs/cs/0105019.Ronald Rosenfeld, Stanley Chen, and Xiaojin Zhu.
2001.Whole-sentence exponential language models: a vehicle forlinguistic-statistical integration.
In Computer Speech andLanguage.Fei Sha and Fernando Pereira.
2003.
Shallow parsing withconditional random fields.
In Proceedings of the HumanLanguage Technology Conference and Meeting of the NorthAmerican Chapter of the Association for Computational Lin-guistics (HLT-NAACL), Edmonton, Canada.Andreas Stolcke and Jonathan Segal.
1994.
Precise n-gramprobabilities from stochastic context-free grammars.
In Pro-ceedings of the 32nd Annual Meeting of the Association forComputational Linguistics, pages 74?79.Andreas Stolcke.
1995.
An efficient probabilistic context-freeparsing algorithm that computes prefix probabilities.
Com-putational Linguistics, 21(2):165?202.Wen Wang and Mary P. Harper.
2002.
The superARV languagemodel: Investigating the effectiveness of tightly integratingmultiple knowledge sources.
In Proc.
EMNLP, pages 238?247.Wen Wang, Andreas Stolcke, and Mary P. Harper.
2004.
Theuse of a linguistically motivated language model in conver-sational speech recognition.
In Proc.
ICASSP.Wen Wang.
2003.
Statistical parsing and language model-ing based on constraint dependency grammar.
Ph.D. thesis,Purdue University.Peng Xu, Ciprian Chelba, and Frederick Jelinek.
2002.
Astudy on richer syntactic dependencies for structured lan-guage modeling.
In Proceedings of the 40th Annual Meet-ing of the Association for Computational Linguistics, pages191?198.Peng Xu, Ahmad Emami, and Frederick Jelinek.
2003.
Train-ing connectionist models for the structured language model.In Proc.
EMNLP, pages 160?167.514
