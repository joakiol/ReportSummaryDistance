Disambiguating Highly AmbiguousWordsGeof f rey  Towell*Siemens Corporate ResearchEl len M. Voorhees*Siemens Corporate ResearchA word sense disambiguator that is able to distinguish among the many senses of commonwords that are found in general-purpose, broad-coverage lexicons would be useful.
For example,experiments have shown that, given accurate sense disambiguation, the lexical relations encodedin lexicons uch as WordNet can be exploited to improve the effectiveness of information retrievalsystems.
This paper describes a classifier whose accuracy may be sufficient for such a purpose.
Theclassifier combines the output of a neural network that learns topical context with the output ofa network that learns local context o distinguish among the senses of highly ambiguous words.The accuracy of the classifier is tested on three words, the noun line, the verb serve, andthe adjective hard; the classifier has an average accuracy of 87%, 90%, and 81%, respectively,when forced to choose a sense for all test cases.
When the classifier is not forced to choose a senseand is trained on a subset of the available senses, it rejects test cases containing unknown sensesas well as test cases it would misclassify if forced to select a sense.
Finally, when there are fewlabeled training examples available, we describe an extension of our training method that usesinformation extracted from unlabeled examples to improve classification accuracy.1.
IntroductionAn information retrieval system returns documents presumed to be of interest o theuser in response to a query.
While there are a variety of different ways the retrievalcan be accomplished, most systems treat he query as a pattern to be matched by doc-uments.
Unfortunately, the effectiveness of these word-matching systems i  depressedby both homographs and synonyms.
Homographs depress the accuracy of the retrievalsystems by making texts about wo different concepts appear to match.
Synonyms im-pair the system's ability to find all matching documents, ince different words maskconceptual matches.
While polysemy is the immediate cause of the first problem, itindirectly contributes to the second problem as well by preventing the effective use ofthesauri.
These considerations motivate our desire for a highly accurate word sensedisambiguator.Our experimental results show that the disambiguator described in this paper isquite accurate.
The disambiguator is a particular formulation of feed-forward neuralnetworks (Rumelhart, Hinton, and Williams 1986) that separately extract opical andlocal contexts of a target word from a set of sample sentences that are tagged with thecorrect sense of the target.
The neural networks responsible for topical and local dis-ambiguation are then combined to form a single, "contextual" representation (Millerand Charles 1991).
Further experiments show that the accuracy of the contextual dis-ambiguator can be improved if the disambiguator is allowed to label some examples as?
Siemens Corporate Research, 755 College Road East, Princeton, NJ 08540t Currently at: National Institute of Standards and Technology, Building 225, Room A-216, Gaithersburg,MD, 20899(~) 1998 Association for Computational LinguisticsComputational Linguistics Volume 24, Number 1unknown.
Since the accumulation of sufficient agged samples is expensive and time-consuming, we finish by describing an extension of our algorithm through which itsaccuracy can be enhanced by using inexpensive untagged examples.Our long-term goal is to be able to incorporate such a contextual disambigua-tion system within a taxonomy such as WordNet (Miller 1990) and thereby to use itfor resolving query word senses at retrieval run-time.
To accomplish this goal, thedisambiguator must be able to construct contextual representations that accuratelydistinguish among the highly ambiguous words found in general-purpose lexicons aswell as build representations that are efficient o use at query run-time.
The systemdescribed in this paper represents a significant step towards that goal.2.
Effects of Polysemy on Retrieval PerformanceThe effectiveness of information retrieval systems is usually measured in terms ofprecision, the percentage of retrieved ocuments that are relevant, and recall, the per-centage of relevant documents that are retrieved.
As mentioned above, in principle,the direct effect of polysemy on word-matching systems is to decrease precision (e.g.,queries about financial banks retrieve documents about rivers).
The impact this directeffect has in practice is less clear.
Schtitze and Pedersen (1995) found noticeable im-provement in precision using sense-based (as opposed to word-based) retrieval.
Onthe other hand, Krovetz and Croft (1992) concluded that polysemy hurt retrieval onlyif the searcher needed very high recall or was using very short (one or two word)queries.
Sanderson (1994) found that resolving senses could degrade retrieval perfor-mance unless the disambiguation procedure was very accurate, although he workedwith large, rich queries.
Other techniques also address the polysemy problem withoutrequiring explicit disambiguation.
One such technique is local-global matching (Saltonand Buckley 1991), where the similarity of a document with a query depends not onlyon the words occurring in the entire document but also on the existence of smallerlexical units, such as sentences, that exhibit particularly close matches with the query.These techniques implicitly accommodate ambiguity: by computing similarity mea-sures based on word co-occurrence, the systems find instances of words used in thesame contexts and thus words that are used in the same sense.Polysemy has a second, indirect effect, however, in that it hampers the successfulapplication of thesauri.
Much as using the same word in different senses can depressprecision by causing false matches, using different words to express the same sense(i.e., synonyms) depresses recall by causing true conceptual matches to be missed.One way to mitigate the effects of synonyms is to use lexical aids to expand a text(usually the query) by words that are closely related to words in the original text.This procedure has met with some success in experiments on small, single-domaincollections.
For example, Salton and Lesk (1971) found that expansion by synonymsonly improved performance, and Wang, Vendendorpe, and Evens (1985) found thata variety of lexical-semantic relations improved retrieval performance.
However, it isdifficult to obtain similar improvements in heterogeneous collections where the lexicalaids necessarily contain multiple senses of words (Voorhees and Hou 1993; Voorhees1994a, 1994b).Selecting the correct sense of a word to be expanded is an essential first step whenexpanding queries by lexically related words.
Experiments run on the diverse TRECtest collection (Harman 1993), and using the WordNet lexical system as a source ofrelated words, demonstrate hat expanding queries by the set of words such that eachword in the set is related to some (any) sense of the query word consistently degradesperformance compared to the base run in which no expansion is done (Voorhees and126Towell and Voorhees Disambiguating Highly Ambiguous WordsHou 1993).
However, query expansion by lexically related words can significantlyimprove retrieval effectiveness: additional experiments in which hand-selected Word-Net synonym sets were used as seeds for expansion improved retrieval performanceby over 30% (Voorhees 1994b).
Because the process used in hand-picking the seedsynonym sets encompassed more than simple sense resolution--other considerationssuch as specificity of the sense and perceived usefulness of the concept also playeda part--simply finding the correct sense of the query terms is not likely to producethis large an improvement.
Nonetheless, ignificant improvement should be possibleif the correct sense can be determined.Unfortunately, determining the correct sense of a query word using simply theparadigmatic relations that organize WordNet and other thesauri is unlikely to besuccessful (Voorhees 1993).
1Instead, the word sense disambiguation literature stronglysuggests that syntagmatic relations are important for sense resolution.
For example,consider the word board and the noun hierarchy of WordNet.
Each of nail, hammer,and carpenter is a good clue for the 'lumber' sense of board, but each is closest o someother sense of board in WordNet when distance is measured by the number of IS-Alinks between the respective nodes.An ideal lexical system would therefore incorporate both paradigmatic and syn-tagmatic relations.
An automatic text retrieval system could exploit such a combinedlexical system by first using the syntagmatic relations to resolve word senses, and thenadding both paradigmatic- and syntagmatic-related words to the query.
The WordNetexpansion experiments discussed above (Voorhees 1994b) suggest that paradigmatic-related words are useful for expansion, while the success of retrieval techniques suchas relevance feedback (Salton and Buckley 1990) demonstrates the usefulness of ex-pansion by syntagmatic-related words.
Since the different relations link quite differentsets of words, the combined effect should be complimentary, resulting in greater im-provement than either type of expansion alone.To test this conjecture, we must build a lexical system that contains both typesof relations.
This in turn requires capturing the syntagmatic relations associated withthe various senses contained within a particular paradigmatic lexicon.
A word sensedisambiguator that can capture these relations is described in the remainder of thepaper.3.
Extracting Contextual RepresentationsCapturing syntagmatic relations is equivalent to creating contextual representationsfor the words within the lexicon.
Miller and Charles (1991) define a contextual repre-sentation as a characterization f the linguistic ontexts in which a word appears.
Inearlier work, we demonstrated that contextual representations consisting of both localand topical components are effective for resolving word senses and can be automat-ically extracted from sample texts (Leacock, Towell, and Voorhees 1996).
The topicalcomponent consists of substantive words that are likely to co-occur with a given senseof the target word.
Word order and grammatical inflections are not used in topicalcontext.
In contrast, the local component includes information on word order, dis-tance, and some information about syntactic structure; it includes all tokens (wordsand punctuation marks) in the immediate vicinity of the target word.
Inclusion of alocal component is motivated in part by a study that showed that Princeton University1 Paradigmatic relations refer to the generalization/specialization relations that give instances orexamples ofrelated words: e.g., plant, flower, tulip.
In contrast, syntagmatic relations define words thatfrequently co-occur or are used in similar contexts: e.g., flower, garden, hoe.127Computational Linguistics Volume 24, Number 1undergraduates were more accurate at resolving word senses when given completesentences than when given only an alphabetized list of content words appearing inthe sentences (Leacock, Towell, and Voorhees 1996).In this paper, we continue to explore contextual representations by using neuralnetworks to extract both topical and local contexts and combining the results of the twonetworks into a single word sense classifier.
While V6ronis and Ide (1990) also use largeneural networks to resolve word senses, their approach is quite different from ours.V6ronis and Ide use a spreading activation algorithm on a network whose structure isautomatically extracted from dictionary definitions.
In contrast, we use feed-forwardnetworks that learn salient features of context from a set of tagged training examples.Many researchers have used learning algorithms to derive a disambiguationmethod from a training corpus.
For example, Hearst (1991) uses orthographic, syn-tactic, and lexical features of the target and local context o train on.
Yarowsky (1993)and Leacock, Towell, and Voorhees (1996) also found that local context is a highlyreliable indicator of sense.
However, their results uniformly confirm that all too oftenthere is not enough local information available for the classifiers to make a decision.Gale, Church, and Yarowsky (1992) developed a topical classifier based on Bayesiandecision theory.
The classifier trains on all and only alphanumeric characters and punc-tuation strings in the training corpus.
Leacock, Towell, and Voorhees (1996), comparingperformance of the Bayesian classifier with a vector-space model used in informationretrieval systems (Salton, Wong, and Yang 1975) and with a neural network, found thatthe neural networks had superior performance.
Black (1988) trained on high-frequencylocal and topical context using a method based upon decision trees.
While Black's re-sults were encouraging, our attempt to use C4.5 (a decision-tree algorithm \[Quinlan1992\]) on the topical encoding of line were uniformly disappointing (Leacock, Towell,and Voorhees 1993).The efficacy of our classifier is tested on three words, each a highly polysemousinstance of a different part of speech: the noun line, the verb serve, and the adjectivehard.
The senses tested for each word are listed in Table 1.
We restrict he test tosenses within a single part of speech to focus the work on the harder part of the dis-ambiguation problem--the accuracy of simple stochastic part-of-speech taggers uchas Brill's (Brill 1992) suggests that distinguishing among senses with different partsof speech can readily be accomplished.
The data set we use is identical to that ofLeacock, Chodorow and Miller (this volume) with two exceptions.
First, we do notuse part-of-speech tags.
Second, we use exactly the same number of examples for eachsense.To create data sets with an equal number of examples of each sense, we tookthe complete set of labeled examples for a word and randomly subsampled it so thatall senses occurred equally often in our subsample.
This meant hat all examples ofthe least frequent sense appeared in every subsample.
We repeated this procedurethree times for each word.
The same three subsamples were used in all of the experi-ments reported below.
Analysis of variance studies have never detected a statisticallysignificant difference between the subsamples.We used the same number of examples of each sense to eliminate any confoundingeffects of occurrence frequency.
We do this because the frequency with which differentsenses occur in a corpus varies depending on the corpus type (the Wall Street Journalhas many more instances of the 'product line' sense of line than other senses of line,for example) and can be difficult to estimate.
Using an equal number of examplesper sense makes the problem more challenging than it is likely to be in practice.
Forexample, Yarowsky (1993) has demonstrated that exploiting frequency informationcan improve disambiguation accuracy.
Indeed, if we had retained all examples of the128Towell and Voorhees Disambiguating Highly Ambiguous WordsTable 1Word senses used in this study.serve - verb hard - adjective line - nounsupply with food not easy (difficult) producthold an office not soft (metaphoric) phonefunction as something not soft (physical) textprovide a service corddivisionformationNumber of Occurrences of the Least Frequent Sense350 350 349'product' sense of l ine from the Wal l  Street Journal, then we could have improved uponthe results presented in the next section by simply always guessing 'product'.4.
Neura l -Network-based Sense D isambiguat ionThis section summarizes a series of experiments hat tests whether neural networks canextract sufficient information from sample usages to accurately resolve word senses.We choose neural networks as the learning method for this study because our previouswork has shown neural networks to be more effective than several other methodsof sense disambiguation (Leacock, Towell, Voorhees 1996).
Moreover, there is ampleempirical evidence which indicates that neural networks are at least as effective asother learning systems on most problems (Shavlik, Mooney, and Towell 1991; Atlaset al 1989).
The major drawback to neural networks is that they may require a largeamount of training time.
For our purposes, training time is not an issue, since it may bedone off-line.
However, the time required to classify an example is significant.
Becauseof its complexity, our approach will almost certainly be slower than methods uch asdecision trees.
Still, the time to classify an example will most likely be dominatedby the time required to transform an example into the appropriate format for inputto the classifier.
This time will be roughly uniform across classification strategies, sothe difference in the speed of the various classification methods themselves should beunnoticable.The first part of the section presents learning curves that plot accuracy versus thenumber of samples in the training set for each of: topical context only, local contextonly, and a combination of topical and local contexts.
The curves demonstrate hat theclassifiers are able to distinguish among the different senses.
Further, the results showthat the combined classifier (i.e., a classifier that uses both topical and local contexts)is at least as good as, and is usually significantly better than, a classifier that usesonly a single component.
The second subsection is motivated by the observation thatit is unlikely that a real-world training set will contain examples of all possible sensesof a word.
Hence, we investigate the effect on classification accuracy of senses thatare missing from the labeled training set.
For this investigation, we slightly modifythe classification procedure to allow a do not know response.
With this modificationthe method rejects unknown samples at rates significantly better than chance; thismodification also tends to reject examples that would have been misclassified by theunmodif ied classifier.
Since labeled training data are rare and expensive, the finalsubsection describes SULU, a method for learning accurate classifiers from a small129Computational Linguistics Volume 24, Number 1number of labeled training examples plus a larger number of unlabeled examples.Experimental results demonstrate hat SULU consistently and significantly improvesclassification accuracy when there are few labeled training examples.4.1 Asymptotic AccuracyAll of the neural networks used here are strictly feed-forward (Rumelhart, Hinton,and Williams 1986).
By this, we mean that there is a set of input units that receiveactivation only from outside the network.
The input units pass their activation on tohidden units via weighted links.
The hidden units, in turn, pass information on toeither additional hidden units or to output units.
There are no recurrent links; that is,the activation sent by a unit can never, even through a series of intermediaries, beaninput to that unit.Units that are not input units receive activation only via links.
The non-input1 where x is the sum of the incoming activations units compute the function y - l+e-xweighted by the links and y is the output activation.
(The translation of words intonumbers o that this formula can be applied to word sense disambiguation is describedin the following paragraphs.)
This nonlinear function has the effect of squashing theinput into the range \[0... 1\].
Output units give the answer for our networks.
Finally,the activation of the output units is normalized so that their sum is 1.0.In all of the experiments reported below, the weights on all the links are initially setto random numbers taken from a uniform distribution over \[-0.5...  0.5\].
The networksare then trained using gradient descent algorithms (e.g., backpropagation \[Rumelhart,Hinton, and Williams 1986\]) so that the activation of the output units is similar to somedesired pattern.
Networks are trained until either each example has been presented tothe network 100 times or at least 99.5% of the training patterns are close enough to thedesired pattern that they would be considered correct.
(The meaning of "correct" willvary in our experiments, it will be clearly defined in each experiment.)
In practice, thesecond stopping criterion always obtained.The networks used in most of this work have a very simple structure: one outputtrait per sense, one input unit per token (the meaning of "token" differs between localand topical networks as described below), and no hidden units.
For both local andtopical encodings, we tested many hidden unit structures, including ones with manylayers and ones with large numbers of hidden units in a single layer.
However, withone exception described below, a structure with no hidden units consistently yieldsthe best results.
Input units are completely connected to the output units; that is, everyinput unit is linked to every output unit.
During training, the activation of the outputtrait corresponding to the correct sense has a target value of 1.0, the other outputs havea target value of 0.0.
During testing, the sense reported by the network is the outputunit with the largest activation.
An example is considered to be classified correctly ifthe sense reported by the network is the same as the tagged sense.In networks that extract opical context, the number of input units is equal to thenumber of tokens that appear three or more times in the training set, where a token isthe string remaining after text processing.
The text processing includes removing capi-talization, conflating words with common roots, and removing a set of high-frequencywords called stopwords.
To encode an example (an example is the sentence containingthe target word and usually the preceding sentence) for the network, it is tokenizedand the input units associated with the resulting tokens are set to 1.0 (regardless ofthe frequency with which the tokens appear in the example).
All other input unitsare set to 0.0.
We investigated many alternatives: both higher and lower bounds onthe frequency of occurrence in the set of examples, including stopwords, and usingfrequency of occurrence.
None of these changes has a significant impact on accuracy.130Towell and Voorhees Disambiguating Highly Ambiguous Words8giN8Figure 1Oc.
-~~OO~= ~ ~ ?~ O Hard-  local 0 ~ , , , ~ ~  ~ Hard  - topical0 ~O ~'?
"~ ~ L ine -  topical  - o -  - o L ine - local?
. '
' e~ ' "  Sere  - topical  Serve - local 0 -  O-SO0 I000 1500 2000Number of ExamplesThe effect of increasing example size on the number of input units needed for encoding.
Foreach of our three data sets and each encoding method, this figure shows the number of inputunits required to encode the examples.
Except for the endpoints, which use the entire exampleset, each point is the average of 11 random selections from the population of examples.To encode an example for a network that extracts local context, each token (wordor punctuation) is prepended with its position relative to the target adding paddingas necessary.
For example, given the sentence "John serves loyally.
", the target serves,and a desire to use three tokens on either side of the target, the input to the networkis \[-3zzz -2zzz -1John 0serves 1loyally 2.
3zzz\] where "zzz" is added as a blank asneeded.
Networks contain input units representing every resulting string within threeof the target word in the set of labeled training examples.
Note that this implies thatthere will be words in positions in the test set that are not matched in the training set.So, while training examples will have exactly seven input units with a value of 1.0,testing examples will have at most seven input units with a value of 1.0.
The windowwe use is slightly wider than a window of two words on either side that experimentswith humans suggest is sufficient (Choueka and Lusignan 1985).
The human studycounted only words, whereas we count both words and punctuation.
Our networksare significantly less accurate using windows smaller than three tokens on either side.On the other hand, wider windows are slightly, but not statistically significantly, moreaccurate.Figure 1 shows that the topical and local encoding methods result in large inputsets.
For example, when the entire population of line examples is used, the local en-coding would require 3,973 input units and the topical encoding would require 2,924inputs.
Fortunately, this figure shows that the rate of increase in the size of the in-put set steadily decreases as the size of the input set increases.
Fitting each of thelines in this figure against exponential functions indicates that none of these data setswould grow to require more than 9,000 inputs units.
While this is certainly large, it istolerable.We investigated many ways of combining the output of the topical and localnetworks.
We report results for a method that takes the max imum of the sum of theoutput units.
2 For example, suppose that a local network for disambiguating the threesenses of hard has outputs of (0.4 0.5 0.1) and a topical network has outputs of (0.42 Among the many alternatives we investigated for merging the local and topical networks, only oneyields slightly better esults.
It is based upon Wolpert's tacked generalization (Wolpert 1992).
In thistechnique, the outputs from the topical and local networks are passed into another network whosefunction is simply to learn how to combine the outputs.
When the input to the combining network isthe concatenation f the inputs and the outputs of both the local and topical networks, the combiningnetwork often outperforms our summing method.
However, the improvement is usually notstatistically significant, so we report only the results from the considerably simpler summing method.131Computational Linguistics Volume 24, Number 10.0 0.6).
Then the local information would suggest the second sense, while the topicalinformation would suggest the third sense.
The summing strategy ields (0.8 0.5 0.7),so the combined classifier would select he first sense.The only approach we have found that consistently, and statistically significantly,outperforms the strategy described above is based upon error-correcting output en-coding (Kong and Dietterich 1995).
The idea of error-correcting codes is to learn allpossible dichotomies of the set of classifications.
For example, given a problem withfour classes, A, B, C, and D, learn to distinguish A and B from C and D; A from B,C, and D; etc.
The major problem with this method is that it can be computationallyintensive when there are many output classes because there are 2 s-1 - 1 dichotomiesfor S output classes.
We implemented error-correcting output codes by independentlytraining a network with one output unit and 10 hidden units to learn each dichotomy.4.1.1 Testing Methodology.
To estimate the accuracy of our disambiguation methods,we built learning curves using several values of N in N-fold cross-validation.
I  cross-validation, the data set is randomly divided into N equal parts, then N - 1 parts areused for training and the remaining part is held aside to assess generalization.
3 As aresult, each example is used for training N - 1 times and once to assess generalization.A drawback of N-fold cross-validation is that it cannot est small portions of the data.So, for points on the learning curves that use less than 50% of the training data, weinvert the cross validation procedure, using one of the N parts for training and theremaining N - 1 parts to assess generalization.
For example, if there are 100 labeledexamples, each iteration of 10-fold cross-validation would use 90 examples for trainingand the remaining 10 for testing.
When complete, each example would be used fortraining exactly nine times and exactly once for testing.
By contrast, in inverted 10-foldcross-validation, each example is used exactly once for training and exactly nine timesfor testing.4.1.2 Results and Discussion.
The learning curves are shown in Figure 2.
Each pointin the figure represents an average over 11 cross-validation trials.
Thus, the point for75% of the training set, which corresponds to4-fold cross-validation, requires training44 networks.
The confusion matrices in Tables 2 to 4 give the complete data for thelargest raining sets of the "standard" curves in Figure 2.
Rows in the table representthe correct answer, and columns represent the answer selected by the classifier.
"Total"gives the number of times the classifier selects the given sense.
"Precision" is thepercentage of that total that is correct.
In contrast, "Percent Correct" gives the accuracyof the classifier over the set of hand-tagged examples of the given sense.Figure 2 shows that, for line and serve, the combined classifier is considerably su-perior to either the local or topical classifier at all training-set sizes.
At the largesttraining-set size, the combined classifier is superior with at least 99.5% confidence ac-cording to a one-tailed paired-sample t-test.
There is no advantage for the combinedclassifier for hard.
In fact, at the largest raining-set size, the local classifier slightly out-performs the combined classifier.
The difference, while small, is statistically significantwith 97.5% confidence according to a one-tailed paired-sample t-test.An obvious reason, as can be seen in Figure 2, for why the combined represen-tation fails to improve classification effectiveness for hard is that the topical classifier3 By randomly separating examples, it is possible that examples taken from the same document appearin the training and testing sets.
In principle, this could inflate the accuracy we report for the classifier.In practice, experiments hat explicitly control for this effect do not yield significantly different resultsfrom those we report.132Towell and Voorhees Disambiguating Highly Ambiguous WordsERHard=o(E r - -2~a-=oi l l  ~ - -  CombinedO Error Correcting i "  / .
.
.
.
.
.
Local?
Standard - -  - -  - -  Topical7b 14o 2~o 2~oNumber of example of each sense used during trainingServe.
, - ' ot~co.
~  .
.
.
.
.
.
.
.
.
.
l i>~- -Q - -~- t - ,~"" _ ~ V ~  ~'~ ~-~ - .
m-  - - t/=" .,Jr"/Combined.
.
.
.
.
.
Local 0 Error Correcting?
Standard - -  - -  - -  Topical~'0 1,i0 2to 2~oNumber of example of each sense used during trainingLine?
/ .
/  o---T.Y_--= .
.
.
.
OLO I m-- - - -~?.O J " ' ' "/ /  .
I - - - ' ' "~ .~ ?I "  - -  Combined" - .
.
.
.
.
Local O Error Correcting?
Standard - -  - -  - -  Topical710 1 40  21  0 a~0 )0Number of example of each sense used during trainingFigure 2Learning curves for classifiers that use local context only (Local), topical context only (Topical),and a combination of local and topical contexts (Combined) for hard, serve, and line.
Each pointin each curve represents an average over 11 repetitions of N-fold cross-validation.
The pointson each of these curves represent 10-, 6-, 4-, 3-, and 2-fold cross-validation and 3-, 4-, and6-fold inverted cross-validation.
Error-correcting codes have results at only 2-, 3-, and 4-foldcross-validation (i.e, 50%, 66%, and 75% of the training data.
)is much worse than the local classifier.
While the differences in accuracy between thetopical and local classifiers are statistically significant with at least 99.5% confidenceaccording to a one-tailed paired-sample t-test on all three senses, the accuracies aremore similar for both serve and line than they are for hard.
This obvious difference inaccuracy is only part of the reason why  the combined classifier is less effective for hard.Perhaps more important is the fact that the errors for the local and topical classifiers133Computational Linguistics Volume 24, Number 1Table 2Average confusion matrices for hard over 11 runs of 10-fold crossvalidation.
Rows in the table represent the correct answer, columnsrepresent the answer given by the classifier.
So, in the local table,22.1 is the average number of times that the classifier selected the'physical' sense when the correct sense was the 'difficult' sense.There are 350 examples of each class.Local-harddifficult metaphoric physical Percent Correctdifficult 301.5 26.5 22.1 86.1%metaphoric 23.2 256.4 70.5 73.2%physical 13.9 33.6 302.5 86.4%Total 338.5 316.5 395.0Precision 89.0% 81.0% 76.6%Topical-harddifficult metaphoric physical Percent Correctdifficult 210.6 98.0 41.4 60.2%metaphoric 114.8 203.6 31.5 58.2%physical 56.1 40.1 253.8 72.5%Total 381.5 341.7 326.7Precision 55.2% 59.6% 77.7%Combined-harddifficult metaphoric physical Percent Correctdifficult 299.0 35.7 15.3 85.4%metaphoric 62.3 247.8 39.9 70.8%physical 20.1 24.6 305.3 87.2%Total 381.4 308.2 360.5Precision 78.4% 80.4% 84.7%are more highly correlated for hard than for either line or serve.
The average correlationof correct and incorrect answers for local and topical classifiers of hard is 0.14 while theaverage correlations for line and serve, respectively, are 0.07 and -0.02.
Many efforts atusing ensembles of classifiers have reported that to get significant improvements,  themembers  of the ensemble should be as uncorrelated as possible (Paramanto, Munro,and Doyle 1996).
Given the correlation between the local and topical classifiers forhard, it is not surprising that the combined classifier provides no additional benefit.The confusion matrices in Tables 2 to 4 provide another piece of evidence aboutfile failure of the combined representation for hard.
Consider the pattern of responsesh)r the 'provide food' sense of serve by the local and topical classifiers (Table 3).
Inparticular, notice that the local classifier is much more likely to select the 'providefood' sense (26.1) when the correct sense is 'provide a service' than it is when thecorrect sense is 'function as' (6.7).
Conversely, the topical classifier is more likely toselect the 'provide food' sense when the correct sense is 'function as' (30.6) than whenthe correct sense is 'provide a service' (5.8).
When these tendencies are combined theyessentially offset each other.
As a result, the combined classifier is unlikely to select134Towell and Voorhees Disambiguating Highly Ambiguous WordsTable 3Average confusion matrices for serve over 11 runs of 10-fold cross validation.Rows in the table represent the correct answer, columns represent theanswer given by the classifier.
So, in the local table, 6.7 is the averagenumber of times that the classifier selected the 'food' sense when the correctsense was the 'function as' sense.
There are 350 examples of each class.Local-servefunction as service food office Percent Correctfunction as 293.7 22.9 6.7 26.6 83.9%service 5.8 312.9 26.1 5.2 89.4%food 5.7 43.4 283.4 17.5 81.0%office 41.5 14.1 18.3 276.1 78.9%Total 346.8 393.3 334.5 325.5Precision 84.7% 79.6% 84.7% 84.8%Topical-servefunction as service food office Percent Correctfunction as 218.0 51.2 30.6 50.2 62.3%service 50.0 261.1 5.8 33.1 74.6%food 22.2 4.3 308.2 15.4 88.1%office 40.1 29.9 10.8 269.2 76.9%Total 330.3 346.5 355.5 367.8Precision 66.0% 75.4% 86.7% 73.2%Combined-servefunction as service food office Percent Correctfunction as 298.4 23.4 5.8 22.5 85.2%service 14.0 321.0 5.7 9.3 91.7%food 2.5 7.5 333.8 6.1 95.4%office 22.7 8.7 3.7 314.8 89.9%Total 337.6 360.6 349.1 352.6Precision 88.4% 89.0% 95.6% 89.3%the 'provide food' sense when the correct sense is either 'function as' (5.8) or 'providea service' (5.7).This pattern of offsetting errors is repeated on all but one of the senses of lineand serve.
Wherever it occurs, the combined classifier is superior to both the localand topical classifiers.
By contrast, errors for the local and topical classifiers of hard(Table 2) rarely offset each other.
Only for the 'physical' sense do the errors offset,and that is the only sense on which the combined classifier outperforms the localclassifier.
For the 'difficult' and 'metaphoric' senses of hard, the erroneous electionsmade by the topical classifier are strictly greater than those of the local classifier.Similarly, the selection of the local classifier for the 'cord' sense of line is strictly worsethan the selections of the topical classifier.
In each of these cases, the effect of thecombined classifier is to roughly average the errors of the local and topical classifiers,with the result that the combined classifier has more errors than the better of thelocal and topical classifiers.
Unfortunately, the number of errors introduced by the135Computational Linguistics Volume 24, Number 1Table 4Average confusion matrices for line over 11 runs of 10-fold cross validation.
Rows in the tablerepresent the correct answer, columns represent the answer given by the classifier.
So, in thelocal table, 16.7 is the average number of times that the classifier selected the 'formation' sensewhen the correct sense was the 'product' sense.
There are 349 examples of each class.Local-lineproduct formation text cord division phone Percent Correctproduct 264.5 16.7 27.4 21.3 5.6 13.5 75.8%formation 32.4 248.0 20.8 31.2 4.8 11.8 71.1%text 31.8 16.5 218.7 44.5 17.5 19.9 62.7%cord 19.1 23.2 35.5 215.8 12.9 42.5 61.8%division 8.2 9.8 21.7 12.7 286.5 10.0 82.1%phone 15.9 10.2 33.7 28.1 8.2 252.9 72.5%Total 371.8 324.5 357.9 353.6 335.5 350.6Precision 71.1% 76.4% 61.1% 61.0% 85.4% 72.1%Topical-lineproduct formation text cord division phone Percent Correctproduct 294.7 13.1 17.8 1.9 9.6 11.8 84.4%formation 16.7 236.1 51.5 8.3 20.0 16.4 67.6%text 19.5 40.8 238.7 4.7 33.7 11.5 68.4%cord 3.2 6.0 13.0 311.6 4.9 10.3 89.3%division 8.5 23.4 28.7 13.5 263.4 11.6 75.5%phone 9.0 9.3 16.1 14.2 6.3 294.2 84.3%Total 351.6 328.6 365.9 354.2 337.9 355.7Precision 83.8% 71.8% 65.2% 88.0% 77.9% 82.7%Combined-lineproduct formation text cord division phone Percent Correctproduct 317.3 8.1 9.8 4.8 2.9 6.1 90.9%formation 16.6 279.3 26.4 11.2 4.8 10.7 80.0%text 13.9 16.2 280.4 11.0 16.5 11.1 80.3%cord 1.7 4.5 8.6 318.0 4.5 11.7 91.1%division 4.4 7.4 11.5 7.5 312.5 5.7 89.6%phone 5.3 5.6 11.5 12.6 2.5 311.5 89.2%Total 359.2 321.0 348.1 365.2 343.7 356.8Precision 88.3% 87.0% 80.5% 87.1% 90.9% 87.3%'difficult' and 'metaphor ic '  senses of hard more than offset the errors el iminated bythe 'physical '  sense.
Therefore, the  local classifier for hard is more accurate than thecombined classifier.The topical classifier outperforms the local classifier for the noun line (Figure 2).Conversely, the local classifier outperforms the topical classifier for the verb serveand the adjective hard.
While we hesitate to draw many conclusions from this pat-tern on the basis of so little data, the pattern is consistent with other observations.Yarowsky (1993) suggests that the sense of an adjective is almost whol ly determinedby the noun it modifies.
If this suggestion is correct, then the added information in136Towell and Voorhees Disambiguating Highly Ambiguous Wordsthe topical representation should add only confusion.
Hence, one would expect osee the local classifier outperforming the topical classifier for all adjectives.
Similarly,some verb senses are determined largely by their direct object.
For example, the 'pro-vide a service' sense of serve almost always has a thing as a direct object, while the'function as' sense of serve almost always has a person.
The added information inthe topical encoding may obscure this difference, thereby adding to the difficulty ofcorrectly disambiguating these senses.
So, we would not be surprised to see the advan-tage of local representations over topical representations continue on other verbs andadjectives.Many techniques for using local context explicitly use diagnostic phrases, suchas wait in line, for the formation sense of line.
In previous work, we took exactly thisapproach and showed that diagnostic phrases could be used to improve the accuracyof a topical classifier (Leacock, Towell, and Voorhees 1996).
Our neural network forlocal disambiguation differs considerably from this approach.
Specifically, it is unableto learn more than one diagnostic phrase per sense because it lacks hidden units.In fact, the network does not learn a single diagnostic phrase.
Instead, it learns thatcertain words in certain positions are indicative of certain senses.
While this mightappear to be a significant handicap, we have been unable to train a network that iscapable of learning phrases o that it outperforms our networks.
In addition, while theylack the ability to learn phrases, our local classifiers are, nonetheless, quite effectiveat determining the correct sense.
It is our belief that hidden units would be usefulfor learning local context given a sufficient amount of training data.
However, thereare currently far more free parameters in our networks than there are examples toconstrain those parameters.
Until there are more constraints, we do not believe thathidden units will be useful for sense disambiguation.Finally, it is interesting tonote that not all senses are equally easy, and that differentclassifiers find different senses easier than others.
For example, in Table 2 the mostdifficult sense of hard for the local classifier is the 'physical' sense, but this is theeasiest sense for both the topical and combined classifiers.
On the other hand, somesenses are just difficult.
The 'text' sense of line (Table 4) is among the hardest for allclassifiers.
We believe that the 'text' sense is difficult because it often contains quotedmaterial which may distract from the meaning of line.
However, the quoted materialis often too far away from the target word for the quotation marks to be seen in thelocal window.
As a result, the topical classifier is confused by distracting material andthe local classifier does not see the most salient feature.4.2 Senses Missing from DataThe results in the previous ection suggest that, given a sufficiently large number oflabeled examples, it is possible to combine topical and local representations i to aneffective sense classifier.
Those results, however, assume that the labeled examplesinclude all possible senses of the word to be disambiguated.
Senses not included inthe training set will be misclassified because the procedure assigns a sense to everyexample.
In this section, we allow the system to respond do not know to address theissue of senses not seen during training.In the previous ection, the sense selected by the network is the sense correspond-ing to the output unit with the largest activation.
If the output units are known torepresent all possible senses, then this is a reasonable procedure.
If, however, there isreason to believe that there may be other senses, then this procedure imparts a strong,and incorrect, bias to the classification step.
When there is reason to believe that thereare senses in the data that are not represented in the training set, we can relax thisbias by using the sense selected by the largest output activation only if that activation137Computational Linguistics Volume 24, Number 1Table 5The chance rate of correct rejection rate for each of the target words.
(All numbers are percentages.
)Target Word Overall Resulting from Resulting fromUnknown Senses Errors on Known Senseshard 47 33 14serve 32 25 7line 25 17 8is greater than a threshold.
When the maximum activation is below the threshold, thenetwork's response is do not know.The logic underlying this modification is that the activation of the output unitcorresponding to the correct answer tends to be close to 1.0 when the instance tobe classified is similar to a training example.
Hence, instances of senses seen duringtraining should have an output unit whose activation is close to 1.0 (assuming thatthe training examples adequately represent the set of possibilities).
On the other hand,instances of senses not seen during training are unlikely to be similar to any trainingexample.
So, they are unlikely to generate an activation that is close to 1.0.4.2.1 Testing Methodology.
We use a leave-one-category-out procedure to test ourhypothesis that we can detect unknown senses by screening for examples that havea low maximum output activation.
Our procedure is as follows: networks are trainedusing 90% of the examples of S - 1 senses when there are S senses for a target word.The trained network is then tested using the unused 10% of the S - 1 classes seenduring training and 10 percent of the examples of the class not seen during training(selected randomly).
In addition, during testing the network is given a threshold valueto determine whether or not to label the example.
Figure 3 shows the effect of varyingthe threshold from 0.4 to 1.0 (values below 0.4 were tried but had no effect) using thecombined classifier.
The leave-one-category-out procedure was repeated 11 times foreach sense.4.2.2 Results and Discussion.
The graphs in Figure 3 show that the slight modificationof the classifier has the hypothesized effect.
Not surprisingly, the number of examplesclassified always decreases as the threshold increases.
Also expected is that the per-centage of correctly rejected examples falls as the threshold increases--increasing thethreshold naturally catches more examples that should be accepted.
(A rejected ex-ample is one for which the classifier esponds do not know.)
The up-tick in the properrejection rate at high thresholds for line is not significant.
Of more interest is that theclassifier is always significantly better than chance at correctly rejecting examples.
Thechance rate of correct rejection is shown in Table 5.
Thus, the modification allows theclassifier to identify senses that do not appear in the training set.Figure 3 also shows that the threshold has the unanticipated benefit of rejectingmisclassified examples of known senses.
Hence, it may be desirable to use a thresholdeven when all senses of a word are represented in the training set.
The exact level ofthe threshold is a matter of choice: a low threshold admits more errors but rejects fewerexamples, while higher thresholds are more accurate but classify fewer examples.138Towell and Voorhees Disambiguating Highly Ambiguous Words?~o~o ('- (DI Q) oa_oHard~ ~?--.x 3-. x= ; Correct (only k n o w ~- - - .
.
.
.
.
.
?
-  Correct (all)o .o Properly rejected- - -~ on-  Classified~.4 01s 016 017 018 019 1.0 ThresholdServe gJ I .j; : Correct (only kno senses)o ---e- - ~- -  Correct (all) " ~- - -o .
.
.
.
-o-- Properly rejected-~  o~ Classified0.4 0.5 0.6 0.7 0'3 0.9 1.0 ThresholdLineQo .
.
.
.
~--ii _ _ _ i _ _ i i i i~e~r i !
i !
; :~ : :wn s e n s ~Classified% 0'.s 016 017 018 0!9 t.o Threshold?3Figure 3The effect of omitting one sense from the training set.
In each figure, the X-axis represents helevel of a threshold.
If the maximum output activation is below the threshold then thenetwork responds do not know.
"Correct (only known senses)" gives the accuracy of thecombined classifier on senses een during training.
"Correct (all)" gives the accuracy over allexamples.
"Properly rejected" is the percentage of all examples for which the classifierresponds do not know that are either in a novel sense or would have been misclassified.
Finally,"Classified" gives the percentage of the data for which the classifier assigns a sense.4.3 Using Small Amounts of Labeled DataAll of the above results have assumed that there exist a large number of hand-labeledexamples to use during training.
Unfortunately, this is not likely to be the case.
Ratherthan working with a number of labeled examples ufficient o approach an asymptoticlevel of accuracy, the classifiers are likely to be working with a number of labeled139Computational Linguistics Volume 24, Number 1RANDOM(min,max):return a uniformly distributed random integer between min and max, inclusiveMAIN(B,M):/* B - in \[0...I00\], controls the rate of example synthesis *//* M - controls neighborhood size during synthesis */Let: E /* a set of labeled examples */U /* a set of unlabeled examples */N /* an appropriate neural network */RepeatPermute EFor each e in Eif random(O,lO0) > B thene <- SYNTHESIZE (e, E, U, random (2, M) )TRAIN N using eUntil a stopping criterion is reachedSYNTHESIZE(e,E,U,m):Let: C /* will hold a collection of examples */For i from I to mc <- ith nearest neighbor of e in E union Uif.
((c is labeled) and (label of c not equal to label of e)) then STOPif c is not labeledcc <- nearest neighbor of c in Eif label of cc not equal to label of e then STOPadd c to Creturn an example whose input is the centroid of theinputs of the examples in C and has the class label of e.Figure 4Pseudocodefor SULU.examples barely sufficient o get them started on the learning curve.While labeled examples will likely always be rare, unlabeled text is already avail-able in huge quantities.
Theoretical results (Castelli and Cover 1995) suggest hat itshould be possible to use both labeled and unlabeled examples to produce a classi-fier that is more accurate than one based on only labeled examples.
We describe analgorithm, SULU (Supervised learning Using Labeled and Unlabeled examples), thatuses both labeled and unlabeled examples and provide empirical evidence of the al-gorithm's effectiveness (Towell 1996).4.3.1 The SULU Algorithm.
SULU uses standard neural-network supervised trainingtechniques except that it may replace a labeled example with a synthetic example.A synthetic example is a point constructed from the nearest neighbors of a labeledexample.
The criterion to stop training in SULU is also slightly modified to requirethat the network correctly classify almost every labeled example and a majority of thesynthetic examples.
For instance, the experiments reported below generate syntheticexamples 50% of the time; the stopping criterion requires that 80% of the examplesseen in a single pass through the training set (an epoch) are classified correctly.Figure 4 shows pseudocode for the SULU algorithm.
The synthesize function de-scribes the process through which an example is synthesized.
Given a labeled exampleto use as a seed, synthesize collects neighboring examples and returns an examplethat is the centroid of the collected examples with the label of the starting point.Synthesize collects neighboring examples until reaching one of the following threestopping points.
First, the maximum number of points is reached: the goal of SULU is140Towell and Voorhees Disambiguating Highly Ambiguous Wordsto get information about the local variance around known points, this criterion guar-antees locality.
Second, the next closest example to the seed is a labeled example witha different label: this criterion prevents the inclusion of obviously incorrect informa-tion in synthetic examples.
Third, the next closest example to the seed is an unlabeledexample and the closest labeled example to that unlabeled example has a differentlabel from the seed: this criterion is intended to detect borders between classificationareas in example space.4.3.2 Testing Methodology.
The following methodology is used to test SULU on eachdata set.
First, the data are split into three sets, 25% is set aside to be used for assessinggeneralization, 50% is stripped of sense labels, and the remaining 25% is used fortraining.
To create learning curves, the training set is further subdivided into sets of5%, 10%, 15%, 20%, and 25% of the data, such that smaller sets are always subsets oflarger sets.
Then, a single neural network (of the structure described in Section 4.1)is created and copied 25 times.
At each training-set size, a new copy of the networkis trained under each of the following conditions: (1) using SULU, (2) using SULU butsupplying only the labeled training examples to synthesize, (3) standard networktraining, (4) using a re-implementation f an algorithm proposed by Yarowsky (1995),and (5) using standard network training but with all training examples labeled toestablish an upper bound.
This procedure is repeated 11 times to average out theeffects of example selection and network initialization.Yarowsky's algorithm expands the region of known, labeled examples out froma small set of hand-labeled seed collocations.
Our instantiation of Yarowsky's algo-rithm differs from the original in three ways.
First, we use neural networks whereasYarowsky uses decision lists.
This difference is almost certainly not significant; in de-scribing his algorithm, Yarowsky notes that a neural network could be used in place ofdecision lists.
Second, we omit the application of the one-sense-per-discourse heuris-tic, as our examples are not part of a larger discourse.
This heuristic ould be equallyapplied to SULU, so eliminating this heuristic from Yarowsky's algorithm places thealgorithms on an equal base.
Finally, we randomly pick the initially labeled contexts.The effect of this difference could be significant.
However, this difference would affectour system as well as Yarowsky's, so it should not invalidate our comparison.When SULU is used, synthetic examples replace labeled examples 50% of the time.Networks using the full SULU (condition i above) are trained until 80% of the examplesin a single epoch are correctly classified.
All other networks are trained until at least99.5% of the examples are correctly classified.4.3.3 Results and Discussion.
The graphs in Figure 5 show the efficacy of the com-bined classifier for each algorithm on each of our three target words.
SULU always re-suits in a statistically significant improvement over the standard neural network withat least 97.5% confidence (according to a one-tailed paired-sample t-test).
Interestingly,SULU'S improvement is consistently between ?
and ?
of that achieved by labeling theunlabeled examples.
This result contrasts with Castelli and Cover's (1995) analysisthat suggests that labeled examples are exponentially more valuable than unlabeledexamples.SULU is consistently and significantly superior to our version of Yarowsky's al-gorithm when there are few labeled examples.
As the number of labeled examplesincreases the advantage of SULU decreases.
At the largest raining-set sizes tested, thetwo systems are roughly equally effective.A possible criticism of SULU is that it does not actually need the unlabeled ex-amples; the procedure may be as effective using only the labeled training data.
This141Computational Linguistics Volume 24, Number 1Hardstandard with 700 labeledc~ _ ~ SULU with 700 unlabeled- -  - Yarowsky with 700 unlabeled~ 0 " _ ~  - - - - - -  SULU with 0 unlabeledEE eq ~ + Statistically superior to SULU" ~  ~ ~"O'---.- 0 Statistically inferior to SULU : "~" "'~-.I-.-~-- .
.
.
.
~ .
.
.
.
+.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
oo"rs'0 160 lg0 260 2g0Size of labeled training setServe-?O_C ,..r~?J~oO.4?o0~oO.
"~LO~oO.0standard with 700 labeledSULU with 700 unlabeled +~.
.
.
.
.
.
Yarowsky with 700 unlabeledSULU with 0 unlabeled?
~ ~-  + Statistically superior to SULU-~ 0 Statistically inferior to SULU0""160 1~0 200 2g0 360 350Size of labeled training setL ine- -  - -  standard with 1046 labeledSULU with 1046 unlabeled- Yarowsky with 1046 unlabeled- - - - - -  SULU with 0 unlabeled+ Statistically superior to SULU~ ~0 Statistically inferior to SULU.~0  - -8t .
.
.
.
.
8-7- -~- -~-8  .
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
)0 1 SO 200 2gO 360 3gO 460 4gO 500Size of labeled training setFigure 5The effect of five training procedures on the target words.
In each of the above graphs, theeffect of standard neural learning has been subtracted from all results to suppress the increasein accuracy that results simply from an increase in the number of labeled training examples.Observations marked by a "o" or a "+", respectively, indicate that the point is statisticallysignificantly inferior or superior to a network trained using SULU.hypothesis is incorrect.
As shown in Figure 5, when SULU is given no unlabeled ex-amples it is consistently and significantly inferior to SULU when it is given a largenumber of unlabeled examples.
In addition, sugu with no unlabeled examples is con-sistently, although not a lways significantly, inferior to a standard neural network (datanot shown).An indication that there is room for improvement  in SULU is the difference in142Towell and Voorhees Disambiguating Highly Ambiguous Wordsgeneralization between sucu and a network trained using data in which the unlabeledexamples provided to SULU have labels (condition 5 above).
On every data set, thegain from labeling the examples i  statistically significant.
The accuracy of a networktrained with all labeled examples is an upper bound for SULU, and one that is likelynot reachable.
However, the distance between this upper bound and SULU'S currentperformance indicates that there is room for improvement.5.
Conc lus ionThe goal of our sense disambiguation work is to develop a classifier that allows in-formation retrieval systems to exploit the semantics encoded in lexical systems uchas WordNet o improve retrieval performance.
To be useful in that environment, theclassifier must be effective at distinguishing the senses included in the lexicon andefficient enough to use during query processing.
As a first step towards this goal, wehave developed a classifier that is able to select he sense of a single highly ambiguousword given the two-sentence ontext in which the word appears.We tested our sense disambiguation approach on three highly polysemous words:six noun senses of line, four verb senses of serve, and three adjective senses of hard.The performance of our disambiguator n these three tasks was quite good; it has anaverage accuracy of 87%, 90%, and 81%, respectively, when it is forced to label all testexamples.
The labeling accuracy of our method can be further improved by allowingit to respond o not know on a small percentage of the test examples.While our current plan is to resolve the sense of each query term independently,some modifications tothe current classifier may provide for the simultaneous classifi-cation of multiple polysemous words in a single context.
By changing from backprop-agation to the EM algorithm (Dempster, Laird, and Rubin 1977), we can jointly refinepoor guesses of the senses of the words with feedback from prior iterations.Our desire to use the sense classifier as part of a query-processing step influencedthe types of classifiers we considered.
The error-correcting codes networks discussedin Section 4.1 offer the potential for slightly higher accuracy rates than our simplesum combination, but at a significantly higher cost in time and space.
Hence, weconcentrated our effort on a simple scheme for combining local and topical neuralnetworks using a sum of the output activations.
Using this method, the expense ofusing the classifier would be dominated by the time and space requirements neededto break the query into tokens and to map those tokens to the correct input units ofthe various networks.The bottleneck in our approach to query processing is obtaining sufficient labeledexamples for the set of polysemous words in a large lexicon.
To minimize the bot-tleneck as much as possible, we developed a technique that substitutes inexpensive,readily available unlabeled examples for a labeled example while maintaining com-parable levels of accuracy.
Nonetheless, it is clearly impossible to gather examples ofall possible senses of all possible words.
Thus, we also examined the impact on senseresolution accuracy of having previously unseen senses occur in the classifier's testcases.
The classifier is allowed to choose do not know for a particular test case by re-quiring the output activation to be above a threshold before a sense is considered tobe selected.
When modified in this manner and trained on a subset of the availablesenses, the classifier chose do not know on a large fraction of the test cases of the senseit had not been trained on.
In addition, the classifier tended to leave unclassified thosecases it would misclassify when forced to choose a sense.143Computational Linguistics Volume 24, Number 1ReferencesAtlas, Les, Ronald Cole, Jerome Connor,Mohamed E1-Sharkawi, Robert J. MarksII, Yeshwant Muthusamy, and EtienneBarnard.
1989.
Performance comparisonsbetween backpropagation networks andclassification trees on three real-worldapplications.
In Advances in NeuralInformation Processing Systems, volume 2,pages 622-629, Denver, CO. MorganKaufmann.Black, Ezra W. 1988.
An experiment incomputational discrimination of Englishword senses.
IBM Journal of Research andDevelopment, 32(2):185-194.Brill, Eric.
1992.
A simple rule-based part ofspeech tagger.
In Proceedings ofthe ThirdConference on Applied ComputationalLinguistics (ACL).Castelli, Vittorio and Thomas M. Cover.1995.
The relative value of labeled andunlabeled samples in pattern recognitionwith an unknown mixing parameter.Technical Report 86, Stanford University,Department of Statistics.Choueka, Yaacov and Serge Lusignan.
1985.Disambiguation by short contexts.Computers and the Humanities, 19:147-157.Dempster, A. P., N. M. Laird, and D. B.Rubin.
1977.
Maximum likelihood fromincomplete data via the EM algorithm.Journal of the Royal Statistical Society B,39:1-38.Gale, William, Kenneth W. Church, andDavid Yarowsky.
1992.
A method fordisambiguating word senses in a largecorpus.
Computers and the Humanities, 26.Harman, Donna K. 1993.
The first TextREtrieval Conference (TREC-1), Rockville,MD, U.S.A, 4-6 November, 1992.Information Processing and Management,29(4):411-~414.Hearst, Marti A.
1991.
Noun homographdisambiguation using local context inlarge text corpora.
In Proceedings oftheSeventh Annual Conference ofthe UW Centrefor the New OED and Text Research: UsingCorpora, pages 1-22, Oxford.Kong, Eun Bae and Thomas G. Dietterich.1995.
Error-correcting output codingcorrects bias and variance.
In Proceedingsof the Twelfth International Conference onMachine Learning, pages 313-321, MorganKaufmann.Krovetz, Robert and W. Bruce Croft.
1992.Lexical ambiguity in informationretrieval.
ACM Transactions on InformationSystems, 10(2):115-141.Leacock, Claudia, Geoffrey Towell, andEllen M. Voorhees.
1993.
On 'line'learning, unpublished.Leacock, Claudia, Geoffrey Towell, andEllen M. Voorhees.
1996.
Towardsbuilding, contextual representations ofword senses using statistical models.
InB.
Boguraev and J. Pustejovsky, editors,Corpus Processing for Lexical Acquisition.MIT Press, pages 97-113.
Originallyappeared in Proceedings ofSIGLEXWorkshop: Acquisition of Lexical Knowledgefrom Text, 1993.Miller, George.
1990.
Special Issue,WordNet: An on-line lexical database.International Journal of Lexicography, 3(4).Miller, George A. and Walter G. Charles.1991.
Contextual correlates of semanticsimilarity.
Language and Cognitive Processes,6(1).Paramanto, Bambang, Paul W. Munro, andHoward R. Doyle.
1996.
Improvingcommittee diagnosis with resamplingtechniques.
In D. Touretzky, M. Mozer,and M. Hasselmo, editors, Advances inNeural Information Processing Systems,volume 8, Denver, CO. MIT Press.Quinlan, J. Ross.
1992.
C4.5.
MorganKaufmann, Santa Cruz, CA.Rumelhart, David, Geoffrey Hinton, andRonald Williams.
1986.
Learning internalrepresentations by error propagation.
InDavid Rumelhart and James McClelland,editors, Parallel Distributed Processing:Explorations in the Microstructure ofCognition.
Volume 1: Foundations.
MITPress, Cambridge, MA, pages 318-363.Salton, Gerald and Michael E. Lesk.
1971.Computer evaluation of indexing and textprocessing.
In Gerard Salton, editor, TheSMART Retrieval System: Experiments inAutomatic Document Processing.Prentice-Hall, Inc., Englewood Cliffs, NJ,pages 143-180.Salton, Gerald, A. Wong, and C. S. Yang.1975.
A vector space model for automaticindexing.
Communications of the ACM,18(11):613-620.Salton, Gerard and Chris Buckley.
1990.Improving retrieval performance byrelevance feedback.
Journal of the AmericanSociety for Information Science,41(4):288-297.Salton, Gerard and Chris Buckley.
1991.Global text matching for informationretrieval.
Science, 253:1012-1015.Sanderson, Mark.
1994.
Word sensedisambiguation a d information retrieval.In W. Bruce Croft and C. J. vanRijsbergen, editors, Proceedings ofthe 17th144Towell and Voorhees Disambiguating Highly Ambiguous WordsAnnual International ACM/SIGIR Conferenceon Research and Development i  InformationRetrieval, pages 142-151, July.Schfitze, Hinrich and Jan O. Pedersen.
1995.Information retrieval based on wordsenses.
In Proceedings ofthe Fourth AnnualSymposium on Document Analysis andInformation Retrieval, pages 161-175, LasVegas NV.Shavlik, Jude W., Raymond J. Mooney, andGeoffrey Towell.
1991.
Symbolic andneural net learning algorithms: Anempirical comparison.
Machine Learning,6:111-143.Towell, Geoffrey.
1996.
Using unlabeled atafor supervised learning.
In D. Touretzky,M.
Mozer, and M. Hasselmo, editors,Advances in Neural Information ProcessingSystems, volume 8, Denver, CO. MITPress.V4ronis, Jean and Nancy Ide.
1990.
Wordsense disambiguation with very largeneural networks extracted from machinereadable dictionaries.
In Proceedings ofCOLING-90, pages 389-394.Voorhees, Ellen M. 1993.
Using WordNet todisambiguate word senses for textretrieval.
In Robert Korfhage, EdieRasmussen, and Peter Willett, editors,Proceedings ofthe Sixteenth AnnualInternational ACM SIGIR Conference onResearch and Development i  InformationRetrieval, pages 171-180.Voorhees, Ellen M. 1994a.
On expandingquery vectors with lexically relatedwords.
In Donna K. Harman, editor,Proceedings ofthe Second Text REtrievalConference (TREC-2), pages 223-231,March.
NIST Special Publication 500-215.Voorhees, Ellen M. 1994b.
Query expansionusing lexical-semantic relations.
InW.
Bruce Croft and C. J. van Rijsbergen,editors, Proceedings ofthe 17th AnnualInternational ACM/SIGIR Conference onResearch and Development i  InformationRetrieval, pages 61-69, July.Voorhees, Ellen M. and Yuan-Wang Hou.1993.
Vector expansion in a largecollection.
In D. K. Harman, editor,Proceedings ofthe First Text REtrievalConference (TREC-1), pages 343-351.
NISTSpecial Publication 500-207, March.Wang, Yih-Chen, James Vandendorpe, andMartha Evens.
1985.
Relational thesauri ininformation retrieval.
Journal of theAmerican Society for Information Science,36(1):15-27.Wolpert, David H. 1992.
Stackedgeneralization.
Technical ReportLA-UR-90-3460, Los Alamos NationalLaboratory, Los Alamos, NM.Yarowsky, David.
1993.
One sense percollocation.
In Proceedings ofthe ARPAWorkshop on Human Language Technology.Yarowsky, David.
1995.
Unsupervised wordsense disambiguation rivaling supervisedmethods.
In Proceedings ofthe AnnualMeeting, pages 189-196.
Association forComputational Linguistics.145
