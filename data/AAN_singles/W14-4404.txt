Proceedings of the 8th International Natural Language Generation Conference, pages 16?25,Philadelphia, Pennsylvania, 19-21 June 2014. c?2014 Association for Computational LinguisticsText simplification using synchronous dependency grammars:Generalising automatically harvested rulesM.A.
AngroshComputing ScienceUniversity of Aberdeen, UKangroshmandya@abdn.ac.ukAdvaith SiddharthanComputing ScienceUniversity of Aberdeen, UKadvaith@abdn.ac.ukAbstractWe present an approach to text simplifi-cation based on synchronous dependencygrammars.
Our main contributions in thiswork are (a) a study of how automaticallyderived lexical simplification rules can begeneralised to enable their application innew contexts without introducing errors,and (b) an evaluation of our hybrid sys-tem that combines a large set of automat-ically acquired rules with a small set ofhand-crafted rules for common syntacticsimplification.
Our evaluation shows sig-nificant improvements over the state of theart, with scores comparable to human sim-plifications.1 IntroductionText simplification is the process of reducing thelinguistic complexity of a text, while still retain-ing the original information content and mean-ing.
Text Simplification is often thought of asconsisting of two components - syntactic simpli-fication and lexical simplification.
While syntac-tic simplification aims at reducing the grammaticalcomplexity of a sentence, lexical simplification fo-cuses on replacing difficult words or short phrasesby simpler variants.Traditionally, entirely different approaches havebeen used for lexical (Devlin and Tait, 1998; Bi-ran et al., 2011; Yatskar et al., 2010; Specia etal., 2012) and syntactic simplification (Canning,2002; Chandrasekar et al., 1996; Siddharthan,2011; De Belder and Moens, 2010; Candido Jret al., 2009).
Recent years have seen the applica-tion of machine translation inspired approaches totext simplification.
These approaches learn fromaligned English and Simplified English sentencesextracted from the Simple English Wikipedia(SEW) corpus (simple.wikipedia.org).
However,even these approaches (Woodsend and Lapata,2011; Wubben et al., 2012; Coster and Kauchak,2011; Zhu et al., 2010) struggle to elegantly modelthe range of lexical and syntactic simplificationoperations observed in the monolingual simplifi-cation task within one framework, often differen-tiating between operation at leaf nodes of parsetrees (lexical) and internal tree nodes (syntactic).The key issue is the modelling of context for appli-cation of lexical rules.
While syntactic rules (forsplitting conjoined clauses, or disembedding rela-tive clauses) are typically not context dependent,words are typically polysemous and can only bereplaced by others in appropriate contexts.Our main contribution in this paper is to presenta unified framework for representing rules for syn-tactic and lexical simplification (including para-phrase involving multiple words), and study forthe first time how the definition of context affectssystem performance.
A second contribution is toprovide a substantial human evaluation (63 sen-tences and 70 participants) to evaluate contempo-rary text simplification systems against manuallysimplified output.2 Related workText simplification systems are characterised bythe level of linguistic knowledge they encode, andby whether their simplification rules are hand-crafted or automatically acquired from a corpus.In recent times, the availability of a corpus ofaligned English Wikipedia (EW) and Simple En-glish Wikipedia (SEW) sentences has lead to theapplication of various ?monolingual translation?approaches to text simplification.
Phrase BasedMachine Translation (PBMT) systems (Specia,2010; Coster and Kauchak, 2011; Wubben et al.,2012) use the least linguistic knowledge (onlyword sequences), and as such are ill equipped tohandle simplifications that require morphologicalchanges, syntactic reordering or sentence splitting.Zhu et al.
(2010) in contrast present an ap-proach based on syntax-based SMT (Yamada and16Knight, 2001).
Their translation model encodesprobabilities for four specific rewrite operationson the parse trees of the input sentences: substitu-tion, reordering, splitting, and deletion.
Woodsendand Lapata (2011) propose quasi-synchronous treesubstitution grammars (QTSG) for a similarlywide range of simplification operations as well aslexical substitution.
Narayan and Gardent (2014)combine PMBT for local paraphrase with a syn-tactic splitting component based on a deep seman-tic representation.
None of these systems modelmorphological information, which means somesimplification operations such as voice conversioncannot be handled correctly.Against this limitation, hand-crafted systemshave an advantage here, as they tend to encode themaximum linguistic information.
We have previ-ously described systems (Siddharthan, 2010; Sid-dharthan, 2011) that can perform voice conversionaccurately and use transformation rules that en-code morphological changes as well as deletions,re-orderings, substitutions and sentence splitting.On the other hand, such hand-crafted systems arelimited in scope to syntactic simplificatio as thereare too many lexico-syntactic and lexical simplifi-cations to enumerate by hand.
We have also previ-ously described how to construct a hybrid systemthat combines automatically derived lexical ruleswith hand-crafted syntactic rules within a singleframework (Siddharthan and Mandya, 2014).
Weextend that work here by describing how such au-tomatically learnt rules can be generalised.3 Simplification using synchronousdependency grammarsWe follow the architecture proposed in Ding andPalmer (2005) for Synchronous Dependency In-sertion Grammars, reproduced in Fig.
1.In this paper, we focus on the decomposition ofa dependency parse into Elementary Trees (ETs),and the learning of rules to transduce a sourceET to a target ET.
We use the datasets of Costerand Kauchak (2011) and Woodsend and LapataInput Sentence ??
Dependency Parse ??
Source ETs?ET Transfer?Output Sentences ??
Generation ??
Target ETsFigure 1: System ArchitecturestormadvmodintensiveadvmodmoststormadvmodstrongestFigure 2: Transduction of Elementary Trees (ETs)(2011) for learning rules.
These datasets consistof?140K aligned simplified and original sentencepairs obtained from Simple English Wikipedia andEnglish Wikipedia.
The rules are acquired in theformat required by the RegenT text simplificationsystem (Siddharthan, 2011), which is used to im-plement the simplification.
This requires depen-dency parses from the Stanford Parser (De Marn-effe et al., 2006), and generates output sentencesfrom dependency parses using the generation-lightapproach described in (Siddharthan, 2011).3.1 Acquiring rules from aligned sentencesTo acquire a synchronous grammar from depen-dency parses of aligned English and simple En-glish sentences, we just need to identify the dif-ferences.
For example, consider two aligned sen-tences from the aligned corpus described in Wood-send and Lapata (2011):1.
(a) It was the second most intensive storm on theplanet in 1989.
(b) It was the second strongest storm on the planet in1989.An automatic comparison of the dependencyparses for the two sentences (using the StanfordParser) reveals that there are two typed dependen-cies that occur only in the parse of the first sen-tence, and one that occur only in the parse of thesecond sentence.
Thus, to convert the first sen-tence into the second, we need to delete two de-pendencies and introduce one other.
From this ex-ample, we extract the following rule:RULE 1: MOST_INTENSIVE2STRONGEST1.
DELETE(a) advmod(?X0[intensive], ?X1[most])(b) advmod(?X2[storm], ?X0[intensive])2.
INSERT(a) advmod(?X2, ?X3[strongest])The rule contains variables (?Xn), which can beforced to match certain words in square brackets.17Such deletion and insertion operations are cen-tral to text simplification, but a few other oper-ations are also needed to avoid broken depen-dency links in the Target ETs.
These are enu-merated in (Siddharthan, 2011) and will not be re-produced here for shortage of space.
By collect-ing such rules, we can produce a meta-grammarthat can translate dependency parses in one lan-guage (English) into the other (simplified En-glish).
The rule above will translate ?most in-tensive?
to ?strongest?, in the immediate lexicalcontext of ?storm?.
For ease of presentation, wepresent the ET Transfer component as transforma-tion rules, but this rule can also be presented as atransduction of elementary trees (Fig.
2).3.2 Generalising rulesIt is clear that the rule shown above will only beapplied if three different words (?storm?, ?most?and ?intensive?)
occur in the exact syntax speci-fied on the left hand side of Figure 2.
The ruleis correct, but of limited use, for ?most intensive?can be simplified to ?strongest?
only when it mod-ifies the word ?storm?.The modelling of lexical context is a partic-ular weak point in previous work; for instance,Woodsend and Lapata (2011), in their quasi-synchronous tree substitution grammar, remove alllexical context for lexical simplification rules, tofacilitate their application in new contexts.
Simi-larly, phrase-based machine translation can defaultto lexical simplification using word level align-ments if longer substrings from the input text arenot found in the alignment table.
However, aswords can have different senses, lexical substitu-tion without a lexical context model is error prone.Our goals here are to enumerate methods togeneralise rules, and to evaluate performance onunseen sentences.
All the methods described areautomated, and do not require manual effort.Generalising from multiple instances: A sin-gle rule can be created from multiple instances inthe training data.
For example, if the modifier ?ex-tensive?
has been simplified to ?big?
in the con-text of a variety of words in the ?X0 position, thiscan be represented succinctly as ?
?X0[networks,avalanches, blizzard, controversy]?.
Note that thislist provides valid lexical contexts for applicationof the rule.
If the word is seen in sufficient con-texts, we make it universal by removing the list.Rule 2 below states that any of the words in ?
[ex-tensive, large, massive, sizable, major, powerful,giant]?
can be replaced by ?big?
in any lexicalcontext ?X0, provided the syntactic context is anamod relation.
To de-lexicalise context in thismanner, each lexical substitution needs to havebeen observed in 10 different contexts.
While notfoolproof, this ensures that lexical context is re-moved only for common simplifications, whichare more likely to be independent of context.RULE 2: *2BIG1.
DELETE(a) amod(?X0, ?X1[extensive, large, massive, siz-able, major, powerful, giant])2.
INSERT(a) amod(?X0, ?X2[big])Reducing context size: Often, single lexicalchanges result in multiple relations in the INSERTand DELETE lists.
Rule 3 shows a rule where theverb ?amend?
has been simplified to ?change?, ina context where the direct object is ?Constitution?and there is an infinitive modifier relation to ?pro-posals?, using the auxiliary ?to?.RULE 3: AMEND2CHANGE1.
DELETE(a) aux(?X0[amend], ?X1[to])(b) infmod(?X2[proposals], ?X0[amend])(c) dobj(?X0[amend], ?X3[Constitution])2.
INSERT(a) aux(?X4[change], ?X1)(b) infmod(?X2, ?X4)(c) dobj(?X4, ?X3)3.
MOVE(a) ?X0 ?X4Rule 3 also shows the MOVE command createdto move any other relations (edges) involving thenode ?X0 to the newly created node ?X4.
TheMOVE list is automatically created when a vari-able (?X0) is present in the DELETE list but notin the INSERT list and ensures correct rule appli-cation in new contexts where there might be addi-tional modifiers connected to the deleted word.Rule 3 clearly encodes too much context.
Insuch cases, we reduce the context by creating threerules, each with a reduced context of one relation(aux, infmod or dobj); for example:RULE 3A: AMEND2CHANGE31.
DELETE: dobj(?X0[amend], ?X1[Constitution])2.
INSERT: dobj(?X2[change], ?X1)3.
MOVE: ?X0 ?X218In this paper, we generate rules with each pos-sible lexical context, but one could filter out rela-tions such as aux that provide a lexical context of aclosed class word.
The generalised Rule 3A makesclear the need for the MOVE operation, which isimplemented in RegenT by rewriting ?X0 as ?X2in the entire dependency parse after rule applica-tion.
We will omit the MOVE command where itis not required to save space.Extracting elementary trees: It is possible forthe DELETE and INSERT lists to contain mul-tiple simplification rules; i.e., multiple transduc-tions over ETs (connected graphs).
We need to en-sure that each extracted rule contains a connectedgraph in the DELETE list.
Where this is not thecase, we split the rule into multiple rules.
An ex-ample follows where three independent simplifi-cations have been performed on a sentence:4.
(a) As a general rule , with an increase in elevationcomes a decrease in temperature and an increasein precipitation .
(b) As a normal rule , with an increase in heightcomes a decrease in temperature and an increasein rain .The original extracted rule contains three rela-tions with no variable in common:RULE 4: INDEPENDENTELEMENTARYTREES1.
DELETE(a) amod(?X0[rule], ?X1[general])(b) prep_in(?X2[comes], ?X3[elevation])(c) prep_in(?X4[increase], ?X5[precipitation])2.
INSERT(a) amod(?X0, ?X6[normal])(b) prep_in(?X2, ?X7[height])(c) prep_in(?X4, ?X8[rain])Relations with no variables in common belongto separate ETs, so we create three new rules:RULE 4ADELETE: amod(?X0[rule], ?X1[general])INSERT: amod(?X0, ?X6[normal])RULE 4BDELETE: prep_in(?X2[comes], ?X3[elevation])INSERT: prep_in(?X2, ?X7[height])RULE 4CDELETE: prep_in(?X4[increase], ?X5[precipitation])INSERT: prep_in(?X4, ?X8[rain])Removing lexical context from longer rules:While preserving lexical context is important toavoid meaning change in new contexts due to pol-ysemy (this claim is evaluated in ?3.5), it is unnec-essary for longer rules involving more than one re-lation, as these tend to encode longer paraphraseswith more standardised meanings.
We thus re-move the lexical context for rules involving multi-ple relations in the DELETE list1.3.3 Overview of extracted rulesetIn addition to the generalisation steps describedabove, we also automatically filtered out rulesthat were undesired for various reasons.
As weuse manually written rules in RegenT for com-mon syntactic simplification (as described in Sid-dharthan (2011)), we filter out rules that involvedependency relations for passive voice, relativeclauses, apposition, coordination and subordina-tion.
We also filter out rules with relations that areerror-prone, based on a manual inspection.
Theseinvolved single lexical changes involving the fol-lowing dependencies: det and num (rules thatchange one determiner to another, or one numberto another) and poss and pobj that mostly appearedin rules due to errorful parses.
We also automat-ically filtered out errorful rules using the trainingset as follows: we applied the rules to the sourcesentence from which they were derived, and fil-tered out rules that did not generate the target sen-tence accurately.
Finally, we restricted the numberof relations in either the DELETE or INSERT listto a maximum of three, as longer rules were neverbeing applied.Tab.
1 shows how the filters and generalisationinfluence the number of rules derived involving 1?5 relations in each of the DELETE and INSERTlists.
In addition, we also extract rules where theDELETE list is longer than the INSERT list; i.e.,simplification that result in sentence shortening(e.g., Rule 1 in Section 3.1).Tab.
2 provides details of the final number of fil-tered and generalised rules for different lengths ofthe DELETE and INSERT lists.
The ruleset shownin Tab.
2 will henceforth be referred to as WIKI.3.4 Generalising context with WordNetTo generalise the context of lexical simplificationrules further, we now consider the use of WordNet1Lexical context is defined as lexical specifications onvariables occurring in both the DELETE and INSERT lists;i.e., words that are unchanged by the simplification.19DELETE INSERT IS FS GS1 1 1111 593 42502 2 1051 357 1713 3 1108 178 524 4 831 - -5 5 628 - -Table 1: Number of extracted rules where the IN-SERT and DELETE lists contain 1?5 relations (IS:initial set; FS: filtered set; GS: generalised set)in expanding lexical context.
The idea is that thelexical specification of context variables in rulescan be expanded by identifying related words inWordNet.
We propose to use Lin?s similarity mea-sure (Lin, 1998), an information content basedsimilarity measure for our experiments as infor-mation content based measures are observed toperform better in deriving similar terms, in com-parison to other methods (Budanitsky and Hirst,2006).
Lin?s formula is based on Resnik?s.
LetIC(c) = ?log p(c) be the information content ofa concept (synset) in WordNet, where p(c) is thelikelihood of seeing the concept (or any of its hy-ponyms) in a corpus.
Resnik defines the similar-ity of two concepts c1 and c2 as simres(c1, c2) =maxc?S(c1,c2)IC(c), the IC of the most specificclass c that subsumes both c1 and c2.
Lin?s for-mula normalises this by the IC of each class:simlin(c1, c2) =2.simres(c1, c2)IC(c1) + IC(c2)Our next goal is to explore how the definitionof lexical context impacts on a text simplificationsystem.3.5 EvaluationTo evaluate our work, we used the text simplifi-cation tool RegenT (Siddharthan, 2011) to applydifferent versions of the acquired rule sets to a testdataset.
For example, consider the following ruleshown in 6(a).
This is the original rule extractedfrom the training data (cf.
Tab.
2).RULE 6(A): RULE-WIKI1.
DELETE(a) nsubjpass(?
?X0[adapted], ??X1[limbs])2.
INSERT(a) nsubjpass(?
?X0, ?
?X2[legs])This rule is transformed to a no-context rule in6(b), where words such as ?adapted?
that occur inDELETE / INSERT 1 2 3 4 51 Relation 42502 Relations 110 1713 Relations 91 165 524 Relations 49 71 209 -5 Relations 24 44 80 - -Table 2: Details of rules derived with differentlength in DELETE and INSERT relationsboth the DELETE and INSERT lists are removedentirely from the rule:RULE 6(B): NO-CONTEXT1.
DELETE(a) nsubjpass(?
?X0, ??X1[limbs])2.
INSERT(a) nsubjpass(?
?X0, ?
?X2[legs])Finally the rule in 6(c), expands the contextword ?adapted?
using WordNet classes with Lin?ssimilarity greater than 0.1.RULE 6(C): RULE-WITH-WORDNET0.1-CONTEXT1.
DELETE(a) nsubjpass(??X0[accommodated,adapted,adjusted,altered,assimilated,changed,complied,conformed,fited,followed,geared,heeded,listened,minded,moved,obeyed,oriented,pitched,tailored,varied],??X1[limbs])2.
INSERT(a) nsubjpass(?
?X0, ?
?X2[legs])Evaluation of generalisability of rules: Weexpanded the context of rules derived fromWikipedia using various thresholds such as 0.1,0.4 and 0.8 for Lin similarity measure and eval-uated how many simplification operations wereperformed on the first 11,000 sentences from thedataset of Coster and Kauchak (2011).
The detailsof rules applied on the test dataset, using differ-ent thresholds along with the Wiki-context and no-context rules are provided in Tab.
3.
As seen, thereis an increase in the application of rules with thedecrease in threshold for Lin similarity measure.Removing the lexical context entirely results in aneven larger increase in rule application.
Next, weevaluate the correctness of rule application.Evaluation of correctness of rule application:To test the correctness of the rule applications with20Rule Version Rules % ChangeWikicontext 7610WordNet context (0.8) 7870 3.41WordNetcontext (0.4) 8488 11.85WordNetcontext (0.1) 10715 40.80Nocontext 31589 315.09Table 3: Application of different versions of ruleson test dataset (% change is the increase in theapplication of rules between Wiki-context and thecorresponding version)different rule sets, we performed a human evalua-tion to gauge how fluent and simple the simpli-fied sentences were, and to what extent they pre-served the meaning of the original.
We comparedthree versions in this experiment: the original rule-set, the context expanded using SimLin >= 0.1(40% increase in rule applications) and with nolexicalised context (315% increase in rule applica-tions).
The goal is to identify a level of generalisa-tion that increases rule application in new contextswithout introducing more errors.We used the first 11,000 sentences from thedataset of Coster and Kauchak (2011), the samedataset used for rule acquisition.
We extracted atrandom 30 sentences where a simplification hadbeen performed using the original ruleset.
Thisgives an upper bound on the performance of theoriginal Wikipedia-context ruleset, as these are allsentences from which the rules have been derived.We then selected a further 30 sentences wherea simplification had been performed using theWordNet-context (Lin=0.1), but not with the origi-nal ruleset.
These are new applications of the gen-eralised ruleset on sentences that it hasn?t directlylearnt rules from.
Similarly, we selected a fur-ther 30 sentences where a simplification had beenperformed using the no-context ruleset, but notthe Wikipedia-context or WordNet-context rule-sets.
Thus each set of 30 sentences contains newapplications of the ruleset, as the lexical context isexpanded, or abandoned completely.This process gave us a total of 90 sentences toevaluate.
We recruited participants through Ama-zon Mechanical Turk.
Participants were filtered tobe in the US and have an approval rating of 80%.These raters were shown 30 examples, each con-taining an original Wikipedia sentence followedby one of the simplified versions (WI, WN or NC).Order of presentation was random.
For each suchpair, raters were asked to rate each simplified ver-sion for fluency, simplicity and the extent to whichit preserved the meaning of the original.
The ex-periment provided 917 ratings for 90 sentences in-volving 28 raters.
We used a Likert scale of 1?5,where 1 is totally unusable output, and 5 is the out-put that is perfectly usable.The mean values and the standard deviationfor fluency, simplicity and meaning preservationfor sentences simplified using WordNet (Lin=0.1),Wiki and no context is shown in Tab.
4.
As seen,the difference between the mean values for allthree criteria of fluency, simplicity and meaningpreservation between WordNet and Wiki versionis very small as compared to simplified sentenceswith no-context rules.
An analysis of variance(ANOVA) test was conducted to measure the ef-fect of fluency, simplicity and meaning preserva-tion for versions of simplified text.Fluency: A one-way ANOVA conducted toevaluate fluency for versions of simplified textshowed a highly significant effect of version (WN,WC, and NC) on the fluency score (F=51.54,p=2x10-16).
A Tukey?s pairwise comparison test(Tukey?s HSD, overall alpha level = 0.05) indi-cated significant difference between WI and NCand between WN and NC at p = 0.01.
However,the difference between WN and WI was not sig-nificant at p = 0.01.Simplicity: The ANOVA conducted to evaluatesimplicity for different versions also showed a sig-nificant effect of version on the simplicity score(F=76.7, p=2x10-16).
A Tukey?s pairwise compar-ison test (Tukey?s HSD, overall alpha level = 0.05)indicated significant difference between WN andNC and WI and NC (p < 0.01).
However, the dif-ference between WN and WI was not significantat p = 0.01.Meaning Preservation: The ANOVA conductedto evaluate meaning preservation for versions ofsimplified text also showed a highly significant ef-fect of version on the meaning preservation score(F=17.22, p=4.55x10-08).
A Tukey?s pairwisecomparison test (Tukey?s HSD, overall alpha level= 0.05) indicated significant difference betweenWN and NC and WI and NC (p < 0.01).
How-ever, the difference between WN and WI was notsignificant at p = 0.01.This study suggests that there is no significanteffect on accuracy of expanding the lexical con-text using WordNet (Lin=0.1), even though thisresults in an increase in rule application of 40%.The study also confirms that there is a sharp and21Rater FLUENCY SIMPLICITY MEANINGWN WI NC WN WI NC WN WI NCMean 3.28 3.59 2.49 3.68 3.51 2.47 2.52 2.72 2.17SD 1.38 1.31 1.44 1.32 1.28 1.34 1.12 1.11 1.27Median 4 4 2 3 4 2 3 3 2Table 4: Results of human evaluation of different versions of simplified text (WN: WordNet-context(Lin=0.1); WI: Wikipedia-context; NC: No-context)significant drop in accuracy from removing lexicalcontext altogether (the approach used by Wubbenet al.
(2012), for example).
Next, we perform anevaluation of our hybrid text simplification sys-tem, that augments the existing RegenT system(Siddharthan, 2011), with its hand-written rulesfor syntactic simplification, with the automaticallyacquired lexicalised rules(the Lin=0.1 ruleset).4 Hybrid text simplification systemThe RegenT text simplification toolkit (Sid-dharthan, 2011) is distributed with a small handcrafted grammar for common syntactic simplifica-tions: 26 hand-crafted rules for apposition, rela-tive clauses, and combinations of the two; a fur-ther 85 rules handle subordination and coordina-tion (these are greater in number because they arelexicalised on the conjunction); 11 further rulescover voice conversion from passive to active; 38rules for light verbs and various cleft construc-tions; 99 rules to handle common verbose con-structions described in the old GNU diction utility;14 rules to standardise quotations.The RegenT system does not have a decoder ora planner.
It also does not address discourse issuessuch as those described in Siddharthan (2003a),though it includes a component that improvesrelative clause attachment based on Siddharthan(2003b).
It applies the simplification rules exhaus-tively to the dependency parse; i.e., every rule forwhich the DELETE list is matched is applied iter-atively (see Siddharthan (2011) for details).We have created a hybrid text simplificationsystem by integrating our automatically acquiredrules (lexical context extended using WordNet forsingle change rules, and lexical context removedfor longer rules) with the existing RegenT systemas described above.
This is sensible, as the ex-isting manually written rules for syntactic simpli-fication are more reliable than automatically ex-tracted ones: They model morphological change,allowing for a linguistically accurate treatmentof syntactic phenomenon such as voice change.The current work addresses a major limitationof hand-crafted text simplification systems?suchsystems restrict themselves to syntactic simplifi-cation, even though vocabulary plays a central rolein reading comprehension.
We hope that the meth-ods described here can extend a hand-crafted sys-tem to create a hybrid text simplification systemthat is accurate as well as wide coverage.
We nextpresent a large scale manual evaluation of this hy-brid system.4.1 EvaluationWe performed a manual evaluation of how fluentand simple the text produced by our simplifica-tion system is, and the extent to which it preservesmeaning.Our system (henceforth, HYBRID) is comparedto QTSG, the system by Woodsend and Lapata(2011) that learns a quasi-synchronous tree substi-tution grammar.
This is the best performing sys-tem in the literature with a similar scope to ours interms of the syntactic and lexical operations per-formed 2.
Further the two systems are trained onthe same data.
QTSG relies entirely on an auto-matically acquired grammar of 1431 rules.
Ourautomatically extracted grammar has 5466 lex-icalised rules to augment the existing manuallywritten syntactic rules in RegenT.We also compare the two systems to the manualgold standard SEW, and against the original EWsentences.Data: We use the evaluation set previously usedby several others (Woodsend and Lapata, 2011;Wubben et al., 2012; Zhu et al., 2010).
Thisconsists of 100 sentences from English Wikipedia(EW), aligned with Simple English Wikipedia(SEW) sentences.
These 100 sentences have beenexcluded from our training data for rule acquisi-tion, as is standard.
Following the protocol ofWubben et al.
(2012), we used all the sentencesfrom the evaluation set for which both QTSG and2The PBMT system of Wubben et al.
(2012) reports betterresults than QTSG, but is not directly comparable becauseit does not perform sentence splitting, and also trains on adifferent corpus of news headlines.22Rater FLUENCY SIMPLICITY MEANINGEW SEW QTSG HYB EW SEW QTSG HYB EW SEW QTSG HYBMean 3.99 4.06 1.97 3.52 3.43 3.58 2.33 3.73 - 4.03 2.23 3.40SD 0.94 1.00 1.24 1.24 1.07 1.22 1.26 1.30 - 1.02 1.23 1.18Median 4 4 1 4 3 4 2 4 - 4 2 3Table 5: Results of human evaluation of different simplified texts (EW: English Wikipedia; SEW: SimpleEnglish Wikipedia; QTSG: Woodsend and Lapata (2011) system; HYB: Our hybrid system)HYBRID had performed at least one simplification(as selecting sentences where no simplification isperformed by one system is likely to boost its flu-ency and meaning preservation ratings).
This gaveus a test set of 62 sentences from the original 100.Method: We recruited participants on AmazonMechanical Turk, filtered to live in the US andhave an approval rating of 80%.
These partici-pants were shown examples containing the orig-inal Wikipedia sentence, followed by QTSG, HY-BRID and SEW in a randomised manner.
For eachsuch set, they were asked to rate each simplifiedversion for fluency, simplicity and the extent towhich it preserved the meaning of the original.Additionally, participants were also asked to ratethe fluency and simplicity of the original EW sen-tence.
We used a Likert scale of 1?5, where 1 istotally unusable output, and 5 is output that is per-fectly usable.
The experiment resulted in obtain-ing a total of 3669 ratings for 62 sentences involv-ing 76 raters.Results: The results are shown in Tab.
5.
Asseen, our HYBRID system outperforms QTSGin all three metrics and is indeed comparable tothe SEW version when one looks at the medianscores.
Interestingly, our system performs betterthan SEW with respect to simplicity, suggestingthat the hybrid system is indeed capable of a widerange of simplification operations.
The ANOVAtests carried out to measure significant differencesbetween versions is presented below.Fluency: A one-way ANOVA was conductedto evaluate fluency for versions of simplifiedtext showed a highly significant effect of ver-sion (EW, SEW, QTSG, HYBRID) on the flu-ency score (F=695.2, p<10-16).
A Tukey?s pair-wise comparison test (Tukey?s HSD, overall al-pha level = 0.05) indicated significant differencesbetween QTSG-EW; HYBRID-EW; HYBRID-QTSG; SEW-QTSG; SEW-HYBRID at p = 0.01.Simplicity: A one-way ANOVA conducted toevaluate fluency for versions of simplified textshowed a highly significant effect of version(EW, SEW, QTSG, HYBRID) on the simplic-ity score (F=29.9, p<10-16).
A Tukey?s pair-wise comparison test (Tukey?s HSD, overall al-pha level = 0.05) indicated significant differencesbetween QTSG-EW; HYBRID-EW; HYBRID-QTSG; SEW-QTSG; all at p<0.01.Meaning Preservation: A one-way ANOVAconducted to evaluate meaning preservation forversions of simplified text showed a highly sig-nificant effect of version (EW, SEW, QTSG,HYBRID) on the meaning preservation score(F=578.1, p=2x10-16).
A Tukey?s pairwise com-parison test (Tukey?s HSD, overall alpha level= 0.05) indicated significant differences betweenQTSG-SEW; HYBRID-SEW; and HYBRID-QTSG all at p<0.01.5 ConclusionWe have described a hybrid system that performstext simplification using synchronous dependencygrammars.
The grammar formalism is intuitiveenough to write rules by hand, and a syntactic ruleset is distributed with the RegenT system.
Thecontributions of this paper are to demonstrate thatthe same framework can be used to acquire lex-icalised rules from a corpus, and that the resul-tant system generates simplified sentences that arecomparable to those written by humans.We have documented how a grammar can beextracted from a corpus, filtered and generalised.Our studies confirm the benefits of generalisingrules in this manner.
The resultant system thatcombines this grammar with the existing manualgrammar for syntactic simplification has outper-formed the best comparable contemporary systemin a large evaluation.
Indeed our system performsat a level comparable to the manual gold standardin a substantial evaluation involving 76 partici-pants, suggesting that text simplification systemsare reaching maturity for real application.AcknowledgementsThis research is supported by an award made bythe EPSRC; award reference: EP/J018805/1.23ReferencesOr Biran, Samuel Brody, and Noemie Elhadad.
2011.Putting it simply: a context-aware approach to lex-ical simplification.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages496?501, Portland, Oregon, USA, June.
Associationfor Computational Linguistics.Alexander Budanitsky and Graeme Hirst.
2006.
Eval-uating wordnet-based measures of lexical semanticrelatedness.
Computational Linguistics, 32(1):13?47.Arnaldo Candido Jr, Erick Maziero, Caroline Gasperin,Thiago AS Pardo, Lucia Specia, and Sandra MAluisio.
2009.
Supporting the adaptation of textsfor poor literacy readers: a text simplification ed-itor for brazilian portuguese.
In Proceedings ofthe Fourth Workshop on Innovative Use of NLPfor Building Educational Applications, pages 34?42.Association for Computational Linguistics.Yvonne Canning.
2002.
Syntactic simplification ofText.
Ph.D. thesis, University of Sunderland, UK.Raman Chandrasekar, Christine Doran, and Banga-lore Srinivas.
1996.
Motivations and methods fortext simplification.
In Proceedings of the 16th In-ternational Conference on Computational Linguis-tics (COLING ?96), pages 1041?1044, Copenhagen,Denmark.William Coster and David Kauchak.
2011.
Learning tosimplify sentences using wikipedia.
In Proceedingsof the Workshop on Monolingual Text-To-Text Gen-eration, pages 1?9.
Association for ComputationalLinguistics.Jan De Belder and Marie-Francine Moens.
2010.Text simplification for children.
In Prroceedings ofthe SIGIR workshop on accessible search systems,pages 19?26.M.C.
De Marneffe, B. MacCartney, and C.D.
Manning.2006.
Generating typed dependency parses fromphrase structure parses.
In Proceedings of LREC,volume 6, pages 449?454.
Citeseer.Siobhan Devlin and John Tait.
1998.
The use of a psy-cholinguistic database in the simplification of textfor aphasic readers.
In J. Nerbonne, editor, Linguis-tic Databases, pages 161?173.
CSLI Publications,Stanford, California.Yuan Ding and Martha Palmer.
2005.
Machine trans-lation using probabilistic synchronous dependencyinsertion grammars.
In Proceedings of the 43rd An-nual Meeting on Association for Computational Lin-guistics, pages 541?548.
Association for Computa-tional Linguistics.Dekang Lin.
1998.
An information-theoretic defini-tion of similarity.
In ICML, volume 98, pages 296?304.Shashi Narayan and Claire Gardent.
2014.
Hybridsimplification using deep semantics and machinetranslation.
In Proc.
of the 52nd Annual Meetingof the Association for Computational Linguistics.Advaith Siddharthan and Angrosh Mandya.
2014.
Hy-brid text simplification using synchronous depen-dency grammars with hand-written and automati-cally harvested rules.
In Proceedings of the 14thConference of the European Chapter of the Associa-tion for Computational Linguistics, pages 722?731,Gothenburg, Sweden, April.
Association for Com-putational Linguistics.Advaith Siddharthan.
2003a.
Preserving discoursestructure when simplifying text.
In Proceedings ofthe European Natural Language Generation Work-shop (ENLG), 11th Conference of the EuropeanChapter of the Association for Computational Lin-guistics (EACL?03), pages 103?110, Budapest, Hun-gary.Advaith Siddharthan.
2003b.
Resolving pronouns ro-bustly: Plumbing the depths of shallowness.
In Pro-ceedings of the Workshop on Computational Treat-ments of Anaphora, 11th Conference of the Euro-pean Chapter of the Association for ComputationalLinguistics (EACL?03), pages 7?14, Budapest, Hun-gary.Advaith Siddharthan.
2010.
Complex lexico-syntacticreformulation of sentences using typed dependencyrepresentations.
In Proc.
of the 6th InternationalNatural Language Generation Conference (INLG2010), pages 125?133.
Dublin, Ireland.Advaith Siddharthan.
2011.
Text simplification usingtyped dependencies: a comparison of the robustnessof different generation strategies.
In Proceedings ofthe 13th European Workshop on Natural LanguageGeneration, pages 2?11.
Association for Computa-tional Linguistics.Lucia Specia, Sujay Kumar Jauhar, and Rada Mihalcea.2012.
Semeval-2012 task 1: English lexical sim-plification.
In Proceedings of the First Joint Con-ference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference andthe shared task, and Volume 2: Proceedings of theSixth International Workshop on Semantic Evalua-tion, pages 347?355.
Association for ComputationalLinguistics.Lucia Specia.
2010.
Translating from complex tosimplified sentences.
In Proceedings of the Con-ference on Computational Processing of the Por-tuguese Language, pages 30?39.
Springer.Kristian Woodsend and Mirella Lapata.
2011.
Learn-ing to simplify sentences with quasi-synchronousgrammar and integer programming.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing, pages 409?420.
Associationfor Computational Linguistics.24Sander Wubben, Antal van den Bosch, and EmielKrahmer.
2012.
Sentence simplification by mono-lingual machine translation.
In Proceedings of the50th Annual Meeting of the Association for Compu-tational Linguistics: Long Papers-Volume 1, pages1015?1024.
Association for Computational Linguis-tics.Kenji Yamada and Kevin Knight.
2001.
A syntax-based statistical translation model.
In Proceedingsof the 39th Annual Meeting on Association for Com-putational Linguistics, pages 523?530.
Associationfor Computational Linguistics.Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-Mizil, and Lillian Lee.
2010.
For the sake of sim-plicity: Unsupervised extraction of lexical simplifi-cations from wikipedia.
In Human Language Tech-nologies: The 2010 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 365?368.
Association forComputational Linguistics.Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.2010.
A monolingual tree-based translation modelfor sentence simplification.
In Proceedings of the23rd international conference on computational lin-guistics, pages 1353?1361.
Association for Compu-tational Linguistics.25
