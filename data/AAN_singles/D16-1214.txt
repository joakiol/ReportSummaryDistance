Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2022?2027,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsEvaluating Induced CCG Parsers on Grounded Semantic ParsingYonatan Bisk1?
Siva Reddy2?
John Blitzer3 Julia Hockenmaier4 Mark Steedman21ISI, University of Southern California2ILCC, School of Informatics, University of Edinburgh3Google, Mountain View4Department of Computer Science, University of Illinois at Urbana-Champaignybisk@isi.edu, siva.reddy@ed.ac.uk, blitzer@google.com,juliahmr@illinois.edu, steedman@inf.ed.ac.uk,AbstractWe compare the effectiveness of four differ-ent syntactic CCG parsers for a semantic slot-filling task to explore how much syntactic su-pervision is required for downstream seman-tic analysis.
This extrinsic, task-based evalua-tion also provides a unique window into the se-mantics captured (or missed) by unsupervisedgrammar induction systems.1 IntroductionThe past several years have seen significant progressin unsupervised grammar induction (Carroll andCharniak, 1992; Yuret, 1998; Klein and Manning,2004; Spitkovsky et al, 2010; Garrette et al, 2015;Bisk and Hockenmaier, 2015).
But how useful areunsupervised syntactic parsers for downstream NLPtasks?
What phenomena are they able to capture,and where would additional annotation be required?Instead of standard intrinsic evaluations ?
attachmentscores that depend strongly on the particular anno-tation styles of the gold treebank ?
we examine theutility of unsupervised and weakly supervised parsersfor semantics.
We perform an extrinsic evaluation ofunsupervised and weakly supervised CCG parsers ona grounded semantic parsing task that will shed lighton the extent to which these systems recover seman-tic information.
We focus on English to perform adirect comparison with supervised parsers (althoughunsupervised or weakly supervised approaches arelikely to be most beneficial for domains or languageswhere supervised parsers are not available).
?Equal contributionSpecifically, we evaluate different parsing scenar-ios with varying amounts of supervision.
These aredesigned to shed light on the question of how wellsyntactic knowledge correlates with performance ona semantic evaluation.
We evaluate the following sce-narios (all of which assume POS-tagged input): 1) nosupervision; 2) a lexicon containing words mappedto CCG categories; 3) a lexicon containing POS tagsmapped to CCG categories; 4) sentences annotatedwith CCG derivations (i.e., fully supervised).
Ourevaluation reveals which constructions are problem-atic for unsupervised parsers (and annotation effortsshould focus on).
Our results indicate that unsuper-vised syntax is useful for semantics, while a simplesemi-supervised parser outperforms a fully unsuper-vised approach, and could hence be a viable optionfor low resource languages.2 CCG Intrinsic EvaluationsCCG (Steedman, 2000) is a lexicalized formalism inwhich words are assigned syntactic types, also knownas supertags, encoding subcategorization informa-tion.
Consider the sentence Google acquired Nestin 2014, and its CCG derivations shown in Figure 1.In (a) and (b), the supertag of acquired, (S\NP)/NP,indicates that it has two arguments, and the preposi-tional phrase in 2014 is an adjunct, whereas in (c) thesupertag ((S\NP)/PP)/NP indicates acquired hasthree arguments including the prepositional phrase.In (a) and (b), depending on the supertag of in, thederivation differs.
When trained on labeled treebanks,(a) is preferred.
However note that all these deriva-tions could lead to the same semantics (e.g., to thelogical form in Equation 1).
Without syntactic su-2022Google acquired Nest in 2014NP (S\NP)/NP NP ((S\NP)\(S\NP))/NP NP> >S\NP (S\NP)\(S\NP) <S\NP <S(a) in 2014 modifies acquired NestGoogle acquired Nest in 2014NP (S\NP)/NP NP (S\S)/NP NP> >S\NP S\S<S <S(b) in 2014 modifies Google acquired NestGoogle acquired Nest in 2014NP ((S\NP)/PP)/NP NP PP/NP NP> >(S\NP)/PP PP >S\NP <S(c) acquired Google takes the argument in 2014Figure 1: Example of multiple valid derivations that can be grounded to the same Freebase logical form (Eq.1) even though they differ dramatically in performance under parsing metrics (5, 4, or 3 ?correct?
supertags).pervision, there may not be any reason for the parserto prefer one analysis over the other.
One proce-dure to evaluate unsupervised induction methods hasbeen to compare the assigned supertags to treebankedsupertags, but this evaluation does not consider thatmultiple derivations could lead to the same semantics.This problem is also not solved by evaluating syntac-tic dependencies.
Moreover, while many dependencystandards agree on the head direction of simple con-stituents (e.g., noun phrases) they disagree on themost semantically useful ones (e.g., coordination andrelative clauses).13 Our Proposed EvaluationThe above syntax-based evaluation metrics concealthe real performance differences and their effect ondownstream tasks.
Here we propose an extrinsicevaluation where we evaluate our ability to convertsentences to Freebase logical forms starting via CCGderivations.
Our motivation is that most sentencescan only have a single realization in Freebase, andany derivation that could lead to this realization ispotentially a correct derivation.
For example, theFreebase logical form for the example sentence inFigure 1 is shown below, and none of its derivationsare penalized if they could result in this logical form.?e.
business.acquisition(e)?
acquiring company(e,GOOGLE)?
company acquired(e,NEST)?
date(e, 2014)(1)Since grammar induction systems are traditionallytrained on declarative sentences, we would ideallyrequire declarative sentences paired with Freebaselogical forms.
But such datasets do not exist in theFreebase semantic parsing literature (Cai and Yates,2013; Berant et al, 2013).
To alleviate this prob-1Please see Bisk and Hockenmaier (2013) for more details.lem, and yet perform Freebase semantic parsing, wepropose an entity slot-filling task.Entity Slot-Filling Task.
Given a declarative sen-tence containing mentions of Freebase entities, werandomly remove one of the mentions to create ablank slot.
The task is to fill this slot by translatingthe declarative sentence into a Freebase query.
Con-sider the following sentence where the entity Nesthas been removed:Google acquired which was founded in Palo AltoTo correctly fill in the blank, one has to query Free-base for the entities acquired by Google (constraint 1)and founded in Palo Alto (constraint 2).
If either ofthose constraints are not applied, there will be manyentities as answers.
For each question, we execute asingle Freebase query containing all the constraintsand retrieve a list of answer entities.
From this list,we pick the first entity as our predicted answer, andconsider the prediction as correct if the gold answeris the same as the predicted answer.4 Sentences to Freebase Logical FormsCCG provides a clean interface between syntaxand semantics, i.e.
each argument of a words syn-tactic category corresponds to an argument of thelambda expression that defines its semantic interpre-tation (e.g., the lambda expression correspondingto the category (S\NP)/NP of the verb acquiredis ?f.?g.?e.?x.
?y.acquired(e) ?
f(x) ?
g(y) ?arg1(e, y)?arg2(e, x)), and the logical form for thecomplete sentence can be constructed by composingword level lambda expressions following the syntac-tic derivation (Bos et al, 2004).
In Figure 2 we showtwo syntactic derivations for the same sentence, andthe corresponding logical forms and equivalent graphrepresentations derived by GRAPHPARSER (Reddyet al, 2014).
The graph representations are possi-ble because GRAPHPARSER assumes access to co-indexations of input CCG categories.
We provide2023Google acquired ?blank?
which was founded inPANP (S\NP)/NP NP (NP\NP)/(S\NP) S\NP>NP\NP<NP >S\NP<Se2 Palo Altotarget x e1 Googleacquired.arg2acquired.arg1founded.in.arg1founded.in.arg2target(x)^ founded.in.arg1(e2, x) ^ founded.in.arg2(e2,Palo Alto)^ acquired.arg1(e1,Google) ^ acquired.arg2(e1, x)?e1.?xe2.
TARGET(x) ?
acquired(e1) ?
arg1(e1,Google) ?
arg2(e1, x) ?
founded(e2) ?
arg2(e2, x) ?
in(e2,PaloAlto)Google acquired ?blank?
which was founded inPANP (S\NP)/NP NP ((S\NP)\(S\NP))/(S\NP) S\NP> >S\NP (S\NP)\S\NP <S\NP <SPalo Alto e2target x e1 Googleacquired.arg2acquired.arg1founded.in.arg1founded.in.arg2?e1.?xe2.
TARGET(x) ?
acquired(e1) ?
arg1(e1,Google) ?
arg2(e1, x) ?
founded(e2) ?
arg2(e2,Google) ?
in(e2,PaloAlto)Figure 2: The lexical categories for which determine the relative clause attachment and therefore the resultingungrounded logical form.
The top derivation correctly executes a query to retrieve companies founded inPalo Alto and acquired by Google.
The bottom incorrectly asserts that Google was founded in Palo Alto.co-indexation for all induced categories, includingmultiple co-indexations when an induced categoryis ambiguous.
For example, (S\N)/(S\N) refers toeither (Sx\Ny)/(Sx\Ny) indicating an auxiliary verbor (Sx\Ny)/(Sz\Ny) indicating a control verb.
Ini-tially, the predicates in the expression/graph will bebased entirely on the surface form of the words inthe sentence.
This is the ?ungrounded?
semanticrepresentation.Our next step is to convert these ungroundedgraphs to Freebase graphs.2 Like Reddy et al (2014),we treat this problem as a graph matching problem.Using GRAPHPARSER we retrieve all the Freebasegraphs that are isomorphic to the ungrounded graph,and select only the graphs that could correctly pre-dict the blank slot, as candidate graphs.
Using thesecandidate graphs, we train a structured perceptronthat learns to rank grounded graphs for a given un-grounded graph.3 We use ungrounded predicate andFreebase predicate alignments as our features.5 Experiments5.1 Training and Evaluation DatasetsOur dataset SPADES (Semantic PArsing ofDEclarative Sentences) is constructed from thedeclarative sentences collected by Reddy et al (2014)from CLUEWEB09 (Gabrilovich et al, 2013) basedon the following constraints: 1) There exists at least2Note that there is one-to-one correspondence between Free-base graphs and Freebase logical forms.3Please see Section 4.3 of Reddy et al (2016) for details.Sentences Tokens Types EntitiesTrain 79,247 685,922 69,095 37,606Dev 4,763 41,102 9,306 4,358Test 9,309 80,437 15,180 7,431Table 1: SPADES Corpus Statisticsone isomorphic Freebase graph to the ungroundedrepresentation of the input sentence; 2) There are novariable nodes in the ungrounded graph (e.g., Googleacquired a company is discarded whereas Googleacquired the company Nest is selected).
We split thisdata into training (85%), development (5%) and test-ing (10%) sentences (Table 1).
We introduce emptyslots into these sentences by randomly removing anentity.
SPADES can be downloaded at http://github.com/sivareddyg/graph-parser.There has been other recent interest in similardatasets for sentence completion (Zweig et al, 2012)and machine reading (Hermann et al, 2015), but un-like other corpora our data is tied directly to Freebaseand requires the execution of a semantic parse to cor-rectly predict the missing entity.
This is made moreexplicit by the fact that one third of the entities inour test set are never seen during training, so withouta general approach to query creation and executionthere is a limit on a system?s performance.5.2 Our ModelsWe use different CCG parsers varying in the amountsof supervision.
For the UNSUPERVISED scenario,we use Bisk and Hockenmaier (2015)?s parser which2024CCGbank (Syntax) Slot Filling (Semantics)LF1 UF1 2 3 4 OverallSentences ?6K ?3K ?600 ?10KBag-of-Words ?
?
50.8 36.8 20.9 45.2Syntax UNSUPERVISED 37.1 64.2 41.6 30.4 24.5 37.3SEMI-SUPERVISED-POS 53.0 68.5 45.9 33.7 29.1 41.4SEMI-SUPERVISED-WORD 53.5 68.9 46.8 38.2 28.3 43.2SUPERVISED 84.2 91.0 49.3 42.0 30.9 46.1Table 2: Syntactic and semantic evaluation of the parsing models.
Left: Simplified labeled F1 and undirectedunlabeled F1 on CCGbank, Section 23.
Right: Slot filling performance (by number of entities per sentence).exploits a small set of universal rules to automaticallyinduce and weight a large set of lexical categories.For the semi-supervised, we explore two options ?SEMI-SUPERVISED-WORD and SEMI-SUPERVISED-POS.
We use Bisk et al in both settings but we con-strain its lexicon manually rather than inducing itfrom scratch.
In the former, we restrict the top 200words in English to occur only with the CCG cat-egories that comprise 95% of the occurrences of aword?s use in Section 22 of WSJ/CCGbank.
In thelatter, we restrict the POS tags instead of words.
Forthe SUPERVISED scenario, we use EasyCCG (Lewisand Steedman, 2014) trained on CCGbank.Finally, in order to further demonstrate the amountof useful information being learned by our parsers,we present a competitive Bag-of-Words baseline,which is a perceptron classifier that performs ?se-mantic parsing?
by predicting either a Freebase or anull relation between the empty slot and every otherentity in the sentence, using the words in the sentenceas features.
This naive approach is competitive onsimple sentences with only two entities, rivaling eventhe fully supervised parser, but falters as complexityincreases.5.3 Results and DiscussionOur primary focus is a comparison of intrinsic syn-tactic evaluation with our extrinsic semantic evalu-ation.
To highlight the differences we present Sec-tion 23 parsing performance for our four models (Ta-ble 2).
Dependency performance is evaluated on boththe simplified labeled F1 of Bisk and Hockenmaier(2015) and Undirected Unlabeled F1.Despite the supervised parser performing almosttwice as well as the semi-supervised parsers on CCG-bank LF1 (53 vs 84), in our semantic evaluation wesee a comparatively small gain in performance (43vs 46).
It is interesting that such weakly supervisedmodels are able to achieve over 90% of the perfor-mance of a fully supervised parser.
To explore thisfurther, we break down the semantics performanceof all our models by the number of entities in a sen-tence.
Each sentence has two, three, or four entities,one of which will be dropped for prediction.
Themore entities there are in a sentence, the more likelythe models are to misanalyze a relation leading totheir making the wrong prediction.
These results arepresented on the right side of Table 2.
There arestill notable discrepancies in performance, which weanalyze more closely in the next section.Another interesting result is the drop in perfor-mance by the Bag-of-Words Model.
As the numberof entities in the sentence increase, the model weak-ens, performing worse than the unsupervised parseron sentences with four entities.
It becomes non-trivialfor it to isolate which entities and relations shouldbe used for prediction.
This seems to indicate thatthe unsupervised grammar is capturing more usefulsyntactic/semantic information than what is availablefrom the words alone.
Ensemble systems that incor-porate syntax and a Bag-of-Words baseline may yieldeven better performance.5.4 The Benefits of AnnotationThe performance of SEMI-SUPERVISED-POS andSEMI-SUPERVISED-WORD suggests that when re-sources are scarce, it is beneficial to create a even asmall lexicon of CCG categories.
We analyze thisfurther in Figure 3.
Here we show how performancechanges as a function of the number of labeled lexicaltypes.
Our values range from 0 to 1000 lexical types.We see syntactic improvements of 16pts and seman-2025Table 1Annotated Words Syntax Semantics0 37.1 37.3100 48.49 40.9200 53.5 43.2500 53.36 42.41000 49.87 42.4F115304560# of Annotated Lexical Types0 100 200 500 1000SyntaxSemantics 1Figure 3: When our word based lexicon grows past 200lexical types the semantic performance plateaus and thesyntax begins to degrade.
This is presumably due to theuse of rare categories coupled with domain differences.tic gains of 6pts with 200 words, before performancedegrades.
It is possible that increasing annotationmay only benefit fully supervised models.
Finally,when computing the most frequent lexical types weexcluded commas.
We found a 3pt performance dropwhen restricting commas to the category , (they arecommonly conj in our data).
Additional in-domainknowledge might further improve performance.5.5 Common ErrorsBisk and Hockenmaier (2015) performed an in-depthanalysis of the types of categories learned and cor-rectly used by their models (the same models as thispaper).
Their analysis was based on syntactic eval-uation against CCGbank.
In particular, they foundthe most egregious ?semantic?
errors to be the mis-use of verb chains, possessives and PP attachment(bottom of Table 3).
Since we now have access toa purely semantic evaluation, we can therefore askwhether these errors exist here, and how commonthey are.
We do this analysis in two steps.
First,we manually analyzed parses for which the unsuper-vised model failed to predict the correct semantics,but where the supervised parser succeeded.
The topof Table 3 presents several of the most common rea-sons for failure.
These mistakes were more mundane(e.g.
incorrect use of a conjunction) than failures touse complex CCG categories or analyze attachments.Second, we can compare grammatical decisionsmade by the semi-supervised and unsupervisedparsers against EasyCCG on sentences they suc-cessfully grounded.
Bisk and Hockenmaier (2015)found that their unsupervised parser made mistakeson many very simple categories.
We found the sameError ExamplePrevalent Incorrect conjunction Stockholm, SwedenAppositive , a chemist ,Introductory clauses In Frankfurt, ...Reduced relatives ... , established in 1909, ...B&H15 Verb chains is also headquarteredPossessive Anderson ?s FoundationPP Attachment of the foundation in VancouverTable 3: Causes of semantic grounding errors with exam-ples not previously isolated via intrinsic evaluation.result.
When evaluating our parsers against the tree-bank we found the unsupervised model only correctlypredicted transitive verbs 20% of the time and ad-verbs 39% of the time.
In contrast, on our data, weproduced the correct transitive category (accordingto EasyCCG) 65% of the time, and the correct adverb68% of the time.
These correct parsing decisions alsolead to improved performance across many other cat-egories (e.g.
prepositions).
This is likely due to ourcorpus containing simpler constructions.
In contrast,auxiliary verbs, relative clauses, and commas stillproved difficult or harder than in the treebank.
Thisimplies that future work should tailor the annotationeffort to their specific domain rather than relying onguidance solely from the treebank.6 ConclusionOur goal in this paper was to present the first seman-tic evaluation of induced grammars in order to betterunderstand their utility and strengths.
We showedthat induced grammars are learning more semanti-cally useful structure than a Bag-of-Words model.Furthermore, we showed how minimal syntactic su-pervision can provide substantial gains in semanticevaluation.
Our ongoing work explores creating asyntax-semantics loop where each benefits the otherwith no human (annotation) in the loop.AcknowledgmentsThis paper is partly based on work that was donewhen the first and second authors were interns atGoogle, and on work that that was supported by NSFgrant 1053856 to JH, and a Google PhD Fellowshipto SR.2026ReferencesJonathan Berant, Andrew Chou, Roy Frostig, and PercyLiang.
2013.
Semantic parsing on Freebase fromquestion-answer pairs.
In Proceedings of the 2013 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1533?1544, Seattle, Washington,USA, October.Yonatan Bisk and Julia Hockenmaier.
2013.
An HDPModel for Inducing Combinatory Categorial Grammars.Transactions of the Association for Computational Lin-guistics, pages 75?88.Yonatan Bisk and Julia Hockenmaier.
2015.
Probingthe linguistic strengths and limitations of unsupervisedgrammar induction.
In Proceedings of the 53rd AnnualMeeting of the Association for Computational Linguis-tics, Beijing,China, July.Johan Bos, Stephen Clark, Mark Steedman, James R Cur-ran, and Julia Hockenmaier.
2004.
Wide-coveragesemantic representations from a CCG parser.
In Pro-ceedings of the 20th international conference on Com-putational Linguistics, page 1240.Qingqing Cai and Alexander Yates.
2013.
Semantic pars-ing freebase: Towards open-domain semantic parsing.In Second Joint Conference on Lexical and Computa-tional Semantics (*SEM), Volume 1: Proceedings ofthe Main Conference and the Shared Task: SemanticTextual Similarity, pages 328?338, Atlanta, Georgia,USA, June.Glenn Carroll and Eugene Charniak.
1992.
Two Experi-ments on Learning Probabilistic Dependency Gram-mars from Corpora.
Working Notes of the Work-shop Statistically-Based NLP Techniques, pages 1?15,March.Evgeniy Gabrilovich, Michael Ringgaard, and AmarnagSubramanya.
2013.
FACC1: Freebase annotation ofClueWeb corpora, Version 1 (Release date 2013-06-26,Format version 1, Correction level 0), June.Dan Garrette, Chris Dyer, Jason Baldridge, and Noah ASmith.
2015.
Weakly-Supervised Grammar-InformedBayesian CCG Parser Learning.
In Proceedings of theAssociation for the Advancement of Artificial Intelli-gence.Karl Moritz Hermann, Tomas Kocisky, Edward Grefen-stette, Lasse Espeholt, Will Kay, Mustafa Suleyman,and Phil Blunsom.
2015.
Teaching machines to readand comprehend.
In Advances in Neural InformationProcessing Systems 28, pages 1693?1701.Dan Klein and Christopher D Manning.
2004.
Corpus-Based Induction of Syntactic Structure: Models of De-pendency and Constituency.
In Proceedings of the42nd Meeting of the Association for ComputationalLinguistics (ACL?04), Main Volume, pages 478?485,Barcelona, Spain, July.Mike Lewis and Mark Steedman.
2014.
A* CCG Parsingwith a Supertag-factored Model.
In Proceedings of the2014 Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP), pages 990?1000, Doha,Qatar, October.Siva Reddy, Mirella Lapata, and Mark Steedman.2014.
Large-scale Semantic Parsing without Question-Answer Pairs.
Transactions of the Association for Com-putational Linguistics, pages 1?16, June.Siva Reddy, Oscar Ta?ckstro?m, Michael Collins, TomKwiatkowski, Dipanjan Das, Mark Steedman, andMirella Lapata.
2016.
Transforming DependencyStructures to Logical Forms for Semantic Parsing.Transactions of the Association for Computational Lin-guistics, 4:127?140.Valentin I Spitkovsky, Hiyan Alshawi, and Daniel Juraf-sky.
2010.
From Baby Steps to Leapfrog: How ?Lessis More?
in Unsupervised Dependency Parsing.
In Hu-man Language Technologies: The 2010 Annual Confer-ence of the North American Chapter of the Associationfor Computational Linguistics, pages 751?759, LosAngeles, California, June.Mark Steedman.
2000.
The Syntactic Process.
The MITPress, September.Deniz Yuret.
1998.
Discovery of Linguistic RelationsUsing Lexical Attraction.
Ph.D. thesis, MassachusettsInstitute of Technology.Geoffrey Zweig, John C. Platt, Christopher Meek, Christo-pher J.C. Burges, Ainur Yessenalina, and Qiang Liu.2012.
Computational approaches to sentence comple-tion.
In Proceedings of the 50th Annual Meeting of theAssociation for Computational Linguistics (Volume 1:Long Papers), pages 601?610, Jeju Island, Korea, July.2027
