Proceedings of the NAACL HLT Workshop on Active Learning for Natural Language Processing, pages 45?48,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsA Web Survey on the Use of Active Learningto Support Annotation of Text DataKatrin TomanekJena University Language & Information Engineering LabFriedrich-Schiller-Universita?t JenaFu?rstengraben 30, D-07743 Jena, Germanykatrin.tomanek@uni-jena.deFredrik OlssonSICSBox 1263SE-164 29 Kista, Swedenfredrik.olsson@sics.seAbstractAs supervised machine learning methods foraddressing tasks in natural language process-ing (NLP) prove increasingly viable, the fo-cus of attention is naturally shifted towards thecreation of training data.
The manual annota-tion of corpora is a tedious and time consum-ing process.
To obtain high-quality annotateddata constitutes a bottleneck in machine learn-ing for NLP today.
Active learning is one wayof easing the burden of annotation.
This pa-per presents a first probe into the NLP researchcommunity concerning the nature of the anno-tation projects undertaken in general, and theuse of active learning as annotation support inparticular.1 IntroductionSupervised machine learning methods have beensuccessfully applied to many NLP tasks in the lastfew decades.
While these techniques have shownto work well, they require large amounts of labeledtraining data in order to achieve high performance.Creating such training data is a tedious, time con-suming and error prone process.
Active learning(AL) is a supervised learning technique that can beused to reduce the annotation effort.
The main ideain AL is to put the machine learner in control of thedata from which it learns; the learner can ask an or-acle (typically a human) about the labels of the ex-amples for which the model learned so far makesunreliable predictions.
The active learning processtakes as input a set of labeled examples, as well asa larger set of unlabeled examples, and produces aclassifier and a relatively small set of newly labeleddata.
The overall goal is to create as good a classifieras possible, without having to mark-up and supplythe learner with more data than necessary.
AL aimsat keeping the human annotation effort to a mini-mum, only asking the oracle for advice where thetraining utility of the result of such a query is high.Settles (2009) gives a detailed overview of the liter-ature on AL.It has been experimentally shown that AL can in-deed be successfully applied to a range of NLP tasksincluding, e.g., text categorization (Lewis and Gale,1994), part-of-speech tagging (Dagan and Engelson,1995; Ringger et al, 2007), parsing (Becker and Os-borne, 2005), and named entity recognition (Shen etal., 2004; Tomanek et al, 2007).
Despite that some-what impressive results in terms of reduced anno-tation effort have been achieved by such studies, itseems that AL is rarely applied in real-life annota-tion endeavors.This paper presents the results from a web surveywe arranged to analyze the extent to which AL hasbeen used to support the annotation of textual data inthe context of NLP, as well as addressing the reasonsto why or why not AL has been found applicable to aspecific task.
Section 2 describes the survey in gen-eral, Section 3 introduces the questions and presentsthe answers received.
Finally, the answers receivedare discussed in Section 4.2 The SurveyThe survey was realized in the form of a web-basedquestionnaire; the primary reason for this approach,as opposed to reading and compiling information45from academic publications, was that we wanted tofree ourselves and the participants from the dos anddon?ts common to the discourse of scientific papers.The survey targeted participants who were in-volved in the annotation of textual data intended formachine learning for all kinds of NLP tasks.
It wasannounced on the following mailing lists: BioNLP,Corpora, UAI List, ML-news, SIG-IRlist, Linguistlist, as well as lists reaching members of SIGANN,SIGNLL, and ELRA.
By utilizing these mailinglists, we expect to have reached a fairly large por-tion of the researchers likely to participate in anno-tation projects for NLP.
The questionnaire was openFebruary 6?23, 2009.After an introductory description and one initialquestion, the questionnaire was divided into twobranches.
The first branch was answered by thosewho had used AL to support their annotation, whilethe second branch was answered by those who hadnot.
Both branches shared a common first part aboutthe general set-up of the annotation project underscrutiny.
The second part of the AL-branch focusedon experiences made with applied AL.
The secondpart of the non AL-branch asked questions about thereasons why AL had not been used.
Finally, thequestionnaire was concluded by a series of questionstargeting the background of the participant.The complete survey can be downloaded fromhttp://www.julielab.de/ALSurvey.3 Questions and answers147 people participated in the survey.
54 completedthe survey while 93 did not, thus the overall comple-tion rate was 37 %.
Most of the people who did notcomplete the questionnaire answered the first coupleof questions but did not continue.
Their answers arenot part of the discussion below.
We refrain from astatistically analysis of the data but rather report onthe distribution of the answers received.Of the people that finished the survey, the ma-jority (85 %) came from academia, with the restuniformly split between governmental organizationsand industry.
The educational background of theparticipants were mainly computational linguistics(46 %), general linguistics (22 %), and computer sci-ence (22 %).3.1 Questions common to both branchesBoth the AL and the non-AL branch were askedseveral questions about the set-up of the annotationproject under scrutiny.
The questions concerned,e.g., whether AL had been used to support the anno-tation process, the NLP tasks addressed, the size ofthe project, the constitution of the corpus annotated,and how the decision when to stop the annotationprocess was made.The use of AL as annotation support.
The firstquestion posed was whether people had used AL assupport in their annotation projects.
11 participants(20 %) answered this question positively, while 43(80 %) said that they had not used AL.The task addressed.
Most AL-based annotationprojects concerned the tasks information extraction(IE) (52 %), document classification (17.6 %), and(word sense) disambiguation (17.6 %).
Also in nonAL-based projects, most participants had focused onIE tasks (36.8 %).
Here, syntactic tasks includingpart-of-speech tagging, shallow, and deep parsingwere also often considered (19.7 %).
Textual phe-nomena, such as coreferences and discourse struc-ture (9.6 %), and word sense disambiguation (5.5 %)formed two other answer groups.
Overall, the nonAL-based annotation projects covered a wider vari-ety of NLP tasks than the AL-based ones.
All AL-based annotation projects concerned English texts,whereas of the non-AL projects only 62.8 % did.The size of the project.
The participants were alsoasked for the size of the annotation project in termsof number of units annotated, number of annotatorsinvolved and person months per annotator.
The av-erage number of person months spent on non AL-projects was 21.2 and 8.7 for AL-projects.
However,these numbers are subject to a high variance.The constitution of the corpus.
Further, the par-ticipants were asked how the corpus of unlabeledinstances was selected.1 The answer options in-cluded (a) taking all available instances, (b) a ran-dom subset of them, (c) a subset based on key-words/introspection, and (d) others.
In the AL-branch, the answers were uniformly distributed be-1The unlabeled instances are used as a pool in AL, and as acorpus in non AL-based annotation.46tween the alternatives.
In the non AL-branch, themajority of participants had used alternatives (a)(39.5 %) and (b) (34.9 %).The decision to stop the annotation process.
Alast question regarding general annotation projectexecution concerned the stopping of the annotationprocess.
In AL-based projects, evaluation on a held-out gold standard (36.5 %) and the exhaustion ofmoney or time (36.5 %) were the major stopping cri-teria.
Specific stopping criteria based on AL-internalaspects were used only once, while in two cases theannotation was stopped because the expected gainsin model performance fell below a given threshold.In almost half (47.7 %) of the non AL-basedprojects the annotation was stopped since the avail-able money or time had been used up.
Another ma-jor stopping criterion was the fact that the completecorpus was annotated (36 %).
Only in two cases an-notation was stopped based on an evaluation of themodel achievable from the corpus.3.2 Questions specific to the AL-branchThe AL-specific branch of the questionnaire wasconcerned with two aspects: the learning algorithmsinvolved, and the experiences of the participants re-garding the use of AL as annotation support.
Per-centages presented below are all related to the 11persons who answered this branch.Learning algorithms used.
As for the AL meth-ods applied, there was no single most preferredapproach.
27.3 % had used uncertainty sampling,18.2 % query-by-committee, another 18.2% errorreduction-based approaches, and 36.4 % had usedan ?uncanonical?
or totally different approach whichwas not covered by any of these categories.
Asbase learners, maximum-entropy based approachesas well as Support-Vector machines were most fre-quently used (36.4 % each).Experiences.
When asked about their experi-ences, the participants reported that their expecta-tions with respect to AL had been partially (54.4 %)or fully (36.3 %) met, while one of the participantswas disappointed.
The AL participants did not leavemany experience reports in the free text field.
Fromthe few received, it was evident that the samplingcomplexity and the resulting delay or idle time ofthe annotators, as well as the interface design arecritical issues in the practical realization of AL asannotation support.3.3 Question specific to the non-AL branchThe non AL-specific branch of the questionnairewas basically concerned with why people did not useAL as annotation support and whether this situationcould be changed.
The percentages given below arerelated to the 43 people who answered this particularpart of the questionnaire.Why was not AL used?
Participants could givemultiple answers to this question.
Many partici-pants had either never heard of AL (11 %) or didnot use AL due to insufficient knowledge or exper-tise (26 %).
The implementational overhead to de-velop an AL-enabled annotation editor kept 17.8 %of the participants from using AL.
Another 19.2 %of the participants stated that their project specificrequirements did not allow them to use AL.
Giventhe comments given in the free text field, it can bededuced that this was often the case when peoplewanted to create a corpus that could be used for amultitude of purposes (such as building statistics on,cross-validation, learning about the annotation taskper se, and so forth) and not just for classifier train-ing.
In such scenarios, the sampling bias introducedby AL is certainly disadvantageous.
Finally, about20.5 % of the participants were not convinced thatAL would work well in their scenario or really re-duce annotation effort.
Some participants stated intheir free form comments that while they believedAL would reduce the amount of instances to be an-notated it would probably not reduce the overall an-notation time.Would you consider using AL in future projects?According to the answers of another question of thesurvey, 40 % would in general use AL, while 56 %were sceptical but stated that they would possiblyuse a technique such as AL.4 DiscussionAlthough it cannot be claimed that the data collectedin this survey is representative for the NLP researchcommunity as a whole, and the number of partic-ipants was too low to draw statistically firm con-clusions, some interesting trends have indeed been47discovered within the data itself.
The conclusionsdrawn in this section are related to the answers pro-vided in light of the questions posed in the survey.The questionnaire was open to the public and wasnot explicitly controlled with respect to the distribu-tion of characteristics of the sample of the commu-nity that partook in it.
One effect of this, coupledwith the fact that the questionnaire was biased to-wards those familiar with AL, is that we believe thatthe group of people that have used AL are overrep-resented in the data at hand.
However, this cannotbe verified.
Nevertheless, given this and the poten-tial reach of the mailing lists used for announcingthe survey, it is remarkable that not more than 20 %(11 out of 54) of the participants had used AL asannotation support.The doubts of the participants who did not useAL towards considering the technique as a poten-tial aid in annotation in essence boil down to theabsence of an AL-based annotation editor, as wellas the difficulty in estimating the effective reductionin effort (such as time, money, labor) that the useof AL imply.
Put simply: Can AL for NLP reallycut annotation costs?
Can AL for NLP be practi-cally realized without too much overhead in termsof implementation and education of the annotator?Research addressing the former question is ongo-ing which is shown, e.g., by the recent Workshop onCost-Sensitive Learning held in conjunction with theNeural Information Processing Systems Conference2008.
As for the latter question, there is evidently aneed of a general framework for AL in which (spe-cialized) annotation editors can be realized.
Also,hand-in-hand with the theoretical aspects of AL andtheir practical realizations in terms of available soft-ware packages, there clearly is a need for usage anduser studies concerning the effort required by humanannotators operating under AL-based data selectionschemes in real annotation tasks.Two things worth noticing among the answersfrom participants of the survey that had used AL in-clude that most of these participants had positive ex-periences from using AL, although turn-around timeand consequently the idle time of the annotator re-mains a critical issue; and that English was the onlylanguage addressed.
This is somewhat surprisinggiven that AL seems to be a technique well suitedfor bootstrapping language resources for, e.g., socalled ?under resourced?
languages.
Also we weresurprised by the fact that both in AL and non-ALprojects rather ?unsophisticated?
criteria were usedto decide about the stopping of annotation projects.AcknowledgementsThe first author was funded by the German Min-istry of Education and Research within the STEM-NET project (01DS001A-C) and the EC within theBOOTStrep project (FP6-028099).
We wish tothank Bjo?rn Gamba?ck for commenting and proof-reading drafts of this paper.ReferencesMarkus Becker and Miles Osborne.
2005.
A two-stagemethod for active learning of statistical grammars.
InProc.
of the 19th International Joint Conference on Ar-tificial Intelligence, pages 991?996.Ido Dagan and Sean P. Engelson.
1995.
Committee-based sampling for training probabilistic classifiers.
InProc.
of the 12th International Conference on MachineLearning, pages 150?157.David D. Lewis and William A. Gale.
1994.
A Sequen-tial Algorithm for Training Text Classifiers.
In Proc.of the 17th Annual International ACM-SIGIR Confer-ence on Research and Development in Information Re-trieval, pages 3?12.Eric Ringger, Peter McClanahan, Robbie Haertel, GeorgeBusby, Marc Carmen, James Carroll, Kevin Seppi, andDeryle Lonsdale.
2007.
Active learning for part-of-speech tagging: Accelerating corpus annotation.
InProc.
of the Linguistic Annotation Workshop, pages101?108.Burr Settles.
2009.
Active learning literature survey.Computer Sciences Technical Report 1648, Universityof Wisconsin-Madison.Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, and Chew-Lim Tan.
2004.
Multi-criteria-based active learningfor named entity recognition.
In Proc.
of the 42ndAnnual Meeting of the Association for ComputationalLinguistics, pages 589?596.Katrin Tomanek, Joachim Wermter, and Udo Hahn.2007.
An approach to text corpus construction whichcuts annotation costs and maintains reusability of an-notated data.
In Proc.
of the 2007 Joint Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning,pages 486?495.48
