First Joint Conference on Lexical and Computational Semantics (*SEM), pages 565?570,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsDeepPurple: Estimating Sentence Semantic Similarity usingN-gram Regression Models and Web SnippetsNikos Malandrakis, Elias Iosif, Alexandros PotamianosDepartment of ECE, Technical University of Crete, 73100 Chania, Greece[nmalandrakis,iosife,potam]@telecom.tuc.grAbstractWe estimate the semantic similarity betweentwo sentences using regression models withfeatures: 1) n-gram hit rates (lexical matches)between sentences, 2) lexical semantic sim-ilarity between non-matching words, and 3)sentence length.
Lexical semantic similarity iscomputed via co-occurrence counts on a cor-pus harvested from the web using a modifiedmutual information metric.
State-of-the-art re-sults are obtained for semantic similarity com-putation at the word level, however, the fusionof this information at the sentence level pro-vides only moderate improvement on Task 6of SemEval?12.
Despite the simple featuresused, regression models provide good perfor-mance, especially for shorter sentences, reach-ing correlation of 0.62 on the SemEval test set.1 IntroductionRecently, there has been significant research activ-ity on the area of semantic similarity estimationmotivated both by abundance of relevant web dataand linguistic resources for this task.
Algorithmsfor computing semantic textual similarity (STS) arerelevant for a variety of applications, including in-formation extraction (Szpektor and Dagan, 2008),question answering (Harabagiu and Hickl, 2006)and machine translation (Mirkin et al, 2009).
Word-or term-level STS (a special case of sentence levelSTS) has also been successfully applied to the prob-lem of grammar induction (Meng and Siu, 2002)and affective text categorization (Malandrakis et al,2011).
In this work, we built on previous researchon word-level semantic similarity estimation to de-sign and implement a system for sentence-level STSfor Task6 of the SemEval?12 campaign.Semantic similarity between words can be re-garded as the graded semantic equivalence at thelexeme level and is tightly related with the tasks ofword sense discovery and disambiguation (Agirreand Edmonds, 2007).
Metrics of word semantic sim-ilarity can be divided into: (i) knowledge-based met-rics (Miller, 1990; Budanitsky and Hirst, 2006) and(ii) corpus-based metrics (Baroni and Lenci, 2010;Iosif and Potamianos, 2010).When more complex structures, such as phrasesand sentences, are considered, it is much harderto estimate semantic equivalence due to the non-compositional nature of sentence-level semanticsand the exponential explosion of possible interpre-tations.
STS is closely related to the problems ofparaphrasing, which is bidirectional and based onsemantic equivalence (Madnani and Dorr, 2010) andtextual entailment, which is directional and basedon relations between semantics (Dagan et al, 2006).Related methods incorporate measurements of sim-ilarity at various levels: lexical (Malakasiotis andAndroutsopoulos, 2007), syntactic (Malakasiotis,2009; Zanzotto et al, 2009), and semantic (Rinaldiet al, 2003; Bos and Markert, 2005).
Measuresfrom machine translation evaluation are often usedto evaluate lexical level approaches (Finch et al,2005; Perez and Alfonseca, 2005), including BLEU(Papineni et al, 2002), a metric based on word n-gram hit rates.Motivated by BLEU, we use n-gram hit rates andword-level semantic similarity scores as features in565a linear regression model to estimate sentence levelsemantic similarity.
We also propose sigmoid scal-ing of similarity scores and sentence-length depen-dent modeling.
The models are evaluated on the Se-mEval?12 sentence similarity task.2 Semantic similarity between wordsIn this section, two different metrics of word simi-larity are presented.
The first is a language-agnostic,corpus-based metric requiring no knowledge re-sources, while the second metric relies on WordNet.Corpus-based metric: Given a corpus, the se-mantic similarity between two words, wi and wj ,is estimated as their pointwise mutual information(Church and Hanks, 1990): I(i, j) = log p?(i,j)p?(i)p?
(j) ,where p?
(i) and p?
(j) are the occurrence probabili-ties of wi and wj , respectively, while the probabilityof their co-occurrence is denoted by p?
(i, j).
Theseprobabilities are computed according to maximumlikelihood estimation.
The assumption of this met-ric is that co-occurrence implies semantic similarity.During the past decade the web has been used forestimating the required probabilities (Turney, 2001;Bollegala et al, 2007), by querying web search en-gines and retrieving the number of hits requiredto estimate the frequency of individual words andtheir co-occurrence.
However, these approacheshave failed to obtain state-of-the-art results (Bolle-gala et al, 2007), unless ?expensive?
conjunctiveAND queries are used for harvesting a corpus andthen using this corpus to estimate similarity scores(Iosif and Potamianos, 2010).Recently, a scalable approach1 for harvesting acorpus has been proposed where web snippets aredownloaded using individual queries for each word(Iosif and Potamianos, 2012b).
Semantic similar-ity can then be estimated using the I(i, j) metricand within-snippet word co-occurrence frequencies.Under the maximum sense similarity assumption(Resnik, 1995), it is relatively easy to show that a(more) lexically-balanced corpus2 (as the one cre-1The scalability of this approach has been demonstrated in(Iosif and Potamianos, 2012b) for a 10K vocabulary, here weextend it to the full 60K WordNet vocabulary.2According to this assumption the semantic similarity of twowords can be estimated as the minimum pairwise similarity oftheir senses.
The gist of the argument is that although wordsoften co-occur with their closest senses, word occurrences cor-ated above) can significantly reduce the semanticsimilarity estimation error of the mutual informationmetric I(i, j).
This is also experimentally verified in(Iosif and Potamianos, 2012c).In addition, one can modify the mutual informa-tion metric to further reduce estimation error (forthe theoretical foundation behind this see (Iosif andPotamianos, 2012a)).
Specifically, one may intro-duce exponential weights ?
in order to reduce thecontribution of p(i) and p(j) in the similarity met-ric.
The modified metric Ia(i, j), is defined as:Ia(i, j)=12[logp?
(i, j)p??(i)p?
(j) + logp?
(i, j)p?(i)p??(j)].
(1)The weight ?
was estimated on the corpus of (Iosifand Potamianos, 2012b) in order to maximize wordsense coverage in the semantic neighborhood ofeach word.
The Ia(i, j) metric using the estimatedvalue of ?
= 0.8 was shown to significantly out-perform I(i, j) and to achieve state-of-the-art resultson standard semantic similarity datasets (Rubensteinand Goodenough, 1965; Miller and Charles, 1998;Finkelstein et al, 2002).
For more details see (Iosifand Potamianos, 2012a).WordNet-based metrics: For comparison pur-poses, we evaluated various similarity metrics onthe task of word similarity computation on threestandard datasets (same as above).
The best re-sults were obtained by the Vector metric (Patward-han and Pedersen, 2006), which exploits the lexicalinformation that is included in the WordNet glosses.This metric was incorporated to our proposed ap-proach.
All metrics were computed using the Word-Net::Similarity module (Pedersen, 2005).3 N-gram Regression ModelsInspired by BLEU (Papineni et al, 2002), we pro-pose a simple regression model that combines evi-dence from two sources: number of n-gram matchesand degree of similarity between non-matchingwords between two sentences.
In order to incorpo-rate a word semantic similarity metric into BLEU,we apply the following two-pass process: first lexi-cal hits are identified and counted, and then the se-mantic similarity between n-grams not matched dur-respond to all senses, i.e., the denominator of I(i, j) is overes-timated causing large underestimation error for similarities be-tween polysemous words.566ing the first pass is estimated.
All word similar-ity metrics used are peak-to-peak normalized in the[0,1] range, so they serve as a ?degree-of-match?.The semantic similarity scores from word pairs aresummed together (just like n-gram hits) to obtaina BLEU-like semantic similarity score.
The mainproblem here is one of alignment, since we needto compare each non-matched n-gram from the hy-pothesis with an n-gram from the reference.
Weuse a simple approach: we iterate on the hypoth-esis n-grams, left-to-right, and compare each withthe most similar non-matched n-gram in the refer-ence.
This modification to BLEU is only appliedto 1-grams, since semantic similarity scores for bi-grams (or higher) were not available.Thus, our list of features are the hit rates obtainedby BLEU (for 1-, 2-, 3-, 4-grams) and the total se-mantic similarity (SS) score for 1-grams3.
Thesefeatures are then combined using a multiple linearregression model:D?L = a0 +4?n=1an Bn + a5 M1, (2)where D?L is the estimated similarity, Bn is theBLEU hit rate for n-grams, M1 is the total semanticsimilarity score (SS) for non-matching 1-grams andan are the trainable parameters of the model.Motivated by evidence of cognitive scaling ofsemantic similarity scores (Iosif and Potamianos,2010), we propose the use of a sigmoid function toscale DL sentence similarities.
We have also ob-served in the SemEval data that the way humans ratesentence similarity is very much dependent on sen-tence length4.
To capture the effect of length andcognitive scaling we propose next two modificationsto the linear regression model.
The sigmoid fusionscheme is described by the following equation:D?S = a6D?L + a7D?L[1 + exp(a8 ?
la9)]?1, (3)where we assume that sentence length l (average3Note that the features are computed twice on each sentencein a forward and backward fashion (where the word order isreversed), and then averaged between the two runs.4We speculate that shorter sentences are mostly compared atthe lexical level using the short-term memory language buffers,while longer sentences tend to be compared at a higher cogni-tive level, where the non-compositional nature of sentence se-mantics dominate.length for each sentence pair, in words) acts as ascaling factor for the linearly estimated similarity.The hierarchical fusion scheme is actually a col-lection of (overlapping) linear regression models,each matching a range of sentence lengths.
For ex-ample, the first model DL1 is trained with sentenceswith length up to l1, i.e., l ?
l1, the second modelDL2 up to length l2 etc.
During testing, sentenceswith length l ?
[1, l1] are decoded with DL1, sen-tences with length l ?
(l1, l2] with model DL2 etc.Each of these partial models is a linear fusion modelas shown in (2).
In this work, we use four modelswith l1 = 10, l2 = 20, l3 = 30, l4 =?.4 Experimental Procedure and ResultsInitially all sentences are pre-processed by theCoreNLP (Finkel et al, 2005; Toutanova et al,2003) suite of tools, a process that includes namedentity recognition, normalization, part of speech tag-ging, lemmatization and stemming.
The exact typeof pre-processing used depends on the metric used.For the plain lexical BLEU, we use lemmatization,stemming (of lemmas) and remove all non-contentwords, keeping only nouns, adjectives, verbs and ad-verbs.
For computing semantic similarity scores, wedon?t use stemming and keep only noun words, sincewe only have similarities between non-noun words.For the computation of semantic similarity we havecreated a dictionary containing all the single-wordnouns included in WordNet (approx.
60K) and thendownloaded snippets of the 500 top-ranked docu-ments for each word by formulating single-wordqueries and submitting them to the Yahoo!
searchengine.Next, results are reported in terms of correlationbetween the automatically computed scores and theground truth, for each of the corpora in Task 6 ofSemEval?12 (paraphrase, video, europarl, WordNet,news).
Overall correlation (?Ovrl?)
computed on thejoin of the dataset, as well as, average (?Mean?)
cor-relation across all task is also reported.
Training isperformed on a subset of the first three corpora andtesting on all five corpora.Baseline BLEU: The first set of results in Ta-ble 1, shows the correlation performance of theplain BLEU hit rates (per training data set and over-all/average).
The best performing hit rate is the one567calculated using unigrams.Table 1: Correlation performance of BLEU hit rates.par vid euro Mean OvrlBLEU 1-grams 0.62 0.67 0.49 0.59 0.57BLEU 2-grams 0.40 0.39 0.37 0.39 0.34BLEU 3-grams 0.32 0.36 0.30 0.33 0.33BLEU 4-grams 0.26 0.25 0.24 0.25 0.28Semantic Similarity BLEU (Purple): The perfor-mance of the modified version of BLEU that in-corporates various word-level similarity metrics isshown in Table 2.
Here the BLEU hits (exactmatches) are summed together with the normalizedsimilarity scores (approximate matches) to obtain asingle B1+M1 (Purple) score5.
As we can see, thereare definite benefits to using the modified version,particularly with regards to mean correlation.
Over-all the best performers, when taking into accountboth mean and overall correlation, are the WordNet-based and Ia metrics, with the Ia metric winning bya slight margin, earning a place in the final models.Table 2: Correlation performance of 1-gram BLEUscores with semantic similarity metrics (nouns-only).par vid euro Mean OvrlBLEU 0.54 0.60 0.39 0.51 0.58SS-BLEU WordNet 0.56 0.64 0.41 0.54 0.58SS-BLEU I(i, j) 0.56 0.63 0.39 0.53 0.59SS-BLEU Ia(i, j) 0.57 0.64 0.40 0.54 0.58Regression models (DeepPurple): Next, the per-formance of the various regression models (fusionschemes) is investigated.
Each regression model isevaluated by performing 10-fold cross-validation onthe SemEval training set.
Correlation performanceis shown in Table 3 both with and without seman-tic similarity.
The baseline in this case is the Pur-ple metric (corresponding to no fusion).
Clearlythe use of regression models significantly improvesperformance compared to the 1-gram BLEU andPurple baselines for almost all datasets, and espe-cially for the combined dataset (overall).
Amongthe fusion schemes, the hierarchical models performthe best.
Following fusion, the performance gainfrom incorporating semantic similarity (SS) is muchsmaller.
Finally, in Table 4, correlation performanceof our submissions on the official SemEval test set is5It should be stressed that the plain BLEU unigram scoresshown in this table are not comparable to those in Table 1, sincehere scores are calculated over only the nouns of each sentence.Table 3: Correlation performance of regression modelwith (SS) and without semantic similarities on the train-ing set (using 10-fold cross-validation).par vid euro Mean OvrlNone (SS-BLEU Ia) 0.57 0.64 0.40 0.54 0.58Linear (D?L, a5=0) 0.62 0.72 0.47 0.60 0.66Sigmoid (D?S, a5=0) 0.64 0.73 0.42 0.60 0.73Hierarchical 0.64 0.74 0.48 0.62 0.73SS-Linear (D?L) 0.64 0.73 0.47 0.61 0.66SS-Sigmoid (D?S) 0.65 0.74 0.42 0.60 0.74SS-Hierarchical 0.65 0.74 0.48 0.62 0.73shown.
The overall correlation performance of theHierarchical model ranks somewhere in the middle(43rd out of 89 systems), while the mean correla-tion (weighted by number of samples per set) is no-tably better: 23rd out of 89.
Comparing the individ-ual dataset results, our systems underperform for thetwo datasets that originate from the machine transla-tion (MT) literature (and contain longer sentences),while we achieve good results for the rest (19th forparaphrase, 37th for video and 29th for WN).Table 4: Correlation performance on test set.par vid euro WN news Mean OvrlNone 0.50 0.71 0.44 0.49 0.24 0.51 0.49Sigm.
0.60 0.76 0.26 0.60 0.34 0.56 0.55Hier.
0.60 0.77 0.43 0.65 0.37 0.60 0.625 ConclusionsWe have shown that: 1) a regression model thatcombines counts of exact and approximate n-grammatches provides good performance for sentencesimilarity computation (especially for short andmedium length sentences), 2) the non-linear scal-ing of hit-rates with respect to sentence length im-proves performance, 3) incorporating word semanticsimilarity scores (soft-match) into the model can im-prove performance, and 4) web snippet corpus cre-ation and the modified mutual information metricis a language agnostic approach that can (at least)match semantic similarity performance of the bestresource-based metrics for this task.
Future work,should involve the extension of this approach tomodel larger lexical chunks, the incorporation ofcompositional models of meaning, and in generalthe phrase-level modeling of semantic similarity, inorder to compete with MT-based systems trained onmassive external parallel corpora.568ReferencesE.
Agirre and P. Edmonds, editors.
2007.
WordSense Disambiguation: Algorithms and Applications.Springer.M.
Baroni and A. Lenci.
2010.
Distributional mem-ory: A general framework for corpus-based semantics.Computational Linguistics, 36(4):673?721.D.
Bollegala, Y. Matsuo, and M. Ishizuka.
2007.
Mea-suring semantic similarity between words using websearch engines.
In Proc.
of International Conferenceon World Wide Web, pages 757?766.J.
Bos and K. Markert.
2005.
Recognising textual en-tailment with logical inference.
In Proceedings of theHuman Language Technology Conference and Confer-ence on Empirical Methods in Natural Language Pro-cessing, page 628635.A.
Budanitsky and G. Hirst.
2006.
Evaluating WordNet-based measures of semantic distance.
ComputationalLinguistics, 32:13?47.K.
W. Church and P. Hanks.
1990.
Word associationnorms, mutual information, and lexicography.
Com-putational Linguistics, 16(1):22?29.I.
Dagan, O. Glickman, and B. Magnini.
2006.The pascal recognising textual entailment challenge.In Joaquin Quionero-Candela, Ido Dagan, BernardoMagnini, and Florence dAlch Buc, editors, MachineLearning Challenges.
Evaluating Predictive Uncer-tainty, Visual Object Classification, and RecognisingTectual Entailment, volume 3944 of Lecture Notes inComputer Science, pages 177?190.
Springer Berlin /Heidelberg.A.
Finch, S. Y. Hwang, and E. Sumita.
2005.
Using ma-chine translation evaluation techniques to determinesentence-level semantic equivalence.
In Proceedingsof the 3rd International Workshop on Paraphrasing,page 1724.J.
R. Finkel, T. Grenager, and C. D. Manning.
2005.
In-corporating non-local information into information ex-traction systems by gibbs sampling.
In Proceedings ofthe 43rd Annual Meeting on Association for Computa-tional Linguistics, pages 363?370.L.
Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,Z.
Solan, G. Wolfman, and E. Ruppin.
2002.
Plac-ing search in context: The concept revisited.
ACMTransactions on Information Systems, 20(1):116?131.S.
Harabagiu and A. Hickl.
2006.
Methods for Us-ing Textual Entailment in Open-Domain Question An-swering.
In Proceedings of the 21st International Con-ference on Computational Linguistics and 44th AnnualMeeting of the Association for Computational Linguis-tics, pages 905?912.E.
Iosif and A. Potamianos.
2010.
Unsupervised seman-tic similarity computation between terms using webdocuments.
IEEE Transactions on Knowledge andData Engineering, 22(11):1637?1647.E.
Iosif and A. Potamianos.
2012a.
Minimum error se-mantic similarity using text corpora constructed fromweb queries.
IEEE Transactions on Knowledge andData Engineering (submitted to).E.
Iosif and A. Potamianos.
2012b.
Semsim: Resourcesfor normalized semantic similarity computation usinglexical networks.
Proc.
of Eighth International Con-ference on Language Resources and Evaluation (to ap-pear).E.
Iosif and A. Potamianos.
2012c.
Similarity com-putation using semantic networks created from web-harvested data.
Natural Language Engineering (sub-mitted to).N.
Madnani and B. J. Dorr.
2010.
Generating phrasal andsentential paraphrases: A survey of data-driven meth-ods.
Computational Linguistics, 36(3):341387.P.
Malakasiotis and I. Androutsopoulos.
2007.
Learn-ing textual entailment using svms and string similar-ity measures.
In Proceedings of of the ACL-PASCALWorkshop on Textual Entailment and Paraphrasing,pages 42?47.P.
Malakasiotis.
2009.
Paraphrase recognition using ma-chine learning to combine similarity measures.
In Pro-ceedings of the 47th Annual Meeting of ACL and the4th Int.
Joint Conference on Natural Language Pro-cessing of AFNLP, pages 42?47.N.
Malandrakis, A. Potamianos, E. Iosif, andS.
Narayanan.
2011.
Kernel models for affec-tive lexicon creation.
In Proc.
Interspeech, pages2977?2980.H.
Meng and K.-C. Siu.
2002.
Semi-automatic acquisi-tion of semantic structures for understanding domain-specific natural language queries.
IEEE Transactionson Knowledge and Data Engineering, 14(1):172?181.G.
Miller and W. Charles.
1998.
Contextual correlatesof semantic similarity.
Language and Cognitive Pro-cesses, 6(1):1?28.G.
Miller.
1990.
Wordnet: An on-line lexical database.International Journal of Lexicography, 3(4):235?312.S.
Mirkin, L. Specia, N. Cancedda, I. Dagan, M. Dymet-man, and S. Idan.
2009.
Source-language entailmentmodeling for translating unknown terms.
In Proceed-ings of the 47th Annual Meeting of ACL and the 4th Int.Joint Conference on Natural Language Processing ofAFNLP, pages 791?799.K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.
2002.Bleu: a method for automatic evaluation of ma-chine translation.
In Proceedings of the 40th AnnualMeeting on Association for Computational Linguis-tics, pages 311?318.569S.
Patwardhan and T. Pedersen.
2006.
Using WordNet-based context vectors to estimate the semantic related-ness of concepts.
In Proc.
of the EACL Workshop onMaking Sense of Sense: Bringing Computational Lin-guistics and Psycholinguistics Together, pages 1?8.T.
Pedersen.
2005.
WordNet::Similarity.http://search.cpan.org/dist/WordNet-Similarity/.D.
Perez and E. Alfonseca.
2005.
Application of thebleu algorithm for recognizing textual entailments.
InProceedings of the PASCAL Challenges Worshop onRecognising Textual Entailment.P.
Resnik.
1995.
Using information content to evalu-ate semantic similarity in a taxanomy.
In Proc.
of In-ternational Joint Conference for Artificial Intelligence,pages 448?453.F.
Rinaldi, J. Dowdall, K. Kaljurand, M. Hess, andD.
Molla.
2003.
Exploiting paraphrases in a questionanswering system.
In Proceedings of the 2nd Interna-tional Workshop on Paraphrasing, pages 25?32.H.
Rubenstein and J.
B. Goodenough.
1965.
Contextualcorrelates of synonymy.
Communications of the ACM,8(10):627?633.I.
Szpektor and I. Dagan.
2008.
Learning entailmentrules for unary templates.
In Proceedings of the 22ndInternational Conference on Computational Linguis-tics, pages 849?856.K.
Toutanova, D. Klein, C. D. Manning, and Y. Singer.2003.
Feature-rich part-of-speech tagging with acyclic dependency network.
In Proceedings of Con-ference of the North American Chapter of the Associ-ation for Computational Linguistics on Human Lan-guage Technology, pages 173?180.P.
D. Turney.
2001.
Mining the web for synonyms: PMI-IR versus LSA on TOEFL.
In Proc.
of the EuropeanConference on Machine Learning, pages 491?502.F.
Zanzotto, M. Pennacchiotti, and A. Moschitti.2009.
A machine-learning approach to textual en-tailment recognition.
Natural Language Engineering,15(4):551582.570
