Automatic Acquisition of Domain Knowledge for InformationExtractionRoman Yangarber, Ralph Grishman Past TapanainenCourant  Inst i tute of Conexor oyMathemat ica l  Sciences Helsinki, F in landNew York University{roman \[ grishman}@cs, nyu.
edu Pasi.
Tapanainen@conexor.
fiSi!ja ItuttunenUniversity of HelsinkiF inlandsihuttun@ling.helsinki.fiAbstractIn developing an Infbrmation Extraction tIE)system tbr a new class of events or relations, oneof the major tasks is identifying the many waysin which these events or relations may be ex-pressed in text.
This has generally involved themanual analysis and, in some cases, the anno-tation of large quantities of text involving theseevents.
This paper presents an alternative ap-proach, based on an automatic discovery pro-cedure, ExDIsCO, which identifies a set; of rele-wmt documents and a set of event patterns fromun-annotated text, starting from a small set of"seed patterns."
We evaluate ExDIScO by com-paring the pertbrmance of discovered patternsagainst that of manually constructed systemson actual extraction tasks.0 Introduct ionIntbrmation Extraction is the selective xtrac-tion of specified types of intbrmation from nat-ural language text.
The intbrmation to beextracted may consist of particular semanticclasses of objects (entities), relationships amongthese entities, and events in which these entitiesparticipate.
The extraction system places thisintbrmation into a data base tbr retrieval andsubsequent processing.In this paper we shall be concerned primar-ily with the extraction of intbrmation aboutevents.
In the terminology which has evolvedti'om the Message Understanding Conferences(muc, 1995; muc, 1993), we shall use the termsubject domain to refer to a broad class of texts,such as business news, and tile term scenario torefer to tile specification of tile particular eventsto be extracted.
For example, the "Manage-ment Succession" scenario for MUC-6, which weshall refer to throughout this paper, involves in-formation about corporate executives tartingand leaving positions.The fundamental problem we face in port-ing an extraction system to a new scenario isto identify the many ways in which intbrmationabout a type of event may be expressed in thetext;.
Typically, there will be a few commontbrms of expression which will quickly come tonfind when a system is being developed.
How-ever, the beauty of natural language (and thechallenge tbr computational linguists) is thatthere are many variants which an imaginativewriter cast use, and which the system needs tocapture.
Finding these variants may involvestudying very large amounts of text; in the sub-ject domain.
This has been a major impedimentto the portability and performance of event ex-traction systems.We present; in this paper a new approachto finding these variants automatically fl'om alarge corpus, without the need to read or amLo-tate the corpus.
This approach as been evalu-ated on actual event extraction scenarios.In the next section we outline the strncture ofour extraction system, and describe the discov-ery task in the context of this system.
Sections2 and 3 describe our algorithm for pattern dis-covery; section 4 describes our experimental re-sults.
This is tbllowed by comparison with priorwork and discussion in section 5.1 The Extract ion SystemIn the simplest terms, an extraction systemidentifies patterns within the text, and thenmat)s some constituents of these patterns intodata base entries.
(This very simple descrip-lion ignores the problems of anaphora nd in-tersentential inference, which must be addressedby any general event extraction system.)
AI-though these l)atterns could in principle bestated in terms of individual words, it is much940easier to state them in terms of larger SylltaC-tic constituents, uch as noun phrases and verbgroups.
Consequently, extraction ormally con-sists of an analysis of the l;e.xt in terms of generallinguistic structures and dolnain-specifio con-structs, tbllowed by a search for the scenario-specific patterns.It is possible to build these constituent struc-tures through a flfll syntactic analysis of thetext, and the discovery procedure we describebelow woul(1 be applicable to such an architec-ture.
Howe, ver, for re&sellS of slme,(t , coverage,and system rolmstness, the more (:ommon ap-t)roa(:h at present is to peribrni a t)artial syn-tactic analysis using a cascade of finite-statetransducers.
This is the at)t)roa(:h used by oure.xtraction system (Grishman, 1995; Yangarberand Grishman, 1998).At; the heart of our syslx'an is a regular ex-pression pattern matcher which is Cal)al)le ofmatching a set of regular exl)ressions againsta partially-analyzed text and producing addi-tional annotations on the text.
This core drawson a set of knowledge bases of w~rying degreesof domain- and task-specificity.
The lexicon in-cludes both a general English dictionary anddefinitions of domain and scenario terms.
Theconcept base arranges the domain terms intoa semantic hierarchy.
The predicate base.
de-s('ribes the, logical structure of I;he events to beextracl;od.
'Fire pattern \])ase consists of sets ofpatterns (with associated actions), whi(;h maker(;ferollCO to information Kern the other knowl-e(lge bases.
Some t)attorn sots, su(:h as those forn(mn and verb groups, are broadly apl)licable ,wlfile other sets are spe(:ifio to the scenario.V~Ze, have previously (Yangarl)er and Grish-man, 1.997) (lescrit)ed a user interface whichsupt)orts the rapid cust;omization of the extrac-tion system to a new scenario.
This interfaceallows the user to provide examples of role-wmt events, which are automatically convertedinto the appropriate patterns and generalized tocover syntactic variants (passive, relative clause,etc.).
Through this internee, the user can alsogeneralize l;he pattern semanti('ally (to (:over abroader class of words) and modify the concet)tbase and lexicon as needed.
Given an appro-priate set; of examples, thereibre, it; has becomepossible to adapt the extraction system quiteral)idly.However, the burden is still on the user tofind the appropriate set of examples, which mayrequire a painstaldng and expensive search of alarge corpus.
Reducing this cost is essential forenhanced system portability; this is the problemaddressed by the current research.Ilow can we automatically discover a suitableset; of candidate patterns or examples (patternswhich at least have a high likelihood of beingrelevant to the scenario)?
The basic idea is tolook for linguistic patterns which apt)ear withrelatively high frequency in relevant documents.While there has been prior research oll idea|i-lying the primary lexical t)atterns of a sublan-guage or cortms (Orishman et al 1986; Riloff,1996), the task here is more complex, since weare tyt)ically not provided in advance with asub-corpus of relevmlt passages; these passagesmust themselves be tbund as part of t;t1(; discov-ery i)rocedure.
The difficulty is that one of thel)est imlic~tions of the relevance of the passagesis t)recisely the t)resence of these constructs.
Bo-(:ause of this (:ircularity, we l)ropose to a(:quire.the constructs and t)assagos in tandem.2 ExDISCO: the  D iscovery  P rocedureWe tirst outline ExDIsco ,  our procedure fordiscovery of oxl,raction patterns; details of someof the stops arc l)rcse, nted in the section whichfollows, and an earlier t)~q)er on our at)l)roach(Yang~u:bcr ot al., 2000).
ExDIscO is mi ml-supervised 1)rocedure: the training (:ortms doesnot need to t)e amlotated with the specific eventintbrmatkm to be.
e.xtracted, or oven with infor-mation as to whi(;h documents in the ('orpus arerelevant o the scenario.
'i7tlo only intbrmationthe user must provide, as described below, is asmall set of seed patterns regarding the s(:enario.Starting with this seed, the system automati-(:ally pertbnns a repeated, automatic expansionof the pattern set.
This is analogous to the pro-cess of automatic t;enn expansion used in s()meinformation retrieval systems, where, the terlnsDora the most relewmt doculncnts are addedto the user query and then a new retriewfl isimrformed.
However, by expanding in terms of1)atl;erns rather than individual terms, a moreprecise expansion is possit)le.
This process pro-coeds as tbllows:0.
We stm:t with a large, corlms of documentsin the domain (which have not been anne-941tared or classified in any way) and an initial"seed" of scenario patterns selected by theuser - -  a small set of patterns whose pres-ence reliably indicates thai; the documentis relevant o the scenario..
The pattern set is used to divide the cor-tins U into a set of relewmt documents, R(which contain at; least one instance of oneof the patterns), and a set of non-relevantdocuments R = U - R.2.
Search tbr new candidate patterns:?
automatically convert each documentin the eorIms into a set of candidatepatterns, one for each clause?
rank patterns by the degree to whichtheir distribution is correlated withdocmnent relevance (i.e., appears withhigher frequency in relevant docu-ments than in non-relewmt ones).3.
Add the highest ranking pattern to the pat-tern set.
(Optionally, at this point, we maypresent he pattern to the user for review.)4.
Use the new pattern set; to induce a newsplit of the corpus into relevant and non-relevant documents.
More precisely, docu-ments will now be given a relevance confi-dence measure; documents containing oneof the initial seed patterns will be givena score of 1, while documents which arcadded to the relevant cortms through newlydiscovered patterns will be given a lowerscore.
I/,epeat the procedure (from step 1)until some iteration limit is reached, or nomore patterns can be added.3 Methodo logy3.1 Pre-processing: Syntact ic AnalysisBefore at)plying ExDIsco ,  we pre-proeessedthe cortms using a general-purpose d pendencyparser of English.
The parser is based onthe FDG tbrmalism (Tapanainen and Jgrvi-hen, 1997) and developed by the Research Unitfor Multilingual Language Technology at theUniversity of Helsinki, and Conexor Oy.
Theparser is used ibr reducing each clause or nounphrase to a tuple, consisting of the central ar-guments, ms described in detail in (Yangarberet al, 2000).
We used a corlms of 9,224 articlesfrom the Wall Street; Journal.
The parsed arti-cles yielded a total of 440,000 clausal tuples, ofwhich 215,000 were distinct.3.2 Normal izat ionWe applied a name recognition module prior toparsing, and replaced each name with a tokendescribing its (:lass, e.g.
C-Person, C-Company,etc.
We collapsed together all numeric expres-sions, currency wflues, dates, etc., using a singletoken to designate ach of these classes.
Lastly,the parser performed syntactic normalization totranstbrm such variants ms the various passiveand relative clauses into a common tbrm.3.3 General izat ion and Concept ClassesBecause tuples may not repeat with sufficientfrequency to obtain reliable statistics, each tu-ple is reduced to a set of pints: e.g., a verb-object pair, a subject-object pair, etc.
Each pairis used as a generalized pattern during the can-didate selection stage.
Once we have identitiedpairs which are relevant o the scenario, we usethem to gather the set; of words for the miss-ing role(s) (tbr example, a class of verbs whichoccur with a relevant subject-ot@ct pair: "com-pany {hire/fire/expel...} person").3.4 Pat tern  DiscoveryWe (-onducte(1 exi)eriments in several scenarioswithin news domains such as changes in cor-porate ownership, and natural disasters.
Iterewe present results on the "Man~geme.nt Suc-cession" and "Mergers/Acquisitions" cenarios.ExDIsco  was seeded with lninimal pattern sets,namely:Subject Verb Direct ObjectC-Company C-At)point C-PersonC-Person C-Resignibr the Mmmgement task, andSubject Verb Direct Object* C-Buy C-Conlt)anyC-Company merge *for Acquisitions.
Here C-Company and C-Person denote semantic classes containingnamed entities of the corresponding types.
C-Appoint denotes the list of verbs { appoint, elect,promote, name, nominate}, C-Resign = { re-sign, depart, quit }, and C-Buy = { buy , pur-chase }.942\ ])uring ~ single iter~tion, we conqmt(; thescore, See're(p), for each cm~(lidate 1)attern p,using (;he fornmla~:S, :o ' , ' ,@)  = IH n l~lIHI - 1,,~ IHn  ~.1 (:t)where 12.
(Icnotes (;h(', l'clewmt subsc(; of docu-ments, mid I t=  It(p) the, ( locmnents imttchingp, as above; the Iirst (;erm a(:(:ounts for the con-(lition~fl t)robabil ity of relev;m('e oil p~ and |;11(;second tbr its support .
We further impose twosupport criteria: we distrust such frequent pat-(;,~.,-.~ w\]le,:e I1~ n UI > ,~IUI, ,~ uninforn,,~tive,mid rare patte.rns \['or which I1\] r-I \]~.1 < fl asnoise.
2 At the end ot' (.aeh il;eratiol~, the sysl;emselects the pal;tern with the highest Sco'/'d(p)~and adds it (;o (;lie seed scl;.
The (to(:un~enl;swhich t;he winning t)~(;t;ern hits are added (;ot;111(; relevant set.
The  t)al;l;(;rn s(;areh is thenr(;sl;m:l;(;d.3.5 Document  Re- rank ingTh(: above is a simt)lifi(';~l;ion of (;he a,(:tual pro-cedlll'(}~ in severa\] r(',st)e('(;s.Only generalized t)ntl;erns are (:onsidered fi)r(:audi(t~my, with one or mot(', slol;s fill(:(1 wi(;hwihl-cm'ds.
In comput ing the score of th(', ge, n-(;raliz(:d \]);tttern, w(: do not take into ('onsi(h:r-;i,1;i()11 all possible va,hw, s of the, wil(1-('m:d role.\?e instea.d (:()llS(;raJll (;he wild-(:ar(l to thos(~ wd-u(:s wlli(:h l;ht',llls(;lv(;s ill (;llrH \]l;tV(: high scores.Th(:se v~du(:s l;lw, n |)e(:on~e lllClll\])(;l'S of }/.
II(:W(:lass, whi(:h is l)rOdu(:ed in (;:tlldClll with thewimfing 1)att(:rn.\])o('umel~tS reh:wm('e is s(-ored (m ~ s(;ah: l)e-(;ween 0 and 1.
Tlm seed t)atterns a.re a.
(:cet)ted~,s trut\]~; the do('mlw, nts (;hey mat(:\]1 hnve rcle-vmme 1.
On i(;er;~tion i + 1, e~mh t)a(;tern p isassigned a precision measure, t)ase(l on the rel-(':Vall(;e of |;11(; (locllnlelfl;s i|; 111a, l;(;ll(',,q:~ ".d~(d) (~)f f , , :d  +~ (v)  - -  IH(v) l ,~.
(,,)where l~,eli(d) is the re, levmlce of' 1;11(: doeunmn(;fi'om t;t1(', previous iteration, ~md l I(p) is the setof documents where p matched, in general, if Kis a classifier (:onsisting of ~ set of l)al;terns, w(',define H (K)  as the st:l; of documents  where all~similar to that used in (liiloff, 1996)~W(: used ,:-- 0.1 and fl = 2.of t)~d;terns p C K m~l;(:h, mid the "cunmlative"precision of K as1 ~ 1~4~(a,) (3) P~.~d +~(1() = IH U()I <.
(K)Once the wimfing pa,l;l;ern is accepted, the rel-ewmee of the documents is re-adjusted.
For(;~mh document  d which is matched by somesubset of l;he currently accet)t('d pntterns, wecan view thai; sul)s(',t; of  l)~tterns as ~ classitierKd = {pj}.
These  patterns (tel;ermilm the newreh;wmce score of the document  asJ~, "~l,~ " ( ,0  : 111~x (:tc,,.1,*(,O,v,.,;, .~" (K , ) )  (~:)This ensures tha.
(; l;he rclewmce score growsmonotonical ly, and only when there is sufliei(mtpositive evidence, as (;he i)ntterns in etl'e(:I; vote"conjmmtively" on the (loculncnl;s.We also tr ied an alternative, ::disjun(:tive"voting scheme, with weights wlfich accounts tbrvm:intion in support of the p~ttterns,J,.,.1, (d) .
.
.
.
~ "~ I I  (1 - ~',.~,.c~(p))"",' (5)~c K(d)where t;11(', weights ,wp arc (tetint;d using the tel-ewm(:(: of the (loeuments, a,s the total  SUl)l)or(;which the pa, I;I;ern p receives:% = log ~ l;.d,(d)dE 11 (p)and ;,7 is (;11(' largest weight.
The  r(',cursive for-nmb~s ('apl;m:e (;he mul;u~fl dependency of t)~t-terns ~md documents;  this re-computat ion ~mdgrowing of precision and relevmlce rmlks is thecore of the t)rocedure.
:~4 Resu l ts4 .1  Event  Ext ract ion'l'he, most nal;m'a.l measm'e of efl'ecl;iveness of ourdiscovery procedure is the performmme of ml ex-tract ion systmn using the, discovered t)~tterns.However, il; is not 1)ossil)le to apply this reel;-rio direei;ly because the discovered t)al;terns lacksome of the information required tbr entries ill:{\V('.
did not el)serve a significam; difl'erencc in 1)crfi)r-lIiHl\[CO, bet, ween the two tormulas 4 alt(t 5 in o111" experi-in(mrs; the results whit:h tbllow use 5.943the pattern base: information about the eventtype (predicate) associated with the pattern,and the mapping from pattern elements to pred-icate arguments.
We have evaluated ExDIscoby manually incorporating the discovered pat-terns into the Proteus knowledge bases and run-ning a full MUC-style evaluation.We started with our extraction system, Pro-tens, which was used in MUC-6 in 1995, andhas undergone continual improvements sincethe MUC evaluation.
We removed all thescenario-specific clause and nominalization pat-terns.
4 We then reviewed all the patterns whichwere generated by the ExDIsco,  deleting thosewhich were not relewmt to the task, or whichdid not correspond irectly to a predicate al-ready implemented tbr this task)  The remain-ing pat;terns were augmented with intbnnationabout the corresponding predicate, and the re-lation between the pattern and the predicateal'guments, a The resulting variants of Proteuswere applied to the formal training corpus andthe (hidden) formal test corpus for MUC-6, andthe output evaluated with the MUC scorer.The results on the training corpus are:Pattern Base Recall PrecisionSeed 38 83Ex I ) Isco 62 80Union 69 __79Manual-MUC ~ 71 L~1.9~Manual-NOW 6(3~ 79 L7!~z\[)_t_jand on the test cortms:4There are also a few noun phrase patterns which cangive rise to scenario events.
For example, "Mr Smith,former president of IBM", may produce an event recordwhere l%ed Smith left IBM.
These patterns were left inProteus for all the runs, and they make some contribu-tion to the relatively high baseline scores obtained usingjust the seed event patterns.~ExD~sco f und patterns which were relevant to thetask lint could not be easily aceomodated in Proteus.For instance "X remained as president" could be rele-vant, particularly in the case of a merger creating anewcorporate ntity, but Proteus was not equipped to trun-dle such iIfformation, and has not yet been extended toincorporate such patterns.6As with all clause-level patterns in Proteus, thesepatterns m-e automatically generalized tohandle syntac-tic wn'iants uch as passive, relative clause, etc.Pattern Base Recall Precision FSeed 27 74 39.58ExDIsco 52 72 60.16Union 57 73 63.56Manual-NOW -- 56 75 6404.The tables show the recall and precision mea-sures for the patterns, with F-measure beingthe harmonic mean of the two.
The Seed pat-tern base consists of just the initial pattern set,given in the table on the previous page.
~ib thiswe added the patterns which the system discov-ered automatically after about 100 iterations,producing the pattern set called ExDIsco.
Forcomparison, M anual-MUC is the pattern baselnanually develot)ed on the MUC-6 trainingcorpus-1)repared over the course of 1 monthof full-time work by at least one computationallinguist (during which the 100-document train-ing corpus was studied in detail).
The last row,Manual-now, shows the current pertbrmance ofthe Proteus system.
The base called Ultiolt con-tains the union of ExDIScO and Manual-No'w.We find these results very encouraging: Pro-teus performs better with the patterns discov-ered by ExI)IscO than it did after one monthof manual tinting and development; in fact, thisperfi)rmance is close to current levels, whichare the result of substantial additional devel-opmeut.
These results umst be interpreted,however, with several caveats.
First, Proteusperformance depends on many fimtors besidesthe event patterns, such as the quality of namere, cognition, syntactic mmlysis, anaphora reso~lution, inferencing, etc.
Several of these wereimproved since the MUC formal evaluation, sosome of the gain over the MUC formal evalua-tion score is attritmtable to these factors.
How~ever, all of the other scores are comparable inthese regards.
Second, as we noted above, thepatterns were reviewed and augmented manu-ally, so the overall procedure is not entirely au-tomatic.
However, the review and augmenta-tion process took little time, as compared tothe manual corpus analysis and development ofthe pattern base.4.2 Text  f i l ter ingWe can obtain a second measure of pertbr-mance by noting that, in addition to growingthe tmttern set, ExDIsco  also grows the rele-9440.90.80.70.60.5_ .
r -~H .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
r .
.
.
.
.
.
.
T ~ ~ : : ~  T;!\ >.
g~t :- ' il%i\[!\]7\[!JLegend:Management/Test ?
.-{~ ......ManagemenVl-raie - :*: --MUC-6 ?0.2 0.4 0.6 0.8RecallFigure l: Management Suc('cssion0.90.80.70.60.5L_~/rLegend:Acquisition0.2 0.4 0.6Recall0.8Figme 2: Mergers/A(:quisitionsvance rankings of documents.
The latter cnn beevahlated irectly, wil;hollt human intervention.We tested Exl)IsC, o ~tgainst wo cor\])orn: th(;100 documents from MUC-6 tbrmal training,a:nd the 100 documents from the MUC-6 for-mal test (both are contained anlong the 10,000ExDIsoO training set) r. Figure 1 shows recallt)\]otted against precision on the two corpora,over 100 iterations, starting with the seed pat-te, nls in section 3.d.
This view on the discoveryprocedure is closely related to the MUC %ext-till;ering" task, in which the systems are jlulgedat the \]evel of doc,wm, e,'nt.s rather thmt event slots.It; is interesting to (:omt)m:e Exl)IsCO's resultswith how other MUC-6 part\]tit)ants performedon the MUC-b '  test cortms , shown anonymously.ExDIscO attains values within the range ofthe MUC participald;S, all of which were eitherheavily-supervised or m~mually coded systems.II; is important to bear in mind that Ex I ) I scohad no benefit of training material, or any in-tbrmation beyond the seed pattern set.Figure 2 shows the 1)ertbrmance, of text fil-tering on the Acquisition task, again, given theseed in section 3.4.
ExDisco  was trained on|;lie same WSJ eorlms, and tested against a setof 200 documents.
We retrieved this set usingkeyword-based IR, search, and judged their rel-evance by halId.rThesc judgements constituted the truth which wasused only for evaluation, not visible to ExDISCO5 Discuss ionThe development of a w~riety of informationextra(:tion systems over the last decade hasdemonstrated their feasibility but also the lim-itations on their portability and t)erformance.Prcl)aring good t)atterns tbr these syste, ms re-quires (:onsiderable skill, and achieving good(:overage requires |;lie analysis of a large amountof text.
These t)rol)lems h~ve t)een impedinmntsto the -wide\].'
use of extraction systenls.These dit\[iculties have stimulate.d resear('h on1)attel .
'n a ( : ( lu i s i t ion .
So lne  o f  th i s  work  has  en l -i)hasized il\]teractive tools to (:onvert examplesto extractioi~ t)atterlls (Yangarber and Grish-man, 1997); nmch ot:' the re, search has focused onmethods for automatically converting a cortmsannotated with extraction examples into pat-terns (Lehnert et al, 1992; Fisher et al, 1995;Miller el; al., 1998).
These techniques may re-duce the level of systeln expertise required todevelop a new extraction N)plieation, but theydo not lessen the lmrden of studying a large cor-lms in order to .find relevant candidates.The prior work most closely related to ourown is that of (R.ilotf, 1996), who also seeks tolmild pattenls automatically without the needto annotate a corpus with the information tobe extracted.
Itowever, her work ditfers t'rom01217 own in several i lnportant respects.
First,her patterns identit~y phrases that fill individualslots in the template, without specifying howthese slots may be combined at a later stageinto complete templates.
In contrast, our pro-cedure discovers complete, multi-slot event pat-945terns.
Second, her procedure relies on a cort)usin which |;tie documents have been classified forrelevance by hand (it was applied to the MUC-3task, tbr which over 1500 classified documentsare available), whereas ExDIsco requires nomanual relevance judgements.
While classify-ing documents tbr relevance is much easier thanannotating docunlents with the information tobe extracted, it; is still a significant ask, andplaces a limit on |:tie size of the training corpusthat can be effectively used.Our research as demonstrated that for thestudied scenarios automatic pattern discoveryCall yield extraction perfi)rmance colnt)arabh~ tothat obtained through extensive corpus anal-ysis.
There are many directions in which thework reported here needs to be extended:?
nsing larger training corpora, in order tofind less frequent exanlplcs, and in that wayhopefully exceeding the i)erfornlancc of ourbest hand-trained system?
cat)luring the word classes which are gen-erated as a by-product of our pattern dis-covery 1)rocedure (in a manner similar to(Riloff and ,Jones, 1999)) and using themto discover less frequent )atterns in subse-quent iterations- evaluating the effectiveness of the discov-cry procedure on other scenarios.
In par-titular, we need to be able to identi\[y top-its which cast be most effbctively charac-terized by clause-level patterns (as was thecase tbr the business domain), and topicswhich can be better characterized by othermeans.
We.
wouM also like to understandhow the topic clusters (of documents andpatterns) which are developed by our pro-cedure line up with pre-specified scenarios.ReferencesDavid Fisher, Stephen Soderland, Joseph Mc-Carthy, Fangfang Feng, and Wendy Lelmert.1995.
Description of the UMass system asused fbr MUC-6.
In Prec.
Sixth Message Un-dcrstandin9 Conf.
(MUC-6), Columbia, MD,November.
Morgan Kauflnann.R.alph Grishman.
1995.
The NYU systenl tbrMUC-6, or where's the syntax?
Ill Prec.Sixth Message Understanding Conf.
(MUC-6), pages 167 176, Columl)ia, MD, Novem-ber.
Morgan Kauflnann.W.
Lehnert, C. Cardie, D. Fisher, J. McCarthy,E.
Riloff, and S. Soderland.
1992.
Univer-sity of nlassachusetts: MUC-4 test resultsand analysis.
Ill P,'oe.
Fourth Message Un-der.standing Con.\[., McLean, VA, June.
Mor-gan Kauflnaml.Scott Miller, Michael Crystal, Heidi Fox,Lance II,amshaw, R,ichard Schwartz, RebeccaStone, Rall)h Weischedel, and the Annota-tion Group.
1998.
Algorithms that learn toextract intbrmation; BBN: Description of theSIFT systenl as used for MUC-7.
In PTve.
7thMc.ssagc Understanding Co~:f., FMrfax, VA.1993.
Proceedings of the F'~ifth Message UTz.-derstanding Confer(race (MUC-5), Baltimore,MD, August.
Morgan Kauflnann.1995.
PTveeedings of the Sixth Message U~I,-derstav, ding Conference (MUC-6), Colmnt)ia,MD, November.
Morgan Kauflnaml.Ellen Rilotf and Rosie Jones.
1999.
Learn-ing dictionaries for infbrmation extraction bymulti-level bootstrat)ping.
In Prec.
16th Nat'lCord'erenee on Art'~i\[icial Intelli9enee (AAA I99), Orlando, Florida.Ellen Riloff.
1996.
Automatically generatingextraction patterns from m~tagged text.
InPrec.
I3th Nat'l Co~~:f. on Art~ificial Intel-ligence (AAAI-96).
The AAAI Press/MITPress.l?asi '\])~panainen a d Time .J/h:vinen.
1997.
Anon-t)rojectivc dependency parser.
In P'mc.5th Conf.
on Applied Nat'aral Language P~v-cessiu9, pages 64-71, Washington, D.C. ACL.Roman Yangarber and RalI)h Grishman.
1997.Customization of intbrmation extraction sys-tems.
In Paola Velardi, editor, I~tt'l Work-shop on Lexically Driven I~7:forrnation Extrac-tion, Frascati, Italy.
Universith di Roma.Roman Yangarl)er and Ralph Grishman.
1998.NYU: Description of thc Protens/PET sys-tem as used tbr MUC-7 ST.
In 7th MessageUnderstanding Conference, Columbia, MD.Roman Yangarl)er, Ralph Grishman, PastTapanainen, and Silja Huttunen.
2000.
Un-supervised discovery of scenario-level pat-terns tbr information extraction.
Ill PTve.Co~@ on Applied Nat'aral Lang'aage Pr'ocess-tug (ANLP-NAACL), Seattle, WA.946
