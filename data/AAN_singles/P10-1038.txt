Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 366?374,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsConditional Random Fields for Word HyphenationNikolaos TrogkanisComputer Science and EngineeringUniversity of California, San DiegoLa Jolla, California 92093-0404tronikos@gmail.comCharles ElkanComputer Science and EngineeringUniversity of California, San DiegoLa Jolla, California 92093-0404elkan@cs.ucsd.eduAbstractFinding allowable places in words to inserthyphens is an important practical prob-lem.
The algorithm that is used most of-ten nowadays has remained essentially un-changed for 25 years.
This method is theTEX hyphenation algorithm of Knuth andLiang.
We present here a hyphenationmethod that is clearly more accurate.
Thenew method is an application of condi-tional random fields.
We create new train-ing sets for English and Dutch from theCELEX European lexical resource, andachieve error rates for English of less than0.1% for correctly allowed hyphens, andless than 0.01% for Dutch.
Experimentsshow that both the Knuth/Liang methodand a leading current commercial alterna-tive have error rates several times higherfor both languages.1 IntroductionThe task that we investigate is learning to splitwords into parts that are conventionally agreed tobe individual written units.
In many languages, itis acceptable to separate these units with hyphens,but it is not acceptable to split words arbitrarily.Another way of stating the task is that we want tolearn to predict for each letter in a word whether ornot it is permissible for the letter to be followed bya hyphen.
This means that we tag each letter witheither 1, for hyphen allowed following this letter,or 0, for hyphen not allowed after this letter.The hyphenation task is also called ortho-graphic syllabification (Bartlett et al, 2008).
It isan important issue in real-world text processing,as described further in Section 2 below.
It is alsouseful as a preprocessing step to improve letter-to-phoneme conversion, and more generally for text-to-speech conversion.
In the well-known NETtalksystem, for example, syllable boundaries are aninput to the neural network in addition to letteridentities (Sejnowski and Rosenberg, 1988).
Ofcourse, orthographic syllabification is not a fun-damental scientific problem in linguistics.
Nev-ertheless, it is a difficult engineering task that isworth studying for both practical and intellectualreasons.The goal in performing hyphenation is to pre-dict a sequence of 0/1 values as a function of a se-quence of input characters.
This sequential predic-tion task is significantly different from a standard(non-sequential) supervised learning task.
Thereare at least three important differences that makesequence prediction difficult.
First, the set of allpossible sequences of labels is an exponentiallylarge set of possible outputs.
Second, different in-puts have different lengths, so it is not obvioushow to represent every input by a vector of thesame fixed length, as is almost universal in su-pervised learning.
Third and most important, toomuch information is lost if we learn a traditionalclassifier that makes a prediction for each letterseparately.
Even if the traditional classifier is afunction of the whole input sequence, this remainstrue.
In order to achieve high accuracy, correla-tions between neighboring predicted labels mustbe taken into account.Learning to predict a sequence of output labels,given a sequence of input data items, is an instanceof a structured learning problem.
In general, struc-tured learning means learning to predict outputsthat have internal structure.
This structure canbe modeled; to achieve high predictive accuracy,when there are dependencies between parts of anoutput, it must be modeled.
Research on struc-tured learning has been highly successful, withsequence classification as its most important andsuccessful subfield, and with conditional randomfields (CRFs) as the most influential approach tolearning sequence classifiers.
In the present paper,366we show that CRFs can achieve extremely goodperformance on the hyphenation task.2 History of automated hyphenationThe earliest software for automatic hyphenationwas implemented for RCA 301 computers, andused by the Palm Beach Post-Tribune and Los An-geles Times newspapers in 1962.
These were twodifferent systems.
The Florida system had a dic-tionary of 30,000 words; words not in the dictio-nary were hyphenated after the third, fifth, or sev-enth letter, because the authors observed that thiswas correct for many words.
The California sys-tem (Friedlander, 1968) used a collection of rulesbased on the rules stated in a version of Webster?sdictionary.
The earliest hyphenation software fora language other than English may have been arule-based program for Finnish first used in 1964(Jarvi, 2009).The first formal description of an algorithm forhyphenation was in a patent application submit-ted in 1964 (Damerau, 1964).
Other early pub-lications include (Ocker, 1971; Huyser, 1976).The hyphenation algorithm that is by far the mostwidely used now is due to Liang (Liang, 1983).Although this method is well-known now as theone used in TEX and its derivatives, the first ver-sion of TEX used a different, simpler method.Liang?s method was used also in troff andgroff, which were the main original competitorsof TEX, and is part of many contemporary softwareproducts, supposedly including Microsoft Word.Any major improvement over Liang?s method istherefore of considerable practical and commer-cial importance.Over the years, various machine learning meth-ods have been applied to the hyphenation task.However, none have achieved high accuracy.
Onepaper that presents three different learning meth-ods is (van den Bosch et al, 1995).
The lowestper-letter test error rate reported is about 2%.
Neu-ral networks have been used, but also without greatsuccess.
For example, the authors of (Kristensenand Langmyhr, 2001) found that the TEX methodis a better choice for hyphenating Norwegian.The highest accuracy achieved until now for thehyphenation task is by (Bartlett et al, 2008), whouse a large-margin structured learning approach.Our work is similar, but was done fully indepen-dently.
The accuracy we achieve is slightly higher:word-level accuracy of 96.33% compared to their95.65% for English.
Moreover, (Bartlett et al,2008) do not address the issue that false positivehyphens are worse mistakes than false negative hy-phens, which we address below.
Also, they reportthat training on 14,000 examples requires about anhour, compared to 6.2 minutes for our method on65,828 words.
Perhaps more important for large-scale publishing applications, our system is aboutsix times faster at syllabifying new text.
The speedcomparison is fair because the computer we use isslightly slower than the one they used.Methods inspired by nonstatistical natural lan-guage processing research have also been pro-posed for the hyphenation task, in particular(Bouma, 2003; Tsalidis et al, 2004; Woestenburg,2006; Haralambous, 2006).
However, the methodsfor Dutch presented in (Bouma, 2003) were foundto have worse performance than TEX.
Moreover,our experimental results below show that the com-mercial software of (Woestenburg, 2006) allowshyphens incorrectly almost three times more oftenthan TEX.In general, a dictionary based approach has zeroerrors for words in the dictionary, but fails to workfor words not included in it.
A rule-based ap-proach requires an expert to define manually therules and exceptions for each language, which islaborious work.
Furthermore, for languages suchas English where hyphenation does not system-atically follow general rules, such an approachdoes not have good results.
A pattern-learning ap-proach, like that of TEX, infers patterns from atraining list of hyphenated words, and then usesthese patterns to hyphenate text.
Although usefulpatterns are learned automatically, both the TEXlearning algorithm and the learned patterns mustbe hand-tuned to perform well (Liang, 1983).Liang?s method is implemented in a programnamed PATGEN, which takes as input a trainingset of hyphenated words, and outputs a collectionof interacting hyphenation patterns.
The standardpattern collections are named hyphen.tex forAmerican English, ukhyphen.tex for BritishEnglish, and nehyph96.tex for Dutch.
Theprecise details of how different versions of TEXand LATEX use these pattern collections to do hy-phenation in practice are unclear.
At a minimum,current variants of TEX improve hyphenation ac-curacy by disallowing hyphens in the first and lasttwo or three letters of every word, regardless ofwhat the PATGEN patterns recommend.367Despite the success of Liang?s method, incor-rect hyphenations remain an issue with TEX andits current variants and competitors.
For instance,incorrect hyphenations are common in the WallStreet Journal, which has the highest circulationof any newspaper in the U.S. An example is thehyphenation of the word ?sudden?
in this extract:It is the case that most hyphenation mistakes in theWall Street Journal and other media are for propernouns such as ?Netflix?
that do not appear in stan-dard dictionaries, or in compound words such as?sudden-acceleration?
above.3 Conditional random fieldsA linear-chain conditional random field (Laffertyet al, 2001) is a way to use a log-linear modelfor the sequence prediction task.
We use the barnotation for sequences, so x?
means a sequence ofvariable length.
Specifically, let x?
be a sequenceof n letters and let y?
be a corresponding sequenceof n tags.
Define the log-linear modelp(y?|x?
;w) =1Z(x?, w)exp?jwjFj(x?, y?
).The index j ranges over a large set of feature-functions.
Each such function Fj is a sum alongthe output sequence for i = 1 to i = n:Fj(x?, y?)
=n?i=1fj(yi?1, yi, x?, i)where each function fj is a 0/1 indicator functionthat picks out specific values for neighboring tagsyi?1 and yi and a particular substring of x?.
Thedenominator Z(x?, w) is a normalizing constant:Z(x?, w) =?y?exp?jwjFj(x?, y?
)where the outer sum is over all possible labelingsy?
of the input sequence x?.
Training a CRF meansfinding a weight vector w that gives the best pos-sible predictionsy??
= arg maxy?p(y?|x?
;w)for each training example x?.The software we use as an implementation ofconditional random fields is named CRF++ (Kudo,2007).
This implementation offers fast trainingsince it uses L-BFGS (Nocedal and Wright, 1999),a state-of-the-art quasi-Newton method for largeoptimization problems.
We adopt the default pa-rameter settings of CRF++, so no development setor tuning set is needed in our work.We define indicator functions fj that depend onsubstrings of the input word, and on whether ornot a hyphen is legal after the current and/or theprevious letter.
The substrings are of length 2 to5, covering up to 4 letters to the left and right ofthe current letter.
From all possible indicator func-tions we use only those that involve a substringthat occurs at least once in the training data.As an example, consider the wordhy-phen-ate.
For this word x?
= hyphenateand y?
= 010001000.
Suppose i = 3 so p is thecurrent letter.
Then exactly two functions fj thatdepend on substrings of length 2 have value 1:I(yi?1 = 1 and yi = 0 and x2x3 = yp) = 1,I(yi?1 = 1 and yi = 0 and x3x4 = ph) = 1.All other similar functions have value 0:I(yi?1 = 1 and yi = 1 and x2x3 = yp) = 0,I(yi?1 = 1 and yi = 0 and x2x3 = yq) = 0,and so on.
There are similar indicator functions forsubstrings up to length 5.
In total, 2,916,942 dif-ferent indicator functions involve a substring thatappears at least once in the English dataset.One finding of our work is that it is prefer-able to use a large number of low-level features,that is patterns of specific letters, rather than asmaller number of higher-level features such asconsonant-vowel patterns.
This finding is consis-tent with an emerging general lesson about manynatural language processing tasks: the best perfor-mance is achieved with models that are discrimi-native, that are trained on as large a dataset as pos-sible, and that have a very large number of param-eters but are regularized (Halevy et al, 2009).When evaluating the performance of a hyphen-ation algorithm, one should not just count howmany words are hyphenated in exactly the sameway as in a reference dictionary.
One should alsomeasure separately how many legal hyphens areactually predicted, versus how many predicted hy-phens are in fact not legal.
Errors of the sec-ond type are false positives.
For any hyphenation368method, a false positive hyphen is a more seriousmistake than a false negative hyphen, i.e.
a hyphenallowed by the lexicon that the method fails toidentify.
The standard Viterbi algorithm for mak-ing predictions from a trained CRF is not tuned tominimize false positives.
To address this difficulty,we use the forward-backward algorithm (Sha andPereira, 2003; Culotta and McCallum, 2004) to es-timate separately for each position the probabilityof a hyphen at that position.
Then, we only allow ahyphen if this probability is over a high thresholdsuch as 0.9.Each hyphenation corresponds to one paththrough a graph that defines all 2k?1 hyphenationsthat are possible for a word of length k. The over-all probability of a hyphen at any given locationis the sum of the weights of all paths that do havea hyphen at this position, divided by the sum ofthe weights of all paths.
The forward-backwardalgorithm uses the sum operator to compute theweight of a set of paths, instead of the max op-erator to compute the weight of a single highest-weight path.
In order to compute the weight of allpaths that contain a hyphen at a specific location,weight 0 is assigned to all paths that do not have ahyphen at this location.4 Dataset creationWe start with the lexicon for English publishedby the Dutch Centre for Lexical Information athttp://www.mpi.nl/world/celex.
Wedownload all English word forms with legal hy-phenation points indicated by hyphens.
Theseinclude plurals of nouns, conjugated forms ofverbs, and compound words such as ?off-line?.We separate the components of compound wordsand phrases, leading to 204,466 words, of which68,744 are unique.
In order to eliminate abbrevia-tions and proper names which may not be English,we remove all words that are not fully lower-case.In particular, we exclude words that contain capi-tal letters, apostrophes, and/or periods.
This leaves66,001 words.Among these words, 86 have two different hy-phenations, and one has three hyphenations.
Formost of the 86 words with alternative hyphen-ations, these alternatives exist because differentmeanings of the words have different pronuncia-tions, and the different pronunciations have differ-ent boundaries between syllables.
This fact im-plies that no algorithm that operates on words inisolation can be a complete solution for the hy-phenation task.1We exclude the few words that have two or moredifferent hyphenations from the dataset.
Finally,we obtain 65,828 spellings.
These have 550,290letters and 111,228 hyphens, so the average is 8.36letters and 1.69 hyphens per word.
Informal in-spection suggests that the 65,828 spellings containno mistakes.
However, about 1000 words followBritish as opposed to American spelling.The Dutch dataset of 293,681 words is createdfollowing the same procedure as for the Englishdataset, except that all entries from CELEX thatare compound words containing dashes are dis-carded instead of being split into parts, since manyof these are not in fact Dutch words.25 Experimental designWe use ten-fold cross validation for the experi-ments.
In order to measure accuracy, we com-pute the confusion matrix for each method, andfrom this we compute error rates.
We report bothword-level and letter-level error rates.
The word-level error rate is the fraction of words on whicha method makes at least one mistake.
The letter-level error rate is the fraction of letters for whichthe method predicts incorrectly whether or not ahyphen is legal after this letter.
Table 1 explainsthe terminology that we use in presenting our re-sults.
Precision, recall, and F1 can be computedeasily from the reported confusion matrices.As an implementation of Liang?s method weuse TEX Hyphenator in Java software availableat http://texhyphj.sourceforge.net.We evaluate this algorithm on our entire Englishand Dutch datasets using the appropriate languagepattern files, and not allowing a hyphen to beplaced between the first lefthyphenmin andlast righthyphenmin letters of each word.
For1The single word with more than two alternativehyphenations is ?invalid?
whose three hyphenations arein-va-lid in-val-id and in-valid.
Interest-ingly, the Merriam?Webster online dictionary also givesthree hyphenations for this word, but not the same ones:in-va-lid in-val-id invalid.
The AmericanHeritage dictionary agrees with Merriam-Webster.
The dis-agreement illustrates that there is a certain irreducible ambi-guity or subjectivity concerning the correctness of hyphen-ations.2Our English and Dutch datasets are available for otherresearchers and practitioners to use at http://www.cs.ucsd.edu/users/elkan/hyphenation.
Previouslya similar but smaller CELEX-based English dataset was cre-ated by (van den Bosch et al, 1995), but that dataset is notavailable online currently.369Abbr Name DescriptionTP true positives #hyphens predicted correctlyFP false positives #hyphens predicted incorrectlyTN true negatives #hyphens correctly not predictedFN false negatives #hyphens failed to be predictedowe overall word-level errors #words with at least one FP or FNswe serious word-level errors #words with at least one FPower overall word-level error rate owe / (total #words)swer serious word-level error rate swe / (total #words)oler overall letter-level error rate (FP+FN) / (TP+TN+FP+FN)sler serious letter-level error rate FP / (TP+TN+FP+FN)Table 1: Alternative measures of accuracy.
TP, TN, FP, and FN are computed by summing over the testsets of each fold of cross-validation.English the default values are 2 and 3 respectively.For Dutch the default values are both 2.The hyphenation patterns used by TeXHyphen-ator, which are those currently used by essentiallyall variants of TEX, may not be optimal for ournew English and Dutch datasets.
Therefore, wealso do experiments with the PATGEN tool (Liangand Breitenlohner, 2008).
These are learning ex-periments so we also use ten-fold cross validationin the same way as with CRF++.
Specifically, wecreate a pattern file from 90% of the dataset us-ing PATGEN, and then hyphenate the remaining10% of the dataset using Liang?s algorithm and thelearned pattern file.The PATGEN tool has many user-settable pa-rameters.
As is the case with many machine learn-ing methods, no strong guidance is available forchoosing values for these parameters.
For En-glish we use the parameters reported in (Liang,1983).
For Dutch we use the parameters reportedin (Tutelaers, 1999).
Preliminary informal exper-iments found that these parameters work betterthan alternatives.
We also disallow hyphens in thefirst two letters of every word, and the last threeletters for English, or last two for Dutch.We also evaluate the TALO commercial soft-ware (Woestenburg, 2006).
We know of oneother commercial hyphenation application, whichis named Dashes.3 Unfortunately we do not haveaccess to it for evaluation.
We also cannot do aprecise comparison with the method of (Bartlett etal., 2008).
We do know that their training set wasalso derived from CELEX, and their maximumreported accuracy is slightly lower.
Specifically,for English our word-level accuracy (?ower?)
is96.33% while their best (?WA?)
is 95.65%.3http://www.circlenoetics.com/dashes.aspx6 Experimental resultsIn Table 2 and Table 3 we report the performanceof the different methods on the English and Dutchdatasets respectively.
Figure 1 shows how the er-ror rate is affected by increasing the CRF proba-bility threshold for each language.Figure 1 shows confidence intervals for the er-ror rates.
These are computed as follows.
For asingle Bernoulli trial the mean is p and the vari-ance is p(1 ?
p).
If N such trials are taken, thenthe observed success rate f = S/N is a randomvariable with mean p and variance p(1 ?
p)/N .For large N , the distribution of the random vari-able f approaches the normal distribution.
Hencewe can derive a confidence interval for p using theformulaPr[?z ?f ?
p?p(1?
p)/N?
z] = cwhere for a 95% confidence interval, i.e.
for c =0.95, we set z = 1.96.
All differences betweenrows in Table 2 are significant, with one exception:the serious error rates for PATGEN and TALO arenot statistically significantly different.
A similarconclusion applies to Table 3.For the English language, the CRF using theViterbi path has overall error rate of 0.84%, com-pared to 6.81% for the TEX algorithm using Amer-ican English patterns, which is eight times worse.However, the serious error rate for the CRF is lessgood: 0.41% compared to 0.24%.
This weak-ness is remedied by predicting that a hyphen is al-lowable only if it has high probability.
Figure 1shows that the CRF can use a probability thresh-old up to 0.99, and still have lower overall errorrate than the TEX algorithm.
Fixing the probabil-ity threshold at 0.99, the CRF serious error rateis 0.04% (224 false positives) compared to 0.24%(1343 false positives) for the TEX algorithm.37012345678%olerEnglishPATGENTeXTALOCRF0.90 0.91 0.92 0.93 0.94 0.95 0.96 0.97 0.98 0.99Probability threshold0.00.10.20.30.40.50.60.70.8%sler0.10.20.30.40.50.60.70.80.9 Dutch0.90 0.91 0.92 0.93 0.94 0.95 0.96 0.97 0.98 0.99Probability threshold0.000.050.100.150.200.250.300.35Figure 1: Total letter-level error rate and serious letter-level error rate for different values of threshold forthe CRF.
The left subfigures are for the English dataset, while the right ones are for the Dutch dataset.The TALO and PATGEN lines are almost identical in the bottom left subfigure.Method TP FP TN FN owe swe % ower % swer % oler % slerPlace no hyphen 0 0 439062 111228 57541 0 87.41 0.00 20.21 0.00TEX (hyphen.tex) 75093 1343 437719 36135 30337 1311 46.09 1.99 6.81 0.24TEX (ukhyphen.tex) 70307 13872 425190 40921 31337 11794 47.60 17.92 9.96 2.52TALO 104266 3970 435092 6962 7213 3766 10.96 5.72 1.99 0.72PATGEN 74397 3934 435128 36831 32348 3803 49.14 5.78 7.41 0.71CRF 108859 2253 436809 2369 2413 2080 3.67 3.16 0.84 0.41CRF (threshold = 0.99) 83021 224 438838 28207 22992 221 34.93 0.34 5.17 0.04Table 2: Performance on the English dataset.Method TP FP TN FN owe swe % ower % swer % oler % slerPlace no hyphen 0 0 2438913 742965 287484 0 97.89 0.00 23.35 0.00TEX (nehyph96.tex) 722789 5580 2433333 20176 20730 5476 7.06 1.86 0.81 0.18TALO 727145 3638 2435275 15820 16346 3596 5.57 1.22 0.61 0.11PATGEN 730720 9660 2429253 12245 20318 9609 6.92 3.27 0.69 0.30CRF 741796 1230 2437683 1169 1443 1207 0.49 0.41 0.08 0.04CRF (threshold = 0.99) 719710 149 2438764 23255 22067 146 7.51 0.05 0.74 0.00Table 3: Performance on the Dutch dataset.Method TP FP TN FN owe swe % ower % swer % oler % slerPATGEN 70357 6763 432299 40871 35013 6389 53.19 9.71 8.66 1.23CRF 104487 6518 432544 6741 6527 5842 9.92 8.87 2.41 1.18CRF (threshold = 0.99) 75651 654 438408 35577 27620 625 41.96 0.95 6.58 0.12Table 4: Performance on the English dataset (10-fold cross validation dividing by stem).Method TP FP TN FN owe swe % ower % swer % oler % slerPATGEN 727306 13204 2425709 15659 25363 13030 8.64 4.44 0.91 0.41CRF 740331 2670 2436243 2634 3066 2630 1.04 0.90 0.17 0.08CRF (threshold = 0.99) 716596 383 2438530 26369 24934 373 8.49 0.13 0.84 0.01Table 5: Performance on the Dutch dataset (10-fold cross validation dividing by stem).Method TP FP TN FN owe swe % ower % swer % oler % slerTEX 2711 43 21433 1420 1325 43 33.13 1.08 5.71 0.17PATGEN 2590 113 21363 1541 1466 113 36.65 2.83 6.46 0.44CRF 4129 2 21474 2 2 2 0.05 0.05 0.02 0.01CRF (threshold = 0.9) 4065 0 21476 66 63 0 1.58 0.00 0.26 0.00Table 6: Performance on the 4000 most frequent English words.371For the English language, TALO yields overallerror rate 1.99% with serious error rate 0.72%, sothe standard CRF using the Viterbi path is betteron both measures.
The dominance of the CRFmethod can be increased further by using a prob-ability threshold.
Figure 1 shows that the CRFcan use a probability threshold up to 0.94, andstill have lower overall error rate than TALO.
Us-ing this threshold, the CRF serious error rate is0.12% (657 false positives) compared to 0.72%(3970 false positives) for TALO.For the Dutch language, the standard CRF us-ing the Viterbi path has overall error rate 0.08%,compared to 0.81% for the TEX algorithm.
Theserious error rate for the CRF is 0.04% while forTEX it is 0.18%.
Figure 1 shows that any probabil-ity threshold for the CRF of 0.99 or below yieldslower error rates than the TEX algorithm.
Usingthe threshold 0.99, the CRF has serious error rateonly 0.005%.For the Dutch language, the TALO method hasoverall error rate 0.61%.
The serious error ratefor TALO is 0.11%.
The CRF dominance canagain be increased via a high probability thresh-old.
Figure 1 shows that this threshold can rangeup to 0.98, and still give lower overall error ratethan TALO.
Using the 0.98 threshold, the CRFhas serious error rate 0.006% (206 false positives);in comparison the serious error rate of TALO is0.11% (3638 false positives).For both languages, PATGEN has higher seriousletter-level and word-level error rates than TEX us-ing the existing pattern files.
This is expected sincethe pattern collections included in TEX distribu-tions have been tuned over the years to minimizeobjectionable errors.
The difference is especiallypronounced for American English, for which thestandard pattern collection has been manually im-proved over more than two decades by many peo-ple (Beeton, 2002).
Initially, Liang optimized thispattern collection extensively by upweighting themost common words and by iteratively addingexception words found by testing the algorithmagainst a large dictionary from an unknown pub-lisher (Liang, 1983).One can tune PATGEN to yield either betteroverall error rate, or better serious error rate, butnot both simultaneously, compared to the TEX al-gorithm using the existing pattern files for bothlanguages.
For the English dataset, if we useLiang?s parameters for PATGEN as reported in(Sojka and Sevecek, 1995), we obtain overall er-ror rate of 6.05% and serious error rate of 0.85%.It is possible that the specific patterns used in TEXimplementations today have been tuned by handto be better than anything the PATGEN software iscapable of.7 Additional experimentsThis section presents empirical results followingtwo experimental designs that are less standard,but that may be more appropriate for the hyphen-ation task.First, the experimental design used above hasan issue shared by many CELEX-based taggingor transduction evaluations: words are randomlydivided into training and test sets without be-ing grouped by stem.
This means that a methodcan get credit for hyphenating ?accents?
correctly,when ?accent?
appears in the training data.
There-fore, we do further experiments where the foldsfor evaluation are divided by stem, and not byword; that is, all versions of a base form of aword appear in the same fold.
Stemming usesthe English and Dutch versions of the Porter stem-mer (Porter, 1980).4 The 65,828 English words inour dictionary produce 27,100 unique stems, whilethe 293,681 Dutch words produce 169,693 uniquestems.
The results of these experiments are shownin Tables 4 and 5.The main evaluation in the previous section isbased on a list of unique words, which means thatin the results each word is equally weighted.
Be-cause cross validation is applied, errors are alwaysmeasured on testing subsets that are disjoint fromthe corresponding training subsets.
Hence, theaccuracy achieved can be interpreted as the per-formance expected when hyphenating unknownwords, i.e.
rare future words.However, in real documents common wordsappear repeatedly.
Therefore, the second less-standard experimental design for which we reportresults restricts attention to the most common En-glish words.
Specifically, we consider the top4000 words that make up about three quarters ofall word appearances in the American NationalCorpus, which consists of 18,300,430 words fromwritten texts of all genres.5 From the 4,471 most4Available at http://snowball.tartarus.org/.A preferable alternative might be to use the information aboutthe lemmas of words available directly in CELEX.5Available at americannationalcorpus.org/SecondRelease/data/ANC-written-count.txt372frequent words in this list, if we omit the wordsnot in our dataset of 89,019 hyphenated Englishwords from CELEX, we get 4,000 words.
Thewords that are omitted are proper names, contrac-tions, incomplete words containing apostrophes,and abbreviations such as DNA.
These 4,000 mostfrequent words make up 74.93% of the whole cor-pus.We evaluate the following methods on the 4000words: Liang?s method using the American pat-terns file hyphen.tex, Liang?s method usingthe patterns derived from PATGEN when trainedon the whole English dataset, our CRF trained onthe whole English dataset, and the same CRF witha probability threshold of 0.9.
Results are shownin Table 6.
In summary, TEX and PATGEN makeserious errors on 43 and 113 of the 4000 words,respectively.
With a threshold of 0.9, the CRF ap-proach makes zero serious errors on these words.8 TimingsTable 7 shows the speed of the alternative meth-ods for the English dataset.
The column ?Fea-tures/Patterns?
in the table reports the number offeature-functions used for the CRF, or the numberof patterns used for the TEX algorithm.
Overall,the CRF approach is about ten times slower thanthe TEX algorithm, but its performance is still ac-ceptable on a standard personal computer.
All ex-periments use a machine having a Pentium 4 CPUat 3.20GHz and 2GB memory.
Moreover, infor-mal experiments show that CRF training would beabout eight times faster if we used CRFSGD ratherthan CRF++ (Bottou, 2008).From a theoretical perspective, both methodshave almost-constant time complexity per word ifthey are implemented using appropriate data struc-tures.
In TEX, hyphenation patterns are stored ina data structure that is a variant of a trie.
TheCRF software uses other data structures and op-timizations that allow a word to be hyphenated intime that is almost independent of the number offeature-functions used.9 ConclusionsFinding allowable places in words to insert hy-phens is a real-world problem that is still notfully solved in practice.
The main contribu-tion of this paper is a hyphenation method thatis clearly more accurate than the currently usedKnuth/Liang method.
The new method is an ap-Features/ Training Testing SpeedMethod Patterns time (s) time (s) (ms/word)CRF 2916942 372.67 25.386 0.386TEX (us) 4447 - 2.749 0.042PATGEN 4488 33.402 2.889 0.044TALO - - 8.400 0.128Table 7: Timings for the English dataset (trainingand testing on the whole dataset that consists of65,828 words).plication of CRFs, which are a major advance ofrecent years in machine learning.
We hope thatthe method proposed here is adopted in practice,since the number of serious errors that it makesis about a sixfold improvement over what is cur-rently in use.
A second contribution of this pa-per is to provide training sets for hyphenation inEnglish and Dutch, so other researchers can, wehope, soon invent even more accurate methods.
Athird contribution of our work is a demonstrationthat current CRF methods can be used straightfor-wardly for an important application and outper-form state-of-the-art commercial and open-sourcesoftware; we hope that this demonstration acceler-ates the widespread use of CRFs.ReferencesSusan Bartlett, Grzegorz Kondrak, and Colin Cherry.2008.
Automatic syllabification with structuredSVMs for letter-to-phoneme conversion.
Proceed-ings of ACL-08: HLT, pages 568?576.Barbara Beeton.
2002.
Hyphenation exception log.TUGboat, 23(3).Le?on Bottou.
2008.
Stochastic gradient CRF softwareCRFSGD.
Available at http://leon.bottou.org/projects/sgd.Gosse Bouma.
2003.
Finite state methods for hyphen-ation.
Natural Language Engineering, 9(1):5?20,March.Aron Culotta and Andrew McCallum.
2004.
Confi-dence Estimation for Information Extraction.
In Su-san Dumais, Daniel Marcu, and Salim Roukos, edi-tors, HLT-NAACL 2004: Short Papers, pages 109?112, Boston, Massachusetts, USA, May.
Associa-tion for Computational Linguistics.Fred J. Damerau.
1964.
Automatic HyphenationScheme.
U.S. patent 3537076 filed June 17, 1964,issued October 1970.Gordon D. Friedlander.
1968.
Automation comes tothe printing and publishing industry.
IEEE Spec-trum, 5:48?62, April.373Alon Halevy, Peter Norvig, and Fernando Pereira.2009.
The Unreasonable Effectiveness of Data.IEEE Intelligent Systems, 24(2):8?12.Yannis Haralambous.
2006.
New hyphenation tech-niques in ?2.
TUGboat, 27:98?103.Steven L. Huyser.
1976.
AUTO-MA-TIC WORD DI-VI-SION.
SIGDOC Asterisk Journal of ComputerDocumentation, 3(5):9?10.Timo Jarvi.
2009.
Computerized Typesetting andOther New Applications in a Publishing House.
InHistory of Nordic Computing 2, pages 230?237.Springer.Terje Kristensen and Dag Langmyhr.
2001.
Tworegimes of computer hyphenation?a comparison.In Proceedings of the International Joint Confer-ence on Neural Networks (IJCNN), volume 2, pages1532?1535.Taku Kudo, 2007.
CRF++: Yet Another CRFToolkit.
Version 0.5 available at http://crfpp.sourceforge.net/.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
In Proceedings of the 18th Interna-tional Conference on Machine Learning (ICML),pages 282?289.Franklin M. Liang and Peter Breitenlohner, 2008.
PAT-tern GENeration Program for the TEX82 Hyphen-ator.
Electronic documentation of PATGEN pro-gram version 2.3 from web2c distribution on CTAN,retrieved 2008.Franklin M. Liang.
1983.
Word Hy-phen-a-tion byCom-put-er.
Ph.D. thesis, Stanford University.Jorge Nocedal and Stephen J. Wright.
1999.
Limitedmemory BFGS.
In Numerical Optimization, pages222?247.
Springer.Wolfgang A. Ocker.
1971.
A program to hyphenateEnglish words.
IEEE Transactions on Engineering,Writing and Speech, 14(2):53?59, June.Martin Porter.
1980.
An algorithm for suffix stripping.Program, 14(3):130?137.Terrence J. Sejnowski and Charles R. Rosenberg, 1988.NETtalk: A parallel network that learns to readaloud, pages 661?672.
MIT Press, Cambridge, MA,USA.Fei Sha and Fernando Pereira.
2003.
Shallow pars-ing with conditional random fields.
Proceedings ofthe 2003 Conference of the North American Chapterof the Association for Computational Linguistics onHuman Language Technology-Volume 1, pages 134?141.Petr Sojka and Pavel Sevecek.
1995.
Hyphenation inTEX?Quo Vadis?
TUGboat, 16(3):280?289.Christos Tsalidis, Giorgos Orphanos, Anna Iordanidou,and Aristides Vagelatos.
2004.
Proofing ToolsTechnology at Neurosoft S.A. ArXiv Computer Sci-ence e-prints, (cs/0408059), August.P.T.H.
Tutelaers, 1999.
Afbreken in TEX, hoe werkt datnou?
Available at ftp://ftp.tue.nl/pub/tex/afbreken/.Antal van den Bosch, Ton Weijters, Jaap Van DenHerik, and Walter Daelemans.
1995.
The profitof learning exceptions.
In Proceedings of the 5thBelgian-Dutch Conference on Machine Learning(BENELEARN), pages 118?126.Jaap C. Woestenburg, 2006.
*TALO?s Lan-guage Technology, November.
Available athttp://www.talo.nl/talo/download/documents/Language_Book.pdf.374
