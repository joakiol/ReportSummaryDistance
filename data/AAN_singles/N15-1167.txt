Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1446?1451,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsEnhancing Sumerian Lemmatization by Unsupervised Named-EntityRecognitionYudong Liu, Clinton Burkhart, James Hearne and Liang LuoComputer Science DepartmentWestern Washington UniversityBellingham, Washington 98226{yudong.liu@-burkhac@students.-james.hearne@-luol@students.
}wwu.eduAbstractLemmatization for the Sumerian language,compared to the modern languages, is muchmore challenging due to that it is a long deadlanguage, highly skilled language experts areextremely scarce and more and more Sume-rian texts are coming out.
This paper describeshow our unsupervised Sumerian named-entityrecognition (NER) system helps to improvethe lemmatization of the Cuneiform DigitalLibrary Initiative (CDLI), a specialist databaseof cuneiform texts, from the Ur III period.
Ex-periments show that a promising improvementin personal name annotation in such texts anda substantial reduction in expert annotation ef-fort can be achieved by leveraging our systemwith minimal seed annotation.1 IntroductionBecause the Sumerian cuneiform writing system ishistorically the earliest, Sumerian culture is the ear-liest recorded civilization.
The large number of claytablets that have been recovered from Mesopotamiareveal ?an almost obsessive concern for the preser-vation of daily events of the time: the diggingof ditches, the care of livestock, the storage ofgrain, and so on.
Their survival allows insightinto the lives of the city dwellers of remote an-tiquity?
(Garfinkle, 2012).
Today, most cuneiformtexts are held in public institutions, but the textsare widely separated both from each other and of-ten from scholars by great distances and expen-sive journeys.
Current projects like the Digital Li-brary Initiative (CDLI, 2014) and the Database ofNeo-Sumerian Texts (BDTNS, 2014) aim to providescholars immediate access to virtual collections oftens of thousands of texts.The Ur III period (2112-2004 BCE) is particularlyabundant in surviving texts.
Because this era was thespecialty of our principle informant, an Assyriolo-gist at our home university, we focus on the tabletsthat are from this era.
The vast majority of thesetablets record financial transactions, such as recordsof cattle deliveries, receipt of metals, repayment ofloans, and so forth.Figure 1 shows a tablet from the CDLI repository.For expository purposes, we arranged the originalcuneiform drawings on the left (which are not inputto our computations), with its transliteration (a one-to-one transcription of signs in a cuneiform text tocomputer readable text) in the middle, and the mod-ern English translation on the right.
The originalCDLI data includes transliterations in ASCII formatand inline lemmatization markup.
More detail aboutCDLI data will be introduced in Section 2.As we can see in Figure 1, in addition to theprovider and recipient of transference, tablets con-sistently enumerate lists of witnesses (?sealed by?
).This fact makes the tablet an invaluable resourcefor the social history of the time since they record,implicitly, on each tablet, lists of persons whoknew one another and enjoyed professional rela-tions (Widell, 2008).
The recovery of personalnames on the tablets suggests the possibility of re-constructing social networks of actors in the mer-cantile class and also, given the overlap, their socialnetwork connections to royalty.Motivated by this perspective, we built an unsu-pervised Sumerian named-entity recognition (NER)system, also to accommodate the facts of 1) Sume-rian is a dead language; 2) the corpus is of a size toolarge for even a community of scholars to master; 3)the tablets come in many cases damaged by time and1446the circumstances of their recovery which was, inmany cases, looting; 4) new tablets are still being un-covered.
More detail on our Decision List Co-Trainmethod (Collins and Singer, 1999) can be found inSection 3.
In the process of evaluating our NER sys-Figure 1: Tablet with ID of P105955 from CDLI.tem, we noticed that a major inconsistency betweenour result and the lemmata in CDLI lies in the anno-tation of personal names with missing signs in dam-aged tablets.
For example, ?szu-[x]-lum?
is notlabeled as a name in the lemmata, but our systemdoes so with a high confidence score.
As shown thisword contains a damaged sign (indicated by ?[x]?
).Inconsistencies of this kind account for around 50%of the total false positives in our result.
With the helpof the Sumerologist at our home university, around40% of such damaged occurrences have been eas-ily verified as personal names.
This suggests thatthe original lemmatization is performed by a morecritical and conservative approach.
Our work offersa promising automation tool for the annotation taskon this corpus by making good recommendations onname candidates to the annotators.2 CDLI and the AnnotationsThe CDLI is a collaborative project with cuneiformtext capturing and processing efforts underway inNorth America, Europe and the Middle East.
It aimsto provide an open access to electronic documenta-tion of ancient cuneiform, consisting of texts, im-ages, transliterations and glossaries of 3500 yearsof human history.
Adhering to the open-source pol-icy, any contribution to the collection by providingelectronic catalogues, transliterations, or images ofcuneiform artifacts is welcomed (CDLIwiki, 2014).When represented in Roman script in transliter-ations, the syllable signs that make up a Sume-rian word are written joined together with dashes.As there is no concept of upper- or lowercase incuneiform writing, signs in transliterations typicallyoccur in lowercase.
However, signs rendered in up-percase do occur when the phonetic equivalent ofthe sign is unclear, tentative or fairly new (Sahala,2012).
One important property of Cuneiform is ahigh degree of homophony (referred to in the lit-erature on Cuneiform as ?polyvalence?).
This phe-nomenon is conventionally handled by numericalsubscripts.
For example, ?du?
means ?to go?, ?du3?means ?to build?
(Tablan et al, 2006).Royal epithets notwithstanding, Sumerian per-sonal names are exclusively comprised of a sin-gle word, almost always consisting of at least twosigns.
In cases where disambiguation is required, apatronymic may added (for example, szu-esz4-tar2dumu zu-zu, ?Su-Estar, son of Zuzu?).
This disam-biguation is frequent in practice due to the relativelyshallow pool of personal names used (Limet, 1960).In the lemmatization information exposed byCDLI that we make use of in our NER task, whenthe word is a noun or verbal form, the two typesof information included in the lemmata are 1) thecitation form, rendered as the Sumerian stem; 2)the guide word, which functions as a disambigua-tor and is generally the English translation of thestem; otherwise, the lemma contains only the part ofspeech, as is the case with proper names and num-bers.
For example, in the following excerpt (CDLINo: P100032), wherein text is presented with inter-linear lemmata (English translation: Egi-zi-mah re-ceived 2 oxen from runner.
), we see both types oflemmatization.1.
2(disz) gu4#lem: n; gud[ox]2. ki kas4-ta#lem: ki[place]; kasz[runner]3. egi-zi-mah i3-dab5#lem: PN; dab[seize]On line 3, the verbal form i3-dab5, indicatingthe receipt of an animate object, is lemmatized withthe citation form dab, which is the Sumerian rootfor this form, and guide word ?seize?, the bestEnglish translation of the citation form.
On lines 1and 3, we have a number lemmatized with the part ofspeech n and the personal name egi-zi-mah withthe part of speech PN, respectively.
These annotated1447PNs are used as gold standard labels to evaluate ourNER system.In the study of the Ur III corpus, the most exhaus-tive infrastructure and documentation for lemmati-zation is that provided for ?the Open Richly Anno-tated Cuneiform Corpus (Oracc)?
(ORACC, 2014).The lemmatizer for the Oracc system is accessedvia an Emacs interface designed to encourage si-multaneous transliteration and lemmatization by ahuman expert.
The process begins with the humanexpert submitting an unlemmatized transliterationin a format called ATF (ASCII Transliteration For-mat).
This format is the standard interchange for-mat for transliteration across many projects dealingin and exchanging Assyriological textual represen-tations (such as CDLI, BDTNS, the PennsylvaniaSumerian Dictionary (PSD, 2006), and Digital Cor-pus of Cuneiform Lexical Texts (DCCLT, 2014)).Via the Emacs interface, the transliteration is sub-mitted to the linguistic annotatation system, whichidentifies an existing project-specific glossary basedon directives provided by the human expert in thetransliteration, and returns a preliminary lemmatiza-tion whose completeness and content depends on thereferenced project glossary.
The transliterator maythen modify any automatically-generated lemmata,or, in the case of new words or new senses in whichexisting words used, manually lemmatize the wordto allow the lemmatizer to ?harvest?
the new lemmaand add it to the glossary.
Oracc?s lemmatizer alsoperforms normalization and morphological analysisin order to automatically and consistently identifywords in the text.
The lemmatizer is not designedto ?learn?
new insights or induce new rules regard-ing Sumerian morphology on the basis of new lem-mata harvested from submissions, but rather servesas a mechanism to consistently apply rules that havebeen harvested.Based on our statistics, 53,146 tablets (about60%) of the CDLI repository are accompanied bythe in-line annotations described above.
That is theamount of the tablets we used for the NER System.3 Sumerian Personal Name Recognition3.1 Related WorkTo our knowledge, no previous empirical researchexists directly addressing the question of how to rec-ognize named entities from the Sumerian text.
Ourvery preliminary work on this task (Brewer et al,2014) uses an existent name list to recognize exist-ing names, and applies simple heuristics and a sim-ilarity measure to recognize unseen personal namesand dates.
And at the time, no comprehensive evalu-ation and analysis could be done due to the unavail-ability of the language expert.The investigation most closely related to ours isfound in (Jaworski, 2008), which describes a sys-tem for processing Sumerian economic documents.Even though we borrowed 3 rules from their workas our seed rules (more details can be found in Sec-tion 3.2), and we are dealing with the same languagein the same domain, there are a few important differ-ences between our work and theirs.
1) Their goal isto model the content of the text by using an ontol-ogy driven method, whereas our goal is to extractnamed entities from the text by using some statis-tical method.
2) Their data set is strictly smallerthan ours.
The corpus used in their work was re-stricted to ?12,000 tablets containing transactionsinvolving animals, with the contents of these trans-actions being extracted via an a priori ontology.
Ourwork is addressed to almost the entire corpus wherethe lemmatization is available, ?53,000 tablets.
3)Their work involved no learning but rather the appli-cation of pre-defined Finite State Methods for entityrecognition.Supervised named entity recognition has achievedexcellent performance (Bikel et al, 2002) (Zhou andSu, 2002) (McNamee and Mayfield, 2002) (MaCal-lum and Li, 2003) (Oliver et al, 2003).
Semi-supervised approaches and unsupervised approacheshave also achieved notable success on this task.
Al-though our research also has a fairly large amount ofdata, unlike the previous unsupervised methods (Et-zioni et al, 2005) (Nadeau et al, 2006) (Li et al,2012), we do not have extremely large external cor-pora such as Wikipedia to retrieve very precise, butsparse features.
Our work adopted the DL-Cotrainmethod proposed in (Collins and Singer, 1999).However, all their features are at the word sequencelevel, instead of at the token level.
As noted in Sec-tion 2, there is no concept of upper- or lowercase incuneiform writing, features on capitalization are notrelevant here.
Another important observation is thatSumerian personal names are exclusively comprised1448of a single word, thus our spelling features are onthe token level.
In addition, unlike their work wherePOS and parsing information is used for named en-tity candidate selection, we do not have the candi-date selection component given that no SumerianPOS tagger or parser available.
In fact, further com-plicating factors in determining syntactic featuresinclude the lack of standardization in spelling andinconsistent scribal quality.3.2 Our SystemOur NER system has three components: the pre-processing component, the Decision List Co-Train(DL-CoTrain) (Collins and Singer, 1999) compo-nent and the post-processing component.When the Sumerologists transliterate the tablets,they use metacharacters such as ?[...]?
and ?#?
toindicate damage to the text, and ?!
?, ??
?, ?
*?, and?<...>?
to represent correction, querying or colla-tion (Tinney and Robson, 2014).
For ?[...]?
and?<...>?
cases, the Sumerologists put their ?bestguess?
within the brackets.
For example, in the word?
[nu]-su?, the first sign was originally damaged butrestored by the Sumerologists as the ?best guess?.Our system removes the metacharacters as noise,and treats the resulting text as if it were otherwiseunannotated.To utilize the pre-knowledge from the languageexperts and (Weibull, 2004), we apply a tag set of13 tags to pre-annotate the corpus.
The 13 tags inthe tag set {?GN?, ?FN?, ?TN?, ?WN?, ?MN?, ?n?,?TITLE?, ?UNIT?, ?GOODS?, ?OCCUPATION?,?YEAR?, ?MONTH?, ?DAY?}
represent geograph-ical names, field names, temple names, watercoursenames, month names, numbers, title names, unitnames, trade goods names, occupation names andindicators for year, month and day, respectively.After the above pre-processing step, we appliedthe DL-CoTrain method by utilizing contextual andspelling rules to create a decision list.A contextual rule specifies the context for anamed-entity with the window size of 1 or -1 (theright word or the left word).
For example, ac-cording to the contextual rule ?right context = TI-TLE ?
Person?, ?nam-zi?
is recognized as apersonal name in ?nam-zi simug?
given that?simug?
is pre-tagged as ?TITLE?
(Smith) in thepre-processing phase.A spelling rule specifies the spelling of a named-entity.
It is a sign sequence that can be either thefull string of an entity or is contained as a sub-string of the entity.
For example, ?contains(ab-ba)?
Person?
is a spelling rule.
By applying therule, the word ?ab-ba-sab-ga-ta?
is recog-nized as a personal name.
With the spelling rule?full-string = ur-{d}szul-pa-e3 ?
Person?,the word ?ur-{d}szul-pa-e3?
is recognized asa personal name.We use the following three contextual rules (Ja-worski, 2008) as the seed rules for the system 1)left context = giri3 ?
Person 2) left context =kiszib3 ?
Person 3) left context = mu-DU ?Person.The first rule indicates that a person is actingas an intermediary in the transaction.
The sec-ond rule indicates that the tablet was sealed by thenamed individual, and usually appears in adminis-trative records.
The last rule indicates that a deliverywas made to the named individual.
Since these seedrules have a high specificity to personal names, eachof them is given a strength of 0.99999.The major task of the system is to learn a deci-sion list to classify a word as a personal name.
Ini-tialized with the 3 contextual seed rules, the deci-sion list is applied to label the training data to getspelling rules.
In the next iteration, the newly ob-tained spelling rules are applied to label the trainingdata to get new contextual rules.
In this alternatingprocess, each iteration produces a new set of ruleswhich are ranked by their strength.In our NER system, we experimentally settled ona ranking criterion that made use of frequency ofsome feature x, instead of (smoothed) relative fre-quency as used in (Collins and Singer, 1999), in or-der to avoid the problem of some context feature oc-curs once only as the cue of a personal name, and re-verting to the relative frequency formula in the caseof ties.The two post-processing rules are applied to elim-inate false positives 1) A word that starts with a num-ber should not be a name; 2) A word following theword ?iti?
(month indicator) should not be a name.The application of these 2 rules improved the perfor-mance by 0.5%.14494 Experiments and EvaluationWe used a 5-fold cross-validation model to train andtest our NER system.
In each fold, we randomlypicked 85% of the tablets from the corpus for train-ing and the remaining 15% of the tablets for testing.With the top 20 new rules from each iteration be-ing added to the decision list, the system producesa decision list of over 2000 rules and approximately17,000 personal names in these Sumerian texts, after150 iterations.When the lemmata is used as the gold standarddata set in this experiment, the system achieved91.4% recall and 39.6% precision score on averagefrom the 5-fold cross-validation.
The low precisionmotivated us to take a closer look at the cause of thefalse positives from our system.Using fold-2 as an example, the system reported16,657 personal names, and there are 7,406 anno-tated names in the lemmata.
Among all these 7,406names, 91.4% has been correctly identified by thesystem.
However, 60.6% of the names reported byour system are not labeled as names in the lemmata.Through error analysis, we found that nearly 50%of these false positive names contain?missing?
or?damaged?
signs in the transliteration (i.e., anno-tated as [x] or [u] in the lemmata).
They were there-fore not annotated at all in the lemmata, even thoughtheir linguistic context clearly shows that they arepersonal names.
For example, ?szu-x-lum?
in?giri3 szu-x-lum?
is a word in the testing datalabeled as a name by our system after applying oneof the seed rules.
However, owing to physical dam-age to the word in the original tablet (flagged by ?x?in the lemmata), it is unannotated.
As a result, it?sreported as a false positive in the evaluation.Based on this observation, we asked the Sumeriol-ogist at our home university to verify the ?false pos-itives?
that contain ?missing?
or ?damaged?
signs(marked in the lemmata as either ?unknown?
(part ofspeech X) or ?unlemmatizable?
(part of speech u)),restricting our concern to damaged signs to limit theimposition on his time.
It turns out that over 40% ofsuch names should have been labeled as a name inthe first place.
This elevates the precision to 55.8%from 39.4% without sacrificing the recall, for fold-2testing data.
Similar performance gain is obtainedfor other folds.Due to the large number of ?false positives?
andtime constraints, we cannot impose on our Assyriol-ogist informant the task of verifying all of the sys-tem reported names for us at the moment.
However,the current evaluation result reveals that the system-atic lemmatization on CDLI, as discussed in Sec-tion 2, follows an extremely conservative approach.We suspect that the reason for this is to avoid la-beling damaged personal names as such is to pre-vent partial or potentially incorrect sign informationfrom being reused by the morphological analyzer infuture runs of the lemmatizer.
Our result suggeststhat the existing lemmata has its own limitation andshould not be fully relied on for evaluation for ourNER task.
It also suggests that our NER systemcan be used for automatic annotation task given thatit performs well in recovering names based on thecontext and spelling features, even with the minimalprior knowledge.
More details of the algorithms andresult can be found in (Liu et al, 2015).5 Conclusions and Future WorkWe have shown that a DL-CoTrain based name tag-ger, with only three initial seed rules and unlabeleddata, performs well in recovering personal namesfrom Sumerian texts.
This work can potentiallymake the annotation job much less costly, especiallywhen the expert resource is extremely scarce.Our results show that the existing lemmatizationon CDLI corpus was generated by a, perhaps, exces-sively conservative policy, especially when one ormore signs in the name have sustained damage.
Asa result, we consider that the existing lemmata can-not be fully relied on, especially for damaged names,for our NER evaluation.
Our system is able to makegood guesses on such damaged occurrences, basedon the context and the spelling features.
Confirmedby the language expert, such a high-recall, not-so-high-precision system can be particularly useful forthe corpus annotators because they can simply fo-cus on and verify the system?s recommended names.Furthermore, we would expect that by applying su-pervised learning or combining with gazetteer-basedmethod, and by extending the current method to rec-ognizing other types of names in the texts, our sys-tem can work even better as an automation tool forsuch an annotation task.1450ReferencesSteven Garfinkle.
2012.
Entrepreneurs and Enterprise inEarly Mesopotamia: A Study of Three Archives fromthe Third Dynasty of Ur, 36?136 Cornell Univer-sity Studies in Assyriology and Sumerology (CUSAS),Ithaca, NY USA.Michael Collins and Yora Singer.
1999.
UnsupervisedModels for Named Entity Classification, 100?110.
InProceedings of the Joint SIGDAT Conference on Em-pirical Methods in Natural Language Processing andVery Large CorporaWojciech Jaworski.
2008.
Contents Modeling of Neo-Sumerian Ur III Economic Text Corpus, 369?376.Proceedings of the 22nd International Conference onComputational LinguisticsAleksi Sahala.
2012.
Notation in Sumerian Translitera-tion.Nikolai Weibull.
2004.
A Historical Survey of NumberSystems, 1?13.Valentin Tablan, Wim Peters, Diana Maynard, andHamish Cunningham.
2006.
Creating Tools for Mor-phological Analysis of Sumerian, 1762?1765.
Pro-ceedings of the Fifth International Conference on Lan-guage Resources and EvaluationMagnus Widell.
2008.
The Ur III Metal Loans from Ur,207?223.
Consejo Superior de Investigationes Cient-ficas, Madrid.Database of Neo-Sumerian Texts.
2014.http://bdtns.filol.csic.esCuneiform Digital Library Initative.
2014.http://cdli.ucla.eduCuneiform Digital Library Initative wiki.
2014.http://cdli.ox.ac.uk/wiki/Oracc: The Open Richly Annotated Cuneiform Corpus.2014.
http://oracc.museum.upenn.edu/PSD: The Pennsylvania Sumerian Dictionary.
2006.http://psd.museum.upenn.edu/DCCLT - Digital Corpus of Cuneiform Lexical Texts.2014.
http://oracc.museum.upenn.edu/dcclt/Felicity Brewer, Clinton Burkhart, Joe Houng,Liang Luo, Derek Riley, Brandon Toner, Yudong Liu,and James Hearne.
2014.
A Preliminary Study intoNamed Entity Recognition in Cuneiform Tablets,1?3.
The third Pacific Northwest Regional NaturalLanguage Processing WorkshopDaniel Foxvog.
2014.
An Introduction to SumerianGrammar.Henri Limet.
1960.
L?Anthroponymie sumeriennedans les documents de la 3e dynastie d?Ur.
Soci?et?ed?
?Edition Les Belles Lettres, Paris.Manuel Molina.
2008.
The Corpus of Neo-SumerianTablets: An Overview, 19?54.
Consejo Superior deInvestigationes Cientficas, Madrid.Steve Tinney and Eleanor Robson.
2014.
Oracc:The Open Richly Annotated Cuneiform Corpus.http://oracc.museum.upenn.edu/doc/about/aboutoracc/index.htmlRoger Woodard.
2008.
The Ancient Languages ofMesopotamia, Egypt and Aksum.
Cambridge Univer-sity Press, Cambridge, UK.Steve Tinney and Eleanor Robson.2014.
Oracc: Linguistic Annotation.http://build.oracc.org/doc/builder/linganno/Yudong Liu, Clinton Burkhart, James Hearne, andLiang Luo.
2015.
Unsupervised Sumerian PersonalName Recognition.
Proceedings of the Twenty-eighthInternational Florida Artificial Intelligence ResearchSociety Conference, May 18-20, 2015, Hollywood,Florida, USA.
AAAI Press, 2015.Andrew McCallum and Wei Li.
2003.
Early results forNamed Entity Recognition with Conditional RandomFields, Feature Induction and Web-Enhanced Lexi-cons.
Proceedings of the Seventh Conference on Nat-ural Language Learning at HLT-NAACL 2003.Oliver Bender, Franz Josef Och, and Hermann Ney.2003.
Maximum Entropy Models for Named EntityRecognition.
Proceedings of the Seventh Conferenceon Natural Language Learning at HLT-NAACL 2003.Paul McNamee and James Mayfield.
2002.
Entity Ex-traction Without Language-specific Resources.
Pro-ceedings of the 6th Conference on Natural LanguageLearning - Volume 20.Guodong Zhou and Jian Su.
2002.
Named Entity Recog-nition using an HMM-based Chunk Tagger.
Proceed-ings of the 40th Annual Meeting on Association forComputational Linguistics.Daniel M. Bikel and Richard Schwartz and RalphM.
Weischedel.
1999.
An Algorithm That LearnsWhat?s in a Name.
Machine Learning.Etzioni, Oren and Cafarella, Michael and Downey, Dougand Popescu, Ana-Maria and Shaked, Tal and Soder-land, Stephen and Weld, Daniel S and Yates, Alexan-der.
2005.
Unsupervised named-entity extraction fromthe web: An experimental study.
Artificial intelligence- Volume 165.Nadeau, David and Turney, Peter and Matwin, Stan.2006.
Unsupervised named-entity recognition: Gen-erating gazetteers and resolving ambiguity.
Advancedin Artificial Intelligence - Lecture Notes in ComputerScience - Volume 14013.Li, Chenliang, Weng, Jianshu and He, Qi and Yao, Yuxiaand Datta, Anwitaman and Sun, Aixin and Lee, Bu-Sun.
2012.
Twiner: named entity recognition in tar-geted twitter stream.
Proceedings of the 35th interna-tional ACM SIGIR conference on Research and devel-opment in information retrieva.1451
