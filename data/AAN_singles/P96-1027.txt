Chart GenerationMart in  KayStanford Universi~ andXerox Palo Alto Research Centerkay@pare ,  xerox,  eomAbstractCharts constitute a natural uniform architecture forparsing and generation provided string position isreplaced by a notion more appropriate to logicalforms and that measures are taken to curtail gener-ation paths containing semantically incompletephrases.1 ChartsShieber (1988) showed that parsing charts can be also usedin generation and raised the question, which we take upagain here, of whether they constitute a natural uniformarchitecture for parsing and generation.
In particular, wewill be interested in the extent o which they bring to thegeneration process advantages comparable to those thatmake them attractive in parsing.Chart parsing is not a well defined notion.
The usualconception of it involves at least four related ideas:Inactive edges.
In context-free grammar, all phrases of agiven category that cover a given part of the string areequivalent for the purposes of constructing largerphrases.
Efficiency comes from collecting equivalentsets of phrases into (inactive) edges and constructingedges from edges rather than phrases from phrases.Active edges.
New phrases of whatever size can be builtby considering existing edges pair-wise if provision ismade for partial phrases.
Partial phrases are collectedinto edges that are said to be active because they can bethought of as actively seeking material to completethem.The algorithm schema.
Newly created edges are placedon an agenda.
Edges are moved from the agenda to thechart one by one until none remains to be moved.When an edge is moved, all interactions between it andedges already in the chart are considered and any newedges that they give rise to are added to the agenda.Indexing.
The positions in the string at which phrasesbegin and end can be used to index edges so that thealgorithm schema need consider interactions onlybetween adjacent pairs.Chart parsing is attractive for the analysis of natural an-guages, as opposed to programming languages, for the wayin which it treats ambiguity.
Regardless of the number ofalternative structures for a particular string that a givenphrase participates in, it will be constructed once and onlyonce.
Although the number of structures of a string cangrow exponentially with the length of the string, the numberof edges that needs to be constructed grows only with thesquare of the string length and the whole parsing processcan be accomplished in cubic time.Innumerable variants of the basic chart parsing schemeare possible.
For example, if there were languages withtruly free word order, we might attempt o characterizethem by rules like those of context-free grammar, but with asomewhat different interpretation.
I stead of replacing non-terminal symbols in a derivation with strings from the right-hand side of corresponding rules, we would remove thenonterminal symbol and insert the symbols from the right-hand side of the rule at arbitrary places in the string.A chart parser for languages with free word orderwould be a minor variant of the standard one.
An edgewould take the form X~,, where v is a vector with a bit forevery word in the string and showing which of those wordsthe edge covers.
There is no longer any notion of adjacencyso that there would be no indexing by string position.
Inter-esting interactions occur between pairs of edges whose bitvectors have empty intersections, indicating that they coverdisjoint sets of words.
There can now be as many edges asbit-vectors and, not surprisingly, the computational com-plexity of the parsing process increases accordingly.2 GenerationA parser is a transducer from strings to structures orlogical forms.
A generator, for our purposes, is the inverse.One way to think of it, therefore, is as a parser of structuresor logical forms that delivers analyses in the form of strings.This view has the apparent disadvantage of putting insignif-icant differences in the syntax of a logical forms, such asthe relative order of the arguments to symmetric operators,on the same footing as more significant facts about them.We know that it will not generally be possible to reduce200logical expressions to a canonical form but this does notmean that we should expect our generator to be compro-mised, or even greatly delayed, by trivial distinctions.
Con-siderations of this kind were, in part, responsible for therecent resurgence of interest in "flat" representations of log-ical form (Copestake t a/.,1996) and for the representa-tions used for transfer in Shake-and-Bake translation(Whitelock, 1992).
They have made semantic formalismslike those now usually associated with Davison (Davidson,1980, Parsons, 1990) attractive in artificial intelligence formany years (Hobbs 1985, Kay, 1970).
Operationally, theattraction is that the notations can be analyzed largely asfree word-order languages in the manner outlined above.Consider the expression ( 1 )(1) r: run(r), past(r), fast(r), argl (r,j), name(j, John)which we will take as a representation f the logical form ofthe sentences John ran fast and John ran quickly.
It consistsof a distinguished index (r) and a list of predicates whoserelative order is immaterial.
The distinguished index identi-fies this as a sentence that makes a claim about a runningevent.
"John" is the name of the entity that stands in the'argl '  relation to the running which took place in the pastand which was fast.
Nothing turns on these details whichwill differ with differing ontologies, logics, and views ofsemantic structure.
What concerns us here is a procedurefor generating a sentence from a structure of this generalkind.Assume that the lexicon contains entries like those in(2) in which the italicized arguments to the semantic predi-cates are variables.
(2)Words Cat SemanticsJohn np(x) x: name(x, John)ran vp(x, y) x: run(x), argl (x, y),past(x)fast adv(x) x: fast(x)quickly adv(x) x: fast(x)A prima facie argument for the utility of these particularwords for expressing (I) can be made simply by noting that,modulo appropriate instantiation of the variables, thesemantics of each of these words subsumes (1).3 The Algorithm SchemaThe entries in (2), with their variables uitably instantiated,become the initial entries of an agenda and we begin tomove them to the chart in accordance with the algorithmschema, say in the order given.The variables in the 'Cat' and 'Semantics' columns of(2) provide the essential link between syntax and semantics.The predicates that represent the semantics of a phrase willsimply be the union of those representing the constituents.The rules that sanction a phrase (e.g.
(3) below) showwhich variables from the two parts are to be identified.When the entry for John is moved, no interactions arepossible because the chart is empty.
When run is moved, thesequence John ran is considered as a possible phrase on thebasis of rule (3).
(3) s(x) ~ rip(y), vp(x, 3').With appropriate r placements for variables, this maps ontothe subset (4) of the original semantic specification i (1).
(4) r: run(r), past(r), argl(r,j), name(j, John)Furthermore it is a complete sentence.
However, it does notcount as an output to the generation process as a wholebecause it subsumes ome but not all of (1).
It thereforesimply becomes a new edge on the agenda.The string ran fast constitutes a verb phrase by virtueof rule (5) giving the semantics (6), and the phrase ranquickly with the same semantics i put on the agenda whenthe quickly edge is move to the chart.
(5) vp(x) ~ vp(x) adv(x)(6) r: run(r), past(r), fast(r), arg 1 (r, y)The agenda now contains the entries in (7).
(7)Words Cat SemanticsJohn ran s(r) r: run(r), past(r), argl(r,j),name(j, John)ran fast vp(r, j) r: run(r), past(r), fast(r),argl(r,j)ran quickly vp(r, j) r: run(r), past(r), fast(r),argl(r,j)Assuming that adverbs modify verb phrases and not sen-tences, there will be no interactions when the John ran edgeis moved to the chart.When the edge for ran .fast is moved, the possibilityarises of creating the phrase tan fast quickly as well as ranfast fast.
Both are rejected, however, on the grounds thatthey would involve using a predicate from the originalsemantic specification more than once.
This would be simi-lar to allowing a given word to be covered by overlappingphrases in free word-order parsing.
We proposed eliminat-ing this by means of a bit vector and the same techniqueapplies here.
The fruitful interactions that occur here arebetween ran .fast and ran quickly on the one hand, and John201on the other.
Both give sentences whose semantics ub-sumes the entire input.Several things are noteworthy about the process justoutlined.!.
Nothing turns on the fact that it uses a primitive versionof event semantics.
A scheme in which the indiceswere handles referring to subexpressions i  any varietyof fiat semantics could have been treated in the sameway.
Indeed, more conventional formalisms with richlyrecursive syntax could be converted to this form on thefly.2.
Because all our rules are binary, we make no use ofactive edges.3.
While it fits the conception of chart parsing given atthe beginning of this paper, our generator does notinvolve string positions centrally in the chart represen-tation.
In this respect, it differs from the proposal ofShieber (1988) which starts with all word edges leav-ing and entering asingle vertex.
But there is essentiallyno information in such a representation.
Neither thechart nor any other special data structure is required tocapture the fact that a new phrase may be constructibleout of any given pair, and in either order, if they meetcertain syntactic and semantic riteria.4.
Interactions must be considered explicitly betweennew edges and all edges currently in the chart, becauseno indexing is used to identify the existing edges thatcould interact with a given new one.5.
The process is exponential in the worst case because, ifa sentence contains a word with k modifiers, then aversion it will be generated with each of the 2 k subsetsof those modifiers, all but one of them being rejectedwhen it is finally discovered that their semantics doesnot subsume the entire input.
If the relative orders ofthe modifiers are unconstrained, matters only getworse.Points 4 and 5 are serious flaws in our scheme for which weshall describe remedies.
Point 2 will have some importancefor us because it will turn out that the indexing scheme wepropose will require the use of distinct active and inactiveedges, even when the rules are all binary.
We take up thecomplexity issue first, and then turn to bow the efficiency ofthe generation chart might be enhanced through indexing.4 Internal and External IndicesThe exponential factor in the computational complexity ofour generation algorithm is apparent in an example like (8).
(8) Newspaper reports aid the tall young Polish athleteran fastThe same set of predicates that generate this sentenceclearly also generate the same sentence with deletion of allsubsets of the words tall, young, and Polish for a total of 8strings.
Each is generated in its entirety, though finallyrejected because it fails to account for all of the semanticmaterial.
The words newspaper and fast can also be deletedindependently giving a grand total of 32 strings.We concentrate on the phrase tall young Polish athletewhich we assumed would be combined with the verb phraseran fast by the rule (3).
The distinguished index of the nounphrase, call it p, is identified with the variable y in the rule,but this variable is not associated with the syntactic ate-gory, s, on the left-hand side of the rule.
The grammar hasaccess to indices only through the variables that annotategrammatical categories in its rules, so that rules that incor-porate this sentence into larger phrases can have no furtheraccess to the index p. We therefore say that p is internal tothe sentence the tall young Polish athlete ran fast.The index p would, of course, also be internal to thesentences the young Polish athlete ran fast, the tall Polishathlete ran fast, etc.
However, in these cases, the semanticmaterial remaining to be expressed contains predicates thatrefer to this internal index, say 'tall(p)', and 'young(p)'.While the lexicon may have words to express these predi-cates, the grammar has no way of associating their referentswith the above noun phrases because the variables corre-sponding to those referents are internal.
We conclude that,as a matter of principle, no edge should be constructed ifthe result of doing so would be to make internal an indexoccurring in part of the input semantics that the new phrasedoes not subsume.
In other words, the semantics of a phrasemust contain all predicates from the input specification thatrefer to any indices internal to it.
This strategy does not pre-vent the generation of an exponential number of variants ofphrases containing modifiers.
It limits proliferation of the illeffects, however, by allowing only the maximal one to beincorporated in larger phrases.
In other words, if the finalresult has phrases with m and n modifiers respectively, then2 n versions of the first and 2 m of the second will be created,but only one of each set will be incorporated into largerphrases and no factor of 2 (n+m) will be introduced into thecost of the process.5 IndexingString positions provide a natural way to index the stringsinput to the parsing process for the simple reason that thereare as many of them as there are words but, for there to beany possibility of interaction between apair of edges, theymust come together at just one index.
These are the naturalpoints of articulation i  the domain of strings.
They cannotfill this role in generation because they are not natural prop-erties of the semantic expressions that are the input to theprocess.
The corresponding atural points of articulation i202flat semantic structures are the entities that we have alreadybeen referring to as indices.In the modified version of the procedure, whenever anew inactive dge is created with label B(b ...), then for allrules of the form in (9), an active edge is also created withlabel A(...)/C(c ...).
(9) A(...) ~ B(b ...) C(c ...)This represents a phrase of category A that requires aphraseof category C on the right for its completion.
In these labels,b and c are (variables representing) the first, or distin-guished indices associated with B and C. By analogy withparsing charts, an inactive edge labeled B(b ...) can bethought of as incident from vertex b, which means simplythat it is efficiently accessible through the index b.
An activeedge A(...)/C(c ...) should be thought of as incident from, oraccessible through, the index c. The key property of thisscheme is that active and inactive dges interact by virtue ofindices that they share and, by letting vertices correspond toindices, we collect ogether sets of edges that could interact.We illustrate the modified procedure with the sentence(10) whose semantics we will take to be (11), the grammarrules (12)-(14), and the lexical entries in (15).
(10) The dog saw the cat.
(11 ) dog(d), def(d), saw(s), past(s), cat(c), def(c),argl (s, d), arg2(s, c).
(l 2) s(x) ~ np(y) vp(x, y)(13) vp(x,y) --* v(x,y, z) np(z)(14) rip(x) ~ det(x) n(x)(15)WordscatsawdogtheCat Semanticsn(x) x: cat(x)v(x, y, z) x: see(x), past(x), argl (x, y),arg2(xcz)n(x) x: dog(x)det(x) x: def(x)The procedure will be reminiscent of left-corner parsing.Arguments have been made in favor of a head-driven strat-egy which would, however, have been marginally morecomplex (e.g.
in Kay (1989), Shieber, et el.
(1989)) and thedifferences are, in any case, not germane to our current con-cerns.The initial agenda, including active edges, and collect-ing edges by the vertices that they are incident from, isgiven in (16).The grammar is consulted only for the purpose of cre-ating active edges and all interactions in the chart arebetween active and inactive pairs of edges incident from thesame vertex.
(16)VertdWordsthethedogsawsawthethecatCatdet(d)npfd)/n(d)n(d)v(s, d, c)vp(s, d)/np(c)det(c)np(c)/n(c)n(c)Semanticsd: deffd)d: def(d)d: dog(d)s: see(s), past(s),argl(s, d), arg2(s, c)r: see(s), past(s),argl(r,j)c: def(c)c: def(c)c: dog(c)(17)VertdWordsthe dogsaw thecatc the cats saw thecatCatnp(d)vp(s, d)/np(d)np(c)vp(s, d)Semanticsd: dog(d), def(d)s: see(s), past(s),arg 1 (s, d), arg2(s, c),cat(c), def(c)c: cat(c), def(c)s: see(s), past(s),argl (s, d), arg2(s, c),cat(c), def(c)Among the edges in (16), there are two interactions,one at vertices c and d. They cause the first and third edgesin (17) to be added to the agenda.
The first interacts with theactive edge originally introduced by the verb "saw" produc-ing the fourth entry in (17).
The label on this edge matchesthe first item on the right-hand side of rule (12) and theactive edge that we show in the second entry is also intro-duced.
The final interaction is between the first and secondedges in (17) which give rise to the edge in (18).This procedure confirms perfectly to the standard algo-rithm schema for chart parsing, especially in the versionthat makes predictions immediately following the recogni-tion of the first constituent ofa phrase, that is, in the versionthat is essentially a caching left-comer parser.203(18)VertsWords Cat SemanticsThe dog saw the cat s(s) dog(d), def(d),see(s),past(s),arg 1 (s, d),arg2(s, c), cat(c),def(c).6 AcknowledgmentsWhatever there may be of value in this paper owes much tothe interest, encouragement, and tolerance of my colleaguesMarc Dymetman, Ronald Kaplan, John Maxwell, andHadar Shem Toy.
I am also indebted to the anonymousreviewers of this paper.ReferencesCopestake, A., Dan Flickinger, Robert Malouf, andSusanne Riehemann, and Ivan Sag (1996).
TranslationUsing Minimal Recursion Semantics.
Proceedings of TheSixth International Conference on Theoretical nd Method-ological Issues in Machine Translation, Leuven (in press).Davidson, D. (1980).
Essays on Actions and Events.Oxford: The Clarendon Press.Hobbs, J. R. (1985).
Ontological Promiscuity.
23rdAnnual Meeting of the Association for Computational Lin-guistics, Chicago, ACL.Kay, M. (1970).
From Semantics to Syntax.
Progressin Linguistics.
Bierwisch Manfred, and K. E. Heidolf.
TheHague, Mouton: !
14-126.Kay, M. (1989).
Head-driven Parsing.
Proceedings ofWorkshop on Parsing Technologies, Pittsburgh, PA.Parsons, T. (1990).
Events in the Semantics of English.Cambridge, Mass.
: MIT Press.Shieber, S. (1988).
A Uniform Architecture for Parsingand Generation.
COLING-88, Budapest, John yon Neu-mann Society for Computing Sciences.Shieber, S. M. et al (1989).
A Semantic-Head-DrivenGeneration Algorithm for Unification Based Formalisms.27th Annual Meeting of the Association for ComputationalLinguistics, Vancouver.
B.C.Whitelock, P. (1992).
Shake and-Bake Translation.COLING-92, Nantes.204
