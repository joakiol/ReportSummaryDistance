Proceedings of the SIGDIAL 2013 Conference, pages 251?260,Metz, France, 22-24 August 2013. c?2013 Association for Computational LinguisticsSurface Text based Dialogue Models for Virtual HumansSudeep Gandhe and David TraumUSC Institute for Creative Technologies,12015 Waterfront Drive, Playa Vista, CA 90094, USAsrgandhe@gmail.com, traum@ict.usc.eduAbstractWe present virtual human dialogue mod-els which primarily operate on the surfacetext level and can be extended to incor-porate additional information state annota-tions such as topics or results from simplermodels.
We compare these models withpreviously proposed models as well as twohuman-level upper baselines.
The mod-els are evaluated by collecting appropri-ateness judgments from human judges forresponses generated for a set of fixed dia-logue contexts.
Our results show that thebest performing models achieve close tohuman-level performance and require onlysurface text dialogue transcripts to train.1 IntroductionVirtual Humans (VH) are autonomous agents whocan play the role of humans in simulations (Rickeland Johnson, 1999; Traum et al 2005).
For thesesimulations to be convincing these agents musthave the ability to communicate with humans andother agents using natural language.
Like other di-alogue system types, different architectures havebeen proposed for virtual human dialogue sys-tems.
These architectures can afford different fea-tures and require different sets of resources.
E.g.,an information state based architecture such as theone used in SASO-ST (Traum et al 2005) canmodel detailed understanding of the task at handand progression of dialogue, but at the cost of re-quiring resources such as information state updaterules and an annotated corpus or grammar to beable to map surface text to dialogue acts.For some virtual human dialogue genres suchas simple question-answering or some negotiationdomains, a simple model of dialogue progressionwould suffice.
In such a case we can build dia-logue models that primarily operate on a surfacetext level.
These models only require surface textdialogue transcripts as a resource, and don?t re-quire expensive manual update rules, grammars,or even extensive corpus annotation.In this paper, we describe the construction andevaluation of several models for engaging in dia-logue by selecting an utterance that has been seenpreviously in a corpus.
We include one model thathas been used for this task previously (Gandhe andTraum, 2007b), an adaptation of a model that hasbeen used in a similar manner, though on hand-authored data sets, rather than data sets extractedautomatically from a corpus (Leuski and Traum,2008), as well as a new set of models, using per-ceptrons on surface text features as well as moreabstract information state annotations such as top-ics.
We also tackle the question of evaluating suchdialogue models manually as well as automati-cally, starting with systematically analyzing var-ious decisions involved in the evaluation process.We situate our work with respect to previous eval-uation methods.2 Related WorkThe task of a dialogue model is to formulate anutterance given a dialogue context.
There aretwo approaches towards formulating an utterance:Generation, where a response is compositionallycreated from elements of the information state,including the context of previous utterances, andSelection, where a response is chosen from pre-viously seen set of responses.
In (Gandhe andTraum, 2010), we examined the theoretical poten-tial for the selection approach, looking at a widevariety of domains, and evaluating based on sim-ilarity between the actual utterance and the bestmatch in the previously seen corpus.
We saw awide variance in scores across domains, both as tothe similarity scores and improvement of scores asmore data is considered.
For task-oriented plan-ning domains, such as Monroe (Stent, 2000) and251TRAINS (Heeman and Allen, 1994), as well asopen conversation in Switchboard (Godfrey et al1992), the performance was very low.
On the otherhand, for more limited domains such as simplequestion-answering (Leuski et al 2006) or role-play negotiation in a scenario, the performancewas high, with METEOR scores averaging over0.8.One possible selection criterion is to assumethat the most appropriate response is the mostprobable response according to a model trainedon human-human dialogues.
More formally, letthere be a dialogue ?u1, u2, .
.
.
, ut?1, ut, .
.
.
, uT ?,where utterance ut appears in contextt =?u1, u2, .
.
.
, ut?1?.
If we have a dialogue modelP estimated from the training corpus then the for-mulated response uq for some unseen contextq isgiven by,ut = argmaxiP (ui|contextt) ?ui ?
Upossiblewhere Upossible is a set of all possible response ut-terances.
Ideally we would like to estimate a prob-ability distribution P , but since it?s hard to esti-mate and we only need argmax for this applica-tion, we approximate P with a ranking function.We can compare previous work within this frame-work.In our previous work (Gandhe and Traum,2007a), we used context similarity as the rank-ing function P (see section 3.1 for details).
Thismodel is trained from in-domain surface text di-alogue transcripts.
Leuski et al(2006) model Pas cross-lingual relevance, where the task of se-lecting an appropriate response is seen as cross-lingual information retrieval where the responseutterance ut is the relevant document and thecontextt is treated as a query from different lan-guage.
This model has been applied to simplequestion answering where context is the previousutterance and the training data is manually anno-tated question-answer pairs.
DeVault et al(2011)have proposed to use a multi-class classificationmodel (such as maximum entropy) for estimat-ing P .
Their method restricts the set Upossibleto a set of canonical utterances which representdistinct dialogue acts.
This allows for a limitednumber of classes (|Upossible|) and also maximizesthe number of distinct contexts seen per utterance.This model is also trained from manually anno-tated utterance-context pairs and can additionallyuse manually created utterance paraphrases.Apart from the models discussed above whichhave been mainly applied to dialogue domainssituated in a story context, there has been somework in surface text based dialogue models foropen domains.
Ritter et al(2011) use informa-tion retrieval based and statistical machine trans-lation (SMT) based approaches towards predictingthe next response in Twitter conversations.
AlsoChatbots typically use surface text based process-ing such as string transformations (e.g., AIMLrules (Wallace, 2003)).
Such rules can also belearned from a dialogue corpus (Abu Shawar andAtwell, 2005).
Systems employing SMT or stringtransformation rules are formulating a responseby Generation approach and it can be frequentlyungrammatical or incoherent, unlike the selectionapproach which will always pick something thatsomeone has once said (even though it might beinappropriate in the current context).3 Dialogue Models3.1 Nearest ContextIn previous work (Gandhe and Traum, 2007a), wemodeled P as,P (ui|contextq) ?
Sim(contexti, contextq)where contexti is the context in which utteranceui was seen in training corpus and Sim is con-text similarity in a customized vector-space model.The model restricts the set of possible responseutterances (Upossible) to the set of utterances ob-served in the training data (Utrain).
The contextis approximated using the previous two utterances(one from each speaker).
This model does not usethe contents of the utterance ui itself.3.2 Cross-lingual Relevance ModelLeuski et al(2006) model P as a cross-lingual rel-evance model.
This model takes into account thecontent of the utterance ui as well as the content ofthe context.
It does not impose any restriction onUpossible, but in practice it is restricted to the setof utterances in the training data.
The model al-lows the context to be composed of multiple fields,each with its own weight.
This allows us to ex-tend the model where the context is approximatedby the previous two utterances.
The weights needto be learned using a held-out development set,which presents a challenge in the case of multiplefields (possible if we add more information stateannotations), modest amounts of training data and252non-availability of an automatic and reliable esti-mate of the model?s performance.
Here, for thefirst time, we apply this model to automaticallyextracted pairs of utterance-context and evaluateit.
For our model we used the implementation thatis available as a part of NPCEditor (Leuski andTraum, 2011) and manually set the field weightscorresponding to the two previous utterances to beequal (0.5).3.3 PerceptronAs discussed earlier, the task of selecting the mostappropriate response can be viewed as multi-classclassification.
But there are a couple of issues.First, since we operate at the surface text level,each unique response utterance will be labeled asa separate class.
The number of classes is thenumber of unique utterances seen in the trainingset, which is relatively large.
As the training datagrows, the number of classes will increase.
Sec-ond, there are very few examples (on average asingle example) per class.
We need a classifier thatcan overcome these issues.The perceptron algorithm and its variants ?voted perceptron and averaged perceptron arewell known classification models (Freund andSchapire, 1999).
They have been extended for usein various natural language processing tasks suchas part-of-speech tagging (Collins, 2002), pars-ing (Collins, 2004) and discriminative languagemodeling (Roark et al 2007).
Here we use theaveraged perceptron model for mapping from dia-logue context to an appropriate response utterance.Collins (2002) outlines the following four com-ponents of a perceptron model:?
The training data.
In our case it is a set of au-tomatically extracted utterance-context pairs{.
.
.
, ?ui, contexti?, .
.
.}?
A function GEN(context) that enumerates aset of all possible outputs (response utter-ances) for any possible input (dialogue con-text)?
A feature extraction function ?
:?u, context?
?
Rd that is defined overall possible pairings of response utterancesand dialogue contexts.
d is the total numberof possible features.?
A parameter vector ??
?
RdUsing such a perceptron model, the most appropri-ate response utterance (ut) for the given dialoguecontext (contextt) is given by,uq = argmaxui?GEN(context)?
(ui, contextq) ?
?
?Algorithm 1 Perceptron Training AlgorithmInitialize: t?
0 ; ?
?0 ?
0for iter = 1 to MAX ITER dofor i = 1 to N dori ?
argmaxu?GEN(contexti) ?
(u, contexti) ?
?
?tif ri 6= ui then?
?t+1 ?
?
?t + ?
(ui, contexti) ??
(ri, contexti)else?
?t+1 ?
?
?tend ift?
t+ 1end forend forreturn ???
(?t ?
?t)/(MAX ITER?N)The parameter vector ??
is trained using thetraining algorithm described in Algorithm 1.
Thealgorithm goes through the training data one in-stance at a time.
For every training instance, itcomputes the best response utterance (ri) for thecontext based on its current estimate of the param-eter vector ??t.
The algorithm changes the param-eter vector only if it makes an error (ri 6= ui).
Theupdate drives the parameter vector away from theerror (ri) and towards the correct output (ui).
Thefinal parameter vector ??
is an average of all the in-termediate ?
?t values.
The averaging of parametervectors avoids overfitting.The feature extraction function ?
can list anyarbitrary features from the pair ?u, context?.
Weconsider information state annotations (ISt) alongwith the surface text corresponding to the previoustwo turns.
The features could also include scorescomputed from other models, such as those pre-sented in sections 3.1 and 3.2.
Figure 1 illustratesan example context and utterance, and several fea-tures.
We examine several sets of features, Surfacetext based features (?S), Retrieval model basedfeatures (?R), and Topic based features (?T ).Surface text based features (?S) are the fea-tures extracted from the surface text of the previ-ous utterances in the dialogue context (contextj)and the response utterance (ui).
?S(d)(ux, uy) ex-tracts surface text features from two utterances ?
aresponse utterance (ux) and an utterance (uy) fromthe context that is (d) utterances away.
There arefour types of features we extract:253?
common term(d,w) features indicate thenumber of times a wordw appears in both theutterances.
The total number of possible fea-tures is O(|V |) and we select a small subsetof words (Selected common(d)) from thevocabulary.?
The common term count(d) feature indi-cates the number of words that appear in bothutterances.?
The unique common term count(d) fea-ture indicates the number of unique wordsthat appear in both utterances.?
cross term(d,wx, wy) features indicate thenumber of times the word wx appears in theutterance ux and the word wy appears in theutterance uy.
The total possible number ofsuch cross features is very large (O(|V |2)),where |V | is the utterance vocabulary size.In order to keep the training tractable andavoid overfitting, we select a small subset ofcross features (Selected cross(d)) from allpossible features.In this model, we perform feature selection byselecting the subsets Selected cross(d) andSelected common(d).
The training algorithm re-quires evaluating the feature extraction (?S) func-tion for all possible pairings of response utterancesand contexts.
One simple feature selection crite-rion is to allow the features only appearing in truepairings of response utterance and context (i.e.features from ?S(?ui, contextj?)
?i = j).
Thesubset Selected common(d) for common termfeatures is selected by extracting features fromonly such true pairings.For selecting cross term(d,wx, wy) featureswe use only true pairings but we need toreduce this subset even further.
We im-pose additional constraints based on the col-lection frequency of lexical events such as,cf(wx) > thresholdx, cf(wy) > thresholdy,cf(?wx, wy?)
> thresholdxy.
Further reductionin size of the selected subset of cross term fea-tures is achieved by ranking the features using asuitable ranking function and choosing the top nfeatures.
In this model, we rank the cross termfeatures based on pointwise mutual-informationpmi(?wx, wy?)
given by,log p(?wx, wy?
)p(wx)p(wy)= log(#?wx,wy?#??,??)(#?wx,??#??,??)?(#??,wy?#??,??
)Summing up, ?S(d)(ux, uy) ={cross term(d,wx, wy) : wx ?
ux?wy ?
uy ?
?wx, wy?
?
Selected cross(d)}?
{common term(d,w) : w ?
ux ?w ?
uy ?w ?
Selected common(d)}?
{common term count(d)}?
{unique common term count(d)}Retrieval model based features (?R) arethe scores computed in a fashion similar tothe Nearest Context model.
Sim(ux, uy) isa cosine similarity function for tf-idf weightedvector space representations of utterances andSim(contexta, contextb) is the same functionfrom Nearest Context model.
We define three fea-tures,?
retrieval score =|L|maxk=1Sim(contextj , contextk) ?
Sim(ui, uk)?
context sim@best utt match =Sim(contextj , contextb)where, b = |L|argmaxk=1Sim(ui, uk)?
utt sim@best context match = Sim(ui, ub)where, b = |L|argmaxk=1Sim(contextj , contextk)?R(?ui, contextj?)
= {retrieval score,context sim@best utt match,utt sim@best context match}Topic based feature (?T ) tracks the topic sim-ilarity between the topic of the dialogue contextand the response utterance.
A topic is markedas mentioned if a set of keywords triggering thattopic have been previously mentioned in the dia-logue.
Each information state (IS) consists of atopic signature which can be viewed as a booleanvector representing mentions of topics.
?T (?ui, contextj?)
= {topic similarity}topic similarity = cosine(ISi, ISj)where, ISi is the topic and is part of contextiwhich is the context associated with the utteranceui.The perceptron model presented here allowsnovel combinations of resources such as combin-ing surface text transcripts with information stateannotations for tracking topics in the conversa-tion.
As compared to the generative cross-lingualrelevance model approach, the perceptron modelis a discriminative model.
It is also a paramet-ric model and the inference requires linear timewith respect to the size of candidate utterances(|GEN(context)|) and the number of features (|?
?|).Although, computing some of the features them-selves (e.g., ?R features) requires linear time with254...contextj [uj(?2)] Doctor you are the threat i need protection from you[uj(?1)] Captain no noyou do you do not need protection from mei am here to help youuh what i would like to do is move your your clinic to a safer locationand uh give you money and medicine to help build itutterance [ui] Doctor i have no way of moving?S(?ui, contextj?)
= { cross term(?2, ?moving?, ?need?)
= 1,common term(?2, ?i?)
= 1,common term count(?2) = 1, unique common term count(?2) = 1,cross term(?1, ?moving?, ?give?)
= 1,common term(?1, ?i?)
= 1, common term(?1, ?no?)
= 1,common term count(?1) = 2, unique common term count(?1) = 2,retrieval score = 0.198, context sim@best utt match = 0.198,utt sim@best context match = 0,topic similarity = 0.667 }Figure 1: Features extracted from a context (contextj) and a response utterance (ui)respect to the size of the training data.
The per-ceptron model can rank an arbitrary set of utter-ances given a dialogue context.
But some of thefeatures (e.g., topic similarity) require that theutterance ui(ui ?
|GEN(context)|) be associatedwith a known context (contexti).
For all our mod-els we use GEN(context) = Utrain.We have implemented three different vari-ants of the perceptron model based on thechoice of features used.
Perceptron(surface)model uses only surface text features (?
=?S).
The other two models are Percep-tron(surface+retrieval) where ?
= ?S ?
?R andPerceptron(surface+retrieval+topic) where ?
=?S ?
?R ?
?T .Figure 2 shows a schematic representation ofthese models along with the set of resources be-ing used by each model.
The figure also shows therelationships between these models.
The arrowspoint from a less informative model to a more in-formative model and the annotations on these ar-rows indicate the additional information used.4 EvaluationFor the experiments reported in this paper, weused the human-human spoken dialogue corpuscollected for the project SASO-ST (Traum et al2005).
In this scenario, the trainee acts as anArmy Captain negotiating with a simulated doc-Figure 2: A schematic representation of imple-mented unsupervised dialogue models and the re-lationships between the information used by theirranking functions.tor to convince him to move his clinic to anotherlocation.
The corpus is a collection of 23 roleplaydialogues and 13 WoZ dialogues lasting an aver-age of 40 turns (a total of ?
1400 turns and ?
30kwords).We perform a Static Context evalua-tion (Gandhe, 2013).
In Static Context evaluation,all the dialogue models being evaluated receivethe same set of contexts as input.
These dialoguecontexts are extracted from actual in-domain255human-human dialogues and are not affected bythe dialogue model being evaluated.
For everyturn whose role is to be played by the system, wepredict the most appropriate response in place ofthat turn given the dialogue context.Since the goal for virtual humans is to be ashuman-like as possible, a suitable evaluation met-ric is how appropriate or human-like the responsesare for a given dialogue context.
The evaluationreported here employs human judges.
We set up asimple subjective 5-point likert scale for rating ap-propriateness ?
1 being a very inappropriate non-sensical response and 5 being a perfectly appropri-ate response.We built five dialogue models to play the roleof the doctor in SASO-ST domain, viz.
: Near-est Context (section 3.1), Cross-lingual RelevanceModel (section 3.2) and three perceptron models(section 3.3) with different feature sets.
Thesedialogue models are evaluated using 5 in-domainhuman-human dialogues from the training data (2roleplay and 3 WoZ dialogues, referred to as testdialogues).
A dialogue model is trained in a leave-one-out fashion where the training data consists ofall dialogues except the one test dialogue that isbeing evaluated.
A dialogue model trained in thisfashion is then used to predict the most appropri-ate response for every context that appears in thetest dialogue.
This process is repeated for each testdialogue and for each dialogue model being evalu-ated.
In this evaluation setting, the actual responseutterance found in the original human-human dia-logue may not belong to the set of utterances beingranked by the dialogue model.
We also comparethese five dialogue models with two human-levelupper baselines.
Figure 4 in the appendix showssome examples of utterances returned by a coupleof the models.4.1 Human-level Upper BaselinesIn order to establish an upper baseline for human-level performance for the evaluation task, we con-ducted a wizard data collection.
We asked humanvolunteers (wizards) to perform a similar task tothat performed by the dialogue models being eval-uated.
The wizard is presented with a set of ut-terances (Utrain) and is asked to select a subsetfrom these that will be appropriate as a responsefor the presented dialogue context.
Compared tothis, the task of the dialogue model is to selecta single most appropriate response for the givencontext.DeVault et al(2011) carried out a similar wiz-ard data collection but at the dialogue act level,where wizards were asked to select only one re-sponse dialogue act for each dialogue context.Their findings suggest that there are several validresponse dialogue acts for a dialogue context.
Aspecific dialogue act can be realized in severalways at the surface text level.
For these reasonswe believe that for a given dialogue context thereare often several appropriate response utterancesat the surface text level.
In our setting the dia-logue models work at the surface text level andhence the wizards were asked to select a subset ofsurface text utterances that would be appropriateresponses.
Each wizard was asked to select sev-eral (ideally between five and ten, but always atleast one) appropriate responses for each dialoguecontext.
Four wizards participated in this data col-lection with each wizard selecting responses forthe contexts from the same five human-human testdialogues.
The set of utterances to chose from(Utrain) for every test dialogue was built in thesame leave-one-out fashion as used for evaluatingthe implemented dialogue models.There are a total of 89 dialogue contexts wherethe next turn belongs to doctor.
As expected, wiz-ards frequently chose multiple utterances as ap-propriate responses (mean = 7.80, min = 1, max= 25).This data collected from wizards is used to buildtwo human-level upper-baseline models for thetask of selecting a response utterance given a di-alogue context:Wizard Max Voted model returns the responsewhich gets the maximum number of votesfrom the four wizards.
Ties are brokenrandomly.Wizard Random model returns a random utter-ance from the list of all utterances marked asappropriate by one of the wizards.4.2 Comparative Evaluation of ModelsWe performed a static context evaluation usingfour judges for the above-mentioned two human-level baselines (Wizard Random and Wizard MaxVoted) and five dialogue models (Nearest Con-text, Cross-lingual Relevance Model and threeperceptron models), as described in section 3.3.We tune the parameters used for the perceptron256models based on the automatic evaluation met-ric, Weak Agreement (DeVault et al 2011).
Ac-cording to this evaluation metric a response utter-ance is judged as perfectly appropriate (a scoreof 5) if any of the wizards chose this responseutterance for given context and inappropriate (ascore of 0) otherwise.
The Perceptron(surface)model was trained using 30 iterations, the Per-ceptron(surface+retrieval) using 20 iterations,and the Perceptron(surface+retrieval+topic) wastrained using 25 iterations.
For all perceptronmodels we used thresholdx = thresholdy =thresholdxy = 3.For a comparative evaluation of dialogue mod-els, we need an evaluation setup where judgescould see the complete dialogue context alongwith the response utterances generated by the di-alogue models to be evaluated.
In this setup, weshow all the response utterances next to each otherfor easy comparison and we do not show the ac-tual response utterance that was encountered inthe original human-human dialogue.
We built aweb interface for collecting appropriateness rat-ings that addresses the above requirements.
Fig-ure 3 shows the web interface used by the fourjudges to evaluate the appropriateness of responseutterances for given dialogue context.
The appro-priateness was rated on the same scale of 1 to 5.The original human-human dialogue (roleplay orWoZ) is shown on the left hand side and the re-sponse utterances from different dialogue modelsare shown on the right hand side.
In cases wheredifferent dialogue models produce the same sur-face text response only one candidate surface textis shown to judge.
Once the judge has rated all thecandidate responses they can proceed to the nextdialogue context.
This setting allows for compar-ative evaluation of different dialogue models.
Thepresentation order of responses from different di-alogue models is randomized.
Two of the judgesalso performed the role of the wizards in our wiz-ard data collection as outlined in section 4.1, butthe wizard data collection and the evaluation taskswere separated by a period of over 3 months.Table 1 shows the results of our compara-tive evaluation for each judge and averaged overall judges.
We also computed inter-rater agree-ment for individual ratings for all response ut-terances using Krippendorff?s ?
(Krippendorff,2004).
There were a total of n = 397 distinctresponse utterances that were judged by the eval-uators.
The Krippendorff?s ?
for all four judgeswas 0.425 and it ranges from 0.359 to 0.495 fordifferent subsets of judges.
The value of ?
indi-cates that the inter-rater agreement is substantiallyabove chance (?
> 0), but indicates a fair amountof disagreement, indicating that judging appropri-ateness is a hard task even for human judges.
Al-though there is low inter-rater agreement at theindividual response utterance level there is highagreement at the dialogue model level.
Pearson?scorrelation between the average appropriatenessfor different dialogue models ranges from 0.928to 0.995 for different pairs of judges.We performed a paired Wilcoxon test to checkfor statistically significant differences in differ-ent dialogue models.
Wizard Max Voted is sig-nificantly more appropriate than all other models(p < 0.001).
Wizard Random is significantly moreappropriate than Cross-lingual Relevance Model(p < 0.05) and significantly more appropriatethan the three perceptron models as well as Near-est Context model (p < 0.001).
Cross-lingualRelevance Model is significantly more appropri-ate than Nearest Context (p < 0.01).
All otherdifferences are not statistically significant at the 5percent level.We found that adding topic annotations did nothelp.
This is in contrast with previous observa-tion (Gandhe and Traum, 2007b), where topic in-formation helped when evaluation was performedin Dynamic Context setting.
In Dynamic Contextsetting, the dialogue model is used in an onlinefashion where the response utterances it generatesbecome part of the dialogue contexts with respectto which the subsequent responses are predictedand evaluated.
The topic information ensures sys-tematic progression of dialogue.
But for staticcontext evaluation such help is not required as thedialogue contexts are extracted from human hu-man dialogues and are fixed.5 ConclusionIn this paper we introduced dialogue models thatcan be trained simply from in-domain surfacetext dialogue transcripts.
Some of these modelsalso allow for incorporating additional informa-tion state features such as topics or results of sim-pler models.
We have evaluated the appropriate-ness of responses and have compared these mod-els with two human-level baselines.
Evaluatingresponse appropriateness is highly subjective as257Figure 3: Screenshot of the user interface for static context comparative evaluation of dialogue modelsModel #Utts Avg.
appropriateness Appropriateness(All judges)Judge 1 Judge 2 Judge 3 Judge 4 Avg stddevNearest Context 89 4.12 3.98 3.40 3.53 3.76 1.491Perceptron(surface) 89 3.97 4.11 3.51 3.62 3.80 1.445Perceptron(surface+retrieval)89 4.26 4.12 3.51 3.72 3.90 1.414Perceptron(surface+retrieval+topic)89 4.21 4.09 3.51 3.57 3.85 1.433Cross-lingual RelevanceModel89 4.28 4.31 3.70 3.91 4.05 1.314Wizard Random 89 4.55 4.55 4.03 4.16 4.32 1.153Wizard Max Voted 89 4.76 4.84 4.40 4.52 4.63 0.806Table 1: Offline comparative evaluation of dialogue models.can be seen from the fact that utterances whichreceive more wizard votes (Wizad Max Voted) re-ceive significantly higher appropriateness ratingsthan those which receive fewer votes (Wizard Ran-dom).
The performance of best performing dia-logue models are close to human-level baselines.In future we plan to use larger datasets whichshould be easy, since no additional annotations arerequired for training these dialogue models.AcknowledgmentsThe effort described here has been sponsored bythe U.S. Army.
Any opinions, content or informa-tion presented does not necessarily reflect the posi-tion or the policy of the United States Government,and no official endorsement should be inferred.ReferencesBayan Abu Shawar and Eric Atwell.
2005.
Using cor-pora in machine-learning chatbot systems.
Interna-tional Journal of Corpus Linguistics, 10:489?516.258Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: theory and experi-ments with perceptron algorithms.
In Proceedingsof the ACL-02 conference on Empirical methods innatural language processing - Volume 10, EMNLP?02, pages 1?8, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Michael Collins, 2004.
Parameter estimation for sta-tistical parsing models: theory and practice ofdistribution-free methods, pages 19?55.
KluwerAcademic Publishers, Norwell, MA, USA.David DeVault, Anton Leuski, and Kenji Sagae.
2011.Toward learning and evaluation of dialogue policieswith text examples.
In Proceedings of the SIGDIAL2011 Conference, pages 39?48, Portland, Oregon,June.
Association for Computational Linguistics.Yoav Freund and Robert E. Schapire.
1999.
Largemargin classification using the perceptron algorithm.Mach.
Learn., 37:277?296, December.Sudeep Gandhe and David Traum.
2007a.
Creatingspoken dialogue characters from corpora without an-notations.
In Proceedings of Interspeech-07.Sudeep Gandhe and David Traum.
2007b.
First stepstowards dialogue modeling from an un-annotatedhuman-human corpus.
In 5th Workshop on knowl-edge and reasoning in practical dialogue systems,Hyderabad, India.Sudeep Gandhe and David Traum.
2010.
I?ve said itbefore, and i?ll say it again: an empirical investiga-tion of the upper bound of the selection approach todialogue.
In Proceedings of the 11th Annual Meet-ing of the Special Interest Group on Discourse andDialogue, SIGDIAL ?10, pages 245?248, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Sudeep Gandhe.
2013.
Rapid prototyping and evalu-ation of dialogue systems for virtual humans.
Ph.D.thesis, University of Southern California.J.
J. Godfrey, E. C. Holliman, and J. McDaniel.
1992.Switchboard: Telephone speech corpus for researchand development.
In Proc.
of ICASSP-92, pages517?520.P.
A. Heeman and J. Allen.
1994.
The TRAINS 93 di-alogues.
TRAINS Technical Note 94-2, Departmentof Computer Science, University of Rochester.Klaus Krippendorff.
2004.
Content Analysis, An Intro-duction to Its Methodology 2nd Edition.
Sage Pub-lications.Anton Leuski and David Traum.
2008.
A statisticalapproach for text processing in virtual humans.
InProccedings of 26th Army Science Conference.Anton Leuski and David Traum.
2011.
Npceditor:Creating virtual human dialogue using informationretrieval techniques.
AI Magazine, 32(2):42?56.Anton Leuski, Ronakkumar Patel, David Traum, andBrandon Kennedy.
2006.
Building effective ques-tion answering characters.
In Proceedings of the7th SIGdial Workshop on Discourse and Dialogue,pages 18?27, Sydney, Australia, July.
Associationfor Computational Linguistics.Jeff Rickel and W. Lewis Johnson.
1999.
Virtual hu-mans for team training in virtual reality.
In Proceed-ings of the Ninth International Conference on Artifi-cial Intelligence in Education, pages 578?585.
IOSPress.Alan Ritter, Colin Cherry, and William B. Dolan.
2011.Data-driven response generation in social media.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing, EMNLP ?11,pages 583?593, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Brian Roark, Murat Saraclar, and Michael Collins.2007.
Discriminative n-gram language modeling.Comput.
Speech Lang., 21:373?392, April.A.
J. Stent.
2000.
The monroe corpus.
TechnicalReport 728, Computer Science Dept.
University ofRochester.David Traum, William Swartout, Jonathan Gratch, andStacy Marsella.
2005.
Virtual humans for non-team interaction training.
In AAMAS-05 Workshopon Creating Bonds with Humanoids, July.Richard Wallace.
2003.
AIML Overview.
ALICE A. I.Foundation.259Original Human-Human Dialogue Model responseCaptain hello how are you doing sirDoctor what do you wanti have patients waiting for mehellowhat can i do for you mrCaptain sir i come by to offer you some assistance ah onbehalf of my commanderah we want to try to support you in this hospitaland ah improving the health and welfare of thecivilians that you have under your careDoctor well this is your opinion i see captainis this a is this a suggestion from yourcommanderCaptain i am sorry sircould you say againDoctor well this is your opinion you are the threati need protection from you(a) Perceptron(surface)Original Human-Human Dialogue Model responseCaptain hello doctor perezDoctor hello hellowhat was your namecaptain dezois very nice to meet youi am sorry but i am very busy todayso i only have a limited amount of timewhat can i help you withCaptain i am captain xxDoctor sodo you need helpokwhat do you want i have patients waiting for meCaptain yesi have a very urgent matter to discusswith youDoctor are you injured pl please captaini i am sorry to cut you offbut i really must uh goi was in the middle of examining a patient(b) Cross-lingual Relevance ModelFigure 4: Example interaction for the dialogue models in static context setting.
The second columnshows the original human-human dialogue and the third column shows the dialogue model?s responsefor the corresponding system turn.260
