Simple but effective feedback generation to tutor abstract problem solvingXin Lu, Barbara Di Eugenio, Stellan Ohlsson and Davide FossatiUniversity of Illinois at ChicagoChicago, IL 60607, USAxinlu@northsideinc.combdieugen,stellan,dfossa1@uic.eduAbstractTo generate natural language feedback for anintelligent tutoring system, we developed asimple planning model with a distinguishingfeature: its plan operators are derived auto-matically, on the basis of the association rulesmined from our tutorial dialog corpus.
Auto-matically mined rules are also used for real-ization.
We evaluated 5 different versions ofa system that tutors on an abstract sequencelearning task.
The version that uses our plan-ning framework is significantly more effectivethan the other four versions.
We compared thisversion to the human tutors we employed inour tutorial dialogs, with intriguing results.1 IntroductionIntelligent Tutoring Systems (ITSs) are softwaresystems that provide individualized instruction, likehuman tutors do in one-on-one tutoring sessions.Whereas ITSs have been shown to be effective inengendering learning gains, they still are not equiv-alent to human tutors.
Hence, many researchersare exploring Natural Language (NL) as the key tobridging the gap between human tutors and currentITSs.
A few results are now available, that show thatITS with relatively sophisticated language interfacesare more effective than some other competitive con-dition (Graesser et al, 2004; Litman et al, 2006;Evens and Michael, 2006; Kumar et al, 2006; Van-Lehn et al, 2007; Di Eugenio et al, 2008).
Ascer-taining which specific features of the NL interactionare responsible for learning still remains an open re-search question.In our experiments, we contrasted the richnesswith which human tutors respond to student ac-tions with poorer forms of providing feedback, e.g.only graphical.
Our study starts exploring the rolethat positive feedback plays in tutoring and in ITSs.While it has long been observed that most tutors tendto avoid direct negative feedback, e.g.
(Fox, 1993;Moore et al, 2004), ITSs mostly provide negativefeedback, as they react to student errors.In this paper, we will first briefly describe our tu-torial dialog collection.
We will then present theplanning architecture that underlies our feedbackgenerator.
Even if our ITS does not currently al-low for student input, our generation architecture isinspired by state-of-the art tutorial dialog manage-ment (Freedman, 2000; Jordan et al, 2001; Zinn etal., 2002).
One limitation of these approaches is thatplan operators are difficult to maintain and extend,partly because they are manually defined and tuned.Crucially, our plan operators are automatically de-rived via the association rules mined from our cor-pus.
Finally, we will devote a substantial amountof space to evaluation.
Our work is among the firstto show not only that a more sophisticated languageinterface results in more learning, but that it favor-ably compares with human tutors.
Full details onour work can be found in (Lu, 2007).2 Task and curriculumOur domain concerns extrapolating letter patterns,such as inferring EFMGHM, given the patternABMCDM and the new initial letter E. This task isused in cognitive science to investigate human in-formation processing (Kotovsky and Simon, 1973;Reed and Johnson, 1994; Nokes and Ohlsson, 2005).The curriculum we designed consists of 13 patternsof increasing length and difficulty; it was used un-changed in both our human data collection and ITSexperiments.
The curriculum is followed by two104Tutor MovesAnswer student?s questionsEvaluate student?s actionsSummarize what done so farPrompt student into activityDiagnose what student is doingInstructDemonstrate how to solve (portions of) problemSupport ?
Encourage studentConversation ?
Acknowledgments, small talkStudent MovesQuestionExplain what student said or didReflect ?
Evaluate own understandingAnswer tutor?s questionAction Response ?
Perform non-linguistic action(e.g.
write letter down)Complete tutor?s utteranceConversation ?
Acknowledgments, small talkTable 1: Tutor and student movespost-test problems, each 15 letter long: subjects (allfrom the psychology subject pool) have n trials toextrapolate each pattern, always starting from a dif-ferent letter (n = 6 in the human conditions, andn = 10 in the ITS conditions).
While the ear-lier example was kept simple for illustrative pur-poses, our patterns become very complex.
Start-ing e.g.
from the letter L, we invite the reader toextrapolate problem 9 in our curriculum: BDDFF-FCCEEGGGC, or the second test problem: ACZD-BYYDFXGEWWGI.13 Human dialogsThree tutors ?
one expert, one novice, and one(the lecturer) experienced in teaching, but not inone-on-one tutoring ?
were videotaped as they in-teracted with 11 subjects each.2 A repeated mea-sures ANOVA, followed by post-hoc tests, revealedthat students with the expert tutor performed signifi-cantly better than the students with the other two tu-tors on both test problems (p < 0.05 in both cases).36 dialog excerpts were transcribed, taken from1The solutions are, respectively: LNNPPPMMOOQQQM,and LNZOMYYOQXRPWWRT .2One goal of ours was to ascertain whether expert tutors areindeed more effective than non-expert tutors, not at all a fore-gone conclusion since very few studies have contrasted expertand non expert tutors, e.g.
(Glass et al, 1999).18 different subjects (6 per tutor), for a total ofabout 2600 tutor utterances and 660 student ut-terances (transcription guidelines were taken from(MacWhinney, 2000)).
For each subject, these twodialog excerpts cover the whole interaction with thetutor on one easy and one difficult problem (# 2 and9 respectively).
2 groups of 2 coders each, annotatedhalf of the transcripts each, with dialogue moves.Our move inventory comprises 9 tutor moves and7 student moves, as listed in Table 1.3 Table 2presents an annotated fragment from one of the di-alogues with the expert tutor.
Kappa measures forintercoder agreement had values in the followingranges, according to the scale in (Rietveld and vanHout, 1993): for tutor moves, from moderate (0.4for Support) to excellent (0.82 for Prompt); for stu-dent moves, from substantial (0.64 for Explanation)to excellent (0.82 for Question, 0.97 for ActionRe-sponse).
Whereas some of these Kappa measuresare lower than what we had strived for, we decidedto nonetheless use the move inventory in its entirety,after the coders reconciled their codings.
In fact, ourultimate evaluation measure concerns learning, andindeed the ITS version that uses that entire move in-ventory engenders the most learning.
Please see (Luet al, 2007) for a detailed analysis of these dialogsand for a discussion of differences among the tutorsin terms of tutor and student moves.The transcripts were further annotated by onecoder for tutor attitude (whether the tutor agreeswith the student?s response ?
positive, negative, neu-tral), for correctness of student move and for stu-dent confidence (positive, negative, neutral).
Stu-dent hesitation time (long, medium, short) was es-timated by the transcribers.
Additionally, we an-notated for the problem features under discussion.Of the 8 possible relationships between letters, mostrelevant to the examples discussed in this paper areforward, backward, progression and marker.
E.g.
inABMCDM, M functions as chunk marker, and thesequence moves forward by one step, both withinone chunk and across chunks.
Within and across aretwo out of 4 relationship scopes, which encode thecoverage of a particular relationship within the se-quence.3There is no explicit tutor question move because we focuson the goal of a tutor?s question, either prompt or diagnose.105Line Utterances Annotation38 Tutor: how?d you actually get the n in the first place?
Diagnose39 Student: from here I count from c to g and then just from n to r. Answer40 Tutor: okay so do the c to g. Prompt41 Tutor: do it out loud so I can hear you do it.
Prompt42 Student: c d e f. Explain43 Student: so it?s three spaces.
Answer44 Tutor: okay so it?s three spaces in between.
SummarizeTable 2: An annotated fragment from a dialogue with the expert tutor4 Learning rules to provide feedbackOnce the corpus was annotated, we mined the ex-pert tutor portion via Classification based on Associ-ations (CBA) (Liu et al, 1998).
CBA generates un-derstandable rules and has been effectively appliedto various domains.
CBA finds all rules that existin the data, which is especially important for smalltraining sets such as ours.To modularize what the rules should learn, we de-composed what the tutor should do into two com-ponents pertaining to content: letter relationship andrelationship scope; and two components pertainingto how to deliver that content: tutor move and tutorattitude.
Hence, we derived 4 sets of tutorial rules.Features used in the rules are those annotated on thetutoring dialogs, plus the student?s Knowledge State(KS) on each type of letter relationship rel, com-puted as follows:KS(rel) = bp?
0.5 + wt?
5c (1)p is the number of partially correct student inputs, wis the number of wrong student inputs and t is the to-tal number of student inputs (?inputs?
here are onlythose relevant to the relationship rel, as logged bythe ITS from the beginning of the session).
KS(rel)ranges from 0 to 5.
The higher the value, the worsethe performance on rel.
The scale of 5 was chosento result in just enough values for KS(rel) to be use-ful for classification.We ran experiments with different lengths of dia-log history, but using only the last utterance gave usthe best results.
Three of the four rule sets have ac-curacies between 88 and 90% (results are based on6-way cross-validation, and the cardinality of eachset of rules is in the low hundreds.
).
Whereas thetutor move rule set only has 57% accuracy, as forsome of the low Kappa values mentioned earlier, ourrelation-marker = No, relation-forward = Yes,student-move = ActionResponse?
relation-forward = Yes(Confidence = 100%, Support = 4.396%)correctness = wrong, scope-within = No,KS(backward) = 0, relation-forward = Yes?
tutor-move = Summarize(Confidence = 100%, Support = 6.983%)correctness = wrong, relation-forward = Yes,KS(forward) = 1, hesitation = no?
tutor-attitude = negative(Confidence = 100%, Support = 1.130%)Figure 1: Example Tutorial Rulesultimate evaluation measure is that the NL feedbackbased on these rules does improve learning.Figure 1 shows three example rules, for choosingrelationship, move and attitude respectively ?
we?lldiscuss two of them.
The first rule predicts that theITS will continue focusing on the forward relation,if it was focusing on forward and not on marker, andthe student just input something.
The second rulechooses the summarize move if the student made amistake, the ITS was focusing on forward but not onrelationships within chunks, and the student showedperfect knowledge of backward.Two strength measurements are associated witheach rule X ?
y.
A rule holds with confidenceconf if conf% of cases that containX also contain y;and with support sup if sup% of cases contain X ory.
Rules are ordered, with confidence having prece-dence over support.
Ties over confidence are solvedvia support; any remaining ties are solved accordingto the order rules were generated.106For each Tut-Move-Rule TMRi,k whose Left-Hand Side LHS matches ISi do:1.
Create and Populate New Plan pi,k:(a) preconditions = ISi; actions = tutor move from TMRi,k; strength = confidence and support from TMRi,k(b) Fill remaining slots in pi,k:i. contents = relationship ?
scope (from highest ranked rules that match ISi from relationship and scoperule sets);ii.
modifiers = attitude (from highest ranked rule that matches ISi from tutor attitude rule set)2.
Augment Plan: do the following n times :(a) make copy of ISi and name it ISi+1;(b) change agent to ?tutor?
;(c) change corresponding elements in ISi+1 to move, attitude, letter relationship and scope from pi,k;(d) from the two rule sets for tutor move and tutor attitude, retrieve highest ranked rules that match ISi+1,TMRi+1,j and TARi+1,j(e) add to actions tutor move from TMRi+1,j ; add to modifiers tutor attitude from TARi+1,jFigure 2: Plan generation5 From rules to plansFor our task of extrapolating abstract sequences, webuilt a model-tracing ITS by means of the TutoringDevelopment Kit (TDK) (Koedinger et al, 2003).Model-tracing ITSs codify cognitive skills via pro-duction rules.
The student?s solution is monitoredby rules that fire according to the underlying model.When the student steps differ from that model, anerror is recognized.
A portion of the student inter-face of the ITS is shown in Figure 4a.
It mainly in-cludes two rows, one showing the Example Pattern,the other for the student to input the New Patternextrapolated starting with the letter in the first cell.In model-tracing ITSs, production rules providethe capability to generate simple template-basedmessages.
We developed a more sophisticated NLfeedback generator consisting of three major mod-ules: update, planning and feedback realization.The update module maintains the context, rep-resented by the Information State (IS) (Larssonand Traum, 2000), which captures the overall dia-log context and interfaces with external knowledgesources (e.g., curriculum, tutorial rules) and the pro-duction rule system.
As the student performs a newaction, the IS is updated.
The planning module gen-erates or revises the system plan and selects the nexttutoring move based on the newly updated IS.
Atlast the feedback realization module transforms thismove into NL feedback.The planning module consists of three compo-nents, plan generation, plan selection and plan mon-itoring.
A plan includes an ordered collection of tu-toring moves meant to help the student correctly filla single cell.
The structure of our plans is shown inFigure 3.Plan generation generates a plan set which con-tains one plan for each tutor move rule that matchesthe current ISi.
Each of these plans is augmented atplan generation time by ?simulating?
the next ISi+1that would result if the move is executed but its ef-fects are not achieved.
The algorithm is sketchedin Figure 2.
The planner iterates through the tutormove rule set.4 Recall that our four rule sets are to-tally ordered.
Also, note that each rule set contains adefault rule that fires when no rule matches the cur-rent ISi.
In Step 1b, at every iteration only the rulesthat have not been checked yet from those three rulesets are considered.
In Step 2, n is set to 3, i.e., eachplan contains 3 additional moves and correspondingattitudes, which will provide hints when no responsefrom the student occurs.
Three hints plus one orig-inal move makes 4, which is the average number ofmoves in one turn of the expert tutor.An example plan is shown in Figure 3.
It is gen-erated in reaction to the mistake in Figure 4a, and by4Since there is no language input, rules which include stu-dent moves other than ActionResponse in their LHS will neverbe activated.
Additionally, we recast tutor answers as confirmmoves, since students cannot ask questions.107Preconditions (same as the IS in Figure 4b)Effects student?s input = WContents relationship = forwardscope = acrossActions summarize, evaluate, prompt,summarizeModifiers negative, negative, neutral, neutralStrength conf = 100%, sup = 6.983%Figure 3: An Example Planfiring, among others, the rules in Figure 1.
The IS inFigure 4b reflects some of the history of this interac-tion (in the slots Relationships, Scopes and KS), andas such corresponds to the situation depicted in Fig-ure 4a in a specific context (this plan was generatedin one of our user experiments).The plan selection component retrieves the high-est ranked plan in the newly generated plan set, se-lects a template for each tutoring move in its ?Ac-tions?
slot and puts each tutoring move onto the di-alog move (DM) stack.
Earlier we mentioned thatrules are totally ordered according to confidence,then support and finally rule generation order.
Whena plan set contains more than one plan, plans arealso totally ordered, since they inherit strength mea-surements from the rule that engenders the first tutormove in the Actions slot.After the student receives the message whichrealizes the top tutoring move in the DM stack,plan monitoring checks whether its intended effectshave been obtained.
If the effects have not been ob-tained, and the student?s input is unchanged, the nextmove from the DM stack will be executed to pro-vide the student with hint messages until either thestudent?s input changes or the DM stack becomesempty.
If the DM stack becomes empty, the nextplan is selected from the original plan set and thetutoring moves within that plan are pushed onto theDM stack.
Whenever the student?s input changes,or after every plan in the plan set has been selected,control returns to plan generation.The realization module.
A tutor move is pushedonto the DM stack by plan selection together witha template to realize it.
50 templates were writ-ten manually upon inspection of the expert tutor di-alogs.
Since several templates can realize each tutormove, we used CBA to learn rules to choose amongtemplates.
Features used to learn when to use each(a) A Student Action in Problem 41.
Agent: student (producer of current move);2.
Relationships: forward, progress in length3.
Scopes: across (for ?forward?
), within (for?progress in length?);4.
Agent?s move: action response;5.
Agent?s attitude: positive (since student showsno hesitation before inputting letter);6.
Correctness: wrong (correct letter is W);7.
Student?s input: X;8.
Student?s selection: 4th cell in New Pattern row;9.
Hesitation time: no;10.
Student?s knowledge state (KS): 1 (on ?for-ward?
), 3 (on ?progress in length?).
(b) The corresponding ISFigure 4: A snapshot of an ITS interactiontemplate also include the tutor attitude.
For the firstSummarize move in the plan in Figure 3, given theIS in Figure 4b, the rule in Fig.
5 will fire (tutor at-titude does not affect this specific rule).
As a result,the following feedback message is generated: ?FromV to X, you are going forward 2 in the alphabet.
?6 EvaluationTo demonstrate the utility of our feedback genera-tor, we developed five different versions of our ITS,named according to how feedback is generated:1.
No feedback: The ITS only provides the basicinterface, so that subjects can practice solvingthe 13 problems in the curriculum, but does notprovide any kind of feedback.2.
Color only: The ITS provides graphic feed-back by turning the input green if it is corrector red if it is wrong.3.
Negative: In addition to the color feedback, the108scope-within = No, relation-marker = No,relation-forward = Yes, move= Summarize ?template = TPL11[where TPL11: From ?<reference-pattern>?to ?<input>?, you are going <input-relation><input-number> in the alphabet.
]Figure 5: Example Realization RuleITS provides feedback messages when the in-put is wrong.4.
Positive: In addition to the color feedback, theITS provides feedback messages when the in-put is correct.5.
Model: In addition to the color feedback, theITS provides feedback messages generated bythe feedback generator just described.Feedback is given for each input letter.
Positiveand negative verbal feedback messages are givenout whenever the student?s input is correct or incor-rect, respectively.
Positive feedback messages con-firm the correct input and explain the relationshipswhich this input is involved in.
Negative feedbackmessages flag the incorrect input and deliver hints.The feedback messages for the ?negative?
and ?pos-itive?
versions were developed earlier in the project,to avoid repetitions and inspired by the expert tu-tor?s language but before we performed any anno-tation and mining.
They are directly generated byTDK production rules.Although in reality positive and negative feedbackare both present in tutoring sessions, one study forthe letter pattern task shows that positive/negativefeedback, given independently, perform differentfunctions (Corrigan-Halpern and Ohlsson, 2002).
Inaddition, our negative condition is meant to embodythe ?classical?
model-tracing ITS, that only reactsto student errors.
Hence, in our experiments, weelected to keep these two types of feedback separate,other than in the ?model?
version of the ITS.To evaluate the five versions of the ITS, we ran abetween-subjects study in which each group of sub-jects interacted with one version of the system.
Agroup of control subjects took the post-test with notraining at all but only read a short description of theScoreCondition Prob 1 Prob 2 Total0 Control 36.50 32.84 69.341 No feedback 58.21 75.27 133.482 Color only 68.32 66.30 134.623 Negative 70.33 66.06 141.834 Positive 75.06 79.00 154.065 Model 91.95 101.76 193.71Table 3: Average Post-test Scores of the ITSdomain.5 Subjects were trained to solve the same13 problems in the curriculum that were used in thehuman tutoring condition.
They also did the samepost-test (2 problems, each pattern 15 letters long).For each post-test problem, each subject had 10 tri-als, where each trial started with a new letter.6.1 ResultsTable 3 reports the average post-test scores of thesix groups of subjects, corresponding to the five ver-sions of the ITS and the control condition.
Perfor-mance on each problem is measured by the numberof correct letters out of a total of 150 letters (15 let-ters by 10 trials); hence, cumulative post-test score,is the number of correct letters out of 300 possible.A note before we proceed.
In ITS research it iscommon to administer the same test before (pre-test)and after treatment (post-test), but we only have thepost-test.
The pre/post-test paradigm is used for tworeasons.
First, for evaluation proper, to gauge learn-ing gains.
Second, to verify that the groups have thesame level of pre-tutoring ability, as shown when thepre-tests of the different groups are statistically in-distinguishable, and hence, that they can be rightlycompared.
Even without a pre-test we can assessthis.
An ANOVA on ?time spent on the first 3 prob-lems?
revealed no significant differences across thedifferent groups.
Since time spent on the first 3 prob-lems is highly correlated with post-test score (multi-ple regression, p < 0.03), this provides indirect evi-dence that all subjects before treatment have equiva-lent ability for this task.
Hence, we can trust that ourevaluation, in terms of absolute scores, does revealdifferences between conditions.Our main findings are based on one-wayANOVAs, followed by Tukey post-hoc tests:5The number of subjects in each condition varies from 32 to38.
Groups differ in size because of technical problems.109?
A main effect of ITS (p ?
0.05).
Subjects whointeracted with any version of the ITS had sig-nificantly higher total post-test scores than sub-jects in the control condition.?
A main effect of modeled feedback (p< 0.05).Subjects who interacted with the ?model?
ver-sion of the ITS had significantly higher totalpost-test scores than control subjects, and sub-jects with any other version of the ITS.?
No other effects.
Subjects trained by the threeversions ?color only?, ?negative?, ?positive?,did not have significantly higher total post-testscores than subjects with the ?no feedback?version; neither did subjects trained by the twoversions ?negative?, ?positive?, wrt subjectswith the ?color-only?
version.If we examine individual problems, the same pat-tern of results hold, other than, interestingly, themodel and positive versions are not significantly dif-ferent any more.
As customary, we also analyze ef-fect sizes , i.e., how much more subjects learn withthe ?model?
ITS in comparison to the other condi-tions.
On the Y axis, Figure 6 shows Cohen?s d, acommon measure of effect size.
Each point repre-sents the difference between the means of the scoresin the ?model?
ITS and in one of the other condi-tion, divided by the standard deviation of either con-dition.
According to (Cohen, 1988), the effect sizesshown in Figure 6 are large as concerns the compari-son with the ?no feedback?, ?color only?
and ?nega-tive?
conditions, and moderate as concerns the ?pos-itive?
condition.6ITSs and Human Tutors.
After we establishedthat, at least cumulatively, the ?model?
ITS is moreeffective than the other ITSs, we wanted to assesshow well the ?model?
ITS fares in comparison tothe expert tutor it is modeled on.
Since in the humandata each post-test problem consists of only 6 trials,the first 6 trials per problem from the ITSs are usedto run this comparison, for a maximum total scoreof 180 (15 letters by 6 trials, by 2 problems).
Fig-ure 7 shows the overall post-test performance of all9 conditions.
The error bars in the figure representthe standard deviations.6A very large effect size with respect to control is not shownin Figure 6.Figure 6: Effect sizes: how much more subjectslearn with the ?model?
ITSFigure 7: Post-test performance ?
all conditionsPaired t-tests between the model ITS and each ofthe human tutors show that:7?
on problem 1, the ?model?
ITS is indistinguish-able from the expert tutor, and is significantlybetter than the novice and the lecturer(p = 0.05 and p = 0.039 respectively);?
on problem 2, the model ITS is significantlyworse than the expert tutor (p = 0.020), andis not different from the other two tutors;?
cumulatively, there are no significant differ-ences between the ?model?
ITS and any of thethree human tutors.7A 9-way ANOVA among all conditions is not appropriate,since in a sense we have two ?super conditions?, human andITS.
It is better to compare the ?model?
ITS to each of the humantutors via t-tests, as a follow-up to the differences highlighted bythe separate analyses on the two ?super conditions?.1107 Discussion and ConclusionsOur results add to the growing body of evidencethat language feedback engenders more learning notonly than simple practice, but also, than less sophis-ticated language feedback.
Importantly, our ?model?ITS appears intriguingly close to our expert tutor ineffectiveness: on post-test problem 1, it is as effec-tive as the expert tutor himself, and significantly bet-ter than the other two tutors, as the expert tutor is.
Itappears our ?model?
ITS does capture at least somefeatures of successful tutoring.As concerns the specific language the ITS gen-erates, we compared different ways of providingverbal feedback.
A subject receives both positiveand negative verbal feedback when interacting withthe ?model?
version, while a subject receives onlyone type of verbal feedback when interacting withthe ?positive?
and ?negative?
versions (recall thatin all these versions including the ?model?
ITS thered/green graphical feedback is provided on everyinput).
While we cannot draw definite conclusionsregarding the functions of positive and negativefeedback, since the ?model?
version provides othertutorial moves beyond positive / negative feedback,we have suggestive evidence that negative feedbackby itself is not as effective.
Additionally, positivefeedback appears to play an important role.
First,the ?model?
and the ?positive?
versions are statisti-cally equivalent when we analyze performance onindividual problems.
Further, in the ?model?
ver-sion, the ratio of positive to negative messages turnsout to be 9 to 1.
In our tutoring dialogs, positivefeedback still outnumbers negative feedback, but bya lower margin, 4 to 1.
We hypothesize that convey-ing a positive attitude in an ITS is perhaps even moreimportant than in human tutoring since a human hasmany more ways of conveying subtle shades of ap-proval and disapproval.From the NLG point of view, we have presenteda simple generation architecture that turns out to berather effective.
Among its clear limitations are thelack of hierarchical planning, and the fact that dif-ferent components of a plan are generated indepen-dently one from the other.
Among its strengths arethat the plan operators are derived automatically viathe rules we mined, both for content planning and,partly, for realization.It clearly remains to be seen whether our NLGframework can easily be ported to other domains?
the issue is not domain dependence, but whethera more complex domain will require some form ofhierarchical planning.
We are now working in thedomain of Computer Science data structures and al-gorithms, where we continue exploring the role ofpositive feedback.
We collected data with two tu-tors in that domain, and there again, we found thatin the human data positive feedback occurs about 8times more often than negative feedback.
We arenow annotating the data to mine it as we did here,and developing the core ITS.AcknowledgmentsThis work was supported by awards N00014-00-1-0640 and N00014-07-1-0040 from the Office ofNaval Research, by Campus Research Board S02and S03 awards from the University of Illinois atChicago, and in part, by awards IIS 0133123 andALT 0536968 from the National Science Founda-tion.ReferencesJ.
Cohen.
1988.
Statistical power analysis for the be-havioral sciences (2nd ed.).
Hillsdale, NJ: LawrenceEarlbaum Associates.Andrew Corrigan-Halpern and Stellan Ohlsson.
2002.Feedback effects in the acquisition of a hierarchicalskill.
In Proceedings of the 24th Annual Conferenceof the Cognitive Science Society.Barbara Di Eugenio, Davide Fossati, Susan Haller, DanYu, and Michael Glass.
2008.
Be brief, and theyshall learn: Generating concise language feedback fora computer tutor.
International Journal of AI in Edu-cation, 18(4).
To appear.Martha W. Evens and Joel A. Michael.
2006.
One-on-one Tutoring by Humans and Machines.
Mahwah, NJ:Lawrence Erlbaum Associates.Barbara A.
Fox.
1993.
The Human Tutorial DialogueProject: Issues in the design of instructional systems.Lawrence Erlbaum Associates, Hillsdale, NJ.Reva K. Freedman.
2000.
Plan-based dialogue manage-ment in a physics tutor.
In Proceedings of the SixthApplied Natural Language Conference, Seattle, WA,May.Michael Glass, Jung Hee Kim, Martha W. Evens, Joel A.Michael, and Allen A. Rovick.
1999.
Novice vs. ex-pert tutors: A comparison of style.
In MAICS-99, Pro-ceedings of the Tenth Midwest AI and Cognitive Sci-ence Conference, pages 43?49, Bloomington, IN.111Arthur C. Graesser, S. Lu, G.T.
Jackson, H. Mitchell,M.
Ventura, A. Olney, and M.M.
Louwerse.
2004.AutoTutor: A tutor with dialogue in natural language.Behavioral Research Methods, Instruments, and Com-puters, 36:180?193.Pamela Jordan, Carolyn Penstein Rose?, and Kurt Van-Lehn.
2001.
Tools for authoring tutorial dialogueknowledge.
In Proceedings of AI in Education 2001Conference.Kenneth R. Koedinger, Vincent Aleven, and Neil T. Hef-fernan.
2003.
Toward a rapid development environ-ment for cognitive tutors.
In 12th Annual Conferenceon Behavior Representation in Modeling and Simula-tion.K.
Kotovsky and H. Simon.
1973.
Empirical tests of atheory of human acquisition of information-processinganalysis.
British Journal of Psychology, 61:243?257.Rohit Kumar, Carolyn P.
Rose?, Vincent Aleven, Ana Igle-sias, and Allen Robinson.
2006.
Evaluating the Ef-fectiveness of Tutorial Dialogue Instruction in an Ex-ploratory Learning Context.
In Proceedings of theSeventh International Conference on Intelligent Tutor-ing Systems, Jhongli, Taiwan, June.Staffan Larsson and David R. Traum.
2000.
Informationstate and dialogue management in the trindi dialoguemove engine toolkit.
Natural Language Engineering,6(3-4):323?340.Diane J. Litman, Carolyn P.
Rose?, Kate Forbes-Riley,Kurt VanLehn, Dumisizwe Bhembe, and Scott Silli-man.
2006.
Spoken versus typed human and computerdialogue tutoring.
International Journal of ArtificialIntelligence in Education, 16:145?170.Bing Liu, Wynne Hsu, and Yiming Ma.
1998.
Inte-grating classification and association rule mining.
InKnowledge Discovery and Data Mining, pages 80?86,New York, August.Xin Lu, Barbara Di Eugenio, Trina Kershaw, StellanOhlsson, and Andrew Corrigan-Halpern.
2007.
Ex-pert vs. non-expert tutoring: Dialogue moves, in-teraction patterns and multi-utterance turns.
In CI-CLING07, Proceedings of the 8th International Con-ference on Intelligent Text Processing and Computa-tional Linguistics, pages 456?467.
Best Student PaperAward.Xin Lu.
2007.
Expert tutoring and natural languagefeedback in intelligent tutoring systems.
Ph.D. thesis,University of Illinois - Chicago.Brian MacWhinney.
2000.
The CHILDES project.
Toolsfor analyzing talk: Transcription Format and Pro-grams, volume 1.
Lawrence Erlbaum, Mahwah, NJ,third edition.Johanna D. Moore, Kaska Porayska-Pomsta, SebastianVarges, and Claus Zinn.
2004.
Generating TutorialFeedback with Affect.
In FLAIRS04, Proceedings ofthe Seventeenth International Florida Artificial Intel-ligence Research Society Conference.Timothy J. Nokes and Stellan Ohlsson.
2005.
Compar-ing multiple paths to mastery: What is learned?
Cog-nitive Science, 29:769?796.Jonathan Reed and Peder Johnson.
1994.
Assessing im-plicit learning with indirect tests: Determining what islearned about sequence structure.
Journal of Exper-imental Psychology: Learning, Memory, and Cogni-tion, 20(3):585?594.Toni Rietveld and Roeland van Hout.
1993.
StatisticalTechniques for the Study of Language and LanguageBehaviour.
Mouton de Gruyter, Berlin - New York.Kurt VanLehn, Arthur C. Graesser, G. Tanner Jackson,Pamela W. Jordan, Andrew Olney, and Carolyn P.Rose?.
2007.
When are tutorial dialogues more effec-tive than reading?
Cognitive Science, 31(1):3?62.Claus Zinn, Johanna D. Moore, and Mark G. Core.
2002.A 3-tier planning architecture for managing tutorialdialogue.
In ITS 2002, 6th.
Intl.
Conference on In-telligent Tutoring Systems, pages 574?584, Biarritz,France.112
