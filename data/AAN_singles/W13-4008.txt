Proceedings of the SIGDIAL 2013 Conference, pages 61?69,Metz, France, 22-24 August 2013. c?2013 Association for Computational LinguisticsStance Classification in Online Debates by Recognizing Users?
IntentionsSarvesh Ranade, Rajeev Sangal, Radhika MamidiLanguage Technologies Research CentreInternational Institute of Information TechnologyHyderabad, Indiasarvesh.ranade@research.iiit.ac.in, {sangal, radhika.mamidi}@iiit.ac.inAbstractOnline debate forums provide a rich col-lection of differing opinions on vari-ous topics.
In dual-sided debates, userspresent their opinions or judge other?sopinions to support their stance.
In thispaper, we examine the use of users?
inten-tions and debate structure for stance clas-sification of the debate posts.
We proposea domain independent approach to captureusers?
intent at sentence level using its de-pendency parse and sentiWordNet and tobuild the intention structure of the post toidentify its stance.
To aid the task of clas-sification, we define the health of the de-bate structure and show that maximizingits value leads to better stance classifica-tion accuracies.1 IntroductionOnline debate forums provide Internet users a plat-form to discuss popular ideological debates.
De-bate in essence is a method of interactive and rep-resentational arguments.
In an online debate, usersmake assertions with superior content to supporttheir stance.
Factual accuracy and emotional ap-peal are important features used to persuade thereaders.
It is easy to observe that personal opin-ions are important in ideological stance taking(Somasundaran and Wiebe, 2009).
Because of theavailability of Internet resources and time, peopleintelligently use the factual data to support theiropinions.Online debates differ from public debates interms of logical consistency.
In online debates,users assert their opinion towards either side,sometimes ignoring discourse coherence requiredfor logical soundness of the post.
Generally theyuse strong degree of sentiment words including in-sulting or sarcastic remarks for greater emphasisof their point.
Apart from supporting/opposing aside, users make factual statements such as ?Stanlee once said Superman is superior than batman inall areas.?
to strengthen their stance.We collected debate posts from an online sitecalled ?convinceme.net?
which allows users to in-stantiate debates on questions of their choice.
Thedebates are held between two topics.
To gener-alize the debate scenarios, we refer to these top-ics as Topic A and B.
When users participatein the debate, they support their stance by post-ing on the appropriate side, thus self-labeling theirstance.
Users?
stance is determined by the debatetopic they are supporting and we refer to each in-stance of users?
opinion as a post.
Each post canhave multiple utterances which are the smallestdiscourse units.
This site has an option to rebutanother post, thus enabling users to comment onothers?
opinion.A post with most of its utterances supporting adebate topic most likely supports that topic.
Thisshows that users?
intentions play an important rolein supporting their stance.
In this paper, we em-ploy topic directed sentiment analysis based ap-proach to capture utterance level intentions.
Wehave designed debate specific utterance level in-tentions which denote users?
attitude of support-ing/opposing a specific debate topic or stating aknown fact.Message level intention is denoted by the stanceusers are taking in the debate.
We build posts?
in-tention structure and calculate posts?
debate topicsrelated sentiment scores to classify their stance inideological debates.
Intuitively, posts by same au-thor support same stance and rebutting posts haveopposite stances.
This inter-post information pre-sented by debates?
structure is also used to reviseposts?
stance.
As mentioned earlier, we use the de-bate data collected from ?convinceme.net?
to eval-uate our approach on stance classification task andit beats baseline systems and a previous approach61by significant margin and achieves overall accu-racy of 74.4%.The rest of the paper is organized as follows:Section 2 presents previous approaches to stanceclassification and sentiment analysis.
Section 3highlights the importance of users?
intentions inideological debates and presents our algorithm tocapture utterance intentions using topic directedsentiment analysis.
In Section 4, we describe theuse the utterance level intentions to capture inten-tion of the entire post.
We explain our stance clas-sification method using post content features andpost intention structure in this section.
Section 5describes the use of the dialogue structure of thedebate and presents a gradient ascent method forre-evaluating posts?
stance.
We present experi-ments and results on capturing users?
intentionsand stance classification in Section 6.
This is fol-lowed by conclusions in Section 7.2 Related WorkTo classify posts?
stance in dual-sided debates,previous approaches have used probabilistic (So-masundaran and Wiebe, 2009) as well as machinelearning techniques (Anand et al 2011; Somasun-daran and Wiebe, 2010).
Some approaches exten-sively used the dialogue structure to identify posts?stance (Walker et al 2012) whereas others consid-ered opinion expressions and their targets essen-tial to capture sentiment in the posts towards de-bate topics (Somasundaran and Wiebe, 2009; So-masundaran and Wiebe, 2010).Pure machine learning approaches (Anand etal., 2011) have extracted lexical and contextualfeatures of debate posts to classify their stance.Walker et al(2012) partitioned the debate postsbased on the dialogue structure of the debate andassigned stance to a partition using lexical featuresof candidate posts.
This approach has a disadvan-tage that it loses post?s individuality because it as-signs stance based on the entire partitions whereasour approach treats each post individually.To extract opinion expressions, Somasundaranand Wiebe (2009) used the Subjectivity lexiconand Somasundaran and Wiebe (2010) used theMPQA opinion corpus (Wiebe et al 2005).
Theseopinion expressions were attached to the targetwords using different techniques.
Somasundaranand Wiebe (2009) attached opinion expressions toall plausible sentence words whereas Somasun-daran and Wiebe (2010) attached opinion expres-sions to the debate topic closest to them.
Proba-bilistic association learning of target-opinion pairand the debate topic was used by Somasundaranand Wiebe (2010) as an integer linear program-ming problem to classify posts?
stance.
Eventhough opinions might not be directed towards de-bate topics, these approaches attach the opinionsto debate topics based only on their context co-occurrence.
Our approach finds the target wordfor an opinion expression by analyzing the full de-pendency parse of the utterance.There has also been a lot of work done in socialmedia on target directed sentiment analysis (Agar-wal et al 2011; O?Hare et al 2009; Mukher-jee and Bhattacharyya, 2012) which we incorpo-rate for capturing users?
intentions.
Agarwal etal.
(2011) used syntactic features as target de-pendent features to differentiate sentiment?s ef-fect on different targets in a tweet.
O?Hare et al(2009) employed a word-based approach to deter-mine sentiments directed towards companies andtheir stocks from financial blogs.
Mukherjee andBhattacharyya (2012) applied clustering to extractfeature specific opinions and calculated the overallfeature sentiment using subjectivity lexicon.Discourse markers cues were used by Sood(2013) to prioritize the conversational sentencesand by Yelati and Sangal (2011) to identify users?intentions in help desk emails.
Most of the dis-course analysis theories defined their own dis-course segment tagging schema to understandthe meaning of each utterance.
Yelati and San-gal (2011) devised a help desk specific taggingschema to capture the queries and build emailstructure in help desk emails.
Lampert et al(2006) used verbal response modes to understandthe client/therapist conversations.
We incorporatetarget directed sentiment analysis to capture utter-ance level intentions using a sentiment lexicon anddependency parses as described in the followingsection.3 Capturing User IntentionsUsers?
intention at the utterance level plays a vitalrole in overall stance taking.
We define a set ofintentions each utterance can hold.
The proposedtopic directed sentiment analysis based approachwill automatically identify users?
intention behindeach utterance.Because of the unstructured and noisy nature ofsocial media, we need to pre-process the debate62data before analyzing it further for users?
inten-tions.3.1 PreprocessingThe posts data is split into utterances, i.e.
smallestdiscourse units, based on sentence ending mark-ers and a few specific Penn Discourse Tree Bank(PDTB) (Prasad et al 2008) discourse markerslisted in Table 2.
Merged words like ?mindbog-gling?, ?super-awesome?, etc.
are split based onthe default Unix dictionary and special charac-ter delimiters.
Once the debate posts are brokeninto utterances, we identify the intention behindeach utterance in the post to compute entire post?sstance.Table 1 presents the statistics of the debate datacollected from ?convinceme.net?.Debates Posts Author P/A Utterances28 2040 1333 1.53 12438Table 1: Debate Data Statistics3.2 Debate Intention Tagging SchemaBased on the intent each utterance can have, wehave devised a debate specific intention taggingschema.
In debates, users either express theiropinion or state a known fact.For a dual-sided debate between topic A andtopic B, our tagging schema includes the follow-ing intention tags:1.
A+ and B+ : These tags capture users?
in-tent to support topic A or B.
For example,in utterance ?Superman is very nearly inde-structible.?
the user is supporting Superman?sindestructibility in the debate between Super-man and Batman.2.
A?
and B?
: These tags capture users?
in-tent to oppose topic A or B.
For example,?Superman is a stupid person who has an ob-vious weakness, like cyclops.?
the user is op-posing Superman by pointing out his weak-ness.3.
NI: This category includes utterances whichhold no sentiment towards the debate topicsor can utter about non-debate topic entities,In utterance ?We are voting for who will winin a battle between these two.?
is neitherpraising nor opposing either of the sides.Type Discourse ConnectivesContrast but, by comparison, by con-trast, conversely, even though,in contrast, in fact, instead, nev-ertheless, on the contrary, onthe other hand, rather, whereas,even if, however, because, asReason because, asResult as a result, so, thus, therefore,therebyElaboration for instance, in particular, in-deedConjunction and, also, further, furthermore,in addition, in fact, similarly,indeed, meanwhile, more ever,whileTable 2: PDTB Discourse Markers ListEvaluation data was created by five linguistswho were provided with a complete set of instruc-tions along with the sample annotated data.
Eachutterance was annotated with its intention tag by 2linguists and the inter-annotator agreement for theevaluation data was 81.4%.Table 3 gives a quantitative overview of the an-notations in the corpus.
There are total 12438 ut-terances spread over 2420 debate posts.Tag A+ A?
B+ B?
NICorpus% 20.8 18.4 16.7 21.8 22.3Table 3: Utterance Annotation Statistics3.3 Topic Directed Sentiment AnalysisTo identify intetion behind each utterance, we cal-culate debate topic directed sentiment.
In topic di-rected sentiment analysis, the sentiment score iscalculated using dependency parses of utterancesand the sentiment lexicon sentiWordNet (Bac-cianella et al 2010).
sentiWordNet is a lexicalcorpus used for opinion mining.
It stores positiveand negative sentiment scores for every sense ofthe word present in the wordNet (Fellbaum, 2010).First, pronoun referencing is resolved using theStanford co-reference resolution system (Lee etal., 2011).
Using the Stanford dependency parser(De Marneffe et al 2006), utterances are repre-sented in a tree format where each node representsan utterance word storing its sentiment score andthe edges represents dependency relations.
Each63parentScore = sign(parentScore)?
(|parentScore|+ updateScore(childScore)) (1)utterance word is looked in the sentiWordNet andthe sentiment score calculated using Algorithm1 is stored in the word?s tree node.
For wordsmissing from sentiWordNet, average of sentimentscores of its synset member words is stored in theword?s tree node, otherwise a zero sentiment scoreis stored.
If words are modified by negation wordslike {?never?,?not?,?nonetheless?,etc.
}, their senti-ment scores are negated.Algorithm 1 Word Sentiment Score1: S ?
Senses of word W2: wordScore?
03: for all s ?
S do4: sscore = sposScore ?
snegscore5: wordScore = wordScore+ sscore6: end for7: wordScore = wordScore|S|In noun phrases such as ?great warrior?, ?cruelperson?, etc., the first word being the adjective ofthe latter, it influences its sentiment score.
Thus,based on the semantic significance of the depen-dency relation each edge holds, sentiment score ofparent nodes are updated with that of child nodesusing Equation 1.
Tree structure and recursive na-ture of Equation 1 ensures that sentiment scoresof child nodes are updated before updating theirparents?
sentiment scores.
Table 4 lists the seman-tically significant dependency relations used to up-date parent node scores.Type Dependency RelationsNoun Modifying nn, amod, appos, abbrev,infmod, poss, rcmod, rel,prepVerb Modifying advmod, acomp, ad-vcl, ccomp, prt, purpcl,xcomp, parataxis, prepTable 4: List of Dependency RelationsIn a sentence, ?Batman killed a bad guy.
?, sen-timent score of word ?Batman?
is affected by ac-tion ?kill?
and thus for verb-predicate relationslike ?nsubj?,?dobj?,?cobj?,?iobj?,etc., predicate sen-timent scores are updated with that of verb scoresusing Equation 1.Extended targets (extendedTargets) are the en-tities closely related to debate topics.
For exam-ple, ?Joker?,?Clarke Kent?
are related to ?Batman?and ?Darth Vader?, ?Yoda?
to ?Star Wars?.
To ex-tract the extended targets, we capture named enti-ties (NE) from the Wikipedia page of the debatetopic (fetched using jsoup java library) using theStanford Named Entity Recognizer (Finkel et al2005) and sort them based on their page occur-rence count.
Out of top-k (k = 20) NEs, some canbelong to both of the debate topics.
For example,?DC Comics?
is common between ?Superman?
and?Batman?.
We remove these NEs from individuallists and the remaining NEs are treated as extendedtargets (extendedTargets) of the debate topics.Debate topic directed sentiment scores are cal-culated by adding the sentiment scores of the utter-ance words which belong to the extended targetslist of each debate topic.
We refer to these scoresas AScore and BScore representing scores directedtowards topics A and B.
We also count the oc-currences of each debate topic in the utterance bychecking word with topics?
extended targets.We use these topic sentiment scores along withutterance lexical features mentioned in Table 5 toclassify utterance intentions into one of the pro-posed 5 intention tags.Set Description/ExampleUnigrams,BigramsWord and word pair frequen-ciesCue Words Sentece beginning unigramsand bigramsVerb Frame Opinion, action or statementverbSentimentCountcount of subjective adjectivesand adverbstopic Count count of words representingdebate topicsTable 5: Lexical Features for Intention CapturingWe analyze the experiments and results on cap-turing user intention in Subsection 6.1.
User inten-tions are used in building the intention structure,thus to calculate the sentiment score of the entirepost.64Post Sentiment Score =?A(A Score) where A ?
Argument Structure (2)4 Argument Structure and PostSentiment ScoreArguments are the basis of persuasive communi-cation.
An argument is a set of statements ofwhich one (conclusion) is supported by others(premises).
In our debate data, the implicit con-clusion is to support/oppose the debate topics andpremises are users?
opinion/knowledge about thetopics.
Thus, neighboring utterances with same in-tentions are merged into single argument formingthe argument structure for debate posts.
Argumentstructure, also referred to as ?Intention Structure?,may contain multiple arguments with different in-tentions.
But to identify the intention behind theentire post, we need to consider sentiment strengthand correlation of each argument.Sentiment Strength: Sentiment strength of ar-guments with different intentions are compared tocompute intention behind entire post.
Algorithm2 the computes sentiment strength of an argumentfrom its constituent utterances.Algorithm 2 Argument Sentiment Score1: U ?
Argument Utterances2: for all u ?
U do3: uscore = uAScore ?
uBscore4: end for5: Argument Score =?u ?
U (uscore)First example in Table 6 shows two utterancesone of which praising ?Superman?
and other prais-ing ?Batman?.
Our argument structure has two ar-guments containing an utterance each.
Comparingthe sentiment strength of the 2 arguments, we canconclude that author supports ?Batman?
in this ex-ample.Debate Post ScoreA1 Superman is a good person.
0.34A2 Batman is the best hero ever.
0.62A1 Superman has high speed,agility and awesome strength.1.23A2 But, Batman is a better hero.
0.42Table 6: Argument Structure ExamplesCorrelation Between Arguments: The Secondexample in Table 6 shows that though the first ar-gument has a higher sentiment strength, the con-trasting discourse marker ?but?
nullifies it, result-ing in an overall stance supporting ?Batman?.
Dis-course markers listed in the first row of Table 2 areused to identify ?contrast?
between two utterancesout of which sentiment strength of the former ut-terance is nullified.Algorithm 3 Utterance Level Sentiment Score1: U ?
Post Utterances2: postScore?
03: for u ?
U do4: uscore = uAScore ?
uBscore5: uweight = | |U |2 ?
uposition |6: postScore = postScore+uscore?uweight7: end for4.1 Calculating Post Sentiment ScoreTo calculate sentiment score of the entire post,three different approaches mentioned below aretried out:1. uttrScore (Utterance Level): Given two utter-ances connected by a contrasting discoursemarkers from Table 2, sentiment score ofthe former is nullified.
The posts?
sentimentscores are calculated using Algorithm 3.
Inthis algorithm, the utterance score is multi-plied by function of its position (line 5) whichgives more importance to the initial and end-ing utterances than to those in the middle.2.
argScore (Argument Level): First, the sen-timent score of each argument is calculatedusing Algorithm 2.
As in the above method,sentiment score of the former argument con-nected with contrast discourse marker is nul-lified and then posts?
sentiment scores arecalculated using Equation 2.3. argSpanScore (Argument Level with Span):For each argument in the posts, argumentscore is multiplied by its span i.e., the numberof utterances in an argument.
We use Equa-tion 2 to calculate posts?
sentiment score.65Count of each debate topic entities in the postsand of each intention type are used as post featuresalong with the posts?
sentiment scores to classifytheir stance, the results of which are discussed inSubsection 6.2.5 Gradient Ascent MethodIn the previous section, sentiment score and inten-tion of the utterances were used to calculate theposts?
sentiment scores.
In this section, we focuson the use of the dialogue structure of the debateto improve stance classification.
convinceme.netstores user information for posts and also providesan option to rebut another posts.
Intuitively, therebutting posts should have opposite stances andsame author posts should support the same stance.Walker et al(2012) uses the same intuition to splitthe debate posts in two partitions using a max-cutalgorithm.
This approach loses the post?s indi-viduality because it assigns the same stance to allposts belonging to a partition.
Our approach de-scribed below uses the debate structure to refineposts sentiment scores, calculated in the previoussection, thus maintaining post individuality.If two posts by same author P1 and P2 havesentiment scores ?0.1 and 0.7, the previous ap-proach would classify post P1 as supporting topicB and P2 as supporting A, even if they are bysame author and supporting the same stance.
Whatif an error crept in while calculating post senti-ment or utterance sentiment score?
Can we use thedebate structure to refine posts?
sentiment scoressuch that same author posts support same stanceand rebuttal author posts support opposite stance?We use a gradient ascent method to accomplishthis task.Gradient ascent is a greedy, hill-climbing ap-proach used to find the local maxima of a function.It maximizes a health/cost function by taking stepsproportional to gradient of the health function at agiven point.
In our case, the dialogue structure ofthe debate is represented by a Graph G(V,E) us-ing rebuttal and same author links.
Nodes (v ?
V )of graph represents debate posts with their senti-ment score, and edges (e ?
E) represent the di-alogue information between two posts with value?1?
denoting same author posts and ??1?
rebuttingparticipant posts.We formulate the health function H(G) whichmeasures the health of the given graph G(V,E)in Algorithm 4.
This health function signifies thehealth or correctness of each edge in the debatestructure.Algorithm 4 Debate Health FunctionRequire: Debate Graph G(V,E)1: H(G)?
02: for all eij ?
E do3: if eij = 1 then4: if Vi ?
Vj > 0 then5: H(G) = H(G) + 16: else7: H(G) = H(G) + (1?
|Vi?Vj |2 )8: end if9: else10: if Vi ?
Vj < 0 then11: H(G) = H(G) + 112: else13: H(G) = H(G) + |Vi ?
Vj |14: end if15: end if16: end forReturn H(G)It calculates health of each edge based on dia-logue information it holds and participating nodes?scores.
A perfect score of 1 is assigned to eachedge if participating nodes satisfy edge criteria(line 4?5, 10?11).
If not, difference of nodes?
sen-timent scores are used to calculate edges?
health.
(line 6?7, 12?13)For an imperfect edge, updating sentimentscores of its connecting nodes will increase itshealth thus improving health of the graph.
Thuswe aim to increase the health of the graph by grad-ually modifying posts?
sentiment scores.Gradient Ascent algorithm (Algorithm 5) is fedwith parameters set to (EPOCH = 1000, ?
=0.01 and ?
= 0.05).
For each iteration, letG(V,E) represent the current state of the graphand H its health.
For each node, the sentimentscore is updated by adding partial derivative ofhealth function with respect to given node at thecurrent state (line 9).
Partial derivative of theHealth function with respect to current node is de-fined in line 8.
This continues (line 1 ?
15) untillthere is no such node which improves the graph?shealth or till the number of iterations reach epoch.These refined post sentiment scores along withpost features (topic Count and intention typecount) are used to classify posts?
stance.
We dis-cuss the results in Subsection 6.2.66Algorithm 5 Gradient Ascent ApproachRequire: Debate Graph G(V,E) and H(G) HealthFunction1: for iteraton = 1?
EPOCH do2: H ?
Health(G(V,E))3: newH ?
H4: for all vi ?
V do5: V ?
?
V6: v?i ?
v?i + ?7: H ?
?
Health(G?
(V ?, E))8: PDi ?
(H?
?H)?9: vi ?
vi + ?
?
PDi10: newH = max(newH,H ?
)11: end for12: if newH = H then13: Break14: end if15: end forFigure 1 gives a working example of our ap-proach.
It clearly shows improving health of thegraph using the gradient ascent method helps inrectifying post P1?s stance.Figure 1: Working Example of Gradient Ascent6 Experiments and ResultsThis section highlights experiments, results, ad-vantages and shortcomings of our approach on in-tention capturing and posts?
stance classificationtasks.6.1 Capturing User IntentionsExperiments on debate posts from following de-bates are carried out: Superman vs Batman, Fire-fox vs Internet Explorer, Cats vs Dogs, Ninja vsPirates and Star Wars vs Lord Of The Rings.
Ourexperimental data for utterances?
intention captur-ing includes 1928 posts and 9015 utterances from5 debates with equal intention class distributionfor each domain.
Thus our data has 1803 correctlyannotated utterances belonging to each intentionclass.
The first task focuses on classifying eachutterance into one of the proposed intention tags.Our first baseline is a Unigram system whichuses unigram content information of the utter-ances.
Unigram systems are proved reliable insentiment analysis (Mullen and Collier, 2004;Pang and Lee, 2004).
The second baseline sys-tem LexFeatures uses the lexical features (Table5).
This baseline system is a strong baseline for theevaluation because it captures sentiment as well aspragmatic information of the utterances.
We con-struct two systems to capture intentions: a Topic-Score system which uses the topic directed sen-timent scores (described in Subsection 3.3) andtopic occurrence counts to capture utterance in-tentions, and a TopicScore+LexFeatures systemwhich uses topic sentiment scores (described inSubsection 3.3) along with lexical features in Ta-ble 5.
All systems are implemented using theWeka toolkit with its standard SVM implementa-tion.
Table 7 shows the accuracies of classifyingutterance intentions for each of described baselineand proposed systems.Accuracy Total A+ A?
B+ B?
NIUnigram 64.2 63.2 65.4 60.3 66.5 65.6LexFeatures 62.7 64.3 60.7 64.2 61.9 62.4TopicScore 68.4 68.1 68.7 67.2 68.7 69.3TopicScore+LexFeatures74.3 73.9 74.8 75.1 73.6 74.1Table 7: Accuracy of Utterance Intention Classifi-cationOverall we notice that the proposed approachesperform better than baseline systems, with Top-icScore+LexFeatures outperforming all systems.This shows that topic directed sentiment scorehelps in capturing users?
intentions better than theword level sentiment analysis.
We can also con-clude that the Unigram system achieves higher ac-curacies than the lexFeatures system, showing thatwhat the user says is a better indicator of user?sintentions than his sentiments and thus confirm-ing previous research results (Somasundaran andWiebe, 2010; Pang and Lee, 2008).
TopicScoreperforms lower in capturing ?NI?
tag than the base-line systems, denoting that TopicScore is not cap-turing debate topics and their sentiments correctly.Thus it assigns non-NI tagged utterances an ?NI?tag, lowering its accuracy.We run the same approach but comparing utter-ance words only with the debate topics in calculat-ing topic directed sentiment score and not with thelists of extended targets.
This produces an accu-racy of 70.8% clearly highlighting the importance67of extended targets in calculating debate topic di-rected sentiment analysis.6.2 Post Stance ClassificationExperiment data covers 2040 posts with equaltopic stance distribution from each of the follow-ing domains: Superman vs Batman, Firefox vsInternet Explorer, Cats vs Dogs, Ninja vs Piratesand Star Wars vs Lord Of The Rings.
Two base-line systems are designed for this task of debatepost?s stance classification.
The first baseline,sentVicinity, assigns each word?s sentiment scoreto the closest debate topic entity.
Then, the senti-ment score of the debate topics over an entire postis compared to classify post stance.
The secondbaseline, subjTopic, counts the number of subjec-tive words in each utterance of the post and assignsthem to debate topic related entity if present inthe utterance.
It compares overall subjective pos-itivity of debate topics to assign post stance.
Wealso compared our approach with the (Arg+Sent)method proposed by Somasundaran and Wiebe(2010).Three systems described in Subsection 4.1 areused to compute post?s sentiment score by ana-lyzing its content namely, uttrScore, argScore andargSpanScore.
Post sentiment scores from thesethree techniques along with post features (topicCount and intention type count) are used to clas-sify post stance and results are compared in Table8.
Table 8 shows that the second approach of cal-culating posts?
sentiment scores using their argu-ment structure outperforms the other approaches.System AccuracysentVicinity 61.6%subjTopic 58.1%Arg+Sent 63.9%uttrScore 67.4%argScore 70.3%argSpanScore 69.2%Table 8: Stance Classification Using Post ContentOur approach perform better than Somasun-daran and Wiebe (2010)?s approach signifying theimportance of identifying target-opinion depen-dency relation as opposed to assigning the opin-ion words to each content word in the sentence.It is important to notice that the argSpanScoremethod which multiplies argument score by itsspan doesn?t perform as well as argScore alone.This shows the utterance sentiment strength mat-ters more than neighboring same intention utter-ance.
This supports our hypothesis that online de-bate posts focus more on sentiments rather thandiscourse coherence.We experiment with gradient ascent approachand study how refining posts?
sentiment scoresbased on the dialogue structure of the debate helpsimproving stance classification.
Table 9 gives theclassification accuracies between argScore tech-nique and gradient ascent method.System AccuracyTotal Dialogue Non-dialogueargScore 70.3% 70.5% 70.1%argScore + gra-dientAscent74.4% 80.1% 70.1%Table 9: Stance Classification: Dialogue StructureThe dialogue column in Table 9 shows accura-cies for posts participating in dialogue structurei.e., those linked to other post with same author orrebutting links.
It shows a remarkable improve-ment (10% gain) which clearly signifies impor-tance of the dialogue structure.
The non-dialoguecolumn shows the accuracies for posts not in-volved in dialogue structure.
As health functionfor debate graph is a function of dialogue partici-pating posts, it does not affect stance classificationaccuracy for non-dialogue participating posts.
Di-alogue participating posts cover 41% of the exper-iment data giving 4% accuracy improvement overargScore system on complete dataset.7 ConclusionsIn this paper, We designed debate specific utter-ance level intention tags and described a topicdirected sentiment analysis approach to capturethese intentions.
We proposed a novel approach tocapture the posts?
intention structure.
Our resultsvalidate our hypothesis that capturing user inten-tions and post intention structure helps in classi-fying posts?
stance.
It also emphasizes the impor-tance of building the intention structure rather thanjust aggregating utterances?
sentiment scores.This is the first application of Gradient Ascentmethod for stance classification.
Results showre-modifying the posts?
sentiment scores by tak-ing the debates?
structure into account highly im-proves stance classification accuracies over inten-tion based method.
We aim to apply topic directedsentiment scores along with lexical features for de-bate summarization in our future work.68ReferencesApoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Ram-bow, and Rebecca Passonneau.
2011.
Sentimentanalysis of twitter data.
In Proceedings of the Work-shop on Languages in Social Media, pages 30?38.Association for Computational Linguistics.P.
Anand, M. Walker, R. Abbott, J.E.F.
Tree, R. Bow-mani, and M. Minor.
2011.
Cats rule and dogsdrool!
: Classifying stance in online debate.
In Pro-ceedings of the 2nd Workshop on ComputationalApproaches to Subjectivity and Sentiment Analysis(WASSA 2.011), pages 1?9.S.
Baccianella, A. Esuli, and F. Sebastiani.
2010.
Sen-tiwordnet 3.0: An enhanced lexical resource for sen-timent analysis and opinion mining.
In Proceed-ings of the Seventh conference on International Lan-guage Resources and Evaluation (LREC10), Val-letta, Malta, May.
European Language ResourcesAssociation (ELRA).M.C.
De Marneffe, B. MacCartney, and C.D.
Manning.2006.
Generating typed dependency parses fromphrase structure parses.
In Proceedings of LREC,volume 6, pages 449?454.Christiane Fellbaum.
2010.
WordNet.
Springer.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by gibbssampling.
In Proceedings of the 43rd Annual Meet-ing on Association for Computational Linguistics,pages 363?370.
Association for Computational Lin-guistics.A.
Lampert, R. Dale, and C. Paris.
2006.
Classifyingspeech acts using verbal response modes.
In Aus-tralasian Language Technology Workshop, page 34.Heeyoung Lee, Yves Peirsman, Angel Chang,Nathanael Chambers, Mihai Surdeanu, and Dan Ju-rafsky.
2011.
Stanford?s multi-pass sieve coref-erence resolution system at the conll-2011 sharedtask.
In Proceedings of the Fifteenth Conference onComputational Natural Language Learning: SharedTask, pages 28?34.
Association for ComputationalLinguistics.Subhabrata Mukherjee and Pushpak Bhattacharyya.2012.
Feature specific sentiment analysis for prod-uct reviews.
In Computational Linguistics and In-telligent Text Processing, pages 475?487.
Springer.Tony Mullen and Nigel Collier.
2004.
Sentiment anal-ysis using support vector machines with diverse in-formation sources.
In Proceedings of EMNLP, vol-ume 4, pages 412?418.Neil O?Hare, Michael Davy, Adam Bermingham,Paul Ferguson, Pa?raic Sheridan, Cathal Gurrin, andAlan F Smeaton.
2009.
Topic-dependent sentimentanalysis of financial blogs.
In Proceedings of the 1stinternational CIKM workshop on Topic-sentimentanalysis for mass opinion, pages 9?16.
ACM.Bo Pang and Lillian Lee.
2004.
A sentimental educa-tion: Sentiment analysis using subjectivity summa-rization based on minimum cuts.
In Proceedings ofthe 42nd annual meeting on Association for Compu-tational Linguistics, page 271.
Association for Com-putational Linguistics.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and trends in infor-mation retrieval, 2(1-2):1?135.Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-sakaki, Livio Robaldo, Aravind K Joshi, and Bon-nie L Webber.
2008.
The penn discourse treebank2.0.
In LREC.
Citeseer.S.
Somasundaran and J. Wiebe.
2009.
Recognizingstances in online debates.
In Proceedings of theJoint Conference of the 47th Annual Meeting of theACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP: Vol-ume 1-Volume 1, pages 226?234.
Association forComputational Linguistics.S.
Somasundaran and J. Wiebe.
2010.
Recognizingstances in ideological on-line debates.
In Proceed-ings of the NAACL HLT 2010 Workshop on Com-putational Approaches to Analysis and Generationof Emotion in Text, pages 116?124.
Association forComputational Linguistics.Arpit Sood, Thanvir P Mohamed, and Vasudeva Varma.2013.
Topic-focused summarization of chat conver-sations.
In Advances in Information Retrieval, pages800?803.
Springer.M.A.
Walker, P. Anand, R. Abbott, and R. Grant.2012.
Stance classification using dialogic proper-ties of persuasion.
Proceedings of the 2012 Con-ference of the North American Chapter of the As-sociaotion for Computational Linguistics: HumanLanguage Technologies, pages 592?596.J.
Wiebe, T. Wilson, and C. Cardie.
2005.
Annotatingexpressions of opinions and emotions in language.Language Resources and Evaluation, 39(2):165?210.S.
Yelati and R. Sangal.
2011.
Novel approach fortagging of discourse segments in help-desk e-mails.In Proceedings of the 2011 IEEE/WIC/ACM Inter-national Conferences on Web Intelligence and Intel-ligent Agent Technology-Volume 03, pages 369?372.IEEE Computer Society.69
