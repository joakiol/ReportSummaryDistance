A Polynomial-Time Algorithm for Statistical Machine TranslationDeka i  WuHKUSTDepartment  of Computer  ScienceUniversity of Science and TechnologyClear Water  Bay, Hong Kongdekai?cs, ust.
hkAbst rac tWe introduce a polynomial-time algorithmfor statistical machine translation.
Thisalgorithm can be used in place of theexpensive, slow best-first search strate-gies in current statistical translation ar-chitectures.
The approach employs thestochastic bracketing transduction gram-mar (SBTG) model we recently introducedto replace earlier word alignment channelmodels, while retaining a bigram languagemodel.
The new algorithm in our experi-ence yields major speed improvement withno significant loss of accuracy.1 MotivationThe statistical translation model introduced by IBM(Brown et al, 1990) views translation as a noisychannel process.
Assume, as we do throughout thispaper, that the input language is Chinese and thetask is to translate into English.
The underlyinggenerative model, shown in Figure 1, contains astochastic English sentence generator whose outputis "corrupted" by the translation channel to produceChinese sentences.
In the IBM system, the languagemodel employs imple n-grams, while the transla-tion model employs everal sets of parameters asdiscussed below.
Estimation of the parameters hasbeen described elsewhere (Brown et al, 1993).Translation is performed in the reverse directionfrom generation, as usual for recognition under gen-erative models.
For each Chinese sentence c that isto be translated, the system must attempt o findthe English sentence * such that:(1) e* = argmaxPr(elc )e(2) = argmaxPr(cle ) Pr(e)eIn the IBM model, the search for the optimal e* isperformed using a best-first heuristic "stack search"similar to A* methods.One of the primary obstacles to making the statis-tical translation approach practical is slow speed oftranslation, as performed inA* fashion.
This price ispaid for the robustness that is obtained by using veryflexible language and translation models.
The lan-guage model allows sentences of arbitrary order andthe translation model allows arbitrary word-orderpermutation.
The models employ no structural con-straints, relying instead on probability parametersto assign low probabilities to implausible sentences.This exhaustive space, together with massive num-ber of parameters, permits greater modeling accu-racy.But while accuracy is enhanced, translation ef-ficiency suffers due to the lack of structure in thehypothesis space.
The translation channel is char-acterized by two sets of parameters: translation andalignment probabilities3 The translation probabil-ities describe lexical substitution, while alignmentprobabilities describe word-order permutation.
Thekey problem is that the formulation of alignmentprobabilities a(ilj, V, T) permits the Chinese word inposition j of a length-T sentence to map to any po-sition i of a length-V English sentence.
So V T align-ments are possible, yielding an exponential spacewith correspondingly slow search times.Note there are no explicit linguistic grammars inthe IBM channel model.
Useful methods do existfor incorporating constraints fed in from other pre-processing modules, and some of these modules doemploy linguistic grammars.
For instance, we previ-ously reported a method for improving search timesin channel translation models that exploits bracket-ing information (Wu and Ng, 1995).
If any bracketsfor the Chinese sentence can be supplied as addi-tional input information, produced for example bya preprocessing stage, a modified version of the A*-based algorithm can follow the brackets to guide thesearch heuristically.
This strategy appears to pro-duces moderate improvements in search speed andslightly better translations.Such linguistic-preprocessing techniques could1Various models have been constructed by the IBMteam (Brown et al, 1993).
This description correspondsto one of the simplest ones, "Model 2"; search costs forthe more complex models are correspondingly higher.152stochasticEnglishgeneratorEnglish i Chinesestrings I noisy strings\[ channeli Jkd i rec t ion  of generative model ---~-~< - -  d i rec t ion  of translationFigure 1: Channel translation model.also be used with the new model described below,but the issue is independent of our focus here.
Inthis paper we address the underlying assumptionsof core channel model itself which does not directlyuse linguistic structure.A slightly different model is employed for aword alignment application by Dagan et al (Da-gan, Church, and Gale, 1993).
Instead of alignmentprobabilities, offset probabilities o(k) are employed,where k is essentially the positional distance betweenthe English words aligned to two adjacent Chinesewords:(3) k = i - (A(jpreo) + (j - jp~ev)N)where jpr~v is the position of the immediately pre-ceding Chinese word and N is a constant hat nor-malizes for average sentence lengths in different lan-guages.
The motivation is that words that are closeto each other in the Chinese sentence should tendto be close in the English sentence as well.
Thesize of the parameter set is greatly reduced fromthe lil x IJl x ITI x Iv I parameters of the alignmentprobabilities, down to a small set of Ikl parameters.However, the search space remains the same.The A*-style stack-decoding approach is in someways a carryover from the speech recognition archi-tectures that inspired the channel translation model.It has proven highly effective for speech recognitionin both accuracy and speed, where the search spacecontains no order variation since the acoustic andtext streams can be assumed to be linearly aligned.But in contrast, for translation models the stacksearch alone does not adequately compensate forthe combinatorially more complex space that resultsfrom permitting arbitrary order variations.
Indeed,the stack-decoding approach remains impracticallyslow for translation, and has not achieved the samekind of speed as for speech recognition.The model we describe in this paper, like Daganet al's model, encourages related words to stay to-gether, and reduces the number of parameters usedto describe word-order variation.
But more impor-tantly, it makes structural assumptions that elimi-nate large portions of the space of alignments, basedon linguistic motivatations.
This greatly reduces thesearch space and makes possible a polynomial-timeoptimization algorithm.2 ITG and  BTG Overv iewThe new translation model is based on the recentlyintroduced bilingual language modeling approach.Specifically, the model employs a bracketing trans-duction grammar or BTG (Wu, 1995a), which isa special case of inversion transduction grammarsor ITGs (Wu, 1995c; Wu, 1995c; Wu, 1995b; Wu,1995d).
These formalisms were originally developedfor the purpose of parallel corpus annotation, withapplications for bracketing, alignment, and segmen-tation.
This paper finds they are also useful for thetranslation system itself.
In this section we summa-rize the main properties of BTGs and ITGs.An ITG consists of context-free productions whereterminal symbols come in couples, for example x/y,where z is a Chinese word and y is an English trans-lation of x.
2 Any parse tree thus generates twostrings, one on the Chinese stream and one on theEnglish stream.
Thus, the tree:(1) \[~/I liST/took \[--/a $:/e ~t/book\]Np \]vP\[,,~/for ~/you\]pp \]vP Isproduces, for example, the mutual translations:(2) a.
\[~ \[\[ST \[--*~\]NP \]vP \ [~\ ]PP \]vP Is\[W6 \[\[nA le \[yi b~n shfi\]Np \]vp \[g@i ni\]pp \]vP\]sb.
\[I \[\[took \[a book\]Np \]vP \[for you\]pp \]vP IsAn additional mechanism accommodates a con-servative degree of word-order variation between thetwo languages.
With each production of the gram-mar is associated either a straight orientation or aninverted orientation, respectively denoted as follows:VP --~ \[VP PP\]VP ---* (VP PP)In the case of a production with straight orien-tation, the right-hand-side symbols are visited left-to-right for both the Chinese and English streams.But for a production with inverted orientation, the2Readers of the papers cited above should note thatwe have switched the roles of English and Chinese here,which helps simplify the presentation of the new trans-lation algorithm.153BTG all matchings ratio1 1 1.0001 1 1 12002 2 2 1.0003 6 6 1.0004 22 24 0.9175 90 120 0.7506 394 720 0.5477 1806 5040 0.3588 8558 40320 0.2129 41586 362880 0.11510 206098 3628800 0.05711 1037718 39916800 0.02612 5293446 479001600 0.01113 27297738 6227020800 0.00414 142078746 87178291200 0.00215 745387038 1307674368000 0.00116 3937603038 20922789888000 0.000Figure 2: Number of legal word alignments betweensentences of length f,  with and without the BTGrestriction.right-hand-side symbols are visited left-to-right forChinese and right-to-left or English.
Thus, the tree:(3) \ [~/ I  (\[,.~/for ~/you\]pp \ [$~' / took \[--/a ak/e~idt/book\]Np \]vp )vP \]sproduces translations with different word order:(4) a.
\[~J~ \[\[,,~*l~\]pp [~Y \[--2\[~-~\]Np \]VP \]VP \]Sb.
\[I \[\[took \[a book\]Np \]vP \[for you\]pp \]vP \]sIn the special case of BTGs which are employedin the model presented below, there is only one un-differentiated nonterminal category (aside from thestart symbol).
Designating this category A, thismeans all non-lexical productions are of one of thesetwo forms:A ---+ \ [AA .
.
.A \ ]A ---+ (AA.
.
.A}The degree of word-order flexibility is the criti-cal point.
BTGs make a favorable trade-off betweenefficiency and expressiveness: constraints are strongenough to allow algorithms to operate fficiently, butwithout so much loss of expressiveness a to hinderuseful translation.
We summarize here; details aregiven elsewhere (Wu, 1995b).With regard to efficiency, Figure 2 demonstratesthe kind of reduction that BTGs obtain in the spaceof possible alignments.
The number of possiblealignments, compared against he unrestricted casewhere any English word may align to any Chineseposition, drops off dramatically for strings longerthan four words.
(This table makes the simplifica-tion of counting only 1-1 matchings and is merelyrepresentative.
)With regard to expressiveness, we believe that al-most all variation in the order of arguments in asyntactic frame can be accommodated, a Syntac-tic frames generally contain four or fewer subcon-stituents.
Figure 2 shows that for the case of foursubconstituents, BTGs permit 22 out of the 24 pos-sible alignments.
The only prohibited arrangementsare "inside-out" transformations (Wu, 1995b), whichwe have been unable to find any examples of in ourcorpus.
Moreover, extremely distorted alignmentscan be handled by BTGs (Wu, 1995c), without re-sorting to the unrestricted-alignment model.The translation expressiveness of BTGs is by nomeans perfect.
They are nonetheless proving veryuseful in applications and are substantially more fea-sible than previous models.
In our previous corpusanalysis applications, any expressiveness limitationswere easily tolerable since degradation was graceful.In the present ranslation application, any expres-siveness limitation simply means that certain trans-lations are not considered.For the remainder of the paper, we take advantageof a convenient normal-form theorem (Wu, 1995a)that allows us to assume without loss of generalitythat the BTG only contains the binary-branchingform for the non-lexicM productions.
43 BTG-Based  Search  fo r  theOr ig ina l  Mode lsA first approach to improving the translation searchis to limit the allowed word alignment patterns tothose permitted by a BTG.
In this case, Equation (2)is kept as the objective function and the translationchannel can be parameterized similarly to Dagan etal.
(Dagan, Church, and Gale, 1993).
The effect ofthe BTG restriction is just to constrain the shapes ofthe word-order distortions.
A BTG rather than ITGis used since, as we discussed earlier, pure channeltranslation models operate without explicit gram-mars, providing no constituent categories aroundwhich a more sophisticated ITG could be structured.But the structural constraints of the BTG can im-prove search efficiency, even without differentiatedconstituent categories.
Just as in the baseline sys-tem, we rely on the language and translation modelsto take up the slack in place of an explicit grammar.In this approach, an O(T 7) algorithm similar to theone described later can be constructed to replace A*search.3Note that these points are not directed at free word-order languages.
But in such languages, explicit mor-phological inflections make role identification and trans-lation easier.4But see the conclusion for a caveat.154However we do not feel it is worth preserving off-set (or alignment or distortion) parameters simplyfor the sake of preserving the original translationchannel model.
These parameterizations were onlyintended to crudely model word-order variation.
In-stead, the BTG itself can be used directly to proba-bilistically rank alternative alignments, as describednext.4 Rep lac ing  the  Channe l  Mode lw i th  a SBTGThe second possibility is to use a stochastic brack-eting transduction grammar (SBTG) in the channelmodel, replacing the translation model altogether.In a SBTG, a probability is associated with each pro-duction.
Thus for the normal-form BTG, we have:The translation lexicon is encoded in productions ofa T \] g \[AA\]aOA -+ (A A)b(x ,y )A ~ x /y5(~ e) A ~ z/eb(qu)A --+ elyfor all x, y lexical translationsfor all x Chinese vocabularyfor all y English vocabularythe third kind.
The latter two kinds of productionsallow words of either Chinese or English to go un-matched.The SBTG assigns a probability Pr(c, e, q) to allgenerable trees q and sentence-pairs.
In principleit can be used as the translation channel model bynormalizing with Pr(e) and integrating out Pr(q) togive Pr(cle ) in Equation (2).
In practice, a stronglanguage model makes this unnecessary, so we caninstead optimize the simpler Viterbi approximation(4) e* = argmaxPr(c, e, q) Pr(e)eTo complete the picture we add a bigram modelge~-lej = g(ej lej_l) for the English language modelPr(e).Offset, alignment, or distortion parameters areentirely eliminated.
A large part of the im-plicit function of such parameters--to prevent align-ments where too many frame arguments becomeseparated--is rendered unnecessary by the BTG'sstructural constraints, which prohibit many suchconfigurations altogether.
Another part of the pa-rameters' ~urpose is subsumed by the SBTG's prob-abilities at\] and a0, which can be set to preferstraight or inverted orientation depending on thelanguage pair.
As in the original models, the lan-guage model heavily influences the remaining order-ing decisions.Matters are complicated by the presence of the bi-gram model in the objective function (which word-alignment models, as opposed to translation models,do not need to deal with).
As in our word-alignmentmodel, the translation algorithm optimizes Equa-tion (4) via dynamic programming, similar to chartparsing (Earley, 1970) but with a probabilistic ob-jective function as for HMMs (Viterbi, 1967).
Butunlike the word-alignment model, to accommodatethe bigram model we introduce indexes in the recur-rence not only on subtrees over the source Chinesestring, but also on the delimiting words of the targetEnglish substrings.Another feature of the algorithm is that segmen-tation of the Chinese input sentence is performedin parallel with the translation search.
Conven-tional architectures for Chinese NLP generally at-tempt to identify word boundaries as a preprocess-ing stage.
5 Whenever the segmentation preprocessorprematurely commits to an inappropriate segmenta-tion, difficulties are created for later stages.
Thisproblem is particularly acute for translation, sincethe decision as to whether to regard a sequence as asingle unit depends on whether its components canbe translated compositionally.
This in turn oftendepends on what the target language is.
In otherwords, the Chinese cannot be appropriately seg-mented except with respect o the target language oftranslation--a task-driven definition of correct seg-mentation.The algorithm is given below.
A few remarksabout the notation used: c~..t denotes the subse-quence of Chinese tokens cs+t, cs+2, ?
?
?
, ct. We useE(s..t) to denote the set of English words that aretranslations the Chinese word created by taking alltokens in c,..t together.
E(s,t)  denotes the set ofEnglish words that are translations of any of theChinese words anywhere within c,..t. Note also thatwe assume the explicit sentence-start and sentence-end tokens co = <s> and CT+l = </s>, which makesthe algorithm description more parsimonious.
Fi-nally, the argmax operator is generalized to vectornotation to accomodate multiple indices.1.
In i t ia l i zat iono ?
O<s<t<T6~trr(~) = b~(c~..t/Y), :~ ~ E(s..-t)2.
Recurs ion  For all s , t ,y , z  such that{ -1_<s<t_<T+1~E(8,t)zEE(s , t )6,~v~ maxrx\[l xO x0 1= ==~ tVstyz  ~ Vstyz  ~ VstyzJ2 if 6 \[1 "-6 0 and 6 \[\] 0 \[\] ~ty~ - st~ ,tyz > 6sty~Ostyz : if 6 0 "~6 \[\] " and 6 0 o styz !
styz styz > 6styzotherwise5Written Chinese contains no spaces to delimit words;any spaces in the earlier examples are artifacts of theparse tree brackets.155CategoryCorrectIncorrectOr ig inal  A* Bracket  A*  BTG-Channe l67.5 69.8 68.232.5 30.2 31.8Figure 3: Translation accuracy (percentage correct).where6\[\] a \[ \] , iv.
= max ,~sSyY 6StZz gYZ s<S<tYeE(s,S)ZEE(S,t) \[ \] \[1 ~bstyz\[1 uJ styz6O styzargmaxs<S<tYfE(s,S)ZEE(S,t)max s<S<tYeE(S,t)ZEE(s,S)a\[\] 6,syY 6stz~ gvza 0 ~,sz~ 6StyY gYZstyz0 Cstvz = argmax a 0 ~sszz( j)  6styy(k)  gYz0 s<s<tWstyz YEE(S,t)zeE(, ,s)3.
Reconst ruct ion  Initialize by setting the rootof the parse tree to q0 = (-1, T -  1, <s>, </s>).
Theremaining descendants in the optimal parse tree arethen given recursively for any q = (s,t ,  y, z) by:a probabilistic optimization problem.
But perhapsmost importantly, our goal is to constrain as tightlyas possible the space of possible transduction rela-tionships between two languages with fixed word-order, making no other language-specific assump-tions; we are thus driven to seek a kind of language-universal property.
In contrast, the ID/LP workwas directed at parsing a single language with freeword-order.
As a consequence, it would be neces-sary to enumerate a specific set of linear-precedence(LP) relations for the language, and moreover theimmediate-dominance (ID) productions would typi-cally be more complex than binary-branching.
Thissignificantly increases time complexity, compared toour BTG model.
Although it is not mentioned intheir paper, the time complexity for ID/LP pars-ing rises exponentially with the length of produc-tion right-hand-sides, due to the number of permuta-tions.
ITGs avoid this with their restriction to inver-sions, rather than permutations, and BTGs furtherminimize the grammar size.
We have also confirmedempirically that our models would not be feasibleunder general permutations.LEFT(q)RIGHT(q)NIL if t - s<1( s ,a  \[1 " ,,,\[1~ ifOq \[\] = q , ,Y, ~Yq ) ~-(s ,a 0q,w 0q,z~j i f0q=0NIL otherwiseNIL if ~- ,<1= (g~\],t,w~\],z) if0q = \[\](a~),t, y, ?~)) if Oq = 0NIL otherwiseAssume the number of translations per word isbounded by some constant.
Then the maximum sizeof E(s , t )  is proportional to t - s. The asymptotictime complexity for the translation algorithm is thusbounded by O(T7).
Note that in practice, actualperformance is improved by the sparseness of thetranslation matrix.An interesting connection has been suggested todirect parsing for ID/LP grammars (Shieber, 1984),in which word-order variations would be accommo-dated by the parser, and related ideas for genera-tion of free word-order languages in the TAG frame-work (Joshi, 1987).
Our work differs from the ID/LPwork in several important respects.
First, we are notmerely parsing, but translating with a bigram lan-guage model.
Also, of course, we are dealing with5 Resu l tsThe algorithm above was tested in the SILC transla-tion system.
The translation lexicon was largely con-structed by training on the HKUST English-ChineseParallel Bilingual Corpus, which consists of govern-mental transcripts.
The corpus was sentence-alignedstatistically (Wu, 1994); Chinese words and colloca-tions were extracted (Fung and Wu, 1994; Wu andFung, 1994); then translation pairs were learned viaan EM procedure (Wu and Xia, 1995).
The re-sulting English vocabulary is approximately 6,500words and the Chinese vocabulary is approximately5,500 words, with a many-to-many translation map-ping averaging 2.25 Chinese translations per Englishword.
Due to the unsupervised training, the transla-tion lexicon contains noise and is only at about 86%percent weighted precision.With regard to accuracy, we merely wish todemonstrate hat for statistical MT, accuracy is notsignificantly compromised by substituting our effi-cient optimization algorithm.
It is not our purposehere to argue that accuracy can be increased withour model.
No morphological processing has beenused to correct the output, and until now we haveonly been testing with a bigram model trained onextremely limited samples.
A coarse evaluation of156Input:Output:Corpus:Input:Output:Corpus:Input:Output:Corpus:Input:Output:Corpus:Input:Output:Corpus:(Xigng g~mg de ~n dlng f~n r6ng shl w6 m~n sh~ng hu6 fgmg shi de zhi zh~.
)Hong Kong's stabilize boom is us life styles's pillar.Our prosperity and stability underpin our way of life.
(B6n g~ng de jing ji qian jing yfi zhSng gu6, t~ bi~ shl gu~ng dSng shrug de ringjl qi?n jing xi xi xi~ng gu~n.
)Hong Kong's economic foreground with China, particular Guangdong province'seconomic foreground vitally interrelated.Our economic future is inextricably bound up with China, and with GuangdongProvince in particular.
(W6 wgm qu?n zhi chi ta de yl jign.
)I absolutely uphold his views.I fully support his views.
(Zh~ xi~ gn pdi k~ ji~ qi?ng w6 m~n rl hbu w~i chi jin r6ng w6n ding de n~ng li.
)These arrangements can enforce us future kept financial stabilization's competency.These arrangements will enhance our ability to maintain monetary stability inthe years to come.
(Bh gub, w6 xihn zhi k~ yi k6n ding de shuS, w6 m~n ji~ng hul ti gSng w~i d?
d~og~ xihng zhfi yho mfl biao su6 xfi de jing f~i.
)However, I now can certainty's ay, will provide for us attain various dominantgoal necessary's current expenditure.The consultation process is continuing but I can confirm now that the necessaryfunds will be made available to meet the key targets.Figure 4: Example translation outputs.translation accuracy was performed on a randomsample drawn from Chinese sentences of fewer than20 words from the parallel corpus, the results ofwhich are shown in Figure 3.
We have judged onlywhether the correct meaning (as determined by thecorresponding English sentence in the parallel cor-pus) is conveyed by the translation, paying particu-lar attention to word order, but otherwise ignoringmorphological nd function word choices.
For com-parison, the accuracies from the A*-based systemsare also shown.
There is no significant differencein the accuracy.
Some examples of the output areshown in Figure 4.On the other hand, the new algorithm has indeedproven to be much faster.
At present we are unableto use direct measurement to compare the speed ofthe systems meaningfully, because of vast implemen-tational differences between the systems.
However,the order-of-magnitude improvements are immedi-ately apparent.
In the earlier system, translation ofsingle sentences required on the order of hours (SunSparc 10 workstations).
In contrast he new algo-rithm generally takes less than one minute--usuallysubstantially ess--with no special optimization ofthe code.6 Conc lus ionWe have introduced a new algorithm for the run-time optimization step in statistical machine trans-lation systems, whose polynomial-time complexityaddresses one of the primary obstacles to practicalityfacing statistical MT.
The underlying model for thealgorithm is a combination of the stochastic BTGand bigram models.
The improvement in speed doesnot appear to impair accuracy significantly.We have implemented a version that accepts ITGsrather than BTGs, and plan to experiment withmore heavily structured models.
However, it is im-portant o note that the search complexity rises ex-ponentially rather than polynomially with the size ofthe grammar, just as for context-free parsing (Bar-ton, Berwick, and Ristad, 1987).
This is not relevantto the BTG-based model we have described since itsgrammar size is fixed; in fact the BTG's minimalgrammar size has been an important advantage overmore linguistically-motivated ITG-based models.157We have also implemented a generalized versionthat accepts arbitrary grammars not restricted tonormal form, with two motivations.
The pragmaticbenefit is that structured grammars become easierto write, and more concise.
The expressiveness ben-efit is that a wider family of probability distribu-tions can be written.
As stated earlier, the normalform theorem guarantees that the same set of shapeswill be explored by our search algorithm, regardlessof whether a binary-branching BTG or an arbitraryBTG is used.
But it may sometimes be useful toplace probabilities on n-ary productions that varywith n in a way that cannot be expressed by com-posing binary productions; for example one mightwish to encourage longer straight productions.
Thegeneralized version permits uch strategies.Currently we are evaluating robustness extensionsof the algorithm that permit words suggested by thelanguage model to be inserted in the output sen-tence, which the original A* algorithms permitted.AcknowledgementsThanks to an anonymous referee for valuable com-ments, and to the SILC group members: XuanyinXia, Eva Wai-Man Fong, Cindy Ng, Hong-singWong, and Daniel Ka-Leung Chan.
Many thanksMso to Kathleen McKeown and her group for dis-cussion, support, and assistance.Re ferencesBarton, G. Edward, Robert C. Berwick, andEric Sven Ristad.
1987.
Computational Complex-ity and Natural Language.
MIT Press, Cambridge,MA.Brown, Peter F., John Cocke, Stephen A. DellaPi-etra, Vincent J. DellaPietra, Frederick Jelinek,John D. Lafferty, Robert L. Mercer, and Paul S.Roossin.
1990.
A statistical approach to machinetranslation.
Computational Linguistics, 16(2):29-85.Brown, Peter F., Stephen A. DellaPietra, Vincent J.DellaPietra, and Robert L. Mercer.
1993.
Themathematics of statisticM machine translation:Parameter estimation.
Computational Linguis-tics, 19(2):263-311.Dagan, Ido, Kenneth W. Church, and William A.Gale.
1993.
Robust bilingual word alignmentfor machine aided translation.
In Proceedings ofthe Workshop on Very Large Corpora, pages 1-8,Columbus, OH, June.Earley, Jay.
1970.
An efficient context-free pars-ing algorithm.
Communications of the Associa-tion for Computing Machinery, 13(2):94-102.Fung, Pascale and Dekai Wu.
1994.
Statistical aug-mentation of a Chinese machine-readable dictio-nary.
In Proceedings of the Second Annual Work-shop on Very Large Corpora, pages 69-85, Kyoto,August.Joshi, Aravind K. 1987.
Word-order variation innatural language generation.
In Proceedings ofAAAI-87, Sixth National Conference on ArtificialIntelligence, pages 550-555.Shieber, Stuart M. 1984.
Direct parsing of ID/LPgrammars.
Linguistics and Philosophy, 7:135-154.Viterbi, Andrew J.
1967.
Error bounds for convolu-tional codes and an asymptotically optimal decod-ing Mgorithm.
IEEE Transactions on InformationTheory, 13:260-269.Wu, Dekai.
1994.
Aligning a parallel English-Chinese corpus statistically with lexical criteria.In Proceedings of the 32nd Annual Conferenceof the Association for Computational Linguistics,pages 80-87, Las Cruces, New Mexico, June.Wu, Dekai.
1995a.
An algorithm for simultaneouslybracketing parallel texts by aligning words.
InProceedings of the 33rd Annual Conference of theAssociation for Computational Linguistics, pages244-251, Cambridge, Massachusetts, June.Wu, Dekai.
1995b.
Grammarless extraction ofphrasal translation examples from parallel texts.In TMI-95, Proceedings of the Sixth InternationalConference on Theoretical and Methodological Is-sues in Machine Translation, volume 2, pages354-372, Leuven, Belgium, July.Wu, Dekai.
1995c.
Stochastic inversion trans-duction grammars, with application to segmen-tation, bracketing, and alignment of parallel cor-pora.
In Proceedings of IJCAL95, Fourteenth In-ternational Joint Conference on Artificial Intelli-gence, pages 1328-1334, Montreal, August.Wu, Dekai.
1995d.
Trainable coarse bilingual gram-mars for parMlel text bracketing.
In Proceed-ings of the Third Annual Workshop on Very LargeCorpora, pages 69-81, Cambridge, Massachusetts,June.Wu, Dekai and Pascale Fung.
1994.
ImprovingChinese tokenization with linguistic filters on sta-tistical lexicM acquisition.
In Proceedings of theFourth Conference on Applied Natural LanguageProcessing, pages 180-181, Stuttgart, October.Wu, Dekai and Cindy Ng.
1995.
Using bracketsto improve search for statistical machine transla-tion.
In PACLIC-IO, Pacific Asia Conference onLanguage, Information and Computation, pages195-204, Hong Kong, December.Wu, Dekai and Xuanyin Xia.
1995.
Large-scale au-tomatic extraction of an English-Chinese l xicon.Machine Translation, 9(3-4):285-313.158
