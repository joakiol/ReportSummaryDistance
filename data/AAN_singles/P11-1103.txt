Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1027?1035,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsReordering Metrics for MTAlexandra Birch Miles Osbornea.birch@ed.ac.uk miles@inf.ed.ac.ukUniversity of Edinburgh10 Crichton StreetEdinburgh, EH8 9AB, UKAbstractOne of the major challenges facing statisticalmachine translation is how to model differ-ences in word order between languages.
Al-though a great deal of research has focussedon this problem, progress is hampered by thelack of reliable metrics.
Most current metricsare based on matching lexical items in thetranslation and the reference, and their abil-ity to measure the quality of word order hasnot been demonstrated.
This paper presentsa novel metric, the LRscore, which explic-itly measures the quality of word order byusing permutation distance metrics.
We showthat the metric is more consistent with humanjudgements than other metrics, including theBLEU score.
We also show that the LRscorecan successfully be used as the objective func-tion when training translation model parame-ters.
Training with the LRscore leads to outputwhich is preferred by humans.
Moreover, thetranslations incur no penalty in terms of BLEUscores.1 IntroductionResearch in machine translation has focused broadlyon two main goals, improving word choice and im-proving word order in translation output.
Currentmachine translation metrics rely upon indirect meth-ods for measuring the quality of the word order, andtheir ability to capture the quality of word order ispoor (Birch et al, 2010).There are currently two main approaches to eval-uating reordering.
The first is exemplified by theBLEU score (Papineni et al, 2002), which countsthe number of matching n-grams between the refer-ence and the hypothesis.
Word order is captured bythe proportion of longer n-grams which match.
Thismethod does not consider the position of match-ing words, and only captures ordering differencesif there is an exact match between the words in thetranslation and the reference.
Another approach istaken by two other commonly used metrics, ME-TEOR (Banerjee and Lavie, 2005) and TER (Snoveret al, 2006).
They both search for an alignment be-tween the translation and the reference, and fromthis they calculate a penalty based on the numberof differences in order between the two sentences.When block moves are allowed the search space isvery large, and matching stems and synonyms in-troduces errors.
Importantly, none of these metricscapture the distance by which words are out of order.Also, they conflate reordering performance with thequality of the lexical items in the translation, makingit difficult to tease apart the impact of changes.
Moresophisticated metrics, such as the RTE metric (Pado?et al, 2009), use higher level syntactic or semanticanalysis to determine the grammaticality of the out-put.
These approaches require annotation and can bevery slow to run.
For most research, shallow metricsare more appropriate.We introduce a novel shallow metric, the LexicalReordering Score (LRscore), which explicitly mea-sures the quality of word order in machine trans-lations and interpolates it with a lexical metric.This results in a simple, decomposable metric whichmakes it easy for researchers to pinpoint the effectof their changes.
In this paper we show that theLRscore is more consistent with human judgements1027than other metrics for five out of eight different lan-guage pairs.
We also apply the LRscore during Mini-mum Error Rate Training (MERT) to see whether in-formation on reordering allows the translation modelto produce better reorderings.
We show that hu-mans prefer the output of systems trained with theLRscore 52.5% as compared to 43.9% when train-ing with the BLEU score.
Furthermore, training withthe LRscore does not result in lower BLEU scores.The rest of the paper proceeds as follows.
Sec-tion 2 describes the reordering and lexical metricsthat are used and how they are combined.
Section 3presents the experiments on consistency with humanjudgements and describes how to train the languageindependent parameter of the LRscore.
Section 4 re-ports the results of the experiments on MERT.
Fi-nally we discuss related work and conclude.2 The LRscoreIn this section we present the LRscore which mea-sures reordering using permutation distance metrics.These reordering metrics have been demonstrated tocorrelate strongly with human judgements of wordorder quality (Birch et al, 2010).
The LRscore com-bines the reordering metrics with lexical metrics toprovide a complete metric for evaluating machinetranslations.2.1 Reordering metricsThe relative ordering of words in the source and tar-get sentences is encoded in alignments.
We can in-terpret algnments as permutations which allows usto apply research into metrics for ordered encodingsto measuring and evaluating reorderings.
We use dis-tance metrics over permutations to evaluate reorder-ing performance.
Figure 1 shows three permutations.Each position represents a source word and eachvalue indicates the relative positions of the alignedtarget words.
In Figure 1 (a) represents the identitypermutation, which would result from a monotonealignment, (b) represents a small reordering consist-ing of two words whose orders are inverted, and (c)represents a large reordering where the two halvesof the sentence are inverted in the target.A translation can potentially have many validword orderings.
However, we can be reasonably cer-tain that the ordering of the reference sentence mustbe acceptable.
We therefore compare the ordering(a) (1 2 3 4 5 6 7 8 9 10)(b) (1 2 3 4 ?6 ?5 ?7 8 9 10)(c) (6 7 8 9 10 ?1 2 3 4 5)Figure 1.
Three permutations: (a) monotone (b) with asmall reordering and (b) with a large reordering.
Bulletpoints highlight non-sequential neighbours.of a translation with that of the reference sentence.Where multiple references exist, we select the clos-est, i.e.
the one that gives the best score.
The un-derlying assumption is that most reasonable wordorderings should be fairly similar to the reference,which is a necessary assumption for all automaticmachine translation metrics.Permutations encode one-one relations, whereasalignments contain null alignments and one-many,many-one and many-many relations.
We make somesimplifying assumptions to allow us to work withpermutations.
Source words aligned to null are as-signed the target word position immediately afterthe target word position of the previous source word.Where multiple source words are aligned to the sametarget word or phrase, a many-to-one relation, thetarget ordering is assumed to be monotone.
Whenone source word is aligned to multiple target words,a one-to-many relation, the source word is assumedto be aligned to the first target word.
These simplifi-cations are chosen so as to reduce the alignment to abijective relationship without introducing any extra-neous reorderings, i.e.
they encode a basic monotoneordering assumption.We choose permutation distance metrics whichare sensitive to the number of words that are outof order, as humans are assumed to be sensitive tothe number of words that are out of order in a sen-tence.
The two permutations we refer to, pi and ?,are the source-reference permutation and the source-translation permutation.
The metrics are normalisedso that 0 means that the permutations are completelyinverted, and 1 means that they are identical.
We re-port these scores as percentages.2.1.1 Hamming DistanceThe Hamming distance (Hamming, 1950) mea-sures the number of disagreements between two per-mutations.
It is defined as follows:dh(pi, ?)
= 1?
?ni=1 xin, xi ={0 if pi(i) = ?
(i)1 otherwise1028Eg.
BLEU METEOR TER dh dk(a) 100.0 100.0 100.0 100.0 100.0(b) 61.8 86.9 90.0 80.0 85.1(c) 81.3 92.6 90.0 0.0 25.5Table 1.
Metric scores for examples in Figure 1 which arecalculated by comparing the permutations to the identity.All metrics are adjusted so that 100 is the best score and0 the worst.where n is the length of the permutation.
TheHamming distance is the simplest permutation dis-tance metric and is useful as a baseline.
It has noconcept of the relative ordering of words.2.1.2 Kendall?s Tau DistanceKendall?s tau distance is the minimum numberof transpositions of two adjacent symbols necessaryto transform one permutation into another (Kendall,1938).
It represents the percentage of pairs of ele-ments which share the same order between two per-mutations.
It is defined as follows:dk(pi, ?)
= 1??
?ni=1?nj=1 zijZwhere zij ={1 if pi(i) < pi(j) and ?
(i) > ?
(j)0 otherwiseZ =(n2 ?
n)2Kendalls tau seems particularly appropriate formeasuring word order differences as the relative or-dering words is taken into account.
However, mosthuman and machine ordering differences are muchcloser to monotone than to inverted.
The range ofvalues of Kendall?s tau is therefore too narrow andclose to 1.
For this reason we take the square rootof the standard metric.
This adjusted dk is alsomore correlated with human judgements of reorder-ing quality (Birch et al, 2010).We use the example in Figure 1 to highlight theproblem with current MT metrics, and to demon-strate how the permutation distance metrics are cal-culated.
In Table 1 we present the metric results forthe example permutations.
The metrics are calcu-lated by comparing the permutation string with themonotone permutation.
(a) receives the best scorefor all metrics as it is compared to itself.
BLEUand METEOR fail to recognise that (b) represents asmall reordering and (c) a large reordering and theyassign a lower score to (b).
The reason for this is thatthey are sensitive to breaks in order, but not to theactual word order differences.
BLEU matches moren-grams for (c) and consequently assigns it a higherscore.
METEOR counts the number of blocks thatthe translation is broken into, in order to align it withthe source.
(b) is aligned using four blocks, whereas(c) is aligned using only two blocks.
TER counts thenumber of edits, allowing for block shifts, and ap-plies one block shift for each example, resulting inan equal score for (b) and (c).
Both the Hammingdistance dh and the Kendall?s tau distance dk cor-rectly assign (c) a worse score than (b).
Note thatfor (c), the Hamming distance was not able to re-ward the permutation for the correct relative order-ing of words within the two large blocks and gave(c) a score of 0, whereas Kendall?s tau takes relativeordering into account.Wong and Kit (2009) also suggest a metric whichcombines a word choice and a word order compo-nent.
They propose a type of F-measure which usesa matching function M to calculate precision andrecall.
M combines the number of matched words,weighted by their tfidf importance, with their posi-tion difference score, and finally subtracting a scorefor unmatched words.
Including unmatched wordsin the M function undermines the interpretation ofthe supposed F-measure.
The reordering componentis the average difference of absolute and relativeword positions which has no clear meaning.
Thisscore is not intuitive or easily decomposable and it ismore similar to METEOR, with synonym and stemfunctionality mixed with a reordering penalty, thanto our metric.2.2 Combined MetricThe LRscore consists of a reordering distance met-ric which is linearly interpolated with a lexical scoreto form a complete machine translation evaluationmetric.
The metric is decomposable because the in-dividual lexical and reordering components can belooked at individually.
The following formula de-scribes how to calculate the LRscore:LRscore = ?R+ (1?
?
)L (1)The metric contains only one parameter, ?, whichbalances the contribution of the reordering metric,R, and the lexical metric, L. Here we use BLEU as1029the lexical metric.
R is the average permutation dis-tance metric adjusted by the brevity penalty and it iscalculated as follows:R =?s?S dsBPs|S|(2)Where S is a set of test sentences, ds is the reorder-ing distance for a sentence and BP is the brevitypenalty.The brevity penalty is calculated as:BP ={1 if t > re1?r/t if t ?
r(3)where t is the length of the translation, and r is theclosest reference length.
If the reference sentence isslightly longer than the translation, then the brevitypenalty will be a fraction somewhat smaller than1.
This has the effect of penalising translations thatare shorter than the reference.
The brevity penaltywithin the reordering component is necessary as thedistance-based metric would provide the same scorefor a one word translation as it would for a longermonotone translation.
R is combined with a systemlevel lexical score.In this paper we apply the BLEU score as the lex-ical metric, as it is well known and it measures lexi-cal precision at different n-gram lengths.
We experi-ment with the full BLEU score and the 1-gram BLEUscore, BLEU1, which is purely a measure of the pre-cision of the word choice.
The 4-gram BLEU scoreincludes some measure of the local reordering suc-cess in the precision of the longer n-grams.
BLEUis an important baseline, and improving on it by in-cluding more reordering information is an interest-ing result.
The lexical component of the system canbe any meaningful metric for a particular target lan-guage.
If a researcher was interested in morpholog-ically rich languages, for example, METEOR couldbe used.
We use the LRscore to return sentence levelscores as well system level scores, and when doingso the smoothed BLEU (Lin and Och, 2004) is used.3 Consistency with Human JudgementsAutomatic metrics must be validated by compar-ing their scores with human judgements.
We trainthe metric parameter to optimise consistency withhuman preference judgements across different lan-guage pairs and then we show that the LRscore ismore consistent with humans than other commonlyused metrics.3.1 Experimental DesignHuman judgement of rank has been chosen as the of-ficial determinant of translation quality for the 2009Workshop on Machine Translation (Callison-Burchet al, 2009).
We used human ranking data from thisworkshop to evaluate the LRscore.
This consistedof German, French, Spanish and Czech translationsystems that were run both into and out of English.In total there were 52,265 pairwise rank judgementscollected.Our reordering metric relies upon word align-ments that are generated between the source and thereference sentences, and the source and the trans-lated sentences.
In an ideal scenario, the transla-tion system outputs the alignments and the refer-ence set can be selected to have gold standard hu-man alignments.
However, the data that we use toevaluate metrics does not have any gold standardalignments and we must train automatic alignmentmodels to generate them.
We used version two ofthe Berkeley alignment model (Liang et al, 2006),with the posterior threshold set at 0.5.
Our Spanish-,French- and German-English alignment models aretrained using Europarl version 5 (Koehn, 2005).
TheCzech-English alignment model is trained on sec-tions 0-2 of the Czech-English Parallel Corpus, ver-sion 0.9 (Bojar and Zabokrtsky, 2009).The metric scores are calculated for the test setfrom the 2009 workshop on machine translation.
Itconsists of 2525 sentences in English, French, Ger-man, Spanish and Czech.
These sentences have beentranslated by different machine translation systemsand the output submitted to the workshop.
The sys-tem output along with human evaluations can bedownloaded from the web1.The BLEU score has five parameters, one for eachn-gram, and one for the brevity penalty.
These pa-rameters are set to a default uniform value of one.METEOR has 3 parameters which have been trainedfor human judgements of rank (Lavie and Agarwal,2008).
METEOR version 0.7 was used.
The otherbaseline metric used was TER version 0.7.25.
Weadapt TER by subtracting it from one, so that all1http://www.statmt.org/wmt09/results.html1030metric increases mean an improvement in the trans-lation.
The TER metric has five parameters whichhave not been trained.Using rank judgements, we do not have absolutescores and so we cannot compare translations acrossdifferent sentences and extract correlation statistics.We therefore use the method adopted in the 2009workshop on machine translation (Callison-Burch etal., 2009).
We ascertained how consistent the auto-matic metrics were with the human judgements bycalculating consistency in the following manner.
Wetake each pairwise comparison of translation outputfor single sentences by a particular judge, and werecorded whether or not the metrics were consistentwith the human rank.
I.e.
we counted cases whereboth the metric and the human judge agreed that onesystem is better than another.
We divided this by thetotal number of pairwise comparisons to get a per-centage.
We excluded pairs which the human anno-tators ranked as ties.de-en es-en fr-en cz-endk 73.9 80.5 80.4 81.1Table 2.
The average Kendall?s tau reordering distancebetween the test and reference sentences.
100 meansmonotone thus de-en has the most reordering.We present a novel method for setting theLRscore parameter.
Using multiple language pairs,we train the parameter according to the amount ofreordering seen in each test set.
The advantage ofthis approach is that researchers do not need to trainthe parameter for new language pairs or test do-mains.
They can simply calculate the amount of re-ordering in the test set and adjust the parameter ac-cordingly.
The amount of reordering is calculatedas the Kendall?s tau distance between the sourceand the reference sentences as compared to dummymonotone sentences.
The amount of reordering forthe test sentences is reported in Table 2.
German-English shows more reordering than other languagepairs as it has a lower dk score of 73.9.
The languageindependent parameter (?)
is adjusted by applyingthe reordering amount (dk) as an exponent.
?
is al-lowed to takes values of between 0 and 1.
This worksin a similar way to the brevity penalty.
With more re-ordering, the dk becomes smaller which leads to anincrease in the final value of ?.
?
represents the per-centage contribution of the reordering component inthe LRscore:?
= ?dk (4)The language independent parameter ?
is trainedonce, over multiple language pairs.
This procedureoptimises the average of the consistency resultsacross the different language pairs.
We use greedyhillclimbing in order to find the optimal setting.
Ashillclimbing can end up in a local minima, we per-form 20 random restarts, and retaining only the pa-rameter value with the best consistency result.3.2 ResultsTable 3 reports the optimal consistency of theLRscore and baseline metrics with human judge-ments for each language pair.
The LRscore vari-ations are named as follows: LR refers to theLRscore, ?H?
refers to the Hamming distance and?K?
to Kendall?s tau distance.
?B1?
and ?B4?
referto the smoothed BLEU score with the 1-gram andthe complete scores.
Table 3 shows that the LRscoreis more consistent with human judgement for 5 outof the 8 language pairs.
This is an important resultwhich shows that combining lexical and reorderinginformation makes for a stronger metric than thebaseline metrics which do not have a strong reorder-ing component.METEOR is the most consistent for the Czech-English and English-Czech language pairs, whichhave the least amount of reordering.
METEOR lagsbehind for the language pairs with the most reorder-ing, the German-English and English-German pairs.Here LR-KB4 is the best metric, which shows thatmetrics which are sensitive to the distance words areout of order are more appropriate for situations witha reasonable amount of reordering.4 Optimising Translation ModelsAutomatic metrics are useful for evaluation, but theyare essential for training model parameters.
In thissection we apply the LRscore as the objective func-tion in MERT training (Och, 2003).
MERT min-imises translation errors according to some auto-matic evaluation metric while searching for the bestparameter settings over the N-best output.
A MERTtrained model is likely to exhibit the properties that1031Metric de-en es-en fr-en cz-en en-de en-es en-fr en-cz aveMETEOR 58.6 58.3 58.3 59.4 52.6 55.7 61.2 55.6 57.5TER 53.2 50.1 52.6 47.5 48.6 49.6 58.3 45.8 50.7BLEU1 56.1 57.0 56.7 52.5 52.1 54.2 62.3 53.3 55.6BLEU 58.7 55.5 57.7 57.2 54.1 56.7 63.7 53.1 57.1LR-HB1 59.7 60.0 58.6 53.2 54.6 55.6 63.7 54.5 57.5LR-HB4 60.4 57.3 58.7 57.2 54.8 57.3 63.3 53.8 57.9LR-KB1 60.4 59.7 58.0 54.0 54.1 54.7 63.4 54.9 57.5LR-KB4 61.0 57.2 58.5 58.6 54.8 56.8 63.1 55.0 58.7Table 3.
The percentage consistency between human judgements of rank and metrics.
The LRscore variations (LR-*)are optimised for average consistency across language pair (shown in right hand column).
The bold numbers representthe best consistency score per language pair.the metric rewards, but will be blind to aspects oftranslation quality that are not directly captured bythe metric.
We apply the LRscore in order to im-prove the reordering performance of a phrase-basedtranslation model.4.1 Experimental DesignWe hypothesise that the LRscore is a good metricfor training translation models.
We test this by eval-uating the output of the models, first with automaticmetrics, and then by using human evaluation.
Wechoose to run the experiment with Chinese-Englishas this language pair has a large amount of mediumand long distance reorderings.4.1.1 Training SetupThe experiments are carried out with Chinese-English data from GALE.
We use the official testset of the 2006 NIST evaluation (1994 sentences).For the development test set, we used the evalu-ation set from the GALE 2008 evaluation (2010sentences).
Both development set and test set havefour references.
The phrase table was built from1.727M parallel sentences from the GALE Y2 train-ing data.
The phrase-based translation model calledMOSES was used, with all the default settings.
Weextracted phrases as in (Koehn et al, 2003) by run-ning GIZA++ in both directions and merging align-ments with the grow-diag-final heuristic.
We usedthe Moses translation toolkit, including a lexicalisedreordering model.
The SRILM language modellingtoolkit (Stolcke, 2002) was used with interpolatedKneser-Ney discounting.
There are three separate 3-gram language models trained on the English sideof parallel corpus, the AFP part of the Gigawordcorpus, and the Xinhua part of the Gigaword cor-LR-HB1 LR-HB4 LR-KB1 LR-KB426.40 07.19 43.33 26.23Table 4.
The parameter setting representing the % impactof the reordering component for the different versions ofthe LRscore metric.pus.
A 4 or 5-gram language model would haveled to higher scores for all objective functions, butwould not have changed the findings in this paper.We used the MERT code available in the MOSESrepository (Bertoldi et al, 2009).The reordering metrics require alignments whichwere created using the Berkeley word alignmentpackage version 1.1 (Liang et al, 2006), with theposterior probability to being 0.5.We first extracted the LRscore Kendall?s tau dis-tance from the monotone for the Chinese-Englishtest set and this value was 66.1%.
This is far more re-ordering than the other language pairs shown in Ta-ble 2.
We then calculated the optimal parameter set-ting, using the reordering amount as a power expo-nent.
Table 4 shows the parameter settings we usedin the following experiments.
The optimal amount ofreordering for LR-HB4 is low, but the results showit still makes an important contribution.4.1.2 Human Evaluation SetupHuman judgements of translation quality are nec-essary to determine whether humans prefer sen-tences from models trained with the BLEU scoreor with the LRscore.
There have been some recentstudies which have used the online micro-market,Amazons Mechanical Turk, to collect human anno-tations (Snow et al, 2008; Callison-Burch, 2009).While some of the data generated is very noisy, in-valid responses are largely due to a small numberof workers (Kittur et al, 2008).
We use Mechanical1032Turk and we improve annotation quality by collect-ing multiple judgements, and eliminating workerswho do not achieve a certain level of performanceon gold standard questions.We randomly selected a subset of sentences fromthe test set.
We use 60 sentences each for compar-ing training with BLEU to training with LR-HB4and with LR-KB4.
These sentences were between15 and 30 words long.
Shorter sentences tend to haveuninteresting differences, and longer sentences mayhave many conflicting differences.Workers were presented with a reference sen-tence and two translations which were randomlyordered.
They were told to compare the transla-tions and select their preferred translation or ?Don?tKnow?.
Workers were screened to guarantee reason-able judgement quality.
20 sentence pairs were ran-domly selected from the 120 test units and anno-tated as gold standard questions.
Workers who gotless than 60% of these gold questions correct weredisqualified and their judgements discarded.After disagreeing with a gold annotation, a workeris presented with the gold answer and an expla-nation.
This guides the worker on how to performthe task and motivates them to be more accurate.We used the Crowdflower2 interface to MechanicalTurk, which implements the gold functionality.Even though experts can disagree on preferencejudgements, gold standard labels are necessary toweed out the poor standard workers.
There were 21trusted workers who achieved an average accuracyof 91% on the gold.
There were 96 untrusted work-ers who averaged 29% accuracy on the gold.
Theirjudgements were discarded.
Three judgements werecollected from the trusted workers for each of the120 test sentences.4.2 Results4.2.1 Automatic Evaluation of MERTIn this experiment we demonstrate that the re-ordering metrics can be used as learning criterion inminimum error rate training to improve parameterestimation for machine translation.Table 5 reports the average of three runs of MERTtraining with different objective functions.
The lexi-cal metric BLEU is used as an objective function in2http://www.crowdflower.comMetricsPPPPObj.Func.
BLEU LR-HB4 LR-KB4 TER MET.BLEU 31.1 32.1 41.0 60.7 55.5LRHB4 31.1 32.2 41.3 60.6 55.7LRKB4 31.0 32.2 41.2 61.0 55.8Table 5.
Average results of three different MERT runs fordifferent objective functions.isolation, and also as part of the LRscore togetherwith the Hamming distance and Kendall?s tau dis-tance.
We test with these metrics, and we also reportthe TER and METEOR scores for comparison.The first thing we note in Table 5 is that we wouldexpect the highest scores when training with thesame metric as that used for evaluation as MERTmaximises the objective function on the develop-ment data set.
Here, however, when testing withBLEU, we see that training with BLEU and withLR-HB4 leads to equally high BLEU scores.
Thereordering component is more discerning than theBLEU score.
It reliably increases as the word orderapproaches that of the reference, whereas BLEU canreports the same score for a large number of differentalternatives.
This might make the reordering metriceasier to optimise, leading to the joint best scoresat test time.
This is an important result, as it showsthat by training with the LRscore objective function,BLEU scores do not decrease, which is desirable asBLEU scores are usually reported in the field.The LRscore also results in better scores whenevaluated with itself and the other two baseline met-rics, TER and METEOR.
Reordering and the lexi-cal metrics are orthogonal information sources, andthis shows that combining them results in better per-forming systems.
BLEU has shown to be a strongbaseline metric to use as an objective function (Ceret al, 2010), and so the LRscore performance in Ta-ble 5 is a good result.Examining the weights that result from the dif-ferent MERT runs, the only notable difference isthat the weight of the distortion cost is considerablylower with the LRscore.
This shows more trust inthe quality of reorderings.
Although it is interestingto look at the model weights, any final conclusion onthe impact of the metrics on training must depend onhuman evaluation of translation quality.1033Type SentenceReference silicon valley is still a rich area in the united states.
the average salary in the area was us$62,400 a year, which was 64% higher than the american average.LR-KB4 silicon valley is still an affluent area of the united states, the regional labor with an averageannual salary of 6.24 million us dollars, higher than the average level of 60 per cent.BLEU silicon valley is still in the united states in the region in an affluent area of the workforce,the average annual salary of 6.24 million us dollars, higher than the average level of 60 percentTable 7.
A reference sentence is compared with output from models trained with BLEU and with the LR-KB4 lrscore.Prefer LR Prefer BLEU Don?t KnowLR-KB4 96 79 5LR-HB4 93 79 8Total 189 (52.5%) 158 (43.9%) 13Table 6.
The number of the times human judges preferredthe output of systems trained either with the LRscore orwith the BLEU score, or were unable to choose.4.2.2 Human EvaluationWe collect human preference judgements for out-put from systems trained using the BLEU score andthe LRscore in order to determine whether trainingwith the LRscore leads to genuine improvements intranslation quality.
Table 6 shows the number of thetimes humans preferred the LRscore or the BLEUscore output, or when they did not know.
We can seethat humans have a greater preference for the out-put for systems trained with the LRscore, which ispreferred 52.5% of the time, compared to the BLEUscore, which was only preferred 43.9% of the time.The sign test can be used to determine whetherthis difference is significant.
Our null hypothesisis that the probability of a human preferring theLRscore trained output is the same as that of prefer-ring the BLEU trained output.
The one-tailed alter-native hypothesis is that humans prefer the LRscoreoutput.
If the null hypothesis is true, then there isonly a probability of 0.048 that 189 out of 347(189 + 158) people will select the LRscore output.We therefore discard the null hypothesis and the hu-man preference for the output of the LRscore trainedsystem is significant to the 95% level.In order to judge how reliable our judgements arewe calculate the inter-annotator agreement.
This isgiven by the Kappa coefficient (K), which balancesagreement with expected agreement.
The Kappa co-efficient is 0.464 which is considered to be a moder-ate level of agreement.In analysis of the results, we found that outputfrom the system trained with the LRscore tend toproduce sentences with better structure.
In Table 7we see a typical example.
The word order of thesentence trained with BLEU is mangled, whereasthe LR-KB4 model outputs a clear translation whichmore closely matches the reference.
It also garnershigher reordering and BLEU scores.We expect that more substantial gains can bemade in the future by using models which have morepowerful reordering capabilities.
A richer set of re-ordering features, and a model capable of longerdistance reordering would better leverage metricswhich reward good word orderings.5 ConclusionWe introduced the LRscore which combines a lexi-cal and a reordering metric.
The main motivation forthis metric is the fact that it measures the reorder-ing quality of MT output by using permutation dis-tance metrics.
It is a simple, decomposable metricwhich interpolates the reordering component witha lexical component, the BLEU score.
This paperdemonstrates that the LRscore metric is more con-sistent with human preference judgements of ma-chine translation quality than other machine trans-lation metrics.
We also show that when training aphrase-based translation model with the LRscore asthe objective function, the model retains its perfor-mance as measured by the baseline metrics.
Cru-cially, however, optimisation using the LRscore im-proves subjective evaluation.
Ultimately, the avail-ability of a metric which reliably measures reorder-ing performance should accelerate progress towardsdeveloping more powerful reordering models.1034ReferencesSatanjeev Banerjee and Alon Lavie.
2005.
Meteor: Anautomatic metric for MT evaluation with improvedcorrelation with human judgments.
In Workshop onIntrinsic and Extrinsic Evaluation Measures for MTand/or Summarization.Nicola Bertoldi, Barry Haddow, and Jean-Baptiste Fouet.2009.
Improved Minimum Error Rate Training inMoses.
The Prague Bulletin of Mathematical Linguis-tics, 91:7?16.Alexandra Birch, Phil Blunsom, and Miles Osborne.2010.
Metrics for MT Evaluation: Evaluating Re-ordering.
Machine Translation, 24(1):15?26.Ondrej Bojar and Zdenek Zabokrtsky.
2009.
CzEng0.9:Large Parallel Treebank with Rich Annotation.Prague Bulletin of Mathematical Linguistics, 92:63?84.Chris Callison-Burch, Philipp Koehn, Christof Monz, andJosh Schroeder.
2009.
Findings of the 2009 Workshopon Statistical Machine Translation.
In Proceedings ofthe Fourth Workshop on Statistical Machine Transla-tion, pages 1?28, Athens, Greece, March.
Associationfor Computational Linguistics.Chris Callison-Burch.
2009.
Fast, cheap, and cre-ative: evaluating translation quality using Amazon?sMechanical Turk.
In Proceedings of the 2009 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 286?295, Singapore, August.
Associa-tion for Computational Linguistics.Daniel Cer, Christopher D. Manning, and Daniel Juraf-sky.
2010.
The best lexical metric for phrase-basedstatistical MT system optimization.
In Human Lan-guage Technologies: The 2010 Annual Conference ofthe North American Chapter of the Association forComputational Linguistics, pages 555?563, Los An-geles, California, June.Richard Hamming.
1950.
Error detecting and er-ror correcting codes.
Bell System Technical Journal,26(2):147?160.Maurice Kendall.
1938.
A new measure of rank correla-tion.
Biometrika, 30:81?89.A.
Kittur, E. H. Chi, and B.
Suh.
2008.
Crowdsourcinguser studies with Mechanical Turk.
In Proceeding ofthe twenty-sixth annual SIGCHI conference on Humanfactors in computing systems, pages 453?456.
ACM.Philipp Koehn, Franz Och, and Daniel Marcu.
2003.
Sta-tistical Phrase-Based translation.
In Proceedings ofthe Human Language Technology and North Ameri-can Association for Computational Linguistics Con-ference, pages 127?133, Edmonton, Canada.
Associ-ation for Computational Linguistics.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In Proceedings of MT-Summit.Alon Lavie and Abhaya Agarwal.
2008.
Meteor,m-BLEU and m-TER: Evaluation metrics for high-correlation with human rankings of machine transla-tion output.
In Proceedings of the Workshop on Sta-tistical Machine Translation at the Meeting of the As-sociation for Computational Linguistics (ACL-2008),pages 115?118.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by agreement.
In Proceedings of the HumanLanguage Technology Conference of the NAACL, MainConference, pages 104?111, New York City, USA,June.
Association for Computational Linguistics.Chin-Yew Lin and Franz Och.
2004.
ORANGE: amethod for evaluating automatic evaluation metrics formachine translation.
In Proceedings of the Conferenceon Computational Linguistics, pages 501?507.Franz J. Och.
2003.
Minimum error rate training in sta-tistical machine translation.
In Proceedings of the As-sociation for Computational Linguistics, pages 160?167, Sapporo, Japan.Sebastian Pado?, Daniel Cer, Michel Galley, Dan Jurafsky,and Christopher D. Manning.
2009.
Measuring ma-chine translation quality as semantic equivalence: Ametric based on entailment features.
Machine Trans-lation, pages 181?193.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic evalu-ation of machine translation.
In Proceedings of the As-sociation for Computational Linguistics, pages 311?318, Philadelphia, USA.Matthew Snover, Bonnie Dorr, R. Schwartz, L. Micciulla,and J. Makhoul.
2006.
A study of translation editrate with targeted human annotation.
In Proceedingsof Association for Machine Translation in the Ameri-cas, pages 223?231.Rion Snow, Brendan O?Connor, Daniel Jurafsky, andAndrew Y. Ng.
2008.
Cheap and fast?but is itgood?
: Evaluating non-expert annotations for naturallanguage tasks.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing,pages 254?263.
Association for Computational Lin-guistics.Andreas Stolcke.
2002.
SRILM - an extensible languagemodeling toolkit.
In Proceedings of Spoken LanguageProcessing, pages 901?904.Billy Wong and Chunyu Kit.
2009.
ATEC: automaticevaluation of machine translation via word choice andword order.
Machine Translation, 23(2-3):141?155.1035
