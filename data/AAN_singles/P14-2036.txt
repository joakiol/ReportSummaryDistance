Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 218?223,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsA Novel Content Enriching Model for Microblog Using News CorpusYunlun Yang1, Zhihong Deng2?, Hongliang Yu3Key Laboratory of Machine Perception (Ministry of Education),School of Electronics Engineering and Computer Science,Peking University, Beijing 100871, China1incomparable-lun@pku.edu.cn2zhdeng@cis.pku.edu.cn3yuhongliang324@gmail.comAbstractIn this paper, we propose a novel model forenriching the content of microblogs by ex-ploiting external knowledge, thus improv-ing the data sparseness problem in shorttext classification.
We assume that mi-croblogs share the same topics with ex-ternal knowledge.
We first build an opti-mization model to infer the topics of mi-croblogs by employing the topic-word dis-tribution of the external knowledge.
Thenthe content of microblogs is further en-riched by relevant words from externalknowledge.
Experiments on microblogclassification show that our approach iseffective and outperforms traditional textclassification methods.1 IntroductionDuring the past decade, the short text represen-tation has been intensively studied.
Previous re-searches (Phan et al, 2008; Guo and Diab, 2012)show that while traditional methods are not sopowerful due to the data sparseness problem, somesemantic analysis based approaches are proposedand proved effective, and various topic models areamong the most frequently used techniques in thisarea.
Meanwhile, external knowledge has beenfound helpful (Hu et al, 2009) in tackling the da-ta scarcity problem by enriching short texts withinformative context.
Well-organized knowledgebases such as Wikipedia and WordNet are com-mon tools used in relevant methods.Nowadays, most of the work on short text fo-cuses on microblog.
As a new form of short tex-t, microblog has some unique features like infor-mal spelling and emerging words, and many mi-croblogs are strongly related to up-to-date topicsas well.
Every day, a great quantity of microblogs?Corresponding authormore than we can read is pushed to us, and find-ing what we are interested in becomes rather dif-ficult, so the ability of choosing what kind of mi-croblogs to read is urgently demanded by commonuser.
Such ability can be implemented by effectiveshort text classification.Treating microblogs as standard texts and di-rectly classifying them cannot achieve the goal ofeffective classification because of sparseness prob-lem.
On the other hand, news on the Internet isof information abundance and many microblogsare news-related.
They share up-to-date topicsand sometimes quote each other.
Thus, externalknowledge, such as news, provides rich supple-mentary information for analysing and mining mi-croblogs.Motivated by the idea of using topic model andexternal knowledge mentioned above, we presentan LDA-based enriching method using the newscorpus, and apply it to the task of microblog clas-sification.
The basic assumption in our model isthat news articles and microblogs tend to share thesame topics.
We first infer the topic distributionof each microblog based on the topic-word distri-bution of news corpus obtained by the LDA esti-mation.
With the above two distributions, we thenadd a number of words from news as additionalinformation to microblogs by evaluating the relat-edness of between each word and microblog, sincewords not appearing in the microblog may still behighly relevant.To sum up, our contributions are:(1) We formulate the topic inference problem forshort texts as a convex optimization problem.
(2) We enrich the content of microblogs by infer-ring the association between microblogs andexternal words in a probabilistic perspective.
(3) We evaluate our method on the real dataset-s and experiment results outperform the base-line methods.2182 Related WorkBased on the idea of exploiting external knowl-edge, many methods are proposed to improve therepresentation of short texts for classification andclustering.
Among them, some directly utilizethe structure information of organized knowledgebase or search engine.
Banerjee et al (2007) usethe title and the description of news article as twoseparate query strings to select related concept-s as additional feature.
Hu et al (2009) presenta framework to improve the performance of shorttext clustering by mining informative context withthe integration of Wikipedia and WordNet.However, to better leverage external resource,some other methods introduce topic models.
Phanet al (2008) present a framework including anapproach for short text topic inference and addsabstract words as extra feature.
Guo and Diab(2012) modify classic topic models and propos-es a matrix-factorization based model for sentencesimilarity calculation tasks.Those methods without topic model usually re-ly greatly on the performance of search system orthe completeness of knowledge base, and lack in-depth analysis for external resources.
Comparedwith our method, the topic model based method-s mentioned above remain in finding latent spacerepresentation of short text and ignore that rele-vant words from external knowledge are informa-tive as well.3 Our ModelWe formulate the problem as follows.
LetEK = {de1, .
.
.
, deMe} denote external knowl-edge consisting of Medocuments.
Ve={we1, .
.
.
, weNe} represents its vocabulary.
LetMB = {dm1, .
.
.
, dmMm} denote microblog set andits vocabulary is Vm= {wm1, .
.
.
, wmNm}.
Ourtask is to enrich each microblog with additionalinformation so as to improve microblog?s repre-sentation.The model we proposed mainly consists of threesteps:(a) Topic inference for external knowledge byrunning LDA estimation.
(b) Topic inference for microblogs by employingthe word distributions of topics obtained fromstep (a).
(c) Select relevant words from external knowl-edge to enrich the content of microblogs.3.1 Topic Inference for External KnowledgeWe do topic analysis for EK using LDA esti-mation (Blei et al, 2003) in this section and wechoose LDA as the topic analysis model becauseof its broadly proved effectivity and ease of under-standing.In LDA, each document has a distribution overall topics P (zk|dj), and each topic has a distri-bution over all words P (wi|zk), where zk, djandwirepresent the topic, document and word respec-tively.
The optimization problem is formulated asmaximizing the log likelihood on the corpus:max?i?jXijlog?kP (zk|dj)P (wi|zk) (1)In this formulation, Xijrepresents the term fre-quency of word wiin document dj.
P (zk|dj)and P (wi|zk) are parameters to be inferred, cor-responding to the topic distribution of each doc-ument and the word distribution of each topic re-spectively.
Estimating parameters for LDA by di-rectly and exactly maximizing the likelihood ofthe corpus in (1) is intractable, so we use GibbsSampling for estimation.After performing LDA model (K topics) esti-mation on EK , we obtain the topic distribution-s of document dej(j = 1, .
.
.
,Me), denoted asP (zek|dej) (k = 1, .
.
.
,K), and the word distri-bution of topic zek(k = 1, .
.
.
,K), denoted asP (wei|zek) (i = 1, .
.
.
, Ne).
Step (b) greatly re-lies on the word distributions of topics we haveobtained here.3.2 Topic Inference for MicroblogIn this section, we infer the topic distribution ofeach microblog.
Because of the assumption thatmicroblogs share the same topics with externalcorpus, the ?topic distribution?
here refers to a dis-tribution over all topics on EK .Differing from step (a), the method used fortopic inference for microblogs is not directly run-ning LDA estimation on microblog collection butfollowing the topics from external knowledge toensure topic consistence.
We employ the worddistributions of topics obtained from step (a), i.e.P (wei|zek), and formulate the optimization prob-lem in a similar form to Formula (1) as follows:219maxP (zek|dmj)?i?jXijlog?kP (zek|dmj)P (wei|zek),(2)where Xijrepresents the term frequency of wordweiin microblog dmj, and P (zek|dmj) denote the dis-tribution of microblog dmjover all topics on EK .Obviously most Xijare zero and we ignore thosewords that do not appear in Ve.Compared with the original LDA optimizationproblem (1), the topic inference problem for mi-croblog (2) follows the idea of document gener-ation process, but replaces topics to be estimatedwith known topics from other corpus.
As a result,parameters to be inferred are only the topic distri-bution of every microblog.It is noteworthy that since the word distributionof every topic P (wei|zek) is known, Formula (2) canbe further solved by separating it into Mmsub-problems:maxP (zek|dmj)?iXijlog?kP (zek|dmj)P (wei|zek)for j = 1, .
.
.
,Mm(3)These Mmsubproblems correspond to the Mmmicroblogs and can be easily proved convexity.After solving them, we obtain the topic distribu-tions of microblog dmj(j = 1, .
.
.
,Mm), denotedas P (zek|dmj) (k = 1, .
.
.
,K).3.3 Select Relevant Words for MicroblogTo enrich the content of every microblog, we s-elect relevant words from external knowledge inthis section.Based on the results of step (a)&(b), we calcu-late the word distributions of microblogs as fol-lows:P (wei|dmj) =?kP (zek|dmj)P (wei|zek), (4)where P (wei|dmj) represents the probability thatword weiwill appear in microblog dmj.
In otherwords, though some words may not actually ap-pears in a microblog, there is still a probability thatit is highly relevant to the microblog.
Intuitively,this probability indicates the strength of associa-tion between a word and a microblog.
The worddistribution of every microblog is based on topicanalysis and its accuracy relies heavily on the ac-curacy of topic inference in step (b).
In fact, themore words a microblog includes, the more accu-rate its topic inference will be, and this can be re-garded as an explanation of the low efficiency ofdata sparseness problem.For microblog dmj, we sort all words byP (wei|dmj) in descending order.
Having knownthe top L relevant words according to the result ofsorting, we redefine the ?term frequency?
of everyword after adding these L words to microblog dmjas additional content.
Supposing these L wordsare wej1, wej2, .
.
.
, wejL, the revised term frequencyof word w ?
{wej1, .
.
.
, wejL} is defined as fol-lows:RTF (w, dmj) =P (w|dmj)?Lp=1P (wejp|dmj)?
L, (5)where RTF (?)
is the revised term frequency.As the Equation (5) shows, the revised term fre-quency of every word is proportional to probabili-ty P (wi|dmj) rather than a constant.So far, we can add these L words and their re-vised term frequency as additional information tomicroblog dmj.
The revised term frequency playsthe same role as TF in common text representationvector, so we calculate the TFIDF of the addedwords as:TFIDF (w, dmj) = RTF (w, dmj) ?IDF (w) (6)Note that IDF (w) is changed as arrival of newwords for each microblog.
The TFIDF vector ofa microblog with additional words is called en-hanced vector.4 Experiment4.1 Experimental SetupTo evaluate our method, we build our own dataset-s. We crawl 95028 Chinese news reports fromSina News website, segment them, and removestop words and rare words.
After preprocessing,these news documents are used as external knowl-edge.
As for microblog, we crawl a number ofmicroblogs from Sina Weibo, and ask unbiasedassessors to manually classify them into 9 cate-gories following the column setting of Sina News.Sina News: http://news.sina.com.cn/Sina Weibo: http://www.weibo.com/220After the manual classification, we remove shortmicroblogs (less than 10 words), usernames, linksand some special characters, then we segmen-t them and remove rare words as well.
Finally, weget 1671 classified microblogs as our microblogdataset.
The size of each category is shown in Ta-ble 1.Category #MicroblogFinance 229Stock 80Entertainment 162Military Affairs 179Technologies 204Digital Products 194Sports 195Society 214Daily Life 214Table 1: Microblog number of every categoryThere are some important details of our imple-mentation.
In step (a) of Section 3.1 we estimateLDA model using GibbsLDA++, a C/C++ imple-mentation of LDA using Gibbs Sampling.
In step(b) of Section3.2, OPTI toolbox on Matlab is usedto help solve the convex problems.
In the clas-sification tasks shown below, we use LibSVM asclassifier and perform ten-fold cross validation toevaluate the classification accuracy.4.2 Classification ResultsRepresentation Average AccuracyTFIDF vector 0.7552Boolean vector 0.7203Enhanced vector 0.8453Table 2: Classification accuracy with different rep-resentationsIn this section, we report the average preci-sion of each method as shown in Table 2.
Theenhanced vector is the representation generatedby our method.
Two baselines are TFIDF vec-tor (Jones, 1972) and boolean vector (word oc-currence) of the original microblog.
In the table,our method increases the classification accuracyGibbsLDA++: http://gibbslda.sourceforge.netOPTI Toolbox: http://www.i2c2.aut.ac.nz/Wiki/OPTI/SVM.NET: http://www.matthewajohnson.org/software/svm.htmlfrom 75.52% to 84.53% when considering addi-tional information, which means our method in-deed improves the representation of microblogs.4.3 Parameter Tuning4.3.1 Effect of Added Words0.8050.810.8150.820.8250.830.8350.840.8450.8550 100 150 200 300 400 500AverageAccuracyNumber of Added Words (L)Figure 1: Classification accuracy changes accord-ing to topics and added wordsThe experiment corresponding to Figure 1 is todiscover how the classification accuracy changeswhen we fix the number of topics (K = 100)and change the number of added words (L) in ourmethod.
Result shows that more added words donot mean higher accuracy.
By studying some cas-es, we find out that if we add too many words,the proportion of ?noisy words?
will increase.
Wereach the best result when number of added wordsis 300.4.3.2 Effect of Topic Number0.820.8250.830.8350.840.8450.8550 100 200 300AverageAccuracyNumber of Topics (K)Figure 2: Classification accuracy changing ac-cording to the number of topicsThe experiment corresponding to Figure 2 is todiscover how the classification accuracy changeswhen we fix the number of added words (L =221Microblog (Translated) Top Relevant Words (Translated)Kim Jong Un held an emergency meeting this morn-ing, and commanded the missile units to prepare forattacking U.S. military bases at any time.South Korea, America, North Korea, work,safety, claim, military, exercise, united, reportShenzhou Nine will carry three astronauts, includingthe first Chinese female astronaut, and launch in aproper time during the middle of June.day, satellite, launch, research, technology,system, mission, aerospace, success, Chang?eTwoTable 3: Case study (Translated from Chinese)300) and change the number of topics (K) inour method.
As we can see, the accuracy doesnot grow monotonously as the number of topic-s increases.
Blindly enlarging the topic numberwill not improve the accuracy.
The best result isreached when topic number is 100, and similar ex-periments adding different number of words showthe same condition of reaching the best result.4.3.3 Effect of Revised Term Frequency0.8050.810.8150.820.8250.830.8350.840.8450.8550 100 150 200 300 400 500AverageAccuracyNumber of Added Words (L)Using RTF Using TFFigure 3: Classification accuracy changing ac-cording to the redefinition of term frequencyThe experiment corrsponding to Figure 3 is todiscover whether our redefining ?term frequency?as revised term frequency in step (c) of Section3.3 will affect the classification accuracy and how.The results should be analysed in two aspects.
Onone hand, without redefinition, the accuracy re-mains in a stable high level and tends to decreaseas we add more words.
One reason for the de-creasing is that ?noisy words?
have a increasingnegative impact on the accuracy as the propor-tion of ?noisy words?
grows with the number ofadded words.
On the other hand, the best resultis reached when we use the revise term frequen-cy.
This suggests that our redefinition for term fre-quency shows better improvement for microblogrepresentation under certain conditions, but is notoptimal under all situations.4.4 Case StudyIn Table 3, we select several cases consisting ofmicroblogs and their top relevant words .In the first case, we successfully find the countryname according to its leader?s name and limitedinformation in the sentence.
Other related coun-tries and events are also selected by our model asthey often appear together in news.
In the othercase, relevant words are among the most frequent-ly used words in news and have close semantic re-lations with the microblogs in certain aspects.As we can see, based on topic analysis, ourmodel shows strong ability of mining relevan-t words.
Other cases show that the model can befurther improved by removing the noisy and mean-ingless ones among added words.5 Conclusion and Future WorkWe propose an effective content enriching methodfor microblog, to enhance classification accuracy.News corpus is exploited as external knowledge.As for techniques, our method uses LDA as itstopic analysis model and formulates topic infer-ence for new data as convex optimization prob-lems.
Compared with traditional representation,enriched microblog shows great improvement inclassification tasks.As we do not control the quality of added words,our future work starts from building a filter to se-lect better additional information.
And to make themost of external knowledge, better ways to buildtopic space should be considered.AcknowledgmentsThis work is supported by National Natural Sci-ence Foundation of China (Grant No.
61170091).222ReferencesBanerjee, S., Ramanathan, K., and Gupta, A.
2007,July.
Clustering short texts using wikipedia.
In Pro-ceedings of the 30th annual international ACM SI-GIR conference on Research and development in in-formation retrieval (pp.
787-788).
ACM.Blei, D. M., Ng, A. Y., and Jordan, M. I.
2003.
LatentDirichlet Allocation.
In Journal of machine Learn-ing research, 3, 993-1022.Bollegala, D., Matsuo, Y., and Ishizuka, M. 2007.Measuring semantic similarity between words usingweb search engines.
www, 7, 757-766.Boyd, S. P., and Vandenberghe, L. 2004.
Convex opti-mization.
Cambridge university press.Gabrilovich, E., and Markovitch, S. 2007, January.Computing Semantic Relatedness Using Wikipedia-based Explicit Semantic Analysis.
In IJCAI (Vol.
7,pp.
1606-1611).Guo,W., and Diab, M. 2012, July.
Modeling sentencesin the latent space.
In Proceedings of the 50th An-nual Meeting of the Association for ComputationalLinguistics: Long Papers-Volume 1 (pp.
864-872).Guo, W., and Diab, M. 2012, July.
Learning the latentsemantics of a concept from its definition.
In Pro-ceedings of the 50th Annual Meeting of the Associ-ation for Computational Linguistics: Short Papers-Volume 2 (pp.
140-144).Hu, X., Sun, N., Zhang, C., and Chua, T. S. 2009,November.
Exploiting internal and external seman-tics for the clustering of short texts using worldknowledge.
In Proceedings of the 18th ACM con-ference on Information and knowledge management(pp.
919-928).
ACM.Jones, K. S. 1972.
A statistical interpretation of termspecificity and its application in retrieval.
In Journalof documentation, 28(1), 11-21Phan, X. H., Nguyen, L. M., and Horiguchi, S. 2008,April.
Learning to Classify Short and Sparse Text &Web with Hidden Topics from Large-scale Data Col-lections.
In Proceedings of the 17th internationalconference on World Wide Web (pp.
91-100).
ACM.Sahami, M., and Heilman, T. D. 2006, May.
A web-based kernel function for measuring the similarity ofshort text snippets.
In Proceedings of the 15th inter-national conference on World Wide Web (pp.
377-386).
ACM.Zubiaga, A., and Ji, H. 2013, May.
Harnessing we-b page directories for large-scale classification oftweets.
In Proceedings of the 22nd internationalconference on World Wide Web companion (pp.
225-226).
InternationalWorld WideWeb Conferences S-teering Committee.223
