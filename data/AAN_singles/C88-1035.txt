Lexical Functional Grammar in SpeechKlaus Jiirgen EngelbergFraunhofer Institut IAOHolzgartenstr.
17D 7000 StuttgartWest GermanyRecognitionAbstract~he syntax component of the speech recognition system IKAROS t isdescribed, clhe usefidness of a probabilistic Le~jcal Functional Grammarboth for cow,straining bottom-up hypotheses and top-down predicting isshowtL1.
In t roduct ionThe most important  problem in all speechrecognition systems is the inherent uncertaintyassociated with the acoustic-phonetic decodingprocess at the basis of such a system.
One approachtaken in many existing system to overcome thesedifficulties is to integrate higher level knowledgesources that have a certain a-priori knowledgeabout specific problem areas.
Following this line ofthought, the system architecture adopted in theIKAROS-pro ject  assumes di f ferent levels ofknowledge ( representat ions)  e.g.
acoust icparameters ,  phonemes ,  words,  const i tuentstructures etc.
The interaction between theseknowledge sources is control led by a centralblackboard control module (like in HEARSAY II).This whole system is embedded in an object-oriented environment and communication betweenthe modules is realized by message passing.Within IKAROS particular attention is given to theprob lem of using the same knowledgerepresentations both for data-driven bottom-uphypothesiz ing and expectat ion-dr iven top-downprediction and to the problem of providing ageneral framework of uncertainty management.According to this rationale, the main purpose of thesyntax component is to constrain the number ofword sequences to be dealt with in the recognitionprocess and to predict or insert poorly recognizedwords.
Grammaticaless in itself is of no importanceto us.
Quite to the contrary, in a real- l iveapplication a certain degree of error tolerance is adesired "effect.1 Research in IKAROS is partially funded by the ESPRITprogramme of th6 European Community under contractP954In the syntax component of IKAROS we workwithin the formal framework of a probab i l i s t i cLexical Functional Grammar.
Certain modificationsto the formalism as expounded in /Bresnan1982/have been made to suit our purposes.
We use as animplementation an event-driven chart-parser thatis capable of all the necessary parsing strategies i.e.top-down, bottom-up and left-to-right and right-to-left parsing.2.
Probabil ist ic context.
f ree Grammars2.1.
The event-driven parserThe interaction between the blackboard managerand the syntax component is roughly as follows:the blackboard manager sends a message to thesyntax component indicating that a particular wordhas been recognized (or rather "hypothesized") at acertain position in the input stream (or in chartoparser terminology with starting and ending verte~number) together with a certain numericalconf idence  score.
The syntax componentaccumulates information about these (in arbitraryorder) incoming word hypotheses and in turn postshypotheses about predicted and recognized wordsor constituents on the blackboard.
The job of thesyntax component  now is to decide betweenseveral conf l ict ing (or competing) const i tuentstructures tored in the chart i.e.
to choose the bestgrammatical structure.2.2.
The fo rmal i smWe assume a probabilistic context-free grammarG=<VN,  VT, R ,S>:VN denotes the nonterminal vocabularyNonterminals are denoted by A, B, C ....strings of these by X, Y, Z...lexical categories by P, Q .
.
.
.VT denotes the terminal vocabularyterminals (words) denoted by a, b, c .
.
.
.
.strings of both types of symbols aredenoted by w, x, y, z .R denotes the set of rules {R1, R2 .
.
.
.
.
Ri}with each rule having the formatRi = < Ai -> Xi ,  qi >where qi indicates the a-priori172Z p( xi a Q Yi <-Ti- S ) probability for the application of thisi ru le  ,,c.~ ,~.
~., n,, ~= .
.
.
.
.
.
.
.
.
.
.
.
.
.
.S denotes the initial symbolLexical rtdes have the formatLj = <Aj->tj,oj>In a probabil ist ic grammar, there is no clearcutd ichotomy between grammat ica l  andungrammatical sentences.
Rather, we can deviseour langt~age model in such a way that morefrequent phrases receive a higher probability thanless frequ,mt ones.
Even different word orders willhave different probabilit ies.Now we are able to compute the a -pr io r ip r o b a b i 1 i t y of a (partial) derivation T startingwith the symbol S in the following recursivemanner :p(S <- s)  -- Ip(xYz <-T- S)= p(xAy<-S)*q  ,if there is a rule < A -> Y ,  q> in RIn our implementation, these a-priori probabilitiesare weighted with the scores del ivered forind iv idua l  words by the acoust i c -phonet iccomponem to yield accumulated grammatical-acoustic scores for whole phrases.Quite the opposite problem arises in the analysiscontext when we ask for the (relative) probabilityof a given string y being derived by a particularder ivat ion  Tk (when there may be severaldi f ferent der ivat ion histories Ti for the samestring).We may comPute the a -poster io r i  der ivat ionprobab i l i ty  of a string y by using Bayes" Theoremp(S<-Tk  o y )  =p( y <-Tk- S)Z P(  Y <-T i -  S)iAs a specialization, this formula is of particularinterest if we want to predict e.g.
words orcategor ies fo l lowing or preceding a alreadyrecognized word etc.
(This is useful for "islandparsing" when only the most promising parsesshould be continued.
)Consequently, the a-posteriori probabil ity that thelexical category Q immediately follows the word "a"can be calculated asp(S <- xaQy ) :  p( wj a Pj zj <-Tj- S )JAll derivations appearing on the right side areminimal derivations for the substring "aQ" or "aPj"and the Pj 's range ow~r all lexical categories in G(In the formula, of course, we assume p(waPz <-- S)= 0 if the substring "alP" isn't derivable in G).
Thisformula ref lects  the common probab i l i s t i cassumption that the  derivation probabil ity of asubstring is the sum of all distinct alternativederivation probabil it ies of this string (if there ismore than one possibility).2 .3.
Example Grammar  G1The fo l lowing toy grammar is designed todemonstrate the formalism.
That it generates manyunwanted sentences need not concern us here.Our grammar has the following rulesS -> # NP V NP #,  1.0NP -> Q N , 0.7NP -> Q , 0.3Lexical rulesN-> board 0.2 V-> board 0.3N-> boards 0.2 V-> boards  0.3N-> men 0.3 V-> boarded 0.3N-> man 0.3 V-> man 0.1Q-> some 0.4 0-> the 0.6Let us assume the word "board" has beenrecognized somewhere in the input stream (but notat its end).
We obtain the fol lowing a-prioriprobabi l i t ies for minimal derivat ions involving"board" with a subsequent lexical categoryp( # Q board V NP # <- S) = 0.7 * 0.2p( # NP board Q N # <- S) = 0.3 * 0.7p( # NP board Q # <- S) = 0.3 * 0.3Actually, there are no more minimal derivations ofthe desired type.
We may now calculate the a-posteriori  probabi l i ty of V fol lowing the word"board"p(# x board V y # <- S) ---"0 .7*0 .20 .7*0 .2+0.3*0 .7+0.3*0 .3  = 0.32173The a-poster ior i  probabi l i ty of the other("conflicting") possibility i.e.
that a Q follows theword ',board" isp(# x board Q y # <- S)= 1 - 0.32 = 0.68In our implementat ion these a-poster ior iprobabilities can easily be computed from thederivation probabilities attached to the activeedges in the chart parser.3.
Lexical Funct ional  GrammarLFG assumes two layers of grammatical descriptionof sentences i.e.
the constituent structure level andthe functional structure level.
The constituentstructure level caters for the surface orientedrealization of sentences (e.g.
word order etc.
)whereas the fuctional structure level is concernedwith more abstract and supposedly universalgrammatical functions like SUBJect, OBject, OBLiqueobject and the like.
Lexical functional Grammarsuse context-free rules (like in the example above)coupled with functional schemata.
These schemata(normally) relate F-structures associated withcorresponding mother and daughter nodes in a c-structure (roughly speaking).
The functionalschemata attached to lexical items so-calledsemantic forms may include grammatical orsemantic features, but more important, they allowa case frame notation (in particular important withverbs).
It is these case frames (or valencies) thatmake LFG in particular attractive for predictionpurposes in speech recognition.,in the implementation of the LFG system F-structures are incrementally constructed by usingunif ication, i.e.
a process that accumulatesinformation in structures and never backtracks.This process is independent of the particular orderin which these structures are constructed animportant aspect in speech recognition where thereis inherently no predetermined order  of theoperations to follow.3.1.
Example Grammar  G2The following small grammar fragment should givea rough impression of the basic features of ourapproach.
Trivial rules are omitted.
Since we workwithin a railway inquiry environment we takespecial care of locative and temporal expressions.As an example, we have a special lexical categoryfor place and station names (N-lot) and for timeintervalls like "day" and "week" ete (N-temp).
Aparticular problem in LFG is the treatment of(oblique) objects and free adjuncts.
In our context,we assume all temporal modifiers to be freead juncts  and verbs to be subeategorizable foroblique locat  iv  e objects only (besides thenormal arguments SUB J, OBJ etc.).
Our approachdiffers from /Bresnan 1982/ in various aspects.
(Technically speaking, functional schemata of the!p7t,form ( $ ($.
.
. )
)  = $ pose certain problems forstructure prediction (generation).
So we avoidthem.S ->{AUX} NP VP { PP-temp}(1' SUBJ ) = $ ($ADJUNCT)=$Temporal propositional phrases are treated asadjuncts.S -> XP AUX S$OBLLOC =This is the rule for questions with a questionelement in front.VP -> V {NP}(1" OBJ ) = $VP -> V { PP-loc }($ OBLLOC) = $Verbs take a direct or oblique lo c a t i v e object.PP-loc-> P NP(1" OBJ ) = $Lexiconcall V (1' PRED)="CALL<($SUBJ) (I"OBLLOC)>"($ OBLLOC OBJ PCASE) = LocThis lexical rule is viewed in the bottom-upanalysis process as predicting a subject and anoblique object to appear somewhere in thesentence.depart V (I"PRED)="DEPART<(I"SUBJ)(tOBLLOC)>"($ OBLLOC OBJ PCASE) = GoalThis entry predicts a subject and an oblique objectwhich denotes a goal (like in "depart ...for..." or"depart...to...").arrive V (~PRED)="ARRIVE<(i"SUBL)(I'OBLLOC)>"(1" OBLLOC OBJ PCASE) = Sourcea t P-loc (1" PRED ) = "AT<('\[' OBJ )>"(1' OBJ PEASE ) = Loeto P-Ioc (I" PRED ) = "TO<('I" OBJ )>"(1" OBJ PCASE ) = Goalfor P-loc (1" PRED ) = "FOR<(1' OBJ)>"(1' OBJ PCASE ) = Goalwhere  approach("semant ic  grammars")  and a purelysurface oriented word order approach.XP ($ PRED ) = "WHERE"f 1 Loc (1" OULLOC OBJ PCASE)= {Goal}This rule reflects the fact that "where" may playthe role of an oblique location or goal object (like inexamples "Where does the  train stop" and "Wheredoes the train go" but not in "From where does thetrain arrive").Coventry N- lot  ($ PRED) ="CO ~VENTRY ''This is an example entry for a place name.day  N- temp (1" PRED ) = "DAY"For the analysis of the sentence "where did thetrain call" we get the c-structure\[ S \[ XP where\] \[AUX did\]\[ S\[Npthe train\] \[Vp\[ V call\]\]\]\]and the f-structureSUBJPREDOBLLDC= the train= "CALL<0" SUBJ ) (1` OBLLOC )>"PRED = "WHERE"OBJ PCASE = LocIn order lo demonstrate the hole-fil l ing capabilitiesof this formalism we consider the phrase "call *Coventry" with * indicating a word that was notrecognized by the acoustic-phonetic component.
Wewould get the c-structure\[VP \[V calt \[PP-loc \[P-loc * \] \[ N-loc Coventry\]\]\]\]and the t-structurePRED ="CALL<(1' SUBJXT OBLLOC)>"OBIZOC PREI) = "Coventry"OBJ PCASE = LocThis little example shows how our LFG-approach iscapable to predict certain features of constituentsthat might appear somewhere  in the sentence.Now, another important point is that L F Gsubcategorizes for grammatical functions not forgrammatical categories.
That means we have acertain f lexibi l i ty  at hand in that the samegrammatical function (e.g.
the Location deep case)may be realized in different ways (compare forinstance the example sentence in L(G2) "Where didthe train call" with a WH-Adverb vs. "The traincalls at Coventry" with an oblique object).
As theexample clearly shows, grammatical functions inLFG provide an additional intermediate level ofdescr ip t ion  between a semant ic  featureSince there are sentences that are syntacticallyquite acceptable (i.e.
on the constituent structurelevel) but devious in semantic terms LFG imposes 3addit ional  wel l - formedness  condit ions on F-structures.
We have to assess these conditions fromthe pragmatic viewpoint of a real-l ife application(e.g.
with respect to predictive power and errortolerance)(i) Functional Uniqueness (no conflicting values foran attribute allowed)This is a useful principle since we want to excludefeature "clashes".
So we would like to exclude"Where did the train stops" (tense clash) but wewould not want to undertake great an effort inorder to exclude "Where does the train stops"("since it is clear what is meant!").
(ii) Completeness (A f-structure must contain allthe governable grammatical functions that itspredicate governs)This is an awkward condition.
First of all, given theuncertainty in speech re, cognition it is hard todecide at any rate when the analysis of several(conflicting) utterances is complete.
In addition, webelieve that there are never ending problems withthe distinction between obligatory and optionalarguments of a verb.
Hence we decided that allarguments in a semantic form should be regardedas optional (Only SUBJ is obligatory).
A f-structurethat contains more grammatical functions (out ofthe list given in the predicate) is grammaticalbetter than one with less functions in itself.
(iii) Coherence ( There must be no grammaticalfunction in a f-structure that is not governed by apred icate)This is a good principle since we want to excludesuperfluous arguments.4.
Conc lus ionsWe showed tile usefulness of a probabilistic lexicalfunctional grammar for a speech recognit ionsystem by demonstrat ing its two re lat ive lyindependent  const ra in ing  and pred ic t ingmechanisms : the constraining power of a context-free grammar (which allows global predictionsfrom a global point of view) and of valency-or iented lex icon (which a l lows bot tom-uppredictions from a local point of view).
In addition,we gave an account of the probabil ity treatmentwithin this framework.1755.
Re ferencesBlock, H-U.,Hunze, R.,"Incremental Construction of C- and F-Structures in a LFG-Parser",Proc.
COLING-86, Bonn, 1986, p. 490 - 493Bresnan, J.
(ed) ,The Mental Representation ofGrammatical  Relations,MIT Press 1982Ermann, L., Hayes-Roth.F., Lesser,V., Ray Reedy,"The Hearsay-I I  Speech UnderstandingSystem: Integrating Knowledge toResolve Uncertainty",Computing Surveys, v.12, n.2, June 1980Levelt, W,Formal Grammars in Linguistics andPsycho l ingu is t i cs ,Vol 1, Mouton 1974Winograd, T.,Language as a Cognitive Process,Addison-Wesley, 1983176
