Improving Statistical Language ModelPerformance with AutomaticallyGenerated Word HierarchiesJohn G. McMahon*The Queen's University of BelfastFrancis J. SmithAn automatic word-classification system has been designed that uses word unigram and bigramfrequency statistics to implement a binary top-down form of word clustering and employs anaverage class mutual information metric.
Words are represented as structural tags--n-bit num-bers the most significant bit-patterns of which incorporate class information.
The classificationsystem has revealed some of the lexical structure of English, as well as some phonemic and se-mantic structure.
The system has been compared---directly and indirectly--with other recentword-classification systems.
We see our classification as a means towards the end of construct-ing multilevel class-based interpolated language models.
We have built some of these modelsand carried out experiments hat show a 7% drop in test set perplexity compared to a standardinterpolated trigram language model.1.
IntroductionMany applications that process natural language can be enhanced by incorporatinginformation about he probabilities of word strings; that is, by using statistical languagemodel information (Church et al 1991; Church and Mercer 1993; Gale, Church, andYarowsky 1992; Liddy and Paik 1992).
For example, speech recognition systems oftenrequire some model of the prior likelihood of a given utterance (Jelinek 1976).
Forconvenience, the quality of these components can be measured by test set perplexity,PP (Bahl, Jelinek, and Mercer 1983; Bahl et al 1989; Jelinek, Mercer, and Roukos 1990),in spite of some limitations (Ueberla 1994): PP = P(wlN) - ~, where there are N words inthe word stream (w~/and ib is some estimate of the probability of that word stream.Perplexity is related to entropy, so our goal is to find models that estimate a lowperplexity for some unseen representative sample of the language being modeled.Also, since entropy provides a lower bound on the average code length, the project ofstatistical language modeling makes some connections with text compression--goodcompression algorithms correspond to good models of the source that generated thetext in the first place.
With an arbitrarily chosen standard test set, statistical languagemodels can be compared (Brown, Della Pietra, Mercer, Della Pietra, and Lai 1992).
Thisallows researchers to make incremental improvements o the models (Kuhn and Mori1990).
It is in this context hat we investigate automatic word classification; also, somecognitive scientists are interested in those features of automatic word classificationthat have implications for language acquisition (Elman 1990; Redington, Chater, andFinch 1994).One common model of language calculates the probability of the ith word wi* Department of Computer Science, The Queen's University of Belfast, Belfast BT7 1NN, NorthernIreland.
E-maih {J.McMahon,FJ.Smith}@qub.ac.uk@ 1996 Association for Computational LinguisticsComputational Linguistics Volume 22, Number 2in a test set by considering the n - 1 most recent words (Wi_n+l,Wi_n .
.
.
.
,Wi_l>, ori -1  (wi_,+l> in a more compact notation.
The model is finitary (according to the Chomskyhierarchy) and linguistically naive, but it has the advantage of being easy to constructand its structure allows the application of Markov model theory (Rabiner and Juang1986).Much work has been carried out on word-based n-gram models, although thereare recognized weaknesses in the paradigm.
One such problem concerns the way thatn-grams partition the space of possible word contexts.
In estimating the probabilityof the ith word in a word stream, the model considers all previous word contexts tobe identical if and only if they share the same final n - 1 words.
This simultaneouslyfails to differentiate some linguistically important contexts and unnecessarily fracturesothers.
For example, if we restrict our consideration to the two previous words in astream--that is, to the trigram conditional probability estimate P(wilwi-~)--then thesentences:(1) a.
The boys eat the sandwiches quickly.and(2) a.
The cheese in the sandwiches i delicious.contain points where the context is inaccurately considered identical.
We can illustratethe danger of conflating the two sentence contexts by considering the nonsentences:(1) b.
*The boys eat the sandwiches i delicious.and(2) b.
"The cheese in the sandwiches quickly.There are some techniques to alleviate this problem--for example O'Boyle's n-gram(n > 3) weighted average language model (O'Boyle, Owens, and Smith 1994).
A sec-ond weakness of word-based language models is their unnecessary fragmentation ofcontexts--the familiar sparse data problem.
This is a main motivation for the multilevelclass-based language models we shall introduce later.
Successful approaches aimed attrying to overcome the sparse data limitation include backoff (Katz 1987), Turing-Goodvariants (Good 1953; Church and Gale 1991), interpolation (Jelinek 1985), deleted esti-mation (Jelinek 1985; Church and Gale 1991), similarity-based models (Dagan, Pereira,and Lee 1994; Essen and Steinbiss 1992), Pos-language models (Derouault and Meri-aldo 1986) and decision tree models (Bahl et al 1989; Black, Garside, and Leech 1993;Magerman 1994).
We present an approach to the sparse data problem that sharessome features of the similarity-based approach, but uses a binary tree representationfor words and combines models using interpolation.Consider the word <boys> in (la) above.
We would like to structure our entirevocabulary around this word as a series of similarity layers.
A linguistically significantlayer around the word <boys> is one containing all plural nouns; deeper layers containmore semantic similarities.If sentences (la) and (2a) are converted to the word-class treams <determinernoun verb determiner noun adverb> and <determiner noun preposition deter-miner noun verb adjective> respectively, then bigram, trigram, and possibly even218McMahon and Smith Improving Statistical Language Modelshigher n-gram statistics may become available with greater eliability for use as con-text differentiators (although Sampson \[1987\] suggests that no amount of word-classn-grams may be sufficient o characterize natural anguage fully).
Of course, this stillfails to differentiate many contexts beyond the scope of n-grams; while n-gram modelsof language may never fully model ong-distance linguistic phenomena, we argue thatit is still useful to extend their scope.In order to make these improvements, we need access to word-class information(Pos information \[Johansson et al 1986; Black, Garside, and Leech 1993\] or semanticinformation \[Beckwith et al 1991\]), which is usually obtained in three main ways:Firstly, we can use corpora that have been manually tagged by linguistically informedexperts (Derouault and Merialdo 1986).
Secondly, we can construct automatic part-of-speech taggers and process untagged corpora (Kupiec 1992; Black, Garside, and Leech1993); this method boasts a high degree of accuracy, although often the construction ofthe automatic tagger involves a bootstrapping process based on a core corpus whichhas been manually tagged (Church 1988).
The third option is to derive a fully au-tomatic word-classification system from untagged corpora.
Some advantages of thislast approach include its applicability to any natural anguage for which some corpusexists, independent of the degree of development of its grammar, and its parsimo-nious commitment to the machinery of modern linguistics.
One disadvantage is thatthe classes derived usually allow no linguistically sensible summarizing label to beattached (Schfitze \[1995\] is an exception).
Much research as been carried out recentlyin this area (Hughes and Atwell 1994; Finch and Chater 1994; Redington, Chater, andFinch 1993; Brill et al 1990; Kiss 1973; Pereira and Tishby 1992; Resnik 1993; Ney,Essen, and Kneser 1994; Matsukawa 1993).
The next section contains a presentation ofa top-down automatic word-classification algorithm.2.
Word Classification and Structural TagsMost statistical language models making use of class information do so with a singlelayer of word classes--often at the level of common linguistic classes: nouns, verbs,etc.
(Derouault and Merialdo 1986).
In contrast, we present he structural tag rep-resentation, where the symbol representing the word simultaneously represents theclassification of that word (McMahon and Smith \[1994\] make connections betweenthis and other representations; Black et al \[1993\] contains the same idea applied to thefield of probabilistic parsing; also structural tags can be considered a subclass of themore general tree-based statistical language model of Bahl et al \[1989\]).
In our model,each word is represented by an s-bit number the most significant bits of which corre-spond to various levels of classification; so given some word represented as structuraltag w, we can gain immediate access to all s levels of classification of that word.Generally, the broader the classification granularity we chose, the more confidentwe can be about the distribution of classes at that level, but the less information thisdistribution offers us about next-word prediction.
This should be useful for dealingwith the range of frequencies of n-grams in a statistical language model.
Some n-gramsoccur very frequently, so word-based probability estimates can be used.
However,as n-grams become less frequent, we would prefer to sacrifice predictive specificityfor reliability.
Ordinary Pos-language models offer a two-level version of this ideal;it would be preferable if we could defocus our predictive machinery to some stagesbetween all-word n-grams and Pos n-grams when, for example, an n-gram distributionis not quite representative enough to rely on all-word n-grams but contains predictivelysignificant divisions that would be lost at the relatively coarse POS level.
Also, forrare n-grams, even Pos distributions uccumb to the sparse data problem (Sampson219Computational Linguistics Volume 22, Number 21987); if very broad classification i formation was available to the language-modelingsystem, coarse-grained predictions could be factored in, which might improve theoverall performance of the system in just those circumstances.In many word-classification systems, the hierarchy is not explicitly representedand further processing, often by standard statistical clustering techniques, is required;see, for example, Elman (1990), Schtitze (1993), Brill et al (1990), Finch and Chater(1994), Hughes and Atwell (1994), and Pereira and Tishby (1992).
With the structuraltag representation, each tag contains explicitly represented classification information;the position of that word in class-space can be obtained without reference to the posi-tions of other words.
Many levels of classification granularity can be made availablesimultaneously, and the weight which each of these levels can be given in, for ex-ample, a statistical language model, can alter dynamically.
Using the structural tagrepresentation, the computational overheads for using class information can be keptto a minimum.
Furthermore, it is possible to organize an n-gram frequency databaseso that close structural tags are stored near to each other; this could be exploited toreduce the search space explored in speech recognition systems.
For example, if thesystem is searching for the frequency of a particular noun in an attempt o find themost likely next word, then alternative words should already be nearby in the n-gramdatabase.
Finally, we note that in the current implementation of the structural tagrepresentation we allow only one tag per orthographic word-form; although many ofthe current word-classification systems do the same, we would prefer a structural tagimplementation that models the multimodal nature of some words more successfully.For example, ( l ight) can occur as a verb and as a noun, whereas our classificationsystem currently forces it to reside in a single location.Consider sentences (la) and (2a) again; we would like to construct a clusteringalgorithm that assigns some unique s-bit number to each word in our vocabularyso that the words are distributed according to some approximation of the layeringdescribed above that is, (boys) should be close to (people) and (is) should be closeto (eat).
We would also like semantically related words to cluster, so that, although(boys) may be near (sandwiches) because both are nouns, (gir ls)  should be evencloser to (boys) because both are human types.
In theory, structural tag representationscan be dynamically updated--for example, (bank) might be close to (r iver) in somecontexts and closer to (money) in others.
Although we could ,construct a useful setof structural tags manually (McMahon 1994), we prefer to design an algorithm thatbuilds such a classification.For a given vocabulary V, the mapping t initially translates words into their cor-responding unique structural tags.
This mapping is constructed by making randomword-to-tag assignments.The mutual information (Cover and Thomas 1991) between any two events x andy is:1 P(x,y)I(x,y) = og P(x )If the two events x and y stand for the occurrence of certain word-class unigrams in asample, say ci and cj, then we can estimate the mutual information between the twoclasses.
In these experiments, we use maximum likelihood probability estimates basedon a training corpus.
In order to estimate the average class mutual information for aclassification depth of s bits, we compute the average class mutual information:Ms(t) = ~_P(ci ,  cj) x log P(ci, cj) c,,c, P(ci)P(cj) (1)220McMahon and Smith Improving Statistical Language Modelswhere ci and cj are word classes and Ms(t) is the average class mutual information forstructural tag classification t at bit depth s. This criterion is the one used by Brown,Della Pietra, DeSouza, Lai, and Mercer (1992); Kneser and Ney (1993) show how it isequivalent to maximizing the bi-Pos-language model probability.
We are interested inthat classification which maximizes the average class mutual information; we call thist o and it is found by computing:Ms(t ?)
- maxMs(t) (2)tCurrently, no method exists that can find the globally optimal classification, but sub-optimal strategies exist that lead to useful classifications.
The suboptimal strategyused in the current automatic word-classification system involves selecting the locallyoptimal structure between t and t', which differ only in their classification of a singleword.
An initial structure is built by using the computer's pseudorandom numbergenerator to produce a random word hierarchy.
Its M(t) value is calculated.
Next,another structure, t r is created as a copy of the main one, with a single word movedto a different place in the classification space.
Its M(t t) value is calculated.
This secondcalculation is repeated for each word in the vocabulary and we keep a record ofthe transformation which leads to the highest M(t').
After an iteration through thevocabulary, we select that t' having the highest M(t ~) value and continue until nosingle move leads to a better classification.
With this method, words which at onetime are moved to a new region in the classification hierarchy can move back at alater time, if licensed by the mutual information metric.
In practice, this does happen.Therefore, each transformation performed by the algorithm is not irreversible withina level, which should allow the algorithm to explore a larger space of possible wordclassifications.The algorithm is embedded in a system that calculates the best classificationsfor all levels beginning with the highest classification level.
Since the structural tagrepresentation is binary, this first level seeks to find the best distribution of wordsinto two classes.
Other versions of the top-down approach are used by Pereira andTishby (1992) and Kneser and Ney (1993) to classify words; top-down procedures arealso used in other areas (Kirkpatrick, Gelatt, and Vecchi 1983).
The system of Pereiraand Tishby (1992; Pereira, Tishby, and Lee 1993) has the added advantage that classmembership is probabilistic rather than fixed.When the locally optimal two-class hierarchy has been discovered by maximizingMl(t), whatever later reclassifications occur at finer levels of granularity, words willalways remain in the level 1 class to which they now belong.
For example, if manynouns now belong to class 0 and many verbs to class 1, later subclassifications willnot influence the M1 (t) value.
This reasoning also applies to all classes = 2, 3. .
.
16(see Figure 1).We note that, in contrast with a bottom-up approach, a top-down system makesits first decisions about class structure at the root of the hierarchy; this constrainsthe kinds of classification that may be made at lower levels, but the first clusteringdecisions made are based on healthy class frequencies; only later do we start noticingthe effects of the sparse data problem.
We therefore xpect he topmost classificationsto be less constrained, and hopefully more accurate.
With a bottom-up approach, thereverse may be the case.
The tree representation also imposes its own constraints,mentioned later.This algorithm, which is O(V 3) for vocabulary size V, works well with the most221Computational Linguistics Volume 22, Number 2level s-3 ?level s-2 ?level s-1 -~level sClass M Class NFigure 1Top-down clustering.
The algorithm is designed so that, at a given level s, words will havealready been re-arranged atlevels s - 1, etc.
to maximize the average class mutualinformation.
Any alterations at level s will not bear on the classification achieved at s - 1.Therefore, a word in class M may only move to class N to maximize the mutualinformation--any other move would violate a previous level's classification.frequent words from a corpus1; however, we have developed a second algorithm, tobe used after the first, to allow vocabulary coverage in the range of tens of thousandsof word types.
This second algorithm exploits Zipf's law (1949)--the most frequentwords account for the majority of word tokens--by adding in low-frequency wordsonly after the first algorithm has finished processing high-frequency ones.
We makethe assumption that any influence that these infrequent words have on the first setof frequent words can be discounted.
The algorithm is an order of magnitude lesscomputationally intensive and so can process many more words in a given time.
Bythis method, we can also avoid modeling only a simplified subset of the phenomenain which we are interested and hence avoid the danger of designing systems that donot scale-up adequately (Elman 1990).
Once the positions of high-frequency wordshas been fixed by the first algorithm, they are not changed again; we can add anynew word, in order of frequency, to the growing classification structure by making 16binary decisions: Should its first bit be a 0 or a 1?
And its second?
Of our 33,360 wordvocabulary, we note that the most frequent 569 words are clustered using the main1 In a worst case analysis, the mutual  information metric will be O(V 2) and we need to evaluate the treeon V occasions--~ach t ime with one word reclassified; lower order terms (for example, the number  ofiterations at each level) can be ignored.
In practice, the mutual  information calculation is much  lessthan O(V 2) since there are far fewer than V 2 bigrams observed in our training text.222McMahon and Smith Improving Statistical Language Modelsalgorithm; the next 15,000 are clustered by our auxiliary algorithm and the remaining17,791 words are added to the tree randomly.
We add these words randomly dueto hardware limitations, though we notice that the 15,000th most frequent word inour vocabulary occurs twice only--a very difficult task for any classification system.The main algorithm takes several weeks to cluster the most frequent 569 words on aSparc-IPC and several days for the supplementary algorithm.3.
Word Classification PerformanceIn evaluating this clustering algorithm, we were interested to see if it could discoversome rudiments of the structures of language at the phonemic, syntactic, and semanticlevels; we also wanted to investigate he possibility that the algorithm was particularlysuited to English.To these ends, we applied our algorithm to several corpora.
It successfully dis-covered major noun-verb distinctions in a toy regular grammar introduced by Elman(1990), made near perfect vowel-consonant distinctions when applied to a phonemiccorpus and made syntactic and semantic distinctions in a Latin corpus (McMahon1994).
It also discovered some fine-grained semantic detail in a hybrid pos-word cor-pus.
However, classification groups tended to be dispersed at lower levels; we shalldiscuss this phenomenon with respect o the distribution of number words and offersome reasons in a later section.3.1 Clustering ResultsWe report on the performance of our top-down algorithm when applied to the mostfrequent words from an untagged version of the LOB corpus (Johansson et al 1986) andalso when applied to a hybrid word-and-class version of the LOB.
We used structuraltags 16 bits long and we considered the 569 most frequent words; this gave us 46,393bigrams to work with--all other word bigrams were ignored.
We present the followingfigures as illustrations of the clustering results: our main use for the classificationsystem will be as a way to improve statistical language models; we eschew any detaileddiscussion of the linguistic or cognitive relevance of the clustering results.
Illustrativeclusterings of this type can also be found in Pereira, Tishby, and Lee (1993), Brown,Della Pietra, Mercer, Della Pietra, and Lai (1992), Kneser and Ney (1993), and Brillet al (1990), among others.In Figure 2, we observe the final state of the classification, to a depth of five bits.Many syntactic and some semantic divisions are apparent--prepositions, pronouns,verbs, nouns, and determiners cluster--but many more distinctions are revealed whenwe examine lower levels of the classification.
For example, Figure 3 shows the sub-cluster of determiners whose initial structural tag is identified by the four-bit schema0000.
In Figure 4 we examine the finer detail of a cluster of nouns.
Here, some se-mantic differences become clear (we have internally ordered the words to make thesemantic relations easier to spot).
Many of the 25 groups listed in Figure 2 show thistype of fine detail.
It is clear, also, that there are many anomalous classifications, froma linguistic point of view.
We shall say more about this later.In a second experiment (see Figure 5), a hybrid version of the LOB corpus wascreated: we replaced each word and part-of-speech pair by the word alone if thepart-of-speech was a singular noun, the base form of a verb, or the third personsingular present ense of a verb; otherwise we replaced it by the part-of-speech.
Bydoing this, we hoped to lighten the burden of inducing syntactic structure in thevocabulary to see if the classification system could move beyond syntax and intosemantic lustering.
We considered that, of the word tokens replaced by their part-223Computational Linguistics Volume 22, Number 20- )~ 3 4 6 Britian John Sir all another both each her keep let make making many once several some such taking ten these this those too whom Dr Miss Mr Mrs a an any every his its my no our their whose yourI he it she there they we who you..... although but cent certainly even everything having how however indeed nor particularlyperhaps so sometimes then therefore though thus what whether which while why yet -*'- -  English French Minister President act age air amount answer area art bed board boody book boybuilding business car case cases century change child children church committee companyconditions control cost council countries country course day days deal death development doordoubt early education effect end evening evidence experience yes face fact family father feelingfeet field figure figures food form friends full future general gid govemrnent group hand handsheart history house idea increase individual indust~ influence interest job kind knowledge land levellife light line man market matter means meeting members men mind moment money morning mostmother movement music name nature night number office paper part particular party people pedodperson place point police policy political position power private problem problems public questionrate reason result results room school section sense sen/ice short side simple social society stagestate story subject system table terms thing things time top town trade type use value various viewvoice war water way week west wife woman women word words work wodd year- -  ) *'\[formula**\] ...
I 10 2 5 A England God London Lord again ago alone away back certain close ddifferent either enough example five forward four free further half hard here herself high himhimself home hours important itself later least less living love me months more need one open orderothers out play right six them themselves three times today together true turn two us working years__~- able added almost anything asked began being believe better brought came clear come comingconcerned considered cut decided difficult doing done due ever except expected far feel followedfound getting given go going gone got heard held help hope just kept know known left likely looklooked looking made mean much necessary nothing now often only possible put rather read requiredsaid seem seemed seems seen set show shown something soon start still stood sure taken talkL thought urned used usually want wanted well went yesalready also always be become been bdng find get give leave meet n't never not pay probably sayse~htakee\[__\ [ -~  have really tellI L--- about after along around as because before if like near outside over quite reached since that whenwhere withoutLr - -  and orL(- -  rL_  tol I - -  de ofL---- across against among at behind between by during for from in into on through towards under untilupon with within'd '11 can could did do does may might must shall should think will would1__ 'm 're's 're am are became felt gave had has is knew saw says told took was werei F big carried complete few good great large little long longer own personal real small special very~--- American British above best common first following human labour last local main modern nationalnew next old other past present same second total white whole yound i!
, called down met off round s up~---.
than- -  t : ?
oerFigure 2Final distribution of the most frequent words from the LOB corpus.
Only the first five levels ofclassification are given here, but important syntactic relations are already clear.
The emptyclasses are shown to make the binary topology clear.224McMahon and Smith Improving Statistical Language Models- ->~--her, I L_ anotherI each\] ~ s?mera l  those/ r '?
'W_~ --~ make~Fall both once whom___t L--making talking suchLC3o~ 6 tenF~any_~ everya nno-ouz_.
_ \[ its- -  their, ~ his'--i L my your whose!
r-MrI ~ ~-  Mrsi ~--Dr- -  MissFigure 3Word classification results.
Detail of relationship between words whose final tag value startswith the four bits 0000.
Many of these words exhibit determiner-like b havior.of-speech, the vast majority would be function words and hence would contributelittle to any semantic lassification.
Also, we hoped that relatively rare content wordswould now find themselves within slightly more substantial bigram contexts, againaiding the clustering process.When we examine the most frequent "words" of this hybrid corpus, we find thatthere are many more content words present, but that the remaining content wordsstill have an indirect effect on word classification, since they are represented by thepart-of-speech of which they are an example.
Figure 5 shows many of the largestgroupings of words found after processing, at a classification level of nine bits.
Byinspection, we observe a variety of semantic associations, although there are manyobvious anomalies.
We offer several explanations for this--words are polysemic inreality, the clustering criterion exploits bigram information only, and this algorithm,225Computational Linguistics Volume 22, Number 2I-- doubt~._j Lcases daysI ~ day night morning evening week year momenti I L reason thing place way time job- -1  i - -book  line word field system position story subjectL_ boy gid child man woman personI r -  number amount value figure level rate case matter sense end pedod stageI group meeting idea point question problem result kind type part side_~ L act means effect feeling form answer cost deal change increase influenceuse evidenceE able section terms__~ fact__iindividual particular general public pdvate political social English Frenchshort simple early west airL full most top various President--board body committee party Minister company council government policesociety industry war trade market policy office building church door housej - -  room school country town world car paper family century future movementlife death food light water land development age education expedence--knowledge history conditions figures work view name area nature interestbusiness service course members problems results art music money bedcontrol power stateI m eyes face feet hand hands head heart mind voice father mother wife',__ children friends people men women countries words thingsFigure 4Detail, from Level 5 to Level 9, of many noun-like words.
Semantic differences are registered.like others, finds a locally optimal classification.
In each word group we include here,the entire membership is listed (except for the irrelevant POS tags).
The remaininggroups not presented here also display strong semantic lustering.
From this secondexperiment, we conclude that bigram statistics can be used to make some semanticdifferentiations.
This lends support o our case for using multilevel statistical languagemodels--we can see the kinds of distinctions that structural tags can make and whichwill be lost in the more usual two-level (word and Pos) language models.Finally, Figure 6 shows the complete phoneme classification of a phonemic ver-sion of the vor)IS corpus.
The most obvious feature of this figure is the successfuldistinction between vowels and consonants.
Beyond the vowel-consonant distinction,other similarities emerge: vowel sounds with similar vocal tract positions are clusteredc losely- - the/a/sounds,  for example, and the phoneme pa i r /o /and/oo / ;  some con-sonants that are similarly articulated also map onto local regions of the classificationspace- - - / r /and/ rx / , / ch /and/z / ,  and /n /and/ng / ,  for example.3.2 Clustering ComparisonsA scientifically reliable method of comparing classifications would be to measure howdifferent hey are from randomly generated classifications.
This kind of approach asbeen taken by Redington, Chater, and Finch (1994) but is not used here because itis apparent hat the classifications are clearly different from random ones and, moresignificantly, because many classification processes could produce distributions that226Computational Linguistics Volume 22, Number 2= iL_"Iiuua ar awai  olie ouI eyo 00. e ruIe aa~"- -  Wrrx__~y 'h  z~ __~-- k p i L_I sh  t l_~--m vi L__Is- - -  nng- -  oa- -  ei  iach z- - fFigure 6Automatic phoneme clustering.
Differentiation between vowels and consonants.
From aphonemic version of the VODIS corpus.are nonrandom, but have nothing to do with lexical categories and are not useful forclass-based statistical language modeling.The question of the criterion of a successful classification is dependent upon re-search motivations, which fall into two broad schools.
The first school is made up ofthose who primarily would like to recover the structures linguists posit--the struc-tures they seek are mainly syntactic but can also be semantic.
The second schoolis interested in classifications that help to improve some language model or otherlanguage-processing system and that may or may not exhibit linguistically perspic-uous categories.
Unless modem linguistics is radically wrong, a degree of overlap228McMahon and Smith Improving Statistical Language Modelsshould occur in these two ideals.Researchers who claim that linguistically well-formed classifications are not theimmediate goal of their research must find some other way of measuring the appli-cability of their classifications.
For example, we can operationally define good wordclassifications as those conferring performance improvements o statistical languagemodels.
We shall make this our goal later, but first we compare our system with othersby inspection.In Brill et al (1990), another automatic word-classification algorithm was devel-oped and trained using the Brown corpus; they report success at partitioning wordsinto word classes.
They note that pronouns have a disjoint classification, since the+nominative and -nominative pronouns--for example, <I), <they) and (me), <them)respectively--have dissimilar distributions.
These effects are replicated in our experi-ment.
They report other, more fine-grained features such as possessives, singular deter-miners, definite-determiners and Wh-adjuncts.
Our algorithm also distinguishes thesefeatures.
Brill et al do not report any substantial adjective clustering, or noun clus-tering, or singular-plural differences, or co-ordinating and subordinating conjunctiondistinction, or verb tense differentiation.
At lower levels, the only semantic clusteringthey report involves the group: <man world time life work people years) and thegroup: <give make take find).The results described in Brown, Della Pietra, DeSouza, Lai, and Mercer (1992)are based on a training set two orders of magnitude greater than the one used inthe above experiment.
Even the vocabulary size is an order of magnitude bigger.
Asthe vocabulary size is increased, the new vocabulary items tend, with a probabilityapproaching unity, to be content words: after approximately one thousand words, fewfunction words are left undiscovered.
This increase in resources makes contexts morebalanced and, simultaneously, more statistically significant.
It also allows many morecontent words to be grouped together semantically.
The authors give two tables ofgenerated word classes, one being specially selected by them and the other containingrandomly selected classes.
They do not report on any overall taxonomic relationsbetween these classes, so it is not possible to compare the broad detail of the two setsof data.The results of Finch and Chater (1992, 1991) are also based on a substantially largercorpus.
Finch and Chater also run a version of the Elman experiment (see below).
Theirsystem fails to produce a complete noun-verb distinction at the highest level, thoughthey offer an argument to suggest that the inadequacy lies in the nature of Elman'spseudo-natural language corpus; our system uses Elman's corpus but succeeds inmaking the primary noun-verb distinction.
Finch and Chater also cluster letters andphonemes--their system succeeds in distinguishing between vowels and consonantsin the letter experiment, and only the phoneme /u/ is incorrectly classified in thephoneme experiment.
Conversely, our algorithm completely clusters phonemes intovowels and consonants, but performs less well with letters (McMahon 1994).Pereira and Tishby (1992) do not give details of syntactic similarity--they con-centrate on a small number of words and make fine-grained semantic differentiationsbetween them.
Their evaluation techniques include measuring how helpful their sys-tem is in making selectional restrictions and in disambiguating verb-noun pairs.Schiitze (1993) uses a standard sparse matrix algorithm with neural networks; hissystem is the only one that attempts to side-step the problem of deciding what hisclusters are clusters of, by producing a system that generates its own class labels.
Hedoes not report the overall structure of his one-level classification.
His training set isone order of magnitude bigger than the largest one used in the present experiments.229Computational Linguistics Volume 22, Number 23.2.1 Ceteris Paribus Qualitative Comparison.
This section describes comparisonsbetween our algorithm and others, where some of the experimental parameters arecontrolled--for example, corpus size.
We considered it useful to compare the perfor-mance of our algorithm with others' on precisely the same input data because webelieve that factors like vocabulary, corpus size, and corpus complexity make evalua-tion difficult.A Recurrent Neural Network and a Regular Grammar.
We redescribe the salient details ofone of the experiments performed by Elman (1990).
The grammar that generates thelanguage upon which this experiment is based is, according to the Chomsky classifica-tion, type 4 (regular, or finite-state).
Its production rules are shown in Figure 7.
Someof the words belong to two or more word classes.
The sentence frames encode a sim-ple semantics--noun types of certain classes engage in behavior unique to that class.Elman generates a 10,000-sentence corpus to be used as the training corpus.
Each sen-tence frame is just as likely to be selected as any other; similarly, each word memberof a particular word group has an equiprobable chance of selection.
No punctuationis included in the corpus, so sentence ndings are only implicitly represented--forexample, the segment stream/cat smell cookie clog ex is t  boy smash plate / con-tains a three-word sentence followed by a two-word sentence followed by anotherthree-word sentence.After training, Elman's net was tested on an unseen set, generated by the sameunderlying rammar.
The network's performance was poor--only achieving a predic-tion error rate slightly above chance.
Elman then presented the training data to thenet a further four times, but the prediction was still poor.
He claimed that, with evenmore training, the net could have improved its performance further.
But this was notthe main goal of the experiment; instead, hierarchical cluster analysis was performedon the averaged hidden unit activations for each of the 29 words.
Figure 8 reproducesthe similarity tree that cluster analysis of the recurrent net produced.
His analysis re-veals that the network has learned most of the major syntactic differences and manyof the semantic ones coded in the original language.
For example, there is a cleardistinction ,between verbs and nouns; within the main class of nouns, there is a clearanimate-inanimate distinction; within that, the classes of agent-patient, aggressor, andnonhuman animal have been induced.
The analysis is not perfect: the most importantdistinction is considered to be between a handful of inanimate noun objects (bread,cookie, sandwich, glass, and plate) and the rest of the vocabulary.We now discuss the results obtained when our algorithm is applied to a similartest corpus.
Elman's grammar of Figure 7 was used to produce a corpus of 10,000 sen-tences with no sentence breaks.
Unigram and bigram word-frequency statistics weregenerated.
Our structural tag word-classification algorithm was applied to the initialmapping, which randomly assigned tag values to the 29 words.
Figure 9 shows the im-portant classification decisions made by this algorithm.
Unlike the Elman classification(see Figure 8), informationally useful class structure xists from level 1 onwards.
Thisalgorithm also produces a classification some features of which are qualitatively betterthan Elman's--all nouns and all verbs are separated; all animates and inanimates areseparated.
The multicontext noun/verb /break / is identified as different from otherverbs; intransitive verbs cluster together and the aggressive nouns are identified.
Thisalgorithm does not recapture the complete syntax and semantics of the language--human nouns and non-aggressive animate nouns remain mixed, and the food nouncluster failed to attract he word (sandwich/.
This experiment was repeated severaltimes, each time resulting in a classification whose overall structure was similar butwhose fine detail was slightly different.
One run, for example, correctly differentiated230McMahon and Smith Improving Statistical Language ModelsSSSSSSSSSSSSSSSS?
NOUN-HUMAN VERB-EAT NOUN-FOOD?
NOUN-HUMAN VERB-PERCEPT NOUN-INAN?
NOUN-HUMAN VERB-DESTROY NOUN-FRAGILE?
NOUN-HUMAN VERB-INTRAN?
NOUN-HUMAN VERB~RAN NOUN-HUMAN?
NOUN-HUMAN VERB-AGPAT NOUN-INAN- - ?
NOUN-HUMAN VERB-AGPAT?
NOUN-ANIM VERB-EAT NOUN-FOOD- - ?
NOUN-ANIM VERB-TRAN NOUN-ANIM?
NOUN-ANIM VERB-AGPAT NOUN-INANIM?
NOUN-ANIM VERB-AGPAT?
NOUN-INAN VERB-AGPAT~-}1=.
NOUN-AGRESS VERB-DESTROY NOUN-FRAGILE?
NOUN-AGRESS VERB-EAT NOUN-HUMAN?
NOUN-AGRESS VERB-EAT NOUN-ANIM------?
NOUN-AGRESS VERB-EAT NOUN-FOODNOUN-HUMAN )~NOUN-ANIM ?NOUN-INAN ?NOUN-AGRESS - - ?NOUN-FRAGILE - - ?NOUN-FOOD ?VERB-INTRAN ?VERB-TRAN ?VERB-AGPAT ?VERB-PERCEPT-- ?VERB-DESTROY ?VERB-EAT- - ?Figure 7man woman girl boycat mouse dog man woman girl boy dragon monster lionbook rock car cookie break bread sandwich glass platedragon monster lionglass platecookie break bread sandwichthink sleep existsee chase likemove breaksmell seebreak smasheatThe Elman grammar.
There are 16 nonterminal rules and 12 terminals.
Notice also thatterminals can belong to more than one word class--for example, {break) is an inanimatenoun, a food noun, an agent-patient verb, and a destroy verb.231Computational Linguistics Volume 22, Number 2bread {--CC cookiesandwichglassplateeatsmash~ chase likemove__~-  break- -  smells,e:0\[_/---thinkt~ex is tbookLi---car L--rock.
_~.
.
ragonL monster_ _  girlanmanFigure 8Elman's results.
A Cluster analysis of the hidden units of a trained recurrent net, showing themajor verb-noun distinction, as well as many other syntactic and semantic fine-graineddistinctions.between small animals and humans, but failed to recognize food nouns as a completegroup.
Another un identified food nouns perfectly but failed to separate aggressorsfrom other animates.Classi~cation Using a Merging Algorithm.
The systems described in Brown, Della Pietra,DeSouza, Lai, and Mercer (1992) and Brill and Marcus (1992) both provide examples ofbottom-up, merge-based classification systems; a version of such a system was chosento be implemented and tested against our algorithm, using the same input data.
TheBrown system uses a principle of class merging as its main clustering technique.
Theinitial classification contains as many classes as there are words to classify, each word inits own class.
Initially these classes are all mutually independent.
Then two classes arechosen to merge; the criterion of choice is based on a mutual information calculation(see Equation 2).
The process is repeated until only one class remains.
Next, the orderof merging provides enough information for a hierarchical c uster to be constructed.
Acomparison experiment was designed using the 70,000-word VODIS corpus (Cookson1988) as a source of frequency information; our system and the merging system weregiven a set of those words from the decapitalized and depunctuated corpus (exceptfor the apostrophe when it is a part of a word) whose frequencies were greater than30.
This accounted for the 256 most frequent words.The final classifications, to a depth of five levels, are shown in Figure 10 and Fig-ure 11 for the bottom-up and top-down systems, respectively.
The difficulty of corn-232Computational Linguistics Volume 22, Number 2->~-minutes  o'clockI t__ adults hundred eighty sixtyI I I--P ?unds~- -  thirteen fifteen sixteen seventeen,---l--fiftyI I "  -fou,ty- - t  Ltwenty~ ~  ften eleventwelveive~- -  eight sevenL_  two three four six nine,_~cambfidge peterborough london birmingham manchester norwich buryI I stowmarket here ipswich euston felixstowe woodbfidge\] ~ me have like need want know say see mean think wonder change leave|a r r ive  leaves leaving come coming go pay travel them travelling be get- -  | catch make take find work~ - to  from into viaI--don't can't haven't help give tell can could thank might should will wouldyou i they we that'll i'll you'd you'll~ are do if no did pardon actually really all as when where but so well afraid again because obviously they're sorry now which alright fine lovely okay fight thanks then oh past and or please yesat about before by after between than is does isn't gets was of on for what's/ with .
.t - - jus t  not only probably still hang hold half quarter got going goes possibletrying one in there country cross cheaper more round straight hroughmonday friday saturday tomorrow back off over down up it that early lateshe something this that's it's there's i'm you're i've you've~ .~  bdtish bye liverpool rail good hello card enquiries street~ --  much day class thing time out sort cost price times!
.
'__ aftemoon evening morning first best other next same indeed bit lot momentpound child second single sunday return children journey line saver stationfare ticket train trains adult hour way!?
-- how very~ - -~ whatI c -  the~-- a any each long an been my yourFigure 10Merge-based clustering.
Classification of the most frequent words of a formatted VODIScorpus, using a merge-based method.for this kind of division between umbers.
The merge-based system includes thesenumbers at Level I only.
Both systems identify place names well ((liverpool> is mostoften seen in this corpus as part of the place name--<liverpool street station>);however, the nearest class in the merge-based system is a group of verb-like words,whereas in the top-down system, the nearest class is a group of nouns.
With verbs,the merge-based system performs better~ producing a narrower dispersion of words.However, this success is slightly moderated by the consideration that one half ofthe entire merge  classification system was  allocated to number  words, leaving the234McMahon and Smith Improving Statistical Language Models.__\[--are can could did do help if thank wheni~  L--give how tell very\[ ~--  can't don't haven't indeed wonderq ~- -  afraid know mean see think\[ ~-- again any coming cost for goes going just off on only over possible\[ \[ travelling up what withas because but does is isn't liverpool might no not obviously should so stillthan that'll was well where will would_ _  ~ catch come find get go have leave like make manchester need pay say~ take travel trying want workL _ _  been bit friday got lot monday saturday sunday through~ i i 'm she they we you much me long a i've straight you'vei'll probably to you'd you'll~--actually be cheaper early it it's now really something street that there- '  | there ' s  this you're=-back day down hang hold more out them time your" fternoon card enquiries hello railbritish goodbyeat countrychange cross quartereight eleven five four nine seven six sixteen ten thirteen three twelve twoeighty fifteen fifty forty hundred minutes o'clock oh past poundsseventeen sixty thirty twenty ~ all and before late or pardon please sorry which yes okay that's they'reI !
, ~ arrive between from in into leaves leaving one viai i L_ about an by each first half my sort the tomorrow-~ ~ gets thanksI i i--adults aldght fine lovely right then what's;--after birmingham bury cambridge uston felixstowe here ipswich london!
: norwich peterborough pound stowmarket woodbridgeadult best child children class evening fare hour journey line momentmorning next of other price return round same saver second single stationthing ticket times train trains wayFigure 11Top-down clustering.
Classification of the most frequent words of a formatted VODm corpus,using a top-down method.rest of the vocabulary, including the verbs, to be distributed through the other half.Considering the distributions of pronouns and determiners, the merge-based systemperforms lightly better.In conclusion, the two systems display the same kinds of differences and similar-ities as were seen when we compared our system to Elman's neural network--that is,our method performs lightly better with respect o overall classification topology, butloses in quality at lower levels.
This loss in performance is also noted by Magerman(1994), who applies a binary classification tree to the task of parsing.
Magerman alsomakes the point that trees (as opposed to directed graphs) are inherently vulnerable235Computational Linguistics Volume 22, Number 2Table 1Reduced tag set used in Hughes-Atwell evaluation system.ADJ ADV ART CCONCARD DET EX EXPLLET MD NEG NOUNORD OTH PAST PREPPRES PRON PUNC QUALSCON TO WHto unnecessary data fragmentation.
The inaccuracies introduced by the first of thesecharacteristics may be controlled, to a limited extent only, by using a hybrid top-downand bottom-up approach: instead of clustering vocabulary items from the top down,we could first merge some words into small word classes.
Later top-down clusteringwould operate on these word groups as if they were words.3.2.2 Quantitative comparison.
Arriving at more quantitatively significant conclusionsis difficult; Hughes (1994), for example, suggests benchmark evaluation--a standardtagged corpus (e.g., the LOB) is used as a reference against which automatic om-parisons can be made.
While this may not be appropriate for the designers of everyautomatic classification system, such as researchers whose main interest is in automaticclassification i statistical language modeling, it has many advantages over qualitativeinspection by an expert as an evaluation method, which to date has been the dominantmethod.
Brill and Marcus (1992) suggest a similar idea for evaluating an automaticpart-of-speech tagger.Classification trees can be sectioned into distinct clusters at different points in thehierarchy; each of these clusters can then be examined by reference to the distributionof LOB classes associated toeach word member of the cluster.
A high-scoring cluster isone whose members are classified similarly in the tagged LOB corpus.
In the following,we follow Hughes' method.The evaluation is performed on the 195 most frequent words of the LOB corpus.The words are automatically classified using our top-down algorithm.
The resultingclassification is then passed to the evaluator, which works as follows: The first stageinvolves producing successive sections, cutting the tree into distinct clusters (fromone cluster to as many clusters as there are vocabulary items), so that an evaluationscore can be generated for each level; these evaluations can be plotted against henumber of clusters.
At each section, and for each cluster, we make an estimate ofthe preferred classification label for that cluster by finding the most common parts ofspeech associated with each word in the classification under question.
For that partof speech most frequently associated with the word, we give a high weight, withdecreasing weight for the second most frequent part of speech, and so on for the topfour parts of speech.
We then estimate the most likely part-of-speech ategory andlabel this cluster accordingly.
Then, for each member of this cluster, a partial scoreis calculated that rates our classification of the word against its distribution of LOBclasses.
The summed score is then normalized as a percentage.
An outline of theevaluation scheme is shown in Figure 12.Hughes does not use the classification system provided with the LOB corpus--instead, he uses a reduced classification system consisting of 23 class tags, shown inTable 1.
The results are shown in Figure 13.Both of the compared classification systems use familiar statistical measures of cor-relation (Spearman's rank correlation coefficient and Manhattan metric) and grouping236McMahon and Smith Improving Statistical Language ModelsJs~ome the "\ all /lu:ta;;~ebde(1) Estimate Preferred Cluster Labelarticleadverbnounadjectivedetermineretc.i i o 2C 314 0 3C 0 0 Cg 0 0 00 1 1LOB-class informationthe: <article,38458><adverb,19><noun,l>all : <article, 1559><adverb,160><adjective,l>some:<det,1111>the all someick Highest ScorerEstimated ClusterLabel = article(2) Update Evaluation Score(i) "the': matches Estimated Cluster Label with most frequent LOB-classADD 1(ii) "all": also matches Estimated Cluster Label with top LOB-classADD 1(iii) "some": doesn't match Estimated Cluster Label with any of the topfour of its LOB-classesADD 0Generally, for each word, if the Estimated Cluster Label matches the kth top LOB-class,ADD (5-k)/4 to the score; k<5.
Contributions to the estimate of the Preferred ClusterLabel work similarly.Figure 12Hughes-Atwell Cluster Evaluation.
(group averaging and Ward's method) as their main method.
Our system scores higherthan the Finch system at all levels; the Hughes ystem scores better than ours over thefirst eighty classes, but worse at lower levels.
However, we note that both Hughes andFinch use contiguous and noncontiguous bigram information, whereas we use con-tiguous bigram information only--the simplest estimate of word context possible--toexplore just how much information could be extracted from this minimal context.One limitation of Hughes' evaluation system is that fractured class distributionsare not penalized: if some subbranch of the classification contains nothing but number237Computational Linguistics Volume 22, Number 210090 _ - -P~.
7o-~ 6o~ 50uJ~ 40~) "1~ ~ o d  and Manhattan MetricT~ 30 Spearman Rank and Group Average \[0 20 40 60 80 100 120 140 160 180 200Number of ClustersFigure 13Performance comparison.
Graph showing the performance of the top-down classificationsystem compared to two recent systems -- those of Hughes and Atwell and of Finch andChater.
Performance is measured by the Hughes-Atwell cluster evaluation system.words (<ten), (three/, etc.)
then that branch gets a certain score, regardless of howspread-out the words are within that branch.
On the other hand, there may wellbe good engineering reasons to treat linguistically homogeneous words as belongingto different classes.
For example, in a corpus of conversations about train timetables,where numbers occur in two main situations--as ticket prices and as times--we mightexpect o observe a difference between, say, the numbers from 1 to 12, and numbersup to 59 (hour numbers and minute numbers respectively); Figure 11 lends somesupport o this speculation.
Similarly, phrases like ( f ive pounds ninety nine pence/ could lead to different patterns of collocation for number words.
This sort of effect isindeed observed (McMahon 1994).
It is less clear whether our main clustering resultseparates number words into different classes for the same kind of reason (in Figure 2,class 00000 contains 4number words and class 00101 contains 11).
A second limitationlies in the evaluation scheme stimating the canonical part of speech based on the rankof the parts of speech of each word in it--a better system would make the weight besome function of the probability of the parts of speech.
A third criticism of the schemeis its arbitrariness in weighting and selecting canonical classes; the criticism is onlyslight, however, because the main advantage of any benchmark is that it provides astandard, regardless of the pragmatically influenced etails of its construction.Automatic word-classification systems are intrinsically interesting; an analysis oftheir structure and quality is itself an ongoing research topic.
However, these systemscan also have more immediate uses.
The two types of use are related to the two types ofapproach to the subject--linguistic and engineering.
Consequently, indirect evaluationcan be linguistic or engineering-based.Indirect linguistic evaluation examines the utility of the derived classes in solv-ing various linguistic problems: pronoun reference (Elman 1990; Fisher and Riloff238McMahon and Smith Improving Statistical Language Models1992), agreement, word-sense disambiguation (Liddy and Paik 1992; Gale, Church,and Yarowsky 1992; Yarowsky 1992; Pereira, Tishby, and Lee 1993) and resolution ofanaphoric reference (Burger and Connolly 1992).
A classification is said to be useful ifit can contribute to a more accurate linguistic parse of given sentences.
If our main in-terest were linguistic or cognitive scientific, we would be even more concerned aboutthe way our system cannot handle multimodal word behavior and about the resultingmisclassifications and fracturing of the classes.One main engineering application that can use word classes is the statistical lan-guage model.
Classifications which, when incorporated into the models, lower the testset perplexity are judged to be useful.4.
Structural Tags in Multiclass Statistical Language ModelsThere are several ways of incorporating word-classification i formation into statisti-cal language models using the structural tag representation (McMahon 1994).
Here,we shall describe a method, derived from Markov model theory (Jelinek and Mercer1980), which is based on interpolating several anguage components.
The interpola-tion parameters are estimated by using a held-out corpus.
We decided to build aninterpolated language model partly because it has been well studied and is familiarto the research community and partly because we can examine the lambda param-eters directly to see if weight is indeed distributed across multiple class levels.
Apoor language model component will receive virtually no weight in an interpolatedsystem--if we find that weight is distributed mostly with one or two components, wecan conclude that interpolated language models do not find much use for multipleclass information.For the following experiments, a formatted version (punctuation removed, allwords decapitalized, control characters removed) of the one-million-word Brown cor-pus was used as a source of language data; 60% of the corpus was used to generatemaximum likelihood probability estimates, 30% to estimate frequency-dependent in-terpolation parameters, and the remaining 10% as a test set.
The vocabulary itemsextracted from the training set were clustered according to the method described ear-lier.For comparison, we calculated some test set perplexities of other language models.Improved performance can be obtained by making interpolation parameters dependupon some distinguishing feature of the prediction context.
One easily calculated fea-ture is the frequency of the previously processed word.
In our main experiment, hisresulted in 428 sets of & values, corresponding to428 different previous-word frequen-cies.
The parameters are fitted into an interpolated language model the core of whichis described by the equation:P(Wk) "~- "~u(f) X P(Wk) -{- )~b(f) X P(w k I Wk-1) q- )~t(f) X P(w k I Wk-2,Wk-1)where f = f(wj), the frequency of word (wj) if a valid wj exists and 0 otherwise--namely at the beginning of the test set, and when the previous word is not in thetraining vocabulary.
The & values are selected using a standard re-estimation algo-rithm (Baum et al 1970).
The resulting perplexity value for this system is 621.6.
Thisrepresents a pragmatically sensible baseline value against which any variant languagemodel should be compared.
A similar word-based language model, the weighted av-erage language model, has been developed by O'Boyle, Owens, and Smith (1994).
This239Computational Linguistics Volume 22, Number 2model is described as follows:m Wk_ i ) q- ~0 X PML(Wk) P(Wk I W~ -1) = ~i=1/~i X PML(Wk I k-1m A ~i=0 iwhere there are statistically significant segments up to m + 1 words long and PML(Wk)is the maximum likelihood probability estimate of a word.
The numerator acts as anormalizer.
It has been found that:kmlAi = 2 (Iwk-~ I) x logf(w~-~)where k-1 IWk_il is the size of the segment, results in a useful anguage model of this form.When applied to the Brown corpus, excluding the 30% allocated for interpolation andonly using n-grams up to 3, the model still performs well, achieving aperplexity scoreof 644.6; adding the extra training text should remove the disadvantage suffered bythe weighted average model but at the probable cost of introducing new vocabularyitems, making the test set perplexity comparisons even more difficult to interpret.An important component of many statistical language-modeling systems is thebigram conditional probability estimator P(wi \] wi-1) (Church and Gale 1991); weshall restrict our attention to the case where both words have been seen before,though the bigram (Wi_l,Wi) itself may be unseen.
We shall suggest an alternativeto the familiar maximum likelihood bigram estimate, which estimates the probabilityas P(wi I wi-1) f(wi_~,wi) wheref(w) is just the frequency of occurrence of w in some- f (w i_~)  ,training corpus.The general form of the multilevel smoothed bigram model is:SP(wi I wi-1) = ~/~)(wi-l),sP(CS(wi) I CS(wi-1))P( wi I CS(wi)) (3)where there are S levels of class granularity and CS(wi) is the class at level s of theword wi; ),4(w~_l),s i an interpolation weight for language model component s and de-pends upon some function ~(Wi_I) of the conditioning word wi-1; common functionsinclude a frequency-based interpolation ~(wi-1) = f (w i -1 )  and a depth-s class-basedinterpolation, qS(wi_l) = CS(wi_l), though ~ can partition the conditioning context inany way and this context does not necessarily have to be a recent word.
The ,~ valuesare estimated as before, using the frequency of the previous word to partition theconditioning context.
Parameter setting for the smoothed bigram takes less than a dayon a Sparc-IPC.Our 16-bit structural tag representation allows us to build an interpolated bigrammodel containing 16 levels of bigram-class information.
As suggested earlier, we canlook at the spread of )~ values used by the smoothed bigram component as a functionof the class granularity and frequency of the conditioning word.
Figure 14 showsclearly that the smoothed bigram component does indeed find each class level useful,at different frequencies of the conditioning word.
Next, we need to find out how muchof an improvement we can achieve using this new bigram model.We can replace the maximum likelihood bigram estimator in our interpolatedtrigram model with the smoothed bigram estimator.
When we do, we get a perplexityof 577.4, a 7.1% improvement on standard interpolation, which scores 621.6.
Otherexperiments with )~ depending on the class (at a certain depth) of the previous wordlead to smaller improvements and are not reported here.240McMahon and Smith Improving Statistical Language Modelslambda0 .5 -0.450.40.350.30.250.20.15 " ',0.1 .
.
.
.
,0.050' "  1 151ularityFigure 14Bigram lambda weights.
Surface showing how lambda varies with frequency (log scale) ofprevious word and bigram-class granularity.
The projected contour map highlights the mainfeature of this relationship--at various frequencies, each of the 16 class bigram models is used.Figure 15 summarizes the test set perplexity results.
We note that our 7.1% im-provement is larger than that obtained by Brown, Della Pietra, DeSouza, Lai, andMercer (1992), who report a 3.3% improvement.
The smaller absolute perplexity scoresthey quote are a consequence of the much larger training data they use.
One reasonfor this apparent improvement may be that their baseline model, constructed as it isout of much more training data, is already better than our equivalent baseline, so thatthey find improvements harder to achieve.
Another eason may be due to the differentvocabulary sizes used (Ueberla 1994).
A third reason, and one which we consider to beimportant, is that multilevel class-based language models may perform significantlybetter than two-level ones.
We carried out another experiment to support his claim.We constructed a frequency-dependent interpolated unigram and bigram modelas a baseline.
Its test set perplexity was 635.
We then replaced the maximum likeli-hood bigram component with the smoothed bigram estimate.
The perplexity for thissystem was 580, a 9% improvement.
We also replaced the maximum likelihood bigramcomponent with a series of 15 two-level smoothed bigram models--from a 16-plus-15 smoothed bigram to a 16-plus-1 smoothed bigram.
Figure 16 details these results.The best of these two-level systems is the 16-plus-8 model, which scores 606.
So, ona bigram model, the multilevel system is 4.3% better than the best two-level system,which supports our claim.
We chose bigram models in this experiment so that wecould make some comparisons with similarity-based bigram models.Dagan, Markus, and Markovitch (1993) claim that word-classification systems ofthis type may lead to substantial information loss when compared to similarity meth-ods (Dagan, Pereira, and Lee 1994; Essen and Steinbiss 1992).
The similarity-basedsystem of Dagan, Pereira, and Lee (1994) improves a baseline Turing-Good bigrammodel by 2.4% and the co-occurrence system of Essen and Steinbiss (1992) leads toa 10% improvement over an interpolated baseline bigram model.
This latter result isbased on a similarly sized training set and so our 9% improvement compared to their241Computational Linguistics Volume 22, Number 2Language Model Test Set PerplexityWeighted Average 644.626Interpolated Trigram 621.632Interpolated Trigram (smoothed bigram component) 577.421Figure 15Test set perplexity improvements.
When an interpolated trigram language model usessmoothed bigram estimates, test set perplexity reduced by approximately 7.1% compared to asimilar system with maximum likelihood bigram estimates, and 10% compared to theweighted average language model.Language Model Test Set PerplexityBaseline bigramword plus class 1word plus class 2word plus class 3word plus class 4word plus class 5word plus class 6word plus class 7word plus class 8word plus class 9word plus class 10word plus class 11word plus class 12word plus class 13word plus class 14word plus class 15Multilevel635634633626621616614609606609614618622627631633580Figure 16Multilevel versus two-level bigram performances.
A multilevel smoothed bigram model is 9%better than a baseline maximum likelihood model and 4.3% better than the best two-levelclass-based bigram model.10% suggests that language models based upon fixed-place classes can be only slightlyworse than some similarity models, given approximately equal training texts.4.1 An ExampleAs an illustration of the kind of advantage structural tag language models can offer,we introduce nine oronyms (word strings which, when uttered, can produce the samesound) based upon the uttered sentence:The boys eat the sandwiches.If we assume that we already possess a perfect speech recognition acoustic model(Jelinek, Mercer, and Roukos 1992), it may be able to recover the phoneme string:/DH a b 01 z EE t DH A s AA n d w i j i z/242McMahon and Smith Improving Statistical Language ModelsSentence W.A.
Smoothed Grammaticalthe boy seat the sandwichesthe boys eat the sandwichesthe boy seat this and which isthe boys eat this and which isthe buoys eat the sandwichesthe buoys eat this and which isthe boys eat the sand which isthe buoys eat the sand which isthe buoy seat this and which is3,4191,78743523219525141.507,8488,8211371494698211.10noyesnonoyesnoyesyesnoFigure 17Improvements in a simulated speech-recognition example.
Nine versions of a phonemicallyidentical oronym, ordered by weighted average (W.A.)
probability (x 10-20).
The W.A.language model ranks the preferred sentence second.
The smoothed structural tag modelsuccessfully predicts the original utterance as the most likely.
(buoy) is an unseen vocabularyitem in this test.
Also, in all but two nonzero cases, the smoothed model makes grammaticallycorrect sentences more likely and vice versa.The original sentence is not the only speech utterance that could give rise to theobserved phoneme string; for example, the meaningless and ungrammatical sentence:*The buoy seat this and which is.can also give rise to the observed phonemic stream.
Humans usually reconstruct themost likely sentence successfully, but artificial speech recognizers with no languagemodel component cannot.
Nonprobabilistic models, while theoretically well-grounded,so far tend to have poor coverage.
Another limitation can be seen if we consider athird hypothesized sentence:The buoys eat the sand which is.This simultaneously surreal and metaphysical sentence may be accepted by grammarsystems that detect well-formedness, but it is subsequently considered just as plausibleas the original sentence.
A probabilistic language model should assign a relatively lowprobability to the third sentence.
We constructed nine hypothesized sentences, eachof which could have produced the phoneme string; we presented these sentences asinput to a high-quality word-based language model (the weighted average languagemodel) and to another smoothed structural tag language model.
Neither the Hughessystem nor the Finch system are ever applied to language models; also, the details ofthe Brown language model are insufficient for us to rebuild it and run our sentencesthrough it.
Figure 17 shows the normalized probability results of these experiments.The new language model successfully identifies the most likely utterance.
In all buttwo nonzero cases, grammatically well-formed sentences are assigned a higher rawprobability by the new model, and vice-versa for ungrammatical sentences.Using the top two sentences (the boy seat the sandwiches)and (the boys eatthe sandwiches), we can examine the practical benefits of class information for statis-tical language modeling.
An important difference between the two is in the bigrams(boy seat) and (boys eat), neither of which occurred in the training corpus.
Themodel that uses word frequencies exclusively differentiates between the two hypoth-esized sentences by examining the unigrams (boy), (seat), (boys), and (eat).
In our243Computational Linguistics Volume 22, Number 2training corpus, <boy> and <seat> are individually more likely than <boys> and <eat>.However, with the structural tag model, extra word-class information allows the sys-tem to prefer the more common oun-verb pattern.
This sort of advantage becomeseven more apparent with number words: for example, if we were trying to predict helikelihood of <seconds> given <six>, even though the bigram <six seconds> does notoccur in our training text, we find that <three seconds>, (four seconds>, and <fiveseconds> occur, as do <six years>, <six months>, <six weeks>, and <six days>.5.
DiscussionThe automatic word-classification system based on a binary top-down mutual infor-mation algorithm leads to qualitatively interesting syntactic and semantic lusteringresults; quantitatively, it fares well compared with other systems, demonstrating com-plementary strengths and weaknesses compared to the more usual merge-based classi-fication systems.
Results from an implementation f one version of a multilevel class-based language model (an interpolated trigram model with the maximum likelihoodbigram component replaced with a smoothed bigram component) show a 7% improve-ment in statistical language model performance compared to a standard interpolatedlanguage model.
We have incorporated structural tag information i to an interpolatedmodel because it provides a well-attested and successful base system against whichimprovement can be measured; it also offers us the opportunity to visualize the ), dis-tribution across 16 classes o that we can observe in which circumstances ach classlevel is preferred (see Figure 14).
However, we believe that the weighted average sys-tem described earlier, with its scope for improvements including n-gram informationbeyond the trigram and its avoidance of data-intensive and computationally intensiveparameter optimization, could offer a more convenient platform within which to placestructural tag information.
Although variable granularity class-based language mod-els will never fully capture linguistic dependencies, they can offer modest advancesin coverage compared to exclusively word-based systems.AcknowledgmentsBoth authors thank the Oxford Text Archiveand British Telecom for use of their corpora.The first author wishes to thank BritishTelecom and the Department ofEducationfor Northern Ireland for their support.ReferencesBahl, Lalit R., Peter F. Brown,Peter V. DeSouza, and Robert L. Mercer.1989.
A tree-based statistical languagemodel for natural language speechrecognition.
IEEE Transactions on Acoustics,Speech and Signal Processing,37(7):1001-1008, July.Bahl, Lalit R., Frederick Jelinek, andRobert L. Mercer.
1983.
A maximumlikelihood approach to continuous peechrecognition.
I.E.E.E.
Transactions on PatternAnalysis and Machine Intelligence,PAMI-5(2):179-190, March.Baum, Leonard E., Ted Petrie, GeorgeSoules, and Norman Weiss.
1970.
Amaximization technique occurring in thestatistical nalyses of probabilisticfunctions of Markov chains.
The Annals ofMathematical Statistics, 41(1):164-171.Beckwith, Richard, Christiane Fellbaum,Derek Gross, and George A. Miller.
1991.WordNet: A lexical database organized onpsycholinguistic principles.
In Uri Zernik,editor, Lexical Acquisition: ExploitingOn-Line Resources toBuild a Lexicon.Lawrence Erlbaum Associates, chapter 9,pages 211-232.Black, Ezra, Roger Garside, and GeoffreyLeech.
1993.
Statistically-Driven ComputerGrammars of English: The IBM~LancasterApproach.
Rodopi.Black, Ezra, Frederick Jelinek, John Lafferty,David M. Magerman, Robert Mercer, andSalim Roukos.
1993.
Towardshistory-based grammars: Using richermodels for probabilistic parsing.
InProceedings ofthe 31st Annual Meeting of theAssociation/or Computational Linguistics,pages 31-37, Ohio State University, June.244McMahon and Smith Improving Statistical Language ModelsBrill, Eric, David Magerman, MitchellMarcus, and Beatrice Santorini.
1990.Deducing linguistic structure from thestatistics of large corpora.
In Proceedings ofthe DARPA Speech and Natural LanguageWorkshop.Brill, Eric, and Mitch Marcus.
1992.
Taggingan unfamiliar text with minimal humansupervision.
In Probabilistic Approaches toNatural Language.
American Associationfor Artificial Intelligence, AAAI Press.Technical report FS-92-05.Brown, Peter E, Vincent Della Pietra, PeterDeSouza, Jennifer C. Lai, and RobertMercer.
1992.
Class-based n-gram modelsof natural angauge.
ComputationalLinguistics, 18(4):467-479.Brown, Peter E, Vincent J. Della Pietra,Robert L. Mercer, Stephen A. Della Pietra,and Jennifer C. Lai.
1992.
An estimate ofan upper bound for the entropy ofEnglish.
Computational Linguistics,18(1):31-40.Burger, John D. and Dennis Connolly.
1992.Probabilistic resolution of anaphoricreference.
In Probabilistic Approaches toNatural Language.
American Associationfor Artificial Intelligence, AAAI Press.Technical report FS-92-05.Church, Kenneth Ward.
1988.
A stochasticparts program and noun phrase parserfor unrestricted text.
In Second Conferenceon Applied Natural Language Processing.Church, Kenneth W. and William A. Gale.1991.
A comparison of the enhancedGood-Turing and deleted estimationmethods for estimating probabilities ofEnglish bigrams.
Computer Speech andLanguage, 5:19-54.Church, Kenneth W., William A. Gale,Patrick Hanks, and Donald Hindle.
1991.Using statistics in lexical analysis.
In UriZernik, editor, Lexical Acquisition:Exploiting On-Line Resources to Build aLexicon.
Lawrence Erlbaum Associates,chapter 6, pages 115-164.Church, Kenneth W. and Robert L. Mercer.1993.
Introduction to the special issue oncomputational linguistics using largecorpora.
Computational Linguistics,19(1):1-23.Cookson, S. 1988.
Final evaluation of VODIS.In Proceedings ofSpeech "88, Seventh FASESymposium, pages 1311-1320, Edinburgh.Institute of Acoustics.Cover, Thomas M. and Joy A. Thomas.1991.
Elements of Information Theory.
JohnWiley and Sons.Dagan, Ido, Shaul Markus, and ShaulMarkovitch.
1993.
Contextual wordsimilarity and estimation from sparsedata.
In Proceedings ofthe Association forComputational Linguistics, pages 164-171.Dagan, Ido, Fernando Pereira, and LillianLee.
1994.
Similarity-based stimation ofword cooccurence probabilities.
InProceedings ofthe Association forComputational Linguistics.Derouault, Anne-Marie and BernardMerialdo.
1986.
Natural anguagemodelling for phoneme-to-texttranscription.
I.E.E.E.
Transactions onPattern Analysis and Machine Intelligence,PAMI-8(6), November.Elman, Jeffrey L. 1990.
Finding structure intime.
Cognitive Science, 14:179-211.Essen, Ute and Volker Steinbiss.
1992.Co-occurrence smoothing for stochasticlanguage modelling.
In Proceedings ofICASSP, volume 1, pages 161-164.Finch, Steven and Nich Chater.
1991.
Ahybrid approach to the automatic learningof linguistic categories.
A.LS.B.
Quarterly.Finch, Steven and Nick Chater.
1992.Bootstrapping syntactic ategories usingstatistical methods.
In Walter Daelemansand David Powers, editors, Backgroundand Experiments inMachine Learning ofNatural Language, pages 229-235.
Institutefor Language Technology and AI.Finch, Steven and Nick Chater.
1994.Learning syntactic ategories: A statisticalapproach.
In M. Oaksford and G.D.A.Brown, editors, Neurodynamics andPsychology.
Academic Press, chapter 12.Fisher, David and Ellen Riloff.
1992.Applying statistical methods to smallcorpora: Benefitting from a limiteddomain.
In Probabilistic Approaches toNatural Language.
American Associationfor Artificial Intelligence, AAAI Press.Technical report FS-92-05.Gale, William A., Kenneth W. Church, andDavid Yarowsky.
1992.
Work on statisticalmethods for word sense disambiguation.In probabilistic Approaches to NaturalLanguage.
American Association forArtificial Intelligence, AAAI Press.Technical report FS-92-05.Good, I. J.
1953.
The population frequenciesof species and the estimation ofpopulation parameters.
Biometrika,40:237-264, December.Hughes, John.
1994.
Automatically Acquiring aClassification of Words.
Ph.D. thesis, Schoolof Computer Studies, University of Leeds.Hughes, John and Eric Atwell.
1994.
Theautomated evaluation of inferred wordclassifications.
In Eleventh EuropeanConference on Artificial Intelligence.Jelinek, Frederick.
1976.
Continuous peechrecognition by statistical methods.245Computational Linguistics Volume 22, Number 2Proceedings ofthe I.E.E.E., 64(4), April.Jelinek, Frederick.
1985.
The development ofan experimental discrete dictationrecogniser.
Proceedings ofthe I.E.E.E.,73(11).Jelinek, Frederick and Robert L. Mercer.1980.
Interpolated estimation of Markovsource parameters from sparse data.
InProceedings ofWorkshop on PatternRecognition i  Practice, pages 381-397,Amsterdam.Jelinek, Frederick, Robert L. Mercer, andSalim Roukos.
1990.
Classifying words forimproved statistical language models.
InProceedings ofthe International Conference onAcoustics, Speech and Signal Processing,pages 621-624, Albuquerque, NewMexico.Jelinek, Frederick, Robert L. Mercer, andSalim Roukos.
1992.
Principles of lexicallanguage modelling for speechrecognition.
In S. Furui andM.
M. Sondhi, editors, Advances in SpeechSignal Processing.
Maral Dekku, Inc.Johansson, Stig J., Eric S. Atwell, RogerGarside, and Geoffrey Leech.
1986.
TheTagged LOB Corpus: User's Manual.
TheNorwegian Centre for the Humanities,Bergen.Katz, Slava M. 1987.
Estimation ofprobabilities for sparse data for thelanguage model component of a speechrecogniser.
I.E.E.E.
Transactions onAcoustics, Speech and Signal Processing,ASSP-35(3):400-401, March.Kirkpatrick, S., C. D. Gelatt, andM.
P. Vecchi.
1983.
Optimization bysimulated annealing.
Science,220(4598):671-680, May.Kiss, George R. 1973.
Grammatical wordclasses: A learning process and itssimulation.
Psychology of Learning andMotivation, 7:1-41.Kneser, Reinhard and Hermann Ney.
1993.Forming word classes by statisticalclustering for statistical languagemodelling.
In R. Kfihler and B.
B. Rieger,editors, Contributions to QuantitativeLinguistics.
Kluwer Academic Publishers,pages 221-226.Kuhn, Ronald and Renato De Mori.
1990.
Acache-based natural anguage model forspeech recognition.
I.E.E.E.
Transactions onPattern Analysis and Machine Intelligence,12(6):570-583, June.Kupiec, Julian.
1992.
Robust part-of-speechtagging using a hidden Markov model.Computer Speech and Language, 6:225-242.Liddy, Elizabeth D. and Woojin Paik.
1992.Statistically-guided word sensedisambiguation.
I  Probabilistic Approachesto Natural Language.
American Associationfor Artificial Intelligence, AAAI Press.Technical report FS-92-05.Magerman, David M. 1994.
Natural LanguageParsing as Statistical Pattern Recognition.Ph.D.
thesis, Stanford UniversityComputer Science Department, February.Matsukawa, Tomoyoshi.
1993.Hypothesizing word association fromuntagged text.
In ARPA Workshop onHuman Language Technology, Princeton,March.McMahon, John.
1994.
Statistical LanguageProcessing Based on Self-Organising WordClassifi'cation.
Ph.D. thesis, Department ofComputer Science, Queen's University ofBelfast.McMahon, John and F. J. Smith.
1994.Structural tags, annealing and automaticword classification.
Artificial Intelligenceand the Simulation of Behaviour Quarterly, 90.Ney, Hermann, Ute Essen, and ReinhardKneser.
1994.
On structuring probabilisticdependencies in stochastic languagemodelling.
Computer Speech and Language,8:1-38.O'Boyle, Peter, Marie Owens, andF.
J. Smith.
1994.
A weighted averageN-gram model of natural anguage.Computer Speech and Language, 8:337-349.Pereira, Fernando and Naftali Tishby.
1992.Distributed similarity, phase transitionsand hierarchical c ustering.
In ProbabilisticApproaches to Natural Language.
AmericanAssociation for Artificial Intelligence,AAAI Press.
Technical report FS-92-05.Pereira, Fernando, Naftali Tishby, andLillian Lee.
1993.
Distributional clusteringof English words.
In Proceedings oftheAssociation for Computational Linguistics,pages 183-190.Rabiner, L. R. and B. J. Juang.
1986.
Anintroduction to hidden Markov models.I.E.E.E.
A.S.S.P.
Magazine, pages 4-16,January.Redington, Martin, Nick Chater, and StevenFInch.
1993.
Distributional informationand the acquisition of linguisticcategories: A statistical approach.
InProceedings ofthe Fifteenth AnnualConference ofthe Cognitive Science Society.Redington, Martin, Nich Chater, and StevenFinch.
1994.
The potential contribution ofdistributional Information to earlysyntactic ategory acquisition.Unpublished Report.Resnik, Philip S. 1993.
Selection andInformation: A Class-Based Approach toLexical Relationships.
Ph.D. thesis,Computer and Information Science,University of Pennsylvania, December.246McMahon and Smith Improving Statistical Language ModelsInstitute for Research in Cognitive ScienceReport I.R.C.S.-93-42.Sampson, Geoffrey.
1987.
Evidence againstthe grammatical/ungrammaticaldistinction.
In Wilem Meijs, editor, CorpusLinguistics and Beyond--Proceedings of theSeventh International Conference on EnglishLanguage Research on Computerized Corpora.Rodopi, Amsterdam, pages 219-226.Schfitze, Hinrich.
1993.
Part-of-speechinduction from scratch.
In Proceedings ofthe Association for Computational Linguistics31, pages 251-258.Scht/tze, Hinrich.
1995.
Distributionalpart-of-speech tagging.
In Proceedings ofthe Seventh European Chapter of theAssociation for Computational Linguistics,University College Dublin, March.Ueberla, Joerg.
1994.
Analysing a simplelanguage model--some generalconclusions for lanaguage models forspeech recognition.
Computer Speech andLanguage, 8:153-176.Yarowsky, David.
1992.
Word-sensedisarnbiguation using statistical models ofRoget's categories trained on largecorpora.
In Proceedings ofthe FifteenthInternational Conference on ComputationalLinguistics, pages 454-460.Zipf, George K. 1949.
Human Behaviour andthe Principle of Least Effort.
Addison-Wesley.247
