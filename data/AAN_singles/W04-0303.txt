Efficient incremental beam-search parsingwith generative and discriminative modelsBrian RoarkCenter for Spoken Language UnderstandingOGI School of Science & Engineering, Oregon Health & Science UniversityExtended Abstract:This talk will present several issues related to incre-mental (left-to-right) beam-search parsing of natu-ral language using generative or discriminative mod-els, either individually or in combination.
The firstpart of the talk will provide background in incre-mental top-down and (selective) left-corner beam-search parsing algorithms, and in stochastic modelsfor such derivation strategies.
Next, the relative ben-efits and drawbacks of generative and discriminativemodels with respect to heuristic pruning and searchwill be discussed.
A range of methods for using mul-tiple models during incremental parsing will be de-tailed.
Finally, we will discuss the potential for ef-fective use of fast, finite-state processing, e.g.
part-of-speech tagging, to reduce the parsing search spacewithout accuracy loss.
POS-tagging is shown to im-prove efficiency by as much as 20-25 percent withthe same accuracy, largely due to the treatment of un-known words.
In contrast, an ?islands-of-certainty?approach, which quickly annotates labeled bracket-ing over low-ambiguity word sequences, is shown toprovide little or no efficiency gain over the existingbeam-search.The basic parsing approach that will be describedin this talk is stochastic incremental top-down pars-ing, using a beam-search to prune the search space.Grammar induction occurs from an annotated tree-bank, and non-local features are extracted from eachderivation to enrich the stochastic model.
Left-cornergrammar and tree transforms can be applied to thetreebank or the induced grammar, either fully or se-lectively, to change the derivation order while retain-ing the same underlying parsing algorithm.
This ap-proach has been shown to be accurate, relatively effi-cient, and robust using both generative and discrim-inative models (Roark, 2001; Roark, 2004; Collinsand Roark, 2004).The key to effective beam-search parsing is com-parability of analyses when the pruning is done.
Iftwo competing parses are at different points in theirrespective derivations, e.g.
one is near the end of thederivation and another is near the beginning, then itwill be difficult to evaluate which of the two is likelyto result in a better parse.
With a generative model,comparability can be accomplished by the use of alook-ahead statistic, which estimates the amount ofprobability mass required to extend a given deriva-tion to include the word(s) in the look-ahead.
Ev-ery step in the derivation decreases the probabilityof the derivation, but also takes the derivation onestep closer to attaching to the look-ahead.
For goodparses, the look-ahead statistic should increase witheach step of the derivation, ensuring a certain degreeof comparability among competing parses with thesame look-ahead.Beam-search parsing using an unnormalized dis-criminative model, as in Collins and Roark (2004),requires a slightly different search strategy than theoriginal generative model described in Roark (2001;2004).
This alternate search strategy is closer to theapproach taken in Costa et al (2001; 2003), in thatit enumerates a set of possible ways of attaching thenext word before evaluating with the model.
This en-sures comparability for models that do not have thesort of behavior described above for the generativemodels, rendering look-ahead statistics difficult toestimate.
This approach is effective, although some-what less so than when a look-ahead statistic is used.A generative parsing model can be used on itsown, and it was shown in Collins and Roark (2004)that a discriminative parsing model can be used onits own.
Most discriminative parsing approaches,e.g.
(Johnson et al, 1999; Collins, 2000; Collinsand Duffy, 2002), are re-ranking approaches, inwhich another model (typically a generative model)presents a relatively small set of candidates, whichare then re-scored using a second, discriminativelytrained model.
There are other ways to combine agenerative and discriminative model apart from wait-ing for the former to provide a set of completed can-didates to the latter.
For example, the scores canbe used simultaneously; or the generative model canpresent candidates to the discriminative model at in-termediate points in the string, rather than simply atthe end.
We discuss these options and their potentialbenefits.Finally, we discuss and present a preliminary eval-uation of the use of rapid finite-state tagging to re-duce the parsing search space, as was done in (Rat-naparkhi, 1997; Ratnaparkhi, 1999).
When the pars-ing algorithm is integrated with model training, suchefficiency improvements can be particularly impor-tant.
POS-tagging using a simple bi-tag model im-proved parsing efficiency by nearly 25 percent with-out a loss in accuracy, when 1.2 tags per word wereproduced on average by the tagger.
Producing a sin-gle tag sequence for each string resulted in furtherspeedups, but at the loss of 1-2 points of accuracy.We show that much, but not all, of the speedup fromPOS-tagging is due to more constrained tagging ofunknown words.In a second set of trials, we make use of whatwe are calling ?syntactic collocations?, i.e.
collo-cations that are (nearly) unambiguously associatedwith a particular syntactic configuration.
For ex-ample, a chain of auxiliaries in English will alwayscombine in a particular syntactic configuration, mod-ulo noise in the annotation.
In our approach, the la-beled bracketing spanning the sub-string is treatedas a tag for the sequence.
A simple, finite-statemethod for finding such collocations, and an effi-cient longest match algorithm for labeling stringswill be presented.
The labeled-bracketing ?tags?
areintegrated with the parse search as follows: when aderivation reaches the first word of such a colloca-tion, the remaining words are attached in the givenconfiguration.
This has the effect of extending thelook-ahead beyond the collocation, as well as po-tentially reducing the amount of search required toextend the derivations to include the words in thecollocation.
However, while POS-tagging improvedefficiency, we find that using syntactic collocationsdoes not, indicating that ?islands-of-certainty?
ap-proaches are not what is needed from shallow pro-cessing; rather genuine dis-ambiguation of the sortprovided by the POS-tagger.AcknowledgmentsMost of this work was done while the author was atAT&T Labs - Research.
Some of it was in collabora-tion with Michael Collins.ReferencesMichael Collins and Nigel Duffy.
2002.
New rank-ing algorithms for parsing and tagging: Kernelsover discrete structures and the voted perceptron.In Proceedings of the 40th Annual Meeting of theAssociation for Computational Linguistics, pages263?270.Michael Collins and Brian Roark.
2004.
Incremen-tal parsing with the perceptron algorithm.
In Pro-ceedings of the 42nd Annual Meeting of the Asso-ciation for Computational Linguistics.
To appear.Michael J. Collins.
2000.
Discriminative rerankingfor natural language parsing.
In The Proceedingsof the 17th International Conference on MachineLearning.Fabrizio Costa, Vincenzo Lombardo, Paolo Frasconi,and Giovanni Soda.
2001.
Wide coverage in-cremental parsing by learning attachment prefer-ences.
In Conference of the Italian Association forArtificial Intelligence (AIIA), pages 297?307.Fabrizio Costa, Paolo Frasconi, Vincenzo Lombardo,and Giovanni Soda.
2003.
Towards incrementalparsing of natural language using recursive neuralnetworks.
Applied Intelligence.
to appear.Mark Johnson, Stuart Geman, Steven Canon, ZhiyiChi, and Stefan Riezler.
1999.
Estimators forstochastic ?unification-based?
grammars.
In Pro-ceedings of the 37th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 535?541.Adwait Ratnaparkhi.
1997.
A linear observed timestatistical parser based on maximum entropy mod-els.
In Proceedings of the Second Conference onEmpirical Methods in Natural Language Process-ing (EMNLP-97), pages 1?10.Adwait Ratnaparkhi.
1999.
Learning to parse natu-ral language with maximum entropy models.
Ma-chine Learning, 34:151?175.Brian Roark.
2001.
Probabilistic top-down parsingand language modeling.
Computational Linguis-tics, 27(2):249?276.Brian Roark.
2004.
Robust garden path parsing.Natural Language Engineering, 10(1):1?24.
