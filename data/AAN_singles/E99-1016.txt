Proceedings of EACL '99Cascaded Markov  Mode lsThorsten BrantsUniversit/it des Saarlandes, ComputerlinguistikD-66041 Saarbriicken, Germanythorsten@coli, uni-sb, deAbstractThis paper presents a new approach topartial parsing of context-free structures.The approach is based on Markov Mod-els.
Each layer of the resulting structureis represented byits own Markov Model,and output of a lower layer is passed asinput to the next higher layer.
An em-pirical evaluation of the method yieldsvery good results for NP/PP chunking ofGerman ewspaper texts.1 IntroductionPartial parsing, often referred to as chunking, isused as a pre-processing step before deep analysisor as shallow processing for applications like in-formation retrieval, messsage extraction and textsummarization.
Chunking concentrates on con-structs that can be recognized with a high degreeof certainty.
For several applications, this type ofinformation with high accuracy is more valuablethan deep analysis with lower accuracy.We will present a new approach to partial pars-ing that uses Markov Models.
The presentedmodels are extensions of the part-of-speech tag-ging technique and are capable of emitting struc-ture.
They utilize context-free grammar rules andadd left-to-right transitional context information.This type of model is used to facilitate the syntac-tic annotation of the NEGRA corpus of Germannewspaper texts (Skut et al, 1997).Part-of-speech tagging is the assignment ofsyn-tactic categories (tags) to words that occur in theprocessed text.
Among others, this task is ef-ficiently solved with Markov Models.
States ofa Markov Model represent syntactic ategories(or tuples of syntactic ategories), and outputsrepresent words and punctuation (Church, 1988;DeRose, 1988, and others).
This technique of sta-tistical part-of-speech tagging operates very suc-cessfully, and usually accuracy rates between 96and 97% are reported for new, unseen text.Brants et al (1997) showed that the techniqueof statistical tagging can be shifted to the nextlevel of syntactic processing and is capable of as-signing grammatical functions.
These are func-tions like subject, direct object, head, etc.
Theymark the function of a child node within its par-ent phrase.Figure 1 shows an example sentence and itsstructure.
The terminal sequence is complemen-ted by tags (Stuttgart-Tiibingen-Tagset, Thielenand Schiller, 1995).
Non-terminal nodes are la-beled with phrase categories, edges are labeledwith grammatical functions (NEGRA tagset).In this paper, we will show that Markov Mod-els are not restricted to the labeling task (i.e., theassignment ofpart-of-speech labels, phrase labels,or labels for grammatical functions), but are alsocapable of generating structural elements.
We willuse cascades of Markov Models.
Starting withthe part-of-speech layer, each layer of the result-ing structure is represented by its own MarkovModel.
A lower layer passes its output as inputto the next higher layer.
The output of a layercan be ambiguous and it is complemented by aprobability distribution for the alternatives.This type of parsing is inspired by finite statecascades which are presented by several authors.CASS (Abney, 1991; Abney, 1996) is a partialparser that recognizes non-recursive basic phrases(chunks) with finite state transducers.
Eachtransducer mits a single best analysis (a longestmatch) that serves as input for the transducer atthe next higher level.
CASS needs a special gram-mar for which rules are manually coded.
Eachlayer creates a particular subset of phrase types.FASTUS (Appelt et al, 1993) is heavily basedon pattern matching.
Each pattern is associatedwith one or more trigger words.
It uses a series ofnon-deterministic finite-state ransducers tobuildchunks; the output of one transducer is passed118Proceedings of EACL '99,D,\]an Arbeit und GelciAPPR NN KON NNof work and moneyEin enormer Posten wird von den 37 beteiligten Vereinen aufgebrachtART ADJA NN VAFIN APPR ART CARD ADJA NN WPPAn enormous amount is by the 37 involved organizations raised'A large amount of money and work was raised by the involved organizations'Figure 1: Example sentence and annotation.
The structure consists of terminal nodes (words and theirparts-of-speech), non-terminal nodes (phrases) and edges (labeled with grammatical functions).as input to the next transducer.
(Roche, 1994)uses the fix point of a finite-state transducer.
Thetransducer is iteratively applied to its own out-put until it remains identical to the input.
Themethod is successfully used for efficient processingwith large grammars.
(Cardie and Pierce, 1998)present an approach to chunking based on a mix-ture of finite state and context-free techniques.They use N P rules of a pruned treebank grammar.For processing, each point of a text is matchedagainst he treebank rules and the longest matchis chosen.
Cascades of automata nd transducerscan also be found in speech processing, see e.g.
(Pereira et al, 1994; Mohri, 1997).Contrary to finite-state transducers, CascadedMarkov Models exploit probabilities when pro-cessing layers of a syntactic structure.
They donot generate longest matches but most-probablesequences.
Furthermore, a higher layer sees dif-ferent alternatives and their probabilities for thesame span.
It can choose a lower ranked alterna-tive if it fits better into the context of the higherlayer.
An additional advantage is that CascadedMarkov Models do not need a "stratified" gram-mar (i.e., each layer encodes a disjoint subset ofphrases).
Instead the system can be immediatelytrained on existing treebank data.The rest of this paper is structured as follows.Section 2 addresses the encoding of parsing pro-cesses as Markov Models.
Section 3 presents Cas-caded Markov Models.
Section 4 reports on theevaluation of Cascaded Markov Models using tree-bank data.
Finally, section 5 will give conclusions.2 Encoding of SyntacticalInformation as Markov ModelsWhen encoding a part-of-speech tagger as aMarkov Model, states represent syntactic cate-gories 1 and outputs represent words.
Contex-tual probabilities of tags are encoded as transi-tion probabilities of tags, and lexical probabilitiesof the Markov Model are encoded as output prob-abilities of words in states.We introduce a modification to this encoding.States additionally may represent non-terminalcategories (phrases).
These new states emit par-tial parse trees (cf.
figure 2).
This can be seen ascollapsing a sequence of terminals into one non-terminal.
Transitions into and out of the newstates are performed in the same way as for wordsand parts-of-speech.Transitional probabilities for this new type ofMarkov Models can be estimated from annotateddata in a way very similar to estimating proba-bilities for a part-of-speech tagger.
The only dif-ference is that sequences of terminals may be re-placed by one non-terminal.Lexical probabilities need a new estimationmethod.
We use probabilities of context-free par-tim parse trees.
Thus, the lexical probability ofthe state NP in figure 2 is determined byP(NP ~ ART ADJA NN,ART ~ ein, ADJA --~ enormer, NN ~ Posten)= P(NP ~ ART ADJA NN)?
P(ART ~ ein)- P(ADJA --+ enormer)?
P(NN -+ Posten)Note that the last three probabilities are the sameas for the part-of-speech model.1Categories and states directly correspond in bi-gram models.
For higher order models, tuples of cat-egories are combined to one state.119Proceedings of EACL '99z K" A o.
_za.
n- u.
Z K"~, a. a.
< ~.
n Z <- o.
> < " ~ '~ 12.
> e~ --~ z ~ z ~.
a: e~ rr E. ~.
O a_/ I\P(AINP)IP(anlAPPR)/ I'~p(AICNP)IIVAFINJ?/ P(Z~IPP) ~P(aufgebrachtlVVPp)/ ~ ~ a'n / ~ k w i rd~/ /  k 'X~aufgebrachtART ADJA NN NN KON NN APPR ART CARD ADJANNein enormer Posten Arbeit und Geld von den 37 beteiligten VereinenFigure 2: Part of the Markov Models for layer I that is used to process the sentence of fignre 1.
Contraryto part-of-speech tagging, outputs of states may consist of structures with probabilities according to astochastic context-free grammar.3 Cascaded Markov  Mode lsThe basic idea of Cascaded Markov Models is toconstruct the parse tree layer by layer, first struc-tures of depth one, then structures of depth two,and so forth.
For each layer, a Markov Model de-termines the best set of phrases.
These phrasesare used as input for the next layer, which addsone more layer.
Phrase hypotheses at each layerare generated according to stochastic context-freegrammar rules (the outputs of the Markov Model)and subsequently filtered from left to right byMarkov Models.Figure 3 gives an overview of the parsing model.Starting with part-of-speech tagging, new phrasesare created at higher layers and filtered by MarkovModels operating from left to right.3.1 Tagging LatticesThe processing example in figure 3 only shows thebest hypothesis at each layer.
But there are alter-native phrase hypotheses and we need to deter-mine the best one during the parsing process.All rules of the generated context-free grammarwith right sides that are compatible with part ofthe sequence are added to the search space.
Fig-ure 4 shows an example for hypotheses atthe firstlayer when processing the sentence of figure 1.Each bar represents one hypothesis.
The positionof the bar indicates the covered words.
It is la-beled with the type of the hypothetical phrase,an index in the upper left corner for later ref-erence, the negative logarithm of the probabilitythat this phrase generates the terminal yield (i.e.,the smaller the better; probabilities for part-of-speech tags are omitted for clarity).
This part isvery similar to chart entries of a chart parser.All phrases that are newly introduced at thislayer are marked with an asterisk (*).
They areproduced according to context-free rules, basedon the elements passed from the next lower layer.The layer below layer 1 is the part-of-speech layer.The hypotheses form a lattice, with the wordboundaries being states and the phrases beingedges.
Selecting the best hypothesis means to findthe best path from node 0 to the last node (node14 in the example).
The best path can be effi-ciently found with the Viterbi algorithm (Viterbi,1967), which runs in time linear to the length ofthe word sequence.
Having this view of finding thebest hypothesis, processing of a layer is similar toword lattice processing in speech recognition (cf.Samuelsson, 1997).Two types of probabilities are important whensearching for the best path in a lattice.
First,these are probabilities ofthe hypotheses (phrases)generating the underlying terminal nodes (words).They are calculated according to a stochasticcontext-free grammar and given in figure 4.
Thesecond type are context probabilities, i.e., theprobability that some type of phrase follows orprecedes another.
The two types of probabilitiescoincide with lexical and contextual probabilitiesof a Markov Model, respectively.According to a trigram model (generated froma corpus), the path in figure 4 that is marked greyis the best path in the lattice.
Its probability iscomposed ofPbesfP(NP\[$, $)P(NP ~* ein enormer Posten)?
P(APPRI$, NP)P(APPR ~ an)?
P(CNPINP, APPR)P(?NP ~* Arbeit und Geld)?
P(VAFINIAPPR , CNP)P(VAFIN --+ wird)120Proceedings of EACL '9932>,"10Input I== ~ ~-Cascaded Markov Models~, {Z @art-of-Speech Tagging~ ( Gramma"t~al )(.~.Kronos haben mit ihrer MusikBrOckengeschlagen ~!~:!
:~:~:~ ' !
Kronos haben mit ihrer MusikBrOckengeschlagenKronos have w i th  their music bridges built"Kronos built bridges with their music"Figure 3: The combined, layered processing model.
Starting with part-of-speech tagging (layer 0), pos-sibly ambiguous output together with probabilities i passed to higher layers (only the best hypothesesare shown for clarity).
At each layer, new phrases and grammatical functions are added.-P(PPICNP, VAFIN)P(PP =~* yon den 37 beteiligten Vereinen)?
P(VVPP\]VAFIN, PP)P(VVPP --+ aufgebracht)?
P($1PP, VVPP).Start and end of the path are indicated by adollar sign ($).
This path is very close to the cor-rect structure for layer 1.
The CNP and PP arecorrectly recognized.
Additionally, the best pathcorrectly predicts that APPR, VAFIN and VVPPshould not be attached in layer 1.
The only erroris the NP ein enormer Posten.
Although this is onits own a perfect NP, it is not complete becausethe PP an Arbeit und Geld is missing.
ART, ADJAand NN should be left unattached in this layer inorder to be able to create the correct structure athigher layers.The presented Markov Models act as filters.The probability of a connected structure is de-termined only based on a stochastic ontext-freegrammar.
The joint probabilities of unconnectedpartial structures are determined by additionallyusing Markov Models.
While building the struc-ture bottom up, parses that are unlikely accordingto the Markov Models are pruned.3.2 The  MethodThe standard Viterbi algorithm is modified in or-der to process Markov Models operating on lat-tices.
In part-of-speech tagging, each hypothesis(a tag) spans exactly one word.
Now, a hypothesiscan span an arbitrary number of words, and thesame span can be covered by an arbitrary num-ber of alternative word or phrase hypotheses.
Us-ing terms of a Markov Model, a state is allowedto emit a context-free partial parse tree, startingwith the represented non-terminal symbol, yield-ing part of the sequence of words.
This is in con-trast to standard Markov Models.
There, statesemit atomic symbols.
Note that an edge in the lat-tice is represented by a state in the correspondingMarkov Model.
Figure 2 shows the part of theMarkov Model that represents the best path inthe lattice of figure 4.The equations of the Viterbi algorithm areadapted to process a language model operatingon a lattice.
Instead of the words, the gaps be-tween the words are enumerated (see figure 4),and an edge between two states can span one ormore words, such that an edge is represented bya triple <t, t', q>, starting at time t, ending at timet' and representing state q.We introduce accumulators At,t, (q) that col-lect the maximum probability of state q coveringwords from position t to t '.
We use 6i,j (q) to de-note the probability of the deriviation emitted bystate q having a terminal yield that spans posi-tions i to j.
These are needed here as part of theaccumulators A.Initialization:Ao,t(q) = P(qlqs)6o,t(q) (1)121Proceedings of EACL '9929NM* 9.23  \]12sNp * 8.63  \[I~sAP * zo.2s I ~:~CN~*..: ':::::i;~OS\] ~6pp, 10.23 IF'=NP * zz.s* I1;7 ~,:~ :: : ,,:~ :: :; :;~;':,: 1 ,  :,li pP I * o.s I 'NP* ,2.2  J'?NP* ,.,0 1 I ?AP * 9.2 I  .00 II"PP* 0.22 II ?AP* iI I I I I I I t I I I i I I0 Ein 1 2 3 5 6 8 9 1037 II, ~12.
13 .
14 enor- Po- an 4 Ar- und Geld 7 wird von den oetel- ver- autge-met sten beit ligten einen brachtFigure 4: Phrase hypotheses according to a context-free grammar for the first layer.
Hypotheses markedwith an asterisk (*) are newly generated at this layer, the others are passed from the next lower layer(layer 0: part-of-speech tagging).
The best path in the lattice is marked grey.Recursion:At,t, (q) = max At,,,t(q')P(qlq')6t,t, (q),(t,,,t,q,>ELattice(2)for l<t  <T.Termination:max P(Q, Lattice) = m_ax At T(q)P(qe\]q).QEQ.
* (t,T,q)eUattice '(3)Additionally, it is necessary to keep track of the el-ements in the lattice that maximized each At,r (q).When reaching time T, we get the best last ele-ment in the lattice(t~ n, T, q~n) = argmax At,T(q)P(qe\[q).
(4)<t,T,q>eLatticeSetting t~ n = T, we collect the arguments<t", t, q') E Lattice that maximized equation 2 bywalking backwards in time:rn  rn  m (ti+i,ti , qi+i) =,p  m ,g~ ~, argmax At,,,t 7 (q) (q~ Iq ) t, ,t,_ x(q~)<t,',t T ,a,>?Lattice(5)for i > 1, until we reach t~ = 0.
Now, q~.
.
.
q~is the best sequence of phrase hypotheses (readbackwards).3.3 Pass ing Ambigu i ty  to the Next  LayerThe process can move on to layer 2 after the firstlayer is computed.
The results of the first layer aretaken as the base and all context-free rules thatapply to the base are retrieved.
These again forma lattice and we can calculate the best path forlayer 2.The Markov Model for layer 1 operates on theoutput of the Markov Model for part-of-speechtagging, the model for layer 2 operates on the out-put of layer 1, and so on.
Hence the name of theprocessing model: Cascaded Markov Models.Very often, it is not sufficient o calculate justthe best sequences of words/tags/phrases.
Thismay result in an error leading to subsequent er-rors at higher layers.
Therefore, we not only cal-culate the best sequence but several top rankedsequences.
The number of the passed hypothesesdepends on a pre-defined threshold ~ > 1.
We se-lect all hypotheses with probabilities P > Pbest/8.These are passed to the next layer together withtheir probabilities.3.4 Parameter Est imat ionTransitional parameters for Cascaded MarkovModels are estimated separately for each layer.Output parameters are the same for all layers,they are taken from the stochastic ontext-freegrammar that is read off the treebank.Training on annotated ata is straight forward.First, we number the layers, starting with 0 forthe part-of-speech layer.
Subsequently, informa-tion for the different layers is collected.Each sentence in the corpus represents onetraining sequence for each layer.
This sequenceconsists of the tags or phrases at that layer.
Ifa span is not covered by a phrase at a particularlayer, we take the elements of the highest layerbelow the actual layer.
Figure 5 shows the train-ing sequences for layers 0 - 4 generated from thesentence in figure 1.
Each sentence gives rise toone training sequence for each layer.
Contextualparameter estimation is done in analogy to modelsfor part-of-speech tagging, and the same smooth-ing techniques can be applied.
We use a linearinterpolation of uni-, bi-, and trigram models.A stochastic ontext-free grammar is read offthe corpus.
The rules derived from the anno-tated sentence in figure 1 are also shown in figure5.
The grammar is used to estimate output pa-rameters for all Markov Models, i.e., they are the122Proceedings of EACL '99L.3er Sequence SNP VAFIN VP2 ART ADJA NN PP VAFIN VPi ART ADJA NN APPR CNP VAFIN PP VVPP0 ART ADJA NN APPR NN KON NN VAFIN APPR ART CARD ADJA NN VVPPContext-free rules and their frequenciesS --> NP VAFIN VP (1) PP ~ APPR ART CARD ADJA NN (1)NP --> ART ADJA NN PP (1) ART --> Ein (1)PP .--)- APPR CNP (I) ADJA --).
enormer (I)CNP ~ NN KON NN (I) .
.
.
.
.
.VP -+ PP VVPP (I) VVPP -+ aufgebracht (i)Figure 5: Training material generated from the sentence in figure 1.
The sequences for layers 0 - 4 areused to estimate transition probabilities for the corresponding Markov Models.
The context-free rulesare used to estimate the SCFG, which determines the output probabilities of the Markov Models.same for all layers.
We could estimate probabil-ities for rules separately for each layer, but thiswould worsen the sparse data problem.4 ExperimentsThis section reports on results of experiments withCascaded Markov Models.
We evaluate chunkingprecision and recall, i.e., the recognition of kernelNPs and PPs.
These exclude prenominal adverbsand postnominal PPs and relative clauses, but in-clude all other prenominal modifiers, which can befairly complex adjective phrases in German.
Fig-ure 6 shows an example of a complex N P and theoutput of the parsing process.For our experiments, we use the NEGRA corpus(Skut et al, 1997).
It consists of German news-paper texts (Frankfurter Rundschau) that are an-notated with predicate-argument structures.
Weextracted all structures for NPs, PPs, APs, AVPs(i.e., we mainly excluded sentences, VPs and co-ordinations).
The version of the corpus used con-tains 17,000 sentences (300,000 tokens).The corpus was divided into training part (90%)and test part (10%).
Experiments were repeated10 times, results were averaged.
Cross-evaluationwas done in order to obtain more reliable perfor-mance estimates than by just one test run.
Inputof the process is a sequence of words (dividedinto sentences), output are part-of-speech tagsand structures like the one indicated in figure 6.Figure 7 presents results of the chunking taskusing Cascaded Markov Models for different num-bers of layers.
2 Percentages are slightly belowthose presented by (Skut and Brants, 1998).
But2The figure indicates unlabeled recall and preci-sion.
Differences to labeled recall/precision are small,since the number of different non-terminal categoriesis very restricted.they started with correctly tagged data, so ourtask is harder since it includes the process of part-of-speech tagging.Recall increases with the number of layers.
Itranges from 54.0% for 1 layer to 84.8% for 9 lay-ers.
This could be expected, because the num-ber of layers determines the number of phrasesthat can be parsed by the model.
The additionalline for "topline recall" indicates the percentage ofphrases that can be parsed by Cascaded MarkovModels with the given number of layers.
All nodesthat belong to higher layers cannot be recognized.Precision slightly decreases with the number oflayers.
It ranges from 91.4% for 1 layer to 88.3%for 9 layers.The F-score is a weighted combination of recallR and precision P and defined as follows:F - (/32 + 1)PR/32p -b R (6)/3 is a parameter ncoding the importance of recalland precision.
Using an equal weight for both(/3 = 1), the maximum F-score is reached for 7layers (F =86.5%).The part-of-speech tagging accuracy slightly in-creases with the number of Markov Model layers(bottom line in figure 7).
This can be explained bytop-down decisions of Cascaded Markov Models.A model at a higher layer can select a tag with alower probability if this increases the probabilityat that layer.
Thereby some errors made at lowerlayers can be corrected.
This leads to the increaseof up to 0.3% in accuracy.Results for chunking Penn Treebank datawere previously presented by several authors(Ramshaw and Marcus, 1995; Argamon et al,1998; Veenstra, 1998; Cardie and Pierce, 1998).These are not directly comparable to our results,123Proceedings of EACL '99die von der Bundesregierung angestrebte Entlassung des Bundes aus einzelnen BereichenART APPR ART NN ADJA NN ART NN APPR ADJA NNthe by the government intended dismissal (of) the federation f rom several areas'the dismissal of the federation from several areas that was intended by the government'Figure 6: Complex German NP and chunker output (postnominal genitive and PP are not attached)..2~9~9NEGKA Corpus: Chunking Results10090807O6O196.2Topline Recallrain = 72.6%max= 100.0%?
Recall u ~  0 0 0 0= = = - rain = 54.0%/ I ~  max= 84.8%o Precisionrain = 88.3%max= 91.4%I I i I I I I2 3 4 5 6 7 8 9 # Layers96.3 96.4 96.4 96.5 96.5 96.5 96.5 96.5 % POS accuracyFigure 7: NP/PP chunking results for the NEGI~A Corpus.
The diagram shows recall and precisiondepending on the number of layers that are used for parsing.
Layer 0 is used for part-of-speech tagging,for which tagging accuracies are given at the bottom line.
Topline recall is the maximum recall possiblefor that number of layers.because they processed a different language andgenerated only one layer of structure (the chunkboundaries), while our algorithm also generatesthe internal structure of chunks.
But generally,Cascaded Markov Models can be reduced to gen-erating just one layer and can be trained on PennTreebank data.5 Conc lus ion  and  Future  WorkWe have presented a new parsing model for shal-low processing.
The model parses by represent-ing each layer of the resulting structure as a sep-arate Markov Model.
States represent categoriesof words and phrases, outputs consist of partialparse trees.
Starting with the layer for part-of-speech tags, the output of lower layers is passedas input to higher layers.
This type of model isrestricted to a fixed maximum number of layers inthe parsed structure, since the number of MarkovModels is determined before parsing.
While theeffects of these restrictions on the parsing of sen-tences and VPs are still to be investigated, we ob-tain excellent results for the chunking task, i.e.,the recognition of kernel NPs and PPs.It will be interesting to see in future work if Cas-caded Markov Models can be extended to parsingsentences and VPs.
The average number of lay-ers per sentence in the NEGRA corpus is only 5;99.9% of all sentences have 10 or less layers, thusa very limited number of Markov Models wouldbe sufficient.Cascaded Markov Models add left-to-rightcontext-information t  context-free parsing.
Thiscontextualization is orthogonal to another impor-tant trend in language processing: lexicalization.We expect that the combination of these tech-niques results in improved models.We presented the generation ofparameters fromannotated corpora and used linear interpolationfor smoothing.
While we do not expect ira-124Proceedings of EACL '99provements by re-estimation on raw data, othersmoothing methods may result in better accura-cies, e.g.
the maximum entropy framework.
Yet,the high complexity of maximum entropy parame-ter estimation requires careful pre-selection of rel-evant linguistic features.The presented Markov Models act as filters.The probability of the resulting structure is de-termined only based on a stochastic ontext-freegrammar.
While building the structure bottomup, parses that are unlikely according to theMarkov Models are pruned.
We think that acombined probability measure would improve themodel.
For this, a mathematically motivated com-bination needs to be determined.AcknowledgementsI would like to thank Hans Uszkoreit, YvesSchabes, Wojciech Skut, and Matthew Crocker forfruitful discussions and valuable comments on thework presented here.
And I am grateful to SabineKramp for proof-reading this paper.This research was funded by the DeutscheForschungsgemeinschaft in the Sonderforschungs-bereich 378, Project C3 NEGRA.ReferencesSteven Abney.
1991.
Parsing by chunks.
InRobert Berwick, Steven Abney, and CarolTenny, editors, Principle-Based Parsing, Dor-drecht.
Kluwer Academic Publishers.Steven Abney.
1996.
Partial parsing via finite-state cascades.
In Proceedings of the ESSLLIWorkshop on Robust Parsing, Prague, CzechRepublic.D.
Appelt, J. Hobbs, J.
Bear, D. J. Israel, andM.
Tyson.
1993.
FASTUS: a finite-state proces-sor for information extraction from real-worldtext.
In Proceedings of IJCAI-93, Washington,DC.Shlomo Argamon, Ido Dagan, and Yuval Kry-molowski.
1998.
A memory-based approach tolearning shallow natural anguage patterns.
InProceedings of the 17th International Confer-ence on Computational Linguistics COLING-ACL-98), Montreal, Canada.Thorsten Brants, Wojciech Skut, and BrigitteKrenn.
1997.
Tagging grammatical functions.In Proceedings of the Conference on Empir-ical Methods in Natural Language ProcessingEMNLP-97, Providence, RI, USA.Claire Cardie and David Pierce.
1998.
Error-driven pruning of treebank grammars for basenoun phrase identification.
In Proceedings ofthe 17th International Conference on Compu-tational Linguistics COLING-A CL-98), Mon-treal, Canada.Kenneth Ward Church.
1988.
A stochasticparts program and noun phrase parser for unre-stricted text.
In Proceedings of the Second Con-ference on Applied Natural Language ProcessingANLP-88, pages 136-143, Austin, Texas, USA.Steven J. DeRose.
1988.
Grammatical cate-gory disambiguation bystatistical optimization.Computational Linguistics, 14(1):31-39.Mehryar Mohri.
1997.
Finite-state transducers inlanguage and speech processing.
ComputationalLinguistics, 23(2).Fernando Pereira, Michael Riley, and RichardSproat.
1994.
Weighted rational transductionsand their application to human language pro-cessing.
In Proceedings of the Workshop on Hu-man Language Technology, San Francisco, CA.Morgan Kanfmann.Lance A. Ramshaw and Mitchell P. Marcus.1995.
Text chunking using transformation-based learning.
In Proceedings of the thirdWorkshop on Very Large Corpora, Dublin, Ire-land.Emmanuel Roche.
1994.
Two parsing algorithmsby means of finite state transducers.
In Proceed-ings of the 15th International Conference onComputational Linguistics COLING-94, pages431-435, Kyoto, Japan.Christer Samuelsson.
1997.
Extending n-gram tagging to word graphs.
In Proceed-ings of the 2nd International Conference on Re-cent Advances in Natural Language ProcessingRANLP-97, Tzigov Chark, Bulgaria.Wojciech Skut and Thorsten Brants.
1998.A maximum-entropy artial parser for unre-stricted text.
In Sixth Workshop on Very LargeCorpora, Montreal, Canada.Wojciech Skut, Brigitte Krenn, Thorsten Brants,and Hans Uszkoreit.
1997.
An annotationscheme for free word order languages.
In Pro-ceedings of the Fifth Conference on AppliedNatural Language Processing ANLP-97, Wash-ington, DC.Christine Thielen and Anne Schiller.
1995.Ein kleines und erweitertes Tagset ffirsDeutsche.
In Tagungsberichte des Arbeitstr-effens Lexikon + Text 17./18.
Februar 1994,Schlofl Hohentiibingen.
Lexicographica SeriesMajor, Tiibingen.
Niemeyer.Jorn Veenstra.
1998.
Fast NP chunking usingmemory-based learning techniques.
In Proceed-ings of the Eighth Belgian-Dutch Conference onMachine Learning, Wageningen.A.
Viterbi.
1967.
Error bounds for convolutionalcodes and an asymptotically optimum decodingalgorithm.
In IEEE Transactions on Informa-tion Theory, pages 260-269.125
