In: Proceedings of CoNLL-2000 and LLL-PO00, pages 1-6, Lisbon, Portugal, 2000.Learning in Natural Language: Theory and AlgorithmicApproaches*Dan RothDepartment  of Computer  ScienceUniversity of Illinois at Urbana-Champaign1304 W Springfield Ave., Urbana, IL 61801danr@cs, uiuc.
eduAbst rac tThis article summarizes work on developing alearning theory account for the major learningand statistics based approaches used in naturallanguage processing.
It shows that these ap-proaches can all be explained using a single dis-tribution free inductive principle related to thepac model of learning.
Furthermore, they allmake predictions using the same simple knowl-edge representation - a linear representationover a common feature space.
This is signifi-cant both to explaining the generalization androbustness properties of these methods and tounderstanding how these methods might be ex-tended to learn from more structured, knowl-edge intensive xamples, as part of a learningcentered approach to higher level natural lan-guage inferences.1 In t roduct ionMany important natural language inferencescan be viewed as problems of resolving phonetic,syntactic, semantics or pragmatics ambiguities,based on properties of the surrounding context.It is generally accepted that a learning compo-nent must have a central role in resolving thesecontext sensitive ambiguities, and a significantamount of work has been devoted in the last fewyears to developing learning methods for thesetasks, with considerable success.
Yet, our un-derstanding of when and why learning works inthis domain and how it can be used to supportincreasingly higher level tasks is still lacking.This article summarizes work on developing alearning theory account for the major learningapproaches used in NL.While the major statistics based methodsused in NLP are typically developed with a* This research is supported by NSF grants IIS-9801638,SBR-9873450 and IIS-9984168.Bayesian view in mind, the Bayesian principlecannot directly explain the success and robust-ness of these methods, since their probabilisticassumptions typically do not hold in the data.Instead, we provide this explanation using a sin-gle, distribution free inductive principle relatedto the pac model of learning.
We describe theunified learning framework and show that, inaddition to explaining the success and robust-ness of the statistics based methods, it also ap-plies to other machine learning methods, suchas rule based and memory based methods.An important component of the view devel-oped is the observation that most methods usethe same simple knowledge representation.
Thisis a linear representation over a new featurespace - a transformation f the original instancespace to a higher dimensional nd more expres-sive space.
Methods vary mostly algorithmicly,in ways they derive weights for features in thisspace.
This is significant both to explainingthe generalization properties of these methodsand to developing an understanding forhow andwhen can these methods be extended to learnfrom more structured, knowledge intensive x-amples, perhaps hierarchically.
These issues arebriefly discussed and we emphasize the impor-tance of studying knowledge representation a dinference in developing a learning centered ap-proach to NL inferences.2 Learning FrameworksGenerative probability models provide a princi-pled way to the study of statistical classificationin complex domains uch as NL.
It is commonto assume a generative model for such data, es-timate its parameters from training data andthen use Bayes rule to obtain a classifier forthis model.
In the context of NL most clas-sifters are derived from probabilistic languagemodels which estimate the probability of a sen-tence 8 using Bayes rule, and then decomposethis probability into a product of conditionalprobabilities according to the generative model.Pr(s) = Pr(wl, W2, .
.
.
Wn) ----= H~=lPr(wilwl,...wi-1) = H~=lPr(wilhi)where hi is the relevant history when predictingwi, and s is any sequence of tokens, words, part-of-speech (pos) tags or other terms.This general scheme has been used to de-rive classifiers for a variety of natural lan-guage applications including speech applica-tions (Rab89), pos tagging (Kup92; Sch95),word-sense ambiguation (GCY93) and context-sensitive spelling correction (Go195).
While theuse of Bayes rule is harmless, most of the workin statistical language modeling and ambiguityresolution is devoted to estimating terms of theform Pr(wlh ).
The generative models used toestimate these terms typically make Markov orother independence assumptions.
It is evidentfrom studying language data that these assump-tions are often patently false and that thereare significant global dependencies both withinand across sentences.
For example, when using(Hidden) Markov Model (HMM) as a generativemodel for pos tagging, estimating the probabil-ity of a sequence of tags involves assuming thatthe pos tag ti of the word wi is independent ofother words in the sentence, given the preced-ing tag ti-1.
It is not surprising therefore thatthis results in a poor estimate of the probabil-ity density function.
However, classifiers builtbased on these false assumptions neverthelessseem to behave quite robustly in many cases.A different, distribution free inductive princi-ple that is related to the pac model of learningis the basis for the account developed here.In an instance of the agnostic variant of paclearning (Val84; Hau92; KSS94), a learner isgiven data elements (x, l) that are sampled ac-cording to some fixed but arbitrary distribu-tion D on X x {0, 1}.
X is the instance spaceand I E {0, 1} is the label 1.
D may simply re-flect the distribution of the data as it occurs"in nature" (including contradictions) withoutassuming that the labels are generated accord-ing to some "rule".
Given a sample, the goalof the learning algorithm is to eventually out-put a hypothesis h from some hypothesis class7/ that closely approximates the data.
The1The model can be extended todeal with any discreteor continuous range of the labels.true error of the hypothesis h is defined tobe errorD(h) = Pr(x,O~D\[h(x) 7~ If, and thegoal of the (agnostic) pac learner is to com-pute, for any distribution D, with high prob-ability (> 1 -5 ) ,  a hypothesis h E 7/ withtrue error no larger than ~ + inffhenerrorD(h).In practice, one cannot compute the true er-ror errorD(h).
Instead, the input to the learn-ing algorithm is a sample S = {(x i,l)}i=li m ofm labeled examples and the learner tries tofind a hypothesis h with a small empirical er-ror errors(h) = I{x e Slh(x) ?
l}l/ISl, andhopes that it behaves well on future examples.The hope that a classifier learned from a train-ing set will perform well on previously unseenexamples is based on the basic inductive prin-ciple underlying learning theory (Val84; Vap95)which, stated informally, guarantees that if thetraining and the test data are sampled from thesame distribution, good performance on largeenough training sample guarantees good per-formance on the test data (i.e., good "true" er-ror).
Moreover, the quality of the generalizationis inversely proportional to the expressivity ofthe class 7-/.
Equivalently, for a fixed samplesize IsI, the quantified version of this princi-ple (e.g.
(Hau92)) indicates how much can onecount on a hypothesis elected according to itsperformance on S. Finally, notice the underly-ing assumption that the training and test dataare sampled from the same distribution; thisframework addresses this issue.
(See (GR99).
)In our discussion functions learned over theinstance space X are not defined directly overthe raw instances but rather over a transforma-tion of it to a feature space.
A feature is an in-dicator function X : X ~ {0, 1} which defines asubset of the instance space - all those elementsin X which are mapped to 1 by X- X denotesa class of such functions and can be viewed asa transformation f the instance space; each ex-ample (Xl, .
.
.
xn) E X is mapped to an example(Xi,...Xlxl) in the new space.
We sometimesview a feature as an indicator function over thelabeled instance space X x {0, 1} and say thatX(x, l) = 1 for examples x E x (X)  with label l.3 Exp la in ing  Probabi l i s t ic  MethodsUsing the abovementioned inductive principlewe describe a learning theory account hat ex-plains the success and robustness of statisticsbased classifiers (Rot99a).
A variety of meth-ods used for learning in NL are shown to maketheir prediction using Linear Statistical Queries(LSQ) hypotheses.
This is a family of linearpredictors over a set of features which are di-rectly related to the independence assumptionsof the probabilistic model assumed.
The successof these classification methods is then shown tobe due to the combination of two factors:?
Low expressive power of the derived classifier.?
Robustness properties hared by all linear sta-tistical queries hypotheses.Since the hypotheses are computed over a fea-ture space chosen so that they perform well ontraining data, learning theory implies that theyperform well on previously unseen data, irre-spective of whether the underlying probabilisticassumptions hold.3.1 Robust  Learn ingThis section defines a learning algorithm anda class of hypotheses with some generaliza-tion properties, that capture many probabilis-tic learning methods used in NLP.
The learn-ing algorithm is a Statistical Queries(SQ) algo-r i thm (Kea93).
An SQ algorithm can be viewedas a learning algorithm that interacts with itsenvironment in a restricted way.
Rather thanviewing examples, the algorithm only requeststhe values of various statistics on the distribu-tion of the examples to construct its hypothesis.(E.g.
"What is the probability that a randomlychosen example (x, l) has xi = 0 and l = 1"?
)A statistical query has the form IX, l, 7-\], whereX 6 X is a feature, l 6 {0, 1} is a further (op-tional) restriction imposed on the query and ~"is an error parameter.
A call to the SQ oraclereturns an estimate ~n of \[x,z,~\]P,\[xDj\] = PrD{ (X, i)lx(x) = 1 A i = l}which satisfies \]15x - Px\] < T. (We usually omitT and/or  l from this notation.)
A statisticalqueries algorithm is a learning algorithm thatconstructs its hypothesis only using informationreceived from an SQ oracle.
An algorithm issaid to use a query space X if it only makesqueries of the form \[X, l, T\] where X 6 A'.
AnSQ algorithm is said to be a good learning al-gorithm if, with high probability, it outputs ahypothesis h with small error, using sample sizethat is polynomial in the relevant parameters.Given a query \[X, l, T\] the SQ oracle is sim-ulated by drawing a large sample S of labeledexamples (x, l) according to D and evaluatingPrs \ [x(x ,  l)\] = I{(x ,  l) : X(x, l) = l l} / IS l .Chernoff bounds guarantee that the nUmber ofexamples required to achieve tolerance T withprobability at least 1 - 5 is polynomial in 1/Tand log 1/5.
(See (Zea93; Dec93; AD95)).Let X be a class of features and f : {0, 1}a function that depends only on the values~D for E X.
Given x 6 X, a Linear Statis- \[x,~\] Xtical Queries (LSQ) hypothesis predictsl argmaxte{o,1} ~xeX ^ D = f \ [x j \ ]  ({P\[x,z\] } ) "  X(x).Clearly, the LSQ is a linear discriminator overthe feature space A', with coefficients f thatare computed given (potentially all) the values^D P\[x,t\]" The definition generalizes naturally tonon-binary classifiers; in this case, the discrim-inator between predicting l and other values islinear.
A learning algorithm that outputs anLSQ hypothesis is called an LSQ algorithm.Example  3.1 The naive Bayes predictor(DH73) is derived using the assumption thatgiven the label l E L the features' values arestatistically independent.
Consequently, theBayes optimal prediction is given by:h(x) = argmaxteLH~n=l Pr(xill)Pr(1),where Pr(1) denotes the prior probability of l(the fraction of examples labeled l) and Pr(xill)are the conditional feature probabilities (thefraction of the examples labeled l in which theith feature has value xi).
Therefore, we get:Cla im:  The naive Bayes algorithm is an LSQalgorithm over a set ,.~ which consists of n + 1features: X0 --- 1, Xi -- xi for i = 1 , .
.
.
,nand where f\[1J\]O = log/5\[~z\],, and f\[x,J\]O =^D ^D logP\[x,,l\]/P\[1,l\], i = 1,... ,n.The observation that the LSQ hypothesisis linear over X' yields the first generalizationproperty of LSQ.
VC theory implies that theVC dimension of the class of LSQ hypothe-ses is bounded above by IXI.
Moreover, ifthe LSQ hypothesis is sparse and does notmake use of unobserved features in X (as inEx.
3.1) it is bounded by the number of featuresused (Rot00).
Together with the basic general-ization property described above this implies:Coro l la ry  3.1 For LSQ, the number of train-ing examples required in order to maintain aspecific generalization performance guaranteescales linearly with the number o/features used.3The robustness property of LSQ can be castfor the case in which the hypothesis i learnedusing a training set sampled according to a dis-tribution D, but tested over a sample from D ~.It still performs well as long as the distributionaldistance d(D, D') is controlled (Rot99a; Rot00).Theorem 3.2 Let .A be an SQ(T, X) learningalgorithm for a function class ~ over the distri-bution D and assume that d(D, D I) < V (for Vinversely polynomial in T).
Then .A is also anSQ(T, ,~') learning algorithm for ~ over D I.Finally, we mention that the robustness of thealgorithm to different distributions depends onthe sample size and the richness of the featureclass 2?
plays an important role here.
Therefore,for a given size sample, the use of simpler fea-tures in the LSQ representation provides betterrobustness.
This, in turn, can be traded off withthe ability to express the learned function withan LSQ over a simpler set of features.3.2 Addi t ional  ExamplesIn addition to the naive Bayes (NB) classifierdescribed above several other widely used prob-abilistic classifiers can be cast as LSQ hypothe-ses.
This property is maintained even if the NBpredictor is generalized in several ways, by al-lowing hidden variables (GR00) or by assuminga more involved independence structure aroundthe target variable.
When the structure is mod-eled using a general Bayesian etwork (since wecare only about predicting a value for a singlevariable having observed the others) the Bayesoptimal predictor is an LSQ hypothesis over fea-tures that are polynomials X = IIxilxi2 ?
.. xik ofdegree that depends on the number of neighborsof the target variable.
A specific case of greatinterest o NLP is that of hidden Markov Mod-els.
In this case there are two types of variables,state variables S and observed ones, O (Rab89).The task of predicting the value of a state vari-able given values of the others can be cast as anLSQ, where X C {S, O, 1} ?
{S, O, 1}, a suitablydefined set of singletons and pairs of observablesand state variables (Rot99a).Finally, Maximum Entropy (ME) models(Jay82; Rat97) are also LSQ models.
In thisframework, constrains correspond to features;the distribution (and the induced classifier) aredefined in terms of the expected value of the fea-tures over the training set.
The induced clas-sifter is a linear classifier whose weights are de-rived from these expectations; the weights axecomputed iteratively (DR72) since no closedform solution is known for the optimal values.4 Learning Linear ClassifiersIt was shown in (Rot98) that several otherlearning approaches widely used in NL workalso make their predictions by utilizing a lin-ear representation.
The SNoW learning archi-tecture (Rot98; CCRR99; MPRZ99) is explic-itly presented this way, but this holds also formethods that are presented in different ways,and some effort is required to cast them thisway.
These include Brill's transformation basedmethod (Bri95) 2, decision lists (Yax94) andback-off estimation (Kat87; CB95).Moreover, even memory-based methods(ZD97; DBZ99) can be cast in a similarway (Rot99b).
They can be reformulated asfeature-based algorithms, with features of typesthat are commonly used by other features-basedlearning algorithms in NLP; the prediction com-puted by MBL can be computed by a linearfunction over this set of features.Some other methods that have been recentlyused in language related applications, such asBoosting (CS99) and support vector machinesare also making use of the same representation.At a conceptual level all learning methodsare therefore quite similar.
They transform theoriginal input (e.g., sentence, sentence+pos in-formation) to a new, high dimensional, featurespace, whose coordinates are typically smallconjunctions (n-grams) over the original input.In this new space they search for a linear func-tion that best separates the training data, andrely on the inductive principle mentioned toyield good behavior on future data.
Viewed thisway, methods are easy to compare and analyzefor their suitability to NL applications and fu-ture extensions, as we sketch below.The goal of blowing up the instance space to ahigh dimensional space is to increase the expres-sivity of the classifier so that a linear functioncould represent the target concepts.
Within thisspace, probabilistic methods are the most lim-ited since they do not actually search in theeThis holds only in cases in which the TBL condi-tions do not depend on the labels, as in Context Sensi-tive Spelling (MB97) and Prepositional Phrase Attach-ment (BR94) and not in the general case.4space of linear functions.
Given the featurespace they directly compute the classifier.
Ingeneral, even when a simple linear function gen-erates the training data, these methods are notguaranteed to be consistent with it (Rot99a).However, if the feature space is chosen so thatthey are, the robustness properties shown abovebecome significant.
Decision lists and MBLmethods have advantages in their ability to rep-resent exceptions and small areas in the featurespace.
MBL, by using long and very specializedconjunctions (DBZ99) and decision lists, due totheir functional form - a linear function withexponentially decreasing weights - at the costof predicting with a single feature, rather thana combination (Go195).
Learning methods thatattempt to find the best linear function (relativeto some loss function) are typically more flexi-ble.
Of these, we highlight here the SNoW ar-chitecture, which has some specific advantagesthat favor NLP-like domains.SNoW determines the features' weights usingan on-line algorithm that attempts to minimizethe number of mistakes on the training data us-ing a multiplicative weight update rule (Lit88).The weight update rule is driven by the maxi-mum entropy principle (KW95).
The main im-plication is that SNoW has significant advan-tages in sparse spaces, those in which a few ofthe features are actually relevant to the tar-get concept, as is typical in NLP.
In domainswith these characteristics, for a given numberof training examples, SNoW generalizes betterthan additive update methods like perceptronand its close relative SVMs (Ros58; FS98) (andin general,it has better learning curves).Furthermore, although in SNoW the transfor-mation to a large dimensional space needs to bedone explicitly (rather than via kernel functionsas is possible in perceptron and SVMs) its use ofvariable size examples nevertheless gives it com-putational advantages, due to the sparse featurespace in NLP applications.
It is also significantfor extensions to relational domain mentionedlater.
Finally, SNoW is a multi-class classifier.5 Future  Research  I ssuesResearch on learning in NLP needs to be inte-grated with work on knowledge representationand inference to enable studying higher level NLtasks.
We mention two important directions theimplications on the learning issues.The unified view presented reveals that allmethods blow up the dimensionality of the orig-inal space in essentially the same way; they gen-erate conjunctive features over the linear struc-ture of the sentence (i.e., n-gram like features inthe word and/or pos space).This does not seem to be expressive nough.Expressing complex concepts and relationsnecessary for higher level inferences will re-quire more involved intermediate representa-tions ("features") over the input; higher orderstructural and semantic properties, long termdependencies and relational predicates need tobe represented.
Learning will stay manageableif done in terms of these intermediate r presen-tations as done today, using functionally simplerepresentations (perhaps cascaded).Inductive logic programming (MDR94;Coh95) is a natural paradigm for this.
How-ever, computational limitations that includeboth learnability and subsumption render thisapproach inadequate for large scale knowledgeintensive problems (KRV99; CR00).In (CR00) we suggest an approach that ad-dresses the generation of complex and relationalintermediate r presentations and supports effi-cient learning on top of those.
It allows thegeneration and use of structured examples whichcould encode relational information and longterm functional dependencies.
This is done us-ing a construct hat defines "types" of (poten-tially, relational) features the learning processmight use.
These represent infinitely many fea-tures, and are not generated explicitly; onlythose present in the data are generated, on thefly, as part of the learning process.
Thus ityields hypotheses that are as expressive as re-lational earners in a scalable fashion.
This ap-proach, however, makes some requirements onthe learning process.
Most importantly, thelearning approach needs to be able to processvariable size examples.
And, it has to be featureefficient in that its complexity depends mostlyon the number of relevant features.
This seemsto favor the SNoW approach over other algo-rithms that learn the same representation.Eventually, we would like to perform infer-ences that depend on the outcomes of severaldifferent classifiers; together these might need tocoherently satisfy some constrains arising fromthe sequential nature of the data or task and do-main specific issues.
There is a need to study,along with learning and knowledge representa-tion, inference methods that suit this frame-work (KR97).
Work in this direction requires aconsistent semantics of the learners (Val99) andwill have implications on the knowledge repre-sentations and learning methods used.
Prel im-inary work in (PRO0) suggests everal ways toformalize this problem and is evaluated in thecontext of identifying phrase structure.ReferencesJ.
A. Aslam and S. E. Decatur.
Specification and simu-lation of statistical query algorithms for efficiency andnoise tolerance.
In COLT 1995, pages 437-446.E.
Brill and P. Resnik.
A rule-based approach to prepo-sitional phrase attachment disambiguation.
In Proc.of COLING, 1994.E.
Brill.
Transformation-based error-driven learningand natural language processing: A case study inpart of speech tagging.
Computational Linguistics,21(4):543-565, 1995.M.
Collins and J Brooks.
Prepositional phrase attach-ment through a backed-off model.
In WVLC 1995.A.
Carleson, C. Cumby, J. Rosen, and D. Roth.The SNoW learning architecture.
Technical ReportUIUCDCS-R-99-2101, UIUC CS, May 1999.W.
Cohen.
PAC-learning recursive logic programs: Effi-cient algorithms.
JAIR, 2:501-539, 1995.C.
Cumby and D. Roth.
Relational representations thatfacilitate learning.
In KR 2000, pages 425-434.M.
Collins and Y.
Singer.
Unsupervised models for nameentity classification.
In EMNLP- VLC'99.W.
Daelemans, A. van den Bosch, and J. Zavrel.
Forget-ting exceptions i harmful in language learning.
Ma-chine Learning, 34(1-3):11-43, 1999.S.
E. Decatur.
Statistical queries and faulty PAC oracles.In COLT 1993, pages 262-268.R.
O. Duda and P. E. Hart.
Pattern Classification andScene Analysis.
Wiley, 1973.J.
N. Darroch and D. Ratcliff.
Generalized iterativescaling for log-linear models.
Annals of Mathemati-cal Statistics, 43(5):1470-1480, 1972.Y.
Freund and R. Schapire.
Large margin classificationusing the Perceptron algorithm.
In COLT 1998.W.
Gale, K. Church, and D. Yarowsky.
A method fordisambiguating word senses in a large corpus.
Com-puters and the Humanities, 26:415-439~ 1993.A.
R. Golding.
A Bayesian hybrid method for context-sensitive spelling correction.
In Proceedings of the 3rdworkshop on very large corpora, ACL-95, 1995.A.
R. Golding and D. Roth.
A Winnow based ap-proach to context-sensitive spelling correction.
Ma-chine Learning, 34(1-3):107-130, 1999.A.
Grove and D. Roth.
Linear concepts and hidden vari-ables.
Machine Learning, 2000.
To Appear.D.
Haussler.
Decision theoretic generalizations of thePAC model for neural net and other learning appli-cations.
Inform.
Comput., 100(1):78-150, 1992.E.
T. Jaynes.
On the rationale of maximum-entropymethods.
Proc.
of the IEEE, 70(9):939-952, 1982.S.
M. Katz.
Estimation of probabilities from sparse datafor the language model component of a speech recog-nizer.
IEEE Transactions on Acoustics, speech, andSignal Processing, 35(3):400-401, 1987.M.
Kearns.
Efficient noise-tolerant learning from statis-tical queries.
In COLT 1993, pages 392-401.R.
Khardon and D. Roth.
Learning to reason.
Journalof the ACM, 44(5):697-725, Sept. 1997.R.
Khardon, D. Roth, and L. G. Valiant.
Relationallearning for NLP using linear threshold elements.
InIJCAI 1999, pages 911-917.M.
J. Kearns, R. E. Schapire, and L. M. Sellie.
To-ward efficient agnostic learning.
Machine Learning,17(2/3):115-142, 1994.J.
Kupiec.
Robust part-of-speech tagging using a hid-den Markov model.
Computer Speech and Language,6:225-242, 1992.J.
Kivinen and M. K. Warmuth.
Exponentiated gradi-ent versus gradient descent for linear predictors.
InSTOC, 1995.N.
Littlestone.
Learning quickly when irrelevant at-tributes abound: A new linear-threshold algorithm.Machine Learning, 2:285-318, 1988.L.
Mangu and E. Brill.
Automatic rule acquisition forspelling correction.
In ICML 1997, pages 734-741.S.
Muggleton and L. De Raedt.
Inductive logic program-ming: Theory and methods.
Journal of Logic Pro-gramming, 20:629-679, 1994.M.
Munoz, V. Punyakanok, D. Roth, and D. Zimak.A learning approach to shallow parsing.
In EMNLP-VLC'99, pages 168-178.V.
Punyakanok and D. Roth.
Inference with classifiers.Technical Report UIUCDCS-R-2000-2181, UIUC CS.L.
R. Rabiner.
A tutorial on hidden Markov models andselected applications in speech recognition.
Proceed-ings of the IEEE, 77(2):257-285, 1989.A.
Ratnaparkhi.
A linear observed time statistical parserbased on maximum entropy models.
In EMNLP-97.F.
Rosenblatt.
The perceptron: A probabilistic modelfor information storage and organization i the brain.Psychological Review, 65:386-407, 1958.D.
Roth.
Learning to resolve natural anguage ambigui-ties: A unified approach.
In AAAI'98, pages 806-813.D.
Roth.
Learning in natural anguage.
In IJCAI 1999,pages 898-904.D.
Roth.
Memory based learning in NLP.
Technical Re-port UIUCDCS-R-99-2125, UIUC CS, March 1999.D.
Roth.
Learning in natural anguage.
Technical Re-port UIUCDCS-R-2000-2180, UIUC CS July 2000.H.
Schfitze.
Distributional part-of-speech tagging.
InEACL 1995.L.
G. Valiant.
A theory of the learnable.
Communica-tions of the ACM, 27(11):1134-1142, November 1984.L.
G. Valiant.
Robust logic.
In STOC 1999.V.
N. Vapnik.
The Nature of Statistical Learning Theory.Springer-Verlag, New York, 1995.D.
Yarowsky.
Decision lists for lexical ambiguity resolu-tion: application to accent restoration i Spanish andFrench.
In ACL 1994, pages 88-95.J.
Zavrel and W. Daelemans.
Memory-based learning:Using similarity for smoothing.
In ACL, 1997.
