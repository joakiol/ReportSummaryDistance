Proceedings of Third Workshop on Semantic Web and Information Extraction, pages 17?24,Dublin, Ireland, 24 August, 2014.Seed Selection for Distantly Supervised Web-Based Relation ExtractionIsabelle AugensteinDepartment of Computer ScienceThe University of SheffieldUnited Kingdomi.augenstein@dcs.shef.ac.ukAbstractIn this paper we consider the problem of distant supervision to extract relations (e.g.
origin(musicalartist, location)) for entities (e.g.
?The Beatles?)
of certain classes (e.g.
musical artist) from Webpages by using background information from the Linking Open Data cloud to automatically labelWeb documents which are then used as training data for relation classifiers.
Distant supervisionapproaches typically suffer from the problem of ambiguity when automatically labelling text,as well as the problem of incompleteness of background data to judge whether a mention is atrue relation mention.
This paper explores the hypothesis that simple statistical methods basedon background data can help to filter unreliable training data and thus improve the precision ofrelation extractors.
Experiments on a Web corpus show that an error reduction of 35% can beachieved by strategically selecting seed data.1 IntroductionOne important aspect to every relation extraction approach is how to annotate training and test data forlearning classifiers.
In the past, four different types of approaches for this have been proposed.For supervised approaches, training and test data is annotated manually by one or several annotators.While this approach results in a high-quality corpus, it is very expensive and time-consuming.
As aconsequence, the corpora used tend to be small and biased towards a certain domain or type of text.Unsupervised approaches do not need annotated data for training; they instead cluster similar wordsequences and generalise them to relations.
Although unsupervised aproaches can process very largeamounts of data, resulting relations are hard to map to particular schemas.
In addition, Fader et al.
(2011)observe that these approaches often produce uninformative or incoherent extractions.Semi-supervised methods are methods that only require a small number of seed instances.
Hand-craftedseeds are used to extract patterns from a corpus, which are then used to extract more instances and thoseagain to extract new patterns in an iterative way.
However, since many iterations are needed, these methodsare prone to semantic drift, i.e.
an unwanted shift of meaning.
As a consequence these methods require acertain amount of human effort - to create seeds initially and also to help keep systems ?on track?.A fourth group of approaches, distant supervision or self-supervised approaches, exploit big knowledgebases such as Freebase (2008) to automatically label entities in text and use the annotated text to extractfeatures and train a classifier (Wu and Weld, 2007; Mintz et al., 2009).
Unlike supervised systems, theydo not require manual effort to label data and can be applied to large corpora.
Since they extract relationswhich are defined by schemas, these approaches also do not produce informative or incoherent relations.Distant supervision approaches are based on the following assumption (Mintz et al., 2009):?If two entities participate in a relation, any sentence that contains those two entities might express thatrelation.?
In practice, if the information that two entities participate in a relation is contained in theknowledge base, whenever they appear in the same sentence, that sentence is used as positive training datafor that relation.
This heuristic causes problems if different entities have the same surface form.
ConsiderThis work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedings footerare added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/17the following example:?Let It Be is the twelfth album by The Beatles which contains their hit single ?Let It Be?.
?In that sentence, the first mention of Let It Be is an example of the album relation, whereas the secondmention is an example of the track relation.
If both mentions are used as positive training examples forboth relations, this impairs the learning of weights of the relation classifiers.
We therefore argue for thecareful selection of training data for distant supervision by using measures to discard highly ambiguoustraining examples.
One further aspect that can be problematic when automatically creating negativetraining data is incompleteness.
What Riedel et al.
(2010) point out, and our observations also confirm,is that about 20%, or even more, of all true relation mentions in a corpus are not contained in Freebase,although it is a very big knowledge base.The main contributions of this paper are: to propose and evaluate several measures for detecting anddiscarding unreliable seeds; and to document a distant supervision system for fine-grained class-basedrelation extraction on noisy data from the Web.2 Distantly Supervised Relation ExtractionDistant supervision is defined as the automatic labelling of a corpus with properties, P and entities, Efrom a knowledge base, KB to train a classifier to learn to predict relations.
Following previous distantsupervision approaches, we only consider binary relations of the form (s, p, o), consisting of a subject, apredicate and an object (Mintz et al., 2009).
We use the established Semantic Web formalisation, ratherthan unary and binary first order predicates, to reflect our special and consistent treatment of subjectsversus objects.
Each subject and object entity e ?
E has a set of lexicalisations, Le?
L. Furthermore, weconsider only those subjects which have a particular Freebase class C.3 Seed SelectionBefore using the automatically labelled corpus to train a classifier, we include a seed selection step, whichconsist of several measures to discard unreliable seeds.Ambiguity Within An EntityUnam: Our first approach is to discard lexicalisations of objects if they are ambiguous for the subjectentity, i.e.
if a subject is related to two different objects which have the same lexicalisation, and expresstwo different relations.
To illustrate this, let us consider the problem outlined in the introduction again:Let It Be can be both an album and a track of the subject entity The Beatles, therefore we would like todiscard Let It Be as a seed for the class Musical Artist.
We measure the degree to which a lexicalisationl ?
Loof an object o is ambiguous by the number of senses the lexicalisation has.
For a given subject s, ifwe discover a lexicalisation for a related entity, i.e.
(s, p, o) ?
KB and l ?
Lo, then, since it may be thecase that l ?
Lrfor some R 3 r , o, where also (s, q, r) ?
KB for some q ?
P, we say in this case that lhas a ?sense?
o and r, giving rise to ambiguity.
We then define Asl, the ambiguity of a lexicalisation withrespect to the subject as follows: Asl= |{e | l ?
Lo?
Lw?
(s, p, o) ?
KB ?
(s, v, w) ?
KB ?
w , o}|.Ambiguity Across ClassesIn addition to being ambiguous for a subject of a specific class, lexicalisations of objects can be ambiguousacross classes.
Our assumption is that the more senses an object lexicalisation has, the more likely it isthat that object occurence is confused with an object lexicalisation of a different property of any class.An example for this are common names of book authors or common genres as in the sentence ?Jackmentioned that he read On the Road?, in which Jack is falsely recognised as the author Jack Kerouac.Stop: One type of very ambiguous words with many senses are stop words.
Since some objects of relationsin our training set might have lexicalisations which are stop words, we discard those lexicalisations if theyappear in a stop word list (we use the one described in Lewis et al.
(2004)).Stat: For other highly ambiguous lexicalisations of object entities our approach is to estimate cross-classambiguity, i.e.
to estimate how ambiguous a lexicalisation of an object is compared to other lexicalisationsof objects of the same relation.
If its ambiguity is comparatively low, we consider it a reliable seed,18otherwise we want to discard it.
For the set of classes under consideration, we know the set of propertiesthat apply, D ?
P and can retrieve the set {o | (s, p, o) ?
KB?
p ?
D}, and retrieve the set of lexicalisationsfor each member, Lo.
We then compute Ao, the number of senses for every lexicalisation of an object Lo,where Ao= |{o | ?
Lo}|.We view the number of senses of each lexicalisation of an object per relation as a frequency distribution.We then compute min, max, median (Q2), the lower (Q1) and the upper quartile (Q3) of those frequencydistributions and compare it to the number of senses of each lexicalisation of an object.
If Al> Q, whereQ is either Q1, Q2 or Q3 depending on the model, we discard the lexicalisation of the object.IncompletessOne further aspect of knowledge bases that can be problematic when automatically creating negativetraining data is incompleteness.
Our method for creating negative training data is to assume that allentities which appear in a sentence with the subject s, but are not in a relation with it according to theknowledge base, can be used as negative training data.
Other distant supervision approaches (Mintz et al.,2009) follow a similar approach, but only use a random sample of unrelated entities pairs.Incomp: Our approach is to discard negative training examples which are likely to be truerelation mentions, but missing from the knowledge base.
If we find a lexicalisation l where@ o, p ?
l ?
Lo?
(s, p, o) ?
KB, then before we consider this a negative example we check if?
t ?
C ?
(t, q, r) ?
KB and l ?
Lr, i.e.
if any of the properties of the class we examine has an objectlexicalisation l.4 System4.1 CorpusTo create a corpus for Web relation extraction using Linked Data, three Freebase classes and their six toseven most prominent properties (see Table 1) are selected and their values retrieved using the FreebaseAPI.
To avoid noisy training data, entities which only have values for some of those relations were notused.
This resulted in 1800 to 2200 entities per class which were split equally for training and test.
Foreach entity, at most 10 Web pages were retrieved via the Google Search API using the search pattern??subject_entity?
class property_name?, e.g.
?
?The Beatles?
Musical Artist Origin?
resulting in a totalof 450,000 pages1.
By adding the class, we expect the retrieved Web pages to be more relevant to ourextraction task.
For entities, Freebase distinguishes between the most prominant lexicalisation (the entityname) and other lexicalisations (entity aliases).
We use the entity name for all of the search patterns.Class Property Class Property Class PropertyBook author Musical Artist album Politician birthdatecharacters active (start) birthplacepublication date active (end) educational institutiongenre genre nationalityISBN record label partyoriginal language origin religiontrack spousesTable 1: Freebase classes and properties we use for our evaluation4.2 NLP PipelineText content is extracted from HTML pages using the jsoup API2, which strips text from each elementrecurvisely.
Each paragraph is then processed with Stanford CoreNLP3to split the text into sentences,1URLs of those Web pages are available via http://staffwww.dcs.shef.ac.uk/people/I.Augenstein/SWAIE2014/2http://jsoup.org/3http://nlp.stanford.edu/software/corenlp.shtml19tokenise, POS tag it and normalise time expressions.
Named entities are classified using the 7 class (time,location, organisation, person, money, percent, date) named entity model.4.3 Relation candidate identificationSome of the objects of relations cannot be categorised according to the 7 named entity (NE) classesdetected by the Stanford named entity classifier (NERC) and are therefore not recognised, for exampleMusicalArtist:album or Book:genre.
Therefore, in addition to recognising entities with Stanford NERC,we also implement our own named entity recogniser (NER), which only recognises entity boundaries, butdoes not classify them.
To detect entity boundaries, we recognise sequences of nouns and sequences ofcapitalised words and apply both greedy and non-greedy matching.
For greedy matching, we considerwhole noun phrases and for non-greedy matching all subsequences starting with the first word of the thosephrases, i.e.
for ?science fiction book?, we would consider ?science fiction book?, ?science fiction?
and?book?
as candidates.
The reason to do greedy as well as non-greedy matching is because the lexicalisationof an object does not always span a whole noun phrase, e.g.
while ?science fiction?
is a lexicalisation ofan object of Book:genre, ?science fiction book?
is not.
However, for MusicalArtist:genre, ?pop music?would be a valid lexicalisation of an object.
We also recognise short sequences of words in quotes.
This isbecause lexicalisation of objects of MusicalArtist:track and MusicalArtist:album often appear in quotes,but are not necessarily noun phrases.4.4 Identifying Relation Candidates and Selecting SeedsThe next step is to identify which sentences potentially express relations.
We only use sentences fromWeb pages which were retrieved using a query which contains the subject of the relation.
We then select,or rather discard seeds for training according to the different methods outlined in Section 3.
Our baselinemodel does not discard any training seeds.4.5 FeaturesOur system uses some of the features described in Mintz et al.
(2009), and other standard lexical featuresand named entity features:?
The object occurrence?
The bag of words of the occurrence?
The number of words of the occurrence?
The named entity class of the occurrence assigned by the 7-class Stanford NERC?
A flag indicating if the object or the subject entity came first in the sentence?
The sequence of part of speech (POS) tags of the words between the subject and the occurrence?
The bag of words between the subject and the occurrence?
The pattern of words between the subject entity and the occurrence (all words except for nouns,verbs, adjectives and adverbs are replaced with their POS tag, nouns are replaced with their namedentity class if a named entity class is available)?
Any nouns, verbs, adjectives, adverbs or NEs in a 3-word window to the left of the occurrence?
Any nouns, verbs, adjectives, adverbs or NEs in a 3-word window to the right of the occurrence4.6 Classifier and ModelsAs a classifier, we choose a first-order conditional random field model (Lafferty et al., 2001).
We use thesoftware CRFSuite4and L-BFGS (Nocedal, 1980) for training our classifiers.
We train one classifier perFreebase class and model.
Our models only differ in the way training data is selected (see Section 3).
Themodels are then used to classify each object candidate into one of the relations of the Freebase class orNONE (no relation).4http://www.chokkan.org/software/crfsuite/204.7 Merging and Ranking ResultsWe understand relation extraction as the task of predicting the relations which can be found in a corpus.While some approaches aim at correctly predicting every single mention of a relation seperately, weinstead choose to aggregate predictions of relation mentions.
For every Freebase class, we get all relationmentions from the corpus and the classifier?s confidence values for Freebase classes assigned to objectoccurences.
There are usually several different predictions, e.g.
the same occurence could be predicted tobe MusicalArtist:album, MusicalArtist:origin and MusicalArtist:NONE.
By aggregating relation mentionsacross documents we have increased chances of choosing the right relation, since some contexts ofoccurences are inconclusive or ambiguous and thus the classifier chooses the wrong property wfor those.For a given lexicalisation l, representing an object to which the subject is related, the classifier gives eachobject occurence a prediction which is the combination of a predicted relation and a confidence.
Wecollect these across the chosen documents to form a set of confidence values, for each predicted relation,per lexicalisation Elp.
For instance if the lexicalisation l occurs three times across the documents and ispredicted to represent an object to relation p1once with confidence 0.2, and in other cases to representthe object to relation p2with confidence 0.1 and 0.5 respectively, then Elp1= 0.2 and Elp2= {0.1, 0.5}.
Inorder to form an aggregated confidence for each relation with respect to the lexicalisation, gpl, we calculatethe mean average for each such set and normalise across relations, as follows: glp= Elp?|Elp|?q?P|Elq|5 Evaluation5.1 CorpusAlthough we automatically annotate the training and test part of the Web corpus with properties, wehand-annotate a portion of the test corpus.
The portion of the corpus we manually annotate is the onewhich has NONE predictions for object occurrences, i.e.
for which occurences do not have a representationin Freebase.
They could either get NONE predictions because they are not relation mentions, or becausethey are missing from Freebase.
We find that on average 45% of the occurences which are predicted byour models are true relation mentions, but missing from Freebase.
Note that some of the automaticallyevaluated positive predictions could in fact be false positives.5.2 ResultsFigures 1 and 2 show the precision with respect to confidence and precision@K for our self-supervisedrelation extraction models which only differ in the way training data is selected, as described in Section 3.0.70.750.80.850.90.9510  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9unam_stop_stat25unam_stop_stat50unam_stop_stat75unam_stopstopbaselineincomplFigure 1: Precision / confidence graph0.880.90.920.940.960.9810  200  400  600  800  1000  1200  1400  1600  1800  2000unam_stop_stat50unam_stop_stat75unam_stopstopunam_stop_stat25baselineincomplFigure 2: Precision@KFigure 1 shows the precision of our models on the y-axis with respect to a cutoff at a minimumconfidence displayed on the x-axis.
The precision at a minimum confidence of 0 is equivalent to theprecision over the whole test corpus.
For all of the models, the precision rises with increasing confidence,which means our confidence measure succeeds at ranking results by precision.
With respect to the baseline21which does not filter seeds, our best-performing model increases the total precision from 0.825 to 0.896,which is a total error reduction of 35%.
We achieve the best results in terms of total precision withthe model unam_stop_stat25, which filters lexicalisations which are ambiguous for a subject, filterslexicalisations which are stop words and filters lexicalisations with an ambiguity value higher than thelower quartile of the distribution for the relation in question.
The worst-performing model is, surprisingly,incompl, the model we built to discard negative training data which are likely to be true relation mentions,but missing from the knowledge base.
We discuss this further in Section 8.
Figures 2 shows the precision,sorted by confidence, for the K highest ranked documents.
We decided to use the precision@K measureinstead of computing recall because it is not feasible to manually annotate 450,000 Web pages withrelation mentions.
Note, however, that distant supervision is not aimed at high recall anyway - becauseonly sentences which contain both the subject and the object entity explicitely are used, many relationmentions will be missed out on.
The highest value on the x-axis is the number of predicted relations of themodel with the smallest number of predicted relations.
The models which filter seeds improve all abovethe baseline in terms of precision@K for 0% to about 65% of the maximum K, from 65% to 100%, onlystop and unam_stop improve on the baseline.6 DiscussionAlthough we cannot directly compare our results to that of other distantly supervised relation extractionmodels because we use different evaluation data and a different set of relations, our baseline model, whichhas a total precision of 0.825, as well as our best-performing model, which has a total precision of 0.896seem to be perform as well as, if not better than previous systems.
Overall, our seed selection methodsseem to perform well at removing unreliable training data to improve precision.What is still unsuccessful is our incompl model.
The idea behind it was that relations which, for a givensubject, have more than one object (e.g.
Book:genre) are prone to be ?incomplete?
- the objects in theknowledge base are often just the most prominent ones and other objects, which could be discovered fromtext, are missing.
When annotating training data for distant supervision, those missing objects wouldbe considered negative training data, which could potentially be harmful for training.
However, justassuming that all negative training examples could potentially be false negatives if they match one ofthe objects does not lead to improved results.
One of the reasons for this could be that most of thosepotential false negatives are instead objects of relations which expect the same kinds of values - and thuscrucial for training the models.
Some relations for which we observed this are are Book:originalLanguageand Book:translations, as well as Book:firstPublicationDate and Book:dateWritten.
Interestingly, neitherBook:originalLanguage nor Book:firstPublicationDate are n:n relations.7 Related WorkWhile lots of approaches in the past have focused on supervised, unsupervised (Yates et al., 2007; Fader etal., 2011) or semi-supervised relation extraction (Hearst, 1992; Carlson et al., 2010), there have also beensome distantly supervised relation extraction approaches in the past few years, which aim at exploitingbackground knowledge for relation extraction, most of them for extracting relations from Wikipedia.Mintz et al.
(2009) describe one of the first distant supervision approaches which aims at extractingrelations between entities in Wikipedia for the most frequent relations in Freebase.
They report precisionof about 0.68 for their highest ranked 10% of results depending what features they used.
In contrastto our approach, Mintz et al.
do not experiment with changing the distance supervision assumption orremoving ambiguous training data, they also do not use fine-grained relations and their approach is notclass-based.
Nguyen et al.
(2011)?s approach is very similar to that of Mintz et al.
(2009), except thatthey use a different knowledge base, YAGO (Suchanek et al., 2008).
They use a Wikipedia-based NERC,which, like the Stanford NERC classifies entities into persons, relations and organisations.
They report aprecision of 0.914 for their whole test set, however, those results might be skewed by the fact that YAGOis a knowledge based derived from Wikipedia.A few strategies for seed selection for distant supervision have already been investigated: At-least-onemodels (Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2010; Yao et al., 2010; Min et al., 2013),22hierarchical topic models (Alfonseca et al., 2012; Roth and Klakow, 2013), pattern correlations (Takamatsuet al., 2012), and an information retrieval approach (Xu et al., 2013).
At-least-one models (Hoffmannet al., 2011; Surdeanu et al., 2012; Riedel et al., 2010; Yao et al., 2010; Min et al., 2013) are basedon the idea that ?if two entities particpate in a relation, at least one sentence that mentions these twoentities might express that relation?.
While positive results have been reported for those models, Riedel etal.
(Riedel et al., 2010) argues that it is challenging to train those models because they are quite complex.Hierarchical topic models (Alfonseca et al., 2012; Roth and Klakow, 2013) assume that the context ofa relation is either specific for the pair of entities, the relation, or neither.
Min et al.
(Min et al., 2013)further propose a 4-layer hierarchical model to only learn from positive examples to address the problemof incomplete negative training data.
Pattern correlations (Takamatsu et al., 2012) are also based on theidea of examining the context of pairs of entities, but instead of using a topic model as a pre-processingstep for learning extraction patterns, they first learn patterns and then use a probabilistic graphical modelto group extraction patterns.
Xu et al.
(Xu et al., 2013) propose a two-step model based on the idea ofpseudo-relevance feedback which first ranks extractions, then only uses the highest ranked ones to re-traintheir model.
Our research is based on a different assumption: Instead of trying to address the problem ofnoisy training data by using more complicated multi-stage machine learning models, we want to examinehow background data can be even further exploited by testing if simple statistical methods based on dataalready present in the knowledge base can help to filter unreliable training data.8 Future WorkIn this paper, we have documented and evaluated an approach to discard unreliable seed data for distantlysupervised relation extraction.
Our two hypotheses were that discarding highly ambiguous relationmentions and discarding unreliable negative training seeds could help to improve precision of self-supervised relation extraction models.
While our evaluation indicates that discarding highly ambiguousrelation mentions based on simple statistical methods helps to improve the precision of distantly supervisedrelation extraction systems, discarding negative training data does not.
We have also described our distantlysupervised relation extraction system, which, unlike other previous systems learns to extract from Webpages and also learns to extract fine-grained relations for specific classes instead of relations which areapplicable to several broad classes.In future work, we want to work on increasing the number of extractions for distant supervision systems:The distant supervision assumption requires sentences to contain both the subject and the object of arelation.
While this ensures high precision and is acceptable for creating training data, most sentences - atleast those in Web documents - do not mention the subject of relations explicitly and we thus miss outon a lot of data to extract from.
We further want to extend our distant supervision approach to extractinformation not only from free text, but also from lists and relational tables from Web pages.
Finally, wewould like to train distantly supervised models for entity classification to assist relation extraction.
Amore detailed description of future work can also be found in Augenstein (2014).AcknowledgementsWe thank Barry Norton, Diana Maynard, as well as the anonymous reviewers for their valuable feed-back.
This research was partly supported by the EPSRC funded project LODIE: Linked Open Data forInformation Extraction, EP/J019488/1.ReferencesEnrique Alfonseca, Katja Filippova, Jean-Yves Delort, and Guillermo Garrido.
2012.
Pattern Learning for Rela-tion Extraction with a Hierarchical Topic Model.
In Proceedings of the 50th Annual Meeting of the Associationfor Computational Linguistics: Short Papers, volume 2, pages 54?59.Isabelle Augenstein.
2014.
Joint Information Extraction from the Web using Linked Data.
Doctoral ConsortiumProceedings of the 13th International Semantic Web Conference.
to appear.23Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor.
2008.
Freebase: A Collabora-tively Created Graph Database For Structuring Human Knowledge.
In Proceedings of the 2008 ACM SIGMODinternational conference on Management of data, pages 1247?1250.A.
Carlson, J. Betteridge, B. Kisiel, B.
Settles, E.R.
Hruschka Jr., and T.M.
Mitchell.
2010.
Toward an Architecturefor Never-Ending Language Learning.
In Proceedings of the Conference on Artificial Intelligence, pages 1306?1313.Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011.
Identifying relations for open information extraction.In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1535?1545.Marti A. Hearst.
1992.
Automatic Acquisition of Hyponyms from Large Text Corpora.
In Proceedings of the 14thInternational Conference on Computational Linguistics, pages 539?545.Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke S. Zettlemoyer, and Daniel S. Weld.
2011.
Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations.
In Proceedings of the 49thAnnual Meeting of the Association for Computational Linguistics, pages 541?550.John Lafferty, Andrew McCallum, and Fernando Pereira.
2001.
Conditional random fields: Probabilistic modelsfor segmenting and labeling sequence data.
In Proceedings of the 18th International Conference on MachineLearning, pages 282?289.D.
D. Lewis, Y. Yang, T. G. Rose, and F. Li.
2004.
RCV1: A New Benchmark Collection for Text CategorizationResearch.
Journal of Machine Learning Research, 5:361?397.Bonan Min, Ralph Grishman, Li Wan, Chang Wang, and David Gondek.
2013.
Distant Supervision for RelationExtraction with an Incomplete Knowledge Base.
In Proceedings of HLT-NAACL, pages 777?782.Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.
2009.
Distant supervision for relation extraction withoutlabeled data.
In Proceedings of ACL-IJCNLP, volume 2, pages 1003?1011.Truc-Vien T. Nguyen and Alessandro Moschitti.
2011.
End-to-End Relation Extraction Using Distant Supervi-sion from External Semantic Repositories.
In Proceedings of the 50th Annual Meeting of the Association forComputational Linguistics: Short Papers, volume 2, pages 277?282.Jorge Nocedal.
1980.
Updating Quasi-Newton Matrices with Limited Storage.
Mathematics of Computation,35(151):773?782.Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010.
Modeling Relations and Their Mentions withoutLabeled Text.
In Proceedings of the 2010 European conference on Machine learning and knowledge discoveryin databases: Part III, pages 148?163.Benjamin Roth and Dietrich Klakow.
2013.
Combining Generative and Discriminative Model Scores for DistantSupervision.
In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,pages 24?29.Fabian M Suchanek, Gjergji Kasneci, and Gerhard Weikum.
2008.
YAGO: A Large Ontology from Wikipediaand WordNet.
Web Semantics: Science, Services and Agents on the World Wide Web, 6(3):203?217.Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, and Christopher D. Manning.
2012.
Multi-instance Multi-label Learning for Relation Extraction.
In Proceedings of EMNLP-CoNLL, pages 455?465.Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.
2012.
Reducing Wrong Labels in Distant Supervisionfor Relation Extraction.
In Proceedings of the 50th Annual Meeting of the Association for ComputationalLinguistics, pages 721?729.Fei Wu and Daniel S. Weld.
2007.
Autonomously Semantifying Wikipedia.
In Proceedings of the Sixteenth ACMConference on Information and Knowledge Management, pages 41?50.Wei Xu, Raphael Hoffmann, Le Zhao, and Ralph Grishman.
2013.
Filling Knowledge Base Gaps for DistantSupervision of Relation Extraction.
In Proceedings of the 51st Annual Meeting of the Association for Computa-tional Linguistics, pages 665?670.Limin Yao, Sebastian Riedel, and Andrew McCallum.
2010.
Collective Cross-document Relation ExtractionWithout Labelled Data.
In Proceedings of the 2010 Conference on Empirical Methods in Natural LanguageProcessing, pages 1013?1023.Alexander Yates, Michael Cafarella, Michele Banko, Oren Etzioni, Matthew Broadhead, and Stephen Soderland.2007.
TextRunner: Open Information Extraction on the Web.
In Proceedings of HLT-NAACL: Demonstrations,pages 25?26.24
