SENSEVAL-3 TASKAutomatic Labeling of Semantic RolesKenneth C. LitkowskiCL Research9208 Gue RoadDamascus, MD 20872ken@clres.comAbstractThe SENSEVAL-3 task to perform automaticlabeling of semantic roles was designed toencourage research into and use of the FrameNetdataset.
The task was based on the considerableexpansion of the FrameNet data since thebaseline study of automatic labeling of semanticroles by Gildea and Jurafsky.
The FrameNet dataprovide an extensive body of ?gold standard?data that can be used in lexical semanticsresearch, as the basis for its further exploitationin NLP applications.
Eight teams participated inthe task, with a total of 20 runs.
Discussionsamong participants during development of thetask and the scoring of their runs contributed toa successful task.
Participants used a widevariety of techniques, investigating many aspectsof the FrameNet data.
They achieved resultsshowing considerable improvements from Gildeaand Jurafsky?s baseline study.
Importantly, theirefforts have contributed considerably to makingthe complex FrameNet dataset more accessible.They have amply demonstrated that FrameNet isa substantial lexical resource that will permitextensive further research and exploitation inNLP applications in the future.IntroductionWord-sense disambiguation has frequently beencriticized as a task in search of a reason.
Since aconsiderable portion of a sense inventory has only asingle sense, the question has been raised whether theamount of effort required by disambiguation isworthwhile.
Heretofore, the focus of disambiguationhas been on the sense inventory and has not examinedthe major reason why we would have lexicalknowledge bases: how the meanings would berepresented and thus, available for use in naturallanguage processing applications.
At the presenttime, a major paradigm for representing meaning hasemerged in frame semantics, specifically in theFrameNet project.A worthy objective for the Senseval communityis the development of a wide range of methods forautomating frame semantics, specifically identifyingand labeling semantic roles in sentences.
Animportant baseline study of this process has recentlyappeared in the literature (Gildea and Jurafsky,2002).
The FrameNet project (Johnson et al, 2003)has put together a body of hand-labeled data and theGildea and Jurafsky study has put together a set ofsuitable metrics for evaluating the performance of anautomatic system.1 The Senseval-3 TaskThis Senseval-3 task calls for the development ofsystems to meet the same objectives as the Gildea andJurafsky study.
The data for this task is a sample ofthe FrameNet hand-annotated data.
Evaluation ofsystems is measured using precision and recall offrame elements and overlap of a system?s frameelement sentence positions with those identified in theFrameNet data.The basic task for Senseval-3 is: Given asentence, a target word and its frame, identify theframe elements within that sentence and tag themwith the appropriate frame element name.The FrameNet project has just released a majorrevision (FrameNet 1.1) to its database, with 487frames using 696 distinctly-named  frame elements(although it is not guaranteed that frame elementswith the same name have the same meaning).
Thisrelease includes 132,968 annotated sentences (mostlytaken from the British National Corpus).
TheSenseval-3 task used 8,002 of these sentencesAssociation for Computational Linguisticsfor the Semantic Analysis of Text, Barcelona, Spain, July 2004SENSEVAL-3: Third International Workshop on the Evaluation of Systemsselected randomly from 40 frames (also selectedrandomly) having at least 370 annotations (out of the100 frames having the most annotations).1Participants were provided with a training setthat identified, for each of the 40 frames, the lexicalunit identification number (which equates to a filename) and a sentence identification name.
They werealso provided with the answers, i.e., the frameelement names and their beginning and endingpositions.
Since the training set was much larger thanthe test set, participants were required to use theFrameNet 1.1 dataset to obtain the full sentence, itstarget word, and the tagged frame elements.For the test data, participants were provided, foreach frame, with sentence instances that identified thelexical unit, the lexical unit identification number, thesentence identification number, the full sentence, anda specification of the target alng with its start andend positions.Participants were required to submit theiranswers in a text file, with one answer per line.
Eachline was to identify the frame name and the sentenceidentifier and then all the frame elements with theirstart and end positions that their systems were able toidentify.
For example, for the sentenceHowever, its task is made much moredifficult by the fact that derogationsgranted to the Welsh water authority allow<Agent>it</> to <Target>pump</><Fluid>raw sewage</> <Goal>into boththose rivers</>.the correct answer would appear as follows:Cause_fluidic_motion.256263 Agent (119,120)Fluid (130,139) Goal (141,162)The sentences provided to participants were notpresegmented (as defined in the Gildea and Jurafskystudy); this was left to the participants' systems.
TheFrameNet dataset contains considerable informationthat was tagged by the FrameNet lexicographers.Participants could use (and were strongly encouragedto use) any and all of the FrameNet data indeveloping and training their systems.
In the test,participants could use any of this data, but werestrongly encouraged to use only data available in thesentence itself and in the frame that is identified.
(This corresponds to the ?more difficult task?identified by Gildea and Jurafsky.)
Participants couldsubmit two runs, one with (non-restrictive case) andone without (restrictive case) using the additionaldata; these were scored separately.FrameNet recognizes the permissibility of?conceptually salient?
frame elements that have notbeen instantiated in a sentence; these are called nullinstantiations (see Johnson et al for a fullerdescription).
An example occurs in the followingsentence (sentID="1087911") from the Motionframe: ?I went and stood in the sitting room doorway,but I couldn't get any further -- my legs wouldn'tmove.?
In this case, the FrameNet taggers consideredthe Path frame element to be an indefinite nullinstantiation (INI).
Frame elements that have been sodesignated for a particular sentence appear to beCore frame elements, but not all core frame elementsmissing from a sentence have designated as nullinstantiations.
The correct answer for this case, basedon the tagging, is as follows:Motion.1087911 Theme (82,88) Path (0,0)Participants were instructed to identify nullinstantiations in submissions by giving a (0,0) valuefor the frame element?s position.2 Participants weretold in the task description that null instantiationswould be analyzed separately.3For this Senseval task, participants were allowedto download the training data at any time; the 21-day1The test set was generated with the Windows-basedprogram FrameNet Explorer, available athttp://www.clres.com/SensSemRoles.html.
FrameNetExplorer provides several facilities for examining theFrameNet data: by frame, frame element, and lexicalunits.
For each unit, a user can explore a frame?selements, associated lexical units, frame-to-framerelations, frame and frame element definitions, lexicalunits and their definitions, and all sentences.2This turned out to be an incorrect method, since someframe elements (notably ?I?
at the beginning of asentence) would have a position of (0,0), i.e, thebeginning and ending positions are both 0.
Suchinstances in the test set were identified and handledseparately to distinguish them from null instantiations.3No analysis of null instantiations has yet beenperformed.restriction on submission of results after downloadingthe training data was waived since this is a newSenseval task and the dataset is very complex.Participants could work with the training data as longas they wished.
The 7-day restriction of submittingresults after downloading the test data still applied.In general, FrameNet frames contain many frameelements (perhaps an average of 10), most of whichare not instantiated in a given sentence.
Systems werenot penalized if they returned more frame elementsthan those identified by the FrameNet taggers.
Forthe 8002 sentences in the test set, only 16212 frameelements constituted the answer set.In scoring the runs, each frame element (not anull instantiation) returned by a system was countedas an item attempted.
If the frame element was onethat had been identified by the FrameNet taggers, theanswer was scored as correct.
In addition, however,the scoring program required that the frameboundaries identified by the system?s answer had tooverlap with the boundaries identified by FrameNet.An additional measure of system performance wasthe degree of overlap.
If a system?s answer coincidedexactly to FrameNet?s start and end position, thesystem received an overlap score of 1.0.
If not, theoverlap score was the number of charactersoverlapping divided by the length of the FrameNetstart and end positions (i.e., end-start+1)4The number attempted was the number of non-null frame elements generated by a system.
Precisionwas computed as the number of correct answersdivided by the number attempted.
Recall wascomputed as the number of correct answers dividedby the number of frame elements in the test set.Overlap was the average overlap of all correctanswers.
The percent Attempted was the number offrame elements generated divided by the number offrame elements in the test set, multiplied by 100.
If asystem returned frame elements not identified in thetest set, its precision would be lower.2 ResultsEight teams submitted 20 runs.
Three teamssubmitted runs only for the restricted case (no priorknowledge about frame boundaries).
The other fiveteams submitted at least two runs, with one teamsubmitting 8 runs and another submitting 4 runs.Four of these five teams submitted a restricted runand an unrestricted run (frame boundaries wereidentified, i.e., the task was a classification task ofidentifying the applicable frame element).The results for the classification task are shownin Table 1.
The average precision over all these runsis 0.803 and the average recall is 0.757.
The overlapin each run is almost identical to the precision, anddiffers slightly because there may have been someslight positional errors in either the FrameNet data orthe sentence string provided in the test data.Table 1.
System Performance (Unrestricted)Run Prec Over Rec Att01b (HKPolyU) 0.874 0.873 0.867 99.201c (HKPolyU) 0.905 0.904 0.846 93.501d (HKPolyU) 0.859 0.858 0.852 99.201e (HKPolyU) 0.902 0.901 0.849 94.101f (HKPolyU) 0.908 0.907 0.846 93.201g1 (HKPolyU) 0.819 0.817 0.812 99.201g2 (HKPolyU) 0.819 0.817 0.812 99.201h (HKPolyU) 0.926 0.925 0.705 76.102b (InfoSciInst) 0.867 0.866 0.858 99.004b (UTDMorarescu) 0.946 0.946 0.907 95.807a (UTDMoldovan) 0.898 0.897 0.839 93.408b (UUtah) 0.728 0.725 0.721 99.108c (UUtah) 0.858 0.857 0.849 98.9The results for the restricted case are shown inTable 2.
The average precision over all these runs is0.595 and the average recall is 0.481.
The averageoverlap is noticeably lower than the precision,indicating the additional difficulty for these runs ofidentifying the frame element boundaries.Table 2.
System Performance (Restricted)Run Prec Over Rec Att02a (InfoSciInst) 0.802 0.784 0.654 81.503 (CLResearch) 0.583 0.480 0.111 19.004a (UTDMorarescu) 0.899 0.882 0.772 85.905a (USaarland) 0.654 0.602 0.471 72.005b (USaarland) 0.736 0.675 0.594 80.706 (UAmsterdam) 0.869 0.847 0.752 86.407b (UTDMoldovan) 0.807 0.777 0.780 96.708a (UUtah) 0.355 0.255 0.453 127.908e (UUtah) 0.387 0.295 0.335 86.74Hence the problem with an element having (0,0) asthe start and end positions.In both cases, the percent attempted is quite high,except for one system in the restricted runs.
Thisindicates that systems were able to identify potentialframe elements in quite a large percentage of thecases.
Systems were allowed to return any number offrame elements for a sentence and it is possible for asystem to identify more frame elements than wereidentified by the FrameNet taggers.
For example, run08a asserted many more frame elements than wereidentified in the answer key.
As a result, its percentattempted was much higher than 100 percent.
Thenumber of frame elements in other runs not identifiedin the answer key is unknown.
The effect of a highernumber attempted lowers the precision for a run andincreases the percent attempted.3 DiscussionOverall, the results achieved in this SENSEVAL-3task were quite high.
Several teams achieved resultsmuch better than those obtained by Gildea andJurafsky.
The average precision of 0.80 for all runsin the unrestricted case  is only slightly lower than the82% accuracy achieved in that study when usingpresegmented constituents.
Many teams achievedprecision at or above 0.90, indicating that theirroutines for classifying constituents is quite good.
Inview of the fact that the number of frames and frameelements in FrameNet has expanded considerablysince the Gildea and Jurafsky study, it appears thatthe methods employed have become quite accurate inclassifying constituents.5Results for the restricted were also quite good incomparison with the Gildea and Jurafsky study,which achieved 65% precision and 61% recall at the?more difficult task of simultaneously segmentingconstituents and identifying their semantic role.?
Inthis task, four teams achieved results between 80 and90 percent for precision and between 65 and 78percent for recall.The participants in this task used a wide varietyof methods and data in their systems.
In addition,they used the FrameNet dataset from a wide diversityof perspectives.
In some cases, they developedmechanisms for grouping the FrameNet data by partof speech or making use of the nascent inheritancehierarchy in FrameNet.
In some cases, they used allframes as a basis for training and in others, theyemployed novel grouping methods based on thesimilarities among different frames.The successes of many teams seems to indicatethat the FrameNet dataset is an excellent lexicalresource and that the resources devoted to itsdevelopment have been quite valuable.
The collectiveefforts of the participants have contributed greatly tomaking this complex database more accessible andmore amenable to even further development, not onlyfor research purposes, but also for use in many NLPapplications.ReferencesGildea, Daniel, and Daniel Jurafsky.
Automatic Labelingof Semantic Roles.
Computational Linguistics, 28 (3),245-288.Johnson, Christopher; Miriam Petruck, Collin Baker,Michael Ellsworth, Josef Ruppenhofer, and CharlesFillmore, (2003).
FrameNet: Theory and Practice.Berkeley, California.5The diversity of frame elements in the test data hasnot yet been investigated, so the assertion that this taskis more difficult is based solely on the generalexpansion of FrameNet.
