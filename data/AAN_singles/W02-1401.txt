Disambiguating Noun Compounds with Latent Semantic IndexingAlan M. Buckeridge and Richard F. E. SutcliffeDepartment of Computer Science and Information Systems,University of Limerick, Limerick, Ireland{Alan.Buckeridge, Richard.Sutcliffe}@ul.ieAbstractTechnical terms in text often appear as nouncompounds, a frequently occurring yet highlyambiguous construction whose interpretationrelies on extra-syntactic information.
Severalstatistical methods for disambiguating com-pounds have been reported in the literature, of-ten with quite impressive results.
However, astriking feature of all these approaches is thatthey rely on the existence of previously seen un-ambiguous compounds, meaning they are proneto the problem of sparse data.
This difficultyhas been overcome somewhat through the useof hand-crafted knowledge resources to collectstatistics on ?concepts?
rather than noun to-kens, but domain-independence has been sacri-ficed by doing so.
We report here on work inves-tigating the application of Latent Semantic In-dexing to provide a robust domain-independentsource of the extra-syntactic knowledge neces-sary for noun compound disambiguation.1 IntroductionNoun compounds are a frequently encounteredconstruction in natural language processing(NLP), consisting of a sequence of two or morenouns which together function syntactically as anoun.
In English, compounds consisting of twonouns are predominantly right-headed.
How-ever, compound construction is recursive andboth the modifier and the head can themselvesbe compounds, resulting in structural ambigui-ties.
Consider the following pair of noun com-pounds:1.
(a) (cantilever (swing wing))(b) ((information retrieval) experiment)Both compounds consist of the same parts-of-speech, yet the structures differ: (1a) is right-branching, while (1b) is left-branching.Phrase structure grammar rules for nouncompounds are often similar in form toN ?
N N (Lauer, 1995).
This rule is appliedonce to two-word noun compounds, and recur-sively in the case of longer compounds; thereforethe syntax of compounds longer than two wordsis underconstrained by grammar, resulting in asyntactic ambiguity which grows exponentiallywith the length of the compound.Besides causing problems for syntacticparsers, the ambiguities inherent in these struc-tures pose difficulties for NLP systems whichattempt to analyse the underlying semanticrelationships present in compound technicalterms.
Often, the first step in such analyses isto decompose terms into nested modifier-headpairs (Barker, 1998).
However, such a decom-position is non-trivial for the case of compoundsconsisting of three or more nouns due to thestructural ambiguity of these constructions.The identification of modifier-head pairs incompounds also has applications within the fieldof information retrieval (IR).
Several stud-ies have shown that extracting modifier-headpairs from text and including these as com-pound indexing terms can improve recall andprecision (Evans and Zhai, 1996; Pohlmann andKraaij, 1997; Strzalkowski and Vauthey, 1992).Identification of these noun modifier relation-ships is also important for terminology transla-tion.
However, obtaining correct modifier-headpairs is once again hampered by ?the notori-ous ambiguity of nominal compounds?
(Strza-lkowski and Vauthey, 1992, p.107).To summarise, the syntactic disambigua-tion of noun compounds is important for sev-eral NLP applications; however, disambiguationis difficult because attachments within com-pounds are not syntactically governed.
Clearly,then, this lack of syntactic constraints forces usto consider the use of extra-syntactic factors inthe process of disambiguation.
The work re-ported here describes an approach for automat-ically deriving a syntactical analysis of nouncompounds by adapting Latent Semantic Index-ing, a well-established IR technique, to supplythis extra-syntactic information.2 Previous WorkThe majority of corpus statistical approachesto compound disambiguation use a variation ofwhat Lauer (1995) refers to as the adjacencyalgorithm.
This algorithm was originally pro-posed by Marcus (1980), and essentially op-erates by comparing the acceptability of im-mediately adjacent noun pairs.
Specifically,given a sequence of three nouns n1 n2 n3 , if(n2 n3 ) is a more acceptable constituent than(n1 n2 ), then build (n1 (n2 n3 )); else build((n1 n2 ) n3 ).There remains the question of how ?accept-ability?
is to be determined computationally.Several researchers (e.g., (Barker, 1998; Evansand Zhai, 1996; Pohlmann and Kraaij, 1997;Pustejovsky et al, 1993; Strzalkowski and Vau-they, 1992)) collect statistics on the occur-rence frequency of structurally unambiguoustwo-noun compounds to inform the analysisof the ambiguous compound.
For example,given the compound ?computer data bases?, thestructure (computer (data bases)) would be pre-ferred if (data bases) occurred more frequentlythan (computer data) in the corpus.
However,by assuming that sufficient examples of sub-components exist in the training corpus, all theabove approaches risk falling foul of the sparsedata problem.
Most noun-noun compounds arerare, and statistics based on such infrequentevents may lead to an unreliable estimationof the acceptability of particular modifier-headpairs.The work of Resnik (1993) goes some waytowards alleviating this problem.
Rather thancollecting statistics on individual words, he in-stead counts co-occurrences of concepts (as rep-resented by WordNet synsets).
He uses thesestatistics to derive a measure, motivated byinformation theory, called selectional associa-tion (see Resnik (1993) for full details).
?Ac-ceptability?
in the adjacency algorithm is thenmeasured in terms of the selectional associationbetween a modifier and head.
Selectional as-sociations were calculated by training on ap-proximately 15,000 noun-noun compounds fromthe Wall Street Journal corpus in the PennTreebank.
Of a sample of 156 three-nouncompounds drawn from the corpus, Resnik?smethod achieved 72.6% disambiguation accu-racy.Lauer (1995) similarly generalises from indi-vidual nouns to semantic classes or concepts;however, his classes are derived from semanticcategories in Roget?s Thesaurus.
Similar to theapproaches discussed above, Lauer extracts atraining set of approximately 35,000 unambigu-ous noun-noun modifier-head compounds to es-timate the degree of association between Ro-get categories.
He calls this measure concep-tual association, and uses this to calculate theacceptability of noun pairs for the disambigua-tion of three-noun compounds.
However, his ap-proach differs from most others in that he doesnot use the adjacency algorithm, instead usinga dependency algorithm which operates as fol-lows: Given a three-noun compound n1 n2 n3 ,if (n1 n3 ) is more acceptable than (n1 n2 ), thenbuild (n1 (n2 n3 )); else build ((n1 n2 ) n3 ).Lauer tested both the dependency and ad-jacency algorithms on a set of 244 three-nouncompounds extracted from Grolier?s Encyclope-dia and found that the dependency algorithmconsistently outperformed the adjacency algo-rithm, achieving a maximum of 81% accuracyon the task.
Overall, he found that estimatingthe parameters of his probabilistic model basedon the distribution of concepts rather than thatof individual nouns resulted in superior perfor-mance, thus providing further evidence of theeffectiveness of conceptual association in nouncompound disambiguation.All these approaches rely on a variation offinding subconstituents elsewhere in the corpusand using these to decide how the longer, am-biguous compounds are structured.
However,there is always the possibility that these sys-tems might encounter modifier-head pairs intesting which never occurred in training, forcingthe system to ?back off?
to some default strat-egy.
This problem is alleviated somewhat inthe work of Resnik and Lauer where statisticsare collected on pairs of concepts rather thanpairs of noun tokens.
However, the methods ofResnik and Lauer both depend on hand-craftedknowledge sources; the applicability of their ap-proaches is therefore limited by the coverage ofthese resources.
Thus their methods would al-most certainly perform less well when appliedto more technical domains where much of thevocabulary used would not be available in ei-ther WordNet or Roget?s Thesaurus.
Knowl-edge sources such as these would have to bemanually augmented each time the system wasported to a new domain.
Therefore, it wouldbe preferable to have a method of measuringconceptual associations which is less domain-dependent and which does not rely on the pres-ence of unambiguous subconstituents in train-ing; we investigated whether Latent SemanticIndexing might satisfy these requirements.3 Latent Semantic IndexingLatent Semantic Indexing (LSI) is a variant ofthe vector-space approach to information re-trieval.
It takes as input a collection of docu-ments, from which it constructs an m?n word-document matrix A; cell aij of the matrix de-notes the frequency with which term i occursin document j.
At the core of LSI is singu-lar value decomposition (SVD), a mathematicaltechnique closely related to eigenvector decom-position and factor analysis.
SVD factors thematrix A into the product of three matrices:A = U?V T .
U and V contain the left andright singular vectors of A, respectively, while?
is a diagonal matrix containing the singularvalues of A in descending order.
By retainingonly the k largest singular values1 and settingthe remaining smaller ones to zero, a new diag-onal matrix ?k is obtained; then the product ofU?kV T is the m ?
n matrix Ak which is onlyapproximately equal to A.
This truncated SVDre-represents the word-document relationshipsin A using only the axes of greatest variation,in effect compressing and smoothing the data inA.
It is this compression step which is said tocapture important regularities in the patternsof word co-occurrences while ignoring smallervariations that may be due to idiosyncrasies inthe word usage of individual documents.
Theresult of condensing the matrix in this way isthat words which occur in similar documents1The optimal value of k may only be determined em-pirically, and will depend on the particular application.will be represented by similar vectors, even ifthese words never actually co-occur in the samedocument.
Thus it is claimed that LSI cap-tures deeper associative relationships than mereword-word co-occurrences.
See Berry et al(1995) and Deerwester et al (1990) for morethorough discussions of SVD and its applicationto information retrieval.Because word vectors are originally based ontheir distribution of occurrence across docu-ments, each vector can be interpreted as a sum-mary of a word?s contextual usage; words arethus similar to the extent that they occur insimilar contexts.
Of interest for our purposes isthe fact that a measure of the similarity or as-sociation between pairs of words can be calcu-lated geometrically, typically by computing thecosine of the angle between word vectors.
Anytwo words, which may or may not occur ad-jacently in text, can be compared in this way;this frees us from the restriction of relying onunambiguous subconstituents in training to in-form the analysis of ambiguous compounds intesting.There is a growing body of literature indicat-ing that distributional information of the kindcaptured by LSI plays an important role in var-ious aspects of human cognition.
For the workreported here, the most interesting aspect ofdistributional information is its purported abil-ity to model conceptual categorisation.
Sev-eral studies (Burgess and Lund, 1999; Laham,1997; Landauer et al, 1998; Levy and Bulli-naria, 2001) have shown that similarity betweenconcepts can be measured quite successfully us-ing simple vectors of contextual usage; resultsshow that the performance of such systems cor-relates well with that of humans on the sametasks.
These results are all the more impres-sive when we consider that such systems use nohand-coded semantic knowledge; the conceptualrepresentations are derived automatically fromtraining corpora.Noun compound disambiguation appears tobe an NLP application for which such measuresof conceptual association would be useful.
Boththe adjacency and dependency algorithms de-scribed above in Section 2 rely on some mea-sure of the ?acceptability?
of pairs of nounsto disambiguate noun compounds.
Techniquessuch as LSI offer a simple, robust, and domain-Noun Compound Branching((Ami Pro) document) Left(volunteer (rescue workers)) Right(tourist (exchange rates)) Right((cluster analysis) procedure) Left((data base) subcommittee) Left(Windows (Control Panel)) RightTable 1: Some example noun compounds takenfrom our test set.
Each row shows an exampleof a manually bracketed compound, along withits branching direction.independent way in which concepts and the as-sociations between them can be represented.
Inthe next section, we describe an experiment ex-ploring the efficacy of LSI?s conceptual repre-sentations in disambiguating noun compounds.4 LSI and Noun CompoundDisambiguation4.1 Method4.1.1 MaterialsWe used four corpora in our study: The LotusAmi Pro Word Processor for Windows User?sGuide Release 3, a software manual (AmiPro);document abstracts in library science (CISI);document abstracts on aeronautics (CRAN);and articles from Time magazine (Time).
Wefirst ran the LSI software on the corpora to cre-ate the word-by-document matrices.
The soft-ware also subsequently performed singular valuedecomposition on the resulting matrices.
Stop-words were not excluded, as previous experiencehad shown that doing so degraded performanceslightly.We used Brill?s (1994) tagger to identifythree-noun sequences in each of the corpora.Tagging was imperfect, and sequences whichwere not true three-noun compounds were dis-carded.
The remaining noun compounds werebracketed manually and constituted test sets foreach corpus; some examples are shown in Ta-ble 1.
Table 2 summarises the datasets used inour study.4.1.2 ProcedureBoth the adjacency and dependency modelswere investigated (see Section 2).
Recall thatthe adjacency algorithm operates by comparingthe acceptability of the subcomponents (n1 n2 )and (n2 n3 ), whereas the dependency algo-rithm compares the acceptability of (n1 n2 ) and(n1 n3 ).
?Acceptability?
in our approach wasmeasured by calculating the cosine of the anglebetween each pair of word vectors.
The cosineranges from ?1.0 to 1.0; a higher cosine indi-cated a stronger association between each wordin a pair.
In the case of a tie, a left branchinganalysis was preferred, as the literature suggeststhat this is the more common structure (Lauer,1995; Resnik, 1993).
Thus a default strategy ofalways guessing a left branching analysis servedas the baseline in this study.
Each of the cor-pora contained terms not covered by WordNetor Roget?s; thus it was not possible to use thetechniques of Resnik (1993) and Lauer (1995)as baselines.As we could not tell beforehand what the op-timal value of k would be (see Section 3 above),we used a range of factor values.
The valuesused ranged from 2 to the total number of doc-uments in each collection.
For each factor value,we obtained the percentage accuracy of both theadjacency and dependency models.4.2 Results and DiscussionThe results of the experiment are summarised inTable 3 and Figure 1.
In most cases the perfor-mance rises quickly as the number of SVD fac-tors used increases, and then tends to level off.The best performance was 84% for the AmiProcollection, obtained using the adjacency algo-rithm and 280 SVD factors.
As the task in-volved choosing the best binary bracketing fora noun compound, we would expect an accu-racy of 50% by chance.
These results com-pare favourably with those of Resnik (1993)and Lauer (1995) (73% and 81%, respectively),but as their studies were conducted on differ-ent corpora, it would be imprudent to make di-rect comparisons at this stage.
Results for theother collections were less impressive?however,above-baseline performances were obtained ineach case.Substantial differences in the performances ofthe adjacency and dependency algorithms wereonly observed for the AmiPro collection, sug-gesting that the superior performance of the de-pendency algorithm in Lauer?s (1995) study waslargely corpus-dependent.
This is reinforced bythe considerably superior performance of theCollection Name AmiPro CISI CRAN TimeNumber of Documents 704 1,460 1,400 425Number of Tokens 138,091 187,696 217,035 252,808Mean Tokens per Type 46.3 18.7 26.2 11.5Number of test compounds 307 235 223 214Table 2: Characteristics of the datasets.Name AmiPro CISI CRAN TimeBaseline 58% 63% 74% 48%Adjacency 84% (280) 73% (800) 75% (700) 62% (370)Dependency 70% (200) 70% (1100) 75% (600) 62% (240)Table 3: Percentage disambiguation accuracy on each collection.
The Baseline row shows theaccuracy of always choosing a left-branching analysis.
Highest accuracies for the Adjacency andDependency algorithms are shown, with the corresponding number of SVD factors in parentheses.adjacency algorithm on the AmiPro data set.Another interesting finding was that therewere more right-branching (52%) than left-branching (48%) compounds in the Time collec-tion.
This contrasts with previous studies whichdiscuss the predominance of left-branching com-pounds, and suggests that the choice for the de-fault branching must be corpus-dependent (seeBarker (1998) for similar findings).There also appears to be a positive relation-ship between performance and the token-typeratio.
The number of tokens per type in theAmiPro collection was 46.3; the worst perfor-mance was found for the Time collection, whichhad only 11.5 tokens per type.
There are atleast two possible explanations for this relation-ship between performance and token-type ratio:First, there were more samples of each wordtype in the AmiPro collection?this may havehelped LSI construct vectors which were morerepresentative of each word?s contextual usage,thus leading to the superior performance on theAmiPro compounds.Second, LSI constructs a single vector foreach token?if a particular token is polysemousin text then its vector will be a ?noisy?
amalga-mation of its senses, a factor often contribut-ing to poor performance.
However, due tothe controlled language and vocabulary used inthe software manual domain, few if any of thewords in the AmiPro collection are used to con-vey more than one sense; once again, this mayhave resulted in ?cleaner?, more accurate vec-tors leading to the superior disambiguation per-formance on the AmiPro compounds.These points lead us to the tentative sugges-tion that our approach appears most suitablefor technical writing such as software manuals.As usual, however, this is a matter for futureinvestigation.5 Conclusions and Future ResearchIn this study, we extended LSI beyond its usualremit by adopting it as a measure of conceptualassociation for noun compound disambiguation.The results reported here are encouraging, thehighest accuracy of 84% on the AmiPro collec-tion indicating the potential of our approach.However, poorer performance was obtained forthe other collections indicating that there ismuch room for improvement.
We therefore in-tend to pursue our investigation of the utilityof applying vector-based measures of seman-tic similarity to the problem of syntactic dis-ambiguation.
An attractive feature of this ap-proach for the processing of terminology is thatit requires no manually constructed knowledgesources, meaning that it does not suffer thesame coverage limitations as the methods ofLauer (1995) and Resnik (1993).
In principle,our approach can be applied to any domain.Another attractive feature is that it does notrely on counts of unambiguous subconstituentsin training.
This means that it can be applied tonovel compounds for which no subcompoundsexist in training, something which would not be556065707580850 100 200 300 400 500 600 700 800Accuracy(%)No.
of FactorsAmiProAdjacencyDependencyBaseline(a) AmiPro455055606570750 200 400 600 800 1000 1200 1400 1600Accuracy(%)No.
of FactorsCISIAdjacencyDependencyBaseline(b) CISI6062646668707274760 200 400 600 800 1000 1200 1400Accuracy(%)No.
of FactorsCRANAdjacencyDependencyBaseline(c) CRAN464850525456586062640 50 100 150 200 250 300 350 400 450Accuracy(%)No.
of FactorsTimeAdjacencyDependencyBaseline(d) TimeFigure 1: Results of an experiment investigating noun compound disambiguation using LSI.
Eachfigure shows percentage disambiguation accuracy of the adjacency and dependency models for arange of SVD factors.
The percentage of left-branching compounds in each test set, which servedas the baseline in our study, is also shown for comparison.possible for the statistical techniques outlinedin Section 2.
Our next step will thus be to in-vestigate the efficacy of our approach on novelcompounds.We are currently examining the use of othertechniques for deriving vector-based measuresof conceptual association; preliminary investiga-tions using a ?sliding window?
method (Burgessand Lund, 1999; Levy and Bullinaria, 2001) todisambiguate compounds from the AmiPro cor-pus show results even better than those reportedhere.
Present work involves setting various pa-rameters (e.g., window size, similarity metric,weighting method) to study their effect on per-formance.
We are continuing to test both theadjacency and dependency algorithms on thiscorpus, and have consistently found better per-formance using the former.Future work will involve continuing to testthe technique in other domains; we also intendtraining on larger and more diverse corpora.Furthermore, we plan to investigate other ex-amples of syntactic ambiguity, such as preposi-tional phrase attachment.
Such structures posemany problems for traditional NLP systems,but may prove amenable to the techniques dis-cussed in this paper.6 AcknowledgementsOur thanks go to Pat Hickey of ECE, Universityof Limerick, for technical assistance; and to twoanonymous reviewers for helpful comments.ReferencesK.
Barker.
1998.
A trainable bracketer fornoun modifiers.
In Proceedings of the TwelfthCanadian Conference on Artificial Intelli-gence, pages 196?210, Vancouver.M.
W. Berry, S. T. Dumais, and T. A. Letsche.1995.
Computational methods for intelligentinformation access.
In Proceedings of Super-computing ?95, San Diego, CA.E.
Brill.
1994.
Some advances intransformation-based part of speech tagging.In Proceedings of the 12th National Confer-ence on Artificial Intelligence (AAAI-94).C.
Burgess and K. Lund.
1999.
The dy-namics of meaning in memory.
In E. Di-etrich and A. Markman, editors, CognitiveDynamics: Conceptual and RepresentationalChange in Humans and Machines, pages 17?56.
Lawrence Erlbaum Associates Inc., Hills-dale, NJ.S.
Deerwester, S. T. Dumais, G. W. Furnas,T.
K. Landauer, and R. Harshman.
1990.
In-dexing by latent semantic analysis.
Journalof the American Society for Information Sci-ence, 41:391?407.D.
A. Evans and C. Zhai.
1996.
Noun-phraseanalysis in unrestricted text for informationretrieval.
In Proceedings of the 34th AnnualMeeting of the Association for ComputationalLinguistics, pages 17?24, Santa-Cruz, CA,June.D.
Laham.
1997.
Latent Semantic Analysis ap-proaches to categorization.
In Proceedings ofthe 19th Annual Conference of the CognitiveScience Society, page 979.
Erlbaum.T.
K. Landauer, P. W. Foltz, and D. Laham.1998.
Introduction to Latent Semantic Anal-ysis.
Discourse Processes, 25:259?284.M.
Lauer.
1995.
Designing Statistical LanguageLearners: Experiments on Noun Compounds.Ph.D.
thesis, Macquarie University, Sydney,Australia.J.
P. Levy and J.
A. Bullinaria.
2001.
Learn-ing lexical properties from word usage pat-terns: Which context words should be used?In Proceedings of the Sixth Neural Computa-tion and Psychology Workshop, pages 273?282.
London: Springer.M.
Marcus.
1980.
A Theory of Syntactic Recog-nition for Natural Language.
MIT Press,Cambridge, MA.R.
Pohlmann and W. Kraaij.
1997.
The effectof syntactic phrase indexing on retrieval per-formance for Dutch texts.
In Proceedings ofRIAO ?97, pages 176?187, Montre?al, June.J.
Pustejovsky, S. Bergler, and P. Anick.
1993.Lexical semantic techniques for corpus anal-ysis.
Computational Linguistics, 19(2):331?358.P.
S. Resnik.
1993.
Selection and Information:A Class-Based Approach to Lexical Relation-ships.
Ph.D. thesis, University of Pennsylva-nia.T.
Strzalkowski and B. Vauthey.
1992.
In-formation retrieval using robust natural lan-guage processing.
In Proceedings of the30th Annual Meeting of the Association forComputational Linguistics, pages 104?111,Newark, Delaware, USA.
