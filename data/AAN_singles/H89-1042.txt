SRI's DECIPHER SystemHy Murveit, Michael Cohen, Patti Price, Gay Baldwin,Mitch Weintraub, and Jared BernsteinSpeech Research Program, SRI InternationalMenlo Park, CA 94025AbstractSRI has developed a speaker-independent con-tinuous speech, large vocabulary speech recogni-tion system, DECIPHER, that provides state-of-the-art performance on the DARPA standard speaker-independent resource management training and test-ing materials.
SRI's approach is to integrate speechand linguistic knowledge into the HMM framework.This paper describes performance improvements aris-ing from detailed phonological modeling and from theincorporation of cross-word coarticulatory constraints.1 In t roduct ionThe hidden Markov model (HMM) formulation isa powerful statistical framework that is well-suited tothe speech recognition problem.
Systems based on thisformulation have improved dramatically, however, asdevelopers have learned how to modify them appropri-ately to take into account principles from speech re-search and from linguistics.
Concepts arising from thestudy of the sound system of a language, i.e., phonol-ogy, are language-specific; they are done once for En-glish and little additional labor is required when, forexample, the vocabulary or the domain changes.
Careshould be taken, however, in modeling detailed linguis-tic structure since the practice can lead to models withmany additional parameters to be estimated; unlessthis problem is addressed irectly, performance gainswill be compromised.SRI is not the first group to incorporate speechknowledge and concepts from linguistics in an HMMformulation for speech recognition.
In fact, many im-provements in I-IMM-based systems are implicitly re-lated to principles from speech and linguistics, eventhough this was not their original motivation.
We sur-vey some of these modifications below.Phonet ic  Units.
Not all recognizers are basedon phonetic units.
A number of HMM-based speechrecognition systems have been based on word-levelmodels \[1, 12, 9\].
The use of phonetic units allowsfor larger vocabularies by sharing training across sub-word units that repeat more frequently than words do.Phonetic-based units are now common to many HMM-based recognizers (e.g., \[7, 8, 10\]).Tr iphones.
Triphones are phonetic models condi-tioned on the immediately surrounding phonetic units.BBN \[3\] was able to show significant performancegain (roughly halving the error rate compared to asimilar system without the triphone models), pro-vided the context-dependent models were averaged("smoothed") with the context-independent ones in or-der to deal with the large number of poorly trainedtriphone models.
Triphones are used extensively now(e.g., at BBN, CMU, Lincoln Laboratories, SPd).
Tri-phones can model major coarticulatory effects de-scribed in the speech research literature, as well asphonological variation conditioned on the immediatelysurrounding phones.
In general, more detailed modelswill perform better than less detailed models, providedthere is sufficient data to estimate the parameters.
For60 phones, there are 60 cubed triphones, which repre-sents a significant increase in parameters to estimate.Triphones would not have shown a performance gainhad they been introduced without a mechanism to takethis into account, i.e., in this case, smoothing withmore general models.Difference Parameters .
The use of additional,independently trainable parameters means that moredetails can be included in the model without a dra-matic increase in the amount of training material.
Inparticular, recognition performance has been signifi-cantly increased \[8, 10\] through the use of codebooksthat represent the difference between the current valueof a parameter and its value several frames previously.Spectral and energy difference parameters are used inaddition to their current values.
This captures impor-tant dynamic patterns exhibited in speech as well asthe standard static information.238In this paper we describe SRI's recent work in incor-porating linguistic oncepts in the DECIPHER system:improved phonological modeling and modeling cross-word coarticulation.2 DECIPHEI:Us Basic DesignSRI's DECIPHER speech recognition system usesdiscrete density 3-state hidden-Markov models to rep-resent phones.. Four discrete densities per state modelthe variation of vector quantized Mel-cepstra, Vectorquantized Mel-cepstral time-derivatives, and quantizedenergy and energy time-derivatives.
Word models areconstructed from network representations of word pro-nunciations and from a set of phone models (context-independent, left biphones, right biphones, triphones,and phone-in-word models).
The more samples of aword available in the system's training set, the morespecific the contexts used for the phone models inthe word.
The most detailed, primary, models aresmoothed by averaging in other models of less specificcontext with weights estimated automatically using anSRI version of IBM's deleted-interpolation algorithm\[6\].3 DatabaseThe speech database used for training and testingSRI's DECIPHER system is described in \[11\].
Thisdatabase, intended for the design and evaluation ofalgorithms for continuous peech recognition, consistsof sentences read in a sound-isolated room.
The sen-tences are appropriate to a naval resource managementtask based on existing interactive database and graph-ics programs.
The database includes 160 male andfemale talkers with a variety of dialects.
The designincludes a partition of the database into independenttraining and testing portions.The training materials used for the results reportedhere are the 3950 sentences from 97 training and devel-opment alkers that do not overlap the test set reportedon.
The testing materials used for most of the resultsreported here are the 150 sentences (1287 words) fromthe 1987 designated test sets designated by the Na-tional Institute of Standards and Technology (NIST,formerly NBS).The results reported here were obtained with andwithout he use of a grammar to constrain the recogni-tion search.
These conditions are not those that wouldbe used in a real application, but they are simply de-fined, allow recognition systems to be evaluated overmore than one condition of grammatical constraint,and they have been accepted as standards of com-parisons.
The degree of constraint provided by thegrammar is measured by test set perplexity \[7\], or, thegeometric mean of the number of words allowed bythe grammar at each point in the test set, given theprevious words.
In the case of no grammatical con-straint, any word can follow any other word and theperplexity is equal to the vocabulary size, in this case1000.
The DARPA standard word-pair grammar wascreated by collecting all two-word sequences allowedin the sentence-patterns used to generate the task sen-tences (as described in \[11\]).
The perplexity of thisgrammar as measured on several different 25-sentencetest sets from the database is about 60.4 Phonological ModelingPronunciation varies significantly across speakers,as well as in the speech of individuals \[5\].
However,most current speech recognition systems model wordswith a single pronunciation or a small number of alter-nate pronunciations.
For systems which use statisticaltraining of models of speech segments, this lack of ex-plicit representation f the range of variation of pro-nunciation causes different phenomena to be averagedtogether into the same model, resulting in a less pre-cise model.
These less precise models are likely to be-come more problematic as speech recognition systemsmove from eorpera of read speech to the spontaneousspeech that can be expected in real applications, incesignificantly more pronunciation variability occurs inspontaneous than in read speech (c.f.
\[2\]).Some previous attempts to explicitly model manypronunciations for each word have led to performancedegradation possibly resulting from (1) many addi-tional parameters to be estimated with the sameamount of training data, and (2) unlikely pronuncia-tions not previously modeled causing new false alarms.To deal with the first condition, we have designed amethod for developing phonological rule sets based onmeasures of coverage and overcoverage of a database ofpronunciations \[4\] in order to maximize the coverage ofpronunciations observed in a corpus, while minimizingthe size of the pronunciation networks.To address the problem of hypothesizing unlikelypronunciations in inappropriate places, the DECI-PHER system incorporates probabilities into our net-work representation f word pronunciations.
The in-corporation of pronunciation probabilities has beenshown to significantly increase the predictive power ofour representation \[4\].239Current databases for training speech recognitionsystems have too few occurrences of all but the mostfrequent words to make accurate stimates of pronun-ciation probabilities.
Therefore, we have developedand implemented an automatic method for tying to-gether frequently occurring sub-word units for train-ing.
Knowledge mbedded in the rule set can be usedto determine quivalence classes of nodes that sharesimilar contextual constraints \[4\].
Nodes in the sameequivalence class share training samples.
The proba-bilities in the pronunciation etworks combine word-trained probabilities for frequently occurring wordswith these node equivalence class trained probabilities.LEXICONBBN-lexiconCMU-lexiconHand-SingleRule-SingleRule:SparseRule-FullMean Numberof Pronunciationsper Word1.11.01.01.01.34.2Px:lO00wordcorrect67.067.569.372.874.172.9Px:60wordcorrect91.692.490.692.693.792.6Table 1: Phonological Effects in DECIPHER: Wordaccuracy on the 1987 test set with various lexicons andexpansions.5 Lexicon PerformanceWe compared performance of the DECIPHER systemfor a number of different lexicons, based on the ruleset development method described above.
A rule setwith high coverage of a corpus of pronunciations wasdeveloped, and pronunciation probabilities were com-puted for the resulting pronunciation etworks usingthe node equivalence classes described above.
Thedata used to estimate the pronunciation probabilitieswas the same data used to train the phonetic models.A series of less bushy networks was derived by elimi-nating the least probable pronunciations from the net-works.
We refer to this series as Rule-Single ( ach wordhas only one pronunciation), Rule-Sparse (the meannumber of pronunciations per word is 1.3), and Rule-Full (the mean number of pronunciations per word is4.2).
Performance was also measured using the lexi-con from the BBN BYBLOS system (BBN), from theCMU SPHINX system (CMU), and the lexicon devel-oped for an early version of the DECIPHER system,prior to the incorporation of multiple pronunciations.This latter lexicon is referred to as Hand-Single sinceit consists of a single pronunciation per word and wasspecified by hand by an expert linguist.Table 1 shows the results we have obtained withthe SRI DECIPHER system using the various lexi-cons described above.
The recognized word stringswere aligned against the correct reference wordstring and differences tallied using the DARPA-NIST software package.
The word correct is 1 -100 substituti?ns+deleti?ns+inserti?ns where ref is ther~fnumber of words in the set of reference words.
TheDARPA-NIST homophone-equivalency table is usedfor the no-grammar condition (Perplexity P=1000)and not for the grammar condition (Perplexity P=60).The lexicons labeled BBN and CMU do not comparethe DECIPHER system to the BYBLOS system orto the SPHINX system, rather it compares the CMU,BBN and SRI lexicons all as used in the DECIPHERsystem without cross-word coarticulatory modeling.The results of Table 1 show that the lexicon has asignificant effect on performance: for perplexity 1000,percent word correct ranges from 67.0% (for the BBNlexicon) to 74.1% (for SRI's Rule-Sparse lexicon); forperplexity 60, the range is from 90.6% (Hand-Single)to 93.7% (Rule-Sparse).
Within the set of single (ornear single) pronunciation lexicons, the range is nearlyas large.
Thus, a system that explicitly models onlya single pronunciation per word can be improved withcareful design of the dictionary of pronunciations.
Au-tomatically deriving a dictionary of most common pro-nunciations (as in the Rule-Single lexicon) was shownto improve performance over a dictionary of pronunci-ations carefully designed by hand by an expert linguist(Hand-Single).The improvement from the rule-single to the rule-sparse lexicon suggests that modeling multiple proba-bilistic pronunciations can improve recognition perfor-mance.
The degradation in performance from Rule-Sparse to Rule-Full illustrates the importance of keep-ing pronunciation etworks from getting too bushy,while mMntaining coverage of likely pronunciations.6 Cross-word Model ingThe use of triphone modeling and models of wholewords has been used extensively (e.g., \[3\]) to takeinto account coarticulatory effects.
However, extend-ing this notion to operate across word boundaries hadnot been been done before 1989.
~Vord-boundary con-texts have not typically been used because the sizesof the resulting word networks can get very large, andbecause it requires keeping track of which ending arcscan map to which starting arcs.
Since we have alreadydealt with these issues in our large pronunciation net-240Px:1000 Px:60SYSTEM Context word wordModel?
correct correctDECIPHER No 74.1 93.7Yes 78.3 95.0Table 2: Word Accuracy Results With and WithoutCross-Word Coarticulatory Modeling for the 1987 testset).Speakersin Training1097210972Px %sub %del %ins %error60 5.9 2.5 0.4 8.860 6.4 2.5 0.3 9.21000 I 21.0 5.2 1.5 27.710001 23.4 6.1 1.8 31.3Table 3: DECIPHER'S Performance using DARPA's1989 Speaker-Independent Test Setworks, cross-word boundary contexts were a naturalextension to the SRI DECIPHER system.Modeling acoustic variations across words was lim-ited to initial and final phones in words with sufficienttraining data.
To illustrate how the algorithm works,let us consider the initial phone "dh" in the word lhe.In the training database, there are many instances ofwords ending in "n" before the.
An additional "dh"arc is added to the pronunciation graph of lhe, thoughthis arc is only allowed to connect o arcs with the "n"phonetic label.
The original "dh" arc is prevented fromconnecting with arcs with the "n" phonetic label.
Theabove algorithm is applied to all words in the vocabu-lary, provided that 15 occurrences of a (previous/next)phone occurred in the training database.Table 2 shows that the addition of the cross-wordcontext models improves performance of the DECI-PHER system in both the perplexity 60 condition andthe perplexity 1000 condition.
Also shown in the table,labeled SPHINX, are the best previous results reportedon for this database (actually using a little more train-ing data) \[8\].7 1989 Test  Resu l tsTable 3 shows SRI's official results reported at the1989 DARPA speech and natural anguage workshop.These results use the Rule-Sparse pronunciation et-works and the across-word-boundary pronunciationconstraints.
Px stands for perplexity.If the tradeoff or insertions and deletions is appro-priately changed, which was not done for the results inTable 3, performance call be improved slightly: 5.7%substitutions, 1.3% deletions, and 0.8% insertions, foran overall error rate of 7.9%.Speaker-by-speaker performance varies greatly inthe DECIPHER system, and in other systems thatwere reported in this workshop.
For the official DE-CIPHER performance results for the 1989 workshopfor the 109 speaker training set, speaker performanceranged from 17.8% to 37.1% (perplexity=1000), and3.7% error to 14.3% error (perplexity=60).
This vari-ability causes difficulties when trying to compare sys-tems from different sites.
For instance, when compar-ing DECIPHER's results to Carnegie-Mellon's Sphinxsystem, we find that with perplexity=1000, Sphinxoutperformed DECIPHER on six speakers, and DE-CIPHER outperformed Sphinx on four speakers.
Withperplexity=1000, Sphinx had better performance on 7speakers.
When comparing DECIPHER to the LincolnLaboratories system, DECIPHER had fewer errors on6 of 10 with perplexity=1000, and 7 of 10 with perplex-ity=60.
This analysis hows that the variation acrossspeakers wamps the variation among these three sys-tems, and that the apparent system differences may bedue to sampling error.
To properly differentiate thesesystems at a high confidence l vel would require a testwith many more speakers.8 D iscuss ionIn this section we discuss first the results relatingto choice of lexicon and then the results relating tocross-word models.Though we have shown an important performancegain through improved phonological modeling, we be-lieve that more substantial gains will be shown in thefuture for the following reasons:1.
The system tests were based on read speech ratherthan spontaneous speech.
The significant increasein phonological reduction and deletion in sponta-neous compared to read speech \[2\] should resultin a bigger difference between systems that in-clude techniques for modeling multiple probabilis-tic pronunciations and those that do not.2.
The rule sets used in the studies described herewere developed using a corpus of hand phonetictranscriptions, rather than some form of systemoutput.
Different ypes of variation may be moreimportant to model explicitly in different systems,241and are likely to be different from those capturedby hand transcriptions.3.
Larger amounts of training data will allow the de-sign of more detailed models of phonological vari-ation.Lee has suggested \[8\] that modeling multiple pro-nunciations is not worth-while because (1) it makessystems run too slowly, (2) it is impossible to estimatepronunciation probabilities, and (3) it unfairly penal-izes words with too many pronunciations.
Although,we believe that improvements can certainly still bemade in the way we estimate and use our pronunci-ation probabilities, it is clear from our studies thatmodelingpronunciation has significant positive impacton recognition performance without an excessive costin speed.
We suggest that the reason for our oppositeconclusions lies in the difference between the multiple-pronunciation word-networks that CMU and SRI havetried.
As shown in Table 1, SRI's best network modelson average about 1.3 pronunciations per word.
Thenetwork shown as an example in \[8\] allows thousandsof pronunciations, partly because of the excessive de-tail in the example and partly because of the lack ofconstraint to correlate the many possibilities.As for the cross-word coarticulatory modeling we re-port on here, we believe that the performance improve-ment can be attributed to the following: (1) for shortwords (and the most frequent words are short, i.e.,one to three phones long), the word boundaries forma significant portion of the context hat should not beignored, and (2) many triphones that otherwise wouldnot be observed can be found across word boundaries,and the additional triphone training can help modelthe less frequent words.In sum, the use of speech and linguistic knowledgesources can be used to improve the performance ofHMM-based speech recognition systems, provided thatcare is taken to incorporate these knowledge sourcesappropriately.Acknowledgement.
This work was principallysupported by SRI IR&D and investment funding.
Wealso gratefully acknowledge the National Science Foun-dation and the Defense Advanced Research ProjectsAgency for partial support.References\[1\] Bakis, R. (1976) "Continuous Speech Recognitionvia Centisecond Acoustic States," J. Acoust Soc.Am., Suppl.
1, Vol.
59, $97.\[2\] Bernstein, J., G. Baldwin, M. Cohen, H. Murveit,and M. Weintraub (1986) "Phonological Studiesfor Speech Recognition", Proc.
DARPA SpeechRecognition Workshop, pp.
41-48.\[3\] Chow, Y., R. Schwartz, S. Roucos, O. Kim-ball, P. Price, F. Kubala, M. Dunham, M.Krasner, and J. Makhoul (1986) "The Roleof Word-Dependent Coarticulatory Effects ina Phoneme-Based Speech Recognition System,"Proc.
ICASSP, pp.
1593-1596.\[4\] Cohen, M. (1989) Phonological Structures forSpeech Recognition, U. C. Berkeley Ph.D. thesis.\[5\] Cohen, M., J. Sernstein, and H. Murveit (1987)"Pronunciation Variation Within and AcrossSpeakers," l l3th Meeting Acoust.
Soc.
Am., Pg.\[6\] Jelinek, F. and R. Mercer (1980) "InterpolatedEstimation of Markov Source Parameters fromSparse Data," pp.
381-397 in E. S. Gelsema nd L.N.
Kanal (editors), Pattern Recognition in Prac-tice, North-Holland Publishing Company, Ams-terdam, the Netherlands.\[7\] Kubala, F., Y. Chow, A. Derr, M. Feng, O. Kim-ball, J. Makhoul, P. Price, J. Rohlicek, S. Roucos,R.
Schwartz, and J. Vandegrift (1988) "Continu-ous Speech Recognition Results of the BYBLOSSystem on the DARPA 1000-Word Resource Man-agement Database," Proc.
ICASSP, pp.
291-294.\[8\] K. F. Lee  (1988) Large-Vocabulary Speaker-Independent Continuous Speech Recognition: theSPHINX System, CMU Ph.D. Thesis.\[9\] Lippmann, R., E. Martin, and D. Paul (1987)"Multi-Style Training for Robust Isolated-WordSpeech Recognition," Proc.
ICASSP, pp.
705-708.\[10\] Murveit, H. and M. Weintraub (1988) "1000-Word Speaker-Independent Continuous-SpeechRecognition Using Hidden Markov Models,"Proc.
ICASSP, pp.
115-118.\[11\] Price, P., W. Fisher, J. Bernstein, and D. Pallett(1988) "The DARPA 1000-Word Resource Man-agement Database for Continuous Speech Recog-nition," Proc.
ICASSP, pp.
651-654.\[12\] Rabiner, L. R., B. H. Juang, S. E. Levinson, andM.
M. Sondhi (1985) "Recognition of Isolated Dig-its using Hidden Markov with Continuous MixtureDensities," ATT Technical Journal, 64(6):1211-1233.242
