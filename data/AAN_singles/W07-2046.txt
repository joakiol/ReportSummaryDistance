Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 219?222,Prague, June 2007. c?2007 Association for Computational LinguisticsLCC-TE: A Hybrid Approach toTemporal Relation Identification in News TextCongmin MinLanguage Computer Corporation1701 N. Collins Blvd.
Suite 2000Richardson, TX 75080cmin@languagecomputer.comMunirathnam SrikanthLanguage Computer Corporation1701 N. Collins Blvd, Suite 2000Richardson, TX 75080Srikanth.munirathnam@languagecomputer.comAbraham FowlerLanguage Computer Corporation1701 N. Collins Blvd, Suite 2000Richardson, TX 75080abraham@languagecomputer.comAbstractThis paper explores a hybrid approach totemporal information extraction within theTimeML framework.
Particularly, we focus onour initial efforts to apply machine learningtechniques to identify temporal relations asdefined in a constrained manner by theTempEval-2007 task.
We explored severalmachine learning models and human rules toinfer temporal relations based on the featuresavailable in TimeBank, as well as a number ofother features extracted by our in-house tools.We participated in all three sub-tasks of theTempEval task in SemEval-2007 workshopand the evaluation shows that we achievedcomparable results in Task A & B andcompetitive results in Task C.1 IntroductionThere has been a growing interest in temporalinformation extraction in recent years, as more andmore operational NLP systems demands dealingwith time-related issues in natural languages.
Inthis paper, we report on an end-to-end system thatis capable of automating identification of temporalreferring expressions, events and temporalrelations in text by leveraging various NLP toolsand linguistic resources at LCC.It has to be noted that the system we report hereis not only intended for TempEval 2007evaluation, but will also be used as a NLP tool forour other applications (e.g.
temporal QuestionAnswering).
That is why we experimented to useour own temporal and event extraction capabilitiesin this work, although time and event tags havealready been provided in the testing/training data.Another reason we use our own temporal taggingis that our temporal tagger extracts moreinformation than that available in thetraining/testing data.
For instance, temporal signalsare removed from the data that the task organizersprovide, but our temporal tagger detects that, aspart of the tagging procedure.
The following is anexample for the tagged expression ?on this comingSunday?.<ArgStructure id="65" type="timex"><argRef type="determiner" tokStr="this"/><argRef type="directionIndicator?
tokStr="coming"/><argRef type="focus"  tokStr="Sunday"/><argRef type="prepSignal?
tokStr="on"/><argRef type="head"  tokStr="this coming Sunday"/><argRef type="root"  tokStr="on this coming Sunday"/><argValue type="focusType" value="weekOfDay"/><argValue type="subType" value="Fuzzy"/><argValue type="type" value="Date"/></ArgStructure>Our data structure allows us to easily access andmanipulate any part of the tagged chunk of text,which leaves the interpretation of whether thetemporal signal on in the example is part of thetemporal expression to users of temporal tagger.Taking as input this data structure, thenormalization, including relative date resolution, isa straightforward process, provided that thereference time can be computed from the context.For temporal relation identification, byleveraging the capabilities of our temporal tagger,event tagger and several other in-house NLP tools,we derive a rich set of syntactic and semanticfeatures for use by machine learning.
We alsoexplored the possibility of combining the rule-based approach with machine learning in anintegrated manner so that our system can takeadvantage of these two approaches for temporalrelation identification.2 System ArchitectureThe overall architecture of our end-to-end systemis illustrated in Figure 1 (Page 2).219In addition to several common NLP tools, e.g.Named Entity Recognizer, we use syntactic andsemantic parsers to identify syntactic and semanticroles (e.g.
AGENT or SUBJECT) of event termsand a context detector to detect linguistic contextsin a discourse.
We use such information asextended features for machine learning.
TheTemporal Tagger tags and normalizes temporalexpressions conforming to the TimeML guideline.The Temporal Merger compares our own temporaland event tagging with those supplied intraining/testing data.
If there is any inconsistency,it will replace the former with the latter, whichguarantees that our temporal and event tagging arethe same as those in training/testing data.
FeatureExtractor extracts and composes features fromdocuments processed by the NLP tools.
MachineLearner and Human Rule Predictor take as inputthe feature vector for each instance to predicttemporal relation.
The Human Rule Predictor is arule interpreter that read hand-crafted rules fromplain text file to match each event instancerepresented by a feature vector.Note that in Figure 1, Syntactic Parsing is doneby a probabilistic chart parser, which generates fullparse tree for each sentence.
Syntactic PatternMatching is performed by a syntactic patternmatcher, which operates on parse trees producedby chart parser and used by Temporal Tagger totag and normalize temporal expressions.Figure 1.
Overall System Architecture3 Feature EngineeringWhile temporal tagging and normalization is rule-based in our system, temporal relationidentification is a combination of machine learningand rule-based approaches.
For machine learning,the feature set for the three tasks A, B and C weengineered consist of what we call 1) first-classfeatures; 2) derived features; 3) extended features,and 4) merged features.
The way we name the typeof features is primarily for illustrating purpose.3.1 First-class FeaturesThe first-class features consist of:?
Event Class?
Event Stem?
Event and time strings?
Part of Speech of event terms?
Event Polarity?
Event Tense?
Event Aspect?
Type of temporal expression?
Value of temporal expressionThe set of first-class features, which are directlyobtained from the markups of training/testing data,are important, because most of them, includingEvent Class, Event Stem, POS, Tense and Type ofTemporal Expression, have a great impact onperformance of machine learning classifiers,compared with effects of other features.3.2.2 Derived FeaturesFrom the first-class features, we derive andcompute a number of other features:?
Tense and aspect shifts1?
Temporal Signal?
Whether an event is enclosed in quotes?
Whether an event has modals prior to it?
Temporal relation between the DocumentCreation Time and temporal expression in thetarget sentence.The way we compute tense and aspect shifts istaking pair of contiguous events and assign atrue/false value to each relation instance based onwhether tense or shift change in this pair.
Ourexperiments show that these two features didn'tcontribute to the overall score, probably becausethey are redundant with the Tense and Aspectfeatures of each event term.
Temporal Signal1 Initially used in (Mani, et.
al.
2003)Human Rule Predictor Machine LearningML Testing Documents with NewTLINKsWord Sense DisambiguationNE & POS TaggingSyntactic ParsingSyntactic Pattern MatchingSemantic ParsingContext DetectionTemporal Tagging & NormalizingTemporal MergingFeature Extraction & CompositionDocuments withTempEval Markups220represents temporal prepositions and they slightlycontribute to the overall score of classifiers.The last feature in this category is the TemporalRelation between the Document Creation Time andthe Temporal Expression in the target sentence.The value of this feature could be ?greater than?,?less than?, ?equal?, or ?none?.
Experiments showthat this is an important feature for Task A and B,because it contributes several points to the overallscore.
This value may be approximate for anumber of reasons.
For example, we can?t directlycompare a temporal expression of type Date withanother expression of type Duration.
However,even if we apply a simple algorithm to computethis relationship, it results in a noticeably positiveeffect on the performance of the classifier.3.2.3 Extended FeaturesFeatures in the third category are extracted by ourin-house tools, including:?
Whether an event term plays primary semanticor syntactic roles in a sentence?
Whether an event and a temporal expressionare situated within the same linguistic context?
Whether two event terms co-refer in adiscourse (This feature is only used for Task C)Investigation reveals that different types ofevents defined in TimeML may or may not havespecific semantic or syntactic roles (e.g.
THM orOBJECT) in a particular context, therefore havingan impact on their ways to convey temporalmeanings.
Experiments show that use of semanticand syntactic roles as binary features slightlyincreases performance.The second feature in this category is Contextfeature.
We use a context detection tool, whichdetects typical linguistic contexts, such asReporting, Belief, Modal, etc.
to decide whether anevent and a temporal expression are within onecontext.
For example2,?
The company has reported declines inoperating profit in each of the past threeyears, despite steady sales growth.In this example, we identify a Reporting contextwith its signal reported.
The temporal expressioneach of the past three years and the event declinesare within the same context (the feature valuewould be TRUE).
We intend this feature can help2 This sentence is taken from the file wsj_0027.tml inTempEval 2007?s training data.solve the problem of anchoring an event to itsactual temporal expressions.
In fact, we don'tbenefit from the use of this feature, probablybecause detecting those linguistic contexts is aproblem in itself.The third feature in this category is co-referential feature, which is only used for Task C.This feature indicates if two event terms within oroutside one sentence are referring to the sameevent.
Experiments show that this global featureproduces a positive effect on the overallperformance of the classifier.3.2.4 Merged FeaturesThe last type of feature we engineered is themerged feature.
Due to time constraint, as well asthe fact that the system for Task B produces betterresults than Task A and C, we only experimentedmerging the output of the system for Task B intothe feature set of Task C and we achievednoticeable improvements because of adding thisfeature.Most of the features introduced above areexperimented in all three tasks A, B and C, exceptthat the co-referential feature and the mergedfeature are only used in Task C. Also, in Task Csince for each relation there are two events andpossibly two temporal expressions, the number offeatures used is much more than that in Task A andB.
The total number of features for Task C'straining is 35 and 33 for testing.3.1 Combination of Machine Learning andHuman RuleThe design of our system allows both human rule-based and machine learning-based decisionmaking.
However, we have not decided exactly inwhat situations machine learning and human ruleprediction should be used given a particularinstance.
The basic idea here is that we want tohave the option to call either component on the flyin different situations so that we can takeadvantage of the two empirical approaches in anintegrated way.
We did some initial experimentson dynamically applying Human Rule Predictorand Machine Learner on Task B and we were ableto obtain comparable results with or without usinghand-crafted rules.
As pointed out in (Li, et, al.2006), Support Vector Machine, as well as otherclassifiers, makes most mistakes near the decisionplane in feature space.
We will investigate the221possibility of applying human rule prediction tothose relation instances where Machine Learningmakes most mistakes.3.2 Experiments and ResultsBased on the features discussed in Section 3.3, wedid a series of experiments for each task on fourmodels: Naive-Bayes, Decision Tree (C5.0),Maximum Entropy and Support Vector Machine.Due to space constraint, we only report resultsfrom SVM model 3 , which produces bestperformance in our case.We here report two sets of performance numbers.The first set is based on our evaluation against aset of held-out data, 20 documents for each task,which were taken from the training data.
Thesecond set of performance numbers is based onevaluation against the final testing data providedby task organizers.strict relaxedP R F P R FTask A 0.68 0.68 0.68 0.69 0.69 0.69Task B 0.80 0.80 0.80 0.82 0.82 0.82Task C 0.63 0.63 0.63 0.67 0.67 0.67Table 1.
Performance figures evaluated against held-out datastrict relaxedP R F P R FTask A 0.59 0.57 0.58 0.61 0.60 0.60Task B 0.75 0.71 0.73 0.75 0.72 0.74Task C 0.55 0.55 0.55 0.60 0.60 0.60Table 2.
Performance figures evaluated against testing datastrict relax TeamP R F P R FOurs 0.59 0.57 0.58 0.61 0.60 0.60Average 0.59 0.54 0.56 0.62 0.57 0.59Best 0.62 0.62 0.62 0.64 0.64 0.64Table 3.
Performance figures in Comparison for Task Astrict relax TeamP R F P R FOurs 0.75 0.71 0.73 0.76 0.72 0.74Average 0.76 0.72 0.74 0.78 0.74 0.75Best 0.80 0.80 0.80 0.84 0.81 0.81Table 4.
Performance figures in comparison for Task Bstrict relax TeamP R F P R FOurs 0.55 0.55 0.55 0.60 0.60 0.60Average 0.51 0.51 0.51 0.60 0.60 0.60Best 0.55 0.55 0.55 0.66 0.66 0.66Table 5.
Performance figures in comparison for Task C3 We use the LIBSVM implementation of SVM,available at http://www.csie.ntu.edu.tw/cjlin/libsvmAccording to Table 1 and 2, it appears that thereare significant differences between the TLINKpatterns in the held-out data and the final testingdata, since the performance of the classifier showsan apparent discrepancy in two cases.Table 3, 4 and 5 show performance numbers ofour system, the average and the best system incomparison.
There are six teams in totalparticipating in the TempEval 2007 evaluation thisyear.4 ConclusionWe participated in the SemEval2007 workshop andachieved encouraging results by devoting ourinitial efforts in this area.
In next step, we plan toseek ways to expand the training data, implementquality human rules by performing rigorous dataanalysis, and explore use of more features formachine learning through feature engineering.ReferencesB.
Boguraev and R.K. Ando.
2005.
TimeML-compliantText Analysis for Temporal Reasoning.
Proceedingsof IJCAI, UK.D.
Ahn, S.F.
Adafre and M.D.
Rijke.
2005.
TowardsTask-based Temporal Extraction and Recognition.Dagstuhl Seminar Proceedings 05151.Inderjeet Mani and George Wilson.
2000.
RobustTemporal Processing of News.
Proceedings ofACL?2000.Inderjeet Mani, Barry Schiffman, and Jianping Zhang.2003.
Inferring Temporal Ordering of Events inNews.
Proceedings of HLT-NAACL?03, 55-57.K.
Hacioglu, Y. Chen and B. Douglas.
2005.
AutomaticTime Expression Labeling for English and ChineseText, Proceedings of CICLing-2005.L.
Li, T. Mao, D. Huang and Y. Yang.
2006.
HybridModels for Chinese Named Entity Recognition.Proceedings of the Fifth SIGHAN Workshop onChinese Language Processing.The TimeML Working Group.
2005.
The TimeML 1.2Specification.http://www.timeml.org/site/publications/specs.html222
