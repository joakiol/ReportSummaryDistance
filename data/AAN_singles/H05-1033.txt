Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 257?264, Vancouver, October 2005. c?2005 Association for Computational LinguisticsDiscourse Chunking and its Application to Sentence CompressionCaroline Sporleder and Mirella LapataSchool of InformaticsUniversity of Edinburgh2 Buccleuch PlaceEdinburgh EH8 9LW, UK{csporled,mlap}@inf.ed.ac.ukAbstractIn this paper we consider the problem ofanalysing sentence-level discourse struc-ture.
We introduce discourse chunking(i.e., the identification of intra-sententialnucleus and satellite spans) as an al-ternative to full-scale discourse parsing.Our experiments show that the proposedmodelling approach yields results com-parable to state-of-the-art while exploit-ing knowledge-lean features and smallamounts of discourse annotations.
We alsodemonstrate how discourse chunking canbe successfully applied to a sentence com-pression task.1 IntroductionThe computational treatment of discourse phenom-ena has recently attracted much attention, partly dueto their increasing importance for potential appli-cations.
In summarisation, for example, the extrac-tion of sentences to include in a summary cruciallydepends on their rhetorical status (Marcu, 2000;Teufel and Moens, 2002); one might want to extractcontrastive or explanatory statements while omit-ting sentences that contain background information.In information extraction, discourse-level knowl-edge can be used to identify co-referring events(Humphreys et al, 1997) and to determine their tem-poral order.
Discourse processing could further en-hance question answering systems by interpretingthe user?s question either in isolation or in the con-text of preceding questions (Chai and Jing, 2004).Discourse analysis is often viewed as a parsingtask.
Rhetorical Structure Theory (RST, Mann andThomson, 1988), one of the most influential frame-works in discourse processing, represents texts bytrees whose leaves correspond to elementary dis-course units (edus) and whose nodes specify howthese and larger units (e.g., multi-sentence seg-ments) are linked to each other by rhetorical rela-tions (e.g., Contrast, Elaboration).
Discourse unitsare further characterised in terms of their text im-portance: nuclei denote central segments, whereassatellites denote peripheral ones.Recent advances in discourse modelling havegreatly benefited from the availability of resourcesannotated with discourse-level information such asthe RST Discourse Treebank (RST-DT, Carlson etal., 2002).
Even though discourse parsing at thedocument-level still poses a significant challenge todata-driven methods, sentence-level discourse mod-els (e.g., Soricut and Marcu, 2003) trained on theRST-DT have attained accuracies comparable to hu-man performance.
The availability of discourse an-notations is partly responsible for the success ofthese models.
Another important reason is the devel-opment of robust syntactic parsers (e.g., Charniak,2000) that can be used to provide critical structuraland lexical information to the discourse parser.
Un-fortunately, discourse annotated corpora are largelyabsent for languages other than English.
Further-more, reliance on syntactic parsing renders dis-course parsing practically impossible for languagesfor which state-of-the-art parsers are unavailable.In this paper we propose discourse chunking as analternative to discourse parsing.
Analogous to sen-tence chunking, discourse chunking is an interme-diate step towards full parsing.
Following an RST-style analysis, we focus solely on two subtasks:(a) discourse segmentation, i.e., determining whichword sequences form edus and (b) inferring whetherthese edus function as nuclei or satellites.
The moti-vation for tackling these subtasks is two-fold.
First,they are of crucial importance for full-scale dis-course parsing.
For example, Soricut and Marcu(2003) show that perfect discourse segmentation de-livers an error reduction of 29% in the performanceof their discourse parser.
Second, some applicationsmay not require full-scale discourse parsing.
For ex-ample, it has been shown that nuclearity is important257for summarisation, i.e., nuclei are more likely to beretained when summarising than satellites (Marcu,2000).
While nuclearity alone may not be sufficientfor document summarisation (Marcu, 1998), suchknowledge could prove useful at the sentence level,for example for producing sentence compressions.The algorithms introduced in this paper are pur-posely knowledge-lean.
We abstain from using syn-tactic parsers or semantic databases such as Word-Net (Fellbaum, 1998), thus exploring the portabil-ity of our methods to languages for which suchresources are not available.
We employ lexicaland low-level syntactic information (e.g., parts ofspeech, syntactic chunks) and show that the perfor-mance of our discourse chunker on the two subtasks(mentioned above) is comparable to that of a state-of-the-art sentence-level discourse parser (Soricutand Marcu, 2003).
We also assess its application po-tential on a sentence compression task (Knight andMarcu, 2003).2 Related WorkInitial work towards the development of discourseparsers has primarily relied on hand-crafted rules forspecifying world knowledge or constraints on treestructures (e.g., Hobbs 1993).
Recent work has seenthe emergence of treebanks annotated with discoursestructure, thus enabling the development of morerobust, data-driven models.
Marcu (2000) presentsa shift-reduce parsing model that segments textsinto edus and determines how they should be as-sembled into rhetorical structure trees.
Soricut andMarcu (2003) introduce a syntax-based sentence-level discourse parser, which consists of two compo-nents: a statistical segmentation model and a parserworking on the output of the segmenter.
Both com-ponents are trained on the RST-DT and exploit lexi-cal features as well as syntactic dominance features(which are taken from syntactic parse trees).Given that discourse-level information plays animportant role in human summarisation (Endres-Niggemeyer, 1998), it is not surprising that mod-els of discourse structure have found use in auto-matic summarisation.
For instance, Marcu (2000)proposes a summarisation algorithm that builds anRST tree for the entire text, and identifies its mostimportant parts according to discourse salience.Our work differs from previous approaches intwo key respects.
First, we do not attempt to pro-duce a hierarchical discourse structure.
We intro-duce discourse chunking, a less resource demandingtask than full discourse parsing.
We show that goodsaid Mr. Smith as the market plunged.Nucleus Satellite SatelliteAttributionNucleus Circumstance"I am optimistic"Figure 1: Discourse Tree in RST-DTchunking performance can be achieved with low-level information.
Second, we apply our discoursechunker to sentence compression.
Although previ-ous approaches have utilised discourse informationfor document summarisation, its application to sen-tence condensation is novel to our knowledge.3 Discourse Chunking3.1 Data and RepresentationWe propose a supervised machine learning approachto discourse chunking.
Our data were obtained fromthe RST-DT (Carlson et al, 2002), which consists of385 Wall Street Journal articles manually annotatedwith discourse structures in the framework of Mannand Thompson (1987).
An example of an RST-basedtree representation is shown in Figure 1; rectangu-lar boxes denote edus and arcs indicate which re-lations (e.g., Circumstance or Attribution) hold be-tween them.
Relations are typically binary with oneunit being the nucleus (indicated by arrows in Fig-ure 1) and the other the satellite, but multi-nuclearand non-binary relations are also possible.We are only interested in the lowest level of thetree, i.e., we aim to identify the edus and determinewhether they are nuclei or satellites.
For example,in the sentence in Figure 1 we want to identify thethree edus ?I am optimistic?, said Mr. Smith, and asthe market plunged.
and determine that the first ofthese functions as a nucleus at the lowest level ofthe tree whereas the latter two function as satellites.We do not try to determine that the first two edusare merged at a higher level and then function as theoverall nucleus of the sentence.The discourse chunking task assumes a non-hierarchical representation.
We converted eachsentence-level discourse tree into a flat chunk rep-resentation by assigning each token (i.e., word orpunctuation mark) a tag encoding its nuclearity sta-tus at the edu level.
We adopted the chunk repre-sentation proposed by Ramshaw and Marcus (1995)and used four different tags: B-NUC and B-SAT fornucleus and satellite-initial tokens, and I-NUC andI-SAT for non-initial tokens, i.e., tokens inside a nu-cleus and satellite span.
As all tokens belong either258to a nucleus or a satellite span, we do not need a spe-cial tag (typically denoted by O in syntactic chunk-ing) to indicate elements outside a chunk.
The chunkrepresentation for the sentence in Figure 1 is thus:?/B-NUC I/I-NUC am/I-NUC optimistic/I-NUC?/I-NUC said/B-SAT Mr./I-SAT Smith/I-SATas/B-SAT the/I-SAT market/I-SAT plunged/I-SAT ./I-SATDiscourse and sentence structure do not alwayscorrespond, and for 5% of sentences in the RST-DTno discourse tree exists.
We excluded these from ourdata.
We also disregarded sentences without internalstructure, i.e., those which consist of only one edu.The RST-DT is partitioned into a training (342 arti-cles) and test set (43 articles).
We preserved this splitin all our experiments.
52 articles in the RST-DT aredoubly annotated.
We used these to compute humanagreement on the discourse chunking task (see Sec-tion 4.1).3.2 ModellingUsing a chunk-based representation effectively ren-ders discourse processing a sequence labelling task.Two modelling approaches are possible.
The sim-plest model performs segmentation and labelling si-multaneously.
In our case this involves training aclassifier that labels each token with one of our fourtags (i.e., B-NUC, I-NUC, B-SAT, I-SAT).
Alterna-tively, we could treat discourse chunking as two dis-tinct subtasks involving two binary classifiers: a seg-menter, which determines the chunk boundaries andassigns each token a chunk-initial (B) or non-chunk-initial tag (I), and a labeller, which classifies eachchunk identified by the segmenter as either nucleus(NUC) or satellite (SAT).1The second approach has a number of advantages.First, abstracting away from a token-based represen-tation in the second step makes it easier to modelsentence-level distributional properties of nuclei andsatellites, e.g., the fact that every sentence has atleast one nucleus.
This can be achieved by incor-porating additional features into the labeller, suchas the number of chunks in the sentence or thelength of the current chunk.
A two-step approachalso avoids the creation of illegal chunk sequences,such as ?B-SAT I-NUC?.
However, a potential draw-back is that the number of training examples for thelabeller is reduced as the instances to be classifiedare chunks rather than tokens.
We explore the per-formance of the one-step and the two-step methodsin Sections 4.2 and 4.3, respectively.1A similar approach has been proposed for syntactic chunk-ing, e.g., Tjong Kim Sang (2000).A variety of learning schemes can be employedfor the discourse chunking task.
We have experi-mented with Boosting (Schapire and Singer, 2000),Conditional Random Fields (Lafferty et al, 2001),and Support Vector Machines (Vapnik, 1998).
Dis-cussion of our results focuses exclusively on boost-ing, since it had a slight advantage over the othermethods.
Boosting combines many simple, mod-erately accurate categorisation rules into a sin-gle, highly accurate rule.
We used BoosTexter?s(Schapire and Singer, 2000) implementation, whichcombines boosting with simple decision rules.
Thesystem permits three different types of features:numeric, nominal and ?text?.
Text-valued featurescan, for example, encode sequences of words orparts of speech.
BoosTexter applies n-gram mod-els when forming classification hypotheses for text-valued features.3.3 Features for the Token-Based ModelsWhile we use similar features for all our classifiers,their concrete implementation depends on whetherthe classifier is token-based (i.e., the one-step modeland the segmenter in the two-step method) or span-based (i.e., the labeller in the two-step method).
Wefirst describe the features for the former.Each token is represented as a feature vector en-coding information about the token itself and its con-text.
We intentionally limited our features to a basicset representing grammatical, syntactic, and lexicalinformation.Tokens This feature simply encodes the identityof the current token; we used raw tokens, withoutlemmatisation or stemming.Part-of-Speech Tags Tokens were also anno-tated with parts of speech using a publicly availablestate-of-the-art tagger (Mikheev, 1997).Syntactic Chunks Chunk information is a valu-able cue for determining discourse segments; it isunlikely that a segment boundary occurs within asyntactic chunk.
We applied a chunker (Mikheev,1997) to our data to discover noun and verb phrasechunks.
The chunker assigned one of five labels toeach token, encoding the first element of a noun orverb chunk (B-NP and B-VP, respectively), a non-initial element in a chunk (I-NP and I-VP), and anelement outside a chunk (O).
We used these chunklabels directly as features and also encoded gener-alisations over chunk and boundary types (i.e., VPvs.
NP and B vs.
I, respectively).Clause Information Knowing where clauseboundaries lie is important for segmentation, since259discourse segments often correspond to clauses.
Weused a rule-based algorithm (Leffa, 1998) to iden-tify clauses from the syntactic chunker?s output andrecorded for every token whether it is clause-initial(S) or not (X).Discourse Connectives Discourse connectivessuch as but often indicate which rhetorical relationholds between two spans.
While we do not aim to in-fer the relation proper, knowing the type of relationholding between spans often helps in determiningwhether they should be labelled as nucleus or satel-lite.
For example, Contrast relations (e.g., signalledby but) hold between two nuclei whereas Cause re-lations (e.g., signalled by because) hold between anucleus and a satellite.
Hence, we recorded the pres-ence of discourse connectives in a sentence to cap-ture, albeit in a shallow manner, the interdependencybetween rhetorical relations and nuclearity.We used Knott?s (1996) inventory of discourseconnectives and encoded two types of informationfor each token: (a) whether the token is a connective(C) or not (X) and (b) the identity of the connectiveif the token is a connective (zero otherwise).2Token Position For each token we calculated itsrelative position in the sentence (defined as the to-ken position divided by the number of tokens).
Thisinformation is useful to capture potential positionaldifferences between nuclei and satellites, i.e., it maybe that nuclei are more likely at the beginning of asentence than at the end.Context In addition to the nine features above,which encode information about the token itself, wealso implemented 16 contextual features to encodeinformation about its neighbouring tokens.
Syntac-tic chunking approaches typically capture contextualinformation by defining a small window of a few to-kens to the left and right of the current token (seeVeenstra, 1998).
However, we used the whole sen-tence as context, since BoosTexter is fairly good atdetermining automatically relevant n-grams within alonger string of tokens.
We included this contextualinformation for all nominal features; that is, we en-coded not only the string of preceding and followingtokens but also the string of preceding and followingpart-of-speech tags, syntactic chunk labels, clauselabels, and connectives.
For example, we had threetoken features, one encoding the current token itself,and two contextual features (one encoding the string2Some words can have syntactic as well as discourse mark-ing functions (e.g., but sometimes functions as a synonym forexcept rather than as a Contrast marker).
We do not disam-biguate between these two usages.of preceding tokens, and one encoding the string offollowing tokens); similarly we had three part-of-speech features, nine syntactic chunk features threeusing the complete chunk tags, three using only thechunk type, and three using the boundary type), andso on.3.4 Features for the Span-Based ModelFor the labeller we encoded information aboutspans rather than tokens.
This gave rise to six non-contextual, text-valued features: the string of tokensin the current span, their parts of speech, syntacticchunk tags, clause tags, and the presence and iden-tity of connectives.
The positional feature was re-defined in terms of relative span position, i.e., theposition of the current span divided by the numberof spans in the sentence.
We restricted contextualfeatures to information about immediately preced-ing and following spans (within a sentence).
We didnot include information about non-adjacent spansbecause only a minority of sentences in our data con-tained more than three spans.
Again, we includedcontextual information for all nominal features.
Fi-nally, to capture intra-sentential span-structure, weadded the following features:Span Length Span length was measured interms of the number of tokens in it and was repre-sented by three features: the length of the currentspan, and the lengths of its adjacent spans.
Spanlength information captures differences in the aver-age length of nuclei and satellite spans.Number of Spans We encoded the number ofspans in the sentence overall and the number ofspans preceding and following the current span.4 ExperimentsIn this section we describe the experiments that as-sess the merits of the discourse chunking frameworkintroduced above.
We also give details regarding pa-rameter estimation and training for our models andintroduce the baseline and state-of-the-art methodsused for comparison with our approach.4.1 Upper BoundBefore presenting the results of our modelling ex-periments, it is worth considering how well humansagree on discourse chunk segmentation and labellingin order to establish an upper bound for the task.
Wemeasured both unlabelled and labelled agreement onthe 52 doubly annotated RST-DT texts.
The formermeasures whether humans agree in placing chunkboundaries, whereas the latter additionally measures260whether humans agree in assigning chunk labels.To facilitate comparison with our models we reportinter-annotator agreement in terms of accuracy andF-score.3 For the unlabelled case we also report Win-dow Difference (WDiff), a commonly used evalua-tion measure for segmentation tasks (Pevzner andHearst, 2002).
It returns values between 0 (identicalsegmentations) and 1 (maximally different segmen-tations) and differs from accuracy in that predictedboundaries which are only slightly off are penalisedless than those which are completely wrong.Human agreement is relatively high4 on both seg-mentation and span labelling (see Table 1), whichcan be explained by the fact that (i) the RST-DTannotators were given very detailed and precise in-structions and (ii) assigning boundaries and labelsis an easier task than creating full-scale discoursetrees.4.2 One-Step ChunkingFor the one-step chunking method, our training setconsists of approximately 130,000 instances (i.e., to-kens).
We set aside 10% as a development set foroptimising BoosTexter?s parameters (i.e., the num-ber of training iterations and the maximal length ofthe n-grams considered for text-valued features).
Wethen re-trained BoosTexter with the optimal setting(700 iterations, n = 2) and applied it to the test set,which contained around 15,500 instances.By default, the one-step method treats every tokenin isolation, i.e., it assigns each token a tag withouttaking its neighbouring tags into account.
This is notan entirely adequate model, since the likelihood of atag is influenced by its surrounding tags.
For exam-ple, the probability of a token being tagged as I-NUCshould increase if the preceding token was taggedas B-NUC.
One way to take information about sur-rounding tags into account is by stacking classifiers,i.e., adding the output of one classifier to the inputof another.
Stacking is frequently used in chunkingtasks (e.g., Veenstra, 1998).
We stack two BoosTex-ter classifiers, by adding the string of all precedingand following tags (within a given sentence) to eachtoken?s feature vector for the second classifier.It would be possible to generate training mate-rial for the second classifier directly from the orig-inal training set by using the gold standard outputtags in the augmented feature vector.
However, we3For the unlabelled case, we report the F-score on bound-aries; for the labelled case, we report the average F-score overall class labels weighted by class frequency in the training set.4Using the Kappa statistic agreement on segmentationis K = .97 and on span labelling K = .81.found that this leads BoosTexter to rely too muchon these tags, largely ignoring other features.
Thiscauses problems when the model is applied to thetest set where the class tags are predicted and maycontain errors.
Hence, we applied the original model(BT-1-Step) to obtain predicted output tags for thetraining data and then used these, rather than thegold standard tags, to train the second classifier.Similarly, during testing, we first applied BT-1-Stepand used its output tags to augmented the featurevectors of the second classifier.For comparison, we also applied two baselinemodels to our data.
The first (BaseMaj) is obtainedby always assigning the tag that is most commonin the training data (I-NUC).
This strategy makesno attempt at guessing span boundaries.
The second(BaseClMaj) indirectly assesses the importance ofclause boundary detection.
It implements a strategywhich assumes that span boundaries always coin-cide with clause boundaries.
To obtain clause bound-aries, we used the gold standard annotation of ourdata in the Penn Treebank.
We then labelled allclause-initial tokens as B-NUC and all other tokensas I-NUC.
Note, that the use of gold standard clauseboundaries makes this a relatively high baseline.
Wealso applied Spade5, Soricut and Marcu?s (2003)sentence-level discourse parser (see Section 2) toour test set.
For evaluation purposes, Spade?s out-put was converted to our chunk representation.
It isimportant to note that Spade is a much more sophis-ticated model than the ones presented in this paper.We therefore do not expect to be able to obtain a bet-ter performance.
It is nevertheless interesting to seehow far one can go with a modest feature space andconsiderably less structural information.Table 1 shows the results.
A set of diacritics isused to indicate significance (on accuracy) through-out this paper, see Table 2.
On the segmentation task(unlabelled) BT-1-Step and its stacked variant sig-nificantly outperform the majority baseline (Base-Maj) but are significantly less accurate than Base-ClMaj, which uses gold standard clause boundaries.The two BoosTexter models also perform signifi-cantly worse than Spade on segmentation.
However,the higher WDiff for Spade on the segmentation tasksuggests that the boundaries predicted by our mod-els contain more ?near misses?
than those predictedby Spade.
When segmentation and span labelling aretaken into account (labelled), our one-step modelssignificantly outperform both baselines but are sig-nificantly less accurate than Spade.
Classifier stack-5The software is publicly available from http://www.isi.edu/licensed-sw/spade/.261unlabelled labelledModels Acc % F-score WDiff Acc % F-scoreBaseMaj 88.50 ?
.4021 53.87 38.77BaseClMaj 93.51 70.06 .2008 56.64 43.62BT-1-Step 90.07??
?$ 64.64 .2148 74.40??
?$ 74.13BT-1-Step, stacked 91.86?6 ?
?$ 68.95 .1795 75.55??
?$ 75.37BT-2-Step 97.37??
?$ 88.28 .0733 78.27??
6 ?$ 78.38BT-2-Step, stacked 97.41??
?$ 88.40 .0727 76.31??
?$ 76.34Spade 93.49?6 ?$ 87.06 .5071 79.21?
?$ 80.91Humans 99.05 97.96 .0012 89.10 89.03Table 1: Results on discourse segmentation and span labellingSymbols Meaning?
6 ?
(not) sig different from BaseMaj?
6 ?
(not) sig different from BaseClMaj?
6 ?
(not) sig different from Spade$ 6 $ (not) sig different from HumansTable 2: Meaning of diacritics indicating statisticalsignificance (?2 tests, p < 0.05)ing leads to slight improvements over the simpleBoosTexter model, but the difference is not statis-tically significant.4.3 Two-Step ChunkingIn the two-step model, chunking consists of twoseparate subtasks: segmentation and labelling.
Togenerate training material for the segmenter, we re-placed the four chunk labels in the original data setby their corresponding boundary labels (B, I).
Forthe labeller, training instances are spans rather thantokens.
We used the gold standard span boundariesto convert the original training set to a span-basedrepresentation.
This new training set containedaround 15,000 instances (compared to 130,000 in-stances in the token-based set).
For both the seg-menter and labeller, we set aside 10% of the ma-terial as development data to optimise BoosTexter?sparameters (900 iterations, n = 3 for segmentation,and 600 iterations, n = 2 for labelling).For testing, we first applied the segmenter to ob-tain discourse chunk boundaries.
We then used thepredicted boundaries to convert the test data into aspan-based representation, which we then used asinput for the labeller.
For evaluation, the output ofthe labeller was converted back to a token-based rep-resentation.
As with one-step chunking, we also im-plemented a stacked variant, stacking both the seg-mentation and the labelling models.It can be seen in Table 1 that the two-step mod-els outperform the one-step models.
This differenceis significant except for the stacked model on the la-belling task (labelled).
Both two-step models signif-icantly outperform both baselines on segmentation(unlabelled) and labelling (labelled).
They also sig-nificantly outperform Spade on the boundary pre-diction task, which is in itself an important sub-task for discourse parsing.
The unstacked two-stepBoosTexter model performs comparably to Spadewith respect to labelled accuracy; the difference be-tween the two models is not statistically signifi-cant.
Hence, we achieve results similar to Spade butwith much simpler and knowledge-leaner features.As with the one-step method, the stacked modelperforms (insignificantly) better than its unstackedcounterpart on the segmentation task.
However, onthe labelling task, the stacked variant performs sig-nificantly worse.
We conjecture that the reducedtraining set size for the labeller causes the stackedmodel (which is effectively trained twice) to overfit.Expectedly, all models perform significantly worsethan humans on both tasks.To assess whether our discourse chunker couldbe ported to languages for which discourse tree-banks are not yet available, we investigated howmuch annotated data is required to achieve satis-factory results.
Assuming that annotators proceedsentence-by-sentence, we varied the amount of sen-tences in our training data and determined its ef-fect on the learner?s (BT-2-Step) performance.
Fig-ure 2 shows that satisfactory labelled and unlabelledperformance (86.52% and 74.64% F-score, respec-tively) can be achieved with approximately half thetraining data (i.e., around 2,000 sentences).
In fact,using the entire data set yields a moderate increaseof 1.78% for the unlabelled task and 3.68% for thelabelled task.
Hence, it seems that our knowledge-lean method is suitable even for relatively smalltraining sets.
We next examine whether the two-stepchunking model can be usefully employed in a prac-tical application such as sentence compression.2620 472 949 1,428 1,8872,3502,8233,2903,8524,2584,734Number of sentences in training data6065707580859095100F-scoreUnlabelledLabelledFigure 2: Learning curve for discourse segmentation(unlabelled) and span labelling (labelled)4.4 Sentence CompressionSentence compression can be likened to summari-sation at the sentence level.
The task has an imme-diate impact on several applications ranging fromsummarisation to audio scanning devices for theblind and caption generation (see Knight and Marcu,2002 and the references therein).
Previous data-driven approaches (Knight and Marcu, 2003; Riezleret al, 2003) relied on parallel corpora to determinewhat is important in a sentence.
The models learnedcorrespondences between long sentences and theirshorter counterparts, typically employing a rich fea-ture space induced from parse trees.
The task is chal-lenging since the compressed sentences should re-tain essential information and convey it grammati-cally.Here, we propose a complementary approachwhich utilises discourse chunking.
A compressedsentence can be obtained from the output of thechunker simply by removing satellites.
We thus cap-italise on RST?s (Mann and Thompson, 1987) no-tion of nuclearity and the widely held assumptionthat spans functioning as satellites can often bedeleted without disrupting coherence.
To evaluatethe compressions produced by our chunking model,we elicited judgements from human subjects.
We de-scribe our elicitation study and results as follows.Data We randomly selected 40 sentences fromthe test portion of the RST-DT.
Average sentencelength was 38.75.
The sentences were compressedby chunking them with our (unstacked) two-stepmodel (BT-2-Step) and then dropping satellites.
Weapplied the same strategy to derive compressed sen-tences from the output of Spade (Soricut and Marcu,2003), and also produced human compressions.
Fi-OriginalAdministration officials traveling with President Bush inCosta Rica interpreted Mr. Ortega?s wavering as a sign thathe isn?t responding to the military attacks so much as he issearching for ways to strengthen his hand prior to the elec-tions.BaselineAdministration officials interpreted Mr. Ortega?s wavering.BT-2-StepAdministration officials interpreted Mr. Ortega?s wavering asa sign that he isn?t responding to the military attacks so muchas he is searching for ways.SpadeAdministration officials traveling with President Bush inCosta Rica interpreted Mr. Ortega?s wavering as a sign.HumanAdministration officials interpreted Mr. Ortega?s wavering asa sign that he is searching for ways to strengthen his hand priorto the elections.Table 3: Example compressionsCompression AvgLen RatingBaseline 9.70 1.93BT-2-Step 22.06 3.21Spade 19.09 3.10Humans 20.07 3.83Table 4: Mean ratings for automatic compressionsnally, we added a simple baseline compression al-gorithm proposed by Jing and McKeown (2000)which removed all prepositional phrases, clauses, to-infinitives, and gerunds.
Both the baseline and Spadeoperate on parse trees which were obtained fromCharniak?s (2000) parser.
Our set of experimentalmaterials contained 4?40 = 160 compressions.Procedure and Subjects We obtained com-pression ratings during an elicitation study com-pleted by 45 unpaid volunteers, all native speakerof English.
The study was conducted remotely overthe Internet.
Participants first saw a set of instruc-tions that explained the task, and defined sentencecompression using multiple examples.
The materi-als consisted of the original sentences together withtheir compressed versions.
They were randomised inlists following a Latin square design ensuring thatno two compressions in a list were generated fromthe same sentence.
As in Knight and Marcu?s (2003)study, participants were asked to use a five pointscale to rate the systems?
compressions (taking intoaccount the felicity of the compression as well as itsgrammaticality); they were told that all outputs weregenerated automatically.
Examples of the compres-sions our participants saw are given in Table 3.Results We carried out an Analysis of Variance(ANOVA) to examine the effect of different typesof compressions (Baseline, BT-2-Step, Spade, andHuman).
Statistical tests were done using the mean263of the ratings shown in Table 4.
The ANOVA re-vealed a reliable effect of compression type by sub-jects (F1(3,90) = 149.50, p < 0.001) and by items(F2(3;117) = 40.23, p < 0.001).
Post-hoc Tukeytests indicated that human compressions are per-ceived as significantly better than the compressionsproduced by the baseline, BT-2-Step, and Spade(?
= 0.01).
The discourse chunker and Spade aresignificantly better than the baseline (?
= 0.01).
TheTukey test revealed no statistically significant dif-ference between these two algorithms (?
= 0.01).To summarise, both BoosTexter and Spade performcloser to human performance than the baseline; yet,humans perform significantly better than our com-pression algorithms.5 ConclusionsIn this paper we proposed discourse chunking as analternative to full-scale parsing.
Central in our ap-proach is the use of low-level syntactic and gram-matical information which we argue holds promisefor the development of discourse processing mod-els across languages and domains.
We showed thata knowledge-lean feature space achieves good per-formance both on segmentation and span labelling.Furthermore, we assessed the application potentialof our chunker and showed that it can be success-fully employed to generate sentence compressions,thus confirming one of RST?s main claims regard-ing the nuclearity of discourse spans (at least on thesentence-level).An important future direction lies in extendingour model to the document-level and the assign-ment of rhetorical relations, thus going beyond thebasic nucleus-satellite distinction.
Our results indi-cate that a modular approach to discourse process-ing (i.e., treating segmentation as separate from la-belling) could increase performance.
In the future,we plan to investigate how to combine our chunkerwith models like Spade for improved prediction onboth local and global levels.AcknowledgmentsThe authors acknowledge the support of EPSRC (Sporleder,grant GR/R40036/01; Lapata, grant GR/T04540/01).
Thanksto Amit Dubey, Ben Hutchinson, Alex Lascarides, SimoneTeufel, and three anonymous reviewers for helpful commentsand suggestions.ReferencesL.
Carlson, D. Marcu, M. E. Okurowski.
2002.
RST DiscourseTreebank.
Linguistic Data Consortium, 2002.J.
Chai, R. Jing.
2004.
Discourse structure for context questionanswering.
In Proceedings of the Workshop on Pragmaticsof Question Answering at HLT-NAACL 2004, 23?30.E.
Charniak.
2000.
A maximum-entropy-inspired parser.
InProceedings of the 1st NAACL, 132?139.B.
Endres-Niggemeyer.
1998.
Summarising Information.Springer, Berlin.C.
Fellbaum, ed.
1998.
WordNet: An Electronic Database.MIT Press, Cambridge, MA.J.
R. Hobbs, M. Stickel, D. Appelt, P. Martin.
1993.
Interpre-tation as abduction.
Journal of Artificial Intelligence, 63(1?2):69?142.K.
Humphreys, R. Gaizauskas, S. Azzam.
1997.
Event coref-erence for information extraction.
In Proceedings of theACL Workshop on Operational Factors in Practical RobustAnaphora Resolution for Unrestricted Texts, 75?81.H.
Jing, K. McKeown.
2000.
Cut and paste summarization.
InProceedings of the 1st NAACL, 178?185.K.
Knight, D. Marcu.
2003.
Summarization beyond sentenceextraction: A probabilistic approach to sentence compres-sion.
Artificial Intelligence, 139(1):91?107.A.
Knott.
1996.
A Data-Driven Methodology for Motivatinga Set of Coherence Relations.
Ph.D. thesis, Department ofArtificial Intelligence, University of Edinburgh.J.
Lafferty, A. McCallum, F. Pereira.
2001.
Conditional ran-dom fields: Probabilistic models for segmenting and labelingsequence data.
In Proceedings of the 18th ICML, 282?289.V.
J. Leffa.
1998.
Clause processing in complex sentences.
InProceedings of the 1st LREC, 937?943.W.
C. Mann, S. A. Thompson.
1987.
Rhetorical structure the-ory: A theory of text organization.
Technical Report ISI/RS-87-190, ISI, Los Angeles, CA, 1987.D.
Marcu.
1998.
To build text summaries of high quality, nu-clearity is not sufficient.
In Working Notes of the AAAI-98Spring Symposium on Intelligent Text Summarization, 1?8.D.
Marcu.
2000.
The Theory and Practice of Discourse Parsingand Summarization.
The MIT Press, Cambridge, MA.A.
Mikheev.
1997.
The LTG part of speech tagger.
Technicalreport, University of Edinburgh, 1997.L.
Pevzner, M. Hearst.
2002.
A critique and improvement ofan evaluation metric for text segmentation.
ComputationalLinguistics, 28(1):19?36.L.
A. Ramshaw, M. P. Marcus.
1995.
Text chunking usingtransformation-based learning.
In Proceedings of the 3rdACL Workshop on Very Large Corpora, 82?94.S.
Riezler, T. H. King, R. Crouch, A. Zaenen.
2003.
Statisticalsentence condensation using ambiguity packing and stochas-tic disambiguation methods for lexical-functional grammar.In Proceedings of HLT/NAACL 2003, 118?125.R.
E. Schapire, Y.
Singer.
2000.
BoosTexter: A boosting-based system for text categorization.
Machine Learning,39(2/3):135?168.R.
Soricut, D. Marcu.
2003.
Sentence level discourse parsingusing syntactic and lexical information.
In Proceedings ofHLT/NAACL 2003.S.
Teufel, M. Moens.
2002.
Summarizing scientific articles ?experiments with relevance and rhetorical status.
Computa-tional Linguistics, 28(4):409?446.E.
F. Tjong Kim Sang.
2000.
Text chunking by system combi-nation.
In Proceedings of CoNLL-00, 151?153.V.
Vapnik.
1998.
Statistical Learning Theory.
Wiley-Interscience, New York.J.
Veenstra.
1998.
Fast NP chunking using memory-basedlearning techniques.
In Proceedings of BENELEARN, 71?79.264
