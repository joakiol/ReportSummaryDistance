Discovering word senses from a network of lexical cooccurrencesOlivier FerretCEA ?
LIST/LIC2M18, route du Panorama92265 Fontenay-aux-Roses, Franceferreto@zoe.cea.frAbstractLexico-semantic networks such as WordNethave been criticized about the nature of thesenses they distinguish as well as on the waythey define these senses.
In this article, we pre-sent a possible solution to overcome these lim-its by defining the sense of words from the waythey are used.
More precisely, we propose todifferentiate the senses of a word from a net-work of lexical cooccurrences built from alarge corpus.
This method was tested both forFrench and English and was evaluated for Eng-lish by comparing its results with WordNet.1 IntroductionSemantic resources have proved to be useful ininformation retrieval and information extractionfor applications such as query expansion (Voor-hees, 1998), text summarization (Harabagiu andMaiorano, 2002) or question/answering (Pasca andHarabagiu, 2001).
But this work has also shownthat these resources must be used with caution:they bring on an improvement of results only ifword sense disambiguation is performed with agreat accuracy.
These findings bring one of thefirst roles of a semantic resource to light: discrimi-nating and characterizing the senses of a set ofwords.
The main semantic resources with a widecoverage that can be exploited by computers arelexico-semantic networks such as WordNet.
Be-cause of the way they were built, mainly by hand,these networks are not fundamentally differentfrom traditional dictionaries.
Hence, it is not verysurprising that they were criticized, as in (Hara-bagiu et al, 1999), for not being suitable for Natu-ral Language Processing.
They were criticizedboth about the nature of the senses they discrimi-nate and the way they characterize them.
Theirsenses are considered as too fine-grained but alsoincomplete.
Moreover, they are generally definedthrough their relations with synonyms, hyponymsand hyperonyms but not by elements that describethe contexts in which they occur.One of the solutions for solving this problemconsists in automatically discovering the senses ofwords from corpora.
Each sense is defined by a listof words that is not restricted to synonyms or hy-peronyms.
The work done in this area can be di-vided into three main trends.
The first one, repre-sented by (Pantel and Lin, 2002), is not focused onthe problem of discovering word senses: its mainobjective is to build classes of equivalent wordsfrom a distributionalist viewpoint, hence to gatherwords that are mainly synonyms.
In the case of(Pantel and Lin, 2002), the discovering of wordsenses is a side effect of the clustering algorithm,Cluster By Committee, used for building classes ofwords: as a word can belong to several classes,each of them can be considered as one of itssenses.
The second main trend, found in (Sch?tze,1998), (Pedersen and Bruce, 1997) and (Puran-dare, 2003), represents each instance of a targetword by a set of features that occur in itsneighborhood and applies an unsupervised cluster-ing algorithm to all its instances.
Each cluster isthen considered as a sense of the target word.
Thelast trend, explored by (V?ronis, 2003), (Dorowand Widdows, 2003) and (Rapp, 2003), starts fromthe cooccurrents of a word recorded from a corpusand builds its senses by gathering its cooccurrentsaccording to their similarity or their dissimilarity.Our work takes place in this last trend.2 OverviewThe starting point of the method we present in thisarticle is a network of lexical cooccurrences, thatis a graph whose vertices are the significant wordsof a corpus and edges represent the cooccurrencesbetween these words in the corpus.
The discove-ring of word senses is performed word by wordand the processing of a word only relies on thesubgraph that contains its cooccurrents.
The firststep of the method consists in building a matrix ofsimilarity between these cooccurrents by exploit-ing their relations in the subgraph.
An unsuper-vised clustering algorithm is then applied forgrouping these cooccurrents and giving rise to thesenses of the considered word.
This method, as theones presented in (V?ronis, 2003), (Dorow andWiddows, 2003) and (Rapp, 2003), relies on thefollowing hypothesis: in the subgraph gatheringthe cooccurrents of a word, the number of relationsbetween the cooccurrents defining a sense ishigher than the number of relations that thesecooccurrents have with those defining the othersenses of the considered word.
The clustering al-gorithm that we use is an adaptation of the SharedNearest Neighbors (SNN) algorithm presented in(Ert?z et al, 2001).
This algorithm particularly fitsour problem as it automatically determines thenumber of clusters, in our case the number ofsenses of a word, and does not take into accountthe elements that are not representative of the clus-ters it builds.
This last point is especially importantfor our application as there is a lot of ?noise?among the cooccurrents of a word.3 Networks of lexical cooccurrencesThe method we present in this article for discover-ing word senses was applied both for French andEnglish.
Hence, two networks of lexical cooccur-rences were built: one for French, from the LeMonde newspaper (24 months between 1990 and1994), and one for English, from the L.A. Timesnewspaper (2 years, part of the TREC corpus).
Thesize of each corpus was around 40 million words.The building process was the same for the twonetworks.
First, the initial corpus was pre-processed in order to characterize texts by theirtopically significant words.
Thus, we retained onlythe lemmatized form of plain words, that is, nouns,verbs and adjectives.
Cooccurrences were classi-cally extracted by moving a fixed-size window ontexts.
Parameters were chosen in order to catchtopical relations: the window was rather large, 20-word wide, and took into account the boundariesof texts; moreover, cooccurrences were indifferentto word order.
As (Church and Hanks, 1990), weadopted an evaluation of mutual information as acohesion measure of each cooccurrence.
Thismeasure was normalized according to the maximalmutual information relative to the considered cor-pus.
After filtering the less significant cooccur-rences (cooccurrences with less than 10 occur-rences and cohesion lower than 0.1), we got a net-work with approximately 23,000 words and5.2 million cooccurrences for French, 30,000words and 4.8 million cooccurrences for English.4 Word sense discovery algorithm4.1 Building of the similarity matrix be-tween cooccurrentsThe number and the extent of the clusters built bya clustering algorithm generally depend on a set ofparameters that can be tuned in one way or an-other.
But this possibility is implicitly limited bythe similarity measure used for comparing the ele-ments to cluster.
In our case, the elements tocluster are the cooccurrents in the network of lexi-cal cooccurrences of the word whose senses haveto be discriminated.
Within the same framework,we tested two measures for evaluating the similar-ity between the cooccurrents of a word in order toget word senses with different levels of granular-ity.
The first measure corresponds to the cohesionmeasure between words in the cooccurrence net-work.
If there is no relation between two words inthe network, the similarity is equal to zero.
Thismeasure has the advantage of being simple andefficient from an algorithmic viewpoint but somesemantic relations are difficult to catch only fromcooccurrences in texts.
For instance, we experi-mentally noticed that there are few synonyms of aword among its cooccurrents1.
Hence, we can ex-pect that some senses that are discriminated by thealgorithm actually refer to one sense.To overcome this difficulty, we also tested ameasure that relies not only on first order cooccur-rences but also on second order cooccurrences,which are known to be ?less sparse and more ro-bust?
than first order ones (Sch?tze, 1998).
Thismeasure is based on the following principle: a vec-tor whose size is equal to the number of cooccur-rents of the considered word is associated to eachof its cooccurrents.
This vector contains the cohe-sion values between this cooccurrent and the otherones.
As for the first measure, a null value is takenwhen there is no relation between two words in thecooccurrence network.
The similarity matrix isthen built by applying the cosine measure between1 This observation comes from the intersection, for each wordof the L.A. Times network, of its cooccurrents in the networkand its synonyms in WordNet.each couple of vectors, i.e.
each couple of cooc-currents.
With this second measure, two cooccur-rents can be found strongly linked even thoughthey are not directly linked in the cooccurrencenetwork: they just have to share a significant num-ber of words with which they are linked in thecooccurrence network.4.2 The Shared Nearest Neighbors (SNN)algorithmThe SNN algorithm is representative of the algo-rithms that perform clustering by detecting thehigh-density areas of a similarity graph.
In such agraph, each vertex represents an element to clusterand an edge links two vertices whose similarity isnot null.
In our case, the similarity graph directlycorresponds to the cooccurrence network with thefirst order cooccurrences whereas with the secondorder cooccurrences, it is built from the similaritymatrix described in Section 4.1.
The SNN algo-rithm can be split up into two main steps: the firstone aims at finding the elements that are the mostrepresentative of their neighborhood by maskingthe less important relations in the similarity graph.These elements are the seeds of the final clustersthat are built in the second step by aggregating theremaining elements to those selected by the firststep.
More precisely, the SNN algorithm is appliedto the discovering of the senses of a target word asfollows:1. sparsification of the similarity graph: for eachcooccurrent of the target word, only the linkstowards the k (k=15 in our experiments) mostsimilar other cooccurrents are kept.2.
building of the shared nearest neighbor graph:this step only consists in replacing, in the spar-sified graph, the value of each edge by thenumber of direct neighbors shared by the twocooccurrents linked by this edge.3.
computation of the distribution of strong linksamong cooccurrents: as for the first step, thisone is a kind of sparsification.
Its aim is tohelp finding the seeds of the senses, i.e.
thecooccurrents that are the most representative ofa set of cooccurrents.
This step is also a meansfor discarding the cooccurrents that have norelation with the other ones.
More precisely,two cooccurrents are considered as stronglylinked if the number of the neighbors theyshare is higher than a fixed threshold.
Thehigher than a fixed threshold.
The number ofstrong links of each cooccurrent is then com-puted.4.
identification of the sense seeds and filteringof noise: the sense seeds and the cooccurrentsto discard are determined by comparing theirnumber of strong links with a fixed threshold.5.
building of senses: this step mainly consists inassociating to the sense seeds identified by theprevious step the remaining cooccurrents thatare the most similar to them.
The result is a setof clusters that each represents a sense of thetarget word.
For associating a cooccurrent to asense seed, the strength of the link betweenthem must be higher than a given threshold.
Ifa cooccurrent can be tied to several seeds, theone that is the most strongly linked to it is cho-sen.
Moreover, the seeds that are considered astoo close from each other for giving rise toseparate senses can also be grouped during thisstep in accordance with the same criteria thanthe other cooccurrents.6.
extension of senses: after the previous steps, aset of cooccurrents that are not considered asnoise are still not associated to a sense.
Thesize of this set depends on the strictness of thethreshold controlling the aggregation of acooccurrent to a sense seed but as we are inter-ested in getting homogeneous senses, the valueof this threshold cannot be too low.
Neverthe-less, we are also interested in having a defini-tion as complete as possible of each sense.
Assenses are defined at this point more preciselythan at step 4, the integration into these sensesof cooccurrents that are not strongly linked to asense seed can be performed on a larger basis,hence in a more reliable way.4.3 Adaptation of the SNN algorithmFor implementing the SNN algorithm presented inthe previous section, one of the points that must bespecified more precisely is the way its differentthresholds are fixed.
In our case, we chose thesame method for all of them: each threshold is setas a quantile of the values it is applied to.
In thisway, it is adapted to the distribution of these val-ues.
For the identification of the sense seeds(threshold equal to 0.9) and for the definition ofthe cooccurrents that are noise (threshold  equal toLM-1 LM-2 LAT-1 LAT-1.no LAT-2.nonumber of words 17,261 17,261 13,414 6,177 6,177percentage of words with at least one sense 44.4% 42.7% 39.8% 41.8% 39%average number of senses by word 2.8 2.2 1.6 1.9 1.5average number of words describing a sense 16.1 16.3 18.7 20.2 18.9Table 1: Statistics about the results of our word sense discovery algorithm0.2), the thresholds are quantiles of the number ofstrong links of cooccurrents.
For defining stronglinks (threshold equal to 0.65), associating cooc-currents to sense seeds (threshold equal to 0.5) andaggregating cooccurrent to senses (threshold equalto 0.7), the thresholds are quantiles of the strengthof the links between cooccurrents in the sharednearest neighbor graph.We also introduced two main improvements tothe SNN algorithm.
The first one is the addition ofa new step between the last two ones.
This comesfrom the following observation: although a senseseed can be associated to another one during thestep 5, which means that the two senses they rep-resent are merged, some clusters that actually cor-respond to one sense are not merged.
This problemis observed with the first and the second ordercooccurrences and cannot be solved, withoutmerging unrelated senses, only by adjusting thethreshold that controls the association of a cooc-current to a sense seed.
In most of these cases, the?split?
sense is scattered over one large cluster andone or several small clusters that only contain 3 or4 cooccurrents.
More precisely, the sense seeds ofthe small clusters are not associated to the seed ofthe large cluster while most of the cooccurrentsthat are linked to them are associated to this seed.Instead of defining a specific mechanism for deal-ing with these small clusters, we chose to let theSNN algorithm to solve the problem by only delet-ing these small clusters (size < 6) after the step 5and marking their cooccurrents as unclassified.The last step of the algorithm aggregates in mostof the cases these cooccurrents to the large cluster.Moreover, this new step makes the built sensesmore stable when the parameters of the algorithmare only slightly modified.The second improvement, which has a smallerimpact than the first one, aims at limiting the noisethat is brought into clusters by the last step.
In thealgorithm of (Ert?z et al, 2001), an element canbe associated to a cluster when the strength of itslink with one of the elements of this cluster ishigher than a given threshold.
This condition isstricter in our case as it concerns the averagestrength of the links between the unclassifiedcooccurrent and those of the cluster.5 ExperimentsWe applied our algorithm for discovering wordsenses to the two networks of lexical cooccur-rences we have described in Section 3 (LM:French; LAT: English) with the parameters givenin Section 4.
For each network, we tested the useof first order cooccurrences (LM-1 and LAT-1)and second order ones (LM-2 and LAT-2).
ForEnglish, the use of second order cooccurrenceswas tested only for the subpart of the words of thenetwork that was selected for the evaluation ofSection 6 (LAT-2.no).
Table 1 gives some statis-tics about the results of the discovered senses forthe different cases.
We can notice that a significantpercentage of words do not have any sense, evenwith second order cooccurrences.
This comes fromthe fact that their cooccurrents are weakly linkedto each other in the cooccurrence network they arepart of, which probably means that their senses arenot actually represented in this network.
We canalso notice that the use of second order cooccur-rence actually leads to have a smaller number ofsenses by word, hence to have senses with a largerdefinition.
As V?ronis (2003), we give in Table 2as an example of the results of our algorithm someof the words defining the senses of the polysemousFrench word barrage, which was part of theROMANSEVAL evaluation.
Whatever the kind ofcooccurrences it relies on, our algorithm findsthree of the four senses distinguished in (V?ronis,2003): dam (senses 1.3 and 2.1); barricading,blocking (senses 1.1, 1.2 and 2.2); barrier, frontier(senses 1.4 and 2.3).
The sense play-off game(match de barrage), which refers to the domain ofsport, is not found as it is weakly represented inthe cooccurrence network and is linked to words,such as division, that are also ambiguous (it  refersLM-1 1.1 manifestant, forces_de_l?ordre, pr?fecture, agriculteur, protester, incendier, calme, pierre(demonstrator, the police, prefecture, farmer, to protest, to burn, quietness, stone)1.2 conducteur, routier, v?hicule, poids_lourd, camion, permis, trafic, bloquer, voiture, autoroute(driver, lorry driver, vehicule, lorry, truck, driving licence, traffic, to block, car, highway)1.3 fleuve, rivi?re, lac, bassin, m?tre_cube, crue, amont, pollution, affluent, saumon, poisson(river(2), lake, basin, cubic meter, swelling, upstream water, pollution, affluent, salmon, fish)1.4 bless?, casque_bleu, soldat, tir, milice, convoi, ?vacuer, croate, milicien, combattant(wounded, U.N. soldier, soldier, firing, militia, convoy, to evacuate, Croatian, militiaman, combatant)LM-2 2.1 eau, m?tre, lac, pluie, rivi?re, bassin, fleuve, site, poisson, affluent, montagne, crue, vall?e(water, meter, lake, rain, river(2), basin, setting, fish, affluent, mountain, swelling, valley)2.2 conducteur, trafic, routier, route, camion, chauffeur, voiture, chauffeur_routier, poids_lourd(driver, traffic, lorry driver(3), road, lorry, car, truck)2.3 casque_bleu, soldat, tir, convoi, milicien, blind?, milice, a?roport, bless?, incident, croate(U.N. soldier, soldier, firing, convoy, militiaman, tank, militia, airport, wounded, incident, Croatian)Table 2: Senses found by our algorithm for the word barrageboth to the sport and the military domains).
Itshould be note that barrage has only 1,104 occur-rences in our corpus while it has 7,000 occurrencesin the corpus of (V?ronis, 2003), built by crawlingfrom the Internet the pages found by a meta searchengine queried with this word and its morphologi-cal variants.
This example is also a good illustra-tion of the difference of granularity of the sensesbuilt from first order cooccurrences and those builtfrom the second order ones.
The sense 1.1, whichis close to the sense 1.2 as the two refers to dem-onstrations in relation to a category of workers,disappears when the second order cooccurrencesare used.
Table 3 gives examples of discoveredsenses from first order cooccurrences only, one forFrench (LM-1) and two for English (LAT-1).6 EvaluationThe discovering of word senses, as most of thework dedicated to the building of linguistic re-sources, comes up against the problem of evaluat-ing its results.
The most direct way of doing it is tocompare the resource to evaluate with a similarresource that is acknowledged as a golden stan-dard.
For word senses, the WordNet-like lexico-semantic networks can be considered as such astandard.
Using this kind of networks for evaluat-ing the word senses that we find is of course criti-cizable as our aim is to overcome their insufficien-cies.
Nevertheless, as these networks are carefullycontrolled, such an evaluation provides at least afirst judgment about the reliability of the discov-ered senses.
We chose to take up the evaluationmethod proposed in (Pantel and Lin, 2002).
Thismethod relies on WordNet and shows a rathergood agreement between its results and humanjudgments (88% for Pantel and Lin).
As a conse-quence, our evaluation was done only for English,and more precisely with WordNet 1.7.
For eachconsidered word, the evaluation method tries tomap one of its discovered senses with one of itssynsets in WordNet by applying a specific similar-ity measure.
Hence, only the precision of the wordsense discovering algorithm is evaluated butPantel and Lin indicate that recall is not very sig-nificant in this context: a discovered sense may becorrect and not present in WordNet and con-versely, some senses in WordNet are very closeand should be joined for most of the applicationsusing WordNet.
They define a recall measure butonly for ranking the results of a set of systems.Hence, it cannot be applied in our case.The similarity measure between a sense and asynset used for computing precision relies on theLin?s similarity measure between two synsets:)2(log)1(log)(log2)2,1( sPsPsPsssim+?= (1)where s is the most specific synset that subsumess1 and s2 in the WordNet hierarchy and P(s)represents the probability of the synset s estimatedfrom a reference corpus, in this case the SemCorcorpus.
We used the implementation of this meas-ure provided by the Perl module WordNet::Similarity v0.06 (Patwardhan and Pedersen,2003).
The similarity between a sense and a synsetis more precisely defined as the average value ofthe similarity values between the words that char-acterize the sense, or a subset of them, and thesynset.
The similarity between a word and a synsetorgane (1300)2 patient, transplantation, greffe, malade, th?rapeutique, m?dical, m?decine, greffer, rein(patient, transplantation, transplant, sick person, therapeutic, medical, medicine, to transplant,kidney)procr?ation, embryon, ?thique, humain, relatif, bio?thique, corps_humain, g?ne, cellule(procreation, embryo, ethical, human, relative, bioethics, human body, gene, cell)constitutionnel, consultatif, constitution, instituer, ex?cutif, l?gislatif, si?ger, disposition(constitutional, consultative, constitution, to institute, executive, legislative, to sit, clause)article, hebdomadaire, publication, r?daction, quotidien, journal, ?ditorial, r?dacteur(article, weekly, publication, editorial staff, daily, newspaper, editorial, sub-editor)mouse (563) compatible, software, computer, machine, user, desktop, pc, graphics, keyboard, devicelaboratory, researcher, cell, gene, generic, human, hormone, research, scientist, ratparty (16999) candidate, democrat, republican, gubernatorial, presidential, partisan, reapportionmentballroom, cocktail, champagne, guest, bash, gala, wedding, birthday, invitation, festivitycaterer, uninvited, party-goers, black-tie, hostess, buffet, glitches, napkins, cateringTable 3: Senses found by our algorithm from first order cooccurrences (LM-1 and LAT-1)is equal to the highest similarity value amongthose between the synset and the synsets to whichthe word belongs to.
Each of these values is givenby (1).
A sense is mapped to the synset that is themost similar to it, providing that the similaritybetween them is higher than a fixed threshold(equal to 0.25 as in (Pantel and Lin, 2002)).
Fi-nally, the precision for a word is given by the pro-portion of its senses that match one of its synsets.Table 4 gives the results of the evaluation of ouralgorithm for the words of the English cooccur-rence network that are nouns only and for which atleast one sense was discovered.
As Pantel and Lin,we only take into account for evaluation 4 wordsof each sense, whatever the number of words thatdefine it.
But, because of the way our senses arebuilt, we have not a specific measure of the simi-larity between a word and the words that charac-terize its senses.
Hence, we computed two variantsof the precision measure.
The first one selects thefour words of each sense by relying on their num-ber of strong links in the shared nearest neighborgraph.
The second one selects the four words thathave the highest similarity score with one of thesynsets of the target word, which is called ?opti-mal choice?
in Table 43.
A clear difference can benoted between the two variants.
With the optimalchoice of the four words, we get results that aresimilar to those of Pantel and Lin: their precisionis equal to 60.8 with an average number of wordsdefining a sense equal to 14.2 Each word is given with its frequency in the corpus used forbuilding the cooccurrence network.3 This selection procedure is only used for evaluation and wedo no rely on WordNet for building our senses.On the other hand, Table 4 shows that the wordsselected on the basis of their number of stronglinks are not strongly linked in WordNet (accord-ing to Lin?s measure) to their target word.
Thisdoes not mean that the selected words are not in-teresting for describing the senses of the targetword but more probably that the semantic relationsthat they share with the target word are differentfrom hyperonymy.
The results of Pantel and Lincan be explained by the fact that their algorithm isbased on the clustering of similar words, i.e.
wordsthat are likely to be synonyms, and not on the clus-tering of the cooccurrents of a word, which are notoften synonyms of that word.
Moreover, their ini-tial corpus is much larger (around 144 millionswords) than ours and they make use of moreelaborated tools, such as a syntactic analyzer.LAT-1.no LAT-2.nonumber of strong links 19.4 20.8optimal choice 56.2 63.7Table 4: Average precision of discovered sensesfor English in relation with WordNetAs expected, the results obtained with first ordercooccurrences (LAT-1.no), which produce ahigher number of senses by word, are lower thanthe results obtained with second order cooccur-rences (LAT-2.no).
However, without a recallmeasure, it is difficult to draw a clear conclusionfrom this observation: some senses of LAT-1.noprobably result from the artificial division of anactual word sense but the fact to have more homo-geneous senses in LAT-2.no also facilitates in thiscase the mapping with WordNet?s synsets.7 Related workAs they rely on the detection of high-density areasin a network of cooccurrences, (V?ronis, 2003)and (Dorow and Widdows, 2003) are the closestmethods to ours.
Nevertheless, two main differ-ences can be noted with our work.
The first oneconcerns the direct use they make of the networkof cooccurrences.
In our case, we chose a moregeneral approach by working at the level of a simi-larity graph: when the similarity of two words isgiven by their relation of cooccurrence, our situa-tion is comparable to the one of (V?ronis, 2003)and (Dorow and Widdows, 2003); but in the sameframework, we can also take into account otherkinds of similarity relations, such as the secondorder cooccurrences.The second main difference is the fact they dis-criminate senses in an iterative way.
This approachconsists in selecting at each step the most obvioussense and then, to update the graph of cooccur-rences by discarding the words that make up thenew sense.
The other senses are then easier to dis-criminate.
We preferred to put emphasis on theability to gather close or identical senses that areartificially distinguished (see Section 4.3).
From aglobal viewpoint, these two differences lead(V?ronis, 2003) and (Dorow and Widdows, 2003)to build finer senses than ours.
Nevertheless, asmethods for discovering word senses from a cor-pus tend to find a too large number of close senses,it was more important from our viewpoint to fa-vour the building of stable senses with a cleardefinition rather than to discriminate very finesenses.8 Conclusion and future workIn this article, we have presented a new method fordiscriminating and defining the senses of a wordfrom a network of lexical cooccurrences.
Thismethod consists in applying an unsupervised clus-tering algorithm, in this case the SNN algorithm,to the cooccurrents of the word by relying on therelations that these cooccurrents have in the cooc-currence network.
We have achieved a firstevaluation based on the methodology defined in(Pantel and Lin, 2002).
This evaluation has shownthat in comparison with WordNet taken as a refer-ence, the relevance of the discriminated senses iscomparable to the relevance of Pantel and Lin?sword senses.
But it has also shown that thesimilarity between a discovered sense and a synsetlarity between a discovered sense and a synset ofWordNet must be evaluated in our case by takinginto account a larger set of semantic relations, es-pecially those implicitly present in the glosses.Moreover, an evaluation based on the use of thebuilt senses in an application such as query expan-sion is necessary to determine the actual interest ofthis kind of resources in comparison with a lexico-semantic network such as WordNet.ReferencesK.W.
Church and P. Hanks.
1990.
Word AssociationNorms, Mutual Information, And Lexicography.Computational Linguistics, 16(1): 22?29.Dorow B. and D. Widdows.
2003.
Discovering Corpus-Specific Word Senses.
In EACL 2003.L.
Ert?z, M. Steinbach and V. Kumar.
2001.
FindingTopics in Collections of Documents: A Shared Near-est Neighbor Approach.
In Text Mine?01, Workshopof the 1st SIAM International Conference on DataMining.S.
Harabagiu and S. Maiorano.
2002.
Multi-DocumentSummarization with GISTEXTER.
In LREC 2002.S Harabagiu, G.A.
Miller and D. Moldovan.
1999.WordNet 2 - A Morphologically and SemanticallyEnhanced Resource.
In SIGLEX?99.M.
Pasca and S. Harabagiu.
2001.
The informative roleof WordNet in Open-Domain Question Answering.In NAACL 2001 Workshop on WordNet and OtherLexical Resources.P.
Pantel and D. Lin.
2002.
Discovering Word Sensesfrom Text.
In ACM SIGKDD Conference on Knowl-edge Discovery and Data Mining 2002.S.
Patwardhan and T. Pedersen.
2003.
Word-Net::Similarity, http://www.d.umn.edu/~tpederse/ si-milarity.html.T.
Pedersen and R. Bruce.
1997.
Distinguishing WordSenses in Untagged Text.
In EMNLP'97.A.
Purandare.
2003.
Discriminating Among WordSenses Using Mcquitty's Similarity Analysis.
InHLT-NAACL 03 - Student Research Workshop.R.
Rapp.
2003.
Word Sense Discovery Based on SenseDescriptor Dissimilarity.
In Machine TranslationSummit IX.H.
Sch?tze.
1998.
Automatic Word Sense Discrimina-tion.
Computational Linguistics, 24(1): 97-123.J.
V?ronis.
2003.
Cartographie lexicale pour la recher-che d?information.
In TALN 2003.E.M.
Voorhees.
1998.
Using WordNet for text retrieval,In ?WordNet: An Electronic Lexical Database?,Cambridge, MA, MIT Press, pages 285-303.
