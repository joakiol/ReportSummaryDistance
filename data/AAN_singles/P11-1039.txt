Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 379?388,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsTopical Keyphrase Extraction from TwitterWayne Xin Zhao?
Jing Jiang?
Jing He?
Yang Song?
Palakorn Achananuparp?Ee-Peng Lim?
Xiaoming Li?
?School of Electronics Engineering and Computer Science, Peking University?School of Information Systems, Singapore Management University{batmanfly,peaceful.he,songyangmagic}@gmail.com,{jingjiang,eplim,palakorna}@smu.edu.sg, lxm@pku.edu.cnAbstractSummarizing and analyzing Twitter content isan important and challenging task.
In this pa-per, we propose to extract topical keyphrasesas one way to summarize Twitter.
We proposea context-sensitive topical PageRank methodfor keyword ranking and a probabilistic scor-ing function that considers both relevance andinterestingness of keyphrases for keyphraseranking.
We evaluate our proposed methodson a large Twitter data set.
Experiments showthat these methods are very effective for topi-cal keyphrase extraction.1 IntroductionTwitter, a new microblogging website, has attractedhundreds of millions of users who publish shortmessages (a.k.a.
tweets) on it.
They either pub-lish original tweets or retweet (i.e.
forward) oth-ers?
tweets if they find them interesting.
Twitterhas been shown to be useful in a number of appli-cations, including tweets as social sensors of real-time events (Sakaki et al, 2010), the sentiment pre-diction power of Twitter (Tumasjan et al, 2010),etc.
However, current explorations are still in anearly stage and our understanding of Twitter contentstill remains limited.
How to automatically under-stand, extract and summarize useful Twitter contenthas therefore become an important and emergent re-search topic.In this paper, we propose to extract keyphrasesas a way to summarize Twitter content.
Tradition-ally, keyphrases are defined as a short list of terms tosummarize the topics of a document (Turney, 2000).It can be used for various tasks such as documentsummarization (Litvak and Last, 2008) and index-ing (Li et al, 2004).
While it appears natural to usekeyphrases to summarize Twitter content, comparedwith traditional text collections, keyphrase extrac-tion from Twitter is more challenging in at least twoaspects: 1) Tweets are much shorter than traditionalarticles and not all tweets contain useful informa-tion; 2) Topics tend to be more diverse in Twitterthan in formal articles such as news reports.So far there is little work on keyword or keyphraseextraction from Twitter.
Wu et al (2010) proposedto automatically generate personalized tags for Twit-ter users.
However, user-level tags may not be suit-able to summarize the overall Twitter content withina certain period and/or from a certain group of peo-ple such as people in the same region.
Existing workon keyphrase extraction identifies keyphrases fromeither individual documents or an entire text collec-tion (Turney, 2000; Tomokiyo and Hurst, 2003).These approaches are not immediately applicableto Twitter because it does not make sense to ex-tract keyphrases from a single tweet, and if we ex-tract keyphrases from a whole tweet collection wewill mix a diverse range of topics together, whichmakes it difficult for users to follow the extractedkeyphrases.Therefore, in this paper, we propose to study thenovel problem of extracting topical keyphrases forsummarizing and analyzing Twitter content.
In otherwords, we extract and organize keyphrases by top-ics learnt from Twitter.
In our work, we follow thestandard three steps of keyphrase extraction, namely,keyword ranking, candidate keyphrase generation379and keyphrase ranking.
For keyword ranking, wemodify the Topical PageRank method proposed byLiu et al (2010) by introducing topic-sensitive scorepropagation.
We find that topic-sensitive propaga-tion can largely help boost the performance.
Forkeyphrase ranking, we propose a principled proba-bilistic phrase ranking method, which can be flex-ibly combined with any keyword ranking methodand candidate keyphrase generation method.
Ex-periments on a large Twitter data set show thatour proposed methods are very effective in topicalkeyphrase extraction from Twitter.
Interestingly, ourproposed keyphrase ranking method can incorporateusers?
interests by modeling the retweet behavior.We further examine what topics are suitable for in-corporating users?
interests for topical keyphrase ex-traction.To the best of our knowledge, our work is thefirst to study how to extract keyphrases from mi-croblogs.
We perform a thorough analysis of theproposed methods, which can be useful for futurework in this direction.2 Related WorkOur work is related to unsupervised keyphrase ex-traction.
Graph-based ranking methods are thestate of the art in unsupervised keyphrase extrac-tion.
Mihalcea and Tarau (2004) proposed to useTextRank, a modified PageRank algorithm to ex-tract keyphrases.
Based on the study by Mihalceaand Tarau (2004), Liu et al (2010) proposed to de-compose a traditional random walk into multiplerandom walks specific to various topics.
Languagemodeling methods (Tomokiyo and Hurst, 2003) andnatural language processing techniques (Barker andCornacchia, 2000) have also been used for unsuper-vised keyphrase extraction.
Our keyword extractionmethod is mainly based on the study by Liu et al(2010).
The difference is that we model the scorepropagation with topic context, which can lower theeffect of noise, especially in microblogs.Our work is also related to automatic topic label-ing (Mei et al, 2007).
We focus on extracting topicalkeyphrases in microblogs, which has its own chal-lenges.
Our method can also be used to label topicsin other text collections.Another line of relevant research is Twitter-related text mining.
The most relevant work isby Wu et al (2010), who directly applied Tex-tRank (Mihalcea and Tarau, 2004) to extract key-words from tweets to tag users.
Topic discoveryfrom Twitter is also related to our work (Ramage etal., 2010), but we further extract keyphrases fromeach topic for summarizing and analyzing Twittercontent.3 Method3.1 PreliminariesLet U be a set of Twitter users.
Let C ={{du,m}Mum=1}u?U be a collection of tweets gener-ated by U , where Mu is the total number of tweetsgenerated by user u and du,m is the m-th tweet ofu.
Let V be the vocabulary.
du,m consists of asequence of words (wu,m,1, wu,m,2, .
.
.
, wu,m,Nu,m)where Nu,m is the number of words in du,m andwu,m,n ?
V (1 ?
n ?
Nu,m).
We also assumethat there is a set of topics T over the collection C.Given T and C, topical keyphrase extraction is todiscover a list of keyphrases for each topic t ?
T .Here each keyphrase is a sequence of words.To extract keyphrases, we first identify topicsfrom the Twitter collection using topic models (Sec-tion 3.2).
Next for each topic, we run a topicalPageRank algorithm to rank keywords and then gen-erate candidate keyphrases using the top ranked key-words (Section 3.3).
Finally, we use a probabilis-tic model to rank the candidate keyphrases (Sec-tion 3.4).3.2 Topic discoveryWe first describe how we discover the set of topicsT .
Author-topic models have been shown to be ef-fective for topic modeling of microblogs (Weng etal., 2010; Hong and Davison, 2010).
In Twit-ter, we observe an important characteristic of tweets:tweets are short and a single tweet tends to be abouta single topic.
So we apply a modified author-topicmodel called Twitter-LDA introduced by Zhao et al(2011), which assumes a single topic assignment foran entire tweet.The model is based on the following assumptions.There is a set of topics T in Twitter, each representedby a word distribution.
Each user has her topic inter-ests modeled by a distribution over the topics.
Whena user wants to write a tweet, she first chooses a topicbased on her topic distribution.
Then she chooses a3801.
Draw ?B ?
Dir(?
), pi ?
Dir(?)2.
For each topic t ?
T ,(a) draw ?t ?
Dir(?)3.
For each user u ?
U ,(a) draw ?u ?
Dir(?
)(b) for each tweet du,mi.
draw zu,m ?
Multi(?u)ii.
for each word wu,m,nA.
draw yu,m,n ?
Bernoulli(pi)B. draw wu,m,n ?
Multi(?B) ifyu,m,n = 0 and wu,m,n ?Multi(?zu,m) if yu,m,n = 1Figure 1: The generation process of tweets.bag of words one by one based on the chosen topic.However, not all words in a tweet are closely re-lated to the topic of that tweet; some are backgroundwords commonly used in tweets on different topics.Therefore, for each word in a tweet, the user firstdecides whether it is a background word or a topicword and then chooses the word from its respectiveword distribution.Formally, let ?t denote the word distribution fortopic t and ?B the word distribution for backgroundwords.
Let ?u denote the topic distribution of useru.
Let pi denote a Bernoulli distribution that gov-erns the choice between background words and topicwords.
The generation process of tweets is describedin Figure 1.
Each multinomial distribution is gov-erned by some symmetric Dirichlet distribution pa-rameterized by ?, ?
or ?.3.3 Topical PageRank for Keyword RankingTopical PageRank was introduced by Liu et al(2010) to identify keywords for future keyphraseextraction.
It runs topic-biased PageRank for eachtopic separately and boosts those words with highrelevance to the corresponding topic.
Formally, thetopic-specific PageRank scores can be defined asfollows:Rt(wi) = ?
?j:wj?wie(wj , wi)O(wj)Rt(wj)+ (1??
)Pt(wi),(1)where Rt(w) is the topic-specific PageRank scoreof word w in topic t, e(wj , wi) is the weight for theedge (wj ?
wi), O(wj) =?w?
e(wj , w?)
and ?is a damping factor ranging from 0 to 1.
The topic-specific preference value Pt(w) for each word w isits random jumping probability with the constraintthat?w?V Pt(w) = 1 given topic t. A large Rt(?
)indicates a word is a good candidate keyword intopic t. We denote this original version of the Topi-cal PageRank as TPR.However, the original TPR ignores the topic con-text when setting the edge weights; the edge weightis set by counting the number of co-occurrences ofthe two words within a certain window size.
Tak-ing the topic of ?electronic products?
as an exam-ple, the word ?juice?
may co-occur frequently with agood keyword ?apple?
for this topic because of Ap-ple electronic products, so ?juice?
may be rankedhigh by this context-free co-occurrence edge weightalthough it is not related to electronic products.
Inother words, context-free propagation may cause thescores to be off-topic.So in this paper, we propose to use a topic contextsensitive PageRank method.
Formally, we haveRt(wi) = ?
?j:wj?wiet(wj , wi)Ot(wj)Rt(wj)+(1??)Pt(wi).
(2)Here we compute the propagation from wj to wi inthe context of topic t, namely, the edge weight fromwj to wi is parameterized by t. In this paper, wecompute edge weight et(wj , wi) between two wordsby counting the number of co-occurrences of thesetwo words in tweets assigned to topic t. We denotethis context-sensitive topical PageRank as cTPR.After keyword ranking using cTPR or any othermethod, we adopt a common candidate keyphrasegeneration method proposed by Mihalcea and Tarau(2004) as follows.
We first select the top S keywordsfor each topic, and then look for combinations ofthese keywords that occur as frequent phrases in thetext collection.
More details are given in Section 4.3.4 Probabilistic Models for Topical KeyphraseRankingWith the candidate keyphrases, our next step is torank them.
While a standard method is to simplyaggregate the scores of keywords inside a candidatekeyphrase as the score for the keyphrase, here wepropose a different probabilistic scoring function.Our method is based on the following hypothesesabout good keyphrases given a topic:381Figure 2: Assumptions of variable dependencies.Relevance: A good keyphrase should be closely re-lated to the given topic and also discriminative.
Forexample, for the topic ?news,?
?president obama?
isa good keyphrase while ?math class?
is not.Interestingness: A good keyphrase should be inter-esting and can attract users?
attention.
For example,for the topic ?music,?
?justin bieber?
is more inter-esting than ?song player.
?Sometimes, there is a trade-off between these twoproperties and a good keyphrase has to balance both.Let R be a binary variable to denote relevancewhere 1 is relevant and 0 is irrelevant.
Let I be an-other binary variable to denote interestingness where1 is interesting and 0 is non-interesting.
Let k denotea candidate keyphrase.
Following the probabilisticrelevance models in information retrieval (Laffertyand Zhai, 2003), we propose to use P (R = 1, I =1|t, k) to rank candidate keyphrases for topic t. WehaveP (R = 1, I = 1|t, k)= P (R = 1|t, k)P (I = 1|t, k, R = 1)= P (I = 1|t, k, R = 1)P (R = 1|t, k)= P (I = 1|k)P (R = 1|t, k)= P (I = 1|k)?P (R = 1|t, k)P (R = 1|t, k) + P (R = 0|t, k)= P (I = 1|k)?11 + P (R=0|t,k)P (R=1|t,k)= P (I = 1|k)?11 + P (R=0,k|t)P (R=1,k|t)= P (I = 1|k)?11 + P (R=0|t)P (R=1|t) ?P (k|t,R=0)P (k|t,R=1)= P (I = 1|k)?11 + P (R=0)P (R=1) ?P (k|t,R=0)P (k|t,R=1).Here we have assumed that I is independent of t andR given k, i.e.
the interestingness of a keyphrase isindependent of the topic or whether the keyphrase isrelevant to the topic.
We have also assumed that Ris independent of t when k is unknown, i.e.
withoutknowing the keyphrase, the relevance is independentof the topic.
Our assumptions can be depicted byFigure 2.We further define ?
= P (R=0)P (R=1) .
In general wecan assume that P (R = 0)  P (R = 1) becausethere are much more non-relevant keyphrases thanrelevant ones, that is, ?
1.
In this case, we havelogP (R = 1, I = 1|t, k) (3)= log(P (I = 1|k)?11 + ?
?
P (k|t,R=0)P (k|t,R=1))?
log(P (I = 1|k)?P (k|t, R = 1)P (k|t, R = 0)?1?
)= logP (I = 1|k) + logP (k|t, R = 1)P (k|t, R = 0)?
log ?.We can see that the ranking score logP (R = 1, I =1|t, k) can be decomposed into two components, arelevance score log P (k|t,R=1)P (k|t,R=0) and an interestingnessscore logP (I = 1|k).
The last term log ?
is a con-stant and thus not relevant.Estimating the relevance scoreLet a keyphrase candidate k be a sequence ofwords (w1, w2, .
.
.
, wN ).
Based on an independentassumption of words given R and t, we havelogP (k|t, R = 1)P (k|t, R = 0)= logP (w1w2 .
.
.
wN |t, R = 1)P (w1w2 .
.
.
wN |t, R = 0)=N?n=1logP (wn|t, R = 1)P (wn|t, R = 0).
(4)Given the topic model ?t previously learned fortopic t, we can set P (w|t, R = 1) to ?tw, i.e.
theprobability of w under ?t.
Following Griffiths andSteyvers (2004), we estimate ?tw as?tw =#(Ct, w) + ?#(Ct, ?)
+ ?|V|.
(5)Here Ct denotes the collection of tweets assigned totopic t, #(Ct, w) is the number of times w appears inCt, and #(Ct, ?)
is the total number of words in Ct.P (w|t, R = 0) can be estimated using a smoothedbackground model.P (w|R = 0, t) =#(C, w) + ?#(C, ?)
+ ?|V|.
(6)382Here #(C, ?)
denotes the number of words in thewhole collection C, and #(C, w) denotes the numberof times w appears in the whole collection.After plugging Equation (5) and Equation (6) intoEquation (4), we get the following formula for therelevance score:logP (k|t, R = 1)P (k|t, R = 0)=?w?k(log#(Ct, w) + ?#(C, w) + ?+ log#(C, ?)
+ ?|V|#(Ct, ?)
+ ?|V|)=(?w?klog#(Ct, w) + ?#(C, w) + ?
)+ |k|?, (7)where ?
= #(C,?)+?|V|#(Ct,?
)+?|V| and |k| denotes the numberof words in k.Estimating the interestingness scoreTo capture the interestingness of keyphrases, wemake use of the retweeting behavior in Twitter.
Weuse string matching with RT to determine whethera tweet is an original posting or a retweet.
If atweet is interesting, it tends to get retweeted mul-tiple times.
Retweeting is therefore a stronger indi-cator of user interests than tweeting.
We use retweetratio |ReTweetsk||Tweetsk| to estimate P (I = 1|k).
To preventzero frequency, we use a modified add-one smooth-ing method.
Finally, we getlogP (I = 1|k) = log|ReTweetsk|+ 1.0|Tweetsk|+ lavg.
(8)Here |ReTweetsk| and |Tweetsk| denote the num-bers of retweets and tweets containing the keyphrasek, respectively, and lavg is the average number oftweets that a candidate keyphrase appears in.Finally, we can plug Equation (7) and Equa-tion (8) into Equation (3) and obtain the followingscoring function for ranking:Scoret(k) = log|ReTweetsk|+ 1.0|Tweetsk|+ lavg(9)+(?w?klog#(Ct, w) + ?#(C, w) + ?
)+ |k|?.#user #tweet #term #token13,307 1,300,300 50,506 11,868,910Table 1: Some statistics of the data set.Incorporating length preferenceOur preliminary experiments with Equation (9)show that this scoring function usually ranks longerkeyphrases higher than shorter ones.
However, be-cause our candidate keyphrase are extracted withoutusing any linguistic knowledge such as noun phraseboundaries, longer candidate keyphrases tend to beless meaningful as a phrase.
Moreover, for our taskof using keyphrases to summarize Twitter, we hy-pothesize that shorter keyphrases are preferred byusers as they are more compact.
We would there-fore like to incorporate some length preference.Recall that Equation (9) is derived from P (R =1, I = 1|t, k), but this probability does not allowus to directly incorporate any length preference.
Wefurther observe that Equation (9) tends to give longerkeyphrases higher scores mainly due to the term|k|?.
So here we heuristically incorporate our lengthpreference by removing |k|?
from Equation (9), re-sulting in the following final scoring function:Scoret(k) = log|ReTweetsk|+ 1.0|Tweetsk|+ lavg(10)+(?w?klog#(Ct, w) + ?#(C, w) + ?
).4 Experiments4.1 Data Set and PreprocessingWe use a Twitter data set collected from Singaporeusers for evaluation.
We used Twitter REST API1to facilitate the data collection.
The majority of thetweets collected were published in a 20-week periodfrom December 1, 2009 through April 18, 2010.
Weremoved common stopwords and words which ap-peared in fewer than 10 tweets.
We also removed allusers who had fewer than 5 tweets.
Some statisticsof this data set after cleaning are shown in Table 1.We ran Twitter-LDA with 500 iterations of Gibbssampling.
After trying a few different numbers of1http://apiwiki.twitter.com/w/page/22554663/REST-API-Documentation383topics, we empirically set the number of topics to30.
We set ?
to 50.0/|T | as Griffiths and Steyvers(2004) suggested, but set ?
to a smaller value of 0.01and ?
to 20.
We chose these parameter settings be-cause they generally gave coherent and meaningfultopics for our data set.
We selected 10 topics thatcover a diverse range of content in Twitter for eval-uation of topical keyphrase extraction.
The top 10words of these topics are shown in Table 2.We also tried the standard LDA model and theauthor-topic model on our data set and found thatour proposed topic model was better or at least com-parable in terms of finding meaningful topics.
In ad-dition to generating meaningful topics, Twitter-LDAis much more convenient in supporting the compu-tation of tweet-level statistics (e.g.
the number ofco-occurrences of two words in a specific topic) thanthe standard LDA or the author-topic model becauseTwitter-LDA assumes a single topic assignment foran entire tweet.4.2 Methods for ComparisonAs we have described in Section 3.1, there are threesteps to generate keyphrases, namely, keyword rank-ing, candidate keyphrase generation, and keyphraseranking.
We have proposed a context-sensitive top-ical PageRank method (cTPR) for the first step ofkeyword ranking, and a probabilistic scoring func-tion for the third step of keyphrase ranking.
We nowdescribe the baseline methods we use to comparewith our proposed methods.Keyword RankingWe compare our cTPR method with the originaltopical PageRank method (Equation (1)), which rep-resents the state of the art.
We refer to this baselineas TPR.For both TPR and cTPR, the damping factor isempirically set to 0.1, which always gives the bestperformance based on our preliminary experiments.We use normalized P (t|w) to set Pt(w) because ourpreliminary experiments showed that this was thebest among the three choices discussed by Liu et al(2010).
This finding is also consistent with what Liuet al (2010) found.In addition, we also use two other baselines forcomparison: (1) kwBL1: ranking by P (w|t) = ?tw.
(2) kwBL2: ranking by P (t|w) = P (t)?tw?t?
P (t?
)?t?w.Keyphrase RankingWe use kpRelInt to denote our relevance and inter-estingness based keyphrase ranking function P (R =1, I = 1|t, k), i.e.
Equation (10).
?
and ?
are em-pirically set to 0.01 and 500.
Usually ?
can be set tozero, but in our experiments we find that our rank-ing method needs a more uniform estimation of thebackground model.
We use the following rankingfunctions for comparison:?
kpBL1: Similar to what is used by Liu et al(2010), we can rank candidate keyphrases by?w?k f(w), where f(w) is the score assignedto word w by a keyword ranking method.?
kpBL2: We consider another baseline rankingmethod by?w?k log f(w).?
kpRel: If we consider only relevance butnot interestingness, we can rank candidatekeyphrases by?w?k log#(Ct,w)+?#(C,w)+?
.4.3 Gold Standard GenerationSince there is no existing test collection for topi-cal keyphrase extraction from Twitter, we manuallyconstructed our test collection.
For each of the 10selected topics, we ran all the methods to rank key-words.
For each method we selected the top 3000keywords and searched all the combinations of thesewords as phrases which have a frequency larger than30.
In order to achieve high phraseness, we firstcomputed the minimum value of pointwise mutualinformation for all bigrams in one combination, andwe removed combinations having a value below athreshold, which was empirically set to 2.135.
Thenwe merged all these candidate phrases.
We did notconsider single-word phrases because we found thatit would include too many frequent words that mightnot be useful for summaries.We asked two judges to judge the quality of thecandidate keyphrases.
The judges live in Singaporeand had used Twitter before.
For each topic, thejudges were given the top topic words and a shorttopic description.
Web search was also available.For each candidate keyphrase, we asked the judgesto score it as follows: 2 (relevant, meaningful and in-formative), 1 (relevant but either too general or toospecific, or informal) and 0 (irrelevant or meaning-less).
Here in addition to relevance, the other twocriteria, namely, whether a phrase is meaningful andinformative, were studied by Tomokiyo and Hurst384T2 T4 T5 T10 T12 T13 T18 T20 T23 T25eat twitter love singapore singapore hot iphone song study winfood tweet idol road #singapore rain google video school gamedinner blog adam mrt #business weather social youtube time teamlunch facebook watch sgreinfo #news cold media love homework matcheating internet april east health morning ipad songs tomorrow playice tweets hot park asia sun twitter bieber maths chelseachicken follow lambert room market good free music class worldcream msn awesome sqft world night app justin paper unitedtea followers girl price prices raining apple feature math liverpoolhungry time american built bank air marketing twitter finish arsenalTable 2: Top 10 Words of Sample Topics on our Singapore Twitter Dateset.(2003).
We then averaged the scores of the twojudges as the final scores.
The Cohen?s Kappa co-efficients of the 10 topics range from 0.45 to 0.80,showing fair to good agreement2.
We further dis-carded all candidates with an average score less than1.
The number of the remaining keyphrases for eachtopic ranges from 56 to 282.4.4 Evaluation MetricsTraditionally keyphrase extraction is evaluated usingprecision and recall on all the extracted keyphrases.We choose not to use these measures for the fol-lowing reasons: (1) Traditional keyphrase extractionworks on single documents while we study topicalkeyphrase extraction.
The gold standard keyphraselist for a single document is usually short and clean,while for each Twitter topic there can be manykeyphrases, some are more relevant and interestingthan others.
(2) Our extracted topical keyphrases aremeant for summarizing Twitter content, and they arelikely to be directly shown to the users.
It is there-fore more meaningful to focus on the quality of thetop-ranked keyphrases.Inspired by the popular nDCG metric in informa-tion retrieval (Ja?rvelin and Keka?la?inen, 2002), wedefine the following normalized keyphrase qualitymeasure (nKQM) for a methodM:nKQM@K =1|T |?t?T?Kj=11log2(j+1)score(Mt,j)IdealScore(K,t),where T is the set of topics, Mt,j is the j-th keyphrase generated by method M for topic2We find that judgments on topics related to social me-dia (e.g.
T4) and daily life (e.g.
T13) tend to have a higherdegree of disagreement.t, score(?)
is the average score from the two hu-man judges, and IdealScore(K,t) is the normalizationfactor?score of the top K keyphrases of topic t un-der the ideal ranking.
Intuitively, ifM returns moregood keyphrases in top ranks, its nKQM value willbe higher.We also use mean average precision (MAP) tomeasure the overall performance of keyphrase rank-ing:MAP =1|T |?t?T1NM,t|Mt|?j=1NM,t,jj1(score(Mt,j) ?
1),where 1(S) is an indicator function which returns1 when S is true and 0 otherwise, NM,t,j denotesthe number of correct keyphrases among the top jkeyphrases returned byM for topic t, and NM,t de-notes the total number of correct keyphrases of topict returned byM.4.5 Experiment ResultsEvaluation of keyword ranking methodsSince keyword ranking is the first step forkeyphrase extraction, we first compare our keywordranking method cTPR with other methods.
For eachtopic, we pooled the top 20 keywords ranked by allfour methods.
We manually examined whether aword is a good keyword or a noisy word based ontopic context.
Then we computed the average num-ber of noisy words in the 10 topics for each method.As shown in Table 5, we can observe that cTPR per-formed the best among the four methods.Since our final goal is to extract topicalkeyphrases, we further compare the performanceof cTPR and TPR when they are combined with akeyphrase ranking algorithm.
Here we use the two385Method nKQM@5 nKQM@10 nKQM@25 nKQM@50 MAPkpBL1 TPR 0.5015 0.54331 0.5611 0.5715 0.5984kwBL1 0.6026 0.5683 0.5579 0.5254 0.5984kwBL2 0.5418 0.5652 0.6038 0.5896 0.6279cTPR 0.6109 0.6218 0.6139 0.6062 0.6608kpBL2 TPR 0.7294 0.7172 0.6921 0.6433 0.6379kwBL1 0.7111 0.6614 0.6306 0.5829 0.5416kwBL2 0.5418 0.5652 0.6038 0.5896 0.6545cTPR 0.7491 0.7429 0.6930 0.6519 0.6688Table 3: Comparisons of keyphrase extraction for cTPR and baselines.Method nKQM@5 nKQM@10 nKQM@25 nKQM@50 MAPcTPR+kpBL1 0.61095 0.62182 0.61389 0.60618 0.6608cTPR+kpBL2 0.74913 0.74294 0.69303 0.65194 0.6688cTPR+kpRel 0.75361 0.74926 0.69645 0.65065 0.6696cTPR+kpRelInt 0.81061 0.75184 0.71422 0.66319 0.6694Table 4: Comparisons of keyphrase extraction for different keyphrase ranking methods.kwBL1 kwBL2 TPR cTPR2 3 4.9 1.5Table 5: Average number of noisy words among the top20 keywords of the 10 topics.baseline keyphrase ranking algorithms kpBL1 andkpBL2.
The comparison is shown in Table 3.
Wecan see that cTPR is consistently better than the threeother methods for both kpBL1 and kpBL2.Evaluation of keyphrase ranking methodsIn this section we compare keypharse rankingmethods.
Previously we have shown that cTPR isbetter than TPR, kwBL1 and kwBL2 for keywordranking.
Therefore we use cTPR as the keywordranking method and examine the keyphrase rank-ing method kpRelInt with kpBL1, kpBL2 and kpRelwhen they are combined with cTPR.
The results areshown in Table 4.
From the results we can see thefollowing: (1) Keyphrase ranking methods kpRelIntand kpRel are more effective than kpBL1 and kpBL2,especially when using the nKQM metric.
(2) kpRe-lInt is better than kpRel, especially for the nKQMmetric.
Interestingly, we also see that for the nKQMmetric, kpBL1, which is the most commonly usedkeyphrase ranking method, did not perform as wellas kpBL2, a modified version of kpBL1.We also tested kpRelInt and kpRel on TPR, kwBL1and kwBL2 and found that kpRelInt and kpRel areconsistently better than kpBL2 and kpBL1.
Due tospace limit, we do not report all the results here.These findings support our assumption that our pro-posed keyphrase ranking method is effective.The comparison between kpBL2 with kpBL1shows that taking the product of keyword scores ismore effective than taking their sum.
kpRel andkpRelInt also use the product of keyword scores.This may be because there is more noise in Twit-ter than traditional documents.
Common words (e.g.?good?)
and domain background words (e.g.
?Sin-gapore?)
tend to gain higher weights during keywordranking due to their high frequency, especially ingraph-based method, but we do not want such wordsto contribute too much to keyphrase scores.
Takingthe product of keyword scores is therefore more suit-able here than taking their sum.Further analysis of interestingnessAs shown in Table 4, kpRelInt performs betterin terms of nKQM compared with kpRel.
Here westudy why it worked better for keyphrase ranking.The only difference between kpRel and kpRelInt isthat kpRelInt includes the factor of user interests.
Bymanually examining the top keyphrases, we find thatthe topics ?Movie-TV?
(T5), ?News?
(T12), ?Music?
(T20) and ?Sports?
(T25) particularly benefited fromkpRelInt compared with other topics.
We find thatwell-known named entities (e.g.
celebrities, politi-cal leaders, football clubs and big companies) andsignificant events tend to be ranked higher by kpRe-lInt than kpRel.We then counted the numbers of entity and eventkeyphrases for these four topics retrieved by differ-ent methods, shown in Table 6 .
We can see thatin these four topics, kpRelInt is consistently betterthan kpRel in terms of the number of entity and eventkeyphrases retrieved.386T2 T5 T10 T12 T20 T25chicken rice adam lambert north east president obama justin bieber manchester unitedice cream jack neo rent blk magnitude earthquake music video champions leaguefried chicken american idol east coast volcanic ash lady gaga football matchcurry rice david archuleta east plaza prime minister taylor swift premier leaguechicken porridge robert pattinson west coast iceland volcano demi lovato f1 grand prixcurry chicken alexander mcqueen bukit timah chile earthquake youtube channel tiger woodsbeef noodles april fools street view goldman sachs miley cyrus grand slam(tennis)chocolate cake harry potter orchard road coe prices telephone video liverpool fanscheese fries april fool toa payoh haiti earthquake song lyrics final scoreinstant noodles andrew garcia marina bay #singapore #business joe jonas manchester derbyTable 7: Top 10 keyphrases of 6 topics from cTPR+kpRelInt.Methods T5 T12 T20 T25cTPR+kpRel 8 9 16 11cTPR+kpRelInt 10 12 17 14Table 6: Numbers of entity and event keyphrases re-trieved by different methods within top 20.On the other hand, we also find that for sometopics interestingness helped little or even hurt theperformance a little, e.g.
for the topics ?Food?
and?Traffic.?
We find that the keyphrases in these top-ics are stable and change less over time.
This maysuggest that we can modify our formula to handledifferent topics different.
We will explore this direc-tion in our future work.Parameter settingsWe also examine how the parameters in our modelaffect the performance.?
: We performed a search from 0.1 to 0.9 with astep size of 0.1.
We found ?
= 0.1 was the optimalparameter for cTPR and TPR.
However, TPR is moresensitive to ?.
The performance went down quicklywith ?
increasing.?
: We checked the overall performance with?
?
{400, 450, 500, 550, 600}.
We found that ?
=500 ?
0.01|V| gave the best performance gener-ally for cTPR.
The performance difference is notvery significant between these different values of ?,which indicates that the our method is robust.4.6 Qualitative evaluation of cTPR+kpRelIntWe show the top 10 keyphrases discovered bycTPR+kRelInt in Table 7.
We can observe that thesekeyphrases are clear, interesting and informative forsummarizing Twitter topics.We hypothesize that the following applicationscan benefit from the extracted keyphrases:Automatic generation of realtime trendy phrases:For exampoe, keyphrases in the topic ?Food?
(T2)can be used to help online restaurant reviews.Event detection and topic tracking: In the topic?News?
top keyphrases can be used as candidatetrendy topics for event detection and topic tracking.Automatic discovery of important named entities:As discussed previously, our methods tend to rankimportant named entities such as celebrities in highranks.5 ConclusionIn this paper, we studied the novel problem of topicalkeyphrase extraction for summarizing and analyzingTwitter content.
We proposed the context-sensitivetopical PageRank (cTPR) method for keyword rank-ing.
Experiments showed that cTPR is consistentlybetter than the original TPR and other baseline meth-ods in terms of top keyword and keyphrase extrac-tion.
For keyphrase ranking, we proposed a prob-abilistic ranking method, which models both rele-vance and interestingness of keyphrases.
In our ex-periments, this method is shown to be very effec-tive to boost the performance of keyphrase extrac-tion for different kinds of keyword ranking methods.In the future, we may consider how to incorporatekeyword scores into our keyphrase ranking method.Note that we propose to rank keyphrases by a gen-eral formula P (R = 1, I = 1|t, k) and we have madesome approximations based on reasonable assump-tions.
There should be other potential ways to esti-mate P (R = 1, I = 1|t, k).AcknowledgementsThis work was done during Xin Zhao?s visit to theSingapore Management University.
Xin Zhao andXiaoming Li are partially supported by NSFC under387the grant No.
60933004, 61073082, 61050009 andHGJ Grant No.
2011ZX01042-001-001.ReferencesKen Barker and Nadia Cornacchia.
2000.
Using nounphrase heads to extract document keyphrases.
In Pro-ceedings of the 13th Biennial Conference of the Cana-dian Society on Computational Studies of Intelligence:Advances in Artificial Intelligence, pages 40?52.Thomas L. Griffiths and Mark Steyvers.
2004.
Find-ing scientific topics.
Proceedings of the NationalAcademy of Sciences of the United States of America,101(Suppl.
1):5228?5235.Liangjie Hong and Brian D. Davison.
2010.
Empiricalstudy of topic modeling in Twitter.
In Proceedings ofthe First Workshop on Social Media Analytics.Kalervo Ja?rvelin and Jaana Keka?la?inen.
2002.
Cumu-lated gain-based evaluation of ir techniques.
ACMTransactions on Information Systems, 20(4):422?446.John Lafferty and Chengxiang Zhai.
2003.
Probabilisticrelevance models based on document and query gener-ation.
Language Modeling and Information Retrieval,13.Quanzhi Li, Yi-Fang Wu, Razvan Bot, and Xin Chen.2004.
Incorporating document keyphrases in searchresults.
In Proceedings of the 10th Americas Confer-ence on Information Systems.Marina Litvak and Mark Last.
2008.
Graph-based key-word extraction for single-document summarization.In Proceedings of the Workshop on Multi-source Mul-tilingual Information Extraction and Summarization,pages 17?24.Zhiyuan Liu, Wenyi Huang, Yabin Zheng, and MaosongSun.
2010.
Automatic keyphrase extraction via topicdecomposition.
In Proceedings of the 2010 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 366?376.Qiaozhu Mei, Xuehua Shen, and ChengXiang Zhai.2007.
Automatic labeling of multinomial topic mod-els.
In Proceedings of the 13th ACM SIGKDD Interna-tional Conference on Knowledge Discovery and DataMining, pages 490?499.R.
Mihalcea and P. Tarau.
2004.
TextRank: Bringing or-der into texts.
In Proceedings of the 2004 Conferenceon Empirical Methods in Natural Language Process-ing.Daniel Ramage, Susan Dumais, and Dan Liebling.
2010.Characterizing micorblogs with topic models.
In Pro-ceedings of the 4th International Conference on We-blogs and Social Media.Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.2010.
Earthquake shakes Twitter users: real-timeevent detection by social sensors.
In Proceedings ofthe 19th International World Wide Web Conference.Takashi Tomokiyo and Matthew Hurst.
2003.
A lan-guage model approach to keyphrase extraction.
InProceedings of the ACL 2003 Workshop on Multi-word Expressions: Analysis, Acquisition and Treat-ment, pages 33?40.Andranik Tumasjan, Timm O. Sprenger, Philipp G. Sand-ner, and Isabell M. Welpe.
2010.
Predicting electionswith Twitter: What 140 characters reveal about politi-cal sentiment.
In Proceedings of the 4th InternationalConference on Weblogs and Social Media.Peter Turney.
2000.
Learning algorithms for keyphraseextraction.
Information Retrieval, (4):303?336.Jianshu Weng, Ee-Peng Lim, Jing Jiang, and Qi He.2010.
TwitterRank: finding topic-sensitive influentialtwitterers.
In Proceedings of the third ACM Interna-tional Conference on Web Search and Data Mining.Wei Wu, Bin Zhang, and Mari Ostendorf.
2010.
Au-tomatic generation of personalized annotation tags fortwitter users.
In Human Language Technologies: The2010 Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 689?692.Xin Zhao, Jing Jiang, Jianshu Weng, Jing He, Lim Ee-Peng, Hongfei Yan, and Xiaoming Li.
2011.
Compar-ing Twitter and traditional media using topic models.In Proceedings of the 33rd European Conference onInformation Retrieval.388
