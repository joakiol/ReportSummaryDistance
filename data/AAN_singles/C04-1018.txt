Playing the Telephone Game: Determining the Hierarchical Structure ofPerspective and Speech ExpressionsEric Breck and Claire CardieDepartment of Computer ScienceCornell UniversityIthaca, NY 14853USAebreck,cardie@cs.cornell.eduAbstractNews articles report on facts, events, and opin-ions with the intent of conveying the truth.However, the facts, events, and opinions appear-ing in the text are often known only second-or third-hand, and as any child who has played?telephone?
knows, this relaying of facts oftengarbles the original message.
Properly under-standing the information filtering structures thatgovern the interpretation of these facts, then, iscritical to appropriately analyzing them.
In thiswork, we present a learning approach that cor-rectly determines the hierarchical structure ofinformation filtering expressions 78.30% of thetime.1 IntroductionNewswire text has long been a primary target fornatural language processing (NLP) techniques suchas information extraction, summarization, and ques-tion answering (e.g.
MUC (1998); NIS (2003);DUC (2003)).
However, newswire does not offerdirect access to facts, events, and opinions; rather,journalists report what they have experienced, andreport on the experiences of others.
That is, facts,events, and opinions are filtered by the point ofview of the writer and other sources.
Unfortu-nately, this filtering of information through multiplesources (and multiple points of view) complicatesthe natural language interpretation process becausethe reader (human or machine) must take into ac-count the biases introduced by this indirection.
Itis important for understanding both newswire andnarrative text (Wiebe, 1994), therefore, to appropri-ately recognize expressions of point of view, and toassociate them with their direct and indirect sources.This paper introduces two kinds of expressionthat can filter information.
First, we define a per-spective expression to be the minimal span of textthat denotes the presence of an explicit opinion,evaluation, emotion, speculation, belief, sentiment,etc.1 Private state is the general term typically used1Note that implicit expressions of perspective, i.e.
Wiebe etto refer to these mental and emotional states thatcannot be directly observed or verified (Quirk et al,1985).
Further, we define the source of a perspec-tive expression to be the experiencer of that privatestate, that is, the person or entity whose opinionor emotion is being conveyed in the text.
Second,speech expressions simply convey the words of an-other individual ?
and by the choice of words, thereporter filters the original source?s intent.
Considerfor example, the following sentences (in which per-spective expressions are denoted in bold, speech ex-pressions are underlined, and sources are denoted initalics):1.
Charlie was angry at Alice?s claim that Bob wasunhappy.2.
Philip Clapp, president of the National Environ-ment Trust, sums up well the general thrust of thereaction of environmental movements: ?There is noreason at all to believe that the polluters are sud-denly going to become reasonable.
?Perspective expressions in Sentence 1 describe theemotions or opinion of three sources: Charlie?sanger, Bob?s unhappiness, and Alice?s belief.
Per-spective expressions in Sentence 2, on the otherhand, introduce the explicit opinion of one source,i.e.
the reaction of the environmental movements.Speech expressions also perform filtering in theseexamples.
The reaction of the environmental move-ments is filtered by Clapp?s summarization, which,in turn, is filtered by the writer?s choice of quotation.In addition, the fact that Bob was unhappy is filteredthrough Alice?s claim, which, in turn, is filtered bythe writer?s choice of words for the sentence.
Sim-ilarly, it is only according to the writer that Charlieis angry.The specific goal of the research described hereis to accurately identify the hierarchical structure ofperspective and speech expressions (pse?s) in text.2al.
?s (2003) ?expressive subjective elements?
are not the subjectof study here.2For the rest of this paper, then, we ignore the distinctionbetween perspective and speech expressions, so in future ex-Given sentences 1 and 2 and their pse?s, for exam-ple, we will present methods that produce the struc-tures shown in Figure 1, which represent the multi-stage information filtering that should be taken intoaccount in the interpretation of the text.Sentence 1:writer?s implicit speech eventclaimunhappyangrySentence 2:writer?s implicit speech eventsums upreactionFigure 1: Hierarchical structure of the perspective andspeech expressions in sentences 1 and 2We propose a supervised machine learning ap-proach to the problem that relies on a small setof syntactically-based features.
More specifically,the method first trains a binary classifier to makepairwise parent-child decisions among the pse?s inthe same sentence, and then combines the deci-sions to determine their global hierarchical struc-ture.
We compare the approach to two heuristic-based baselines ?
one that simply assumes that ev-ery pse is filtered only through the writer, and asecond that is based on syntactic dominance rela-tions in the associated parse tree.
In an evaluationusing the opinion-annotated NRRC corpus (Wiebeet al, 2002), the learning-based approach achievesan accuracy of 78.30%, significantly higher thanboth the simple baseline approach (65.57%) and theparse-based baseline (71.64%).
We believe that thisstudy provides a first step towards understanding themulti-stage filtering process that can bias and garblethe information present in newswire text.The rest of the paper is organized as follows.
Wepresent related work in Section 2 and describe themachine learning approach in Section 3.
The ex-perimental methodology and results are presentedin Sections 4 and 5, respectively.
Section 6 summa-rizes our conclusions and plans for future work.2 The Larger Problem and Related WorkThis paper addresses the problem of identifying thehierarchical structure of perspective and speech ex-pressions.
We view this as a necessary and im-portant component of a larger perspective-analysisamples, both types of pse appear in boldface.
Note that theacronym ?pse?
has been used previously with a different mean-ing (Wiebe, 1994).pse class countwriter 9808verb 7623noun 2293no parse 278adjective 197adverb 50other 370Table 1: Breakdown of classes of pse?s.
?writer?
de-notes pse?s with the writer as source.
?No parse?
denotespse?s in sentences where the parse failed, and so the partof speech could not be determined.number of pse?s number of sentences1 36122 32563 18104 7785 239>5 113Table 2: Breakdown of number of pse?s per sentencesystem.
Such a system would be able to identifyall pse?s in a document, as well as identify theirstructure.
The system would also identify the directsource of each pse.
Finally, the system would iden-tify the text corresponding to the content of a privatestate or the speech expressed by a pse.3 Such a sys-tem might analyze sentence 2 as follows:(source: writerpse: (implicit speech event)content: Philip ...
reasonable.?
)(source: clapppse: sums upcontent: ?There ...
reasonable.?
)(source: environmental movementspse: reactioncontent: (no text))As far as we are aware, no single system ex-ists that simultaneously solves all these problems.There is, however, quite a bit of work that addressesvarious pieces of this larger task, which we will nowsurvey.Gerard (2000) proposes a computational modelof the reader of a news article.
Her model providesfor multiple levels of hierarchical beliefs, such asthe nesting of a primary source?s belief within thatof a reporter.
However, Gerard does not provide al-gorithms for extracting this structure directly fromnewswire texts.Bethard et al (2004) seek to extract propositional3In (Wiebe, 2002), this is referred to as the inside.opinions and their holders.
They define an opinionas ?a sentence, or part of a sentence that would an-swer the question ?How does X feel about Y??
?
Apropositional opinion is an opinion ?localized in thepropositional argument?
of certain verbs, such as?believe?
or ?realize?.
Their task then correspondsto identifying a pse, its associated direct source, andthe content of the private state.
However, they con-sider as pse?s only verbs, and further restrict atten-tion to verbs with a propositional argument, whichis a subset of the perspective and speech expressionsthat we consider here.
Table 1, for example, showsthe diversity of word classes that correspond to pse?sin our corpus.
Perhaps more importantly for thepurposes of this paper, their work does not addressinformation filtering issues, i.e.
problems that arisewhen an opinion has been filtered through multiplesources.
Namely, Bethard et al (2004) do not con-sider sentences that contain multiple pse?s, and donot, therefore, need to identify any indirect sourcesof opinions.
As shown in Table 2, however, wefind that sentences with multiple non-writer pse?s(i.e.
sentences that contain 3 or more total pse?s)comprise a significant portion (29.98%) of our cor-pus.
An advantage over our work, however, is thatBethard et al (2004) do not require separate solu-tions to pse identification and the identification oftheir direct sources.Automatic identification of sources has alsobeen addressed indirectly by Gildea and Jurafsky?s(2002) work on semantic role identification in thatfinding sources often corresponds to finding thefiller of the agent role for verbs.
Their methods thenmight be used to identify sources and associate themwith pse?s that are verbs or portions of verb phrases.Whether their work will also apply to pse?s that arerealized as other parts of speech is an open question.Wiebe (1994), studies methods to track thechange of ?point of view?
in narrative text (fiction).That is, the ?writer?
of one sentence may not corre-spond to the writer of the next sentence.
Althoughthis is not as frequent in newswire text as in fiction,it will still need to be addressed in a solution to thelarger problem.Bergler (1993) examines the lexical semantics ofspeech event verbs in the context of generative lex-icon theory.
While not specifically addressing ourproblem, the ?semantic dimensions?
of reportingverbs that she extracts might be very useful as fea-tures in our approach.Finally, Wiebe et al (2003) present preliminaryresults for the automatic identification of perspec-tive and speech expressions using corpus-basedtechniques.
While the results are promising (66% F-wasCharlie angryatclaim?sAlicethatwasBob unhappyFigure 2: Dependency parse of sentence 1 according tothe Collins parser.measure), the problem is still clearly unsolved.
Asexplained below, we will instead rely on manuallytagged pse?s for the studies presented here.3 The ApproachOur task is to find the hierarchical structure amongthe pse?s in individual sentences.
One?s first im-pression might be that this structure should be ob-vious from the syntax: one pse should filter an-other roughly when it dominates the other in a de-pendency parse.
This heuristic, for example, wouldsucceed for ?claim?
and ?unhappy?
in sentence 1,whose pse structure is given in Figure 1 and parsestructure (as produced by the Collins parser) in Fig-ure 2.
4Even in sentence 1, though, we can see thatthe problem is more complex: ?angry?
dominates?claim?
in the parse tree, but does not filter it.
Un-fortunately, an analysis of the parse-based heuristicon our training data (the data set will be describedin Section 4), uncovered numerous, rather than justa few, sources of error.
Therefore, rather than tryingto handcraft a more complex collection of heuris-tics, we chose to adopt a supervised machine learn-ing approach that relies on features identified in thisanalysis.
In particular, we will first train a binaryclassifier to make pairwise decisions as to whethera given pse is the immediate parent of another.
Wethen use a simple approach to combine these de-cisions to find the hierarchical information-filteringstructure of all pse?s in a sentence.We assume that we have a training corpus of4For this heuristic and the features that follow, we will speakof the pse?s as if they had a position in the parse tree.
However,since pse?s are often multiple words, and do not necessarilyform a constituent, this is not entirely accurate.
The parse nodecorresponding to a pse will be the highest node in the depen-dency parse corresponding to a word in the pse.
We considerthe writer?s implicit pse to correspond to the root of the parse.sentences, annotated with pse?s and their hier-archical pse structure (Section 4 describes thecorpus).
Training instances for the binary clas-sifier are pairs of pse?s from the same sentence,?psetarget, pseparent?5.
We assign a class valueof 1 to a training instance if pseparent is theimmediate parent of psetarget in the manuallyannotated hierarchical structure for the sentence,and 0 otherwise.
For sentence 1, there are ninetraining instances generated: ?claim,writer?,?angry,writer?, ?unhappy, claim?
(class 1),?claim, angry?, ?claim, unhappy?, ?angry, claim?,?angry, unhappy?, ?unhappy,writer?,?unhappy, angry?
(class 0).
The features usedto describe each training instance are explainedbelow.During testing, we construct the hierarchical psestructure of an entire sentence as follows.
For eachpse in the sentence, ask the binary classifier to judgeeach other pse as a potential parent, and choose thepse with the highest confidence6.
Finally, join theseimmediate-parent links to form a tree.7One might also try comparing pairs of potentialparents for a given pse, or other more direct meansof ranking potential parents.
We chose what seemedto be the simplest method for this first attempt at theproblem.3.1 FeaturesHere we motivate and describe the 23 features usedin our model.
Unless otherwise stated, all featuresare binary (1 if the described condition is true, 0otherwise).Parse-based features (6).
Based on the perfor-mance of the parse-based heuristic, we include apseparent-dominates-psetarget feature in our featureset.
To compensate for parse errors, however, wealso include a variant of this that is 1 if the parent ofpseparent dominates psetarget.Many filtering expressions filter pse?s that occurin their complements, but not in adjuncts.
There-fore, we add variants of the previous two syntax-based features that denote whether the parent node5We skip sentences where there is no decision to make (sen-tences with zero or one non-writer pse).
Since the writer pse isthe root of every structure, we do not generate instances withthe writer pse in the psetarget position.6There is an ambiguity if the classifier assigns the same con-fidence to two potential parents.
For evaluation purposes, weconsider the classifier?s response incorrect if any of the highest-scoring potential parents are incorrect.7The directed graph resulting from flawed automatic pre-dictions might not be a tree (i.e.
it might be cyclic and discon-nected).
Since this occurs very rarely (5 out of 9808 sentenceson the test data), we do not attempt to correct any non-treegraphs.dominates psetarget, but only if the first dependencyrelation is an object relation.For similar reasons, we include a feature calculat-ing the domination relation based on a partial parse.Consider the following sentence:3.
He was criticized more than recognized for hispolicy.One of ?criticized?
or ?recognized?
will be the rootof this dependency parse, thus dominating the other,and suggesting (incorrectly) that it filters the otherpse.
Because a partial parse does not attach all con-stituents, such spurious dominations are eliminated.The partial parse feature is 1 for fewer instancesthan pseparent-dominates-psetarget , but it is moreindicative of a positive instance when it is 1.So that the model can adjust when the parse isnot present, we include a feature that is 1 for allinstances generated from sentences on which theparser failed.Positional features (5).
Forcing the model to de-cide whether pseparent is the parent of psetargetwithout knowledge of the other pse?s in the sen-tence is somewhat artificial.
We therefore includeseveral features that encode the relative position ofpseparent and psetarget in the sentence.
Specifi-cally, we add a feature that is 1 if pseparent is theroot of the parse (and similarly for psetarget ).
Wealso include a feature giving the ordinal position ofpseparent among the pse?s in the sentence, relativeto psetarget (-1 means pseparent is the pse that im-mediately precedes psetarget, 1 means immediatelyfollowing, and so forth).
To allow the model to varywhen there are more potential parents to choosefrom, we include a feature giving the total numberof pse?s in the sentence.Special parents and lexical features (6).
Someparticular pse?s are special, so we specify indicatorfeatures for four types of parents: the writer pse,and the lexical items ?said?
(the most common non-writer pse) and ?according to?.
?According to?
isspecial because it is generally not very high in theparse, but semantically tends to filter everything elsein the sentence.In addition, we include as features the part ofspeech of pseparent and psetarget (reduced to noun,verb, adjective, adverb, or other), since intuitivelywe expected distinct parts of speech to behave dif-ferently in their filtering.Genre-specific features (6).
Finally, journalisticwriting contains a few special forms that are not al-ways parsed accurately.
Examples are:4.
?Alice disagrees with me,?
Bob argued.5.
Charlie, she noted, dislikes Chinese food.The parser may not recognize that ?noted?
and?argued?
should dominate all other pse?s in sen-tences 4 and 5, so we attempt to recognize whena sentence falls into one of these two patterns.For ?disagrees, argued?
generated from sentence 4,features pseparent-pattern-1 and psetarget-pattern-1 would be 1, while for ?dislikes, noted?
generatedfrom sentence 5, feature pseparent-pattern-2 wouldbe 1.
We also add features that denote whether thepse in question falls between matching quote marks.Finally, a simple feature indicates whether pseparentis the last word in the sentence.3.2 ResourcesWe rely on a variety of resources to generate our fea-tures.
The corpus (see Section 4) is distributed withannotations for sentence breaks, tokenization, andpart of speech information automatically generatedby the GATE toolkit (Cunningham et al, 2002).8For parsing we use the Collins (1999) parser.9 Forpartial parses, we employ CASS (Abney, 1997).
Fi-nally, we use a simple finite-state recognizer to iden-tify (possibly nested) quoted phrases.For classifier construction, we use the IND pack-age (Buntine, 1993) to train decision trees (we usethe mml tree style, a minimum message length cri-terion with Bayesian smoothing).4 Data DescriptionThe data for these experiments come from version1.1 of the NRRC corpus (Wiebe et al, 2002).10.
Thecorpus consists of 535 newswire documents (mostlyfrom the FBIS), of which we used 66 (1375 sen-tences) for developing the heuristics and features,while keeping the remaining 469 (9808 sentences)blind (used for 10-fold cross-validation).Although the NRRC corpus provides annotationsfor all pse?s, it does not provide annotations to de-note directly their hierarchical structure within a8GATE?s sentences sometimes extend across paragraphboundaries, which seems never to be warranted.
Inaccuratelyjoining sentences has the effect of adding more noise to ourproblem, so we split GATE?s sentences at paragraph bound-aries, and introduce writer pse?s for the newly created sen-tences.9We convert the parse to a dependency format that makessome of our features simpler using a method similar to the onedescribed in Xia and Palmer (2001).
We also employ a methodfrom Adam Lopez at the University of Maryland to find gram-matical relationships between words (subject, object, etc.
).10The original corpus is available at http://nrrc.mitre.org/NRRC/Docs_Data/MPQA_04/approval_mpqa.htm.
Code and data used in ourexperiments are available at http://www.cs.cornell.edu/?ebreck/breck04playing/.sentence.
This structure must be extracted froman attribute of each pse annotation, which lists thepse?s direct and indirect sources.
For example, the?source chain?
for ?unhappy?
in sentence 1, wouldbe (writer, Alice, Bob).
The source chains allowus to automatically recover the hierarchical struc-ture of the pse?s: the parent of a pse with sourcechain (s0, s1, .
.
.
sn?1, sn) is the pse with sourcechain (s0, s1, .
.
.
sn?1).
Unfortunately, ambiguitiescan arise.
Consider the following sentence:6.
Bob said, ?you?re welcome?
because he was gladto see that Mary was happy.Both ?said?
and ?was glad?
have the source chain(writer, Bob),11 while ?was happy?
has the sourcechain (writer, Bob, Mary).
It is therefore not clearfrom the manual annotations whether ?was happy?should have ?was glad?
or ?said?
as its parent.5.82% of the pse?s have ambiguous parentage (i.e.the recovery step finds a set of parents P (pse) with|P (pse)| > 1).
For training, we assign a class valueof 1 to all instances ?pse, par?, par ?
P (pse).
Fortesting, if an algorithm attaches pse to any elementof P (pse), we score the link as correct (see Sec-tion 5.1).
Since ultimately our goal is to find thesources through which information is filtered (ratherthan the pse?s), we believe this is justified.For training and testing, we used only those sen-tences that contain at least two non-writer pse?s12?
for all other sentences, there is only one way toconstruct the hierarchical structure.
Again, Table 2presents a breakdown (for the test set) of the num-ber of pse?s per sentence ?
thus we only use approx-imately one-third of all the sentences in the corpus.5 Results and Discussion5.1 EvaluationHow do we evaluate the performance of an au-tomatic method of determining the hierarchicalstructure of pse?s?
Lin (1995) proposes a methodfor evaluating dependency parses: the score fora sentence is the fraction of correct parent linksidentified; the score for the corpus is the aver-age sentence score.
Formally, the score for a11The annotators also performed coreference resolution onsources.12Under certain circumstances, such as paragraph-longquotes, the writer of a sentence will not be the same as thewriter of a document.
In such sentences, the NRRC corpus con-tains additional pse?s for any other sources besides the writer ofthe document.
Since we are concerned in this work only withone sentence at a time, we discard all such implicit pse?s be-sides the writer of the sentence.
Also, in a few cases, more thanone pse in a sentence was marked as having the writer as itssource.
We believe this to be an error and so discarded all butone writer pse.metric size heurOne heurTwo decTreeLin 2940 65.57% 71.64% 78.30%perf 2940 36.02% 45.37% 54.52%bin 21933 73.20% 77.73% 82.12%bin + 7882 60.63% 66.94% 70.35%bin ?
14051 80.24% 83.78% 88.72%Table 3: Performance on test data.
?Lin?
is Lin?s depen-dency score, ?perf?
is the fraction of sentences whosestructure was identified perfectly, and ?bin?
is the perfor-mance of the binary classifier (broken down for positiveand negative instances).
?Size?
is the number of sen-tences or pse pairs.# pse?s # sents heurOne heurTwo decTree3 1810 70.88% 75.41% 81.82%4 778 59.17% 67.82% 74.38%5 239 53.87% 61.92% 68.93%>5 113 49.31% 58.03% 68.68%Table 4: Performance by number of pse?s per sentencemethod evaluated on the entire corpus (?Lin?)
is?s?S|{pse|pse?Non writer pse?s(s)?parent(pse)=autopar(pse))}||Non writer pse?s(s)||S| ,where S is the set of all sentences in the corpus,Non writer pse ?s(s) is the set of non-writer pse?sin sentence s, parent(pse) is the correct parentof pse, and autopar(pse) is the automaticallyidentified parent of pse.We also present results using two other (related)metrics.
The ?perf?
metric measures the fractionof sentences whose structure is determined entirelycorrectly (i.e.
?perf?ectly).
?Bin?
is the accuracy ofthe binary classifier (with a 0.5 threshold) on the in-stances created from the test corpus.
We also reportthe performance on positive and negative instances.5.2 ResultsWe compare the learning-based approach (decTree)to the heuristic-based approaches introduced in Sec-tion 3 ?
heurOne assumes that all pse?s are at-tached to the writer?s implicit pse; heurTwo is theparse-based heuristic that relies solely on the domi-nance relation13.We use 10-fold cross-validation on the evalua-tion data to generate training and test data (althoughthe heuristics, of course, do not require training).The results of the decision tree method and the twoheuristics are presented in Table 3.13That is, heurTwo attaches a pse to the pse most immedi-ately dominating it in the dependency tree.
If no other psedominates it, a pse is attached to the writer?s pse.5.3 DiscussionEncouragingly, our machine learning method uni-formly and significantly14 outperforms the twoheuristic methods, on all metrics and in sentenceswith any number of pse?s.
The difference is moststriking in the ?perf?
metric, which is perhapsthe most intuitive.
Also, the syntax-based heuris-tic (heurTwo) significantly15 outperforms heurOne,confirming our intuitions that syntax is important inthis task.As the binary classifer sees many more negativeinstances than positive, it is unsurprising that its per-formance is much better on negative instances.
Thissuggests that we might benefit from machine learn-ing methods for dealing with unbalanced datasets.Examining the errors of the machine learning sys-tem on the development set, we see that for halfof the pse?s with erroneously identified parents, theparent is either the writer?s pse, or a pse like ?said?in sentences 4 and 5 having scope over the entiresentence.
For example,7.
?Our concern is whether persons used to the roleof policy implementors can objectively assess andcritique executive policies which impinge on hu-man rights,?
said Ramdas.Our model chose the parent of ?assess and critique?to be ?said?
rather than ?concern.?
We also see fromTable 4 that the model performs more poorly on sen-tences with more pse?s.
We believe that this reflectsa weakness in our decision to combine binary deci-sions, because the model has learned that in general,a ?said?
or writer?s pse (near the root of the struc-ture) is likely to be the parent, while it sees manyfewer examples of pse?s such as ?concern?
that liein the middle of the tree.Although we have ignored the distinctionthroughout this paper, error analysis suggestsspeech event pse?s behave differently than privatestate pse?s with respect to how closely syntax re-flects their hierarchical structure.
It may behooveus to add features to allow the model to take thisinto account.
Other sources of error include er-roneous sentence boundary detection, parentheticalstatements (which the parser does not treat correctlyfor our purposes) and other parse errors, partial quo-tations, as well as some errors in the annotation.Examining the learned trees is difficult becauseof their size, but looking at one tree to depth three14p < 0.01, using an approximate randomization test with9,999 trials.
See (Eisner, 1996, page 17) and (Chinchor et al,1993, pages 430-433) for descriptions of this method.15Using the same test as above, p < 0.01, except for theperformance on sentences with more than 5 pse?s, because ofthe small amount of data, where p < 0.02.reveals a fairly intuitive model.
Ignoring the prob-abilities, the tree decides pseparent is the parentof psetarget if and only if pseparent is the writer?spse (and psetarget is not in quotation marks), orif pseparent is the word ?said.?
For all the treeslearned, the root feature was either the writer psetest or the partial-parse-based domination feature.6 Conclusions and Future WorkWe have presented the concept of perspective andspeech expressions, and argued that determiningtheir hierarchical structure is important for naturallanguage understanding of perspective.
We haveshown that identifying the hierarchical structure ofpse?s is amenable to automated analysis via a ma-chine learning approach, although there is room forimprovement in the results.In the future, we plan to address the related tasksdiscussed in Section 2, especially identifying pse?sand their immediate sources.
We are also interestedin ways of improving the machine learning formu-lation of the current task, such as optimizing thebinary classifier on the whole-sentence evaluation,or defining a different binary task that is easier tolearn.
Nevertheless, we believe that our results pro-vide a step towards the development of natural lan-guage systems that can extract and summarize theviewpoints and perspectives expressed in text whiletaking into account the multi-stage information fil-tering process that can mislead more na?
?ve systems.AcknowledgmentsThis work was supported in part by NSF Grant IIS-0208028 and by an NSF Graduate Research Fellowship.We thank Rebecca Hwa for creating the dependencyparses.
We also thank the Cornell NLP group for help-ful suggestions on drafts of this paper.
Finally, we thankJanyce Wiebe and Theresa Wilson for draft suggestionsand advice regarding this problem and the NRRC corpus.ReferencesSteven Abney.
1997.
The SCOL manual.
cass is avail-able from http://www.vinartus.net/spa/scol1h.tar.gz.Sabine Bergler.
1993.
Semantic dimensions in the fieldof reporting verbs.
In Proceedings of the Ninth An-nual Conference of the University of Waterloo Centrefor the New Oxford English Dictionary and Text Re-search, Oxford, England, September.Steven Bethard, Hong Yu, Ashley Thornton, VasileiosHatzivassiloglou, and Dan Jurafsky.
2004.
Automaticextraction of opinion propositions and their holders.In Working Notes of the AAAI Spring Symposium onExploring Attitude and Affect in Text: Theories andApplications.
March 22-24, 2004, Stanford.Wray Buntine.
1993.
Learning classification trees.
InD.
J.
Hand, editor, Artificial Intelligence frontiers instatistics, pages 182?201.
Chapman & Hall,London.Available at http://ic.arc.nasa.gov/projects/bayes-group/ind/IND-program.html.Nancy Chinchor, Lynette Hirschman, and David Lewis.1993.
Evaluating message understanding systems:An analysis of the third message understandingconference (MUC-3).
Computational Linguistics,19(3):409?450.Michael John Collins.
1999.
Head-driven StatisticalModels for Natural Language Parsing.
Ph.D. thesis,University of Pennsylvania, Philadelphia.Hamish Cunningham, Diana Maynard, Kalina Bont-cheva, and Valentin Tablan.
2002.
GATE: A frame-work and graphical development environment for ro-bust nlp tools and applications.
In Proceedings of the40th Anniversary Meeting of the Association for Com-putational Linguistics (ACL ?02), Philadelphia, July.2003.
Proceedings of the Workshop on Text Summariza-tion, Edmonton, Alberta, Canada, May.
Presented atthe 2003 Human Language Technology Conference.Jason Eisner.
1996.
An empirical comparison of proba-bility models for dependency grammar.
Technical Re-port IRCS-96-11, IRCS, University of Pennsylvania.Christine Gerard.
2000.
Modelling readers of news ar-ticles using nested beliefs.
Master?s thesis, ConcordiaUniversity, Montre?al, Que?bec, Canada.Daniel Gildea and Daniel Jurafsky.
2002.
Automatic la-beling of semantic roles.
Computational Linguistics,28(3):245?288.Dekang Lin.
1995.
A dependency-based method forevaluating broad-coverage parsers.
In IJCAI, pages1420?1427.1998.
Proceedings of the Seventh Message Understand-ing Conference (MUC-7).
Morgan Kaufman, April.NIST.
2003.
Proceedings of The Twelfth Text REtrievalConference (TREC 2003), Gaithersburg, MD, Novem-ber.
NIST special publication SP 500-255.Randolph Quirk, Sidney Greenbaum, Geoffrey Leech,and Jan Svartvik.
1985.
A Comprehensive Grammarof the English Language.
Longman, New York.J.
Wiebe, E. Breck, C. Buckley, C. Cardie, P. Davis,B.
Fraser, D. Litman, D. Pierce, E. Riloff, and T. Wil-son.
2002.
NRRC Summer Workshop on Multiple-Perspective Question Answering Final Report.
Techreport, NRRC, Bedford, MA.J.
Wiebe, E. Breck, C. Buckley, C. Cardie, P. Davis,B.
Fraser, D. Litman, D. Pierce, E. Riloff, T. Wilson,D.
Day, and M. Maybury.
2003.
Recognizing and Or-ganizing Opinions Expressed in the World Press.
InPapers from the AAAI Spring Symposium on New Di-rections in Question Answering (AAAI tech report SS-03-07).
March 24-26, 2003.
Stanford.Janyce Wiebe.
1994.
Tracking point of view in narrative.Computational Linguistics, 20(2):233?287.Janyce Wiebe.
2002.
Instructions for annotating opin-ions in newspaper articles.
Technical Report TR-02-101, Dept.
of Comp.
Sci., University of Pittsburgh.Fei Xia and Martha Palmer.
2001.
Converting depen-dency structures to phrase structures.
In Proc.
of theHLT Conference.
