Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 14?25,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsTranslation Modeling with Bidirectional Recurrent Neural NetworksMartin Sundermeyer1, Tamer Alkhouli1, Joern Wuebker1, and Hermann Ney1,21Human Language Technology and Pattern Recognition GroupRWTH Aachen University, Aachen, Germany2Spoken Language Processing GroupUniv.
Paris-Sud, France and LIMSI/CNRS, Orsay, France{surname}@cs.rwth-aachen.deAbstractThis work presents two different trans-lation models using recurrent neural net-works.
The first one is a word-based ap-proach using word alignments.
Second,we present phrase-based translation mod-els that are more consistent with phrase-based decoding.
Moreover, we introducebidirectional recurrent neural models tothe problem of machine translation, allow-ing us to use the full source sentence in ourmodels, which is also of theoretical inter-est.
We demonstrate that our translationmodels are capable of improving strongbaselines already including recurrent neu-ral language models on three tasks:IWSLT 2013 German?English, BOLTArabic?English and Chinese?English.We obtain gains up to 1.6% BLEUand 1.7% TER by rescoring 1000-bestlists.1 IntroductionNeural network models have recently experiencedunprecedented attention in research on statisticalmachine translation (SMT).
Several groups havereported strong improvements over state-of-the-artbaselines using feedforward neural network-basedlanguage models (Schwenk et al., 2006; Vaswaniet al., 2013), as well as translation models (Le etal., 2012; Schwenk, 2012; Devlin et al., 2014).Different from the feedforward design, recurrentneural networks (RNNs) have the advantage of be-ing able to take into account an unbounded his-tory of previous observations.
In theory, this en-ables them to model long-distance dependenciesof arbitrary length.
However, while previous workon translation modeling with recurrent neural net-works shows its effectiveness on standard base-lines, so far no notable gains have been presentedon top of recurrent language models (Auli et al.,2013; Kalchbrenner and Blunsom, 2013; Hu et al.,2014).In this work, we present two novel approachesto recurrent neural translation modeling: word-based and phrase-based.
The word-based ap-proach assumes one-to-one aligned source andtarget sentences.
We evaluate different ways ofresolving alignment ambiguities to obtain suchalignments.
The phrase-based RNN approach ismore closely tied to the underlying translationparadigm.
It models actual phrasal translationprobabilities while avoiding sparsity issues by us-ing single words as input and output units.
Fur-thermore, in addition to the unidirectional formu-lation, we are the first to propose a bidirectionalarchitecture which can take the full source sen-tence into account for all predictions.
Our ex-periments show that these models can improvestate-of-the-art baselines containing a recurrentlanguage model on three tasks.
For our compet-itive IWSLT 2013 German?English system, weobserve gains of up to 1.6% BLEU and 1.7% TER.Improvements are also demonstrated on top of ourevaluation systems for BOLT Arabic?Englishand Chinese?English, which also include recur-rent neural language models.The rest of this paper is structured as follows.
InSection 2 we review related work and in Section 3an overview of long short-term memory (LSTM)neural networks, a special type of recurrent neuralnetworks we make use of in this work, is given.Section 4 describes our novel translation models.Finally, experiments are presented in Section 5and we conclude with Section 6.142 Related WorkIn this Section we contrast previous work to ours,where we design RNNs to model bilingual depen-dencies, which are applied to rerank n-best listsafter decoding.To the best of our knowledge, the earliest at-tempts to apply neural networks in machine trans-lation (MT) are presented in (Casta?no et al.,1997; Casta?no and Casacuberta, 1997; Casta?noand Casacuberta, 1999), where they were used forexample-based MT.Recently, Le et al.
(2012) presented translationmodels using an output layer with classes anda shortlist for rescoring using feedforward net-works.
They compare between word-factored andtuple-factored n-gram models, obtaining their bestresults using the word-factored approach, which isless amenable to data sparsity issues.
Both of ourword-based and phrase-based models eventuallywork on the word level.
Kalchbrenner and Blun-som (2013) use recurrent neural networks withfull source sentence representations.
The continu-ous representations are obtained by applying a se-quence of convolutions, and the result is fed intothe hidden layer of a recurrent language model.Rescoring results indicate no improvements overthe state of the art.
Auli et al.
(2013) also in-clude source sentence representations built eitherusing Latent Semantic Analysis or by concatenat-ing word embeddings.
This approach producedno notable gain over systems using a recurrentlanguage model.
On the other hand, our pro-posed bidirectional models include the full sourcesentence relying on recurrency, yielding improve-ments over competitive baselines already includ-ing a recurrent language model.RNNs were also used with minimum translationunits (Hu et al., 2014), which are phrase pairs un-dergoing certain constraints.
At the input layer,each of the source and target phrases are mod-eled as a bag of words, while the output phraseis predicted word-by-word assuming conditionalindependence.
The approach seeks to alleviatedata sparsity problems that would arise if phraseswere to be uniquely distinguished.
Our proposedphrase-based models maintain word order withinphrases, but the phrases are processed in a word-pair manner, while the phrase boundaries remainimplicitly encoded in the way the words are pre-sented to the network.
Schwenk (2012) proposeda feedforward network that predicts phrases of afixed maximum length, such that all phrase wordsare predicted at once.
The prediction is condi-tioned on the source phrase.
Since our phrase-based model predicts one word at a time, it doesnot assume any phrase length.
Moreover, ourmodel?s predictions go beyond phrase boundariesand cover unbounded history and future contexts.Using neural networks during decoding re-quires tackling the costly output normalizationstep.
Vaswani et al.
(2013) avoid this step bytraining feedforward neural language models us-ing noise contrastive estimation, while Devlin etal.
(2014) augment the training objective functionto produce approximately normalized scores di-rectly.
The latter work makes use of translationand joint models, and pre-computes the first hid-den layer beforehand, resulting in large speedups.They report major improvements over strong base-lines.
The speedups achieved by both works al-lowed to integrate feedforward neural networksinto the decoder.3 LSTM Recurrent Neural NetworksOur work is based on recurrent neural networks.In related fields like e. g. language modeling, thistype of neural network has been shown to performconsiderably better than standard feedforward ar-chitectures (Mikolov et al., 2011; Arisoy et al.,2012; Sundermeyer et al., 2013; Liu et al., 2014).Most commonly, recurrent neural networks aretrained with stochastic gradient descent (SGD),where the gradient of the training criterion is com-puted with the backpropagation through time al-gorithm (Rumelhart et al., 1986; Werbos, 1990;Williams and Zipser, 1995).
However, the combi-nation of RNN networks with conventional back-propagation training leads to conceptual difficul-ties which are known as the vanishing (or explod-ing) gradient problem, described e. g. in (Bengioet al., 1994).
To remedy this problem, in (Hochre-iter and Schmidhuber, 1997) it was suggested tomodify the architecture of a standard RNN in sucha way that vanishing and exploding gradients areavoided during backpropagation.
In particular, nomodification of the training algorithm is necessary.The resulting architecture is referred to as longshort-term memory (LSTM) neural network.Bidirectional recurrent neural networks(BRNNs) were first proposed in (Schuster andPaliwal, 1997) and applied to speech recognitiontasks.
They have been since applied to different15Surfers,forexample,knowthisincredibly.SurferzumBeispielkennendaszurGen?uge.
(a) OriginalSurfers,forexample,knowthisunalignedincredibly.SurferalignedzumBeispielunalignedkennendaszurGen?uge.
(b) One-to-one alignmentFigure 1: Example sentence from the German?English IWSLT data.
The one-to-one alignment iscreated by introducing alignedand unalignedtokens.tasks like parsing (Henderson, 2004) and spokenlanguage understanding (Mesnil et al., 2013).Bidirectional long short-term memory (BLSTM)networks are BRNNs using LSTM hidden layers(Graves and Schmidhuber, 2005).
This workintroduces BLSTMs to the problem of machinetranslation, allowing powerful models that employunlimited history and future information to makepredictions.While the proposed models do not make any as-sumptions about the type of RNN used, all of ourexperiments make use of recurrent LSTM neuralnetworks, where we include later LSTM exten-sions proposed in (Gers et al., 2000; Gers et al.,2003).
The cross-entropy error criterion is usedfor training.
Further details on LSTM neural net-works can be found in (Graves and Schmidhuber,2005; Sundermeyer et al., 2012).4 Translation Modeling with RNNsIn the following we describe our word- andphrase-based translation models in detail.
We alsoshow how bidirectional RNNs can enable suchmodels to include full source information.4.1 Resolving Alignment AmbiguitiesOur word-based recurrent models are only de-fined for one-to-one-aligned source-target sen-tence pairs.
In this work, we always evaluate themodel in the order of the target sentence.
How-ever, we experiment with several different waysto resolve ambiguities due to unaligned or mul-tiply aligned words.
To that end, we introducetwo additional tokens, alignedand unaligned.
Un-dev testBLEU TER BLEU TERbaseline 33.5 45.8 30.9 48.4w/o  34.2 45.3 31.8 47.7w/o unaligned34.4 44.8 31.7 47.4source identity 34.5 45.0 31.9 47.5target identity 34.5 44.6 31.9 47.0all  34.6 44.5 32.0 47.1Table 1: Comparison of including different setsof  tokens into the one-to-one alignment on theIWSLT 2013 German?English task using the uni-directional RNN translation model.aligned words are either removed or aligned to anextra unalignedtoken on the opposite side.
If anunalignedis introduced on the target side, its posi-tion is determined by the aligned source word thatis closest to the unaligned source word in question,preferring left to right.
To resolve one-to-manyalignments, we use an IBM-1 translation table todecide for one of the alignment connections to bekept.
The remaining words are also either deletedor aligned to additionally introduced alignedto-kens on the opposite side.
Fig.
1 shows an ex-ample sentence from the IWSLT data, where all tokens are introduced.In a short experiment, we evaluated 5 differ-ent setups with our unidirectional RNN translationmodel (cf.
next Section): without any  tokens,without unaligned, source identity, target identityand using all  tokens.
Source identity means we16introduce no  tokens on source side, but all ontarget side.
Target identity is defined analogously.The results can be found in Tab.
1.
We use thesetup with all  tokens in all following experi-ments, which showed the best BLEU performance.4.2 Word-based RNN ModelsGiven a pair of source sequence fI1= f1.
.
.
fIand target sequence eI1= e1.
.
.
eI, where we as-sume a direct correspondence between fiand ei,we define the posterior translation probability byfactorizing on the target words:p(eI1|fI1) =I?i=1p(ei|ei?11, fI1) (1)?I?i=1p(ei|ei?11, fi+d1) (2)?I?i=1p(ei|fi+d1).
(3)We denote the formulation (1) as the bidirectionaljoint model (BJM).
This model can be simplifiedby several independence assumptions.
First, wedrop the dependency on the future source infor-mation, receiving what we denote as the unidirec-tional joint model (JM) in (2).
Here, d ?
N0isa delay parameter, which is set to d = 0 for allexperiments, except for the comparative results re-ported in Fig.
7.
Finally, assuming conditional in-dependence from the previous target sequence, wereceive the unidirectional translation model (TM)in (3).
Analogously, we can define a bidirectionaltranslation model (BTM) by keeping the depen-dency on the full source sentence fI1, but droppingthe previous target sequence ei?11:p(eI1|fI1) ?I?i=1p(ei|fI1).
(4)Fig.
2 shows the dependencies of the word-based neural translation and joint models.
Thealignment points are traversed in target order andat each time step one target word is predicted.The pure translation model (TM) takes only sourcewords as input, while the joint model (JM) takesthe preceding target words as an additional input.A delay of d > 0 is implemented by shifting thetarget sequence by d time steps and filling the firstd target positions and the last d source positionswith a dedicated paddingsymbol.
The RNN archi-tecture for the unidirectional word-based modelsjointmodelall models bidirectionalSurfers,forexample,knowthisunalignedincredibly.SurferalignedzumBeispielunalignedkennendaszurGen?uge.Figure 2: Dependencies modeled within the word-based RNN models when predicting the targetword ?know?.
Directly processed information isdepicted with solid rectangles, and informationavailable through recurrent connections is markedwith dashed rectangles.is illustrated in Fig.
3, which corresponds to thefollowing set of equations:yi= A1?fi+A2e?i?1zi= ?
(yi;A3, yi?11)p(c(ei)|ei?11, fi1)= ?c(ei)(A4zi)p(ei|c(ei), ei?11, fi1)= ?ei(Ac(ei)zi)p(ei|ei?11, fi1) = p(ei|c(ei), ei?11, fi1)?p(c(ei)|ei?11, fi1)Here, by?fiand e?i?1we denote the one-hot en-coded vector representations of the source andtarget words fiand ei?1.
The outgoing activa-tion values of the projection layer and the LSTMlayer are yiand zi, respectively.
The matrices Ajcontain the weights of the neural network layers.By ?(?
;A3, yi?11) we denote the LSTM formalismthat we plug in at the third layer.
As the LSTMlayer is recurrent, we explicitly include the de-pendence on the previous layer activations yi?11.Finally, ?
is the widely-used softmax function toobtain normalized probabilities, and c denotes aword class mapping from any target word to itsunique word class.
For the bidirectional model,the equations can be defined analogously.Due to the use of word classes, the outputlayer consists of two parts.
The class probabil-ity p(c(ei)|ei?11, fi1)is computed first, and then17p(c(ei)|ei?11 , f i+d1)p(ei|c(ei), ei?11 , f i+d1)class layer output layerLSTM layerprojection layerinput layerei?1fi+d)Figure 3: Architecture of a recurrent unidirec-tional translation model.
By including the dashedparts, a joint model is obtained.the word probability p(ei|c(ei), ei?11, fi1)is ob-tained given the word class.
This trick helps avoid-ing the otherwise computationally expensive nor-malization sum, which would be carried out overall words in the target vocabulary.
In a class-factorized output layer where each word belongsto a single class, the normalization is carried outover all classes, whose number is typically muchless than the vocabulary size.
The other normal-ization sum needed to produce the word probabil-ity is limited to the words belonging to the sameclass (Goodman, 2001; Morin and Bengio, 2005).4.3 Phrase-based RNN ModelsOne of the conceptual disadvantages of word-based modeling as introduced in the previous sec-tion is that there is a mismatch between train-ing and testing conditions: During neural networktraining, the vocabulary has to be extended by ad-ditional  tokens, and a one-to-one alignment isused which does not reflect the situation in decod-ing.
In phrase-based machine translation, morecomplex alignments in terms of multiple wordson both the source and the target sides are used,which allow the decoder to make use of richershort-distance dependencies and are crucial for theperformance of the resulting system.From this perspective, it seems interesting tostandardize the alignments used in decoding, andin training the neural network.
However, it is dif-ficult to use the phrases themselves as the vocab-ulary of the RNN.
Usually, the huge number ofpotential phrases in comparison to the relativelysmall amount of training data makes the learn-ing of continuous phrase representations difficultSurferSurferszum Beispiel, for example ,kennenknowzur Gen?geincredibly..dasthisFigure 4: Example phrase alignment for a sen-tence from the IWSLT training data.due to data sparsity.
This is confirmed by resultspresented in (Le et al., 2012), which show that aword-factored translation model outperforms thephrase-factored version.
Therefore, in this workwe continue relying on source and target word vo-cabularies for building our phrase representations.However, we no longer use a direct correspon-dence between a source and a target word, as en-forced in our word-based models.Fig.
4 shows an example phrase alignment,where a sequence of source words?fiis directlymapped to a sequence of target words e?ifor 1 ?i ?
?I .
By?I , we denote the number of phrases inthe alignment.
We decompose the target sentenceposterior probability in the following way:p(eI1|fJ1) =?I?i=1p(e?i|e?i?11,?f?I1) (5)?
?I?i=1p(e?i|e?i?11,?fi1) (6)where the joint model in Eq.
5 would correspondto a bidirectional RNN, and Eq.
6 only requires aunidirectional RNN.
By leaving out the condition-ing on the target side, we obtain a phrase-basedtranslation model.As there is no one-to-one correspondence be-tween the words within a phrase, the basic idea ofour phrase-based approach is to let the neural net-work learn the dependencies itself, and present thefull source side of the phrase to the network be-fore letting it predict target side words.
Then theprobability for the target side of a phrase can becomputed, in case of Eq.
6, by:p(e?i|e?i?11,?f?I1) =|e?i|?j=1p((e?i)j|(e?i)j?11, e?i?11,?fi1),and analogously for the case of Eq.
5.
Here, (e?i)jdenotes the j-th word of the i-th aligned targetphrase.We feed the source side of a phrase into the neu-ral network one word at a time.
Only when the18output layerLSTM layerprojection layerinput layerSurfers ,, for example know this incredibly .???s?
Surfers,, for exampleknowthisincredibly.SurferzumBeispielkennendas zur Genu?ge??
??
?Figure 5: A recurrent phrase-based joint translation model, unfolded over time.
Source words are printedin normal face, while target words are printed in bold face.
Dashed lines indicate phrases from theexample sentence.
For brevity, we omit the precise handling of sentence begin and end tokens.presentation of the source side is finished we startestimating probabilities for the target side.
There-fore, we do not let the neural network learn a targetdistribution until the very last source word is con-sidered.
In this way, we break up the conventionalRNN training scheme where an input sample is di-rectly followed by its corresponding teacher sig-nal.
Similarly, the presentation of the source sideof the next phrase only starts after the predictionof the current target side is completed.To this end, we introduce a no-operation token,denoted by ?, which is not part of the vocabulary(which means it cannot be input to or predicted bythe RNN).
When the ?
token occurs as input, it in-dicates that no input needs to be processed by theRNN.
When the ?
token occurs as a teacher signalfor the RNN, the output layer distribution is ig-nored, and does not even have to be computed.
Inboth cases, all the other layers are still processedduring forward and backward passes such that theRNN state can be advanced even without addi-tional input or output.Fig.
5 depicts the evaluation of a phrase-basedjoint model for the example alignment from Fig.
4.For a source phrase?fi, we include (|e?i|?1) many ?symbols at the end of the phrase.
Conversely, fora target phrase e?i, we include (|?fi| ?
1) many ?symbols at the beginning of the phrase.E.
g., in the figure, the second dashed rectan-gle from the left depicts the training of the Englishphrase ?, for example ,?
and its German transla-tion ?zum Beispiel?.
At the input layer, we feed inthe source words one at a time, while we present?
tokens at the target side input layer and the out-put layer (with the exception of the very first timestep, where we still have the last target word fromthe previous phrase as input instead of ?).
Withthe last word of the source phrase ?Beispiel?
beingpresented to the network, the full source phrase isstored in the hidden layer, and the neural networkis then trained to predict the target phrase wordsat the output layer.
Subsequently, the source inputis ?, and the target input is the most recent targetside history word.To obtain a phrase-aligned training sequence forthe phrase-based RNN models, we force-align thetraining data with the application of leave-one-outas described in (Wuebker et al., 2010).4.4 Bidirectional RNN ArchitectureWhile the unidirectional RNNs include an un-bounded sentence history, they are still limited inthe number of future source words they include.Bidirectional models provide a flexible means toalso include an unbounded future context, which,unlike the delayed unidirectional models, requireno tuning to determine the amount of delay.Fig.
6 illustrates the bidirectional model archi-tecture, which is an extension of the unidirectionalmodel of Fig.
3.
First, an additional recurrenthidden layer is added in parallel to the existingone.
This layer will be referred to as the back-ward layer, since it processes information in back-ward time direction.
This hidden layer receivessource word input only, while target words in thecase of a joint model are fed to the forward layeras in the unidirectional case.
Due to the backwardrecurrency, the backward layer will make the in-formation fIiavailable when predicting the targetword ei, while the forward layer takes care of thesource history fi1.
Jointly, the forward and back-ward branches include the full source sentence fI1,as indicated in Fig.
2.
Fig.
6 shows the ?deep?variant of the bidirectional model, where the for-19p(c(ei)|ei?11 , fI1)p(ei|c(ei), ei?11 , fI1)class layer output layer2nd LSTM layer1st LSTM layerprojection layerinput layerei?1fi(+)(+)(?
)Figure 6: Architecture of a recurrent bidirectionaltranslation model.
By (+) and (?
), we indicatea processing in forward and backward time direc-tions, respectively.
The inclusion of the dashedparts leads to a bidirectional joint model.
Onesource projection matrix is used for the forwardand backward branches.ward and backward layers converge into a hiddenlayer.
A shallow variant can be obtained if theparallel layers converge into the output layer di-rectly1.Due to the full dependence on the source se-quence, evaluating bidirectional networks requirescomputing the forward pass of the forward andbackward layers for the full sequence, before be-ing able to evaluate the next layers.
In the back-ward pass of backpropagation, the forward andbackward recurrent layers are processed in de-creasing and increasing time order, respectively.5 Experiments5.1 SetupAll translation experiments are performed with theJane toolkit (Vilar et al., 2010; Wuebker et al.,2012).
The largest part of our experiments is car-ried out on the IWSLT 2013 German?Englishshared translation task.2The baseline system istrained on all available bilingual data, 4.3M sen-tence pairs in total, and uses a 4-gram LM withmodified Kneser-Ney smoothing (Kneser and Ney,1995; Chen and Goodman, 1998), trained withthe SRILM toolkit (Stolcke, 2002).
As additional1In our implementation, the forward and backward layersconverge into an intermediate identity layer, and the aggre-gate is weighted and fed to the next layer.2http://www.iwslt2013.orgdata sources for the LM we selected parts of theShuffled News and LDC English Gigaword cor-pora based on cross-entropy difference (Mooreand Lewis, 2010), resulting in a total of 1.7 bil-lion running words for LM training.
The state-of-the-art baseline is a standard phrase-based SMTsystem (Koehn et al., 2003) tuned with MERT(Och, 2003).
It contains a hierarchical reorder-ing model (Galley and Manning, 2008) and a 7-gram word cluster language model (Wuebker etal., 2013).
Here, we also compare against a feed-forward joint model as described by Devlin et al.
(2014), with a source window of 11 words and atarget history of three words, which we denote asBBN-JM.
Instead of POS tags, we predict wordclasses trained with mkcls.
We use a shortlistof size 16K and 1000 classes for the remainingwords.
All neural networks are trained on the TEDportion of the data (138K segments) and are ap-plied in a rescoring step on 1000-best lists.To confirm our results, we run additionalexperiments on the Arabic?English andChinese?English tasks of the DARPA BOLTproject.
In both cases, the neural network modelsare added on top of our most competitive eval-uation system.
On Chinese?English, we use ahierarchical phrase-based system trained on 3.7Msegments with 22 dense features, including an ad-vanced orientation model (Huck et al., 2013).
Forthe neural network training, we selected a subsetof 9M running words.
The Arabic?Englishsystem is a standard phrase-based decoder trainedon 6.6M segments, using 17 dense features.
Theneural network training was performed using aselection amounting to 15.5M running words.For both tasks we apply the neural networks byrescoring 1000-best lists and evaluate results ontwo data sets from the ?discussion forum?
domain,test1 and test2.
The sizes of the data setsfor the Arabic?English system are: 1219 (dev),1510 (test1), and 1137 (test2) segments, andfor the Chinese?English system are: 5074 (dev),1844 (test1), and 1124 (test2) segments.
Allresults are measured in case-insensitive BLEU [%](Papineni et al., 2002) and TER [%] (Snover et al.,2006) on a single reference.5.2 ResultsOur results on the IWSLT German?English taskare summarized in Tab.
2.
At this point, wedo not include a recurrent neural network lan-20dev testBLEU TER BLEU TERbaseline 33.5 45.8 30.9 48.4TM 34.6 44.5 32.0 47.1JM 34.7 44.7 31.8 47.4BTM 34.7 44.9 32.3 47.0BTM (deep) 34.8 44.3 32.5 46.7BJM 34.7 44.5 32.1 47.0BJM (deep) 34.9 44.1 32.2 46.6PTM 34.3 44.9 32.1 47.5PJM 34.3 45.0 32.0 47.5PJM (10-best) 34.4 44.8 32.0 47.3PJM (deep) 34.6 44.7 32.0 47.6PBJM (deep) 34.8 44.9 31.9 47.5BBN-JM 34.4 44.9 31.9 47.6Table 2: Results for the IWSLT 2013German?English task with different RNNmodels.
T: translation, J: joint, B: bidirectional,P: phrase-based.guage model yet.
Here, the delay parameter dfrom Equations 2 and 3 is set to zero.
We ob-serve that for all recurrent translation models, weachieve substantial improvements over the base-line on the test data, ranging from 0.9 BLEUup to 1.6 BLEU.
These results are also consistentwith the improvements in terms of TER, where weachieve reductions by 0.8 TER up to 1.8 TER.These numbers can be directly compared to thecase of feedforward neural network-based transla-tion modeling as proposed in (Devlin et al., 2014)which we include in the very last row of the table.Nearly all of our recurrent models outperform thefeedforward approach, where the RNN model per-forming best on the dev data is better on testby 0.3 BLEU and 1.0 TER.Interestingly, for the recurrent word-based mod-els, on the test data it can be seen that TMs per-form better than JMs, even though TMs do nottake advantage of the target side history words.However, exploiting this extra information doesnot always need to result in a better model, as thetarget side words are only derived from the givensource side, which is available to both TMs andJMs.
On the other hand, including future sourcewords in a bidirectional model clearly improvesthe performance further.
By adding another LSTM3131.53232.53301234BLEU[%]DelayRNN-TMRNN-BTMFigure 7: BLEU scores on the IWSLT test setwith different delays for the unidirectional RNN-TM and the bidirectional RNN-BTM.layer that combines forward and backward timedirections (indicated as ?deep?
in the table), we ob-tain our overall best model.In Fig.
7 we compare the word-based bidirec-tional TM with a unidirectional TM that uses dif-ferent time delays d = 0, .
.
.
, 4.
For a delay d =2, the same performance is obtained as with thebidirectional model, but this comes at the price oftuning the delay parameter.In comparison to the unidirectional word-basedmodels, phrase-based models perform similarly.In the tables, we include those phrase-based vari-ants which perform best on the dev data, wherephrase-based JMs always are at least as good orbetter than the corresponding TMs in terms ofBLEU.
Therefore, we mainly report JM resultsfor the phrase-based networks.
A phrase-basedmodel can also be trained on multiple variants forthe phrase alignment.
For our experiments, wetested 10-best alignments against the single bestalignment, which resulted in a small improvementof 0.2 TER on both dev and test.
We did not ob-serve consistent gains by using an additional hid-den layer or bidirectional models.
To some ex-tent, future information is already considered inunidirectional phrase-based models by feeding thecomplete source side before predicting the targetside.Tab.
3 shows different model combination re-sults for the IWSLT task, where a recurrent lan-guage model is included in the baseline.
Addinga deep bidirectional TM or JM to the recur-rent language model improves the RNN-LM base-line by 1.2 BLEU or 1.1 BLEU, respectively.
Aphrase-based model substantially improves over21dev eval11 testBLEU[%]TER[%]BLEU[%]TER[%]BLEU[%]TER[%]baseline (w/ RNN-LM) 34.3 44.8 36.4 42.9 31.5 47.8BTM (deep) 34.9 43.7 37.6 41.5 32.7 46.1BJM (deep) 35.0 44.4 37.4 41.9 32.6 46.5PBJM (deep) 34.8 44.6 36.9 42.6 32.3 47.24 RNN models 35.2 43.4 38.0 41.2 32.7 46.0Table 3: Results for the IWSLT 2013 German?English task with different RNN models.
All resultsinclude a recurrent language model.
T: translation, J: joint, B: bidirectional, P: phrase-based.the RNN-LM baseline, but performs not as goodas its word-based counterparts.
By adding fourdifferent translation models, including models inreverse word order and reverse translation direc-tion, we are able to improve these numbers evenfurther.
However, especially on the test data, thegains from model combination saturate quickly.Apart from the IWSLT track, we also ana-lyze the performance of our translation models onthe BOLT Chinese?English and Arabic?Englishtranslation tasks.
Due to the large amount of train-ing data, we concentrate on models of high perfor-mance in the IWSLT experiments.
The results canbe found in Tab.
4 and 5.
In both cases, we seeconsistent improvements over the recurrent neuralnetwork language model baseline, improving theArabic?English system by 0.6 BLEU and 0.5 TERon test1.
This can be compared to the rescoringresults for the same task reported by (Devlin et al.,2014), where they achieved 0.3 BLEU, despite thefact that they used multiple references for scoring,whereas in our experiments we rely on a singlereference only.
The models are also able to im-prove the Chinese?English system by 0.5 BLEUand 0.5 TER on test2.5.3 AnalysisTo investigate whether bidirectional models ben-efit from future source information, we comparethe single-best output of a system reranked with aunidirectional model to the output reranked witha bidirectional model.
We choose the modelsto be translation models in both cases, as theypredict target words independent of previouspredictions, given the source information (cf.
Eqs.
(3, 4)).
This makes it easier to detect the effectof including future source information or the lackthereof.
The examples are taken from the IWSLTtest1 test2BLEU TER BLEU TERbaseline 25.2 57.4 26.8 57.3BTM (deep) 25.6 56.6 26.8 56.7BJM (deep) 25.9 56.9 27.4 56.7RNN-LM 25.6 57.1 27.5 56.7+ BTM (deep) 25.9 56.7 27.3 56.8+ BJM (deep) 26.2 56.6 27.9 56.5Table 4: Results for the BOLT Arabic?Englishtask with different RNN models.
The ?+?
sign inthe last two rows indicates that either of the corre-sponding deep models (BTM and BJM) are addedto the baseline including the recurrent languagemodel (i.e.
they are not applied at the same time).T: translation, J: joint, B: bidirectional.task, where we include the one-to-one sourceinformation, reordered according to the targetside.source: nicht so wie ichreference: not like meHypothesis 1:1-to-1 source: so ich  nicht wie1-to-1 target: so I do n?t likeHypothesis 2:1-to-1 source: nicht so wie ich1-to-1 target: not  like meIn this example, the German phrase ?so wie?translates to ?like?
in English.
The bidirectionalmodel prefers hypothesis 2, making use of thefuture word ?wie?
when translating the Germanword ?so?
to , because it has future insight thatthis move will pay off later when translating22BLEU TER BLEU TERbaseline 18.3 63.6 16.7 63.0BTM (deep) 18.7 63.3 17.1 62.6BJM (deep) 18.5 63.1 17.2 62.3RNN-LM 18.8 63.3 17.2 62.8+ BTM (deep) 18.9 63.1 17.7 62.3+ BJM (deep) 18.8 63.3 17.5 62.5Table 5: Results for the BOLT Chinese?Englishtask with different RNN models.
The ?+?
sign inthe last two rows indicates that either of the corre-sponding deep models (BTM and BJM) are addedto the baseline including the recurrent languagemodel (i.e.
they are not applied at the same time).T: translation, B: bidirectional.the rest of the sentence.
This information isnot available to the unidirectional model, whichprefers hypothesis 1 instead.source: das taten wir dann auch und verschafften unsso eine Zeit lang einen Wettbewerbs Vorteil .reference: and we actually did that and it gave us acompetitive advantage for a while .Hypothesis 1:1-to-1 source: das    wir dann auch taten undverschafften uns so eine Zeit lang einen WettbewerbsVorteil .1-to-1 target: that ?s just what we   did and gave us a time , a competitive advantage .Hypothesis 2:1-to-1 source: das    wir dann auch taten undverschafften uns so einen Wettbewerbs Vorteil  eineZeit lang .1-to-1 target: that ?s just what we   did and gave us a competitive advantage for a  while .Here, the German phrase ?eine Zeit lang?
trans-lates to ?for a while?
in English.
Bidirectionalscoring favors hypothesis 2, while unidirectionalscoring favors hypothesis 1.
It seems that the uni-directional model translates ?Zeit?
to ?time?
as theobject of the verb ?give?
in hypothesis 1, beingblind to the remaining part ?lang?
of the phrasewhich changes the meaning.
The bidirectionalmodel, to its advantage, has the full source infor-mation, allowing it to make the correct prediction.6 ConclusionWe developed word- and phrase-based RNN trans-lation models.
The former is simple and performswell in practice, while the latter is more consistentwith the phrase-based paradigm.
The approach in-herently evades data sparsity problems as it workson words in its lowest level of processing.
Ourexperiments show the models are able to achievenotable improvements over baselines containing arecurrent LM.In addition, and for the first time in statisticalmachine translation, we proposed a bidirectionalneural architecture that allows modeling past andfuture dependencies of any length.
Besides itsgood performance in practice, the bidirectional ar-chitecture is of theoretical interest as it allows theexact modeling of posterior probabilities.AcknowledgmentsThis material is partially based upon work sup-ported by the DARPA BOLT project under Con-tract No.
HR0011- 12-C-0015.
Any opinions,findings and conclusions or recommendations ex-pressed in this material are those of the authors anddo not necessarily reflect the views of DARPA.The research leading to these results has also re-ceived funding from the European Union Sev-enth Framework Programme (FP7/2007-2013) un-der grant agreements no287658 and no287755.Experiments were performed with computing re-sources granted by JARA-HPC from RWTHAachen University under project ?jara0085?.
Wewould like to thank Jan-Thorsten Peter for provid-ing the BBN-JM system.ReferencesEbru Arisoy, Tara N. Sainath, Brian Kingsbury, andBhuvana Ramabhadran.
2012.
Deep neural networklanguage models.
In Proceedings of the NAACL-HLT 2012 Workshop: Will We Ever Really Replacethe N-gram Model?
On the Future of LanguageModeling for HLT, pages 20?28.
Association forComputational Linguistics.Michael Auli, Michel Galley, Chris Quirk, and Geof-frey Zweig.
2013.
Joint Language and TranslationModeling with Recurrent Neural Networks.
In Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1044?1054, Seattle, USA, Octo-ber.Yoshua Bengio, Patrice Simard, and Paolo Frasconi.1994.
Learning long-term dependencies with gra-23dient descent is difficult.
Neural Networks, IEEETransactions on, 5(2):157?166.Maria Asunci?on Casta?no and Francisco Casacuberta.1997.
A connectionist approach to machine trans-lation.
In 5th International Conference on SpeechCommunication and Technology (EUROSPEECH-97), Rhodes, Greece.Maria Asunci?on Casta?no and Francisco Casacuberta.1999.
Text-to-text machine translation using theRECONTRA connectionist model.
In Lecture Notesin Computer Science (IWANN 99), volume 1607,pages 683?692, Alicante, Spain.Maria Asunci?on Casta?no, Francisco Casacuberta, andEnrique Vidal.
1997.
Machine translation usingneural networks and finite-state models.
In 7th In-ternational Conference on Theoretical and Method-ological Issues in Machine Translation.
TMI?97,pages 160?167, Santa Fe, USA.Stanley F. Chen and Joshua Goodman.
1998.
AnEmpirical Study of Smoothing Techniques for Lan-guage Modeling.
Technical Report TR-10-98, Com-puter Science Group, Harvard University, Cam-bridge, MA, August.Jacob Devlin, Rabih Zbib, Zhongqiang Huang, ThomasLamar, Richard Schwartz, and John Makhoul.
2014.Fast and Robust Neural Network Joint Models forStatistical Machine Translation.
In 52nd AnnualMeeting of the Association for Computational Lin-guistics, page to appear, Baltimore, MD, USA, June.Michel Galley and Christopher D. Manning.
2008.A simple and effective hierarchical phrase reorder-ing model.
In Proceedings of the Conference onEmpirical Methods in Natural Language Process-ing, EMNLP ?08, pages 848?856, Stroudsburg, PA,USA.
Association for Computational Linguistics.Felix A. Gers, J?urgen Schmidhuber, and Fred Cum-mins.
2000.
Learning to forget: Contin-ual prediction with LSTM.
Neural computation,12(10):2451?2471.Felix A. Gers, Nicol N. Schraudolph, and J?urgenSchmidhuber.
2003.
Learning precise timing withlstm recurrent networks.
The Journal of MachineLearning Research, 3:115?143.Joshua Goodman.
2001.
Classes for fast maximumentropy training.
In Acoustics, Speech, and SignalProcessing, 2001.
Proceedings.(ICASSP?01).
2001IEEE International Conference on, volume 1, pages561?564.
IEEE.Alex Graves and J?urgen Schmidhuber.
2005.
Frame-wise phoneme classification with bidirectionalLSTM and other neural network architectures.
Neu-ral Networks, 18(5):602?610.James Henderson.
2004.
Discriminative training of aneural network statistical parser.
In Proceedings ofthe 42nd Annual Meeting on Association for Compu-tational Linguistics, page 95.
Association for Com-putational Linguistics.Sepp Hochreiter and J?urgen Schmidhuber.
1997.Long short-term memory.
Neural computation,9(8):1735?1780.Yuening Hu, Michael Auli, Qin Gao, and Jianfeng Gao.2014.
Minimum translation modeling with recur-rent neural networks.
In Proceedings of the 14thConference of the European Chapter of the Associ-ation for Computational Linguistics, pages 20?29,Gothenburg, Sweden, April.
Association for Com-putational Linguistics.Matthias Huck, Joern Wuebker, Felix Rietig, and Her-mann Ney.
2013.
A phrase orientation model for hi-erarchical machine translation.
In ACL 2013 EighthWorkshop on Statistical Machine Translation, pages452?463, Sofia, Bulgaria, August.Nal Kalchbrenner and Phil Blunsom.
2013.
Recurrentcontinuous translation models.
In Proceedings ofthe 2013 Conference on Empirical Methods in Natu-ral Language Processing, pages 1700?1709, Seattle,Washington, USA, October.
Association for Compu-tational Linguistics.Reinhard Kneser and Hermann Ney.
1995.
Improvedbacking-off for M-gram language modeling.
In Pro-ceedings of the International Conference on Acous-tics, Speech, and Signal Processingw, volume 1,pages 181?184, May.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical Phrase-Based Translation.
In Pro-ceedings of the 2003 Meeting of the North Ameri-can chapter of the Association for ComputationalLinguistics (NAACL-03), pages 127?133, Edmon-ton, Alberta.Hai Son Le, Alexandre Allauzen, and Franc?ois Yvon.2012.
Continuous Space Translation Models withNeural Networks.
In Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages39?48, Montreal, Canada, June.Xunying Liu, Yongqiang Wang, Xie Chen, Mark J. F.Gales, and Phil C. Woodland.
2014.
Efficient latticerescoring using recurrent neural network languagemodels.
In Acoustics, Speech and Signal Processing(ICASSP), 2014 IEEE International Conference on,pages 4941?4945.
IEEE.Gr?egoire Mesnil, Xiaodong He, Li Deng, and YoshuaBengio.
2013.
Investigation of recurrent-neural-network architectures and learning methods for spo-ken language understanding.
In Interspeech, pages3771?3775.Tomas Mikolov, Stefan Kombrink, Lukas Burget,JH Cernocky, and Sanjeev Khudanpur.
2011.Extensions of recurrent neural network languagemodel.
In Acoustics, Speech and Signal Processing24(ICASSP), 2011 IEEE International Conference on,pages 5528?5531.
IEEE.Robert C. Moore and William Lewis.
2010.
IntelligentSelection of Language Model Training Data.
In ACL(Short Papers), pages 220?224, Uppsala, Sweden,July.Frederic Morin and Yoshua Bengio.
2005.
Hierarchi-cal probabilistic neural network language model.
InProceedings of the international workshop on artifi-cial intelligence and statistics, pages 246?252.Franz Josef Och.
2003.
Minimum Error Rate Trainingin Statistical Machine Translation.
In Proc.
of the41th Annual Meeting of the Association for Compu-tational Linguistics (ACL), pages 160?167, Sapporo,Japan, July.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a Method for AutomaticEvaluation of Machine Translation.
In Proceed-ings of the 41st Annual Meeting of the Associa-tion for Computational Linguistics, pages 311?318,Philadelphia, Pennsylvania, USA, July.David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.Williams.
1986.
Learning Internal Representationsby Error Propagation.
In: J. L. McClelland, D. E.Rumelhart, and The PDP Research Group: ?Paral-lel Distributed Processing, Volume 1: Foundations?.The MIT Press.Mike Schuster and Kuldip K. Paliwal.
1997.
Bidirec-tional recurrent neural networks.
IEEE Transactionson Signal Processing, 45(11):2673?2681.Holger Schwenk, Daniel D?echelotte, and Jean-LucGauvain.
2006.
Continuous Space Language Mod-els for Statistical Machine Translation.
In Proceed-ings of the COLING/ACL 2006 Main ConferencePoster Sessions, pages 723?730, Sydney, Australia,July.Holger Schwenk.
2012.
Continuous Space TranslationModels for Phrase-Based Statistical Machine Trans-lation.
In 25th International Conference on Compu-tational Linguistics (COLING), pages 1071?1080,Mumbai, India, December.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A Study ofTranslation Edit Rate with Targeted Human Annota-tion.
In Proceedings of the 7th Conference of the As-sociation for Machine Translation in the Americas,pages 223?231, Cambridge, Massachusetts, USA,August.Andreas Stolcke.
2002.
SRILM ?
An Extensible Lan-guage Modeling Toolkit.
In Proc.
of the Int.
Conf.on Speech and Language Processing (ICSLP), vol-ume 2, pages 901?904, Denver, CO, September.Martin Sundermeyer, Ralf Schl?uter, and Hermann Ney.2012.
LSTM neural networks for language model-ing.
In Interspeech, Portland, OR, USA, September.Martin Sundermeyer, Ilya Oparin, Jean-Luc Gauvain,Ben Freiberg, Ralf Schl?uter, and Hermann Ney.2013.
Comparison of feedforward and recurrentneural network language models.
In IEEE Interna-tional Conference on Acoustics, Speech, and SignalProcessing, pages 8430?8434, Vancouver, Canada,May.Ashish Vaswani, Yinggong Zhao, Victoria Fossum,and David Chiang.
2013.
Decoding with large-scale neural language models improves translation.In Proceedings of the 2013 Conference on Em-pirical Methods in Natural Language Processing,pages 1387?1392, Seattle, Washington, USA, Oc-tober.
Association for Computational Linguistics.David Vilar, Daniel Stein, Matthias Huck, and Her-mann Ney.
2010.
Jane: Open source hierarchi-cal translation, extended with reordering and lexi-con models.
In ACL 2010 Joint Fifth Workshop onStatistical Machine Translation and Metrics MATR,pages 262?270, Uppsala, Sweden, July.Paul J. Werbos.
1990.
Backpropagation through time:what it does and how to do it.
Proceedings of theIEEE, 78(10):1550?1560.Ronald J. Williams and David Zipser.
1995.
Gradient-Based Learning Algorithms for Recurrent Net-works and Their Computational Complexity.
In:Yves Chauvain and David E. Rumelhart: ?Back-Propagation: Theory, Architectures and Applica-tions?.
Lawrence Erlbaum Publishers.Joern Wuebker, Arne Mauser, and Hermann Ney.2010.
Training phrase translation models withleaving-one-out.
In Proceedings of the 48th AnnualMeeting of the Assoc.
for Computational Linguistics,pages 475?484, Uppsala, Sweden, July.Joern Wuebker, Matthias Huck, Stephan Peitz, MalteNuhn, Markus Freitag, Jan-Thorsten Peter, SaabMansour, and Hermann Ney.
2012.
Jane 2: Opensource phrase-based and hierarchical statistical ma-chine translation.
In International Conference onComputational Linguistics, pages 483?491, Mum-bai, India, December.Joern Wuebker, Stephan Peitz, Felix Rietig, and Her-mann Ney.
2013.
Improving statistical machinetranslation with word class models.
In Conferenceon Empirical Methods in Natural Language Pro-cessing, pages 1377?1381, Seattle, USA, October.25
