Proceedings of CLIAWS3, Third International Cross Lingual Information Access Workshop, pages 38?45,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsInvestigation in Statistical Language-Independent Approaches forOpinion Detection in English, Chinese and JapaneseOlena Zubaryeva Jacques SavoyInstitute of Informatics Institute of InformaticsUniversity of Neuch?tel University of Neuch?telEmile-Argand, 11, 2009 Switzerland Emile-Argand, 11, 2009 Switzerlandolena.zubaryeva@unine.ch jacques.savoy@unine.chAbstractIn this paper we present a new statistical ap-proach to opinion detection and its?
evalua-tion on the English,  Chinese and Japanesecorpora.
Besides,  the  proposed  method  iscompared  with  three  baselines,  namelyNa?ve  Bayes  classifier,  a  language  modeland an approach based on significant collo-cations.
These models being language inde-pendent are improved with the use of lan-guage-dependent technique on the exampleof  the  English  corpus.
We show that  ourmethod almost  always  gives  better  perfor-mance  compared  to  the  considered  base-lines.1 IntroductionThe task of opinion mining has received atten-tion  from  the  research  community  and  industrylately.
The main reasons for extensive research inthe area are the growth of user needs and compa-nies?
desire to analyze and exploit the user-gener-ated content on the Web in the form of blogs anddiscussions.
Thus, users want to search for opin-ions on various topics from products that they wantto buy to  opinions  about  events and well-knownpersons.
A lot of businesses are interested in howtheir  services  are  perceived  by  their  customers.Therefore,  the  detection  of  subjectivity  in  thesearched information may add the additional valueto the interpretation of the results and their relevan-cy to the searched topic.
The growth of user activi-ty on the Web gives substantial amounts of data forthese purposes.In the context of globalization the possibility toprovide search of opinionated information in dif-ferent natural languages might be of great interestto  organizations  and  communities  around  theworld.
Our goal is to design a fully automatic sys-tem capable of working in a language-independentmanner.
In order to compare our approach on dif-ferent languages we chose English, traditional Chi-nese and Japanese corpora.
As a further possibilityto improve the effectiveness of the language inde-pendent  methods  we also consider the  additionalapplication of language dependent techniques spe-cific to the particular natural language.The related work in opinion detection is present-ed in Section 2.
We describe our approach in detailwith  the  three  other  baselines  in  Section  3.
Thefourth  section  describes  language  specific  ap-proach used for the English corpus.
In Section 5we present the evaluation of the three models usingthe NTCIR-6 and NTCIR-7 MOAT English, Chi-nese  and  Japanese  test  collections  (Seki  et  al.,2008).
The main findings of our study and futureresearch possibilities are discussed in the last sec-tions.2 Related WorkThe focus of our work is to propose a generalapproach that can be easily deployed for differentnatural languages.
This task of opinion detection isimportant  in  many  areas  of  NLP  such  asquestion/answering,  information  retrieval,  docu-38ment classification and summarization, and infor-mation  filtering.
There  are  numerous  challengeswhen trying to solve the task of opinion detection.Some of them include the fact that the distinctionbetween opinionated and factual could be denotedby a single word in the underlying text (e.g., ?TheiPhone  price  is  $600.?
vs.  ?The  iPhone  price  ishigh.?).
Most  importantly  evaluating  whether  ornot a given sentence conveys an opinion could bequestionable when judged by different people.
Fur-ther, the opinion classification can be done on dif-ferent levels, from documents to clauses in the sen-tence.We consider the opinion detection task on a sen-tence level.
After retrieving the relevant sentencesusing any IR system we automatically classify asentence according to two classes: opinionated andnot opinionated (factual).
When viewing an opin-ion-finding task as a classification task, it is usual-ly  considered  as  a  supervised  learning  problemwhere a statistical model performs a learning taskby  analyzing  a  pool  of  labeled  sentences.
Twoquestions must therefore be solved, namely defin-ing an effective classification algorithm and deter-mining  pertinent  features  that  might  effectivelydiscriminate between opinionated and factual sen-tences.
From  this  perspective,  during  the  lastTREC  opinion-finding  task  (Macdonald et  al.,2008) and the last NTCIR-7 workshop (Seki et al,2008), a series of suggestions surfaced.As  the  language-dependent  approach  variousteams  proposed  using  Levin  defined  verb  cate-gories  (namely,  characterize,  declare,  conjecture,admire,  judge, assess,  say,  complain, advise)  andtheir features (a verb corresponding to a given cat-egory occurring in the analyzed information item)that  may  be  pertinent  as  a  classification  feature(Bloom  et  al.,  2007).
However,  words  such  asthese  cannot  always  work correctly as  clues,  forexample with the word ?said?
in the two sentences?There were crowds and crowds of people at theconcert,  said  Ann?
and  ?There  were  more  than10,000 people at the concert, said Ann.?
Both sen-tences contain the clue word ?said?
but  only thefirst one contains an opinion on the target product.Turney (2002) suggested comparing the frequencyof  phrase  co-occurrences  with  words  predeter-mined  by  the  sentiment  lexicon.
Specific  to  theopinion  detection  in  Chinese  language  Ku et  al.
(2006) propose a dictionary-based approach for ex-traction and summarization.
For the Japanese lan-guage  in  the  last  NTCIR-6  and  NTCIR-7  work-shops the opinion finding methods included the useof  supervised  machine  learning  approaches  withspecific selection of certain parts-of-speech (POS)and sentence parts in the form of  n-gram featuresto improve performance.There  has  been  a  trend  in  applying  languagemodels  for  opinion  detection  task  (Lavrenko,Croft, 2001).
Pang & Lee (2004) propose the useof language models for sentiment analysis task andsubjectivity extraction.
Usually,  language  modelsare  trained on the  labeled data  and as  an outputthey give probabilities of classified tokens belong-ing to the class.
Eguchi & Lavrenko (2006) pro-pose the use of probabilistic language models forranking the results not only by sentiment but alsoby the topic relevancy.As  an  alternative  other  teams  during  the  lastTREC and NTCIR evaluation campaigns have sug-gested  variations  of  Na?ve  Bayes  classifier,  lan-guage models and SVM, along with the use of suchheuristics  as  word  order,  punctuation,  sentencelength, etc.We might also mention OpinionFinder (Wilsonet al, 2005), a more complex system that performssubjectivity analyses to identify opinions as well assentiments  and other  private  states  (speculations,dreams, etc.).
This system is based on various clas-sical  computational  linguistics  components  (tok-enization, part-of-speech (POS) tagging (Toutano-va  &  Manning,  2000)  as  well  as  classificationtools.
For example, a Na?ve Bayes classifier (Wit-ten & Frank, 2005) is used to distinguish betweensubjective  and  objective  sentences.
A  rule-basedsystem is included to identify both speech events(?said,?
?according to?)
and direct  subjective ex-pressions (?is happy,?
?fears?)
within a given sen-tence.
Of  course  such  learning  system  requiresboth a training set  and a deeper knowledge of agiven  natural  language  (morphological  compo-nents, syntactic analyses, semantic thesaurus).The lack of enough training data for the learn-ing-based  systems  is  clearly  a  drawback.
More-over, it is difficult to objectively establish when acomplex learning system has enough training data(and to objectively measure the amount of trainingdata needed in a complex ML model).393 Language Independent ApproachesIn  this  section  we  propose  our  statistical  ap-proach for opinion detection as well as the descrip-tion of the Na?ve Bayes and language model (LM)baselines.3.1  Logistic ModelOur system is based on two components: the ex-traction and weighting of useful features (limitedto isolated words in this study) to allow an effec-tive  classification,  and  a  classification  scheme.First, we present the feature extraction approach inthe Section 3.1.1.
Next, we discuss our classifica-tion model.
Sections 3.2 and 3.3 describe the cho-sen baselines.3.1.1 Features ExtractionIn order to determine the features that can helpdistinguishing  between  factual  and  opinionateddocuments, we have selected the tokens.
As shownby Kilgarriff (2001), the selection of words (or ingeneral features) in an effort to characterize a par-ticular  category  is  a  difficult  task.
The  goal  istherefore to design a method capable of selectingterms that clearly belong to one of the classes.
Theapproaches that use words and their frequencies ordistributions are usually based on a contingency ta-ble (see Table 1).S C-?
a b a+bnot ?
c d c+da+c b+d n=a+b+c+dTable 1.
Example of a contingency table.In this table, the letter a represents the number ofoccurrences (tokens) of  the word  ?
in the docu-ment set S (corresponding to a subset of the largercorpus C in the current study).
The letter b denotesthe number of tokens of the same word ?
?in therest of the corpus (denoted C-) while a+b is the to-tal number of occurrences in the entire corpus (de-noted C with C=C-?S).
Similarly,  a+c indicatesthe total number of tokens in S.  The entire corpusC corresponds to the union of the subset S and C-that contains n tokens (n = a+b+c+d).Based on the MLE (Maximum Likelihood Esti-mation) principle the values shown in a contingen-cy table could be used to estimate various probabil-ities.
For example we might calculate the probabil-ity of the occurrence of the word  ?
in the entirecorpus C as Pr(?)
= (a+b)/n or the probability offinding in C a word belonging to the set S as Pr(S)= (a+c)/n.Now to define the discrimination power a term?, we suggest defining a weight attached to it ac-cording to Muller's method (Muller, 1992).
We as-sume that the distribution of the number of tokensof the word ?
follows a binomial distribution withthe parameters p and n'.
The parameter p represent-ed the probability of drawing a word ?
also denot-ed in the corpus C (or Pr(?))
and could be estimat-ed as (a+b)/n.
If we repeat this drawing n' =  a+ctimes, we will have an estimate of the number ofword  ?
included in the subset S by Pr(?).n'.
Onthe other hand, Table 1 gives also the number ofobservations of the word ?
in S, and this value isdenoted by a.
A large difference between a and theproduct  Pr(?
).n' is  clearly  an  indication  that  thepresence of a occurrences of the term ?
is not dueby chance but corresponds to an intrinsic charac-teristic of the subset S compared to the subset C-.In order to obtain a clear rule, we suggest com-puting the Z score attached to each word ?.
If themean of a binomial distribution is Pr(?
).n', its vari-ance is n'.Pr(?).(1-Pr(?)).
These two elements areneeded to compute the standard score as describedin Equation 1.))Pr(1()Pr(`)Pr(`)(????????
?= nnaZscore         (1)As a decision rule we consider the words havinga Z score between -2 and 2 as terms belonging to acommon vocabulary, as compared to the referencecorpus  (as  for  example  ?will,?
?with,?
?many,??friend,?
or ?forced?
in our example).
This thresh-old was chosen arbitrary.
A word having a Z score> 2 would be considered as overused (e.g., ?that,??should,?
?must,?
?not,?
or  ?government?
inMOAT NTCIR-6 English corpus), while a Z score<  -2 would be interpreted  as  an underused  term(e.g.,  ?police,?
?cell,?
?year,?
?died,?
or ?accord-ing?).
The  arbitrary  threshold  limit  of  2  corre-sponds to the limit of the standard normal distribu-tion, allowing us to find around 5% of the observa-40tions (around 2.5% less than -2 and 2.5% greaterthan 2).
As shown in Figure 1, the difference be-tween our arbitrary limit of 2 (drawn in solid line)and the limits delimiting the 2.5% of the observa-tions (dotted line) are rather close.Figure 1.
Distribution of the Z score(MOAT NTCIR-6 English corpus, opinionated).Based  on  a  training  sample,  we  were  able  tocompute the Z score for different words and retainonly those having a large or small Z score value.Such a procedure is repeated for all classificationcategories  (opinionated and factual).
It  is  worthmentioning that such a general scheme may workwith  isolated  words  (as  applied  here)  or  n-gram(that  could be a sequence of either characters orwords), as well as with punctuations or other sym-bols  (numbers,  dollar  signs),  syntactic  patterns(e.g., verb-adjective in comparative or superlativeforms) or other features (presence of proper names,hyperlinks, etc.
)3.1.2 Classification ModelWhen our system needs to determine the opin-ionatedness of  a  sentence,  we first  represent  thissentence as a set of words.
For each word, we canthen retrieve the Z scores for each category.
If allZ scores for all words are judged as belonging tothe  general  vocabulary,  our  classification  proce-dure selects the default category.
If not, we mayincrease the weight associated with the correspond-ing category (e.g., for the opinionated class if theunderlying term is overused in this category).Such a simple additive process could be viewedas a first classification scheme, selecting the classhaving  the  highest  score  after  enumerating  allwords occurring in a sentence.
This approach as-sumes that the word order does not have any im-pact.
We also assume that each sentence has a sim-ilar length.For  this  model,  we  can  define  two  variables,namely  SumOP  indicating the sum of the Z scoreof terms overused in opinionated class (i.e.
Z score> 2) and appearing in the input sentence.
Similarly,we can define SumNOOP for the other class.
How-ever, a large  SumOP value can be obtained by asingle word or by a set of two (or more) words.Thus, it could be useful to consider also the num-ber of words (features) that are overused (or under-used)  in  a  sentence.
Therefore,  we  can  define#OpOver  indicated  the  number  of  terms  in  theevaluated  sentence  that  tends  to  be  overused  inopinionated  documents  (i.e.
Z  score  >  2)  while#OpUnder indicated  the  number  of  terms  thattends to be underused in the class of  opinionateddocuments (i.e.
Z score < -2).
Similarly, we candefine the variables #NoopOver, #NoopUnder, butfor the non-opinionated category.With these additional explanatory variables, wecan compute the corresponding subjectivity scorefor each sentence as follows:NoopUnderNoopOverNoopOverscoreNoopOpUnderOpOverOpOverscoreOp###_###_+=+=(2)As a better way to combine different judgmentswe  suggest  following Le Calv?
& Savoy (2000)and normalize the scores using the logistic regres-sion.
The  logistic  transformation  ?
(x)  given  byeach logistic regression model is defined as:?+?===++ki iiki iixxeex10101)(???
?pi                   (3)where ?i are the coefficients obtained from the fit-ting, xi are the variables, and k is the number of ex-planatory variables.
These coefficients reflect  therelative  importance  of  each  variable  in  the  finalscore.For each sentence, we can compute the ?
(x) cor-responding to the two possible categories and thefinal decision is simply to classify the sentence ac-cording to the max ?
(x) value.
This approach takesaccount of the fact that some explanatory variablesmay have more importance than other in assigningthe correct category.413.2 Na?ve BayesFor  comparison  with  our  logistic  model  wechose three baselines: Na?ve Bayes  and languagemodel and finding significant collocations.
Despiteits simplicity Na?ve Bayes classifier tends to per-form relatively well for various text categorizationproblems  (Witten,  Frank,  2005).
In  accordancewith our approach, we used word tokens as classi-fication features for  the English corpora.
For theChinese  and  Japanese  languages  overlapping  bi-gram approach was used (Savoy, 2005).
The train-ing method estimates the relative frequency of theprobability  that  the  chosen  feature  belongs  to  aspecific  category  using  add-one  smoothing  tech-nique.3.3 Language Model (LM)As a second baseline we use the classificationbased on the language model using overlapping n-gram sequences (n was set to 8) as suggested byPang & Lee (2004, 2005) for the English language.Using  the  overlapping  4-gram  sequence  for  theword  ?company?,  we  obtain:  ?comp?,  ?ompa?,?mpan?, etc.
For the Chinese and Japanese corporabigram approach was applied.
As in Na?ve Bayes,the  language  model  gives  the  probability  of  thesentence belonging to a specific class.
Workingwith relatively large n allows a lot of word tokensto be processed as is, at least for the English lan-guage.3.4 Significant Collocations (SC)Another promising approach among the super-vised learning schemes is the use of collocations oftwo  or  more  words  or  features  (Manning  &Sch?tze, 2000).
This method allows classificationof  instances  based  on  significant  collocationslearned from the labeled data.
Some examples ofthe frequent collocations in the corpora would be?in the?, ?of the?.
The idea of the method is to findsignificant collocations (SC) that occur more in theopinionated  corpus  than  in  the  non-opinionatedone.
In order to do so the model returns the collo-cations  of  two  words  for  the  English  languagebased on the degree to which their counts in theopinionated corpus exceed their expected counts inthe not opinionated one.
As an example for the En-glish opinionated corpus the following collocationswere found: ?are worried?, ?pleaded guilty?, ?ea-ger to?, ?expressed hope?.
Clearly, overlooking thelist  of  new  found  collocations  it  is  possible  tojudge their relevancy.
However, it is not clear howto use this method with the Chinese and Japanesetexts,  since  these  languages  do  not  have  whitespace or other usual delimiters as in English.
In or-der to  solve the  problem of  feature selection wechose  bigram  indexing  on  the  Chinese  andJapanese corpora and searched for significant newcollocations of bigrams.4 Language Dependent ApproachAs the language dependent technique to improvethe obtained classification results  we suggest  theuse  of  SentiWordNet  for  the  English  language(Esuli & Sebastiani, 2006).
Since the vocabulary ofwords in SentiWordNet is quite limited it is not al-ways clear how to combine the objectivity scores.The SentiWordNet  score  was computed  in  thefollowing way: to define the opinionated score ofthe sentence the sum of scores representing that theword  belongs  to  opinionated  category  for  eachword in the sentence is calculated.
The not opin-ionated score of  the sentence is  computed in thesame way with the difference that it is divided bythe number of words in the sentence.
Thus, if opin-ionated  score  is  more  than  not  opinionated  one,there is an opinion, otherwise not.
This is a heuris-tic approach that intuitively takes account of the ra-tionalization  that  there  are  more  not  opinionatedwords  than  opinionated  in  the  sentence.
At  thesame  time  the  presence  of  opinionated  wordweighs more than the presence of the not opinion-ated ones.
Especially, this approach seems to givegood result.5 ExperimentsThe experiment was carried out on the NTCIR-6and NTCIR-7 English news corpora using 10-foldcross-validation  model  on  a  lenient  evaluationstandard  as  described  in  Seki  et  al.
We  do  notquestion the construction and structure of opinionsin  this  data  set,  since  those  questions  were  ad-dressed  at the NTCIR workshops.
Using the Chi-nese and Japanese corpora we can verify the quali-ty  of  the  suggested  language-independent  ap-proaches.425.1 Feature Selection & Evaluation in EnglishFor the evaluation of sentences in English, theassumption of isolated words (bag-of-words) pre-viously stemmed was used by our system.
The cor-pora are comprised of more than 13,400 sentences,4,859 (36.3%)  of  which  are  opinionated.
As  theevaluation metrics precision, recall and F1-measurewere used based on gold standard evaluation pro-vided  by NTCIR workshops  (Seki  et  al.,  2008).The precision and recall  are  weighted equally inour  experiment  but  it  should  be  recognized  thatbased on the system's needs and focus there couldbe more accent on precision or recall.Model Precision Recall F1-measureLogistic model 0.583 0.508 0.543Na?ve Bayes 0.415 0.364 0.388LM 0.350 0.339 0.343SC 0.979 0.360 0.527Table 2.
Evaluation results of 10-fold cross-valida-tion on NTCIR-6 and NTCIR-7 English  corpora.Comparing the results in Table 2 to the baselinesof the Na?ve Bayes classifier and LM evaluated onthe same training and testing sets, we see that lo-gistic  model  outperforms  the  baselines.
In  ouropinion, this is due to the use of more explanatoryvariables that better discriminate between opinion-ated and factual sentences.The use  of  language  dependent  techniques  onthe other hand might  further improve the results.Especially, this seems promising observing the re-sults when using the SentiWordNet on the Englishcorpus.
In Table 3 one can see that the first threemodels show improvement.
Specifically, the preci-sion of the logistic model increased from 0.583 to0.766 (by 31.4%).Model Precision Recall F1-measureLogistic model 0.766 0.488 0.597Na?ve Bayes 0.667 0.486 0.562LM 0.611 0.474 0.534SC 0.979 0.420 0.588Table 3.
Evaluation results of 10-fold cross-valida-tion on NTCIR-6 and NTCIR-7 English corporawith SentiWordNet.When considering the F1-measure, the impact ofthe language-dependent approach shows 9% of im-provement, from 0.543 to 0.597.The way that we incorporated the scores provid-ed by SentiWordNet was done with the help of lin-ear  combination  and  normalization  of  scores  foreach of the models.5.2 Feature Selection & Evaluation in ChineseWe have assumed until now that words can beextracted  from a  sentence in  order  to  define  theneeded features used to determine if the underlyinginformation item conveys an opinion or not.
Work-ing  with  the  Chinese  language  this  assumptiondoes no longer hold.
Therefore, we need to deter-mine indexing units by either applying an automat-ing segmentation approach (based on either a mor-phological  (e.g.,  CSeg&Tag)  or  a  statisticalmethod (Murata & Isahara, 2003)) or consideringn-gram indexing approach (unigram or bigram, forexample).
Finally we may also consider a combi-nation  of  both  n-gram and  word-based  indexingstrategies.Based on the work of Savoy,  2005 we experi-mented  with overlapping  bigram and trigram in-dexing schemes for Chinese.
The experimental re-sults  show that  bigram indexing outperforms  tri-gram on all  three  considered  statistical  methods.Therefore, as features for Chinese we used over-lapping bigrams.The  NTCIR-6  and  NTCIR-7  Chinese  corporaconsisted  of  more  than  14,507  sentences,  9960(68.7%) of which are opinionated.
The results ofall three statistical models performed on the Chi-nese corpora are presented in Table 4.Model Precision Recall F1-measureLogistic model 0.943 0.730 0.823Na?ve Bayes 0.729 0.538 0.619LM 0.581 0.634 0.606SC 0.313 0.898 0.464Table 4.
Evaluation results of 10-fold cross-valida-tion on NTCIR-6 and NTCIR-7 Chinesecorpora.From the results in Table 4 we clearly see thatour  approach  gives  better  performance  and  con-firms the results presented in Tables 2 and 3.
Thesignificant improvement in scores could be due tothe fact that Chinese corpus contains more opin-ionated sentences in relevance to not opinionatedonce.
Thus, the training set for opinionated classi-43fication was much larger compared to the Englishlanguage.
This proves the relevance of more train-ing data for the learning-based systems.
But the di-rect  comparison  with  the  results  on  the  Englishcorpus is not possible.5.3 Feature Selection & Evaluation in JapaneseAs with the Chinese language we face the samechallenges  in  feature  definition  for  the  Japaneselanguage.
After experimenting with bigram and tri-gram we chose bigram strategy for indexing andfeature selection.The NTCIR-6  and  NTCIR-7 Japanese corporaconsisted  of  more  than  11,100  sentences  with4,622  opinionated  sentences  (representing  41.6%of the corpus).
The results of the statistical modelsare shown in Table 5.Model Precision Recall F-measureLogistic model 0.527 0.761 0.623Na?ve Bayes 0.565 0.570 0.567LM 0.657 0.667 0.662SC 0.663 0.856 0.747Table 5.
Evaluation results of 10-fold cross-valida-tion on NTCIR-6 and NTCIR-7 Japanesecorpora.From the results we can see that the significantcollocations  model  outperforms  the  others.
Thiscould be due to the fewer number of opinionatedsentences compared to the Chinese or English cor-pora.
This tends to indicate the necessity of an ex-tensive training data for the logistic model in orderto provide reliable opinion estimates.6 Future Work and ConclusionIn  this  paper  we  presented our  language-inde-pendent approach based on using Z scores and thelogistic  model  to  identify  those  terms  that  ade-quately characterize subsets of the corpus belong-ing to opinionated or non-opinionated classes.
Inthis selection, we focused only on the statistical as-pect (distribution difference) of words or bigrams.Our approach was compared to the three baselines,namely  Na?ve  Bayes  classifier,  language  modeland an approach based on finding significant collo-cations.
We have also demonstrated on the Englishcorpora how we can use the language dependenttechniques to identify the possibility of opinion ex-pressed in the sentences that otherwise were classi-fied as not opinionated by the system.
The use ofSentiWordNet (Esuli & Sebastiani, 2006) in com-bination with our methods yields better results forthe English language.This study was limited to isolated words in En-glish corpus but in further research we could easilyconsider  longer  word  sequences  to  include  bothnoun  and  verb  phrases.
The  most  useful  termswould also then be added to the query to improvethe rank of opinionated documents.
As another ap-proach, we could use the evaluation of co-occur-rence terms of pronouns ?I?
and ?you?
mainly withverbs (e.g., ?believe,?
?feel,?
?think,?
?hate?)
usingpart of speech tagging techniques in order to boostthe rank of retrieved items.Using  freely  available  POS  taggers,  we  couldtake POS information into account (Toutanova &Mannning,  2004)  and hopefully develop  a  betterclassifier.
For  example,  the  presence  of  propernames  and  their  frequency or  distribution  mighthelp us classify a document as being opinionatedor not.
The presence of adjectives and adverbs, to-gether  with their  superlative  (e.g.,  best,  most)  orcomparative (e.g., greater, more) forms could alsobe useful hints regarding the presence of opinionat-ed versus factual information.AcknowledgmentsWe would like to thank the MOAT task organiz-ers at NTCIR-7 for their valuable work.ReferencesBloom, K., Stein, S., & Argamon, S.  2007.
Appraisalextraction  for  news  opinion  analysis  at  NTCIR-6.Proceedings NTCIR-6, NII, Tokyo, pp.
279-289.Eguchi, K., Lavrenko, V. 2006.
Sentiment retrieval us-ing generative models.
Proceedings of EMNLP, Syd-ney, pp.
345-354.Esuli, A., Sebastiani, F. 2006.
SentiWordNet: A publiclyavailable lexical  resource for  opinion mining.
Pro-ceedings LREC?06, Genoa.Kilgarriff, A.
2001.
Comparing corpora.
InternationalJournal of Corpus Linguistics, 6(1):97-133.Ku, L.-W., Liang, Y.-T., Chen, H.-H. 2006.
Opinion ex-traction,  summarization  and  tracking  in  news  andblog  corpora.
Proceedings  of  AAAI-2006  SpringSymposium on Computational  Approaches  to  Ana-lyzing Weblogs, pp.
100-107.Lavrenko, V., Croft, W.B.
2001.
Relevance-based lanu-age models.
SIGIR, New Orleans, LA, pp.
120-127.44Le Calv?, A., Savoy, J.
2000.
Database merging strat-egy based on logistic regression.
Information Pro-cessing & Management, 36(3):341-359.Macdonald,  C.,  Ounis,  I.,  &  Soboroff,  I.
2008.Overview of the TREC-2007 blog track.
In Proceed-ings  TREC-2007,  NIST  Publication  #500-274,  pp.1-13.Manning, C. D., Sch?tze, H. 2000.
Foundations of Sta-tistical Natural Language Processing.
MIT Press.Muller,  C. 1992.
Principes  et  m?thodes de statistiquelexicale.
Champion, Paris.Murata, M., Ma, Q., & Isahara, H. 2003.
Applying mul-tiple  characteristics  and  techniques  to  obtain  highlevels of performance in information retrieval.
Pro-ceedings of NTCIR-3, NII, Tokyo.Pang, B., Lee, L. 2004.
A sentimental education: Senti-ment  analysis  using  subjectivity  summarizationbased  on  minimum  cuts.
Proceedings  of  ACL,Barcelona, pp.
271-278.Pang, B., Lee,  L. 2005.
Seeing stars: Exploiting classrelationships  for  sentiment  categorization  with  re-spect to rating scales.
In Proceedings of the Associa-tion  for  Computational  Linguistics  (ACL),  pp.115-124.Savoy,  J.
2005.
Comparative  study  of  monolingualsearch models for use with asian languages.
ACMTransactions  on  Asian  Language  Information  Pro-cessing, 4(2):163-189.Seki, Y., Evans, D. K., Ku, L.-W., Sun, L., Chen, H.-H.,& Kando, N.  2008.
Overview of multilingual opin-ion analysis task at NTCIR-7.
Proceedings NTCIR-7,NII, Tokyo, pp.
185-203.Toutanova,  K.,  &  Manning,  C.  2000.
Enriching  theKnowledge  Sources  Used  in  a  Maximum  EntropyPart-of-Speech  Tagging.
Proceedings  EMNLP /VLC-2000, Hong Kong, pp.
63-70.Turney, P. 2002.
Thumbs up or thumbs down?
Semanticorientation applied to unsupervised classification ofreviews.
Proceedings of the ACL, Philadelphia (PA),pp.
417-424.Wilson, T., Hoffmann,  P.,  Somasundaran, S.,  Kessler,J., Wiebe, J., Choi, Y., Cardie, C., Riloff, E., & Pat-wardhan,  S.,  2005.
OpinionFinder:   A system forsubjectivity  analysis.
Proceedings  HLT/EMNLP,Vancouver (BC), pp.
34-35.Witten, I.A., & Frank, E.  2005.
Data Mining: Practi-cal Machine Learning Tools and Techniques.
Mor-gan Kaufmann, San Francisco (CA).45
