Proceedings of the NAACL HLT 2013 Demonstration Session, pages 10?13,Atlanta, Georgia, 10-12 June 2013. c?2013 Association for Computational LinguisticsTMTprime: A Recommender System for MT and TM IntegrationAswarth Dara?, Sandipan Dandapat?
?, Declan Groves?
and Josef van Genabith??
Centre for Next Generation Localisation, School of ComputingDublin City University, Dublin, Ireland?
Department of Computer Science and EngineeringIIT-Guwahati, Assam, India{adara, dgroves, josef}@computing.dcu.ie, sdandapat@iitg.ernet.inAbstractTMTprime is a recommender system that fa-cilitates the effective use of both transla-tion memory (TM) and machine translation(MT) technology within industrial languageservice providers (LSPs) localization work-flows.
LSPs have long used Translation Mem-ory (TM) technology to assist the translationprocess.
Recent research shows how MT sys-tems can be combined with TMs in ComputerAided Translation (CAT) systems, selectingeither TM or MT output based on sophis-ticated translation quality estimation withoutaccess to a reference.
However, to date thereare no commercially available frameworks forthis.
TMTprime takes confidence estimationout of the lab and provides a commercially vi-able platform that allows for the seamless inte-gration of MT with legacy TM systems to pro-vide the most effective (least effort/cost) trans-lation options to human translators, based onthe TMTprime confidence score.1 IntroductionWithin the LSP community there is growing interestin the use of MT as a means to increase automationand reduce overall localisation project cost.
Whenhigh-quality MT output is available, translators seesignificant productivity gains over translation fromscratch, but poor MT quality leads to frustrationand wasted time as suggested translations are dis-carded in favour of providing a translation fromscratch.
We present a commercially-relevant soft-ware platform providing a translation confidence es-timation metric and, based on this, a mechanism foreffectively integrating MT with TMs in localisationworkflows.
The confidence metric ensures that only?Author did this work during his post doctoral research atCNGL.those MT outputs that are guaranteed to require lesspost-editing effort than the best corresponding TMmatch are presented to the post-editor (He et al2010a).
The MT is integrated seamlessly, and es-tablished localisation cost estimation models basedon TM technologies still apply as upper bounds.2 Related WorkMT confidence estimation and its relation to existingTM scoring methods, together with how to make themost effective use of both technologies, is an activearea of research.
(Specia, 2011) and (Specia et al 2009, 2010) pro-pose a confidence estimator that relates specificallyto the post-editing effort of translators.
This re-search uses regression on both the automatic scoresassigned to the MT and scores assigned by post-editors and aims to model post-editors?
judgementsof the translation quality between good and bad, oramong three levels of post-editing effort.Our work is an extension of (He et al 2010a,b,c),and uses outputs and features relevant to the TMand MT systems.
We focus on using system exter-nal features.
This is important for cases where theinternals of the MT system are not available, as inthe use of MT as a service in a localisation work-flow.1 Furthermore, instead of having to solve aregression problem, our approach is based on solv-ing an easier binary prediction problem (using Sup-port Vector Machines) and can be easily integratedinto TMs.
(He et al 2010b) present a MT/TM seg-ment recommender, (He et al 2010c) a MT/TM n-best list segment re-ranker and (He et al 2010a) aMT/TM integration method that can use matchingsub-segments in MT/TM combination.
Importantly,1(Specia et al 2009) note that using glass-box featureswhen available, in addition to black-box features, offer onlysmall gains and also incur significant computational effort.10translators can tune the models for precision withoutretraining the models.Related research by (Simard and Isabelle., 2009)focuses on combining TM information into an SMTsystem for improving the performance of the MTwhen a close match already exists within the TM.
(Koehn and Haddow, 2009) presents a post-editingenvironment using information from the phrase-based SMT system Moses.2 (Guerberof, 2009) com-pares the post-editing effort required for TM andMT output, respectively.
(Tatsumi, 2009) studies thecorrelation between automatic evaluation scores andpost-editing effort.3 Translation RecommenderFigure 1: TMTprime WorkflowThe workflow of the translation recommender isshown in Figure 1.
We train MT systems using asignificant portion of the training data and use thesemodels as well as TM outputs to obtain a recommen-dation development data set.
MT systems can beeither in-house, e.g.
a Moses-based system, or ex-ternally available systems, such as Microsoft Bing3or Google Translate.4 For each sentence in the de-velopment data set, we have access to the referenceas well as to the outputs for each of the MT and TMsystems.
We then select the best MT (or TM) outputas the translation with the lowest TER score withrespect to the reference and label the data accord-ingly.
System-independent features for each trans-lation output are fed as input to the SVM classi-fier (Cortes and Vapnik, 1995).
The SVM classi-fier outputs class labels and the class labels are con-verted into confidence scores using the techniquesgiven in (Lin et al 2007).
Relying on system inde-pendent black-box features has allowed us to build2http://www.statmt.org/moses/3http://www.bing.com/translator4http://translate.google.com/a fully extendable platform that will allow any num-ber of MT systems (or indeed TM systems) to beplugged into the recommender with little effort.4 Demo DescriptionUsing the Amazon EC25 deployment as a back-end,we have developed a front-end GUI for the system(Figure 2).
The interface allows the user to selectwhich of the available translation systems (whetherthey be MT or TM) they wish to use within the rec-ommender system.
The user can input their ownpre-established estimated cost of post-editing, basedon error ranges.
Typically the costs for post-editingthose translations which have a lower-error rate (i.e.fewer errors) is less than the cost for post-editingtranslations which have a greater number of errors,as they are of lower quality.
The user is requested toupload a file for translation to the system.Figure 2: TMTprime GUIOnce the user has selected their desired options,the TMTprime platform provides various analysismeasures based on its recommendation engine, suchas how many segments from the input file are recom-mended for translation by the various selected trans-lation engines or TMs available.
Based on the inputcosts, it provides a visualisation of overall estimatedcost of either using an individual translation systemon its own, or using the recommender selecting thebest performing system on a segment-by-segmentbasis.
The TMTprime system is an implementa-tion of a segment-based system selector selectingthe most appropriate available translation/TM sys-tem for a given input.
A snapshot of the results pro-duced by TMTprime is given in Figure 3: the pie-chart shows what percentage of segments are rec-ommended from each of the translation systems; the5http://aws.amazon.com/ec2/11bar-graph gives an estimated cost of using a singletranslation system alone and the estimated cost whenusing TMTprime?s combined recommendation.
Theestimated cost using TMTprime is lower when com-pared to using a single MT or TM system alone(in the worst case, it will be the same as the best-performing single translation engine or TM system).This estimated cost includes both the cost for trans-lation (currently uniform cost for each translationsystem) and the cost required for post-editing.
Forexample, if the MT is an in-house system the costof translation will be (close to) zero whereas there ispotentially an additional base cost for using an exter-nal MT engine.
Finally, the interface provides statis-tics related to various confidence levels for differenttranslation outputs across the various translation andTM systems.Figure 3: Results shown by TMTprime system5 Experiments and ResultsEvaluation targets two objectives and is describedbelow.5.1 Correlation with Automatic MetricsTER and METEOR are widely-used automatic met-rics (Snover et al 2006; Denkowski and Lavie,2011) that calculate the quality of translation out-put by comparing it against a human translation,known as the reference translation.
Our data setsfor the experiment consist of English-French trans-lation memories from the IT domain.
In all instancesMT was carried out for English-French translations.As we have access to the reference target languagetranslations for our test set, we are able to calculatethe TER and METEOR scores for the three trans-lation outputs (here TM, MaTrEx (Dandapat et al2010) and Microsoft Bing).
For each sentence in thetest set, TMTprime recommends a particular transla-tion output with a certain estimated confidence levelwithout access to a reference.
We measure Pearson?scorrelation coefficient (Hollander and Wolfe, 1999)between the recommendation scores, TER scoresand METEOR scores (for all system outputs) in or-der to determine how well the TMTprime predictionscore correlates with the widely used automatic eval-uation metrics.
Results of these experiments are pro-vided in Table 1 which shows there is a negative cor-relation between TMTprime scores and TER scores.This shows that both TMTprime scores and TERscores are moving in opposite directions, supportingthe claim that the higher the recommendation scores,the lower the TER scores.
As TER is an error score,the lower the TER score, the higher the quality ofthe machine translation output compared to its refer-ence.
On the other hand, TMTprime scores are pos-itively correlated with METEOR scores which sup-ports the claim that the higher the recommendationscores, the higher the METEOR scores.Pearson?s r TER METEORTMTprime -0.402 0.447Table 1: Correlation with automatic metricsThe evaluation has been performed on a test dataset of 2,500 sentences.
Both the correlations are sig-nificant at the (p<0.01) level.5.2 Correlation with Post-Editing timeThis is the most important and crucial metric for theevaluation.
For this experiment we made use of post-editing data captured during a real-world translationtask, for English-French in the IT domain.Pearson?s r TER METEOR PE TimeTMTprime -0.122 0.129 -0.132Table 2: Correlation with Post-Editing timesFor testing, we collect the post-editing times forMT outputs from two different translators using acommercial computer-aided translation (CAT tool)in a real-world production scenario.
The data setconsists of 1113 samples and is different from theone used in the correlation with automatic metrics.12Post-editing times provide a real measure of theamount of post-editing effort required to perfect theoutput of the MT system.
For this experiment, wetook the output of the MT system used in the task to-gether with the post-editing times and measured thePearsons correlation coefficient between the TMT-prime recommendation scores and the post-editing(PE) times (only for MT output from a single sys-tem since this data set does contain PE times forother translation outputs).
In addition, we also re-peated the previous experiment setup for finding thecorrelation between the TMTprime scores and theautomatically-produced TER, METEOR scores forthis data set.
The results are given in Table 2.The results show that the confidence scores docorrelate with automatic evaluation metrics andpost-editing times.
Although the correlations do notseem as strong as before, the results are statisticallysignificant (p<0.01).6 Conclusions and Future WorkWe present a commercially viable translation recom-mender system which selects the best output frommultiple TM/MT outputs.
We have shown that ourconfidence score correlates with automatic metricsand post-editing times.
For future work, we arelooking into extending and evaluating the system fordifferent language pairs and data sets.AcknowledgmentsThis work is supported by Science Foundation Ire-land (Grants SFI11-TIDA-B2040 and 07/CE/I1142) aspart of the Centre for Next Generation Localisation(www.cngl.ie) at Dublin City University.
We would alsolike to thank Symantec, Autodesk and Welocalize fortheir support and provision of data sets used in our ex-periments.ReferencesCortes, Corinna and Vladimir Vapnik.
1995.
Support-vectornetworks.
In Machine Learning.
pages 273?297.Dandapat, Sandipan, Mikel L. Forcada, Declan Groves, Ser-gio Penkale, John Tinsley, and Andy Way.
2010.
OpenMa-TrEx: A free/open-source marker-driven example-based ma-chine translation system.
In Proceedings of the 7th interna-tional conference on Advances in natural language process-ing.
Springer-Verlag, Berlin, Heidelberg, IceTAL?10, pages121?126.Denkowski, Michael and Alon Lavie.
2011.
Meteor 1.3: Auto-matic metric for reliable optimization and evaluation of ma-chine translation systems.
In Proceedings of the EMNLP2011 Workshop on Statistical Machine Translation.
Edin-burgh, UK.Guerberof, Ana.
2009.
Productivity and quality in mt post-editing.
In Proceedings of Machine Translation Summit XII- Workshop: Beyond Translation Memories: New Tools forTranslators.
Ottawa, Canada.He, Yifan, Yanjun Ma, J Roturier, Andy Way, and Josef vanGenabith.
2010a.
Improving the post-editing experience us-ing translation recommendation: A user study.
In Proceed-ings of the Ninth Conference of the Association for Ma-chine Translation in the Americas.
Denver, Colorado, AMTA2010, pages 247?256.He, Yifan, Yanjun Ma, Josef van Genabith, and Andy Way.2010b.
Bridging smt and tm with translation recommenda-tion.
In Proceedings of the 48th Annual Meeting of the Asso-ciation for Computational Linguistics.
Association for Com-putational Linguistics, Uppsala, Sweden, ACL 2010, pages622?630.He, Yifan, Yanjun Ma, Andy Way, and Josef van Genabith.2010c.
Integrating n-best smt outputs into a tm system.
InProceedings of the 23rd International Conference on Com-putational Linguistics: Posters.
Association for Computa-tional Linguistics, Beijing, China, COLING 2010, pages374?382.Hollander, Myles and Douglas A. Wolfe.
1999.
NonparametricStatistical Methods.
John Wiley and Sons.Koehn, Philip and Barry Haddow.
2009.
Interactive assis-tance to human translators using statistical machine trans-lation methods.
In Proceedings of MT Summit XII.
Ottawa,Canada, pages 73?80.Lin, Hsuan-Tien, Chih-Jen Lin, and Ruby C. Weng.
2007.
Anote on platt?s probabilistic outputs for support vector ma-chines.
Machine Learning 68(3):267?276.Simard, Michael and Pierre Isabelle.
2009.
Phrase-based ma-chine translation in a computer-assisted translation environ-ment.
In Proceedings of Machine Translation Summit XII.Ottawa, Canada, pages 120?127.Snover, Matthew, Bonnie Dorr, Richard Schwartz, Linnea Mic-ciulla, and John Makhoul.
2006.
A study of translation editrate with targeted human annotation.
In Proceedings of Asso-ciation for Machine Translation in the Americas.
Cambridge,MA, pages 223?231.Specia, Lucia.
2011.
Exploiting objective annotations for mea-suring translation post-editing effort.
In Proceedings of the15th Annual Conference of the European Association forMachine Translation.
Leuven, Belgium, EAMT 2011, pages73?80.Specia, Lucia, Nicola Cancedda, and Marc Dymetman.
2010.
Adataset for assessing machine translation evaluation metrics.In Proceedings of LREC 2010.
Valletta, Malta.Specia, Lucia, Marco Turqui, Zhuoran Wang, John Shawe-Taylor, and Craig Saunders.
2009.
Improving the confidenceof machine translation quality estimates.
In Proceedingsof Machine Translation Summit XII.
Ottawa, Canada, pages136?143.Tatsumi, Midori.
2009.
Correlation between automatic evalua-tion scores, post-editing speed and some other factors.
InProceedings of Machine Translation Summit XII.
Ottawa,Canada, pages 332?339.13
