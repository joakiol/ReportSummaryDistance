Visualization of Protocols of the Parsing and SemanticInterpretation Steps in a Machine Translation SystemUl r i ch  GermannUSC Information Sciences InstituteMarina del Rey, CA 90292email: germannC~isi.eduAbst rac tIn this paper, we describe a tool for the vi-sualization of process protocols produced bythe parsing and semantic interpretation mod-ules in a complex machine translation system.These protocols tend to reach considerable sizes,and error tracking in them is tedious and time-consuming.
We show how the data in the pro-tocols can be made more easily accessible byextracting a procedural trace, by splitting theprotocols into a collection of cross-linked hyper-text files, by indexing the files, and by usingsimple text formatting and sorting of structuralelements.1 In t roduct ionThe tool described in this paper was developedin connection with the Gazelle Machine Transla-tion System (Knight et al, 1995), which is cur-rently under development a the USC Informa-tion Sciences Institute.
At the moment, Gazellecovers machine translation from Japanese andArabic to English.Figure 1 sketches the flow of processing.
Theinput text is first segmented and tagged withmorphological information.
It is then parsedand interpreted semantically.
The result of se-mantic interpretation is finally fed into the textgeneration module.Almost all modules relevant o the discussionhere employ bottom up chart parsing mecha-nisms.
For any given input, they may returnmore than one interpretation, as the sampleparse sequence for the string saw the ape withhis binoculars in Figure 2 illustrates.The processes of parsing and semantic inter-pretation are recorded step by step in processprotocols.
A parse step in our system is equiv-alent to the creation of a new parse node.
Eachnode receives a category label which determinesI Text Submission I\[ Preprocessing \[I Segmentation and Tagging \[\[ Parsing \[\[ Semantic Interpretation \]\[ Text Generation \[\[ Result Presentation \[Figure 1: The machine translation processits behavior in the remainder of the parsing pro-cess.
A new parse node can be created in orderto?
integrate a new text element into the parsespace,?
combine two or more exisisting nodes intoone node, or?
change the category of an existing node bymaking it the daughter of the new node,and assigning a different category label tothe new node.If, for any given range of input text, there aretwo interpretations that result in the same cat-egory label, they are combined into one node.For example, parse step No.
6 in Figure 2 cre-ates a node that comprises two interpretations.Each node in the parse tree also has a fea-ture structure associated with it.
This feature83dominanceV,V' PPV NP with his binocularssa~w the ,apeV ~ N P  precedencethe ape with his binocularsParse Sequence (simplified)Step h V -- sawStep 2: NP ~ the apeStep 3: V' - -  V NPStep 4: PP - -  with his binocularsStepS: NP-- NPPPStep 6: V' -- V NPOR:V' -- V' PPFigure 2: The parse space for the string saw the ape with his binoculars.
(simplified)structure specifies various properties of the con-stituent represented by the respective node.
Afeature structure is a set of attribute-value pairs,where a value can either be atomic (somethingthat cannot be described in terms of having par-ticular features or properties), or an embeddedfeature structure.
Typical forms of representa-tion are attribute-value matrices (AVMs), andgraphs with labeled arcs and nodes.
Figure 3shows a partial description of the word sees (3rdperson singular form of the verb see with themeaning \[see\[) in both formats.During semantic interpretation, the featurestructures introduced by the parser are aug-mented by semantic features assigned to thenodes by semantic interpretation rules (Knightand Hatzivassiloglou, 1995).In the protocols, each node receives an indi-vidual entry which keeps track of the followinginformation:?
the node number, which uniquely identifiesthe node / parse step;?
the category label assigned to the node, e.g.NP for 'noun phrase';?
the nodes dominated by the node, or, if thenode is a terminal node, the correspondinglexical symbol (a word or morpheme) fromthe input text.
Daughter nodes of nonter-minal nodes are referenced by their nodeSURFACESYNSEMseespos ,,erqPERSON 3rd \]NUMBER sg j\[INST IseeJ\]/ SURFACE a ~ _ ~  N"-<:2: "--..-.--..,lsee I *sgFigure 3: Partial description of the word sees asan attribute-value matrix (top) and as a graphwith labeled arcs and nodes (bottom)numbers.
Since ambiguities are packedinto one node (instead of spelling them outby creating multiple nodes), a nonterminalnode may have several sets of daughters (cf.node No.
22 in Figure 4).?
the range of input text covered by the node;84category label corresponding lezical symbol ~ the node is a lerminalnodenode number,.,,,.~ / / range(~~~\] ( ( (SEM ((INSTANCE (*OR* "diameter .... distance across"))))I(LEMMA i~.
regNi )  (SYNSEM ((SYN ((HEAD ((POS-L2 IregNi)))))))(EKE  '~{~") )____~ \[ .
.
.
.feature strucl'~tre (2 N ..... 3 4 (((SEM ((INSTANCE llI))) (CONTEXT ((RIGHT ((SUFFIX +)))))(LEMMA i--.Number\]) (SYNSEM ((SYN ((HEAD ((POS-L2 INumber\[)))))))(TREE ..... )))).
.
.
\](22 NP (~-~'~ ~"~)  ~- -~)  S 9 (*OR* ((SEM ((tiP2 ((INSTANCE (*OR* "globe"\ \ tC'"\] A node may have.
several sets of daughters.The daughters o\] a nonierminaTnode are referenced by lheir node numbers.Figure 4: Sample sections from a process protocol.
The samples were slighty modified and refor-matted in order to achieve a more compact and readable display.?
the feature structure(s) associated with thenode.Figures 4 and 5 show sample sections from aprocess protocol in raw format and as displayedby the visualization tool.2 Ana lys i s  of the  DataApart from the obvious, explicit information ex-pressed in the protocol entry of each node - -the node number, the immediate daughters ofthe node, and the features associated with it - -the process protocol also contains ome implicitinformation which can be retrieved by goingthrough the protocol as a whole.
For exam-ple, it is possible to check whether a node isused later on in the parsing process and whatits mother node(s) is.
Going down the tree,it can be determined which part of the inputsentence is covered by a particular node.
The'parse forest' (collection of parse trees) can bereconstructed by linking the individual branch-ings (minimal trees consisting of a mother nodeand its daughter node(s)) by virtue of the nodenumbers.
Last but not least, the original inputsentence can be recovered by concatenating theleaf nodes of the tree.Individual parse trees are two-dimensional,the two dimensions being precedence (the lin-ear order in which the elements occur in thesentence) and dominance (th e relation betweena mother node and its daughter node(s)).
Eventhough both relations are preserved in the pro-cess protocols, none of them is immediately ob-vious from the protocols: Nodes that are neigh-bors in terms of linear order, or nodes that areconnected in terms of dominance do not nec-essarily appear next to each other in the parsesequence.
Consider, for example, the parse se-quence in Figure 2.
The node created by step 1is used again in the steps 3 and 6.
Whereas forstep 3 the sister nodes 1 and 2 happen to beadjacent, this is not true for step 6.
Here, thetwo nodes that make up the first interpretationare separated by the nodes 2, 3 and 4.
Equally,the dominance relation between parse nodes isusually obscured by intermediate parse steps.
Ifwe also take into account the issue of ambiguity,we have to deal with it as a third dimension, asillustrated in Figure 2.Keeping in mind the purpose of examiningthe data, which is first and above all error trac-ing and quality control, we can thus distinguishfour aspects of the information contained in theprocess protocols.
These aspects are:the overall structure, i.e., informationabout the constituent structure (parse for-est) of the sentence, and about the way itis built up from lexical items;the details, i.e., information about theproperties of the constitutents a preservedin the feature structures associated with8521: NP?
NP (t~)?
NP (20)\ [ showr~ge\ ]?
P~e.nts: NP (22\] $ (~3\]) S (79)22: NP: m, eo, ~o.
t?
NP~.?
NP(m )of, S,, No.
2?
NP  (%5)?
NP (22)oFtilm No.
3Notle 22, opXlon 2 ~)COMPLEX-  +SEM?
INSTPdqCE - ~and~?
OP10 LNSTANCE - ( 'OR* "house (mte'$ own)" "J~skle")0 MOD0 INSTANCE = (*OR* "asteroid.
~ "planetoid" smal lplanet")?3 NIEKT-MOD0 INSTANCE - ( 'OR."
"ext+~eding" "this i.s a l l ""ahave" "siru:e" "greater thaJ~" "the end" "as longa~" "the ahox~+m, entianed" mnre titan" "b~ra ' t ""rmt less than" "in excess o1" "arulup" "over ' )o N~r-MOV - \[1\]?
OP2 I INSTANCE - (+OR* "globe" ",be earth" "earth" "the,~'arm"),~mmA .
+~..~.~RULE - "NP - )  PiP\[no case\] NP =SYN ?
.
.
.
.
.
.
~-!+ i" l~  ..... ,, " _ .
, .S~SEM =Figure 5: Sample screen.
The top frame displays the original input text, which serves as an indexto the parse sequence in the lower left frame.
The category labels and node numbers in the lowerleft frame are hyperlinks.
A click on a category label will cause the browser to display the featurestructure(s) associated with that node in the frame on the right.
A click on a node number willlead to the respective node within the parse sequence.each node;?
ambiguity, i.e., the number of interpreta-tions associated with each node;?
the relation between the parse sequenceand the input, i.e., information about theconnection of each node in the parse se-quence to the part of the sentence that iscovered by this node, and, in reverse, theconnection of each word or morpheme ofthe input to the node that integrates thisword or morpheme into the parse sequence.Whereas the connection of each node to theinput can be considered a property of the re-spective node, the connection of the input tothe parse sequence is a matter  of indexing thatwill ease access to 'trouble spots' when testingchanges to the syntactic grammar or the seman-tic interpretation rules.3 D isp lay ing  the  Data3.1 Disp lay o f  the  Const i tuentS t ructureThe obvious solution to the issue of visualizingthe Overall constitutent structure of a sentence,namely displaying it as a conventional parse treerepresentation, turns out to be not very practi-cal at closer examination.
Since some parsersemployed in our system may return more thanjust one structure for any given input, and since86partial parses are very relevant for debugging,we are, in fact, not dealing with a single parsetree but rather with a 'parse forest' which maycontain dozens or even hundreds of completeand partial trees.
A development tool that con-fronts the developer with a large number ofspelled-out ambiguities is not satisfactory, be-cause it does not allow efficient access to rele-vant areas in the data space.
Moreover, even incases where there is only one single parse tree,the tree may not fit on the screen, and even if itdoes, there will be little room left to display thefeature structures associated with each node.Therefore, instead of using conventional treerepresentations, the parse sequence is main-tained as such and displayed as a cross-linkedsequence of minimal trees, essentially in thesame way it is recorded in the process protocol.However, it is formatted in a manner that sup-ports the correct interpretation of the symbolsas mother and daughter(s) (cf.
the lower leftframe in Figure 5).
In addition, the explicit in-formation contained in the individual entries ofthe process protocols is augmented by informa-tion that can be inferred from the process proto-col as a whole and will not be visible when justlooking at the protocol.
Note, for example, thatcategory labels are given not only for the mothernode but also for the daughter nodes, and thatthe mother-daughter relation contained in theindividual protocol entry is back-referenced tothe daughter nodes in the HTML representa-tion, so that the subsequent use of the node canbe traced easily.
In contrast, detailed informa-tion about the properties of the individual con-stituents, which tends to obstruct the view ofthe overall structure, is removed from this traceof display and stored in a separate file.
Never-theless it is readily at hand by means of hyper-links, as will be explained immediately below.What is not visible in Figure 5 is that all cate-gory labels, and the node numbers in subscriptafter the category labels of daughter and par-ent nodes, are hyperlinks.
A mouse click on acategory label causes the browser to display thefeature structure associated with that node inthe frame on the right.
A click on the nodenumber leads to the respective node within thesame (left) frame.
This setup provides the userwith the means to easily navigate the parse treeor parse forest, and, at the same time, allowshim or her to inspect the features associatedwith each node in detail.
In both frames, ambi-guities (multiple structures associated with onenode) are preserved by marking the alternativeswith clear headings.
Since parse sequence andfeature structures are separated and displayedin different, adjoining frames in the browser, avery compact yet informative form of data vi-sualization is achieved.3.2 V isua l i zat ion  o f  the  Re la t ionbetween Const i tu tent  S t ructureand  Input  TextBefore we turn to the representation of featurestructures, let us briefly comment on the topframe in Figure 5.
This frame displays the textof the original input sentence and serves twopurposes.
First, a click on the caption \[showrange\]  in either one of the lower frames 1causesthe browser to display the range covered by thatnode in red (as opposed to black for the sur-rounding context), and, secondly, in the 'click-able' mode of the top frame the input sentenceserves as an index: a click on a word leads tothe respective node in the parse sequence in thelower left frame, so that particular points of in-terest can be accessed easily during the gram-mar development process.3.3 Representat ion  o f  FeatureS t ructuresOur approach to the display of feature struc-tures also diverges from representation formatsthat are well established in the literature.
Com-monly used representations - -  representationsas graphs with labeled arcs and as attribute-value matrices (cf.
Figure 3) - -  tend to becometoo large once the feature structures reach a cer-tain complexity.
Moreover, both of them re-quire extensive calculations and graphical pro-cessing, which have to be paid for in terms ofprocessing time.
Instead, our tool representsfeature structures as unordered lists in HTML.Each list element represents an attribute-valuepair.
Atomic values are separated from theirattributes by an equal sign (=); complex values(embedded feature structures) are representedas embedded lists.1The caption is not visible in the lower ight frame inFigure 5.
See Figure 9 for the location of the caption inthe lower right frame.87"INST \[prize(giftl v \]bootylv \[trophy, prize\[SEM MOD "INST "best dresser"\]T.-LOC.
"INST \[time unit\[ \].
YEAR \[INST 1199211\]Figure 6: AVM representation of the semanticcontent of the phrase the "Best Dresser" Award1992.?
SEMo INST -- ( ,or, Iprize(gift I IbootylItrophy, prizeDo MODo INST = "best dresser"o T.-LOC.o INST = Itime unit\[o YEARt, INST = \]19921Figure 7: The semantic ontent of the phrasethe "Best Dresser" Award 1992, represented asan unordered list.?
SEMo INST = (*or, \]prize(gift\[ Ibooty I\[trophy, prize I)o MOD I INST = "best dresser"o T.-LOC.o INST = Itime unit\[o YEAR \]INST = \]19921Figure 8: The semantic content of the phrasethe "Best Dresser" Award 1992, represented asan unordered list with path abbreviations.In this representation format, the featurestructure represented by the AVM in Figure 6would be rendered as shown in Figure 7.In order to achieve a more compact represen-tation, we use a convention that is commonplacein the HPSG literature (Pollard and Sag, 1994,and others), namely, we abbreviate single pathsby the use of a vertical bar (I).
Thus the rep-resentation i  Figure 8 is equivalent o the oneshown in Figure 7.3.4 Sor t ing  and  Use  o f  Co lorsIn the process protocols, features usually appearin random order.
In order to ease the locatingof features and the comparison of feature struc-tures, features are generally listed in alphabeti-cal order in the display.
However, since not allinformation contained in the feature structuresis equally important, some features receive spe-cial treatment.The main purpose of the parsing and se-mantic interpretation steps within the transla-tion process is to build up a more language-independent representation f the semantic on-tent of the input text.
This information isstored in the SEM feature of the feature struc-ture associated with each node.
Within thesesubstructures, the INSTANCE feature plays acentral role: it specifies the concept from theSensus ontology 2 (Knight and Luk, 1994; Hovyand Knight, 1993) that the object referred to bythe expression in the source language is an in-stance of.
For example the Japanese xpression1992 year "best dresser" awardrefers to an instance of a prize or award (~)which is modified 3 by the string "<.X b V l / , ,@-- ("best dresser") and temporally located inthe year 1992: the "Best Dresser" Award 1992.Other features on the same 'level' as the IN-STANCE feature specify arguments (in the caseof predicates and relations) and further modifi-cations of the object in question.
In order toaccomodate the central role of the INSTANCEfeature, it is always pushed to the top of thelist of features under its governing attr ibute inthe feature structure, and displayed, togetherwith its value, in red instead of black.
Thisincreases its visibility in the display of the fea-ture structures, so that it can be spotted easily.The lower right frame of the main window inFigure 9 shows the feature structure discussedhere.2If an appropriate concept cannot be found in theontology, an English gloss is used instead.ZThis representation does not specify the nature ofthe modification.
We currently do not have the meansto automatically determine the specific haracter of themodification.88P~.S  P output fort .~  cJu.~.
:~-L~L~-~ I I~ ~~: ~ ' ~ l l : ' a rseNodeNo.
8 :N  \[sho~,rangel \[exit}JUCL~SS:  ~\ [~:~.~?~i  ,!
;: :~:: i !
: .
i  i : : : .
.
.~e~e~.N,~+~- l .
lm~l~Or~Rle~0~,~J  :: ::,: ::, .
:lp~iz~ Eft I:/l~mall~ rmal~ln~l~4usd~ r - dl~vll~l~mengN- - N .sl~ndl~l \[INDEX\] :I: .
.
.
.
.
j (1/I) N->NN j .
; POS_L2 : Nv / i( IN  ->  N N)  ; ;  N=no N / /  N~la  N / /  N=tO H / /  N N : CAT( (xo  $gnsem)  = (?2 sgnsem))( ( ?0 1 ) = ( x2 \[ ) )( (~0 cat)  - (x2  cat)  ) ?
~rBCAT_ i  .
( -xoP -  ( ( (~  s~rlsem ~'oo~ ~c~' lomin~lzer )  =c ~o)i " ( I x0  semi  = Ix2  semi )  : ~EMMA - ~I{..N I ( -xor -  ( ( (x2  MAP N0)  =C (x l  SEN) ) ){ ( (x l  lemma) =c ( -o r -  I -~a~e- I  I *TemDor( (xO sere temp?va l - locat lng)  = Cx l  semi  ?
INSTANCE * { 'OR '~dze<~i f~ i l ; '~mgh~'~z .e J}: : .
.
.
.
.
.  "
: ?
MOD I INSTANCE - "~ dot"( ( (x l  lemma) =C I -Ouant l tg - l l  ?
T~4\]POII.AL-LO,~J~T1NG( (x0  sel~ me(I) = (x l  sere ) I )  0 INST,~,IC?
- timetmit~: : .
.
.
.
.
.
.
.
.
O YEAR I LNSTANCE - tL~$1.~.~1--~ :~77 : .
.
.
.
.
.
: ~ : .
.
:~ .
:.~':;:~:~5:,;~:";,::~: .-.:.
:,-,-..--~,-:7-:-:: '~:Figure 9: Integration of additional information.
From left to right: Dictionary output, grammar,and main window.Also, in order to help the developer distin-guish the different frames and windows (see be-low) of the visualization tool, we use differentbackground colors for different frames.
For ex-ample, the background of the lower right frameof the main window is kept in a light blue,whereas other windows have yellow or whitebackgrounds.4 In tegrat ion  of  Add i t iona lResourcesWhereas the data discussed so far is almost al-ways of interest for the developer, other typesof information are requested less frequently.
Forexample, one may want to check a word's en-try in the dictionary, or see the grammar ulethat was used to create a particular node.
Ded-icating a frame in the main window to the dis-play of this kind of data did not seem an ap-propriate solution.
Display space is far too pre-cious to be wasted on a frame that is used onlyoccasionally.
Instead, this kind of informationis provided in separate browser windows.
Thegrammar rules contain unique IDs that are pre-served in the feature structures associated witheach node.
The converter uses these IDs to setlinks to an HTML version of the grammar.
Thelinks are listed under 'Rules applied' in the lowerright frame (cf.
Figure 9).
For terminal nodes, aclick on the lexical symbol in either of the lowerframes will access the cgi interface of the dic-tionary and deliver the complete lexical infor-mation that is available for the respective n-try.
Figure 9 shows a sample of the developer'sscreen with the main and the two subsidiarywindows.5 Eva luat ionThe prime criterion for the evaluation of a visu-alization tool like the one described is its effecton the efficiency of working with the data.
Sincethe tool was mainly developed for in-house useby a small team of developers, we did not carryout formal experiments in order to assess theincrease in productivity.
Based on my personalexperience in working with both the simple pro-cess protocols and the visualized version as agrammar developer for the Japanese modules,89I estimate the speedup at a factor of two to atleast ten, depending on the size of the struc-ture to be analyzed: the larger the structure,the greater the benefit of visualization.Also, the tool has proven to be valuable fordemonstration purposes, as the structuring pro-vided by this kind of display helps people whohave no previous experience with our systemand maybe are even unfamiliar with natural lan-guage processing in general understand the pro-cess in more detail.
In particular, since manypeople are now familiar with the internet andthe interfaces provided by web browsers, our im-pression is that people who see our system forthe first time tend to be able to focus on the datarather than the interface more quickly.
Again,this claim has not been tested experimentally.6 Techn ica l  In fo rmat ionThe converter was implemented in Perl and cur-rently works in off-line mode only, that is, theHTML files have to be created first and can thenbe used.
Processing time depends very much onthe size of the structures as well as other fac-tors beyond the immediate control of the authorsuch as network traffic and overall processingload on the machine.
On a Sun Ultra, process-ing typically takes between a few seconds forshort sentences (converting the protocol for thesentence in Figure 9 takes about 3 seconds) andseveral minutes for very large and ambiguousstructures.7 Conc lus ionIn this paper, we have presented a tool forthe visualization of a complex parse space byseparating lobal information about the over-all structure of the parse space from detailed,local information while preserving the relationbetween both by means of hyperlinks and in-dexing.
Our solution is anything but fancy: itis completely text-based and employs common-place, off-the-shelf tools for the display of hy-pertexts.
This allows for ease of use and a highdegree of portability, because no new softwarehas to be installed at the user-end.
Even thoughsome compromises had to be made, for exampleby deviating from well-established forms of rep-resentation, the interface nevertheless providesappropriate functionality for the given purpose.In a word, HTML is used for what it was in-vented for - -  the encoding of nonlinear infor-mation.8 AcknowledgementsGazelle is funded by the US Government undercontract MDA904-96-C-1077.The use of unordered lists in HTML for therepresentation f feature structures was inspiredby a similar approach taken in a web inter-face for the Gazelle system developed by PhilippKShn.I am very grateful to Kevin Knight, Ulf Her-mjakob and Daniel Marcu for various commentson earlier drafts of this paper.Re ferencesEduard Hovy and Kevin Knight.
1993.
Moti-vating shared knowledge resources: An exam-ple from the Pangloss collaboration.
In Pro-ceedings of the Workshop on Knowledge Shar-ing and Information Interchange (IJCAI).Kevin Knight and Vasileios Hatzivassiloglou.1995.
Unification-based glossing.
In Proceed-ings of the 14th International Joint Confer-ence on Artificial Intelligence.Kevin Knight and Steve K. Luk.
1994.
Build-ing a large-scale knowledge base for machinetranslation.
In Proceedings of the 12th Na-tional Conference on Artificial Intelligence(AAAI).Kevin Knight, Ishwar Chander, MatthewHaines, Vasileios Hatzivassiloglou, EduardHovy, Masayo Iida, Steve K. Luk, RichardWhitney, and Kenji Yamada.
1995.
Fillingknowledge gaps in a broad-coverage machinetranslation system.
In Proceedings of the 14thInternational Joint Conference on ArtificialIntelligence.Carl Pollard and Ivan A.
Sag.
1994.
Head-Driven Phrase Structure Grammar.
Studiesin Contemporary Linguistics.
University ofChicago Press, Chicago.90
