Discriminative Sentence Compression with Soft Syntactic EvidenceRyan McDonaldDepartment of Computer and Information ScienceUniversity of PennsylvaniaPhiladelphia, PA 19104ryantm@cis.upenn.eduAbstractWe present a model for sentence com-pression that uses a discriminative large-margin learning framework coupled witha novel feature set defined on compressedbigrams as well as deep syntactic repre-sentations provided by auxiliary depen-dency and phrase-structure parsers.
Theparsers are trained out-of-domain and con-tain a significant amount of noise.
We ar-gue that the discriminative nature of thelearning algorithm allows the model tolearn weights relative to any noise in thefeature set to optimize compression ac-curacy directly.
This differs from cur-rent state-of-the-art models (Knight andMarcu, 2000) that treat noisy parse trees,for both compressed and uncompressedsentences, as gold standard when calculat-ing model parameters.1 IntroductionThe ability to compress sentences grammaticallywith minimal information loss is an importantproblem in text summarization.
Most summariza-tion systems are evaluated on the amount of rele-vant information retained as well as their compres-sion rate.
Thus, returning highly compressed, yetinformative, sentences allows summarization sys-tems to return larger sets of sentences and increasethe overall amount of information extracted.We focus on the particular instantiation of sen-tence compression when the goal is to produce thecompressed version solely by removing words orphrases from the original, which is the most com-mon setting in the literature (Knight and Marcu,2000; Riezler et al, 2003; Turner and Charniak,2005).
In this framework, the goal is to find theshortest substring of the original sentence that con-veys the most important aspects of the meaning.We will work in a supervised learning setting andassume as input a training set T =(xt,yt)|T |t=1 oforiginal sentences xt and their compressions yt.We use the Ziff-Davis corpus, which is a set of1087 pairs of sentence/compression pairs.
Fur-thermore, we use the same 32 testing examplesfrom Knight and Marcu (2000) and the rest fortraining, except that we hold out 20 sentences forthe purpose of development.
A handful of sen-tences occur twice but with different compres-sions.
We randomly select a single compressionfor each unique sentence in order to create an un-ambiguous training set.
Examples from this dataset are given in Figure 1.Formally, sentence compression aims to shortena sentence x = x1 .
.
.
xn into a substring y =y1 .
.
.
ym, where yi ?
{x1, .
.
.
, xn}.
We definethe function I(yi) ?
{1, .
.
.
, n} that maps wordyi in the compression to the index of the word inthe original sentence.
Finally we include the con-straint I(yi) < I(yi+1), which forces each wordin x to occur at most once in the compression y.Compressions are evaluated on three criteria,1.
Grammaticality: Compressed sentencesshould be grammatical.2.
Importance: How much of the important in-formation is retained from the original.3.
Compression rate: How much compressiontook place.
A compression rate of 65%means the compressed sentence is 65% thelength of the original.Typically grammaticality and importance aretraded off with compression rate.
The longer our297The Reverse Engineer Tool is priced from $8,000 for a single user to $90,000 for a multiuser project site .The Reverse Engineer Tool is available now and is priced on a site-licensing basis , ranging from $8,000 for a single user to $90,000 for a multiuser project site .Design recovery tools read existing code and translate it into defi nitions and structured diagrams .Essentially , design recovery tools read existing code and translate it into the language in which CASE is conversant ?
defi nitions and structured diagrams .Figure 1: Two examples of compressed sentences from the Ziff-Davis corpus.
The compressed versionand the original sentence are given.compressions, the less likely we are to remove im-portant words or phrases crucial to maintaininggrammaticality and the intended meaning.The paper is organized as follows: Section 2discusses previous approaches to sentence com-pression.
In particular, we discuss the advantagesand disadvantages of the models of Knight andMarcu (2000).
In Section 3 we present our dis-criminative large-margin model for sentence com-pression, including the learning framework andan efficient decoding algorithm for searching thespace of compressions.
We also show how toextract a rich feature set that includes surface-level bigram features of the compressed sentence,dropped words and phrases from the original sen-tence, and features over noisy dependency andphrase-structure trees for the original sentence.We argue that this rich feature set alows themodel to learn which words and phrases shouldbe dropped and which should remain in the com-pression.
Section 4 presents an experimental eval-uation of our model compared to the models ofKnight and Marcu (2000) and finally Section 5discusses some areas of future work.2 Previous WorkKnight and Marcu (2000) first tackled this prob-lem by presenting a generative noisy-channelmodel and a discriminative tree-to-tree decisiontree model.
The noisy-channel model defines theproblem as finding the compressed sentence withmaximum conditional probabilityy = arg maxyP (y|x) = arg maxyP (x|y)P (y)P (y) is the source model, which is a PCFG plusbigram language model.
P (x|y) is the channelmodel, the probability that the long sentence is anexpansion of the compressed sentence.
To calcu-late the channel model, both the original and com-pressed versions of every sentence in the trainingset are assigned a phrase-structure tree.
Given atree for a long sentence x and compressed sen-tence y, the channel probability is the product ofthe probability for each transformation required ifthe tree for y is to expand to the tree for x.The tree-to-tree decision tree model looks torewrite the tree for x into a tree for y.
The modeluses a shift-reduce-drop parsing algorithm thatstarts with the sequence of words in x and the cor-responding tree.
The algorithm then either shifts(considers new words and subtrees for x), reduces(combines subtrees from x into possibly new treeconstructions) or drops (drops words and subtreesfrom x) on each step of the algorithm.
A decisiontree model is trained on a set of indicative featuresfor each type of action in the parser.
These mod-els are then combined in a greedy global searchalgorithm to find a single compression.Though both models of Knight and Marcu per-form quite well, they do have their shortcomings.The noisy-channel model uses a source modelthat is trained on uncompressed sentences, eventhough the source model is meant to represent theprobability of compressed sentences.
The channelmodel requires aligned parse trees for both com-pressed and uncompressed sentences in the train-ing set in order to calculate probability estimates.These parses are provided from a parsing modeltrained on out-of-domain data (the WSJ), whichcan result in parse trees with many mistakes forboth the original and compressed versions.
Thismakes alignment difficult and the channel proba-bility estimates unreliable as a result.
On the otherhand, the decision tree model does not rely on thetrees to align and instead simply learns a tree-to-tree transformation model to compress sentences.The primary problem with this model is that mostof the model features encode properties related toincluding or dropping constituents from the treewith no encoding of bigram or trigram surface fea-tures to promote grammaticality.
As a result, themodel will sometimes return very short and un-grammatical compressions.Both models rely heavily on the output of anoisy parser to calculate probability estimates forthe compression.
We argue in the next section that298ideally, parse trees should be treated solely as asource of evidence when making compression de-cisions to be balanced with other evidence such asthat provided by the words themselves.Recently Turner and Charniak (2005) presentedsupervised and semi-supervised versions of theKnight and Marcu noisy-channel model.
The re-sulting systems typically return informative andgrammatical sentences, however, they do so at thecost of compression rate.
Riezler et al (2003)present a discriminative sentence compressor overthe output of an LFG parser that is a packed rep-resentation of possible compressions.
Though thismodel is highly likely to return grammatical com-pressions, it required the training data be humanannotated with syntactic trees.3 Discriminative Sentence CompressionFor the rest of the paper we use x = x1 .
.
.
xnto indicate an uncompressed sentence and y =y1 .
.
.
ym a compressed version of x, i.e., each yjindicates the position in x of the jth word in thecompression.
We always pad the sentence withdummy start and end words, x1 = -START- andxn = -END-, which are always included in thecompressed version (i.e.
y1 = x1 and ym = xn).In this section we described a discriminative on-line learning approach to sentence compression,the core of which is a decoding algorithm thatsearches the entire space of compressions.
Let thescore of a compression y for a sentence x ass(x,y)In particular, we are going to factor this score us-ing a first-order Markov assumption on the wordsin the compressed sentences(x,y) =|y|?j=2s(x, I(yj?1), I(yj))Finally, we define the score function to be the dotproduct between a high dimensional feature repre-sentation and a corresponding weight vectors(x,y) =|y|?j=2w ?
f(x, I(yj?1), I(yj))Note that this factorization will allow us to definefeatures over two adjacent words in the compres-sion as well as the words in-between that weredropped from the original sentence to create thecompression.
We will show in Section 3.2 howthis factorization also allows us to include featureson dropped phrases and subtrees from both a de-pendency and a phrase-structure parse of the orig-inal sentence.
Note that these features are meantto capture the same information in both the sourceand channel models of Knight and Marcu (2000).However, here they are merely treated as evidencefor the discriminative learner, which will set theweight of each feature relative to the other (pos-sibly overlapping) features to optimize the modelsaccuracy on the observed data.3.1 DecodingWe define a dynamic programming table C[i]which represents the highest score for any com-pression that ends at word xi for sentence x. Wedefine a recurrence as followsC[1] = 0.0C[i] = maxj<i C[j] + s(x, j, i) for i > 1It is easy to show that C[n] represents the score ofthe best compression for sentence x (whose lengthis n) under the first-order score factorization wemade.
We can show this by induction.
If we as-sume that C[j] is the highest scoring compressionthat ends at word xj , for all j < i, then C[i] mustalso be the highest scoring compression ending atword xi since it represents the max combinationover all high scoring shorter compressions plusthe score of extending the compression to the cur-rent word.
Thus, since xn is by definition in everycompressed version of x (see above), then it mustbe the case that C[n] stores the score of the bestcompression.
This table can be filled in O(n2).This algorithm is really an extension of Viterbito the case when scores factor over dynamic sub-strings of the text (Sarawagi and Cohen, 2004;McDonald et al, 2005a).
As such, we can useback-pointers to reconstruct the highest scoringcompression as well as k-best decoding algo-rithms.This decoding algorithm is dynamic with re-spect to compression rate.
That is, the algorithmwill return the highest scoring compression re-gardless of length.
This may seem problematicsince longer compressions might contribute moreto the score (since they contain more bigrams) andthus be preferred.
However, in Section 3.2 we de-fine a rich feature set, including features on wordsdropped from the compression that will help disfa-vor compressions that drop very few words since299this is rarely seen in the training data.
In fact,it turns out that our learned compressions have acompression rate very similar to the gold standard.That said, there are some instances when a staticcompression rate is preferred.
A user may specif-ically want a 25% compression rate for all sen-tences.
This is not a problem for our decodingalgorithm.
We simply augment the dynamic pro-gramming table and calculate C[i][r], which is thescore of the best compression of length r that endsat word xi.
This table can be filled in as followsC[1][1] = 0.0C[1][r] = ??
for r > 1C[i][r] = maxj<i C[j][r ?
1] + s(x, j, i) for i > 1Thus, if we require a specific compression rate, wesimple determine the number of words r that sat-isfy this rate and calculate C[n][r].
The new com-plexity is O(n2r).3.2 FeaturesSo far we have defined the score of a compres-sion as well as a decoding algorithm that searchesthe entire space of compressions to find the onewith highest score.
This all relies on a score fac-torization over adjacent words in the compression,s(x, I(yj?1), I(yj)) = w ?
f(x, I(yj?1), I(yj)).In Section 3.3 we describe an online large-marginmethod for learning w. Here we present the fea-ture representation f(x, I(yj?1), I(yj)) for a pairof adjacent words in the compression.
These fea-tures were tuned on a development data set.3.2.1 Word/POS FeaturesThe first set of features are over adjacent wordsyj?1 and yj in the compression.
These includethe part-of-speech (POS) bigrams for the pair, thePOS of each word individually, and the POS con-text (bigram and trigram) of the most recent wordbeing added to the compression, yj .
These fea-tures are meant to indicate likely words to in-clude in the compression as well as some levelof grammaticality, e.g., the adjacent POS features?JJ&VB?
would get a low weight since we rarelysee an adjective followed by a verb.
We also add afeature indicating if yj?1 and yj were actually ad-jacent in the original sentence or not and we con-join this feature with the above POS features.
Notethat we have not included any lexical features.
Wefound during experiments on the development datathat lexical information was too sparse and led tooverfitting, so we rarely include such features.
In-stead we rely on the accuracy of POS tags to pro-vide enough evidence.Next we added features over every droppedword in the original sentence between yj?1 and yj ,if there were any.
These include the POS of eachdropped word, the POS of the dropped words con-joined with the POS of yj?1 and yj .
If the droppedword is a verb, we add a feature indicating the ac-tual verb (this is for common verbs like ?is?, whichare typically in compressions).
Finally we add thePOS context (bigram and trigram) of each droppedword.
These features represent common charac-teristics of words that can or should be droppedfrom the original sentence in the compressed ver-sion (e.g.
adjectives and adverbs).
We also add afeature indicating whether the dropped word is anegation (e.g., not, never, etc.
).We also have a set of features to representbrackets in the text, which are common in the dataset.
The first measures if all the dropped wordsbetween yj?1 and yj have a mismatched or incon-sistent bracketing.
The second measures if the leftand right-most dropped words are themselves bothbrackets.
These features come in handy for ex-amples like, The Associated Press ( AP ) reportedthe story, where the compressed version is TheAssociated Press reported the story.
Informationwithin brackets is often redundant.3.2.2 Deep Syntactic FeaturesThe previous set of features are meant to en-code common POS contexts that are commonly re-tained or dropped from the original sentence dur-ing compression.
However, they do so without alarger picture of the function of each word in thesentence.
For instance, dropping verbs is not thatuncommon - a relative clause for instance may bedropped during compression.
However, droppingthe main verb in the sentence is uncommon, sincethat verb and its arguments typically encode mostof the information being conveyed.An obvious solution to this problem is to in-clude features over a deep syntactic analysis ofthe sentence.
To do this we parse every sentencetwice, once with a dependency parser (McDon-ald et al, 2005b) and once with a phrase-structureparser (Charniak, 2000).
These parsers have beentrained out-of-domain on the Penn WSJ Treebankand as a result contain noise.
However, we aremerely going to use them as an additional sourceof features.
We call this soft syntactic evidencesince the deep trees are not used as a strict gold-standard in our model but just as more evidence for300root0saw2on4 after6Mary1 Ralph3 Tuesday5 lunch7SVPPP PPNP NP NP NPNNP VBD NNP IN NNP IN NNMary1 saw2 Ralph3 on4 Tuesday5 after6 lunch7Figure 2: An example dependency tree from the McDonald et al (2005b) parser and phrase structuretree from the Charniak (2000) parser.
In this example we want to add features from the trees for the casewhen Ralph and after become adjacent in the compression, i.e., we are dropping the phrase on Tuesday.or against particular compressions.
The learningalgorithm will set the feature weight accordinglydepending on each features discriminative power.It is not unique to use soft syntactic features inthis way, as it has been done for many problemsin language processing.
However, we stress thisaspect of our model due to the history of compres-sion systems using syntax to provide hard struc-tural constraints on the output.Lets consider the sentence x = Mary saw Ralphon Tuesday after lunch, with corresponding parsesgiven in Figure 2.
In particular, lets consider thefeature representation f(x,3,6).
That is, the fea-ture representation of making Ralph and after ad-jacent in the compression and dropping the prepo-sitional phrase on Tuesday.
The first set of featureswe consider are over dependency trees.
For everydropped word we add a feature indicating the POSof the words parent in the tree.
For example, ifthe dropped words parent is root, then it typicallymeans it is the main verb of the sentence and un-likely to be dropped.
We also add a conjunctionfeature of the POS tag of the word being droppedand the POS of its parent as well as a feature in-dicating for each word being dropped whether itis a leaf node in the tree.
We also add the samefeatures for the two adjacent words, but indicatingthat they are part of the compression.For the phrase-structure features we find everynode in the tree that subsumes a piece of droppedtext and is not a child of a similar node.
In this casethe PP governing on Tuesday.
We then add fea-tures indicating the context from which this nodewas dropped.
For example we add a feature spec-ifying that a PP was dropped which was the childof a VP.
We also add a feature indicating that a PPwas dropped which was the left sibling of anotherPP, etc.
Ideally, for each production in the tree wewould like to add a feature indicating every nodethat was dropped, e.g.
?VP?VBD NP PP PP ?VP?VBD NP PP?.
However, we cannot neces-sarily calculate this feature since the extent of theproduction might be well beyond the local contextof first-order feature factorization.
Furthermore,since the training set is so small, these features arelikely to be observed very few times.3.2.3 Feature Set SummaryIn this section we have described a rich featureset over adjacent words in the compressed sen-tence, dropped words and phrases from the origi-nal sentence, and properties of deep syntactic treesof the original sentence.
Note that these features inmany ways mimic the information already presentin the noisy-channel and decision-tree models ofKnight and Marcu (2000).
Our bigram featuresencode properties that indicate both good and badwords to be adjacent in the compressed sentence.This is similar in purpose to the source model fromthe noisy-channel system.
However, in that sys-tem, the source model is trained on uncompressedsentences and thus is not as representative of likelybigram features for compressed sentences, whichis really what we desire.Our feature set alo encodes dropped wordsand phrases through the properties of the wordsthemselves and through properties of their syntac-tic relation to the rest of the sentence in a parsetree.
These features represent likely phrases to bedropped in the compression and are thus similar innature to the channel model in the noisy-channelsystem as well as the features in the tree-to-tree de-cision tree system.
However, we use these syntac-tic constraints as soft evidence in our model.
Thatis, they represent just another layer of evidence tobe considered during training when setting param-eters.
Thus, if the parses have too much noise,the learning algorithm can lower the weight of theparse features since they are unlikely to be use-ful discriminators on the training data.
This dif-fers from the models of Knight and Marcu (2000),which treat the noisy parses as gold-standard when301calculating probability estimates.An important distinction we should make is thenotion of supported versus unsupported features(Sha and Pereira, 2003).
Supported features arethose that are on for the gold standard compres-sions in the training.
For instance, the bigram fea-ture ?NN&VB?
will be supported since there ismost likely a compression that contains a adjacentnoun and verb.
However, the feature ?JJ&VB?will not be supported since an adjacent adjectiveand verb most likely will not be observed in anyvalid compression.
Our model includes all fea-tures, including those that are unsupported.
Theadvantage of this is that the model can learn nega-tive weights for features that are indicative of badcompressions.
This is not difficult to do since mostfeatures are POS based and the feature set sizeeven with all these features is only 78,923.3.3 LearningHaving defined a feature encoding and decod-ing algorithm, the last step is to learn the fea-ture weights w. We do this using the MarginInfused Relaxed Algorithm (MIRA), which is adiscriminative large-margin online learning tech-nique shown in Figure 3 (Crammer and Singer,2003).
On each iteration, MIRA considers a singleinstance from the training set (xt,yt) and updatesthe weights so that the score of the correct com-pression, yt, is greater than the score of all othercompressions by a margin proportional to theirloss.
Many weight vectors will satisfy these con-straints so we pick the one with minimum changefrom the previous setting.
We define the loss to bethe number of words falsely retained or droppedin the incorrect compression relative to the correctone.
For instance, if the correct compression of thesentence in Figure 2 is Mary saw Ralph, then thecompression Mary saw after lunch would have aloss of 3 since it incorrectly left out one word andincluded two others.Of course, for a sentence there are exponentiallymany possible compressions, which means thatthis optimization will have exponentially manyconstraints.
We follow the method of McDon-ald et al (2005b) and create constraints only onthe k compressions that currently have the high-est score, bestk(x; w).
This can easily be calcu-lated by extending the decoding algorithm withstandard Viterbi k-best techniques.
On the devel-opment data, we found that k = 10 provided theTraining data: T = {(xt, yt)}Tt=11.
w0 = 0; v = 0; i = 02. for n : 1..N3.
for t : 1..T4.
min??
?w(i+1) ?
w(i)???s.t.
s(xt, yt) ?
s(xt, y?)
?
L(yt, y?
)where y?
?
bestk(x; w(i))5. v = v + w(i+1)6. i = i + 17. w = v/(N ?
T )Figure 3: MIRA learning algorithm as presentedby McDonald et al (2005b).best performance, though varying k did not have amajor impact overall.
Furthermore we found thatafter only 3-5 training epochs performance on thedevelopment data was maximized.The final weight vector is the average of allweight vectors throughout training.
Averaging hasbeen shown to reduce overfitting (Collins, 2002)as well as reliance on the order of the examplesduring training.
We found it to be particularly im-portant for this data set.4 ExperimentsWe use the same experimental methodology asKnight and Marcu (2000).
We provide every com-pression to four judges and ask them to evaluateeach one for grammaticality and importance on ascale from 1 to 5.
For each of the 32 sentences inour test set we ask the judges to evaluate three sys-tems: human annotated, the decision tree modelof Knight and Marcu (2000) and our system.
Thejudges were told all three compressions were au-tomatically generated and the order in which theywere presented was randomly chosen for each sen-tence.
We compared our system to the decisiontree model of Knight and Marcu instead of thenoisy-channel model since both performed nearlyas well in their evaluation, and the compressionrate of the decision tree model is nearer to our sys-tem (around 57-58%).
The noisy-channel modeltypically returned longer compressions.Results are shown in Table 1.
We present the av-erage score over all judges as well as the standarddeviation.
The evaluation for the decision tree sys-tem of Knight and Marcu is strikingly similar tothe original evaluation in their work.
This providesstrong evidence that the evaluation criteria in bothcases were very similar.Table 1 shows that all models had similar com-302Compression Rate Grammaticality ImportanceHuman 53.3% 4.96 ?
0.2 3.91 ?
1.0Decision-Tree (K&M2000) 57.2% 4.30 ?
1.4 3.60 ?
1.3This work 58.1% 4.61 ?
0.8 4.03 ?
1.0Table 1: Compression results.pressions rates, with humans preferring to com-press a little more aggressively.
Not surprisingly,the human compressions are practically all gram-matical.
A quick scan of the evaluations showsthat the few ungrammatical human compressionswere for sentences that were not really gram-matical in the first place.
Of greater interest isthat the compressions of our system are typicallymore grammatical than the decision tree model ofKnight and Marcu.When looking at importance, we see that oursystem actually does the best ?
even better thanhumans.
The most likely reason for this is thatour model returns longer sentences and is thus lesslikely to prune away important information.
Forexample, consider the sentenceThe chemical etching process used for glare protection iseffective and will help if your office has the fluorescent-lightoverkill that?s typical in officesThe human compression was Glare protection iseffective, whereas our model compressed the sen-tence to The chemical etching process used forglare protection is effective.A primary reason that our model does betterthan the decision tree model of Knight and Marcuis that on a handful of sentences, the decision treecompressions were a single word or noun-phrase.For such sentences the evaluators typically ratedthe compression a 1 for both grammaticality andimportance.
In contrast, our model never failedin such drastic ways and always output somethingreasonable.
This is quantified in the standard de-viation of the two systems.Though these results are promising, more largescale experiments are required to really ascer-tain the significance of the performance increase.Ideally we could sample multiple training/testingsplits and use all sentences in the data set to eval-uate the systems.
However, since these systemsrequire human evaluation we did not have the timeor the resources to conduct these experiments.4.1 Some ExamplesHere we aim to give the reader a flavor of somecommon outputs from the different models.
Threeexamples are given in Table 4.1.
The first showstwo properties.
First of all, the decision treemodel completely breaks and just returns a sin-gle noun-phrase.
Our system performs well, how-ever it leaves out the complementizer of the rela-tive clause.
This actually occurred in a few exam-ples and appears to be the most common problemof our model.
A post-processing rule should elim-inate this.The second example displays a case in whichour system and the human system are grammati-cal, but the removal of a prepositional phrase hurtsthe resulting meaning of the sentence.
In fact,without the knowledge that the sentence is refer-ring to broadband, the compressions are mean-ingless.
This appears to be a harder problem ?determining which prepositional phrases can bedropped and which cannot.The final, and more interesting, examplepresents two very different compressions by thehuman and our automatic system.
Here, the hu-man kept the relative clause relating what lan-guages the source code is available in, but droppedthe main verb phrase of the sentence.
Our modelpreferred to retain the main verb phrase and dropthe relative clause.
This is most likely due to thefact that dropping the main verb phrase of a sen-tence is much less likely in the training data thandropping a relative clause.
Two out of four eval-uators preferred the compression returned by oursystem and the other two rated them equal.5 DiscussionIn this paper we have described a new system forsentence compression.
This system uses discrim-inative large-margin learning techniques coupledwith a decoding algorithm that searches the spaceof all compressions.
In addition we defined arich feature set of bigrams in the compression anddropped words and phrases from the original sen-tence.
The model also incorporates soft syntacticevidence in the form of features over dependencyand phrase-structure trees for each sentence.This system has many advantages over previousapproaches.
First of all its discriminative natureallows us to use a rich dependent feature set andto optimize a function directly related to compres-303Full Sentence The fi rst new product , ATF Protype , is a line of digital postscript typefaces that will be sold in packages of up to six fonts .Human ATF Protype is a line of digital postscript typefaces that will be sold in packages of up to six fonts .Decision Tree The fi rst new product .This work ATF Protype is a line of digital postscript typefaces will be sold in packages of up to six fonts .Full Sentence Finally , another advantage of broadband is distance .Human Another advantage is distance .Decision Tree Another advantage of broadband is distance .This work Another advantage is distance .Full Sentence The source code , which is available for C , Fortran , ADA and VHDL , can be compiled and executed on the same system or ported to othertarget platforms .Human The source code is available for C , Fortran , ADA and VHDL .Decision Tree The source code is available for C .This work The source code can be compiled and executed on the same system or ported to other target platforms .Table 2: Example compressions for the evaluation data.sion accuracy during training, both of which havebeen shown to be beneficial for other problems.Furthermore, the system does not rely on the syn-tactic parses of the sentences to calculate probabil-ity estimates.
Instead, this information is incorpo-rated as just another form of evidence to be consid-ered during training.
This is advantageous becausethese parses are trained on out-of-domain data andoften contain a significant amount of noise.A fundamental flaw with all sentence compres-sion systems is that model parameters are set withthe assumption that there is a single correct answerfor each sentence.
Of course, like most compres-sion and translation tasks, this is not true, consider,TapeWare , which supports DOS and NetWare 286 , is avalue-added process that lets you directly connect theQA150-EXAT to a file server and issue a command from anyworkstation to back up the serverThe human annotated compression is, TapeWaresupports DOS and NetWare 286.
However, an-other completely valid compression might be,TapeWare lets you connect the QA150-EXAT to afi le server.
These two compressions overlap by asingle word.Our learning algorithm may unnecessarilylower the score of some perfectly valid compres-sions just because they were not the exact com-pression chosen by the human annotator.
A pos-sible direction of research is to investigate multi-label learning techniques for structured data (Mc-Donald et al, 2005a) that learn a scoring functionseparating a set of valid answers from all invalidanswers.
Thus if a sentence has multiple validcompressions we can learn to score each valid onehigher than all invalid compressions during train-ing to avoid this problem.AcknowledgmentsThe author would like to thank Daniel Marcu forproviding the data as well as the output of hisand Kevin Knight?s systems.
Thanks also to HalDaume?
and Fernando Pereira for useful discus-sions.
Finally, the author thanks the four review-ers for evaluating the compressed sentences.
Thiswork was supported by NSF ITR grants 0205448and 0428193.ReferencesE.
Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proc.
NAACL.M.
Collins.
2002.
Discriminative training methodsfor hiddenMarkov models: Theory and experimentswith perceptron algorithms.
In Proc.
EMNLP.K.
Crammer and Y.
Singer.
2003.
Ultraconservativeonline algorithms for multiclass problems.
JMLR.K.
Knight and D. Marcu.
2000.
Statistical-based sum-marization - step one: Sentence compression.
InProc.
AAAI 2000.R.
McDonald, K. Crammer, and F. Pereira.
2005a.Flexible text segmentation with structured multil-abel classifi cation.
In Proc.
HLT-EMNLP.R.
McDonald, K. Crammer, and F. Pereira.
2005b.
On-line large-margin training of dependency parsers.
InProc.
ACL.S.
Riezler, T. H. King, R. Crouch, and A. Zaenen.2003.
Statistical sentence condensation using ambi-guity packing and stochastic disambiguation meth-ods for lexical-functional grammar.
In Proc.
HLT-NAACL.S.
Sarawagi and W. Cohen.
2004.
Semi-Markov con-ditional random fi elds for information extraction.
InProc.
NIPS.F.
Sha and F. Pereira.
2003.
Shallow parsing with con-ditional random fi elds.
In Proc.
HLT-NAACL, pages213?220.J.
Turner and E. Charniak.
2005.
Supervised and un-supervised learning for sentence compression.
InProc.
ACL.304
