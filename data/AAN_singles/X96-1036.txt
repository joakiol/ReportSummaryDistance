Integration of Document Detection and Information ExtractionLouise GuthrieLockheed Martin CorporationTomek Strzalkowski, Wang Jin and Fang LinGE Corporate Research and DevelopmentSchenectady, NY 12301ABSTRACTWe have conducted a number of experiments toevaluate various modes of building an integrateddetection/extraction system.
The experiments wereperformed using SMART system as baseline.
Thegoal was to determine if advanced informationextraction methods can improve recall and precisionof document detection.
We identified the followingtwo modes of integration:I.
Extraction to Detection:broad-coverage extraction1.
Extraction step: identify concepts forindexing2.
Detection step 1: low recall, high initialprecision3.
Detection step 2: automatic relevancefeedback using top N retrieveddocuments to regain recall.I1.
Detection to Extraction: query-specific extraction1.Detection step 1: high recall, low precisionrun2.Extraction step: learn concept(s) from queryand retrieved subcollection3.Detection step 2: re-rank the subcollectionto increase precisionOur integration effort concentrated on mode I, andthe following issues:1.use of shallow but fast NLP for phraseextractions and disambiguation in place ofa full syntactic parser2.use existing MUC-6 extraction capabilitiesto index a retrieval collection3.mixed Boolean/soft match retrieval model4.create a Universal Spotter algorithm forlearning arbitrary conceptsLEX lCO-SEMANTIC  PATTERNMATCHING FOR SHALLOW NLPThe lexico-semantic pattern matching methodallows for capturing of word sequences in text usinga simple pattern language that can be compiled intoa set of non-deterministic finite automata.
Eachautomoton represents a single rule within thelanguage, with several related rules forming apackage.
As a result of matching a rule against theinput, a series of variables within the rule are boundto lexical elements in text.
These bindings aresubsequently used to generate single-word and/ormultiple-word terms for indexing.Long phrasal terms are decomposed into pairs intwo phases as follows.
In the first phase, onlyunambiguous pairs are collected, while all longerand potentially structurally ambiguous noun phrasesare passed to the second phase.
In the secondphase, the distributional statistics gathered in thefirst phase are used to predict the strength ofalternative two-word sub-components within longphrases.
For example, we may have multipleunambiguous occurrences of "insider trading", whilevery few of "trading case".
At the same time, thereare numerous phrases such as =insider tradingcase", =insider trading legislation", etc., where thepair =insider trading" remains stable while the otherelements get changed, and significantly fewer caseswhere, say, "trading case" is constant and the otherwords change.The experiments performed on a subset of U.S.PTO's patent database show healthy 10%+ increasein average precision over baseline SMART system.The average precision (11-point) has increased from49% SMART baseline on the test sample to 56%.Precision at 5 top retrieved documents jumped from48% to 52%.
We also noticed that phrasedisambiguation step was critical for improvedprecision.195INDEXING WITH MUC-6 CONCEPTSIn these experiments we used actual MUCorganization and people name spotter (fromLockheed Martin) to annotate and index a subset ofTREC-4 collection.
We selected 17 queries out of250 TREC topics which explicitely mentioned someorganizations by names.
The following observationswere made:1.Different queries require different conceptsto be spotted: concepts that are universalenough to be important in most domainsare hard to find, or not discriminatingenough.2.These differences are frequently query-specific, not just domain-specific, whichmakes MUC-style extraction impractical3.The role that a concept plays in a query canaffect its usefullness in retrieval: conceptsfound in focus appear to be radically morediscriminating than those found inbackground roles.Initial results show that targeted concept indexingcan be extremely effective, however, randomannotation may in fact cause loss of performance.Overall, the average precision improved by only 3%;however, some queries, namely those where theindexed concepts were in focus roles, benefiteddramatically.
For example, the query aboutMitsubishi has gained about 25% in precision overSMART baseline (from 42% to 52%).Typical results are summarized in the table below:words annotations both mergeAv.PREC 34.1% 18.3% 28.1% 35.5%R EC @ 50 67% 31% 66% 67%MIXED BOOLEAN/SOFT RETRIEVALMODELWe allow strict-match terms to be included in thesearch queries in a specially designated field.
Thehard/soft query mechanism allows a user to specifyeither in interactive or batch mode a boolean typequery which will restrict documents returned by avector space model match.
Documents notsatisfying the query will be deemed to be non-relevant for the query.A two-pass retrieval has been implemented inSMART to allow proper interpretations of suchqueries.
In interactive mode a normal vector querycan be entered using 'run' command.
When the firstresults are returned using 'boolean' will place you ineditor mode (similar to run).
Construct he query andterminate the query with a period on a line by itself.The documents returned by the latest 'run' commandare filtered and only those satisfying the query areredisplayed.
Using 'more' will always retrieve'num_wanted' unless there are insufficientdocuments remaining that are relevant to the initialvector query.RECOMMENDATIONS FOR ANINTEGRATED SYSTEMThe following were determined to be crucial inbuilding an integrated extraction/detection system:1.
A large variety of extraction capabilities, best ifcould be generated rapidly on an ad-hoc basis.2.
Rapid discource analysis for role determination ofsemantically significant erms3.
The need for well-defined equivalence relation onannotations produced by an extraction system.4.
Use of mixed Boolean/soft retrieval modelUNIVERSAL SPOTTERIdentifying concepts in natural language text isan important information extraction task.
Dependingupon the current information needs one may beinterested in finding all references to people,locations, dates, organizations, companies,products, equipment, and so on.
These concepts,along with their classification, can be used to indexany given text for search or categorization purposes,to generate summaries, or to populate databaserecords.
However, automating the process ofconcept identification in unformatted text has notbeen an easy task.
Various single-purpose spottershave been developed for specific types of concepts,including people names, company names, locationnames, dates, etc.
but those were usually eitherhand crafted for particular applications or domains,or were heavily relying on apriori lexical clues, suchas keywords (e.g., "Co.'), case (e.g., "John K. Big'),predicatable format (e.g., 123 Maple Street), or acombination of thereof.
This makes creation and196extension of such spotters an arduous manual job.Other, less salient entities, such as products,equipment, foodstuff, or generic references of anykind (e.g., "a Japanese automaker') could only beidentified if a sufficiently detailed domain model wasavailable.We take a somewhat different approach to identifyvarious types of text entities, both generic andspecific, without a detailed understanding of the textdomain, and relying instead on a combination ofshallow linguistic processing (to identify candidatelexical entities), statistical knowledge acquisition,unsupervised learning techniques, and possiblybroad (universal but often shallow) knowledgesources, such as on-line dictionaries (e.g., WordNet,Comlex, OALD, etc.).
Our method moves beyondthe traditional name spotters and towards a universalspotter where the requirements on what to spot canbe specified as input parameters, and a specific-purpose spotter could be generated automatically.In this paper, we describe a method of creatingspotters for entities of a specified category given onlyinitial seed examples, and using an unsupervisedlearning process to discover rules for finding moreinstances of the concept.
At this time we place nolimit on what kind of things one may want to build aspotter for, although our experiments thus farconcentrated on entities customarily referred to withnoun phrases, e.g., equipment (e.g., =gas turbineassembly"), tools (e.g., =adjustable wrench"),products (e.g., "canned soup", "Arm Hammerbaking soda"), organizations (e.g., AmericanMedical Association), locations (e.g., Albany CountyAirport), people (e.g., Bill Clinton), and so on.
Weview the semantic categorization problem as a caseof disambiguation, where for each lexical entityconsidered (words, phrases, N-grams), a binarydecision has to be made whether or not it is aninstance of the semantic type we are interested in.The problem of semantic tagging is thus reduced tothe problem of partitioning the space of lexicalentities into those that are used in the desired sense,and those that are not.
We should note here that itis acceptable for homonym entities to have differentclassification depending upon the context in whichthey are used.
Just as the word "'bank" can beassigned different senses in different contexts, socan" Boeing 777 jet" be once a product, and anothertime an equipment and not a product, dependingupon the context.
Other entities may be less contextdependent (e.g., company names) if their definitionsare based on internal context (e.g., "ends with Co.")as opposed to external context (e.g., "followed bymanufactures"), or if they lack negative contexts.The user provides the initial information (seed) aboutwhat kind of things he wishes to identify in text.
Thisinformation should be in a form of a typical lexicalcontext in which the entities to be spotted occur, e.g.,"the name ends with Co.", or "to the right of producedor made", or "'to the right of maker of", and so forth,or simply by listing or highlighting a number ofexamples in text.
In addition, negative examples canbe given, if known, to eliminate certain "obvious'exceptions, e.g., "not to the right of made for ", "nottoothbrushes".
Given a sufficiently large trainingcorpus, an unsupervised learning process is initiatedin which the system will: (1) generate initial contextrules from the seed examples; (2) find furtherinstances of the sought-after concept using the initialcontext while maximizing recall and precision; (3)find additional contexts in which these entities occur;and (4) expand the current context rules based onselected new contexts to find even more entities.We present and evaluate preliminary results ofcreating spotters for organizations and products.What do you want to find: seed selectionIf we want to identify some things in a stream oftext, we first need to learn how to distinguish themfrom other items.
For example, company names areusually capitalized and often end with "Co.', "Corp.',"Inc.' and so forth.
Place names, such as cities, arenormally capitalized, sometimes are followed by astate abbreviation (as in Albany, NY ), and may bepreceded by locative prepositions (e.g., in, at, from,to ).
Products may have no distinctive lexicalappearance, but they tend to be associated withverbs such as "produce', "manufacture', "make',"sell', etc., which in turn may involve a companyname.
Other concepts, such as equipment ormaterials, have few if any obvious associations withthe surrounding text, and one may prefer just to pointthem out directly to the learning program.
There aretexts, e.g., technical manuals, where suchspecialized entities occur more often thanelsewhere, and it may be advantagous to use thesetexts to derive spotters.The seed can be obtained either by hand taggingsome text or using a naive spotter that has highprecision but presumably low recall.
A naive spottermay contain simple contextual rules such as thosementioned above, e.g., for organizations: a nounphrases ending with "Co." or "Inc."; for products: a197noun phrase following "manufacturer of", "producerof", or "retailer of".
When such naive spotter is difficultto come by, one may resort to hand tagging.
Fromseeds to spottersThe seed should identify the sought-after entitieswith a high precision (though not necessarily 100%),however its recall is assumed to be low, or else wewould already have a good spotter.
Our task is nowto increase the recall while maintaining (or evenincrease if possible) the precision.We proceed by examining the lexical context inwhich the seed entities occur.
In the simplestinstance of this process we consider a context toconsist of N words to the left of the seed and N wordsto the right of the seed, as well as the words in theseed itself.
Each piece of significant contextualevidence is then weighted against its distribution inthe balance of the training corpus.
This in turn leadsto selection of some contexts to serve as indicatorsof relevant entities, in other words, they become theinitial rules of the emerging spotter.As an example, let's consider building a spotter forcompany names, starting with seeds as illustratedin the following fragments (with seed contextshighlighted):... HENRY KAUFMAN is president of HenryKaufman Co. ,  a ... Gabelli, chairman of GabelliFunds Inc. ; Claude N. Rosenberg .... is namedpresident of Skandinaviska Enskilda Banken ...become vice chairman of the state-ownedelectronics giant Thomson S.A ....  banking group,said the formal merger of Skanska Banken into ...water maker Source Perrier S.A., according toFrench stock ...Having "Co.", "Inc." to pick out "'Henry KaufmanCo."
and "Gabelli Funds Inc." as seeds, we proceedto find new evidence in the training corpus, using anunsupervised learning process, and discover that"'chairman of" and "'president of" are very likely toprecede company names.
We expand our initial setof rules, which allows us to spot more companies:... HENRY KAUFMAN is president of HenryKaufman Co.,  a ... Gabelli, chairman of GabelliFunds Inc. ; Claude N. Rosenberg ... is namedpresident of Skandinaviska Enskilda Banken ...become vice chairman of the state-ownedelectronics giant Thomson S.A .
.
.
.
banking group,said the formal merger of Skanska Banken into ...water maker Source Perrier S.A., according toFrench stock ...This evidence discovery can be repeated in abootstrapping process by replacing the initial set ofseeds with the new set of entities obtained from thelast iteration.
In the above example, we now have"Skandinaviska Enskilda Banken" and "the state-owned electronics giant Thomson S.A." in additionto the initial two names.
A further iteration may add"S.A." and "'Banken" to the set of contextual rules,and so forth.
In general, entities can be both addedand deleted from the evolving set of examples,depending on how exactly the evidence is weightedand combined.
The details are explained in thefollowing sections.Text preparationIn most cases the text needs to be preprocessedto isolate basic lexical tokens (words, abbreviations,symbols, annotations, etc), and structural units(sections, paragraphs, sentences) wheneverapplicable.
In addition, part-of-speech tagging isusually desirable, in which case the tagger may needto be re-trained on a text sample to optimize itsperformance.
Finally, a limited amount of lexicalnormalization, or stemming, may be performed.
Theentities we are looking for may be expressed bycertain types of phrases.
For example, peoplenames are usually sequences of proper nouns, whileequipment names are contained within nounphrases, e.g., "forward looking infrared radar'.
Weuse part of speech information to delineate thosesequences of lexical tokens that are likely to contain"our' entities.
From then on we restrict any furtherprocessing on these sequences, and their contexts.These preparatory steps are desirable since theyreduce the amount of noise through which thelearning process needs to plow, but they are not,strictly speaking, necessary.
Further experimentsare required to determine the level of preprocessingrequired to optimize the performance of theUniversal Spotter.Evidence itemsThe semantic categorization problem describedhere displays some parallels to the word sensedisambiguation problem where homonym wordsneed to be assigned to one of several possiblesenses.
There are two important differences,however.
First, in the semantic categorization198problem, there is at least one open-ended categoryserving as a grab bag for all things non-relevant.
Thiscategory may be hard, if not impossible, to describeby any finite set of rules.
Second, unlike the wordsense disambiguation where the items to beclassified are known apriori, we attempt toaccomplish two things at the same time: discoverthe items to be considered for categorization;actually decide if an item belongs to a given category,or falls outside of it.
The categorization of a lexicaltoken as belonging to a given semantic class isbased upon the information provided by the wordsoccurring in the token itself, as well as the words thatprecede and follow it in text.
In addition, positionalrelationships among these words may be ofimportance.Experiments and ResultsWe used the Universal Spotter to findorganizations and products in a 7 MBytes corpusconsisting of articles from the Wall Street Journal.First, we pre-processed the text with a part-of-speech tagger and identified all simple noun groupsto be used as candidate phrases.
10 articles wereset aside and hand tagged as key for evaluation.Subsequently, seeds were constructed manually inform of contextual rules.
For organizations, theseinitial rules had a 98% precision and 49% recall; forproducts, the corresponding numbers were 97% and42%.
No lexicon verification has been used in orderto show more clearly the behavior the learningmethod itself (the performance can be enhanced bylexicon verification).
The seeds that we used in ourexperiments are quite simple, perhaps too simple.Better seeds may be needed (possibly developedthrough an interaction with the user) to obtain strongresults for some categories of concepts.For organization tagging, the recall and precisionresults obtained after the fourth bootstrapping cycleare 90% and 95%, respectively.
Examples ofextracted organizations include: "the StateStatistical Institute Istat", "Wertheim Schroder Co","Skandinaviska Enskilda Banken", "StatisticsCanada".The results for products tagging are at 80% recall at85% precision, and 75% recall at 90% precision.Examples of extracted products include: "theMercury Grand Marquis and Ford Crown Victoriacars", "Chevrolet Prizm", "Pump shoe", "AS/400".ACKNOWLEDGEMENTSThis paper is based upon work supported by theAdvanced Research Projects Agency under TipsterPhase-2 Contract 94-F-133200-000 to LockheedMartin Corporation, under a subcontract to GECorporate Research and Development.REFERENCES\[1\] Brown,P., S. Pietra, V. Pietra and R. Mercer.1991.
Word Sense Disambiguation Using StatisticalMethods.
Proceedings of the 29h Annual Meeting ofthe Association for Computational Linguistics, pp.264-270.\[2\] Gale, W., K. Church and D. Yarowsky.
1992.
AMethod for Disambiguating Word Senses in a LargeCorpus.
Computers and the Humanities, 26, pp.415--439.\[3\] Harman, D. 1995.
Overview of the Third TextREtrieval Conference.
Overview of the Third TextREtrieval Conference (TREC-3), pp.1-20.\[4\] Strzalkowski, T. 1995.
Natural LanguageInformation Retrieval.
Information Processing andManagement, vol.
31, no.
3, pp.
397-417.\[5\] Yarowsky, D. 1995.
Unsupervised Word SenseDisambiguation Rivaling Supervised Methods.Proceedings of the 33rd Annual Meeting of theAssociation for Computational Linguistics, pp.
189-196.199
