Instance-Based Generation for InteractiveRestricted Domain Question Answering SystemsMatthias Denecke and Hajime TsukadaNTT Communication Science Laboratories,2-4 Hikaridai, Seika-Cho, Soraku-gun, Kyoto{denecke, tsukada}@cslab.kecl.ntt.co.jpAbstract.
One important component of interactive systems is the gen-eration component.
While template-based generation is appropriate inmany cases (for example, task oriented spoken dialogue systems), inter-active question answering systems require a more sophisticated approach.In this paper, we propose and compare two example-based methods forgeneration of information seeking questions.1 IntroductionQuestion answering is the task of providing natural language answers to naturallanguage questions using an information retrieval engine.
Due to the unrestrictednature of the problem, shallow and statistical methods are paramount.Spoken dialogue systems address the problem of accessing information from astructured database (such as time table information) or controlling appliances byvoice.
Due to the fact that the scope of the application defined by the back-end,the domain of the system is well-defined.
Therefore, in the presence of vague,ill-defined or misrecognized input from the user, dialogue management, relyingon the domain restrictions as given by the application, can interactively requestmore information from the user until the users?
intent has been determined.
Inthis paper, we are interested in generation of information seeking questions ininteractive question-answering systems.1.1 Our SystemWe implemented a system that combines features of question answering systemswith those of spoken dialogue systems.
We integrated the following two featuresin an interactive restricted domain question answering system: (1) As in questionanswering systems, the system draws its knowledge from a database of unstruc-tured text.
(2) As in spoken dialogue systems, the system can interactively queryfor more information in the case of vague or ill-defined user queries.1.2 Problem Addressed in This PaperRestricteddomain question answering systems canbe deployed in interactive prob-lem solving solutions, for example, software trouble shooting.
In these scenarios,interactivity becomes a necessity.
This is because it is highly unlikely that all factsrelevant to retrieving the appropriate response are stated in the query.
For exam-ple, in the software trouble shooting task described in [5], a frequent system gen-erated information seeking question is for the version of the software.
Therefore,R.
Dale et al (Eds.
): IJCNLP 2005, LNAI 3651, pp.
486?497, 2005.c?
Springer-Verlag Berlin Heidelberg 2005Instance-Based Generation 487there is a need to inquire additional problem relevant information from the user,depending on the interaction history and the problem to be solved.In this paper, we specifically address the problem of how to generate in-formation seeking questions in the case of ambiguous, vague or ill-defined userquestions.
We assume that the decision of whether an information seeking ques-tion is needed is made outside of the module described here.
More formally, theproblem we address can be described as follows:Given 1.
A representation of the previous interaction history, consistingof user and system utterances, and retrieval results from the IRsubsystem,2.
A decision for a information seeking questionProduce An information seeking question.Problems of this kind have appeared traditionally in task oriented spokendialogue systems, where missing information needs to be prompted.
However,in the case of spoken dialogue systems, question generation is typically not asubstantial problem: the fact that the back-end is well-structured allows forsimple template-based generation in many cases.
For example, missing valuesfor database queries or remote method invocations can be queried that way.
(But see also Oh and Rudnicky [7] or Walker et al[12] for more elaboratedapproaches to generation for spoken dialogue systems).In our case, however, a template-based approach is unrealistic.
This is dueto the unstructured back-end application.
Unlike as spoken dialogue systems,we cannot make assumptions over what kind of questions to ask as this is de-termined by the result set of articles as returned by the information retrievalengine.
Existing interactive question-answering systems (see section 7.1 for amore detailed description) either use canned text on dialogue cards [5], breakdown the dialogue representation into frames and then techniques from spo-ken dialogue systems [8], or make simplifying assumptions to the extent thatgeneration essentially becomes equivalent to template-based generation.1.3 Proposed SolutionFor reasons discussed above, we propose an example-based approach to genera-tion.
More specifically, we use an existing dialogue corpus to retrieve appropriatequestions and modify in order to fit the situation at hand.
We describe two algo-rithms for instance-based natural language questions generation by first selectingappropriate candidates from the corpus, then modifying the candidates to fit thesituation at hand, and finally re-rank the candidates.
This is an example of amemory-based learning approach, which in turn is a kind of a case-based reason-ing.
To the best of our knowledge, this is the first work addressing the problemof example-based generation information seeking questions in the absence of astructured back-end application.2 Instance Based Natural Language GenerationIn this section, we review the background in memory-based learning and itsapplication in natural language generation.488 M. Denecke and H. Tsukada2.1 Memory-Based ReasoningMemory-based reasoning (MBR) is often considered a subtype of Case-basedreasoning.
Case-based reasoning was proposed in the 80?s as an alternative torule-based approaches.
Instead of expressing regularities about the domain tobe modeled in rules, the primary knowledge source in case-based reasoning isa memory of cases representing episodes of encountered problems.
Generatinga solution to a given problem consists of retrieving an appropriate case frommemory and adapting it to the problem at hand.MBR solves problems by retrieving stored precedents as a starting point fornew problem-solving (e.g., [9]).
However, its primary focus is on the retrieval pro-cess, and in particular on the use of parallel retrieval schemes to enable retrievalwithout conventional index selection.
One aspect of memory-based systems is tochoose a distance that appropriately selects candidate exemplars.Memory-based reasoning has been applied to machine translation, parsing,unit selection text-to-speech synthesis, part-of-speech tagging, and others.
Anoverview of memory-based approaches to natural language processing can befound in the introduction to the special issue [2].2.2 Statistical and Instance-Based GenerationThe most prominent example for statistical generation is Nitrogen [6].
Thissystem has been designed to allows large scale generation while requiring onlya minimal knowledge base.
An abstract meaning representation is turned intoa lattice of surface sentences using a simple keyword based grammar.
Usingstatistical information acquired from a corpus, the sentences in the lattices arere-ranked to determine the optimal surface string.More recently, example-based natural language generation using a corpuswas proposed [11].
It is assumed in this work that content determination hasalready taken place and the input has been broken down to sentence-size pieces.The approach is to use a learned grammar to generate a list of candidates usinga traditional chart based generation algorithm.
The grammar is learned usingstatistical methods.
During generation, edges that are added to the chart areranked depending on their distance to the closest instance in the example base.This is where the memory-based approach comes into play.
In order to allow forcareful generalization in the instance base, the authors propose to add a list of tag(?slots?)
with which the corpus is annotated.
Based on this annotated corpus,a semantic grammar is learned.
For ranking the edge based on the instances,the authors propose the well-known tf-idf scheme with the difference that thosewords that are annotated with a semantic tag are replaced by their tag.3 KernelsMemory-based learning requires a distance metric in order to identify instancessimilar to the problem at hand.
We propose to use convolution kernels as distancemetric.
A kernel K can be seen as a generalized form of a distance metric thatperforms the following calculationK(x, y) = ??
(x), ?
(y)?,Instance-Based Generation 489where ?
is a non-linear mapping from the input space into some higher di-mensional feature space, and ?
?, ??
is the inner product in the feature space.Calculating the inner product in some space of higher dimension than the in-put space is desirable for classifiers because non linearly separable sets can belinearly separated in the higher dimensional feature space.
Kernel methods arecomputationally attractive because the kernel can calculate the mapping andthe inner product implicitly rather than explicitly determining the image under?
of the input.While Bag-of-Words techniques can be employed as an approximation toderive feature vectors for classifiers, the loss of structure is not desirable.
Toaddress this problem, Haussler [3] proposed Convolution Kernels that are capableof processing structured objects x and y.
The structured objects x and y consistof components x1, .
.
.
, xm and y1, .
.
.
, yn.
The convolution kernel of x and y isgiven by the sum of the products of the components?
convolution kernels.
Thisapproach can be applied to structured objects of various kinds, and results havebeen reported for string kernels and tree kernels.3.1 Hierarchical Tree KernelThe idea behind Convolution Kernels is that the kernel of two structures isdefined as the sum of the kernels of their parts.
Formally, let D be a positiveinteger and X, X1, .
.
.
, XD separable metric spaces.
Furthermore, let x and ybe two structured objects, and x = x1, .
.
.
, xD and y = y1, .
.
.
, yD their parts.The relation R ?
X1 ?
.
.
.
?
XD ?
X holds for x and x if x are the parts ofx.
The inverse R?1 maps each structured object onto its parts, i.e.
R?1(x) ={x : R(x, x)}.
Then the kernel of x and y is given by the following generalizedconvolution:K(x, y) =?x?R?1(x)?y?R?1(y)D?1Kd(xd, yd)Informally, the value of a convolution kernel for two objects X and Y is givenby the sum of the kernel value for each of the substructures, i.e.
their convolution.Suzuki et al[10] proposed Hierarchical Directed Acyclic Graph kernels inwhich the substructures contain nodes which can contain graphs themselves.
Thehierarchy of graphs allows extended information from multiple components to berepresented and used in classification.
In addition, nodes may be annotated withattributes, such as part of speech tags, in order to add information.
For example,in a Question-Answering system, components such as Named Entity Extraction,Question Classification, Chunking and so on may each add to the graph.4 CorpusWe collected a corpus for our instance based generation system as follows.
Weset up communications between a wizard and users.
The wizard was instructedto ?act like the system?
we intend to build, that is, she was required to interactwith the user either by prompting for more information or give the user theinformation she thought he wanted.
Altogether, 20 users participated in the490 M. Denecke and H. TsukadaFig.
1.
Extract from the dialogue corpusdata collection effort.
Each user contributed to 8 to 15 dialogues.
The lengthof the dialogues varies between 11 and 84 turns, the median being 34 turns.Altogether, the corpus consists of 201 dialogues.
The corpus consists of 6785turns, 3299 of which are user turns and the remaining 3486 are wizard turns.Due to the strict dialogue regiment prescribed in the onset of the data collection,each dialogue consists either of an equal number of user and wizard turns (incase the user ends the dialogue; 14 cases) or one wizard turn more than userturn in case the wizard ends the dialogue (187 cases).
Figure 1 shows the firstpart of a dialogue from the corpus.5 Generation Algorithm5.1 Overview of the AlgorithmWe now describe our algorithm informally.
Given the dialogue history up untilnow, the last user utterance and the result list as a response to the last userutterance, it is the task of the algorithm to generate an appropriate question toelicit more information from the user.
Recall an external dialogue module (notdescribed in this paper) decides whether an information seeking question shouldbe generated (as opposed to, say, turning the information found in the highestranking article into an answer).Informally, the algorithm works as follows.
Initially, the dialogue corpus ispreprocessed, including word segmentation and part-of-speech labeling (see sec-tion 5.2).
In step 1, a ranked list of question candidates is generated (see section5.3).
In step 2, for each of the candidates, a list of change positions is deter-mined (see section 5.4).
These indicate the part of the questions that need tobe adapted to the current situation.
Subsequently, the portions indicated by thechange positions are replaced by appropriate constituents.
In the step 3, thecandidates generated in the previous step are re-ranked (see section 5.5).
Re-ranking takes place by using the same distance as the one in step 1.
The highestranking candidate is then presented to the user.Instance-Based Generation 4915.2 Corpus PreprocessingSince Japanese does not provide word segmentation, we need to preprocess thecorpus.
The corpus consists of a set of dialogues.
Each dialogue consists of a setof utterances.
Each utterance is annotated for speaker and utterance type.
In adialogue, wizard and user utterance strictly alternate, with no interjections.Preprocessing is done as follows.
Each utterance is stripped of its annotationsand presented to the part-of-speech tagger Chasen [1].
Chasen segments the inputsentence, reduces inflected words to their base forms and assigns part of speechtags to the base forms.
We use the notation cw(u) to designate the content wordsin utterance, sentence or newspaper article u.
For our purposes, content wordsare adjectives, nouns and verbs, de-inflected to their base form, if necessary.
Asubsequent processing step assigns semantic labels and named entity classes tothe de-inflected word forms.5.3 Sentence SelectionIn order to understand the motivation for our approaches to sentence selection,it is necessary to recall the context in which sentences are selected.
We would liketo find a information seeking question similar to the one we want to generate.The question to be generated is determined by the dialogue context.
A naturalapproach is to choose a bag-of-word distance measure for sentences, define adistance for partial dialogues based on this distance and then choose the dialogue,and a sentence from that dialogue with the lowest distance.It turns out, however, that this approach does not work too well.
One problemis that in the beginning of a dialogue not many informative words are contained inthe utterances, therefore making an informed selection of utterances difficult.
Thepoint of this paper is to determine how to overcome this problem.
In the followingtwo sections, we propose two approaches.
The first uses additional information inthe retrieved documents, and the second uses additional syntactic and semantic in-formationwhen calculating the distance between sentences.Bothmethods consistsof calculating a score for candidate sentences and selecting the highest ranking one.Method 1.
Information retrieval over large corpora works well due to the redun-dancy in the document data, a fact that for example Latent Semantic Indexingexploits.
The principal idea of the first method is to use the redundancy in theunrestricted document corpus when scoring sentence candidates.
Instead of de-termining the bag-of-word score between a candidate sentence and the querysentence, we submit the information extracted from the candidate dialogue andthe current dialogue to the information retrieval engine, resulting in two n bestlists of articles L and L?.
In order to score the degree of similarity, we determinethe the intersection of content words in the retrieved articles.
The larger the in-tersection, the higher the score is to be ranked.
In order to take relevance in theresult set into account, the scores are discounted by the position of the articlein the n best list.
More specifically, we calculate the similarity score betweenthe current dialogue and an example dialogue as follows.
Let d be the currentlydeveloping dialogue consisting of t user utterances and u1, .
.
.
ut be the user ut-terances in the current dialogue up until now.
Furthermore, let d?
be an exampledialogue from the corpus and let u?1, .
.
.
u?t?
be the first t?
user utterances in theexample dialogue.
Then:492 M. Denecke and H. Tsukada1.
Form the union of content words CW =?t cw(ut), CW?
=?t?
cw(u?t?
)2.
Submit two queries to the information retrieval engine consisting of CW andCW ?, respectively and obtain two article n best lists L and L?.3.
Calculate the similarity score according tosim(ut, u?t?)
=?l?L?l?
?L?cw(l) ?
cw(l?
)rank(l) + rank(l?
)Method 2.
In the first method described above, we seek to overcome poorscoring function by adding redundancy from the information retrieval engine.The second method we propose attempts to improve scoring by adding syntacticand semantic structure to the distance metric.
More specifically, we directlycompare the last user utterance in the current dialogue with the last utterancein the example dialogue, but do so in a more detailed manner.
To this end, wedetermine the similarity score as the output of the hierarchical directed acyclicgraph kernel.
The similarity is thus defined as sim(ut, u?t?)
= K(ut, u?t?
).5.4 Sentence AdaptationThe adaptation of the highest ranking question to the current dialogue consists offour steps.
First, we determine the location(s) where change should take place.Second, we determine constraints for the substituting constituent.
Third, wedetermine a list of substituents for each location of change.
Fourth, we replacethe phrase(s) at the location(s) of change with the highest ranking element fromthe corresponding list of substituents.Determining Locations of Change.
After the example sentences have beenretrieved from the corpus, we need to determine where and how the questionsneed to be adapted to the current dialogue.
We determine the locations of changeli by identifying suitable head words of phrase to be exchanged.
What are thecriteria for suitable head words?
Recall that the example sentences are drawnfrom dialogue similar in topics but in which the content words are exchanged.This limits the part-of-speech of the words to be exchanged to nouns and verbs.Therefore, we construct a list l of nouns and verbs that are part of the retrievedsentence but cannot be found in the current user query.
Second, since we areinterested in replacing those content words that are specific to the retrieveddialogue with those specific to the current dialogue, we would like to incorporatesome measure of informativeness.
For that reason, we determine the unigramcount for all content words in l. High ranking candidates for change are thosewords that are specific (i.e., have a low unigram count above a certain threshold).Constraints for Substituents.
The constraints for the substituents are givenby the semantic and syntactic information of the phrase at the change location.More specifically, the constraints include the following features: Part of speech,type of named entity, if applicable (the type includes location, state, personname and so on), and semantic class.Determining Substituents.
After having determined the change locationsand constraints of the substituents, we proceed to determine the substituents.Instance-Based Generation 493The primary source for substituents are the retrieved newspaper articles.
How-ever, since we wish to apply the generation component in a dialogue system, weneed to take implicit confirmation into account as well.
For this reason, we deter-mine whether a phrase matching the phrase at change location li occurs beforeli previously in the dialogue.
If this is the case, the source for the substituent isto be the current dialogue.Given the constraints for a change location determined in the previous step,we add all content words from the highest ranking article to the candidate listfor that change location.
The score for a content word is given by the number ofconstraints it fulfills.
Ties are broken by unigram counts so that rare words geta higher score due to their informativeness.Application of Change.
Applying the change simply consists of removing thephrase whose head word is located at the change location and replacing it withthe highest ranking word from the candidate list for that score.5.5 RerankingThe previous steps produce a list of sentence candidates.
For each of the sentencecandidates, we calculate the similarity between the generated sentence with thesentences from a small corpus of desirable sentences.
Finally, the sentence withthe highest score is presented to the user.
Examples of generated sentences areshown in figure 2.
The complete algorithm is given in figure 3.Fig.
2.
Generated questions.
The substituent in the first question comes from thedialogue context, while the other substituents come from retrieved articles.6 EvaluationThe evaluation was done as follows.
We divided the corpus in a example base anda test set.
The example base consists of 151 randomly selected dialogues, the testset consists of the remaining 50 dialogues.
From each of the test examples, wesupplied the initial wizard greeting and the initial user utterance as context forthe dialogue.
Given this context, each method generated an n best list consistingof 3 information seeking questions.The generated lists were labeled by three annotators according to the follow-ing criteria.
For each of the three questions in the n best lists, the annotatorshad to determine a syntactic, a semantic and an overall score.
The scores rangeover the labels poor, acceptable, good.
The same score could be assigned more494 M. Denecke and H. TsukadaInput: Preprocessed dialogue corpus C = {d?1, .
.
.
, d?n}Current dialogue d with user utterances u1, .
.
.
, utOutput: Information seeking questionStep 1: Determine sim(ut, u?t?)
for all user utterances u?t?
from the dialogue corpusSelect the w?1, .
.
.
w?k wizard utterances directly following the khighest ranking utterancesStep 2: for each w?i ?
{w?1, .
.
.
, w?k}:Determine change locations l1, .
.
.
, llfor each lj ?
{l1, .
.
.
, ll}Determine list of substituents s1ij , .
.
.
, spijGenerate modified sentence list v1, .
.
.
, vm by replacing substituentsat change locationsStep 3: Determine and return highest ranking vi?
.Fig.
3.
Generation algorithmthan once, for example, in case the sentence selection algorithm produced anunreliable candidate, the overall score for all three sentence candidates couldbe bad.
Furthermore, the evaluators had to re-arrange the 3 best list accordingto the quality of the generated questions.
Finally, the annotators had providea sentence they consider good.
For easy comparison, the symbolic scores poor,acceptable, good translate to 0,0.5 and 1, respectively, in the tables below.6.1 Scoring ResultsThe results of the three best syntactic and semantic sentence scoring are shownin table 1 (a) and 1 (b).
The inter-annotator agreement is given by their kappascores for each method separately.
Table 1 (c) shows the average of syntacticand semantic scores.
The kappa coefficient for the inter-annotator agreement forthese scores are 0.68, 0.72, and 0.71, respectively.The syntactic scores rank higher than the semantic scores.
This is explainedby the fact that the corpus contains syntactically relatively well-formed examplesentences, and the replacement operator, in addition to being constrained byTable 1.
Average of syntactic and semantic scoresMethod 1 Method 21 0.796 0.8002 0.657 0.7903 0.787 0.780(a)Method 1 Method 21 0.573 0.3932 0.393 0.4263 0.416 0.376(b)Method 1 Method 21 0.685 0.5962 0.525 0.6083 0.602 0.578(c)Instance-Based Generation 495part-of-speech as well as semantic information, does not have much opportunityto create a syntactically malformed sentence.
Furthermore, method 1 producessentences that are semantically more accurate than method 2.6.2 Ranking ResultsIn order to determine the quality of the ranking, the annotators had to rerank thegenerated questions.
We determine the distance between two rankings accordingto the Edit distance.
Since the generated lists are only of length 3, there are onlythree possibilties: the lists are equal (edit distance 0), one element in both lists isthe same (edit distance 2), and no element in the lists is the same, (edit distance3).
In order to allow easy comparison with the table above, we award scores of 1,0.5 and 0 for edit distances of 0, 2 and 3, respectively (i.e., 1 is best, 0 is worst).The annotators were asked to rank the questions according to syntactic criteriaalone, semantic criteria alone and all criteria.
The results are shown in Table 2.Table 2.
Comparison of ranking: Syntactic, semantic and overallMethod 1 Method 21 0.493 0.8932 0.813 0.8603 0.767 0.227(a)Method 1 Method 21 0.720 0.8732 0.760 0.7803 0.567 0.353(b)Method 1 Method 21 0.766 0.8532 0.740 0.7263 0.573 0.213(c)It can be seen that method 2 ranks the example sentences in a way that ismore in line with the choices of the annotators than method 1.6.3 Quality of RankingWe hypothesize that the differences in the performance of the algorithms is dueto the different selection mechanisms.
In order to validate this point, we askedthe three annotators to each provide one utterance they would rank highest foreach system question (called gold standard).
Then, we formed a list of 6 sentencesu?1, .
.
.
u?6 (3 generated by the generation algorithm and 3 by the annotators) andcompared for each dialogue context the scores sim(ut, u?i) for those 6 sentenceswhere ut is the user utterance from the corresponding test case.
We expect aperfect ranking algorithm to value the gold standard as least as high as anysentence from the corpus, and to value the gold standard higher every time theannotators found the generated sentences faulty.
It turns out that method 1places the sentences of the gold standard in the top 3 in 42.3% of the cases whilemethod 2 does this in 59.3% of the cases.7 DiscussionIt can be seen that in general, method 1 produces higher quality sentences whilemethod 2 ranks the sentences better.
We interpret this as follows.
For sentenceselection, the redundancy as provided by the IR engine is helpful, whereas forranking of example sentences, the additional structure as expressed in the ker-nel helps.496 M. Denecke and H. Tsukada7.1 Related WorkKiyota and colleagues [5] describe an interactive restricted domain question an-swering system where users can interactively retrieve causes for problems witha computers?
operating system.
Here, the problem of missing structure is solvedby providing so-called dialogue cards which provide the knowledge necessary fordialogue processing.
A dialogue card contains keywords, a question as asked bythe user in natural language (for example ?Windows does not boot?
), an infor-mation seeking question to be issued by the system (for example ?Which versionof Windows do you use?)
and a list of options associated with actions.
The ac-tions are executed in function of the users?
answer to the question.
Dialogueprocessing takes place by retrieving relevant dialogue cards, where relevance isdetermined by matching the users?
question and keywords with the question andkeywords noted on the dialogue card.
Compared to our method, this method re-quires substantially more structure to be represented in the dialogue cards andis therefore more expensive to develop.
Furthermore, the absence of any sort ofchange operators to adapt the question from the dialogue card to the current sit-uation does not provide as much flexibility as our method.
On the other hand,the highly structured dialogue cards give the developers more control (at theprice of a higher development cost) over the systems behavior than our methodand is therefore less risky in situations where failure is expensive.In Small et al[8], retrieved documents are forced into frame structures.
Mis-matches or between the fillers of the frame structures or missing fillers triggerinformation seeking questions to the user.
While the generation as it is actuallyused is not described in the paper, we believe that the frames provide sufficientstructure for template-based approaches.Hori and coworkers [4] developed an interactive question answering systembased on a Japanese newspaper corpus.
The purpose of information seeking ques-tions is to prompt the user for missing or disambiguating information.
From ageneration point of view, strong assumptions are made on the surface form ofthe generated information seeking question.
More specifically, ambiguous key-words are combined with disambiguating options by means of the Japanese par-ticle ?no?.7.2 SummaryTo summarize, the presented approaches attempt in different ways to compen-sate for the lack of structure in an question answering system.
Structure canbe provided explicitly as in the case of the dialogue cards, can be introducedduring processing as in the case of the frame-based document representations,and can be assumed in the target expression as in the case of the generationtemplates.
In contrast to the described methods, our method does not requirean explicit representation of structure.
Rather, the structure is given by what-ever structure the kernel and the change operators construct during generation.In other words, the structure our approach uses is (1) restricted to the questionto be generated and does not apply to the document level, and (2) in traditionwith the lazy learning characteristics of memory-based approaches is generatedon the fly on an as-needed basis, as opposed to being dictated from the outsetat design time.Instance-Based Generation 497AcknowledgementsWe acknowledge the help of Takuya Suzuki with the implementation.
Jun Suzukiprovided the implementation of the HDAG kernel.
We would like to thankHideki Isozaki and our colleagues at NTT CS labs for discussion andencouragement.References1.
M. Asahara and Y. Matsumoto.
2000.
Extended Models and Tools for High-Performance Part-of-Speech Tagger.
In Proceedings of The 18th InternationalConference on Computational Linguistics, Coling 2000, Saarbru?cken, Germany.2.
W. Daelemans.
1999.
Introduction to the Special Issue on Memory-Based LanguageProcessing.
Journal of Experimental and Theoretical Artificial Intelligence.3.
D. Haussler.
1999.
Convolution kernels on discrete structures.
Technical report,UC Santa Cruz.4.
C. Hori, T. Hori, H. Tsukada, H. Isozaki, Y. Sasaki, and E. Maeda.
2003.
Spokeninteractive odqa system: Spiqa.
In Proc.
of the 41th Annual Meeting of Associationfor Computational Linguistics (ACL-2003), Sapporo, Japan.5.
K. Kiyota, S. Kurohashi, and F. Kido.
2002.
?Dialog Navigator?
: A QuestionAnswering System based on Large Text Knowledge Base.
In Proceedings of The19th International Conference on Computational Linguistics, Coling 2002,Taipei,Taiwan.6.
I. Langkilde and K. Knight.
1998.
Generation that exploits Corpus-Based Statis-tical Knowledge.
In Proceedings of the Conference of the Association for Compu-tational Linguistics (COLING/ACL).7.
A.H. Oh and A. Rudnicky.
2000.
Stochastic Language Generation for SpokenDialogue Systems.
In ANLP/NAACL 2000 Workshop on Conversational Systems,pages 27?32.8.
S. Small and T. Strzalkowski.
2004.
Hitiqa: Towards analytical question answering.In Proceedings of The 20th International Conference on Computational Linguistics,Coling 2004,Geneva Switzerland.9.
C. Stanfill and D. Waltz.
1986.
Toward Memory-based Reasoning.
Communicationsof the ACM, vol.
29, pages 1213-1228.10.
J. Suzuki, T. Hirao, Y. Sasaki, and E. Maeda.
2003.
Hierarchical directed acyclicgraph kernel: Methods for structured natural language data.
In Proc.
of the 41thAnnual Meeting of Association for Computational Linguistics (ACL-2003), Sap-poro, Japan, pages 32?39.11.
S. Varges and C. Mellish.
2001.
Instance-based natural language generation.
InProceedings of the 2nd Meeting of the North American Chapter of the Associationfor Computational Linguistics, pages 1?8.12.
M. Walker, O. Rambow, and M. Rogati.
2001.
SPoT: A Trainable SentencePlanner.
In Proceedings of the North American Meeting of the Association forComputational Linguistics.
