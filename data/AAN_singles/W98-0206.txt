"I just played that a minute ago!
:" Designing User Interfaces forAudio NavigationJu l ia  Hirschberg,  John Choi, Chr ist ine Nakatan i ,  and Steve Whi t takerAT&T Labs - ResearchFlorham Park NJ 07932(j ulia,choi,chn,stevew)@research.att .
ornThe current popularity of multimodal informationretrieval research critically assumes that consumerswill be found for the multimodal information thusretrieved and that interfaces can be designed thatwill allow users to search and browse multimodalinformation effectively.
While there has been con-siderable ffort given to developing the basic tech-nologies needed for information retrieval from au-dio, video and text domains, basic research on howpeople browse and search in any of these domains,let alne in some combination, has lagged behind.
Indeveloping the SCAN (Spoken Content-based AudioNavigation) system to retrieve information from anaudio domain, we have attempted tostudy the prob-lems of how users navigate audio databases, hand inhand with the development of the speech and in-formation retrieval technologies which enable thisnavigation3SCAN was developed initially for the TREC-6Spoken Document Retrieval (SDR) task, which em-ploys the NIST/DARPA HUB4 Broadcast News cor-pus.
However, we are also developing a search andbrowsing system for voicemail access, over the tele-phone and via a GUI interface.
To this end, we havebuilt several user interfaces to both the voicemailand news domains, which we are employing in a se-ries of laboratory experiments designed to identifylimiting and enabling features of audio search andbrowsing interfaces.
We want to examine the fol-lowing questions: a) how do people want to searchaudio data?
what sort of search and play capabil-ities do they make most use of, when given severalalternatives?
b) do people search different sorts ofaudio data (e.g., familiar versus unfamiliar) differ-ently?
c) do people perform different types of audiosearch task (e.g.
finding a single fact vs. summariz-ing a longer audio document, or finding an audio)differently?
d) what are the major barriers to effi-ciency of audio search?
what additional aids mightt The SCAN audio browsing and retrieval system has beenunder development since June 1997 at AT&T Labs - Re-search, and represents collaborative work by Don Hindle, IvanMagrin-Chagnolleau, Fernando Pereira, Amit Singhal, andthe authors, with much additional help from Andrej Ljolje,Aaron Rosenberg and S. Parthasarathy.help to overcome these?
e) what design principlesunderly the creation of effective interfaces to audiodatabases?In this paper we present a brief overview of theSCAN system, describe several browsing prototypesand the different aspects of audio browsing/retrievalthey have been designed to test, and present resultsof two sets of experiments involving their use.
Wethen describe two novel browsers, developed fromthe results of these experiments, which employ doc-ument segmentation information and errorful auto-matic speech recognition transcription as aids to au-dio browsing, and briefly outline additional experi-mental work on their use.1 The  SCAN SystemSCAN was developed for the TREC-96 SDR task,a known item information retrieval (IR) task fromapproximately 47hours of the NIST/DARPA HUB4Broadcast News/SDR speech corpus.
Like most sys-tems participating in this task, SCAN uses auto-matic speech recognition (ASR) techniques to pro-duce an (errorful) transcription of the speech andthen applies text-based IR techniques on the tran-scription to rank the corresponding speech docu-ments as to their relevance to a given text query.Results of the IR ranking are returned to the user viaone of several interfaces, peech or text-transcriptiondriven, which are described below.The system architecture is shown in Figure 1.Speech documents, labeled by hand in the SDRVzzzFigure 1: Architecture of the SCAN audio brows-ing/retrieval system.data, are first run through an intonational phrase41detection procedure, to segment stories into princi-pled "chunks" of speech.
This initial segmentationprovides the ASR engine with manageable units ofspeech to recognize, and later provides users withmanageable units of speech to listen to.
Each phraseidentified is then classified as to its channel condi-tions, so that the most appropriate acoustic mod-els can be applied during recognition; this classifica-tion is done by assigning log-likelihood scores basedupon a partition of the data into 'high', 'medium',and 'low' fidelity speech (corresponding to 'studio8K speech', '8K speech recorded under other con-ditions', and '4K telephone speech' - all recordedwithout background noise or music) and noise (allspeech recorded with noise or music).
The rec-ognizer employs a time-synchronous beam searchwith continuous density, three-state, left-to-right,context-dependent Hidden Markov phone models.The production of word sequences from phones isimplemented using weighted finite-state transduc-ers.
Recognition hypotheses are output as word lat-tices.
Several anguage models have been developedfrom the 116 million word SDR corpus, includingstandard backoff bigram (6.1 million) and trigam(9.4 million) models and a compacted trigram modelwhich has 16% lower perplexity and which is 60%smaller than the bigram model.
We use the SMARTIR system, developed for text-based retrieval usingthe vector space model.
SMART tokenizes the tran-scribed audio, removes common stop words (e.g.
the,o\]), normalizes word variants (e.g.
persons ~ per-son), and weights term occurrences based upon doc-ument length and word frequency.
SMART scoresfor 'hits' within a document are made available tothe user interface, as well as the ranking of docu-ments retrieved for a query.
Two user interfaces cur-rently exist for SCAN, a speech-based interface, anda text-based interface.
Both of these interfaces re-flect lessons learned from our earlier audio browsingexperiments with simpler prototypes and a voicemailbrowsing and retrieval task.2 Audio-Based Browsing andRetrievalIn recent years, various systems have been builtto enable capture and browsing of spoken conver-sational data from meetings and recorded lectures(Hindus, Schmandt, and Horner, 1993; Kazman etal., 1996; Moran et al, 1997; Wolf, Rhyne, andBriggs, 1992; Whittaker, Hyland, and Wiley, 1994),and personally dictated information (Degen, Man-der, and Salomon, 1992; Stifelman et al, 1993).Other systems allow search of multimedia archives oftelevision programmes (Hauptmann and Witbrock,1997; Shahraray, 1995) and videomail (Jones et al,1996).
While extensive valuations of this technol-ogy remain to be carried out, naturalistic studies ofaudio browsing systems demonstrate heir effective-ness in helping users produce accurate meeting sum-maries (Moran et al, 1997; Whittaker, Hyland, andWiley, 1994; Wilcox, Schilit, and Sawhney, 1997).These and other studies also showed that indexedaudio produces more accurate recall, although usersmay take longer to retrieve information (Kazmanet al, 1996; Whittaker, Hyland, and Wiley, 1994).Several factors that may influence browsing behav-ior have been identified: (a) familiarity with subjectmatter: knowledgeable users are more likely to skipportions of the audio record when replaying (Moranet al, 1997) and they generate more effective querieswhen searching the record (Kazman et al, 1996); (b)type of retrieval task: audio search behaviors differwhen users are trying to summarize as opposed toextract verbatim information from the audio record(Moran et al, 1997; Whittaker, Hyland, and Wiley,1994); (c) presence and type of audio indices pro-vided: cue utility is esoteric, with different users re-lying on different types of cue (Kazman et al, 1996);(d) availability of segmental information: users findit easier to navigate the record when structural in-formation is provided (Arons, 1994).
However, thesestudies also identify severe difficulties that users ex-perience with speech browsing and search which maycompromise the utility of these systems.
The firstproblem is navigational: users often report losingtrack of the current audio context (Stifelman, 1996;Arons, 1994), and being unable to determine the se-quence and structure of different elements of the au-dio record (Gould, 1983; Haas and Hayes, 1986).
Asecond set of problems concern search: users seemto be poor at generating effective key word searchqueries, and find it hard to exploit system-generatedkey word indices.
These problems are exacerbatedwhen search material is unfamiliar (Kazman et al,1996).2.1 The Exper iment DesignIn our first set of experiments we focussed on iden-tifying users' own search strategies when given a setof tasks involving access to a relatively small audiodatabase and two relatively impoverished GUI inter-faces to that database.
More specifically we wantedto first identify the strategies users employ to browseand search audio - -  e.g., how do users find infor-mation in audio?
Do they sample small segmentsor listen to large chunks?
Second, we wanted toinvestigate the factors affecting these strategies - -e.g., do users search familiar information differentlyfrom novel material and if so how?
Is their searchstrategy different when they are looking for verba-tim rather than summary information?
Does provid-ing segmental information aid search significantly?Do other kinds of cues or indices promote ffectivesearch?
Third, we hoped to explore users' memoryand mental models of audio and investigate the re-42lationship between memory and search strategies - -do users with more accurate models search audiomore effectively, and what promotes good memory?We based our experiments on findings from anaturalistic study of over 800 voicemail users, inwhich we identified a set of strategies people usedto access a real audio archive, and documentedthe problems users experience in accessing thatarchive (Hirschberg and Whittaker, 1997; Whit-taker, Hirschberg, and Nakatani, 1998).
In our lab-oratory experiments we focussed first on how accessis affected by two factors, task type and familiarityof material.
While previous research as suggestedthat these factors affect browsing, no detailed eval-uation has been done.
Second, we investigated theimpact of two browser features, topic structure andplay duration.
Although these features have beenimplemented in previous browsers, their impact onbrowsing and their interaction with task and famil-iarity has not been systematically tested.
Our hy-potheses were that a) search efficiency (i.e.
numberof search operations and search time) depends onthe amount of speech information users must access:summary tasks requiring access to an entire topicwill less efficient han search for two specific facts,which in turn will be less efficient than search forone fact; b) familiar material will elicit more efficientsearch; c) providing information about where topicsbegin will increase the efficiency of search; and, d)short duration fixed play intervals will be used foridentifying relevant topics, whereas longer fixed playdurations will be used for search within a topic.Fourteen people were given a speech archive, con-sisting of eight voicemail messages, or topics, ap-pended together in one audio file 236.3 seconds long.We chose this domain for our study, both because wewere interested in the specific application, and be-cause Voicemail message retrieval is an example of areal application of audio search and retrieval, whichwe felt would be familiar to our users.
Users ac-cessed the archive to answer sixteen questions aboutthe eight topics.
These questions were based on re-trieval tasks identified as common in our naturalisticstudy of voicemail users.
There were three types oftask: Four questions required users to access onespecific fact, e.g.
a date or phone number froma topic ( l fact),  a further four required access oftwo such facts (2fact), and eight questions requiredusers to reproduce the gist of a topic (summary) .The first eight questions required users to ac-cess each of the eight topics once, and questions 9through 16 required each topic to be accessed again.To investigate the effects of familiarity we comparedusers' performance on the first eight versus the sec-ond eight of the sixteen questions.Users were given one of two GUI browsers: ba-sic and topic.
These are shown in Figure 2.
BothFigure 2: Basic and Topic Audio Browsersbrowsers represent the entire speech archive as a hor-izontal bar and permit random access to it: users canselect any point in the archive and play from thatpoint (e.g.
inserting the cursor halfway across thebar begins play halfway through the archive).
Forboth browsers, users then select one of three playdurations: play short (3 seconds), play long (10 sec-onds) and p lay to end (unrestricted play until playis manually halted by the user).
The topic  browserfurther allows the user to select a given topic by se-rial position (e.g.
topic, or, message 1); play willthen begin at the start of that topic/message.We used a simple GUI for our initial experiments,rather than testing a complex set of possible searchfeatures, for three reasons: First, data on accessing areal speech archive indicate that even highly experi-enced users make little use of sophisticated featuressuch as scanning, speed up/slow down, or jump for-ward/back (Hirschberg and Whittaker, 1997).
Sec-ond, informal evaluations of complex speech UIs re-veal that advanced browsing features are often notwell understood by users, and do not necessarilyimprove search (Arons, 1994; Hauptmann and Wit-brock, 1997).
Given the unclear benefits of complexfeatures, we wanted to establish baseline data forspeech retrieval using a simple prototype.
Finally,the features we tested will most likely be part of anybrowsing interface, and thus are of general interest.Users were given 5-10 minutes on practice tasksbefore the experiment.
After it, we gave users amemory test, asking them to recall the content,name of caller and serial position of each topic.
Wethen administered a questionnaire eliciting reactionsto browser features and comments about the tasks.We logged the number and type of each play opera-tion, duration and location of played speech withinthe archive, and time to answer each question.
Theresults for each hypothesis follow and all differencesdiscussed are statistically significant at p i 0.05, us-ing ANOVA.432.2 Experimental ResultsAs we had expected, 1fact tasks were answered moreefficiently than both other tasks (see Table 1).
How-ever, contrary to expectations, ummary  was moreefficient han 2fact,  despite requiring access to moreinformation.
The results indicate that performancedepends both on the type and the amount of infor-mation users must access.
User comments revealedwhy 2fact were so difficult: with summaries it waspossible to remember several pieces of approximateinformation.
2fact questions required complex nav-igation within topic and the additional precision re-quired to retain verbatim information often meantthat users forgot one fact while searching for thesecond.
They then found it hard to relocate the factthey had just forgotten.
The user logs reveal prob-lems of forgetting and relocating prior facts.
In thecourse of answering each 2fact usrquestion users ac-tually played the two target facts a combined totalof 7.9 times.
In contrast arget facts for l fact  taskswere only accessed 1.5 times and topics 2.9 times forsummary  tasks.As we had suspected, in general, familiar materialelicited more efficient search.
To investigate moredeeply just how this effect was produced, we thenseparated overall search operations into: the identifi-cation of the relevant topic and the actual extractionof the information required to complete the task, i.e.,finding the answer within the target topic.
We thenfound that familiarity only improved the speed oftopic identification, but had no effect on informa-tion extraction once the relevant source had beenidentified.Users made frequent use of topic boundary infor-mation.
Although random access was available withthe topic browser, users only employed it for 33%of their access operations.
Furthermore, users' com-ments about the topic boundary feature were highlypositive.
Despite this positive feedback however,we found that topic-based access seemed less effi-cient than random access: users with access to topicdelimiters took more operations although less timeto answer questions than other users.
Why mightthis counter-intuitive r sult have occurred?
Post-hoc tests showed that topic browser users had worsememory for the eight topics than simple browserusers.
Users of the basic browser reported mak-ing strenuous efforts to learn a mental model of thearchive.
In contrast, reliance on topic structure maypermit topic browser users never to do so.Play duration behavior was independent ofwhether search was within or outside topic.
Further-more, there was little use of either of the fixed playoperations: all users preferred unrestricted play.
Inthe final questionnaire, users reported that fixed du-ration options reduced their comprehension bytrun-cating topic playback in unpredictable places.
Theypreferred the greater control of unrestricted play,even though this meant the overhead of stoppingplay explicitly.From these experiments we conclude, first, thatusers were much better at comprehending the over-all structure of the archive, including the order andgist of topics, than they were at navigating more lo-cally, within a given topic, to find particular pieces ofinformation.
They were unable, for example, to re-locate previously accessed information within topicfor 2fact tasks, and showed no familiarity effects forsearch within topic.
Second, our sampling resultssuggest hat users overwhelmingly reject fixed dura-tion skims of salient speech information, when givenan alternative more within their control.
Instead offixed interval skimming, users prefer to access alientspeech by controlling the precise playback durationthemselves, even though this may involve more ef-fort on their part to start and stop play.
And third,providing topic boundaries may be of limited value:although users all like this feature (and those whoparticipated in the basic browsing condition specifi-cally requested it), heavy use of such signposts maymake it more difficult for users to learn the con-tents of the archive.
It appeared that the segmenta-tion provided was at too coarse a level of granular-ity to provide much additional navigational power;the general topic structure of the archive as a wholecould be learned easily without it.3 Segmentat ion  and  Transcr ip t ionA ids  to  Aud io  Nav igat ionThe results of our basic and topic browser studiesled us to propose two further browser prototypes,providing different ypes of additional, signpostinginformation that might be helpful in local, as wellas global navigation tasks.
Since our goal was topermit users to browse much larger databases thaneight voicemail messages, we also suspect that in-creasing the size of the database might increase theimportance of some form of topic segmentation.The first browser we developed provided a moresophisticated notion of topic segment han simplemessage boundaries, and is shown in Figure 3.In our early study of heavy voicemail users wehad learned anecdotally that callers who are used todoing business via voicemail believe that they andother such callers typically leave their messages incertain standard ways, with return telephone num-bers and names at the beginning and end of mes-sages, for example, and with content arranged insomewhat predictable fashion.
So we prepared handlabelings of our test voicemail messages, identifyingthe following parts within each message: greeting,"Hi, J im"; caller identification, "It's Valerie fromthe Customer Care committee"; topic, "I'm callingabout the meeting next week"; deliverables, "Can44Task Number of Operations Solution Time1fact2factsummaryfamiliarunfamiliartopicno topic2.44.12.9 (F -- 7.43)2.14.1 (F = 35.5)3.72.5 (F = 5.09)23.037.632.3 (F = 11.7)22.540.1 (F = 36.6)30.032.5 (F = 6.60)Table 1: Effects of Task, Familiarity and Topic Structure on Retrieval Efficiency, with Relevant F ANOVAValuesFigure 3: Voicemail Structural Browseryou call Joan and make sure she'll have the num-bers by then?
"; and closing "Bye now."
While wehave tested this interface only informally, the addi-tion of semantic ategories as signposts to brows-ing through a series of messages seems much moreuseful than simply iterating through messages bystart of message.
A browse through caller identify-ing phrases, for example, quickly identifies messagesby caller, while browsing through topics or deliver-ables serves the same function by topic.
And playinggreeting, caller id, topic, deliverables, and closingprovides a very effective summary of many message.Of course, even this outwardly simple identificationof topic structure is beyond the capability of exist-ing technology.
However, we are currently collectingand annotating a voicemail corpus with the goal ofadding this type of structural browsing capability tothe retrieval capabilities provided by our ASR/IRsearch engine.The second browser we developed in order to ex-periment with new types of navigational ids toaudio browsing makes use of the (errorful) ASRtranscription produced in order to carry out IR inour system.
This browser is depicted in Figure 4.The text-aided browser is implemented for the SDRBroadcast News corpus and consists of three maincomponents: a programs overview window, a speechfeedback window, and a player window.
The pro-grams overview presents the results of the IR searchRZSUtTS_,T~__ .
.
.
.
.
.
.
.
i .
i .+~T ................... ~L ._ .
. '
: _ _~_ .~L .
.
.
.
.
.Z~"~ ..."_'~ ........................................................i l ~PI  ~11 Th ing*  .C4ml$41~irld l i6 /17 I I  21i.73 192.14 611: MPI 111 Thl ?
Conl+dqlrld IS /S l  16 I g .
?
l  171.17 344 MPR * I I  Th ing ,  COn l ld l r ld  15/3 \ [  i i  1"7.85 i++.11 21S CNN T~ +ld  Tod l  v 17/12 24 ~S.
:S  l iB .S2  I IlnC ~Id  i~ ,  b ~/ lS  IS 14.S+ 157.12 71 ca  Ear l  v P r tmt l l l  k IS / IS  6 1'4,20 115.96 148 I~PR/PR I ~ l rk l lp  I l ee  I i i ; I  12 G 13,35 178?51 7n OHM ~l~l l~  ~ l  16/11 4 13.11 93.13 H~,nton ' i  ~ .
.
.
.
.
m.R .
i r  .
i ra .
re .
\[ I I I .
,m.  , I ,~ .
: H .
.
| ,  , I I I&~ Transh ip1  s~e  i luK oJ th l  p r l s i~ l l  lot  b l ing ks dvcis lvl  iD4 ~k  ?
i .."1 belt0,,* me ~7.~.~.~ p~opk~ c:sr~d 4e~l~ ~ how m ~  wewed m ~ wodd ~ I b~l~v*  me ip la ls ld*nt  c f in tan 's .
,  :p :  po i~y  tllC:K rlclDrCl of  we lk~@ls  IU't?I Is.
Gt41dlks'l'l Imcl double  t~X b4en.~, .~Qnl l  I~r  ~t  (4voted ~ s~.~u s for ci1JM ~ .~d i~rk :~ p4ww~r ~ p~o l?
tn h i*acl~,~ no doubt ~,bout plf~e~ men" ii "by me p~e~?
po l~ to, yard ~rm k~a s~ ~ ~ i ~ ~ me s~ e d d l ~  Nt  I~t?
~s~n ?
:@pz~bonFigure 4: Text-Aided Audio Browseron the corpus in the form of a list of stories rankedin order of their relevance to a text input query.
Thetop ten most relevant stories are displayed, in thisversion of the interface.
For each story, the title ofthe program from which the story comes, the date ofthe broadcast, and all instances of keywords in thestory that were deemed relevant to the query are dis-played.
Clicking on one of the program/story but-tons loads the corresponding speech into the speech?
feedback window, along with a time-aligned cursor,which shows the location of the story in the speechstream.
The player window then provides controlsfor navigation and play within the displayed pro-gram, permitting the following functionality: a playbutton with plays from the point selected in thespeech feedback window; a stop play button; a move-to-beginning button; buttons which skip forward inthe speech by intonational phrase or larger intona-tionally defined units and buttons which skip back-45ward in the same units.
We have devised a seriesof tasks appropriate to the broadcast news domainbut similar to the tasks used in our voicemail study,and will use this interface to test the utility of au-tomatically derived transcription and keyword iden-tification, as well as acoustically identified prosodicunits, in aiding local navigation.4 D iscuss ionA central problem with current access to large au-dio databases i  the need to listen to large amountsof relevant data; the human eye skims much morequickly than is possible for the human ear to do.Also, when skimming text, humans typically are pro-vided with many conventional orthographic and for-matting guides, such as headings and paragraphs.Our study of audio browsing in even a small au-dio corpus demonstrates that, while some kind ofnavigational ids seem necessary to provide the con-text which permits successful navigation, obvioussignposts uch as topicimessage boundaries may beless helpful than users expect hem to be and per-haps even counter-productive to users acquiring abasic understanding of their data.
Given this re-sult, we are exploring alternatives to simple topicmarkers, including semantic structural information,potentially errorful transcription and key word re-trieval, and acoustic segmentation, particularly as ameans of enhancing users' ability to extract he in-formation they seek from the audio data that hasbeen presented to them.ReferencesArons, B.
1994.
Interactively Skimming Speech.Ph.D.
thesis, MIT Media Lab.Degen, L., R. Mander, and G Salomon.
1992.Working with audio: Integrating personal taperecorders and desk-top computers.
In HumanFactors in Computing Systems: CHI '92 Confer-ence Proceedings, pages 413-418.Gould, J.
1983.
Human factors challenges: Thespeech filing system approach.
ACM Transactionson Office Information Systems, 1(4), October.Haas, C. and J. Hayes.
1986.
What did i just say?reading problems in writing with the machine.
Re-search in the Teaching of English, 20(1).Hauptmann, A. and M. Witbrock.
1997.
News-on-demand multimedia information acquisition andretrieval.
In M. Maybury, editor, Intelligent Mul-timedia Information Retrieval.
AAAI Press.Hindus, D., C. Schmandt, and C. Homer.
1993.Capturing, structuring, and representing ubiqui-tous audio.
ACM Transactions on InformationSystems, 11:376-400.Hirschberg, Julia and Steve Whittaker.
1997.Studying search and archiving in a real audiodatabase.
In Proceedings of the AAA11997 SpringSymposium on Intelligent Integration and Use ofText, Image, Video and Audio Corpora, Stanford,March.
AAAI.Jones, G. J. F., J. T FFoote, K. Sparck Jones, andS.
J.
Young.
1996.
P~etrieving spoken documentsby combining multiple index sources.
In Proceed-ings of SIGIR 96, Zurich, August.
ACM.Kasman, R., P~.
A1Halimi, W. Hunt, and M. Mantei.1996.
Four paradigms for indexing video confer-ences.
IEEE Multimedia, 3(1):63-73.Moran, T. P., L. Palen, S. Harrison, P. Chiu,D.
Kimber, S. Minneman, W. Van Melle, andP.
Zellweger.
1997.
"i'll get that off the au-dio": A case study of salvaging multimedia meet-ing records.
In Human Factors in ComputingSystems: CHI '97 Conference Proceedings, pages202-209.Shahraray, Behzad.
1995.
Scene change detectionand content-based sampling of video sequences.In Robert J. Safranek and Arturo A. Rodriguez,editors, Proceedings of the SPIE Conference onDigital Video Compression: Algorithms and Tech-nologies, February.Stifelman, L. 1996.
Augmenting real-world objects:A paper-based audio notebook.
Human Factorsin Computing Systems: CHI '96 Conference Com-panion, pages 199-200.Stifelman, L., B. Arons, C. Schmandt, and E. Hul-teen.
1993.
Voicenotes: A speech interface for ahand-held voice notetaker.
In Human Factors inComputing Systems: CHI '93 Conference Proceed-ings, pages 179-186.Whittaker, S., P. Hyland, and M. Wiley.
1994.Filochat: Handwritten otes provide access torecorded conversations.
In Human Factors inComputing Systems: CHI '93 Conference Proceed-ings, pages 271-277, New York.
ACM Press.Whittaker, Steve, Julia Hirschberg, and ChristineNakatani.
1998.
All talk and all action: strate-gies for managing voicemail messages.
In HumanFactors in Computing Systems: CHI '98 Confer-ence Proceedings, Los Angeles.Wilcox, L. D., B. N. Schilit, and N. Sawhney.
1997.Dynomite: A dynamically organized ink and au-dio notebook.
In Human Factors in ComputingSystems: CHI '97 Conference Proceedings.Wolf, C., J. Rhyne, and L. Briggs.
1992.
Communi-cation and information retrieval with a pen-basedmeeting support ool.
In Proceedings of CSCW-92, pages 322-329.46
