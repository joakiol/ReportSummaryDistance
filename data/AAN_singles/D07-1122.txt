Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp.
1129?1133,Prague, June 2007. c?2007 Association for Computational LinguisticsA Two-stage Parser for Multilingual Dependency ParsingWenliang Chen, Yujie Zhang, Hitoshi IsaharaComputational Linguistics GroupNational Institute of Information and Communications Technology3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289{chenwl, yujie, isahara}@nict.go.jpAbstractWe present a two-stage multilingual de-pendency parsing system submitted to theMultilingual Track of CoNLL-2007.
Theparser first identifies dependencies using adeterministic parsing method and then labelsthose dependencies as a sequence labelingproblem.
We describe the features used ineach stage.
For four languages with differ-ent values of ROOT, we design some spe-cial features for the ROOT labeler.
Then wepresent evaluation results and error analysesfocusing on Chinese.1 IntroductionThe CoNLL-2007 shared tasks include two tracks:the Multilingual Track and Domain AdaptationTrack(Nivre et al, 2007).
We took part the Multi-lingual Track of all ten languages provided by theCoNLL-2007 shared task organizers(Hajic?
et al,2004; Aduriz et al, 2003; Mart??
et al, 2007; Chenet al, 2003; Bo?hmova?
et al, 2003; Marcus et al,1993; Johansson and Nugues, 2007; Prokopidis etal., 2005; Csendes et al, 2005; Montemagni et al,2003; Oflazer et al, 2003) .In this paper, we describe a two-stage parsingsystem consisting of an unlabeled parser and a se-quence labeler, which was submitted to the Multi-lingual Track.
At the first stage, we use the pars-ing model proposed by (Nivre, 2003) to assign thearcs between the words.
Then we obtain a depen-dency parsing tree based on the arcs.
At the sec-ond stage, we use a SVM-based approach(Kudo andMatsumoto, 2001) to tag the dependency label foreach arc.
The labeling is treated as a sequence la-beling problem.
We design some special featuresfor tagging the labels of ROOT for Arabic, Basque,Czech, and Greek, which have different labels forROOT.
The experimental results show that our ap-proach can provide higher scores than average.2 Two-Stage Parsing2.1 The Unlabeled ParserThe unlabeled parser predicts unlabeled directed de-pendencies.
This parser is primarily based on theparsing models described by (Nivre, 2003).
The al-gorithm makes a dependency parsing tree in one left-to-right pass over the input, and uses a stack to storethe processed tokens.
The behaviors of the parserare defined by four elementary actions (where TOPis the token on top of the stack and NEXT is the nexttoken in the original input string):?
Left-Arc(LA): Add an arc from NEXT to TOP;pop the stack.?
Right-Arc(RA): Add an arc from TOP toNEXT; push NEXT onto the stack.?
Reduce(RE): Pop the stack.?
Shift(SH): Push NEXT onto the stack.Although (Nivre et al, 2006) used the pseudo-projective approach to process non-projective de-pendencies, here we only derive projective depen-dency tree.
We use MaltParser(Nivre et al, 2006)1129V0.41 to implement the unlabeled parser, and usethe SVM model as the classifier.
More specifically,the MaltParser use LIBSVM(Chang and Lin, 2001)with a quadratic kernel and the built-in one-versus-all strategy for multi-class classification.2.1.1 Features for ParsingThe MaltParser is a history-based parsing model,which relies on features of the derivation historyto predict the next parser action.
We represent thefeatures extracted from the fields of the data repre-sentation, including FORM, LEMMA, CPOSTAG,POSTAG, and FEATS.
We use the features for alllanguages that are listed as follows:?
The FORM features: the FORM of TOP andNEXT, the FORM of the token immediatelybefore NEXT in original input string, and theFORM of the head of TOP.?
The LEMMA features: the LEMMA of TOPand NEXT, the LEMMA of the token immedi-ately before NEXT in original input string, andthe LEMMA of the head of TOP.?
The CPOS features: the CPOSTAG of TOP andNEXT, and the CPOSTAG of next left token ofthe head of TOP.?
The POS features: the POSTAG of TOP andNEXT, the POSTAG of next three tokens af-ter NEXT, the POSTAG of the token immedi-ately before NEXT in original input string, thePOSTAG of the token immediately below TOP,and the POSTAG of the token immediately af-ter rightmost dependent of TOP.?
The FEATS features: the FEATS of TOP andNEXT.But note that the fields LEMMA and FEATS are notavailable for all languages.2.2 The Sequence Labeler2.2.1 The Sequence ProblemWe denote by x = x1, ..., xn a sentence with nwords and by y a corresponding dependency tree.
Adependency tree is represented from ROOT to leaves1The tool is available athttp://w3.msi.vxu.se/?nivre/research/MaltParser.htmlwith a set of ordered pairs (i, j) ?
y in which xj is adependent and xi is the head.
We have produced thedependency tree y at the first stage.
In this stage, weassign a label l(i,j) to each pair.As described in (McDonald et al, 2006), we treatthe labeling of dependencies as a sequence labelingproblem.
Suppose that we consider a head xi withdependents xj1, ..., xjM .
We then consider the la-bels of (i, j1), ..., (i, jM) as a sequence.
We use themodel to find the solution:lmax = arg maxls(l, i, y, x) (1)And we consider a first-order Markov chain of la-bels.We used the package YamCha (V0.33)2 to imple-ment the SVM model for labeling.
YamCha is apowerful tool for sequence labeling(Kudo and Mat-sumoto, 2001).2.2.2 Features for LabelingAfter the first stage, we know the unlabeled de-pendency parsing tree for the input sentence.
Thisinformation forms the basis for part of the featuresof the second stage.
For the sequence labeler, wedefine the individual features, the pair features, theverb features, the neighbor features, and the positionfeatures.
All the features are listed as follows:?
The individual features: the FORM, theLEMMA, the CPOSTAG, the POSTAG, andthe FEATS of the parent and child node.?
The pair features: the direction of depen-dency, the combination of lemmata of theparent and child node, the combination ofparent?s LEMMA and child?s CPOSTAG, thecombination of parent?s CPOSTAG and child?sLEMMA, and the combination of FEATS ofparent and child.?
The verb features: whether the parent or childis the first or last verb in the sentence.?
The neighbor features: the combination ofCPOSTAG and LEMMA of the left and rightneighbors of the parent and child, number ofchildren, CPOSTAG sequence of children.2YamCha is available athttp://chasen.org/?taku/software/yamcha/1130?
The position features: whether the child is thefirst or last word in the sentence and whetherthe child is the first word of left or right of par-ent.2.2.3 Features for the Root LabelerBecause there are four languages have differentlabels for root, we define the features for the rootlabeler.
The features are listed as follows:?
The individual features: the FORM, theLEMMA, the CPOSTAG, the POSTAG, andthe FEATS of the parent and child node.?
The verb features: whether the child is the firstor last verb in the sentence.?
The neighbor features: the combination ofCPOSTAG and LEMMA of the left and rightneighbors of the parent and child, number ofchildren, CPOSTAG sequence of children.?
The position features: whether the child is thefirst or last word in the sentence and whetherthe child is the first word of left or right of par-ent.3 Evaluation ResultsWe evaluated our system in the Multilingual Trackfor all languages.
For the unlabeled parser, we chosethe parameters for the MaltParser based on perfor-mance from a held-out section of the training data.We also chose the parameters for Yamcha based onperformance from training data.Our official results are shown at Table 1.
Perfor-mance is measured by labeled accuracy and unla-beled accuracy.
These results showed that our two-stage system can achieve good performance.
For alllanguages, our system provided better results thanaverage performance of all the systems(Nivre et al,2007).
Compared with top 3 scores, our systemprovided slightly worse performance.
The reasonsmay be that we just used projective parsing algo-rithms while all languages except Chinese have non-projective structure.
Another reason was that we didnot tune good parameters for the system due to lackof time.Data Set LA UAArabic 74.65 83.49Basque 72.39 78.63Catalan 86.66 90.87Chinese 81.24 85.91Czech 73.69 80.14English 83.81 84.91Greek 74.42 81.16Hungarian 75.34 79.25Italian 82.04 85.91Turkish 76.31 81.92average 78.06 83.22Table 1: The results of proposed approach.
LA-BELED ATTACHMENT SCORE(LA) and UNLA-BELED ATTACHMENT SCORE(UA)4 General Error Analysis4.1 ChineseFor Chinese, the system achieved 81.24% on labeledaccuracy and 85.91% on unlabeled accuracy.
Wealso ran the MaltParser to provide the labels.
Be-sides the same features, we added the DEPREL fea-tures: the dependency type of TOP, the dependencytype of the token leftmost of TOP, the dependencytype of the token rightmost of TOP, and the de-pendency type of the token leftmost of NEXT.
Thelabeled accuracy of MaltParser was 80.84%, 0.4%lower than our system.Some conjunctions, prepositions, and DE3 at-tached to their head words with much lower ac-curacy: 74% for DE, 76% for conjunctions, and71% for prepositions.
In the test data, these wordsformed 19.7%.
For Chinese parsing, coordinationand preposition phrase attachment were hard prob-lems.
(Chen et al, 2006) defined the special featuresfor coordinations for chunking.
In the future, weplan to define some special features for these words.Now we focused words where most of the errorsoccur as Table 2 shows.
For ?
?/DE?, there was32.4% error rate of 383 occurrences.
And most ofthem were assigned incorrect labels between ?prop-erty?
and ?predication?
: 45 times for ?property?
in-stead of ?predication?
and 20 times for ?predica-tion?
instead of ?property?.
For examples, ?
?/DE?3including ??/?/?/?
?.1131num any head dep both?/ DE 383 124 35 116 27a/ C 117 38 36 37 35?/ P 67 20 6 19 5?
?/ N 31 10 8 4 2?/ V 72 8 8 8 8Table 2: The words where most of errors occur inChinese data.in ???/?/??/??
(popular TV channel)?
wasto be tagged as ?property?
instead of ?predication?,while ??/DE?
in ????/?/??
(volunteer ofmuseum)?
was to be tagged as ?predication?
insteadof ?property?.
It was very hard to tell the labels be-tween the words around ???.
Humans can makethe distinction between property and predication for??
?, because we have background knowledge ofthe words.
So if we can incorporate the additionalknowledge for the system, the system may assignthe correct label.For ?a/C?, it was hard to assign the head, 36wrong head of all 38 errors.
It often appeared atcoordination expressions.
For example, the headof ?a?
at ??/?/?/?/a/?/?/?/?
?/(Besidesextreme cool and too amazing)?
was ???
?, andthe head of ?a?
at ????/??/?/??/a/?/??/?/??
(Give the visitors solid and methodicalknowledge)?
was ???
?.5 ConclusionIn this paper, we presented our two-stage depen-dency parsing system submitted to the MultilingualTrack of CoNLL-2007 shared task.
We used Nivre?smethod to produce the dependency arcs and the se-quence labeler to produce the dependency labels.The experimental results showed that our system canprovide good performance for all languages.ReferencesA.
Abeille?, editor.
2003.
Treebanks: Building and UsingParsed Corpora.
Kluwer.I.
Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,A.
Diaz de Ilarraza, A. Garmendia, and M. Oronoz.2003.
Construction of a Basque dependency treebank.In Proc.
of the 2nd Workshop on Treebanks and Lin-guistic Theories (TLT), pages 201?204.A.
Bo?hmova?, J.
Hajic?, E.
Hajic?ova?, and B. Hladka?.
2003.The PDT: a 3-level annotation scenario.
In Abeille?
(Abeille?, 2003), chapter 7, pages 103?127.C.C.
Chang and C.J.
Lin.
2001.
LIBSVM: a libraryfor support vector machines.
Software available athttp://www.
csie.
ntu.
edu.
tw/cjlin/libsvm, 80:604?611.K.
Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang,and Z. Gao.
2003.
Sinica treebank: Design criteria,representational issues and implementation.
In Abeille?
(Abeille?, 2003), chapter 13, pages 231?248.Wenliang Chen, Yujie Zhang, and Hitoshi Isahara.
2006.An empirical study of chinese chunking.
In COL-ING/ACL 2006(Poster Sessions), Sydney, Australia,July.D.
Csendes, J. Csirik, T. Gyimo?thy, and A. Kocsor.
2005.The Szeged Treebank.
Springer.J.
Hajic?, O.
Smrz?, P. Zema?nek, J.
S?naidauf, and E. Bes?ka.2004.
Prague Arabic dependency treebank: Develop-ment in data and tools.
In Proc.
of the NEMLAR In-tern.
Conf.
on Arabic Language Resources and Tools,pages 110?117.R.
Johansson and P. Nugues.
2007.
Extendedconstituent-to-dependency conversion for English.
InProc.
of the 16th Nordic Conference on ComputationalLinguistics (NODALIDA).Taku Kudo and Yuji Matsumoto.
2001.
Chunkingwith support vector machines.
In In Proceedings ofNAACL01.M.
Marcus, B. Santorini, and M. Marcinkiewicz.
1993.Building a large annotated corpus of English: the PennTreebank.
Computational Linguistics, 19(2):313?330.M.
A.
Mart?
?, M.
Taule?, L. Ma`rquez, and M. Bertran.2007.
CESS-ECE: A multilingual and multilevelannotated corpus.
Available for download from:http://www.lsi.upc.edu/?mbertran/cess-ece/.Ryan McDonald, Kevin Lerman, and Fernando Pereira.2006.
Multilingual dependency analysis with a two-stage discriminative parser.
In Proceedings of theTenth Conference on Computational Natural Lan-guage Learning (CoNLL-X), pages 216?220, NewYork City, June.
Association for Computational Lin-guistics.S.
Montemagni, F. Barsotti, M. Battista, N. Calzolari,O.
Corazzari, A. Lenci, A. Zampolli, F. Fanciulli,M.
Massetani, R. Raffaelli, R. Basili, M. T. Pazienza,D.
Saracino, F. Zanzotto, N. Nana, F. Pianesi, andR.
Delmonte.
2003.
Building the Italian Syntactic-Semantic Treebank.
In Abeille?
(Abeille?, 2003), chap-ter 11, pages 189?210.1132J.
Nivre, J.
Hall, J. Nilsson, G. Eryigit, and S Marinov.2006.
Labeled pseudo-projective dependency parsingwith support vector machines.J.
Nivre, J.
Hall, S. Ku?bler, R. McDonald, J. Nilsson,S.
Riedel, and D. Yuret.
2007.
The CoNLL 2007shared task on dependency parsing.
In Proc.
of theJoint Conf.
on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL).J.
Nivre.
2003.
An efficient algorithm for projectivedependency parsing.
Proceedings of the 8th Inter-national Workshop on Parsing Technologies (IWPT),pages 149?160.K.
Oflazer, B.
Say, D. Zeynep Hakkani-Tu?r, and G. Tu?r.2003.
Building a Turkish treebank.
In Abeille?
(Abeille?, 2003), chapter 15, pages 261?277.P.
Prokopidis, E. Desypri, M. Koutsombogera, H. Papa-georgiou, and S. Piperidis.
2005.
Theoretical andpractical issues in the construction of a Greek depen-dency treebank.
In Proc.
of the 4th Workshop on Tree-banks and Linguistic Theories (TLT), pages 149?160.1133
