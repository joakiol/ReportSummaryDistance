Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1044?1054,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsJoint Language and Translation Modeling with Recurrent Neural NetworksMichael Auli, Michel Galley, Chris Quirk, Geoffrey ZweigMicrosoft ResearchRedmond, WA, USA{michael.auli,mgalley,chrisq,gzweig}@microsoft.comAbstractWe present a joint language and transla-tion model based on a recurrent neural net-work which predicts target words based onan unbounded history of both source and tar-get words.
The weaker independence as-sumptions of this model result in a vastlylarger search space compared to related feed-forward-based language or translation models.We tackle this issue with a new lattice rescor-ing algorithm and demonstrate its effective-ness empirically.
Our joint model builds on awell known recurrent neural network languagemodel (Mikolov, 2012) augmented by a layerof additional inputs from the source language.We show competitive accuracy compared tothe traditional channel model features.
Ourbest results improve the output of a systemtrained on WMT 2012 French-English data byup to 1.5 BLEU, and by 1.1 BLEU on averageacross several test sets.1 IntroductionRecently, several feed-forward neural network-based language and translation models haveachieved impressive accuracy improvements on sta-tistical machine translation tasks (Allauzen et al2011; Le et al 2012b; Schwenk et al 2012).
In thispaper we focus on recurrent neural network archi-tectures, which have recently advanced the state ofthe art in language modeling (Mikolov et al 2010;Mikolov et al 2011a; Mikolov, 2012), outperform-ing multi-layer feed-forward based networks in bothperplexity and word error rate in speech recognition(Arisoy et al 2012; Sundermeyer et al 2013).
Themajor attraction of recurrent architectures is theirpotential to capture long-span dependencies sincepredictions are based on an unbounded history ofprevious words.
This is in contrast to feed-forwardnetworks as well as conventional n-gram models,both of which are limited to fixed-length contexts.Building on the success of recurrent architectures,we base our joint language and translation modelon an extension of the recurrent neural network lan-guage model (Mikolov and Zweig, 2012) that intro-duces a layer of additional inputs (?2).Most previous work on neural networks forspeech recognition or machine translation used arescoring setup based on n-best lists (Arisoy et al2012; Mikolov, 2012) for evaluation, thereby sidestepping the algorithmic and engineering challengesof direct decoder-integration.1 Instead, we exploitlattices, which offer a much richer representationof the decoder output, since they compactly encodean exponential number of translation hypotheses inpolynomial space.
In contrast, n-best lists are typi-cally very redundant, representing only a few com-binations of top scoring arcs in the lattice.
A majorchallenge in lattice rescoring with a recurrent neuralnetwork model is the effect of the unbounded historyon search since the usual dynamic programming as-sumptions which are exploited for efficiency do nothold up anymore.
We apply a novel algorithm to thetask of rescoring with an unbounded language modeland empirically demonstrate its effectiveness (?3).The algorithm proves robust, leading to signif-icant improvements with the recurrent neural net-work language model over a competitive n-grambaseline across several language pairs.
We even ob-serve consistent gains when pairing the model with alarge n-gram model trained on up to 575 times more1One notable exception is Le et al(2012a) who rescore reorder-ing lattices with a feed-forward network-based model.1044data, demonstrating that the model provides comple-mentary information (?4).Our joint modeling approach is based on adding acontinuous space representation of the foreign sen-tence as an additional input to the recurrent neu-ral network language model.
With this extension,the language model can measure the consistencybetween the source and target words in a context-sensitive way.
The model effectively combines thefunctionality of both the traditional channel and lan-guage model features.
We test the power of thisnew model by using it as the only source of tradi-tional channel information.
Overall, we find that themodel achieves accuracy competitive with the olderchannel model features and that it can improve overthe gains observed with the recurrent neural networklanguage model (?5).2 Model StructureWe base our model on the recurrent neural networklanguage model of Mikolov et al(2010) which isfactored into an input layer, a hidden layer with re-current connections, and an output layer (Figure 1).The input layer encodes the target language word attime t as a 1-of-N vector et, where |V | is the sizeof the vocabulary, and the output layer yt representsa probability distribution over target words; both ofsize |V |.
The hidden layer state ht encodes the his-tory of all words observed in the sequence up to timestep t. This model is extended by an auxiliary inputlayer ft which provides complementary informationto the input layer (Mikolov and Zweig, 2012).
Whilethe auxiliary input layer can be used to feed in arbi-trary additional information, we focus on encodingsof the foreign sentence (?5).The state of the hidden layer is determined by theinput layer, the auxiliary input layer and the hiddenlayer configuration of the previous time step ht?1.The weights of the connections between the layersare summarized in a number of matrices: U, F andW, represent weights from the input layer to the hid-den layer, from the auxiliary input layer to the hid-den layer, and from the previous hidden layer to thecurrent hidden layer, respectively.
Matrix V repre-sents connections between the current hidden layerand the output layer; G represents direct weights be-tween the auxiliary input and output layers.etht-1fthtytVGFWUDFigure 1: Structure of the recurrent neural networkmodel, including the auxiliary input layer ft.The hidden and output layers are computed via aseries of matrix-vector products and non-linearities:ht = s(Uet +Wht?1 + Ff t)yt = g(Vht +Gf t)wheres(z) =11 + exp {?z}, g(zm) =exp {zm}?k exp {zk}are sigmoid and softmax functions, respectively.Additionally, the network is interpolated with amaximum entropy model of sparse n-gram featuresover input words (Mikolov et al 2011a).2 The max-imum entropy weights are added to the output acti-vations before computing the softmax.The model is optimized via a maximum likeli-hood objective function using stochastic gradientdescent.
Training is based on the back propaga-tion through time algorithm, which unrolls the net-work and then computes error gradients over mul-tiple time steps (Rumelhart et al 1986).
Af-ter training, the output layer represents posteriorsp(et+1|ett?n+1,ht, ft); the probabilities of words inthe output vocabulary given the n previous inputwords ett?n+1, the hidden layer configuration ht aswell as the auxiliary input layer configuration ft.2While these features depend on multiple input words, we de-picted them for simplicity as a connection between the currentinput word vector et and the output layer (D).1045Na?
?ve computation of the probability distributionover the next word is very expensive for large vo-cabularies.
A well established efficiency trick usesword-classing to create a more efficient two-stepprocess (Goodman, 2001; Emami and Jelinek, 2005;Mikolov et al 2011b) where each word is assigneda unique class.
To compute the probability of aword, we first compute the probability of its class,and then multiply it by the probability of the wordconditioned on the class:p(et+1|ett?n+1,ht, ft) =p(ci|ett?n+1,ht, ft)?
p(et+1|ci, ett?n+1,ht, ft)This factorization reduces the complexity of com-puting the output probabilities from O(|V |) toO(|C| + maxi |ci|) where |C| is the number ofclasses and |ci| is the number of words in classci.
The best case complexity O(?|V |) requires thenumber of classes and words to be evenly balanced,i.e., each class contains exactly as many words asthere are classes.3 Lattice Rescoring with an UnboundedLanguage ModelWe evaluate our joint language and translationmodel in a lattice rescoring setup, allowing us tosearch over a much larger space of translations thanwould be possible with n-best lists.
While veryspace efficient, lattices also impose restrictions onthe context available to features, a particularly chal-lenging setting for our model which depends on theentire prefix of a translation.
In the ensuing de-scription we introduce a new algorithm to efficientlytackle this issue.Phrase-based decoders operate by maintaining aset of states representing competing translations, ei-ther partial or complete.
Each state is scored by anumber of features including the n-gram languagemodel.
The independence assumptions of the fea-tures determine the amount of context each stateneeds to maintain in order for it to be possible toassign a score to it.
For example, a trigram languagemodel is indifferent to any context other than thetwo immediately preceding words.
Assuming thetrigram model dominates the Markov assumptionsof all other features, which is typically the case, thenwe have to maintain at least two words at each state,also known as the n-gram context.1: function RESCORELATTICE(k, V , E, s, T )2: Q?
TOPOLOGICALLY-SORT(V )3: for all v in V do .
Heaps of split-states4: Hv ?
MINHEAP()5: end for6: h0 ?
~0 .
Initialize start-state7: Hs.ADD(h0)8: for all v in Q do .
Examine outgoing arcs9: for ?v, x?
in E do10: for h in Hv do .
Extend LM states11: h?
?
SCORERNN(h, phrase(h))12: parent(h?)?
h .
Backpointers13: if Hx.size() ?
k?
.
Beam width14: Hx.MIN()<score(h?)
then15: Hx.REMOVEMIN()16: if Hx.size()<k then17: Hx.ADD(h?
)18: end for19: end for20: end for21: I = MAXHEAP()22: for all t in T do .
Find best final split-state23: I.MERGE(Ht)24: end for25: return I.MAX()26: end functionFigure 2: Push-forward rescoring with a recurrent neu-ral network language model given a beam-width for lan-guage model split-states k, decoder states V , edges E, astart state s and final states T .However, a recurrent neural network languagemodel makes much weaker independence assump-tions.
In fact, the predictions of such a model dependon all previous words in the sentence, which wouldimply a potentially very large context.
But storingall words is an inefficient solution from a dynamicprogramming point of view.
Fortunately, we do notneed to maintain entire translations as context in thestates: the recurrent model compactly encodes theentire history of previous words in the hidden layerconfiguration hi.
It is therefore sufficient to add hias context, instead of the entire translation.
The lan-guage model can then simply score any new words1046based on hi from the previous state when a new stateis created.A much larger problem is that items, that werepreviously equivalent from a dynamic programmingperspective, may now be different.
Standard phrase-based decoders (Koehn et al 2007) recombine de-coder states with the same context into a singlestate because they are equivalent to the model fea-tures; usually recombination retains only the high-est scoring candidate.3 However, if the context islarge, then the amount of recombination will de-crease significantly, leading to less variety in the de-coder beam.
This was confirmed in preliminary ex-periments where we simulated context sizes of up to100 words but found that accuracy dropped by be-tween 0.5-1.0 BLEU.Integrating a long-span language model na?
?velyrequires to keep context equivalent to the entire leftprefix of the translation, a setting which would per-mit very little recombination.
Instead of using ineffi-cient long-span contexts, we propose to maintain theusual n-gram context and to keep a fixed number ofhidden layer configurations k at each decoder state.This leads to a new split-state dynamic programwhich splits each decoder state into at most k newitems, each with a separate hidden layer configura-tion representing an unbounded history (Figure 2).This maintains diversity in the explored translationhypothesis space and preserves high-scoring hiddenlayer configurations.What is the effect of this strategy?
To answerthis question we measured translation accuracy forvarious settings of k on our lattice rescoring setup(see ?4 for details).
In the same experiment, wecompare lattices to n-best lists in terms of accuracy,model score and wall time impact.4 The results (Ta-ble 1 and Figure 3) show that reranking accuracy onlattices is not significantly better, however, rescor-ing lattices with k = 1 is much faster than n-bestlists.
Similar observations have been made in previ-ous work on minimum error-rate training (Macherey3Assuming a max-translation decision rule.
In a minimum-risksetting, we may assign the sum of the scores of all candidatesto the retained item.4We measured running times on an HP z800 workstationequipped with 24 GB main memory and two Xeon E5640CPUs with four cores each, clocked at 2.66 GHz.
All experi-ments were run single-threaded.BLEU oracle sec/sentBaseline 28.25 - 0.173100-best 28.90 37.22 0.4701000-best 28.99 40.06 3.920lattice (k = 1) 29.00 43.50 0.093lattice (k = 10) 29.04 43.50 0.599lattice (k = 100) 29.03 43.50 4.531Table 1: Rescoring n-best lists and lattices with variouslanguage model beam widths k. Accuracy is based onthe news2011 French-English task.
Timing results are inaddition to the baseline.Figure 3: BLEU vs. log probabilities of 1-best transla-tions when rescoring n-best lists and lattices (cf.
Table 1).et al 2008).
The recurrent language model adds anoverhead of about 54% at k = 1 on top of the timeto produce the baseline 1-best output, a consider-able but not necessarily prohibitive overhead.
Largervalues of k return higher probability solutions, butthere is little impact on accuracy: the BLEU scoreis nearly identical when retaining up to 100 historiescompared to keeping only the highest scoring.While surprising at first, we believe that this ef-fect is due to the high similarity of the translationsrepresented by the histories in the beam.
Each his-tory represents a different translation but all transla-tion hypothesis share the same n-gram context, and,more importantly, they are translations of the sameforeign words, since they have exactly the same cov-erage vector.
These commonalities are likely to re-sult in similar recurrent histories, which in turn re-duces the effect of aggressive pruning.4 Language Model ExperimentsRecurrent neural network language models havepreviously only been used in n-best rescoring1047settings and on small-scale tasks with baselinelanguage models trained on only 17.5m words(Mikolov, 2012).
We extend this work by experi-menting on lattices using strong baselines with n-gram models trained on over one billion words andby evaluating on a number of language pairs.4.1 Experimental SetupBaseline.
We experiment with an in-house phrase-based system similar to Moses (Koehn et al2003), scoring translations by a set of common fea-tures including maximum likelihood estimates ofsource given target mappings pMLE(e|f) and viceversa pMLE(f |e), as well as lexical weighting es-timates pLW (e|f) and pLW (f |e), word and phrase-penalties, a linear distortion feature and a lexicalizedreordering feature.
Log-linear weights are estimatedwith minimum error rate training (Och, 2003).Evaluation.
We use training and test datafrom the WMT 2012 campaign and report resultson French-English, German-English and English-German.
Translation models are estimated on 102mwords of parallel data for French-English, 91mwords for German-English and English-German; be-tween 3.5-5m words are newswire, depending on thelanguage pair, and the remainder are parliamentaryproceedings.
The baseline systems use two 5-grammodified Kneser-Ney language models; the first isestimated on the target-side of the parallel data,while the second is based on a large newswire corpusreleased as part of the WMT campaign.
For French-English and German-English we use a languagemodel based on 1.15bn words, and for English-German we train a model on 327m words.
We eval-uate on the newswire test sets from 2010-2011 con-taining between 2034-3003 sentences.
Log-linearweights are estimated on the 2009 data set compris-ing 2525 sentences.
We rescore the lattices producedby the baseline systems with an aggressive but effec-tive context beam of k = 1 that did not harm accu-racy in preliminary experiments (?3).Neural Network Language Model.
The vocab-ularies of the language models are comprised ofthe words in the training set after removing single-tons.
We obtain word-classes using a version ofBrown-Clustering with an additional regularizationterm to optimize the runtime of the language model(Brown et al 1992; Zweig and Makarychev, 2013).Direct connections use maximum entropy featuresover unigrams, bigrams and trigrams (Mikolov et al2011a).
We use the standard settings for the modelwith the default learning rate ?
= 0.1 that decaysexponentially if the validation set entropy does notincrease after each epoch.
Back propagation throughtime computes error gradients over the past twentytime steps.
Training is stopped after 20 epochs orwhen the validation entropy does not decrease overtwo epochs.
We experiment with varying trainingdata sizes and randomly draw the data from the samecorpora used for the baseline systems.
Throughout,we use a hidden layer size of 100 which provided agood trade-off between time and accuracy in initialexperiments.4.2 ResultsTraining times for neural networks can be a majorbottleneck.
Recurrent architectures are particularlyhard to parallelize due to their inherent dependenceon the previous hidden layer configuration.
Onestraightforward way to influence training time is tochange the size of the training corpus.Our results (Table 2, Table 3 and Table 4) showthat even small models trained on only two millionwords significantly improve over the 1-best decoderoutput (Baseline); this represents only 0.6 percentof the data available to the n-gram model used bythe baseline.
Models of this size can be trained inonly about 3.5 hours.
A model trained on 50m wordstook 63 hours to train.
When paired with an n-grammodel trained on 25 times more data, accuracy im-proved by up to 0.7 BLEU on French-English.5 Joint Model ExperimentsIn the next set of experiments, we turn to the jointlanguage and translation model, an extension of therecurrent neural network language model with ad-ditional inputs for the foreign sentence.
We firstintroduce two continuous space representations ofthe foreign sentence (?5.1).
Using these represen-tations we evaluate the accuracy of the joint modelin the lattice rescoring setup and compare against thetraditional translation channel model features (?5.2).Next, we establish an upper bound on accuracy forthe joint model via an oracle experiment (?5.3).
In-spired by the results of the oracle experiment we1048dev news2010 news2011 newssyscomb2011 Avg(test)Baseline 26.6 27.6 28.3 27.5 27.8+RNNLM (2m) 27.5 28.1 28.6 28.1 28.3+RNNLM (50m) 27.7 28.2 29.0 28.1 28.5Table 2: French-English results when rescoring with the recurrent neural network language model; the baseline relieson an n-gram model trained on 1.15bn words.dev news2010 news2011 newssyscomb2011 Avg(test)Baseline 21.2 20.7 19.2 20.6 20.0+RNNLM (2m) 21.8 20.9 19.4 20.9 20.3+RNNLM (50m) 22.1 21.1 19.7 21.0 20.5Table 3: German-English results when rescoring with the recurrent neural network language model.dev news2010 news2011 newssyscomb2011 Avg(test)Baseline 15.2 15.6 14.3 15.7 15.1+RNNLM (2m) 15.7 15.9 14.6 16.0 15.4+RNNLM (50m) 15.8 15.9 14.7 16.1 15.5Table 4: English-German results when rescoring with the recurrent neural network language model; the baseline relieson an n-gram model trained on 327m words.train a transform between the source words and thereference representations.
This leads to the best re-sults improving 1.5 BLEU over the 1-best decoderoutput and adding 0.2 BLEU on average to the gainsachieved by the recurrent language model (?5.4).Setup.
Conventional language models can betrained on monolingual or bilingual data; however,the joint model can only be trained on the latter.In order to control for data size effects, we restricttraining of all models, including the baseline n-grammodel, to the target side of the parallel corpus, about102m words for French-English.
Furthermore wetrain recurrent models only on the newswire portion(about 3.5m words for training and 250k words forvalidation) since initial experiments showed compa-rable results to using the full parallel corpus, avail-able to the baseline.
This is reasonable since the testdata is newswire.
Also, it allows for more rapid ex-perimentation.5.1 Foreign Sentence RepresentationsWe represent foreign sentences either by latent se-mantic analysis (LSA; Deerwester et al1990) or byword encodings produced as a by-product of train-ing the recurrent neural network language model onthe source words.LSA is widely used for representing words anddocuments in low-dimensional vector space.
Themethod applies reduced singular value decomposi-tion (SVD) to a matrix M of word counts; in oursetting, rows represent sentences and columns rep-resent foreign words.
SVD reduces the numberof columns while preserving similarity among therows, effectively mapping from a high-dimensionalrepresentation of a sentence, as a set of words, to alow-dimensional set of concepts.
The output of SVDis an approximation of M by three matrices: T con-tains single word representations, R represents fullsentences, and S is a diagonal scaling matrix:M ?
TSRTGiven vocabulary V and n sentences, we constructM as a matrix of size |V ?n|.
The ij-th entry is thenumber of times word i occurs in sentence j, alsoknown as the term frequency value; the entry is alsoweighted by the inverse document frequency, the rel-ative importance of word i among all sentences, ex-pressed as the negative logarithm of the fraction ofsentences in which word i occurs.As a second representation we use single word1049embeddings implicitly learned by the input layerweights U of the recurrent neural network languagemodel (?2), denoted as RNN.
Each word is repre-sented by a vector of size |hi|, the number of neu-rons in the hidden layer; in our experiments, weconsider concatenations of individual word vectorsto represent foreign word contexts.
These encodingshave previously been found to capture syntactic andsemantic regularities (Mikolov et al 2013) and arereadily available in our experimental framework viatraining a recurrent neural network language modelon the source-side of the parallel corpus.5.2 ResultsWe first experiment with the two previously intro-duced representations of the source-side sentence.Table 5 shows the results compared to the 1-best de-coder output and an RNN language model (target-only).
We first try LSA encodings of the entireforeign sentence as 80 or 240 dimensional vectors(sent-lsa-dim80, sent-lsa-dim240).
Next, we experi-ment with single-word RNN representations of slid-ing word-windows in the hope of representing rel-evant context more precisely.
Word-windows areconstructed relative to the source words aligned tothe current target word, and individual word vec-tors are concatenated into a single vector.
Wefirst try contexts which do not include the alignedsource words, in the hope of capturing informationnot already modeled by the channel models, start-ing with the next five words (ww-rnn-dim50.n5),the five previous and the next five words (ww-rnn-dim50.p5n5) as well as the previous three words(ww-rnn-dim50.p3).
Next, we experiment withword-windows of up to five aligned source words(ww-rnn-dim50.c5).
Finally, we try contexts basedon LSA word vectors (ww-lsa-dim50.n5, ww-lsa-dim50.p3).5While all models improve over the baseline, nonesignificantly outperforms the recurrent neural net-work language model in terms of BLEU.
However,the perplexity results suggest that the models uti-lize the foreign representations since all joint mod-els improve vastly over the target-only language5We ignore the coverage vector when determining word-windows which risks including already translated words.Building word-windows based on the coverage vector requiresadditional state in a rescoring setting meant to be light-weight.
?p(f |e)?p(e|f) ?p(e|f)Baseline without CM 24.0 22.5+ target-only 24.5 22.6+ sent-lsa-dim240 24.9 23.3+ ww-rnn-dim50.n5 24.9 24.0+ ww-rnn-dim50.p5n5 24.6 23.7+ ww-rnn-dim50.p3 24.6 22.3+ ww-rnn-dim50.c5 24.9 24.0+ ww-lsa-dim50.n5 24.8 23.9+ ww-lsa-dim50.p3 23.8 23.2Table 6: Comparison of the joint model and the chan-nel model features (CM) by removing channel featurescorresponding to ?p(e|f) from the lattices, or both di-rections ?p(e|f),?p(f |e) and replacing them by vari-ous joint models.
We re-tuned the log-linear weights fordifferent feature-sets.
Accuracy is based on the averageBLEU over news2010, newssyscomb2010, news2011.model.
The lowest perplexity is achieved by thecontext covering the aligned source words (ww-rnn-dim50.c5) since the source words are a better pre-dictor of the target words than outside context.The experiments so far measured if the jointmodel can improve in addition to the four channelmodel features used by the baseline, that is, the max-imum likelihood and lexical translation features inboth translation directions.
The joint model clearlyoverlaps with these features, but how well doesthe recurrent model perform compared against thechannel model features?
To answer this question,we removed channel model features correspondingto the same translation direction as the joint model,specifically pMLE(e|f) and pLW (e|f), from the lat-tices and measured the effect of adding the jointmodels.The results (Table 6, column ?p(e|f)) clearlyshow that our joint models are competitive with thechannel model features by outperforming the orig-inal baseline with all channel model features (24.7BLEU) by 0.2 BLEU (ww-rnn-dim50.n5, ww-rnn-dim50.c5).
As a second experiment, we removed allchannel model features (column ?p(e|f), p(f |e)),diminishing baseline accuracy to 22.5 BLEU.
In thissetting, the best joint model is able to make up 1.5of the 2.2 BLEU lost due to removal of the channel1050dev news2010 news2011 newssyscomb2010 Avg(test) PPLBaseline 24.3 24.4 25.1 24.3 24.7 341target-only 25.1 25.1 26.4 25.0 25.6 218sent-lsa-dim80 25.2 25.2 26.3 25.1 25.6 147sent-lsa-dim240 25.1 25.0 26.2 24.9 25.4 126ww-rnn-dim50.n5 24.9 25.0 26.3 24.8 25.4 61ww-rnn-dim50.p5n5 25.0 24.8 26.2 24.7 25.3 59ww-rnn-dim50.p3 25.1 25.1 26.5 24.9 25.6 143ww-rnn-dim50.c5 24.8 24.9 26.0 24.8 25.3 16ww-lsa-dim50.n5 25.0 25.0 26.2 24.8 25.4 76ww-lsa-dim50.p3 25.1 25.1 26.5 24.9 25.6 151Table 5: Translation accuracy of the joint model with various encodings of the foreign sentence measured on theFrench-English task.
Perplexity (PPL) is based on news2011.model features, while modeling only a single trans-lation direction.
This setup also shows the negligibleeffect of the target-only language model in the ab-sence of translation scores, whereas the joint modelsare much more effective since they do model transla-tion.
Overall, the best joint models prove very com-petitive to the traditional channel features.5.3 Oracle ExperimentThe previous section examined the effect of a setof basic foreign sentence representations.
Althoughwe find some benefit from these representations, thedifferences are not large.
One might naturally askwhether there is greater potential upside from thischannel model.
Therefore we turn to measuring theupper bound on accuracy for the joint approach as awhole.Specifically, we would like to find a bound on ac-curacy given an ideal representation of the sourcesentence.
To answer this question, we conducted anexperiment where the joint model has access to anLSA representation of the reference translation.Table 7 shows that the joint approach has an ora-cle accuracy of up to 4.3 BLEU above the baseline.This clearly confirms that the joint approach can ex-ploit the additional information to improve BLEU,given a good enough representation of the foreignsentence.
In terms of perplexity, we see an improve-ment of up to 65% over the target-only model.
Itshould be noted that since LSA representations arecomputed on reference words, perplexity no longerhas its standard meaning.BLEU PPLBaseline 25.2 341target-only 26.4 218oracle (sent-lsa-dim40) 27.7 124oracle (sent-lsa-dim80) 28.5 103oracle (sent-lsa-dim160) 29.0 86oracle (sent-lsa-dim240) 29.5 76Table 7: Oracle accuracy of the joint model when us-ing an LSA encoding of the references, measured on thenews2011 French-English task.5.4 Target Language ProjectionsOur experiments so far showed that joint modelsbased on direct representations of the source wordsare very competitive to the traditional channel mod-els (?5.2).
However, these experiments have notshown any improvements over the normal recurrentneural network language model.
The previous sec-tion demonstrated that good representations can leadto substantial gains (?5.3).
In order to bridge the gap,we propose to learn a separate transform from theforeign words to an encoding of the reference targetwords, thus making the source-side representationslook more like the target-side encodings used in theoracle experiment.Specifically, we learn a linear transformd?
: x?
r mapping directly from a vector en-coding of the foreign sentence x to an l-dimensionalLSA representation r of the reference sentence.
Attest and training time we apply d?
to the foreignwords and use the transformation instead of a direct1051dev news2010 news2011 newssyscomb2010 Avg(test) PPLBaseline 24.3 24.4 25.1 24.3 24.7 341target-only 25.1 25.1 26.4 25.0 25.6 218proj-lsa-dim40 25.1 25.3 26.5 25.2 25.8 145proj-lsa-dim80 25.1 25.3 26.6 25.2 25.8 134Table 8: Translation accuracy of the joint model with a source-target transform, measured on the French-English task.Perplexity (PPL) is based on news2011; differences to target-only are significant at the p < 0.001 level.source-side representation.The transform models all foreign words in the par-allel corpus except singletons, which are collapsedinto a unique class, similar to the recurrent neuralnetwork language model.
We train the transform tominimize the squared error with respect to the ref-erence LSA vector using an SGD online learner:??
= argmin?n?i=1(ri ?
d?
(xi))2(1)We found a simple constant learning rate, tunedon the validation data, to be as effective as sched-ules based on constant decay, or reducing the learn-ing rate when the validation error increased.
Ourfeature-set includes unigram and bigram word fea-tures.
The value of unigram features is simply theunigram count in that sentence; bigram features re-ceive a weight of the bigram count divided by twoto help prevent overfitting.
Then the vector for eachsentence was divided by its L2 norm.
Both weight-ing and normalization led to substantial improve-ments in test set error.
More complex features suchas skip-bigrams, trigrams and character n-grams didnot yield any significant improvements.
Even thisrepresentation of sentences is composed of a largenumber of instances, and so we resorted to featurehashing by computing feature ids as the least signif-icant 20 bits of each feature name.
Our best trans-form achieved a cosine similarity of 0.816 on thetraining data, 0.757 on the validation data, and 0.749on news2011.The results (Table 8) show that the transform im-proves over the recurrent neural network languagemodel on all test sets and by 0.2 BLEU on average.We verified significance over the target-only modelusing paired bootstrap resampling (Koehn, 2004)over all test sets (7526 sentences) at the p < 0.001level.
Overall, we improve accuracy by up to 1.5BLEU and by 1.1 BLEU on average across all testsets over the decoder 1-best with our joint languageand translation model.6 Related WorkOur approach of combining language and translationmodeling is very much in line with recent work onn-gram-based translation models (Crego and Yvon,2010), and more recently continuous space-basedtranslation models (Le et al 2012a; Gao et al2013).
The joint model presented in this paper dif-fers in a number of key aspects: we use a recur-rent architecture representing an unbounded historyof both source and target words, rather than a feed-forward style network.
Feed-forward networks andn-gram models have a finite history which makespredictions independent of anything but a small his-tory of words.
Furthermore, we only model thetarget-side which is different to previous work mod-eling both sides.We introduced a new algorithm to tackle latticerescoring with an unbounded model.
The auto-matic speech recognition community has previouslyaddressed this issue by either approximating long-span language models via simpler but more tractablemodels (Deoras et al 2011b), or by identifying con-fusable subsets of the lattice from which n-best listsare constructed and rescored (Deoras et al 2011a).We extend their work by directly mapping a recur-rent neural network model onto the structure of thelattice, rescoring all states instead of focusing onlyon subsets.7 Conclusion and Future WorkJoint language and translation modeling with recur-rent neural networks leads to substantial gains overthe 1-best decoder output, raising accuracy by upto 1.5 BLEU and by 1.1 BLEU on average across1052several test sets.
The joint approach also improvesover the gains of the recurrent neural network lan-guage model, adding 0.2 BLEU on average acrossseveral test sets.
Our models are competitive to thetraditional channel models, outperforming them in ahead-to-head comparison.Furthermore, we tackled the issue of latticerescoring with an unbounded recurrent model bymeans of a novel algorithm that keeps a beam of re-current histories.
Finally, we have shown that therecurrent neural network language model can sig-nificantly improve over n-gram baselines across arange of language-pairs, even when the baselineswere trained on 575 times more data.In future work we plan to directly learn represen-tations of the source-side during training of the jointmodel.
Thus, the model itself can decide which en-coding is best for the task.
We also plan to changethe cross entropy objective to a BLEU-inspired ob-jective in a discriminative training regime, which wehope to be more effective.
We would also like to ap-ply recent advances in tackling the vanishing gradi-ent problem (Pascanu et al 2013) using a regular-ization term to maintain the magnitude of the gradi-ents during back propagation through time.
Finally,we would like to integrate the recurrent model di-rectly into first-pass decoding, a straightforward ex-tension of lattice rescoring using the algorithm wedeveloped.AcknowledgmentsWe would like to thank Anthony Aue, Hany Has-san Awadalla, Jon Clark, Li Deng, Sauleh Eetemadi,Jianfeng Gao, Qin Gao, Xiaodong He, Will Lewis,Arul Menezes, and Kristina Toutanova for helpfuldiscussions related to this work as well as for com-ments on previous drafts.
We would also like tothank the anonymous reviewers for their comments.ReferencesAlexandre Allauzen, He?le`ne Bonneau-Maynard, Hai-SonLe, Aure?lien Max, Guillaume Wisniewski, Franc?oisYvon, Gilles Adda, Josep Maria Crego, AdrienLardilleux, Thomas Lavergne, and Artem Sokolov.2011.
LIMSI @ WMT11.
In Proc.
of WMT, pages309?315, Edinburgh, Scotland, July.
Association forComputational Linguistics.Ebru Arisoy, Tara N. Sainath, Brian Kingsbury, and Bhu-vana Ramabhadran.
2012.
Deep Neural NetworkLanguage Models.
In NAACL-HLT Workshop on theFuture of Language Modeling for HLT, pages 20?28,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-cent J. Della Pietra, and Jenifer C. Lai.
1992.
Class-based n-gram models of natural language.
Computa-tional Linguistics, 18(4):467?479, Dec.Josep Crego and Franois Yvon.
2010.
Factored bilingualn-gram language models for statistical machine trans-lation.
Machine Translation, 24(2):159?175.Scott Deerwester, Susan T. Dumais, George W. Furnas,Thomas K. Landauer, and Richard Harshman.
1990.Indexing by Latent Semantic Analysis.
Journal of theAmerican Society for Information Science, 41(6):391?407.Anoop Deoras, Toma?s?
Mikolov, and Kenneth Church.2011a.
A Fast Re-scoring Strategy to Capture Long-Distance Dependencies.
In Proc.
of EMNLP, pages1116?1127, Stroudsburg, PA, USA, July.
Associationfor Computational Linguistics.Anoop Deoras, Toma?s?
Mikolov, Stefan Kombrink,M.
Karafiat, and Sanjeev Khudanpur.
2011b.
Varia-tional Approximation of Long-Span Language Modelsfor LVCSR.
In Proc.
of ICASSP, pages 5532?5535.Ahmad Emami and Frederick Jelinek.
2005.
A NeuralSyntactic Language Model.
Machine Learning, 60(1-3):195?227, September.Jianfeng Gao, Xiaodong He, Wen-tau Yih, and Li Deng.2013.
Learning Semantic Representations for thePhrase Translation Model.
Technical Report MSR-TR-2013-88, Microsoft Research, September.Joshua Goodman.
2001.
Classes for Fast Maximum En-tropy Training.
In Proc.
of ICASSP.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical Phrase-Based Translation.
In Proc.of HLT-NAACL, pages 127?133, Edmonton, Canada,May.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open SourceToolkit for Statistical Machine Translation.
In Proc.of ACL Demo and Poster Sessions, pages 177?180,Prague, Czech Republic, Jun.Philipp Koehn.
2004.
Statistical Significance Tests forMachine Translation Evaluation.
In Proc.
of EMNLP,pages 388?395, Barcelona, Spain, Jul.Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.2012a.
Continuous Space Translation Models with1053Neural Networks.
In Proc.
of HLT-NAACL, pages 39?48, Montre?al, Canada.
Association for ComputationalLinguistics.Hai-Son Le, Thomas Lavergne, Alexandre Allauzen,Marianna Apidianaki, Li Gong, Aure?lien Max, ArtemSokolov, Guillaume Wisniewski, and Franc?ois Yvon.2012b.
LIMSI @ WMT12.
In Proc.
of WMT, pages330?337, Montre?al, Canada, June.
Association forComputational Linguistics.Wolfgang Macherey, Franz Josef Och, Ignacio Thayer,and Jakob Uszkoreit.
2008.
Lattice-based MinimumError Rate Training for Statistical Machine Transla-tion.
In Proc.
of EMNLP, pages 725?734, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Toma?s?
Mikolov and Geoffrey Zweig.
2012.
Con-text Dependent Recurrent Neural Network LanguageModel.
In Proc.
of Spoken Language Technologies(SLT), pages 234?239, Dec.Toma?s?
Mikolov, Karafia?t Martin, Luka?s?
Burget, Jan Cer-nocky?, and Sanjeev Khudanpur.
2010.
RecurrentNeural Network based Language Model.
In Proc.
ofINTERSPEECH, pages 1045?1048.Toma?s?
Mikolov, Anoop Deoras, Daniel Povey, Luka?s?Burget, and Jan C?ernocky?.
2011a.
Strategies forTraining Large Scale Neural Network Language Mod-els.
In Proc.
of ASRU, pages 196?201.Toma?s?
Mikolov, Stefan Kombrink, Luka?s?
Burget, JanCernocky?, and Sanjeev Khudanpur.
2011b.
Exten-sions of Recurrent Neural Network Language Model.In Proc.
of ICASSP, pages 5528?5531.Toma?s?
Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013.
Linguistic Regularities in Continuous Space-Word Representations.
In Proc.
of NAACL, pages746?751, Stroudsburg, PA, USA, June.
Associationfor Computational Linguistics.Toma?s?
Mikolov.
2012.
Statistical Language Modelsbased on Neural Networks.
Ph.D. thesis, Brno Uni-versity of Technology.Franz Josef Och.
2003.
Minimum Error Rate Trainingin Statistical Machine Translation.
In Proc.
of ACL,pages 160?167, Sapporo, Japan, July.Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.2013.
On the difficulty of training Recurrent NeuralNetworks.
Proc.
of ICML, abs/1211.5063.David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.Williams.
1986.
Learning Internal Representationsby Error Propagation.
In Symposium on Parallel andDistributed Processing.Holger Schwenk, Anthony Rousseau, and MohammedAttik.
2012.
Large, Pruned or Continuous Space Lan-guage Models on a GPU for Statistical Machine Trans-lation.
In NAACL-HLT Workshop on the Future ofLanguage Modeling for HLT, pages 11?19.
Associa-tion for Computational Linguistics.Martin Sundermeyer, Ilya Oparin, Jean-Luc Gauvain,Ben Freiberg, Ralf Schlu?ter, and Hermann Ney.
2013.Comparison of Feedforward and Recurrent NeuralNetwork Language Models.
In IEEE InternationalConference on Acoustics, Speech, and Signal Process-ing, pages 8430?8434, Vancouver, Canada, May.Geoff Zweig and Konstantin Makarychev.
2013.
SpeedRegularization and Optimality in Word Classing.
InProc.
of ICASSP.1054
