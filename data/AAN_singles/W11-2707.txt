Proceedings of the UCNLG+Eval: Language Generation and Evaluation Workshop, pages 45?53,Edinburgh, Scotland, UK, July 31, 2011. c?2011 Association for Computational LinguisticsExciting and interesting: issues in the generation of binomialsAnn CopestakeComputer Laboratory,University of Cambridge,15 JJ Thomson Avenue,Cambridge, CB3 0FD, UKann.copestake@cl.cam.ac.ukAure?lie HerbelotInstitut fu?r Linguistik,Universita?t Potsdam,Karl-Liebknecht-Stra?e 24-25D-14476 Golm, Germanyherbelot@uni-potsdam.deAbstractWe discuss the preferred ordering of elementsof binomials (e.g., conjunctions such as fishand chips, lager and lime, exciting and in-teresting) and provide a detailed critique ofBenor and Levy?s probabilistic account of En-glish binomials.
In particular, we discuss theextent to which their approach is suitable asa model of language generation.
We describeresources we have developed for the investi-gation of binomials using a combination ofparsed corpora and very large unparsed cor-pora.
We discuss the use of these resources indeveloping models of binomial ordering, con-centrating in particular on the evaluation is-sues which arise.1 IntroductionPhrases such as exciting and interesting and gin andtonic (referred to in the linguistics literature as bi-nomials) are generally described as having a seman-tics which makes the ordering of the conjuncts irrel-evant.
For instance, exciting and interesting mightcorrespond to exciting?(x)?
interesting?
(x) which isidentical in meaning to interesting?(x)?exciting?
(x).However, in many cases, the binomial is realizedwith a preferred ordering, and in some cases thispreference is so strong that the reverse is perceivedas highly marked and may even be difficult to under-stand.
For example, tonic and gin has a corpus fre-quency which is a very small fraction of that of ginand tonic.
Such cases are referred to as irreversiblebinomials, although the term is sometimes usedonly for the fully lexicalised, non-compositional ex-amples, such as odds and ends.Of course, realization techniques that utilize verylarge corpora to decide on word ordering will tend toget the correct ordering for such phrases if they havebeen seen sufficiently frequently in the training data.But the phenomenon is nevertheless of some practi-cal interest because rare and newly-coined phrasescan still demonstrate a strong ordering preference.For instance, the ordering found in the names ofmixed drinks, where the alcoholic component comesfirst, applies not just to the conventional examplessuch as gin and tonic, but also to brandy and coke,lager and lime, sake and grapefruit and (hopefully)unseen combinations such as armagnac and black-currant.1 A second issue is that data from an un-parsed corpus can be misleading in deciding on bi-nomial order.
Furthermore, our own interest is pre-dominantly in developing plausible computationalmodels of human language generation, and from thisperspective, using data from extremely large cor-pora to train a model is unrealistic.
Binomials area particularly interesting construction to look at be-cause they raise two important questions: (1) to whatextent does lexicalisation/establishment of phrasesplay a role in determining order?
and (2) is a detailedlexical semantic classification required to accuratelypredict order?As far as we are aware, the problem of developinga model of binomial ordering for language genera-tion has not previously been addressed.
However,Benor and Levy (2006) have published an importantand detailed paper on binomial ordering which wedraw on extensively in this work.
Their researchhas the objective of determining how the variousconstraints which have been proposed in the lin-guistic literature might interact to determine bino-1One of our reviewers very helpfully consulted a bartenderabout this generalization, and reports the hypothesis that the al-cohol always comes first because it is poured first.
However,there is the counter-example gin and bitters (another name forpink gin), where the bitters are added first (unless the drink ismade in a cocktail shaker, in which case ordering is irrelevant).45mial ordering as observed in a corpus.
We presenta critical evaluation of that work here, in terms ofthe somewhat different requirements for a model forlanguage generation.The issues that we concentrate on in this paperare necessary preliminaries to constructing corpus-based models of binomial reversibility and ordering.These are:1.
Building a suitable corpus of binomials.2.
Developing a corpus-based technique for eval-uation.3.
Constructing an initial model to test the evalu-ation methodology.In ?2, we provide a brief overview of some ofthe factors affecting binomial ordering and discussBenor and Levy?s work in particular.
?3 discussesevaluation issues and motivates some of the deci-sions we made in deciding on the resources we havedeveloped, described in ?4.
?5 illustrates the evalua-tion of a simple model of binomial ordering.2 Benor and Levy?s accountWe do not have space here for a proper discussion ofthe extensive literature on binomials, or indeed for afull discussion of Benor and Levy?s paper (hence-forth B+L) but instead summarise the aspects whichare most important for the current work.For convenience, we follow B+L in referring tothe elements of an ordered binomial as A and B.They only consider binomials of the form ?A andB?
where A and B are of the same syntactic cate-gory.
Personal proper names were excluded fromtheir analysis.
Because they required tagged data,they used a combination of Switchboard, Brown andthe Wall Street Journal portion of the Penn Treebankto extract binomials, selecting 411 binomial typesand all of the corresponding tokens (692 instances).B+L investigate a considerable number of con-straints on binomial ordering which have been dis-cussed in the linguistics literature.
They group thefeatures they use into 4 classes: semantic, wordfrequency, metrical and non-metrical phonological.We will not discuss the last class here, since theyfound little evidence that it was relevant once theother features were taken into account.
The metri-cal constraints were lapse (2 consecutive weak syl-lables are generally avoided), length (A should nothave more syllables than B) and stress (B should nothave ultimate (primary) stress: this feature was actu-ally found to overlap almost entirely with lapse andlength).
The frequency constraint is that B shouldnot be more frequent than A, based on corpus spe-cific counts of frequency (unsurprisingly, frequencycorrelates with the length feature).The semantic constraints are less straightforwardsince the linguistics literature has discussed manyconstraints and a variety of possible generalisations.B+L use:Markedness Divided into Relative formal,which includes cases like flowers and roses(more general term first) among others andPerception-based, which is determined byextra-linguistic knowledge, including caseslike see and hear (seeing is more salient).B should not be less marked than A. Un-fortunately markedness is too complex tosummarise adequately here.
It is clear that itoverlaps with other constraints in some cases,including frequency, since unmarked termstend to be more frequent.Iconicity Sequence ordering of events, numberedentities and so on (e.g., shot and killed, eighthand ninth).
If there is such a sequence, the bi-nomial ordering should mirror it.Power Power includes gender relationships (dis-cussed below), hierarchical relationships (e.g.,clergymen and parishioners), the ?condimentrule?
(e.g., fish and chips) and so on.
B shouldnot be more powerful than A.Set Open Construction This is used for certainconventional cases where a given A may occurwith multiple Bs: e.g., nice and.Pragmatic A miscellaneous context-dependentconstraint, used, for instance, where thebinomial ordering mirrors the ordering of otherwords in the sentence.B+L looked at the binomials in sentential contextto assign the semantic constraints.
The iconicity46constraint, in particular, is context-dependent.
Forexample, although the sequence ninth and eighthlooks as though it violates iconicity, we found thata Google search reveals a substantial number of in-stances, many of which refer to the ninth and eighthcenturies BC.
In this case, iconicity is actually ob-served, if we assume that temporal ordering deter-mines the constraint, rather than the ordering of theordinals.The aspect of binomials which has received mostattention in the literature is the effect of gender:words which refer to (human) males tend to pre-cede those referring to females.
For instance (withGoogle 3-gram percentages for binomials with themasculine term first): men and women (85%), boysand girls (80%), male and female (91%) (exceptionsare father and mother (51%) and mothers and fa-thers (33%)).
There is also an observed bias towardspredominantly male names preceding female names.B+L, following previous authors, take gender as anexample of the Power feature.
For reasons of spacewe can only touch on this issue very superficially,but it illustrates a distinction between semantic fea-tures which we think important.
Iconicity generallyrefers to a sequence of real world events or enti-ties occuring in a particular order, hence its context-dependence.
For verbs, at least, there is a truth con-ditional effect of the ordering of the binomial: shotand killed does not mean the same thing as killedand shot.
Power, on the other hand, is supposed tobe about a conventional relationship between the en-tities.
Even if we are currently more interested inchips rather than fish or biscuits rather than tea, wewill still tend to refer to fish and chips and tea andbiscuits.
The actual ordering may depend on cul-ture,2 but the assumption is that, within a particularcommunity, the power relationship which the bino-mial ordering depends on is fixed.B+L analyse the effects of all the features in de-tail, and look at a range of models for combiningfeatures, with logistic regression being the most suc-cessful.
This predicts the ordering of 79.2% of thebinomial tokens and 76.7% of the types.
When se-mantic constraints apply, they tend to outrank themetrical constraints.
B+L found that iconicity, in2Our favourite example is an English-French parallel textwhere the order of Queen Elizabeth and President Mitterand isreversed in the French.particular, is a very strong predictor of binomial or-der.B+L?s stated assumption is that a speaker/writerknows they want to generate a binomial with thewords A and B and decides on the order based onthe words and the context.
It is this order that theyare trying to predict.
Of course, it is clear that somebinomials are non-compositional multiword expres-sions (e.g., odds and ends) which are listed in con-ventional dictionaries.
These can be thought of as?words with spaces?
and, we would argue that thespeaker does not have a choice of ordering in suchcases.
B+L argue that using a model which listedthe fixed phrases would be valid in the prediction ofbinomial tokens, but not binomial types.
We do notthink this holds in general and return to the issue in?3.B+L?s work is important in being the first accountwhich examines the effect of the postulated con-straints in combination.
However, from our perspec-tive (which is of course quite different from theirs),there are a number of potential problems.
The first isdata sparsity: the vast majority of binomial types intheir data occur only once.
It is impossible to knowwhether both orderings are frequent for most types.Furthermore, the number of binomial types is rathersmall for full investigation of semantic features: e.g.,Power is marked on only 26 types.
The second is-sue is that the combined models which B+L exam-ine are, in effect, partially trained on the test data, inthat the relative contribution of the various factors isoptimized on the test data itself.
Thirdly, the seman-tic factors which B+L consider have no independentverification: they were assigned by the authors forthe binomials under consideration, a methodologywhich makes it impossible to avoid the possibility ofbias.
There was some control over this, in that it wasdone independently by the two authors with subse-quent discussion to resolve disagreements.
How-ever, we think that it would be hard to avoid thepossibility of bias in the ?Set open?
and ?Pragmatic?constraints in particular.
Some of the choices seemunintuitive: e.g., we are unsure why there is a Powerannotation on broccoli and cauliflower, and why goand vote would be marked for Iconicity while wentand voted is not.
It seems to us that the defini-tion of some of these semantic factors in the liter-ature (markedness and power in particular) is suf-47ficiently unclear for reproducible annotation of thetype now expected in computational linguistics to beextremely difficult.Both for practical and theoretical reasons, we areinterested in investigating alternative models whichrely on a corpus instead of explicit semantic fea-tures.
Native speakers are aware of some lexicalisedand established binomials (see (Sag et al 2002) for adiscussion of lexicalisation vs establishment in mul-tiword expressions), and will tend to generate themin the familiar order.
Instead of explicit features be-ing learned for the unseen cases, we want to investi-gate the possible role of analogy to the known bino-mials.
For instance, if tea and biscuits is known,coffee and cake might be generated in that order-ing by semantic analogy.
The work presented inthis paper is essentially preparatory to such experi-ments, although we will discuss an extremely simplecorpus-based model in ?5.3 Evaluating models of binomial orderingIn this section, we discuss what models of binomialordering should predict and how we might evaluatethose predictions.The first question is to decide precisely what weare attempting to model.
B+L take the position thatthe speaker/writer has in mind the two words of thebinomial and chooses to generate them in one orderor other in a particular context, but this seems prob-lematic for the irreversible binomials and, in anycase, is not directly testable.
Alternatively we canask: Given a corpus of sentences where the binomi-als have been replaced with unordered pairs of AB,can we generate the ordering actually found?
Bothof these are essentially token-based evaluations, al-though we could additionally count binomial types,as B+L do.One problem with these formulations is that, to dothem justice, our models would really have to incor-porate features from the surrounding context.
Fac-tors such as postmodification of the binomial affectthe ordering.
This type of evaluation would clearlybe the right one if we had a model of binomials in-corporated into a general realisation model, but it isnot clear it is suitable for looking at binomials in iso-lation.Perhaps more importantly, to model the irre-versible or semi-irreversible binomials, we shouldtake into account the order and degree of reversibil-ity of particular binomial types.
It seems problem-atic to formulate the generation of a lexicalised bino-mial, such as odds and ends, as a process of decidingon the order of the components, since the speakermust have the term in mind as a unit.
In termsof the corpus formulation, given the pair AB, thefirst question in deciding how to realise the phraseis whether the order is actually fixed.
The caseof established but compositional binomials, such asfish and chips, is slightly less clear, but there stillseem good grounds for regarding it as a unit (Cruse,1986).
Furthermore, in evaluating a token-based re-alisation model, we should not penalise the wrongordering of a reversible binomial as severely as ifthe binomial were irreversible.
From these perspec-tives, developing a model of ordering of binomialtypes should be a preliminary to developing a modelof binomial tokens.
Context would be important inproperly modelling the iconicity effect, but is lessof an issue for the other ordering constraints.
Andeven though iconicity is context-dependent, there isa very strongly preferred ordering for many of thebinomial types where iconicity is relevant.Thus we argue that it is appropriate to look at thequestion: Given two words A, B which can be con-joined, what order do we find most frequently in acorpus?
Or, in order to look at degree of reversibil-ity: What proportion of the two orderings do we findin a corpus?
This means that we require relativelylarge corpora to obtain good estimates in order toevaluate a model.Of course, if we are interested in analogical mod-els of binomial ordering, as mentioned at the end of?2, we need a reasonably large corpus of binomialsto develop the model.
Ideally this should be a dif-ferent corpus from the one used for evaluation.
Wenote that some experiments on premodifier order-ing have found a considerable drop in performancewhen testing on a different domain (Shaw and Hatzi-vassiloglou, 1999).
Using a single corpus split intotraining and test data would, of course, be problem-atic when working with binomial types.
We havethus developed a relatively novel methodology of us-ing an automatically parsed corpus in combinationwith frequencies from Web data.
This is discussedin the next section.484 Binomial corpora and corpusinvestigationIn this section, we describe the resources we havedeveloped for investigating binomials and address-ing some of the evaluation questions introduced inthe previous section.
We then present an initial anal-ysis of some of the corpus data.4.1 Benor and Levy dataThe appendix of B+L?s paper3 contains a list of thebinomials they looked at, plus some of their markup.Although the size of the B+L dataset is too smallfor many purposes, we found it useful to considerit as a clean source of binomial types for our initialcorpus investigation and evaluation.
We produced aversion of this list excluding the 10 capitalised ex-amples: some of these seem to arise from sentenceinitial capitals while others are proper names whichwe decided to exclude from this study.
We produceda manually lemmatised version of the list, which re-sults in a slightly reduced number of binomial types:e.g., bought and sold and buy and sell correspond toa single type.
The issue of lemmatisation is slightlyproblematic in that a few examples are lexicalisedwith particular inflections, such as been and gone.However, our use of parsed data meant that we hadto use lemmatization decisions which were compat-ible with the parser.4.2 Wikipedia and the Google n-gram corpusIn line with B+L, we assume that binomials aremade of two conjuncts with the same part of speech.It is not possible to use an unparsed corpus to ex-tract such constructions automatically: first, the rawtext surrounding a conjunction may not correspondto the actual elements of the coordination (e.g., thetrigram dictionary and phrase in She bought a dic-tionary and phrase book); second, the part of speechinformation is not available.
Using a parsed corpus,however, has disadvantages: in particular, it limitsthe amount of data available and, consequently, thenumber of times that a given type can be observed.In this section, we discuss the use of Wikipedia,which is small enough for parsing to be tractable but3http://idiom.ucsd.edu/?rlevy/papers/binomials-sem-alpha-formattedwhich turns out to have a fairly representative distri-bution of binomials.
The latter point is demonstratedby comparison with a large dataset: the Google n-gram corpus (Brants and Franz, 2006).
Althoughthe Google data is not suitable for the actual taskof extracting binomials, because it is not parsed, wehypothesize it is usable to predict the preferred or-der of a given binomial and to estimate the extent towhich it is reversible.In order to build a corpus of binomials, we processthe parsed Wikipedia dump produced by Kummer-feld et al(2010).
The parse consists of grammaticalrelations of the following form:(gr word1 x word2 y ... wordn z)where gr is the name of the grammatical relation,word1...n are the arguments of the relation, andx, y...z are the positions of the arguments in the sen-tence.
The lemmatised forms of the arguments, aswell as their part of speech, are available separately.We used the first one million and coordinations inthe corpus in these experiments.
The conjuncts arerequired to have the same part of speech and to di-rectly precede and follow the coordination.
The lat-ter requirement ensures that we retrieve true binomi-als (phrases, as opposed to distant coordinates).
Foreach binomial in this data, we record a frequencyand whether it is found in the reverse order in thesame dataset.
The frequency of the reverse orderingis similarly collected.
Since we intend to comparethe Wikipedia data to a larger, unparsed corpus, wemerge the counts of all possible parts of speech fora given type in a given ordering, so the counts forEuropean and American as nouns and as adjectives,for instance, are added together.
We also recordthe preferred ordering (the one with the highest fre-quency) of the binomial and the ratio of the frequen-cies as an indication of (ir)reversibility.
In line withour treatment of the B+L data, we disregarded thebinomials that coordinate proper names, but notedthat a large proportion of proper names found inthe Wikipedia data cannot be found in the Googledata.4 The Google corpus also splits (most) hyphen-4This suggests that the Google n-gram corpus does not con-tain much (if any) of the Wikipedia data: the particular dumpof Wikipedia from which the parsed data is extracted being inany case several years later than the date that the Google n-gramcorpus was produced.49ated words.
Since hyphenation is notoriously irreg-ular in English, we disregarded all binomials con-taining hyphenated words.
The resulting data con-tains 279136 unique binomial types.
Around 7600of those types have a frequency of 10 or more in ourWikipedia subset.
As expected, this leaves a largeamount of data with low frequency.We then attempt to verify how close the sparseWikipedia data is to the Google 3-gram corpus.
Foreach binomial obtained from Wikipedia, we retrievethe frequency of both its orderings in the Googledata and, as before, calculate the ratio of the frequen-cies in the larger corpus.
The procedure involvesconverting the lemmatised forms in the Wikipediaparse back into surface forms.
Rather than usinga morphological generator, which would introducenoise in our data, we search for the surface forms asthey appeared in the original Wikipedia data, as wellas for the coordinated base forms (this ensures highrecall in cases where the original frequency is low).So for example, given the one instance of the bino-mial ?sadden and anger?
in Wikipedia, appearing asSaddened and angered in the corpus, we search forSaddened and angered, sadden and anger and angerand sadden.Around 30% of the Wikipedia binomials are notin the Google data.
We manually spot checked anumber of those and confirmed that they were un-available from the Google data, regardless of inflec-tion.
Examples of binomials not found in the n-gramcorpus include dagger and saber, sagacious andfirm and (rather surprisingly) gay and flamboyant.19% of the Wikipedia binomials have a differentpreferred order in the Google corpus.
As expected,most of those have a low frequency in Wikipedia.For the binomials with an occurrence count over 40,the agreement on ordering is high (around 96%).Furthermore, many of those disagreements are not?real?
in that they concern binomials found with ahigh dispreferred to preferred order ratio.
Disre-garding cases where this ratio is over 0.3 lowers theinitial disagreement figure to 7%.
We will argue in?4.4 that true irreversibility can be shown to roughlycorrespond to a ratio of 0.1.
At this cutoff, the per-centage of disagreements between the two corporais only 2%.
Thus we found no evidence that theencyclopaedic nature of Wikipedia has a significantskewing effect on the frequencies.
We thus believethat Wikipedia is a suitable dataset for training anautomatic binomial ordering system.4.3 LexicalisationOur basic methodology for investigation of lexi-calisation was to check online dictionaries for thephrases.
However, deciding whether a binomialshould be regarded as a fixed phrase is not entirelystraightforward.
For instance, consider warm andfuzzy.
At first sight, it might appear compositional,but the particular use of fuzzy, referring to feelings,is not the usual one.
While warm and fuzzy is notlisted in most dictionaries we have examined, it hasan entry in the Urban Dictionary5 and is used in ex-amples illustrating that particular usage of fuzzy inthe online Merriam-Webster.6 Another case fromthe B+L data is nice and toasty, which again is usedin a Merriam-Webster example.7We therefore used a manual search procedureto check for lexicalisation of the B+L binomials.We used a broad notion of lexicalisation, treat-ing a phrase as lexicalised if it occurred as an en-try in one or more online English dictionaries us-ing Google search.
We included a few phrases assemi-lexicalised when they were given in examplesin dictionaries produced by professional lexicogra-phers, but this was, to some extent, a subjectivedecision.
Since such a search is time-consuming,we only checked examples which one of us (a na-tive British English speaker) intuitively consideredmight be lexicalised.
We first validated that thiswould not cause too great a loss of recall by check-ing a small subset of the B+L data exhaustively: thisdid not reveal any additional examples.Using these criteria, we found 39 lexicalised bi-nomial types in the B+L data, of which 7 weresemi-lexicalised.8 The phrases backwards and for-wards, backward and forward, day and night, saltand pepper and in and out are lexicalised (or semi-lexicalised) in both orders.5http://www.urbandictionary.com/6http://www.merriam-webster.com/7The convention of indicating semi-fixed phrases in exam-ples is quite common in lexicography, especially in dictionariesintended for language learners.8There are 40 tokens, because cut and dry and cut and driedare both lexicalised.
An additional example, foot-loose andfancy-free, might be included, but we did not find it in any dic-tionary with that hyphenation.504.4 Reversibility and corpus evidenceThere are a number of possible reasons why a partic-ular binomial type AB might (almost) always appearin one ordering (A and B or B and A):1.
The phrase A and B (B and A) might be fullylexicalised (word with spaces).2.
The binomial might have a compositionalmeaning, but have a conventional ordering.
Aparticular binomial AB might be establishedwith that ordering (e.g., gin and tonic is es-tablished for most British and American speak-ers) or might belong to a conventional pattern(e.g., armagnac and blackcurrant, sole and ar-tichokes).3.
The binomial could refer to a sequence of realworld events or entities which almost invari-ably occur in a particular order.
For example,shot and killed has a frequency of 241675 inthe Google 3-gram corpus, as opposed to 158for killed and shot.
This ratio is larger that thatof many of the lexicalised binomials.Relatively few of the binomials from the B+L dataare completely irreversible according to the Google3-gram data.
There are instances of the reverse ofeven obviously fixed phrases, such as odds and ends.Of course, there is no available context in the 3-gramdata, but we investigated some of these cases by on-line search for the reversed phrases.
This indicatesa variety of sources of noise, including wordplay(e.g., Beckett?s play Ends and Odds), different wordsenses (e.g., toasty and nice occurs when toasty isused to describe wine) and false positives from hy-phenated words etc.We can obtain a crude estimate of extent to whichbinomials which should be irreversible actually turnup in the ?wrong?
order by looking at the clearly lex-icalised phrases discussed in ?4.3.
Excluding thecases where both orders are lexicalised, the meanproportion of inverted cases is about 3%.
There area few outliers, such as there and back and now andthen which have more than 10% inverted: however,these all involve very frequent closed class wordswhich are more likely to show up in spurious con-texts.
We therefore tentatively conclude that up to10% of the tokens of a open-class irreversible bino-mial could be inverted in the 3-gram corpus, but thatwe can take higher ratios as evidence for a degree ofgenuine reversibility.5 An initial modelWe developed an initial n-gram-based model for or-dering using the Wikipedia-derived counts.
The ap-proach is very similar to that presented in (Malouf,2000) for adjective ordering.
We use the observedorder of binomials where possible and back off tocounts of a lexeme?s position as first or second con-junct over all binomials (i.e., we use what Maloufrefers to as positional probabilities).To be more precise, assume that the task is to pre-dict the order a ?
b or b ?
a for a given lexeme paira,b.
We use the notation C(a and b) and C(b and a)to refer to the counts in a given corpus of the twoorderings of the binomial (i.e., we count all inflec-tions of a and b).
C(a and) refers to the count of allbinomials with the lexeme a as the first conjunct,C(and a) all binomials with a as the second con-junct, and so on.
We predict a ?
bif C(a and b) > C(b and a)or C(a and b) = C(a and b)andC(a and)C(and b) > C(b and)C(and a)and conversely for b ?
a.
Most of the cases wherethe condition C(a and b) = C(a and b) is true occurwhen C(a and b) = C(a and b) = 0 but we alsouse the positional probabilities to break ties in thecounts.
We could, of course, define this in terms ofprobability estimates and investigate various formsof smoothing and interpolation, but for our initialpurposes it is adequate to see how this very simplemodel behaves.We obtained counts for the model from theWikipedia-derived data and evaluated it on the bino-mial types derived from B+L (as described in ?4.1).There were only 9 cases where there was no pre-diction, so for the sake of simplicity, we default toalphabetic ordering in those cases.
In Table 1, weshow the results evaluating against the B+L major-ity decision and against the Google 3-gram majority.Because not all the B+L binomials are found in theGoogle data, the numbers of binomial types evalu-ated against the Google data is slightly lower.
In51addition to the overall figures, we also show the rela-tive accuracy of the bigram prediction vs the backoffand the different accuracies on the lexicalised andnon-lexicalised data.
In Table 2, we group the re-sults according to the ratio of the less frequent orderin the Google data and by frequency.Unsurprisingly, performance on more frequent bi-nomials and lexicalised binomials is better and thebigram performance, where available, is better thanthe backoff to positional probabilities.
The scoreswhen evaluated on the Google corpus are generallyhigher than those on the B+L counts, as expectedgiven the noise created by the data sparsity in B+Lcombined with the effect of frequency.One outcome from our experiments is that it doesnot seem essential to treat the lexicalised examplesseparately from the high frequency, low reversibil-ity cases.
Since determining lexicalisation is time-consuming and error-prone, this is a useful result.The model described does not predict whether ornot a given binomial is irreversible, but our analy-sis of the data strongly suggests that this would beimportant in developing more realistic models.
Anobvious extension would be to generate probabilityestimates of orderings and to compare these with theobserved Google 3-gram data.Although n-gram models are completely stan-dard in computational linguistics, their applicabil-ity to modelling human performance on a task isnot straightforward.
Minimally, if we were to pro-pose that humans were using such a model as partof their decision on binomial ordering, it would benecessary to demonstrate that the counts we are re-lying on correspond to data which it is plausible toassume that a human could have been exposed to.This is not a trivial consideration.
We would, ofcourse, expect to obtain higher scores on this task byusing counts derived from the Google n-gram cor-pus rather than from Wikipedia, but this would becompletely unrealistic from a psycholinguistic per-spective.
We should emphasize, therefore, that themodel presented here is simply intended as an initialexercise in developing distributional models of bi-nomial ordering, which allows us to check whetherthe resources we have developed might be an ade-quate basis for more serious modelling and whetherthe evaluation schemes are reasonable.6 ConclusionWe have demonstrated that we can make use of acombination of corpora to build resources for devel-opment and evaluation of models of binomial order-ing.9 One novel aspect is our use of an automaticallyparsed corpus, another is the use of combined cor-pora.
If binomial ordering is primarily determinedby universal linguistic factors, we would not expectthe relative frequency to differ very substantially be-tween large corpora.
The cases where we did ob-serve differences in preferred ordering between theWikipedia and Google data are predominantly oneswhere the Wikipedia frequency is low or the bino-mial is highly reversible.
We have investigated sev-eral properties of binomials using this data and pro-duced a simple initial model.
We tested this on therelatively small number of binomials used by Benorand Levy (2006), but in future work we will evalu-ate on a much larger subset of our corpus.
Our in-tention is to develop further models which use anal-ogy (morphological and distributional semantic sim-ilarity) to known binomials to predict degree of re-versibility and ordering.
This will allow us to inves-tigate whether human performance can be modelledwithout the use of explicit semantic features.We briefly touched on Malouf?s (2000) work onprenominal adjective ordering in our discussion ofthe initial model.
There are some similarities be-tween these tasks, and in fact adjectives in binomialstend to occur in the same order when they appear asprenominal adjectives (e.g., cold and wet and coldwet are preferred over the inverse orders).
However,the binomial problem is considerably more complex.Binomials are much more variable because they in-volve all the main syntactic categories.
Furthermore,adjective ordering is considerably easier to investi-gate because an unparsed corpus can be used, the se-mantic features which have been postulated are morestraightforward than for binomials and lexicalisationof adjective sequences is not an issue.
We hypoth-esize that it should be possible to develop similaranalogical models for adjective ordering and bino-mials which could be relevant for other construc-tions where ordering is only partially determinedby syntax.
In the long term, we would like to in-9Available from http://www.cl.cam.ac.uk/research/nl/nl-download/binomials/52n B+L n Google accuracy B+L (%) accuracy Google (%)Overall 380 305 69 79Bigram 187 185 79 89Pos Prob 184 117 61 65Unknown 9 3 33 0Lexicalised 34 34 87 94Non-lexicalised 346 271 67 77Table 1: Evaluation of initial model, showing effects of lexicalisation.
(n B+L and n Google indicates the number ofbinomial types evaluated)n accuracy B+L (%) accuracy Google (%)Google count 0 75 59 -1?1000 71 56 681001?10000 81 70 67> 10000 153 80 91Google ratio 0 11 64 640?0.1 41 94 930.1?0.25 33 75 85> 0.25 220 68 76Table 2: Evaluation of initial model, showing effects of frequency and reversibility.vestigate using such models in conjunction with agrammar-based realizer (cf (Velldal, 2007), (Cahilland Riester, 2009)).
However, for an initial inves-tigation of the role of semantics and lexicalisation,looking at the binomial construction in isolation ismore tractable.AcknowledgmentsThis work was partially supported by a fellowshipto Aure?lie Herbelot from the Alexander von Hum-boldt Foundation.
We are grateful to the reviewersfor their comments.ReferencesSarah Benor and Roger Levy.
2006.
The Chicken or theEgg?
A Probabilistic Analysis of English Binomials.Language, 82 233?78.Thorsten Brants and Alex Franz.
2006.
The Google Web1T 5-gram Corpus Version 1.1.
LDC2006T13.Aoife Cahill and Arndt Riester.
2009.
IncorporatingInformation Status into Generation Ranking.
In Pro-ceedings of the 47th Annual Meeting of the ACL, pp.817-825, Suntec, Singapore.
Association for Compu-tational Linguistics.D.
Alan Cruse.
1986.
Lexical Semantics.
CambridgeUniversity Press.Jonathan K. Kummerfeld, Jessika Rosener, Tim Daw-born, James Haggerty, James R. Curran, StephenClark.
2010.
Faster parsing by supertagger adapta-tion Proceedings of the 48th Annual Meeting of theAssociation for Computational Linguistics, Uppsala,Sweden, pages 345?355Rob Malouf.
2000.
The order of prenominal adjectivesin natural language generation.
In Proceedings of the38th Annual Meeting of the Association for Computa-tional Linguistics (ACL 2000), Hong Kong.Ivan Sag, Tim Baldwin, Francis Bond, Ann Copestake,and Dan Flickinger.
2002.
Multiword expressions:A pain in the neck for NLP.
In Third InternationalConference on Intelligent Text Processing and Com-putational Linguistics (CICLING 2002), pages 1?15,Mexico City, Mexico.James Shaw and Vasileios Hatzivassiloglou.
1999.
Or-dering among premodifiers.
In Proceedings of the 37thAnnual Meeting of the Association for ComputationalLinguistics, pages 135?143, College Park, Maryland.Eric Velldal.
2007.
Empirical Realization Ranking.Ph.D.
thesis, University of Oslo, Department of Infor-matics.53
