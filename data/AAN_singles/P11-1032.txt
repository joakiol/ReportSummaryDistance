Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 309?319,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsFinding Deceptive Opinion Spam by Any Stretch of the ImaginationMyle Ott Yejin Choi Claire CardieDepartment of Computer ScienceCornell UniversityIthaca, NY 14853{myleott,ychoi,cardie}@cs.cornell.eduJeffrey T. HancockDepartment of CommunicationCornell UniversityIthaca, NY 14853jth34@cornell.eduAbstractConsumers increasingly rate, review and re-search products online (Jansen, 2010; Litvinet al, 2008).
Consequently, websites con-taining consumer reviews are becoming tar-gets of opinion spam.
While recent workhas focused primarily on manually identifi-able instances of opinion spam, in this workwe study deceptive opinion spam?fictitiousopinions that have been deliberately written tosound authentic.
Integrating work from psy-chology and computational linguistics, we de-velop and compare three approaches to detect-ing deceptive opinion spam, and ultimatelydevelop a classifier that is nearly 90% accurateon our gold-standard opinion spam dataset.Based on feature analysis of our learned mod-els, we additionally make several theoreticalcontributions, including revealing a relation-ship between deceptive opinions and imagina-tive writing.1 IntroductionWith the ever-increasing popularity of review web-sites that feature user-generated opinions (e.g.,TripAdvisor1 and Yelp2), there comes an increasingpotential for monetary gain through opinion spam?inappropriate or fraudulent reviews.
Opinion spamcan range from annoying self-promotion of an un-related website or blog to deliberate review fraud,as in the recent case3 of a Belkin employee who1http://tripadvisor.com2http://yelp.com3http://news.cnet.com/8301-1001_3-10145399-92.htmlhired people to write positive reviews for an other-wise poorly reviewed product.4While other kinds of spam have received consid-erable computational attention, regrettably there hasbeen little work to date (see Section 2) on opinionspam detection.
Furthermore, most previous work inthe area has focused on the detection of DISRUPTIVEOPINION SPAM?uncontroversial instances of spamthat are easily identified by a human reader, e.g., ad-vertisements, questions, and other irrelevant or non-opinion text (Jindal and Liu, 2008).
And while thepresence of disruptive opinion spam is certainly anuisance, the risk it poses to the user is minimal,since the user can always choose to ignore it.We focus here on a potentially more insidi-ous type of opinion spam: DECEPTIVE OPINIONSPAM?fictitious opinions that have been deliber-ately written to sound authentic, in order to deceivethe reader.
For example, one of the following twohotel reviews is truthful and the other is deceptiveopinion spam:1.
I have stayed at many hotels traveling for both businessand pleasure and I can honestly stay that The James istops.
The service at the hotel is first class.
The roomsare modern and very comfortable.
The location is per-fect within walking distance to all of the great sights andrestaurants.
Highly recommend to both business trav-ellers and couples.2.
My husband and I stayed at the James Chicago Hotelfor our anniversary.
This place is fantastic!
We knewas soon as we arrived we made the right choice!
Therooms are BEAUTIFUL and the staff very attentive andwonderful!!
The area of the hotel is great, since I loveto shop I couldn?t ask for more!!
We will definatly be4It is also possible for opinion spam to be negative, poten-tially in order to sully the reputation of a competitor.309back to Chicago and we will for sure be back to the JamesChicago.Typically, these deceptive opinions are neithereasily ignored nor even identifiable by a humanreader;5 consequently, there are few good sourcesof labeled data for this research.
Indeed, in the ab-sence of gold-standard data, related studies (see Sec-tion 2) have been forced to utilize ad hoc proceduresfor evaluation.
In contrast, one contribution of thework presented here is the creation of the first large-scale, publicly available6 dataset for deceptive opin-ion spam research, containing 400 truthful and 400gold-standard deceptive reviews.To obtain a deeper understanding of the nature ofdeceptive opinion spam, we explore the relative util-ity of three potentially complementary framings ofour problem.
Specifically, we view the task as: (a)a standard text categorization task, in which we usen-gram?based classifiers to label opinions as eitherdeceptive or truthful (Joachims, 1998; Sebastiani,2002); (b) an instance of psycholinguistic decep-tion detection, in which we expect deceptive state-ments to exemplify the psychological effects of ly-ing, such as increased negative emotion and psycho-logical distancing (Hancock et al, 2008; Newman etal., 2003); and, (c) a problem of genre identification,in which we view deceptive and truthful writing assub-genres of imaginative and informative writing,respectively (Biber et al, 1999; Rayson et al, 2001).We compare the performance of each approachon our novel dataset.
Particularly, we find that ma-chine learning classifiers trained on features tradi-tionally employed in (a) psychological studies ofdeception and (b) genre identification are both out-performed at statistically significant levels by n-gram?based text categorization techniques.
Notably,a combined classifier with both n-gram and psy-chological deception features achieves nearly 90%cross-validated accuracy on this task.
In contrast,we find deceptive opinion spam detection to be wellbeyond the capabilities of most human judges, whoperform roughly at-chance?a finding that is consis-tent with decades of traditional deception detectionresearch (Bond and DePaulo, 2006).5The second example review is deceptive opinion spam.6Available by request at: http://www.cs.cornell.edu/?myleott/op_spamAdditionally, we make several theoretical con-tributions based on an examination of the featureweights learned by our machine learning classifiers.Specifically, we shed light on an ongoing debate inthe deception literature regarding the importance ofconsidering the context and motivation of a decep-tion, rather than simply identifying a universal setof deception cues.
We also present findings that areconsistent with recent work highlighting the difficul-ties that liars have encoding spatial information (Vrijet al, 2009).
Lastly, our study of deceptive opinionspam detection as a genre identification problem re-veals relationships between deceptive opinions andimaginative writing, and between truthful opinionsand informative writing.The rest of this paper is organized as follows: inSection 2, we summarize related work; in Section 3,we explain our methodology for gathering data andevaluate human performance; in Section 4, we de-scribe the features and classifiers employed by ourthree automated detection approaches; in Section 5,we present and discuss experimental results; finally,conclusions and directions for future work are givenin Section 6.2 Related WorkSpam has historically been studied in the contexts ofe-mail (Drucker et al, 2002), and the Web (Gyo?ngyiet al, 2004; Ntoulas et al, 2006).
Recently, re-searchers have began to look at opinion spam aswell (Jindal and Liu, 2008; Wu et al, 2010; Yooand Gretzel, 2009).Jindal and Liu (2008) find that opinion spam isboth widespread and different in nature from eithere-mail or Web spam.
Using product review data,and in the absence of gold-standard deceptive opin-ions, they train models using features based on thereview text, reviewer, and product, to distinguishbetween duplicate opinions7 (considered deceptivespam) and non-duplicate opinions (considered truth-ful).
Wu et al (2010) propose an alternative strategyfor detecting deceptive opinion spam in the absence7Duplicate (or near-duplicate) opinions are opinions that ap-pear more than once in the corpus with the same (or similar)text.
While these opinions are likely to be deceptive, they areunlikely to be representative of deceptive opinion spam in gen-eral.
Moreover, they are potentially detectable via off-the-shelfplagiarism detection software.310of gold-standard data, based on the distortion of pop-ularity rankings.
Both of these heuristic evaluationapproaches are unnecessary in our work, since wecompare gold-standard deceptive and truthful opin-ions.Yoo and Gretzel (2009) gather 40 truthful and 42deceptive hotel reviews and, using a standard statis-tical test, manually compare the psychologically rel-evant linguistic differences between them.
In con-trast, we create a much larger dataset of 800 opin-ions that we use to develop and evaluate automateddeception classifiers.Research has also been conducted on the re-lated task of psycholinguistic deception detection.Newman et al (2003), and later Mihalcea andStrapparava (2009), ask participants to give boththeir true and untrue views on personal issues(e.g., their stance on the death penalty).
Zhou etal.
(2004; 2008) consider computer-mediated decep-tion in role-playing games designed to be playedover instant messaging and e-mail.
However, whilethese studies compare n-gram?based deception clas-sifiers to a random guess baseline of 50%, we addi-tionally evaluate and compare two other computa-tional approaches (described in Section 4), as wellas the performance of human judges (described inSection 3.3).Lastly, automatic approaches to determining re-view quality have been studied?directly (Weimeret al, 2007), and in the contexts of helpful-ness (Danescu-Niculescu-Mizil et al, 2009; Kim etal., 2006; O?Mahony and Smyth, 2009) and credibil-ity (Weerkamp and De Rijke, 2008).
Unfortunately,most measures of quality employed in those worksare based exclusively on human judgments, whichwe find in Section 3 to be poorly calibrated to de-tecting deceptive opinion spam.3 Dataset Construction and HumanPerformanceWhile truthful opinions are ubiquitous online, de-ceptive opinions are difficult to obtain without re-sorting to heuristic methods (Jindal and Liu, 2008;Wu et al, 2010).
In this section, we report our ef-forts to gather (and validate with human judgments)the first publicly available opinion spam dataset withgold-standard deceptive opinions.Following the work of Yoo and Gretzel (2009), wecompare truthful and deceptive positive reviews forhotels found on TripAdvisor.
Specifically, we mineall 5-star truthful reviews from the 20 most popularhotels on TripAdvisor8 in the Chicago area.9 De-ceptive opinions are gathered for those same 20 ho-tels using Amazon Mechanical Turk10 (AMT).
Be-low, we provide details of the collection methodolo-gies for deceptive (Section 3.1) and truthful opinions(Section 3.2).
Ultimately, we collect 20 truthful and20 deceptive opinions for each of the 20 chosen ho-tels (800 opinions total).3.1 Deceptive opinions via Mechanical TurkCrowdsourcing services such as AMT have madelarge-scale data annotation and collection efforts fi-nancially affordable by granting anyone with ba-sic programming skills access to a marketplace ofanonymous online workers (known as Turkers) will-ing to complete small tasks.To solicit gold-standard deceptive opinion spamusing AMT, we create a pool of 400 Human-Intelligence Tasks (HITs) and allocate them evenlyacross our 20 chosen hotels.
To ensure that opin-ions are written by unique authors, we allow only asingle submission per Turker.
We also restrict ourtask to Turkers who are located in the United States,and who maintain an approval rating of at least 90%.Turkers are allowed a maximum of 30 minutes towork on the HIT, and are paid one US dollar for anaccepted submission.Each HIT presents the Turker with the name andwebsite of a hotel.
The HIT instructions ask theTurker to assume that they work for the hotel?s mar-keting department, and to pretend that their bosswants them to write a fake review (as if they werea customer) to be posted on a travel review website;additionally, the review needs to sound realistic andportray the hotel in a positive light.
A disclaimer8TripAdvisor utilizes a proprietary ranking system to assesshotel popularity.
We chose the 20 hotels with the greatest num-ber of reviews, irrespective of the TripAdvisor ranking.9It has been hypothesized that popular offerings are lesslikely to become targets of deceptive opinion spam, since therelative impact of the spam in such cases is small (Jindal andLiu, 2008; Lim et al, 2010).
By considering only the mostpopular hotels, we hope to minimize the risk of mining opinionspam and labeling it as truthful.10http://mturk.com311Time spent t (minutes)All submissionscount: 400tmin: 0.08, tmax: 29.78t?
: 8.06, s: 6.32Length ` (words)All submissions`min: 25, `max: 425?`: 115.75, s: 61.30Time spent t < 1count: 47`min: 39, `max: 407?`: 113.94, s: 66.24Time spent t ?
1count: 353`min: 25, `max: 425?`: 115.99, s: 60.71Table 1: Descriptive statistics for 400 deceptive opinionspam submissions gathered using AMT.
s corresponds tothe sample standard deviation.indicates that any submission found to be of insuffi-cient quality (e.g., written for the wrong hotel, unin-telligible, unreasonably short,11 plagiarized,12 etc.
)will be rejected.It took approximately 14 days to collect 400 sat-isfactory deceptive opinions.
Descriptive statisticsappear in Table 1.
Submissions vary quite dramati-cally both in length, and time spent on the task.
Par-ticularly, nearly 12% of the submissions were com-pleted in under one minute.
Surprisingly, an inde-pendent two-tailed t-test between the mean length ofthese submissions (?`t<1) and the other submissions(?`t?1) reveals no significant difference (p = 0.83).We suspect that these ?quick?
users may have startedworking prior to having formally accepted the HIT,presumably to circumvent the imposed time limit.Indeed, the quickest submission took just 5 secondsand contained 114 words.3.2 Truthful opinions from TripAdvisorFor truthful opinions, we mine all 6,977 reviewsfrom the 20 most popular Chicago hotels onTripAdvisor.
From these we eliminate:?
3,130 non-5-star reviews;?
41 non-English reviews;13?
75 reviews with fewer than 150 characterssince, by construction, deceptive opinions are11A submission is considered unreasonably short if it con-tains fewer than 150 characters.12Submissions are individually checked for plagiarism athttp://plagiarisma.net.13Language is determined using http://tagthe.net.at least 150 characters long (see footnote 11 inSection 3.1);?
1,607 reviews written by first-time authors?new users who have not previously posted anopinion on TripAdvisor?since these opinionsare more likely to contain opinion spam, whichwould reduce the integrity of our truthful re-view data (Wu et al, 2010).Finally, we balance the number of truthful anddeceptive opinions by selecting 400 of the remain-ing 2,124 truthful reviews, such that the documentlengths of the selected truthful reviews are similarlydistributed to those of the deceptive reviews.
Workby Serrano et al (2009) suggests that a log-normaldistribution is appropriate for modeling documentlengths.
Thus, for each of the 20 chosen hotels, weselect 20 truthful reviews from a log-normal (left-truncated at 150 characters) distribution fit to thelengths of the deceptive reviews.14 Combined withthe 400 deceptive reviews gathered in Section 3.1this yields our final dataset of 800 reviews.3.3 Human performanceAssessing human deception detection performanceis important for several reasons.
First, there are fewother baselines for our classification task; indeed, re-lated studies (Jindal and Liu, 2008; Mihalcea andStrapparava, 2009) have only considered a randomguess baseline.
Second, assessing human perfor-mance is necessary to validate the deceptive opin-ions gathered in Section 3.1.
If human performanceis low, then our deceptive opinions are convincing,and therefore, deserving of further attention.Our initial approach to assessing human perfor-mance on this task was with Mechanical Turk.
Un-fortunately, we found that some Turkers selectedamong the choices seemingly at random, presum-ably to maximize their hourly earnings by obviatingthe need to read the review.
While a similar effecthas been observed previously (Akkaya et al, 2010),there remains no universal solution.Instead, we solicit the help of three volunteer un-dergraduate university students to make judgmentson a subset of our data.
This balanced subset, cor-responding to the first fold of our cross-validation14We use the R package GAMLSS (Rigby and Stasinopoulos,2005) to fit the left-truncated log-normal distribution.312TRUTHFUL DECEPTIVEAccuracy P R F P R FHUMANJUDGE 1 61.9% 57.9 87.5 69.7 74.4 36.3 48.7JUDGE 2 56.9% 53.9 95.0 68.8 78.9 18.8 30.3JUDGE 3 53.1% 52.3 70.0 59.9 54.7 36.3 43.6METAMAJORITY 58.1% 54.8 92.5 68.8 76.0 23.8 36.2SKEPTIC 60.6% 60.8 60.0 60.4 60.5 61.3 60.9Table 2: Performance of three human judges and two meta-judges on a subset of 160 opinions, corresponding to thefirst fold of our cross-validation experiments in Section 5.
Boldface indicates the largest value for each column.experiments described in Section 5, contains all 40reviews from each of four randomly chosen hotels.Unlike the Turkers, our student volunteers are notoffered a monetary reward.
Consequently, we con-sider their judgements to be more honest than thoseobtained via AMT.Additionally, to test the extent to which the in-dividual human judges are biased, we evaluate theperformance of two virtual meta-judges.
Specifi-cally, the MAJORITY meta-judge predicts ?decep-tive?
when at least two out of three human judgesbelieve the review to be deceptive, and the SKEP-TIC meta-judge predicts ?deceptive?
when any hu-man judge believes the review to be deceptive.Human and meta-judge performance is given inTable 2.
It is clear from the results that humanjudges are not particularly effective at this task.
In-deed, a two-tailed binomial test fails to reject thenull hypothesis that JUDGE 2 and JUDGE 3 per-form at-chance (p = 0.003, 0.10, 0.48 for the threejudges, respectively).
Furthermore, all three judgessuffer from truth-bias (Vrij, 2008), a common find-ing in deception detection research in which hu-man judges are more likely to classify an opinionas truthful than deceptive.
In fact, JUDGE 2 clas-sified fewer than 12% of the opinions as decep-tive!
Interestingly, this bias is effectively smoothedby the SKEPTIC meta-judge, which produces nearlyperfectly class-balanced predictions.
A subsequentreevaluation of human performance on this task sug-gests that the truth-bias can be reduced if judgesare given the class-proportions in advance, althoughsuch prior knowledge is unrealistic; and ultimately,performance remains similar to that of Table 2.Inter-annotator agreement among the threejudges, computed using Fleiss?
kappa, is 0.11.While there is no precise rule for interpretingkappa scores, Landis and Koch (1977) suggestthat scores in the range (0.00, 0.20] correspondto ?slight agreement?
between annotators.
Thelargest pairwise Cohen?s kappa is 0.12, betweenJUDGE 2 and JUDGE 3?a value far below generallyaccepted pairwise agreement levels.
We suspectthat agreement among our human judges is solow precisely because humans are poor judges ofdeception (Vrij, 2008), and therefore they performnearly at-chance respective to one another.4 Automated Approaches to DeceptiveOpinion Spam DetectionWe consider three automated approaches to detect-ing deceptive opinion spam, each of which utilizesclassifiers (described in Section 4.4) trained on thedataset of Section 3.
The features employed by eachstrategy are outlined here.4.1 Genre identificationWork in computational linguistics has shown thatthe frequency distribution of part-of-speech (POS)tags in a text is often dependent on the genre of thetext (Biber et al, 1999; Rayson et al, 2001).
In ourgenre identification approach to deceptive opinionspam detection, we test if such a relationship existsfor truthful and deceptive reviews by constructing,for each review, features based on the frequencies ofeach POS tag.15 These features are also intended toprovide a good baseline with which to compare ourother automated approaches.4.2 Psycholinguistic deception detectionThe Linguistic Inquiry and Word Count (LIWC)software (Pennebaker et al, 2007) is a popular au-tomated text analysis tool used widely in the so-cial sciences.
It has been used to detect personality15We use the Stanford Parser (Klein and Manning, 2003) toobtain the relative POS frequencies.313traits (Mairesse et al, 2007), to study tutoring dy-namics (Cade et al, 2010), and, most relevantly, toanalyze deception (Hancock et al, 2008; Mihalceaand Strapparava, 2009; Vrij et al, 2007).While LIWC does not include a text classifier, wecan create one with features derived from the LIWCoutput.
In particular, LIWC counts and groupsthe number of instances of nearly 4,500 keywordsinto 80 psychologically meaningful dimensions.
Weconstruct one feature for each of the 80 LIWC di-mensions, which can be summarized broadly underthe following four categories:1.
Linguistic processes: Functional aspects of text(e.g., the average number of words per sen-tence, the rate of misspelling, swearing, etc.)2.
Psychological processes: Includes all social,emotional, cognitive, perceptual and biologicalprocesses, as well as anything related to time orspace.3.
Personal concerns: Any references to work,leisure, money, religion, etc.4.
Spoken categories: Primarily filler and agree-ment words.While other features have been considered in pastdeception detection work, notably those of Zhou etal.
(2004), early experiments found LIWC featuresto perform best.
Indeed, the LIWC2007 softwareused in our experiments subsumes most of the fea-tures introduced in other work.
Thus, we focus ourpsycholinguistic approach to deception detection onLIWC-based features.4.3 Text categorizationIn contrast to the other strategies just discussed,our text categorization approach to deception de-tection allows us to model both content and con-text with n-gram features.
Specifically, we considerthe following three n-gram feature sets, with thecorresponding features lowercased and unstemmed:UNIGRAMS, BIGRAMS+, TRIGRAMS+, where thesuperscript + indicates that the feature set subsumesthe preceding feature set.4.4 ClassifiersFeatures from the three approaches just introducedare used to train Na?
?ve Bayes and Support VectorMachine classifiers, both of which have performedwell in related work (Jindal and Liu, 2008; Mihalceaand Strapparava, 2009; Zhou et al, 2008).For a document ~x, with label y, the Na?
?ve Bayes(NB) classifier gives us the following decision rule:y?
= arg maxcPr(y = c) ?
Pr(~x | y = c) (1)When the class prior is uniform, for examplewhen the classes are balanced (as in our case), (1)can be simplified to the maximum likelihood classi-fier (Peng and Schuurmans, 2003):y?
= arg maxcPr(~x | y = c) (2)Under (2), both the NB classifier used by Mihal-cea and Strapparava (2009) and the language modelclassifier used by Zhou et al (2008) are equivalent.Thus, following Zhou et al (2008), we use the SRILanguage Modeling Toolkit (Stolcke, 2002) to esti-mate individual language models, Pr(~x | y = c),for truthful and deceptive opinions.
We considerall three n-gram feature sets, namely UNIGRAMS,BIGRAMS+, and TRIGRAMS+, with correspondinglanguage models smoothed using the interpolatedKneser-Ney method (Chen and Goodman, 1996).We also train Support Vector Machine (SVM)classifiers, which find a high-dimensional separatinghyperplane between two groups of data.
To simplifyfeature analysis in Section 5, we restrict our evalu-ation to linear SVMs, which learn a weight vector~w and bias term b, such that a document ~x can beclassified by:y?
= sign(~w ?
~x + b) (3)We use SVMlight (Joachims, 1999) to train ourlinear SVM models on all three approaches andfeature sets described above, namely POS, LIWC,UNIGRAMS, BIGRAMS+, and TRIGRAMS+.
We alsoevaluate every combination of these features, butfor brevity include only LIWC+BIGRAMS+, whichperforms best.
Following standard practice, doc-ument vectors are normalized to unit-length.
ForLIWC+BIGRAMS+, we unit-length normalize LIWCand BIGRAMS+ features individually before com-bining them.314TRUTHFUL DECEPTIVEApproach Features Accuracy P R F P R FGENRE IDENTIFICATION POSSVM 73.0% 75.3 68.5 71.7 71.1 77.5 74.2PSYCHOLINGUISTICLIWCSVM 76.8% 77.2 76.0 76.6 76.4 77.5 76.9DECEPTION DETECTIONTEXT CATEGORIZATIONUNIGRAMSSVM 88.4% 89.9 86.5 88.2 87.0 90.3 88.6BIGRAMS+SVM 89.6% 90.1 89.0 89.6 89.1 90.3 89.7LIWC+BIGRAMS+SVM 89.8% 89.8 89.8 89.8 89.8 89.8 89.8TRIGRAMS+SVM 89.0% 89.0 89.0 89.0 89.0 89.0 89.0UNIGRAMSNB 88.4% 92.5 83.5 87.8 85.0 93.3 88.9BIGRAMS+NB 88.9% 89.8 87.8 88.7 88.0 90.0 89.0TRIGRAMS+NB 87.6% 87.7 87.5 87.6 87.5 87.8 87.6HUMAN / METAJUDGE 1 61.9% 57.9 87.5 69.7 74.4 36.3 48.7JUDGE 2 56.9% 53.9 95.0 68.8 78.9 18.8 30.3SKEPTIC 60.6% 60.8 60.0 60.4 60.5 61.3 60.9Table 3: Automated classifier performance for three approaches based on nested 5-fold cross-validation experiments.Reported precision, recall and F-score are computed using a micro-average, i.e., from the aggregate true positive, falsepositive and false negative rates, as suggested by Forman and Scholz (2009).
Human performance is repeated here forJUDGE 1, JUDGE 2 and the SKEPTIC meta-judge, although they cannot be directly compared since the 160-opinionsubset on which they are assessed only corresponds to the first cross-validation fold.5 Results and DiscussionThe deception detection strategies described in Sec-tion 4 are evaluated using a 5-fold nested cross-validation (CV) procedure (Quadrianto et al, 2009),where model parameters are selected for each testfold based on standard CV experiments on the train-ing folds.
Folds are selected so that each contains allreviews from four hotels; thus, learned models arealways evaluated on reviews from unseen hotels.Results appear in Table 3.
We observe that auto-mated classifiers outperform human judges for everymetric, except truthful recall where JUDGE 2 per-forms best.16 However, this is expected given thatuntrained humans often focus on unreliable cues todeception (Vrij, 2008).
For example, one study ex-amining deception in online dating found that hu-mans perform at-chance detecting deceptive pro-files because they rely on text-based cues that areunrelated to deception, such as second-person pro-nouns (Toma and Hancock, In Press).Among the automated classifiers, baseline per-formance is given by the simple genre identifica-tion approach (POSSVM) proposed in Section 4.1.Surprisingly, we find that even this simple auto-16As mentioned in Section 3.3, JUDGE 2 classified fewer than12% of opinions as deceptive.
While achieving 95% truthful re-call, this judge?s corresponding precision was not significantlybetter than chance (two-tailed binomial p = 0.4).mated classifier outperforms most human judges(one-tailed sign test p = 0.06, 0.01, 0.001 for thethree judges, respectively, on the first fold).
Thisresult is best explained by theories of reality mon-itoring (Johnson and Raye, 1981), which suggestthat truthful and deceptive opinions might be clas-sified into informative and imaginative genres, re-spectively.
Work by Rayson et al (2001) has foundstrong distributional differences between informa-tive and imaginative writing, namely that the formertypically consists of more nouns, adjectives, prepo-sitions, determiners, and coordinating conjunctions,while the latter consists of more verbs,17 adverbs,18pronouns, and pre-determiners.
Indeed, we find thatthe weights learned by POSSVM (found in Table 4)are largely in agreement with these findings, no-tably except for adjective and adverb superlatives,the latter of which was found to be an exception byRayson et al (2001).
However, that deceptive opin-ions contain more superlatives is not unexpected,since deceptive writing (but not necessarily imagi-native writing in general) often contains exaggeratedlanguage (Buller and Burgoon, 1996; Hancock et al,2008).Both remaining automated approaches to detect-ing deceptive opinion spam outperform the simple17Past participle verbs were an exception.18Superlative adverbs were an exception.315TRUTHFUL/INFORMATIVE DECEPTIVE/IMAGINATIVECategory Variant Weight Category Variant WeightNOUNSSingular 0.008VERBSBase -0.057Plural 0.002 Past tense 0.041Proper, singular -0.041 Present participle -0.089Proper, plural 0.091 Singular, present -0.031ADJECTIVESGeneral 0.002 Third person0.026Comparative 0.058 singular, presentSuperlative -0.164 Modal -0.063PREPOSITIONS General 0.064ADVERBSGeneral 0.001DETERMINERS General 0.009 Comparative -0.035COORD.
CONJ.
General 0.094PRONOUNSPersonal -0.098VERBS Past participle 0.053 Possessive -0.303ADVERBS Superlative -0.094 PRE-DETERMINERS General 0.017Table 4: Average feature weights learned by POSSVM.
Based on work by Rayson et al (2001), we expect weights onthe left to be positive (predictive of truthful opinions), and weights on the right to be negative (predictive of deceptiveopinions).
Boldface entries are at odds with these expectations.
We report average feature weights of unit-normalizedweight vectors, rather than raw weights vectors, to account for potential differences in magnitude between the folds.genre identification baseline just discussed.
Specifi-cally, the psycholinguistic approach (LIWCSVM) pro-posed in Section 4.2 performs 3.8% more accurately(one-tailed sign test p = 0.02), and the standard textcategorization approach proposed in Section 4.3 per-forms between 14.6% and 16.6% more accurately.However, best performance overall is achieved bycombining features from these two approaches.
Par-ticularly, the combined model LIWC+BIGRAMS+SVMis 89.8% accurate at detecting deceptive opinionspam.19Surprisingly, models trained only onUNIGRAMS?the simplest n-gram feature set?outperform all non?text-categorization approaches,and models trained on BIGRAMS+ perform evenbetter (one-tailed sign test p = 0.07).
This suggeststhat a universal set of keyword-based deceptioncues (e.g., LIWC) is not the best approach to de-tecting deception, and a context-sensitive approach(e.g., BIGRAMS+) might be necessary to achievestate-of-the-art deception detection performance.To better understand the models learned by theseautomated approaches, we report in Table 5 the top15 highest weighted features for each class (truthfuland deceptive) as learned by LIWC+BIGRAMS+SVMand LIWCSVM.
In agreement with theories of realitymonitoring (Johnson and Raye, 1981), we observethat truthful opinions tend to include more sensorialand concrete language than deceptive opinions; in19The result is not significantly better than BIGRAMS+SVM.LIWC+BIGRAMS+SVM LIWCSVMTRUTHFUL DECEPTIVE TRUTHFUL DECEPTIVE- chicago hear i... my number familyon hotel allpunct perspronlocation , and negemo see) luxury dash pronounallpunctLIWC experience exclusive leisurefloor hilton we exclampunct( business sexual sixlettersthe hotel vacation period posemobathroom i otherpunct commasmall spa space causehelpful looking human auxverb$ while past futurehotel .
husband inhibition perceptualother my husband assent feelTable 5: Top 15 highest weighted truthful and deceptivefeatures learned by LIWC+BIGRAMS+SVM and LIWCSVM.Ambiguous features are subscripted to indicate the sourceof the feature.
LIWC features correspond to groupsof keywords as explained in Section 4.2; more detailsabout LIWC and the LIWC categories are available athttp://liwc.net.particular, truthful opinions are more specific aboutspatial configurations (e.g., small, bathroom, on, lo-cation).
This finding is also supported by recentwork by Vrij et al (2009) suggesting that liars haveconsiderable difficultly encoding spatial informationinto their lies.
Accordingly, we observe an increasedfocus in deceptive opinions on aspects external tothe hotel being reviewed (e.g., husband, business,316vacation).We also acknowledge several findings that, on thesurface, are in contrast to previous psycholinguisticstudies of deception (Hancock et al, 2008; Newmanet al, 2003).
For instance, while deception is oftenassociated with negative emotion terms, our decep-tive reviews have more positive and fewer negativeemotion terms.
This pattern makes sense when oneconsiders the goal of our deceivers, namely to createa positive review (Buller and Burgoon, 1996).Deception has also previously been associatedwith decreased usage of first person singular, an ef-fect attributed to psychological distancing (Newmanet al, 2003).
In contrast, we find increased firstperson singular to be among the largest indicatorsof deception, which we speculate is due to our de-ceivers attempting to enhance the credibility of theirreviews by emphasizing their own presence in thereview.
Additional work is required, but these find-ings further suggest the importance of moving be-yond a universal set of deceptive language features(e.g., LIWC) by considering both the contextual (e.g.,BIGRAMS+) and motivational parameters underly-ing a deception as well.6 Conclusion and Future WorkIn this work we have developed the first large-scaledataset containing gold-standard deceptive opinionspam.
With it, we have shown that the detectionof deceptive opinion spam is well beyond the ca-pabilities of human judges, most of whom performroughly at-chance.
Accordingly, we have introducedthree automated approaches to deceptive opinionspam detection, based on insights coming from re-search in computational linguistics and psychology.We find that while standard n-gram?based text cate-gorization is the best individual detection approach,a combination approach using psycholinguistically-motivated features and n-gram features can performslightly better.Finally, we have made several theoretical con-tributions.
Specifically, our findings suggest theimportance of considering both the context (e.g.,BIGRAMS+) and motivations underlying a decep-tion, rather than strictly adhering to a universal setof deception cues (e.g., LIWC).
We have also pre-sented results based on the feature weights learnedby our classifiers that illustrate the difficulties facedby liars in encoding spatial information.
Lastly, wehave discovered a plausible relationship between de-ceptive opinion spam and imaginative writing, basedon POS distributional similarities.Possible directions for future work include an ex-tended evaluation of the methods proposed in thiswork to both negative opinions, as well as opinionscoming from other domains.
Many additional ap-proaches to detecting deceptive opinion spam arealso possible, and a focus on approaches with highdeceptive precision might be useful for productionenvironments.AcknowledgmentsThis work was supported in part by NationalScience Foundation Grants BCS-0624277, BCS-0904822, HSD-0624267, IIS-0968450, and NSCC-0904822, as well as a gift from Google, and theJack Kent Cooke Foundation.
We also thank, al-phabetically, Rachel Boochever, Cristian Danescu-Niculescu-Mizil, Alicia Granstein, Ulrike Gretzel,Danielle Kirshenblat, Lillian Lee, Bin Lu, JackNewton, Melissa Sackler, Mark Thomas, and AngieYoo, as well as members of the Cornell NLP sem-inar group and the ACL reviewers for their insight-ful comments, suggestions and advice on various as-pects of this work.ReferencesC.
Akkaya, A. Conrad, J. Wiebe, and R. Mihalcea.
2010.Amazon mechanical turk for subjectivity word sensedisambiguation.
In Proceedings of the NAACL HLT2010 Workshop on Creating Speech and LanguageData with Amazons Mechanical Turk, Los Angeles,pages 195?203.D.
Biber, S. Johansson, G. Leech, S. Conrad, E. Finegan,and R. Quirk.
1999.
Longman grammar of spoken andwritten English, volume 2.
MIT Press.C.F.
Bond and B.M.
DePaulo.
2006.
Accuracy of de-ception judgments.
Personality and Social PsychologyReview, 10(3):214.D.B.
Buller and J.K. Burgoon.
1996.
Interpersonaldeception theory.
Communication Theory, 6(3):203?242.W.L.
Cade, B.A.
Lehman, and A. Olney.
2010.
An ex-ploration of off topic conversation.
In Human Lan-guage Technologies: The 2010 Annual Conference of317the North American Chapter of the Association forComputational Linguistics, pages 669?672.
Associa-tion for Computational Linguistics.S.F.
Chen and J. Goodman.
1996.
An empirical study ofsmoothing techniques for language modeling.
In Pro-ceedings of the 34th annual meeting on Associationfor Computational Linguistics, pages 310?318.
Asso-ciation for Computational Linguistics.C.
Danescu-Niculescu-Mizil, G. Kossinets, J. Kleinberg,and L. Lee.
2009.
How opinions are received by on-line communities: a case study on amazon.com help-fulness votes.
In Proceedings of the 18th internationalconference on World wide web, pages 141?150.
ACM.H.
Drucker, D. Wu, and V.N.
Vapnik.
2002.
Supportvector machines for spam categorization.
Neural Net-works, IEEE Transactions on, 10(5):1048?1054.G.
Forman and M. Scholz.
2009.
Apples-to-Apples inCross-Validation Studies: Pitfalls in Classifier Perfor-mance Measurement.
ACM SIGKDD Explorations,12(1):49?57.Z.
Gyo?ngyi, H. Garcia-Molina, and J. Pedersen.
2004.Combating web spam with trustrank.
In Proceedingsof the Thirtieth international conference on Very largedata bases-Volume 30, pages 576?587.
VLDB Endow-ment.J.T.
Hancock, L.E.
Curry, S. Goorha, and M. Woodworth.2008.
On lying and being lied to: A linguistic anal-ysis of deception in computer-mediated communica-tion.
Discourse Processes, 45(1):1?23.J.
Jansen.
2010.
Online product research.
Pew Internet& American Life Project Report.N.
Jindal and B. Liu.
2008.
Opinion spam and analysis.In Proceedings of the international conference on Websearch and web data mining, pages 219?230.
ACM.T.
Joachims.
1998.
Text categorization with support vec-tor machines: Learning with many relevant features.Machine Learning: ECML-98, pages 137?142.T.
Joachims.
1999.
Making large-scale support vec-tor machine learning practical.
In Advances in kernelmethods, page 184.
MIT Press.M.K.
Johnson and C.L.
Raye.
1981.
Reality monitoring.Psychological Review, 88(1):67?85.S.M.
Kim, P. Pantel, T. Chklovski, and M. Pennacchiotti.2006.
Automatically assessing review helpfulness.In Proceedings of the 2006 Conference on EmpiricalMethods in Natural Language Processing, pages 423?430.
Association for Computational Linguistics.D.
Klein and C.D.
Manning.
2003.
Accurate unlexical-ized parsing.
In Proceedings of the 41st Annual Meet-ing on Association for Computational Linguistics-Volume 1, pages 423?430.
Association for Computa-tional Linguistics.J.R.
Landis and G.G.
Koch.
1977.
The measurement ofobserver agreement for categorical data.
Biometrics,33(1):159.E.P.
Lim, V.A.
Nguyen, N. Jindal, B. Liu, and H.W.Lauw.
2010.
Detecting product review spammers us-ing rating behaviors.
In Proceedings of the 19th ACMinternational conference on Information and knowl-edge management, pages 939?948.
ACM.S.W.
Litvin, R.E.
Goldsmith, and B. Pan.
2008.
Elec-tronic word-of-mouth in hospitality and tourism man-agement.
Tourism management, 29(3):458?468.F.
Mairesse, M.A.
Walker, M.R.
Mehl, and R.K. Moore.2007.
Using linguistic cues for the automatic recogni-tion of personality in conversation and text.
Journal ofArtificial Intelligence Research, 30(1):457?500.R.
Mihalcea and C. Strapparava.
2009.
The lie detector:Explorations in the automatic recognition of deceptivelanguage.
In Proceedings of the ACL-IJCNLP 2009Conference Short Papers, pages 309?312.
Associationfor Computational Linguistics.M.L.
Newman, J.W.
Pennebaker, D.S.
Berry, and J.M.Richards.
2003.
Lying words: Predicting deceptionfrom linguistic styles.
Personality and Social Psychol-ogy Bulletin, 29(5):665.A.
Ntoulas, M. Najork, M. Manasse, and D. Fetterly.2006.
Detecting spam web pages through contentanalysis.
In Proceedings of the 15th international con-ference on World Wide Web, pages 83?92.
ACM.M.P.
O?Mahony and B. Smyth.
2009.
Learning to rec-ommend helpful hotel reviews.
In Proceedings ofthe third ACM conference on Recommender systems,pages 305?308.
ACM.F.
Peng and D. Schuurmans.
2003.
Combining naiveBayes and n-gram language models for text classifica-tion.
Advances in Information Retrieval, pages 547?547.J.W.
Pennebaker, C.K.
Chung, M. Ireland, A. Gonzales,and R.J. Booth.
2007.
The development and psycho-metric properties of LIWC2007.
Austin, TX, LIWC.Net.N.
Quadrianto, A.J.
Smola, T.S.
Caetano, and Q.V.Le.
2009.
Estimating labels from label proportions.The Journal of Machine Learning Research, 10:2349?2374.P.
Rayson, A. Wilson, and G. Leech.
2001.
Grammaticalword class variation within the British National Cor-pus sampler.
Language and Computers, 36(1):295?306.R.A.
Rigby and D.M.
Stasinopoulos.
2005.
Generalizedadditive models for location, scale and shape.
Jour-nal of the Royal Statistical Society: Series C (AppliedStatistics), 54(3):507?554.318F.
Sebastiani.
2002.
Machine learning in automatedtext categorization.
ACM computing surveys (CSUR),34(1):1?47.M.A?.
Serrano, A. Flammini, and F. Menczer.
2009.Modeling statistical properties of written text.
PloSone, 4(4):5372.A.
Stolcke.
2002.
SRILM-an extensible language mod-eling toolkit.
In Seventh International Conference onSpoken Language Processing, volume 3, pages 901?904.
Citeseer.C.
Toma and J.T.
Hancock.
In Press.
What Lies Beneath:The Linguistic Traces of Deception in Online DatingProfiles.
Journal of Communication.A.
Vrij, S. Mann, S. Kristen, and R.P.
Fisher.
2007.
Cuesto deception and ability to detect lies as a functionof police interview styles.
Law and human behavior,31(5):499?518.A.
Vrij, S. Leal, P.A.
Granhag, S. Mann, R.P.
Fisher,J.
Hillman, and K. Sperry.
2009.
Outsmarting theliars: The benefit of asking unanticipated questions.Law and human behavior, 33(2):159?166.A.
Vrij.
2008.
Detecting lies and deceit: Pitfalls andopportunities.
Wiley-Interscience.W.
Weerkamp and M. De Rijke.
2008.
Credibility im-proves topical blog post retrieval.
ACL-08: HLT,pages 923?931.M.
Weimer, I. Gurevych, and M. Mu?hlha?user.
2007.
Au-tomatically assessing the post quality in online discus-sions on software.
In Proceedings of the 45th An-nual Meeting of the ACL on Interactive Poster andDemonstration Sessions, pages 125?128.
Associationfor Computational Linguistics.G.
Wu, D. Greene, B. Smyth, and P. Cunningham.
2010.Distortion as a validation criterion in the identificationof suspicious reviews.
Technical report, UCD-CSI-2010-04, University College Dublin.K.H.
Yoo and U. Gretzel.
2009.
Comparison of De-ceptive and Truthful Travel Reviews.
Information andCommunication Technologies in Tourism 2009, pages37?47.L.
Zhou, J.K. Burgoon, D.P.
Twitchell, T. Qin, and J.F.Nunamaker Jr. 2004.
A comparison of classifica-tion methods for predicting deception in computer-mediated communication.
Journal of Management In-formation Systems, 20(4):139?166.L.
Zhou, Y. Shi, and D. Zhang.
2008.
A Statistical Lan-guage Modeling Approach to Online Deception De-tection.
IEEE Transactions on Knowledge and DataEngineering, 20(8):1077?1081.319
