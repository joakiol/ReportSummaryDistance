Proceedings of the ACL Interactive Poster and Demonstration Sessions,pages 45?48, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsMultimodal Generation in the COMIC Dialogue SystemMary Ellen Foster and Michael WhiteInstitute for Communicating and Collaborative SystemsSchool of Informatics, University of Edinburgh{M.E.Foster,Michael.White}@ed.ac.ukAndrea Setzer and Roberta CatizoneNatural Language Processing GroupDepartment of Computer Science, University of Sheffield{A.Setzer,R.Catizone}@dcs.shef.ac.ukAbstractWe describe how context-sensitive, user-tailored output is specified and producedin the COMIC multimodal dialogue sys-tem.
At the conference, we will demon-strate the user-adapted features of the dia-logue manager and text planner.1 IntroductionCOMIC1 is an EU IST 5th Framework project com-bining fundamental research on human-human inter-action with advanced technology development formultimodal conversational systems.
The projectdemonstrator system adds a dialogue interface to aCAD-like application used in bathroom sales situa-tions to help clients redesign their rooms.
The inputto the system includes speech, handwriting, and pengestures; the output combines synthesised speech, atalking head, and control of the underlying applica-tion.
Figure 1 shows screen shots of the COMICinterface.There are four main phases in the demonstra-tor.
First, the user specifies the shape of theirown bathroom, using a combination of speech in-put, pen-gesture recognition and handwriting recog-nition.
Next, the user chooses a layout for the sani-tary ware in the room.
After that, the system guidesthe user in browsing through a range of tiling op-tions for the bathroom.
Finally, the user is given a1COnversational Multimodal Interaction with Computers;http://www.hcrc.ed.ac.uk/comic/.three-dimensional walkthrough of the finished bath-room.
We will focus on how context-sensitive, user-tailored output is generated in the third, guided-browsing phase of the interaction.
Figure 2 showsa typical user request and response from COMIC inthis phase.
The pitch accents and multimodal ac-tions are indicated; there is also facial emphasis cor-responding to the accented words.The primary goal of COMIC?s guided-browsingphase is to help users become better informed aboutthe range of tiling options for their bathroom.
Inthis regard, it is similar to the web-based systemM-PIRO (Isard et al, 2003), which generates per-sonalised descriptions of museum objects, and con-trasts with task-oriented embodied dialogue systemssuch as SmartKom (Wahlster, 2003).
Since guidedbrowsing requires extended descriptions, in COMICwe have placed greater emphasis on producing high-quality adaptive output than have previous embodieddialogue projects such as August (Gustafson et al,1999) and Rea (Cassell et al, 1999).
To generateits adaptive output, COMIC uses information fromthe dialogue history and the user model throughoutthe generation process, as in FLIGHTS (Moore etal., 2004); both systems build upon earlier work onadaptive content planning (Carenini, 2000; Walkeret al, 2002).
An experimental study (Foster andWhite, 2005) has shown that this adaptation is per-ceptible to users of COMIC.2 Dialogue ManagementThe task of the Dialogue and Action Manager(DAM) is to decide what the system will show andsay in response to user input.
The input to the45(a) Bathroom-design application (b) Talking headFigure 1: Components of the COMIC interfaceUser Tell me about this design [click on Alt Mettlach]COMIC [Look at screen]THIS DESIGN is in the CLASSIC style.
[circle tiles]As you can see, the colours are DARK RED and OFF WHITE.
[point at tiles]The tiles are from the ALT METTLACH collection by VILLEROY AND BOCH.
[point at design name]Figure 2: Sample COMIC input and outputDAM consists of multiple scored hypotheses con-taining high-level, modality-independent specifica-tions of the user input; the output is a similar high-level specification of the system action.
The DAMitself is modality-independent.
For example, the in-put in Figure 2 could equally well have been the usersimply pointing to a design on the screen, with nospeech at all.
This would have resulted in the sameabstract DAM input, and thus in the same output: arequest to show and describe the given design.The COMIC DAM (Catizone et al, 2003) isa general-purpose dialogue manager which canhandle different dialogue management styles suchas system-driven, user-driven or mixed-initiative.The general-purpose part of the DAM is a sim-ple stack architecture with a control structure;all the application-dependent information is storedin a variation of Augmented Transition Networks(ATNs) called Dialogue Action Forms (DAFs).These DAFs represent general dialogue moves, aswell as sub-tasks or topics, and are pushed onto andpopped off of the stack as the dialogue proceeds.When processing a user input, the control struc-ture decides whether the DAM can stay within thecurrent topic (and thus the current DAF), or whethera topic shift has occurred.
In the latter case, a newDAF is pushed onto the stack and executed.
Afterthat topic has been exhausted, the DAM returns tothe previous topic automatically.
The same princi-ple holds for error handling, which is implementedat different levels in our approach.In the guided-browsing phase of the COMIC sys-tem, the user may browse tiling designs by colour,style or manufacturer, look at designs in detail, orchange the amount of border and decoration tiles.The DAM uses the system ontology to retrieve de-signs according to the chosen feature, and consultsthe user model and dialogue history to narrow downthe resulting designs to a small set to be shown anddescribed to the user.463 Presentation PlanningThe COMIC fission module processes high-levelsystem-output specifications generated by the DAM.For the example in Figure 2, the DAM output indi-cates that the given tile design should be shown anddescribed, and that the description must mention thestyle.
The fission module fleshes out such specifica-tions by selecting and structuring content, planningthe surface form of the text to realise that content,choosing multimodal behaviours to accompany thetext, and controlling the output of the whole sched-ule.
In this section, we describe the planning pro-cess; output coordination is dealt with in Section 6.Full technical details of the fission module are givenin (Foster, 2005).To create the textual content of a description, thefission module proceeds as follows.
First, it gath-ers all of the properties of the specified design fromthe system ontology.
Next, it selects the propertiesto include in the description, using information fromthe dialogue history and the user model, along withany properties specifically requested by the dialoguemanager.
It then creates a structure for the selectedproperties and creates logical forms as input for theOpenCCG surface realiser.
The logical forms mayinclude explicit alternatives in cases where there aremultiple ways of expressing a property; for exam-ple, it could say either This design is in the classicstyle or This design is classic.
OpenCCG makes useof statistical language models to choose among suchalternatives.
This process is described in detail in(Foster and White, 2004; Foster and White, 2005).In addition to text, the output of COMICalso incorporates multimodal behaviours includingprosodic specifications for the speech synthesiser(pitch accents and boundary tones), facial behaviourspecifications (expressions and gaze shifts), and de-ictic gestures at objects on the application screen us-ing a simulated pointer.
Pitch accents and bound-ary tones are selected by the realiser based on thecontext-sensitive information-structure annotations(theme/rheme; marked/unmarked) included in thelogical forms.
At the moment, the other multimodalcoarticulations are specified directly by the fissionmodule, but we are currently experimenting withusing the OpenCCG realiser?s language models tochoose them, using example-driven techniques.4 Surface RealisationSurface realisation in COMIC is performed by theOpenCCG2 realiser, a practical, open-source realiserbased on Combinatory Categorial Grammar (CCG)(Steedman, 2000b).
It employs a novel ensemble ofmethods for improving the efficiency of CCG reali-sation, and in particular, makes integrated use of n-gram scoring of possible realisations in its chart re-alisation algorithm (White, 2004; White, 2005).
Then-gram scoring allows the realiser to work in ?any-time?
mode?able at any time to return the highest-scoring complete realisation?and ensures that agood realisation can be found reasonably quicklyeven when the number of possibilities is exponen-tial.
This makes it particularly suited for use in aninteractive dialogue system such as COMIC.In COMIC, the OpenCCG realiser uses factoredlanguage models (Bilmes and Kirchhoff, 2003) overwords and multimodal coarticulations to select thehighest-scoring realisation licensed by the grammarthat satisfies the specification given by the fissionmodule.
Steedman?s (Steedman, 2000a) theory ofinformation structure and intonation is used to con-strain the choice of pitch accents and boundary tonesfor the speech synthesiser.5 Speech SynthesisThe COMIC speech-synthesis module is imple-mented as a client to the Festival speech-synthesissystem.3 We take advantage of recent advances inversion 2 of Festival (Clark et al, 2004) by usinga custom-built unit-selection voice with support forAPML prosodic annotation (de Carolis et al, 2004).Experiments have shown that synthesised speechwith contextually appropriate prosodic features canbe perceptibly more natural (Baker et al, 2004).Because the fission module needs the timing in-formation from the speech synthesiser to finalise theschedules for the other modalities, the synthesiserfirst prepares and stores the waveform for its inputtext; the sound is then played at a later time, whenthe fission module indicates that it is required.2http://openccg.sourceforge.net/3http://www.cstr.ed.ac.uk/projects/festival/476 Output CoordinationIn addition to planning the presentation content asdescribed earlier, the fission module also controlsthe system output to ensure that all parts of the pre-sentation are properly coordinated, using the tim-ing information returned by the speech synthesiserto create a full schedule for the turn to be generated.As described in (Foster, 2005), the fission moduleallows multiple segments to be prepared in advance,even while the preceding segments are being played.This serves to minimise the output delay, as there isno need to wait until a whole turn is fully preparedbefore output begins, and the time taken to speak theearlier parts of the turn can also be used to preparethe later parts.7 AcknowledgementsThis work was supported by the COMIC project(IST-2001-32311).
This paper describes only partof the work done in the project; please see http://www.hcrc.ed.ac.uk/comic/ for full details.
Wethank the other members of COMIC for their col-laboration during the course of the project.ReferencesRachel Baker, Robert A.J.
Clark, and Michael White.2004.
Synthesizing contextually appropriate intona-tion in limited domains.
In Proceedings of 5th ISCAworkshop on speech synthesis.Jeff Bilmes and Katrin Kirchhoff.
2003.
Factored lan-guage models and general parallelized backoff.
InProceedings of HLT-03.Giuseppe Carenini.
2000.
Generating and EvaluatingEvaluative Arguments.
Ph.D. thesis, Intelligent Sys-tems Program, University of Pittsburgh.Justine Cassell, Timothy Bickmore, Mark Billinghurst,Lee Campbell, Kenny Chang, Hannes Vilhja?lmsson,and Hao Yan.
1999.
Embodiment in conversationalinterfaces: Rea.
In Proceedings of CHI99.Roberta Catizone, Andrea Setzer, and Yorick Wilks.2003.
Multimodal dialogue management in theCOMIC project.
In Proceedings of EACL 2003 Work-shop on Dialogue Systems: Interaction, adaptation,and styles of management.Robert A.J.
Clark, Korin Richmond, and Simon King.2004.
Festival 2 ?
build your own general purposeunit selection speech synthesiser.
In Proceedings of5th ISCA workshop on speech synthesis.Berardina de Carolis, Catherine Pelachaud, IsabellaPoggi, and Mark Steedman.
2004.
APML, amark-up language for believable behaviour generation.In H Prendinger, editor, Life-like Characters, Tools,Affective Functions and Applications, pages 65?85.Springer.Mary Ellen Foster and Michael White.
2004.
Tech-niques for text planning with XSLT.
In Proceedingsof NLPXML-2004.Mary Ellen Foster and Michael White.
2005.
Assessingthe impact of adaptive generation in the COMIC multi-modal dialogue system.
In Proceedings of IJCAI-2005Workshop on Knowledge and Reasoning in PracticalDialogue Systems.
To appear.Mary Ellen Foster.
2005.
Interleaved planning and out-put in the COMIC fission module.
Submitted.Joakim Gustafson, Nikolaj Lindberg, and Magnus Lun-deberg.
1999.
The August spoken dialogue system.In Proceedings of Eurospeech 1999.Amy Isard, Jon Oberlander, Ion Androtsopoulos, andColin Matheson.
2003.
Speaking the users?
lan-guages.
IEEE Intelligent Systems, 18(1):40?45.Johanna Moore, Mary Ellen Foster, Oliver Lemon, andMichael White.
2004.
Generating tailored, compara-tive descriptions in spoken dialogue.
In Proceedingsof FLAIRS 2004.Mark Steedman.
2000a.
Information structure andthe syntax-phonology interface.
Linguistic Inquiry,31(4):649?689.Mark Steedman.
2000b.
The Syntactic Process.
MITPress.Wolfgang Wahlster.
2003.
SmartKom: Symmetric mul-timodality in an adaptive and reusable dialogue shell.In Proceedings of the Human Computer InteractionStatus Conference 2003.M.A.
Walker, S. Whittaker, A. Stent, P. Maloor, J.D.Moore, M. Johnston, and G. Vasireddy.
2002.
Speech-plans: Generating evaluative responses in spoken dia-logue.
In Proceedings of INLG 2002.Michael White.
2004.
Reining in CCG chart realization.In Proceedings of INLG 2004.Michael White.
2005.
Efficient realization of coordinatestructures in Combinatory Categorial Grammar.
Re-search on Language and Computation.
To appear.48
