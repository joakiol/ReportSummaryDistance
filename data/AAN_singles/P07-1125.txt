Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 992?999,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsWeakly Supervised Learning for Hedge Classification in Scientific LiteratureBen MedlockComputer LaboratoryUniversity of CambridgeCambridge, CB3 OFDbenmedlock@cantab.netTed BriscoeComputer LaboratoryUniversity of CambridgeCambridge, CB3 OFDejb@cl.cam.ac.ukAbstractWe investigate automatic classificationof speculative language (?hedging?
), inbiomedical text using weakly supervisedmachine learning.
Our contributions includea precise description of the task with anno-tation guidelines, analysis and discussion,a probabilistic weakly supervised learningmodel, and experimental evaluation of themethods presented.
We show that hedgeclassification is feasible using weaklysupervised ML, and point toward avenuesfor future research.1 IntroductionThe automatic processing of scientific papers usingNLP and machine learning (ML) techniques is anincreasingly important aspect of technical informat-ics.
In the quest for a deeper machine-driven ?under-standing?
of the mass of scientific literature, a fre-quently occuring linguistic phenomenon that mustbe accounted for is the use of hedging to denotepropositions of a speculative nature.
Consider thefollowing:1.
Our results prove that XfK89 inhibits Felin-9.2.
Our results suggest that XfK89 might inhibit Felin-9.The second example contains a hedge, signaledby the use of suggest and might, which rendersthe proposition inhibit(XfK89?Felin-9) speculative.Such analysis would be useful in various applica-tions; for instance, consider a system designed toidentify and extract interactions between genetic en-tities in the biomedical domain.
Case 1 above pro-vides clear textual evidence of such an interactionand justifies extraction of inhibit(XfK89?Felin-9),whereas case 2 provides only weak evidence forsuch an interaction.Hedging occurs across the entire spectrum of sci-entific literature, though it is particularly common inthe experimental natural sciences.
In this study weconsider the problem of learning to automaticallyclassify sentences containing instances of hedging,given only a very limited amount of annotator-labelled ?seed?
data.
This falls within the weakly su-pervised ML framework, for which a range of tech-niques have been previously explored.
The contri-butions of our work are as follows:1.
We provide a clear description of the prob-lem of hedge classification and offer an im-proved and expanded set of annotation guide-lines, which as we demonstrate experimentallyare sufficient to induce a high level of agree-ment between independent annotators.2.
We discuss the specificities of hedge classifica-tion as a weakly supervised ML task.3.
We derive a probabilistic weakly supervisedlearning model and use it to motivate our ap-proach.4.
We analyze our learning model experimentallyand report promising results for the task on anew publicly-available dataset.12 Related Work2.1 Hedge ClassificationWhile there is a certain amount of literature withinthe linguistics community on the use of hedging in1available from www.cl.cam.ac.uk/?bwm23/992scientific text, eg.
(Hyland, 1994), there is little ofdirect relevance to the task of classifying speculativelanguage from an NLP/ML perspective.The most clearly relevant study is Light et al(2004) where the focus is on introducing the prob-lem, exploring annotation issues and outlining po-tential applications rather than on the specificitiesof the ML approach, though they do present someresults using a manually crafted substring match-ing classifier and a supervised SVM on a collectionof Medline abstracts.
We will draw on this workthroughout our presentation of the task.Hedging is sometimes classed under the umbrellaconcept of subjectivity, which covers a variety of lin-guistic phenomena used to express differing formsof authorial opinion (Wiebe et al, 2004).
Riloff et al(2003) explore bootstrapping techniques to identifysubjective nouns and subsequently classify subjec-tive vs. objective sentences in newswire text.
Theirwork bears some relation to ours; however, our do-mains of interest differ (newswire vs. scientific text)and they do not address the problem of hedge clas-sification directly.2.2 Weakly Supervised LearningRecent years have witnessed a significant growthof research into weakly supervised ML techniquesfor NLP applications.
Different approaches are of-ten characterised as either multi- or single-view,where the former generate multiple redundant (orsemi-redundant) ?views?
of a data sample and per-form mutual bootstrapping.
This idea was for-malised by Blum and Mitchell (1998) in theirpresentation of co-training.
Co-training has alsobeen used for named entity recognition (NER)(Collins and Singer, 1999), coreference resolution(Ng and Cardie, 2003), text categorization (Nigamand Ghani, 2000) and improving gene name data(Wellner, 2005).Conversely, single-view learning models operatewithout an explicit partition of the feature space.Perhaps the most well known of such approachesis expectation maximization (EM), used by Nigamet al (2000) for text categorization and by Ng andCardie (2003) in combination with a meta-level fea-ture selection procedure.
Self-training is an alterna-tive single-view algorithm in which a labelled poolis incrementally enlarged with unlabelled samplesfor which the learner is most confident.
Early workby Yarowsky (1995) falls within this framework.Banko and Brill (2001) use ?bagging?
and agree-ment to measure confidence on unlabelled samples,and more recently McClosky et al (2006) use self-training for improving parse reranking.Other relevant recent work includes (Zhang,2004), in which random feature projection and acommittee of SVM classifiers is used in a hybridco/self-training strategy for weakly supervised re-lation classification and (Chen et al, 2006) wherea graph based algorithm called label propagation isemployed to perform weakly supervised relation ex-traction.3 The Hedge Classification TaskGiven a collection of sentences, S, the task is tolabel each sentence as either speculative or non-speculative (spec or nspec henceforth).
Specifically,S is to be partitioned into two disjoint sets, one rep-resenting sentences that contain some form of hedg-ing, and the other representing those that do not.To further elucidate the nature of the task and im-prove annotation consistency, we have developed anew set of guidelines, building on the work of Lightet al (2004).
As noted by Light et al, speculativeassertions are to be identified on the basis of judge-ments about the author?s intended meaning, ratherthan on the presence of certain designated hedgeterms.We begin with the hedge definition given byLight et al (item 1) and introduce a set of furtherguidelines to help elucidate various ?grey areas?
andtighten the task specification.
These were developedafter initial annotation by the authors, and throughdiscussion with colleagues.
Further examples aregiven in online Appendix A2.The following are considered hedge instances:1.
An assertion relating to a result that does notnecessarily follow from work presented, butcould be extrapolated from it (Light et al).2.
Relay of hedge made in previous work.Dl and Ser have been proposed to act redundantly in thesensory bristle lineage.3.
Statement of knowledge paucity.2available from www.cl.cam.ac.uk/?bwm23/993How endocytosis of Dl leads to the activation of N re-mains to be elucidated.4.
Speculative question.A second important question is whether the roX geneshave the same, overlapping or complementing functions.5.
Statement of speculative hypothesis.To test whether the reported sea urchin sequences repre-sent a true RAG1-like match, we repeated the BLASTPsearch against all GenBank proteins.6.
Anaphoric hedge reference.This hypothesis is supported by our finding that both pu-pariation rate and survival are affected by EL9.The following are not considered hedge instances:1.
Indication of experimentally observed non-universal behaviour.proteins with single BIR domains can also have functionsin cell cycle regulation and cytokinesis.2.
Confident assertion based on external work.Two distinct E3 ubiquitin ligases have been shown to reg-ulate Dl signaling in Drosophila melanogaster.3.
Statement of existence of proposed alterna-tives.Different models have been proposed to explain how en-docytosis of the ligand, which removes the ligand from thecell surface, results in N receptor activation.4.
Experimentally-supported confirmation of pre-vious speculation.Here we show that the hemocytes are the main regulatorof adenosine in the Drosophila larva, as was speculatedpreviously for mammals.5.
Negation of previous hedge.Although the adgf-a mutation leads to larval or pupaldeath, we have shown that this is not due to the adenosineor deoxyadenosine simply blocking cellular proliferationor survival, as the experiments in vitro would suggest.4 DataWe used an archive of 5579 full-text papers from thefunctional genomics literature relating to Drosophilamelanogaster (the fruit fly).
The papers were con-verted to XML and linguistically processed usingthe RASP toolkit3.
We annotated six of the pa-pers to form a test set with a total of 380 spec sen-tences and 1157 nspec sentences, and randomly se-lected 300,000 sentences from the remaining papersas training data for the weakly supervised learner.
Toensure selection of complete sentences rather than3www.informatics.susx.ac.uk/research/nlp/raspFrel1 ?Original 0.8293 0.9336Corrected 0.9652 0.9848Table 1: Agreement Scoresheadings, captions etc., unlabelled samples werechosen under the constraints that they must be atleast 10 words in length and contain a main verb.5 Annotation and AgreementTwo separate annotators were commissioned to la-bel the sentences in the test set, firstly one of theauthors and secondly a domain expert with no priorinput into the guideline development process.
Thetwo annotators labelled the data independently us-ing the guidelines outlined in section 3.
RelativeF1 (Frel1 ) and Cohen?s Kappa (?)
were then used toquantify the level of agreement.
For brevity we referthe reader to (Artstein and Poesio, 2005) and (Hripc-sak and Rothschild, 2004) for formulation and dis-cussion of ?
and Frel1 respectively.The two metrics are based on different assump-tions about the nature of the annotation task.
Frel1is founded on the premise that the task is to recog-nise and label spec sentences from within a back-ground population, and does not explicitly modelagreement on nspec instances.
It ranges from 0 (noagreement) to 1 (no disagreement).
Conversely, ?gives explicit credit for agreement on both spec andnspec instances.
The observed agreement is thencorrected for ?chance agreement?, yielding a metricthat ranges between ?1 and 1.
Given our defini-tion of hedge classification and assessing the mannerin which the annotation was carried out, we suggestthat the founding assumption of Frel1 fits the natureof the task better than that of ?.Following initial agreement calculation, the in-stances of disagreement were examined.
It turnedout that the large majority of cases of disagreementwere due to negligence on behalf of one or other ofthe annotators (i.e.
cases of clear hedging that weremissed), and that the cases of genuine disagreementwere actually quite rare.
New labelings were thencreated with the negligent disagreements corrected,resulting in significantly higher agreement scores.Values for the original and negligence-corrected la-994belings are reported in Table 1.Annotator conferral violates the fundamental as-sumption of annotator independence, and so the lat-ter agreement scores do not represent the true levelof agreement; however, it is reasonable to concludethat the actual agreement is approximately lowerbounded by the initial values and upper bounded bythe latter values.
In fact even the lower bound iswell within the range usually accepted as represent-ing ?good?
agreement, and thus we are confident inaccepting human labeling as a gold-standard for thehedge classification task.
For our experiments, weuse the labeling of the genetics expert, corrected fornegligent instances.6 DiscussionIn this study we use single terms as features, basedon the intuition that many hedge cues are singleterms (suggest, likely etc.)
and due to the successof ?bag of words?
representations in many classifica-tion tasks to date.
Investigating more complex sam-ple representation strategies is an avenue for futureresearch.There are a number of factors that make our for-mulation of hedge classification both interesting andchallenging from a weakly supervised learning per-spective.
Firstly, due to the relative sparsity of hedgecues, most samples contain large numbers of irrele-vant features.
This is in contrast to much previouswork on weakly supervised learning, where for in-stance in the case of text categorization (Blum andMitchell, 1998; Nigam et al, 2000) almost all con-tent terms are to some degree relevant, and irrel-evant terms can often be filtered out (e.g.
stop-word removal).
In the same vein, for the case ofentity/relation extraction and classification (Collinsand Singer, 1999; Zhang, 2004; Chen et al, 2006)the context of the entity or entities in considerationprovides a highly relevant feature space.Another interesting factor in our formulation ofhedge classification is that the nspec class is definedon the basis of the absence of hedge cues, render-ing it hard to model directly.
This characteristicis also problematic in terms of selecting a reliableset of nspec seed sentences, as by definition at thebeginning of the learning cycle the learner has lit-tle knowledge about what a hedge looks like.
Thisproblem is addressed in section 10.3.In this study we develop a learning model basedaround the concept of iteratively predicting labelsfor unlabelled training samples, the basic paradigmfor both co-training and self-training.
However wegeneralise by framing the task in terms of the acqui-sition of labelled training data, from which a super-vised classifier can subsequently be learned.7 A Probabilistic Model for Training DataAcquisitionIn this section, we derive a simple probabilisticmodel for acquiring training data for a given learn-ing task, and use it to motivate our approach toweakly supervised hedge classification.Given:?
sample space X?
set of target concept classes Y = {y1 .
.
.
yN}?
target function Y : X ?
Y?
set of seed samples for each class S1 .
.
.SNwhere Si ?
X and ?x ?
Si[Y (x)=yi]?
set of unlabelled samples U = {x1 .
.
.xK}Aim: Infer a set of training samples Ti for each con-cept class yi such that ?x ?
Ti[Y (x) = yi]Now, it follows that ?x?Ti[Y (x)=yi] is satisfiedin the case that ?x?Ti[P (yi|x)=1], which leads toa model in which Ti is initialised to Si and then iter-atively augmented with the unlabelled sample(s) forwhich the posterior probability of class membershipis maximal.
Formally:At each iteration:Ti ?
xj(?
U)where j = argmaxj[P (yi|xj)] (1)Expansion with Bayes?
Rule yields:argmaxj[P (yi|xj)]= argmaxj[P (xj |yi) ?
P (yi)P (xj)](2)An interesting observation is the importance ofthe sample prior P (xj) in the denominator, of-ten ignored for classification purposes because ofits invariance to class.
We can expand further by995marginalising over the classes in the denominator inexpression 2, yielding:argmaxj[P (xj |yi) ?
P (yi)?Nn=1 P (yn)P (xj |yn)](3)so we are left with the class priors and class-conditional likelihoods, which can usually be esti-mated directly from the data, at least under limiteddependence assumptions.
The class priors can beestimated based on the relative distribution sizes de-rived from the current training sets:P (yi) =|Ti|?k |Tk|(4)where |S| is the number of samples in training set S.If we assume feature independence, which as wewill see for our task is not as gross an approximationas it may at first seem, we can simplify the class-conditional likelihood in the well known manner:P (xj |yi) =?kP (xjk|yi) (5)and then estimate the likelihood for each feature:P (xk|yi) =?P (yi) + f(xk, Ti)?P (yi) + |Ti|(6)where f(x,S) is the number of samples in trainingset S in which feature x is present, and ?
is a uni-versal smoothing constant, scaled by the class prior.This scaling is motivated by the principle that with-out knowledge of the true distribution of a partic-ular feature it makes sense to include knowledgeof the class distribution in the smoothing mecha-nism.
Smoothing is particularly important in theearly stages of the learning process when the amountof training data is severely limited resulting in unre-liable frequency estimates.8 Hedge ClassificationWe will now consider how to apply this learningmodel to the hedge classification task.
As discussedearlier, the speculative/non-speculative distinctionhinges on the presence or absence of a few hedgecues within the sentence.
Working on this premise,all features are ranked according to their probabilityof ?hedge cue-ness?
:P (spec|xk) =P (xk|spec) ?
P (spec)?Nn=1 P (yn)P (xk|yn)(7)which can be computed directly using (4) and (6).Themmost probable features are then selected fromeach sentence to compute (5) and the rest are ig-nored.
This has the dual benefit of removing irrele-vant features and also reducing dependence betweenfeatures, as the selected features will often be non-local and thus not too tightly correlated.Note that this idea differs from traditional featureselection in two important ways:1.
Only features indicative of the spec class areretained, or to put it another way, nspec classmembership is inferred from the absence ofstrong spec features.2.
Feature selection in this context is not a prepro-cessing step; i.e.
there is no re-estimation afterselection.
This has the potentially detrimentalside effect of skewing the posterior estimatesin favour of the spec class, but is admissiblefor the purposes of ranking and classificationby posterior thresholding (see next section).9 ClassificationThe weakly supervised learner returns a labelleddata set for each class, from which a classifier canbe trained.
We can easily derive a classifier usingthe estimates from our learning model by:xj ?
spec if P (spec|xj) > ?
(8)where ?
is an arbitrary threshold used to control theprecision/recall balance.
For comparison purposes,we also use Joachims?
SVMlight (Joachims, 1999).10 Experimental Evaluation10.1 MethodTo examine the practical efficacy of the learning andclassification models we have presented, we use thefollowing experimental method:1.
Generate seed training data: Sspec and Snspec2.
Initialise: Tspec?Sspec and Tnspec?Snspec3.
Iterate:?
Order U by P (spec|xj) (expression 3)?
Tspec ?
most probable batch?
Tnspec ?
least probable batch?
Train classifier using Tspec and Tnspec996Rank ?
= 0 ?
= 1 ?
= 5 ?
= 100 ?
= 5001 interactswith suggest suggest suggest suggest2 TAFb likely likely likely likely3 sexta may may may may4 CRYs might might These These5 DsRed seems seems results results6 Cell-Nonautonomous suggests Taken might that7 arva probably suggests observations be8 inter-homologue suggesting probably Taken data9 Mohanty possibly Together findings it10 meld suggested suggesting Our Our11 aDNA Taken possibly seems observations12 Deer unlikely suggested together role13 Borel Together findings Together most14 substripe physiology observations role these15 Failing modulated Given that togetherTable 2: Features ranked by P (spec|xk) for varying ??
Compute spec recall/precision BEP(break-even point) on the test dataThe batch size for each iteration is set to 0.001?
|U|.After each learning iteration, we compute the preci-sion/recall BEP for the spec class using both clas-sifiers trained on the current labelled data.
We useBEP because it helps to mitigate against misleadingresults due to discrepancies in classification thresh-old placement.
Disadvantageously, BEP does notmeasure a classifier?s performance across the wholeof the recall/precision spectrum (as can be obtained,for instance, from receiver-operating characteristic(ROC) curves), but for our purposes it provides aclear, abstracted overview of a classifier?s accuracygiven a particular training set.10.2 Parameter SettingThe training and classification models we have pre-sented require the setting of two parameters: thesmoothing parameter ?
and the number of featuresper sample m. Analysis of the effect of varying ?on feature ranking reveals that when ?
= 0, low fre-quency terms with spurious class correlation dom-inate and as ?
increases, high frequency terms be-come increasingly dominant, eventually smoothingaway genuine low-to-mid frequency correlations.This effect is illustrated in Table 2, and from thisanalysis we chose ?
= 5 as an appropriate level ofsmoothing.
We use m=5 based on the intuition thatfive is a rough upper bound on the number of hedgecue features likely to occur in any one sentence.We use the linear kernel for SVMlight with thedefault setting for the regularization parameter C.We construct binary valued, L2-normalised (unitlength) input vectors to represent each sentence,as this resulted in better performance than usingfrequency-based weights and concords with ourpresence/absence feature estimates.10.3 Seed GenerationThe learning model we have presented requires aset of seeds for each class.
To generate seeds forthe spec class, we extracted all sentences from Ucontaining either (or both) of the terms suggest orlikely, as these are very good (though not perfect)hedge cues, yielding 6423 spec seeds.
Generatingseeds for nspec is much more difficult, as integrityrequires the absence of hedge cues, and this cannotbe done automatically.
Thus, we used the followingprocedure to obtain a set of nspec seeds:1.
Create initial Snspec by sampling randomlyfrom U .2.
Manually remove more ?obvious?
speculativesentences using pattern matching3.
Iterate:?
Order Snspec by P (spec|xj) using esti-mates from Sspec and current Snspec?
Examine most probable sentences and re-move speculative instancesWe started with 8830 sentences and after a couple ofhours work reduced this down to a (still potentiallynoisy) nspec seed set of 7541 sentences.9970.580.60.620.640.660.680.70.720.740.760.780.80  20  40  60  80  100  120  140BEPIterationProb (Prob)Prob (SVM)SVM (Prob)SVM (SVM)BaselineProb (Prob) denotes our probabilistic learning model and classifier (?9)Prob (SVM) denotes probabilistic learning model with SVM classifierSVM (Prob) denotes committee-based model (?10.4) with probabilistic classifierSVM (SVM) denotes committee-based model with SVM classifierBaseline denotes substring matching classifier of (Light et al, 2004)Figure 1: Learning curves10.4 BaselinesAs a baseline classifier we use the substring match-ing technique of (Light et al, 2004), which labelsa sentence as spec if it contains one or more of thefollowing: suggest, potential, likely, may, at least,in part, possibl, further investigation, unlikely, pu-tative, insights, point toward, promise and propose.To provide a comparison for our learning model,we implement a more traditional self-training pro-cedure in which at each iteration a committee of fiveSVMs is trained on randomly generated overlappingsubsets of the training data and their cumulative con-fidence is used to select items for augmenting thelabelled training data.
For similar work see (Bankoand Brill, 2001; Zhang, 2004).10.5 ResultsFigure 1 plots accuracy as a function of the train-ing iteration.
After 150 iterations, all of the weaklysupervised learning models are significantly moreaccurate than the baseline according to a binomialsign test (p < 0.01), though there is clearly stillmuch room for improvement.
The baseline classi-fier achieves a BEP of 0.60 while both classifiersusing our learning model reach approximately 0.76BEP with little to tell between them.
Interestingly,the combination of the SVM committee-based learn-ing model with our classifier (denoted by ?SVM(Prob)?
), performs competitively with both of the ap-proaches that use our probabilistic learning modeland significantly better than the SVM committee-based learning model with an SVM classifier, ?SVM(SVM)?, according to a binomial sign test (p<0.01)after 150 iterations.
These results suggest that per-formance may be enhanced when the learning andclassification tasks are carried out by different mod-els.
This is an interesting possibility, which we in-tend to explore further.An important issue in incremental learning sce-narios is identification of the optimum stoppingpoint.
Various methods have been investigated to ad-dress this problem, such as ?counter-training?
(Yan-garber, 2003) and committee agreement (Zhang,2004); how such ideas can be adapted for this task isone of many avenues for future research.10.6 Error AnalysisSome errors are due to the variety of hedge forms.For example, the learning models were unsuccess-ful in identifying assertive statements of knowledgepaucity, eg: There is no clear evidence for cy-tochrome c release during apoptosis in C elegansor Drosophila.
Whether it is possible to learn suchexamples without additional seed information is anopen question.
This example also highlights the po-tential benefit of an enriched sample representation,in this case one which accounts for the negation ofthe phrase ?clear evidence?
which otherwise mightsuggest a strongly non-speculative assertion.In many cases hedge classification is challengingeven for a human annotator.
For instance, distin-guishing between a speculative assertion and onerelating to a pattern of observed non-universal be-haviour is often difficult.
The following examplewas chosen by the learner as a spec sentence on the150th training iteration: Each component consists ofa set of subcomponents that can be localized withina larger distributed neural system.
The sentencedoes not, in fact, contain a hedge but rather a state-ment of observed non-universal behaviour.
How-ever, an almost identical variant with ?could?
insteadof ?can?
would be a strong speculative candidate.This highlights the similarity between many hedgeand non-hedge instances, which makes such caseshard to learn in a weakly supervised manner.99811 Conclusions and Future WorkWe have shown that weakly supervised ML is ap-plicable to the problem of hedge classification andthat a reasonable level of accuracy can be achieved.The work presented here has application in the wideracademic community; in fact a key motivation inthis study is to incorporate hedge classification intoan interactive system for aiding curators in the con-struction and population of gene databases.
We havepresented our initial results on the task using a sim-ple probabilistic model in the hope that this willencourage others to investigate alternative learningmodels and pursue new techniques for improving ac-curacy.
Our next aim is to explore possibilities ofintroducing linguistically-motivated knowledge intothe sample representation to help the learner identifykey hedge-related sentential components, and also toconsider hedge classification at the granularity of as-sertions rather than text sentences.AcknowledgementsThis work was partially supported by the FlySlipproject, BBSRC Grant BBS/B/16291, and we thankNikiforos Karamanis and Ruth Seal for thorough an-notation and helpful discussion.
The first author issupported by an University of Cambridge Millen-nium Scholarship.ReferencesRon Artstein and Massimo Poesio.
2005.
Kappa3 = al-pha (or beta).
Technical report, University of EssexDepartment of Computer Science.Michele Banko and Eric Brill.
2001.
Scaling to very verylarge corpora for natural language disambiguation.
InMeeting of the Association for Computational Linguis-tics, pages 26?33.Avrim Blum and Tom Mitchell.
1998.
Combining la-belled and unlabelled data with co-training.
In Pro-ceedings of COLT?
98, pages 92?100, New York, NY,USA.
ACM Press.Jinxiu Chen, Donghong Ji, Chew L. Tan, and ZhengyuNiu.
2006.
Relation extraction using label propaga-tion based semi-supervised learning.
In Proceedingsof ACL?06, pages 129?136.M.
Collins and Y.
Singer.
1999.
Unsupervised mod-els for named entity classification.
In Proceedings ofthe Joint SIGDAT Conference on Empirical Methodsin NLP and Very Large Corpora.George Hripcsak and Adam Rothschild.
2004.
Agree-ment, the f-measure, and reliability in information re-trieval.
J Am Med Inform Assoc., 12(3):296?298.K.
Hyland.
1994.
Hedging in academic writing and eaptextbooks.
English for Specific Purposes, 13:239?256.Thorsten Joachims.
1999.
Making large-scale sup-port vector machine learning practical.
In A. SmolaB.
Scho?lkopf, C. Burges, editor, Advances in KernelMethods: Support Vector Machines.
MIT Press, Cam-bridge, MA.M.
Light, X.Y.
Qiu, and P. Srinivasan.
2004.
The lan-guage of bioscience: Facts, speculations, and state-ments in between.
In Proceedings of BioLink 2004Workshop on Linking Biological Literature, Ontolo-gies and Databases: Tools for Users, Boston, May2004.David McClosky, Eugene Charniak, and Mark Johnson.2006.
Effective self-training for parsing.
In HLT-NAACL.Vincent Ng and Claire Cardie.
2003.
Weakly supervisednatural language learning without redundant views.
InProceedings of NAACL ?03, pages 94?101, Morris-town, NJ, USA.K.
Nigam and R. Ghani.
2000.
Understanding the be-havior of co-training.
In Proceedings of KDD-2000Workshop on Text Mining.Kamal Nigam, Andrew K. McCallum, Sebastian Thrun,and Tom M. Mitchell.
2000.
Text classification fromlabeled and unlabeled documents using EM.
MachineLearning, 39(2/3):103?134.Ellen Riloff, Janyce Wiebe, and Theresa Wilson.
2003.Learning subjective nouns using extraction patternbootstrapping.
In Seventh Conference on Natural Lan-guage Learning (CoNLL-03).
ACL SIGNLL., pages25?32.Ben Wellner.
2005.
Weakly supervised learning meth-ods for improving the quality of gene name normal-ization data.
In Proceedings of the ACL-ISMB Work-shop on Linking Biological Literature, Ontologies andDatabases, pages 1?8, Detroit, June.
Association forComputational Linguistics.Janyce Wiebe, Theresa Wilson, Rebecca Bruce, MatthewBell, and Melanie Martin.
2004.
Learning subjectivelanguage.
Comput.
Linguist., 30(3):277?308.Roman Yangarber.
2003.
Counter-training in discoveryof semantic patterns.
In Proceedings of ACL?03, pages343?350, Morristown, NJ, USA.David Yarowsky.
1995.
Unsupervised word sense dis-ambiguation rivaling supervised methods.
In Pro-ceedings of ACL?95, pages 189?196, Morristown, NJ,USA.
ACL.Zhu Zhang.
2004.
Weakly-supervised relation clas-sification for information extraction.
In CIKM ?04:Proceedings of the thirteenth ACM international con-ference on Information and knowledge management,pages 581?588, New York, NY, USA.
ACM Press.999
