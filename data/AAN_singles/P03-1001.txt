Offline Strategies for Online Question Answering:Answering Questions Before They Are AskedMichael Fleischman, Eduard Hovy,Abdessamad EchihabiUSC Information Sciences Institute4676 Admiralty WayMarina del Rey, CA 90292-6695{fleisch, hovy, echihabi} @ISI.eduAbstractRecent work in Question Answering hasfocused on web-based systems thatextract answers using simple lexico-syntactic patterns.
We present analternative strategy in which patterns areused to extract highly precise relationalinformation offline, creating a datarepository that is used to efficientlyanswer questions.
We evaluate ourstrategy on a challenging subset ofquestions, i.e.
?Who is ??
questions,against a state of the art web-basedQuestion Answering system.
Resultsindicate that the extracted relationsanswer 25% more questions correctly anddo so three orders of magnitude fasterthan the state of the art system.1 IntroductionMany of the recent advances in QuestionAnswering have followed from the insight thatsystems can benefit by exploiting the redundancyof information in large corpora.
Brill et al (2001)describe using the vast amount of data available onthe World Wide Web to achieve impressiveperformance with relatively simple techniques.While the Web is a powerful resource, itsusefulness in Question Answering is not withoutlimits.The Web, while nearly infinite in content, isnot a complete repository of useful information.Most newspaper texts, for example, do not remainaccessible on the Web for more than a few weeks.Further, while Information Retrieval techniques arerelatively successful at managing the vast quantityof text available on the Web, the exactnessrequired of Question Answering systems makesthem too slow and impractical for ordinary users.In order to combat these inadequacies, wepropose a strategy in which information isextracted automatically from electronic textsoffline, and stored for quick and easy access.
Weborrow techniques from Text Mining in order toextract semantic relations (e.g., concept-instancerelations) between lexical items.
We enhancethese techniques by increasing the yield andprecision of the relations that we extract.Our strategy is to collect a large sample ofnewspaper text (15GB) and use multiple part ofspeech patterns to extract the semantic relations.We then filter out the noise from these extractedrelations using a machine-learned classifier.
Thisprocess generates a high precision repository ofinformation that can be accessed quickly andeasily.We test the feasibility of this strategy on onesemantic relation and a challenging subset ofquestions, i.e., ?Who is ??
questions, in whicheither a concept is presented and an instance isrequested (e.g., ?Who is the mayor of Boston??
),or an instance is presented and a concept isrequested (e.g., ?Who is Jennifer Capriati??).
Bychoosing this subset of questions we are able tofocus only on answers given by concept-instancerelationships.
While this paper examines only thistype of relation, the techniques we propose areeasily extensible to other question types.Evaluations are conducted using a set of ?Whois ??
questions collected over the period of a fewmonths from the commercial question-basedsearch engine www.askJeeves.com.
We extractapproximately 2,000,000 concept-instancerelations from newspaper text using syntacticpatterns and machine-learned filters (e.g.,?president Bill Clinton?
and ?Bill Clinton,president of the USA,?).
We then compareanswers based on these relations to answers givenby TextMap (Hermjakob et al, 2002), a state of theart web-based question answering system.
Finally,we discuss the results of this evaluation and theimplications and limitations of our strategy.3.1233.2Related WorkA great deal of work has examined the problem ofextracting semantic relations from unstructuredtext.
Hearst (1992) examined extracting hyponymdata by taking advantage of lexical patterns in text.Using patterns involving the phrase ?such as?, shereports finding only 46 relations in 20M of NewYork Times text.
Berland and Charniak (1999)extract ?part-of?
relations between lexical items intext, achieving only 55% accuracy with theirmethod.
Finally, Mann (2002) describes a methodfor extracting instances from text that takesadvantage of part of speech patterns involvingproper nouns.
Mann reports extracting 200,000concept-instance pairs from 1GB of AssociatedPress text, only 60% of which were found to belegitimate descriptions.These studies indicate two distinct problemsassociated with using patterns to extract semanticinformation from text.
First, the patterns yieldonly a small amount of the information that may bepresent in a text (the Recall problem).
Second,only a small fraction of the information that thepatterns yield is reliable (the Precision problem).Relation ExtractionOur approach follows closely from Mann (2002).However, we extend this work by directlyaddressing the two problems stated above.
Inorder to address the Recall problem, we extend thelist of patterns used for extraction to takeadvantage of appositions.
Further, followingBanko and Brill (2001), we increase our yield byincreasing the amount of data used by an order ofmagnitude over previously published work.Finally, in order to address the Precision problem,we use machine learning techniques to filter theoutput of the part of speech patterns, thus purifyingthe extracted instances.Data Collection and PreprocessingApproximately 15GB of newspaper text wascollected from: the TREC 9 corpus (~3.5GB), theTREC 2002 corpus (~3.5GB), Yahoo!
News(.5GB), the AP newswire (~2GB), the Los AngelesTimes (~.5GB), the New York Times (~2GB),Reuters (~.8GB), the Wall Street Journal(~1.2GB), and various online news websites(~.7GB).
The text was cleaned of HTML (whennecessary), word and sentence segmented, and partof speech tagged using Brill?s tagger (Brill, 1994).Extraction PatternsPart of speech patterns were generated to takeadvantage of two syntactic constructions that oftenindicate concept-instance relationships: commonnoun/proper noun constructions (CN/PN) andappositions (APOS).
Mann (2002) notes thatconcept-instance relationships are often expressedby a syntactic pattern in which a proper nounfollows immediately after a common noun.
Suchpatterns (e.g.
?president George Bush?)
are veryproductive and occur 40 times more often thanpatterns employed by Hearst (1992).
Table 1shows the regular expression used to extract suchpatterns along with examples of extracted patterns.${NNP}*${VBG}*${JJ}*${NN}+${NNP}+trainer/NN Victor/NNP Valle/NNPABC/NN spokesman/NN Tom/NNP Mackin/NNPofficial/NN Radio/NNP Vilnius/NNPGerman/NNP expert/NN Rriedhart/NNPDumez/NN Investment/NNPTable 1.
The regular expression used to extract CN/PNpatterns (common noun followed by proper noun).Examples of extracted text are presented below.
Text inbold indicates that the example is judged illegitimate.${NNP}+\s*,\/,\s*${DT}*${JJ}*${NN}+(?
:of\/IN)*\s*${NNP}*${NN}*${IN}*${DT}*${NNP}*${NN}*${IN}*${NN}*${NNP}*,\/,Stevens/NNP  ,/, president/NN of/IN the/DT firm/NN  ,/,Elliott/NNP Hirst/NNP  ,/, md/NN of/IN Oldham/NNP Signs/NNP  ,/,George/NNP McPeck/NNP,/, an/DT engineer/NN from/IN Peru/NN,/,Marc/NNP Jonson/NNP,/, police/NN chief/NN of/IN Chamblee/NN ,/,David/NNP Werner/NNP ,/, a/DT real/JJ estate/NN investor/NN ,/,Table 2.
The regular expression used to extract APOSpatterns (syntactic appositions).
Examples of extractedtext are presented below.
Text in bold indicates that theexample is judged illegitimate.In addition to the CN/PN pattern of Mann(2002), we extracted syntactic appositions (APOS).This pattern detects phrases such as ?Bill Gates,chairman of Microsoft,?.
Table 2 shows theregular expression used to extract appositions andexamples of extracted patterns.
These regularexpressions are not meant to be exhaustive of allpossible varieties of patterns construed as CN/PNor APOS.
They are ?quick and dirty?implementations meant to extract a largeproportion of the patterns in a text, acknowledgingthat some bad examples may leak through.3.3 FilteringThe concept-instance pairs extracted using theabove patterns are very noisy.
In samples ofapproximately 5000 pairs, 79% of the APOSextracted relations were legitimate, and only 45%of the CN/PN extracted relations were legitimate.This noise is primarily due to overgeneralization ofthe patterns (e.g., ?Berlin Wall, the end of the ColdWar,?)
and to errors in the part of speech tagger(e.g., ?Winnebago/CN Industries/PN?).
Further,some extracted relations were considered eitherincomplete (e.g., ?political commentator Mr.Bruce?)
or too general (e.g., ?meeting site BourbonStreet?)
to be useful.
For the purposes of learninga filter, these patterns were treated as illegitimate.In order to filter out these noisy concept-instance pairs, 5000 outputs from each patternwere hand tagged as either legitimate orillegitimate, and used to train a binary classifier.The annotated examples were split into a trainingset (4000 examples), a validation set (500examples); and a held out test set (500 examples).The WEKA machine learning package (Witten andFrank, 1999) was used to test the performance ofvarious learning and meta-learning algorithms,including Na?ve Bayes, Decision Tree, DecisionList, Support Vector Machines, Boosting, andBagging.Table 4 shows the list of features used todescribe each concept-instance pair for training theCN/PN filter.
Features are split between those thatdeal with the entire pattern, only the concept, onlythe instance, and the pattern?s overall orthography.The most powerful of these features examines anOntology in order to exploit semantic informationabout the concept?s head.
This semanticinformation is found by examining the super-concept relations of the concept head in the110,000 node Omega Ontology (Hovy et al, inprep.
).FeatureTypePatternFeaturesBinary ${JJ}+${NN}+${NNP}+Binary ${NNP}+${JJ}+${NN}+${NNP}+Binary ${NNP}+${NN}+${NNP}+Binary ${NNP}+${VBG}+${JJ}+${NN}+${NNP}+Binary ${NNP}+${VBG}+${NN}+${NNP}+Binary ${NN}+${NNP}+Binary ${VBG}+${JJ}+${NN}+${NNP}+Binary ${VBG}+${NN}+${NNP}+Concept FeaturesBinary Concept head ends in "er"Binary Concept head ends in "or"Binary Concept head ends in "ess"Binary Concept head ends in "ist"Binary Concept head ends in "man"Binary Concept head ends in "person"Binary Concept head ends in "ant"Binary Concept head ends in "ial"Binary Concept head ends in "ate"Binary Concept head ends in "ary"Binary Concept head ends in "iot"Binary Concept head ends in "ing"Binary Concept head is-a occupationBinary Concept head is-a personBinary Concept head is-a organizationBinary Concept head is-a companyBinary Concept includes digitsBinary Concept has non-wordBinary Concept head in general listInteger Frequency of concept head in CN/PNInteger Frequency of concept head in APOSInstance FeaturesInteger Number of lexical items in instanceBinary Instance contains honorificBinary Instance contains common nameBinary Instance ends in honorificBinary Instance ends in common nameBinary Instance ends in determinerCase FeaturesInteger Instance: # of lexical items all CapsInteger Instance: # of lexical items start w/ CapsBinary Instance: All lexical items start w/ CapsBinary Instance: All lexical items all CapsInteger Concept: # of lexical items all CapsInteger Concept: # of lexical items start w/ CapsBinary Concept: All lexical items start w/ CapsBinary Concept: All lexical items all CapsInteger Total # of lexical items all CapsInteger Total # of lexical items start w/ CapsTable 4.
Features used to train CN/PN pattern filter.Pattern features address aspects of the entire pattern,Concept features look only at the concept, Instancefeatures examine elements of the instance, and Casefeatures deal only with the orthography of the lexicalitems.Figure 1.
Performance of machine learning algorithmson a validation set of 500 examples extracted using theCN/PN pattern.
Algorithms are compared to a baselinein which only concepts that inherit from ?Human?
or?Occupation?
in Omega pass through the filter.44.1Extraction ResultsMachine Learning ResultsFigure 1 shows the performance of differentmachine learning algorithms, trained on 4000extracted CN/PN concept-instance pairs, and testedon a validation set of 500.
Na?ve Bayes, SupportVector Machine, Decision List and Decision Treealgorithms were all evaluated and the DecisionTree algorithm (which scored highest of all thealgorithms) was further tested with Boosting andBagging meta-learning techniques.
The algorithmsare compared to a baseline filter that acceptsconcept-instance pairs if and only if the concepthead is a descendent of either the concept?Human?
or the concept ?Occupation?
in Omega.It is clear from the figure that the Decision Treealgorithm plus Bagging gives the highest precisionand overall F-score.
All subsequent experimentsare run using this technique.1Since high precision is the most importantcriterion for the filter, we also examine theperformance of the classifier as it is applied with athreshold.
Thus, a probability cutoff is set suchthat only positive classifications that exceed thiscutoff are actually classified as legitimate.
Figure2 shows a plot of the precision/recall tradeoff asthis threshold is changed.
As the threshold israised, precision increases while recall decreases.Based on this graph we choose to set the thresholdat 0.9.Learning Algorithm Performance0.50.60.70.80.91Baseline Na?ve Bayes SVM DecisionListDecisionTreeDT +BoostingDT +BaggingRecall Precision F-Score4.21 Precision and Recall here refer only to the output of theextraction patterns.
Thus, 100% recall indicates that alllegitimate concept-instance pairs that were extracted using thepatterns, were classified as legitimate by the filter.
It does notindicate that all concept-instance information in the text wasextracted.
Precision is to be understood similarly.Applying the Decision Tree algorithm withBagging, using the pre-determined threshold, to theheld out test set of 500 examples extracted with theCN/PN pattern yields a precision of .95 and arecall of .718.
Under these same conditions, butapplied to a held out test set of 500 examplesextracted with the APOS pattern, the filter has aprecision of .95 and a recall of .92.Precision vs. Recallas a Function of Threshold0.955960.965970.975980.985990.9950.4 0.5 0.6 0.7 0.8 0.9RecallPrecision0.0.0.0.Figure 2.
Plot of precision and recall on a 500 examplevalidation set as a threshold cutoff for positiveclassification is changed.
As the threshold is increased,precision increases while recall decreases.
At the 0.9threshold value, precision/recall on the validation set is0.98/0.7, on a held out test set it is 0.95/0.72.Final Extraction ResultsThe CN/PN and APOS filters were used to extractconcept-instance pairs from unstructured text.
Theapproximately 15GB of newspaper text (describedabove) was passed through the regular expressionpatterns and filtered through their appropriatelearned classifier.
The output of this process isapproximately 2,000,000 concept-instance pairs.Approximately 930,000 of these are unique pairs,comprised of nearly 500,000 unique instances 2 ,paired with over 450,000 unique concepts3 (e.g.,2 Uniqueness of instances is judged here solely on the basis ofsurface orthography.
Thus, ?Bill Clinton?
and ?WilliamClinton?
are considered two distinct instances.
The effects ofcollapsing such cases will be considered in future work.3 As with instances, concept uniqueness is judged solely on thebasis of orthography.
Thus, ?Steven Spielberg?
and ?J.
EdgarHoover?
are both considered instances of the single conceptThreshold=0.90Threshold=0.80?sultry screen actress?
), which can be categorizedbased on nearly 100,000 unique complex conceptheads (e.g., ?screen actress?)
and about 14,000unique simple concept heads (e.g., ?actress?
).Table 3 shows examples of this output.A sample of 100 concept-instance pairs wasrandomly selected from the 2,000,000 extractedpairs and hand annotated.
93% of these werejudged legitimate concept-instance pairs.Concept head Concept InstanceProducer Executive producer Av WestinNewspaper Military newspaper Red StarExpert Menopause expert Morris NotwlovitzFlutist Flutist James GalwayTable 3.
Example of concept-instance repository.Table shows extracted relations indexed by concepthead, complete concept, and instance.5Question Answering EvaluationA large number of questions were collected overthe period of a few months fromwww.askJeeves.com.
100 questions of the form?Who is x?
were randomly selected from this set.The questions queried concept-instance relationsthrough both instance centered queries (e.g., ?Whois Jennifer Capriati??)
and concept centeredqueries (e.g., ?Who is the mayor of Boston??
).Answers to these questions were thenautomatically generated both by look-up in the2,000,000 extracted concept-instance pairs and byTextMap, a state of the art web-based QuestionAnswering system which ranked among the top 10systems in the TREC 11 Question Answering track(Hermjakob et al, 2002).Although both systems supply multiplepossible answers for a question, evaluations wereconducted on only one answer.4  For TextMap, thisanswer is just the output with highest confidence,i.e., the system?s first answer.
For the extractedinstances, the answer was that concept-instancepair that appeared most frequently in the list ofextracted examples.
If all pairs appear with equalfrequency, a selection is made at random.Answers for both systems are then classifiedby hand into three categories based upon their?director.?
See Fleischman and Hovy (2002) for techniquesuseful in disambiguating such instances.4 Integration of multiple answers is an open research questionand is not addressed in this work.information content.
5  Answers that unequivocallyidentify an instance?s celebrity (e.g., ?JenniferCapriati is a tennis star?)
are marked correct.Answers that provide some, but insufficient,evidence to identify the instance?s celebrity (e.g.,?Jennifer Capriati is a defending champion?)
aremarked partially correct.
Answers that provide noinformation to identify the instance?s celebrity(e.g., ?Jennifer Capriati is a daughter?)
are markedincorrect.6  Table 5 shows example answers andjudgments for both systems.State of the Art  ExtractionAnswer Mark Answer MarkWho is NadiaComaneci?U.S.citizenP RomanianGymnastCWho is LilianThuram?NewspageI FrenchdefenderPWho is the mayorof Wash., D.C.?AnthonyWilliamsC no answerfoundITable 5.
Example answers and judgments of a state ofthe art system and look-up method using extractedconcept-instance pairs on questions collected online.Ratings were judged as either correct (C), partiallycorrect (P), or incorrect (I).6Question Answering ResultsResults of this comparison are presented in Figure3.
The simple look-up of extracted concept-instance pairs generated 8% more partially correctanswers and 25% more entirely correct answersthan TextMap.
Also, 21% of the questions thatTextMap answered incorrectly, were answeredpartially correctly using the extracted pairs; and36% of the questions that TextMap answeredincorrectly, were answered entirely correctly usingthe extracted pairs.
This suggests that over half ofthe questions that TextMap got wrong could havebenefited from information in the concept-instancepairs.
Finally, while the look-up of extracted pairstook approximately ten seconds for all 100questions, TextMap took approximately 9 hours.5  Evaluation of such ?definition questions?
is an activeresearch challenge and the subject of a recent TREC pilotstudy.
While the criteria presented here are not ideal, they areconsistent, and sufficient for a system comparison.6  While TextMap is guaranteed to return some answer forevery question posed, there is no guarantee that an answer willbe found amongst the extracted concept-instance pairs.
Whensuch a case arises, the look-up method?s answer is counted asincorrect.This difference represents a time speed up of threeorders of magnitude.There are a number of reasons why the state ofthe art system performed poorly compared to thesimple extraction method.
First, as mentionedabove, the lack of newspaper text on the webmeans that TextMap did not have access to thesame information-rich resources that the extractionmethod exploited.
Further, the simplicity of theextraction method makes it more resilient to thenoise (such as parser error) that is introduced bythe many modules employed by TextMap.
Andfinally, because it is designed to answer any typeof question, not just ?Who is??
questions,TextMap is not as precise as the extractiontechnique.
This is due to both its lack of tailormade patterns for specific question types, as wellas, its inability to filter those patterns with highprecision.7Figure 3.
Evaluation results for the state of the artsystem and look-up method using extracted concept-instance pairs on 100 ?Who is ??
questions collectedonline.
Results are grouped by category: partiallycorrect, entirely correct, and entirely incorrect.Discussion and Future WorkThe information repository approach to QuestionAnswering offers possibilities of increased speedand accuracy for current systems.
By collectinginformation offline, on text not readily available tosearch engines, and storing it to be accessiblequickly and easily, Question Answering systemswill be able to operate more efficiently and moreeffectively.In order to achieve real-time, accurateQuestion Answering, repositories of data muchlarger than that described here must be generated.We imagine huge data warehouses where eachrepository contains relations, such as birthplace-of,location-of, creator-of, etc.
These repositorieswould be automatically filled by a system thatcontinuously watches various online news sources,scouring them for useful information.Such a system would have a large library ofextraction patterns for many different types ofrelations.
These patterns could be manuallygenerated, such as the ones described here, orlearned from text, as described in Ravichandranand Hovy (2002).
Each pattern would have amachine-learned filter in order to insure highprecision output relations.
These relations wouldthen be stored in repositories that could be quicklyand easily searched to answer user queries.
7In this way, we envision a system similar to(Lin et al, 2002).
However, instead of relying oncostly structured databases and pain stakinglygenerated wrappers, repositories are automaticallyfilled with information from many differentpatterns.
Access to these repositories does notrequire wrapper generation, because allinformation is stored in easily accessible naturallanguage text.
The key here is the use of learnedfilters which insure that the information in therepository is clean and reliable.Performance on a QuestionAnswering Task101520253035404550Partial Correct Incorrect%CorrectState of the Art System Extraction SystemSuch a system is not meant to be complete byitself, however.
Many aspects of QuestionAnswering remain to be addressed.
For example,question classification is necessary in order todetermine which repositories (i.e., which relations)are associated with which questions.Further, many question types require postprocessing.
Even for ?Who is ??
questionsmultiple answers need to be integrated before finaloutput is presented.
An interesting corollary tousing this offline strategy is that each extractedinstance has with it a frequency distribution ofassociated concepts (e.g., for ?Bill Clinton?
: 105?US president?
; 52 ?candidate?
; 4 ?nominee?
).This distribution can be used in conjunction withtime/stamp information to formulate minibiographies as answers to ?Who is ??
questions.We believe that generating and maintaininginformation repositories will advance many aspectsof Natural Language Processing.
Their uses in7 An important addition to this system would be the inclusionof time/date stamp and data source information.
For, while?George Bush?
is ?president?
today, he will not be forever.data driven Question Answering are clear.
Inaddition, concept-instance pairs could be useful indisambiguating references in text, which is achallenge in Machine Translation and TextSummarization.In order to facilitate further research, we havemade the extracted pairs described here publiclyavailable at www.isi.edu/~fleisch/instances.txt.gz.In order to maximize the utility of these pairs, weare integrating them into an Ontology, where theycan be more efficiently stored, cross-correlated,and shared.AcknowledgmentsThe authors would like to thank Miruna Ticrea forher valuable help with training the classifier.
Wewould also like to thank Andrew Philpot for his workon integrating instances into the Omega Ontology,and Daniel Marcu whose comments and ideas wereinvaluable.ReferencesMichelle Banko, Eric Brill.
2001.
Scaling to Very VeryLarge Corpora for Natural Language Disambiguation.Proceedings of the Association for ComputationalLinguistics, Toulouse, France.Matthew Berland and Eugene Charniak.
1999.
FindingParts in Very Large Corpora.
Proceedings of the 37thAnnual Meeting of the Association for ComputationalLinguistics.
College Park, Maryland.Eric Brill.
1994.
Some advances in rule based part of speechtagging.
Proc.
of AAAI.
Seattle, Washington.Eric Brill, Jimmy Lin, Michele Banko, Susan Dumais,and Andrew Ng.
2001.
Data-Intensive QuestionAnswering.
Proceedings of the 2001 Text REtrievalConference (TREC 2001), Gaithersburg, MD.Michael Fleischman and Eduard Hovy.
2002.
FineGrained Classification of Named Entities.
19thInternational Conference on ComputationalLinguistics (COLING).
Taipei, Taiwan.Ulf Hermjakob, Abdessamad Echihabi, and DanielMarcu.
2002.
Natural Language BasedReformulation Resource and Web Exploitation forQuestion Answering.
In Proceedings of the TREC-2002 Conference, NIST.
Gaithersburg, MD.Marti Hearst.
1992.
Automatic Acquisition ofHyponyms from Large Text Corpora.
Proceedings ofthe Fourteenth International Conference onComputational Linguistics, Nantes, France.Jimmy Lin, Aaron Fernandes, Boris Katz, GregoryMarton, and Stefanie Tellex.
2002.
ExtractingAnswers from the Web Using Data Annotation andData Mining Techniques.
Proceedings of the 2002Text REtrieval Conference (TREC 2002)Gaithersburg, MD.Gideon S. Mann.
2002.
Fine-Grained Proper NounOntologies for Question Answering.
SemaNet'02:Building and Using Semantic Networks, Taipei,Taiwan.Deepak Ravichandran and Eduard Hovy.
2002.Learning surface text patterns for a QuestionAnswering system.
Proceedings of the 40th ACLconference.
Philadelphia, PA.I.
Witten and E. Frank.
1999.
Data Mining: PracticalMachine Learning Tools and Techniques with JAVAimplementations.
Morgan Kaufmann, San Francisco,CA.
