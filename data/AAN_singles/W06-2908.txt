Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),pages 53?60, New York City, June 2006. c?2006 Association for Computational LinguisticsSemantic Role Recognition using Kernels on Weighted Marked OrderedLabeled TreesJun?ichi Kazama and Kentaro TorisawaJapan Advanced Institute of Science and Technology (JAIST)Asahidai 1-1, Nomi, Ishikawa, 923-1292 Japan{kazama, torisawa}@jaist.ac.jpAbstractWe present a method for recognizing se-mantic role arguments using a kernel onweighted marked ordered labeled trees(the WMOLT kernel).
We extend thekernels on marked ordered labeled trees(Kazama and Torisawa, 2005) so that themark can be weighted according to its im-portance.
We improve the accuracy bygiving more weights on subtrees that con-tain the predicate and the argument nodeswith this ability.
Although Kazama andTorisawa (2005) presented fast trainingwith tree kernels, the slow classificationduring runtime remained to be solved.
Inthis paper, we give a solution that uses anefficient DP updating procedure applica-ble in argument recognition.
We demon-strate that the WMOLT kernel improvesthe accuracy, and our speed-up methodmakes the recognition more than 40 timesfaster than the naive classification.1 IntroductionSemantic role labeling (SRL) is a task that recog-nizes the arguments of a predicate (verb) in a sen-tence and assigns the correct role to each argument.As this task is recognized as an important step after(or the last step of) syntactic analysis, many stud-ies have been conducted to achieve accurate seman-tic role labeling (Gildea and Jurafsky, 2002; Mos-chitti, 2004; Hacioglu et al, 2004; Punyakanok etal., 2004; Pradhan et al, 2005a; Pradhan et al,2005b; Toutanova et al, 2005).Most of the studies have focused on machinelearning because of the availability of standarddatasets, such as PropBank (Kingsbury and Palmer,2002).
Naturally, the usefulness of parse trees inthis task can be anticipated.
For example, the recentCoNLL 2005 shared task (Carreras and Ma`rquez,2005) provided parse trees for use and their useful-ness was ensured.
Most of the methods heuristicallyextract features from parse trees, and from othersources, and use them in machine learning methodsbased on feature vector representation.
As a result,these methods depend on feature engineering, whichis time-consuming.Tree kernels (Collins and Duffy, 2001; Kashimaand Koyanagi, 2002) have been proposed to directlyhandle trees in kernel-based methods, such as SVMs(Vapnik, 1995).
Tree kernels calculate the similar-ity between trees, taking into consideration all of thesubtrees, and, therefore there is no need for such fea-ture engineering.Moschitti and Bejan (2004) extensively studiedtree kernels for semantic role labeling.
However,they reported that they could not successfully buildan accurate argument recognizer, although the roleassignment was improved.
Although Moschitti et al(2005) reported on argument recognition using treekernels, it was a preliminary evaluation because theyused oracle parse trees.Kazama and Torisawa (2005) proposed a new treekernel for node relation labeling, as which SRL canbe cast.
This kernel is defined on marked ordered la-beled trees, where a node can have a mark to indicatethe existence of a relation.
We refer to this kernelas the MOLT kernel.
Compared to (Moschitti andBejan, 2004) where tree fragments are heuristicallyextracted before applying tree kernels, the MOLTkernel is general and desirable since it does not re-quire such fragment extraction.
However, the eval-uation conducted by Kazama and Torisawa (2005)was limited to preliminary experiments for role as-signment.
In this study, we first evaluated the per-formance of the MOLT kernel for argument recogni-tion, and found that theMOLT kernel cannot achievea high accuracy if used in its original form.53a catI saw the parkinDT NNPRP VBD DT NNINNPSNP VPNPPP(a)a catI saw the parkinDT NNPRP VBD DT NNINNPSNP VPNPPP(b)a catI saw the parkinDT NNPRP VBD DT NNINNPSNP VPNPPP(c)a catI saw the parkinDT NNPRP VBD DT NNINNPSNP VPNPPP(a')*0*1Figure 1: (a)-(c): Argument recognition as node relation recognition.
(a?
): relation (a) represented as markedordered tree.Therefore, in this paper we propose a modifica-tion of the MOLT kernel, which greatly improvesthe accuracy.
The problem with the original MOLTkernel is that it treats subtrees with one mark, i.e.,those including only the argument or the predicatenode, and subtrees with two marks, i.e., those in-cluding both the argument and the predicate nodesequally, although the latter is likely to be more im-portant for distinguishing difficult arguments.
Thus,we modified the MOLT kernel so that the marks canbe weighted in order to give large weights to the sub-trees with many marks.
We call the modified kernelthe WMOLT kernel (the kernel on weighted markedordered labeled trees).
We show that this modifica-tion greatly improves the accuracy when the weightsfor marks are properly tuned.One of the issues that arises when using tree ker-nels is time complexity.
In general, tree kernels canbe calculated in O(|T1||T2|) time, where |Ti| is thenumber of nodes in tree Ti, using dynamic program-ming (DP) procedures (Collins and Duffy, 2001;Kashima and Koyanagi, 2002).
However, this costis not negligible in practice.
Kazama and Torisawa(2005) proposed a method that drastically speeds upthe calculation during training by converting treesinto efficient vectors using a tree mining algorithm.However, the slow classification during runtime re-mained an open problem.We propose a method for speeding up the runtimeclassification for argument recognition.
In argumentrecognition, we determine whether a node is an ar-gument or not for all the nodes in a tree .
Thisrequires a series of calculations between a supportvector tree and a tree with slightly different mark-ing.
By exploiting this property, we can efficientlyupdate DP cells to obtain the kernel value with lesscomputational cost.In the experiments, we demonstrated that theWMOLT kernel drastically improved the accuracyand that our speed-up method enabled more than40 times faster argument recognition.
Despite thesesuccesses, the performance of our current system isF1 = 78.22 on the CoNLL 2005 evaluation set whenusing the Charniak parse trees, which is far worsethan the state-of-the-art system.
We will presentpossible reasons and future directions.2 Semantic Role LabelingSemantic role labeling (SRL) recognizes the argu-ments of a given predicate and assigns the correctrole to each argument.
For example, the sentence ?Isaw a cat in the park?
will be labeled as follows withrespect to the predicate ?see?.
[A0 I] [V saw] [A1 a cat] [AM-LOC in the park]In the example, A0, A1, and AM-LOC are the rolesassigned to the arguments.
In the CoNLL 2005dataset, there are the numbered arguments (AX)whose semantics are predicate dependent, the ad-juncts (AM-X), and the references (R-X) for rel-ative clauses.Many previous studies employed two-step SRLmethods, where (1) we first recognize the argu-ments, and then (2) classify the argument to the cor-rect role.
We also assume this two-step processingand focus on the argument recognition.Given a parse tree, argument recognition can becast as the classification of tree nodes into twoclasses, ?ARG?
and ?NO-ARG?.
Then, we considerthe words (a phrase) that are the descendants of an?ARG?
node to be an argument.
Since argumentsare defined for a given predicate, this classificationis the recognition of a relation between the predicateand tree nodes.
Thus, we want to build a binary clas-sifier that returns a +1 for correct relations and a -1for incorrect relations.
For the above example, theclassifier will output a +1 for the relations indicatedby (a), (b), and (c) in Figure 1 and a -1 for the rela-tions between the predicate node and other nodes.54Since the task is the classification of trees withnode relations, tree kernels for usual ordered la-beled trees, such as those proposed by Collins andDuffy (2001) and Kashima and Koyanagi (2002),are not useful.
Kazama and Torisawa (2005) pro-posed to represent a node relation in a tree as amarked ordered labeled tree and presented a kernelfor it (MOLT kernel).
We adopted the MOLT kerneland extend it for accurate argument recognition.3 Kernels for Argument Recognition3.1 Kernel-based classificationKernel-based methods, such as support vector ma-chines (SVMs) (Vapnik, 1995), consider a mapping?
(x) that maps the object x into a, (usually high-dimensional), feature space and learn a classifier inthis space.
A kernel function K(xi, xj) is a functionthat calculates the inner product ??(xi),?(xj)?
inthe feature space without explicitly computing?
(x),which is sometimes intractable.
Then, any classifierthat is represented by using only the inner productsbetween the vectors in a feature space can be re-written using the kernel function.
For example, anSVM classifier has the form:f(x) =?i?iK(xi, x) + b,where ?i and b are the parameters learned in thetraining.
With kernel-based methods, we can con-struct a powerful classifier in a high-dimensionalfeature space.
In addition, objects x do not needto be vectors as long as a kernel function is defined(e.g., x can be strings, trees, or graphs).3.2 MOLT kernelA marked ordered labeled tree (Kazama and Tori-sawa, 2005) is an ordered labeled tree in which eachnode can have a mark in addition to a label.
We canencode a k-node relation by using k distinct marks.In this study, we determine an argument node with-out considering other arguments of the same pred-icate, i.e., we represent an argument relation as atwo-node relation using two marks.
For example,the relation (a) in Figure 1 can be represented as themarked ordered labeled tree (a?
).11Note that we use mark *0 for the predicate node and mark*1 for the argument node.Table 1: Notations for MOLT kernel.?
ni denotes a node of a tree.
In this paper, ni is an ID assigned in thepost-order traversal.?
|Ti| denotes the number of nodes in tree Ti.?
l(ni) returns the label of node ni.?
m(ni) returns the mark of node ni.
If ni has no mark, m(ni)returns the special mark no-mark.?
marked(ni) returns true iff m(ni) is not no-mark.?
nc(ni) is the number of children of node ni.?
chk(ni) is the k-th child of node ni.?
pa(ni) is the parent of node ni.?
root(Ti) is the root node of Ti?
ni ?
nj means that ni is an elder sister of nj .Kazama and Torisawa (2005) presented a kernelon marked ordered trees (the MOLT kernel), whichis defined as:2K(T1, T2) =E?i=1W (Si) ?#Si(T1) ?#Si(T2),where Si is a possible subtree and #Si(Tj) isthe number of times Si is included in Tj .
Themapping corresponding to this kernel is ?
(T ) =(?W (S1)#S1(T ), ?
?
?
,?W (SE)#SE (T )), whichmaps the tree into the feature space of all the possi-ble subtrees.The tree inclusion is defined in many ways.
Forexample, Kashima and Koyanagi (2002) presentedthe following type of inclusion.1 DEFINITION S is included in T iff there exists aone-to-one function ?
from a node of S to a nodeof T , such that (i) pa(?
(ni)) = ?
(pa(ni)), (ii)?
(ni) ?
?
(nj) iff ni ?
nj , , and (iii) l(?
(ni)) =l(ni) (and m(?
(ni)) = m(ni) in the MOLT kernel).See Table 1 for the meaning of each function.
Thisdefinition means that any subtrees preserving theparent-child relation, the sibling relation, and label-marks, are allowed.
In this paper, we employ thisdefinition, since Kazama and Torisawa (2005) re-ported that the MOLT kernel with this definition hasa higher accuracy than one with the definition pre-sented by Collins and Duffy (2001).W (Si) is the weight of subtree Si.
The weight-ing in Kazama and Torisawa (2005) is written as fol-2This notation is slightly different from (Kazama and Tori-sawa, 2005).55Table 2: Example of subtree inclusion and sub-tree weights.
The last row shows the weights forWMOLT kernel.T included subtreesW (Si) 0 ?
?
?2 ?2 ?3W (Si) 0 ??
??
?2?
?2?2 ?3?2lows.W (Si) ={?|Si| if marked(Si),0 otherwise,(1)where marked(Si) returns true iff marked(ni) =true for at least one node in tree Si.
By this weight-ing, only the subtrees with at least one mark are con-sidered.
The idea behind this is that subtrees havingno marks are not useful for relation recognition orlabeling.
?
(0 ?
?
?
1) is a factor to prevent the ker-nel values from becoming too large, which has beenused in previous studies (Collins and Duffy, 2001;Kashima and Koyanagi, 2002).Table 2 shows an example of subtree inclusionand the weights given to each included subtree.
Notethat the subtrees are treated differently when themarkings are different, even if the labels are thesame.Although the dimension of the feature spaceis exponential, tree kernels can be calculated inO(|T1||T2|) time using dynamic programming (DP)procedures (Collins and Duffy, 2001; Kashima andKoyanagi, 2002).
The MOLT kernel also has anO(|T1||T2|) DP procedure (Kazama and Torisawa,2005).3.3 WMOLT kernelAlthough Kazama and Torisawa (2005) evaluatedthe MOLT kernel for SRL, the evaluation was onlyon the role assignment task and was preliminary.
Weevaluated the MOLT kernel for argument recogni-tion, and found that theMOLT kernel cannot achievea high accuracy for argument recognition.The problem is that the MOLT kernel treats sub-trees with one mark and subtrees with two marksequally, although the latter seems to be more impor-tant in distinguishing difficult arguments.Consider the sentence, ?He said industry shouldbuild plants?.
For ?say?, we have the following la-beling.
[A0 He] [V said] [A1 industry should build plants]On the other hand, for ?build?, we haveHe said [A0 industry] [AM-MOD should] [V build][A1 plants].As can be seen, ?he?
is the A0 argument of ?say?,but not an argument of ?build?.
Thus, our classifiershould return a +1 for the tree where ?he?
is markedwhen the predicate is ?say?, and a -1 when the pred-icate is ?build?.
Although the subtrees around thenode for ?say?
and ?build?
are different, the subtreesaround the node for ?he?
are identical for both cases.If ?he?
is often the A0 argument in the corpus, it islikely that the classifier returns a +1 even for ?build?.Although the subtrees containing both the predicateand the argument nodes are considered in the MOLTkernel, they are given relatively small weights by Eq.
(1), since such subtrees are large.Thus, we modify the MOLT kernel so that themark can be weighted according to its importanceand the more marks the subtrees contain, the moreweights they get.
The modification is simple.
Wechange the definition of W (Si) as follows.W (Si) ={?|Si|?ni?Si ?
(m(ni)) if marked(Si),0 otherwise,where ?
(m) (?
1) is the weight of mark m. Wecall a kernel with this weight the WMOLT kernel.In this study, we assume ?
(no-mark) = 1 and?
(*0) = ?
(*1) = ?.
Then, the weight is simpli-fied as follows.W (Si) ={?|Si|?#m(Si) if marked(Si),0 otherwise,where #m(Si) is the number of marked nodes inSi.
The last row in Table 2 shows how the subtreeweights change by introducing this mark weighting.For the WMOLT kernel, we can deriveO(|T1||T2|) DP procedure by slightly modify-ing the procedure presented by Kazama andTorisawa (2005).
The method for speeding uptraining described in Kazama and Torisawa (2005)can also be applied with a slight modification.56Algorithm 3.1: WMOLT-KERNEL(T1, T2)for n1 ?
1 to |T1| do // nodes are ordered by the post-order traversalm ?
marked(n1)for n2 ?
1 to |T2| do // actually iterate only on n2 with l(n1) = l(n2)(A)8>>>>>>>>>><>>>>>>>>>>:if l(n1) ?= l(n2) or m(n1) ?= m(n2) thenC(n1, n2) ?
0 Cr(n1, n2) ?
0else if n1 and n2 are leaf nodes thenif m then C(n1, n2) ?
?
?
?
; Cr(n1, n2) ?
?
?
?
else C(n1, n2) ?
?
; Cr(n1, n2) ?
0elseS(0, j) ?
1, S(i, 0) ?
1 (i ?
[0, nc(n1)], j ?
[0, nc(n2)])if m then Sr(0, j) ?
1, Sr(i, 0) ?
1 else Sr(0, j) ?
0, Sr(i, 0) ?
0for i ?
1 to nc(n1) dofor j ?
1 to nc(n2) doS(i, j) ?
S(i?1, j) + S(i, j?1)?
S(i?1, j?1) + S(i?1, j?1) ?
C(chi(n1), chj(n2))Sr(i, j) ?
Sr(i?1, j) + Sr(i, j?1)?
Sr(i?1, j?1) + Sr(i?1, j?1) ?
C(chi(n1), chj(n2))+S(i?1, j?1) ?
Cr(chi(n1), chj(n2))?
Sr(i?1, j?1) ?
Cr(chi(n1), chj(n2))if m then C(n1, n2) ?
?
?
?
?
S(nc(n1), nc(n2)) else C(n1, n2) ?
?
?
S(nc(n1), nc(n2))if m then Cr(n1, n2) ?
?
?
?
?
Sr(nc(n1), nc(n2)) else Cr(n1, n2) ?
?
?
Sr(nc(n1), nc(n2))return (P|T1|n1=1P|T2|n2=1 Cr(n1, n2))We describe this DP procedure in some detail.The key is the use of two DP matrices of size|T1| ?
|T2|.
The first is C(n1, n2) defined as:C(n1, n2)?PSi W?
(Si) ?#Si(T1 ?
n1) ?#Si(T2 ?
n2),where #Si(Tj ?
nk) represents the number of timessubtree Si is included in tree Tj with ?
(root(Si)) =nk.
W ?
(Si) is defined as W ?
(Si) = ?|Si|?#m(Si).This means that this matrix records the values thatignore whether marked(Si) = true or not.
Thesecond is Cr(n1, n2) defined as:Cr(n1, n2)?PSi W (Si) ?#Si(T1 ?
n1) ?#Si(T2 ?
n2).With these matrices, the kernel is calculated as:K(T1, T2) =?n1?T1?n2?T2Cr(n1, n2).C(n1, n2) and Cr(n1, n2) are calculated recur-sively, starting from the leaves of the trees.
The re-cursive procedure is shown in Algorithm 3.1.
Seealso Table 1 for the meaning of the functions used.4 Fast Argument RecognitionWe use the SVMs for the classifiers in argumentrecognition in this study and describe the fast clas-sification method based on SVMs.3 We denote amarked ordered labeled tree where node nk of anordered labeled tree U is marked by mark X , nl byY , and so on, by U@{nk = X,nl = Y, .
.
.
}.3The method can be applied to a wide range of kernel-basedmethods that have the same structure as SVMs.Algorithm 4.1: CALCULATE-T(U, Tj)procedure FAST-UPDATE(nk)diff ?
0, m(nk) ?
*1, U ?
?for n2 ?
1 to |Tj | do change(n2) ?
truen1 ?
nkwhile n1 ?= nil do8>>>>>>>><>>>>>>>>:for n2 ?
1 to |Tj | do// actually iterate only on n2 with l(pa(n1)) = l(n2)nchange(n2) ?
falsefor n2 ?
1 to |Tj | do// actually iterate only on n2 with l(n1) = l(n2)if change(n2) thenpre ?
Cr(n1, n2), U ?
U ?
(n1, n2)update C(n1, n2) and Cr(n1, n2)using (A) of Algorithm 3.1diff += (Cr(n1, n2)?
pre)if pa(n2) ?= nil then nchange(pa(n2)) ?
truen1 ?
pa(n1), change ?
nchangefor (n1, n2) ?
U do //restore DP cellsC(n1, n2) ?
C?
(n1, n2), Cr(n1, n2) ?
Cr ?
(n1, n2)m(nk) ?
no-markreturn (diff )mainm(nv) ?
?0, k ?
WMOLT-KERNEL(U, Tj)C?
(n1, n2) ?
C(n1, n2), Cr ?
(n1, n2) ?
Cr(n1, n2)for nk ?
1 to |U | do (nk ?= nv)diff ?
FAST-UPDATE(nk), t(nk) ?
k + diffGiven a sentence represented by tree U and thenode for the target predicate nv, the argument recog-nition requires the calculation of:s(nk) =?Tj?SV?jK(U@{nv=*0, nk=*1}, Tj)+b,(2)for all nk ?
U (?= nv), where SV represents thesupport vectors.
Naively, this requires O(|U | ?|SV| ?
|U ||Tj |) time, which is rather costly in prac-tice.57However, if we exploit the fact that U@{nv =*0, nk = *1} is different from U@{nv = *0} at onenode, we can greatly speed up the above calculation.At first, we calculate K(U@{nv = *0}, Tj) usingthe DP procedure presented in the previous section,and then calculate K(U@{nv = *0, nk = *1}, Tj)using a more efficient DP that updates only the val-ues of the necessary DP cells of the first DP.
Morespecifically, we only need to update the DP cells in-volving the ancestor nodes of nk.Here we show the procedure for calculatingt(nk) = K(U@{nv = *0, nk = *1}, Tj) for allnk for a given support vector Tj , which will suf-fice for calculating s(nk).
Algorithm 4.1 shows theprocedure.
For each nk, this procedure updates atmost (nk?s depth) ?
|Tj | cells, which is much lessthan |U | ?
|Tj | cells.
In addition, when updatingthe cells for (n1, n2), we only need to update themwhen the cells for any child of n2 have been updatedin the calculation of the cells for the children of n1.To achieve this, change(n2) in the algorithm storeswhether the cells of any child of n2 have been up-dated.
This technique will also reduce the numberof updated cells.5 Non-overlapping ConstraintFinally, in argument recognition, there is a strongconstraint that the arguments for a given predicatedo not overlap each other.
To enforce this constraint,we employ the approach presented by Toutanovaet al (2005).
Given the local classification proba-bility p(nk = Xk) (Xk ?
{ARG,NO-ARG}),this method finds the assignment that maximizes?k p(nk = Xk) while satisfying the above non-overlapping constraint, by using a dynamic pro-gramming procedure.
Since the output of SVMs isnot a probability value, in this study we obtain theprobability value by converting the output from theSVM, s(nk), using the sigmoid function:4p(nk = ARG) = 1/(1 + exp(?s(nk))).6 Evaluation6.1 SettingFor our evaluation we used the dataset pro-vided for the CoNLL 2005 SRL shared task4Parameter fitting (Platt, 1999) is not performed.(www.lsi.upc.edu/?srlconll).
We used only the train-ing part and divided it into our training, develop-ment, and test sets (23,899, 7,966, and 7,967 sen-tences, respectively).
We used the outputs of theCharniak parser provided with the dataset.
We alsoused POS tags, which were also provided, by insert-ing the nodes labeled by POS tags above the wordnodes.
The words were downcased.We used TinySVM5 as the implementation of theSVMs, adding the WMOLT kernel.
We normalizedthe kernel as: K(Ti, Tj)/?K(Ti, Ti)?K(Tj , Tj).To train the classifiers, for a positive example weused the marked ordered labeled tree that encodesan argument in the training set.
Although nodesother than the argument nodes were potentially neg-ative examples, we used 1/5 of these nodes that wererandomly-sampled, since the number of such nodesis so large that the training cannot be performed inpractice.
Note that we ignored the arguments thatdo not match any node in the tree (the rate of sucharguments was about 3.5% in the training set).6.2 Effect of mark weightingWe first evaluated the effect of the mark weight-ing of the WMOLT kernel.
For several fixed ?, wetuned ?
and the soft-margin constant of the SVM,C,and evaluated the recognition accuracy.
We tested30 different values of C ?
[0.1 .
.
.
500] for each?
?
[0.05, 0.1, 0.15, 0.2, 0.25, 0.3].
The tuning wasperformed using the method for speeding up thetraining with tree kernels described by Kazama andTorisawa (2005).
We conducted the above experi-ment for several training sizes.Table 3 shows the results.
This table shows thebest setting of ?
and C, the performance on the de-velopment set with the best setting, and the perfor-mance on the test set.
The performance is shownin the F1 measure.
Note that we treated the regionlabeled C-k in the CoNLL 2005 dataset as an inde-pendent argument.We can see that the mark weighting greatly im-proves the accuracy over the original MOLT kernel(i.e., ?
= 1).
In addition, we can see that the bestsetting for ?
is somewhere around ?
= 4, 000.
Inthis experiment, we could only test up to 1,000 sen-tences due to the cost of SVM training, which were5chasen.org/?taku/software/TinySVM58Table 3: Effect of ?
in mark weighting of WMOLT kernel.training size (No.
of sentences)250 500 700 1,000setting dev test setting dev test setting dev test setting dev test?
(?,C) (F1) (F1) (?,C) (F1) (F1) (?,C) (F1) (F1) (?,C) (F1) (F1)1 0.15, 20.50 63.66 65.13 0.2, 20.50 69.01 70.33 0.2, 20.50 72.11 73.57 0.25, 12.04 75.38 76.25100 0.3, 12.04 80.13 80.85 0.3,500 82.25 82.98 0.3, 34.92 83.93 84.72 0.3, 3.18 85.09 85.851,000 0.2, 2.438 82.65 83.36 0.2, 2.438 84.80 85.45 0.2, 3.182 85.58 86.20 0.2, 7.071 86.40 86.802,000 0.2, 2.438 83.43 84.12 0.2, 2.438 85.56 86.24 0.2, 2.438 86.23 86.80 0.2, 12.04 86.61 87.184,000 0.2, 2.438 83.87 84.50 0.15, 4.15 84.94 85.61 0.15, 7.07 85.84 86.32 0.2, 12.04 86.82 87.314,000 (w/o) 80.81 81.41 80.71 81.51 81.86 82.33 84.27 84.63empirically O(L2) where L is the number of train-ing examples, regardless of the use of the speed-upmethod (Kazama and Torisawa, 2005), However, wecan observe that the WMOLT kernel achieves a highaccuracy even when the training data is very small.6.3 Effect of non-overlapping constraintAdditionally, we observed how the accuracychanges when we do not use the method describedin Section 5 and instead consider the node to be anargument when s(nk) > 0.
The last row in Ta-ble 3 shows the accuracy for the model obtainedwith ?
= 4, 000.
We could observe that the non-overlapping constraint also improves the accuracy.6.4 Recognition speed-upNext, we examined the method for fast argumentrecognition described in Section 4.
Using the clas-sifiers with ?
= 4, 000, we measured the time re-quired for recognizing the arguments for 200 sen-tences with the naive classification of Eq.
(2) andwith the fast update procedure shown in Algorithm4.1.
The time was measured using a computer with2.2-GHz dual-core Opterons and 8-GB of RAM.Table 4 shows the results.
We can see a constantspeed-up by a factor of more than 40, although thetime was increased for both methods as the size ofthe training data increases (due to the increase in thenumber of support vectors).Table 4: Recognition time (sec.)
with naive classifi-cation and proposed fast update.training size (No.
of sentences)250 500 750 1,000naive 11,266 13,008 18,313 30,226proposed 226 310 442 731speed-up 49.84 41.96 41.43 41.346.5 Evaluation on CoNLL 2005 evaluation setTo compare the performance of our system withother systems, we conducted the evaluation on theofficial evaluation set of the CoNLL 2005 sharedtask.
We used a model trained using 2,000 sen-tences (57,547 examples) with (?
= 4, 000, ?
=0.2, C = 12.04), the best setting in the previous ex-periments.
This is the largest model we have suc-cessfully trained so far, and has F1 = 88.00 on thetest set in the previous experiments.The accuracy of this model on the official evalua-tion set was F1 = 79.96 using the criterion from theprevious experiments where we treated a C-k argu-ment as an independent argument.
The official eval-uation script returned F1 = 78.22.
This differenceis caused because the official script takes C-k argu-ments into consideration, while our system cannotoutput C-k labels since it is just an argument rec-ognizer.
Therefore, the performance will becomeslightly higher than F1 = 78.22 if we perform therole assignment step.
However, our current systemis worse than the systems reported in the CoNLL2005 shared task in any case, since it is reported thatthey had F1 = 79.92 to 83.78 argument recognitionaccuracy (Carreras and Ma`rquez, 2005).7 DiscussionAlthough we have improved the accuracy by intro-ducing theWMOLT kernel, the accuracy for the offi-cial evaluation set was not satisfactory.
One possiblereason is the accuracy of the parser.
Since the Char-niak parser is trained on the same set with the train-ing set of the CoNLL 2005 shared task, the pars-ing accuracy is worse for the official evaluation setthan for the training set.
For example, the rate of thearguments that do not match any node of the parsetree is 3.93% for the training set, but 8.16% for the59evaluation set.
This, to some extent, explains whyour system, which achieved F1 = 88.00 for our testset, could only achieved F1 = 79.96.
To achieve ahigher accuracy, we need to make the system morerobust to parsing errors.
Some of the non-matchingarguments are caused by incorrect treatment of quo-tation marks and commas.
These errors seem to besolved by using simple pre-processing.
Other majornon-matching arguments are caused by PP attach-ment errors.
To solve these errors, we need to ex-plore more, such as using n-best parses and the useof several syntactic views (Pradhan et al, 2005b).Another reason for the low accuracy is the size ofthe training data.
In this study, we could train theSVM with 2,000 sentences (this took more than 30hours including the conversion of trees), but this isa very small fraction of the entire training set.
Weneed to explore the methods for incorporating a largetraining set within a reasonable training time.
Forexample, the combination of small SVMs (Shen etal., 2003) is a possible direction.The contribution of this study is not the accuracyachieved.
The first contribution is the demonstrationof the drastic effect of the mark weighting.
We willexplore more accurate kernels based on theWMOLTkernel.
For example, we are planning to use dif-ferent weights depending on the marks.
The sec-ond contribution is the method of speeding-up argu-ment recognition.
This is of great importance, sincethe proposed method can be applied to other taskswhere all nodes in a tree should be classified.
In ad-dition, this method became possible because of theWMOLT kernel, and it is hard to apply to Moschittiand Bejan (2004) where the tree structure changesduring recognition.
Thus, the architecture that usesthe WMOLT kernel is promising, if we assume fur-ther progress is possible with the kernel design.8 ConclusionWe proposed a method for recognizing semantic rolearguments using the WMOLT kernel.
The markweighting introduced in the WMOLT kernel greatlyimproved the accuracy.
In addition, we presenteda method for speeding up the recognition, which re-sulted in more than a 40 times faster recognition.
Al-though the accuracy of the current system is worsethan the state-of-the-art system, we expect to furtherimprove our system.ReferencesX.
Carreras and L. Ma`rquez.
2005.
Introduction to theCoNLL-2005 shared task: Semantic role labeling.
InCoNLL 2005.M.
Collins and N. Duffy.
2001.
Convolution kernels fornatural language.
In NIPS 2001.D.
Gildea and D. Jurafsky.
2002.
Automatic labeling ofsemantic roles.
Computational Linguistics, 28(3).K.
Hacioglu, S. Pradhan, W. Ward, J. H. Martin, andD.
Jurafsky.
2004.
Semantic role labeling by taggingsyntactic chunks.
In CoNLL 2004.H.
Kashima and T. Koyanagi.
2002.
Kernels for semi-structured data.
In ICML 2002, pages 291?298.J.
Kazama and K. Torisawa.
2005.
Speeding up trainingwith tree kernels for node relation labeling.
In EMNLP2005.P.
Kingsbury and M. Palmer.
2002.
From treebank topropbank.
In LREC 02.A.
Moschitti and C. A. Bejan.
2004.
A semantic kernelsfor predicate argument classification.
In CoNLL 2004.A.
Moschitti, B. Coppola, D. Pighin, and B. Basili.
2005.Engineering of syntactic features for shallow semanticparsing.
In ACL 2005 Workshop on Feature Enginner-ing for Machine Learning in Natural Language Pro-cessing.A.
Moschitti.
2004.
A study on convolution kernels forshallow semantic parsing.
In ACL 2004.J.
C. Platt.
1999.
Probabilistic outputs for support vectormachines and comparisons to regularized likelihoodmethods.
Advances in Large Margin Classifiers.S.
Pradhan, K. Hacioglu, W. Ward, D. Jurafsky, and J. H.Martin.
2005a.
Support vector learning for semanticargument classification.
Machine Learning, 60(1).S.
Pradhan, W. Ward, K. Hacioglu, J. H. Martin, andD.
Jurafsky.
2005b.
Semantic role labeling using dif-ferent syntactic views.
In ACL 2005.V.
Punyakanok, D. Roth, W. Yih, and D. Zimak.
2004.Semantic role labeling via integer linear programminginference.
In COLING 2004.L.
Shen, A. Sarkar, and A. K. Joshi.
2003.
Using LTAGbased features in parse reranking.
In EMNLP 2003.K.
Toutanova, A. Haghighi, and C. D. Manning.
2005.Joint learning improves semantic role labeling.
In ACL2005.V.
Vapnik.
1995.
The Nature of Statistical Learning The-ory.
Springer Verlag.60
