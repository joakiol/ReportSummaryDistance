One Sense per Collocation and Genre/Topic VariationsDavid MartinezIXA NLP GroupUniversity of the Basque Country649 pk.
20.080Donostia.
Spainjibmaird@si.ehu.esEneko AgirreIXA NLP GroupUniversity of the Basque Country649 pk.
20.080Donostia.
Spaineneko@si.ehu.esAbstractThis paper revisits the one sense percollocation hypothesis using fine-grainedsense distinctions and two different corpora.We show that the hypothesis i  weaker forfine-grained sense distinctions (70% vs.99% reported earlier on 2-way ambiguities).We also show that one sense per collocationdoes hold across corpora, but thatcollocations vary from one corpus to theother, following genre and topic variations.This explains the low results whenperforming word sense disambiguationacross corpora.
In fact, we demonstrate hatwhen two independent corpora share arelated genre/topic, the word sensedisambiguation results would be better.Future work on word sense disambiguationwill have to take into account genre andtopic as important parameters on theirmodels.IntroductionIn the early nineties two famous papers claimedthat the behavior of word senses in texts adheredto two principles: one sense per discourse (Galeet al, 1992) and one sense per collocation(Yarowsky, 1993).These hypotheses were shown to hold forsome particular corpora (totaling 380 Mwords)on words with 2-way ambiguity.
The wordsense distinctions came from different sources(translations into French, homophones,homographs, pseudo-words, etc.
), but nodictionary or lexical resource was linked tothem.
In the case of the one sense percollocation paper, several corpora were used,but nothing is said on whether the collocationshold across corpora.Since the papers were published, word sensedisambiguation has moved to deal with fine-grained sense distinctions from widelyrecognized semantic lexical resources;ontologies like Sensus, Cyc, EDR, WordNet,EuroWordNet, etc.
or machine-readabledictionaries like OALDC, Webster's, LDOCE,etc.
This is due, in part, to the availability ofpublic hand-tagged material, e.g.
SemCor(Miller et al, 1993) and the DSO collection (Ng& Lee, 1996).
We think that the old hypothesesshould be tested under the conditions of thisnewly available data.
This paper focuses on theDSO collection, which was tagged withWordNet senses (Miller et al 1990) andcomprises sentences extracted from twodifferent corpora: the balanced Brown Corpusand the Wall Street Journal corpus.Krovetz (1998) has shown that the one senseper discourse hypothesis does not hold for fine-grained senses in SemCor and DSO.
His resultshave been confirmed in our own experiments.We will therefore concentrate on the one senseper collocation hypothesis, considering thesetwo questions:?
Does the collocation hypothesis hold acrosscorpora, that is, across genre and topicvariations (compared to a single corpus,probably with little genre and topicvariations)??
Does the collocation hypothesis hold for free-grained sense distinctions (compared tohomograph level granularity)?The experimental tools to test the hypothesiswill be decision lists based on various kinds ofcollocational information.
We will compare theperformance across several corpora (the BrownCorpus and Wall Street Journal parts of theDSO collection), and also across differentsections of the Brown Corpus, selectedaccording to the genre and topics covered.
Wewill also perform a direct comparison, usingagreement statistics, of the collocations usedand of the results obtained.207This study has special significance at thispoint of word sense disambiguation research.
Arecent study (Agirre & Martinez, 2000)concludes that, for currently available hand-tagged data, the precision is limited to around70% when tagging all words in a running text.In the course of extending available data, theefforts to use corpora tagged by independentteams of researchers have been shown to fail(Ng et al, 1999), as have failed some tuningexperiments (Escudero et al, 2000), and anattempt to use examples automatically acquiredfrom the Internet (Agirre & Martinez, 2000).
Allthese studies obviated the fact that the examplescome from different genre and topics.
Futurework that takes into account he conclusionsdrawn in this paper will perhaps be able toautomatically extend the number of examplesavailable and tackle the acquisition problem.The paper is organized as follows.
Theresources used and the experimental settings arepresented first.
Section 3 presents thecollocations considered and Section 4 explainshow decision lists have been adapted to n-wayambiguities.
Sections 5 and 6 show the in-corpus and cross-corpora experiments,respectively.
Section 7 discusses the effect ofdrawing training and testing data from the samedocuments.
Section 8 evaluates the impact ofgenre and topic variations, which is fiartherdiscussed in Section 9.
Finally, Section 10presents some conclusions.1 Resources usedThe DSO collection (Ng and Lee, 1996) focuseson 191 frequent and polysemous words (nounsand verbs), and contains around 1,000 sentencesper word.
Overall, there are 112,800 sentences,where 192,874 occurrences of the target wordswere hand-tagged with WordNet senses (Milleret al, 1990).The DSO collection was built with examplesfrom the Wall Street Journal (WSJ) andBrown Corpus (BC).
The Brown Corpus isbalanced, and the texts are classified accordingsome predefined categories (el.
Table 1).
Theexamples from the Brown Corpus comprise78,080 occurrences of word senses, and theexamples from the WSJ 114,794 occurrences.The sentences in the DSO collection weretagged with parts of speech using TnT (Brants,2000) trained on the Brown Corpus itself.A.
Press: ReportageB.
Press: EditorialC.
Press: Reviews (theatre, books, music, dance)D. ReligionE.
Skills and HobbiesF.
Popular LoreG.
Belles Lettres, Biography, Memoirs, etc.H.
MiscellaneousJ.
LearnedK.
General FictionL.
Mystery and Detective FictionM.
Science FictionN.
Adventure and Western FictionP.
Romance and Love StoryR.
HumorTable 1: List of categories of texts from theBrown Corpus, divided into informative prose(top) and imaginative prose (bottom).1.1 Categories in the Brown Corpusand genre/topic variationThe Brown Corpus manual (Francis & Kucera,1964) does not detail the criteria followed to setthe categories in Table 1:The samples represent a wide range of  stylesand varieties of  prose...
The list of maincategories and their subdivisions was drawnup at a conference held at Brown Universityin February 1963.These categories have been previously used ingenre detection experiments (Karlgrcn &Cutting, 1994), where each category was usedas a genre.
We think that the categories not onlyreflect genre variations but also topic variations(e.g.
the Religion category follows topicdistinctions rather than genre).
Nevertheless weare aware that some topics can be covered inmore than one category.
Unfortunately there areno topically tagged corpus which also haveword sense tags.
We thus speak of genre andtopic variation, knowing that further analysiswould be needed to measure the effect of eachof them.2 Experimental settingIn order to analyze and compare the behavior ofseveral kinds of collocations (cf.
Section 3),Yarowsky (1993) used a measure of entropy aswell as the results obtained when tagging held-out data with the collocations organized asdecision lists (el.
Section 4) .
As Yarowskyshows, both measures correlate closely, so we208only used the experimental results of  decision Word PoS #Senses #Ex.
BC #Ex.
WSJlists.
Age N 5 243 248When comparing the performance on Art N 4 200 194decision lists trained on two different corpora Body N 9 296 110(or sub-corpora) we always take an equal Car N 5 357 1093amount of examples per word from each Child N 6 577 484corpora.
This is done to discard the amount-of- Cost N 3 317 1143data factor.
Head N 28 432 434As usual, we use 10-fold cross-validation Interest N 8 364 1115Line N 28 453 880when training and testing on the same corpus.
Point N 20 442 249No significance tests could be found for our State N 6 757 706comparison, as training and test sets differ.
Thing N 11 621 805Because of the large amount of experiments Work N 6 596 825involved, we focused on 21 verbs and nouns (el.
Become V 4 763 736Table 2), selected from previous works (Agirre Fall V 17 221 1227& Martinez, 2000; Escudero et al, 2000).
Grow V 8 243 731Lose V 10 245 935Set V 20 925 355Speak V 5 210 307Strike V 17 159 95Tell V 8 740 7443 Collocations consideredFor the sake of this work we take a broaddefinition of collocations, which were classifiedin three subsets: local content word collocations,local part-of-speech and function-wordcollocations, and global content-wordcollocations.
If a more strict linguisticperspective was taken, rather than collocationswe should speak about co-occurrence r lations.In fact, only local content word collocationswould adhere to this narrower view.We only considered those collocations thatcould be easily exlracted form a part of speechtagged corpus, like word to left, word to right,etc.
Local content word collocations comprisebigrams (word to left, word to right) andtrigrams (two words to left, two words to rightand both words to right and left).
At least one ofthose words needs to be a content word.
Localfunction-word collocations comprise also allkinds of bigrams and trigrams, as before, but thewords need to be function words.
Local PoScollocations take the Part of Speech of thewords in the bigrams and trigrams.
Finallyglobal content word collocations comprise thecontent words around the target word in twodifferent contexts: a window of 4 words aroundthe target word, and all the words in thesentence.
Table 3 summarizes the collocationsused.
These collocations have been used in otherword sense disambiguation research and are alsoreferred to as features (Gale et al, 1993; Ng &Lee, 1996; Escudero et al, 2000).Compared to Yarowsky (1993), who alsotook into account grammatical relations, weonly share the content-word-to-left and thecontent-word-to-right collocations.Table 2: Data for selected words.
Part ofspeech, number of senses and number ofexamples m BC and WSJ are shown.Local content word collocationsWord-to-left Content WordWord-to-right Content WordTwo-words-to-left At least oneTwo-words-to-right Content WordWord-to-right-and-leftLocal PoS and function word collocationsWord-to-left PoS Function WordWord-to-right PoS Function WordTwo-words-to-left PoS Both Function Two-words-to-fight PoS Words Word-to-fight-and-left PoSGlobal content word collocationsWord in Window of 4 Content Word Word in sentenceTable 3: Kinds of collocations consideredWe did not lemmatize content words, and wetherefore do take into account he form of thetarget word.
For instance, governing body andgoverning bodies are different collocations forthe sake of this paper.4 Adaptation of decision lists to n-wayambiguitiesDecision lists as defined in (Yarowsky, 1993;1994) are simple means to solve ambiguityproblems.
They have been successfully appliedto accent restoration, word" sense disambiguation209and homograph disambiguation (Yarowsky,1994; 1995; 1996).
In order to build decisionlists the training examples are processed toextract he features (each feature corresponds toa kind of collocation), which are weighted witha log-likelihood measure.
The list of all featuresordered by log-likelihood values constitutes thedecision list.
We adapted the original formula inorder to accommodate ambiguities higher thantwo:.
, Pr(sense i I features)  , weight(sensei ,  feature , )  = ~ogt-  )Pr(sense~ l feature , ),i=iWhen testing, the decision list is checked inorder and the feature with highest weight hat ispresent in the test sentence selects the winningword sense.
For this work we also considerednegative weights, which were not possible ontwo-way ambiguities.The probabilities have been estimated usingthe maximum likelihood estimate, smoothedusing a simple method: when the denominatorin the formula is 0 we replace it with 0.1.
It isnot clear how the smoothing technique proposedin (Yarowsky, 1993) could be extended to n-way ambiguities.More details of the implementation can befound in (Agirre & Martinez, 2000).5 In-corpus experiments:collocations are weak  (80%)We extracted the collocations in the BrownCorpus section of the DSO corpus and, using10-fold cross-validation, tagged the samecorpus.
Training and testing examples were thusfrom the same corpus.
The same procedure wasfollowed for the WSJ part.
The results areshown in Tables 4 and 5.
We can observe thefollowing:?
The best kinds of collocations are localcontent word collocations, especially if twowords from the context are taken intoconsideration, but the coverage is low.Function words to right and left also attainremarkable precision.?
Collocations are stronger in the WSJ, surelydue to the fact that the BC is balanced, andtherefore includes more genres and topics.This is a first indicator than genre and topicvariations have to be taken into account.?
Collocations for fine-gained word-senses aresensibly weaker than those reported byYarowsky (1993) for two-way ambiguouswords.
Yarowsky reports 99% precision,N V OverallCollocations Pr.
Cov.
Pr.
Cov.
Pr.
Coy.Word-to-righ~ .768.254.529.264 1680.258Word-to-left .724.185.867.182.775.184Two-words-to-righ1.784.191 .623.113.744.163Two-words-to-left.
811 .
160.862.179.830.166Word-to-right-and-left.820.169.728.129.793.155Word-to-righ1.600.457.527.370.577.426Word-to-left .545.609.629.472.570.560Two-words-to-righ1.638.133.687.084.650.116Two-words-to-left .600.140.657.108.617.128Word-to-right-and-left.721.220.694.138.714.191PoS-to-righ1.490.993.488.993.489.993PoS -to-left .465.991 .584.994.508.992Two- PoS -to-righ1.526.918.534.879.529.904Two- PoS -to-left .518.822.614.912.555.854PoS -to-right-and-left .555.918.634.891 .583.908O~daii~ib:~al;P~g,~.Fiifii~ !622 7o6 i64b:i~00 i629:Ii60Word in sentence .611 1.00.572 1.00.597 1.00Word in Window of 4.627.979.611.975.622.977OVERAM.
; : i::/::: i:~ .661i,L00,635I'.00.652:11200Table 4: Train on WSJ, tag WSJ.N V OverallCollocations Pr.
Coy.
Pr.
Cov.
Pr.
Cov.Word-to-right,644.203 4 2.230 .562.212Word-to-left,626.124 770.139 .681.129Two-words-to-right,657.146 500.103 ,613.131Two-words-to-left,740.092 ,819.122 ,774.103Word-to-right-and-left.647.088 686.114 .663.098Word-to-right 480.503 452.406 ,471.468Word-to-leA 414.639 572.527 :,464.599Two-words-to-right,520.183 624.113 ,547.158Two-words-to-left ,420.131 648.173 ,516.146Word-to-right-and-leg 549.238 654.160 ,577.210PoS4o-righ~ 340.992 356.992 i,346.992PoS -to-left,350.994 483.992 ,398.993Two- PoS -to-righ' 406.923 422.876 ,412.906Two- PoS -to-lef 396.792 539.897 i,452.829PoS -to-right-and-lef ,416.921 545.885 ,461.908Word in sentence 545 1.00 !.492 1.00 ,526 1.00Word in Window of 4 550.972 1.525.951 ,541.964Table 5: Train on BC, tag BC.while our highest results do not reach 80%.It has to be noted that the test and trainingexamples come from the same corpus, whichmeans that, for some test cases, there aretraining examples from the same document.
Insomesense we can say that one sense  perd i scourse  comes into play.
This point will befurther explored in Section 7.2101. state -- (the group of people comprising the government ofa sovereign)2. state, province-- (the territory occupied by one of the constituent administrative districts of a nation)3. state, nation, country, land, commonwealth, res publica, body politic-- (a politically organized body of people under a single government)4. state -- (the way something iswith respect o its main attributes)5.
Department of  State, State Department, State-- (the federal department that sets and maintains foreign policies)6. country, state, land, nation -- (the territory occupied by a nation)F igure  1: Word senses for state in WordNet 1.6 (6 out of  8 are shown)In the rest o f  this paper, only the overallresults for each subset of  the collocations will beshown.
We will pay special attention to local-content collocations, as they are the strongest,and also closer to strict definitions ofcollocation.As an example of  the learned collocationsTable 6 shows some strong local content wordcol locat ions for the noun state, and Figure 1shows the word senses of  state (6 out of  the 8senses are shown as the rest were not present inthe corpora).6 Cross-corpora experiments:one sense per col location in doubt.In these experiments we train on the BrownCorpus and tag the WSJ corpus and vice versa.Tables 7 and 8, when compared to Tables 4 and5 show a significant drop in performance (bothprecision and coverage) for all kind ofcollocations (we only show the results for eachsubset of  collocations).
For instance, Table 7shows a drop in .16 in precision for localcontent collocations when compared to Table 4.These results confirm those by (Escudero etal.
2000) who conclude that the informationlearned in one corpus is not useful to tag theother.In order to analyze the reason of  thisperformance degradation, we compared thelocal content-word collocations extracted fromone corpus and the other.
Table 9 shows theamount of  collocations extracted from eachcorpus, how many of  the collocations are sharedon average and how many of  the sharedcollocations are in contradiction.
The lowamount of  collocations shared between bothcorpora could explain the poor figures, but fo rsome words (e.g.
point) there is a worryingproportion of  contradicting collocations.We inspected some of  the contradictingcollocations and saw that m all the cases theywere caused by errors (or at least differingSensesCollocations Log #1 #2 #3 #4 #5 #6State government 3.68 - - 4six states 3.68 - - 4State's largest 3.68 - - 4State of emergency 3.68 - 4Federal, state 3.68 - - 4State, including 3.68 - - 4Current state of 3.40 - 3 -State aid 3.40 - 3State where Farmers 3.40 3State of rnind 3.40 3Current state 3.40 3State thrift 3.40 - 3Distributable state aid 3.40 - 3State judges 3.40 3a state court 3.40 3 -said the state 3.40 3Several states 3.40 - 3State monopolies 3.40 - 3State laws 3.40 3State aid bonds 3.40 - 3 -Distributable state 3.40 - 3State and local 2.01 1 1 15Federal and state 1.60 1 5 -State court 1.38 - 12 3 -Other state.
1.38 4 1 -State$overnments 1.09 1 3 -Table  6: Local content-word collocations forState in WSJCollocations Pr.Overall ocal content .597Overall ocal PoS&Fun .478Overall global content .442OVERALL .485N V \[OverallCov.
Pr.
Cov.
Pr.
Cov..338 591 .356 595 .344.999 ,491 .997 483 .9981.00:455 .999 .447 1.001.00 497 1.00 489 1.00Tab le  7: Train on BC, tag WSJN V i OverallCollocations Pr.
Cov.
Pr.
Cov.
i Pr.
Cov.Overall ocal content 512 .273 .556 .336 530 .295Overall local PoS&Fun 421 1.00 .486 1.00 44.4 1.00Overall global content !.392 1.00 .423 1.00 403 1.00OVERALL 429 1.00 .483 1.00 448 1.00Tab le  8: Train on WSJ, tag BC211criteria) of the hand-taggers when dealing withwords with difficult sense: distinctions.
Forinstance, Table 10 shows some collocations ofpoint which receive contradictory senses in theBC and the WSJ.
The collocation importantpoint, for instance, is assigned the second sense Iin all 3 occurrences in the 13C, and the fourthsense 2in all 2 occurrences in the WSJ.We can therefore conclude that the one senseper collocation holds across corpora, as thecontradictions found were due to tagging errors.The low amount of collocations in commonwould explain in itself the low figures on cross-corpora tagging.But yet, we wanted to further study thereasons of the low number of collocations incommon, which causes the low cross-corporaperformance.
We thought of several factors thatcould come into play:a) As noted earlier, the training and testexamples from the in-corpus experiments aretaken at random, and they could be drawnfrom the same document.
This could makethe results appear better for in-corporaexperiments.
On the contrary, in the cross-corpora experiments training and testingexample come from different documents.b) The genre and topic changes caused by theshift from one corpus to the other.c) Corpora have intrinsic features that carmotbe captured by sole genre and topicvariations.d) The size of the data, being small, wouldaccount for the low amount of collocationsshared.We explore a) in Section 7 mad b) in Section 8.c) and d) are commented in Section 8.7 Drawing training and testingexamples from the same documentsaffects performanceIn order to test whether drawing training andtesting examples from the same document or notexplains the different performance in in-corporaand cross-corpora tagging, low cross-corporaresults, we performed the following experiment.Instead of organizing the 10 random subsets forcross-validation on the examples, we choose 10subsets of the documents (also at random).
Thisi The second sense of point is defined as the preciselocation of something; a spatially limited location.2 Defined as an isolated fact that is consideredseparately from the whole.# Coll.
# Coll.
% Coil % Coll.
Word PoS BC WSJ Shared Contradict.Age N 45 60 27 0Art N 24 35 34 20Body N 12 20 12 0Car N 92 99 17 0Child N 77 111 40 05Cost N 88 88 32 0Head N 77 95 07 33Interest N 80 141 32 33Line N 110 145 20 38Point N 44 44 32 86State N 196 214 28 48Thing N 197 183 66 52Work N 112 149 46 63Become V 182 225 51 15Fall V 36 68 19 60Grow V 61 71 36 33Lose V 63 56 47 43Set V 94 113 54 43Speak V 34 38 28 0Strike V 12 17 14 0Tell V 137 190 45 57Table 9: Collocations hared and mcontradiction between BC and WSJ.BC WSJ Collocation #2 #4 Other #2 #4 Otherimportant point 3 0 0 0 2 0pointofview 1 13 1 19 0 0Table 10: Contradictory senses of pointway, the testing examples and training examplesare guaranteed to come from differentdocuments.
We also think that this experimentwould show more realistic performance figures,as a real application can not expect to findexamples from the documents used for training.Unfortunately, there are not any explicitdocument boundaries, neither in the BC nor inthe WSJ.In the BC, we took files as documents, evenif files might contain more than one excerptfrom different documents.
This guarantees thatdocument boundaries are not crossed.
It has tobe noted that following this organization, thetarget examples would share fewer examplesfrom the same topic.
The 168 files from the BCwere divided in 10 subsets at random: we took 8subsets with 17 files and 2 subsets with 16 files.For the WSJ, the only cue was the directoryorganization.
In this case we were unsure aboutthe meaning of this organization, but handinspection showed that document boundarieswere not crossing discourse boundaries.
The 61directories were divided in 9 subsets with 6directories and 1 subset with 7.212Again, 10-fold cross-validation was used, onthese subsets and the results in Tables 11 and 12were obtained.
The ,5 column shows the changein precision with respect to Tables 5 and 6.Table 12 shows that, for the BC, precisionand coverage, compared to Table 5, aredegraded significantly.
On the contrary resultsfor the WSJ are nearly the same (el.
Tables 11and 4).The results for WSJ indicate that drawingtraining and testing data from the same ordifferent documents in itself does not affect somuch the results.
On the other hand, the resultsfor BC do degrade significantly.
This could beexplained by the greater variation in topic andgenre between the files in the BC corpus.
Thiswill be further studied in Section 8.Table 13 summarizes the overall results onWSJ and BC for each of the differentexperiments performed.
The figures show thatdrawing training and testing data from the sameor different documents would not in any caseexplain the low figures in cross-corpora t gging.8 Genre and topic variation affectsperformanceTrying to shed some light on this issue weobserved that the category press:reportage, isrelated to the genre/topics of the WSJ.
Wetherefore designed the following experiment: wetagged each category in the BC with thedecision lists trained on the WSJ, and also withthe decision lists trained on the rest of thecategories in the BC.Table 14 shows that the local content-wordcollocations trained in the WSJ attain the bestprecision and coverage for press:reportage,both compared to the results for the othercategories, and to the results attained by the restof the BC on press:reportage.
That is:?
From all the categories, the collocations frompress:reportage are the most similar to thoseof WSJ.?
WSJ contains collocations which are closerto those of press:reportage, than those fromthe rest of the BC.In other words, having related genre/topics helphaving common collocations, and therefore,warrant better word sense disambiguationperformance.Overall Localcontentpr.
coy.
Apr.
pr.
cov.
Apr.N .650 1.00 -.011 .762 .486 -.002V .634 1.00 -.001 .697 .494 -.040Overall .644 1.00 -.011 .738 .489 -.017Table 11: Train on WSJ, tag WSJ,crossvalidation according to filesOverall Local contentpr.
cov.
Apr.
pr.
cov.
Apr.N .499 1.00 -.078 .573 .307 -.102V .543 1.00 -.021 .608 .379 -.027Overall .514 1.00 -.058 .587 .333 -.074Table 12: Train on BC, tag BC,crossvalidation according to filesOverall (prec.
)In-corpora In-corpora(examples) (files) Cross-corporaWSJ .652 .644 .489BC .572 .514 .448Table 13: Overall results in differentexperimentsCategoryWSJ Rest of BClocal content local contentpr.
coy.
pr.
cov.Press: Reportage .625 .330 .541 .285Press: Editorial .504 .283 .593 .334Press: Reviews .438 .268 .488 .404Religion .409 .306 .537 .326Skills and Hobbies .569 .296 .571 .302Popular Lore .488 .304 .563 .353Belles Lettres .
.
.
.
.
516 .272 .524 .314Miscellaneous .534 .321 .534 .304Learned .518 .257 .563 .280General Fiction .525 .239 .605 .321Mystery and .
.
.
.
523 .243 .618 .369Science Fiction .459 .211 .586 .307Adventure and .
.
.
.
551 .223 .702 .312Romance and .
.
.
.
561 .271 .595 .340Humor .516 .321 .524 .337Table 14: Tagging different categories in BC.Best precision results are shown in bold.9 Reasons for cross-corpor a degradationThe goal of sections 7 and 8 was to explore thepossible causes for the low number ofcollocations in common between BC and WSJ.Section 7 concludes that drawing the examplesfrom different files is not the main reason forthe degradation.
This is specially true when thecorpus has low genre/topic variation (e.g.
WSJ).Section 8 shows that sharing enre/topic s a keyfactor; as the WSJ corpus attains better esultson the press:reportage category than the rest of213the categories on the BC itself.
Texts on thesame genre/topic share more collocations thantexts on disparate genre/topics, even if theycome from different corpora.This seems to also rule out explanation c)(cf.
Section 6), as a good measure of topic/genresimilarity would help overcome cross-corporaproblems.That only leaves the low amount of dataavailable for this study (explanation d).
It is truethat data-scarcity can affect the number ofcollocations hared across corpora.
We thinkthat larger amounts will make', this number grow,especially if the corpus draws texts fromdifferent genres and topics.
Nevertheless, thefigures in Table 14 indicate that even in thoseconditions genre/topic relatedness would help tofind common collocations.10 -ConclusionsThis paper shows that the one sense percollocation hypothesis is weaker for fine-grained word sense distinctions (e.g.
those inWordNet): from the 99% precision mentionedfor 2-way ambiguities in (Yarowsky, 1993) wedrop to 70% figures.
These figures couldperhaps be improved using more available data.We also show that one sense per collocationdoes hold across corpora, but that collocationsvary from one corpus to other, following genreand topic variations.
This explains the lowresults when performing word sensedisambiguation across corpora.
In fact, wedemonstrated that when two independentcorpora share a related genre/topic, the wordsense disambiguation results would be better.This has considerable impact in future workon word sense disambiguation, as genre andtopic are shown to be crucial parameters.
Asystem trained on a specific genre/topic wouldhave difficulties to adapt to new genre/topics.Besides, methods that try to extendautomatically the amount of examples fortraining need also to account for genre and topicvariations.As a side effect, we have shown that theresults on usual WSD exercises, which mixtraining and test data drawn from the samedocuments, are higher than those from a morerealistic setting.We also discovered several hand-taggingerrors, which distorted extracted collocations.We did not evaluate the extent of these errors,but they certainly affected the performance oncross-corpora t gging.Further work will focus on evaluating theseparate weight of genre and topic in word sensedisambiguation performance, and on studyingthe behavior of each particular word andfeatures through genre and topic variations.
Weplan to devise ways to integrate genre/topicparameters into the word sense disambiguationmodels, and to apply them on a system toacquire training examples automatically.ReferencesAgirre, E. and D. Martinez.
Exploring automaticword sense disambiguation with decision lists andthe Web.
Proceedings of the COLING Workshopon Semantic Annotation and Intelligent Content.Saarbrticken, Germany.
2000.Brants, T. TnT- A Statistical Part-of-Speech Tagger.In Proceedings of the Sixth Applied NaturalLanguage Processing Conference, Seattle, WA.2000.Escudero, G. , L. Mhrquez and G. Rigau.
On thePortability and Tuning of Supervised Word SenseDisambiguation Systems.
In Proceedings of theJoint Sigdat Conference on Empirical Methods inNatural Language Processing and Very LargeCorpora, Hong Kong.
2000.Francis, W. M. and H. Kucera.
Brown CorpusManual oflnformation.
Department ofLinguistics,Brown University.
Also available athttp://khnt.hit.uib.no/icame/manuals/brown/.
1964.Gale, W., K. W. Church, and D. Yarowsky.
AMethod for Disambiguating Word Senses in aLarge Corpus, Computers and the Humanities, 26,415--439, 1993.Ide, N. and J. Veronis.
Introduction to the SpecialIssue on Word Sense Disambiguation: The State ofthe Art.
Computational Linguistics, 24(1), 1--40,1998.Karlgren, J. and D. Cutting.
Recognizing Text Genreswith Simple Metrics Using Discriminant Analysis.Proceedings of the International Conference onComputational Linguistics.
1994Krovetz, R. More Than One Sense Per Discourse,Proceedings of SENSEVAL and the LexicographyLoop Workshop.
http://www.itri.brighton.ac.uk/events/senseval/PROCEEDINGS/.
1998Leacock, C., M. Chodorow, and G. A. Miller.
UsingCorpus Statistics and WordNet Relations for SenseIdentification.
Computational Linguistics, 24(1),147--166, 1998.Miller, G. A., R. Beckwith, C. Fellbaum, D. Gross,and K. Miller.
Five Papers on WordNet.
SpecialIssue of International Journal of Lexicography,3(4), 1990.214Miller, G. A., C. Leacock, R. Tengi, and R. T.Bunker, A Semantic Concordance.
Proceedings ofthe ARPA Workshop on Human LanguageTechnology, 1993.Ng, H. T. and H. B. Lee.
Integrating MultipleKnowledge Sources to Disambiguate Word Sense:An Exemplar-based Approach.
Proceedings of the34th Annual Meeting of the Association forComputational Linguistics.
1996.Ng, H. T., C. Y. Lira and S. K. Foo.
A Case Study onInter-Annotator Agreement for Word SenseDisambiguation.
Proceedings of the Siglex-ACLWorkshop on Standarizing Lexical Resources.1999.Yarowsky, D. One Sense per Collocation.
Proc.
ofthe 5th DARPA Speech and Natural LanguageWorkshop.
1993Yarowsky, D. Decision Lists for Lexical AmbiguityResolution: Application to Accent Restoration inSpanish and French.
Proceedings of the 32ridAnnual Meeting of the Association forComputational Linguistics, pp.
88--95.
1994.Yarowsky, D. Unsupervised Word SenseDisambiguation Rivaling Supervised Methods.Proceedings of the 33rd Annual Meeting of theAssociation for Computational Linguistics.Cambridge, MA, pp.
189-196, 1995.Yarowsky, D. Homograph Disambiguation in Text-to-speech Synthesis.
J Hirschburg, R. Sproat and J.Van Santen (eds.)
Progress in Speech Synthesis,Springer-Vorlag, pp.
159-175.
1996.215
