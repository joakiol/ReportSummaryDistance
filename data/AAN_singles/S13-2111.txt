Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 675?683, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsSCAI: Extracting drug-drug interactions using a rich feature vectorTamara Bobic?1,2, Juliane Fluck1, Martin Hofmann-Apitius1,21Fraunhofer SCAISchloss Birlinghoven53754 Sankt AugustinGermany2B-IT, Bonn Universita?tDahlmannstra?e 253113 BonnGermany{tbobic, jfluck, hofmann-apitius}@scai.fraunhofer.deAbstractAutomatic relation extraction provides greatsupport for scientists and database curators indealing with the extensive amount of biomed-ical textual data.
The DDIExtraction 2013challenge poses the task of detecting drug-drug interactions and further categorizingthem into one of the four relation classes.
Wepresent our machine learning system whichutilizes lexical, syntactical and semantic basedfeature sets.
Resampling, balancing and en-semble learning experiments are performed toinfer the best configuration.
For general drug-drug relation extraction, the system achieves70.4% in F1 score.1 IntroductionDrug-drug interactions (DDI) describe possible in-terference between pharmacological substances andare of critical importance in drug development andadministration (August et al 1997).
A drug mayalter the metabolism of another, thus causing an en-hanced, reduced or even toxic effect in certain med-ical treatments.
For example: ?Fluvoxamine in-hibits the CYP2C9 catalyzed biotransformation oftolbutamide.?
Automated extraction of DDI frombiomedical literature allows for a more efficientmaintenance of the drug knowledge databases andis beneficial for patients, health care professionalsand the pharmaceutical industry.Having in mind their biomedical importance, theobjective of the first DDIExtraction challenge1 in1http://labda.inf.uc3m.es/DDIExtraction2011/2011 was to motivate the development and to eval-uate the automatic relation extraction (RE) systemsfor DDI.
Given annotated drug entities, the partic-ipants addressed the task of identifying undirectedbinary relations among them.
The knowledge ex-traction was performed on the sentence level and thebest system achieved 65.74% F1 score (Thomas etal., 2011a).The 2013 DDIExtraction challenge2 (organizedas Task 9 of SemEval 2013 (Segura-Bedmar et al2013)) is based on a similar task definition, but ad-ditionally includes the disambiguation between fourtypes of interaction: mechanism, effect, advise andint.
The evaluation of participating systems is two-fold, i. e. partial and strict.
Partial evaluation con-siders that a prediction is correct when the pair la-bel matches the gold annotation, while strict eval-uation requires also a correct relation type to beassigned.
The train and test corpora were gener-ated from textual resources of DrugBank (Knox etal., 2011) database and MedLine3 abstracts, dealingwith the topic of DDI.In the following sections we describe our super-vised machine learning based approach for the ex-traction of DDI, using a rich feature vector (see Sec-tion 2.1).
The base system employed LibLINEARclassifier, generating the first run submitted to theDDIExtraction challenge.
Configurations comingfrom the two ensemble strategies (Section 2.2) pro-duced the remaining prediction runs.
Furthermore,we experimentally investigated the impact of train2http://www.cs.york.ac.uk/semeval-2013/task9/3http://www.ncbi.nlm.nih.gov/pubmed/675corpora imbalance on DDI detection through resam-pling strategies (Section 2.3).
Finally, relation typedisambiguation methodology is presented in Sec-tion 2.4.2 MethodsWe formulate the task of relation extraction asfeature-based classification of co-occurring entitiesin a sentence.
A sentence with n entities contains atmost(n2)interacting pairs.
For entity pairs that theclassifier detects as ?true?, a post-processing step isperformed where one of the four relation types isassigned, depending on the identified type-specifictrigger words.2.1 FeaturesTo improve generalization of lexical informationPorter stemming algorithm (Porter, 1980) was ap-plied.
All entities present in the sentence, whichwere not a part of the investigated pair, are renamedto a common neutral name (entity blinding).For the generation of dependency-based features,sentences in the provided corpora were parsed usingCharniak-Lease parser (Lease and Charniak, 2005;Thomas et al 2011b).
The resulting constituentparse trees were converted into Stanford dependencygraphs (Marneffe et al 2006).
Following the idea ofThomas et al(2011b), similar relations are treatedequally by using their common parent type (unifica-tion of dependency types).
An example is generaliz-ing relations ?subj?, ?nsubj?
and ?csubj?
to a parentrelation ?subj?.In the following subsections the three groups offeatures (lexical, syntactical and semantic) with theircorresponding members are described.
Table 1 givesa more structured overview of the feature vector, or-ganized by type.
It should be noted that the listedfeatures are used for the generation of all three pre-diction sets submitted to the DDI challenge.2.1.1 Lexical featuresLexical features capture the token informationaround the inspected entity pair (EP).
The sentencetext is divided into three parts: text between the EP,text before the EP (left from the first entity) and textafter the EP (right from the second entity).
It hasbeen observed that much of the relation informationcan be extracted by only considering these three con-texts (Bunescu and Mooney, 2005b; Giuliano et al2006).The majority of features are n-grams based, withn ?
{1, 2, 3}.
They encompass a narrow (win-dow=3) and wide (window=10) surrounding con-text, along with the area between the entities.
Addi-tionally, combinations of the tokens from the threeareas is considered, thus forming before-between,between-after and before-after conjunct features(narrow context).2.1.2 Syntactic/Dependency featuresVertices (v) in the dependency graph are analyzedfrom a lexical (stemmed token text) and syntacti-cal (POS tag) perspective, while the edges (e) areincluded using the grammatical relation they repre-sent.The majority of dependency-based features areconstructed using the properties of edges and ver-tices along the shortest path (SP) of an entity pair.The shortest path subtree is conceived to encodegrammatical relations with highest information con-tent for a specific EP (Bunescu and Mooney, 2005a).Similarly to lexical features, n-grams of vertices(edges) along the SP are captured.
Furthermore, al-ternating sequences of vertices and edges (v-walksand e-walks) of length 3 are accounted for, follow-ing previous work (Kim et al 2010; Miwa et al2010).Apart from the SP-related features, incorporat-ing information about the entities?
parents and theircommon ancestor in the dependency graph is alsobeneficial.
The lexical and syntactical properties ofthese vertices are encoded, along with the grammat-ical relations on the path from the entities to theircommon ancestor.2.1.3 Semantic featuresSemantic group of features deals with understand-ing and meaning of the context in which a particularentity pair appears.A feature that accounts for hypothetical state-ments was introduced in order to reduce the num-ber of false positives (phrases that indicate investi-gation in progress, but not actual facts).
Negation(e. g.
?not?)
detected close to the entity pair (narrowcontext) along with a check whether entities in the676pair refer to the same real-word object (abbreviationor a repetition) represent features which also con-tribute to the reduction of false positive predictions.Drug entities in the corpora were annotated withone of four classes (drug, drug n, brand, group),which provided another layer of relation informa-tion for the classifier.
Prior knowledge about trueDDI coming from the train corpora is used as a fea-ture, if a previously known EP is observed in the testdata.
Presence of other entities (which are not partof the inspected EP) in the sentence text is captured,together with their position relative to the EP.Finally, mentions of general trigger (interaction)terms are checked in all three context areas.
More-over, interaction phrases specific to a certain DDItype (see Section 2.4) are accounted for.2.2 Ensemble learningCombining different machine learning algorithmswas proposed as a direction for improvement of theclassification accuracy (Bauer and Kohavi, 1999).A synthesis of predictions using LibLINEAR,Na?
?ve Bayes and Voting Perceptron classifiers is anattempt to approach and learn the relation informa-tion from different angles with a goal of increasingthe system?s performance.
The three base models in-cluded in the ensemble are employed through theirWEKA4 (Hall et al 2009) implementation with de-fault parameter values and trained on the full featurevector described in Section 2.1.LibLINEAR (Fan et al 2008) is a linear supportvector machine classifier, which has shown high per-formance (in runtime as well as model accuracy) onlarge and sparse data sets.
Support vector machines(SVM, Cortes and Vapnik (1995)) have gained a lotof popularity in the past decade and very often arestate-of-the-art approach for text mining challenges.Na?
?ve Bayes (Domingos and Pazzani, 1996) isa simple form of Bayesian networks which relieson the assumption that every feature is independentfrom all other features.
Despite their naive designand apparently oversimplified assumptions, Na?
?veBayes can often outperform more sophisticated clas-sification methods and has worked quite well inmany complex real-world situations.
Furthermore,it can be robust to noise features and is quite insen-4http://www.cs.waikato.ac.nz/ml/weka/Corpus Pos Neg TotalMedLine 232 (0.13) 1,555 (0.87) 1,787DrugBank 3,788 (0.15) 22,217 (0.85) 26,005Table 2: Ratio of positive and negative instances in theDrugBank and MedLine train corpora.sitive to stratification (Provost, 2000), which is ofhigh value in class imbalance scenarios.Voting Perceptron (Freund and Schapire, 1999)combines a series of perceptrons, which are lin-ear classification algorithms that process elementsin the train set one at a time (?online?).
The sys-tem stores the number of iterations the perceptron?survives?, i. e. when the training set instances areclassified correctly.
The obtained count represents aweight used for combining the prediction vectors bya weighted majority vote.In the ensemble learning scenario we considertwo strategies that aim at increasing the system?sperformance by either favoring precision or recall:1.
?majority?
?
a pair represents true relation onlyif majority of the classifiers support that claim2.
?union?
?
a pair represents true relation if atleast one of the classifiers supports that claim2.3 Train corpora imbalanceAnalysis of the basic train corpora statistics re-veals an unequal ratio of positive and negative in-stances, i. e. under-representation of true interactingpairs (see Table 2).
Class distribution imbalance of-ten causes machine learning algorithms to performpoorly on the minority class (Hulse et al 2007),thus, in this case, affecting the recall of true rela-tions.In order to explore the sensitivity of our systemto the positive/negative ratio, we performed randomundersampling of the data, artificially obtaining adesirable ratio (50-50).
All positive instances in thedataset were kept, while the same number of neg-ative instances were randomly chosen.
The reverseapproach of oversampling was considered, but giventhe ample train data provided by the organizers, suchstrategy could pose run-time challenges.The experimental setting is described as follows.MedLine and DrugBank train corpora were dividedfurther into train (exp-train) and test (exp-test) sets,677FeatureLexical1.
n-grams of tokens between the EP2.
n-grams of tokens before the EP (narrow context, window = 3)3. n-grams of tokens after the EP (narrow context, window = 3)4. n-grams of tokens before the EP (wide context, window = 10)5. n-grams of tokens after the EP (wide context, window = 10)6. conjucted positions: before-between, between-after and before-afterSyntactical/Dependency7.
dependency n-grams on the SP8.
syntactical n-grams on the SP9.
lexical n-grams on the SP10.
lexical and syntactical e-walks11.
lexical and syntactical v-walks12.
SP length (number of edges)13. lexical and syntactical information of the entities?
parents14.
lexical and syntactical information of the entities?
common ancestor15.
dependency n-grams from both entities to their common ancestor16.
common ancestor represents a verb or a nounSemantic17.
hypothetical context18.
negation close to the EP19.
entities refer to the same object20.
type of entities that form the EP21.
prior knowledge (from the train data)22. other entities present close to the EP23.
DDI trigger words (general)24.
DDI types trigger words (specific)Table 1: Overview of features used, stratified into groups.
EP denotes an entity pair, SP represent the shortest path.27.5?64?36.4?
36.9?41.2?55.9?41.1?69.5?43.8?69.3?46?62?51.6?72?0?10?20?30?40?50?60?70?80?MedLine?
DrugBank?F1??lex?
sem?
syn?
lex+sem?
lex+syn?
sem+syn?
lex+sem+syn?Figure 1: Contribution of individual feature sets and their combinations to the system?s performance, evaluated by 10-fold cross-validation on the train corpora.
Lex is an abbreviation for lexical, sem for semantic and syn for syntacticalfeatures.678Corpus Exp-train pairs Exp-test pairsMedLine 1,259 (70.4%) 528 (29.6%)DrugBank 18,148 (69.8%) 7,857 (30.2%)Table 3: Experimental train and test subsets derived fromthe MedLine and DrugBank train corpora.Relation MedLine DrugBankmechanism 62 (0.27) 1257 (0.33)effect 152 (0.66) 1535 (0.41)advise 8 (0.03) 818 (0.21)int 10 (0.04) 178 (0.05)Table 4: The number of positive pairs for different DDItypes in the train corpora.
Ratios are given in brackets.with an approximate ratio of 70-30.
Instances froma particular document were always sampled to thesame subset, in order to avoid information leakage.Table 3 gives an overview of the number of entitypairs each set comprises.
The exp-train corpora wereused for training the model in an original (full-size)and balanced (subsample) scenario, evaluated on theexp-test sets.It should be noted that undersampling experi-ments were performed on the train corpora in orderto inspect the impact of data imbalance on our sys-tem (results shown in Section 3.4).
However, dueto the challenge limitation of submitting only threeruns, this configuration was ignored in favor of uti-lizing the complete train corpora.2.4 Relation type assignmentThe DDIExtraction challenge guidelines specifyfour classes of relations: advise, mechanism, effectand int.
Table 4 illustrates the ratio of positive pairsassigned to each type in MedLine and DrugBanktrain corpora.In Section 2.4.1, a brief outlook on the interactiontype characteristics is given, along with some of themost common relation (trigger) phrases specific tothem.
Section 2.4.2 explains the methodology be-hind the process of relation type assignment.2.4.1 Relations overviewAdvise pertains to recommendations regarding co-administration of two or more drugs.
Sentences de-scribing these relations usually contain words suchas: should, recommended, advisable, caution, avoidetc., as seen in the following examples:?
Barbiturates and glutethimide should not beadministered to patients receiving coumarindrugs.?
Concurrent therapy with ORENCIA and TNFantagonists is not recommended.?
The co-administration of Fluvoxamine Tabletsand diazepam is generally not advisable.Effect is a relation type describing the signs orsymptoms linked to the DDI, including the phar-macodynamic effect, i. e. mechanism of interaction.Some of the phrases often found to denote this typeof relation are: effect, cause, decrease, increase, in-hibit, activate, modulate etc.
The following exam-ples present expressions of an effect relation:?
Pretreatment of megakaryocytes with extracel-lular RR (50 microM) also inhibited InsP(3)-induced responses.?
It is concluded that neurotensin modulates inan opposite way the function of the enkephalin-ergic neurons and the central action of tuftsin.?
Diazepam at doses of 0.25 mg/kg and 2.5mg/kg injected with morphine was found todecrease the antinociceptive effect of mor-phine.Mechanism illustrates a more detailed descriptionof the observed pharmacokinetic changes that in-cludes biochemical information about metabolism,absorption, biotransformation, excretion etc.
Mech-anism relations often include mentions of effect-related interaction phrases, but provide an additionalknowledge layer by addressing more complex bio-logical concepts:?
Cholestyramine, an anionic-binding resin, hasa considerable effect in lowering the rate andextent of fluvastatin bioavailability.?
Additional iron significantly inhibited theabsorption of cobalt in both dietary cobalttreatments.?
Macrolide antibiotics inhibit the metabolism ofHMG-CoA reductase inhibitors that are me-tabolized by CYP3A4.679Int relation implies sentences which only statethat an interaction occurs, without providing muchadditional information about it.
Trigger phrases thatcan be found in such sentences are usually limited todifferent lexical forms of ?interaction?:?
Rifampin and warfarin: a drug interaction.?
In vitro interaction of prostaglandin F2alphaand oxytocin in placental vessels.?
Treatment with antidepressant drugs can di-rectly interfere with blood glucose levels ormay interact with hypoglycemic agents.2.4.2 Type disambiguation methodologyWe approach the problem of relation type disam-biguation as a post-processing step, utilizing identi-fied (sentence level) trigger words as classificationdeterminants.
Precompiled relation trigger lists aregenerated by manual inspection of the train corpora,largely focusing on MedLine.
The lists are specificto the four interaction types and non-overlapping.Cases when a sentence contains trigger phrasesfrom different relation classes are resolved by fol-lowing a priority list:1. advise2.
mechanism3.
effect4.
intThe rationale behind such priority assignment arethe following observed patterns in the train corpora.Regardless of effect or mechanism connotation, ifthe sentence contains recommendation-like phrases(e. g.
?should?, ?advisable?
), it is almost alwaysclassified as an advise.
Likewise, even though a re-lation might be describing an effect, if it containsa more detailed biochemical description, it is mostlikely representing mechanism.
Finally, effect hasadvantage over int due to the simplicity of the intrelation, along with the lowest observed frequency.3 Results and Discussion3.1 Baseline relation extraction performancePerformances of the submitted prediction runs areshown in Table 5, where the first row (run1) repre-sents a system trained on the original (unbalanced)train corpora, using LibLINEAR classifier and a richfeature vector (see Section 2.1).
The table offers re-sults overview on MedLine, DrugBank and joinedtest corpora (?All?
), using partial evaluation (generalDDI detection).The difference in performance on MedLine andDrugbank is apparent, measuring up to almost 25percentage points (pp) in F1 score (46.2% for Med-Line and 71.1% for DrugBank).
Due to a consid-erably larger size of the DrugBank corpus, overallresults are greatly influenced by this corpus (F1 =69.0%).The results imply system?s sensitivity towardsclass imbalance, which manifests in favored preci-sion over recall.
However, this discrepancy is muchless observed on DrugBank test corpus.
Despite thesimilarity in class ratio, DrugBank is a more com-pact and homogenous corpus, with a relatively uni-fied writing style.
Coming from a manually curateddatabase, it has a rather standardized way of describ-ing interactions, resulting in higher performance ofthe relation extraction system.
MedLine corpora,however, are derived from different journals and re-search groups which gives rise to extremely diversewriting styles and a more challenging task for infor-mation extraction.3.2 Features contributionFigure 1 illustrates the performance of the LibLIN-EAR classifier, when all combinations of the threedifferent feature sets are explored.It can be observed that the highest performance isalways achieved when all the features are includedduring training (lex+syn+sem), resulting in 51.6%and 72.0% F1 score for 10-fold cross-validation onMedLine and DrugBank train corpora respectively.Lexical features appear to be most useful for theDrugBank corpus, achieving 88.9% of the maxi-mum performance when used solely.
MedLine, onthe other hand, benefits the most from syntactic fea-tures that reach 79.8% of the best result, compared to53.3% with lexical features.
Semantic group of fea-tures exhibits a uniform performance for both cor-pora, achieving 36.4% and 36.9% of F1 score.
Fi-nally, grouping of two or all three feature sets isalways beneficial and results in higher performancethan the constituting base configurations.680MedLine DrugBank AllClassifier P R F1 P R F1 P R F1run1: LibLINEAR 68.8 34.7 46.2 83.6 61.9 71.1 82.6 59.2 69.0run2: Majority 68.6 25.3 36.9 83.7 61.7 71.0 82.9 58.1 68.3run3: Union 43.1 52.6 47.4 79.6 68.1 73.4 74.8 66.6 70.4Table 5: Results of the three submitted runs on the test corpora.Classifier DrugBank MedLineLibLinear 654 48Na?
?ve Bayes 854 88V.
Perceptron 608 30Majority 693 35Union 980 116Table 6: Number of positive predictions on MedLine andDrugBank test corpora, using different configurations.3.3 Ensemble experimentsPerformance of the majority and union ensembleconfigurations on the test corpora is presented in Ta-ble 5.
Table 6 gives an overview of the number ofpredicted positive pairs by the ensemble, as well asthose by the individual base classifiers.Voting Perceptron behaves similarly to LibLin-ear, while Na?
?ve Bayes demonstrates insensitivityin terms of class imbalance, predicting the high-est number of positive pairs for both MedLine andDrugBank test corpora.Union voting strategy tends to overcome the lim-itations of poor recall, resulting in highest perfor-mance on all test corpora (47.4% for MedLine,73.4% for DrugBank and 70.4% for All) among thethree runs.
The superior result is obtained by dimin-ishing precision in favor or recall, which was shownas beneficial in these use-cases.
However, the F1score difference is slight (1.2 pp, 2.3 pp and 1.4 pp),as compared to the baseline system (run1).Predictions using the union ensemble ranked 3rdin the general DDI extraction evaluation, achieving5.5 pp and 9.6 pp of F1 score less than the top twoparticipating teams.MedLine DrugBankTrain set P R F1 P R F1original 48.4 39.6 43.6 75.1 62.4 68.2balanced 37.2 70.4 48.7 60.8 72.7 66.2Table 7: Comparison of results on the full train set anda balanced subsample, as evaluated on the MedLine andDrugBank train corpora.3.4 Balanced training corporaTable 7 presents relation extraction performance fortraining on a balanced subset, compared to the orig-inal unbalanced corpus.In case of MedLine, an increase of around 5 ppin F1 score can be observed for the balanced sub-sample.
However, given a relatively high initial per-formance on DrugBank and the characteristics ofthat corpus, training on a subsample results in 2 ppreduced F1 score.
The raise of 30.8 pp in recallcontributes greatly to the increased performance onMedLine, even though 11.2 pp of precision are lost.However, in case of DrugBank, a 10.3 pp increasein recall is not enough to compensate for the 14.3 pploss in precision.It can be observed that although undersamplingapproach removes information from the model train-ing stage, the class balance plays a more significantrole for the final performance.3.5 Relation type disambiguationCorrect classification of interacting pairs into fourdefined classes was evaluated using macro and mi-cro average measures.While micro-averaged F1 score is calculated byconstructing a global contingency table and thencalculating precision and recall, macro-averaged F1score is obtained by first calculating precision andrecall for each relation type and then taking their681MedLine DrugBank AllP R F1 P R F1 P R F1micro avg.
62.5 31.6 42.0 51.3 43.9 47.3 55.1 39.5 46.0macro avg.
42.0 19.7 26.9 66.5 35.3 46.1 66.6 33.8 44.8mechanism 70.0 29.2 41.2 58.0 39.2 46.8 53.2 39.1 45.0effect 64.7 35.5 45.8 52.4 44.6 48.2 48.8 43.9 46.2advise 18.2 28.6 22.2 50.7 65.0 57.0 50.5 63.3 56.2int 0 0 0 100 1.1 2.1 100 1.0 2.1Table 8: Results of DDI extraction when relation class detection is evaluated.average (Segura-Bedmar et al 2013).
Therefore,macro average takes into consideration the relativefrequency of each interaction class, while micro av-erage treats all classes equally.Table 8 shows an overview of performances forDDI extraction with relation class disambiguation,evaluated for each type separately, as well as cumu-latively using micro and macro scores.
For Med-Line test corpus, the micro average F1 score of 42%ranked 1st among all participating systems.
How-ever, the macro average score is much lower, due topoor performance on advise and int relation classesand occupies 5th position.
Considering that ourmethodology gives advantage to relations which areobserved more frequently, it is more adapted to-wards the micro measure.The process of manually generating type-specifictrigger lists was largely based on the MedLine traincorpus due to its size, with the assumption thatthe relations in DrugBank are similarly expressed.However, both micro and macro scores for Drug-Bank ranked 7th, showing that adaptation of triggerword lists needs to be done, depending on the targetcorpus.In general, lower performance for relation classassignment is partially due to incompleteness of thetrigger lists, but also coming intrinsically from therelation priority hierarchy.
Most of classificationerrors occur when a trigger word belonging to a?higher?
priority class is identified in the sentence.In the following example the word ?should?
im-plies advise relation, although guanfacine and CNS-depressant drug express an effect relation:The potential for increased sedation when guan-facine is given with other CNS-depressant drugshould be appreciated.Another example is a sentence mentioning ?ef-fect?, but actually describing a simple int relation:Chloral hydrate and methaqualone interactpharmacologically with orally administered antico-agulant agents, but the effect is not clinically signif-icant.Furthermore, a lot of missclassifications occur insentences which contain pairs and triggers from dif-ferent types, resulting in all relations being assignedto the highest identified type.4 ConclusionWe present a machine learning based system forextraction of drug-drug interactions, using lexical,syntactic and semantic properties of the sentencetext.
The system achieves competitive performancefor the general DDI extraction, albeit demonstrat-ing sensitivity to the train corpora class imbalance.We show that, depending on the use case, resam-pling, balancing and ensemble strategies are suc-cessful in tuning the system to favor recall over pre-cision.
The post-processing step of relation type as-signment achieves top ranked results for the Med-Line corpus, however, needs more adaption in caseof DrugBank.
Future work includes a comparisonwith a multi-classifier approach, which circumventsthe manual task of trigger list generation, supportingthe fully automated scenario of relation extraction.AcknowledgmentsThe authors would like to thank Roman Klinger forfruitful discussions.
T. Bobic?
was funded by theBonn-Aachen International Center for InformationTechnology (B-IT) Research School.682ReferencesJ.T.
August, F. Murad, W. Anders, J.T.
Coyle, and A.P.
Li.1997.
Drug-Drug Interactions: Scientific and Regula-tory Perspectives: Scientific and Regulatory Perspec-tives.
Advances in pharmacology.
Elsevier Science.E.
Bauer and R. Kohavi.
1999.
An empirical comparisonof voting classification algorithms: Bagging, boosting,and variants.
Machine Learning, 36(1-2).R.
C. Bunescu and R. J. Mooney.
2005a.
A shortest pathdependency kernel for relation extraction.
In HLT andEMNLP.R.
C. Bunescu and R. J. Mooney.
2005b.
SubsequenceKernels for Relation Extraction.
NIPS.C.
Cortes and V. Vapnik.
1995.
Support vector networks.In Machine Learning.P.
Domingos and M. Pazzani.
1996.
Beyond indepen-dence: Conditions for the optimality of the simplebayesian classifier.
In ICML.E.
Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin.
2008.LIBLINEAR: A Library for Large Linear Classifica-tion.
Machine Learning Research, 9.Y.
Freund and R. E. Schapire.
1999.
Large margin clas-sification using the perceptron algorithm.
MachineLearning, 37(3).C.
Giuliano, A. Lavelli, and L. Romano.
2006.
Exploit-ing shallow linguistic information for relation extrac-tion from biomedical literature.
In Proc.
of the 11stConf.
of the European Chapter of the Association forComputational Linguistics (EACL?06).M.
Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-mann, and I. H. Witten.
2009.
The weka data miningsoftware: An update.
SIGKDD Explorations, 11.J.
V. Hulse, T. M. Khoshgoftaar, and A. Napolitano.2007.
Experimental perspectives on learning from im-balanced data.
In ICML.S.
Kim, J. Yoon, J. Yang, and S. Park.
2010.
Walk-weighted subsequence kernels for protein-protein in-teraction extraction.
BMC Bioinformatics, 11.C.
Knox, V. Law, T. Jewison, P. Liu, S. Ly, A. Frolkis,A.
Pon, K. Banco, C. Mak, V. Neveu, Y. Djoumbou,R.
Eisner, A. Chi Guo, and D.S Wishart.
2011.
Drug-bank 3.0: a comprehensive resource for ?omics?
re-search on drugs.
Nucleic Acids Res, 39.M.
Lease and E. Charniak.
2005.
Parsing biomedicalliterature.
In Proc.
of IJCNLP?05.M.
C. De Marneffe, B. Maccartney, and C. D. Man-ning.
2006.
Generating typed dependency parses fromphrase structure parses.
In LREC.M.
Miwa, R. Saetre, J. D. Kim, and J. Tsujii.
2010.
Eventextraction with complex event classification using richfeatures.
Journal of bioinformatics and computationalbiology, 8.M.
Porter.
1980.
An algorithm for suffix stripping.
Pro-gram, 14.F.
Provost.
2000.
Machine learning from imbalanceddata sets 101 (extended abstract).I.
Segura-Bedmar, P. Martnez, and M. Herrero-Zazo.2013.
Semeval-2013 task 9: Extraction of drug-druginteractions from biomedical texts.
In Proceedings ofthe 7th International Workshop on Semantic Evalua-tion (SemEval 2013).P.
Thomas, M. Neves, I. Solt, D. Tikk, and U. Leser.2011a.
Relation extraction for drug-drug interactionsusing ensemble learning.
In Proceedings of the 1stChallenge Task on Drug-Drug Interaction Extraction2011.P.
Thomas, S. Pietschmann, I. Solt, D. Tikk, and U. Leser.2011b.
Not all links are equal: Exploiting dependencytypes for the extraction of protein-protein interactionsfrom text.
In Proceedings of BioNLP 2011 Workshop.683
