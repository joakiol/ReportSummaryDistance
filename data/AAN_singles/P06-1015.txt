Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 113?120,Sydney, July 2006. c?2006 Association for Computational LinguisticsEspresso: Leveraging Generic Patterns forAutomatically Harvesting Semantic RelationsPatrick PantelInformation Sciences InstituteUniversity of Southern California4676 Admiralty WayMarina del Rey, CA  90292pantel@isi.eduMarco PennacchiottiART Group - DISPUniversity of Rome ?Tor Vergata?Viale del Politecnico 1Rome, Italypennacchiotti@info.uniroma2.itAbstractIn this paper, we present Espresso, aweakly-supervised, general-purpose,and accurate algorithm for harvestingsemantic relations.
The main contribu-tions are: i) a method for exploiting ge-neric patterns by filtering incorrectinstances using the Web; and ii) a prin-cipled measure of pattern and instancereliability enabling the filtering algo-rithm.
We present an empirical com-parison of Espresso with various state ofthe art systems, on different size andgenre corpora, on extracting variousgeneral and specific relations.
Experi-mental results show that our exploita-tion of generic patterns substantiallyincreases system recall with small effecton overall precision.1 IntroductionRecent attention to knowledge-rich problemssuch as question answering (Pasca and Harabagiu2001) and textual entailment (Geffet and Dagan2005) has encouraged natural language process-ing researchers to develop algorithms for auto-matically harvesting shallow semantic resources.With seemingly endless amounts of textual dataat our disposal, we have a tremendous opportu-nity to automatically grow semantic term banksand ontological resources.To date, researchers have harvested, withvarying success, several resources, includingconcept lists (Lin and Pantel 2002), topic signa-tures (Lin and Hovy 2000), facts (Etzioni et al2005), and word similarity lists (Hindle 1990).Many recent efforts have also focused on extract-ing semantic relations between entities, such asentailments (Szpektor et al 2004), is-a (Ravi-chandran and Hovy 2002), part-of (Girju et al2006), and other relations.The following desiderata outline the propertiesof an ideal relation harvesting algorithm:?
Performance: it must generate both high preci-sion and high recall relation instances;?
Minimal supervision: it must require little or nohuman annotation;?
Breadth: it must be applicable to varying cor-pus sizes and domains; and?
Generality: it must be applicable to a wide va-riety of relations (i.e., not just is-a or part-of).To our knowledge, no previous harvesting algo-rithm addresses all these properties concurrently.In this paper, we present Espresso, a general-purpose, broad, and accurate corpus harvestingalgorithm requiring minimal supervision.
Themain algorithmic contribution is a novel methodfor exploiting generic patterns, which are broadcoverage noisy patterns ?
i.e., patterns with highrecall and low precision.
Insofar, difficulties inusing these patterns have been a major impedi-ment for minimally supervised algorithms result-ing in either very low precision or recall.
Wepropose a method to automatically detect genericpatterns and to separate their correct and incor-rect instances.
The key intuition behind the algo-rithm is that given a set of reliable (highprecision) patterns on a corpus, correct instancesof a generic pattern will fire more with reliablepatterns on a very large corpus, like the Web,than incorrect ones.
Below is a summary of themain contributions of this paper:?
Algorithm for exploiting generic patterns:Unlike previous algorithms that require signifi-cant manual work to make use of generic pat-terns, we propose an unsupervised Web-filtering method for using generic patterns; and?
Principled reliability measure: We propose anew measure of pattern and instance reliabilitywhich enables the use of generic patterns.113Espresso addresses the desiderata as follows:?
Performance: Espresso generates balancedprecision and recall relation instances by ex-ploiting generic patterns;?
Minimal supervision: Espresso requires as in-put only a small number of seed instances;?
Breadth: Espresso works on both small andlarge corpora ?
it uses Web and syntactic ex-pansions to compensate for lacks of redun-dancy in small corpora;?
Generality: Espresso is amenable to a widevariety of binary relations, from classical is-aand part-of to specific ones such as reactionand succession.Previous work like (Girju et al 2006) that hasmade use of generic patterns through filtering hasshown both high precision and high recall, at theexpensive cost of much manual semantic annota-tion.
Minimally supervised algorithms, like(Hearst 1992; Pantel et al 2004), typically ignoregeneric patterns since system precision dramati-cally decreases from the introduced noise andbootstrapping quickly spins out of control.2 Relevant WorkTo date, most research on relation harvesting hasfocused on is-a and part-of.
Approaches fall intotwo categories: pattern- and clustering-based.Most common are pattern-based approaches.Hearst (1992) pioneered using patterns to extracthyponym (is-a) relations.
Manually buildingthree lexico-syntactic patterns, Hearst sketched abootstrapping algorithm to learn more patternsfrom instances, which has served as the modelfor most subsequent pattern-based algorithms.Berland and Charniak (1999) proposed a sys-tem for part-of relation extraction, based on the(Hearst 1992) approach.
Seed instances are usedto infer linguistic patterns that are used to extractnew instances.
While this study introduces statis-tical measures to evaluate instance quality, it re-mains vulnerable to data sparseness and has thelimitation of considering only one-word terms.Improving upon (Berland and Charniak 1999),Girju et al (2006) employ machine learning al-gorithms and WordNet (Fellbaum 1998) to dis-ambiguate part-of generic patterns like ?X?s Y?and ?X of Y?.
This study is the first extensive at-tempt to make use of generic patterns.
In order todiscard incorrect instances, they learn WordNet-based selectional restrictions, like ?X(scene#4)?sY(movie#1)?.
While making huge grounds onimproving precision/recall, heavy supervision isrequired through manual semantic annotations.Ravichandran and Hovy (2002) focus on scal-ing relation extraction to the Web.
A simple andeffective algorithm is proposed to infer surfacepatterns from a small set of instance seeds byextracting substrings relating seeds in corpus sen-tences.
The approach gives good results on spe-cific relations such as birthdates, however it haslow precision on generic ones like is-a and part-of.
Pantel et al (2004) proposed a similar, highlyscalable approach, based on an edit-distancetechnique, to learn lexico-POS patterns, showingboth good performance and efficiency.
Espressouses a similar approach to infer patterns, but wemake use of generic patterns and apply refiningtechniques to deal with wide variety of relations.Other pattern-based algorithms include (Riloffand Shepherd 1997), who used a semi-automaticmethod for discovering similar words using afew seed examples, KnowItAll (Etzioni et al2005) that performs large-scale extraction offacts from the Web, Mann (2002) who used partof speech patterns to extract a subset of is-a rela-tions involving proper nouns, and (Downey et al2005) who formalized the problem of relationextraction in a coherent and effective combinato-rial model that is shown to outperform previousprobabilistic frameworks.Clustering approaches have so far been ap-plied only to is-a extraction.
These methods useclustering algorithms to group words accordingto their meanings in text, label the clusters usingits members?
lexical or syntactic dependencies,and then extract an is-a relation between eachcluster member and the cluster label.
Caraballo(1999) proposed the first attempt, which usedconjunction and apposition features to build nounclusters.
Recently, Pantel and Ravichandran(2004) extended this approach by making use ofall syntactic dependency features for each noun.The advantage of clustering approaches is thatthey permit algorithms to identify is-a relationsthat do not explicitly appear in text, howeverthey generally fail to produce coherent clustersfrom fewer than 100 million words; hence theyare unreliable for small corpora.3 The Espresso AlgorithmEspresso is based on the framework adopted in(Hearst 1992).
It is a minimally supervised boot-strapping algorithm that takes as input a few seedinstances of a particular relation and iterativelylearns surface patterns to extract more instances.The key to Espresso lies in its use of generic pat-ters, i.e., those broad coverage noisy patterns that114extract both many correct and incorrect relationinstances.
For example, for part-of relations, thepattern ?X of Y?
extracts many correct relationinstances like ?wheel of the car?
but also manyincorrect ones like ?house of representatives?.The key assumption behind Espresso is that invery large corpora, like the Web, correct in-stances generated by a generic pattern will beinstantiated by some reliable patterns, wherereliable patterns are patterns that have high preci-sion but often very low recall (e.g., ?X consists ofY?
for part-of relations).
In this section, we de-scribe the overall architecture of Espresso, pro-pose a principled measure of reliability, and givean algorithm for exploiting generic patterns.3.1 System ArchitectureEspresso iterates between the following threephases: pattern induction, pattern rank-ing/selection, and instance extraction.The algorithm begins with seed instances of aparticular binary relation (e.g., is-a) and then it-erates through the phases until it extracts ?1 pat-terns or the average pattern score decreases bymore than ?2 from the previous iteration.
In ourexperiments, we set ?1 = 5 and ?2 = 50%.For our tokenization, in order to harvest multi-word terms as relation instances, we adopt aslightly modified version of the term definitiongiven in (Justeson 1995), as it is one of the mostcommonly used in the NLP literature:((Adj|Noun)+|((Adj|Noun)*(NounPrep)?
)(Adj|Noun)*)NounPattern InductionIn the pattern induction phase, Espresso infers aset of surface patterns P that connects as many ofthe seed instances as possible in a given corpus.Any pattern learning algorithm would do.
Wechose the state of the art algorithm described in(Ravichandran and Hovy 2002) with the follow-ing slight modification.
For each input instance{x, y}, we first retrieve all sentences containingthe two terms x and y.
The sentences are thengeneralized into a set of new sentences Sx,y byreplacing all terminological expressions by aterminological label, TR.
For example:?Because/IN HF/NNP is/VBZ a/DT weak/JJ acid/NNand/CC x is/VBZ a/DT y?is generalized as:?Because/IN TR is/VBZ a/DT TR and/CC x is/VBZ a/DT y?Term generalization is useful for small corpora toease data sparseness.
Generalized patterns arenaturally less precise, but this is ameliorated byour filtering step described in Section 3.3.As in the original algorithm, all substringslinking terms x and y are then extracted from Sx,y,and overall frequencies are computed to form P.Pattern Ranking/SelectionIn (Ravichandran and Hovy 2002), a frequencythreshold on the patterns in P is set to select thefinal patterns.
However, low frequency patternsmay in fact be very good.
In this paper, instead offrequency, we propose a novel measure of pat-tern reliability, r?, which is described in detail inSection 3.2.Espresso ranks all patterns in P according toreliability r?
and discards all but the top-k, wherek is set to the number of patterns from the previ-ous iteration plus one.
In general, we expect thatthe set of patterns is formed by those of the pre-vious iteration plus a new one.
Yet, new statisti-cal evidence can lead the algorithm to discard apattern that was previously discovered.Instance ExtractionIn this phase, Espresso retrieves from the corpusthe set of instances I that match any of the pat-terns in P. In Section 3.2, we propose a princi-pled measure of instance reliability, r?, forranking instances.
Next, Espresso filters incor-rect instances using the algorithm proposed inSection 3.3 and then selects the highest scoring minstances, according to r?, as input for the subse-quent iteration.
We experimentally set m=200.In small corpora, the number of extracted in-stances can be too low to guarantee sufficientstatistical evidence for the pattern discoveryphase of the next iteration.
In such cases, the sys-tem enters an expansion phase, where instancesare expanded as follows:Web expansion: New instances of the patternsin P are retrieved from the Web, using theGoogle search engine.
Specifically, for each in-stance {x, y}?
I, the system creates a set of que-ries, using each pattern in P instantiated with y.For example, given the instance ?Italy, country?and the pattern ?Y such as X?, the resultingGoogle query will be ?country such as *?.
Newinstances are then created from the retrieved Webresults (e.g.
?Canada, country?)
and added to I.The noise generated from this expansion is at-tenuated by the filtering algorithm described inSection 3.3.Syntactic expansion: New instances are cre-ated from each instance {x, y}?
I by extractingsub-terminological expressions from x corre-sponding to the syntactic head of terms.
For ex-115ample, the relation ?new record of a criminalconviction part-of FBI report?
expands to: ?newrecord part-of FBI report?, and ?record part-ofFBI report?.3.2 Pattern and Instance ReliabilityIntuitively, a reliable pattern is one that is bothhighly precise and one that extracts many in-stances.
The recall of a pattern p can be approxi-mated by the fraction of input instances that areextracted by p. Since it is non-trivial to estimateautomatically the precision of a pattern, we arewary of keeping patterns that generate many in-stances (i.e., patterns that generate high recall butpotentially disastrous precision).
Hence, we de-sire patterns that are highly associated with theinput instances.
Pointwise mutual information(Cover and Thomas 1991) is a commonly usedmetric for measuring this strength of associationbetween two events x and y:( ) ( )( ) ( )yPxPyxPyxpmi,log, =We define the reliability of a pattern p, r?
(p),as its average strength of association across eachinput instance i in I, weighted by the reliability ofeach instance i:( )( )IirpipmiprIi pmi??
????????
?=?
?max),(where r?
(i) is the reliability of instance i (definedbelow) and maxpmi is the maximum pointwisemutual information between all patterns and allinstances.
r?
(p) ranges from [0,1].
The reliabilityof the manually supplied seed instances are r?
(i)= 1.
The pointwise mutual information betweeninstance i = {x, y} and pattern p is estimated us-ing the following formula:( ),**,,*,,,log,pyxypxpipmi =where |x, p, y| is the frequency of pattern p in-stantiated with terms x and y and where the aster-isk (*) represents a wildcard.
A well-knownproblem is that pointwise mutual information isbiased towards infrequent events.
We thus multi-ply pmi(i, p) with the discounting factor sug-gested in (Pantel and Ravichandran 2004).Estimating the reliability of an instance issimilar to estimating the reliability of a pattern.Intuitively, a reliable instance is one that ishighly associated with as many reliable patternsas possible (i.e., we have more confidence in aninstance when multiple reliable patterns instanti-ate it.)
Hence, analogous to our pattern reliabilitymeasure, we define the reliability of an instancei, r?
(i), as:( )( )Pprpipmiir Pp pmi????=?
?max),(where r?
(p) is the reliability of pattern p (definedearlier) and maxpmi is as before.
Note that r?
(i)and r?
(p) are recursively defined, where r?
(i) = 1for the manually supplied seed instances.3.3 Exploiting Generic PatternsGeneric patterns are high recall / low precisionpatterns (e.g, the pattern ?X of Y?
can ambigu-ously refer to a part-of, is-a and possession rela-tions).
Using them blindly increases systemrecall while dramatically reducing precision.Minimally supervised algorithms have typicallyignored them for this reason.
Only heavily super-vised approaches, like (Girju et al 2006) havesuccessfully exploited them.Espresso?s recall can be significantly in-creased by automatically separating correct in-stances extracted by generic patterns fromincorrect ones.
The challenge is to harness theexpressive power of the generic patterns whileremaining minimally supervised.The intuition behind our method is that in avery large corpus, like the Web, correct instancesof a generic pattern will be instantiated by manyof Espresso?s reliable patterns accepted in P. Re-call that, by definition, Espresso?s reliable pat-terns extract instances with high precision (yetoften low recall).
In a very large corpus, like theWeb, we assume that a correct instance will oc-cur in at least one of Espresso?s reliable patterneven though the patterns?
recall is low.
Intui-tively, our confidence in a correct instance in-creases when, i) the instance is associated withmany reliable patterns; and ii) its associationwith the reliable patterns is high.
At a given Es-presso iteration, where PR represents the set ofpreviously selected reliable patterns, this intui-tion is captured by the following measure of con-fidence in an instance i = {x, y}:( ) ( ) ( )??
?=RPpp TpriSiS ?where T is the sum of the reliability scores r?
(p)for each pattern p ?
PR, and( ) ( ),**,,*,,,log,pyxypxpipmiiS p ?==116where pointwise mutual information betweeninstance i and pattern p is estimated with Googleas follows:( )pyxypxiS p ??
?,,An instance i is rejected if S(i) is smaller thansome threshold ?.Although this filtering may also be applied toreliable patterns, we found this to be detrimentalin our experiments since most instances gener-ated by reliable patterns are correct.
In Espresso,we classify a pattern as generic when it generatesmore than 10 times the instances of previouslyaccepted reliable patterns.4 Experimental ResultsIn this section, we present an empirical compari-son of Espresso with three state of the art sys-tems on the task of extracting various semanticrelations.4.1 Experimental SetupWe perform our experiments using the followingtwo datasets:?
TREC: This dataset consists of a sample ofarticles from the Aquaint (TREC-9) newswiretext collection.
The sample consists of5,951,432 words extracted from the followingdata files: AP890101 ?
AP890131, AP890201?
AP890228, and AP890310 ?
AP890319.?
CHEM: This small dataset of 313,590 wordsconsists of a college level textbook of introduc-tory chemistry (Brown et al 2003).Each corpus is pre-processed using the AlembicWorkbench POS-tagger (Day et al 1997).Below we describe the systems used in ourempirical evaluation of Espresso.?
RH02: The algorithm by Ravichandran andHovy (2002) described in Section 2.?
GI03: The algorithm by Girju et al (2006) de-scribed in Section 2.?
PR04: The algorithm by Pantel and Ravi-chandran (2004) described in Section 2.?
ESP-: The Espresso algorithm using the pat-tern and instance reliability measures, butwithout using generic patterns.?
ESP+: The full Espresso algorithm describedin this paper exploiting generic patterns.For ESP+, we experimentally set ?
from Section3.3 to ?
= 0.4 for TREC and ?
= 0.3 for CHEMby manually inspecting a small set of instances.Espresso is designed to extract various seman-tic relations exemplified by a given small set ofseed instances.
We consider the standard is-a andpart-of relations as well as the following morespecific relations:?
succession: This relation indicates that a personsucceeds another in a position or title.
For ex-ample, George Bush succeeded Bill Clintonand Pope Benedict XVI succeeded Pope JohnPaul II.
We evaluate this relation on theTREC-9 corpus.?
reaction: This relation occurs between chemi-cal elements/molecules that can be combinedin a chemical reaction.
For example, hydrogengas reacts-with oxygen gas and zinc reacts-withhydrochloric acid.
We evaluate this relation onthe CHEM corpus.?
production: This relation occurs when a proc-ess or element/object produces a result1.
Forexample, ammonia produces nitric oxide.
Weevaluate this relation on the CHEM corpus.For each semantic relation, we manually ex-tracted a small set of seed examples.
The seedswere used for both Espresso as well as RH02.Table 1 lists a sample of the seeds as well assample outputs from Espresso.4.2 Precision and RecallWe implemented the systems outlined in Section4.1, except for GI03, and applied them to the1 Production is an ambiguous relation; it is intended to bea causation relation in the context of chemical reactions.Table 1.
Sample seeds used for each semantic relation and sample outputs from Espresso.
The numberin the parentheses for each relation denotes the total number of seeds used as input for the system.Is-a (12) Part-Of (12) Succession (12) Reaction (13) Production (14)Seedswheat :: cropGeorge Wendt :: starnitrogen :: elementdiborane :: substanceleader :: panelcity :: regionion :: matteroxygen :: waterKhrushchev :: StalinCarla Hills :: YeutterBush :: ReaganJulio Barbosa :: Mendesmagnesium :: oxygenhydrazine :: wateraluminum metal :: oxygenlithium metal :: fluorine gasbright flame :: flareshydrogen :: metal hydridesammonia :: nitric oxidecopper :: brown gasEs-pressoPicasso :: artisttax :: chargeprotein :: biopolymerHCl :: strong acidtrees :: landmaterial :: FBI reportoxygen :: airatom :: moleculeFord :: NixonSetrakian :: John GriesemerCamero Cardiel :: CamachoSusan Weiss :: editorhydrogen :: oxygenNi :: HClcarbon dioxide :: methaneboron :: fluorineelectron :: ionsglycerin :: nitroglycerinkidneys :: kidney stonesions :: charge117Table 8.
System performance: CHEM/production.SYSTEM INSTANCES PRECISION* REL RECALL?RH02 197 57.5% 0.80ESP- 196 72.5% 1.00ESP+ 1676 55.8% 6.58TREC and CHEM datasets.
For each output set,per relation, we evaluate the precision of the sys-tem by extracting a random sample of instances(50 for the TREC corpus and 20 for the CHEMcorpus) and evaluating their quality manuallyusing two human judges (a total of 680 instanceswere annotated per judge).
For each instance,judges may assign a score of 1 for correct, 0 forincorrect, and ?
for partially correct.
Exampleinstances that were judged partially correct in-clude ?analyst is-a manager?
and ?pilot is-ateacher?.
The kappa statistic (Siegel and Castel-lan Jr. 1988) on this task was ?
= 0.692.
The pre-cision for a given set of instances is the sum ofthe judges?
scores divided by the total instances.Although knowing the total number of correctinstances of a particular relation in any non-trivial corpus is impossible, it is possible to com-pute the recall of a system relative to another sys-tem?s recall.
Following (Pantel et al 2004), wedefine the relative recall of system A given sys-tem B, RA|B, as:BPAPCCRRRBABACCCCBABABA?
?====|where RA is the recall of A, CA is the number ofcorrect instances extracted by A, C is the (un-known) total number of correct instances in thecorpus, PA is A?s precision in our experiments,2 The kappa statistic jumps to ?
= 0.79 if we treat partiallycorrect classifications as correct.and |A| is the total number of instances discov-ered by A.Tables 2 ?
8 report the total number of in-stances, precision, and relative recall of each sys-tem on the TREC-9 and CHEM corpora 3 4 .
Therelative recall is always given in relation to theESP- system.
For example, in Table 2, RH02 hasa relative recall of 5.31 with ESP-, which meansthat the RH02 system outputs 5.31 times morecorrect relations than ESP- (at a cost of muchlower precision).
Similarly, PR04 has a relativerecall of 0.23 with ESP-, which means that PR04outputs 4.35 fewer correct relations than ESP-(also with a smaller precision).
We did not in-clude the results from GI03 in the tables since thesystem is only applicable to part-of relations andwe did not reproduce it.
However, the authorsevaluated their system on a sample of the TREC-9 dataset and reported 83% precision and 72%recall (this algorithm is heavily supervised.
)* Because of the small evaluation sets, we estimate the95% confidence intervals using bootstrap resampling to bein the order of ?
10-15% (absolute numbers).?
Relative recall is given in relation to ESP-.Table 2.
System performance: TREC/is-a.SYSTEM INSTANCES PRECISION* REL RECALL?RH02 57,525 28.0% 5.31PR04 1,504 47.0% 0.23ESP- 4,154 73.0% 1.00ESP+ 69,156 36.2% 8.26Table 4.
System performance: TREC/part-of.SYSTEM INSTANCES PRECISION* REL RECALL?RH02 12,828 35.0% 42.52ESP- 132 80.0% 1.00ESP+ 87,203 69.9% 577.22Table 3.
System performance: CHEM/is-a.SYSTEM INSTANCES PRECISION* REL RECALL?RH02 2556 25.0% 3.76PR04 108 40.0% 0.25ESP- 200 85.0% 1.00ESP+ 1490 76.0% 6.66Table 5.
System performance: CHEM/part-of.SYSTEM INSTANCES PRECISION* REL RECALL?RH02 11,582 33.8% 58.78ESP- 111 60.0% 1.00ESP+ 5973 50.7% 45.47Table 7.
System performance: CHEM/reaction.SYSTEM INSTANCES PRECISION* REL RECALL?RH02 6,083 30% 53.67ESP- 40 85% 1.00ESP+ 3102 91.4% 89.39Table 6.
System performance: TREC/succession.SYSTEM INSTANCES PRECISION* REL RECALL?RH02 49,798 2.0% 36.96ESP- 55 49.0% 1.00ESP+ 55 49.0% 1.00118In all tables, RH02 extracts many more rela-tions than ESP-, but with a much lower precision,because it uses generic patterns without filtering.The high precision of ESP- is due to the effectivereliability measures presented in Section 3.2.4.3 Effect of Generic PatternsExperimental results, for all relations and the twodifferent corpus sizes, show that ESP- greatlyoutperforms the other methods on precision.However, without the use of generic patterns, theESP- system shows lower recall in all but theproduction relation.As hypothesized, exploiting generic patternsusing the algorithm from Section 3.3 substan-tially improves recall without much deteriorationin precision.
ESP+ shows one to two orders ofmagnitude improvement on recall while losingon average below 10% precision.
The successionrelation in Table 6 was the only relation whereEspresso found no generic pattern.
For other re-lations, Espresso found from one to five genericpatterns.
Table 4 shows the power of generic pat-terns where system recall increases by 577 timeswith only a 10% drop in precision.
In Table 7, wesee a case where the combination of filteringwith a large increase in retrieved instances re-sulted in both higher precision and recall.In order to better analyze our use of genericpatterns, we performed the following experiment.For each relation, we randomly sampled 100 in-stances for each generic pattern and built a goldstandard for them (by manually tagging each in-stance as correct or incorrect).
We then sorted the100 instances according to the scoring formulaS(i) derived in Section 3.3 and computed the av-erage precision, recall, and F-score of each top-Kranked instances for each pattern5.
Due to lack ofspace, we only present the graphs for four of the22 generic patterns: ?X is a Y?
for the is-a rela-tion of Table 2, ?X in the Y?
for the part-of rela-tion of Table 4, ?X in Y?
for the part-of relationof Table 5, and ?X and Y?
for the reaction rela-tion of Table 7.
Figure 1 illustrates the results.In each figure, notice that recall climbs at amuch faster rate than precision decreases.
Thisindicates that the scoring function of Section 3.3effectively separates correct and incorrect in-stances.
In Figure 1a), there is a big initial dropin precision that accounts for the poor precisionreported in Table 1.Recall that the cutoff points on S(i) were set to?
= 0.4 for TREC and ?
= 0.3 for CHEM.
Thefigures show that this cutoff is far from themaximum F-score.
An interesting avenue of fu-ture work would be to automatically determinethe proper threshold for each individual genericpattern instead of setting a uniform threshold.5 We can directly compute recall here since we built agold standard for each set of 100 samples.Figure 1.
Precision, recall and F-score curves of the Top-K% ranking instances of patterns ?X is a Y?
(TREC/is-a), ?X in Y?
(TREC/part-of), ?X in the Y?
(CHEM/part-of), and ?X and Y?
(CHEM/reaction).a) TREC/is-a: "X is a Y"00.20.40.60.815 15 25 35 45 55 65 75 85 95Top-K%d) CHEM/reaction: "X and Y"00.20.40.60.815 15 25 35 45 55 65 75 85 95Top-K%c) CHEM/part-of: "X in Y"00.20.40.60.815 15 25 35 45 55 65 75 85 95Top-K%b) TREC/part-of: "X in the Y"00.20.40.60.815 15 25 35 45 55 65 75 85 95Top-K%1195 ConclusionsWe proposed a weakly-supervised, general-purpose, and accurate algorithm, called Espresso,for harvesting binary semantic relations from rawtext.
The main contributions are: i) a method forexploiting generic patterns by filtering incorrectinstances using the Web; and ii) a principledmeasure of pattern and instance reliability ena-bling the filtering algorithm.We have empirically compared Espresso?sprecision and recall with other systems on both asmall domain-specific textbook and on a largercorpus of general news, and have extracted sev-eral standard and specific semantic relations: is-a, part-of, succession, reaction, and production.Espresso achieves higher and more balanced per-formance than other state of the art systems.
Byexploiting generic patterns, system recall sub-stantially increases with little effect on precision.There are many avenues of future work both inimproving system performance and making useof the relations in applications like question an-swering.
For the former, we plan to investigatethe use of WordNet to automatically learn selec-tional constraints on generic patterns, as pro-posed by (Girju et al 2006).
We expect here thatnegative instances will play a key role in deter-mining the selectional restrictions.Espresso is the first system, to our knowledge,to emphasize concurrently performance, minimalsupervision, breadth, and generality.
It remainsto be seen whether one could enrich existing on-tologies with relations harvested by Espresso,and it is our hope that these relations will benefitNLP applications.ReferencesBerland, M. and E. Charniak, 1999.
Finding parts in verylarge corpora.
In Proceedings of ACL-1999.
pp.
57-64.College Park, MD.Brown, T.L.
; LeMay, H.E.
; Bursten, B.E.
; and Burdge, J.R.2003.
Chemistry: The Central Science, Ninth Edition.Prentice Hall.Caraballo, S. 1999.
Automatic acquisition of a hypernym-labeled noun hierarchy from text.
In Proceedings ofACL-99.
pp 120-126, Baltimore, MD.Cover, T.M.
and Thomas, J.A.
1991.
Elements ofInformation Theory.
John Wiley & Sons.Day, D.; Aberdeen, J.; Hirschman, L.; Kozierok, R.;Robinson, P.; and Vilain, M. 1997.
Mixed-initiativedevelopment of language processing systems.
InProceedings of ANLP-97.
Washington D.C.Downey, D.; Etzioni, O.; and Soderland, S. 2005.
AProbabilistic model of redundancy in informationextraction.
In Proceedings of IJCAI-05.
pp.
1034-1041.Edinburgh, Scotland.Etzioni, O.; Cafarella, M.J.; Downey, D.; Popescu, A.-M.;Shaked, T.; Soderland, S.; Weld, D.S.
; and Yates, A.2005.
Unsupervised named-entity extraction from theWeb: An experimental study.
Artificial Intelligence,165(1): 91-134.Fellbaum, C. 1998.
WordNet: An Electronic LexicalDatabase.
MIT Press.Geffet, M. and Dagan, I.
2005.
The Distributional InclusionHypotheses and Lexical Entailment.
In Proceedings ofACL-2005.
Ann Arbor, MI.Girju, R.; Badulescu, A.; and Moldovan, D. 2006.Automatic Discovery of Part-Whole Relations.Computational Linguistics, 32(1): 83-135.Hearst, M. 1992.
Automatic acquisition of hyponyms fromlarge text corpora.
In Proceedings of COLING-92.
pp.539-545.
Nantes, France.Hindle, D. 1990.
Noun classification from predicate-argument structures.
In Proceedings of ACL-90.
pp.
268?275.
Pittsburgh, PA.Justeson J.S.
and Katz S.M.
1995.
Technical Terminology:some linguistic properties and algorithms foridentification in text.
In Proceedings of ICCL-95.pp.539-545.
Nantes, France.Lin, C.-Y.
and Hovy, E.H.. 2000.
The Automatedacquisition of topic signatures for text summarization.
InProceedings of COLING-00.
pp.
495-501.
Saarbr?cken,Germany.Lin, D. and Pantel, P. 2002.
Concept discovery from text.
InProceedings of COLING-02.
pp.
577-583.
Taipei,Taiwan.Mann, G. S. 2002.
Fine-Grained Proper Noun Ontologiesfor Question Answering.
In Proceedings of SemaNet?
02:Building and Using Semantic Networks, Taipei, Taiwan.Pantel, P. and Ravichandran, D. 2004.
Automaticallylabeling semantic classes.
In Proceedings ofHLT/NAACL-04.
pp.
321-328.
Boston, MA.Pantel, P.; Ravichandran, D.; Hovy, E.H. 2004.
Towardsterascale knowledge acquisition.
In Proceedings ofCOLING-04.
pp.
771-777.
Geneva, Switzerland.Pasca, M. and Harabagiu, S. 2001.
The informative role ofWordNet in Open-Domain Question Answering.
InProceedings of NAACL-01 Workshop on WordNet andOther Lexical Resources.
pp.
138-143.
Pittsburgh, PA.Ravichandran, D. and Hovy, E.H. 2002.
Learning surfacetext patterns for a question answering system.
InProceedings of ACL-2002.
pp.
41-47.
Philadelphia, PA.Riloff, E. and Shepherd, J.
1997.
A corpus-based approachfor building semantic lexicons.
In Proceedings ofEMNLP-97.Siegel, S. and Castellan Jr., N. J.
1988.
NonparametricStatistics for the Behavioral Sciences.
McGraw-Hill.Szpektor, I.; Tanev, H.; Dagan, I.; and Coppola, B.
2004.Scaling web-based acquisition of entailment relations.
InProceedings of EMNLP-04.
Barcelona, Spain.120
