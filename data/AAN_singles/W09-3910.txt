Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 71?78,Queen Mary University of London, September 2009. c?2009 Association for Computational LinguisticsDetecting the Noteworthiness of Utterances in Human MeetingsSatanjeev BanerjeeLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213banerjee@cs.cmu.eduAlexander I. RudnickyLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213air@cs.cmu.eduAbstractOur goal is to make note-taking easier inmeetings by automatically detectingnoteworthy utterances in verbal ex-changes and suggesting them to meetingparticipants for inclusion in their notes.To show feasibility of such a process weconducted a Wizard of Oz study wherethe Wizard picked automatically tran-scribed utterances that he judged asnoteworthy, and suggested their contentsto the participants as notes.
Over 9 meet-ings, participants accepted 35% of thesesuggestions.
Further, 41.5% of their notesat the end of the meeting contained Wi-zard-suggested text.
Next, in order to per-form noteworthiness detection automati-cally, we annotated a set of 6 meetingswith a 3-level noteworthiness annotationscheme, which is a break from the binary?in summary?/ ?not in summary?
labe-ling typically used in speech summariza-tion.
We report Kappa of 0.44 for the 3-way classification, and 0.58 when two ofthe 3 labels are merged into one.
Finally,we trained an SVM classifier on this an-notated data; this classifier?s performancelies between that of trivial baselines andinter-annotator agreement.1 IntroductionWe regularly exchange information verbally withothers over the course of meetings.
Often weneed to access this information afterwards.
Typi-cally we record the information we consider im-portant by taking notes.
Note taking at meetingsis a difficult task, however, because the partici-pant must summarize and write down the infor-mation in a way such that it is comprehensibleafterwards, while paying attention to and partici-pating in the ongoing discussion.
Our goal is tomake note-taking easier by automatically extract-ing noteworthy items from spoken interactions inreal time, and proposing them to the humans forinclusion in their notes.Judging which pieces of information in ameeting are noteworthy is a very subjective task.The subjectivity of this task is likely to be moreacute than even that of meeting summarization,where low inter-annotator agreement is typicale.g.
(Galley, 2006), (Liu & Liu, 2008), (Penn &Zhu, 2008), etc ?
whether a piece of informationshould be included in a participant?s notes de-pends not only on its importance, but also onfactors such as the participant?s need to remem-ber, his perceived likelihood of forgetting, etc.To investigate whether it is feasible even for ahuman to predict what someone else might findnoteworthy in a meeting, we conducted a Wizardof Oz-based user study where a human suggestednotes (with restriction) to meeting participantsduring the meeting.
We concluded from thisstudy (presented in section 2) that this task ap-pears to be feasible for humans.Assuming feasibility, we then annotated 6meetings with a 3-level noteworthiness scheme.Having 3 levels instead of the typical 2 allows usto explicitly separate utterances of middlingnoteworthiness from those that are definitelynoteworthy or not noteworthy, and allows us toencode more human knowledge than a 2-levelscheme.
We describe this annotation scheme inmore detail in section 3, and show high inter-annotator agreement compared to that typicallyreported in the summarization literature.
Finallyin sections 4 and 5 we use this annotated data totrain and test a simple Support Vector Machine-based predictor of utterance noteworthiness.2 Can Humans Do this Task?As mentioned in the introduction, given the de-gree of subjectivity involved in identifying note-71worthy utterances, it is reasonable to ask whetherthe notes-suggestion task can be accomplishedby humans, let alne by automatic systems.
Thatis, we ask the question: Is it possible for a humanto identify noteworthy utterances in a meetingsuch that(a) For at least some fraction of the suggestions,one or more meeting participants agree thatthe suggested notes should indeed be in-cluded in their notes, and(b) The fraction of suggested notes that meetingparticipants find noteworthy is high enoughthat, over a sequence of meetings, the meet-ing participants do not learn to simply ignorethe suggestions.Observe that this task is more restricted than thatof generic note-taking.
While a human who isallowed to summarize discussions and produceto-the-point notes is likely to be useful, we as-sume here that our system will not be able tocreate such abstractive summaries.
Rather, ourgoal here is to explore the feasibility of an ex-tractive summarization system that simply picksnoteworthy utterances and suggests their con-tents to the participants.
To answer this question,we conducted a Wizard of Oz-based pilot userstudy, as follows.2.1 Wizard of Oz Study DesignWe designed a user study in which a human Wi-zard listened to the utterances being uttered dur-ing the meeting, identified noteworthy utter-ances, and suggested their contents to one ormore participants for inclusion in their notes.
Inorder to minimize differences between the Wi-zard and the system (except for the Wizard?shuman-level ability to judge noteworthiness), werestricted the Wizard in the following ways:(a) The Wizard was allowed to only suggest thecontents of individual utterances to the par-ticipants, and not summarize the contents ofmultiple utterances.
(b) The Wizard was allowed to listen to themeeting speech, but when suggesting thecontents of an utterance to the participants,he was restricted to using a real-time auto-matic transcription of the utterance.
(He wasallowed to withhold suggestions becausethey were too erroneously transcribed.
)(c) In order to be closer to a system that has lit-tle or no ?understanding?
of the meetings,we chose a human (to play the role of theWizard) who had not participated in themeetings before, and thus had little priorknowledge of the meetings?
contents.2.2 Notes Suggestion InterfaceIn order to suggest notes to meeting participantsduring a meeting ?
either automatically orthrough a Wizard ?
we have modified theSmartNotes system, whose meeting recordingand note-taking features have been describedearlier in (Banerjee & Rudnicky, 2007).
Briefly,each meeting participant comes to the meetingwith a laptop running SmartNotes.
At the begin-ning of the meeting, each participant?s Smart-Notes client connects to a server, authenticatesthe participant and starts recording and transmit-ting his speech to the server.
In addition, Smart-Notes also provides meeting participants with anote-taking interface that is split into two majorpanes.
In the ?notes?
pane the participant typeshis notes that are then recorded for research pur-poses.
In the ?suggestions?
pane, Wizard-suggested notes are displayed.
If at any time dur-ing the meeting a participant double-clicks onone of the suggested notes in the ?suggestions?pane, its text gets included in his notes in the?notes?
pane.
The Wizard uses a different appli-cation to select real-time utterance transcriptions,and insert them into each participant?s ?sugges-tions?
pane.
(While we also experimented withhaving the Wizard target his suggestions at indi-vidual participants, we do not report on thoseexperiments here; those results were similar tothe ones presented below.
)2.3 ResultsWe conducted the Wizard of Oz study on 9meetings that all belonged to the same sequence.That is, these meetings featured a largely over-lapping group of participants who met weekly todiscuss progress on a single project.
The sameperson played the role of the Wizard in each ofthese 9 meetings.
The meetings were on average33 minutes long, and there were 3 to 4 partici-pants in each meeting.
Although we have notevaluated the accuracy of the speech recognizeron these particular meetings, the typical averageword error rate for these speakers is around 0.4 ?i.e., 4 out of 10 words are incorrectly transcribed.On average, the Wizard suggested the contentsof 7 utterances to the meeting participants, for atotal of 63 suggestions across the 9 meetings.
Ofthese 63 suggestions, 22 (34.9%) were acceptedby the participants and included in their notes.Thus on average, about 2.5 Wizard-suggestednotes were accepted and included in participants?notes in each meeting.
On average, meeting par-ticipants took a total of 5.9 lines of notes per72meeting; thus, 41.5% of the notes in each meet-ing were Wizard-suggested.It cannot be ascertained if the meeting partici-pants would have written the suggested notes ontheir own if they weren?t suggested to them.However the fact that some Wizard-suggestednotes were accepted implies that the participantsprobably saw some value in including those sug-gestions in their notes.
Further, there was nodrop-off in the fraction of meeting notes that wasWizard-suggested: the per-meeting average per-centage of notes that was Wizard-suggested wasaround 41% for both the first 4 meetings, as wellas the last 5.
This implies that despite a seeming-ly low acceptance rate (35%), participants didnot ?give up?
on the suggestions, but continuedto make use of them over the course of the 9-meeting meeting sequence.
We conclude that anextractive summarization system that detectsnoteworthy utterances and suggests them tomeeting participants can be perceived as usefulby the participants, if the detection of noteworthyutterances is ?accurate enough?.3 Meeting Data Used in this PaperAssuming the feasibility of an extraction-basednotes suggestion system, we turn our attention todeveloping a system that can automaticallydetect the noteworthiness of an utterance.
Ourgoal here is to learn to do this task over a se-quence of related meetings.
Towards this end, wehave recorded sequences of natural meetings ?meetings that would have taken place even ifthey weren?t being recorded.
Meetings in eachsequence featured largely overlapping participantsets and topics of discussion.
For each meeting,we used SmartNotes (Banerjee & Rudnicky,2007) (described in section 2 above) to recordboth the audio from each participant as well ashis notes.
The audio recording and the noteswere both time stamped, associated with the par-ticipant?s identity, and uploaded to the meetingserver.
After the meeting was completed the au-dio was manually segmented into utterances andtranscribed both manually and using a speechrecognizer (more details in section 5.2).In this paper we use a single sequence of 6meetings held between April and June of 2006.
(These were separate from the ones used for theWizard of Oz study above.)
The meetings wereon average 28 minutes and 43 seconds long (?
3minutes and 48 seconds standard error) countingfrom the beginning of the first recorded utteranceto the end of the last one.
On average each meet-ing had 28 minutes and 38 seconds of speech ?this includes overlapped speech when multipleparticipants spoke on top of each other.
Acrossthe 6 meetings there were 5 unique participants;each meeting featured between 2 and 4 of theseparticipants (average: 3.5 ?
0.31).The meetings had, on average, 633.67 (?85.60) utterances each, for a total of 3,796 utter-ances across the 6 meetings.
(In this paper, these3,796 utterances form the units of classification.
)As expected, utterances varied widely in length.On average, utterances were 2.67 ?
0.18 secondslong and contained 7.73 (?
0.44) words.4 Multilevel Noteworthiness AnnotationIn order to develop approaches to automaticallyidentify noteworthy utterances, we have manual-ly annotated each utterance in the meeting datawith its degree of ?noteworthiness?.
While re-searchers in the related field of speech summari-zation typically use a binary labeling ?
?in sum-mary?
versus ?out of summary?
(e.g.
(Galley,2006), (Liu & Liu, 2008), (Penn & Zhu, 2008),etc) ?
we have observed that there are oftenmany utterances that are ?borderline?
at best, andthe decision to label them as ?in summary?
or?out?
is arbitrary.
Our approach instead has beento create three levels of noteworthiness.
Doing soallows us to separate the ?clearly noteworthy?utterances from the ?clearly not noteworthy?,and to label the rest as being between these twoclasses.
(Of course, arbitrary choices must stillbe made between the edges of these threeclasses.
However, having three levels preservesmore information in the labels than having two,and it is always possible to create two labelsfrom the three, as we do in later sections.
)These multilevel noteworthiness annotationswere done by two annotators.
One of them ?denoted as ?annotator 1?
?
had attended each ofthe meetings, while the other ?
?annotator 2?
?had not attended any of the meetings.
Althoughannotator 2 was given a brief overview of thegeneral contents of the meetings, his understand-ing of the meeting was expected to be lower thanthat of the other annotator.
By using such an an-notator, our aim was to identify utterances thatwere ?obviously noteworthy?
even to a humanbeing who lacks a deep understanding of the con-text of the meetings.
(In section 5.2 we describehow we merge the two sets of annotations.
)The annotators were asked to make a 3-leveljudgment about the relative noteworthiness ofeach utterance.
That is, for each utterance, the73annotators were asked to decide whether a note-suggestion system should ?definitely show?
thecontents of the utterance to the meeting partici-pants, or definitely not show (labeled as ?don?tshow?).
Utterances that did not quite belong toeither category were asked to be labeled as?maybe show?.
Utterances labeled ?definitelyshow?
were thus at the highest level of notewor-thiness, followed by those labeled ?maybe show?and those labeled ?don?t show?.
Note that wedid not ask the annotators to label utterances di-rectly in terms of noteworthiness.
Anecdotally,we have observed that asking people to label ut-terances with their noteworthiness leaves the taskinsufficiently well defined because the purposeof the labels is unclear.
On the other hand, askingusers to identify utterances they would have in-cluded in their notes leads to annotators takinginto account the difficulty of writing particularnotes, which is also not desirable for this set oflabels.
Instead, we asked annotators to directlyperform (in some sense) the task that the even-tual notes-assistance system will perform.In order to gain a modicum of agreement inthe annotations, the two annotators discussedtheir annotation strategies after annotating eachof the first two meetings (but not after the latermeetings).
A few general annotation patternsemerged, as follows: Utterances labeled?definitely show?
typically included:(a) Progress on action items since the last week.
(b) Concrete plans of action for the next week.
(c) Announcements of deadlines.
(d) Announcements of bugs in software, etc.In addition, utterances that contained the cruxof any seemingly important discussion werelabeled as ?definitely show?.
On the other hand,utterances that contained no information worthincluding in the notes (by the annotators?judgment) were labeled as ?don?t show?.Utterances that did contain some additionalelaborations of the main point, but without whichthe main point could still be understood by futurereaders of the notes were typically labeled as?maybe show?.Table 1 shows the distribution of the three la-bels across the full set of 3,796 utterances in thedataset for both annotators.
Both annotators la-beled only a small percentage of utterances as?definitely show?, a larger fraction as ?maybeshow?
and most utterances as ?don?t show?.
Al-though the annotators were not asked to shoot fora certain distribution, observe that they both la-beled a similar fraction of utterances as ?definite-ly show?.
On the other hand, annotator 2, whodid not attend the meetings, labeled 50% moreutterances as ?maybe show?
than annotator 1who did attend the meetings.
This difference islikely due to the fact that annotator 1 had a betterunderstanding of the utterances in the meeting,and was more confident in labeling utterances as?don?t show?
than annotator 2 who, not havingattended the meetings, was less sure of some ut-terances, and thus more inclined to label them as?maybe show?.Annotator#DefinitelyshowMaybeshowDon?tshow1 13.5% 24.4% 62.1%2 14.9% 38.8% 46.3%Table 1: Distribution of Labels for Each Annotator4.1 Inter-Annotator Kappa AgreementTo gauge the level of agreement between the twoannotators, we compute the Kappa score.
Givenlabels from different annotators on the same data,this metric quantifies the difference between theobserved agreement between the labels and theexpected agreement, with larger values denotingstronger agreement.For the 3-way labeling task, the two annota-tors achieve a Kappa agreement score of 0.44 (?0.04).
This seemingly low number is typical ofagreement scores obtained in meeting summari-zation.
(Liu & Liu, 2008) reported Kappa agree-ment scores between 0.11 and 0.35 across 6 an-notators while (Penn & Zhu, 2008) with 3 anno-tators achieved Kappa of 0.383 and 0.372 on ca-sual telephone conversations and lecture speech.
(Galley, 2006) reported inter-annotator agree-ment of 0.323 on data similar to ours.To further understand where the disagree-ments lie, we converted the 3-way labeled datainto 2 different 2-way labeled datasets by merg-ing two labels into one.
First we evaluate the de-gree of agreement the annotators have in separat-ing utterances labeled ?definitely show?
from theother two levels.
We do so by re-labeling all ut-terances not labeled ?definitely show?
with thelabel ?others?.
For the ?definitely show?
versus?others?
labeling task, the annotators achieve aninter-annotator agreement of 0.46.
Similarly wecompute the agreement in separating utteranceslabeled ?do not show?
from the two other labels?
in this case the Kappa value is 0.58.
This im-plies that it is easier to agree on the separationbetween ?do not show?
and the other classes,than between ?definitely show?
and the otherclasses.744.2 Inter-Annotator Accuracy, Prec/Rec/FAnother way to gauge the agreement between thetwo sets of annotations is to compute accuracy,precision, recall and f-measure between them.That is, we can designate one annotator?s labelsas the ?gold standard?, and use the other annota-tor?s labels to find, for each of the 3 labels, thenumber of utterances that are true positives, falsepositives, and false negatives.
Using these num-bers we can compute precision as the ratio oftrue positives to the sum of true and false posi-tives, recall as the ratio of true positives to thesum of true positives and false negatives, and f-measure as the harmonic mean of precision andrecall.
(Designating the other annotator?s labelsas ?gold standard?
simply swaps the precisionand recall values, and keeps f-measure the same).Accuracy is the number of utterances that havethe same label from the two annotators, dividedby the total number of utterances.Table 2 shows the evaluation over the 6-meeting dataset using annotator 1?s data as ?goldstandard?.
The standard error for each cell is lessthan 0.08.
Observe in Table 2 that while both the?definitely show?
and ?maybe show?
classeshave nearly equal f-measure, the precision andrecall values for the ?maybe show?
class aremuch farther apart from each other than those forthe ?definitely show?
class.
This is due to thefact that while both annotators label a similarnumber of utterances as ?definitely show?, theylabel very different numbers of utterances as?maybe show?.
If the same accuracy, precision,recall and f-measure scores are computed for the?definitely show?
vs. ?others?
split, the accuracyjumps to 87%, possibly because of the small sizeof the ?definitely show?
category.
The accuracyremains at 78% for the ?don?t show?
vs. ?others?split.DefinitelyshowMaybeshowDon?tshowPrecision 0.57 0.70 0.70Recall 0.53  0.46 0.93F-measure 0.53  0.54 0.80Accuracy 69%Table 2 Inter-Annotator Agreement using Accuracy Etc.4.3 Inter-Annotator Rouge ScoresAnnotations can also be evaluated by computingthe ROUGE metric (Lin, 2004).
ROUGE, a pop-ular metric for summarization tasks, comparestwo summaries by computing precision, recalland f-measure over ngrams that overlap betweenthem.
Following previous work on meetingsummarization (e.g.
(Xie, Liu, & Lin, 2008),(Murray, Renals, & Carletta, 2005), etc), we re-port evaluation using ROUGE-1 F-measure,where the value ?1?
implies that overlapping un-igrams are used to compute the metric.
Unlikeprevious research that had one summary fromeach annotator per meeting, our 3-level annota-tion allows us to have 2 different summaries: (a)the text of all the utterances labeled ?definitelyshow?
and, (b) the text of all the utterances la-beled either ?definitely show?
or ?maybe show?.On average (across both annotators over the 6meetings) the ?definitely show?
utterance textsare 18.72% the size of the texts of all the utter-ances in the meetings, while the ?definitely ormaybe show?
utterance texts are 61.6%.
Thus,these two texts represent two distinct points onthe compression scale.
The average R1 F-measure score is 0.62 over the 6 meetings whencomparing the ?definitely show?
texts of the twoannotators.
This is twice the R1 score ?
0.3 ?
ofthe trivial baseline of simply labeling every ut-terance as ?definitely show?.
The inter-annotatorR1 F-measure for the ?definitely or maybe show?texts is 0.79, marginally higher than the trivial?all utterances?
baseline of 0.71.
In the next sec-tion, we compare the scores achieved by the au-tomatic system against these inter-annotator andtrivial baseline scores.5 Automatic Label PredictionSo far we have presented the annotation of themeeting data, and various analyses thereof.
Inthis section we present our approach for theautomatic prediction of these labels.
We apply aclassification based approach to the problem ofpredicting the noteworthiness level of anutterance, similar to (Banerjee & Rudnicky,2008).
We use leave-one-meeting-out crossvalidation: for each meeting m, we train theclassifier on manually labeled utterances fromthe other 5 meetings, and test the classifier on theutterances of meeting m. We then average theresults across the 6 meetings.
Given the smallamount of data, we do not test on separate data,nor do we perform any tuning.Using the 3-level annotation described above,we train a 3-way classifier to label each utterancewith one of the multilevel noteworthiness labels.In addition, we use the two 2-way merged-labelannotations ?
?definitely show?
vs. others and?don?t show?
vs. others ?
to train two more 2-way classifiers.
In each of these classification75problems we use the same set of features and thesame classification algorithms described below.5.1 Features UsedNgram features: As has been shown by(Banerjee & Rudnicky, 2008), the strongestfeatures for noteworthiness detection are ngramfeatures, i.e.
features that capture the occurrenceof ngrams (consecutive occurrences of one ormore words) in utterances.
Each ngram featurerepresents the presence or absence of a singlespecific ngram in an utterance.
E.g., the ngramfeature ?action item?
represents the occurrenceof the bigram ?action item?
in a given utterance.Unlike (Banerjee & Rudnicky, 2008) where eachngram feature captured the frequency of aspecific ngram in an utterance, in this paper weuse boolean-valued ngram features to capture thepresence/absence of ngrams in utterances.
We doso because in tests on separate data, boolean-valued features out-performed frequency-basedfeatures, perhaps due to data sparseness.
Beforengram features are extracted, utterances arenormalized: partial words, non-lexicalized fillerwords (like ?umm?, ?uh?
), punctuations,apostrophes and hyphens are removed, and allremaining words are changed to upper case.
Next,the vocabulary of ngrams is defined as the set ofngrams that occur at least 5 times in the entiredataset of meetings, for ngram sizes of 1 through6 word tokens.
Finally, the occurrences of eachof these vocabulary ngrams in an utterance arerecorded as the feature vector for that utterance.In the dataset used in this paper, there are 694unique unigrams that occur at least 5 timesacross the 6 meetings, 1,582 bigrams, 1,065trigrams, 1,048 4-grams, 319 5-grams and 102 6-grams.
In addition to these ngram features, foreach utterance we also include the number of Outof Vocabulary ngram ?
ngrams that occur lessthan 5 times across all the meetings.Overlap-based Features: We assume that wehave access to the text of the agenda of the testmeeting, and also the text of the notes taken bythe participants in previous meetings (but notthose taken in the test meeting).
Since theseartifacts are likely to contain important keywordswe compute two sets of overlaps features.
In thefirst set we compute the number of ngrams thatoverlap between each utterance and the meetingagenda.
That is, for each utterance we count thenumber of unigrams, bigrams, trigrams, etc thatalso occur in the agenda of that meeting.Similarly in the second set we compute thenumber of ngrams in each utterance that alsooccur in the notes of previous meetings.
Finally,we compute the degree of overlap between thisutterance and other utterances in the meeting.The motivation for this last feature is to findutterances that are repeats (or near-repeats) ofother utterances ?
repetition may correlate withimportance.Other features: In addition to the ngram andngram overlap features, we also include termfrequency ?
inverse document frequency (tf-idf)features to capture the information content of thengrams in the utterance.
Specifically we computethe TF-IDF of each ngram (of sizes 1 through 5)in the utterance, and include the maximum,minimum, average and standard deviation ofthese values as features of the utterance.
We alsoinclude speaker-based features to capture who isspeaking when.
We include the identity of thespeaker of the current utterance and those of theprevious and next utterances as features.
Lastlywe include the length of the utterance (in seconds)as a feature.5.2 Evaluation ResultsIn this paper we use a Support Vector Machines-based classifier, which is a popular choice forextractive meeting summarization, e.g.
(Xie, Liu,& Lin, 2008); we use a linear kernel in this pa-per.
In the results reported here we use the outputof the Sphinx speech recognizer, using speaker-independent acoustic models, and language mod-els trained on publicly available meeting data.The word error rate was around 44% ?
moredetails of the speech recognition process are in(Huggins-Daines & Rudnicky, 2007).
For train-ing purposes, we merged the annotations fromthe two annotators by choosing a ?middle orlower ground?
for all disagreements.
Thus, if foran utterance the two labels are ?definitely show?and ?don?t show?, we set the merged label as themiddle ground of ?maybe show?.
On the otherhand if the two labels were on adjacent levels,we chose the lower one ?
?maybe show?
whenthe labels were ?definitely show?
and ?maybeshow?, and ?don?t show?
when the labels were?maybe show?
and ?don?t show?.
Thus only ut-terances that both annotators labeled as ?definite-ly show?
were also labeled as ?definitely show?in the merged annotation.
We plan to try othermerging strategies in the future.
For testing, weevaluated against each annotator?s labels sepa-rately, and averaged the results.76DefinitelyshowMaybeshowDon?tshowPrecision 0.21 0.47 0.72Recall 0.16  0.40 0.79F-measure 0.16  0.43 0.75Accuracy 61.4%Table 3 Results of the 3-Way ClassificationTable 3 presents the accuracy, precision, recalland f-measure results of the 3-way classificationtask.
(We use the Weka implementation of SVMthat internally devolves the 3-way classificationtask into a sequence of pair-wise classifications.We use the final per-utterance classificationhere.)
Observe that the overall accuracy of61.4% is only 11% lower relative to the accuracyobtained by comparing the two annotators?
anno-tations (69%, Table 2).
However, the precision,recall and f-measure values for the ?definitelyshow?
class are substantially lower for the pre-dicted labels than the agreement between the twoannotators.
The numbers are closer for the ?may-be show?
and the ?don?t show?
classes.
This im-plies that it is more difficult to accurately detectutterances labeled ?definitely show?
than it is todetect the other classes.
One reason for this dif-ference is the size of each utterance class.
Utter-ances labeled ?definitely show?
are only around14% of all utterances, thus there is less data forthis class than the others.
We also ran the algo-rithm using manually transcribed data, and foundimprovement in only the ?Definitely show?
classwith an f-measure of 0.21.
This improvement isperhaps because the speech recognizer is particu-larly prone to getting names and other technicalterms wrong, which may be important clues ofnoteworthiness.Table 4 presents the ROUGE-1 F-measurescores averaged over the 6 meetings.
(ROUGE isdescribed briefly in section 4.3 and in detail in(Lin, 2004)).
Similar to the inter-annotatoragreement computations, we computed ROUGEbetween the text of the utterances labeled ?defi-nitely show?
by the system against that of utter-ances labeled ?definitely show?
by the two anno-tators.
(We computed the scores separatelyagainst each of the annotators in turn and thenaveraged the two values.)
We did the same thingfor the set of utterances labeled either ?definitelyshow?
or ?maybe show?.
Observe that the R1-Fscore for the ?definitely show?
comparison isnearly 50% relative higher than the trivial base-line of labeling every utterance as ?definitelyshow?.
However the score is 30% lower than thecorresponding inter-annotator agreement.
Thecorresponding R1-Fmeasure score using manualtranscriptions is only marginally better ?
0.47.The set of utterances labeled either definitely ormaybe shows (second row of table 4) does notoutperform the all-utterances baseline when us-ing automatic transcriptions, but does so withmanual transcriptions, whose R1-F value is 0.74.Comparing What R1-FmeasureDefinitely show 0.43Definitely or maybe show 0.63Table 4 ROUGE Scores for the 3-Way ClassificationThese results show that while the detection ofdefinitely show utterances is better than the trivi-al baselines even when using automatic tran-scriptions, there is a lot of room for improve-ment, as compared to human-human agreement.Although direct comparisons to other resultsfrom the meeting summarization literature aredifficult because of the difference in the datasets,numerically it appears that our results are similarto those obtained previously.
(Xie, Liu, & Lin,2008) uses Rouge-1 F-measure solely, andachieve scores between 0.6 to 0.7.
(Murray,Renals, & Carletta, 2005) also achieve Rouge-1scores in the same range with manual transcripts.The trend in the results for the two 2-way clas-sifications is similar to the trend for the inter an-notator agreements.
Just as inter-annotator accu-racy increased to 87% for the ?definitely show?vs.
?others?
classification, so does accuracy ofthe predicted labels increase to 88.3%.
The f-measure for the ?definitely show?
class falls to0.13, much lower than the inter-annotator f-measure of 0.53.
For the ?don?t show?
vs. ?oth-ers?
classification, the automatic system achievesan accuracy of 66.6%.
For the ?definitely plusmaybe?
class, the f-measure is 0.59, which is22% relatively lower than the inter-annotator f-measure for that class.
(As with the 3-way classi-fication, these results are all slightly worse thanthose obtained using manual transcriptions.
)5.3 Useful FeaturesIn order to understand which features contributemost to these results, we used the Chi-Squaredtest of association to find features that are moststrongly correlated to the 3 output classes.
Thebest features are those that measure word over-laps between the utterances and the text in theagenda labels and the notes in previous meetings.This is not a surprising finding ?
the occurrenceof an ngram in an agenda label or in a previousnote is highly indicative of its importance, and77consequently that of the utterances that containthat ngram.
Max and average TF-IDF scores arealso highly ranked features.
These features scorehighly for utterances with seldom-used words,signifying the importance of those utterances.Domain independent ngrams such as ?actionitem?
are strongly correlated with noteworthiness,as are a few domain dependent ngrams such as?time shift problem?.
These latter featuresrepresent knowledge that is transferred from ear-lier meetings to latter ones in the same sequence.The identity of the speaker of the utterance doesnot seem to correlate well with the utterance?snoteworthiness, although this finding couldsimply be an artifact of this particular dataset.6 Related WorkNoteworthiness detection is closely related tomeeting summarization.
Extractive techniquesare popular, e.g.
(Murray, Renals, & Carletta,2005), and many algorithms have been attemptedincluding SVMs (Xie, Liu, & Lin, 2008), Gaus-sian Mixture Models and Maximal Marginal Re-levance (Murray, Renals, & Carletta, 2005), andsequence labelers (Galley, 2006).
Most ap-proaches use a mixture of ngram features, andother structural and semantic features ?
a goodevaluation of typical features can be found in(Xie, Liu, & Lin, 2008).
Different evaluationtechniques have also been tried, with ROUGEoften being shown as at least adequate (Liu &Liu, 2008).
Our work is an application and ex-tension of the speech summarization field to theproblem of assistive note-taking.7 Conclusions and Future WorkIn our work we investigated the problem of de-tecting the noteworthiness of utterances pro-duced in meetings.
We conducted a Wizard-of-Oz-based user study to establish the usefulnessof extracting the text of utterances and suggest-ing these as notes to the meeting participants.
Weshowed that participants were willing to acceptabout 35% of these suggestions over a sequenceof 9 meetings.
We then presented a 3-level note-worthiness annotation scheme that breaks withthe tradition of 2-way ?in/out of summary?
anno-tation.
We showed that annotators have strongagreement for separating the highest level ofnoteworthiness from the other levels.
Finally weused these annotations as labeled data to train aSupport Vector Machine-based classifier whichperformed better than trivial baselines but not aswell as inter-annotator agreement levels.For future work, we plan to use automaticnoteworthiness predictions to suggest notes tomeeting participants during meetings.
We arealso interested in training the noteworthiness de-tector directly from the notes that participantstook in previous meetings, thus reducing theneed for manually annotated data.ReferenceBanerjee, S, and A. I. Rudnicky.
"Segmenting Meet-ings into Agenda Items by Extracting Implicit Su-pervision from Human Note-Taking."
Proceedingsof the International Conference on Intelligent UserInterfaces.
Honolulu, HI, 2007.Banerjee, Satanjeev, and A. I. Rudnicky.
"An Extrac-tive-Summarization Baseline for the AutomaticDetection of Noteworthy Utterances in Multi-PartyHuman-Human Dialog."
IEEE Workshop on Spo-ken Language Technology.
Goa, India, 2008.Galley, Michel.
"A Skip-Chain Conditional RandomField for Ranking Meeting Utterances by Impor-tance."
Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing.
Syd-ney, Australia, 2006.Huggins-Daines, David, and A. I. Rudnicky.
"Impli-citly Supervised Language Model Adaptation forMeeting Transcription."
Proceedings of the HLT-NAACL.
Rochester, NY.
2007.Lin, Chin-Yew.
"ROUGE: A Package for AutomaticEvaluation of Summaries."
Proceedings of theACL-04 Workshop: Text Summarization BranchesOut.
Barcelona, Spain: Association for Computa-tional Linguistics, 2004.
74-81.Liu, Feifan, and Y. Liu.
"Correlation betweenROUGE and Human Evaluation of ExtractiveMeeting Summaries."
Proceedings of ACL-HLT.Columbus, OH, 2008.Murray, Gabriel, S. Renals, and J. Carletta.
"Extrac-tive Summarization of Meeting Recordings."
Pro-ceedings of Interspeech.
Lisbon, Portugal, 2005.Penn, Gerald, and X. Zhu.
"A Critical Reassessmentof Evaluation Baselines for Speech Summariza-tion."
Proceedings of ACL-HLT.
Columbus, OH,2008.Xie, Shasha, Y. Liu, and H. Lin.
"Evaluating the Ef-fectiveness of Features and Sampling in ExtractiveMeeting Summarization."
IEEE Workshop onSpoken Language Technology.
Goa, India, 2008.78
