Proceedings of the TextInfer 2011 Workshop on Textual Entailment, EMNLP 2011, pages 1?9,Edinburgh, Scotland, UK, July 30, 2011. c?2011 Association for Computational LinguisticsEvaluating Answers to Reading Comprehension Questions in Context:Results for German and the Role of Information StructureDetmar Meurers Ramon Ziai Niels Ott Janina KoppSeminar fu?r Sprachwissenschaft / SFB 833Universita?t Tu?bingenWilhelmstra?e 19 / Nauklerstra?e 3572074 Tu?bingen, Germany{dm,rziai,nott,jkopp}@sfs.uni-tuebingen.deAbstractReading comprehension activities are an au-thentic task including a rich, language-basedcontext, which makes them an interesting real-life challenge for research into automatic con-tent analysis.
For textual entailment research,content assessment of reading comprehensionexercises provides an interesting opportunityfor extrinsic, real-purpose evaluation, whichalso supports the integration of context andtask information into the analysis.In this paper, we discuss the first results forcontent assessment of reading comprehensionactivities for German and present results whichare competitive with the current state of theart for English.
Diving deeper into the results,we provide an analysis in terms of the differ-ent question types and the ways in which theinformation asked for is encoded in the text.We then turn to analyzing the role of the ques-tion and argue that the surface-based accountof information that is given in the questionshould be replaced with a more sophisticated,linguistically informed analysis of the informa-tion structuring of the answer in the context ofthe question that it is a response to.1 IntroductionReading comprehension exercises offer a real-lifechallenge for the automatic analysis of meaning.Given a text and a question, the content assessmenttask is to determine whether the answer given to areading comprehension question actually answersthe question or not.
Such reading comprehensionexercises are a common activity in foreign languageteaching, making it possible to use activities whichare authentic and for which the language teachersprovide the gold standard judgements.Apart from the availability of authentic exercisesand independently motivated gold standard judge-ments, there are two further reasons for putting read-ing comprehension tasks into the spotlight for au-tomatic meaning analysis.
Firstly, such activitiesinclude a text as an explicit context on the basis ofwhich the questions are asked.
Secondly, answers toreading comprehension questions in foreign languageteaching typically are between a couple of words andseveral sentences in length ?
too short to rely purelyon the distribution of lexical material (as, e.g., inLSA, Landauer et al, 1998).
The answers also ex-hibit a significant variation in form, including a highnumber of form errors, which makes it necessary todevelop an approach which is robust enough to de-termine meaning correspondences in the presence oferrors yet flexible enough to support the rich vari-ation in form which language offers for expressingrelated meanings.There is relatively little research on content assess-ment for reading comprehension tasks and it so farhas focused exclusively on English, including bothreading comprehension questions answered by na-tive speakers (Leacock and Chodorow, 2003; Nielsenet al, 2009) and by language learners (Bailey andMeurers, 2008).
The task is related to the increas-ingly popular strand of research on Recognizing Tex-tual Entailment (RTE, Dagan et al, 2009) and theAnswer Validation Exercise (AVE, Rodrigo et al,2009), which both have also generally targeted En-glish.1The RTE challenge abstracts away from concretetasks to emphasize the generic semantic inferencecomponent and it has significantly advanced the fieldunder this perspective.
At the same time, an inves-tigation of the role of the context under which aninference holds requires concrete tasks, for whichcontent assessment of reading comprehension tasksseems particularly well-suited.
Borrowing the ter-minology Spa?rck Jones (2007) coined in the contextof evaluating automatic summarization systems, onecan say that we pursue an extrinsic, full-purpose eval-uation of aspects of textual inference.
The contentassessment task provides two distinct opportunitiesto investigate textual entailment: On the one hand,one can conceptualize it as a textual inference taskof deciding whether a given text T supports a partic-ular student answer H .
On the other hand, if targetanswers are provided by the teachers, the task can beseen as a special bi-directional case of textual entail-ment, namely a paraphrase recognition task compar-ing the student answers to the teacher target answers.In this paper, we focus on this second approach.The aim of this paper is twofold.
On the one hand,we want to present the first content assessment ap-proach for reading comprehension activities focusingon German.
In the discussion of the results, we willhighlight the impact of the question types and theway in which the information asked for is encodedin the text.
On the other hand, we want to discussthe importance of the explicit language-based contextand how an analysis of the question and the way atext encodes the information being asked for can helpadvance research on automatic content assessment.Overall, the paper can be understood as a step in thelong-term agenda of exploring the role and impactof the task and the context on the automatic analysisand interpretation of natural language.2 DataThe experiments described in this paper are basedon the Corpus of Reading comprehension Exercisesin German (CREG), which is being collected in col-laboration with two large German programs in theUS, at Kansas University (Prof. Nina Vyatkina) andat The Ohio State University (Prof. Kathryn Corl).German teachers are using the WEb-based LearnerCOrpus MachinE (WELCOME, Meurers et al, 2010)interface to enter the regular, authentic reading com-prehension exercises used in class, which are therebysubmitted to a central corpus repository.
These exer-cises consist of texts, questions, target answers, andcorresponding student answers.
Each student answeris transcribed from the hand-written submission bytwo independent annotators.
These two annotatorsthen assess the contents of the answers with respectto meaning: Did the student provide a meaningfulanswer to the question?
In this binary content as-sessment one thus distinguishes answers which areappropriate from those which are inappropriate interms of meaning, independent of whether the an-swers are grammatically well-formed or not.From the collected data, we selected an even dis-tribution of unique appropriate and inappropriate stu-dent answers in order to obtain a 50% random base-line for our system.
Table 1 lists how many questions,target answers and student answers each of the twodata sets contains.
The data used for this paper ismade freely available upon request under a standardCreative Commons by-nc-sa licence.1KU data set OSU data setTarget Answers 136 87Questions 117 60Student Answers 610 422# of Students 141 175avg.
Token # 9.71 15.00Table 1: The reading comprehension data sets used3 ApproachOur work builds on the English content assessmentapproach of Bailey and Meurers (2008), who pro-pose a Content Assessment Module (CAM) whichautomatically compares student answers to target re-sponses specified by foreign language teachers.
As afirst step we reimplemented this approach for Englishin a system we called CoMiC (Comparing Mean-ing in Context) which is discussed in Meurers et al(2011).
This reimplementation was then adaptedfor German, resulting in the CoMiC-DE system pre-sented in this paper.The comparison of student answers and target an-swer is based on an alignment of tokens, chunks, and1http://creativecommons.org/licenses/by-nc-sa/3.0/2dependency triples between the student and the targetanswer at different levels of abstraction.
Figure 1shows a simple example including token-level andchunk-level alignments between the target answer(TA) and the student answer (SA).Figure 1: Basic example for alignment approachAs the example suggests, it is not sufficient to alignonly identical surface forms given that significant lex-ical and syntactic variation occurs in typical studentanswers.
Alignment thus is supported at differentlevels of abstraction.
For example, the token unitsare enriched with lemma and synonym informationusing standard NLP tools.
Table 2 gives an overviewof which NLP tools we use for which task in CoMiC-DE.
In general, the components are very similar tothose used in the English system, with different sta-tistical models and parameters where necessary.Annotation Task NLP ComponentSentence Detection OpenNLPhttp://incubator.apache.org/opennlpTokenization OpenNLPLemmatization TreeTagger (Schmid, 1994)Spell Checking Edit distance (Levenshtein, 1966),igerman98 word listhttp://www.j3e.de/ispell/igerman98Part-of-speech Tagging TreeTagger (Schmid, 1994)Noun Phrase Chunking OpenNLPLexical Relations GermaNet (Hamp and Feldweg, 1997)Similarity Scores PMI-IR (Turney, 2001)Dependency Relations MaltParser (Nivre et al, 2007)Table 2: NLP tools used in the German systemIntegrating the multitude of units and their rep-resentations at different levels of abstraction posessignificant challenges to the system architecture.Among other requirements, different representationsof the same surface string need to be stored withoutinterfering with each other, and various NLP toolsneed to collaborate in order to produce the final richdata structures used for answer comparison.
To meetthese requirements, we chose to implement our sys-tem in the Unstructured Information ManagementArchitecture (UIMA, cf.
Ferrucci and Lally, 2004).UIMA allows automatic analysis modules to accesslayers of stand-off annotation, and hence allows forthe coexistence of both independent and interdepen-dent annotations, unlike traditional pipeline-style ar-chitectures, where the output of each component re-places its input.
The use of UIMA in recent success-ful large-scale projects such as DeepQA (Ferrucciet al, 2010) confirms that UIMA is a good candi-date for complex language processing tasks whereintegration of various representations is required.In order to determine the global alignment con-figuration, all local alignment options are computedfor every mappable unit.
These local candidates arethen used as input for the Traditional Marriage Al-gorithm (Gale and Shapley, 1962) which computes aglobal alignment solution where each mappable unitis aligned to at most one unit in the other response,such as the one we saw in Figure 1.On the basis of the resulting global alignment con-figuration, the system performs the binary contentassessment by evaluating whether the meaning of thelearner and the target answer are sufficiently similar.For this purpose, it extracts features which encodethe numbers and types of alignment and feeds themto the memory-based classifier TiMBL (Daelemanset al, 2007).
The features used are listed in Table 3.Features Description1.
Keyword Overlap Percent of keywords aligned(relative to target)2./3.
Token Overlap Percent of aligned target/learner tokens4./5.
Chunk Overlap Percent of aligned target/learner chunks6./7.
Triple Overlap Percent of aligned target/learner triples8.
Token Match Percent of token alignmentsthat were token-identical9.
Similarity Match Percent of token alignmentsthat were similarity-resolved10.
Type Match Percent of token alignmentsthat were type-resolved11.
Lemma Match Percent of token alignmentsthat were lemma-resolved12.
Synonym Match Percent of token alignmentsthat were synonym-resolved13.
Variety of Match Number of kinds of token-level(0-5) alignmentsTable 3: Features used for the memory-based classifier34 Content Assessment Experiment4.1 SetupWe ran our content assessment experiment usingthe two data sets introduced in section 2, one fromKansas University and the other from The Ohio StateUniversity.
Both of these contain only records whereboth annotators agreed on the binary assessment (ap-propriate/inappropriate meaning).
Each set is bal-anced, i.e., they contain the same number of appro-priate and inappropriate student answers.In training and testing the TiMBL-based classi-fier, we followed the methodology of Bailey (2008,p.
240), where seven classifiers are trained using thedifferent available distance metrics (Overlap, Leven-shtein, Numeric Overlap, Modified value difference,Jeffrey divergence, Dot product, Cosine).
Trainingand testing was performed using the leave-one-outscheme (Weiss and Kulikowski, 1991) and for eachitem the output of the seven classifiers was combinedvia majority voting.4.2 ResultsThe classification accuracy for both data sets is sum-marized in Table 4.
We report accuracy and the totalnumber of answers for each data set.KU data set OSU data set# of answers 610 422Accuracy 84.6% 84.6%Table 4: Classification accuracy for the two data setsThe 84.6% accuracy figure obtained for both datasets shows that CoMiC-DE is quite successful inperforming content assessment for the German datacollected so far, a result which is competitive withthe one for English obtained by Bailey and Meurers(2008), who report an accuracy of 78% for the binaryassessment task on a balanced English data set.A remarkable feature is the identity of the scoresfor the two data sets, considering that the data wascollected at different universities from different stu-dents in different classes run by different teachers.Moreover, there was no overlap in exercise materialbetween the two data sets.
This indicates that thereis some characteristic uniformity of the learner re-sponses in authentic reading comprehension tasks,suggesting that the course setting and task type effec-tively constrains the degree of syntactic and lexicalvariation in the student answers.
This includes thestage of the learners in this foreign language teachingsetting, which limits their exposure to linguistic con-structions, as well as the presence of explicit readingtexts that the questions are about, which may leadlearners to use the lexical material provided insteadof rephrasing content in other words.
We intend to ex-plore these issues in our future work to obtain a moreexplicit picture of the contextual and task propertiesinvolved.Another aspect which should be kept in mind isthat the scores we obtained are based on a data setfor which the two human annotators had agreed ontheir assessment.
We expect automatic classificationresults to degrade given more controversial data aboutwhich human annotators disagree, especially sincesuch data will presumably contain more ambiguouscues, giving rise to multiple interpretations.4.3 Evaluation by question typeThe overall results include many different questiontypes which pose different kinds of challenges toour system.
To develop an understanding of thosechallenges, we performed a more fine-grained evalu-ation by question types.
To distinguish relevant sub-cases, we applied the question classification schemeintroduced by Day and Park (2005).
This scheme ismore suitable here than other common answer-typingschemata such as the one in Li and Roth (2002),which tend to focus on questions asking for factualknowledge.Day and Park (2005) distinguish five differentquestion forms: yes/no (question to be answeredwith either yes or no), alternative (two or moreyes/no questions connected with or), true or false(a statement to be classified as true or false),who/what/when/where/how/why (wh-question con-taining the respective question word), and multiplechoice (choice between several answers presentedwith a question, of any other question type).
In addi-tion, they introduce a second dimension distinguish-ing the types of comprehension involved, i.e., howthe information asked for by the question can be ob-tained from the text: literal (questions that can be an-swered directly and explicitly from the text), reorga-nization (questions where information from various4parts of the text must be combined), inference (ques-tions where literal information and world knowledgemust be combined), prediction (prediction of howa story might continue), evaluation (comprehensivejudgement about aspects of the text) and personalresponse (personal opinion or feelings about the textor the subject).Out of the five different forms of question, ourdata contains questions of all forms except for themultiple choice category and the true or false cate-gory given that we are explicitly targeting free textresponses.
To obtain a more detailed picture of thewh-question category, we decided to split that cat-egory into its respective wh-words and added onemore category to it, for which.
Also, we added thetype ?several?
for questions which contain more thanone question presented to the student at a time.
Of thesix comprehension types, our data contained literal,reorganization and inference questions.Table 5 reports the accuracy results by questionforms and comprehension types for the combinedOSU and KU data set.
The counts encode the num-ber of student answers for which accuracy is reported(micro-averages).
The numbers in brackets specifythe number of distinct questions and the correspond-ing accuracy measures are computed by groupinganswers by their question (macro-averages).
Com-paring answer-based (micro-average) accuracy withquestion-based (macro-average) accuracy allows usto see whether the results for questions with a highnumber of answers outweigh questions with a smallnumber of answers.
In general the micro- and macro-averages reported are very similar and the overallaccuracy is the same (84.6%).
Overall, the resultsthus do not seem to be biased towards a specific, fre-quently answered question instance.
Where largerdifferences between micro- and macro-averages doarise, as for alternative, when, and where questions,these are cases with few overall instances in the dataset, cautioning us against overinterpreting results forsuch small subsets.
The 4.2% gap for the relativelyfrequent ?several?
question type underlines the het-erogeneous nature of this class, which may warrantmore specific subclasses in the future.Overall, the accuracy of content assessment forwh-questions that can be answered with a concretepiece of information from the text are highest, with92.6% for ?which?
questions, and results in the upper80s for five other wh-questions.
Interestingly, ?who?questions fare comparatively badly, pointing to a rel-atively high variability in the expression of subjects,which would warrant the integration of a dedicatedapproach to coreference resolution.
Such a direct so-lution is not available for ?why?
questions, which at79.3% is the worst wh-question type.
The high vari-ability of those answers is rooted in the fact that theyask for a cause or reason, which can be expressed ina multitude of ways, especially for comprehensiontypes involving inferences or reorganization of theinformation given in the text.This drop between comprehension types, from lit-eral (86.0%) to inference (81.5%) and reorganization(78.0%), can also be observed throughout and is ex-pected given that the CoMiC-DE system makes useof surface-based alignments where it can find them.For the system to improve on the non-literal com-prehension types, features encoding a richer set ofabstractions (e.g., to capture distributional similarityat the chunk level or global linguistic phenomenasuch as negation) need to be introduced.Just as in the discussion of the micro- and macro-averages above, the ?several?
question type againrears its ugly heads in terms of a low overall accuracy(77.7%).
This supports the conclusion that it requiresa dedicated approach.
Based on an analysis of thenature and sequence of the component questions, infuture work we plan to determine how such combi-nations constrain the space of variation in acceptableanswers.Finally, while there are few instances for the ?al-ternative?
question type, the fact that it resulted inthe lowest accuracy (57.1%) warrants some attention.The analysis indeed revealed a general issue, whichis discussed in the next section.5 From eliminating repeated elements toanalyzing information structureBailey (2008, sec.
5.3.12) observed that answers fre-quently repeat words given in the question.
In her cor-pus example (1), the first answer repeats ?the moralquestion raised by the Clinton incident?
from thequestion, whereas the second one reformulates thisgiven material.
But both sentences essentially answerthe question in the same way.22Independent of the issue discussed here, note the presuppo-5Comprehension typeLiteral Reorganization Inference TotalQuestion type Acc.
# Acc.
# Acc.
# Acc.
#Alternative 0 1 (1) ?
0 66.7 (58.3) 6 (3) 57.1 (43.8) 7 (4)How 85.7 (83.3) 126 (25) 83.3 (77.8) 12 (3) 100 7 (1) 86.2 (83.3) 145 (29)What 87.0 (87.6) 247 (40) 74.2 (71.7) 31 (4) 83.3 (83.3) 6 (1) 85.6 (86.1) 284 (45)When 85.7 (93.3) 7 (3) ?
0 ?
0 85.7 (93.3) 7 (3)Where 88.9 (94.4) 9 (3) ?
0 ?
0 88.9 (94.4) 9 (3)Which 92.3 (90.7) 183 (29) 100.0 14 (5) 83.3 (83.3) 6 (2) 92.6 (91.6) 203 (36)Who 73.9 (80.2) 23 (9) 94.4 (88.9) 18 (3) ?
0 82.9 (82.4) 41 (12)Why 80.5 (83.3) 128 (17) 57.1 (57.9) 14 (3) 84.4 (81.1) 32 (4) 79.3 (79.7) 174 (24)Yes/No ?
0 100.0 5 (1) ?
0 100.0 5 (1)Several 82.1 (85.6) 95 (13) 68.4 (75.1) 38 (5) 75 (74.3) 24 (2) 77.7 (81.9) 157 (20)Total 86.0 (86) 819 (140) 78.0 (80.7) 132 (24) 81.5 (76.8) 81 (13) 84.6 (84.6) 1032 (177)Table 5: Accuracy by question form and comprehension types following Day and Park (2005).
Counts denoting numberof student answers, in brackets: number of questions and macro-average accuracy computed by grouping by questions.
(1) What was the major moral question raised bythe Clinton incident?a.
The moral question raised by the Clintonincident was whether a politician?s personlife is relevant to their job performance.b.
A basic question for the media is whethera politician?s personal life is relevant to hisor her performance in the job.The issue arising from the occurrence of suchgiven material for a content assessment approachbased on alignment is that all alignments are counted,yet those for given material do not actually con-tribute to answering the question, as illustrated bythe (non)answer containing only given material ?Themoral question raised by the Clinton incident waswhatever.?
Bailey (2008) concludes that an answershould not be rewarded (or punished) for repeatingmaterial that is given in the question and her imple-mentation thus removes all words from the answerswhich are given in the question.While such an approach successfully eliminatesany contribution from these given words, it has the un-fortunate consequence that any NLP processes requir-ing well-formed complete sentences (such as, e.g.,dependency parsers) perform poorly on sentencesfrom which the given words have been removed.
Inour reimplementation of the approach, we thereforekept the sentences as such intact and instead madesition failure arising for this authentic reading comprehensionquestion ?
as far as we see, there was no ?major moral questionraised by the Clinton incident?.use of the UIMA architecture to add a givennessannotation to those words of the answer which arerepeated from the question.
Such given tokens andany representations derived from them are ignoredwhen the local alignment possibilities are computed.While successfully replicating the givenness filterof Bailey (2008) without the negative consequenceson other NLP analysis, targeting given words in thisway is problematic, which becomes particularly ap-parent when considering examples for the ?alterna-tive?
question type.
In this question type, exemplifiedin Figure 2 by an example from the KU data set, theanswer has to select one of the options from an ex-plicitly given set of alternatives.Q: Ist die Wohnung in einem Neubau oder einem Altbau?
?Is the flat in a new building or in an old building?
?TA: DieTheWohnungflatistisinineinemaNeubaunew building.SA: DieTheWohnungflatistisinineinemaNeubaunew buildingFigure 2: ?Alternative?
question with answers consistingentirely of given words, resulting in no alignments.The question asks whether the apartment is in anew or in an old building, and both alternatives areexplicitly given in the question.
The student pickedthe same alternative as the one that was selected inthe target answer.
Indeed, the two answers are iden-tical, but the givenness filter excludes all materialfrom alignment and hence the content assessment6classification fails to identify the student answer asappropriate.
This clearly is incorrect and essentiallyconstitutes an opportunity to rethink the givennessfilter.The givenness filter is based on a characterizationof the material we want to ignore, which was moti-vated by the fact that it is easy to identify the materialthat is repeated from the question.
On the other hand,if we analyze the reading comprehension questionsmore closely, it becomes possible to connect thisissue to research in formal pragmatics which inves-tigates the information structure (cf.
Krifka, 2007)imposed on a sentence in a discourse addressingan explicit (or implicit) question under discussion(Roberts, 1996).
Instead of removing given elementsfrom an answer, under this perspective we want toidentify which part of an answer constitutes the so-called focus answering the question.3The advantage of linking our issue to the moregeneral investigation of information structure in lin-guistics is readily apparent if we consider the signif-icant complexity involved (cf., e.g., Bu?ring, 2007).The issue of asking what constitutes the focus of asentence is distinct from asking what new informa-tion is included in a sentence.
New information canbe contained in the topic of a sentence.
On the otherhand, the focus can also contain given information.In (2a), for example, the focus of the answer is ?agreen apple?, even though apples are explicitly givenin the question and only the fact that a green one willbe bought is new.
(2) You?ve looked at the apples long enough now,what do you want to buy?a.
I want to buy a green apple.In some situations the focus can even consist en-tirely of given information.
This is one way of in-terpreting what goes on in the case of the alternativequestions discussed at the end of the last section.This question type explicitly mentions all alternativesas part of the question, so that the focus of the an-swer selecting one of those alternatives will typically3The information structure literature naturally also providesa more sophisticated account of givenness.
For example, forSchwarzschild (1999), givenness also occurs between hypernymsand coreferent expressions, which would not be detected by thesimple surface-based givenness filter included in the currentCoMiC-DE.consist entirely of given information.As a next step we plan to build on the notion offocus characterized in (a coherent subset of) the infor-mation structure literature by developing an approachwhich identifies the part of an answer which consti-tutes the focus so that we can limit the alignmentprocedure on which content assessment is based tothe focus of each answer.6 Related WorkThere are few systems targeting the short answer eval-uation tasks.
Most prominent among them is C-Rater(Leacock and Chodorow, 2003), a short answer scor-ing system for English meant for deployment in Intel-ligent Tutoring Systems (ITS).
The authors highlightthe fact that C-Rater is not simply a string matchingprogram but instead uses more sophisticated NLPsuch as shallow parsing and synonym matching.
C-Rater reportedly achieved an accuracy of 84% in twodifferent studies, which is remarkably similar to thescores we report in this paper although clearly thesetting and target language differ from ours.More recently in the ITS field, Nielsen et al (2009)developed an approach focusing on recognizing tex-tual entailment in student answers.
To that end, acorpus of questions and answers was manually an-notated with word-word relations, so-called ?facets?,which represent individual semantic propositions in aparticular answer.
By learning how to recognize andclassify these facets in student answers, the systemis then able to give a more differentiated rating ofa student answer than ?right?
or ?wrong?.
We findthat this is a promising move in the fields of answerscoring and textual entailment since it also breaksdown the complex entailment problem into a set ofsub-problems.7 ConclusionWe presented CoMiC-DE, the first content assess-ment system for German.
For the data used in evalu-ation so far, CoMiC-DE performs on a competitivelevel when compared to previous work on English,with accuracy at 84.6%.
In addition to these results,we make our reading comprehension corpus freelyavailable for research purposes in order to encouragemore work on content assessment and related areas.In a more detailed evaluation by question and com-7prehension type, we gained new insights into howquestion types influence the content assessment tasks.Specifically, our system had more difficulty classify-ing answers to ?why?-questions than other questionforms, which we attribute to the fact that causal re-lations exhibit more form variation than other typesof answer material.
Also, the comprehension type?reorganization?, which requires the reader to collectand combine information from different places in thetext, posed more problems to our system than the?literal?
type.Related to the properties of questions, we showedby example that simply marking given material ona surface level is insufficient and a partitioning intofocused and background material is needed instead.This is especially relevant for alternative questions,where the exclusion of all given material renders thealignment process useless.
Future work will thereforeinclude focus detection in answers and its use in thealignment process.
For example, given a weightingscheme for individual alignments, focused materialcould be weighted more prominently in alignment inorder to reflect its importance in assessing the answer.AcknowledgementsWe would like to thank two anonymous TextInferreviewers for their helpful comments.ReferencesStacey Bailey, 2008.
Content Assessment in Intelli-gent Computer-Aided Language Learning: Mean-ing Error Diagnosis for English as a Second Lan-guage.
Ph.D. thesis, The Ohio State University.http://osu.worldcat.org/oclc/243467551.Stacey Bailey and Detmar Meurers, 2008.
Diagnos-ing Meaning Errors in Short Answers to Read-ing Comprehension Questions.
In Proceedingsof the 3rd Workshop on Innovative Use of NLPfor Building Educational Applications (BEA-3)at ACL?08.
Columbus, Ohio, pp.
107?115.
http://aclweb.org/anthology/W08-0913.Daniel Bu?ring, 2007.
Intonation, Semantics and In-formation Structure.
In Gillian Ramchand andCharles Reiss (eds.
), The Oxford Handbook of Lin-guistic Interfaces, Oxford University Press.Walter Daelemans, Jakub Zavrel, Ko van der Slootand Antal van den Bosch, 2007.
TiMBL: TilburgMemory-Based Learner Reference Guide, ILKTechnical Report ILK 07-03.
Version 6.0.
TilburgUniversity.Ido Dagan, Bill Dolan, Bernardo Magnini and DanRoth, 2009.
Recognizing textual entailment: Ratio-nal, evaluation and approaches.
Natural LanguageEngineering, 15(4):i?xvii.Richard R. Day and Jeong-Suk Park, 2005.
Develop-ing Reading Comprehension Questions.
Readingin a Foreign Language, 17(1):60?73.David Ferrucci, Eric Brown et al, 2010.
BuildingWatson: An Overview of the DeepQA Project.
AIMagazine, 31(3):59?79.David Ferrucci and Adam Lally, 2004.
UIMA: AnArchitectural Approach to Unstructured Informa-tion Processing in the Corporate Research Envi-ronment.
Natural Language Engineering, 10(3?4):327?348.David Gale and Lloyd S. Shapley, 1962.
College Ad-missions and the Stability of Marriage.
AmericanMathematical Monthly, 69:9?15.Birgit Hamp and Helmut Feldweg, 1997.
GermaNet?
a Lexical-Semantic Net for German.
In Pro-ceedings of ACL workshop Automatic Informa-tion Extraction and Building of Lexical SemanticResources for NLP Applications.
Madrid.
http://aclweb.org/anthology/W97-0802.Manfred Krifka, 2007.
Basic Notions of InformationStructure.
In Caroline Fery, Gisbert Fanselow andManfred Krifka (eds.
), The Notions of InformationStructure, Universita?tsverlag Potsdam, Potsdam,volume 6 of Interdisciplinary Studies on Informa-tion Structure (ISIS).Thomas Landauer, Peter Foltz and Darrell Laham,1998.
An Introduction to Latent Semantic Analysis.Discourse Processes, 25:259?284.Claudia Leacock and Martin Chodorow, 2003.
C-rater: Automated Scoring of Short-Answer Ques-tions.
Computers and the Humanities, 37:389?405.Vladimir I. Levenshtein, 1966.
Binary Codes Capa-ble of Correcting Deletions, Insertions, and Rever-sals.
Soviet Physics Doklady, 10(8):707?710.Xin Li and Dan Roth, 2002.
Learning Question Clas-sifiers.
In Proceedings of the 19th International8Conference on Computational Linguistics (COL-ING 2002).
Taipei, Taiwan, pp.
1?7.Detmar Meurers, Niels Ott and Ramon Ziai, 2010.Compiling a Task-Based Corpus for the Analysisof Learner Language in Context.
In Proceedings ofLinguistic Evidence.
Tu?bingen, pp.
214?217.
http://purl.org/dm/papers/meurers-ott-ziai-10.html.Detmar Meurers, Ramon Ziai, Niels Ott and StaceyBailey, 2011.
Integrating Parallel Analysis Mod-ules to Evaluate the Meaning of Answers toReading Comprehension Questions.
IJCEELL.Special Issue on Automatic Free-text Evalua-tion, 21(4):355?369.
http://purl.org/dm/papers/meurers-ziai-ott-bailey-11.html.Rodney D. Nielsen, Wayne Ward and James H. Mar-tin, 2009.
Recognizing entailment in intelligenttutoring systems.
Natural Language Engineering,15(4):479?501.Joakim Nivre, Jens Nilsson, Johan Hall, AtanasChanev, Gu?lsen Eryigit, Sandra Ku?bler, SvetoslavMarinov and Erwin Marsi, 2007.
MaltParser: ALanguage-Independent System for Data-DrivenDependency Parsing.
Natural Language Engineer-ing, 13(1):1?41.Craige Roberts, 1996.
Information Structure in Dis-course: Towards an Integrated Formal Theory ofPragmatics.
In Jae-Hak Yoon and Andreas Kathol(eds.
), OSU Working Papers in Linguistics No.
49:Papers in Semantics, The Ohio State University.A?lvaro Rodrigo, Anselmo Pen?as and Felisa Verdejo,2009.
Overview of the Answer Validation Exercise2008.
In Carol Peters, Thomas Deselaers, NicolaFerro, Julio Gonzalo, Gareth Jones, Mikko Ku-rimo, Thomas Mandl, Anselmo Pen?as and VivienPetras (eds.
), Evaluating Systems for Multilin-gual and Multimodal Information Access, SpringerBerlin / Heidelberg, volume 5706 of Lecture Notesin Computer Science, pp.
296?313.Helmut Schmid, 1994.
Probabilistic Part-of-SpeechTagging Using Decision Trees.
In Proceedings ofthe International Conference on New Methods inLanguage Processing.
Manchester, UK, pp.
44?49.Roger Schwarzschild, 1999.
GIVENness, AvoidFand other Constraints on the Placement of Accent.Natural Language Semantics, 7(2):141?177.Karen Spa?rck Jones, 2007.
Automatic Summarising:The State of the Art.
Information Processing andManagement, 43:1449?1481.Peter Turney, 2001.
Mining the Web for Synonyms:PMI-IR Versus LSA on TOEFL.
In Proceedingsof the Twelfth European Conference on MachineLearning (ECML-2001).
Freiburg, Germany, pp.491?502.Sholom M. Weiss and Casimir A. Kulikowski, 1991.Computer Systems That Learn: Classification andPrediction Methods from Statistics, Neural Nets,Machine Learning, and Expert Systems.
MorganKaufmann, San Mateo, CA.9
