Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 144?152,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsA Comparative Study of Bayesian Models for Unsupervised SentimentDetectionChenghua LinSchool of Engineering,Computing and MathematicsUniversity of ExeterExeter, EX4 4QF, UK.cl322@exeter.ac.ukYulan HeKnowledge Media InstituteThe Open UniversityMilton KeynesMK7 6AA, UKY.He@open.ac.ukRichard EversonSchool of Engineering,Computing and MathematicsUniversity of ExeterExeter, EX4 4QF, UK.R.E.Everson@exeter.ac.ukAbstractThis paper presents a comparative studyof three closely related Bayesian mod-els for unsupervised document level senti-ment classification, namely, the latent sen-timent model (LSM), the joint sentiment-topic (JST) model, and the Reverse-JSTmodel.
Extensive experiments have beenconducted on two corpora, the movie re-view dataset and the multi-domain senti-ment dataset.
It has been found that whileall the three models achieve either bet-ter or comparable performance on thesetwo corpora when compared to the exist-ing unsupervised sentiment classificationapproaches, both JST and Reverse-JST areable to extract sentiment-oriented topics.In addition, Reverse-JST always performsworse than JST suggesting that the JSTmodel is more appropriate for joint senti-ment topic detection.1 IntroductionWith the explosion of web 2.0, various types ofsocial media such as blogs, discussion forums andpeer-to-peer networks present a wealth of infor-mation that can be very helpful in assessing thegeneral public?s sentiments and opinions towardsproducts and services.
Recent surveys have re-vealed that opinion-rich resources like online re-views are having greater economic impact on bothconsumers and companies compared to the tradi-tional media (Pang and Lee, 2008).
Driven bythe demand of gleaning insights of such greatamounts of user-generated data, work on newmethodologies for automated sentiment analysishas bloomed splendidly.Compared to the traditional topic-based textclassification, sentiment classification is deemedto be more challenging as sentiment is often em-bodied in subtle linguistic mechanisms such asthe use of sarcasm or incorporated with highlydomain-specific information.
Although the taskof identifying the overall sentiment polarity ofa document has been well studied, most of thework is highly domain dependent and favouredin supervised learning (Pang et al, 2002; Pangand Lee, 2004; Whitelaw et al, 2005; Kennedyand Inkpen, 2006; McDonald et al, 2007), re-quiring annotated corpora for every possible do-main of interest, which is impractical for realapplications.
Also, it is well-known that senti-ment classifiers trained on one domain often failto produce satisfactory results when shifted to an-other domain, since sentiment expression can bequite different in different domains (Aue and Ga-mon, 2005).
Moreover, aside from the diversityof genres and large-scale size of Web corpora,user-generated contents evolve rapidly over time,which demands much more efficient algorithmsfor sentiment analysis than the current approachescan offer.
These observations have thus motivatedthe problem of using unsupervised approaches fordomain-independent joint sentiment topic detec-tion.Some recent research efforts have been made toadapt sentiment classifiers trained on one domainto another domain (Aue and Gamon, 2005; Blitzeret al, 2007; Li and Zong, 2008; Andreevskaiaand Bergler, 2008).
However, the adaption perfor-mance of these lines of work pretty much dependson the distribution similarity between the sourceand target domain, and considerable effort is stillrequired to obtain labelled data for training.Intuitively, sentiment polarities are dependenton contextual information, such as topics or do-mains.
In this regard, some recent work (Mei etal., 2007; Titov and McDonald, 2008a) has tried tomodel both sentiment and topics.
However, thesetwo models either require postprocessing to calcu-late the positive/negative coverage in a documentfor polarity identification (Mei et al, 2007) or re-144quire some kind of supervised setting in whichreview text should contain ratings for aspects ofinterest (Titov and McDonald, 2008a).
More re-cently, Dasgupta and Ng (2009) proposed an unsu-pervised sentiment classification algorithm by in-tegrating user feedbacks into a spectral clusteringalgorithm.
Features induced for each dimension ofspectral clustering can be considered as sentiment-oriented topics.
Nevertheless, human judgementof identifying the most important dimensions dur-ing spectral clustering is required.Lin and He (2009) proposed a joint sentiment-topic (JST) model for unsupervised joint senti-ment topic detection.
They assumed that top-ics are generated dependent on sentiment distri-butions and then words are generated conditionedon sentiment-topic pairs.
While this is a reason-able design choice, one may argue that the re-verse is also true that sentiments may vary ac-cording to topics.
Thus in this paper, we studiedthe reverse dependence of the JST model calledReverse-JST, in which sentiments are generateddependent on topic distributions in the modellingprocess.
We also note that, when the topic num-ber is set to 1, both JST and reversed-JST es-sentially become a simple latent Dirichlet aloca-tion (LDA) model with only S (number of sen-timent label) topics, each of which correspondsto a sentiment label.
We called it latent senti-ment model (LSM) in this paper.
Extensive ex-periments have been conducted on the movie re-view (MR)1 (Pang et al, 2002) and multi-domainsentiment (MDS)2 (Blitzer et al, 2007) datasetsto compare the performance of LSM, JST andReverse-JST.
Results show that all these threemodels are able to give either better or compara-ble performance compared to the existing unsu-pervised sentiment classification approaches.
Inaddition, both JST and reverse-JST are able to ex-tract sentiment-oriented topics.
Furthermore, thefact that reverse-JST always performs worse thanJST suggests that the JST model is more appropri-ate for joint sentiment topic detection.The rest of the paper is organized as follows.Section 2 presents related work.
Section 3 de-scribes the LSM, JST and Reserver-JST models.Experimental setup and results on the MR andMDS datasets are discussed in Section 4 and 5 re-1http://www.cs.cornell.edu/people/pabo/movie-review-data2http://www.cs.jhu.edu/?mdredze/datasets/sentiment/index2.htmlspectively.
Finally, Section 6 concludes the paperand outlines the future work.2 Related WorkAs opposed to the work (Pang et al, 2002; Pangand Lee, 2004; Whitelaw et al, 2005; Kennedyand Inkpen, 2006) that only focused on senti-ment classification in one particular domain, re-cent research attempts have been made to addressthe problem of sentiment classification across do-mains.
Aue and Gamon (2005) explored vari-ous strategies for customizing sentiment classifiersto new domains, where the training is based ona small number of labelled examples and largeamounts of unlabelled in-domain data.
However,their experiments achieved only limited success,with most of the classification accuracy below80%.
In the same vein, some more recent workfocused on domain adaption for sentiment classi-fiers.
Blitzer et al (2007) used the structural corre-spondence learning (SCL) algorithm with mutualinformation.
Li and Zong (2008) combined multi-ple single classifiers trained on individual domainsusing SVMs.
However, the adaption performancein (Blitzer et al, 2007) depends on the selection ofpivot features that used to link the source and tar-get domains; whereas the approach of Li and Zong(2008) heavily relies on labelled data from all thedomains to train the integrated classifier and thuslack the flexibility to adapt the trained classifier todomains where label information is not available.Recent years have also seen increasing interestsin modelling both sentiment and topics simultane-ously.
The topic-sentiment mixture (TSM) model(Mei et al, 2007) can jointly model sentiment andtopics by constructing an extra background com-ponent and two additional sentiment subtopics ontop of the probabilistic latent semantic indexing(pLSI) (Hofmann, 1999).
However, TSM maysuffer from the problem of overfitting the datawhich is known as a deficiency of pLSI, and post-processing is also required in order to calculatethe sentiment prediction for a document.
Themulti-aspect sentiment (MAS) model (Titov andMcDonald, 2008a), which is extended from themulti-grain latent Dirichlet alocation (MG-LDA)model (Titov and McDonald, 2008b), allows sen-timent text aggregation for sentiment summary ofeach rating aspect extracted from MG-LDA.
Onedrawback of MAS is that it requires that every as-pect is rated at least in some documents, which145is practically infeasible.
More recently, Dasguptaand Ng (2009) proposed an unsupervised sen-timent classification algorithm where user feed-backs are provided on the spectral clustering pro-cess in an interactive manner to ensure that text areclustered along the sentiment dimension.
Featuresinduced for each dimension of spectral cluster-ing can be considered as sentiment-oriented top-ics.
Nevertheless, human judgement of identify-ing the most important dimensions during spectralclustering is required.Among various efforts for improving senti-ment detection accuracy, one direction is to in-corporate prior information or subjectivity lexi-con (i.e., words bearing positive or negative sen-timent) into the sentiment model.
Such sen-timent lexicons can be acquired from domain-independent sources in many different ways, frommanually built appraisal groups (Whitelaw etal., 2005), to semi-automatically (Abbasi et al,2008) and fully automatically (Kaji and Kitsure-gawa, 2006) constructed lexicons.
When incor-porating lexical knowledge as prior informationinto a sentiment-topic model, Andreevskaia andBergler (2008) integrated the lexicon-based andcorpus-based approaches for sentence-level sen-timent annotation across different domains; Li etal.
(2009) employed lexical prior knowledge forsemi-supervised sentiment classification based onnon-negative matrix tri-factorization, where thedomain-independent prior knowledge was incor-porated in conjunction with domain-dependent un-labelled data and a few labelled documents.
How-ever, this approach performed worse than the JSTmodel on the movie review data even with 40%labelled documents as will be shown in Section 5.3 Latent Sentiment-Topic ModelsThis section describes three closely relatedBayesian models for unsupervised sentiment clas-sification, the latent sentiment model (LSM), thejoint sentiment-topic (JST) model, and the jointtopic sentiment model by reversing the generativeprocess of sentiment and topics in the JST model,called Reverse-JST.3.1 Latent Sentiment Model (LSM)The LSM model, as shown in Figure 1(a), can betreated as a special case of LDA where a mixtureof only three sentiment labels are modelled, i.e.positive, negative and neutral.Assuming that we have a total number of S sen-timent labels3; a corpus with a collection of Ddocuments is denoted by C = {d1, d2, ..., dD};each document in the corpus is a sequence of Ndwords denoted by d = (w1, w2, ..., wNd), andeach word in the document is an item from a vo-cabulary index with V distinct terms denoted by{1, 2, ..., V }.
The procedure of generating a wordin LSM starts by firstly choosing a distributionover three sentiment labels for a document.
Fol-lowing that, one picks up a sentiment label fromthe sentiment label distribution and finally draws aword according to the sentiment label-word distri-bution.The joint probability of words and sentiment la-bel assignment in LSM can be factored into twoterms:P (w, l) = P (w|l)P (l|d).
(1)Letting the superscript ?t denote a quantity thatexcludes data from the tth position, the conditionalposterior for lt by marginalizing out the randomvariables ?
and pi isP (lt = k|w, l?t, ?,?)
?N?twt,k + ?N?tk + V ?
?N?tk,d + ?kN?td +?k ?k, (2)where Nwt,k is the number of times word wt hasassociated with sentiment label k; Nk is the thenumber of times words in the corpus assigned tosentiment label k; Nk,d is the number of timessentiment label k has been assigned to some wordtokens in document d; Nd is the total number ofwords in the document collection.Gibbs sampling is used to estimate the poste-rior distribution of LSM, as well as the JST andReverse-JST models that will be discussed in thefollowing two sections.3.2 Joint Sentiment-Topic Model (JST)In contrast to LSM that only models documentsentiment, the JST model (Lin and He, 2009)can detect sentiment and topic simultaneously, bymodelling each document with S (number of sen-timent labels) topic-document distributions.
Itshould be noted that when the topic number isset to 1, JST effectively becomes the LSM modelwith only three topics corresponding to each of the3For all the three models, i.e., LSM, JST and Reverse-JST, we set the sentiment label number S to 3 representingthe positive, negative and neutral polarities, respectively.146wlN d DS(a)wzlN d DSS * T(b)wlfizN d DTT * S$%& '(c)Figure 1: (a) LSM model; (b) JST model; (c) Reverse-JST model.three sentiment labels.
Let T be the total num-ber of topics, the procedure of generating a wordwi according to the graphical model shown in Fig-ure 1(b) is:?
For each document d, choose a distributionpid ?
Dir(?).?
For each sentiment label l of document d,choose a distribution ?d,l ?
Dir(?).?
For each word wi in document d?
choose a sentiment label li ?Multinomial(pid),?
choose a topic zi ?
Multinomial(?d,li),?
choose a word wi from ?lizi , a Multi-nomial distribution over words condi-tioned on topic zi and sentiment label li.In JST, the joint probability of words and topic-sentiment label assignments can be factored intothree terms:P (w, z, l) = P (w|z, l)P (z|l, d)P (l|d).
(3)The conditional posterior for zt and lt can be ob-tained by marginalizing out the random variables?, ?, and pi:P (zt = j, lt = k|w, z?t, l?t, ?, ?,?)
?N?twt,j,k + ?N?tj,k + V ?
?N?tj,k,d + ?N?tk,d + T?
?N?tk,d + ?kN?td +?k ?k, (4)where Nwt,j,k is the number of times word wt ap-peared in topic j and with sentiment label k; Nj,kis the number of times words assigned to topicj and sentiment label k, Nk,j,d is the number oftimes a word from document d has been associ-ated with topic j and sentiment label k; Nk,d isthe number of times sentiment label k has beenassigned to some word tokens in document d.3.3 Reverse Joint Sentiment-Topic Model(Reverse-JST)We also studied a variant of the JST model,called Reverse-JST.
As opposed to JST in whichtopic generation is conditioned on sentiment la-bels, sentiment label generation in Reverse-JST isdependent on topics.
As shown in Figure 1(c),Reverse-JST is effectively a four-layer hierarchi-cal Bayesian model, where topics are associatedwith documents, under which sentiment labels areassociated with topics and words are associatedwith both topics and sentiment labels.The procedure of generating a word wi inReverse-JST is shown below:?
For each document d, choose a distribution?d ?
Dir(?).?
For each topic z of document d, choose a dis-tribution pid,z ?
Dir(?).?
For each word wi in document d?
choose a topic zi ?
Multinomial(?d),?
choose a sentiment label li ?Multinomial(pid,zi),?
choose a word wi from ?lizi , a multi-nomial distribution over words condi-tioned on the topic zi and sentiment la-bel li.Analogy to JST, in Reverse-JST the joint prob-ability of words and the topic-sentiment label as-signments can be factored into the following threeterms:P (w, l, z) = P (w|l, z)P (l|z, d)P (z|d), (5)and the conditional posterior for zt and lt can bederived by integrating out the random variables ?,147?, and pi, yieldingP (zt = j, lt = k|w, z?t, l?t, ?, ?,?)
?N?twt,j,k + ?N?tj,k + V ?
?N?tk,j,d + ?kN?tj,d +?k ?k?N?tj,d + ?N?td + T?.
(6)It it noted that most of the terms in the Reverse-JST posterior is identical to the posterior of JST inEquation 4, except that Nj,d is the number of timestopic j has been assigned to some word tokens indocument d.As we do not have a direct sentiment label-document distribution in Reverse-JST, a distribu-tion over sentiment label for document P (l|d) iscalculated as P (l|d) = ?z P (l|z, d)P (z|d).
Forall the three models, the probability P (l|d) willbe used to determine document sentiment polar-ity.
We define that a document d is classifiedas a positive-sentiment document if its probabil-ity of positive sentiment label given documentP (lpos|d), is greater than its probability of neg-ative sentiment label given document P (lneg|d),and vice versa.4 Experimental Setup4.1 Dataset DescriptionTwo publicly available datasets, the MR and MDSdatasets, were used in our experiments.
The MRdataset (also known as the polarity dataset) hasbecome a benchmark for many studies since thework of Pang et al (2002).
The version 2.0 used inour experiment consists of 1000 positive and 1000negative movie reviews drawn from the IMDBmovie archive, with an average of 30 sentences ineach document.
We also experimented with an-other dataset, namely subjective MR, by removingthe sentences that do not bear opinion informationfrom the MR dataset, following the approach ofPang and Lee (2004).
The resulting dataset stillcontains 2000 documents with a total of 334,336words and 18,013 distinct terms, about half thesize of the original MR dataset without perform-ing subjectivity detection.First used by Blitzer et al (2007), the MDSdataset contains 4 different types of product re-views taken from Amazon.com including books,DVDs, electronics and kitchen appliances, with1000 positive and 1000 negative examples for eachdomain4.4We did not perform subjectivity detection on the MDSdataset since its average document length is much shorterPreprocessing was performed on both of thedatasets.
Firstly, punctuation, numbers, non-alphabet characters and stop words were removed.Secondly, standard stemming was performed inorder to reduce the vocabulary size and address theissue of data sparseness.
Summary statistics of thedatasets before and after preprocessing are shownin Table 1.4.2 Defining Model PriorsIn the experiments, two subjectivity lexicons,namely the MPQA5 and the appraisal lexicon6,were combined and incorporated as prior infor-mation into the model learning.
These two lexi-cons contain lexical words whose polarity orien-tation have been fully specified.
We extracted thewords with strong positive and negative orienta-tion and performed stemming in the preprocess-ing.
In addition, words whose polarity changed af-ter stemming were removed automatically, result-ing in 1584 positive and 2612 negative words, re-spectively.
It is worth noting that the lexicons usedhere are fully domain-independent and do not bearany supervised information specifically to the MR,subjMR and MDS datasets.
Finally, the prior in-formation was produced by retaining all words inthe MPQA and appraisal lexicons that occurred inthe experimental datasets.
The prior informationstatistics for each dataset is listed in the last row ofTable 1.In contrast to Lin and He (2009) that only uti-lized prior information during the initialization ofthe posterior distributions, we use the prior infor-mation in the Gibbs sampling inference step andargue that this is a more appropriate experimentalsetting.
For the Gibbs sampling step of JST andReverse-JST, if the currently observed word tokenmatches a word in the sentiment lexicon, a cor-responding sentiment label will be assigned andonly a new topic will be sampled.
Otherwise, anew sentiment-topic pair will be sampled for thatword token.
For LSM, if the current word tokenmatches a word in the sentiment lexicon, a corre-sponding sentiment label will be assigned and skipthe Gibbs sampling procedure.
Otherwise, a newsentiment label will be sampled.than that of the MR dataset, with some documents even hav-ing one sentence only.5http://www.cs.pitt.edu/mpqa/6http://lingcog.iit.edu/arc/appraisal_lexicon_2007b.tar.gz148Table 1: Dataset and sentiment lexicon statistics.
(Note:?denotes before preprocessing and * denotesafter preprocessing.
)Dataset# of wordsMR subjMR MDSBook DVD Electronic KitchenCorpus size?
1,331,252 812,250 352,020 341,234 221,331 186,122Corpus size* 627,317 334,336 157,441 153,422 95,441 79,654Vocabulary?
38,906 34,559 22,028 21,424 10,669 9,525Vocabulary* 25,166 18,013 14,459 14,806 7,063 6,252# of lexicon 1248/1877 1150/1667 1000/1352 979/1307 574/552 582/504(pos./neg.
)*Table 2: LSM sentiment classification results.Aaccuracy (%)MDSMR subjMR Book DVD Electronic Kitchen MDS overallLSM (without prior info.)
61.7 57.9 51.6 53.5 58.4 56.8 55.1LSM (with prior info.)
74.1 76.1 64.2 66.3 72.5 74.1 69.3Dasgupta and Ng (2009) 70.9 N/A 69.5 70.8 65.8 69.7 68.9Li et al(2009) with 10% doc.
label 60 N/A N/A 62Li et al(2009) with 40% doc.
label 73.5 N/A 735 Experimental Results5.1 LSM Sentiment Classification ResultsIn this section, we discuss the sentiment classifica-tion results of LSM at document level by incorpo-rating prior information extracted from the MPQAand appraisal lexicon.
The symmetry Dirichletprior ?
was set to 0.01, and the asymmetric Dirich-let sentiment prior ?
was set to 0.01 and 0.9 forthe positive and negative sentiment label, respec-tively.
Classification accuracies were averagedover 5 runs for each dataset with 2000 Gibbs sam-pling iterations.As can be observed from Table 2, the perfor-mance of LSM is only mediocre for all the 6datasets when no prior information was incorpo-rated.
A significant improvement, with an aver-age of more than 13%, is observed after incor-porating prior information, especially notable forsubjMR and kitchen with 18.2% and 17.3% im-provement, respectively.
It is also noted that LSMwith subjMR dataset achieved 2% improvementover the original MR dataset, implying that thesubjMR dataset has better representation of sub-jective information than the original dataset by fil-tering out the objective contents.
For the MDSdataset, LSM achieved 72.5% and 74.1% accu-racy on electronic and kitchen domain respec-tively, which is much better than the book andDVD domain with only around 65% accuracy.Manually analysing the MDS dataset reveals thatthe book and DVD reviews often contain a lotof descriptions of book contents or movie plots,which make the reviews from these two domainsdifficult to classify; whereas in the electronic andkitchen domain, comments on the product are of-ten expressed in a straightforward manner.When compared to the recently proposed un-supervised approach based on a spectral cluster-ing algorithm (Dasgupta and Ng, 2009), exceptfor the book and DVD domain, LSM achievedbetter performance in all the other domains withmore than 5% overall improvement.
Neverthe-less, the approach proposed by Dasgupta and Ng(2009) requires users to specify which dimensions(defined by the eigenvectors in spectral cluster-ing) are most closely related to sentiment by in-specting a set of features derived from the re-views for each dimension, and clustering is per-formed again on the data to derive the final re-sults.
In all the Bayesian models studied here, nohuman judgement is required.
Another recentlyproposed non-negative matrix tri-factorization ap-proach (Li et al, 2009) also employed lexical priorknowledge for semi-supervised sentiment classi-fication.
However, when incorporating 10% oflabelled documents for training, the non-negativematrix tri-factorization approach performed muchworse than LSM, with only around 60% accu-racy achieved for all the datasets.
Even with 40%labelled documents, it still performs worse thanLSM on the MR dataset and slightly outperformsLSM on the MDS dataset.
It is worth noting thatno labelled documents were used in the LSM re-sults reported here.149w   w l  N N  dN D  dw d  N S  lN D  lw   wN S  DN lw   DN   wN dw d  DN  N ww   dN l  DN N N N NNw w w w Nw  z w  w  l w  z w  w  lN ddDS*dTw l  w S  w d  Figure 2: JST and Reverse-JST sentiment classification results with multiple topics.5.2 JST and Reverse-JST Results withMultiple TopicsAs both JST and Reverse-JST model documentlevel sentiment and mixture of topic simulta-neously, it is worth to explore how the senti-ment classification and topic extraction tasks af-fect/benifit each other.
With this in mind, weconducted a set of experiments on both JST andReverse-JST, with topic number varying from 30,50 to 100.
The symmetry Dirichlet prior ?
and ?were set to 50/T and 0.01 respectively for bothmodels.
The asymmetry sentiment prior ?
wasempirically set to (0.01, 1.8) for JST and (0.01,0.012) for Reverse-JST, corresponding to positiveand negative sentiment prior, respectively.
Resultswere averaged over 5 runs with 2000 Gibbs sam-pling iterations.As can be seen from Figure 2 that, for both mod-els, the sentiment classification accuracy based onthe subjMR dataset still outperformed the resultsbased on the original MR dataset, where an over-all improvement of 3% is observed for JST andabout 2% for Reverse-JST.
When comparing JSTand Reverse-JST, it can be observed that Reverse-JST performed slightly worse than JST for all setsof experiments with about 1% to 2% drop in ac-curacy.
By closely examining the posterior of JSTand Reverse-JST (c.f.
Equation 4 and 6), we no-ticed that the count Nj,d (number of times topic jassociated with some word tokens in document d)in the Reverse-JST posterior would be relativelysmall due to the factor of large topic number set-ting.
On the contrary, the count Nk,d (number oftimes sentiment label k assigned to some word to-kens in document d) in the JST posterior would berelatively large as k is only defined over 3 differ-ent sentiment labels.
This essentially makes JSTless sensitive to the data sparseness problem andthe perturbation of hyperparameter setting.
In ad-dition, JST encodes an assumption that there is ap-proximately a single sentiment for the entire docu-ment, i.e.
the documents are usually either mostlypositive or mostly negative.
This assumption isimportant as it allows the model to cluster differentterms which share similar sentiment.
In Reverse-JST, this assumption is not enforced unless onlyone topic for each sentiment is defined.
Therefore,JST appears to be a more appropriate model de-sign for joint sentiment topic detection.In addition, it is observed that the sentimentclassification accuracy of both JST and Reverse-JST drops slightly when the topic number in-creases from 30 to 100, with the changes of 2%(MR) and 1.5% (subjMR and MDS overall re-sult) being observed for both models.
This islikely due to the fact that when the topic numberincreases, the probability mass attracted under asentiment-topic pair would become smaller, whichessentially creates data sparseness problem.
Whencomparing with LSM, we notice that the differ-ence in sentiment classification accuracy is onlymarginal by additionally modelling a mixture oftopics.
But both JST and Reverse-JST are able toextract sentiment-oriented topics apart from docu-ment level sentiment detection.150Table 3: Topic examples extracted by JST under different sentiment labels.Book DVD Electronic Kitchenpos.
neg.
pos.
neg.
pos.
neg.
pos.
neg.recip war action murder mous drive color fanfood militari good killer hand fail beauti roomcook armi fight crime logitech data plate coolcookbook soldier right cop comfort complet durabl airbeauti govern scene crime scroll manufactur qualiti loudsimpl thing chase case wheel failur fiestawar noiseat evid hit prison smooth lose blue livefamili led art detect feel backup finger annoiic iraq martial investig accur poorli white blowkitchen polici stunt mysteri track error dinnerwar vornadovarieti destruct chan commit touch storag bright bedroomgood critic brilliant thriller click gb purpl inferiorpictur inspect hero attornei conveni flash scarlet windowtast invas style suspect month disast dark vibratcream court chines shock mice recogn eleg power5.3 Topic ExtractionWe also evaluated the effectiveness of topic sen-timent captured.
In contrast to LDA in which aword is drawn from the topic-word distribution,in JST or Reverse-JST, a word is drawn from thedistribution over words conditioned on both topicand sentiment label.
As an illustration, Table 3shows eight topic examples extracted from theMDS dataset by JST, where each topic was drawnfrom a particular product domain under positive ornegative sentiment label.As can be seen from Table 3, the eight extractedtopics are quite informative and coherent, and eachof the topics represents a certain product reviewfrom the corresponding domain.
For example,the positive book topic probably discusses a goodcookbook; the positive DVD topic is apparentlyabout a popular action movie by Jackie Chan; thenegative electronic topic is likely to be complainsregarding data lose due to the flash drive failure,and the negative kitchen topic is probably the dis-satisfaction of the high noise level of the Vornadobrand fan.
In terms of topic sentiment, by examin-ing through the topics in the table, it is evident thattopics under the positive and negative sentimentlabel indeed bear positive and negative sentimentrespectively.
The above analysis reveals the effec-tiveness of JST in extracting topics and capturingtopic sentiment from text.6 Conclusions and Future WorkIn this paper, we studied three closed relatedBayesian models for unsupervised sentiment de-tection, namely LSM, JST and Reverse-JST.
Asopposing to most of the existing approaches tosentiment classification which favour in super-vised learning, these three models detect senti-ment in a fully unsupervised manner.
While all thethree models gives either better or comparable per-formance compared to the existing approaches onunsupervised sentiment classification on the MRand MDS datasets, JST and Reverse-JST can alsomodel a mixture of topics and the sentiment as-sociated with each topic.
Moreover, extensive ex-periments conducted on the datasets from differ-ent domains reveal that JST always outperformedReverse-JST, suggesting JST being a more appro-priate model design for joint sentiment topic de-tection.There are several directions we plan to inves-tigate in the future.
One is incremental learn-ing of the JST parameters when facing with newdata.
Another one is semi-supervised learningof the JST model with some supervised informa-tion being incorporating into the model parameterestimation procedure such as some known topicknowledge for certain product reviews or the doc-ument labels derived automatically from the user-supplied review ratings.ReferencesAhmed Abbasi, Hsinchun Chen, and Arab Salem.2008.
Sentiment analysis in multiple languages:Feature selection for opinion classification in webforums.
ACM Trans.
Inf.
Syst., 26(3):1?34.Alina Andreevskaia and Sabine Bergler.
2008.
Whenspecialists and generalists work together: Overcom-ing domain dependence in sentiment tagging.
InProceedings of (ACL-HLT), pages 290?298.A.
Aue and M. Gamon.
2005.
Customizing sentiment151classifiers to new domains: a case study.
In Pro-ceedings of Recent Advances in Natural LanguageProcessing (RANLP).John Blitzer, Mark Dredze, and Fernando Pereira.2007.
Biographies, bollywood, boom-boxes andblenders: Domain adaptation for sentiment classi-fication.
In Proceedings of the Association for Com-putational Linguistics (ACL), pages 440?447.S.
Dasgupta and V. Ng.
2009.
Topic-wise, Sentiment-wise, or Otherwise?
Identifying the Hidden Dimen-sion for Unsupervised Text Classification.
In Pro-ceedings of the 2009 Conference on Empirical Meth-ods in Natural Language Processing, pages 580?589.Thomas Hofmann.
1999.
Probabilistic latent semanticindexing.
In Proceedings of the ACM Special Inter-est Group on Information Retrieval (SIGIR), pages50?57.Nobuhiro Kaji and Masaru Kitsuregawa.
2006.
Au-tomatic construction of polarity-tagged corpus fromhtml documents.
In Proceedings of the COL-ING/ACL on Main conference poster sessions, pages452?459.A.
Kennedy and D. Inkpen.
2006.
Sentiment classi-fication of movie reviews using contextual valenceshifters.
Computational Intelligence, 22(2):110?125.Shoushan Li and Chengqing Zong.
2008.
Multi-domain sentiment classification.
In Proceedings ofthe Association for Computational Linguistics andthe Human Language Technology Conference (ACL-HLT), Short Papers, pages 257?260.Tao Li, Yi Zhang, and Vikas Sindhwani.
2009.
A non-negative matrix tri-factorization approach to senti-ment classification with lexical prior knowledge.
InProceedings of (ACL-IJCNLP), pages 244?252.Chenghua Lin and Yulan He.
2009.
Joint senti-ment/topic model for sentiment analysis.
In Pro-ceedings of the ACM international conference on In-formation and knowledge management (CIKM).Ryan McDonald, Kerry Hannan, Tyler Neylon, MikeWells, and Jeff Reynar.
2007.
Structured models forfine-to-coarse sentiment analysis.
In Proceedings ofthe Annual Meeting of the Association of Computa-tional Linguistics (ACL), pages 432?439.Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su,and ChengXiang Zhai.
2007.
Topic sentiment mix-ture: modeling facets and opinions in weblogs.
InProceedings of the conference on World Wide Web(WWW), pages 171?180.Bo Pang and Lillian Lee.
2004.
A sentimental ed-ucation: sentiment analysis using subjectivity sum-marization based on minimum cuts.
In Proceedingsof the Annual Meeting on Association for Computa-tional Linguistics (ACL), page 271.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Found.
Trends Inf.
Retr., 2(1-2):1?135.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
: sentiment classification usingmachine learning techniques.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing (EMNLP), pages 79?86.Ivan Titov and Ryan McDonald.
2008a.
A joint modelof text and aspect ratings for sentiment summariza-tion.
In Proceedings of the Aunal Meeting on Asso-ciation for Computational Linguistics and the Hu-man Language Technology Conference (ACL-HLT),pages 308?316.Ivan Titov and Ryan McDonald.
2008b.
Modeling on-line reviews with multi-grain topic models.
In Pro-ceeding of the International Conference on WorldWide Web (WWW 08?
), pages 111?120.Casey Whitelaw, Navendu Garg, and Shlomo Arga-mon.
2005.
Using appraisal groups for sentimentanalysis.
In Proceedings of the ACM internationalconference on Information and Knowledge Manage-ment (CIKM), pages 625?631.152
