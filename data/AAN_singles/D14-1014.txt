Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 131?141,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsSubmodularity for Data Selection in Statistical Machine TranslationKatrin KirchhoffDepartment of Electrical EngineeringUniversity of WashingtonSeattle, WA, USAkk2@u.washington.eduJeff BilmesDepartment of Electrical EngineeringUniversity of WashingtonSeattle, WA, USAbilmes@u.washington.eduAbstractWe introduce submodular optimizationto the problem of training data subsetselection for statistical machine translation(SMT).
By explicitly formulating dataselection as a submodular program, we ob-tain fast scalable selection algorithms withmathematical performance guarantees, re-sulting in a unified framework that clarifiesexisting approaches and also makes bothnew and many previous approaches easilyaccessible.
We present a new class ofsubmodular functions designed specificallyfor SMT and evaluate them on two differ-ent translation tasks.
Our results show thatour best submodular method significantlyoutperforms several baseline methods,including the widely-used cross-entropybased data selection method.
In addition,our approach easily scales to large data setsand is applicable to other data selectionproblems in natural language processing.1 IntroductionSMT has made significant progress over the lastdecade, not least due to the availability of increas-ingly larger data sets.
Large-scale SMT systemsare now routinely trained on millions of sentencesof parallel data, and billions of words of mono-lingual data for language modeling.
Large datasets are often beneficial, but they do create certainother problems.
First, they place higher demandson computational resources (storage and compute).Hence, existing software infrastructure may needto be adapted and optimized to handle such largedata sets.
Second, experimental turn-around timeis increased as well, making it more difficult toquickly train, fine-tune, and evaluate novel model-ing approaches.
Most importantly, however, SMTperformance does not increase linearly with thetraining data size but levels off after a certain point.This is because the additional training data may benoisy, irrelevant to the task at hand, or inherentlyredundant.
Thus, a linear increase in the amount oftraining data typically leads to a sublinear increasein performance, an effect known as diminishingreturns.
Several recent papers (Bloodgood andCallison-Burch, 2010; Turchi et al., 2012a; Turchiet al., 2012b) have amply demonstrated this effect.A way to counteract this is to perform data sub-set selection, i.e., choose a subset of the availabletraining data to optimize a particular quality cri-terion.
One scheme is to select a subset that ex-presses as much of the information in the originaldata set as possible - i.e., the data set should be?summarized?
by excluding redundant information.Another scheme, popular in the context of SMT, isto subselect the original training set to match theproperties of a particular test set.In this paper, we introduce submodularity forsubselecting SMT training data, a methodologythat follows both of the above schemes.1Sub-modular functions (Fujishige, 2005) are a classof discrete set functions having the property of di-minishing returns.
They occur naturally in a widerange of problems in a diverse set of fields includ-ing economics, game theory, operations research,circuit theory, and more recently machine learn-ing.
Submodular functions share certain propertieswith convexity (e.g., naturalness and mathematicaltractability) although submodularity is still quitedistinct from convexity.We present a novel class of submodular func-tions particularly suited for SMT subselection andevaluate it against state-of-the-art baseline meth-ods on two different translation tasks, showing thatour method outperforms them significantly in mostcases.
While many approaches to SMT data se-lection have been developed previously (a detailedoverview is provided in Section 3), many of themare heuristic and do not offer performance guaran-tees.
Certain previous approaches, however, have1As far as we know, submodularity has not before beenexplicitly utilized for SMT subset selection.131inadvertently made use of submodular methods.This, in addition to our own positive results, pro-vides strong evidence that submodularity is a natu-ral and practical framework for data subset selec-tion in SMT and related fields.An additional advantage of this framework isthat many submodular programs (e.g., the greedyprocedure reviewed in Section 2) are fast andscalable to large data sets.
By contrast, tryingto solve a submodular problem using, say, aninteger-linear programming (ILP) procedure,would lead to impenetrable scalability problems.Initial value f(X) = 2 colors in urn.Updated value f(X?
{v}) = 3 withadded blue ball.Initial value f(Y) = 3 colors in urn.Updated value f(Y?
{v}) = 3 withadded blue ball.XYv vFigure 1: f (Y ) measures the number of distinct col-ors in the set of balls Y , and hence is submodular.This paper makes several contributions: First, wepresent a brief overview of submodular functions(Section 2) and their potential application to naturallanguage processing (NLP).
Next we review pre-vious approaches to MT data selection (Section 3)and analyze them with respect to their submodularproperties.
We find that some previous approachesare submodular in nature although this connectionwas not heretofore made explicit.
Section 4 detailsour new approach.
We discuss desirable propertiesof an SMT data selection objective and present anew class of submodular functions tailored towardsthis problem.
Section 5 presents the data andsystems used for the experiments, and results arereported in Section 6.
Section 7 then concludes.2 Submodular Functions/OptimizationSubmodular functions (Edmonds, 1970; Fujishige,2005), are widely used in mathematics, economics,circuit theory (Narayanan, 1997), and operationsresearch.
More recently, they have attracted muchinterest in machine learning (e.g., (Narasimhanand Bilmes, 2004; Kolmogorov and Zabih, 2004;Krause et al., 2008; Krause and Guestrin, 2011;Jegelka and Bilmes, 2011; Iyer and Bilmes, 2013)),where they have been applied to a variety of prob-lems.
In natural language and speech processing,they have been applied to document summariza-tion (Lin and Bilmes, 2011; Lin and Bilmes, 2012)and speech data selection (Wei et al., 2013).We are given a finite size-n set of objects V (i.e.,|V |= n).
A valuation function f : 2V?
R+is de-fined that returns a non-negative real value for anysubset X ?V .
The function f is said to be submodu-lar if it satisfies the property of diminishing returns:namely, for all X ?
Y and v /?
Y , we must have:f (X ?{v})?
f (X)?
f (Y ?{v})?
f (Y ).
(1)This means that the incremental value (or gain) ofelement v decreases when the context in which vis considered grows from X to Y ?
X .
We definethe ?gain?
as f (v|X), f (X ?{v})?
f (X).
Hence,f is submodular if f (v|X)?
f (v|Y ).
We note thata function m : 2V?
R+is said to be modularif it satisfies the above with equality, meaningm(v|X) = m(v|Y ) for all X ?
Y ?
V \ {v}.
If mis modular and m( /0) = 0, it can be written asm(X) =?x?Xm(x) and, moreover, is seen simplyas a n-dimensional vector m ?
RV.As an example, suppose we have a set V of ballsand f (X) counts the number of colors presentin any subset X ?
V .
In Figure 1, |X | = 5 andf (X) = 2, |Y | = 7 and f (Y ) = 3, and X ?
Y .Adding v (a blue ball) to X has a unity gainf (v|X) = 1 but since a blue ball exists in Y , wehave f (v|Y ) = 0 < f (v|X) = 1.Submodularity is a natural model for data subsetselection in SMT.
In this case, each v ?
V is adistinct training data sentence and V correspondsto a training set.
An important characteristic ofany good model for this problem is that we wishto decrease the ?value?
of a sentence v ?V basedon how much that sentence has in common withthose sentences, say X , that have already beenchosen.
The value f (v|X) of a given sentencev in a context of previously chosen sentencesX ?
V further diminishes as the context growsY ?
X .
When, for example, a sentence?s value isrepresented as the value of its set of features (e.g.,n-grams), it is natural for those features?
values tobe discounted based on how much representationof those features already exists in a previouslychosen subset.
This corresponds to submodularity,which can easily be expressed mathematically byfunctions such as Eqn.
(4) below.Not only are submodular functions natural forSMT subset selection, they can also be optimizedefficiently and scalably such that the result hasmathematical performance guarantees.
In the re-mainder of this paper we will assume that f is notonly submodular, but also non-negative ( f (X)?
0for all X), and monotone non-decreasing ( f (X)?f (Y ) for all X ?
Y ).
Such functions are trivial touselessly maximize, since f (V ) is the largest possi-ble valuation.
Typically, however, we wish to have132Algorithm 1: The Greedy Algorithm1 Input: Submodular function f : 2V?
R+,cost vector m, budget b, finite set V .2 Output: Xkwhere k is the number ofiterations.3 Set X0?
/0 ; i?
0 ;4 while m(Xi)< b do5 Choose vias follows:vi?
{argmaxv?V\Xif ({v}|Xi)m(v)};6 Xi+1?
Xi?
{vi} ; i?
i+1 ;a valuable subset of bounded and small cost, wherecost is measured based on a modular function m(X).For example, the cost m(v) of a sentence v ?
Vmight be its length, so m(X) =?x?Xm(x) is a sumof sentence lengths.
This leads to the followingoptimization problem:X??
argmaxX?V,m(X)?bf (X), (2)where b is a known budget.
Solving this problemexactly is NP-complete (Feige, 1998), and express-ing it as an ILP procedure renders it impractical forlarge data sizes.
When f is submodular the cost isjust size (m(X) = |X |), then the simple greedy algo-rithm (detailed below) will have a worst-case guar-antee of f (?X?)
?
(1?
1/e) f (Xopt) ?
0.63 f (Xopt)where Xoptis the optimal and?X?is the greedy so-lution (Nemhauser et al., 1978).This constant factor guarantee has practical im-portance.
First, a constant factor guarantee staysthe same as n grows, so the relative worst-case qual-ity of the solution is the same for small and for bigproblem instances.
Second, the worst-case resultis achieved only by very contrived and unrealisticfunction instances ?
the typical case is almost al-ways much better.
Third, the worst-case guaranteeimproves depending on the ?curvature?
?
?
[0,1]of the submodular function (Conforti and Cornue-jols, 1984).
When the submodular function is notfully curved (?
< 1, something true of the func-tions used in this paper), the worst case guaranteeis better, namely1?(1?e??)
(e.g., a function f with?
= 0.2 has a worst-case guarantee of 0.91).
Lastly,when the cost m is not just cardinality but an arbi-trary non-negative modular function, a greedy al-gorithm has similar guarantees (Sviridenko, 2004),and a scalable variant has a worst-case guaranteeof 1?1/?e (Lin and Bilmes, 2010).The basic greedy algorithm has a very simpleform.
Starting with X ?
/0, we repeat the operationX ?
X ?
argmaxv?V\Xf (v|X)m(v)until the budget isexceeded (m(X) > b) and then backoff to theprevious iteration (complete details are given inAlgorithm 1).
While the algorithm has complexityO(n2), there is an accelerated instance of thisalgorithm (Minoux, 1978; Leskovec et al., 2007)that has empirical computational complexity ofO(n logn) where n = |V |.
The greedy algorithm,therefore, scales practically to very large n.Recently, still much faster (Wei et al., 2014) andalso parallel distributed (Mirzasoleiman et al.,2013) greedy procedures have been advancedoffering still better scalability.There are many submodular functions thatare appropriate for subset selection (Lin andBilmes, 2011; Lin and Bilmes, 2012).
Someof them are graph-based, where we are given anon-negative weighted graph G = (V,E,w) andw : E?R+is a set of edge weights (i.e., w(x,y) isa non-negative similarity score between sentencesx and y).
A submodular function is obtained viaa graph cut function f (X) =?x?X ,y?V\Xw(x,y)or via a monotone truncated graph cutfunction f (X) =?v?Vmin(Cv(X),?Cv(V ))where ?
?
(0,1) is a scalar parameter andCv(X) =?x?Xw(v,x) is a v-specific modularfunction.
Alternatively, the class of facility loca-tion functions f (X) =?v?Vmaxx?Xw(x,v) havebeen widely and successfully used in the field ofoperations research, and are also applicable here.In the worse case, the required graph construc-tion has a worst-case complexity of O(n2).
Whilesparse graphs can be used, this can be prohibitivewhen n = |V | gets large.
Another class of sub-modular functions that does not have this prob-lem is based on a weighted bipartite graph G =(V,U,E,w) where V are the left vertices, U are theright vertices, E ?
V ?U is a set of edges, andw : U?R+is a set of non-negative weights on thevertices U .
For X ?V , the bipartite neighborhoodfunction is defined as:f (X) = w({u ?U : ?x ?
X with (x,u) ?
E}) (3)This function is interesting for NLP applicationssince U can be seen as a set of ?features?
of the ele-ments v?V (i.e., if V is a set of sentences, U can bethe collective set of n-grams for multiple values ofn, and f (X) is the weight of the n-grams containedcollectively in sentences X).2Given a set X ?
V ,2To be consistent with standard notation in previous liter-ature, we overload the use of n in ?n-grams?
and the size ofour set ?n = |V |?, even though the two ns have no relationshipwith each other.133we get value from the features of the elementsx ?
X , but we get credit for each feature only onetime ?
once a given object x ?
X has a given fea-ture u?U , any additions to X by elements also hav-ing feature u offer no further credit via that feature.Another interesting class of submodular func-tions, allowing additional credit from an elementeven when its features already exist in X , are whatwe call feature-based submodular functions.
Theyinvolve sums of non-decreasing concave functionsapplied to modular functions (Stobbe and Krause,2010) and take the following form:f (X) =?u?Uwu?u(mu(X)) (4)where wu> 0 is a feature weight, mu(X) =?x?Xmu(x) is a non-negative modular functionspecific to feature u, mu(x) is a relevance score (anon-negative scalar score indicating the relevanceof feature u in object x), and ?uis a u-specificnon-negative non-decreasing concave function.The gain is f (v|X) =?u?U(?
(mu(X ?
{v}))??
(mu(X))), and thanks to ?u?s concavity, theterm ?
(mu(X ?{v}))??
(mu(X)) for each featureu ?U is decaying as X grows.
The rate of decay,and hence the degree of diminishing returns andultimately the measure of redundancy of theinformation provided by the feature, is controlledby the concave function.
The rate of decay isalso related to the curvature ?
of the submodularfunction (c.f.
?2), with more aggressive decayhaving higher curvature (and a worse worst-caseguarantee).
The decay is a modeling choice thatshould be decided based on a given application.Feature-based functions have the advantage thatthey do not require the construction of a pairwisegraph; they have a cost of only O(n|U |), which islinear in the data size and therefore scalable tolarge data set sizes.We utilize this class for our subset selection ex-periments described in Section 4, where we use oneglobal concave function ?u= ?
for all u ?U .
Inthis work we chose one particular set of features U .However, given the large body of research into NLPfeature engineering (Jurafsky and Martin, 2009),this class is extensible beyond just this set, whichmakes it suitable for many other NLP applications.Before describing our SMT-specific functions indetail, we review previous work on subset selectionfor SMT in the context of submodularity.3 Previous ApproachesThere have been many previous approaches to datasubset selection in SMT.
In this section, we showthat some of them in fact correspond to submodularmethods, thus introducing a connection betweensubmodularity and the practical problem of SMTdata selection.
The fact that submodularity isimplicitly and unintentionally used in previouswork suggests that it is natural for this problem.A currently widely-used data selection method inSMT (which we also use as a baseline in Section 6)uses the cross-entropy between two language mod-els (Moore and Lewis, 2010), one trained on thetest set of interest, and another trained on a large setof generic or out-of-domain training data.
We callthis the cross-entropy method.
This method trainsa test-set specific (or in-domain) language model,LMin, and a generic (out-of- or mixed-domain) lan-guage model, LMout.
Each sentence x ?
V in thetraining data is given a probability score with bothlanguage models and then ranked in descendingorder based on the log ratiomce(x) =1`(x)log[Pr(x|LMin)/Pr(x|LMout)] (5)where `(x) is the length of sentence x.
Finally, thetop N sentences are chosen.
In (Axelrod et al.,2011) this method is extended to take both sidesof the parallel corpus into account rather than justthe source side.
The cross-entropy approach valueseach sentence individually, without regard to any in-teraction with already selected sentences.
This ap-proach, therefore, is modular (a special case of sub-modular) and values a set X via m(X) =?x?Xm(x).Moreover, the thresholding method for choosinga subset corresponds exactly to the optimizationproblem in Eqn.
(2) where f ?
m and the budgetb is set to the sum of the top N sentence scores.Thanks to modularity, the problem is no longer NP-complete, and the threshold method solves Eqn.
(2)exactly.
On the other hand, a modular functiondoes not have the diminishing returns property, andthus has no chance to represent interaction or re-dundancy between sentences.
The chosen subset,therefore, might have an enormous overrepresenta-tion of one aspect of the training data while havingminimal or no representation of another aspect, amajor vulnerability of this approach.Other methods use information retrieval (Hilde-brand et al., 2005; L?u et al., 2007) which can alsobe described as modular function optimization(e.g., take the top k scoring sentences).
Duplicate134sentence removal is easily represented by a feature-based submodular function, Equation (4), wherethere is one sentence-specific feature per sentenceand where ?u(mu(X)) = min(|X ?
{u}|,1) ?
oncea sentence is chosen, its contribution is saturatedso any duplicate sentence has a gain of zero.
Also,the unseen n-gram function of (Eck et al., 2005;Bloodgood and Callison-Burch, 2010) correspondsto a bipartite neighborhood submodular function,with a weight function defined based on n-gramcounts.
Moreover their functions are optimizedusing the greedy algorithm; hence they in facthave a 1?
1/e guarantee.
Other methods havenoted and dealt with the existence of redundancyin phrase-based systems (Ittycheriah and Roukos,2007) by limiting the set of phrases ?
submodularoptimization inherently removes redundancy.
Also,(Callison-Burch et al., 2005; Lopez, 2007) involvemodular functions but where selection is oversubsets of phrases (rather than sentences as in ourcurrent work) and where multiple selections occur,each specific to an individual test set sentencerather than the entire test set.In the feature-decay method, presented in (Bic?ici,2011; Bic?ici and Yuret, 2011; Bic?ici, 2013), thevalue of a sentence is based on its decompositioninto a set of feature values.
As sentences are addedto a set, the feature decay approach in general di-minishes the value of each feature depending onhow much of that feature has already been coveredby those sentences previously chosen ?
the pa-pers define a set of feature decay functions for thispurpose.Our analysis of (Bic?ici, 2011; Bic?ici and Yuret,2011; Bic?ici, 2013), from the perspective of sub-modularity, has revealed an interesting connection.The feature decay functions used in these papersturn out to be derivatives of non-decreasing con-cave functions.
For example, in one case ??
(a) =1/(1+ a) which is the derivative of the concavefunction ?
(a) = ln(1+a).
We are given a constantinitialization wufor feature u ?U ?
in the papers,they set either wu?
1, or wu?
log(m(V )/mu(V )),or wu?
log(m(V )/(1+mu(V ))), where m(V ) =?umu(V ), and where mu(X) =?x?Xmu(x) is thecount of feature u within the set of sentencesX ?V .
This yields the submodular feature functionfu(X) = wu?(mu(X)).
The value of sentence v asmeasured by feature u in the context of X is the gainfu(v|X), which is a discrete derivative correspond-ing to wu/(1+mu(X ?{v})).
An alternative decayfunction they define is given as ??
(a) = 1/(1+ba)for a base b (they set b?
2) which is the derivativeof the following non-decreasing concave function:?
(a) =[1?1ln(b)ln(1+ exp(?a ln(b)))](6)We note that this function is saturating, meaningthat it quickly reaches its asymptote at its maxi-mum possible value.
We can, once again, definea function specific for feature u ?U as fu(X) =wu?
(mu(X)) with a gain fu(v|X) being a discretederivative corresponding to wu/(1+bmu(X?
{v})).The connection between this work and submod-ularity is not complete, however, without consider-ing the method used for optimization.
In fact, Algo-rithm 1 of (Bic?ici and Yuret, 2011) is precisely theaccelerated greedy algorithm of (Minoux, 1978)applied to the submodular function correspondingto f (X) =?u?Ufu(X), and Algorithm 1 of (Bic?ici,2013) is the cost-normalized variant of this greedyalgorithm corresponding to a knapsack constraint(Sviridenko, 2004).
Thus, our analysis shows thatthese methods also have a 1?
1/e performanceguarantee and also the O(n logn) empirical com-plexity mentioned in Section 2.
This is an impor-tant connection, as it furthers the evidence thatsubmodularity is natural for the problem of SMTsubset selection.
This also increases the accessibil-ity of this method since we may view it as a specialcase of Equation (4).Another class of approaches focuses on activelearning.
In (Haffari et al., 2009) a large corpusof noisy parallel data is created automatically; asmaller set of samples is then selected from thisset that receive human translations.
A combinationof several ?informativeness?
scores is computedon a sentence-level basis, and samples are selectedvia hierarchical adaptive sampling (Dasgupta andHsu, 2008).
In (Mandal et al., 2008) a measureof disagreement between different MT systems, aswell as an entropy-based criterion are used to selectadditional data for annotation.
In (Bloodgood andCallison-Burch, 2010) and (Ambati et al., 2010),active learning is combined with crowd-sourced an-notations to produce large, human-translated datasets that are as informative as possible.
In (Caoand Khudanpur, 2012), samples are selected fordiscriminative training of an MT system accord-ing to a greedy algorithm that tries to maximizeoverall quality.
These methods address a differ-ent scenario (data selection for annotation or dis-criminative training) than the one considered here;however, we also note that the actual selection tech-niques employed in these papers do not appear tobe submodular.1354 Novel Submodular Functions for SMTIn this section, we design a parameterized classof submodular functions useful for SMT trainingdata subset selection.
By staying within the realmof submodularity, we retain the advantages of thegreedy algorithm, its theoretical performance as-surances, and its scalability properties.
At the sametime this opens the door to a general framework forquickly exploring a much larger class of functions(with the same desirable properties) than before.It is important to note that we are using sub-modularity as a ?model?
of the selection process,and the submodular objective acts as a surrogatefor the actual SMT objective function.
Thus, themathematical guarantee we have is in terms of thesurrogate objective rather than the true SMT ob-jective.
Evaluating one point of the actual SMTobjective would require the complete training andtesting of an SMT system, so even an algorithm asefficient as Algorithm 1 would be infeasible, evenon small data.
It is therefore important to design anatural and scalable surrogate objective.We do not consider the graph-based functionsdiscussed in Section 2 here since they require apairwise similarity matrix over all training sen-tences and thus have O(n2) worst-case complexity.For large tasks with millions or even billions ofsentences, this eventually becomes impractical.Instead we focus on feature-based functions of thetype presented in Eqn.
(4), where each sentenceis represented as a set of features rather than as avertex in a graph.
In this function there are fourcomponents to specify: 1) U , the linguistic featureset; 2) mu(x), the relevance scores for each featureu and sentence x; 3) wu, the feature weights; and4) ?
, the concave function (we use one concavefunction, so ?u= ?
for all u ?U).Feature set: U is the set of n-grams from eitherthe source language Usrc, or from both the sourceand target language Usrc?Utgt(see Section 6);since we are interested in selecting a training setthat matches a given test set, we use the set of n-grams that occur both in the training set and inthe development/test data (for target features, onlydevelopment set features are used).
I.e., Usrc=(Usrcdev?Usrctest)?Usrctrainand Utgt=Utgtdev?Utgttrain.Relevance scores: A feature u within a sentencex should be valued based on how salient that fea-ture is within the ?document?
in which it occurs;here, the ?document?
is the set of training sen-tences.
This is a task well suited to TFIDF.
Asan alternative to raw feature counts we thus alsoconsider scores of the form mu(x)?
tfidf(u,x) =tf(u,x)?
idftrn(u), where tf(u,x) and idftrn(u) aredefined as usual.Feature weights: We wish to select those trainingsamples that contain features occurring frequentlyin the test data while avoiding the over-selectionof features that are very frequent in the trainingdata because those are likely to be translatedcorrectly anyway.
This is similar to the approachin (Moore and Lewis, 2010) (see Equation (5)),where a log-probability ratio of in-domain toout-of-domain language model is utilized.
In thepresent case, we need a value that is specific tofeature u ?U ; a natural approach is to use the ratioof counts ctst(u)/ctrn(u) where ctst(u) is the rawcount of u in the development/test data, and ctrn(u)is its raw count in the training data (note thatctrn(u) is never zero due to the way U is defined).As an additional factor we allow feature lengthto have an influence.
In general, longer n-gramsmight be considered more valuable since theytypically lead to better translations and are morerelevant for BLEU.
Thus, we include a rewardterm for longer n-grams in the form of ?|u|where?
?
1 and |u| is the length of feature u.
This givesgreater weight to longer n-grams when ?
> 1.Concave function: It is imperative to find the rightform of concave function since, as described in Sec-tion 2, the concave shape determines the degree towhich redundancy and diminishing returns are rep-resented.
Intuitively, when the shape of the concavefunction for a feature becomes ?flat?
rapidly, thatfeature quickly looses its ability to provide addi-tional value to a candidate subset.
Many differentconcave functions were tested for ?
, including oneof the two functions implicit in (Bic?ici and Yuret,2011) and derived in Section 3, and a variety ofroots of the form ?
(a) = a?for 0 < ?
< 1.
InTable 2, for example, we find evidence that thesimple square root ?
(a) =?a performs slightlybetter than the log function.
The square root ismuch less curved and decays much more graduallythan either of the two functions implicit in (Bic?iciand Yuret, 2011), of which one is a log form andthe other is even more curved and quickly satu-rates (see ?3).
The square root function yields aless curved submodular function, in the sense of(Conforti and Cornuejols, 1984), resulting in betterworst-case guarantees.
Indeed, Table 1 in (Bic?iciand Yuret, 2011) corroborates by showing that themore curved saturating function does worse thanthe less curved log function.Four Components Together: Different instantia-136tions of the four components discussed above willresult in different submodular functions of the gen-eral class defined in Eqn.
(4).
Particular settingsof these general parameters produce the methodsconsidered in (Bic?ici and Yuret, 2011), thus mak-ing that approach easily accessible once the generalsubmodular framework is set up.
As a very specialcase, this is also true of the cross-entropy method(Moore and Lewis, 2010), where |U |= 1, mu(x)?exp(mce(x)) of Equation (5)3, wu?
1, and ?
(a) =a is the identity function.
In Section 6, we specifythe parameter settings used in our experiments.Task Train Dev Test LMNIST 189M 48k 49k 2.5BEuroparl 52.8M 57.7k 58.1k 53MTable 1: Data set sizes (number of source-sidewords) for MT tasks.
LM = language model data.5 Data and SystemsWe evaluate our approach on the NIST Arabic-English translation task, using the NIST 2006 setfor development and the NIST 2009 set for eval-uation.
The training data consists of all ModernStandard Arabic-English parallel LDC corpora per-mitted in the NIST evaluations (minus the restrictedtime periods).
Together these sets form a mixed-domain training set containing relevant in-domaindata similar to the NIST data sets but also less rele-vant data (e.g., the UN parallel corpora); we thusexpect data selection to work well on this task.
Ad-ditional English language modeling data was drawnfrom several other LDC corpora (English Giga-word, AQUAINT, HARD, ANC/DCI and the Amer-ican National Corpus).
Preprocessing included con-version of the Arabic data to Buckwalter format,tokenization, spelling normalization, and morpho-logical segmentation using MADA (Habash et al.,2009).
Numbers and URLs were replaced withvariables.
The English data was tokenized andlowercased.
Postprocessing involved recasing thetranslation output, replacing variable names withtheir original corresponding tokens, and normal-izing spelling and stray punctuation marks.
Therecasing model is an SMT system without reorder-ing, trained on parallel cased and lowercased ver-sions of the training data.
The recasing model re-mains fixed for all experiments and is not retrained3Due to modularity, any monotone increasing transforma-tion from mce(x) to mu(x) that ensures mu(x)?
0 is equivalent.for different sizes of the training data.
Evalua-tion follows the NIST guidelines and was done bycomputing BLEU scores using the official NISTevaluation tool mteval-v13a.pl with the ?c flagfor case-sensitive scoring.
In addition to the NISTtask we also applied our method to the EuroparlGerman-English translation task.
The training datacomes from the Europarl-v7 collection4; the devel-opment set is the 2006 dev set, and the test set is the2007 test set.
The number of reference translationsis 1.
The German data was preprocessed by tok-enization, lower-casing, splitting noun compoundsand lemmatization to address morphological vari-ation in German.
The English side was tokenizedand lowercased.
Evaluation was done by comput-ing BLEU on the lowercased versions of the data.Since test and training data for this task come fromlargely the same domain we expect the trainingdata to be less redundant or irrelevant; neverthelessit will be interesting to see how much different dataselection methods can contribute even to in-domaintranslation tasks.
The sizes of the various data setsare shown in Table 1.All translation systems were trained using theGIZA++/Moses infrastructure (Koehn et al., 2007).The translation model is a standard phrase-basedmodel with a maximum phrase length of 7.
Since alarge number of experiments had to be run for thisstudy, more complex hierarchical or syntax-basedtranslation models were deliberately excluded inorder to limit the experimental turn-around timeneeded for each experiment.
The reordering modelis a hierarchical model according to (Galley andManning, 2008).
The feature weights in the log-linear function were optimized on the developmentset BLEU score using minimum error-rate training.The language models for the NIST task (5-grams)were trained on three different data sources (Gi-gaword, GALE data, and all remaining corpora),which were then interpolated into a single model.The interpolation weights were optimized sepa-rately for the two different genres present in theNIST task (newswire and web text).
All modelsused Witten-Bell discounting and interpolation ofhigher-order and lower-order models.
Languagemodels remain fixed for all experiments, i.e., thelanguage model training data is not subselectedsince we were interested in the effect of data subsetselection on the translation model only.
The lan-guage model for the Europarl system was a 5-gramtrained on Europarl data only.4http://http://www.statmt.org/europarl/137Method Data Subset Sizes10% 20% 30% 40%Rand 0.3991 (?
0.004) 0.4142 (?
0.003) 0.4205 (?
0.002) 0.4220 (?
0.002)Xent 0.4235 (?
0.004) 0.4292 (?
0.002) 0.4290 (?
0.003) 0.4292 (?
0.001)SM-1 0.4309 (?
0.000) 0.4367 (?
0.001) 0.4330 (?
0.004) 0.4351 (?
0.002)SM-2 0.4330?(?
0.001) 0.4395?(?
0.003) 0.4333 (?
0.001) 0.4366?(?
0.003)SM-3 0.4313?(?
0.002) 0.4338 (?
0.002) 0.4361?(?
0.002) 0.4351 (?
0.003)SM-4 0.4276 (?
0.003) 0.4303 (?
0.002) 0.4324 (?
0.002) 0.4329 (?
0.000)SM-5 0.4285 (?
0.004) 0.4356 (?
0.002) 0.4333 (?
0.003) 0.4324 (?
0.002)SM-6 0.4302?(?
0.004) 0.4334 (?
0.003) 0.4371?(?
0.002) 0.4349 (?
0.003)SM-7 0.4295 (?
0.002) 0.4374 (?
0.002) 0.4344 (?
0.001) 0.4314 (?
0.0004)SM-8 0.4304?(?
0.002) 0.4323 (?
0.000) 0.4358 (?
0.003) 0.4337 (?
0.001)100% 0.4257Table 2: BLEU scores (standard deviations) on the NIST 2009 (Ara-En) test set for random (Rand),cross-entropy (Xent), and submodular (SM) data selection methods defined in Table 4.
100% = systemusing all of the training data.
Boldface numbers indicate a statistically significant improvement (p?
0.05)over the median Xent system.
Starred scores are also significantly better than SM-5.Method Data Subset Sizes10% 20% 30% 40%Rand 0.2590 (?
0.003) 0.2652 (?
0.001) 0.2677 (?
0.002) 0.2697 (?
0.001)Xent 0.2639 (?
0.002) 0.2687 (?
0.002) 0.2704 (?
0.001) 0.2723 (?
0.001)SM-5 0.2653 (?
0.001) 0.2727 (?
0.000) 0.2697 (?
0.002) 0.2720 (?
0.002)SM-6 0.2697?
(?
0.001) 0.2700 (?
0.002) 0.2740?
(?
0.002) 0.2723 (?
0.000)100% 0.2651Table 3: BLEU scores (standard deviation) on the Europarl translation task for random (Rand), cross-entropy (Xent), and submodular (SM) data selection methods.
100% = system using all of the trainingdata.
Boldface numbers indicate a statistically significant improvement (p?
0.05) over the median Xentsystem.
Starred scores are significantly better than SM-5.6 ExperimentsFunction parametersw(u) ?
(a) mu(x) USM-1ctst(u)ctrn(u)?|u|?a tfidf(u,x) UsrcSM-2?ctst(u)ctrn(u)?|u|?a tfidf(u,x) Usrc?UtgtSM-3ctst(u)ctrn(u)?|u|?a c(u,x) UsrcSM-4 ctst(u)?a tfidf(u,x) UsrcSM-5 1 ln(1+a) c(u,x) UsrcSM-6?ctst(u)ctrn(u)?a tfidf(u,x) UsrcSM-7ctst(u)ctrn(u)?
(a) tfidf(u,x) Usrc?UtgtSM-8ctst(u)ctrn(u)ln(1+a) tfidf(u,x) Usrc?UtgtTable 4: Different instantiations of the general sub-modular function in Eq.
4 (?
= 1.5 in all cases).We first trained a baseline system on 100% ofthe training data.
Different data selection methodswere then used to select subsets of 10%, 20%, 30%and 40% of the data.
While not reported in thetables, above 40%, the performance slowly dropsto the 100% performance.The first baseline selection method utilizes ran-dom data selection, for which 3 different data setsof the specified size were drawn randomly fromthe training data.
Individual systems were trainedon all random subsets of the same size, and theirscores were averaged.
The second baseline isthe cross-entropy method by (Moore and Lewis,2010).
In-domain language models were trained onthe combined development and test data, and out-of-domain models were trained on an equivalentamount of data drawn randomly from the trainingset.
Sentences were ranked by the function in Eq.
5,and the top k percent were chosen.
The order of then-gram models was optimized on the developmentset and was found to be 3.
Larger model ordersresulted in worse performance, possibly due to the138limited size of the data used for their training.
Sincethis method also involves random data selection,we report the average BLEU score over 5 differenttrials.
For the submodular selection method, Ta-ble 4 shows the different values that were tested forthe four components listed in Section 4.
The combi-nation was optimized on the development set.
Theselection algorithm (Alg.
1) runs within a few min-utes on our complete training set of 189M words.Results on the NIST 2009 test set are shown inTable 2.
The scores for the submodular systemsare averages over 3 different runs of MERT tuning.Random data subset selection (Row 1) falls shortof the baseline system using 100% of the trainingdata.
The cross-entropy method (Row 2) surpassesthe performance of the baseline system at about20% of the data, demonstrating that data subsetselection is a suitable technique for such mixed-domain translation tasks.
The following rows showresults for the various submodular functions shownin Table 4.
Out of these, SM-5 corresponds to thebest approach in (Bic?ici and Yuret, 2011).
SM-6is our own best-performing function, beating thecross-entropy method by a statistically significantmargin (p?
0.05) under all conditions.5SM-6 isalso significantly better than SM-5 in two cases.Finally, it surpasses the performance of the all-datasystem at only 10% of the training data; possibly,even smaller training data sets could be usedbut this option was not investigated.
While thebilingual submodular functions SM-2 and SM-7)yield an improvement of up to 0.015 BLEU pointson the dev set (not shown in the table), they do notconsistently outperform the monolingual functionson the test set.
Since test set target features cannotbe used in our scenario, bilingual features areonly helpful to the extent that the development setclosely matches the test set.
However, target fea-tures should be quite helpful when selecting datafrom an out-of-domain set to match an in-domaintraining set (as in e.g.
(Axelrod et al., 2011)).
Wefound no gain from the length reward ?|u|.The Europarl results (Table 3) show a similarpattern.
Although the differences in BLEU scoresare smaller overall (as expected on an in-domaintranslation task), data subset selection improvesover the all-data baseline system in this case aswell.
The cross-entropy method again outperformsrandom data selection.
On this task we only testedour submodular function that worked best on the5Statistical significance was measured using the pairedbootstrap resampling test of (Koehn, 2004), applied to thesystems with the median BLEU scores.NIST task; again we find that it outperforms thecross-entropy method.
In two conditions (10% and30%) these differences are statistically significant.10% of the training data suffices to outperform theall-data system, and up to a full BLEU point can begained on this task using 20-30% of the data and asubmodular data selection method.7 ConclusionsWe have introduced submodularity to SMT datasubset selection, generalizing previous approachesto this problem.
Our method has theoretical perfor-mance guarantees, comes with scalable algorithms,and significantly improves over current, widely-used data selection methods on two different trans-lation tasks.
There are many possible extensionsto this work.
One strategy would be to extend thefeature set U with features representing differenttypes of linguistic information - e.g., when usinga syntax-based system it might be advantageousto select training data that covers the set of syn-tactic structures seen in the test data.
Secondly,the selected data was test data specific.
In somecontexts, it is not possible to train test data spe-cific systems dynamically; in that case, differentsubmodular functions could be designed to selecta representative ?summary?
of the training data.Finally, the use of submodular functions for subsetselection is applicable to other data sets that canbe represented as features or as a pairwise similar-ity graph.
Submodularity thus can be applied to awide range of problems in NLP beyond machinetranslation.AcknowledgmentsThis material is based on research sponsored byIntelligence Advanced Research Projects Activity(IARPA) under agreement number FA8650-12-2-7263, and is also supported by the National ScienceFoundation under Grant No.
(IIS-1162606), and bya Google, a Microsoft, and an Intel research award.The U.S. Government is authorized to reproduceand distribute reprints for Governmental purposesnotwithstanding any copyright notation thereon.The views and conclusions contained herein arethose of the authors and should not be interpretedas necessarily representing the official policies orendorsements, either expressed or implied, of In-telligence Advanced Research Projects Activity(IARPA) or the U.S. Government.139References[Ambati et al.2010] V. Ambati, S. Vogel, and J. Car-bonell.
2010.
Active learning and crowd-sourcingfor machine translation.
In Proceedings of LREC,pages 2169?2174, Valletta, Malta.
[Axelrod et al.2011] A. Axelrod, X.
He, and J. Gao.2011.
Domain adaptation via pseudo in-domain dataselection.
In Proceedings of EMNLP, pages 355?362, Edinburgh, Scotland.
[Bic?ici and Yuret2011] E. Bic?ici and D. Yuret.
2011.Instance selection for machine translation using fea-ture decay algorithms.
In Proceedings of the 6thWorkshop on Statistical Machine Translation, pages272?283.
[Bic?ici2013] E. Bic?ici.
2013.
Feature decay algorithmsfor fast deployment of accurate statistical machinetranslation systems.
In Proceedings of the 8th Work-shop on Statistical Machine Translation, pages 78?84.
[Bic?ici2011] E. Bic?ici.
2011.
The Regression Model ofMachine Translation.
Ph.D. thesis, KOC?
University.
[Bloodgood and Callison-Burch2010] M. Bloodgoodand C. Callison-Burch.
2010.
Bucking the trend:large-scale cost-focused active learning for statis-tical machine translation.
In Proceedings of ACL,pages 854?864.
[Callison-Burch et al.2005] Chris Callison-Burch,Colin Bannard, and Josh Schroeder.
2005.
Scalingphrase-based statistical machine translation to largercorpora and longer phrases.
In Proceedings of the43rd Annual Meeting on Association for Computa-tional Linguistics, pages 255?262.
Association forComputational Linguistics.
[Cao and Khudanpur2012] Y. Cao and S. Khudanpur.2012.
Sample selection for large-scale MT discrimi-native training.
In Proceedings of AMTA.
[Conforti and Cornuejols1984] M. Conforti and G. Cor-nuejols.
1984.
Submodular set functions, matroidsand the greedy algorithm: tight worst-case boundsand some generalizations of the Rado-Edmonds the-orem.
Discrete Applied Mathematics, 7(3):251?274.
[Dasgupta and Hsu2008] S. Dasgupta and D. Hsu.2008.
Hierarchical sampling for active learning.
InProceedings of ICML.
[Eck et al.2005] M. Eck, S. Vogel, and A. Waibel.
2005.Low cost portability for statistical machine transla-tion based on n-gram frequency and tf-idf.
In Pro-ceedings of the 10th Machine Translation Summit X,pages 227?234.
[Edmonds1970] J. Edmonds, 1970.
CombinatorialStructures and their Applications, chapter Submodu-lar functions, matroids and certain polyhedra, pages69?87.
Gordon and Breach.
[Feige1998] U. Feige.
1998.
A threshold of ln n for ap-proximating set cover.
Journal of the ACM (JACM),45(4):634?652.
[Fujishige2005] S. Fujishige.
2005.
Submodular func-tions and optimization.
Annals of Discrete Mathe-matics, volume 58.
Elsevier Science.
[Galley and Manning2008] M. Galley and C. D. Man-ning.
2008.
A simple and effective hierarchicalphrase reordering model.
In Proceedings of EMNLP,pages 847?855.
[Habash et al.2009] N. Habash, O. Rambow, andR.
Roth.
2009.
A toolkit for Arabic tokenization,diacritization, morphological disambiguation, POStagging, stemming and lemmatization.
In Proceed-ings of the MEDAR conference, pages 102?109.
[Haffari et al.2009] G. Haffari, M. Roy, and A. Sarkar.2009.
Active learning for statistical machine transla-tion.
In Proceedings of HLT, pages 415?423.
[Hildebrand et al.2005] A. Hildebrand, M. Eck, S. Vo-gel, and A. Waibel.
2005.
Adaptation of the transla-tion model for statistical machine translation basedon information retrieval.
In Proceedings of EAMT,pages 133?142.
[Ittycheriah and Roukos2007] A. Ittycheriah andS.
Roukos.
2007.
Direct translation model 2.
InProceedings of HLT/NAACL, page 5764.
[Iyer and Bilmes2013] R. Iyer and J. Bilmes.
2013.Submodular optimization with submodular coverand submodular knapsack constraints.
In Neural In-formation Processing Society (NIPS), Lake Tahoe,CA, December.
[Jegelka and Bilmes2011] Stefanie Jegelka and Jeff A.Bilmes.
2011.
Submodularity beyond submodularenergies: coupling edges in graph cuts.
In ComputerVision and Pattern Recognition (CVPR), ColoradoSprings, CO, June.
[Jurafsky and Martin2009] D. Jurafsky and J. H. Mar-tin.
2009.
Speech and Language Processing:An Introduction to Natural Language Processing,Speech Recognition, and Computational Linguistics.Prentice-Hall, 2nd edition.
[Koehn et al.2007] P. Koehn, H. Hoang, A. Birch,C.
Callison-Burch, M. Federico, N. Bertoldi,B.
Cowan, W. Shen, C. Moran, R. Zens, C. Dyer,O.
Bojar, A. Constantin, and E. Herbst.
2007.Moses: Open source toolkit for statistical machinetranslation.
In Proceedings of ACL.
[Koehn2004] P. Koehn.
2004.
Statistical significancetests for machine translation evaluation.
In Proceed-ings of EMNLP.
[Kolmogorov and Zabih2004] V. Kolmogorov andR.
Zabih.
2004.
What energy functions canbe minimized via graph cuts?
IEEE TPAMI,26(2):147?159.140[Krause and Guestrin2011] A. Krause and C. Guestrin.2011.
Submodularity and its applications in opti-mized information gathering.
ACM Transactions onIntelligent Systems and Technology, 2(4).
[Krause et al.2008] A. Krause, H.B.
McMahan,C.
Guestrin, and A. Gupta.
2008.
Robust sub-modular observation selection.
Journal of MachineLearning Research, 9:2761?2801.
[Leskovec et al.2007] J. Leskovec, A. Krause,C.
Guestrin, C. Faloutsos, J. VanBriesen, andN.
Glance.
2007.
Cost-effective outbreak detectionin networks.
In Proceedings of the 13th ACMSIGKDD International Conference on KnowledgeDiscovery and Data Mining, pages 420?429.
[Lin and Bilmes2010] H. Lin and J. Bilmes.
2010.Multi-document summarization via budgeted maxi-mization of submodular functions.
In Proceedingsof NAACL-HLT, pages 2761?2801.
[Lin and Bilmes2011] H. Lin and J. Bilmes.
2011.
Aclass of submodular functions for document summa-rization.
In Proceedings of ACL, pages 510?520.
[Lin and Bilmes2012] H. Lin and J. Bilmes.
2012.Learning mixtures of submodular shells with appli-cation to document summarization.
In Uncertaintyin Artifical Intelligence (UAI), Catalina Island, USA,July.
AUAI.
[Lopez2007] A. Lopez.
2007.
Hierarchical phrase-based translation with suffix arrays.
In EMNLP-CoNLL, pages 976?985.
[L?u et al.2007] Y. L?u, J. Huang, and Q. Liu.
2007.
Im-proving statistical machine translation performanceby training data selection and optimization.
In Pro-ceedings of EMNLP, pages 343?350.
[Mandal et al.2008] A. Mandal, D. Vergyri, W. Wang,J.
Zheng, A. Stolcke, D. Hakkani-T?ur G. T?ur, andN.F.
Ayan.
2008.
Efficient data selection for ma-chine translation.
In Proceedings of the Spoken Lan-guage Technology Workshop, pages 261?264.
[Minoux1978] M. Minoux.
1978.
Accelerated greedyalgorithms for maximizing submodular functions.In Lecture Notes in Control and Information Sci-ences, volume 7, pages 234?243.
[Mirzasoleiman et al.2013] B. Mirzasoleiman, A. Kar-basi, R. Sarkar, and A. Krause.
2013.
Distributedsubmodular maximization: Identifying representa-tive elements in massive data.
In Neural InformationProcessing Systems (NIPS).
[Moore and Lewis2010] R. Moore and W. Lewis.
2010.Intelligent selection of language model training data.In Proceedings of the Association for ComputationalLinguistics, pages 220?224.
[Narasimhan and Bilmes2004] M. Narasimhan andJ.
Bilmes.
2004.
PAC-learning bounded tree-widthgraphical models.
In Uncertainty in Artificial Intel-ligence: Proceedings of the Twentieth Conference(UAI-2004).
Morgan Kaufmann Publishers, July.
[Narayanan1997] H. Narayanan.
1997.
Submodularfunctions and electrical networks.
Elsevier.
[Nemhauser et al.1978] G.L.
Nemhauser, L.A. Wolsey,and M.L.
Fisher.
1978.
An analysis of approxi-mations for maximizing submodular set functions i.Mathematical Programming, 14:265?294.
[Stobbe and Krause2010] P. Stobbe and A. Krause.2010.
Efficient minimization of decomposable sub-modular functions.
In NIPS.
[Sviridenko2004] M. Sviridenko.
2004.
A note onmaximizing a submodular set function subject to aknapsack constraint.
Operations Research Letters,32(1):41?43.
[Turchi et al.2012a] M. Turchi, T. de Bie, C. Goutte,and N. Cristianini.
2012a.
Learning to translate: Astatistical and computational analysis.
Advances inArtificial Intellligence, 2012:484580:15 pages.
[Turchi et al.2012b] M. Turchi, C. Goutte, and N. Cris-tianini.
2012b.
Learning machine translation fromin-domain and out-of-domain data.
In Proceedingsof EAMT, Trento, Italy.
[Wei et al.2013] K. Wei, Y. Liu, K. Kirchhoff, andJ.
Bilmes.
2013.
Using document summarizationtechniques for speech data subset selection.
In Pro-ceedings of NAACL, pages 721?726, Atlanta, Geor-gia, June.
[Wei et al.2014] K. Wei, R. Iyer, and Jeff Bilmes.
2014.Fast multi-stage submodular maximization.
In Pro-ceedings of ICML, Beijing, China.141
