Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 125?132,Vancouver, October 2005. c?2005 Association for Computational LinguisticsA Classifier-Based Parser with Linear Run-Time ComplexityKenji Sagae and Alon LavieLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213{sagae,alavie}@cs.cmu.eduAbstractWe present a classifier-based parser thatproduces constituent trees in linear time.The parser uses a basic bottom-up shift-reduce algorithm, but employs a classifierto determine parser actions instead of agrammar.
This can be seen as an exten-sion of the deterministic dependencyparser of Nivre and Scholz (2004) to fullconstituent parsing.
We show that, withan appropriate feature set used in classifi-cation, a very simple one-path greedyparser can perform at the same level ofaccuracy as more complex parsers.
Weevaluate our parser on section 23 of theWSJ section of the Penn Treebank, andobtain precision and recall of 87.54% and87.61%, respectively.1 IntroductionTwo classifier-based deterministic dependencyparsers for English have been proposed recently(Nivre and Scholz, 2004; Yamada and Matsumoto,2003).
Although they use different parsing algo-rithms, and differ on whether or not dependenciesare labeled, they share the idea of greedily pursu-ing a single path, following parsing decisions madeby a classifier.
Despite their greedy nature, theseparsers achieve high accuracy in determining de-pendencies.
Although state-of-the-art statisticalparsers (Collins, 1997; Charniak, 2000) are moreaccurate, the simplicity and efficiency of determi-nistic parsers make them attractive in a number ofsituations requiring fast, light-weight parsing, orparsing of large amounts of data.
However, de-pendency analyses lack important information con-tained in constituent structures.
For example, thetree-path feature has been shown to be valuable insemantic role labeling (Gildea and Palmer, 2002).We present a parser that shares much of thesimplicity and efficiency of the deterministic de-pendency parsers, but produces both dependencyand constituent structures simultaneously.
Like theparser of Nivre and Scholz (2004), it uses the basicshift-reduce stack-based parsing algorithm, andruns in linear time.
While it may seem that thelarger search space of constituent trees (comparedto the space of dependency trees) would make itunlikely that accurate parse trees could be builtdeterministically, we show that the precision andrecall of constituents produced by our parser areclose to those produced by statistical parsers withhigher run-time complexity.One desirable characteristic of our parser is itssimplicity.
Compared to other successful ap-proaches to corpus-based constituent parsing, oursis remarkably simple to understand and implement.An additional feature of our approach is its modu-larity with regard to the algorithm and the classifierthat determines the parser?s actions.
This makes itvery simple for different classifiers and differentsets of features to be used with the same parserwith very minimal work.
Finally, its linear run-time complexity allows our parser to be considera-bly faster than lexicalized PCFG-based parsers.On the other hand, a major drawback of the classi-fier-based parsing framework is that, depending on125the classifier used, its training time can be muchlonger than that of other approaches.Like other deterministic parsers (and unlikemany statistical parsers), our parser considers theproblem of syntactic analysis separately from part-of-speech (POS) tagging.
Because the parsergreedily builds trees bottom-up in one pass, con-sidering only one path at any point in the analysis,the task of assigning POS tags to words is donebefore other syntactic analysis.
In this work wefocus only on the processing that occurs once POStagging is completed.
In the sections that follow,we assume that the input to the parser is a sentencewith corresponding POS tags for each word.2 Parser DescriptionOur parser employs a basic bottom-up shift-reduceparsing algorithm, requiring only a single pass overthe input string.
The algorithm considers onlytrees with unary and binary branching.
In order touse trees with arbitrary branching for training, orgenerating them with the parser, we employ aninstance of the transformation/detransformationprocess described in (Johnson, 1998).
In our case,the transformation step involves simply convertingeach production with n children (where n > 2) inton ?
1 binary productions.
Trees must be lexical-ized1, so that the newly created internal structure ofconstituents with previous branching of more thantwo contains only subtrees with the same lexicalhead as the original constituent.
Additional non-terminal symbols introduced in this process areclearly marked.
The transformed (or ?binarized?
)trees may then be used for training.
Detransforma-tion is applied to trees produced by the parser.This involves the removal of non-terminals intro-1If needed, constituent head-finding rules such as those men-tioned in Collins (1996) may be used.TransformNPNP                                                               NP*PP                                                      NP*NP                                                       PPDet     Adj     N        P         N                                                              NPThe    big    dog    with    fleas                    Det   Adj      N       P         NThe   big    dog    with    fleasDetransformFigure 1: An example of the binarization transform/detransform.
The original tree (left) has onenode (NP) with four children.
In the transformed tree, internal structure (marked by nodes with as-terisks) was added to the subtree rooted by the node with more than two children.
The word ?dog?is the head of the original NP, and it is kept as the head of the transformed NP, as well as the head ofeach NP* node.126duced in the transformation process, producingtrees with arbitrary branching.
An example oftransformation/detransformation is shown in figure1.2.1 Algorithm OutlineThe parsing algorithm involves two main datastructures: a stack S, and a queue W.  Items in Smay be terminal nodes (POS-tagged words), or(lexicalized) subtrees of the final parse tree for theinput string.
Items in W are terminals (wordstagged with parts-of-speech) corresponding to theinput string.
When parsing begins, S is empty andW is initialized by inserting every word from theinput string in order, so that the first word is infront of the queue.Only two general actions are allowed: shift andreduce.
A shift action consists only of removing(shifting) the first item (POS-tagged word) from W(at which point the next word becomes the newfirst item), and placing it on top of S.  Reduce ac-tions are subdivided into unary and binary cases.In a unary reduction, the item on top of S ispopped, and a new item is pushed onto S.  The newitem consists of a tree formed by a non-terminalnode with the popped item as its single child.
Thelexical head of the new item is the same as thelexical head of the popped item.
In a binary reduc-tion, two items are popped from S in sequence, anda new item is pushed onto S.  The new item con-sists of a tree formed by a non-terminal node withtwo children: the first item popped from S is theright child, and the second item is the left child.The lexical head of the new item is either the lexi-cal head of its left child, or the lexical head of itsright child.If S is empty, only a shift action is allowed.
IfW is empty, only a reduce action is allowed.
Ifboth S and W are non-empty, either shift or reduceactions are possible.
Parsing terminates when W isempty and S contains only one item, and the singleitem in S is the parse tree for the input string.
Be-cause the parse tree is lexicalized, we also have adependency structure for the sentence.
In fact, thebinary reduce actions are very similar to the reduceactions in the dependency parser of Nivre andScholz (2004), but they are executed in a differentorder, so constituents can be built.
If W is empty,and more than one item remain in S, and no furtherreduce actions take place, the input string is re-jected.2.2 Determining Actions with a ClassifierA parser based on the algorithm described in theprevious section faces two types of decisions to bemade throughout the parsing process.
The firsttype concerns whether to shift or reduce when bothactions are possible, or whether to reduce or rejectthe input when only reduce actions are possible.The second type concerns what syntactic structuresare created.
Specifically, what new non-terminal isintroduced in unary or binary reduce actions, orwhich of the left or right children are chosen as thesource of the lexical head of the new subtree pro-duced by binary reduce actions.
Traditionally,these decisions are made with the use of a gram-mar, and the grammar may allow more than onevalid action at any single point in the parsing proc-ess.
When multiple choices are available, a gram-mar-driven parser may make a decision based onheuristics or statistical models, or pursue everypossible action following a search strategy.
In ourcase, both types of decisions are made by a classi-fier that chooses a unique action at every point,based on the local context of the parsing action,with no explicit grammar.
This type of classifier-based parsing where only one path is pursued withno backtracking can be viewed as greedy or deter-ministic.In order to determine what actions the parsershould take given a particular parser configuration,a classifier is given a set of features derived fromthat configuration.
This includes, crucially, thetwo topmost items in the stack S, and the item infront of the queue W.  Additionally, a set of contextfeatures is derived from a (fixed) limited numberof items below the two topmost items of S, andfollowing the item in front of W.  The specific fea-tures are shown in figure 2.The classifier?s target classes are parser actionsthat specify both types of decisions mentionedabove.
These classes are:?
SHIFT: a shift action is taken;?
REDUCE-UNARY-XX: a unary reduce ac-tion is taken, and the root of the new subtreepushed onto S is of type XX (where XX is anon-terminal symbol, typically NP, VP, PP,for example);?
REDUCE-LEFT-XX: a binary reduce actionis taken, and the root of the new subtreepushed onto S is of non-terminal type XX.127Additionally, the head of the new subtree isthe same as the head of the left child of theroot node;?
REDUCE-RIGHT-XX: a binary reduce ac-tion is taken, and the root of the new subtreepushed onto S is of non-terminal type XX.Additionally, the head of the new subtree isthe same as the head of the right child of theroot node.2.3 A Complete Classifier-Based Parser thanRuns in Linear TimeWhen the algorithm described in section 2.1 iscombined with a trained classifier that determinesits parsing actions as described in section 2.2, wehave a complete classifier-based parser.
Trainingthe parser is accomplished by training its classifier.To that end, we need training instances that consistof sets of features paired with their classes corre-Let:S(n) denote the nth item from the top of the stack S, andW(n) denote the nth item from the front of the queue W.Features:?
The head-word (and its POS tag) of: S(0), S(1), S(2), and S(3)?
The head-word (and its POS tag) of: W(0), W(1), W(3) and W(3)?
The non-terminal node of the root of: S(0), and S(1)?
The non-terminal node of the left child of the root of: S(0), and S(1)?
The non-terminal node of the right child of the root of: S(0), and S(1)?
The non-terminal node of the left child of the root of: S(0), and S(1)?
The non-terminal node of the left child of the root of: S(0), and S(1)?
The linear distance (number of words apart) between the head-words of S(0) and S(1)?
The number of lexical items (words) that have been found (so far) to be dependents ofthe head-words of: S(0), and S(1)?
The most recently found lexical dependent of the head of the head-word of S(0) that isto the left of S(0)?s head?
The most recently found lexical dependent of the head of the head-word of S(0) that isto the right of S(0)?s head?
The most recently found lexical dependent of the head of the head-word of S(0) that isto the left of S(1)?s head?
The most recently found lexical dependent of the head of the head-word of S(0) that isto the right of S(1)?s headFigure 2: Features used for classification.
The features described in items 1 ?
7 are more di-rectly related to the lexicalized constituent trees that are built during parsing, while the fea-tures described in items 8 ?
13 are more directly related to the dependency structures that arebuilt simultaneously to the constituent structures.128sponding to the correct parsing actions.
These in-stances can be obtained by running the algorithmon a corpus of sentences for which the correctparse trees are known.
Instead of using the classi-fier to determine the parser?s actions, we simplydetermine the correct action by consulting the cor-rect parse trees.
We then record the features andcorresponding actions for parsing all sentences inthe corpus into their correct trees.
This set of fea-tures and corresponding actions is then used totrain a classifier, resulting in a complete parser.When parsing a sentence with n words, theparser takes n shift actions (exactly one for eachword in the sentence).
Because the maximumbranching factor of trees built by the parser is two,the total number of binary reduce actions is n ?
1,if a complete parse is found.
If the input string isrejected, the number of binary reduce actions isless than n ?
1.
Therefore, the number of shift andbinary reduce actions is linear with the number ofwords in the input string.
However, the parser asdescribed so far has no limit on the number ofunary reduce actions it may take.
Although inpractice a parser properly trained on trees reflect-ing natural language syntax would rarely makemore than 2n unary reductions, pathological casesexist where an infinite number of unary reductionswould be taken, and the algorithm would not ter-minate.
Such cases may include the observation inthe training data of sequences of unary productionsthat cycle through (repeated) non-terminals, suchas A->B->A->B.
During parsing, it is possible thatsuch a cycle may be repeated infinitely.This problem can be easily prevented by limit-ing the number of consecutive unary reductionsthat may be made to a finite number.
This may bethe number of non-terminal types seen in the train-ing data, or the length of the longest chain of unaryproductions seen in the training data.
In our ex-periments (described in section 3), we limited thenumber of consecutive unary reductions to three,although the parser never took more than twounary reduction actions consecutively in any sen-tence.
When we limit the number of consecutiveunary reductions to a finite number m, the parsermakes at most (2n ?
1)m unary reductions whenparsing a sentence of length n.  Placing this limitnot only guarantees that the algorithm terminates,but also guarantees that the number of actionstaken by the parser is O(n), where n is the length ofthe input string.
Thus, the parser runs in lineartime, assuming that classifying a parser action isdone in constant time.3 Similarities to Previous WorkAs mentioned before, our parser shares similaritieswith the dependency parsers of Yamada and Ma-tsumoto (2003) and Nivre and Scholz (2004) inthat it uses a classifier to guide the parsing processin deterministic fashion.
While Yamada and Ma-tsumoto use a quadratic run-time algorithm withmultiple passes over the input string, Nivre andScholz use a simplified version of the algorithmdescribed here, which handles only (labeled orunlabeled) dependency structures.Additionally, our parser is in some ways similarto the maximum-entropy parser of Ratnaparkhi(1997).
Ratnaparkhi?s parser uses maximum-entropy models to determine the actions of a shift-reduce-like parser, but it is capable of pursuingseveral paths and returning the top-K highest scor-ing parses for a sentence.
Its observed time is lin-ear, but parsing is somewhat slow, with sentencesof length 20 or more taking more than one secondto parse, and sentences of length 40 or more takingmore than three seconds.
Our parser only pursuesone path per sentence, but it is very fast and ofcomparable accuracy (see section 4).
In addition,Ratnaparkhi?s parser uses a more involved algo-rithm that allows it to work with arbitrary branch-ing trees without the need of the binarizationtransform employed here.
It breaks the usual re-duce actions into smaller pieces (CHECK andBUILD), and uses two separate passes (not includ-ing the POS tagging pass) for determining chunksand higher syntactic structures separately.Finally, there have been other deterministicshift-reduce parsers introduced recently, but theirlevels of accuracy have been well below the state-of-the-art.
The parser in Kalt (2004) uses a similaralgorithm to the one described here, but the classi-fication task is framed differently.
Using decisiontrees and fewer features, Kalt?s parser has signifi-cantly faster training and parsing times, but its ac-curacy is much lower than that of our parser.Kalt?s parser achieves precision and recall of about77% and 76%, respectively (with automaticallytagged text), compared to our parser?s 86% (seesection 4).
The parser of Wong and Wu (1999)uses a separate NP-chunking step and, like Ratna-parkhi?s parser, does not require a binary trans-129form.
It achieves about 81% precision and 82%recall with gold-standard tags (78% and 79% withautomatically tagged text).
Wong and Wu?s parseris further differentiated from the other parsersmentioned here in that it does not use lexical items,working only from part-of-speech tags.4 ExperimentsWe conducted experiments with the parser de-scribed in section 2 using two different classifiers:TinySVM (a support vector machine implementa-tion by Taku Kudo)2, and the memory-basedlearner TiMBL (Daelemans et al, 2004).
Wetrained and tested the parser on the Wall StreetJournal corpus of the Penn Treebank (Marcus etal., 1993) using the standard split: sections 2-21were used for training, section 22 was used for de-velopment and tuning of parameters and features,and section 23 was used for testing.
Every ex-periment reported here was performed on a Pen-tium IV 1.8GHz with 1GB of RAM.Each tree in the training set had empty-nodeand function tag information removed, and the2http://chasen.org/~taku/software/TinySVMtrees were lexicalized using similar head-tablerules as those mentioned in (Collins, 1996).
Thetrees were then converted into trees containingonly unary and binary branching, using the binari-zation transform described in section 2.
Classifiertraining instances of features paired with classes(parser actions) were extracted from the trees in thetraining set, as described in section 2.3.
The totalnumber of training instances was about 1.5 million.The classifier in the SVM-based parser (de-noted by SVMpar) uses the polynomial kernel withdegree 2, following the work of Yamada and Ma-tsumoto (2003) on SVM-based deterministic de-pendency parsing, and a one-against-all scheme formulti-class classification.
Because of the largenumber of training instances, we used Yamada andMatsumoto?s idea of splitting the training instancesinto several parts according to POS tags, and train-ing classifiers on each part.
This greatly reducedthe time required to train the SVMs, but even withthe splitting of the training set, total training timewas about 62 hours.
Training set splitting comeswith the cost of reduction in accuracy of the parser,but training a single SVM would likely take morethan one week.
Yamada and Matsumoto experi-enced a reduction of slightly more than 1% in de-Precision Recall Dependency Time (min)Charniak 89.5 89.6 92.1 28Collins 88.3 88.1 91.5 45Ratnaparkhi 87.5 86.3 Unk UnkY&M - - 90.3 UnkN&S - - 87.3 21MBLpar 80.0 80.2 86.3 127SVMpar 87.5 87.6 90.3 11Table 1: Summary of results on labeled precision and recall of constituents, dependency accu-racy, and time required to parse the test set.
The parsers of Yamada and Matsumoto (Y&M) andNivre and Scholz (N&S) do not produce constituent structures, only dependencies.
?unk?
indi-cates unknown values.
Results for MBLpar and SVMpar using correct POS tags (if automaticallyproduced POS tags are used, accuracy figures drop about 1.5% over all metrics).130pendency accuracy due to training set splitting, andwe expect that a similar loss is incurred here.When given perfectly tagged text (gold tags ex-tracted from the Penn Treebank), SVMpar has la-beled constituent precision and recall of 87.54%and 87.61%, respectively, and dependency accu-racy of 90.3% over all sentences in the test set.The total time required to parse the entire test setwas 11 minutes.
Out of more than 2,400 sen-tences, only 26 were rejected by the parser (about1.1%).
For these sentences, partial analyses werecreated by combining the items in the stack in flatstructures, and these were included in the evalua-tion.
Predictably, the labeled constituent precisionand recall obtained with automatically POS-taggedsentences were lower, at 86.01% and 86.15%.
Thepart-of-speech tagger used in our experiments wasSVMTool (Gim?nez and M?rquez, 2004), and itsaccuracy on the test set is 97%.The MBL-based parser (denoted by MBLpar)uses the IB1 algorithm, with five nearestneighbors, and the modified value difference met-ric (MVDM), following the work of Nivre andScholz (2004) on MBL-based deterministic de-pendency parsing.
MBLpar was trained with alltraining instances in under 15 minutes, but its ac-curacy on the test set was much lower than that ofSVMpar, with constituent precision and recall of80.0% and 80.2%, and dependency accuracy of86.3% (24 sentences were rejected).
It was alsomuch slower than SVMpar in parsing the test set,taking 127 minutes.
In addition, the total memoryrequired for running MBLpar (including the classi-fier) was close to 1 gigabyte (including the trainedclassifier), while SVMpar required only about 200megabytes (including all the classifiers).Table 1 shows a summary of the results of ourexperiments with SVMpar and MBLpar, and alsoresults obtained with the Charniak (2000) parser,the Bikel (2003) implementation of the Collins(1997) parser, and the Ratnaparkhi (1997) parser.We also include the dependency accuracy fromYamada and Matsumoto?s (2003) SVM-based de-pendency parser, and Nivre and Scholz?s (2004)MBL-based dependency parser.
These resultsshow that the choice of classifier is extremely im-portant in this task.
SVMpar and MBLpar use thesame algorithm and features, and differ only on theclassifiers used to make parsing decisions.
Whilein many natural language processing tasks differentclassifiers perform at similar levels of accuracy, wehave observed a dramatic difference between usingsupport vector machines and a memory-basedlearner.
Although the reasons for such a large dis-parity in results is currently the subject of furtherinvestigation, we speculate that a relatively smalldifference in initial classifier accuracy results inlarger differences in parser performance, due to thedeterministic nature of the parser (certain errorsmay lead to further errors).
We also believe classi-fier choice to be one major source of the differencein accuracy between Nivre and Scholz?s parser andYamada and Matsumoto?s parser.While the accuracy of SVMpar is below that oflexicalized PCFG-based statistical parsers, it issurprisingly good for a greedy parser that runs inlinear time.
Additionally, it is considerably fasterthan lexicalized PCFG-based parsers, and offers agood alternative for when fast parsing is needed.MBLpar, on the other hand, performed poorly interms of accuracy and speed.5 Conclusion and Future WorkWe have presented a simple shift-reduce parserthat uses a classifier to determine its parsing ac-tions and runs in linear time.
Using SVMs forclassification, the parser has labeled constituentprecision and recall higher than 87% when usingthe correct part-of-speech tags, and slightly higherthan 86% when using automatically assigned part-of-speech tags.
Although its accuracy is not ashigh as those of state-of-the-art statistical parsers,our classifier-based parser is considerably fasterthan several well-known parsers that employsearch or dynamic programming approaches.
Atthe same time, it is significantly more accuratethan previously proposed deterministic parsers forconstituent structures.We have also shown that much of the successof a classifier-based parser depends on what classi-fier is used.
While this may seem obvious, the dif-ferences observed here are much greater than whatwould be expected from looking, for example, atresults from chunking/shallow parsing (Zhang etal., 2001; Kudo and Matsumoto, 2001; Veenstraand van den Bosch, 2000).Future work includes the investigation of the ef-fects of individual features, the use of additionalclassification features, and the use of different clas-sifiers.
In particular, the use of tree features seemsappealing.
This may be accomplished with SVMs131using a tree kernel, or the tree boosting classifierBACT described in (Kudo and Matsumoto, 2004).Additionally, we plan to investigate the use of thebeam strategy of Ratnaparkhi (1997) to pursuemultiple parses while keeping the run-time linear.ReferencesCharniak, E.  2000.
A maximum-entropy-inspiredparser.
Proceedings of the First Annual Meeting ofthe North American Chapter of the Association forComputational Linguistics.
Seattle, WA.Collins, M. 1997.
Three generative, lexicalized modelsfor statistical parsing.
Proceedings of the 35th An-nual Meeting of the Association for ComputationalLinguistics (pp.
16-23).
Madrid, Spain.Daelemans, W., Zavrel, J., van der Sloot, K., and vanden Bosch, A.
2004.
TiMBL: Tilburg MemoryBased Learner, version 5.1, reference guide.
ILK Re-search Group Technical Report Series no.
04-02,2004.Gildea, D., and Palmer, M.  2002.
The necessity of syn-tactic parsing for predicate argument recognition.Proceedings of the 40th Annual Meeting of the Asso-ciation for Computational Linguistics (pp.
239-246).Philadelphia, PA.Kalt, T. 2004.
Induction of greedy controllers for de-terministic treebank parsers.
Proceedings of the2004 Conference on Empirical Methods in NaturalLanguage Processing.
Barcelona, Spain.Kudo, T., and Matsumoto, Y.
2004.
A boosting algo-rithm for classification of semi-structured text.
Pro-ceedings of the 2004 Conference on EmpiricalMethods in Natural Language Processing.
Barce-lona, Spain.Kudo, T., and Matsumoto, Y.
2001.
Chunking withsupport vector machines.
Proceedings of the SecondMeeting of the North American Chapter of the Asso-ciation for Computational Linguistics.
Pittsburgh,PA.Johnson, M. 1998.
PCFG models of linguistic tree rep-resentations.
Computational Linguistics, 24:613-632.Marcus, M. P., Santorini, B., and Marcinkiewics, M. A.1993.
Building a large annotated corpus of English:the Penn Treebank.
Computational Linguistics, 19.Nivre, J., and Scholz, M.  2004.
Deterministic depend-ency parsing of English text.
Proceedings of the 20thInternational Conference on Computational Linguis-tics  (pp.
64-70).
Geneva, Switzerland.Ratnaparkhi, A.
1997.
A linear observed time statisticalparser based on maximum entropy models.
Proceed-ings of the Second Conference on Empirical Methodsin Natural Language Processing.
Providence, RhodeIsland.Veenstra, J., van den Bosch, A.
2000.
Single-classifiermemory-based phrase chunking.
Proceedings ofFourth Workshop on Computational Natural Lan-guage Learning (CoNLL 2000).
Lisbon, Portugal.Wong, A., and Wu.
D. 1999.
Learning a lightweightrobust deterministic parser.
Proceedings of the SixthEuropean Conference on Speech Communication andTechnology.
Budapest.Yamada, H., and Matsumoto, Y.
2003.
Statistical de-pendency analysis with support vector machines.Proceedings of the Eighth International Workshop onParsing Technologies.
Nancy, France.Zhang, T., Damerau, F., and Johnson, D. 2002.
Textchunking using regularized winnow.
Proceedings ofthe 39th Annual Meeting of the Association for Com-putational Linguistics.
Tolouse, France.132
