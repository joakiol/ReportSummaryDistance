Proceedings of the Workshop on Innovative Hybrid Approaches to the Processing of Textual Data (Hybrid2012), EACL 2012, pages 10?18,Avignon, France, April 23 2012. c?2012 Association for Computational LinguisticsA Study of Hybrid Similarity Measures for Semantic Relation ExtractionAlexander Panchenko and Olga MorozovaCenter for Natural Language Processing (CENTAL)Universite?
catholique de Louvain, Belgium{alexander.panchenko, olga.morozova}@uclouvain.beAbstractThis paper describes several novel hybridsemantic similarity measures.
We studyvarious combinations of 16 baseline mea-sures based on WordNet, Web as a cor-pus, corpora, dictionaries, and encyclope-dia.
The hybrid measures rely on 8 com-bination methods and 3 measure selectiontechniques and are evaluated on (a) the taskof predicting semantic similarity scores and(b) the task of predicting semantic relationbetween two terms.
Our results show thathybrid measures outperform single mea-sures by a wide margin, achieving a correla-tion up to 0.890 and MAP(20) up to 0.995.1 IntroductionSemantic similarity measures and relations areproven to be valuable for various NLP and IRapplications, such as word sense disambiguation,query expansion, and question answering.Let R be a set of synonyms, hypernyms, andco-hyponyms of terms C, established by a lexi-cographer.
A semantic relation extraction methodaims at discovering a set of relations R?
approx-imating R. The quality of the relations providedby existing extractors is still lower than the qualityof the manually constructed relations.
This moti-vates the development of new relation extractionmethods.A well-established approach to relation extrac-tion is based on lexico-syntactic patterns (Augerand Barrie`re, 2008).
In this paper, we study an al-ternative approach based on similarity measures.These methods do not return a type of the rela-tion between words (R?
?
C ?
C).
However,we assume that the methods should retrieve a mixof synonyms, hypernyms, and co-hyponyms forpractical use in text processing applications andevaluate them accordingly.A multitude of measures was used in the pre-vious research to extract synonyms, hypernyms,and co-hyponyms.
Five key approaches are thosebased on a distributional analysis (Lin, 1998b),Web as a corpus (Cilibrasi and Vitanyi, 2007),lexico-syntactic patterns (Bollegala et al, 2007),semantic networks (Resnik, 1995), and defini-tions of dictionaries or encyclopedias (Zesch etal., 2008a).
Still, the existing approaches based onthese single measures are far from being perfect.For instance, Curran and Moens (2002) compareddistributional measures and reported Precision@1of 76% for the best one.
For improving the per-formance, some attempts were made to combinesingle measures, such as (Curran, 2002; Ceder-berg and Widdows, 2003; Mihalcea et al, 2006;Agirre et al, 2009; Yang and Callan, 2009).
How-ever, most studies are still not taking into accountthe whole range of existing measures, combiningmostly sporadically different methods.The main contribution of the paper is a system-atic analysis of 16 baseline measures, and theircombinations with 8 fusion methods and 3 tech-niques for the combination set selection.
We arefirst to propose hybrid similarity measures basedon all five extraction approaches listed above; ourcombined techniques are original as they exploitall key types of resources usable for semantic re-lation extraction ?
corpus, web corpus, semanticnetworks, dictionaries, and encyclopedias.
Ourexperiments confirm that the combined measuresare more precise than the single ones.
The bestfound hybrid measure combines 15 baseline mea-sures with the supervised learning.
It outperforms10Figure 1: (a) Single and (b) hybrid relation extractorsbased on similarity measures.all tested single and combined measures by a largemargin, achieving a correlation of 0.870 with hu-man judgements and MAP(20) of 0.995 on the re-lation recognition task.2 Similarity-based Relation ExtractionIn this paper a similarity-based relation extractionmethod is used.
In contrast to the traditionalapproaches, relying on a single measure, ourmethod relies on a hybrid measure (see Figure 1).A hybrid similarity measure combines severalsingle similarity measures with a combinationmethod to achieve better extraction results.
Toextract relations R?
between terms C, the methodcalculates pairwise similarities between themwith the help of a similarity measure.
Therelations are established between each termc ?
C and the terms most similar to c (its nearestneighbors).
First, a term-term (C ?
C) similaritymatrix S is calculated with a similarity measuresim, as depicted in Figure 1 (a).
Then, thesesimilarity scores are mapped to the interval [0; 1]with a norm function as follows: S?
= S?min(S)max(S) .Dissimilarity scores are transformed into sim-ilarity scores: S?
= 1 ?
norm(S).
Finally,the knn function calculates semantic relationsbetween terms with a k-NN thresholding: R?
=?|C|i=1 {?ci, cj?
: (cj ?
top k% of ci) ?
(sij > 0)} .Here, k is a percent of top similar terms to a termci.
Thus, the method links each term ci with k%of its nearest neighbours.3 Single Similarity MeasuresA similarity measure extracts or recalls a sim-ilarity score sij ?
S between a pair of termsci, cj ?
C. In this section we list 16 baselinemeasures exploited by hybrid measures.
The mea-sures were selected as (a) the previous researchsuggests that they are able to capture synonyms,hypernyms, and co-hyponyms; (b) they rely on allmain resources used to derive semantic similarity?
semantic networks, Web as a corpus, traditionalcorpora, dictionaries, and encyclopedia.3.1 Measures Based on a Semantic NetworkWe test 5 measures relying on WORDNET seman-tic network (Miller, 1995) to calculate the simi-larities: Wu and Palmer (1994) (1), Leacock andChodorow (1998) (2), Resnik (1995) (3), Jiangand Conrath (1997) (4), and Lin (1998a) (5).These measures exploit the lengths of the short-est paths between terms in a network and proba-bility of terms derived from a corpus.
We use im-plementation of the measures available in WORD-NET::SIMILARITY (Pedersen et al, 2004).A limitation of these measures is that similari-ties can only be calculated upon 155.287 Englishterms from WordNet 3.0.
In other words, thesemeasures recall rather than extract similarities.Therefore, they should be considered as a sourceof common lexico-semantic knowledge for a hy-brid semantic similarity measure.3.2 Web-based MeasuresWeb-based metrics use Web search engines forcalculation of similarities.
They rely on the num-ber of times the terms co-occur in the documentsas indexed by an information retrieval system.We use 3 baseline web measures based on indexof YAHOO!
(6), BING (7), and GOOGLE overthe domain wikipedia.org (8).
These threemeasures exploit Normalized Google Distance(NGD) formula (Cilibrasi and Vitanyi, 2007) fortransforming the number of hits into a similarityscore.
Our own system implements BING mea-sure, while Measures of Semantic Relatedness(MSR) web service1 calculates similarities withYAHOO!
and GOOGLE.The coverage of languages and vocabularies byweb-based measures is huge.
Therefore, it is as-sumed that they are able to extract new lexico-semantic knowledge.
Web-based measures arelimited by constraints of a search engine API(hundreds of thousands of queries are needed).1http://cwl-projects.cogsci.rpi.edu/msr/113.3 Corpus-based MeasuresWe tested 5 measures relying on corpora to cal-culate similarity of terms: two baseline distri-butional measures, one novel measure based onlexico-syntactic patterns, and two other baselinemeasures.
Each of them uses a different corpus.Corpus-based measures are able to extract sim-ilarity between unknown terms.
Extraction capa-bilities of these measures are limited by a corpus.If terms do not occur in a text, then it would beimpossible to calculate similarities between them.Distributional MeasuresThese measures are based on a distributionalanalysis of a 800M tokens corpus WACYPE-DIA (Baroni et al, 2009) tagged with TREETAG-GER and dependency-parsed with MALTPARSER.We rely on our own implementation of two distri-butional measures.
The distributional measure (9)performs Bag-of-words Distributional Analysis(BDA) (Sahlgren, 2006).
We use as features the5000 most frequent lemmas (nouns, adjectives,and verbs) from a context window of 3 words,excluding stopwords.
The distributional measure(10) performs Syntactic Distributional Analysis(SDA) (Lin, 1998b).
For this one, we use asfeatures the 100.000 most frequent dependency-lemma pairs.
In our implementation of SDA aterm ci is represented with a feature ?dtj , wk?,if wk is not in a stoplist and dtj has one of thefollowing dependency types: NMOD, P, PMOD,ADV, SBJ, OBJ, VMOD, COORD, CC, VC, DEP,PRD, AMOD, PRN, PRT, LGS, IOBJ, EXP, CLF,GAP .
For both BDA and SDA: the feature matrixis normalized with PointwiseMutual Information;similarities between terms are calculated with acosine between their respective feature vectors.Pattern-based MeasureWe developed a novel similarity measure Pat-ternWiki (13), which relies on 10 lexico-syntacticpatterns.
2 First, we apply the patterns to the WA-CYPEDIA corpus and get as a result a list of con-cordances (see below).
Next, we select the con-cordances which contain at least two terms fromthe input vocabulary C. The semantic similaritysij between each two terms ci, cj ?
C is equalto the number of their co-occurences in the sameconcordance.The set of the patterns we used is a compilation2Available at http://http://cental.fltr.ucl.ac.be/team/?morozova/pattern-wiki.tar.gzof the 6 classical Hearst (1992) patterns, aiming atthe extraction of hypernymic relations, as well as3 patterns retrieving some other hypernyms andco-hyponyms and 1 synonym extraction pattern,which we found in accordance with Hearst?s pat-tern discovery algorithm.
The patterns are en-coded in a form of finite-state transducers with thehelp of a corpus processing tool UNITEX 3 (Pau-mier, 2003).
The main graph is a cascade of thesubgraphs, each of which encodes one of the pat-terns.
For example, Figure 2 presents the graphwhich extracts, e. g.:?
such diverse {[occupations]} as{[doctors]}, {[engineers]} and{[scientists]}[PATTERN=1]Figure brackets mark the noun phrases, which arein the semantic relation, nouns and compoundnouns stand between the square brackets.
Uni-tex enables the exclusion of meaningless adjec-tives and determiners out of the tagging, whilethe patterns containing them are still being recog-nized.
So, the notion of a pattern has more generalsense with respect to other works such as (Bolle-gala et al, 2007), where each construction witha different lexical item, a word form or even apunctuation mark is regarded as a unique pat-tern.
The nouns extracted from the square brack-ets are lemmatized with the help of DELA dictio-nary4, which consists of around 300,000 simpleand 130,000 compound words.
If the noun to ex-tract is a plural form of a noun in the dictionary,then it is re-written into the respective singularform.
Semantic similarity score is equal to thenumber of co-occurences of terms in the squarebrackets within the same concordance (the num-ber of extractions between the terms).Other Corpus-based MeasuresIn addition to the three measures presentedabove, we use two other corpus-based measuresavailable via the MSR web service.
The mea-sure (11) relies on the Latent Semantic Analysis(LSA) (Landauer and Dumais, 1997) trained onthe TASA corpus (Veksler et al, 2008).
LSA cal-culates similarity of terms with a cosine betweentheir respective vectors in the ?concept space?.The measure (12) relies on the NGD formula (seeSection 3.2), where counts are derived from theFactiva corpus (Veksler et al, 2008).3http://igm.univ-mlv.fr/?unitex/4Available at http://infolingu.univ-mlv.fr/12Figure 2: An example of a UNITEX graph for hypernym extraction (subgraphs are marked with gray; <E>defines zero; <DET> defines determiners; bold symbols and letters outside of the boxes are annotation tags)3.4 Definition-based MeasuresWe test 3 measures which rely on explicit defini-tions of terms specified in dictionaries.
The firstmetric WktWiki (14) is a novel similarity measurethat stems from the Lesk algorithm (Pedersen etal., 2004) and the work of Zesch et al (2008a).WktWiki operates on Wiktionary definitions andrelations and Wikipedia abstracts.
WktWiki cal-culates similarity as follows.
First, definitions foreach input term c ?
C are built.
A ?definition?is a union of all available glosses, examples, quo-tations, related words, and categories from Wik-tionary and a short abstract of the correspondingWikipedia article (a name of the article must ex-actly match the term c).
We use all senses corre-sponding to a surface form of term c. Then, eachterm c ?
C of the 1000 most frequent lemmasis represented as a bag-of-lemma vector, derivedfrom its ?definition?.
Feature vectors are normal-ized with Pointwise Mutual Information and simi-larities between terms are calculated with a cosinebetween them.
Finally, the pairwise similaritiesbetween terms S are corrected.
The highest simi-larity score is assigned to the pairs of terms whichare directly related in Wiktionary.
5WktWiki is different to the work of Zesch et al(2008b) in three aspects: (a) terms are representedin a word space, and not in a document space;(b) both texts from Wiktionary and Wikipedia areused; (c) relations of Wiktionary are used to up-date similarity scores.In addition to WktWiki, we operate with 2baseline measures relying on WordNet glossesavailable in a WORDNET::SIMILARITY package:Gloss Vectors (Patwardhan and Pedersen, 2006)5We used JWKTL library (Zesch et al, 2008a), as an API toWiktionary and DBpedia.org as a source of Wikipedia short ab-stracts (dumps were downloaded in October 2011).
(15) and Extended Lesk (Banerjee and Pedersen,2003) (16).
The key difference between WktWikiand WordNet-based measures is that the latteruses definitions of related terms.Extraction capabilities of definition-based mea-sures are limited by the number of available def-initions.
As of October 2011, WordNet con-tains 117.659 definitions (glosses); Wiktionarycontains 536.594 definitions in English and4.272.902 definitions in all languages; Wikipediahas 3.866.773 English articles and around 20.8millons of articles in all languages.4 Hybrid Similarity MeasuresA hybrid similarity measure combines several sin-gle similarity measures described above with oneof the combination methods described below.4.1 Combination MethodsA goal of a combination method is to producesimilarity scores which perform better than thescores of input single measures.
A combinationmethod takes as an input a set of similarity ma-trices {S1, .
.
.
,SK} produced by K single mea-sures and outputs a combined similarity matrixScmb.
We denote as skij a pairwise similarity scoreof terms ci and cj produced by k-th measure.
Wetest the 8 following combination methods:Mean.
A mean ofK pairwise similarity scores:Scmb =1KK?k=1Sk ?
scmbij =1K?k=1,Kskij .Mean-Nnz.
A mean of those pairwise similar-ity scores which have a non-zero value:scmbij =1|k : skij > 0, k = 1,K|?k=1,Kskij .13Mean-Zscore.
A mean of K similarity scorestransformed into Z-scores:scmbij =1KK?k=1skij ?
?k?k,where ?k is a mean and ?k is a standard deviationof similarity scores of k-th measure (Sk).Median.
A median of K pairwise similarities:scmbij = median(s1ij , .
.
.
, sKij ).Max.
A maximum of K pairwise similarities:scmbij = max(s1ij , .
.
.
, sKij ).Rank Fusion.
First, this combination methodconverts each pairwise similarity score skij to arank rkij .
Here, rkij = 5 means that term cj is the5-th nearest neighbor of the term ci, according tothe k-th measure.
Then, it calculates a combinedsimilarity score as a mean of these pairwise ranks:scmbij = 1K?k=1,K rkij .Relation Fusion.
This combination methodgathers and unites the best relations provided byeach measure.
First, the method retrieves rela-tions extracted by single measures with the func-tion knn described in Section 2.
We have empiri-cally chosen an ?internal?
kNN threshold of 20%for this combination method.
Then, a set of ex-tracted relations Rk, obtained from the k-th mea-sure, is encoded as an adjacency matrix Rk .
Anelement of this matrix indicates whether terms ciand cj are related:rkij ={1 if semantic relation ?ci, cj?
?
Rk0 elseThe final similarity score is a mean of adjacencymatrices: Scmb = 1K?Ki=1 Ri.
Thus, if two mea-sures are combined and the first extracted the re-lation between ci and cj , while the second did not,then the similarity sij will be equal to 0.5.Logit.
This combination method is based onlogistic regression (Agresti, 2002).
We train a bi-nary classifier on a set of manually constructedsemantic relations R (we use BLESS and SNdatasets described in Section 5).
Positive trainingexamples are ?meaningful?
relations (synonyms,hyponyms, etc.
), while negative training examplesare pairs of semantically unrelated words (gener-ated randomly and verified manually).
A seman-tic relation ?ci, cj?
?
R is represented with a vec-tor of pairwise similarities between terms ci, cjcalculated with K measures (s1ij , .
.
.
, sKij ) and abinary variable rij (category):rij ={0 if ?ci, cj?
is a random relation1 otherwiseFor evaluation purposes, we use a special 10-foldcross validation ensuring that all relations of oneterm c are always in the same training/test fold.The results of the training are K + 1 coefficientsof regression (w0, w1, .
.
.
, wK).
We apply themodel to combine similarity measures as follows:scmbij =11 + e?z, z = w0 +K?k=1wkskij .4.2 Combination SetsAny of the 8 combination methods presentedabove may combine from 2 to 16 singlemeasures.
Thus, there are?16m=2 Cm16 =?16m=216!m!(16?m)!
= 65535 ways to choose whichsingle measures to use in a combination method.We apply three methods to find an efficient com-bination of measures in this search space: expertchoice of measures, forward stepwise procedure,and analysis of a logistic regression model.Expert choice of measures is based on the an-alytical and empirical properties of the measures.We chose 5 or 9 measures which perform well andrely on complimentary resources: corpus, Web,WordNet, etc.
Additionally, we selected a groupof all measures except for one which has shownthe worst results on all datasets.
Thus, accord-ing to this selection method we have chosen threegroups of measures (see Section 3 and Table 1 fornotation):?
E5 = {3, 9, 10, 13, 14}?
E9 = {1, 3, 9 ?
11, 13 ?
16}?
E15 = {1, 2, 3, 4, 5, 6, 8 ?
16}Forward stepwise procedure is a greedy algo-rithm which works as follows.
It takes as an in-put all measures, a method of their combinationsuch as Mean, and a criterion such as Precisionat k = 50.
It starts with a void set of measures.Then, at each iteration it adds to the combinationone measure which brings the biggest improve-ment to the criterion.
The algorithm stops whenno measure can improve the criteria.
According14to this method, we have chosen four groups of themeasures 6:?
S7 = {9 ?
11, 13 ?
16}?
S8a = {9 ?
16}?
S8b = {1, 9 ?
11, 13 ?
16}?
S10 = {1, 6, 9 ?
16}The last measure selection technique is basedon analysis of logistic regression trained on all 16measures as features.
Only measures with pos-itive coefficients are selected.
According to thismethod, 12 measures were chosen:?
R12 = {3, 5, 6, 8 ?
16}We test combination methods on the 8 sets ofmeasures specified above.
Remarkably, all threeselection techniques constantly choose six fol-lowing measures ?
9, 10, 11, 14, 15, 16, i. e., C-BowDA, C-SynDA, C-LSA-Tasa, D-WktWiki,N-GlossVectors, and N-ExtendedLesk.5 EvaluationEvaluation relies on human judgements about se-mantic similarity and on manually constructed se-mantic relations.
7Human Judgements Datasets.
This kind ofground truth enables direct assessment of measureperformance and indirect assessment of extractionquality with this measure.
Each of these datasetsconsists of N tuples ?ci, cj , sij?, where ci, cj areterms, and sij is their similarity obtained by hu-man judgement.
We use three standard humanjudgements datasets ?
MC (Miller and Charles,1991), RG (Rubenstein and Goodenough, 1965)and WordSim353 (Finkelstein et al, 2001), com-posed of 30, 65, and 353 pairs of terms respec-tively.
Let s = (si1, si2, .
.
.
, siN ) be a vector ofground truth scores, and s?
= (s?i1, s?i2, .
.
.
, s?iN )be a vector of similarity scores calculated with asimilarity measure.
Then, the quality of this mea-sure is assessed with Spearman?s correlation be-tween s and s?.Semantic Relations Datasets.
This kindof ground truth enables indirect assessment ofmeasure performance and direct assessment of6We used Mean as a hybrid measure and the followingcriteria: MAP(20), MAP(50), P(10), P(20) and P(50).
Wekept measures which were selected by most of the criteria.7An evaluation script is available at http://cental.fltr.ucl.ac.be/team/?panchenko/sre-eval/extraction quality with the measure.
Eachof these datasets consists of a set of seman-tic relations R, such as ?agitator, syn, activist?,?hawk , hyper, predator?, ?gun, syn,weapon?, and?dishwasher, cohypo, reezer?.
Each ?target?
termhas roughly the same number of meaningful andrandom relations.
We use two semantic relationdatasets: BLESS (Baroni and Lenci, 2011) andSN.
The first is used to assess hypernyms and co-hyponyms extraction.
BLESS relates 200 targetterms (100 animate and 100 inanimate nouns) to8625 relatum terms with 26554 semantic relations(14440 are meaningful and 12154 are random).Every relation has one of the following types: hy-pernym, co-hyponym, meronym, attribute, event,or random.
We use the second dataset to evalu-ate synonymy extraction.
SN relates 462 targetterms (nouns) to 5910 relatum terms with 14682semantic relations (7341 are meaningful and 7341are random).
We built SN from WordNet, Roget?sthesaurus, and a synonyms database 8.This kind of evaluation is based on the numberof correctly extracted relations with the methoddescribed in Section 2.
Let R?k be a set of ex-tracted semantic relations at a certain level ofthe kNN threshold k. Then, precision, recall,and mean average precision (MAP) at k are cal-culated correspondingly as follows: P (k) =|R?R?k||R?k|, R(k) = |R?R?k||R| ,M(k) =1k?ki=1 P (i).The quality of a similarity measure is assessedwith the six following statistics: P (10), P (20),P (50), R(50), M(20), and M(50).6 ResultsTable 1 and Figure 3 present performance of thesingle and hybrid measures on the five groundtruth datasets listed above.
The first three columnsof the table contain correlations with humanjudgements, while the other columns present per-formance on the relation extraction task.The first part of the table reports on scores of16 single measures.
Our results show that themeasures are indeed complimentary ?
there is nomeasure which performs best on all datasets.
Forinstance, the measure based on a syntactic dis-tributional analysis C-SynDA performed best onthe MC dataset achieving a correlation of 0.790;the WordNet measure WN-LeacockChodorowachieved the top score of 0.789 on the RG dataset;8http://synonyms-database.downloadaces.com15Figure 3: Precision-Recall graphs calculated on the BLESS dataset of (a) 16 single measures and the best hybridmeasure H-Logit-E15; (b) 8 hybrid measures.the corpus based measure C-NGD-Factiva wasbest on the WordSim353 dataset, achieving a cor-relation of 0.600.
On the BLESS dataset, syn-tactic distributional analysis C-SynDA performedbest for high precision among single measuresachieving MAP(20) of 0.984, while the bag-of-words distributional measure C-BowDA was thebest for high recall with R(50) of 0.772.
Onthe SN dataset, the WordNet-based measure N-WuPalmer was best both for precision and recall.The second part of Table 1 presents perfor-mance of the hybrid measures.
Our results showthat if signals from complimentary resources areused, then the retrieval of semantically similarwords is significantly improved.
Most of the hy-brid measures outperform the single measures onall the datasets.
We tested each of the 8 combina-tion methods presented in Section 4.1 with eachof the 8 sets of measures specified in Section 4.2.We report on the best metrics among all 64 hy-brid measures.
A notion H-Mean-S8a means thatthe Mean combination method provides the bestresults with the set of measures S8a.Measures based on the mean of non-zero simi-larities H-MeanNnz-S8a and H-MeanNnz-E5 per-formed best on MC and WordSim353 datasets re-spectively.
They achieved correlations of 0.878and 0.740, which is higher than scores of anyother measure.
At the same time, measure H-MeanZscore-S8b provided the best scores on theRG dataset among all single and hybrid measures,achieving correlation of 0.890.
Supervised mea-sure H-Logit-E15 based on Logistic Regressionprovided the very best results on both semanticrelations datasets BLESS and SN.
Furthermore, itoutperformed all single and hybrid measures onthat task, in terms of both precision and recall,achieving MAP(20) of 0.995 and R(50) of 0.818on BLESS and MAP(20) of 0.993 and R(50) of0.819 on SN.
H-Logit-E15 makes use of 15 simi-larity measures and disregards only the worst sin-gle measure W-NGD-Bing.As we can see in Figure 3 (b), combining simi-larity scores with a Max function appears to be theworst solution.
Combination methods based on anaverage and a median, including Rank and Rela-tion Fusion, perform much better.
These methodsprovide quite similar results: in the high precisionrange, they perform nearly as well as a supervisedcombination.
Relation Fusion even manages toslightly outperform Logit on the first 10-15 k-NN(see Figure 3).
However, all unsupervised com-bination methods are significantly worse if higherrecall is needed.We conclude that the H-Logit-E15 is the besthybrid similarity measure for semantic relationextraction and in terms of plausibility with humanjudgements among all single and hybrid measuresexamined in this paper.7 DiscussionHybrid measures achieve higher precision and re-call than single measures.
First, it is due tothe reuse of common lexico-semantic information(such as that a ?car?
is a synonym of a ?vehicle?
)via knowledge- and definition-based measures.Measures based on WordNet and dictionary defi-nitions achieve high precision as they rely on fine-grained manually constructed resources.
How-ever, due to limited coverage of these resources,16Similarity Measure MC RG WS BLESS SN?
?
?
P(10) P (20) M(20) P(50) M(50) R(50) P(10) P(20) M(20) P(50) M(50) R(50)Random 0.056 -0.047 -0.122 0.546 0.542 0.549 0.544 0.546 0.522 0.504 0.502 0.507 0.499 0.502 0.4981.
N-WuPalmer 0.742 0.775 0.331 0.974 0.929 0.972 0.702 0.879 0.674 0.982 0.959 0.981 0.766 0.917 0.7632.
N-Leack.Chod.
0.724 0.789 0.295 0.953 0.901 0.954 0.702 0.863 0.648 0.984 0.953 0.981 0.757 0.913 0.7553.
N-Resnik 0.784 0.757 0.331 0.970 0.933 0.970 0.700 0.879 0.647 0.948 0.908 0.948 0.724 0.874 0.7224.
N-JiangConrath 0.719 0.588 0.175 0.956 0.872 0.920 0.645 0.817 0.458 0.931 0.857 0.911 0.625 0.808 0.5705.
N-Lin 0.754 0.619 0.204 0.949 0.884 0.918 0.682 0.822 0.451 0.939 0.877 0.920 0.611 0.827 0.5666.
W-NGD-Yahoo 0.330 0.445 0.254 0.940 0.907 0.941 0.783 0.885 0.648 ?
?
?
?
?
?7.
W-NGD-Bing 0.063 0.181 0.060 0.724 0.706 0.713 0.650 0.690 0.600 0.659 0.619 0.671 0.633 0.648 0.6338.
W-NGD-GoogleWiki 0.334 0.502 0.251 0.874 0.837 0.872 0.703 0.814 0.649 ?
?
?
?
?
?9.
C-BowDA 0.693 0.782 0.466 0.971 0.947 0.969 0.836 0.928 0.772 0.974 0.932 0.968 0.742 0.896 0.74010.
C-SynDA 0.790 0.786 0.491 0.985 0.953 0.984 0.811 0.925 0.749 0.978 0.945 0.972 0.751 0.907 0.74311.
C-LSA-Tasa 0.694 0.605 0.566 0.968 0.937 0.967 0.802 0.912 0.740 0.903 0.846 0.895 0.641 0.803 0.60912.
C-NGD-Factiva 0.603 0.599 0.600 0.959 0.916 0.959 0.786 0.894 0.681 0.906 0.857 0.904 0.731 0.835 0.54313.
C-PatternWiki 0.461 0.542 0.357 0.972 0.951 0.976 0.944 0.957 0.287 0.920 0.904 0.907 0.891 0.900 0.29514.
D-WktWiki 0.759 0.754 0.521 0.943 0.905 0.946 0.750 0.876 0.679 0.922 0.887 0.918 0.725 0.854 0.65615.
D-GlossVectors 0.653 0.738 0.322 0.894 0.860 0.901 0.742 0.843 0.686 0.932 0.899 0.933 0.722 0.864 0.70916.
D-ExtenedLesk 0.792 0.718 0.409 0.937 0.866 0.939 0.711 0.843 0.657 0.952 0.873 0.943 0.655 0.832 0.654H-Mean-S8a 0.834 0.864 0.734 0.994 0.980 0.994 0.870 0.960 0.804 0.985 0.965 0.985 0.788 0.928 0.787H-MeanZscore-S8a 0.830 0.864 0.728 0.994 0.981 0.993 0.874 0.961 0.808 0.986 0.967 0.986 0.793 0.932 0.792H-MeanNnz-S8a 0.843 0.847 0.740 0.993 0.977 0.991 0.865 0.956 0.799 0.986 0.967 0.985 0.803 0.933 0.802H-Median-S10 0.821 0.842 0.647 0.995 0.976 0.992 0.843 0.950 0.779 0.975 0.934 0.970 0.724 0.892 0.721H-Max-S7 0.802 0.816 0.654 0.979 0.957 0.979 0.839 0.936 0.775 0.980 0.957 0.979 0.786 0.922 0.785H-RankFusion-S10 ?
?
?
0.994 0.978 0.993 0.864 0.956 0.798 0.976 0.929 0.971 0.745 0.896 0.744H-RelationFusion-S10 ?
?
?
0.996 0.982 0.995 0.840 0.952 0.758 0.986 0.963 0.981 0.781 0.920 0.749H-Logit-E15 0.793 0.870 0.690 0.995 0.987 0.995 0.885 0.968 0.818 0.995 0.984 0.993 0.821 0.951 0.819H-MeanNnz-E5 0.878 0.878 0.482 0.986 0.956 0.984 0.784 0.922 0.725 0.975 0.938 0.969 0.768 0.906 0.766H-MeanZscore-S8b 0.844 0.890 0.616 0.992 0.977 0.991 0.844 0.953 0.780 0.995 0.985 0.995 0.815 0.950 0.814Table 1: Performance of 16 single and 8 hybrid similarity measures on human judgements datasets (MC, RG,WordSim353) and semantic relation datasets (BLESS and SN).
The best scores in a group (single/hybrid) are inbold; the very best scores are in grey.
Correlations in italics mean p > 0.05, otherwise p ?
0.05.they only can determine relations between a lim-ited number of terms.
On the other hand, mea-sures based on web and corpora are nearly unlim-ited in their coverage, but provide less precise re-sults.
Combination of the measures enables keep-ing high precision for frequent terms (e. g., ?dis-ease?)
present in WordNet and dictionaries, andempowers calculation of relations between rareterms unlisted in the handcrafted resources (e.
g.,?bronchocele?)
with web and corpus measures.Second, combinations work well because, as itwas found in previous research (Sahlgren, 2006;Heylen et al, 2008), different measures providecomplementary types of semantic relations.
Forinstance, WordNet-based measures score higherhypernyms than associative relations; distribu-tional analysis score high co-hyponyms and syn-onyms, etc.
In that respect, a combination helpsto recall more different relations.
For example, aWordNet-based measure may return a hypernym?salmon, seafood?, while a corpus-based measurewould extract a co-hyponym ?salmon, mackerel?.Finally, the supervised combination methodworks better than unsupervised ones becauseof two reasons.
First, the measures generatescores which have quite different distributions onthe range [0; 1].
The averaging of such scoresmay be suboptimal.
Logistic Regression over-comes this issue by assigning appropriate weights(w1, .
.
.
, wk) to the measures in the linear combi-nation z.
Second, training procedure enables themodel to assign higher weights to the measureswhich provide better results, while for the meth-ods based on averaging all weight are equal.8 ConclusionIn this work, we designed and studied severalhybrid similarity measures in the context of se-mantic relation extraction.
We have undertakena systematic analysis of 16 baseline measures, 8combination methods, and 3 measure selectiontechniques.
The combined measures were thor-oughly evaluated on five ground truth datasets:MC, RG, WordSim353, BLESS, and SN.
Our re-sults have shown that the hybrid measures out-perform the single measures on all datasets.
Inparticular, a combination of 15 baseline corpus-, web-, network-, and dictionary-based measureswith Logistic Regression provided the best re-sults.
This method achieved a correlation of 0.870with human judgements and MAP(20) of 0.995and Recall(50) of 0.818 at predicting semantic re-lation between terms.This paper also sketched two novel singlesimilarity measures performing comparably withthe baselines ?
WktWiki, based on definitionsof Wikipedia and Wiktionary; and PatternWiki,based on patterns applied on Wikipedia abstracts.In the future research, we are going to apply thedeveloped methods to query expansion.17ReferencesEneko Agirre, Enrique Alfonseca, Keith Hall, JanaKravalova, Marius Pas?ca, and Aitor Soroa.
2009.A study on similarity and relatedness using distribu-tional and wordnet-based approaches.
In Proceed-ings of NAACL-HLT 2009, pages 19?27.Alan Agresti.
2002.
Categorical Data Analysis (WileySeries in Probability and Statistics).
2 edition.Alain Auger and Caroline Barrie`re.
2008.
Pattern-based approaches to semantic relation extraction: Astate-of-the-art.
Terminology Journal, 14(1):1?19.Satanjeev Banerjee and Ted Pedersen.
2003.
Ex-tended gloss overlaps as a measure of semantic re-latedness.
In IJCAI, volume 18, pages 805?810.Marco Baroni and Alexandro Lenci.
2011.
How weblessed distributional semantic evaluation.
GEMS(EMNLP), 2011, pages 1?11.Marco Baroni, Silvia Bernardini, Adriano Ferraresi,and Eros Zanchetta.
2009.
The wacky wide web:A collection of very large linguistically processedweb-crawled corpora.
LREC, 43(3):209?226.D.
Bollegala, Y. Matsuo, and M. Ishizuka.
2007.Measuring semantic similarity between words us-ing web search engines.
In WWW, volume 766.S.
Cederberg and D. Widdows.
2003.
Using LSA andnoun coordination information to improve the pre-cision and recall of automatic hyponymy extraction.In Proceedings HLT-NAACL, page 111118.Rudi L. Cilibrasi and Paul M. B. Vitanyi.
2007.
TheGoogle Similarity Distance.
IEEE Trans.
on Knowl.and Data Eng., 19(3):370?383.James R. Curran and Marc Moens.
2002.
Improve-ments in automatic thesaurus extraction.
In Pro-ceedings of the ACL-02 workshop on UnsupervisedLexical Acquisition, pages 59?66.James R. Curran.
2002.
Ensemble methods for au-tomatic thesaurus extraction.
In Proceedings of theEMNLP-02, pages 222?229.
ACL.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-tan Ruppin.
2001.
Placing search in context: Theconcept revisited.
In WWW 2001, pages 406?414.Marti A. Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In ACL, pages539?545.Kris Heylen, Yves Peirsman, Dirk Geeraerts, and DirkSpeelman.
2008.
Modelling word similarity: anevaluation of automatic synonymy extraction algo-rithms.
LREC?08, pages 3243?3249.Jay J. Jiang and David W. Conrath.
1997.
SemanticSimilarity Based on Corpus Statistics and LexicalTaxonomy.
In ROCLING X, pages 19?33.Thomas K. Landauer and Susan T. Dumais.
1997.A solution to plato?s problem: The latent semanticanalysis theory of acquisition, induction, and repre-sentation of knowledge.
Psych.
review, 104(2):211.Claudia Leacock and Martin Chodorow.
1998.
Com-bining Local Context and WordNet Similarity forWord Sense Identification.
An Electronic LexicalDatabase, pages 265?283.Dekang Lin.
1998a.
An Information-Theoretic Defi-nition of Similarity.
In ICML, pages 296?304.Dekang Lin.
1998b.
Automatic retrieval and cluster-ing of similar words.
In ACL, pages 768?774.Rada Mihalcea, Courtney Corley, and Carlo Strappa-rava.
2006.
Corpus-based and knowledge-basedmeasures of text semantic similarity.
In AAAI?06,pages 775?780.George A. Miller and Walter G. Charles.
1991.
Con-textual correlates of semantic similarity.
Languageand Cognitive Processes, 6(1):1?28.G.
A. Miller.
1995.
Wordnet: a lexical database forenglish.
Communications of ACM, 38(11):39?41.Siddharth Patwardhan and Ted Pedersen.
2006.
UsingWordNet-based context vectors to estimate the se-mantic relatedness of concepts.
Making Sense ofSense: Bringing Psycholinguistics and Computa-tional Linguistics Together, page 1.Se?bastien Paumier.
2003.
De la reconnaissance deformes linguistiques a` l?analyse syntaxique.
Ph.D.thesis, Universite?
de Marne-la-Valle?e.Ted Pedersen, Siddaharth Patwardhan, and JasonMichelizzi.
2004.
Wordnet:: Similarity: measur-ing the relatedness of concepts.
In DemonstrationPapers at HLT-NAACL 2004, pages 38?41.
ACL.Philip Resnik.
1995.
Using Information Content toEvaluate Semantic Similarity in a Taxonomy.
InIJCAI, volume 1, pages 448?453.Herbert Rubenstein and John B. Goodenough.
1965.Contextual correlates of synonymy.
Communica-tions of the ACM, 8(10):627?633.Magnus Sahlgren.
2006.
The Word-Space Model: Us-ing distributional analysis to represent syntagmaticand paradigmatic relations between words in high-dimensional vector spaces.
Ph.D. thesis.Vladislav D. Veksler, Ryan Z. Govostes, andWayne D.Gray.
2008.
Defining the dimensions of the humansemantic space.
In 30th Annual Meeting of the Cog-nitive Science Society, pages 1282?1287.Zhibiao Wu and Martha Palmer.
1994.
Verbs se-mantics and lexical selection.
In Proceedings ofACL?1994, pages 133?138.Hui Yang and Jamie Callan.
2009.
A metric-basedframework for automatic taxonomy induction.
InACL-IJCNLP, page 271279.Torsen Zesch, Christof Mu?ller, and Irina Gurevych.2008a.
Extracting lexical semantic knowledgefrom wikipedia and wiktionary.
In Proceedings ofLREC?08, pages 1646?1652.Torsen Zesch, Christof Mu?ller, and Irina Gurevych.2008b.
Using wiktionary for computing semanticrelatedness.
In Proceedings of AAAI, page 45.18
