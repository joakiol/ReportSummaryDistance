Stochastic phonological grammars and acceptabilityJohn ColemanPhonetics LaboratoryUniversity of OxfordUnited Kingdomjohn.
co leman@phon,  ox.
ac.
ukJanet PierrehumbertDepartment of Linguistics,Northwestern University, andDrpartement Signal, ENST, Parisj bp @nwu.
eduAbstractIn foundational works of generativephonology it is claimed that subjects canreliably discriminate between possible butnon-occurring words and words that couldnot be English.
In this paper we examine theuse of a pr0babilistic phonological parser forwords to model experimentally-obtainedjudgements of the acceptability of a set ofnonsense words.
We compared variousmethods oft scoring the goodness of the parseas a predictor of acceptability.
We found thatthe probability of the worst part is not thebest score of acceptability, indicating thatclassical generative phonology andOptimality Theory miss an important fact, asthese app\[oaches do not recognise amechanism by which the frequency of well-formed parts may ameliorate theunacceptability of low-frequency parts.
Weargue that probabilistic generative grammarsare demonstrably a more psychologicallyrealistic model of phonological competencethan standard generative phonology orOptimality Theory.1 IntroductionIn standard models of phonology, the phonologicalrepresentation f a word is understood to be ahierarchical structure in which the phonologicalmaterial (features and/or phonemes) is organizedinto syllables, which are in turn organized intofeet, prosodic Words and intonation phrases.
Theexistence of ~uch structure is supported by aconfluence of evidence from phonotacticconstraints, patterns of allophony, and results ofpsycholinguisti~ experiments.
In this paper, wei .
.
.
.
present a probabdlstlc phonological parser forwords, based on a context free grammar.
Unlikeclassical probabilistic context-free grammars(Suppes 1972), it attaches probabilities to entireroot-to-frontier :paths instead of to individual rules.This approach makes it possible to exploitregularities in the horizontal, or time-wise,location of frequency effects.
The grammar isapplied to model phonological productivity asrevealed in acceptability ratings of nonsensewords.
Specifically, we examine the issue ofwhether acceptability is related to expectedfrequency as computed over the whole word (withdeviations in different locations having acumulative ffect), or whether the judgments ofacceptability are dominated by the local extremevalues.
We find that an experimentally obtainedmeasure of subjective phonotactic "badness"correlates with three probabilistic measures: wordprobability, log word probability, and frequency ofthe lowest frequency (i.e.
"worst") constituent.The hierarchical structures of phonologyobviously lend themselves to being formalizedusing standard types of grammars.
Formalizationmakes it possible to rigorously relate generationand parsing.
It allows us to test particularlinguistic theories of prosody by evaluating theirpredictions over large data sets.
Previous workwhich has established these points includesChurch (1983), Randolph (1989), and Coleman(1992).Prosodic structure in some respects presents asimpler problem than syntactic strficture, becausethe inventory of different node types is small andthe grammar lacks recursion.
In terms of weakgenerative capacity, the grammar can obviously betreated as finite state.
The linguisticallytransparent prosodic grammar presented in thispaper was developed for the purpose of modelingphonological productivity.
The grammar is trainedon an existing dictionary, and it is applied tomodel judgments of well-formedness obtained fora study of the psychological reality of phonotacticconstraints (Coleman 1996).
For the study,nonsense words were constructed which eitherrespected or violated known phonotacticconstraints, and subjects indicated by pressing oneof two buttons whether or not the nonsense wordcould be a possible English word.
The totalnumber of "votes" against each word, from 6subjects on 2 runs yields a scale of 0 (good) to 12(bad).
For example, the nonsense word /smlofit/4-'1contains an extremely anomalous onset cluster,and it received 10 votes against.
In contrast, thenonsense word/'taehn/did not violate any knownphonotactic onstraints, and it received only 2votes against.We undertake to model productivity because itis a standard diagnostic for the psychologicalreality of abstractions.
Modeling in detail theperceived well-formedness of neologisms providesus with an opportunity to assess how prosodicstructure figures in the cognitive system.
Althoughearlier work has established a connection betweenlexical statistics and acceptability, no generalarchitecture for manipulating lexical statistics in astructure-sensitive fashion has yet been developed.The connection between lexical statistics andacceptability is demonstrated by a rathersubstantial iterature on lexical neighborhoods,where the "lexical neighborhood" of an existing ornonsense forms is defined by the set of wordswhich differ in a single phoneme (according to thedefinition of Luce et al 1990).
Studies by Luceand colleagues demonstrate that the lexicalneighborhood density of a word has a strong effecton word perception, which may be attributed tothe number of active competitors for a word ateach point in the speech signal.
Studies relatinglexical neighborhoods to acceptability includeOhala and Ohala (1986), who asked subjects torate forms which violated an equal number ofmorpheme structure conditions, but which differedin their distance from actual words.
The differencein ratings showed that the acceptability of a wordwas correlated with its distance from actual words,as proposed by Greenberg and Jenkins (1964), notwith the number of MSC violations.A smaller literature considers tructural factorsthrough intensive study of particularconfigurations.
In a study of medial triconsonantalclusters, such as /lfr/ in "palfrey", Pierrehumbert(1994) showed that the independent probabilitiesof the coda and the following onset was the singlebiggest factor in predicting which complexclusters exist.
Almost all of the 40 existingdifferent triconsonantal clusters are among the 200most probable if the complete cross-product of(frequency-tagged) onsets and codas is computed.Since the complete cross-product yields more than8000 different candidate medial clusters, this is avery powerful factor.
Results of an experimentdescribed in that paper showed that subjects havean implicit awareness of the statisticalunderrepresentation of consonant sequences andreveal this awareness in judgments of well-formedness.These two groups of papers leave manyunanswered questions.
Pierrehumbert (1994)provides no suggestions about how effects overthe whole word may be combined.
IfPierrehumbert's claim is extrapolated withoutelaboration, it entails that longer words shouldalmost always be worse than shorter ones.
Longerwords, having more parts, would have morefactors in their computed likelihoods, with eachfactor less than one (since the probability of anygiven choice is always less than one).
Hence thelonger the word, the more probable that itslikelihood would be at a very 10w value.
Thisdifficulty is a classic problem for stochasticparsing, and it leads to suggestions aboutnormalizing the scores.
But a scoring systemwhich completely normalized for length (e.g.
byconsidering mean log probabilities) would provideno way of capturing the effect that Pierrehumbertreports, since the mean log probabilities of thenonexistent complex clusters would be no worsethan the log probabilities of their component parts.The lexical neighborhood literature also avoidsthe question of integration over the word, byvirtue of threshholding on a single distance(obviously, a crude expedient adopted during afirst pass at the problem).
The question of howstructure figures in the perceived relatedness ofwords has also not been taken up in the lexicalneighborhood literature.
The phoneme-wisecalculation may be reasonably well-behaved ifcomputed over monosyllables, but it is too crude ameasure if the situation is considered in its fullgenerality.
For example, a single phonemesubstitution which had a drastic effect on thesyllable structure must surely yield a lesscognitively related form than one which does not.In order to advance our understanding of theseissues, we have developed a probabilistic parserwhich handles the interactions amongst thefollowing factors: 1) the phonemic ontent of theonset and of the rhyme; 2) the location withrespect to the word edge; 3) the stress patternwithin the word.
These factors cover a substantialfragment of English phonotactics.
We then parse aset of neologisms and compare various methods ofscoring the goodness of the parse as a predictor ofacceptability: 1) The overall acceptability of aform is the likelihood of the best parse.
However,because long words contain more constituentsthan short words, their likelihood is lower, as5omore multiplilcations are involved.
In order tooffset this , multiplicative effect, we alsoconsidered the following score: 2) The overallacceptability c~f a form is the log likelihood of thebest parse.
3) The overall acceptability of the formis dominated by the worst component (the singlelowest probability onset or rhyme).
Thisalternative is loosely inspired by the phonologicalliterature, from classical generative phonology toOptimality Theory, in which the badness of a formdepends on fits most egregious phonotacticconstraint violation.
4) We also examined the ideathat the overall acceptability of a form isdominated by the best constituent, in recognitionof the experimental result that nonsense wordssuch as "mrupation" are often not regarded bysubjects as being particularly bad, since despitecontaining a very un-English onset, the remainderof the word, :including its morphological andprosodic structgres, are well-formed.We find tha t of these four proposals for scoringphonotactic well-formedness, 1), 2) and 3) yieldstatistically significant correlations withexperimentally i obtained judgements.2 Grammar and ParsingFor the present paper, we consider a grammar ofEnglish words which is extremely simple butwhich still offers enough complexity to cover alarge fraction Of the English vocabulary and toraise serious issues for a stochasticimplementation.
We consider all monosyllablesand disyllables'in Mitton (1992).
Since these maydiffer in the stress of each syllable, this yields thefollowing CF rules for expanding the word nodeW into strong and weak syllables Ss and Sw:1) W ~ Ss (monosyllabic words)2) W ~ Sw Ss (iambic words, such as "about")3) W ~ Ss Sw: (trochaic words, such as "party")4) W ~ Ss Ss  (words with two stresses,such as "Rangoon")The disyllabic words in the dictionary alsoinclude quite a few compounds, which behavephonotactically:like two monosyllabic words.
Inorder to provide for such cases, the actual rootnode in the system is U ("utterance"), supportingexpansions:5) U~W6) U---~ W WSyllables have internal structure which isimportant for their phonotactics.
According toclassical treatments, uch as Fudge (1969), eachsyllable has an onset and a rhyme, yielding thefollowing rule schema:7) S--> ORSome more recent theories of the syllable do nothave onsets and rhymes as such, but distinguishthe region of the syllable up to the head vowelfrom the region consisting of the head vowel andany following tautosyllabic consonants.
Theinternal decomposition of the onset and the rhymeare highly controversial, with some theoriespositing highly articulated tree structures andothers no structure at all.
We sidestep this issue bytaking onsets and rhymes to be unanalyzed strings.We adopted this approach because a prosodicgrammar with two node levels is alreadysufficiently complex for our purposes, which is tocompare the effects of local and diffusephonotactic deviance.One might think that rules 1) - 7), augmentedby a large set of rules for spelling out theterminals, would provide a sufficient grammar todescribe English monosyllabic and disyllabicwords.
But they do not.
Difficulties arise becausethe inventories of onsets and rhymes are not thesame at all positions in the word.
Attempts toaccommodate his fact provide a mainstay of theliterature on syllabification.
The main qualitativeobservations are the following: 1) Extraconsonants are found at the end of the word whichare non-existent or rare at the end of word internalsyllables.
The coronal affixes (/s/, /t/, and /0/)provide the best known example of extraconsonants.
However, the pattern is much morepervasive, with many cases involving neitherindependent morphemes nor coronal consonants.Rhymes such as/elnp/(as in "hemp") and/~elk/asin "talc" are also more prevalent at the end of theword than in the middle.
2) Light syllables with alax full vowel are permitted only nonfinally.
3)Word-initial syllables need not have an onset,whereas word-medial syllables usually have anonset (of at least one consonant): hiatus isuncommon.Extraneous consonants at the words edges canbe generated by supplementing a grammar of type1) - 7) with rules such as 8).8) W ---r Ss CAs noted in McCarthy and Prince (1993), sucha treatment fails to capture the fact that wordedges provide a location for defective syllables inaddition to overlarge ones.
When we turn toprobabilistic models, the limitations of theapproach in 8) become even more apparent.
Theprobability distributions for all onsets and rhymesdepend on the position in the word.
For example,/t/ is possible in coda position both finally (as in"pat") and medially (as in "jit.ney").
A classicalgrammar would stop at that.
But a probabilisticgrammar must undertake to the model the fact that/t/ is much more common as a word-final codathan as a word-medial one, and that acceptabilityjudgments by native speakers reflect this fact(Pierrehumbert, 1994).
Therefore, we handledeviance at the word edges in a different manner.Stochastic grammars provide us with thepossibility of describing such effects by expanding(or rather, failing to collapse) the rules forsubordinate nodes in the tree.
Instead ofattempting to assign a probability to rule 7), whichapplies regardless of the position of the syllable inthe tree, we label the syllable nodes according totheir position in the word, and propagate thislabelling through all lower expansions.
The totalinventory of syllable types is then:9) SsiSsfSsifstrong initial syllables whichare not also finalstrong final syllables whichare not also initialstrong syllables which are bothinitial and finaland similarly for weak syllables, Swi, Swf andSwif.
For a lexicon which included longer words,it would of course also be necessary to provide formedial syllables.Propagating this type of indexing, we can thenprovide for the fact that the rhyme/emp/ is  morecommon word finally than elsewhere as follows:10) Ss f~ Osf RsfRsf ~ "emp", p = X.11)  Ss i  ~ Osi RsiRsi ~ "emp", p=Y,  whereY<X.This is, obviously, a brute force solution to theproblem.
It has the penalty that it treats asunrelated cases which are, in fact, related.
In orderto allow monosyllabic words to display both word-initial anomalies for the onset, and word-finalanomalies for the rhyme, it is necessary to positthe categories Ssif and Swif.
But then theexpansion of the Ssif rhyme becomes formallyunrelated to that of the Ssf rhyme, and that of theSsif onset is unrelated to that of the Ssi onset.
Thepractical penalty is that proliferation of logicallydifferent types under this approach reduces thecount of words which can be used in training theprobabilities for any individual case.
For the rarercases, the result can be that the sample sizes arereduced to a point at which statistically reliableestimates of the probabilities are no longeravailable from a full-size dictionary.This is a scientific problem in ~ addition to anengineering problem.
In developing robust andproductive phonotactics, speakers must have abetter ability than standard stochastic CFGsprovide to treat different contexts as analogous othat data over these contexts can be collapsedtogether.
In developing the present parser, we havemade a further assumption which allows us tocircumvent this problem.
In general, thephonological effects of edges are concentratedright at the edge in question.
This means that theeffect of the left word edge is concentrated on theonset, while the effect of the right word edge isconcentrated on the rhyme.
The tabulation ofprobabilities can then be organized according tothe vertical, root-to-frontier paths through the treewith only a highly restricted reference to thehorizontal context.
Specifically, we claim that theroot-to-frontier paths are tagged only for whetherthe frontier is at the left and/or the right edge ofthe word.
Some example paths, those of the word"candle", are:14) U U U UI I I IW W W WI I I ISsi Ssi Swf SwfI I I IOsi Rsi Owf RwfI I I Ik ~en d 1which we write for convenience U : W : Ssi : Osi :k, U : W : Ssi : Rsi : a~n, etc.Although the resulting representations areremiscent of those used in data-oriented parsing(see Bod, 1995), there is a very importantdifference.
The paths we use partition the data;each terminal string is an instance of only one path =type, with the.
result that the probabilities add upto one over all paths.
The result is that paths areproperly treated as statistically independent,modulo any empirical dependencies which wehave failed to model.
DOP posits multipledescriptions Which can subsume each other, sothat any given Syntactic fragment can contribute tomany different descriptions.
As a result, thedescriptions are not independent by the verynature of the Way they are set up.
=To use the paths in parsing new examples, wezip consistent paths together from their rootsdownwards, unifying neighbouring categories asfar down the paths as possible, an operation wecall sequential ':path unification.
The probability ofthe combined l~ath is taken to be the product of theprobabilities of the two parts.
That is, since theoriginal path sit partitioned the data, a finite statemodel is a justifiable method of combining paths.Onsets and rhymes which are unattested in theoriginal dictioiaary are assigned a nominal lowprobability by Good-Turing estimation (Good,1953) which Bod (1995) argues to be betterbehaved than alternative methods for dealing withmissing probab!lity estimates for infrequent i ems.The sequencing constraints described by theoriginal gramn~ar (for example, the requirementthat an onset be followed by a rhyme and not byanother onset) are enforced by tagging some nodesfor the type of e~lement which must succeed it, in afashion reminiscent of categorial grammar.
Thatis, onsets must ~ be followed by rhymes with thesame i/f and s/'w subscripts, and initial syllables =must be followed by final syllables, with an initialweak syllable followed by a strong syllable or aninitial strong syllable followed by a weak one.15) a) A successful instance of path unificationU U UI I IW W WI :1 ISsi/Swf ~,Ssi/Swf SwfI :1 IOsi/Rsi Rsi Owf/RwfI :1 Ik i~en dUIWISwfIRwfI1U Ul IW WI ISsi/Swf SwfOsi/Rsi Rsi Owf/Rwf RwfI I I Ik ~en d 1UIWSsi/Swf SwOsi/Rsi Rsi Owf/Rwf RwfI I I Ik ~en d 1b) An unsuccessful attempt at path unificationU U UI I IW W WI I ISsi/Swf Swf SwfI l IOsi/Rsi Owf/Rwf RwfI I Ik d 1U "IWSsi/Swf fOsi/Rsi ~ Owf/Rwf RwfI I Ik d IIn 15b), the parse fails as the initial Osi is notfollowed by an Rsi, as it requires.3 How the  t ra in ing  was  car r ied  outTo establish the path probabilities for Englishmonosyllabic and disyllabic words, the paths weretabulated over the 48,580 parsed instances of suchwords in Mitton (1992).
With each word~3containing two to four paths, there was a total of98,697 paths in the training set.Parsing such a large set of words requires one Osfto take a stand on some issues which are disputed s 234in the literature.
Here are the most important of t 206these decisions.
1) We included every single form 1 193in the dictionary, including proper nouns, no r 164matter how foreign or anomalous it might appear p 157to be, because we have the working hypothesis m 152 that low probabilities can explain the poor v 152 productivity of anomalous patterns.
2) Following f 139current phonological theory (see e.g.
Ito 1988), wed 123 syllabified all word-medial VCV sequences asV.CV.
As a related point, we took medial clusters k 123beginning with/s / to  be syllable onsets when theywere possible as word onsets.
If the sC sequence Rsfis not an attested word onset, it was split medially em 45(e.g.
'bus.boy").
elt 41There are a number of situations in which the eIts 37dictionary does not mark phonologicalinformation which we know to be important.
We et 37have done our best to work around this fact, but in es 34some cases our estimates are inevitably iz34contaminated.
Specifically: although compounds ekt 33which are hyphenated in the dictionary can be ekts 33(correctly) parsed as two phonological words,many compounds have no indication of their ent 33status and are parsed as if they were single words, eI 32Similarly, words # affixes such as -ly and -nesshave been parsed as if they had no internalstructure.
This contaminates the counts fornonfinal rhymes with a certain number of finalrhymes, and it contaminates the counts fornoninitial onsets with a certain number of word-initial onsets.
Second, stress is not marked inmonosyllabic words.
We have therefore taken allmonosyllabic words to have a main word stress.As a result, a few reduced pronunciations forfunction words are included, with the result thatthere is a small, rather than a zero, probability forstressed syllable rhymes with a schwa.
Third,secondary stresses are not reliably marked,particularly when adjacent to a primary stress (asin the word "Rangoon").
This means that a certainnumber of stressed rhymes have been tabulated asif they were unstressed.
These problems for themost part can be viewed as sources of noise.
Webelieve that the main trends of our tabulations arecorrect.
To illustrate the fact that positionalprobabilities differ, table 1 compares the 10 mostfrequent onsets and rimes in each position.Table 1.Osif Osi Owf0 836 O 1180 1 979r 616 k 848 b 934b 614 s 813 t 8841 490 p 767 s 748k 489 m 765 0 746p 459 b 725 d 708t 453 h 688 n 698s 445 t 627 r 656h 444 r 584 m 621m 430 1 567 k 601Rsif Rsi Rwf Rwii 365 ~e 950 1 740 1 815eI 147 1 819 IZ 703 ~ 742ztJ114 e694 o ~661 In 203ai107 ~654 ~z644 zn120An 95 i 584 1 514 An 87U 91 eI 558 1z420 O 69Ap89 CU537 ~S417 ZU60eIz 89 A 503 zn 398 Ik 59ctut88 ztJ472 ~226 Im 59m76 0429 zu213 zb494 NeologismsThe data set we used to evaluate the parser wasobtained in a prior study (Coleman 1996).
Thegoal of this study was to "evaluate thepsychological reality of phonotactic onstraints.The materials were designed to permit minimalcomparisons between a nonsense word which wasin principle possible and one which was expectedto be impossible by virtue of containing an onsetor a rhyme which does not occur at all in theMitton (1992) dictionary.
Thus, the materials weremade up of paired words such as /'mhsl~s/(impossible by virtue of the cluster /rnl/) and/'9hslzs/ (otherwise identical, but containing theattested cluster 191/instead of/ml/).The materials were randomized, with a post-hoctest to ensure that related items in a pair wereseparated in the presentation.
The words wererecorded by John Coleman and presented aurally,twice over, to 6 naive subjects, who judgedwhether each word could or could not be apossible English word by pressing one of tworesponse buttons.
The total number of responsesagainst the well-formedness of each word wastaken as a score of subjective degree of well-formedness.
:The distributions of scores of forms containingnon-occuring clusters and those containingoccuring clusters were significantly distinct.Forms which~ were designed to be "bad" werejudged " .
.
i  sigmfiCantly worse than forms which weredesigned to be "good".
This was the case for thepooled data, ahd for each matched pair, the "bad"variant received a lower score than "good" variantfor 61/75 "~ pmrs.
However the data contained anumber of surprises, some of which, indeed,motivated thel present study.
The scores of the"bad" forms ,were much more variable thananticipated.
"Bad" forms in some pairs (e.g./nuu'pe~J'n/) were scored better than "good" formsin other pairs (e.g.
/'splet,soM).
Apparently, asingle subpart ,of zero (observed) probability is notenough to render a form impossible.
Conversely,forms which v~iolate no constraints, but which arecomposed of 10w frequency constituents and havefew lexical r neighbors, are assigned lowacceptability s~cores e.g.
/'firjkslAp/ and /\]'o'lencS/,which scored 1,2, i.e.
completely unacceptable.These findings are contrary to the predictionsboth of a ~classical phonological treatment(according to  which linguistic competence iscategorical, and forms which cannot be parsed areimpossible) a~ well as to the predictions ofOptimality T!eory (in which a single severedeviation should determine the evaluation of theform).
Appare~)ly, the well-formed subparts of anotherwise ill-f0rmed word may alleviate the ill-formed parts, especially if their frequency is high,as in the "ation" part of "mrupation" (/nuu'pelJ'n/).We used the stochastic grammar to parse the116 mono- and di-syllabic neologisms from theearlier study, and compared various methods ofscoring the goodness of the parse as a predictor ofthe experimentally obtained measure ofacceptability.
Specifically, we compared the fouralternatives discussed in the introduction.
Of thefour proposals' for scoring phonotactic well-formedness, tfiree yield statistically significantcorrelations :with experimentally obtainedjudgzements.
(Significance was assessed via a t-teston r ,  two-tailed, d f= 114.
)Scoring method1) p(word) , p < .012) ln(p(word)) p < .0013) p(worst part) p < .01Significance of correlation4) p(best part) n.s.Scoring method 2) is a better model ofacceptability than 1) because it.- linearizes theexponential shape of p(word) arising from themultiplication of successive parts.
Figure 1 is ascatterplot of the best correlation, ln(p(word))against the number of votes against well-formedness.
It is apparent that less probable wordsare less acceptable.Figure 1..&ol Ill>oA A A ?-25 -2049-15 "10 "5In(parse probability)o 55 Discussion and ConclusionsWe have compared several methods of usingfrequency information to predict the acceptabilityof neologisms.
Both the probability of the wordand the probability of the worst part are significantcorrelates of acceptability.
This finding issignificant, because the single worst violationdominates the determination of well-formednessin almost all varsions of generative.
phonology.
InChomsky and Halle (1968), morpheme structureconditions act as a filter on underlyingrepresentations.
The same concept ofgrammatically is proposed in approaches foundedon Boolean logic, such as Declarative Phonology.According to Optimality Theory, "impossiblewords" are those in which a constraint is so strongthat a null parse is prefered to a parse in which theconstraint is violated.
This means that impossiblewords are those which are egregious according toa single constraint.However, the probability of the worst part is notthe best score of acceptability: the log probabilityof the whole word is a better measure, a result atodds with standard generative phonology and OTalike.
In classical generative phonology, a URwhich violates any single morpheme structurecondition is ruled out absolutely.
In more recentversions of generative phonology which buildprosodic structure through some version of parsingor template mapping, the entire parse fails if itfails at any single point.
The same idea shows upin a new guise in Optimality Theory.
According toOptimality Theory, constraint violations do notinteract cumulatively.
A rank-ordering ofconstraints has the consequence that weakconstraints can be violated to meet stronger ones,but there is no mechanism by which adherence tomany weak constraints ameliorates the effect of asingle violation of a stronger constraint.
Ourresults indicate that these models achieve somesuccess, but miss an important fact: the well-formedness of lexically attested parts amelioratesthe unacceptability of the unattested or low-frequency parts.
When statistically valid data onacceptability is gathered (as against the isolatedintuitions of individual researchers/authors), it isfound that deviations are partially redeemed bygood parts, and that forms which are locally well-formed, in the sense that each piece is reasonablywell-attested, can nonetheless be viewed asimprobable overall.
This finding supports the viewthat phonotactic constraints are probabilisticdescriptions of the lexicon, and that probabilisiticgenerative grammars are a more psychologicallyrealistic model of phonological competence thanstandard generative phonology and OptimalityTheory.ReferencesBod, R. 1995.
Enriching Linguistics with Statistics:Performance Models of Natural Language.
PhDdissertation, University of Amsterdam.Chomsky, N. and M. Halle 1968.
The Sound Pattern ofEnglish.
New York: Harper and Row.Church, K. W. 1983.
Phrase-Structure Parsing: AMethod for Taking Advantage of AllophonicConstraints.
PhD thesis, MIT.Coleman, J. S. 1992.
"Synthesis-by-rule" withoutsegments or rewrite-rules.
In G. Bailly, C. Benoit andT.
R. Sawallis, eds.
Talking Machines; Theories,Models, and Designs.
Amsterdam: North-Holland.43-60.Coleman, J. S. 1996.
The psychological reality oflanguage-specific constraints.
Paper presented at theFourth Phonology Meeting, University ofManchester, 16-18 May 1996.Greenberg, J. H. and J. J. Jenkins 1964.
Studies in thepsychological correlates of the sound system ofAmerican English.
Word 20, 157-177.Good, I.
1953.
The population frequencies of speciesand the estimation of population parameters.Biometrika 40, 237-264.Ito, J 1988.
Syllable theory in prosodic phonology.
NewYork: Garland.Luce, P. A., D. B. Pisoni and S. D. Goldinger 1990.Similarity neighborhoods of spoken words.
In G. T.M.
Altmann, ed.
Cognitive models of speechprocessing: psycholinguistic and computationalperspectives.
Cambridge, MA: MIT Press.
105-121.McCarthy, J. and A.
Prince 1993.
Generalizedalignment.
Yearbook of Morphology 1993.79-153.Mitton, R. 1992.
A computer-usable dictionary filebased on the Oxford Advanced Learner's Dictionaryof Current English.
ftp://ota.ox.ac.uk/pub/ota/public/dicts/710/text710.dat.Ohala, J. and M. Ohala 1986.
Testing hypothesesregarding the psychological manifestation ofmorpheme structure constraints.
In J. Ohala and J. J.Jaeger, eds.
Experimental Phonology.
239-252.Pierrehumbert, J.
1994.
Syllable structure and wordstructure: a study of triconsonantal clusters inEnglish.
In P. A. Keating, ed.
Phonological Structureand Phonetic Form: Papers in LaboratoryPhonology IlL Cambridge: Cambt;idge UniversityPress.
168-188.Randolph, M. A.
1989.
Syllable-based Constraints onProperties of English Sounds.
PhD thesis, MIT.Suppes, P. 1972.
Probabilistic grammars for naturallanguages.
In D. Davidson and G. Harman, eds.Semantics of Natural Language.
Dordrecht: D.Reidel.
741-762.AcknowledgmentsThis work was supported by NSF grant number BNS-9022484 to Northwestern University, by a fellowshipfrom the John Simon Guggenheim MemorialFoundation to Janet Pierrehumbert, and by theUniversity of Oxford.Fudge, E. C. 1969.
Syllables.
J. Ling.
5,253-286.56
