Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 398?401,Prague, June 2007. c?2007 Association for Computational LinguisticsUNIBA: JIGSAW algorithm for Word Sense DisambiguationP.
Basile and M. de Gemmis and A.L.
Gentile and P. Lops and G. SemeraroDepartment of Computer Science - University of Bari - Via E. Orabona, 4 70125 Bari ITALY{basilepp, degemmis, al.gentile, lops, semeraro}@di.uniba.itAbstractWord Sense Disambiguation (WSD) is tra-ditionally considered an AI-hard problem.A breakthrough in this field would have asignificant impact on many relevant web-based applications, such as information re-trieval and information extraction.
This pa-per describes JIGSAW, a knowledge-basedWSD system that attemps to disambiguateall words in a text by exploiting WordNet1senses.
The main assumption is that a spe-cific strategy for each Part-Of-Speech (POS)is better than a single strategy.
We evalu-ated the accuracy of JIGSAW on SemEval-2007 task 1 competition2.
This task is anapplication-driven one, where the applica-tion is a fixed cross-lingual information re-trieval system.
Participants disambiguatetext by assigning WordNet synsets, then thesystem has to do the expansion to other lan-guages, index the expanded documents andrun the retrieval for all the languages inbatch.
The retrieval results are taken as ameasure for the effectiveness of the disam-biguation.1 The JIGSAW algorithmThe goal of a WSD algorithm consists in assigninga word wioccurring in a document d with its appro-priate meaning or sense s, by exploiting the contextC in where wiis found.
The context C for wiis de-fined as a set of words that precede and follow wi.The sense s is selected from a predefined set of pos-sibilities, usually known as sense inventory.
In theproposed algorithm, the sense inventory is obtainedfrom WordNet 1.6, according to SemEval-2007 task1 instructions.
JIGSAW is a WSD algorithm basedon the idea of combining three different strategies todisambiguate nouns, verbs, adjectives and adverbs.The main motivation behind our approach is that1http://wordnet.princeton.edu/2http://www.senseval.org/the effectiveness of a WSD algorithm is stronglyinfluenced by the POS tag of the target word.
Anadaptation of Lesk dictionary-based WSD algorithmhas been used to disambiguate adjectives and ad-verbs (Banerjee and Pedersen, 2002), an adaptationof the Resnik algorithm has been used to disam-biguate nouns (Resnik, 1995), while the algorithmwe developed for disambiguating verbs exploits thenouns in the context of the verb as well as the nounsboth in the glosses and in the phrases that WordNetutilizes to describe the usage of a verb.
JIGSAWtakes as input a document d = {w1, w2, .
.
.
, wh} andreturns a list of WordNet synsets X = {s1, s2, .
.
.
,sk} in which each element siis obtained by disam-biguating the target word wibased on the informa-tion obtained from WordNet about a few immedi-ately surrounding words.
We define the context C ofthe target word to be a window of n words to the leftand another n words to the right, for a total of 2nsurrounding words.
The algorithm is based on threedifferent procedures for nouns, verbs, adverbs andadjectives, called JIGSAWnouns, JIGSAWverbs,JIGSAWothers, respectively.
More details for eachone of the above mentioned procedures follow.1.1 JIGSAWnounsThe procedure is obtained by making some varia-tions to the algorithm designed by Resnik (1995) fordisambiguating noun groups.
Given a set of nounsW = {w1, w2, .
.
.
, wn}, obtained from documentd, with each wihaving an associated sense inven-tory Si= {si1, si2, .
.
.
, sik} of possible senses, thegoal is assigning each wiwith the most appropri-ate sense sih?
Si, according to the similarity ofwiwith the other words in W (the context for wi).The idea is to define a function ?
(wi, sij), wi?
W ,sij?
Si, that computes a value in [0, 1] representingthe confidence with which word wican be assignedwith sense sij.
The intuition behind this algorithmis essentially the same exploited by Lesk (1986) andother authors: The most plausible assignment ofsenses to multiple co-occurring words is the one thatmaximizes relatedness of meanings among the cho-398sen senses.
JIGSAWnounsdiffers from the originalalgorithm by Resnik (1995) in the similarity mea-sure used to compute relatedness of two senses.
Weadopted the Leacock-Chodorow measure (Leacockand Chodorow, 1998), which is based on the lengthof the path between concepts in an IS-A hierarchy.The idea behind this measure is that similarity be-tween two synsets, s1and s2, is inversely propor-tional to their distance in the WordNet IS-A hierar-chy.
The distance is computed by finding the mostspecific subsumer (MSS) between s1and s2(eachancestor of both s1and s2in the WordNet hierar-chy is a subsumer, the MSS is the one at the lowestlevel) and counting the number of nodes in the pathbetween s1and s2that traverse their MSS.
We ex-tended this measure by introducing a parameter kthat limits the search for the MSS to k ancestors (i.e.that climbs the WordNet IS-A hierarchy until eitherit finds the MSS or k + 1 ancestors of both s1ands2have been explored).
This guarantees that ?tooabstract?
(i.e.
?less informative?)
MSSs will be ig-nored.
In addition to the semantic similarity func-tion, JIGSAWnounsdiffers from the Resnik algo-rithm in the use of:1. a Gaussian factor G, which takes into account the dis-tance between the words in the text to be disambiguated;2. a factor R, which gives more importance to the synsetsthat are more common than others, according to the fre-quency score in WordNet;3. a parametrized search for the MSS between two concepts(the search is limited to a certain number of ancestors).Algorithm 1 describes the complete procedure forthe disambiguation of nouns.
This algorithm consid-ers the words in W pairwise.
For each pair (wi,wj),the most specific subsumer MSSijis identified, byreducing the search to depth1 ancestors at most.Then, the similarity sim(wi, wj, depth2) betweenthe two words is computed, by reducing the searchfor the MSS to depth2 ancestors at most.
MSSijisconsidered as supporting evidence for those synsetssikin Siand sjhin Sjthat are descendants ofMSSij.
The MSS search is computed choosing thenearest MSS in all pairs of synsets sik,sjh.
Like-wise, the similarity for (wi,wj) is the max similaritycomputed in all pairs of sik,sjhand is weighted bya gaussian factor that takes into account the posi-tion of wiand wjin W (the shorter is the distanceAlgorithm 1 The procedure for disambiguatingnouns derived from the algorithm by Resnik1: procedure JIGSAWnouns(W, depth1, depth2) finds the proper synset for each polysemous noun in the setW = {w1, w2, .
.
.
, wn}, depth1 and depth2 are used inthe computation of MSS2: for all wi, wj?
W do3: if i < j then4: sim ?
sim(wi, wj, depth1) ?G(pos(wi), pos(wj))  G(x, y) is a Gaussianfunction which takes into account the difference betweenthe positions of wiand wj5: MSSij?
MSS(wi, wj, depth2) MSSijis the most specific subsumer between wiand wj,search for MSS restricted to depth2 ancestors6: for all sik?
Sido7: if is-ancestor(MSSij,sik) then  ifMSSijis an ancestor of sik8: supik?
supik+ sim9: end if10: end for11: for all sjh?
Sjdo12: if is-ancestor(MSSij,sjh) then13: supjh?
supjh+ sim14: end if15: end for16: normi?
normi+ sim17: normj?
normj+ sim18: end if19: end for20: for all wi?
W do21: for all sik?
Sido22: if normi> 0 then23: ?
(i, k) ?
?
?
supik/normi+ ?
?
R(k)24: else25: ?
(i, k) ?
?/|Si| + ?
?
R(k)26: end if27: end for28: end for29: end procedurebetween the words, the higher is the weight).
Thevalue ?
(i, k) assigned to each candidate synset sikfor the word wiis the sum of two elements.
Thefirst one is the proportion of support it received, outof the support possible, computed as supik/normiin Algorithm 1.
The other element that contributesto ?
(i, k) is a factor R(k) that takes into accountthe rank of sikin WordNet, i.e.
how common is thesense sikfor the word wi.
R(k) is computed as:R(k) = 1 ?
0.8 ?kn ?
1(1)where n is the cardinality of the sense inventory Sifor wi, and k is the rank of sikin Si, starting from 0.Finally, both elements are weighted by two pa-rameters: ?, which controls the contribution given399to ?
(i, k) by the normalized support, and ?, whichcontrols the contribution given by the rank of sik.We set ?
= 0.7 and ?
= 0.3.
The synset assignedto each word in W is the one with the highest ?value.
Notice that we used two different parameters,depth1 and depth2 for setting the maximum depthfor the search of the MSS: depth1 limits the searchfor the MSS computed in the similarity function,while depth2 limits the computation of the MSSused for assigning support to candidate synsets.
Weset depth1 = 6 and depth2 = 3.1.2 JIGSAWverbsBefore describing the JIGSAWverbsprocedure, thedescription of a synset must be defined.
It is thestring obtained by concatenating the gloss and thesentences that WordNet uses to explain the usageof a synset.
First, JIGSAWverbsincludes, in thecontext C for the target verb wi, all the nouns inthe window of 2n words surrounding wi.
For eachcandidate synset sikof wi, the algorithm computesnouns(i, k), that is the set of nouns in the descrip-tion for sik.maxjk= maxwl?nouns(i,k){sim(wj,wl,depth)} (2)where sim(wj,wl,depth) is defined as inJIGSAWnouns.
In other words, maxjkis thehighest similarity value for wjwrt the nouns relatedto the k-th sense for wi.
Finally, an overall simi-larity score among sikand the whole context C iscomputed:?
(i, k) = R(k) ?Pwj?CG(pos(wi), pos(wj)) ?
maxjkPhG(pos(wi), pos(wh))(3)where R(k) is defined as in Equation 1 with a differ-ent constant factor (0.9) and G(pos(wi), pos(wj)) isthe same Gaussian factor used in JIGSAWnouns,that gives a higher weight to words closer to the tar-get word.
The synset assigned to wiis the one withthe highest ?
value.
Algorithm 2 provides a detaileddescription of the procedure.1.3 JIGSAWothersThis procedure is based on the WSD algorithm pro-posed by Banerjee and Pedersen (2002).
The idea isto compare the glosses of each candidate sense forAlgorithm 2 The procedure for the disambiguationof verbs1: procedure JIGSAWverbs(wi, d, depth)  finds theproper synset of a polysemous verb wiin document d2: C ?
{w1, ..., wn}  C isthe context for wi.
For example, C = {w1, w2, w4, w5},if the sequence of words {w1, w2, w3, w4, w5} occurs in d,w3being the target verb, wjbeing nouns, j 6= 33: Si?
{si1, ...sim}  Siis the sense inventory for wi, that is the set of all candidatesynsets for wireturned by WordNet4: s ?
null  s is the synset to be returned5: score ?
?MAXDOUBLE  score is thesimilarity score assigned to s6: p ?
1  p is the position of the synsets for wi7: for all sik?
Sido8: max ?
{max1k, ..., maxnk}9: nouns(i, k) ?
{noun1, ..., nounz} nouns(i, k) is the set of all nouns in the description of sik10: sumGauss ?
011: sumTot ?
012: for all wj?
C do  computation of the similaritybetween C and sik13: maxjk?
0  maxjkis the highest similarityvalue for wj, wrt the nouns related to the k-th sense for wi.14: sumGauss ?
G(pos(wi), pos(wj)) Gaussian function which takes into account the differencebetween the positions of the nouns in d15: for all nounl?
nouns(i, k) do16: sim ?
sim(wj, nounl, depth)  sim isthe similarity between the j-th noun in C and l-th noun innouns(i, k)17: if sim > maxjkthen18: maxjk?
sim19: end if20: end for21: end for22: for all wj?
C do23: sumTot ?
sumTot+G(pos(wi), pos(wj))?maxjk24: end for25: sumTot ?
sumTot/sumGauss26: ?
(i, k) ?
R(k) ?
sumTot  R(k) is defined as inJIGSAWnouns27: if ?
(i, k) > score then28: score ?
?
(i, k)29: p ?
k30: end if31: end for32: s ?
sip33: return s34: end procedurethe target word to the glosses of all the words in itscontext.
Let Wibe the sense inventory for the tar-get word wi.
For each sik?
Wi, JIGSAWotherscomputes the string targetGlossikthat contains thewords in the gloss of sik.
Then, the procedurecomputes the string contextGlossi, which containsthe words in the glosses of all the synsets corre-400sponding to each word in the context for wi.
Fi-nally, the procedure computes the overlap betweencontextGlossiand targetGlossik, and assigns thesynset with the highest overlap score to wi.
Thisscore is computed by counting the words that occurboth in targetGlossikand in contextGlossi.
If tiesoccur, the most common synset in WordNet is cho-sen.2 ExperimentWe performed the experiment following the instruc-tions for SemEval-2007 task 1 (Agirre et al, 2007).JIGSAW is implemented in JAVA, by using JWNLlibrary3 in order to access WordNet 1.6 dictionary.We ran the experiment on a Linux-based PC withIntel Pentium D processor having a speed of 3 GHzand 2 GB of RAM.
The dataset consists of 29,681documents, including 300 topics.
Results are re-ported in Table 1.
Only two systems (PART-A andPART-B) partecipated to the competition, thus theorganizers decided to add a third system (ORGA-NIZERS) developed by themselves.
The systemswere scored according to standard IR/CLIR mea-sures as implemented in the TREC evaluation pack-age4.
Our system is labelled as PART-A.system IR documents IR topics CLIRno expansion 0.3599 0.1446full expansion 0.1610 0.1410 0.26761st sense 0.2862 0.1172 0.2637ORGANIZERS 0.2886 0.1587 0.2664PART-A 0.3030 0.1521 0.1373PART-B 0.3036 0.1482 0.1734Table 1: SemEval-2007 task 1 ResultsAll systems show similar results in IR tasks, whiletheir behaviour is extremely different on CLIR task.WSD results are reported in Table 2.
These re-sults are encouraging as regard precision, consid-ering that our system exploits only WordNet askwnoledge-base, while ORGANIZERS uses a su-pervised method that exploits SemCor to train akNN classifier.3 ConclusionsIn this paper we have presented a WSD algorithmthat exploits WordNet as knowledge-base and uses3http://sourceforge.net/projects/jwordnet4http://trec.nist.gov/system precision recall attemptedSENSEVAL-2ORGANIZERS 0.584 0.577 93.61%PART-A 0.498 0.375 75.39%PART-B 0.388 0.240 61.92%SENSEVAL-3ORGANIZERS 0.591 0.566 95.76%PART-A 0.484 0.338 69.98%PART-B 0.334 0.186 55.68%Table 2: WSD results on all-words taskthree different methods for each part-of-speech.
Thealgorithm has been evaluated by SemEval-2007 task1.
The system shows a good performance in alltasks, but low precision in CLIR evaluation.
Prob-ably, the negative result in CLIR task depends oncomplex interaction of WSD, expansion and index-ing.
Contrarily to other tasks, organizers do not planto provide a ranking of systems on SemEval-2007task 1.
As a consequence, the goal of this task - whatis the best WSD system in the context of a CLIRsystem?
- is still open.
This is why the organizersstressed in the call that this was ?a first try?.ReferencesE.
Agirre, B. Magnini, o. Lopez de Lacalle, A. Otegi,G.
Rigau, and Vossen.
2007.
Semeval-2007 task1: Evaluating wsd on cross-language information re-trieval.
In Proceedings of SemEval-2007.
Associationfor Computational Linguistics.S.
Banerjee and T. Pedersen.
2002.
An adapted leskalgorithm for word sense disambiguation using word-net.
In CICLing?02: Proc.
3rd Int?l Conf.
on Com-putational Linguistics and Intelligent Text Processing,pages 136?145, London, UK.
Springer-Verlag.C.
Leacock and M. Chodorow.
1998.
Combining localcontext and wordnet similarity for word sense identifi-cation.
In C. Fellbaum (Ed.
), WordNet: An ElectronicLexical Database, pages 305?332.
MIT Press.M.
Lesk.
1986.
Automatic sense disambiguation usingmachine readable dictionaries: how to tell a pine conefrom an ice cream cone.
In Proceedings of the 1986SIGDOC Conference, pages 20?29.
ACM Press.P.
Resnik.
1995.
Disambiguating noun groupings withrespect to WordNet senses.
In Proceedings of theThird Workshop on Very Large Corpora, pages 54?68.Association for Computational Linguistics.401
