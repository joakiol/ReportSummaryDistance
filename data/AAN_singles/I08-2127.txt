A Discriminative Approach to Japanese Abbreviation ExtractionNaoaki Okazaki?okazaki@is.s.u-tokyo.ac.jpMitsuru Ishizuka?ishizuka@i.u-tokyo.ac.jpJun?ichi Tsujii?
?tsujii@is.s.u-tokyo.ac.jp?Graduate School of InformationScience and Technology,University of Tokyo7-3-1 Hongo, Bunkyo-ku,Tokyo 113-8656, Japan?School of Computer Science,University of ManchesterNational Centre for Text Mining (NaCTeM)Manchester Interdisciplinary Biocentre,131 Princess Street, Manchester M1 7DN, UKAbstractThis paper addresses the difficulties in rec-ognizing Japanese abbreviations through theuse of previous approaches, examining ac-tual usages of parenthetical expressions innewspaper articles.
In order to bridge thegap between Japanese abbreviations andtheir full forms, we present a discrimina-tive approach to abbreviation recognition.More specifically, we formalize the abbrevi-ation recognition task as a binary classifica-tion problem in which a classifier determinesa positive (abbreviation) or negative (non-abbreviation) class, given a candidate of ab-breviation definition.
The proposed methodachieved 95.7% accuracy, 90.0% precision,and 87.6% recall on the evaluation corpuscontaining 7,887 (1,430 abbreviations and6,457 non-abbreviation) instances of paren-thetical expressions.1 IntroductionHuman languages are rich enough to be able toexpress the same meaning through different dic-tion; we may produce different sentences to conveythe same information by choosing alternative wordsor syntactic structures.
Lexical resources such asWordNet (Miller et al, 1990) enhance various NLPapplications by recognizing a set of expressions re-ferring to the same entity/concept.
For example, textretrieval systems can associate a query with alterna-tive words to find documents where the query is notobviously stated.Abbreviations are among a highly productive typeof term variants, which substitutes fully expandedterms with shortened term-forms.
Most previousstudies aimed at establishing associations betweenabbreviations and their full forms in English (Parkand Byrd, 2001; Pakhomov, 2002; Schwartz andHearst, 2003; Adar, 2004; Nadeau and Turney,2005; Chang and Schu?tze, 2006; Okazaki and Ana-niadou, 2006).
Although researchers have proposedvarious approaches to solving abbreviation recog-nition through methods such as deterministic algo-rithm, scoring function, and machine learning, thesestudies rely on the phenomenon specific to Englishabbreviations: all letters in an abbreviation appear inits full form.However, abbreviation phenomena are heavily de-pendent on languages.
For example, the term one-segment broadcasting is usually abbreviated as one-seg in Japanese; English speakers may find this pe-culiar as the term is likely to be abbreviated as 1SBor OSB in English.
We show that letters do not pro-vide useful clues for recognizing Japanese abbrevia-tions in Section 2.
Elaborating on the complexity ofthe generative processes for Japanese abbreviations,Section 3 presents a supervised learning approach toJapanese abbreviations.
We then evaluate the pro-posed method on a test corpus from newspaper arti-cles in Section 4 and conclude this paper.2 Japanese Abbreviation SurveyResearchers have proposed several approaches toabbreviation recognition for non-alphabetical lan-guages.
Hisamitsu and Niwa (2001) compared dif-ferent statistical measures (e.g., ?2 test, log like-889Table 1: Parenthetical expressions used in Japanese newspaper articleslihood ratio) to assess the co-occurrence strengthbetween the inner and outer phrases of parenthet-ical expressions X (Y).
Yamamoto (2002) utilizedthe similarity of local contexts to measure the para-phrase likelihood of two expressions based on thedistributional hypothesis (Harris, 1954).
Chang andTeng (2006) formalized the generative processes ofChinese abbreviations with a noisy channel model.Sasano et al (2007) designed rules about letter typesand occurrence frequency to collect lexical para-phrases used for coreference resolution.How are these approaches effective in recogniz-ing Japanese abbreviation definitions?
As a prelimi-nary study, we examined abbreviations described inparenthetical expressions in Japanese newspaper ar-ticles.
We used the 7,887 parenthetical expressionsthat occurred more than eight times in Japanese ar-ticles published by the Mainichi Newspapers andYomiuri Shimbun in 1998?1999.
Table 1 summa-rizes the usages of parenthetical expressions in fourgroups.
The field ?para?
indicates whether the innerand outer elements of parenthetical expressions areinterchangeable.The first group acronym (I) reduces a full form toa shorter form by removing letters.
In general, theprocess of acronym generation is easily interpreted:the left example in Table 1 consists of two Kanji let-ters taken from the heads of the two words, whilethe right example consists of the letters at the end ofthe 1st, 2nd, and 4th words in the full form.
Sinceall letters in an acronym appear in its full form, pre-vious approaches to English abbreviations are alsoapplicable to Japanese acronyms.
Unfortunately, inthis survey the number of such ?authentic?
acronymsamount to as few as 90 (1.2%).The second group acronym with translation (II) ischaracteristic of non-English languages.
Full formsare imported from foreign terms (usually in En-glish), but inherit the foreign abbreviations.
Thethird group alias (III) presents generic paraphrasesthat cannot be interpreted as abbreviations.
For ex-ample, Democratic People?s Republic of Korea isknown as its alias North Korea.
Even though theformal name does not refer to the ?northern?
part, thealias consists of Korea, and the locational modifierNorth.
Although the second and third groups retaintheir interchangeability, computers cannot recognizeabbreviations with their full forms based on letters.The last group (IV) does not introduce inter-changeable expressions, but presents additional in-formation for outer phrases.
For example, a locationusage of a parenthetical expression X (Y) describesan entity X, followed by its location Y.
Inner andouter elements of parenthetical expressions are notinterchangeable.
We regret to find that as many as81.9% of parenthetical expressions were describedfor this usage.
Thus, this study regards acronyms(with and without translation) and alias as Japanese890Table 2: Top 10 frequent parenthetical expressionsused in Japanese newspapers from 1998?1999abbreviations in a broad sense, based on their in-terchangeabilities.
In other words, the goal of thisstudy is to classify parenthetical expressions X (Y)into true abbreviations (groups I, II, III) and otherusages of parentheses (group IV).How much potential do statistical approacheshave to identify Japanese abbreviations?
Table 2shows the top 10 most frequently appearing paren-thetical expressions in this survey.
The ?class?
fieldrepresents the category1: T: acronym with transla-tion, A: alias, and O: non-abbreviation.
The mostfrequently occurring parenthetical expression wasDemocratic People?s Republic of Korea (North Ko-rea) (4,160 occurrences).
7 instances in the tablewere acronyms with translation (#2?5, #7?8), andan alias (#1), but 3 non-abbreviation instances (#6,#9, and #10) expressed nationalities of informationsources.
Even if we designed a simple methodto choose the top 10 parenthetical expressions, therecognition performance would be no greater than70% precision.3 A discriminative approach toabbreviation recognitionIn order to bridge the gap between Japanese abbre-viations and their full forms, we present a discrim-inative approach to abbreviation recognition.
Morespecifically, we formalize the abbreviation recogni-tion task as a binary classification problem in which1No acronym was included in the top 10 list.Figure 1: Paraphrase occurrence with parenthesesa classifier determines a positive (abbreviation) ornegative (non-abbreviation) class, given a parenthet-ical expression X (Y).
We model the classifier byusing Support Vector Machines (SVMs) (Vapnik,1998).
The classifier combines features that char-acterize various aspects of abbreviation definitions.Table 3 shows the features and their values for theabbreviation EU, and its full form: O-shu Rengo(European Union).
A string feature is converted intoa set of boolean features, each of which indicates?true?
or ?false?
of the value.
Due to the space limita-tion, the rest of this section elaborates on paraphraseratio and SKEW features.Paraphrase ratio Let us consider the situation inwhich an author describes an abbreviation definitionX (Y) to state a paraphrase X ?
Y in a document.The effect of the statement is to define the meaningof the abbreviation Y as X in case the reader maybe unaware/uncertain of the abbreviation Y.
For ex-ample, if an author wrote a parenthetical expression,Multi-Document Summarization (MDS), in a docu-ment, readers would recognize the meaning of theexpression MDS.
Even if they were aware of the def-inition, MDS alone would be ambiguous; it couldstand for Multi Dimensional Scaling, Missile De-fense System, etc.
Therefore, an author rarely usesthe expression Y before describing its definition.At the same time, the author would use the expres-sion Y more than X after describing the definition, ifit were to declare the abbreviation Y for X.
Figure 1illustrates this situation with two documents.
Doc-ument (a) introduces the abbreviation EU for Euro-pean Union because the expression EU occurs morefrequently than European Union after the parentheti-cal expression.
In contrast, the parenthetical expres-891Feature Type Description ExamplePR(X,Y ) numeric Paraphrase ratio 0.426SKEW(X,Y ) numeric Similarity of local contexts measured by the skew divergence 1.35freq(X) numeric Frequency of occurrence of X 2,638freq(Y ) numeric Frequency of occurrence of Y 8,326freq(X,Y ) numeric Frequency of co-occurrence of X and Y 3,121?2(X,Y ) numeric Co-occurrence strength measured by the ?2 test 2,484,521LLR(X,Y ) numeric Co-occurrence strength measured by the log-likelihood ratio 6.8match(X,Y ) boolean Predicate to test whether X contains all letters in Y 0Letter types string Pair of letter types of X and Y Kanji/AlphaFirst letter string The first letter in the abbreviation Y ELast letter string The last letter in the abbreviation Y UPOS tags string Pair of POS tags for X and Y NNP/NNPPOS categories string Pair of POS categories for X and Y NN/NNNE tags string Pair of NE tags for X and Y ORG/ORGTable 3: Features for the SVM classifier and their values for the abbreviation EU.sion in document (b) describes the property (nation-ality) of a person Beckham.Suppose that we have a document that has a par-enthetical expression with expressionsX and Y .
Weregard a document introducing an abbreviation Y forX if the document satisfies both of these conditions:1.
The expression Y appears more frequently thanthe expression X does after the definition pat-tern.2.
The expression Y does not appear before thedefinition pattern.Formula 1 assesses the paraphrase ratio of the ex-pressions X and Y,PR(X,Y ) =dpara(X,Y )d(X,Y ).
(1)In this formula, dpara(X,Y ) denotes the numberof documents satisfying the above conditions, andd(X,Y ) presents the number of documents havingthe parenthetical expression X(Y ).
The functionPR(X, Y) ranges from 0 (no abbreviation instance)to 1 (all parenthetical expressions introduce the ab-breviation).Similarity of local contexts We regard words thathave dependency relations from/to the target expres-sion as the local contexts of the expression, apply-ing all sentences to a dependency parser (Kudo andMatsumoto, 2002).
Collecting the local context ofthe target expressions, we compute the skew diver-gence (Lee, 2001), which is a weighted version ofKullback-Leibler (KL) divergence, to measure theresemblance of probability distributions P and Q:SKEW?
(P ||Q) = KL(P ||?Q+ (1?
?
)P ), (2)KL(P ||Q) =?iP (i) logP (i)Q(i).
(3)In these formulas, P is the probability distributionfunction of the words in the local context for the ex-pression X , Q is for Y , and ?
is a skew parameterset to 0.99.
The function SKEW?
(P ||Q) becomesclose to zero if the probability distributions of localcontexts for the expressions X and Y are similar.Other features In addition, we designed twelvefeatures for abbreviation recognition: five fea-tures, freq(X), freq(Y ), freq(X,Y ), ?2(X,Y ), andLLR(X,Y ) to measure the co-occurrence strengthof the expressions X and Y (Hisamitsu and Niwa,2001), match(X,Y ) feature to test whether or notall letters in an abbreviation appear in its full form,three features letter type, first letter, and last let-ter corresponding to rules about letter types in ab-breviation definitions, and three features POS tags,POS categories, and NE tags to utilize informationfrom a morphological analyzer and named-entitytagger (Kudo and Matsumoto, 2002).4 Evaluation4.1 ResultsWe built a system for Japanese abbreviation recogni-tion by using the LIBSVM implementation2 with a2http://www.csie.ntu.edu.tw/?cjlin/libsvm892Group RecallAcronym 94.4%Acronym with translation 97.4%Alias 81.4%Total 87.6%Table 4: Recall for each role of parentheseslinear kernel, which obtained the best result throughexperiments.
The performance was measured undera ten-fold cross-validation on the corpus built in thesurvey, which contains 1,430 abbreviation instancesand 6,457 non-abbreviation instances.The proposed method achieved 95.7% accuracy,90.0% precision, and 87.6% recall for recognizingJapanese abbreviations.
We cannot compare thisperformance directly with the previous work be-cause of the differences in the task design and cor-pus.
For reference, Yamamoto (2002) reported 66%precision (he did not provide the recall value) fora similar task: the acquisition of lexical paraphrasefrom Japanese newspaper articles.Table 4 reports the recall value for each groupof abbreviations.
This analysis shows the distribu-tion of abbreviations unrecognized by the proposedmethod.
Japanese acronyms, acronyms with transla-tion, and aliases were recognized at 94.4%, 97.4%,and 81.4% recall respectively.
It is interesting to seethat the proposed method could extract acronymswith translation and aliases even though we did notuse any bilingual dictionaries.4.2 Analyses for individual featuresThe numerical and boolean features are monotoneincreasing functions (decreasing for the SKEW fea-ture) as two expressions X and Y are more likelyto present an abbreviation definition.
For example,the more authors introduce a paraphrase X ?
Y,the higher the value that PR(X,Y ) feature yields.Thus, we emulate a simple classifier for each featurethat labels a candidate of abbreviation definition as apositive instance only if the feature value is higherthan a given threshold ?, e.g., PR(X,Y ) > 0.9.Figure 2 shows the precision?recall curve for eachfeature with variable thresholds.The paraphrase ratio (PR) feature outperformedother features with a wide margin: the precision andrecall values for the best F1 score were 66.2% and00.20.40.60.810  0.2  0.4  0.6  0.8  1PrecisionRecallCo-occurrence frequencyLog likelihood ratioSkew divergenceLetter matchParaphrase rateChi squareFigure 2: Precision?recall curve of each featureFeature Accuracy ReductionAll 95.7% ?- PR(X,Y ) 95.2% 0.5%- SKEW(X,Y ) 95.4% 0.3%- freq(X,Y ) 95.6% 0.1%- ?2(X,Y ) 95.6% 0.1%- LLR(X,Y ) 95.3% 0.4%- match(X,Y ) 95.5% 0.2%- Letter type 94.5% 1.2%- POS tags 95.6% 0.1%- NE tags 95.7% 0.0%Table 5: Contribution of the features48.1% respectively.
Although the performance ofthis feature alone was far inferior to the proposedmethod, to some extent Formula 1 estimated actualoccurrences of abbreviation definitions.The performance of the match (letter inclusion)feature was as low as 58.2% precision and 6.9% re-call3.
It is not surprising that the match feature hadquite a low recall, because of the ratio of ?authentic?acronyms (about 6%) in the corpus.
However, thematch feature did not gain a good precision either.Examining false cases, we found that this featurecould not discriminate cases where an outer elementcontains its inner element accidentally; e.g., TokyoDaigaku (Tokyo), which describes a university namefollowed by its location (prefecture) name.Finally, we examined the contribution of each fea-ture by eliminating a feature one by one.
If a featurewas important for recognizing abbreviations, the ab-sence of the feature would drop the accuracy.
Eachrow in Table 5 presents an eliminated feature, theaccuracy without the feature, and the reduction of3This feature drew the precision?recall locus in a steppingshape because of its discrete values (0 or 1).893the accuracy.
Unfortunately, the accuracy reductionswere so few that we could not discuss contributionsof features with statistical significance.
The lettertype feature had the largest influence (1.2%) on therecognition task, followed by the paraphrase ratio(0.5%) and log likelihood ratio (0.4%).5 ConclusionIn this paper we addressed the difficulties in rec-ognizing Japanese abbreviations by examining ac-tual usages of parenthetical expressions in news-paper articles.
We also presented the discrimina-tive approach to Japanese abbreviation recognition,which achieved 95.7% accuracy, 90.0% precision,and 87.6% recall on the evaluation corpus.
A futuredirection of this study would be to apply the pro-posed method to other non-alphabetical languages,which may have similar difficulties in modeling thegenerative process of abbreviations.
We also plan toextend this approach to the Web documents.AcknowledgmentsThis work was partially supported by Grant-in-Aidfor Scientific Research on Priority Areas (MEXT,Japan), and Solution-Oriented Research for Scienceand Technology (JST, Japan).
We used MainichiShinbun and Yomiuri Shinbun newspaper articles forthe evaluation corpus.ReferencesEytan Adar.
2004.
SaRAD: A simple and robust abbre-viation dictionary.
Bioinformatics, 20(4):527?533.Jeffrey T. Chang and Hinrich Schu?tze.
2006.
Abbre-viations in biomedical text.
In S. Ananiadou andJ.
McNaught, editors, Text Mining for Biology andBiomedicine, pages 99?119.
Artech House, Inc.Jing-Shin Chang and Wei-Lun Teng.
2006.
Miningatomic chinese abbreviation pairs: A probabilisticmodel for single character word recovery.
In Proceed-ings of the Fifth SIGHAN Workshop on Chinese Lan-guage Processing, pages 17?24, Sydney, Australia,July.
Association for Computational Linguistics.Zellig S. Harris.
1954.
Distributional structure.
Word,10:146?162.Toru Hisamitsu and Yoshiki Niwa.
2001.
Extractinguseful terms from parenthetical expression by combin-ing simple rules and statistical measures: A compara-tive evaluation of bigram statistics.
In Didier Bouri-gault, Christian Jacquemin, and Marie-C L?Homme,editors, Recent Advances in Computational Terminol-ogy, pages 209?224.
John Benjamins.Taku Kudo and Yuji Matsumoto.
2002.
Japanese de-pendency analysis using cascaded chunking.
In Pro-ceedings of the CoNLL 2002 (COLING 2002 Post-Conference Workshops), pages 63?69.Lillian Lee.
2001.
On the effectiveness of the skew di-vergence for statistical language analysis.
In ArtificialIntelligence and Statistics 2001, pages 65?72.George A. Miller, Richard Beckwith, Christiane Fell-baum, Derek Gross, and Katherine J. Miller.
1990.Introduction to wordnet: An on-line lexical database.Journal of Lexicography, 3(4):235?244.David Nadeau and Peter D. Turney.
2005.
A su-pervised learning approach to acronym identification.In 8th Canadian Conference on Artificial Intelligence(AI?2005) (LNAI 3501), pages 319?329.Naoaki Okazaki and Sophia Ananiadou.
2006.
A termrecognition approach to acronym recognition.
In Pro-ceedings of the COLING-ACL 2006 Main ConferencePoster Sessions, pages 643?650, Sydney, Australia.Serguei Pakhomov.
2002.
Semi-supervised maximumentropy based approach to acronym and abbreviationnormalization in medical texts.
In Proceedings of 40thannual meeting of ACL, pages 160?167.Youngja Park and Roy J. Byrd.
2001.
Hybrid text min-ing for finding abbreviations and their definitions.
InProceedings of the EMNLP 2001, pages 126?133.Ryohei Sasano, Daisuke Kawahara, and Sadao Kuro-hashi.
2007.
Improving coreference resolution us-ing bridging reference resolution and automaticallyacquired synonyms.
In Anaphora: Analysis, Alo-gorithms and Applications, 6th Discourse Anaphoraand Anaphor Resolution Colloquium, DAARC2007,pages 125?136.Ariel S. Schwartz and Marti A. Hearst.
2003.
A sim-ple algorithm for identifying abbreviation definitionsin biomedical text.
In Pacific Symposium on Biocom-puting (PSB 2003), number 8, pages 451?462.Vladimir N. Vapnik.
1998.
Statistical Learning Theory.John Wiley & Sons.Kazuhide Yamamoto.
2002.
Acquisition of lexical para-phrases from texts.
In 2nd International Workshopon Computational Terminology (Computerm 2002, inconjunction with COLING 2002), pages 1?7, Morris-town, NJ, USA.
Association for Computational Lin-guistics.894
