First Joint Conference on Lexical and Computational Semantics (*SEM), pages 497?501,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsDuluth : Measuring Degrees of Relational Similaritywith the Gloss Vector Measure of Semantic RelatednessTed PedersenDepartment of Computer ScienceUniversity of MinnesotaDuluth, MN 55812 USAtpederse@d.umn.eduAbstractThis paper describes the Duluth systems thatparticipated in Task 2 of SemEval?2012.These systems were unsupervised and reliedon variations of the Gloss Vector measurefound in the freely available software pack-age WordNet::Similarity.
This method wasmoderately successful for the Class-Inclusion,Similar, Contrast, and Non-Attribute cate-gories of semantic relations, but mimicked arandom baseline for the other six categories.1 IntroductionThis paper describes the Duluth systems that par-ticipated in Task 2 of SemEval?2012, Measuringthe Degree of Relational Similarity (Jurgens et al,2012).
The goal of the task was to rank sets ofword pairs according to the degree to which theyrepresented an underlying category of semantic re-lation.
A highly ranked pair would be considereda good or prototypical example of the relation.
Forexample, given the relation Y functions as an X thepair weapon:knife (X:Y) would likely be consideredmore representative of that relation than would betool:spoon.The task included word pairs from 10 differentcategories of relational similarity, each with a num-ber of subcategories.
In total the evaluation dataconsisted of 69 files, each containing a set of ap-proximately 40 word pairs.
While training exampleswere also provided, these were not used by the Du-luth systems.
The system?generated rankings werecompared with gold standard data created via Ama-zon Mechanical Turk.The Duluth systems relied on the Gloss Vec-tor measure of semantic relatedness (Patwardhanand Pedersen, 2006) as implemented in Word-Net::Similarity (Pedersen et al, 2004)1.
This quanti-fies the degree of semantic relatedness between twoword senses.
It does not, however, discover or in-dicate the nature of the relation between the words.When given two words as input (as was the case inthis task), it measures the relatedness of all possi-ble combinations of word senses associated with thispair and reports the highest resulting score.
Notethat throughout this paper we use word and wordsense somewhat interchangeably.
In general it maybe assumed that the term word or examples of wordsrefers to a word sense.A key characteristic of this task was that the wordpairs in each of the 69 sets were scored assuminga particular specified underlying semantic relation.Given this, the limitation that the Gloss Vector mea-sure does not discover the nature of relations wasless of a concern, and led to the hypothesis that aword pair that was highly related would also be aprototypical example of the underlying category ofsemantic relation.
Unfortunately the results fromthis task do not generally support this hypothesis,although for a few categories at least it appears tohave some validity.This paper continues with a review of the GlossVector measure, and explains its connections to theAdapted Lesk measure.
The paper then summarizesthe results of the three Duluth systems in this task,and concludes with some discussion and analysis ofwhere this method had both successes and failures.1wn-similarity.sourceforge.net4972 Semantic RelatednessSemantic relatedness is a more general notion thansemantic similarity.
We follow (Budanitsky andHirst, 2006) and limit semantic similarity to thosemeasures based on distances and perhaps depths ina hierarchy made up of is?a relations.
For exam-ple, car and motorcycle are similar in that they areconnected via an is?a relation with vehicle.
Seman-tic similarity is most often applied to nouns, but canalso be used with verbs.Two word senses can be related in many ways,including similarity.
car and furnace might be con-sidered related because they are both made of steel,and firefighter and hose might be considered relatedbecause one uses the other, but neither pair is likelyto be considered similar.
Measures of relatednessgenerally do not specify the nature of the relation-ship between two word senses, but rather indicatethat they are related to a certain degree in some un-specified way.
As a result, measures of relatednesstend to be symmetric, so A is related to B to the samedegree that B is related to A.
It should be noted thatsome of the relations in Task 2 were not symmetric,which was no doubt a complicating factor for theDuluth systems.3 Adapted Lesk MeasureThe Gloss Vector measure was originally devel-oped in an effort to generalize and improve uponthe Adapted Lesk measure (Banerjee and Pedersen,2003).2 Both the Gloss Vector measure and theAdapted Lesk measure start with the idea of a su-pergloss.
A supergloss is the definition (or gloss) ofa word sense that is expanded by concatenating itwith the glosses of other surrounding senses that areconnected to it via some WordNet relation.
For ex-ample, a supergloss for car might consist of the def-inition of car, the definition of car?s hypernym (e.g.,vehicle), and the definitions of the meronyms (part-of) of car (e.g., wheel, brake, bumper, etc.)
Otherrelations as detailed later in this paper may also beused to expand a supergloss.In the Adapted Lesk measure, the relatedness be-tween two word senses is a function of the numberand length of their matching overlaps in their super-glosses.
Consecutive words that match are scored2WordNet::Similarity::leskmore highly than single words, and a higher scorefor a pair of words indicates a stronger relation.
TheAdapted Lesk measure was developed to overcomethe fact that most dictionary definitions are rela-tively short, which was a concern noted by (Lesk,1986) when he introduced the idea of using defini-tion overlaps for word sense disambiguation.
Whilethe Adapted Lesk measure expands the size of thedefinitions, there are still difficulties.
In particular,the matches between words in superglosses must beexact, so morphological variants (run versus ran),synonyms (gas versus petrol), and closely relatedwords (tree versus shrub) won?t be considered over-laps and will be treated the same as words with noapparent connection (e.g., goat and vase).4 Gloss Vector MeasureThe Gloss Vector measure3 is inspired by a 2nd or-der word sense discrimination approach (Schu?tze,1998) which is in turn related to Latent SemanticIndexing or Analysis (Deerwester et al, 1990).
Thebasic idea is to replace each word in a written con-text with a vector of co-occurring words as observedin some corpus.
In this task, the contexts are def-initions (and example text) from WordNet.
A su-pergloss is formed exactly as described for AdaptedLesk, and then each word in the supergloss is re-placed by a vector of co?occurring words.
Then, allthe vectors in the supergloss are averaged together tocreate a new high dimensional representation of thatword sense.
The semantic relatedness between twoword senses is measured by taking the cosine be-tween their two averaged vectors.
The end result isthat rather than finding overlaps in definitions basedon exact matches, a word in a definition is matchedto whatever degree its co-occurrences match withthe co-occurrences of the words in the other super-gloss.
This results in a more subtle and fine grainedmeasure of relatedness than Adapted Lesk.The three Duluth systems only differ in the re-lations used to create the superglosses, otherwisethey are identical.
The corpus used to collect co-occurrence information was the complete collectionof glosses and examples from WordNet 3.0, whichconsists of about 1.46 million word tokens and al-most 118,000 glosses.
Words that appeared in a3WordNet::Similarity::vector498stop list of about 200 common words were excludedas co-occurrences, as were words that occurred lessthan 5 times or more than 50 times in the WordNetcorpus.
Two words are considered to co-occur ifthey occur in the same definition (including the ex-ample) and are adjacent to each other.
These are thedefault settings as used in WordNet::Similarity.5 Creating the Duluth SystemsThere were three Duluth systems, V0, V1, and V2.These all used the Gloss Vector measure, and differonly in how their superglosses were created.
The su-pergloss is defined using a set of relations that indi-cate which additional definitions should be includedin the definition for a sense.
All systems start witha gloss and example for each sense in a pair, whichis then augmented with definitions from additionalsenses as defined for each system.5.1 Duluth-V0V0 is identical to the default configuration of theGloss Vector measure in WordNet::Similarity.
Thisconsists of the following relations:hypernym (hype) : class that includes a member,e.g., a car is a kind of vehicle (hypernym).hyponym (hypo) : the member of a class, e.g., acar (hyponym) is a kind of vehicle.holonym (holo) : whole that includes the part,e.g., a ship (holonym) includes a mast.meronym (mero) : part included in a whole, e.g.,a mast (meronym) is a part of a ship.see also (also) : related adjectives, e.g., egocentricsee also selfish.similar to (sim) : similar adjectives, satanic issimilar to evil.is attribute of (attr) : adjective related to a noun,e.g., measurable is an attribute of magnitude.synset words (syns) : synonyms of a word, e.g.,car and auto are synonyms.4For V0 the definition and example of a nounis augmented with its synonyms and the defini-tions and examples of any hypernyms, hyponyms,meronyms, and holonyms to which it is directly con-nected.
If the word is a verb it is augmented with4Since synonyms have the same definition, this relation aug-ments the supergloss with the synonyms themselves.its synonyms and any hypernyms/troponyms and hy-ponyms to which it is directly connected.
If theword is an adjective then its definition and exam-ple are augmented with those of adjectives directlyconnected via see also, similar to, and is attribute ofrelations.5.2 Duluth-V1V1 uses the relations in V0, plus the holonyms, hy-pernyms, hyponyms, and meronyms (X) of the seealso, holonym, hypernym, hyponym, and meronymrelations (Y).
This leads to an additional 20 relationsthat bring in definitions ?2 steps?
away from theoriginal word.
These take the form of the holonymof the hypernym of the word sense, or more gener-ally the X of the Y of the word sense, where X and Yare as noted above.5.3 Duluth-V2V2 uses the relations in V0 and V1, and then addsthe holonym, hypernyms, hyponyms, and meronymsof the 20 relations added for V1.
This leads to anadditional 80 relations of the form the hypernyms ofthe meronym of the hyponym, or more generally theX of the X of the Y of the word.For example, if the word is weapon, then a hyper-nym of the meronym of the hyponym (of weapon)would add the definitions and example of bow (hy-ponym), bowstring (meronym of the hyponym), andcord (hypernym of the meronym of the hyponym) tothe gloss of weapon to create the supergloss.6 ResultsThere were two evaluation scores reported for theparticipating systems, Spearman?s Rank CorrelationCoefficient, and a score based on Maximum Differ-ence Scaling.
Since the Gloss Vector measure isbased on WordNet, there was a concern that a lackof WordNet coverage might negatively impact theresults.
However, of the 2,791 pairs used in the eval-uation, there were only 3 that contained words un-known to WordNet.6.1 Spearman?s Rank CorrelationThe ranking of word pairs in each of the 69 fileswere evaluated relative to the gold standard usingSpearman?s Rank Correlation Coefficient.
The av-erage of these results over all 10 categories of se-499Table 1: Selected Spearman?s ValuesCategory rand v0 v1 v2SIMILAR .026 .183 .206 .198CLASS-INCLUSION .057 .045 .178 .168CONTRAST -.049 .142 .120 .198average (of all 10) .018 .050 .039 .038Table 2: Selected MaxDiff ValuesCategory rand v0 v1 v2SIMILAR 31.5 37.1 39.2 37.4CLASS-INCLUSION 31.0 29.2 35.6 33.1CONTRAST 30.4 38.3 36.0 33.8NON-ATTRIBUTE 28.9 36.0 33.0 33.5average (of all 10) 31.2 32.4 31.5 31.1mantic relations was quite low.
Random guessingachieved an averaged Spearman?s value 0.018, whileDuluth-V0 scored 0.050, Duluth-V1 scored 0.039,and Duluth-V2 scored 0.038.However, there were specific categories where theDuluth systems fared somewhat better.
In particular,results for category 1 (CLASS-INCLUSION), cate-gory 3 (SIMILAR) and category 4 (CONTRAST)represent improvements on the random baseline(shown in Table 1) and at least some modest agree-ment with the gold standard.The results from the other categories were gener-ally equivalent to what would be obtained with ran-dom selection.6.2 Maximum Difference ScalingMaximum Difference Scaling is based on identify-ing the least and most prototypical pair for a givenrelation from among a set of four pairs.
A ran-dom baseline scores 31.2%, meaning that it got ap-proximately 1 in 3 of the MaxDiff questions correct.None of the Duluth systems improved upon randomto any significant degree : Duluth-V0 scored 32.4,Duluth-V1 scored 31.5, and Duluth-V2 scored 31.1.However, the same categories that did well withSpearman?s also did well with MaxDiff (see Table2).
In addition, there is some improvement in cat-egory 6 (NON-ATTRIBUTE) at least with MaxDiffscoring.7 Discussion and ConclusionsThe Gloss Vector measure was able to perform rea-sonably well in measuring the degree of relatednessfor the following four categories (where the defini-tions come from (Bejar et al, 1991)):CLASS-INCLUSION : one word names a classthat includes the entity named by the other wordSIMILAR : one word represents a different de-gree or form of the ... otherCONTRAST : one word names an opposite orincompatible of the other wordNON-ATTRIBUTE : one word names a quality,property or action that is characteristically not an at-tribute of the other wordOf these, CLASS-INCLUSION and SIMILARare well represented by the hypernym/hyponym re-lations present in WordNet and used by the GlossVector measure.
WordNet?s greatest strength liesin its hypernym tree for nouns, and that was mostlikely the basis for the success of the CLASS-INCLUSION and SIMILAR categories.
While thesuccess with CONTRAST may seem unrelated, infact it may be that pairs of opposites are often quitesimilar, for example happy and sad are both emo-tions and are similar except for their polarity.A number of the relations used in Task 2 arenot well represented in WordNet.
For example,there was a CASE RELATION which could ben-efit from information about selectional restrictionsor case frames that just isn?t available in WordNet.The same is true of the CAUSE-PURPOSE relationas there is relatively little information about casualrelations in WordNet.
While there are part-of rela-tions in WordNet (meronyms/holonyms), these didnot prove to be common enough to be a significantbenefit for the PART-WHOLE relations in the task.For many of the relations in the task the GlossVector measure was most likely relying primarily onhypernym and hyponym relations, which explainsthe bias towards categories that featured similarity-based relations.
We are however optimistic thata Gloss Vector approach could be more successfulgiven a richer set of relations from which to drawinformation for superglosses.500ReferencesS.
Banerjee and T. Pedersen.
2003.
Extended gloss over-laps as a measure of semantic relatedness.
In Proceed-ings of the Eighteenth International Joint Conferenceon Artificial Intelligence, pages 805?810, Acapulco,August.I.
Bejar, R. Chaffin, and S. Embretson.
1991.
Cogni-tive and Psychometric Analysis of Analogical ProblemSolving.
Springer?Verlag, New York, NY.A.
Budanitsky and G. Hirst.
2006.
Evaluating WordNet-based measures of semantic distance.
ComputationalLinguistics, 32(1):13?47.S.
Deerwester, S.T.
Dumais, G.W.
Furnas, T.K.
Landauer,and R. Harshman.
1990.
Indexing by Latent SemanticAnalysis.
Journal of the American Society for Infor-mation Science, 41:391?407.D.
Jurgens, S. Mohammad, P. Turney, and K. Holyoak.2012.
Semeval-2012 task 2: Measuring degrees ofrelational similarity.
In Proceedings of the 6th Inter-national Workshop on Semantic Evaluation (SemEval2012), Montreal, June.M.E.
Lesk.
1986.
Automatic sense disambiguation us-ing machine readable dictionaries: how to tell a pinecone from an ice cream cone.
In Proceedings of the5th annual international conference on Systems docu-mentation, pages 24?26.
ACM Press.S.
Patwardhan and T. Pedersen.
2006.
Using WordNet-based Context Vectors to Estimate the Semantic Relat-edness of Concepts.
In Proceedings of the EACL 2006Workshop on Making Sense of Sense: Bringing Com-putational Linguistics and Psycholinguistics Together,pages 1?8, Trento, Italy, April.T.
Pedersen, S. Patwardhan, and J. Michelizzi.
2004.Wordnet::Similarity - Measuring the relatedness ofconcepts.
In Proceedings of the Nineteenth NationalConference on Artificial Intelligence, pages 1024?1025, San Jose.H.
Schu?tze.
1998.
Automatic word sense discrimination.Computational Linguistics, 24(1):97?123.501
