Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 678?687,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsA Taxonomy, Dataset, and Classifier for Automatic Noun CompoundInterpretationStephen Tratz and Eduard HovyInformation Sciences InstituteUniversity of Southern CaliforniaMarina del Rey, CA 90292{stratz,hovy}@isi.eduAbstractThe automatic interpretation of noun-nouncompounds is an important subproblemwithin many natural language processingapplications and is an area of increasinginterest.
The problem is difficult, with dis-agreement regarding the number and na-ture of the relations, low inter-annotatoragreement, and limited annotated data.
Inthis paper, we present a novel taxonomyof relations that integrates previous rela-tions, the largest publicly-available anno-tated dataset, and a supervised classifica-tion method for automatic noun compoundinterpretation.1 IntroductionNoun compounds (e.g., ?maple leaf?)
occur veryfrequently in text, and their interpretation?determining the relationships between adjacentnouns as well as the hierarchical dependencystructure of the NP in which they occur?is animportant problem within a wide variety of nat-ural language processing (NLP) applications, in-cluding machine translation (Baldwin and Tanaka,2004) and question answering (Ahn et al, 2005).The interpretation of noun compounds is a difficultproblem for various reasons (Sp?rck Jones, 1983).Among them is the fact that no set of relations pro-posed to date has been accepted as complete andappropriate for general-purpose text.
Regardless,automatic noun compound interpretation is the fo-cus of an upcoming SEMEVAL task (Butnariu etal., 2009).Leaving aside the problem of determining thedependency structure among strings of three ormore nouns?a problem we do not address in thispaper?automatic noun compound interpretationrequires a taxonomy of noun-noun relations, anautomatic method for accurately assigning the re-lations to noun compounds, and, in the case of su-pervised classification, a sufficiently large datasetfor training.Earlier work has often suffered from using tax-onomies with coarse-grained, highly ambiguouspredicates, such as prepositions, as various labels(Lauer, 1995) and/or unimpressive inter-annotatoragreement among human judges (Kim and Bald-win, 2005).
In addition, the datasets annotated ac-cording to these various schemes have often beentoo small to provide wide coverage of the nouncompounds likely to occur in general text.In this paper, we present a large, fine-grainedtaxonomy of 43 noun compound relations, adataset annotated according to this taxonomy, anda supervised, automatic classification method fordetermining the relation between the head andmodifier words in a noun compound.
We com-pare and map our relations to those in other tax-onomies and report the promising results of aninter-annotator agreement study as well as an au-tomatic classification experiment.
We examine thevarious features used for classification and iden-tify one very useful, novel family of features.
Ourdataset is, to the best of our knowledge, the largestnoun compound dataset yet produced.
We willmake it available via http://www.isi.edu.2 Related Work2.1 TaxonomiesThe relations between the component nouns innoun compounds have been the subject of variouslinguistic studies performed throughout the years,including early work by Jespersen (1949).
Thetaxonomies they created are varied.
Lees createdan early taxonomy based primarily upon grammar(Lees, 1960).
Levi?s influential work postulatedthat complex nominals (Levi?s name for noun com-pounds that also permits certain adjectival modi-fiers) are all derived either via nominalization or678by deleting one of nine predicates (i.e., CAUSE,HAVE, MAKE, USE, BE, IN, FOR, FROM, ABOUT)from an underlying sentence construction (Levi,1978).
Of the taxonomies presented by purelylinguistic studies, our categories are most similarto those proposed by Warren (1978), whose cat-egories (e.g., MATERIAL+ARTEFACT, OBJ+PART)are generally less ambiguous than Levi?s.In contrast to studies that claim the existence ofa relatively small number of semantic relations,Downing (1977) presents a strong case for theexistence of an unbounded number of relations.While we agree with Downing?s belief that thenumber of relations is unbounded, we contend thatthe vast majority of noun compounds fits within arelatively small set of categories.The relations used in computational linguisticsvary much along the same lines as those proposedearlier by linguists.
Several lines of work (Finin,1980; Butnariu and Veale, 2008; Nakov, 2008) as-sume the existence of an unbounded number of re-lations.
Others use categories similar to Levi?s,such as Lauer?s (1995) set of prepositional para-phrases (i.e., OF, FOR, IN, ON, AT, FROM, WITH,ABOUT) to analyze noun compounds.
Some work(e.g., Barker and Szpakowicz, 1998; Nastase andSzpakowicz, 2003; Girju et al, 2005; Kim andBaldwin, 2005) use sets of categories that aresomewhat more similar to those proposed by War-ren (1978).
While most of the noun compound re-search to date is not domain specific, Rosario andHearst (2001) create and experiment with a taxon-omy tailored to biomedical text.2.2 ClassificationThe approaches used for automatic classificationare also varied.
Vanderwende (1994) presents oneof the first systems for automatic classification,which extracted information from online sourcesand used a series of rules to rank a set of mostlikely interpretations.
Lauer (1995) uses corpusstatistics to select a prepositional paraphrase.
Sev-eral lines of work, including that of Barker andSzpakowicz (1998), use memory-based methods.Kim and Baldwin (2005) and Turney (2006) usenearest neighbor approaches based upon WordNet(Fellbaum, 1998) and Turney?s Latent RelationalAnalysis, respectively.
Rosario and Hearst (2001)utilize neural networks to classify compounds ac-cording to their domain-specific relation taxon-omy.
Moldovan et al (2004) use SVMs as well asa novel algorithm (i.e., semantic scattering).
Nas-tase et al (2006) experiment with a variety of clas-sification methods including memory-based meth-ods, SVMs, and decision trees.
?
S?aghdha andCopestake (2009) use SVMs and experiment withkernel methods on a dataset labeled using a rela-tively small taxonomy.
Girju (2009) uses cross-linguistic information from parallel corpora to aidclassification.3 Taxonomy3.1 CreationGiven the heterogeneity of past work, we decidedto start fresh and build a new taxonomy of re-lations using naturally occurring noun pairs, andthen compare the result to earlier relation sets.We collected 17509 noun pairs and over a periodof 10 months assigned one or more relations toeach, gradually building and refining our taxon-omy.
More details regarding the dataset are pro-vided in Section 4.The relations we produced were then comparedto those present in other taxonomies (e.g., Levi,1978; Warren, 1978; Barker and Szpakowicz,1998; Girju et al, 2005), and they were found tobe fairly similar.
We present a detailed comparisonin Section 3.4.We tested the relation set with an initialinter-annotator agreement study (our latest inter-annotator agreement study results are presented inSection 6).
However, the mediocre results indi-cated that the categories and/or their definitionsneeded refinement.
We then embarked on a se-ries of changes, testing each generation by anno-tation using Amazon?s Mechanical Turk service, arelatively quick and inexpensive online platformwhere requesters may publish tasks for anony-mous online workers (Turkers) to perform.
Me-chanical Turk has been previously used in a va-riety of NLP research, including recent work onnoun compounds by Nakov (2008) to collect shortphrases for linking the nouns within noun com-pounds.For the Mechanical Turk annotation tests, wecreated five sets of 100 noun compounds fromnoun compounds automatically extracted from arandom subset of New York Times articles writtenbetween 1987 and 2007 (Sandhaus, 2008).
Eachof these sets was used in a separate annotationround.
For each round, a set of 100 noun com-pounds was uploaded along with category defini-679Category Name % Example Approximate MappingsCausal GroupCOMMUNICATOR OF COMMUNICATION 0.77 court order ?BGN:Agent, ?L:Acta+Producta, ?V:SubjPERFORMER OF ACT/ACTIVITY 2.07 police abuse ?BGN:Agent, ?L:Acta+Producta, ?V:SubjCREATOR/PROVIDER/CAUSE OF 2.55 ad revenue ?BGV:Cause(d-by), ?L:Cause2, ?N:EffectPurpose/Activity GroupPERFORM/ENGAGE_IN 13.24 cooking pot ?BGV:Purpose, ?L:For, ?N:Purpose, ?W:Activity?PurposeCREATE/PROVIDE/SELL 8.94 nicotine patch?BV:Purpose, ?BG:Result,?G:Make-Produce, ?GNV:Cause(s),?L:Cause1?Make1?For, ?N:Product, ?W:Activity?PurposeOBTAIN/ACCESS/SEEK 1.50 shrimp boat ?BGNV:Purpose, ?L:For, ?W:Activity?PurposeMODIFY/PROCESS/CHANGE 1.50 eye surgery ?BGNV:Purpose, ?L:For, ?W:Activity?PurposeMITIGATE/OPPOSE/DESTROY 2.34 flak jacket ?BGV:Purpose, ?L:For, ?N:Detraction, ?W:Activity?PurposeORGANIZE/SUPERVISE/AUTHORITY 4.82 ethics board ?BGNV:Purpose/Topic, ?L:For/Abouta, ?W:ActivityPROPEL 0.16 water gun ?BGNV:Purpose, ?L:For, ?W:Activity?PurposePROTECT/CONSERVE 0.25 screen saver ?BGNV:Purpose, ?L:For, ?W:Activity?PurposeTRANSPORT/TRANSFER/TRADE 1.92 freight train ?BGNV:Purpose, ?L:For, ?W:Activity?PurposeTRAVERSE/VISIT 0.11 tree traversal ?BGNV:Purpose, ?L:For, ?W:Activity?PurposeOwnership, Experience, Employment, and UsePOSSESSOR + OWNED/POSSESSED 2.11 family estate ?BGNVW:Possess*, ?L:Have2EXPERIENCER + COGINITION/MENTAL 0.45 voter concern ?BNVW:Possess*, ?G:Experiencer, ?L:Have2EMPLOYER + EMPLOYEE/VOLUNTEER 2.72 team doctor ?BGNVW:Possess*, ?L:For/Have2, ?BGN:BeneficiaryCONSUMER + CONSUMED 0.09 cat food ?BGNVW:Purpose, ?L:For, ?BGN:BeneficiaryUSER/RECIPIENT + USED/RECEIVED 1.02 voter guide ?BNVW:Purpose, ?G:Recipient, ?L:For, ?BGN:BeneficiaryOWNED/POSSESSED + POSSESSION 1.20 store owner ?G:Possession, ?L:Have1, ?W:Belonging-PossessorEXPERIENCE + EXPERIENCER 0.27 fire victim ?G:Experiencer,?L:Have1THING CONSUMED + CONSUMER 0.41 fruit fly ?W:Obj-SingleBeingTHING/MEANS USED + USER 1.96 faith healer ?BNV:Instrument, ?G:Means?Instrument, ?L:Use,?W:MotivePower-ObjTemporal GroupTIME [SPAN] + X 2.35 night work ?BNV:Time(At), ?G:Temporal, ?L:Inc, ?W:Time-ObjX + TIME [SPAN] 0.50 birth date ?G:Temporal, ?W:Obj-TimeLocation and Whole+Part/Member ofLOCATION/GEOGRAPHIC SCOPE OF X 4.99 hillside home ?BGV:Locat(ion/ive), ?L:Ina?Fromb, B:Source,?N:Location(At/From), ?W:Place-Obj?PlaceOfOriginWHOLE + PART/MEMBER OF 1.75 robot arm ?B:Possess*, ?G:Part-Whole, ?L:Have2, ?N:Part,?V:Whole-Part, ?W:Obj-Part?Group-MemberComposition and Containment GroupSUBSTANCE/MATERIAL/INGREDIENT + WHOLE 2.42 plastic bag ?BNVW:Material*,?GN:Source,?L:Froma, ?L:Have1,?L:Make2b,?N:ContentPART/MEMBER + COLLECTION/CONFIG/SERIES 1.78 truck convoy ?L:Make2ac, ?N:Whole, ?V:Part-Whole, ?W:Parts-WholeX + SPATIAL CONTAINER/LOCATION/BOUNDS 1.39 shoe box ?B:Content?Located, ?L:For, ?L:Have1, ?N:Location,?W:Obj-PlaceTopic GroupTOPIC OF COMMUNICATION/IMAGERY/INFO 8.37 travel story ?BGNV:Topic, ?L:Aboutab, ?W:SubjectMatter, ?G:DepictionTOPIC OF PLAN/DEAL/ARRANGEMENT/RULES 4.11 loan terms ?BGNV:Topic, ?L:Abouta, ?W:SubjectMatterTOPIC OF OBSERVATION/STUDY/EVALUATION 1.71 job survey ?BGNV:Topic, ?L:Abouta, ?W:SubjectMatterTOPIC OF COGNITION/EMOTION 0.58 jazz fan ?BGNV:Topic, ?L:Abouta, ?W:SubjectMatterTOPIC OF EXPERT 0.57 policy wonk ?BGNV:Topic, ?L:Abouta, ?W:SubjectMatterTOPIC OF SITUATION 1.64 oil glut ?BGNV:Topic, ?L:AboutcTOPIC OF EVENT/PROCESS 1.09 lava flow ?G:Theme, ?V:SubjAttribute GroupTOPIC/THING + ATTRIB 4.13 street name ?BNV:Possess*, ?G:Property, ?L:Have2, ?W:Obj-QualityTOPIC/THING + ATTRIB VALUE CHARAC OF 0.31 earth toneAttributive and CoreferentialCOREFERENTIAL 4.51 fighter plane ?BV:Equative, ?G:Type?IS-A, ?L:BEbcd, ?N:Type?Equality,?W:CopulaPARTIAL ATTRIBUTE TRANSFER 0.69 skeleton crew ?W:Resemblance, ?G:TypeMEASURE + WHOLE 4.37 hour meeting ?G:Measure, ?N:TimeThrough?Measure, ?W:Size-WholeOtherHIGHLY LEXICALIZED / FIXED PAIR 0.65 pig ironOTHER 1.67 contact lensTable 1: The semantic relations, their frequency in the dataset, examples, and approximate relationmappings to previous relation sets.
?-approximately equivalent; ?/?-super/sub set; ?-some overlap;?-union; initials BGLNVW refer respectively to the works of (Barker and Szpakowicz, 1998; Girju etal., 2005; Girju, 2007; Levi, 1978; Nastase and Szpakowicz, 2003; Vanderwende, 1994; Warren, 1978).680tions and examples.
Turkers were asked to selectone or, if they deemed it appropriate, two cate-gories for each noun pair.
After all annotations forthe round were completed, they were examined,and any taxonomic changes deemed appropriate(e.g., the creation, deletion, and/or modification ofcategories) were incorporated into the taxonomybefore the next set of 100 was uploaded.
The cate-gories were substantially modified during this pro-cess.
They are shown in Table 1 along with exam-ples and an approximate mapping to several othertaxonomies.3.2 Category DescriptionsOur categories are defined with sentences.
Forexample, the SUBSTANCE category has thedefinition n1 is one of the primary physi-cal substances/materials/ingredients that n2 ismade/composed out of/from.
Our LOCATION cat-egory?s definition reads n1 is the location / geo-graphic scope where n2 is at, near, from, gener-ally found, or occurs.
Defining the categories withsentences is advantageous because it is possible tocreate straightforward, explicit defintions that hu-mans can easily test examples against.3.3 Taxonomy GroupingsIn addition to influencing the category defini-tions, some taxonomy groupings were altered withthe hope that this would improve inter-annotatoragreement for cases where Turker disagreementwas systematic.
For example, LOCATION andWHOLE + PART/MEMBER OF were commonly dis-agreed upon by Turkers so they were placed withintheir own taxonomic subgroup.
The ambiguitybetween these categories has previously been ob-served by Girju (2009).Turkers also tended to disagree between thecategories related to composition and contain-ment.
Due this apparent similarity they were alsogrouped together in the taxonomy.The ATTRIBUTE categories are positioned nearthe TOPIC group because some Turkers chose aTOPIC category when an ATTRIBUTE category wasdeemed more appropriate.
This may be becauseattributes are relatively abstract concepts that areoften somewhat descriptive of whatever possessesthem.
A prime example of this is street name.3.4 Contrast with other TaxonomiesIn order to ensure completeness, we mapped intoour taxonomy the relations proposed in most pre-vious work including those of Barker and Sz-pakowicz (1998) and Girju et al (2005).
Theresults, shown in Table 1, demonstrate that ourtaxonomy is similar to several taxonomies usedin other work.
However, there are three maindifferences and several less important ones.
Thefirst major difference is the absence of a signif-icant THEME or OBJECT category.
The secondmain difference is that our taxonomy does not in-clude a PURPOSE category and, instead, has sev-eral smaller categories.
Finally, instead of pos-sessing a single TOPIC category, our taxonomy hasseveral, finer-grained TOPIC categories.
These dif-ferences are significant because THEME/OBJECT,PURPOSE, and TOPIC are typically among themost frequent categories.THEME/OBJECT is typically the category towhich other researchers assign noun compoundswhose head noun is a nominalized verb and whosemodifier noun is the THEME/OBJECT of the verb.This is typically done with the justification that therelation/predicate (the root verb of the nominaliza-tion) is overtly expressed.While including a THEME/OBJECT category hasthe advantage of simplicity, its disadvantages aresignificant.
This category leads to a significantambiguity in examples because many compoundsfitting the THEME/OBJECT category also matchsome other category as well.
Warren (1978) givesthe examples of soup pot and soup containerto illustrate this issue, and Girju (2009) notes asubstantial overlap between THEME and MAKE-PRODUCE.
Our results from Mechanical Turkshowed significant overlap between PURPOSE andOBJECT categories (present in an earlier version ofthe taxonomy).
For this reason, we do not includea separate THEME/OBJECT category.
If it is im-portant to know whether the modifier also holds aTHEME/OBJECT relationship, we suggest treatingthis as a separate classification task.The absence of a single PURPOSE categoryis another distinguishing characteristic of ourtaxonomy.
Instead, the taxonomy includes anumber of finer-grained categories (e.g., PER-FORM/ENGAGE_IN), which can be conflated tocreate a PURPOSE category if necessary.
Duringour Mechanical Turk-based refinement process,our now-defunct PURPOSE category was foundto be ambiguous with many other categories aswell as difficult to define.
This problem has beennoted by others.
For example, Warren (1978)681points out that tea in tea cup qualifies as both thecontent and the purpose of the cup.
Similarly,while WHOLE+PART/MEMBER was selected bymost Turkers for bike tire, one individual chosePURPOSE.
Our investigation identified five mainpurpose-like relations that most of our PURPOSEexamples can be divided into, including activityperformance (PERFORM/ENGAGE_IN), cre-ation/provision (CREATE/PROVIDE/CAUSE OF),obtainment/access (OBTAIN/ACCESS/SEEK),supervision/management (ORGA-NIZE/SUPERVISE/AUTHORITY), and opposition(MITIGATE/OPPOSE/DESTROY).The third major distinguishing different be-tween our taxonomy and others is the absence of asingle TOPIC/ABOUT relation.
Instead, our taxon-omy has several finer-grained categories that canbe conflated into a TOPIC category.
Unlike theprevious two distinguishing characteristics, whichwere motivated primarily by Turker annotations,this separation was largely motivated by authordissatisfaction with a single TOPIC category.Two differentiating characteristics of less im-portance are the absence of BENEFICIARY orSOURCE categories (Barker and Szpakowicz,1998; Nastase and Szpakowicz, 2003; Girju etal., 2005).
Our EMPLOYER, CONSUMER, andUSER/RECIPIENT categories combined more orless cover BENEFICIARY.
Since SOURCE is am-biguous in multiple ways including causation(tsunami injury), provision (government grant),ingredients (rice wine), and locations (northwind), we chose to exclude it.4 DatasetOur noun compound dataset was created fromtwo principal sources: an in-house collection ofterms extracted from a large corpus using part-of-speech tagging and mutual information and theWall Street Journal section of the Penn Treebank.Compounds including one or more proper nounswere ignored.
In total, the dataset contains 17509unique, out-of-context examples, making it by farthe largest hand-annotated compound noun datasetin existence that we are aware of.
Proper nounswere not included.The next largest available datasets have a vari-ety of drawbacks for noun compound interpreta-tion in general text.
Kim and Baldwin?s (2005)dataset is the second largest available dataset, butinter-annotator agreement was only 52.3%, andthe annotations had an usually lopsided distribu-tion; 42% of the data has TOPIC labels.
Most(73.23%) of Girju?s (2007) dataset consists ofnoun-preposition-noun constructions.
Rosario andHeart?s (2001) dataset is specific to the biomed-ical domain, while ?
S?aghdha and Copestake?s(2009) data is labeled with only 5 extremelycoarse-grained categories.
The remaining datasetsare too small to provide wide coverage.
See Table2 below for size comparison with other publiclyavailable, semantically annotated datasets.Size Work17509 Tratz and Hovy, 20102169 Kim and Baldwin, 20052031 Girju, 20071660 Rosario and Hearst, 20011443 ?
S?aghdha and Copestake, 2007505 Barker and Szpakowicz, 1998600 Nastase and Szpakowicz, 2003395 Vanderwende, 1994385 Lauer, 1995Table 2: Size of various available noun compounddatasets labeled with relation annotations.
Ital-ics indicate that the dataset contains n-prep-n con-structions and/or non-nouns.5 Automated ClassificationWe use a Maximum Entropy (Berger et al, 1996)classifier with a large number of boolean features,some of which are novel (e.g., the inclusion ofwords from WordNet definitions).
Maximum En-tropy classifiers have been effective on a variety ofNLP problems including preposition sense disam-biguation (Ye and Baldwin, 2007), which is some-what similar to noun compound interpretation.
Weuse the implementation provided in the MALLETmachine learning toolkit (McCallum, 2002).5.1 Features UsedWordNet-based Features?
{Synonyms, Hypernyms} for all NN and VBentries for each word?
Intersection of the words?
hypernyms?
All terms from the ?gloss?
for each word?
Intersection of the words?
?gloss?
terms?
Lexicographer file names for each word?s NNand VB entries (e.g., n1:substance)682?
Logical AND of lexicographer file namesfor the two words (e.g., n1:substance ?n2:artifact)?
Lists of all link types (e.g., meronym links)associated with each word?
Logical AND of the link types (e.g.,n1:hasMeronym(s) ?
n2:hasHolonym(s))?
Part-of-speech (POS) indicators for the exis-tence of VB, ADJ, and ADV entries for eachof the nouns?
Logical AND of the POS indicators for thetwo words?
?Lexicalized?
indicator for the existence of anentry for the compound as a single term?
Indicators if either word is a part of the otherword according to Part-Of links?
Indicators if either word is a hypernym of theother?
Indicators if either word is in the definition ofthe otherRoget?s Thesaurus-based Features?
Roget?s divisions for all noun (and verb) en-tries for each word?
Roget?s divisions shared by the two wordsSurface-level Features?
Indicators for the suffix types (e.g., de-adjectival, de-nominal [non]agentive, de-verbal [non]agentive)?
Indicators for degree, number, order, or loca-tive prefixes (e.g., ultra-, poly-, post-, andinter-, respectively)?
Indicators for whether or not a prepositionoccurs within either term (e.g., ?down?
in?breakdown?)?
The last {two, three} letters of each wordWeb 1T N-gram FeaturesTo provide information related to term usage tothe classifier, we extracted trigram and 4-gram fea-tures from the Web 1T Corpus (Brants and Franz,2006), a large collection of n-grams and theircounts created from approximately one trillionwords of Web text.
Only n-grams containing low-ercase words were used.
5-grams were not useddue to memory limitations.
Only n-grams con-taining both terms (including plural forms) wereextracted.
Table 3 describes the extracted n-gramfeatures.5.2 Cross Validation ExperimentsWe performed 10-fold cross validation on ourdataset, and, for the purpose of comparison,we also performed 5-fold cross validation on ?S?aghdha?s (2007) dataset using his folds.
Ourclassification accuracy results are 79.3% on ourdata and 63.6% on the ?
S?aghdha data.
Weused the ?2 measure to limit our experimentsto the most useful 35000 features, which is thepoint where we obtain the highest results on ?S?aghdha?s data.
The 63.6% figure is similar to thebest previously reported accuracy for this datasetof 63.1%, which was obtained by ?
S?aghdha andCopestake (2009) using kernel methods.For comparison with SVMs, we used ThorstenJoachims?
SVMmulticlass, which implements anoptimization solution to Cramer and Singer?s(2001) multiclass SVM formulation.
The best re-sults were similar, with 79.4% on our dataset and63.1% on ?
S?aghdha?s.
SVMmulticlass was, how-ever, observed to be very sensitive to the tuningof the C parameter, which determines the tradeoffbetween training error and margin width.
The bestresults for the datasets were produced with C setto 5000 and 375 respectively.Trigram Feature Extraction Patternstext <n1> <n2><*> <n1> <n2><n1> <n2> text<n1> <n2> <*><n1> text <n2><n2> text <n1><n1> <*> <n2><n2> <*> <n1>4-Gram Feature Extraction Patterns<n1> <n2> text text<n1> <n2> <*> texttext <n1> <n2> texttext text <n1> <n2>text <*> <n1> <n2><n1> text text <n2><n1> text <*> <n2><n1> <*> text <n2><n1> <*> <*> <n2><n2> text text <n1><n2> text <*> <n1><n2> <*> text <n1><n2> <*> <*> <n1>Table 3: Patterns for extracting trigram and 4-Gram features from the Web 1T Corpus for a givennoun compound (n1 n2).To assess the impact of the various features, weran the cross validation experiments for each fea-ture type, alternating between including only one683feature type and including all feature types exceptthat one.
The results for these runs using the Max-imum Entropy classifier are presented in Table 4.There are several points of interest in these re-sults.
The WordNet gloss terms had a surpris-ingly strong influence.
In fact, by themselves theyproved roughly as useful as the hypernym features,and their removal had the single strongest negativeimpact on accuracy for our dataset.
As far as weknow, this is the first time that WordNet definitionwords have been used as features for noun com-pound interpretation.
In the future, it may be valu-able to add definition words from other machine-readable dictionaries.
The influence of the Web 1Tn-gram features was somewhat mixed.
They had apositive impact on the ?
S?aghdha data, but theiraffect upon our dataset was limited and mixed,with the removal of the 4-gram features actuallyimproving performance slightly.Our Data ?
S?aghdha Data1 M-1 1 M-1WordNet-basedsynonyms 0.674 0.793 0.469 0.626hypernyms 0.753 0.787 0.539 0.626hypernyms?
0.250 0.791 0.357 0.624gloss terms 0.741 0.785 0.510 0.613gloss terms?
0.226 0.793 0.275 0.632lexfnames 0.583 0.792 0.505 0.629lexfnames?
0.480 0.790 0.440 0.629linktypes 0.328 0.793 0.365 0.631linktypes?
0.277 0.792 0.346 0.626pos 0.146 0.793 0.239 0.633pos?
0.146 0.793 0.235 0.632part-of terms 0.372 0.793 0.368 0.635lexicalized 0.132 0.793 0.213 0.637part of other 0.132 0.793 0.216 0.636gloss of other 0.133 0.793 0.214 0.635hypernym of other 0.132 0.793 0.227 0.627Roget?s Thesaurus-baseddiv info 0.679 0.789 0.471 0.629div info?
0.173 0.793 0.283 0.633Surface levelaffixes 0.200 0.793 0.274 0.637affixes?
0.201 0.792 0.272 0.635last letters 0.481 0.792 0.396 0.634prepositions 0.136 0.793 0.222 0.635Web 1T-basedtrigrams 0.571 0.790 0.437 0.6154-grams 0.558 0.797 0.442 0.604Table 4: Impact of features; cross validation ac-curacy for only one feature type and all but onefeature type experiments, denoted by 1 and M-1respectively.
?
?features shared by both n1 and n2;?
?n1 and n2 features conjoined by logical AND(e.g., n1 is a ?substance?
?
n2 is a ?artifact?
)6 Evaluation6.1 Evaluation DataTo assess the quality of our taxonomy and classi-fication method, we performed an inter-annotatoragreement study using 150 noun compounds ex-tracted from a random subset of articles takenfrom New York Times articles dating back to 1987(Sandhaus, 2008).
The terms were selected basedupon their frequency (i.e., a compound occurringtwice as often as another is twice as likely to beselected) to label for testing purposes.
Using aheuristic similar to that used by Lauer (1995), weonly extracted binary noun compounds not part ofa larger sequence.
Before reaching the 150 mark,we discarded 94 of the drawn examples becausethey were included in the training set.
Thus, ourtraining set covers roughly 38.5% of the binarynoun compound instances in recent New YorkTimes articles.6.2 AnnotatorsDue to the relatively high speed and low cost ofAmazon?s Mechanical Turk service, we chose touse Mechanical Turkers as our annotators.Using Mechanical Turk to obtain inter-annotator agreement figures has several draw-backs.
The first and most significant drawback isthat it is impossible to force each Turker to labelevery data point without putting all the terms ontoa single web page, which is highly impracticalfor a large taxonomy.
Some Turkers may labelevery compound, but most do not.
Second,while we requested that Turkers only work onour task if English was their first language, wehad no method of enforcing this.
Third, Turkerannotation quality varies considerably.6.3 Combining AnnotatorsTo overcome the shortfalls of using Turkers for aninter-annotator agreement study, we chose to re-quest ten annotations per noun compound and thencombine the annotations into a single set of selec-tions using a weighted voting scheme.
To com-bine the results, we calculated a ?quality?
score foreach Turker based upon how often he/she agreedwith the others.
This score was computed as theaverage percentage of other Turkers who agreedwith his/her annotations.
The score for each labelfor a particular compound was then computed asthe sum of the Turker quality scores of the Turkers684who annotated the compound.
Finally, the labelwith the highest rating was selected.6.4 Inter-annotator Agreement ResultsThe raw agreement scores along with Cohen?s ?
(Cohen, 1960), a measure of inter-annotator agree-ment that discounts random chance, were calcu-lated against the authors?
labeling of the data foreach Turker, the weighted-voting annotation set,and the automatic classification output.
Thesestatistics are reported in Table 5 along with theindividual Turker ?quality?
scores.
The 54 Turk-ers who made fewer than 3 annotations were ex-cluded from the calculations under the assumptionthat they were not dedicated to the task, leaving atotal of 49 Turkers.
Due to space limitations, onlyresults for Turkers who annotated 15 or more in-stances are included in Table 5.We recomputed the ?
statistics after conflatingthe category groups in two different ways.
Thefirst variation involved conflating all the TOPICcategories into a single topic category, resulting ina total of 37 categories (denoted by ?
* in Table5).
For the second variation, in addition to con-flating the TOPIC categories, we conflated the AT-TRIBUTE categories into a single category and thePURPOSE/ACTIVITY categories into a single cate-gory, for a total of 27 categories (denoted by ?
**in Table 5).6.5 Results DiscussionThe .57-.67 ?
figures achieved by the Voted an-notations compare well with previously reportedinter-annotator agreement figures for noun com-pounds using fine-grained taxonomies.
Kim andBaldwin (2005) report an agreement of 52.31%(not ?)
for their dataset using Barker and Sz-pakowicz?s (1998) 20 semantic relations.
Girjuet al (2005) report .58 ?
using a set of 35 se-mantic relations, only 21 of which were used, anda .80 ?
score using Lauer?s 8 prepositional para-phrases.
Girju (2007) reports .61 ?
agreementusing a similar set of 22 semantic relations fornoun compound annotation in which the annota-tors are shown translations of the compound in for-eign languages.
?
S?aghdha (2007) reports a .68?
for a relatively small set of relations (BE, HAVE,IN, INST, ACTOR, ABOUT) after removing com-pounds with non-specific associations or high lex-icalization.
The correlation between our automatic?quality?
scores for the Turkers who performed atId N Weight Agree ?
?
* ?
**1 23 0.45 0.70 0.67 0.67 0.742 34 0.46 0.68 0.65 0.65 0.723 35 0.34 0.63 0.60 0.61 0.614 24 0.46 0.63 0.59 0.68 0.765 16 0.58 0.63 0.59 0.59 0.54Voted 150 NA 0.59 0.57 0.61 0.676 52 0.45 0.58 0.54 0.60 0.607 38 0.35 0.55 0.52 0.54 0.568 149 0.36 0.52 0.49 0.53 0.58Auto 150 NA 0.51 0.47 0.47 0.459 88 0.38 0.48 0.45 0.49 0.5910 36 0.42 0.47 0.43 0.48 0.5211 104 0.29 0.46 0.43 0.48 0.5212 38 0.33 0.45 0.40 0.46 0.4713 66 0.31 0.42 0.39 0.39 0.4914 15 0.27 0.40 0.34 0.31 0.2915 62 0.23 0.34 0.29 0.35 0.3816 150 0.23 0.30 0.26 0.26 0.3017 19 0.24 0.26 0.21 0.17 0.1418 144 0.21 0.25 0.20 0.22 0.2219 29 0.18 0.21 0.14 0.17 0.3120 22 0.18 0.18 0.12 0.10 0.1621 51 0.19 0.18 0.13 0.20 0.2622 41 0.02 0.02 0.00 0.00 0.01Table 5: Annotation results.
Id ?
annotator id; N?
number of annotations; Weight ?
voting weight;Agree ?
raw agreement versus the author?s annota-tions; ?
?
Cohen?s ?
agreement; ?
* and ?
** ?
Co-hen?s ?
results after conflating certain categories.Voted ?
combined annotation set using weightedvoting; Auto ?
automatic classification output.least three annotations and their simple agreementwith our annotations was very strong at 0.88.The .51 automatic classification figure is re-spectable given the larger number of categories inthe taxonomy.
It is also important to rememberthat the training set covers a large portion of thetwo-word noun compound instances in recent NewYork Times articles, so substantially higher accu-racy can be expected on many texts.
Interestingly,conflating categories only improved the ?
statis-tics for the Turkers, not the automatic classifier.7 ConclusionIn this paper, we present a novel, fine-grained tax-onomy of 43 noun-noun semantic relations, thelargest annotated noun compound dataset yet cre-ated, and a supervised classification method forautomatic noun compound interpretation.We describe our taxonomy and provide map-pings to taxonomies used by others.
Our inter-annotator agreement study, which utilized non-experts, shows good inter-annotator agreement685given the difficulty of the task, indicating that ourcategory definitions are relatively straightforward.Our taxonomy provides wide coverage, with only2.32% of our dataset marked as other/lexicalizedand 2.67% of our 150 inter-annotator agreementdata marked as such by the combined Turker(Voted) annotation set.We demonstrated the effectiveness of a straight-forward, supervised classification approach tonoun compound interpretation that uses a large va-riety of boolean features.
We also examined theimportance of the different features, noting a noveland very useful set of features?the words com-prising the definitions of the individual words.8 Future WorkIn the future, we plan to focus on the interpretationof noun compounds with 3 or more nouns, a prob-lem that includes bracketing noun compounds intotheir dependency structures in addition to noun-noun semantic relation interpretation.
Further-more, we would like to build a system that canhandle longer noun phrases, including preposi-tions and possessives.We would like to experiment with including fea-tures from various other lexical resources to deter-mine their usefulness for this problem.Eventually, we would like to expand our dataset and relations to cover proper nouns as well.We are hopeful that our current dataset and re-lation definitions, which will be made availablevia http://www.isi.edu will be helpful to other re-searchers doing work regarding text semantics.AcknowledgementsStephen Tratz is supported by a National DefenseScience and Engineering Graduate Fellowship.ReferencesAhn, K., J. Bos, J. R. Curran, D. Kor, M. Nissim, andB.
Webber.
2005.
Question Answering with QEDat TREC-2005.
In Proc.
of TREC-2005.Baldwin, T. & T. Tanaka 2004.
Translation by machineof compound nominals: Getting it right.
In Proc.
ofthe ACL 2004 Workshop on Multiword Expressions:Integrating Processing.Barker, K. and S. Szpakowicz.
1998.
Semi-AutomaticRecognition of Noun Modifier Relationships.
InProc.
of the 17th International Conference on Com-putational Linguistics.Berger, A., S. A. Della Pietra, and V. J. Della Pietra.1996.
A Maximum Entropy Approach to NaturalLanguage Processing.
Computational Linguistics22:39-71.Brants, T. and A. Franz.
2006.
Web 1T 5-gram CorpusVersion 1.1.
Linguistic Data Consortium.Butnariu, C. and T. Veale.
2008.
A concept-centeredapproach to noun-compound interpretation.
In Proc.of 22nd International Conference on ComputationalLinguistics (COLING 2008).Butnariu, C., S.N.
Kim, P. Nakov, D. ?
S?aghdha, S.Szpakowicz, and T. Veale.
2009.
SemEval Task 9:The Interpretation of Noun Compounds Using Para-phrasing Verbs and Prepositions.
In Proc.
of theNAACL HLT Workshop on Semantic Evaluations:Recent Achievements and Future Directions.Cohen, J.
1960.
A coefficient of agreement for nomi-nal scales.
Educational and Psychological Measure-ment.
20:1.Crammer, K. and Y.
Singer.
On the Algorithmic Imple-mentation of Multi-class SVMs In Journal of Ma-chine Learning Research.Downing, P. 1977.
On the Creation and Use of EnglishCompound Nouns.
Language.
53:4.Fellbaum, C., editor.
1998.
WordNet: An ElectronicLexical Database.
MIT Press, Cambridge, MA.Finin, T. 1980.
The Semantic Interpretation of Com-pound Nominals.
Ph.D dissertation University ofIllinois, Urbana, Illinois.Girju, R., D. Moldovan, M. Tatu and D. Antohe.
2005.On the semantics of noun compounds.
ComputerSpeech and Language, 19.Girju, R. 2007.
Improving the interpretation of nounphrases with cross-linguistic information.
In Proc.of the 45th Annual Meeting of the Association ofComputational Linguistics (ACL 2007).Girju, R. 2009.
The Syntax and Semantics ofPrepositions in the Task of Automatic Interpreta-tion of Nominal Phrases and Compounds: a Cross-linguistic Study.
In Computational Linguistics 35(2)- Special Issue on Prepositions in Application.Jespersen, O.
1949.
A Modern English Grammar onHistorical Principles.
Ejnar Munksgaard.
Copen-hagen.Kim, S.N.
and T. Baldwin.
2007.
Interpreting NounCompounds using Bootstrapping and Sense Collo-cation.
In Proc.
of the 10th Conf.
of the Pacific As-sociation for Computational Linguistics.Kim, S.N.
and T. Baldwin.
2005.
AutomaticInterpretation of Compound Nouns using Word-Net::Similarity.
In Proc.
of 2nd International JointConf.
on Natural Language Processing.686Lauer, M. 1995.
Corpus statistics meet the compoundnoun.
In Proc.
of the 33rd Meeting of the Associa-tion for Computational Linguistics.Lees, R.B.
1960.
The Grammar of English Nominal-izations.
Indiana University.
Bloomington, IN.Levi, J.N.
1978.
The Syntax and Semantics of Com-plex Nominals.
Academic Press.
New York.McCallum, A. K. MALLET: A Machine Learning forLanguage Toolkit.
http://mallet.cs.umass.edu.
2002.Moldovan, D., A. Badulescu, M. Tatu, D. Antohe, andR.
Girju.
2004.
Models for the semantic classifi-cation of noun phrases.
In Proc.
of ComputationalLexical Semantics Workshop at HLT-NAACL 2004.Nakov, P. and M. Hearst.
2005.
Search Engine Statis-tics Beyond the n-gram: Application to Noun Com-pound Bracketing.
In Proc.
the Ninth Conference onComputational Natural Language Learning.Nakov, P. 2008.
Noun Compound InterpretationUsing Paraphrasing Verbs: Feasibility Study.
InProc.
the 13th International Conference on Artifi-cial Intelligence: Methodology, Systems, Applica-tions (AIMSA?08).Nastase V. and S. Szpakowicz.
2003.
Exploring noun-modifier semantic relations.
In Proc.
the 5th Inter-national Workshop on Computational Semantics.Nastase, V., J. S. Shirabad, M. Sokolova, and S. Sz-pakowicz 2006.
Learning noun-modifier semanticrelations with corpus-based and Wordnet-based fea-tures.
In Proc.
of the 21st National Conference onArtificial Intelligence (AAAI-06).?
S?aghdha, D. and A. Copestake.
2009.
Using lexi-cal and relational similarity to classify semantic re-lations.
In Proc.
of the 12th Conference of the Euro-pean Chapter of the Association for ComputationalLinguistics (EACL 2009).?
S?aghdha, D. 2007.
Annotating and Learning Com-pound Noun Semantics.
In Proc.
of the ACL 2007Student Research Workshop.Rosario, B. and M. Hearst.
2001.
Classifying the Se-mantic Relations in Noun Compounds via Domain-Specific Lexical Hierarchy.
In Proc.
of 2001 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP-01).Sandhaus, E. 2008.
The New York Times AnnotatedCorpus.
Linguistic Data Consortium, Philadelphia.Sp?rck Jones, K. 1983.
Compound Noun Interpreta-tion Problems.
Computer Speech Processing, eds.F.
Fallside and W A.
Woods, Prentice-Hall, NJ.Turney, P. D. 2006.
Similarity of semantic relations.Computation Linguistics, 32(3):379-416Vanderwende, L. 1994.
Algorithm for AutomaticInterpretation of Noun Sequences.
In Proc.
ofCOLING-94.Warren, B.
1978.
Semantic Patterns of Noun-NounCompounds.
Acta Universitatis Gothobugensis.Ye, P. and T. Baldwin.
2007.
MELB-YB: Prepo-sition Sense Disambiguation Using Rich SemanticFeatures.
In Proc.
of the 4th International Workshopon Semantic Evaluations (SemEval-2007).687
