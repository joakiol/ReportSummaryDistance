Proceedings of the 12th European Workshop on Natural Language Generation, pages 66?73,Athens, Greece, 30 ?
31 March 2009. c?2009 Association for Computational LinguisticsA Model for Human Readable Instruction Generation Using Level-BasedDiscourse Planning and Dynamic Inference of Attributes DisambiguationDaniel Dionne, Salvador de la Puente, Carlos Leo?n, Raquel Herva?s, Pablo Gerva?sUniversidad Complutense de MadridMadrid, Spain{dionnegonzalez,neo.salvador}@gmail.com,{cleon,raquelhb}@fdi.ucm.es,pgervas@sip.ucm.esAbstractThis paper shows a model of automatic in-struction giving for guiding human usersin virtual 3D environments.
A multilevelmodel for choosing what instruction togive in every state is presented, and soare the different modules that compose thewhole generation system.
How 3D in-formation in the virtual world is used isexplained, and the final order generationis detailed.
This model has been imple-mented as a solution for the GIVE Chal-lenge, an instruction generation challenge.1 IntroductionRecent technology advances have made it possi-ble to use handheld devices, like mobile phonesor PDAs, to guide the user by issuing commandsor descriptions about the world the user is per-ceiving in some sense (Muller, 2002).
This pos-sibility opens interesting avenues of research inthe shape of Natural Language Generation (NLG)Systems that adapt to the user in order to providehim with the most accurate expression.
However,fully operational systems applicable in real life sit-uations are difficult and expensive to implement.Under these circumstances, virtual environmentsmay be seen as an intermediate solution, suitablefor fast prototyping of experimental solutions.
Vir-tual environments permit experimenting in a re-duced, closed world, where everything that is rel-evant for the purpose at hand is explicitly repre-sented in a graphical model and under the directcontrol of the researcher.
This allows fast set up ofexperimental situations where the topography, theposition of landscape features, colour, light con-ditions and visibility factors can be modified andadapted to suit the best conditions for testing par-ticular approaches (Blue et al, 2002) or challenges(such as guidance for disabled users with differentdisabilities, for instance).
In view of these obser-vations, our research is focused on developing aninteractive virtual guide (VG), based on NLG, togive to a human user the required set of instruc-tions to complete a specific task.Such a set of instructions is called a plan.
For-mally, a plan is a sorted-in-time list of instructionsthat the user must fulfill in order to reach somegoal.
There are many planning algorithms that,with the proper world representation and a list ofgoals, can return a list like this (LaValle, 2006).The VG can take this basic plan as the actual setof instructions to convert into natural language toexplain what the user must do to complete the task.However, these instructions are usually exhaustive(step by step) and very simple because they arebased on basic world representations (and inter-pretations) and are simple enough to perform com-putational operations on them.
A VG that gener-ates this kind of simple instructions, from the pointof view of a human user, can be tedious, boringand a time wasting.
Consider the discourse ?Turnright.
Turn right.
Go ahead.
Turn left.
Pressbutton-1.
Turn around.
Go ahead.
Go ahead.
Takeitem-1.
.
.
?
as an example.
Instead, the VG shouldtake advantage of the environmental knowledge ofthe user inferring higher level instructions (less de-tailed and more human-like) from the basic plan(something more along the lines of ?Go press thebuton in the far wall, come back and take item-1?
).The difference is shown graphically for a simpleexample in Figure 1.There are several aspects to be considered inachieving this goal.
First, a human guide wouldphrase his or her instructions at different levels ofabstraction, to optimise the communicative effectof his/her utterances in terms of striking balancebetween sufficient informative content and econ-omy of expression.
Second, a human guide mayoperate in a reactive manner, providing additionalfeedback whenever the user requests help.
But667 instructions 1 instructionFigure 1: A comparison of a step by step planversus a human readable plan like ?Walk out thedoor?.
Note the difference in the number of in-structions given.human guides are also likely to observe the per-son that is being guided, and be ready to interveneproactively if they notice the user seems lost or atrisk.
These two points are elaborated below.In order to build more human levels, a VG mustconsider the virtual environment in a manner asclose as possible to the way a human being sensesthe real world.
To model the different levels of ab-straction employed by human guides, a good solu-tion may be to model the world as a hierarchy ofspatial levels.
People tend to limit the area wherethey do certain activities by some kind of logicalborders.
Sometimes, these borders match physi-cal borders such as the walls that define a roomor a corridor, the outside perimeter of a building,the limits of a park, or a city boundary.
In othercases, such as outdoor settings, borders can bemore abstract, such as the line of horizon in all di-rections from the observer?s current position.
Theareas defined by these borders may be containedinside one other, resulting in a tree-like structurefrom the smallest spaces to greater areas, i.e.
fromthe room where the user is standing to the city helives in.
Of course, the areas are connected in amultigraph way where each edge is a connectionlike a door or a natural transition.
To build a us-able model of this type of cognitive representationof the world is far from trivial.
We will describehow we faced this point in Section 3.1 (Construct-ing the World).
Considering such a hierarchicalview of the environment when generating instruc-tions, results in more natural and human-friendlyresults.
Instructing someone to ?exit the room?works better than asking them to ?advance untilpassing through the door?
; ?leave the buildingusing the main entrance?
is better than a set ofinstructions refering to more specific spaces like?exit this room, now go down the stairs, now go tothe elevator?
and so on.
We return to this matterin Section 3.2 (Planning the Discourse).The issue of abstractions in world modellingalso affects a different language generation task:referring expression generation.
In providing in-structions, human guides often refer to abstractconcepts such as corners or ?the middle of theroom?.
These are not usually represented explic-itly in your run of the mill world representation,which usually prevents NLG systems from em-ploying them as means of optimising references.In Section 3.4 (Hidden Reference Discovery), wewill see how, besides visible information, a naturalapproach based on the inference of other ?hidden?elements or references that can be extracted fromthe environment helps to reduce the length of theexplanation needed, and to build better references.These elements are hidden because they are notvisible or trivial, and they require a specific studyand calculation.The second point to consider is reactive versusproactive guidance.
A reactive guidance systemmay rely on feedback from the user to decide whento intervene.
Consider the following two represen-tative examples: the user can say ?I did not un-dertand last instruction?
and the VG system cananswer by repeating the instruction or building anew one phrased in a different way but with thesame meaning; or the user can say ?I am lost?and the VG will ask the planning software to re-calculate the plan considering the new user?s sit-uation.
However, there are situations where theuser may not realize that he is lost or that he isabout to perform a dangerous action (like walkingon a slippery surface, pressing an incorrect button,going in the wrong direction or crossing a streetwhen the traffic light is red).
A good guide willwarn the user before he does something wrong butit should not oppress the user each time he decidesto explore another route to reach the goal.
In otherwords, the VG must watch the user actions andtake part when he is on the verge of commitinga serious mistake.
We will discuss about how towarn the user in Section 3.3 (Warning the User).2 Previous WorkMany NLG systems have considered generationof instructions in the past.
A good review is pro-vided in (Bourne, 1999).
However, most existinginstruction generating system focused on perform-67ing different types of static actions (actions that donot involve changes of location of the user).
Thepresent work is focused on the task of guiding theuser through virtual environments.The GIVE (Generating Instructions in VirtualEnvironments) Challenge (Byron et al, 2007) op-erates on a scenario where a user has to solve aparticular task in a simulated 3D space.
A gen-eration module has to guide the human user usingnatural language instructions.
A software architec-ture is provided that allows the generation moduleto abstract away from the rest of the system, whilehaving access to world information from the 3Denvironment, user feedback from the client mod-ule, and plans generated by an off-the-shelf plan-ner.
The work presented in this paper arose fromthe author?s participation in the GIVE Challenge,and relies on the software architecture providedfor the challenge to implement all details of thesystem other than the NLG module.A fundamental task to be solved for correct in-struction generation is the construction of appro-priate referring expressions.
This task has beenthe object of many research efforts in the recentpast.
To construct a reference to a particular en-tity, the algorithm takes as input a symbol corre-sponding to the intended referent and a list of sym-bols corresponding to other entities in focus basedthe intended referent, known as the contrast set.The algorithm returns a list of attribute-value pairsthat correspond to the semantic content of the re-ferring expression to be realized.
The algorithmoperates by iterating over the list of available at-tributes, looking for one that is known to the userand rules out the largest number of elements of thecontrast set that have not already been ruled out.Referring Expression Generation in physicallysituated environments has been studied in (Kelle-her and Kruijff, 2005).
The goal of this work is todevelop embodied conversational robots that arecapable of natural, fluent visually situated dialogwith one or more interlocutors.
In this kind ofsituation a very important aspect to take into ac-count is how to refer to objects located in the phys-ical environment.
The authors present in the papera computational framework for the generation ofspatial locative expressions in such contexts, rely-ing on the Reiter and Dale (Reiter and Dale, 1992)algorithm.Another interesting work related to referring ex-pression generation in spatial environments can befound in (Varges, 2005).
The author uses the mapsof the Map Task dialogue corpus as domain mod-els, and treats spatial descriptions as referring ex-pressions that distinguish particular points on themap from all other points (considered as distrac-tors).Related research can be found in (Stoia et al,2006), where a study of how humans give orders innavigation environmnets and an algorithm imple-menting the observed behaviour is shown.
Thereare many other approaches to instruction giving.Directly related with this work, it is worth men-tioning CORAL (Dale and Geldof, 2003), whichshows a full architecture for instruction giving,and REAL (Muller, 2002), which shows a multi-modal system (graphics and text) for communicat-ing with the user, adapting them to user behaviour.3 A Functional Model of a Virtual GuideThe model of a virtual guide presented here ad-dresses four specific issues: how to construct arepresentation of the world with higher levels ofrepresentation, how to generate higher instructionsreferring to the more abstract levels of represen-tation, how the construction of references is im-plemented in terms of reference agents.
A briefoverview of the complete architecture of the mod-ule is also included.3.1 Constructing the WorldIn GIVE, the world is discretized as a set of tiles.These tiles are the minimum portions of space andthe user can move around from tile to tile.
Orienta-tions are discretized: the user can only face North,East, South or West.
By default, the world consistsof an infinite area of adjacent and accesible tiles.World representation assertations may state thereis a wall between two adjacent tiles, blocking ac-cess from one to other.
A 3D representation of thisbasic world gives the user an illusion of rooms but,from the point of view of the VG there is no datastructure that reflects a hierarchy of rooms.
Thisrepresentation does not fit very well with the hu-man sense of space, so a more abstract one had tobe built to provide the abstract referents (rooms,corners, intersections, doors...) which we wantedour guide to use.The first problem we had was defining a room.In architecture, a definition of room is ?any dis-tinguishable space within a structure?, but distin-guishable is too vague to be of use.
Figure 2 illus-68A) One big roomB) Three smaller roomsFigure 2: Defining a distinguishable space.trates the problem of defining when two spaces aredistiguishable.
Notice the only difference betwenA and B is the width of the gaps in relation to thesize of the rooms.
This problem has been exten-sively studied in robotics.
An interesting exam-ple (Galindo et al, 2005) consists on identifyinginterconected ?open spaces?
in order to obtain anadjacency graph.
From that graph, another graphcan be calculated, grouping spaces to form rooms,corridors, etc.For practical purposes, we have decided to con-sider that two spaces are distinguishable when theuser has to go through a door to get from one to theother, with a door being a one-tile gap in a wall.Based on this definition, we have developed analgorithm to group adjacent tiles into rooms.
Theidea is to follow a wall around the room until thestarting point is reached, thereby establishing theperimeter of the room, then establish the set oftiles corresponding to the room using a floodfillalgorithm.
Breaks in walls are handled by check-ing whether they are small enough to be consid-ered doors into other rooms or not.
If they aredoors, they are noted as entrances to other rooms(which are stored in a room list for subsequent pro-cessing).
If they are not, the wall beyond the gapis followed as part of the boundary of the currentroom.
A small practical example of the algorithmin operation is shown in Figure 3.Adjoining rooms stored in the room list arerecursively processed.
Each new room discoveredis connected to its adjacent rooms to obtain a highlevel map of the available space.
An analyzer isapplied to each room to establish its type (room,hall, corridor, etc) and additional properties suchas size or shape.
This new world representationA) First, nd anywall B) Not a door,the gap is too bigC) Was not a door,so go back.
D) Small gap (door),so add it to DC.Figure 3: Looking for rooms.GOALA5A3H1 H2 H3 H5H4A5Time owH7H6Figure 4: Tree representation of the plan at severallevels.allows the VG to refer to doors and rooms.3.2 Planning the DiscourseDiscourse planning must take place at two differ-ent levels of detail.
The VG must plan the dis-course corresponding to the whole set of instruc-tions to be imparted until the final goal is reached.But it also needs to plan how much of that is tobe communicated to the user in the next turn ofthe dialogue.
We solve the first issue by build-ing a multi-level representation of the expecteddiscourse for the whole of the plan to be carriedout by the user.
This representation is structuredlike a tree, with the set of low-level instructions asleafs, and subsequent nodes of the tree represent-ing higher level instructions that group togetherthe lower level instructions represented by theirsubtrees.
The solution to the second issue is de-scribed below.We define action as anything the user can do69Line of sightCheckpoint User?srouteFigure 5: An n-shaped room does not let the usersee the exit of the room so VG can guide the userfrom checkpoint to checkpoint.that modifies the state of the world and instruc-tion as an action that the user should perform inorder to advance in the plan.
Instructions are de-fined in terms of preconditions and postconditions.Preconditions are conditions that must be satis-fied for the instruction to be performed, and post-conditions are the conditions that must be satisfiedto consider the instruction done.
The instructiontree representation of the plan is built by group-ing together sets of low-level instructions into asingle high-level instruction.
For instance, wegroup all tile-by-tile steps inside the same roomto build a new instruction such as ?go from room1to room2?.
We do not discard any low-level in-struction, we just group them under the new high-level instruction, building a tree that represents theplan at different levels of abstraction (see Figure4).
This allows the user to fall back on low-levelinstructions at need (if, for instance, the light goesout and the VG has to guide him step by step).An additional abstraction has been introducedto account for the tendency of humans to breakthe description of a complex path (where not allof the path is visible at the start) into segmentsmade of the portions of the path that are visible ateach particular point (see Figure 5).
The conceptof checkpoint is introduced for the end of each ofthese segments.We have defined five types of high-level in-structions: MovementInstruction (guides theuser from tile to tile), CheckPointInstruction(guides the user from a his current position toa checkpoint), Room2RoomInstruction (guidesthe user from room to room), ActionInstruc-tion (tells the user to interact with some ele-ment) and GoalInstruction (subtype of ActionIn-struction concerned with achieving the final goal).Each of these high-level instructions has its ownpreconditions and postconditions.The issue of how much of the instruction treerepresentation of the plan is addressed in terms oftwo conditions: how far in the original plan theuser has advanced, and what level of abstraction isrequired for the next instruction.
The first condi-tion is easily checked over the state of the world, toestablish what the current situation is.
The secondcondition is determined by checking for satisfac-tion of preconditions and postconditions of the in-structions at all possible levels that start from thecurrent situation.
The check starts at the highestpossible level.Instructions whose postconditions are alreadysatisfied are pruned from the tree, as there is nolonger any need to provide that instruction to theuser.
If preconditions are met but postconditionsare not, the VG uses this instruction in the nextturn, and then waits for a user action.
If neitherpostconditions nor preconditions are satisfied forthis instruction, the next (lower) level of instruc-tions in the instruction tree is considered insteadof this one.
These decisions are handled by mod-ules known as Guide Agents.3.3 Warning the UserIf the user is going to cross a street when the trafficlight is red, the VG will have to warn him about it.If the warning information is more important thanthe guiding, the VG will have to delay instructiongiving, and warn the user first.
To decide about theimportance of the warning part of the discourse,we defined agents as entities in charge of watch-ing for special situations.
Each agent takes care ofa specific kind of situation that may imply somesort of hazardous or bad result.
They are all inde-pendent, and may differ depending on the kind ofenvironment, goals or even the kind of user.Each agent has a weight that reflects its prioritywhen being considered.
An agent always evalu-ates its situation and returns a value in the [0, 1]interval.
A near zero value means there are lowprobabilities for the situation to happen and a nearto one value means the situation is on the vergeto happening.
All agents that exceed a thresholdvalue will be considered as contributors to the dis-course.
We sort them in descending order basedon the result of multiplying each return value bythe weight of the agent.
If an agent is considered70as a contributor, its warning is introduced in thediscourse.We defined three types of agents: informationagents watch for interesting hotspots in an area,status agents watch over the user?s status, andarea agents watch over special areas, includingdangerous areas.In our entry for the GIVE challenge there wasa status agent that checked how much time hadpassed since the last user action to identify whenthe user might be lost.
There was one agent thatchecked for booby traps the user might step on(some of them resulted in loosing the game in-mediately).
Another one ensured the user re-mained within a security area that abstracted allpossible common routes to reach the intended des-tination.
If a user leaves the security area, he isgoing in the wrong direction.This security area isdynamicaly updated attending to the current user?sposition.
Finally, alarm agents watch for wrongactions, controlling if user is on the verge of press-ing the wrong button or leaving the room usinga wrong exit.
We implemented no informationagents, but they would be interesting in real sit-uations.3.4 Hidden Reference DiscoveryThe center spot in a room is not a visible or tan-gible object, and finding it requires a non-trivialcalculation of the room?s shape.
Adding it tothe references container can help creating simplerand richer sentences.
A reference like ?the tableacross the room?
can be generated when the lis-tener and the target are in line with the center spotof the room, on opposite sides, independently ofwhere the user is facing.
In an indoor environ-ment, architectural elements usually make manyinferences possible.
Two hallways that intersectmake an intersection, two walls make a corner, etc.and though these elements might not be referencedas they are in the given environment, they shouldbe taken into account.
In a similar way, hiddenrelations discovery can be accomplished.
Objectalignments or arrangements can be revealed andused for the same purpose.
Sentences like ?the carin line with these pillars?
can be generated.
All ofthese additional high-level concepts and relationsbetween them and low-level world entities are ob-tained by abstraction over the available represen-tation.
We create a family of reference agents,each one specialized in identifying candidate dis-Oppositethe green doorRoom CenterCornerBetweenthe bluedoorsFigure 6: Hidden references in a room.ambiguating properties of a different kind.
Someof these properties are already explicit in the worldrepresentation (colour) and some require a pro-cess of abstraction (relations to corners, for in-stance).
Once obtained, they become available asadditional properties that may be used to disam-biguate references.The goal of our design is to leverage the sys-tem?s ability to express itself using different com-binations of the complete set of disambiguatingproperties made available in this manner.
Thisgives system designers a choice between havingmany simple agents or fewer more expressive,complex agents.
This choice should be consideredin terms of particular implementation details.Reference agents rely on the Reiter and Dale al-gorithm (Reiter and Dale, 1992).
Considering alist of distractors and the reference object, the goalis to filter the distractors list, building a referencethat takes out all the distractors, so that the refer-ence is good, not ambiguous.
Each reference agenthas the ability of taking out a different set of dis-tractors, using different properties that are trivialor hidden, as explained above.
Combining theseagents in different ways generates different refer-ence sentences, some of them longer but more spe-cific, others shorter but ambiguous.
What we triedto achieve is to find the right combination of refer-ence agents that create the shortest non-ambiguoussentence.
This is not a natural approach, as some-one could prefer to have an ambiguous (but morehuman) spatial relation (Viethen and Dale, 2008)in a reference sentence.
Or for example, someonecould prefer having a longer reference like ?the bigred box that?s on the third shelf from the bottom?than a perfectly specific (but not natural) referencelike ?the 3 kg box?.71REALWORLDWORLDANALYSIS EXPANDEDWORLDGOALSDISAMBIGUATIONAPPROXIMATION STAGEALERTSINSTRUCTION TREE123LEVELSGUIDE MANAGERGUIDE AGENT 1GUIDE AGENT 2G.
3.1G.
3.n ...REFERRER MANAGER ReferrerReferrerReferrerReferrerReferrerReferrerALARMMANAGERAlarm AgentAlarm AgentAlarm AgentAlarm AgentAlarm AgentAlarm AgentGOAL SUBSETCURRENTINSTRUCTIONGOALPLANNERgeneratedoutputGENERATIONMANAGERFigure 7: General design.3.5 Guide architectureThe architecture design can be divided into twomain parts.
The instruction tree, shown as threeinterconnected lists in Figure 7, that contains allthe generated levels of instructions as explainedin section 3.2, and a set of components that per-form the different guiding tasks.
One input for thesystem is the ?Real World?, as opposed to the Ex-panded World that is generated after the analysis,as explained in sections 3.1 and 3.4.
The secondinput is the set of goals to be achieved.
After thebasic instruction set is generated by the plannerfrom the given set of goals, the instruction tree isgenerated, level by level.Figure 7 represents a state of the guiding pro-cess where the user is trying to achieve some in-termediate GOAL.
The current instruction markerrepresents the location of the instruction that is tobe given to the user to achieve the current GOAL(the one on the upper level).
Since at this pointthe system has determined that level 2 instructionsshould be used, the level 2 subset of instructionsare represented here as part of the current instruc-tion.
As explained in section 3.2, the algorithmchooses what level should be used at each mo-ment.The Guide Manager makes use of the AlarmManager and Referrer Manager to create theproper output.
As explained in 3.3, the AlarmAgents examine the environment, and tell theGuide Manager if the user should be warned aboutany hazardous situation.
The Referrers help build-ing the proper reference sentences, as explainedin sections 3.2 and 3.4, finally the different Guidehelp building the proper guiding sentences.
TheGuide Manager sends the output to the Genera-tion Manager, which is in charge of generating thefinal output.4 DiscussionThe layered, multilevel hierarchy tries to imitatethe way humans think about local plans, and theagent based view attemps to make instruction giv-ing proactive rather than reactive.
The algorithmfirst gives generalistic, global orders to get theuser near the particular objective.
Then, oncethe irrelevant information has been removed fromthe user point of view and it can not confuse theuser, more specific orders are given.
In this way,the algorithm decides what to say the ?humanway?.
Although the ?human?
generation of in-structions could have been obtained with differentalgorithms, doing it the same way creates a moremaintainable, natural form of expressing the oper-ation.
It would be interesting to input real humandata, as done in (Stoia et al, 2006), in order toguarantee this objective.Traditionally, planning systems have certainworld representation based on discrete stateswhich are more or less useful for finding a goodsolution (Chih-Wei Hsu and Chen, 2006).
How-ever, this representation is not necessarily usefulfor creating a natural language representation ofeach planning operator.
For a good instructionto be generated, plain operators like ?turn right?usually do not contain much information.
Instruc-tion generation systems have to find a compromisebetween planning efficiency and natural languagecontent.
Creating the instruction tree depends di-rectly on figuring out what elements to include inthe discourse.The architecture shown in Section 3 has beendesigned with adaptability in mind, following thearchitecture presented in (Dale and Geldof, 2003).This shows a module layout where the text plan-72ner and the surface realizer are independently con-nected in the generation pipeline.5 Conclusions and Future WorkThe decisions to consider higher level of abstrac-tion for both the representation of the world andthe granularity of instructions, and the introduc-tion of alarms have shown very satisfactory resultsover informal tests with users.
Further evaluationis in process as part of the GIVE Challenge (Kolleret al, 2007)1.
The decisions presented in this pa-per should be revised in view of these results.
Thedefinition of a security area enables the system toprovide suitable warning when the user really goesout of the way, but makes the system robust withrespect to minor variations with respect to the lit-eral plan provided by the planner.The GIVE challenge set up was a good startingpoint to begin our experiments, but we are con-sidering more complex environments to test ad-vanced features.
Extensions that promise interest-ing challenges are: the consideration of a contin-uous world representation (rather than discretisedin terms of tiles and four cardinal points), more re-alistic test maps to extend the level of hierarchy tobuildings and urban areas, and new environmentsdesigned to experiment with distorted representa-tions of the scenary in order to simulate physicalimpediments like blindness.AcknowledgmentsThis research is funded by the Ministerio de In-vestigacio?n, Ciencia e Innovacio?n (GALANTE:TIN2006-14433-C02-01), and Universidad Com-plutense de Madrid and Comunidad de Madrid(MILU: CCG07-UCM/TIC 2803).ReferencesRussell S. Blue, Jeff Wampler, G. Bowden Wise,Louis J. Hoebel, Boris Yamrom, Christopher R.Volpe, Bruce Wilde, Pascale Rondot, Ann E. Kelly,Anne Gilman, Wesley Turner, Steve Linthicum, andGeorge Ryon.
2002.
An automated approach andvirtual environment for generating maintenance in-structions.
In CHI ?02: CHI ?02 extended abstractson Human factors in computing systems, pages 494?495, New York, NY, USA.
ACM.Juliet C. Bourne.
1999.
Generating Effective Natu-ral Language Instructions based on Agent Expertise.Ph.D.
thesis, University of Pennsylvania.1The results of this challenge will be made available aspart of the ENLG 2009 Workshop.Donna Byron, Alexander Koller, Jon Oberlander, LauraStoia, and Kristina Striegnitz.
2007.
Generating in-structions in virtual environments (GIVE): A chal-lenge and evaluation testbed for NLG.
In Proceed-ings of the Workshop on Shared Tasks and Compar-ative Evaluation in Natural Language Generation,Arlington.Ruoyun Huang Chih-Wei Hsu, Benjamin W. Wah andYixin Chen.
2006.
Handling soft constraints andgoals preferences in SGPlan.
In ICAPS Workshopon Preferences and Soft Constraints in Planning.Robert Dale and Sabine Geldof.
2003.
Coral: Usingnatural language generation for navigational assis-tance.
In Proceedings of the 26th Australasian Com-puter Science Conference.C.
Galindo, A. Saffiotti, S. Coradeschi, P. Buschka,J.A.
Fernandez-Madrigal, and J. Gonzalez.
2005.Multi-hierarchical semantic maps for mobilerobotics.
Intelligent Robots and Systems, 2005.
(IROS 2005).
2005 IEEE/RSJ International Confer-ence on, pages 2278?2283, Aug.John D. Kelleher and Geert-Jan M. Kruijff.
2005.
Acontext-dependent algorithm for generating locativeexpressions in physically situated environments.
InProceedings of ENLG-05, Aberdeen, Scotland.Alexander Koller, Johanna Moore, Barbara di Eugenio,James Lester, Laura Stoia, Donna Byron, Jon Ober-lander, and Kristina Striegnitz.
2007.
Shared taskproposal: Instruction giving in virtual worlds.
InMichael White and Robert Dale, editors, Workinggroup reports of the Workshop on Shared Tasks andComparative Evaluation in Natural Language Gen-eration.S.
M. LaValle.
2006.
Planning Algorithms.
Cam-bridge University Press, Cambridge, U.K. Availableat http://planning.cs.uiuc.edu/.Christian Muller.
2002.
Multimodal dialog in a mobilepedestrian navigation system.
IDS-2002.E.
Reiter and R. Dale.
1992.
A fast algorithm for thegeneration of referring expressions.
In Proceedingsof the 14th conference on Computational linguistics,Nantes, France.Laura Stoia, Donna Byron, Darla Shockley, and EricFosler-Lussier.
2006.
Sentence planning for real-time navigational instructions.
In Proceedings ofthe Human Language Technology Conference of theNorth American Chapter of the ACL.Sebastian Varges.
2005.
Spatial descriptions as refer-ring expressions in the maptask domain.
In Proc.
ofthe 10th European Workshop on Natural LanguageGeneration.Jett Viethen and Robert Dale.
2008.
The use of spatialrelations in referring expression generation.
In FifthInternational Natural Language Generation Confer-ence.73
