Proceedings of the 8th International Workshop on Tree Adjoining Grammar and Related Formalisms, pages 57?64,Sydney, July 2006. c?2006 Association for Computational LinguisticsStochastic Multiple Context-Free Grammarfor RNA Pseudoknot ModelingYuki KatoGraduate School ofInformation Science,Nara Institute ofScience and Technology8916-5 Takayama, Ikoma,Nara 630-0192, Japanyuuki-ka@is.naist.jpHiroyuki SekiGraduate School ofInformation Science,Nara Institute ofScience and Technology8916-5 Takayama, Ikoma,Nara 630-0192, Japanseki@is.naist.jpTadao KasamiGraduate School ofInformation Science,Nara Institute ofScience and Technology8916-5 Takayama, Ikoma,Nara 630-0192, Japankasami@naist.jpAbstractSeveral grammars have been proposedfor modeling RNA pseudoknotted struc-ture.
In this paper, we focus on multiplecontext-free grammars (MCFGs), whichare natural extension of context-free gram-mars and can represent pseudoknots, andextend a specific subclass of MCFGs toa probabilistic model called SMCFG.
Wepresent a polynomial time parsing algo-rithm for finding the most probable deriva-tion tree and a probability parameter esti-mation algorithm.
Furthermore, we showsome experimental results of pseudoknotprediction using SMCFG algorithm.1 IntroductionNon-coding RNAs fold into characteristic struc-tures determined by interactions between mostlyWatson-Crick complementary base pairs.
Such abase paired structure is called the secondary struc-ture.
Pseudoknot (Figure 1 (a)) is one of the typi-cal substructures found in the secondary structuresof several RNAs, including rRNAs, tmRNAs andviral RNAs.
An alternative graphic representationof a pseudoknot is arc depiction where arcs con-nect base pairs (Figure 1 (b)).
It has been rec-ognized that pseudoknots play an important rolein RNA functions such as ribosomal frameshiftingand regulation of translation.Many attempts have so far been made at mod-eling RNA secondary structure by formal gram-mars.
In a grammatical approach, secondary struc-ture prediction can be viewed as parsing problem.However, there may be many different derivationtrees for an input sequence.
Thus, it is necessary tohave a method of extracting biologically realistic5?
?C  A  G  G?
?
?U  C  C  A  G  U?
?
?U  C  A  G?3?CGC(a) Pseudoknotc  a  g  g  c  u  g  a  c  c  u  g  c  u  c  a  g(b) Arc depiction of (a)Figure 1: Example of RNA secondary structurederivation trees among them.
One solution to thisproblem is to extend a grammar to a probabilisticmodel and find the most likely derivation tree, andanother is to take free energy minimization into ac-count.
Eddy and Durbin (1994), and Sakakibara etal.
(1994) modeled RNA secondary structure with-out pseudoknots by using stochastic context-freegrammars (stochastic CFGs or SCFGs).
For pseu-doknotted structure (Figure 1 (a)), however, an-other approach has to be taken since a single CFGcannot represent crossing dependencies of basepairs in pseudoknots (Figure 1 (b)) for the lack ofgenerative power.
Brown and Wilson (1996) pro-posed a model based on intersections of SCFGsto describe RNA pseudoknots.
Cai et al (2003)introduced a model based on parallel communi-cation grammar systems using a single CFG syn-chronized with a number of regular grammars.Akutsu (2000) provided dynamic programming al-gorithms for RNA pseudoknot prediction withoutusing grammars.
On the other hand, several gram-mars have been proposed where the grammar itselfcan fully describe pseudoknots.
Rivas and Eddy(1999, 2000) provided a dynamic programming57algorithm for predicting RNA secondary structureincluding pseudoknots, and introduced a new classof grammars called RNA pseudoknot grammars(RPGs) for deriving sequences with gap.
Ue-mura et al (1999) defined specific subclasses oftree adjoining grammars (TAGs) named SL-TAGsand extended SL-TAGs (ESL-TAGs) respectively,and predicted RNA pseudoknots by using pars-ing algorithm of ESL-TAG.
Matsui et al (2005)proposed pair stochastic tree adjoining grammars(PSTAGs) based on ESL-TAGs and tree automatafor aligning and predicting pseudoknots, whichshowed good prediction accuracy.
These gram-mars have generative power stronger than CFGsand polynomial time algorithms for parsing prob-lem.In our previous work (Kato et al, 2005),we identified RPGs, SL-TAGs and ESL-TAGsas subclasses of multiple context-free grammars(MCFGs) (Kasami et al, 1988; Seki et al, 1991),which can model RNA pseudoknots, and showed acandidate subclass of the minimum grammars forrepresenting pseudoknots.
The generative powerof MCFGs is stronger than that of CFGs andMCFGs have a polynomial time parsing algo-rithm like the CYK (Cocke-Younger-Kasami) al-gorithm for CFGs.
In this paper, we extend theabove candidate subclass of MCFGs to a prob-abilistic model called a stochastic MCFG (SM-CFG).
We present a polynomial time parsing algo-rithm for finding the most probable derivation tree,which is applicable to RNA pseudoknot predic-tion.
In addition, we mention a probability param-eter estimation method based on the EM (expec-tation maximization) algorithm.
Finally, we showsome experimental results on pseudoknot predic-tion for three RNA families using SMCFG algo-rithm, which show good prediction accuracy.2 Stochastic Multiple Context-FreeGrammarA stochastic multiple context-free grammar(stochastic MCFG, or SMCFG) is a probabilisticextension of MCFG (Kasami et al, 1988; Seki etal., 1991) or linear context-free rewriting system(Vijay-Shanker et al, 1987).
An SMCFG is a 5-tuple G = (N,T, F, P, S) where N is a finite setof nonterminals, T is a finite set of terminals, F isa finite set of functions, P is a finite set of (pro-duction) rules and S ?
N is the start symbol.
Foreach A ?
N , a positive integer denoted by dim(A)is given and A derives dim(A)-tuples of terminalsequences.
For the start symbol S, dim(S) = 1.For each f ?
F , positive integers di (0 ?
i ?
k)are given and f is a total function from (T ?
)d1 ??
?
?
?
(T ?
)dk to (T ?
)d0 where each component off is defined as the concatenation of some compo-nents of arguments and constant sequences.
Notethat each component of an argument should oc-cur in the function value at most once (linear-ity).
For example, f [(x11, x12), (x21, x22)] =(x11x21, x12x22).
Each rule in P has the formof A0p?
f [A1, .
.
.
, Ak] where Ai ?
N (0 ?i ?
k), f : (T ?
)dim(A1) ?
?
?
?
?
(T ?
)dim(Ak) ?
(T ?
)dim(A0) ?
F and p is a real number with 0 ?p ?
1 called the probability of this rule.
The sum-mation of the probabilities of the rules with thesame left-hand side should be one.
If we are notinterested in p, we just write A0 ?
f [A1, .
.
.
, Ak].If k ?
1, then the rule is called a nonterminat-ing rule, and if k = 0, then it is called a termi-nating rule.
A terminating rule A0 ?
f [ ] withf [h][ ] = ?h (1 ?
h ?
dim(A0)) is simply writtenas A0 ?
(?1, .
.
.
, ?dim(A0)).We recursively define the relation ??
by the fol-lowing (L1) and (L2): (L1) if A p?
?
?
P (?
?
(T ?
)dim(A)), then we write A ??
?
with proba-bility p, and (L2) if A p?
f [A1, .
.
.
, Ak] ?
Pand Ai??
?i ?
(T ?
)dim(Ai) (1 ?
i ?
k)with probabilities p1, .
.
.
, pk, respectively, thenwe write A ??
f [?1, .
.
.
, ?k] with probabilityp ?
?ki=1 pi.
In parallel with the relation?
?, wedefine derivation trees as follows: (D1) if A p??
?
P (?
?
(T ?
)dim(A)), then the ordered treewith the root labeled A which has ?
as the onlyone child is a derivation tree for ?
with proba-bility p, and (D2) if A p?
f [A1, .
.
.
, Ak] ?
P ,Ai??
?i ?
(T ?
)dim(Ai) (1 ?
i ?
k) andt1, .
.
.
, tk are derivation trees for ?1, .
.
.
, ?k withprobabilities p1, .
.
.
, pk, respectively, then the or-dered tree with the root labeled A (or A : fif necessary) which has t1, .
.
.
, tk as (immediate)subtrees from left to right is a derivation tree forf [?1, .
.
.
, ?k] with probability p ?
?ki=1 pi.
Ex-ample rules are A 0.3?
f [A] where f [(x1, x2)] =(ax1b, cx2d) and A0.7?
(ab, cd).
Then, A ??
(ab, cd) by the second rule, which is followedby A ??
f [(ab, cd)] = (aabb, ccdd) by the firstrule.
The probability of the latter derivation is0.3 ?
0.7 = 0.21.
The language generated by anSMCFG G is defined as L(G) = {w ?
T ?
| S ?
?58Table 1: SMCFG GsType Rule set Function Transition probability Emission probabilityE Wv ?
(?, ?)
1 1S Wv ?
J [Wy] J [(x1, x2)] = x1x2 tv(y) 1D Wv ?
SK[Wy] SK[(x1, x2)] = (x1, x2) tv(y) 1B1 Wv ?
C1[Wy,Wz] C1[x1, (x21, x22)] = (x1x21, x22) 1 1B2 Wv ?
C2[Wy,Wz] C2[x1, (x21, x22)] = (x21x1, x22) 1 1B3 Wv ?
C3[Wy,Wz] C3[x1, (x21, x22)] = (x21, x1x22) 1 1B4 Wv ?
C4[Wy,Wz] C4[x1, (x21, x22)] = (x21, x22x1) 1 1U1L Wv ?
UP ai1L[Wy] UPai1L[(x1, x2)] = (aix1, x2) tv(y) ev(ai)U1R Wv ?
UPaj1R[Wy] UPaj1R[(x1, x2)] = (x1aj , x2) tv(y) ev(aj)U2L Wv ?
UP ak2L [Wy] UPak2L [(x1, x2)] = (x1, akx2) tv(y) ev(ak)U2R Wv ?
UP al2R[Wy] UPal2R[(x1, x2)] = (x1, x2al) tv(y) ev(al)P Wv ?
BP aial [Wy] BP aial [(x1, x2)] = (aix1, x2al) tv(y) ev(ai, al)w with probability greater than 0}.In this paper, we focus on an SMCFG Gs =(N,T, F, P, S) that satisfies the following condi-tions: Gs has m different nonterminals denotedby W1, .
.
.
,Wm, each of which uses the only onetype of a rule denoted by E, S, D, B1, B2, B3, B4,U1L, U1R, U2L, U2R or P 1 (see Table 1).
Thetype of Wv is denoted by type(v) and we prede-fine type(1) = S, that is, W1 is the start symbol.Consider a sample rule set Wv ?
UP?1L[Wy] |UP?1L[Wz] where UP?1L[(x1, x2)] = (?x1, x2)and ?
?
T .
For each rule r, two real valuescalled transition probability p1 and emission prob-ability p2 are specified in Table 1.
The probabilityof r is simply defined as p1 ?
p2.
In application,p1 = tv(y) and p2 = ev(ai), .
.
.
in Table 1 are theparameters of the grammar, which are set by handor by a training algorithm (Section 3.3) dependingon the set of possible sequences to be analyzed.3 Algorithms for SMCFGIn RNA structure analysis using stochastic gram-mars, we have to deal with the following threeproblems: (1) calculate the optimal alignment ofa sequence to a stochastic grammar (alignmentproblem), (2) calculate the probability of a se-quence given a stochastic grammar (scoring prob-lem), and (3) estimate optimal probability param-eters for a stochastic grammar given a set of exam-ple sequences (training problem).
In this section,we give solutions to each problem for the specificSMCFG Gs = (N,T, F, P, S).3.1 Alignment ProblemThe alignment problem for Gs is to find themost probable derivation tree for a given input se-1These types stand for END, START, DELETE, BIFURCA-TION, UNPAIR and PAIR respectively.quence.
This problem can be solved by a dynamicprogramming algorithm similar to the CYK algo-rithm for SCFGs (Durbin et al, 1998), and in thispaper, we also call the parsing algorithm for Gsthe CYK algorithm.
We fix an input sequence w =a1 ?
?
?
an (|w| = n).
Let ?v(i, j) and ?y(i, j, k, l)be the logarithm of maximum probabilities of aderivation subtree rooted at a nonterminal Wv fora terminal subsequence ai ?
?
?
aj and of a deriva-tion subtree rooted at a nonterminal Wy for a tupleof terminal subsequences (ai ?
?
?
aj , ak ?
?
?
al) re-spectively.
The variables ?v(i, i?
1) and ?y(i, i?1, j, j ?
1) are the logarithm of maximum prob-abilities for an empty sequence ?
and a pair of ?.Let ?v(i, j) and ?y(i, j, k, l) be traceback variablesfor constructing a derivation tree, which are calcu-lated together with ?v(i, j) and ?y(i, j, k, l).
Wedefine Cv = {y | Wv ?
f [Wy] ?
P, f ?
F}.To avoid non-emitting cycles, we assume that thenonterminals are numbered such that v < y forall y ?
Cv.
The CYK algorithm uses five dimen-sional dynamic programming matrix to calculate?, which leads to logP (w, ??
| ?)
where ??
is themost probable derivation tree and ?
is an entire setof probability parameters.
The detailed descrip-tion of the CYK algorithm is as follows:Algorithm 1 (CYK).Initialization:for i ?
1 to n + 1, j ?
i to n + 1, v ?
1 to mdo if type(v) = Ethen ?v(i, i?
1, j, j ?
1) ?
0else ?v(i, i?
1, j, j ?
1) ?
?
?Iteration:for i ?
n downto 1, j ?
i ?
1 to n, k ?
n + 1downto j + 1, l ?
k ?
1 to n, v ?
1 to mdo if type(v) = Ethen if j = i?
1 and l = k ?
1then skip59else ?v(i, j, k, l) ?
?
?if type(v) = Sthen ?v(i, j)?
maxy?Cvmaxh=i?1,...,j[log tv(y)+?y(i, h, h + 1, j)]?v(i, j)?
argmax(y,h)[log tv(y)+?y(i, h, h+1, j)]if type(v) = B1 and Wv ?
C1[Wy,Wz]then ?v(i, j, k, l)?
maxh=i?1,...,j[?y(i, h)+?z(h+1, j, k, l)]?v(i, j, k, l)?
arg max(y,z,h)[?y(i, h)+?z(h+1, j, k, l)]if type(v) = B2 and Wv ?
C2[Wy,Wz]then ?v(i, j, k, l)?
maxh=i?1,...,j[?y(h+1, j)+?z(i, h, k, l)]?v(i, j, k, l)?
arg max(y,z,h)[?y(h+1, j)+?z(i, h, k, l)]if type(v) = B3 and Wv ?
C3[Wy,Wz]then ?v(i, j, k, l)?
maxh=k?1,...,l[?z(i, j, h+1, l)+?y(k, h)]?v(i, j, k, l)?
arg max(y,z,h)[?z(i, j, h+1, l)+?y(k, h)]if type(v) = B4 and Wv ?
C4[Wy,Wz]then ?v(i, j, k, l)?
maxh=k?1,...,l[?z(i, j, k, h)+?y(h+1, l)]?v(i, j, k, l)?
arg max(y,z,h)[?z(i, j, k, h)+?y(h+1, l)]if type(v) = Pthen if j = i?
1 or l = k ?
1then ?v(i, j, k, l) ?
?
?else ?v(i, j, k, l)?
maxy?Cv[log ev(ai, al) + log tv(y)+?y(i + 1, j, k, l ?
1)]?v(i, j, k, l)?
argmaxy[log ev(ai, al) + log tv(y)+?y(i + 1, j, k, l ?
1)]else ?v(i, j, k, l)?
maxy?Cv[log ev(ai, aj , ak, al) + log tv(y)+?y(i + ?1Lv , j ?
?1Rv , k + ?2Lv ,l ?
?2Rv )]?v(i, j, k, l)?
argmaxy[log ev(ai, aj , ak, al)+ log tv(y) + ?y(i + ?1Lv , j ?
?1Rv ,k + ?2Lv , l ?
?2Rv )]Note: ev(ai, aj , ak, al) = ev(ai) for type(v) =U1L, ev(ai, aj , ak, al) = ev(aj) for type(v) =U1R, ev(ai, aj , ak, al) = ev(ak) for type(v) =U2L, ev(ai, aj , ak, al) = ev(al) for type(v) =U2R, ev(ai, aj , ak, al) = 1 for the other typesexcept P. Also, ?1Lv = 1 for type(v) = U1L,?1Rv = 1 for type(v) = U1R, ?2Lv = 1 fortype(v) = U2L, ?2Rv = 1 for type(v) = U2R,and ?1Lv , .
.
.
?2Rv are set to 0 for the other typesexcept P.When the calculation terminates, we obtainlogP (w, ??
| ?)
= ?1(1, n).
If there are b BI-FURCATION nonterminals and a other nontermi-nals, the time and space complexities of the CYKalgorithm are O(amn4 + bn5) and O(mn4), re-spectively.
To recover the optimal derivation tree,we use the traceback variables ?
.
Due to limitationof the space, the full description of the tracebackalgorithm is omitted (see (Kato and Seki, 2006)).3.2 Scoring ProblemAs in SCFGs (Durbin et al, 1998), the scor-ing problem for Gs can be solved by the insidealgorithm.
The inside algorithm calculates thesummed probabilities ?v(i, j) and ?y(i, j, k, l) ofall derivation subtrees rooted at a nonterminal Wvfor a subsequence ai ?
?
?
aj and of all derivationsubtrees rooted at a nonterminal Wy for a tupleof subsequences (ai ?
?
?
aj , ak ?
?
?
al) respectively.The variables ?v(i, i?1) and ?y(i, i?1, j, j?1)are defined for empty sequences in a similar wayto the CYK algorithm.
Therefore, we can easilyobtain the inside algorithm by replacing max op-erations with summations in the CYK algorithm.When the calculation terminates, we obtain theprobability P (w | ?)
= ?1(1, n).
The time andspace complexities of the algorithm are identicalwith those of the CYK algorithm.In order to re-estimate the probability parame-ters of Gs, we need the outside algorithm.
Theoutside algorithm calculates the summed prob-ability ?v(i, j) of all derivation trees excludingsubtrees rooted at a nonterminal Wv generat-ing a subsequence ai ?
?
?
aj .
Also, it calculates?y(i, j, k, l), the summed probability of all deriva-tion trees excluding subtrees rooted at a non-terminal Wy generating a tuple of subsequences(ai ?
?
?
aj , ak ?
?
?
al).
In the algorithm, we will usePv = {y | Wy ?
f [Wv] ?
P, f ?
F}.
Notethat calculating the outside variables ?
requiresthe inside variables ?.
Unlike CYK and inside al-gorithms, the outside algorithm recursively works60its way inward.
The time and space complexitiesof the outside algorithm are the same as those ofCYK and inside algorithms.
Formally, the outsidealgorithm is as follows:Algorithm 2 (Outside).Initialization:?1(1, n) ?
1Iteration:for i ?
1 to n+1, j ?
n downto i?1, k ?
j+1to n + 1, l ?
n downto k ?
1, v ?
1 to mdo if type(v) = S and Wy ?
C1[Wv,Wz]then ?v(i, j)?n?h=jn+1?k?=h+1n?l?=k?
?1?y(i, h, k?, l?
)?z(j + 1, h, k?, l?
)if type(v) = S and Wy ?
C2[Wv,Wz]then ?v(i, j)?i?h=1n+1?k?=j+1n?l?=k?
?1?y(h, j, k?, l?
)?z(h, i?
1, k?, l?
)if type(v) = S and Wy ?
C3[Wv,Wz]then ?v(i, j)?i?h=1i?1?k?=h?1n?l?=j?y(h, k?, i, l?
)?z(h, k?, j + 1, l?
)if type(v) = S and Wy ?
C4[Wv,Wz]then ?v(i, j)?i?h=1i?1?k?=h?1i?l?=k?+1?y(h, k?, l?, j)?z(h, k?, l?, i?
1)if type(v) ?= S and Wy ?
C1[Wz,Wv]then ?v(i, j, k, l)?i?h=1?y(h, j, k, l)?z(h, i?
1)if type(v) ?= S and Wy ?
C2[Wz,Wv]then ?v(i, j, k, l)?k?1?h=j?y(i, h, k, l)?z(j + 1, h)if type(v) ?= S and Wy ?
C3[Wz,Wv]then ?v(i, j, k, l)?k?h=j+1?y(i, j, h, l)?z(h, k ?
1)if type(v) ?= S and Wy ?
C4[Wz,Wv]then ?v(i, j, k, l)?n?h=l?y(i, j, k, h)?z(l + 1, h)else ?v(i, j, k, l)??y?Pv?y(i?
?1Ly , j + ?1Ry , k ?
?2Ly ,l+?2Ry )ey(ai?
?1Ly , aj+?1Ry , ak?
?2Ly ,al+?2Ry )ty(v)3.3 Training ProblemThe training problem for Gs can be solved by theEM algorithm called the inside-outside algorithmwhere the inside variables ?
and outside variables?
are used to re-estimate probability parameters.First, we consider the probability that a nonter-minal Wv is used at positions i, j, k and l in aderivation of a single sequence w. If type(v) =S, the probability is 1P (w|?
)?v(i, j)?v(i, j), other-wise 1P (w|?
)?v(i, j, k, l)?v(i, j, k, l).
By summingthese over all positions in the sequence, we can ob-tain the expected number of times that Wv is usedfor w as follows: for type(v) = S, the expectedcount is1P (w | ?
)n+1?i=1n?j=i?1?v(i, j)?v(i, j),otherwise1P (w | ?
)n+1?i=1n?j=i?1n+1?k=j+1n?l=k?1?v(i, j, k, l)?v(i, j, k, l).Next, we extend these expected values from a sin-gle sequence w to multiple independent sequencesw(r) (1 ?
r ?
N).
Let ?
(r) and ?
(r) be the in-side and outside variables calculated for each in-put sequence w(r).
Then we can obtain the ex-pected number of times that a nonterminal Wv isused for training sequences w(r) (1 ?
r ?
N) bysumming the above terms over all sequences: fortype(v) = S,E(v) =N?r=1n+1?i=1n?j=i?11P (w(r) | ?)?
(r)v (i, j)?
(r)v (i, j),otherwiseE(v) =N?r=1n+1?i=1n?j=i?1n+1?k=j+1n?l=k?11P (w(r) | ?)?
(r)v (i, j, k, l)?
(r)v (i, j, k, l).Similarly, for a given Wy, the expected number oftimes that a rule Wv ?
f [Wy] is applied can be61obtained as follows: for type(v) = S,E(v ?
y) =N?r=1n+1?i=1n?j=i?1j?h=i?11P (w(r) | ?)?
(r)v (i, j)tv(y)?
(r)y (i, h, h + 1, j),otherwiseE(v ?
y) =N?r=1n+1?i=1n?j=i?1n+1?k=j+1n?l=k?11P (w(r) | ?)?
(r)v (i, j, k, l)ev(ai, aj , ak, al)tv(y)?
(r)y (i + ?1Lv , j ?
?1Rv , k + ?2Lv ,l ?
?2Rv ).For a given terminal a or a pair of terminals (a, b),the expected number of times that a rule contain-ing a (or a and b) is applied is as shown below: fortype(v) = U1L,E(v ?
a) =N?r=1n?i=1n?j=in+1?k=j+1n?l=k?11P (w(r) | ?)?
(a(r)i = a)?
(r)v (i, j, k, l)?
(r)v (i, j, k, l),for type(v) = U1R,E(v ?
a) =N?r=1n?i=1n?j=in+1?k=j+1n?l=k?11P (w(r) | ?)?
(a(r)j = a)?
(r)v (i, j, k, l)?
(r)v (i, j, k, l),for type(v) = U2L,E(v ?
a) =N?r=1n?1?i=1n?1?j=i?1n?k=j+1n?l=k1P (w(r) | ?)?
(a(r)k = a)?
(r)v (i, j, k, l)?
(r)v (i, j, k, l),for type(v) = U2R,E(v ?
a) =N?r=1n?1?i=1n?1?j=i?1n?k=j+1n?l=k1P (w(r) | ?)?
(a(r)l = a)?
(r)v (i, j, k, l)?
(r)v (i, j, k, l),and for type(v) = P,E(v ?
ab) =N?r=1n?1?i=1n?1?j=in?k=j+1n?l=k1P (w(r) | ?)?
(a(r)i = a, a(r)l = b)?
(r)v (i, j, k, l)?
(r)v (i, j, k, l),where ?
(C) is 1 if the condition C in the parenthe-sis is ture, and 0 if C is false.Now, we re-estimate probability parameters byusing the above expected counts.
Let t?v(y) be re-estimated transition probabilities from Wv to Wy.Also, let e?v(a) and e?v(a, b) be re-estimated emis-sion probabilities that Wv emits a symbol a andtwo symbols a and b respectively.
We can ob-tain each re-estimated probability by the followingequations:t?v(y) =E(v ?
y)E(v), e?v(a) =E(v ?
a)E(v),e?v(a, b) =E(v ?
ab)E(v).
(3.1)Note that the expected count correctly correspond-ing to its nonterminal type must be substitutedfor the above equations.
In summary, the inside-outside algorithm is as follows:Algorithm 3 (Inside-Outside).Initialization: Pick arbitrary probability parame-ters of the model.Iteration: Calculate the new probability parame-ters using (3.1).
Calculate the new log likelihood?Nr=1 logP (w(r) | ?)
of the model.Termination: Stop if the change in log likelihoodis less than predefined threshold.4 Experimental Results4.1 Data for ExperimentsThe dataset for experiments was taken from anRNA family database called ?Rfam?
(version 7.0)(Griffiths-Jones et al, 2003) which is a databaseof multiple sequence alignment and covariancemodels (Eddy and Durbin, 1994) representingnon-coding RNA families.
We selected three vi-ral RNA families with pseudoknot annotationsnamed Corona pk 3 (Corona), HDV ribozyme(HDV) and Tombus 3 IV (Tombus) (see Table 2).Corona pk 3 has a simple pseudoknotted struc-ture, whereas HDV ribozyme and Tombus 3 IVhave more complicated structures with pseudo-knot.62Table 2: Three RNA families from Rfam ver.
7.0Family Range of length # of annotated sequences # of test sequencesCorona pk 3 62?64 14 10HDV ribozyme 87?91 15 10Tombus 3 IV 89?92 18 12Table 3: Prediction resultsFamily Precision [%] Recall [%] CPU time [sec]Average Min Max Average Min Max Average Min MaxCorona pk 3 99.4 94.4 100.0 99.4 94.4 100.0 27.8 26.0 30.4HDV ribozyme 100.0 100.0 100.0 100.0 100.0 100.0 252.1 219.0 278.4Tombus 3 IV 100.0 100.0 100.0 100.0 100.0 100.0 244.8 215.2 257.54.2 ImplementationWe specified a particular SMCFG Gs by utiliz-ing secondary structure annotation of each fam-ily.
Rules were determined by considering con-sensus secondary structure.
Probability parame-ters were estimated in a few selected sequences bythe simplest pseudocounting method known as theLaplace?s rule (Durbin et al, 1998): to add one ex-tra count to the true counts for each base configu-ration observed in a few selected sequences.
Notethat the inside-outside algorithm was not used inthe experiments.
The other sequences in the align-ment were used as the test sequences for predic-tion (see Table 2).
We implemented the CYK al-gorithm with traceback in ANSI C on a machinewith Intel Pentium D CPU 2.80 GHz and 2.00 GBRAM.
Straightforward implementation gives riseto a serious problem of lack of memory space dueto the higher order dynamic programming matrix(remember that the space complexity of the CYKalgorithm is O(mn4)).
The dynamic program-ming matrix in our specified model is sparse, andtherefore, we successfully implemented the matrixas a hash table storing only nonzero probabilityvalues (equivalently, finite values of the logarithmof probabilities).4.3 TestsWe tested prediction accuracy by calculating pre-cision and recall (sensitivity), which are the ratioof the number of correct base pairs predicted bythe algorithm to the total number of predicted basepairs, and the ratio of the number of correct basepairs predicted by the algorithm to the total num-ber of base pairs specified by the trusted annota-tion, respectively.
The results are shown in Table3.
A nearly correct prediction (94.4% precisionand recall) for Corona pk 3 is shown in Figure2 where underlined base pairs agree with trustedones.
The secondary structures predicted by ouralgorithm agree very well with the trusted struc-tures.4.4 Comparison with PSTAGWe compared the prediction accuracy of our SM-CFG algorithm with that of PSTAG algorithm(Matsui et al, 2005) (see Table 4).
PSTAGs,as we have mentioned before, are proposed formodeling pairwise alignment of RNA sequenceswith pseudoknots and assign a probability to eachalignment of TAG derivation trees.
PSTAG al-gorithm, based on dynamic programming, calcu-lates the most likely alignment for the pair ofTAG derivation trees where one of them is in theform of an unfolded sequence and the other is aTAG derivation tree for known structure.
SMCFGmethod shows better performance in accuracy thanPSTAG method in the same test sets.5 ConclusionIn this paper, we have proposed a probabilisticmodel named SMCFG, and designed a polyno-mial time parsing and a parameter estimation al-gorithm for SMCFG.
Moreover, we have demon-strated computational experiments of RNA sec-ondary structure prediction with pseudoknots us-ing SMCFG parsing algorithm, which show goodperformance in accuracy.AcknowledgmentsThis work is supported in part by Grant-in-Aidfor Scientific Research from Japan Society for thePromotion of Science (JSPS).
We also wish tothank JSPS Research Fellowships for Young Sci-entists for their generous financial assistance.
Theauthors thank Dr. Yoshiaki Takata for his useful63CUAGUCUUAUACACAAUGGUAAGCCAGUGGUAGUAAAGGUAUAAGAAAUUUGCUACUAUGUUA[[[[[[[[              ((( ((((((( ]]]]]]]]  ))))))) )))CUAGUCUUAUACACAAUGGUAAGCCAGUGGUAGUAAAGGUAUAAGAAAUUUGCUACUAUGUUA[[[[[[[[              ((((((((((  ]]]]]]]]   ))))))))))[Trusted structure in Rfam][Prediction by SMCFG]Corona_pk3 (EMBL accession #: X51325.1)Figure 2: Comparison of a prediction result with a trusted structure in RfamTable 4: Comparison between SMCFG and PSTAGModel Average precision [%] Average recall [%]Corona HDV Tombus Corona HDV TombusSMCFG 99.4 100.0 100.0 99.4 100.0 100.0PSTAG 95.5 95.6 97.4 94.6 94.1 97.4comments on implementation of high dimensionaldynamic programming.ReferencesTatsuya Akutsu.
2000.
Dynamic programming al-gorithms for RNA secondary structure predictionwith pseudoknots.
Discrete Applied Mathematics,104:45?62.Michael Brown and Charles Wilson.
1996.
RNA pseu-doknot modeling using intersections of stochasticcontext free grammars with applications to databasesearch.
Proc.
Pacific Symposium on Biocomputing,109?125.Liming Cai, Russell L. Malmberg, and Yunzhou Wu.2003.
Stochastic modeling of RNA pseudoknottedstructures: a grammatical approach.
Bioinformatics,19(1):i66?i73.Richard Durbin, Sean R. Eddy, Anders Krogh, andGraeme Mitchison.
1998.
Biological SequenceAnalysis, Cambridge University Press.Sean R. Eddy and Richard Durbin.
1994.
RNAsequence analysis using covariance models.
Nuc.Acids Res., 22(11):2079?2088.Sam Griffiths-Jones, Alex Bateman, Mhairi Marshall,Ajay Khanna, and Sean R. Eddy.
2003.
Rfam: anRNA family database.
Nuc.
Acids Res., 31(1):439?441.Tadao Kasami, Hiroyuki Seki, and Mamoru Fujii.1988.
Generalized context-free grammar and multi-ple context-free grammar.
IEICE Trans.
Inf.
& Syst.,J71-D(5):758?765 (in Japanese).Yuki Kato, Hiroyuki Seki, and Tadao Kasami.
2005.On the generative power of grammars for RNA sec-ondary structure.
IEICE Trans.
Inf.
& Syst., E88-D(1):53?64.Yuki Kato and Hiroyuki Seki.
2006.
Stochasticmultiple context-free grammar for RNA pseudoknotmodeling.
NAIST Info.
Sci.
Tech.
Rep. (NAIST-IS-TR2006002).Hiroshi Matsui, Kengo Sato, and Yasubumi Sakak-ibara.
2005.
Pair stochastic tree adjoining gram-mars for aligning and predicting pseudoknot RNAstructures.
Bioinformatics, 21(11):2611?2617.Elena Rivas and Sean R. Eddy.
1999.
A dynamic pro-gramming algorithm for RNA structure predictionincluding pseudoknots.
J. Mol.
Biol., 285:2053?2068.Elena Rivas and Sean R. Eddy.
2000.
The language ofRNA: A formal grammar that includes pseudoknots.Bioinformatics, 16(4):334?340.Yasubumi Sakakibara, Michael Brown, RichardHughey, I. Saira Mian, Kimmen Sjo?lander, RebeccaC.
Underwood, and David Haussler.
1994.
Stochas-tic context-free grammars for tRNA modeling.
Nuc.Acids Res., 22:5112?5120.Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii, andTadao Kasami.
1991.
On multiple context-freegrammars.
Theor.
Comput.
Sci., 88:191?229.Yasuo Uemura, Aki Hasegawa, Satoshi Kobayashi, andTakashi Yokomori.
1999.
Tree adjoining grammarsfor RNA structure prediction.
Theor.
Comput.
Sci.,210:277?303.K.
Vijay-Shanker, David J. Weir, and Aravind K. Joshi.1987.
Characterizing structural descriptions pro-duced by various grammatical formalisms.
Proc.25th Annual Meeting of Association for Computa-tional Linguistics (ACL87), 104?111.64
