2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 508?512,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsActive Learning for Coreference ResolutionFlorian Laws1 Florian Heimerl2 Hinrich Schu?tze11 Institute for Natural Language Processing (IMS)Universita?t Stuttgartflorian.laws@ims.uni-stuttgart.de2 Institute for Visualization and Interactive SystemsUniversita?t Stuttgartflorian.heimerl@vis.uni-stuttgart.deAbstractWe present an active learning method forcoreference resolution that is novel in three re-spects.
(i) It uses bootstrapped neighborhoodpooling, which ensures a class-balanced pooleven though gold labels are not available.
(ii)It employs neighborhood selection, a selectionstrategy that ensures coverage of both posi-tive and negative links for selected markables.
(iii) It is based on a query-by-committee selec-tion strategy in contrast to earlier uncertaintysampling work.
Experiments show that thisnew method outperforms random sampling interms of both annotation effort and peak per-formance.1 IntroductionCoreference resolution (CR) ?
the task of determin-ing if two expressions in natural language text re-fer to the same real-world entity ?
is an importantNLP task.
One popular approach to CR is super-vised classification.
This approach needs manuallylabeled training data that is expensive to create.
Ac-tive learning (AL) is a technique that can reduce thiscost by setting up an interactive training/annotationloop that selects and annotates training examplesthat are maximally useful for the classifier that isbeing trained.
However, while AL has been provensuccessful for many other NLP tasks, such as part-of-speech tagging (Ringger et al, 2007), parsing(Osborne and Baldridge, 2004), text classification(Tong and Koller, 2002) and named entity recogni-tion (Tomanek et al, 2007), AL has not been suc-cessfully applied to coreference resolution so far.In this paper, we present a novel approach to ALfor CR based on query-by-committee sampling andbootstrapping and show that it performs better thana number of baselines.2 Related workCoreference resolution.
The perhaps most widelyused supervised learning approach to CR is themention-pair model (Soon et al, 2001).
This modelclassifies links (pairs of two mentions) as corefer-ent or disreferent, followed by a clustering stage thatpartitions entities based on the link decisions.
OurAL method is partially based on the class balancingstrategy proposed by Soon et al (2001).While models other than mention-pair have beenproposed (Culotta et al, 2007), none performsclearly better as evidenced by recent shared evalu-ations such as SemEval 2010 (Recasens et al, 2010)and CoNLL 2011 (Pradhan et al, 2011).Active learning.
The only existing publicationon AL for CR that we are aware of is (Gasperin,2009).
She uses a mention-pair model on a biomed-ical corpus.
The classifier is Naive Bayes and theAL method uncertainty sampling (Lewis and Gale,1994).
The results are negative: AL is not bet-ter than random sampling.
In preliminary experi-ments, we replicated this result for our corpus andour system: Uncertainty sampling is not better thanrandom sampling for CR.
Uncertainty sampling canfail if uncertainty assessments are too unstable forsuccessful example selection (cf.
Dwyer and Holte(2007)).
This seems to be the case for the decisiontrees we use.
Naive Bayes is also known to give baduncertainty assessments (Domingos and Pazzani,5081997).
We therefore adopted a query-by-committeeapproach combined with a class-balancing strategy.3 Active learning for CRThe classifier in the mention-pair model is facedwith a severe class imbalance: there are many moredisreferent than coreferent links.
To address this im-balance, we use a neighborhood pool or N-pool asproposed by Soon et al (2001).Generation of the N-pool.
The neighborhoodof markable x used in N-pooling is defined as theset consisting of the link between x and its closestcoreferent markable y(x) to the left and all disref-erent links in between.
For a particular markable x,let y(x) be the closest coreferent markable for x tothe left of x.
Between y(x) and x, there are disref-erent markables zi, so we have a constellation likey(x), z1, .
.
.
, zn, x.
The neighborhood of x is thenthe set of links{(y, x), (z1, x) .
.
.
, (zn, x)}This set is empty if x does not have a coreferentmarkable to the left.We call the set of all such neighborhoods the N-pool.
The N-pool is a subset of the entire pool oflinks.Bootstrapping the neighborhood.
Soon et al(2001) introduce N-pooling for labeled data.
In AL,no labeled data (or very little of it) is available.
In-stead, we employ the committee of classifiers thatwe use for AL example selection for bootstrappingthe N-pool.
We query the committee of classifiersfrom the last AL iteration and treat a link as coref-erent if and only if the majority of the classifiersclassifies it as coreferent.
We then construct the N-pool using these bootstrapped labels to determinethe coreferent markables y(x) and then construct theneighborhoods as described above.If this procedure yields no coreferent links in aniteration, we sample links left of randomly selectedmarkables instead of N-pooling.Example selection granularity.
We use a query-by-committee approach to AL.
The committee con-sists of 10 instances of the link classifier of the CRsystem, each trained on a randomly chosen subset ofthe links that have been manually labeled so far.In each iteration, the N-pool is recomputed anda small subset of the N-pool is selected for label-ing.
We experiment with two selection granularities.In neighborhood selection, entire neighborhoods areselected and labeled in each iteration.
We define theutility of a neighborhood as the average of the voteentropies (Argamon-Engelson and Dagan, 1999) ofits links.In link selection, individual links with the highestutility are selected ?
in most cases these will be fromdifferent neighborhoods.
Utility is again defined asvote entropy.Our hypothesis is that, compared to selection ofindividual links, neighborhood selection yields amore balanced sample that covers both positive andnegative links for a markable.
At the same time,neighborhood selection retains the benefits of ALsampling: difficult (or highly informative) links areselected.4 ExperimentsWe use the mention-pair CR system SUCRE (Kob-dani et al, 2011).
The link classifier is a deci-sion tree and the clustering algorithm a variant ofbest-first clustering (Ng and Cardie, 2002).
SUCREresults were competitive in SEMEVAL 2010 (Re-casens et al, 2010).
We implemented N-pool boot-strapping and selection methods on top of the ALframework of Tomanek et al (2007).We use the English part of the SemEval-2010 CRtask data set, a subset of OntoNotes 2.0 (Hovy et al,2006).
Training and test set sizes are about 96,000and 24,000 words.
Since we focus on the coref-erence resolution subtask, we use the true mentionboundaries for the markables.The pool for example selection is created by pair-ing every markable with every preceding markablewithin a window of 100 markables.
This yields apool of 1.7 million links, of which only 1.5% arelabeled as coreferent.
This drastic class imbalancenecessitates our bootstrapped class-balancing.We run two baseline experiments for compari-son: (i) random selection on the entire pool, with-out any class balancing, and (ii) random selectionfrom a gold-label-based N-pool.
We chose to usegold neighborhood information for the baseline toremove the influence of badly predicted neighbor-50920,000 links 50,000 linksMUC B3 CEAF mean MUC B3 CEAF mean(1) random entire pool 49.68 86.07 82.34 72.70 48.81 86.00 82.24 72.34(2) N-pooling 61.60 85.00 82.85 76.48 62.60 85.99 83.44 77.33(3) AL link selection 55.65 86.91?
83.67?
75.41 55.84 86.94?
83.70 75.49(4) neighborhood sel.
63.07?
86.94?
84.42?
78.14?
63.81?
87.11?
84.33?
78.42?Table 1: Performance of different methods.
All measures are F1 measures.hoods and focus on the performance of random sam-pling.
Hence, this is a very strong random baseline.The performance with bootstrapped neighborhoodswould likely be lower.We run 10 runs of each experiment, starting from10 different seed sets.
These seed sets contained 200links, drawn randomly from the entire pool, for ran-dom sampling; and 20 neighborhoods for neighbor-hood selection, with a comparable number of links.We verified that each seed set contained instances ofboth classes.5 ResultsWe determine the performance of CR depending onthe number of links used for training.
The resultsof the experiments are shown in Table 1 and Fig-ures 1a to 1d.
We show results for four coreferencemeasures: MUC, B3, entity-based CEAF (hence-forth: CEAF), and the arithmetic mean of MUC, B3and CEAF (as suggested by the CoNLL-2011 sharedevaluation).In all four figures, the AL curves have reached aplateau at 20,000 links.
At this point, neighborhoodselection AL (line 4 in Table 1) outperforms randomsampling from the N-pool (line 2) for all coreferencemeasures, with gains from 1.47 points for MUC to1.94 points for B3.At 20,000 links, the N-pooling random baseline(line 2) has not yet reached maximum performance,but even at 50,000 links, neighborhood selection ALstill outperforms the baselines.
(AL and baselineperformance will eventually converge when mostlinks from the pool are sampled, but this will hap-pen much later, since the pool has 1.7 million linksin total).
?Statistically significant at p < .05 compared to baseline 2using the sign test (N = 10, k ?
9 successes).Link selection AL (line 3) outperforms the base-lines for B3 and CEAF, but is performing markedlyworse than the N-pooling random baseline (line 2)for MUC (due to low recall for MUC) and mean F1.Link selection yields a CR system that proposes alot of singleton entities that are not coreferent withany other entity.
The MUC scoring scheme does notgive credit to singletons at all, thus the lower recall.Neighborhood selection AL initially has lowMUC, but starts to outperform the baseline at 15,000links (Figure 1a).
For B3 and CEAF, neighborhoodselection AL outperforms the baselines much ear-lier, at a few 1000 links (Figures 1b and 1c).
It thusshows more robust performance for all evaluationmetrics.Neighborhood selection AL also performs at leastas well as (for B3) or better than (MUC and CEAF)link selection AL.
Learning curves of neighborhoodselection AL are consistently above the link selec-tion curves.
We therefore consider neighborhood se-lection AL to be the preferred AL setup for CR.6 ConclusionWe have presented a new AL method for corefer-ence resolution.
The proposed method is novel inthree respects.
(i) It uses bootstrapped N-pooling,which ensures a class-balanced pool even thoughgold labels are not available.
(ii) It further improvesclass balancing by neighborhood selection, a selec-tion strategy that ensures coverage of positive andnegative links per markable while still focusing onselecting difficult links.
(iii) It is based on a query-by-committee selection strategy in contrast to ear-lier uncertainty sampling work.
Experiments showthat this new method outperforms random samplingin terms of both annotation effort and peak perfor-mance.510AcknowledgmentsFlorian Laws is a recipient of the Google EuropeFellowship in Natural Language Processing, andthis research is supported in part by his fellowship.Florian Heimerl was supported by the DeutscheForschungsgemeinschaft as part of the priority pro-gram 1335 ?Scalable Visual Analytics?.0 10000 30000 500000.30.40.50.6Links sampledMUC.FRandom, entire poolRandom, N?poolingAL, link selectionAL, neighborhood sel.
(a) Learning curve for MUC0 10000 30000 500000.700.750.800.850.90Links sampledB3.FRandom, entire poolRandom, N?poolingAL, link selectionAL, neighborhood sel.
(b) Learning curve for B30 10000 30000 500000.700.750.800.850.90Links sampledCEAF.E.FRandom, entire poolRandom, N?poolingAL, link selectionAL, neighborhood sel.
(c) Learning curve for CEAF0 10000 30000 500000.600.650.700.750.80Links sampledCONLL.MEAN.FRandom, entire poolRandom, N?poolingAL, link selectionAL, neighborhood sel.
(d) Learning curve for the mean of the CR measures.Figure 1: Learning curves for AL and baseline experiments.
All measures are F1 measures.511ReferencesS.
Argamon-Engelson and I. Dagan.
1999.
Committee-based sample selection for probabilistic classifiers.JAIR, 11:335?360.A.
Culotta, M. Wick, R. Hall, and A. McCallum.
2007.First-order probabilistic models for coreference reso-lution.
In HLT-NAACL 2007.Pedro Domingos and Michael J. Pazzani.
1997.
On theoptimality of the simple bayesian classifier under zero-one loss.
Mach.
Learn., 29(2-3):103?130.K.
Dwyer and R. Holte.
2007.
Decision tree instabilityand active learning.
In ECML.C.
Gasperin.
2009.
Active learning for anaphora resolu-tion.
In Proceedings of the NAACL HLT 2009 Work-shop on Active Learning for Natural Language Pro-cessing.E.
Hovy, M. Marcus, M. Palmer, L. Ramshaw, andR.
Weischedel.
2006.
Ontonotes: The 90% solution.In HLT-NAACL.H.
Kobdani, H. Schu?tze, M. Schiehlen, and H. Kamp.2011.
Bootstrapping coreference resolution usingword associations.
In ACL.D.
Lewis and W. Gale.
1994.
A sequential algorithm fortraining text classifiers.
In SIGIR.V.
Ng and C. Cardie.
2002.
Improving machine learningapproaches to coreference resolution.
In ACL.M.
Osborne and J. Baldridge.
2004.
Ensemble-basedactive learning for parse selection.
In HLT-NAACL.S.
Pradhan, L. Ramshaw, M. Marcus, M. Palmer,R.
Weischedel, and N. Xue.
2011.
Conll-2011 sharedtask: Modeling unrestricted coreference in ontonotes.In CoNLL.M.
Recasens, L. Ma`rquez, E. Sapena, M. A.
Mart??,M.
Taule?, V. Hoste, M. Poesio, and Y. Versley.
2010.Semeval-2010 task 1: Coreference resolution in multi-ple languages.
In Proceedings of the 5th InternationalWorkshop on Semantic Evaluation.E.
Ringger, P. McClanahan, R. Haertel, G. Busby,M.
Carmen, J. Carroll, K. Seppi, and D. Lonsdale.2007.
Active learning for part-of-speech tagging: Ac-celerating corpus annotation.
In Linguistic AnnotationWorkshop at ACL-2007.W.
M. Soon, D. Chung, D. Chung Yong Lim, Y. Lim,and H. T. Ng.
2001.
A machine learning approachto coreference resolution of noun phrases.
Computa-tional Linguistics, 27(4).K.
Tomanek, J. Wermter, and U. Hahn.
2007.
An ap-proach to text corpus construction which cuts annota-tion costs and maintains reusability of annotated data.In EMNLP-CoNLL.S.
Tong and D. Koller.
2002.
Support vector machineactive learning with applications to text classification.JMLR, 2:45?66.512
