The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 180?189,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsVocabulary Profile as a Measure of Vocabulary SophisticationSu-Youn Yoon, Suma Bhat*, Klaus ZechnerEducational Testing Service, 660 Rosedale Road, Princeton, NJ, USA{syoon,kzechner}@ets.org* University of Illinois, Urbana-Champaign, IL, USAsumapramod@gmail.comAbstractThis study presents a method that assessesESL learners?
vocabulary usage to improvean automated scoring system of sponta-neous speech responses by non-native Englishspeakers.
Focusing on vocabulary sophistica-tion, we estimate the difficulty of each wordin the vocabulary based on its frequency ina reference corpus and assess the mean diffi-culty level of the vocabulary usage across theresponses (vocabulary profile).Three different classes of features were gen-erated based on the words in a spoken re-sponse: coverage-related, average word rankand the average word frequency and the extentto which they influence human-assigned lan-guage proficiency scores was studied.
Amongthese three types of features, the average wordfrequency showed the most predictive power.We then explored the impact of vocabularyprofile features in an automated speech scor-ing context, with particular focus on the im-pact of two factors: genre of reference corporaand the characteristics of item-types.The contribution of the current study lies inthe use of vocabulary profile as a measure oflexical sophistication for spoken language as-sessment, an aspect heretofore unexplored inthe context of automated speech scoring.1 IntroductionThis study provides a method that measures ESL(English as a second language) learners?
compe-tence in vocabulary usage.Spoken language assessments typically measuremultiple dimensions of language ability.
Overallproficiency in the target language can be assessedby testing the abilities in various areas including flu-ency, pronunciation, and intonation; grammar andvocabulary; and discourse structure.
With the recentmove toward the objective assessment of languageability (spoken and written), it is imperative that wedevelop methods for quantifying these abilities andmeasuring them automatically.A majority of the studies in automated speechscoring have focused on fluency (Cucchiarini et al,2000; Cucchiarini et al, 2002), pronunciation (Wittand Young, 1997; Witt, 1999; Franco et al, 1997;Neumeyer et al, 2000), and intonation (Zechner etal., 2011).
More recently, Chen and Yoon (2011)and Chen and Zechner (2011) have measured syn-tactic competence in speech scoring.
However, onlya few have explored features related to vocabularyusage and they have been limited to type-token ratio(TTR) related features (e.g., Lu (2011)).
In addi-tion, Bernstein et al (2010) developed vocabularyfeatures that measure the similarity between the vo-cabulary in the test responses and the vocabulary inthe pre-collected texts in the same topic.
However,their features assessed content and topicality, not vo-cabulary usage.The speaking construct of vocabulary usage com-prises two sub-constructs: sophistication and preci-sion.
The aspect of vocabulary that we intend tomeasure in this paper is that of lexical sophistication,also termed lexical diversity and lexical richness insecond language studies.
Measures of lexical so-phistication attempt to quantify the degree to whicha varied and large vocabulary is used (Laufer andNation, 1995).
In order to assess the degree of lex-180ical sophistication, we employ a vocabulary profile-based approach (partly motivated from the results ofa previous study, as will be explained in Section 2).By a vocabulary profile, it is meant that the fre-quency of each vocabulary item is calculated froma reference corpus covering the language variety ofthe target situation.
The degree of lexical sophisti-cation is captured by the word frequency - low fre-quency words are considered to be more difficult,and therefore more sophisticated.
We then designfeatures that capture the difficulty level of vocabu-lary items in test takers?
responses.
Finally, we per-form correlation analyses between these new fea-tures and human proficiency scores and assess thefeature?s importance with respect to the other fea-tures in an automatic scoring module.
The noveltyof this study lies in the use of vocabulary profile inan automatic scoring set-up to assess lexical sophis-tication.This paper will proceed as follows: we will re-view related work in Section 2.
Data and experimentsetup will be explained in Section 3 and Section 4.Next, we will present the results in Section 5, discussthem in Section 6, and conclude with a summary ofthe importance of our findings in Section 7.2 Related WorkMeasures of lexical richness have been the focus ofseveral studies involving assessment of L1 and L2language abilities (Laufer and Nation, 1995; Ver-meer, 2000; Daller et al, 2003; Kormos and Denes,2004).
The types of measures considered in thesestudies can be grouped into quantitative and qualita-tive measures.The quantitative measures give insight into thenumber of words known, but do not distinguish themfrom one another based on their category or fre-quency in language use.
They have evolved to makeup for the widely applied measure type-token-ratio(TTR).
However, owing to its sensitivity to the num-ber of tokens, TTR has been considered as an un-stable measure in differing proficiency levels of lan-guage learners.
The Guiraud index, Uber index, andHerdan index (Vermeer, 2000; Daller et al, 2003;Lu, 2011) are some measures in this category mostlyderived from TTR as either simpler transformationsof the TTR or its scaled versions to ameliorate theeffect of differing token cardinalities.Qualitative measures, on the other hand, dis-tinguish themselves from those derived from TTRsince they take into account distinctions betweenwords such as their parts of speech or difficulty lev-els.
Adding a qualitative dimension gives more in-sight into lexical aspects of language ability thanthe purely quantitative measures such as TTR-basedmeasures.
Some measures in this category in-clude a derived form of the limiting relative diver-sity (LRD) given by?D(verbs)/D(nouns) usingthe D-measure proposed in (Malvern and Richards,1997), Lexical frequency profile (LFP) (Laufer andNation, 1995) and P-Lex (Meara and Bell, 2003).LFP uses a vocabulary profile (VP) for a givenbody of written text or spoken utterance and givesthe percentage of words used at different frequencylevels (such as from the one-thousand most com-mon words, the next thousand most common words)where the words themselves come from a pre-compiled vocabulary list, such as the AcademicWord List (AWL) with its associated frequency dis-tribution on words by Coxhead(1998).
Frequencylevel refers to a class of words (or appropriately cho-sen word units) that are grouped based on their fre-quencies of actual usage in corpora.
P-Lex is an-other approach that uses the frequency level of thewords to assess lexical richness.
These measures arebased on the differing frequencies of lexical itemsand hence rely on the availability of frequency listsfor the language being considered.These two different types of measures have beenused in the analysis of essays written by second lan-guage learners of English (ESL).
Laufer and Nation(1995) have shown that LFP correlates well with anindependent measure of vocabulary knowledge andthat it is possible to categorize learners according todifferent proficiency levels using this measure.
Inanother study seeking to understand the extent towhich VP based on students?
essays predicted theiracademic performance (Morris and Cobb, 2004), itwas observed that students?
vocabulary profile re-sults correlated significantly with their grades.
Ad-ditionally, VP was found to be indicative of finer dis-tinctions in the language skills of high proficiencynonnative speakers than oral interviews can cover.Furthermore, these measures have been employedin automated essay scoring.
Attali and Burstein181(2006) used average word frequency and averageword length in characters across the words in theessay.
In addition to the average word frequencymeasure, the average word length measure was im-plemented to assess the average difficulty of theword used in the essay under the assumption thatthe words with more characters were more difficultthan the words with fewer characters.
These fea-tures showed promising performance in estimatingtest takers?
proficiency levels.In contrast to qualitative measures, quantitativemeasures did not achieve promising performance.Vermeer (2000) showed that quantitative measuresachieve neither the validity nor the reliability of themeasures, regardless of the transformations and cor-rections.More recently, the relationship of lexical rich-ness to ESL learners?
speaking task performancehas been studied by Lu (2011).
The comprehensivestudy was aimed at measuring lexical richness alongthe three dimensions of lexical density, sophistica-tion, and variation, using 25 different metrics (be-longing to both the qualitative and quantitative cate-gories above) available in the language acquisitionliterature.
His results, based on the manual tran-scription of a spoken corpus of English learners, in-dicate that a) lexical variation (the number of wordtypes) correlated most strongly with the raters?
judg-ments of the quality of ESL learners?
oral narratives,b) lexical sophistication only had a very small ef-fect, and c) lexical density (indicative of proportionof lexical words) in an oral narrative did not appearto relate to its quality.In this study, we seek to quantify vocabulary us-age in terms of measures of lexical sophistication:VP based on a set of reference word lists.
The nov-elty of the current study lies in the use of VP asa measure of lexical sophistication for spoken lan-guage assessment.
It derives support from otherstudies (Morris and Cobb, 2004; Laufer and Nation,1995) but is carried out in a completely differentcontext, that of automatic scoring of proficiency lev-els in spontaneous speech, an area not explored thusfar in existing literature.Furthermore, we investigate the impact of thegenre of the reference corpus on the performance ofthese lexical measures.
For this purpose, three dif-ferent corpora will be used to generate reference fre-quency levels.
Finally, we will investigate how thecharacteristics of the item types influence the perfor-mance of these measures.3 DataThe AEST balanced data set, a collection of re-sponses from the AEST, is used in this study.AEST is a high-stakes test of English proficiency,and it consists of 6 items in which speakers areprompted to provide responses lasting between 45and 60 seconds per item, yielding approximately 5minutes of spoken content per speaker.Among the 6 items, two items elicit informationor opinions on familiar topics based on the exam-inees?
personal experience or background knowl-edge.
These constitute the independent (IND) items.The four remaining items are integrated tasks thatinclude other language skills such as listening andreading.
These constitute the integrated (INT)items.
Both sets of items extract spontaneous andunconstrained natural speech.
The primary dif-ference between the two elicitation types is thatIND items only provide a prompt whereas INT itemsprovide a prompt, a reading passage, and a listeningstimulus.
The size, purpose, and speakers?
nativelanguage information for each dataset are summa-rized in Table 1.
All items extract spontaneous, un-constrained natural speech.Each response was rated by a trained human raterusing a 4-point scoring scale, where 1 indicatesa low speaking proficiency and 4 indicates a highspeaking proficiency.
The scoring guideline is sum-marized in the AEST rubrics.Since none of the AEST balanced data wasdouble-scored, we estimate the inter-rater agreementratio of the corpus by using a large double-scoreddataset which used the same scoring guidelines andscoring process; using the 41K double-scored re-sponses collected from AEST, we calculate the Pear-son correlation coefficient to be 0.63, suggesting areasonable agreement.
The distribution of scores forthis data can be found in Table 2.4 Experiments4.1 OverviewIn this study, we developed vocabulary profile fea-tures.
From a reference corpus, we pre-compiled182CorpusnamePurpose # ofspeakers# of re-sponsesNative languages Size(Hrs)AEST bal-anced dataFeature evaluation, Scor-ing model training andevaluation480 2880 Korean (15%), Chinese (14%),Japanese (7%), Spanish (9%),Others (55%)44Table 1: Data size and speakers?
native languagesSize Score1 Score2 Score3 Score4Numberof files141 1133 1266 340(%) 5 40 45 12Table 2: Distribution of proficiency scores in the datasetmultiple sets of vocabulary lists (e.g., a list of the100 most frequent words in a reference corpus).Next, for each test response, a transcription was gen-erated using the speech recognizer.
For each re-sponse with respect to each reference word list, vo-cabulary profile features were calculated.
In addi-tion to vocabulary profile features, type-token ratio(TTR) was calculated as a baseline feature.
Despiteits instability, TTR has been employed in the auto-mated speech scoring systems such as (Zechner etal., 2009), and its use here allows a direct compar-ison of the performance of the features with the re-sults of previous studies.4.2 Vocabulary list generationThe three reference corpora we used in this studyare presented in Table 3: The General ServiceList (GSL), the TOEFL 2000 Spoken and WrittenAcademic Language Corpus (T2K-SWAL) and theAEST data.Corpus Genre Tokens TypesGSL Written - 2,284T2K-SWAL Spoken 1,869,346 28,855AEST data Spoken 5,520,375 23,165Table 3: Three reference corpora used in this studyGSL (West, 1953) comprises 2,284 words se-lected to be of ?general service?
to learners of En-glish.
In this study, we used the version with fre-quency information from (Bauman, 1995).
The orig-inal version did not include word frequency andwas ?enhanced?
by John Bauman and Brent Culli-gan with the frequency information obtained fromthe Brown Corpus, a collection of written texts.T2K-SWAL (Biber et al, 2002) is a collection ofspoken and written texts covering a broad languagevariety and use in the academic setting.
In this study,only its spoken texts were used.
The spoken corpusincluded manual transcriptions of discussions, con-versations, and lectures that occurred in class ses-sions, study-group meetings, office hours, and ser-vice encounters.Finally, AEST data is a collection of manual tran-scriptions of spoken responses from the AEST fornon-native English speakers.
Although there was nooverlap between AEST data and the evaluation data(AEST balanced data), the vocabulary lists in AESTdata might be a closer match to the vocabulary listsin the evaluation data since both of them come fromthe same test products.
From a content perspective,this dataset is likely to better reflect characteristicsof non-native English speakers than the other tworeference corpora.For T2K-SWAL and AEST, all transcriptionswere normalized; all the tokens were further de-capitalized and removed of all non-alphanumericcharacters except for dash and quote.
The morpho-logical variants were considered as different words.All words were sorted by the word occurrences inthe corpus, and a set of 6 lists were generated:top-100 words (TOP1), word frequency ranks 101-300 (TOP2), ranks 301-700 (TOP3), ranks 701-1500(TOP4), ranks 1501-3000 (TOP5), and all otherwords with ranks of 3001 and above (TOP6).
ForGSL, a set of 5 lists was generated; TOP6 wasnot generated since GSL only included about 2200words.Compared to written texts, speakers tended to usea much smaller vocabulary in speech.
For instance,the percentage of words within the top-1000 wordson the total word types of AEST data responses wasover 90% on average, and they were similar across183proficiency levels.
This is the reason why we sub-classified the top 1000 words into three lists, unlikethe vocabulary profile features using top-1000 wordsas one list like (Morris and Cobb, 2004), which didnot have any power to differentiate between profi-ciency levels.4.3 Transcription generation for evaluationdataA Hidden Markov Model (HMM) speech recognizerwas trained on the AEST dataset, approximately733 hours of non-native speech collected from 7872speakers.
A gender independent triphone acousticmodel and a combination of bigram, trigram, andfour-gram language models was used.
The worderror rate (WER) on the held-out test dataset was27%.
For each response in the evaluation partition,an ASR-based transcription was generated using thespeech recognizer.4.4 Feature generationEach response comprised less than 60 seconds ofspeech with an average of 113 word tokens.
Dueto the short response length, there was wide varia-tion in the proportion of low-frequency word typesfor the same speaker.
In order to address this issue,for each speaker, two responses from the same item-type (IND/INT) were concatenated and used as onelarge response.
As a result, three concatenated re-sponses (one IND response and two INT responses)were generated for each speaker, yielding a total of480 concatenated responses for IND items and 960concatenated responses for INT items for our exper-iment.First, a list of word types was generated fromthe ASR hypothesis of each concatenated response.IND items provide only a one-sentence prompt,while INT items provide stimuli including a prompt,a reading passage, and a listening stimulus.
In orderto minimize the influence of the vocabulary in thestimuli on that of the speakers, we excluded the con-tent words that occurred in the prompts or stimulifrom the word type list1.1This process prevents to measure the content relevance;whether the response is off-topic or not.
However, this is notproblematic since the features in this study will be used in theconjunction with the features that measure the accuracy of theaspects of content and topicality such as (Xie et al, 2012)?s fea-Table 4: List of features.Feature # of Feature Descriptionfeatures typeTTR 1 Ratio Type-token ratioTOPn 5 or 6a Listrel Proportion of typesthat occurred boththe response andTOPn list in the to-tal types of the re-sponse.aRank 1 Rank Avg.
word rankbaFreq 1 Freq Avg.
word freq.clFreq 1 Freq Avg.
log(wordfreq)da For GSL, five different features were created usingTOP1-TOP5 lists, but TOP6 was not created.
ForT2K-SWAL and AEST data, six different features werecreated using TOP1-TOP6 lists separately.b ?rank?
is the ordinal number of words in a list that is sorted indescending order of word frequency; words not present in thereference corpus get the default rank of RefMaxRank+1.c Avg.
word frequency is the sum of the word-frequencies ofword types in the reference corpus divided by the totalnumber of words in the reference corpus; words not in thereference corpus get assigned a default frequency of 1.d Same as feature aFreq, but the logarithm of the wordfrequency is taken hereNext, we generated five types of features usingthree reference vocabulary lists.
A maximum of 10features were generated for each reference list.
Thefeature-types are tabulated in Table 4.All features above were generated from wordtypes, not word tokens, i.e., multiple occurrences ofthe same word in a response were only counted once.Below we delineate the step-by-step process witha sample response that leads to the feature genera-tion outlined in Table 5.?
Step 1: Generate ASR hypothesis for the givenspeech response.
e.g: Every student has dif-ferent perspective about how to relax.
Playingxbox.?
Step 2: Generate type list from ASR hypoth-esis.
For the response above we get the list- about, how, different, xbox, to, relax, every,perspective, student, has, playing.tures.184wordfreq.
inreferencecorpusword rank inthe referencecorpusTOPnabout 25672 30 TOP1how 8944 96 TOP1has 18105 53 TOP1to 218976 2 TOP1different 5088 153 TOP2every 2961 236 TOP2playing 798 735 TOP4perspec-tive139 1886 TOP5xbox 1 20000 NoTable 5: An example of feature calculation.?
Step 3: Generate type list excluding words thatoccurred in the prompt - about, how, different,xbox, to, every, perspective, has, playing.From the ASR hypotheses (result of Step 1), thecorresponding type list was generated (Step 2) andtwo words (?student?, ?relax?)
were excluded fromthe final list due to overlap with the prompt.
Thefinal word list used in the feature generation has 9types (Step 3).Next, for each word in the above type list, if it oc-curs in the reference corpus (a list of words sortedby frequency), its word frequency, word rank andthe TOPn information (whether the word belongedto the TOPn list or not) are obtained.
If it did not oc-cur in the reference corpus, the default frequency (1)and the default word rank (20000) were assigned.
In5, the default values were assigned for ?xbox?
sinceit was not in the reference corpus.Finally, the average of the word frequencies andthe average of the the word ranks were calculated(aFreq and aRank).
For lFreq, the log value of eachfrequency was calculated and then averaged.
ForTOPn features, we obtain the proportion of the wordtypes that belong to the TOPn category.
For theabove sample, the TOP1 feature value was 0.444since 4 words belong to TOP1 and the total numberof word types was 9 (4/9=0.444).5 Results5.1 CorrelationWe analyzed the relationship between the proposedfeatures and human proficiency scores to assess theirinfluence on predicting the proficiency score.
Thereference proficiency score for a concatenated re-sponse was estimated by summing up the two scoresof the constituent responses.
Thus, the new scorescale was 2-8.
Table 6 presents Pearson correlationcoefficients (r).The best performing feature was aFreq followedby TOP1.
Both features showed statistically signif-icant negative correlations with human proficiencyscores.
TOP6 also showed statistically significantcorrelation with human scores, but it was 10-20%lower than TOP1.
This suggests that a human ratermore likely assigned high scores when the vocabu-lary of the response was not limited to a few mostfrequent words.
However, the use of difficult words(low-frequency) shows a weaker relationship withthe proficiency scores.Features based on AEST data outperformed fea-tures based on T2K-SWAL or GSL.
The correlationof the AEST data-based aFreq feature was ?0.61for the IND items and?0.51 for the INT items; theywere approximately 0.1 higher than the correlationsof T2K-SWAL or GSL-based features.
A similartendency was found for the TOP1-TOP6 features,although differences between AEST data-based fea-tures and other reference-based features were lesssalient overall.For top-performing vocabulary profile featuresincluding aFreq and TOP1, the correlations ofINT items were weaker than those of the IND items.In general, the correlations of INT items were 10-20% lower than those of the IND items in absolutevalue.aFreq and TOP1 consistently achieved betterperformance than TTR across all item-types.5.2 Scoring model buildingTo arrive at an automatic scoring model, we includedthe new vocabulary profile features with other fea-tures previously found to be useful in a multiple lin-ear regression (MLR) framework.
A total of 80 fea-tures were generated by the automated speech pro-ficiency scoring system from Zechner et al (2009),185Reference TTR TOP1 TOP2 TOP3 TOP4 TOP5 TOP6 aRank aFreq lFreqIND GSL -.147 -.347 .027 .078 .000 .053 - .266 -.501 -.260T2K-SWAL -.147 -.338 .085 .207 .055 .020 .168 .142 -.509 -.159ATEST -.147 -.470 .014 .275 .172 .187 .218 .236 -.613 -.232INT GSL -.245 -.255 -.086 -.019 -.068 -.031 - .316 -.404 -.318T2K-SWAL -.245 -.225 .010 .094 .047 .079 .124 .087 -.405 -.198ATEST -.245 -.345 -.092 .156 .135 .188 .194 .214 -.507 -.251Table 6: Correlations between features and human proficiency scoresand they were classified into 5 sub-groups: fluency,pronunciation, prosody, vocabulary complexity, andgrammar usage.
For each sub-group, at least onefeature that correlated well with human scores buthad a low inter-correlation with other features wasselected.
A total of following 6 features were se-lected and used in the base model (base):?
wdpchk (fluency): Average chunk length in words;a chunk is a segment whose boundaries are set bylong silences?
tpsecutt (fluency): Number of types per sec.?
normAM (pronunciation): Average acoustic modelscore normalized by the speaking rate?
phn shift (pronunciation): Average absolute dis-tance of the normalized vowel durations comparedto standard normalized vowel durations estimatedon a native speech corpus?
stretimdev (prosody): Mean deviation of distancebetween stressed syllables in sec.?
lmscore (grammar): Average language model scorenormalized by number of wordsWe first calculated correlations between these fea-tures and human proficiency scores and comparedthem with the most predictive vocabulary profilefeatures.
Table 7 presents Pearson correlation co-efficients (r) of these features.In both item-types, the most correlated featuresrepresented the aspect of fluency in production.While tpsecutt was the best feature in IND itemsand the correlation with human scores was approx-imately 0.66, in INT items, wdpchk was the bestfeature and the correlation was even higher, 0.73.The performance of aFreq was particularly highin IND items; it was the second best feature and onlymarginally lower than the best feature (by 0.04).aFreq also achieved promising performance in INT;Features IND INTwdpchk .538 .612tpsecutt .659 .729normAM .467 .429phn shift -.503 -.535stretimemdev -.442 -.397lmscore .257 .312aFreq -.613 -.507TOP1 -.470 -.345TTR -.147 -.245Table 7: Comparison of feature-correlations with human-assigned proficiency scores.it was the fourth best feature.
However, the perfor-mance was considerably lower than the the best fea-ture, and the difference between the best feature andaFreq was approximately 22%.We compared the performances of this basemodel with an augmented model (base + TTR + allvocabulary profile features) whose feature set wasthe base augmented with our proposed measures ofvocabulary sophistication.
Item-type specific multi-ple linear regression models were trained using five-fold cross validation.
The 480 IND responses 960INT responses were partitioned into five sets, sepa-rately.
In each fold, an item-type specific regressionmodel was trained using four of these partitions andtested on the remaining one.The averages of the five-fold models are sum-marized in Table 8, showing weighted kappa toindicate agreement between automatic scores andhuman-assigned scores and also the Pearson?s cor-relation (r) of the unrounded (un-rnd) and rounded(rnd) scores with the human-assigned scores.
Weused the correlation and weighted kappa as perfor-mance evaluation measures to maintain the consis-tency with the previous studies such as (Zechneret al, 2009).
In addition, the correlation metric186matches better with our goal to investigate the rela-tionship between the predicted scores and the actualscores rather than the difference between the pre-dicted scores and the actual scores.Features un-rndcorr.rndcorr.weightedkappaIND base 0.66 0.62 0.55base + TTR 0.66 0.63 0.56base + TTR +all0.66 0.64 0.57INT base 0.76 0.73 0.69base + TTR 0.76 0.74 0.70base + TTR +all0.77 0.74 0.70Table 8: Performance of item-type specific multiple lin-ear regression based scoring models.The new scores show slightly better agreementwith human-assigned scores, but the improvementwas small in both item-types, approximately 1%.6 DiscussionIn general, we found that the test takers used a fairlysmall number of vocabulary items in the spoken re-sponses.
On average, the total types used in theresponses was 87.21 for IND items and 98.52 forINT items.
Furthermore, the proportions of highfrequency words on test takers?
spoken responseswere markedly high.
The proportion of top-100words was almost 50% and the proportion of top-1500 words (summation of TOP1-TOP4) was over89% on average.
This means that only 1500 wordsrepresent almost 90% of the active vocabulary ofthe test takers in their spontaneous speech.
Figure1 presents the average TOP1-TOP6 features acrossall proficiency levels.The values of INT items were similar to INDitems, but the TOP3-TOP6 values were slightlyhigher than IND items; INT items tended to includemore low frequency words.
In order to investigatethe impact of the higher proportion of low frequencywords in INT items, we selected two features (aFreqand TOP1) and further analyzed them.Table 9 provides the mean of aFreq and TOP1 foreach score level.
The features were generated usingAEST as a reference.Figure 1: Proportion of top-N frequent words on averageScore aFreq TOP1IND INT IND INT2 43623 36175 .60 .523 38165 32493 .55 .494 33861 28884 .51 .485 30599 27118 .49 .466 28485 26327 .46 .457 27358 25093 .45 .438 26065 24711 .43 .43Table 9: Mean of vocabulary profile features for eachscore levelOn average, the differences between adjacentscore levels in INT items were smaller than thosein IND items.
The weaker distinction between scorelevels may result in the lower performance of vo-cabulary profile features in INT items.
Particularly,the differences were smaller in lower score levels (2-4) than in higher score levels (5-8).
The relativelyhigh proportion of low frequency words in the lowscore level reduced the predictive power of vocabu-lary profile features.This difference between the item-types stronglysupports item-type-specific modeling.
We combinedthe IND and INT item responses and computeda correlation between the features and the profi-ciency scores over the entirety of data sets.
De-spite increase in sample sizes, the correlations werelower than both the corresponding correlations ofthe IND items and the INT items.
For instance, thecorrelation of the T2K-SWAL-based aFreq featurewas?0.393, and that of the AEST data-based aFreqwas?0.50, which was approximately 3% lower thanthe INT items and 10% lower than the IND items.The difference in the vocabulary distributions be-tween the two item-types decreased the performance187of the features.In this study, AEST data-based features outper-formed T2K-SWAL-based features.
Although noitems in the evaluation data overlapped with itemsin AEST data, the similarity in the speakers?
profi-ciency levels and task types might have resulted ina better match between the vocabulary and its dis-tributions of AEST data with AEST balanced data,finally the AEST data-based features achieved thebest performance.In order to explore the degree to which AEST bal-anced data (test responses) and the reference cor-pora matched, we calculated the proportion of wordtypes that occurred in test responses and referencecorpora (the coverage of reference list).
The ASRhypotheses of AEST balanced data comprised 6,024word types.
GSL covered 73%, T2K-SWAL cov-ered 99%, and AEST data covered close to 100%.Considering the fact that, a) despite high coverageof both T2K-SWAL and AEST data, T2K-SWAL-based features achieved much lower performancethan AEST data, and, b) despite huge differencesin the coverage between T2K-SWAL and GSL, theperformance of features based on these referencecorpora were comparable, coverage was not likelyto have been a factor having a strong impact on theperformance.
The large differences in the perfor-mance of TOP1 across reference lists support thepossibility of the strong influence of high frequencyword types on proficiency; the kinds of word typesthat were in the TOP1 bins were an important factorthat influenced the performance of vocabulary pro-file features.
Finally, genre differences (spoken textsvs.
written texts) in reference corpora did not havestrong impact on the predictive ability of the fea-tures; the performance of features based on writtenreference corpus (GSL) were comparable to thosebased on a spoken reference corpus (T2K-SWAL).Despite the high correlation shown by the indi-vidual features (such as aFreq), we do not see a cor-responding increase in the performance of the scor-ing model with all the best performing features.
Themost likely explanation to this is the small trainingdata size; in each fold, only about 380 responses forIND and about 760 responses for INT were usedin the scoring model training.
Another possibilityis overlap with the existing features; the aspect thatvocabulary profile features are modeling may be al-ready covered to some extent in existing feature set.In future research, we will further investigate this as-pect in details.7 ConclusionsIn this study, we presented features that measureESL learners?
vocabulary usage.
In particular, wefocused on vocabulary sophistication, and exploredthe suitability of vocabulary profile features to cap-ture sophistication.
From three different referencecorpora, the frequency of vocabulary items was cal-culated which was then used to estimate the sophis-tication of test takers?
vocabulary.
Among the threedifferent reference corpora, features based on AESTdata, a collections of responses similar to that of thetest set, showed the best performance.
A total of 29features were generated, and the average word fre-quency (aFreq) achieved the best correlation withhuman proficiency scores.
In general, vocabularyprofile features showed strong correlations with hu-man proficiency scores, but when used in an auto-matic scoring model in combination with an existingset of predictors of language proficiency, the aug-mented feature set showed marginal improvement inpredicting human-assigned scores of proficiency.ReferencesYigal Attali and Jill Burstein.
2006.
Automated essayscoring with e?rater R v.2.
The Journal of Technology,Learning, and Assessment, 4(3).John Bauman.
1995.
About the GSL.
RetrievedMarch 17, 2012 from http://jbauman.com/gsl.html.Jared Bernstein, Jian Cheng, and Masanori Suzuki.
2010.Fluency and structural complexity as predictors of L2oral proficiency.
In Proceedings of InterSpeech 2010,Tokyo, Japan, September.Douglas Biber, Susan Conrad, Randi Reppen, Pat Byrd,and Marie Helt.
2002.
Speaking and writing in theuniversity: A multidimensional comparison.
TESOLQuarterly, 36:9?48.Lei Chen and Su-Youn Yoon.
2011.
Detecting structuralevents for assessing non-native speech.
In Proceed-ings of the 6th Workshop on Innovative Use of NLP forBuilding Educational Applications, pages 38?45.Miao Chen and Klaus Zechner.
2011.
Computing andevaluating syntactic complexity features for automatedscoring of spontaneous non-native speech.
In Pro-188ceedings of the 49th Annual Meeting of the Associationfor Computational Linguistics 2011, pages 722?731.Catia Cucchiarini, Helmer Strik, and Lou Boves.
2000.Quantitative assessment of second language learners?fluency: Comparisons between read and spontaneousspeech.
The Journal of the Acoustical Society of Amer-ica, 107(2):989?999.Catia Cucchiarini, Helmer Strik, and Lou Boves.
2002.Quantitative assessment of second language learners?fluency: Comparisons between read and spontaneousspeech.
The Journal of the Acoustical Society of Amer-ica, 111(6):2862?2873.Helmut Daller, Roeland van Hout, and Jeanine Treffers-Daller.
2003.
Lexical richness in the spontaneousspeech of bilinguals.
Applied Linguistics, 24(2):197?222.Horacio Franco, Leonardo Neumeyer, Yoon Kim, andOrith Ronen.
1997.
Automatic pronunciation scoringfor language instruction.
In Proceedings of ICASSP97, pages 1471?1474.Judit Kormos and Mariann Denes.
2004.
Exploring mea-sures and perceptions of fluency in the speech of sec-ond language learners.
System, 32:145?164.Batia Laufer and Paul Nation.
1995.
Vocabulary size anduse: lexical richness in L2 written production.
AppliedLinguistics, 16:307?322.Xiaofei Lu.
2011.
The relationship of lexical richnessto the quality of ESL learners?
oral narratives.
TheModern Language Journal.David D. Malvern and Brian J. Richards.
1997.
Anew measure of lexical diversity.
In Evolving mod-els of language: Papers from the Annual Meeting ofthe British Association of Applied Linguists held at theUniversity of Wales, Swansea, September, pages 58?71.Paul Meara and Huw Bell.
2003.
P lex: A simple andeffective way of describing the lexical characteristicsof short L2 texts.
Applied Linguistics, 24(2):197?222.Lori Morris and Tom Cobb.
2004.
Vocabulary profilesas predictors of the academic performance of teachingenglish as a second language trainees.
System, 32:75?87.Leonardo Neumeyer, Horacio Franco, Vassilios Di-galakis, and Mitchel Weintraub.
2000.
Automaticscoring of pronunciation quality.
Speech Communi-cation, pages 88?93.Anne Vermeer.
2000.
Coming to grips with lexical rich-ness in spontaneous speech data.
Language Testing,17(1):65?83.Michael West.
1953.
A General Service List of EnglishWords.
Longman, London.Silke Witt and Steve Young.
1997.
Performance mea-sures for phone-level pronunciation teaching in CALL.In Proceedings of the Workshop on Speech Technologyin Language Learning, pages 99?102.Silke Witt.
1999.
Use of the speech recognition incomputer-assisted language learning.
Unpublisheddissertation, Cambridge University Engineering de-partment, Cambridge, U.K.Shasha Xie, Keelan Evanini, and Klaus Zechner.
2012.Exploring content features for automated speech scor-ing.
In Proceedings of the NAACL-HLT, Montreal,July.Klaus Zechner, Derrick Higgins, Xiaoming Xi, andDavid M. Williamson.
2009.
Automatic scoring ofnon-native spontaneous speech in tests of spoken en-glish.
Speech Communication, 51:883?895, October.Klaus Zechner, Xiaoming Xi, and Lei Chen.
2011.
Eval-uating prosodic features for automated scoring of non-native read speech.
In IEEE Workshop on AutomaticSpeech Recognition and Understanding 2011, Hawaii,December.189
