Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 638?647,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPImproving Verb Clustering with Automatically Acquired SelectionalPreferencesLin Sun and Anna KorhonenUniversity of Cambridge, Computer Laboratory15 JJ Thomson Avenue, Cambridge CB3 0GD, UKls418,alk23@cl.cam.ac.ukAbstractIn previous research in automatic verbclassification, syntactic features haveproved the most useful features, althoughmanual classifications rely heavily on se-mantic features.
We show, in contrastwith previous work, that considerable ad-ditional improvement can be obtained byusing semantic features in automatic clas-sification: verb selectional preferences ac-quired from corpus data using a fully unsu-pervised method.
We report these promis-ing results using a new framework forverb clustering which incorporates a re-cent subcategorization acquisition system,rich syntactic-semantic feature sets, anda variation of spectral clustering whichperforms particularly well in high dimen-sional feature space.1 IntroductionVerb classifications have attracted a great dealof interest in natural language processing (NLP).They have proved useful for various importantNLP tasks and applications, including e.g.
parsing,word sense disambiguation, semantic role label-ing, information extraction, question-answering,and machine translation (Swier and Stevenson,2004; Dang, 2004; Shi and Mihalcea, 2005; Za-pirain et al, 2008).Verb classes are useful because they offer apowerful tool for generalization and abstractionwhich can be beneficial when faced e.g.
with theproblem of data sparsity.
Particularly useful canbe classes which capture generalizations over arange of (cross-)linguistic properties, such as theones proposed by Levin (1993).
Being defined interms of similar meaning and (morpho-)syntacticbehaviour of words, Levin style classes gener-ally incorporate a wider range of properties thane.g.
classes defined solely on semantic grounds(Miller, 1995).In recent years, a variety of approaches havebeen proposed for automatic induction of verbclasses from corpus data (Schulte im Walde, 2006;Joanis et al, 2008; Sun et al, 2008; Li and Brew,2008; Korhonen et al, 2008;?O S?eaghdha andCopestake, 2008; Vlachos et al, 2009).
This workopens up the opportunity of learning and tuningclassifications tailored to the application and do-main in question.
Although manual classificationmay always yields higher accuracy, automatic verbclassification is cost-effective and gathers statisti-cal information as a side-effect of the acquisitionprocess which is difficult for humans to gather butcan be highly useful for NLP applications.To date, both supervised and unsupervised ma-chine learning (ML) methods have been proposedfor verb classification and used to classify a vari-ety of features extracted from raw, tagged and/orparsed corpus data.
The best performing featureson cross-domain verb classification have been syn-tactic in nature (e.g.
syntactic slots, subcategoriza-tion frames (SCFs)).
Disappointingly, semanticfeatures have not yielded significant additional im-provement, although they play a key role in man-ual and theoretical work on verb classification andcould thus be expected to offer a considerable con-tribution to classification performance.Since the accuracy of automatic verb classifi-cation shows room for improvement, we furtherinvestigate the potential of semantic features ?verb selectional preferences (SPs) ?
for the task.We introduce a novel approach to verb cluster-ing which involves the use of (i) a recent subcate-gorization frame (SCF) acquisition system (Preisset al, 2007) which produces rich lexical, SCF andsyntactic data, (ii) novel syntactic-semantic fea-ture sets extracted from this data which incorpo-rate a variety of linguistic information, includingSPs, and (iii) a new variation of spectral cluster-638ing based on the MNCut algorithm (Meila and Shi,2001) which is well-suited for dealing with the re-sulting, high dimensional feature space.Using this approach, we show on two well-established test sets that automatically acquiredSPs can be highly useful for verb clustering.
Theyyield high performance when used in combinationwith syntactic features.
We obtain our promis-ing results using a fully unsupervised approachto SP acquisition which differs from previousapproaches in that it does not exploit WordNet(Miller, 1995) or other lexical resources.
It isbased on clustering argument head data in thegrammatical relations associated with verbs.We describe our features in section 2 and theclustering methods in section 3.
Experimentalevaluation and results are reported in sections 4and 5, respectively.
Section 6 provides discus-sion and describes related work, and section 7 con-cludes.2 FeaturesOur target classification is the taxonomy of Levin(1993) where verbs taking similar diathesis al-ternations are assumed to share meaning compo-nents and are organized into semantically coherentclasses.
The main feature of this classification is adiathesis alternation which manifests at the levelof syntax in alternating sets of SCF (e.g.
in thecausative/inchoative alternation an NP frame alter-nates with an intransitive frame: Tony broke thewindow?
The window broke).Since automatic detection of diathesis alterna-tions is very challenging (McCarthy, 2001), mostwork on automatic classification has exploited thefact that similar alternations tend to result in sim-ilar SCFs.
The research reported so far1has usedmainly syntactic features for classification, rang-ing from shallow syntactic slots (e.g.
NPs preced-ing or following the verb) to SCFs.
Some re-searchers have discovered that supplementing ba-sic syntactic features with information about ad-juncts, co-occurrences, tense, and/or voice of theverb have resulted in better performance.However, additional information about seman-tic SPs of verbs has not yielded considerable im-provement on verb classification although SPs canbe strong indicators of diathesis alternations (Mc-Carthy, 2001) and although fairly precise semanticdescriptions, including information about verb se-1See section 6 for discussion on previous work.lectional restrictions, can be assigned to the major-ity of Levin classes, as demonstrated by VerbNet(Kipper-Schuler, 2005).SP acquisition from undisambiguated corpusdata is arguably challenging (Brockmann and La-pata, 2003; Erk, 2007; Bergsma et al, 2008).
It isespecially challenging in the context of verb clas-sification where SP models are needed for specificsyntactic slots for which the data may be sparse,and the resulting feature vectors integrating bothsyntactic and semantic features may be high di-mensional.
However, we wanted to investigatewhether better results could be obtained if the fea-tures were optimised for richness, the feature ex-traction for accuracy, and a clustering method ca-pable of dealing with the resulting high dimen-sional feature space was employed.2.1 Feature extractionWe adopted a recent SCF acquisition system whichhas proved more accurate than previous compa-rable systems2but which has not been employedfor verb clustering before: the system of Preisset al (2007).
This system tags, lemmatizes andparses corpus data using the current version of theRASP (Robust Accurate Statistical Parsing) toolkit(Briscoe et al, 2006), and on the basis of resultinggrammatical relations (GRs) assigns each occur-rence of a verb to one of 168 verbal SCFs classes3.The system provides a filter which can be usedto remove adjuncts from the resulting lexicon.We do not employ this filter since adjuncts haveproved informative for verb classification (Sunet al, 2008; Joanis et al, 2008).
However, wedo frequency-based thresholding to minimise thenoise (e.g.
erroneous scfs) and sparse data in verbclassification and to ensure that only features sup-ported by several verbs are used in classification:we only consider SCFs and GRs which have fre-quency larger than 40 with 5 or more verbs4.The system produces a rich lexicon which in-cludes raw and processed input sentences and pro-vides a variety of material for verb clustering, in-cluding e.g.
(statistical) information related to thepart-of-speech (POS) tags, GRs, SCFs, argumentheads, and adjuncts of verbs.
Using this mate-rial, we constructed a wide range of feature sets2See Preiss et al (2007) for the details of evaluation.3We used an implementation of the SCF classifier pro-vided by Paula Buttery.4These and other threshold values mentioned in this paperwere determined empirically on corpus data.639for experimentation, both shallow and deep syn-tactic and semantic features.
As described below,some of the feature types have been employed inprevious works and some are novel.2.2 Feature setsThe first feature set F1 includes informationabout the lexical context (co-occurrences) of verbswhich has proved useful for supervised verb clas-sification (Li and Brew, 2008):F1: Co-occurrence (CO): We adopt the bestmethod of Li and Brew (2008) where collo-cations are extracted from the four words im-mediately preceding and following a lemma-tized verb.
Stop words are removed prior toextraction, and the 600 most frequent result-ing COs are kept.F2-F3 provide information about lexical prefer-ences of verbs in argument head positions of spe-cific GRs associated with the verb:F2: Prepositional preference (PP): the type andfrequency of prepositions in the indirect ob-ject relation.F3: Lexical preference (LP): the type and fre-quency of nouns and prepositions in the sub-ject, object, and indirect object relation.All the other feature sets include informationabout SCFs which have been widely employed inverb classification, e.g.
(Schulte im Walde, 2006;Sun et al, 2008; Li and Brew, 2008; Korhonenet al, 2008).
F4-F7 include basic SCF informationand/or refine it with additional information whichhas proved useful in previous works:F4: SCFs and relative frequencies with verbs.SCFs abstract over particles and prepositions.F5: F4 with COs (F1).
The SCF and CO featurevectors are concatenated.F6: F4 with the tense of the verb.
The frequencyof verbal POS tags is calculated specific toeach SCF.F7: F4 with PPs (F2).
This feature parameterizesSCFs for prepositions.F8: Basic SCF feature corresponding to F4 but ex-tracted from the VALEX lexicon (Korhonenet al, 2006)5.The following 9 feature sets are novel.
Theybuild on F7, refining it further.
F9-F11 refine F7with information about LPs:F9: F7 with F3 (subject only)F10: F7 with F3 (object only)F11: F7 with F3 (subject, object, indirect object)F12-17 refine F7 with SPs.
We adopt a fully un-supervised approach to SP acquisition.
We acquirethe SPs by1.
taking the GR relations (subject, object, indi-rect object) associated with verbs,2.
extracting all the argument heads in these re-lations which occur with frequency> 20 withmore than 3 verbs, and3.
clustering the resulting N most frequent ar-gument heads into M classes using the spec-tral clustering method described in the fol-lowing section.We tried the N settings {200, 500} and the Msettings {10, 20, 30, 80}.
The best settings N =200,M = 20 and N = 500,M = 30 are reportedin this paper.
We enforce the features to be sharedby all the potential members of a verb class.
Theexpected class size is approximatelyN/K, and weallow for 10% outliers (the features occurring lessthan (N/K)?
0.9 verbs are thus removed).The resulting SPs are combined with SCFs in asimilar fashion as LPs are combined with SCFs inF9-F11:F12-F14: as F9-F11 but SPs (20 clusters from 200argument heads) are used instead of LPsF15-F17: as F9-F11 but SPs (30 clusters from 500argument heads) are used instead of LPs5This feature was included to enable comparing the con-tribution of the recent SCF system to that of an older, com-parable system which was used for constructing the VALEXlexicon.6403 Clustering methodsWe use two clustering methods: (i) pairwise clus-tering (PC) which obtained the best performancein comparison with several other methods in re-cent work on biomedical verb clustering (Korho-nen et al, 2008), and (ii) a method which isnew to the task (and to the best of our knowl-edge, to NLP): a variation of spectral clusteringwhich exploits the MNCut algorithm (Meila andShi, 2001) (SPEC).
Spectral clustering has beenshown to be effective for high dimensional andnon-convex data in NLP (Chen et al, 2006) andit has been applied to German verb clustering byBrew and Schulte im Walde (2002).
However, pre-vious work has used Ng et al (2002)?s algorithm,while we adopt the MNCut algorithm.
The lat-ter has shown a wider applicability (von Luxburg,2007; Verma and Meila, 2003) and it can be justi-fied from the random walk view, which has a clearprobabilistic interpretation.Clustering groups a given set of items (verbs inour experiment) V = {vn}Nn=1into a disjoint par-tition of K classes I = {Ik}Kk=1.
Both our algo-rithms take a similarity matrix as input.
We con-struct this from the skew divergence (Lee, 2001).The skew divergence between two feature vectorsv and v?is dskew(v, v?)
= D(v?||a ?v+(1?a) ?v?
)where D is the KL-divergence.
v is smoothedwith v?.
The level of smoothing is controlled bya whose value is set to a value close to 1 (e.g.0.9999).
We symmetrize the skew divergenceas follows: d(v, v?
)sskew=12(dskew(v, v?)
+dskew(v?, v)).SPEC is typically used with the Radial BasisFunction (RBF) kernel.
We adopt a new kernelsimilar to the symmetrized KL divergence kernel(Moreno et al, 2004) which avoids the need forscale parameter estimation.w(v, v?)
= exp(?dsskew(v, v?
))The similarity matrix W is constructed whereWij= w(vi, vj).Pairwise clusteringPC (Puzicha et al, 2000) is a method where a costcriterion guides the search for a suitable partition.This criterion is realized through a cost function ofthe similarity matrix W and partition I:H = ??nj?
Avgsimj,Avgsimj=P{a,b?Aj}w(a,b)nj?
(nj?1)where njis the size of the jthcluster and Avgsimjis the average similarity between cluster members.Spectral clusteringIn SPEC, the similarities Wijare viewed as theweight on the edges ij of a graph G over V .
Thesimilarity matrix W is thus the adjacency matrixfor G. The degree of a vertex i is di=?Nj=1wij.A cut between two partitions A and A?is definedto be Cut(A,A?)
=?m?A,n?A?Wmn.In MNCut algorithm, the similarity matrixW istransformed to a stochastic matrix P .P = D?1W (1)The degree matrix D is a diagonal matrix whereDii= di.It was shown by Meila and Shi (2001) that if Phas the K leading eigenvectors that are piecewiseconstant6with respect to a partition I?and theireigenvalues are not zero, then I?minimizes themultiway normalized cut(MNCut):MNCut(I) = K ?
?Kk=1Cut(Ik,Ik)Cut(Ik,I)Pmncan be interpreted as the transition probabil-ity between vertices m,n.
The criterion can thusbe expressed as MNCut(I) =?Kk=1(1?P (Ik?Ik|Ik)) (Meila, 2001), which is the sum of transi-tion probabilities across different clusters.
The cri-terion finds the partition where the random walksare most likely to happen within the same cluster.In practice, the K leading eigenvectors of P isnot piecewise constant.
But we can extract thepartition by finding the approximately equal ele-ments in the eigenvectors using a clustering algo-rithm like K-means.The numerator of MNCut is similar to the costfunction of PC.
The main differences between thetwo algorithms are: 1) MNCut takes into accountof the cross cluster similarity, while PC does not.2) PC optimizes the cost function using determin-istic annealing, whereas SPEC uses eigensystemdecomposition.The spectral clustering algorithm is based on theMulticut algorithm (Meila and Shi, 2001).6The eigenvector v is piecewise constant with respect to Iif v(i) = v(j)?i, j ?
Ikand k ?
1, 2...K641Input: Dataset S, Number of clusters K1.
Compute similarity matrixW and Degree ma-trix D2.
Construct stochastic matrix P using equation13.
Compute the eigenvalues and eigenvectors{?n, xn}Nn=1of P , where ?n?
?n+1, form amatrix X = [x2, .
.
.
, xk] by stacking the eigen-vectors in columns.4.
Form a matrix Y from X by normalizing therow sums to have norm 1: Yij= Xij/(?jX2ij)125.
Consider the row of Y to be the transformedfeature vectors for each verb and cluster theminto clusters C1.
.
.
CkusingK-means clusteringalgorithm.Output: Clusters C1.
.
.
Ck4 Experimental evaluation4.1 Test setsWe employed two test sets which have been usedto evaluate previous work on English verb classi-fication:T1 The test set of Joanis et al (2008) providesa classification of 835 verbs into 15 (somecoarse, some fine-grained) Levin classes.
11tests are provided for 2-14 way classifica-tions.
We employ the 14 way classifica-tion because this corresponds the closest toour target (Levin?s fine-grained) classifica-tion7.
We select 586 verbs according to Joa-nis et al?s selection criteria, resulting in 10-120 verbs per class.
We restrict the classimbalance to 1:1.5.8.
This yields 205 verbs(10-15 verbs per class) which is similar tothe sub-set of T1 employed by Stevenson andJoanis (2003).T2 The test set of Sun et al (2008) classifies 204verbs to 17 fine-grained Levin classes, so thateach class has 12 member verbs.Table 1 shows the classes in T1 and T2.4.2 Data processingFor each verb in T1 and T2, we extracted allthe occurrences (up to 10,000) from the raw cor-pus data gathered originally for constructing the7However, the correspondence is not perfect with halfof the classes including two or more Levin?s fine-grainedclasses.8Otherwise, in the case of a large class imbalance the eval-uation measure would be dominated by the classes with largepopulation.T1Object Drop 26.
{1,3,7}Recipient 13.
{1,3}Admire 31.2Amuse 31.1Run 51.3.2Sound 43.2Light &43.
{1,4}SubstanceCheat 10.6Steal &10.
{5,1}RemoveWipe 10.4.
{1,2}Spray / Load 9.7Fill 9.8Putting 9.1-6Change of State 45.1-4T2Remove 10.1Send 11.1Get 13.5.1Hit 18.1Amalgamate 22.2Characterize 29.2Peer 30.3Amuse 31.1Correspond 36.1Manner37.3of speakingSay 37.7Nonverbal40.2expressionLight 43.1Other change45.4of stateMode with47.3MotionRun 51.3.2Put 9.1Table 1: Levin classes in T1 and T2T1 T2total avg total avgCO F1 1328 764 743 382LP (p) F2 61 37 55 25LP (all) F3 2521 526 1481 295SCF F4 88 46 86 38SCF+CO F5 1466 833 856 422SCF+POS F6 319 114 299 87SCF+P F7 282 96 273 76SCF (V) F8 - - 92 45SCF+LP (s) F9 1747 324 1474 225SCF+LP (o) F10 2817 424 2319 279SCF+LP (all) F11 4250 649 3515 426SCF+SP20 (s) F12 821 235 690 145SCF+SP20 (o) F13 792 218 706 135SCF+SP20 (all) F14 1333 357 1200 231SCF+SP30 (s) F15 977 274 903 202SCF+SP30 (o) F16 1026 273 1012 205SCF+SP30 (all) F17 1720 451 1640 330Table 2: (i) The total number of features and (ii)the average per verb for all the feature setsVALEX lexicon (Korhonen et al, 2006).
The datawas gathered from five corpora, including e.g.
theBritish National Corpus (Leech, 1992) and theNorth American News Text Corpus (Graff, 1995).The average frequency of verbs in T1 was 1448and T2 2166, showing that T1 is a more sparsedata set.The data was first processed using the featureextraction module.
Table 2 shows (i) the totalnumber of features in each feature set and (ii) theaverage per verb in the resulting lexicons for T1and T2.We normalized the feature vectors by the sumof the feature values before applying the clusteringtechniques.
Since both clustering algorithms have642an element of randomness, we run them multipletimes.
The step 5 of SPEC (K-means) was run for50 times.
The result that minimizes the distortion(the distances to cluster centroid) is reported.
PCwas run 20 times, and the results are averaged.4.3 Evaluation measuresTo facilitate meaningful comparisons, we em-ploy the same measures for evaluation as previ-ously employed e.g.
by Korhonen et al (2008);?OS?eaghdha and Copestake (2008).The first measure is modified purity (mPUR) ?a global measure which evaluates the mean preci-sion of clusters.
Each cluster is associated with itsprevalent class.
The number of verbs in a clusterK that take this class is denoted by nprevalent(K).Verbs that do not take it are considered as errors.Clusters where nprevalent(K) = 1 are disregardedas not to introduce a bias towards singletons:mPUR =?nprevalent(ki)>2nprevalent(ki)number of verbsThe second measure is weighted class accuracy(ACC): the proportion of members of dominantclusters DOM-CLUSTiwithin all classes ci.ACC =?Ci=1verbs in DOM-CLUSTinumber of verbsmPUR and ACC can be seen as a measure of pre-cision(P) and recall(R) respectively.
We calculateF measure as the harmonic mean of P and R:F =2 ?
mPUR ?
ACCmPUR + ACCThe random baseline(BL) is calculated as follows:BL = 1/number of classes5 Results5.1 Quantitative evaluationTable 3 includes the F-measure results for all thefeature sets when the two methods (PC and SPEC)are used to cluster verbs in the test sets T1 and T2,respectively.
A number of tendencies can be ob-served in the results.
Firstly, the results for T2 areclearly better than those for T1.
Including a highernumber of verbs lower in frequency from classesof variable granularity, T1 is probably a more chal-lenging test set than T2.
T2 is controlled for thenumber and frequency of verbs to facilitate cross-class comparisons.
While this may contribute tobetter results, T2 is a more accurate test set for usin the sense that it offers a better correspondencewith our target (fine-grained Levin) classes.T1 T2PC SPEC PC SPECBL 7.14 7.14 5.88 5.88CO F1 15.62 33.85 17.86 40.94LP (p) F2 40.40 38.97 50.98 49.02LP (all) F3 42.94 47.50 41.08 74.55SCF F4 34.22 36.16 52.33 57.78SCF+CO F5 26.43 28.70 19.52 29.10SCF+POS F6 36.14 34.75 44.44 46.70SCF+P F7 43.57 43.85 63.40 63.28SCF (V) F8 - - 34.08 38.30SCF+LP (s) F9 47.72 56.09 65.94 71.65SCF+LP (o) F10 43.09 48.43 57.11 73.97SCF+LP (all) F11 45.87 54.63 56.30 72.97SCF+SP20 (s) F12 46.67 57.75 39.52 71.67SCF+SP20 (o) F13 44.95 51.70 40.76 70.78SCF+SP20(all) F14 48.19 55.12 39.68 73.09SCF+SP30 (s) F15 45.89 56.10 64.44 80.35SCF+SP30 (o) F16 42.01 48.74 52.75 70.52SCF+SP30(all) F17 46.66 52.68 51.07 68.67Table 3: Results on testsets T1 and T2Secondly, the difference between the two clus-tering methods is clear: the new SPEC outperformsPC on both test sets and across all the feature sets.The performance of the two methods is still fairlysimilar with the more basic, less sparse feature sets(F1-F2, F4, F6-7) but when the more sophisticatedfeature sets are used (F3, F5, F9-F17) SPEC per-forms considerably better.
This demonstrates thatit is clearly a better suited method for high dimen-sional feature sets.Comparing the feature sets, the simple co-occurrence based F1 performs clearly better thanthe random baseline.
F2 and F3 which exploit lex-ical data in the argument head positions of GRsprove significantly better than F1.
F3 yields sur-prisingly good results on T2: it is the second bestfeature set on this test set.
Also on T1, F3 per-forms better than the SCF-based feature sets F4-F7.
This demonstrates the usefulness of lexicaldata when obtained from argument positions inrelevant GRs.Our basic SCF feature set F4 performs consid-erably better than the comparable feature set F8obtained from the VALEX lexicon.
The differenceis 19.50 in F-measure.
As both lexicons were ex-tracted from the same corpus data, the improve-ment can be attributed to improved parser and SCFacquisition performance (Preiss et al, 2007).F5-F7 refine the basic SCF feature set F4 fur-ther.
F5 which combines a SCF with CO in-formation proved the best feature set in the su-pervised verb classification experiment of Li andBrew (2008).
In our experiment, F5 produces sub-stantially lower result than CO and SCF alone (i.e.643F1 and F4).
However, our corpus is smaller (Liand Brew used the large Gigaword corpus), ourSCFs are different, and our approach is unsuper-vised, making meaningful comparisons difficult.F6 combines F4 with information about verbtense.
This was not helpful: F6 produces worse re-sults than F4.
F7, on the other hand, yields betterresults than F4 on both test sets.
This demonstrateswhat the previous research has shown: SCF per-form better when parameterized for prepositions.Looking at our novel feature sets F9-F17, F9-F11 combine the most accurate SCF feature setF4 with the LP-based features F2-F3.
Althoughthe feature space gets more sparse, all the featuresets outperform F2-F3 on T1.
On T2, F3 per-forms exceptionally well, and thus yields a betterresult than F9-F11, but F9-F11 nevertheless per-form clearly better than the best SCF-based featureset F4 alone.
The differences among F9, F10 andF11 are small on T2, but on T1 F9 yields the bestperformance.
It could be that F9 works the bestfor the more sparse T1 because it suffers the leastfrom data sparsity (it uses LPs only for the subjectrelation).F12-F17 replace the LPs in F9-F11 by semanticSPs.
When only 20 clusters are used as SP modelsand acquired from the smaller sample of (200) ar-gument heads (F12-F14), SPs do not perform bet-ter than LPs on T2.
A small improvement can beobserved on T1, especially with F12 which usesonly the subject data (yielding the best F measureon T1: 57.75%).
However, when 30 more fine-grained clusters are acquired from a bigger sampleof (500) argument heads (F15-F17), lower resultscan be seen on T1.
On T2, on the other hand, F15yields dramatic improvement and we get the bestperformance for this test set: 80.35% F-measure.The fact that no improvement is observed whenusing F16 and F17 on T2 could be explained bythe fact that SPs are stronger for the subject posi-tion which also suffers less from the sparse dataproblem than e.g.
i. object position.
The fact thatno improvement is observed on T1 is likely to bedue to the fact that verbs have strong SPs only atthe finer-grained level of Levin classification.
Re-call that in T1, as many as half of the classes arecoarser-grained.5.2 Qualitative evaluationThe best performing feature sets on both T1 andT2 were thus our new SP-based feature sets.
Weconducted qualitative analysis of the best 30 SPHuman mother, wife, parent, girl, childRole patient, student, user, worker, teacherBody-part neck, shoulder, back, knee, cornerAuthority committee, police, court, council, boardOrganization society, firm, union, bank, institutionMoney cash, currency, pound, dollar, fundAmount proportion, value, size, speed, degreeTime minute, moment, night, hour, yearPath street, track, road, stair, routeBuilding office, shop, hotel, hospital, houseRegion site, field, area, land, islandTechnology system, model, facility, engine, machineTask operation, test, study, analysis, dutyArrangement agreement, policy, term, rule, procedureMatter aspect, subject, issue, question, caseProblem difficulty, challenge, loss, pressure, fearIdea argument, concept, idea, theory, beliefPower control, lead, influence, confidence, abilityForm colour, style, pattern, shape, designItem letter, book, goods, flower, cardTable 4: Cluster analysis: 20 clusters, their SP la-bels, and prototypical member nounsclusters in the T2 data created using SPEC to findout whether these clusters were really semantic innature, i.e.
captured semantically meaningful pref-erences.
As no gold standard specific to our verbclassification task was available, we did manualcluster analysis using VerbNet (VN) as aid.
In VN,Levin classes are assigned with semantic descrip-tions: the arguments of SCFs involved in diathesisalternations are labeled with thematic roles someof which are labeled with selectional restrictions.From the 30 thematic role types in VN, as manyas 20 are associated with the 17 Levin classes inT2.
The most frequent role in T2 is agent, fol-lowed by theme, location, patient, recipient, andsource.
From the 36 possible selectional restric-tion types, 7 appear in T2; the most frequent onesbeing +animate and +organization, followed by+concrete, +location, and +communication.As SP clusters capture selectional preferencesrather than restrictions, we examined manu-ally whether the 30 clusters (i) capture seman-tically meaningful classes, and whether they (ii)are plausible given the VN semantic descrip-tions/restrictions for the classes in T2.The analysis revealed that all the 30 clusters hada predominant, semantically motivated SP sup-ported by the majority of the member nouns.
Al-though many clusters could be further divided intomore specific SPs (and despite the fact that somenouns were clearly misclassified), we were able toassign each cluster a descriptive label characteriz-ing the predominant SP.
Table 4 shows 15 sam-644ple clusters, the SP labels assigned to them, and anumber of example nouns in these clusters.When comparing each SP cluster against theVN semantic descriptions/restrictions for T2, wefound that each predominant SP was plausible.Also, the SPs frequent in our data were also fre-quent among the 17 classes according to VN.
Forexample, the many SP clusters labeled as arrange-ments, issues, ideas and other abstract conceptswere also frequent in T2, e.g.
among COMMUNI-CATION (37), CHARACTERISE (29.2), AMALGA-MATE (22.2) and other classes.This analysis showed that the SP models whichperformed well in verb clustering were semanti-cally meaningful for our task.
An independentevaluation using one of the standard datasets avail-able for SP acquisition research (Brockmann andLapata, 2003) is of course needed to determinehow well the acquisition method performs in com-parison with other existing methods.Finally, we evaluated the quality of the verbclusters created using the SP-based features.
Wefound that some of the errors were similar to thoseseen on T2 when using syntactic features: errorsdue to polysemy and syntactic idiosyncracy.
How-ever, a new error type clearly due to the SP-basedfeature was detected.
A small number of classesgot confused because of strong similar SPs in thesubject (agent) position.
For example, some PEER(30.3) verbs (e.g.
look, peer) were found in thesame cluster with SAY (37.7) verbs (e.g.
shout,yell) ?
an error which purely syntactic features donot produce.
Such errors were not numerous andcould be addressed by developing more balancedSP models across different GRs.6 Discussion and related workAlthough features incorporating semantic infor-mation about verb SPs make theoretical sense theyhave not proved equally promising in previous ex-periments which have compared them against syn-tactic features in verb classification.
Joanis et al(2008) incorporated an ?animacy?
feature (a kindof a ?SP?)
which was determined by classifyinge.g.
pronouns and proper names in data to this sin-gle SP class.
A small improvement was obtainedwhen this feature was used in conjunction withsyntactic features in supervised classification.Joanis (2002) and Schulte im Walde (2006) ex-perimented with more conventional SPs with syn-tactic features in English and German verb clas-sification, respectively.
They employing top levelMethod ResultT1Li et al 2008 supervised 66.3Joanis et al 2008 supervised 58.4Stevenson et al 2003semi-supervised 29unsupervised 31SPEC unsupervised 57.55T2 Sun et al 2008supervised 62.50unsupervised 51.6?O S?eaghdha et al 2008 supervised 67.3SPEC unsupervised 80.35Table 5: Previous verb classification resultsWordNet (Miller, 1995) and Germanet (Kunze andLemnitzer, 2002) classes as SP models.
Joanis(2002) obtained no improvement over syntacticfeatures, whereas Schulte im Walde (2006) ob-tained insignificant improvement.Korhonen et al (2008) combined SPs with SCFswhen clustering biomedical verbs.
The SPs wereacquired automatically from syntactic slots ofSCFs (not from GRs as in our experiment) usingPC clustering.
A small improvement was obtainedusing LPs extracted from the same syntactic slots,but the SP clusters offered no improvement.
Re-cently, Schulte im Walde et al (2008) proposed aninteresting SP acquisition method which involvescombining EM training and the MDL principle foran verb classification incorporating SPs.
However,no comparison against purely syntactic features isprovided.In our experiment, we obtained a considerableimprovement over syntactic features, despite usinga fully unsupervised approach to both verb clus-tering and SP acquisition.
In addition to the rich,syntactic-semantic feature sets, our good resultscan be attributed to the clustering technique capa-ble of dealing with them.
The potential of spectralclustering for the task was recognised earlier byBrew and Schulte im Walde (2002).
Although adifferent version of the algorithm was employedand applied to German (rather than to English),and although no SP features were used, these ear-lier experiments did demonstrate the ability of themethod to perform well in high dimensional fea-ture space.To get an idea of how our performance com-pares with that of related approaches, we exam-ined recent works on verb classification (super-vised and unsupervised) which were evaluated onsame test sets using comparable evaluation mea-sures.
These works are summarized in table 5.ACC and F-measure are shown for T1 and T2, re-spectively.645On T1, the best performing supervised methodreported so far is that of Li and Brew (2008).
Liand Brew used Bayesian Multinomial Regressionfor classification.
A range of feature sets integrat-ing COs, SCFs and/or LPs were evaluated.
Thecombination of COs and SCFs gave the best result,shown in the table.
Joanis et al (2008) report thesecond best supervised result on T1, using SupportVector Machines for classification and features de-rived from linguistic analysis: syntactic slots, slotoverlaps, tense, voice, aspect, and animacy of NPs.Stevenson and Joanis (2003) report a semi- andunsupervised experiment on T1.
A feature set sim-ilar to that of Joanis et al (2008) was employed(features were selected in a semi-supervised fash-ion) and hierarchical clustering was used.Our unsupervised method SPEC performs sub-stantially better than the unsupervised method ofStevenson et al and nearly as well as the super-vised approach of Joanis et al (2008) (note, how-ever, that the different experiments involved differ-ent sub-sets of T1 so are not entirely comparable).On T2, the best performing supervised methodso far is that of?O S?eaghdha and Copestake (2008)which employs a distributional kernel method toclassify SCF features parameterized for preposi-tions in the automatically acquired VALEX lexicon.Using exactly the same data and feature set, Sunet al (2008) obtain a slightly lower result when us-ing a supervised method (Gaussian) and a notablylower result when using an unsupervised method(PC clustering).
Our method performs consider-ably better and also outperforms the supervisedmethod of?O S?eaghdha and Copestake (2008).7 Conclusion and Future WorkWe introduced a new approach to verb cluster-ing which involves the use of (i) rich lexical, SCFand GR data produced by a recent SCF system, (ii)novel syntactic-semantic feature sets which com-bine a variety of linguistic information, and (iii) anew variation of spectral clustering which is par-ticularly suited for dealing with the resulting, highdimensional feature space.
Using this approach,we showed on two well-established test sets thatautomatically acquired SPs can be highly usefulfor verb clustering.
This result contrasts with mostprevious works but is in line with theoretical workon verb classification which relies not only on syn-tactic but also on semantic features (Levin, 1993).In addition to the ideas mentioned earlier, ourfuture plans include looking into optimal waysof acquiring SPs for verb classification.
Consid-erable research has been done on SP acquisitionmost of which has involved collecting argumentheadwords from data and generalizing to Word-Net classes.
Brockmann and Lapata (2003) haveshowed that WordNet-based approaches do notalways outperform simple frequency-based mod-els, and a number of techniques have been re-cently proposed which may offer ideas for refin-ing our current unsupervised approach (Erk, 2007;Bergsma et al, 2008).
The number and type (andcombination) of GRs for which SPs can be reliablyacquired, especially when the data is sparse, re-quires also further investigation.In addition, we plan to investigate other po-tentially useful features for verb classification(e.g.
named entities and preposition classes) andexplore semi-automatic ML technology and activelearning for guiding the classification.
Finally, weplan to conduct a bigger experiment with a largernumber of verbs, and conduct evaluation in thecontext of practical application tasks.AcknowledgmentsOur work was funded by the Dorothy HodgkinPostgraduate Award, the Royal Society Univer-sity Research Fellowship, and the EPSRC grantEP/F030061/1, UK.
We would like to thank PaulaButtery for letting us use her implementation ofthe SCF classifier and Yuval Krymolowski for thesupport he provided for feature extraction.ReferencesShane Bergsma, Dekang Lin, and Randy Goebel.
Dis-criminative learning of selectional preference fromunlabeled text.
In Proc.
of EMNLP, 2008.Chris Brew and Sabine Schulte im Walde.
Spectralclustering for german verbs.
In Proc.
of EMNLP,2002.Ted Briscoe, John Carroll, and Rebecca Watson.
Thesecond release of the rasp system.
In Proc.
of theCOLING/ACL on Interactive presentation sessions,2006.Carsten Brockmann and Mirella Lapata.
Evaluatingand combining approaches to selectional preferenceacquisition.
In Proc.
of EACL, 2003.Jinxiu Chen, Dong-Hong Ji, Chew Lim Tan, andZheng-Yu Niu.
Unsupervised relation disambigua-tion using spectral clustering.
In Proc.
of COL-ING/ACL, 2006.Hoa Trang Dang.
Investigations into the Role of Lexi-cal Semantics in Word Sense Disambiguation.
PhDthesis, CIS, University of Pennsylvania, 2004.Katrin Erk.
A simple, similarity-based model for selec-tional preferences.
In Proc.
of ACL, 2007.646David Graff.
North american news text corpus.
Lin-guistic Data Consortium, 1995.Eric Joanis.
Automatic Verb Classification Using aGeneral Feature Space.
Master?s thesis, Universityof Toronto, 2002.Eric Joanis, Suzanne Stevenson, and David James.
Ageneral feature space for automatic verb classifica-tion.
Natural Language Engineering, 2008.Karin Kipper-Schuler.
VerbNet: A broad-coverage,comprehensive verb lexicon.
2005.Anna Korhonen, Yuval Krymolowski, and Ted Briscoe.A large subcategorization lexicon for natural lan-guage processing applications.
In Proc.
of the 5thLREC, 2006.Anna Korhonen, Yuval Krymolowski, and Nigel Col-lier.
The Choice of Features for Classification ofVerbs in Biomedical Texts.
In Proc.
of COLING,2008.Claudia Kunze and Lothar Lemnitzer.
GermaNet-representation, visualization, application.
In Proc.of LREC, 2002.Lillian.
Lee.
On the effectiveness of the skew diver-gence for statistical language analysis.
In ArtificialIntelligence and Statistics, 2001.Geoffrey Leech.
100 million words of english: thebritish national corpus.
Language Research, 1992.Beth.
Levin.
English verb classes and alternations: Apreliminary investigation.
Chicago, IL, 1993.Jianguo Li and Chris Brew.
Which Are the Best Fea-tures for Automatic Verb Classification.
In Proc.
ofACL, 2008.Diana McCarthy.
Lexical Acquisition at the Syntax-Semantics Interface: Diathesis Alternations, Sub-categorization Frames and Selectional Preferences.PhD thesis, University of Sussex, UK, 2001.Marina.
Meila.
The multicut lemma.
Technical report,University of Washington, 2001.Marina Meila and Jianbo Shi.
A random walks view ofspectral segmentation.
AISTATS, 2001.George A. Miller.
WordNet: a lexical database for En-glish.
Communications of the ACM, 1995.Pedro J. Moreno, Purdy P. Ho, and Nuno Vasconce-los.
A Kullback-Leibler divergence based kernel forSVM classification in multimedia applications.
InProc.
of NIPS, 2004.Andrew Y. Ng, Michael Jordan, and Yair Weiss.
Onspectral clustering: Analysis and an algorithm.
InProc.
of NIPS, 2002.Diarmuid?O S?eaghdha and Ann Copestake.
Semanticclassification with distributional kernels.
In Proc.
ofCOLING, 2008.Judita Preiss, Ted Briscoe, and Anna Korhonen.
A sys-tem for large-scale acquisition of verbal, nominaland adjectival subcategorization frames from cor-pora.
In Proc.
of ACL, 2007.Jan Puzicha, Thomas Hofmann, and Joachim M. Buh-mann.
A theory of proximity based clustering:Structure detection by optimization.
Pattern Recog-nition, 2000.Sabine Schulte im Walde.
Experiments on the auto-matic induction of german semantic verb classes.Computational Linguistics, 2006.Sabine Schulte im Walde, Christian Hying, ChristianScheible, and Helmut Schmid.
Combining EMTraining and the MDL Principle for an AutomaticVerb Classification incorporating Selectional Pref-erences.
In Proc.
of ACL, pages 496?504, 2008.Lei Shi and Rada Mihalcea.
Putting pieces together:Combining FrameNet, VerbNet and WordNet for ro-bust semantic parsing.
In Proc.
of CICLING, 2005.Suzanne Stevenson and Eric Joanis.
Semi-supervisedverb class discovery using noisy features.
In Proc.of HLT-NAACL 2003, pages 71?78, 2003.Lin Sun, Anna Korhonen, and Yuval Krymolowski.Verb class discovery from rich syntactic data.
Lec-ture Notes in Computer Science, 4919:16, 2008.Robert Swier and Suzanne Stevenson.
Unsupervisedsemantic role labelling.
In Proc.
of EMNLP, 2004.Deepak Verma and Marina Meila.
Comparison of spec-tral clustering methods.
Advances in Neural Infor-mation Processing Systems (NIPS 15), 2003.Andreas Vlachos, Anna Korhonen, and ZoubinGhahramani.
Unsupervised and constrained dirich-let process mixture models for verb clustering.
InProc.
of the Workshop on Geometrical Models ofNatural Language Semantics, 2009.Ulrike von Luxburg.
A tutorial on spectral clustering.Statistics and Computing, 2007.Be?nat Zapirain, Eneko Agirre, and Llu?
?s M`arquez.
Ro-bustness and generalization of role sets: PropBankvs.
VerbNet.
In Proc.
of ACL, 2008.647
