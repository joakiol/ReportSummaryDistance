R. Dale et al (Eds.
): IJCNLP 2005, LNAI 3651, pp.
366 ?
377, 2005.?
Springer-Verlag Berlin Heidelberg 2005Relation Extraction Using Support Vector MachineGumwon HongUniversity of Michigan,Ann Arbor, MI 48109gwhong@umich.eduAbstract.
This paper presents a supervised approach for relation extraction.
Weapply Support Vector Machines to detect and classify the relations in AutomaticContent Extraction (ACE) corpus.
We use a set of features including lexical to-kens, syntactic structures, and semantic entity types for relation detection andclassification problem.
Besides these linguistic features, we successfully utilizethe distance between two entities to improve the performance.
In relation detec-tion, we filter out the negative relation candidates using entity distance thresh-old.
In relation classification, we use the entity distance as a feature for SupportVector Classifier.
The system is evaluated in terms of recall, precision, and F-measure, and errors of the system are analyzed with proposed solution.1   IntroductionThe goal of Information Extraction (IE) is to pull out pieces of information that aresalient to the user?s need from large volume of text.
With the dramatic increase ofWorld Wide Web, there has been growing interest in extracting relevant informationfrom large textual documents.
The IE tasks may vary in detail and reliability, but twosubtasks are very common and closely related: named entity recognition and relationextraction.
Named entity recognition identifies named objects of interest such as per-son, organizations or locations.
Relation extraction involves the identification of ap-propriate relations among these entities.
Examples of the specific relations are em-ployee-of and parent-of.
Employee-of relation holds between a particular person and acertain organization and parent-of holds between a father and his child.
According toMessage Understanding Conferences (MUC), while named entities can be extractedwith 90% or more in F measure with a state of the art system, relation extraction wasnot so successful [8].In this paper we propose a supervised machine learning approach for relation ex-traction.
The goal of this paper is to investigate an empirical method to find out usefulfeatures and experimental procedure to increase the performance of relation extrac-tion.
We divide the extraction task into two individual subtasks: relation detection andrelation classification.
Relation detection is involved in identifying from every pair ofentities positive examples of relations which can fall into one of many relation catego-ries.
In relation classification, we assign a specific class to each detected relation.
Toeach task, we apply distinct linguistic features ranging from lexical tokens to syntacticstructures as well as the semantic type information of the entities.
We also applied thedistance between two entities to make the detection problem easier and to increase theRelation Extraction Using Support Vector Machine 367performance of both the relation detection and classification.
We apply Support Vec-tor Machines (SVMs) partly because it represents the state of the art performance formany classification tasks [3].
As we will discuss in section 3 about features, we areworking on very high dimensional feature space, and this often leads to overfitting.Thus, the main reason to use SVMs is that this learning algorithm has a robust ration-ale for avoiding overfitting [11].This paper is structured in the following manner.
In section 2, we survey the previ-ous work on relation extraction with emphasis on supervised machine learning ap-proaches.
In section 3, the problem formalization, the description of dataset and thefeature extraction methods will be discussed.
In section 4, we conduct a performanceevaluation of the proposed approach and error analysis on ACE dataset.
Finally, insection 5, a conclusion and discussion of future work will be followed.2   Related WorkRelation extraction was formulated as part of Message Understanding Conferences(MUC) [8], which has focused on a series of information extraction tasks, includinganalyzing free text, recognizing named entities, and identifying relations of a speci-fied type.
Work on relation extraction over the last two decades has progressed fromlinguistically unsophisticated models to the adaptation of NLP techniques that useshallow parsers or full parsers and complicated machine learning methods.
[7] addressed the relation extraction problem by extending the statistical parsingmodel which simultaneously recovers syntactic structures, named entities, and rela-tions.
They first annotated sentences for entities, descriptors, coreference links and re-lation links, and trained the sentences from Penn Treebank.
Then they applied to newtraining sentences and enhanced the parse trees to include the IE information.
Finallythey re-retrained the parser on newly enriched training data.More recently, [12] introduced the kernel methods to extract relations from 200news articles from different publications.
The kernels are defined over shallow parserepresentations of text and computed through a dynamic programming fashion.
Theygenerated relation examples from shallow parses for two relations: person-affiliationand organization-location.
Performance of the kernel methods was compared withfeature-based methods and it showed that kernels have promising performance.
[4] extended work in [12] to estimate kernel functions between augmented depend-ency trees.
Their experiment has two steps.
Relations were first detected and thenclassified in cascading manner.
However, the recall was relatively low because manypositive examples of relations were dropped during detection phase.
Detecting rela-tions is hard job for kernel methods because, in detection, all negative examples areheterogeneous and it is difficult to use similarity function.
[13] proposed weakly supervised learning algorithm for classifying relations inACE dataset.
He introduced feature extraction methods for his supervised approach aswell as a bootstrapping algorithm using random feature projection (called BootPro-ject) on top of SVMs.
He compared BootProject with supervised approach, andshowed that BootProject algorithm reduced much work needed for labeling trainingdata without decreasing the performance.368 G. HongOur approach to relation extraction adapts some feature extraction methods from[13] but differs from [13] in two important aspects.
First, instead of only classifyingrelations by assuming all candidate relations are detected, we perform relation detec-tion before classifying relations by regarding every pair of entities in a sentence as atarget of classification.
Second, we used total 7652 (6140 for training and 1512 fordevelopment testing) relation examples in ACE dataset, but in [13] only 4238 rela-tions were used for training and 1022 for testing.
We increase the performance by us-ing more training data and reducing errors in data processing stage.3   Data Processing3.1   Problem DefinitionOur task is to extract relations from unstructured natural language text.
Because wedetermine the relationship for every pair of entities in a sentence, our task is formal-ized with a standard classification problem as follows:(e1 ,e2, s) ?
rwhere e1 and e2 are two entities existing in sentence s, and r is a label of the relation.Every pair of entities in a sentence can be a relation or not, so we call the triple (e1,e2, s) a relation candidate.
With the possible set of values of r, there can be at leastthree tasks.1.
Detection only: For each relation candidate, we predict whether it constitutes anactual relation or not.
In this task, a label r can be either +1 (two entities are re-lated) or -1 (not related).2.
Combined detection and classification: For each relation candidate, we per-form N+1 way classification, where N is the number of relation types.
Five rela-tion types are specified in next subsection.
The additional class is -1 (two enti-ties are not related).3.
Classification only: For each relation, we perform N way classification.
In thistask, we assume that all relation candidates in test data are manually labeledwith N+1 way classification.
We only consider the classifier?s performance onthe N relation types.We use a supervised machine learning technique, specifically Support Vector Ma-chine classifiers, and we empirically evaluate the performance in terms of recall, pre-cision and F measure.
To make the problem more precisely, we constrain the task ofrelation extraction with following assumptions:1.
Entities should be tagged beforehand so that all information regarding entities isavailable when relations are extracted.2.
Relations are binary, i.e., every relation takes exactly two primary arguments.3.
The two arguments of a relation, i.e., an entity pair, should explicitly occurwithin a sentence.4.
Evaluation is performed over five limited types of relations as in Table 1.Relation Extraction Using Support Vector Machine 369A relation is basically an ordered pair, thus ?Sam was flown to Canada?
containsthe relation AT(Sam, Canada) but not AT(Canada, Sam).
However, 7 relations inNEAR (relative location) and SOCIAL (associate, other-personal, other-professional,other-relative, sibling, and spouse) types are symmetric.
For example, the sentencessuch as ?Bill is the neighbor of Sarah.?
and ?The park is two blocks from WalnutStreet?
do not distinguish the order of entities.Table 1.
Relation types and their subtypes in ACE 2 corpusAT BASED-IN, LOCATED, RESIDENCENEAR RELATIVE-LOCATIONPART OTHER, PART-OF, SUBSDIARYROLE AFILIATE-PARTNER, CITIZEN-OF, CLIENT, FOUNDER, GENERAL-STAFF,MANAGEMENT, MEMBER, OTHER, OWNERSOCIAL ASSOCIATE, GRANDPARENT, OTHER-PERSONAL, OTHER-PROFESSIONAL,OTHER-RELATIVE, PARENT, SIBLING, SPOUSE3.2   Data SetWe extract relations from the Automatic Content Extraction (ACE) corpus, specifi-cally version 1.0 of the ACE 2 corpus, provided by the National Institute for Stan-dards and Technology (NIST).
ACE corpora consist of 519 annotated text documentsassembled from a variety of sources selected from broadcast news programs, newspa-pers, and newswire reports [1].According to the scope of the LDC ACE program 1, current research in informa-tion extraction has three main objectives: Entity Detection and Tracking (EDT), Rela-tion Detection and Characterization (RDC), and Event Detection and Characterization(EDC).
This paper focuses on the second sub-problem, RDC.
For example, we wantto determine whether a particular person is at certain location, based on the contextualevidence.
According to the RDC guideline version 3.6, Entities are limited to 5 types(PERSON, ORGANIZATION, GEO-POLITICAL ENTITY (GPE), LOCATION, andFACILITY), and relations are also classified into 5 types:Role: Role relations represent an affiliation between a Person entity and an Organiza-tion, Facility, or GPE entity.Part: Part relations represent part-whole relationships between Organization, Facilityand GPE entities.At: At relations represent that a Person, Organization, GPE, or Facility entity is lo-cated at a Location entity.Near: Near relations represent the fact that a Person, Organization, GPE or Facilityentity is near (but not necessarily ?at?)
a Location or GPE entity.Social: Social relations represent personal and professional affiliations between Per-son entities.Table 1 lists the 5 major relation types and their subtypes.
The numbers in the bot-tom row indicate the number of instances in training data and development testingdata.
We do not classify the 24 subtypes of relations due to the sparse instances ofmany subtypes.370 G. HongTable 2.
Breakdown of the ACE dataset (#f and #r means number of files and number ofrelations respectively)Training data Held-out data Test data#f # r #f # r #f # r325 4628 97 1512 97 1512Table 2 shows the breakdown of the ACE dataset for our task.
Training data takesabout 60% of all ACE corpora, and each held-out data and test data contains about20% of the whole distribution in the number of files as well as in the number ofrelations.13.3   Data ProcessingWe start with entity marked data and the first step is to detect the sentence boundaryusing the sentence segmenter provided by DUC competition2.
Performance of the sen-tence segmentater is crucial for relation detection because the number of entity pairscombinatorially increases as a sentence grows in size.
The original DUC segmenterfails to detect sentence boundary if the last word of a sentence is a named entity andannotated with brackets, i.e., ?<?
and ?>?.
We modified the DUC segmenter by addingsimple rules where we mark sentence boundary if every bracket is followed by ???,?!
?, or ?.?.
At this step, we drop the sentences if the sentence has less than two entities.Sentence parsing is then performed using Charniak parser [6] on these target sen-tences in order to obtain the necessary linguistic information.
To facilitate the featureextraction, parse trees are converted into chunklink format [2].
Chunklink format con-tains the same information as the original parse tree, but it provides for each word thechunk to which it belongs, its grammatical function, grammatical structure hierarchy,and trace information.3.4   FeaturesWe are ready to introduce our feature sets.
The following features include lexical to-kens, part of speech, syntactic features, semantic entity types, and the distance be-tween two entities (we will say ?entity distance?
to refer to distance between two enti-ties in the remaining part of this paper).
These features are selectively used for threetasks defined in section 3.1.1.
Words.
Lexical token of both entity mentions3 as well as all the words betweenthem.
If two or more words constitute an entity, each individual word in an entity is aseparate feature.2.
Part of speech.
Part of speech corresponding to each word feature describedabove.1Training data and held-out data together are called "train", and test data is called "devtest" inthe ACE distribution.2http://duc.nist.gov/past_duc/duc2003/software/3Entities are objects and they have a group of mentions, and the mentions are textual refer-ences to the objects.Relation Extraction Using Support Vector Machine 3713.
Entity type.
Entities have one of five types (person, location, organization, geo-political entity, or facility).4.
Entity mention type.
An entity mention can be named, nominal, or pronominal.
En-tity mention type is also called the level of a mention.5.
Chunk tag.
A word is inside (I), outside (O) or in the beginning (B) of a chunk.
Achunk here stands for a standard phrase.
Thus, the word ?George?
at the beginningof NP chunk ?George Bush?
has B-NP tag.
Similarly, the word ?named?
has I-VPtag in VP chunk ?was named?.6.
Grammatical function tag.
If a word is the head word of a chunk, it has function ofwhole chunk.
The other words in a chunk that are not the head have "NOFUNC" astheir function.
For example, the word ?task?
is head of NP chunk ?the task?
and ithas grammatical function tag ?NP?, while ?the?
has ?NOFUNC?.7.
IOB chain.
The syntactic categories of all the constituents on the path from the rootnode to the leaf node of a parse tree.
For example, a word?s IOB chain I-S/I-NP/B-PPmeans that it is inside a sentence, inside a NP chunk, and in the beginning of PPchunk.8.
Head word Path.
The head word sequence in the path between two entity mentionsin the parse tree.
This feature is used after removing duplicate elements.9.
Distance.
The number of words between two entity mentions.10.
Order.
Relative position of two relation arguments.Two entities and all words between them have separate features of chunk tags (5),grammatical function tags (6), and IOB chains (7), and these three features are allautomatically computed by chunklink.pl.
See [2] for further description of these threeterminologies.
We use the same method as in [13] to use the features described in 3,5, 6, 7, and 10.
The concept of the head word path is adapted from [10].Let us take a sentence ?Officials in New York took the task?
for an example.
Twoentity mentions, officials and New York, constitute a relation AT(officials, NewYork).
For the noun phrase ?officials in New York?, the corresponding parse tree isshown in Fig 2.To compute the head word path, we need to know what a head word is.
At the ter-minal nodes, the head word is simply the word from the sentence the node represents.At non-terminal nodes in the parse tree, ?The head of a phrase is the element insidethe phrase whose properties determine the distribution of that phrase, i.e.
the envi-ronments in which it can occur.?
[9]The head word path is computed in the following way.
The head words of the twoentity mentions are officials and York respectively.
The path between these two headwords is the sequence officials-NP-PP-NP-York which is in bold in Fig.
1.
Then wechange each non-terminal into its head word, and we get officials-officials-in-York-York.
If we remove the duplicate elements, we get officials-in-York.
Finally, we re-place the two entity mentions with their entity mention types, resulting in PERSON-in-GPE.372 G. HongFig.
1.
Parse tree for the fragment ?officials in New York?.
The path between two entity men-tions is in bold.Table 3 shows the features for the noun phrase ?officials in New York?.
Note thatall features are binary and they are assigned 1 or -1 depending on whether an examplecontains the feature or not.
We could have n+1 possible distance features, where n isthe entity distance threshold.Table 3.
Example of features for Officials in New York.
e1 and e2 are two entities thatconstitute a relation AT(officials, New York).Features ExampleWords officials(e1), in, New(e2-1), York(e2-2)Part-of-speech NNS(e1), IN, NNP(e2-1), NNP(e2-2)Entity type officials: PERSON, New York: GPEEntity mention type officials: NOMINAL, New York: NAMEChunk tag B-NP, B-PP,B-NP, I-NPGrammatical function tag NP, PP, NOFUNC, NPIOB chain officials: B-S/B-NP/B-NP, in: I-S/I-NP/B-PP, New: I-S/I-NP/I-PP/B-NP York: I-S/I-NP/I-PP/I-NPHead word path PERSON-in-GPEDistance Distance=1Order e1 before e2Besides the linguistic features, we apply entity distance to each task.
When detect-ing actual relations, we use the entity distance to filter the unnecessary examples.
Toachieve the optimal performance, we set a threshold to keep the balance between re-call and precision.
We choose the threshold of entity distance based on the detectionperformance on held-out data.
When classifying the individual relation types, the en-tity distance is also successfully used as a feature.
We will discuss how the entity dis-tance is utilized as a feature in the next section.When performing combined detection and classification, we use two-staged extrac-tion: we first detect relations in the same way as detection only task, and then performclassification on detected relations to predict the specific relation types.
There are twoimportant reasons that we choose this two-staged extraction.York New took OfficialsNNSin ?IN NNP NNPNPPPNPRelation Extraction Using Support Vector Machine 373First, we detect relations to increase the recall for whole extraction task.
Becauseevery combination of two entities in a sentence can construct a relation, we have farmore negative examples of relation4 in our training data.
If we perform single-stagedN+1 classification, the classifier is more likely to identify a testing instance as a non-relation because of the disproportionate size in samples which often decreases the per-formance.
Thus in detection stage (or detection only task), we perform binary classifi-cation by enriching our training samples by assuming 5 types of relations to be onepositive class.Second, before performing relation classification, we can reduce a significantnumber of negative examples with only limited sacrifice of the detection performanceby using a filtering technique with entity distance threshold.
Fig.
2 shows the positiveand negative examples distributed over the entity distances in training data.
As we cansee from the positive example curve (lower curve in the graph), the number of posi-tive examples decreases as the entity distance exceeds 1.
Positive examples do not ex-ist where the entity distance is more than 32, while negative examples can have entitydistances more than 60.010002000300040005000600070000 5 10 15 20 25 30 35 40 45 50 55 60distance between entities#examplesnegative examplepositive exampleFig.
2.
Distribution of samples over entity distances in training dataWe can also discover that almost 99% of the positive examples are distributedbelow the entity distance 20.
If we restrict an entity distance threshold to 5, that is ifwe remove samples whose entity distances are more than 5, then whereas 71% ofnegative examples are dropped out, we can still maintain 80% of the positive exam-ples.
As can be seen in the graph, negative relations take more than 95% of wholeentity pairs.
This class imbalance makes the classifier less likely to identify a test-ing instance as a relation.
The threshold makes relation detection easier because wecan remove the large portion of negative examples, and we can get the result inreasonable time.4We have total 6551 positive relation examples out of 131510 entity pairs in training data.374 G. HongIn our experiment we tested the following 3 sets of features:F1: words + entity type + entity mention type + orderF2: F1 + all syntactic features (2, 5, 6, 7 and 8)F3: F2 + distanceF1 is the set of simple features that can be obtained directly from the ACE corpuswithout applying any NLP techniques.
Features in F2 require sentence boundary de-tection and parsing to acquire the syntactic structure of a sentence.
F3 is used to de-termine whether an entity distance can be successfully used for our task.4   Experiments and Error AnalysisWe applied Support Vector Machines (SVMs) classifier to the task.
More specifically,we used LIBSVM with RBF (Radial Basis Function) kernel [3].
SVMs attempt to finda hyperplane that split the data points with maximum margin, i.e., the greatest dis-tance between opposing classes [11].The system performance is usually reflected using the performance measures of in-formation retrieval: precision, recall, and F-measure.
Precision is the ratio of the numberof correctly predicted positive examples to the number predicted positive examples.
Re-call is the ratio of the number of correctly predicted positive examples to the number oftrue positive examples.
F-measure combines precision and recall as follows:F-measure = 2 * precision * recall / (precision + recall)We report precision, recall, and F-measure for each experiment.We use different features for relation detection and relation classification.
In rela-tion detection, which is either in detection only task or in the first stage of combineddetection and classification, we do not use entity distance as a feature.
Instead, we getrid of the negative examples of relations by setting a threshold value of the entity dis-tances.
The threshold is chosen in a way that we could achieve the best F-measurescore of relation detection performance on held-out data with a classifier trained ontraining data.
We have fixed the entity distance threshold to be 7 by this experiment,and this threshold enables us to remove 67% of the negative examples and keep 93%of the positive example.
We will use this value in the remaining experiments.In relation classification, which is either in the second stage of combined extractionor in classification only task, we use all the features described in Table 3.
Entity dis-tance is successfully used as a feature to classify the individual relation types.
Let ustake a relation type NEAR for an example to see why entity distance is useful.
Asshown in table 4, the average entity distance of NEAR relations is relatively greaterthan the average distances of any other types of relations.Table 4.
Average entity distance for 5 relation types in training data.
The first row indicates theaverage entity distances over the relations whose entity distances are less than or equal to 7.Relation ROLE PART AT NEAR SOCIAL ALLAvg Entity distance (<= 7) 1.95 1.63 2.79 3.23 1.55 2.13Avg Entity distance (all) 2.89 2.2 4.38 5.55 2.42 3.26Relation Extraction Using Support Vector Machine 375Table 5.
Frequency of misclassification of relations (R: ROLE, A: AT, S: SOCIAL, N: NEAR,P: PART).
Model was trained on training data with F2 features and tested on held-out data.System R A S P P A P P A R N R NGold A R R R N P P A N S A N PFrequency 46 34 29 23 21 19 12 11 7 6 1 1 1This does not necessarily mean that we can choose NEAR when the entity distance inan example is close to 3.23.
However, there is an interesting result about NEAR whenwe apply the classifier trained on training data to held-out data.
Table 5 shows classifi-cation errors discovered after testing on held-out data.
As shown in table 5, NEAR ismisclassified as PART relatively often (almost 60% of the times).
We also find by look-ing at samples in training data that only a small portion (about 5%) of the PART rela-tions have entity distance more than 4, while more than 50% of NEAR relations haveentity distance more than 4.
This discovery suggests that we can prevent the misclassifi-cation by adding the distance features to original feature set to increase the performance.Table 6.
Detection only performance trained on training + held-out data and applied on testdata (P: preprocessed with distance threshold = 7, N: no threshold is used)Features Precision Recall F-measureF1 (N) 82.6 43.2 56.7F2 (N) 69.4 62.1 65.5F1 (P) 92.3 44.1 59.6F2 (P) 77.8 69.3 73.3Table 6 shows the performance of detection only task.
The filtering process withdistance threshold resulted in performance increase in all measurements.As described in previous section, we divide the corpus into three different sets:training data, held-out data, and test data.
We first build the classifier on training data,and applied the classifier to held-out data.
In this experiment, we choose the thresholdof entity distance until we get the optimal performance of relation detection on held-out data.
We also count the misclassification of relations on held-out data.
Next, weretrained the data on whole training set, i.e., training data plus held-out data, and thenapplied to the test data.Table 7 shows the performance of combined detection and classification task ontest data.
We can see in table 7 that the system using all features achieved the highestperformance.
We can also find that the system using all syntactic features performsbetter in F-measure than the system without syntactic features.
However, the featureswhich do not require any language processing (F1) achieved relatively high precisioncompared to F2 and F3.
When entities are very close, sometimes syntactic featureswould not be of much help.
For example, relations such as ?a town west of Jerusa-lem?, ?park outside Paris?, ?his friend/wife/brother?
are highly dependent on thelexical feature, i.e., ?west of?, ?outside?, and ?friend/wife/brother?
are the most im-portant clue to determine the class of the relations.
Finally, as we previously dis-cussed, F3 is always better, in all kinds of measurements, than F1 and F2.
This provesthat our system certainly benefits from the distance feature.376 G. HongTable 7.
Combined detection and classification performance.
Model is trained on training data+ held-out data and tested on test data (trained over 6140 relations and tested on 1512 relations)Features Precision Recall F-measureF1 76.2 38.3 50.9F2 65.2 49.7 56.4F3 68.8 51.4 58.8Table 8.
Classification only performance.
Model is trained on training data + held-out data andtested on test data (trained over 6140 relations and tested on 1512 relations).Relation Type Precision Recall F-measureROLE 85.7 84.3 85.0PART 67.8 73.6 70.6AT 81.5 82.9 82.2NEAR 43.2 62.6 51.1SOCIAL 74.9 88.2 81.0We perform classification only task to compare the result with [13].
Table 8 showsthe performance evaluation on 5 types of relation in test data.
Direct comparison with[13] at each relation type is not a relevant process because the published result in [13]regarded a reduced set of relations (5,260 relations) as total relations tagged in theACE data set.
Nevertheless, our system performs better in F-measure in all relationtypes by capable of using much more relation examples and by utilizing entitydistance feature.5   ConclusionWe have presented a supervised approach for relation extraction where we applySupport Vector Machines to detect actual relations and to classify the specific types ofthose relations.
We combine diverse lexical, syntactic and semantic features as well asentity distance.Our system benefits from performing two-staged extraction as well as using the en-tity distance.
In detection, we use binary classification to use more positive examples,and entity distance threshold also helps to remove large part of negative candidatesfor relations in reasonable time.
Furthermore, entity distance is successfully used as afeature in classifying specific relations to increase the performance of the system.The most immediate extension is to include semantic information such as seman-tic indexing and disambiguating the sense of entities.
For example, as in table 5,SOCIAL relations are always confused with ROLE relations which often have simi-lar lexical pattern.
Moreover, NEAR and AT relations often have very similar syn-tactic structures.
These examples do not benefit from lexical or syntacticfeatures alone.The next goal in this research will be to develop a classifier to apply other tasks ofinformation extraction problem.
We plan to integrate the entity recognition andrelation extraction.Relation Extraction Using Support Vector Machine 377References1.
ACE.
2004.
The NIST ACE evaluation website.
http://www.nist.gov/speech/tests/ace/.2.
Buchholz, S.: The chunklink script (2000) http://ilk.uvt.nl/~sabine/chunklink/3.
Chang, C.-C. and Lin, C.-J.
: LIBSVM: a library for support vector machines (2001) Soft-ware available at.
http://www.csie.ntu.edu.tw/~cjlin/libsvm.4.
Culotta, A. and Sorensen, J.: Dependency tree kernels for relation extraction.
Proceedingsof the 42nd Annual Meeting of the Association for Computational Linguistics, Barcelona(2004)5.
Cortes, C. and Vapnik, V.: Supportvector networks.
Machine Learning, (1995) 20(3):273?2976.
Charniak, E.: A maximum-entropy-inspired parser.
Technical Report CS-99-12, ComputerScicence Department, Brown University (1999)7.
Miller, S., Crystal, M., Fox, H., Ramshaw, L., Schwartz, R., Stone, R., Weischedel, R.,and The Annotation Group.
: Algorithms that learn to extract information, bbn: Descriptionof the sift system as used for muc-7.
Technical report, BBN Technologies (2000)8.
National Institute of Standars and Technology.
Proceedings of the 6th Message Under-tanding Conference (MUC-7) (1998)9.
Sag, I., Wasow, T., and Bender, E.: Syntactic Theory: A Formal Introduction, volume 152of CSLI Lecture Notes.
CSLI Publications, Stanford, California, 2 edition (2003)10.
Singh, Natasha.
: Syntactic features in relation extraction.
MEng thesis.
MIT.
(2004)11.
Vapnik, V.: Statistical Learning Theory.
John Wiley, (1998)12.
Zelenko, D., Aone, C., and Richardella, A.: Kernel methods for relation extraction.
Journalof Machine Learning Research (2003) 1083?110613.
Zhang, Z.: Weakly-supervised relation classification for information extraction.
Proceed-ings of the Thirteenth ACM conference on Information and knowledge management.Washington D.C. (2004)
