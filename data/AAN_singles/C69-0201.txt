I. IntroductionTo a great extent, the lack of large-scale effective solutions toproblems of computatlonal linguistics can be traced to the lack of anadequate linguistic theory that could be used as a framework for computa-tional work.
Most of the linguistic theorizing that has taken place inthe United States has been done under the banner of Transformationalgrammar.
Fundamental to transformational theory is the sharp distinc-tion between 'competence' and !performance'.This distinction between competence and performance provides fortransformatlonallsts the platform from which to make their statementsabout transformations.
'Competence', according to Chomsky is what thespeaker-hearer knows about his language, as opposed to his use of thatknowledge, labeled 'performance'.
Chomsky includes in his discussion ofwhat a 'performance' model should do, factors such as memory limitations,inattentlon~ distraction, and non-linguistlc knowledge.
He thus leavesfor 'competence' the formalization of linguistic processes representativeof the speaker-hearer's knowledge of the language.This relegation of competence makes a basic mistake however.
Itis necessary to differentiate the problem of formalization of linguisticknowledge and processes, i.e., competence, from the simulation of lin-guistic knowledge and processes, which we shall call 'simulative perfor-mance'.
There is a difference between the simulation of knowledge andprocesses ('simulative performance') and the simulation of actual ver-Jbal behavior (Chomsky's 'performance').
It is here that we must speak,as chomsky does, of the ideal speaker-hearer.
Clearly the ideal speaker-hearer is not inattentive or distracted.
He does however have memory-I-limitations and non-linguistle knowledge.
This is certainly what mustbe simulated as an inclusive part of linguistic theory.
The kind oftheory of 'performance' of which Chomsky speaks may well be in the fardistant future to which Chomsky relegates it (1965).
However, a theoryof simulative performance is not so far off.
It would seem very reason-able that the possibility of the construction of a linguistic theory thatboth accounts for the data and does this in such a way as to appear tobe consonant with the human method for doing so, is not so remote.
Clear-ly, such a theory must deal with non-linguistic knowledge and problemsof human memory as well as the problems that Chomsky designates as 'com-petence'.
Thus, it seems that the sharp distinction between competenceand performance is artificial at best.
In particular, after elimina-tion of some of the behavioristic problems such as distraction, we canexpect to find a linguistic theory that is neither one of 'competence'nor 'performance' but something in between and therefore inclusive ofboth.Chomsky (1965:139) has stated:'thus it seems absurd to suppose that the speaker firstforms a generalized Phrase-marker by base rules and thentests it for well-formedness by applying transformationalrules to see if it gives, finally, a well-formed sentence.But this absurdity is simply a corollary to the deeper ab-surdity of regarding the system of generative rules as apoint-by-point model for the actual construction of a sen-tence by a speaker.
"We could, on the other hand, attempt to formulate a system ofrules that are a point-by-point model for the actual construction of asentence by a speaker.
Furthermore, we might expect that that systemcould also be a point-by-polnt model for the actual analysis of a sen-tenee by a hearer.
These claims, however, would be largely unverifiable-2-except by the use of computers as simulative devices.Chomsky (1965:141) has further stated that:'She gralmnar does not, in itself, provide any sensibleprocedure for finding the deep structure of a given sentence,or for producing a given sentence, just as it provides nosensible procedure for finding a paraphrase to a given sen-tence.
It merely defines these tasks in a precise way.
Aperformance model must certainly incorporate a grammar; itis not to be confused with grammar.
"Insofar as the notion of a performance model here can be taken asbeing somewhere between Chomsky's notion of competence and performance,our notion of grammar also lies somewhere between Chomsky's notion ofa grammar and the incorporation of a grammar.-3-II.
Conceptual DependencyThe Conceptual Dependency framework (see Schank \[ 1969\] ) is astratified linguistic system that attempts to provide a computationaltheory of simulative performance.
The highest level of the stratifi-catlonal system (similar to Lanab \[ 1966\], Sgall \[1965\] and others) em-ployed by the Conceptual Dependency framework is an interlingua consis-ting of a network of language-free dependent concepts, where a conceptmay be considered to be an unambiguous word-sense, (except see Schank,\[1968\]).
(The notion of dependency used here is related to those of Hays (1964)and Klein (1965), however, the dependencies are not at all restrictedto any syntactic criterion.)
The graumaar of a language is defined bythe framework as consisting of Realization Rules that map conceptualconstructs into syntactically correct language on the 'sentential level'The linguistic process can be thought of, in Conceptual Dependencyterms, as a mapping into and out of some mental representation.
Thismental representation consists of concepts related to each other by var-ious meaning-contingent dependency links.
Each concept in the inter-lingual network may be associated with some word that is its realizateon a sentential level.The conceptual dependency representation is a linked network thatcan be said to characterize the conceptualization inherent in a pieceof wrltten language.
The rule of thumb in representing concepts asdependent on other concepts is to see if the dependen t concept will fur-ther explain its governor and if the dependent concept cannot make sensewithout its governor.For example, in the sentence, '~he big man steals the red book-4-from the girl."
the analysis is as follows: 'The' is stored for use inconnecting sentences in paragraphs, i.e., 'the' specifies that 'man' mayhave been referred to previously.
'Big' refers to the concept 'big'which cannot stand alone conceptually.
The concept 'man' can standalone and is modified, conceptually by 'big', so it is realized in thenetwork as a governor with its dependent.
'Steals' denotes an actionthat is dependent on the concept that is doing the acting.
A conceptual-ization (a proposition about a conceptual actor) cannot be completewithout a concept acting (or an attribute statement), so a two-way depen-dency link may be said to exist between 'man' and 'steal' That is, theyare dependent on each other and govern each other.
Every conceptualiza-tion must have a two-way dependency llnk.
'Book' governs 'red' attribu-tively and the whole entity is placed as objectively dependent on 'steals'.The construction 'from the girl' is realized as being dependent on theaction through the conceptual object.
This is a different type of de-pendency (denoted by 4 ) .
There are different forms of this 'prepositionaldependency', each of which is noted by writing the preposition over thellnk to indicate the kind of prepositional relationship.
(Although alanguage may use inflections or nothing at all instead of prepositionsto indicate prepositional dependency, we are discussing a language-freesystem here and it is only the relation of the parts conceptually thatis under consideration.
)The conceptual network representation of this sentence is thenas follows:fromman ~ steals ~ book ~ , girlt tbig red-5-The conceptual level works with a system of rules (shown in theAppendix) that operate on conceptual categories.
These rules generateall the permissible dependencies in a conceptualization.
Multiplecombinatlon of conceptualizations in various relationships are intendedto account for the totality of human language activity at the conceptuallevel.The conceptual categories are divided into governing and assistinggroups:Governin~ CategoriesPPACTLOCTAssisting CategoriesPAAAAn actor or object; corresponds syntactically(in English) to concrete nominal nouns or nounforms.An action; corresponds syntactically (in English)to verbs, verbal nouns, and most abstract nouns.A location of a conceptualization.A time of conceptualization; often has variantforms consisting of parts of a conceptualization.Attribute of a PP; corresponds (in English) toadjectives and some abstract nouns.Attribute of an ACT; corresponds (in English) toadverbs and indirectly objective abstract nouns.Thus, the categories assigned in the above network correspond closely totheir syntactic correlates:PP ~ ACT ~ PP ~ PPPA PAHowever, in the sentence, 'Visiting relatives can be a nuisance', thesyntactic categories often do not correspond with the conceptual actors-6-and actions.
The ambiguous interpretations of this sentence are:one PP(I) ~ ~ bother ~ one ~ ~ ACT ~ PPvisit ACTrelatives PP(Here we use the conditional present \[denotedby c\] form of the two-way dependency link,one of eight posslble tense-mood forms.
)relatives ~ bother ~ one PP ~ ACT ~ PP (2) % $visit ACTrelatives PP(3) bother, one $visit ACTone PPA conceptualization is written in a conceptual dependency analysison a straight line.
Dependents written perpendicular to the line areattributes of their governor except when they are part of another con-ceptualization line.
Whole conceptualizations can relate to other con-ceptualizations as actors (\[i\] and \[3\]) or attributes (\[2\] where ~ in-dicates that the PP at its head is the actor in a main and subordinateconceptualization \[ ~ is the subordinate, written below the line\]).The Conceptual Dependency framework, at the conceptual level, isthus responsible for representing the meaning of a piece of writtenlanguage in language-free terms.
The representation is in terms ofactor-action-object conceptualizations in a topic-cogent form.
Thus,words that have many syntactic forms will have only one conceptual form.This is true interlinguistically as well as intralinguistically.
The-7-meaning of a construction is always the consideration used in represen-tation.
For example, 'of' in 'a chp of water' is realized as '~-~ con-talns X' where X is water.cupcontainswaterSimi lar ly,  in ' John's  love is  good',  ' love '  is  rea l ized conceptual ly asX = loves ~Y.John~ goodlovetoneIn order to make this framework serve as a generative theory,semantics and realization rules must be added.
The realization rules areused in conjunction with a dictionary of realizates.
These rules mappieces of the network in accord with the granmaar.
Thus, a simple rulein English might be:PP= Adj + NPAIn facts the rules are not this simple since criteria of usualness andcontext enter into each application of a rule.
These problems are dis-cussed elsewhere (Sehank \[1969\] ) and are not the point of this paper.The semantics that Conceptual Dependency employs is a conceptualsemantics in that it serves only to limit the range of conceptualizationsin such a way as to make them consonant with experience.
The form andmajor content of this semantics is thus universal, but Since we are deal-ing with experience we are required to speak of someone's experience.-8-We will thus begin to talk about some arbitrary human's experience, orsince we are dealing with a computer, we can talk of the systems' ex-perience.
Thus, the conceptual semantics consists of lists of potentialdependents for any given concept.
These lists are listed with respectto semantic categories if there is a generalization that can be made onthat basis.-9-III.
The ParserThe Conceptual Dependency framework is used for a natural languageparser by reversing the realization rules and using the semantics as acheck with reality.
The system for analyzing a sentence into its con-ceptual representation operates on pieces of a sentence looking up thepotential conceptual realizates.All conceptualizations are checked against a list of experiencesto see if that particular part of the construction has occurred before.If the construction has not occurred, or has occurred only in somepeculiar context, this is noted.
Thus, in the construction 'ideassleep', it is discovered that this connection has never been made before,and is therefore meaningless to the system.
If the user says that thisconstruction is all right, it is added to the memory; otherwise the con-struction is looked up in a metaphor list or aborted.
The system thusemploys a record of what it has heard before in order to analyze whatit is presently hearing.In order for the system to choose between two analyses of a sen-tence both of which are feasible with respect to the conceptual rules(see Appendix) the conceptual semantics is incorporated.
The conceptualsemantics limits the possible conceptual dependencies to statements con-sonant with the system's knowledge of the real world.
The definition ofeach concept is composed of records organized by dependency type and bythe conceptual category of the dependent.
For each type of dependency,semantic categories (such as animate object, human institution, animalmotion) are delimited with respect to the conceptual category of agiven concept, and defining characteristics are inserted when they are- I0 -known.
For example, concepts in the semantic category 'physical object'all have the characteristic 'shape' Sometimes this information is in-trinsic to the particular concept involved, for example, 'balls areround'The semantic categories are organized into hierarchical structuresin which limitations on any category are assumed to apply as well to allcategories subordinate to it.
The system of semantic categories and amethod of constructing semantic files is discussed more fully in Schank(1969).In the present system, the files are constructed by incorporatinginformation derived from rules presented as English sentences.
Theprogram parses each of these sentences and observes which dependenciesare new and then adds them to the files.As an example of the use of the conceptual semantics, considerthe parse of 'the tall boy went to the park with the girl'.
At thepoint in the parse where the network isboy ~ go ~ parkttallwe are faced with the problem of where to attach the construct ~ ~tb girl.A problem exists since at least two realization rules may apply: 'ArTPR~P ~:  1 ~3;  '~P P~EP ~P: ~2 ' The problem is resolved by the3conceptual semantics.
The semantics for 'go' contains a llst of concep-tual prepositions.
Under 'with' is listed 'anyl movable physical object'and since a girl is a physical object the dependency is allowed.
Thesemantics for 'park' are also checked.
Under 'with' for 'park' arelisted the various items that parks are known to contain, e.g., statues,-ll-junglegyms, etc.
'Girl' is not found so the network (I) is allowedwhile (e) is aborted.
(I) boy g go ~to park <with girltall(2) boy g go <t=o parkttall ~withgir lAlthough ,~'th g i r l '  is dependent on 'go' i t  is dependent through'park'.
That is, these are not isolated dependencies since we wouldwant to be able to answer the question 'Did the girl go to the park?'affirmatively.
In (2) the below-the-line notation indicates that it isthe 'park with a girl' as opposed to another 'park'.
Now it may well bethe case that this is what was intended.
The conceptual semantics func-tions as an experience file in that it limits conceptualization to onesconsonant with the system's past experience.
Since it has never encoun-tered 'parks with girls' it wil l  assume that this is not the meaningintended.
It is possible, as it is in an ordinary conversation, for theuser to correct the system if an error was made.
That is, if (2) werethe intended network it might become apparent to the user that thesystem had misunderstood and a correction could easily be made.
Thesystem would then learn the new permissible construct and would add itto its semantics.
The system can always learn from the user (as des-cribed in Schank \[1968\] ) and in fact the semantics were originally inputin this way, by noticing occurrences in sample sentences.Thus, the system purports to be analyzing a sentence in a way-12-analogous to the human method.
It handles input one word at a time asit is encountered checks potential linkings with its ~n knowledge of theworld and past experience, and places its output into a language-freeformulation that can be operated on, realized in a paraphrase, or trans-lated.Thus the Coneeputal Dependency parser is a conceptual analyzerrather than a syntactic parser.
It is primarily concerned with expli-cating the underlying meaning and conceptual relationships present ina piece of discourse in any natural language.
The parser described herebears some similarity to certain deep structure parsers (Kay \[1967\] ,Thorne et al\[ 1968\] and Walker \[1966\] ) only insofar as all these parsersare concerned to some extent with the meaning of the piece of discoursebeing operated upon.
However, the parser is not limited by the problemsinherent in transformational grammar (such as the difficulty in revers-ing transformational rules and the notion that semantics is somethingthat 'operates' on syntactic output).
Also, the parser does not haveas a goal the testing of a previously formulated grammar \[as doesWalker (1966) for example) so that the theory underlying the parser hasbeen able to be changed as was warranted'by obstacles that we encountered.The parser's output is a language-free network consisting of unambiguousconcepts and their relations to other concepts.
Pieces of discoursewith identical meanings, whether in the same or different languages,parse into the same conceptual network.The parser is being used to understand natural language state-ments in Colby's (1967) on-line dialogue program for psychiatric inter-viewing, but is not restricted to this context.
In interviewing-13-programs llke Colby's, as well as in question-answering programs, adiscourse-generating algorithm must be incorporated to reverse thefunction of the parser.
The conceptual parser is based on a linguistictheory that uses the same rules for both parsing and generating, thusfacilitating man-machine dialogues.In an interviewing program, the input may contain words that theprogram has never encountered, or which it has encountered only indifferent environments.
The input may deal with a conceptual structurethat is outside the range of experience of the program, or even use asyntactic combination that is unknown.
The program is designed tolearn new words and word-senses, new semantic possibilities, and newrules of syntax both by encountering new examples during the dialogueand by receiving explicit instruction.-14-IV.
ImplementationThe parser is presently operating in a limited form.
It iscoded in MLISP for the iPDP-iO and can be adapted to other LISP processorswith minor revisions.Rather than attaching new dependencies to a growing network duringthe parse, the program determines all the dependencies present in thenetwork and then assembles the entire network at the end.
Thus, thesentence 'The big boy gives apples to the pig.'
is parsed into:i) ~ boytbig2) boy ~ give3) gives ~ apples~) give <~ pigand then these are assembled into:boy ~ give ~ apples ~ pigtbigThe input sentence is processed word-by-word.
After "hearing"each word, the program attempts to determine as much as it c~n about thesentence before "listening" for more.
To this end, dependencies arediscovered as each word is processed.
Furthermore, the program antici-pates what kinds of concepts and structures may be expected later in thesentence.
If what it hears does not conform wlth its anticipation, itmay be"confused", "surprised", or even "amused".In case of semantic or syntactic ambiguity, the program shoulddetermine which of several possible interpretations was intended by the"speaker".
It first selects one interpretation by means of miscellaneous-15-heuristics and stacks the rests.
In case later tests and further inputrefute or cast doubt upon the initial guess, that guess is discarded orshelved, and a different interpretation is removed from the stack to beprocessed.
To process an interpretation, it may be necessary to back upthe scan to an earlier point in the sentence and rescan several words.To avoid repetitious work during rescans, any information learned aboutthe words of the sentence is kept in core memory.The parse involves five steps: the dictionary lookup, the appli-cation of realization rules, the elimination of idioms, the rewritingof abstracts~ and the check against the conceptual semantics.The dictionary of words is kept mostly on the disk, but the mostfrequently encountered words remain in core memory to minimize processingtime.
Under each word are listed all its senses.
"Senses" are definedpragmatically as interpretations of the word that can lead to differentnetwork structures or that denote different concepts.
For example,some of the senses of "fly" are:fly I - (intransitive ACT): what a passenger does in an airplane.fly 2 - (intransitive ACT): what an airplane or bird does in theair.fly 3 - (PP): an insectfly 4 - (transitive ACT): what a pilot does by operating an airplane.fly 5 - (intransitive AcT--metaphoric): to go fast.fly 6 - (PP): a flap as on trousers.If ther~ are several senses from which to choose, the programsees whether it was anticipating a concept or connective from some spe-cific category; if so it restricts its first guesses to senses in thatcategory.
Recent contextual usage of some sense also can serve to prefer-16-one interpretation over others.
To choose among several senses withotherwise equal likelihoods, the sense with lowest subscript is chosenfirst.
Thus, by ordering senses in the dictionary according to theirempirical frequency of occurrence, the system can try to improve itsguessing ability.The realization rules that apply to each word sense are referencedin the dictionary under each sense.
Most of the rules fall into cate-gories that cover large conceptual classes and are referenced by manyconcepts.
Such categories are PP, PA, AA, PPloc, PPt, LOC, T, simplytransitive ACT, intransitive ACT, ACT that can take an entire concep-tualization as direct object ("state ACT") and ACT that can take anindirect object without a preposition ("transport ACT").
In contrastto most concepts, each connective (e.g., an auxiliary, preposition, ordeterminer) tends to have its own rules or to share its rules with afew other words.A realization rule consists of two parts: a recognizer and adependency chart.
The recognizer determines whether the rule appliesand the dependency chart shows the dependencies that exist when it does.In the recognizer are specified the ordering, categories, and Inflectionof the concepts and connectives that normally would appear in a sentenceif the rule applied.
If certain concepts or connectives are omissiblein the input, the rule can specify what to assume when they are missing.Agreement of inflected words can be specified in an absolute (e.g.,"plural") or a relative manner (e.g., "same tease").
Rules for alanguage like English have a preponderance of word order specificationswhile rules for a more highly inflected language would have a preponderance-0-of inflection specifications.Realization rules are used both to fit concepts into the networkas they are encountered and to anticipate further concepts and theirpotential realizates in the network.
When a rule is selected for thecurrent word sense, it is compared with the rules of preceding wordsenses to find one that "fits".
For example, if "very hot" is heard,one realization rule for "very" is:vdrywhere the tags "0" and "i" indicate the relative order of the word sensein the recognizer and identify them for reference by the dependencychart; '~" means the current word.
One rule for "hot" is:0AA PA :- i0 - iThe program notices that "very" fits in the "-i" slot of the "hot" ruleand verifies that "hot" fits in the "i" slot of the "very" rule.
There-fore, the dependency suggested by the chart can he postulated for thenetwork:hot (PA)very (AA)After the rules for two adjacent word senses are processed, otherrules are tried, and more distant word senses are checked.Whenever a dependency is postulated, it is looked up in an idiomfile to see if it is an idiom or a proper name and should be restructured.Thus, the construct:makeup-18-is reduced to the single conceptmake-upThis idiom will be detected by the parser even if several words inter-vene between '~make" and "up" in the sentence.After eliminating idioms from the network, there still may beconstructs that do not reflect language-free conceptions.
The mostconspicuous eases are caused in English by abstract nouns.
Most suchnouns do not correspond to PP's but rather are abbreviations for con-ceptualizations in which the concept represented is actually an ACT ora PA.The program treats an abstract noun as a PP temporarily in orderto obtain its dependents, because abstract nouns have the syntax not ofACT's but of PP's.
After obtaining its dependents, the PP is rewrittenas an entire conceptualization according to rules obtained from an ab-stract file.
These rules also specify what to do with the dependents ofthe PP; they may be dependent on the entire conceptualization , dependenton the ACT only, or appear elsewhere in the conceptualization.By way of example, the sentence:Tom's love for Sue is beautiful.leads to the following dependencies;love (PP) love (PP)forTom (PP) SueAfter hearing "is", the program expects no mor~ dependents for "love"(by a heuristic in the program), so it checks the abstract file andfinds rules for "love" including:-19-off,for :PP PP(a) (b)where "(a)" and "(b)" identify concepts without reference to senten~alorder.
The network Is now rewritten:TomlovetSuewhere the hor i zonta l  main l ink  represents  " i s " ,  wai t ing for a r ight -handconcept.
When 'beaut i fu l "  Is heard, the network is  complete, g iv ing:Tom~ beaut i fu llovetSueThe network above may be realized alternative~ as either of theparaphrases:That Tom loves Sue is beautiful.For Tom to love Sue is beautiful.In conceptual dependency theory, connectives like "that", "for", "to"~and "of" are cues to the structure of the network and need not appearin the network at all.
The network above demonstrates such a situation.Conversely, portions of the network may be absent from the sentence.For example, the sentence:It is good to hit the ball near the fence.-20-is parsed as:one~ goodhittballtofence n~arPlaeeHere, "one" and "place" are not realized.
Notice that the relevantrealization rule for "it" is: - \] (al)Ione)If?r PP it be PA ACT: 2~a0) (al) 0 i 2 3The square brackets indicate optional words.
The tags "(a0)" and "(al)"indicate that "for" precedes the "PP" but the whole phrase may occur inany position of the construct.
"(al)!one" in the dependency chart meansthat if "(al)", i.e., "for PP", is omitted, and the subject of the actionis not obvious from context, then the concept "one" is to be assumed.The conceptual network should reflect the Beliefs inherent inthe original discourse in a language-free representation.
The inter-linguistic file of conceptual semantics is checked to verify that thedependencies are indeed acceptable.
This check is made after abstractshave been rewritten.After the five parsing steps are completed, the program proceedsto the next word.
At the end of the sentence, it outputs the final net-work in two dimensions on a printed page or on a display console.-21-V.
Examples of AlgorithmOnly a few of the relevant realization rules will be shown inthe examples.Example I'John saw birds flying to California.Realization Rule PatternsWords (for posslble senses)John 1: (all PP patterns)saw I: (all PP patterns2: (tO see, past tense)PP ACT PP: "i ~ 0 ~- i-i 0 IPP ACT ~ PP: 2 ~ 0 ~ I- i0  Y 2Dependencies(rectangle around new dependencies~ohn ~"see\[~ Pe(note: "to"means "tense of ACTNumber O)3: (to saw):.., etc .b i rds  1: (a l l  PP pat terns )ying l:a) A -Ing: 1 ol:b) ACT PP ACT-Ing :0 i 2\ [see ,- birds_~~birds ~ ~l~ IBut now there  are two main l inkson one l lne so go back and t ry  asob ject  of ' see ' .Ib i rdssee ~ f~lyi: ACT to PPLoc-I ~ i-I 0 I2: to ACT ~ i0 1etc .fly ~ PPLoc-22-California I: (all PPLoc patterns)Final output:fly <to Californiabirds g John see ,- $flytonCaliforniaExample 2'John saw Texas flying to California.
'Words PatternsJohn I: (all PP patterns )saw (as above)Texas I: (all PPLoc patterns)flying (as above)(rest as above)Final Output:John ~ see ~ TexasPJohn ~ fly<t=?CaliforniaDependenciesJ~n l  ~ see!~ PPTexas ~ fly: rejected bysemantics.
Laugh and goback and try (Ib):J ?hnF Isee"" p l John!
\[fl dExample'Jane ate the hamburgers in the park.
'Words , PatternsJane I: (all PP patterns)ate i: (eat - past tense)PP ACT PP: -it~O0 ~ I-I 0 1DependenciesJohn ~ eateat ~ PPe tc.-25 -Wordshamburgersin the parkPatternsi: (all PP patterns)I: ACT In PP: -i 0?_ i-I O i2: PP in PP: -I-1 01  0 ~13: PP ~2A~ in PP ?-3 " " 0 1 LOC"-2-3 ~ ?
-iiDependencieseat ~ hamburgerseat ~ park: rejected bysemanticshamburgers~ inparkset aside as unlikely(would accept if not otheralternatives)PJohn ~ eatin IIpark-24 -VI.
Examples of Parses'Flying planes can be dangerous.
'planes g dangerousflyoneg dangerousflytplane'The shooting of the hunters was terrible.
'hunters~ terribleshootone~ terribleshootthunters'John, who was in the park yesterday, wanted to hit Fred in themouth today'.todayJohn ~ wantJohn ~ hit ~ Fred park ~ be <~ moutht ay ofyesterday Fred'John was persuaded by the doctor in New York to be easy to please.
'doctor ~ persuaded ~ Johnn one ~ please ~ JohnNew York easy'The girl i llke left.
'glrl ~ leaveI ~ like"25 -Vll.
ConclusionBefore computers can understand natural language they must be ableto make a decision as to preclsely what has been said.
The conceptualparser described here is intended to take a natural language input andplace the concepts derivable from that input into a network that expli-cates the relations between those concepts.
The conceptual network thatis then formed is not intended to point out the syntactic relationspresent and there is some question as to why any system would want thisinformation.
Although Chomsky's deep structures convey a good dealmore information than just syntactic relations, it is clear that a parserthat uses deep structures for output would be oriented syntactically.We see no point in limiting our system by trying to test out a previouslyformulated grammar.
The output of a transformational parser, whilemaking explicit some important aspects of the meaning of the sentence,does not make explicit all the conceptual relationships that are to befound, does not limit its parses with a check with reality, and mostimportantly is syntax based.
The parser presented here is semanticsbased.
We aver that the system that humans employ is also semanticsbased.
It seems clear to us that our parser satisfies the requirementsthat a parser must satisfy and in so doing points out the advantages ofregarding language from a Conceptual Dependency point of view.-26 -APPENDIXI.
Conceptual Rules (permissible dependencies):PP * ACT; PP = PP; PP * PA; ACT ~ PP; ACT '~PP;PP PP ACT PA AA PA T IDC ACT~;  t; t ;  t ;  ~; ~; ~;  , ;  ~ ;~.PP PA AA PA PA PP c~ ~ oII.
Realization RulesThere are about I00 of these rules, presently.here.A few are shownPP ACT : I ~ 2 ; PP ACT to ACT : 1 @ 2 ; PA PP : 2 ;i 2 1 2 3 t 1 2 ?i~3  IACT PP PP : i "~2 ~-3 ; PP Prep PP : i12 3 1 2 3 ,'~'2 ;3PP ACT Prep PP : i ~ 2 ~4 for I ~2  ; PP PP ACT ACT: i ~4;l 2 3 ~ ~3 i e 3 42~3lACT PP ACT-lug : ~ ; PP who ACT ACT : i * 3i 2 3 2~3 i ~ 3 % 2III.
A Sample of the Conceptual Semantics for 'ball'.ball ,inanimate motion object-27"~-PA PPsize any in phys objshape round on phys objcolor any for phys objtexture usually smooth by placeelasticity bounces of animalat noto noACTspecificmotion objectconcreteanybounceroll, come, spinfall, hit ...begin, cause ...-28-Bibliographyi.
Chomsky, N., Aspects of the Theory of Syntax, MIT Press, Cambridge,1~5.2.
Colby, K., and Enea, H., "Heuristic Methods for Computer Understan-ding of Natural Language in Context-Restricted On-Line Dia-logues," Mathematical Biosciences, 1967.3.
Hays, D., "Dependency Theory: A Formalism and Some Observations",V.40, December 1964.4.
Kay, M., "Experiments with a Powerful Parser", RAND, Santa Moniea,California, 1967.5.
Klein, S., "Automatic Paraphrasing in Essay Format", MechanicalTranslation, 1965.6.
Lamb, S., 'The Sememic Approach to Structural Semantics", AmericanAnthropologist 1964.7.
Schank, R., "A Conceptual Dependency Representation for a Computer-Oriented Semantics", Ph.D. Thesis University of Texas, Austin1969 (Also available as Stanford AI Memo 83, Stanford Arti-ficial Intelligence Project, Computer Science Department,Stanford University, Stanford, California.)8.
Schank, R., "A Notion of Linguistic Concept: A Prelude to MechanicalTranslation", Stanford AI Memo 7~.
December 1968.9.
Sgall, P., "Generation, Production and Translation", Presented to1965 International Conference on Computational Linguistics,New York.IO.
Thorne, J., Bratley, P., and Dewar, H., '~he Syntactic Analysis ofEnglish by Machine", in Machine Intelli~ence III, Universityof Edinburgh, 1968.II.
Walker, D., Chapin, P., Gels, M., and Gross, L., 'Recent Develop-ments in the MITRE Syntactic Analysis Procedure", MITRE Corp.,Bedford, Mass., June i~6.
