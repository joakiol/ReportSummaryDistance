Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 84?85,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsDiscriminative Models for Semi-Supervised Natural Language LearningSajib Dasgupta and Vincent NgHuman Language Technology Research InstituteUniversity of Texas at DallasRichardson, TX 75083-0688{sajib,vince}@hlt.utdallas.edu1 Discriminative vs. Generative ModelsAn interesting question surrounding semi-supervised learning for NLP is: should we usediscriminative models or generative models?
De-spite the fact that generative models have beenfrequently employed in a semi-supervised settingsince the early days of the statistical revolutionin NLP, we advocate the use of discriminativemodels.
The ability of discriminative models tohandle complex, high-dimensional feature spacesand their strong theoretical guarantees have madethem a very appealing alternative to their gen-erative counterparts.
Perhaps more importantly,discriminative models have been shown to offercompetitive performance on a variety of sequentialand structured learning tasks in NLP that aretraditionally tackled via generative models , suchas letter-to-phoneme conversion (Jiampojamarnet al, 2008), semantic role labeling (Toutanovaet al, 2005), syntactic parsing (Taskar et al,2004), language modeling (Roark et al, 2004), andmachine translation (Liang et al, 2006).
Whilegenerative models allow the seamless integrationof prior knowledge, discriminative models seemto outperform generative models in a ?no prior?,agnostic learning setting.
See Ng and Jordan (2002)and Toutanova (2006) for insightful comparisons ofgenerative and discriminative models.2 Discriminative EM?A number of semi-supervised learning systems canbootstrap from small amounts of labeled data usingdiscriminative learners, including self-training, co-training (Blum and Mitchell, 1998), and transduc-tive SVM (Joachims, 1999).
However, none of themseems to outperform the others across different do-mains, and each has its pros and cons.
Self-trainingcan be used in combination with any discriminativelearning model, but it does not take into account theconfidence associated with the label of each datapoint, for instance, by placing more weight on the(perfectly labeled) seeds than on the (presumablynoisily labeled) bootstrapped data during the learn-ing process.
Co-training is a natural choice if thedata possesses two independent, redundant featuresplits.
However, this conditional independence as-sumption is a fairly strict assumption and can rarelybe satisfied in practice; worse still, it is typically noteasy to determine the extent to which a dataset sat-isfies this assumption.
Transductive SVM tends tolearn better max-margin hyperplanes with the useof unlabeled data, but its optimization procedure isnon-trivial and its performance tends to deteriorate ifa sufficiently large amount of unlabeled data is used.Recently, Brefeld and Scheffer (2004) have pro-posed a new semi-supervised learning technique,EM-SVM, which is interesting in that it incorpo-rates a discriminative model in an EM setting.
Un-like self-training, EM-SVM takes into account theconfidence of the new labels, ensuring that the in-stances that are labeled with less confidence by theSVM have less impact on the training process thanthe confidently-labeled instances.
So far, EM-SVMhas been tested on text classification problems, out-performing transductive SVM.
It would be interest-ing to see whether EM-SVM can beat existing semi-supervised learners for other NLP tasks.843 Effectiveness of BootstrappingHow effective are the aforementioned semi-supervised learning systems in bootstrapping fromsmall amounts of labeled data?
While there are quitea few success stories reporting considerable perfor-mance gains over an inductive baseline (e.g., parsing(McClosky et al, 2008), coreference resolution (Ngand Cardie, 2003), and machine translation (Ueff-ing et al, 2007)), there are negative results too (seePierce and Cardie (2001), He and Gildea (2006),Duh and Kirchhoff (2006)).
Bootstrapping perfor-mance can be sensitive to the setting of the param-eters of these semi-supervised learners (e.g., whento stop, how many instances to be added to the la-beled data in each iteration).
To date, however, re-searchers have relied on various heuristics for pa-rameter selection, but what we need is a principledmethod for addressing this problem.
Recently, Mc-Closky et al (2008) have characterized the condi-tions under which self-training would be effectivefor semi-supervised syntactic parsing.
We believethat the NLP community needs to perform more re-search of this kind, which focuses on identifying thealgorithm(s) that achieve good performance under agiven setting (e.g., few initial seeds, large amountsof unlabeled data, complex feature space, skewedclass distributions).4 Domain AdaptationDomain adaptation has recently become a popularresearch topic in the NLP community.
Labeled datafor one domain might be used to train a initial classi-fier for another (possibly related) domain, and thenbootstrapping can be employed to learn new knowl-edge from the new domain (Blitzer et al, 2007).
Itwould be interesting to see if we can come up witha similar semi-supervised learning model for pro-jecting resources from a resource-rich language toa resource-scarce language.ReferencesJohn Blitzer, Mark Dredze, and Fernando Pereira.
2007.Biographies, bollywood, boom-boxes and blenders:Domain adaptation for sentiment classification.
InProceedings of the ACL.Avrim Blum and Tom Mitchell.
1998.
Combining la-beled and unlabeled data with co-training.
In Proceed-ings of COLT.Ulf Brefeld and Tobias Scheffer.
2004.
Co-EM supportvector learning.
In Proceedings of ICML.Kevin Duh and Katrin Kirchhoff.
2006.
Lexicon acqui-sition for dialectal Arabic using transductive learning.In Proceedings of EMNLP.Shan He and Daniel Gildea.
2006.
Self-training and co-training for semantic role labeling.Sittichai Jiampojamarn, Colin Cherry, and GrzegorzKondrak.
2008.
Joint processing and discriminativetraining for letter-to-phoneme conversion.
In Proceed-ings of ACL-08:HLT.Thorsten Joachims.
1999.
Transductive inference fortext classification using support vector machines.
InProceedings of ICML.Percy Liang, Alexandre Bouchard, Dan Klein, and BenTaskar.
2006.
An end-to-end discriminative approachto machine translation.
In Proceedings of the ACL.David McClosky, Eugene Charniak, and Mark Johnson.2008.
When is self-training effective for parsing?
InProceedings of COLING.Vincent Ng and Claire Cardie.
2003.
Weakly supervisednatural language learning without redundant views.
InProceedings of HLT-NAACL.Andrew Ng and Michael Jordan.
2002.
On discrimina-tive vs.generative classifiers: A comparison of logisticregression and Naive Bayes.
In Advances in NIPS.David Pierce and Claire Cardie.
2001.
Limitations ofco-training for natural language learning from largedatasets.
In Proceedings of EMNLP.Brian Roark, Murat Saraclar, Michael Collins, and MarkJohnson.
2004.
Discriminative language modelingwith conditional random fields and the perceptron al-gorithm.
In Proceedings of the ACL.Ben Taskar, Dan Klein, Michael Collins, Daphne Koller,and Christopher Manning.
2004.
Max-margin pars-ing.
In Proceedings of EMNLP.Kristina Toutanova, Aria Haghighi, , and Christopher D.Manning.
2005.
Joint learning improves semantic rolelabeling.
In Proceedings of the ACL.Kristina Toutanova.
2006.
Competitive generative mod-els with structure learning for NLP classification tasks.In Proceedings of EMNLP.Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar.2007.
Transductive learning for statistical machinetranslation.
In Proceedings of the ACL.85
