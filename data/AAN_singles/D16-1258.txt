Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2337?2342,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsHuman-in-the-Loop ParsingLuheng He Julian Michael Mike Lewis Luke ZettlemoyerComputer Science & EngineeringUniversity of WashingtonSeattle, WA{luheng,julianjm,mlewis,lsz}@cs.washington.eduAbstractThis paper demonstrates that it is possible fora parser to improve its performance with a hu-man in the loop, by posing simple questionsto non-experts.
For example, given the firstsentence of this abstract, if the parser is un-certain about the subject of the verb ?pose,?
itcould generate the question What would posesomething?
with candidate answers this pa-per and a parser.
Any fluent speaker cananswer this question, and the correct answerresolves the original uncertainty.
We applythe approach to a CCG parser, converting un-certain attachment decisions into natural lan-guage questions about the arguments of verbs.Experiments show that crowd workers can an-swer these questions quickly, accurately andcheaply.
Our human-in-the-loop parser im-proves on the state of the art with less than2 questions per sentence on average, with again of 1.7 F1 on the 10% of sentences whoseparses are changed.1 IntroductionThe size of labelled datasets has long been recog-nized as a bottleneck in the performance of nat-ural language processing systems (Marcus et al,1993; Petrov and McDonald, 2012).
Such datasetsare expensive to create, requiring expert linguistsand extensive annotation guidelines.
Even rela-tively large datasets, such as the Penn Treebank,are much smaller than required?as demonstratedby improvements from semi-supervised learning(S?gaard and Rish?j, 2010; Weiss et al, 2015).We take a step towards cheap, reliable annotationsby introducing human-in-the-loop parsing, whereTemple also said Sea Containers?
plan raises numer-ous legal, regulatory, financial and fairness issues, butdidn?t elaborate.Q: What didn?t elaborate?
[1] **** Temple[2] * Sea Containers?
plan[3] None of the above.Table 1: An automatically generated query from CCGbank.4 out of 5 annotators correctly answered Temple, providing asignal that can be used to improve parse predictions.non-experts improve parsing accuracy by answeringquestions automatically generated from the parser?soutput.
We develop the approach for CCG parsing,leveraging the link between CCG syntax and seman-tics to convert uncertain attachment decisions intonatural language questions.
The answers are used assoft constraints when re-parsing the sentence.Previous work used crowdsourcing for less struc-tured tasks such as named entity recognition (Wer-ling et al, 2015) and prepositional phrase attach-ment (Jha et al, 2010).
Our work is most relatedto that of Duan et al (2016), which automaticallygenerates paraphrases from n-best parses and gainedsignificant improvement by re-training from crowd-sourced judgments on two out-of-domain datasets.Choe and McClosky (2015) improve a parser by cre-ating paraphrases of sentences, and then parsing thesentence and its paraphrase jointly.
Instead of usingparaphrases, we build on the approach of QA-SRL(He et al, 2015), which shows that untrained crowdworkers can annotate predicate?argument structuresby writing question?answer pairs.Our experiments for newswire and biomedical2337text demonstrate improvements to parsing accuracyof 1.7 F1 on the sentences changed by re-parsing,while asking only less than 2 questions per sentence.The annotations we collected1 are a representation-independent resource that could be used to developnew models or human-in-the-loop algorithms forrelated tasks, including semantic role labeling andsyntactic parsing with other formalisms.2 Mapping CCG Parses to QueriesOur annotation task consists of multiple-choicewhat-questions that admit multiple answers.
To gen-erate them, we produce question?answer (QA) pairsfrom each parse in the 100-best scored output of aCCG parser and aggregate the results together.We designed the approach to generate querieswith high question confidence?questions shouldbe simple and grammatical, so annotators are morelikely to answer them correctly?and high answeruncertainty?the parser should be uncertain aboutthe answers, so there is potential for improvement.Our questions only apply to core arguments ofverbs where the argument phrase is an NP, whichaccount for many of the parser?s mistakes.
Preposi-tional phrase attachment mistakes are also a largesource of errors?we tried several approaches togenerate questions for these, but the greater ambi-guity and inconsistency among both annotators andthe gold parses made it difficult to extract meaning-ful signal from the crowd.Generating Question?Answer Pairs Figure 1shows how we generate QA pairs.
Each QA pair cor-responds to a dependency such that if the answer iscorrect, it indicates that the dependency is in the cor-rect parse.
We determine a verb?s set of argumentsby the CCG supertag assigned to it in the parse (seeSteedman (2000) for an introduction to CCG).
Forexample, in Figure 1 the word put takes the category((S\NP)/PP)/NP (not shown), indicating that it hasa subject, a prepositional phrase argument, and anobject.
CCG parsing assigns dependencies to eachargument position, even when the arguments are re-ordered (as with put?
pizza) or span long distances(as with eat?
I).1Our code and data are available at https://github.com/luheng/hitl_parsing.I want to eat the pizza you put on the tablesubj xcompsubj obj objsubj prep pobjsubj Verb obj prep xcompyou put the pizza on the tableI want to eat the pizzaI eat the pizzaDependency Question Answerwant?
I What wants to eat something?
Ieat?
I What would eat something?
Ieat?pizza What would something eat?
the pizzaput?you What put something?
youput?pizza What did something put?
the pizzaon?
table What did something put something on?
the tableFigure 1: From a labeled dependency graph, we extract phrasescorresponding to every argument of every verb using sim-ple heuristics.
We then create questions about dependencies,adding a would modal to untensed verbs and placing argumentsto the left or right of the verb based on its CCG category.
Weonly generate QA pairs for subj, obj, and pobj dependencies.To identify multiple answer options, we create QA pairs fromall parses in the 100-best list and pool equivalent questions withdifferent answers.
See Table 2 for example queries.To reduce the chance of parse errors causing non-sensical questions (for example, What did the pizzaput something on?
), we replace all noun phraseswith something and delete unnecessary prepositionalphrases.
The exception to this is with copular predi-cates, where we include the span of the argument inthe question (see Example 4 in Table 2).Grouping QA Pairs into Queries After generat-ing QA pairs for every parse in the 100-best outputof the parser, we pool the QA pairs by the head ofthe dependency used to generate them, its CCG cat-egory, and their question strings.
We also computemarginalized scores for each question and answerphrase by summing over the scores of all the parsesthat generated them.
Each pool becomes a query,and for each unique dependency used to generateQA pairs in that pool, we add a candidate answer tothe query by choosing the answer phrase that has thehighest marginalized score for that dependency.
Forexample, if some parses generated the answer phrasepizza for the dependency eat ?
pizza, but most ofthe high-scoring parses generated the answer phrasethe pizza, then only the pizza appears as an answer.2338Sentence Question Votes Answers(1) Structural Dynamics Research Corp. .
.
.
said itintroduced new technology in mechanical designautomation that will improve mechanical engineeringproductivity.What will improvesomething?0 Structural Dynamics ResearchCorp5 new technology0 mechanical design automation(2) He said disciplinary proceedings are confidentialand declined to comment on whether any are being heldagainst Mr. Trudeau.What wouldcomment?5 he0 disciplinary proceedings(3) To avoid these costs, and a possible default,immediate action is imperative.What wouldsomething avoid?4 these costs3 a possible default(4) The price is a new high for California CabernetSauvignon, but it is not the highest.What is not thehighest?2 the price3 it(5) Kalipharma is a New Jersey-based pharmaceuticalsconcern that sells products under the Purepac label.What sellssomething?5 Kalipharma0 a New Jersey-based pharma-ceuticals concern(6) Further, he said, the company doesn?t have thecapital needed to build the business over the next yearor two.What would buildsomething?4 the company1 the capital(7) Timex had requested duty-free treatment for manytypes of watches, covered by 58 different U.S. tariffclassifications.What would becovered?0 Timex0 duty-free treatment2 many types of watches3 watches(8) You either believe Seymour can do it again or youdo n?t .
What does?3 you0 Seymour2 None of the aboveTable 2: Example annotations from the CCGbank development set.
Answers that agree with the gold parse are in bold.
The answerchoice None of the above was present for all examples, but we only show it when it was chosen by annotators.From the resulting queries, we filter out questionsand answers whose marginalized scores are below acertain threshold and queries that only have one an-swer choice.
This way we only ask confident ques-tions with uncertain answer lists.3 CrowdsourcingWe collected data on the crowdsourcing platformCrowdFlower.2 Annotators were shown a sentence,a question, and a list of answer choices.
Annota-tors could choose multiple answers, which was use-ful in case of coordination (see Example 3 in Ta-ble 2).
There was also a None of the above optionfor when no answer was applicable or the questionwas nonsensical.We instructed annotators to only choose optionsthat explicitly and directly answer the question,to encourage their answers to closely mirror syn-tax.
We also instructed them to ignore who/whatand someone/something distinctions and overlookmistakes where the question was missing a nega-tion.
The instructions included 6 example queries2www.crowdflower.comwith answers and explanations.
We used Crowd-Flower?s quality control mechanism, displaying pre-annotated queries 20% of the time and requiring an-notators to maintain high accuracy.Dataset Statistics Table 3 shows how many sen-tences we asked questions for and the total numberof queries annotated.
We collected annotations forthe development and test set for CCGbank (Hock-enmaier and Steedman, 2007) as in-domain dataand the test set of the Bioinfer corpus (Pyysalo etal., 2007) as out-of-domain.
The CCGbank devel-opment set was used for building question genera-tion heuristics and setting hyperparameters for re-parsing.5 annotators answered each query; on CCGbankwe required 85% accuracy on test questions andon Bioinfer we set the threshold at 80% becauseof the difficulty of the sentences.
Table 4 showsinter-annotator agreement.
Annotators unanimouslychose the same set of answers for over 40% of thequeries; an absolute majority is achieved for over90% of the queries.2339Dataset Sentences Covered Queries Q/SCCG-Dev 1913 1155 1904 1.7CCG-Test 2407 1460 2511 1.7Bioinfer 500 360 680 1.9Table 3: Sentence coverage, number of queries annotated, andaverage number of queries per sentence (Q/S).k-Agreed CCG-Dev CCG-Test Bioinfer5 48.0% 40.2% 47.7%?
4 76.6% 68.0% 75.0%?
3 94.9% 91.5% 94.0%Table 4: The percentage of queries with at least k annotatorsagreeing on the exact same set of answers.Qualitative Analysis Table 2 shows examplequeries from the CCGbank development set.
Exam-ples 1 and 2 show that workers could annotate long-range dependencies and scoping decisions, whichare challenging for existing parsers.However, there are some cases where annota-tors disagree with the gold syntax, mostly involv-ing semantic phenomena which are not reflectedin the syntactic structure.
Many cases involve co-reference, where annotators often prefer a propernoun referent over a pronoun or indefinite (see Ex-amples 4 and 5), even if it is not the syntactic ar-gument of the verb.
Example 6 shows a complexcontrol structure, where the gold CCGbank syntaxdoes not recover the true agent of build.
CCGbankalso does not distinguish between subject and objectcontrol.
For these cases, our method could be usedto extend existing treebanks.
Another common errorcase involved partitives and related constructions,where the correct attachment is subtle?as reflectedby the annotators?
split decision in Example 7.Question Quality Table 5 shows the percentage ofquestions that are answered with None of the above(written N/A below) by at most k annotators.
On alldomains, about 80% of the queries are consideredanswerable by all 5 annotators.
To have a betterunderstanding of the quality of automatically gen-erated questions, we did a manual analysis on 50questions for sentences from the CCGbank devel-opment set that are marked N/A by more than oneannotator.
Among the 50 questions, 31 of them areeither generated from an incorrect supertag or unan-swerable given the candidates.
So the N/A answerk-N/A CCG-Dev CCG-Test Bioinfer0 77.6% 81.6% 79.3%?
1 89.6% 92.6% 89.1%?
2 93.8% 96.1% 92.8%Table 5: The percentage of queries with at most k annotatorschoosing the None of the above (N/A) option.can provide useful signal that the parses that gen-erated the question are likely incorrect.
Commonmistakes in question generation include: bad argu-ment span in a copula question (4 questions), badmodality/negation (3 questions), and missing argu-ment or particle (5 questions).
Example 8 in Table 2shows an example of a nonsensical question.
Whilethe parses agreed with the gold category S\NP, thequestion they generated omitted the negation and theverb phrase that was elided in the original sentence.In this case, 3 out of 5 annotators were able to an-swer with the correct dependency, but such mistakescan make re-parsing more challenging.Cost and Speed We paid 6 cents for each answer.With 5 judgments per query, 20% test questions, andCrowdFlower?s 20% service fee, the average costper query was about 46 cents.
On average, we col-lected about 1000 judgments per hour, so we wereable to annotate all the queries generated from theCCGbank test set within 15 hours.4 Re-Parsing with QA AnnotationTo improve the output of the parser, we re-parseeach sentence with an augmented scoring functionthat penalizes parses for disagreeing with annota-tors?
choices.
If q is a question, a is an answer to q, dis the dependency that produced the QA pair ?q, a?,and v(a) annotators chose a, we add re-parsing con-straints as follows:?
If v(None of the above) ?
T+, penalize parsesthat agree with q?s supertag on the verb by wt?
If v(a) ?
T?, penalize parses containing d by w??
If v(a) ?
T+, penalize parses that do not containd by w+where T+, T?, wt, w?, and w+ are hyperparame-ters.
We incorporate these penalties into the parsingmodel during decoding.
By using soft constraints,we mitigate the risk of incorrect annotations wors-ening a high-confidence parse.2340Data L16 HITLCCG-Dev 87.9 88.4CCG-Test 88.1 88.3Bioinfer 82.2 82.8Table 6: CCG parsing accuracy with human in the loop (HITL)versus the state-of-the-art baseline (L16) in terms of labeled F1score.
For both in-domain and out-domain, we have a modestgain over the entire corpus.Some errors are predictable: for example, if a isa non-possessive pronoun and is closer to the verbthan its referent a?, annotators often choose a?
whena is correct (See Example 4 in Table 2).
If a is a sub-span of another answer a?
and their votes differ by atmost one (See Example 7 in Table 2), it is unlikelythat both a and a?
are correct.
In these cases weuse disjunctive constraints, where the parse needs tohave at least one of the desired dependencies.Experimental Setup We use Lewis et al (2016)?sstate-of-the-art CCG parser for our baseline.
Wechose the following set of hyperparameters basedon performance on development data (CCG-Dev):w+ = 2.0, w?
= 1.5, wt = 1.0, T+ = 3, T?
= 0.In the Bioinfer dataset, we found during develop-ment that the pronoun/subspan heuristics were notas useful, so we did not use them in re-parsing.Results Table 6 shows our end-to-end parsing re-sults.
The larger improvement on out-of-domainsentences shows the potential for using our methodfor domain adaptation.
There is a much smallerimprovement on test data than development data,which may be related to the lower annotator agree-ment reported in Table 4.There was much larger improvement (1.7 F1) onthe subset of sentences that are changed after re-parsing, as shown in Table 7.
This suggests that ourmethod could be effective for semi-supervised learn-ing or re-training parsers.
Overall improvements onCCGbank are modest, due to only modifying 10%of sentences.5 Discussion and Future WorkWe introduced a human-in-the-loop framework forautomatically correcting certain parsing mistakes.Our method identifies attachment uncertainty forcore arguments of verbs and automatically generatesData L16 HITL Pct.CCG-Dev 83.9 87.1 12%CCG-Test 84.2 85.9 10%Table 7: Improvements of CCG parsing accuracy on changedsentences for in-domain data.
We achieved significant improve-ment over the 10%?12% (Pct.)
sentences that were changed byre-parsing.questions that can be answered by untrained annota-tors.
These annotations improve performance, par-ticularly on out-of-domain data, demonstrating forthe first time that untrained annotators can improvestate-of-the-art parsers.Sentences modified by our framework show sub-stantial improvements in accuracy, but only 10% ofsentences are changed, limiting the effect on overallaccuracy.
This work is a first step towards a com-plete approach to human-in-the-loop parsing.Future work will explore the possibility of askingquestions about other types of parsing uncertainties,such as nominal and adjectival argument structure,and a more thorough treatment of prepositional-phrase attachment, including distinctions betweenarguments and adjuncts.
We hope to scale thesemethods to large unlabelled corpora or other lan-guages, to provide data for re-training parsers.AcknowledgmentsThis work was supported by the NSF (IIS-1252835,IIS-1562364), DARPA under the DEFT programthrough the AFRL (FA8750-13-2-0019), an AllenDistinguished Investigator Award, and a gift fromGoogle.
We are grateful to Chloe?
Kiddon for help-ful comments on the paper, and Kenton Lee for helpwith the CCG parser.
We would also like to thankour workers on Crowdflower for their annotation andthe anonymous reviewers for their valuable feed-back.ReferencesDo Kook Choe and David McClosky.
2015.
Parsingparaphrases with joint inference.
In Proceedings ofthe 53rd Annual Meeting of the Association for Com-putational Linguistics.Manjuan Duan, Ethan Hill, and Michael White.
2016.Generating disambiguating paraphrases for struc-2341turally ambiguous sentences.
In Proceedings of the10th Linguistic Annotation Workshop.Luheng He, Mike Lewis, and Luke Zettlemoyer.
2015.Question-answer driven semantic role labeling: Usingnatural language to annotate natural language.
In Pro-ceedings of the 2015 Conference on Empirical Meth-ods in Natural Language Processing.Julia Hockenmaier and Mark Steedman.
2007.
Ccgbank:a corpus of ccg derivations and dependency structuresextracted from the penn treebank.
Computational Lin-guistics.Mukund Jha, Jacob Andreas, Kapil Thadani, Sara Rosen-thal, and Kathleen McKeown.
2010.
Corpus creationfor new genres: A crowdsourced approach to pp at-tachment.
In Proceedings of the NAACL HLT 2010Workshop on Creating Speech and Language Datawith Amazon?s Mechanical Turk.Mike Lewis, Kenton Lee, and Luke Zettlemoyer.
2016.Lstm ccg parsing.
In Proceedings of the Human Lan-guage Technology Conference of the North AmericanChapter of the Association of Computational Linguis-tics.Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beat-rice Santorini.
1993.
Building a large annotated cor-pus of english: The penn treebank.
ComputationalLinguistics.Slav Petrov and Ryan McDonald.
2012.
Overview ofthe 2012 Shared Task on Parsing the Web.
Notesof the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL).Sampo Pyysalo, Filip Ginter, Juho Heimonen, JariBjo?rne, Jorma Boberg, Jouni Ja?rvinen, and TapioSalakoski.
2007.
Bioinfer: a corpus for informationextraction in the biomedical domain.
BMC bioinfor-matics.Anders S?gaard and Christian Rish?j.
2010.
Semi-supervised dependency parsing using generalized tri-training.
In Proceedings of the 23rd InternationalConference on Computational Linguistics.Mark Steedman.
2000.
The syntactic process.David Weiss, Chris Alberti, Michael Collins, and SlavPetrov.
2015.
Structured training for neural networktransition-based parsing.
In Proceedings of the 53rdAnnual Meeting of the Association for ComputationalLinguistics and the 7th International Joint Conferenceon Natural Language Processing.Keenon Werling, Arun Tejasvi Chaganty, Percy S Liang,and Christopher D Manning.
2015.
On-the-job learn-ing with bayesian decision theory.
In Advances inNeural Information Processing Systems.2342
