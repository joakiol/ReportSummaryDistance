Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 1?9,Sofia, Bulgaria, August 9, 2013. c?2013 Association for Computational LinguisticsMeaning Unit Segmentation in English and Chinese: a New Approach toDiscourse PhenomenaJennifer Williams ?1,2, Rafael Banchs2, and Haizhou Li21Department of Linguistics, Georgetown University, Washington, D.C., USA2Institute for Infocomm Research, 1 Fusionpolis Way, Singaporejaw97@georgetown.edu {rembanchs,hli}@i2r.a-star.edu.sgAbstractWe present a new approach to dialogueprocessing in terms of ?meaning units?.
Inour annotation task, we asked speakers ofEnglish and Chinese to mark boundarieswhere they could construct the maximalconcept using minimal words.
We com-pared English data across genres (news,literature, and policy).
We analyzed theagreement for annotators using a state-of-the-art segmentation similarity algorithmand compared annotations with a randombaseline.
We found that annotators areable to identify meaning units systemati-cally, even though they may disagree onthe quantity and position of units.
Ouranalysis includes an examination of phrasestructure for annotated units using con-stituency parses.1 IntroductionWhen humans translate and interpret speech inreal-time, they naturally segment speech in ?min-imal sense units?
(Ole?ron & Nanpon, 1965;Ben?
?tez & Bajo, 1998) in order to convey thesame information from one language to another asthough there were a 1-to-1 mapping of conceptsbetween both languages.
Further, it is known thatpeople can hold up to 7+/- 2 ?chunks?
of informa-tion in memory at a time by creating and applyingmeaningful organization schemes to input (Miller,1956).
However, there is no definitive linguisticdescription for the kind of ?meaning units?
thathuman translators create (Signorelli et al 2011;Hamon et al 2009; Mima et al 1998).The ability to chunk text according to units ofmeaning is key to developing more sophisticatedmachine translation (MT) systems that operate in?
Now affiliated with Massachusetts Institute of Tech-nology Lincoln Laboratory.real-time, as well as informing discourse process-ing and natural language understanding (NLU)(Kola?r?, 2008).
We present an approach to dis-course phenomena to address Keller?s (2010) callto find a way to incorporate ?cognitive plausibil-ity?
into natural language processing (NLP) sys-tems.
As it has been observed that human trans-lators and interpreters naturally identify a certainkind of ?meaning unit?
when translating speechin real-time (Ole?ron & Nanpon, 1965; Ben?
?tez &Bajo, 1998), we want to uncover the features ofthose units in order to automatically identify themin discourse.This paper presents an experimental approachto annotating meaning units using human anno-tators from Mechanical Turk.
Our goal was touse the results of human judgments to informus if there are salient features of meaning unitsin English and Chinese text.
We predicted thathuman-annotated meaning units should systemat-ically correspond to some other linguistic featuresor combinations of those features (i.e.
syntax,phrase boundaries, segments between stop words,etc.).
We are interested in the following researchquestions:?
At what level of granularity do English andChinese speakers construct meaning units intext??
Do English and Chinese speakers organizemeaning units systematically such that mean-ing unit segmentations are not random??
How well do English and Chinese speakersagree on meaning unit boundaries??
Are there salient syntactic features of mean-ing units in English and Chinese??
Can we automatically identify a 1-to-1 map-ping of concepts for parallel text, even if thereis paraphrasing in one or both languages?1While we have not built a chunker or classifierfor meaning unit detection, it is our aim that thiswork will inform how to parse language system-atically in a way that is human-understandable.
Itremains to be seen that automatic tools can be de-veloped to detect meaning units in discourse.
Still,we must be informed as to what kinds of chunksare appropriate for humans to allow them to under-stand information transmitted during translation(Kola?r?, 2008).
Knowledge about meaning unitscould be important for real-time speech process-ing, where it is not always obvious where an ut-terance begins and ends, due to any combinationof natural pauses, disfluencies and fillers such as?like, um..?.
We believe this work is a step towardscreating ultra-fast human-understandable simulta-neous translation systems that can be used for con-versations in different languages.This paper is organized as follows: Section 2discusses related work, Section 3 describes thesegmentation similarity metric that we used formeasuring annotator agreement, Section 4 de-scribes our experiment design, Section 5 showsexperiment results, Section 6 provides analysis,and Section 7 discusses future work.2 Related WorkAt the current state of the art, automatic simultane-ous interpretation systems for speech function tooslowly to allow people to conduct normal-pacedconversations in different languages.
This prob-lem is compounded by the difficulty of identifyingmeaningful endpoints of utterances before trans-mitting a translation.
For example, there is a per-ceived lag time for speakers when trying to bookflights or order products over the phone.
This lagtime diminishes conversation quality since it takestoo long for each speaker to receive a translationat either end of the system (Paulik et al 2009).
Ifwe can develop a method to automatically identifysegments of meaning as they are spoken, then wecould significantly reduce the perceived lag timein real-time speech-to-speech translation systemsand improve conversation quality (Baobao et al2002; Hamon et al 2009).The problem of absence of correspondencearises when there is a lexical unit (single wordsor groups of words) that occurs in L1 but notin L2 (Lambert et al 2005).
It happens whenwords belonging to a concept do not correspond tophrases that can be aligned in both languages.
Thisproblem is most seen when translating speech-to-speech in real-time.
One way to solve this prob-lem is to identify units for translation that cor-respond to concepts.
A kind of meaning unithad been previously proposed as information units(IU), which would need to be richer than seman-tic roles and also be able to adjust when a mis-take or assumption is realized (Mima et al 1998).These units could be used to reduce the explosionof unresolved structural ambiguity which happenswhen ambiguity is inherited by a higher level syn-tactic structure, similar to the use of constituentboundaries for transfer-driven machine translation(TDMT) (Furuse et al 1996).The human ability to construct concepts in-volves both bottom-up and top-down strategies inthe brain.
These two kinds of processes inter-act and form the basis of comprehension (Kintsch,2005).
The construction-integration model (CI-2)describes how meaning is constructed from bothlong-term memory and short-term memory.
Oneof the challenges of modeling meaning is that itrequires a kind of world-knowledge or situationalknowledge, in addition to knowing the meaningsof individual words and knowing how words canbe combined.
Meaning is therefore constructedfrom long-term memory ?
as can be modeled bylatent semantic analysis (LSA) ?
but also fromshort-term memory which people use in the mo-ment (Kintsch & Mangalath, 2011).
In our work,we are asking annotators to construct meaningfrom well-formed text and annotate where units ofmeaning begin and end.3 Similarity AgreementWe implemented segmentation similarity (S) fromFournier and Inkpen (2012).
Segmentation sim-ilarity was formulated to address some gaps ofthe WindowDiff (WD) metric, including unequalpenalty for errors as well as the need to addpadding to the ends of each segmentation (Pevzner& Hearst, 2002).
There are 3 types of segmenta-tion errors for (S), listed below:1. s1 contains a boundary that is off by n poten-tial boundaries in s22.
s1 contains a boundary that s2 does not, or3.
s2 contains a boundary that s1 does notThese three types of errors are understood astranspositions in the case of error type 1, and as2substitutions in the case of error types 2 and 3.Note that there is no distinction between insertionsand deletions because neither of the segmentationsare considered reference or hypothesis.
We showthe specification of (S) in (1):S(si1,si2) =t ?
mass(i)  t  d(si1,si2,T )t ?
mass(i)  t(1)such that S scales the cardinality of the set ofboundary types t because the edit distance func-tion d(si1,si2,T ) will return a value for potentialboundaries of [0, t ?
mass(i)] normalized by thenumber of potential boundaries per boundary type.The value of mass(i) depends on task, in ourwork we treat mass units as number of words, forEnglish, and number of characters for Chinese.Since our annotators were marking only units ofmeaning, there was only one boundary type, and(t = 1).
The distance function d(si1,si2,T ) is theedit distance between segments calculated as thenumber of boundaries involved in transpositionoperations subtracted from the number of substi-tution operations that could occur.
A score of 1.0indicates full agreement whereas a score of 0 indi-cates no agreement.In their analysis and comparison of this newmetric, Fournier and Inkpen (2012) demonstratedthe advantages of using (S) over using (WD)for different kinds of segmentation cases suchas maximal/minimal segmentation, full misses,near misses, and segmentation mass scale effects.They found that in each of these cases (S) wasmore stable than (WD) over a range of segmentsizes.
That is, when considering different kindsof misses (false-positive, false-negative, and both),the metric (S) is less variable to internal segmentsize.
These are all indications that (S) is a morereliable metric than (WD).Further, (S) properly takes into account chanceagreement - called coder bias - which arises insegmentation tasks when human annotators eitherdecide not to place a boundary at all, or are un-sure if a boundary should be placed.
Fournier andInkpen (2012) showed that metrics that follow (S)specification reflect most accurately on coder bias,when compared to mean pairwise 1   WD met-rics.
Therefore we have decided to use segmenta-tion similarity as a metric for annotator agreement.4 Experiment DesignThis section describes how we administered ourexperiment as an annotation task.
We surveyedparticipants using Mechanical Turk and presentedparticipants with either English or Chinese text.While the ultimate goal of this research direc-tion is to obtain meaning unit annotations forspeech, or transcribed speech, we have used well-structured text in our experiment in order to findout more about the potential features of meaningunits in the simplest case.4.1 Sample Text PreparationGenre: Our text data was selected from three dif-ferent genres for English (news, literature, andpolicy) and one genre for Chinese (policy).
Weused 10 articles from the Universal Declaration ofHuman Rights (UDHR) in parallel for English andChinese.
The English news data (NEWS) con-sisted of 10 paragraphs that were selected onlinefrom www.cnn.com and reflected current eventsfrom within the United States.
The English liter-ature data (LIT) consisted of 10 paragraphs fromthe novel Tom Sawyer by Mark Twain.
The En-glish and Chinese UDHR data consisted of 12 par-allel paragraphs from the Universal Declaration ofHuman Rights.
The number of words and numberof sentences by language and genre is presentedbelow in Table 1.Preprocessing: To prepare the text samples forannotation, we did some preprocessing.
We re-moved periods and commas in both languages,since these markings can give structure and mean-ing to the text which could influence annotator de-cisions about meaning unit boundaries.
For theEnglish data, we did not fold to lowercase and weacknowledge that this was a design oversight.
TheChinese text was automatically segmented intowords before the task using ICTCLAS (Zhang etal., 2003).
This was done in order to encourageChinese speakers to look beyond the character-level and word-level, since word segmentation isa well-known NLP task for the Chinese language.The Chinese UDHR data consisted of 856 charac-ters.
We placed checkboxes between each word inthe text.4.2 Mechanical Turk AnnotationWe employed annotators using Amazon Mechan-ical Turk Human Intelligence Tasks (HITs).
Allinstructions for the task were presented in En-3Language and Genre # words # SentencesChinese UDHR 485 20English NEWS 580 20English LIT 542 27English UDHR 586 20Table 1: Number of words and sentences by lan-guage and genre.glish.
Each participant was presented with a set ofnumbered paragraphs with a check-box betweeneach word where a boundary could possibly ex-ist.
In the instructions, participants were askedto check the boxes between words correspond-ing to the boundaries of meaning units.
Theywere instructed to create units of meaning largerthan words but that are also the ?maximal conceptthat you can construct that has the minimal set ofwords that can be related to each individual con-cept?1.
We did not provide marked examples tothe annotators so as to avoid influencing their an-notation decisions.Participants were given a maximum of 40 min-utes to complete the survey and were paid USD$1.00 for their participation.
As per AmazonMechanical Turk policy, each of the participantswere at least 18 years of age.
The annotationtask was restricted to one task per participant, inother words if a participant completed the EnglishNEWS annotation task then they could not partic-ipate in the Chinese UDHR task, etc.
We did nottest any of the annotators for language aptitudeor ability, and we did not survey language back-ground.
It is possible that for some annotators,English and Chinese were not a native language.5 ResultsWe omitted survey responses for which partici-pants marked less than 30 boundaries total, as wellas participants who completed the task in less than5 minutes.
We did this in an effort to eliminateannotator responses that might have involved ran-dom marking of the checkboxes, as well as thosewho marked only one or two checkboxes.
We de-cided it would be implausible that less than 30boundaries could be constructed, or that the task1The definition of ?meaning units?
we provide is very am-biguous and can justify for different people understanding thetask differently.
However, this is part of what we wanted tomeasure, as giving a more precise and operational definitionwould bias people to some specific segmentation criteria.could be completed in less than 5 minutes, con-sidering that there were several paragraphs andsentences for each dataset.
After we removedthose responses, we had solicited 47 participantsfor English NEWS, 40 participants for EnglishLIT, 59 participants for English UDHR, and 10participants for Chinese UDHR.
The authors ac-knowledge that the limited sample size for Chi-nese UDHR data does not allow a direct compar-ison across the two languages, however we haveincluded it in results and analysis as supplementalfindings and encourage future work on this taskacross multiple languages.
We are unsure as towhy there was a low number of Chinese annota-tors in this task, except perhaps the task was not asaccessible to native Chinese speakers because thetask instructions were presented in English.5.1 Distributions by GenreWe show distributions of number of annotatorsand number of units identified for each languageand genre in Figures 1 ?
4.
For each of thelanguage/genres, we removed one annotator be-cause the number of units that they found wasgreater than 250, which we considered to bean outlier in our data.
We used the Shapiro-Wilk Test for normality to determine which, ifany, of these distributions were normally dis-tributed.
We failed to reject the null hypothesis forChinese UDHR (p = 0.373) and English NEWS(p = 0.118), and we rejected the null hypothe-sis for English LIT (p = 1.8X10 04) and EnglishUDHR (p = 1.39X10 05).Dataset N Avg AvgUnits Words/UnitChinese UDHR 9 70.1 ?English NEWS 46 84.9 6.8English LIT 39 85.4 6.3English LIT G1 26 66.9 8.1English LIT G2 13 129.0 4.2English UDHR 58 90.1 6.5English UDHR G1 17 52.2 11.2English UDHR G2 19 77.3 7.6English UDHR G3 22 132.2 4.4Table 2: Number of annotators (N), average num-ber of units identified, average number of wordsper unit identified, by language and genre.Since the number of units were not normallydistributed for English LIT and English UDHR,4Figure 1: Distribution of total number of annota-tions per annotator for Chinese UDHR.Figure 2: Distribution of total number of annota-tions per annotator for English UDHR.Figure 3: Distribution of total number of annota-tions per annotator for English NEWS.we used 2-sample Kolmogorov-Smirnov (KS)Tests to identify separate distributions for each ofthese genres.
We found 3 distinct groups in En-glish UDHR (G1?G3) and 2 distinct groups in En-glish LIT (G1 and G2).
Table 2 provides moreFigure 4: Distribution of total number of annota-tions per annotator for English LIT.detailed information about distributions for num-ber of annotations, as well as the average numberof units found, and average words per unit.
Thisinformation informs us as to how large or smallon average the meaning units are.
Note that in Ta-ble 2 we include information for overall EnglishUDHR and overall English LIT distributions forreference.
The authors found it interesting that,from Table 2, the number of words per meaningunit generally followed the 7 +/- 2 ?chunks?
phe-nomenon, where chunks are words.5.2 Annotator AgreementEven though some of the annotators agreed aboutthe number of units, that does not imply thatthey agreed on where the boundaries were placed.We used segmentation similarity (S) as a metricfor annotator agreement.
The algorithm requiresspecifying a unit of measurement between bound-aries ?
in our case we used word-level units forEnglish data and character-level units for Chinesedata.
We calculated average similarity agreementfor segment boundaries pair-wise within-groupfor annotators from each of the 9 language/genredatasets, as presented in Table 3.While the segmentation similarity agreementsseem to indicate high annotator agreement, wewanted to find out if that agreement was bet-ter than what we could generate at random, sowe compared annotator agreement with randombaselines.
To generate the baselines, we usedthe average number of segments per paragraph ineach language/genre dataset and inserted bound-aries at random.
For each of the 9 language/genredatasets, we generated 30 baseline samples.
Wecalculated the baseline segmentation similarity5Dataset (S) (SBL)Chinese UDHR 0.930 0.848English NEWS 0.891 0.796English LIT 0.875 0.790English LIT G1 0.929 0.824English LIT G2 0.799 0.727English UDHR 0.870 0.802English UDHR G1 0.929 0.848English UDHR G2 0.910 0.836English UDHR G3 0.826 0.742Table 3: Within-group segmentation similarityagreement (S) and segmentation similarity agree-ment for random baseline (SBL).
(SBL) in the same way using average pair-wiseagreement within-group for all of the baselinedatasets, shown in Table 3.For English UDHR, we also calculated averagepair-wise agreement across groups, shown in Ta-ble 4.
For example, we compared English UDHRG1 with English UDHR G2, etc.
Human annota-tors consistently outperformed the baseline acrossgroups for English UDHR.Dataset (S) (SBL)English UDHR G1?G2 0.916 0.847English UDHR G1?G3 0.853 0.782English UDHR G2?G3 0.857 0.778Table 4: English UDHR across-group segmenta-tion similarity agreement (S) and random baseline(SBL).6 AnalysisConstructing concepts in this task is systematicas was shown from the segmentation similarityscores.
Since we know that the annotators agreedon some things, it is important to find out whatthey have agreed on.
In our analysis, we exam-ined unit boundary locations across genres in addi-tion to phrase structure using constituency parses.In this section, we begin to address another ofour original research questions regarding how wellspeakers agree on meaning unit boundary posi-tions across genres and which syntactic featuresare the most salient for meaning units.6.1 Unit Boundary Positions for GenresBoundary positions are interesting because theycan potentially indicate if there are salient partsof the texts which stand out to annotators acrossgenres.
We have focused this analysis across gen-res for the overall data for each of the 4 lan-guage/genre pairs.
Therefore, we have omitted thesubgroups ?
English UDHR groups (G1,G2, G3)and English LIT groups (G1, G2).
Although seg-mentation similarity is greater within-group fromTable 3, this was not enough to inform us of whichboundaries annotators fully agree on.
For each ofthe datasets, we counted the number of annotatorswho agreed on a given boundary location and plot-ted histograms.
In these plots we show the numberof annotators of each potential boundary betweenwords.
We show the resulting distributions in Fig-ures 5 ?
8.Figure 5: Annotated boundary positions ChineseUDHR.Figure 6: Annotated boundary positions EnglishUDHR.While there were not many annotators for theChinese UDHR data, we can see from Figure 56Figure 7: Annotated boundary positions EnglishNEWS.Figure 8: Annotated boundary positions EnglishLIT.that at most 4 annotators agreed on boundary po-sitions.
We can see from Figures 6 ?
8 that thereis high frequency of agreement in the text whichcorresponds to paragraph boundaries for the En-glish data, however paragraph boundaries were ar-tificially introduced into the experiment becauseeach paragraph was numbered.Since we had removed all punctuation mark-ings, including periods and commas for both lan-guages, it is interesting to note there was not fullagreement about sentence boundaries.
While wedid not ask annotators to mark sentence bound-aries, we hoped that these would be picked up bythe annotators when they were constructing mean-ing units in the text.
Only 3 sentence boundarieswere identified by at most 2 Chinese UDHR an-notators.
On the other hand, all of the sentenceboundaries were idenfied for English UDHR andEnglish NEWS, and one sentence boundary wasunmarked for English LIT.
However, there wereno sentence boundaries in the English data thatwere marked by all annotators - in fact the sin-gle most heavily annotated sentence boundary wasfor English NEWS, where 30% of the annota-tors marked it.
The lack for identifying sentenceboundaries could be due to an oversight by anno-tators, or it could also be indicative of the difficultyand ambiguity of the task.6.2 Phrase StructureTo answer our question of whether or not there aresalient syntactic features for meaning units, we didsome analysis with constituency phrase structureand looked at the maximal projections of meaningunits.
For each of the 3 English genres (UDHR,NEWS, and LIT) we identified boundaries whereat least 50% of the annotators agreed.
For the Chi-nese UDHR data, we identified boundaries whereat least 30% of annotators agreed.
We used theStanford PCFG Parser on the original English andChinese text to obtain constituency parses (Klein& Manning, 2003), then aligned the agreeablesegment boundaries with the constituency parses.We found the maximal projection correspondingto each annotated unit and we calculated the fre-quency of each of the maximal projections.
Thefrequencies of part-of-speech for maximal projec-tions are shown in Tables 5 - 8.
Note that the part-of-speech tags reflected here come from the Stan-ford PCFG Parser.Max.
Projection Description Freq.S, SBAR, SINV Clause 28PP Prepositional Phrase 14VP Verb Phrase 11NP Noun Phrase 5ADJP Adjective Phrase 3ADVP Adverb Phrase 1Table 5: Frequency of maximal projections for En-glish UDHR, for 62 boundaries.Max.
Projection Description Freq.S, SBAR, SINV Clause 30VP Verb Phrase 23NP Noun Phrase 11PP Prepositional Phrase 3ADVP Adverb Phrase 2Table 6: Frequency of maximal projections for En-glish NEWS, for 69 boundaries.7Max.
Projection Description Freq.S, SBAR Clause 32VP Verb Phrase 10NP Noun Phrase 3PP Prepositional Phrase 2ADVP Adverb Phrase 2Table 7: Frequency of maximal projections for En-glish LIT, for 49 boundaries.Max.
Projection Description Freq.NN, NR Noun 22VP Verb Phrase 8NP Noun Phrase 8CD Determiner 3ADVP Adverb Phrase 1AD Adverb 1VV Verb 1JJ Other noun mod.
1DP Determiner Phrase 1Table 8: Frequency of maximal projections forChinese UDHR, for 46 boundaries.Clauses were by far the most salient bound-aries for annotators of English.
On the other hand,nouns, noun phrases, and verb phrases were themost frequent for annotators of Chinese.
Thereis some variation across genres for English.
Thisanalysis begins to address whether or not it ispossible to identify syntactic features of meaningunits, however it leaves open another question asto if it is possible to automatically identify a 1-to-1mapping of concepts across languages.7 Discussion and Future WorkWe have presented an experimental frameworkfor examining how English and Chinese speakersmake meaning out of text by asking them to la-bel places that they could construct concepts withas few words as possible.
Our results show thatthere is not a unique ?meaning unit?
segmentationcriteria among annotators.
However, there seemsto be some preferential trends on how to performthis task, which suggest that any random segmen-tation is not acceptable.
As we have simplified thetask of meaning unit identification by using well-structured text from the Universal Declaration ofHuman Rights, news, and literature, future workshould examine identifying meaning units in tran-scribed speech.Annotators for the English UDHR and EnglishLIT datasets could be characterized by their dif-ferent granularities of annotation in terms of num-ber of units identified.
These observations are in-sightful to our first question: what granularity dopeople use to construct meaning units?
For some,meaning units consist of just a few words, whereasfor others they consist of longer phrases or possi-bly clauses.
As we did not have enough responsesfor the Chinese UDHR data, we are unable to com-ment if identification of meaning units in Chinesefit a similar distribution as with English and weleave in-depth cross-language analysis to futurework.A particularly interesting finding was that hu-man annotators share agreement even acrossgroups, as seen from Table 4.
This means that al-though annotators may not agree on the number ofmeaning units found, they do share some agree-ment regarding where in the text they are creatingthe meaning units.
These findings seem to indicatethat annotators are creating meaning units system-atically regardless of granularity.Our findings suggest that different people orga-nize and process information differently.
This is avery important conclusion for discourse analysis,machine translation and many other applicationsas this suggests that there is no optimal solutionto the segmentation problems considered in thesetasks.
Future research should focus on better un-derstanding the trends we identified and the ob-served differences among different genres.
Whilewe did not solicit feedback from annotators in thisexperiment, we believe that it will be importantto do so in future work to improve the annota-tion task.
We know that the perceived lag time inspeech-to-speech translation cannot be completelyeliminated but we are interested in systems that are?fast?
enough for humans to have quality conver-sations in different languages.AcknowledgmentsThis work was partly supported by SingaporeAgency for Science, Technology and Research(A-STAR) and the Singapore International Pre-Graduate Award (SIPGA) and was partly sup-ported by the National Science Foundation (NSF)award IIS-1225629.
Any opinions expressed inthis material are those of the authors and do notnecessarily reflect the views of A-STAR and NSF.8ReferencesChang Baobao, Pernilla Danielsson, and WolfgangTeubert.
2002.
Extraction of translation units fromChinese-English parallel corpora.
In Proceedingsof the first SIGHAN workshop on Chinese languageprocessing - Volume 18 (SIGHAN ?02), 1?5.Presentacio?n Padilla Ben?
?tez and Teresa Bajo.
1998.Hacia un modelo de memoria y atencio?n en inter-pretacio?n simulta?nea.
Quaderns.
Revista de tra-duccio?, 2:107?117.Chris Fournier and Diana Inkpen.
2012.
Segmenta-tion and similarity agreement.
In Proceedings ofthe 2012 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies (NAACL HLT ?12),Montreal, Canada, 152?161.Osamu Furuse and Hitashi Iida.
1996.
Incrementaltranslation utilizing constituent boundary patterns.In Proceedings of the 16th conference on Computa-tional linguistics (COLING ?96), Copenhagen, Den-mark, 412?417.Olivier Hamon, Christian Fgen, Djamel Mostefa, Vic-toria Arranz1, Munstin Kolss, Alex Waibel, andKhalid Choukri.
2009.
End-to-End Evaluation inSimultaneous Translation.
In Proceedings of the12th Conference of the European Chapter of theAssociation for Computational Linguistics, (EACL?09), Athens, Greece, 345?353.Daniel Jurafsky.
1988.
Issues in relating syntax andsemantics.
In Proceedings of the 12th Internationalconference on Computational Linguistics (COLING?88), Budapest, Hungary, 278?284.Frank Keller.
2010.
Cognitively plausible models ofhuman language processing.
In Proceedings of theACL 2010 Conference Short Papers, Uppsala, Swe-den, 60?67.Walter Kintsch.
2005.
An Overview of Top-down andBottom?up Effects in Comprehension: The CI Per-spective.
Discourse Processes, 39(2&3):125?128.Walter Kintsch and Praful Mangalath.
2011.
The Con-struction of Meaning.
Topics in Cognitive Science,3:346?370.Dan Klein and Christopher D. Manning 2003.
Ac-curate Unlexicalized Parsing.
In Proceedings of the41st Meeting of the Association for ComputationalLinguistics, 423?430.Ja?chym Kola?r?.
2008.
Automatic Segmentation ofSpeech into Sentence-like Units.
Ph.D. thesis, Uni-versity of West Bohemia, Pilsen, Czech Republic.Patrik Lambert, Adria`.
De Gispert, Rafael Banchs, andJose?
B. Marin?o.
2005.
Guidelines for Word Align-ment Evaluation and Manual Alignment.
LanguageResources and Evaluation (LREC), 39:267?285.Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai Wu.2012.
Fully automatic semantic MT evaluation.
InProceedings of the Seventh Workshop on StatisticalMachine Translation (WMT ?12), Montreal, Canada243?252.George A. Miller.
1956.
The Magical Number Seven,Plus or Minus Two: Some Limits on Our Capacity ofProcessing Information.
The Psychological Review,Vol 63:81?97.Hideki Mima, Hitoshi Iida, and Osamu Furuse.
1998.Simultaneous interpretation utilizing example-basedincremental transfer.
In Proceedings of the 17th In-ternational Conference on Computational Linguis-tics (COLING ?98) Montreal, Quebec, Canada, 855?861.Pierre Ole?ron and Hubert Nanpon.
1965.
Recherchessur la traduction simultane?e.
Journal de PsychologieNormale et Pathologique, 62(1):73?94.Mathais Paulik and Alex Waibel.
2009.
AutomaticTranslation from Parallel Speech: Simultaneous In-terpretation as MT Training Data.
IEEE Workshopon Automatic Speech Recognition and Understand-ing, Merano, Italy, 496?501.Lev Pevzner and Marti A. Hearst 2002.
A critique andimprovement of an evaluation metric for text seg-mentation.
Computational Linguistics, 28(1):1936.MIT Press, Cambridge, MA, USA.Sameer Pradhan, Wayne Ward, Kadri Hacioglu, JamesH.
Mar- tin, and Dan Jurafsky.
2004.
Shallow Se-mantic Parsing Using Support Vector Machines.
InProceedings of the 2004 Conference on Human Lan-guage Technology and the North American Chap-ter of the Association for Computational Linguistics(HLT-NAACL-04).Baskaran Sankaran, Ajeet Grewal, and Anoop Sarkar.2010.
Incremental Decoding for Phrase-based Sta-tistical Machine Translation.
In Proceedings of theJoint 5th Workshop on Statistical Machine Transla-tion and Metrics (MATR), Uppsala, Sweden, 222?229.Teresa M. Signorelli, Henk J. Haarmann, and LoraineK.
Obler.
2011.
Working memory in simultaneousinterpreters: Effects of task and age.
InternationalJournal of Billingualism, 16(2): 192?212.Hua-Ping Zhang, Hong-Kui Yu, De-Yi Xiong, QunLiu.
2003.
HHMM-based Chinese Lexical An-alyzer ICTCLAS.
In Proceedings of the SecondSIGHAN Workshop on Chinese Language Process-ing (SIGHAN ?03) - Volume 17, Sapporo, Japan,184-187.9
