GRADED UNIF ICAT ION:  A FRAMEWORK FORINTERACTIVE  PROCESSINGAlber t  K im *Depar tment  of  Computer  and  In fo rmat ion  SciencesUn ivers i ty  of  Pennsy lvan iaPh i lade lph ia ,  Pennsy lvan ia ,  USAemail :  a lk im?unagi ,  cis.
upenn, eduAbst rac tAn extension to classical unification, called graded unifica-tion is presented.
It is capable of combining contradictoryinformation.
An interactive processing paradigm and parserbased on this new operator are also presented.In t roduct ionImproved understanding of the nature of knowledgeused in human language processing suggests the fea-sibility of interactive models in computational linguis-tics (CL).
Recent psycholinguistic work such as (Stowe,1989; Trueswell et al, 1994) has documented rapid em-ployment of semantic information to guide human syn-tactic processing.
In addition, corpus-based stochas-tic modelling of lexical patterns (see Weischedel et al,1993) may provide information about word sense fre-quency of the kind advocated since (Ford et al, 1982).Incremental employment of such knowledge to resolvesyntactic ambiguity is a natural step towards improvedcognitive accuracy and efficiency in CL models.This exercise will, however, pose difficulties for theclassical ('hard') constraint-based paradigm.
As illus-trated by the Trueswell et al (1994) results, this viewof constraints is too rigid to handle the kinds of effectsat hand.
These experiments used pairs of locally am-biguous reduced relative clauses such as:1) the man recognized by the spy took off down the street2) the van recognized by the spy took off down the streetThe verb recognized is ambiguously either a past par-ticipial form or a past tense form.
Eye tracking showedthat subjects resolved the ambiguity rapidly (beforereading the by-phrase) in 2) but not in 1) 1.
The con-clusion they draw is that subjects use knowledge aboutthematic roles to guide syntactic decisions.
Since van,which is inanimate, makes a good Theme but a poorAgent for recognized, the past participial analysis in2) is reinforced and the main clause (past tense) sup-pressed.
Being animate, man performs either thematicrole well, allowing the main clause reading to remain*I thank Christy Doran, Jason Eisner, Jeff Reynar, andJohn Trueswell for valuable comments.
I am grateful toEwan Klein and the Centre for Cognitive Science, Edin-burgh, where most of this work was conducted, and also ac-knowledge the support of DARPA grant N00014-90-J-1863.1In fact, ambiguity effects were often completely elimi-nated in examples like 2), with reading times matching thosefor the unambiguous case:3) the man/van that was recognized by the spy ...plausible until the disambiguating by-phrase is encoun-tered.
At this point, readers of 1) displayed confusion.Semantic onstraints do appear to be at work here.However, the effects observed by Trueswell et al aregraded.
Verb-complement combinations occupy a con-tinuous spectrum of "thematic fit", which influencesreading times.
This likely stems from the variance ofverbs with respect o the thematic roles they allow (e.g.,Agent, Instrument, Patient, etc.)
and the syntactic po-sitions of these.The upshot of such observations i  that classical uni-fication (see Shieber, 1986), which has served well as thecombinatory mechanism in classical constraint-basedparsers, is too brittle to withstand this onslaught ofuncertainty.This paper presents an extension to classical unifi-cation, called graded unification.
Graded unificationcombines two feature structures, and returns a strengthwhich reflects the compatibility of the information en-coded by the two structures.
Thus, two structureswhich could not unify via classical unification may unifyvia graded unification, and all combinatory decisionsmade during processing are endowed with a level ofgoodness.
The operator is similar in spirit to the op-erators of fuzzy logic (see Kapcprzyk, 1992), which at-tempts to provide a calculus for reasoning in uncertaindomains.
Another related approach is the "UnificationSpace" model of Kempen & Vosse (1989), which unifiesthrough a process of simulated annealing, and also usesa notion of unification strength.A parser has been implemented which combines con-stituents via graded unification and whose decisions areinfluenced by unification strengths.
The result is aparadigm of incremental processing, which maintainsa feature-based system of knowledge representation.System Descr ipt ionThough the employment of graded unification engen-ders a new processing style, the system's architectureparallels that of a conventional unification-based parser.Feature Structures: Prioritized FeaturesThe feature structures which encode the grammar inthis system are conventional feature structures aug-mented by the association of priorities with eachatomic-valued feature.
Prioritizing features allowsthem to vary in terms of influence over the strength ofunification.
The priority of an atomic-valued feature fiin a feature structure X will be denoted by Pr i ( f i ,  X) .The effect of feature prioritization is clarified in the fol-lowing sections.313Graded UnificationGiven two feature structures, the graded unificationmechanism (Ua) computes two results, a unifying struc-ture and a unification strength.S t ructura l  Uni f icat ion Graded unification buildsstructure xactly as classical unification except in thecase of atomic unification, where it deviates crucially.Atoms in this framework are weighted isjunctive val-ues.
The weight associated with a disjunct is viewed asthe confidence with which the processor believes thatdisjunct o be the 'correct' value.
Figures l(a) and l(b)depict atoms (where l(a) is "truly atomic" because itcontains only one disjunct).
(a) (b) (?
)Figure h Examples of AtomsAtomic unification creates a mixture of its two ar-gument atoms as follows.
When two atoms are unified,the set union of their disjuncts is collected in the result.For each disjunct in the result, the associated weight be-comes the average of the weights associated with thatdisjunct in the two argument atoms.
Figure l(c) showsan example unification of two atoms.
The result is anatom which is 'believed' to be SG (singular), but couldpossibly be PL (plural).Unification Strength The unification strength (de-noted t3aStrength)  is a weighted average of atomic uni-fication strengths, defined in terms of two sums, theactual compatibil ity and the perfect compatibility.If A and B are non-atomic feature structures to beunified, then the following holds:I l aS t rength(A ,  B )  = ActualCornpatibility(A,B)Per \] ectC ornpatibility( A,B ) "The actual  compat ib i l i ty  is the sum:Pri(f i ,A)+Pri( l i ,B) , UGStrength(v ia ,V iB)~.
if fi shared by A and B?
Pv i ( f i ,  A)  if f i  occurs only in APr i ( f i ,  B )  if fi occurs only in Bwhere i indexes all atomic-valued features in A or B,and v;a and ViB are the values of f i  in A and B respec-tively.
The perfect  compat ib i l i ty  is computed by aformula identical to this except hat UaSt rength  is setto 1.If A and B are atomic, then I IGStreng lh(A,  B)  isthe total weight of disjuncts shared by A and B:t J cS t rength(A ,B)  = ~-~i M in (w iA ,  WiB) where i in-dexes all disjuncts di shared by A and B,  and wia andwiB are the weights of di in A and B respectively.By taking atomic unification strengths into account,the actual compatibility provides a raw measure of theextent to which two feature structures agree.
By ig-noring unification strengths (assuming a value of 1.o),the perfect compatibility is an idealization of the actualcompatibility; it is what the actual compatibility wouldbe if the two structures were able to unify via classicalunification.
Thus, unification strength is always a valuebetween 0 and 1.The Parser: Activated Chart EdgesThe parser is a modified unification-based chart parser.Chart edges are assigned activation levels, which repre-sent the 'goodness' of (or confidence in) their associatedanalyses.
Each new edge is activated according to thestrength of the unification which licenses its creationand the activations of its constituent edges.Constraining Graded Unification Without somestrict limit on its operation, graded unification will over-generate wildly.
Two mechanisms exist to constraingraded unification.
First, if a particular unificationcompletes with strength below a specified unificationthreshold, it fails.
Second, if a new edge is constructedwith activation below a specified activation threshold,it is not allowed to enter the chart, and is suspended.Parsing Strategy  The chart is initialized to containone inactive edge for each lexical entry of each wordin the input.
Lexical edges are currently assigned aninitial activation of 1.o.The chart can then be expanded in two ways:1.
An active edge may be extended by unifying its firstunseen constituent with the LrlS of an inactive edge.2.
A new active edge may be created by unifying theLHS of a rule with the first unseen constituent of someactive edge in the chart (top down rule invocation).E~EI IA  ~ s o/c~>~,r~e.2 I I G ~  \[ c" - -  o ,oFigure 2: Extension of an Active Edge by an Inactive EdgeFigure 2 depicts the extension of the active EDGE1 withthe inactive EDGE2.
The characters represent featurestructures, and the ovular nodes on the right end ofeach edge represent activation level.
The parser triesto unify C', the mother node of EDGE2, with C, thefirst needed constituent of EDGE1.
If this unificationsucceeds, the parser builds the extended edge, EDGE3(where C Ua C' produces C").
The activation of thenew edge is a function of the strength of the unificationand the current activations of EDGE1 and EDGE2:activ3 = wl ?
t J cSTRENGTH(C,  C')+ w~ ?
activl9- w 3 .
activ2 (The weights wi sum to 1.
)EDGE3 enters the chart only if its activation exceedsthe activation threshold.
Rule invocation is depicted infigure 3.
The first needed constituent in EDGE1 is uni-fied with the LHS of aULE1.
EDGE2 is created to beginsearching for C. The new edge's activation is again afunction of unification strength and other activations:activ 3 --- w l  ?
UGSTRENGTH(C,  C')9- w2 ?
activl+ w 3 .
activ2314E~E~ I A - -  B o / C ~RULEI \[_IGOr-------------'/ \[ C ' - -  D E ~EDGE2 ~ ' J ~ "  ~ o D EFigure 3: Top Down Rule InvocationThe activation levels of grammar rule edges, like thosefor lexical edges, are currently pegged to 1.o.A Framework for Interact ive ProcessingThe system described above provides a flexible frame-work for the interactive use of non-syntactic knowledge.An imacy  and  Themat ic  Ro lesKnowledge about animacy and its important functionin the filling of thematic roles can be modelled as abinary feature, ANIMATE.
A (active voice) verb canstrongly 'want' an animate Agent by specifying that itssubject be \[ANIMATE Jr\] and assigning ahigh priority tothe feature ANIMATE.
Thus, any parse combining thisverb with an inanimate subject will suffer in terms ofunification strength.
A noun can be strongly animateby having a high weight associated with the positivevalue of ANIMATE.
Animacy has been encoded in a toygrammar.
However, principled settings for the priorityof this feature are left to future work.S ta t i s t i ca l  In fo rmat ion  f rom CorporaCorpus-based part-of-speech (POS) statistics can alsobe naturally incorporated into the current model.
Itis proposed here that a Viterbi decoder could be usedto generate the likelihoods of the n best POS tagsfor a given word in the input string.
Lexical chartedges would then be initially activated to levels pro-portional to the predicted likelihoods of their associ-ated tags.
Since these activations will be propagatedto larger edges, parses involving predicted word senseswould consequently be given a head start in a race of ac-tivations.
Attractively, this strategy allows a fuller useof statistical information than one which uses the in-formation simply to deterministically choose the n besttags, which are then treated as equally likely.In teract ion  o f  D iverse  In fo rmat ionA crucial feature of this framework is its potential formodelling the interaction between sources of informa-tion like the two above when they disagree.
Sentences1} and 2) again provide illustration.
In such sentences,knowledge about word sense frequency supports thewrong analysis, and semantic onstraints must be em-ployed to achieve the correct (human) performance.Intuitively, the raw frequency (without consideringcontext) of the past tense form of recognized is higherthan that of the past participial.
POS taggers, despiteconsidering local context, consistently mis-tag the verbin reduced relatives.
The absence of a disambiguatingrelativizer (e.g., that) is one obvious ource of difficultyhere.
But even the ostensibly disambiguating prepo-sition by, is itself ambiguous, since it might introducea manner or locative phrase consistent with the mainclause analysis.
2Modelling human performance in such contextsrequires allowing thematic information to competeagainst and defeat word frequency information.
Thecurrent model allows such competition, as follows.
POSinformation may incorrectly predict the main clauseanalysis, boosting the lexical edge associated with thepast tense, and thereby boosting the main clause parse.However, the unification combining the past tense formof recognized with an inanimate subject (van) will beweak, due to the constraints encoded in the verb's lexi-cal entry.
Since the activations of constituent edges de-pend on the strengths of the unifications used to buildthem, the main clause parse Will lose activation.
Theparse combining the past participial with an inanimatesubject (Theme) will suffer no losses, allowing it to over-take the incorrect parse.Conclusions and Future WorkAssigning feature priorities and activation thresholdsin this model will certainly be a considerable task.
Itis hoped that principled and automated methods canbe found for assigning values to these variables.
Onepromising idea is to glean information about patternsof subcategorization a d thematic roles from annotatedcorpora.
Annotation of such information has been sug-gested as a future direction for the Treebank project(Marcus el al., 1993).
It should be noted that learningsuch information will require more training data (hencelarger corpora) than learning to tag part of speech.In addition, psycholinguistic studies such as the largenorming study 3 of MacDonald and Pearlmutter (de-scribed in Trueswell et al, 1994) may prove useful inencoding thematic information in small lexicons.ReferencesFord~ M., J. Bresnan, &: B.. Kaplan (1982).
A Competence Based Theoryof Syntact ic Closure.
In Bresnan, J.
(Ed.
), The Mental Representationof Grammatica l  l:telations (pp.
727-796).
MIT Press, Cambridge, MA.Kempen, O. and T. Vosse (1989).
Incremental  Syntact ic  Tree Formation inHuman Sentence Processing: a Cognit ive Architecture Based on Activa-tion Decay and Simulated Annealing.
Connection Science, 1(3), 273-290.Kapcprzyk, J.
(1992).
Fuzzy Sets and Fuzzy Logic.
In Shapiro, S.
(gd.)
TheEncyclopedia of Artificial Intelligence.
John Wiley 8z Sons., New York.Marcus, M., B. Santorini, and M Markiewicz (1993).
Building a Large An-notated Corpus of English: The Penn Treebank.
Computational Lin-guistics, 19(2), 1993.Shieber, S. (1986).
An Introduction to Unification-Based Approaches toGrammar.
CSLI Lecture Notes, Chicago University Press, Chicago.Stowe, L. (1989).
Thematic Structures and Sentence Comprehension.
InCarlsonp G. and M. Tanenhaus (Eds.)
Linguistic Structure in LanguageProcessing Kluwer Academic Publishers.Trueswell, J., M. T~nnenh&us, S. Garnsey (1994).
Semantic Influences onParsing: Use of Thematic Role Information in Syntactic Ambiguity B.es-olutlon.
Journal of Memory and Language, 33, In Press.Weischedel, R., B.. Schwartz, J. Palmucci, M. Meteer, and L. P~amshaw(1993).
Coping with Ambiguity and Unknown Words through Proba-bilistic Models.
Computational Linguistics, 19(2), 359-382.=In fact, the utility of byis neutralized in the case of POStagging, since prepositions are uniformly tagged (e.g., usingthe tag IN in the Penn Treebank; see Marcus et al, 1993).3These studies attempt o establish thematic patternsby asking large numbers of subjects to answer questions like"How typical is it for a van to be recognized by someone?
"with a rating between 1 and 7.315
