Proceedings of the Workshop on Automatic Summarization for Different Genres, Media, and Languages, pages 49?54,Portland, Oregon, June 23, 2011. c?2011 Association for Computational LinguisticsExtractive Multi-Document Summaries Should Explicitly Not ContainDocument-Specific ContentRebecca Mason and Eugene CharniakBrown Laboratory for Linguistic Information Processing (BLLIP)Brown UniversityProvidence, RI 02912{rebecca,ec}@cs.brown.eduAbstractUnsupervised approaches to multi-documentsummarization consist of two steps: find-ing a content model of the documents to besummarized, and then generating a summarythat best represents the most salient informa-tion of the documents.
In this paper, wepresent a sentence selection objective for ex-tractive summarization in which sentences arepenalized for containing content that is spe-cific to the documents they were extractedfrom.
We modify an existing system, HIER-SUM (Haghighi & Vanderwende, 2009), to useour objective, which significantly outperformsthe original HIERSUM in pairwise user eval-uation.
Additionally, our ROUGE scores ad-vance the current state-of-the-art for both su-pervised and unsupervised systems with sta-tistical significance.1 IntroductionMulti-document summarization is the task of gener-ating a single summary from a set of documents thatare related to a single topic.
Summaries should con-tain information that is relevant to the main ideas ofthe entire document set, and should not contain in-formation that is too specific to any one document.For example, a summary of multiple news articlesabout the Star Wars movies could contain the words?Lucas ?and ?Jedi?, but should not contain the nameof a fan who was interviewed in one article.
Mostapproaches to this problem generate summaries ex-tractively, selecting whole or partial sentences fromthe original text, then attempting to piece them to-gether in a coherent manner.
Extracted text is se-lected based on its relevance to the main ideas of thedocument set.
Summaries can be evaluated manu-ally, or with automatic metrics such as ROUGE (Lin,2004).The use of structured probabilistic topic modelshas made it possible to represent document set con-tent with increasing complexity (Daume?
& Marcu,2006; Tang et al, 2009; Celikyilmaz & Hakkani-Tur, 2010).
Haghighi and Vanderwende (2009)demonstrated that these models can improve thequality of generic multi-document summaries oversimpler surface models.
Their most complex hier-archial model improves summary content by teasingout the words that are not general enough to repre-sent the document set as a whole.
Once those wordsare no longer included in the content word distri-bution, they are implicitly less likely to appear inthe extracted summary as well.
But this objectivedoes not sufficiently keep document-specific contentfrom appearing in multi-document summaries.In this paper, we present a selection objective thatexplicitly excludes document-specific content.
Were-implement the HIERSUM system from Haghighiand Vanderwende (2009), and show that using ourobjective dramatically improves the content of ex-tracted summaries.2 Modeling ContentThe easiest way to model document content is to finda probability distribution of all unigrams that appearin the original documents.
The highest frequencywords (after removing stop words) have a high like-lihood of appearing in human-authored summaries(Nenkova & Vanderwende, 2005).
However, the raw49Figure 1: The graphical model for HIERSUM (Haghighi& Vanderwende, 2009).unigram distribution may contain words that appearfrequently in one document, but do not reflect thecontent of the document set as a whole.Probabilistic topic models provide a more prin-cipled approach to finding a distribution of contentwords.
This idea was first presented by Daume?and Marcu (2006) for their BAYESUM system forquery-focused summarization, and later adapted fornon-query summarization in the TOPICSUM systemby Haghighi and Vanderwende (2009).
1 In thesesystems, each word from the original documents isdrawn from one of three vocabulary distributions.The first, ?b, is the background distribution of gen-eral English words.
The second, ?d, contains vo-cabulary that is specific to that one document.
Andthe third, ?c, is the distribution of content words forthat document set, and contains relevant words thatshould appear in the generated summary.HIERSUM (Haghighi & Vanderwende, 2009)adds more structure to TOPICSUM by further split-ting the content distribution into multiple sub-topics.The content words in each sentence can be gener-ated by either the general content topic or the con-tent sub-topic for that sentence, and the words fromthe general content distribution are considered whenbuilding the summary.1The original BAYESUM can also be used without a query,in which case, BAYESUM and TOPICSUM are the exact samemodel.3 KL SelectionThe KL-divergence between two unigram word dis-tributions P and Q is given by KL(P ||Q) =?w P (w) logP (w)Q(w) .
This quantity is used for sum-mary sentence selection in several systems includ-ing Lerman and McDonald (2009) and Haghighiand Vanderwende (2009), and was used as a featurein the discrimitive sentence ranking of Daume?
andMarcu (2006).TOPICSUM and HIERSUM use the following KLobjective, which finds S?, the summary that min-imizes the KL-divergence between the estimatedcontent distribution ?c and the summary word dis-tribution PS:S?
= minS:|S|?LKL(?c||PS)A greedy approximation is used to find S?.
Start-ing with an empty summary, sentences are greedilyadded to the summary one at a time until the sum-mary has reached the maximum word limit, L. Thevalues of PS are smoothed uniformly in order to en-sure finite values of KL(?c||PS).4 Why Document-Specific Words are aProblemThe KL selection objective effectively ensures thepresence of highly weighted content words in thegenerated summary.
But it is asymmetric in that itallows a high proportion of words in the summaryto be words that appear infrequently, or not at all,in the content word distribution.
This asymmetryis the reason why the KL selection metric does notsufficiently keep document-specific words out of thegenerated summary.Consider what happens when a document-specificword is included in summary S. Assume that theword wi does not appear (has zero probability) inthe content word distribution ?c, but does appear inthe document-specific distribution ?d for documentd.
Then wi appearing in S has very little impacton KL(?c||PS) =?j ?c(wj) log?c(wj)PS(wj)because?c(wi) = 0.
There will be a slight impact becausethe presence of the wordwi in S will cause the prob-ability of other words in the summary to be sligntlysmaller.
But in a summary of length 250 words (the50length used for the DUC summarization task) thedifference is negligible.The reason why we do not simply substitutea symmetrical metric for comparing distributions(e.g., Information Radius) is because we want the se-lection objective to disprefer only document-specificwords.
Specifically, the selection objective shouldnot disprefer background English vocabulary.5 KL(c)-KL(d) SelectionIn contrast to the KL selection objective, our ob-jective measures the similarity of both content anddocument-specific word distributions to the ex-tracted summary sentences.
We combine these mea-sures linearly:S?
= minS:|S|?LKL(?c||PS)?KL(?d||PS)Our objective can be understood in comparisonto the MMR criterion by (Carbonell & Goldstein,1998), which also utilizes a linear metric in order tomaximize informativeness of summaries while min-imizing some unwanted quality of the extracted sen-tences (in their case, redundancy).
In contrast, ourcriterion utilizes information about what kind of in-formation should not be included in the summary,which to our knowledge has not been done in previ-ous summarization systems.2For comparison to the previous KL objective, wealso use a greedy approximation for S?.
However,because we are extracting sentences from many doc-uments, the distribution ?d is actually several distri-butions, a separate distribution for each documentin the document set.
The implementation we usedin our experiments is that, as we consider a sen-tence s to be added to the previously selected sen-tences S, we set ?d to be the document-specificdistribution of the document that s has been ex-tracted from.
So each time we add a sentence tothe summary, we find the sentence that minimizesKL(?c||PS?s)?KL(?d(s)||PS?s).
Another imple-mentation we tried was combining all of the ?d dis-tributions into one distribution, but we did not noticeany difference in the extracted summaries.2A few anonymous reviewers asked if we tried to optimizethe value of ?
for KL(?c||PS) ?
?KL(?d||PS).
The answeris yes, but optimizing ?
to maximize ROUGE results in sum-maries that are perceptibly worse, and manually tuning ?
didnot seem to produce any benefit.6 Evaluation6.1 DataWe developed our sentence selection objective us-ing data from the Document Understanding Con-ference3 (DUC) 2006 summarization task, and useddata from DUC 2007 task for evaluations.
In thesetasks, the system is given a set of 25 news arti-cles related to an event or topic, and needs to gen-erate a summary of under 250 words from thosedocuments.4 For each document set, four human-authored summaries are provided for use with eval-uations.
The DUC 2006 data has 50 document sets,and the DUC 2007 data has 45 document sets.6.2 Automatic EvaluationSystems are automatically evalatued using ROUGE(Lin, 2004), which has good correlation with hu-man judgments of summary content.
ROUGE com-pares n-gram recall between system-generated sum-maries, and human-authored reference summaries.The first two metrics we compare are unigram andbigram recall, R-1 and R-2, respectively.
The lastmetric, R-SU4, measures recall of skip-4 bigrams,which may skip one or two words in between thetwo words to be measured.
We set ROUGE to stemboth the system and reference summaries, scale ourresults by 102 and present scores with and withoutstopwords removed.The ROUGE scores of the original HIERSUM sys-tem are given in the first row of table 1, followedby the scores of HIERSUM using our KL(c-d) se-lection.
The KL(c-d) selection outperforms the KLselection in each of the ROUGE metrics shown.
Infact, these results are statistically significant overthe baseline KL selection for all but the unigrammetrics (R-1 with and without stopwords).
Theseresults show that our KL(c-d) selection yields sig-nificant improvements in terms of ROUGE perfor-mance, since having fewer irrelevant words in thesummaries leaves room for words that are more rel-evant to the content topic, and therefore more likelyto appear in the reference summaries.The last two rows of table 1 show the scoresof two recent state-of-the-art multi-document sum-3http://duc.nist.gov/4Some DUC summarization tasks also provide a query orfocus for the summary, but we ignore these in this work.51System ROUGE w/o stopwords ROUGE w/ stopwordsR-1 R-2 R-SU4 R-1 R-2 R-SU4HIERSUM w/ KL 34.6 7.3 10.4 43.1 9.7 15.3HIERSUM w/ KL(c)-KL(d) 35.6 9.9 12.8 43.2 11.6 16.6PYTHY 35.7 8.9 12.1 42.6 11.9 16.8HYBHSUM 35.1 8.3 11.8 45.6 11.4 17.2Table 1: ROUGE scores on the DUC 2007 document sets.
The first two rows compare the results of the unigramHIERSUM system with its original and our improved selection metrics.
Bolded scores represent where our system hasa significant improvement over the orignal HIERSUM.
For further comparison, the last two rows show the ROUGEscores of two other state-of-the-art multi-document summarization systems (Toutanova et al, 2007; Celikyilmaz &Hakkani-Tur, 2010).
See section 6.2 for more details.marization systems.
Both of these systems se-lect sentences discriminatively on many featuresin order to maximize ROUGE scores.
The first,PYTHY (Toutanova et al, 2007), trains on dozensof sentence-level features, such as n-gram and skip-gram frequency, named entities, sentence length andposition, and also utilizes sentence compression.The second, HYBHSUM (Celikyilmaz & Hakkani-Tur, 2010), uses a nested Chinese restaurant process(Blei et al, 2004) to model a hierarchical contentdistribution with more complexity than HIERSUM,and uses a regression model to predict scores for newsentences.For both of these systems, our summaries are sig-nificantly better for R-2 and R-SU4 without stop-words, and comparable in all other metrics.5 Theseresults show that our selection objective can makea simple unsupervised model competitive with morecomplicated supervised models.6.3 Manual EvaluationFor manual evaluation, we performed a pairwisecomparison of summaries generated by HIERSUMwith both the original and our modified sentence se-lection objective.
Users were given the two sum-maries to compare, plus a human-generated refer-ence summary.
The order that the summaries ap-peared in was random.
We asked users to selectwhich summary was better for the following ques-5Haghighi and Vanderwende (2009) presented a version ofHIERSUM that models documents as a bag of bigrams, and pro-vides results comparable to PYTHY.
However, the bigram HI-ERSUM model does not find consistent bags of bigrams.System Q1 Q2 Q3 Q4HIERSUM w/ KL 29 36 31 36. .
.
w/ KL(c)-KL(d) 58 51 56 51Table 2: Results of manual evaluation.
Our criterion out-performs the original HIERSUM for all attributes, and issignificantly better for Q1 and Q3.
See section 6.3 fordetails.tions:6Q1 Which was better in terms of overall content?Q2 Which summary had less repetition?Q3 Which summary was more coherent?Q4 Which summary had better focus?We took 87 pairwise preferences from participantsover Mechanical Turk.7 The results of our evalu-ation are shown in table 2.
For all attributes, ourcriterion performs better than the original HIERSUMselection criterion, and our results for Q1 and Q3 aresignificantly better as determined by Fisher sign test(two-tailed P value < 0.01).These results confirm that our objective noticablyimproves the content of extractive summaries by se-lecting sentences that contain less document-specific6These are based on the manual evaluation questions fromDUC 2007, and are the same questions asked in Haghighi andVanderwende (2009).7In order to ensure quality results, we asked participants towrite a sentence on why they selected their preference for eachquestion.
We also monitored the time taken to complete eachcomparison.
Overall, we rejected about 25% of responses wereceived, which is similar to the percentage of responses re-jected by Gillick and Liu (2010).52information.
This leaves more room in the summaryfor content that is relevant to the main idea of thedocument set (Q1) and keeps out content that is notrelevant (Q4).
Additionally, although neither crite-rion explicitly addresses coherence, we found that asignificant proportion of users found our summariesto be more coherent (Q3).
We believe this may bethe case because the presence of document-specificinformation can distract from the main ideas of thesummary, and make it less likely that the extractedsentences will flow together.There is no immediate explanation for why usersfound our our summaries less repetitive (Q2), sinceif anything the narrowing of topics due to the neg-ative KL(?d||PS) term should make for more rep-etition.
We currently hypothesize that the improvedscore is simply a spillover from the general improve-ment in document quality.7 ConclusionWe have described a new objective for sentence se-lection in extractive multi-document summarization,which is different in that it explicitly gives negativeweight to sentences that contain document-specificwords.
Our objective significantly improves the per-formance of an existing summarization system, andimproves on current best ROUGE scores with sig-nificance.We have observed that while the content in ourextracted summaries is often comparable to the con-tent in human-written summaries, the extracted sum-maries are still far weaker in terms of coherence andrepetition.
Even though our objective significantlyimproves coherence, more sophisticated methods ofdecoding are still needed to produce readable sum-maries.
These problems could be addressed throughfurther refinement of the selection objective, throughsimplification or compression of selected sentences,and through improving the coherence of generatedsummaries.ReferencesBlei, D. M., Griffiths, T. L., Jordan, M. I., & Tenen-baum, J.
B.
(2004).
Hierarchical topic models andthe nested chinese restaurant process.
Advancesin Neural Information Processing Systems.Carbonell, J., & Goldstein, J.
(1998).
The useof mmr, diversity-based reranking for reorderingdocuments and producing summaries.
Proceed-ings of the 21st annual international ACM SIGIRconference on Research and development in infor-mation retrieval (pp.
335?336).
New York, NY,USA: ACM.Celikyilmaz, A., & Hakkani-Tur, D. (2010).
A hy-brid hierarchical model for multi-document sum-marization.
Proceedings of the 48th Annual Meet-ing of the Association for Computational Linguis-tics (pp.
815?824).
Stroudsburg, PA, USA: Asso-ciation for Computational Linguistics.Daume?, III, H., & Marcu, D. (2006).
Bayesianquery-focused summarization.
ACL-44: Proceed-ings of the 21st International Conference on Com-putational Linguistics and the 44th annual meet-ing of the Association for Computational Linguis-tics (pp.
305?312).
Morristown, NJ, USA: Asso-ciation for Computational Linguistics.Gillick, D., & Liu, Y.
(2010).
Non-expert evaluationof summarization systems is risky.
Proceedingsof the NAACL HLT 2010 Workshop on CreatingSpeech and Language Data with Amazon?s Me-chanical Turk (pp.
148?151).
Stroudsburg, PA,USA: Association for Computational Linguistics.Haghighi, A., & Vanderwende, L. (2009).
Exploringcontent models for multi-document summariza-tion.
Proceedings of Human Language Technolo-gies: The 2009 Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (pp.
362?370).
Boulder, Col-orado: Association for Computational Linguis-tics.Lerman, K., & McDonald, R. (2009).
Contrastivesummarization: an experiment with consumer re-views.
Proceedings of Human Language Tech-nologies: The 2009 Annual Conference of theNorth American Chapter of the Association forComputational Linguistics, Companion Volume:Short Papers (pp.
113?116).
Stroudsburg, PA,USA: Association for Computational Linguistics.Lin, C.-Y.
(2004).
Rouge: a package for auto-matic evaluation of summaries.
Proceedings of53the Workshop on Text Summarization BranchesOut (WAS 2004).
Barcelona, Spain.Nenkova, A., & Vanderwende, L. (2005).
The im-pact of frequency on summarization (TechnicalReport).
Microsoft Research.Tang, J., Yao, L., & Chen, D. (2009).
Multi-topicbased query-oriented summarization.
SDM?09(pp.
1147?1158).Toutanova, K., Brockett, C., Gamon, M., Jagarla-mudi, J., Suzuki, H., & Vanderwende, L. (2007).The PYTHY Summarization System: MicrosoftResearch at DUC 2007.
Proc.
of DUC.54
