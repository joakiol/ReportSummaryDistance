Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 57?60,Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLPTransliteration of Name Entity via Improved Statistical Translation onCharacter SequencesYan Song Chunyu Kit Xiao ChenDepartment of Chinese, Translation and LinguisticsCity University of Hong Kong83 Tat Chee Ave., Kowloon, Hong KongEmail: {yansong, ctckit}@cityu.edu.hk, cxiao2@student.cityu.edu.hkAbstractTransliteration of given parallel name en-tities can be formulated as a phrase-basedstatistical machine translation (SMT) pro-cess, via its routine procedure compris-ing training, optimization and decoding.In this paper, we present our approach totransliterating name entities using the log-linear phrase-based SMT on character se-quences.
Our proposed work improves thetranslation by using bidirectional models,plus some heuristic guidance integrated inthe decoding process.
Our evaluated re-sults indicate that this approach performswell in all standard runs in the NEWS2009Machine Transliteration Shared Task.1 IntroductionTo transliterate a foreign name into a target lan-guage, a direct instrument is to make use of ex-isting rules for converting text to syllabus, orat least a phoneme base to support such trans-formation.
Following this path, the well devel-oped noisy channel model used for transliterationusually set an intermediate layer to represent thesource and target names by phonemes or phonetictags (Knight and Graehl, 1998; Virga and Khu-danpur, 2003; Gao et al, 2004).
Having beenstudied extensively though, the phonemes-basedapproaches cannot break its performance ceilingfor two reasons (Li et al, 2004): (1) Language-dependent phoneme representation is not easy toobtain; (2) The phonemic representation to sourceand target names usually causes error spread.Several approaches have been proposed for di-rect use of parallel texts for performance enhance-ment (Li et al, 2004; Li et al, 2007; Gold-wasser and Roth, 2008).
There is no straight-forward mean for grouping characters or letters inthe source or target language into better transliter-ation units for a better correspondence.
There isno consistent deterministic mapping between twolanguages either, especially when they belong todifferent language families, such as English andChinese.
Usually, a single character in a sourcename is not enough to form a phonetic patternin a target name.
Thus a better way to modeltransliteration is to map character sequences be-tween source and target name entities.
The map-ping is actually an alignment process.
If a cer-tain quantity of bilingual transliterated entities areavailable for training, it is a straight-forward ideato tackle this transliteration problem with a ma-ture framework such as phrase-based SMT.
It canbe considered a general statistical translation taskif the character sequences involved are treated likephrases.In so doing, however, a few points need to behighlighted.
Firstly, only parallel data are requiredfor generating transliteration outputs via SMT, andthis SMT translation process can be easily in-tegrated as a component into a general-purposeSMT system.
Secondly, on character sequences,the mapping between source and target name en-tities can be performed on even larger units.
Con-sequently, contextual information can be exploitedto facilitate the alignment, for a string can be usedas a context for every one of its own characters.It is reasonable to expect such relevant informa-tion to produce more precisely statistical resultsfor finding corresponding transliterations.
Thirdly,transliteration as a monotonic word ordering trans-formation problem allows the alignment to be per-formed monotonously from the beginning to theend of a text.
Thus its decoding is easy to performas its search space shrinks this way, for re-orderingis considered not to be involved, in contrast to thegeneral SMT process.This paper is intended to present our workon applying phrased-based SMT technologies totackle transliteration.
The following sections willreport how we have carried out our experiments57for the NEWS2009 task (Li et al, 2009) andpresent the experimented results.2 Transliteration as SMTIn order to transliterate effectively via a phrasebased SMT process for our transliteration task, weopt for the log-linear framework (Och and Ney,2002), a straight-forward architecture to have sev-eral feature models integrated together asP (t|s) = exp[?ni=1 ?ihi(s, t)]?t exp[?ni=1 ?ihi(s, t)](1)Then the transliteration task is to find the propersource and corresponding target chunks to maxi-mize P (t|s) ast = argmaxtP (t|s) (2)In (1), hi(s, t) is a feature model formulated as aprobability functions on a pair of source and targettexts in logarithmic form, and ?i is a parameter tooptimize its contribution.
The two most importantmodels in this framework are the translation model(i.e., the transliteration model in our case), and thetarget language model.
The former is defined ashi(s, t) = log p(s, t) (3)where p(s, t) is p(s|t) or p(t|s) according to thedirection of training corresponding phrases.
(Ochand Ney, 2002) show that p(t|s) gives a resultcomparable to p(s|t), as in the source-channelframework.
(Gao et al, 2004) also confirm ontransliteration that the direct model with p(t|s)performs well while working on the phonemiclevel.
For our task, we have tested these choicesfor p(s, t) on all our development data, arrivingat a similar result.
However, we opt to use bothp(s|t) and p(t|s) if they give similar transliter-ation quality in some language pairs.
Thus wetake p(t|s) for our primary transliteration modelfor searching candidate corresponding charactersequences, and p(s|t) as a supplement.In addition to the translation model feature, an-other feature for the language model can be de-scribed ashi(s, t) = log p(t) (4)Usually the n-gram language model is used for itseffectiveness and simplicity.2.1 TrainingFor the purpose of modeling the training data, thecharacters from both the source and target nameentities for training are split up for alignment, andthen phrase extraction is conducted to find themapping pairs of character sequence.The alignment is performed by expectation-maximization (EM) iterations in the IBM model-4SMT training using the GIZA++ toolkit1.
In someruns, however, e.g., English to Chinese and En-glish to Korean transliteration, the character num-ber of the source text is always more than thatof the target text, the training conducted only oncharacters may lead to many abnormal fertilitiesand then affect the character sequence alignmentlater.
To alleviate this, a pre-processing step beforeGIZA++ training applies unsupervised learning toidentify many frequently co-occurring charactersas fixed patterns in the source texts, including allavailable training, development and testing data.All possible tokens of the source names are con-sidered.Afterwards, the extraction and probability esti-mation of corresponding sequences of charactersor pre-processed small tokens aligned in the priorstep is performed by ?diag-growth-final?
(Koehnet al, 2003), with maximum length 10, which istuned on development data, for both the source-to-target and the target-to-source character align-ment.
Then two transliteration models, namelyp(t|s) and p(s|t), are generated by such extractionfor each transliteration run.Another component involved in the training isan n-gram language model.
We set n = 3 andhave it trained with the available data of the targetlanguage in question.2.2 OptimizationUsing the development sets for the NEWS2009task, a minimum error rate training (MERT) (Och,2003) is applied to tune the parameters for the cor-responding feature models in (1).
The training isperformed with regard to the mean F-score, whichis also called fuzziness in top-1, measuring on av-erage how different the top transliteration candi-date is from its closest reference.
It is worth notingthat a high mean F-score indicates a high accuracyof top candidates, thus a high mean reciprocal rank(MRR), which is used to quantify the overall per-formance of transliteration.1http://code.google.com/p/giza-pp/58Table 1: Comparison: baseline v.s.
optimizedperformance on EnCh and EnRu developmentsets.
?1a ?2 ?3 Mean F MRREnChb Bc 1 1 1 0.803 0.654O 2.38 0.33 0.29 0.837 0.709EnRu B 1 1 1 0.845 0.485O 2.52 0.27 0.21 0.927 0.687a The subscripts 1, 2 and 3 refer to the two transliter-ation models p(t|s) and p(s|t) and another languagemodel respectively, and normalized asP3i=1 ?i = 3.b EnCh stands for English to Chinese run and EnRu forEnglish to Russian run.c B stands for baseline configuration and O for opti-mized case.As shown in Table 1, the optimization of thethree major models leads to a significant per-formance improvement, especially when trainingdata is limited, such as the EnRu run, only 5977entries of name entities are provided for train-ing.
And, it is also found that the optimized fea-ture weights for other language pairs are similar tothese for the two runs as shown in the table above2.Note for the optimization of the parameters, thatonly the training data is used for construction ofmodels.
For the test, both the training and the de-velopment sets are used for training.2.3 DecodingThe trained source-to-target and target-to-sourcetransliteration models are integrated with the lan-guage model as given in (1) for our decoding.We implement a beam-search decoder to dealwith these multiple transliteration models, whichtakes both the forward- and backward-directionalaligned character sequences as factors to con-tribute to the transliteration probability.
Consid-ering the monotonic transformation order, the de-coding is performed sequentially from the begin-ning to the end of a source text.
No re-orderingis needed for such transliteration.
As the searchspace is restricted in this way, the accuracy ofmatching possible transliteration pairs is not af-fected when the decoding is maintained at a fasterspeed than that for ordinary translation.
In ad-dition, another heuristic condition is also used toguide this monotonic decoding.
For those tar-get character sequences found in the training data,their positions in a name entity can help the decod-2Interestingly, the first model contributes much more thanothers.
It can achieve a comparable result even without model2 and 3, according to our experiments.Table 3: Numbers of name entities in NEWS2009training data6.EnCh 34857 EnHi 10990EnJa 29811 EnTa 9031EnKo 5838 EnKa 9040JnJk 19891 EnRu 6920ing to find better corresponding transliterations,for some texts appear more frequently at the be-ginning of a name entity and others at the end.
Weuse the probabilities for all aligned target charac-ter sequences in different positions, and exploit thedata as an auxiliary feature model for the gener-ation.
Finally, all possible target candidates aregenerated by (2) for source names.3 Evaluation ResultsFor NEWS2009, we participated in all 8 standardruns of transliteration task, namely, EnCh (Li etal., 2004), EnJa, EnKo, JnJk3, EnHi, EnTa, EnKaand EnRu (Kumaran and Kellner, 2007).
Ten bestcandidates generated for each source name aresubmitted for each run.
The transliteration per-formance is evaluated by the official script4, usingsix metrics5.
The official evaluation results for oursystem are presented in Table 2.The effectiveness of our approach is revealed bythe fact that many of our Mean F-scores are above0.8 for various tasks.
These high scores suggestthat our top candidates are close to the given ref-erences.
Besides, it is also interesting to look intohow well the desired targets are generated undera certain recall rate, by examining if the best an-swers are among the ten candidates produced foreach source name.
If the recall rate goes far be-yond MRR, it can be a reliable indication that thedesired targets are found for most source names,but just not put at the top of the ten-best.
From thelast column in Table 2, we can see a great chanceto improve our performance, especially for EnCh,JnJk and EnRu runs.3http://www.cjk.org4https://translit.i2r.a-star.edu.sg/news2009/evaluation/5The six metrics are Word Accuracy in Top-1 (ACC),Fuzziness in Top-1 (Mean F-score), Mean Reciprocal Rank(MRR), Precision in the n-best candidates (Map ref), Prece-sion in the 10-best candidates (Map 10) and Precision in thesystem produced candidates (Map sys).6Note that in some of the runs, when a source name hasmultiple corresponding target names, the numbers are calcu-lated according to the total target names in both the trainingand development data.59Table 2: Evaluation result of NEWS2009 task.Task Source Target ACC Mean F MRR Map ref Map 10 Map sys RecallEnCh English Chinese 0.643 0.854 0.745 0.643 0.228 0.229 0.917EnJa English Katakana 0.406 0.800 0.529 0.393 0.180 0.180 0.786EnKo English Hangul 0.332 0.648 0.425 0.331 0.134 0.135 0.609JnJk Japanese Kanji 0.555 0.708 0.653 0.538 0.261 0.261 0.852EnHi English Hindi 0.349 0.829 0.455 0.341 0.151 0.151 0.681EnTa English Tamil 0.316 0.848 0.451 0.307 0.154 0.154 0.724EnKa English Kannada 0.177 0.799 0.307 0.178 0.109 0.109 0.576EnRu English Russian 0.500 0.906 0.613 0.500 0.192 0.192 0.828But still, since SMT is a data-driven approach,the amount of training data could affect thetransliteration results significantly.
Table 3 showsthe training data size in our task.
It gives a hinton the connections between the performance, es-pecially Mean F-score, and the data size.
In spiteof the low ACC, EnKa test has a Mean F-scoreclose to other two runs, namely EnHi and EnTa,of similar data size.
For EnRu test, although thetraining data is limited, the highest Mean F-scoreis achieved thanks to the nice correspondence be-tween English and Russian characters.4 ConclusionIn this paper we have presented our recent work toapply the phrase-based SMT technology to nameentity transliteration on character sequences.
Fortraining, the alignment is carried out on charactersand on those frequently co-occurring character se-quences identified by unsupervised learning.
Theextraction of bi-directional corresponding sourceand target sequence pairs is then performed forthe construction of our transliteration models.
Indecoding, a beam search decoder is applied togenerate transliteration candidates using both thesource-to-target and target-to-source translitera-tion models, the target language model and someheuristic guidance integrated.
The MERT is ap-plied to tune the optimum feature weights for thesemodels.
Finally, ten best candidates are submittedfor each source name.
The experimental resultsconfirm that our approach is effective and robustin the eight runs of the NEWS2009 transliterationtask.AcknowledgmentsThe research described in this paper was sup-ported by City University of Hong Kong throughthe Strategic Research Grants (SRG) 7002267 and7002388.ReferencesW.
Gao, K. F. Wong, and W. Lam.
2004.
Improvingtransliteration with precise alignment of phonemechunks and using context features.
In Proceedingsof AIRS-2004.Dan Goldwasser and Dan Roth.
2008.
Translitera-tion as constrained optimization.
In Proceedings ofEMNLP-2008, pages 353?362, Honolulu, USA, Oc-tober.Kevin Knight and Jonathan Graehl.
1998.
Ma-chine transliteration.
Computational Linguistics,24(4):599?612.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Pharaoh: A beam search decoder for phrase-base statistical machine translation models.
In Pro-ceedings of the 6th AMTA, Edomonton, Canada.A Kumaran and Tobias Kellner.
2007.
A genericframework for machine transliteration.
In Proceed-ings of the 30th SIGIR.Haizhou Li, Min Zhang, and Jian Su.
2004.
Ajoint source-channel model for machine transliter-ation.
In Proceedings of ACL-04, pages 159?166,Barcelona, Spain, July.Haizhou Li, Khe Chai Sim, Jin-Shea Kuo, and MinghuiDong.
2007.
Semantic transliteration of personalnames.
In Proceedings of ACL-07, pages 120?127,Prague, Czech Republic, June.Haizhou Li, A Kumaran, Vladimir Pervouchine, andMin Zhang.
2009.
Report on news 2009 machinetransliteration shared task.
In Proceedings of ACL-IJCNLP 2009 Named Entities Workshop, Singapore.Franz Josef Och and Hermann Ney.
2002.
Discrimina-tive training and maximum entropy models for sta-tistical machine translation.
In Proceedings of ACL-02, pages 295?302, Philadelphia, USA, July.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofACL-03, pages 160?167, Sapporo, Japan, July.Paola Virga and Sanjeev Khudanpur.
2003.
Translit-eration of proper names in cross-lingual informationretrieval.
In Proceedings of the ACL 2003 Workshopon Multilingual and Mixed-language Named EntityRecognition, pages 57?64, Sapporo, Japan, July.60
