Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 588?593,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsGround Truth for Grammatical Error Correction MetricsCourtney Napoles1and Keisuke Sakaguchi1and Matt Post2and Joel Tetreault31Center for Language and Speech Processing, Johns Hopkins University2Human Language Technology Center of Excellence, Johns Hopkins University3Yahoo LabsAbstractHow do we know which grammatical errorcorrection (GEC) system is best?
A num-ber of metrics have been proposed overthe years, each motivated by weaknessesof previous metrics; however, the metricsthemselves have not been compared to anempirical gold standard grounded in hu-man judgments.
We conducted the firsthuman evaluation of GEC system outputs,and show that the rankings produced bymetrics such as MaxMatch and I-measuredo not correlate well with this groundtruth.
As a step towards better metrics,we also propose GLEU, a simple variantof BLEU, modified to account for both thesource and the reference, and show that ithews much more closely to human judg-ments.1 IntroductionAutomatic metrics are a critical component for alltasks in natural language processing.
For manytasks, such as parsing and part-of-speech tagging,there is a single correct answer, and thus a sin-gle metric to compute it.
For other tasks, suchas machine translation or summarization, there isno effective limit to the size of the set of correctanswers.
For such tasks, metrics proliferate andcompete with each other for the role of the domi-nant metric.
In such cases, an important questionto answer is by what means such metrics shouldbe compared.
That is, what is the metric metric?The answer is that it should be rooted in theend-use case for the task under consideration.
Thiscould be some other metric further downstream ofthe task, or something simpler like direct humanevaluation.
This latter approach is the one oftentaken in machine translation; for example, the or-ganizers of the Workshop on Statistical MachineTranslation have long argued that human evalua-tion is the ultimate ground truth, and have there-fore conducted an extensive human evaluation toproduce a system ranking, which is then used tocompare metrics (Bojar et al., 2014).Unfortunately, for the subjective task of gram-matical error correction (GEC), no such groundtruth has ever been established.
Instead, the rank-ings produced by new metrics are justified by theircorrelation with explicitly-corrected errors in oneor more references, and by appeals to intuition forthe resulting rankings.
However, arguably evenmore so than for machine translation, the use casefor grammatical error correction is human con-sumption, and therefore, the ground truth rankingshould be rooted in human judgments.We establish a ground truth for GEC by con-ducting a human evaluation and producing a hu-man ranking of the systems entered into theCoNLL-2014 Shared Task on GEC.
We find thatexisting GEC metrics correlate very poorly withthe ranking produced by this human evaluation.As a step in the direction of better metrics, we de-velop the Generalized Language Evaluation Un-derstanding metric (GLEU) inspired by BLEU,which correlates much better with the human rank-ing than current GEC metrics.12 Grammatical error correction metricsGEC is often viewed as a matter of correcting iso-lated grammatical errors, but is much more com-plicated, nuanced, and subjective than that.
As dis-cussed in Chodorow et al.
(2012), there is oftenno single correction for an error (e.g., whether tocorrect a subject-verb agreement error by chang-ing the number of the subject or the verb), and er-rors cover a range of factors including style, reg-ister, venue, audience, and usage questions, about1Our code and rankings of the CoNLL-2014 Shared Tasksystem outputs can be downloaded from github.com/cnap/gec-ranking/.588which there can be much disagreement.
In addi-tion, errors are not always errors, as can be seenfrom the existence of different style manuals atnewspapers, and questions about the legitimacy ofprescriptivist grammar conventions.Several automatic metrics have been used forevaluating GEC systems.
F-score, the harmonicmean of precision and recall, is one of the mostcommonly used metrics.
It was used as an officialevaluation metric for several shared tasks (Dale etal., 2012; Dale and Kilgarriff, 2011), where par-ticipants were asked to detect and correct closed-class errors (i.e., determiners and prepositions).One of the issues with F-score is that it fails tocapture phrase-level edits.
Thus Dahlmeier andNg (2012) proposed the MaxMatch (M2) scorer,which calculates the F-score over an edit latticethat captures phrase-level edits.
For GEC, M2is the standard, having been used to rank errorcorrection systems in the 2013 and 2014 CoNLLshared tasks, where the error types to be correctedwere not limited to closed-class errors.
(Ng et al.,2013; Ng et al., 2014).
M2was assessed by com-paring its output against that of the official Help-ing Our Own (HOO) scorer (Dale and Kilgarriff,2011), itself based on the GNU wdiff utility.2Inother words, it was evaluated under the assump-tion that evaluating GEC can be reduced to check-ing whether a set of predefined errors have beenchanged into a set of associated corrections.M2is not without its own issues.
First, phrase-level edits can be gamed because the lattice treatsa long phrase deletion as one edit.3Second, theF-score does not capture the difference between?no change?
and ?wrong edits?
made by systems.Chodorow et al.
(2012) also list other complica-tions arising from using F-score or M2, dependingon the application of GEC.Considering these problems, Felice and Briscoe(2015) proposed a new metric, I-measure, whichis based on accuracy computed by edit distancebetween the source, reference, and system output.Their results are striking: there is a negative corre-lation between the M2and I-measure scores (Pear-son?s r = ?0.694).A difficulty with all these metrics is that theyrequire detailed annotations of the location and er-2http://www.gnu.org/s/wdiff/3For example, when we put a single character ?X?
as sys-tem output for each sentence, we obtain P = 0.27, R =0.29,M2= 0.28, which would be ranked 6/13 systems inthe 2014 CoNLL shared task.-6-5-4-3-2-1012818283848586870 10 20 30 40 50 M2 ScoreBLEU I-MeasureFigure 1: Correlation among M2, I-measure, andBLEU scores: M2score shows negative correla-tions to other metrics.ror type of each correction in response to an ex-plicit error annotation scheme.
Due to the inherentsubjectivity and poor definition of the task, men-tioned above, it is difficult for annotators to reli-ably produce these annotations (Bryant and Ng,2015).
However, this requirement can be relin-quished by treating GEC as a text-to-text rewritingtask and borrowing metrics from machine trans-lation, as Park and Levy (2011) did with BLEU(Papineni et al., 2002) and METEOR (Lavie andAgarwal, 2007).As we will show in more detail in Section 5,taking the twelve publicly released system out-puts from the CoNLL-2014 Shared Task,4we ac-tually find a negative correlation between the M2and BLEU scores (r = ?0.772) and positivecorrelation between I-measure and BLEU scores(r = 0.949) (Figure 1).
With the earlier-reportednegative correlation between I-measure and M2,we have a troubling picture: which of these met-rics is best?
Which one actually captures and re-wards the behaviors we would like our systemsto report?
Despite these many proposed metrics,no prior work has attempted to answer these ques-tions by comparing them to human judgments.
Wepropose to answer these questions by producing adefinitive human ranking, against which the rank-ings of different metrics can be compared.3 The human rankingThe Workshop on Statistical Machine Translation(WMT) faces the same question each year as part4www.comp.nus.edu.sg/?nlp/conll14st.html589Figure 2: The Appraise evaluation system.of its metrics shared task.
Arguing that humansare the ultimate judge of quality, they gather hu-man judgments and use them to produce a rankingof the systems for each task.
Machine translationmetrics are then evaluated based on how closelythey match this ranking, using Pearson?s r (priorto 2014) or Spearman?s ?
(2014).We borrow their approach to conduct a humanevaluation.
We used Appraise (Federmann, 2012)5to collect pairwise judgments among 14 systems:the output of 12 systems entered in the CoNLL-14Shared Task, plus the source and a reference sen-tence.
Appraise presents the judge with the sourceand reference sentence6and asks her to rank fourrandomly selected systems from best to worst, tiesallowed (Figure 2).
The four-way ranking is trans-formed into a set of pairwise judgments.We collected data from three native Englishspeakers, resulting in 28,146 pairwise systemjudgements.
Each system?s quality was estimatedand the total ranking was produced on this datasetusing the TrueSkill model (Sakaguchi et al., 2014),as done in WMT 2014.
The annotators had strongcorrelations in terms of the total system rankingand estimated quality, with the reference beingranked at the top (Table 1).4 Generalized BLEUCurrent metrics for GEC rely on references withexplicitly labeled error annotations, the type andform of which vary from task to task and can5github.com/cfedermann/Appraise6CoNLL-14 has two references.
For each sentence, werandomly chose one to present as the answer and one to beamong the systems to be ranked.Judges r ?1 and 2 0.80 0.691 and 3 0.73 0.802 and 3 0.81 0.71Table 1: Pearson?s r and Spearman?s ?
correla-tions among judges (excluding the reference).be difficult to convert.
Recognizing the inher-ent ambiguity in the error-correction task, a bettermetric might be independent of such an annota-tion scheme and only require corrected references.This is the view of GEC as a generic text-rewritingtask, and it is natural to apply standard metricsfrom machine translation.
However, applied off-the-shelf, these metrics yield unintuitive results.For example, BLEU ranks the source sentence assecond place in the CoNLL-2014 shared task.7The problem is partially due to the subtle butimportant difference between machine translationand monolingual text-rewriting tasks.
In MT, anuntranslated word or phrase is almost always anerror, but in grammatical error correction, this isnot the case.
Some, but not all, regions of thesource sentence should be changed.
This obser-vation motivates a small change to BLEU thatcomputes n-gram precisions over the reference butassigns more weight to n-grams that have beencorrectly changed from the source.
This revisedmetric, Generalized Language Evaluation Under-standing (GLEU), rewards corrections while alsocorrectly crediting unchanged source text.Recall that BLEU(C,R) (Papineni et al., 2002)is computed as the geometric mean of the modifiedprecision scores of the test sentences C relative tothe references R, multiplied by a brevity penaltyto control for recall.
The precisions are computedover bags of n-grams derived from the candidatetranslation and the references.
Each n-gram in thecandidate sentence is ?clipped?
to the maximumcount of that n-gram in any of the references, en-suring that no precision is greater than 1.Similar to I-measure, which calculates aweighted accuracy of edits, we calculate aweighted precision of n-grams.
In our adaptation,we modify the precision calculation to assign ex-tra weight to n-grams present in the candidate thatoverlap with the reference but not the source (theset of n-grams R \S).
The precision is also penal-7Of course, it could be the case that the source sentenceis actually the second best, but our human evaluation (?5)confirms that this is not the case.590p?n=?n-gram?CCountR\S(n-gram)?
?
(CountS\R(n-gram))+ CountR(n-gram)?n-gram??C?CountS(n-gram?)
+?n-gram?R\SCountR\S(n-gram)(1)ized by a weighted count of n-grams in the can-didate that are in the source but not the reference(false negatives, S \R).
For a correction candidateC with a corresponding source S and reference R,the modified n-gram precision for GLEU(C,R,S)is shown in Equation 1.
The weight ?
determinesby how much incorrectly changed n-grams are pe-nalized.
Equations 2?3 describe how the countsare collected given a bag of n-grams B.CountB(n-gram) =?n-gram?
?Bd(n-gram, n-gram?)
(2)d(n-gram, n-gram?)
={1 if n-gram = n-gram?0 otherwise(3)BP ={1 if c > re(1?c/r)if c ?
r(4)GLEU (C,R, S) = BP ?
exp(N?n=1wnlog p?n)(5)In our experiments, we used N = 4 and wn=1N, which are standard parameters for MT, thesame brevity penalty as BLEU (Equation 4), andreport results on ?
= {0.1, 0} (GLEU0.1andGLEU0, respectively).
For this task, not penal-izing false negatives correlates best with humanjudgments, but the weight can be tuned for dif-ferent tasks and datasets.
GLEU can be easily ex-tended to additionally punish false positives (in-correctly editing grammatical text) as well.5 ResultsThe respective system rankings of each metric arepresented in Table 2.
The human ranking is con-siderably different from those of most of the met-rics, a fact that is also captured in correlation co-efficients (Table 3).8From the human evaluation,we learn that the source falls near the middle ofthe rankings, even though the BLEU, I-measureand M2rank it among the best or worst systems.M2, the metric that has been used for theCoNLL shared tasks, only correlates moderatelywith human rankings, suggesting that it is not anideal metric for judging the results of a competi-tion.
Even though I-measure perceptively aims to8Pearson?s measure assumes the scores are normally dis-tributed, which may not be true here.Metric r ?GLEU00.542 0.555M20.358 0.429GLEU0.10.200 0.412I-measure -0.051 -0.005BLEU -0.125 -0.225Table 3: Correlation of metrics with the humanranking (excluding the reference), as calculatedwith Pearson?s r and Spearman?s ?.predict whether an output is better or worse thanthe input, it actually has a slight negative correla-tion with human rankings.
GLEU0is the only met-ric that strongly correlates with the human ranks,and performs closest to the range of human-to-human correlation (0.73 ?
r ?
0.81) GLEU0correctly ranks four out of five of the top human-ranked systems at the top of its list, while the othermetrics rank at most three of these systems in thetop five.All metrics deviate from the human rankings,which may in part be because automatic metricsequally weight all error types, when some errorsmay be more tolerable to human judges than oth-ers.
For example, inserting a missing token is re-warded the same by automatic metrics, whether itis a comma or a verb, while a human would muchmore strongly prefer the insertion of the latter.
Anexample of system outputs with their automaticscores and human rankings is included in Table 4.This example illustrates some challenges facedwhen using automatic metrics to evaluate GEC.The automatic metrics weight all correctionsequally and are limited to the gold-standard refer-ences provided.
Both automatic metrics, M2andGLEU, prefer the AMU output in this example,even though it corrects one error and introducesanother.
The human judges rank the UMC out-put as the best for correcting the main verb eventhough it ignored the spelling error.
The UMC andNTHU sentences both receive M2= 0 becausethey make none of the gold-standard edits, eventhough UMC correctly inserts be into the sentence.M2does not recognize this since it is in a differ-ent location from where the annotators placed it.591Human BLEU I-measure M2GLEU0GLEU0.1CAMB UFC UFC CUUI CUUI CUUIAMU source source CAMB AMU AMURAC IITB IITB AMU UFC CAMBCUUI SJTU SJTU POST CAMB UFCsource UMC CUUI UMC source IITBPOST CUUI PKU NTHU IITB SJTUUFC PKU AMU PKU SJTU PKUSJTU AMU UMC RAC PKU UMCIITB IPN IPN SJTU UMC NTHUPKU NTHU POST UFC NTHU POSTUMC CAMB RAC IPN POST RACNTHU RAC CAMB IITB RAC IPNIPN POST NTHU source IPN sourceTable 2: System outputs scored by different metrics, ranked best to worst.System Sentence ScoresOriginalsentenceWe may in actual fact communicating with a hoax Facebook acccount of a cyberfriend , which we assume to be real but in reality , it is a fake account .
?Reference 1 We may in actual fact be communicating with a hoax Facebook acccount of acyber friend , which we assume to be real but in reality , it is a fake account .
?Reference 2 We may in actual fact be communicating with a fake Facebook account of anonline friend , which we assume to be real but , in reality , it is a fake account .
?UMC We may be in actual fact communicating with a hoax Facebook acccount of acyber friend , we assume to be real but in reality , it is a fake account .GLEU = 0.62M2= 0.00Human rank= 1AMU We may in actual fact communicating with a hoax Facebook account of a cyberfriend , which we assume to be real but in reality , it is a fake accounts .GLEU = 0.64M2= 0.39Human rank= 2NTHU We may of actual fact communicating with a hoax Facebook acccount of a cyberfriend , which we assumed to be real but in reality , it is a fake account .GLEU = 0.60M2= 0.00Human rank= 4Table 4: Examples of system output (changes are in bold) and the sentence-level scores assigned bydifferent metrics.However, GLEU awards UMC partial credit foradding the correct unigram, and further assigns allsentences a real score.6 SummaryAs with other metrics used in natural languageprocessing tasks, grammatical error correctionmetrics must be evaluated against ground truth.The inherent subjectivity in what constitutes agrammatical correction, together with the fact thatthe use case for grammatically-corrected output ishuman readers, argue for grounding metric evalu-ations in a human evaluation, which we producedfollowing procedures established by the Workshopon Statistical Machine Translation.
This humanranking shows us that the metric commonly usedfor GEC is not appropriate, since it does not cor-relate strongly; newly proposed alternatives farelittle better.Attending to how humans perceive the qualityof the sentences, we developed GLEU by makinga simple variation to an existing metric.
GLEUmore closely models human judgments than previ-ous metrics because it rewards correct edits whilepenalizing ungrammatical edits, while capturingfluency and grammatical constraints by virtue ofusing n-grams.
While this simple modification toBLEU accounts for crucial differences in a mono-lingual setting, fares well, and could take the placeof existing metrics, especially for rapid system de-velopment as in machine translation, there is stillroom for further work as there is a gap in how wellit correlates with human judgments.Most importantly, the results and data from thispaper establish a method for objectively evaluatingfuture metric proposals, which is crucial to yearlyincremental improvements to the GEC task.AcknowledgmentsWe would like to thank Christopher Bryant, HweeTou Ng, Mariano Felice, and Ted Briscoe for shar-ing their research with us pre-publication.
We alsothank the reviewers and Wei Xu for their valuablefeedback, and Benjamin Van Durme for his sup-port.592ReferencesOndrej Bojar, Christian Buck, Christian Federmann,Barry Haddow, Philipp Koehn, Johannes Leveling,Christof Monz, Pavel Pecina, Matt Post, HerveSaint-Amand, Radu Soricut, Lucia Specia, and Ale?sTamchyna.
2014.
Findings of the 2014 Workshopon Statistical Machine Translation.
In Proceedingsof the Ninth Workshop on Statistical Machine Trans-lation, pages 12?58, Baltimore, Maryland, USA,June.
Association for Computational Linguistics.Christopher Bryant and Hwee Tou Ng.
2015.
Howfar are we from fully automatic high quality gram-matical error correction?
In Proceedings of the53rd Annual Meeting of the Association for Com-putational Linguistics, Beijing, China, July.
Associ-ation for Computational Linguistics.Martin Chodorow, Markus Dickinson, Ross Israel, andJoel Tetreault.
2012.
Problems in evaluating gram-matical error detection systems.
In Proceedings ofCOLING 2012, pages 611?628, Mumbai, India, De-cember.
The COLING 2012 Organizing Committee.Daniel Dahlmeier and Hwee Tou Ng.
2012.
Bet-ter evaluation for grammatical error correction.
InProceedings of the 2012 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 568?572.
Association for Computational Lin-guistics, June.Robert Dale and Adam Kilgarriff.
2011.
Helpingour own: The HOO 2011 pilot shared task.
InProceedings of the Generation Challenges Sessionat the 13th European Workshop on Natural Lan-guage Generation, pages 242?249, Nancy, France,September.
Association for Computational Linguis-tics.Robert Dale, Ilya Anisimoff, and George Narroway.2012.
HOO 2012: A report on the preposition anddeterminer error correction shared task.
In Pro-ceedings of the Seventh Workshop on Building Ed-ucational Applications Using NLP, pages 54?62,Montr?eal, Canada, June.
Association for Computa-tional Linguistics.Christian Federmann.
2012.
Appraise: An open-source toolkit for manual evaluation of machinetranslation output.
The Prague Bulletin of Mathe-matical Linguistics, 98:25?35, September.Mariano Felice and Ted Briscoe.
2015.
Towards astandard evaluation method for grammatical errordetection and correction.
In Proceedings of the 2015Conference of the North American Chapter of theAssociation for Computational Linguistics, Denver,CO, June.
Association for Computational Linguis-tics.Alon Lavie and Abhaya Agarwal.
2007.
METEOR:An automatic metric for MT evaluation with highlevels of correlation with human judgments.
In Pro-ceedings of the Second Workshop on Statistical Ma-chine Translation, pages 228?231, Prague, CzechRepublic, June.
Association for Computational Lin-guistics.Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, ChristianHadiwinoto, and Joel Tetreault.
2013.
The CoNLL-2013 shared task on grammatical error correction.In Proceedings of the Seventeenth Conference onComputational Natural Language Learning: SharedTask, pages 1?12, Sofia, Bulgaria, August.
Associa-tion for Computational Linguistics.Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, ChristianHadiwinoto, Raymond Hendy Susanto, and Christo-pher Bryant.
2014.
The CoNLL-2014 shared taskon grammatical error correction.
In Proceedings ofthe Eighteenth Conference on Computational Natu-ral Language Learning: Shared Task, pages 1?14,Baltimore, Maryland, June.
Association for Compu-tational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automaticevaluation of machine translation.
In Proceedingsof 40th Annual Meeting of the Association for Com-putational Linguistics, pages 311?318, Philadelphia,Pennsylvania, USA, July.
Association for Computa-tional Linguistics.Y.
Albert Park and Roger Levy.
2011.
Automatedwhole sentence grammar correction using a noisychannel model.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages934?944, Portland, Oregon, USA, June.
Associationfor Computational Linguistics.Keisuke Sakaguchi, Matt Post, and BenjaminVan Durme.
2014.
Efficient elicitation of annota-tions for human evaluation of machine translation.In Proceedings of the Ninth Workshop on StatisticalMachine Translation, pages 1?11, Baltimore, Mary-land, USA, June.
Association for ComputationalLinguistics.593
