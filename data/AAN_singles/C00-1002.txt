Learning Word Clusters from Data TypesPao lo  A l legr in i ,  S imonet ta  Montemagn i ,  V i to  P i r re l l iIst ituto di Linguistica Comlmtazionale - CNRVia della Faggiola 32, Pisa, Italy{ allegrii),simo,vito } @ilc.pi.cnr.itAbst ractThe paper illustrates a linguistic knowledge ac-quisition model making use of data types, in-finite nlenlory, and an inferential mechanismtbr inducing new intbrmation Dora known data.The mode\] is colnpared with standard stochas-tic lnethods applied to data tokens, and testedon a task of lexico semantic lassification.1 I n t roduct ion  and  BackgroundOf late, consideral)le interest has been raised bythe use of local syntactic contexts to automati-cally in&me lexico-semantic classes from parsedcorI)ora (Pereira and Tishby 1992; Pereira ctal.
1993; Rooth 1995; Rooth et al 1999).
Thisfamily of approaches takes a 1)air of words (usu-ally a verb plus a noun), and a syntactic rela-tion holding 1)etween the two in context (llSll-ally the ol)ject), and calculates its token distri-bution in a training corI)us.
These (:omits de-line the range of more or less tyi)ical syntac-tic collocates elected by a verb.
The semat>tic similarity between words is then delined interms of sul)stitutability in local contexts (seealso Grefenstette 1994; I.,in 1998): two verbsare semantically close if they typically share thesame range of collocates; conversely, two nounsare semantically close if they take l)art in thesame tyl)e of selection dependencies, i.e.
if theyarc selected t)y the same verbs, with the samefunction.
\]~q'onl this perspective, a syntacticallyasymmetric relation (a dependency) is reinter-preted as a semantic co-selection, where eachterm of the relation can be defined with respectto the other.This symmetric similarity metric is often ac-(;omi)anied by the non trivial assuml)tion thatth, c ,semantic classification of both vcrb,s andnouns be symmetric too.
This is enforced bymaximizing I7 ?tj), withto1--1where p(Ck) is the probability of class Ck 1)e-ing tbund in the training corpus, and p(vi\[Ck)and p(nj\[Ci~) define the probability that verb viand noun nj be associated with the semanticdimension (or meaning component) of class C/~.Intuitively, the joint distribution of fimctiona.llymmotated verb-noun pairs is accounted for t)yassuming thai; each l)air meml)er indel)endentlycorrelates with the same semantic dimension, orselection type (Rooth 1995), a concel)tual pairde.fining all pairs in the class: <g.
"scalar mo-tion", "communicative action" etc.The al)proach has the potential of dealingwith polysemous words: as the same word canin principle belong to more than one (-lass, thereare good reasons to expect hat the correspond-ing selection type positively correlates with oneand only one sense of polysemous words.
A fur-ther t)omls of the al)I)roach is that it makes it ex-i)licit the perspectivizing factor underlying thediscovered similarity of words in a class.On a less positive side, poorly selective verbs(i.e.
verbs which potentially combine with any1101111) Sllch as give, find or get tend to stick to-gether in highly probable classes, 1)ut apl)ear tostake out rather uIfinlbrmative senlantic dimen-sions, relating a motley collection of 11011118, suchas part, way, reason and problem (Rooth 1995),whose only cominonality is the property of be-ing freely interchangeable in the context of theabove-mentioned verbs.Another related issue is how many such di-nlens ions are necessary to account br the entirevariety of senses attested in the training corpus.This is an empirical question, but we contendan inipor~ant one, as the usal)ility of the result;-lag (:lasses heavily dot)ends (m it.
It is (:ommonknowledge that verbs can 1)c, exceedingly (-hoosyill tile way they select their collocates.
Hence,one is Mlowed to use the class Ck to make t)re -dictions about the set of collocates of a verbv~, only if P(v,:\]C~) is sufficiently high.
Con-versely, if CI~ hal)pens to poorly (:orrelate withany verb, the set of nouns in Ck is unlikely 1;oreflect any lexical selection.
This coml)oundswii;h the 1)rol)lent hat the meaning of a verb vican significantly involve more lihalt Olte Selltall-tic (timension: at i;he plesenl; stage of researchin (:Olnlmtat;iollal lexical SO, lnanti('s, Ire scholarhas shown what fun(:tion relal;es l;lm nmaningcomponents of vi to its sehx:tional behaviom'.There is wide room for flu'ther resear(:h in thisarea, but truly ext)lorative tools are still needed.Finally, the des(:ribe(t method is a(:ul;c, lyt)rone to the 1)roblem of si)arse data.
A ll;l)oughi,(cl,,,) is rightly ext)e(:ted to converg(, faster,:ha:, p(,,l.,), still (:o,,vergcu(:e of,(Cl ' , , )  ,:al, b(,ex(:(;edingly sh)w with low frequen('y nouns.
Itis moo|; i;ll;~I; sieving more and more (:()rims (tat~is a solution in all (:ases, its word fl'cquen(:y ishighly sensitive to changes in text genre, topicand domain (S(:hiitze and Pede, rsen 1993).2 The  approachIh~re we illustrate a (lifl'er(mt; at)l)roa('h t() a(>(is|ring lexico semant;i(" (:lasses from sy~,ta('t;i-(:ally lo(-al ('ontexts.
Like the family of sto(:has-tie metho(ts of se(:tion 1, we make use of ~tsimibu:ity ntel;ric 1)ased on sul)stitui;ability ill(ve, rb,noun,flntction) tril)les.
We also share theassumption that lexi(:o semantic lasses are in-herently multidimensional, as they heavily de-pend on cxis|;ence of a perspectivizing factor.Yet, we depart from other assmnt)tions.
Clas-sification of verl)s an(l n(mns is asymmetric:two IIOIIIIS {Ll'e similar if (;hey collo(:ate with asmany semantically diverse vcrl)s as possible inas many (tifli:rent syntactic ontexts as l)ossit)le.The converse apl)lies to verbs.
In other words,semantic similarity of nom:s is not conditionalon the similarity of their ac(:oml)anying verbs,and vicevcrsa.
In a sells(',, classification brc, aksth, c symmetry: maximization of the silnilarityof nouns (verbs) may cause minimization of thesimilarity of i;heir accoml)anying verbs (nouns).A (:lass where a maximum of noun similaritycorrelates with a lni~ximmn of verb similaritycm~ be uninforniative, as exeml)litied above bythe ease of t)oorly selective verbs.Secondly, we assmne (following Fodor 1998)that the number of t)erst)ectivizing factors gov-erning lexieal selection may have the order ofmagnitude of the lexicon itself.
The use ofglobal semantic dimensions may smooth out lex-ical t)refcrences.
This is hardly what we needto semantically annotate lexical l)reti;rences.
Amore conservative al)proa(:h to the t)rol)lem, in-ducing local semantic ('lasses, (:an (:oml)ine al)-1)licability to real language 1)recessing l)roblen~swith the fln'l;lmr t)omls of exploring a relativelymmharted territory.Thirdly, p(vi, nj) ai)t)ears to be too sensitiveto changes in text genre, tol)ic lind domain tobe eXl)ectcd to converge reliably.
We prefer toground a similarity metric oIx measuring the cor-relation among verb noun, type, s ratlw, r than |e-ke, as, tbr two basic reasons: i) verl) noun typesarc (tis(:retc, ;m(t less l)rone t;o random varia-tion in it (parsed) (:orpus, ii) verl) noun tyl)eS(:ml reliably l)e a(:(tuir(xl from highly intbrm~ttivetrot hardly redundant knowledge sources uch ash~xi(:a n(1 encyclot)aedias.Finally, our information refit tbr measuringwor(t similarity is not a (:Oul)le of contextsharing pairs (e.g.
(set, sta.ndard,obj) and(sct, re.cord,obj)) but a qv, ad'r'uplc, of such con-text;s, tbrme(t 1)y (:ombining two verbs with two.
( , . )
) s  ( , .
, .a.d  ),such that they enter an av, ah)gical proportion.2.1 The  analogica l  p ropor t ionIn the t)resenl; conl;ext, an anah)gieal prot)ortion(hereafl;er AP)  is a quadrui)le of flmctionallymmotated t)airs resulting from tile combinationof any two ltOllllS 'l~, i and ~3.j wit;h any two verbsv/,.
and vt su(:h as (2) holds:(v~.,ni,f,,,) : (v~.,nj,L,,)=(v,,ni, fn) : (vt,nj,fiz), (2)where terms along the two diagonals can sw~pt)lace in the 1)rot)ortion , and identity of sub-s(:ript indi(:ates identity of wflues.
Three aspectsof (2) are worth eml)hasizing in this context.First, it does not require that the stone syn-tactic time|ion hold 1)etween all pairs, but onlythat time|ions be pairwisc identical.
Moreover,(2) does not cover all possible syntactic ontextswhere hi, uj, "vk and vt may coral)|he, but onlyth, ose where verb and .function values co-vary.
(.set, standard, obj) : (,set, record, obj) =(meet, standard, obj) : (,,,.tet:,record, x) (3)We call this constraint ile "same-verb-sameflmction" principle.
As we will see in section 2.3,the principle has important consequences on tilesort of similarity induced by (2).
Finally, if oneuses subscripts as tbrmal constraints on typeidentity, then any term can be derived from (2)if the values of all other terms are known.
Forexample given tile partially instantiated propoftion in (3), the last term is filled in unambigu-ously by substituting x = fn = obj.AP  is an important generalization of theinter-substitutabil ity assumption, as it extendstile assumption to cases of flmctionally hetero-geneous verb-nonn pairs.
Intuitively, an APsays that, for two nouns to be taken as sys-tematically similar, one has to be ready to 'aseth, cm interchangeably in at lea,st wo different lo-cal contexts.
This is where the inferential andthe classificatory perspectives meet.2.2 Mathemat ica l  backgroundWe gave reasons for defining the similarity met-ric as a flmction of verb-noun type correlationrather than verb noun token correlation.
Inthis section we sketch the mathematical frame-work underlying this assumption, to show that,tbr a set of verb nonn pairs with a unique syn-tactic function, AP  is the smallest C that sat-isfies eq.(1).Eq.
(1) says that vi and nj are conditionallyindependent given C, meaning that their corre-lation only det)ends on the probability of theirbelonging to C, as tbrmally restated in eq.
(4).p(n, vlC) = p(nlC)p(vlC) (4)In passing from token to type frequency, we as-sume that a projection operator simply assignsa mfitbrm type probability to each event (pair)with a nonzero token probability in the train-ing corpus.
From a learning perspective, thiscorresponds to the assumption that an infinitememory filters out events already seen duringtraining.
The type probability pT(n,v) is de-fined as in eq.
(5), where Np is the number ofdifferent pairs attested in the training corpus.pT(n,v) = 1/Np if the pair is attested,pT(n,v) = 0 otherwise.
(5)By eq.
(4), pT(n, vlC ) ?
0 if and only ifpT(nlC) ~ 0 and pT(vlC) ~ O.
This amountsto saying that all verbs in C are freely inter-changeable in the context of all nouns in C, andviceversa.
We will hereafter efer to C as a sub-stitutability island ( SI).
AP  can accordingly belooked at as the minimal SI.The strength of correlation of nouns andverbs in each S I  can be measured as a sum-mation over the strength of all APs where theyenter.
Formally, one can define a correlationscore or(v, n) as the probability of v and n be-ing attested in a pair.
This can be derived fromour definition of pr(v ,n) ,  as shown in eq.
(6),by substituting pT(n,v) = pr(v)pT(nlv) andpT(nlv ) = 1/w(v), where w(a) is tile type fi'e-quency of a (i.e.
number of different attestedpairs containing a).1- = =G(6) N,, GEq.
(6), after simplification, yields the tbllowingo< (7)By the same token, the correlation flmctionc7(AP) relative to the 4 possit)le pairs in APis calculated ascr(dP) = 1)7'('/)11~3,1 )PSF (~?,2 \[V\] )pT(V2 I'rl,2)PT(7~l \[?
)2 )(3( \[C0(~%1)C0(Vl )C0(~'1,2)C0('02)\] -1.
(g)Eq.
(8) captures the intuition that the corre-lation score between verbs and nouns in APis an inverse function of their type frequency.Nouns and verbs with high type frequency oc-cur in many different pairs: the less selectivethey are, the smaller their semantic contribu-tion to cr(AP).Our preference tbr a(AP) over el(v, n) under-lies the definition of correlation score of S I  givenin eq.
(9) (see also section 4).-_- Z (9)APESI2.3 Breaking the symmetryIn section 2.2 we assumed, for the sake of sim-plicity, that verbs and nouns are possibly re-lated through one syntactic function only.
In a10proportion like (2), however, l;he syntactic time-tion is allowed to wtry.
Nonetheless each rclal;edS\] contains nouns which always combine witha given verb with one and the .same syntactic./:unction.
Clearly, the Sallle is no|; true of verbs.Suppose that an S1 contains two verbs vk and vt(say drive and pierce) and two nouns ni and nj(say nail and peg) that ~re respecl;ively el)jeerand subject of 'vl~ and yr.
The type of similar-ity in the resulting n(mn and verb clusters isof a completely ditti;rent nature: in the case ofn(mns, we acquire dist'rib,utionally parallel words(e.g.
nail and peg); in the case of verbs, weget distrib'utionaIly correlated words (say driveand pierce) which are not interchangeable in thesame conl;exl;.
Mixing the two types of distribu-tional similarity in the same class makes littlesense.
Hereafter, we will aim at maximizing thesimilarity of disl;ributionally parallel nouns.
Indoing so, we will use functionally hel;erogencouscontexts as in (2).
This breaks classiticationsymlne|,ry, and there is no guarantee I;hal; se-mantically coher(mt verb clust;ers be rcl;m'ncd.3 The  methodThe section illusl;ratcs an at)plication of theprinciples of section 2 1;() 1;t5o task of clusteringthe set of object;s ot!
a vo, rl) on t;he basis of arepository of flmctionally mmol;al;ed cont(;xts.3.1 The  knowledge  baseThe training evidence is a Knowledge.
Base(KB)  of flm('tionally anno|;ated verb noun1)airs, ins|;mltiating a wide rallg~e of syntactic re-lations:a) vert)objecl;, e.g.
(,~'a,,,.~,,,'c, ~, 'ol, lc,,z, obj)'cause-1)roblenl';b) verb subject, e.g.
(capitare, pwblcma subj)'occur-problem';c) verb prepositional_complement, e.g.
(recap-pare, probIcma, in,) 'run_into-problenl'.The KaY contains 43,000 pair types, auto-matically extracted from different knowledgesources: dictionaries, both bilingual and mono-lingual (Montemagni 1995), and a corpus of ti-nancial newspapers (Federici st al.
1998).
Thetwo sources rettect two ditt'erent modes of lexi-cal usage: dictionaries give typical examples ofuse of a word, and rmming corpora attest ac-tual usage of words in specific enfl)edding do-mains.
These differences have all impact onthe typology of senses which the two sources1)rovi(le evidence for.
General dictionaries tes-titly all possible senses of a given word; tyl)icalword collocates acquired from dictionaries tendto cover the entire range of possible senses ofa headword.
On the other hand, unrestrictedtexts reIlect actual usage and possibly bear wit-hess to senses which are relevant to a specificdomain only.3.2 The  i nput  wordsThere is abundant psycholinguistic evidencethat semantic similarity between words is em-inently conI;exl; sensitive (Miller and Charles1991).
Moreover, in many language processingtasks, word similarity is typically judged rela-tive to an actual context, as in the cases ofsyntactic disambiguation (both structural andfulwtional), word sense disambiguation, and se-lection of the contextually approt)riate transla-1;ion equiwflent of a word given its neighl)ouringwords.
Finally, close examination of real datashows that (titl'erellt word senses elect classes ofcomplements according to different dilnensionsof semantic similarity.
This is so pervasive, thatit soon be(:omes impossit)le to t)rovide an efl'ec-live account of these, dimensions independentlyof the sense in question.Ewduation of botll accuracy and usability ofany autontatic classitication of words into se-mantic clusters cammt lint artificially elude th(;1)asic question "similar in what respc, ct;?".
Ourchoice of input words retlects these concerns.\?e automatically clustered the set of objects ofa given verb, as they arc attested in a test co lpus.
This yields local lexico seman{;ic lasses,i.e.
conditional on the selected verb head, asopposed to global classes, i.e.
built once andtbr all to accomlt tbr the collocates of any verb.Among the practical advantages of local clas-sitication we should at least mention the follow-ing two.
Choice of a verb head as a perspec-tivizing factor considerably reduces the possi-bility that the same polysemous object collocateis used in different senses with the same verb.Fnrthermore, the resulting clusters can give in-tormal;ion about the senses, or meaning facets,of the verb head.a.a Ident i f icat ion and ranking of  nounclustersFor the sake of concreteness, let us consider thetbllowing object-collocates of the Italian verb11{APPESANTIMENTO~ CRESCITA,FLESSIONE, RIALZO}{CRESCITA, FLESSIONE}{CRESCITA, GUAI0}{CRESCITA, PROBLEMA}{CRESCITA, RITARD0}{FLESSIONE, PROBLEMA}{FLESSIONE, RIALZO}{GUAI0, PROBLEMA}{RIDIMENSIONAMENT0, RITARD0}Figure h Some Sisheadword causarc.
: CAUSARE/O, REGISTRARE/O: CAUSARE/O, EVIDENZIARE/S,MEDIARE/O, MOSTRARE/O~PRESENTARE/S, PRESENTIRE/OREGISTRARE/O, REGISTRARE/$: CAUSARE/0, PROVOCARE/0AVERE/S~ CAUSARE/0, EVIDENZIARE/0PORRE/S, PRESENTARE/SCAUSARE/O, USARE/SCAUSARE/0, PRESENTARE/S, STARE/SCAUSARE/0, REGISTRARE/0REGISTRARE/S, SUBIRE/0CAUSARE/0, CAVARE -- SI/S, INCAPPARE/SCAUSARE/O, GIUSTIFICARE/Orelative to the collocates of thecausare 'cause', as they are found in a test cor-pus:appcsantimento 'increase in weight',crescita 'growth', flessionc 'decrease',guaio 'trouble', p~vblcma 'prol)lem', rialzo'rise', ridimensionamcnto 'reduction',ritardo 'delay', turbolenza 'turbulence'.Clustering these input words requires prelim-inary identification of Substitutability Islands(Sis).
An example of SI  is the quadrupletbrmed by the verb pair causare 'cause' and in-eapparc 'rnn into' and the noun pair guaio 'trou-ble' and problema 'problem', where menfl)ers ofthe same pair are inter-substitutable in context,give:: the constraints entbrced by the AP typein (2).
Note that guaio and problema are ob-jects of eausare, and prepositional complements(headed by in 'in') of incappare.
This makes itpossible to maximize the sinfilarity of troubleand problem across fimctionally heterogeneouscontexts.Bigger Sis than the one just shown will formas many APs as there are quadruples of col:-textually interchangeable nouns and verbs.
Weconsider a lexico-semantic cluster of nouns theprojection of an SI  onto the set of nouns.
Fig.1illustrates a sample of noun clusters (betweencurly brackets) projected from a set of Sis, to-gether with a list of the verbs tbund in the sameSis (the suffix 'S' stands tbr subject, and 'O'for object).
Due to the asymmetry of classifica-tion, verbs in Sis are not taken to tbrm part ofa lexico-semantic cluster in the same sense asnonns  are .7.04509e-O5{GUAIO,PKOBLEMA}7.01459e-O5{RIDIMENSIONAHENTO,RITARDO}4.65858e-O5{CRESCITA,FLESSIONE}I.T5699e-O5{FLESSIONE,RIALZO}9.49509e-O6{APPESANTIMENTO,CRESClTA,FLESSIONE,RIALZO}1.88964e-O6{CRESCITA,GUAIO}1.19814e-O6{CRESCITA,RITARDO}8.84254e-OT{CRESCITA,PROBLEMA}6,TI41e-OT{FLESSIONE,PROBLEMA}Figure 2: Nine top-most scored noun clustersNot all projected noun clusters exhibit thesame degree of semantic oherence.
Intuitively,the cluster {appesantimento crescita flessioneriaIzo} 'increase in weight, growth, decrease,rise' is semantically more appeMing tlmn thecluster {crescita problema} 'growth problem'(Fig.l).A quantitative measure of the sen:antic ohe-sion of a noun cluster CN is give:: by the con'e-lation score ~(SI) of the SI  of which UN is aprojection.
In Fig.2 noun clusters are ranked bydecreasing vahms of cr(SI), calculated accordingto eq.
(9).3.4 Cent ro id  ident i f i ca t ionNoun clusters of Figs.1 and 2 are admittedlyconsiderably fine grained.
A coarser grain canbe attained trivially through set union of inte:'-secting clusters.
In fact, what we want to obtainis a set of mazimally orthogonal and semanti-cally coherent noun classes, under the assump-tion that these (:lasses highly correlate with theprincipal meaning components of the verb headof which input nouns are objects.In the algorithm ewfluated here this isachieved in two steps: i) first, we select thebest possible centroids of the prospective classesamong the noun clusters of Fig.2; secondly,ii) we lmnp outstanding clusters (i.e.
clusterswhich have not been selected in step i)) aroundthe identified centroids.
In what tbllows, we willonly focus on step i).
Results and evaluation ofstep ii) are reported in (Allegrini et al 2000).In step i) we assume that centroids aredisjunctively defined, maximally coherentclasses; hence, there exists no pair of intersect-ing centroids.
The best possible selection ofcentroids will include non intersecting clusterswith the highest possible cumulative score.
Inpractice, the best centroid corresponds to the12cluster with the topmost cr(SI).
The secondbest centroid is the cluster with the secondhighest o-(SI) and no intersection with the first;centroid, and so on (the i-th centroid is 1;11(',i.-th highest chlster with no intersection withthe tirst i - 1. centroids) until all clusters in therank are used Ill).
Clusters selected as centroidsin the causare example above ~tre: {GUAIOPROBLEMA}, {RIDIMENSIONAMENTO RITARDO},{CaP.SCITA FL~.SS-rONE}.Clearly, this is not the only t)ossible strategyt'or centroid selection, lint certainly a suitabh;one giv(;n our assulnl)tions mM goals.
To stunUl) , the targeted classification is local, i.
('., con-ditional on a specific verl~ head, and orthogonal ,i.c.
it aims at identifying maximally disjulmtiveclasses with high correlation with the principalmeaning ('Oral)orients of the vert) head.
Thisstrategy h;ads to identificalfion of the differentsenses, or possibly me,ruing facets, of a vert).In tlteir turn, noun clusters may capture sul)-tle semm~tic distinctions.
For instance,, a dis-tinction is made between incremental eveni;s orresults of incrt'anental e, vents, which 1)resut)l)OSe~ scalar dimension (as in the ease of {cre,,s'cita,/lcssione} 'growth, decrease') and re, schedulingeve, nts, where a change occurs with respect toa previously planned event or object (see thecentroid {ridimensiov, amento  ritardo} :reduc-tion debw' ).4 Exper iment  and  eva luat ionWe, were able to extract all S l 's relative to theentire K\]3.
However, we report here an intr ins icevaluation of the accuracy of acquired ce.ntroidswhich involves ()lily a small subset of our results,since provision of a refhrence class tyl)ology isextremely labour intensive.
1We consider 20 Italian verbs and their objectcollocates.gThe object collocates were automat-ically extracted fi'om the "Italian SPAIIKLEReference Corpus", a corpus of Italian financial1For an extrinsic evahlation of the proposed similar-ity measure the reader is referred to (Montemagni et aI.1996; Briscoe ct al.
1999; Federici ct al.
1999a).2The |x,'st verbs are: a.qgiungcrc 'add', aiutare 'hell)' ,a,sl)cttarc 'expect', cambiarc 'change', C(t*taO,?
't: t(:}tllSe:~chicdcrc 'ask', considc.rarc 'consider', dare.
'give', dc-ciderc 'decide', fornive 'provide', muoverc.
'move', pcrm~'.-ttere 'allow', portarc 'bring', p~wlurrc 'produce', sccglicrc'choose', sentirc 'feel', stabilire 'establislF, tagliarc 'cut',terminate 'end', trovarc 'find'.newspapers of about one million word tokens(Federici ct al.
1998).l?or each test verb, an indetmndent classifica-tion of its collocates was created lnanually, bypartitioning the collocates into disjoint sets ofsemantically coherent lexical preferences, eachset pointing to distinct senses of the test; verb,according to a reference monolingual dictionary(Garzanti 1984).
This considerably reduces theanlount of subjectivity inevitably involved inthe creation of a reference partition, and mini-mizes the probability that more than one senseof a t)olysemous noun can appear in the sameclass of collocates.The inferred centroids, selected from clustersranked by c~(SI) defined as in (9), are t)rojected~gainst he reference classification.
Precision isdelined as the ratio between 1;t1(; mmflmr of con-troids t)roperly inchlded in one reference classand l;he nmnber of inferred centroids.
Recall isdefined as the ratio between the number of relhr-time classes which properly inchlde at least onecentroid an(t the nmnber of all reference classes.Fig.3 shows results for the sets of object collo-cates of 1)olysemous {;est verbs only, as lttOltOSe-mous verbs trivially yMd 100% precision recall.An average, wflue over the sets of object col-locates of all verbs is also shown, with 86%88% of precision recall.
Another average valueis also l)lotted (as a black ul)right triangle), ol)-l:ained \])y ranking n(mn clusters l)y ~(S\]) calcu-lated as ill (10).
This average wflue (53% 53%precision recall) provides a sort of baseline ofthe difliculty of the task, and sheds consider-able light on the use of APs ,  rather than simpleverb noun pairs, as inforlnation units ibr mea-suring internal cohesion of centroids.
(si) =_ (10)(n,v)G915 Conc lus ionWe described a linguistic knowledge acquisitionmodel and tested it; on a word classification task.The main points of our prot)osal are:?
classification is asymmetric, grounded onprinciples of machine learning with intinitememory;?
the algorithm is explorative and non-reductionist; no a priori  model of class dis-13' i .
.
.
.
.
.
.
.
.
.
.
.
T L .
.
.
.
.
.
.
.
.
i:i:!
AIUTAREI ICAMBIARE0.9 CHIEDERECONSIDERARE ?- bARE' ,?DECIDERE0.8 PORTAREPRODURRE?
SCEGLIERE ~ SENTIRE STABILIRE~TAGLIARE \ [ :1~ ?~:TFIOVA RE0.6 ~'AVERAGE(AP)~AVERAGE(pai 0i ?
io.5 ::.i ? '
1Ii0.3 0.4 0.5 0.6 0,7 0.8 0.9 1PRECISIONd < ~ 0.7Figure 3: Centroid precision and recall for objectcollocates of polysemous verbs.tril)ution is assmned;?
classification is modelled z~s the task offorming a web of context dependent se-mantic associations among words;?
the approach uses a context--sensitive no-tion of semantic similarity;?
the approach rests on the notion of analog-ical proportion, which proves to t)e a reli-able intbrmation refit for measuring seman-tic similarity;?
analogical t)roportions are harder to trackdown than simple pairs, and interconnectedin a highly complex way; yet, reliance ondata types, as opposed to token frequen-cies, makes the proposed method comtm-rationally tractable and resistant o datasparseness.ReferencesAllegrini P., Montemagni S., Pirrelli V. (2000) Con-trolled Bootstrapt)ing of Lexico-semantic Classesas a Bridge between Paradigmatic and Syntag-matic Knowledge: Methodology and Evaluation.In Precccdings of LREC-2000, Athens, GreeceMay-June 2000, pp.
601-608.Briseoe T., McCarthy D., Carroll J., Allegrini P.,Calzolari N., Federici S., Montemagni S., Pir-relli V., Abney S., Beil F., Carroll G., Light M.,Prescher D., Riezler S., Rooth M. (5999) Acqui-sition System for Syntactic and Semantic Typeand Selection.
Deliverable 7.2.
WP 7, EC projectSPARKLE "Shallow Parsing and Knowledge Ex-traction for Language Engineering" (LE-2111).Federici, S., Montemagni, S., Pirrelli, V. (1999)SENSE: an Analogy based Word Sense Disam-biguation System.
In M. Light and M.
Pahner(eds.
), Special Issue of Natural Language Engi-neerin 9on Lexical Semantic Tagging.Federici, S., Montemagni, S., Pirrelli, V., Calzolari,N.
(1998) Analogy-based Extraction of LexicalKnowledge from Corpora: the SPARKLE Expe-rience.
In Proceedings of LREC-1998, Granada,SP, May 1998.Fodor, J.A.
(1998) Concepts.
Where Co.qnitivc Sci-ence Went Wzvng.
Clarendon Press, Oxtbrd,1998.Garzanti (1984) ll Nuovo Dizionario ItalianoGarzanti.
Garzanti, Milano, 1984.Grefenstette, G. (1994) Explorations in AutomaticThesaurus Discovery.
Kluwer Acadenfic Publish-ers, Boston, 1994.Lin D. (1998) Automatic Retrieval and Clusteringof Similar Words.
In Proceedings of COLING-A CL'98, Montreal, Canada, August 1998.Miller, G.A., Charles, W.G.
(1991) Contextual Cor-relates of Semantic SimilariW.
In Language andCognitive Processes, 6 (1), pp.
1-28.Montemagni, S. (1.995) Subject and Object in ItalianSentence Processing.
PhD Dissertation, UMIST,Manchester, UK, 1995.MontemagIfi, S., Federici, S., Pirrelli, V. (1996)Resolving syntactic ambiguities with lexicosemantic patterns: an analogy-based apl)roach.In Proceedings of COLING-96, Copenhagen, Au-gust 1996, pp.
376 381.Pereira, F., Tishby, N. (1992) Distributional Sim-ilarity, Phase Transitions and Hierarchical Clus-tering.
In Working Notes, Fall Symposium Series.AAAI, pp.
54 64.Pereira, F., Tishby, N., Lee, L. (1993) DistrilmtionalClustering Of English Words.
In Proceedings ofthe 30th Annual Meeting of the Association forComputational Linguistics, pp.
183 190.Rooth, M. (Ms) Two-dimensional clusters in gram-matical relations.
In Symposium on 12epresenta-tion and Acquisition of Lcxical Knowledge: Pol-ysemy, Ambiguity and Gcncrativity, AAAI 1995Spring Symposium Series, Stanford University.Rooth, M., Riezler, S., Prescher, D., Carroll, G.,Bell, F. (1999) Inducing aSemanticallt AnnotatedLexicon via EM-Based Clustering.
In Procccdingsof the 37th Annual Meeting of the Association for'Computational Linguistics, Maryland, USA, June1999, I)P. 104-111.Schfitze, H., Pedersen, J.
(1993) A vector modelfor syntagmatic and paradigmatic relatedness.
InProceedings of the 9th Annual Confcrcncc of theUW Centrc for" the New OED and Text 12cscarch,Oxford, England, 1993, pp.
104-113.14
