Redundancy: helping semantic disambiguationCaroline Barri~reSchool of Information Technology and EngineeringUniversity of OttawaOttawa, Canada, K IN  7Z3barriere@site.uottawa.caAbstractRedundancy is a good thing, at least in a learn-ing process.
To be a good teacher you mustsay what you are going to say, say it, then saywhat you have just said.
Well, three times isbetter than one.
To acquire and learn knowl-edge from text for building a lexical knowledgebase, we need to find a source of informationthat states facts, and repeats them a few timesusing slightly different sentence structures.
Atechnique is needed for gathering informationfrom that source and identify the redundant in-formation.
The extraction of the commonalityis an active learning of the knowledge xpressed.The proposed research is based on a clusteringmethod developed by Barri~re and Popowich(1996) which performs a gathering of relatedinformation about a particular topic.
Individ-ual pieces of information are represented via theConceptual Graph (CG) formalism and the re-sult of the clustering is a large CG embeddingall individual graphs.
In the present paper, wesuggest hat the identification of the redundantinformation within the resulting graph is veryuseful for disambiguation of the original infor-mation at the semantic level.1 IntroductionThe construction of a Lexical Knowledge Base(LKB), if performed automatically (or semi-automatically), attempts at extracting knowl-edge from text.
The extraction can be viewedas a learning process.
Simplicity, clarity and re-dundancy of the information given in the sourcetext are key features for a successful acquisitionof knowledge.
We assume success is attainedwhen a sentence from the source text expressedin natural anguage can be transformed into anunambiguous internal representation.
Using aconceptual graph (CG) representation (Sowa,1984) of sentences means that a successful ac-quisition of knowledge corresponds to trans-forming each sentence from the source text intoa set of unambiguous concepts (correct wordsenses found) and unambiguous relations (cor-rect semantic relations between concepts).This paper will look at the idea of makinggood use of the redundancy found in a text tohelp the knowledge acquisition task.
Things arenot always understood when they are first en-countered.
A sentence xpressing new knowl-edge might be ambiguous (at the level of theconcepts it introduces and/or at the level of thesemantic relations between those concepts).
Asearch through previously acquired knowledgemight help disambiguate he new sentence or itmight not.
A repetition of the exact same sen-tence would be of no help, but a slightly differ-ent format of expression might reveal necessaryaspects for the comprehension.
This is the av-enue explored in this paper which will unfoldas follows.
Section 2 will present briefly a pos-sible good source of knowledge and a gather-ing/clustering technique.
Section 3 will presenthow the redundancy resulting from the cluster-ing process can be used in solving some typesof semantic ambiguity.
Section 4 will emphasizethe importance of semantic relations for the pro-cess of semantic disambiguation.
Section 5 willconclude.2 Source of information andclustering techniqueTo acquire and learn knowledge from text forbuilding a lexical knowledge base, we need tofind a source of information that states facts,and repeats them a few times using slightlydifferent sentence structures.
A technique isneeded for gathering information from thatsource and identify the redundant information.103These two aspects are discussed hereafter: (1)the choice of a source of information and (2) theinformation gathering technique.2.1 Choice of source of informationWhen we think of learning about words, wethink of textbooks and dictionaries.
Redun-dancy might be present but not always simplic-ity.
Any text is written at a level which assumessome common knowledge among potential read-ers.
In a textbook on science, the author willdefine the scientific terms but not the generalEnglish vocabulary.
In an adult's dictionary,all words are defined, but a certain knowledge ofthe "world" (common sense, typical situations)is assumed as common adult knowledge, so theemphasis of the definitions might not be on sim-ple cases but on more ambiguous or infrequentcases.
To learn the basic vocabulary used inday to day life, a very simple children's first dic-tionary is a good place to start.
In (Barri~re,1997), such a dictionary is used for an appli-cation of LKB construction in which no priorsemantic knowledge was assumed.
In the sameresearch the author explains how to use a multi-stage process, to transform the sentences fromthe dictionary into conceptual graph represen-tations.
This dictionary, the Amer ican  Her-itage First Dictionary 1 (AHFD), is an ex-ample of a good source of knowledge in termsof simplicity, clarity and redundancy.
Some def-initions introduce concepts that are mentionedagain in other definitions.2.2 Gathering of i n format ionBarri~re and Popowich (1996) presented theidea of concept clustering for knowledge integra-tion.
First, a Lexical Knowledge Base (LKB) isbuilt automatically and contains all the nounsand verbs of the AHFD, each word having itsdefinition represented using the CG formalism.Here is a brief summary of the clustering pro-cess from there.
It is not a statistical cluster-ing but more a "graph matching" type of clus-tering.
A trigger word is chosen and the CGrepresentation f its defining sentences make upthe initial CCKG (Concept clustering knowl-edge graph).
The trigger word can be any word,1Copyright (~)1994 by Houghton Mifflin Company.Reproduced by permission from THE AMERICANHERITAGE FIRST DICTIONARY.but preferably it should be a semantically sig-nificant word.
A word is semantically signifi-cant if it occurs less than a maximal numberof times in the text, therefore xcluding gen-eral words such as place, or person.
The clus-tering is really an iterative forward and back-ward search within the LKB to find definitionsof words that are somewhat "related" to thetrigger word.
A forward search looks at the def-inition of the words used in the trigger word'sdefinition.
A backward search looks at the defi-nition of the words that use the trigger word tobe defined.
A word becomes part of the clusterif its CG representation shares a common sub-graph of a minimal size with the CCKG.
Theprocess is then extended to perform forward andbackward searches based on the words in thecluster and not only on the trigger word.The cluster becomes a set of words related tothe trigger word, and the CCKG presents thetrigger word within a large context by showingall the links between all the words of the clus-ter.
The CCKG is a merge of all individual CGsfrom the words in the cluster.Table 1 shows examples of clusters found byusing the clustering technique on the AHFD.
Ifa word is followed by _#, it means the sense #of that word.
The CCKGs corresponding to theclusters are not illustrated as it would requiremuch space to show all the links between allthe words in the clusters?The clustering method described is based onthe principle that information is acquired froma machine readable dictionary (the AHFD), andtherefore ach word is associated with someknowledge pertaining to it.
To extend this clus-tering technique to a knowledge base containingnon-classified pieces of information, we wouldneed to use some indexing scheme allowing ac-cess to all the sentences containing a particular2The reader might wonder why such word as \[rain-bow\] is associated with \[needle_l\] or why \[kangaroo\] isassociated with \[stomach\].
The AHFD tells the childthat "A rainbow looks like a ribbon of many colors acrossthe sky."
and "Kangaroo mothers carry their babies in apocket in ~ront of their stomachs."
The threshold usedto define the minimal size of the common subgraph nec-essary to include a new word in the cluster is establishedexperimentally.
Changing that threshold will changethe size of the resulting cluster therefore affecting whichwords will be included.
The clustering technique, anda derived extended clustering technique are explained inmuch details in (Barri~re and Fass, 1998).104Table 1: Multiple clusters from different wordsTrigger Clusterwordneedle_l {needle_l, sew, cloth, thread, wool,handkerchief, pin, ribbon, string, rainbow}sewkitchenstovestomachairplaneelephantsoapwash{sew, cloth, needle_i, needle_2, thread,button, patch_i, pin, pocket, wool,ribbon, rug, string, nest, prize, rainbow}kitchen, stove, refrigerator, pan}{stove; pan, kitchen, refrigerator, pot, clay}{stomach, kangaroo, pain, swallow, mouth}{airplane, wing, airport, fly_2, helicopter,jet, kit, machine, pilot, plane}{elephant, skin, trunk_l, ear, zoo, bark,leather, rhinoceros}{soap, dirt, mix, bath, bubble, suds,wash, boil, steam}{wash, soap, bath, bathroom, suds,bubble, boil, steam}word in them.3 Semant ic  d i sambiguat ionWe propose ill this section a way to attempt atsolving different ypes of semantic ambiguitiesby using the redundancy of information result-ing from the clustering technique as briefly de-scribed in the previous ection.
Going throughan example, we will look at three types of se-mantic ambiguity: anaphora resolution, wordsense disambiguation, and relation disambigua-tion.In Figure 1, Definition 3.1 shows one sen-tence in the definition of mail_l (taken fromthe AHFD, as all other definitions in Figure 1)with its corresponding CG representation.
Def-inition 3.2 shows one sentence in the definitionof stamp also with its CG representation.
Usingthe clustering technique briefly described in theprevious ection, the two words are put togetherinto a cluster triggered by the concept \[mail_l\].Result 3.1 shows the maximal join 3 betweenthe two previous graphs around shared concept\[mail_l\].
Combining the information from stampand mail_l, puts in evidence the redundant in-formation.
The reduction process for eliminat-ing this redundancy will solve some ambigui-ties.
This process is based on the idea of find-ing "compatible" concepts within a graph.
Twoconcepts are compatible if their semantic dis-tance is small.
That distance is often based onaA maximal join is an operation defined within theCG formalism to gather knowledge from two graphsaround a concept hat they both share.the relative positions of concepts within the con-cept hierarchy (Delugach, 1993; Foo et ai., 1992;Resnik, 1995).
For the present discussion we as-sume that two concepts are compatible if theyshare a semantically significant common super-type, or if one concept is a supertype of theother.In Result 3.1, the concept \[send\] is presenttwice, and also the concept \[letter\] is presentin two compatible forms: \[letter\] and \[mes-sage\].
The compatibility comes from the pres-ence in the type hierarchy 4 of one sense of\[letter\], \[letter_2\], as being a subtype of \[mes-sage\].
These compatible forms actually allowthe disambiguation of concept \[letter\] into \[let-ter_2\].
This should update the definition ofstamp shown in Definition 3.2.
The other senseof \[letter\], [letter_l\] is a subtype of \[symbol\].The pronoun they in Result 3.1 must refer tosome word, either previously mentioned in thesentence, or assumed known (as a default) in theLKB.
Both (agent) relations attached to con-cept \[send\] lead to compatible concepts: \[they\]and \[person\].
We can therefore go back to thegraph definition of \[stamp\] in which the pronoun\[they\] could have referred to the concepts \[let-ters\], \[packages\], \[people\] or \[stamps\], and nowdisambiguate it to \[people\].Result 3.2 shows the internal join which es-tablishes coreference links (shown by *x, *y,*z) between compatible concepts that are in anidentical relation with another concept.
The re-duced join, after the redundancy is eliminated,is shown in Result 3.3.Two types of disambiguation (anaphora res-olution and word sense disambiguation) wereshown up to now.
The third type of disam-biguation is at the level of the semantic re-lations.
For this type of ambiguity, we mustbriefly introduce the idea of a relation hierar-chy which is described and justified in more de-tails in (Barri~re, 1998).
A relation hierarchy,as presented in (Sowa, 1984), is simply a wayto establish an order between the possible rela-tions.
The idea is to include relations that cor-respond to the English prepositions (it could bethe prepositions of any language studied) at thetop of the hierarchy, and consider them gener-alizations of possible deeper semantic relations.4The type hierarchy has been built automaticallyfrom information extracted from the AHFD.105Def in i t ion  3.1 -MAIL_I : People send messages through the mail.\[send\]-> (agent)->\[person:plural\]-> (object)->\[message:plural\]-> (through)->\[mail_l\]Def in i t ion  3 .2  -STAMP : People buy stamps to put on letters and packages they send through the mail.\[send\]-> (object)-> \[letter:plural\]-> (and)- > \[package:plu ral\]<-(on) <-\[put\] <-(goal) <-\[buy\]->(agent)-> \[person:plural\]-> (object)->\[stamp:plural\]- > (agent)- > \[they\]-> (through)->\[mail_l\]Resu l t  3 .1 - Maximal join between mail_l and stamp\[send\]-> (object)-> \[letter:plural\]-> (and)- > \[package:plu ral\]<-(on) <-\[put\] <-(goal) <-\[buy\]->(agent)-> \[person:plural\]-> (object)->\[stamp:plural\]-> (agent)-> \[they\]-> (through)-> \[mail_l\] <-(through)<-\[send\]-> (object)-> \[message:plural\]-> (agent)->\[person:plural\]Resu l t  3 .2  - Internal Join on Graph maiL1/stamp\[send *y\]->(object)->\[letter:plural *z\]->(and)->\[package:plural\]<-(on) <-\[put\] <-(goal) <-\[buy\]->(agent)-> \[person:plural\]->(object)->\[stamp:plural\]- > (agent)- > \[they *x\]->(through)->\[mail_l\]<-(through)<-\[send *y\]->(object)->\[message:plural *z\]-> (agent)->\[person:plural *x\]Resu l t  3 .3  - After reduction of graph mail_l/stamp\[letter_2:plural\]-> (and)-> \[package:plu ral\]<-(object) <-\[send\]-> (agent)-> \[person:plural\]-> (through)->\[mail_l\]<-(on) <-\[put\] <-(goal) <-\[buy\]-> (agent)->\[person:plural\]-> (object)-> \[stamp: plural\]F igure  1: Example  of ambigu i ty  reduct ion106Def in i t ion 3.3 -CARD_2 : You send cards to people in the mail.\[send\]-> (agent)- > \[you\]- > (object)-> \[card_2:plu ral\]-> (to)-> \[person:plural\]-> (in)->\[mail_l\]Resu l t  3.4 - Graph mail_l/stamp joined to card (after internal join)\[letter_2:plural\]-> (and)- > \[package:plural\]<-(object) <-\[send *y\]->(agent)->\[person:plural *z\]-> (through)-> \[mail- l \] <-(in)<-\[send *y\]-> (to)-> \[person :plural\]-> (agent)-> \[you *z\]->(object)->\[card_2:plural\]<-(on) <-\[put\] <-(goal) <-\[buy\]-> (agent)-> \[person :plural\]-> (object)->\[stamp:plural\]Resu l t  3.5 - After reduction of graph mail_l/stamp/card_2\[letter.2:plural\]-> (and)- > \[package: plural\]- >(and)-> \[card-2:plural\]<-(object) <-\[send\]-> (agent)- > \[person:plural\]->(manner)->\[mail_l\]->(to) -> \[person:plural\]<-(on)<-\[put\] <-(goal)<-\[buy\]-> (agent)-> \[person :plural\]-> (object)-> \[sta m p:plu ral\]Figure 1: Example of ambiguity reduction (continued)This relation hierarchy is important for thecomparison of graphs expressing similar ideasbut using different sentence patterns that arereflected in the graphs by different prepositionsbecoming relations.
Let us look in Figure 1 atDefinition 3.3 which gives a sentence in the defi-nition of \[card_2\] and Result 3.4 which gives themaximal join with graph mail_l/stamp from re-sult 3.3 around concept \[mail_l\].Subgraphs \[send\]->(in)->\[mail_l\] nd [send\]-> (through)-> \[mail_l\] have compatible conceptson both sides of two different relations.
Thesetwo prepositions are both supertypes of a re-stricted set of semantic relations.
On Figure 2which shows a small part of the relation hierar-chy, we highlighted the compatibility betweenthrough and in.
It shows that the two prepo-sitions interact at manner  (at locat ion as wellbut more indirectly).
Therefore, we can estab-lish the similarity of those two relations via themanner  relation, and the ambiguity is resolvedas shown in Result 3.5.Note that the concept \[person\] is presentmany time among the different graphs in Fig-ure 1.
This gives the reader an insight into thecomplexity behind clustering.
It all relies oncompatibility of concepts and relations.
Com-patibility of concepts alone might be sufficient ifthe concepts are highly semantically significant,but for general concepts like \[person\], \[place\],\[animal\] we cannot assume so.
In the graph pre-sented in Result 3.5, there are buyers of stamps,receivers and senders of letters and they are allpeople, but not necessarily the same ones.We saw the redundancy resulting from theclustering process and how to exploit this re-dundancy for semantic disambiguation.
We seehow redundancy at the concept level withoutthe relations can be very misleading, and thefollowing section emphasize the importance ofsemantic relations.107with \ [ '~ I throughl at onaccompanimen/ par t -o /~ ,~~instrument y ~ ~ point-in-timeaboutImannerl Ilocationldestination direction sourceFigure 2: Small part of relation taxonomy.4 The impor tance  o f  semant icre la t ionsClusters are and have been used in differentapplications for information retrieval and wordsense disambiguation.
Clustering can be donestatistically by analyzing text corpora (Wilkset al, 1989; Brown et al, 1992; Pereira etal., 1995) and usually results in a set of wordsor word senses.
In this paper, we are usingthe clustering method used in (Barri~re andPopowich, 1996) to present our view on re-dundancy and disambiguation.
The clusteringbrings together a set of words but also builds aCCKG which shows the actual links (semanticrelations) between the members of the cluster.We suggest that those links are essential in an-alyzing and disambiguating texts.
When linksare redundant in a graph (that is we find twoidentical links between two compatible conceptsat each end) we are able to reduce semantic am-biguity relating to anaphora nd word sense.The counterpart to this, is that redundancy atthe concept level allows us to disambiguate hesemantic relations.To show our argument of the importance oflinks, we present an example.
Example 4.1shows a situation where an ambiguous wordchicken (sense 1 for the animal and sense 2for the meat) is used in a graph and needsto be disambiguated.
If two graphs stored ina LKB contain the word chicken in a disam-biguated form they can help solving the ambi-guity.
In Example 4.1, Graph 4.1 and Graph 4.2have two isolated concepts in common: eat andchicken.
Graph 4.1 and Graph 4.3 have thesame two concepts in common, but the addi-tion of a compatible relation, creating the com-mon subgraph \[eat\]->(object)->\[chicken\], makesthem more similar.
The different relations be-tween words have a large impact on the meaningof a sentence.
In Graph 4.1, the word chickencan be disambiguated to chicken_2.Example  4.1 -John eats chicken with a fork.Graph 4.1 -\[eat\]-> (agent)-> \[John\]->(with)->\[fork\]-> (object)- > \[chicken\]John's chicken eats grain.Graph 4.2 -\[eat\]-> (agent)-> \[chicken_l\] <-(poss) <-\[John\]-> (object)->\[grain\]108John likes to eat chicken at noon.Graph 4.3 -\[like\]-> (agent)-> \[John\]-> (goal)->\[eat\]-> (object)->\[chicken_2\]->(tirne)-> \[noon\]Only if we look at the relations between wordscan we understand how different each statementis.
It's all in the links... Of course those linksmight not be necessary at all levels of text anal-ysis.
If we try to cluster documents based onkeywords, well we don't  need to go to such adeep level of understanding.
But when we areanalyzing one text and trying to understand themeaning it conveys, we are probably within anarrow domain and the relations between wordstake all their importance.
For example, if weare trying to disambiguate the word baseball(the sport or the ball), both senses of the wordswill occur in the same context, therefore usingclusters of words that identify a context will notallow us to disambiguate between both senses.On the other hand, having a CCKG showingthe relations between the baseball_l (ball), thebat, the player and the baseball_2 (sport), willexpress the desired information.5 Conc lus ionWe presented the problem of semantic disam-biguation as solving ambiguities at the conceptlevel (word sense and anaphora) but also atthe link level (the relations between concepts).We showed that when gathering informationaround a particular subject via a clusteringmethod, we tend to cumulate similar facts ex-pressed in slightly different ways.
That redun-dancy is expressed by multiple copies of com-patible/identical concepts and relations in theresulting raph which is called a CCKG (Con-cept Clustering Knowledge Graph).
The re-dundancy within the links (relations) helps dis-ambiguate the concepts they connect and theredundancy within the concepts helps disam-biguate the links connecting them.
Clusteringhas been used a lot in previous research but onlyat the concept level; we propose that it is essen-tial to understand the links between the con-cepts in the cluster if we want to disambiguatebetween elements that share a similar contextof usage.ReferencesC.
Barri6re and D. Fass.
1998.
Dictionary vali-dation through a clustering technique.
To bepublished in the Proceedings of Euralex'98:Eight EURALEX International Congress onLexicography, Belgium, August 1998.C.
Barri~re and F. Popowich.
1996.
Conceptclustering and knowledge integration from achildren's dictionary.
In Proc.
o\] the 16 ~hCOLING, Copenhagen, Danemark, August.C.
Barri~re.
1997.
From a Children's First Dic-tionary to a Lexical Knowledge Base of Con-ceptual Graphs.
Ph.D. thesis, Simon FraserUniversity, June.C.
Barri~re.
1998.
The relation hierarchy:one key to representing natural languageusing conceptual graphs.
Submitted atICCS98: International Conference on Con-ceptual Structures, to be held in Montpellier,France, August 1998.P.
Brown, V.J.
Della Pietra, P.V.
deSouza, J.C.Lai, and R.L.
Mercer.
1992.
Class-based 11-gram models of natural anguage.
Computa-tional Linguistics, 18(4):467-480.H.
S. Delugach.
1993.
An exploration intosemantic distance.
In H. D. Pfeiffer andT.
E. Nagle, editors, Conceptual Structures:Theory and Implementation, pages 119-124.Springer, Berlin, Heidelberg.N.
Foo, B.J.
Garner, A. Rao, and E. Tsui.
1992.Semantic distance in conceptual graphs.
InT.E.
Nagle, J.A.
Nagle, L.L.
Gerholz, andP.W.Eklund, editors, Conceptual Structures:Current Research and Practice, chapter 7,pages 149-154.
Ellis Horwood.F.
Pereira, N. Tishby, and L. Lee.
1995.
Distri-butional clustering of english words.
In Proc.of the 33 th A CL, Cambridge,MA.Philip Resnik.
1995.
Using information contentto evaluate semantic similarity in a taxonomy.In Proc.
o\] the 1.~ th IJCAI, volume 1, pages448-453, Montreal, Canada.J.
Sowa.
1984.
Conceptual Structures in Mindand Machines.
Addison-Wesley.Y.
Wilks, D. Fass, G-M Guo, J. McDonald,T.
Plate, and B. Slator.
1989.
A tractablemachine dictionary as a resource for computa-tional semantics.
In Bran Boguraev and TedBriscoe, editors, Computational Lexicographyfor Natural Language Processing, chapter 9,pages 193-231.
Longman Group UK Limited.109
