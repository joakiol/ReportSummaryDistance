Proceedings of the 6th Workshop on Statistical Machine Translation, pages 379?385,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsThe Karlsruhe Institute of Technology Translation Systemsfor the WMT 2011Teresa Herrmann, Mohammed Mediani, Jan Niehues and Alex WaibelKarlsruhe Institute of TechnologyKarlsruhe, Germanyfirstname.lastname@kit.eduAbstractThis paper describes the phrase-based SMTsystems developed for our participationin the WMT11 Shared Translation Task.Translations for English?German andEnglish?French were generated using aphrase-based translation system which isextended by additional models such asbilingual and fine-grained POS languagemodels, POS-based reordering, lattice phraseextraction and discriminative word alignment.Furthermore, we present a special filteringmethod for the English-French Giga corpusand the phrase scoring step in the training isparallelized.1 IntroductionIn this paper we describe our systems for theEMNLP 2011 Sixth Workshop on Statistical Ma-chine Translation.
We participated in the SharedTranslation Task and submitted translations forEnglish?German and English?French.
We use aphrase-based decoder that can use lattices as inputand developed several models that extend the stan-dard log-linear model combination of phrase-basedMT.
These include advanced reordering models andcorresponding adaptations to the phrase extractionprocess as well as extension to the translation andlanguage model in form of discriminative wordalignment and a bilingual language model to ex-tend source word context.
For English-German, lan-guage models based on fine-grained part-of-speechtags were used to address the difficult target lan-guage generation due to the rich morphology of Ger-man.We also present a filtering method directly ad-dressing the problems of web-crawled corpora,which enabled us to make use of the French-EnglishGiga corpus.
Another novelty in our systems thisyear is the parallel phrase scoring method that re-duces the time needed for training which is espe-cially convenient for such big corpora as the Gigacorpus.2 System DescriptionThe baseline systems for all languages use a trans-lation model that is trained on EPPS and the NewsCommentary corpus and the phrase table is basedon a GIZA++ word alignment.
The language modelwas trained on the monolingual parts of the samecorpora by the SRILM Toolkit (Stolcke, 2002).
Itis a 4-gram SRI language model using Kneser-Neysmoothing.The problem of word reordering is addressed us-ing the POS-based reordering model as describedin Section 2.4.
The part-of-speech tags for the re-ordering model are obtained using the TreeTagger(Schmid, 1994).An in-house phrase-based decoder (Vogel, 2003)is used to perform translation and optimization withregard to the BLEU score is done using MinimumError Rate Training as described in Venugopal et al(2005).
During decoding only the top 20 translationoptions for every source phrase were considered.2.1 DataWe trained all systems using the parallel EPPS andNews Commentary corpora.
In addition, the UNcorpus and the Giga corpus were used for training379the French-English systems.Optimization was done for most languages usingthe news-test2008 data set and news-test2010 wasused as test set.
The only exception is German-English, where news-test2009 was used for opti-mization due to system combination arrangements.The language models for the baseline systems weretrained on the monolingual versions of the trainingcorpora.
Later on, we used the News Shuffle and theGigaword corpus to train bigger language models.For training a discriminative word alignment model,a small amount of hand-aligned data was used.2.2 PreprocessingThe training data is preprocessed prior to trainingthe system.
This includes normalizing special sym-bols, smart-casing the first words of each sentenceand removing long sentences and sentences withlength mismatch.For the German parts of the training corpuswe use the hunspell1 lexicon to map words writ-ten according to old German spelling to new Ger-man spelling, to obtain a corpus with homogenousspelling.Compound splitting as described in Koehn andKnight (2003) is applied to the German part of thecorpus for the German-to-English system to reducethe out-of-vocabulary problem for German com-pound words.2.3 Special filtering of the Giga parallel CorpusThe Giga corpus incorporates non-neglegibleamounts of noise even after our usual preprocess-ing.
This noise may be due to different causes.For instance: non-standard HTML characters,meaningless parts composed of only hypertextcodes, sentences which are only partial translationof the source, or eventually not a correct translationat all.Such noisy pairs potentially degrade the transla-tion model quality, therefore it seemed more conve-nient to eliminate them.Given the size of the corpus, this task could not beperformed manually.
Consequently, we used an au-tomatic classifier inspired by the work of Munteanuand Marcu (2005) on comparable corpora.
This clas-1http://hunspell.sourceforge.net/sifier should be able to filter out the pairs whichlikely are not beneficial for the translation model.In order to reliably decide about the classifier touse, we evaluated several techniques.
The trainingand test sets for this evaluation were built respec-tively from nc-dev2007 and nc-devtest2007.
In eachset, about 30% randomly selected source sentencesswitch positions with the immediate following sothat they form negative examples.
We also used lex-ical dictionaries in both directions based on EPPSand UN corpora.We relied on seven features in our classifiers:IBM1 score in both directions, number of unalignedsource words, the difference in number of words be-tween source and target, the maximum source wordfertility, number of unaligned target words, and themaximum target word fertility.
It is noteworthythat all the features requiring alignment information(such as the unaligned source words) were computedon the basis of the Viterbi path of the IBM1 align-ment.
The following classifiers were used:Regression Choose either class based on aweighted linear combination of the featuresand a fixed threshold of 0.5.Logistic regression The probability of the class isexpressed as a sigmoid of a linear combinationof the different features.
Then the class withthe highest probability is picked.Maximum entropy classifier We used the same setof features to train a maximum entropy classi-fier using the Megam package2.Support vector machines classifier An SVM clas-sifier was trained using the SVM-light pack-age3.Results of these experiments are summarized inTable 1.The regression weights were estimated so that tominimize the squared error.
This gave us a prettypoor F-measure score of 90.42%.
Given that the lo-gistic regression is more suited for binary classifica-tion in our case than the normal regression, it led tosignificant increase in the performance.
The training2http://www.cs.utah.edu/?hal/megam/3http://svmlight.joachims.org/380Approach Precision Recall F-measureRegression 93.81 87.27 90.42LogReg 93.43 94.84 94.13MaxEnt 93.69 94.54 94.11SVM 98.20 96.87 97.53Table 1: Results of the filtering experimentswas held by maximizing the likelihood to the datawith L2 regularization (with ?
= 0.1).
This gave anF-measure score of 94.78%.The maximum entropy classifier performed betterthan the logistic regression in terms of precision buthowever it had worse F-measure.Significant improvements could be noticed us-ing the SVM classifier in both precision and recall:98.20% precision, 96.87% recall, and thus 97.53%F-measure.As a result, we used the SVM classifier to filterthe Giga parallel corpus.
The corpus contained orig-inally around 22.52 million pairs.
After preprocess-ing and filtering it was reduced to 16.7 million pairs.Thus throwing around 6 million pairs.2.4 Word ReorderingIn contrast to modeling the reordering by a distance-based reordering model and/or a lexicalized distor-tion model, we use a different approach that relieson part-of-speech (POS) sequences.
By abstractingfrom surface words to parts-of-speech, we expect tomodel the reordering more accurately.2.4.1 POS-based Reordering ModelTo model reordering we first learn probabilisticrules from the POS tags of the words in the train-ing corpus and the alignment information.
Contin-uous reordering rules are extracted as described inRottmann and Vogel (2007) to model short-range re-orderings.
When translating between German andEnglish, we apply a modified reordering model withnon-continuous rules to cover also long-range re-orderings (Niehues and Kolss, 2009).
The reorder-ing rules are applied to the source text and the orig-inal order of words and the reordered sentence vari-ants generated by the rules are encoded in a wordlattice which is used as input to the decoder.2.4.2 Lattice Phrase ExtractionFor the test sentences, the POS-based reorderingallows us to change the word order in the source sen-tence so that the sentence can be translated more eas-ily.
If we apply this also to the training sentences, wewould be able to extract the phrase pairs for orig-inally discontinuous phrases and could apply themduring translation of reordered test sentences.Therefore, we build reordering lattices for alltraining sentences and then extract phrase pairs fromthe monotone source path as well as from the re-ordered paths.To limit the number of extracted phrase pairs, weextract a source phrase only once per sentence evenif it is found in different paths.2.5 Translation and Language ModelsIn addition to the models used in the baseline sys-tem described above we conducted experiments in-cluding additional models that enhance translationquality by introducing alternative or additional in-formation into the translation or language modellingprocess.2.5.1 Discriminative Word AlignmentIn most of our systems we use the PGIZA++Toolkit4 to generate alignments between words inthe training corpora.
The word alignments are gen-erated in both directions and the grow-diag-final-andheuristic is used to combine them.
The phrase ex-traction is then done based on this word alignment.In the English-German system we applied theDiscriminative Word Alignment approach as de-scribed in Niehues and Vogel (2008) instead.
Thisalignment model is trained on a small corpus ofhand-aligned data and uses the lexical probabilityas well as the fertilities generated by the PGIZA++Toolkit and POS information.2.5.2 Bilingual Language ModelIn phrase-based systems the source sentence issegmented by the decoder according to the best com-bination of phrases that maximize the translationand language model scores.
This segmentation intophrases leads to the loss of context information atthe phrase boundaries.
Although more target sidecontext is available to the language model, source4http://www.cs.cmu.edu/?qing/381side context would also be valuable for the decoderwhen searching for the best translation hypothesis.To make also source language context available weuse a bilingual language model, an additional lan-guage model in the phrase-based system in whicheach token consist of a target word and all sourcewords it is aligned to.
The bilingual tokens enterthe translation process as an additional target factorand the bilingual language model is applied to theadditional factor like a normal language model.
Formore details see (Niehues et al, 2011).2.5.3 Parallel phrase scoringThe process of phrase scoring is held in two runs.The objective of the first run is to compute the nec-essary counts and to estimate the scores, all basedon the source phrases; while the second run is sim-ilarly held based on the target phrases.
Thus, theextracted phrases have to be sorted twice: once bysource phrase and once by target phrase.
These twosorting operations are almost always done on an ex-ternal storage device and hence consume most of thetime spent in this step.The phrase scoring step was reimplemented in or-der to exploit the available computation resourcesmore efficiently and therefore reduce the process-ing time.
It uses optimized sorting algorithms forlarge data volumes which cannot fit into memory(Vitter, 2008).
In its core, our implementation re-lies on STXXL: an extension of the STL library forexternal memory (Kettner, 2005) and on OpenMPfor shared memory parallelization (Chapman et al,2007).Table 2 shows a comparison between Moses andour phrase scoring tools.
The comparison was heldusing sixteen-core 64-bit machines with 128 GbRAM, where the files are accessed through NFS ona RAID disk.
The experiments show that the gaingrows linearly with the size of input with an averageof 40% of speed up.2.5.4 POS Language ModelsIn addition to surface word language models, wedid experiments with language models based onpart-of-speech for English-German.
We expect thathaving additional information in form of probabil-ities of part-of-speech sequences should help espe-cially in case of the rich morphology of German and#pairs(G) Moses ?103(s) KIT ?103(s)0.203 25.99 17.581.444 184.19 103.411.693 230.97 132.79Table 2: Comparison of Moses and KIT phrase extractionsystemstherefore the more difficult target language genera-tion.The part-of-speeches were generated using theTreeTagger and the RFTagger (Schmid and Laws,2008), which produces more fine-grained tags thatinclude also person, gender and case information.While the TreeTagger assigns 54 different POS tagsto the 357K German words in the corpus, the RF-Tagger produces 756 different fine-grained tags onthe same corpus.We tried n-gram lengths of 4 and 7.
While no im-provement in translation quality could be achievedusing the POS language models based on the normalPOS tags, the 4-gram POS language model basedon fine-grained tags could improve the translationsystem by 0.2 BLEU points as shown in Table 3.Surprisingly, increasing the n-gram length to 7 de-creased the translation quality again.To investigate the impact of context length, weperformed an analysis on the outputs of two differentsystems, one without a POS language model and onewith the 4-gram fine-grained POS language model.For each of the translations we calculated the aver-age length of the n-grams in the translation whenapplying one of the two language models using 4-grams of surface words or parts-of-speech.
The re-sults are also shown in Table 3.The average n-gram length of surface words onthe translation generated by the system without POSlanguage model and the one using the 4-gram POSlanguage model stays practically the same.
Whenmeasuring the n-gram length using the 4-gram POSlanguage model, the context increases to 3.4.
Thisincrease of context is not surprising, since withthe more general POS tags longer contexts can bematched.
Comparing the POS context length forthe two translations, we can see that the context in-creases from 3.18 to 3.40 due to longer matchingPOS sequences.
This means that the system using382the POS language model actually generates trans-lations with more probable POS sequences so thatlonger matches are possible.
Also the perplexitydrops by half since the POS language model helpsconstructing sentences that have a better structure.System BLEU avg.
ngram length PPLWord POS POSno POS LM 16.64 2.77 3.18 66.78POS LM 16.88 2.81 3.40 33.36Table 3: Analysis of context length3 ResultsUsing the models described above we performedseveral experiments leading finally to the systemsused for generating the translations submitted to theworkshop.
The following sections describe the ex-periments for the individual language pairs and showthe translation results.
The results are reported ascase-sensitive BLEU scores (Papineni et al, 2002)on one reference translation.3.1 German-EnglishThe German-to-English baseline system appliesshort-range reordering rules and uses a languagemodel trained on the EPPS and News Commen-tary.
By exchanging the baseline language modelby one trained on the News Shuffle corpus we im-prove the translation quality considerably, by morethan 3 BLEU points.
When we expand the cov-erage of the reordering rules to enable long-rangereordering we can improve even further by 0.4 andadding a second language model trained on the En-glish Gigaword corpus we gain another 0.3 BLEUpoints.
To ensure that the phrase table also includesreordered phrases, we use lattice phrase extractionand can achieve a small improvement.
Finally, abilingual language model is added to extend the con-text of source language words available for transla-tion, reaching the best score of 23.35 BLEU points.This system was used for generating the translationsubmitted to the German-English Translation Task.3.2 English-GermanThe English-to-German baseline system also in-cludes short-range reordering and uses translationSystem Dev TestBaseline 18.49 19.10+ NewsShuffle LM 20.63 22.24+ LongRange Reordering 21.00 22.68+ Additional Giga LM 21.80 22.92+ Lattice Phrase Extraction 21.87 22.96+ Bilingual LM 22.05 23.35Table 4: Translation results for German-Englishand language model based on EPPS and News Com-mentary.
Exchanging the language model by theNews Shuffle language model again yields a big im-provement by 2.3 BLEU points.
Adding long-rangereordering improves a lot on the development setwhile the score on the test set remains practicallythe same.
Replacing the GIZA++ alignments byalignments generated using the Discriminative WordAlignment Model again only leads to a small im-provement.
By using the bilingual language modelto increase context we can gain 0.1 BLEU pointsand by adding the part-of-speech language modelwith rich parts-of-speech including case, numberand gender information for German we achieve thebest score of 16.88.
This system was used to gener-ate the translation used for submission.System Dev TestBaseline 13.55 14.19+ NewsShuffle LM 15.10 16.46+ LongRange Reordering 15.79 16.46+ DWA 15.81 16.52+ Bilingual LM 15.85 16.64+ POS LM 15.88 16.88Table 5: Translation results for English-German3.3 English-FrenchTable 6 summarizes how our system for English-French evolved.
The baseline system for this direc-tion was trained on the EPPS and News Commen-tary corpora, while the language model was trainedon the French part of the EPPS, News Commen-tary and UN parallel corpora.
Some improvementcould be already seen by introducing the short-rangereorderings trained on the baseline parallel corpus.383Apparently, the UN data brought only slight im-provement to the overall performance.
On the otherhand, adding bigger language models trained on themonolingual French version of EPPS, News Com-mentary and the News Shuffle together with theFrench Gigaword corpus introduces an improvementof 3.7 on test.
Using a system trained only on theGiga corpus data with the same last configurationshows a significant gain.
It showed an improvementof around 1.0.
We were able to obtain some furtherimprovements by merging the translation models ofthe last two systems.
i.e.
the one system based onEPPS, UN, and News Commentary and the other onthe Giga corpus.
This merging increased our scoreby 0.2.
Finally, our submitted system for this direc-tion was obtained by using a single language modeltrained on the union of all the French corpora in-stead of using multiple models.
This resulted in animprovement of 0.1 leading to our best score: 28.28.System Dev TestBaseline 20.62 22.36+ Reordering 21.29 23.11+ UN 21.27 23.24+ Big LMs 23.77 26.90Giga data 24.53 27.94Merge 24.74 28.14+ Merged LMs 25.07 28.28Table 6: Translation results for English-French3.4 French-EnglishThe development of our system for the French-English direction is summarized in Table 7.
Our sys-tem for this direction evolved quite similarly to theopposite direction.
The largest improvement accom-panied the integration of the bigger language mod-els (trained on the English version of EPPS, NewsCommentary, News Shuffle and the Gigaword cor-pus): 3.3 BLEU points, whereas smaller improve-ments could be gained by applying the short reorder-ing rules and almost no change by including the UNdata.
Further gains were obtained by training thesystem on the Giga corpus added to the previousparallel data.
This increased our performance by0.6.
The submitted system was obtained by aug-menting the last system with a bilingual languagemodel adding around 0.2 to the previous score andthus giving 28.34 as final score.System Dev TestBaseline 20.76 23.78+ Reordering 21.42 24.28+ UN 21.55 24.21+ Big LMs 24.16 27.55+ Giga data 24.86 28.17+ BiLM 25.01 28.34Table 7: Translation results for French-English4 ConclusionsWe have presented the systems for our participationin the WMT 2011 Evaluation for English?Germanand English?French.
For English?French, a spe-cial filtering method for web-crawled data was de-veloped.
In addition, a parallel phrase scoring tech-nique was implemented that could speed up the MTtraining process tremendously.
Using these two fea-tures, we were able to integrate the huge amounts ofdata available in the Giga corpus into our systemstranslating between English and French.We applied POS-based reordering to improve ourtranslations in all directions, using short-range re-ordering for English?French and long-range re-ordering for English?German.
For German-English, reordering also the training corpus lead tofurther improvements of the translation quality.A Discriminative Word Alignment Model led toan increase in BLEU for English-German.
For thisdirection we also tried fine-grained POS languagemodels of different n-gram lengths.
The best trans-lations could be obtained by using 4-grams.For nearly all experiments, a bilingual languagemodel was applied that expands the context ofsource words that can be considered during decod-ing.
The improvements range from 0.1 to 0.4 inBLEU score.AcknowledgmentsThis work was realized as part of the Quaero Pro-gramme, funded by OSEO, French State agency forinnovation.384ReferencesBarbara Chapman, Gabriele Jost, and Ruud van der Pas.2007.
Using OpenMP: Portable Shared Memory Par-allel Programming (Scientific and Engineering Com-putation).
The MIT Press.Roman Dementiev Lutz Kettner.
2005.
Stxxl: Stan-dard template library for xxl data sets.
In Proceedingsof ESA 2005.
Volume 3669 of LNCS, pages 640?651.Springer.Philipp Koehn and Kevin Knight.
2003.
Empirical Meth-ods for Compound Splitting.
In EACL, Budapest,Hungary.Dragos Stefan Munteanu and Daniel Marcu.
2005.
Im-proving machine translation performance by exploit-ing non-parallel corpora.
Computational Linguistics,31:477?504.Jan Niehues and Muntsin Kolss.
2009.
A POS-BasedModel for Long-Range Reorderings in SMT.
InFourth Workshop on Statistical Machine Translation(WMT 2009), Athens, Greece.Jan Niehues and Stephan Vogel.
2008.
DiscriminativeWord Alignment via Alignment Matrix Modeling.
InProc.
of Third ACL Workshop on Statistical MachineTranslation, Columbus, USA.Jan Niehues, Teresa Herrmann, Stephan Vogel, and AlexWaibel.
2011.
Wider Context by Using Bilingual Lan-guage Models in Machine Translation.
In Sixth Work-shop on Statistical Machine Translation (WMT 2011),Edinburgh, UK.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a Method for AutomaticEvaluation of Machine Translation.
Technical ReportRC22176 (W0109-022), IBM Research Division, T. J.Watson Research Center.Kay Rottmann and Stephan Vogel.
2007.
Word Reorder-ing in Statistical Machine Translation with a POS-Based Distortion Model.
In TMI, Sko?vde, Sweden.Helmut Schmid and Florian Laws.
2008.
Estimation ofConditional Probabilities with Decision Trees and anApplication to Fine-Grained POS Tagging.
In COL-ING 2008, Manchester, Great Britain.Helmut Schmid.
1994.
Probabilistic Part-of-Speech Tag-ging Using Decision Trees.
In International Con-ference on New Methods in Language Processing,Manchester, UK.Andreas Stolcke.
2002.
SRILM ?
An Extensible Lan-guage Modeling Toolkit.
In Proc.
of ICSLP, Denver,Colorado, USA.Ashish Venugopal, Andreas Zollman, and Alex Waibel.2005.
Training and Evaluation Error MinimizationRules for Statistical Machine Translation.
In Work-shop on Data-drive Machine Translation and Beyond(WPT-05), Ann Arbor, MI.Jeffrey Scott Vitter.
2008.
Algorithms and Data Struc-tures for External Memory.
now Publishers Inc.Stephan Vogel.
2003.
SMT Decoder Dissected: WordReordering.
In Int.
Conf.
on Natural Language Pro-cessing and Knowledge Engineering, Beijing, China.385
