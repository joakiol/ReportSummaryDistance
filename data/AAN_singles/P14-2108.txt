Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 662?667,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsThe Penn Parsed Corpus of Modern British English:First Parsing Results and AnalysisSeth KulickLinguistic Data ConsortiumUniversity of Pennsylvaniaskulick@ldc.upenn.eduAnthony Kroch and Beatrice SantoriniDept.
of LinguisticsUniversity of Pennsylvania{kroch,beatrice}@ling.upenn.eduAbstractThis paper presents the first results onparsing the Penn Parsed Corpus of Mod-ern British English (PPCMBE), a million-word historical treebank with an annota-tion style similar to that of the Penn Tree-bank (PTB).
We describe key features ofthe PPCMBE annotation style that differfrom the PTB, and present some exper-iments with tree transformations to bet-ter compare the results to the PTB.
Firststeps in parser analysis focus on problem-atic structures created by the parser.1 IntroductionWe present the first parsing results for thePenn Parsed Corpus of Modern British English(PPCMBE) (Kroch et al, 2010), showing that itcan be parsed at a few points lower in F-score thanthe Penn Treebank (PTB) (Marcus et al, 1999).We discuss some of the differences in annotationstyle and source material that make a direct com-parison problematic.
Some first steps at analysisof the parsing results indicate aspects of the anno-tation style that are difficult for the parser, and alsoshow that the parser is creating structures that arenot present in the training material.The PPCMBE is a million-word treebank cre-ated for researching changes in English syntax.
Itcovers the years 1700-1914 and is the most mod-ern in the series of treebanks created for histori-cal research.1Due to the historical nature of thePPCMBE, it shares some of the characteristics oftreebanks based on modern unedited text (Bies etal., 2012), such as spelling variation.1The other treebanks in the series cover Early Modern En-glish (Kroch et al, 2004) (1.8 million words), Middle Eng-lish (Kroch and Taylor, 2000) (1.2 million words), and EarlyEnglish Correspondence (Taylor et al, 2006) (2.2 millionwords).The size of the PPCMBE is roughly the sameas the WSJ section of the PTB, and its annotationstyle is similar to that of the PTB, but with dif-ferences, particularly with regard to coordinationand NP structure.
However, except for Lin et al(2012), we have found no discussion of this corpusin the literature.2There is also much additionalmaterial annotated in this style, increasing the im-portance of analyzing parser performance on thisannotation style.32 Corpus descriptionThe PPCMBE4consists of 101 files, but we leaveaside 7 files that consist of legal material with verydifferent properties than the rest of the corpus.The remaining 94 files contain 1,018,736 tokens(words).2.1 Part-of-speech tagsThe PPCMBE uses a part-of-speech (POS) tag setcontaining 248 POS tags, in contrast to the 45 tagsused by the PTB.
The more complex tag set ismainly due to the desire to tag orthographic vari-ants consistently throughout the series of historicalcorpora.
For example ?gentlemen?
and its ortho-graphic variant ?gen?l?men?
are tagged with thecomplex tag ADJ+NS (adjective and plural noun)on the grounds that in earlier time periods, the lex-ical item is spelled and tagged as two orthographicwords (?gentle?/ADJ and ?men?/NS).While only 81 of the 248 tags are ?simple?
(i.e.,not associated with lexical merging or splitting),2Lin et al (2012) report some results on POS tagging us-ing their own mapping to different tags, but no parsing results.3Aside from the corpora listed in fn.
1, there are alsohistorical corpora of Old English (Taylor et al, 2003), Ice-landic (Wallenberg et al, 2011), French (Martineau and oth-ers, 2009), and Portuguese (Galves and Faria, 2010), totaling4.5 million words.4We are working with a pre-release copy of the next re-vision of the official version.
Some annotation errors in thecurrently available version have been corrected, but the dif-ferences are relatively minor.662Type # Tags # Tokens % coverageSimple 81 1,005,243 98.7%Complex 167 13,493 1.3%Total 248 1,018,736 100.0%Table 1: Distribution of POS tags.
Complex tagsindicate lexical merging or splitting.
(1) (a) NPNPa HamCONJPand NPa Hare(b) NPNPa Hamand NPa HareFigure 1: Coordination in the PPCMBE (1a) andthe PTB (1b).they cover the vast majority of the words in thecorpus, as summarized in Table 1.
Of these 81tags, some are more specialized than in the PTB,accounting for the increased number of tags com-pared to the PTB.
For instance, for historical con-sistency, words like ?one?
and ?else?
each havetheir own tag.2.2 Syntactic annotationAs mentioned above, the syntactic annotationguidelines do not differ radically from those of thePTB.
There are some important differences, how-ever, which we highlight in the following threesubsections.2.2.1 CoordinationA coordinating conjunction and conjunct form aCONJP, as shown in (1a) in Figure 1.
(1b) showsthe corresponding annotation in the PTB.In a conjoined NP, if part of a first conjunctpotentially scopes over two or more conjuncts(shared pre-modifiers), the first conjunct has nophrasal node in the PPCMBE, and the label of the(2) (a) NPtheir husbands CONJPor NXfathers(b) NPtheir husbands or fathersFigure 2: (2a) is an example of coordination witha shared pre-modifier in the PPCMBE, and (2b)shows the corresponding annotation in the PTB.
(3) (a) NPThe back PPof this Spider(b) NPNPa teacherPPof chemistry(4) (a) NPThe Spiders CP-RELwhich have..(b) NPa conviction CP-THTthat..Figure 3: (3a) shows that a PP is sister to thenoun in the PPCMBE, in contrast to the adjunctionstructure in the PTB (3b).
(4ab) show that clausalcomplements and modifiers of a noun are distin-guished by function tags, rather than structurallyas in the PTB, which would adjoin the CP in (a),but not in (b).subsequent conjuncts becomes NX instead of NP,as shown in (2a) in Figure 2.
The correspondingPTB annotation is flat, as in (2b).52.2.2 Noun Phrase structureNeither the PPCMBE nor the PTB distinguish be-tween PP complements and modifiers of nouns.However, the PPCMBE annotates both types ofdependents as sisters of the noun, while the PTBadjoins both types.
For instance in (3a) in Fig-ure 3, the modifier PP is a sister to the noun inthe PPCMBE, while in (3b), the complement PPis adjoined in the PTB.Clausal complements and modifiers are alsoboth treated as sisters to the noun in the PPCMBE.In this case, though, the complement/modifier dis-tinction is encoded by a function tag.
For exam-ple, in (4a) and (4b), the status of the CPs as mod-ifier and complement is indicated by their func-tion tags: REL for relative clause and THT ?that?complement.
In the PTB, the distinction would beencoded structurally; the relative clause would beadjoined, whereas the ?that?
complement wouldnot.2.2.3 Clausal structureThe major difference in the clausal structure ascompared to the PTB is the absence of a VP level6,yielding flatter trees than in the PTB.
An exampleclause is shown in (5) in Figure 4.5Similar coordination structures exist for categories otherthan NP, although NP is by far the most common.6This is due to the changing headedness of VP in the over-all series of English historical corpora.663(5) IPNP-SBJThe poor fellowwas shot PPwith NPthree ArrowsFigure 4: An example of clausal structure, withoutVP.
(6) (a) NPNPThe backPPof this Spider(b)NPNPThe SpidersCP-RELwhich have..Figure 5: (6a) shows how (3a) is transformed inthe ?reduced +NPs?
version to include a level ofNP recursion, and (6b) shows the same for (4a).3 Corpus transformationsWe refer to the pre-release version of the corpusdescribed in Section 2 as the ?Release?
version,and experiment with three other corpus versions.3.1 ReducedAs mentioned earlier, the PPCMBE?s relativelylarge POS tag set aims to maximize annotationconsistency across the entire time period coveredby the historical corpora, beginning with MiddleEnglish.
Since we are concerned here with pars-ing just the PPCMBE, we simplified the tag set.The complex tags are simplified in a fully deter-ministic way, based on the trees and the tags.
Forexample, the POS tag for ?gentleman?, originallyADJ+N is changed to N. The P tag is split, so thatit is either left as P, if a preposition, or changedto CONJS, if a subordinating conjunction.
The re-duced tag set contains 76 tags.
We call the versionof the corpus with the reduced tag set the ?Re-duced?
version.3.2 Reduced+NPsAs discussed in Section 2.2.2, noun modifiers aresisters to the noun, instead of being adjoined, as inthe PTB.
As a result, there are fewer NP bracketsin the PPCMBE than there would be if the PTB-style were followed.
To evaluate the effect of thedifference in annotation guidelines on the parsingscore, we added PTB-style NP brackets to the re-duced corpus described in Section 3.1.
For ex-ample, (3a) in Figure 3 is transformed into (6a)Section # Files Token count %Train 81 890,150 87.4%Val 4 38,670 3.8%Dev 4 39,527 3.9%Test 5 50,389 4.9%Total 94 1,018,736 100.0%Table 2: Token count and data split for PPCMBEin Figure 5, and likewise (4a) is transformed into(6b).
However, (4b) remains as it is, because thefollowing CP in that case is a complement, as in-dicated by the THT function tag.
This is a signif-icant transformation of the corpus, adding 43,884NPs to the already-existing 291,422.3.3 Reduced+NPs+VPsWe carry out a similar transformation to add VPnodes to the IPs in the Reduced+NPs version,making them more like the clausal structures inthe PTB.
This added 169,877 VP nodes to the cor-pus (there are 131,671 IP nodes, some of whichcontain more than one auxiliary verb).It is worth emphasizing that the brackets addedin Sections 3.2 and 3.3 add no information, sincethey are added automatically.
They are added onlyto roughly compensate for the difference in anno-tation styles between the PPCMBE and the PTB.4 Data splitWe split the data into four sections, as shown inTable 2.
The validation section consists of the fourfiles beginning with ?a?
or ?v?
(spanning the years1711-1860), the development section consists ofthe four files beginning with ?l?
(1753-1866), thetest section consists of the five files beginning with?f?
(1749-1900), and the training section consistsof the remaining 81 files (1712-1913).
The datasplit sizes used here for the PPCMBE closely ap-proximate that used for the PTB, as described inPetrov et al (2006).7For this first work, we useda split that was roughly the same as far as time-spans across the four sections.
In future work, wewill do a more proper cross-validation evaluation.Table 3 shows the average sentence length andpercentage of sentences of length <= 40 in thePPCMBE and PTB.
The PPCMBE sentences area bit longer on average, and fewer are of length<= 40.
However, the match is close enough that7Sections 2-21 for Training Section 1 for Val, 22 for Devand 23 for Test.664Gold Tags Parser Tagsall <=40 all <=40Corpus Prec Rec F Prec Rec F Prec Rec F Prec Rec F Tags1 Rl/Dev 83.7 83.7 83.7 86.3 86.4 86.3 83.8 83.1 83.4 86.2 85.8 86.0 96.92 Rd/Dev 84.9 84.5 84.7 86.6 86.7 86.7 84.5 83.7 84.1 86.5 86.2 86.3 96.93 Rd/Tst 85.8 85.2 85.5 87.9 87.3 87.6 84.8 83.9 84.3 86.7 85.8 86.2 97.14 RdNPs/Dev 87.1 86.3 86.7 88.9 88.5 88.7 86.3 85.1 85.7 88.4 87.6 88.0 96.95 RdNPsVPs/Dev 87.2 87.0 87.1 89.5 89.4 89.5 86.3 85.7 86.0 88.6 88.2 88.4 97.06 PTB/23 90.3 89.8 90.1 90.9 90.4 90.6 90.0 89.5 89.8 90.6 90.1 90.3 96.9Table 4: Parsing results with Berkeley Parser.
The corpus versions used are Release (Rl), Reduced (Rd),Reduced+NPs (RdNPs), and Reduced+NPs+VPs (RdNPsVPs).
Results are shown for the parser forcedto use the gold POS tags from the corpus, and with the parser supplying its own tags.
For the latter case,the tagging accuracy is shown in the last column.Corpus Section Avg.
len % <= 40PPCMBE Dev 24.1 85.5Test 21.2 89.9PTB Dev 23.6 92.9Test 23.5 91.3Table 3: Average sentence length and percentageof sentences of length <=40 in the PPCMBE andPTB.we will report the parsing results for sentences oflength <= 40 and all sentences, as with the PTB.5 Parsing ExperimentsThe PPCMBE is a phrase-structure corpus, and sowe parse with the Berkeley parser (Petrov et al,2008) and score using the standard evalb program(Sekine and Collins, 2008).
We used the Train andVal sections for training, with the parser using theVal section for fine-tuning parameters (Petrov etal., 2006).
Since the Berkeley parser is capableof doing its own POS tagging, we ran it using thegold tags or supplying its own tags.
Table 4 showsthe results for both modes.8Consider first the results for the Dev sectionwith the parser using the gold tags.
The scorefor all sentences increases from 83.7 for the Re-lease corpus (row 1) to 84.7 for the Reduced cor-pus (row 2), reflecting the POS tag simplificationsin the Reduced corpus.
The score goes up by a fur-ther 2.0 to 86.7 (row 2 to 4) for the Reduced+NPscorpus and up again by 0.4 to 87.1 (row 5) forthe Reduced+NPs+VPs corpus, showing the ef-8We modified the evalb parameter file to exclude punctu-ation in PPCMBE, just as for PTB.
The results are based on asingle run for each corpus/section.
We expect some varianceto occur, and in future work will average results over severalruns of the training/Dev cycle, following Petrov et al (2006).fects of the extra NP and VP brackets.
We evalu-ated the Test section on the Reduced corpus (row3), with a result 0.8 higher than the Dev (85.5 inrow 3 compared to 84.7 in row 2).
The score forsentences of length <= 40 (a larger percentageof the PPCMBE than the PTB) is 2.4 higher thanthe score for all sentences, with both the gold andparser tags (row 5).The results with the parser choosing its ownPOS tags naturally go down, with the Test sectionsuffering more.
In general, the PPCMBE is af-fected by the lack of gold tags more than the PTB.In sum, the parser results show that thePPCMBE can be parsed at a level approaching thatof the PTB.
We are not proposing that the currentversion be replaced by the Reduced+NPs+VPsversion, on the grounds that the latter gets thehighest score.
Our goal was to determine whetherthe parsing results fell in the same general rangeas for the PTB by roughly compensating for thedifference in annotation style.
The results in Table4 show that this is the case.As a final note, the PPCMBE consists ofunedited data spanning more than 200 years, whilethe PTB is edited newswire, and so to some extentthere would almost certainly be some difference inscore.6 Parser AnalysisWe are currently developing techniques to betterunderstand the types of errors is making, whichhave already led to interesting results.
The parseris creating some odd structures that violate basicwell-formedness conditions of clauses.
Tree (7a)in Figure 6 is a tree from from the ?Reduced?
cor-pus, in which the verb ?formed?
projects to IP,665(7) (a) IP-SUBNP-SBJthe earth?s crusthad been formed PPby NPcauses RRCADVP-TMPnowacting(b) IPNPthe earth?s crusthad been formed PPby NPcausesADVPnowacting(8) (a) VPwould VPbe VPteaching NPthe doctrine(b) VPwould VPbe IPVPteaching NPthe doctrine(9) IPIt VPis IP-INFVPto VPbe VPobservedFigure 6: Examples of issues with parser outputwith two auxiliary verbs (?had?
and ?been?).
Inthe corresponding parser output (7b), the parsermisses the reduced relative RRC, turning ?acting?into the rightmost verb in the IP.
The parser is cre-ating an IP with two main verbs - an ungrammati-cal structure that is not attested in the gold.It might be thought that the parser is havingtrouble with the flat-IP annotation style, but theparser posits incorrect structures that are not at-tested in the gold even in the Reduced+NPs+VPsversion of the corpus.
Tree (8a) shows a fragmentof a gold tree from the corpus, with the VPs ap-propriately inserted.
The parser output (8b) hasan extra IP above ?teaching?.
The POS tags for?be?
(BE) and ?teaching?
(VAG) do not appear inthis configuration at all in the training material.
Ingeneral, the parser seems to be getting confusedas to when such an IP should appear.
We hypoth-esized that this is due to confusion with infiniti-val clauses, which can have an unary-branching IPover a VP, as in the gold tree (9).
We retrained theparser, directing it to retain the INF function tagthat appears in infinitival clauses as in (9).
Over-all, the evalb score went down slightly, but it didfix cases such as (8b).
We do not yet know why theoverall score went down, but what?s surprising isone would have thought that IP-INF is recoverablefrom the absence of a tensed verb.Preliminary analysis shows that the CONJPstructures are also difficult for the parser.
Sincethese are structures that are different than thePTB9, we were particularly interested in them.Cases where the CONJP is missing an overt co-ordinating cord (such as ?and?
), are particularlydifficult, not surprisingly.
These can appear as in-termediate conjuncts in a string of conjuncts, withthe structure (CONJP word).
The shared pre-modifier structure described in (2a) is also difficultfor the parser.7 ConclusionWe have presented the first results on parsing thePPCMBE and discussed some significant annota-tion style differences from the PTB.
Adjusting fortwo major differences that are a matter of anno-tation convention, we showed that the PPCMBEcan be parsed at approximately the same level ofaccuracy as the PTB.
The first steps in an inves-tigation of the parser differences show that theparser is generating structures that violate basicwell-formedness conditions of the annotation.For future work, we will carry out a more se-rious analysis of the parser output, trying to moreproperly account for the differences in bracketingstructure between the PPCMBE and PTB.
Thereis also a great deal of data annotated in the styleof the PPCMBE, as indicated in footnotes 1 and3, and we are interested in how the parser per-forms on these, especially comparing the resultson the modern English corpora to the older histor-ical ones, which will have greater issues of ortho-graphic and tokenization complications.AcknowledgmentsThis work was supported by National ScienceFoundation Grant # BCS-114749.
We would liketo thank Ann Bies, Justin Mott, and Mark Liber-man for helpful discussions.9The CONJP nonterminal in the PTB serves a differentpurpose than in the PPCMBE and is much more limited.666ReferencesAnn Bies, Justin Mott, Colin Warner, and Seth Kulick.2012.
English Web Treebank.
LDC2012T13.
Lin-guistic Data Consortium.Charlotte Galves and Pabol Faria.
2010.
TychoBrahe Parsed Corpus of Historical Portuguese.http://www.tycho.iel.unicamp.br/?tycho/corpus/en/index.html.Anthony Kroch and Ann Taylor.
2000.
Penn-Helsinki Parsed Corpus of Middle English, secondedition.
http://www.ling.upenn.edu/hist-corpora/PPCME2-RELEASE-3/index.html.Anthony Kroch, Beatrice Santorini, and ArielDiertani.
2004.
Penn-Helsinki Parsed Cor-pus of Early Modern English.
http://www.ling.upenn.edu/hist-corpora/PPCEME-RELEASE-2/index.html.Anthony Kroch, Beatrice Santorini, and Ariel Dier-tani.
2010.
Penn Parsed Corpus of ModernBritish English.
http://www.ling.upenn.edu/hist-corpora/PPCMBE-RELEASE-1/index.html.Yuri Lin, Jean-Baptiste Michel, Erez Aiden Lieberman,Jon Orwant, Will Brockman, and Slav Petrov.
2012.Syntactic annotations for the google books ngramcorpus.
In Proceedings of the ACL 2012 SystemDemonstrations, pages 169?174, Jeju Island, Korea,July.
Association for Computational Linguistics.Mitchell P. Marcus, Beatrice Santorini, Mary AnnMarcinkiewicz, and Ann Taylor.
1999.
Treebank-3.LDC99T42, Linguistic Data Consortium, Philadel-phia.France Martineau et al 2009.
Mod?eliser le change-ment: les voies du franc?ais, a Parsed Corpus of His-torical French.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, andinterpretable tree annotation.
In Proceedings ofCOLING-ACL, pages 433?440, Sydney, Australia,July.
Association for Computational Linguistics.Slav Petrov, Leon Barrett, Romain Thibaux, andDan Klein.
2008.
The Berkeley Parser.https://code.google.com/p/berkeleyparser/.Satoshi Sekine and Michael Collins.
2008.
Evalb.http://nlp.cs.nyu.edu/evalb/.Ann Taylor, Anthony Warner, Susan Pintzuk,and Frank Beths.
2003.
The York-Toronto-Helsinki Parsed Corpus of Old English Prose.Distributed through the Oxford Text Archive.http://www-users.york.ac.uk/?lang22/YCOE/YcoeHome.htm.Ann Taylor, Arja Nurmi, Anthony Warner, SusanPintzuk, and Terttu Nevalainen.
2006.
ParsedCorpus of Early English Correspondence.
Com-piled by the CEEC Project Team.
York: Uni-versity of York and Helsinki: University ofHelsinki.
Distributed through the Oxford TextArchive.
http://www-users.york.ac.uk/?lang22/PCEEC-manual/index.htm.Joel Wallenberg, Anton Karl Ingason, Einar FreyrSigursson, and Eirkur Rgnvaldsson.
2011.Icelandic Parsed Historical Corpus (IcePaHC)version 0.4. http://www.linguist.is/icelandic_treebank.667
