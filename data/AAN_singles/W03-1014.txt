Learning Extraction Patterns for Subjective Expressions?Ellen RiloffSchool of ComputingUniversity of UtahSalt Lake City, UT 84112riloff@cs.utah.eduJanyce WiebeDepartment of Computer ScienceUniversity of PittsburghPittsburgh, PA 15260wiebe@cs.pitt.eduAbstractThis paper presents a bootstrapping processthat learns linguistically rich extraction pat-terns for subjective (opinionated) expressions.High-precision classifiers label unannotateddata to automatically create a large training set,which is then given to an extraction patternlearning algorithm.
The learned patterns arethen used to identify more subjective sentences.The bootstrapping process learns many subjec-tive patterns and increases recall while main-taining high precision.1 IntroductionMany natural language processing applications couldbenefit from being able to distinguish between factualand subjective information.
Subjective remarks comein a variety of forms, including opinions, rants, allega-tions, accusations, suspicions, and speculations.
Ideally,information extraction systems should be able to distin-guish between factual information (which should be ex-tracted) and non-factual information (which should bediscarded or labeled as uncertain).
Question answeringsystems should distinguish between factual and specula-tive answers.
Multi-perspective question answering aimsto present multiple answers to the user based upon specu-lation or opinions derived from different sources.
Multi-document summarization systems need to summarize dif-ferent opinions and perspectives.
Spam filtering systems?This work was supported by the National Science Founda-tion under grants IIS-0208798, IIS-0208985, and IRI-9704240.The data preparation was performed in support of the North-east Regional Research Center (NRRC) which is sponsored bythe Advanced Research and Development Activity (ARDA), aU.S.
Government entity which sponsors and promotes researchof import to the Intelligence Community which includes but isnot limited to the CIA, DIA, NSA, NIMA, and NRO.must recognize rants and emotional tirades, among otherthings.
In general, nearly any system that seeks to iden-tify information could benefit from being able to separatefactual and subjective information.Some existing resources contain lists of subjectivewords (e.g., Levin?s desire verbs (1993)), and some em-pirical methods in NLP have automatically identified ad-jectives, verbs, and N-grams that are statistically associ-ated with subjective language (e.g., (Turney, 2002; Hatzi-vassiloglou and McKeown, 1997; Wiebe, 2000; Wiebeet al, 2001)).
However, subjective language can be ex-hibited by a staggering variety of words and phrases.
Inaddition, many subjective terms occur infrequently, suchas strongly subjective adjectives (e.g., preposterous, un-seemly) and metaphorical or idiomatic phrases (e.g., dealta blow, swept off one?s feet).
Consequently, we believethat subjectivity learning systems must be trained on ex-tremely large text collections before they will acquire asubjective vocabulary that is truly broad and comprehen-sive in scope.To address this issue, we have been exploring the useof bootstrapping methods to allow subjectivity classifiersto learn from a collection of unannotated texts.
Our re-search uses high-precision subjectivity classifiers to au-tomatically identify subjective and objective sentences inunannotated texts.
This process allows us to generate alarge set of labeled sentences automatically.
The sec-ond emphasis of our research is using extraction patternsto represent subjective expressions.
These patterns arelinguistically richer and more flexible than single wordsor N-grams.
Using the (automatically) labeled sentencesas training data, we apply an extraction pattern learningalgorithm to automatically generate patterns represent-ing subjective expressions.
The learned patterns can beused to automatically identify more subjective sentences,which grows the training set, and the entire process canthen be bootstrapped.
Our experimental results show thatthis bootstrapping process increases the recall of the high-precision subjective sentence classifier with little loss inprecision.
We also find that the learned extraction pat-terns capture subtle connotations that are more expressivethan the individual words by themselves.This paper is organized as follows.
Section 2 discussesprevious work on subjectivity analysis and extraction pat-tern learning.
Section 3 overviews our general approach,describes the high-precision subjectivity classifiers, andexplains the algorithm for learning extraction patterns as-sociated with subjectivity.
Section 4 describes the datathat we use, presents our experimental results, and showsexamples of patterns that are learned.
Finally, Section 5summarizes our findings and conclusions.2 Background2.1 Subjectivity AnalysisMuch previous work on subjectivity recognition has fo-cused on document-level classification.
For example,(Spertus, 1997) developed a system to identify inflamma-tory texts and (Turney, 2002; Pang et al, 2002) developedmethods for classifying reviews as positive or negative.Some research in genre classification has included therecognition of subjective genres such as editorials (e.g.,(Karlgren and Cutting, 1994; Kessler et al, 1997; Wiebeet al, 2001)).In contrast, the goal of our work is to classify individ-ual sentences as subjective or objective.
Document-levelclassification can distinguish between ?subjective texts?,such as editorials and reviews, and ?objective texts,?
suchas newspaper articles.
But in reality, most documentscontain a mix of both subjective and objective sentences.Subjective texts often include some factual information.For example, editorial articles frequently contain factualinformation to back up the arguments being made, andmovie reviews often mention the actors and plot of amovie as well as the theatres where it?s currently playing.Even if one is willing to discard subjective texts in theirentirety, the objective texts usually contain a great deal ofsubjective information in addition to facts.
For example,newspaper articles are generally considered to be rela-tively objective documents, but in a recent study (Wiebeet al, 2001) 44% of sentences in a news collection werefound to be subjective (after editorial and review articleswere removed).One of the main obstacles to producing a sentence-level subjectivity classifier is a lack of training data.
Totrain a document-level classifier, one can easily find col-lections of subjective texts, such as editorials and reviews.For example, (Pang et al, 2002) collected reviews froma movie database and rated them as positive, negative, orneutral based on the rating (e.g., number of stars) givenby the reviewer.
It is much harder to obtain collections ofindividual sentences that can be easily identified as sub-jective or objective.
Previous work on sentence-level sub-jectivity classification (Wiebe et al, 1999) used trainingcorpora that had been manually annotated for subjectiv-ity.
Manually producing annotations is time consuming,so the amount of available annotated sentence data is rel-atively small.The goal of our research is to use high-precision sub-jectivity classifiers to automatically identify subjectiveand objective sentences in unannotated text corpora.
Thehigh-precision classifiers label a sentence as subjective orobjective when they are confident about the classification,and they leave a sentence unlabeled otherwise.
Unanno-tated texts are easy to come by, so even if the classifierscan label only 30% of the sentences as subjective or ob-jective, they will still produce a large collection of labeledsentences.
Most importantly, the high-precision classi-fiers can generate a much larger set of labeled sentencesthan are currently available in manually created data sets.2.2 Extraction PatternsInformation extraction (IE) systems typically use lexico-syntactic patterns to identify relevant information.
Thespecific representation of these patterns varies across sys-tems, but most patterns represent role relationships sur-rounding noun and verb phrases.
For example, an IEsystem designed to extract information about hijackingsmight use the pattern hijacking of <x>, which looks forthe noun hijacking and extracts the object of the prepo-sition of as the hijacked vehicle.
The pattern <x> washijacked would extract the hijacked vehicle when it findsthe verb hijacked in the passive voice, and the pattern<x> hijacked would extract the hijacker when it findsthe verb hijacked in the active voice.One of our hypotheses was that extraction patternswould be able to represent subjective expressions thathave noncompositional meanings.
For example, considerthe common expression drives (someone) up the wall,which expresses the feeling of being annoyed with some-thing.
The meaning of this expression is quite differentfrom the meanings of its individual words (drives, up,wall).
Furthermore, this expression is not a fixed wordsequence that could easily be captured by N-grams.
It isa relatively flexible construction that may be more gener-ally represented as <x> drives <y> up the wall, where xand y may be arbitrary noun phrases.
This pattern wouldmatch many different sentences, such as ?George drivesme up the wall,?
?She drives the mayor up the wall,?or ?The nosy old man drives his quiet neighbors up thewall.
?We also wondered whether the extraction pattern rep-resentation might reveal slight variations of the same verbor noun phrase that have different connotations.
For ex-ample, you can say that a comedian bombed last night,which is a subjective statement, but you can?t expressthis sentiment with the passive voice of bombed.
In Sec-tion 3.2, we will show examples of extraction patternsrepresenting subjective expressions which do in fact ex-hibit both of these phenomena.A variety of algorithms have been developed to au-tomatically learn extraction patterns.
Most of thesealgorithms require special training resources, such astexts annotated with domain-specific tags (e.g., Au-toSlog (Riloff, 1993), CRYSTAL (Soderland et al,1995), RAPIER (Califf, 1998), SRV (Freitag, 1998),WHISK (Soderland, 1999)) or manually defined key-words, frames, or object recognizers (e.g., PALKA (Kimand Moldovan, 1993) and LIEP (Huffman, 1996)).AutoSlog-TS (Riloff, 1996) takes a different approach,requiring only a corpus of unannotated texts that havebeen separated into those that are related to the target do-main (the ?relevant?
texts) and those that are not (the ?ir-relevant?
texts).
Most recently, two bootstrapping algo-rithms have been used to learn extraction patterns.
Meta-bootstrapping (Riloff and Jones, 1999) learns both extrac-tion patterns and a semantic lexicon using unannotatedtexts and seed words as input.
ExDisco (Yangarber et al,2000) uses a bootstrapping mechanism to find new ex-traction patterns using unannotated texts and some seedpatterns as the initial input.For our research, we adopted a learning process verysimilar to that used by AutoSlog-TS, which requires onlyrelevant texts and irrelevant texts as its input.
We describethis learning process in more detail in the next section.3 Learning and Bootstrapping ExtractionPatterns for SubjectivityWe have developed a bootstrapping process for subjec-tivity classification that explores three ideas: (1) high-precision classifiers can be used to automatically iden-tify subjective and objective sentences from unannotatedtexts, (2) this data can be used as a training set to auto-matically learn extraction patterns associated with sub-jectivity, and (3) the learned patterns can be used to growthe training set, allowing this entire process to be boot-strapped.Figure 1 shows the components and layout of the boot-strapping process.
The process begins with a large collec-tion of unannotated text and two high precision subjec-tivity classifiers.
One classifier searches the unannotatedcorpus for sentences that can be labeled as subjectivewith high confidence, and the other classifier searchesfor sentences that can be labeled as objective with highconfidence.
All other sentences in the corpus are leftunlabeled.
The labeled sentences are then fed to an ex-traction pattern learner, which produces a set of extrac-tion patterns that are statistically correlated with the sub-jective sentences (we will call these the subjective pat-terns).
These patterns are then used to identify more sen-tences within the unannotated texts that can be classifiedas subjective.
The extraction pattern learner can then re-train using the larger training set and the process repeats.The subjective patterns can also be added to the high-precision subjective sentence classifier as new features toimprove its performance.
The dashed lines in Figure 1represent the parts of the process that are bootstrapped.In this section, we will describe the high-precision sen-tence classifiers, the extraction pattern learning process,and the details of the bootstrapping process.3.1 High-Precision Subjectivity ClassifiersThe high-precision classifiers (HP-Subj and HP-Obj) uselists of lexical items that have been shown in previouswork to be good subjectivity clues.
Most of the items aresingle words, some are N-grams, but none involve syntac-tic generalizations as in the extraction patterns.
Any dataused to develop this vocabulary does not overlap with thetest sets or the unannotated data used in this paper.Many of the subjective clues are from manually de-veloped resources, including entries from (Levin, 1993;Ballmer and Brennenstuhl, 1981), Framenet lemmas withframe element experiencer (Baker et al, 1998), adjec-tives manually annotated for polarity (Hatzivassiloglouand McKeown, 1997), and subjectivity clues listed in(Wiebe, 1990).
Others were derived from corpora, in-cluding subjective nouns learned from unannotated datausing bootstrapping (Riloff et al, 2003).The subjectivity clues are divided into those that arestrongly subjective and those that are weakly subjective,using a combination of manual review and empirical re-sults on a small training set of manually annotated data.As the terms are used here, a strongly subjective clue isone that is seldom used without a subjective meaning,whereas a weakly subjective clue is one that commonlyhas both subjective and objective uses.The high-precision subjective classifier classifies a sen-tence as subjective if it contains two or more of thestrongly subjective clues.
On a manually annotated testset, this classifier achieves 91.5% precision and 31.9%recall (that is, 91.5% of the sentences that it selected aresubjective, and it found 31.9% of the subjective sentencesin the test set).
This test set consists of 2197 sentences,59% of which are subjective.The high-precision objective classifier takes a differentapproach.
Rather than looking for the presence of lexicalitems, it looks for their absence.
It classifies a sentence asobjective if there are no strongly subjective clues and atmost one weakly subjective clue in the current, previous,and next sentence combined.
Why doesn?t the objectiveclassifier mirror the subjective classifier, and consult itsown list of strongly objective clues?
There are certainlylexical items that are statistically correlated with the ob-Known SubjectiveVocabularyHigh?Precision ObjectiveSentence Classifier (HP?Obj)High?Precision SubjectiveSentence Classifier (HP?Subj)Unannotated Text Collectionunlabeled sentencesunlabeled sentencesunlabeled sentencesPattern?based SubjectiveSentence ClassifierExtraction PatternLearnersubjectivesentencessubjective sentencesobjective sentencessubjective patternssubjective patternsFigure 1: Bootstrapping Processjective class (examples are cardinal numbers (Wiebe etal., 1999), and words such as per, case, market, and to-tal), but the presence of such clues does not readily leadto high precision objective classification.
Add sarcasmor a negative evaluation to a sentence about a dry topicsuch as stock prices, and the sentence becomes subjec-tive.
Conversely, add objective topics to a sentence con-taining two strongly subjective words such as odious andscumbag, and the sentence remains subjective.The performance of the high-precision objective classi-fier is a bit lower than the subjective classifier: 82.6% pre-cision and 16.4% recall on the test set mentioned above(that is, 82.6% of the sentences selected by the objectiveclassifier are objective, and the objective classifier found16.4% of the objective sentences in the test set).
Al-though there is room for improvement, the performanceproved to be good enough for our purposes.3.2 Learning Subjective Extraction PatternsTo automatically learn extraction patterns that are associ-ated with subjectivity, we use a learning algorithm similarto AutoSlog-TS (Riloff, 1996).
For training, AutoSlog-TS uses a text corpus consisting of two distinct sets oftexts: ?relevant?
texts (in our case, subjective sentences)and ?irrelevant?
texts (in our case, objective sentences).A set of syntactic templates represents the space of pos-sible extraction patterns.The learning process has two steps.
First, the syntac-tic templates are applied to the training corpus in an ex-haustive fashion, so that extraction patterns are generatedfor (literally) every possible instantiation of the templatesthat appears in the corpus.
The left column of Figure 2shows the syntactic templates used by AutoSlog-TS.
Theright column shows a specific extraction pattern that waslearned during our subjectivity experiments as an instan-tiation of the syntactic form on the left.
For example, thepattern <subj> was satisfied1 will match any sentencewhere the verb satisfied appears in the passive voice.
Thepattern <subj> dealt blow represents a more complex ex-pression that will match any sentence that contains a verbphrase with head=dealt followed by a direct object withhead=blow.
This would match sentences such as ?Theexperience dealt a stiff blow to his pride.?
It is importantto recognize that these patterns look for specific syntacticconstructions produced by a (shallow) parser, rather thanexact word sequences.SYNTACTIC FORM EXAMPLE PATTERN<subj> passive-verb <subj> was satisfied<subj> active-verb <subj> complained<subj> active-verb dobj <subj> dealt blow<subj> verb infinitive <subj> appear to be<subj> aux noun <subj> has positionactive-verb <dobj> endorsed <dobj>infinitive <dobj> to condemn <dobj>verb infinitive <dobj> get to know <dobj>noun aux <dobj> fact is <dobj>noun prep <np> opinion on <np>active-verb prep <np> agrees with <np>passive-verb prep <np> was worried about <np>infinitive prep <np> to resort to <np>Figure 2: Syntactic Templates and Examples of Patternsthat were Learned1This is a shorthand notation for the internal representation.PATTERN FREQ %SUBJ<subj> was asked 11 100%<subj> asked 128 63%<subj> is talk 5 100%talk of <np> 10 90%<subj> will talk 28 71%<subj> put an end 10 90%<subj> put 187 67%<subj> is going to be 11 82%<subj> is going 182 67%was expected from <np> 5 100%<subj> was expected 45 42%<subj> is fact 38 100%fact is <dobj> 12 100%Figure 3: Patterns with Interesting BehaviorThe second step of AutoSlog-TS?s learning process ap-plies all of the learned extraction patterns to the train-ing corpus and gathers statistics for how often eachpattern occurs in subjective versus objective sentences.AutoSlog-TS then ranks the extraction patterns using ametric called RlogF (Riloff, 1996) and asks a human toreview the ranked list and make the final decision aboutwhich patterns to keep.In contrast, for this work we wanted a fully automaticprocess that does not depend on a human reviewer, andwe were most interested in finding patterns that can iden-tify subjective expressions with high precision.
So weranked the extraction patterns using a conditional proba-bility measure: the probability that a sentence is subjec-tive given that a specific extraction pattern appears in it.The exact formula is:Pr(subjective | patterni) = subjfreq(patterni)freq(patterni)where subjfreq(patterni) is the frequency of patterniin subjective training sentences, and freq(patterni) isthe frequency of patterni in all training sentences.
(Thismay also be viewed as the precision of the pattern on thetraining data.)
Finally, we use two thresholds to select ex-traction patterns that are strongly associated with subjec-tivity in the training data.
We choose extraction patternsfor which freq(patterni) ?
?1 and Pr(subjective |patterni) ?
?2.Figure 3 shows some patterns learned by our system,the frequency with which they occur in the training data(FREQ) and the percentage of times they occur in sub-jective sentences (%SUBJ).
For example, the first tworows show the behavior of two similar expressions us-ing the verb asked.
100% of the sentences that containasked in the passive voice are subjective, but only 63%of the sentences that contain asked in the active voice aresubjective.
A human would probably not expect the ac-tive and passive voices to behave so differently.
To un-derstand why this is so, we looked in the training dataand found that the passive voice is often used to querysomeone about a specific opinion.
For example, here isone such sentence from our training set: ?Ernest Bai Ko-roma of RITCORP was asked to address his supporters onhis views relating to ?full blooded Temne to head APC?.
?In contrast, many of the sentences containing asked inthe active voice are more general in nature, such as ?Themayor asked a newly formed JR about his petition.
?Figure 3 also shows that expressions using talk as anoun (e.g., ?Fred is the talk of the town?)
are highly cor-related with subjective sentences, while talk as a verb(e.g., ?The mayor will talk about...?)
are found in a mixof subjective and objective sentences.
Not surprisingly,longer expressions tend to be more idiomatic (and sub-jective) than shorter expressions (e.g., put an end (to) vs.put; is going to be vs. is going; was expected from vs. wasexpected).
Finally, the last two rows of Figure 3 show thatexpressions involving the noun fact are highly correlatedwith subjective expressions!
These patterns match sen-tences such as The fact is... and ... is a fact, which appar-ently are often used in subjective contexts.
This exampleillustrates that the corpus-based learning method can findphrases that might not seem subjective to a person intu-itively, but that are reliable indicators of subjectivity.4 Experimental Results4.1 Subjectivity DataThe text collection that we used consists of English-language versions of foreign news documents from FBIS,the U.S. Foreign Broadcast Information Service.
Thedata is from a variety of countries.
Our system takesunannotated data as input, but we needed annotated datato evaluate its performance.
We briefly describe the man-ual annotation scheme used to create the gold-standard,and give interannotator agreement results.In 2002, a detailed annotation scheme (Wilson andWiebe, 2003) was developed for a government-sponsoredproject.
We only mention aspects of the annotationscheme relevant to this paper.
The scheme was inspiredby work in linguistics and literary theory on subjectiv-ity, which focuses on how opinions, emotions, etc.
areexpressed linguistically in context (Banfield, 1982).
Thegoal is to identify and characterize expressions of privatestates in a sentence.
Private state is a general coveringterm for opinions, evaluations, emotions, and specula-tions (Quirk et al, 1985).
For example, in sentence (1)the writer is expressing a negative evaluation.
(1) ?The time has come, gentlemen, for Sharon, the as-sassin, to realize that injustice cannot last long.
?Sentence (2) reflects the private state of Western coun-tries.
Mugabe?s use of overwhelmingly also reflects a pri-vate state, his positive reaction to and characterization ofhis victory.
(2) ?Western countries were left frustrated and impotentafter Robert Mugabe formally declared that he had over-whelmingly won Zimbabwe?s presidential election.
?Annotators are also asked to judge the strength of eachprivate state.
A private state may have low, medium, highor extreme strength.To allow us to measure interannotator agreement, threeannotators (who are not authors of this paper) indepen-dently annotated the same 13 documents with a total of210 sentences.
We begin with a strict measure of agree-ment at the sentence level by first considering whetherthe annotator marked any private-state expression, of anystrength, anywhere in the sentence.
If so, the sentence issubjective.
Otherwise, it is objective.
The average pair-wise percentage agreement is 90% and the average pair-wise ?
value is 0.77.One would expect that there are clear cases of objec-tive sentences, clear cases of subjective sentences, andborderline sentences in between.
The agreement studysupports this.
In terms of our annotations, we define asentence as borderline if it has at least one private-stateexpression identified by at least one annotator, and allstrength ratings of private-state expressions are low.
Onaverage, 11% of the corpus is borderline under this def-inition.
When those sentences are removed, the averagepairwise percentage agreement increases to 95% and theaverage pairwise ?
value increases to 0.89.As expected, the majority of disagreement cases in-volve low-strength subjectivity.
The annotators consis-tently agree about which are the clear cases of subjectivesentences.
This leads us to define the gold-standard thatwe use when evaluating our results.
A sentence is subjec-tive if it contains at least one private-state expression ofmedium or higher strength.
The second class, which wecall objective, consists of everything else.4.2 Evaluation of the Learned PatternsOur pool of unannotated texts consists of 302,163 indi-vidual sentences.
The HP-Subj classifier initially labeledroughly 44,300 of these sentences as subjective, and theHP-Obj classifier initially labeled roughly 17,000 sen-tences as objective.
In order to keep the training set rel-atively balanced, we used all 17,000 objective sentencesand 17,000 of the subjective sentences as training data forthe extraction pattern learner.17,073 extraction patterns were learned that havefrequency ?
2 and Pr(subjective | patterni) ?
.60 onthe training data.
We then wanted to determine whetherthe extraction patterns are, in fact, good indicators of sub-jectivity.
To evaluate the patterns, we applied differentsubsets of them to a test set to see if they consistently oc-cur in subjective sentences.
This test set consists of 3947Figure 4: Evaluating the Learned Patterns on Test Datasentences, 54% of which are subjective.Figure 4 shows sentence recall and pattern (instance-level) precision for the learned extraction patterns on thetest set.
In this figure, precision is the proportion of pat-tern instances found in the test set that are in subjectivesentences, and recall is the proportion of subjective sen-tences that contain at least one pattern instance.We evaluated 18 different subsets of the patterns, byselecting the patterns that pass certain thresholds in thetraining data.
We tried all combinations of ?1 = {2,10}and ?2 = {.60,.65,.70,.75,.80,.85,.90,.95,1.0}.
The datapoints corresponding to ?1=2 are shown on the upper linein Figure 4, and those corresponding to ?1=10 are shownon the lower line.
For example, the data point correspond-ing to ?1=10 and ?2=.90 evaluates only the extraction pat-terns that occur at least 10 times in the training data andwith a probability ?
.90 (i.e., at least 90% of its occur-rences are in subjective training sentences).Overall, the extraction patterns perform quite well.The precision ranges from 71% to 85%, with the expectedtradeoff between precision and recall.
This experimentconfirms that the extraction patterns are effective at rec-ognizing subjective expressions.4.3 Evaluation of the Bootstrapping ProcessIn our second experiment, we used the learned extrac-tion patterns to classify previously unlabeled sentencesfrom the unannotated text collection.
The new subjec-tive sentences were then fed back into the Extraction Pat-tern Learner to complete the bootstrapping cycle depictedby the rightmost dashed line in Figure 1.
The Pattern-based Subjective Sentence Classifier classifies a sentenceas subjective if it contains at least one extraction patternwith ?1?5 and ?2?1.0 on the training data.
This processproduced approximately 9,500 new subjective sentencesthat were previously unlabeled.Since our bootstrapping process does not learn new ob-jective sentences, we did not want to simply add the newsubjective sentences to the training set, or it would be-come increasingly skewed toward subjective sentences.Since HP-Obj had produced roughly 17,000 objectivesentences used for training, we used the 9,500 new sub-jective sentences along with 7,500 of the previously iden-tified subjective sentences as our new training set.
Inother words, the training set that we used during the sec-ond bootstrapping cycle contained exactly the same ob-jective sentences as the first cycle, half of the same sub-jective sentences as the first cycle, and 9,500 brand newsubjective sentences.On this second cycle of bootstrapping, the extractionpattern learner generated many new patterns that were notdiscovered during the first cycle.
4,248 new patterns werefound that have ?1?2 and ?2?.60.
If we consider only thestrongest (most subjective) extraction patterns, 308 newpatterns were found that had ?1?10 and ?2?1.0.
This isa substantial set of new extraction patterns that seem tobe very highly correlated with subjectivity.An open question was whether the new patterns pro-vide additional coverage.
To assess this, we did a sim-ple test: we added the 4,248 new patterns to the origi-nal set of patterns learned during the first bootstrappingcycle.
Then we repeated the same analysis that we de-pict in Figure 4.
In general, the recall numbers increasedby about 2-4% while the precision numbers decreased byless, from 0.5-2%.In our third experiment, we evaluated whether thelearned patterns can improve the coverage of the high-precision subjectivity classifier (HP-Subj), to completethe bootstrapping loop depicted in the top-most dashedline of Figure 1.
Our hope was that the patterns would al-low more sentences from the unannotated text collectionto be labeled as subjective, without a substantial drop inprecision.
For this experiment, we selected the learnedextraction patterns that had ?1?
10 and ?2?
1.0 on thetraining set, since these seemed likely to be the most reli-able (high precision) indicators of subjectivity.We modified the HP-Subj classifier to use extractionpatterns as follows.
All sentences labeled as subjectiveby the original HP-Subj classifier are also labeled as sub-jective by the new version.
For previously unlabeled sen-tences, the new version classifies a sentence as subjectiveif (1) it contains two or more of the learned patterns, or(2) it contains one of the clues used by the original HP-Subj classifier and at least one learned pattern.
Table 1shows the performance results on the test set mentionedin Section 3.1 (2197 sentences) for both the original HP-Subj classifier and the new version that uses the learnedextraction patterns.
The extraction patterns produce a 7.2percentage point gain in coverage, and only a 1.1 percent-age point drop in precision.
This result shows that thelearned extraction patterns do improve the performanceof the high-precision subjective sentence classifier, allow-ing it to classify more sentences as subjective with nearlythe same high reliability.HP-Subj HP-Subj w/PatternsRecall Precision Recall Precision32.9 91.3 40.1 90.2Table 1: Bootstrapping the Learned Patterns into theHigh-Precision Sentence ClassifierTable 2 gives examples of patterns used to augment theHP-Subj classifier which do not overlap in non-functionwords with any of the clues already known by the originalsystem.
For each pattern, we show an example sentencefrom our corpus that matches the pattern.5 ConclusionsThis research explored several avenues for improving thestate-of-the-art in subjectivity analysis.
First, we demon-strated that high-precision subjectivity classification canbe used to generate a large amount of labeled training datafor subsequent learning algorithms to exploit.
Second, weshowed that an extraction pattern learning technique canlearn subjective expressions that are linguistically richerthan individual words or fixed phrases.
We found thatsimilar expressions may behave very differently, so thatone expression may be strongly indicative of subjectivitybut the other may not.
Third, we augmented our origi-nal high-precision subjective classifier with these newlylearned extraction patterns.
This bootstrapping processresulted in substantially higher recall with a minimal lossin precision.
In future work, we plan to experiment withdifferent configurations of these classifiers, add new sub-jective language learners in the bootstrapping process,and address the problem of how to identify new objec-tive sentences during bootstrapping.6 AcknowledgmentsWe are very grateful to Theresa Wilson for her invaluableprogramming support and help with data preparation.ReferencesC.
Baker, C. Fillmore, and J. Lowe.
1998.
The BerkeleyFrameNet Project.
In Proceedings of the COLING-ACL-98.T.
Ballmer and W. Brennenstuhl.
1981.
Speech Act Classifi-cation: A Study in the Lexical Analysis of English SpeechActivity Verbs.
Springer-Verlag.A.
Banfield.
1982.
Unspeakable Sentences.
Routledge andKegan Paul, Boston.seems to be <dobj> I am pleased that there now seems to be broad political consensus .
.
.underlined <dobj> Jiang?s subdued tone .
.
.
underlined his desire to avoid disputes .
.
.pretext of <np> On the pretext of the US opposition .
.
.atmosphere of <np> Terrorism thrives in an atmosphere of hate .
.
.<subj> reflect These are fine words, but they do not reflect the reality .
.
.to satisfy <dobj> The pictures resemble an attempt to satisfy a primitive thirst for revenge .
.
.way with <np> .
.
.
to ever let China use force to have its way with .
.
.bring about <np> ?Everything must be done by everyone to bring about de-escalation,?
Mr Chirac added.expense of <np> at the expense of the world?s security and stabilityvoiced <dobj> Khatami .
.
.
voiced Iran?s displeasure.turn into <np> .
.
.
the surging epidemic could turn into ?a national security threat,?
he said.Table 2: Examples of Learned Patterns Used by HP-Subj and Sample Matching SentencesM.
E. Califf.
1998.
Relational Learning Techniques for NaturalLanguage Information Extraction.
Ph.D. thesis, Tech.
Rept.AI98-276, Artificial Intelligence Laboratory, The Universityof Texas at Austin.Dayne Freitag.
1998.
Toward General-Purpose Learning forInformation Extraction.
In Proceedings of the ACL-98.V.
Hatzivassiloglou and K. McKeown.
1997.
Predicting theSemantic Orientation of Adjectives.
In Proceedings of theACL-EACL-97.S.
Huffman.
1996.
Learning information extraction pat-terns from examples.
In Stefan Wermter, Ellen Riloff,and Gabriele Scheler, editors, Connectionist, Statistical, andSymbolic Approaches to Learning for Natural LanguageProcessing, pages 246?260.
Springer-Verlag, Berlin.J.
Karlgren and D. Cutting.
1994.
Recognizing Text Genreswith Simple Metrics Using Discriminant Analysis.
In Pro-ceedings of the COLING-94.B.
Kessler, G. Nunberg, and H. Schu?tze.
1997.
Automatic De-tection of Text Genre.
In Proceedings of the ACL-EACL-97.J.
Kim and D. Moldovan.
1993.
Acquisition of Semantic Pat-terns for Information Extraction from Corpora.
In Proceed-ings of the Ninth IEEE Conference on Artificial Intelligencefor Applications.Beth Levin.
1993.
English Verb Classes and Alternations: APreliminary Investigation.
University of Chicago Press.B.
Pang, L. Lee, and S. Vaithyanathan.
2002.
Thumbs up?
Sen-timent Classification Using Machine Learning Techniques.In Proceedings of the EMNLP-02.R.
Quirk, S. Greenbaum, G. Leech, and J. Svartvik.
1985.
AComprehensive Grammar of the English Language.
Long-man, New York.E.
Riloff and R. Jones.
1999.
Learning Dictionaries for In-formation Extraction by Multi-Level Bootstrapping.
In Pro-ceedings of the AAAI-99.E.
Riloff, J. Wiebe, and T. Wilson.
2003.
Learning SubjectiveNouns using Extraction Pattern Bootstrapping.
In Proceed-ings of the Seventh Conference on Computational NaturalLanguage Learning (CoNLL-03).E.
Riloff.
1993.
Automatically Constructing a Dictionary forInformation Extraction Tasks.
In Proceedings of the AAAI-93.E.
Riloff.
1996.
Automatically Generating Extraction Patternsfrom Untagged Text.
In Proceedings of the AAAI-96.S.
Soderland, D. Fisher, J. Aseltine, and W. Lehnert.
1995.CRYSTAL: Inducing a Conceptual Dictionary.
In Proceed-ings of the IJCAI-95.S.
Soderland.
1999.
Learning Information Extraction Rules forSemi-Structured and Free Text.
Machine Learning, 34(1-3):233?272.E.
Spertus.
1997.
Smokey: Automatic Recognition of HostileMessages.
In Proceedings of the IAAI-97.P.
Turney.
2002.
Thumbs Up or Thumbs Down?
Semantic Ori-entation Applied to Unsupervised Classification of Reviews.In Proceedings of the ACL-02.J.
Wiebe, R. Bruce, and T. O?Hara.
1999.
Development andUse of a Gold Standard Data Set for Subjectivity Classifica-tions.
In Proceedings of the ACL-99.J.
Wiebe, T. Wilson, and M. Bell.
2001.
Identifying Collo-cations for Recognizing Opinions.
In Proceedings of theACL-01 Workshop on Collocation: Computational Extrac-tion, Analysis, and Exploitation.J.
Wiebe.
1990.
Recognizing Subjective Sentences: A Compu-tational Investigation of Narrative Text.
Ph.D. thesis, StateUniversity of New York at Buffalo.J.
Wiebe.
2000.
Learning Subjective Adjectives from Corpora.In Proceedings of the AAAI-00.T.
Wilson and J. Wiebe.
2003.
Annotating Opinions in theWorld Press.
In Proceedings of the ACL SIGDIAL-03.R.
Yangarber, R. Grishman, P. Tapanainen, and S. Huttunen.2000.
Automatic Acquisiton of Domain Knowledge for In-formation Extraction.
In Proceedings of COLING 2000.
