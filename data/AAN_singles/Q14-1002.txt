FLORS: Fast and Simple Domain Adaptation for Part-of-Speech TaggingTobias SchnabelDepartment of Computer ScienceCornell Universitytbs49@cornell.eduHinrich Sch?tzeCenter for Information & Language ProcessingUniversity of Munichinquiries@cislmu.orgAbstractWe present FLORS, a new part-of-speech tag-ger for domain adaptation.
FLORS uses ro-bust representations that work especially wellfor unknown words and for known words withunseen tags.
FLORS is simpler and faster thanprevious domain adaptation methods, yet ithas significantly better accuracy than severalbaselines.1 IntroductionIn this paper we describe FLORS, a part-of-speech(POS) tagger that is Fast in training and tagging,uses LOcal context only (as opposed to finding theoptimal tag sequence for the entire sentence), per-forms Robustly on target domains (TDs) in unsu-pervised domain adaptation (DA) and is Simple inarchitecture and feature representation.FLORS constructs a robust representation of thelocal context of the word v that is to be tagged.This representation consists of distributional fea-tures, suffixes and word shapes of v and its localneighbors.
We show that it has two advantages.First, since the main predictors used by FLORSare distributional features (not the word?s identity),FLORS predicts unseen tags of known words bet-ter than prior work on DA for POS.
Second, sinceFLORS uses representations computed from unla-beled text, representations of unknown words arein principle of the same type as representations ofknown words; this property of FLORS results inbetter performance on unknown words compared toprior work.
These two advantages are especiallybeneficial for TDs that contain high rates of unseentags of known words and high rates of unknownwords.
We show that FLORS achieves excellent DAtagging results on the five domains of the SANCL2012 shared task (Petrov and McDonald, 2012) andoutperforms three state-of-the-art taggers on Blitzeret al.
?s (2006) biomedical data.FLORS is also simpler and faster than other POSDA methods.
It is simple in that the input repre-sentation consists of three simple types of features:distributional count features and two types of binaryfeatures, suffix and shape features.
Many other wordrepresentations that are used for improving general-ization (e.g., (Brown et al., 1992; Collobert et al.,2011)) are costly to train or have difficulty han-dling unknown words.
Our representations are fastto build and can be created on-the-fly for unknownwords that occur during testing.The learning architecture is simple and fast aswell.
We train k binary one-vs-all classifiers thatuse local context only and no sequence informa-tion (where k is the number of tags).
Thus, tag-ging complexity is O(k).
Many other learning se-tups for DA are more complex; e.g., they learn rep-resentations (as opposed to just counting), they learnseveral classifiers for different subclasses of words(e.g., known vs. unknown) or they combine left-to-right and right-to-left taggings.The next two sections describe experimental data,setup and results.
Results are discussed in Section 4.We compare FLORS to alternative word representa-tions in Section 5 and to related work in Section 6.Section 7 presents our conclusions.2 Experimental data and setupData.
Our source domain is the Penn Treebank(Marcus et al., 1993) of Wall Street Journal (WSJ)15Transactions of the Association for Computational Linguistics, 2 (2014) 15?26.
Action Editor: Sharon Goldwater.Submitted 9/2013; Revised 11/2013; Published 2/2014.
c?2014 Association for Computational Linguistics.text.
Following Blitzer et al.
(2006), we use sections2-21 for training and 100,000 WSJ sentences from1988 as unlabeled data in training.We evaluate on six different TDs.
The firstfive TDs (newsgroups, weblogs, reviews, answers,emails) are from the SANCL shared task (Petrovand McDonald, 2012).
Additionally, the SANCLdataset contains sections 22 and 23 of the WSJfor in-domain development and testing, respectively.Each SANCL TD has an unlabeled training set of100,000 sentences and development and test sets ofabout 1000 labeled sentences each.
The sixth TD isBIO, the Penn BioTreebank data set distributed byBlitzer.
It consists of dev and test sets of 500 sen-tences each and 100,000 unlabeled sentences.Classification setup.
Similar to SVMTool(Gim?nez and M?rquez, 2004) and Choi and Palmer(2012) (henceforth: C&P), we use local context onlyfor tagging instead of performing sequence classifi-cation.
For a word w occurring as token vi in a sen-tence, we build a feature vector for a local windowof size 2l + 1 around vi.
The representation of theobject to be classified is this feature vector and thetarget class is the POS tag of vi.We use the linear L2-regularized L2-loss SVMimplementation provided by LIBLINEAR (Fan etal., 2008) to train k one-vs-all classifiers on the train-ing set where k is the number of POS tags in thetraining set (in our case k = 45).
We train with un-tuned default parameters; in particular, C = 1.
Inthe special case of linear SVMs, the value of C doesnot need to be tuned exhaustively as the solution re-mains constant after C has reached a certain thresh-old value C?
(Keerthi and Lin, 2003).
Training caneasily be parallelized by giving each binary SVM itsown thread.Windows.
The local context for tagging tokenvi is a window of size 2l + 1 centered around vi:(vi?l, .
.
.
, vi, .
.
.
, vi+l).
We pad sentences on eitherside with ?BOUNDARY?
to ensure sufficient con-text for all words.
Given a mapping f from wordsto feature vectors (see below), the representation Fof a token vi is the concatenation of the 2l+ 1 wordvectors in its windowF (vi) = f(vi?l)?
.
.
.?
f(vi+l)where ?
is vector concatenation.Word features.
We represent each word w byfour components: (i) counts of left neighbors, (ii)counts of right neighbors, (iii) binary suffix featuresand (iv) binary shape features.
These four compo-nents are concatenated:f(w) = f left(w)?f right(w)?f suffix(w)?f shape(w)We consider these sources of information equallyimportant and normalize each of the four compo-nent vectors to unit length.
Normalization also hasa beneficial effect on SVM training time because italleviates numerical problems (Fan et al., 2008).Distributional features.
We follow a long tra-dition of older (Finch and Chater, 1992; Sch?tze,1993; Sch?tze, 1995) and newer (Huang and Yates,2009) work on creating distributional features forPOS tagging based on local left and right neighbors.Specifically, the ith entry xi of f left(w) is theweighted number of times that the indicator wordci occurs immediately to the left of w:xi = tf (freq (bigram(ci, w)))where ci is the word with frequency rank i in the cor-pus, freq (bigram(ci, w)) is the number of times thebigram ?ci w?
occurs in the corpus and we weightthe non-zero frequencies logarithmically: tf(x) =1 + log(x).
tf-weighting has been used by other re-searchers (Huang and Yates, 2009) and showed goodperformance in our own previous work.f right(w) is defined analogously.
We restrict theset of indicator words to the n = 500 most fre-quent words in the corpus.
To avoid zero vectors, weadd an entry xn+1 to each vector that counts omittedcontexts:xn+1 = tf??
?j:j>nfreq (bigram(cj , w))?
?We compute distributional vectors on the jointcorpus DALL of all labeled and unlabeled text ofsource domain and TD.
The text is preprocessed bylowercasing everything ?
which is often done whencomputing word representations, e.g., by Turianet al.
(2010) ?
and by padding sentences with?BOUNDARY?
tokens.Suffix features.
Suffixes are promising for DAbecause basic morphology rules are the same in dif-ferent domains.
In contrast to other work on tagging16model classifier features1 TnT HMM p?
{0,1,2}, v0, suffixes (for OOVs)2 Stanford bidir.
MEMM p?
{0,1,2}, v?
{0,1}, affixes, orthography3 SVMTool SVM p?
{0,1,2,3}, v?
{0,1,2,3}, affixes, orthography, word length4 C&P SVM p?
{0,1,2,3}, v?
{0,1,2,3}, affixes, orthography5 FLORS SVM distributions of v?
{0,1,2}, suffixes, orthographyTable 1: Overview of baseline taggers and FLORS.
vi: token, pi: POS tag.
Positions included in the sets of tokenindices are relative to the position i of the word v0 to be tagged; e.g., p?
{0,1,2} is short for {p?0, p?1, p?2, p0, p1, p2}.To represent tokens vi, models 1?4 use vocabulary indices and FLORS uses distributional representations.
Models2?4 use combinations of features (e.g., tag-word) as well.
(e.g., Ratnaparkhi (1996), Toutanova et al.
(2003),Miller et al.
(2007)) we simply use all (lowercase)suffixes to avoid the need for selecting a subset ofsuffixes; and we treat all words equally as opposedto using suffix features for only a subset of words.For suffix s, we set the dimension corresponding tos in f suffix(w) to 1 if lowercased w ends in s and to0 otherwise.
Note that w is a suffix of itself.1Shape features.
We use the Berkeley parser wordsignatures (Petrov and Klein, 2007).
Each wordis mapped to a bit string encompassing 16 binaryindicators that correspond to different orthographic(e.g., does the word contain a digit, hyphen, upper-case character) and morphological (e.g., does theword end in -ed or -ing) features.
There are 50unique signatures in WSJ.
We set the dimension off shape(w) that corresponds to the signature of w to 1and all other dimensions to 0.
We note that the shapefeatures we use were designed for English and prob-ably would have to be adjusted for other languages.Baselines.
We address the problem of unsuper-vised domain adaptation for POS tagging.
For thisproblem, we consider three types of baselines: (i)high-performing publicly available systems, (ii) thetaggers used at SANCL and (iii) POS DA resultspublished for BIO.Most of our experiments use taggers from cate-gory (i) because we can ensure that experimentalconditions are directly comparable.
The four base-lines in category (i) are shown in Table 1.
Threehave near state-of-the-art performance on WSJ:SVMTool (Gim?nez and M?rquez, 2004), Stanford1One could also compute these suffixes for _w (w prefixedby underscore) instead of for w to include words as distinguish-able special suffixes.
We test this alternative in Table 2, line15.
(Toutanova et al., 2003) (a birectional MEMM) andC&P.
TnT (Brants, 2000) is included as a represen-tative of fast and simple HMM taggers.
In addition,C&P is a tagger that has been extensively tested inDA scenarios with excellent results.
Unless other-wise stated, we train all models using their defaultconfiguration files.
We use the optimized parameterconfiguration published by C&P for the C&P model.Test set results will be compared with the SANCLtaggers (category (ii)) at the end of Section 3.As far as category (iii) is concerned, most workon POS DA has been evaluated on BIO.
We discussour concerns about the BIO evaluation sets in Sec-tion 4, but also show that FLORS beats previouslypublished results on BIO as well (see Table 6).3 Experimental resultsWe train k binary SVM classifiers on the trainingset.
A token in the test set is classified by buildingits feature vector, running the classifiers on it andthen assigning it to the POS class whose one-vs-allLIBLINEAR classifier returns the largest score.Results for ALL accuracy (accuracy for all to-kens) and OOV accuracy (accuracy for tokens notoccurring in the labeled WSJ data) are reported inTable 2.
Results with an asterisk are significantlyworse than a column?s best result using McNemar?stest (p < .001).
We use the same test and p-valuethroughout this paper.The basic FLORS model (Table 2, line 5) useswindow size 5 (l = 2).
Each word in the windowhas 1002 distributional features (501 left and right),91,161 suffix features and 50 shape features.
Thefinal feature vector for a token has a dimensionalityof about 500,000, but is very sparse.FLORS outperforms all baselines on the five TDs17newsgroups reviews weblogs answers emails wsjALL OOV ALL OOV ALL OOV ALL OOV ALL OOV ALL OOV1 TnT 88.66?
54.73?
90.40?
56.75?
93.33?
74.17?
88.55?
48.32?
88.14?
58.09?
95.75?
88.302 Stanford 89.11?
56.02?
91.43?
58.66?
94.15?
77.13?
88.92?
49.30?
88.68?
58.42?
96.83 90.253 SVMTool 89.14?
53.82?
91.30?
54.20?
94.21?
76.44?
88.96?
47.25?
88.64?
56.37?
96.63 87.964 C&P 89.51?
57.23?
91.58?
59.67?
94.41?
78.46?
89.08?
48.46?
88.74?
58.62?
96.78 88.655FLORSbasic 90.86 66.42 92.95 75.29 94.71 83.64 90.30 62.15 89.44?
62.61 96.59 90.376 n = 250 90.93 67.03 92.93 75.45 94.69 83.69 90.29 62.20 89.63 63.43 96.56?
89.457v ?
{0,1,2} n = 0 89.14?
55.59?
91.80?
66.31?
93.40?
72.55?
89.47?
55.82?
88.21?
57.83?
96.29?
85.55?8 no suffixes 90.60 65.17 92.74 71.94?
94.77 84.92 89.77?
58.71?
89.30?
62.09 96.28?
88.889 no shapes 89.70?
63.10?
92.24?
68.70?
92.60?
74.72?
89.55?
59.08?
89.63 64.17 95.52?
83.94?10v ?
{1,2} n = 0 90.61?
65.95 92.76?
75.56 94.62 84.62 90.23 61.87 89.40?
63.82 96.51?
90.0211 no suffixes 90.66 64.78?
92.88 75.08 94.83 84.52 90.36 61.92 89.42 62.74 96.64 89.4512 no shapes 90.74 67.03 93.02 75.88 94.57 83.83 90.23 61.73 89.41?
63.49 96.57 90.2513 l = 1 90.44?
63.62?
92.69?
75.72 94.48?
84.03 90.02?
62.66 89.17?
62.71 96.44?
88.6514 L-to-R 90.56?
66.08 92.97 75.40 94.57 83.79 90.43 62.80 89.43 63.13 96.53?
90.9415 voc.
indices 90.93 66.64 92.91 75.03 94.71 84.08 90.27 61.92 89.37?
62.26 96.63 90.60Table 2: Tagging accuracy of four baselines and FLORS on the dev sets.
The table is structured as follows: baselines(lines 1?4), basic FLORS setup (lines 5?6), effect of omitting one of the three feature types if the word to be taggedis changed compared to the basic FLORS setup (lines 7?9) and if the word to be tagged is not changed compared tobasic FLORS (lines 10?12), effect of three important configuration choices on tagging accuracy: window size (line13), inclusion of prior tagging decision (line 14) and vocabulary index (line 15).
n: number of indicator words.
2l+1:size of the local context window.
Lines 10?12: Only the neighbors of v0 are modified compared to basic (line 5).Lines 7?9: All five token representations (including v0) are modified.
A column?s best result is bold.
(line 5 vs. lines 1?4).
Only in-domain on WSJ, threebaselines are slightly superior.
The baselines areslightly better on ALL accuracy because they weredesigned for tagging in-domain data and use featuresets that have been found to work well on the sourcedomain.
Generally, C&P performs best for DAamong the baselines.
On answers and WSJ, how-ever, Stanford has better overall accuracies.
Theseresults are in line with C&P.On lines 6?15, we investigate how different mod-ifications of the basic FLORS model affect perfor-mance.
First, we examine the effect of leaving outcomponents of the representation: distributional fea-tures (f left(w), f right(w)), suffixes (f suffix(w)) andshape features (f shape(w)).Distributional features boost performance in alldomains: ALL and OOV accuracies are consistentlyworse for n = 0 (line 7) than for n ?
{250, 500}(lines 6&5).
FLORS with n = 250 has better OOVaccuracies in 5 of 6 domains.
However, ALL accu-racy for FLORS with n = 500 is better in the major-ity of domains.
The main result of this comparisonis that FLORS does not seem to be very sensitive tothe value of n if n is large enough.Shape features also improve results in all do-mains, with one exception: emails (lines 9 vs 5).For emails, shape features decrease ALL accuracyby .19 and OOV accuracy by 1.56.
This may be dueto the fact that many OOVs are NNP/NN and thattagging conventions for NNP/NN vary between do-mains.
See Section 4 for discussion.Performance benefits from suffixes in all domainsbut weblogs (lines 8 vs 5).
Weblogs contain manyforeign names such as Abdul and Yasim.
For thesewords, shapes apparently provide better informa-tion for classification than suffixes.
ALL accura-cies suffer little when leaving out suffixes, but thefeature space is much smaller: about 3000 dimen-sions.
Thus, for domains where we expect fewOOVs, omitting suffix features could be considered.Lines 7?9 omit one of the components of f(vi)for all five words in the local context: i ?
{?2,?1, 0, 1, 2}.
Lines 10?12 omit the same com-ponents for the neighbor words only ?
i.e., i ?
{?2,?1, 1, 2} ?
and leave f(v0) unchanged.
14 ofthe 6 ?
3 ALL accuracies on lines 10?12 are worsethan FLORS basic, 4 are better.
The largest differ-ences are .25 for newsgroups and .19 for reviews(lines 5 vs 10), but differences for the other domainsare negligible.
This shows that the most important18feature representation is that of v0 (not surprisingly)and that the distributional features of the other wordscan be omitted at the cost of some loss in accuracy ifa small average number of active features is desired.Another FLORS parameter is the size of the localcontext.
Surprisingly, OOV accuracies benefit a bitin four domains if we reduce l from 2 to 1 (lines 13vs 5).
However, ALL accuracy consistently drops inall six domains.
This argues for using l = 2, i.e., awindow size of 5.Results for left-to-right (L-to-R) tagging are givenon line 14.
Similar to SVMTool and C&P, each sen-tence is tagged from left to right and previous tag-ging decisions are used for the current classification.In this setting, we use the previous tag pi?1 as oneadditional feature in the feature vector of vi.The effect of left-to-right is similar to the effectof omitting suffixes: OOV accuracies go up in somedomains, but ALL accuracies decrease (except foran increase of .02 for reviews).
This is in line withthe experiments in (Schnabel and Sch?tze, 2013)where sequential information in a CRF was not ro-bust across domains.
OOV tagging may benefit fromcorrect previous tags because the larger left contextthat is indirectly made available by left-to-right tag-ging compensates partially for the lack of informa-tion about the OOV word.In contrast to standard approaches to POS tag-ging, the FLORS basic representation does not con-tain vocabulary indices.
Line 15 shows what hap-pens if we add them; the dimensionality of the fea-ture vector is increased by 5|V | ?
where V is thetraining set vocabulary ?
and in training one binaryfeature is set to one for each of the five local con-text words.
Performance is almost indistinguishablefrom FLORS basic, suggesting that only using suf-fixes ?
which can be viewed as ?ambiguous?
vocab-ulary indices, e.g., ?at?
is on for ?at?, ?mat?, ?hat?,?laundromat?
etc ?
is sufficient.In summary, we find that distributional features,word signatures and suffixes all contribute to suc-cessful POS DA.
Factors with only minor impacton performance are the number of indicator wordsused for the distributional representations, the win-dow size l and the tagging scheme (L-to-R vs. non-L-to-R).
Unknown words and known words behavedifferently with respect to certain feature choices.The different behavior of unknown and knownwords suggests that training and optimizing two sep-arate models ?
an approach used by SVMTool ?would further increase tagging accuracy.
Note thatthere has been at least one publication (Schnabel andSch?tze, 2013) on optimizing a separate model forunknown words that has in some cases better per-formance on OOV accuracy than what we publishhere.2 However, this would complicate the architec-ture of FLORS.
We opted for a maximally simplemodel in this paper, potentially at the cost of someperformance.Test set results.
Table 3 reports results on the testsets.
FLORS again performs significantly better onall five TDs, both on ALL and OOV.
Only in-domainon WSJ, ALL performance is worse.Finally, we compare our results to the POStaggers for which performance was reported atSANCL 2012 (Petrov and McDonald, 2012, Ta-ble 4).
Constituency-based parsers ?
which alsotag words as a by-product of deriving completeparse trees ?
are excluded from the comparison be-cause they are trained on a richer representation, thesyntactic structure of sentences.3 FLORS?
resultsare better than the best non-parsing-based resultsat SANCL 2012, which were accuracies of 92.32on newsgroups (HIT), 90.65 on reviews (HIT) and91.07 on answers (IMS-1).4 DiscussionAdvantages of FLORS representation.
As we cansee in Table 1, the main representational differencebetween FLORS and the other taggers is that theFLORS representation does not include vocabularyindices of the word to be tagged or its neighbors?
the FLORS vector only consists of distributional,suffix and shape features.This is an obvious advantage for OOVs.
In otherrepresentational schemes, OOVs have representa-tions that are fundamentally different from known2Schnabel and Sch?tze (2013) report OOV accuracies of56.62 (newsgroups), 64.61 (reviews), 71.86 (weblogs), 54.28(answers), 61.05 (emails) and 64.64 (BIO) for their basic modeland even higher OOV accuracies if parameters are optimized ona per-domain basis.3DCU-Paris13 is listed in the dependency parser tables, butDCU-Paris13 results are derived from a constituency parser.DCU also developed sophisticated preprocessing rules for thedifferent domains, which can be viewed as a kind of manualdomain adaptation.19newsgroups reviews weblogs answers emails wsjALL OOV ALL OOV ALL OOV ALL OOV ALL OOV ALL OOV1 TnT 90.85?
56.60?
89.67?
50.98?
91.37?
62.65?
89.36?
51.82?
87.38?
55.12?
96.57?
86.272 Stanford 91.25?
57.96?
90.30?
51.87?
92.32?
67.85?
89.74?
53.41?
87.77?
57.10?
97.43 88.713 SVMTool 91.21?
54.40?
90.01?
45.05?
92.05?
63.59?
89.90?
51.07?
87.74?
53.23?
97.26 86.474 C&P 91.68?
60.58?
90.42?
51.12?
92.22?
66.91?
89.90?
53.31?
87.91?
54.47?
97.44 88.205 FLORS basic 92.41 66.91 92.25 70.87 93.14 75.32 91.17 67.93 88.67 61.09 97.11?
87.79Table 3: Tagging accuracy of four baselines and FLORS on the test sets.newsgroups reviews weblogs answers emails wsj biopcttokens unknown tag 0.31 0.06 0.00 0.25 0.80 0.00 0.98OOV 10.34 6.84 8.45 8.53 10.56 2.72 19.86unseen word+tag 2.44 2.22 1.46 2.91 3.47 0.61 2.50accuracyonunseenword+tagTnT 0.00 0.00 0.00 0.00 0.00 0.00 0.00Stanford 3.66 5.74 9.40 5.46 2.77 15.23 4.64SVMTool 0.00 0.16 0.00 0.00 0.10 0.00 0.00C&P 14.47 14.75 20.51 13.37 10.29 38.07 8.98FLORS basic 21.06 21.97 21.65 17.19 15.13 41.12 12.69Table 4: Top: Percentage of unknown tags, OOVs and unseen word+tag combinations (i.e., known words tagged withunseen tags) in the dev sets.
Bottom: Tagging accuracy on unseen word+tag.words ?
since their vocabulary index does not oc-cur in the training set and cannot be used for predic-tion.
In contrast, given enough unlabeled TD data,FLORS represents known and unknown words in es-sentially the same way and prediction of the correcttag is easier.
This explanation is supported by theexperiments in Table 2: FLORS beats all other sys-tems on OOVs ?
even in-domain on WSJ.In our analysis we found that apart from betterhandling of OOVs there is a second beneficial ef-fect of distributional representations: they facilitatethe correct tagging of known words occurring withtags unseen in the training set, which we call un-seen word+tags.
Table 4 gives statistics on this caseand shows that unseen word+tags occur at least twotimes as often out-of-domain (e.g., 1.46% for we-blogs) than in-domain (.61% for WSJ).
The bottompart of the table shows performance of the five tag-gers on unseen word+tags.
FLORS is the top per-former on all seven domains, with large differencesof more than 5% in some domains.The explanation is similar to the OOV case:FLORS does not restrict the set of possible POS?s ofa word.
The other taggers in Table 2 use the vocabu-lary index of the word to be tagged and will thereforegive a strong preference to seen tags.
Since FLORSuses distributional features, it can more easily assignan unseen tag as long as it is compatible with theoverall pattern of distribution, suffixes and shapestypical of the tag.
C&P also perform relatively wellon unseen word+tag due to the ambiguity classes intheir model, but FLORS representations are betterfor every domain.
We take these results to mean thatconstraints on a word?s possible POS tags may wellbe helpful for in-domain data, but for out-of-domaindata an overly strong bias for a word?s observed tagsis harmful.It is important to stress that representations sim-ilar to FLORS representations have been used fora long time; we would expect many of them tohave similar advantages for unseen word+tags.
E.g.,Brown clusters (Brown et al., 1992) and word em-beddings (Collobert et al., 2011) are similar toFLORS in this respect.
However, FLORS represen-tations are extracted by simple counting whereas thecomputation of Brown clusters or word embeddingsis much more expensive.
The speed with whichFLORS representations can be computed is partic-ularly beneficial when taggers need to be adaptedto new domains.
FLORS can easily adapt its rep-resentations on the fly ?
as each new occurrence ofa word is encountered, the counts that are the basis20for the xi can simply be incremented.
We presenta direct comparison of FLORS representations withother representations in Section 5.?Local context?
vs. sequence classification.
Themost common approach to POS tagging is to tag asentence with its most likely sequence; in contrast,independent tagging of local context is not guaran-teed to find the best sequence.
Recent work on En-glish suggests that window-based tagging can per-form as well as sequence-based methods (Liang etal., 2008; Collobert et al., 2011).
Toutanova et al.
(2003) report similar results.
In our experiments,we also did not find consistent improvements whenwe incorporated sequence constraints (Table 2, line14).
However, there may be languages and appli-cations involving long-distance relationships wherelocal-context classification is suboptimal.Local-context classification has two advantagescompared to sequence classification.
(i) It simplifiesthe classification and tagging setup: we can use anyexisting statistical classifier.
Sequence classificationlimits the range of methods that can be applied; e.g.,it is difficult to find a good CRF implementation thatcan handle real-valued features ?
which are of criti-cal importance for our representation.
(ii) The time complexity of FLORS in tagging isO(skf) where s is the length of the sentence, k is thenumber of tags and f is the number of non-zero fea-tures per local-context representation.
In contrast,sequence decoding complexity is O(sk2f).
Thisdifference is not of practical importance for stan-dard English POS sets, but it could be an argumentagainst sequence classification for tagging problemswith much larger tag sets.In summary, replacing sequence classificationwith local-context classification is attractive forlarge-scale, practical tagging.What DA can and cannot do.
Despite the supe-rior DA tagging results we report for FLORS in thispaper, there is still a gap of 2%?7% (depending onthe domain) between in-domain WSJ accuracy andDA accuracy on SANCL.
In our analysis of this gap,we found some evidence that DA performance canbe further improved ?
especially as more unlabeledTD data becomes available.
But we also found tworeasons for low performance that unsupervised DAcannot do anything about: differences in tag sets ?
orunknown tags ?
and differences in annotation guide-lines.Table 4 shows that unknown tags occur in five ofthe seven TDs at rates between 0% (weblogs) and1% (BIO).
Each token that is tagged with an un-known tag is necessarily an error in unsupervisedDA.
Furthermore, the unknown tag can also im-pact tagging accuracy in the local context4 ?
so theunknown tag rates in Table 4 are probably lowerbounds for the error that is due to unknown tags.Based on these considerations, it is not surprisingthat tagging accuracy (e.g., of FLORS basic) andunknown tag rate are correlated as we can see in Ta-bles 2, 4 and 6; e.g., we get the highest accuraciesin the two domains that do not have unknown tags(weblogs and WSJ) and the lowest accuracy in thedomain with the highest rate (BIO).Since unknown tags cannot be predicted correctly,one could simply report accuracy on known tags.However, given the negative effect of unknown tagson tagging accuracy of the local context in whichthey occur, excluding unknown tags does not fullyaddress the problem.
For this reason, it is probablybest to keep the common practice of simply report-ing accuracy on all tokens, including unknown tags.But the percentages of unknown tags should also bereported for each dataset as a basis for a more accu-rate interpretation of results.Another type of error that cannot be avoided inunsupervised DA is due to differences in annota-tion guidelines.
There are a few such problems inSANCL; e.g., file names like ?Services.doc?
are an-notated as NN in the email domain.
But their dis-tributional and grammatical behavior is more simi-lar to NNPs; as a consequence, most file names areincorrectly tagged.
In general, it is difficult to dis-criminate NNs from NNPs.
The Penn Treebank an-notation guidelines (Santorini, 1990) are compatiblewith either tag in many cases and it may simply beimpossible to write annotation guidelines that avoidthese problems (cf.
Manning (2011)).
NN-NNP in-consistencies are especially problematic for OOVtagging since most OOVs are NNs or NNPs.4For example, there is a special tag ADD in the web do-main for web addresses.
The last two words of the sentence?I would like to host my upcoming website to/IN Liquid-web.com/ADD?
are mistagged by Stanford tagger as ?...
to/TOLiquidweb.com/VB?.
So the missing tag in this case also affectsthe tagging of surrounding words.21bio dev wsj trainOOV ALL ALLNN 62.4 25.4 14.4JJ 15.9 8.9 6.2NNS 10.2 7.5 6.3NNP 0.5 0.2 9.5NNPS 0.0 0.0 0.3Table 5: Frequency of some tags (percent of tokens) forbio dev and wsj train.While the amount of inconsistent annotation islimited for SANCL, it is a serious problem for BIO.Table 5 shows that the proportion of NNPs in BIOis less than a tenth of that in WSJ (.2 in BIO vs.9.5 in WSJ).
This is due to the fact that many bio-specific names, in particular genes, are annotated asNN.
In contrast, the distributionally and orthograph-ically most similar names in WSJ are tagged as NNP.For example, we find ?One cell was teased out, andits DNA/NNP extracted?
in WSJ vs. ?DNA/NN wasisolated?
in BIO.standard setup NNP?NNALL OOV ALL OOVTnT 87.49?
59.08?
91.75?
78.33?Stanford 88.46?
62.55?
92.36?
79.19?SVMTool 88.33?
61.30?
92.47 79.46?C&P 87.82?
60.60?
92.06?
79.30?FLORSbasic 88.90 64.74 92.91 82.58n = 250 88.90 64.51 92.93 82.47n = 0 87.27?
57.75?
90.91?
73.57?no suffixes 88.09?
62.20?
91.98?
79.27?no shapes 87.78?
59.82?
91.81?
77.31?l = 1 89.12 65.52 92.99 82.90Table 6: Tagging accuracy on bio dev.
NNP?NN resultswere obtained by replacing NNPs with NNs.Given this large discrepancy in the frequency ofthe tag NNP ?
which arguably is due to differentannotation guidelines, not due to underlying differ-ences between the two genres ?
BIO should proba-bly not be used for evaluating DA.
This is why wedid not include it in our comparison in Table 2.For sake of completeness, we provide tagging ac-curacies for BIO in Table 6, ?standard setup?.
Theresults are in line with SANCL results: FLORSbeats the baselines on ALL and OOV accuracies.However, if we build the NN bias into our modelby simply replacing all NNP tags with NN tags, thenaccuracy goes up by 4% on ALL and by almost 20%on OOV.
Even TnT, the most basic tagger, achievesALL/OOV accuracy of 91.75/78.33, better than anymethod in the standard setup.
These accuracies arewell above those in (Blitzer et al., 2006) and (Huangand Yates, 2010).Since simply replacing NNPs with NNs has sucha large effect, BIO cannot be used sensibly for eval-uating DA methods.
In practice, it is not possibleto separate ?true?
improvements due to generic bet-ter DA from elements of the proposed method thatsimply introduce a negative bias for NNP.In summary, when comparing different DA meth-ods caution should be exercised in the choice of do-mains.
In particular, the effect of unknown tagsshould be made transparent and the gold standardsshould be analyzed to determine whether the taskaddressed in the TD differs significantly in some as-pects from that addressed in the source domain.5 Comparison of word representationsOur approach to DA is an instance of representationlearning: we aim to find representations that are ro-bust across domains.
In this section, we compareFLORS with two other widely used representationlearning methods: (i) Brown clusters (Brown et al.,1992) and (ii) C&W embeddings, the word embed-dings of Collobert et al.
(2011).
We use fdist(w) =f left(w)?f right(w) to refer to our own distributionalword representations (see Section 2).The perhaps oldest and most frequently used low-dimensional representation of words is based onBrown clusters.
Typically, prefixes of Brown clus-ters (Brown et al., 1992) are added to increase therobustness of POS taggers (e.g., Toutanova et al.(2003)).
Computational costs are high (quadratic inthe vocabulary size) although the computation canbe parallelized (Uszkoreit and Brants, 2008).More recently, general word representations (Col-lobert et al., 2011; Turian et al., 2010) have beenused for robust POS tagging.
These word represen-tations are typically trained on a large amount of un-labeled text and fine-tuned for specific NLP tasks.Similar to Brown clusters, they are low-dimensionaland can be used as features in many NLP tasks, ei-22ther alone or in combination with other features.To compare fdist(w) (our distributional repre-sentations) with Brown clusters, we induced 1000Brown clusters on the joint corpus data DALL (seeSection 2) using the publicly available implemen-tation of Liang (2005).
We padded sentences with?BOUNDARY?
tokens on each side and used pathprefixes of length 4, 6, 10 and 20 as features foreach word (cf.
Ratinov and Roth (2009), Turian etal.
(2010)).C&W embeddings are provided by Collobert et al.
(2011): 50-dimensional vectors for 130,000 wordsfrom WSJ, trained on Wikipedia.
Similar to our dis-tributional representations fdist(w), the embeddingsalso contain a ?BOUNDARY?
token (which theycall PADDING).
Moreover, they have a special em-bedding for unknown words (called UNKNOWN)which we use whenever we encounter a word thatis not in their lookup table.
We preprocess our rawtokens the same way they do (lowercase and replacesequences of digits by ?0?)
before we look up a rep-resentation during training and testing.We replaced the distributional features in our ba-sic setup by either Brown cluster features or C&Wembeddings.
Table 7 repeats lines 5 and 7 of Table 2and gives results of the modified FLORS setup.All three representations improve both ALL andOOV accuracies in all domains.
fdist outperformsBrown in all cases except for OOV on emails.Brown may suffer from noisy data; cleaning meth-ods have been used in the literature (Liang, 2005;Turian et al., 2010), but they are not unproblematicsince a large part of the data available is lost, whichresults in more unknown words.Brown and fdist can be directly compared sincethey were trained on exactly the same data.
fdistand C&W are harder to compare directly becausethere are many differences.
(i) C&W is trained ona much larger dataset.
One consequence of this isthat OOV accuracy on WSJ may be higher becausesome words that are unknown for other methods areactually known to C&W.
(ii) C&W vectors are nottrained on the SANCL TD data sets ?
this gives fdistan advantage.
(iii) C&W vectors are not trained onthe WSJ.
Again, this could give fdist an advantage.
(iv) C&W and fdist are fundamentally different in theway they handle unknown words.
C&W has a lim-ited vocabulary and must replace all words not inthis vocabulary by the token UNKNOWN.
In con-trast, fdist can create a meaningful individual repre-sentation for any OOV word it encounters.Our FLORS tagger provides best ALL accuraciesin all domains but WSJ, where C&W has best re-sults.
The good performance of C&W is rather un-surprising since the embeddings were created for the130,000 most frequent words of the WSJ and thuscover the WSJ domain much better.
Also, WSJwas used to tune parameters during development.As with our previous experiments, OOV results onemails seem slightly more sensitive to parameterchoices than on other domains (recall the discussionof this issue in Section 4).In summary, we have shown that fdist represen-tations work better for POS DA than Brown clus-ters.
Furthermore, the evidence we have presentedsuggests that fdist are comparable in performance toC&W embeddings if not better for POS DA.The most important difference between fdist andBrown / C&W is that fdist are much simpler andmuch faster to compute.
They are simpler becausethey are just slightly transformed counts in contrastto the other two approaches, which solve complexoptimization problems.
fdist can be computed effi-ciently through simple incrementation in one passthrough the corpus.
In contrast, the other two ap-proaches are an order of magnitude slower.6 Related workUnsupervised DA methods can be broadly putinto four categories: representation learning andconstraint-based frameworks ?
which require sometailoring to a task ?
and instance weighting and boot-strapping ?
which can be more generally applied to awide range of problems.
Since many approaches areapplication-specific, we focus on the ones that havebeen applied to POS tagging.Representation learning.
We already discussedtwo important approaches to representation learningin Section 5: C&W embeddings and Brown clusters.Blitzer et al.
?s (2006) structural correspondencelearning (SCL) supports DA by creating similarrepresentations for correlated features in the pivotfeature space.
This is a potentially powerfulmethod.
FLORS is simpler in that correlations aremade directly accessible to the supervised learner.23newsgroups reviews weblogs answers emails wsjALL OOV ALL OOV ALL OOV ALL OOV ALL OOV ALL OOV1FLORS fdist(w), n=500 90.86 66.42 92.95 75.29 94.71 83.64 90.30 62.15 89.44 62.61 96.59 90.372 fdist(w), n=0 89.14?
55.59?
91.80?
66.31?
93.40?
72.55?
89.47?
55.82?
88.21?
57.83?
96.29?
85.55?3 C&W for fdist(w) 90.57 64.57 92.54?
72.48?
94.51 80.58?
90.23 60.99 89.44 63.13 96.72 90.484 Brown for fdist(w) 90.34?
62.41?
92.23?
71.47?
94.45 81.76 89.71?
56.28?
89.02?
63.20 96.48?
87.50Table 7: Tagging accuracy of different word representations on the dev sets.
Line 1 corresponds to FLORS basic.
n:number of indicator words.
A column?s best result is bold.Moreover, FLORS representations consist of simplecounts whereas SCL solves a separate optimizationproblem for each pivot feature.Umansky-Pesin et al.
(2010) derive distributionalinformation for OOVs by running web queries.
Thisapproach is slow since it depends on a search engine.Ganchev et al.
(2012) successfully use searchlogs.
This is a promising enhancement for FLORS.Huang and Yates (2009) evaluate CRFs with dis-tributional features.
They examine lower dimen-sional feature representations using SVD or the la-tent states of an unsupervised HMM.
They find bet-ter accuracies for their HMM method than Blitzeret al.
(2006); however, they do not compare themagainst a CRF baseline using distributional features.In later work, Huang and Yates (2010) add the la-tent states of multiple, differently trained HMMs asfeatures to their CRF.
Huang and Yates (2012) ar-gue that finding an optimal feature representationis computationally intractable and propose a newframework that allows prior knowledge to be inte-grated into representation learning.Latent sequence states are a form of word repre-sentation.
Thus, it would be interesting to comparethem to the non-sequence-based distributional rep-resentation that FLORS uses.Constraint-based methods.
Rush et al.
(2012)use global constraints on OOVs to improve out-of-domain tagging.
Although constraints ensure con-sistency, they require careful manual engineering.Distributional features can also be seen as a formof constraint since feature weights will be sharedamong all words.Subramanya et al.
(2010) construct a graph to en-courage similar n-grams to be tagged similarly, re-sulting in moderate gains in one domain, but nogains on BIO when compared to self-training.
Thereason could be an insufficient amount of unsuper-vised data for BIO (100,000 sentences).
Our ap-proach does not seem to suffer from this problem.Bootstrapping.
Both self-training (McClosky etal., 2006) ?
which uses one classification model ?and co-training (Blum and Mitchell, 1998) ?
whichuses?2 models ?
have been applied to POS tagging.Self-training usually improves a POS baselineonly slightly if at all (Huang et al., 2009; Huang andYates, 2010).
Devising features based on labeled in-stances (instead of training on them) has been moresuccessful (Florian et al., 2004; S?gaard, 2011).Chen et al.
(2011) use co-training for DA.
In eachround of their algorithm, both new training instancesfrom the unlabeled data and new features are added.Their model is limited to binary classification.
Theco-training method of K?bler and Baucom (2011)trains several taggers and adds sentences from theTD to the training set on which they agree.
Theyreport slight, but statistically significant increases inaccuracy for POS tagging of dialogue data.Instance weighting.
Instance weighting formal-izes DA as the problem of having data from differ-ent probability distributions in each domain.
Thegoal is to make these two distributions align by us-ing instance-specific weights during training.
Jiangand Zhai (2007) propose a framework that integratesprior knowledge from different data sets into thelearning objective by weights.In related work, C&P train generalized anddomain-specific models.
An input sentence is taggedby the model that is most similar to the sentence.FLORS could be easily extended along these lines,an experiment we plan for the future.In terms of the basic classification setup, our POStagger is most similar to the SVM-based approachesof Gim?nez and M?rquez (2004) and C&P.
How-ever, we do not use a left-to-right approach whentagging sentences.
Moreover, SVMTool trains twoseparate models, one for OOVs and one for knownwords.
FLORS only has a single model.
In addition,24we do not make use of ambiguity classes, token-tagdictionaries and rare feature thresholds.
Instead, werely only on three types of features: distributionalrepresentations, suffixes and word shapes.The local-context-only approach of SVMTool,C&P and FLORS is different from standard se-quence classification such as MEMMs (e.g., Rat-naparkhi (1996), Toutanova et al.
(2003), Tsuruokaand Tsujii (2005)) and CRFs (e.g., Collins (2002)).Sequence models are more powerful in theory, butthis may not be an advantage in DA because the sub-tle dependencies they exploit may not hold acrossdomains.7 ConclusionWe have presented FLORS, a new POS tagger forDA.
FLORS uses robust representations that workespecially well for unknown words and for knownwords with unseen tags.
FLORS is simpler andfaster than previous DA methods, yet we were ableto demonstrate that it has significantly better accu-racy than several baselines.Acknowledgments.
This work was supported byDFG (Deutsche Forschungsgemeinschaft).ReferencesJohn Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In EMNLP, pages 120?128.Avrim Blum and Tom Mitchell.
1998.
Combining la-beled and unlabeled data with co-training.
In COLT,pages 92?100.Thorsten Brants.
2000.
TnT: A statistical part-of-speechtagger.
In ANLP, pages 224?231.Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-cent J. Della Pietra, and Jenifer C. Lai.
1992.
Class-based n-gram models of natural language.
Computa-tional Linguistics, 18:467?479.Minmin Chen, Kilian Q. Weinberger, and John Blitzer.2011.
Co-training for domain adaptation.
In NIPS,pages 1?9.Jinho D. Choi and Martha Palmer.
2012.
Fast and robustpart-of-speech tagging using dynamic model selection.In ACL: Short Papers, pages 363?367.Michael Collins.
2002.
Discriminative training methodsfor hidden Markov models: Theory and experimentswith perceptron algorithms.
In EMNLP, pages 1?8.Ronan Collobert, Jason Weston, L?on Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011.Natural language processing (almost) from scratch.The Journal of Machine Learning Research, 12:2493?2537.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-RuiWang, and Chih-Jen Lin.
2008.
LIBLINEAR: A li-brary for large linear classification.
The Journal ofMachine Learning Research, 9:1871?1874.Steven Finch and Nick Chater.
1992.
Bootstrapping syn-tactic categories using statistical methods.
In Back-ground and Experiments in Machine Learning of Nat-ural Language, pages 229?235.Radu Florian, Hany Hassan, Abraham Ittycheriah,Hongyan Jing, Nanda Kambhatla, Xiaoqiang Luo,Nicolas Nicolov, and Salim Roukos.
2004.
A statisti-cal model for multilingual entity detection and track-ing.
In HLT-NAACL, pages 1?8.Kuzman Ganchev, Keith Hall, Ryan McDonald, and SlavPetrov.
2012.
Using search-logs to improve query tag-ging.
In ACL: Short Papers, pages 238?242.Jes?s Gim?nez and Llu?s M?rquez.
2004.
SVMTool: Ageneral pos tagger generator based on support vectormachines.
In LREC, pages 43?46.Fei Huang and Alexander Yates.
2009.
Distributionalrepresentations for handling sparsity in supervisedsequence-labeling.
In ACL-IJCNLP, pages 495?503.Fei Huang and Alexander Yates.
2010.
Exploringrepresentation-learning approaches to domain adapta-tion.
In DANLP, pages 23?30.Fei Huang and Alexander Yates.
2012.
Biased repre-sentation learning for domain adaptation.
In EMNLP-CoNLL, pages 1313?1323.Zhongqiang Huang, Vladimir Eidelman, and MaryHarper.
2009.
Improving a simple bigram HMM part-of-speech tagger by latent annotation and self-training.In NAACL-HLT: Short Papers, pages 213?216.Jing Jiang and ChengXiang Zhai.
2007.
Instance weight-ing for domain adaptation in NLP.
In ACL, pages 264?271.S.
Sathiya Keerthi and Chih-Jen Lin.
2003.
Asymptoticbehaviors of support vector machines with Gaussiankernel.
Neural computation, 15(7):1667?1689.Sandra K?bler and Eric Baucom.
2011.
Fast domainadaptation for part of speech tagging for dialogues.
InRANLP, pages 41?48.Percy Liang, Hal Daum?
III, and Dan Klein.
2008.Structure compilation: trading structure for features.In ICML, pages 592?599.Percy Liang.
2005.
Semi-supervised learning for naturallanguage processing.
Master?s thesis, MassachusettsInstitute of Technology.Christopher D. Manning.
2011.
Part-of-speech taggingfrom 97% to 100%: Is it time for some linguistics?
InCICLing, pages 171?189.25Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-rice Santorini.
1993.
Building a large annotated cor-pus of English: The Penn treebank.
ComputationalLinguistics, 19(2):313?330.David McClosky, Eugene Charniak, and Mark Johnson.2006.
Reranking and self-training for parser adapta-tion.
In ACL, pages 337?344.John Miller, Manabu Torii, and Vijay K. Shanker.
2007.Building domain-specific taggers without annotated(domain) data.
In EMNLP-CoNLL, pages 1103?1111.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In HLT-NAACL, pages 404?411.Slav Petrov and Ryan McDonald.
2012.
Overview of the2012 Shared Task on Parsing the Web.
Notes of the1st SANCL Workshop.Lev Ratinov and Dan Roth.
2009.
Design challengesand misconceptions in named entity recognition.
InCoNLL, pages 147?155.Adwait Ratnaparkhi.
1996.
A maximum entropy modelfor part-of-speech tagging.
In EMNLP, pages 133?142.Alexander M. Rush, Roi Reichart, Michael Collins, andAmir Globerson.
2012.
Improved parsing and POStagging using inter-sentence consistency constraints.In EMNLP-CoNLL, pages 1434?1444.Beatrice Santorini.
1990.
Part-of-speech tagging guide-lines for the Penn Treebank project (3rd revision, 2ndprinting).
Technical report, Department of Linguistics,University of Pennsylvania.Tobias Schnabel and Hinrich Sch?tze.
2013.
Towardsrobust cross-domain domain adaptation for part-of-speech tagging.
In IJCNLP, pages 198?206.Hinrich Sch?tze.
1993.
Part-of-speech induction fromscratch.
In ACL, pages 251?258.Hinrich Sch?tze.
1995.
Distributional part-of-speechtagging.
In EACL, pages 141?148.Anders S?gaard.
2011.
Semisupervised condensed near-est neighbor for part-of-speech tagging.
In ACL: Shortpapers, pages 48?52.Amarnag Subramanya, Slav Petrov, and FernandoPereira.
2010.
Efficient graph-based semi-supervisedlearning of structured tagging models.
In EMNLP,pages 167?176.Kristina Toutanova, Dan Klein, Christopher D. Manning,and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In NAACL-HLT, pages 173?180.Yoshimasa Tsuruoka and Jun?ichi Tsujii.
2005.
Bidirec-tional inference with the easiest-first strategy for tag-ging sequence data.
In EMNLP-HLT, pages 467?474.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: A simple and general methodfor semi-supervised learning.
In ACL, pages 384?394.Shulamit Umansky-Pesin, Roi Reichart, and Ari Rap-poport.
2010.
A multi-domain web-based algorithmfor POS tagging of unknown words.
In COLING,pages 1274?1282.Jakob Uszkoreit and Thorsten Brants.
2008.
Distributedword clustering for large scale class-based languagemodeling in machine translation.
In ACL, pages 755?762.26
