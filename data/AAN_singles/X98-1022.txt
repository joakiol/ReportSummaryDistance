AN NTU-APPROACH TO AUTOMATIC SENTENCEEXTRACTION FOR SUMMARY GENERATIONKuang-hua ChertLanguage & In format ion Process ing System Lab.
(L IPS)Depar tment  of  L ibrary and Informat ion Sc ienceNat ional  Taiwan Univers i ty1, SEC.
4, Rooseve l t  RD., TaipeiTA IWAN,  10617, R.O.C.E-mai l :  khchen @ ccms.ntu.edu.twFax: +886-2-23632859Sheng- J ie  Huang,  Wen-Cheng Lin and Hsin-Hsi  ChenNatural  Language Process ing Laboratory  (NLPL)Depar tment  o f  Computer  Sc ience and In format ion Eng ineer ingNat ional  Taiwan Univers i ty1, SEC.
4, Rooseve l t  RD.,  TaipeiTA IWAN,  10617, R.O.C.E-mai l :  { s jhuang,denis l in } @nlg.csie.ntu.edu.tw, hh_chen @csie.ntu.edu.twFax: +886-2-23638167ABSTRACTAutomatic summarization a d information extractionare two important Internet services.
MUC andSUMMAC play their appropriate roles in the nextgeneration Internet.
This paper focuses on theautomatic summarization and proposes two differentmodels to extract sentences for summary generationunder two tasks initiated by SUMMAC-1.
Forcategorization task, positive feature vectors andnegative feature vectors are used cooperatively toconstruct generic, indicative summaries.
For adhoctask, a text model based on relationship betweennouns and verbs is used to filter out irrelevantdiscourse segment, o rank relevant sentences, and togenerate the user-directed summaries.
The resultshows that the NormF of the best summary and thatof the fixed summary for adhoc tasks are 0.456 and0.447.
The NormF of the best summary and that ofthe fixed summary for categorization task are 0.4090and 0.4023.
Our system outperforms the averagesystem in categorization task but does a common jobin adhoc task.1.
INTRODUCTIONTowards the end of the 20 th century, the Internethas become a part of life style.
People enjoy Internetservices from various providers and these ISPs(Internet Services Providers) do their best to fulfillusers' information eed.
However, if we investigatethe techniques used in these services, we will find outthat they are not different from those used intraditional Information Retrieval or NaturalLanguage Processing.
However, the cyberspaceprovides us an environment to utilize thesetechniques to serve more persons than ever before.The members under the leadership of ProfessorHsin-Hsi Chen of Natural Language Processing Lab.
(NLPL) in Department of Computer Science andInformation Engineering, National TaiwanUniversity have dedicated themselves in researchesof NLP for many years.
The research results havebeen reported in literature and received the reputationfrom colleagues of NLP field.
Many systems forvarious NLP applications have been developed,especially for Chinese and English.
Some systemscould be accessed irectly via WWW browsers.
Forexample, an MT meta-server \[1\] provides an onlineEnglish-to-Chinese translation service.
(http://nlg3.csie.
ntu.edu.tw/mtir/mtir.html)Language & Information Processing System Lab.
(LIPS) in Department of Library and InformationScience, National Taiwan University also devotesitself in researches of language, information andlibrary sciences.
Chen and Chen \[2\] proposed hybridmodel for noun extraction from running texts andprovided an automatic evaluation method.
Chen \[3\]proposed a cowus-based model to identify topics andused it to determine sub-topical structures.163Generally speaking, we are capable of dealing withnumerous NLP applications or apply NLP techniquesto other applications using our current researchresults.
The two laboratories think that currentInternet services are not enough for the people livingin the next century.
At least, two kinds of services areimportant and crucial in the 21 st century: one is theinformation extraction; the other is automaticsummarization.Information Extraction (IE) \[4\] systems manage toextract predefined information from data ordocuments.
What kind of information is appropriateis a domain-dependent problem.
For example, theinformation conveyed by business news and byterrorism news is very different.
As a result, thepredefined information plays an important role in IEsystems.
In fact, the predefined information is the so-called metadata \[5\].
The joint efforts on IE andmetadata will benefit both sides.Automatic summarization is to use automaticmechanism to produce a finer version for the originaldocument.
Two possible methodologies could beapplied to constructing summaries.
The first is toextract sentences directly from texts; the second is toanalyze the text, extract he conceptual representationof the text, and then generate summary based on theconceptual representation.
No matter whatmethodology is adopted, the processing time shouldbe as little as possible for Internet applications.As we mentioned above, information extractionand automatic summarization are regarded as twoimportant Internet services in the next century.Therefore, we take part in MET-2 and SUMMAC-1for the respective purposes.
In this paper, we willfocus on the tasks of SUMMAC-1 and the details ofMET-2 can be referred to the paper presented inMET-2 Conference \[6\].This paper is organized as follows.
Section 2discusses the types of summaries and their functions.In addition, the tasks of SUMMAC-1 and thecorresponding functions to the traditional summariesare also described.
Sections 3 and 4 propose themodels to carry out the categorization task and adhoctask, respectively.
The method for extracting featurevectors, calculating extraction strengths, andidentifying discourse segments are illustrated indetail in the two sections.
Section 5 shows our resultsin summary and compares with other systems.Section 6 gives a short conclusion.2.
SUMMARY AND SUMMAC-1  TASKSIn general, summarization is to create a shortversion for the original document.
The functions ofsummaries are shown as follows \[7\]:?
Announcement: announce the existence of theoriginal document?
Screening: determine the relativeness of theoriginal document?
Substitution: replace the original document?
Retrospection: point to the original documentA summary can be one of four types, i.e., indicativesummary, informative summary, critical summary,and extract.
Indicative summaries are usually offunctions of announcement and screening.
Bycontrast, informative summaries are of function ofsubstitution.
It is very difficult to generate criticalsummaries in automatic ways.
Extract can be ofannouncement, and replacement.
In general, all of thefour types of summaries are retrospective.The most important summary types are indicativesummary and informative summary in the Internetenvironment.
However, for researchers devotingthemselves in automatic summarization, the commontype of summary is extract.
This is because theextract is produced through extracting the sentencesin the original document and this is an easier way toproduce a summary.
But, how to make extractpossess the functionality of informative summary andthat of indicative summary?
A common way is toproduce a fix-length extract for indicative summaryand to produce a best extract for informativesummary.
That is the also two different summariesunderlying the tasks of SUMMAC- 1.SUMMAC-1 announces three tasks for automaticsummarization: the first is categorization task; thesecond is adhoc task; the third is Q&A task.
Thesethree tasks have their own designated purposes.
Asthe SUMMAC-1 design, the tasks address thefollowing types of summaries:?
Categorization: Generic, indicative summary?
Adhoc: Query-based, indicative summary?
Q&A: Query-based, informative summaryAlthough the definitions shown above are not thesame as we talk about in previous paragraph, this willnot interfere the development of an automaticsummarization system.Because we have many experiences in applyinglanguage techniques to dealing with the similar tasks\[3, 8\], we decide to take part in Categorization taskand Adhoc task after long discussion.
The reasonsare described as follows.
For an application in theInternet environment, to provide introductoryinformation for naive users is very important.
It isvery suitable to use generic indicative summaries tofulfill this function.
However, the users have theirown innate knowledge and they want that thegenerated summary is relative to the issued query attimes.
Therefore, the two different needs are fulfilledas the first and the second tasks initiated bySUMMAC-1.
As to the third task, Q&A, we thinkthat it is much more relative to the information164extraction.
It can be resolved in association with IEas a part of MUC's tasks.3.CATEGORIZATION TASKAs the call for paper of SUMMAC- 1 says, the goalof the categorization task is to evaluate genericsummaries to determine if the key concept in a givendocument is captured in the summary.
TheSUMMAC-1 documents fall into sets of topics andeach topic contains approximately 100 documents.The task asks summarization systems to producesummary for each document, The assessor will readthe summary and then assign the summary into oneof five topics or the sixth topic, 'non-relevant' topic.The testing set of documents consists of twogeneral domains, environment and global economy.Each domain in turn consists of five topics and eachtopic contains 100 documents.
As a result, thesedocuments could be regarded as the positive cues forthe corresponding topic.
By contrast, documents ofother topics could be treated as the negative cues forthe topic under consideration.
The training stage andthe testing stage are described in the followingparagraph.For each topic, the following procedure isexecuted in the training stage.
(1) Screen out function words for each document(2) Calculate word frequency for current topic aspositive feature vector (PFV)(3) Calculate word frequency for other topics asnegative feature vector (NFV)The testing stage is shown as follows.
(1) Exclude function words in test documents(2) Identify the appropriate topic for testingdocuments(3) Use PFV and NFV of the identified topic to ranksentences in test documents(4) Select sentences to construct abest summary(5) Select sentences to construct a fixed-lengthsummaryBased on this line, the approach for summarygeneration under the categorization task could bedepicted as Figure 1 shows.Step (1) in training stage and testing stage are toexclude function words.
A stop list is used as thispurpose.
A stop list widely distributed in the Internetand another list collected by us are combined.
Theresultant stop list consists of 744 words, such as abaft,aboard, about, above, across, afore, after, again,against, ain't, aint, albeit, all, almost, alone, along,alongside, already, also, although, always, am, amid,and so on.Steps (2) and (3) in training stage regard thedocument collection of a topic as a whole to extractthe PFV and NFV.
Firstly, the document collection ofa topic is thought as the pool of words.
Step (2)calculates the frequency of each word in this pooland screens out those words with frequency lowerthan 3.
Step (3) repeats the same procedure.
However,this time the pool consists of words from documentcollections of other topics.
After normalization, twofeature vectors PFV = (pwl, pw2, pw 3 .
.
.
.
.
pwn) andNFV = (nw 1, nw2, nw 3 .
.
.
.
.
nwn) are constructed to beunit vectors.
The PFV and NFV are used to extractsentences of document and those extracted sentencesconsist of the summary.
The idea behind thisapproach is that we use documents to retrieve thestrongly related sentences inparallel to IR system usequery sentence to retrieve the related ocuments.k D?cument LTopic 1 IPositiveFeatureVectorTrainingModule l I Other ?ics JNegativeFeatureVectorFigure 1.
The Training Procedure for Categorization Task165Step (2) in testing stage is to identify which topicthe testing document belongs to.
The PFVs and theNFVs are used to compare with testing documents.Assume that the testing document D consists of dw t,dw2, dw 3 .
.
.
.
.
and dw.
words, i.e., D = (dw l, dw 2,dw 3 .
.
.
.
.
dw,) and there are m pairs of PFV and NFV.The following equation is used to determine that the~"th topic is best for the document underconsideration.= argmax(sim(PFE, D) - s im(NFV, D))l~i~mThe similarity shown in the following is measured byinner product.nsim(PFV, D) = Z (pwj ?
dwj)j=lWhile the topic is determined, Step (3) uses thecorresponding PFV~ and NFV~ to select sentences inthe document.
Whether a sentence S = (sw~, sw2,sw 3 .
.
.
.
.
sw,) is selected as part of a summarydepends on the relative score shown as follows.
Thesimilarity is also measured by inner product.RS(S) = sim(PFV~, S)-sim(NFV~, S)In Step (4), the ranked list of RSes is examined andthe maximal score gap between two immediate RSesis identified.
If the number of sentences above theidentified gap is between 10% to 50% of that of allsentences, these sentences are extracted as the bestsummary.
Otherwise, the next maximal gap isexamined whether it is a suitable gap or not.
Step (5)just uses the best summary generated in Step (4) andmakes a fixed-length summary according to theSUMMAC-1 rule.4.
ADHOC TASKAdhoc Task is designed to evaluate user-directedsummaries, that is to say, the generated summaryshould be closely related to the user's query.
Thiskind of summary is much more important for Internetapplications.
We have devoted ourselves in relatedresearches for a long time.
A text model based on theinteraction of nouns and verbs was proposed in \[3\],which is used to identify topics of documents.
Chenand Chen \[8\] extended the text model to partitiontexts into discourse segments.The following shows the process of NTU'sapproach to adhoc task in SUMMAC-1 formal run.
(1) Assign a part of speech to each word in texts.
(2) Calculate the extraction strength (ES) for eachsentence.
(3) Partition the text into meaningful segments.
(4) Filter out irrelevant segments according to theuser's query.
(5) Filter out irrelevant sentences based on ES.
(6) Generate the best summary.
(7) Generate the fixed-length summary from thebest summary.Step (1) is used to identify the nouns and the verbs intexts, which are regarded as the core words in textsand will be used in Step (2).
Step (2) is the majorstage in our approach and will be discussed in detail.Generally speaking, each word in a sentence hasits role.
Some words convey ideas, suggestions, andconcepts; some words are functional rather thanmeaningful.
Therefore, it is much more reasonable tostrip out these function words, while we manage tomodel information flow in texts.
Nouns and verbs aretwo parts of speech under consideration.
In addition,a measure for word importance should be worked outto treat each noun or verb in an appropriate scale.
Intradition, term frequency (TF) is widely used inresearches of information retrieval.
The idea is thatafter excluding the functional words, the words occurfrequently would carry the meaning underlying a text.However, if these words appear in many documents,the discriminative power of words will decrease.Spack Jones \[9\] proposed inverse documentfrequency (IDF) to rectify the aforementionedshortcoming.
The IDF is shown as follows:IDF(w) = log(P-O(w))/O(w),where P is the number of documents in a collection,O(w) is the number of documents with word w.Nouns and verbs in well-organized texts arecoherent in general.
In order to automaticallysummarize texts, it is necessary to analyze the factorsof composing texts.
That is, the writing process ofhuman beings.
We use four distributional parametersto construct a text model:?
Word importance?
Word frequency?
Word co-occurrence?
Word distanceThe following will discuss each factor in sequence.The word importance means that when a wordappears in texts, how strong it is to be the core wordof texts.
In other words, it represents the possibilityof selecting this word as an index term.
The IDF ischosen to measure the word importance in this paper.In addition, the frequency of a word itself does alsoplay an important role in texts.
For example, theword with high frequency usually makes readersimpressive.
The proposed model combines the twofactors as the predecessors did.If a text discusses a special subject, there should bemany relative words together to support his subject.That is to say, these relative words will co-occurfrequently.
From the viewpoint of statistics, somekind of distributional parameters like mutualinformation \[10\] could be used to capture thisphenomenon.166Including the distance factor is motivated by thefact that related events are usually located in thesame texthood.
The distance is measured by thedifference between cardinal numbers of two words.We assign a cardinal number to each verb and nounin sentences.
The cardinal numbers are keptcontinuous across sentences in the same paragraph.As a result, the distance between two words, w~ andw 2, is calculated using the following equation.D(wl,w2) = abs(C(wl)-C(w2)),where the D denotes the distance and C the cardinalnumber.Consider the four factors together, the proposedmodel for adhoc task is shown as follows:CS(n) = pnx SNN(n) + pvx SNV(n)CS is the connective strength for a noun n, whereSNN denotes the strength of a noun with other nouns,SNV the strength of a noun with other verbs, and pnand pv are the weights for SNN and SNV, respectively.The determination of pn and pv is via deletedinterpolation \[11\] (Jelinek, 1985).
The equations forSNV and SNN are shown as follows.SNV (ni) = '~" 1DF (hi) X 1DF (vj) x f (nl, vj)f(nl)?
f(vj)?D(ni, vj)SNN (m) = "~".
IDF (m)?
IDF (nj)x f (ni, nj)?
f(nl)?f(nj)?D(ni, nj)f(wi,wj) is the co-occurrence of words wi and wj, andf(w) is the frequency of word w. In fact,f(wi,wj)/f(wi)xf(wj) is a normalized co-occurrencemeasure with the same form as the mutualinformation.When the connectivity score for each noun in asentence is available, the chance for a sentence to beextracted as a part of summary can be expressed asfollows.
We call it extraction strength (ES).mES(S,) = ~CS(n, j ) /m,j=lwhere m is the number of nouns in sentence Si.Because texts are well organized and coherent, it isnecessary to take the paragraph into consideration forsummary generation.
However, the number ofsentences in paragraphs may be one or two,especially in newswire.
It is indispensable to groupsentences into meaningful segments or discoursesegments before carrying out the summarization task.Step (3) is for this purpose.
A sliding window withsize W is moved from the first sentence to the lastsentence and the score for sentences within thewindow is calculated.
Accordingly, a series of scoresis generated.
The score-sentence r lation determinesthe boundaries of discourse segments.
Figure 2shows aforementioned process and how to calculatethe scores.
The window size W is 3 in thisexperiment.While discourse segments are determined, theuser's query is used to filter out less relevantsegments.
This is fulfilled in Step (4).
The nouns of aquery are compared to the nouns in each segment andthe same technique for calculating SNN mentionedabove is used \[8\].
As a result, the precedence ofsegments to the query is calculated and then themedium score is identified.
The medium is used tonormalize the calculated score for each segment.
Thesegments with normalized score lower than 0.5 arefiltered out.oEW?
, ,~ lo lo lo t  ol ?.
.
.
S i S i+ l  .
.
.SENTENCEFigure 2.
Determination of discourse segmentsStep (5) is to filter out the irrelevant sentences in theselected segments in Step (4).
The ES of eachsentence calculated in Step (2) is used as the rankingbasis, but the ES of first sentence and that of the lastsentence are doubled.
Again, the medium of theseESes is chosen to normalize these score.
Thesentences with normalized score higher than 0.5 areselected as the best summary in Step (6).
Because thelength of fixed-length summary cannot exceed the10% of the original text, Step (7) selects the topsentences that do not break this rule to form thefixed-length summary.5.
EXPERIMENT RESULTSIn general, the results are evaluated by assessors,and then measured by recall (R), precision (P), F-score (F) and the normalized F-score (NormF).
Table1 shows the contingence table of the real answeragainst he assessors.I RealAnswerGiven Answer by AssessorsTP FNFP TNTable 1.
Contingence TableThe meanings of TP, FP, FN, and TN are shown inthe following:?
TP : Decides relevant, relevant is correct = truepositive?
FP : Decides relevant, relevant is incorrect = falsepositive1670.70.60.50.4 ~d80.30.20.10IIA.FSBI ?
t I tA.NFB A.FSF A.NFF C.FSB C.NFBI IC.FSF C.NFFFigure 3.
The performance of our system$ FN : Decides irrelevant, relevant is correct = falsenegative?
TN : Decides irrelevant, irrelevant is correct =true negativeThe aforementioned measures for evaluation basedon Table 1 are shown in the following:?
Precision (P) = (TP/(TP+FP))?
Recall (R) = (TP/TP+FN)?
F-score (F) = (2*P*R/(P+R))Each group could provide up to two kinds ofsummary.
One is the fixed-length summary and theother is the best summary.
In order to level off theeffect of length of summary, compression factor isintroduced to normalize the F-score.?
Compression (C) = (Summary Length/Full TextLength)?
NormF = ((1-C)*F)Table 2 shows the result of our adhoc summarytask.
Table 3 shows the result of our categorizationsummary task.
The NormF of the best summary andthat of the fixed summary for adhoc tasks are 0.456and 0.447, respectively.
In comparison to othersystems, the performance of our system is not good.One reason is that we have not developed anappropriate method to determine the threshold forselection of sentence.
Besides, we are the only oneteam not from Indo-European language family.
Thismaybe has some impacts on the performance.However, considering the time factor, our systemperform much better than many systems.The NormF of the best summary and that of thefixed summary for categorization task are 0.4090 and0.4023, respectively.
Basically, this task is like thetraditional categorization problem.
Our systemperforms much well.
However, there is no significantdifference among all participating systems.Table 4 shows our system's performance againstaverage performance of all systems.
Although somemeasures of our performance are worse than thatthose of the average performance, the difference isnot very significant.
In categorization task, weoutperform the average performance of all systems.Table 5 is the standard deviation of all systems.Essentially, the difference of all systems is notsignificant.
Figure 3 shows each measure ofperformance for our system.
Figure 4 shows oursystem against the best system.A.FSBA.NFBA.FSFA.NFFF-Score Best summary 0.6090NormF Best summary 0.4560F-Score Fixed summary 0.4850NormF Fixed summary 0.4470Table 2.
Result of AdhocC.FSB F-Score Best summaryC.NFB NormF Best summaryC.FSFC.NFF0.50850.4090F-Score Fixed summary 0.4470NormF Fixed summary 0.4023Table 3.
Result of CategorizationA.FSB -0.040 C.FSB +0.0045A.NFB -0.064 C.NFB +0.0140A.FSF -0.054 C.FSF +0.0120A.NFF -0.067 C.NFF -0.0057Table 4.
Performance against AverageA.FSB 0.0451A.NFB 0.0420A.FSF 0.0438A.NFF 0.0379C.FSB 0.0203C.NFB 0.0202C.FSF 0.0211C.NFF 0.0182Table 5.
Standard Deviation of All systems1680.90.80.70.60.50.40.30.20.10A.FSB A.NFB A.FSF A.NFF C.FSB C.NFB C.FSF C.NFFFigure 4.
Comparison with the best participantSUMMAC also conducts a series of baselineexperiments to compare the system performance.From the report of these experiments, we find that forcategorization task, the fixed-length summary ispretty good enough.
For adhoc task, the bestsummary will do the better job.
Another importantfinding is that the assessors are highly inconsistent.How to find out a fair and consistent evaluationmethodology is worth further investigating.6.
CONCLUDING REMARKSThis paper proposes models to generate summaryfor two different applications.
The first is to producegeneric summaries, which do not take the user'sinformation need into account.
The second is toproduce summaries, while the user's informationneed is an important issue.
That is to say, theautomatic summarization system interacts with usersand takes user's query as a clue to produce user-oriented summaries.
In addition, our approach isextract-based, which generates summaries using thesentences extracted from original texts.
For thecategorization task, the positive feature vector andthe negative feature vector trained from theSUMMAC-1 texts are used as the comparative basisfor sentence selection to produce generic summaries.As to adhoc task, the ES of each sentence iscalculated based on the interaction of nouns andverbs.
Then, the nouns of a query are compared withnouns in sentences and the closely related sentencesare selected to form the summary.
The result showsthat the NormF of the best summary and that of thefixed summary for adhoc tasks are 0.456 and 0.447,respectively.
The NorrnF of the best summary andthat of the fixed summary for categorization task are0.4090 and 0.4023, respectively.
Our systemoutperforms the average system in categorizationtask but does a common job in adhoc task.
We thinkthat there are many further works to be studied in thefuture, e.g., extending the proposed approach to otherlanguages, optimizing parameters of the proposedmodel, investigating the impact of errors introducedin tagging step, and developing a appropriate methodto setup the threshold for sentence selection.REFERENCES\[l\] Bian, Guo-Wei and Chen, Hsin-Hsi (1997) "AnMT Meta-Server for Information Retrieval onWWW."
Natural Language Processing for theWorld Wide Web, AAAI-97 Spring Symposium,10-16.\[2\] Chen, Kuang-hua and Chen, Hsin-Hsi (1994)"Extracting Noun Phrases from Large-ScaleTexts: A Hybrid Approach and Its AutomaticEvaluation."
Proceedings of the 32nd AnnualMeeting of the Association for ComputationalLinguistics (ACL94), New Mexico, USA, June27-July 1,234-241.\[3\] Chen, Kuang-hua (1995) "Topic Identificationin Discourse."
Proceedings of the 7 thConference of the European Chapter of ACL,267-271.\[4\] Appelt, D.E.
and Israel, D. (1997) Tutorial onBuilding Information Extraction Systems,Washington, DC.\[5\] Weibel, S.; Godby, J. and Miller, E. (1995)OCLC/NCSA Metadata Workshop Report,(http://gopher.sil.org/sgml/metadata.html).\[6\] Chen, Hsin-Hsi et al (1998) "Description ofthe NTU System Used for MET 2.
"Proceedings of the MUC-7 Conference,forthcoming.\[7\] Rush, J.E.
; Salvador, R. and Zamora, A.
(1971)"Automatic Abstracting and Indexing.Production of Indicative Abstracts byApplication of Contextual Inference andSyntactic Coherence Criteria."
Journal ofAmerican Society for Information Sciences,22(4), 260-274.\[8\] Chen, Kuang-hua and Chen, Hsin-Hsi.
(1995)"A Corpus-Based Approach to Text Partition.
"169Proceedings of the Workshop of RecentAdvances in Natural Language Processing,Sofia, Bulgaria, 152-161.
[9] Sparck Jones, Karen (1972) "A StatisticalInterpretation of Term Specificity and ItsApplication in Retrieval."
Journal ofDocumentation, 28(1), 11-21.
[10] Church, K.W.
and Hanks, P. (1990) "WordAssociation Norms, Mutual Information, andLexicography."
Computational Linguistics,16(1), 22-29.
[11] Jelinek, E (1985) "Markov Source Modelingof Text Generation."
In J.K. Skwirzynski (ed.
),The Impact of Processing Techniques onCommunication, Nijhoff, Dordrecht, TheNetherlands.170
