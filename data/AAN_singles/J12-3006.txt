Tree-Adjoining Grammars Are NotClosed Under Strong LexicalizationMarco Kuhlmann?Uppsala UniversityGiorgio Satta?
?University of PaduaA lexicalized tree-adjoining grammar is a tree-adjoining grammar where each elementary treecontains some overt lexical item.
Such grammars are being used to give lexical accounts ofsyntactic phenomena, where an elementary tree defines the domain of locality of the syntacticand semantic dependencies of its lexical items.
It has been claimed in the literature that forevery tree-adjoining grammar, one can construct a strongly equivalent lexicalized version.
Weshow that such a procedure does not exist: Tree-adjoining grammars are not closed under stronglexicalization.1.
IntroductionMany contemporary linguistic theories give lexical accounts of syntactic phenomena,where complex syntactic structures are analyzed as the combinations of elementarystructures taken from a finite lexicon.
In the computational linguistics community, thistrend has been called lexicalization, and has been extensively investigated since the1990s.
From a mathematical perspective, the main question that arises in the context oflexicalization is whether the restriction of a given class of grammars to lexicalized formhas any impact on the generative or computational properties of the formalism.As a simple example, consider the class of context-free grammars (CFGs).
Recallthat a CFG is in Greibach normal form if the right-hand side of every rule in the gram-mar starts with a terminal symbol, representing an overt lexical item.
Although severalprocedures for casting a CFG in Greibach normal form exist, all of them substantiallyalter the structure of the parse trees of the source grammar.
In technical terms, theseprocedures provide a weak lexicalization of the source grammar (because the stringlanguage is preserved) but not a strong lexicalization (because the sets of parse treesthat the two grammars assign to the common string language are not the same).
Stronglexicalization is highly relevant for natural language processing, however, where theparse tree assigned by a grammar represents the syntactic analysis of interest, and isused by other modules such as semantic interpretation or translation.
In this article, weinvestigate the problem of strong lexicalization.?
Department of Linguistics and Philology, Box 635, 75126 Uppsala, Sweden.E-mail: marco.kuhlmann@lingfil.uu.se.??
Department of Information Engineering, via Gradenigo 6/A, 35131 Padova, Italy.E-mail: satta@dei.unipd.it.Submission received: 16 July 2011; accepted for publication: 10 September 2011.?
2012 Association for Computational LinguisticsComputational Linguistics Volume 38, Number 3Two important results about strong lexicalization have been obtained by Schabes(1990).
The first result is that CFGs are not closed under strong lexicalization.
(Theauthor actually shows a stronger result involving a formalism called tree substitutiongrammar, as will be discussed in detail in Section 3.)
Informally, this means that wecannot cast a CFG G in a special form in which each rule has an overt lexical item in itsright-hand side, under the restriction that the new grammar generates exactly the sameset of parse trees as G. As a special case, this entails that no procedure can cast a CFGin Greibach normal form, under the additional condition that the generated parse treesare preserved.The second result obtained by Schabes concerns the relation between CFGs and theclass of tree-adjoining grammars (TAGs) (Joshi, Levy, and Takahashi 1975; Joshi andSchabes 1997).
A TAG consists of a finite set of elementary trees, which are phrasestructure trees of unbounded depth, and allows for the combination of these trees bymeans of two operations called substitution and adjunction (described in more detail inthe next section).
A lexicalized TAG is one where each elementary tree contains at leastone overt lexical item called the anchor of the tree; the elementary tree is intended toencapsulate the syntactic and semantic dependencies of its anchor.
Because CFG rulescan be viewed as elementary trees of depth one, and because context-free rewriting canbe simulated by the substitution operation defined for TAGs, we can view any CFG asa special TAG.
Under this view, one can ask whether lexicalized TAGs can provide astrong lexicalization of CFGs.
Schabes?
second result is that this is indeed the case.
Thismeans that, given a CFG G, one can always construct a lexicalized TAG generating thesame set of parse trees as G, and consequently the same string language.Following from this result, there arose the possibility of establishing a third result,stating that TAGs are closed under strong lexicalization.
Schabes (1990) states that thisis the case, and provides an informal argument to justify the claim.
The same claimstill appears in two subsequent publications (Joshi and Schabes 1992, 1997), but noprecise proof of it has appeared until now.
We speculate that the claim could be dueto the fact that adjunction is more powerful than substitution with respect to weakgenerative capacity.
It turns out, however, that when it comes to strong generativecapacity, adjunction also shares some of the restrictions of substitution.
This observationleads to the main result of this article: TAGs are not closed under strong lexicalization.In other words, there are TAGs that lack a strongly equivalent lexicalized version.In the same line of investigation, Schabes and Waters (1995) introduce a restrictedvariant of TAG called tree insertion grammars (TIGs).
This formalism severely restrictsthe adjunction operation originally defined for TAGs, in such a way that the class ofgenerated string languages, as well as the class of generated parse trees, are the sameas those of CFGs.
Schabes and Waters then conjecture that TIGs are closed under stronglexicalization.
In this article we also disprove their conjecture.2.
PreliminariesWe assume familiarity with the TAG formalism; for a survey, we refer the reader toJoshi and Schabes (1997).
We briefly introduce here the basic terminology and notationfor TAG that we use in this article.2.1 Basic DefinitionsA TAG is a rewriting system that derives trees starting from a finite set of elementarytrees.
Elementary trees are trees of finite but arbitrary depth, with internal nodes labeled618Kuhlmann and Satta TAGs Are Not Closed Under Strong Lexicalizationwith nonterminal symbols and frontier nodes labeled with terminal and nonterminalsymbols.
Each elementary tree is either an initial tree or else an auxiliary tree.
Initialtrees serve as the starting point for derivations, and may combine with other trees bymeans of an operation called substitution.
Tree substitution replaces a node labeledwith a nonterminal A in the frontier of some target tree with an initial tree whoseroot is labeled with A.
The nodes that are the target of the substitution operation areidentified by a down arrow (?).
The substitution operation is illustrated in the left halfof Figure 1.Auxiliary trees are elementary trees in which a special node in the frontier has thesame nonterminal label as the root node.
This special node is called the foot node and isidentified by an asterisk (?).
Auxiliary trees may combine with other trees by means ofan operation called adjunction.
The adjunction operation entails splitting some targettree at an internal node with label A, and inserting an auxiliary tree whose root (andfoot) node is labeled with A.
The adjunction operation is illustrated in the right half ofFigure 1.A derivation in a TAG can be specified by a derivation tree d; this is a rootedtree whose nodes are labeled with (instances of) elementary trees, and whose edgesare labeled with (addresses of) nodes at which substitution or adjunction takes place.More specifically, an edge v ?u v?
in d represents the information that the elementarytree at v?
is substituted at or adjoined into node u of the elementary tree at v. Whenwe combine the elementary trees of our TAG as specified by d, we obtain a (unique)phrase structure tree called the derived tree associated with d, which we denoteas t(d).We use the symbol ?
as a variable ranging over elementary trees, ?
as a variableranging over initial trees, and ?
as a variable ranging over auxiliary trees.
We alsouse the symbols u and v as variables ranging over nodes of generic trees (elementary,derived, or derivation trees).
For an elementary tree ?, a derivation tree d is said to havetype ?
if the root node of d is labeled with ?.
A derivation tree d is called sentential if d isof some type ?, and the root node of ?
is labeled with the start symbol of the grammar,denoted as S.A node u in an elementary tree ?may be annotated with an adjunction constraint,which for purposes here is a label in the set {NA,OA}.
The label NA denotes NullAdjunction, forbidding adjunction at u; the label OA denotes Obligatory Adjunction,forcing adjunction at u.
A derivation tree d is called saturated if, at each node v of dthere is an arc v ?u v?, for some v?, for every node u of the elementary tree at v thatrequires substitution or is annotated with an OA constraint.For a TAGG, we denote by T(G) the set of all the derived trees t such that t = t(d) forsome sentential and saturated derivation tree d obtained in G. Each such derived tree isFigure 1Combination operations in TAG.619Computational Linguistics Volume 38, Number 3(uniquely) associated with a string y(t) called the yield of t, obtained by concatenatingall terminal symbols labeling the frontier of t, from left to right.
The string languagegenerated by G is the setL(G) = { y(t) | t ?
T(G) }A TAG G is said to be finitely ambiguous if, for every string w ?
L(G), the subset ofthose trees in T(G) that have w as their yield is finite.An elementary tree ?
of G is called useless if ?
never occurs in a sentential and sat-urated derivation tree of G, that is, if no sentential and saturated derivation of G uses ?.A grammar G is called reduced if none of its elementary trees is useless.
Throughoutthis article we shall assume that the grammars that we deal with are reduced.2.2 LexicalizationIn a tree, a node labeled with a terminal symbol is called a lexical node.
A TAG iscalled lexicalized if each of its elementary trees has at least one lexical node.
Observethat a lexicalized grammar cannot generate the empty string, denoted by ?, becauseevery derived tree yields at least one lexical element.
Similarly, a lexicalized grammaris always finitely ambiguous, because the length of the generated strings provides anupper bound on the size of the associated derived trees.
Let G and G?
be two subclassesof the class of all TAGs.
We say that G?
strongly lexicalizes G, if, for every grammarG ?
G that is finitely ambiguous and that satisfies ?
?
L(G), there exists a lexicalizedgrammar G?
?
G?
such that T(G?)
= T(G).
We also say that G is closed under stronglexicalization if the class G strongly lexicalizes itself.Using this terminology, we can now restate the two main results obtained bySchabes (1990) about strong lexicalization for subclasses of TAGs, already mentioned inthe Introduction.
The first result states that the class of CFGs is not closed under stronglexicalization.
Here we view a CFG as a special case of a TAG using only substitutionand elementary trees of depth one.
Informally, this means that we cannot cast a CFGG in a special form in which each rule has an overt lexical item in its right-hand side,under the restriction that the new grammar generates exactly the same tree set as G. Thesecond result is that the class of TAGs strongly lexicalizes the class of tree substitutiongrammars (TSGs).
The latter class is defined as the class of all TAGs that use substitutionas the only tree combination operation, and thus includes all context-free grammars.This means that, given a TSG or a CFG G, we can always construct a TAG that islexicalized and that generates exactly the same tree set as G.3.
Tree Substitution Grammars Are Not Closed Under Strong LexicalizationBefore turning to our main result in Section 4, we find it useful to technically revisit therelated result for TSGs.Theorem 1Tree substitution grammars are not closed under strong lexicalization.To prove this result, Schabes (1990) uses a proof by contradiction: The author considers aspecific TSGG1, reported in Figure 2.
It is not difficult to see thatG1 is finitely ambiguousand that ?
?
L(G1).
The author then assumes that G1 can be lexicalized by another TSG,620Kuhlmann and Satta TAGs Are Not Closed Under Strong LexicalizationFigure 2The counterexample tree substitution grammar G1.and derives a contradiction.
We provide here an alternative, direct proof of Theorem 1.This alternative proof will be generalized in Section 4 to obtain the main result of thisarticle.We use the following notation.
For a derived tree t and a terminal symbol a, wewrite Nodes(a, t) to denote the set of all nodes in t that are labeled with a. Furthermore,for a node u of t we write depth(u, t) to denote the length of the unique path from theroot node of t leading to u.3.1 IntuitionIn order to convey the basic idea behind Schabes?s proof and our alternative versionherein, we first consider a specific candidate grammar for the lexicalization of G1.
Forexample, one might think that the following TSG G?1 lexicalizes G1:This grammar is obtained from G1 by taking the lexicalized tree ?1, as well as everyelementary tree that can be obtained by substituting ?1 into the non-lexicalized tree ?2.The grammar G?1 only generates a subset of the trees generated by G1, however.
Thefollowing tree, for example, cannot be generated by G?1:To see this, we reason as follows.
Consider a lexical node v in an elementary tree ?of G?1, and let t be a tree obtained by substituting some elementary tree into ?.
Becausesubstitution takes place at the frontier of ?, depth(v, t) must be the same as depth(v,?
).More generally, the depth of a lexical node in an elementary tree ?
is the same in all treesderived starting from ?.
Because the maximal depth of a lexical node in an elementary621Computational Linguistics Volume 38, Number 3tree of G?1 is 2, we deduce that every tree generated by G?1 contains a lexical node withdepth at most 2.
In contrast, all lexical nodes in the tree t1 have depth 3.
Therefore thetree t1 is not generated by G?1.3.2 Main PartWe now generalize this argument to arbitrary candidate grammars.
For this, we areinterested in the following class G1 of all (reduced) TSGs that derive a subset of the treesderived by G1:G1 = {G | G is a TSG, T(G) ?
T(G1) }For a grammar G ?
G1, we define the d-index of G as the maximum in N ?
{?}
of theminimal depths of a-labeled nodes in trees derived by G:d-index(G) = maxt?T(G)minv?Nodes(a,t)depth(v, t)Note that, for two grammars G,G?
?
G1, T(G) = T(G?)
implies that G and G?
have thesame d-index.
This means that two grammars in G1 with different d-indices cannot gen-erate the same tree language.
Then Theorem 1 directly follows from the two statementsin the next lemma.Lemma 1The grammar G1 has infinite d-index.
Every lexicalized grammar in G1 has finited-index.ProofThe first statement is easy to verify: Using longer and longer derivations, the mini-mal depth of an a-labeled node in the corresponding tree can be pushed beyond anybound.To prove the second statement, let G be a lexicalized grammar in G1, and lett ?
T(G).
The tree t is derived starting from some initial tree; call this tree ?.
Because Gis lexicalized, at least one of the a-labeled nodes in Nodes(a, t) is contributed by ?.
Let vabe any such node in t, and let ua be the node of ?
that corresponds to va. Rememberthat the only tree combination operation allowed in a TSG derivation is substitution.Because substitution can only take place at the frontier of a derived tree, we mustconclude that depth(va, t) = depth(ua,?).
There are only finitely many initial trees in G,therefore depth(ua,?)
must be upper bounded by some constant depending only on G,and the same must hold for depth(va, t).
Lastly, because t has been arbitrarily chosen inT(G), we must conclude that d-index(G) is finite.
3.3 Lexicalization of Tree Substitution GrammarsWhat we have just seen is that lexicalized TSGs are unable to derive the tree structuresgenerated by the grammar G1 in Figure 2.
This is essentially because tree substitutioncannot stretch the depth of a lexical node in an elementary tree.
In contrast, tree adjunc-tion allows the insertion of additional structure at internal nodes of elementary trees,622Kuhlmann and Satta TAGs Are Not Closed Under Strong Lexicalizationand enables TAGs to provide a strong lexicalization of TSGs.
For example, the followingTAG G?
?1 lexicalizes G1.Note that this grammar looks almost like G?1, except that adjunction now is allowed atinternal nodes, and substitution nodes have become foot nodes.
The following deriva-tion tree witnesses that the tree t1 can be derived in G?
?1 .
We write 0 to denote the rootnode of an elementary tree, and 1 to denote its leftmost child.
?6 ?0 ?1 ?0 ?1 ?1 ?1Schabes (1990) provides a general procedure for constructing a lexicalized TAG for agiven context-free grammar.4.
Tree-Adjoining Grammars Are Not Closed Under Strong LexicalizationIn this section we develop the proof of the main result of this article.Theorem 2Tree-adjoining grammars are not closed under strong lexicalization.4.1 Proof IdeaThe basic idea underlying the proof of Theorem 2 is essentially the same as the one usedin the proof of Theorem 1 in Section 3.
Some discussion of this issue is in order at thispoint.
In the previous section, we have seen that adjunction, in contrast to substitution,allows the insertion of additional structure at internal nodes of elementary trees, andenables TAGs to provide a strong lexicalization of TSGs.
One might now be tempted tobelieve that, because the depth-based argument that we used in the proof of Lemma 1can no longer be applied to TAGs, they might be closed under strong lexicalization.There is a perspective under which adjunction quite closely resembles substitution,however.
Let us first look at substitution as an operation on the yield of the derivedtree.
Under this view, substitution is essentially context-free rewriting: It replaces a non-terminal symbol in the yield of a derived tree with a new string consisting of terminalsand nonterminals, representing the yield of the tree that is substituted.
Under the sameperspective, adjunction is more powerful than tree substitution, as is well known.
Butjust as substitution can be seen as context-free rewriting on tree yields, adjunction canbe seen as context-free rewriting on the paths of trees: It replaces a nonterminal symbolin some path of a derived tree with a string representing the spine of the tree that isadjoined?the unique path from the root node of the tree to the foot node.This observation gives us the following idea for how to lift the proof of Theorem 1to TAGs.
We will specify a TAG G2 such that the paths of the derived trees of G2 encodein a string form the derived trees of the counterexample grammar G1.
This encodingis exemplified in Figure 3.
Each internal node of a derived tree of G1 is represented in623Computational Linguistics Volume 38, Number 3Figure 3A derived tree of G1, and the corresponding encoding, drawn from left to right.
Everyinternal node of the original tree is represented by a pair of matching brackets [S (, )S].The correspondence is indicated by the numerical subscripts.the spine of the corresponding derived tree of G2 as a pair of matching brackets.
Byour encoding, any TAG generating trees from T(G2) will have to exploit adjunction atnodes in the spine of its elementary trees, and will therefore be subject to essentially thesame restrictions as the grammar G1 which used substitution at nodes in the yield.
Thiswill allow us to lift our argument from Lemma 1.
The only difference is that instead ofworking with the actual depth of a lexical node in a tree t ?
T(G2), we will now needto work with the depth of the node in the encoded tree.
As will be explained later, thismeasure can be recovered as the excess of left parentheses over right parentheses in thespine above the lexical node.4.2 PreliminariesAs alreadymentioned, our proof of Theorem 2 follows the same structure as our proof ofTheorem 1.
As our counterexample grammar, we use the grammar G2 given in Figure 4;this grammar generates the encodings of the derived trees of G1 that we discussedpreviously.
Note that the left parenthesis symbol ?(?
and the right parenthesis symbol?)?
are nonterminal symbols.
As with the grammar G1 before, it is not difficult to seethat G2 is finitely ambiguous and that ?
/?
L(G2).Figure 4The counterexample TAG G2.624Kuhlmann and Satta TAGs Are Not Closed Under Strong LexicalizationGrammar G2 derives trees that we call right spinal: Each node in such a tree hasat most two children, and the left child of every node with two children is always aleaf node.
The path from the root node of a right spinal tree t to the rightmost leafof t is called spine.
To save some space, in the following we write right spinal treeshorizontally and from left to right, as already done in Figure 3.
Thus the grammar G2can alternatively be written as follows:For a node u in a right spinal tree derived by G2, we definec(u) =????
?+1 if u is labeled with (0 if u is labeled with S or a?1 if u is labeled with )We exploit this function to compute the excess of left parentheses over right parenthesesin a sequence of nodes, and write:excess(?u1, .
.
.
,un?)
=n?i=1c(ui)Let t be some right spinal tree in T(G2), and let v be some node in t. Assume that?u1, .
.
.
,un = v?
is the top?down sequence of all the nodes in the path from t?s root u1to v. We write excess(v, t) as a shorthand notation for excess(?u1, .
.
.
,un?).
If ?u1, .
.
.
,un?is the top?down sequence of all the nodes in the spine of t, we also write excess(t) as ashort hand notation for excess(?u1, .
.
.
,un?
).It is easy to prove by induction that, for each tree t ?
T(G2), the excess of thesequence of nodes in the spine of t is always zero.
Thus, we omit the proof of thefollowing statement.Lemma 2Every derived tree t ?
T(G2) is a right spinal tree, and excess(t) = 0.In order to get a better understanding of the construction used in the followingproofs, it is useful at this point to come back to our discussion of the relation betweenthat construction and the construction presented in Section 3.
We observe that for eachtree t1 generated by G1 there is a tree t2 ?
T(G2) such that the sequence of labels in t2?sspine encodes t1, following the scheme exemplified in Figure 3.
Using such encoding,we can establish a bijection between the a-labeled nodes in the frontier of t1 and thea-labeled nodes in the frontier of t2.
Furthermore, if v1 in t1 and v2 in t2 are twonodes related by such a correspondence, then it is not difficult to see that depth(v1, t1) =excess(v2, t2).4.3 IntuitionBefore we give the actual proof of Theorem 2, let us attempt to get some intuitionabout why our counterexample grammar G2 cannot be strongly lexicalized by some625Computational Linguistics Volume 38, Number 3other TAG.
One might think that the following TAG G?2 is a lexicalized versionof G2:This grammar is obtained from G2 by taking the lexicalized tree ?3 (repeated hereas ?5), as well as all trees that can be obtained by adjoining ?3 into some non-lexicalizedelementary tree.
G?2 does not generate all trees generated by G2, however.
The followingtree t2 for example is not generated by G?2:Note that this tree is the encoded version of the counterexample tree t1 from the previoussection (cf.
Figure 3).To see that t2 is not generated by G?2, we reason as follows.
Consider a lexical node uin an elementary tree ?
of G?2, and let t be a tree obtained by adjoining some elementarytree into ?.
Although this adjunction increases the depth of u, it does not increase itsexcess, as it adds a balanced sequence of parentheses into the spine of ?.
More generally,the excess of a lexical node in an elementary ?
is constant in all trees derived startingfrom ?.
From this we conclude that every tree generated by G?2 contains a lexical nodewith excess at most 2; this is the maximal excess of a lexical node in an elementary treeof G?2.
In contrast, all lexical nodes in the tree t2 have excess 3.
This shows that t2 is notgenerated by G?2.4.4 Main PartIn what follows, we consider the class G2 of (reduced) TAGs that generate subsets of thetrees derived by G2:G2 = {G | G is a TAG, T(G) ?
T(G2) }For a grammar G ?
G2, we define the e-index of G as the maximum in N ?
{?}
of theminimal excess of a-labeled nodes in trees derived by G:e-index(G) = maxt?T(G)minv?Nodes(a,t)excess(v, t)As we will see, the notion of e-index plays exactly the same role as the notion of d-indexin Section 3.626Kuhlmann and Satta TAGs Are Not Closed Under Strong LexicalizationThere is one last obstacle that we need to overcome.
For TSGs we noted (in the proofof Lemma 1) that the minimal depth of lexical nodes in a derived tree t is bounded bythe minimal depth of lexical nodes in the elementary tree ?
from which t was derived.For the TAGs in G2, the situation is not quite as simple, as an adjunction of an auxiliarytree ?
into an elementary tree ?
might affect the excess of a lexical node of ?.
It turnsout, however, that this potential variation in the excess of a lexical node of ?
is boundedby a grammar-specific constant.
This observation is expressed in the following lemma.It is the correspondent of Lemma 4 in Knuth?s paper on parenthesis languages (Knuth1967), and is proved in essentially the same way.
Recall that a derivation tree d is oftype ?, ?
some elementary tree, if d is derived starting from ?.Lemma 3Let G ?
G2.
For each elementary tree ?
of G, there exists a number e(?)
such that, forevery saturated derivation tree d of type ?, excess(t(d)) = e(?
).ProofBecause ?
is not useless, we can find at least one sentential and saturated derivation treeof G that contains an occurrence of ?.
Let d be any such derivation tree, and let v be anynode of d labeled with ?.
Let d1 be the subtree of d rooted at v. Observe that t(d1) mustbe a spinal tree.
We then let e(?)
= excess(t(d1)).If d1 is the only derivation tree of type ?
available inG, then we are done.
Otherwise,let d2 	= d1 be some derivation tree of type ?
occurring within some other sententialand saturated derivation tree of G. We can replace d1 with d2 in d at v to obtain a newsentential and saturated derivation tree d?
= d. Every derived tree in T(G) must be aright spinal tree: This follows from the assumption that G ?
G2 and from Lemma 2.
Wecan then writeexcess(t(d?))
= excess(t(d))?
excess(t(d1))+ excess(t(d2))Because excess(t(d)) = 0 and excess(t(d?))
= 0 (by Lemma 2), we conclude thatexcess(t(d2)) = excess(t(d1)) = e(?
)Using Lemma 3, we can now prove the following result.Lemma 4The grammarG2 has infinite e-index.
Every lexicalized grammar in G2 has finite e-index.ProofAs in the case of Lemma 1, the first statement is easy to verify and we omit its proof.
Toprove the second statement, let G ?
G2.
Let ?
be the set of all elementary trees of G, andlet s be the maximal number of nodes in an elementary tree in ?.
We show thate-index(G) ?
k , where k = s+ s ?max???|e(?
)|Note that k is a constant that only depends on G.627Computational Linguistics Volume 38, Number 3Let d be a sentential and saturated derivation tree of G. It has the following shape:Here ?
is some initial tree, m ?
0, each ui is a node of ?
at which a tree combinationoperation takes place, each ?i is an elementary tree, and each di is a derivation tree oftype ?i that is a subtree of d. According to this derivation tree, the derived tree t(d) isobtained by substituting or adjoining the derived trees t(di) at the respective nodes uiof ?.Because G is lexicalized, at least one a-labeled node on the frontier of t(d) is con-tributed by ?.
Let va be any such node, and let ua be the node of ?
that correspondsto va.
The quantity excess(va, t(d)), representing the excess of the path in t(d) from itsroot to the node va, can be computed as follows.
Let ?u?1, .
.
.
,u?n = ua?
be the top?downsequence of nodes in the path from the root node of ?
to ua.
For each i with 1 ?
i ?
nwe definec?
(u?i ) ={excess(t(dj)) if u?i = uj for some 1 ?
j ?
mc(u?i ) otherwiseBecause G ?
G2 and because t(d) is a right spinal tree (Lemma 2), we can writeexcess(va, t(d)) =n?i=1c?
(u?i )By Lemma 3, we have excess(t(dj)) = e(?j), for each jwith 1 ?
j ?
m. We can then writeexcess(va, t(d)) ?
n+m?i=1|e(?i)| ?
s+ s ?max???|e(?
)| = kThus, every derived tree t in T(G) contains at least one node va in its frontier such thatexcess(va, t) ?
k. Therefore, e-index(G) ?
k. Two grammars in G2 that have a different e-index cannot generate the same tree lan-guage, thus we have concluded the proof of Theorem 2.5.
Tree Insertion Grammars Are Not Closed Under Strong LexicalizationAs mentioned earlier Schabes and Waters (1995) introduce a restricted variant of TAGcalled TIG.
The essential restriction in that formalism is the absence of wrapping trees,which are trees derived starting from auxiliary trees with overt lexical material on bothsides of the foot node.
Schabes and Waters (1995, Section 5.1.4) conjecture that the classof all TIGs is closed under strong lexicalization.628Kuhlmann and Satta TAGs Are Not Closed Under Strong LexicalizationIt is easy to see that the counterexample grammar G2 that we gave in Figure 4does not derive wrapping trees; this means that G2 actually is a TIG.
Using the proofof Section 4, we then obtain the following result.Theorem 3Tree insertion grammars are not closed under strong lexicalization.In fact, we have even proved the stronger result that the class of TAGs does not lexicalizethe class of TIGs.6.
ConclusionWe have shown that, in contrast to what has been claimed in the literature, TAGs are notclosed under strong lexicalization: The restriction to lexicalized TAGs involves a loss instrong generative capacity.In this article we have only considered TAGs with Null Adjunction and ObligatoryAdjunction constraints.
A third kind of adjunction constraint that has been used in theliterature is Selective Adjunction, where a set of trees is provided thatmay be adjoined atsome node.
It is not difficult to see that the proofs of Lemma 3, Lemma 4, and Theorem 3still hold if Selective Adjunction constraints are used.Our result triggers a number of follow-up questions.
First, are TAGs closed underweak lexicalization, defined in Section 1?
We know that, in the case of CFGs, this ques-tion can be answered affirmatively, because Greibach normal form is a special case oflexicalized form, and for every CFG there is a weakly equivalent grammar in Greibachnormal form.
But to our knowledge, no comparable result exists for TAG.
Second, ifTAGs cannot strongly lexicalize themselves, what would a grammar formalism looklike that is capable of providing strong lexicalization for TAGs?AcknowledgmentsWe are grateful to Aravind Joshi fordiscussion on previous versions of thisarticle and for helping us in shapingthe text in the Introduction of thecurrent version.
We also acknowledgethree anonymous reviewers for theirhelpful comments.ReferencesJoshi, Aravind K., Leon S. Levy, andMasako Takahashi.
1975.
Tree AdjunctGrammars.
Journal of Computer andSystem Sciences, 10(2):136?163.Joshi, Aravind K. and Yves Schabes.
1992.Tree-adjoining grammars and lexicalizedgrammars.
In Maurice Nivat andAndreas Podelski, editors, Tree Automataand Languages.
North-Holland,Amsterdam, pages 409?431.Joshi, Aravind K. and Yves Schabes.
1997.Tree-adjoining grammars.
In GrzegorzRozenberg and Arto Salomaa, editors,Handbook of Formal Languages, volume 3.Springer, Berlin, pages 69?123.Knuth, Donald E. 1967.
A characterizationof parenthesis languages.
Informationand Control, 11(3):269?289.Schabes, Yves.
1990.Mathematical andComputational Aspects of LexicalizedGrammars.
Ph.D. thesis, University ofPennsylvania, Philadelphia.Schabes, Yves and Richard C. Waters.1995.
Tree insertion grammar:A cubic-time parsable formalism thatlexicalizes context-free grammarswithout changing the trees produced.Computational Linguistics, 21(4):479?513.629
