Proceedings of the Workshop on Computational Approaches to Figurative Language, pages 21?28,Rochester, NY, April 26, 2007. c?2007 Association for Computational LinguisticsActive Learning for the Identification of Nonliteral Language ?Julia Birke and Anoop SarkarSchool of Computing Science, Simon Fraser UniversityBurnaby, BC, V5A 1S6, Canadajbirke@alumni.sfu.ca, anoop@cs.sfu.caAbstractIn this paper we present an active learn-ing approach used to create an annotatedcorpus of literal and nonliteral usagesof verbs.
The model uses nearly unsu-pervised word-sense disambiguation andclustering techniques.
We report on exper-iments in which a human expert is askedto correct system predictions in differentstages of learning: (i) after the last iter-ation when the clustering step has con-verged, or (ii) during each iteration of theclustering algorithm.
The model obtainsan f-score of 53.8% on a dataset in whichliteral/nonliteral usages of 25 verbs wereannotated by human experts.
In compari-son, the same model augmented with ac-tive learning obtains 64.91%.
We alsomeasure the number of examples requiredwhen model confidence is used to selectexamples for human correction as com-pared to random selection.
The results ofthis active learning system have been com-piled into a freely available annotated cor-pus of literal/nonliteral usage of verbs incontext.1 IntroductionIn this paper, we propose a largely automatedmethod for creating an annotated corpus of literal vs.nonliteral usages of verbs.
For example, given theverb ?pour?, we would expect our method to iden-tify the sentence ?Custom demands that cognac bepoured from a freshly opened bottle?
as literal, andthe sentence ?Salsa and rap music pour out of thewindows?
as nonliteral, which, indeed, it does.
?This research was partially supported by NSERC, Canada(RGPIN: 264905).
We would like to thank Bill Dolan, FredPopowich, Dan Fass, Katja Markert, Yudong Liu, and theanonymous reviewers for their comments.We reduce the problem of nonliteral languagerecognition to one of word-sense disambiguation(WSD) by redefining literal and nonliteral as twodifferent senses of the same word, and we adaptan existing similarity-based word-sense disambigua-tion method to the task of separating usages of verbsinto literal and nonliteral clusters.
Note that treat-ing this task as similar to WSD only means thatwe use features from the local context around theverb to identify it as either literal or non-literal.
Itdoes not mean that we can use a classifier trained onWSD annotated corpora to solve this issue, or useany existing WSD classification technique that re-lies on supervised learning.
We do not have any an-notated data to train such a classifier, and indeed ourwork is focused on building such a dataset.
Indeedour work aims to first discover reliable seed dataand then bootstrap a literal/nonliteral identificationmodel.
Also, we cannot use any semi-supervisedlearning algorithm for WSD which relies on reliablyannotated seed data since we do not possess any reli-ably labeled data (except for our test data set).
How-ever we do exploit a noisy source of seed data ina nearly unsupervised approach augmented with ac-tive learning.
Noisy data containing example sen-tences of literal and nonliteral usage of verbs is usedin our model to cluster a particular instance of a verbinto one class or the other.
This paper focuses on theuse of active learning using this model.
We suggestthat this approach produces a large saving of effortcompared to creating such an annotated corpus man-ually.An active learning approach to machine learn-ing is one in which the learner has the ability toinfluence the selection of at least a portion of itstraining data.
In our approach, a clustering algo-rithm for literal/nonliteral recognition tries to anno-tate the examples that it can, while in each iterationit sends a small set of examples to a human expertto annotate, which in turn provides additional ben-efit to the bootstrapping process.
Our active learn-21ing method is similar to the Uncertainty Samplingalgorithm of (Lewis & Gale, 1994) but in our caseinteracts with iterative clustering.
As we shall see,some of the crucial criticisms leveled against un-certainty sampling and in favor of Committee-basedsampling (Engelson & Dagan, 1996) do not apply inour case, although the latter may still be more accu-rate in our task.2 Literal vs. Nonliteral IdentificationFor the purposes of this paper we will take the sim-plified view that literal is anything that falls withinaccepted selectional restrictions (?he was forced toeat his spinach?
vs. ?he was forced to eat his words?
)or our knowledge of the world (?the sponge ab-sorbed the water?
vs. ?the company absorbed theloss?).
Nonliteral is then anything that is ?not lit-eral?, including most tropes, such as metaphors, id-ioms, as well as phrasal verbs and other anomalousexpressions that cannot really be seen as literal.
Weaim to automatically discover the contrast betweenthe standard set of selectional restrictions for the lit-eral usage of verbs and the non-standard set whichwe assume will identify the nonliteral usage.Our identification model for literal vs. nonliteralusage of verbs is described in detail in a previouspublication (Birke & Sarkar, 2006).
Here we pro-vide a brief description of the model so that the useof this model in our proposed active learning ap-proach can be explained.Since we are attempting to reduce the problemof literal/nonliteral recognition to one of word-sensedisambiguation, we use an existing similarity-basedword-sense disambiguation algorithm developed by(Karov & Edelman, 1998), henceforth KE.
The KEalgorithm is based on the principle of attraction:similarities are calculated between sentences con-taining the word we wish to disambiguate (the targetword) and collections of seed sentences (feedbacksets).
It requires a target set ?
the set of sentencescontaining the verbs to be classified into literal ornonliteral ?
and the seed sets: the literal feedbackset and the nonliteral feedback set.
A target set sen-tence is considered to be attracted to the feedback setcontaining the sentence to which it shows the highestsimilarity.
Two sentences are similar if they containsimilar words and two words are similar if they arecontained in similar sentences.
The resulting transi-tive similarity allows us to defeat the knowledge ac-quisition bottleneck ?
i.e.
the low likelihood of find-ing all possible usages of a word in a single corpus.Note that the KE algorithm concentrates on similari-ties in the way sentences use the target literal or non-literal word, not on similarities in the meanings ofthe sentences themselves.Algorithms 1 and 2 summarize our approach.Note that p(w, s) is the unigram probability of wordw in sentence s, normalized by the total number ofwords in s. We omit some details about the algo-rithm here which do not affect our discussion aboutactive learning.
These details are provided in a pre-vious publication (Birke & Sarkar, 2006).As explained before, our model requires a targetset and two seed sets: the literal feedback set andthe nonliteral feedback set.
We do not explain thedetails of how these feedback sets were constructedin this paper, however, it is important to note that thefeedback sets themselves are noisy and not carefullyvetted by human experts.
The literal feedback setwas built from WSJ newswire text, and for the non-literal feedback set, we use expressions from vari-ous datasets such as the Wayne Magnuson EnglishIdioms Sayings & Slang and George Lakoff?s Con-ceptual Metaphor List, as well as example sentencesfrom these sources.
These datasets provide lists ofverbs that may be used in a nonliteral usage, but wecannot explicitly provide only those sentences thatcontain nonliteral use of that verb in the nonliteralfeedback set.
In particular, knowing that an expres-sion can be used nonliterally does not mean that youcan tell when it is being used nonliterally.
In facteven the literal feedback set has noise from nonlit-eral uses of verbs in the news articles.
To deal withthis issue (Birke & Sarkar, 2006) provides automaticmethods to clean up the feedback sets during theclustering algorithm.
Note that the feedback sets arenot cleaned up by human experts, however the testdata is carefully annotated by human experts (detailsabout inter-annotator agreement on the test set areprovided below).
The test set is not large enough tobe split up into a training and test set that can supportlearning using a supervised learning method.The sentences in the target set and feedback setswere augmented with some shallow syntactic in-formation such as part of speech tags provided22Algorithm 1 KE-train: (Karov & Edelman, 1998) algorithm adapted to literal/nonliteral identificationRequire: S: the set of sentences containing the target word (each sentence is classified as literal/nonliteral)Require: L: the set of literal seed sentencesRequire: N : the set of nonliteral seed sentencesRequire: W: the set of words/features, w ?
s means w is in sentence s, s 3 w means s contains wRequire: ?
: threshold that determines the stopping condition1: w-sim0(wx, wy) := 1 if wx = wy, 0 otherwise2: s-simI0(sx, sy) := 1, for all sx, sy ?
S ?
S where sx = sy, 0 otherwise3: i := 04: while (true) do5: s-simLi+1(sx, sy) :=?wx?sx p(wx, sx)maxwy?sy w-simi(wx, wy), for all sx, sy ?
S ?
L6: s-simNi+1(sx, sy) :=?wx?sx p(wx, sx)maxwy?sy w-simi(wx, wy), for all sx, sy ?
S ?N7: for wx, wy ?
W ?W do8: w-simi+1(wx, wy) :={i = 0?sx3wx p(wx, sx)maxsy3wy s-simIi (sx, sy)else?sx3wx p(wx, sx)maxsy3wy{s-simLi (sx, sy), s-simNi (sx, sy)}9: end for10: if ?wx,maxwy{w-simi+1(wx, wy)?
w-simi(wx, wy)} ?
?
then11: break # algorithm converges in 1?
steps.12: end if13: i := i + 114: end whileby a statistical tagger (Ratnaparkhi, 1996) and Su-perTags (Bangalore & Joshi, 1999).This model was evaluated on 25 target verbs:absorb, assault, die, drag, drown, escape,examine, fill, fix, flow, grab, grasp, kick,knock, lend, miss, pass, rest, ride, roll,smooth, step, stick, strike, touchThe verbs were carefully chosen to have vary-ing token frequencies (we do not simply learn onfrequently occurring verbs).
As a result, the tar-get sets contain from 1 to 115 manually annotatedsentences for each verb to enable us to measure ac-curacy.
The annotations were not provided to thelearning algorithm: they were only used to evaluatethe test data performance.
The first round of anno-tations was done by the first annotator.
The secondannotator was given no instructions besides a fewexamples of literal and nonliteral usage (not cov-ering all target verbs).
The authors of this paperwere the annotators.
Our inter-annotator agreementon the annotations used as test data in the experi-ments in this paper is quite high.
?
(Cohen) and ?
(S&C) on a random sample of 200 annotated exam-ples annotated by two different annotators was foundto be 0.77.
As per ((Di Eugenio & Glass, 2004), cf.refs therein), the standard assessment for ?
values isthat tentative conclusions on agreement exists when.67 ?
?
< .8, and a definite conclusion on agree-ment exists when ?
?
.8.In the case of a larger scale annotation effort, hav-ing the person leading the effort provide one or twoexamples of literal and nonliteral usages for each tar-get verb to each annotator would almost certainlyimprove inter-annotator agreement.The algorithms were evaluated based on howaccurately they clustered the hand-annotated sen-tences.
Sentences that were attracted to neither clus-ter or were equally attracted to both were put in theopposite set from their label, making a failure tocluster a sentence an incorrect clustering.Evaluation results were recorded as recall, preci-sion, and f-score values.
Literal recall is defined as(correct literals in literal cluster / total correct liter-als).
Literal precision is defined as (correct literalsin literal cluster / size of literal cluster).
If there areno literals, literal recall is 100%; literal precision is100% if there are no nonliterals in the literal clus-ter and 0% otherwise.
The f-score is defined as (2 ?23Algorithm 2 KE-test: classifying literal/nonliteral1: For any sentence sx ?
S2: if maxsy s-simL(sx, sy) >maxsys-simN (sx, sy)then3: tag sx as literal4: else5: tag sx as nonliteral6: end ifprecision ?
recall) / (precision + recall).
Nonliteralprecision and recall are defined similarly.
Averageprecision is the average of literal and nonliteral pre-cision; similarly for average recall.
For overall per-formance, we take the f-score of average precisionand average recall.We calculated two baselines for each word.
Thefirst was a simple majority-rules baseline (assigneach word to the sense which is dominant whichis always literal in our dataset).
Due to the imbal-ance of literal and nonliteral examples, this baselineranges from 60.9% to 66.7% for different verbs withan average of 63.6%.
Keep in mind though that us-ing this baseline, the f-score for the nonliteral setwill always be 0% ?
which is the problem we aretrying to solve in this work.
We calculated a secondbaseline using a simple attraction algorithm.
Eachsentence in the target set is attracted to the feedbackset with which it has the most words in common.For the baseline and for our own model, sentencesattracted to neither, or equally to both sets are putin the opposite cluster to which they belong.
Thissecond baseline obtains a f-score of 29.36% whilethe weakly supervised model without active learn-ing obtains an f-score of 53.8%.
Results for eachverb are shown in Figure 1.3 Active LearningThe model described thus far is weakly supervised.The main proposal in this paper is to push the re-sults further by adding in an active learning compo-nent, which puts the model described in Section 2 inthe position of helping a human expert with the lit-eral/nonliteral clustering task.
The two main pointsto consider are: what to send to the human annotator,and when to send it.We always send sentences from the undecidedcluster ?
i.e.
those sentences where attraction toeither feedback set, or the absolute difference ofthe two attractions, falls below a given threshold.The number of sentences falling under this thresholdvaries considerably from word to word, so we ad-ditionally impose a predetermined cap on the num-ber of sentences that can ultimately be sent to thehuman.
Based on an experiment on a held-out setseparate from our target set of sentences, sendinga maximum of 30% of the original set was deter-mined to be optimal in terms of eventual accuracyobtained.
We impose an order on the candidate sen-tences using similarity values.
This allows the origi-nal sentences with the least similarity to either feed-back set to be sent to the human first.
Further, wealternate positive similarity (or absolute difference)values and values of zero.
Note that sending ex-amples that score zero to the human may not helpattract new sentences to either of the feedback sets(since scoring zero means that the sentence was notattracted to any of the sentences).
However, humanhelp may be the only chance these sentences have tobe clustered at all.After the human provides an identification for aparticular example we move the sentence not onlyinto the correct cluster, but also into the correspond-ing feedback set so that other sentences might beattracted to this certifiably correctly classified sen-tence.The second question is when to send the sentencesto the human.
We can send all the examples afterthe first iteration, after some intermediate iteration,distributed across iterations, or at the end.
Sendingeverything after the first iteration is best for coun-teracting false attractions before they become en-trenched and for allowing future iterations to learnfrom the human decisions.
Risks include sendingsentences to the human before our model has hada chance to make potentially correct decision aboutthem, counteracting any saving of effort.
(Karov &Edelman, 1998) state that the results are not likelyto change much after the third iteration and we haveconfirmed this independently: similarity values con-tinue to change until convergence, but cluster al-legiance tends not to.
Sending everything to thehuman after the third iteration could therefore en-tail some of the damage control of sending every-thing after the first iteration while giving the model24a chance to do its best.
Another possibility is tosend the sentences in small doses in order to gainsome bootstrapping benefit at each iteration i.e.
thecertainty measures will improve with each bit of hu-man input, so at each iteration more appropriate sen-tences will be sent to the human.
Ideally, this wouldproduce a compounding of benefits.
On the otherhand, it could produce a compounding of risks.
A fi-nal possibility is to wait until the last iteration in thehope that our model has correctly clustered every-thing else and those correctly labeled examples donot need to be examined by the human.
This imme-diately destroys any bootstrapping possibilities forthe current run, although it still provides benefits foriterative augmentation runs (see Section 4).A summary of our results in shown in Figure 1.The last column in the graph shows the averageacross all the target verbs.
We now discuss the vari-ous active learning experiments we performed usingour model and a human expert annotator.3.1 Experiment 1Experiments were performed to determine the besttime to send up to 30% of the sentences to the humanannotator.
Sending everything after the first iterationproduced an average accuracy of 66.8%; sending ev-erything after the third iteration, 65.2%; sending asmall amount at each iteration, 60.8%; sending ev-erything after the last iteration, 64.9%.
Going just bythe average accuracy, the first iteration option seemsoptimal.
However, several of the individual word re-sults fell catastrophically below the baseline, mainlydue to original sentences having been moved into afeedback set too early, causing false attraction.
Thisrisk was compounded in the distributed case, as pre-dicted.
The third iteration option gave slightly bet-ter results (0.3%) than the last iteration option, butsince the difference was minor, we opted for the sta-bility of sending everything after the last iteration.These results show an improvement of 11.1% overthe model from Section 2.
Individual results for eachverb are given in Figure 1.3.2 Experiment 2In a second experiment, rather than letting our modelselect the sentences to send to the human, we se-lected them randomly.
We found no significant dif-ference in the results.
For the random model to out-perform the non-random one it would have to selectonly sentences that our model would have clusteredincorrectly; to do worse it would have to select onlysentences that our model could have handled on itsown.
The likelihood of the random choices comingexclusively from these two sets is low.3.3 Experiment 3Our third experiment considers the effort-savings ofusing our literal/nonliteral identification model.
Themain question must be whether the 11.1% accuracygain of active learning is worth the effort the hu-man must contribute.
In our experiments, the hu-man annotator is given at most 30% of the sentencesto classify manually.
It is expected that the humanwill classify these correctly and any additional ac-curacy gain is contributed by the model.
Withoutsemi-supervised learning, we might expect that ifthe human were to manually classify 30% of the sen-tences chosen at random, he would have 30% of thesentences classified correctly.
However, in order tobe able to compare the human-only scenario to theactive learning scenario, we must find what the av-erage f-score of the manual process is.
The f-scoredepends on the distribution of literal and nonliteralsentences in the original set.
For example, in a setof 100 sentences, if there are exactly 50 of each, andof the 30 chosen for manual annotation, half comefrom the literal set and half come from the nonlit-eral set, the f-score will be exactly 30%.
We couldcompare our performance to this, but that would beunfair to the manual process since the sets on whichwe did our evaluation were by no means balanced.We base a hypothetical scenario on the heavy imbal-ance often seen in our evaluation sets, and suggesta situation where 96 of our 100 sentences are literaland only 4 are nonliteral.
If it were to happen that all4 of the nonliteral sentences were sent to the human,we would get a very high f-score, due to a perfectrecall score for the nonliteral cluster and a perfectprecision score for the literal cluster.
If none of thefour nonliteral sentences were sent to the human, thescores for the nonliteral cluster would be disastrous.This situation is purely hypothetical, but should ac-count for the fact that 30 out of 100 sentences an-notated by a human will not necessarily result in anaverage f-score of 30%: in fact, averaging the re-sults of the three sitatuations described above results25Figure 1: Active Learning evaluation results.
Baseline refers to the second baseline from Section 2.
Semi-supervised: Trust Seed Data refers to the standard KE model that trusts the seed data.
Optimal Semi-supervised refers to the augmented KE model described in (Birke & Sarkar, 2006).
Active Learning refersto the model proposed in this paper.in an avarage f-score of nearly 36.9%.
This is 23%higher than the 30% of the balanced case, which is1.23 times higher.
For this reason, we give the hu-man scores a boost by assuming that whatever thehuman annotates in the manual scenario will resultin an f-score that is 1.23 times higher.
For our ex-periment, we take the number of sentences that ouractive learning method sent to the human for eachword ?
note that this is not always 30% of the to-tal number of sentences ?
and multiply that by 1.23?
to give the human the benefit of the doubt, so tospeak.
Still we find that using active learning givesus an avarage accuracy across all words of 64.9%,while we get only 21.7% with the manual process.This means that for the same human effort, usingthe weakly supervised classifier produced a three-fold improvement in accuracy.
Looking at this con-versely, this means that in order to obtain an ac-curacy of 64.9%, by a purely manual process, thehuman would have to classify nearly 53.6% of thesentences, as opposed to the 17.7% he needs to dousing active learning.
This is an effort-savings ofabout 35%.
To conclude, we claim that our modelcombined with active learning is a helpful tool fora literal/nonliteral clustering project.
It can save thehuman significant effort while still producing rea-sonable results.4 Annotated corpus built using activelearningIn this section we discuss the development of an an-notated corpus of literal/nonliteral usages of verbsin context.
First, we examine iterative augmenta-tion.
Then we discuss the structure and contents ofthe annotated corpus and the potential for expansion.After an initial run for a particular target word, wehave the cluster results plus a record of the feedbacksets augmented with the newly clustered sentences.26***pour****nonliteral cluster*wsj04:7878 N As manufacturers get bigger , they are likely topour more money into the battle for shelf space , raising theante for new players ./.wsj25:3283 N Salsa and rap music pour out of the windows ./.wsj06:300 U Investors hungering for safety and high yieldsare pouring record sums into single-premium , interest-earningannuities ./.
*literal cluster*wsj59:3286 L Custom demands that cognac be poured from afreshly opened bottle ./.Figure 2: Excerpt from our annotated corpus of lit-eral/nonliteral usages of verbs in context.Each feedback set sentence is saved with a weight,with newly clustered sentences receiving a weightof 1.0.
Subsequent runs may be done to augmentthe initial clusters.
For these runs, we use the theoutput identification over the examples from our ini-tial run as feedback sets.
New sentences for cluster-ing are treated like a regular target set.
Running thealgorithm in this way produces new clusters and are-weighted model augmented with newly clusteredsentences.
There can be as many runs as desired;hence iterative augmentation.We used the iterative augmentation process tobuild a small annotated corpus consisting of the tar-get words from Table 1, as well as another 25 wordsdrawn from the examples of previously publishedwork (see Section 5).
It is important to note thatin building the annotated corpus, we used the Ac-tive Learning component as described in this paper,which improved our average f-score from 53.8% to64.9% on the original 25 target words, and we ex-pect also improved performance on the remainder ofthe words in the annotated corpus.An excerpt from the annotated corpus is shown inFigure 2.
Each entry includes an ID number and aNonliteral, Literal, or Unannotated tag.
Annotationsare from testing or from active learning during anno-tated corpus construction.
The corpus is available athttp://www.cs.sfu.ca/?anoop/students/jbirke/.
Fur-ther unsupervised expansion of the existing clustersas well as the production of additional clusters is apossibility.5 Previous WorkTo our knowledge there has not been any previouswork done on taking a model for literal/nonliterallanguage and augmenting it with an active learningapproach which allows human expert knowledge tobecome part of the learning process.Our approach to active learning is similar to theUncertainty Sampling approach of (Lewis & Gale,1994) and (Fujii et.
al., 1998) in that we pick thoseexamples that we could not classify due to low con-fidence in the labeling at a particular point.
Weemploy a resource-limited version in which onlya small fixed sample is ever annotated by a hu-man.
Some of the criticisms leveled against un-certainty sampling and in favor of Committee-basedsampling (Engelson & Dagan, 1996) (and see refstherein) do not apply in our case.Our similarity measure is based on two views ofsentence- and word-level similarity and hence weget an estimate of appropriate identification ratherthan just correct classification.
As a result, by em-bedding an Uncertainty Sampling active learningmodel within a two-view clustering algorithm, wegain the same advantages as other uncertainty sam-pling methods obtain when used in bootstrappingmethods (e.g.
(Fujii et.
al., 1998)).
Other machinelearning approaches that derive from optimal exper-iment design are not appropriate in our case becausewe do not yet have a strong predictive (or generative)model of the literal/nonliteral distinction.Our machine learning model only does identifi-cation of verb usage as literal or nonliteral but itcan be seen as a first step towards the use of ma-chine learning for more sophisticated metaphor andmetonymy processing tasks on larger text corpora.Rule-based systems ?
some using a type of interlin-gua (Russell, 1976); others using complicated net-works and hierarchies often referred to as metaphormaps (e.g.
(Fass, 1997; Martin, 1990; Martin, 1992)?
must be largely hand-coded and generally workwell on an enumerable set of metaphors or in lim-ited domains.
Dictionary-based systems use exist-ing machine-readable dictionaries and path lengthsbetween words as one of their primary sourcesfor metaphor processing information (e.g.
(Dolan,1995)).
Corpus-based systems primarily extract orlearn the necessary metaphor-processing informa-tion from large corpora, thus avoiding the need formanual annotation or metaphor-map construction.Examples of such systems are (Murata et.
al., 2000;Nissim & Markert, 2003; Mason, 2004).27Nissim & Markert (2003) approach metonymyresolution with machine learning methods, ?which[exploit] the similarity between examples of con-ventional metonymy?
((Nissim & Markert, 2003),p. 56).
They see metonymy resolution as a classi-fication problem between the literal use of a wordand a number of pre-defined metonymy types.
Theyuse similarities between possibly metonymic words(PMWs) and known metonymies as well as contextsimilarities to classify the PMWs.Mason (2004) presents CorMet, ?a corpus-basedsystem for discovering metaphorical mappings be-tween concepts?
((Mason, 2004), p. 23).
His systemfinds the selectional restrictions of given verbs inparticular domains by statistical means.
It then findsmetaphorical mappings between domains based onthese selectional preferences.
By finding seman-tic differences between the selectional preferences,it can ?articulate the higher-order structure of con-ceptual metaphors?
((Mason, 2004), p. 24), findingmappings like LIQUID?MONEY.Metaphor processing has even been ap-proached with connectionist systems storingworld-knowledge as probabilistic dependencies(Narayanan, 1999).6 ConclusionIn this paper we presented a system for separatingliteral and nonliteral usages of verbs through statis-tical word-sense disambiguation and clustering tech-niques.
We used active learning to combine the pre-dictions of this system with a human expert anno-tator in order to boost the overall accuracy of thesystem by 11.1%.
We used the model together withactive learning and iterative augmentation, to buildan annotated corpus which is publicly available, andis a resource of literal/nonliteral usage clusters thatwe hope will be useful not only for future research inthe field of nonliteral language processing, but alsoas training data for other statistical NLP tasks.ReferencesSrinivas Bangalore and Aravind K. Joshi.
1999.
Supertagging:an approach to almost parsing.
Comput.
Linguist.
25, 2 (Jun.1999), 237-265.Julia Birke and Anoop Sarkar.
2006.
In Proceedings of the 11thConference of the European Chapter of the Association forComputational Linguistics, EACL-2006.
Trento, Italy.
April3-7.Barbara Di Eugenio and Michael Glass.
2004.
The kappastatistic: a second look.
Comput.
Linguist.
30, 1 (Mar.
2004),95-101.William B. Dolan.
1995.
Metaphor as an emergent propertyof machine-readable dictionaries.
In Proceedings of Repre-sentation and Acquisition of Lexical Knowledge: Polysemy,Ambiguity, and Generativity (March 1995, Stanford Univer-sity, CA).
AAAI 1995 Spring Symposium Series, 27-29.Sean P. Engelson and Ido Dagan.
1996.
In Proc.
of 34th Meet-ing of the ACL.
319?326.Dan Fass.
1997.
Processing metonymy and metaphor.
Green-wich, CT: Ablex Publishing Corporation.Atsushi Fujii, Takenobu Tokunaga, Kentaro Inui and HozumiTanaka.
1998.
Selective sampling for example-based wordsense disambiguation.
Comput.
Linguist.
24, 4 (Dec. 1998),573?597.Yael Karov and Shimon Edelman.
1998.
Similarity-based wordsense disambiguation.
Comput.
Linguist.
24, 1 (Mar.
1998),41-59.David D. Lewis and William A. Gale.
1994.
A sequential algo-rithm for training text classifiers.
In Proc.
of SIGIR-94.James H. Martin.
1990.
A computational model of metaphorinterpretation.
Toronto, ON: Academic Press, Inc.James H. Martin.
1992.
Computer understanding of conven-tional metaphoric language.
Cognitive Science 16, 2 (1992),233-270.Zachary J. Mason.
2004.
CorMet: a computational, corpus-based conventional metaphor extraction system.
Comput.Linguist.
30, 1 (Mar.
2004), 23-44.Masaki Murata, Qing Ma, Atsumu Yamamoto, and Hitoshi Isa-hara.
2000.
Metonymy interpretation using x no y exam-ples.
In Proceedings of SNLP2000 (Chiang Mai, Thailand,10 May 2000).Srini Narayanan.
1999.
Moving right along: a computationalmodel of metaphoric reasoning about events.
In Proceed-ings of the 16th National Conference on Artificial Intelli-gence and the 11th IAAI Conference (Orlando, US, 1999).121-127.Malvina Nissim and Katja Markert.
2003.
Syntactic featuresand word similarity for supervised metonymy resolution.
InProceedings of the 41st Annual Meeting of the Associationfor Computational Linguistics (ACL-03) (Sapporo, Japan,2003).
56-63.Adwait Ratnaparkhi.
1996.
A maximum entropy part-of-speech tagger.
In Proceedings of the Empirical Methodsin Natural Language Processing Conference (University ofPennsylvania, May 17-18 1996).Sylvia W. Russell.
1976.
Computer understanding ofmetaphorically used verbs.
American Journal of Computa-tional Linguistics, Microfiche 44.28
