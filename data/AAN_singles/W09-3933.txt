Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 225?234,Queen Mary University of London, September 2009. c?2009 Association for Computational LinguisticsModels for Multiparty Engagement in Open-World DialogDan BohusMicrosoft ResearchOne Microsoft WayRedmond, WA, 98052dbohus@microsoft.comEric HorvitzMicrosoft ResearchOne Microsoft WayRedmond, WA, 98052horvitz@microsoft.comAbstractWe present computational models that allowspoken dialog systems to handle multi-participant engagement in open, dynamic envi-ronments, where multiple people may enter andleave conversations, and interact with the sys-tem and with others in a natural manner.
Themodels for managing the engagement processinclude components for (1) sensing the en-gagement state, actions and intentions of mul-tiple agents in the scene, (2) making engage-ment decisions (i.e.
whom to engage with, andwhen) and (3) rendering these decisions in a setof coordinated low-level behaviors in an embo-died conversational agent.
We review resultsfrom a study of interactions "in the wild" with asystem that implements such a model.1 IntroductionTo date, nearly all spoken dialog systems research hasfocused on the challenge of engaging single users ontasks defined within a relatively narrow context.
Effortsin this realm have led to significant progress includinglarge-scale deployments that now make spoken dialogsystems common features in the daily lives of millionsof people.
However, research on dialog systems haslargely overlooked important challenges with the initia-tion, maintenance, and suspension of conversations thatare common in the course of natural communication andcollaborations among people.
In (Bohus and Horvitz,2009) we outlined a set of core challenges for extendingtraditional closed-world dialog systems to systems thathave competency in open-world dialog.
The work de-scribed here is part of a larger research effort aimed ataddressing these challenges, and constructing computa-tional models to support the core interaction skills re-quired for open-world dialog.
In particular, we focus ourattention in this paper on the challenges of managingengagement ?
?the process by which two (or more) par-ticipants establish, maintain and end their perceivedconnection during interactions they jointly undertake?,cf.
Sidner et al (2004) in open-world settings.We begin by reviewing the challenges of managingengagement in the open-world in the next section.
InSection 3, we survey the terrain of related efforts thatprovides valuable context for the new work described inthis paper.
In Section 4, we introduce a computationalmodel for multiparty situated engagement.
The modelharnesses components for sensing the engagement state,actions, and intentions of people in the scene for makinghigh-level engagement decisions (whom to engage with,and when), and for rendering these decisions into a setof low-level coordinated behaviors (e.g., gestures, eyegaze, greetings, etc.).
Then, we describe an initial ob-servational study with the proposed model, and discusssome of the lessons learned through this experiment.Finally, in Section 6, we summarize this work and out-line several directions for future research.2 Engagement in Open-World DialogIn traditional, single-user systems the engagement prob-lem can often be resolved in a relatively simple manner.For instance, in telephony-based applications, it is typi-cally safe to assume that a user is engaged with a dialogsystem once a call has been received.
Similarly, push-to-talk buttons are often used in multimodal mobile ap-plications.
Although these solutions are sufficient andeven natural in closed, single-user contexts, they be-come inappropriate for open-world systems that mustoperate continuously in open, dynamic environments,such as robots, interactive billboards, or embodied con-versational agents.Interaction in the open-world is characterized by twoaspects that capture key departures from assumptionstraditionally made in spoken dialog systems (Bohus andHorvitz, 2009).
The first one is the dynamic, multipartynature of the interaction, i.e., the world typically con-tains not just one, but multiple agents that are relevant225to the interactive system.
Engagements in open worldsare often dynamic and asynchronous, i.e.
relevant agentsmay enter and leave the observable world at any time,may interact with the system and with each other, andtheir goals, needs, and intentions may change over time.Managing the engagement process in this context re-quires that a system explicitly represents, models, andreasons about multiple agents and interaction contexts,and maintains and leverages long-term memory of theinteractions to provide support and assistance.A second important aspect that distinguishes open-world from closed-world dialog is the situated nature ofthe interaction, i.e., the fact that the surrounding physi-cal environment provides rich, streaming context that isrelevant for conducting and organizing the interactions.Situated interactions among people often hinge onshared information about physical details and relation-ships, including structures, geometric relationships andpathways, objects, topologies, and communication af-fordances.
The often implicit, yet powerful physicalityof situated interaction, provides opportunities for mak-ing inferences in open-world dialog systems, and chal-lenges system designers to innovate across a spectrumof complexity and sophistication.
Physicality and em-bodiment also provide important affordances that can beused by a system to support the engagement process.For instance, the use of a rendered or physically embo-died avatar in a spoken dialog system provides a naturalpoint of visual engagement between the system andpeople, and allows the system to employ natural signal-ing about attention and engagement with head pose,gaze, facial expressions, pointing and gesturing.We present in this paper methods that move beyondthe realm of closed-world dialog with a situated multi-party engagement model that can enable a computation-al system to fluidly engage, disengage and re-engageone or multiple people, and support natural interactionsin an open-world context.3 Related WorkThe process of engagement between people, and be-tween people and computational systems has received afair amount of attention.
Observational studies in thesociolinguistics and conversational analysis communi-ties have revealed that engagement is a complex, mixed-initiative, highly-coordinated process that often involvesa variety of non-verbal cues and signals, (Goffman,1963; Kendon, 1990), spatial trajectory and proximity(Hall, 1966; Kendon, 1990b), gaze and mutual attention(Argyle and Cook, 1976), head and hand gestures (Ken-don, 1990), as well as verbal greetings.A number of researchers have also investigated is-sues of engagement in human-computer and human-robot interaction contexts.
Sidner and colleagues (2004)define engagement as ?the process by which two (ormore) participants establish, maintain and end their per-ceived connection during interactions they jointly un-dertake?, and focus on the process of maintaining en-gagement.
They show in a user study (Sidner et al,2004; 2005) that people directed their attention to a ro-bot more often when the robot made engagement ges-tures throughout the interaction (i.e.
tracked the user?sface, and pointed to relevant objects at appropriate timesin the conversation.)
Peters (2005; 2005b) uses an alter-native definition of engagement as ?the value that a par-ticipant in an interaction attributes to the goal of beingtogether with the other participant(s) and of continuingthe interaction,?
and present the high-level schematicsfor an algorithm for establishing and maintaining en-gagement.
The algorithm highlights the importance ofmutual attention and eye gaze and relies on a heuristi-cally computed ?interest level?
to decide when to start aconversation.
Michalowski and colleagues (2006) pro-pose and conduct experiments with a model of engage-ment grounded in proxemics (Hall, 1966) which classi-fies relevant agents in the scene in four different catego-ries (present, attending, engaged  and interacting) basedon their distance to the robot.
The robot?s behaviors arein turn conditioned on the four categories above.In our work, we follow Sidner?s definition of en-gagement as a process (Sidner et al, 2004) and describea computational model for situated multiparty engage-ment.
The proposed model draws on several ideas fromthe existing body of work, but moves beyond it andprovides a more comprehensive framework for manag-ing the engagement process in a dynamic, open-worldcontext, where multiple people with different andchanging goals may enter and leave, and communicateand coordinate with each other and with the system.4 Models for Multiparty EngagementThe proposed framework for managing engagement iscentered on a reified notion of interaction, defined hereas a basic unit of sustained, interactive problem-solving.Each interaction involves two or more participants, andthis number may vary in time; new participants mayjoin an existing interaction, or current participants mayleave an interaction at any point in time.
The system isactively engaged in at most one interaction at a time(with one or multiple participants), but it can simulta-neously keep track of additional, suspended interactions.In this context, engagement is viewed as the processsubsuming the joint, coordinated activities by whichparticipants initiate, maintain, join, abandon, suspend,resume, or terminate an interaction.
Appendix A showsby means of an example the various stages of an interac-tion and the role played by the engagement process.Successfully modeling the engagement process in asituated, multi-participant context requires that the sys-tem (1) senses and reasons about the engagement state,226not-engaged engagedEA=disengage |SEA=disengageFigure 2.
Engagement state transition diagram.
EA is theagent?s engagement action; SEA is the system?s action.EA=maintain &SEA=maintainEA=engage &SEA=engageEA=no-action |SEA=no-actionFigure 3.
Graphical model showing key variables anddependencies in managing engagement.ESEAEIt   t+1SEAES?EIA?additionalcontextengagementsensing?GAGEASEB?actions and intentions of multiple agents in the scene,(2) makes high-level engagement control decisions (i.e.about whom to engage or disengage with, and when)and (3) executes and signals these decisions to the otherparticipants in an appropriate and expected manner (e.g.renders them in a set of coordinated behaviors such asgestures, greetings, etc.).
The proposed model subsumesthese three components, which we discuss in more de-tail in the following subsections.4.1 Engagement State, Actions, IntentionsAs a prerequisite for making informed engagement de-cisions, a system must be able to recognize various en-gagement cues, and to reason about the engagementactions and intentions of relevant agents in the scene.
Toaccomplish this, the sensing subcomponent of the pro-posed engagement model tracks over time three relatedengagement variables for each agent ?
and interaction ?
:the engagement state ????
(?)
, the engagement action????
(?)
and the engagement intention ????
(?
).The engagement state, ????
(?
), captures whether anagent ?
is engaged in interaction ?
and is modeled as adeterministic variable with two possible values: en-gaged and not-engaged.
The state is updated based onthe joint actions of the agent and the system (see Figures3 and 4).
Since engagement is a collaborative process,the transitions to the engaged state require that both theagent and the system take either an engage action (if theagent was previously not engaged) or a maintain action(if the agent was already engaged); we discuss theseactions in more detail shortly.
On the other hand, disen-gagement can be a unilateral act: an agent transitions tothe not-engaged state if either the agent or the systemtake a disengage action or a no-action.The second engagement variable, ????
(?
), models theactions that an agent takes to initiate, maintain or termi-nate engagement.
There are four engagement actions:engage, no-action, maintain, disengage.
The first twoare possible only from the not-engaged state, while thelast two are possible only from the engaged state.
Theengagement actions are estimated based on a condition-al probabilistic model of the form:?(????
(?)|????
?
,????
?
?
1 , ?????
?
?
1 ,?(?
))The inference is conditioned on the current engage-ment state, on the previous agent and system actions,and on additional sensory evidence ?(t).
?
t  includesthe detection of explicit engagement cues such as: salu-tations (e.g.
?Hi!
?, ?Bye bye?
); calling behaviors (e.g.?Laura!?
); the establishment or the breaking of an F-formation (Kendon, 1990b), i.e.
the agent approachesand positions himself in front of the system and attendsto the system; an expected, opening dialog move (e.g.
?Come here!?).
Note that each of these cues is explicit,and marks a committed engagement action.A third variable in the proposed model, ????
(?)
,tracks the engagement intention of an agent with respectto a conversation.
Like the engagement state, the inten-tion can either be engaged or not-engaged.
Intentionsare tracked separately from actions since an agent mightintend to engage or disengage the system, but not yettake an explicit engagement action.
For instance, let usconsider the case in which the system is already en-gaged in an interaction and another agent is waiting inline to interact with the system.
Although the waitingagent does not take an explicit, committed engagementaction, she might still intend to engage in a new conver-sation with the system once the opportunity arises.
Shemight also signal this engagement intention via variouscues (e.g.
pacing around, glances that make brief butclear eye contact with the system, etc.)
More generally,the engagement intention variable captures whether ornot an agent would respond positively should the systeminitiate engagement.
In that sense, it roughly corres-ponds to Peters?
(2005; 2005b) ?interest level?, i.e.
tothe value the agent attaches to being engaged in a con-versation with the system.Like engagement actions, engagement intentions areinferred based on a direct conditional model:227?(????
(?)|????
?
,????
?
,?????
?
?
1 ,????
t?
1 ,?(?
))This model leverages information about the currentengagement state, the previous agent and system ac-tions, the previous engagement intention, as well as ad-ditional evidence ?(?)
capturing implicit engagementcues.
Such cues include the spatiotemporal trajectory ofthe participant and the level of sustained mutual atten-tion.
The models for inferring engagement actions andintentions are generally independent of the application.They capture the typical behaviors and cues by whichpeople signal engagement, and, as such, should be reus-able across different domains.
In other work (Bohus andHorvitz, 2009b), we describe these models in more de-tail and show how they can be learned automaticallyfrom interaction data.4.2 Engagement Control PolicyBased on the inferred state, actions and intentions of theagents in the scene, as well as other additional evidenceto be discussed shortly, the proposed model outputshigh-level engagement actions, denoted by SEA deci-sion node in Figure 3.
The action-space on the systemside contains the same four actions previously dis-cussed: engage, disengage, maintain and no-action.Each action is parameterized with a set of agents {??
}and an interaction ?.
Additional parameters that controlthe lower level execution of these actions, such as spe-cific greetings, waiting times, urgency, etc.
may also bespecified.
The actual execution mechanisms are dis-cussed in more detail in the following subsection.In making engagement decisions in an open-worldsetting, a conversational system must balance the goalsand needs of multiple agents in the scene and resolvevarious tradeoffs (for instance between continuing thecurrent interaction or interrupting it temporarily to ad-dress another agent), all the while observing rules ofsocial etiquette in interaction.
Apart from the detectedengagement state, actions and intentions of an agent???
=  ????
,????
,????
, the control policy can be en-hanced through leveraging additional observational evi-dence, including high-level information ??
about thevarious agents in the scene, such as their long-termgoals and activities, as well as other global context (?
),including the multiple tasks at hand, the history of theinteractions, relationships between various agents in thescene (e.g.
which agents are in a group together), etc.For instance, a system might decide to temporarilyrefuse engagement even though an agent takes an en-gage action, because it is currently involved in a higherpriority interaction.
Or, a system might try to take theinitiative and engage an agent based on the current con-text (e.g.
the system has a message to deliver) and activ-ity of the agent (e.g.
the agent is passing by), eventhough the agent has no intention to engage.Engagement control policies have therefore the form,????({???
}?
,?
,  ??
?
,?
)where we have omitted the time index for simplicity.
Incontrast to the models for inferring engagement inten-tions and action, the engagement control policy can of-ten be application specific.
Such policies can be au-thored manually to capture the desired system behavior.We will discuss a concrete example of this in Section5.2.
In certain contexts, a more principled solution canbe developed by casting the control of engagement as anoptimization problem for scheduling collaborations withmultiple parties under uncertainties about the estimatedgoals and needs, the duration of the interactions, timeand frustration costs, social etiquette, etc.
We are cur-rently exploring such models, where the system alsouses information-gathering actions (e.g.
?Are the two ofyou together??
?Are you here for X?,?
etc.
), based onvalue-of-information computations to optimize in thenature and flow of attention and collaboration in multi-party interactions.4.3 Behavioral Control PolicyAt the lower level, the engagement decisions taken bythe system have to be executed and rendered in an ap-propriate manner.
With the use of a rendered or physicalembodied agent, these actions are translated into a set ofcoordinated lower-level behaviors, such as head ges-tures, making and breaking eye contact, facial expres-sions, salutations, interjections, etc.
The coordination ofthese behaviors is governed by a behavioral control pol-icy, conditioned on the estimated engagement state,actions and intentions of the considered agents, as wellas other information extracted from the scene:????(??
?, {???
}?
,?
,?
)For example, in the current implementation, the en-gage system action subsumes three sub-behaviors per-formed in a sequence: EstablishAttention, Greeting, andMonitor.
First, the system attempts to establish sus-tained mutual attention with the agent(s) to be engaged.This is accomplished by directing the gaze towards theagents, and if the agent?s focus of attention is not on thesystem, triggering an interjection like ?Excuse me!
?Once mutual attention is established, on optional Greet-ing behavior is performed; a greeting can be specified asan execution parameter of the engage action.
Finally,the system enters a Monitor behavior, in which it moni-tors for the completion of engagement.
The action com-pletes successfully once the agent(s) are in an engagedstate.
Alternatively if a certain period of time elapsesand the agent(s) have not yet transitioned to the engagedstate, the engage system action completes with failure(which is signaled to the engagement control layer).Like the high-level engagement control policies, thebehavioral control policies can either be authored ma-nually, or learned from data, either in a supervised (e.g.228from a human-human interaction corpus) or unsuper-vised learning setting.
Also, like the engagement sens-ing component, the behavioral control component isdecoupled from the task at hand, and should be largelyreusable across multiple application domains.5 Observational StudyAs an initial step towards evaluating the proposed si-tuated multiparty engagement models, we conducted apreliminary observational study with a spoken dialogsystem that implements these models.
The goals of thisstudy were (1) to investigate whether a system can usethe proposed engagement models to effectively createand conduct multiparty interactions in an open-worldsetting, (2) to study user behavior and responses in thissetting, and (3) to identify some of the key technicalchallenges in supporting multiparty engagement anddialog in open-world context.
In this section, we de-scribe this study and report on the lessons learned.5.1 Experimental platformStudying multiparty engagement and more generallyopen-world interaction poses significant challenges.Controlled laboratory studies are by their very natureclosed-world.
Furthermore, providing participants withinstructions, such as ?Go interact with this system?, or?Go join the existing interaction?
can significantlyprime and alter the engagement behaviors they wouldotherwise display upon encountering the system in anunconstrained setting.
This can in turn cast seriousdoubts on the validity of the results.
Open-world inte-raction is best observed in the open-world.To provide an ecologically valid basis for studyingsituated, multiparty engagement we therefore developeda conversational agent that implements the proposedmodel, and deployed it in the real-world.
The system,illustrated in Figure 4, takes the form of an interactivemulti-modal kiosk that displays a realistically renderedavatar head which can interact via natural language.
Theavatar can engage with one or more participants andplays a simple game, in which the users have to respondto multiple-choice trivia questions.The system?s hardware and software architecture isillustrated in Figure 4.
Data gathered from a wide-anglecamera, a 4-element linear microphone array, and a 19?touch-screen is forwarded to a scene analysis modulethat fuses the incoming streams and constructs in real-time a coherent picture of the dynamics in the surround-ing environment.
The system detects and tracks the lo-cation of multiple agents in the scene, tracks the headpose for engaged agents, tracks the current speaker, andinfers the focus of attention, activities, and goals of eachagent, as well as the group relationships among differentagents.
An in-depth description of the hardware andscene analysis components falls beyond the scope ofthis paper, but details are available in (Bohus and Hor-vitz, 2009).
The scene analysis results are forwarded tothe control level, which is structured in a two-layer reac-tive-deliberative architecture.
The reactive layer imple-ments and coordinates various low-level behaviors, in-cluding engagement, conversational floor managementand turn-taking, and coordinating spoken and gesturaloutputs.
The deliberative layer plans the system?s dialogmoves and high-level engagement actions.Overall, the game task was purposefully designed tominimize challenges in terms of speech recognition ordialog management, and allow us to focus our attentionon the engagement processes.
The avatar begins theinteractions by asking the engaged user if they wouldlike to play a trivia game.
If the user agrees, the avatargoes through four multiple-choice questions, one at atime.
After each question, the possible answers are dis-played on the screen (Figure 4) and users can respondby either speaking an answer or by touching it.
Whenthe answer provided by the user is incorrect, the systemprovides a short explanation regarding the correct an-swer before moving on to the next question.The system also supports multi-participant interac-tions.
The engagement policy used to attract and engageDialog ManagementBehavioral ControlScene Analysis Output PlanningVision Speech Synthesis Avatarwide-angle camera4-element linear microphone arraytouch screenspeakersFigure 4.
Trivia game dialog system: prototype, architectural overview, and runtime scene analysis229multiple users in a game is the focus of this observa-tional study, and is discussed in more detail in the nextsubsection.
Once the system is engaged with multipleusers, it uses a multi-participant turn taking modelwhich allows it to continuously track who the currentspeaker is, and who has the conversational floor (Bohusand Horvitz, 2009).
At the behavioral level, the avatarorients its head pose and gaze towards the currentspeaker, or towards the addressee(s) of its own utter-ances.
During multiplayer games, the avatar alternatesbetween the users when asking questions.
Also, after aresponse is received from one of the users, the avatarconfirms the answer with the other user(s), e.g.
?Do youagree with that??
A full sample interaction with the sys-tem is described in Appendix A, and the correspondingvideo is available online (Situated Interaction, 2009).5.2 Multiparty Engagement PolicyThe trivia game system implements the situated, multi-party engagement model described in Section 4.
Thesensing and behavioral control components are applica-tion independent and were previously described.
Wenow describe the system?s engagement policy, which isapplication specific.As previously discussed, apart from using the in-ferred engagement state, actions and intentions for theagents in the scene, the proposed model also uses in-formation about the high-level goals and activities ofthese agents when making engagement decisions.
Spe-cifically, the system tracks the goal of each agent in thescene, which can be play, watch, or other, and their cur-rent activity, which can be passing-by, interacting, play-ing, watching, or departing.
The goal and activity rec-ognition models are application specific, and in this caseare inferred based on probabilistic conditional modelsthat leverage information about the spatiotemporal tra-jectory of each agent and their spoken utterances, aswell as global scene information (e.g.
is the system en-gaged in an active interaction, etc.
).Initially, when the system is idle, it uses a conserva-tive engagement policy and waits for the user to initiateengagement via an explicit action.
Such actions includethe user approaching and entering in an F-formation(Kendon, 1990b) with the system, i.e.
standing right infront of it, swiping their badge, or pushing the start but-ton (in the idle state the GUI displays ?swipe yourbadge or press here to begin?
below the avatar head).While engaged in an interaction, the system attemptsto engage bystanders in an effort to create a collabora-tive, multi-participant game.
In this case, the engage-ment policy is conditioned on the inferred activities ofthe agents in the scene.
Specifically, if a watching bys-tander is detected, the system temporarily disengagesthe current participant, and engages and attempts to?convince?
the watching bystander to join the existinggame.
The prompts in this side interaction depend onthe current game context, as shown in Table 1.
If thewatching bystander agrees to join in, the system addshim to the existing interaction, and continues a multi-participant game (see Table 1.A.)
Conversely, if thebystander refuses, the system re-engages the previousparticipant and resumes the single-user game (see Table1.B.)
Additional examples are available in Appendix A.Finally, if the system is already engaged and a watch-ing bystander is detected but only during the last ques-tion, the system engages them temporarily to let themknow that the current game will end shortly and, if theywait, they can also start a new game (see Table 1.D).5.3 Results and Lessons LearnedWe deployed the system described above for 20 daysnear one of the kitchenettes in our building.
The systemattracted attention of passer-bys with the tracking mo-tion of its virtual face that followed people as theypassed by.
Most people that interacted with the systemdid so for the first time; only a small number of peopleinteracted several times.
No instructions were providedfor interacting with the system.
We shall now reviewresults from analysis of the collected data.Throughout the 20 days of deployment, the systemengaged in a total of 121 interactive events.
Of these, in54 cases (44%), a participant engaged the system butdid not play the game.
Typically, the participant wouldapproach and enter in an F-formation with the system,If a watching bystander B is detected during the first 3 questionsand the engaged participant E has not made any mistakesA[S to E]: Hold on a second.
[S to B]: Hi.
Excuse me, would you like to join in?
[B]:  Okay.
[S to E&B]: Great.
So let?s see how well you can do together.Next question [?
]and the engaged participant E has just made a mistake, andthe system did not previously engage the bystander B.B[S to E]: Perhaps you can use some help.
Hold on asecond.
[S to B]: Hi.
Do you think you could help out?
[B]:  No thanks[S to B]: Okay.
Well, I guess you can just watch for nowand if you wait for a bit we?ll be done and you canstart a new game yourself[S to E]: Guess you?re still on your own.
Next question [?
]and  the engaged participant E has just made a mistake, andthe system did previously attempt to engage the bystander B.C[S to E]: I think you could really use some help.
Hold on asecond.
[S to B]: Are you sure you don?t want to help out?
Comeon, this is fun.
[B]:  Sure[S to E&B]: Great.
So let?s see how well you can do together.Next question [?
]If a watching bystander B is detected during the last questionD[S to E]: Excuse me for one moment.
[S to B]: We?re almost done here.
If you wait for a bit we can starta new game right after[S to E]: Sorry about that [?
]Table 1.
Multiparty engagement policy230but, once the system engaged and asked if they wouldlike to play the trivia game, they responded negativelyor left without responding.
In 49 cases (40%), a singleparticipant engaged and played the game, but no bys-tanders were observed during these interactions.
In onecase, two participants approached and engaged simulta-neously; the system played a multi-participant game, butno other bystanders were observed.
Finally, in the re-maining 17 cases (14% of all engagements, 25% of ac-tual interactions), at least one bystander was observedand the system engaged in multiparty interaction.
Thesemultiparty interactions are the focus of our observation-al analysis, and we will discuss them in more detail.In 2 of these 17 cases, bystanders appeared only latein the interaction, after the system had already asked thelast question.
In these cases, according to its engage-ment policy, the system notified the bystander that theywould be attended to momentarily (see Table 1.D), andthen proceeded to finish the initial game.
In 8 of theremaining 15 cases (53%), the system successfully per-suaded bystanders to join the current interaction andcarried on a multi-participant game.
In the remaining 7cases (47%), bystanders turned down the offer to jointhe existing game.
Although this corpus is still relativelysmall, these statistics indicate that the system can suc-cessfully engage bystanders and create and managemulti-participant interactions in the open world.Next, we analyzed more closely the responses andreactions from bystanders and already engaged partici-pants to the system?s multiparty engagement actions.Throughout the 17 multiparty interactions, the systemplanned and executed a total of 23 engagement actionssoliciting a bystander to enter the game, and 6 engage-ment actions letting a bystander know that they will beengaged momentarily.
The system actions and res-ponses from bystanders and engaged participants arevisually summarized in Figure 5, and are presented infull in Appendix B.
Overall, bystanders successfullyrecognize that they are being engaged and solicited bythe system and respond (either positively or negatively)in the large majority of cases (20 out of 23).
In 2 of theremaining 3 cases, the previously engaged participantresponded instead of the bystander; finally, in one casethe bystander did not respond and left the area.While bystanders generally respond when engagedby the system, the system?s engagement actions towardsbystanders also frequently elicits spoken responses fromthe already engaged participants; this happened in 14out of 23 cases (61%).
The responses are sometimesaddressed to the system e.g.
?Yes he does,?
or towardsthe bystander, e.g.
?Say yes!
?, or they reflect generalcomments, e.g.
?That?s crazy!?
These results show that,when creating the side interaction to solicit a bystanderto join the game, the system should engage both thebystander and the existing user in this side interaction,or at least allow the previous user to join this side inte-raction (currently the system engages only the bystanderin this interaction; see example from Appendix A.
)Furthermore, we noticed that, in several cases, bys-tanders provided responses to the system?s questionseven prior to the point the system engaged them in inte-raction (sometimes directed toward the system, some-times toward the engaged participant.)
We employed asystem-initiative engagement policy towards bystandersin the current experiment.
The initiative being taken byparticipants highlights the potential value of implement-ing a mixed-initiative policy for engagement.
If a rele-vant response is detected from a bystander, this can beinterpreted as an engagement action (recall from subsec-tion 4.1 that engagement actions subsume expectedopening dialog moves), and a mixed-initiative policycan respond by engaging the bystander, e.g.
?Did youwant to join in??
or ?Please hang on, let?s let him finish.We can play a new game right after that.?
This policycould be easily implemented under the proposed model.We also noted side comments by both bystander andthe existing participant around the time of multipartyengagement.
These remarks typically indicate surpriseand excitement at the system?s multiparty capabilities.Quotes include: ?That?s awesome!
?, ?Isn?t that great!
?,?That?s funny!
?, ?Dude!
?, ?Oh my god that?s creepy!
?,?That?s cool!
?, ?It multitasks!
?, ?That is amazing!
?,?That?s pretty funny?, plus an abundance of laughterand smiles.
Although such surprise might be expectedtoday with a first-time exposure to an interactive systemthat is aware of and can engage with multiple parties,we believe that expectations will change in the future, asthese technologies become more commonplace.Figure 5.
System multiparty engagement actions and responses from bystanders and already engaged participants.For bystander responses,     denotes a positive response;      denotes a negative response;    denotes no response.
For responsesfrom previously engaged participant,      denotes utterances addressed to the bystander,      denotes side comments,      denotesresponses directed to the systemresponse frompreviously engagedparticipantExcuse me for one second ?
Hi, would you like to join in?
[12 cases]Y Y Y N Y N T N N N N N N N N N N Y Y Y YPerhaps you can use somehelp?
Do you think you couldhelp out?
[6 cases][after non-understanding]Sorry, did you want to joinin?
[5 cases]We?re almost done here.
If youwait for a bit we can start a newgame right after  [6 cases]response fromsolicited bystandersystempromptB BY NBB B S S S S S S C C C C CSC231Overall, this preliminary study confirmed that thesystem can effectively initiate engagement in multipartysettings, and also highlighted several core challenges formanaging engagement and supporting multiparty inte-ractions in the open world.
A first important challengewe have identified is developing robust models fortracking the conversational dynamics in multiparty situ-ations, i.e.
identifying who is talking to whom at anygiven point.
Secondly, the experiment has highlightedthe opportunity for using more flexible, mixed-initiativeengagement policies.
Such policies will rely heavily onthe ability to recognize engagement intentions; in (Bo-hus and Horvitz, 2009b), we describe the automatedlearning of engagement intentions from interaction data.Finally, another lesson we learned from these initialexperiments is the importance of accurate face trackingfor supporting multiparty interaction.
Out of the 17 mul-tiparty interactions, 7 were affected by vision problems(e.g.
the system momentarily lost a face, or swapped theidentity of two faces); 4 of these were fatal errors thateventually led to interaction breakdowns.6 Summary and Future WorkWe have described a computational model for managingengagement decisions in open-world dialog.
The modelharnesses components for sensing and reasoning aboutthe engagement state, actions, and intentions of multipleparticipants in the scene, for making high-level en-gagement control decisions about who and when to en-gage, and for executing and rendering these actions inan embodied agent.
We reviewed an observational studythat showed that, when weaved together, these compo-nents can provide support for effectively managing en-gagement, and for creating and conducting multipartyinteractions in an open-world context.We believe that the components and policies we havepresented provide a skeleton for engagement and inte-raction in open-world settings.
However, there are im-portant challenges and opportunities ahead.
Future re-search includes developing methods for fine tuning andoptimizing each of these subcomponents and their inte-ractions.
Along these lines, there are opportunities toemploy machine learning to tune and adapt multipleaspects of the operation of the system.
In (Bohus andHorvitz, 2009b) we introduce and evaluate an approachto learning models for inferring engagement actions andintentions online, through interaction.
On another direc-tion, we are investigating the use of decision-theoreticapproaches for optimizing mixed-initiative engagementpolicies by taking into account the underlying uncertain-ties, the costs and benefits of interruption versus contin-uing collaboration, queue etiquette associated with ex-pectations of fairness, etc.
Another difficult challenge isthe creation of accurate low-level behavioral models,including the fine-grained control of pose, gesture, andfacial expressions.
Developing such methods will likelyhave subtle, yet powerful influences on the effectivenessof signaling and overall grounding in multiparty set-tings.
We believe that research on these and other prob-lems of open-world dialog will provide essential andnecessary steps towards developing computational sys-tems that can embed interaction deeply into the naturalflow of everyday tasks, activities, and collaborations.AcknowledgmentsWe thank George Chrysanthakopoulos, Zicheng Liu,Tim Paek, Cha Zhang, and Qiang Wang for discussionsand feedback in the development of this work.ReferencesM.
Argyle and M. Cook, 1976, Gaze and Mutual Gaze, Cam-bridge University Press, New YorkD.
Bohus and E. Horvitz, 2009a, Open-World Dialog: Chal-lenges, Directions and Prototype, to appear in KRPD?09,Pasadena, CAD.
Bohus and E. Horvitz, 2009b, An Implicit-Learning BasedModel for Detecting Engagement Intentions, submitted toSIGdial?09, London, UKE.
Goffman, 1963, Behaviour in public places: notes on thesocial order of gatherings, The Free Press, New YorkE.T.
Hall, 1966, The Hidden Dimension: man?s use of space inpublic and private, New York: Doubleday.A.
Kendon, 1990, A description of some human greetings,Conducting Interaction: Patterns of behavior in focused en-counters, Studies in International Sociolinguistics, Cam-bridge University PressA.
Kendon, 1990b, Spatial organization in social encounters:the F-formation system, Conducting Interaction: Patterns ofbehavior in focused encounters, Studies in InternationalSociolinguistics, Cambridge University PressM.P.
Michalowski, S. Sabanovic, and R. Simmons, A spatialmodel of engagement for a social robot, in 9th IEEE Work-shop on Advanced Motion Control, pp.
762-767C.
Peters, C. Pelachaud, E. Bevacqua, and M. Mancini, 2005,A model of attention and interest using gaze behavior, Lec-ture Notes in Computer Science, pp.
229-240.C.
Peters, 2005b, Direction of Attention Perception for Con-versation Initiation in Virtual Environments, in IntelligentVirtual Agents, 2005, pp.
215-228.C.L.
Sidner, C.D.
Kidd, C. Lee, and N. Lesh, 2004, Where toLook: A Study of Human-Robot Engagement, IUI?2004, pp.78-84, Madeira, PortugalC.L.
Sidner, C. Lee, C.D.
Kidd, N. Lesh, and C. Rich, 2005,Explorations in engagement for humans and robots, Artifi-cial Intelligence, 166 (1-2), pp.
140-164Situated Interaction, 2009, Project page:http://research.microsoft.com/~dbohus/research_situated_interaction.htmlR.
Vertegaal, R. Slagter, G.C.v.d.Veer, and A. Nijholt, 2001,Eye gaze patterns in conversations: there is more to con-versational agents than meets the eyes, CHI?012321:S:S:1:S:1:S:1:S:1:S:S:S:S:2:S:2:S:S:S:S:1:S:1:S:1:S:1:S:S:S:S:2:S:2:S:S:S:1:S:2:S:1:S:2:S:2:S:1:[approaching][engaging]Hi!Would you like to play a trivia game?
If you answer threeout of four questions correctly I can enter you in a rafflefor a Zune player.Sure.Okay.
First, could you please swipe your badge belowso I can email you the raffle ticket?
[swipes badge; in the meantime, bystander 2 appears]Got it!
Here is the first question.The state with the most shark attacks each year isCalifornia, Florida or Hawaii?Florida.Florida.
Is that final?YupThat?s right[disengaging]Excuse me for one second.
[engaging]HiWould you like to join in?Uh.
No thanks.Sorry, did you want to join in?No.Well, you can just watch for now.
And if you wait for abit, we?ll be done and then you can start a new gameyourself[disengaging][engaging]Sorry about that.Next question.Which mammal lives the longest?The humanYou think it?s human, right?Uh no, no no.So what?s the correct answer?Elephant.Elephant.
Is that final?Yup.Sorry but that?s actually incorrect.
Current scientificthought is that the longest possible human life span is120 years, making humans the longest lived among themammals.
Perhaps you could use some help.
Hold onone more second.
[disengaging][engaging]Are you sure you don?t want to help out?
Come on, thisis fun.Okay.Sorry, so did you wanna join in?Yes.Great[disengaging][re-engaging with both 1 and 2]So let?s see if you can do better together.
So far youhave one correct answer.
Now on to the next question.Is it true that Black Friday, the Friday after Thanksgivingis the busiest shopping day in the year?I heard that it wasn?t true actually?
false.Do you also think that?s false?I guess.You got it!
Okay, so far you have 2 out of 3 correctanswers.
If you get this last question right you can enterthe raffle to win a Zune.
How many people are killed byvending machines at the beach in a typical year?It?s probably more than one?
I?ll say thirteen.Do you think that?s correct?No, thirteen.Is that final?Yes.Congratulations, you had 3 correct answers out of 4questions.
I will record an entry for you in the Zuneraffle.Great, thanksBye-byeActivet1t2t3t4t5t6t7t8t9t10t11t12t13t14t15Engage({1},i1)Maintain({1},i1)Disengage({1},i1)Engage({2},i2)Maintain({2},i2)Diseng ({2},i2)Engage({1},i1)Maintain({1},i1)Disengage({1},i1)Engage({2},i3)Maintain({2},i3)Diseng({2},i3)Engage({1,2},i4)Maintain({1,2},i4)Disengage({1,2},i4)t0Appendix A.
Sample multiparty interaction with trivia game dialog system (not part of the experiment)1212121112ActiveSuspendedSuspended12first person engages -around time t2bystander appears ?
prior to t3system engages bystander ~ t5participants play together ~ t14systemagentactive conversa-tion suspended conver-sation other conversationActiveActiveActive233Appendix B.
User responses to multiparty engagement actions.S denotes the system, E denotes the already engaged participant, B denotes a watching bystander.Actions and Resposes Response from B Response from E Timing[S to E]: Hold on one second.
[S to B]: Excuse me, would you like to join in?4 positive answers from B7 negative answers from B1 no answer from B (E answers)Yes  B onlyYes Say yes OverlapSure  B onlyYes [to B]: Would you like to joinin?E firstNo That?s crazy!
B firstOh, no.
No + [moves away] That?s funny!
B firstNo thank you  B onlyNo No?
B firstWoah, no.
That?s cool!
B firstNo + [moves away] + That?spretty funny.
[laughs looking at B] B first[laughs] [laughs] Yes.
Oh yes.
E only[S to E]: Perhaps you could use some help.Excuse me for one second.
[S to B]: Hi, do you think you could help out?3 positive answers from B2 negative answers from B1 no-answer from B (moves away)Yes.
Yes.
B firstYes Yes he does.
Overlap[laughs] + No.
B only[to E]: Isn?t that weird?
[to S]: No.
[to E]: Isn?t that great?
[to B]: That is amazing!
B first[laughs] + [moves out] Quit E only[laughs] + Sure Sure B firstIf the initial response from B was not unders-tood by the system, system asks one more time[S to B]: Sorry, did you want to join in?1 positive answer from B3 negative answer from B1 no-answer from B (E answers)No.Please.Yes, I don?t know, help me!
B firstNo.
B onlyNo.
B onlyNo.
B onlyNo.
E only[S to B]: We?re almost done here.
If you waitfor a bit we can start a new gameright after.1 answer from B1 answer from E4 no-answer from either B or EGreat, thanks.
B onlyThat?s awesome E only234
