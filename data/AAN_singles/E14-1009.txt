Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 78?87,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsMulti-Granular Aspect Aggregation in Aspect-Based Sentiment AnalysisJohn Pavlopoulos and Ion AndroutsopoulosDepartment of InformaticsAthens University of Economics and BusinessPatission 76, GR-104 34 Athens, Greecehttp://nlp.cs.aueb.gr/AbstractAspect-based sentiment analysis estimatesthe sentiment expressed for each particu-lar aspect (e.g., battery, screen) of an en-tity (e.g., smartphone).
Different wordsor phrases, however, may be used to re-fer to the same aspect, and similar as-pects may need to be aggregated at coarseror finer granularities to fit the availablespace or satisfy user preferences.
We in-troduce the problem of aspect aggrega-tion at multiple granularities.
We decom-pose it in two processing phases, to al-low previous work on term similarity andhierarchical clustering to be reused.
Weshow that the second phase, where aspectsare clustered, is almost a solved prob-lem, whereas further research is neededin the first phase, where semantic simi-larity measures are employed.
We alsointroduce a novel sense pruning mecha-nism for WordNet-based similarity mea-sures, which improves their performancein the first phase.
Finally, we provide pub-licly available benchmark datasets.1 IntroductionGiven a set of texts discussing a particular en-tity (e.g., reviews of a laptop), aspect-based senti-ment analysis (ABSA) attempts to identify the mostprominent (e.g., frequently discussed) aspects ofthe entity (e.g., battery, screen) and the averagesentiment (e.g., 1 to 5 stars) for each aspect orgroup of aspects, as in Fig.
1.
Most ABSA systemsperform all or some of the following (Liu, 2012):subjectivity detection to retain only sentences (orother spans) expressing subjective opinions; as-pect extraction to extract (and possibly rank) termscorresponding to aspects (e.g., ?battery?
); aspectaggregation to group aspect terms that are near-synonyms (e.g., ?price?, ?cost?)
or to obtain aspectsFigure 1: Aspect groups and scores of an entity.at a coarser granularity (e.g., ?chicken?,?steak?,and ?fish?
may be replaced by ?food?
in restaurantreviews); and aspect sentiment score estimation toestimate the average sentiment for each aspect orgroup of aspects.
In this paper, we focus on aspectaggregation, the least studied stage of the four.Aspect aggregation is needed to avoid reportingseparate sentiment scores for aspect terms that arevery similar.
In Fig.
1, for example, showing sep-arate lines for ?money?, ?price?, and ?cost?
wouldbe confusing.
The extent to which aspect termsshould be aggregated, however, also depends onthe available space and user preferences.
On de-vices with smaller screens, it may be desirable toaggregate aspect terms that are similar, though notnecessarily near-synonyms (e.g., ?design?, ?color?,?feeling?)
to show fewer lines (Fig.
1), but finer as-pects may be preferable on larger screens.
Usersmay also wish to adjust the granularity of aspects,e.g., by stretching or narrowing the height of Fig.
1on a smartphone to view more or fewer lines.Hence, aspect aggregation should be able to pro-duce groups of aspect terms for multiple granular-ities.
We assume that the aggregated aspects aredisplayed as lists of terms, as in Fig.
1.
We makeno effort to order (e.g., by frequency) the terms ineach list, nor do we attempt to produce a single(more general) term to describe each aggregatedaspect, leaving such tasks for future work.ABSA systems usually group synonymous (ornear-synonymous) aspect terms (Liu, 2012).
Ag-78gregating only synonyms (or near-synonyms),however, does not allow users to select the desir-able aspect granularity, and ignores the hierarchi-cal relations between aspect terms.
For example,?pizza?
and ?steak?
are kinds of ?food?
and, hence,the three terms can be aggregated to show fewer,coarser aspects, even though they are not syn-onyms.
Carenini et al.
(2005) used a predefineddomain-specific taxonomy to hierarchically aggre-gate aspect terms, but taxonomies of this kindare often not available.
By contrast, we use onlygeneral-purpose taxonomies (e.g., WordNet), termsimilarity measures based on general-purpose tax-onomies or corpora, and hierarchical clustering.We define multi-granular aspect aggregation tobe the task of partitioning a given set of aspectterms (generated by a previous aspect extractionstage) into k non-overlapping clusters, for multi-ple values of k. A further constraint is that theclusters have to be consistent for different k val-ues, meaning that if two aspect terms t1, t2areplaced in the same cluster for k = k1, then t1and t2must also be grouped together (in the samecluster) for every k = k2with k2< k1, i.e., forevery coarser grouping.
For example, if ?waiter?and ?service?
are grouped together for k = 5, theymust also be grouped together for k = 4, 3, 2and (trivially) k = 1, to allow the user to feelthat selecting a smaller number of aspect groups(narrowing the height of Fig.
1) has the effect ofzooming out (without aspect terms jumping un-expectedly to other aspect groups), and similarlyfor zooming in.1This requirement is satisfied byusing agglomerative hierarchical clustering algo-rithms (Manning and Sch?utze, 1999; Hastie et al.,2001), which in our case produce term hierarchieslike the ones of Fig.
2.
By using slices (nodes at aparticular depth) of the hierarchies that are closerto the root or the leaves, we obtain fewer or moreclusters.
The vertical dotted lines of Fig.
2 illus-trate two slices for k = 4.
By contrast, flat clus-tering algorithms (e.g., k-means) do not satisfy theconsistency constraint for different k values.Agglomerative clustering algorithms require ameasure of the distance between individuals, inour case a measure of how similar two aspectterms are, and a linkage criterion to specify whichclusters should be merged to form larger (coarser)clusters.
To experiment with different term sim-1We also require the clusters to be non-overlapping tomake this zooming in and out metaphor clearer to the user.Figure 2: Example aspect hierarchies produced byagglomerative hierarchical clustering.food fish sushi dishes winefood 5 4 4 4 2fish 4 5 4 2 1sushi 4 4 5 3 1dishes 4 2 3 5 2wine 2 1 1 2 5Table 1: An aspect term similarity matrix.ilarity measures and linkage criteria, we decom-pose multi-granular aspect aggregation in two pro-cessing phases.
Phase A fills in a symmetric ma-trix, like the one of Table 1, with scores show-ing the similarity of each pair of input aspectterms; the matrix in effect defines the distancemeasure to be used by agglomerative clustering.In Phase B, the aspect terms are grouped into knon-overlapping clusters, for varying values of k,given the matrix of Phase A and a linkage crite-rion; a hierarchy like the ones of Fig.
2 is firstformed via agglomerative clustering, and fewer ormore clusters (for different values of k) are thenobtained by using different slices of the hierarchy,as already discussed.
Our two-phase decomposi-tion can also accommodate non-hierarchical clus-tering algorithms, provided that the consistencyconstraint is satisfied, but we consider only ag-glomerative hierarchical clustering in this paper.The decomposition in two phases has threemain advantages.
Firstly, it allows reusing previ-ous work on term similarity measures (Zhang etal., 2013), which can be used to fill in the ma-trix of Phase A. Secondly, the decomposition al-lows different linkage criteria to be experimen-tally compared (in Phase B) using the same sim-ilarity matrix (of Phase A), i.e., the same distance79measure.
Thirdly, the decomposition leads to highinter-annotator agreement, as we show experimen-tally.
By contrast, in preliminary experiments wefound that asking humans to directly evaluate as-pect hierarchies produced by hierarchical cluster-ing, or to manually create gold aspect hierarchiesled to poor inter-annotator agreement.We show that existing term similarity measuresperform reasonably well in Phase A, especiallywhen combined, but there is a large scope for im-provement.
We also propose a novel sense pruningmethod for WordNet-based similarity measures,which leads to significant improvements in PhaseA.
In Phase B, we experiment with agglomera-tive clustering using four different linkage criteria,concluding that they all perform equally well andthat Phase B is almost a solved problem when thegold similarity matrix of Phase A is used; how-ever, further improvements are needed in the sim-ilarity measures of Phase A to produce a suffi-ciently good similarity matrix.
We also make pub-licly available the datasets of our experiments.Our main contributions are: (i) to the bestof our knowledge, we are the first to considermulti-granular aspect aggregation (not just merg-ing near-synonyms) in ABSA without manuallycrafted domain-specific ontologies; (ii) we pro-pose a two-phase decomposition that allows previ-ous work on term similarity and hierarchical clus-tering to be reused and evaluated with high inter-annotator agreement; (iii) we introduce a novelsense pruning mechanism that improves WordNet-based similarity measures; (iv) we provide the firstpublic datasets for multi-granular aspect aggrega-tion; (v) we show that the second phase of our de-composition is almost a solved problem, and thatresearch should focus on the first phase.
Althoughwe experiment with customer reviews of productsand services, ABSA and the work of this paper inparticular are, at least in principle, also applicableto texts expressing opinions about other kinds ofentities (e.g., politicians, organizations).Section 2 below discusses related work.
Sec-tions 3 and 4 present our work for Phase A and B,respectively.
Section 5 concludes.2 Related workMost existing approaches to aspect aggregationaim to produce a single, flat partitioning of as-pect terms into aspect groups, rather than aspectgroups at multiple granularities.
The most com-mon approaches (Liu, 2012) are to aggregate onlysynonyms or near-synonyms, using WordNet (Liuet al., 2005), statistics from corpora (Chen et al.,2006; Bollegala et al., 2007a; Lin and Wu, 2009),or semi-supervised learning (Zhai et al., 2010;Zhai et al., 2011), or to cluster the aspect termsusing (latent) topic models (Titov and McDonald,2008a; Guo et al., 2009; Brody and Elhadad, 2010;Jo and Oh, 2011).
Topic models do not performbetter than other methods (Zhai et al., 2010), andtheir clusters may overlap.2The topic model ofTitov et al.
(2008b) uses two granularity levels;we consider many more (3?10 levels).Carenini et al.
(2005) used a predefined domain-specific taxonomy and similarity measures to ag-gregate related terms.
Yu et al.
(2011) used a tai-lored version of an existing taxonomy.
By con-trast, we assume no domain-specific taxonomy.Kobayashi et al.
(2007) proposed methods to ex-tract aspect terms and relations between them, in-cluding hierarchical relations.
They extract, how-ever, relations by looking for clues in texts (e.g.,particular phrases).
By contrast, we employ simi-larity measures and hierarchical clustering, whichallows us to group similar aspect terms even whenthey do not cooccur in texts.
Also, in contrastto Kobayashi et al.
(2007), we respect the consis-tency constraint discussed in Section 1.A similar task is taxonomy induction.
Cimi-ano and Staab (2005) automatically construct tax-onomies from texts via agglomerative clustering,much as in our Phase B, but not in the context ofABSA, and without trying to learn a similarity ma-trix first.
They also label the hierarchy?s concepts,a task we do not consider.
Klapaftis and Manand-har (2010) show how word sense induction can becombined with agglomerative clustering to obtainmore accurate taxonomies, again not in the con-text of ABSA.
Our sense pruning method was in-fluenced by their work, but is much simpler thantheir word sense induction.
Fountain and Lapata(2012) study unsupervised methods to induce con-cept taxonomies, without considering ABSA.3 Phase AWe now discuss our work for Phase A.
Recall thatin this phase the input is a set of aspect terms and2Topic models are typically also used to perform aspectextraction, apart from aspect aggregation, but simple heuris-tics (e.g., most frequent nouns) often outperform them in as-pect extraction (Liu, 2012; Moghaddam and Ester, 2012).80the goal is to fill in a matrix (Table 1) with scoresshowing the similarity of each pair of aspect terms.3.1 Datasets used in Phase AWe used two benchmark datasets that we had pre-viously constructed to evaluate ABSA methods forsubjectivity detection, aspect extraction, and as-pect score estimation, but not aspect aggregation.We extended them to support aspect aggregation,and we make them publicly available.3The two original datasets contain sentencesfrom customer reviews of restaurants and laptops,respectively.
The reviews are manually split intosentences, and each sentence is manually anno-tated as ?subjective?
(expressing opinion) or ?ob-jective?
(not expressing opinion).
The restaurantsdataset contains 3,710 English sentences from therestaurant reviews of Ganu et al.
(2009).
The lap-tops dataset contains 3,085 English sentences from394 customer reviews, collected from sites thathost customer reviews.
In the experiments of thispaper, we use only the 3,057 (out of 3,710) sub-jective restaurant sentences and the 2,631 (out of3,085) subjective laptop sentences.For each subjective sentence, our datasets showthe words that human annotators marked as aspectterms.
For example, in ?The dessert was divine!
?the aspect term is ?dessert?, and in ?Really badwaiter.?
it is ?waiter?.
Among the 3,057 subjectiverestaurant sentences, 1,129 contain exactly one as-pect term, 829 more than one, and 1,099 no aspectterm; a subjective sentence may express an opin-ion about the restaurant (or laptop) being reviewedwithout mentioning a specific aspect (e.g., ?Reallynice restaurant!?
), which is why no aspect termsare present in some subjective sentences.
Thereare 558 distinct multi-word aspect terms and 431distinct single-word aspect terms in the subjectiverestaurant sentences.
Among the 2,631 subjectivesentences of the laptop reviews, 823 contain ex-actly one aspect term, 389 more than one, and1,419 no aspect term.
There are 273 distinct multi-word aspect terms and 330 distinct single-word as-pect terms in the subjective laptop sentences.From each dataset, we selected the 20 (distinct)aspect terms that the human annotators had anno-tated most frequently, taking annotation frequencyto be an indicator of importance; there are onlytwo multi-word aspect terms (?hard drive?, ?bat-3The datasets are available at http://nlp.cs.aueb.gr/software.html.tery life?)
among the 20 most frequent ones in thelaptops dataset, and none among the 20 most fre-quent aspect terms of the restaurants dataset.
Wethen formed all the 190 possible pairs of the 20terms and constructed an empty similarity matrix(Fig.
1), one for each dataset, which was givento three human judges to fill in (1: strong dis-similarity, 5: strong similarity).4For each aspectterm, all the subjective sentences mentioning theterm were also provided, to help the judges un-derstand how the terms are used in the particu-lar domains (e.g., ?window?
and ?Windows?
havedomain-specific meanings in laptop reviews).The Pearson correlation coefficient indicatedhigh inter-annotator agreement (0.81 for restau-rants, 0.74 for laptops).
We also measured the ab-solute inter-annotator agreement a(l1, l2), definedbelow, where l1, l2are lists containing the scores(similarity matrix values) of two judges, N is thelength of each list, and vmax, vminare the largestand smallest possible scores (5 and 1).a(l1, l2) =1NN?i=1[1?|l1(i)?
l2(i)|vmax?
vmin]The absolute interannotator agreement was alsohigh (0.90 for restaurants, 0.91 for laptops).5Withboth measures, we compute the agreement of eachjudge with the averaged (for each matrix cell)scores of the other two judges, and we report themean of the three agreement estimates.
Finally, wecreated the gold similarity matrix of each datasetby placing in each cell the average scores that thethree judges had provided for that cell.In preliminary experiments, we gave aspectterms to human judges, asking them to group anyterms they considered near-synonyms.
We thenasked the judges to group the aspect terms intofewer, coarser groups by grouping terms that couldbe viewed as direct hyponyms of the same broaderterm (e.g., ?pizza?
and ?steak?
are both kinds of?food?
), or that stood in a hyponym-hypernym re-lation (e.g., ?pizza?
and ?food?).
We used theDice coefficient to measure inter-annotator agree-ment, and we obtained reasonably good agreementfor near-synonyms (0.77 for restaurants, 0.81 forlaptops), but poor agreement for the coarser as-4The matrix is symmetric; hence, the judges had to fill inonly half of it.
The guidelines and an annotation tool thatwere given to the judges are available upon request.5The Pearson correlation ranges from ?1 to 1, whereasthe absolute inter-annotator agreement ranges from 0 to 1.81pects (0.25 and 0.11).6In other preliminary ex-periments, we asked human judges to rank alter-native aspect hierarchies that had been producedby applying agglomerative clustering with differ-ent linkage criteria to 20 aspect terms, but we ob-tained very poor inter-annotator agreement (Pear-son score ?0.83 for restaurants and 0 for laptops).3.2 Phase A methodsWe employed five term similarity measures.
Thefirst two are WordNet-based (Budanitsky andHirst, 2006).
The next two combine WordNet withstatistics from corpora.
The fifth one is a corpus-based distributional similarity measure.The first measure is Wu and Palmer?s (1994).
Itis actually a sense similarity measure (a term mayhave multiple senses).
Given two senses sij, si?j?of terms ti, ti?, the measure is defined as follows:WP(sij, si?j?)
= 2 ?depth(lcs(sij, si?j?
))depth(sij) + depth(sij),where lcs(sij, si?j?)
is the least common sub-sumer, i.e., the most specific common ancestor ofthe two senses in WordNet, and depth(s) is thedepth of sense s in WordNet?s hierarchy.Most terms have multiple senses, however,and word sense disambiguation methods (Navigli,2009) are not yet robust enough.
Hence, whengiven two aspect terms ti, ti?, rather than particularsenses of the terms, a simplistic greedy approachis to compute the similarities of all the possiblepairs of senses sij, si?j?of ti, ti?, and take the sim-ilarity of ti, ti?to be the maximum similarity ofthe sense pairs (Bollegala et al., 2007b; Zesch andGurevych, 2010).
We use this greedy approachwith all the WordNet-based measures, but we alsopropose a sense pruning mechanism below, whichimproves their performance.
In all the WordNet-based measures, if a term is not in WordNet, wetake its similarity to any other term to be zero.7The second measure, PATH (sij, si?j?
), is sim-ply the inverse of the length (plus one) of the short-est path connecting the senses sij, si?j?in WordNet(Zhang et al., 2013).
Again, the greedy approachcan be used with terms having multiple senses.6The Dice coefficient ranges from 0 to 1.
There was a verylarge number of possible responses the judges could provideand, hence, it would be inappropriate to use Cohen?s K.7This never happened in the restaurants dataset.
In thelaptops dataset, it only happened for ?hard drive?
and ?bat-tery life?.
We use the NLTK implementation of the first fourmeasures (see http://nltk.org/) and our own imple-mentation of the distributional similarity measure.The third measure is Lin?s (1998), defined as:LIN (sij, si?j?)
=2 ?
ic(lcs(sij, si?j?
))ic(sij) + ic(si?j?
),where sij, si?j?are senses of terms ti, ti?,lcs(sij, si?j?)
is the least common subsumer ofsij, si?j?in WordNet, and ic(s) = ?
logP(s) isthe information content of sense s (Pedersen et al.,2004), estimated from a corpus.
When the cor-pus is not sense-tagged, we follow the commonapproach of treating each occurrence of a word asan occurrence of all of its senses, when estimat-ing ic(s).8We experimented with two variants ofLin?s measure, one where the ic(s) scores wereestimated from the Brown corpus (Marcus et al.,1993), and one where they were estimated fromthe (restaurant or laptop) reviews of our datasets.The fourth measure is Jiang and Conrath?s(1997), defined below.
Again, we experimentedwith two variants of ic(s), as above.JCN (sij, si?j?)
=1ic(sij) + ic(si?j?)?
2 ?
lcs(sij, si?j?
)For all the above WordNet-based measures, weexperimented with a sense pruning mechanism,which discards some of the senses of the aspectterms, before applying the greedy approach.
Foreach aspect term ti, we consider all of its Word-Net senses sij.
For each sijand each other aspectterm ti?, we compute (using PATH ) the similar-ity between sijand each sense si?j?of ti?, and weconsider the relevance of sijto ti?to be:9rel(sij, ti?)
= maxsi?j??
senses(ti?
)PATH (sij, si?j?
)The relevance of sijto all of the N other aspectterms ti?is taken to be:rel(sij) =1N?
?i?6=irel(sij, ti?
)For each aspect term ti, we retain only its sensessijwith the top rel(sij) scores, which tends to8http://www.d.umn.edu/?tpederse/Data/README-WN-IC-30.txt.
We use the default counting.9We also experimented with other similarity measureswhen computing rel(sij, ti?
), instead of PATH , but therewas no significant difference.
We use NLTK to tokenize, re-move punctuation, and stop-words.82without SP with SPMethod Rest.
Lapt.
Rest.
Lapt.WP 0.475 0.216 0.502 0.265PATH 0.524 0.301 0.529 0.332LIN@domain 0.390 0.256 0.456 0.343LIN@Brown 0.434 0.329 0.471 0.391JCN@domain 0.467 0.348 0.509 0.448JCN@Brown 0.403 0.469 0.419 0.539DS 0.283 0.517 (0.283) (0.517)AVG 0.499 0.352 0.537 0.426WN 0.490 0.328 0.530 0.395WNDS 0.523 0.453 0.545 0.546Table 2: Phase A results (Pearson correlation togold similarities) with and without sense pruning.prune senses that are very irrelevant to the par-ticular domain (e.g., laptops).
This sense prun-ing mechanism is novel, and we show experimen-tally that it improves the performance of all theWordNet-based similarity measures we examined.We also implemented a distributional simi-larity measure (Harris, 1968; Pad?o and Lap-ata, 2007; Cimiano et al., 2009; Zhang et al.,2013).
Following Lin and Wu (2009), foreach aspect term t, we create a vector ~v(t) =?PMI (t, w1), .
.
.
,PMI (t, wn)?.
The vector com-ponents are the Pointwise Mutual Informationscores of t and each word wiof a corpus:PMI (t, wi) = ?
logP (t, wi)P (t) ?
P (wi)We treat P (t, wi) as the probability of t, wicooc-curring in the same sentence, and we use the (lap-top or restaurant) reviews of our datasets as thecorpus to estimate the probabilities.
The distribu-tional similarity DS (t, t?)
of two aspect terms t, t?is the cosine similarity of ~v(t), ~v(t?
).10Finally, we tried combinations of the similaritymeasures: AVG is the average of all five; WN isthe average of the first four, which employ Word-Net; and WNDS is the average of WN and DS ;all the scores range in [0, 1].
We also tried regres-sion (e.g., SVR), but there was no improvement.3.3 Phase A experimental resultsEach similarity measure was evaluated by comput-ing its Pearson correlation with the scores of thegold similarity matrix.
Table 2 shows the results.Our sense pruning consistently improves allfour WordNet-based measures.
It does not apply to10We also experimented with Euclidean distance, a nor-malized PMI (Bouma, 2009), and the Brown corpus, butthere was no improvement.DS , which is why the DS results are identical withand without pruning.
A paired t test indicates thatthe other differences (with and without pruning) ofTable 2 are statistically significant (p < 0.05).
Weused the senses with the top five rel(sij) scores foreach aspect term tiduring sense pruning.
We alsoexperimented with keeping fewer senses, but theresults were inferior or there was no improvement.Lin?s measure performed better when infor-mation content was estimated on the (muchlarger, but domain-independent) Brown corpus(LIN@Brown), as opposed to using the (domain-specific) reviews of our datasets (LIN@domain),but we observed no similar consistent pattern forJCN .
Given its simplicity, PATH performed re-markably well in the restaurants dataset; it wasthe best measure (including combinations) withoutsense pruning, and the best uncombined measurewith sense pruning.
It performed worse, however,compared to several other measures in the laptopsdataset.
Similar comments apply to WP , which isamong the top-performing uncombined measuresin restaurants, both with and without sense prun-ing, but the worst overall measure in laptops.
DSis the best overall measure in laptops when com-pared to measures without sense pruning, and thethird best overall when compared to measures thatuse sense pruning, but the worst overall in restau-rants both with and without pruning.
LIN andJCN , which use both WordNet and corpus statis-tics, have a more balanced performance across thetwo datasets, but they are not top-performers inany of the two.
Combinations of similarity mea-sures seem more stable across domains, as the re-sults of AVG , WN , and WNDS indicate, thoughexperiments with more domains are needed to in-vestigate this issue.
WNDS is the best overallmethod with sense pruning, and among the bestthree methods without pruning in both datasets.To get a better view of the performance ofWNDS with sense pruning, i.e., the best overallmeasure of Table 2, we compared it to two state ofthe art semantic similarity systems.
First, we ap-plied the system of Han et al.
(2013), one of thebest systems of the recent *Sem 2013 semantictext similarity competition, to our Phase A data.The performance (Pearson correlation with goldsimilarities) of the same system on the widely usedWordSim353 word similarity dataset (Agirre et al.,2009) is 0.73, much higher than the same system?sperformance on our Phase A data (see Table 3),83Method Restaurants LaptopsHan et al.
(2013) 0.450 0.471Word2Vec 0.434 0.485WNDS with SP 0.545 0.546Judge 1 0.913 0.875Judge 2 0.914 0.894Judge 3 0.888 0.924Table 3: Phase A results (Pearson correlation togold similarities) of WNDS with SP against se-mantic similarity systems and human judges.which suggests that our data are more difficult.11We also employed the recent Word2Vec sys-tem, which computes continuous vector space rep-resentations of words from large corpora and hasbeen reported to improve results in word similaritytasks (Mikolov et al., 2013).
We used the EnglishWikipedia to compute word vectors with 200 fea-tures.12The similarity between two aspect termswas taken to be the cosine similarity of their vec-tors.
This system performed better than Han etal.
?s with laptops, but not with restaurants.Table 3 shows that WNDS (with sense prun-ing) performed clearly better than the system ofHan et al.
and Word2Vec.
Table 3 also showsthe Pearson correlation of each judge?s scores tothe gold similarity scores, as an indication of thebest achievable results.
Although WNDS (withsense pruning) performs reasonably well in bothdomains,13there is large scope for improvement.4 Phase BIn Phase B, the aspect terms are to be groupedinto k non-overlapping clusters, for varying val-ues of k, given a Phase A similarity matrix.
Weexperimented with both the gold similarity matrixof Phase A and similarity matrices produced byWNDS (with SP), the best Phase A method.4.1 Phase B methodsWe experimented with agglomerative clusteringand four linkage criteria: single, complete, av-erage, and Ward (Manning and Sch?utze, 1999;Hastie et al., 2001).
Let d(t1, t2) be the distance of11The system of Han et al.
(2013) is available fromhttp://semanticwebarchive.cs.umbc.edu/SimService/; we use the STS similarity.12Word2Vec is available from https://code.google.com/p/word2vec/.
We used the continuousbag of words model with default parameters, the first billioncharacters of the English Wikipedia, and the preprocessing ofhttp://mattmahoney.net/dc/textdata.html.13Recall that the Pearson correlation ranges from ?1 to 1.two individual instances t1, t2; in our case, the in-stances are aspect terms and d(t1, t2) is the inverseof the similarity of t1, t2, defined by the Phase Asimilarity matrix (gold or produced by WNDS ).Different linkage criteria define differently the dis-tance of two clusters D(C1, C2), which affectsthe choice of clusters that are merged to producecoarser (higher-level) clusters:Dsingle(C1, C2) = mint1?C1,t2?C2d(t1, t2)Dcompl(C1, C2) = maxt1?C1,t2?C2d(t1, t2)Davg(C1, C2) =1|C1||C2|?t1?C1?t2?C2d(t1, t2)Complete linkage tends to produce more compactclusters, compared to single linkage, with averagelinkage being in between.
Ward minimizes the to-tal in-cluster variance; consult Milligan (1980) forfurther details.144.2 Phase B experimental resultsTo evaluate the k clusters produced at each aspectgranularity by the different linkage criteria, weused the Silhouette Index (SI ) (Rousseeuw, 1987),a cluster evaluation measure that considers bothinter- and intra-cluster coherence.15Given a set ofclusters {C1, .
.
.
, Ck}, each SI (Ci) is defined as:SI (Ci) =1|Ci|?|Ci|?j=1bj?
ajmax(bj, aj),where ajis the mean distance from the j-th in-stance of Cito the other instances in Ci, and bjisthe mean distance from the j-th instance of Citothe instances in the cluster nearest to Ci.
Then:SI ({C1, .
.
.
, Ck}) =1k?k?i=1SI (Ci)We always use the correct (gold) distances of theinstances (terms) when computing the SI scores.As shown in Fig.
3, no linkage criterion clearlyoutperforms the others, when the gold matrix ofPhase A is used; all four criteria perform reason-ably well.
Note that the SI ranges from ?1 to14We used the SCIPY implementations of agglomera-tive clustering with the four criteria (see http://www.scipy.org), relying on maxclust to obtain the slice of theresulting hierarchy that leads to k (or approx.
k) clusters.15We used the SI implementation of Pedregosa etal.
(2011); see http://scikit-learn.org/.
We alsoexperimented with the Dunn Index (Dunn, 1974) and theDavies-Bouldin Index (1979), but we obtained similar results.84(a) restaurants (b) laptopsFigure 3: Silhouette Index (SI) results for PhaseB, using the gold similarity matrix of Phase A.
(a) restaurants (b) laptopsFigure 4: SI results for Phase B, using the WNDS(with SP) similarity matrix of Phase A.1, with higher values indicating better clustering.Figure 4 shows that when the similarity matrix ofWNDS (with SP) is used, the SI scores deterio-rate significantly; again, there is no clear winneramong the linkage criteria, but average and Wardseem to be overall better than the others.
(a) Restaurants (b) LaptopsFigure 5: Human evaluation of aspect groups.In a final experiment, we showed clusteringsof varying granularities (k values) to four humanjudges (graduate CS students).
The clusteringswere produced by two systems: one that used thegold similarity matrix of Phase A and agglomer-ative clustering with average linkage in Phase B,and one that used the similarity matrix of WNDS(with SP) and again agglomerative clustering withaverage linkage.
We showed all the clusteringsto all the judges.
Each judge was asked to eval-uate each clustering on a 1?5 scale.
We measuredthe absolute inter-annotator agreement, as in Sec-tion 3.1, and found high agreement in all cases(0.93 and 0.83 for the two systems, respectively,in restaurants; 0.85 for both in laptops).16Figure 5 shows the average human scores ofthe two systems for different granularities.
Thejudges considered the aspect groups always per-fect or near-perfect when the gold similarity ma-trix of Phase A was used, but they found the as-pect groups to be of rather poor quality whenthe similarity matrix of the best Phase A mea-sure was used.
These results, along with those ofFig.
3?4, show that more effort needs to be devotedto improving the similarity measures of Phase A,whereas Phase B is in effect an almost solvedproblem, if a good similarity matrix is available.5 ConclusionsWe considered a new, more demanding form ofaspect aggregation in ABSA, which aims to aggre-gate aspects at multiple granularities, as opposedto simply merging near-synonyms, and without as-suming that manually crafted domain-specific on-tologies are available.
We decomposed the prob-lem in two processing phases, which allow pre-vious work on term similarity and hierarchicalclustering to be reused and evaluated appropri-ately with high inter-annotator agreement.
Weshowed that the second phase, where we used ag-glomerative clustering, is an almost solved prob-lem, whereas further research is needed in the firstphrase, where term similarity measures are em-ployed.
We also introduced a sense pruning mech-anism that significantly improves WordNet-basedsimilarity measures, leading to a measure that out-performs state of the art similarity methods in thefirst phase of our decomposition.
We also madepublicly available the datasets of our experiments.AcknowledgmentsWe thank G. Batistatos, A. Zosakis, and G. Lam-pouras for their annotations in Phase A.
We thankA.
Kosmopoulos, G. Lampouras, P. Malakasiotis,and I. Lourentzou for their annotations in Phase B.16The Pearson correlation cannot be computed, as severaljudges gave the same rating to the first system, for all k.85ReferencesE.
Agirre, E. Alfonseca, K. Hall, J. Kravalova,M.
Pas?ca, and A. Soroa.
2009.
A study on similar-ity and relatedness using distributional and wordnet-based approaches.
In Proceedings of the AnnualConference of NAACL, pages 19?27, Boulder, CO,USA.D.
Bollegala, Y. Matsuo, and M. Ishizuka.
2007a.An integrated approach to measuring semantic sim-ilarity between words using information availableon the web.
In Proceedings of HLT-NAACL, pages340?347, Rochester, NY, USA.D.
Bollegala, Y. Matsuo, and M. Ishizuka.
2007b.Measuring semantic similarity between words usingweb search engines.
In Proceedings of the 16th In-ternational Conference of WWW, volume 766, pages757?766, Banff, Alberta, Canada.G.
Bouma.
2009.
Normalized (pointwise) mutual in-formation in collocation extraction.
Proceedings ofthe Biennial Conference of GSCL, pages 31?40.S.
Brody and N. Elhadad.
2010.
An unsupervisedaspect-sentiment model for online reviews.
In Pro-ceedings of the Annual Conference of NAACL, pages804?812, Los Angeles, CA, USA.A.
Budanitsky and G. Hirst.
2006.
EvaluatingWordNet-based measures of lexical semantic relat-edness.
Computational Linguistics, 32(1):13?47.G.
Carenini, R. T. Ng, and E. Zwart.
2005.
Extract-ing knowledge from evaluative text.
In Proceedingsof the 3rd International Conference on KnowledgeCapture, pages 11?18, Banff, Alberta, Canada.H.
Chen, M. Lin, and Y. Wei.
2006.
Novel associationmeasures using web search with double checking.In Proceedings of the 21st International Conferenceof COLING and the 44th Annual Meeting of ACL,pages 1009?1016, Sydney, Australia.P.
Cimiano and S. Staab.
2005.
Learning concept hier-archies from text with a guided hierarchical cluster-ing algorithm.
In Proceedings of ICML ?
Workshopon Learning and Extending Lexical Ontologies withMachine Learning Methods, Bonn, Germany.P.
Cimiano, A. M?adche, S. Staab, and J. V?olker.
2009.Ontology learning.
In Handbook on Ontologies,pages 245?267.
Springer.D.
L. Davies and D. W. Bouldin.
1979.
A cluster sepa-ration measure.
IEEE Transactions on Pattern Anal-ysis and Machine Intelligence, 1(2):224?227.J.
C. Dunn.
1974.
Well-separated clusters and optimalfuzzy partitions.
Journal of Cybernetics, 4(1):95?104.T.
Fountain and M. Lapata.
2012.
Taxonomy inductionusing hierarchical random graphs.
In Proceedings ofNAACL:HLT, pages 466?476, Montreal, Canada.G.
Ganu, N. Elhadad, and A. Marian.
2009.
Beyondthe stars: Improving rating predictions using reviewtext content.
In Proceedings of the 12th Interna-tional Workshop on the Web and Databases, Prov-idence, RI, USA.H.
Guo, H. Zhu, Z. Guo, X. Zhang, and Z. Su.
2009.Product feature categorization with multilevel latentsemantic association.
In Proceedings of the 18thCIKM, pages 1087?1096.L.
Han, A. Kashyap, T. Finin, J. Mayfield, andJ.
Weese.
2013.
Umbc ebiquity-core: Semantic tex-tual similarity systems.
In Proceedings of the 2ndJoint Conference on Lexical and Computational Se-mantics, pages 44?52, Atlanta, GA, USA.Z.
Harris.
1968.
Mathematical Structures of Lan-guage.
Wiley.T.
Hastie, R. Tibshirani, and J. Friedman.
2001.
TheElements of Statistical Learning.
Springer.J.
J. Jiang and D. W. Conrath.
1997.
Semantic similar-ity based on corpus statistics and lexical taxonomy.In Proceedings of ROCLING, pages 19?33, Taiwan,China.Y.
Jo and A. H. Oh.
2011.
Aspect and sentiment unifi-cation model for online review analysis.
In Proceed-ings of the 4th International Conference of WSDM,pages 815?824, Hong Kong, China.I.
P. Klapaftis and S. Manandhar.
2010.
Taxonomylearning using word sense induction.
In Proceedingsof NAACL, pages 82?90, Los Angeles, CA, USA.N.
Kobayashi, K. Inui, and Y. Matsumoto.
2007.
Ex-tracting aspect-evaluation and aspect-of relations inopinion mining.
In Proceedings of the Joint Confer-ence on EMNLP-CoNLL, pages 1065?1074, Prague,Czech Republic.D.
Lin and X. Wu.
2009.
Phrase clustering for dis-criminative learning.
In Proceedings of ACL, pages1030?1038, Suntec, Singapore.
ACL.D.
Lin.
1998.
An information-theoretic definition ofsimilarity.
In Proceedings of the 15th ICML, pages296?304, Madison, WI, USA.B.
Liu, M. Hu, and J. Cheng.
2005.
Opinion observer:analyzing and comparing opinions on the web.
InProceedings of the 14th International Conference ofWWW, pages 342?351, Chiba, Japan.B.
Liu.
2012.
Sentiment Analysis and Opinion Mining.Synthesis Lectures on Human Language Technolo-gies.
Morgan & Claypool.C.
D. Manning and H. Sch?utze.
1999.
Foundationsof Statistical Natural Language Processing.
MITPress, Cambridge, MA, USA.M.
P. Marcus, M. A. Marcinkiewicz, and B. Santorini.1993.
Building a large annotated corpus of en-glish: The penn treebank.
Computational Linguis-tics, 19(2):313?330.86T.
Mikolov, C. Kai, G. Corrado, and J.
Dean.
2013.Efficient estimation of word representations in vec-tor space.
CoRR, abs/1301.3781.G.W.
Milligan.
1980.
An examination of the effect ofsix types of error perturbation on fifteen clusteringalgorithms.
Psychometrika, 45(3):325?342.S.
Moghaddam and M. Ester.
2012.
On the design oflda models for aspect-based opinion mining.
In Pro-ceedings of the 21st CIKM, pages 803?812, Maui,HI, USA.R.
Navigli.
2009.
Word sense disambiguation: A sur-vey.
ACM Computing Surveys, 41(2):10:1?10:69.S.
Pad?o and M. Lapata.
2007.
Dependency-based con-struction of semantic space models.
ComputationalLinguistics, 33(2):161?199.T.
Pedersen, S. Patwardhan, and J. Michelizzi.
2004.Wordnet::similarity: measuring the relatedness ofconcepts.
In Proceedings of NAACL:HTL ?
Demon-strations, pages 38?41, Boston, MA, USA.F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B.
Thirion, O. Grisel, M. Blondel, P. Pretten-hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-sos, D. Cournapeau, M. Brucher, M. Perrot, andE.
Duchesnay.
2011.
Scikit-learn: Machine learn-ing in python.
Journal of Machine Learning Re-search, 12:2825?2830.P.
Rousseeuw.
1987.
Silhouettes: a graphical aid tothe interpretation and validation of cluster analysis.Journal of Computational and Applied Mathemat-ics, 20(1):53?65.I.
Titov and R. T. McDonald.
2008a.
A joint model oftext and aspect ratings for sentiment summarization.In Proceedings of the 46th Annual Meeting of ACL-HLT, pages 308?316, Columbus, OH, USA.I.
Titov and R. T. McDonald.
2008b.
Modeling onlinereviews with multi-grain topic models.
In Proceed-ings of the 17th International Conference of WWW,pages 111?120, Beijing, China.Z.
Wu and M. Palmer.
1994.
Verbs semantics and lexi-cal selection.
In Proceedings of the 32nd ACL, pages133?138, Las Cruces, NM, USA.J.
Yu, Z. Zha, M. Wang, K. Wang, and T. Chua.
2011.Domain-assisted product aspect hierarchy genera-tion: towards hierarchical organization of unstruc-tured consumer reviews.
In Proceedings of EMNLP,pages 140?150, Edinburgh, UK.T.
Zesch and I. Gurevych.
2010.
Wisdom of crowdsversus wisdom of linguists - measuring the semanticrelatedness of words.
Natural Language Engineer-ing, 16(1):25?59.Z.
Zhai, B. Liu, H. Xu, and P. Jia.
2010.
Group-ing product features using semi-supervised learningwith soft-constraints.
In Proceedings of the 23rdInternational Conference of COLING, pages 1272?1280, Beijing, China.Z.
Zhai, B. Liu, H. Xu, and P. Jia.
2011.
Clusteringproduct features for opinion mining.
In Proceedingsof the 4th International Conference of WSDM, pages347?354, Hong Kong, China.Z.
Zhang, A. Gentile, and F. Ciravegna.
2013.
Re-cent advances in methods of lexical semantic relat-edness - a survey.
Natural Language Engineering,FirstView(1):1?69.87
