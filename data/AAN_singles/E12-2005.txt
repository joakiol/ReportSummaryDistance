Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 20?24,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsSWAN ?
Scientific Writing AssistaNtA Tool for Helping Scholars to Write Reader-Friendly Manuscriptshttp://cs.joensuu.fi/swan/Tomi Kinnunen?
Henri Leisma Monika Machunik Tuomo Kakkonen Jean-Luc LebrunAbstractDifficulty of reading scholarly papers is sig-nificantly reduced by reader-friendly writ-ing principles.
Writing reader-friendly text,however, is challenging due to difficulty inrecognizing problems in one?s own writing.To help scholars identify and correct poten-tial writing problems, we introduce SWAN(Scientific Writing AssistaNt) tool.
SWANis a rule-based system that gives feedbackbased on various quality metrics based onyears of experience from scientific writ-ing classes including 960 scientists of var-ious backgrounds: life sciences, engineer-ing sciences and economics.
According toour first experiences, users have perceivedSWAN as helpful in identifying problem-atic sections in text and increasing overallclarity of manuscripts.1 IntroductionA search on ?tools to evaluate the quality of writ-ing?
often gets you to sites assessing only one ofthe qualities of writing: its readability.
Measur-ing ease of reading is indeed useful to determineif your writing meets the reading level of your tar-geted reader, but with scientific writing, the sta-tistical formulae and readability indices such asFlesch-Kincaid lose their usefulness.In a way, readability is subjective and depen-dent on how familiar the reader is with the spe-cific vocabulary and the written style.
Scien-tific papers are targeting an audience at ease with?
T. Kinnunen, H. Leisma, M. Machunik and T.Kakkonen are with the School of Computing, Univer-sity of Eastern Finland (UEF), Joensuu, Finland, e-mail:tkinnu@cs.joensuu.fi.
Jean-Luc Lebrun is an inde-pendent trainer of scientific writing and can be contacted atjllebrun@me.com.a more specialized vocabulary, an audience ex-pecting sentence-lengthening precision in writing.The readability index would require recalibrationfor such a specific audience.
But the need forreadability indices is not questioned here.
?Sci-ence is often hard to read?
(Gopen and Swan,1990), even for scientists.Science is also hard to write, and finding faultwith one?s own writing is even more challengingsince we understand ourselves perfectly, at leastmost of the time.
To gain objectivity scientiststurn away from silent readability indices and findmore direct help in checklists such as the peer re-view form proposed by Bates College1, or scor-ing sheets to assess the quality of a scientific pa-per.
These organise a systematic and critical walkthrough each part of a paper, from its title to itsreferences in peer-review style.
They integratereadability criteria that far exceed those coveredby statistical lexical tools.
For example, they ex-amine how the text structure frames the contentsunder headings and subheadings that are consis-tent with the title and abstract of the paper.
Theytest whether or not the writer fluidly meets the ex-pectations of the reader.
Written by expert review-ers (and readers), they represent them, their needsand concerns, and act as their proxy.
Such man-ual tools effectively improve writing (Chuck andYoung, 2004).Computer-assisted tools that support manualassessment based on checklists require naturallanguage understanding.
Due to the complexityof language, today?s natural language processing(NLP) techniques mostly enable computers to de-liver shallow language understanding when the1http://abacus.bates.edu/?ganderso/biology/resources/peerreview.html20vocabulary is large and highly specialized ?
as isthe case for scientific papers.
Nevertheless, theyare mature enough to be embedded in tools as-sisted by human input to increase depth of under-standing.
SWAN (ScientificWriting AssistaNt) issuch a tool (Fig.
1).
It is based on metrics testedon 960 scientists working for the research Insti-tutes of the Agency for Science, Technology andResearch (A*STAR) in Singapore since 1997.The evaluation metrics used in SWAN are de-scribed in detail in a book written by the designerof the tool (Lebrun, 2011).
In general, SWAN fo-cuses on the areas of a scientific paper that createthe first impression on the reader.
Readers, and inparticular reviewers, will always read these partic-ular sections of a paper: title, abstract, introduc-tion, conclusion, and the headings and subhead-ings of the paper.
SWAN does not assess the over-all quality of a scientific paper.
SWAN assessesits fluidity and cohesion, two of the attributes thatcontribute to the overall quality of the paper.
Italso helps identify other types of potential prob-lems such as lack of text dynamism, overly longsentences and judgmental words.Figure 1: Main window of SWAN.2 Related WorkAutomatic assessment of student-authored texts isan active area of research.
Hundreds of researchpublications related to this topic have been pub-lished since Page?s (Page, 1966) pioneering workon automatic grading of student essays.
The re-search on using NLP in support of writing scien-tific publications has, however, gained much lessattention in the research community.Amadeus (Aluisio et al 2001) is perhaps thesystem that is the most similar to the work out-lined in this system demonstration.
However, thefocus of the Amadeus system is mostly on non-native speakers on English who are learning towrite scientific publications.
SWAN is targetedfor more general audience of users.Helping our own (HOO) is an initiative thatcould in future spark a new interest in the re-search on using of NLP for supporting scientificwriting (Dale and Kilgarriff, 2010).
As the namesuggests, the shared task (HOO, 2011) focuses onsupporting non-native English speakers in writingarticles related specifically to NLP and computa-tional linguistics.
The focus in this initiative ison what the authors themselves call ?domain-and-register-specific error correction?, i.e.
correctionof grammatical and spelling mistakes.Some NLP research has been devoted to apply-ing NLP techniques to scientific articles.
Paquotand Bestgen (Paquot and Bestgen, 2009), for in-stance, extracted keywords from research articles.3 Metrics Used in SWANWe outline the evaluation metrics used in SWAN.Detailed description of the metrics is given in (Le-brun, 2011).
Rather than focusing on Englishgrammar or spell-checking included in most mod-ern word processors, SWAN gives feedback onthe core elements of any scientific paper: title, ab-stract, introduction and conclusions.
In addition,SWAN gives feedback on fluidity of writing andpaper structure.SWAN includes two types of evaluation met-rics, automatic and manual ones.
Automatic met-rics are solely implemented as text analysis of theoriginal document using NLP tools.
An examplewould be locating judgemental word patterns suchas suffers from or locating sentences with passivevoice.
The manual metrics, in turn, require user?sinput for tasks that are difficult ?
if not impossible?
to automate.
An example would be highlightingtitle keywords that reflect the core contribution ofthe paper, or highlighting in the abstract the sen-tences that cover the relevant background.Many of the evaluation metrics are stronglyinter-connected with each other, such as?
Checking that abstract and title are consis-tent; for instance, frequently used abstractkeywords should also be found in the title;21and the title should not include keywords ab-sent in the abstract.?
Checking that all title keywords are alsofound in the paper structure (from headingsor subheadings) so that the paper structure isself-explanatory.An important part of paper quality metrics is as-sessing text fluidity.
By fluidity we mean the easewith which the text can be read.
This, in turn,depends on how much the reader needs to mem-orize about what they have read so far in orderto understand new information.
This memorizingneed is greatly reduced if consecutive sentencesdo not contain rapid change in topic.
The aim ofthe text fluidity module is to detect possible topicdiscontinuities within and across paragraphs, andto suggest ways of improving these parts, for ex-ample, by rearranging the sentences.
The sugges-tions, while already useful, will improve in futureversions of the tool with a better understandingof word meanings thanks to WordNet and lexicalsemantics techniques.Fluidity evaluation is difficult to fully auto-mate.
Manual fluidity evaluation relies on thereader?s understanding of the text.
It is thereforesuperior to the automatic evaluation which relieson a set of heuristics that endeavor to identify textfluidity based on the concepts of topic and stressdeveloped in (Gopen, 2004).
These heuristics re-quire the analysis of the sentence for which theStanford parser is used.
These heuristics are per-fectible, but they already allow the identificationof sentences disrupting text fluidity.More fluidityproblems would be revealed through the manualfluidity evaluation.Simply put, here topic refers to the main fo-cus of the sentence (e.g.
the subject of the mainclause) while stress stands for the secondary sen-tence focus, which often becomes one of the fol-lowing sentences?
topic.
SWAN compares the po-sition of topic and stress across consecutive sen-tences, as well as their position inside the sentence(i.e.
among its subclauses).
SWAN assigns eachsentence to one of four possible fluidity classes:1.
Fluid: the sentence is maintaining connec-tion with the previous sentences.2.
Inverted topic: the sentence is connectedto a previous sentence, but that connectiononly becomes apparent at the very end ofthe sentence (?The cropping should preserveall critical points.
Images of the same sizeshould also be kept by the cropping?).3.
Out-of-sync: the sentence is connected to aprevious one, but there are disconnected sen-tences in between the connected sentences(?The cropping should preserve all criticalpoints.
The face features should be normal-ized.
The cropping should also preserve allcritical points?).4.
Disconnected: the sentence is not connectedto any of the previous sentences or there aretoo many sentences in between.The tool also alerts the writer when transitionwords such as in addition, on the other hand,or even the familiar however are used.
Eventhough these expressions are effective when cor-rectly used, they often betray the lack of a log-ical or semantic connection between consecutivesentences (?The cropping should preserve all crit-ical points.
However, the face features should benormalized?).
SWAN displays all the sentenceswhich could potentially break the fluidity (Fig.
2)and suggests ways of rewriting them.Figure 2: Fluidity evaluation result in SWAN.4 The SWAN Tool4.1 Inputs and outputsSWAN operates on two possible evaluationmodes: simple and full.
In simple evaluationmode, the input to the tool are the title, abstract,introduction and conclusions of a manuscript.These sections can be copy-pasted as plain textto the input fields.In full evaluation mode, which generally pro-vides more feedback, the user provides a full pa-per as an input.
This includes semi-automaticimport of the manuscript from certain standard22document formats such as TeX, MS Office andOpenOffice, as well as semi-automatic structuredetection of the manuscript.
For the well-knownAdobe?s portable document format (PDF) we usestate-of-the-art freely available PdfBox extractor2.Unfortunately, PDF format is originally designedfor layout and printing and not for structured textinterchange.
Most of the time, simple copy &paste from a source document to the simple eval-uation fields is sufficient.When the text sections have been input to thetool, clicking the Evaluate button will trigger theevaluation process.
This has been observed tocomplete, at most, in a minute or two on a mod-ern laptop.
The evaluation metrics in the tool arestraight-forward, most of the processing time isspent in the NLP tools.
After the evaluation iscomplete, the results are shown to the user.SWAN provides constructive feedback fromthe evaluated sections of your paper.
The tool alsohighlights problematic words or sentences in themanuscript text and generates graphs of sentencefeatures (see Fig.
2).
The results can be saved andreloaded to the tool or exported to html formatfor sharing.
The feedback includes tips on howto maintain authoritativeness and how to convincethe scientist reader.
Use of powerful and precisesentences is emphasized together with strategicaland logical placement of key information.In addition to these two main evaluation modes,the tool also includes a manual fluidity assessmentexercise where the writer goes through a giventext passage, sentence by sentence, to see whetherthe next sentence can be predicted from the previ-ous sentences.4.2 Implementation and External LibrariesThe tool is a desktop application written in Java.It uses external libraries for natural language pro-cessing from Stanford, namely Stanford POS Tag-ger (Toutanova et al 2003) and Stanford Parser(Klein and Manning, 2003).
This is one of themost accurate and robust parsers available and im-plemented in Java, as is the rest of our system.Other external libraries include Apache Tika3,which we use in extracting textual content fromfiles.
JFreeChart4 is used in generating graphs2http://pdfbox.apache.org/3http://tika.apache.org/4http://www.jfree.org/jfreechart/and XStream5 in saving and loading inputs andresults.5 Initial User Experiences of SWANSince its release in June 2011, the tool hasbeen used in scientific writing classes in doc-toral schools in France, Finland, and Singapore,as well as in 16 research institutes from A*STAR(Agency for Science Technology and Research).Participants to the classes routinely enter intoSWAN either parts, or the whole paper they wishto immediately evaluate.
SWAN is designed towork on multiple platforms and it relies com-pletely on freely available tools.
The feedbackgiven by the participants after the course revealsthe following benefits of using SWAN:1.
Identification and removal of the inconsis-tencies that make clear identification of thescientific contribution of the paper difficult.2.
Applicability of the tool across vast domainsof research (life sciences, engineering sci-ences, and even economics).3.
Increased clarity of expression through theidentification of the text fluidity problems.4.
Enhanced paper structure leading to a morereadable paper overall.5.
More authoritative, more direct and more ac-tive writing style.Novice writers already appreciate SWAN?sfunctionalityand even senior writers, although ev-idence remains anecdotal.
At this early stage,SWAN?s capabilities are narrow in scope.We con-tinue to enhance the existing evaluation metrics.And we are eager to include a new and alreadytested metric that reveals problems in how figuresare used.AcknowledgmentsThis works of T. Kinnunen and T. Kakkonen were supportedby the Academy of Finland.
The authors would like to thankArttu Viljakainen, Teemu Turunen and Zhengzhe Wu in im-plementing various parts of SWAN.References[Aluisio et al001] S.M.
Aluisio, I. Barcelos, J. Sam-paio, and O.N.
Oliveira Jr. 2001.
How to learnthe many ?unwritten rules?
of the game of the aca-demic discourse: a hybrid approach based on cri-tiques and cases to support scientific writing.
In5http://xstream.codehaus.org/23Proc.
IEEE International Conference on AdvancedLearning Technologies, Madison, Wisconsin, USA.
[Chuck and Young2004] Jo-Anne Chuck and LaurenYoung.
2004.
A cohort-driven assessment task forscientific report writing.
Journal of Science, Edu-cation and Technology, 13(3):367?376, September.
[Dale and Kilgarriff2010] R. Dale and A. Kilgarriff.2010.
Text massaging for computational linguis-tics as a new shared task.
In Proc.
6th Int.
NaturalLanguage Generation Conference, Dublin, Ireland.
[Gopen and Swan1990] George D. Gopen and Ju-dith A. Swan.
1990.
The science of scien-tific writing.
American Scientist, 78(6):550?558,November-December.
[Gopen2004] George D. Gopen.
2004.
Expectations:Teaching Writing From The Reader?s perspective.Longman.
[HOO2011] 2011.
HOO - helping our own.
Web-page, September.
http://www.clt.mq.edu.au/research/projects/hoo/.
[Klein and Manning2003] Dan Klein and Christo-pher D. Manning.
2003.
Accurate unlexicalizedparsing.
In Proc.
41st Meeting of the Associationfor Computational Linguistics, pages 423?430.
[Lebrun2011] Jean-Luc Lebrun.
2011.
Scientific Writ-ing 2.0 ?
A Reader and Writer?s Guide.
World Sci-entific Publishing Co. Pte.
Ltd., Singapore.
[Page1966] E. Page.
1966.
The imminence of gradingessays by computer.
In Phi Delta Kappan, pages238?243.
[Paquot and Bestgen2009] M. Paquot and Y. Bestgen.2009.
Distinctive words in academic writing: Acomparison of three statistical tests for keyword ex-traction.
In A.H. Jucker, D. Schreier, and M. Hundt,editors, Corpora: Pragmatics and Discourse, pages247?269.
Rodopi, Amsterdam, Netherlands.
[Toutanova et al003] Kristina Toutanova, Dan Klein,Christopher Manning, and Yoram Singer.
2003.Feature-rich part-of-speech tagging with a cyclicdependency network.
In Proc.
HLT-NAACL, pages252?259.24
