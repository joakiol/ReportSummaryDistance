Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 750?756,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsLifelong Learning for Sentiment ClassificationZhiyuan Chen, Nianzu Ma, Bing LiuDepartment of Computer ScienceUniversity of Illinois at Chicago{czyuanacm,jingyima005}@gmail.com,liub@cs.uic.eduAbstractThis paper proposes a novel lifelong learn-ing (LL) approach to sentiment classifica-tion.
LL mimics the human continuouslearning process, i.e., retaining the knowl-edge learned from past tasks and use itto help future learning.
In this paper, wefirst discuss LL in general and then LL forsentiment classification in particular.
Theproposed LL approach adopts a Bayesianoptimization framework based on stochas-tic gradient descent.
Our experimental re-sults show that the proposed method out-performs baseline methods significantly,which demonstrates that lifelong learningis a promising research direction.1 IntroductionSentiment classification is the task of classifyingan opinion document as expressing a positive ornegative sentiment.
Liu (2012) and Pang and Lee(2008) provided good surveys of the existing re-search.
In this paper, we tackle sentiment clas-sification from a novel angle, lifelong learning(LL), or lifelong machine learning.
This learn-ing paradigm aims to learn as humans do: re-taining the learned knowledge from the past anduse the knowledge to help future learning (Thrun,1998, Chen and Liu, 2014b, Silver et al., 2013).Although many machine learning topics andtechniques are related to LL, e.g., lifelong learn-ing (Thrun, 1998, Chen and Liu, 2014b, Silver etal., 2013), transfer learning (Jiang, 2008, Pan andYang, 2010), multi-task learning (Caruana, 1997),never-ending learning (Carlson et al., 2010), self-taught learning (Raina et al., 2007), and onlinelearning (Bottou, 1998), there is still no unifieddefinition for LL.Based on the prior work and our research, tobuild an LL system, we believe that we need toanswer the following key questions:1.
What information should be retained from thepast learning tasks?2.
What forms of knowledge will be used to helpfuture learning?3.
How does the system obtain the knowledge?4.
How does the system use the knowledge to helpfuture learning?Motivated by these questions, we present thefollowing definition of lifelong learning (LL).Definition (Lifelong Learning): A learner hasperformed learning on a sequence of tasks, from1 to N ?
1.
When faced with the N th task, it usesthe knowledge gained in the past N ?
1 tasks tohelp learning for the N th task.
An LL system thusneeds the following four general components:1.
Past Information Store (PIS): It stores the in-formation resulted from the past learning.
Thismay involve sub-stores for information such as(1) the original data used in each past task, (2)intermediate results from the learning of eachpast task, and (3) the final model or patternslearned from the past task, respectively.2.
Knowledge Base (KB): It stores the knowledgemined or consolidated from PIS (Past Informa-tion Store).
This requires a knowledge repre-sentation scheme suitable for the application.3.
Knowledge Miner (KM).
It mines knowledgefrom PIS (Past Information Store).
This min-ing can be regarded as a meta-learning processbecause it learns knowledge from informationresulted from learning of the past tasks.
Theknowledge is stored to KB (Knowledge Base).4.
Knowledge-Based Learner (KBL): Given theknowledge in KB, this learner is able to lever-age the knowledge and/or some information inPIS for the new task.Based on this, we can define lifelong sentimentclassification (LSC):Definition (Lifelong Sentiment Classification):A learner has performed a sequence of supervised750sentiment classification tasks, from 1 to N ?
1,where each task consists of a set of training doc-uments with positive and negative polarity labels.Given the N th task, it uses the knowledge gainedin the past N ?
1 tasks to learn a better classifierfor the N th task.It is useful to note that although many re-searchers have used transfer learning for super-vised sentiment classification, LL is different fromthe classic transfer learning or domain adapta-tion (Pan and Yang, 2010).
Transfer learning typi-cally uses labeled training data from one (or more)source domain(s) to help learning in the target do-main that has little or no labeled data (Aue andGamon, 2005, Bollegala et al., 2011).
It does notuse the results of the past learning or knowledgemined from the results of the past learning.
Fur-ther, transfer learning is usually inferior to tradi-tional supervised learning when the target domainalready has good training data.
In contrast, ourtarget (or future) domain/task has good trainingdata and we aim to further improve the learningusing both the target domain training data and theknowledge gained in past learning.
To be consis-tent with prior research, we treat the classificationof one domain as one learning task.One question is why the past learning tasks cancontribute to the target domain classification giventhat the target domain already has labeled trainingdata.
The key reason is that the training data maynot be fully representative of the test data due tothe sample selection bias (Heckman, 1979, Shi-modaira, 2000, Zadrozny, 2004).
In few real-lifeapplications, the training data are fully represen-tative of the test data.
For example, in a senti-ment classification application, the test data maycontain some sentiment words that are absent inthe training data of the target domain, while thesesentiment words have appeared in some past do-mains.
So the past domain knowledge can providethe prior polarity information in this situation.Like most existing sentiment classification pa-pers (Liu, 2012), this paper focuses on binary clas-sification, i.e., positive (+) and negative (?)
polar-ities.
But the proposed method is also applicableto multi-class classification.
To embed and use theknowledge in building the target domain classifier,we propose a novel optimization method based onthe Na?
?ve Bayesian (NB) framework and stochas-tic gradient descent.
The knowledge is incorpo-rated using penalty terms in the optimization for-mulation.
This paper makes three contributions:1.
It proposes a novel lifelong learning approachto sentiment classification, called lifelong sen-timent classification (LSC).2.
It proposes an optimization method that usespenalty terms to embed the knowledge gainedin the past and to deal with domain dependentsentiment words to build a better classifier.3.
It creates a large corpus containing reviewsfrom 20 diverse product domains for extensiveevaluation.
The experimental results demon-strate the superiority of the proposed method.2 Related WorkOur work is mainly related to lifelong learningand multi-task learning (Thrun, 1998, Caruana,1997, Chen and Liu, 2014b, Silver et al., 2013).Existing lifelong learning approaches focused onexploiting invariances (Thrun, 1998) and othertypes of knowledge (Chen and Liu, 2014b, Chenand Liu, 2014a, Ruvolo and Eaton, 2013) acrossmultiple tasks.
Multi-task learning optimizes thelearning of multiple related tasks at the sametime (Caruana, 1997, Chen et al., 2011, Saha et al.,2011, Zhang et al., 2008).
However, these meth-ods are not for sentiment analysis.
Also, our na?
?veBayesian optimization based LL method is quitedifferent from all these existing techniques.Our work is also related to transfer learning ordomain adaptation (Pan and Yang, 2010).
In thesentiment classification context, Aue and Gamon(2005) trained sentiment classifiers for the targetdomain using various mixes of labeled and un-labeled reviews.
Blitzer et al.
(2007) proposed tofirst find some common or pivot features from thesource and the target, and then identify correlatedfeatures with the pivot features.
The final classifieris built using the combined features.
Li and Zong(2008) built a meta-classifier (called CLF) usingthe outputs of each base classifier constructed ineach domain.
Other works along similar linesinclude (Andreevskaia and Bergler, 2008, Bol-legala et al., 2011, He et al., 2011, Ku et al.,2009, Li et al., 2012, Li et al., 2013, Pan and Yang,2010, Tan et al., 2007, Wu et al., 2009, Xia andZong, 2011, Yoshida et al., 2011).
Additional de-tails about these and other related works can befound in (Liu, 2012).
However, as we discussedin the introduction, these methods do not focus onthe ability to accumulate learned knowledge andleverage it in new learning in a lifelong manner.7513 Proposed LSC Technique3.1 Na?
?ve Bayesian Text ClassificationBefore presenting the proposed method, we brieflyreview the Na?
?ve Bayesian (NB) text classificationas our method uses it as the foundation.NB text classification (McCallum and Nigam,1998) basically computes the conditional proba-bility of each word w given each class cj(i.e.,P (w|cj)) and the prior probability of each classcj(i.e., P (cj)), which are used to calculate theposterior probability of each class cjgiven a testdocument d (i.e., P (cj|d)).
cjis either positive(+) or negative (?)
in our case.The key parameter P (w|cj) is computed as:P (w|cj) =?+Ncj,w?
|V |+?|V |v=1Ncj,v(1)where Ncj,wis the frequency of word w in docu-ments of class cj.
|V | is the size of vocabulary Vand ?
(0 ?
?
?
1) is used for smoothing.3.2 Components in LSCThis subsection describes our proposed methodcorresponding to the proposed LL components.1.
Past Information Store (PIS): In this work, wedo not store the original data used in the pastlearning tasks, but only their results.
For eachpast learning task?t, we store a) P?t(w|+) andP?t(w|?)
for each word w which are from task?t?s NB classifier (see Eq 1); and b) the numberof times that w appears in a positive (+) doc-ument N?t+,wand the number of times that wappears in a negative documents N?t?,w.2.
Knowledge Base (KB): Our knowledge basecontains two types of knowledge:(a) Document-level knowledge NKB+,w(andNKB?,w): number of occurrences of w inthe documents of the positive (and nega-tive) class in the past tasks, i.e., NKB+,w=?
?tN?t+,wand NKB?,w=??tN?t?,w.
(b) Domain-level knowledge MKB+,w(andMKB?,w): number of past tasks inwhich P (w|+) > P (w|?)
(andP (w|+) < P (w|?)).3.
Knowledge Miner (KM).
Knowledge miner isstraightforward as it just performs counting andaggregation of information in PIS to generateknowledge (see 2(a) and 2(b) above).4.
Knowledge-Based Learner (KBL): This learnerincorporates knowledge using regularization aspenalty terms in our optimization.
See the de-tails in 3.4.3.3 Objective FunctionIn this subsection, we introduce the objective func-tion used in our method.
The key parameters thataffect NB classification results are P (w|cj) whichare computed using empirical counts of word wwith class cj, i.e., Ncj,w(Eq.
1).
In binary classifi-cation, they are N+,wand N?,w.
This suggeststhat we can revise these counts appropriately toimprove classification.
In our optimization, wedenote the optimized variables X+,wand X?,was the number of times that a word w appears inthe positive and negative class.
We called themvirtual counts to distinguish them from empiricalcounts N+,wand N?,w.
For correct classification,ideally, we should have the posterior probabilityP (cj|di) = 1 for labeled class cj, and for the otherclass cf, we should have P (cf|di) = 0.
Formally,given a new domain training dataDt, our objectivefunction is:|Dt|?i=1(P (cj|di)?
P (cf|di)) (2)Here cjis the actual labeled class of di?
Dt.In this paper, we use stochastic gradient descent(SGD) to optimize on the classification of eachdocument di?
Dt.
Due to the space limit, weonly show the optimization process for a positivedocument (the process for a negative document issimilar).
The objective function under SGD for apositive document is:F+,i= P (+|di)?
P (?|di) (3)To further save space, we omit the derivationsteps and give the final derivatives below (See thedetailed derivation steps in the separate supple-mentary note):g (X) =(?
|V |+?|V |v=1X+,v?
|V |+?|V |v=1X?,v)|di|(4)?F+,i?X+,u=nu,di?+X+,u+P (?
)P (+)?w?di(?+X?,w?+X+,w)nw,di?
?g?X+,u1 +P (?
)P (+)?w?di(?+X?,w?+X+,w)nw,di?
g(X)?nu,di?+X+,u(5)?F+,i?X?,u=nu,di?+X?,u?
g(X) + ?g?X?,uP (+)P (?
)?w?di(?+X+,w?+X?,w)nw,di+ g(X)(6)752Alarm Clock 30.51 Flashlight 11.69 Home Theater System 28.84 Projector 20.24Baby 16.45 GPS 19.50 Jewelry 12.21 Rice Cooker 18.64Bag 11.97 Gloves 13.76 Keyboard 22.66 Sandal 12.11Cable Modem 12.53 Graphics Card 14.58 Magazine Subscriptions 26.88 Vacuum 22.07Dumbbell 16.04 Headphone 20.99 Movies TV 10.86 Video Games 20.93Table 1: Names of the 20 product domains and the proportion of negative reviews in each domain.where nu,diis the term frequency of word u indocument di.
X denotes all the variables consist-ing of X+,wand X?,wfor each word w. The par-tial derivatives for a word u, i.e.,?g?X+,uand?g?X?,u,are quite straightforward and thus not shown here.X0+,w= Nt+,w+NKB+,wandX0?,w= Nt?,w+NKB?,ware served as a reasonable starting point for SGD,where Nt+,wand Nt?,ware the empirical counts ofword w and classes + and?
from domainDt, andNKB+,wand NKB?,ware from knowledge KB (Sec-tion 3.2).
The SGD runs iteratively using the fol-lowing rules for the positive document diuntilconvergence, i.e., when the difference of Eq.
2 fortwo consecutive iterations is less than 1e?3 (samefor the negative document), where ?
is the learningrate:Xl+,u= Xl?1+,u??
?F+,i?X+,u, Xl?,u= Xl?1?,u??
?F+,i?X?,u3.4 Exploiting Knowledge via Penalty TermsThe above optimization is able to update the vir-tual counts for a better classification in the targetdomain.
However, it does not deal with the issueof domain dependent sentiment words, i.e., somewords may change the polarity across different do-mains.
Nor does it utilize the domain-level knowl-edge in the knowledge baseKB (Section 3.2).
Wethus propose to add penalty terms into the opti-mization to accomplish these.The intuition here is that if a word w can dis-tinguish classes very well from the target domaintraining data, we should rely more on the targetdomain training data in computing counts relatedto w. So we define a set of words VTthat consistsof distinguishable target domain dependent words.A word w belongs to VTif P (w|+) is much largeror much smaller than P (w|?)
in the target do-main, i.e.,P (w|+)P (w|?)?
?
orP (w|?
)P (w|+)?
?, where ?is a parameter.
Such words are already effectivein classification for the target domain, so the vir-tual counts in optimization should follow the em-pirical counts (Nt+,wand Nt?,w) in the target do-main, which are reflected in the L2 regularizationpenalty term below (?
is the regularization coeffi-cient):12?
?w?VT((X+,w?Nt+,w)2+(X?,w?Nt?,w)2)(7)To leverage domain-level knowledge (the sec-ond type of knowledge in KB in Section 3.2), wewant to utilize only those reliable parts of knowl-edge.
The rationale here is that if a word onlyappears in one or two past domains, the knowl-edge associated with it is probably not reliable orit is highly specific to those domains.
Based onit, we use domain frequency to define the relia-bility of the domain-level knowledge.
For w, ifMKB+,w?
?
or MKB?,w?
?
(?
is a parameter), weregard it as appearing in a reasonable number ofdomains, making its knowledge reliable.
We de-note the set of such words as VS. Then we add thesecond penalty term as follows:12??w?VS(X+,w?Rw?X0+,w)2+12??w?VS(X?,w?
(1?Rw)?X0?,w)2(8)where the ratio Rwis defined as MKB+,w/(MKB+,w+MKB?,w).
X0+,wandX0?,ware the starting points forSGD (Section 3.3).
Finally, we revise the partialderivatives in Eqs.
4-6 by adding the correspond-ing partial derivatives of Eqs.
7 and 8 to them.4 ExperimentsDatasets.
We created a large corpus contain-ing reviews from 20 types of diverse productsor domains crawled from Amazon.com (i.e., 20datasets).
The names of product domains arelisted in Table 1.
Each domain contains 1,000 re-views.
Following the existing work of other re-searchers (Blitzer et al., 2007, Pang et al., 2002),we treat reviews with rating > 3 as positive andreviews with rating < 3 as negative.
The datasetsare publically available at the authors websites.Natural class distribution: We keep the natural(or skewed) distribution of the positive and nega-tive reviews to experiment with the real-life situa-tion.
F1-score is used due to the imbalance.753NB-T NB-S NB-ST SVM-T SVM-S SVM-ST CLF LSC56.21 57.04 60.61 57.82 57.64 61.05 12.87 67.00Table 2: Natural class distribution: Average F1-score of the negative class over 20 domains.
Negativeclass is the minority class and thus harder to classify.NB-T NB-S NB-ST SVM-T SVM-S SVM-ST CLF LSC80.15 77.35 80.85 78.45 78.20 79.40 80.49 83.34Table 3: Balanced class distribution: Average accuracy over 20 domains for each system.Balanced class distribution: We also created abalance dataset with 200 reviews (100 positive and100 negative) in each domain dataset.
This set issmaller because of the small number of negativereviews in each domain.
Accuracy is used for eval-uation in this balanced setting.We used unigram features with no feature se-lection in classification.
We followed (Pang et al.,2002) to deal with negation words.
For evalua-tion, each domain is treated as the target domainwith the rest 19 domains as the past domains.
Allthe models are evaluated using 5-fold cross vali-dation.Baselines.
We compare our proposed LSCmodel with Na?
?ve Bayes (NB), SVM1, andCLF (Li and Zong, 2008).
Note that NB and SVMcan only work on a single domain data.
To havea comprehensive comparison, they are fed withthree types of training data:a) labeled training data from the target domainonly, denoted by NB-T and SVM-T;b) labeled training data from all past source do-mains only, denoted by NB-S and SVM-S;c) merged (labeled) training data from all past do-mains and the target domain, referred to as NB-ST and SVM-ST.For LSC, we empirically set ?
= 6 and ?
= 6.The learning rate ?
and regularization coefficient?
are set to 0.1 empirically.
?
is set to 1 for(Laplace) smoothing.Table 2 shows the average F1-scores for thenegative class in the natural class distribution, andTable 3 shows the average accuracies in the bal-anced class distribution.
We can clearly see thatour proposed model LSC achieves the best perfor-mance in both cases.
In general, NB-S (and SVM-S) are worse than NB-T (and SVM-T), both ofwhich are worse than NB-ST (and SVM-ST).
Thisshows that simply merging both past domains andthe target domain data is slightly beneficial.
Note1http://www.csie.ntu.edu.tw/?cjlin/libsvm/NB-T    5   10   15   190.790.810.830.85NB-T    5   10   15   190.550.60.650.7Figure 1: (Left): Negative class F1-score of LSCwith #past domains in natural class distribution.
(Right): Accuracy of LSC with #past domains inbalanced class distribution.that the average F1-score for the positive class isnot shown as all classifiers perform very well be-cause the positive class is the majority class (whileour model performs slightly better than the base-lines).
The improvements of the proposed LSCmodel over all baselines in both cases are statisti-cally significant using paired t-test (p < 0.01 com-pared to NB-ST and CLF, p < 0.0001 comparedto the others).
In the balanced class setting (Ta-ble 3), CLF performs better than NB-T and SVM-T, which is consistent with the results in (Li andZong, 2008).
However, it is still worse than ourLSC model.Effects of #Past Domains.
Figure 1 shows theeffects of our model using different number of pastdomains.
We clearly see that LSC performs bet-ter with more past domains, showing it indeed hasthe ability to accumulate knowledge and use theknowledge to build better classifiers.5 ConclusionsIn this paper, we proposed a lifelong learning ap-proach to sentiment classification using optimiza-tion, which is based on stochastic gradient de-scent in the framework of Bayesian probabilities.Penalty terms are introduced to effectively exploitthe knowledge gained from past learning.
Ourexperimental results using 20 diverse product re-view domains demonstrate the effectiveness of themethod.
We believe that lifelong learning is apromising direction for building better classifiers.754ReferencesAlina Andreevskaia and Sabine Bergler.
2008.
WhenSpecialists and Generalists Work Together: Over-coming Domain Dependence in Sentiment Tagging.In ACL, pages 290?298.Anthony Aue and Michael Gamon.
2005.
Customiz-ing Sentiment Classifiers to New Domains: A CaseStudy.
In RANLP.John Blitzer, Mark Dredze, and Fernando Pereira.2007.
Biographies, Bollywood, Boom-boxes andBlenders: Domain Adaptation for Sentiment Clas-sification.
In ACL, pages 440?447.Danushka Bollegala, David J Weir, and John Carroll.2011.
Using Multiple Sources to Construct a Sen-timent Sensitive Thesaurus for Cross-Domain Senti-ment Classification.
In ACL HLT, pages 132?141.L?eon Bottou.
1998.
Online algorithms and stochas-tic approximations.
In David Saad, editor, OnlineLearning and Neural Networks.
Cambridge Univer-sity Press, Cambridge, UK.
Oct 2012.Andrew Carlson, Justin Betteridge, and Bryan Kisiel.2010.
Toward an Architecture for Never-EndingLanguage Learning.
In AAAI, pages 1306?1313.Rich Caruana.
1997.
Multitask Learning.
Machinelearning, 28(1):41?75.Zhiyuan Chen and Bing Liu.
2014a.
Mining Topics inDocuments : Standing on the Shoulders of Big Data.In KDD, pages 1116?1125.Zhiyuan Chen and Bing Liu.
2014b.
Topic Modelingusing Topics from Many Domains, Lifelong Learn-ing and Big Data.
In ICML, pages 703?711.Jianhui Chen, Jiayu Zhou, and Jieping Ye.
2011.
Inte-grating low-rank and group-sparse structures for ro-bust multi-task learning.
In KDD, pages 42?50.Yulan He, Chenghua Lin, and Harith Alani.
2011.
Au-tomatically Extracting Polarity-Bearing Topics forCross-Domain Sentiment Classification.
In ACL,pages 123?131.James J Heckman.
1979.
Sample selection bias as aspecification error.
Econometrica: Journal of theeconometric society, pages 153?161.Jing Jiang.
2008.
A literature survey on domain adap-tation of statistical classifiers.
Technical report.Lun-Wei Ku, Ting-Hao Huang, and Hsin-Hsi Chen.2009.
Using morphological and syntactic structuresfor Chinese opinion analysis.
In EMNLP, pages1260?1269.Shoushan Li and Chengqing Zong.
2008.
Multi-domain sentiment classification.
In ACL HLT, pages257?260.Fangtao Li, Sinno Jialin Pan, Ou Jin, Qiang Yang, andXiaoyan Zhu.
2012.
Cross-domain Co-extraction ofSentiment and Topic Lexicons.
In ACL, pages 410?419.Shoushan Li, Yunxia Xue, Zhongqing Wang, andGuodong Zhou.
2013.
Active learning for cross-domain sentiment classification.
In AAAI, pages2127?2133.Bing Liu.
2012.
Sentiment Analysis and Opinion Min-ing.
Synthesis Lectures on Human Language Tech-nologies, 5(1):1?167.Sinno Jialin Pan and Qiang Yang.
2010.
A Survey onTransfer Learning.
IEEE Trans.
Knowl.
Data Eng.,22(10):1345?1359.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and Trends in In-formation Retrieval, 2(1-2):1?135.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
Sentiment Classification usingMachine Learning Techniques.
In EMNLP, pages79?86.Rajat Raina, Alexis Battle, Honglak Lee, BenjaminPacker, and Andrew Y Ng.
2007.
Self-taught Learn-ing : Transfer Learning from Unlabeled Data.
InICML, pages 759?766.Paul Ruvolo and Eric Eaton.
2013.
ELLA: An efficientlifelong learning algorithm.
In ICML, pages 507?515.Avishek Saha, Piyush Rai, Suresh Venkatasubrama-nian, and Hal Daume.
2011.
Online learning ofmultiple tasks and their relationships.
In AISTATS,pages 643?651.Hidetoshi Shimodaira.
2000.
Improving predictive in-ference under covariate shift by weighting the log-likelihood function.
Journal of statistical planningand inference, 90(2):227?244.Daniel L Silver, Qiang Yang, and Lianghao Li.2013.
Lifelong Machine Learning Systems: BeyondLearning Algorithms.
In AAAI Spring Symposium:Lifelong Machine Learning, pages 49?55.Songbo Tan, Gaowei Wu, Huifeng Tang, and XueqiCheng.
2007.
A novel scheme for domain-transferproblem in the context of sentiment analysis.
InCIKM, pages 979?982.Sebastian Thrun.
1998.
Lifelong Learning Algo-rithms.
In S Thrun and L Pratt, editors, LearningTo Learn, pages 181?209.
Kluwer Academic Pub-lishers.Qiong Wu, Songbo Tan, and Xueqi Cheng.
2009.Graph Ranking for Sentiment Transfer.
In ACL-IJCNLP, pages 317?320.755Rui Xia and Chengqing Zong.
2011.
A POS-basedEnsemble Model for Cross-domain Sentiment Clas-sification.
In IJCNLP, pages 614?622.
Citeseer.Yasuhisa Yoshida, Tsutomu Hirao, Tomoharu Iwata,Masaaki Nagata, and Yuji Matsumoto.
2011.Transfer Learning for Multiple-Domain Sen-timent Analysis-Identifying Domain Depen-dent/Independent Word Polarity.
In AAAI, pages1286?1291.Bianca Zadrozny.
2004.
Learning and evaluating clas-sifiers under sample selection bias.
In ICML, page114.
ACM.Jian Zhang, Zoubin Ghahramani, and Yiming Yang.2008.
Flexible latent variable models for multi-tasklearning.
Machine Learning, 73(3):221?242.756
