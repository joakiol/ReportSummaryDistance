Proceedings of the SIGDIAL 2013 Conference, pages 242?250,Metz, France, 22-24 August 2013. c?2013 Association for Computational LinguisticsPredicting Tasks in Goal-Oriented Spoken Dialog Systemsusing Semantic Knowledge BasesAasish Pappu and Alexander I. RudnickyLanguage Technologies InstituteCarnegie Mellon University{aasish, air}@cs.cmu.eduAbstractGoal-oriented dialog agents are expectedto recognize user-intentions from an utter-ance and execute appropriate tasks.
Typi-cally, such systems use a semantic parserto solve this problem.
However, semanticparsers could fail if user utterances containout-of-grammar words/phrases or if the se-mantics of uttered phrases did not matchthe parser?s expectations.
In this work,we have explored a more robust methodof task prediction.
We define task predic-tion as a classification problem, rather than?parsing?
and use semantic contexts to im-prove classification accuracy.
Our classi-fier uses semantic smoothing kernels thatcan encode information from knowledgebases such as Wordnet, NELL and Free-base.com.
Our experiments on two spokenlanguage corpora show that augmentingsemantic information from these knowl-edge bases gives about 30% absolute im-provement in task prediction over a parser-based method.
Our approach thus helpsmake a dialog agent more robust to userinput and helps reduce number of turns re-quired to detected intended tasks.1 IntroductionSpoken dialog agents are designed with particulartasks in mind.
These agents could provide infor-mation or make reservations, or other such tasks.Many dialog agents often can perform multipletasks: think of a customer service kiosk systemat a bank.
The system has to decide which task ithas to perform by talking to its user.
This problemof identifying what to do based on what a user hassaid is called task prediction.Task prediction is typically framed as a parsingproblem: A grammar is written to semanticallyparse the input utterance from users, and these se-mantic labels in combination are used to decidewhat the intended task is.
However, this methodis less robust to errors in user-input.
A dialog sys-tem consists of a pipeline of cascaded modules,such as speech recognition, parsing, dialog man-agement.
Any errors made by these modules pro-pogate and accumulate through the pipeline.
Bo-hus and Rudnicky (2005) have shown that thiscascade of errors, coupled with users employ-ing out-of-grammar phrases results in many ?non-understanding?
and ?misunderstanding?
errors.There have been other approaches to performdialog task prediction.
Gorin et al(1997) has pro-posed a salience-phrase detection technique thatmaps phrases to their corresponding tasks.
Chu-Carroll and Carpenter (1999) casted the task de-tection as an information retrieval ?
detect tasksby measuring the distance between the query vec-tor and representative text for each task.
Bui(2003) and Blaylock and Allen (2006) have cast itas a hierarchical sequence labeling problem usingHidden Markov Models (HMM).
More recently,(Bangalore and Stent, 2009) built an incremen-tal parser that gradually determines the task basedon the incoming dialog utterances.
(Chen andMooney, 2010) have developed a route instruc-tions frame parser to determine the task in the con-text of a mobile dialog robot.
These approachesmainly use local features such as dialog context,speech features and grammar-based-semantic fea-tures to determine the task.
However grammar-based-semantic features would be insufficient ifan utterance uses semantically similar phrases thatare not in the system?s domain or semantics.
Ifthe system could explore semantic information be-yond the scope of its local knowledge and use ex-ternal knowledge sources then they will help im-prove the task prediction.
(Cristianini et al 2002) (Wang and Domeni-coni, 2008) (Moschitti, 2009) found that open-242domain semantic knowledge resources are use-ful for text classification problems.
Their successin limited data scenario is an attractive prospect,since most dialog agents operate in scarce train-ing data scenarios.
(Bloehdorn et al 2006) hasproposed a semantic smoothing kernel based ap-proach for text classification.
The intuition be-hind their approach is that terms (particularly con-tent words) of two similar sentences or documentsshare superconcepts (e.g., hypernyms) in a knowl-edge base.
Semantic Similarity between two termscan be computed using different metrics (Pedersenet al 2004) based on resources like WordNet.Open domain resources such as world-wide-web, had been used in the context of speech recog-nition.
(Misu and Kawahara, 2006) and (Creutzet al 2009) used web-texts to improve the lan-guage models for speech recognition in a targetdomain.
They have used a dialog corpus in or-der to query relevant web-texts to build the targetdomain models.
Although (Araki, 2012) did notconduct empirical experiments, yet they have pre-sented an interesting architecture that exploits anopen-domain resource like Freebase.com to buildspoken dialog systems.In this work, we have framed the task predictionproblem as a classification problem.
We use theuser?s utterances to extract lexical semantic fea-tures and classify it into being one of the manytasks the system was designed to perform.
Weharness the power of semantic knowledge basesby bootstraping an utterance with semantic con-cepts related to the tokens in the utterance.
The se-mantic distance/similarity between concepts in theknowledge base is incorporated into the model us-ing a kernel.
We show that our approach improvesthe task prediction accuracy over a grammar-basedapproach on two spoken corpora (1) Navagati(Pappu and Rudnicky, 2012): a corpus of spo-ken route instructions, and (2) Roomline (Bohus,2003): a corpus of spoken dialog sessions in room-reservation domain.This paper is organized as following: Section2 describes the problem of dialog task predic-tion and the standard grammar based approach topredict the dialog task.
Then in Section 3, wedescribe the open-domain knowledge resourcesthat were used in our approach and their advan-tages/disadvantages.
We will discuss our semantickernel based approach in the Section 4.
We reportour experiment results on task prediction in Sec-tion 5.
In Section 6, we will analyze the errors thatoccur in our approach, followed by concluding re-marks and possible directions to this work.2 Parser based Dialog Task PredictionIn a dialog system, there are two functions of asemantic grammar ?
encode linguistic constructsused during the interactions and represent the do-main knowledge in-terms of concepts and their in-stances.
Table 1 illustrates the tasks and the con-cepts used in a navigation domain grammar.
Thelinguistic constructions help the parser to segmentan utterance into meaningful chunks.
The domainknowledge helps in labeling the tokens/phraseswith concepts.
The parser uses the labeled tokensand the chunked form of the utterance, to classifythe utterance into one of the tasks.Table 1: Tasks and Concepts in GrammarTasks ExamplesImperative GoToPlace, Turn, etcAdvisory Instructions You_Will_See_LocationGrounding Instructions You_are_at_LocationConcepts ExamplesLocations buildings, other landmarksAdjectives-of-Locations large, open, black, small etc.Pathways hallway, corridor, bridge, etc.LiftingDevice elevator, staircase, etc.Spatial Relations behind, above, on left, etc.Numbers turn-angles, distance, etc.Ordinals first, second, etc.
floor numbersThe dialog agent uses the root node of a parseroutput as the task.
Figure 1 illustrates a semanticparser output for a fictitious utterance in the nav-igation domain.
The dialog manager would con-sider the utterance as an ?Imperative?
for this ex-ample.Imperativego directionforwarddistancenumberfiveunitsmetersFigure 1: Illustration of Semantic Parse Tree usedin a Dialog System2432.1 Grammar: A Knowledge ResourceGrammar is a very useful resource for a dialog sys-tem because it could potentially represent an ex-pert?s view of the domain.
Since knowledge en-gineering requires time and effort, very few di-alog systems can afford to have grammars thatare expert-crafted and robust to various artefactsof spoken language.
This becomes a major chal-lenge for real world dialog systems.
If the sys-tem?s grammar or the domain knowledge does notconform to its users and their utterances, the parserwill fail to produce a correct parse, if the parseis incorrect and/or the concept labeling is incor-rect.
Lack of comprehensive semantic knowledgeis the cause of this problem.
An open-domainknowledge base like Wordnet (Miller, 1995), Free-base (Bollacker et al 2008) or NELL (Carlsonet al 2010) contains comprehensive informationabout concepts and their relationships present inthe world.
If used appropriately, open-domainknowledge resources can help compensate for in-complete semantic knowledge of the system.3 Open-Domain Semantic KnowledgeBasesLike grammars, open-domain knowledge re-sources contain concepts, instances and relations.The purpose of these resources is to organizecommon sense and factoid information known tothe mankind in a machine-understandable form.These resources, if filtered appropriately, containvaluable domain-specific information for a dialogagent.
To this end, we propose to use three knowl-edge resources along with the domain grammar forthe task prediction.
A brief overview of each of theknowledge resources is given below:3.1 Wordnet: Expert Knowledge BaseWordnet (Miller, 1995) is an online lexicaldatabase of words and their semantics curatedby language experts.
It organizes the words andtheir morphological variants in a hierarchical fash-ion.
Every word has at least one synset i.e.,sense and a synset has definite meaning and agloss to illustrate the usage.
Synsets are con-nected through relationships such as hypernyms,hyponyms, meronyms, antonyms etc.
Each synsetcan be considered as an instance and their par-ent synsets as concepts.
Although Wordnet con-tains several ( 120,000) word forms, some of ourdomain-specific word forms (e.g., locations in anavigation domain) will not be present.
Therefore,we would like to use other open-domain knowl-edge bases to augment the agent?s knowledge.3.2 Freebase: Community Knowledge BaseFreebase.com (Bollacker et al 2008) is a col-laboratively evolving knowledge base with theeffort of volunteers.
It organizes the factsbased on types/concepts along with several predi-cates/properties and their values for each fact.
Thetypes are arranged in a hierarchy and the hierar-chy is rooted at ?domain?.
Freebase facts are con-stantly updated by the volunteers.
Therefore, it is agood resource to help bootstrap the domain knowl-edge of a dialog agent.3.3 NELL: Automated Knowledge BaseNever-Ending Language Learner(NELL) (Carlsonet al 2010) is a program that learns and organizesthe facts from the web in an unsupervised fashion.NELL is on the other end of the knowledge basespectrum which is not curated either by experts orby volunteers.
NELL uses a two-step approach tolearn new facts: (1) extract information from thetext using pattern-based, semi-structured relationextractors (2) improve the learning for next itera-tion based on the evidence from previous iteration.Every belief/fact in its knowledge base has con-cepts, source urls, extraction patterns, predicate,the surface forms of the facts and a confidencescore for the belief.
Although the facts could benoisy in comparison to ones in other knowledgebases, NELL continually adds and improves thefacts without much human effort.4 Semantic Kernel based Dialog TaskPredictionWe would like to use this apriori knowledge aboutthe world and the domain to help us predict thedialog task.
The task prediction problem can betreated as a classification problem.
Classificationalgorithms typically use bag-of-words representa-tion that converts a document or sentence into avector with terms as components of the vector.This representation produces very good results inscenarios with sufficient training data.
Howeverin a limited training data or extreme sparsenessscenario such as ours, (Siolas and d?Alch?
Buc,2000) has shown that Semantic Smoothing Ker-nel technique is a promising approach.
The majoradvantage of this approach is that they can incor-244porate apriori knowledge from existing knowledgebases.
The semantic dependencies between terms,dependencies between concepts and instances, canbe encoded in these kernels.
The semantic kernelscan be easily plugged into a kernel based classi-fier help us predict the task from the goal-orienteddialog utterances.In our experiments, we used an implementationof Semantic Kernel from (Bloehdorn et al 2006)and plugged it into a Support Vector Machine(SVM) classifier (SVMlight) (Joachims, 1999).
Asa part of experimental setup, we will describe thedetails of how did we extract the semantic depen-dencies from each knowledge base and encodedthem into the kernel.5 ExperimentsOur goal is to improve the task prediction for agiven spoken dialog utterance by providing addi-tional semantic context to the utterance with thehelp of relevant semantic concepts from the se-mantic knowledge bases.
The baseline approachwould use the Phoenix parser?s output to deter-mine the intended task for an utterance.
From ourexperiments, we show that our knowledge-drivenapproach will improve upon the baseline perfor-mance on two corpora (1) Navagati Corpus: a nav-igation directions corpus (2) Roomline Corpus: aroom reservation dialog corpus.5.1 SetupWe have divided each corpus into training and test-ing datasets.
We train our task classification mod-els on the manual transcriptions of the trainingdata and evaluated the models on the ASR outputof the testing data.
Both Navagati and Roomlinecorpora came with manually annotated task labelsand manual transcriptions for the utterances.
Wefiltered out the non-task utterances such as ?yes?,?no?
and other clarifications from the Roomlinecorpus.
We obtained the ASR output for the Nava-gati corpus by running the test utterances throughPocketSphinx (Huggins-Daines et al 2006).
TheRoomline corpus already had the ASR output forthe utterances.
Table 2 illustrates some of thestatistics for each corpus.Our baseline model for the task detection is thePhoenix (Ward, 1991) parser output, which is thedefault method used in the Ravenclaw/Olympusdialog systems (Bohus et al 2007).
For the Nava-gati Corpus we have obtained the parser output us-ing the grammar and method described in (Pappuand Rudnicky, 2012).
For the Roomline corpus,we extracted the parser output from the sessionlogs from the the corpus distribution.Corpus-Stats Navagati RoomLineTasks 4 7Words 503 498Word-Error-rate 46.3% 25.6%Task Utts 934 18911Task Training-Utts 654 1324Task Testing-Utts 280 567TasksN1.
Meta R1.
NeedRoomN2.
Advisory R2.
ChooseRoomN3.
Imperative R3.
QueryFeaturesN4.
Grounding R4.
ListRoomsR5.
IdentificationR6.
CancelReservationR7.
RejectRoomsTable 2: Corpus Statistics5.1.1 Semantic Facts to Semantic KernelThe semantic kernel takes a term proximity ma-trix as an input, then produces a positive semidef-inite matrix which can be used inside the kernelfunction.
In our case, we build a term proxim-ity matrix for the words in the recognition vocabu-lary.
(Bloehdorn et al 2006) found that using theterm-concept pairs in the proximity matrix is moremeaningful following the intuition that terms thatshare more number of concepts are similar as op-posed to terms that share fewer concepts.
We haveused following measures to compute the proximityvalue P and some of them are specific to respec-tive knowledge bases:?
gra: No weighting for term-concept pairs inthe Grammar, i.e.,P = 1, for all concepts ci of t, P = 0 other-wise.?
fb: Weighting using normalized Free-base.com relevance score, i.e.,P = fbscore(t, ci)?
fbscore(t, cmin)fbscore(t, cmax)?
fbscore(t, cmin)(1)?
nell: Weighting for the NELL term-conceptpairs using the probability for a belief i.e.,P = nellprob(t, ci) (2), for all concepts ci of t, P = 0 otherwise.1Originally has 10356 utts; filtered out non-task utts.245?
wnpath: Weighting for the term-conceptpairs in the Wordnet based on the shortestpath, i.e.,P = wnPATH(t, ci) (3)for all concepts ci of t, P = 0 otherwise.?
wnlch: Weighting for the term-conceptpairs in the Wordnet based on the Leacock-Chodorow Similiarity, i.e.,P = wnLCH(t, ci) (4)for all concepts ci of t, P = 0 otherwise.?
wnwup: Weighting for the term-conceptpairs in the Wordnet based on the Wu-PalmerSimilarity, i.e.,P = wnWUP (t, ci) (5)for all concepts ci of t, P = 0 otherwise.?
wnres: Weighting for the term-conceptpairs in the Wordnet based on the ResnikSimilarity using Information Content, i.e.,P = wnRES(t, ci) (6)for all concepts ci of t, P = 0 otherwise.To create a grammar-based proximity matrix,we extracted the concept-token pairs from theparser output on the reference transcriptions inboth corpora.
In order to create a wordnet-basedproximity matrix, we retrieve the hypernyms forthe corresponding from Wordnet using the Word-net 3.0 database 2.
For the freebase concept-tokenpairs, we query tokens for a list of types with thehelp of the MQL query interface3 to the freebase.To retrieve beliefs from NELL we downloaded atsv formatted database called every-belief-in-the-KB4 and then queried for facts using unix grepcommand.5.2 ResultsOur objective is to evalute the effect of augmentedsemantic features on the task detection.
As notedearlier, we divided both corpora into training andtesting datasets.
We build our models on the man-ual transcriptions from the training data and eval-uate on the ASR hypotheses of the testing data.2http://www.princeton.edu/wordnet/download/3https://www.googleapis.com/freebase/v1/search4http://rtw.ml.cmu.edu/rtw/resourcesFor the Navagati corpus, we use the same training-testing split that we used in our previous work be-cause the grammar was developed based on thetraining data.
For the Roomline corpus, we ran-domly sample 30% of the testing data from theentire corpus.Our first semantic-kernel based model SEM-GRA uses the domain grammar as a ?knowledgebase?.
This is a two step process: (1) we extractthe concept-token pairs from the parse output ofthe training data.
(2) Then, assign a uniform prox-imity score (1.0) for all pairs of words that ap-pear under a particular concept otherwise 0.0 (graas mentioned in the previous section).
We aug-ment the grammar concepts to the utterances inthe datasets, learn SEM-GRA model and classifythe test-hypotheses.
For all our models we usea fixed C = 0.07 value (soft-margin parameter)for the SVM classifiers.
This model achieved high-est performance at this value during a parameter-sweep.
SEM-GRA model outperformed the parser-baseline on both datasets (see Table 3).
It clearlytakes advantage of the domain knowledge encodedin the form of semantic-relatedness between con-cepts and token pairs.What if a dialog system does not have gram-mar to begin with?
We use the same two step pro-cess to build semantic-kernel based models usingone open-domain knowledge base at a time.
Webuilt Wordnet based models (SEM-WNWUP, SEM-WNPATH, SEM-WNLCH, SEM-WNRES) using dif-ferent proximity measures described in the previ-ous section.
From Table 3 SEM-WNRES model,one that uses information content performs thebest among all wordnet based models.
In orderto compute the information content we used thepair-wise mutual information scores available forbrown-corpus.dat in the NLTK (Bird et al 2009)distribution.
Other path based scores were alsocomputed using NLTK API for Wordnet.We observed that our wordnet-based modelscapture relatedness between most-common nouns(e.g., room numbers) and their concepts but notfor some of the less-common ones (e.g., loca-tion names).
To compensate this imbalance, weuse larger knowledge resources freebase.com andNELL.
First we searched for the facts in each ofthese knowledge bases using every token in the vo-cabulary of both corpora.
We pick the top conceptfor each token based on the score provided by therespective search interfaces.
In freebase we have246Table 3: F1 (in %) comparison of parse baseline against semantic-kernel models with their correspondingsimilarity metricsCorpus baseline SEMGRA SEMWNWUP SEMWNPATH SEMWNLCH SEMWNRES SEMFBASE SEMNELLNavagati 40.1 65.8 67.1 67.7 66.4 69 68.7 66.2Roomline 54.3 79.7 77.3 79.5 79.6 80.6 83.3 81.1about 100 concepts that are relevant to the vocab-ulary and in the NELL model we have about 250concepts that are relevant to the vocabulary in eachof the corpora.
The models based on NELL (SEM-NELL) and Freebase (SEM-FBASE) capture relat-edness between less-common nouns and their con-cepts.
We can see that both of these models per-form comparable to the domain grammar modelSEM-GRA which also captures the relatedness be-tween less-common nouns and their concepts.
Webelieve that both freebase and NELL has a supe-rior performance because of wider-range of con-cept coverage and non-uniform proximity mea-sures used in the semantic kernel, which givesa better judgement of relatedness than a uniformmeasure used in the SEM-GRA model.Since we observed that an individual model isgood at capturing a particular aspect of an utter-ance, we extended the individual semantic modelsby combining the proximity matrices from eachof them and augmenting their semantic conceptsto the training and testing datasets.
We built fourcombined models as shown in Table 4 by varyingthe wordnet?s proximity metric to identify whichone of them works best in combination with othersemantic metrics.
The wnres metric performs thebest both in standalone and combination settings.
(Bloehdorn et al 2006) also found that wnresparticularly performs well for lower values of thesoft-margin parameter in their experiments.Table 4: F1-Score (in %): Models with semanticscombined from different KBs (ALL-KB)Model Navagati RoomlineGRA+WNWUP+FBASE+NELL 70.8 82.2GRA+WNPATH+FBASE+NELL 70.1 81.4GRA+WNLCH+FBASE+NELL 70.8 81.3GRA+WNRES+FBASE+NELL 73.4 83.76 DiscussionWe have built a model that exploits different se-mantic knowledge bases and predicts the task onboth corpora with high accuracy.
But how is it af-fected by factors like misrecognition and contextambiguity?6.1 Influence of Recognition ErrorsWhen the recognition is bad, it is obvious that theaccuracy would go down.
We would like to knowwhich of these knowledge resources can augmentuseful semantics despite misrecognitions.
Table 2shows that WER on the Navagati corpus is about46% and the Roomline corpus is about 25%.
Wecompared the F1-score of different models on ut-terances for different ranges of WER as shown inthe Figure 2 on the Navagati Corpus.
We noticethat the model built using all knowledge bases ismore robust even at higher WER.
We did similaranalysis on the Roomline corpus and did not no-tice any differences across models due to relativelylower WER (25.6%).0 20 40 605060708090Word Error RateF1Scoreall-kbwnresnellfbaseFigure 2: Word Error Rate vs F1-Score for KB-based Models on Navagati Corpus6.2 Confusion among TasksWe found that a particular pair of tasks are moreconfusing than others.
Here we present an analysisof such confusion pairs for both corpora for dif-ferent classification models.
Table 5 and Table 6show the pairs of tasks that are most confused inthe experiments.
The ALL-KB model (a combina-tion of all knowledge bases) has least number of247Table 5: Most confusable pairs of tasks in Navagati Corpus for KB based classification models(See Table 2 for task labels)KBType ALL-KB SEM-WNRES SEM-NELL SEM-FBASEActualTask N2 N4 N2 N4 N2 N4 N1 N2 N4Predicted N3 N1 N3 N3 N3 N3 N3 N3 N3ConfusionPerTask 10.5% 27.7% 26.3% 33.3% 26.3% 38.8% 22.2% 28.9% 44.4%Table 6: Most confusable pairs of tasks in Roomline Corpus for KB based classification models(See Table 2 for task labels)KBType ALL-KB SEM-WNRES SEM-NELL SEM-FBASEActualTask R4 R4 R6 R4 R6 R3 R4 R5 R6Predicted R3 R5 R5 R1 R1 R1 R3 R1 R1ConfusionPerTask 36.6% 48.7% 44.4% 25.6% 44.5% 32.5% 23% 53.4% 55.5%confusion pairs among all the models.
This is dueto more relevant concepts are augmented to an ut-terance compared to fewer relevant concepts thataugmented while using individual models.We inspected the confused tasks by examin-ing the feature vectors of misclassified examples.While using the ALL-KB model 10% of the utter-ances from N2 (Advisory) were confused for N3(Imperative) because of phrases like ?your left?,?your right?.
These phrases were often associatedwith N3 utterances.
To recovery from such ambi-guities, the agent could ask a clarification questione.g., ?are we talking about going there or find iton the way??
to resolve the differences betweenthese tasks.
The system could not only get clar-ification but also bootstrap the original utteranceof the user with the clarification to gather addi-tional context to retrain the task detection models.The individual models were also confused aboutN2 and N3 tasks, where we could use similar clar-ification strategies to improve the task prediction.27% of the N4 (grounding about current robot?sposition) utterances were confused for N1 (metacomments about the robot?s rounavigation route)because these utterances shared more number offreebase concepts with the N1 model.
The systemcould resolve such utterances by asking a clarifi-cation question ?are we talking about the currentposition??.
Individual models i.e., SEM-WNRES,SEM-FBASE and SEM-NELL suffered mostly fromthe lack of concepts capturing semantics relatedto all types of entities (e.g., most common nouns,less common entities etc.,) found in an utterance.We examined the confusion pairs in the Room-line corpus and observed that R4 (ListRooms) andR3 (Queries) tasks were most confused in theALL-KB model.
On closer inspection, we foundthat R4 utterances are about listing the rooms thatare retrieved by the system.
Whereas, R3 utter-ances are about asking whether a room has a facil-ity (e.g., projector availability).
In the ambiguousutterances, often the R4 utterances were about fil-tering the list of rooms by a facility type.7 ConclusionWe proposed framing the dialog task predictionproblem as a classification problem.
We used anSVM classifier with semantic smoothing kernelsthat incorporate information from external knowl-edge bases such as Wordnet, NELL, Freebase.
Ourmethod shows good improvements over a parser-based baseline.
Our analysis also shows that ourproposed method makes task prediction be morerobust to moderate recognition errors.We presented an analysis on task ambiguity andfound that these models can confuse one task foranother.
We believe that this analysis highlightsthe need for dialog based clarification strategiesthat cannot only help the system for that instancebut also help the system improve its task predic-tion accuracy in future dialog sessions.8 Future WorkThis work stands as a platform to make a spokendialog system learn relevant semantic informationfrom external knowledge sources.
We would liketo extend this paradigm to let the system acquiremore information through dialog with a user.
Thesystem could elicit new references to a known se-mantic concept.
For example, a navigation agentknows a task called ?GoToRestaurant?
but theuser-utterance had the word ?diner?
and it was248not seen in the context of ?restaurant?.
The agentsomewhat predicts this utterance is related to ?Go-ToRestaurant?
using the approach described in thispaper.
It could ask the user an elicitation question:?You used diner in the context of a restaurant, isdiner really a restaurant??.
The answer to thisquestion will help the system gradually understandwhat parts of an open-domain knowledge base canbe added into its own domain knowledge base.
Webelieve that the holistic approach of learning fromautomated processes and learning through dialog,will help the dialog systems get better interactionby interaction.ReferencesMasahiro Araki.
2012.
Rapid development process ofspoken dialogue systems using collaboratively con-structed semantic resources.
In Proceedings of the13th Annual Meeting of the Special Interest Groupon Discourse and Dialogue, pages 70?73, Seoul,South Korea, July.
Association for ComputationalLinguistics.Srinivas Bangalore and Amanda J Stent.
2009.
In-cremental parsing models for dialog task structure.In Proceedings of the 12th Conference of the Euro-pean Chapter of the Association for ComputationalLinguistics, pages 94?102.
Association for Compu-tational Linguistics.Steven Bird, Ewan Klein, and Edward Loper.
2009.Natural language processing with Python.
O?ReillyMedia.Nate Blaylock and James Allen.
2006.
Hierarchicalinstantiated goal recognition.
In Proceedings of theAAAI Workshop on Modeling Others from Observa-tions.Stephan Bloehdorn, Roberto Basili, Marco Cammisa,and Alessandro Moschitti.
2006.
Semantic ker-nels for text classification based on topological mea-sures of feature similarity.
In Data Mining, 2006.ICDM?06.
Sixth International Conference on, pages808?812.
IEEE.Dan Bohus and Alexander I Rudnicky.
2005.Sorry, I didn?t catch that!-an investigation of non-understanding errors and recovery strategies.
In 6thSIGdial Workshop on Discourse and Dialogue.Dan Bohus, Antoine Raux, Thomas K Harris, MaxineEskenazi, and Alexander I Rudnicky.
2007.
Olym-pus: an open-source framework for conversationalspoken language interface research.
In Proceedingsof the workshop on bridging the gap Academic andindustrial research in dialog technologies, numberApril, pages 32?39.
Association for ComputationalLinguistics.Dan Bohus.
2003.
Roomline.
http://www.cs.cmu.edu/~dbohus/RoomLine.Kurt Bollacker, Colin Evans, Praveen Paritosh, TimSturge, and Jamie Taylor.
2008.
Freebase: a col-laboratively created graph database for structuringhuman knowledge.
SIGMOD 08 Proceedings of the2008 ACM SIGMOD international conference onManagement of data, pages 1247?1249.Hung H Bui.
2003.
A general model for online proba-bilistic plan recognition.
In International Joint Con-ference on Artificial Intelligence, volume 18, pages1309?1318.
Citeseer.Andrew Carlson, Justin Betteridge, Bryan Kisiel, BurrSettles, Estevam R Hruschka Jr., and Tom MMitchell.
2010.
Toward an Architecture for Never-Ending Language Learning.
Artificial Intelligence,2(4):1306?1313.D.L.
Chen and R.J. Mooney.
2010.
Learning to in-terpret natural language navigation instructions fromobservations.
Journal of Artificial Intelligence Re-search, 37:397?435.Jennifer Chu-Carroll and Bob Carpenter.
1999.Vector-based natural language call routing.
Compu-tational linguistics, 25(3):361?388.Mathias Creutz, Sami Virpioja, and Anna Kovaleva.2009.
Web augmentation of language models forcontinuous speech recognition of sms text messages.In Proceedings of the 12th Conference of the Euro-pean Chapter of the Association for ComputationalLinguistics, pages 157?165.
Association for Com-putational Linguistics.Nello Cristianini, John Shawe-Taylor, and HumaLodhi.
2002.
Latent semantic kernels.
Journal ofIntelligent Information Systems, 18(2):127?152.Allen L Gorin, Giuseppe Riccardi, and Jeremy HWright.
1997.
How may i help you?
Speech com-munication, 23(1-2):113?127.D.
Huggins-Daines, M. Kumar, A. Chan, A.W.
Black,M.
Ravishankar, and A.I.
Rudnicky.
2006.
Pocket-sphinx: A free, real-time continuous speech recogni-tion system for hand-held devices.
In ICASSP, vol-ume 1.
IEEE.Thorsten Joachims.
1999.
Making large-scale supportvector machine learning practical.
In Advances inkernel methods, pages 169?184.
MIT Press.George A Miller.
1995.
WordNet: a lexicaldatabase for English.
Communications of the ACM,38(11):39?41.Teruhisa Misu and Tatsuya Kawahara.
2006.
A boot-strapping approach for developing language modelof new spoken dialogue systems by selecting webtexts.
In Proc.
Interspeech, pages 9?12.249Alessandro Moschitti.
2009.
Syntactic and semantickernels for short text pair categorization.
In Pro-ceedings of the 12th Conference of the EuropeanChapter of the ACL (EACL 2009), pages 576?584.Aasish Pappu and Alexander I Rudnicky.
2012.
TheStructure and Generality of Spoken Route Instruc-tions.
Proceedings of the 13th Annual Meeting ofthe Special Interest Group on Discourse and Dia-logue, pages 99?107.Ted Pedersen, Siddharth Patwardhan, and Jason Miche-lizzi.
2004.
WordNet:: Similarity: measuring therelatedness of concepts.
In Demonstration Papersat HLT-NAACL 2004, pages 38?41.
Association forComputational Linguistics.Georges Siolas and Florence d?Alch?
Buc.
2000.
Sup-port vector machines based on a semantic kernelfor text categorization.
In Neural Networks, 2000.IJCNN 2000, Proceedings of the IEEE-INNS-ENNSInternational Joint Conference, volume 5, pages205?209.
IEEE.Pu Wang and Carlotta Domeniconi.
2008.
Build-ing semantic kernels for text classification usingwikipedia.
In Proceeding of the 14th ACM SIGKDDinternational conference on Knowledge discoveryand data mining, pages 713?721.
ACM.W.
Ward.
1991.
Understanding spontaneous speech:the phoenix system.
In ICASSP.
IEEE.250
