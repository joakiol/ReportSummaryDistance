Proceedings of the 12th Conference of the European Chapter of the ACL, pages 781?789,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsText Summarization Modelbased on Maximum Coverage Problem and its VariantHiroya Takamura and Manabu OkumuraPrecision and Intelligence Laboratory, Tokyo Institute of Technology4259 Nagatsuta Midori-ku Yokohama, 226-8503takamura@pi.titech.ac.jp oku@pi.titech.ac.jpAbstractWe discuss text summarization in terms ofmaximum coverage problem and its vari-ant.
We explore some decoding algorithmsincluding the ones never used in this sum-marization formulation, such as a greedyalgorithm with performance guarantee, arandomized algorithm, and a branch-and-bound method.
On the basis of the resultsof comparative experiments, we also aug-ment the summarization model so that ittakes into account the relevance to the doc-ument cluster.
Through experiments, weshowed that the augmented model is su-perior to the best-performing method ofDUC?04 on ROUGE-1 without stopwords.1 IntroductionAutomatic text summarization is one of the tasksthat have long been studied in natural languageprocessing.
This task is to create a summary, ora short and concise document that describes thecontent of a given set of documents (Mani, 2001).One well-known approach to text summariza-tion is the extractive method, which selects somelinguistic units (e.g., sentences) from given doc-uments in order to generate a summary.
The ex-tractive method has an advantage that the gram-maticality is guaranteed at least at the level of thelinguistic units.
Since the actual generation oflinguistic expressions has not achieved the levelof the practical use, we focus on the extractivemethod in this paper, especially the method basedon the sentence extraction.
Most of the extractivesummarization methods rely on sequentially solv-ing binary classification problems of determiningwhether each sentence should be selected or not.In such sequential methods, however, the view-point regarding whether the summary is good asa whole, is not taken into consideration, althougha summary conveys information as a whole.We represent text summarization as an opti-mization problem and attempt to globally solvethe problem.
In particular, we represent text sum-marization as a maximum coverage problem withknapsack constraint (MCKP).
One of the advan-tages of this representation is that MCKP can di-rectly model whether each concept in the givendocuments is covered by the summary or not,and can dispense with rather counter-intuitive ap-proaches such as giving penalty to each pair of twosimilar sentences.
By formally apprehending thetarget problem, we can use a lot of knowledge andtechniques developed in the combinatorial mathe-matics, and also analyse results more precisely.
Infact, on the basis of the results of the experiments,we augmented the summarization model.The contributions of this paper are as follows.We are not the first to represent text summarizationas MCKP.
However, no researchers have exploitedthe decoding algorithms for solving MCKP inthe summarization task.
We conduct compre-hensive comparative experiments of those algo-rithms.
Specifically, we test the greedy algorithm,the greedy algorithm with performance guarantee,the stack decoding, the linear relaxation problemwith randomized decoding, and the branch-and-bound method.
On the basis of the experimentalresults, we then propose an augmented model thattakes into account the relevance to the documentcluster.
We empirically show that the augmentedmodel is superior to the best-performing methodof DUC?04 on ROUGE-1 without stopwords.2 Related WorkCarbonell and Goldstein (2000) used sequentialsentence selection in combination with maximalmarginal relevance (MMR), which gives penaltyto sentences that are similar to the already se-lected sentences.
Schiffman et al?s method (2002)is also based on sequential sentence selection.Radev et al (2004), in their method MEAD, useda clustering technique to find the centroid, that781is, the words with high relevance to the topicof the document cluster.
They used the centroidto rank sentences, together with the MMR-likeredundancy score.
Both relevance and redun-dancy are taken into consideration, but no globalviewpoint is given.
In CLASSY, which is thebest-performing method in DUC?04, Conroy etal.
(2004) scored sentences with the sum of tf-idfscores of words.
They also incorporated sentencecompression based on syntactic or heuristic rules.McDonald (2007) formulated text summariza-tion as a knapsack problem and obtained theglobal solution and its approximate solutions.
Itsrelation to our method will be discussed in Sec-tion 6.1.
Filatova and Hatzivassiloglou (2004) firstformulated text summarization as MCKP.
Theirdecoding method is a greedy one and will be em-pirically compared with other decoding methodsin this paper.
Yih et al (2007) used a slightly-modified stack decoding.
The optimization prob-lem they solved was the MCKP with the last sen-tence truncation.
Their stack decoding is one ofthe decoding methods discussed in this paper.
Yeet al (2007) is another example of coverage-basedmethods.
Shen et al (2007) regarded summariza-tion as a sequential labelling task and solved itwith Conditional Random Fields.
Although themodel is globally optimized in terms of likelihood,the coverage of concepts is not taken into account.3 Modeling text summarizationIn this paper, we focus on the extractive summa-rization, which generates a summary by select-ing linguistic units (e.g., sentences) in given doc-uments.
There are two types of summarizationtasks: single-document summarization and multi-document summarization.
While single-documentsummarization is to generate a summary from asingle document, multi-document summarizationis to generate a summary frommultiple documentsregarding one topic.
Such a set of multiple docu-ments is called a document cluster.
The methodproposed in this paper is applicable to both tasks.In both tasks, documents are split into several lin-guistic units D = {s1, ?
?
?
, s|D|} in preprocess-ing.
We will select some linguistic units from D togenerate a summary.
Among other linguistic unitsthat can be used in the method, we use sentencesso that the grammaticality at the sentence level isgoing to be guaranteed.We introduce conceptual units (Filatova andHatzivassiloglou, 2004), which compose themeaning of a sentence.
Sentence si is representedby a set of conceptual units {ei1, ?
?
?
, ei|si|}.
Forexample, the sentence ?The man bought a bookand read it?
could be regarded as consisting of twoconceptual units ?the man bought a book?
and ?theman read the book?.
It is not easy, however, todetermine the appropriate granularity of concep-tual units.
A simple way would be to regard theabove sentence as consisting of four conceptualunits ?man?, ?book?, ?buy?, and ?read?.
Thereis some work on the definition of conceptual units.Hovy et al (2006) proposed to use basic elements,which are dependency subtrees obtained by trim-ming dependency trees.
Although basic elementswere proposed for evaluation of summaries, theycan probably be used also for summary genera-tion.
However, such novel units have not provedto be useful for summary generation.
Since we fo-cus more on algorithms and models in this paper,we simply use words as conceptual units.The goal of text summarization is to cover asmany conceptual units as possible using only asmall number of sentences.
In other words, thegoal is to find a subset S(?
D) that covers asmany conceptual units as possible.
In the follow-ing, we introduce models for that purpose.
Wethink of the situation that the summary length mustbe at most K (cardinality constraint) and the sum-mary length is measured by the number of wordsor bytes in the summary.Let xi denote a variable which is 1 if sentencesi is selected, otherwise 0, aij denote a constantwhich is 1 if sentence si contains word ej , oth-erwise 0.
We regard word ej as covered when atleast one sentence containing ej is selected as partof the summary.
That is, word ej is covered if andonly if?i aijxi ?
1.
Now our objective is to findthe binary assignment on xi with the best coveragesuch that the summary length is at most K:max.
|{j|?i aijxi ?
1}|s.t.
?i cixi ?
K; ?i, xi ?
{0, 1},where ci is the cost of selecting si, i.e., the numberof words or bytes in si.For convenience, we rewrite the problem above:max.
?j zjs.t.
?i cixi ?
K; ?j,?i aijxi ?
zj ;?i, xi ?
{0, 1}; ?j, zj ?
{0, 1},782where zj is 1 when ej is covered, 0 otherwise.
No-tice that this new problem is equivalent to the pre-vious one.Since not all the words are equally important,we introduce weights wj on words ej .
Then theobjective is restated as maximizing the weightedsum?j wjzj such that the summary length is atmost K. This problem is called maximum cov-erage problem with knapsack constraint (MCKP),which is an NP-hard problem (Khuller et al,1999).
We should note that MCKP is differentfrom a knapsack problem.
MCKP merely has aconstraint of knapsack form.
Filatova and Hatzi-vassiloglou (2004) pointed out that text summa-rization can be formalized by MCKP.The performance of the method depends on howto represent words and which words to use.
Werepresent words with their stems.
We use onlythe words that are content words (nouns, verbs,or adjectives) and not in the stopword list used inROUGE (Lin, 2004).The weights wj of words are also an impor-tant factor of good performance.
We tested twoweighting schemes proposed by Yih et al (2007).The first one is interpolated weights, which are in-terpolated values of the generative word probabil-ity in the entire document and that in the beginningpart of the document (namely, the first 100 words).Each probability is estimated with the maximumlikelihood principle.
The second one is trainedweights.
These values are estimated by the logis-tic regression trained on data instances, which arelabeled 1 if the word appears in a summary in thetraining dataset, 0 otherwise.
The feature set forthe logistic regression includes the frequency ofthe word in the document cluster and the positionof the word instance and others.4 Algorithms for solving MCKPWe explain how to solve MCKP.
We first explainthe greedy algorithm applied to text summariza-tion by Filatova and Hatzivassiloglou (2004).
Wethen introduce a greedy algorithm with perfor-mance guarantee.
This algorithm has never beenapplied to text summarization.
We next explain thestack decoding used by Yih et al (2007).
We thenintroduce an approximate method based on linearrelaxation and a randomized algorithm, followedby the branch-and-bound method, which providesthe exact solution.Although the algorithms used in this paperthemselves are not novel, this work is the firstto apply the greedy algorithm with performanceguarantee, the randomized algorithm, and thebranch-and-bound to solve the MCKP and auto-matically create a summary.
In addition, we con-duct a comparative study on summarization algo-rithms including the above.There are some other well-known methods forsimilar problems (e.g., the method of conditionalprobability (Hromkovic?, 2003)).
A pipage ap-proach (Ageev and Sviridenko, 2004) has beenproposed for MCKP, but we do not use this algo-rithm, since it requires costly partial enumerationand solutions to many linear relaxation problems.As in the previous section, D denotes the set ofsentences {s1, ?
?
?
, s|D|}, and S denotes a subsetof D and thus represents a summary.4.1 Greedy algorithmFilatova and Hatzivassiloglou (2004) used agreedy algorithm.
In this section, Wl denotes thesum of the weights of the words covered by sen-tence sl.
W ?l denotes the sum of the weights of thewords covered by sl, but not by current summaryS.
This algorithm sequentially selects sentence slwith the largest W ?l .Greedy AlgorithmU ?
D, S ?
?while U 6= ?si ?
argmaxsl?U W ?lif ci +?sl?S cl ?
K then insert si into Sdelete si in Uend whileoutput S.This algorithm has performance guaranteewhen the problem has a unit cost (i.e., when eachsentence has the same length), but no performanceguarantee for the general case where costs canhave different values.4.2 Greedy algorithm with performanceguaranteeWe describe a greedy algorithm with performanceguarantee proposed by Khuller et al (1999), whichproves to achieve an approximation factor of (1 ?1/e)/2 for MCKP.
This algorithm sequentially se-lects sentence sl with the largest ratio W ?l /cl.
Af-ter the sequential selection, the set of the selectedsentences is compared with the single-sentencesummary that has the largest value of the objec-tive function.
The larger of the two is going to783be the output of this new greedy algorithm.
Herescore(S) is?j wjzj , the value of the objectivefunction for summary S.Greedy Algorithm with Performance GuaranteeU ?
D, S ?
?while U 6= ?si ?
argmaxsl?U W ?l /clif ci +?sl?S cl ?
K then insert si into Sdelete si in Uend whilest ?
argmaxsl Wlif score(S) ?
Wt, output S,otherwise, output {st}.They also proposed an algorithm with a better per-formance guarantee, which is not used in this pa-per because it is costly due to its partial enumera-tion.4.3 Stack decodingStack decoding is a decoding method proposed byJelinek (1969).
This algorithm requires K priorityqueues, k-th of which is the queue for summariesof length k. The objective function value is usedfor the priority measure.
A new solution (sum-mary) is generated by adding a sentence to a cur-rent solution in k-th queue and inserted into a suc-ceeding queue.1 The ?pop?
operation in stack de-coding pops the candidate summary with the leastpriority in the queue.
By restricting the size ofeach queue to a certain constant stacksize, we canobtain an approximate solution within a practicalcomputational time.Stack Decodingfor k = 0 to K ?
1for each S ?
queues[k]for each sl ?
Dinsert sl into Sinsert S into queues[k + cl]pop if queue-size exceeds the stacksizeend forend forend forreturn the best solution in queues[K]4.4 Randomized algorithmKhuller et al (2006) proposed a randomized al-gorithm (Hromkovic?, 2003) for MCKP.
In this al-gorithm, a relaxation linear problem is generatedby replacing the integer constraints xi ?
{0, 1}1We should be aware that stack in a strict data-structuresense is not used in the algorithm.and zj ?
{0, 1} with linear constraints xi ?
[0, 1]and zj ?
[0, 1].
The optimal solution x?i to the re-laxation problem is regarded as the probability ofsentence si being selected as a part of summary:x?i = P (xi = 1).
The algorithm randomly se-lects sentence si with probability x?i , in order togenerate a summary.
It has been proved that theexpected length of each randomly-generated sum-mary is upper-bounded by K, and the expectedvalue of the objective function is at least the op-timal value multiplied by (1?1/e) (Khuller et al,2006).
This random generation of a summary is it-erated many times, and the summaries that are notlonger than K are stored as candidate summaries.Among those many candidate summaries, the onewith the highest value of the objective function isgoing to be the output by this algorithm.4.5 Branch-and-bound methodThe branch-and-bound method (Hromkovic?,2003) is an efficient method for finding the exactsolutions to integer problems.
Since MCKP is anNP-hard problem, it cannot generally be solved inpolynomial time under a reasonable assumptionthat NP 6=P.
However, if the size of the problemis limited, sometimes we can obtain the exactsolution within a practical time by means of thebranch-and-bound method.4.6 Weakly-constrained algorithmsIn evaluation with ROUGE (Lin, 2004), sum-maries are truncated to a target length K. Yih etal.
(2007) used a stack decoding with a slight mod-ification, which allows the last sentence in a sum-mary to be truncated to a target length.
Let us callthis modified algorithm the weakly-constrainedstack decoding.
The weakly-constrained stack de-coding can be implemented simply by replacingqueues[k + cl] with queues[min(k + cl,K)].
Wecan also think of weakly-constrained versions ofthe greedy and randomized algorithms introducedbefore.In this paper, we do not adopt weakly-constrained algorithms, because although an ad-vantage of the extractive summarization is theguaranteed grammaticality at the sentence level,the summaries with a truncated sentence will relin-quish this advantage.
We mentioned the weakly-constrained algorithms in order to explain the re-lation between the proposed model and the modelproposed by Yih et al (2007).7845 Experiments and Discussion5.1 Experimental SettingWe conducted experiments on the dataset ofDUC?04 (2004) with settings of task 2, which isa multi-document summarization task.
50 docu-ment clusters, each of which consists of 10 doc-uments, are given.
One summary is to be gen-erated for each cluster.
Following the most rel-evant previous method (Yih et al, 2007), we setthe target length to 100 words.
DUC?03 (2003)dataset was used as the training dataset for trainedweights.
All the documents were segmentedinto sentences using a script distributed by DUC.Words are stemmed by Porter?s stemmer (Porter,1980).
ROUGE version 1.5.5 (Lin, 2004) wasused for evaluation.2 Among others, we focuson ROUGE-1 in the discussion of the result, be-cause ROUGE-1 has proved to have strong corre-lation with human annotation (Lin, 2004; Lin andHovy, 2003).
Wilcoxon signed rank test for pairedsamples with significance level 0.05 was used forthe significance test of the difference in ROUGE-1.
The simplex method and the branch-and-boundmethod implemented in GLPK (Makhorin, 2006)were used to solve respectively linear and integerprogramming problems.The methods that are compared here are thegreedy algorithm (greedy), the greedy algorithmwith performance guarantee (g-greedy), the ran-domized algorithm (rand), the stack decoding(stack), and the branch-and-bound method (exact).5.2 ResultsThe experimental results are shown in Tables 1and 2.
The columns 1, 2, and SU4 in the ta-bles respectively refer to ROUGE-1, ROUGE-2,and ROUGE-SU4.
In addition, rand100k refers tothe randomized algorithmwith 100,000 randomly-generated solution candidates, and stack30 refersto stack with the stacksize being 30.
The right-most column (?time?)
shows the average computa-tional time required for generating a summary fora document cluster.Both with interpolated (Table 1) and trainedweights (Table 2), g-greedy significantly outper-formed greedy.
With interpolated weights, therewas no significant difference between exact andg-greedy, and between exact and stack30.
Withtrained weights, there was no significant differ-2With options -n 4 -m -2 4 -u -f A -p 0.5 -l 100 -t 0 -d -s.Table 1: ROUGE of MCKP with interpolatedweights.
Underlined ROUGE-1 scores are signif-icantly different from the score of exact.
Compu-tational time was measured in seconds.ROUGE time1 2 SU4 (sec)greedy 0.283 0.083 0.123 <0.01g-greedy 0.294 0.080 0.121 0.01rand100k 0.300 0.079 0.119 1.88stack30 0.304 0.078 0.120 4.53exact 0.305 0.081 0.121 4.04Table 2: ROUGE of MCKP with trained weights.Underlined ROUGE-1 scores are significantly dif-ferent from the score of exact.
Computational timewas measured in seconds.ROUGE time1 2 SU4 (sec)greedy 0.283 0.080 0.121 < 0.01g-greedy 0.310 0.077 0.118 0.01rand100k 0.299 0.077 0.117 1.93stack30 0.309 0.080 0.120 4.23exact 0.307 0.078 0.119 4.56ence between exact and the other algorithms ex-cept for greedy and rand100k.
The result sug-gests that approximate fast algorithms can yieldresults comparable to the exact method in terms ofROUGE-1 score.
We will later discuss the resultsin terms of objective function values and searcherrors in Table 4.We should notice that stack outperformed ex-act with interpolated weights.
To examine thiscounter-intuitive point, we changed the stack-size of stack with interpolated weights (inter) andtrained weights (train) from 10 to 100 and ob-tained Table 3.
This table shows that the ROUGE-1 value does not increase as the stacksize does;ROUGE-1 for stack with interpolated weightsdoes not change much with the stacksize, and thepeak of ROUGE-1 for trained weights is at thestacksize of 20.
Since stack with a larger stack-size selects a solution from a larger number of so-lution candidates, this result is counter-intuitive inthe sense that non-global decoding by stack has afavorable effect.We also counted the number of the documentclusters for which an approximate algorithm withinterpolated weights yielded the same solution as785Table 3: ROUGE of stack with various stacksizessize 10 20 30 50 100inter 0.304 0.304 0.304 0.304 0.303train 0.308 0.310 0.309 0.308 0.307Table 4: Search errors of MCKP with interpolatedweightssolution same search errorROUGE (=) = ?
?greedy 0 1 35 14g-greedy 0 5 26 19rand100k 6 5 25 14stack30 16 11 8 11exact (?same solution?
column in Table 4).
Ifthe approximate algorithm failed to yield the ex-act solution (?search error?
column), we checkedwhether the search error made ROUGE scoreunchanged (?=?
column), decreased (???
col-umn), or increased (???
column) compared withROUGE score of exact.
Table 4 shows that (i)stack30 is a better optimizer than other approx-imate algorithms, (ii) when the search error oc-curs, stack30 increases ROUGE-1 more often thanit decreases ROUGE-1 compared with exact inspite of stack30?s inaccurate solution, (iii) ap-proximate algorithms sometimes achieved betterROUGE scores.
We observed similar phenomenafor trained weights, though we skip the details dueto space limitation.These observations on stacksize and search er-rors suggest that there exists another maximizationproblem that is more suitable to summarization.We should attempt to find the more suitable maxi-mization problem and solve it using some existingoptimization and approximation techniques.6 Augmentation of the modelOn the basis of the experimental results in the pre-vious section, we augment our text summarizationmodel.
We first examine the current model morecarefully.
As mentioned before, we used wordsas conceptual units because defining those unitsis hard and still under development by many re-searchers.
Suppose here that a more suitable unithas more detailed information, such as ?A did Bto C?.
Then the event ?A did D to E?
is a com-pletely different unit from ?A did B to C?.
How-ever, when words are used as conceptual units, thetwo events have a redundant part ?A?.
It can hap-pen that a document is concise as a summary, butredundant on word level.
By being to some extentredundant on the word level, a summary can havesentences that are more relevant to the documentcluster, as both of the sentences above are relevantto the document cluster if the document cluster isabout ?A?.
A summary with high cohesion and co-herence would have redundancy to some extent.
Inthis section, we will use this conjecture to augmentour model.6.1 Augmented summarization modelThe objective function of MCKP consists of onlyone term that corresponds to coverage.
We addanother term?i(?j wjaij)xi that correspondsto relevance to the topic of the document clus-ter.
We represent the relevance of sentence si bythe sum of the weights of words in the sentence(?j wjaij).
We take the summation of the rele-vance values of the selected sentences:max.
(1?
?
)?j wjzj + ?
?i(?j wjaij)xis.t.
?i cixi ?
K; ?j,?i aijxi ?
zj ;?i, xi ?
{0, 1}; ?j, zj ?
{0, 1},where ?
is a constant.
We call this model MCKP-Rel, because the relevance to the document clusteris taken into account.We discuss the relation to the model proposedby McDonald (2007), whose objective functionconsists of a relevance term and a negative re-dundancy term.
We believe that MCKP-Rel ismore intuitive and suitable for summarization, be-cause coverage in McDonald (2007) is measuredby subtracting the redundancy represented withthe sum of similarities between two sentences,while MCKP-Rel focuses directly on coverage.Suppose sentence s1 contains conceptual units Aand B, s2 contains A, and s3 contains B. Theproposed coverage-based methods can capture thefact that s1 has the same information as {s2, s3},while similarity-based methods only learn that s1is somewhat similar to each of s2 and s3.
Wealso empirically showed that our method outper-forms McDonald (2007)?s method in experimentson DUC?02, where our method achieved 0.354ROUGE-1 score with interpolated weights and0.359 with trained weights when the optimal ?
isgiven, while McDonald (2007)?s method yieldedat most 0.348.
However, this very point can also786Table 5: ROUGE-1 of MCKP-Rel with inter-polated weights.
The values in the parenthesesare the corresponding values of ?
predicted usingDUC?03 as development data.
Underlined are thevalues that are significantly different from the cor-responding values of MCKP.interpolated trainedgreedy 0.287 (0.1) 0.288 (0.8)g-greedy 0.307 (0.3) 0.320 (0.4)rand100k 0.310 (0.1) 0.316 (0.5)stack30 0.324 (0.1) 0.327 (0.3)exact 0.320 (0.3) 0.329 (0.5)exactopt 0.327 (0.2) 0.329 (0.5)be a drawback of our method, since our methodpremises that a sentence is represented as a setof conceptual units.
Similarity-based methods arefree from such a premise.
Taking advantages ofboth models is left for future work.The decoding algorithms introduced before arealso applicable toMCKP-Rel, becauseMCKP-Relcan be reduced to MCKP by adding, for each sen-tence si, a dummy conceptual unit which existsonly in si and has the weight?j wjaij .6.2 Experiments of the augmented modelWe ran greedy, g-greedy, rand100k, stack30and exact to solve MCKP-Rel.
We experimentedon DUC?04 with the same experimental setting asthe previous ones.6.2.1 Experiments with the predicted ?We determined the value of ?
for each method us-ing DUC?03 as development data.
Specifically, weconducted experiments on DUC?03 with different?
(?
{0.0, 0.1, ?
?
?
, 1.0}) and simply selected theone with the highest ROUGE-1 value.The results with these predicted ?
are shownin Table 5.
Only ROUGE-1 values are shown.Method exactopt is exact with the optimal ?, andcan be regarded as the upperbound of MCKP-Rel.To evaluate the appropriateness of models withoutregard to search quality, we first focused on exactand found that MCKP-Rel outperformed MCKPwith exact.
This means that MCKP-Rel modelis superior to MCKP model.
Among the algo-rithms, stack30 and exact performed well.
Allmethods except for greedy yielded significantlybetter ROUGE values compared with the corre-sponding results in Tables 1 and 2.Figures 1 and 2 show ROUGE-1 for differentvalues of ?.
The leftmost part (?
= 0.0) cor-responds to MCKP.
We can see from the figures,that MCKP-Rel at the best ?
always outperformsMCKP, and that MCKP-Rel tends to degrade forvery large ?.
This means that excessive weight onrelevance has an adversative effect on performanceand therefore the coverage is important.0.280.290.30.310.320.330.340  0.2  0.4  0.6  0.8  1ROUGE-1lambdaexactstack30rand100kg-greedygreedyFigure 1: MCKP-Rel with interpolated weights0.280.290.30.310.320.330.340  0.2  0.4  0.6  0.8  1ROUGE-1lambdaexactstack30rand100kg-greedygreedyFigure 2: MCKP-Rel with trained weights6.2.2 Experiments with the optimal ?In the experiments above, we found that ?
=0.2 is the optimal value for exact with interpo-lated weights.
We suppose that this ?
gives thebest model, and examined search errors as wedid in Section 5.2.
We obtained Table 6, whichshows that search errors in MCKP-Rel counter-intuitively increase (?)
ROUGE-1 score less of-ten than MCKP did in Table 4.
This was the casealso for trained weights.
This result suggests thatMCKP-Rel is more suitable to text summariza-tion than MCKP is.
However, exact with trainedweights at the optimal ?
(= 0.4) in Figure 2 wasoutperformed by stack30.
It suggests that there isstill room for future improvement in the model.787Table 6: Search errors of MCKP-Rel with interpo-lated weights (?
= 0.2).solution same search errorROUGE (=) = ?
?greedy 0 2 42 6g-greedy 1 0 34 15rand100k 3 6 33 8stack30 14 13 14 106.2.3 Comparison with DUC resultsIn Section 6.2.1, we empirically showed thatthe augmented model MCKP-Rel is better thanMCKP, whose optimization problem is used alsoin one of the state-of-the-art methods by Yih etal.
(2007).
It would also be beneficial to read-ers to directly compare our method with DUCresults.
For that purpose, we conducted experi-ments with the cardinality constraint of DUC?04,i.e., each summary should be 665 bytes long orshorter.
Other settings remained unchanged.
Wecompared the MCKP-Rel with peer65 (Conroy etal., 2004) of DUC?04, which performed best interms of ROUGE-1 in the competition.
Tables 7and 8 are the ROUGE-1 scores, respectively eval-uated without and with stopwords.
The latter is theofficial evaluation measure of DUC?04.Table 7: ROUGE-1 of MCKP-Rel with byte con-straints, evaluated without stopwords.
Underlinedare the values significantly different from peer65.interpolated traingreedy 0.289 (0.1) 0.284 (0.8)g-greedy 0.297 (0.4) 0.323 (0.3)rand100k 0.315 (0.2) 0.308 (0.4)stack30 0.324 (0.2) 0.323 (0.3)exact 0.325 (0.3) 0.326 (0.5)exactopt 0.325 (0.3) 0.329 (0.4)peer65 0.309In Table 7, MCKP-Rel with stack30 and exactyielded significantly better ROUGE-1 scores thanpeer65.
Although stack30 and exact yieldedgreater ROUGE-1 scores than peer65 also in Ta-ble 8, the difference was not significant.
Onlygreedy was significantly worse than peer65.3 One3We actually succeeded in greatly improving theROUGE-1 value of MCKP-Rel evaluated with stopwords byusing all the words including stopwords as conceptual units.However, we ignore those results in this paper, because itTable 8: ROUGE-1 of MCKP-Rel with byte con-straints, evaluated with stopwords.
Underlined arethe values significantly different from peer65.interpolated traingreedy 0.374 (0.1) 0.377 (0.4)g-greedy 0.371 (0.0) 0.385 (0.2)rand100k 0.373 (0.2) 0.366 (0.3)stack30 0.384 (0.1) 0.386 (0.3)exact 0.383 (0.3) 0.384 (0.4)exactopt 0.385 (0.1) 0.384 (0.4)peer65 0.382possible explanation on the difference between Ta-ble 7 and Table 8 is that peer65 would probably betuned to the evaluation with stopwords, since it isthe official setting of DUC?04.From these results, we can conclude that theMCKP-Rel is at least comparable to the best-performing method, if we choose a powerful de-coding method, such as stack and exact.7 ConclusionWe regarded text summarization as MCKP.
Weapplied some algorithms to solve the MCKP andconducted comparative experiments.
We con-ducted comparative experiments.
We also aug-mented our model to MCKP-Rel, which takes intoconsideration the relevance to the document clus-ter and performs well.For future work, we will try other conceptualunits such as basic elements (Hovy et al, 2006)proposed for summary evaluation.
We also plan toinclude compressed sentences into the set of can-didate sentences to be selected as done by Yih etal.
(2007).
We also plan to design other decod-ing algorithms for text summarization (e.g., pipageapproach (Ageev and Sviridenko, 2004)).
As dis-cussed in Section 6.2, integration with similarity-based models is worth consideration.
We will in-corporate techniques for arranging sentences intoan appropriate order, while the current work con-cerns only selection.
Deshpande et al (2007) pro-posed a selection and ordering technique, which isapplicable only to the unit cost case such as selec-tion and ordering of words for title generation.
Weplan to refine their model so that it can be appliedto general text summarization.just trickily uses non-content words to increase the evalua-tion measure, disregarding the actual quality of summaries.788ReferencesAlexander A. Ageev and Maxim Sviridenko.
2004.
Pi-page rounding: A new method of constructing algo-rithms with proven performance guarantee.
Journalof Combinatorial Optimization, 8(3):307?328.JohnM.
Conroy, Judith D. Schlesinger, John Goldstein,and Dianne P. O?Leary.
2004.
Left-brain/right-brainmulti-document summarization.
In Proceedings ofthe Document Understanding Conference (DUC).Pawan Deshpande, Regina Barzilay, and David Karger.2007.
Randomized decoding for selection-and-ordering problems.
In Proceedings of the HumanLanguage Technologies Conference and the NorthAmerican Chapter of the Association for Compu-tational Linguistics Annual Meeting (HLT/NAACL),pages 444?451.DUC.
2003.
Document Understanding Conference.
InHLT/NAACL Workshop on Text Summarization.DUC.
2004.
Document Understanding Conference.
InHLT/NAACL Workshop on Text Summarization.Elena Filatova and Vasileios Hatzivassiloglou.
2004.A formal model for information selection in multi-sentence text extraction.
In Proceedings of the20th International Conference on ComputationalLinguistics (COLING), pages 397?403.Jade Goldstein, Vibhu Mittal, Jaime Carbonell, andMark Kantrowitz.
2000.
Multi-document summa-rization by sentence extraction.
In Proceedings ofANLP/NAACL Workshop on Automatic Summariza-tion, pages 40?48.Eduard Hovy, Chin-Yew Lin, Liang Zhou, and Ju-nichi Fukumoto.
2006.
Automated summarizationevaluation with basic elements.
In Proceedings ofthe Fifth International Conference on Language Re-sources and Evaluation (LREC).Juraj Hromkovic?.
2003.
Algorithmics for Hard Prob-lems.
Springer.Frederick Jelinek.
1969.
Fast sequential decoding al-gorithm using a stack.
IBM Journal of Research andDevelopment, 13:675?685.Samir Khuller, Anna Moss, and Joseph S. Naor.
1999.The budgeted maximum coverage problem.
Infor-mation Processing Letters, 70(1):39?45.Samir Khuller, Louiqa Raschid, and Yao Wu.
2006.LP randomized rounding for maximum coverageproblem and minimum set cover with thresholdproblem.
Technical Report CS-TR-4805, The Uni-versity of Maryland.Chin-Yew Lin and Eduard Hovy.
2003.
Auto-matic evaluation of summaries using n-gram co-occurrence statistics.
In Proceedings of the 2003Conference of the North American Chapter of theAssociation for Computational Linguistics on Hu-man Language Technology (HLT-NAACL?03), pages71?78.Chin-Yew Lin.
2004.
ROUGE: a package for auto-matic evaluation of summaries.
In Proceedings ofthe Workshop on Text Summarization Branches Out,pages 74?81.Andrew Makhorin, 2006.
Reference Manual of GNULinear Programming Kit, version 4.9.Inderjeet Mani.
2001.
Automatic Summarization.John Benjamins Publisher.Ryan McDonald.
2007.
A study of global inference al-gorithms in multi-document summarization.
In Pro-ceedings of the 29th European Conference on Infor-mation Retrieval (ECIR), pages 557?564.Martin F. Porter.
1980.
An algorithm for suffix strip-ping.
Program, 14(3):130?137.Dragomir R. Radev, Hongyan Jing, Ma?gorzata Stys?,and Daniel Tam.
2004.
Centroid-based summariza-tion of multiple documents.
Information ProcessingManagement, 40(6):919?938.Barry Schiffman, Ani Nenkova, and Kathleen McKe-own.
2002.
Experiments in multidocument sum-marization.
In Proceedings of the Second Interna-tional Conference on Human Language TechnologyResearch, pages 52?58.Dou Shen, Jian-Tao Sun, Hua Li, Qiang Yang, andZheng Chen.
2007.
Document summarization us-ing conditional random fields.
In Proceedings of the20th International Joint Conference on Artificial In-telligence (IJCAI), pages 2862?2867.Shiren Ye, Tat-Seng Chua, Min-Yen Kan, and LongQiu.
2007.
Document concept lattice for text un-derstanding and summarization.
Information Pro-cessing and Management, 43(6):1643?1662.Wen-Tau Yih, Joshua Goodman, Lucy Vanderwende,and Hisami Suzuki.
2007.
Multi-document summa-rization by maximizing informative content-words.In Proceedings of the 20th International Joint Con-ference on Artificial Intelligence (IJCAI), pages1776?1782.789
