Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1287?1292,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsMorphological Word-EmbeddingsRyan Cotterell1,2Department of Computer Science1Johns Hopkins University, USAryan.cotterell@jhu.eduHinrich Sch?tze2Center for Information and Language Processing2University of Munich, Germanyinquiries@cislmu.orgAbstractLinguistic similarity is multi-faceted.
For in-stance, two words may be similar with re-spect to semantics, syntax, or morphology in-ter alia.
Continuous word-embeddings havebeen shown to capture most of these shadesof similarity to some degree.
This work con-siders guiding word-embeddings with mor-phologically annotated data, a form of semi-supervised learning, encouraging the vectorsto encode a word?s morphology, i.e., wordsclose in the embedded space share morpho-logical features.
We extend the log-bilinearmodel to this end and show that indeed ourlearned embeddings achieve this, using Ger-man as a case study.1 IntroductionWord representation is fundamental for NLP.
Re-cently, continuous word-embeddings have gainedtraction as a general-purpose representation frame-work.
While such embeddings have proven them-selves useful, they typically treat words holistically,ignoring their internal structure.
For morphologi-cally impoverished languages, i.e., languages with alow morpheme-per-word ratio such as English, thisis often not a problem.
However, for the processingof morphologically-rich languages exploiting word-internal structure is necessary.Word-embeddings are typically trained to pro-duce representations that capture linguistic similar-ity.
The general idea is that words that are close inthe embedding space should be close in meaning.A key issue, however, is that meaning is a multi-faceted concept and thus there are multiple axes,along which two words can be similar.
For example,ice and cold are topically related, ice and fireare syntactically related as they are both nouns, andice and icy are morphologically related as theyare both derived from the same root.
In this work,we are interested in distinguishing between thesevarious axes and guiding the embeddings such thatsimilar embeddings are morphologically related.We augment the log-bilinear model (LBL) ofMnih and Hinton (2007) with a multi-task objective.In addition to raw text, our model is trained on acorpus annotated with morphological tags, encour-aging the vectors to encode a word?s morphology.To be concrete, the first task is language modeling?the traditional use of the LBL?and the second isakin to unigram morphological tagging.
The LBL,described in section 3, is fundamentally a languagemodel (LM)?word-embeddings fall out as low di-mensional representations of context used to pre-dict the next word.
We extend the model to jointlypredict the next morphological tag along with thenext word, encouraging the resulting embeddingsto encode morphology.
We present a novel met-ric and experiments on German as a case studythat demonstrates that our approach produces word-embeddings that better preserve morphological rela-tionships.2 Related WorkHere we discuss the role morphology has played inlanguage modeling and offer a brief overview of var-ious approaches to the larger task of computationalmorphology.2.1 Morphology in Language ModelingMorphological structure has been previously inte-grated into LMs.
Most notably, Bilmes and Kirch-1287ARTICLE ADJECTIVE NOUNART.DEF.NOM.SG.FEM ADJ.NOM.SG.FEM N.NOM.SG.FEMdie gr?
?te Stadtthe biggest cityTable 1: A sample German phrase in TIGER (Brantset al, 2004) annotation with an accompanying En-glish translation.
Each word is annotated with a com-plex morphological tag and its corresponding coarse-grained POS tag.
For instance, Stadt is annotated withN.NOM.SG.FEM indicating that it is a noun in the nomi-native case and also both singular and feminine.
Each tagis composed of meaningful sub-tag units that are sharedacross whole tags, e.g., the feature NOM fires on both ad-jectives and nouns.hoff (2003) introduced factored LMs, which effec-tively add tiers, allowing easy incorporation of mor-phological structure as well as part-of-speech (POS)tags.
More recently, M?ller and Sch?tze (2011)trained a class-based LM using common suffixes?often indicative of morphology?achieving state-of-the-art results when interpolated with a Kneser-Ney LM.
In neural probabilistic modeling, Luonget al (2013) described a recursive neural networkLM, whose topology was derived from the out-put of MORFESSOR, an unsupervised morpholog-ical segmentation tool (Creutz and Lagus, 2005).Similarly, Qiu et al (2014) augmented WORD2VEC(Mikolov et al, 2013) to embed morphs as well aswhole words?also taking advantage of MORFES-SOR.
LMs were tackled by dos Santos and Zadrozny(2014) with a convolutional neural network with ak-best max-pooling layer to extract character leveln-grams, efficiently inserting orthographic featuresinto the LM?use of the vectors in down-streamPOS tagging achieved state-of-the-art results in Por-tuguese.
Finally, most similar to our model, Bothaand Blunsom (2014) introduced the additive log-bilinear model (LBL++).
Best summarized as a neu-ral factored LM, the LBL++ created separate em-beddings for each constituent morpheme of a word,summing them to get a single word-embedding.2.2 Computational MorphologyOur work is also related to morphological tagging,which can be thought of as ultra-fine-grained POStagging.
For morphologically impoverished lan-guages, such as English, it is natural to considera small tag set.
For instance, in their univer-sal POS tagset, Petrov et al (2011) propose thecoarse tag NOUN to represent all substantives.
Ininflectionally-rich languages, like German, consid-ering other nominal attributes, e.g., case, gender andnumber, is also important.
An example of an anno-tated German phrase is found in table 1.
This oftenleads to a large tag set; e.g., in the morphological tagset of Haji?c (2000), English had 137 tags whereasmorphologically-rich Czech had 970 tags!Clearly, much of the information needed to deter-mine a word?s morphological tag is encoded in theword itself.
For example, the suffix ed is generallyindicative of the past tense in English.
However, dis-tributional similarity has also been shown to be animportant cue for morphology (Yarowsky and Wi-centowski, 2000; Schone and Jurafsky, 2001).
Muchas contextual signatures are reliably exploited ap-proximations to the semantics of the lexicon (Har-ris, 1954)?you shall know the meaning of the wordby the company it keeps (Firth, 1957)?they can besimilarly exploited for morphological analysis.
Thisis not an unexpected result?in German, e.g., wewould expect nouns that follow an adjective in thegenitive case to also be in the genitive case them-selves.
Much of what our model is designed to ac-complish is the isolation of the components of thecontextual signature that are indeed predictive ofmorphology.3 Log-Bilinear ModelThe LBL is a generalization of the well-known log-linear model.
The key difference lies in how itdeals with features?instead of making use of hand-crafted features, the LBL learns the features alongwith the weights.
In the language modeling setting,we define the following model,p(w | h)def=exp (s?
(w, h))?w?exp (s?
(w?, h)), (1)where w is a word, h is a history and s?is an energyfunction.
Following the notation of Mnih and Teh(2012), in the LBL we defines?
(w, h)def=(n?1?i=1Cirhi)Tqw+ bw, (2)1288where n ?
1 is history length and the parameters ?consist ofC, a matrix of context specific weights,R,the context word-embeddings, Q, the target word-embeddings, and b, a bias term.
Note that a sub-scripted matrix indicates a vector, e.g., qwindicatesthe target word-embedding for word w and rhiis theembedding for the ith word in the history.
The gra-dient, as in all energy-based models, takes the formof the difference between two expectations (LeCunet al, 2006).4 Morph-LBLWe propose a multi-task objective that jointly pre-dicts the next word w and its morphological tag tgiven a history h. Thus we are interested in a jointprobability distribution defined asp(w, t | h) ?
exp((fTtS +n?1?i=1Cirhi)Tqw+ bw),(3)where ftis a hand-crafted feature vector for a mor-phological tag t and S is an additional weight ma-trix.
Upon inspection, we see thatp(t | w, h) ?
exp(ftSTqw).
(4)Hence given a fixed embedding qwfor word w, wecan interpret S as the weights of a conditional log-linear model used to predict the tag t.Morphological tags lend themselves to easy fea-turization.
As shown in table 1, the morpholog-ical tag ADJ.NOM.SG.FEM decomposes into sub-tag units ADJ, NOM, SG and FEM.
Our model in-cludes a binary feature for each sub-tag unit in thetag set and only those present in a given tag fire;e.g., FADJ.NOM.SG.FEMis a vector with exactly fournon-zero components.4.1 Semi-Supervised LearningIn the fully supervised case, the method we proposedabove requires a corpus annotated with morpholog-ical tags to train.
This conflicts with a key use caseof word-embeddings?they allow the easy incorpo-ration of large, unannotated corpora into supervisedtasks (Turian et al, 2010).
To resolve this, we trainour model on a partially annotated corpus.
The keyidea here is that we only need a partial set of la-beled data to steer the embeddings to ensure they 40  30  20  10 0 10 20 30 40 50 40 30 20 1001020304050sprichtschreibtbleibtgehtsprachschriebbliebginggeschriebengegangengesprochenschweresozialeerfolgreichekaltebilligeszweitesaltessozialenerfolgreichenkaltenKlangKonfliktFriedenZweckKrankheitFrauStundeFamilieHausKindLandZielKonfliktsFriedensHausesLandesStundenFrauenFamilienPaulHaraldNasdaqAndreashierwiederdortalterneuerzweiterFigure 1: Projections of our 100 dimensional embeddingsonto R2through t-SNE (Van der Maaten and Hinton,2008).
Each word is given a distinct color determinedby its morphological tag.
We see clear clusters reflect-ing morphological tags and coarse-grained POS?verbsare in various shades of green, adjectives in blue, adverbsin grey and nouns in red and orange.
Moreover, we seesimilarity across coarse-grained POS tags, e.g., the gen-itive adjective sozialen lives near the genitive nounFriedens, reflecting the fact that ?sozialen Friedens?
?social peace?
is a frequently used German phrase.capture morphological properties of the words.
Wemarginalize out the tags for the subset of the datafor which we do not have annotation.5 EvaluationIn our evaluation, we attempt to intrinsically deter-mine whether it is indeed true that words similar inthe embedding space are morphologically related.Qualitative evaluation, shown in figure 1, indicatesthat this is the case.5.1 MorphoDistWe introduce a new evaluation metric formorphologically-driven embeddings to quanti-tatively score models.
Roughly, the question wewant to evaluate is: are words that are similar inthe embedded space also morphologically related?Given a word w and its embedding qw, let Mwbe the set of morphological tags associated with wrepresented by bit vectors.
This is a set becausewords may have several morphological parses.
Our1289measure is then defined below,MORPHODIST(w)def= ??w??Kwminmw,mw?dh(mw,mw?
),where mw?
Mw, mw??
Mw?, dhis the Ham-ming distance and Kwis a set of words close to w inthe embedding space.
We are given some freedomin choosing the set Kw?in our experiments we takeKwto be the k-nearest neighbors (k-NN) in the em-bedded space using cosine distance.
We report per-formance under this evaluation metric for various k.Note that MORPHODIST can be viewed as a soft ver-sion of k-NN?we measure not just whether a wordhas the same morphological tag as its neighbors, butrather has a similar morphological tag.Metrics similar to MORPHODIST have been ap-plied in the speech recognition community.
For ex-ample, Levin et al (2013) had a similar motivationfor their evaluation of fixed-length acoustic embed-dings that preserve linguistic similarity.6 Experiments and ResultsTo show the potential of our approach, we chose toperform a case study on German, a morphologically-rich language.
We conducted experiments on theTIGER corpus of newspaper German (Brants et al,2004).
To the best of our knowledge, no previ-ous word-embedding techniques have attempted toincorporate morphological tags into embeddings ina supervised fashion.
We note again that therehas been recent work on incorporating morpholog-ical segmentations into embeddings?generally in apipelined approach using a segmenter, e.g., MOR-FESSOR, as a preprocessing step, but we distinguishour model through its use of a different view on mor-phology.We opted to compare Morph-LBL with twofully unsupervised models: the original LBL andWORD2VEC (code.google.com/p/word2vec/,Mikolov et al (2013)).
All models were trained onthe first 200k words of the train split of the TIGERcorpus; Morph-LBL was given the correct morpho-logical annotation for the first 100k words.
TheLBL and Morph-LBL models were implemented inPython using THEANO (Bastien et al, 2012).
Allvectors had dimensionality 200.
We used the Skip-Gram model of the WORD2VEC toolkit with con-text n = 5.
We initialized parameters of LBLMorph-LBL LBL WORD2VECAll Types 81.5% 22.1% 10.2%No Tags 44.8% 15.3% 14.8%Table 2: We examined to what extent the individual em-beddings store morphological information.
To quantifythis, we treated the problem as supervised multi-wayclassification with the embedding as the input and themorphological tag as the output to predict.
Note that ?AllTypes?
refers to all types in the training corpus and ?NoTags?
refers to the subset of types, whose morphologicaltag was not seen by Morph-LBL at training time.and Morph-LBL randomly and trained them usingstochastic gradient descent (Robbins and Monro,1951).
We used a history size of n = 4.6.1 Experiment 1: Morphological ContentWe first investigated whether the embeddingslearned by Morph-LBL do indeed encode morpho-logical information.
For each word, we selectedthe most frequently occurring morphological tag forthat word (ties were broken randomly).
We thentreated the problem of labeling a word-embeddingwith its most frequent morphological tag as a multi-way classification problem.
We trained a k nearestneighbors classifier where k was optimized on de-velopment data.
We used the scikit-learn li-brary (Pedregosa et al, 2011) on all types in the vo-cabulary with 10-fold cross-validation, holding out10% of the data for testing at each fold and an addi-tional 10% of training as a development set.
Theresults displayed in table 2 are broken down bywhether MorphLBL observed the morphological tagat training time or not.
We see that embeddings fromMorph-LBL do store the proper morphological anal-ysis at a much higher rate than both the vanilla LBLand WORD2VEC.Word-embeddings, however, are often trained onmassive amounts of unlabeled data.
To this end,we also explored on how WORD2VEC itself encodesmorphology, when trained on an order of magnitudemore data.
Using the same experimental setup asabove, we trained WORD2VEC on the union of theTIGER German corpus and German section of Eu-roparl (Koehn, 2005) for a total of ?
45 million to-kens.
Looking only at those types found in TIGER,we found that the k-NN classifier predicted the cor-1290?0.06?0.05?0.04?0.03?0.02?0.010.00MorphoSimk = 5?0.06?0.05?0.04?0.03?0.02?0.010.00k = 10M-LBL LBL word2vec?0.06?0.05?0.04?0.03?0.02?0.010.00MorphoSimk = 25M-LBL LBL word2vec?0.06?0.05?0.04?0.03?0.02?0.010.00k = 50Figure 2: Results for the MORPHODIST measure for k ?
{5, 10, 25, 50}.
Lower MORPHODIST values are better?they indicate that the nearest neighbors of each word arecloser morphologically.rect tag with ?
22% accuracy (not shown in the ta-ble).6.2 Experiment 2: MORPHODISTWe also evaluated the three types of embeddings us-ing the MORPHODIST metric introduced in section5.1.
This metric roughly tells us how similar eachword is to its neighbors, where distance is measuredin the Hamming distance between morphologicaltags.
We only evaluated on words that MorphLBLdid not observe at training time to get a fair idea ofhow well our model has managed to encode mor-phology purely from the contextual signature.
Fig-ure 2 reports results for k ?
{5, 10, 25, 50} nearestneighbors.
We see that the values of k studied donot affect the metric?the closest 5 words are aboutas similar as the closest 50 words.
We see again thatthe Morph-LBL embeddings generally encode mor-phology better than the baselines.6.3 DiscussionThe superior performance of Morph-LBL over boththe original LBL and WORD2VEC under both eval-uation metrics is not surprising as we provide ourmodel with annotated data at training time.
That theLBL outperforms WORD2VEC is also not surpris-ing.
The LBL looks at a local history thus making itmore amenable to learning syntactically-aware em-beddings than WORD2VEC, whose skip-grams oftenlook at non-local context.What is of interest, however, is Morph-LBL?sability to robustly maintain morphological relation-ships only making use of the distributional signature,without word-internal features.
This result showsthat in large corpora, a large portion of morphol-ogy can be extracted through contextual similarity.7 Conclusion and Future WorkWe described a new model, Morph-LBL, forthe semi-supervised induction of morphologicallyguided embeddings.
The combination of morpho-logically annotated data with raw text allows us totrain embeddings that preserve morphological rela-tionships among words.
Our model handily outper-formed two baselines trained on the same corpus.While contextual signatures provide a strong cuefor morphological proximity, orthographic featuresare also requisite for a strong model.
Consider thewords loving and eating.
Both are likely to oc-cur after is/are and thus their local contextual sig-natures are likely to be similar.
However, perhaps anequally strong signal is that the two words end in thesame substring ing.
Future work will handle suchintegration of character-level features.We are interested in the application of our em-beddings to morphological tagging and other tasks.Word-embeddings have proven themselves as usefulfeatures in a variety of tasks in the NLP pipeline.Morphologically-driven embeddings have the po-tential to leverage raw text in a way state-of-the-art morphological taggers cannot, improving tag-ging performance downstream.AcknowledgementsThis material is based upon work supported bya Fulbright fellowship awarded to the first authorby the German-American Fulbright Commissionand the National Science Foundation under GrantNo.
1423276.
The second author was supportedby Deutsche Forschungsgemeinschaft (grant DFGSCHU 2246/10-1).
We thank Thomas M?ller forseveral insightful discussions on morphological tag-ging and Jason Eisner for discussions about exper-imental design.
Finally, we thank the anonymousreviewers for their many helpful comments.1291ReferencesFr?d?ric Bastien, Pascal Lamblin, Razvan Pascanu,James Bergstra, Ian J. Goodfellow, Arnaud Berg-eron, Nicolas Bouchard, and Yoshua Bengio.
2012.Theano: new features and speed improvements.
DeepLearning and Unsupervised Feature Learning NIPS2012 Workshop.Jeff A Bilmes and Katrin Kirchhoff.
2003.
Factoredlanguage models and generalized parallel backoff.
InHLT-NAACL.Jan A. Botha and Phil Blunsom.
2014.
CompositionalMorphology for Word Representations and LanguageModelling.
In ICML.Sabine Brants, Stefanie Dipper, Peter Eisenberg, Sil-via Hansen-Schirra, Esther K?nig, Wolfgang Lezius,Christian Rohrer, George Smith, and Hans Uszkor-eit.
2004.
TIGER: Linguistic interpretation of a Ger-man corpus.
Research on Language and Computation,2(4):597?620.Mathias Creutz and Krista Lagus.
2005.
UnsupervisedMorpheme Segmentation and Morphology Inductionfrom Text Corpora using Morfessor.
Publications inComputer and Information Science, Report A, 81.C?cero Nogueira dos Santos and Bianca Zadrozny.
2014.Learning character-level representations for part-of-speech tagging.
In ICML.John Rupert Firth.
1957.
Papers in linguistics,1934?1951.
Oxford University Press.Jan Haji?c.
2000.
Morphological tagging: Data vs. dictio-naries.
In HLT-NAACL.Zellig Harris.
1954.
Distributional Structure.
Word.Philipp Koehn.
2005.
Europarl: A parallel corpus for sta-tistical machine translation.
In MT Summit, volume 5,pages 79?86.Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato,and F Huang.
2006.
A Tutorial on Energy-basedLearning.
Predicting Structured Data.Keith Levin, Katharine Henry, Aren Jansen, and KarenLivescu.
2013.
Fixed-dimensional acoustic embed-dings of variable-length segments in low-resource set-tings.
In Automatic Speech Recognition and Under-standing (ASRU), pages 410?415.
IEEE.Minh-Thang Luong, Richard Socher, and C Manning.2013.
Better word representations with recursive neu-ral networks for morphology.
In CoNLL, volume 104.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient estimation of word represen-tations in vector space.
In ICRL.Andriy Mnih and Geoffrey Hinton.
2007.
Three newgraphical models for statistical language modelling.
InICML.Andriy Mnih and Yee Whye Teh.
2012.
A fast and sim-ple algorithm for training neural probabilistic languagemodels.
In ICML.Thomas M?ller and Hinrich Sch?tze.
2011.
Improvedmodeling of out-of-vocabularly words using morpho-logical classes.
In ACL.Fabian Pedregosa, Ga?l Varoquaux, Alexandre Gram-fort, Vincent Michel, Bertrand Thirion, Olivier Grisel,Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vin-cent Dubourg, et al 2011.
Scikit-learn: Machinelearning in Python.
The Journal of Machine LearningResearch, 12:2825?2830.Slav Petrov, Dipanjan Das, and Ryan McDonald.
2011.A universal part-of-speech tagset.
In LREC.Siyu Qiu, Qing Cui, Jiang Bian, Bin Gao, and Tie-YanLiu.
2014.
Co-learning of word representations andmorpheme representations.
In COLING.Herbert Robbins and Sutton Monro.
1951.
A StochasticApproximation Method.
The Annals of MathematicalStatistics, pages 400?407.Patrick Schone and Daniel Jurafsky.
2001.
Knowledge-free induction of inflectional morphologies.
In ACL.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: A simple and general methodfor semi-supervised learning.
In ACL.Laurens Van der Maaten and Geoffrey Hinton.
2008.Visualizing Data using t-SNE.
Journal of MachineLearning Research, 9(2579-2605):85.David Yarowsky and Richard Wicentowski.
2000.
Min-imally supervised morphological analysis by multi-modal alignment.
In ACL.1292
