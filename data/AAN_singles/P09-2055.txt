Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 217?220,Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLPiChi: a bilingual dictionary generating toolVarga Istv?nYamagata University,Graduate School of Science and Engineeringdyn36150@dip.yz.yamagata-u.ac.jpYokoyama ShoichiYamagata University,Graduate School of Science and Engineeringyokoyama@yz.yamagata-u.ac.jpAbstractIn this paper we introduce a bilingual diction-ary generating tool that does not use any largebilingual corpora.
With this tool we implementour novel pivot based bilingual dictionarygeneration method that uses mainly theWordNet of the pivot language to build a newbilingual dictionary.
We propose the usage ofWordNet for good accuracy, introducing also adouble directional selection method with localthresholds to maximize recall.1 IntroductionBilingual dictionaries are an essential, perhaps evenindispensable tool not only as resources for ma-chine translation, but also in every day activities orlanguage education.
While such dictionaries areavailable to and from numerous widely used lan-guages, less represented language pairs have rarelya reliable dictionary with good coverage.
The needfor bilingual dictionaries for these less commonlanguage pairs is increasing, but qualified humanresources are scarce.
Considering that in these con-ditions manual compilation is highly costly, alter-native methods are imperative.Pivot language based bilingual dictionary gen-eration is one plausible such alternative (Tanakaand Umemura, 1994; Sj?bergh, 2005; Shirai andYamamoto, 2001; Bond and Ogura, 2007).
Thesemethods do not use large bilingual corpora, thusbeing suitable for low-resourced languages.Our paper presents iChi, the implementationof our own method, an easy-to-use, customizabletool that generates a bilingual dictionary.The paper is structured as follows: first webriefly describe the methodological backgroundof our tool, after which we describe its basicfunctions, concluding with discussions.
Thor-ough description and evaluation, including com-parative analysis, are available in Varga and Yo-koyama (2009).2 Methodological background2.1 Pivot based dictionary generationPivot language based bilingual dictionary gen-eration methods rely on the idea that the lookupof a word in an uncommon language through athird, intermediated language can be automated.Bilingual dictionaries to a third, intermediatelanguage are used to link the source and targetwords.
The pivot language translations of thesource and target head words are compared, thesuitability of the source-target word pair beingestimated based on the extent of the commonelements.There are two known problems of conven-tional pivot methods.
First, a global threshold isused to determine correct translation pairs.
How-ever, the scores highly depend on the entry itselfor the number of translations in the intermediatelanguage, therefore there is a variance in whatthat score represents.
Second, current methodsperform a strictly lexical overlap of the source-intermediate and target-intermediate entries.Even if the translations from the source and tar-get languages are semantically transferred to theintermediate language, lexically it is rarely thecase.
However, due to the different word-usageor paraphrases, even semantically identical orvery similar words can have different definitionsin different dictionaries.
As a result, because ofthe lexical characteristic of their overlap, currentmethods cannot identify the differences betweentotally different definitions resulted by unrelatedconcepts, and differences in only nuances re-sulted by lexicographers describing the sameconcept, but with different words.2.2 Specifics of our methodTo overcome the limitations, namely low preci-sion of previous pivot methods, we expand thetranslations in the intermediate language using217information extracted from WordNet (Miller et.al., 1990).
We use the following information:sense description, synonymy, antonymy and se-mantic categories, provided by the tree structureof nouns and verbs.To improve recall, we introduce bidirectionalselection.
As we stated above, the global thresh-old eliminates a large number of good translationpairs, resulting in a low recall.
As a solution, wecan group the translations that share the samesource or target entry, and set local thresholdsfor each head word.
For example, for a sourcelanguage head word entry_source there could bemultiple target language candidates:  en-try_target1, ?
,entry_targetn.
If the top scoringentry_targetk candidates are selected, we ensurethat at least one translation will be available forentry_source, maintaining a high recall.
Since wecan group the entries in the source language andtarget language as well, we perform this selectiontwice, once in each direction.
Local thresholdsdepend on the top scoring entry_target, being setto maxscore?c.
Constant c varies between 0 and 1,allowing a small window for not maximum, buthigh scoring candidates.
It is language and selec-tion method dependent (See 3.2 for details).2.3 Brief method descriptionFirst, using the source-pivot and pivot-target dic-tionaries, we connect the source (s) and target (t)entries that share at least one common translationin the intermediate (i) language.
We considereach such source-target pair a translation candi-date.
Next we eliminate erroneous candidates.We examine the translation candidates one byone, looking up the source-pivot and target-pivotdictionaries, comparing pivot language transla-tions.
There are six types of translations that welabel A-F and explain below as follows.First, we select translation candidates whosetranslations into the intermediate language matchperfectly (type A translations).For most words WordNet offers sense descrip-tion in form of synonyms for most of its senses.For a given translation candidate (s,t) we look upthe source-pivot and target-pivot translations(s?I={s?i1,?,s?in}, t?I={t?i1,?,t?im}).We select the elements that are common in thetwo definitions (I?=(s?I)?
(t?I)) and we at-tempt to identify their respective senses fromWordNet (sns(I?
)), comparing each synonym inthe WordNet?s synonym description with eachword from the pivot translations.
As a result, wearrive at a certain set of senses from the source-pivot definitions (sns((s?I?))
and target-pivotdefinitions (sns((t?I?)).
We mark scoreB(s,t) theJaccard coefficient of these two sets.
Scores thatpass a global threshold (0.1) are selected astranslation pairs.
Since synonymy information isavailable for nouns (N), verbs (V), adjectives (A)and adverbs (R), four separate scores are calcu-lated for each POS (type B).
( ) ( ) ( )( ) ( )''''max,' itsnsissnsitsnsissnstsscoreItIsiB??????=???
I(1)We expand the source-to-pivot and target-to-pivot definitions with information from WordNet(synonymy, antonymy and semantic category).The similarity of the two expanded pivot lan-guage descriptions gives a better indication onthe suitability of the translation candidate.
Sincethe same word or concept?s translations into thepivot language also share the same semanticvalue, the extension with synonyms(ext(l?i)=(l?i)?syn(l?i), where l={s,t}) theextended translation should share more commonelements (type C).In case of antonymy, we expand the initialdefinitions with the antonyms of the antonyms(ext(l?i)=(l?i)?ant(ant(l?i)), where l={s,t}).This extension is different from the synonymyextension, in most cases the resulting set ofwords being considerably larger (type D).Synonymy and antonymy information areavailable for nouns, verbs, adjectives and ad-verbs, thus four separate scores are calculated foreach POS.Semantic categories are provided by the treestructure (hypernymy/hyponymy) of nouns andverbs of WordNet.
We transpose each entry fromthe pivot translations to its semantic category(ext(l?i)=(l?i)?semcat(l?i), where l={s,t}).We assume that the correct translation pairsshare a high percentage of semantic categories.Local thresholds are set based on the bestscoring candidate for a given entry.
The thresh-olds were maxscore?0.9 for synonymy and an-tonymy; and maxscore?0.8 for the semantic cate-gories (see ?3.2 for details).
( ) ( ) ( )( ) ( )itextisextitextisexttsscore EDC?????
?=,,,(2)For a given entry, the three separate candidatelists of type C, D and E selection methods re-sulted in slightly different results.
The goodtranslations were among the top scoring ones, butnot always scoring best.
To correct this fault, acombined selection method is performed com-bining these lists.
For every translation candidatewe select the maximum score (scorerel(s,t)) from218the several POS (noun, verb, adjective and ad-verb for synonymy and antonymy relations; nounand verb for semantic category) based scores,multiplied by a multiplication factor (mfactor).This factor varies between 0 and 1, awarding thecandidates that were selected both times duringthe double directional selection; and punishingwhen selection was made only in a single direc-tion.
c1, c2 and c3 are adjustable language de-pendent constants, the defaults being 1, 0.5 and0.8, respectively (type F).
( ) ( )( )( )( )( )?
????????
?+?+=rel relrelF tsmfactorcctsscorectsscore,,max,321(3)2.4 EvaluationWe generated a Japanese-Hungarian dictionaryusing selection methods A, B and F; with C, Dand E contributing indirectly through F.(a) Recall evaluationWe used a Japanese frequency dictionary that wegenerated from the Japanese EDR corpus (Isa-hara, 2007) to weight each Japanese entry.
Set-ting the standard to the frequency dictionary (itsrecall value being 100), we automatically searcheach entry from the frequency dictionary, verify-ing whether or not it is included in the bilingualdictionary.
If it is recalled, we weight it with itsfrequency from the frequency dictionary.Our method maintains the recall value of theinitial translation candidates, owing to the bidi-rectional selection method with local thresholds.However, the recall value of a manually createdJapanese-English dictionary is higher than anyautomatically generated dictionary?s value (Ta-ble 1).method recallour method 51.68initial candidates 51.68Japanese-English(*) 73.23Table 1: Recall evaluation results (* marks a manu-ally created dictionary)(b) 1-to-1 precision evaluationWe evaluated 2000 randomly selected translationpairs, manually scoring them as correct (thetranslation conveys the same meaning, or themeanings are slightly different, but in a certaincontext the translation is possible: 79.15%), un-decided (the translation pair?s semantic value issimilar, but a translation based on them would befaulty: 6.15%) or wrong (the translation pair?stwo entries convey a different meaning: 14.70%).
(c) 1-to-multiple evaluationWith 1-to-multiple evaluation we quantify thetrue reliability of the dictionary: when looking upthe meanings or translations of a certain key-word, the user, whether he?s a human or a ma-chine, expects all translations to be accurate.
Weevaluated 2000 randomly selected Japanese en-tries from the initial translation candidates, scor-ing all Hungarian translations as correct (alltranslations are correct: 71.45%), acceptable (thegood translations are predominant, but there areup to 2 erroneous translations: 13.85%), wrong(the number or wrong translations exceeds 2:14.70%).3 iChiiChi is an implementation of our method.
Pro-grammed in Java, it is a platform-independenttool with a user friendly graphical interface (Im-age 1).
Besides the MySql database it consists of:iChi.jar (java executable), iChi.cfg (configura-tion file), iChi.log (log file) and iChip.jar (pa-rameter estimation tool).
The major functions ofiChi are briefly explained below.Image 1: User interface of iChi3.1 ResourcesThe two bilingual dictionaries used as resourcesare text files, with a translation pair in each line:source entry 1@pivot entry 1source entry 2@pivot entry 2The location of the pivot language?s WordNetalso needs to be specified.
All paths are stored inthe configuration file.3.2 Parameter settingsiChip.jar estimates language dependent parame-ters needed for the selection methods.
Its singleargument is a text file that contains marked (cor-rect: $+ or incorrect: $-) translation pairs:219$+source entry 1@correct target entry 1$-source entry 2@incorrect target entry 2The parameter estimation tool experimentswith various threshold settings on the same (cor-rect or incorrect) source entries.
For example,with Hungarian-Japanese we considered alltranslation candidates whose Hungarian entrystarts with ?zs?
(IPA: ?).
133 head words total-ling 515 translation candidates comprise this set,273 entries being marked as correct.
iChip ex-perimented with a number of thresholds to de-termine which ones provide with the best F-scores, e.g.
retain most marked correct transla-tions (Table 2).
The F-scores were determined asfollows: for example using synonymy informa-tion (type C) in case of threshold=0.85%, 343 ofthe 515 translation pairs were above the thresh-old.
Among these, 221 were marked as correct,thus the precision being 221/343?100=64.43 andthe recall being 221/273?100=80.95.
F-score isthe harmonic mean of precision and recall (71.75in this case).threshold value (%) selectiontype 0.75 0.80 0.85 0.90 0.95C 70.27 70.86 71.75 72.81 66.95D 69.92 70.30 70.32 70.69 66.66E 73.71 74.90 72.52 71.62 65.09F 78.78 79.07 79.34 78.50 76.94Table 2: Selection type F-scores with varying thresh-olds (best scores in bold)The output is saved into the configuration file.If no parameter estimation data is available, theparameters estimated using Hungarian-Japaneseare used as default.3.3 Save settingsThe generated source-target dictionary is savedinto a text file that uses the same format de-scribed in ?3.1.
The output can be customized bychoosing the desired selection methods.
The de-fault value is a dictionary with selection types A,B and F; selection types C, D and E are usedonly indirectly with type F.3.4 TasksThe tasks are run sequentially, every step beingsaved in the internal database, along with beinglogged into the log file.4 DiscussionIf heavily unbalanced resources dictionaries areused, due to the bidirectional selection methodmany erroneous entries will be generated.
If onepolysemous pivot entry has multiple translationsinto the source, but only some of them are trans-lated into the target languages, unique, but incor-rect source-target pairs will be generated.
Forexample, with an English pivoted dictionary thathas multiple translation of ?bank?
onto the source(?financial institution?, ?river bank?
), but onlyone into the target language (?river bank?
), theincorrect source(?financial institution?
)-target(?river bank?)
pair will be generated, sincetarget(?river bank?)
has no other alternative.Thorough discussion on recall and precisionproblems concerning the methodology of iChi,are available in Varga and Yokoyama (2009).5 ConclusionsIn this paper we presented iChi, a user friendlytool that uses two dictionaries into a third, inter-mediate language together with the WordNet ofthat third language to generate a new dictionary.We briefly described the methodology, togetherwith the basic functions.
The tool is freely avail-able online (http://mj-nlp.homeip.net/ichi).ReferencesBond, F., Ogura, K. 2007.
Combining linguistic re-sources to create a machine-tractable Japanese-Malay dictionary, Language Resources andEvaluation, 42(2), pp.
127-136.Breen, J.W.
1995.
Building an Electric Japanese-English Dictionary, Japanese Studies Associationof Australia Conference, Brisbane, Queensland,Australia.Isahara, H. (2007).
EDR Electronic Dictionary ?
pre-sent status (EDR ????????
), NICT-EDRsymposium, pp.
1-14.
(in Japanese)Miller G.A., Beckwith R., Fellbaum C., Gross D.,Miller K.J.
(1990).
Introduction to WordNet: AnOnline Lexical Database, Int J Lexicography 3(4),pp.
235-244.Sj?bergh, J.
2005.
Creating a free Japanese-Englishlexicon, Proceedings of PACLING, pp.
296-300.Shirai, S., Yamamoto, K. 2001.
Linking Englishwords in two bilingual dictionaries to generate an-other pair dictionary, ICCPOL-2001, pp.
174-179.Tanaka, K., Umemura, K. 1994.
Construction of abilingual dictionary intermediated by a third lan-guage, Proceedings of COLING-94, pp.
297-303.Varga, I., Yokoyama, S. 2009.
Bilingual dictionarygeneration for low-resourced language pairs, Pro-ceedings of EMNLP 2009.220
