Coling 2010: Poster Volume, pages 374?382,Beijing, August 2010Integrating N-best SMT Outputs into a TM SystemYifan He Yanjun Ma Andy Way Josef van GenabithCentre for Next Generation LocalisationSchool of ComputingDublin City University{yhe,yma,away,josef}@computing.dcu.ieAbstractIn this paper, we propose a novel frame-work to enrich Translation Memory (TM)systems with Statistical Machine Trans-lation (SMT) outputs using ranking.
Inorder to offer the human translators mul-tiple choices, instead of only using thetop SMT output and top TM hit, wemerge the N-best output from the SMTsystem and the k-best hits with highestfuzzy match scores from the TM sys-tem.
The merged list is then ranked ac-cording to the prospective post-editing ef-fort and provided to the translators to aidtheir work.
Experiments show that ourranked output achieve 0.8747 precision attop 1 and 0.8134 precision at top 5.
Ourframework facilitates a tight integrationbetween SMT and TM, where full advan-tage is taken of TM while high qualitySMT output is availed of to improve theproductivity of human translators.1 IntroductionTranslation Memories (TM) are databases thatstore translated segments.
They are often used toassist translators and post-editors in a ComputerAssisted Translation (CAT) environment by re-turning the most similar translated segments.
Pro-fessional post-editors and translators have longbeen relying on TMs to avoid duplication of workin translation.With the rapid development in statistical ma-chine translation (SMT), MT systems are begin-ning to generate acceptable translations, espe-cially in domains where abundant parallel corporaexist.
It is thus natural to ask if these translationscan be utilized in some way to enhance TMs.However advances in MT are being adoptedonly slowly and sometimes somewhat reluctantlyin professional localization and post-editing envi-ronments because of 1) the usefulness of the TM,2) the investment and effort the company has putinto TMs, and 3) the lack of robust SMT confi-dence estimation measures which are as reliableas fuzzy match scores (cf.
Section 4.1.2) used inTMs.
Currently the localization industry relies onTM fuzzy match scores to obtain both a good ap-proximation of post-editing effort and an estima-tion of the overall translation cost.In a forthcoming paper, we propose a trans-lation recommendation model to better integrateMT outputs into a TM system.
Using a binaryclassifier, we only recommend an MT output tothe TM-user when the classifier is highly confi-dent that it is better than the TM output.
In thisframework, post-editors continue to work with theTM while benefiting from (better) SMT outputs;the assets in TMs are not wasted and TM fuzzymatch scores can still be used to estimate (the up-per bound of) post-editing labor.In the previous work, the binary predictorworks on the 1-best output of the MT and TM sys-tems, presenting either the one or the other to thepost-editor.
In this paper, we develop the idea fur-ther by moving from binary prediction to ranking.We use a ranking model to merge the k-best listsof the two systems, and produce a ranked merged374list for post-editing.
As the list is an enriched ver-sion of the TM?s k-best list, the TM related assetsare better preserved and the cost estimation is stillvalid as an upper bound.More specifically, we recast SMT-TM integra-tion as a ranking problem, where we apply theRanking SVM technique to produce a ranked listof translations combining the k-best lists of boththe MT and the TM systems.
We use features in-dependent of the MT and TM systems for rank-ing, so that outputs from MT and TM can havethe same set of features.
Ideally the transla-tions should be ranked by their associated post-editing efforts, but given the very limited amountsof human annotated data, we use an automaticMT evaluation metric, TER (Snover et al, 2006),which is specifically designed to simulate post-editing effort to train and test our ranking model.The rest of the paper is organized as follows:we first briefly introduce related research in Sec-tion 2, and review Ranking SVMs in Section 3.The formulation of the problem and experimentswith the ranking models are presented in Sections4 and 5.
We analyze the post-editing effort ap-proximated by the TER metric in Section 6.
Sec-tion 7 concludes and points out avenues for futureresearch.2 Related WorkThere has been some work to help TM users toapply MT outputs more smoothly.
One strand isto improve the MT confidence measures to bet-ter predict post-editing effort in order to obtain aquality estimation that has the potential to replacethe fuzzy match score in the TM.
To the best ofour knowledge, the first paper in this area is (Spe-cia et al, 2009a), which uses regression on boththe automatic scores and scores assigned by post-editors.
The method is improved in (Specia etal., 2009b), which applies Inductive ConfidenceMachines and a larger set of features to modelpost-editors?
judgment of the translation qualitybetween ?good?
and ?bad?, or among three levelsof post-editing effort.Another strand is to integrate high confidenceMT outputs into the TM, so that the ?good?
TMentries will remain untouched.
In our forthcomingpaper, we recommend SMT outputs to a TM userwhen a binary classifier predicts that SMT outputsare more suitable for post-editing for a particularsentence.The research presented here continues the lineof research in the second strand.
The differenceis that we do not limit ourselves to the 1-best out-put but try to produce a k-best output in a rank-ing model.
The ranking scheme also enables usto show all TM hits to the user, and thus furtherprotects the TM assets.There has also been work to improve SMT us-ing the knowledge from the TM.
In (Simard andIsabelle, 2009), the SMT system can produce abetter translation when there is an exact or closematch in the corresponding TM.
They use regres-sion Support Vector Machines to model the qual-ity of the TM segments.
This is also related toour work in spirit, but our work is in the oppositedirection, i.e.
using SMT to enrich TM.Moreover, our ranking model is related toreranking (Shen et al, 2004) in SMT as well.However, our method does not focus on produc-ing better 1-best translation output for an SMTsystem, but on improving the overall quality of thek-best list that TM systems present to post-editors.Some features in our work are also different in na-ture to those used in MT reranking.
For instancewe cannot use N-best posterior scores as they donot make sense for the TM outputs.3 The Support Vector Machines3.1 The SVM ClassifierClassical SVMs (Cortes and Vapnik, 1995) arebinary classifiers that classify an input instancebased on decision rules which minimize the reg-ularized error function in (Eq.
1):minw,b,?12wTw + Cl?i=1?isubject to: yi(wT xi + b) > 1 ?
?i?i > 0(1)where (xi, yi) ?
Rn ?
{1,?1} are l training in-stances.
w is the weight vector, ?
is the relaxationvariable and C > 0 is the penalty parameter.3.2 Ranking SVM for SMT-TM IntegrationThe SVM classification algorithm is extended tothe ranking case in (Joachims, 2002).
For a cer-375tain group of instances, the Ranking SVM aimsat producing a ranking r that has the maximumKendall?s ?
coefficient with the the gold standardranking r?.Kendall?s ?
measures the relevance of two rank-ings: ?
(ra, rb) = P?QP+Q , where P and Q arethe amount of concordant and discordant pairs inra and rb.
In practice, this is done by buildingconstraints to minimize the discordant pairs Q.Following the basic idea, we show how RankingSVM can be applied to MT-TM integration as fol-lows.Assume that for each source sentence s, wehave a set of outputs from MT, M and a set ofoutputs from TM, T. If we have a ranking r(s)over translation outputs M?T where for eachtranslation output d ?
M?T, (di, dj) ?
r(s) iffdi <r(s) dj , we can rewrite the ranking constraintsas optimization constraints in an SVM, as in Eq.
(2).minw,b,?12wTw + C?
?subject to:?
(di, dj) ?
r(s1) : w(?
(s1, di)?
?
(s1, dj)) > 1 ?
?i,j,1...?
(di, dj) ?
r(sn) : w(?
(sn, di)?
?
(sn, dj)) > 1?
?i,j,n?i,j,k > 0(2)where ?
(sn, di) is a feature vector of translationoutput di given source sentence sn.
The RankingSVM minimizes the discordant number of rank-ings with the gold standard according to Kendall?s?
.When the instances are not linearly separable,we use a mapping function ?
to map the featuresxi (?
(sn, di) in the case of ranking) to high di-mensional space, and solve the SVMwith a kernelfunction K in where K(xi, xj) = ?(xi)T?
(xj).We perform our experiments with the RadialBasis Function (RBF) kernel, as in Eq.
(3).K(xi, xj) = exp(?
?||xi ?
xj ||2), ?
> 0 (3)4 The Ranking-based Integration ModelIn this section we present the Ranking-basedSMT-TM integration model in detail.
We first in-troduce the k-best lists in MT (called N-best list)and TM systems (called m-best list in this section)and then move on to the problem formulation andthe feature set.4.1 K-Best Lists in SMT and TM4.1.1 The SMT N-best ListThe N-best list of the SMT system is generatedduring decoding according to the internal featurescores.
The features include language and transla-tion model probabilities, reordering model scoresand a word penalty.4.1.2 The TM M-Best List and the FuzzyMatch ScoreThe m-best list of the TM system is gener-ated in descending fuzzy match score.
The fuzzymatch score (Sikes, 2007) uses the similarity ofthe source sentences to predict a level to which atranslation is reusable or editable.The calculation of fuzzy match scores is one ofthe core technologies in TM systems and variesamong different vendors.
We compute fuzzymatch cost as the minimum Edit Distance (Lev-enshtein, 1966) between the source and TM en-try, normalized by the length of the source as inEq.
(4), as most of the current implementationsare based on edit distance while allowing someadditional flexible matching.FuzzyMatch(t) = mineEditDistance(s, e)Len(s) (4)where s is the source side of the TM hit t, and eis the source side of an entry in the TM.4.2 Problem FormulationRanking lists is a well-researched problem inthe information retrieval community, and RankingSVMs (Joachims, 2002), which optimizes on theranking correlation ?
have already been appliedsuccessfully in machine translation evaluation (Yeet al, 2007).
We apply the same method here torerank a merged list of MT and TM outputs.Formally given an MT-produced N-best listM = {m1,m2, ...,mn}, a TM-produced m-bestlist T = {t1, t2, ..., tm} for a input sentence s,we define the gold standard using the TER met-ric (Snover et al, 2006): for each d ?
M?T,(di, dj) ?
r(s) iff TER(di) < TER(dj).
Wetrain and test a Ranking SVM using cross vali-dation on a data set created according to this cri-terion.
Ideally the gold standard would be cre-ated by human annotators.
We choose to use TER376as large-scale annotation is not yet available forthis task.
Furthermore, TER has a high correla-tion with the HTER score (Snover et al, 2006),which is the TER score using the post-edited MToutput as a reference, and is used as an estimationof post-editing effort.4.3 The Feature SetWhen building features for the Ranking SVM, weare limited to features that are independent of theMT and TM system.
We experiment with system-independent fluency and fidelity features below,which capture translation fluency and adequacy,respectively.4.3.1 Fluency FeaturesSource-side Language Model Scores.
Wecompute the LM probability and perplexity of theinput source sentence on a language model trainedon the source-side training data of the SMT sys-tem, which is also the TM database.
The inputsthat have lower perplexity on this language modelare more similar to the data set on which the SMTsystem is built.Target-side LanguageModel Scores.
We com-pute the LM probability and perplexity as a mea-sure of the fluency of the translation.4.3.2 Fidelity FeaturesThe Pseudo-Source Fuzzy Match Score.
Wetranslate the output back to obtain a pseudo sourcesentence.
We compute the fuzzy match scorebetween the original source sentence and thispseudo-source.
If the MT/TM performs wellenough, these two sentences should be the sameor very similar.
Therefore the fuzzy match scorehere gives an estimation of the confidence level ofthe output.The IBMModel 1 Score.
We compute the IBMModel 1 score in both directions to measure thecorrespondence between the source and target, asit serves as a rough estimation of how good atranslation it is on the word level.5 Experiments5.1 Experimental Settings5.1.1 DataOur raw data set is an English?French trans-lation memory with technical translation from amulti-national IT security company, consisting of51K sentence pairs.
We randomly select 43K totrain an SMT system and translate the English sideof the remaining 8K sentence pairs, which is usedto run cross validation.
Note that the 8K sentencepairs are from the same TM, so that we are able tocreate a gold standard by ranking the TER scoresof the MT and TM outputs.Duplicated sentences are removed from thedata set, as those will lead to an exact match inthe TM system and will not be translated by trans-lators.
The average sentence length of the trainingset is 13.5 words and the size of the training setis comparable to the (larger) translation memoriesused in the industry.5.1.2 SMT and TM systemsWe use a standard log-linear PB-SMTmodel (Och and Ney, 2002): GIZA++ imple-mentation of IBM word alignment model 4, thephrase-extraction heuristics described in (Koehnet al, 2003), minimum-error-rate training (Och,2003), a 5-gram language model with Kneser-Neysmoothing trained with SRILM (Stolcke, 2002)on the English side of the training data, andMoses (Koehn et al, 2007) to decode.
We train asystem in the opposite direction using the samedata to produce the pseudo-source sentences.We merge distinct 5-best lists from MT and TMsystems to produce a new ranking.
To create thedistinct list for the SMT system, we search overa 100-best list and keep the top-5 distinct out-puts.
Our data set consists of mainly short sen-tences, leading to many duplications in the N-bestoutput of the SMT decoder.
In such cases, top-5 distinct outputs are good representations of theSMT?s output.5.2 Training, Tuning and Testing theRanking SVMWe run training and prediction of the RankingSVM in 4-fold cross validation.
We use the377SVMlight1 toolkit to perform training and testing.When using the Ranking SVM with the RBFkernel, we have two free parameters to tune on:the cost parameter C in Eq.
(1) and the radiusparameter ?
in Eq.
(3).
We optimize C and?
using a brute-force grid search before runningcross-validation and maximize precision at top-5,with an inner 3-fold cross validation on the (outer)Fold-1 training set.
We search within the range[2?6, 29], the step size is 2 on the exponent.5.3 The Gold Standard                                                                                                                                                                                                                                                                                                                                                                                                                         0%20%40%60%80%100%Top1 Top3 Top5Gold Standard %TMMTFigure 1: MT and TM?s percentage in gold stan-dardFigure 1 shows the composition of translationsin the gold standard.
Each source sentence is asso-ciated with a list of translations from two sources,i.e.
MT output and TM matches.
This list oftranslations is ranked from best to worst accord-ing TER scores.
The figure shows that over 80%of the translations are from the MT system if weonly consider the top-1 translation.
As the num-ber of top translations we consider increases, moreTM matches can be seen.
On the one hand, thisdoes show a large gap in quality between MT out-put and TM matches; on the other hand, however,it also reveals that we will have to ensure two ob-jectives in ranking: the first is to rank the 80%MT translations higher and the second is to keepthe 20% ?good?
TM hits in the Top-5.
We designour evaluation metrics accordingly.5.4 Evaluation MetricsThe aim of this research is to provide post-editorswith translations that in many cases are easier to1http://svmlight.joachims.org/edit than the original TM output.
As we formulatethis as a ranking problem, it is natural to measurethe quality of the ranking output by the numberof better translations that are ranked high.
Some-times the top TM output is the easiest to edit; insuch a case we need to ensure that this translationhas a high rank, otherwise the system performancewill degrade.Based on this observation, we introduce theidea of relevant translations, and our evaluationmetrics: PREC@k and HIT@k.Relevant Translations.
We borrow the ideaof relevence from the IR community to definethe idea of translations worth ranking high.
Fora source sentence s which has a top TM hit t,we define an MT/TM output m as relevant, ifTER(m) ?
TER(t).
According to the defini-tion, relevant translations should need no morepost-edits than the original top hit from the TMsystem.
Clearly the top TM hit is always relevant.PREC@k. We calculate the precision(PREC@k) of the ranking for evaluation.
As-suming that there are n relevant translations inthe top k list for a source sentence s, we havePREC@k= n/k for s. We test PREC@k, fork = 1...10, in order to evaluate the overall qualityof the ranking.HIT@k. We also estimate the probability ofhaving one of the relevant translations in the topk, denoted as HIT@k. For a source sentence s,HIT@k equals to 1 if there is at least one relevanttranslation in top k, and 0 otherwise.
This mea-sures the quality of the best translation in top k,which is the translation the post-editor will findand work on if she reads till the kth place in thelist.
HIT@k equals to 1.0 at the end of the list.We report the mean PREC@k and HIT@k forall s with the 0.95 confidence interval.5.5 Experimental ResultsIn Table 1 we report PREC@k and HIT@kfor k = 1..10.
The ranking receives 0.8747PREC@1, which means that most of the topranked translations have at least the same qualityas the top TM output.
We notice that precision re-mains above 0.8 till k = 5, leading us to concludethat most of the relevant translations are ranked inthe top-5 positions in the list.378Table 1: PREC@k and HIT@k of RankingPREC % HIT %k=1 87.47?1.60 87.47?1.60k=2 85.42?1.07 93.36?0.53k=3 84.13?0.94 95.74?0.61k=4 82.79?0.57 97.08?0.26k=5 81.34?0.51 98.04?0.23k=6 79.26?0.59 99.41?0.25k=7 74.99?0.53 99.66?0.29k=8 70.87?0.59 99.84?0.10k=9 67.23?0.48 99.94?0.08k=10 64.00?0.46 100.0?0.00Using the HIT@k scores we can further con-firm this argument.
The HIT@k score growssteadily from 0.8747 to 0.9941 for k = 1...6, somost often there will be at least one relevant trans-lation in top-6 for the post-editor to work with.After that room for improvement becomes verysmall.In sum, both of the PREC@k scores and theHIT@k scores show that the ranking model effec-tively integrates the two translation sources (MTand TM) into one merged k-best list, and ranksthe relevant translations higher.Table 2: PREC@k - MT and TM SystemsMT % TM %k=1 85.87?1.32 100.0?0.00k=2 82.52?1.60 73.58?1.04k=3 80.05?1.11 62.45?1.14k=4 77.92?0.95 56.11?1.11k=5 76.22?0.87 51.78?0.78To measure whether the ranking model is ef-fective compared to pure MT or TM outputs, wereport the PREC@k of those outputs in Table 2.The k-best output used in this table is ranked bythe MT or TM system, without being ranked byour model.
We see the ranked outputs consistentlyoutperform the MT outputs for all k = 1...5 w.r.t.precision at a significant level, indicating that oursystem preserves some high quality hits from theTM.The TM outputs alone are generally of muchlower quality than the MT and Ranked outputs, asis shown by the precision scores for k = 2...5.
ButTM translations obtain 1.0 PREC@1 according tothe definition of the PREC calculation.
Note thatit does not mean that those outputs will need lesspost-editing (cf.
Section 6.1), but rather indicatesthat each one of these outputs meet the lowest ac-ceptable criterion to be relevant.6 Analysis of Post-Editing EffortA natural question follows the PREC and HITnumbers: after reading the ranked k-best list, willthe post-editors edit less than they would have to ifthey did not have access to the list?
This questionwould be best answered by human post-editors ina large-scale experimental setting.
As we have notyet conducted a manual post-editing experiment,we try to measure the post-editing effort impliedby our model with the edit statistics captured bythe TER metric, sorted into four types: Insertion,Substitution, Deletion and Shift.
We report the av-erage number of edits incurred along with the 0.95confidence interval.6.1 Top-1 Edit StatisticsWe report the results on the 1-best output of TM,MT and our ranking system in Table 3.In the single best results, it is easy to see thatthe 1-best output from the MT system requiresthe least post-editing effort.
This is not surpris-ing given the distribution of the gold standard inSection 5.3, where most MT outputs are of betterquality than the TM hits.Moreover, since TM translations are generallyof much lower quality as is indicated by the num-bers in Table 3 (e.g.
2x as many substitutionsand 3x as many deletions compared to MT), un-justly including very few of them in the rankingoutput will increase loss in the edit statistics.
Thisexplains why the ranking model has better rank-ing precision in Tables 1 and 2, but seems to in-cur more edit efforts.
However, in practice post-editors can neglect an obvious ?bad?
translationvery quickly.6.2 Top-k Edit StatisticsWe report edit statistics of the Top-3 and Top-5outputs in Tables 4 and 5, respectively.
For eachsystem we report two sets of statistics: the Best-statistics calculated on the best output (according379Table 3: Edit Statistics on Ranked MT and TM Outputs - Single BestInsertion Substitution Deletion ShiftTM-Top1 0.7554 ?
0.0376 4.2461 ?
0.0960 2.9173 ?
0.1027 1.1275 ?
0.0509MT-Top1 0.9959 ?
0.0385 2.2793 ?
0.0628 0.8940 ?
0.0353 1.2821 ?
0.0575Rank-Top1 1.0674 ?
0.0414 2.6990 ?
0.0699 1.1246 ?
0.0412 1.2800 ?
0.0570to TER score) in the list, and the Mean- statisticscalculated on the whole Top-k list.The Mean- numbers allow us to have a generaloverview of the ranking quality, but it is stronglyinfluenced by the poor TM hits that can easily beneglected in practice.
To control the impact ofthose TM hits, we rely on the Best- numbers to es-timate the edits performed on the translations thatare more likely to be used by post-editors.In Table 4, the ranking output?s edit statisticsis closer to the MT output than the Top-1 casein Table 3.
Table 5 continues this tendency, inwhich the Best-in-Top5 Ranking output requiresmarginally less Substitution and Deletion opera-tions and significantly less Insertion and Shift op-erations (starred) than its MT counterpart.
Thisshows that when more of the list is explored, theadvantage of the ranking model ?
utilizing mul-tiple translation sources ?
begins to compensatefor the possible large number of edits required bypoor TM hits and finally leads to reduced post-editing effort.There are several explanations to why the rel-ative performance of the ranking model improveswhen k increases, as compared to other models.The most obvious explanation is that a single poortranslation is less likely to hurt edit statistics ona k-best list with large k, if most of the transla-tions in the k-best list are of good quality.
We seefrom Tables 1 and 2 that the ranking output is ofbetter quality than the MT and TM outputs w.r.t.precision.
For a larger k, the small number of in-correctly ranked translations are less likely to bechosen as the Best- translation and hold back theBest- numbers.A further reason is related to our ranking modelwhich optimizes on Kendall?s ?
score.
Accord-ingly the output might not be optimal when weevaluate the Top-1 output, but will behave betterwhen we evaluate on the list.
This is also in ac-cordance with our aim, which is to enrich the TMwith MT outputs and help the post-editor, insteadof choosing the translation for the post-editor.6.3 Comparing the MT, TM and RankingOutputsOne of the interesting findings from Tables 3 and4 is that according to the TER edit statistics, theMT outputs generally need a smaller number ofedits than the TM and Ranking outputs.
This cer-tainly confirms the necessity to integrate MT intotoday?s TM systems.However, this fact should not lead to the con-clusion that TMs should be replaced by MT com-pletely.
First of all, all of our experiments excludeexact TM matches, as those translations will sim-ply be reused and not translated.
While this is arealistic setting in the translation industry, it re-moves all sentences for which the TM works bestfrom our evaluations.Furthermore, Table 5 shows that the Best-in-Top5 Ranking output performs better than the MToutputs, hence there are TM outputs that lead tosmaller number of edits.
As k increases, the rank-ing model is able to better utilize these outputs.Finally, in this task we concentrate on rank-ing useful translations higher, but we are not in-terested in how useless translations are ranked.Ranking SVM optimizes on the ranking of thewhole list, which is slightly different from whatwe actually require.
One option is to use otheroptimization techniques that can make use of thisproperty to get better Top-k edit statistics for asmaller k. Another option is obviously to performregression directly on the number of edits insteadof modeling on the ranking.
We plan to explorethese ideas in future work.7 Conclusions and Future WorkIn this paper we present a novel ranking-basedmodel to integrate SMT into a TM system, in or-der to facilitate the work of post-editors.
In such380Table 4: Edit Statistics on Ranked MT and TM Outputs - Top 3Insertion Substitution Deletion ShiftTM-Best-in-Top3 0.4241 ?
0.0250 3.7395 ?
0.0887 2.9561 ?
0.0966 0.9738 ?
0.0505TM-Mean-Top3 0.6718 ?
0.0200 5.1428 ?
0.0559 3.6192 ?
0.0649 1.3233 ?
0.0310MT-Best?in-Top3 0.7696 ?
0.0351 1.9210 ?
0.0610 0.7706 ?
0.0332 1.0842 ?
0.0545MT-Mean-Top3 1.1296 ?
0.0229 2.4405 ?
0.0368 0.9341 ?
0.0209 1.3797 ?
0.0344Rank-Best-in-Top3 0.8170 ?
0.0355 2.0744 ?
0.0608 0.8410 ?
0.0338 1.0399 ?
0.0529Rank-Mean-Top3 1.0942 ?
0.0234 2.7437 ?
0.0392 1.0786 ?
0.0231 1.3309 ?
0.0334Table 5: Edit Statistics on Ranked MT and TM OutputsInsertion Substitution Deletion ShiftTM-Best-in-Top5 0.4239 ?
0.0250 3.7319 ?
0.0885 2.9552 ?
0.0967 0.9673 ?
0.0504TM-Mean-Top5 0.6143 ?
0.0147 5.5092 ?
0.0473 3.9451 ?
0.0521 1.3737 ?
0.0240MT-Best-in-Top5 0.7690 ?
0.0351 1.9163 ?
0.0610 0.7685 ?
0.0332 1.0811 ?
0.0544MT-Mean-Top5 1.1912 ?
0.0182 2.5326 ?
0.0291 0.9487 ?
0.0165 1.4305 ?
0.0272Rank-Best-in-Top5 0.7246 ?
0.0338* 1.8887 ?
0.0598 0.7562 ?
0.0327 0.9705 ?
0.0515*Rank-Mean-Top5 1.1173 ?
0.0181 2.8777 ?
0.0312 1.1585 ?
0.0200 1.3675 ?
0.0260a model, the user of the TM will be presentedwith an augmented k-best list, consisting of trans-lations from both the TM and theMT systems, andranked according to ascending prospective post-editing effort.From the post-editors?
point of view, the TMremains intact.
And unlike in the binary transla-tion recommendation, where only one translationrecommendation is provided, the ranking modeloffers k-best post-editing candidates, enabling theuser to use more resources when translating.
Aswe do not actually throw away any translation pro-duced from the TM, the assets represented by theTM are preserved and the related estimation of theupper bound cost is still valid.We extract system independent features fromtheMT and TM outputs and use Ranking SVMs totrain the ranking model, which outperforms boththe TM?s and MT?s k-best list w.r.t.
precision at k,for all ks.We also analyze the edit statistics of the inte-grated k-best output using the TER edit statistics.Our ranking model results in slightly increasednumber of edits compared to the MT output (ap-parently held back by a small number of poor TMoutputs that are ranked high) for a smaller k, butrequires less edits than both the MT and the TMoutput for a larger k.This work can be extended in a number of ways.Most importantly, We plan to conduct a user studyto validate the effectiveness of the method andto gather HTER scores to train a better rankingmodel.
Furthermore, we will try to experimentwith learning models that can further reduce thenumber of edit operations on the top ranked trans-lations.
We also plan to improve the adaptabilityof this method and apply it beyond a specific do-main and language pair.AcknowledgementsThis research is supported by the Science Foun-dation Ireland (Grant 07/CE/I1142) as part ofthe Centre for Next Generation Localisation(www.cngl.ie) at Dublin City University.
Wethank Symantec for providing the TM databaseand the anonymous reviewers for their insightfulcomments.ReferencesCortes, Corinna and Vladimir Vapnik.
1995.
Support-vector networks.
Machine learning, 20(3):273?297.Joachims, Thorsten.
2002.
Optimizing search enginesusing clickthrough data.
In KDD ?02: Proceed-ings of the eighth ACM SIGKDD international con-ference on Knowledge discovery and data mining,pages 133?142, New York, NY, USA.381Koehn, Philipp., Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
InProceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology(NAACL/HLT-2003), pages 48 ?
54, Edmonton, Al-berta, Canada.Koehn, Philipp, Hieu Hoang, Alexandra Birch,Chris Callison-Burch, Marcello Federico, NicolaBertoldi, Brooke Cowan, Wade Shen, ChristineMoran, Richard Zens, Chris Dyer, Ondrej Bojar,Alexandra Constantin, and Evan Herbst.
2007.Moses: Open source toolkit for statistical ma-chine translation.
In Proceedings of the 45th An-nual Meeting of the Association for ComputationalLinguistics Companion Volume Proceedings of theDemo and Poster Sessions (ACL-2007), pages 177?180, Prague, Czech Republic.Levenshtein, Vladimir Iosifovich.
1966.
Binary codescapable of correcting deletions, insertions, and re-versals.
Soviet Physics Doklady, 10(8):707?710.Och, Franz Josef and Hermann Ney.
2002.
Discrim-inative training and maximum entropy models forstatistical machine translation.
In Proceedings of40th Annual Meeting of the Association for Com-putational Linguistics (ACL-2002), pages 295?302,Philadelphia, PA, USA.Och, Franz Josef.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofthe 41st Annual Meeting on Association for Com-putational Linguistics (ACL-2003), pages 160?167,Morristown, NJ, USA.Shen, Libin, Anoop Sarkar, and Franz Josef Och.2004.
Discriminative reranking for machine trans-lation.
In HLT-NAACL 2004: Main Proceedings,pages 177?184, Boston, Massachusetts, USA.
As-sociation for Computational Linguistics.Sikes, Richard.
2007.
Fuzzy matching in theory andpractice.
Multilingual, 18(6):39 ?
43.Simard, Michel and Pierre Isabelle.
2009.
Phrase-based machine translation in a computer-assistedtranslation environment.
In Proceedings of theTwelfth Machine Translation Summit (MT SummitXII), pages 120 ?
127, Ottawa, Ontario, Canada.Snover, Matthew, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of Association for Machine Transla-tion in the Americas (AMTA-2006), pages 223?231,Cambridge, MA, USA.Specia, Lucia, Nicola Cancedda, Marc Dymetman,Marco Turchi, and Nello Cristianini.
2009a.
Esti-mating the sentence-level quality of machine trans-lation systems.
In Proceedings of the 13th An-nual Conference of the European Association forMachine Translation (EAMT-2009), pages 28 ?
35,Barcelona, Spain.Specia, Lucia, Craig Saunders, Marco Turchi, Zhuo-ran Wang, and John Shawe-Taylor.
2009b.
Improv-ing the confidence of machine translation qualityestimates.
In Proceedings of the Twelfth MachineTranslation Summit (MT Summit XII), pages 136 ?143, Ottawa, Ontario, Canada.Stolcke, Andreas.
2002.
SRILM-an extensible lan-guage modeling toolkit.
In Proceedings of the Sev-enth International Conference on Spoken LanguageProcessing, volume 2, pages 901?904, Denver, CO,USA.Ye, Yang, Ming Zhou, and Chin-Yew Lin.
2007.Sentence level machine translation evaluation as aranking.
In Proceedings of the Second Workshopon Statistical Machine Translation, pages 240?247,Prague, Czech Republic.382
