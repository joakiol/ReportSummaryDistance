Planning Text for Advisory Dialogues:Capturing Intentional and RhetoricalInformationJ ohanna D. Moore"University of PittsburghC6c i le  L. Par i s  tUSC/Information Sciences InstituteTo participate in a dialogue a system must be capable of reasoning about its own previous utter-ances.
Follow-up questions must be interpreted in the context of the ongoing conversation, andthe system's previous contributions form part of this context.
Furthermore, if a system is to be ableto clarify misunderstood explanations or to elaborate on prior explanations, it must understandwhat it has conveyed in prior explanations.
Previous approaches to generating multisententialtexts have relied solely on rhetorical structuring techniques.
In this paper, we argue that, tohandle explanation dialogues uccessfully, a discourse model must include information about theintended effect of individual parts of the text on the hearer, as well as how the parts relate to oneanother hetorically.
We present a text planner that records this information and show how theresulting structure is used to respond appropriately to a follow-up question.1.
In t roduct ionExplanation systems must produce multisentential texts, including justifications oftheir actions, descriptions of their problem-solving strategies, and definitions of theterms they use.
Previous research in natural language generation has shown thatschemata of rhetorical predicates (McKeown 1985; McCoy 1989; Paris 1988) or rhetori-cal relations (Hovy 1991) can be used to capture the structure of coherent multisenten-tial texts.
Schemata re scriptlike entities that encode standard patterns of discoursestructure.
Associating a schema with a communicative goal allows a system to generatea text that achieves the goal.
However, we have found that schemata re insufficient asa discourse model for advisory dialogues.
Although they encode standard patterns ofdiscourse structure, schemata do not include a representation f the intended effects ofthe components of a schema, nor how these intentions are related to one another or tothe rhetorical structure of the text.
While this may not present a problem for systemsthat generate one-shot explanations, it is a serious limitation in a system intended toparticipate in a dialogue where users can, and frequently do, ask follow-up questions.In this paper, we argue that to participate in explanation dialogues uccessfully, ageneration system must represent and reason about the intended effect of individualparts of the text on the hearer, as well as how the parts relate to one another hetorically.We present a text planner that constructs explanations based on the intentions of thespeaker at each step and that notes the rhetorical relation that holds between each pairof text spans.
By recording the planning process behind the system's utterances as wellas the user's utterances in a dialogue history, our system is able to reason about itsDepartment ofComputer Science and Learning Research and Development Center, University ofPittsburgh, Pittsburgh, PA 15260.
E-mail: jmoore@cs.pitt.edut USC/Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292-6695.
E-mail:paris@isi.edu?
1994 Association for Computational LinguisticsComputational Linguistics Volume 19, Number 4previous utterances both to interpret and to answer users' follow-up questions.
Wedescribe the plan language mployed and the plan structure built by our system andprovide an example of how this structure is used in responding appropriately to afollow-up question.
Additional examples appear in Moore and Swartout (1989) andMoore (in press).2.
Motivation: A Naturally Occurring Advisory DialogueWhen we began our work on interactive xplanations, we gathered samples of nat-urally occurring dialogues from several sources: transcripts of electronic dialoguesbetween system users and operators collected by Robinson (1984), protocols of pro-grammers interacting with a mock program enhancement advisor, and tape recordingsof office-hour interactions between first-year computer science students and teachingassistants.
A portion of a dialogue extracted from the office-hour interactions appearsin Figure 1.In this dialogue, a student and a teaching assistant are discussing a programmingassignment that involves writing a procedure to swap the values stored at two loca-tions in the C programming language.
The student is confused about how to writethe procedure because he does not understand that C is a call-by-value language, andso he must pass the addresses of the two variables to be swapped.
In the teacher'sresponse in turn 8, she explains that in C one cannot change the value of a variabledefined outside of a procedure.
She justifies this by saying that C is call-by-value andthen goes on to define this term.
The student hen asks "What's call-by-value?"
(turn9).
To respond appropriately to this question, the teacher must realize that she hasdefined call-by-value in abstract erms as part of her previous explanation and thather first attempt was not fully understood.
In this dialogue, the instructor ecoversfrom this failure by giving a very specific example of how call-by-value works (turn10).
The teacher explains call-by-value differently the second time because she realizesthat she has tried to explain this once before, and that the strategy she used the firsttime was not sufficient.Note that giving a very general description of a concept is a good first strategy.General definitions are an efficient way to convey knowledge because if the hearerunderstands a general definition, it covers a wide range of cases.
Giving example(s)first would be a bad approach because this strategy requires the hearer to form thecorrect generalization from the example(s).
Unless the examples are numerous enoughand well chosen, this may be difficult to achieve (Klausmeier 1976).
However, when ageneral description is not understood by itself, examples provide effective laborationand may help the hearer assimilate the general description (Charney, Reder, and Wells1988).
In the sample dialogue, because the teacher knows that she has already givena general description in turn 8, she can give a very specific example in turn 10.Another characteristic of advisory dialogues is exemplified in this sample.
Advice-seekers often ask questions uch as "Why?"
and "How come?".
To interpret suchquestions, the system must have a representation f what it has said just prior to thequery, so that it can determine what the user could be asking about.3.
Limitations of Previous ApproachesThere are two main approaches to text generation.
In attempting to construct a sys-tem capable of participating in dialogues like the one above, we found that neitherapproach could be directly applied to this task.
Here we describe the limitations ofthese approaches for our purposes.652Johanna D. Moore and C6cile L. Paris Planning Text for Advisory DialogsSTUDENT I can't figure out how to write the SWAP procedure in C. \[1\]TEACHER You write a procedure called SWAP that takes the addresses of the two \[2\]variables to be swapped as its arguments.STUDENT I did that.
\[3\]\[Student shows teacher his program code.\]TEACHER Here \[points to call to SWAP in the main program\], when you call SWAP, \[4\]you have to pass the addresses of the variables you want swapped.STUDENT Why?
\[5\]TEACHER Because otherwise the procedure just swaps its local copies of the argu- \[6\]ments.STUDENT How come?
\[7\]TEACHER You cannot change the value of a variable defined outside a procedure \[8\]unless you pass its address.
This is because C is call-by-value.
Thatmeans that when you pass an argument to a procedure in C, the pro-cedure makes a copy of the argument, and does its work on that copy.The value of the variable passed in as an argument remains unchanged.STUDENT What's call-by-value?
\[9\]TEACHER OK, suppose in our main program we have a variable V with value \[10\]3, and a procedure called PLUS-ONE that takes one argument, call itA.
Now when we call PLUS-ONE(V) what actually happens is that thevalue of V, which is 3, is copied to A.
So A equals 3 and our procedureadds 1 to A, so A now equals 4.
The original variable V that appearedin the call to PLUS-ONE does not change.
V still equals 3.STUDENT So, how can I ever write a procedure that swaps two values?
\[11\]TEACHER You write a procedure that takes addresses of variables as its arguments.
\[12\]When you call it, you pass the pointers to the variables to be swapped.So you write SWAP(&X,&Y).
The & sign means take the address of thevariable.
When you say &X, the address of X is copied to a temporaryvariable, and SWAP uses the temporary copy of the address.
But theaddress always points to the same place, namely the location where Xis held.
So SWAP will operate on X, using a copy of X's address, ratherthan operating on a copy of X.STUDENT Oh, okay.
\[13\]Figure 1Portion of a naturally occurring advisory dialogue.3.1 Planning with Speech ActsWork by Cohen and Perrault (1979), Appelt (1985), and Cohen and Levesque (1990)demonstrated that planning techniques could be useful in text generation.
These re-searchers provide a formal axiomatization of illocutionary actions that may be usedto reason about the beliefs of the hearer and speaker and the effects of surface speechacts on these beliefs.
To use this approach in a generation process, the system firstgenerates hypotheses about what combinations of actions to perform.
For efficiency,Appelt (1985) uses simplified versions of the axioms (called action summaries) en-coded in NOAH-style plan operators (Sacerdoti 1977) to generate these hypotheses.Theorem-proving is then used to determine if a series of proposed actions will havethe desired effect on the hearer's mental state.
The systems that have been built within653Computational Linguistics Volume 19, Number 4this framework to date (Cohen 1978; Appelt 1985) plan short (one- or two-sentence)texts to achieve the speakers' goal(s).In this approach, the intentional structure describing the speaker's purposes andthe relationships between them (Grosz and Sidner 1986) is explicitly represented.
How-ever, this approach does not represent or use rhetorical knowledge about how speechacts may be combined into larger bodies of coherent text to achieve a speaker's goals.It assumes that appropriate axioms could be added to generate longer texts, and thatthe text produced will be coherent as a byproduct of the planning process.
However,this has not been demonstrated.Moreover, we believe that building a system to produce multisentential texts di-rectly from the logics proposed by proponents of this approach would prove to becomputationally infeasible.
We see two problems.
First, this approach requires the sys-tem to acquire and maintain a correct, detailed model of the hearer's beliefs.
SparckJones (1989) has questioned not only the feasibility of acquiring such a model, butalso of verifying its correctness, and the tractability of utilizing such a model to af-fect a system's reasoning and the generation of responses.
Second, all of the formalaxiomatizations espoused by proponents of this approach are based on extensions tofirst-order logic.
In the general case, theorem-proving in first-order logic is undecid-able.
To be fair, some proponents of this approach, e.g.
Cohen and Levesque (1990),claim to provide a specification of an agent, and do not claim that the axiomatizationshould be used directly as a specification for the implementation of an agent.
Heuris-tics are clearly needed in order to make an implementation based on such a formalismtractable.
Our approach employs rhetorical strategies as compiled knowledge aboutwhat actions may be used to satisfy certain intentions.
In fact, our operators havemuch in common with Appelt's action summaries.3.2 The Schema-Based ApproachTo produce the longer bodies of text required for advisory dialogues in an efficientmanner, other researchers turned to an approach that makes use of script-like struc-tures, schemata, to generate coherent multisentential texts achieving a given commu-nicative goal.
Schemata, originally proposed by McKeown (1985), represent standardpatterns of discourse structure by encoding the set of communicative t chniques thata speaker can use for a particular discourse purpose.
Schemata re made up of rhetor-ical predicates that characterize the means that speakers use to achieve their goalsand delineate the structural relations between propositions in a text.
Linguists, e.g.,(Shepherd 1926; Grimes 1975), found that rhetorical predicates tend to occur in certaincombinations, and McKeown further observed that certain combinations are more ap-propriate than others depending on the discourse purpose.
For example, she foundthat speakers frequently describe objects by:1.
Identifying the object as a member of some generic class and givingattributive or functional information about the object.2.
Providing analogical, constituent, or additional attributive informationabout the object.3.
Providing examples of the object.To encode these standard patterns of discourse structure, McKeown devised severalschemata that represent combinations of rhetorical predicates.
For example, the abovepattern is embodied in the IDENTIFICATION schema, shown in Figure 2.
By associatingeach rhetorical predicate with an access function for an underlying knowledge base,654Johanna D. Moore and C6cile L. Paris Planning Text for Advisory DialogsIdentification Schema 1Identification (class & attributive/function){Analogy/Constituency/Attributive/Renaming/Amplification}*Particular-illustration / Evidence+{Amplification/Analogy/Attributive}{Particular-illustration / Evidence}Sample Definition Generated using Identification Schema:(1) A ship is a water-going vehicle that travels on the surface.
(2) Its surface-going capabilities are provided by the DB attributesDISPLACEMENT and DRAFT.
(3) Other DB attributes of the ship in-clude MAXIMUM_SPEED, PROPULSION, FUEL (FUEL_CAPACITY and FUEL_TYPE),DIMENSIONS, SPEED_DEPENDENT_RANGE and 0FFICIAL_NAME.
(4) The DOWNES,for example, has MAXIMUM_SPEED of 29, PROPULSION of STMTURGRD,FUEL of 810 (FUEL_CAPACITY) and SSKR (FUEL_TYPE), DIMENSIONS of 25(DRAFT), 46 (BEAM), and 438 (LENGTH) and SPEED_DEPENDENT_RANGE of 4200(EC0NOMIC_RANGE) and 2200 (ENDURANCE_RANGE).Figure 2TEXT identification schema nd sample generated text (from McKeown \[1985\], pp.
210-212).these schemata can be used to guide both the selection of content and its organizationinto a coherent text.
Figure 2 also shows a sample text generated from a knowledgebase of naval concepts using the IDENTIFICATION schema.
McKeown identified fourschemata, each of which could be used to achieve one or more discourse purposes.As shown in Figure 2, schemata contain many options and alternatives.
To instan-tiate a schema, its components are filled in sequentially by using the access functions tosearch the underlying knowledge base for information that satisfies the rhetorical pred-icates.
In McKeown's theory, each entry in the schema can be filled by an instantiatedpredicate or a full schema of the same name.
So, for example, in the IDENTIFICATIONschema, the first entry can either be satisfied by an instance of the IDENTIFICATIONpredicate, or a recursive instantiation of the IDENTIFICATION schema itself.McKeown found that schemata lone were not sufficient to constrain the genera-tion process.
To overcome this, when a schema indicates that more than one choice ispossible, McKeown's ystem appeals to constraints on the shift of focus of attention(Sidner 1979).
These constraints guide the selection of the information that fits in bestwith the previous discourse.
Since McKeown's eminal work, many other researchershave used schemata s the basis for producing multisentential texts.
In many cases,these researchers found that schemata provided only a partial solution, and they haveidentified additional factors that control the generation process: Paris (1988) uses in-formation about the user's knowledge of domain concepts to tailor descriptions ofcomplex physical objects to a particular user; McCoy (1989) uses object perspectivesand a user model to provide corrective responses to users' misconceptions about ob-1 The "{}" indicate optionality, " / "  indicates alternative, "+" indicates that the item may appear one ormore times, and "*" indicates that the item may appear zero or more  times.655Computational Linguistics Volume 19, Number 4SYSTEM What characteristics of the program would you like to enhance?
\[1\]USER Readability and maintainability.
\[2\]SYSTEM You should replace (8ETO X 1) with (SETF X 1).
SETQ can only be used \[3\]to assign a value to a simple-variable.
In contrast, SETF can be used toassign a value to any generalized-variable.
A generalized-variable is astorage location that can be named by any access function.Figure 3Partial dialogue.jects; and Hovy (1988) uses pragmatic and stylistic information to produce differentaccounts of the same incident.3.2.1 Inadequacies of Schemata for Advisory Dialogue.
Like others, we found thatschemata were not sufficient o handle the issues we wished to investigate.
When weattempted to use schemata for our purposes, two main problems arose.
First, schematalack an explicit representation f the intentional structure of the text being produced,and therefore are missing the information eeded to recover from explanatory failures.Second, we found that schemata re too rigid to handle certain of the opportunisticphenomena we observed in naturally occurring dialogues.
We discuss these two prob-lems in more detail.Lack of Intentional Structure.
As we have seen, schemata encode standard patternsof discourse structure.
However, they do not include an explicit representation f theeffects that individual components of a schema are intended to have on the hearer,or of how these intentions relate to one another or to the rhetorical structure of thetext.
This presents a serious problem for a system that must participate in a dialoguewhere users can ask follow-up questions like the ones we saw in the sample dialogueof Figure 1.
If a system does not keep a record of the intentions behind its utterances,it cannot determine what went wrong when the user indicates that an explanationwas not completely understood, nor provide an alternative xplanation to correct heproblem.To allow a system to handle follow-up questions that may arise if the user doesnot fully understand an explanation, a generation facility must be able to determinewhat portion of the text failed to achieve its intended purpose.
If the generation systemonly knows the top-level communicative goal that was being achieved by the text (e.g.,to make the hearer know a concept, or to make the hearer want to perform an action),and not what effect the individual parts of the text were intended to have on thehearer or how they fit together to achieve this top-level goal, its only recourse is touse a different strategy to achieve the top-level goal.
It is not able to re-explain orclarify any part of the explanation.We illustrate this important point by working through an example taken froman actual dialogue with a system called the Program Enhancement Advisor (PEA)(Neches, Swartout, and Moore 1985).
(Note that, for precisely the reasons we de-scribe in this paper, PEA does not employ schemata to generate its utterances.
Wedescribe PEA's text planner in Section 5.)
As shown in Figure 3, PEA begins its in-teraction with the user by asking what characteristics of the user's program are to beenhanced and then suggests changes that will improve these aspects of the program.Now consider what a schema that could produce the system's utterance in turn 3 ofthe sample dialogue in Figure 3 would look like.
One schema that would suffice, whichwe have called the Recommend-Replacement Schema, is shown instantiated in Figure 4.656Johanna D. Moore and C6cile L. Paris Planning Text for Advisory DialogsSystem's Utterance(1) You should replace (SETQ X 1) with (SETF X 1).
(2) SETQ can onlybe used to assign a value to a simple-variable.
(3) SETF can be used toassign a value to any generalized-variable.
(4) A generalized-variableis a storage location that can be named by any access function.Recommend-Replacement Schema (Instantiated)(Recommendation (replace-setq-with-setf)) (1)(Compare&Contrast-Attributive)(Attributive SETQ use assign-value-to-simple-variable) (2)(Attributive SETF use assign-value-to-generalized-variable) (3)(Identification generalized-variable storage-location (4)(restrictive named-by access-function))Figure 4Hypothetical schema representation f system's utterance.This schema begins with a RECOMMENDATION of a replacement act, followed by aCOMPARE & CONTRAST predicate that highlights the important difference(s) betweenthe replacee and the replacer.
2 Instead of using a simple predicate, we instantiate theCOMPARE & CONTRAST predicate using a schema that expands into two ATTRIBUTIVEpredicates and an IDENTIFICATION predicate that defines a term introduced in thesecond ATTRIBUTIVE.Note that this schema indicates what to do when, i.e., recommend the action andcontrast he replacee with replacer, but it does not say why this information is beingpresented.
For example, the schema does not indicate that, by contrasting SETQ withSETF, the speaker is trying to persuade the hearer to do the replace act.
Nor does itindicate that the text produced by the IDENTIFICATION predicate appears because thespeaker is trying to make the hearer know about the concept genera l i zed-var iab le .In addition, the relationships between these intentions are not represented.
To makeclear what is missing, we have represented in Figure 5 the intentional structure of thistext using Grosz and Sidner's (1986) notions of dominance and satisfaction-precedence.In Grosz and Sidner's theory (1986, p. 179), if an action that satisfies one intention, h,is intended to provide part of the satisfaction of another intention,/2, then/2 dominatesh.
h satisfaction-precedes 12 whenever h must be satisfied before/2.
The representationshown in Figure 5 makes it clear that the expert system's (E) top-level intention (I0) isto get the user (U) to intend to replace (SETQ X 1) with (SETF X 1), and this intentiondominates E's intentions to recommend this act (/1) and to persuade U to perform it(/2).
In addition, for this schema, the recommendation (h) must be satisfied before thepersuade (/2) is attempted.A schema can be viewed as the result of a "compilation" process where the rationalefor all of the steps in the process has been compiled out.
What remains is the top-levelcommunicative goal that invoked the schema (in this case something like Get the user toadopt he goal of replacing SETQ with SETF), and the sequence of actions (i.e.
instantiated2 The first step of the schema is to identify commonalities of the two entities being contrasted.
InMcKeown (1985), this step is optional if no commonalities xist.
We have changed this definitionslightly to render this step optional if the speaker believes these commonalities are known to thehearer.
This is the case here, so the instantiated schema does not contain this step.657Computational Linguistics Volume 19, Number 4System's Utterance:You should replace (SETQ X i) with (SETF X I).
SETQ can only be used to assign avalue to a simple-variable.
SETF can be used to assign a value to any generalized-variable.
Ageneralized-variable is a storage location that can be named by any access function.Intentional Structure:I0: (Intend E (Intend U (Replace U (SETQ X 1) (SEIT X 1))I1: 0ntendE(RocommendEU I2: (IntendE(Pcnuad~lEU(Replace U (SETQ X 1) (SETF X I)))) (Replace U (SETQ X 1) (SERF X 1))))II3: O~c~l E (Believe U(Som~rcf (Diff~ences-wa-goal SETQ SETF enhance-maintainability))))15: (Intend E (Believe U I4: (Intend E (Believe U(Use SETQ as sign-value4o-simple-variable))) (Usc SETF aHign-valuo-t o-gener alizeai-variable)))II6: (Intend E ( I~ow-~ut  U (Cxmeept generaliz~l-w?iabl?
)))I17: (Intend E ( I~eve U (I~ ~-~u ' iab le  (stor~e4oe~ion (re~'iet ~med--by symbol)))))Figure 5Intentional structure of system's utterance.rhetorical predicates that cause sentences to be generated) that are used to achievethat goal.
All of the intermediate structure shown in Figure 5 has been lost.
3Because of this compilation, schemata provide a computationally efficient wayto produce multisentential texts for achieving discourse purposes.
They are "recipes"of rhetorical actions that encode frequently occurring patterns of discourse structure.Using schemata, the system need not reason directly about how speech acts affect hebeliefs of the hearer and speaker, nor about he effects of juxtaposing speech acts.
Thesystem is guaranteed that each schema will lead to a coherent text that achieves thespecified iscourse purpose.However, this compilation isalso a disadvantage.
If the hearer does not understandthe utterance produced by a schema, it is very difficult for the system to recover.
In theexample above, without understanding why the COMPARE & CONTRAST schema ndIDENTIFICATION predicate are present in the instantiated structure, the system cannotdetermine how to proceed if the user does not immediately understand and accept hesystem's utterance.
In the sample text under consideration, the COMPARE & CONTRASToccurs as part of the top-level schema in order to persuade the hearer to perform therecommended replacement action.
This is represented in the intentional representationof Figure 5 by intentions 12 and 13 and the dominance relationship dominates(I2, 3).3 Note that the schema shown in Figure 4 would not be compiled directly from the structure shown inFigure 5.
The schema would be compiled from an hierarchical text plan that included the intentions aswell as the rhetorical methods and speech acts that are used to achieve the intentions.
Figure 5 showsonly the intentional structure.658Johanna D. Moore and C6cile L. Paris Planning Text for Advisory DialogsIf the text produced by the COMPARE & CONTRAST portion of the schema fails topersuade the hearer, the speaker should try other strategies for achieving this intention.For example, the speaker may paraphrase the reasoning that led to recommendingthis act, cite the advice of experts, or invoke authority.
However, because informationabout intentions has been "compiled out" of the schema representation, the systemcannot recover because it cannot determine which goal failed or what other linguisticstrategies can be used to achieve the goal.Note that the problem stems from the fact that, in general, there is not a one-to-one mapping from intentions to schemata or rhetorical predicates.
For example,the COMPARE & CONTRAST schema can be used to persuade the hearer to performa replacement action as in the example above, but it could also be used for severalother purposes.
This schema could be used to identify the differences between twoobjects o that the hearer can discriminate one from the other.
It could also be used ina strategy where a new concept is defined by comparing it to a known concept.
Thus,simply knowing that a COMPARE & CONTRAST schema (or predicate) appears in atext does not tell the system why that COMPARE & CONTRAST appears.
As a result, ifit fails to achieve its intended effect, the system cannot determine how to recover.
Tofind alternative strategies, the system must know what goal it was trying to achieve!Rigidity of Schemata.
A second problem with schemata is that they are too rigid.
Inthe naturally occurring dialogues we analyzed, we have often observed what appear tobe opportunistic effects.
One such effect we identified is the tendency of the explainerto define a concept immediately after mentioning that concept in the explanation.
Onepossible reason for this is that the explainer only realizes that the hearer may not knowthat concept after a sentence including it has been planned or uttered.
Recall that thisoccurred in the teacher-student dialogue in Figure 1 (turn 8).
As another example,consider the following explanation given by a doctor when asked about the possibleways to treat migraine (italics indicate our present concern, not spoken emphasis).
4Some of the possible ways that I approach migraine would be, de-pending on the frequency of your headaches, we would determinewhich approach I would recommend.
\[... \] So, for example, say thatyou told me that you had three to four headaches \[... \] in the month,what I would recommend with that frequency is that you should beon something prophylactically.
Prophylactically basically means preventingthe headache from occurring before it actually starts.
If you had infrequentheadaches, maybe several times a year, \[... \] then I would recommendmore something abortive.
That means that when the headache came on, Iwould treat you at that point.
I would rather, to help prevent side effectsfrom you having to take a medicine on a daily basis, just try to abortthem, if they were infrequent.Because a new term can be introduced in virtually any statement, one could onlyhandle this phenomenon i  the schema-based approach by incorporating an optionalIDENTIFICATION predicate for every term mentioned in the previous predicate after ev-ery entry in every schema.
Note that this would be inefficient because if these predicatesappear in the schema, the system must consider them (i.e., search for instantiations in4 This example is taken from transcripts gathered by Claudia Tapia nd Johanna Moore at the Universityof Pittsburgh.659Computational Linguistics Volume 19, Number 4the knowledge base and invoke the focusing mechanism) for every additional predi-cate added to the schema.
5 We believe that a more elegant and efficient approach isto use planning techniques that permit new goals to be posted as they arise and tobe worked into an evolving text plan according to rules of discourse as representedin plan operators.
The framework that we propose in this paper handles this type ofopportunistic planning in a limited way.
Suthers (1991) handles a wider array of op-portunistic effects using data-driven plan critics to decide when additional informationshould be included.3.3 Requirements for a Text PlannerLike others, e.g., Levy (1979) and Appelt (1985), we take the view that speakers havegoals to affect the mental states of their hearers and must choose from among the lin-guistic resources available to them the ones that will satisfy these goal(s).
We arguedthat an approach to text planning that attempts to reason directly about how speechacts can be combined into coherent multisentential texts to achieve a speaker's inten-tions is likely to be computationally infeasible.
However, our discussion of schematapointed out that we must be careful about what and how much is compiled out of ourrepresentations for the sake of efficiency.
While we believe the schema-based approachto be correct in spirit, we think that too much information has been lost in schematacomposed simply of rhetorical predicates.Ideally, what we desire is an approach that:records enough information about the system's intentions and how thoseintentions were achieved, so that the system can reason about its ownprevious utterances to determine how to recover when its explanationsare not understood, andis capable of producing texts in a computationally efficient manner.To achieve these goals, we propose an approach to text planning in which plan opera-tors encode knowledge about how intentions may be achieved via a set of techniquescommonly found in natural discourse.
Thus we require a plan language that linksintentions to the rhetorical means for achieving them.4.
Linking Intentions and Rhetorical StructureTwo theories of discourse structure that make the connection between rhetorical re-lations and speaker intentions have been proposed: Hobbs' (1979, 1983, 1985) theoryof coherence relations, and Mann and Thompson's (1988) Rhetorical Structure Theory.As we will see, Rhetorical Structure Theory can be adapted to a computational modelin a fairly natural way, and in fact there is an implemented prototype of the theory(Hovy 1991).
However, the straightforward operationalization that is used in this pro-totype suffers from a fundamental problem for our purposes.
After discussing the twotheories that link intention to rhetorical structure, we discuss this problem in detail.In Section 5, we describe how this problem may be solved and present a text plannerthat implements this solution.5 It is also unclear how we could represent this opportunistic strategy in the schema-based approach,since it is only when "unpacking" complex concepts such as assign-value-to-generalized-variablethat the system recognizes that it will introduce the terms assign, value and generalized-variable.See Moore and Paris (1992) for more discussion about his topic.660Johanna D. Moore and C6cile L. Paris Planning Text for Advisory Dialogs4.1 Theoretical BackgroundHobbs characterizes coherence in terms of a set of binary coherence relations be-tween a current utterance and the preceding discourse.
He identified four reasonswhy a speaker breaks a discourse into more than one clause and classified the rela-tions accordingly.
For example, if a speaker needs to connect new information withwhat is already known by the hearer, the speaker chooses one of the linkage relations,such as BACKGROUND or EXPLANATION.
As another example, when a speaker wishes tomove between specific and general statements, he or she must employ one of the ex-pansion relations, such as ELABORATION or GENERALIZATION.
According to Hobbs, howthe speaker chooses to continue a discourse is equivalent to deciding which relationto employ.
From the hearer's perspective, understanding why the speaker continuedas he or she did is equivalent to determining what relation was used.Hobbs originally proposed coherence relations as a way of solving some of theproblems in interpreting discourse, e.g., anaphora resolution (Hobbs 1979).
He definescoherence r lations in terms of inferences that can be drawn from the propositions as-serted in the items being related.
For example, Hobbs (1985, p. 25) defines ELABORATIONas follows:ELABORATION: S 1 is an ELABORATION of So if the hearer can inferthe same proposition P from the assertions of So and St.Here $1 represents he current clause or larger segment of discourse, and So an imme-diately preceding segment.
$1 usually adds crucial information, but this is not part ofthe definition, since Hobbs wishes to include pure repetitions under ELABORATION.Hobbs' theory of coherence is attractive because it relates rhetorical relations tothe functions that speakers wish to accomplish in a discourse.
Thus, Hobbs' theorycould potentially tell a text planner what kind of rhetorical relation should be used toachieve aparticular goal of the speaker.
For example, Hobbs (1979) notes two functionsof ELABORATION.
One is to overcome misunderstanding or lack of understanding, andanother is to "enrich the understanding of the listener by expressing the same thoughtfrom a different perspective."
However, note that such specifications of the speaker'sintentions are not an explicit part of the formal definition of the relation.
For thisreason we have chosen an alternative theory of text structure, Rhetorical StructureTheory (RST) (Mann and Thompson 1988), as a basis for our text planning system.In contrast to Hobbs' coherence r lations, the definition of each rhetorical relationin RST indicates constraints on the two entities being related as well as constraints ontheir combination, and a specification of the effect that the speaker is attempting to achieve onthe hearer's beliefs or inclinations.
Thus RST provides an explicit connection between thespeaker's intention and the rhetorical means used to achieve those intentions.
As anexample, consider the RST definition of the MOTIVATION relation shown in Figure 6.As shown, an RST relation has two parts: a nucleus (N) and a satellite (S).
6 TheMOTIVATION relation associates text expressing the speaker's desire that the hearerperform an action (the nucleus) with material intended to increase the hearer's desireto perform the action (the satellite).
For example, in the text below, (1) and (2) arerelated by MOTIVATION:(1) Come to the party for the new President.
(2) There will be lots ofgood food.6 This is an oversimplification.
In fact, there are a small number of RST relations, e.g., SEQUENCE and30INT, that are multinuclear and can relate more than two pieces of text.661Computational Linguistics Volume 19, Number 4relation name: MOTIVATIONconstraints on N: Presents an action (unrealized with respect o N)in which the Hearer is the actor.constraints on S: noneconstraints on N + S combination:Comprehending S increases the Hearer's desire toperform the action presented in N.effect: The Hearer's desire to perform the action presented in Nis increased.Figure 6RST relation--MOTIVATION.motivation enablementFigure 7Graphical representation f an RST schema.The nucleus of the relation is that item in the pair that is most essential to the writer'spurpose.
In the example above, assuming that the writer's intent is to make the hearergo to the party, clause (1) is nuclear.
In general, the nucleus could stand on its own, butthe satellite would be considered a non sequitur without its corresponding nucleus.In our example, without the recommendation to come to the party the satellite in (2) isout of place.
Moreover, RST states that the satellite portion of a text may be replacedwithout significantly altering the intended function of the text.
The same is not truefor the nucleus.
For example, replacing (2) above with:(2') All the important people will be there.does not greatly change the function of the text as a whole.
However, replacing therecommendation in the nucleus, e.g.,(1') Don't go to the party.significantly alters the purpose of the text.RST relations may be combined into schemata that define how a text can be brokendown into smaller units.
Each schema contains a nucleus and zero or more satellitesrelated to the nucleus by one of the RST relations.
RST schemata do not constrain the662Johanna D. Moore and C6cile L. Paris Planning Text for Advisory Dialogsordering of the nucleus and satellites, and relations may occur any number of timeswithin a schema.
Furthermore, the schemata are recursive; text serving as a satellite inone schema may itself consist of a nucleus and any number of satellites.
A graphicaldepiction of one schema defined by Mann and Thompson (1988) appears in Figure 7.This schema consists of a nucleus and two satellites: one providing MOTIVATION forthe material in the nucleus, and the other providing ENABLEMERT for the material inthe nucleus.4.2 Using RST in Text GenerationAlthough originally intended for generation, until recently RST was used primarily asa tool for analyzing texts in order to investigate various linguistic issues.
The analystbreaks the text down into parts called text spans and then tries to find an RST relationthat connects each pair of spans until all pairs are accounted for.
To determine whetheror not a relation holds between two spans of text, the analyst checks to see whetherthe constraints on the nucleus and satellite hold and whether it is plausible that thewriter desires the condition specified in the Effect field.
All of the text is given.
Oneneed only determine whether or not the constraints are satisfied.Using RST in a constructive process, such as generating a response to a question, isa very different task.
For example, in order to produce text that will succeed in gettingthe hearer to perform an action, a system must determine what (if any) informationin its knowledge base could be used to increase the hearer's desire to perform theaction (MOTIVATION), what information could be used to increase the hearer's abilityto perform the action (ENABLEMENT), how much of this information to present, in whatorder, at what level of detail, etc.
Moreover, the theory states that the nucleus andsatellite portions of a text may occur in any order, relations may occur any numberof times, and a nucleus or satellite may be expanded into a text employing any otherrelation at any point.
In order to use RST, a text generation system must have controlstrategies that dictate how to find such knowledge in the knowledge base, when andwhat relations hould occur, how many times, and in what order.Mann (1984) suggested that goal pursuit methods used in artificial intelligencecould be applied to RST for text generation.
Schemata can be viewed as means forachieving the goals stated as their effects, and the constraints on relations as a kindof precondition to using a particular schema.
However, much work must be done toformalize the constraints and effects of the RST relations and schemata in order to useRST in a text generation system.One attempt at formalization was made by Hovy (1991), who operationalized asubset of the RST relation definitions for use as plan operators in a text structuringprocess.
Hovy's structurer employs a top-down planning mechanism to order a givenset of input elements into a coherent text.
To form plan operators from RST relationdefinitions, Hovy maps the intended effect of the relation to the l~esults field ofthe corresponding operator.
In Hovy's system, the contents of the Results field areviewed as the communicative goal(s) that may be achieved by using the associatedrelation, and the relation name as the rhetorical strategy that achieves the goal(s).The constraints on the nucleus, satellite, and their combination that are specified inthe relation definition become subgoals in Hovy's operators.
The relation ame is alsoincluded in the plan operator (and in the evolving plan) so that appropriate connectivescan be inserted in the final text.Figure 8 shows the RST relation definition for the CIRCUMSTANCE relation, and Fig-ure 9 shows Hovy's characterization f this relation as a plan operator.
Note from thisfigure that-Hovy's operator includes fields called growth points.
These post optionalsubgoals, which will be expanded if information satisfying these goals appears in the663Computational Linguistics Volume 19, Number 4relation name: CIRCUMSTANCEconstraints on N: noneconstraints on S: presents a situation (not unrealized)constraints on N + S combination:S sets a framework in the subject matter within which Hearer isintended to interpret he situation presented in N.effect: The Hearer recognizes that the situation presented in S providesthe framework for interpreting N.Figure 8RST relation--CIRCUMSTANCE.Name: CIRCUMSTANCEResults:((BMB SPEAKER HEARER (CIRCUMSTANCE-OF ?X ?CIRC)))Nucleus + Satellite requirements/subgoals:((OR (BMB SPEAKER HEARER (HEADING.R ?X ?CIRC))(BMB SPEAKER HEARER (TIME.R ?X ?CIRC))))Nucleus requirements/subgoals:((BMB SPEAKER HEARER (TOPIC ?X)))Nucleus growth points:(BMB SPEAKER HEARER (ATTRIBUTE-OF ?X ?ATT))Satellite requirements:((BMB SPEAKER HEARER (TOPIC ?CIRC)))Satellite growth points:((BMB SPEAKER HEARER (ATTRIBUTE-OF ?CIRC ?VAL)))Order: (NUCLEUS SATELLITE)Relation phrases: (.... )Figure 9Hovy's RST plan for CIRCUMSTANCE (from Hovy \[1991\] p. 90).set of input items to be expressed.
As Hovy (1991, p. 94) points out, RST operatorswith growth points can be viewed as schemata, and this is how they are used in hisimplementation.
He also argues that by viewing growth points as "suggestions" ratherthan "injunctions," the RST operators can be used in a more "open-ended" approachto planning.
Hovy (1991, p. 98) goes on to suggest a range of criteria that a plannermight use to determine when additional material should be included/excluded, butthese have not been implemented.The operator in Figure 9 says that in order to achieve the state where the speakerbelieves that the hearer and speaker mutually believe that ?
?IRC is a circumstance ofsome event ?X, the speaker can state ?X (this will be the result of posting the nucleussubgoal (BMB SPEAKER HEARER (TOPIC ?X))), and then state the circumstantial infor-mation (this will be the result of posting the satellite subgoal (BMB SPEAKER HEARER(TOPIC ?CIRC))).
The constraints on the Nucleus + Satellite check that whatever isbound to the variable ?CIRC either stands in a HEADING.R or TIME.R relation to theevent bound to ?X.
7 Using this operator, Hovy's structurer can generate portions of7 This particular operator was used in a naval application where the main events were ship movements664Johanna D. Moore and C6cile L. Paris Planning Text for Advisory Dialogstext such as:Knox is en route to Sasebo.
Knox is heading SSW.The second sentence (the satellite) stands in a CIRCUMSTANCE relation to the first sen-tence (the nucleus).
Typically, this text would be only a portion of a larger paragraph.4.3 A Problem with the Straightforward Operationalization of RSTAbove we have argued that, in order for a system to be able to participate in dialogues,it must have an explicit representation f the intentional structure of its own utterances.Further, we have seen that RST provides a link between intentions and rhetoricalrelations, and that RST can be adapted in a straightforward manner for use in a text-structuring task by encoding the specification of the intended effect of an RST relationas the goal that the plan operator can be used to achieve, and the constraints onrelations as the subgoals that must be satisfied.However, in our efforts to use RST to construct a text plan that includes boththe intentions of individual segments of the text and an indication of the rhetoricalrelations between segments, we found such an operationalization to be inadequatein the general case.
More specifically, we found that there is an important distinctionbetween two previously identified classes of RST relations that must be taken intoaccount when attempting to use RST for text planning.
Mann and Thompson (1988,pp.
256-257) break the RST relations into two classes: presentational and subjectmatter.
Presentational relations are those whose intended effect is to increase someinclination in the hearer, such as the desire to act (MOTIVATION), or the degree ofpositive regard for (ANTITHESIS), the degree of belief in (EVIDENCE), or the degreeof acceptance of (JUSTIFY) the information presented in the nucleus.
In contrast, theintended effect of a subject matter elation is that the hearer ecognize that the relationin question holds "in the subject matter."
For example, VOLITIONAL-CAUSE relates twotext spans if the speaker intends the hearer to recognize that the situation presentedin the satellite is a cause for the volitional action presented in the nucleus.The E f fec ts  of the presentational relations can be adopted in a straightforwardmanner as intentions for plan operators.
However, the E f fec ts  of the subject matterrelations are not sufficient for representing speakers' intentions.
The E f fec ts  of subjectmatter elations capture the speaker's intention to make the hearer understand a pieceof subject matter, but do not indicate why the speaker is presenting this information.Consider again the CIRCUMSTANCE relation in Figure 8.
The specified effect of thisrelation is that the hearer knows some circumstance of the situation presented in thenucleus.
As we have seen from Hovy's work, this relation is typically used to provideinformation about the time of an event, the location of an object, etc.
But in order toparticipate in a dialogue, the system must know more than the fact that it intendedto convey the time of an event or location of an object: it must know why it intendedto convey that.
We illustrate the problem with an example.Consider the hypothetical fragment of task-oriented dialogue shown in Figure 10,in which a system is instructing a user in how to take apart a physical device.
Thesystem tells the user to remove the cover.
In order to increase the user's ability toperform the act, the system employs an ENABLEMENT relation that tells the user whattool to use.
Then, in order to help the user find the appropriate tool, the system tells the userwhere the tool is located, a CIRCUMSTANCE.
Now, suppose that we have a text plannerand the circumstances that were important to report were heading and time.
The operator could begeneralized toa wider range of circumstances.665Computational Linguistics Volume 19, Number 4SYSTEMUSERRemove the cover.
You'll need the Phillips screwdriver.
It's in the topdrawer of the toolbox.
Do you have it?No.Figure 10Fragment of hypothetical task-oriented dialogue.\[1\]\[2\]I Relation: ENABLEMENT ~ ER REMOVE-COVER))(COMMAND USER(DO USER REMOVE-COVER))Relation: CIRCUMSTANCEEffect: (BEL USER (CIRC-OF PHILUPS DRAWER-I))(INFORM USER(INSTRUMENT REMOVE-COVER PHILLIPS))(INFORM USER0N-LOCATION PHILLIPS DRAWER-I))Figure 11RST tree for system's utterance.that only records the RST relations used and the speech acts they relate, as shown inFigure 11.
Since each RST relation has a relation name as well as an Ef fect ,  these haveboth been recorded on the arcs representing the relations.Now, let us consider how to respond to the user's "No" in the sample dialogue.Clearly the user is indicating an inability to locate the Phillips screwdriver, i.e., eithershe or he is unable to identify the referent of the term "Phillips screwdriver," or thereis no Phillips screwdriver in the toolbox.
For the sake of this example, let us assumethat the hearer is unable to identify the referent.
In terms of building a system thatcan participate in such dialogues, there are two problems with the representationshown in Figure 11.
First, as the user indicates that he or she cannot identify thePhillips screwdriver, it is clear that the portion of the text that failed is the part thatattempts to uniquely identify the Phillips screwdriver, namely the text generated inthe CIRCUMSTANCE satellite: It's in the top drawer of the toolbox.
However, it is difficultfor a system to determine that this is the portion of text that failed, since the onlyintention represented is that the system wants the hearer to know that the screwdriverbeing in the toolbox is a circumstance of the Phillips screwdriver.
The system has norepresentation f why this bit of circumstantial information is being conveyed.Second, even if the system could identify this as the offending portion of theplan tree, it is very limited in terms of recovery strategies.
In this case, the system'sonly recourse is to try to find different ways of achieving the subgoal (BEL USER(CIRCUMSTANCE-0F PHILLIPS ?CIRC)), i.e., other operators that have this effect butpost different subgoals, or by finding different pieces of information that satisfy theconstraints on the operator of Figure 9, i.e., alternative bindings for the variable ?CIRC,666Johanna D. Moore and C6cile L. Paris Planning Text for Advisory Dialogswhich is currently bound to (IN-LOCATION PHILLIPS DRAWER-I).
But it is likely thatthe location of the Phillips screwdriver is the only piece of circumstantial informationrelevant to identifying the screwdriver.
8 So presenting other circumstantial informationthat may be stored in the knowledge base (e.g., when it was purchased, or how long ithas been in the toolbox) is almost certainly not relevant and should not be mentionedhere.
But how can the system tell what is relevant without knowing why circumstantialinformation is being presented?
What is missing from the representation in Figure 11is a critical piece of information indicating that the speaker is presenting the circum-stantial information because she or he intends this information to allow the hearer toidentify the tool.
(We could denote this intention as (KNOW USER (REF PHILLIPS))).One may argue that the system could recover by trying other operators thatimplement he ENABLEMENT relation.
But again, there are two problems.
First, theremay not be any other such operators whose constraints are satisfied in this situa-tion.
That is, perhaps the only way to remove the cover without damaging it is touse a Phillips screwdriver.
Second, even if there were other operators for implement-ing the ENABLEMENT relation, the system is not behaving intelligently.
It will neverbe able to figure out that it can recover by telling the user some attributive infor-mation about the object it is trying to identify, e.g., The Phillips screwdriver has a yel-low handle and a cone-shaped head.
This is because such a text would be produced byan ELABORATION-0BJECT-ATTRIBUTE relation, not a CIRCUMSTANCE relation.
Again, thisproblem arises because the representation lacks the crucial piece of intentional in-formation: (KNOW USER (REF PHILLIPS)), as well as the information that once thePhillips screwdriver has been mentioned, the rhetorical continuations that can be usedto help achieve the intention (KNOW USER (REF PHILLIPS)) are CIRCUMSTANCE, ELAB-ORATION-OBJECT-ATTRIBUTE, CONTRAST (one could identify an object by contrasting itwith an object known to the user), etc.The argument ultimately hinges on the observation that, in the case of the subjectmatter RST relations, the mapping between intentions and rhetorical relations is nota one-to-one mapping.
In fact, the mapping is many-to-many.
This is similar to theargument we made earlier with respect to schemata of rhetorical predicates.
Becausethe mapping of intentions to rhetorical predicates is not one-to-one, intentions cannotbe recovered from a record of the rhetorical predicates used to produce a text.With respect to subject matter relations, we  have just shown that an intention suchas (KNOW USER (REF PHILLIPS)) may be achieved by a variety of different rhetoricalrelations.
Similarly, a given subject matter relation may be used in service of severaldifferent intentions.
For example, the CIRCUMSTANCE relation can be used when thespeaker wants the hearer to identify the referent of a description ("It's in the toolbox.
")or know a precondition for an action ("You want flight I01.
It leaves at 8:00 pm.')
Inaddition, the CIRCUMSTANCE relation is used when the speaker wants the hearer toknow how entities are temporally or spatially related.
("He volunteered as a classicalmusic announcer.
That was in 1970.
He left to go to graduate school.
That would've been1973.
")Contrast his with the presentational RST relations where the mapping betweenintention and rhetorical relation is a one-to-one mapping.
For example, if the speaker'sgoal is to "increase the hearer's desire to perform the action presented in the nucleus,"then whatever text is used to achieve this goal, it stands in a MOTIVATION relation tothe nucleus.8 This is because the knowledge base search originally retrieved this as the information that is mostrelevant to helping this hearer identify the Phillips screwdriver.667Computational Linguistics Volume 19, Number 4Because of this dichotomy between the two classes of RST relations, we concludethat any approach to discourse structure that relies solely on rhetorical relations orpredicates and does not explicitly encode information about intentions is inadequatefor handling dialogues.
Hovy's (1991) approach suffers from this problem.
9 Moreover,as Moore and Pollack (1992) argue, a straightforward approach to revising such anoperationalization f RST by modifying subject matter operators to indicate associ-ated intentions cannot succeed.
Such an approach "presumes a one-to-one mappingbetween the ways in which information can be related and the ways in which inten-tions combine into a coherent plan to affect a hearer's mental state."
We have justshown examples indicating that no such mapping exists.5.
A Text Planner for Advisory DialoguesIn this section we present a text planner that constructs explanations based on theintentions of the speaker at each step and the linguistic means available for realizingthese intentions.
Given a goal representing the speaker's intention, our planner findsthe linguistic resources available for achieving that goal.
These linguistic resources canbe either speech acts, which are directly satisfiable, or rhetorical strategies indicatinghow what can be said next relates to what has already been said.
While planning, oursystem records both the intentions behind text spans and the rhetorical relation thatholds between each two text spans.This approach improves upon Hovy's work in two ways.
First, as we have seen,Hovy's operationalization f RST relations into plan operators conflates intentionaland rhetorical structure.
In contrast, our plan language preserves an explicit repre-sentation of both intentional and rhetorical knowledge, and thus is more suitable forparticipating in dialogues.
Second, our text planning system is able to select he con-tent to include in its explanations in addition to structuring that content.
Recall thatHovy's system is given a set of input elements to express.
Like Appelt (1985, pp.
6-10),we believe that the tasks of choosing what to say and choosing a strategy for sayingit cannot be divided.
They are intertwined and influence one another in many ways.What knowledge is included in a response greatly depends on the speaker's inten-tion and the linguistic strategy chosen to achieve it.
For example, the information tobe included when describing an object by drawing an analogy with a similar objectwill be quite different from the information to be included in describing the objectby discussing its components.
At the same time, which strategy is chosen to satisfyan intention must depend on what knowledge is available.
For example, whether thesystem chooses to draw an analogy will depend on whether an analogous concept fa-miliar to the hearer is available in the system's knowledge sources.
In our text planner,decisions are made locally each time alternative strategies for achieving a (sub)goalare considered.
The content of the text is not selected a priori.5.1 The Plan LanguageTo enable our planner to construct text plans that explicitly capture the intentionaland rhetorical structures of the text it produces, we distinguish two types of goals:?
Communicative goals.
These represent the speaker's intentions to affectthe beliefs or goals of the hearer.
They are denoted as states, such as the9 To be fair, the goal of Hovy's work was to show that RST could be used to order aset of inputpropositions into a coherent text.
He did not set out to provide asystem that could participate in adialogue.668Johanna D. Moore and C6cile L. Paris Planning Text for Advisory Dialogsstate in which the hearer believes a certain proposition, has a goal toperform an action, or has the knowledge necessary to perform an action.The presence of a communicative goal in a text plan does not cause anytext to be generated directly.
Achieving goals of this type leads to theposting of linguistic goals.Linguistic goals.
These correspond to the linguistic means available tospeakers for achieving their communicative goals.
They lead to thegeneration of text and are of two types: speech acts and rhetorical goals.In our system, we assume that speech acts, such as INFORM or RECOMMEND,can be straightforwardly mapped into utterances that form part of thefinal text.
1?
They are considered primitives by the text planner.
Rhetoricalgoals, such as MOTIVATION and CIRCUMSTANCE, cannot be achieveddirectly and must be refined into one or more subgoals.
The strategiesfor achieving them may post further communicative goals or speech actgoals in order to express atellite information i  a text.
They areintended to establish rhetorical links between text spans, and often causeconnective markers to be generated in the final text.We distinguish these two types of goals for two reasons.
First, the distinction isnecessary to handle the many-to-many mapping between intentions and rhetoricalstrategies for achieving them.
Table 1 summarizes the mapping between intentionsand rhetorical relations we have identified in generating the texts necessary for ourapplication.
These mappings have been encoded in our library of plan operators.
Inthis table, the presentational RST relations appear above the double line.
As shown,there is a one-to-one mapping between these relations and speaker intentions.
How-ever, note that for the subject-matter r lations that appear below the double line, thereis not a one-to-one mapping between rhetorical relations and speaker's intentions.
Ingeneral, there may be many different rhetorical strategies for achieving any given in-tention.
For example, as discussed above and indicated in Table 1, the intention (KNOW?Ix (REF ?object-descr iptor))  can be achieved by telling the hearer circumstantialinformation about the object (CIRCUMSTANCE), by contrasting the object with an ob-ject known to the user (CONTRAST), or by telling the hearer some of the attributes orparts of the object (ELABORATION-0BJECT-ATTRIBUTE or ELABORATION-WHOLE-PART re-spectively).
Moreover, the mapping in Table I makes it clear that a particular rhetoricalstrategy may serve many intentions.
For example, CONTRAST may be used to identifythe referent of a description, to define a new term, to identify the differences betweenentities, to make the hearer believe that a method is the best one for achieving a do-main goal, etc.
As we will see, operators in our plan language achieve intentions byposting rhetorical subgoals and/or speech acts.The second reason for separately and explicitly representing the two types of goalsis so that the completed plan structure will contain an explicit representation of thespeaker's intentions as well as a record of the speech acts and rhetorical strategiesused to achieve them.
As we have argued above, it is essential that the intentionalstructure of a text be recorded so that the system may respond to the user's follow-up questions.
In addition, having the rhetorical structure explicitly noted in the textplan allows the text generator to include discourse markers in the final text in astraightforward manner.
Such markers enhance the coherence of the text and aid thereader in understanding the text as a whole, by helping him or her understand how10 Here we are using the term "speech act" where Appelt (1985) would use "surface speech act.
"669Computational LinguisticsTable 1Intention to rhetorical relation mapping.Volume 19, Number 4Intention Rhetorical Relation(PERSUADED ?h ?proposition) EVIDENCE Presentational(PERSUADED ?h (DO ?h ?act)) MOTIVATION Relations(COMPETENT ?h (COMPREHEND ?h ?x)) BACKGROUND(COMPETENT ?h (DO ?h ?act)) ENABLEMENT(KNOW ?h (REF ?desc)) CIRCUMSTANCE Subject MatterCONTRAST RelationsELABORATION-OBJECT-ATTRIBUTEELABORATION-WHOLE-PART(KNOW-ABOUT ?h ?concept) CIRCUMSTANCECONDITIONCONTRASTELABORATION-GENERAL-SPECIFICELABORATION-OBJECT-ATTRIBUTEELABORATION-SET-MEMBERELABORATION-SPECIFIC-GENERALELABORATION-WHOLE-PARTPURPOSESEQUENCE(BEL ?h (REF (DIFFS ?x ?y) ?d)) CONTRAST(BEL ?h (STEP ?act ?goal)) ELABORATION-GENERAL-SPECIFICELABORATION-PROCESS-STEPSEQUENCE(BEL ?h ?proposition) CONTRASTELABORATION (all types)(BEL ?h (METHOD-FOR ?goal ?method)) CONDITIONCONTRASTMEANSSEQUENCE(BEL ?h (BEST-METHOD-FOR ?goal ?method)) CONCESSIONCONDITIONCONTRASTOTHERWISEthe parts of the text relate to one another (Brewer 1980; Cahour, Falzon, find Robert1990; Ehrlich and Cahour 1991; Goldman and Dur~n 1988; Levy 1979; Meyer, Brandt,and Bluth 1980).
11 In a system such as ours, sets of connectives are associated witheach rhetorical relation, and one can be chosen based on features of the final textbeing produced (e.g., whether or not a sentence boundary occurs between nucleus andsatellite, etc.).
The system need not reason directly about the relationships between theeffects of speech acts to determine a suitable connective a process we believe to bemuch more computationally intensive.5.2 Representing Plan OperatorsOur plan language provides operators for achieving the two types of goals presentedin the previous section.
Each operator consists of:?
an effect: a characterization of the goal that this operator can be used toachieve.
This may be a communicative goal, such as The speaker intends to11 Note that rhetorical structure is not the only source of discourse markers.
They may be used to markshifts in attentional structure, discourse segment boundaries, or aspects of the exchange structure ininteractive discourse (Grosz and Sidner 1986; Redeker 1990; Schiffrin 1987).670Johanna D. Moore and C6cile L. Paris Planning Text for Advisory Dialogsachieve the state in which the hearer believes aproposition or a linguistic goal,such as Establish motivation between an act and a goal or Inform the user of aproposition.?
a constraint list: a list of conditions that should be true in order for theoperator to have the intended effect.
Constraints may refer to facts in thesystem's domain knowledge base, information in the user model,information i  the dialogue history, or information about the evolvingtext plan.?
a nucleus: a subgoal that is most essential to achievement of theoperator's effect.
Every operator must contain a nucleus.?
satel l ites: additional subgoal(s) that may contribute to achieving theeffect of the operator.
An operator can have zero or more satellites.When present, satellites may be required or optional.
Unless otherwiseindicated, a satellite is assumed to be required.5.2.1 Represent ing  Menta l  States.
Before providing examples of operators encodedin the plan language, we introduce the representational primitives used to express theknowledge contained in plan operators.
The 12 predicates listed here were sufficient torepresent the communicative goals that were needed to produce responses to the rangeof questions handled by the PEA system.
Development of other application systems ina range of domains using the text planner described here is underway, and experiencewith these systems will help us determine whether this set of mental states is sufficientor must be extended (Carenini and Moore 1993; Rosenblum and Moore 1993).We express information about agents' mental states in the following terms.
In thenotation below, constants are denoted by symbols written in all uppercase l tters, e.g.,DO, and typed variables are denoted by lowercase symbols beginning with a "?
',  e.g.,?agent.1.
(KN0W-ABOUT ?agent (CONCEPT ?c)): The agent knows of the existenceof the concept ?c.
This does not imply that the agent knows anyparticular properties of the concept, its subconcepts, the instances of thisconcept, or how this concept is used in problem solving.2.
(KNOW ?agent (REF ?descript ion)):  The agent can identify the realworld entity described by ?description.3.
(BEL ?agent (?predicate ?el ?e2)): The agent believes that thetwo-place predicate holds between entities ?el and ?e2.4.
(BEL ?agent (SOMEREF (?predicate ?argl ?arg2 ... ?argN-1))): Theagent believes that there exists some entity(ies) satisfying the N-arypredicate, i.e., there is some referent of this N-ary predicate.5.
(BEL ?agent (REF (?predicate ?argl ?arg2 ...?argN-1)?referent)) :  The agent believes that the referent satisfies the N-arypredicate.
This notation is used when N > 2.
When N = 2, we use thenotation shown in 3 above.6.
(GOAL ?agent ?goal): The agent has adopted the specified omain goal.In this expression, ?goal must be a nonprimitive domain action, i.e., agoal that requires further efinement before it can be achieved.671Computational Linguistics Volume 19, Number 47.
(GOAL ?agent (D0 ?agent ?action)): ?agent has adopted a goal toperform the specified action.
The action must be a primitive in thedomain.8.
(PERSUADED ?agent (DO ?agent ?act)):  The agent is persuaded toperform the action at some unspecified time in the future.
This is howwe have chosen to represent the RST effect Increase the hearer's desire toperform an action.9.
(PERSUADED ?agent (ACHIEVE ?agent ?goal)): The agent is persuadedto adopt the goal.10.
(PERSUADED ?agent ?proposition): The agent is persuaded thatproposition is true.
This is how we have chosen to represent the RSTeffect Increase the hearer's belief in a proposition.11.
(COMPETENT ?agent (DO ?agent ?act)):  The agent knows theinformation ecessary to perform the primitive domain action.12.
(COMPETENT ?agent (ACHIEVE ?agent ?goal)): The agent knows amethod for achieving the nonprimitive domain action ?goal.Representing Communicative and Linguistic Goals.
Communicative goals representthe speaker's intention to produce a certain effect on the hearer's mental state, e.g., tomake the hearer believe some proposition or adopt a goal to perform a certain action.In the plan language, communicative goals are written simply in terms of mentalstates of the hearer.
When a goal of the form (BEL ?hearer ?proposit ion) appearsin a text plan, it should be read as The speaker intends to achieve the state where ?hearerbelieves ?proposition.
When (BEL ?hearer ?proposit ion) appears in the effect field ofan operator, it indicates that the operator is capable of having this effect on the hearer'smental state.To achieve a communicative goal, operators post linguistic (rhetorical and speechact) subgoals.
Rhetorical goals are of the form (relation-name argl ... argN), whererelation-name is one of the relations defined in RST.
An expression of this formrepresents a goal to establish the rhetorical relation between the entities listed as argu-ments.
For example, (MOTIVATION REPLACE-SETQ-WITH-SETF ENHANCE-READABILITY)indicates the speaker's rhetorical goal to establish that the domain goal ENHANCE-READ-ABILITY is motivation for the act REPLACE-SETQ-WITH-SETF.Communicative and rhetorical goals are eventually refined into primitive actionsthat can be executed to cause changes in the hearer's mental state.
In our planningformalism, we treat speech acts as primitive.
Speech act goals thus appear at the leafnodes of text plans.
There are currently four speech acts in our system: INFORM, ASK,RECOMMEND, and COMMAND.Finally, there are two special forms in the plan language: FORALL and SETQ.
Thesemay appear in the nucleus or satellite fields of plan operators.
FORALL has the form(FORALL ?variable-name ?goal).A FORALL clause in a nucleus or satellite field causes an instance of the goal, ?goal, tobe posted for each possible binding of the variable named by ?variable-name.
?goalmay be a communicative goal, a rhetorical goal, or a speech act.
Its specification mustcontain an occurrence of the variable named by ?variable-name.
An example of thisspecial form appears in the nucleus of the plan operator in Figure 14.672Johanna D. Moore and C6cile L. Paris Planning Text for Advisory DialogsThe SETQ special form is expressed as:(SETQ ?variable-name ?expression).SETQ causes the variable named ?variable-name to be bound to the result of eval-uating the expression ?expression.
This special form is useful for assigning valuesto variables that will be used in later steps of the nucleus or satellites.
An exampleappears in the operator in Figure 16.Representing Operator Constraints.
Constraints on the user's knowledge and goalsare expressed using the notation given above for representing the hearer's mentalstates.
Any of the expressions described above can appear in the constraint list of anoperator.
For example, if the expression(KNOW-ABOUT USER (CONCEPT STORAGE-LOCATION))appears in the constraint field of an operator, it indicates that in order to use theoperator (without making assumptions), the hearer should be familiar with the conceptSTORAGE-L0CATION.Constraints on the expert system's knowledge are of the form (7predicate ?argl... ?arg-N), where ?predicate is an N-ary predicate referring to the expert system'sdomain knowledge.
Since the expert system's knowledge base is made up of severalcomplex data structures, predicates are not tested by a simple unification process.Instead an access function must be written for each predicate in order to test if aninstantiated predicate is true, or to find acceptable bindings when a predicate con-tains variables in some argument positions.
The range of predicates over the domainknowledge is quite large, and therefore we will not enumerate them here.
We provideEnglish paraphrases of knowledge base constraint predicates wherever they appear inexamples.Finally, constraints may refer to the dialogue history or the status of the current textplan under construction.
There are currently two types of constraints in this category.First, there are constraints that indicate whether the operator can be used in nucleusor satellite position in a text plan.
The clause (NUCLEUS) in the constraint field of anoperator, indicates that this operator can be used to expand the nucleus branch of atext plan.
Likewise, the clause (SATELLITE) indicates that the operator can be used toexpand a satellite branch.Second, there are constraints on the focus of attention.
We are currently using asimple implementation of local focus rules based on the work of Sidner (1979) andMcKeown (1985).
We have found the need for two such constraints in the operatorswe have encoded thus far:?
(CURRENT-FOCUS ?ent i ty) :  indicates that the operator can be used if?ent i ty  is currently in focus.?
(IN-POTENTIAL-F0CUS-LIST ?ent i ty) :  indicates that the operator can beused if ?ent i ty  is on the list of items that have just been mentioned, andtherefore could become the next focus.5.2.20perationalizing RST Schemata.
Given these notational conventions, let us con-sider how we operationalize an RST schema in our plan language.
In general, severaloperators are required to represent the knowledge in an RST schema.
For example,Figures 12, 14, and one of Figures 15 or 16 operationalize a portion of the RST schema673Computational Linguistics Volume 19, Number 4In Plan Language Notation:EFFECT: (GOAL ?hearer (DO ?hearer ?act))CONSTRAINTS: (NUCLEUS)NUCLEUS:(RECOMMEND ?speaker ?hearer (DO ?hearer ?act))SATELLITES:(((PERSUADED ?hearer (DO ?hearer ?act)) *optional*)((COMPETENT ?hearer (DO ?hearer ?act)) *optional*))English Paraphrase:To make the hearer want to do an act,IF this text span is to appear in nucleus position, THEN1.
Recommend the actAND optionally,2.
Achieve state where the hearer is persuaded to do the act3.
Achieve state where the hearer is competent to do the actFigure 12Plan operator for recommending an act.motivation enablementFigure 13Graphical representation f an RST schema.depicted in Figure 7, repeated in Figure 13 for the reader's convenience.
The operatorshown in Figure 12 says that one way of achieving the state where the hearer has thegoal of performing an action is to recommend the act (the nucleus), and, optionally,to post a subgoal to achieve the state where the hearer is persuaded to perform therecommended act (the first satellite), and, optionally, to post a subgoal to achieve thestate where the hearer has the knowledge necessary to perform the act (the secondsatellite).
RECOMMEND is considered a primitive in our system, and can be mapped i-rectly into a specification for the sentence generator.
Therefore no operators are neededto achieve it.However, the system does require operators for achieving the two satellite sub-goals.
Figure 14 shows an operator that can be used to achieve the first satellite.Informally, this plan operator states that if an act is a step in achieving some goal(s) of674Johanna D. Moore and C6cile L. Paris Planning Text for Advisory DialogsIn Plan Language Notation:EFFECT: (PERSUADED ?hearer (DO ?hearer ?act))CONSTRAINTS: (AND (STEP ?act ?goal)(GOAL ?hearer ?goal)(MOST-SPECIFIC ?goal)(CURRENT-FOCUS ?act)(SATELLITE))NUCLEUS: (FORALL ?goal(MOTIVATION ?act ?goal))SATELLITES: nilEnglish Paraphrase:To achieve the state in which the hearer is persuaded to do an act,IF the act is a step in achieving some goal(s) of the hearer,AND the goal(s) are the most specific along any refinement pathAND act is the current focus of attentionAND the planner is expanding a satellite branch of the text planTHEN motivate the act in terms of those goal(s).Figure 14Plan operator for persuading user to do an act.the hearer, the speaker can persuade the hearer to perform the action by motivating theaction in terms of those goals.
This plan operator thus indicates that the communica-tive goal of persuading the hearer to do an act can be achieved by using the rhetoricalstrategy MOTIVATION.
This operator thus links the intention with the rhetorical meansused to achieve it.Finally, the system needs an operator that can achieve a MOTIVATION subgoal.
Inour current operator library, there are several such operators.
Two of these are shownin Figures 15 and 16.
The operator in Figure 15 is very general and can be used tomotivate any action in terms of a hearer goal that the act may help to achieve.
It postsa subgoal to make the hearer believe that the act is a step in achieving the goal.
Inthe simplest case, this subgoal will be refined directly into a surface speech act thatinforms the hearer of this fact.
The operator in Figure 16 is a more specific operatorand can be used only when the act to be motivated is a replacement (e.g., replace SETQwith SETF).
In this case, one strategy for motivating the act is to compare the objectbeing replaced and the object hat replaces it with respect o a goal of the hearer.The three operators hown in Figures 12, 14, and 15 together form one opera?tionalization of a portion of the RST schema shown in Figure 13.
Referring back tothis schema nd the definition of the RST relation MOTIVATION shown in Figure 6, thereader will note that these three operators can be used to produce the nucleus and theMOTIVATION satellite portion of the schema.
To complete the operationalization f thefull schema shown in Figure 13, we must also provide operators to refine the secondoptional satellite, (COMPETENT ?hearer (DO ?hearer ?act)) .
For the sake of brevity,we have omitted these operators here.There are three important points to note about our operationalization.
First, op-erators like the one shown in Figure 14 provide an explicit link between speakerintentions and the rhetorical means that achieve them.
In the case of this particular op-erator, there is actually a one-to-one mapping between the intention (Achieve state where675Computational Linguistics Volume 19, Number 4EFFECT: (MOTIVATION ?act ?goal)CONSTRAINTS: (AND (STEP ?act ?goal)(GOAL ?hearer ?goal))NUCLEUS: (BEL ?hearer (STEP ?act ?goal))SATELLITES: nilFigure 15Plan operator for motivating any action by stating the shared goals that act is a step inachieving.EFFECT: (MOTIVATION ?act ?goal)CONSTRAINTS: (AND (STEP ?act ?goal)(GOAL ?hearer ?goal)(ISA ?act REPLACE))NUCLEUS: ((SETQ ?replacee (FILLER-OF OBJECT ?act))(SETQ ?replacer (FILLER-OF GENERALIZED-MEANS ?act))(BEL ?hearer(SOMEREF (DIFFERENCES-WRT ?replacee ?replacer ?goal))))SATELLITES: nilFigure 16Plan operator for motivating a replace act by describing differences between replacer andreplacee.hearer is persuaded to do an act) and the rhetorical strategy for achieving it (MOTIVATION).However, as Table 1 shows, this is not the case for subject matter relations.
By pro-viding operators that explicitly link intentions with rhetorical strategies, our systemcan handle the many-to-many mapping between intentions and the communicativestrategies that achieve them.
So, to return to the Phillips screwdriver example, oursystem would have several plan operators for achieving the intention: (KNOW ?hearer(REF ?descr ipt ion)) .
One plan operator would indicate that a rhetorical strategyfor achieving this goal is to tell the hearer the location of the object (CIRCUMSTANCE),another would indicate that another way to achieve the goal is to tell the hearer theidentifying attributes of the object (ELABOKATION-0BJECT-ATTKIBUTE), etc.Second, operators like the one shown in Figure 14 contribute to the computa-tional efficiency of the system.
These operators encode knowledge about the rhetoricalstrategies that may be used to satisfy particular intentions.
Other operators, e.g., theMOTIVATION operators hown in Figures 15 and 16, encode different methods for real-izing these rhetorical strategies in different situations.
Thus we have provided a planlanguage that preserves an explicit representation f the intention behind each portionof the text, while maintaining the efficiency advantages that originally motivated theuse of schemata of rhetorical predicates or relations for natural anguage generation.Third, as illustrated by the two alternative operators for achieving a MOTIVATIONsubgoal shown in Figures 15 or 16, in our plan language we can represent very generalstrategies that are applicable across domains, as well as very specific strategies thatmay be necessary to handle the idiosyncratic language used in a particular domain.While one may argue that the operator in Figure 16 could be replaced by a more gen-eral, domain-independent operator, this does not obviate the need for domain-specificcommunication strategies.
Rambow (1990) argues that domain-specific communicationknowledge must be used (whether implicitly or explicitly) in all planned communica-676Johanna D. Moore and C6cile L. Paris Planning Text for Advisory Dialogstion, and advocates that domain communication knowledge be represented xplicitly.In our plan language, some types of domain-specific communication strategies can berepresented in plan operators.
When there are multiple operators capable of achiev-ing a given effect, the constraint mechanism controls which operators are deemedappropriate in a given context.
Note, however, that we do not wish to claim that atop-down planning formalism that maps speakers' intentions to rhetorical and speechacts can or should be used to generate all text types.
In particular, Kittredge, Korelsky,and Rambow (1991) have shown that RST cannot account for the structure of reporttexts.
Moreover, they argue that report generation does not require reasoning about hespeaker's intentions to affect he mental attitudes of the hearer.
For report generation,they advocate a data-driven approach in which domain communication k owledgeplays a central role.5.3 Constraints Integrate Multiple Sources of KnowledgeOperators contain applicability constraints that specify the knowledge that must beavailable and the state of the text-planning process that must exist if the operatoris to be used.
These constraints integrate multiple sources of knowledge; they mayrefer to the expert system's knowledge bases, the user model, the dialogue history, orthe evolving text plan.
To our knowledge, our text planner is unique in its explicitrepresentation f constraints from all of these knowledge sources.It is important to recognize that in our formalism, constraints do more than justlimit the applicability of an operator.
They also specify the type of knowledge to beincluded in an explanation, so that the process of satisfying constraints causes theplanner to find information that will be included in the text.
Thus, the selection ofinformation to be presented and the determination f how to present that informationare truly integrated in our planning model.For example, when attempting toapply the operator shown in Figure 14, the vari-ables ?act and ?hearer will be bound.
What is not yet determined is which domaingoals should be used to motivate the act.
Checking to see if the first three constraintson this operator are satisfied causes the planner to search its knowledge sources to findacceptable bindings for the variable ?goal.
The first constraint, (STEP ?act ?goal),says that there must be some domain goal(s) that the act is a step in achieving.
Satis-fying this constraint requires the planner to search the expert system's domain plan-ning knowledge for such goals.
The second constraint, (GOAL ?hearer ?goal), furtherspecifies that any such domain goals must be goals of the hearer (user).
To check thisconstraint, he system must inspect he user model.
The third constraint requires that?goal be the most specific goal along any refinement path satisfying the first twoconstraints.The last two constraints of the operator in Figure 14 refer to the evolving textplan.
The fourth constraint, (CURRENT-FOCUS ?act), says that this operator can beused when the act is the current focus of attention.
The fifth and final constraint,(SATELLITE), says that this operator can only be used when the planner is workingon a satellite branch of the current ext plan.In our system, constraints are treated differently depending on the knowledgesource to which they refer.
We consider the expert system's domain knowledge to becomplete and correct.
Therefore, constraints referring to the expert system's knowl-edge bases are considered "rigid."
If any one of these constraints fails to be satisfied,the operator is rejected from consideration immediately.
In contrast, we do not assumethat the system's model of the user is either complete or correct.
Thus, constraints re-ferring to the user's knowledge state are treated more loosely.
They specify what theuser should know in order to understand the text that will be produced by the oper-677Computational Linguistics Volume 19, Number 4ator.
However, when selecting an operator, if the user model contains no informationrelevant to a particular constraint, he planner may simply assume that the constraintis satisfied.
When it does so, the assumption is recorded in the plan structure so thatthis assumption can be questioned if the explanation is not understood.In the current implementation, constraints on the dialogue history and evolvingtext plan are rigid.
However, we are exploring the idea of treating these as precondi-tions, since, if they are not true in a given situation, they could be made true by gener-ating some additional text.
For example, if a constraint such as (IN-POTENTIAL-FOCUS-LIST ?act) is not true, it can be made true by using rhetorical techniques to intro-duce ?act, if it is a new topic, or to return to ?act, if it has been mentioned before.User model constraints can also be treated as preconditions in cases where the systemhas the underlying knowledge to support explanations that could make them true.We would like to provide our text planner with the ability to choose between mak-ing an assumption or planning text to satisfy a user model constraint.
Modifying thearchitecture to support his type of reasoning is relatively straightforward.
The moredifficult problem is to identify heuristics that guide the text planner in making themost effective choices.
When we incorporate the notion of preconditions into our planoperators, we will make a distinction between constraints that the system can try tosatisfy and those that must already be satisfied before the operator can be applied.This distinction was first made by Litman and Allen (1987) and later by Maybury(1992).5.4 The Planning MechanismAn overview of the explanation generation facility and its relation to other compo-nents in the system is shown in Figure 17.
The text planner produces a plan for anexplanation using the operators in its plan library.
The planning process begins whena communicative goal is posted.
This may come about in one of two ways.
First, inthe process of performing a domain task, the expert system may need to communicatewith the user, e.g., to ask a question or to recommend that the user perform an action.To do so, it posts a communicative goal to the text planner.
Alternatively, the usermay request information from the system.
In this case, the query analyzer interpretsthe user's question and formulates a communicative goal.
Note that a communica-tive goal such as Achieve the state where hearer knows about concept c is really an abstractspecification of the response to be produced.When a goal is posted, the planner identifies all of the potentially applicable oper-ators by searching its library for all operators whose effect field matches the goal.
Tomake this search more efficient, plan operators are stored in a discrimination networkbased on their effect field.
For each operator found, the planner then checks to see ifall of its constraints are satisfied.
Those operators whose constraints are satisfied (or, inthe case of user model constraints, can be assumed to be satisfied) become candidatesfor achieving the goal.From the candidates, the planner selects an operator based on several factors,including what the user knows (as indicated in the user model), the conversation thathas occurred so far (as indicated in the dialogue history), the relative specificity ofthe candidate operators, and whether or not each operator equires assumptions tobe made.
The knowledge of preferences i  encoded into a set of selection heuristics.A discussion of the selection process is beyond the scope of this paper; see Moore(in press) for details.
Once a plan operator has been selected, it is recorded in thecurrent plan node as the selected operator, and all other candidate plan operators arerecorded in the plan node as untried alternatives.
If the operator chosen requires anyassumptions to be made, they are also recorded in the plan node.
The planner then678Johanna D. Moore and C6cile L. Paris Plannir~g Text for Advisory DialogsExpert SystemKnowledgeBases cornmunicaUve - ' - '1  goalUserModeltextplannergrammarinterfacesentencespecificaUonsPenman textgenerationsystemdialogue plan J EnglishoperatorsExplanation GeneratorUserresponseFigure 17Architecture of explanation system.expands the selected plan operator by posting its nucleus and required satellites assubgoals to be refined.Recall that in Section 4.2 we noted that a text generator must decide on the orderingof the text spans it produces.
When instantiating an operator, the planner must decideon the ordering of the subgoals in the nucleus and satellite.
There are really twoordering issues: (1) for each satellite, the planner must determine whether that satelliteshould appear before or after the nucleus, and (2) whenever there are multiple subgoalsin a set of satellites, the planner must determine the order of these subgoals.In our current framework, the second type of ordering knowledge is compiledinto our text-planning strategies.
That is, subgoals hould be expanded in the order inwhich they appear in the plan operator.
This is the approach taken by most systemsemploying schemata, e.g., McKeown (1985); Paris (1991b), and is also common insystems that make use of linear planners, e.g., Cawsey (1993); Hovy (1991); Maybury(1992).
Again, this leads to computational efficiency, since the planner does not haveto reason about ordering among sibling subgoals.
However, some flexibility is lost.
Weplan to investigate this tradeoff in our future work.To solve the first ordering problem, we appeal to ordering information provided byRST.
Although RST does not strictly constrain ordering, Mann and Thompson (1988)have observed that, for some relations, one ordering is significantly more frequent than679Computational Linguistics Volume 19, Number 4the other.
In RST, these ordering observations are treated as "strong tendencies" ratherthan constraints.
For example, in an EVIDENCE relation, the nuclear claim usually pre-cedes the satellite that provides the evidence for the claim; in contrast, in a BACKGROUNDrelation, the satellite providing the background information ecessary to understandthe nuclear proposition usually precedes the nucleus.
Our planner expands the nu-cleus before the satellite xcept for those relations that have been identified as havinga typical order of satellite before nucleus.Finally, the planner must decide when to expand optional satellites.
In the currentimplementation, the planner has two modes, terse and verbose.
In terse mode, nooptional satellites are expanded.
In verbose mode, each satellite is checked againstthe user model, and those that do not duplicate the user's existing knowledge areexpanded.
For example, the plan operator in Figure 12 has two satellites.
The firstsatellite calls for persuading the hearer to perform the act.
The second satellite callsfor making the hearer competent to perform the act and would, if expanded, provideany information the hearer needs to know in order to be capable of performing theact.
These satellites are both marked "optional," indicating that it would be sufficientto simply state the recommendation.
I  verbose mode, the planner will check eachof these satellites against he user model.
To avoid expanding the first satellite, theuser model would have to contain information indicating that the user already hasthe goal of performing the recommended act.
However, note that this would never bethe case, since if it were, the system would not be planning text to make the heareradopt the goal of doing the act.
That is, it would not be expanding this operator inthe first place.
The second satellite provides a more interesting example.
In verbosemode, if the user model indicates that the user knows how to perform replacementactions, the second satellite will not be expanded, since the system believes that theuser has the knowledge necessary to perform the recommended act.The planner maintains an agenda of pending oals to be satisfied.
When instanti-ating a plan operator, it creates a new node for the nucleus ubgoal and puts it intoa list.
The planner then expands each of the required satellites and adds them to thislist.
If the satellite is one that should precede the nucleus, the new satellite node isappended to the front of the list.
Otherwise, it is appended to the back of this list.
Theplanner then considers each of the optional satellites.
For each of the ones it decidesto expand, the planner creates a new node and adds it to the appropriate end of thelist.
The list is then appended to the front of the agenda of goals to be expanded.
Inthis way a text plan is built in depth-first order.When a speech act is reached, the system constructs a specification that directsthe realization component, Penman (Mann and Matthiessen 1985; Penman NaturalLanguage Generation Group 1989), to produce the corresponding English utterance.The system builds these specifications based on the type of speech act, its arguments,and the context in which it occurs .
12 As the planner examines each of the argumentsof the speech act, new goals may be posted as a side effect.
If one of the argumentsis a concept hat the user does not know (as indicated in the user model), a satellitesubgoal to define this new concept is posted.
In addition, if the argument is a complexdata structure that cannot be realized irectly by English lexical items, the planner willhave to "unpack" this complex structure, and, in so doing, will discover additionalconcepts that must be mentioned.
Again, if any of these concepts is not known to theuser, subgoals to explain them are posted.
An example of this phenomenon is given12 Bateman and Paris (1989, 1991) are investigating the problem of phrasing utterances for different typesof users and situations.680Johanna D. Moore and C6cile L. Paris Planning Text for Advisory DialogsSYSTEM What characteristics of the program would you like to enhance?USER Readability and maintainability.SYSTEM You should replace (SETQ X 1) with (SETF X 1).
SETQ can only be usedto assign a value to a simple-variable.
In contrast, SETF can be used toassign a value to any generalized-variable.
A generalized-variable is astorage location that can be named by any accessor function.Figure 18Sample dialogue.\[11\[2\]\[3\]in the next section.
In this way, our system can opportunistically define a new termwhen the need arises.
Contrast this approach to the schema-based approach describedearlier.
Recall that handling this type of phenomenon with schemata would require thatthe definition of each schema explicitly include an optional Ident i f i ca t ion  predicateafter every entry in every schema.Planning is complete when all subgoals in the text plan have been refined tospeech acts.
It is important to note that text planning proceeds in such a way thatspeech acts are planned in the order in which they will appear in the final text.
This providestwo advantages.
First, the text plan is a record of the system's utterances in the order inwhich they are generated.
Because our text plans also include the intentional structureof the final text, focus information can be derived from a completed text plan, andthere is no need to maintain a separate data structure for managing focus information.Second, by doing the planning in this manner, the planner can easily be extended forincremental generation i which planning and realization are interleaved.6.
Participating in Explanatory Dialogues: An ExampleIn this section, we provide an example illustrating how our system constructs a textplan for recommending anaction.
We contrast the text plan produced by our systemwith the schema representation we showed in Figure 4, and show how the text planmay then be used to handle two follow-up questions.
See Moore (in press) for a moredetailed iscussion of the planning process and additional examples.Let us return to the sample dialogue with the PEA system that was shown inFigure 3 and that we include again in Figure 18 for the reader's convenience.
In thisdialogue the user indicates a desire to enhance the readability and maintainability ofhis or her program.
To enhance maintainability, he expert system determines that theuser should replace SETQ with SETF.
To recommend this transformation, the expertsystem posts the communicative goal (GOAL USER (DO USER REPLACE-I)) to the textplanner.
This goal says that the speaker would like to achieve the state where thehearer has adopted the goal of performing the act REPLACE-1.A plan operator capable of satisfying this goal was shown in Figure 12.
The nu-cleus is expanded first, causing (RECOMMEND SYSTEM USER (DO USER REPLACE-I)) tobe posted as a subgoal.
RECOMMEND is a speech act goal that can be achieved irectly,and thus expansion of this branch of the plan is complete.
Focus information--the cur-rent focus and the potential focus list--is also updated at this point.
In this case, thecurrent focus is the act REPLACE-1 and the potential focus list includes the participantsin this act, i.e., USER, SETQ, and SETF.Next, the planner must expand the satellites.
Since both satellites are optional inthis case, the planner must decide which, if any, are to be posted as subgoals.
For thepurposes of this example, assume that the planner is in verbose mode and that the usermodel indicates that the user has the knowledge necessary to perform replacement681Computational Linguistics Volume 19, Number 4acts (i.e., he or she knows how to use the text editor).
Thus, only the first satellite willbe expanded, posting the communicative subgoal to achieve the state where the user ispersuaded to perform the replacement, i.e., (PERSUADED USER (DO USER REPLACE-I)).A plan operator for achieving this goal using the rhetorical relation MOTIVATION wasshown in Figure 14.When attempting to satisfy the constraints of the operator in Figure 14, the systemfirst checks the constraint (STEP REPLACE-I ?goal).
This constraint states that, inorder to use this operator, the system must find a domain goal, ?goal, which REPLACE-Iis a step in achieving.
To find such goals, the planner searches the expert system'sproblem-solving knowledge.
A detailed discussion of the expert system framework weuse is beyond the scope of this paper.
However, it is important to note that the type ofexplanation capability we are describing in this paper places stringent requirements onthe way domain knowledge is represented and used in reasoning.
Interested readersmay find a thorough treatment of this topic in Swartout (1983), Clancey (1983), Neches,Swartout, and Moore (1985), Swartout, Paris, and Moore (1991), and Moore and Paris(1991).In this example, the applicable xpert system goals, listed in order from most toleast specific, are:apply-SETQ-t o-SETF-transf ormationapply-local-transformat ions-whose-rhs-use-is-more-general-than-lhs-useapply-local-t ransI ormat ions-that-enhance-maint ainabilit yapply-transformations-that-enhance-maintainabilityenhance-maintainabilityenhance-programThus, six possible bindings for the variable ?goal result from the search for domaingoals that REPLACE-I is a step in achieving.The second constraint of the current plan operator, (GOAL ?hearer ?goal), is aconstraint on the user model stating that ?goal must be a goal of the hearer.
Not allof the bindings found so far will satisfy this constraint.
Those that do not will not berejected immediately, however, as we do not assume that the user model is complete.Instead, they will be noted as possible bindings, and each will be marked to indicatethat, if this binding is used, an assumption is being made, namely that the binding of?goal is assumed to be a goal of the user.
The selection heuristics can be set to tellthe planner to prefer choosing bindings that require no assumptions tobe made.In this example, since the user is employing the system to enhance a program andhas indicated a desire to enhance the readability and maintainability of the program,the system infers the user shares the top-level goal of the system (ENHANCE-PROGRAM),as well as the two more specific goals ENHANCE-READABILITY and ENHANCE-MAINTAIN-ABILITY.
Of these two more specific goals, only ENHANCE-MAINTAINABILITY is on therefinement path leading to the act REPLACE-I.
Therefore, the two goals that completelysatisfy the first two constraints of the operator shown in Figure 14 are ENHANCE-PROGRAMand ENHANCE-MAINTAINABILITY.
Finally, the third constraint indicates that only themost specific goal along any refinement path to the act should be chosen.
This con-straint encodes the explanation principle that, in order to avoid explaining parts ofthe reasoning chain that the user is familiar with, when one goal is a subgoal of an-other, the goal that is lowest in the expert system's refinement structure, i.e., mostspecific, should be chosen.
Note that ENHANCE-MAINTAINABILITY is a refinement ofENHANCE-PROGRAM.
Therefore, ENHANCE-MAINTAINABILITY is now the preferred candi-date binding for the variable ?goal.The last two constraints of the operator are also satisfied: REPLACE-1 is the currentfocus, and the operator is being used to expand a satellite branch of the text plan.682Johanna D. Moore and C6cile L. Paris Planning Text for Advisory DialogsThe plan operator is thus instantiated with ENHANCE-MAINTAINABILITY as the bindingfor the variable ?goal.
The selected plan operator is recorded as such, and all othercandidate operators are recorded as untried alternatives.The nucleus of the chosen plan operator is now posted, resulting in the subgoal(MOTIVATION REPLACE-I ENHANCE-MAINTAINABILITY).
The plan operator chosen forachieving this goal is the one shown in Figure 16.
This operator motivates the replace-ment by describing differences between the object being replaced and the object re-placing it with respect to the user's goal, i.e., by posting the subgoal (BEL USER (SOME-REF (DIFFERENCES-WRT-GOAL SETF-FUNCTION SETQ-FUNCTION ENHANCE-MAINTAINAB-ILITY) )).
Although there are many differences between SETQ and SETF, only the dif-ferences relevant to the domain goal at hand (ENHANCE-MAINTAINABILITY) should beexpressed.The relevant differences are determined in the following way.
From the expertsystem's problem-solving knowledge, the planner determines what roles SETQ andSETF play in achieving the goal ENHANCE-MAINTAINABILITY.
In this case, the system isenhancing maintainability by applying transformations that replace a specific constructwith one that has a more general usage.
SETQ has a more specific usage than SETF, andtherefore the comparison between SETQ and SETF should be based on the generalityof their usage.
Thus, the goal:(BEL USER (SOMEREF (DIFFERENCES-WRT-GDAL SETQ-FUNCTIONSETF-FUNCTIONENHANCE-MAINTAINABILITY)))posts the single subgoal(BEL USER (REF (DIFFERENCE SETF-FUNCTION SETQ-FUNCTION) USE)).To satisfy this goal, the system uses an operator that informs the user that SETQ canbe used to assign a value to a simple variable, and contrasts this with the use ofSETF.
Focus information is again updated at this point.
SETF becomes the currentfocus, and USE, ASSIGN-T0-GV and its arguments, VALUE and GENERALIZED-VARIABLE,become potential foci.Finally, the text planner expands the speech act(INFORM SYSTEM USER (USE SETF-FUNCTION ASSIGN-T0-GV))in order to form a specification for the surface generator.
In doing so, the system mustexpress the complex concept ASSIGN-T0-GV whereASSIGN-T0-GV = (ASSIGN (OBJECT VALUE)(DESTINATION GENERALIZED-VARIABLE)).When expressing processes such as ASSIGN, the system expresses the process it-self, as well as the participants (e.g., OBJECT) involved in and circumstances (e.g.,DESTINATION) surrounding the process.
In this case, to express the concept ASSIGN-T0-GV, the system will express the assignment action, the object being assigned (i.e.,VALUE), and the destination of the assignment (i.e., GENERALIZED-VARIABLE).
To under-stand the final utterance that will be generated, the listener must know the conceptsASSIGN, VALUE, and GENERALIZED-VARIABLE.When building specifications for complex processes uch as ASSIGN-T0-GV, theplanner checks each of the fillers (e.g., VALUE, GENERALIZED-VARIABLE) of the roles (e.g.,OBJECT, DESTINATION) of the concept (e.g., ASSIGN) to determine if the user knows that683Computational Linguistics Volume 19, Number 4filler.
If so, the planner can simply mention that filler concept by name in the generatedtext.
If, on the other hand, the user model does not indicate that the user is familiarwith the concept o be mentioned, the planner must either make an assumption thatthe user knows the concept (if the planner is in terse mode) or post a subgoal to makethe hearer know this concept o the front of its current agenda (if the planner is inverbose mode).In the current example, recall that the planner is in verbose mode and furthersuppose that the user model indicates that the user knows the following concepts:CAR-FUNCTIONCDR-FUNCTIONSETQ-FUNCTIONCAR-0F-CONSCDR-0F-CONSSIMPLE-VARIABLEASSIGNVALUEThus, the user model indicates that the user knows the concepts ASSIGN and VALUE buthas no indication that the user knows the concept GENERALIZED-VARIABLE.
As a result,the system posts a subgoal to make the user know this concept, i.e., (KNOW-ABOUT USER(CONCEPT GENERALIZED-VARIABLE) ).
Since GENERALIZED-VARIABLE is a member  of thepotential focus list, no special care need be taken to introduce it.
This goal can thusbe achieved by elaborating on the previous text to define this new term.
This is donewith a plan operator that describes concepts by stating their class membership anddescribing their attributes.The text plan for response 3 of the sample dialogue is now completed, and it isshown in its entirety in Figure 19.
Contrast this text plan with the instantiated schemarepresentation for the same utterance shown in Figure 4.
Note that in addition to rep-resenting rhetorical relations between portions of the text (e.g., MOTIVATION, CONTRAST,and ELABORATION) that are analogous to the rhetorical predicates contained in theschema, the text plan includes the intentional structure of the text (as shown in Fig-ure 5).
Although in this case the text span boundaries coincide with the text segment(as defined by Grosz and Sidner) boundaries, this need not always be the case.
In RST,a text span is either a minimal unit or a schema application made up of one or morerelations between minimal units.
The minimal unit is "essentially a clause, except thatclausal subjects and complements and restrictive relative clauses are considered partsof their host clause rather than as separate units" (Mann and Thompson 1988, p. 248).Thus, at least at the lowest level of analysis, text spans can be determined on the basisof syntactic structure alone.
For Grosz and Sidner, intentions are the basic determinerof discourse segmentation.
A segment must have an identifiable discourse segmentpurpose (DSP), and embedding relationships between segments are a surface reflec-tion of relationships among their associated DSPs (Grosz and Sidner 1986, pp.
177-178).Therefore, text spans and text segments are based on quite different criteria.In a text plan produced by our system, any subtree headed by a communicativegoal (i.e., an intention) corresponds to a discourse segment in Grosz and Sidner'stheory.
A segment may consist of more than one span.
In such cases, the spans willbe connected by subject matter RST relations.
Such a case would arise, for example,if the system needed to inform the hearer of a procedure involving several steps.
Thesubtree for the entire procedure would be associated with an intention, and the spansstating the steps would be related to one another by SEQUENCE relations.
To construct684Johanna D. Moore and C6cile L. Paris Planning Text for Advisory Dialogs(GOAL USER (DO USER REPLACE-I))(RECOMMEND SYSTEM USER (DO USER REPLACE-I)) (PERSUADED USERI (DO USER REPLACE-I))"'You should replaoe ~ETQ X 1) N |(SETFX I)" (MOTIVATION REPLACE-1 ENHANCE-MAINTAINABILITY)NI(BEL USER (SOMEREF (DIFFERENCES SETF SETQ ENHANCE-MAINTAINABILITY)))"1(BEL USER (USE SETF ASSIGN-TO-GV))J(INFORM SYSTEM USER (USE SErF ASSIQN-TO-QV))"'SETF can be used to ass@n a valueto any generalized variable.
"(ELABORATION GENERALIZED-VARIABLE).I(BEL USER (CLASS-ASCRIPTION GENERALIZED-VARIABLESTORAGE-LOCATION(RESTRICT NAMED-BY ACCESS-FUNC)))I(INFORM SYSTEM USER N(CLASS-ASCRIPTION GENERALIZED-VARIABLESTORAGE-LOCATION(RESTRICT NAMED-BY ACCESS-FUNC)))"" A generalized variable is a storage locationthat can be named by any access func~on.
"Figure 19Completed text plan for recommending replace SETQ with SETEplan operators for our system based on sample texts, we first segment the texts basedon intentional structure.
We then identify the rhetorical structure and relate it to theintentional structure.
In our formalism, rhetorical techniques are viewed as linguisticstrategies for achieving communicative intentions.In our system, after a text plan like the one shown in Figure 19 is constructed,it is recorded in the system's dialogue history and passed to the grammar interface,which translates the hierarchical text plan into a sequence of sentence specificationssuitable for the sentence generator.
In the process of this translation, the system decideswhere to place sentence boundaries and which, if any, connective markers to include.We currently use a simple set of heuristics that make these decisions based on therhetorical relation between two text spans.
To determine sentence boundaries, we havedivided the space of RST relations into those that may appear within a clause complexand those that must start a new sentence.
To choose connectives, we have associateda small set of possible connectives with each rhetorical relation.
Some connectives aresuitable for starting new sentences, while others are suitable for constructing complexsentences.
If the system finds more than one suitable connective for expressing an RSTrelation, the first one is chosen.Thus, in translating the text plan shown in Figure 19 into response 3, we see thatMOTIVATION, CONTRAST, and ELABORATION cause a sentence break.
Although not shown685Computational Linguistics Volume 19, Number 4in this example, other relations, e.g., MEANS, cause a complex sentence to be formed.
Inaddition, the CONTRAST relation is expressed explicitly via the connective "In contrast,"whereas the MOTIVATION and ELABORATION relations do not cause any connectivesto be added.
The heuristics we have described here are clearly too simplistic, andwe are currently working on more sophisticated techniques for linearizing our textplans.
However, note that the information recorded in our text plan is already usefulin making these decisions, and we believe this information will also facilitate morecomplex strategies.6.1 Recovering From Failure: Avoiding RepetitionAfter the system has produced its recommendation, suppose that the user asks\[USER\] What is a generalized variable?
\[4\]Recall that, from our analysis of naturally occurring dialogues uch as the one shownin Figure 1, we observed that advice-seekers frequently ask such questions.The query analyzer interprets this question and formulates the communicativegoal: (KNOW-ABOUT USER (CONCEPT GENERALIZED-VARIABLE)).
At this point, the ex-plainer must recognize that this goal was attempted by the last sentence of the pre-vious explanation and was not fully achieved.
Failure to do so might lead to simplyrepeating the description of a generalized variable that the user did not understand.Note that this is precisely what would occur if we  had generated the previous expla-nation using the schema shown in Figure 4.
From the schema, the system would notbe able to recognize that part of the text previously generated was intended to makethe user know about generalized variables.
A schema to define a term would thus betriggered and it would give the same answer as was previously generated.
Even if aschema-based system were to keep track of the information it already mentioned sothat it could avoid literally repeating the same content, it would still not be capable ofgenerating a new text, taking into consideration the fact that a goal was previously attemptedbut failed.
The same argument can be made about a system that generates explanationsbased solely on "RST plans" of the type used in Hovy's (1991) structurer.By examining the text plan of the previous explanation recorded in the dialoguehistory, our system is able to determine whether the current goal (resulting from thefollow-up question) is a goal that was previously attempted, as it is in this case.This time, when attempting toachieve the goal, the planner must select an alternativestrategy.
Recovery heuristics are responsible for selecting an alternative strategy whenresponding to such follow-up questions (Moore and Swartout 1989; Moore in press).One of these indicates that when the goal is to make the user know a concept, a goodrecovery strategy is to give examples.In the current case, the user model indicates that there are examples of the con-cept GENERALIZED-VARIABLE that the user is familiar with, namely CAR-0F-CONS andCDR-OF-CONS.
Thus, the strategy of giving examples can be applied to yield the fol-lowing system response:\[SYSTEM\] For example, the car of a cons is a generalized variable \[5\]named by the access function CAR, and the cdr of a consis a generalized variable named by the access function CDR.Providing an alternative xplanation would not be possible without the explicitrepresentation f the intentional structure that underlies the generated text recorded inour text plans.
To avoid repetitions, the system must realize what goals it has tried toachieve in the previous discourse and what strategies it used to achieve them.
Then, if686Johanna D. Moore and C6cile L. Paris Planning Text for Advisory Dialogsthe same goal is posted later in the discourse, the system can realize that its previousstrategy was not successful and can employ an alternative strategy.A similar situation would arise if the user were to indicate that he or she has notbeen persuaded to replace SETQ with SETF, e.g., with an utterance such as: 13\[USER\] I don't see why I should replace SETQ with SETF.
\[4'\]This would cause the query analyzer to post a goal to persuade the user to perform thisact.
Once again, the system must recognize that it has already attempted to persuadethe user to perform the act by contrasting the usage of SETQ with the usage of SETF.Since this strategy did not succeed, it must be able to persuade the user in a differentway.
From the information recorded in the text plan in Figure 19, our system candetermine that it has already tried to persuade the user to do the replacement.
SinceMOTIVATION is a presentational RST relation, the system has no other strategies forachieving the persuade goal.
However, it does have several other rhetorical strategiesfor MOTIVATION.
One of these is to provide a trace of the expert system's reasoningand could be used to generate the following explanation:\[SYSTEM\] I 'm trying to enhance the maintainability of the program \[5 ~\]by applying transformations that enhance maintainability.A transformation enhances maintainability if the usage ofthe construct on its right hand side is more general thanusage of the construct on its left hand side.
The usage ofSETF is more general than the usage of SETQ.Again, a schema-based system would not be able to recover correctly from thisfailure.
As we argued earlier, because the schemata do not record the intentions of theschema components, the system cannot determine that it contrasted SETQ with SETFin order to persuade the user to perform the replacement and thus would not knowwhat part of the schema to replan nor what other strategies to try.7.
Comparison to Related WorkBuilding on our work, Maybury (1992) devised a system to plan "communicativeacts."
Like Appelt (1985, p. 9), Maybury's ystem makes use of a hierarchy of linguisticactions.
At the highest level, Maybury has added rhetorical acts (e.g., describe, explain,convince).
The next two levels correspond to the top two levels of Appelt's hierarchy:illocutionary acts (e.g., inform, request), and locutionary acts (assert, ask, command).
TMThe actions at each level in the hierarchy have been encoded into plan operators thatare used in a process of hierarchical decomposition (Sacerdoti 1977) to refine rhetoricalacts through illocutionary acts into locutionary acts.An example operator from Maybury's system is shown in Figure 20.
This is arhetorical operator that can be used to define an entity by giving its "logical defini-tion" (the entity's genus and differentia.)
Note that operators in Maybury's language13 We do not currently allow users to ask questions phrased in such a manner because we do not have asophisticated natural language understanding component.
Instead, we have implemented a directmanipulation i terface that allows users to use the mouse to point at the noun phrases or clauses inthe text that were not understood or accepted.
To approximate he query above, the user couldhighlight the system's original recommendation t  replace SETQ with SETF, and select "Why?"
from themenu that appears.
As a result of interpreting this "Why?
", the system posts the communicative goal(PERSUADED USER (DO USER REPLACE-I)); see Moore and Swartout (1990) for more details.14 Appelt has two layers below locutionary acts that are not included in Maybury's ystem: conceptactivation and utterance acts.687Computational Linguistics Volume 19, Number 4contain both a "header" and an "effects" field.
The header field designates the type ofact (e.g., define, inform) associated with the operator, whereas the effects field specifiesthe effect(s) that this act is expected to have on the hearer's mental state.
To cause theplanner to generate a text, a "discourse controller" posts a goal to cause an effect inthe hearer's mental state, such as KNOW-ABOUT(USER, KC-135).
This goal is matchedagainst he effects field of operators in the plan library, and one is chosen.
Therefore,at the top level, Maybury's ystem has a record of the intention causing the text to beproduced.
But now consider what happens during goal refinement.
When an operatoris instantiated, the clauses in its decomposition field are posted as subgoals.
Note fromFigure 20 that in Maybury's operator language xpressions in the decomposition fieldare not subgoals to affect he hearer's mental state.
Rather, they are linguistic actionssuch as define, inform, and assert.
Unless these actions are locutionary, they becomesubgoals.
To achieve an action subgoal, the planner matches the subgoal against heheader field of operators in the plan library.
When an operator is chosen, the propo-sitions in the effects field are recorded, and the actions in the decomposition becomefurther subgoals.So, in fact Maybury's ystem does not have a record of the intentional structurebehind the text it is producing.
There are two problems.
First, except at the top level,planning is done by matching against he header field of operators, not against theeffects field.
Because there are multiple effects listed for most of the operators, thesystem cannot know which one is the intended effect!
Maybury's ystem thus cannotdistinguish between intended effects and side effects.
It is crucial that agents be ableto distinguish their intentions from the side effects of their actions in order to recoverfrom plan failures (Davis 1979; Bratman 1987).
Therefore, Maybury's ystem does nothave the knowledge necessary for recovering from failures, as our system does.A second problem with Maybury's text plans is that they do not capture the rela-tionship between intentions, i.e., that some intentions are in the plan because they inturn serve other intentions that appear higher in the plan tree.
Once again, this is be-cause Maybury's ystem simply records all the effects of each action.
It is impossible totell from the sets of effects at each level of the decomposition how effects are related toone another.
Grosz and Sidner (1986) argue that such relations between intentions are acrucial part of intentional structure.
Contrast Maybury's plans with those produced byour system.
Our text plans explicitly represent the intended effects of actions and therelationships between these intentions.
While we believe that it is useful to representadditional effects of operators, it is crucial to distinguish intended effects from sideeffects.
Therefore, we argue that while Maybury's approach does indeed represent theeffects of all of the "communicative acts" in his plans, it does not capture intentionalstructure and therefore cannot be used to recover from communication failures.In related work, Cawsey (1993) built a system called EDGE that allows the userto interrupt with clarification questions while a text is being generated.
EDGE plansextended tutorial explanations about he structure and input/output behavior of sim-ple electrical circuits.
This system is novel because it addresses i sues of conversationmanagement, such as turn-taking and topic control.
The system separates discourseplanning rules from content planning rules for this purpose.
EDGE plans an explana-tion at a high level, following a specified curriculum.
This plan is fleshed out as thedialogue progresses, causing sentences to be generated.
After each sentence is gen-erated, the system pauses to allow the user to supply feedback by choosing an itemfrom a menu.The EDGE discourse planner maintains an agenda indicating the topics that willbe covered later in the explanation.
Based on this agenda, the system can recognizewhen the user is asking about a topic that will be covered eventually, and can make688Johanna D. Moore and C6cile L. Paris Planning Text for Advisory DialogsNAMEHEADERCONSTRAINTSPRECONDITIONSESSENTIALDESIRABLEEFFECTSDECOMPOSITIONdefine-by-logical-definitionDefine(speaker, hearer, entity)3c Superclass(entity, c)3c Superclass( entity, c)A KNOW-ABOUT(speaker, c)KNOW-ABOUT(hearer, entity)Vx c superclasses(entity)KNOW(hearer, Superclass( entity, x ) ) AVy E differentia(entity)KNOW(hearer, Differentia(entity, y) )Inform(speaker, hearer, Logical-Definition(entity))Figure 20A plan operator for defining (from Maybury \[1992\]).comments such as We'll be getting to that in a moment.
If this is not the case, or if theuser insists, EDGE answers the user's question immediately.
Once the interruption hasbeen addressed, EDGE alters its subsequent explanation plan based on what took placeduring the interruption, and proceeds with its explanation as specified in the overallexplanation plan.
To resume from interruptions coherently, the discourse planningrules that manage the conversation i clude markers and meta-comments (e.g., Anyway,I was talking about... ).Because of its extended explanation plan and its discourse management rules,EDGE can handle interruptions in ways that are beyond the current capability of oursystem.
However, as Cawsey points out, EDGE is based on a largely syntactic model ofdialogue structure, and the system does not explicitly represent why different dialogueactions are selected.
The effects that dialogue operators are intended to have on theuser's knowledge and goals are not represented.
EDGE content plans are much likeschemas and therefore suffer from the limitations we discussed in Section 3.2.1.
If a userfails to understand a text produced by a content plan, the system's only recourse is totry another strategy to achieve the top-level content goal, e.g., "describe how an entityworks."
Moreover, the content plans of EDGE are domain-specific, and are not basedon general rhetorical techniques.
Because rhetorical structure is not represented, thesystem cannot choose connective markers or use other hetorical devices that make texteasier to comprehend.
For these reasons, EDGE cannot handle the types of phenomenonour system handles.It is also important to note that, because we are dealing with expert and advi-sory applications, our system must be able to manage a dialogue whose structureemerges dynamically as the user asks questions.
In advisory interactions, the systempresents the user with a recommendation r result and only provides explanationswhen the user requests them.
It is not appropriate for the system to plan extendedexplanations, testing the user's understanding and elaborating without provocation.Therefore, Cawsey's approach, which relies on the fact that the system has an extendedexplanation plan to follow, cannot be used directly.It is clear, however, that our approach and Cawsey's are complementary, andthat a complete system would need to incorporate aspects of both.
In particular, oursystem should be augmented to include conversation management operators in orderto manage topic shifts, to handle interruptions, and to generate meta-comments aboutthe discourse itself.689Computational Linguistics Volume 19, Number 48.
Status and Future Direct ionsThe text planner presented in this paper is implemented in Common Lisp and canproduce the text plans necessary to participate in the sample dialogue described inthis paper and several others; see Moore (in press) and Paris (1991a).
We currentlyhave over 150 plan operators that can answer the following types of questions:- -  Why?- -  Why conclusion?- -  Why are you trying to achieve goal?- -  Why are you using method to achieve goal?- -  Why are you doing act?- -  How do you achieve goal (in the general case)?- -  How did you achieve goal (in this case)?- -  What is a concept?- -  What is the difference between concept1 and concept2?- -  Huh?The text planner is being incorporated into several knowledge-based systems andtwo intelligent tutoring systems currently under development.
Two of these systemsare intended to be installed and used in the field.
This will give us an opportunityto evaluate the techniques proposed here and extend the system as appropriate.
Ithas also been employed in Reithinger's (1991) system for incremental language gen-eration, and serves as the basis of the presentational planner for WIP, a multimediasystem that plans text and graphics to achieve communicative goals (Wahlster et al1991).
Finally, it is the basis for a text planner capable of generating explanatory textsthat integrate xamples with their surrounding context (Mittal and Paris 1993).
Thisintegration would not be possible without our system's explicit representation f theintentions for generating portions of the text and the rhetorical strategies used toachieve them.We have begun to investigate how the discourse history should be indexed and ex-ploited to control the dialogue and affect subsequent responses in more general ways.As reported here, the dialogue history is used primarily to determine how to interpretand answer follow-up questions (e.g., Why?, How come?
), and to determine how torespond when the user asks a question that has already been answered or indicatesthat an explanation was not understood (Huh?).
In Carenini and Moore (1993) andRosenblum and Moore (1993) we discuss additional ways in which prior explanationscan affect he generation of the current utterance.We currently do not allow the user to return to a previous topic (e.g., once thesystem has moved on to a new topic, Let's go back to replacing SETQ with SETF... ), or tointroduce new goals into the dialogue (e.g., Well, now suppose I wanted to enhance fficiency... ).
In order to allow the user to change topics and introduce new goals at will, thesystem will need to be able to track the user's hifting oals and attention.
Sidner (1985)and Carberry (1987) have proposed approaches for tracking the topic of conversationin task-oriented dialogues.
However, their approaches rely on the assumption that thetopic of conversation closely follows the structure of the domain task.
Litman and690Johanna D. Moore and C6cile L. Paris Planning Text for Advisory DialogsAllen (1987) identified types of subdialogues in task-oriented interactions, includingclarifications and corrections, in which topic shift deviates from task structure, andthey devised a plan recognition model for handling such subdialogues.
Our systemcurrently handles what Litman and Allen call clarification subdialogues.
We believethat our model could be extended to handle other types of subdialogues, and thatthe text plans recorded in our dialogue history will aid in more general discoursemanagement tasks than the ones we currently address.
To perform these tasks, oursystem must understand how the previous responses tored in its discourse historyrelate to one another.
That is, we must address issues of how to build a representationof the intentional structure of the dialogue that is emerging across conversationalturns (Grosz and Sidner 1986) and to track global focus (Grosz 1977).
In addition,we will need communicative strategies for managing the dialogue, e.g., strategies forintroducing a topic, strategies for returning to a topic, etc.9.
ConclusionsWe have presented an approach to natural anguage generation that extends previoustheories and implementations in order to enable a computational system to play therole of a dialogue participant in an advisory setting.
We began by illustrating thetypes of phenomena that are prevalent in advisory dialogues.
We argued that, inorder to participate in such dialogues, a system must be capable of reasoning aboutits own previous utterances.
Follow-up questions must be interpreted in the contextof the ongoing conversation, and the system's previous contributions form part of thiscontext.We claimed that to handle explanation dialogues uccessfully, a discourse modelmust include the intended effect of individual parts of the text on the hearer, as well asa representation f how the parts relate to one another hetorically.
Through principledarguments and detailed examples, we showed that previous approaches to multisen-tential text generation, which do not explicitly represent the intentional structure oftheir utterances, cannot be used for advisory dialogues.
We presented our approachto text generation in which the system reasons about and records the intentions be-hind each text span as well as the rhetorical means used to achieve them.
Finally, wedemonstrated how this record can be used to overcome some of the limitations ofearlier approaches.AcknowledgmentsThe research described in this paper wassupported in part by the AdvancedResearch Projects Agency under a NASAAmes cooperative agreement Number NCC2-520.
Johanna Moore is currently supportedby grants from the Office of Naval ResearchCognitive and Neural Sciences Division(Grant Number N00014-91-J-1694), theNational Science Foundation ResearchInitiation Award (Grant NumberIRI-9113041), and the National Library ofMedicine.
C6cile Paris gratefullyacknowledges the support of the AdvancedResearch Projects Agency under the contractDABT63-91-C-0025 and the National ScienceFoundation (Grant Number IRI-9003078)while writing this paper.The authors would like to thank WilliamSwartout, who has advised us on this worksince its inception and has also given ususeful comments on this paper.
We wouldalso like to thank Martha Pollack for herhelp in clarifying some of the ideas presen-ted in this paper, and Giuseppe Carenini,Violetta Cavalli-Sforza, Vibhu Mittal,Richmond Thomason, and Arlene Weiner,who provided useful comments on earlierdrafts of this paper.
Finally, we are indebtedto the anonymous reviewers whose detailedand insightful comments greatly improvedthe final version of this paper.ReferencesAppelt, Douglas E. (1985).
Planning EnglishSentences.
Cambridge University Press.691Computational Linguistics Volume 19, Number 4Bateman, John A., and Paris, C6cile L.(1989).
"Phrasing a text in terms the usercan understand."
In Proceedings, EleventhInternational Joint Conference on ArtificialIntelligence, 1511-1517.
Detroit, MI.Bateman, John A., and Paris, C6cile L.(1991).
"Constraining the deployment oflexicogrammatical resources during textgeneration: Towards a computationalinstantiation of register theory."
InFunctional and Systemic Linguistics:Approaches and Uses, edited by EijaVentola, 81-106.
Mouton de Gruyter.Bratman, Michael (1987).
Intentions, Plans,and Practical Reason.
Harvard UniversityPress.Brewer, W. F. (1980).
"Literacy theory,rhetoric, and stylistics: Implications forpsychology."
In Theoretical Issues inReading Comprehension, edited byR.
J. Shapiro, B. C. Bruce, andW.
E Brewer, 221-239.
Lawrence Erlbaum.Cahour, B6atrice; Falzon, Pierre; and Robert,Jean Marc (1990).
"From text coherence tointerface consistency: A psycholinguisticapproach."
In Work with Display Units 89,edited by L. Berlinguet and D. Berthelette.Elsevier Science Publishers B.V.Carberry, Sandra M. (1987).
"Pragmaticmodeling: Toward a robust naturallanguage interface."
ComputationalIntelligence, 3(3), 117-136.Carenini, Giuseppe, and Moore, Johanna D.(1993).
"Generating explanations incontext."
In Proceedings, InternationalWorkshop on Intelligent User Interfaces,edited by Wayne D. Gray, WilliamE.
Hefley, and Dianne Murray, 175-182.ACM Press.Cawsey, Alison (1993).
Explanation andInteraction: The Computer Generation ofExplanatory Dialogues.
MIT Press.Charney, Davida H.; Reder, Lynne M.; andWells, Gail W. (1988).
"Studies ofelaboration i  instructional texts."
InEffective Documentation: What We HaveLearned from Research, edited byStephen Doheny-Farina, 48-72.
MIT Press.Clancey, William J.
(1983).
"Theepistemology of a rule-based expertsystem: A framework for explanation.
"Artificial Intelligence, 20(3), 215-251.Cohen, Philip R. (1978).
On knowing what tosay: Planning speech acts.
Doctoraldissertation, Department of ComputerScience, University of Toronto.Cohen, Philip R., and Levesque, Hector(1990).
"Rational interaction as the basisfor communication."
In Intentions inCommunication, edited by Philip R. Cohen,Jerry Morgan, and Martha E. Pollack,221-255.
MIT PressCohen, Philip R., and Perrault, C. Raymond(1979).
"Elements of a plan-based theoryof speech acts."
Cognitive Science, 3,177-212.Davis, Lawrence H. (1979).
Theory of Action.Prentice Hall.Ehrlich, Marie-France, and Cahour, B6atrice(1991).
"Contr61e m6tacognitif de lacompr6hension: coh6sion d'un texteexpositif et auto-6valuation de lacompr6hension."
Bulletin de Psychologie,XLIV(399), 147-155.
Special edition editedby E. Cauzinille and J. Beaudichon.Goldman, Susan, and Dur~in, Richard P.(1988).
"Answering questions fromoceanography texts: Learner, task, andtext characteristics."
Discourse Processes, 1,373-412.Grimes, Joseph E. (1975).
The Thread ofDiscourse.
Mouton.Grosz, Barbara J.
(1977).
"The representationand use of focus in dialogueunderstanding."
Technical Report 151, SRIInternational, Menlo Park, CA.Grosz, Barbara J., and Sidner, Candace L.(1986).
"Attention, intention, and thestructure of discourse."
ComputationalLinguistics, 12(3), 175-204.Hobbs, Jerry R. (1979).
"Coherence andcoreference."
Cognitive Science, 3(1), 67-90.Hobbs, Jerry R. (1983).
"Why is discoursecoherent?"
In Coherence inNaturalLanguage Texts, edited by F. Neubauer,29-69.
H. Buske.Hobbs, Jerry R. (1985).
"On the coherenceand structure of discourse."
TechnicalReport CSLI-85-37, Center for the Studyof Language and Information, LelandStanford Junior University, Stanford,California.Hovy, Eduard H. (1988).
Generating NaturalLanguage Under Pragmatic Constraints.Lawrence Erlbaum.Hovy, Eduard H. (1991).
"Approaches to theplanning of coherent text."
In NaturalLanguage Generation i  Artificial Intelligenceand Computational Linguistics, edited byC6cile L. Paris, William R. Swartout, andWilliam C. Mann, 83-102.
KluwerAcademic Publishers.Kittredge, R.; Korelsky, T.; and Rambow, O.(1991).
"On the need for domaincommunication k owledge.
"Computational Intelligence, 7(4), 305-314.Klausmeier, Herbert J.
(1976).
"Instructionaldesign and the teaching of concepts."
InCognitive Learning in Children, edited byJ.
R. Levin and V. L. Allen.
AcademicPress.Levy, David M. (1979).
"Communicative692Johanna D. Moore and C6cile L. Paris Planning Text for Advisory Dialogsgoals and strategies: Between discourseand syntax."
In Syntax and Semantics,Volume 12: Discourse and Syntax, edited byP.
Cole and J. L. Morgan, 183-210.Academic Press.Litman, Diane J., and Allen, James E (1987).
"A plan recognition model forsubdialogues in conversations."
CognitiveScience, 11, 163-200.Mann, William C. (1984).
"Discoursestructures for text generation."
InProceedings, Tenth International Conferenceon Computational Linguistics, 367-375.Stanford, CA.Mann, William C., and Matthiessen,Christian M. I. M. (1985).
"Ademonstration f the Nigel textgeneration computer program."
InSystemic Perspectives on Discourse: SelectedPapers from the Ninth International SystemicsWorkshop, edited by R. Benson andJ.
Greaves, 50-83.
Ablex.Mann, William C., and Thompson,Sandra A.
(1988).
"Rhetorical structuretheory: Towards a functional theory oftext organization."
TEXT, 8(3), 243-281.Maybury, Mark T. (1992).
"Communicativeacts for explanation generation.
"In ternational Journal of Man-MachineStudies, 37(2), 135-172.McCoy, Kathleen E (1989).
"Generatingcontext sensitive responses toobject-related misconceptions."
ArtificialIntelligence, 41(2), 157-195.McKeown, Kathleen R. (1985).
TextGeneration: Using Discourse Strategies andFocus Constraints to Generate NaturalLanguage Text.
Cambridge UniversityPress.Meyer, B. J. E; Brandt, D. M.; and Bluth,G.
J.
(1980).
"Use of top-level structure intexts: Key for reading comprehension inninth-grade students."
Reading ResearchQuarterly, 16, 72-102.Mittal, Vibhu O., and Paris, C6cile L.
(1993).
"Automatic documentation generation:The interaction between text andexamples."
In Proceedings, ThirteenthInternational Joint Conference on ArtificialIntelligence (IJCAI), Chambery, France.Moore, Johanna D. (In press.)
Participating inExplanatory Dialogues: Interpreting andResponding to Questions in Context.
MITPress.Moore, Johanna D., and Paris, C6cile L.(1991).
"Requirements for an expertsystem explanation facility.
"Computational Intelligence, 7(4), 367-370.Moore, Johanna D., and Paris, C6cile L.(1992).
"Exploiting user feedback tocompensate for the unreliability of usermodels."
User Modeling and User-AdaptedInteraction, 2(4), 331-365.Moore, Johanna D., and Pollack, Martha E.(1992).
"A problem for RST: The need formulti-level discourse analysis.
"Computational Linguistics, 18(4), 537-544.Moore, Johanna D., and Swartout,William R. (1989).
"A reactive approach toexplanation."
In Proceedings, EleventhInternational Joint Conference on ArtificialIntelligence, 1504-1510.
Detroit, MI.Moore, Johanna D., and Swartout,William R. (1990).
"Pointing: A waytoward explanation dialogue."
InProceedings, National Conference on ArtificialIntelligence, 457-464.
Boston, MA.Neches, Robert; Swartout, William R.; andMoore, Johanna D. (1985).
"Enhancedmaintenance and explanation of expertsystems through explicit models of theirdevelopment."
IEEE Transactions onSoftware Engineering, SE-11(11), 1337-1351.Paris, C6cile L. (1988).
"Tailoring objectdescriptions to the user's level ofexpertise."
Computational Linguistics, 14(3),64-78.Paris, C6cile L. (1991a).
"Generation andexplanation: Building an explanationfacility for the explainable xpert systemsframework."
In Natural LanguageGeneration in Artificial Intelligence andComputational Linguistics, edited byC6cile L. Paris, William R. Swartout, andWilliam C. Mann, 49-81.
KluwerAcademic Publishers.Paris, C6cile L. (1991b).
The Use of ExplicitUser Models in Text Generation: Tailoring to aUser's Level of Expertise.
Frances Pinter.Penman Natural Language GenerationGroup (1989).
The Penman User Guide.Available from USC/Information SciencesInstitute, 4676 Admiralty Way, Marina delRey, CA.Rambow, Owen (1990).
"Domaincommunication k owledge."
InProceedings, Fifth International Workshop onNatural Language Generation, 87-94.Pittsburgh, PARedeker, Gisela (1990).
"Ideational andpragmatic markers of discoursestructure."
Journal of Pragmatics, 14,367-381.Reithinger, Norbert (1991).
Eine paralleleArchitektur zur inkrementellen Generierungmultimodaler Dialogbeitrfz'ge.
Doctoraldissertation, Technischen Fakultfit derUniversit~it des Saarlandes, Saarbriicken,Germany.Robinson, Jane J.
(1984).
"Extendinggrammars to new domains."
TechnicalReport ISI/RR?83-123, USC/Information693Computational Linguistics Volume 19, Number 4Sciences Institute.Rosenblum, James A., and Moore,Johanna D. (1993).
"Participating ininstructional dialogues: Finding andexploiting relevant prior explanations."
InProceedings, World Conference on ArtificialIntelligence in Education.Sacerdoti, Earl D. (1977).
A Structure forPlans and Behavior.
Elsevier.Schiffrin, Deborah (1987).
Discourse Markers.Cambridge University Press.Shepherd, H. R. (1926).
The Fine Art ofWriting.
Macmillan.Sidner, Candace L. (1979).
Toward acomputational theory of definite anaphoracomprehension in English discourse.
Doctoraldissertation, Massachusetts Institute ofTechnology, Cambridge, MA.Sidner, Candace L. (1985).
"Plan parsing forintended response recognition idiscourse."
Computational Intelligence, 1(1),1-10.Sparck Jones, Karen (1989).
"Realism aboutuser modelling."
In User Models in DialogSystems, edited by Alfred Kobsa andWolfgang Wahlster, 341-363.
SymbolicComputation Series, Springer-Verlag.Suthers, Daniel D. (1991).
"Task-appropriatehybrid architectures for explanation.
"Computational Intelligence, 7(4), 315-333.Swartout, William R. (1983).
"XPLAIN: Asystem for creating and explaining expertconsulting systems."
Artificial Intelligence,21(3), 285-325.Swartout, William R.; Paris, C6cile L.; andMoore, Johanna D. (1991).
"Design forexplainable xpert systems."
IEEE Expert,6(3), 58-64.Wahlster, Wolfgang; Andr6, Elisabeth; Graf,Winfried; and Rist, Thomas (1991).
"Designing illustrated texts: Howlanguage production is influenced bygraphics generation."
In Proceedings,European Chapter of the Association forComputational Linguistics, 8-14.
Berlin.694
