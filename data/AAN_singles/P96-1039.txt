An Information Structural  Approachto Spoken Language Generat ionScot t  PrevostThe  Med ia  LaboratoryMassachuset ts  Ins t i tu te  of Techno logy20 Ames  St reetCambr idge ,  Massachuset ts  02139-4307 USAprevost@media, mit.
eduAbst ractThis paper presents an architecture for thegeneration of spoken monologues with con-textually appropriate intonation.
A two-tiered information structure representationis used in the high-level content planningand sentence planning stages of generationto produce efficient, coherent speech thatmakes certain discourse relationships, uchas explicit contrasts, appropriately salient.The system is able to produce appropriateintonational patterns that cannot be gen-erated by other systems which rely solelyon word class and given/new distinctions.1 IntroductionWhile research on generating coherent written texthas flourished within the computational linguisticsand artificial intelligence communities, research onthe generation of spoken language, and particularlyintonation, has received somewhat less attention.In this paper, we argue that commonly employedmodels of text organization, such as schemata ndrhetorical structure theory (RST), do not adequatelyaddress many of the issues involved in generatingspoken language.
Such approaches fail to considercontextually bound focal distinctions that are mani-fest through a variety of different linguistic and par-alinguistic devices, depending on the language.In order to account for such distinctions of fo-cus, we employ a two-tiered information structurerepresentation as a framework for maintaining lo-cal coherence in the generation of natural anguage.The higher tier, which delineates the lheme, thatwhich links the utterance to prior utterances, andthe theme, that which forms the core contribution ofthe utterance to the discourse, is instrumental in de-termining the high-level organization of informationwithin a discourse segment.
Dividing semantic rep-resentations into their thematic and rhematic partsallows propositions to be presented in a way thatmaximizes the shared material between utterances.The lower tier in the information structure repre-sentation specifies the semantic material that is in"focus" within themes and themes.
Material may bein focus for a variety of reasons, such as to empha-size its "new" status in the discourse, or to contrastit with other salient material.
Such focal distinc-tions may affect the linguistic presentation of infor-mation.
For example, the it-cleft in (1) may markJohn as standing in contrast o some other recentlymentioned person.
Similarly, in (2), the pitch accenton red may mark the referenced car as standing incontrast o some other car inferable from the dis-course context 3(1) It was John who spoke first.
(2) Q: Which car did Mary drive?A: (MARY drove)th (the RED car.
)rhL+H* LH(%) H* LL$By appealing to the notion that the simple rise-falltune (H* LL%) very often accompanies the rhematicmaterial in an utterance and the rise-fall-rise tune of-ten accompanies the thematic material (Steedman,1991 i Prevost and Steedman, 1994), we present aspoken language generation architecture for produc-ing short spoken monologues with contextually ap-propriate intonation.2 In fo rmat ion  St ructureInformation Structure refers to the organization ofinformation within an utterance.
In particular, it1In this example, and throughout the remainder ofthe paper, the intonation contour is informally notedby placing prosodic phrases in parentheses and markingpitch accented words with capital etters.
The tunes aremore formally annotated with a variant of (Pierrehum-bert, 1980) notation described in (Prevost, 1995).
Threedifferent pause lengths are associated with boundariesin the modified notation.
'(%)' marks intra-utteranceboundaries with very little pausing, '%' marks intra-utterance boundaries associated with clauses demar-cated by commas, and '$' marks utterance-final bound-aries.
For the purposes of generation and synthesis, thesedistinctions are crucial.294(3) Q: I know the AMERICAN amplifier produces MUDDY treble,(But WHAT) (does the BRITISH amplifier produce?
)L+H* L(H%) H* LL$A: (The BRITISH \[ amplifier produces)L+H* \[ L(H%)theme-\]?CU Thleme(CLEANH, \]rherne-focusRhemetreble.
)LL$(4) Q: I know the AMERICAN amplifier produces MUDDY treble,(But WHAT) (produces CLEAN treble?
)L+H* L(H%) H* LL$A: (The BRITISHH*theme-focusRhemeamplifier)L(L%)(produces CLEANL+H*theme-locusThemetreble.
)LH$defines how the information conveyed by a sentenceis related to the knowledge of the interlocutors andthe structure of their discourse.
Sentences conveyingthe same propositional content in different contextsneed not share the same information structure.
Thatis, information structure refers to how the semanticcontent of an utterance is packaged, and amountsto instructions for updating the models of the dis-course participants.
The realization of informationstructure in a sentence, however, differs from lan-guage to language.
In English, for example, intona-tion carries much of the burden of information struc-ture, while languages with freer word order, such asCatalan (Engdahl and Vallduvi, 1994) and Turkish(Hoffman, 1995) convey information structure syn-tactically.2.1 In fo rmat ion  S t ructure  and  In tonat ionThe relationship between intonational structure andinformation structure is illustrated by (3) and (4).
Ineach of these examples, the answer contains the samestring words but different intonational patterns andinformation structural representations.
The themeof each utterance is considered to be represented bythe material repeated from the question.
That is,the theme of the answer is what links it to the ques-tion and defines what the utterance is about.
Therheme of each utterance is considered to be repre-sented by the material that is new or forms the corecontribution of the utterance to the discourse.
Bymapping the rise-fall tune (H* LL%) onto rhemesand the rise-fall-rise tune (L+H* LH%) onto themes(Steedman, 1991; Prevost and Steedman, 1994), wecan easily identify the string of words over whichthese two prominent unes occur directly from theinformation structure.
While this mapping is cer-tainly overly simplistic, the results presented in Sec-tion 4.3 demonstrate its appropriateness forthe classof simple declarative sentences under investigation.Knowing the strings of words to which these twotunes are to be assigned, however, does not pro-vide enough information to determine the location ofthe pitch accents (H* and L+H*) within the tunes.Moreover, the simple mapping described above doesnot account for the frequently occurring cases inwhich thematic material bears no pitch accents andis consequently unmarked intonationally.
Previousapproaches to the problem of determining whereto place accents have utilized heuristics based on"givenness."
That is, content-bearing words (e.g.nouns and verbs) which had not been previouslymentioned (or whose roots had not been previ-ously mentioned) were assigned accents, while func-tion words were de-accented (Davis and Hirschberg,1988; Hirschberg, 1990).
While these heuristics ac-count for a broad range of intonational possibilities,they fail to account for accentual patterns that serveto contrast entities or propositions that were previ-ously "given" in the discourse.
Consider, for ex-ample the intonational pattern in (5), in which thepitch accent on amplifier in the response cannot beattributed to its being "new" to the discourse.
(5) Q: Do critics prefer the BRITISH amplifierL* Hor the AMERICAN amplifier?H* LL$A: They prefer the AMERICAN amplifier.H* LL$For the determination of pitch accent placement,we rely on a secondary tier of information structurewhich identifies focused properties within themesand rhemes.
The theme-foci and the rheme-focimark the information that differentiates propertiesor entities in the current utterance from propertiesor entities established in prior utterances.
Conse-295quently, the semantic material bearing "new" infor-mation is considered to be in focus.
Furthermore,the focus may include semantic material that servesto contrast an entity or proposition from alterna-tive entities or propositions already established inthe discourse.
While the types of pitch accents (H*or L+H*) are determined by the theme/theme d lin-eation and the aforementioned mapping onto tunes,the locations of pitch accents are determined by theassignment of foci within the theme and rheme, asillustrated in (3) and (4).
Note that it is in pre-cisely those cases where thematic material, which is"given" by default, does not contrast with any otherpreviously established properties or entities that thismaterial is intonationally unmarked, as in (6).
(6) Q: Which amplifier does Scott PREFER?H* LL$A: (He prefers)th (the BI~ITISH amplifier.
)rhH* LL$2.2 Contrastive Focus AlgorithmThe determination of contrastive focus, and con-sequently the determination of pitch accent loca-tions, is based on the premise that each object inthe knowledge base is associated with a set of alter-natives from which it must be distinguished if refer-ence is to succeed.
The set of alternatives i deter-mined by the hierarchical structure of the knowledgebase.
For the present implementation, only proper-ties with the same parent or grandparent class areconsidered to be alternatives to one another.Given an entity z and a referring expression for x,the contrastive focus feature for its semantic repre-sentation is computed on the basis of the contrastivefocus algorithm described in (7), (8) and (9).
Thedata structures and notational conventions are givenbelow.
(7) DElist: a collection of discourse entitiesthat have been evoked in prior dis-course, ordered by recency.
The listmay be limited to some size k so thatonly the k most recent discourse nti-ties pushed onto the list are retrievable.ASet(z): the set of alternatives for objectx, i.e.
those objects that belong tothe same class as x, as defined in theknowledge base.RSet(z,S): the set of alternatives for ob-ject z as restricted by the referring ex-pressions in DElist and the set of prop-erties S.CSet(x, S): the subset of properties of S tobe accented for contrastive purposes.Props(z): a list of properties for object x,ordered by the grammar so that nomi-nal properties take precedence over ad-jectival properties.The algorithm, which assigns contrastive focusin both thematic and thematic onstituents, beginsby isolating the discourse ntities in the given con-stituent.
For each such entity x, the structures de-fined above are initialized as follows:(8) Props(x) :-- \[P I P(x) is true in KB \]ASet(x) :-- {y I aZt(x, y)}, x's alternativesRSet(x,{}) :-- {x}U{y \[ y 6 ASet(x) ~ y EDEiist}, evoked alternativesCSet(x,{}) :=  {}The algorithm appears in pseudo-code in (9).
2(9) S := {}for  each P in Props(x)RSet(x, S u {P}) :={Y I Y e RSet(x, S) ~ P(y)}i f  RSet(x, S U {P}) = RSet(x, S) then% no restrictions were made% based on property P.CSet(x, S O {P}) := CSet(z, S)else% property P eliminated some% members of the RSeLCSet(x, S U {P}) := CSe~(x, S) U {P}endifS:=SU{P}endforIn other words, given an object x, a list of its prop-erties and a set of alternatives, the set of alternativesis restricted by including in the initial RSet only xand those objects that are explicitly referenced in theprior discourse.
Initially, the set of properties to becontrasted (CSe~) is empty.
Then, for each propertyof x in turn, the RSet is restricted to include onlythose objects satisfying the given property in theknowledge base.
If imposing this restriction on theRSet for a given property decreases the cardinalityof the RSe~, then the property serves to distinguishx from other salient alternatives evoked in the priordiscourse, and is therefore added to the contrast set.Conversely, if imposing the restriction on the RSetfor a given property does not change the RSet, theproperty is not necessary for distinguishing x fromits alternatives, and is not added to the CSet.Based on this contrastive focus algorithm and themapping between information structure and into-nation described above, we can view informationstructure as the representational bridge between dis-course and intonational variability.
The followingsections elucidate how such a formalism can be in-tegrated into the computational task of generatingspoken language.2An in-depth discussion of the algorithm and numer-ous examples ate presented in (Prevost, 1995).2963 Generat ion  Arch i tec tureThe task of natural anguage generation (NLG) hasoften been divided into three stages: content plan-ning, in which high-level goals are satisfied and dis-course structure is determined, sentence planning, inwhich high-level abstract semantic representationsare mapped onto representations that more fullyconstrain the possible sentential realizations (Ram-bow and Korelsky, 1992; Reiter and Mellish, 1992;Meteer, 1991), and surface generation, in which thehigh-level propositions are converted into sentences.The selection and organization of propositionsand their divisions into theme and rheme are de-termined by the content planner, which maintainsdiscourse coherence by stipulating that semantic in-formation must be shared between consecutive utter-ances whenever possible.
That is, the content plan-ner ensures that  the theme of an utterance links itto material in prior utterances.The process of determining foci within themes andrhemes can be divided into two tasks: determiningwhich discourse entities or propositions are in fo-cus, and determining how their linguistic realizationsshould be marked to convey that focus.
The firstof these tasks can be handled in the content phaseof the NLG model described above.
The second ofthese tasks, however, relies on information, such asthe construction of referring expressions, that is of-ten considered the domain of the sentence planningstage.
For example, although two discourse ntitiesel and e2 can be determined to stand in contrastto one another by appealing only to the discoursemodel and the salient pool of knowledge, the methodof contrastively distinguishing between them by theplacement of pitch accents cannot be resolved untilthe choice of referring expressions has been made.Since referring expressions are generally taken to bein the domain of the sentence planner (Dale andHaddock, 1991), the present approach resolves is-sues of contrastive focus assignment at the sentenceprocessing stage as well.During the content generation phase, the contentof the utterance is planned based on the previousdiscourse.
While template-based systems (McKe-own, 1985) have been widely used, rhetorical struc-ture theory (RST) approaches (Mann and Thomp-son, 1986; Hovy, 1993), which organize texts byidentifying rhetorical relations between clause-levelpropositions from a knowledge base, have recentlyflourished.
Sibun (Sibun, 1991) offers yet anotheralternative in which propositions are linked to oneanother not by rhetorical relations or pre-plannedtemplates, but rather by physical and spatial prop-erties represented in the knowledge-base.The present framework for organizing the con-tent of a monologue is a hybrid of the templateand RST approaches.
The implementation, whichis presented in the following section, produces de-scriptions of objects from a knowledge base withcontext-appropriate intonation that makes properdistinctions of contrast between alternative, salientdiscourse ntities.
Certain constraints, such as therequirement that objects be identified or defined atthe beginning of a description, are reminiscent ofMcKeown's schemata.
Rather than imposing strictrules on the order in which information is presented,the order is determined by domain specific knowl-edge, the communicative intentions of the speaker,and beliefs about the hearer's knowledge.
Finally,the system includes a set of rhetorical constraintsthat may rearrange the order of presentation for in-formation in order to make certain rhetorical rela-tionships salient.
While this approach has proveneffective in the present implementation, further re-search is required to determine its usefulness for abroader ange of discourse types.4 The Prolog ImplementationThe monologue generation program produces textand contextually-appropriate n onation contours todescribe an object from the knowledge base.
Thesystem exhibits the ability to intonationally contrastalternative ntities and properties that have beenexplicitly evoked in the discourse even when theyoccur with several intervening sentences.4.1 Content  Generat ionThe architecture for the monologue generation pro-gram is shown in Figure 1, in which arrows repre-sent the computational flow and lines represent de-pendencies among modules.
The remainder of thissection contains a description of the computationalpath through the system with respect to a singleexample.
The input to the program is a goal to de-scribe an object from the knowledge base, which inthis case contains a variety of facts about hypothet-ical stereo components.
In addition, the input pro-vides a communicative intention for the goal whichmay affect its ultimate realization, as shown in (1O).For example, given the goal descr ibe(x ) ,  the in-tention persuade- to -buy(hearer ,x )  may result ina radically different monologue than the intentionpersuade- t  o-s e l l  (hearer ,  x).
(10) Goal: describe e lInput: generat e (int ention(bel (hl,good-t o-buy (e I) ) )Information from the knowledge base is selected tobe included in the output by a set of relations thatdetermines the degree to which knowledge base factsand rules support the communicative intention ofthe speaker.
For example, suppose the system "be-lieves" that conveying the proposition in (11) mod-erately supports the intention of making hearer hlwant to buy el,  and further that the rule in (12) isknown by hl.297Communicative Goals and Intentionsienten?~ Planner ~-~Accen, Assignment Rules )~ i Sudace !enerator ~ CCG )lProsodical~/ Annotated Monologue11Spoken Ou~utFigure 1: An Architecture for Monologue Genera-tion(II) bel(hl, holds(rating(X, powerful)))(12) holds(rating(X, powerful)) "-holds(produce(X, Y)),holds (isa(Y, watts-per-channel) ),holds(amount(Y, Z)),number(Z),z >= 100.The program then consults the facts in the knowl-edge base, verifies that the property does indeed holdand consequently includes the corresponding facts inthe set of properties to be conveyed to the hearer,as shown in (13).
(13) holds(produce(el ,  e7)).holds(isa(e7, watts-per-channel)).holds(amount(e7, I00)).The content generator starts with a simple de-scription template that specifies that an object is tobe explicitly identified or defined before other propo-sitions concerning it are put forth.
Other relevantpropositions concerning the object in question arethen linearly organized according to beliefs abouthow well they contribute to the overall intention.
Fi-nMly, a small set of rhetorical predicates rearrangesthe linear ordering of propositions o that sets ofsentences that stand in some interesting rhetoricalrelationship to one another will be realized togetherin the output.
These rhetorical predicates employinformation structure to assist in maintaining thecoherence of the output.
For example, the conjunc-tion predicate specifies that propositions sharing thesame theme or theme be realized together in orderto avoid excessive topic shifting.
The contrast pred-icate specifies that pairs of themes or rhemes thatexplicitly contrast with one another be realized to-gether.
The result is a set of properties roughly or-dered by the degree to which they support he givenintention, as shown in (14).
(14) holds (defn(isa(el, amplifier)))holds (design(el, solid-state) ,pres )holds(cost (el, e9) ,pres)holds (produce (el, e7) ,pres)holds (contrast (praise (e4, el ),revile (eS, el) ) ,past)The top-level propositions shown in (14) were se-lected by the program because the hearer (hl) isbelieved to be interested in the design of the am-plifier and the reviews the amplifier has received.Moreover, the belief that the hearer is interested inbuying an expensive, powerful amplifier justifies in-cluding information about its cost and power rat-ing.
Different sets of propositions would be gener-ated for other (perhaps thriftier) hearers.
Addition-ally, note that the propositions pra ise(e4,  el) andrev i le (e5,  el) are combined into the larger propo-sition contrast (praise ( e4, el ), revile (e5, e I ) ).This is accomplished by the rhetorical constraintsthat determine the two propositions to be con-trastive because 4 and e5 belong to the same setof alternative ntities in the knowledge base andpraise and rev i le  belong to the same set of al-ternative propositions in the knowledge base.The next phase of content generation recognizesthe dependency relationships between the proper-ties to be conveyed based on shared discourse nti-ties.
This phase, which represents an extension ofthe rhetorical constraints, arranges propositions toensure that consecutive utterances share semanticmaterial (cf.
(McKeown et el., 1994)).
This rule,which in effect imposes a strong bias for CenteringTheory's continue and retain transitions (Grosz etel., 1986) determines the theme-rheme s gmentationfor each proposition.4.2 Sentence PlanningAfter the coherence constraints from the previoussection are applied, the sentence planner is respon-sible for making decisions concerning the form inwhich propositions are realized.
This is accom-plished by the following simple set of rules.
First,Definitional isa properties are realized by the ma-trix verb.
Other isa properties are realized by nounsor noun phrases.
Top-level properties (such as thosein (14)) are realized by the matrix verb.
Finally,embedded properties (those evoked for building re-ferring expressions for discourse ntities) are realizedby adjectival modifiers if possible and otherwise byrelative clauses.While there are certainly a number of linguis-tically interesting aspects to the sentence planner,the most important aspect for the present purposesis the determination of theme-foci and rheme-foci.The focus assignment algorithm employed by the298sentence planner, which has access to both the dis-course model and the knowledge base, works as fol-lows.
First, each property or discourse ntity in thesemantic and information structural representationsis marked as either previously mentioned or new tothe discourse.
This assignment is made with re-spect to two data structures, the discourse entitylist (DEList), which tracks the succession of entitiesthrough the discourse, and a similar structure forevoked properties.
Certain aspects of the semanticform are considered unaccentable because they cor-respond to the interpretations of closed-class itemssuch as function words.
Items that are assigned fo-cus based on their "newness" are assigned the o focusoperator, as shown in (15).
(15) Semantics: defn(isa(oel, ocl))Theme: oelRheme: Ax.isa(x, ocl)Supporting Props: isa(cl, oamplifier)o design(cl, osolidstate)The second step in the focus assignment algorithmchecks for the presence of contrasting propositionsin the ISStore, a structure that stores a history ofinformation structure representations.
Propositionsare considered contrastive if they contain two con-trasting pairs of discourse ntities, or if they containone contrasting pair of discourse entities as well ascontrasting functors.Discourse ntities are determined to be contrastiveif they belong to the same set of alternatives in theknowledge base, where such sets are inferred fromthe isa-l inks that define class hierarchies.
While thepresent implementation only considers entities withthe same parent or grandparent class to be alterna-tives for the purposes of contrastive stress, a gradu-ated approach that entails degrees of contrastivenessmay also be possible.The effects of the focus assignment algorithm areeasily shown by examining the generation of an ut-terance that contrasts with the utterance shownin (15).
That  is, suppose the generation programhas finished generating the output corresponding tothe examples in (10) through (15) and is assignedthe new goal of describing entity e2, a different am-plifier.
After applying the second step on the focusassignment algorithm, contrasting discourse ntitiesare marked with the ?
contrastive focus operator, asshown in (16).
Since el  and e2 are both instances ofthe class amplifiers and c l  and c2 both describethe class a rap l i f i e rs  itself, these two pairs of dis-course entities are considered to stand in contrastiverelationships.
(16) Semantics: defn(isa(.e2, .c2))Theme: -e2Rheme: Ax.isa(x, .c2)Supporting Props: class(c2, amplifier)design(c2, otube)While the previous step of the algorithm deter-mined which abstract discourse ntities and proper-ties stand in contrast, the third step uses the con-trastive focus algorithm described in Section 2 todetermine which elements need to be contrastivelyfocused for reference to succeed.
This algorithm de-termines the minimal set of properties of an entitythat must be "focused" in order to distinguish itfrom other salient entities.
For example, althoughthe representation i (16) specifies that e2 standsin contrast to some other entity, it is the propertyof e2 having a tube design rather than a solid-statedesign that needs to be conveyed to the hearer.
Af-ter applying the third step of the focus assignmentto (16), the result appears as shown in (17), with"tube" contrastively focused as desired.
(17) Semantics: defn(isa(.e2, .c2))Theme: .e2Rheme: Ax.isa(x, .c2)Supporting Props: isa(c2, amplifier)design(c2, .tube)The final step in the sentence planning phase ofgeneration is to compute a representation that canserve as input to a surface form generator based onCombinatory Categorial Grammar  (CCG) (Steed-man, 1991), as shown in (18).
3(18) Theme: np(3, s) :(el^S) ^ d#(el,.xh(el)~s)~u/rhRheme:  s :( acU pres)^ indeI(el, (amplifier(cl )&?
tube(el))~isa(el, l))\np(a, s): elerh4.3  Resu l t sGiven the focus-marked output of the sentenceplanner, the surface generation module consults aCCG grammar which encodes the information struc-ture/intonation mapping and dictates the genera-tion of both the syntactic and prosodic constituents.The result is a string of words and the appropriateprosodic annotations, as shown in (19).
The outputof this module is easily translated into a form suit-able for a speech synthesizer, which produces pokenoutput with the desired intonation.
4(19) The X5 is a TUBE amplifier.L+H~ L(H%) H* LL$The modules described above and shown in Fig-ure 1 are implemented in Quintus Prolog.
The sys-tem produces the types of output shown in (20) andaA complete description of the CCG generator canbe found in (Prevost and Steedman, 1993).
CCG waschosen as the grammatical formalism because it licensesnon-traditional syntactic onstituents hat are congruentwith the bracketings imposed by information structureand intonational phrasing, as illustrated in (3).4The system currently uses the AT&T Bell Laborato-ries TTS system, but the implementation is easily adapt-able to other synthesizers.299(21), which should be interpreted as a single (twoparagraph) monologue satisfying a goal to describetwo different objects.
5 Note that both paragraphsinclude very similar types of information, but radi-cally different intonational contours, due to the dis-course context.
In fact, if the intonational patternsof the two examples are interchanged, the resultingspeech sounds highly unnatural.
(20) a.
Describe the x4.b.
The X4L+H* L(H%)is a SOLID-state AMPLIFIER.H* 14" LL$It COSTS EIGHT HUNDRED DOLLARS,H* H* H* H* LL%and PRODUCESH*ONE hundred watts-per-CHANNEL.H* H* LL$It was PRAISED by STEREOFOOL,Hi !H~ LH%an AUDIO JOURNAL,H* H* LH%but was REVILED by AUDIOFAD,H=" !H~" LH%ANOTHER audio journal.H* LL$(21) a.
Describe the x5.b.
The X5 is a TUBE amplifier.L+HI L(H%) Hi LL$IT costs NINE hundred ollars,L+H~ L(H%) Hi LH%produces TWO hundred watts-per-channel.Hi LH%and was praisedby Stereofool AND Audiofad.Hi LL$Several aspects of the output shown above areworth noting.
Initially, the program assumes thatthe hearer has no specific knowledge of any partic-ular objects in the knowledge base.
Note however,that every proposition put forth by the generatoris assumed to be incorporated into the bearer's etof beliefs.
Consequently, the descriptive phrase "anaudio journal," which is new information in the firstparagraph, is omitted from the second.
Additionally,when presenting the proposition 'Audiofad is an au-dio journal, '  the generator is able to recognize thesimilarity with the corresponding proposition aboutStereofool (i.e.
both propositions are abstractionsover the single variable open proposition 'X is an au-dio journal') .
The program therefore interjects theo~her property and produces "another audio jour-nal.
"5The implementation assigns slightly higher pitch toaccents bearing the subscript c (e.g.
H~), which markcontrastive focus as determined by the algorithm de-scribe above and in (Prevost, 1995).Several aspects of the contrastive intonational ef-fects in these examples also deserve attention.
Be-cause of the content generator's use of the rhetoricalcontrast predicate, items are eligible to receive stressin order to convey contrast before the contrast-ing items are even mentioned.
This phenomenonis clearly illustrated by the clause "PRAISED bySTEREOFOOL" in (20), which is contrastivelystressed before "REVILED by AUDIOFAD" is ut-tered.
Such situations are produced only when thecontrasting propositions are gathered by the contentplanner in a single invocation of the generator andidentified as contrastive when the rhetorical predi-cates are applied.
Moreover, unlike systems that relysolely on word class and given/new distinctions fordetermining accentual patterns, the system is ableto produce contrastive accents on pronouns despitetheir "given" status, as shown in (21).5 Conc lus ionsThe generation architecture described above and im-plemented in Quintus Prolog produces paragraph-length, spoken monologues concerning objects in asimple knowledge base.
The architecture relies ona mapping between a two-tiered information struc-ture representation and intonational tunes to pro-duce speech that makes appropriate contrastive dis-tinctions prosodically.
The process of natural lan-guage generation, in accordance with much of the re-cent literature in the field, is divided into three pro-cesses: high-level content planning, sentence plan-ning, and surface generation.
Two points concern-ing the role of intonation in the generation processare emphasized.
First, since intonational phrasing isdependent on the division of utterances into themeand theme, and since this division relates consecu-tive sentences to one another, matters of informationstructure (and hence intonational phrasing) mustbe largely resolved during the high-level planningphase.
Second, since accentual decisions are madewith respect o the particular linguistic realizationsof discourse properties and entities (e.g.
the choiceof referring expressions), these matters cannot befully resolved until the sentence planning phase.6 AcknowledgmentsThe author is grateful for the advice and help-ful suggestions of Mark Steedman, Justine Cassell,Kathy McKeown, Aravind Joshi, Ellen Prince, MarkLiberman, Matthew Stone, Beryl Hoffman and KrisTh6risson as well as the anonymous ACL review-ers.
Without the AT&T Bell Laboratories TTS sys-tem, and the patient advice on its use from JuliaHirschberg and Richard Sproat, this work would nothave been possible.
This research was funded byNSF grants IRI91-17110 and IRI95-04372 and thegenerous ponsors of the MIT Media Laboratory.300ReferencesBolinger, D. (1989).
Intonation and Its Uses.
Stan-ford University Press.Culicover, P. and Rochemont, M. (1983).
Stress andfocus in English.
Language, 59:123-165.Dale, R. and Haddock, N. (1991).
Content determi-nation in the generation of referring expressions.Computational Intelligence, 7(4):252-265.Davis, J. and Hirschberg, J.
(1988).
Assigning into-national features in synthesized spoken discourse.In Proceedings of the 26th Annual Meeting of theAssociation for Computational Linguistics, pages187-193, Buffalo.Engdahl, E. and Vallduvl, E.(1994).
Information packaging and grammar ar-chitecture: A constraint-based approach.
In Eng-dahl, E., editor, Integrating Information Structureinto Constraint-Based and Categorial Approaches(DYANA-2 Report R.1.3.B).
CLLI, Amsterdam.Grosz, B. J., Joshi, A. K., and Weinstein, S. (1986).Towards a computational theory of discourse in-terpretation.
Unpublished manuscript.Gussenhoven, C. (1983b).
On the Grammar and Se-mantics of Sentence Accent.
Foris, Dodrecht.Halliday, M. (1970).
Language structure and lan-guage function.
In Lyons, J., editor, New Horizonsin Linguistics, pages 140-165.
Penguin.Hirschberg, J.
(1990).
Accent and discourse context:Assigning pitch accent in synthetic speech.
In Pro-ceedings of the Eighth National Conference on Ar-tificial Intelligence, pages 952-957.Hoffman, B.
(1995).
The Computational Analysis ofthe Syntax and Interprelation of 'Free' Word Or-der in Turkish.
PhD thesis, University of Pennsyl-vania, Philadelphia.Hovy, E. (1993).
Automated iscourse generation us-ing discourse structure relations.
Artificial Intelli-gence, 63:341-385.Mann, W. and Thompson, S. (1986).
Rhetoricalstructure theory: Description and construction oftext structures.
In Kempen, G., editor, NaturalLanguage Generation: New Results in ArtificialIntelligence, Psychology and Linguistics, pages279-300.
Kluwer Academic Publishers, Boston.McKeown, K., Kukich, K., and Shaw, J.
(1994).Practical issues in automatic documentation gen-eration.
In Proceedings of the Fourth ACL Con-ference on Applied Natural Language Processing,pages 7-14, Stuttgart.
Association for Computa-tional Linguistics.McKeown, K. R. (1985).
Text Generation: Us-ing Discourse Strategies and Focus Constraints toGenerate Natural Language Text.
Cambridge Uni-versity Press, Cambridge.Meteer, M. (1991).
Bridging the generation gapbetween text planning and linguistic realization.Computational Intelligence, 7(4):296-304.Pierrehumbert, J.
(1980).
The Phonology and Pho-netics of English Intonation.
PhD thesis, Mas-sachusetts Institute of Technology.
Distributed byIndiana University Linguistics Club, Blooming-ton, IN.Prevost, S. (1995).
A Semantics of Contrast and In-formation Structure for Specifying Intonation inSpoken Language Generation.
PhD Thesis, Uni-versity of Pennsylvania.Prevost, S. and Steedman, M. (1993).
Generatingcontextually appropriate intonation.
In Proceed-ings of the 6th Conference of the European Chap-ter of the Association for Computational Linguis-tics, pages 332-340, Utrecht.Prevost, S. and Steedman, M. (1994).
Specifying in-tonation from context for speech synthesis.
SpeechCommunication, 15:139-153.Rainbow, O. and Korelsky, T. (1992).
Applied textgeneration.
In Proceedings of the Third Conferenceon Applied Natural Language Processing (ANLP-1992), pages 40-47.Reiter, E. and Mellish, C. (1992).
Using classifica-tion to generate text.
In Proceedings of the 30thAnnual Meeting of the Association for Computa-tional Linguistics, pages 265-272.Robin, J.
(1993).
A revision-based generation ar-chitecture for reporting facts in their historicalcontext.
In Horacek, H. and Zock, M., editors,New Concepts in Natural Language Generation:Planning, Realization and Systems, pages 238-265.
Pinter Publishers, New York.Rochemont, M. (1986).
Focus in Generative Gram-mar.
John Benjamins, Philadelphia.Sibun, P. (1991).
The Local Organization and Incre-mental Generation of Text.
PhD thesis, Universityof Massachusetts.Steedman, M. (1991a).
Structure and intonation.Language, pages 260-296.301
