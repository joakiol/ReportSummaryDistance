Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1060?1065,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsRobust Gram EmbeddingsTaygun Kekec?
and D.M.J.
TaxPattern Recognition and Bioinformatics LaboratoryDelft University of TechnologyDelft, 2628CD, The Netherlandstaygunkekec@gmail.com,D.M.J.Tax@tudelft.nlAbstractWord embedding models learn vectorial wordrepresentations that can be used in a varietyof NLP applications.
When training data isscarce, these models risk losing their gener-alization abilities due to the complexity ofthe models and the overfitting to finite data.We propose a regularized embedding formu-lation, called Robust Gram (RG), which pe-nalizes overfitting by suppressing the dispar-ity between target and context embeddings.Our experimental analysis shows that the RGmodel trained on small datasets generalizesbetter compared to alternatives, is more robustto variations in the training set, and correlateswell to human similarities in a set of word sim-ilarity tasks.1 IntroductionWord embeddings represent each word as a uniquevector in a linear vector space, encoding particularsemantic and syntactic structure of the natural lan-guage (Arora et al, 2016).
In various lingual tasks,these sequence prediction models shown superior re-sults over the traditional count-based models (Ba-roni et al, 2014).
Tasks such as sentiment analysis(Maas et al, 2011) and sarcasm detection (Ghosh etal., 2015) enjoys the merits of these features.These word embeddings optimize features andpredictors simultaneously, which can be interpretedas a factorization of the word cooccurence matrixC.
In most realistic scenarios these models have tobe learned from a small training set.
Furthermore,word distributions are often skewed, and optimiz-ing the reconstruction of C?
puts too much empha-sis on the high frequency pairs (Levy and Goldberg,2014).
On the other hand, by having an unlucky andscarce data sample, the estimated C?
rapidly deviatesfrom the underlying true cooccurence, in particu-lar for low-frequency pairs (Lemaire and Denhire,2008).
Finally, noise (caused by stemming, removalof high frequency pairs, typographical errors, etc.
)can increase the estimation error heavily (Arora etal., 2015).It is challenging to derive a computationallytractable algorithm that solves all these problems.Spectral factorization approaches usually employLaplace smoothing or a type of SVD weighting toalleviate the effect of the noise (Turney and Pantel,2010).
Alternatively, iteratively optimized embed-dings such as Skip Gram (SG) model (Mikolov etal., 2013b) developed various mechanisms such asundersampling of highly frequent hub words apriori,and throwing rare words out of the training.Here we propose a fast, effective and general-izable embedding approach, called Robust Gram,that penalizes complexity arising from the factorizedembedding spaces.
This design alleviates the needfrom tuning the aforementioned pseudo-priors andthe preprocessing procedures.
Experimental resultsshow that our regularized model 1) generalizes bet-ter given a small set of samples while other methodsyield insufficient generalization 2) is more robust toarbitrary perturbations in the sample set and alterna-tions in the preprocessing specifications 3) achievesmuch better performance on word similarity task,especially when similarity pairs contains unique andhardly observed words in the vocabulary.10602 Robust Gram EmbeddingsLet |y| = V the vocabulary size and N be the totalnumber of training samples.
Denote x, y to be V ?1discrete word indicators for the context and target:corresponding to the context and word indicatorsc, w in word embedding literature.
Define ?d?Vand ?d?V as word and context embedding matri-ces.
The projection on the matrix column space, ?x,gives us the embedding ~x ?
Rd.
We use ?x and ?xinterchangeably.
Using a very general formulationfor the regularized optimization of a (embedding)model, the following objective is minimized:J =N?iL(?,?, xi, yi) + g(?,?)
(1)where L(?,?, xi, yi) is the loss incurred by embed-ding example target yi using context xi and embed-ding parameters ?, ?, and where g(?,?)
is a reg-ularization of the embedding parameters.
Differentembedding methods differ in the form of specifiedloss function and regularization.
For instance, theSkip Gram likelihood aims to maximize the follow-ing conditional:L(?,?, x, y) = ?
log p(y|x,?,?
)= ?
logexp(?Ty ?x)?y?
exp(?Ty?
?x)(2)This can be interpreted as a generalization ofMultinomial Logistic Regression (MLR).
Rewriting(?y)T (?x) = (yT?T?x) = yTWx = Wyx showsthat the combination of ?
and ?
become the weightsin the MLR.
In the regression the input x is trans-formed to directly predict y.
The Skip Gram model,however, transforms both the context x and the tar-get y, and can therefore be seen as a generalizationof the MLR.It is also possible to penalize the quadratic lossbetween embeddings (Globerson et al, 2007):L(.)
= ?
log exp(?||?y ?
?x||2)?y?
exp(?||?y?
?
?x||2)(3)Since these formulations predefine a particularembedding dimensionality d, they impose a lowrank constraint on the factorization W = ?T?.This means that g(?,?)
contains ?rank(?T?
)with a sufficiently large ?.
The optimization withan explicit rank constraint is NP hard.
Instead,approximate rank constraints are utilized with aTrace Norm (Fazel et al, 2001) or Max Norm (Sre-bro and Shraibman, 2005).
However, adding suchconstraints usually requires semidefinite programswhich quickly becomes computationally prohibitiveeven with a moderate vocabulary size.Do these formulations penalize the complexity?Embeddings under quadratic loss are already reg-ularized and avoids trivial solutions thanks to thesecond term.
They also incorporate a bit weighteddata-dependent `2 norm.
Nevertheless, choosing alog-sigmoid loss for Equation 1 brings us to the SkipGram model and in that case, `p regularization is notstated.
Without such regularization, unbounded op-timization of 2V d parameters has potential to con-verge to solutions that does not generalize well.To avoid this overfitting, in our formulation wechoose g1 as follows:g1 =V?v?1(||?v||22 + ||?v||22) (4)where ?v is the row vector of words.Moreover, an appropriate regularization can alsopenalize the deviance between low rank matrices ?and ?.
Although there are words in the languagethat may have different context and target represen-tations, such as the 1, it is natural to expect that alarge proportion of the words have a shared repre-sentation in their context and target mappings.
Tothis end, we introduce the following regularization:g2 = ?2||??
?||2F (5)where F is the Frobenius matrix norm.
This as-sumption reduces learning complexity significantlywhile a good representation is still retained, opti-mization under this constraint for large vocabular-ies is going to be much easier because we limit thedegrees of freedom.The Robust Gram objective therefore becomes:LL+?1V?v(||?v||22 + ||?v||22)+?2||??
?||2F(6)1Consider prediction of Suleiman from the, and the from oa-sis.
We expect the to have different vectorial representations.1061where LL = ?Ni L(p(yi|xi,?,?))
is the data log-likelihood, p(yi|xi,?,?)
is the loglinear predictionmodel, and L the cross entropy loss.
Since we are inthe pursuit of preserving/restoring low masses in C?,norms such as `2 allow each element to still possessa small probability mass and encourage smoothnessin the factorized ?T?
matrix.
As L is picked asthe cross entropy, Robust Gram can be interpretedas a more principled and robust counterpart of SkipGram objective.One may ask what particular factorization Equa-tion 6 induces.
The objective searches for ?,?
ma-trices that have similar eigenvectors in the vectorspace.
A spectral PCA embedding obtains an asym-metric decomposition W = U?V T with ?
= Uand ?
= ?V , albeit a convincing reason for embed-ding matrices to be orthonormal lacks.
In the SkipGram model, this decomposition is more symmet-ric since neither ?
nor ?
are orthonormal and di-agonal weights are distributed across the factorizedembeddings.
A symmetric factorization would be:?
= U?0.5,?
= ?0.5V T as in (Levy and Goldberg,2014).
The objective in Eq.
6 converges to a moresymmetric decomposition since ||?
?
?|| is penal-ized.
Still some eigenvectors across context and tar-get maps are allowed to differ if they pay the cost.In this sense our work is related to power SVD ap-proaches (Caron, 2000) in which one searches an ato minimize ||W ?
U?a?1?aV T ||.
In our formula-tion, if we enforce a solution by applying a strongconstraint on ||?
?
?||2F , then our objective willgradually converge to a symmetric powered decom-position such that U ?
V .3 ExperimentsThe experiments are performed on a subset of theWikipedia corpus containing approximately 15Mwords.
For a systematic comparison, we use thesame symmetric window size adopted in (Penning-ton et al, 2014), 10.
Stochastic gradient learningrate is set to 0.05.
Embedding dimensionality isset to 100 for model selection and sensitivity anal-ysis.
Unless otherwise is stated, we discard the mostfrequent 20 hub words to yield a final vocabularyof 26k words.
To understand the relative merit of0 2 4 6 8 10?20246810?
1LLFigure 1: The LL objective for varying ?1, ?2.our approach 2 , Skip Gram model is picked as thebaseline.
To retain the learning speed, and avoidinctractability of maximum likelihood learning, welearn our embeddings with Noise Contrastive Es-timation using a negative sample (Gutmann andHyva?rinen, 2012).3.1 Model SelectionFor model selection, we are going to illustrate thelog likelihood of different model instances.
How-ever, exact computation of the LL is computation-ally difficult since a full pass over the validationlikelihood is time-consuming with millions of sam-ples.
Hence, we compute a stochastic likelihoodwith a few approximation steps.
We first subsam-ple a million samples rather than a full evaluationset, and then sample few words to predict in thewindow context similar to the approach followed in(Levy and Goldberg, 2014).
Lastly, we approximatethe normalization factor with one negative samplefor each prediction score (Mnih and Kavukcuoglu,2013)(Gutmann and Hyva?rinen, 2012).
Such anapproximation works fine and gives smooth errorcurves.
The reported likelihoods are computed byaveraging over 5-fold cross validation sets.Results.
Figure 1 shows the likelihood LL ob-tained by varying {?1, ?2}.
The plot shows thatthere exits a unique minimum and both constraintscontribute to achieve a better likelihood comparedto their unregularized counterparts (for which ?1 =?2 = 0).
In particular, the regularization imposed bythe differential of context and target embeddings g2contributes more than the regularization on the em-2Our implementation can be downloaded fromgithub.com/taygunk/robust gram embeddings1062beddings ?
and ?
separately.
This is to be expectedas g2 also incorporates an amount of norm boundon the vectors.
The region where both constraintsare employed gives the best results.
Observe thatwe can simply enhance the effect of g2 by addinga small amount of bounded norm g1 constraint in astable manner.
Doing this with pure g2 is risky be-cause it is much more sensitive to the selection of?2.
These results suggest that the convex combina-tion of stable nature of g1 with potent regularizer ofg2, finally yields comparably better regularization.3.2 Sensitivity AnalysisIn order to test the sensitivity of our model and base-line Skip Gram to variations in the training set, weperform two sensitivity analyses.
First, we simu-late a missing data effect by randomly dropping out?
?
[0, 20] percent of the training set.
Under sucha setting, robust models are expected to be effectedless from the inherent variation.
As an addition,we inspect the effect of varying the minimum cut-off parameter to measure the sensitivity.
In this ex-periment, from a classification problem perspective,each instance is a sub-task with different numberof classes (words) to predict.
Instances with smallcut-off introduces classification tasks with very fewtraining samples.
This cut-off choice varies in differ-ent studies (Pennington et al, 2014; Mikolov et al,2013b), and it is usually chosen based on heuristicand storage considerations.0 5 10 15 20?0.20.30.40.50.6LLRGSGFigure 2: Training dropouts effect on LL.Results.
Figure 2 illustrates the likelihood of theRobust and Skip Gram model by varying the dropoutratio on the training set.
As the training set shrinks,both models get lower LL.
Nevertheless, likelihooddecay of Skip Gram is relatively faster.
When 20%10 15 20 25 30 35 40 45 50cut-off0.200.250.300.350.400.450.50LLRGSGFigure 3: LL w.r.t the cut-off parameter.drop is applied, the LL drops to 74% in the SGmodel.
On the other hand the RG model not onlystarts with a much higher LL, the drop is also to75.5%, suggesting that RG objective is more resis-tant to random variations in the training data.Figure 3 shows the results of varying the rare-words cut-off threshold.
We observe that the like-lihood obtained by the Skip Gram is consistentlylower than that of the Robust Gram.
The graphshows that throwing out these rare words helps theobjective of SG slightly.
But for the Robust Gram re-moving the rare words actually means a significantdecrease in useful information, and the performancestarts to degrade towards the SG performance.
RGavoids the overfitting occurring in SG, but still ex-tracts useful information to improve the generaliza-tion.3.3 Word Similarity PerformanceThe work of (Schnabel et al, 2015) demonstratesthat intrinsic tasks are a better proxy for measuringthe generic quality than extrinsic evaluations.
Mo-tivated by this observation, we follow the experi-mental setup of (Schnabel et al, 2015; Agirre etal., 2009), and compare word correlation estimatesof each model to human estimated similarities withSpearman?s correlation coefficient.
The evaluationis performed on several publicly available word sim-ilarity datasets having different sizes.
For datasetshaving multiple subjects annotating the word simi-larity, we compute the average similarity score fromall subjects.We compare our approach to set of techniques onthe horizon of spectral to window based approaches.A fully spectral approach, HPCA, (Lebret and Le-1063bret, 2013) extracts word embeddings by running aHellinger PCA on the cooccurrence matrix.
For thismethod, context vocabulary upper and lower boundparameters are set to {1, 10?5}, as promoted by itsauthor.
GLoVe (Pennington et al, 2014) approachformulates a weighted least squares problem to com-bine global statistics of cooccurence and efficiencyof window-based approaches.
Its objective can beinterpreted as an alternative to the cross-entropy lossof Robust Gram.
The xmax, ?
values of the GLoVeobjective is by default set to 100, 3/4.
Finally, wealso compare to shallow representation learning net-works such as Skip Gram and Continuous Bag ofWords (CBoW) (Mikolov et al, 2013a), competitivestate of the art window based baselines.We set equal window size for all these models,and iterate three epochs over the training set.
Toyield more generality, all results obtained with 300dimensional embeddings and subsampling parame-ters are set to 0.
For Robust Gram approach, we haveset ?1, ?2 = {0.3, 0.3}.
To obtain the similarity re-sults, we use the final ?
context embeddings.Results.
Table 1 depicts the results.
The first ob-servation is that in this setting, obtaining word sim-ilarity using HPCA and GLoVe methods are subop-timal.
Frankly, we can conjecture that this scarcedata regime is not in the favor of the spectral meth-ods such as HPCA.
Its poor performance can be at-tributed to its pure geometric reconstruction formu-lation, which runs into difficulties by the amount ofinherent noise.
Compared to these, CBoW?s perfor-mance is moderate except in the RW dataset whereit performs the worst.
Secondly, the performanceof the SG is relatively better compared to these ap-proaches.
Surprisingly, under this small data setting,RG outperforms all of its competitors in all datasetsexcept for RG65, a tiny dataset of 63 words con-taining very common words.
It is admissible thatRG sacrifices a bit in order to generalize to a largevariety of words.
Note that it especially wins bya margin in MEN and Rare Words (RW) datasets,having the largest number of similarity query sam-ples.
As the number of query samples increases,RG embeddings?
similarity modeling accuracy be-comes clearly perceptible.
The promising result Ro-bust Gram achieves in RW dataset alo sheds lighton why CBoW performed worst on RW: CBOWoverfits rapidly confirming the recent studies on theRG65 WS WSS WSR MEN RWSize 63 353 203 252 3000 2034CBoW 48.5 59.7 71.8 61.3 56.5 26.4GloVe 48.9 56.2 61.5 59.1 53.0 30.0SG 59.2 71.7 74.6 66.5 64.7 33.5HPCA 32.1 48.6 52.9 51.5 49.9 30.7RG 59.0 71.7 74.8 66.7 65.8 34.0Table 1: Spearman?s ?
coefficient.
Higher is better.stability of CBoW (Luo et al, 2014).
Finally, theseword similarity results suggest that RG embeddingscan yield much more generality under data scarcity.4 ConclusionThis paper presents a regularized word embeddingapproach, called Robust Gram.
In this approach, themodel complexity is penalized by suppressing de-viations between the embedding spaces of the tar-get and context words.
Various experimental resultsshow that RG maintains a robust behaviour undersmall sample size situations, sample perturbationsand it reaches a higher word similarity performancecompared to its competitors.
The gain from RobustGram increases notably as diverse test sets are usedto measure the word similarity performance.In future work, by taking advantage of the promis-ing results of Robust Gram, we intend to explore themodel?s behaviour in various settings.
In particu-lar, we plan to model various corpora, i.e.
predictivemodeling of sequentially arriving network packages.Another future direction might be encoding avail-able domain knowledge by additional regularizationterms, for instance, knowledge on synonyms can beused to reduce the degrees of freedom of the opti-mization.
We also plan to enhance the underlyingoptimization by designing Elastic constraints (Zouand Hastie, 2005) specialized for word embeddings.AcknowledgmentsThe authors acknowledge funding by the DutchOrganization for Scientific Research (NWO; grant612.001.301).
We also would like to thank HamdiDibeklioglu and Mustafa Unel for their kind supportduring this work.1064ReferencesEneko Agirre, Enrique Alfonseca, Keith Hall, JanaKravalova, Marius Pas?ca, and Aitor Soroa.
2009.
Astudy on similarity and relatedness using distributionaland wordnet-based approaches.
In Proceedings of Hu-man Language Technologies: The 2009 Annual Con-ference of the North American Chapter of the Asso-ciation for Computational Linguistics, NAACL ?09,pages 19?27.Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma,and Andrej Risteski.
2015.
Random walks on contextspaces: Towards an explanation of the mysteries of se-mantic word embeddings.
CoRR, abs/1502.03520.Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma,and Andrej Risteski.
2016.
Linear algebraic structureof word senses, with applications to polysemy.
CoRR,abs/1601.03764.Marco Baroni, Georgiana Dinu, and Germa?n Kruszewski.2014.
Don?t count, predict!
A systematic compari-son of context-counting vs. context-predicting seman-tic vectors.
In Proceedings of the 52nd Annual Meet-ing of the Association for Computational Linguistics,pages 238?247, June.John Caron.
2000.
Experiments with lsa scoring: Opti-mal rank and basis.
In Proc.
of SIAM ComputationalInformation Retrieval Workshop.Maryam Fazel, Haitham Hindi, and Stephen P. Boyd.2001.
A rank minimization heuristic with applicationto minimum order system approximation.
In In Pro-ceedings of the 2001 American Control Conference,pages 4734?4739.Debanjan Ghosh, Weiwei Guo, and Smaranda Muresan.2015.
Sarcastic or not: Word embeddings to predictthe literal or sarcastic meaning of words.
In EMNLP,pages 1003?1012.
The Association for ComputationalLinguistics.Amir Globerson, Gal Chechik, Fernando Pereira, andNaftali Tishby.
2007.
Euclidean embedding of co-occurrence data.
J. Mach.
Learn.
Res., 8:2265?2295.Michael U. Gutmann and Aapo Hyva?rinen.
2012.Noise-contrastive estimation of unnormalized statisti-cal models, with applications to natural image statis-tics.
J. Mach.
Learn.
Res., 13(1):307?361, February.Re?mi Lebret and Ronan Lebret.
2013.
Word emdeddingsthrough hellinger PCA.
CoRR, abs/1312.5542.Benot Lemaire and Guy Denhire.
2008.
Effects of high-order co-occurrences on word semantic similarities.CoRR, abs/0804.0143.Omer Levy and Yoav Goldberg.
2014.
Neural word em-bedding as implicit matrix factorization.
In Advancesin Neural Information Processing Systems 27, pages2177?2185.Qun Luo, Weiran Xu, and Jun Guo.
2014.
A study onthe cbow model?s overfitting and stability.
In Proceed-ings of the 5th International Workshop on Web-scaleKnowledge Representation Retrieval &#38; Reason-ing, Web-KR ?14, pages 9?12.
ACM.Andrew L. Maas, Raymond E. Daly, Peter T. Pham, DanHuang, Andrew Y. Ng, and Christopher Potts.
2011.Learning word vectors for sentiment analysis.
In Pro-ceedings of the 49th Annual Meeting of the Associa-tion for Computational Linguistics: Human LanguageTechnologies - Volume 1, HLT ?11, pages 142?150.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013a.
Efficient estimation of word representa-tions in vector space.
CoRR, abs/1301.3781.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-rado, and Jeffrey Dean.
2013b.
Distributed represen-tations of words and phrases and their compositional-ity.
CoRR, abs/1310.4546.Andriy Mnih and Koray Kavukcuoglu.
2013.
Learningword embeddings efficiently with noise-contrastive es-timation.
In C. J. C. Burges, L. Bottou, M. Welling,Z.
Ghahramani, and K. Q. Weinberger, editors, Ad-vances in Neural Information Processing Systems 26,pages 2265?2273.Jeffrey Pennington, Richard Socher, and ChristopherManning.
2014.
In Proceedings of the 2014 Confer-ence on Empirical Methods in Natural Language Pro-cessing (EMNLP), pages 1532?1543, Doha, Qatar.Tobias Schnabel, Igor Labutov, David M. Mimno, andThorsten Joachims.
2015.
Evaluation methods for un-supervised word embeddings.
In EMNLP, pages 298?307.Nathan Srebro and Adi Shraibman.
2005.
Rank, trace-norm and max-norm.
In COLT, volume 3559 ofLecture Notes in Computer Science, pages 545?560.Springer.Peter D. Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of semantics.J.
Artif.
Int.
Res., 37(1):141?188, January.Hui Zou and Trevor Hastie.
2005.
Regularization andvariable selection via the elastic net.
Journal of theRoyal Statistical Society, Series B, 67:301?320.1065
