Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 2140?2150, Dublin, Ireland, August 23-29 2014.Left-corner Transitions on Dependency ParsingHiroshi Noji and Yusuke MiyaoDepartment of InformaticsThe Graduate University for Advanced StudiesNational Institute of Informatics, Tokyo, Japan{noji,yusuke}@nii.ac.jpAbstractWe propose a transition system for dependency parsing with a left-corner parsing strategy.
Unlikeparsers with conventional transition systems, such as arc-standard or arc-eager, a parser with oursystem correctly predicts the processing difficulties people have, such as of center-embedding.We characterize our transition system by comparing its oracle behaviors with those of other tran-sition systems on treebanks of 18 typologically diverse languages.
A crosslinguistical analysisconfirms the universality of the claim that a parser with our system requires less memory forparsing naturally occurring sentences.1 IntroductionIt is sometimes argued that transition-based dependency parsing is appealing not only from an engi-neering perspective due to its efficiency, but also from a scientific perspective: These parsers process asentence incrementally similar to a human parser, which have motivated several studies concerning theircognitive plausibility (Nivre, 2004; Boston and Hale, 2007; Boston et al., 2008).
A cognitively plausibledependency parser is attractive for many reasons, one of the most important being that dependency tree-banks are available in many languages, so it is suitable for crosslinguistical studies of human languageprocessing (Keller, 2010).
However, current transition systems based on shift-reduce actions fully orpartially employ a bottom-up strategy1, which is problematic from a psycholinguistical point of view:Bottom-up or top-down strategies are known to fail in predicting the difficulty for certain sentences, suchas center-embedding, which people have troubles in comprehending (Abney and Johnson, 1991).We propose a transition system for dependency parsing with a left-corner strategy.
For constituencyparsing, unlike other strategies, the arc-eager left-corner strategy is known to correctly predict processingdifficulties people have (Abney and Johnson, 1991).
To the best of our knowledge, however, the idea ofleft-corner strategy has not been introduced in the dependency parsing literature.
We define the memorycost for a transition system as the number of unconnected subtrees on a stack.
Under this condition, theproposed system incurs non-constant memory cost only when encountering center-embedded structures.After developing the transition system, we characterize it by looking into the following question: Isit true that naturally occurring sentences can be parsed on this system with a lower memory overhead?This should be true under the assumptions that 1) people avoid generating a sentence that causes diffi-culty for them, and 2) center-embedding is a kind of such structure.
Specifically, we focus on analyzingthe oracle transitions of the system, i.e., parser actions to recover the gold dependency tree for a sen-tence.
In English, it is known that left-corner transformed treebank sentences can be parsed with lessmemory (Schuler et al., 2010), but our focus in this paper is on the language universality of the claim ina crosslingual setting.
Two different but relevant motivations exist for this analysis.
The first is to an-swer the following scientific question: Is the claim that people tend to avoid generating center-embeddedsentences language universal?
This is unclear since the observation that a center-embedded sentence isThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1The top-down parser of Hayashi et al.
(2012) is an exception, but its processing is not incremental.2140difficult to comprehend is from psycholinguistic studies mainly on English.
The second motivation isto verify whether a parser with the developed system can be viable for crosslinguistical study of humanlanguage processing.
There is evidence that a human parser cannot store elements of a small constantnumber, such as three or four (Cowan, 2001).
If our system confirms to such a severe constraint, we mayclaim its cognitive plausibility across languages.
We will pursue these questions using the multilingualdependency treebanks from the CoNLL-X and 2007 shared tasks (Buchholz and Marsi, 2006; Nivre etal., 2007).In short, our contributions of this paper can be sketched as follows:1.
We formulate a transition system for dependency parsing with a left-corner strategy.2.
We characterize our transition system with its memory cost by simulating oracle transitions alongwith other transition systems on the CoNLL multilingual treebanks.
This is the first empirical studyof required memory for left-corner parsing in a crosslinguistical setting.2 Memory cost of Parsing AlgorithmsIn this work, we focus on the memory cost for dependency parsing transition systems.
While therehave been many studies concerning the memory cost for an algorithm in constituency parsing (Abneyand Johnson, 1991; Resnik, 1992), the same kind of study is rare in dependency parsing.
This sectiondiscusses the memory cost for the current dependency parsing transition systems.
Before that, we firstreview the known results in constituency parsing regarding memory cost.2.1 Center-Embedding and the Left-Corner Strategy10X12d6X8c2X4b1a357911132X5X9X12d8c10134b6111a372X9X12d5X7c4b6810 131a311The structures on the right side are called left-branching,right-branching, and center-embedding, respectively.
Peo-ple have difficulty when parsing center-embedded struc-tures, while no difficulty with right-branching or left-branching structures.
An example of a center-embedded sentence is the rat [the cat [the dog chased]bit] ate the cheese, which is difficult, but if we rewrite it as the cheese was eaten [by the rat [that bit thecat [that chased the dog]]], which is a kind of right-branching structure, the parse becomes easier.Abney and Johnson (1991) showed that top-down or bottom-up strategies2fail to predict this result.For example, for the right-branching structure, a bottom-up strategy requires O(n) memory, since itmust first construct a subtree of c and d, but the center-embedded structure requires less memory.
Thearc-eager left-corner strategy correctly predicts the difficulty of a center-embedded structure, which ischaracterized by the following order of recognitions of nodes and arcs:1.
A node is enumerated when the subtree of its first child has been enumerated.2.
An arc is enumerated when two nodes it connects have been enumerated.The numbers on the trees above indicate the order of recognition for this strategy.
We can see that itrequires a constant memory for both right-branching and left-branching structures.
For example, forthe right-branching structure, it reaches 7 after reading b, which means that a and b are connected by asubtree.
On the other hand, for the center-embedded structure, it reaches 6 after reading b, but a and bcannot be connected at this point, requiring extra memory.2.2 Transition-based Dependency ParsingNext, we summarize the issues with current transition systems for dependency parsing with regards totheir memory cost.
A transition-based dependency parser processes a sentence on a transition system,which is defined as a set of configurations and a set of transitions between configurations (Nivre, 2008).Each configuration has a stack preserving constructed subtrees on which we define the memory cost as afunction for each system.2We should distinguish between two types of characterizations of parsing: strategy and algorithm.
A parsing strategy isan abstract notion that defines ?a way of enumerating the nodes and arcs of parse trees?
(Abney and Johnson, 1991), whilea parsing algorithm defines the implementation of that strategy, typically with push-down automata (Johnson-Laird, 1983;Resnik, 1992).
A parsing strategy is useful for characterizing the properties of each parser, and we concentrate on the strategyfor exposition of constituency parsing.
For dependency parsing, we mainly discuss the algorithm, i.e., the transition system.2141a b c(a)a b c(b)a b c(c)XXcba(d)Figure 1: (a)?
(c): Right-branching dependencytrees for three words; (d): the corresponding CNF.Arc-standard Arc-eager Left-cornerleft-branching O(1) O(1) O(1)right-branching O(n) O(1 ?
n) O(1)center-embedding O(n) O(1 ?
n) O(n)Table 1: Order of the memory cost for each struc-ture for each transition system.
O(1 ?
n) meansthat it processes some structures with a constantcost but some with a non-constant cost.Arc-Standard We define the memory cost as the number of elements on the stack since all stackelements are disjoint.
In this setting, we can see that the arc-standard system has a problem for a right-branching structure, such as aybycy?
?
?
, in which the system first pushes all words on the stack beforeconnecting each pair of words, requiring O(n) memory.
Nivre (2004) discussed the problem with thissystem in greater detail, observing that its stack size grows when processing structures that become right-branching when converted to the Chomsky normal form (CNF) of a context-free grammar (CFG).
Figure1 lists those dependency structures for three words, for which the system must construct a subtree of band c before connecting a to either, requiring extra memory.
This is because the system builds a treebottom-up: each token collects all dependents before being attached to its head.
In fact, the arc-standardsystem is essentially equivalent to the push-down automaton of a CFG in the CNF with a bottom-upstrategy (Nivre, 2004), so it has the same property as the bottom-up parser for a CFG.Arc-Eager In the arc-eager system, the stack contains sequences of tokens comprising connected com-ponents, so we can define the memory cost as the number of connected components on the stack.
Withthis definition, we can partially resolve the problem with the arc-standard system.
The arc-eager systemdoes not incur any cost for processing the structure in Figure1(a) and aybycy?
?
?
since it can connectall tokens on the stack (Nivre, 2004).
Because its construction is no longer pure bottom-up, it is difficultto formally characterize the cost based on the type of tree structure.
However, this transition system can-not correctly predict difficulties with center-embedding because the cost never increases as long as alldependency arcs are left-to-right, e.g., a sentence aybyc d becomes center-embedding when convertedto a CNF, but it does not incur any cost for it.
Note that this system still incurs cost for some right-branching structures, such as in Figures 1(b?c), and some center-embedded structures.
Therefore, for thearc-eager system, it is complicated to discuss the required order of memory cost.
We summarize theseresults in Table 1.
Our goal is to develop an algorithm with the properties of the last column, requiringnon-constant memory for only center-embedded structures.Other systems All systems where stack elements cannot be connected have the same problem as thearc-standard system because of their bottom-up constructions, including the hybrid system of Kuhlmannet al.
(2011).
Kitagawa and Tanaka-Ishii (2010) and Sartorio et al.
(2013) present an interesting variant,which attaches a node to another node that may not be the head of a subtree on the stack.
We can usethe same reasoning for the arc-eager system for these systems: they sometimes do not incur costs forcenter-embedded structures, while they incur a non-constant cost for some right-branching structures.3 Left-corner Dependency ParsingWe now discuss the construction of our transition system with the left-corner strategy.
Resnik (1992)proposed a push-down recognizer for a CFG.
In the following, we instead characterize his algorithm byinference rules, which are more intuitive and helpful to adapt the idea for dependency parsing.Prediction:B??????
A?B CACBComposition:ACB D????????
C?D EACEDBPrediction and Composition There are twocharacteristic operations in the push-down recog-nizer of Resnik (1992): prediction and composi-tion.
We show inference rules of these operationson the right side:Prediction is used to predict the parent node andthe sibling of a recognized subtree when the sub-2142SHIFT (?, j|?,A) 7?
(?|?j?, ?, A)INSERT (?|???1|i|x(?)?
), j|?,A) 7?
(?|??
?1|i|j?, ?, A ?
{(i, j)} ?
{?k??
(j, k))LEFT-PRED (?|?
?11, ?
?
?
?, ?, A) 7?
(?|?x(?11)?, ?, A)RIGHT-PRED (?|?
?11, ?
?
?
?, ?, A) 7?
(?|?
?11, x(?
)?, ?, A)LEFT-COMP (?|???2|x(?)?|?
?11, ?
?
?
?, ?, A) 7?
(?|???2|x(?
?
{?11})?, ?, A)RIGHT-COMP (?|???2|x(?)?|?
?11, ?
?
?
?, ?, A) 7?
(?|???2|?11|x(?
)?, ?, A ?
{?k??
(?11, k)})Figure 2: Actions of the left-corner transition system.LEFT-PRED: RIGHT-PRED:axaaX(x)xaaaxaX(a)xaLEFT-COMP: RIGHT-COMP:abxbxaaX(b)xbX(b)X(x)xababxbaxaX(b)xbX(b)X(a)xabFigure 3: Correspondences of reduce actions between dependency and CFG.
Nonterminal X(t) meansthat its lexical head is t. We only show minimal example subtrees for simplicity.
However, a can havean arbitrary number of children, so can b or x, as long as x is on a right spine and has no right children.tree is complete and its parent node is not yet recognized.
Composition composes two subtrees by firstpredicting the parent and the sibling of a recognized subtree then immediately connecting trees by iden-tifying the same node on two trees (C, in this case).
This is used when the parent node of a completedsubtree has already been predicted as a part of another tree in a top-down fashion.Dummy Node We now turn to the discussion of dependency parsing.
The key characteristic of ourtransition system is the introduction of a dummy node on a subtree, which is needed to represent asubtree containing some predicted structures as in constituency subtrees for Resnik?s recognizer.
To getan intuition of the parser actions, we present a simulation of transitions for the sentence in Figure 1(b),of which current systems fail to predict its difficulty.
Our system first shifts a then conducts a kind ofprediction operation, resulting in a subtreexa, where x is a dummy node.
This means that we predictthat a will become a left dependent of an incoming word.
Next, it shifts b to the stack then conducts acomposition operation to obtain a treexab.
It finally inserts c to the position of x, recovering the tree.Transition system As in many other transition systems, a configuration for our system is a tuple c =(?, ?,A), where ?
is a stack, and we use a vertical bar to signify an append operation, e.g., ?
= ?
?|?1denoting ?1is the top most element of the stack ?, and ?
is an input buffer consisting of token indexesnot processed yet.
?
= j|?
?means j is a first element of ?, and A ?
Vw?
Vwis a set of arcs given Vw,a set of token indexes for a sentence w.Stack:w2w1xw3w5w4w6w7?
= [?2, x({3, 5})?, ?6, 7?]?
= [8, 9, ?
?
?
, n]A = {(2, 1), (5, 4), (6, 7)}Each element of a stack is a list representing a rightspine of a subtree, as in Kitagawa and Tanaka-Ishii(2010) and Sartorio et al.
(2013).
A right spine ?i=?
?i1, ?i2, ?
?
?
, ?ik?
consists of all nodes in a descendingpath from the head of ?i, i.e., ?i1, taking the rightmostchild at each step.
We also write ?i= ?
?i|?ikmeaning that ?ikis the right most node of spine ?i.
Eachelement of ?iis an index of a token in a sentence, or a dummy node x(?
), where ?
is a set of the leftdependents of x.
The figure above depicts an example of the configuration, where the i-th word in asentence is written as wion the stack.In the following, we say a right spine ?iis complete if it does not contain any dummy nodes, while?icontaining a dummy node is referred to as incomplete.
Our transition system uses six actions, two of2143which are shift actions and four are reduce actions.
All actions are defined in Figure 2.Shift Actions There are two kinds of shift actions: SHIFT and INSERT3.
SHIFT moves a token fromthe top of the buffer to the stack.
INSERT replaces a dummy node on the top of the stack with a tokenfrom the top of the buffer.
This adds arcs from/to tokens connected to the dummy node.
Note that thisaction can be performed for a configuration where x(?)
is the top of ?1or ?
is empty, in which casearcs (i, j) or ?k??
(j, k) are not added.
Resnik (1992) does not define this action, but instead uses averification operation (Rule 9).
One can view our INSERT action as a composition of two actions: SHIFTand a verification.
We note that after these shift actions, the top element of the stack must be complete.Reduce Actions Reduce actions create new arcs for subtrees on the stack.
LEFT-PRED and RIGHT-PRED correspond to the predictions of the CFG counterpart.
Figure 3 describes these transitions forminimal subtrees.
LEFT-PRED assigns a dummy node x as the head of a (this corresponds to ?11), whileRIGHT-PRED creates x as a new right dependent.
When we convert the resulting tree into a CNF, we cansee that the difference between these two operations lies in the predicted parent node of a: LEFT-PREDpredicts a nonterminal X(x), i.e., it predicts that the head of this subtree is the head of the predictingsibling node, while RIGHT-PRED predicts that the head is a.
Note that different from CFG rules, we donot have to predict the actual sibling node; rather, we can abstract this predicted node as a dummy nodex.
A similar correspondence holds between the composition actions: RIGHT-COMP and LEFT-COMP.We note that to obtain a valid tree, shift and reduce actions must be performed alternatively.
Wecan prove this as follows: Let c = (?|?2|?1, ?, A).
Since reduce actions turn an incomplete ?1into acomplete subtree, we cannot perform two consecutive reduce actions.
Shift actions make ?1complete.After a shift action, we cannot perform INSERT since it requires ?ito be incomplete; if we performSHIFT, the top two elements on the stack become complete, but we cannot connect these two trees sincethe only way to connect two trees on the stack is composition, but this requires ?2to be incomplete.Defining Oracle The oracle for a transition system is a function that returns a correct action given thecurrent configuration and a set of gold arcs.
It is typically used for training a parser (Nivre, 2008), butwe define it to analyze the behavior of our system on treebank sentences.First, we show that our system has the spurious ambiguity, and discuss its implications.
Consider asentence axbyc, which can be parsed with two different action sequences as follows:1.
SHIFT?
LEFT-PRED?
INSERT?
RIGHT-PRED?
INSERT2.
SHIFT?
LEFT-PRED?
SHIFT?
RIGHT-COMP?
INSERTThe former INSERTs b at step 3, then RIGHT-PREDs to wait for a right dependent (c).
The latter, on theother hand, SHIFTs b at step 3, then RIGHT-COMPs to combine two subtrees (axx and b) to obtain atree axbyx.
These ambiguities between action sequences and the resulting tree are referred to as thespurious ambiguity.
Next, we analyze the underlying differences between these two operations.
Weargue that the difference lies in the form of the recognized constituency tree: The former RIGHT-PREDsat step 4, which means that it recognizes a constituency of the form ((a b) c), while the latter recognizes(a (b c)) due to its RIGHT-COMP operation.
Therefore, the spurious ambiguity of our system is caused bythe ambiguity of converting a dependency tree to a constituency tree.
Recently, some transition systemshave exploited similar ambiguities using dynamic oracles (Goldberg and Nivre, 2013; Sartorio et al.,2013; Honnibal et al., 2013).
The same type of analysis might be possible for our system, but we leaveit for future work; here we only present a static oracle and discuss its properties.Since our system performs shift and reduce actions interchangeably, we need two functions to definethe oracle.
Let c = (?|?2|?1, ?, A).
The next shift action is determined as follows:?
INSERT: if ?1= ???1|i|x(?)?
and (i, j) ?
Agand j has no dependents in ?
(if i exists) or ?k ??
; (j, k) ?
Ag(otherwise).?
SHIFT: otherwise.The next reduce action is determined as follows:3We use small caps to refer to a specific action, e.g., SHIFT, while ?shift?
refers to an action type.2144?
LEFT-COMP: if ?2= ???2|i|x(?
)?, ?1= ?
?11, ?
?
?
?, ?11has no dependents in ?, and ?11can be aleft dependent of x: i?s next dependent is the head of ?11(if i exists) or k ?
?
and ?11share thesame head (otherwise).?
RIGHT-COMP: if ?2= ???2|i|x(?
)?, ?1= ?
?11, ?
?
?
?, ?11has one more dependent in ?, and ?11canbe insertable at the position of x: (i, ?11) ?
Agor ?k ?
?
; (?11, k) ?
Ag.?
RIGHT-PRED: if ?1= ?
?11, ?
?
?
?
and ?11has one more dependent in ?.?
LEFT-PRED: otherwise.Each condition checks whether we can obtain the gold dependency arcs after the transition.
This oraclefollows the strategy of ?compose or insert when possible?.
As we saw in the example, sometimes INSERTand SHIFT both can be valid to recover the gold arcs; however, we always select INSERT.
Sometimes thesame ambiguity occurs between LEFT-COMP and LEFT-PRED or RIGHT-COMP and RIGHT-PRED, butwe always prefer composition.As we saw above, the spurious ambiguity of our system occurs when the conversion from a depen-dency tree to a constituency tree is not deterministic.
This oracle has the property that its recognizedconstituency tree corresponds to the one that can be obtained by constructing all left-arcs first given a de-pendency tree.
For example, in the above example for axbyc, we select action sequences 1 to recognizea constituency of ((a b) c).
We can prove this property by showing that the algorithm always collects allleft-arcs for a head before any right-arcs; Not doing INSERT or composition when possible means thatwe create a right-arc for a head when the left-arcs are not yet completed.
We can also verify that thisalgorithm can parse all no-center-embedded sentences in a CNF converted in this manner with the stackdepth never exceeding three, requiring non-constant memory for only center-embedded structures.4 Memory Cost AnalysisTo characterize our transition system, we compare it to other systems by observing the incurred memorycost during running oracle transitions for sentences on a set of typologically diverse languages.
For thisanalysis, we aim to verify the language universality of the claim: naturally occurring sentences shouldbe parsed with a left-corner parser with less required memory.Settings We collect 18 treebanks from the CoNLL-X and 2007 shared tasks (Buchholz and Marsi,2006; Nivre et al., 2007).
Some languages were covered by both shared tasks; we use only 2007 data.We remove sentences with non-projective arcs (Nivre, 2008) or without any root nodes.
We follow thecommon practice adding a dummy root token to each sentence.
This token is placed at the end of eachsentence, as in Ballesteros and Nivre (2013), since it does not change the cost on sentences with one roottoken on all systems.We compare three transition systems: arc-standard, arc-eager, and left-corner.
For each system, weperform oracle transitions for all sentences and languages, measuring the memory cost for each configu-ration defined as follows.
For the arc-standard and left-corner systems, we use the number of elements onthe stack.
This arc-standard system uses the original formulation of Nivre (2003), connecting two itemson the stack at the reduce action.
For the arc-eager system, we use the number of connected components.The system can create a subtree at the beginning of a buffer, in which case we add 1 to the cost.We run a static oracle for each system.
For the left-corner system, we implemented the algorithmpresented in Section 3.
For the arc-standard and arc-eager systems, we implemented an oracle preferringreduce actions over shift, which can minimizes the memory cost.Memory costs for general sentences For each language, we count the number of configurations foreach memory cost during performing oracles on all sentences.
In Figure 4, we show the cumulativefrequencies of configurations having each memory cost (see solid lines in the figure).
These lines cananswer the question: What memory cost is required to cover X% of configurations when recovering allgold trees?
Note that comparing absolute values are not meaningful since the minimal cost to constructan arc is different for each system, e.g., the arc-standard system requires at least two items on the stack,21451 2 3 4 5 678 9 105060708090100Arabic1 2 3 4 5 678 9 105060708090100Basque1 2 3 4 5 678 9 105060708090100Bulgarian1 2 3 4 5 678 9 105060708090100Catalan1 2 3 4 5 678 9 105060708090100Chinese1 2 3 4 5 678 9 105060708090100Czech1 2 3 4 5 678 9 105060708090100Danish1 2 3 4 5 678 9 105060708090100Dutch1 2 3 4 5 678 9 105060708090100English1 2 3 4 5 678 9 105060708090100Greek1 2 3 4 5 678 9 105060708090100Hungarian1 2 3 4 5 678 9 105060708090100Italian1 2 3 4 5 678 9 105060708090100Japanese1 2 3 4 5 678 9 105060708090100Portuguese1 2 3 4 5 678 9 105060708090100Slovene1 2 3 4 5 678 9 105060708090100Spanish1 2 3 4 5 678 9 105060708090100Swedish1 2 3 4 5 678 9 105060708090100TurkishArc-standardArc-standard (random)Arc-eagerArc-eager (random)Left-cornerLeft-corner (random)Figure 4: Crosslinguistic comparison of cumulative frequency of memory cost when processing all sen-tences for each system.
For example, in Arabic, for the arc-eager system, around 90% of all config-urations incurred memory cost of ?
5.
Dotted lines (random) are results on the sentences, which arerandomly reordered while preserving the graph structure and projectivity.while the arc-eager system can create a right arc if the stack contains one element.
Instead, we focus onthe universality of each system?s behavior for different languages.As we discussed in section 2.2, the arc-standard system can process only left-branching structures witha constant memory, which are typical in head-final languages such as Japanese or Turkish, and we can2146see this tendency.
The system behaves poorly in many other languages.The arc-eager and left-corner systems behave similarly for many languages, but we can see that thereare some languages for which the left-corner system behaves similarly to other languages, while the arc-eager system requires larger cost; Arabic, Hungarian, or Japanese, for example.
In fact, except Arabic,the left-corner system reaches 98% of configurations with a memory cost of?
3, which indicates that theproperty of the left-corner system requiring less memory is more universal than that of other systems.Comparison to randomized sentences One might wonder that the results above come from the natureof left-corner parsing reducing the stack size, not from the bias in language avoiding center-embeddedstructures.
To partially answer this question, we conduct another experiment comparing oracle transitionson original treebank sentences and on wrong sentences.
We create these wrong sentences by using themethod from Gildea and Temperley (2007).
We reorder words in each sentence by first extracting adirected graph then randomly reordering the children of each node while preserving projectivity.
Thedotted lines in Figure 4 denotes the results of randomized sentences for each system.There are notable differences in required memory between original and random sentences for manylanguages.
This result indicates that our system can parse with less memory for only naturally occurringsentences.
For Chinese and Hungarian, the differences are subtle.
However, the differences are alsosmall for the other systems, which implies that these corpora have some biases on graphs reducing thedifferences.5 Related Work and DiscussionTo the best of our knowledge, parsing with a left-corner strategy has only been studied for constituency.Roark (2001) proposed a top-down parser for a CFG with a left-corner grammar transform (Johnson,1998), which is essentially the same as left-corner parsing but enables several extensions in a unifiedframework.
Roark et al.
(2009) studied the psychological plausibility of Roark?s parser, observing that itfits well to human reading time data.
Another model with a left-corner strategy is Schuler et al.
(2010):they observed that the transformed grammar of English requires only limited memory, proposing a finitestate approximation with a hierarchical hidden Markov model.
This parser was later extended by vanSchijndel and Schuler (2013), which defined a left-corner parser for constituency with shift and reduceactions.
In fact, they used the same kind of actions as our transition system: shift, insert, predict, andcomposition.
Though they did not mentioned explicitly, we showed how to construct a left-corner parsingalgorithm with these actions by decomposing the push-down recognizer of Resnik (1992).
These areexamples of broad-coverage parsing models with cognitively plausibility, which has recently receivedconsiderable attention in interdisciplinary research on psycholinguistics and computational linguistics(Schuler et al., 2010; Keller, 2010; Demberg et al., 2013).Differently from previous models, our target is dependency.
A dependency-based cognitively plausiblemodel is attractive, especially from a crosslinguistical viewpoint.
Keller (2010) argued that currentmodels only work for English, or German in few exceptions, and the importance of crosslinguisticallyvalid models of human language processing.
There has been some attempts to use a transition system forstudying human language processing (Boston and Hale, 2007; Boston et al., 2008), so it is interesting tocompare automatic parsing behaviors with various transition systems to human processing.We introduced a dummy node for representing a subtree with an unknown head or dependent.
Re-cently, Menzel and colleagues (Beuck and Menzel, 2013; Kohn and Menzel, 2014) have also studieddependency parsing with a dummy node.
While conceptually similar, the aim of introducing a dummynode is different between our approach and theirs: We need a dummy node to represent a subtree cor-responding to that in Resnik?s algorithm, while they introduced it to confirm that every dependency treeon a sentence prefix is fully connected.
This difference leads to a technical difference; a subtree of theirparser can contain more than one dummy node, while we restrict each subtree to containing only onedummy node on a right spine.Our experiments in section 4 can be considered as a study on functional biases existing in language orlanguage evolution (Jaeger and Tily, 2011).
In computational linguistics, Gildea and Temperley (2007;2010) examined the bias on general sentences called dependency length minimization (DLM), which2147argues that grammar should favor the dependency structures that reduce the sum of dependency arclengths.
They reordered English and German treebank sentences with various criteria: original, randomwith projectivity, and optimal that minimizes the sum of dependency lengths.
They observed that theword order of English fits very well to the optimal ordering, while German does not.
We examined theuniversality of the bias to reduce memory cost for left-corner parsing.
Although we cannot compare theincurred cost with the optimal reordered sentences, our results on original sentences, in which there arefew configurations requiring the stack depth?
4, suggest the bias to avoid center-embedded structures islanguage universal.
It will be interesting to analyze in more detail the relationships between DLM and thebias of our system since the two biases are not independent, e.g., center-embed structures typically appearwith longer dependencies.
Are there languages that do not hold DLM while requiring less memory, orvice versa?
For these analyses, we might have to take care of the grammar construction, e.g., thereare several definitions for coordination structures for dependency grammars (Popel et al., 2013).
Thefunctional views discussed above might shed some light on the desired construction for these cases.6 ConclusionWe have pointed out that the memory cost on current transition systems for dependency parsing do notcoincide with observations in people, proposing a system with a left-corner strategy.
Our crosslinguisticalanalysis confirms the universality of the claim that people avoid generating center-embedded sentences,which also suggests that it is worthy for crosslinguistical studies of human language processing.As a next stage, we are seeking to train a parser model as in other transition systems with a discrimi-native framework such as a structured perceptron (Zhang and Nivre, 2011; Huang and Sagae, 2010).
Aparser with our transition system might also be attractive for the problem of grammar induction, whererecovering dependency trees are a central problem (Klein and Manning, 2004), and where some linguis-tic biases have been exploited, such as reducibility (Mare?cek and?Zabokrtsk?y, 2012) or acoustic cues(Pate and Goldwater, 2013).
Recently, Cohen et al.
(2011) showed how to interpret shift-reduce actionsas a generative model; combining their idea and our transition system might enable the model to exploitmemory biases that exist in natural sentences.Finally, dependency grammars are suitable for treating non-projective structures.
Extensions for tran-sition systems have been proposed to handle non-projective structures with additional actions (Attardi,2006; Nivre, 2009).
Although our system cannot handle non-projective structures, a similar extensionmight be possible, which would enable a left-corner analysis for non-projective structures.AcknowledgementsWe thank Pontus Stenetorp and anonymous reviewers for their valuable feedbacks on a preliminaryversion of this paper.ReferencesSteven Abney and Mark Johnson.
1991.
Memory requirements and local ambiguities of parsing strategies.
Journalof Psycholinguistic Research, 20(3):233?250.Giuseppe Attardi.
2006.
Experiments with a multilanguage non-projective dependency parser.
In Proceedingsof the Tenth Conference on Computational Natural Language Learning (CoNLL-X), pages 166?170, New YorkCity, June.
Association for Computational Linguistics.Miguel Ballesteros and Joakim Nivre.
2013.
Going to the roots of dependency parsing.
Computational Linguis-tics, 39(1):5?13.Niels Beuck and Wolfgang Menzel.
2013.
Structural prediction in incremental dependency parsing.
In AlexanderGelbukh, editor, Computational Linguistics and Intelligent Text Processing, volume 7816 of Lecture Notes inComputer Science, pages 245?257.
Springer Berlin Heidelberg.Marisa Ferrara Boston and John T. Hale.
2007.
Garden-pathing in a statistical dependency parser.
In Proceedingsof the Midwest Computational Linguistics Colloquium, West Lafayette, IN.
Midwest Computational LinguisticsColloquium.2148Marisa Ferrara Boston, John T. Hale, Umesh Patil, Reinhold Kliegl, and Shravan Vasishth.
2008.
Parsing costs aspredictors of reading difficulty: An evaluation using the Potsdam Sentence Corpus.
Journal of Eye MovementResearch, 2(1):1?12.Sabine Buchholz and Erwin Marsi.
2006.
Conll-x shared task on multilingual dependency parsing.
In Proceedingsof the Tenth Conference on Computational Natural Language Learning (CoNLL-X), pages 149?164, New YorkCity, June.
Association for Computational Linguistics.Shay B. Cohen, Carlos G?omez-Rodr?
?guez, and Giorgio Satta.
2011.
Exact inference for generative probabilisticnon-projective dependency parsing.
In Proceedings of the 2011 Conference on Empirical Methods in Natu-ral Language Processing, pages 1234?1245, Edinburgh, Scotland, UK., July.
Association for ComputationalLinguistics.Nelson Cowan.
2001.
The magical number 4 in short-term memory: A reconsideration of mental storage capacity.Behavioral and Brain Sciences, 24(1):87?114.Vera Demberg, Frank Keller, and Alexander Koller.
2013.
Incremental, predictive parsing with psycholinguisti-cally motivated tree-adjoining grammar.
Computational Linguistics, 39(4):1025?1066.Daniel Gildea and David Temperley.
2007.
Optimizing grammars for minimum dependency length.
In Proceed-ings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 184?191, Prague, CzechRepublic, June.
Association for Computational Linguistics.Daniel Gildea and David Temperley.
2010.
Do grammars minimize dependency length?
Cognitive Science,34(2):286?310.Yoav Goldberg and Joakim Nivre.
2013.
Training deterministic parsers with non-deterministic oracles.
TACL,1:403?414.Katsuhiko Hayashi, Taro Watanabe, Masayuki Asahara, and Yuji Matsumoto.
2012.
Head-driven transition-based parsing with top-down prediction.
In Proceedings of the 50th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers), pages 657?665, Jeju Island, Korea, July.
Association forComputational Linguistics.Matthew Honnibal, Yoav Goldberg, and Mark Johnson.
2013.
A non-monotonic arc-eager transition systemfor dependency parsing.
In Proceedings of the Seventeenth Conference on Computational Natural LanguageLearning, pages 163?172, Sofia, Bulgaria, August.
Association for Computational Linguistics.Liang Huang and Kenji Sagae.
2010.
Dynamic programming for linear-time incremental parsing.
In Proceed-ings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1077?1086, Uppsala,Sweden, July.
Association for Computational Linguistics.T.Florian Jaeger and Harry Tily.
2011.
On language utility: Processing complexity and communicative efficiency.Wiley Interdisciplinary Reviews: Cognitive Science, 2(3):323?335.P.
N. Johnson-Laird.
1983.
Mental models: towards a cognitive science of language, inference, and consciousness.Harvard University Press, Cambridge, MA, USA.Mark Johnson.
1998.
Finite-state approximation of constraint-based grammars using left-corner grammar trans-forms.
In Christian Boitet and Pete Whitelock, editors, COLING-ACL, pages 619?623.
Morgan KaufmannPublishers / ACL.Frank Keller.
2010.
Cognitively plausible models of human language processing.
In Proceedings of the ACL 2010Conference Short Papers, pages 60?67, Uppsala, Sweden, July.
Association for Computational Linguistics.Kotaro Kitagawa and Kumiko Tanaka-Ishii.
2010.
Tree-based deterministic dependency parsing ?
an applicationto nivre?s method ?.
In Proceedings of the ACL 2010 Conference Short Papers, pages 189?193, Uppsala,Sweden, July.
Association for Computational Linguistics.Dan Klein and Christopher Manning.
2004.
Corpus-based induction of syntactic structure: Models of depen-dency and constituency.
In Proceedings of the 42nd Meeting of the Association for Computational Linguistics(ACL?04), Main Volume, pages 478?485, Barcelona, Spain, July.Arne Kohn and Wolfgang Menzel.
2014.
Incremental predictive parsing with turboparser.
In Proceedings of theACL 2014 Conference Short Papers, Baltimore, USA, June.
Association for Computational Linguistics.2149Marco Kuhlmann, Carlos G?omez-Rodr?
?guez, and Giorgio Satta.
2011.
Dynamic programming algorithms fortransition-based dependency parsers.
In Proceedings of the 49th Annual Meeting of the Association for Compu-tational Linguistics: Human Language Technologies, pages 673?682, Portland, Oregon, USA, June.
Associationfor Computational Linguistics.David Mare?cek and Zden?ek?Zabokrtsk?y.
2012.
Exploiting reducibility in unsupervised dependency parsing.
InProceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Compu-tational Natural Language Learning, pages 297?307, Jeju Island, Korea, July.
Association for ComputationalLinguistics.Joakim Nivre, Johan Hall, Sandra K?ubler, Ryan McDonald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.2007.
The CoNLL 2007 shared task on dependency parsing.
In Proceedings of the CoNLL Shared Task Sessionof EMNLP-CoNLL 2007, pages 915?932.Joakim Nivre.
2003.
An efficient algorithm for projective dependency parsing.
In Proceedings of the 8th Interna-tional Workshop on Parsing Technologies (IWPT), pages 149?160.Joakim Nivre.
2004.
Incrementality in deterministic dependency parsing.
In Frank Keller, Stephen Clark,Matthew Crocker, and Mark Steedman, editors, Proceedings of the ACL Workshop Incremental Parsing: Bring-ing Engineering and Cognition Together, pages 50?57, Barcelona, Spain, July.
Association for ComputationalLinguistics.Joakim Nivre.
2008.
Algorithms for deterministic incremental dependency parsing.
Computational Linguistics,34(4):513?553.Joakim Nivre.
2009.
Non-projective dependency parsing in expected linear time.
In Proceedings of the JointConference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on NaturalLanguage Processing of the AFNLP, pages 351?359, Suntec, Singapore, August.
Association for ComputationalLinguistics.John K. Pate and Sharon Goldwater.
2013.
Unsupervised dependency parsing with acoustic cues.
TACL, 1:63?74.Martin Popel, David Mare?cek, Jan?St?ep?anek, Daniel Zeman, and Zden?ek?Zabokrtsk?y.
2013.
Coordination struc-tures in dependency treebanks.
In ACL, pages 517?527.Philip Resnik.
1992.
Left-corner parsing and psychological plausibility.
In COLING, pages 191?197.Brian Roark, Asaf Bachrach, Carlos Cardenas, and Christophe Pallier.
2009.
Deriving lexical and syntacticexpectation-based measures for psycholinguistic modeling via incremental top-down parsing.
In Proceedingsof the 2009 Conference on Empirical Methods in Natural Language Processing, pages 324?333, Singapore,August.
Association for Computational Linguistics.Brian Edward Roark.
2001.
Robust Probabilistic Predictive Syntactic Processing: Motivations, Models, andApplications.
Ph.D. thesis, Providence, RI, USA.
AAI3006783.Francesco Sartorio, Giorgio Satta, and Joakim Nivre.
2013.
A transition-based dependency parser using a dynamicparsing strategy.
In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics(Volume 1: Long Papers), pages 135?144, Sofia, Bulgaria, August.
Association for Computational Linguistics.William Schuler, Samir AbdelRahman, Tim Miller, and Lane Schwartz.
2010.
Broad-coverage parsing usinghuman-like memory constraints.
Computational Linguistics, 36(1):1?30.Marten van Schijndel and William Schuler.
2013.
An analysis of frequency- and memory-based processing costs.In Proceedings of the 2013 Conference of the North American Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages 95?105, Atlanta, Georgia, June.
Association for Computa-tional Linguistics.Yue Zhang and Joakim Nivre.
2011.
Transition-based dependency parsing with rich non-local features.
In Pro-ceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Tech-nologies, pages 188?193, Portland, Oregon, USA, June.
Association for Computational Linguistics.2150
