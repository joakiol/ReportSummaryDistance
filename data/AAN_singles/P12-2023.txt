Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 115?119,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsTopic Models for Dynamic Translation Model AdaptationVladimir EidelmanComputer Scienceand UMIACSUniversity of MarylandCollege Park, MDvlad@umiacs.umd.eduJordan Boyd-GraberiSchooland UMIACSUniversity of MarylandCollege Park, MDjbg@umiacs.umd.eduPhilip ResnikLinguisticsand UMIACSUniversity of MarylandCollege Park, MDresnik@umd.eduAbstractWe propose an approach that biases machinetranslation systems toward relevant transla-tions based on topic-specific contexts, wheretopics are induced in an unsupervised wayusing topic models; this can be thought ofas inducing subcorpora for adaptation with-out any human annotation.
We use these topicdistributions to compute topic-dependent lex-ical weighting probabilities and directly in-corporate them into our translation model asfeatures.
Conditioning lexical probabilitieson the topic biases translations toward topic-relevant output, resulting in significant im-provements of up to 1 BLEU and 3 TER onChinese to English translation over a strongbaseline.1 IntroductionThe performance of a statistical machine translation(SMT) system on a translation task depends largelyon the suitability of the available parallel trainingdata.
Domains (e.g., newswire vs. blogs) may varywidely in their lexical choices and stylistic prefer-ences, and what may be preferable in a general set-ting, or in one domain, is not necessarily preferablein another domain.
Indeed, sometimes the domaincan change the meaning of a phrase entirely.In a food related context, the Chinese sentence?????
?
(?fe?nsi?
he?nduo??)
would mean ?Theyhave a lot of vermicelli?
; however, in an informal In-ternet conversation, this sentence would mean ?Theyhave a lot of fans?.
Without the broader context, itis impossible to determine the correct translation inotherwise identical sentences.This problem has led to a substantial amount ofrecent work in trying to bias, or adapt, the transla-tion model (TM) toward particular domains of inter-est (Axelrod et al, 2011; Foster et al, 2010; Snoveret al, 2008).1 The intuition behind TM adapta-tion is to increase the likelihood of selecting rele-vant phrases for translation.
Matsoukas et al (2009)introduced assigning a pair of binary features toeach training sentence, indicating sentences?
genreand collection as a way to capture domains.
Theythen learn a mapping from these features to sen-tence weights, use the sentence weights to bias themodel probability estimates and subsequently learnthe model weights.
As sentence weights were foundto be most beneficial for lexical weighting, Chianget al (2011) extends the same notion of condition-ing on provenance (i.e., the origin of the text) by re-moving the separate mapping step, directly optimiz-ing the weight of the genre and collection featuresby computing a separate word translation table foreach feature, estimated from only those sentencesthat comprise that genre or collection.The common thread throughout prior work is theconcept of a domain.
A domain is typically a hardconstraint that is externally imposed and hand la-beled, such as genre or corpus collection.
For ex-ample, a sentence either comes from newswire, orweblog, but not both.
However, this poses sev-eral problems.
First, since a sentence contributes itscounts only to the translation table for the source itcame from, many word pairs will be unobserved fora given table.
This sparsity requires smoothing.
Sec-ond, we may not know the (sub)corpora our training1Language model adaptation is also prevalent but is not thefocus of this work.115data come from; and even if we do, ?subcorpus?
maynot be the most useful notion of domain for bettertranslations.We take a finer-grained, flexible, unsupervised ap-proach for lexical weighting by domain.
We induceunsupervised domains from large corpora, and weincorporate soft, probabilistic domain membershipinto a translation model.
Unsupervised modeling ofthe training data produces naturally occurring sub-corpora, generalizing beyond corpus and genre.
De-pending on the model used to select subcorpora, wecan bias our translation toward any arbitrary distinc-tion.
This reduces the problem to identifying whatautomatically defined subsets of the training corpusmay be beneficial for translation.In this work, we consider the underlying latenttopics of the documents (Blei et al, 2003).
Topicmodeling has received some use in SMT, for in-stance Bilingual LSA adaptation (Tam et al, 2007),and the BiTAM model (Zhao and Xing, 2006),which uses a bilingual topic model for learningalignment.
In our case, by building a topic distri-bution for the source side of the training data, weabstract the notion of domain to include automati-cally derived subcorpora with probabilistic member-ship.
This topic model infers the topic distributionof a test set and biases sentence translations to ap-propriate topics.
We accomplish this by introduc-ing topic dependent lexical probabilities directly asfeatures in the translation model, and interpolatingthem log-linearly with our other features, thus allow-ing us to discriminatively optimize their weights onan arbitrary objective function.
Incorporating thesefeatures into our hierarchical phrase-based transla-tion system significantly improved translation per-formance, by up to 1 BLEU and 3 TER over a strongChinese to English baseline.2 Model DescriptionLexical Weighting Lexical weighting features es-timate the quality of a phrase pair by combiningthe lexical translation probabilities of the words inthe phrase2 (Koehn et al, 2003).
Lexical condi-tional probabilities p(e|f) are obtained with maxi-mum likelihood estimates from relative frequencies2For hierarchical systems, these correspond to translationrules.c(f, e)/?e c(f, e) .
Phrase pair probabilities p(e|f)are computed from these as described in Koehn etal.
(2003).Chiang et al (2011) showed that is it benefi-cial to condition the lexical weighting features onprovenance by assigning each sentence pair a setof features, fs(e|f), one for each domain s, whichcompute a new word translation table ps(e|f) esti-mated from only those sentences which belong to s:cs(f, e)/?e cs(f, e) , where cs(?)
is the number ofoccurrences of the word pair in s.Topic Modeling for MT We extend provenanceto cover a set of automatically generated topics zn.Given a parallel training corpus T composed of doc-uments di, we build a source side topic model overT , which provides a topic distribution p(zn|di) forzn = {1, .
.
.
,K} over each document, using LatentDirichlet Allocation (LDA) (Blei et al, 2003).
Then,we assign p(zn|di) to be the topic distribution forevery sentence xj ?
di, thus enforcing topic sharingacross sentence pairs in the same document insteadof treating them as unrelated.
Computing the topicdistribution over a document and assigning it to thesentences serves to tie the sentences together in thedocument context.To obtain the lexical probability conditioned ontopic distribution, we first compute the expectedcount ezn(e, f) of a word pair under topic zn:ezn(e, f) =?di?Tp(zn|di)?xj?dicj(e, f) (1)where cj(?)
denotes the number of occurrences ofthe word pair in sentence xj , and then compute:pzn(e|f) =ezn(e, f)?e ezn(e, f)(2)Thus, we will introduce 2?K new word trans-lation tables, one for each pzn(e|f) and pzn(f |e),and as many new corresponding features fzn(e|f),fzn(f |e).
The actual feature values we compute willdepend on the topic distribution of the document weare translating.
For a test document V , we infertopic assignments on V , p(zn|V ), keeping the topicsfound from T fixed.
The feature value then becomesfzn(e|f) = ?
log{pzn(e|f) ?
p(zn|V )}, a combi-nation of the topic dependent lexical weight and the116topic distribution of the sentence from which we areextracting the phrase.
To optimize the weights ofthese features we combine them in our linear modelwith the other features when computing the modelscore for each phrase pair3:?p?php(e, f)?
??
?unadapted features+?zn?znfzn(e|f)?
??
?adapted features(3)Combining the topic conditioned word translationtable pzn(e|f) computed from the training corpuswith the topic distribution p(zn|V ) of the test sen-tence being translated provides a probability on howrelevant that translation table is to the sentence.
Thisallows us to bias the translation toward the topic ofthe sentence.
For example, if topic k is dominant inT , pk(e|f) may be quite large, but if p(k|V ) is verysmall, then we should steer away from this phrasepair and select a competing phrase pair which mayhave a lower probability in T , but which is more rel-evant to the test sentence at hand.In many cases, document delineations may not bereadily available for the training corpus.
Further-more, a document may be too broad, covering toomany disparate topics, to effectively bias the weightson a phrase level.
For this case, we also propose alocal LDA model (LTM), which treats each sentenceas a separate document.While Chiang et al (2011) has to explicitlysmooth the resulting ps(e|f), since many word pairswill be unseen for a given domain s, we are alreadyperforming an implicit form of smoothing (whencomputing the expected counts), since each docu-ment has a distribution over all topics, and thereforewe have some probability of observing each wordpair in every topic.Feature Representation After obtaining the topicconditional features, there are two ways to presentthem to the model.
They could answer the questionF1: What is the probability under topic 1, topic 2,etc., or F2: What is the probability under the mostprobable topic, second most, etc.A model using F1 learns whether a specific topicis useful for translation, i.e., feature f1 would bef1 := pz=1(e|f) ?
p(z = 1|V ).
With F2, we3The unadapted lexical weight p(e|f) is included in themodel features.are learning how useful knowledge of the topic dis-tribution is, i.e., f1 := p(argmaxzn (p(zn|V ))(e|f) ?p(argmaxzn(p(zn|V ))|V ).Using F1, if we restrict our topics to have a one-to-one mapping with genre/collection4 we see thatour method fully recovers Chiang (2011).F1 is appropriate for cross-domain adaptationwhen we have advance knowledge that the distribu-tion of the tuning data will match the test data, as inChiang (2011), where they tune and test on web.
Ingeneral, we may not know what our data will be, sothis will overfit the tuning set.F2, however, is intuitively what we want, sincewe do not want to bias our system toward a spe-cific distribution, but rather learn to utilize informa-tion from any topic distribution if it helps us cre-ate topic relevant translations.
F2 is useful for dy-namic adaptation, where the adapted feature weightchanges based on the source sentence.Thus, F2 is the approach we use in our work,which allows us to tune our system weights towardhaving topic information be useful, not toward a spe-cific distribution.3 ExperimentsSetup To evaluate our approach, we performed ex-periments on Chinese to English MT in two set-tings.
First, we use the FBIS corpus as our trainingbitext.
Since FBIS has document delineations, wecompare local topic modeling (LTM) with model-ing at the document level (GTM).
The second settinguses the non-UN and non-HK Hansards portions ofthe NIST training corpora with LTM only.
Table 1summarizes the data statistics.
For both settings,the data were lowercased, tokenized and aligned us-ing GIZA++ (Och and Ney, 2003) to obtain bidi-rectional alignments, which were symmetrized us-ing the grow-diag-final-and method (Koehnet al, 2003).
The Chinese data were segmented us-ing the Stanford segmenter.
We trained a trigramLM on the English side of the corpus with an addi-tional 150M words randomly selected from the non-NYT and non-LAT portions of the Gigaword v4 cor-pus using modified Kneser-Ney smoothing (Chenand Goodman, 1996).
We used cdec (Dyer et al,4By having as many topics as genres/collections and settingp(zn|di) to 1 for every sentence in the collection and 0 to ev-erything else.117Corpus Sentences TokensEn ZhFBIS 269K 10.3M 7.9MNIST 1.6M 44.4M 40.4MTable 1: Corpus statistics2010) as our decoder, and tuned the parameters ofthe system to optimize BLEU (Papineni et al, 2002)on the NIST MT06 tuning corpus using the Mar-gin Infused Relaxed Algorithm (MIRA) (Crammeret al, 2006; Eidelman, 2012).
Topic modeling wasperformed with Mallet (Mccallum, 2002), a stan-dard implementation of LDA, using a Chinese sto-plist and setting the per-document Dirichlet parame-ter ?
= 0.01.
This setting of was chosen to encour-age sparse topic assignments, which make inducedsubdomains consistent within a document.Results Results for both settings are shown in Ta-ble 2.
GTM models the latent topics at the documentlevel, while LTM models each sentence as a separatedocument.
To evaluate the effect topic granularitywould have on translation, we varied the number oflatent topics in each model to be 5, 10, and 20.
OnFBIS, we can see that both models achieve moderatebut consistent gains over the baseline on both BLEUand TER.
The best model, LTM-10, achieves a gainof about 0.5 and 0.6 BLEU and 2 TER.
Although theperformance on BLEU for both the 20 topic modelsLTM-20 and GTM-20 is suboptimal, the TER im-provement is better.
Interestingly, the difference intranslation quality between capturing document co-herence in GTM and modeling purely on the sen-tence level is not substantial.5 In fact, the oppositeis true, with the LTM models achieving better per-formance.6On the NIST corpus, LTM-10 again achieves thebest gain of approximately 1 BLEU and up to 3 TER.LTM performs on par with or better than GTM, andprovides significant gains even in the NIST data set-ting, showing that this method can be effectively ap-plied directly on the sentence level to large training5An avenue of future work would condition the sentencetopic distribution on a document distribution over topics (Tehet al, 2006).6As an empirical validation of our earlier intuition regardingfeature representation, presenting the features in the form of F1caused the performance to remain virtually unchanged from thebaseline model.Model MT03 MT05?BLEU ?TER ?BLEU ?TERBL 28.72 65.96 27.71 67.58GTM-5 28.95ns 65.45 27.98ns 67.38nsGTM-10 29.22 64.47 28.19 66.15GTM-20 29.19 63.41 28.00ns 64.89LTM-5 29.23 64.57 28.19 66.30LTM-10 29.29 63.98 28.18 65.56LTM-20 29.09ns 63.57 27.90ns 65.17Model MT03 MT05?BLEU ?TER ?BLEU ?TERBL 34.31 61.14 30.63 65.10MERT 34.60 60.66 30.53 64.56LTM-5 35.21 59.48 31.47 62.34LTM-10 35.32 59.16 31.56 62.01LTM-20 33.90ns 60.89ns 30.12ns 63.87Table 2: Performance using FBIS training corpus (top)and NIST corpus (bottom).
Improvements are significantat the p <0.05 level, except where indicated (ns).corpora which have no document markings.
De-pending on the diversity of training corpus, a vary-ing number of underlying topics may be appropriate.However, in both settings, 10 topics performed best.4 Discussion and ConclusionApplying SMT to new domains requires techniquesto inform our algorithms how best to adapt.
This pa-per extended the usual notion of domains to finer-grained topic distributions induced in an unsuper-vised fashion.
We show that incorporating lexi-cal weighting features conditioned on soft domainmembership directly into our model is an effectivestrategy for dynamically biasing SMT towards rele-vant translations, as evidenced by significant perfor-mance gains.
This method presents several advan-tages over existing approaches.
We can constructa topic model once on the training data, and useit infer topics on any test set to adapt the transla-tion model.
We can also incorporate large quanti-ties of additional data (whether parallel or not) inthe source language to infer better topics without re-lying on collection or genre annotations.
Multilin-gual topic models (Boyd-Graber and Resnik, 2010)would provide a technique to use data from multiplelanguages to ensure consistent topics.118AcknowledgmentsVladimir Eidelman is supported by a National De-fense Science and Engineering Graduate Fellow-ship.
This work was also supported in part byNSF grant #1018625, ARL Cooperative Agree-ment W911NF-09-2-0072, and by the BOLT andGALE programs of the Defense Advanced ResearchProjects Agency, Contracts HR0011-12-C-0015 andHR0011-06-2-001, respectively.
Any opinions, find-ings, conclusions, or recommendations expressedare the authors?
and do not necessarily reflect thoseof the sponsors.ReferencesAmittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011.Domain adaptation via pseudo in-domain data selec-tion.
In Proceedings of Emperical Methods in NaturalLanguage Processing.David M. Blei, Andrew Y. Ng, Michael I. Jordan, andJohn Lafferty.
2003.
Latent Dirichlet Allocation.Journal of Machine Learning Research, 3:2003.Jordan Boyd-Graber and Philip Resnik.
2010.
Holisticsentiment analysis across languages: Multilingual su-pervised latent Dirichlet alocation.
In Proceedings ofEmperical Methods in Natural Language Processing.Stanley F. Chen and Joshua Goodman.
1996.
An empir-ical study of smoothing techniques for language mod-eling.
In Proceedings of the 34th Annual Meeting ofthe Association for Computational Linguistics, pages310?318.David Chiang, Steve DeNeefe, and Michael Pust.
2011.Two easy improvements to lexical weighting.
In Pro-ceedings of the Human Language Technology Confer-ence.Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer.
2006.
Online passive-aggressive algorithms.
Journal of Machine LearningResearch, 7:551?585.Chris Dyer, Adam Lopez, Juri Ganitkevitch, JonathanWeese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,Vladimir Eidelman, and Philip Resnik.
2010. cdec: Adecoder, alignment, and learning framework for finite-state and context-free translation models.
In Proceed-ings of ACL System Demonstrations.Vladimir Eidelman.
2012.
Optimization strategies foronline large-margin learning in machine translation.In Proceedings of the Seventh Workshop on StatisticalMachine Translation.George Foster, Cyril Goutte, and Roland Kuhn.
2010.Discriminative instance weighting for domain adapta-tion in statistical machine translation.
In Proceedingsof Emperical Methods in Natural Language Process-ing.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of the 2003 Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics on Human Language Technology - Volume 1,NAACL ?03, Stroudsburg, PA, USA.Spyros Matsoukas, Antti-Veikko I. Rosti, and BingZhang.
2009.
Discriminative corpus weight estima-tion for machine translation.
In Proceedings of Em-perical Methods in Natural Language Processing.A.
K. Mccallum.
2002.
MALLET: A Machine Learningfor Language Toolkit.Franz Och and Hermann Ney.
2003.
A systematic com-parison of various statistical alignment models.
InComputational Linguistics, volume 29(21), pages 19?51.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic evalu-ation of machine translation.
In Proceedings of the As-sociation for Computational Linguistics, pages 311?318.Matthew Snover, Bonnie Dorr, and Richard Schwartz.2008.
Language and translation model adaptation us-ing comparable corpora.
In Proceedings of EmpericalMethods in Natural Language Processing.Yik-Cheung Tam, Ian Lane, and Tanja Schultz.
2007.Bilingual LSA-based adaptation for statistical machinetranslation.
Machine Translation, 21(4):187?207.Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, andDavid M. Blei.
2006.
Hierarchical Dirichlet pro-cesses.
Journal of the American Statistical Associa-tion, 101(476):1566?1581.Bing Zhao and Eric P. Xing.
2006.
BiTAM: Bilingualtopic admixture models for word alignment.
In Pro-ceedings of the Association for Computational Lin-guistics.119
