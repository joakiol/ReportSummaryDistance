A LAYERED APPROACH TO NLP-BASED INFORMATIONRETRIEVALSharon F lankSRA In ternat iona l4300 Fair  Lakes Cour tFair fax,  VA 22033, USAf lanks~sra .comAbst ractA layered approach to information retrievalpermits the inclusion of multiple search en-gines as well as multiple databases, with anatural language layer to convert Englishqueries for use by the various search en-gines.
The NLP layer incorporates mor-phological analysis, noun phrase syntax,and semantic expansion based on Word-Net.1 In t roduct ionThis paper describes a layered approach to infor-mation retrieval, and the natural language compo-nent that is a major element in that approach.
Thelayered approach, packaged as Intermezzo TM, wasdeployed in a pre-product form at a governmentsite.
The NLP component has been installed, witha proprietary IR engine, PhotoFile, (Flank, Martin,Balogh and Rothey, 1995), (Flank, Garfield, andNorkin, 1995), at several commercial sites, includ-ing Picture Network International (PNI), Simon andSchuster, and John Deere.Intermezzo employs an abstraction layer to per-mit simultaneous querying of multiple databases.
Auser enters a query into a client, and the query isthen passed to the server.
The abstraction layer,part of the server, converts the query to the ap-propriate format for each of the databases (e.g.Fulcrum TM, RetrievalWare TM, Topic TM, WAIS).In Boolean mode, queries are translated, using anSGML-based intermediate query language, into theappropriate form; in NLP mode the queries un-dergo morphological analysis, NP syntax, and se-mantic expansion before being converted for use bythe databases.The following example illustrates how a user'squery is translated.Unexpanded query  natural disasters in NewEnglandSearch-engine specific natural AND disaster(s)AND New AND EnglandSemant ic  expans ion  ((natural and disaster(s)) orhurricane(s) or earthquake(s) or tornado(es) in("New England" or Maine or Vermont or "NewHampshire" or "Rhode Island" or Connecticutor Massachusetts)The NLP component has been deployed with asmany as 500,000 images, at Picture Network In-ternational (PNI).
The original commercial use ofPNI was as a dialup system, launched with ap-proximately 100,000 images.
PNI now operates onthe World Wide Web (www.publishersdepot.com).Adjustment of the NLP component continued ac-tively up through about 250,000 images, includingadditions to the semantic net and tuning of theparameters for weighting.
Retrieval speed for theNLP component averages under a second.
Semanticexpansion is performed in advance on the captiondatabase, not at runtime; runtime expansion makesoperation too slow.The remainder of this paper describes how theNLP mode works, and what was required to createit.2 The  NLP  Techn iquesThe natural language processing techniques usedin this system are well known, including in infor-mation retrieval applications (Strzalkowski, 1993),(Strzalkowski, Perez Carballo and Marinescu, 1995),(Evans and Zhai, 1996).
The importance of thiswork lies in the scale and robustness of the tech-niques as combined into a system for querying largedatabases.The NLP component is also layered, in effect.
Ituses a conventional search algorithm (several weretested, and the architecture supports plug-and-playhere).
User queries undergo several types of NLPprocessing, detailed below, and each element in theprocessing contributes new query components (e.g.synonyms) and/or weights.
The resulting query, asin the example above, natural disasters in New Eng-land, contains expanded terms and weighting infor-mation that can be passed to any search engine.Thus the Intermezzo multisearch layer can be seen397as a natural extension of the layered design of theNLP search system.When texts (or captioned images) are loaded intothe database, each word is looked up, words thatmay be related in the semantic net are found basedon stored links, and the looked-up word, along withany related words, are all displayed as the "expan-sion" of that word.
Then a check is made to de-termine whether the current word or phrase corre-sponds to a proper name, a location, or somethingelse.
If it corresponds to a name, a name expansionprocess is invoked that displays the name and relatednames such as nicknames and other variants, basedon a linked name file.
If the current word or phrasecorresponds to a location, a location expansion pro-cess is invoked that, accessing a gazetteer, displaysthe location and related locations, such as Arlington,Virginia and Arlington, Massachusetts for Arlington,based on linked location information in the gazetteerand supporting files.
If the current word or phrase isneither a name nor a location, it is expanded usingthe semantic net links and weights associated withthose links.
Strongly related concepts are given highweights, while more remotely related concepts re-ceive lower weights, making them less exact matches.Thus, for a query on car, texts or captions contain-ing car and automobile are listed highest, followed bythose with sedan, coupe, and convertible, and thenby more remotely related concepts uch as transmis-sion, hood, and trunk.Once the appropriate xpansion is complete, thecurrent word or phrase is stored in an indexdatabase, available for use in searching as describedbelow.
Processing then returns to the next word orphrase in the text.Once a user query is received, it is tokenized sothat it is divided into individual tokens, which maybe single words or multiwords.
For this process, avariation of conventional pattern matching is used.If a single word is recognized as matching a wordthat is part of a stored multiword, a decision onwhether to treat the single word as part of a multi-word is made based on the contents of the stored pat-tern and the input pattern.
Stored patterns includenot just literal words, but also syntactic categories(e.g.
adjective, non-verb), semantic ategories (e.g.nationality, government entity), or exact matches.If the input matches the stored pattern information,then it is interpreted as a multiword rather than in-dependent words.A part-of-speech tagger then makes use of linguis-tic and statistical information to tag the parts ofspeech of incoming query portions.
Only words thatmatch by part of speech are considered to match,and if two or more parts of speech are possible for aparticular word, it is tagged with both.
After tag-ging, word affixes (i.e.
suffixes) are stripped fromquery words to obtain a word root, using conven-tional inflectional morphology.
If a word in a queryis not known, affixes are stripped fi'om the word oneby one until a known word is found.
Derivationalmorphology is not currently implemented.Processing then checks to determine whether theresulting word is a function word (closed-class) orcontent word (open-class).
Function words are ig-nored.
1 For content words, the related conceptsfor each sense of the word are retrieved from the se-mantic net.
If the root word is unknown, the wordis treated as a keyword, requiring an exact match.Multiwords are matched as a whole unit, and namesand locations are identified and looked up in the sep-arate name and location files.
Next, noun phrasesand other syntactic units are identified.An intermediate query is then formulated tomatch against he index database.
Texts or captionsthat match queries are then returned, ranked, anddisplayed to the user, with those that match bestbeing displayed at the top of the list.
In the currentsystem, the searching is implemented by first build-ing a B-tree of ID lists, one for each concept in thetext database.
The ID lists have an entry for eachobject whose text contains a reference to a given con-cept.
An entry consists of an object ID and a weight.The object ID provides a unique identifier and is apositive integer assigned when the object is indexed.The weight reflects the relevance of the concept tothe object's text, and is a positive integer.To add an object to an existing index, the objectID and a weight are inserted into the ID list of everyconcept hat is in any way relevant o the text.
Forsearching, the ID lists of every concept in the queryare retrieved and combined as specified by the query.Since ID lists contain IDs with weights in sorted or-der, determining existence and relevance of a matchis simultaneous and fast, using only a small numberof processor instructions per concept-object pair.The following sections treat the NLP issues inmore detail.2.1 Semant ic  Expans ion ,  Par t -o f -SpeechTagging,  and  WordNetSemantic expansion, based on WordNet 1.4 (Milleret al, 1994), makes it possible to retrieve words bysynonyms, hypernyms, and other relations, not sim-ply by exact matches.
The expansion must be con-strained, or precision will suffer drastically.
The firstconstraint is part of speech: retrieve only those ex-pansions that apply to the correct part of speechin context.
A Church-style tagger (Church, 1988)tin a few cases, the loss oI prepositions presents aproblem.
In practice, the problem is largely restricted topictures howing unexpected relationships, e.g.
a pack-age under a table.
Treating prepositions just like contentworks leads to odd partial matches (things under tablesbefore other pictures of packages and tables, for exam-ple).
The solution will involve an intermediate reatmentof prepositions.398marks parts of speech.
Sense tagging is a further re-finement: the algorithm first distinguishes between,e.g.
crane as a noun versus crane as a verb.
Oncenoun has been selected, further ambiguity still re-mains, since a crane can be either a bird or a pieceof construction equipment.
This additional disam-biguation can be ignored, or it can be performedmanually (impractical for large volumes of text andimpractical for queries, at least for most users).
Itcan also be performed automatically, based on asense-tagged corpus.The semantic net used in this application incor-porates information from a variety of sources be-sides WordNet; to some extent it was hand-tailored.Senses were ordered according to thdir frequency ofoccurrence in the first 150,000 texts used for re-trieval, in this case photo captions consisting of oneto three sentences each.
WordNet 1.5 and subse-quent releases have the senses ordered by frequency,so this step would not be necessary now.The top level of the semantic net splits into eventsand entities, as is standard for knowledge bases sup-porting natural anguage applications.
There are ap-proximately 100,000 entries, with several links foreach entry.
The semantic net supplies informationabout synonymy and hierarchical relations, as wellas more sophisticated links, like part-of.
The closestsynonyms, like dangerous and perilous, are rankedmost highly, while subordinate types, like skatingand rollerblading, are next.
More distant links, likethe relation between shake hands and handshake,links between adjectives and nouns, e.g.
danger-ous and danger, and part-of links, e.g.
brake andbrake shoe, contribute lesser amounts to the rankand therefore yield a lower overall ranking.
Eachreturned image has an associated weight, with 100being a perfect match.
Exact matches (disregard-ing inflectional morphology) rank 100.
The systemmay be configured so that it does not return matchesranked below a certain threshold, say 50.Table 1 presents the weights currently in use forthe various relations in WordNet.
The depth figureindicates how many levels a particular relation isfollowed.
Some relations, like hypernyms and per-tainyms, are clearly relevant for retrieval, while oth-ers, such as antonyms, are irrelevant.
If the depth iszero, as with antonyms, the relation is not followedat all: it is not useful to include antonyms in thesemantic expansion of a term.
If the depth is non-zero, as with hypernyms, its relative weight is givenin the weight figure.
Hypernyms make sense for re-trieval (animals retrieves hippos) but hyponyms donot (hippos hould not retrieve animals).
The weightindicates the degree to which each succeeding level isdiscounted.
Thus a ladybug is rated 90% on a queryfor beetle, but only 81% (90% x 90%) on a query forinsect, 73% (90% x 81%) on a query for arthropod,66% (90% x 73%) on a query for invertebrate, 59%(90% x 66%) on a query for animal, and not at, allTable 1: Expansion depth for WordNet relationsRelation Part of Speech Depth WeightANTONYM noun 0ANTONYM verb 0ANTONYM adj 0ANTONYM adv 0HYPERNYM noun 4 90HYPERNYM verb 4 90HYPONYM noun 0HYPONYM verb 0MEM MERONYM noun 3 90SUB MERONYM noun 0PART MERONYM noun 3 90MEM HOLONYM noun 0SUB HOLONYM noun 0PART HOLONYM noun 0ENTAILMENT verb 2 90CAUSE verb 2 90ALSO SEE verb 1 90ALSO SEE adj 1 90ALSO SEE adv 1 90ALSO SEE noun 1 90SIMILAR TO adj 2 90PERTAINYM adj 2 95PERTAINYM noun 2 95ATTRIBUTE noun 0ATTRIBUTE adj 1 80(more than four levels) on a query for organism.
Aquery for organisms returns images that match therequest more closely, for example:?
An amorphous amoeba speckled with greenish-yellow blobs.It might appear that ladybugs should be re-trieved in queries for organism, but in fact suchhigh-level queries generate thousands of hits evenwith only four-level expansion.
In practical terms,then.
the number of levels must be limited.
Excal-ibur's WordNet-based retrieval product., Retrieval-Ware, does not limit expansion levels, instead al-lowing the expert user to eliminate particular sensesof words at query time, in recognition of the needto limit term expansion in one aspect of the sys-tem if not in another.
The depth and weight fig-ures were tuned by trial and error on a corpus ofseveral hundred thousand paragraph-length picturecaptions.
For longer texts, the depth, particularlyfor hypernyms, hould be less.The weights file does not affect which images areselected as relevant, but it does affect their relevanceranking, and thus the ordering that the user sees.
Inpractical terms this means that for a query on ani-mal, exact matches on animal appear first, and hip-pos appear before ladybugs.
Of course, if the thresh-old is set at 50 and the weights alter a ranking from39951 to 49, the user will no longer see that image inthe list at all.
Technically, however, the image hasnot been removed from the relevance list, but rathersimply downgraded.WordNet was designed as a multifunction aturallanguage resource, not as an IR expansion et.
In-evitably, certain changes were required to tailor itfor NLP-based IR.
First, there were a few links highin the hierarchy that caused bizarre behavior, likeanimals being retrieved for queries including man ormen.
Other problems were some "unusual" correla-tions, such as:** grimace linked to smileo juicy linked to sexySecond, certain slang entries were inappropriatefor a commercial system and had to be removed inorder to avoid giving offense.
Single sense words (e.g.crap) were not particularly problematic, since userswho employed them in a query presumably did soon purpose.
Polysemous terms such as nuts, skirt,and frog, however, were eliminated, since they couldinadvertently cause offense.Third, there were low-level edits of single words.Before the senses were reordered by frequency, somesenses were disabled in response to user feedback.These senses caused retrieval behavior that usersfound inexplicable.
For example, the battle sense ofengagement, the fervor sense of fire, and the Indianlanguage sense of Massachusetts, all were removed,because they retrieved images that users could notlink to the query.
Although users were forgivingwhen they could understand why a bad match hadoccurred, they were far less patient with what theyviewed as random behavior.
In this case, the rarityof the senses made it difficult for users to trace thelogic at work in the sense expansion.Finally, since language evolves so quickly, newterms had to be added, e.g.
rollerblade.
This taskwas the most common and the one requiring the leastexpertise.
Neologisms and missing terms numberedin the dozens for 500,000 sentences, a testament toWordNet's coverage.2.2 Gazet teer  In tegrat ionLocations are processed using a gazetteer and sev-eral related files.
The gazetteer (supplied by the U.S.Government for the Message Understanding Confer-ences \[MUC\]), is extremely large and comprehen-sive.
In some ways, it is almost too large to be use-ful.
Algorithms had to be added, for example, toselect which of many choices made the most sense.Moscow is a town in Idaho, but the more relevantcity is certainly the one in Russia.
The gazetteercontains information on administrative units as wellas rough data on city size, which we used to developa sense-preference algorithm.
The largest adminis-trative unit (country, then province, then city) isalways given a higher weight, so that New York isfirst interpreted as a state and then as a city.
Withinthe city size rankings, the larger cities are weightedhigher.
Of course explicit designations are under-stood more precisely, i.e.
New York State and NewYork City are unambiguous references only to thestate and only to the city, respectively.
And Moscow,Idaho clearly does not refer to any Moscow outside ofIdaho.
Furthermore, since this was a U.S. product,U.S.
states were weighted higher than other loca-tions, e.g.
Georgia was first understood as a state,then as a country.At the most basic level, the gazetteer is a hierar-chy.
It permits subunits to be retrieved, e.g.
LosAngeles and San Francisco for a query California.An alias table converted the various state abbrevia-tions and other variant forms, e.g.Washington D.C.; Washington, DC; Washington,District of Columbia; Washington DC; Washington,D.C.
; DC; and D.C.Some superunits were added, e.g.
Eastern Europe,New England, and equivalences based on changingpolitical situations, e.g.
Moldavia, Moldova.
To han-dle queries like northern Montana, initial steps weretaken to include latitude and longitude information.The algorithm, never implemented, was to take tilenorthernmost 50% of the unit.
So if Montana coversX to Y north latitude, northern Montana would bebetween (X+Y)/2 and Y.Additional locations are matched oil the fly bypatterns and then treated as units for purposes ofretrieval.
For example, Glacier National Park orMount Hood should be treated as phrases.
To ac-complish this, a pattern matcher, based oil finitestate automata, operates on simple patterns suchas :(LOCATION - -(& (* {word "\[a-Z\]\[a-z\]*"}) {word "\[Nn\]ational"}{OR {word "\[Pp\]ark"} {word "\[Ff\]orest"}})2.3 Syntact ic and Other PatternsThe pattern matcher also performs noun phrase(NP) identification, using the following patterns forcore NPs:(& {tag deter} \[MODIFIER (& (?
(& {tagadj} {tag conj})) (* - -  {tag noun} {tag adj}{tag number} {tag listmark}))\] \[HEAD_NOUN {tagnoun}\])Identification of core NPs (i.e.
modifier-head groupings, without any trailing prepositionalphrases or other modifiers) makes it possible to dis-tinguish stock cars from car stocks, and, for a queryon little girl in a red shirt, to retrieve girls in redshirts in preference to a girl in a blue shirt and redhat.Examples of images returned for the little girl ina red shirt query, rated at 92%, include:?
Two smiling young girls wearing matching jeanoveralls, red shirts.
The older girl wearing a400blue baseball cap sideways has blond pigtailswith yellow ribbons.
The younger girl wears ayellow baseball cap sideways.?
An African American little girl wearing a redshirt, jeans, colorful hairband, ties her shoelaceswhile sitting on a patterned rug on the floor.?
A young girl in a bright red shirt reads a bookwhile sitting in a chair with her legs folded.
Thehedges of a garden surround the girl while awoods thick with green leaves lies nearby.?
A young Hispanic girl in a red shirt smiles toreveal braces on her teeth.The following image appears with a lower rating,90%, because the red shirt is later in the sentence.The noun phrase ratings do not play a role here,since red does modify shirt in this case; the ratingsapply only to core noun phrases, not prepositionalmodifiers.?
A young girl in a blue shirt presents a gift toher father.
The father wears a red shirt.hnages with girls in non-red shirts appear witheven lower ratings if no red shirt is mentioned at all.This image was ranked at 88%.?
A laughing little girl wearing a straw hat witha red flower, a purple shirt, blue jean overalls.Of course, in a fully NLP-based IR system, neitherof these examples would match at all.
But full NLPis too slow for this application, and partial matchesdo seem to be useful to its users, i.e.
do seem to leadto licensing of photos.Using the output of the part-of-speech tagger, thepatterns yield weights that prefer syntactically sinai-lar matches over scrambled or partial matches.
Theweights file for NPs contains three multipliers thatcan be set:scale noun 200 This sets the relative weight of thehead noun itself to 200%.scale modi f ie r  50 This sets the relative impor-tance of each modifier to half of what it wouldbe otherwise.scale phrase  200 This sets the relative weight ofthe entire noun phrase, compared to the oldranking values.
This effect multiplies the nounand modifier effects, i.e.
it is cumulative.2.4 Name Recogn i t ionPatterns are also the basis for the name recognitionmodule, supporting recognition of the names of per-sons and organizations.
Elements marked as namesare then marked with a preference that they be re-trieved as a unit, and the names are expanded tomatch related forms.
Thus Bob Dole does not matchBob Packwood worked with Dole Pineapple at 100%,but it does match Senator Robert Dole.The name recognition patterns employ a large fileof name variants, set up as a simple alias table:the nicknames and variants of each name appear ona single line in the file.
The name variants werederived manually from standard sources, includingbaby-naming books.3 In teract ionsIn developing the system, interactions between sub-systems posed particular challenges.
In general, theproblems arose fi'om conflicts in data files.
Ill keep-ing with the layered approach and with good soft-ware engineering in general, the system is maximallymodular and data-driven.
Several of the modulesutilize the same types of information, and inconsis-tencies caused conflicts in several areas.
The part-of-speech tagger, morphological nalyzer, tokenizer,gazetteer, semantic net, stop-word list, and Booleanlogic all had to be made to cooperate.
This sectiondescribes everal problems in.
interaction and howthey were addressed.
In most cases, the solution wastighter data integration, i.e.
having the conflictingsubsystems access a single shared data file.
Othercases were addressed by loosening restrictions, pro-viding a backup in case of inexact data coordination.The morphological analyzer sometimes temmeddifferently from WordNet, complicating synonymlookup.
The problem was solved by using WordNet'smorphology instead.
In both cases, morphologicalvariants are created in advance and stored, so thatstemming is a lookup rather than a run-time process.Switching to WordNet's morphology was thereforequite simple.
However, some issues remain.
For ex-ample, pies lists the three senses of pi first, beforethe far more likely pie.The database on which the part-of-speech taggertrained was a collection of Wall Street Journal arti-cles.
This presented a problem, since the domain wasspecialized.
In any event, since the training data setwas not WordNet, they did not always agree.
Thiswas sorted out by performing searches independentof part of speech if no match was found for the initialpart of speech choice.
That is, if the tagger markedshort as a verb only (as in to short a stock), andWordNet did not find a verb sense, the search wasbroadened to allow any part of speech in WordNet.Apostrophes in possessives are tokenized as sep-arate words, turning Alzheimer's into Alzheimer'sand Nicole's into Nicole 's.
In the former case, thefull form is in WordNet and therefore should betaken as a unit; in the latter case, it should not.The fix here was to look up both, preferring the fullform.For pluralia tantum words (shorts, fatigues, dou-bles, AIDS, twenties), stripping the affix -s and thenlooking up the root word gives incorrect results.
In-stead, when the word is plural, the pluralia tantum,if there is one, is preferred; when it.
is singular, that.401Table 2: Conversions from English to BooleanEnglishandorwithnotbutwithoutexceptnorBooleanandorandnotandnotnotnotmeaning is ruled out.WordNet contains ome location information, butit is not nearly as complete as a gazetteer.
Somelocations, such as major cities, appear in both thegazetteer and in WordNet, and, particularly whenthere are multiple "senses" (New York state andcity, Springfield), must be reconciled.
We used thegazetteer for all location expansions, and recast it sothat it was in effect a branch of the WordNet seman-tic net, i.e.
hierarchically organized and attachedat the appropriate WordNet node.
This recastingenabled us to take advantage of WordNet's genericterms, so that city lights, for example, would matchlights on a Philadelphia street.
It also preserved thevarious gazetteer enhancements, uch as the sensepreference algorithm, superunits, and equivalences.Boolean operators appear covertly as Englishwords.
Many IR systems ignore them, but thatyields counterintuitive results.
Instead of treatingoperators as stop words and discarding them, we in-stead perform special handling on the standard setof Boolean operators, as well as an expandable set ofsynonyms.
For example, given insects except ants,many IR systems imply discard except, turning thequery, incorrectly, into insects and ants, retrievingexactly the items the user does not want.
To avoidthis problem, we convert the terms in Table 2 intoBoolean operators.4 Eva luat ionEvaluation has two primary goals in commercialwork.
First, is the software robust enough and accu-rate enough to satisfy paying customers?
Second, isa proposed change or new feature an improvementor a step backward?Customers are more concerned with precision, be-cause they do not like to see matches they can-not explain.
Precision above about 80% eliminatedthe majority of customer complaints about accuracy.Oddly enough, they are quite willing to make ex-cuses for bad system behavior, explaining away im-plausible matches, once they have been convinced ofthe system's basic accuracy.
The customers rarelytest recall, since it is rare either for them to knowwhich pictures are available or to enter successiverelated queries and compare the match sets.
Com-plaints about recall in the initial stages of systemdevelopment came from suppliers, who wanted toensure their own pictures could be retrieved reliably.To test recall as well as precision in a controlledenvironment, in tile early phase of development, atest set of 1200 images was created, and manuallymatched, by a photo researcher, against queries sub-mitted by other photo researchers.
The process wastime-consuming and frustratingly imprecise: it wasdifficult to score, since matches call be partial, andit was hard to determine how much credit to assignfor, say, a 70% match that seemed more like a 90%match to the human researcher.
Precision tests onthe live (500,000-image) PNI system were much eas-ier to evaluate, since the system was more likely tohave the images requested.
For example, while adatabase containing no little girls in red shirts willoffer up girls with any kind of shirt and anything red,a comprehensive database will bury those imperfectmatches beneath the more highly ranked, more ac-curate matches.
Ultimately, precision was tested on50 queries on the full system; any bad match, or par-tial match if ranked above a more complete match,was counted as a miss, and only the top 20 imageswere rated.
Recall was tested on a 50-image subsetcreated by limiting such non-NLP criteria as imageorientation and photographer.
Precision was 89.6%and recall was 92%.In addition, precision was tested by comparingquery results for each new feature added (e.g.
"Doesnoun phrase syntax do us any good?
What rank-ings work best?"}.
It was also tested by series ofrelated queries, to test, for example, whether pen-guins swimming retrieved the same images as swim-ming penguins.
Recall was tested by more relatedqueries and for each new feature, and, more formally,in comparison to keyword searches and to Excal-ibur's RetrievalWare.
Major testing occurred whenthe database contained 30,000 images, and againat 150,000.
At 150,000, one major result was thatWordNet senses were rearranged so that they werein frequency order based on the senses hand-taggedby captioners for the initial 150,000 images.In one of our retrieval tests, the combination ofnoun phrase syntax and name recognition improvedrecall by 18% at a fixed precision point.
While wehave not yet attempted to test the two capabili-ties separately, it does appear that name recogni-tion played a larger role in the improvement thandid noun phrase syntax.
This is in accord with pre-vious literature on the contributions of noun phrasesyntax (Lewis, 1992), (Lewis and Croft, 1990).4.1 Does Manua l  Sense-Tagg ing  ImprovePrec is ion?Preliminary experiments were performed on twosubcorpora, one with WordNet senses manuallytagged, and the other completely untagged.
The402corpora are not strictly comparable: since the pho-tos are different, the correct answers are different ineach case.
Nonetheless, ince each corpus includesover 20,000 pictures, there should be enough datato provide interesting comparisons, even at this pre-liminary stage.
Certain other measures have beentaken to ensure that the test is as useful as possi-ble within the constraints given; these are describedbelow.
Results are consistent with those shown inVoorhees (1994).Only precision is measured here, since the princi-pal effect of tagging is on precision: untagged irrel-evant captions are likely to show up in the results,but lack of tagging will not cause correct matches tobe missed.
Only crossing matches are scored as bad.That is, if Match 7 is incorrect, but Match 8, 9 and10 are correct, then the score is 90% precision.
If,on the other hand, Match 7 is incorrect and Matches8, 9 and 10 are also incorrect, there is no precisionpenalty, since we want and expect partial matchesto follow the good matches.Only the top ten matches are scored.
There arethree reasons for this: first, scoring hundreds orthousands of matches is impractical.
Second, in ac-tual usage, no one will care if Match 322 is betterthan Match 321, whereas incongruities in the top tenwill matter very much.
Third, since the threshold isset at 50%, some of the matches are by definitiononly "half right."
Raising the threshold would in-crease perceived precision but provide less insightabout system performance.Eleven queries scored better in the sense-taggedcorpus, while only two scored better in the untaggedcorpus.
The remainder scored the same in both cor-pora.
In terms of precision, the sense-tagged corpusscored 99% while the untagged corpus scored 89%(both figures are artificially inflated, but in parallel,since only crossing matches are scored as bad).5 Future  D i rec t ionsFuture work will concentrate on speed and space op-timizations, and determining how subcomponents ofthis NLP capability can be incorporated into ex-isting IR packages.
This fine-grained NLP-basedIR can also answer questions uch as who, when,and where, so that the items retrieved can be morespecifically targeted to user needs.
The next stepfor caption-based systems will be to incorporate au-tomatic disambiguation, so that captioners will notneed to select a WordNet sense for each ambigu-ous word.
In this auto-disambiguation investiga-tion, it will be interesting to determine whether aspecialized corpus, e.g.
of photo captions, performssense-tagging significantly better than a general-purpose corpus, such as the Brown corpus (Francisand Ku~era, 1979).ReferencesChurch, K. W. 1988.
Stochastic Parts Programand Noun Phrase Parser for Unrestricted Text.
InProceedings of the Second Conference on AppliedNatural Language Processing, Austin, TX, 1988.Evans, D. and C. Zhai 1996.
Noun-Phrase Analy-sis in Unrestricted Text for Information Retrieval.In Proceedings of the 34th Annual Meeting of theAssociation for Computational Linguistics (A CL),Santa Cruz, CA, 24-27 June 1996, pp.17-24.Flank, S., P. Martin, A. Balogh and J. Rothey 1995.PhotoFile: A Digital Library for Image Retrieval.In Proceedings of the International Conferenceo17 Multimedia Computing and Systems (IEEE),Washington, DC, 15-18 May 1995, pp.
292-295.Flank, S., D. Garfield, and D. Norkin 1995.
Dig-ital Image Libraries: An Innovative Method forStorage, Retrieval, and Selling of Color Images.In Proceedings of the First International Sympo-sium on Voice, Video, and Data Communicationsof the Society of Photo-Optical InstrumentationEngineers (SPIE}, Philadelphia, PA, 23-26 Octo-ber 1995.Francis, W. N. and H. Ku~era 1979.
Manualof btformation to Accompany a Standard Corpusof Present-Day Edited American English, for usewith Digital Computers (Corrected and RevisedEdition), Department of Linguistics, Brown Uni-versity, Providence, RI.Lewis, D. D. 1992.
An Evaluation of Phrasal andClustered Representations on a Text Categoriza-tion Task.
In Proceedings of ACM SIGIR, 1992,pp.
37-50.Lewis, D. D. and W. B. Croft 1990.
Term Cluster-ing of Syntactic Phrases.
In Proceedings of ACMSIGIR, 1990, pp.
385-404.Miller, G., M. Chodorow, S. Landes, C. Leacockand R. Thomas 1994.
Using a semantic oncor-dance for sense identification.
In ARPA Workshopof Human Language Technology, Plainsboro, N J,March 1994, pp.
240-243.Strzalkowski, T. 1993.
Natural Language Process-ing in Large-Scale Text Retrieval Tasks.
In FirstText Retrieval Conference (TREC-1), NationalInstitute of Standards and Technology, March1993, pp.
173-187.Strzalkowski, T., J. Perez Carballo and M. Mari-nescu 1995.
Natural Language Information Re-trieval: TREC-3 Report.
In Third Text RetrievalConference (TREC-3), National Institute of Stan-dards and Technology, March 1995.Voorhees, E. 1994.
Query Expansion Using Lexical-Semantic Relations.
In Proceedings of ACM SI-GIR 1994, pp.
61-69.403
