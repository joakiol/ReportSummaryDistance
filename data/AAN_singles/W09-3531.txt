Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 136?142,Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLPBridging Languagesby SuperSense Entity TaggingDavide Picca and Alfio Massimiliano Gliozzo* and Simone Campora**University of Lausanne, CH 1015-Lausanne-Switzerland*Semantic Technology Lab (STLab - ISTC - CNR), Via Nomentana 56-0016, Rome, Italy**Ecole Polytechnique Federale de Lausanne (EPFL)davide.picca@unil.ch, alfio.gliozzo@istc.cnr.it, simone.campora@gmail.comAbstractThis paper explores a very basic linguis-tic phenomenon in multilingualism: thelexicalizations of entities are very oftenidentical within different languages whileconcepts are usually lexicalized differ-ently.
Since entities are commonly re-ferred to by proper names in natural lan-guage, we measured their distribution inthe lexical overlap of the terminologies ex-tracted from comparable corpora.
Resultsshow that the lexical overlap is mostlycomposed by unambiguous words, whichcan be regarded as anchors to bridge lan-guages: most of terms having the samespelling refer exactly to the same entities.Thanks to this important feature of NamedEntities, we developed a multilingual su-per sense tagging system capable to distin-guish between concepts and individuals.Individuals adopted for training have beenextracted both by YAGO and by a heuristicprocedure.
The general F1 of the Englishtagger is over 76%, which is in line withthe state of the art on super sense taggingwhile augmenting the number of classes.Performances for Italian are slightly lower,while ensuring a reasonable accuracy levelwhich is capable to show effective resultsfor knowledge acquisition.1 IntroductionThe Semantic Web paradigm is often requiredto provide a structured view of the unstructuredinformation expressed in texts (Buitelaar et al,2005; Cimiano, 2006).
Semantic technology re-quires abundance of such kind of knowledge inorder to cover the web scale in almost any lan-guage.
Natural Language Processing (NLP) hasbeen adopted with the purpose of knowledge ac-quisition, and in particular for ontology learn-ing and information extraction.
Structured infor-mation in ontologies is often expressed by tax-onomies of concepts, and then populated by in-stances.Nonetheless, automatically distinguish con-cepts from entities in taxonomies is not an easytask, especially as far as the problem of acquiringsuch knowledge from texts is concerned (Zirn etal., 2008; Picca and Popescu, 2007; Miller andHristea, 2006).
First of all because such a dis-tinction is quite vague.
From a description log-ics perspective, that is incidently widely adoptedin ontology engineering, instances are the leavesof any taxonomy as they cannot be further sub-categorized and populated by other instances.
Forexample,?Bill Clinton?
is clearly an individual,since it is instance of many concepts, such as per-son or president, but at the same time it is a nonsense describing individuals belonging to the classBill Clinton.In order to tackle this issue, we aim to provideempirical evidence to a very basic linguistic phe-nomenon in multilingualism, which allows the ex-ploitation of comparable corpora for bilingual lex-ical acquisition.
It consists on the fact that the lexi-calizations of entities is very often identical withindifferent languages while concepts are usually lex-icalized differently (de Pablo et al, 2006).
Theexistence of this phenomenon is quite intuitive andcan be easily justified by considering entities as of-ten referred to by means of ostensive acts (i.e.
theact of nominating objects by indicating them), per-formed in presentia during every day life.
Sinceentities are usually referred to using proper namesin natural language, we measured their distribu-tion in the lexical overlap of the terminologies ex-tracted from comparable corpora in two differentsample languages (i.e.
Italian and English).Named Entities are instances of particular con-cepts (such as person or location) and are referred136to by proper names.
Named Entity Recognition(NER) is a basic task in NLP that has the in-tent of automatically recognizing Named Entities.Incidentally, NER systems can be a useful stepfor broad-coverage ontology engineering but theyhave two main limitations:?
Traditional categories (e.g., person, location,and organization) are too few and generic.
Itis quite evident that taxonomies require morecategories than the three mentioned above.?
Even though NER systems are supposed torecognize individuals, very often they also re-turns common names and no clear distinctionwith concepts is made.A Super Sense Tagger (SST) (Ciaramita andJohnson, 2003) is an extended NER system thatuses the wider set of categories composed by the41 most general concepts defined by WordNet.WordNet has been organized according to psy-cholinguistic theories on the principles governinglexical memory (Beckwith et al, 1991).
Thus thebroadest WordNet?s categories can serve as basisfor a set of categories which exhaustively covers,at least as a first approximation, all possible con-cepts occurring in a sentence.The aim of this paper is to develop and explorethe property of instances being lexicalized identi-cally in different languages in order to produce aSST having the following two features:?
Make explicit distinction between instancesand concepts.?
Analyze the terminology of different lan-guages adopting a common category set.Nevertheless, the first point demands to facewith the vague distinction between concepts andindividuals belonging to those concepts.
So oneof the main issues explored in this paper is the au-tomatic tagging of which categories clearly havethis distinction.The paper is organized as follows.
In Section2 we describe the multilingual SST, an Italian ex-tension of the English SST that we exploited inSection 3 to show that the lexical overlap betweenlanguages is mostly composed by unambiguouswords, which can be also regarded as anchors tobridge the two languages.
Most of terms havingthe same spelling in the two languages exactly re-fer to the same entities.
We measured those oc-currencies with respect to all different ontologi-cal types identified by our tagging device, observ-ing that most of the overlapped terms are propernames of persons, organization, locations and ar-tifact, while the remaining ontological types aremostly lexicalized by common nouns and have aquite empty overlap.
This confirms our claim thatentities of tangible types are always lexicalized bythe same terms.In Section 4 we extended the SuperSense Tag-ger in order to distinguish instances from individ-uals, while Section 5 is about evaluation.
FinallySection 6 concludes the paper proposing new di-rections for future investigation.2 Multilingual Supersense TaggingSuperSense Tagging is the problem to identifyterms in texts, assigning a ?supersense?
category(e.g.
person, act) to their senses within theircontext and apply it to recognize concepts and in-stances in large scale textual collections of texts.An example of tagging is provided here:GunsB?noun.group andI?noun.groupRosesI?noun.group playsB?verb.communicationatO theO stadiumB?noun.locationThese categories are extracted from WordNet.WordNet (Fellbaum, 1998) defines 45 lexicogra-pher?s categories, also called supersenses (Cia-ramita and Johnson, 2003).
They are used by lex-icographers to provide an initial broad classifica-tion for the lexicon entries 1.Although simplistic in many ways, the super-sense ontology has several attractive features forNLP purposes.
First of all, concepts are easily rec-ognizable, however very general.
Secondly, thesmall number of classes makes the implementa-tion of state of the art methods possible (e.g.
se-quence taggers) to annotate text with supersenses.Finally, similar word senses tend to be merged to-gether reducing ambiguity.
This technology hasbeen also adopted for Ontology Learning (Picca etal., 2007), as the top level WordNet supersensescover almost any high level ontological type ofinterest in ontology design.
Compared to othersemantic tagsets, supersenses have the advantageof being designed to cover all possible open classwords.
Thus, in principle there is a supersense cat-1We have used the WordNet version 2.0 for all the exper-iments in the paper.137egory for each word, known or novel.
Addition-ally, no distinction is made between proper andcommon nouns, whereas standard NER systemstends to be biased towards the former.Following the procedure described in (Picca etal., 2008), we developed a multilingual SST work-ing on both Italian and English languages by train-ing the same system on MultiSemcor (Bentivogliet al, 2004), a parallel English/Italian corpus com-posed of 116 texts which are the translation oftheir corresponding English texts in SemCor.
Thisresource has been developed by manually trans-lating the English texts to Italian.
Then, the sogenerated parallel corpus has been automaticallyaligned at the Word Level.
Finally, sense labelshave been automatically transferred from the En-glish words to their Italian translations.The sense labels adopted in the Italian part ofMultiSemCor (Bentivogli et al, 2004) have beenextracted by Multi WordNet 2.
It is a multilingualcomputational lexicon, conceived to be strictlyaligned with the Princeton WordNet.
The avail-able languages are Italian, Spanish, Hebrew andRomanian.
In our experiment we used the En-glish and the Italian components.
The last versionof the Italian WordNet contains around 58,000Italian word senses and 41,500 lemmas organizedinto 32,700 synsets aligned with WordNet Englishsynsets.
The Italian synsets are created in cor-respondence with the Princeton WordNet synsetswhenever possible, and the semantic relations areported from the corresponding English synsets.This implies that the synset index structure is thesame for the two languages.The full alignment between the English and theItalian WordNet is guaranteed by the fact that bothresources adopts the same synset IDs to refer toconcepts.
This nice feature has allowed us to in-fer the correct super-sense for each Italian senseby simply looking at the English structure.
In thisway, we assign exactly the same ontological typesto both Italian and English terms, thus obtaining anItalian corpus tagged by its supersenses as shownbelow:IO GunsB?noun.group andI?noun.groupRosesI?noun.group suonanoB?verb.communicationalloO stadioB?noun.location2Available at http://multi WordNet.itc.it.3 Lexical Overlap in ComparableCorporaComparable corpora are collections of texts in dif-ferent languages that regard similar topics (e.g.a collection of news published by press agenciesin the same period).
More restrictive require-ments are expected for parallel corpora (i.e.
cor-pora composed of texts which are mutual transla-tions), while the class of the multilingual corpora(i.e.
collection of texts expressed in different lan-guages without any additional requirement) is themore general.
Obviously parallel corpora are alsocomparable, while comparable corpora are alsomultilingual.In comparable corpora, most of the individu-als preserve the same spelling across different lan-guages, while most concepts are translated differ-ently.
The analysis of the acquired terms for differ-ent ontological types shows a huge percentage ofoverlapped Named Entities.
For our experiments,we assumed that the distinction between commonnames and proper names reflect as well the dif-ference between concepts and entities in a formalontology.
Since proper names are recognized bythe PoS tagger with relatively high precision, weinterpreted occurrences of proper names in the ac-quired terminology as an evidence for detectingentities.The Leipzig Corpora Collection (Quasthoff,2006) presents corpora in different languages us-ing the same format and comparable sources.
Thecorpora are identical in format and similar in sizeand content.
They contain randomly selected sen-tences in the language of the corpus.
For the ex-periments reported in this paper, we used the Ital-ian and the English part composed by 300,000sentences.
As shown in Figure 1 and in Figure2, Named Entities are mostly concentrated intotangible types: Groups (organizations), Locations,Persons and Artifacts.The results analysis is more impressive.
Figure3 shows that the lexical overlap (i.e.
the subsetof terms in common between English and Italian)is composed almost exclusively by entities (i.e.proper nouns).
Instead if we take a look at Figure4, we can observe that concepts are generally notshared, having an average percentage lower than0.1%, independently of the ontological type.
Wecan also observe the predictable result that onto-logical categories denoting material objects (i.e.persons, locations and groups, artifacts) still have138Figure 1: Distribution of discovered entity typesin EnglishFigure 2: Distribution of discovered entity typesin Italiangreater percentage of shared entities.This is in line with the common practice oftraining NER on these categories.
Examples ofshared terms (entities) in concrete categories are:?
noun.group: e.g.
NATO, Boeing, NASA;?
noun.location: e.g.
Canada, Austria, Hous-ton;?
noun.person: e.g.
Romano Prodi, Blair,Kofi Annan.Incidentally, exceptions can be found to ourhypothesis (i.e.
some concept is also shared).Figure 3: Shared Named Entities in both lan-guagesFigure 4: Shared Concepts in both languagesExamples are terms belonging to the supersensenoun.object such as Radio and Computer.Anyhow, being them ported from one languageto another, they generally do not cause problems,since they tend to share the same meaning.
In ourexperiments (i.e.
in the sample we manually ana-lyzed), we did not find any false friend, suggestingthat the impact of those words is relatively small,in spite of the fact that it is very often overempha-sized.Inversely, many abstract types (e.g.noun.possession and noun.feeling) do notshare terminology at all.4 Distinguishing entities from conceptsSuccessively, we subdivided each category intotwo sub-categories for both languages, Instanceand Concept so that now the term ?president?
istagged as noun.person Concept and the term ?BillClinton?
as noun.person Instance.
In order to au-tomate this task and create a reliable training set,we adopted the following strategy.We used the concept/instances distinction pro-vided by YAGO (Suchanek et al, 2007b).
YAGOis a huge semantic knowledge base developed bythe Max-Plack-Institute of Saarbrcken.
YAGOknows over 1.7 million entities (like persons,organizations, cities, etc.).
YAGO, exploitsWikipedia?s info-boxes and category pages.
Info-boxes are standardized tables that contain basic in-formation about the entity described in the article(Suchanek et al, 2007a).
For our purposes it isfundamental that YAGO?s components are repre-sented as entities.
In our experiment we exploitentities as proper names and we use only YAGOentity database containing named entities.For each term belonging to one of the concretecategories, we check if it appears in YAGO en-tity dataset, otherwise, if the term is not found in139YAGO, it has to satisfy all the following condi-tions to be tagged as Instance:?
The part of speech of the term belongs toone of the noun categories as ?NN?, ?NNS?,?NNP?
or ?NNPS?.?
The first letter of the term is a capital letter.?
The term does not come after a full stop.Upon a total of 12817 instances, almost 14 havebeen found in YAGO, 3413 have been found usingthe heuristic strategy and the rest have been classi-fied as concepts.
If we take the previous example,the new output has now this form:?
GunsB?noun.group?InstanceandI?noun.group?InstanceRosesI?noun.group?InstanceplaysB?verb.communication atO theOstadiumB?noun.location?Conceptor?
GunsB?noun.group?InstanceandI?noun.group?InstanceRosesI?noun.group?InstancesuonanoB?verb.communication alloOstadioB?noun.location?ConceptAfterwards, we trained the SST engine.
It im-plements a Hidden Markov Model, trained withthe perceptron algorithm introduced in (Collins,2002) and it achieves a recall of 77.71% anda precision of 76.65% .
Perception sequencelearning provides an excellent trade-off accu-racy/performance, sometimes outperforming morecomplex models such as Conditional RandomFields (Nguyen and Guo, 2007).
We optimized therequired parameters by adopting a cross validationtechnique.
As for the settings developed by (Cia-ramita and Johnson, 2003), the best results havebeen obtained by setting 50 trials and 10 epochs totrain the perceptron algorithm.
The basic featureset used for the training process, includes:?
word = lower-cased form of each token forthe current position i and in addition for i-1and i+1?
sh = shape of the token as a simple regularexpression-like representation?
pos = POS of i, i-1 and i+1Category Recall Prec.
F1noun.artifact Concept 0.72 0.73 0.73noun.artifact Instance 0.59 0.64 0.62noun.group Concept 0.72 0.73 0.73noun.group Instance 0.68 0.70 0.69noun.location Concept 0.68 0.65 0.66noun.location Instance 0.75 0.80 0.77noun.person Concept 0.83 0.80 0.82noun.person Instance 0.92 0.88 0.90Table 1: Recall, precision and F1 for each categoryfor English?
sb= bi- and tri-grams of characters of the suf-fix of word i?
pr= bi- and tri-grams of characters of the pre-fix of word i?
rp = coarse relative position of word i,rp=begin if i = 0, rp=end if i = ?sentence?-1, sb=mid otherwise?
kf = constant features on each token for reg-ularization purposesFinally, we trained the SST engine in the Italiancorpus generated so far, and we evaluated the su-per sense tagging accuracy by adopting the sameevaluation method as described in (Ciaramita andJohnson, 2003), obtaining F1 close to 0.70.
How-ever quite lower than the English F1, this result isin line with the claim, since the Italian corpus issmaller and lower in quality.5 SST Performance and EvaluationWe evaluated the performances of the SST gen-erated so far by adopting a n-fold cross valida-tion strategy on the Semcor adopted for training.Results for the chosen categories are illustratedin Table 1 and Table 2, reporting precision, re-call and F1 for any Supersense.
If we cast adeeper glance at the tables, we can clearly no-tice that for some category the F1 is exception-ally high.
Some of those best categorized cat-egories are really essential for ontology learn-ing.
For example, important labels as noun.personor noun.group achieve results among the 70%.For some categories we have found a F1 over0.80% as noun.person Instance (F1 0.90% ) ornoun.person Concept (F1 0.85% )On the other hand, the Italian tagger achievedlower performances if compared with the English.140Category Recall Prec.
F1noun.artifact Concept 0.64 0.63 0.63noun.artifact Instance 0.66 0.67 0.66noun.group Concept 0.61 0.65 0.63noun.group Instance 0.66 0.66 0.66noun.location Concept 0.55 0.53 0.54noun.location Instance 0.56 0.76 0.64noun.person Concept 0.81 0.76 0.78noun.person Instance 0.88 0.81 0.85Table 2: Recall, precision and F1 for each categoryfor ItalianIt can be explained by (i) the lower quality of thetraining resource, (ii) the lower quantity of trainingdata and (iii) the unavailability of the first senseinfo.Regarding the first point, it is worthwhile to re-mark that even if the quality of transfer developedby (Bentivogli et al, 2004) is high, many incor-rect sense transfers (around 14%) can be found.Because of that our work suffers of the same in-herited faults by the automatic alignment.
For in-stance, we report here the most relevant errors wefaced with during the preprocessing step.
One ofthe main errors that has badly influenced the train-ing set especially for multiword recognition is thecase in which the translation equivalent is indeed across-language synonym of the source expressionbut not a lexical unit.
It occurs when a languageexpresses a concept with a lexical unit whereas theother language expresses the same concept with afree combination of words (for instance occhialida sole annotated with the sense of sunglasses).Regarding the second problem, we noticedthat the quantity of sense labeled words adoptedfor English is higher than 200,000, whereas theamount of Italian tokens adopted is around 92,000.Therefore, the amount of Italian training datais sensibly lower, explaining the lower perfor-mances.Moreover, the italian SST lacks in one of themost important feature used for the English SST,first sense heuristics.
In fact, for the Italian lan-guage, the first sense baseline cannot be estimatedby simply looking at the first sense in WordNet,since the order of the Italian WordNet does not re-flect the frequency of senses.
Therefore, we didnot estimate this baseline for the Italian SST, incontrast to what has been done for the EnglishSST.6 Conclusion and Future WorkIn this work, we presented an empirical investiga-tion about the role of Named Entities in compara-ble corpora, showing that they largely contributein finding bridges between languages since theytend to refer to the same entities.
This featureallows us to discover bridges among languagesby simply looking for common Named Entities incorpora that are generally not parallels since suchterms are usually associated to the same objectsin the external world.
We demonstrated that mostterms in the lexical overlap between languages areentities, and we showed that they belong to fewfundamentals categories (including persons, loca-tions and groups).A predominant amount of entities in the lexi-cal overlap could be conceived as a support to ourclaim that Named Entities can be used to bridgethe languages, since they preserve meaning andprovide a set of highly accurate anchors to bridgelanguages in multilingual knowledge bases.
Thoseanchors can be used as a set of seeds to boost fur-ther statistical or logical lexical acquisition pro-cesses.
In addition, the impact of false friends re-vealed to be less problematic than expected.We trained a multilingual super sense taggeron the Italian and English language and we in-troduced the distinction between concept and in-stance in a subset of its target classes, where ourinvestigation suggested to look for concrete types.The resulting tagger largely extend the capabilitiesof the state of art supersense technology, by pro-viding a multilingual tool which can be effectivelyused for multilingual knowledge induction.For the future, we are going to further explorethe direction of multilingual knowledge induction,exploiting the tagger developed so far for ontologyengineering and knowledge retrieval.
In addition,we plan to leverage more on the lexical overlapproperty analyzed in this paper, for example to de-velop unsupervised super sense taggers for all lan-guages where annotated corpora are not available.AcknowledgmentsAlfio Massimiliano Gliozzo has been supported bythe BONy project, financed by the Education andculture DG of the EU, grant agreement N 135263-2007-IT-KA3-KA3MP, under the Lifelong Learn-ing Programme 2007 managed by EACEA.141ReferencesR.
Beckwith, C. Fellbaum, D. Gross, and G. Miller.1991.
9. wordnet: A lexical database organized onpsycholinguistic principles.
Lexicons: Using On-Line Resources to Build a Lexicon, pages 211?232,Jan.L.
Bentivogli, P. Forner, and E. Pianta.
2004.
Evalu-ating cross-language annotation transfer in the mul-tisemcor corpus.
In COLING ?04: Proceedings ofthe 20th international conference on ComputationalLinguistics, page 364, Morristown, NJ, USA.
Asso-ciation for Computational Linguistics.P.
Buitelaar, P. Cimiano, and B. Magnini.
2005.
On-tology learning from texts: methods, evaluation andapplications.
IOS Press.M.
Ciaramita and M. Johnson.
2003.
Supersense tag-ging of unknown nouns in wordnet.
In Proceedingsof EMNLP-03, pages 168?175, Sapporo, Japan.P.
Cimiano.
2006.
Ontology Learning and Popula-tion from Text: Algorithms, Evaluation and Appli-cations.
Springer-Verlag New York, Inc., Secaucus,NJ, USA.M.
Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and exper-iments with perceptron algorithms.
In Proceedingsof EMNLP-02.C.
de Pablo, J.L.
Mart?
?nez, and P.
Mart??nez.
2006.Named entity processing for cross-lingual and mul-tilingual ir applications.
In proceedings of the SI-GIR2006 workshop on New Directions In Multilin-gual Information Access.C.
Fellbaum.
1998.
WordNet.
An Electronic LexicalDatabase.
MIT Press.G.
A. Miller and F. Hristea.
2006.
Wordnet nouns:Classes and instances.
Computational Linguistics,32(1):1?3.N.
Nguyen and Y. Guo.
2007.
Comparison of se-quence labeling algorithms and extensions.
In Pro-ceedings of ICML 2007, pages 681?688.D.
Picca and A. Popescu.
2007.
Using wikipedia andsupersense tagging for semi-automatic complex tax-onomy construction.
In proceedings RANLP.D.
Picca, A. Gliozzo, and M. Ciaramita.
2007.
Se-mantic domains and supersens tagging for domain-specific ontology learning.
In proceedings RIAO2007.D.
Picca, A. M. Gliozzo, and M. Ciaramita.
2008.Supersense tagger for italian.
In proceedings ofthe sixth international conference on Language Re-sources and Evaluation (LREC 2008).C.
B. Quasthoff, U. M. Richter.
2006.
Corpus portalfor search in monolingual corpora,.
In Proceedingsof the fifth international conference on LanguageResources and Evaluation, LREC, pages pp.
1799?1802.F.
Suchanek, G. Kasneci, and G. Weikum.
2007a.Yago: A large ontology from wikipedia and word-net.
Technical Report.F.
M. Suchanek, G. Kasneci, and G. Weikum.
2007b.Yago: a core of semantic knowledge.
In WWW ?07:Proceedings of the 16th international conference onWorld Wide Web, pages 697?706, New York, NY,USA.
ACM Press.C.
Zirn, V. Nastase, and M. Strube.
2008.
Distinguish-ing between instances and classes in the wikipediataxonomy.
Lecture notes in computer science, Jan.142
