Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 103?110,Sydney, July 2006. c?2006 Association for Computational LinguisticsAutomatic classication of citation functionSimone Teufel Advaith Siddharthan Dan TidharNatural Language and Information Processing GroupComputer LaboratoryCambridge University, CB3 0FD, UK{Simone.Teufel,Advaith.Siddharthan,Dan.Tidhar}@cl.cam.ac.ukAbstractCitation function is defined as the author?sreason for citing a given paper (e.g.
ac-knowledgement of the use of the citedmethod).
The automatic recognition of therhetorical function of citations in scientifictext has many applications, from improve-ment of impact factor calculations to textsummarisation and more informative ci-tation indexers.
We show that our anno-tation scheme for citation function is re-liable, and present a supervised machinelearning framework to automatically clas-sify citation function, using both shallowand linguistically-inspired features.
Wefind, amongst other things, a strong re-lationship between citation function andsentiment classification.1 IntroductionWhy do researchers cite a particular paper?
Thisis a question that has interested researchers indiscourse analysis, sociology of science, and in-formation sciences (library sciences) for decades(Garfield, 1979; Small, 1982; White, 2004).
Manyannotation schemes for citation motivation havebeen created over the years, and the question hasbeen studied in detail, even to the level of in-depthinterviews with writers about each individual cita-tion (Hodges, 1972).Part of this sustained interest in citations canbe explained by the fact that bibliometric met-rics are commonly used to measure the impact ofa researcher?s work by how often they are cited(Borgman, 1990; Luukkonen, 1992).
However, re-searchers from the field of discourse studies havelong criticised purely quantitative citation analy-sis, pointing out that many citations are done outof ?politeness, policy or piety?
(Ziman, 1968),and that criticising citations or citations in pass-ing should not ?count?
as much as central cita-tions in a paper, or as those citations where a re-searcher?s work is used as the starting point ofsomebody else?s work (Bonzi, 1982).
A plethoraof manual annotation schemes for citation motiva-tion have been invented over the years (Garfield,1979; Hodges, 1972; Chubin and Moitra, 1975).Other schemes concentrate on citation function(Spiegel-Ru?sing, 1977; O?Connor, 1982; Wein-stock, 1971; Swales, 1990; Small, 1982)).
Oneof the best-known of these studies (Moravcsikand Murugesan, 1975) divides citations in runningtext into four dimensions: conceptual or opera-tional use (i.e., use of theory vs. use of technicalmethod); evolutionary or juxtapositional (i.e., ownwork is based on the cited work vs. own work is analternative to it); organic or perfunctory (i.e., workis crucially needed for understanding of citing ar-ticle or just a general acknowledgement); and fi-nally confirmative vs. negational (i.e., is the cor-rectness of the findings disputed?).
They found,for example, that 40% of the citations were per-functory, which casts further doubt on the citation-counting approach.Based on such annotation schemes and hand-analyzed data, different influences on citation be-haviour can be determined.
Nevertheless, re-searchers in the field of citation content analysisdo not normally cross-validate their schemes withindependent annotation studies with other humanannotators, and usually only annotate a small num-ber of citations (in the range of hundreds or thou-sands).
Also, automated application of the annota-tion is not something that is generally consideredin the field, though White (2004) sees the future ofdiscourse-analytic citation analysis in automation.Apart from raw material for bibliometric stud-ies, citations can also be used for search purposesin document retrieval applications.
In the libraryworld, printed or electronic citation indexes suchas ISI (Garfield, 1979) serve as an orthogonal103Nitta and Niwa 94Resnik 95Brown et al 90aRose et al 90Church and Gale 91Dagan et al 94Li and Abe 96Hindle 93Hindle 90Dagan et al93Pereira et al 93Following Pereira et al we measureword similarity by the relative entropyor Kulbach?Leibler (KL) distance, bet?ween the corresponding conditionaldistributions.His notion of similarityseems to agree with ourintuitions in many cases,but it is not clear how itcan  be used directly toconstruct word classesand corresponding models of association.Figure 1: A rhetorical citation mapsearch tool to find relevant papers, starting from asource paper of interest.
With the increased avail-ability of documents in electronic form in recentyears, citation-based search and automatic citationindexing have become highly popular, cf.
the suc-cessful search tools Google Scholar and CiteSeer(Giles et al, 1998).1But not all search needs are fulfilled by currentcitation indexers.
Experienced researchers are of-ten interested in relations between articles (Shum,1998).
They want to know if a certain article crit-icises another and what the criticism is, or if thecurrent work is based on that prior work.
Thistype of information is hard to come by with currentsearch technology.
Neither the author?s abstract,nor raw citation counts help users in assessing therelation between articles.Fig.
1 shows a hypothetical search tool whichdisplays differences and similarities between a tar-get paper (here: Pereira et al, 1993) and the pa-pers that it cites and that cite it.
Contrastive linksare shown in grey ?
links to rival papers and pa-pers the current paper contrasts itself to.
Continu-ative links are shown in black ?
links to papers thatuse the methodology of the current paper.
Fig.
1also displays the most characteristic textual sen-tence about each citation.
For instance, we can seewhich aspect of Hindle (1990) our example papercriticises, and in which way the example paper?swork was used by Dagan et al (1994).Note that not even the CiteSeer text snippet1These tools automatically citation-index all scientific ar-ticles reached by a web-crawler, making them available tosearchers via authors or keywords in the title, and displayingthe citation in context of a text snippet.can fulfil the relation search need: it is alwayscentered around the physical location of the ci-tations, but the context is often not informativeenough for the searcher to infer the relation.
Infact, studies from our annotated corpus (Teufel,1999) show that 69% of the 600 sentences stat-ing contrast with other work and 21% of the246 sentences stating research continuation withother work do not contain the corresponding cita-tion; the citation is found in preceding sentences(which means that the sentence expressing thecontrast or continuation is outside the CiteSeersnippet).
A more sophisticated, discourse-awarecitation indexer which finds these sentences andassociates them with the citation would add con-siderable value to the researcher?s bibliographicsearch (Ritchie et al, 2006b).Our annotation scheme for citations is basedon empirical work in content citation analysis.
Itis designed for information retrieval applicationssuch as improved citation indexing and better bib-liometric measures (Teufel et al, 2006).
Its 12 cat-egories mark relationships with other works.
Eachcitation is labelled with exactly one category.
Thefollowing top-level four-way distinction applies:?
Explicit statement of weakness?
Contrast or comparison with other work (4categories)?
Agreement/usage/compatibility with otherwork (6 categories), and?
A neutral category.In this paper, we show that the scheme can bereliably annotated by independent coders.
We alsoreport results of a supervised machine learning ex-periment which replicates the human annotation.2 An annotation scheme for citationsOur scheme (given in Fig.
2) is adapted from thatof Spiegel-Ru?sing (1977) after an analysis of acorpus of scientific articles in computational lin-guistics.
We avoid sociologically orientated dis-tinctions (?paying homage to pioneers?
), as theycan be difficult to operationalise without deepknowledge of the field and its participants (Swales,1986).
Our redefinition of the categories aims atreliably annotation; at the same time, the cate-gories should be informative enough for the docu-ment management application sketched in the in-troduction.104Category DescriptionWeak Weakness of cited approachCoCoGM Contrast/Comparison in Goals or Meth-ods(neutral)CoCo- Author?s work is stated to be superior tocited workCoCoR0 Contrast/Comparison in Results (neutral)CoCoXY Contrast between 2 cited methodsPBas Author uses cited work as basis or startingpointPUse Author usestools/algorithms/data/definitionsPModi Author adapts or modifiestools/algorithms/dataPMot This citation is positive about approachused or problem addressed (used to mo-tivate work in current paper)PSim Author?s work and cited work are similarPSup Author?s work and cited work are compat-ible/provide support for each otherNeut Neutral description of cited work, or notenough textual evidence for above cate-gories, or unlisted citation functionFigure 2: Annotation scheme for citation function.Our categories are as follows: One category(Weak) is reserved for weakness of previous re-search, if it is addressed by the authors.
The nextfour categories describe comparisons or contrastsbetween own and other work.
The difference be-tween them concerns whether the contrast is be-tween methods employed or goals (CoCoGM), orresults, and in the case of results, a difference ismade between the cited results being worse thanthe current work (CoCo-), or comparable or bet-ter results (CoCoR0).
As well as considering dif-ferences between the current work and other work,we also mark citations if they are explicitly com-pared and contrasted with other work (i.e.
notthe work in the current paper).
This is expressedin category CoCoXY.
While this is not typicallyannotated in the literature, we expect a potentialpractical benefit of this category for our applica-tion, particularly in searches for differences andrival approaches.The next set of categories we propose concernspositive sentiment expressed towards a citation, ora statement that the other work is actively usedin the current work (which we consider the ulti-mate praise).
We mark statements of use of dataand methods of the cited work, differentiating un-changed use (PUse) from use with adaptations(PModi).
Work which is stated as the explicitstarting point or intellectual ancestry is markedwith our category PBas.
If a claim in the liter-ature is used to strengthen the authors?
argument,or vice versa, we assign the category PSup.
Wealso mark similarity of (an aspect of) the approachto the cited work (PSim), and motivation of ap-proach used or problem addressed (PMot).Our twelfth category, Neut, bundles truly neu-tral descriptions of cited work with those caseswhere the textual evidence for a citation functionwas not enough to warrant annotation of that cate-gory, and all other functions for which our schemedid not provide a specific category.Citation function is hard to annotate because itin principle requires interpretation of author inten-tions (what could the author?s intention have beenin choosing a certain citation?).
One of our mostfundamental principles is thus to only mark explic-itly signalled citation functions.
Our guidelinesexplicitly state that a general linguistic phrase suchas ?better?
or ?used by us?
must be present; thisincreases the objectivity of defining citation func-tion.
Annotators must be able to point to textualevidence for assigning a particular function (andare asked to type the source of this evidence intothe annotation tool for each citation).
Categoriesare defined in terms of certain objective types ofstatements (e.g., there are 7 cases for PMot, e.g.
?Citation claims that or gives reasons for whyproblem Y is hard?).
Annotators can use generaltext interpretation principles when assigning thecategories (such as anaphora resolution and par-allel constructions), but are not allowed to use in-depth knowledge of the field or of the authors.Guidelines (25 pages, ?
150 rules) describe thecategories with examples, provide a decision treeand give decision aids in systematically ambigu-ous cases.
Nevertheless, subjective judgement ofthe annotators is still necessary to assign a singletag in an unseen context, because of the many dif-ficult cases for annotation.
Some of these concernthe fact that authors do not always state their pur-pose clearly.
For instance, several earlier studiesfound that negational citations are rare (Moravc-sik and Murugesan, 1975; Spiegel-Ru?sing, 1977);MacRoberts and MacRoberts (1984) argue that thereason for this is that they are potentially politi-cally dangerous.
In our data we found ample evi-dence of the ?meekness?
effect.
Other difficultiesconcern the distinction of the usage of a methodfrom statements of similarity between a methodand the own method (i.e., the choice between cat-egories PSim and PUse).
This happens in caseswhere authors do not want to admit (or stress)105that they are using somebody else?s method.
An-other difficult distinction concerns the judgementof whether the authors continue somebody?s re-search (i.e., consider their research as intellectualancestry, i.e.
PBas), or whether they simply usethe work (PUse).The unit of annotation is a) the full citation (asrecognised by our automatic citation processor onour corpus), and b) names of authors of cited pa-pers anywhere in running text outside of a for-mal citation context (i.e., without date).
Theselatter are marked up, slightly unusually in com-parison to other citation indexers, because we be-lieve they function as important referents compa-rable in importance to formal citations.2 In prin-ciple, there are many other linguistic expressionsby which the authors could refer to other people?swork: pronouns, abbreviations such as ?Muellerand Sag (1990), henceforth M & S?, and names ofapproaches or theories which are associated withparticular authors.
The fact that in these contextscitation function cannot be annotated (because itis not technically feasible to recognise them wellenough) sometimes causes problems with contextdependencies.While there are unambiguous example caseswhere the citation function can be decided on thebasis of the sentence alone, this is not always thecase.
Most approaches are not criticised in thesame sentence where they are also cited: it is morelikely that there are several descriptive sentencesabout a cited approach between its formal cita-tion and the evaluative statement, which is often atthe end of the textual segment about this citation.Nevertheless, the annotator must mark the func-tion on the nearest appropriate annotation unit (ci-tation or author name).
Our rules decree that con-text is in most cases constrained to the paragraphboundary.
In rare cases, paper-wide informationis required (e.g., for PMot, we need to know thata praised approach is used by the authors, infor-mation which may not be local in the paragraph).Annotators are thus asked to skim-read the paperbefore annotation.One possible view on this annotation schemecould consider the first two sets of categories as?negative?
and the third set of categories ?posi-tive?, in the sense of Pang et al (2002) and Turney(2002).
Authors need to make a point (namely,2Our citation processor can recognise these after parsingthe citation list.that they have contributed something which is bet-ter or at least new (Myers, 1992)), and they thushave a stance towards their citations.
But althoughthere is a sentiment aspect to the interpretation ofcitations, this is not the whole story.
Many of our?positive?
categories are more concerned with dif-ferent ways in which the cited work is useful to thecurrent work (which aspect of it is used, e.g., just adefinition or the entire solution?
), and many of thecontrastive statements have no negative connota-tion at all and simply state a (value-free) differ-ence between approaches.
However, if one looksat the distribution of positive and negative adjec-tives around citations, it is clear that there is a non-trivial connection between our task and sentimentclassification.The data we use comes from our corpus of360 conference articles in computational linguis-tics, drawn from the Computation and LanguageE-Print Archive (http://xxx.lanl.gov/cmp-lg).
Thearticles are transformed into XML format; head-lines, titles, authors and reference list items are au-tomatically marked up.
Reference lists are parsedusing regular patterns, and cited authors?
namesare identified.
Our citation parser then finds cita-tions and author names in running text and marksthem up.
Ritchie et al (2006a) report high ac-curacy for this task (94% of citations recognised,provided the reference list was error-free).
On av-erage, our papers contain 26.8 citation instances inrunning text3.
For human annotation, we use ourown annotation tool based on XML/XSLT tech-nology, which allows us to use a web browser tointeractively assign one of the 12 tags (presentedas a pull-down list) to each citation.We measure inter-annotator agreement betweenthree annotators (the three authors), who indepen-dently annotated 26 articles with the scheme (con-taining a total of 120,000 running words and 548citations), using the written guidelines.
The guide-lines were developed on a different set of articlesfrom the ones used for annotation.Inter-annotator agreement was Kappa=.72(n=12;N=548;k=3)4 .
This is quite high, consider-ing the number of categories and the difficulties3As opposed to reference list items, which are fewer.4Following Carletta (1996), we measure agreement inKappa, which follows the formula K = P (A)?P (E)1?P (E) whereP(A) is observed, and P(E) expected agreement.
Kapparanges between -1 and 1.
K=0 means agreement is only asexpected by chance.
Generally, Kappas of 0.8 are consideredstable, and Kappas of .69 as marginally stable, according tothe strictest scheme applied in the field.106(e.g., non-local dependencies) of the task.
Therelative frequency of each category observed inthe annotation is listed in Fig.
3.
As expected,the distribution is very skewed, with more than60% of the citations of category Neut.5 Whatis interesting is the relatively high frequencyof usage categories (PUse, PModi, PBas)with a total of 18.9%.
There is a relatively lowfrequency of clearly negative citations (Weak,CoCo-, total of 4.1%), whereas the neutral?contrastive categories (CoCoR0, CoCoXY,CoCoGM) are slightly more frequent at 7.6%.This is in concordance with earlier annotationexperiments (Moravcsik and Murugesan, 1975;Spiegel-Ru?sing, 1977).3 Features for automatic recognition ofcitation functionThis section summarises the features we use formachine learning citation function.
Some of thesefeatures were previously found useful for a dif-ferent application, namely Argumentative Zoning(Teufel, 1999; Teufel and Moens, 2002), some arespecific to citation classification.3.1 Cue phrasesMyers (1992) calls meta-discourse the set of ex-pressions that talk about the act of presenting re-search in a paper, rather than the research itself(which is called object-level discourse).
For in-stance, Swales (1990) names phrases such as ?toour knowledge, no.
.
.
?
or ?As far as we aware?
asmeta-discourse associated with a gap in the cur-rent literature.
Strings such as these have beenused in extractive summarisation successfully eversince Paice?s (1981) work.We model meta-discourse (cue phrases) andtreat it differently from object-level discourse.There are two different mechanisms: A finitegrammar over strings with a placeholder mecha-nism for POS and for sets of similar words whichcan be substituted into a string-based cue phrase(Teufel, 1999).
The grammar corresponds to 1762cue phrases.
It was developed on 80 papers whichare different to the papers used for our experimentshere.The other mechanism is a POS-based recog-niser of agents and a recogniser for specific actionsthese agents perform.
Two main agent types (the5Spiegel-Ru?sing found that out of 2309 citations she ex-amined, 80% substantiated statements.authors of the paper, and everybody else) are mod-elled by 185 patterns.
For instance, in a paragraphdescribing related work, we expect to find refer-ences to other people in subject position more of-ten than in the section detailing the authors?
ownmethods, whereas in the background section, weoften find general subjects such as ?researchers incomputational linguistics?
or ?in the literature?.For each sentence to be classified, its grammaticalsubject is determined by POS patterns and, if pos-sible, classified as one of these agent types.
Wealso use the observation that in sentences withoutmeta-discourse, one can assume that agenthoodhas not changed.20 different action types model the main verbsinvolved in meta-discourse.
For instance, there isa set of verbs that is often used when the over-all scientific goal of a paper is defined.
Theseare the verbs of presentation, such as ?propose,present, report?
and ?suggest?
; in the corpus wefound other verbs in this function, but with a lowerfrequency, namely ?describe, discuss, give, intro-duce, put forward, show, sketch, state?
and ?talkabout?.
There are also specialised verb clusterswhich co-occur with PBas sentences, e.g., thecluster of continuation of ideas (eg.
?adopt, agreewith, base, be based on, be derived from, be orig-inated in, be inspired by, borrow, build on,.
.
.
?
).On the other hand, the semantics of verbs in Weaksentences is often concerned with failing (of otherresearchers?
approaches), and often contain verbssuch as ?abound, aggravate, arise, be cursed, beincapable of, be forced to, be limited to, .
.
.
?.We use 20 manually acquired verb clusters.Negation is recognised, but too rare to define itsown clusters: out of the 20 ?
2 = 40 theoreticallypossible verb clusters, only 27 were observed inour development corpus.
We have recently auto-mated the process of verb?object pair acquisitionfrom corpora for two types of cue phrases (Abdallaand Teufel, 2006) and are planning on expandingthis work to other cue phrases.3.2 Cues Identified by annotatorsDuring the annotator training phase, the anno-tators were encouraged to type in the meta-description cue phrases that justify their choice ofcategory.
We went through this list by hand andextracted 892 cue phrases (around 75 per cate-gory).
The files these cues came from were notpart of the test corpus.
We included 12 features107Neut PUse CoCoGM PSim Weak PMot CoCoR0 PBas CoCoXY CoCo- PModi PSup62.7% 15.8% 3.9% 3.8% 3.1% 2.2% 0.8% 1.5% 2.9% 1.0% 1.6% 1.1%Figure 3: Distribution of citation categoriesWeak CoCoGM CoCoR0 CoCo- CoCoXY PBas PUse PModi PMot PSim PSup NeutP .78 .81 .77 .56 .72 .76 .66 .60 .75 .68 .83 .80R .49 .52 .46 .19 .54 .46 .61 .27 .64 .38 .32 .92F .60 .64 .57 .28 .62 .58 .63 .37 .69 .48 .47 .86Percentage Accuracy 0.77Kappa (n=12; N=2829; k=2) 0.57Macro-F 0.57Figure 4: Summary of Citation Analysis results (10-fold cross-validation; IBk algorithm; k=3).that recorded the presence of cues that our annota-tors associated with a particular class.3.3 Other featuresThere are other features which we use for thistask.
We know from Teufel and Moens (2002) thatverb tense and voice should be useful for recogniz-ing statements of previous work, future work andwork performed in the paper.
We also recognisemodality (whether or not a main verb is modifiedby an auxiliary, and which auxiliary it is).The overall location of a sentence containinga reference should be relevant.
We observe thatmore PMot categories appear towards the begin-ning of the paper, as do Weak citations, whereascomparative results (CoCoR0, CoCoR-) appeartowards the end of articles.
More fine-grained lo-cation features, such as the location within theparagraph and the section, have also been imple-mented.The fact that a citation points to own previouswork can be recognised, as we know who the pa-per authors are.
As we have access to the infor-mation in the reference list, we also know the lastnames of all cited authors (even in the case wherean et al statement in running text obscures thelater-occurring authors).
With self-citations, onemight assume that the probability of re-use of ma-terial from previous own work should be higher,and the tendency to criticise lower.4 ResultsOur evaluation corpus for citation analysis con-sists of 116 articles (randomly drawn from the partof our corpus which was not used for guidelinedevelopment or cue phrase acquisition).
The 116articles contain 2829 citation instances.
Eachcitation instance was manually tagged as oneWeakness Positive Contrast NeutralP .80 .75 .77 .81R .49 .65 .52 .90F .61 .70 .62 .86Percentage Accuracy 0.79Kappa (n=12; N=2829; k=2) 0.59Macro-F 0.68Figure 5: Summary of results (10-fold cross-validation; IBk algorithm; k=3): Top level classes.Weakness Positive NeutralP .77 .75 .85R .42 .65 .92F .54 .70 .89Percentage Accuracy 0.83Kappa (n=12; N=2829; k=2) 0.58Macro-F 0.71Figure 6: Summary of results (10-fold cross-validation; IBk algorithm; k=3): Sentiment Anal-ysis.of {Weak, CoCoGM, CoCo-, CoCoR0, CoCoXY,PBas, PUse, PModi, PMot, PSim, PSup, Neut}.The papers are then further processed (e.g.
to-kenised and POS-tagged).
All other features areautomatically determined (e.g.
self-citations aredetected by overlap of citing and cited authors);then, machine learning is applied to the featurevectors.The 10-fold cross-validation results for citationclassification are given in Figure 4, comparing thesystem to one of the annotators.
Results are givenin three overall measures: Kappa, percentage ac-curacy, and Macro-F (following Lewis (1991)).Macro-F is the mean of the F-measures of alltwelve categories.
We use Macro-F and Kappa be-cause we want to measure success particularly onthe rare categories, and because Micro-averagingtechniques like percentage accuracy tend to over-estimate the contribution of frequent categories in108heavily skewed distributions like ours6.In the case of Macro-F, each category is treatedas one unit, independent of the number of itemscontained in it.
Therefore, the classification suc-cess of the individual items in rare categoriesis given more importance than classification suc-cess of frequent category items.
However, oneshould keep in mind that numerical values inmacro-averaging are generally lower (Yang andLiu, 1999), due to fewer training cases for the rarecategories.
Kappa has the additional advantageover Macro-F that it filters out random agreement(random use, but following the observed distribu-tion of categories).For our task, memory-based learning outper-formed other models.
The reported results use theIBk algorithm with k = 3 (we used the Weka ma-chine learning toolkit (Witten and Frank, 2005)for our experiments).
Fig.
7 provides a few ex-amples from one file in the corpus, along with thegold standard citation class, the machine predic-tion, and a comment.Kappa is even higher for the top level distinc-tion.
We collapsed the obvious similar categories(all P categories into one category, and all CoCocategories into another) to give four top levelcategories (Weak, Positive, Contrast,Neutral; results in Fig.
5).
Precision for all thecategories is above 0.75, and K=0.59.
For con-trast, the human agreement for this situation wasK=0.76 (n=3,N=548,k=3).In a different experiment, we grouped the cate-gories as follows, in an attempt to perform senti-ment analysis over the classifications:Old Categories New CategoryWeak, CoCo- NegativePMot, PUse, PBas, PModi, PSim, PSup PositiveCoCoGM, CoCoR0, CoCoXY, Neut NeutralThus negative contrasts and weaknesses aregrouped into Negative, while neutral contrastsare grouped into Neutral.
All positive classesare conflated into Positive.Results show that this grouping raises resultsto a smaller degree than the top-level distinctiondid (to K=.58).
For contrast, the human agree-ment for these collapsed categories was K=.75(n=3,N=548,k=3).6This situation has parallels in information retrieval,where precision and recall are used because accuracy over-estimates the performance on irrelevant items.5 ConclusionWe have presented a new task: annotation of ci-tation function in scientific text, a phenomenonwhich we believe to be closely related to the over-all discourse structure of scientific articles.
Ourannotation scheme concentrates on weaknesses ofother work, and on similarities and contrast be-tween work and usage of other work.
In thispaper, we present machine learning experimentsfor replicating the human annotation (which is re-liable at K=.72).
The automatic result reachedK=.57 (acc=.77) for the full annotation scheme;rising to Kappa=.58 (acc=.83) for a three-wayclassification (Weak, Positive, Neutral).We are currently performing an experiment tosee if citation processing can increase perfor-mance in a large-scale, real-world informationretrieval task, by creating a test collection ofresearchers?
queries and relevant documents forthese (Ritchie et al, 2006a).6 AcknowledgementsThis work was funded by the EPSRC projects CIT-RAZ (GR/S27832/01, ?Rhetorical Citation Mapsand Domain-independent Argumentative Zon-ing?)
and SCIBORG (EP/C010035/1, ?Extractingthe Science from Scientific Publications?
).ReferencesRashid M. Abdalla and Simone Teufel.
2006.
A bootstrap-ping approach to unsupervised detection of cue phrasevariants.
In Proc.
of ACL/COLING-06.Susan Bonzi.
1982.
Characteristics of a literature as predic-tors of relatedness between cited and citing works.
JASIS,33(4):208?216.Christine L. Borgman, editor.
1990.
Scholarly Communica-tion and Bibliometrics.
Sage Publications, CA.Jean Carletta.
1996.
Assessing agreement on classificationtasks: The kappa statistic.
Computational Linguistics,22(2):249?254.Daryl E. Chubin and S. D. Moitra.
1975.
Content analysisof references: Adjunct or alternative to citation counting?Social Studies of Science, 5(4):423?441.Eugene Garfield.
1979.
Citation Indexing: Its Theory andApplication in Science, Technology and Humanities.
J.Wiley, New York, NY.C.
Lee Giles, Kurt D. Bollacker, and Steve Lawrence.
1998.Citeseer: An automatic citation indexing system.
In Proc.of the Third ACM Conference on Digital Libraries, pages89?98.T.L.
Hodges.
1972.
Citation Indexing: Its Potential for Bibli-ographical Control.
Ph.D. thesis, University of Californiaat Berkeley.David D. Lewis.
1991.
Evaluating text categorisation.
InSpeech and Natural Language: Proceedings of the ARPAWorkshop of Human Language Technology.109Context Human Machine CommentWe have compared four complete and three partial data rep-resentation formats for the baseNP recognition task pre-sented in Ramshaw and Marcus (1995).PUse PUse Cues can be weak: ?for... task...presented in?In the version of the algorithm that we have used, IB1-IG,the distances between feature representations are computedas the weighted sum of distances between individual features(Bosch 1998).Neut PUse Human decided citation was fordetail in used package, not di-rectly used by paper.We have used the baseNP data presented in Ramshaw andMarcus (1995).
PUse PUse Straightforward caseWe will follow Argamon et al (1998) and usea combination of the precision and recall rates:F=(2*precision*recall)/(precision+recall).PSim PUse Human decided F-measure wasnot attributable to citation.
Hencesimilarity rather than usage.This algorithm standardly uses the single training item clos-est to the test i.e.
However Daelemans et al (1999) reportthat for baseNP recognition better results can be obtained bymaking the algorithm consider the classification values of thethree closest training items.Neut PUse Shallow processing by Machinemeans that it is mislead by thestrong cue in preceding sentence.They are better than the results for section 15 because moretraining data was used in these experiments.
Again thebest result was obtained with IOB1 (F=92.37) which is animprovement of the best reported F-rate for this data setRamshaw and Marcus 1995 (F=92.03).CoCo- PUse Machine attached citation to thedata set being used.
Human at-tached citation to the result beingcompared.Figure 7: Examples of classifications by the machine learner.Terttu Luukkonen.
1992.
Is scientists?
publishing behaviourreward-seeking?
Scientometrics, 24:297?319.Michael H. MacRoberts and Barbara R. MacRoberts.
1984.The negational reference: Or the art of dissembling.
So-cial Studies of Science, 14:91?94.Michael J. Moravcsik and Poovanalingan Murugesan.
1975.Some results on the function and quality of citations.
So-cial Studies of Science, 5:88?91.Greg Myers.
1992.
In this paper we report...?speech actsand scientific facts.
Journal of Pragmatics, 17(4).John O?Connor.
1982.
Citing statements: Computer recogni-tion and use to improve retrieval.
Information Processingand Management, 18(3):125?131.Chris D. Paice.
1981.
The automatic generation of literaryabstracts: an approach based on the identification of self-indicating phrases.
In R. Oddy, S. Robertson, C. van Rijs-bergen, and P. W. Williams, editors, Information RetrievalResearch.
Butterworth, London, UK.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002.Thumbs up?
Sentiment classification using machinelearning techniques.
In Proc.
of EMNLP-02.Anna Ritchie, Simone Teufel, and Stephen Robertson.2006a.
Creating a test collection for citation-based IR ex-periments.
In Proc.
of HLT/NAACL 2006, New York, US.Anna Ritchie, Simone Teufel, and Stephen Robertson.2006b.
How to find better index terms through citations.In Proc.
of ACL/COLING workshop ?Can ComputationalLinguistics improve IR?.Simon Buckingham Shum.
1998.
Evolving the web for sci-entific knowledge: First steps towards an ?HCI knowledgeweb?.
Interfaces, British HCI Group Magazine, 39.Henry Small.
1982.
Citation context analysis.
In P. Dervinand M. J. Voigt, editors, Progress in Communication Sci-ences 3, pages 287?310.
Ablex, Norwood, N.J.Ina Spiegel-Ru?sing.
1977.
Bibliometric and content analy-sis.
Social Studies of Science, 7:97?113.John Swales.
1986.
Citation analysis and discourse analysis.Applied Linguistics, 7(1):39?56.John Swales, 1990.
Genre Analysis: English in Academicand Research Settings.
Chapter 7: Research articles inEnglish, pages 110?176.
Cambridge University Press,Cambridge, UK.Simone Teufel and Marc Moens.
2002.
Summarising scien-tific articles ?
experiments with relevance and rhetoricalstatus.
Computational Linguistics, 28(4):409?446.Simone Teufel, Advaith Siddharthan, and Dan Tidhar.
2006.An annotation scheme for citation function.
In Proc.
ofSIGDial-06.Simone Teufel.
1999.
Argumentative Zoning: InformationExtraction from Scientific Text.
Ph.D. thesis, School ofCognitive Science, University of Edinburgh, UK.Peter D. Turney.
2002.
Thumbs up or thumbs down?
se-mantic orientation applied to unsupervised classificationof reviews.
In Proc.
of ACL-02.Melvin Weinstock.
1971.
Citation indexes.
In Encyclope-dia of Library and Information Science, volume 5.
Dekker,New York, NY.Howard D. White.
2004.
Citation analysis and discourseanalysis revisited.
Applied Linguistics, 25(1):89?116.Ian H. Witten and Eibe Frank.
2005.
Data Mining: Practi-cal machine learning tools and techniques.
Morgan Kauf-mann, San Francisco.Yiming Yang and Xin Liu.
1999.
A re-examination of textcategorization methods.
In Proc.
of SIGIR-99.John M. Ziman.
1968.
Public Knowledge: An Essay Con-cerning the Social Dimensions of Science.
CambridgeUniversity Press, Cambridge, UK.110
