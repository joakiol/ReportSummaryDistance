BioNLP 2007: Biological, translational, and clinical language processing, pages 1?8,Prague, June 2007. c?2007 Association for Computational LinguisticsSyntactic complexity measures for detecting Mild Cognitive ImpairmentBrian Roark, Margaret Mitchell and Kristy HollingsheadCenter for Spoken Language UnderstandingOGI School of Science & EngineeringOregon Health & Science UniversityBeaverton, Oregon, 97006 USA{roark,meg.mitchell,hollingk}@cslu.ogi.eduAbstractWe consider the diagnostic utility of vari-ous syntactic complexity measures when ex-tracted from spoken language samples ofhealthy and cognitively impaired subjects.We examine measures calculated from man-ually built parse trees, as well as the samemeasures calculated from automatic parses.We show statistically significant differencesbetween clinical subject groups for a num-ber of syntactic complexity measures, andthese differences are preserved with auto-matic parsing.
Different measures show dif-ferent patterns for our data set, indicatingthat using multiple, complementary mea-sures is important for such an application.1 IntroductionNatural language processing (NLP) techniques areoften applied to electronic health records and otherclinical datasets.
Another potential clinical use ofNLP is for processing patient language samples,which can be used to assess language development(Sagae et al, 2005) or the impact of neurodegenera-tive impairments on speech and language (Roark etal., 2007).
In this paper, we present methods for au-tomatically measuring syntactic complexity of spo-ken language samples elicited during neuropsycho-logical exams of elderly subjects, and examine theutility of these measures for discriminating betweenclinically defined groups.Mild Cognitive Impairment (MCI), and in par-ticular amnestic MCI, the earliest clinically de-fined stage of Alzheimer?s-related dementia, oftengoes undiagnosed due to the inadequacy of com-mon screening tests such as the Mini-Mental StateExamination (MMSE) for reliably detecting rela-tively subtle impairments.
Linguistic memory tests,such as word list and narrative recall, are more ef-fective than the MMSE in detecting MCI, yet arestill individually insufficient for adequate discrimi-nation between healthy and impaired subjects.
Be-cause of this, a battery of examinations is typicallyused to improve psychometric classification.
Yet thesummary recall scores derived from these linguisticmemory tests (total correctly recalled) ignore poten-tially useful information in the characteristics of thespoken language itself.Narrative retellings provide a natural, conversa-tional speech sample that can be analyzed for manyof the characteristics of speech and language thathave been shown to discriminate between healthyand impaired subjects, including syntactic complex-ity (Kemper et al, 1993; Lyons et al, 1994) andmean pause duration (Singh et al, 2001).
Thesemeasures go beyond simply measuring fidelity tothe narrative, thus providing key additional dimen-sions for improved diagnosis of impairment.
Recentwork (Roark et al, 2007) has shown significant dif-ferences between healthy and MCI groups for bothpause related and syntactic complexity measures de-rived from transcripts and audio of narrative recalltests.
In this paper, we look more closely at syntac-tic complexity measures.There are two key considerations when choos-ing how to measure syntactic complexity of spokenlanguage samples for the purpose of psychometricevaluation.
First and most importantly, the syntacticcomplexity measures will be used for discriminationbetween groups, hence high discriminative utility isdesired.
It has been demonstrated in past studies(Cheung and Kemper, 1992) that many competingmeasures are in fact very highly correlated, so it maybe the case that many measures are equally discrimi-native.
For this reason, previous results (Roark et al,2007) have focused on a single syntactic complexitymetric, that of Yngve (1960).A second key consideration, however, is the fi-delity of the measure when derived from transcriptsvia automatic parsing.
Different syntactic complex-ity measures rely on varying levels of detail from1the parse tree.
Some syntactic complexity measures,such as that of Yngve (1960), make use of unla-beled tree structures to derive their scores; others,such as that of Frazier (1985), rely on labels withinthe tree, in addition to the tree structure, to pro-vide the scores.
Given these different uses of detail,some measures may be less reliable with automa-tion, hence dis-preferred in the context of automatedevaluation.
Ideally, simple, easy-to-automate mea-sures with high discriminative utility are preferred.In the current paper, we demonstrate that varioussyntactic complexity measures capture complemen-tary systematic differences between subject groups,suggesting that the best approach to discriminatingbetween healthy and impaired subjects is to collectvarious measures, as a way of capturing language?signatures?
of the impairment.For many measures of syntactic complexity, thenature of the syntactic annotation is critical ?
differ-ent conventions of structural annotation will yielddifferent scores.
We will thus spend the next sec-tion briefly detailing the syntactic annotation con-ventions that were followed for this work.
This isfollowed by a section describing a range of complex-ity measures to be derived from these annotations.Finally, we present empirical results on the samplesof spoken narrative retellings.2 Syntactic annotationFor manual syntactic annotation of collected data(see Section 4), we followed the syntactic annota-tion conventions of the well-known Penn Treebank(Marcus et al, 1993).
This provides several key ben-efits.
First, there is an extensive annotation guidethat has been developed, not just for written but alsofor spoken language, so that consistent annotationwas facilitated.
Second, the large out-of-domaincorpora, in particular the 1 million words of syn-tactically annotated Switchboard telephone conver-sations, provide a good starting point for trainingdomain adapted parsing models.
Finally, we can usemultiple domains for evaluating the correlations be-tween various syntactic complexity measures.There are characteristics of Penn Treebank anno-tation that can impact syntactic complexity scoring.First, prenominal modifiers are typically grouped ina flat constituent with no internal structure.
This an-notation choice can result in very long noun phrases(NPs) which pose very little difficulty in terms ofhuman processing performance, but can inflate com-plexity measures that measure deviation from right-branching structures, such as that of Yngve (1960).Second, in spoken language annotations, a reparan-dum1 is denoted with a special non-terminal cate-gory EDITED.
For this paper, we remove from thetree these non-terminals, and the structures under-neath them, prior to evaluating syntactic complexity.3 Syntactic complexityThere is no single agreed-upon measurement ofsyntactic complexity.
A range of measures havebeen proposed, with different primary considera-tions driving the notion of complexity for each.Many measures focus on the order in which vari-ous constructions are acquired by children learningthe syntax of their native language ?
later acquisi-tions being taken as higher complexity.
Examplesof this sort of complexity measure are: mean lengthof utterance (MLU), which is typically measuredin morphemes (Miller and Chapman, 1981); theIndex of Productive Syntax (Scarborough, 1990),a multi-point scale which has recently been auto-mated for child-language transcript analysis (Sagaeet al, 2005); and Developmental Level (Rosenbergand Abbeduto, 1987), a 7-point scale of complex-ity based on the presence of specific grammaticalconstructions.
Other approaches have relied uponthe right-branching nature of English syntactic trees(Yngve, 1960; Frazier, 1985), under the assump-tion that deviations from that correspond to morecomplexity in the language.
Finally, there are ap-proaches focused on the memory demands imposedby ?distance?
between dependent words (Lin, 1996;Gibson, 1998).3.1 Yngve scoringThe scoring approach taken in Yngve (1960) is re-lated to the size of a ?first in/last out?
stack at eachword in a top-down, left-to-right parse derivation.Consider the tree in Figure 1.
If we knew exactlywhich productions to use, the parse would beginwith an S category on the stack and advance asfollows: pop the S and push VP and NP onto thestack; pop NP and push PRP onto the stack; popPRP from the stack; pop VP from the stack andpush NP and VBD onto the stack; and so on.
Atthe point when the word ?she?
is encountered, onlyVP remains on the stack of the parser.
When ?was?1A reparandum is a sequence of words that are aborted bythe speaker, then repaired within the same utterance.2S1 0NP0VP1 0PRP VBD NP1 0NP1 0PP1 0DT NN IN NP2 1 0DT NN NNYngve score:she1 was1 a2 cook1 in1 a2 school1 cafeteria01Figure 1: Parse tree with branch scores for Yngve scoring.is reached, just NP is on the stack.
Thus, the Yn-gve score for these two words is 1.
When the nextword ?a?
is reached, however, there are two cate-gories on the stack: PP and NN, so this word re-ceives an Yngve score of 2.
Stack size has been re-lated by some (Resnik, 1992) to working memorydemands, although it most directly measures devia-tion from right-branching trees.To calculate the size of the stack at each word,we can use the following simple algorithm.
At eachnode in the tree, label the branches from that nodeto each of its children, beginning with zero at therightmost child and continuing to the leftmost child,incrementing the score by one for each child.
Hence,each rightmost branch in the tree of Figure 1 is la-beled with 0, the leftmost branch in all binary nodesis labeled with 1, and the leftmost branch in theternary node is labeled with 2.
Then the score foreach word is the sum of the branch scores from theroot of the tree to the word.Given the score for each word, we can then de-rive an overall complexity score by summing themor taking the maximum or mean.
For this paper,we report mean scores for this and other word-basedmeasures, since we have found these means to pro-vide better performing scores than either total sumor maximum.
For the tree in Figure 1, the maximumis 2, the total is 9 and the mean over 8 words is 118 .3.2 Frazier scoringFrazier (1985) proposed an approach to scoring syn-tactic complexity that traces a path from a word upthe tree until reaching either the root of the tree orthe lowest node which is not the leftmost child of itsparent.2 For example, Figure 2 shows the tree from2An exception is made for empty subject NPs, in which caseS1.5NP1VP1PRP VBD NP1NP1PP1DT NN IN NP1DT NN NNFrazier score:she2.5was1a2cook0in1a1school0cafeteria01Figure 2: Parse tree fragments with scores for Frazier scoring.Figure 1 broken into distinct paths for each wordin the string.
The first word has a path up to theroot, while the second word just up to the VP, sincethe VP has an NP sibling to its left.
The word isthen scored, as in the Yngve measure, by summingthe scores on the links along the path.
Each non-terminal node in the path contributes a score of 1,except for sentence nodes and sentence-complementnodes,3 which score 1.5 rather than 1.
Thus em-bedded clauses contribute more to the complexitymeasure than other embedded categories, as an ex-plicit acknowledgment of sentence embeddings as asource of syntactic complexity.As with the Yngve score, we can calculate thetotal and the mean of these word scores.
In con-trast to the maximum score calculated for the Yngvemeasure, Frazier proposed summing the word scoresfor each 3-word sequence in the sentence, then tak-ing the maximum of these sums as a measure ofhighly-localized concentrations of grammatical con-stituents.
For the example in Figure 2, the maximumis 2.5, the maximum 3-word sum is 5.5, and the totalis 7.5, yielding a mean of 1516 .3.3 Dependency distanceRather than examining the tree structure itself, onemight also extract measures from lexical depen-dency structures.
These dependencies can be de-rived from the tree using standard rules for estab-lishing head children for constituents, originally at-the succeeding verb receives an additional score of 1 (for thedeleted NP), and its path continues up the tree.
Empty NPs areannotated in our manual parse trees but not in the automaticparses, which may result in a small disagreement in the Frazierscores for manual and automatic trees.3Every non-terminal node beginning with an S, includingSQ and SINV, were counted as sentence nodes.
Sequences ofsentence nodes, i.e.
an SBAR appearing directly under an Snode, were only counted as a single sentence node and thus onlycontributed to the score once.3she was a cook in a school cafeteria1Figure 3: Dependency graph for the example string.tributed to Magerman (1995), to percolate lexicalheads up the tree.
Figure 3 shows the dependencygraph that results from this head percolation ap-proach, where each link in the graph represents a de-pendency relation from the modifier to the head.
Forexample, conventional head percolation rules spec-ify the VP as the head of the S, so ?was?, as the headof the VP, is thus the lexical head of the entire sen-tence.
The lexical heads of the other children of theS node are called modifiers of the head of the S node;thus, since ?she?
is the head of the subject NP, thereis a dependency relation between ?she?
and ?was?.Lin (1996) argued for the use of this sort of depen-dency structure to measure the difficulty in process-ing, given the memory overhead of very long dis-tance dependencies.
Both Lin (1996) and Gibson(1998) showed that human performance on sentenceprocessing tasks could be predicted with measuresof this sort.
While details may differ ?
e.g., howto measure distance, what counts as a dependency ?we can make use of the general approach given Tree-bank style parses and head percolation, resulting ingraphs of the sort in Figure 3.
For the current paper,we count the distance between words for each de-pendency link.
For Figure 3, there are 7 dependencylinks, a distance total of 11, and a mean of 147 .3.4 Developmental level (D-Level)D-Level defines eight levels of sentence complex-ity, from 0-7, based on the development of complexsentences in normal-development children.
Eachlevel is defined by the presence of specific grammat-ical constructions (Rosenberg and Abbeduto, 1987);we follow Cheung and Kemper (1992) in assigningscores equivalent to the defined level of complex-ity.
A score of zero corresponds to simple, single-clause sentences; embedded infinitival clauses geta score of 1 (She needs to pay the rent); conjoinedclauses (She worked all day and worried all night),compound subjects (The woman and her four chil-dren had not eaten for two days), and wh-predicatecomplements score 2.
Object noun phrase rela-tive clauses or complements score 3 (The policecaught the man who robbed the woman), whereasthe same constructs in subject noun phrases score5 (The woman who worked in the cafeteria wasrobbed).
Gerundive complements and comparatives(They were hungrier than her) receive a score of 4;subordinating conjunctions (if, before, as soon as)score 6.
Finally, a score of 7 is used as a catch-allcategory for sentences containing more than one ofany of these grammatical constructions.3.5 POS-tag sequence cross entropyOne possible approach for detecting rich syntacticstructure is to look for infrequent or surprising com-binations of parts-of-speech (POS).
We can measurethis over an utterance by building a simple bi-grammodel over POS tags, then measuring the cross en-tropy of each utterance.4Given a bi-gram model over POS-tags, we cancalculate the probability of the sequence as a whole.Let ?i be the POS-tag of word wi in a sequence ofwordsw1 .
.
.
wn, and assume that ?0 is a special startsymbol, and that ?n+1 is a special stop symbol.
Thenthe probability of the POS-tag sequence isP(?1 .
.
.
?n) =n+1?i=1P(?i | ?i?1) (1)The cross entropy is then calculated asH(?1 .
.
.
?n) = ?1nlog P(?1 .
.
.
?n) (2)With this formulation, this basically boils down tothe mean negative log probability of each tag giventhe previous tag.4 Data4.1 SubjectsWe collected audio recordings of 55 neuropsycho-logical examinations administered at the Layton Ag-ing & Alzheimer?s Disease Center, an NIA-fundedAlzheimer?s center for research at OHSU.
For thisstudy, we partitioned subjects into two groups: thosewho were assigned a Clinical Dementia Rating(CDR) of 0 (healthy) and those who were assigneda CDR of 0.5 (Mild Cognitive Impairment; MCI).The CDR (Morris, 1993) is assigned with access toclinical and cognitive test information, independentof performance on the battery of neuropsychologi-cal tests used for this research study, and has beenshown to have high expert inter-annotator reliability(Morris et al, 1997).4For each test domain, we used cross-validation techniquesto build POS-tag bi-gram models and evaluate with them in thatdomain.4CDR = 0 CDR = 0.5(n=29) (n=18)Measure M SD M SD t(45)Age 88.1 9.0 91.9 4.4 ?1.65Education (Y) 15.0 2.2 14.3 2.8 1.04MMSE 28.4 1.4 25.9 2.6 4.29***Word List (A) 20.0 4.0 15.4 3.3 4.06***Word List (R) 6.8 2.0 3.9 1.7 5.12***Wechsler LM I 17.2 4.0 10.9 4.2 5.20***Wechsler LM II 15.8 4.3 9.5 5.4 4.45***Cat.Fluency (A) 17.2 4.1 13.9 4.2 2.59*Cat.Fluency (V) 12.8 4.5 9.6 3.6 2.57*Digits (F) 6.6 1.4 6.1 1.2 1.11Digits (B) 4.7 1.0 4.7 1.1 ?0.04Table 1: Neuropsychological test results for subjects.
***p < 0.001; **p < 0.01 ; *p < 0.05Of the collected recordings, three subjects wererecorded twice; for the current study only onerecording was used for each subject.
Three subjectswere assigned a CDR of 1.0 and were excluded fromthe study; two further subjects were excluded for er-rors in the recording that resulted in missing audio.Of the remaining 47 subjects, 29 had CDR = 0, and18 had CDR = 0.5.4.2 Neuropsychological testsTable 1 presents means and standard deviations forage, years of education and the manually-calculatedscores of a number of standard neuropsychologicaltests that were administered during the recorded ses-sion.
These tests include: the Mini Mental State Ex-amination (MMSE); the CERAD Word List Acqui-sition (A) and Recall (R) tests; the Wechsler LogicalMemory (LM) I (immediate) and II (delayed) narra-tive recall tests; Category Fluency, Animals (A) andVegetables (V); and Digit Span (WAIS-R) forward(F) and backward (B).The Wechsler Logical Memory I/II tests are thebasis of our study on syntactic complexity measures.The original narrative is a short, 3 sentence story:Anna Thompson of South Boston, employed as a cook in aschool cafeteria, reported at the police station that she hadbeen held up on State Street the night before and robbed offifty-six dollars.
She had four small children, the rent wasdue, and they had not eaten for two days.
The police, touchedby the woman?s story, took up a collection for her.Subjects are asked to re-tell this story immediatelyafter it is told to them (LM I), as well as after ap-proximately 30 minutes of unrelated activities (LMII).
We transcribed each retelling, and manually an-notated syntactic parse trees according to the PennTreebank annotation guidelines.
Algorithms for au-tomatically extracting syntactic complexity markersfrom parse trees were written to accept either man-System LR LP F-measureOut-of-domain (WSJ) 77.7 80.1 78.9Out-of-domain (SWBD) 84.0 86.2 85.1Domain adapted from SWBD 87.9 88.3 88.1Table 2: Parser accuracy on Wechsler Logical Memory re-sponses using just out-of-domain data (either from the Wall St.Journal (WSJ) or Switchboard (SWBD) treebanks) versus usinga domain adapted system.ually annotated trees or trees output from an auto-matic parser, to demonstrate the plausibility of usingautomatically generated parse trees.4.3 ParsingFor automatic parsing, we made use of the well-known Charniak parser (Charniak, 2000).
Followingbest practices (Charniak and Johnson, 2001), we re-moved sequences covered by EDITED nodes in thetree from the strings prior to parsing.
For this pa-per, EDITED nodes were identified from the manualparse, not automatically.
Table 2 shows parsing ac-curacy of our annotated retellings under three pars-ing model training conditions: 1) trained on approx-imately 1 million words of Wall St. Journal (WSJ)text; 2) trained on approximately 1 million wordsof Switchboard (SWBD) corpus telephone conver-sations; and 3) using domain adaptation techniquesstarting from the SWBD Treebank.
The SWBD out-of-domain system reaches quite respectable accura-cies, and domain adaptation achieves 3 percent ab-solute improvement over that.For domain adaptation, we used MAP adapta-tion techniques (Bacchiani et al, 2006) via cross-validation over the entire set of retellings.
Foreach subject, we trained a model using the SWBDtreebank as the out-of-domain treebank, and theretellings of the other 46 subjects as in-domain train-ing.
We used a count merging approach, with thein-domain counts scaled by 1000 relative to the out-of-domain counts.
See Bacchiani et al (2006) formore information on stochastic grammar adaptationusing these techniques.5 Experimental results5.1 CorrelationsOur first set of experimental results regard correla-tions between measures.
Table 3 shows results forfive of our measures over all three treebanks that wehave been considering: Penn WSJ Treebank, PennSWBD Treebank, and the Wechsler LM retellings.The correlations along the diagonal are between thesame measure when extracted from manually an-notated trees and when extracted from automatic5Penn WSJ Treebank Penn SWBD Treebank Wechsler LM Retellings(a) (b) (c) (d) (e) (a) (b) (c) (d) (e) (a) (b) (c) (d) (e)(a) Frazier 0.89 0.96 0.94(b) Yngve -0.31 0.96 -0.72 0.96 -0.69 0.95(c) Tree nodes 0.91 -0.16 0.92 0.58 -0.06 0.93 0.93 -0.48 0.85(d) Dep len -0.29 0.75 -0.13 0.93 -0.74 0.97 -0.08 0.96 -0.72 0.96 -0.51 0.96(e) Cross Ent 0.17 0.18 0.15 0.19 0.93 -0.55 0.76 0.09 0.76 0.98 -0.13 0.45 0.05 0.41 0.97Table 3: Correlation matrices for several measures on an utterance-by-utterance basis.
Correlations along the diagonal are betweenthe manual measures and the measures when automatically parsed.
All other correlations are between measures when derived frommanual parse trees.parses.
All other correlations are between mea-sures derived from manual trees.
All correlationsare taken per utterance.From this table, we can see that all of the mea-sures derived from automatic parses have a highcorrelation with the manually derived measures, in-dicating that they may preserve any discriminativeutility of these markers.
Interestingly, the num-ber of nodes in the tree per word tends to corre-late well with the Frazier score, while the depen-dency length tends to correlate well with the Yngvescore.
Cross entropy correlates with Yngve and de-pendency length for the SWBD and Wechsler tree-banks, but not for the WSJ treebank.5.2 Manually derived measuresTable 4 presents means and standard deviationsfor measures derived from the LM I and LM IIretellings, along with the t-value and level of sig-nificance.
The first three measures presented in thetable are available without syntactic annotation: to-tal number of words, total number of utterances, andwords per utterance in the retelling.
None of thesethree measures on either retelling show statisticallysignificant differences between the groups.The first measure to rely upon syntactic annota-tions is words per clause.
The number of clauses areautomatically extracted from the parses by countingthe number of S nodes in the tree.5 Normalizing thenumber of words by the number of clauses ratherthan the number of utterances (as in words per ut-terance) results in statistically significant differencesbetween the groups for LM I though not for LM II.The other measures are as described in Section3.
Interestingly, Frazier score per word, the numberof tree nodes per word, and POS-tag cross entropyall show a significant negative t-value on the LM Iretellings, meaning the CDR 0.5 subjects had sig-nificantly higher scores than the CDR 0 subjects for5For coordinated S nodes, the root of the coordination,which in Penn Treebank style annotation also has an S label,does not count as an additional clause.these measures on this task.
These measures showedno significant difference on the LM II retellings.The Yngve score per word and the dependencylength per word showed no significant difference onLM I retellings but a statistically significant differ-ence on LM II, with the expected outcome of higherscores for the CDR 0 subjects.
The D-Level measureshowed no significant differences.5.3 Automatically derived measuresIn addition to manual-parse derived measures, Table4 also presents the same measures when automatic,rather than manual, parses are used.
Given the rela-tively high quality of the automatic parses, most ofthe means and standard deviations are quite close,and all of the patterns observed in the upper half ofTable 4 are preserved, except that the Yngve scoreper word no longer shows a statistically significantdifference for the LM II retelling.5.4 Left-corner treesFor the tree-based complexity metrics (Frazier andYngve), we also investigated alternative imple-mentations that make use of the left-corner trans-formation (Rosenkrantz and Lewis II, 1970) ofthe tree from which the measures were extracted.This transformation is widely known for remov-ing left-recursion from a context-free grammar, andit changes the tree shape by transforming left-branching structures into right-branching structures,while leaving center-embedded structures center-embedded.
This property led Resnik (1992) to pro-pose left-corner processing as a plausible mecha-nism for human sentence processing, since it is pre-cisely these center-embedded structures, and not theleft- or right-branching structures, that are problem-atic for humans to process.Table 5 presents results using either manually an-notated trees or automatic parses to extract the Yn-gve and Frazier measures after a left-corner trans-form has been applied to the tree.
The Frazierscores are very similar to those without the left-6Logical Memory I Logical Memory IICDR = 0 CDR = 0.5 CDR = 0 CDR = 0.5Measure M SD M SD t(45) M SD M SD t(45)Total words in retelling 71.0 26.0 58.1 31.9 1.49 70.6 21.5 58.5 36.7 1.43Total utterances in retelling 8.86 4.16 7.72 3.28 0.99 8.17 2.77 7.06 4.86 1.01Words per utterance in retelling 8.57 2.44 7.78 3.67 0.89 9.16 3.06 7.82 4.76 1.18Manually extracted: Words per clause 6.33 1.39 5.25 1.25 2.68* 6.12 1.20 5.48 3.37 0.93Frazier score per word 1.19 0.09 1.26 0.11 ?2.68* 1.19 0.09 1.13 0.43 0.67Tree nodes per word 1.96 0.07 2.01 0.10 ?2.08* 1.96 0.07 1.80 0.66 1.36Yngve score per word 1.44 0.23 1.39 0.30 0.61 1.53 0.27 1.26 0.62 2.01*Dependency length per word 1.54 0.25 1.47 0.27 0.90 1.63 0.30 1.34 0.60 2.19*POS-tag Cross Entropy 1.83 0.16 1.96 0.26 ?2.18* 1.93 0.14 1.86 0.59 0.54D-Level 1.07 0.75 1.03 1.23 0.14 1.23 0.81 1.68 1.41 ?1.42Auto extracted: Words per clause 6.42 1.53 5.10 1.16 3.13** 6.04 1.25 5.61 3.67 0.59Frazier score per word 1.16 0.10 1.24 0.10 ?2.92** 1.15 0.10 1.09 0.41 0.69Tree nodes per word 1.96 0.07 2.03 0.10 ?2.55* 1.96 0.08 1.79 0.66 1.38Yngve score per word 1.41 0.23 1.37 0.29 0.54 1.50 0.27 1.28 0.64 1.70Dependency length per word 1.51 0.25 1.47 0.28 0.54 1.61 0.28 1.35 0.61 2.04*POS-tag Cross Entropy 1.83 0.17 1.96 0.26 ?2.12* 1.92 0.14 1.86 0.58 0.53D-Level 1.09 0.73 1.11 1.20 ?0.08 1.28 0.77 1.61 1.22 ?1.15Table 4: Syntactic complexity measure group differences when measures are derived from either manual or automatic parse trees.
**p < 0.01 ; *p < 0.05corner transform, while the Yngve scores are re-duced across the board.
With the left-corner trans-formed tree, the automatically derived Yngve mea-sure retains the statistically significant differenceshown by the manually derived measure.6 Discussion and future directionsThe results presented in the last section demonstratethat NLP techniques applied to clinically elicitedspoken language samples can be used to automat-ically derive measures that may be useful for dis-criminating between healthy and MCI subjects.
Inaddition, we see that different measures show differ-ent patterns when applied to these language samples,with Frazier scores and tree nodes per word givingquite different results than Yngve scores and depen-dency length.
It would thus appear that, for PennTreebank style annotations at least, these measuresare quite complementary.There are two surprising aspects of these results:the significantly higher means of three measures onLM I samples for MCI subjects, and the fact that oneset of measures show significant differences on LMI while another shows differences on LM II.
We donot have definitive explanations for these phenom-ena, but we can speculate about why such resultswere obtained.First, there is an important difference between themanner of elicitation for LM I versus LM II.
LM Iis an immediate recall, so there will likely be, forunimpaired subjects, much higher verbatim recall ofthe story than in the delayed recall of LM II.
Forthe MCI group, which exhibits memory impairment,there will be little in the way of verbatim recall, andpotentially much more in the way of spoken lan-guage phenomena such as filled pauses, parenthet-icals and off-topic utterances.
This may account forthe higher Frazier score per word for the MCI groupon LM I.
Such differences will likely be lessened inthe delayed recall.Second, the Frazier and Yngve metrics differ inhow they score long, flat phrases, such as typicalbase NPs.
Consider the ternary NP in Figure 1.
Thefirst word in that NP (?a?)
receives an Yngve scoreof 2, but a Frazier score of only 1 (Figure 2), whilethe second word in the NP receives an Yngve scoreof 1 and a Frazier score of 0.
For a flat NP with5 children, that difference would be 4 to 1 for thefirst child, 3 to 0 for the second child, and so forth.This difference in scoring relatively common syn-tactic constructions, even those which may not affecthuman memory load, may account for such differentscores achieved with these different measures.In summary, we have demonstrated an importantclinical use for NLP techniques, where automaticsyntactic annotation provides sufficiently accurateparse trees for use in automatic extraction of syntac-tic complexity measures.
Different syntactic com-plexity measures appear to be measuring quite com-plementary characteristics of the retellings, yieldingstatistically significant differences from both imme-diate and delayed retellings.There are quite a number of questions that we will7Logical Memory I Logical Memory IICDR = 0 CDR = 0.5 CDR = 0 CDR = 0.5Measure M SD M SD t(45) M SD M SD t(45)Manually extracted: Left-corner Frazier 1.20 0.10 1.28 0.12 ?2.60* 1.20 0.11 1.18 0.45 0.29Left-corner Yngve 1.33 0.20 1.25 0.23 1.20 1.37 0.21 1.14 0.52 2.14*Auto extracted: Left-corner Frazier 1.16 0.10 1.27 0.13 ?3.02** 1.15 0.11 1.10 0.42 0.64Left-corner Yngve 1.31 0.19 1.23 0.21 1.33 1.36 0.21 1.13 0.51 2.11*Table 5: Syntactic complexity measure group differences when measures are derived from left-corner parse trees.
**p < 0.01 ; *p < 0.05continue to pursue.
Most importantly, we will con-tinue to examine this data, to try to determine whatcharacteristics of the spoken language are leading tothe unexpected patterns in the results.
In addition,we will begin to explore composite measures, suchas differences in measures between LM I and LM II,which promise to better capture some of the patternswe have observed.
Ultimately, we would like tobuild classifiers making use of a range of measuresas features, although in order to demonstrate statisti-cally significant differences between classifiers, wewill need much more data than we currently have.Eventually, longitudinal tracking of subjects may bethe best application of such measures on clinicallyelicited spoken language samples.AcknowledgmentsThis research was supported in part by NSF Grant #IIS-0447214 and pilot grants from the Oregon Center for Aging& Technology (ORCATECH, NIH #1P30AG024978-01) andthe Oregon Partnership for Alzheimer?s Research.
Also, thethird author of this paper was supported under an NSF Grad-uate Research Fellowship.
Any opinions, findings, conclusionsor recommendations expressed in this publication are those ofthe authors and do not necessarily reflect the views of the NSF.Thanks to Jeff Kaye, John-Paul Hosom, Jan van Santen, TracyZitzelberger, Jessica Payne-Murphy and Robin Guariglia forhelp with the project.ReferencesM.
Bacchiani, M. Riley, B. Roark, and R. Sproat.
2006.
MAPadaptation of stochastic grammars.
Computer Speech andLanguage, 20(1):41?68.E.
Charniak and M. Johnson.
2001.
Edit detection and parsingfor transcribed speech.
In Proceedings of the 2nd Confer-ence of the North American Chapter of the ACL.E.
Charniak.
2000.
A maximum-entropy-inspired parser.
InProceedings of the 1st Conference of the North AmericanChapter of the ACL, pages 132?139.H.
Cheung and S. Kemper.
1992.
Competing complexity met-rics and adults?
production of complex sentences.
AppliedPsycholinguistics, 13:53?76.L.
Frazier.
1985.
Syntactic complexity.
In D.R.
Dowty,L.
Karttunen, and A.M. Zwicky, editors, Natural LanguageParsing.
Cambridge University Press, Cambridge, UK.E.
Gibson.
1998.
Linguistic complexity: locality of syntacticdependencies.
Cognition, 68(1):1?76.S.
Kemper, E. LaBarge, F.R.
Ferraro, H. Cheung, H. Cheung,and M. Storandt.
1993.
On the preservation of syntax inAlzheimer?s disease.
Archives of Neurology, 50:81?86.D.
Lin.
1996.
On structural complexity.
In Proceedings ofCOLING-96.K.
Lyons, S. Kemper, E. LaBarge, F.R.
Ferraro, D. Balota, andM.
Storandt.
1994.
Oral language and Alzheimer?s disease:A reduction in syntactic complexity.
Aging and Cognition,1(4):271?281.D.M.
Magerman.
1995.
Statistical decision-tree models forparsing.
In Proceedings of the 33rd Annual Meeting of theACL, pages 276?283.M.P.
Marcus, B. Santorini, and M.A.
Marcinkiewicz.
1993.Building a large annotated corpus of English: The PennTreebank.
Computational Linguistics, 19(2):313?330.J.F.
Miller and R.S.
Chapman.
1981.
The relation betweenage and mean length of utterance in morphemes.
Journal ofSpeech and Hearing Research, 24:154?161.J.
Morris, C. Ernesto, K. Schafer, M. Coats, S. Leon, M. Sano,L.
Thal, and P. Woodbury.
1997.
Clinical dementiarating training and reliability in multicenter studies: TheAlzheimer?s disease cooperative study experience.
Neurol-ogy, 48(6):1508?1510.J.
Morris.
1993.
The clinical dementia rating (CDR): Currentversion and scoring rules.
Neurology, 43:2412?2414.P.
Resnik.
1992.
Left-corner parsing and psychological plausi-bility.
In Proceedings of COLING-92, pages 191?197.B.
Roark, J.P. Hosom, M. Mitchell, and J.A.
Kaye.
2007.
Au-tomatically derived spoken language markers for detectingmild cognitive impairment.
In Proceedings of the 2nd Inter-national Conference on Technology and Aging (ICTA).S.
Rosenberg and L. Abbeduto.
1987.
Indicators of linguis-tic competence in the peer group conversational behavior ofmildly retarded adults.
Applied Psycholinguistics, 8:19?32.S.J.
Rosenkrantz and P.M. Lewis II.
1970.
Deterministic leftcorner parsing.
In IEEE Conference Record of the 11th An-nual Symposium on Switching and Automata, pages 139?152.K.
Sagae, A. Lavie, and B. MacWhinney.
2005.
Automaticmeasurement of syntactic development in child langugage.In Proceedings of the 43rd Annual Meeting of the ACL.H.S.
Scarborough.
1990.
Index of productive syntax.
AppliedPsycholinguistics, 11:1?22.S.
Singh, R.S.
Bucks, and J.M.
Cuerden.
2001.
Evaluation ofan objective technique for analysing temporal variables inDAT spontaneous speech.
Aphasiology, 15(6):571?584.V.H.
Yngve.
1960.
A model and an hypothesis for languagestructure.
Proceedings of the American Philosophical Soci-ety, 104:444?466.8
