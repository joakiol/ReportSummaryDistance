Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 21?28,Columbus, June 2008. c?2008 Association for Computational LinguisticsLearning N-Best Correction Models from Implicit User Feedbackin a Multi-Modal Local Search ApplicationDan Bohus, Xiao Li, Patrick Nguyen, Geoffrey ZweigMicrosoft ResearchOne Microsoft WayRedmond, WA, 98052{dbohus, xiaol, panguyen, gzweig}@microsoft.comAbstractWe describe a novel n-best correction modelthat can leverage implicit user feedback (inthe form of clicks) to improve performance ina multi-modal speech-search application.
Theproposed model works in two stages.
First, then-best list generated by the speech recognizeris expanded with additional candidates, basedon confusability information captured via userclick statistics.
In the second stage, this ex-panded list is rescored and pruned to producea more accurate and compact n-best list.
Re-sults indicate that the proposed n-best correc-tion model leads to significant improvementsover the existing baseline, as well as other tra-ditional n-best rescoring approaches.1 IntroductionSupported by years of research in speech recogni-tion and related technologies, as well as advancesin mobile devices, speech-enabled mobile applica-tions are finally transitioning into day-to-day use.One example is Live Search for Windows Mobile(2008), a speech-enabled application that allowsusers to get access to local information by speakinga query into their device.
Several other systemsoperating in similar domains have recently becomeavailable (TellMeByMobile, 2008; Nuance MobileSearch, 2008; V-Lingo Mobile, 2008; VoiceSignalSearch, 2008.
)Traditionally, multi-modal systems leverage theadditional input channels such as text or buttons tocompensate for the current shortcomings of speechrecognition technology.
For instance, after the userspeaks a query, the Live Search for Windows Mo-bile application displays a confirmation screen thatcontains the n-best recognition results.
The userselects the correct hypothesis using the buttons onthe device, and only then the system displays thecorresponding search results (see Figure 1.
)We argue that ideally multi-modal systemscould use the additional, more accurate input chan-nels not only for confirmation or immediate cor-rection, but also to learn from the interaction andimprove their performance over time, without ex-plicit human supervision.
For example, in the inte-raction paradigm described above, apart fromproviding the means for selecting the correct rec-ognition result from an n-best list, the user click ona hypothesis can provide valuable informationabout the errors made by system, which could beexploited to further improve performance.Consider for instance the following numbersfrom an analysis of logged click data in the LiveSearch for Windows Mobile system.
Over a certainperiod of time, the results Beer and Gear were dis-played together in an n-best list 122 times.
Out ofthese cases, Beer was clicked 67% of the time, andGear was never clicked.
In 25% of the cases whenBeer was selected, Gear was incorrectly presentedabove (i.e.
higher than) Beer in the n-best list.More importantly, there are also 82 cases in whichGear appears in an n-best list, but Beer does not.
Amanual inspection reveals that, in 22% of thesecases, the actual spoken utterance was indeed Beer.The clicks therefore indicate that the engine oftenmisrecognizes Gear instead of Beer.21Ideally, the system should be able to take advan-tage of this information and use the clicks to createan automatic positive feedback loop.
We can envi-sion several ways in which this could be accom-plished.
A possible approach would be to use allthe clicked results to adapt the existing language oracoustic models.
Another, higher-level approach isto treat the recognition process as a black-box, anduse the click feedback (perhaps also in conjunctionwith other high-level information) to post-processthe results recognition results.While both approaches have their merits, in thiswork we concentrate on the latter paradigm.
Weintroduce a novel n-best correction model that le-verages the click data to improve performance in aspeech-enabled multi-modal application.
The pro-posed model works in two stages.
First, the n-bestlist generated by the speech recognizer is expandedwith additional candidates, based on results confu-sability information captured by the click statistics.For instance, in the 82 cases we mentioned abovewhen Gear was present in the n-best list but Beerwas not, Beer (as well as potentially other results)would also be added to form an expanded n-bestlist.
The expanded list is then rescored and prunedto construct a corrected, more accurate n-best list.The proposed approach, described in detail inSection 3, draws inspiration from earlier work inpost-recognition error-correction models (Ringgerand Allen, 1996; Ringger and Allen, 1997) and n-best rescoring (Chotimongkol and Rudnicky, 2001;Birkenes et al, 2007).
The novelty of our approachlies in: (1) the use of user click data in a deployedmulti-modal system for creating a positive feed-back loop, and (2) the development of an n-bestcorrection model based on implicit feedback thatoutperforms traditional rescoring-only approaches.Later on, in Section 5, we will discuss in more de-tail the relationship of the proposed approach tothese and other works previously reported in theliterature.Before moving on to describe the n-best correc-tion model in more detail, we give a high-leveloverview of Live Search for Windows Mobile, themulti-modal, mobile local search application thatprovided the test-bed for evaluating this work.2 Live Search for Windows MobileLive Search for Windows Mobile is an applicationthat enables local web-search on mobile devices.
Inits current version, it allows users to find informa-tion about local businesses and restaurants, to ob-tain driving directions, explore maps, view currenttraffic, get movie show-times, etc.
A number ofscreen-shots are illustrated in Figure 1.Recently, Live Search for Windows Mobile hasbeen extended with a speech interface (notice theSpeak button assigned to the left soft-key in Figure1.a.)
The speech-based interaction with the systemproceeds as follows: the user clicks the Speak but-ton and speaks the name of a local business, forinstance A-B-C Hauling, or a general category suchas Vietnamese Restaurants.
The application end-points the audio and forwards it over the datachannel to a server (Figure 1.b.)
Recognition isperformed on the server side, and the resulting n-best list is sent back to the client application, whereit is displayed to the user (Figure 1.c.)
The user canselect the correct item from the n-best list, re-speakthe request, or abandon the interaction altogetherby pressing Cancel.
Once the user selects an item inthe n-best list, the corresponding search results aredisplayed (Figure 1.d.
)(a) (b) (c) (d)Figure 1.
Windows Live Search for Mobile.
(a) initial screen; (b) user is speaking a request; (c) n-best listis presented; (d) final search results are displayed22Apart from business names, the system alsohandles speech input for addresses, as well ascompound requests, such as Shamiana Restaurantin Kirkland, Washington.
For the latter cases, atwo-tier recognition and confirmation process isused.
In the first stage a location n-best list is gen-erated and sent to the client for confirmation.
Afterthe user selects the location, a second recognitionstage uses a grammar tailored to that specific loca-tion to re-recognize the utterance.
The client thendisplays the final n-best list from which the usercan select the correct result.Several details about the system architecture andthe structure of the recognition process have beenomitted here due to space considerations.
For theinterested reader, a more in-depth description ofthis system is available in (Acero et al, 2008).3 ApproachWe now turn our attention to the proposed n-bestcorrection model3.1 OverviewThe model works in two stages, illustrated in Fig-ure 2.
In the first stage the n-best list produced bythe speech recognizer is expanded with severalalternative hypotheses.
In the second stage, theexpanded n-best list is rescored to construct thefinal, corrected n-best list.The n-best expansion step relies on a result con-fusion matrix, constructed from click information.The matrix, which we will describe in more detailin the following subsection, contains informationabout which result was selected (clicked) by theuser when a certain result was displayed.
For in-stance, in the example from Figure 2, the matrixindicates that when Burlington appeared in the n-best list, Bar was clicked once, Bowling wasclicked 13 times, Burger King was clicked twice,and Burlington was clicked 15 times (see hashedrow in matrix.)
The last element in the row indi-cates that there were 7 cases in which Burlingtonwas decoded, but nothing (?)
was clicked.
Essen-tially, the matrix captures information about theconfusability of different recognition results.The expansion step adds to an n-best list gener-ated by the recognizer all the results that were pre-viously clicked in conjunction with any one of theitems in the given n-best list.
For instance, in theexample from Figure 2, the n-best list containsSterling, Stirling, Burlington and Cooling.
Basedon the confusion matrix, this list will be expandedto also include Bar, Bowling, Burger King, Tow-ing, and Turley.
In this particular case, the correctrecognition result, Bowling, is added in the ex-panded n-best list.In the final step, the expanded list is rescored.
Inthe previous example, for simplicity of explana-tion, a simple heuristic for re-scoring was used:add all the counts on the columns corresponding toeach expanded result.
As a consequence, the cor-BurlingtonCoolingSterlingStirling0  ?
7    0     0    ?
0    0  ?
1   0    90  ?
4    0     0    ?
10    1  ?
2   2    50  ?
4    0     0    ?
4    1  ?
0   0    9BurlingtonBowlingBurgerKingTowingTurleyStirlingBarSterlingSterlingStirlingBurlingtonCooling + ?Bowling  28Burlington  15Sterling  14Towing  3Burger King  2Stirling  2Turley  2Bar  1BarBowlingBurger KingBurlingtonSterlingStirlingTowingTurley?Result Confusion MatrixInitialN-BestExpandedN-BestCorrected(expanded &rescored)N-BestFigure 2.
A confusion-based n-best correction model1  ?
13    2   15    ?
0    0  ?
0   0    7?Stage 1: Expansion Stage 2: Rescoring23rect recognition result, Bowling, was pushed to thetop of the n-best list.We begin by formally describing the construc-tion of the results confusability matrix and the ex-pansion process in the next two sub-sections.
Then,we describe three rescoring approaches.
The firstone is based on an error-correction model con-structed from the confusion matrix.
The other two,are more traditional rescoring approaches, basedon language model adaptation.3.2 The Result Confusion MatrixThe result confusion matrix is computed in a sim-ple traversal of the click logs.
The rows in the ma-trix correspond to decoded results, i.e.
results thathave appeared in an n-best list.
The columns in thematrix correspond to clicked (or intended) results,i.e.
results that the user has clicked on in the n-bestlist.
The entries at the intersection of row ?
andcolumn ?
correspond to the number of times result?
was clicked when result ?
was decoded:??
,?
= #(???????
= ?, ???????
= ?
).In addition, the last column in the matrix, de-noted ?
contains the number of times no result wasclicked when result ?
was displayed:??
,?
= #(???????
= ?, ???????
= ?
).The rows in the matrix can therefore be used tocompute the maximum likelihood estimate for theconditional probability distribution:???(?|?)
=??
,???
,?
?.The full dimensions of the result confusion ma-trix can grow very large since the matrix is con-structed at the result level (the average number ofwords per displayed result is 2.01).
The number ofrows equals the number of previously decoded re-sults, and the number of columns equals the num-ber of previously clicked results.
However, thematrix is very sparse and can be stored efficientlyusing a sparse matrix representation.3.3 N-Best ExpansionThe first step in the proposed n-best correctionmodel is to expand the initial n-best list with allresults that have been previously clicked in con-junction with the items in the current n-best list.Let?s denote by ?
= {??}?=1..?
the initial n-bestlist produced by the speech recognizer.
Then, theexpanded n-best list ??
will contain all ??
, as wellas all previously clicked results ?
such that thereexists ?
with ???
,?
> 0.3.4 Confusion Matrix Based RescoringIdeally, we would like to rank the hypotheses inthe expanded list ??
according to ?(?|?
), where ?represents the intended result and ?
represents theacoustics of the spoken utterance.
This can be re-written as follows:?
?
?
=  ?(?|?)
?
?(?|?)?
.
[1]The first component in this model is an error-correction model ?(?|?).
This model describes theconditional probability that the correct (or in-tended) result is ?
given that result ?
has been de-coded.
While this conditional model cannot beconstructed directly, we can replace it by a proxy -?(?|?
), which models the probability that the re-sult ?
will be clicked, given that result ?
was de-coded.
As mentioned earlier in subsection 3.2, thisconditional probability distribution can be com-puted from the result confusion matrix.
In replac-ing ?
?
?
with ?(?|?
), we are making theassumption that the clicks correspond indeed to thecorrect, intended results, and to nothing else1.Notice that the result confusion matrix is gener-ally very sparse.
The maximum likelihood estima-tor ???(?|?)
will therefore often be inappropriate.To address this data sparsity issue, we linearly in-terpolate the maximum likelihood estimator withan overall model ??(?|?):?
?
?
=  ????
?
?
+ (1?
?)??
?
?
.The overall model is defined in terms of twoconstants, ?
and ?, as follows:??
?
?
=?, ??
?
= ?
?, ??
?
?
?where ?
is the overall probability in the wholedataset of clicking on a given decoded result, and?
is computed such that ??
?
?
normalizes to 1.1 While this assumption generally holds, we have also ob-served cases where it is violated: sometimes users (perhapsaccidentally) click on an incorrect result; other times the cor-rect result is in the list but nothing is clicked (perhaps the userwas simply testing out the recognition capabilities of the sys-tem, without having an actual information need)24Finally, the ?
interpolation parameter is determinedempirically on the development set.The second component in the confusion basedrescoring model from equation [1] is ?(?|?).
Thisis the recognition score for hypothesis ?.
The n-best rescoring model from [1] becomes:?
?
?
=   ????
?
??
+ (1?
?)??
?
??
?
?(??
|?)???
?3.5 Language Model Based RescoringA more traditional alternative for n-best rescoringis to adapt the bigram language model used by thesystem in light of the user click data, and re-rankthe decoded results by:?
?
?
?
?
??
?
?
?
?
??
?(??
)Here ?
?
??
is the acoustic score assigned bythe recognizer to hypothesis ??
, and ?(??)
is theadapted language model score for this hypothesis.A simple approach for adapting the system?slanguage model is to add the word sequences ofthe user-clicked results to the original training sen-tences and to re-estimate the language model ?(?
).We will refer to this method as maximum likelih-ood (ML) estimation.
A second approach, referredto as conditional maximum likelihood (CML) es-timation, is to adapt the language model such as todirectly maximize the conditional likelihood of thecorrect result given acoustics, i.e.,?
?
?
=?
?
?
?(?)?
?
??
?(??)???
?Note that this is the same objective function asthe one used in Section 3.4, except that here theclick data is used to estimate the language modelinstead of the error correction model.
Again, inpractice we assume that users click on correct re-sults, i.e.
?
= ?.4 ExperimentsWe now discuss a number of experiments and theresults obtained using the proposed n-best correc-tion approach.4.1 DataFor the purposes of the experiments described be-low we extracted just over 800,000 queries fromthe server logs in which the recognizer had gener-ated a simple n-best list2.
For each recognitionevent, we collected from the system logs the n-bestlist, and the result clicked by the user (if the userclicked on any result).In addition, for testing purposes, we also makeuse of 11529 orthographically transcribed user re-quests.
The transcribed set was further divided intoa development set containing 5680 utterances anda test set containing 5849 utterances.4.2 Initial N-Best RescoringTo tease apart the effects of expansion and rescor-ing in the proposed n-best correction model, webegan by using the rescoring techniques on theinitial n-best lists, without first expanding them.Since the actual recognition confidence scores?(??
|?)
were not available in the system logs, wereplaced them with an exponential probability den-sity function based on the rank of the hypothesis:?
??
?
= 2?
?We then rescored the n-best lists from the testset according to the three rescoring models de-scribed earlier: confusion matrix, maximum like-lihood (ML), and conditional maximum likelihood(CML).
We computed the sentence level accuracyfor the rescored n-best list, at different cutoffs.
Theaccuracy was measured by comparing the rescoredhypotheses against the available transcripts.Note that the maximum depth of the n-best listsgenerated by the recognizer is 10; this is the max-imum number of hypotheses that can be displayedon the mobile device.
However, the system maygenerate fewer than 10 hypotheses.
The observedaverage n-best list size in the test set was 4.2.The rescoring results are illustrated in Figure 3and reported in Table 1.
The X axis in Figure 3shows the cutoff at which the n-best accuracy wascomputed.
For instance in the baseline system, thecorrect hypothesis was contained in the top resultin 46.2% of cases, in the top-2 results in 50.5% ofthe cases and in the top-3 results in 51.5% of thecases.
The results indicate that all the rescoringmodels improve performance relative to the base-2 We did not consider cases where a false-recognition eventwas fired (e.g.
if no speech was detected in the audio signal) ?in these cases no n-best list is generated.
We also did not con-sider cases where a compound n-best was generated (e.g.
forcompound requests like Shamiana in Kirkland, Washington)25line.
The improvement is smallest for the maxi-mum likelihood (ML) language model rescoringapproach, but is still statistically significant(?
= 0.008 in a Wilcoxon sign-rank test.)
The con-fusion-matrix based rescoring and the CML rescor-ing models perform similarly well, leading to a 1%absolute improvement in 1-best and 2-best sen-tence-level accuracy from the baseline (?
< 10?5).No statistically significant difference can be de-tected between these two models.
At the sametime, they both outperform the maximum likelih-ood rescoring model (?
< 0.03).4.3 N-Best CorrectionNext, we evaluated the end-to-end n-best correc-tion approach.
The n-best lists were first expanded,as described in section 3.3, and the expanded listswere ranked using the confusion matrix based res-coring model described in Section 3.4.The expansion process enlarges the original n-best lists.
Immediately after expansion, the averagen-best size grows from 4.2 to 96.9.
The oracle per-formance for the expanded n-best lists increases to59.8% (versus 53.5% in the initial n-best lists.
)After rescoring, we trimmed the expanded n-bestlists to a maximum of 10 hypotheses: we still wantto obey the mobile device display constraint.
Theresulting average n-best size was 7.09 (this is low-er than 10 since there are cases when the systemcannot generate enough expansion hypotheses.
)The sentence-level accuracy of the corrected n-best lists is displayed in line 4 from Table 1.
A di-rect comparison with the rescoring-only models orwith the baseline is however unfair, due to thelarger average size of the corrected n-best lists.
Tocreate a fair comparison and to better understandthe performance of the n-best correction process,we pruned the corrected n-best lists by eliminatingall hypotheses with a score below a certain thre-shold.
By varying this rejection threshold, we cantherefore control the average depth of the resultingcorrected n-best lists.
At a rejection threshold of0.004, the average corrected n-best size is 4.15,comparable to the baseline of 4.2 .The performance for the corresponding cor-rected (and pruned) n-best lists is shown in line 5from Table 1 and illustrated in Figure 4.
In contrastto a rescoring-only approach, the expansion pro-cess allows for improved performance at higherdepths in the n-best list.
The maximum n-best per-formance (while keeping the average n-best size at4.15), is 56.5%, a 3% absolute improvement overthe baseline (?
< 10?5).Figure 5 provides more insight into the relation-ship between the sentence-level accuracy of thecorrected (and pruned) n-best lists and the averagen-best size (the plot was generated by varying therejection threshold.)
The result we discussed abovecan also be observed here: at the same average n-best size, the n-best correction model significantlyoutperforms the baseline.
Furthermore, we can seethat we can attain the same level of accuracy as thebaseline system while cutting the average n-bestsize by more than 50%, from 4.22 to 2.
In the op-posite direction, if we are less sensitive to thenumber of items displayed in the n-best list (exceptfor the 10-maximum constraint we already obey),we can further increase the overall performance byanother 0.8% absolute to 57.3%; this overall accu-racy is attained at an average n-best size of 7.09.Figure 3.
Initial n-best rescoring (test-set)Table 1.
Test-set sentence-level n-best accuracy;(0) baseline; (1)-(3) initial n-best rescoring;(4)-(5) expansion + rescoringModel 1-Best2-Best3-Best10-Best0 Baseline 46.2 50.5 51.5 53.51 ML Rescoring  46.8 50.9 52.1 53.52 CML Rescoring 47.4 51.4 52.6 53.53 Confusion Matrix Resc.
47.3 51.5 52.5 53.54 Expansion + Rescoring(size=7.09)46.8 52.3 54.5 57.35 Expansion + Rescoring(size=4.15)46.8 52.3 54.4 56.526Finally, we also investigated rescoring the ex-panded n-best lists using the CML approach.
Toapply CML, an initial ranking of the expanded n-best lists is however needed.
If we use the rankingproduced by the confusion-matrix based modeldiscussed above, no further performance improve-ments can be observed.5 Related workThe n-best correction model we have described inthis paper draws inspiration from earlier works onpost-recognition error correction models, n-bestrescoring and implicitly supervised learning.
Inthis section we discuss some of the similarities anddifferences between the proposed approach andprevious work.The idea of correcting speech recognition errorsin a post-processing step has been proposed earlierby (Ringger and Allen, 1996; Ringger and Allen,1997).
The authors showed that, in the presence oftranscribed data, a translation-based post-processorcan be trained to correct the results of a speechrecognizer, leading to a 15% relative WER im-provement in a corpus of TRAINS-95 dialogues.The n-best correction approach described here isdifferent in two important aspects.
First, instead ofmaking use of transcripts, the proposed error-correction model is trained using implicit userfeedback obtained in a multi-modal interface (inthis case user clicks in the n-best list.)
This is a lesscostly endeavor, as the system automatically ob-tains the supervision signal directly from the inte-raction; no transcripts are necessary.
Second, theapproach operates on the entire n-best list, ratherthan only on the top hypothesis; as such, it has ad-ditional information that can be helpful in makingcorrections.
At Figure 2 illustrates, there is a poten-tial for multiple incorrect hypotheses to point to-wards and reinforce the same correctionhypothesis, leading to improved performance (inthis example, Burlington, Cooling, Sterling andStirling were all highly confusable with Bowling,which was the correct hypothesis).The n-best correction model we have describedincludes a rescoring step.
N-best rescoring ap-proaches have been investigated extensively in thespeech recognition community.
In the dialogcommunity, n-best rescoring techniques that usehigher-level, dialog features have also been pro-posed and evaluated (Chotimongkol and Rudnicky,2001).
Apart from using the click feedback, thenovelty in our approach lies in the added expansionstep and in the use of an error-correction model forrescoring.
We have seen that the confusability-based n-best expansion process leads to signifi-cantly improved performance, even if we force themodel to keep the same average n-best size.Finally, the work discussed in this paper hascommonalities with previous works on lightly su-pervised learning in the speech community, e.g.
(Lamel and Gauvain, 2002) and leveraging implicitfeedback for learning from interaction, e.g.
(Baner-jee and Rudnicky, 2007; Bohus and Rudnicky,2007).
In all these cases, the goal is to minimizethe need for manually-labeled data, and learn di-Figure 5.
Overall n-best accuracy as a function ofthe average n-best size53.5%56.5%57.3%Figure 4.
N-Best correction (test-set)27rectly from the interaction.
We believe that in thelong term this family of learning techniques willplay a key role towards building autonomous, self-improving systems.6 Conclusion and future workWe have proposed and evaluated a novel n-bestcorrection model that leverages implicit user feed-back in a multi-modal interface to create a positivefeedback loop.
While the experiments reportedhere were conducted in the context of a localsearch application, the approach is applicable inany multi-modal interface that elicits selection inan n-best list from the user.The proposed n-best correction model works intwo stages.
First, the n-best list generated by thespeech recognizer is expanded with additional hy-potheses based on confusability information cap-tured from previous user clicks.
This expanded listis then rescored and pruned to create a more accu-rate and more compact n-best list.
Our experimentsshow that the proposed n-best correction approachsignificantly outperforms both the baseline andother traditional n-best rescoring approaches, with-out increasing the average length of the n-best lists.Several issues remain to be investigated.
Themodels discussed in this paper focus on post-recognition processing.
Other ways of using theclick data can also be envisioned.
For instance, oneapproach would be to add all the clicked results tothe existing language model training data andcreate an updated recognition language model.
Inthe future, we plan to investigate the relationshipbetween these two approaches, and to whether theycan be used in conjunction.
Earlier related work(Ringger and Allen, 1997) suggests that this shouldindeed be the case.Second, the click-based error-correction modelwe have described in section 3.4 operates at theresult level.
The proposed model is essentially asentence level, memory-based translation model.In the future, we also plan to investigate word-level error-correction models, using machine trans-lation techniques like the ones discussed in (Ring-ger and Allen, 1997; Li et al, 2008).Finally, we plan to investigate how this processof learning from implicit feedback in a multi-modal interface can be streamlined, such that thesystem continuously learns online, with a minimalamount of human intervention.AcknowledgmentsThis work would have not been possible withoutthe help of a number of other people.
We wouldlike to especially thank Oliver Scholz, JulianOdell, Christopher Dac, Tim Paek, Y.C.
Ju, PaulBennett, Eric Horvitz and Alex Acero for theirhelp and for useful conversations and feedback.ReferencesAcero, A., N. Bernstein, et al (2008).
"Live Search forMobile: Web Services by Voice on the Cellphone".ICASSP'08.
Las Vegas, NV.Banerjee, S. and A. Rudnicky (2007).
"SegmentingMeetings into Agenda Items by Extracting ImplicitSupervision from Human Note-Taking".
IUI'2007.Honolulu, Hawaii.Birkenes, O., T. Matsui, et al (2007).
"N-Best Rescor-ing for Speech Recognition using Penalized Logis-tic Regression Machines with Garbage Class".ICASSP'2007, Honolulu, Hawaii.Bohus, D. and A. Rudnicky (2007).
"Implicitly-supervised learning in spoken language interfaces:an application to the confidence annotation prob-lem".
SIGdial 2007, Antwerp, Belgium.Chotimongkol, A. and A. Rudnicky (2001).
"N-bestSpeech Hypotheses Reordering Using Linear Re-gression".
Eurospeech'2001, Aalborg, Denmark.Lamel, L. and J.-L. Gauvain (2002).
"Lightly Super-vised and Unsupervised Acoustic Model Training.
"Computer Speech and Language 16: 115-129.Li, X., Y.-C. Ju, et al (2008).
"Language Modeling forVoice Search: a Machine Translation Approach".ICASSP'08, Las Vegas, NV.Live Search for Windows Mobile (2008):http://mobile.search.live.comNuance Mobile Search (2008):http://www.nuance.com/mobilesearch.Ringger, E. and J. Allen (1996).
"Error Correction viaPost-Processor for Continuous Speech Recogni-tion".
ICASSP'96, Atlanta, GA.Ringger, E. and J. Allen (1997).
"Robust Error Correc-tion of Continuous Speech Recognition".
ESCA-NATO Workshop on Robust Speech Recognitionfor Unknown Communication Channels, Pont-a-Mousson, France.TellMeByMobile (2008):http://www.tellme.com/products/tellmebymobile.V-Lingo Mobile.
(2008):http://www.vlingomobile.com/downloads.html.VoiceSignal Search.
(2008):http://www.voicesignal.com/solutions/vsearch.php.28
