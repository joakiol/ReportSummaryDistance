Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 57?64, Vancouver, October 2005. c?2005 Association for Computational LinguisticsRedundancy-based Correction of Automatically Extracted FactsRoman Yangarber and Lauri JokipiiDepartment of Computer ScienceUniversity of Helsinki, Finlandfirst.last@cs.helsinki.fiAbstractThe accuracy of event extraction is lim-ited by a number of complicating factors,with errors compounded at all sages in-side the Information Extraction pipeline.In this paper, we present methods for re-covering automatically from errors com-mitted in the pipeline processing.
Recov-ery is achieved via post-processing factsaggregated over a large collection of doc-uments, and suggesting corrections basedon evidence external to the document.
Afurther improvement is derived from prop-agating multiple, locally non-best slot fillsthrough the pipeline.
Evaluation showsthat the global analysis is over 10 timesmore likely to suggest valid corrections tothe local-only analysis than it is to suggesterroneous ones.
This yields a substantialoverall gain, with no supervised training.1 IntroductionInformation Extraction (IE) is a technology for find-ing facts in plain text, and coding them in a logicalrepresentation, such as, e.g., a relational database.IE is typically viewed and implemented as a se-quence of stages?a ?pipeline?:1.
Layout, tokenization, lexical analysis2.
Name recognition and classification3.
Shallow (commonly,) syntactic parsing4.
Resolution of co-reference among entities5.
Pattern-based event matching and role mapping6.
Normalization and output generationWhile accuracy at the lowest levels can reach high90?s, as the stages advance, complexity increasesand performance degrades considerably.The problem of IE as a whole, as well each ofthe listed subproblems, has been studied intensivelyfor well over a decade, in many flavors and varieties.Key observations about much state-of-the-art IE are:a. IE is typically performed by a pipeline process;b.
Only one hypothesis is propagated through thepipeline for each fact?the ?best guess?
thesystem can make for each slot fill;c. IE is performed in a document-by-documentfashion, applying a priori knowledge locally toeach document.The a priori knowledge may be encoded in a set ofrules, an automatically trained model, or a hybridthereof.
Information extracted from documents?which may be termed a posteriori knowledge?is usually not reused across document boundaries,because the extracted facts are imprecise, and aretherefore not a reliable basis for future extraction.Furthermore, locally non-best slot fills are notpropagated through the pipeline, and are conse-quently not available downstream, nor for any globalanalysis.In most systems, these stages are performed in se-quence.
The locally-best slot fills are passed from57the ?lower-?
to the ?higher-level?
modules, with-out feedback.
Improvements are usually sought(e.g., the ACE research programme, (ACE, 2004))by boosting performance at the lower levels, to reapbenefits in the subsequent stages, where fewer errorsare propagated.The point of departure for this paper is: theIE process is noisy and imprecise at the single-document level; this has been the case for some time,and though there is much active research in the area,the situation is not likely to change radically in theimmediate future?rather, we can expect slow, in-cremental improvements over some years.In our experiments, we approach the performanceproblem from the opposite end: start with the ex-tracted results and see if the totality of a posteri-ori knowledge about the domain?knowledge gen-erated by the same noisy process we are trying toimprove?can help recover from errors that stemfrom locally insufficient a priori knowledge.The aim of the research presented in this paperis to improve performance by aggregating relatedfacts, which were extracted from a large documentcollection, and to examine to what extent the cor-rectly extracted facts can help correct those that wereextracted erroneously.The rest of the paper is organized as follows.
Sec-tion 2 contains a brief review of relevant prior work.Section 3 presents the experimental setup: the textcorpus, the IE process, the extracted facts, and whataspects of the the extracted facts we try to improvein this paper.
Section 4 presents the methods for im-proving the quality of the data using global analysis,starting with a naive, baseline method, and proceed-ing with several extensions.
Each method is thenevaluated, and the results are examined in section 5.In section 6, we present further extensions currentlyunder research, followed by the conclusion.2 Prior WorkAs we stated in the introduction, typical IE sys-tems consist of modules arranged in a cascade, ora pipeline.
The modules themselves are be basedon heuristic rules or automatically trained, there isan abundance of approaches in both camps (and ev-erywhere in between,) to each of the pipeline stageslisted in the introduction.It is our view that to improve performance weought to depart from the traditional linear, pipeline-style design.
This view is shared by others in theresearch community; the potential benefits have pre-viously been recognized in several contexts.In (Nahm and Mooney, 2000a; Nahm andMooney, 2000b), it was shown that learning rulesfrom a fact base, extracted from a corpus of job post-ings for computer programmers, improves future ex-traction, even though the originally extracted factsthemselves are far from error-free.
The idea is tomine the data base for association rules, and then tointegrate these rules into the extraction process.The baseline system is obtained by supervisedlearning from a few hundred manually annotated ex-amples.
Then the IE system is applied to succes-sively larger sets of unlabeled examples, and associ-ation rules are mined from the extracted facts.
Theresulting combined system (trained model plus as-sociation rules) showed an improvement in perfor-mance on a test set, which correlated with the sizeof the unlabeled corpus.In work on improving (Chinese) named entity tag-ging, (Ji and Grishman, 2004; Ji and Grishman,2005), show benefits to this component from in-tegrating decisions made in later stages, viz.
co-reference, and relation extraction.1Tighter coupling and integration between IE andKDD components for mutual benefit is advocated by(McCallum and Jensen, 2003), which present mod-els based on CRFs and supervised training.This work is related in spirit to the work pre-sented in this paper, in its focus on leveraging cross-document information that information?though itis inherently noisy?to improve local decisions.
Weexpect that the approach could be quite powerfulwhen these ideas are used in combination, and ourexperiments seem to confirm this expectation.3 Experimental SetupIn this section we describe the text corpus, the un-derlying IE process, the form of the extracted facts,and the specific problem under study?i.e., whichaspects of these facts we first try to improve.1Performance on English named entity tasks reaches mid tohigh 90?s in many domains.583.1 CorpusWe conducted experiments with redundancy-basedauto-correction over a large database of facts ex-tracted from the texts in ProMED-Mail, a mailinglist which carries reports about outbreaks of infec-tious epidemics around the world and the effortsto contain them.
This domain has been exploredearlier; see, e.g., (Grishman et al, 2003) for anoverview.Our underlying IE system is described in (Yan-garber et al, 2005).
The system is a hybridautomatically- and manually-built pattern base forfinding facts, an HMM-based name tagger, auto-matically compiled and manually verified domain-specific ontology, based in part on MeSH, (MeS,2004), and a rule-based co-reference module, thatuses the ontology.The database is live on-line, and is continuouslyupdated with new incoming reports; it can be ac-cessed at doremi.cs.helsinki.fi/plus/.Text reports have been collected by ProMED-Mail for over 10 years.
The quality of reporting (andediting) has been rising over time, which is easy toobserve in the text data.
The distribution of the data,aggregated by month is shown in Figure 1, whereone can see a steady increase in volume over time.23.2 Extracted FactsWe now describe the makeup of the data extractedfrom text by the IE process, with basic terminology.Each document in the corpus, contains a single re-port, which may contain one or more stories.
Storybreaks are indicated by layout features, and are ex-tracted by heuristic rules, tuned for this domain andcorpus.
When processing a multi-story report, theIE system treats each story as a separate document;no information is shared among stories, except thatthe text of the main headline of a multi-story reportis available to each story.
3Since outbreaks may be described in complexways, it is not obvious how to represent a single factin this context.
To break down this problem, we usethe notion of an incident.
Each story may contain2This is beneficial to the IE process, which operates betterwith formulaic, well-edited text.3The format of the documents in the archive can be exam-ined by browsing the source site www.promedmail.org.020040060080010001995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005CountDateRecords   (46,317)Stories   (30,015)Documents (22,560)Figure 1: Distribution of data in ProMED-Mailmultiple outbreak-related incidents/facts.4We analyze an outbreak as a series of incidents.The incidents may give ?redundant?
informationabout an outbreak, e.g., by covering overlappingtime intervals or geographic areas.
For example, areport may first state the number of cases within thelast month, and then give the total for the entire year.We treat each of these statements as a separate inci-dent; the containment relations among them are be-yond the scope of our current goals.5Thus each incident corresponds to a partial de-scription of an outbreak, over a period of time andgeographic area.
This makes it easy to representeach incident/fact as a separate row in the table.The key fields of the incident table are:  Disease Name  Location  Date (start and end)Where possible, the system also extracts informa-tion about the victims affected in the incident?theircount, severity (affected or dead), and a descriptor(people, animals, etc.).
The system also extractsbookkeeping information about each incident: loca-tions of mentions of the key fields in the text, etc.The system?s performance is currently at 71.16 F-measure: 67% recall, 74% precision.
This score isobtained by a MUC scorer (Douthat, 1998) on a 50-document test corpus, which was manually taggedwith correct incidents with these slots.
We have4In this paper, we use the terms fact, incident, and eventinterchangeably.5This problem is addressed in, e.g., (Huttunen et al, 2002).59no blind-test corpus at present, but prior experiencesuggests that we ought to expect about a 10% reduc-tion in F-measure on unseen data; this is approxi-mately borne out by our informal evaluations.Further, the system attempts to ?normalize?
thekey fields.
An alias for a disease name (e.g., ?birdflu?)
is mapped to a canonical name (?avian in-fluenza.?
)6 Date expressions are normalized to astandard format yyyy.mm.dd?yyyy.mm.dd.7Note that the system may not be able to normalizesome entities, which then remain un-normalized.Such normalization is clearly helpful for search-ing, but it is not only a user-interface issue.
Normal-izing reduces sparseness of data; and since our intentis to aggregate related facts across a large fact base,excessive variation in the database fields would re-duce the effectiveness of the proposed methods.3.3 Experimental Focus: LocationNormalizationA more complex problem arises out of the need tonormalize location names.
For each record, we nor-malize the location field?which may be a name ofa small village or a larger area?by relating it to thename of the containing country; we also decided tomap locations in the United States to the name of thecontaining state, (rather than the name of the coun-try, ?USA?
).8 This mapping will be henceforth re-ferred to as ?location?state,?
for short.
The ideaspresented in the introduction are explored in the re-mainder of this paper in the context of correcting thelocation?state mapping.Section 6 will touch upon our current work on ex-tending the methodology to slots other than state.
(Please see Section 5 for further justification of thischoice for our initial experiments.
)To make the experiments interesting and fair, wekept the size of the gazetteer small.
The a priori geo-graphic knowledge base contains names of countriesof the world (270), with aliases for several of them; alist of capitals and other selected major cities (300);a list of states in the USA and acronyms (50); major6This is done by means of a set of scenario-specific patternsand a dictionary of about 2500 disease names with aliases.7Some date intervals may not have a starting date, e.g., if thetext states ?As of last Tuesday, the victim count is N...?8This decision was made because otherwise records withstate = USA strongly skew the data, and complicate learning.US cities (100); names of the (sub)continents (10),and oceans.
In our current implementation, conti-nents are treated semantically as ?states?
as well.9The IE system operates in a local, document-by-document fashion.
Upon encountering a locationname that is not in its dictionaries, the system hastwo ways to map it to the state name.
One way isby matching patterns over the immediate local con-text, (?Milan, Italy?).
Failing that, it tries to findthe corresponding state by positing an ?underspeci-fied?
state name (as if referred to by a kind of spe-cial ?pronoun?)
and mapping the location name tothat.
The reference resolution module then finds themost likely antecedent entity, of the semantic type?state/country,?
where likelihood is determined byits proximity to the mention of the location name.Note that the IE system outputs only a single, besthypothesis for the state fill for each record.3.4 The DataThe database currently contains about  in-dividual facts/incidents, extracted from  sto-ries, from reports (cf.
Fig.
1).
Each incidenthas a location and a state filler.
We say a locationname is ?ambiguous?
if it appears in the location slotof at least two records, which have different namesin the state slot.
The number of distinct ?ambigu-ous?
location names is.Note, this terminology is a bit sloppy: the fillersto which we refer as ?ambiguous location names?,may not be valid location names at all; they maysimply be errors in the IE process.
E.g., at the nameclassification stage, a disease name (especially if notin the disease dictionary) may be misclassified, andused as a filler for the location slot.We further group together the location fills bystripping lower-case words that are not part of theproper name, from the front and the end of the fill.E.g., we group together ?southern Mumbai?
and?the Mumbai area,?
as referring to the same name.After grouping and trimming insignificant words,the number of distinct names appearing in locationfills is, which covers a total ofrecords,or  of all extracted facts.
As an estimate ofthe potential for erroneous mapping from locationsto states, this is quite high, about  in 	 records.109By the same token, both Connecticut and USA are ?states.
?10Of course, it can be higher as well, if the IE system con-604 Experiments and ResultsWe now present the methods of correcting possibleerrors in the location?state relation.
A method  tries to suggest a new value for the state fill for everyincident I that contains an ambiguous location fill:	fiffflffi!"$#&%('*),+.-/.0213 (1)where 4 03is a set of all candidate states consid-ered by   for I; 5)6+-/ 0 213 is the scoring func-tion specific to   .
The method chooses the candi-date state which maximizes the score.For each method below, we discuss how 4 0 and*),+-/ 0are constructed.4.1 Baseline: Raw MajorityWe begin with a simple recovery approach.
We sim-ply assume that the correct state for an ambiguouslocation name is the state most frequently associatedwith it in the database.
We denote by 7 the set of allincidents in the database.
For an incident 98 7 , wewrite :;1=<>when location : , state1, etc., ?belong?to I, i.e., are extracted as fills in I.
In the baselinemethod, ?
, for each incident  where : <> is one oftheambiguous location names, we define:4A@3CB.13DFEHGH,DI87KJ:;13DLM<N,DPO*),+-Q@213D 35JB3,DI87KJ:;13DLM<N,DPOJi.e.,1D is a candidate if it is a state fill in some in-cident whose location fill is also : ; the score is thenumber of times the pair  : ;1DL appear together insome incident in 7 .
The majority winner is thensuggested as the ?correct?
state, for every recordcontaining : .
By ?majority?
winner we mean thecandidate with the maximal count, rather than a statewith more than half of the votes.
When the candi-dates tie for first place, no suggestions are made?although it is quite likely that some of the recordscarrying : will have incorrect state fills.A manual evaluation of the performance of thismethod is shown in Table 1, the Baseline column.The first row shows for how many records thismethod suggested a change from the original, IE-filled state.
The baseline changed 858 incidents.sistently always maps some location name to the same wrongstate; these cases are below the radar of our scheme, in whichthe starting point is the ?ambiguous?
locations.This constitutes about 13% out of the maximumnumber of changeable records,  .Thus, this number represents the volume of thepotential gain or loss from the global analysis: theproportion of records that actually get modified.The remaining records were unchanged, either be-cause the majority winner coincides with the origi-nal IE-extracted state, or because there was a tie forthe top score, so no decision could be made.We manually verified a substantial sample of themodified records.
When verifying the changes, wereferred back to the text of the incident, and, whennecessary, consulted further geographical sources todetermine exactly whether the change was correct ineach case.For the baseline we had manually verified 27.7%of the changes.
Of these, 68.5% were a clear gain:an incorrect state was changed to a correct state.6.3% were a clear loss, a correct state lost to an in-correct one.
This produces quite a high baseline, sur-prisingly difficult to beat.The next two rows represent the ?grey?
areas.These are records which were difficult to judge,for one of two technical reasons.
A: the ?loca-tion?
name was itself erroneous, in which case theseredundancy-based approaches are not meaningful;or, B: the suggestion replaces an area by its sub-region or super-region, e.g., changing ?Connecticut?to ?USA?, or ?Europe?
to ?France.
?11Although it is not strictly meaningful to judgewhether these changes constitute a gain or a loss,we nonetheless tried to assess whether changing thestate hurt the accuracy of the incident, since the in-cident may have a correct state even though its loca-tion is erroneous (case A); likewise, it may be cor-rect to say that a given location is indeed a part ofConnecticut, in which case changing it to USA losesinformation, and is a kind of loss.That is the interpretation of the grey gain and lossinstances.
The final row, no loss, indicates the pro-portion of cases where an originally incorrect statename was changed to a new one, also incorrect.11Note, that for some locations, which are not within any onestate?s boundary, a continent is a ?correct state?, for example,?the Amazon Region,?
or ?Serengeti.
?61Records Baseline DB-filtered Confidence Multi-candidateChanged       	  		   	            	   Verified  	  	          		 	                   	 Gain        	          	            Loss            Grey gain                             	       Grey loss  	                                  No loss    	         Table 1: Performance of Correction Methods4.2 Database FilteringNext we examined a variant of baseline raw major-ity vote, noting that simply choosing the state mostfrequently associated with a location name is a bitnaive: the location?state relation is not functional?i.e., some location names map to more than one statein reality.
There are many locations which share thesame name.12To approach this more intelligently, we define:43 4A@3 	.1 	 +- 3*),+.-/21D35 *),+-/@21D3The baseline vote counting across the data base (DB)produced a ranked list of candidate states 1 D for thelocation :< .
We then filtered this list through	31 * +- 3, the list of states mentioned inthe story containing the incident  .
The filtered ma-jority winner was selected as the suggested change.For example, the name ?Athens?
may refer to thecity in Greece, or to the city in Georgia (USA).Suppose that Greece is the raw majority winner.The baseline method will always tag all instancesof Athens as being in Greece.
However, in a storyabout Georgia, Greece will likely not be mentionedat all, so it is safe to rule it out.
This helps a minoritywinner, when the majority is not present in the story.Surprisingly, this method did not yield a substan-tial improvement over the baseline, (though it wasmore careful by changing fewer records).
This mayindicate that NWP is not an important source of er-rors here: though many truly ambiguous locations12We refer to this as the ?New-World phenomenon?
(NWP),due to its prevalence in the Americas: ?Santa Cruz?
occurs inseveral Latin American countries; locations named after saintsare common.
In the USA, city and county names often appearin multiple states?Winnebago County, Springfield; many citiesare named after older European cities.do exist, they do not account for many instances inthis DB.4.3 Confidence-Based RankingA more clear improvement over the baseline is ob-tained by taking the local confidence of the state?location association into account.
For each record,we extend the IE analysis to produce a confidencevalue for the state.
Confidence is computed by sim-ple, document-local heuristics, as follows:If the location and state are both within the spanof text covered by the incident?text which was ac-tually matched by a rule in the IE system,?or if thestate is the unique state mentioned in the story, it getsa score of 2?the incident has high confidence in thestate.
Otherwise, if the state is outside the incident?sspan, but is inside the same sentence as the incident,and is also the unique state mentioned in that sen-tence, it gets a score of 1.
Otherwise it receives ascore of zero.Given the confidence score for each (location : ,state1 ) pair, the majority counting is based on thecumulative confidence, ),+ffflfi  ffi!
"ffi!#  : ;1 in the DB,rather than on the cumulative count of occurrencesof this pair in the DB:4fl$343*),+-Q$21D35 %%'&  (*) #,+.-  & '0/ %'&),+ffflfiffi!
"ffi!#DFiltering through the story is also applied, as inthe previous method.
The resulting method favorsmore correct decisions, and fewer erroneous ones.We should note here, that the notion of confidenceof a fill (here, the state fill) is naturally extended tothe notion of confidence of a record: For each of62the three key fills?location, date, disease name?compute a confidence based on the same heuristics.Then we say that a record  has high confidence, if ithas non-zero confidence in all three of the key fills.The notion of record confidence is used in Section 6.4.4 Multi-Candidate PropagationFinally, we tried propagating multiple candidatestate hypotheses for each instance of an ambiguouslocation name : :4 35 %'&  ( ) + / %'&	.1 	 +- D*),+-/ 21D3 %%'&  (*) +!/ %'&-Q+*21DDwhere the proximity is inversely proportional to thedistance of 13D from incident D , in the story of  D :-+ 213 =3213   fi91 <>+ -1For an incident  mentioning location : , the IE sys-tem outputs the list of all statesB.1QOmentioned inthe same story; we then rank each1according tothe inverse of distance: the number of sentencesbetween  and 1 .
=3 is a normalization factor.The proximity for each pair:;1, is betweenand  .
Rather than giving a full point to a single,locally-best guess among the 1 ?s, this point is sharedproportionately among all competing1?s.
For exam-ple, if states1 ;1;1are in the same sentence as, one, and five sentences away, respectively, then=3 fffifl, and-Q+	21  flfl,-Q+	215ffffiflfl,and-Q+	21fifl.The score for each state1for the given : is thenthe sum of proximities of1to : across all stories.The resulting performance is substantially bet-ter than the baseline, while the number of changedrecords is substantially higher than in the competingmethods.
This is due to the fact that this method al-lows for a much larger pool of candidates than theothers, and assigns to them much smoother weights,virtually eliminating ties in the ranking among hy-potheses.5 DiscussionAmong the four competing approaches presentedabove, the baseline performs surprisingly well.
Weshould note that this research is not aimed specifi-cally at improving geographic Named Entity resolu-tion.
It is the first in a series of experiments aimingto leverage redundancy across a large fact base ex-tracted from text, to improve the quality of extracteddata.
We chose to experiment with this relation firstbecause of its simplicity, and because the state fieldis a key field in our application.For this reason, the a priori geographic knowl-edge base was intentionally not as extensive as itmight have been, had we tried in earnest to matchlocations with corresponding states (e.g., by incor-porating the CIA Factbook, or other gazetteer).The intent here is to investigate how a relationcan be improved by leveraging redundancy acrossa large body of records.
The support we used for ge-ographic name resolution was therefore deliberatelymodest, cf.
Section 3.3.It is quite feasible to enumerate the countries andthe larger regions, since they number in the low hun-dreds, whereas there are many tens of thousands ofcities, towns, villages, regions, districts, etc.6 Current WorkThree parallel lines of current research are:1. combining evidence from multiple features2.
applying redundancy-based correction to otherfields in the database3.
back-propagation of corrected results, to repaircomponents that induced incorrect information.The results so far presented show that even anaive, intuitive approach can help correct local er-rors via global analysis.
We are currently workingon more complex extensions of these methods.Each method exploits one main feature of the un-derlying data: the distance from candidate state tothe mention of the location name.
In the multi-candidate hypothesis method, this distance is ex-ploited explicitly in the scoring function.
In theother methods, it is used inside the co-referencemodule of the IE pipeline, to find the (single)locally-best state.However, other textual features of the state can-didate should contribute to establishing the relations63to a location mention, besides the raw distance.
Forexample, at a given distance, it is very importantwhether the state is mentioned before the location(more likely to be related) vs. after the location (lesslikely).
Another important feature: is the state men-tioned in the main story/report headline?
If so, itsscore should be raised.
It is quite common for doc-uments to declaim the focal state only once in theheadline, and never mention it again, instead men-tioning other states, neighboring, or otherwise rele-vant to the story.
The distance measure used alonemay be insufficient in such cases.How are these features to be combined?
One pathis to use some combination of features, such as aweighted sum, with parameters trained on a man-ually tagged data set.
As we already have a rea-sonably sized set tagged for evaluation, we can splitit into two, train the parameter on a larger portion,evaluate on a smaller one, and cross-validate.We will be using this approach as a baseline.However, we aim to use a much larger set of data totrain the parameters, without manually tagging largetraining sets.The idea is to treat the set of incidents with highrecord confidence, Sec.
4.3, rather than manuallytagged data, as ground truth.
Again, there ?con-fident?
truth will not be completely error-free, butbecause error rates are lower among the confidentrecords, we may be able to leverage global analy-sis to produce the desired effect: training parame-ters for more complex models?involving multiplefeatures?for global re-ranking of decisions.ConclusionOur approach rests on the idea that evidence aggre-gated across documents should help resolve difficultproblems at the level of a given document.Our experiments confirm that aggregating globalinformation about related facts, and propagating lo-cally non-best analyses through the pipeline, providepowerful sources of additional evidence, which areable to reverse incorrect decisions, based only on lo-cal and a priori information.The proposed approach requires no supervision ortraining of any kind.
It does, however require a sub-stantial collection of evidence across a large bodyof extracted records; this approach needs a ?criticalmass?
of data to be effective.
Although large volumeof facts is usually not reported in classic IE experi-ments, obtaining high volume should be natural inprinciple.AcknowledgementWe?d like to thank Winston Lin of New York Uni-versity, who worked on an early version of these ex-periments.References2004.
Automatic content extraction.A.
Douthat.
1998.
The Message Understanding Con-ference scoring software user?s manual.
In Proc.
7thMessage Understanding Conf.
(MUC-7), Fairfax, VA.R.
Grishman, S. Huttunen, and R. Yangarber.
2003.
In-formation extraction for enhanced access to diseaseoutbreak reports.
J. of Biomed.
Informatics, 35(4).S.
Huttunen, R. Yangarber, and R. Grishman.
2002.Complexity of event structure in information extrac-tion.
In Proc.
19th Intl.
Conf.
Computational Linguis-tics (COLING 2002), Taipei.H.
Ji and R. Grishman.
2004.
Applying coreference toimprove name recognition.
In Proc.
Reference Reso-lution Wkshop, (ACL-2004), Barcelona, Spain.H.
Ji and R. Grishman.
2005.
Improving name taggingby reference resolution and relation detection.
In Proc.ACL-2005, Amherst, Mass.A.
McCallum and D. Jensen.
2003.
A note on the uni-fication of information extraction and data mining us-ing conditional-probability, relational models.
In IJ-CAI?03 Workshop on Learning Statistical Models fromRelational Data.2004.
Medical subject headings.U.
Y. Nahm and R. Mooney.
2000a.
A mutually benefi-cial integration of data mining and information extrac-tion.
In AAAI-2000, Austin, TX.U.
Y. Nahm and R. Mooney.
2000b.
Using informationextraction to aid the discovery of prediction rules fromtext.
In KDD-2000 Text Mining Wkshop, Boston, MA.R.
Yangarber, L. Jokipii, A. Rauramo, and S. Hut-tunen.
2005.
Extracting information about outbreaksof infectious epidemics.
In Proc.
HLT-EMNLP 2005Demonstrations, Vancouver, Canada.64
