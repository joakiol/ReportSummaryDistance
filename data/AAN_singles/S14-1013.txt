Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 93?98,Dublin, Ireland, August 23-24 2014.Compositional Distributional Semantics Modelsin Chunk-based Smoothed Tree KernelsNghia The PhamUniversity of Trentothenghia.pham@unitn.itLorenzo FerroneUniversity of Rome ?Tor Vergata?lorenzo.ferrone@gmail.comFabio Massimo ZanzottoUniversity of Rome ?Tor Vergata?fabio.massimo.zanzotto@uniroma2.itAbstractThe field of compositional distributionalsemantics has proposed very interestingand reliable models for accounting thedistributional meaning of simple phrases.These models however tend to disregardthe syntactic structures when they are ap-plied to larger sentences.
In this paper wepropose the chunk-based smoothed treekernels (CSTKs) as a way to exploit thesyntactic structures as well as the reliabil-ity of these compositional models for sim-ple phrases.
We experiment with the rec-ognizing textual entailment datasets.
Ourexperiments show that our CSTKs per-form better than basic compositional dis-tributional semantic models (CDSMs) re-cursively applied at the sentence level, andalso better than syntactic tree kernels.1 IntroductionA clear interaction between syntactic and semanticinterpretations for sentences is important for manyhigh-level NLP tasks, such as question-answering,textual entailment recognition, and semantic tex-tual similarity.
Systems and models for these tasksoften use classifiers or regressors that exploit con-volution kernels (Haussler, 1999) to model bothinterpretations.Convolution kernels are naturally defined onspaces where there exists a similarity function be-tween terminal nodes.
This feature has been usedto integrate distributional semantics within treekernels.
This class of kernels is often referred to assmoothed tree kernels (Mehdad et al., 2010; Croceet al., 2011), yet, these models only use distribu-tional vectors for words.Compositional distributional semantics models(CDSMs) on the other hand are functions map-ping text fragments to vectors (or higher-order ten-sors) which then provide a distributional meaningfor simple phrases or sentences.
Many CDSMshave been proposed for simple phrases like non-recursive noun phrases or verbal phrases (Mitchelland Lapata, 2008; Baroni and Zamparelli, 2010;Clark et al., 2008; Grefenstette and Sadrzadeh,2011; Zanzotto et al., 2010).
Non-recursivephrases are often referred to as chunks (Abney,1996), and thus, CDSMs are good and reliablemodels for chunks.In this paper, we present the chunk-basedsmoothed tree kernels (CSTK) as a way to mergethe two approaches: the smoothed tree kernelsand the models for compositional distributional se-mantics.
Our approach overcomes the limitationof the smoothed tree kernels which only use vec-tors for words by exploiting reliable CDSMs overchunks.
CSTKs are defined over a chunk-basedsyntactic subtrees where terminal nodes are wordsor word sequences.
We experimented with CSTKson data from the recognizing textual entailmentchallenge (Dagan et al., 2006) and we comparedour CSTKs with other standard tree kernels andstandard recursive CDSMs.
Experiments showthat our CSTKs perform better than basic compo-sitional distributional semantic models (CDSMs)recursively applied at the sentence level and betterthan syntactic tree kernels.The rest of the paper is organized as follows.Section 2 describes the CSTKs.
Section 3 re-ports on the experimental setting and on the re-sults.
Finally, Section 4 draws the conclusions andsketches the future work.2 Chunk-based Smoothed Tree KernelsThis section describes the new class of kernels.We first introduce the notion of the chunk-basedsyntactic subtree.
Then, we describe the recursiveformulation of the class of kernels.
Finally, we in-troduce the basic CDSMs we use and we introducetwo instances of the class of kernels.932.1 Notation and preliminariesShhhhh(((((NPXXXDTthe:dNNrock:nNNband:nVPXXXVBZholds:vNPXXXXPRPits:pJJfinal:jNNconcert:nFigure 1: Sample Syntactic TreeA Chunk-based Syntactic Sub-Tree is a subtreeof a syntactic tree where each non-terminal nodedominating a contiguous word sequence is col-lapsed into a chunk and, as usual in chunks (Ab-ney, 1996), the internal structure is disregarded.For example, Figure 2 reports some chunk-basedsyntactic subtrees of the tree in Figure 1.
Chunksare represented with a pre-terminal node dominat-ing a triangle that covers a word sequence.
Thefirst subtree represents the chunk covering the sec-ond NP and the node dominates the word sequenceits:d final:n concert:n. The second subtree repre-sents the structure of the whole sentence and onechunk, that is the first NP dominating the wordsequence the:d rock:n band:n. The third subtreeagain represents the structure of the whole sen-tence split into two chunks without the verb.NP````its:p final:j concert:nSXXXNPXXXXthe:d rock:n band:nVPZVBZ NPS````NPXXXXthe:d rock:j band:nVPPPPVBZ NP````its:p final:j concert:nFigure 2: Some Chunk-based Syntactic Sub-Treesof the tree in Figure 1In the following sections, generic trees are de-noted with the letter t and N(t) denotes the set ofnon-terminal nodes of tree t. Each non-terminalnode n ?
N(t) has a label snrepresenting its syn-tactic tag.
As usual for constituency-based parsetrees, pre-terminal nodes are nodes that have a sin-gle terminal node as child.
Terminal nodes of treesare words denoted with w:pos where w is the ac-tual token and pos is its postag.
The structure ofthese trees is represented as follows.
Given a treet, ci(n) denotes i-th child of a node n in the set ofnodes N(t).
The production rule headed in noden is prod(n), that is, given the node nwithm chil-dren, prod(n) is:prod(n) = sn?
sc1(n).
.
.
scm(n)Finally, for a node n in N(t), the function d(n)generates the word sequence dominated by thenon-terminal node n in the tree t. For example,d(VP) in Figure 1 is holds:v its:p final:j concert:n.Chunk-based Syntactic Sub-Trees (CSSTs) areinstead denoted with the letter ?
.
Differentlyfrom trees t, CSSTs have terminal nodes thatcan represent subsequences of words of theoriginal sentence.
The explicit syntactic structureof a CSST is the structure not falling in chunksand it is represented as s(?).
For example, s(?3) is:SHHNP VPZVBZ NPwhere ?3is the third subtree of Figure 2.Given a tree t, the set S(t) is defined as the setcontaining all the relevant CSSTs of the tree t.As for the tree kernels (Collins and Duffy, 2002),the set S(t) contains all CSSTs derived from thesubtrees of t such that if a node n belongs to asubtree ts, all the siblings of n in t belongs to ts.In other words, productions of the initial subtreesare complete.
A CSST is obtained by collapsingin a single terminal nodes a contiguous sequenceof words dominated by a single non-terminalnode.
For example:NPPPDT NNHHNNrock:nNNband:nis collapsed into:NPaa!
!DT NN:XPPProck:n band:nFinally,?wn?
Rmrepresent the distributionalvectors for words wnand f(w1.
.
.
wk) representsa compositional distributional semantics modelapplied to the word sequence w1.
.
.
wk.942.2 Smoothed Tree Kernels on Chunk-basedSyntactic TreesAs usual, a tree kernel, although written in a re-cursive way, computes the following general equa-tion:K(t1, t2) =??i?
S(t1)?j?
S(t2)?|N(?i)|+|N(?j)|KF(?i, ?j)(1)In our case, the basic similarity KF(ti, tj) is de-fined to take into account the syntactic structureand the distributional semantic part.
Thus, we de-fine it as follows in line with what done with sev-eral other smoothed tree kernels:KF(?i, ?j) = ?
(s(?i), s(?j))?a ?
PT (?i)b ?
PT (?j)?f(a), f(b)?where ?
(s(?i), s(?j)) is the Kroneker?s deltafunction between the the structural part of twochunk-based syntactic subtrees, PT (?)
are thenodes in ?
directly covering a chunk or a word,and ?
?x,?y ?
is the cosine similarity between thetwo vectors?x and?y .
For example, given thechunk-based subtree ?3in Figure 2 and?4=SXXXXNPXXXthe:d orchestra:nVPaa!
!VBZ NPPPits:p show:nthe similarity KF(?3, ?4) is:?f(the:d orchestra:n), f(the:d rock:n band:n)?
?
?f(its:p show:n), f(its:p final:j concert:n)?.The recursive formulation of the Chunk-basedSmoothed Tree Kernel (CSTK) is a bit more com-plex but very similar to the recursive formulationof the syntactic tree kernels:K(t1, t2) =?n1?
N(t1)n2?
N(t2)C(n1, n2) (2)where C(n1, n2) =???????????????????????????????
?f(d(n1)), f(d(n2))?
if label(n1) = label(n2)and prod(n1) 6= prod(n2)?f(d(n1)), f(d(n2))?+?nc(n1)j=1(1 + C(cj(n1), cj(n2)))?
?nc(n1)j=1?f(d(cj(n1))), f(d(cj(n2)))?if n1, n2are not pre-terminals andprod(n1) = prod(n2)0 otherwisewhere nc(n1) is the lenght of the productionprod(n1).2.3 Compositional Distributional SemanticModels and two Specific CSTKsTo define specific CSTKs, we need to introducethe basic compositional distributional semanticmodels (CDSMs).
We use two CDSMs: the Ba-sic Additive model (BA) and teh Full Additivemodel (FA).
We thus define two specific CSTKs:the CSTK+BA that is based on the basic additivemodel and the CSTK+FA that is based on the fulladditive model.
We describe the two CDSMs inthe following.The Basic Additive model (BA) (introduced in(Mitchell and Lapata, 2008)) computes the disti-butional semantics vector of a pair of words a =a1a2as:ADD(a1, a2) = ?
?a1+ ?
?a2where ?
and ?
weight the first and the secondword of the pair.
The basic additive model forword sequences s = w1.
.
.
wkis recursively de-fined as follows:fBA(s) ={?w1if k = 1?
?w1+ ?fBA(w2.
.
.
wk) if k > 1The Full Additive model (FA) (used in (Gue-vara, 2010) for adjective-noun pairs and (Zanzottoet al., 2010) for three different syntactic relations)computes the compositional vector?a of a pair us-ing two linear tranformations ARand BRrespec-tively applied to the vectors of the first and thesecond word.
These matrices generally only de-pends on the syntactic relation R that links thosetwo words.
The operation follows:fFA(a1, a2, R) = AR?a1+BR?a295RR RRTWSRTE1 RTE2 RTE3 RTE5 Average RTE1 RTE2 RTE3 RTE5 AverageAdd 0.541 0.496 0.507 0.520 0.516 0.560 0.538 0.643 0.578 0.579FullAdd 0.512 0.516 0.507 0.569 0.526 0.571 0.608 0.643 0.643 0.616TK 0.561 0.552 0.531 0.54 0.546 0.608 0.627 0.648 0.630 0.628CSTK+BA 0.553 0.545 0.562 0.568 0.557?0.626 0.616 0.648 0.628 0.629?CSTK+FA 0.543 0.550 0.574 0.576 0.560?0.628 0.616 0.652 0.630 0.631?Table 1: Task-based analysis: Accuracy on Recognizing Textual Entailment (?
is different from both ADD andFullADD with a stat.sig.
of p > 0.1.
)The full additive model for word sequences s =w1.
.
.
wk, whose node has a production rule s ?sc1.
.
.
scmis also defined recursively:fFA(s) =?????????????????????
?w1if k = 1Avn?V +BvnfFA(NP )if s?
V NPAan?A +BanfFA(N)if s?
A N?fFA(sci) otherwisewhere Avn, Bvnare matrices used for verb andnoun phrase interaction, andAan, Banare used foradjective, noun interaction.3 Experimental Investigation3.1 Experimental set-upWe experimented with the Recognizing TextualEntailment datasets (RTE) (Dagan et al., 2006).RTE is the task of deciding whether a long textT entails a shorter text, typically a single sen-tence, called hypothesis H .
It has been often seenas a classification task (see (Dagan et al., 2013)).We used four datasets: RTE1, RTE2, RTE3, andRTE5, with the standard split between training andtesting.
The dev/test distribution for RTE1-3, andRTE5 is respectively 567/800, 800/800, 800/800,and 600/600 T-H pairs.Distributional vectors are derived withDISSECT (Dinu et al., 2013) from a cor-pus obtained by the concatenation of ukWaC(wacky.sslmit.unibo.it), a mid-2009 dump of theEnglish Wikipedia (en.wikipedia.org) and theBritish National Corpus (www.natcorp.ox.ac.uk),for a total of about 2.8 billion words.
We collecteda 35K-by-35K matrix by counting co-occurrenceof the 30K most frequent content lemmas inthe corpus (nouns, adjectives and verbs) and allthe content lemmas occurring in the datasetswithin a 3 word window.
The raw count vectorswere transformed into positive Pointwise MutualInformation scores and reduced to 300 dimensionsby Singular Value Decomposition.
This setup waspicked without tuning, as we found it effective inprevious, unrelated experiments.We built the matrices for the full additive mod-els using the procedure described in (Guevara,2010).
We considered only two relations: theAdjective-Noun and Verb-Noun.
The full addi-tive model falls back to the basic additional modelwhen syntactic relations are different from thesetwo.To build the final kernel to learn the clas-sifier, we followed standard approaches (Daganet al., 2013), that is, we exploited two models:a model with only a rewrite rule feature space(RR) and a model with the previous space alongwith a token-level similarity feature (RRTWS).The two models use our CSTKs and the stan-dard TKs in the following way as kernel func-tions: (1) RR(p1, p2) = ?
(ta1, ta2) + ?
(tb1, tb2);(2) RRTWS(p1, p2) = ?
(ta1, ta2) + ?
(tb1, tb2) +(TWS(a1, b1) ?
TWS(a2, b2) + 1)2where TWSis a weighted token similarity (as in (Corley andMihalcea, 2005)).3.2 ResultsTable 1 shows the results of the experiments, thetable is organised as follows: columns 2-6 re-port the accuracy of the RTE systems based onrewrite rules (RR) and columns 7-11 report the ac-curacies of RR systems along with token similar-ity (RRTS).
We compare five differente models:ADD is the Basic Additive model with parameters?
= ?
= 1 (as defined in 2.3) applied to the wordsof the sentence (without considering its tree struc-ture), the same is done for the Full Additive (Ful-lADD), defined as in 2.3.
The Tree Kernel (TK) asdefined in (Collins and Duffy, 2002) are applied to96the constituency-based tree representation of thetree, without the intervening collapsing step de-scribed in 2.2.
These three models are the base-line against which we compare the CSTK modelswhere the collapsing procedure is done via BasicAdditive (CSTK + BA, again with ?
= ?
= 1) andFullAdditive (CSTK + FA), as described in sec-tion 2.2, again, with the aforementioned restric-tion on the relation considered.
For RR models wehave that CSTK+BA and CSTK+FA both achievehigher accuracy than ADD and FullAdd, with astatistical significante greater than 93.7%, as com-puted with the sign test.
Specifically we have thatCSTK+BA has an average accuracy 7.94% higherthan ADD and 5.89% higher than FullADD, whileCSTK+FA improves on ADD and FullADD by8.52% and 6.46%, respectively.
The same trend isvisible for the RRTS model, again both models arestatistically better than ADD and FullADD, in thiscase we have that CSTK+BA is 8.63% more ac-curate then ADD and 2.11% more than FullADD,CSTK+FA is respectively 8.98% and 2.43% moreaccurate than ADD and FullADD.
As for the TKmodels we have that both CSTK models achieveagain an higher average accuracy: for RR modelsCSTK+BA and CSTK+FA are respectively 2.01%and 0.15% better than TK, while for RRTS modelsthe number are 2.54% and 0.47%.
These resultsthough are not statistically significant, as is thedifference between the two CSTK models them-selves.4 Conclusions and Future WorkIn this paper, we introduced a novel sub-classof the convolution kernels in order exploit reli-able compositional distributional semantic mod-els along with the syntactic structure of sen-tences.
Experiments show that this novel sub-class, namely, the Chunk-based Smoothed TreeKernels (CSTKs), are a promising solution, per-forming significantly better than a naive recursiveapplication of the compositional distributional se-mantic models.
We experimented with CSTKSequipped with the basic additive and the full addi-tive CDSMs but these kernels are definitely opento all the CDSMs.AcknowledgmentsWe acknowledge ERC 2011 Starting IndependentResearch Grant n. 283554 (COMPOSES).ReferencesSteven Abney.
1996.
Part-of-speech tagging and par-tial parsing.
In G.Bloothooft K.Church, S.Young,editor, Corpus-based methods in language andspeech.
Kluwer academic publishers, Dordrecht.Marco Baroni and Roberto Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: Representingadjective-noun constructions in semantic space.
InProceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, pages1183?1193, Cambridge, MA, October.
Associationfor Computational Linguistics.Stephen Clark, Bob Coecke, and MehrnooshSadrzadeh.
2008.
A compositional distributionalmodel of meaning.
Proceedings of the SecondSymposium on Quantum Interaction (QI-2008),pages 133?140.Michael Collins and Nigel Duffy.
2002.
New rank-ing algorithms for parsing and tagging: Kernels overdiscrete structures, and the voted perceptron.
In Pro-ceedings of ACL02.Courtney Corley and Rada Mihalcea.
2005.
Measur-ing the semantic similarity of texts.
In Proc.
of theACL Workshop on Empirical Modeling of Seman-tic Equivalence and Entailment, pages 13?18.
As-sociation for Computational Linguistics, Ann Arbor,Michigan, June.Danilo Croce, Alessandro Moschitti, and RobertoBasili.
2011.
Structured lexical similarity via con-volution kernels on dependency trees.
In Proceed-ings of the Conference on Empirical Methods inNatural Language Processing, EMNLP ?11, pages1034?1046, Stroudsburg, PA, USA.
Association forComputational Linguistics.Ido Dagan, Oren Glickman, and Bernardo Magnini.2006.
The pascal recognising textual entailmentchallenge.
In Quionero-Candela et al., editor,LNAI 3944: MLCW 2005, pages 177?190.
Springer-Verlag, Milan, Italy.Ido Dagan, Dan Roth, Mark Sammons, and Fabio Mas-simo Zanzotto.
2013.
Recognizing Textual Entail-ment: Models and Applications.
Synthesis Lectureson Human Language Technologies.
Morgan & Clay-pool Publishers.Georgiana Dinu, Nghia The Pham, and Marco Baroni.2013.
DISSECT: DIStributional SEmantics Com-position Toolkit.
In Proceedings of ACL (SystemDemonstrations), pages 31?36, Sofia, Bulgaria.Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011.Experimental support for a categorical composi-tional distributional model of meaning.
In Proceed-ings of the Conference on Empirical Methods inNatural Language Processing, EMNLP ?11, pages1394?1404, Stroudsburg, PA, USA.
Association forComputational Linguistics.97Emiliano Guevara.
2010.
A regression model ofadjective-noun compositionality in distributional se-mantics.
In Proceedings of the 2010 Workshop onGEometrical Models of Natural Language Seman-tics, pages 33?37, Uppsala, Sweden, July.
Associa-tion for Computational Linguistics.David Haussler.
1999.
Convolution kernels on discretestructures.
Technical report, University of Califor-nia at Santa Cruz.Yashar Mehdad, Alessandro Moschitti, and Fabio Mas-simo Zanzotto.
2010.
Syntactic/semantic struc-tures for textual entailment recognition.
In HumanLanguage Technologies: The 2010 Annual Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics, HLT ?10, pages1020?1028, Stroudsburg, PA, USA.
Association forComputational Linguistics.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In Proceedingsof ACL-08: HLT, pages 236?244, Columbus, Ohio,June.
Association for Computational Linguistics.Fabio Massimo Zanzotto, Ioannis Korkontzelos,Francesca Fallucchi, and Suresh Manandhar.
2010.Estimating linear models for compositional distribu-tional semantics.
In Proceedings of the 23rd Inter-national Conference on Computational Linguistics(COLING), August,.98
