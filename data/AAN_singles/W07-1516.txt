Proceedings of the Linguistic Annotation Workshop, pages 101?108,Prague, June 2007. c?2007 Association for Computational LinguisticsActive Learning for Part-of-Speech Tagging:Accelerating Corpus AnnotationEric Ringger*, Peter McClanahan*, Robbie Haertel*, George Busby*, Marc Carmen**,James Carroll*, Kevin Seppi*, Deryle Lonsdale***Computer Science Department; **Linguistics DepartmentBrigham Young UniversityProvo, Utah, USA 84602AbstractIn the construction of a part-of-speech an-notated corpus, we are constrained by afixed budget.
A fully annotated corpus isrequired, but we can afford to label only asubset.
We train a Maximum Entropy Mar-kov Model tagger from a labeled subsetand automatically tag the remainder.
Thispaper addresses the question of where tofocus our manual tagging efforts in order todeliver an annotation of highest quality.
Inthis context, we find that active learning isalways helpful.
We focus on Query by Un-certainty (QBU) and Query by Committee(QBC) and report on experiments with sev-eral baselines and new variations of QBCand QBU, inspired by weaknesses particu-lar to their use in this application.
Experi-ments on English prose and poetry testthese approaches and evaluate their robust-ness.
The results allow us to make recom-mendations for both types of text and raisequestions that will lead to further inquiry.1 IntroductionWe are operating (as many do) on a fixed budgetand need annotated text in the context of a largerproject.
We need a fully annotated corpus but canafford to annotate only a subset.
To address ourbudgetary constraint, we train a model from a ma-nually annotated subset of the corpus and automat-ically annotate the remainder.
At issue is where tofocus manual annotation efforts in order to producea complete annotation of highest possible quality.A follow-up question is whether these techniqueswork equally well on different types of text.In particular, we require part-of-speech (POS)annotations.
In this paper we employ a state-of-the-art tagger on both prose and poetry, and we ex-amine multiple known and novel active learning(or sampling) techniques in order to determinewhich work best in this context.
We show that theresults obtained by a state-of-the-art tagger trainedon a small portion of the data selected through ac-tive learning can approach the accuracy attained byhuman annotators and are on par with results fromexhaustively trained automatic taggers.In a study based on English language data pre-sented here, we identify several active learningtechniques and make several recommendations thatwe hope will be portable for application to othertext types and to other languages.
In section 2 webriefly review the state of the art approach to POStagging.
In section 3, we survey the approaches toactive learning employed in this study, includingvariations on commonly known techniques.
Sec-tion 4 introduces the experimental regime andpresents results and their implications.
Section 5draws conclusions and identifies opportunities forfollow-up research.2 Part of Speech TaggingLabeling natural language data with part-of-speechtags can be a complicated task, requiring mucheffort and expense, even for trained annotators.Several efforts, notably the Alembic workbench(Day et al, 1997) and similar tools, have providedinterfaces to aid annotators in the process.Automatic POS tagging of text using probabilis-tic models is mostly a solved problem but requiressupervised learning from substantial amounts oftraining data.
Previous work demonstrates the sui-tability of Hidden Markov Models for POS tagging(Kupiec, 1992; Brants, 2000).
More recent workhas achieved state-of-the-art results with Maxi-101mum entropy conditional Markov models (MaxEntCMMs, or MEMMs for short) (Ratnaparkhi, 1996;Toutanova & Manning, 2000; Toutanova et al,2003).
Part of the success of MEMMs can be attri-buted to the absence of independence assumptionsamong predictive features and the resulting ease offeature engineering.
To the best of our knowledge,the present work is the first to present results usingMEMMs in an active learning framework.An MEMM is a probabilistic model for se-quence labeling.
It is a Conditional Markov Model(CMM as illustrated in Figure 1) in which a Max-imum Entropy (MaxEnt) classifier is employed toestimate the probability distribution1.. 1 1 2( | , ) ( | , , , )i i ME i i i i ip t w t p t w f t t?
?
??
overpossible labels it  for each element in the se-quence?in our case, for each word iw  in a sen-tence w .
The MaxEnt model is trained from la-beled data and has access to any predefinedattributes (represented here by the collection if ) ofthe entire word sequence and to the labels of pre-vious words ( 1.. 1it ?
).
Our implementation employsan order-two Markov assumption so the classifierhas access only to the two previous tags 1 2,i it t?
?
.We refer to the features 1 2( , , , )i i i iw f t t?
?
fromwhich the classifier predicts the distribution overtags as ?the local trigram context?.A Viterbi decoder is a dynamic programmingalgorithm that applies the MaxEnt classifier toscore multiple competing tag-sequence hypothesesefficiently and to produce the best tag sequence,according to the model.
We approximate Viterbivery closely using a fast beam search.
Essentially,the decoding process involves sequential classifi-cation, conditioned on the (uncertain) decisions ofthe previous local trigram context classifications.The chosen tag sequence t?
is the tag sequencemaximizing the following quantity:1 21..?
arg max ( | )arg max ( | , , , )tt ME i i i i ii nt P t wp t w f t t?
?=== ?The features used in this work are reasonablytypical for modern MEMM feature-based POStagging and consist of a combination of lexical,orthographic, contextual, and frequency-based in-formation.
In particular, for each word the follow-ing features are defined: the textual form of theword itself, the POS tags of the preceding twowords, and the textual form of the following word.Following Toutanova and Manning (2000) approx-imately, more information is defined for words thatare considered rare (which we define here as wordsthat occur fewer than fifteen times).
We considerthe tagger to be near-state-of-the-art in terms oftagging accuracy.Figure 1.
Simple Markov order 2 CMM, with focus onthe i-th hidden label (or tag).3 Active LearningThe objective of this research is to produce morehigh quality annotated data with less human anno-tator time and effort.
Active learning is an ap-proach to machine learning in which a model istrained with the selective help of an oracle.
Theoracle provides labels on a sufficient number of?tough?
cases, as identified by the model.
Easycases are assumed to be understood by the modeland to require no additional annotation by theoracle.
Many variations have been proposed in thebroader active learning and decision theory litera-ture under many names, including ?active sam-pling?
and ?optimal sampling.
?In active learning for POS tagging, as in otherapplications, the oracle can be a human.
For expe-rimental purposes, a human oracle is simulatedusing pre-labeled data, where the labels are hiddenuntil queried.
To begin, the active learning processrequires some small amount of training data toseed the model.
The process proceeds by identify-ing the data in the given corpus that should betagged first for maximal impact.3.1 Active Learning in the Language ContextWhen considering the role of active learning, wewere initially drawn to the work in active learningfor classification.
In a simple configuration, eachinstance (document, image, etc.)
to be labeled canbe considered to be independent.
However, for ac-tive learning for the POS tagging problem we con-sidered the nature of human input as an oracle forthe task.
As an approximation, people read sen-tences as propositional atoms, gathering contextualcues from the sentence in order to assemble the102meaning of the whole.
Consequently, we thought itunreasonable to choose the word as the granularityfor active learning.
Instead, we begin with the as-sumption that a human will usually require muchof the sentence or at least local context from thesentence in order to label a single word with itsPOS label.
While focusing on a single word, thehuman may as well label the entire sentence or atleast correct the labels assigned by the tagger forthe sentence.
Consequently, the sentence is thegranularity of annotation for this work.
(Futurework will question this assumption and investigatetagging a word or a subsequence of words at atime.)
This distinguishes our work from activelearning for classification since labels are notdrawn from a fixed set of labels.
Rather, every sen-tence of length n can be labeled with a tag se-quence drawn from a set of size nT , where T  isthe size of the per-word tag set.
Granted, many ofthe options have very low probability.To underscore our choice of annotating at thegranularity of a sentence, we also note that a max-imum entropy classifier for isolated word taggingthat leverages attributes of neighboring words?but is blind to all tags?will underperform anMEMM that includes the tags of neighboringwords (usually on the left) among its features.
Pre-vious experiments demonstrate the usefulness oftags in context on the standard Wall Street Journaldata from the Penn Treebank (Marcus et al, 1999).A MaxEnt isolated word tagger achieves 93.7% onwords observed in the training set and 82.6% onwords unseen in the training set.
Toutanova andManning (2000) achieves 96.9% (on seen) and86.9% (on unseen) with an MEMM.
They sur-passed their earlier work in 2003 with a ?cyclicdependency network tagger?, achieving97.2%/89.05% (seen/unseen) (Toutanova et al,2003).
The generally agreed upon upper bound isaround 98%, due to label inconsistencies in theTreebank.
The main point is that effective use ofcontextual features is necessary to achieve state ofthe art performance in POS tagging.In active learning, we employ several sets ofdata that we refer to by the following names:?
Initial Training: the small set of data usedto train the original model before activelearning starts?
Training: data that has already been la-beled by the oracle as of step i in the learn-ing cycle?
Unannotated: data not yet labeled by theoracle as of step i?
Test (specifically Development Test): la-beled data used to measure the accuracy ofthe model at each stage of the active learn-ing process.
Labels on this set are held inreserve for comparison with the labelschosen by the model.
It is the accuracy onthis set that we report in our experimentalresults in Section 4.Note that the Training set grows at the expense ofthe Unannotated set as active learning progresses.Active Learning for POS Tagging consists of thefollowing steps:1.
Train a model with Initial Training data2.
Apply model to Unannotated data3.
Compute potential informativeness ofeach sentence4.
Remove top n sentences with most po-tential informativeness from Unanno-tated data and give to oracle5.
Add n sentences annotated (or corrected)by the oracle to Training data6.
Retrain model with Training data7.
Return to step 2 until stopping conditionis met.There are several possible stopping conditions,including reaching a quality bar based on accuracyon the Test set, the rate of oracle error correctionsin the given cycle, or even the cumulative numberof oracle error corrections.
In practice, the exhaus-tion of resources, such as time or money, maycompletely dominate all other desirable stoppingconditions.Several methods are available for determiningwhich sentences will provide the most information.Expected Value of Sample Information (EVSI)(Raiffa & Schlaiffer, 1967) would be the optimalapproach from a decision theoretic point of view,but it is computationally prohibitive and is not con-sidered here.
We also do not consider the relatednotion of query-by-model-improvement or othermethods (Anderson & Moore, 2005; Roy &McCallum, 2001a, 2001b).
While worth exploring,they do not fit in the context of this current workand should be considered in future work.
We focushere on the more widely used Query by Committee(QBC) and Query by Uncertainty (QBU), includ-ing our new adaptations of these.Our implementation of maximum entropy train-ing employs a convex optimization procedureknown as LBFGS.
Although this procedure is rela-tively fast, training a model (or models in the case103of QBC) from scratch on the training data duringevery round of the active learning loop would pro-long our experiments unnecessarily.
Instead westart each optimization search with a parameter setconsisting of the model parameters from the pre-vious iteration of active learning (we call this ?FastMaxEnt?).
In practice, this converges quickly andproduces equivalent results.3.2 Query by CommitteeQuery by Committee (QBC) was introduced bySeung, Opper, and Sompolinsky (1992).
Freund,Seung, Shamir, and Tishby (1997) provided a care-ful analysis of the approach.
Engelson and Dagan(1996) experimented with QBC using HMMs forPOS tagging and found that selective sampling ofsentences can significantly reduce the number ofsamples required to achieve desirable tag accura-cies.
Unlike the present work, Engelson & Daganwere restricted by computational resources to se-lection from small windows of the Unannotated set,not from the entire Unannotated set.
Related workincludes learning ensembles of POS taggers, as inthe work of Brill and Wu (1998), where an ensem-ble consisting of a unigram model, an N-grammodel, a transformation-based model, and anMEMM for POS tagging achieves substantial re-sults beyond the individual taggers.
Their conclu-sion relevant to this paper is that different taggerscommit complementary errors, a useful fact to ex-ploit in active learning.
QBC employs a committeeof N models, in which each model votes on thecorrect tagging of a sentence.
The potential infor-mativeness of a sentence is measured by the totalnumber of tag sequence disagreements (comparedpair-wise) among the committee members.
Possi-ble variants of QBC involve the number of com-mittee members, how the training data is splitamong the committee members, and whether thetraining data is sampled with or without replace-ment.A potential problem with QBC in this applica-tion is that words occur with different frequenciesin the corpus.
Because of the potential for greaterimpact across the corpus, querying for the tag of amore frequent word may be more desirable thanquerying for the tag of a word that occurs less fre-quently, even if there is greater disagreement onthe tags for the less frequent word.
We attemptedto compensate for this by weighting the number ofdisagreements by the corpus frequency of the wordin the full data set (Training and Unannotated).Unfortunately, this resulted in worse performance;solving this problem is an interesting avenue forfuture work.3.3 Query by UncertaintyThe idea behind active sampling based on uncer-tainty appears to originate with Thrun and Moeller(1992).
QBU has received significant attention ingeneral.
Early experiments involving QBU wereconducted by Lewis and Gale (1994) on text classi-fication, where they demonstrated significant bene-fits of the approach.
Lewis and Catlett (1994) ex-amined its application for non-probabilistic learn-ers in conjunction with other probabilistic learnersunder the name ?uncertainty sampling.?
BrighamAnderson (2005) explored QBU using HMMs andconcluded that it is sometimes advantageous.
Weare not aware of any published work on the appli-cation of QBU to POS tagging.
In our implementa-tion, QBU employs a single MEMM tagger.
TheMaxEnt model comprising the tagger can assessthe probability distribution over tags for any wordin its local trigram context, as illustrated in the ex-ample in Figure 2.Figure 2.
Distribution over tags for the word ?hurdle?
initalics.
The local trigram context is in boldface.In Query by Uncertainty (QBU), the informa-tiveness of a sample is assumed to be the uncer-tainty in the predicted distribution over tags forthat sample, that is the entropy of1 2( | , , , )ME i i i i ip t w f t t?
?
.
To determine the poten-tial informativeness of a word, we can measure theentropy in that distribution.
Since we are selectingsentences, we must extend our measure of uncer-tainty beyond the word.3.4 Adaptations of QBUThere are several problems with the use of QBU inthis context:?
Some words are more important; i.e., theycontain more information perhaps becausethey occur more frequently.NN 0 .85VB  0.13...RB    DT JJS CD  2.0E-7Perhaps     the biggest   hurdle ?104?
MaxEnt estimates per-word distributionsover tags, not per-sentence distributionsover tag sequences.?
Entropy computations are relatively costly.We address the first issue in a new version of QBUwhich we call ?Weighted Query by Uncertainty?(WQBU).
In WQBU, per-word uncertainty isweighted by the word's corpus frequency.To address the issue of estimating per-sentenceuncertainty from distributions over tag sequences,we have considered several different approaches.The per-word (conditional) entropy is defined asfollows:where iT  is the random variable for the tag it  onword iw , and the features of the context in whichiw  occurs are denoted, as before, by the collectionif  and the prior tags 1 2,i it t?
?
.
It is straightforwardto calculate this entropy for each word in a sen-tence from the Unannotated set, if we assume thatprevious tags 1 2,i it t?
?
are from the Viterbi (best)tag sequence (for the entire sentence) according tothe model.For an entire sentence, we estimate the tag-sequence entropy by summing over all possible tagsequences.
However, computing this estimate ex-actly on a 25-word sentence, where each word canbe labeled with one of 35 tags, would require 3525= 3.99*1038 steps.
Instead, we approximate the per-sentence tag sequence distribution entropy bysumming per-word entropy:This is the approach we refer to as QBU in theexperimental results section.
We have experi-mented with a second approach that estimates theper-sentence entropy of the tag-sequence distribu-tion by Monte Carlo decoding.
Unfortunately, cur-rent active learning results involving this MC POStagging decoder are negative on small Training setsizes, so we do not present them here.
Another al-ternative approximation worth pursuing is compu-ting the per-sentence entropy using the n-best POStag sequences.
Very recent work by Mann andMcCallum (2007) proposes an approach in whichexact sequence entropy can be calculated efficient-ly.
Further experimentation is required to compareour approximation to these alternatives.An alternative approach that eliminates theoverhead of entropy computations entirely is toestimate per-sentence uncertainty with ?1 ( )P t?
,where t?
is the Viterbi (best) tag sequence.
We callthis scheme QBUV.
In essence, it selects a sampleconsisting of the sentences having the highestprobability that the Viterbi sequence is wrong.
Toour knowledge, this is a novel approach to activelearning.4 Experimental ResultsIn this section, we examine the experimental setup,the prose and poetry data sets, and the results fromusing the various active learning algorithms onthese corpora.4.1 SetupThe experiments focus on the annotation scenarioposed earlier, in which budgetary constraints af-ford only some number x of sentences to be anno-tated.
The x-axis in each graph captures the num-ber of sentences.
For most of the experiments, thegraphs present accuracies on the (Development)Test set.
Later in this section, we present results foran alternate metric, namely number of words cor-rected by the oracle.In order to ascertain the usefulness of the activelearning approaches explored here, the results arepresented against a baseline in which sentences areselected randomly from the Unannotated set.
Weconsider this baseline to represent the use of astate-of-the-art tagger trained on the same amountof data as the active learner.
Due to randomization,the random baseline is actually distinct from expe-riment to experiment without any surprising devia-tions.
Also, each result curve in each graphrepresents the average of three distinct runs.Worth noting is that most of the graphs includeactive learning curves that are run to completion;namely, the rightmost extent of all curvesrepresents the exhaustion of the Unannotated data.At this extreme point, active learning and randomsample selection all have the same Training set.
Inthe scenarios we are targeting, this far right side isnot of interest.
Points representing smaller amountsof annotated data are our primary interest.In the experiments that follow, we address sev-eral natural questions that arise in the course ofapplying active learning.
We also compare the va-1 21 21 2( | , , , )( | , , , )log ( | , , , )ii i i i iME i i i i it TagsetME i i i i iH T w f t tp t w f t tp t w f t t?
??
???
?= ??
?1 2?
( | ) ( | , , , )ii i i i iw wH T w H T w f t t?
???
?
?105riants of QBU and QBC.
For QBC, committeemembers divide the training set (at each stage ofthe active learning process) evenly.
All committeemembers and final models are MEMMs.
Likewise,all variants of QBU employ MEMMs.4.2 Data SetsThe experiments involve two data sets in searchof conclusions that generalize over two very dif-ferent kinds of English text.
The first data set con-sists of English prose from the POS-tagged one-million-word Wall Street Journal text in the PennTreebank (PTB) version 3.
We use a random sam-ple of the corpus constituting 25% of the tradition-al training set (sections 2?21).
Initial Training dataconsists of 1% of this set.
We employ section 24 asthe Development Test set.
Average sentence lengthis approximately 25 words.Our second experimental set consists of Englishpoetry from the British National Corpus (BNC)(Godbert & Ramsay, 1991; Hughes, 1982; Raine,1984).
The text is also fully tagged with 91 parts ofspeech from a different tag set than the one usedfor the PTB.
The BNC XML data was taken fromthe files B1C.xml, CBO.xml, and H8R.xml.
Thisresults in a set of 60,056 words and 8,917 sen-tences.4.3 General ResultsTo begin, each step in the active learning processadds a batch of 100 sentences from the Unanno-tated set at a time.
Figure 3 demonstrates (usingQBU) that the size of a query batch is not signifi-cant in these experiments.The primary question to address is whether ac-tive learning helps or not.
Figure 4 demonstratesthat QBU, QBUV, and QBC all outperform therandom baseline in terms of total, per-word accu-racy on the Test set, given the same amount ofTraining data.
Figure 5 is a close-up version ofFigure 4, placing emphasis on points up to 1000annotated sentences.
In these figures, QBU andQBUV vie for the best performing active learningalgorithm.
These results appear to give some usefuladvice captured in Table 1.
The first column in thetable contains the starting conditions.
The remain-ing columns indicate that for between 800-1600sentences of annotation, QBUV takes over fromQBU as the best selection algorithm.The next question to address is how much initialtraining data should be used; i.e., when should westart using active learning?
The experiment in Fig-ure 6 demonstrates (using QBU) that one shoulduse as little data as possible for Initial TrainingData.
There is always a significant advantage tostarting early.
In the experiment documented inFigure 3.
Varying the size of the query batch in activelearning yields identical results after the first query batch.Figure 4.
The best representatives of each type of activelearner beat the baseline.
QBU and QBUV trade off thetop position over QBC and the Baseline.Figure 5.
Close-up of the low end of the graph from Figure4.
QBUV and QBU are nearly tied for best performance.7580859095100  1000  10000Accuracy(%)Number of Sentences in Training SetBatch Query Size of 10 SentencesBatch Query Size of 100 SentencesBatch Query Size of 500 Sentences7580859095100  1000  10000Accuracy(%)Number of Sentences in Training SetQBUVQBUQBCBaseline767880828486889092100  1000Accuracy(%)Number of Sentences in Training SetQBUVQBUQBCBaseline106this figure, a batch query size of one was employedin order to make the point as clearly as possible.Larger batch query sizes produce a graph with sim-ilar trends as do experiments involving larger Un-annotated sets and other active learners.100 200 400 800 1600 3200 6400QBU 76.26 86.11 90.63 92.27 93.67 94.65 95.42QBUV 76.65 85.09 89.75 92.24 93.72 94.96 95.60QBC 76.19 85.77 89.37 91.78 93.49 94.62 95.36Base 76.57 82.13 86.68 90.12 92.49 94.02 95.19Table 1.
The best models (on PTB WSJ data) with variousamounts of annotation (columns).Figure 6.
Start active learning as early as possible for ahead start.4.4 QBC ResultsAn important question to address for QBC iswhat number of committee members produces thebest results?
There was no significant difference inresults from the QBC experiments when using be-tween 3 and 7 committee members.
For brevity weomit the graph.4.5 QBU ResultsFor Query by Uncertainty, the experiment in Fig-ure 7 demonstrates that QBU is superior to QBUVfor low counts, but that QBUV slightly overtakesQBU beyond approximately 300 sentences.
In fact,all QBU variants, including the weighted version,surpassed the baseline.
WQBU has been omittedfrom the graph, as it was inferior to straight-forward QBU.4.6 Results on the BNCNext we introduce results on poetry from the Brit-ish National Corpus.
Recall that the feature setemployed by the MEMM tagger was optimized forperformance on the Wall Street Journal.
For theexperiment presented in Figure 8, all data in theTraining and Unannotated sets is from the BNC,but we employ the same feature set from the WSJexperiments.
This result on the BNC data showsfirst of all that tagging poetry with this taggerleaves a final shortfall of approximately 8% fromthe WSJ results.
Nonetheless and more importantly,the active learning trends observed on the WSJ stillhold.
QBC is better than the baseline, and QBUand QBUV trade off for first place.
Furthermore,for low numbers of sentences, it is overwhelminglyto one?s advantage to employ active learning forannotation.Figure 7.
QBUV is superior to QBU overall, but QBU isbetter for very low counts.
Both are superior to the ran-dom baseline and the Longest Sentence (LS) baseline.Figure 8.
Active learning results on the BNC poetry data.Accuracy of QBUV, QBU, and QBC against the randombaseline.
QBU and QBUV are nearly indistinguishable.010203040506070809010  100Accuracy(%)Number of Sentences in Training Set1%5%10%25%7580859095100  1000  10000Accuracy(%)Number of Sentences in Training SetQBUQBUVLSBaseline4045505560657075808590100  1000  10000Accuracy(%)Number of Sentences in Training SetQBUQBUVBaselineQBC1074.7 Another PerspectiveNext, briefly consider a different metric on the ver-tical axis.
In Figure 9, the metric is the total num-ber of words changed (corrected) by the oracle.This quantity reflects the cumulative number ofdifferences between the tagger?s hypothesis on asentence (at the point in time when the oracle isqueried) and the oracle?s answer (over the trainingset).
It corresponds roughly to the amount of timethat would be required for a human annotator tocorrect the tags suggested by the model.
This fig-ure reveals that QBUV makes significantly morechanges than QBU, QBC, or LS (the Longest Sen-tence baseline).
Hence, the superiority of QBUover QBUV, as measured by this metric, appears tooutweigh the small wins provided by QBUV whenmeasured by accuracy alone.
That said, the randombaseline makes the fewest changes of all.
If thismetric (and not some combination with accuracy)were our only consideration, then active learningwould appear not to serve our needs.This metric is also a measure of how well a par-ticular query algorithm selects sentences that espe-cially require assistance from the oracle.
In thissense, QBUV appears most effective.Figure 9.
Cumulative number of corrections made by theoracle for several competitive active learning algorithms.QBU requires fewer corrections than QBUV.5 ConclusionsActive learning is a viable way to accelerate theefficiency of a human annotator and is most effec-tive when done as early as possible.
We have pre-sented state-of-the-art tagging results using a frac-tion of the labeled data.
QBUV is a cheap approachto performing active learning, only to be surpassedby QBU when labeling small numbers of sentences.We are in the midst of conducting a user study toassess the true costs of annotating a sentence at atime or a word at a time.
We plan to incorporatethese specific costs into a model of cost measuredin time (or money) that will supplant the metricsreported here, namely accuracy and number ofwords corrected.
As noted earlier, future work willalso evaluate active learning at the granularity of aword or a subsequence of words, to be evaluatedby the cost metric.ReferencesAnderson, B., and Moore, A.
(2005).
?Active Learning for HMM:Objective Functions and Algorithms.?
ICML, Germany.Brants, T., (2000).
?TnT -- a statistical part-of-speech tagger.?
ANLP,Seattle, WA.Brill, E., and Wu, J.
(1998).
?Classifier combination for improvedlexical disambiguation.?
Coling/ACL, Montreal, Quebec, Canada.Pp.
191-195.Day, D., et al (1997).
?Mixed-Initiative Development of LanguageProcessing Systems.?
ANLP, Washington, D.C.Engelson, S. and Dagan, I.
(1996).
?Minimizing manual annotationcost in supervised training from corpora.?
ACL, Santa Cruz, Cali-fornia.
Pp.
319-326.Freund, Y., Seung, H., Shamir, E., and Tishby, N. (1997).
?Selectivesampling using the query by committee algorithm.?
MachineLearning, 28(2-3):133-168.Godbert, G. and Ramsay, J.
(1991).
?For now.?
In the British NationalCorpus file B1C.xml.
London: The Diamond Press (pp.
1-108).Hughes, T. (1982).
?Selected Poems.?
In the British National Corpusfile H8R.xml.
London: Faber & Faber Ltd. (pp.
35-235).Kupiec, J.
(1992).
?Robust part-of-speech tagging using a hiddenMarkov model.?
Computer Speech and Language 6, pp.
225-242.Lewis, D., and Catlett, J.
(1994).
?Heterogeneous uncertainty sam-pling for supervised learning.?
ICML.Lewis, D., and Gale, W. (1995).
?A sequential algorithm for trainingtext classifiers: Corrigendum and additional data.?
SIGIR Forum,29 (2), 13--19.Mann, G., and McCallum, A.
(2007).
"Efficient Computation of En-tropy Gradient for Semi-Supervised Conditional Random Fields".NAACL-HLT.Marcus, M. et al (1999).
?Treebank-3.?
Linguistic Data Consortium,Philadelphia, PA.Raiffa, H. and Schlaiffer, R. (1967).
Applied Statistical DecisionTheory.
New York: Wiley Interscience.Raine, C. (1984).
?Rich.?
In the British National Corpus file CB0.xml.London: Faber & Faber Ltd. (pp.
13-101).Ratnaparkhi, A.
(1996).
?A Maximum Entropy Model for Part-Of-Speech Tagging.?
EMNLP.Roy, N., and McCallum, A.
(2001a).
?Toward optimal active learningthrough sampling estimation of error reduction.?
ICML.Roy, N. and McCallum, A.
(2001b).
?Toward Optimal Active Learn-ing through Monte Carlo Estimation of Error Reduction.?
ICML,Williamstown.Seung, H., Opper, M., and Sompolinsky, H. (1992).
?Query by com-mittee?.
COLT.
Pp.
287-294.Thrun S., and Moeller, K. (1992).
?Active exploration in dynamicenvironments.?
NIPS.Toutanova, K., Klein, D., Manning, C., and Singer, Y.
(2003).
?Fea-ture-Rich Part-of-Speech Tagging with a Cyclic Dependency Net-work.?
HLT-NAACL.
Pp.
252-259.Toutanova, K. and Manning, C. (2000).
?Enriching the KnowledgeSources Used in a Maximum Entropy Part-of-Speech Tagger.
?EMNLP, Hong Kong.
Pp.
63-70.010002000300040005000600070008000900010000100  1000  10000NumberofChangedWordsNumber of Sentences in Training SetQBUVQBUQBCBaselineLS108
