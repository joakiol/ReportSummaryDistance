Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 987?997,Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational LinguisticsToward Future Scenario Generation: Extracting Event CausalityExploiting Semantic Relation, Context, and Association FeaturesChikara Hashimoto?
Kentaro Torisawa?
Julien Kloetzer?
Motoki Sano?Istva?n Varga?
Jong-Hoon Oh?
Yutaka Kidawara???
?
?
?
?
?
?National Institute of Information and Communications Technology, Kyoto, 619-0289, Japan?NEC Knowledge Discovery Research Laboratories, Nara, 630-0101, Japan{?
ch, ?
torisawa, ?
julien, ?
msano, ?
rovellia, ?
?kidawara}@nict.go.jpAbstractWe propose a supervised method ofextracting event causalities like conductslash-and-burn agriculture?exacerbatedesertification from the web using se-mantic relation (between nouns), context,and association features.
Experimentsshow that our method outperforms base-lines that are based on state-of-the-artmethods.
We also propose methods ofgenerating future scenarios like conductslash-and-burn agriculture?exacerbatedesertification?increase Asian dust (fromChina)?asthma gets worse.
Experi-ments show that we can generate 50,000scenarios with 68% precision.
We alsogenerated a scenario deforestation con-tinues?global warming worsens?seatemperatures rise?vibrio parahaemolyti-cus fouls (water), which is written in nodocument in our input web corpus crawledin 2007.
But the vibrio risk due to globalwarming was observed in Baker-Austinet al (2013).
Thus, we ?predicted?
thefuture event sequence in a sense.1 IntroductionThe world can be seen as a network of causal-ity where people, organizations, and other kindsof entities causally depend on each other.
Thisnetwork is so huge and complex that it is almostimpossible for humans to exhaustively predict theconsequences of a given event.
Indeed, after theGreat East Japan Earthquake in 2011, few ex-pected that it would lead to an enormous tradedeficit in Japan due to a sharp increase in en-ergy imports.
For effective decision making thatcarefully considers any form of future risks andchances, we need a system that helps humans doscenario planning (Schwartz, 1991), which is adecision-making scheme that examines possiblefuture events and assesses their potential chancesand risks.
Our ultimate goal is to develop a systemthat supports scenario planning through generat-ing possible future events using big data, whichwould contain what Donald Rumsfeld called ?un-known unknowns?1 (Torisawa et al, 2010).To this end, we propose a supervised methodof extracting such event causality as conductslash-and-burn agriculture?exacerbate desertifi-cation and use its output to generate future sce-narios (scenarios), which are chains of causal-ity that have been or might be observed inthis world like conduct slash-and-burn agricul-ture?exacerbate desertification?increase Asiandust (from China)?asthma gets worse.
Note that,in this paper, A?B denotes that A causes B, whichmeans that ?if A happens, the probability of B in-creases.?
Our notion of causality should be inter-preted probabilistically rather than logically.Our method extracts event causality based onthree assumptions that are embodied as featuresof our classifier.
First, we assume that two nouns(e.g.
slash-and-burn agriculture and desertifica-tion) that take some specific binary semantic rela-tions (e.g.
A CAUSES B) tend to constitute eventcausality if combined with two predicates (e.g.conduct and exacerbate).
Note that semantic re-lations are not restricted to those directly relevantto causality like A CAUSES B but can be those thatmight seem irrelevant to causality like A IS ANINGREDIENT FOR B (e.g.
plutonium and atomicbomb as in plutonium is stolen?atomic bomb ismade).
Our underlying intuition is the observationthat event causality tends to hold between two en-tities linked by semantic relations which roughlyentail that one entity strongly affects the other.Such semantic relations can be expressed by (oth-erwise unintuitive) patterns like A IS AN INGRE-DIENT FOR B.
As such, semantic relations like theMATERIAL relation can also be useful.
(See Sec-1http://youtu.be/GiPe1OiKQuk987tion 3.2.1 for a more intuitive explanation.
)Our second assumption is that there are gram-matical contexts in which event causality is morelikely to appear.
We implement what we con-sider likely contexts for event causality as con-text features.
For example, a likely context ofevent causality (underlined) would be: CO2 levelsrose, so climatic anomalies were observed, whilean unlikely context would be: It remains uncertainwhether if the recession is bottomed the decliningbirth rate is halted.
Useful context information in-cludes the mood of the sentences (e.g., the uncer-tainty mood expressed by uncertain above), whichis represented by lexical features (Section 3.2.2).The last assumption embodied in our associa-tion features is that each word of the cause phrasemust have a strong association (i.e., PMI, for ex-ample) with that of the effect phrase as slash-and-burn agriculture and desertification in the aboveexample, as in Do et al (2011).Our method exploits these features on top of ourbase features such as nouns and predicates.
Exper-iments using 600 million web pages (Akamine etal., 2010) show that our method outperforms base-lines based on state-of-the-art methods (Do et al,2011; Hashimoto et al, 2012) by more than 19%of average precision.We require that event causality be self-contained, i.e., intelligible as causality without thesentences from which it was extracted.
For ex-ample, omit toothbrushing?get a cavity is self-contained, but omit toothbrushing?get a girl-friend is not since this is not intelligible without acontext: He omitted toothbrushing every day andgot a girlfriend who was a dental assistant of den-tal clinic he went to for his cavity.
This is im-portant since future scenarios, which are gener-ated by chaining event causality as described be-low, must be self-contained, unlike Hashimoto etal.
(2012).
To make event causality self-contained,we wrote guidelines for manually annotating train-ing/development/test data.
Annotators regardedas event causality only phrase pairs that wereinterpretable as event causality without contexts(i.e., self-contained).
From the training data, ourmethod seemed to successfully learn what self-contained event causality is.Our scenario generation method generates sce-narios by chaining extracted event causality; gen-erating A?B?C from A?B and B?C.
The chal-lenge is that many acceptable scenarios are over-looked if we require the joint part of the chain (Babove) to be an exact match.
To increase the num-ber of acceptable scenarios, our method identifiescompatibility w.r.t causality between two phrasesby a recently proposed semantic polarity, exci-tation (Hashimoto et al, 2012), which properlyrelaxes the chaining condition (Section 3.1 de-scribes it).
For example, our method can iden-tify the compatibility between sea temperaturesare high and sea temperatures rise to chain globalwarming worsens?sea temperatures are highand sea temperatures rise?vibrio parahaemolyti-cus2 fouls (water).
Accordingly, we generateda scenario deforestation continues?global warm-ing worsens?sea temperatures rise?vibrio para-haemolyticus fouls (water), which is written inno document in our input web corpus that wascrawled in 2007, but the vibrio risk due to globalwarming has actually been observed in the Balticsea and reported in Baker-Austin et al (2013).
Ina sense, we ?predicted?
the event sequence re-ported in 2013 by documents written in 2007.
Ourexperiments also show that we generated 50,000scenarios with 68% precision, which include con-duct terrorist operations?terrorist bombing oc-curs?cause fatalities and injuries?cause eco-nomic losses and the above ?slash-and-burn agri-culture?
scenario (Section 5.2).
Neither is writtenin any document in our input corpus.In this paper, our target language is Japanese.However, we believe that our ideas and methodsare applicable to many languages.
Examples aretranslated into English for ease of explanation.Supplementary notes of this paper are availableat http://khn.nict.go.jp/analysis/member/ch/acl2014-sup.pdf.2 Related WorkFor event causality extraction, clues used byprevious methods can roughly be categorizedas lexico-syntactic patterns (Abe et al, 2008;Radinsky et al, 2012), words in context (Oh etal., 2013), associations among words (Torisawa,2006; Riaz and Girju, 2010; Do et al, 2011), andpredicate semantics (Hashimoto et al, 2012).
Be-sides features similar to those described above, wepropose semantic relation features3 that includethose that are not obviously related to causality.We show that such thorough exploitation of newand existing features leads to high performance.2A bacterium in the sea causing food-poisoning.3Radinsky et al (2012) and Tanaka et al (2012) used se-mantic relations to generalize acquired causality instances.988Other clues include shared arguments (Torisawa,2006; Chambers and Jurafsky, 2008; Chambersand Jurafsky, 2009), which we ignore since we tar-get event causality about two distinct entities.To the best of our knowledge, future scenariogeneration is a new task, although previous workshave addressed similar tasks (Radinsky et al,2012; Radinsky and Horvitz, 2013).
Neither in-volves chaining and restricts themselves to onlyone event causality step.
Besides, the events theypredict must be those for which similar eventshave previously been observed, and their methodonly applies to news domain.Some of the scenarios we generated are writtenon no page in our input web corpus.
Similarly,Tsuchida et al (2011) generated semantic knowl-edge like causality that is written in no sentence.However, their method cannot combine more thantwo pieces of knowledge unlike ours, and their tar-get knowledge consists of nouns, but ours consistsof verb phrases, which are more informative.Tanaka et al (2013)?s web information analy-sis system provides a what-happens-if QA service,which is based on our scenario generation method.3 Event Causality Extraction MethodThis section describes our event causality extrac-tion method.
Section 3.1 describes how to extractevent causality candidates, and Section 3.2 detailsour features.
Section 3.3 shows how to rank eventcausality candidates.3.1 Event Causality Candidate ExtractionWe extract the event causality between two eventsrepresented by two phrases from single sentencesthat are dependency parsed.4 We obtained sen-tences from 600 million web pages.
Each phrasein the event causality must consist of a predicatewith an argument position (template, hereafter)like conduct X and a noun like slash-and-burnagriculture that completes X.
We also require thepredicate of the cause phrase to syntactically de-pend on the effect phrase in the sentence fromwhich the event causality was extracted; we guar-antee this by verifying the dependencies of theoriginal sentence.
In Japanese, since the tempo-ral order between events is usually determined byprecedence in a sentence, we require the causephrase to precede the effect phrase.
For context4We used a Japanese dependency parser called J.DepP(Yoshinaga and Kitsuregawa, 2009), available at http://www.tkl.iis.u-tokyo.ac.jp/?ynaga/jdepp/.feature extraction, the event causality candidatesare accompanied by the original sentences fromwhich they were extracted.Excitation We only keep the event causalitycandidates each phrase of which consists of exci-tation templates, which have been shown to be ef-fective for causality extraction (Hashimoto et al,2012) and other semantic NLP tasks (Oh et al,2013; Varga et al, 2013; Kloetzer et al, 2013a).Excitation is a semantic property of templates thatclassifies them into excitatory, inhibitory, and neu-tral.
Excitatory templates such as cause X entailthat the function, effect, purpose or role of their ar-gument?s referent is activated, enhanced, or man-ifested, while inhibitory templates such as lowerX entail that it is deactivated or suppressed.
Neu-tral ones like proportional to X belong to neitherof them.
We collectively call both excitatory andinhibitory templates excitation templates.
We ac-quired 43,697 excitation templates by Hashimotoet al?s method and the manual annotation of exci-tation template candidates.5 We applied the exci-tation filter to all 272,025,401 event causality can-didates from the web and 132,528,706 remained.After applying additional filters (see Section Ain the supplementary notes) including those basedon a stop-word list and a causal connective listto remove unlikely event causality candidates thatare not removed by the above filter, we finally ac-quired 2,451,254 event causality candidates.3.2 Features for Event Causality Classifier3.2.1 Semantic Relation FeaturesWe hypothesize that two nouns with some particu-lar semantic relations are more likely to constituteevent causality.
Below we describe the semanticrelations that we believe are likely to constituteevent causality.CAUSATION is the causal relation between twoentities and is expressed by binary patterns likeA CAUSES B. Deforestation and global warmingmight complete the A and B slots.
We manuallycollected 748 binary patterns for this relation.
(SeeSection B in the supplementary notes for examplesof our binary patterns.
)MATERIAL is the relation between a materialand a product made of it (e.g.
plutonium and5Hashimoto et al?s method constructs a network of tem-plates based on their co-occurrence in web sentences with asmall number of polarity-assigned seed templates and infersthe polarity of all the templates in the network by a constraintsolver based on the spin model (Takamura et al, 2005).989atomic bomb) and can be expressed by A IS MADEOF B.
Its relation to event causality might seemunclear, but a material can be seen as a ?cause?of a product.
Indeed materials can participatein event causality with the help of such templatepairs as A is stolen?B is made as in plutonium isstolen?atomic bomb is made.
We manually col-lected 187 binary patterns for this relation.NECESSITY?s patterns include A IS NECES-SARY FOR B, which can be filled with verbal apti-tude and ability to think.
Noun pairs with this rela-tion can constitute event causality when combinedwith template pairs like improve A?cultivate B.We collected 257 patterns for this relation.USE is the relation between means (or instru-ments) and the purpose for using them.
A IS USEDFOR B is a pattern of the relation, which can befilled with e-mailer and exchanges of e-mail mes-sages.
Note that means can be seen as ?causing?or ?realizing?
the purpose of using the means inthis relation, and actually event causality can beobtained by incorporating noun pairs of this rela-tion into template pairs like activate A?conductB.
2,178 patterns were collected for this relation.PREVENTION is the relation expressed by pat-terns like A PREVENTS B, which can be filled withtoothbrushing and periodontal disease.
This rela-tion is, so to speak, ?negative CAUSATION?
sincethe entity denoted by the noun completing the Aslot makes the entity denoted by the B noun NOTrealized.
Such noun pairs mean event causalityby substituting them into template pairs like omitA?get B.
The number of patterns is 490.The experiments in Section 5.1.1 show that notonly CAUSATION and PREVENTION (?negativeCAUSATION?)
but the other relations are also ef-fective for event causality extraction.In addition, we invented the EXCITATION rela-tion that is expressed by binary patterns made ofexcitatory and inhibitory templates (Section 3.1).For instance, we make binary patterns A RISES Band A LOWERS B from excitatory template rise Xand inhibitory template lower X respectively.
TheEXCITATION relation roughly means that A acti-vates B (excitatory) or suppresses it (inhibitory).We simply add an additional argument position toeach template in the 43,697 excitation templates tomake binary patterns.
We restricted the argumentpositions (represented by Japanese postpositions)of the A slot to either ha (topic marker), ga (nomi-native), or de (instrumental) and those of the B slotto either ha, ga, de, wo (accusative), or ni (dative),SR1: Binary pattern of our semantic relations that co-occurs with two nouns of an event causality candi-date in our web corpus.SR2: Semantic relation types (e.g CAUSATION and EN-TAILMENT) of the binary pattern of SR1.
EXCITA-TION is divided into six sub types based on the ex-citation polarity of the binary patterns, the argumentpositions, and the existence of causative markers.
ACAUSATION pattern, B BY A, constitutes an indepen-dent relation called the BY relation.Table 1: Semantic relation features.and obtained 55,881 patterns.Moreover, for broader coverage, we acquiredbinary patterns that entail or are entailed by oneof the patterns of the above six semantic relations.Those patterns were acquired from our web cor-pus by Kloetzer et al (2013b)?s method, which ac-quired 185 million entailment pairs with 80% pre-cision from our web corpus and was used for con-tradiction acquisition (Kloetzer et al, 2013a).
Weacquired 335,837 patterns by this method.
Theyare class-dependent patterns, which have seman-tic class restrictions on arguments.
The semanticclasses were obtained from our web corpus basedon Kazama and Torisawa (2008).
See De Saegeret al (2009), De Saeger et al (2011) and Kloet-zer et al (2013a) for more on our patterns.
Theycollectively constitute the ENTAILMENT relation.Table 1 shows our semantic relation features.
Touse them, we first make a database that recordswhich noun pairs co-occur with each binary pat-tern.
Then we check a noun pair (the nouns of thecause and effect phrases) for each event causalitycandidate, and give the candidate all the patternsin the database that co-occur with the noun pair.3.2.2 Context FeaturesWe believe that contexts exist where event causal-ity candidates are more likely to appear, as de-scribed in Section 1.
We developed features thatcapture the characteristics of likely contexts forJapanese event causality (See Section C in the sup-plementary notes).
In a nutshell, they represent aconnective (C1 and C2 in Section C), the distancebetween the elements of event causality candidate(C3 and C4), words in context (C5 to C8), the ex-istence of adnominal modifier (9 to C10), and theexistence of additional arguments of cause and ef-fect predicates (C13 to C20), among others.3.2.3 Association FeaturesThese features measure the association strengthbetween slash-and-burn agriculture and deser-990AC1: The CEA value, the sum of AC2, AC3, and AC4.AC2: Do et al?s Spp.
This is the association measurebetween predicates, which is the product of AC5,AC6 and AC7 below.
They are calculated from the132,528,706 event causality candidates in Section3.1.
We omit Do et al?s Dist, which is a constantsince we set our window size to one.AC3: Do et al?s Spa.
This is the association measure be-tween arguments and predicates, which is the sumof AC8 and AC9.
They are calculated from the132,528,706 event causality candidates.AC4: Do et al?s Saa, which is PMI between arguments.We obtained it in the same way as Filter 5 in the sup-plementary notes.AC5: PMI between predicates.AC6 / AC7: Do et al?s max / IDF .AC8: PMI between a cause noun and an effect predicate.AC9: PMI between a cause predicate and an effect noun.Table 2: CEA-based association features.tification in conduct slash-and-burn agricul-ture?exacerbate desertification for instance andconsist of CEA-, Wikipedia-, definition-, and web-based features.
CEA-based features are basedon the Cause Effect Association (CEA) measureof Do et al (2011).
It consists of associationmeasures like PMI between arguments (nouns),between arguments and predicates, and betweenpredicates (Table 2).
Do et al used it (alongwith discourse relations) to extract event causality.Wikipedia-based features are the co-occurrencecounts and the PMI values between cause and ef-fect nouns calculated using Wikipedia (as of 2013-Sep-19).
We also checked whether an Wikipediaarticle whose title is a cause (effect) noun con-tains its effect (cause) noun, as detailed in SectionD.1 in the supplementary notes.
Definition-basedfeatures, as detailed in Section D.2 in the sup-plementary notes, resemble the Wikipedia-basedfeatures except that the information source is thedefinition sentences automatically acquired fromour 600 million web pages using the method ofHashimoto et al (2011).
Web-based featuresprovide association measures between nouns us-ing various window sizes in the 600 million webpages.
See Section D.3 for detail.
Web-based as-sociation measures were obtained from the samedatabase as AC4 in Table 2.3.2.4 Base FeaturesBase features represent the basic properties ofevent causality like nouns, templates, and their ex-citation polarities (See Section E in the supple-mentary notes).
For B3 and B4, 500 semanticclasses were obtained from our web corpus usingthe method of Kazama and Torisawa (2008).3.3 Event Causality ScoringUsing the above features, a classifier6 classifieseach event causality candidate into causality andnon-causality.
An event causality candidate isgiven a causality score CScore, which is the SVMscore (distance from the hyperplane) that is nor-malized to [0, 1] by the sigmoid function 11+e?x.Each event causality candidate may be given mul-tiple original sentences, since a phrase pair can ap-pear in multiple sentences, in which case it is givenmore than one SVM score.
For such candidates,we give the largest score and keep only one origi-nal sentence that corresponds to the largest score.7Original sentences are also used for scenario gen-eration, as described below.4 Future Scenario Generation MethodOur future scenario generation method createsscenarios by chaining event causalities.
A naiveapproach chains two phrase pairs by exact match-ing.
However, this approach would overlook manyacceptable scenarios as discussed in Section 1.
Forexample, global warming worsens?sea tempera-tures are high and sea temperatures rise?vibrioparahaemolyticus fouls (water) can be chained toconstitute an acceptable scenario, but the joint partis not the same string.
Note that the two phrasesare not simply paraphrases; temperatures may berising but remain cold, or they may be decreasingeven though they remain high.What characterizes two phrases that can be thejoint part of acceptable scenarios?
Although wehave no definite answer yet, we name it the causal-compatibility of two phrases and provide its pre-liminary characterization based on the excitationpolarity.
Remember that excitatory templates likecause X entail that X?s function or effect is acti-vated, but inhibitory templates like lower X entailthat it is suppressed (Section 3.1).
Two phrasesare causally-compatible if they mention the sameentity (typically described by a noun) that is pred-icated by the templates of the same excitation po-larity.
Indeed, both X rise and X are high are ex-citatory and hence sea temperatures are high andsea temperatures rise are causally-compatible.86We used SVMlight with the polynominal kernel (d = 2),available at http://svmlight.joachims.org.7Future work will exploit other original sentences, as sug-gested by an anonymous reviewer.8Using other knowledge like verb entailment (Hashimotoet al, 2009) can be helpful too, which is further future work.991Scenarios (scs) generated by chaining causally-compatible phrase pairs are scored by Score(sc),which embodies our assumption that an acceptablescenario consists of plausible event causality pairs:Score(sc) =?cs?CAUS(sc)CScore(cs)where CAUS(sc) is a set of event causalitypairs that constitutes sc and cs is a member ofCAUS(sc).
CScore(cs), which is cs?s score,was described in Section 3.3.Our method optionally applies the followingtwo filters to scenarios for better precision: Anoriginal sentence filter removes a scenario if twoevent causality pairs that are chained in it are ex-tracted from original sentences between which noword overlap exists other than words constitutingcausality pairs.
In this case, the two event causal-ity pairs tend to be about different topics and con-stitute an incoherent scenario.
A common argu-ment filter removes a scenario if a joint part con-sists of two templates that share no argument inour ?argument, template?
database, which is com-piled from the syntactic dependency data betweenarguments and templates extracted from our webcorpus.
Such a scenario tends to be incoherent too.5 Experiments5.1 Event Causality ExtractionNext we describe our experiments on event causal-ity extraction and show (a) that most of our fea-tures are effective and (b) that our method outper-forms the baselines based on state-of-the-art meth-ods (Do et al, 2011; Hashimoto et al, 2012).
Ourmethod achieved 70% precision at 13% recall; wecan extract about 69,700 event causality pairs with70% precision, as described below.For the test data, we randomly sampled 23,650examples of ?event causality candidate, origi-nal sentence?
among which 3,645 were positivefrom 2,451,254 event causality candidates ex-tracted from our web corpus (Section 3.1).
Forthe development data, we identically collected11,711 examples among which 1,898 were posi-tive.
These datasets were annotated by three anno-tators (not the authors), who annotated the eventcausality candidates without looking at the origi-nal sentences.
The final label was determined bymajority vote.
The training data were createdby the annotators through our preliminary experi-ments and consists of 112,110 among which 9,657Method Ave. prec.
(%)Proposed 46.27w/o Context features 45.68w/o Association features 45.66w/o Semantic relation features 44.44Base features only 41.29Table 3: Ablation tests.Semantic relations Ave. prec.
(%)All semantic relations (Proposed) 46.27CAUSATION 45.86CAUSATION and PREVENTION 45.78None (w/o Semantic relation features) 44.44Table 4: Ablation tests on semantic relations.were positive.
The Kappa (Fleiss, 1971) of theirjudgments was 0.67 (substantial agreement (Lan-dis and Koch, 1977)).
These three datasets haveno overlap in terms of phrase pairs.
About nineman-months were required to prepare the data.Our evaluation is based on average precision;9we believe that it is important to rank the plausibleevent causality candidates higher.5.1.1 Ablation TestsWe evaluated the features of our method by ab-lation tests.
Table 3 shows the results of remov-ing the semantic relation, the context, and the as-sociation features from our method.
All the fea-ture types are effective and contribute to the per-formance gain that was about 5% higher than theBase features only.
Proposed achieved 70% pre-cision at 13% recall.
We then estimated that, withthe precision rate, we can extract 69,700 eventcausality pairs from the 2,451,254 event causalitycandidates, among which the estimated number ofpositive examples is 377,794.Next we examined whether the semantic rela-tions that do not seem directly relevant to causalitylike MATERIAL are effective.
Table 4 shows thatthe performance degraded (46.27 ?
45.86) whenwe only used the CAUSATION binary patterns andtheir entailing and entailed patterns compared toProposed.
Even when adding the PREVENTION(?negative CAUSATION?)
patterns and their entail-ing and entailed patterns, the performance was stillslightly worse than Proposed.
The performancewas even worse when using no semantic relation(?None?
in Table 4).
Consequently we concludethat not only semantic relations directly relevant9It is obtained by computing the precision for each pointin the ranked list where we find a positive sample and aver-aging all the precision figures (Manning and Schu?tze, 1999).992Method Ave. prec.
(%)w/o Wikipedia-based features 46.52Proposed 46.27w/o definition-based features 46.21w/o Web-based features 46.15w/o CEA-based features 45.80Table 5: Ablation tests on association features.Method Ave. prec.
(%)Proposed 46.27Proposed-CEA 45.80CEAsup21.77CEAuns16.57Table 6: Average precision of our proposed meth-ods and baselines using CEA.to causality like CAUSATION but also those thatseem to lack direct relevance to causality like MA-TERIAL are somewhat effective.Finally, Table 5 shows the performance dropby removing the Wikipedia-, definition-, web-,and CEA-based features.
The CEA-based fea-tures were the most effective, while the Wikipedia-based ones slightly degraded the performance.5.1.2 Comparison to Baseline MethodsWe compared our method and two baselines basedon Do et al (2011): CEAunsis an unsupervisedmethod that uses CEA to rank event causality can-didates, and CEAsupis a supervised method us-ing SVM and the CEA features, whose ranking isbased on the SVM scores.
The baselines are notcomplete implementations of Do et al?s methodwhich uses discourse relations identified based onLin et al (2010) and exploits them with CEAwithin an ILP framework.
Nonetheless, we believethat this comparison is informative since CEA canbe seen as the main component; they achieved aF1 of 41.7% for extracting causal event relations,but with only CEA they still achieved 38.6%.Table 6 shows the average precision of the com-pared methods.
Proposed is our proposed method.Proposed-CEA is Proposed without the CEA-features and shows their contribution.
Proposedis the best and the CEA features slightly contributeto the performance, as Proposed-CEA indicates.We observed that CEAsupand CEAunsperformedpoorly and tended to favor event causality candi-dates whose phrase pairs were highly relevant toeach other but described the contrasts of eventsrather than event causality (e.g.
build a slow mus-cle and build a fast muscle) probably because their00.20.40.60.810  0.2  0.4  0.6  0.8  1PrecisionRecall		Figure 1: Precision-recall curves of proposedmethods and baselines using CEA.Method Ave. prec.
(%)Proposed 49.64Csuns30.38Cssup27.49Table 7: Average precision of our proposedmethod and baselines using Cs.main components are PMI values.
Figure 1 showstheir precision-recall curves.Next we compared our method with the base-lines based on Hashimoto et al (2012).
They de-veloped an automatic excitation template acqui-sition method that assigns each template an ex-citation value in range [?1, 1] that is positive ifthe template is excitatory and negative if it is in-hibitory.
They ranked event causality candidatesby Cs(p1, p2) = |s1| ?
|s2|, where p1and p2arethe two phrases of event causality candidates, and|s1| and |s2| are the absolute excitation values ofp1?s and p2?s templates.
The baselines are as fol-lows: Csunsis an unsupervised method that usesCs for ranking, and Cssupis a supervised methodusing SVM with Cs as the only feature that usesSVM scores for ranking.
Note that some eventcausality candidates were not given excitation val-ues for their templates, since some templates wereacquired by manual annotation without Hashimotoet al?s method.
To favor the baselines for fairness,the event causality candidates of the developmentand test data were restricted to those with excita-tion values.
Since Cssupperformed slightly betterwhen using all of the training data in our prelimi-nary experiments, we used all of it.Table 7 shows the average precision of the com-pared methods.
Proposed is our method.
Its av-erage precision is different from that in Table 6due to the difference in test data described above.Csunsand Cssupdid not perform well.
Many99300.20.40.60.810  0.2  0.4  0.6  0.8  1PrecisionRecall		Figure 2: Precision-recall curves of proposedmethods and baselines using Cs.phrase pairs described two events that often hap-pen in parallel but are not event causality (e.g.
re-duce the intake of energy and increase the energyconsumption) in the highly ranked event causalitycandidates of Csunsand Cssup.
Figure 2 showstheir precision-recall curves.Hashimoto et al (2012) extracted 500,000 eventcausalities with about 70% precision.
However, asdescribed in Section 1, our event causality crite-ria are different; since they regarded phrase pairsthat were not self-contained as event causality(their annotators checked the original sentences ofphrase pairs to see if they were event causality),their judgments tended to be more lenient thanours, which explains the performance difference.In preliminary experiments, since our proposedmethod?s performance degraded when Cs was in-corporated, we did not use it in our method.5.2 Future Scenario GenerationTo show that our future scenario generation meth-ods can generate many acceptable scenarios withreasonable precision, we experimentally com-pared four methods: Proposed, our scenariogeneration method without the two filters, Pro-posed+Orig, our method with the original sen-tence filter, Proposed+Orig+Comm, our methodwith the original sentence and common argumentfilters, and Exact, a method that chains eventcausality by exact matching.Beginning events As the beginning event of ascenario, we extracted nouns that describe socialproblems (social problem nouns, e.g.
deforesta-tion) from Wikipedia to focus our evaluation onthe ability to generate scenarios about them, whichis a realistic use-case of scenario generation.
Weextracted 557 social problem nouns and used thecause phrases of the event causality candidates thatTwo-step Three-stepExact 1,000 (44.10) 1,000 (23.50)Proposed 2,000 (32.25) 2,000 (12.55)Proposed+Orig 995 (36.28) 602 (17.28)Proposed+Orig+Comm 708 (38.70) 339 (17.99)Table 8: Number of scenario samples and theirprecision (%) in parentheses.consisted of one of the social problem nouns as thescenario?s beginning event.Event causality We applied our event causalityextraction method to 2,451,254 candidates (Sec-tion 3.1) and culled the top 1,200,000 phrase pairsfrom them (See Section F in the supplementarynotes for examples).
Some phrase pairs have thesame noun pairs and the same template polar-ity pairs (e.g.
omit toothbrushing?get a cavityand neglect toothbrushing?have a cavity, whereomit X and neglect X are inhibitory and get X andhave X are excitatory).
We removed such phrasepairs except those with the highest CScore, and960,561 phrase pairs remained, from which wegenerated two- or three-step scenarios that con-sisted of two or three phrase pairs.Evaluation samples The numbers of two- andthree-step scenarios generated by Proposed were217,836 and 5,288,352, while those of Exact were22,910 and 72,746.
We sampled 2,000 from Pro-posed?s two- and three-step scenarios and 1,000from those of Exact.
We applied the filters to thesampled scenarios of Proposed, and the resultswere regarded as the sample scenarios of Pro-posed+Orig and Proposed+Orig+Comm.
Table8 shows the number and precision of the samples.Note that, for the diversity of the sampled scenar-ios, our sampling proceeded as follows: (i) Ran-domly sample a beginning event phrase from thegenerated scenarios.
(ii) Randomly sample an ef-fect phrase for the beginning event phrase from thescenarios.
(iii) Regarding the effect phrase as acause phrase, randomly sample an effect phrasefor it, and repeat (iii) up to the specified numberof steps (2 or 3).
The samples were annotated bythree annotators (not the authors), who were in-structed to regard a sample as acceptable if eachevent causality that constitutes it is plausible andthe sample as a whole constitutes a single coherentstory.
Final judgment was made by majority vote.Fleiss?
kappa of their judgments was 0.53 (moder-ate agreement), which is lower than the kappa forthe causality judgment.
This is probably because994Two-step Three-stepExact 2,085 1,237Proposed 5,773 0Proposed+Orig 4,107 0Proposed+Orig+Comm 3,293 21,153Table 9: Estimated number of acceptable scenar-ios with a 70% precision rate.00.20.40.60.810  10000 20000 30000 40000 50000 60000 70000PrecisionEstimated number of acceptable scenariosFigure 3: Precision-scenario curves (2-step).scenario judgment requires careful considerationabout various possible futures for which individ-ual annotators tend to draw different conclusions.Result 1 Table 9 shows the estimated numberof acceptable scenarios generated with 70% pre-cision.
The estimated number is calculated as theproduct of the recall at 70% precision and thenumber of acceptable scenarios in all the gener-ated scenarios, which is estimated by the anno-tated samples.
Figures 3 and 4 show the precision-scenario curves for the two- and three-step sce-narios, which illustrate how many acceptable sce-narios can be generated with what precision.
Thecurve is drawn in the same way as the precision-recall curve except that the X-axis indicates theestimated number of acceptable scenarios.
At70% precision, all of the proposed methods out-performed Exact in the two-step setting, and Pro-posed+Orig+Comm outperformed Exact in thethree-step setting.Result 2 To evaluate the top-ranked scenariosof Proposed+Orig+Comm in the three-step set-ting with more samples, the annotators labeled 500samples from the top 50,000 of its output.
341(68.20%) were acceptable, and the estimated num-ber of acceptable scenarios at a precision rate of70% and 80% are 26,700 and 5,200 (See Section Hin the supplementary notes).
The ?terrorist oper-ations?
scenario and the ?slash-and-burn agricul-ture?
scenario in Section 1 were ranked 16,386th00.20.40.60.810  100000 200000 300000 400000 500000 600000 700000PrecisionEstimated number of acceptable scenariosFigure 4: Precision-scenario curves (3-step).and 21,968th.
Next we examined how many ofthe top 50,000 scenarios were acceptable and non-trivial, i.e., found in no page in our input web cor-pus, using the 341 acceptable samples.
A scenariowas regarded as non-trivial if its nouns co-occur inno page of the corpus.
22 among the 341 sampleswere non-trivial.
Accordingly, we estimate thatwe can generate 2,200 (50,000?22500) acceptable andnon-trivial scenarios from the top 50,000.
(SeeSection G in the supplementary notes for exam-ples of the generated scenarios.
)Discussion Scenario deforestation contin-ues?global warming worsens?sea temperaturesrise?vibrio parahaemolyticus fouls (water)was generated by Proposed+Orig+Comm.
Itis written in no page in our input web corpus,which was crawled in 2007.10 But we did finda paper Baker-Austin et al (2013) that observedthe emerging vibrio risk in the Baltic sea due toglobal warming.
In a sense, we ?predicted?
anevent observed in 2013 from documents writtenin 2007, although the scenario was ranked as lowas 240,738th.6 ConclusionWe proposed a supervised method for eventcausality extraction that exploits semantic rela-tion, context, and association features.
We alsoproposed methods for our new task, future sce-nario generation.
The methods chain event causal-ity by causal-compatibility.
We generated non-trivial scenarios with reasonable precision, and?predicted?
future events from web documents.Increasing their rank is future work.10The corpus has pages where global warming, sea tem-peratures, and vibrio parahaemolyticus happen to co-occur.But they are either diaries where the three words appear sep-arately in different topics or lists of arbitrary words.995ReferencesShuya Abe, Kentaro Inui, and Yuji Matsumoto.
2008.Two-phrased event relation acquisition: Couplingthe relation-oriented and argument-oriented ap-proaches.
In Proceedings of the 22nd InternationalConference on Computational Linguistics (COLING2008), pages 1?8.Susumu Akamine, Daisuke Kawahara, YoshikiyoKato, Tetsuji Nakagawa, Yutaka I. Leon-Suematsu,Takuya Kawada, Kentaro Inui, Sadao Kurohashi,and Yutaka Kidawara.
2010.
Organizing informa-tion on the web to support user judgments on in-formation credibility.
In Proceedings of 2010 4thInternational Universal Communication SymposiumProceedings (IUCS 2010), pages 122?129.Craig Baker-Austin, Joaquin A. Trinanes, Nick G. H.Taylor, Rachel Hartnell, Anja Siitonen, and JaimeMartinez-Urtaza.
2013.
Emerging vibrio risk athigh latitudes in response to ocean warming.
NatureClimate Change, 3:73?77.Nathanael Chambers and Dan Jurafsky.
2008.
Unsu-pervised learning of narrative event chains.
In Pro-ceedings of the 48th Annual Meeting of the Asso-ciation of Computational Linguistics: Human Lan-guage Technologies (ACL-08: HLT), pages 789?797.Nathanael Chambers and Dan Jurafsky.
2009.
Unsu-pervised learning of narrative schemas and their par-ticipants.
In Proceedings of the 47th Annual Meet-ing of the ACL and the 4th IJCNLP of the AFNLP(ACL-IJCNLP 2009), pages 602?610.Stijn De Saeger, Kentaro Torisawa, Jun?ichi Kazama,Kow Kuroda, and Masaki Murata.
2009.
Largescale relation acquisition using class dependent pat-terns.
In Proceedings of the IEEE InternationalConference on Data Mining (ICDM 2009), pages764?769.Stijn De Saeger, Kentaro Torisawa, Masaaki Tsuchida,Jun?ichi Kazama, Chikara Hashimoto, Ichiro Ya-mada, Jong-Hoon Oh, Istva?n Varga, and Yulan Yan.2011.
Relation acquisition using word classes andpartial patterns.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Pro-cessing (EMNLP 2011), pages 825?835.Quang Xuan Do, Yee Seng Chan, and Dan Roth.
2011.Minimally supervised event causality identification.In Proceedings of the 2011 Conference on EmpiricalMethods in Natural Language Processing (EMNLP2011), pages 294?303.Joseph L. Fleiss.
1971.
Measuring nominal scaleagreement among many raters.
Psychological Bul-letin, 76(5):378?382.Chikara Hashimoto, Kentaro Torisawa, Kow Kuroda,Masaki Murata, and Jun?ichi Kazama.
2009.
Large-scale verb entailment acquisition from the web.
InProceedings of EMNLP 2009: Conference on Em-pirical Methods in Natural Language Processing,pages 1172?1181.Chikara Hashimoto, Kentaro Torisawa, StijnDe Saeger, Jun?ichi Kazama, and Sadao Kuro-hashi.
2011.
Extracting paraphrases from definitionsentences on the web.
In Proceedings of the 49thAnnual Meeting of the Association for Computa-tional Linguistics: Human Language Technologies,pages 1087?1097.Chikara Hashimoto, Kentaro Torisawa, Stijn DeSaeger, Jong-Hoon Oh, and Jun?ichi Kazama.
2012.Excitatory or inhibitory: A new semantic orienta-tion extracts contradiction and causality from theweb.
In Proceedings of EMNLP-CoNLL 2012: Con-ference on Empirical Methods in Natural LanguageProcessing and Natural Language Learning, pages619?630.Jun?ichi Kazama and Kentaro Torisawa.
2008.
Induc-ing gazetteers for named entity recognition by large-scale clustering of dependency relations.
In Pro-ceedings of the 46th Annual Meeting of the Associ-ation for Computational Linguistics: Human Lan-guage Technologies (ACL-08: HLT), pages 407?415.Julien Kloetzer, Stijn De Saeger, Kentaro Torisawa,Chikara Hashimoto, Jong-Hoon Oh, and KiyonoriOhtake.
2013a.
Two-stage method for large-scaleacquisition of contradiction pattern pairs using en-tailment.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing(EMNLP 2013), pages 693?703.Julien Kloetzer, Kentaro Torisawa, Stijn De Saeger,Motoki Sano, Chikara Hashimoto, and Jun Gotoh.2013b.
Large-scale acquisition of entailment patternpairs.
In Information Processing Society of Japan(IPSJ) Kansai-Branch Convention 2013.J.
Richard Landis and Gary G. Koch.
1977.
The mea-surement of observer agreement for categorical data.Biometrics, 33(1):159?174.Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2010.
Apdtb-styled end-to-end discourse parser.
Technicalreport, School of Computing, National University ofSingapore.Chris Manning and Hinrich Schu?tze.
1999.
Foun-dations of Statistical Natural Language Processing.MIT Press.Jong-Hoon Oh, Kentaro Torisawa, Chikara Hashimoto,Motoki Sano, Stijn De Saeger, and Kiyonori Ohtake.2013.
Why-question answering using intra- andinter-sentential causal relations.
In Proceedings ofthe 51st Annual Meeting of the Association for Com-putational Linguistics (ACL 2013), pages 1733?1743.996Kira Radinsky and Eric Horvitz.
2013.
Mining theweb to predict future events.
In Proceedings of SixthACM International Conference on Web Search andData Mining (WSDM 2013), pages 255?264.Kira Radinsky, Sagie Davidovich, and ShaulMarkovitch.
2012.
Learning causality for newsevents prediction.
In Proceedings of InternationalWorld Wide Web Conference 2012 (WWW 2012),pages 909?918.Mehwish Riaz and Roxana Girju.
2010.
Another lookat causality: Discovering scenario-specific contin-gency relationships with no supervision.
In 2010IEEE Fourth International Conference on SemanticComputing, pages 361?368.Peter Schwartz.
1991.
The Art of the Long View.
Dou-bleday.Hiroya Takamura, Takashi Inui, and Manabu Okumura.2005.
Extracting semantic orientation of words us-ing spin model.
In Proceedings of the 43rd AnnualMeeting of the Association for Computational Lin-guistics (ACL 2005), pages 133?140.Shohei Tanaka, Naoaki Okazaki, and Mitsuru Ishizuka.2012.
Acquiring and generalizing causal inferencerules from deverbal noun constructions.
In Proceed-ings of 24th International Conference on Compu-tational Linguistics (COLING 2012), pages 1209?1218.Masahiro Tanaka, Stijn De Saeger, Kiyonori Ohtake,Chikara Hashimoto, Makoto Hijiya, Hideaki Fujii,and Kentaro Torisawa.
2013.
WISDOM2013: Alarge-scale web information analysis system.
InCompanion Volume of the Proceedings of the 6th In-ternational Joint Conference on Natural LanguageProcessing (IJCNLP 2013) (Demo Track), pages45?48.Kentaro Torisawa, Stijn de Saeger, Jun?ichi Kazama,Asuka Sumida, Daisuke Noguchi, Yasunari Kak-izawa, Masaki Murata, Kow Kuroda, and Ichiro Ya-mada.
2010.
Organizing the web?s information ex-plosion to discover unknown unknowns.
New Gen-eration Computing (Special Issue on InformationExplosion), 28(3):217?236.Kentaro Torisawa.
2006.
Acquiring inference ruleswith temporal constraints by using japanese coordi-nated sentences and noun-verb co-occurrences.
InProceedings of the Human Language TechnologyConference of the North American Chapter of theACL (HLT-NAACL2006), pages 57?64.Masaaki Tsuchida, Kentaro Torisawa, Stijn DeSaeger, Jong Hoon Oh, Jun?ichi Kazama, ChikaraHashimoto, and Hayato Ohwada.
2011.
Towardfinding semantic relations not written in a single sen-tence: An inference method using auto-discoveredrules.
In Proceedings of the 5th International JointConference on Natural Language Processing (IJC-NLP 2011), pages 902?910.Istva?n Varga, Motoki Sano, Kentaro Torisawa, ChikaraHashimoto, Kiyonori Ohtake, Takao Kawai, Jong-Hoon Oh, and Stijn De Saeger.
2013.
Aid is outthere: Looking for help from tweets during a largescale disaster.
In Proceedings of the 51st AnnualMeeting of the Association for Computational Lin-guistics (ACL 2013), pages 1619?1629.Naoki Yoshinaga and Masaru Kitsuregawa.
2009.Polynomial to linear: Efficient classification withconjunctive features.
In Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP 2009), pages 542?1551.997
