Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 266?270,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsRelation Guided Bootstrapping of Semantic LexiconsTara McIntosh?
Lars Yencken?
James R. Curran?
Timothy Baldwin??
NICTA, Victoria Research Lab ?
School of Information TechnologiesDept.
of Computer Science and Software Engineering The University of SydneyThe University of Melbournenlp@taramcintosh.org james@it.usyd.edu.aulars@yencken.org tb@ldwin.netAbstractState-of-the-art bootstrapping systems rely onexpert-crafted semantic constraints such asnegative categories to reduce semantic drift.Unfortunately, their use introduces a substan-tial amount of supervised knowledge.
Wepresent the Relation Guided Bootstrapping(RGB) algorithm, which simultaneously ex-tracts lexicons and open relationships to guidelexicon growth and reduce semantic drift.This removes the necessity for manually craft-ing category and relationship constraints, andmanually generating negative categories.1 IntroductionMany approaches to extracting semantic lexiconsextend the unsupervised bootstrapping framework(Riloff and Shepherd, 1997).
These use a small setof seed examples from the target lexicon to identifycontextual patterns which are then used to extractnew lexicon items (Riloff and Jones, 1999).Bootstrappers are prone to semantic drift, causedby selection of poor candidate terms or patterns(Curran et al, 2007), which can be reduced bysemantically constraining the candidates.
Multi-category bootstrappers, such as NOMEN (Yangar-ber et al, 2002) and WMEB (McIntosh and Curran,2008), reduce semantic drift by extracting multiplecategories simultaneously in competition.The inclusion of manually-crafted negative cate-gories to multi-category bootstrappers achieves thebest results, by clarifying the boundaries betweencategories (Yangarber et al, 2002).
For exam-ple, female names are often bootstrapped withthe negative categories flowers (e.g.
Rose, Iris)and gem stones (e.g.
Ruby, Pearl) (Curran et al,2007).
Unfortunately, negative categories are dif-ficult to design, introducing a substantial amountof human expertise into an otherwise unsupervisedframework.
McIntosh (2010) made some progresstowards automatically learning useful negative cate-gories during bootstrapping.In this work we identify an unsupervised sourceof semantic constraints inspired by the Coupled Pat-tern Learner (CPL, Carlson et al (2010)).
In CPL,relation bootstrapping is coupled with lexicon boot-strapping in order to control semantic drift in thetarget relation?s arguments.
Semantic constraintson categories and relations are manually crafted inCPL.
For example, a candidate of the relation IS-CEOOF will only be extracted if its arguments canbe extracted into the ceo and company lexiconsand a ceo is constrained to not be a celebrityor politician.
Negative examples such as IS-CEOOF(Sergey Brin, Google) are also introduced toclarify boundary conditions.
CPL employs a largenumber of these manually-crafted constraints to im-prove precision at the expense of recall (only 18 IS-CEOOF instances were extracted).
In our approach,we exploit open relation bootstrapping to minimisesemantic drift, without any manual seeding of rela-tions or pre-defined category lexicon combinations.Orthogonal to these seeded and constraint-basedmethods is the relation-independent Open Informa-tion Extraction (OPENIE) paradigm.
OPENIE sys-tems, such as TEXTRUNNER (Banko et al, 2007),define neither lexicon categories nor predefined re-lationships.
They extract relation tuples by exploit-266ing broad syntactic patterns that are likely to indi-cate relations.
This enables the extraction of inter-esting and unanticipated relations from text.
How-ever these patterns are often too broad, resulting inthe extraction of tuples that do not represent rela-tions at all.
As a result, heavy (supervised) post-processing or use of supervised information is nec-essary.
For example, Christensen et al (2010) im-prove TEXTRUNNER precision by using deep pars-ing information via semantic role labelling.2 Relation Guided BootstrappingRather than relying on manually-crafted categoryand relation constraints, Relation Guided Bootstrap-ping (RGB) automatically detects, seeds and boot-straps open relations between the target categories.These relations anchor categories together, e.g.
IS-CEOOF and ISFOUNDEROF anchor person andcompany, preventing them from drifting into othercategories.
Relations can also identify new terms.We demonstrate that this relation guidance effec-tively reduces semantic drift, with performance ap-proaching manually-crafted constraints.RGB can be applied to any multi-category boot-strapper, and in these experiments we use WMEB(McIntosh and Curran, 2008), as shown in Figure 1.RGB alternates between two phases of WMEB, onefor terms and the other for relations, with a one-offrelation discovery phase in between.Term ExtractionThe first stage of RGB follows the term extractionprocess of WMEB.
Each category is initialised by aset of hand-picked seed terms.
In each iteration, acategory?s terms are used to identify candidate pat-terns that can match the terms in the text.
Seman-tic drift is reduced by forcing the categories to bemutually exclusive (i.e.
patterns must be nominatedby only one category).
The remaining patterns areranked according to reliability and relevance, andthe top-n patterns are then added to the pattern set.1The reliability of a pattern for a given category isthe number of extracted terms in the category?s lex-icon that match the pattern.
A pattern?s relevanceweight is defined as the sum of the ?2 values be-tween the pattern (p) and each of the lexicon terms1In this work, n is set to 5.WMEBWMEBlexiconPersonget patternsget termslexiconCompanyget patternsget termsrelationget patternsget tuples?
??
?arg ?arg ?relationdiscoveryLee Scott, WalmartSergey Brin, GoogleJoe Bloggs, WalmartTermextractionRelationextractionFigure 1: Relation Guided Bootstrapping framework(t): weight(p) =?t?T ?2(p, t).
These metrics aresymmetrical for both candidate terms and pattern.In WMEB?s term selection phase, a category?s pat-tern set is used to identify candidate terms.
Like thecandidate patterns, terms matching multiple cate-gories are excluded.
The remaining terms are rankedand the top-n terms are added to the lexicon.Relation DiscoveryIn CPL (Carlson et al, 2010), a relation is instanti-ated with manually-crafted seed tuples and patterns.In RGB, the relations and their seeds are automati-cally identified in relation discovery.
Relation dis-covery is only performed once after the first 20 iter-ations of term extraction, which ensures the lexiconshave adequate coverage to form potential relations.Each ordered pair of categories (C1, C2) = R1,2is checked for open (not pre-defined) relations be-tween C1 and C2.
This check removes all pairs ofterms, tuples (t1, t2) ?
C1 ?
C2 with freq(t1, t2) <5 and a cooccurrence score ?2(t1, t2) ?
0.2 If R1,2has fewer than 10 remaining tuples, it is discarded.The tuples for R1,2 are then used to find its ini-tial set of relation patterns.
Each pattern must matchmore than one tuple and must be mutually exclusivebetween the relations.
If fewer than n relation pat-terns are found forR1,2, it is discarded.
At this stage2This cut-off is used as the ?2 statistic is sensitive to lowfrequencies.267TYPE 5gm 5gm + 4gm 5gm + DCTerms 1 347 002Patterns 4 090 412Tuples 2 114 243 3 470 206 14 369 673Relation Patterns 5 523 473 10 317 703 31 867 250Table 1: Statistics of three filtered MEDLINE datasetswe have identified the open relations that link cate-gories together and their initial extraction patterns.Using the initial relation patterns, the top-n mu-tually exclusive seed tuples are identified for the re-lation R1,2.
In CPL, these tuple seeds are manuallycrafted.
Note that R1,2 can represent multiple rela-tions betweenC1 andC2, which may not apply to allof the seeds, e.g.
isCeoOf and isEmployedBy.We discover two types of relations, inter-categoryrelations where C1 6= C2, and intra-category rela-tions where C1 = C2.Relation ExtractionThe relation extraction phase involves runningWMEB over tuples rather than terms.
If multiple re-lations are found, e.g.
R1,2 and R2,3, these are boot-strapped simultaneously, competing with each otherfor tuples and relation patterns.
Mutual exclusionconstraints between the relations are also forced.In each iteration, a relation?s set of tuples is usedto identify candidate relation patterns, as for termextraction.
The top-n non-overlapping patterns areextracted for each relation, and are used to identifythe top-n candidate tuples.
The tuples are scoredsimilarly to the relation patterns, and any tuple iden-tified by multiple relations is excluded.For tuple extraction, a relation R1,2 is constrainedto only consider candidates where either t1 or t2has previously been extracted into C1 or C2, respec-tively.
To extract a candidate tuple with an unknownterm, the term must also be a valid candidate of itsassociated category.
That is, the term must matchat least one pattern assigned to the category and notmatch patterns assigned to another category.This type-checking anchors relations to the cat-egories they link together, limiting their drift intoother relations.
It also provides guided term growthin the categories they link.
The growth is ?guided?because the relations define, semantically coher-ent subregions of the category search spaces.
Forexample, ISCEOOF defines the subregion ceoCAT DESCRIPTIONANTI Antibodies: MAb IgG IgM rituximab infliximabCELL Cells: RBC HUVEC BAEC VSMC SMCCLNE Cell lines: PC12 CHO HeLa Jurkat COSDISE Diseases: asthma hepatitis tuberculosis HIV malariaDRUG Drugs: acetylcholine carbachol heparin penicillintetracyclinFUNC Molecular functions and processes:kinase ligase acetyltransferase helicase bindingMUTN Mutations: Leiden C677T C282Y 35delG nullPROT Proteins and genes: p53 actin collagen albumin IL-6SIGN Signs and symptoms: anemia cough feverhypertension hyperglycemiaTUMR Tumors: lymphoma sarcoma melanomaneuroblastoma osteosarcomaTable 2: The MEDLINE semantic categorieswithin person.
This guidance reduces semanticdrift.3 Experimental SetupTo compare the effectiveness of RGB we considerthe task of extracting biomedical semantic lexi-cons, building on the work of McIntosh and Curran(2008).
Note however the method is equally appli-cable to any corpus and set of semantic categories.The corpus consists of approximately 18.5 mil-lion MEDLINE abstracts (up to Nov 2009).
The textwas tokenised and POS-tagged using bio-specificNLP tools (Grover et al, 2006), and parsed usingthe biomedical C&C CCG parser (Rimell and Clark,2009; Clark and Curran, 2007).The term extraction data is formed from the raw5-grams (t1, t2, t3, t4, t5), where the set of candi-date terms correspond to the middle tokens (t3) andthe patterns are formed from the surrounding tokens(t1, t2, t4, t5).
The relation extraction data is alsoformed from the 5-grams.
The candidate tuples cor-respond to the tokens (t1, t5) and the patterns areformed from the intervening tokens (t2, t3, t4).The second relation dataset (5gm + 4gm), also in-cludes length 2 patterns formed from 4-grams.
Thefinal relation dataset (5gm + DC) includes depen-dency chains up to length 5 as the patterns betweenterms (Greenwood et al, 2005).
These chains areformed using the Stanford dependencies generatedby the Rimell and Clark (2009) parser.
All candi-dates occurring less than 10 times were filtered.
Thesizes of the resulting datasets are shown in Table 1.2681-500 501-1000 1-1000WMEB 76.1 56.4 66.3+negative 86.9 68.7 77.8intra-RGB 75.7 62.7 69.2+negative 87.4 72.4 79.9inter-RGB 80.5 69.9 75.1+negative 87.7 76.4 82.0mixed-RGB 74.7 69.9 72.3+negative 87.9 73.5 80.7Table 3: Performance comparison of WMEB and RGBWe follow McIntosh and Curran (2009) in us-ing the 10 biomedical semantic categories andtheir hand-picked seeds in Table 2, and manu-ally crafted negative categories: amino acid,animal, body part and organism.
Our eval-uation process involved manually judging each ex-tracted term and we calculate the average precisionof the top-1000 terms over the 10 target categories.We do not calculate recall, due to the open-endednature of the categories.4 Results and DiscussionTable 3 compares the performance of WMEB andRGB, with and without the negative categories.
ForRGB, we compare intra-, inter- and mixed relationtypes, and use the 5gm format of tuples and relationpatterns.
In WMEB, drift dominates in the later iter-ations with ?19% precision drop between the firstand last 500 terms.
The manually-crafted negativecategories give a substantial boost in precision onboth the first and last 500 terms (+11.5% overall).Over the top 1000 terms, RGB significantly out-performs the corresponding WMEB with and with-out negative categories (p < 0.05).3 In particu-lar, inter-RGB significantly improves upon WMEBwith no negative categories (501-1000: +13.5%,1-1000: +8.8%).
In similar experiments, NEG-FINDER, used during bootstrapping, was shown toincrease precision by ?5% (McIntosh, 2010).
Inter-RGB without negatives approaches the precision ofWMEB with the negatives, trailing only by 2.7%overall.
This demonstrates that RGB effectively re-duces the reliance on manually-crafted negative cat-egories for lexicon bootstrapping.The use of intra-category relations was far less3Significance was tested using intensive randomisation tests.INTER-RGB 1-500 501-1000 1-10005gm 80.5 69.9 75.1+negative 87.7 76.4 82.05gm + 4gm 79.6 71.5 75.5+negative 87.7 76.1 81.95gm + DC 77.2 70.1 73.5+negative 86.6 80.2 83.5Table 4: Comparison of different relation pattern typeseffective than inter-category relations, and the com-bination of intra- and inter- was less effective thanjust using inter-category relations.
In intra-RGB thecategories are more susceptible to single-categorydrift.
The additional constraints provided by anchor-ing two categories appear to make inter-RGB lesssusceptible to drift.
Many intra-category relationsrepresent listings commonly identified by conjunc-tions.
However, these patterns are identified by mul-tiple intra-category relations and are excluded.Through manual inspection of inter-RGB?s tuplesand patterns, we identified numerous meaningful re-lations, such as isExpressedIn(prot, cell).Relations like this helped to reduce semantic driftwithin the CELL lexicon by up to 23%.Table 4 compares the effect of different relationpattern representations on the performance of inter-RGB.
The 5gm+4gm data, which doubles the num-ber of possible candidate relation patterns, performssimilarly to the 5gm representation.
Adding depen-dency chains decreased and increased precision de-pending on whether negative categories were used.In Wu and Weld (2010), the performance of anOPENIE system was significantly improved by us-ing patterns formed from dependency parses.
How-ever in our DC experiments, the earlier bootstrap-ping iterations were less precise than the simple5gm+4gm and 5gm representations.
Since thechains can be as short as two dependencies, someof these patterns may not be specific enough.
Theseresults demonstrate that useful open relations can berepresented using only n-grams.5 ConclusionIn this paper, we have proposed Relation GuidedBootstrapping (RGB), an unsupervised approach todiscovering and seeding open relations to constrainsemantic lexicon bootstrapping.269Previous work used manually-crafted lexical andrelation constraints to improve relation extraction(Carlson et al, 2010).
We turn this idea on its head,by using open relation extraction to provide con-straints for lexicon bootstrapping, and automaticallydiscover the open relations and their seeds from theexpanding bootstrapped lexicons.RGB effectively reduces semantic drift deliveringperformance comparable to state-of-the-art systemsthat rely on manually-crafted negative constraints.AcknowledgementsWe would like to thank Dr Cassie Thornley, our sec-ond evaluator, and the reviewers for their helpfulfeedback.
NICTA is funded by the Australian Gov-ernment as represented by the Department of Broad-band, Communications and the Digital Economyand the Australian Research Council through theICT Centre of Excellence program.
This work hasbeen supported by the Australian Research Councilunder Discovery Project DP1097291 and the CapitalMarkets Cooperative Research Centre.ReferencesMichele Banko, Michael J Cafarella, Stephen Soderland,Matt Broadhead, and Oren Etzioni.
2007.
Open in-formation extraction from the web.
In Proceedings ofthe 20th International Joint Conference on ArtificialIntelligence, pages 2670?2676, Hyderabad, India.Andrew Carlson, Justin Betteridge, Richard C. Wang, Es-tevam R. Hruschka, Jr., and Tom M. Mitchell.
2010.Coupled semi-supervised learning for information ex-traction.
In Proceedings of the Third ACM Interna-tional Conference on Web Search and Data Mining,pages 101?110, New York, USA.Janara Christensen, Mausam, Stephen Soderland, andOren Etzioni.
2010.
Semantic role labeling foropen information extraction.
In Proceedings of theNAACL HLT 2010 First International Workshop onFormalisms and Methodology for Learning by Read-ing, pages 52?60, Los Angeles, California, USA, June.Stephen Clark and James R. Curran.
2007.
Wide-coverage efficient statistical parsing with ccg and log-linear models.
Computational Linguistics, 33(4):493?552.James R. Curran, Tara Murphy, and Bernhard Scholz.2007.
Minimising semantic drift with mutual exclu-sion bootstrapping.
In Proceedings of the 10th Con-ference of the Pacific Association for ComputationalLinguistics, pages 172?180, Melbourne, Australia.Mark A. Greenwood, Mark Stevenson, Yikun Guo, HenkHarkema, and Angus Roberts.
2005.
Automaticallyacquiring a linguistically motivated genic interactionextraction system.
In Proceedings of the 4th Learn-ing Language in Logic Workshop, pages 46?52, Bonn,Germany.Claire Grover, Michael Matthews, and Richard Tobin.2006.
Tools to address the interdependence betweentokenisation and standoff annotation.
In Proceed-ings of the 5th Workshop on NLP and XML: Multi-Dimensional Markup in Natural Language Process-ing, pages 19?26, Trento, Italy.Tara McIntosh and James R. Curran.
2008.
Weightedmutual exclusion bootstrapping for domain indepen-dent lexicon and template acquisition.
In Proceedingsof the Australasian Language Technology AssociationWorkshop, pages 97?105, Hobart, Australia.Tara McIntosh and James R. Curran.
2009.
Reducingsemantic drift with bagging and distributional similar-ity.
In Proceedings of the 47th Annual Meeting of theAssociation for Computational Linguistics and the 4thInternational Conference on Natural Language Pro-cessing of the Asian Federation of Natural LanguageProcessing, pages 396?404, Suntec, Singapore, Au-gust.Tara McIntosh.
2010.
Unsupervised discovery of neg-ative categories in lexicon bootstrapping.
In Pro-ceedings of the 2010 Conference on Empirical Meth-ods in Natural Language Processing, pages 356?365,Boston, USA.Ellen Riloff and Rosie Jones.
1999.
Learning dictionar-ies for information extraction by multi-level bootstrap-ping.
In Proceedings of the 16th National Conferenceon Artificial Intelligence and the 11th Innovative Ap-plications of Artificial Intelligence Conference, pages474?479, Orlando, USA.Ellen Riloff and Jessica Shepherd.
1997.
A corpus-basedapproach for building semantic lexicons.
In Proceed-ings of the Second Conference on Empirical Meth-ods in Natural Language Processing, pages 117?124,Providence, USA.Laura Rimell and Stephen Clark.
2009.
Porting alexicalized-grammar parser to the biomedical domain.Journal of Biomedical Informatics, pages 852?865.Fei Wu and Daniel S. Weld.
2010.
Open informationextraction using wikipedia.
In Proceedings of the 48thAnnual Meeting of the Association of ComputationalLinguistics, pages 118?127, Uppsala, Sweden.Roman Yangarber, Winston Lin, and Ralph Grishman.2002.
Unsupervised learning of generalized names.
InProceedings of the 19th International Conference onComputational Linguistics, pages 1135?1141, Taipei,Taiwan.270
