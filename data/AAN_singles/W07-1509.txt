Proceedings of the Linguistic Annotation Workshop, pages 53?56,Prague, June 2007. c?2007 Association for Computational LinguisticsSemi-Automated Named Entity AnnotationKuzman Ganchev and Fernando PereiraComputer and Information Science,University of Pennsylvania,Philadelphia PA{ kuzman and pereira } @cis.upenn.eduMark MandelLinguistic Data Consortium,University of Pennsylvania, Philadelphia PAmamandel@ldc.upenn.eduSteven Carroll and Peter WhiteDivision of Oncology, Children?s Hospital of Philadelphia Philadelphia PA{ carroll and white }@genome.chop.eduAbstractWe investigate a way to partially automatecorpus annotation for named entity recogni-tion, by requiring only binary decisions froman annotator.
Our approach is based on a lin-ear sequence model trained using a k-bestMIRA learning algorithm.
We ask an an-notator to decide whether each mention pro-duced by a high recall tagger is a true men-tion or a false positive.
We conclude that ourapproach can reduce the effort of extendinga seed training corpus by up to 58%.1 IntroductionSemi-automated text annotation has been the subjectof several previous studies.
Typically, a human an-notator corrects the output of an automatic system.The idea behind our approach is to start annota-tion manually and to partially automate the processin the later stages.
We assume that some data hasalready been manually tagged and use it to train atagger specifically for high recall.
We then run thistagger on the rest of our corpus and ask an annotatorto filter the list of suggested gene names.The rest of this paper is organized as follows.
Sec-tion 2 describes the model and learning algorithm.Section 3 relates our approach to previous work.Section 4 describes our experiments and Section 5concludes the paper.2 MethodsThroughout this work, we use a linear sequencemodel.
This class of models includes popular tag-ging models for named entities such as conditionalrandom fields, maximum entropy Markov modelsand max-margin Markov networks.
Linear sequencemodels score possible tag sequences for a given in-put as the dot product between a learned weight vec-tor and a feature vector derived from the input andproposed tas sequence.
Linear sequence models dif-fer principally on how the weight vector is learned.Our experiments use the MIRA algorithm (Cram-mer et al, 2006; McDonald et al, 2005) to learnthe weight vector.2.1 NotationIn what follows, x denotes the generic input sen-tence, Y (x) the set of possible labelings of x, andY +(x) the set of correct labelings of x.
There isalso a distinguished ?gold?
labeling y(x) ?
Y +(x).For each pair of a sentence x and labeling y ?Y (x), we compute a vector-valued feature represen-tation f(x, y).
Given a weight vector w, the scorew ?
f(x, y) ranks possible labelings of x, and we de-note by Yk,w(x) the set of k top scoring labelings forx.We use the standard B,I,O encoding for namedentities (Ramshaw and Marcus, 1995).
Thus Y (x)for x of length n is the set of all sequences of lengthn matching the regular expression (O|(BI?))?.
In alinear sequence model, for suitable feature functionsf , Yk,w(x) can be computed efficiently with Viterbidecoding.2.2 k-best MIRA and Loss FunctionsThe learning portion of our method finds a weightvector w that scores the correct labelings of the testdata higher than incorrect labelings.
We used a k-53best version of the MIRA algorithm (Crammer etal., 2006; McDonald et al, 2005).
This is an onlinelearning algorithm that starts with a zero weight vec-tor and for each training sentence makes the small-est possible update that would score the correct la-bel higher than the old top k labels.
That is, for eachtraining sentence x we update the weight vector waccording to the rule:wnew = argminw ?w ?
wold?s.
t. w ?
f(x, y(x)) ?
w ?
f(x, y) ?
L(Y +(x), y)?y ?
Yk,wold(x)where L(Y +(x), y) is the loss, which measures theerrors in labeling y relative to the set of correct la-belings Y +(x).An advantage of the MIRA algorithm (over manyother learning algorithms such as conditional ran-dom fields) is that it allows the use of arbitrary lossfunctions.
For our experiments, the loss of a label-ing is a weighted combination of the number of falsepositive mentions and the number of false negativementions in that labeling.2.3 Semi-Automated TaggingFor our semi-automated annotation experiments, weimagine the following scenario: We have already an-notated half of our training corpus and want to anno-tate the remaining half.
The goal is to save annotatoreffort by using a semi-automated approach insteadof annotating the rest entirely manually.In particular we investigate the following method:train a high-recall named entity tagger on the anno-tated data and use that to tag the remaining corpus.Now ask a human annotator to filter the resultingmentions.
The mentions rejected by the annotatorare simply dropped from the annotation, leaving theremaining mentions.3 Relation to Previous WorkThis section relates our approach to previous workon semi-automated approaches.
First we discusshow semi-automated annotation is different from ac-tive learning and then discuss some previous semi-automated annotation work.3.1 Semi-Automated versus Active LearningIt is important not to confuse semi-automated anno-tation with active learning.
While they both attemptto alleviate the burden of creating an annotated cor-pus, they do so in a completely orthogonal manner.Active learning tries to select which instances shouldbe labeled in order to make the most impact on learn-ing.
Semi-automated annotation tries to make theannotation of each instance faster or easier.
In par-ticular, it is possible to combine active learning andsemi-automated annotation by using an active learn-ing method to select which sentences to label andthen using a semi-automated labeling method.3.2 Previous work on semi-automatedannotationThe most common approach to semi-automatic an-notation is to automatically tag an instance and thenask an annotator to correct the results.
We restrictour discussion to this paradigm due to space con-straints.
Marcus et al (1994), Chiou et al (2001)and Xue et al (2002) apply this approach with someminor modifications to part of speech tagging andphrase structure parsing.
The automatic system ofMarcus et al only produces partial parses that arethen assembled by the annotators, while Chiou et almodified their automatic parser specifically for usein annotation.
Chou et al (2006) use this tag andcorrect approach to create a corpus of predicate ar-gument structures in the biomedical domain.
Culotaet al (2006) use a refinement of the tag and correctapproach to extract addressbook information from e-mail messages.
They modify the system?s best guessas the user makes corrections, resulting in less anno-tation actions.4 ExperimentsWe now evaluate to what extent our semi-automatedannotation framework can be useful, and how mucheffort it requires.
For both questions we comparesemi-automatic to fully manual annotation.
In ourfirst set of experiments, we measured the usefulnessof semi-automatically annotated corpora for traininga gene mention tagger.
In the second set of exper-iments, we measured the annotation effort for genementions with the standard fully manual method andwith the semi-automated methods.4.1 Measuring EffectivenessThe experiments in this section use the training datafrom the the Biocreative II competition (Tanabe et54Sentence Expression of SREBP-1a stimulated StAR promoter activity in the context of COS-1 cellsgold label Expression of SREBP-1a stimulated StAR promoter activity in .
.
.alternative Expression of SREBP-1a stimulated StAR promoter activity in .
.
.alternative Expression of SREBP-1a stimulated StAR promoter activity in .
.
.Figure 1: An example sentence and its annotation in Biocreative II.
The evaluation metric would give fullcredit for guessing one of the alternative labels rather than the ?gold?
label.al., 2005).
The data is supplied as a set of sentenceschosen randomly fromMEDLINE and annotated forgene mentions.Each sentence in the corpus is provided as a list of?gold?
gene mentions as well as a set of alternativesfor each mention.
The alternatives are generated bythe annotators and count as true positives.
Figure 1shows an example sentence with its gold and alter-native mentions.
The evaluation metric for these ex-periments is F-score augmented with the possibilityof alternatives (Yeh et al, 2005).We used 5992 sentences as the data that has al-ready been annotated manually (set Data-1), andsimulated different ways of annotating the remain-ing 5982 sentences (set Data-2).
We compare thequality of annotation by testing taggers trained us-ing these corpora on a 1493 sentence test set.We trained a high-recall tagger (recall of 89.6%)on Data-1, and ran it on Data-2.
Since we havelabels available for Data-2, we simulated an anno-tator filtering these proposed mentions by acceptingthem only if they exactly match a ?gold?
or alterna-tive mention.
This gave us an F-score of 94.7% onData-2 and required 9981 binary decisions.Figure 2 shows F1 score as a function of the num-ber of extra sentences annotated.
Without any ad-ditional data, the F-measure of the tagger is 81.0%.The two curves correspond to annotation with andwithout alternatives.
The horizontal line at 82.8%shows the level achieved by the semi-automaticmethod (when using all of Data-2).From the figure, we can see that to get compa-rable performance to the semi-automatic approach,we need to fully manually annotate roughly a thirdas much data with alternatives, or about two thirds asmuch data without alternatives.
The following sec-tion examines what this means in terms of annotatortime by providing timing results for semi-automaticand fully-manual annotation without alternatives.81 81.5 82 82.5 83 83.5 84 84.5 850100020003000400050006000ExtraAnnotatedSentences (fromData-2)Manual With AlternativesManual w/oAlternativesSemi-Automatic (on all of Data-2)Figure 2: Effect of the number of annotated in-stances on F1 score.
In all cases the original 5992instances were used; the curves show manual an-notation while the level line is the semi-automaticmethod.
The curves are averages over 3 trials.4.2 Measuring EffortThe second set of experiments compares annotatoreffort between fully manual and semi-automatic an-notation.
Because we did not have access to an expe-rienced annotator from the Biocreative project, andgene mention annotations vary subtly among anno-tation efforts, we evaluated annotator effort on on thePennBioIE named entity corpus.1 Furthermore, wehave not yet annotated enough data locally to per-form both effectiveness and effort experiments onthe local corpus alone.
However, both corpora an-notate gene mentions in MEDLINE abstracts, so weexpect that the timing results will not be significantlydifferent.We asked an experienced annotator to tag 194MEDLINE abstracts: 96 manually and 98 using thesemi-automated method.
Manual annotation wasdone using annotation software familiar to the an-notator.
Semi-automatic annotation was done with a1Available from http://bioie.ldc.upenn.edu/55Web-based tool developed for the task.
The new toolhighlights potential gene mentions in the text and al-lows the annotator to filter them with a mouse click.The annotator had been involved in the creation ofthe local manually annotated corpus, and had a lot ofexperience annotating named entities.
The abstractsfor annotation were selected randomly so that theydid not contain any abstracts tagged earlier.
There-fore, we did not expect the annotator to have seenany of them before the experiment.To generate potential gene mentions for the semi-automated annotation, we ran two taggers on thedata: a high recall tagger trained on the local corpusand a high recall tagger trained on the Biocreativecorpus.
At decode time, we took the gene mentionsfrom the top two predictions of each of these taggerswhenever there were any gene mentions predicted.As a result, the annotator had to make more binarydecisions per sentence than they would have for ei-ther training corpus alone.
For the semi-automatedannotation, the annotator had to examine 682 sen-tences and took on average 10 seconds per sentence.For the fully-manual annotation, they examined 667sentences and took 40 seconds per sentence on av-erage.
We did not ask the annotator to tag alterna-tives because they did not have any experience withtagging alternatives and we do not have a tool thatmakes the annotation of alternatives easy.
Conse-quently, effort totals for annotation with alternativeswould have been skewed in our favor.
The four-foldspeedup should be compared to the lower curve inFigure 2.5 Discussion and Further WorkWe can use the effort results to estimate the relativeeffort of annotating without alternatives and of semi-automated annotation.
To obtain the same improve-ment in F-score, we need to semi-automatically an-notate roughly a factor of 1.67 more data than usingthe fully manual approach.
Multiplying that by the0.25 factor reduction in annotation time, we get thatthe time required for a comparable improvement inF-score is 0.42 times as long ?
a 58% reduction inannotator time.We do not have any experiments on annotatingalternatives, but the main difference between semi-automated and fully-manual annotation is that theformer does not require the annotator to decide onboundaries.
Consequently, we expect that annota-tion with alternatives will be considerably more ex-pensive than without alternatives, since more bound-aries have to be outlined.In future work, it would be interesting to comparethis approach to the traditional approach of manuallycorrecting output of a system.
Due to constraintson annotator time, it was not possible to do theseexperiments as part of the current work.ReferencesFu-Dong Chiou, David Chiang, and Martha Palmer.2001.
Facilitating treebank annotation using a statisti-cal parser.
In HLT ?01.
ACL.Wen-Chi Chou, Richard Tzong-Han Tsai, Ying-Shan Su,Wei Ku, Ting-Yi Sung, and Wen-Lian Hsu.
2006.A semi-automatic method for annotating a biomedicalproposition bank.
In FLAC?06.
ACL.Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer.
2006.
Online passive-aggressive algorithms.
JMLR, 7.Aron Culota, Trausti Kristjansson, Andrew McCallum,and Paul Viola.
2006.
Corrective feedback and per-sistent learning for information extraction.
ArtificialIntelligence, 170:1101?1122.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1994.
Building a large annotated cor-pus of english: The penn treebank.
ComputationalLinguistics, 19(2):313?330.Ryan McDonald, Koby Crammer, and Fernando Pereira.2005.
Online large-margin training of dependencyparsers.
In ACL?05.
ACL.Lance Ramshaw and Mitch Marcus.
1995.
Text chunk-ing using transformation-based learning.
In DavidYarovsky and Kenneth Church, editors, Proceedingsof the Third Workshop on Very Large Corpora.
ACL.Lorraine Tanabe, Natalie Xie, Lynne H. Thom, WayneMatten, and W. John Wilbur.
2005.
GENETAG: atagged corpus for gene/protein named entity recogni-tion.
BMC Bioinformatics, 6(Suppl.
1).Nianwen Xue, Fu-Dong Chiou, and Martha Palmer.2002.
Building a large-scale annotated chinese corpus.In Proceedings of the 19th international conference onComputational linguistics.
ACL.Alexander Yeh, Alexander Morgan, Marc Colosimo, andLynette Hirschman.
2005.
BioCreAtIvE Task 1A:gene mention finding evaluation .
BMC Bioinformat-ics, 6(Suppl.
1).56
