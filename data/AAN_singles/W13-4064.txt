Proceedings of the SIGDIAL 2013 Conference, pages 394?403,Metz, France, 22-24 August 2013. c?2013 Association for Computational LinguisticsWhich ASR should I choose for my dialogue system?Fabrizio Morbini, Kartik Audhkhasi, Kenji Sagae, Ron Artstein,Dog?an Can, Panayiotis Georgiou, Shri Narayanan, Anton Leuski and David TraumUniversity of Southern CaliforniaLos Angeles, California, USA{morbini,sagae,artstein,leuski,traum}@ict.usc.edu{audhkhas,dogancan}@usc.edu {georgiou,shri}@sipi.usc.eduAbstractWe present an analysis of several pub-licly available automatic speech recogniz-ers (ASRs) in terms of their suitability foruse in different types of dialogue systems.We focus in particular on cloud basedASRs that recently have become availableto the community.
We include featuresof ASR systems and desiderata and re-quirements for different dialogue systems,taking into account the dialogue genre,type of user, and other features.
We thenpresent speech recognition results for sixdifferent dialogue systems.
The most in-teresting result is that different ASR sys-tems perform best on the data sets.
Wealso show that there is an improvementover a previous generation of recognizerson some of these data sets.
We also inves-tigate language understanding (NLU) onthe ASR output, and explore the relation-ship between ASR and NLU performance.1 IntroductionDialogue system developers who are not alsospeech recognition experts are in a better posi-tion than ever before in terms of the ease of in-tegrating existing speech recognizers in their sys-tems.
While there have been commercial solutionsand toolkits for a number of years, there were anumber of problems in getting these systems towork.
For example, early toolkits relied on spe-cific machine hardware, software, and firmwareto function properly, often had a difficult instal-lation process, and moreover often didn?t workwell for complex dialogue domains, or challeng-ing acoustic environments.
Fortunately the situ-ation has greatly improved in recent years.
Nowthere are a number of easy to use solutions, in-cluding open-source systems (like PocketSphinx),as well as cloud-based approaches.While this increased choice of quality recogniz-ers is of great benefit to dialogue system develop-ers, it also creates a dilemma ?
which recognizerto use?
Unfortunately, the answer is not simple ?it depends on a number of issues, including thetype of dialogue domain, availability and amountof training data, availability of internet connectiv-ity for the runtime system, and speed of responseneeded.
In this paper we assess several freelyavailable speech recognition engines, and exam-ine their suitability and performance in several di-alogue systems.
Here we extend the work done inYao et al(2010) focusing in particular on cloudbased freely available ASR systems.
We include2 local ASRs for reference, one of which was alsoused in the earlier work for easy comparison.2 Speech Recognizer Features andEnginesThe following are some of the major criteria forselection of a speech recognizer.Customization Some of the available speechrecognizers allow the users to tune the recognizerto the environment it will operate in, by providinga specialized lexicon, trained language models oracoustic models.
Customization is especially im-portant for dialogue systems whose input containsspecialized vocabulary (see section 4).Output options A basic recognizer will outputa string of text, representing its best hypothesisabout the transcription of the speech input.
Somerecognizers offer additional outputs which are use-ful for dialogue systems: ranked n-best hypothe-ses allow later processing to use context for dis-ambiguation, and incremental results allow thesystem to react while the user is still speaking.Performance characteristics Dialogue systemsdiffer in their requirements for response speed; a394System Customization Output options Open Source PerformanceN-best Incremental Speed InstallationPocketsphinx Full Yes Yes Yes realtime LocalApple No Noa No No network CloudGoogle No Yes Yesb No network CloudAT&T Partialc Yes No No network CloudOtosense-Kaldi Full Yes No Yesd variablee LocalaSingle output annotated with alternative hypotheses.
bOnly for web-delivered applications in a Google Chrome browser.cCustom language models.
dRelease scheduled for Fall 2013. eUser controls trade-off between speed and output quality.Table 1: Speech recognizer features important for use in dialogue systemsspeech recognizer that runs locally can help byavoiding network latencies.Output quality Typically, a dialogue systemwould want the best recognition accuracy pos-sible given the constraints.
Ultimately, dialoguesystems want the output that would yield the bestperformance for Natural Language Understand-ing and other downstream processes.
As a rule,better speech recognition leads to better languageunderstanding, though this is not necessarily thecase for specific applications (see section 5).We evaluated 5 freely available speech recog-nizers.
Their features are summarized in Table 1.We did not include the MIT WAMI toolkit1 as weare focused on speech services that can directlybe used by stand alone applications as opposed toweb delivered ones.
We did not include commer-cial recognizers such as Nuance, because licensingterms can be difficult for research institutions, andin particular, disallow publishing benchmarks.Pocketsphinx is a version of the CMU SphinxASR system optimized to run also on embeddedsystems (Huggins-Daines et al 2006).
Pocket-sphinx is fast, runs locally, and requires relativelymodest computational resources.
It provides n-best lists and lattices, and supports incrementaloutput.
It also provides a voice activity detec-tion functionality for continuous ASR.
This ASRis fully customizable and trainable, but users areexpected to provide language models suitable fortheir applications.
A few acoustic models are pro-vided, and can be adapted using the CMUSphinxtools.21http://wami.csail.mit.edu/2http://cmusphinx.sourceforge.net/wiki/tutorialadaptApple Dictation is the OS level feature in bothMacOSX and iOS.3 It is integrated into the text in-put system pipeline so a user can replace her key-board with a microphone for entering text in anyapplication.
Dictation is often associated with theSiri personal assistant feature of iOS.
While it islikely that Dictation and Siri share the same ASRtechnology, Dictation only does speech recogni-tion.
Apple states that Dictation learns the charac-teristics of the user?s voice and adapts to her accent(Apple Inc, 2012).
Dictation requires an internetconnection to send recorded user speech to Ap-ple?s servers and receive ASR results.
Processingstarts as soon as the user starts speaking so the de-lay of getting the recognition results after the userfinishes speaking is minimal.To integrate Dictation into a dialogue system,a system designer needs to include any system de-fined text input control into her application and usethe control APIs to observe text changes.
The userwould need to press a key when starting to speakand push the key again once she is done speak-ing.
The ASR result is a text string annotated withalternative interpretations of individual words orphrases in the text.
There is an API for extract-ing those interpretations from the result.
While theDictation feature is reasonably fast and easy to in-tegrate, dialogue system developers have no con-trol over the ASR process, which must be treatedas a black box.
Apple dictation is limited in thatno customization is possible, no partial recogni-tion results are provided, and there is an unspeci-fied limit on the number of utterances dictated fora period of time, which is not a problem for inter-action between a single user and a dialogue sys-tem, but may be an issue in dialogue systems thatsupport multiple concurrent users.3Dictation was introduced in iOS 5.0 and MacOSX 10.8.395Google Speech API provides support for theHTML 5 speech input feature.4 It is a cloud basedservice in which a user submits audio data usingan HTML POST request and receives as reply theASR output in the form of an n-best list.
The au-dio data is limited to roughly 10 seconds in length,longer clips are rejected and return no ASR results.The user can (1) customize the number of hy-potheses returned by the ASR, (2) specify whichlanguage the audio file contains and (3) enable afilter to remove profanities from the output text.As is the case with Apple Dictation, ASR must betreated as a black box, and no task customizationis possible for dialogue system developers.
Userscannot specify or provide custom language modelsor acoustic models.
The service returns only the fi-nal hypothesis, there is no incremental output.5 Inaddition, results for the same inputs may changeunpredictably, since Google may update or other-wise change its service and models, and modelsmay be adapted using specific audio data suppliedby users.
In our experiments, we observed accu-racy improvements when submitting the same au-dio files over repeated trials over two weeks.AT&T Watson is the ASR engine availablethrough the AT&T Speech Mashup service.6 It isa cloud based service that can be accessed throughHTML POST requests, like the Google SpeechAPI.
AT&T Watson is designed to support the de-mands of online spoken dialogue systems, and canbe customized with data specific to a dialogue sys-tem.
Additionally, in our tests we did not observeany limitation in the maximum length of the in-put audio data.
However, AT&T does not providea default general-purpose language model, andapplication-specific models must be built withinthe Speech Mashup service using user-providedtext data.
The acoustic model must be selectedfrom a list provided by the AT&T service, andacoustic models can be further customized withinthe Speech Mashup service.
The ASR returns ann-best list of hypotheses but does not provide in-cremental output.Otosense-Kaldi Another ASR we employedwas the Kaldi-based OtoSense-Kaldi engine de-4https://www.google.com/speech-api/v1/recognize5The demo page shows continuous speech understandingwith incremental results but requires Google Chrome to runand is specific to web delivered applications:http://www.google.com/intl/en/chrome/demos/speech.html6https://service.research.att.com/smmveloped at SAIL.7 OtoSense-Kaldi8 is an on-line,multi-threaded architecture based on the Kalditoolkit (Povey et al 2011) that allows for dynam-ically configurable and distributed ASR.3 Dialogue Systems, Users, and DataAll spoken dialogue systems are similar in somerespects, in that there is speech by a user (or users)that needs to be recognized, and this speech ispunctuated by speech from the system.
More-over, the speech is not fully independent, but ut-terances are connected to other utterances, e.g.
an-swers to questions, or clarifications.
There are,however many ways in which systems can differ,that have implications for which speech recogniz-ers are most appropriate.
Some of the dimensionsto consider are:Type of microphone(s) One of the biggest im-pacts on ASR is the acoustic environment.
Willthe audio be clean, coming from a close-talkinghead or lapel-mounted microphone, or will it needto be picked up from a broader directional micro-phone or microphone array?Number of speakers/microphones Will therebe one designated microphone per person, or willspeaker identification need to be performed?
Willaudio from the system confuse the ASR?Push to talk or continuous speech Will theuser clearly identify the start and end of speech,or will the system need to detect speech acousti-cally?Type of Users Will there be designated long-term users, where user-training or system modeladaptation is feasible, or will there be many un-known users, where training is not feasible?
Seealso section 3.1 for more on user types.Genre What kinds of things will people be say-ing to the system?
Is it mostly commands or shortanswers to questions, or more open-ended conver-sation?
See section 3.2 for more on genre issues.Training Data Is within-domain training dataavailable, and if so how much?3.1 Types of UsersThe type of user is important for the overalldesign of the system and has implications for7http://sail.usc.edu8OtoSense-Kaldi will be released (BSD license) in 2013.396ASR performance as well.
One important as-pect is the broad physical differences amongspeakers, such as male vs female, adult vs child(e.g.
Bell and Gustafson, 2003), or language pro-ficiency/accent, that will have implications for theacoustics of what is said, and ASR results.
Otheraspects of users have implications for what willbe said, and how successful the interface maybe, overall.
Many (e.g.
Hassel and Hagen, 2006;Jokinen and Kanto, 2004) have looked at the dif-ferences between novice and expert users.
Ai etal.
(2007a) also points out a difference betweenreal users and recruited subjects.
Real users alsocome in many different flavors, depending on theirpurposes.
E.g.
are they interacting with the systemfor fun, to do a specific task that they need to getdone, to learn something (specific or general), orwith some other purpose in mind?We considered the following classes of users,ordered from easiest to hardest to get to acceptableperformance and robustness levels:Demonstrators are generally the easiest for a sys-tem to understand ?
a demonstrator is trained inuse of the system, knows what can and can?t besaid, is motivated toward success, and is gener-ally interested in showing off the most impres-sive/successful aspects of the system to an audi-ence rather than using it for its own sake.Trained/Expert Users are similar to demonstra-tors, but use the system to achieve specific resultsrather than just to show off its capabilities.
Thismeans that users may be forced down lines thatare not ideal for the system, if these are necessaryto accomplish the task.Motivated Users do not have the training of ex-pert users, and may say many things that the sys-tem can not handle as opposed to equivalent ex-pressions that could be handled.
However moti-vated users do want the system to succeed, and ingeneral are willing to do whatever they think isnecessary to improve system performance.
Unlikeexpert users, motivated users might be incorrectabout what will help the system (e.g.
hyperarticu-lation in response to system misunderstanding).Casual Users are interested in finding out whatthe system can do, but do not have particular moti-vations to help or hinder the system.
Casual Usersmay also leave in the middle of an interaction, if itis not engaging enough.Red Teams are out to test or ?break?
the system,or show it as not-competent, and may try to dothings the system can?t understand or react wellto, even when an alternative formulation is knownto work.3.2 Types of Dialogue System GenresDialogue Genres can be distinguished along manylines, e.g.
the number and relationship of partic-ipants, specific conversational rules, purposes ofthe participants, etc.
We distinguish here four gen-res of dialogue system that have been in use atthe Institute for Creative Technologies and that wehave available corpora for (there are many othertypes of dialogue genres, including tutoring, ca-sual conversation, interviewing,.
.
.
).
Each genrehas implications for the internal representationsand system architectures needed to engage in thatgenre of dialogue.Simple Question-answering This genre in-volves strong user-initiative and weak global di-alogue coherence.
The user can ask any ques-tion to the system at any time, and the systemshould respond, with an appropriate answer ifable, or with some other reply indicating eitherinability or unwillingness to provide the answer.This genre allows modeling dialogue at a surface-text level (Gandhe, 2013), without internal se-mantic representations of the input, and wherethe result of ?understanding?
input is the system?sexpected output.
The NCPEditor9 (Leuski andTraum, 2011) is a toolkit that provides an author-ing environment, classification, and dialogue ca-pability for simple question-answering characters.The SGT Blackwell, SGT Star, and Twins systemsdescribed below are all systems in this genre.Advanced Question-answering This genre issimilar to the simple question-answering charac-ters, in that the main task of the user is to elicitinformation from the system character.
The differ-ence is that there is more long-range and interme-diate dialogue coherence, in that questions can beanswered several utterances after they have beenasked, there can be intervening sub-dialogues, andcharacters sometimes take the initiative to pursuetheir own goals rather than just responding to theuser.
Because of the requirements for somewhatdeeper understanding, and relation of input to con-9Available free for academic research purposes fromhttps://confluence.ict.usc.edu/display/VHTK/Home397text and character goals and policies, there is aneed of at least a shallow semantic representa-tion and representation of the dialogue informa-tion state, and the character must distinguish un-derstanding of the input from the character out-put (since the latter will depend on the dialoguepolicy and information state, not just the under-standing of input).
The tactical questioning archi-tecture (Gandhe et al 2009)10 provides author-ing and run-time support for advanced question-answering characters, and has been used to buildover a dozen characters for purposes such as train-ing tactical questioning, training culture, and psy-chology experiments (Gandhe et al 2011).
TheAmani character described below is in this genre.Slot-filling Probably the most common type ofdialogue system (at least in the research commu-nity) is slot-filling.
Here the dialogue is fairlystructured, with an initial greeting phase, then oneor more tasks, which all start with the user se-lecting the task, and the system taking over ini-tiative to ?fill?
and possibly confirm the neededslots, before retrieving some information from adatabase, or performing a simple service.11 Thisgenre also requires a semantic representation, atleast of the slots and acceptable values.
Gener-ally, the set of possible values is large enough, thatsome form of NLG is needed (at least templatefilling), rather than authoring of all full sentences.There are a number of toolkits and developmentframeworks that are well suited to slot-filling sys-tems, e.g.
Ravenclaw (Bohus and Rudnicky, 2003)or Trindikit (Larsson and Traum, 2000).
The Ra-diobots system, described below is in this genre.Negotiation and Planning In this genre, thesystem is more of an equal partner with the user,than a servant, as in the slot-filling systems.
Thesystem must not merely understand user requests,but must also evaluate whether they meet the sys-tem goals, what the consequences and precondi-tions of requests are, and whether there are betteralternatives.
For this kind of inference, a more de-tailed semantic representation is required than justfilling in slots.
While we are not aware of publiclyavailable software that makes this kind of systemeasy to construct, there have been several built us-ing an information-state approach, or the soar cog-10Soon to be released as part of the virtual human toolkit.11Mixed-initiative versions of this genre exist, where theuser can also provide unsolicited information, which reducesthe number of system queries needed.nitive architecture.
The TRIPS system (Allen etal., 2001) also has many similarities.3.3 ICT Dialogue Systems TestedWe tested the recognizers described in section 2on data sets collected from six different dialoguedomains.
Five are the same ones tested in Yao etal.
(2010), to which we added the Twins set.
De-tails on the size of the training and developmentsets may be found in Yao et al(2010), here wereport only the numbers relevant to the Twins do-main and to the NLU analysis, which are not inYao et al(2010).SGT Blackwell was created as a virtual humantechnology demonstration for the 2004 Army Sci-ence Conference.
This is a question-answeringcharacter, with no internal semantic representationand the primary NLU task merged with Dialoguemanagement as selecting the best response.The original users were ICT demonstrators.However, there were also some experiments withrecruited participants (Leuski et al 2006a; Leuskiet al 2006b).
Later SGT Blackwell became a partof the ?best design in America?
triennial at theCooper-Hewitt Museum in New York City, andthe data set here is from visitors to the museum,who are mostly casual users, but range from expertto red-team.
Users spoke into a mounted direc-tional microphone (see Robinson et al 2008 formore details).SGT STAR (Artstein et al 2009a) is a question-answering character similar to SGT Blackwell, al-though designed to talk about Army careers ratherthan general knowledge.
The users are Army per-sonnel who went to job fairs and visited schools inthe mobile Army adventure vans, speaking usingheadset microphones, and performing for an audi-ence.
The users are somewhere between demon-strators and expert users.
They are speaking toSGT STAR for the benefit of an audience, but theirprimary purpose is to convey information to theaudience in a memorable way (through dialoguewith SGT STAR) rather than to show off the high-lights of the character.The Twins are two life-size virtual characterswho serve as guides at the Museum of Sciencein Boston (Swartout et al 2010).
The charac-ters promote interest in Science, Technology, En-gineering and Mathematics (STEM) in childrenbetween the ages of 7 and 14.
They are question-398answering characters, but unlike SGTs Blackwelland Star, the response is a whole dialogue se-quence, potentially involving interchange fromboth characters, rather than a single character turn.There are two types of users for the Twins:demonstrators, who are museum staff members,using head-mounted microphones, and museumvisitors, who use a Shure 522 table-top mountedmicrophone (Traum et al 2012).
More on analy-sis of the museum data can be found in (Aggarwalet al 2012).
We also investigated speech recog-nition and NLU performance in this domain inMorbini et al(2012).This dataset contains 14K audio files each an-notated with one of the 168 possible response se-quences.
The division in training development andtest is the same used in Morbini et al(2012) (10Kfor training, the rest equally divided between de-velopment and test).Amani (Artstein et al 2009b; Artstein et al2011) is an advanced question-answering char-acter used as a prototype for systems meant totrain soldiers to perform tactical questioning.
Theusers are in between real users and test subjects:they were cadets at the U.S. Military Academy inApril 2009, who interacted with Amani as a uni-versity course exercise on negotiation techniques.They used head-mounted microphones to talk withAmani.This dataset comprises of 1.8K audio files eachannotated with one of the 105 possible NLU se-mantic classes.Radiobots (Roque et al 2006) is a training pro-totype that responds to military calls for artilleryfire in a virtual reality urban combat environment.This is a domain in the slot-filling genre, wherethere is a preferred protocol for the order in whichinformation is provided and confirmed.
Users aregenerally trainees, learning how to do calls for fire,they are motivated users with some training.
Thesemantic processing involved tagging each wordwith the dialogue act and parameter that it was as-sociated with (Ai et al 2007b).This data set was collected during the develop-ment of the system in 2006 at Fort Sill, Oklahoma,during two evaluation sessions from recruited vol-unteer trainees who performed calls for specificmissions (Robinson et al 2006).
These subjectsused head-mounted microphones rather than theASTI simulated radios from later data collection.SASO-EN (Traum et al 2008) is a negotiationtraining prototype in which two virtual charactersnegotiate with a human ?trainee?
about moving amedical clinic.
The genre is negotiation and plan-ning, where the human participant must try to forma coalition, and the characters reason about utili-ties of different proposals, as well as causes andeffects.
The output of NLU is a frame represen-tation including both semantic elements, like the-matic argument structure, and pragmatic elements,such as addressee and referring expressions.
Fur-ther contextual interpretation is performed by eachof the virtual characters to match the (possibly par-tial) representation to actions and states in theirtask model, resolve other referring expressions,and determine a full set of dialogue acts (Traum,2003).
Speech was collected at the USC Insti-tute for Creative Technologies (ICT) during 2006?2009, mostly from visitors and new hires, whoacted as test subjects.This dataset has 4K audio files each anno-tated with one of the 117 different NLU semanticclasses.4 ASR PerformanceWe tested each of the Datasets described in Sec-tion 3.3 with some of the recognizers describedin Section 2.
All recognizers were tested on theAmani, SASO-EN, and Twins domains, and wealso tested a natural language understanding com-ponent on these data sets (Section 5).
For SGTBlackwell, SGT STAR, and Radiobots, we reportthe performance on the same development set usedin Yao et al(2010).
For Amani and SASO-EN(where we also report the NLU performance), werun a 10-fold cross-validation in which 9 foldswhere used to train the NLU and ASR languagemodel and the 10th was used for testing.
For theTwins dialogue system, we used the same partitioninto training, development and testing reported inMorbini et al(2012) and the results reported hereare from the development set.
Due to differencesin training/testing regimens, performance of sys-tems are only comparable within each domain.Table 2 summarizes the performance of the var-ious ASR engines on the evaluation data sets.
Per-formance is measured as Word Error Rate and wasobtained using the NIST SCLITE tool.12Note that only Otosense-Kaldi in the Twins do-main had adapted acoustic models.
In the remain-12http://www.itl.nist.gov/iad/mig/tools/399Speech recognizer Evaluation data setAmani Radiobots SASO-EN SGT Blackwell SGT Star TwinsPocketsphinx 39.7 11.8 28.4 51 28.6 81Apple 28 ?
30.9 ?
?
29AT&T 29 12.1 16.3 27.3 21.7 28.8Google 23.8 36.3 20 18 26 20.6Otosense-Kaldi 33.7 ?
22.1 ?
?
18.7Table 2: Word Error Rates (%) for the various dialogue systems and ASR systems tested.ing cases only the language model was adapted.Looking at the results on the development set re-ported in Yao et al(2010), we have improvementsin 3 out of 5 domains: Amani (?11.8% Google),SASO-EN (?11.7% AT&T) and SGT Blackwell(?13% Google).
In Radiobots and SGT Star theperformance achieved with just language modeladaptation, when permitted, is worse: +4.8% and+1.7% respectively.We find that there is no single best performingspeech recognizer: results vary greatly betweenthe evaluation test sets.
In 4 of the 6 datasets over-all, and 2 of the 3 datatests tested with Otosense-Kaldi, the best performer is a cloud-based ser-vice (Google or AT&T).
There are two datasetsfor which a local, fully customizable recognizerperforms better than the cloud-based services.
Ra-diobots, consisting of military calls for artilleryfire, has a fairly limited and very specialized vo-cabulary, and indeed the two recognizers with cus-tom language models (Pocketsphinx and AT&T)perform much better than the non-customizablerecognizer (Google).The Twins dataset is unique in that for theOtosense-Kaldi system we custom-trained acous-tic and language models, while standard WSJacoustic models and adapted language modelswere used for the other dialogue systems.
Inboth cases the models were triphone based witha Linear Discriminant Analysis (LDA) front end,and Maximum Likelihood Linear Transforma-tion (MLLT) and Maximum Mutual Information(MMI) training.
This reflects on the very goodperformance in the Twins domain, decent perfor-mance on the SASO-EN domain (reasonable mis-match of WSJ and SASO-EN) and very degradedperformance in Amani (highly mismatched Amaniand WSJ domains).
The observed degradation inperformance is accentuated by the MMI discrim-inative training on the mismatched-WSJ data.
Aswith PocketSphinx and Watson, and unlike withApple Dictation and Google Speech API, withKaldi we fully control experimental conditionsand can guarantee no contamination of the train-test data.In summary, our evaluation shows that cus-tomizable recognizers are useful when the ex-pected speech is highly specialized, or when sub-stantial resources are available for tuning the rec-ognizer.5 NLU Accuracy & Relation betweenASR and NLUWhile the different genres of system have differenttypes of output for NLU: response text, dialogueact and parameter tags, speech acts, or semanticframes, many of them can be coerced into a se-lection task, in which the NLU selects the rightoutput from a set of possible outputs.
This allowsany multiclass classification algorithm to be usedfor NLU.
A possible drawback is that for someinputs, the right output might not be available inthe set considered by the training data, even if itmight easily be constructed from known parts us-ing a generative approach.A second issue is that even though we can castthe problem as multi-class classification, classifi-cation accuracy is not always the most appropriatemetric of NLU quality.
For question-answeringcharacters, getting an appropriate and relevant re-ply is more important than picking the exact re-ply selected by a human domain designer or an-notator: there might be multiple good answers, oreven the best available answer might not be verygood.
For that reason, the question-answeringcharacters allow an ?off-topic?
answer and Error-return plots (Artstein, 2011) might be necessaryto choose an optimal threshold.
For the SASO-ENsystem, slot-filler metrics such as precision, recall,and f-score are more appropriate than frame accu-400racy, because some frames may have many slotsin common and few that are different (e.g.
just adifferent addressee).
Nonetheless, we begin ouranalysis within this common framework.
For sim-plicity, we start with just three domains: Twins,Amani, and SASO-EN.
SGT STAR and Blackwellare very similar to Twins in terms of NLU.
Ra-diobots is more challenging to coerce to multiclassclassification.Conventional wisdom in the speech and lan-guage processing community is that performanceof ASR and NLU are closely tied: improvedspeech recognition leads to better language under-standing, while deficiencies in speech recognitioncause difficulty in understanding.
This conven-tional wisdom is borne out by decades of experi-ence with speech and dialogue systems, though weare not aware of attempts to systematically demon-strate it.
The present study shows that the expectedrelation between speech recognition and languageunderstanding holds for the systems we tested.Accepted assumptions about the relation be-tween speech recognition and language under-standing have been repeatedly challenged.
Directchallenges are typically limited to specific appli-cations.
Wang et al(2003) show that for a slot-filling NLU, ASR can be specifically tuned to rec-ognize those words that are relevant to the slot-filling task, resulting in improved understandingdespite a decrease in performance on overall wordrecognition.
However, Boros et al(1996) foundthat when not optimizing the ASR for the specificslot filling task there is a nearly linear correlationbetween word accuracy and NLU accuracy.
Al-shawi (2003) and Huang and Cox (2006) show thatin call-routing applications the word level can bedispensed with altogether and calls routed basedon phonetic information alone without noticeableloss in performance.
These challenges suggest thatthe speech-language divide is not as clean as thetheory suggests.To investigate the relation between ASR andNLU, we ran each ASR output from each ofthe 5 recognizers through an understanding com-ponent to obtain an NLU output (each datasethad a separate NLU component, which was heldconstant for all speech recognizers).
ASR andNLU performance are conventionally measured onscales of opposite polarity: better performanceshows up as lower word error rates but higherNLU accuracies.
For the correlations we invert theconventional ASR scale and use word accuracy, sothat higher numbers signify better performance onboth scales.13Figure 1 shows the results obtained in the 3 di-alogue systems by the various ASR systems.
Thefigures plot ASR performance against NLU per-formance; NLU results on manual transcriptionsare included for comparison.
There are too fewdata points for the correlations between ASR andNLU performance to be significant, but the trendsare positive, as expected.Our experiments lend supporting evidence tothe claim that in general, ASR performance is pos-itively linked to NLU performance (special casesnotwithstanding).
The 3 datasets exhibit posi-tive correlations between speech recognition andlanguage understanding performance.
Thus, weclaim that the basis of the conventional wisdomis sound: speech recognition directly affects lan-guage understanding.
This conclusion holds whenthe speech recognizer has been optimized to pro-duce the most accurate transcript, rather than for aspecific NLU.6 Conclusion and Future WorkWe have extended here the ASR system evaluationpublished in Yao et al(2010) including some newcloud based ASR services that achieve very goodperformance showing an improvement of around12%.
We also showed that ASR and NLU perfor-mance are correlated.One possible avenue of future work is to ex-tract importance weights for each word from thelearnt NLU models and use these weights to tryto explain those cases that diverge from the corre-lation between ASR and NLU performance.
Thismay also give us a better measure than WER forassessing ASR performance in dialogue systems.Another avenue of future work involves examin-ing different types of NLU engines, and differentmetrics for the different dialogue system genres,which, again, may lead to a more relevant assess-ment of ASR performance.AcknowledgmentsThe effort described here has been sponsored bythe U.S. Army.
Any opinions, content or informa-tion presented does not necessarily reflect the posi-13We define ?accuracy?
as 1 minus WER, so this numbercan in principle dip below zero if there are more errors thanwords.401Amanir = 0.54, df = 3, p = 0.345Word accuracy (%)NLUaccuracy(%)50 60 70 80 90 10051545760636669????
?
?SASO-ENr = 0.77, df = 3, p = 0.130Word accuracy (%)NLUaccuracy(%)60 70 80 90 10030405060708090?????
?Twinsr = 0.99, df = 3, p = 0.002Word accuracy (%)NLUaccuracy(%)0 20 40 60 80 100405060708090100????
?Figure 1: Relation between ASR and NLU performance (red dots are manual transcriptions)tion or the policy of the United States Government,and no official endorsement should be inferred.ReferencesPriti Aggarwal, Ron Artstein, Jillian Gerten, Athana-sios Katsamanis, Shrikanth Narayanan, AngelaNazarian, and David Traum.
2012.
The Twins cor-pus of museum visitor questions.
In LREC-2012,Istanbul, Turkey, May.Hua Ai, Antoine Raux, Dan Bohus, Maxine Eskenazi,and Diane Litman.
2007a.
Comparing spoken dia-log corpora collected with recruited subjects versusreal users.
In SIGdial 2007.Hua Ai, Antonio Roque, Anton Leuski, and DavidTraum.
2007b.
Using information state to improvedialogue move identification in a spoken dialoguesystem.
In Proceedings of the 10th Interspeech Con-ference, Antwerp, Belgium, August.James F. Allen, George Ferguson, and Amanda Stent.2001.
An architecture for more realistic conversa-tional systems.
In IUI, pages 1?8.Hiyan Alshawi.
2003.
Effective utterance classifica-tion with unsupervised phonotactic models.
In HLT-NAACL 2003, pages 1?7, Edmonton, Alberta, May.Apple Inc. 2012.
Mac basics: Dictation (TechnoteHT5449), November.R.
Artstein, S. Gandhe, J. Gerten, A. Leuski, andD.
Traum.
2009a.
Semi-formal evaluation of con-versational characters.
In O. Grumberg, M. Kamin-ski, S. Katz, and S. Wintner, editors, Languages:From Formal to Natural.
Essays Dedicated to Nis-sim Francez on the Occasion of His 65th Birthday,volume 5533 of Lecture Notes in Computer Science,pages 22?35.
Springer, Berlin.Ron Artstein, Sudeep Gandhe, Michael Rushforth, andDavid R. Traum.
2009b.
Viability of a simple dia-logue act scheme for a tactical questioning dialoguesystem.
In DiaHolmia 2009: Proceedings of the13th Workshop on the Semantics and Pragmatics ofDialogue, page 43?50, Stockholm, Sweden, June.Ron Artstein, Michael Rushforth, Sudeep Gandhe,David Traum, and MAJ Aram Donigian.
2011.Limits of simple dialogue acts for tactical question-ing dialogues.
In 7th IJCAI Workshop on Knowl-edge and Reasoning in Practical Dialogue Systems,Barcelona, Spain, July.Ron Artstein.
2011.
Error return plots.
In 12th SIG-dial Workshop on Discourse and Dialogue, Port-land, OR, June.Linda Bell and Joakim Gustafson.
2003.
Child andadult speaker adaptation during error resolution in apublicly available spoken dialogue system.
In IN-TERSPEECH 2003.Dan Bohus and Alexander I. Rudnicky.
2003.
Raven-claw: dialog management using hierarchical task de-composition and an expectation agenda.
In INTER-SPEECH 2003.M.
Boros, W. Eckert, F. Gallwitz, G. Grz, G. Han-rieder, and H. Niemann.
1996.
Towards understand-ing spontaneous speech: Word accuracy vs. conceptaccuracy.
In In Proceedings of (ICSLP 96), pages1009?1012.Sudeep Gandhe, Nicolle Whitman, David R. Traum,and Ron Artstein.
2009.
An integrated authoringtool for tactical questioning dialogue systems.
In 6thWorkshop on Knowledge and Reasoning in PracticalDialogue Systems, Pasadena, California, July.Sudeep Gandhe, Michael Rushforth, Priti Aggarwal,and David R. Traum.
2011.
Evaluation ofan integrated authoring tool for building advancedquestion-answering characters.
In Proceedings ofInterspeech-11, Florence, Italy, 08/2011.Sudeep Gandhe.
2013.
Rapid prototyping and evalu-ation of dialogue systems for virtual humans.
Ph.D.thesis, University of Southern California.402Liza Hassel and Eli Hagen.
2006.
Adaptation of anautomotive dialogue system to users?
expertise andevaluation of the system.
Language Resources andEvaluation, 40(1):67?85.Quiang Huang and Stephen Cox.
2006.
Task-independent call-routing.
Speech Communication,48(3?4):374?389.D.
Huggins-Daines, M. Kumar, A. Chan, A.W.
Black,M.
Ravishankar, and A.I.
Rudnicky.
2006.
Pocket-sphinx: A free, real-time continuous speech recog-nition system for hand-held devices.
In Acoustics,Speech and Signal Processing, 2006.
ICASSP 2006Proceedings.
2006 IEEE International Conferenceon, volume 1, pages I?I.Kristiina Jokinen and Kari Kanto.
2004.
User ex-pertise modeling and adaptivity in a speech-basede-mail system.
In Donia Scott, Walter Daelemans,and Marilyn A. Walker, editors, ACL, pages 87?94.ACL.Staffan Larsson and David Traum.
2000.
Informationstate and dialogue management in the TRINDI dia-logue move engine toolkit.
Natural Language En-gineering, 6:323?340, September.
Special Issue onSpoken Language Dialogue System Engineering.Anton Leuski and David R. Traum.
2011.
NPCEditor:Creating virtual human dialogue using informationretrieval techniques.
AI Magazine, 32:42?56.Anton Leuski, Brandon Kennedy, Ronakkumar Patel,and David Traum.
2006a.
Asking questions tolimited domain virtual characters: How good doesspeech recognition have to be?
In 25th Army Sci-ence Conference.Anton Leuski, Ronakkumar Patel, David Traum, andBrandon Kennedy.
2006b.
Building effective ques-tion answering characters.
In Proceedings of the7th SIGdial Workshop on Discourse and Dialogue,pages 18?27.Fabrizio Morbini, Kartik Audhkhasi, Ron Artstein,Maarten Van Segbroeck, Kenji Sagae, Panayio-tis S. Georgiou, David R. Traum, and Shrikanth S.Narayanan.
2012.
A reranking approach for recog-nition and classification of speech input in conversa-tional dialogue systems.
In SLT, pages 49?54.
IEEE.Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Luka?s?Burget, Ondr?ej Glembek, Nagendra Goel, MirkoHannemann, Petr Motl?
?c?ek, Yanmin Qian, PetrSchwarz, Jan Silovsky?, Georg Stemmer, and KarelVesely?.
2011.
The Kaldi speech recognitiontoolkit.
In IEEE 2011 Workshop on AutomaticSpeech Recognition and Understanding, December.S.M.
Robinson, A. Roque, A. Vaswani, D. Traum,C.
Hernandez, and B. Millspaugh.
2006.
Evalua-tion of a spoken dialogue system for virtual realitycall for fire training.
In 25th Army Science Confer-ence, Orlando, Florida, USA.S.
Robinson, D. Traum, M. Ittycheriah, and J. Hen-derer.
2008.
What would you ask a conversationalagent?
Observations of human-agent dialogues ina museum setting.
In Proc.
of Sixth InternationalConference on Language Resources and Evaluation(LREC), Marrakech, Morocco.A.
Roque, A. Leuski, V. Rangarajan, S. Robinson,A.
Vaswani, S. Narayanan, and D. Traum.
2006.Radiobot-CFF: A spoken dialogue system for mil-itary training.
In Proc.
of Interspeech, Pittsburgh,Pennsylvania, USA.W.
Swartout, D. Traum, R. Artstein, D. Noren, P. De-bevec, K. Bronnenkant, J. Williams, A. Leuski,S.
Narayanan, D. Piepol, C. Lane, J. Morie, P. Ag-garwal, M. Liewer, J. Chiang, J. Gerten, S. Chu,and K. White.
2010.
Ada and Grace: Towardrealistic and engaging virtual museum guides.
InJ.
Allbeck, N. Badler, T. Bickmore, C. Pelachaud,and A. Safonova, editors, Intelligent Virtual Agents:10th International Conference, IVA 2010, Philadel-phia, PA, USA, September 20?22, 2010 Proceed-ings, volume 6356 of Lecture Notes in Artificial In-telligence, pages 286?300.
Springer, Heidelberg.David R. Traum, Stacy Marsella, Jonathan Gratch, JinaLee, and Arno Hartholt.
2008.
Multi-party, multi-issue, multi-strategy negotiation for multi-modalvirtual agents.
In IVA, pages 117?130.David Traum, Priti Aggarwal, Ron Artstein, SusanFoutz, Jillian Gerten, Athanasios Katsamanis, AntonLeuski, Dan Noren, and William Swartout.
2012.Ada and grace: Direct interaction with museumvisitors.
In The 12th International Conference onIntelligent Virtual Agents (IVA), Santa Cruz, CA,September.David Traum.
2003.
Semantics and pragmatics ofquestions and answers for dialogue agents.
In pro-ceedings of the International Workshop on Compu-tational Semantics, pages 380?394.Ye-Yi Wang, A. Acero, and C. Chelba.
2003.
Isword error rate a good indicator for spoken lan-guage understanding accuracy.
In IEEE Workshopon Automatic Speech Recognition and Understand-ing (ASRU ?03), pages 577?582.Xuchen Yao, Pravin Bhutada, Kallirroi Georgila, KenjiSagae, Ron Artstein, and David R. Traum.
2010.Practical evaluation of speech recognizers for vir-tual human dialogue systems.
In Nicoletta Calzo-lari, Khalid Choukri, Bente Maegaard, Joseph Mar-iani, Jan Odijk, Stelios Piperidis, Mike Rosner, andDaniel Tapias, editors, LREC.
European LanguageResources Association.403
