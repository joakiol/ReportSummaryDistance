Using the Web to Overcome Data SparsenessFrank Keller and Maria LapataDivision of InformaticsUniversity of Edinburgh2 Buccleuch PlaceEdinburgh, EH8 9LW, UKfkeller, mlapg@cogsci.ed.ac.ukOlga OurioupinaDepartment of Computational LinguisticsSaarland UniversityPO Box 15 11 5066041 Saarbru?cken, Germanyourioupi@coli.uni-sb.deAbstractThis paper shows that the web can be em-ployed to obtain frequencies for bigramsthat are unseen in a given corpus.
Wedescribe a method for retrieving countsfor adjective-noun, noun-noun, and verb-object bigrams from the web by queryinga search engine.
We evaluate this methodby demonstrating that web frequenciesand correlate with frequencies obtainedfrom a carefully edited, balanced corpus.We also perform a task-based evaluation,showing that web frequencies can reliablypredict human plausibility judgments.1 IntroductionIn two recent papers, Banko and Brill (2001a;2001b) criticize the fact that current NLP algo-rithms are typically optimized, tested, and comparedon fairly small data sets (corpora with millions ofwords), even though data sets several orders of mag-nitude larger are available, at least for some tasks.Banko and Brill go on to demonstrate that learningalgorithms typically used for NLP tasks benefit sig-nificantly from larger training sets, and their perfor-mance shows no sign of reaching an asymptote asthe size of the training set increases.Arguably, the largest data set that is availablefor NLP is the web, which currently consists ofat least 968 million pages.1 Data retrieved fromthe web therefore provides enormous potential1This is the number of pages indexed by Google inMarch 2002, as estimated by Search Engine Showdown (seehttp://www.searchengineshowdown.com/).for training NLP algorithms, if Banko and Brill?sfindings generalize.
There is a small body ofexisting research that tries to harness the potentialof the web for NLP.
Grefenstette and Nioche (2000)and Jones and Ghani (2000) use the web togenerate corpora for languages where elec-tronic resources are scarce, while Resnik (1999)describes a method for mining the web for bilin-gual texts.
Mihalcea and Moldovan (1999) andAgirre and Martinez (2000) use the web for wordsense disambiguation, and Volk (2001) proposes amethod for resolving PP attachment ambiguitiesbased on web data.A particularly interesting application is pro-posed by Grefenstette (1998), who uses the webfor example-based machine translation.
His task isto translate compounds from French into English,with corpus evidence serving as a filter for candi-date translations.
As an example consider the Frenchcompound groupe de travail.
There are five transla-tion of groupe and three translations for travail (inthe dictionary that Grefenstette (1998) is using), re-sulting in 15 possible candidate translations.
Onlyone of them, viz., work group has a high corpusfrequency, which makes it likely that this is thecorrect translation into English.
Grefenstette (1998)observes that this approach suffers from an acutedata sparseness problem if the corpus counts areobtained from a conventional corpus such as theBritish National Corpus (BNC) (Burnard, 1995).However, as Grefenstette (1998) demonstrates, thisproblem can be overcome by obtaining countsthrough web searches, instead of relying on theBNC.
Grefenstette (1998) therefore effectively usesthe web as a way of obtaining counts for compoundsthat are sparse in the BNC.Association for Computational Linguistics.Language Processing (EMNLP), Philadelphia, July 2002, pp.
230-237.Proceedings of the Conference on Empirical Methods in NaturalWhile this is an important initial result, it raisesthe question of the generality of the proposed ap-proach to overcoming data sparseness.
It remainsto be shown that web counts are generally usefulfor approximating data that is sparse or unseen ina given corpus.
It seems possible, for instance, thatGrefenstette?s (1998) results are limited to his par-ticular task (filtering potential translations) or to hisparticular linguistic phenomenon (noun-noun com-pounds).
Another potential problem is the fact thatweb counts are far more noisy than counts obtainedfrom a well-edited, carefully balanced corpus suchas the BNC.
The effect of this noise on the useful-ness of the web counts is largely unexplored.The aim of the present paper is to generalizeGrefenstette?s (1998) findings by testing the hypoth-esis that the web can be employed to obtain frequen-cies for bigrams that are unseen in a given corpus.Instead of having a particular task in mind (whichwould introduce a sampling bias), we rely on sets ofbigrams that are randomly selected from the corpus.We use a web-based approach not only for noun-noun bigrams, but also for adjective-noun and verb-object bigrams, so as to explore whether this ap-proach generalizes to different predicate-argumentcombinations.
We evaluate our web counts in twodifferent ways: (a) comparison with actual corpusfrequencies, and (b) task-based evaluation (predict-ing human plausibility judgments).2 Obtaining Frequencies from the Web2.1 Sampling BigramsTwo types of adjective-noun bigrams were used inthe present study: seen bigrams, i.e., bigrams thatoccur in a given corpus, and unseen bigrams, i.e.,bigrams that fail to occur in the corpus.
For theseen adjective-noun bigrams, we used the data ofLapata et al (1999), who compiled a set of 90 bi-grams as follows.
First, 30 adjectives were randomlychosen from a lemmatized version of the BNC sothat each adjective had exactly two senses accord-ing to WordNet (Miller et al, 1990) and was unam-biguously tagged as ?adjective?
98.6% of the time.The 30 adjectives ranged in BNC frequency from 1.9to 49.1 per million.
Gsearch (Corley et al, 2001),a chart parser which detects syntactic patterns in atagged corpus by exploiting a user-specified con-text free grammar and a syntactic query, was usedto extract all nouns occurring in a head-modifier re-lationship with one of the 30 adjectives.
Bigrams in-volving proper nouns or low-frequency nouns (lessthan 10 per million) were discarded.
For each ad-jective, the set of bigrams was divided into three fre-quency bands based on an equal division of the rangeof log-transformed co-occurrence frequencies.
Thenone bigram was chosen at random from each band.Lapata et al (2001) compiled a set of 90 unseenadjective-noun bigrams using the same 30 adjec-tives.
For each adjective, the Gsearch chunker wasused to compile a list of all nouns that failed to co-occur in a head-modifier relationship with the adjec-tive.
Proper nouns and low-frequency nouns werediscarded from this list.
Then each adjective waspaired with three randomly chosen nouns from itslist of non-co-occurring nouns.For the present study, we applied the procedureused by Lapata et al (1999) and Lapata et al (2001)to noun-noun bigrams and to verb-object bigrams,creating a set of 90 seen and 90 unseen bigrams foreach type of predicate-argument relationship.
Morespecifically, 30 nouns and 30 verbs were chosen ac-cording to the same criteria proposed for the adjec-tive study (i.e., minimal sense ambiguity and unam-biguous part of speech).
All nouns modifying one ofthe 30 nouns were extracted from the BNC using aheuristic which looks for consecutive pairs of nounsthat are neither preceded nor succeeded by anothernoun (Lauer, 1995).
Verb-object bigrams for the30 preselected verbs were obtained from the BNCusing Cass (Abney, 1996), a robust chunk parser de-signed for the shallow analysis of noisy text.
Theparser?s output was post-processed to remove brack-eting errors and errors in identifying chunk cate-gories that could potentially result in bigrams whosemembers do not stand in a verb-argument relation-ship (see Lapata (2001) for details on the filteringprocess).
Only nominal heads were retained fromthe objects returned by the parser.
As in the adjec-tive study, noun-noun bigrams and verb-object bi-grams with proper nouns or low-frequency nouns(less than 10 per million) were discarded.
The setsof noun-noun and verb-object bigrams were dividedinto three frequency bands and one bigram was cho-sen at random from each band.The procedure described by Lapata et al (2001)was followed for creating sets of unseen noun-nounand verb-object bigrams: for each of noun or verb,we compiled a list of all nouns with which it failedto co-occur with in a noun-noun or verb-object bi-gram in the BNC.
Again, Lauer?s (1995) heuristicand Abney?s (1996) partial parser were used to iden-tify bigrams, and proper nouns and low-frequencynouns were excluded.
For each noun and verb, threebigrams were randomly selected from the set of theirnon-co-occurring nouns.Table 1 lists examples for the seen and unseennoun-noun and verb-object bigrams generated bythis procedure.2.2 Obtaining Web CountsWeb counts for bigrams were obtained using a sim-ple heuristic based on queries to the search enginesAltavista and Google.
All search terms took intoaccount the inflectional morphology of nouns andverbs.The search terms for verb-object bigrams matchednot only cases in which the object was directly ad-jacent to the verb (e.g., fulfill obligation), but alsocases where there was an intervening determiner(e.g., fulfill the/an obligation).
The following searchterms were used for adjective-noun, noun-noun, andverb-object bigrams, respectively:(1) "A N", where A is the adjective and N is the sin-gular or plural form of the noun.
(2) "N1 N2" where N1 is the singular form of thefirst noun and N2 is the singular or plural formof the second noun.
(3) "V Det N" where V is the infinitive, singularpresent, plural present, past, perfect, or gerundfor of the verb, Det is the determiner the, a orthe empty string, and N is the singular or pluralform of the noun.Note that all searches were for exact matches, whichmeans that the search terms were required to be di-rectly adjacent on the matching page.
This is en-coded using quotation marks to enclose the searchterm.
All our search terms were in lower case.For Google, the resulting bigram frequencieswere obtained by adding up the number of pagesthat matched the expanded forms of the search termsin (1), (2), and (3).
Altavista returns not only thenumber of matches, but also the number of wordsadj-noun noun-noun verb-objectAltavista 14 10 16Google 5 3 5Table 2: Number of zero counts returned by thequeries to search engines (unseen bigrams)that match the search term.
We used this count, as ittakes multiple matches per page into account, and isthus likely to produce more accurate frequencies.The process of obtaining bigram frequencies fromthe web can be automated straightforwardly using ascript that generates all the search terms for a givenbigram (from (1)?
(3)), issues an Altavista or Googlequery for each of the search terms, and then addsup the resulting number of matches for each bigram.We applied this process to all the bigrams in our dataset, covering seen and unseen adjective-noun, noun-noun, and verb-object bigrams, i.e., 540 bigrams intotal.A small number of bigrams resulted in zerocounts, i.e., they failed to yield any matches in theweb search.
Table 2 lists the number of zero bigramsfor both search engines.
Note that Google returnedfewer zeros than Altavista, which presumably indi-cates that it indexes a larger proportion of the web.We adjusted the zero counts by setting them to one.This was necessary as all further analyses were car-ried out on log-transformed frequencies.Table 3 lists the descriptive statistics for thebigram counts we obtained using Altavista andGoogle.From these data, we computed the average fac-tor by which the web counts are larger than theBNC counts.
The results are given in Table 4 andindicate that the Altavista counts are between 331and 467 times larger than the BNC counts, whilethe Google counts are between 759 and 977 timeslarger than the BNC counts.
As we know the sizeof the BNC (100 million words), we can use thesefigures to estimate the number of words on the web:between 33.1 and 46.7 billion words for Altavista,and between 75.9 and 97.7 billion words for Google.These estimates are in the same order of magnitudeas Grefenstette and Nioche?s (2000) estimate that48.1 billion words of English are available on theweb (based on Altavista counts in February 2000).noun-noun bigramshigh medium low unseen predicateprocess 1.14 user .95 gala 0 collection, clause, coat directorytelevision 1.53 satellite .95 edition 0 chain, care, vote broadcastplasma 1.78 nylon 1.20 unit .60 fund, theology, minute membraneverb-object bigramspredicate high medium low unseenfulfill obligation 3.87 goal 2.20 scripture .69 participant, muscle, gradeintensify problem 1.79 effect 1.10 alarm 0 score, quota, chestchoose name 3.74 law 1.61 series 1.10 lift, bride, listenerTable 1: Example stimuli for seen and unseen noun-noun and verb-object bigrams (with log-transformedBNC counts)seen bigramsadj-noun noun-noun verb-objectMin Max Mean SD Min Max Mean SD Min Max Mean SDAltavista 0 5.67 3.55 1.06 .67 6.28 3.41 1.21 0 5.46 3.20 1.14Google 1.26 5.98 3.89 1.00 .90 6.11 3.66 1.20 0 5.85 3.56 1.16BNC 0 2.19 .90 .69 0 2.14 .74 .64 0 2.55 .68 .58unseen bigramsadj-noun noun-noun verb-objectMin Max Mean SD Min Max Mean SD Min Max Mean SDAltavista 0 4.04 1.29 .94 0 3.80 1.08 1.12 0 3.72 1.38 1.06Google 0 3.99 1.68 .96 0 4.00 1.42 1.09 0 4.07 1.76 1.04Table 3: Descriptive statistics for web counts and BNC counts (log-transformed)adj-noun noun-noun verb-objectAltavista 447 467 331Google 977 831 759Table 4: Average factor by which the web counts arelarger than the BNC counts (seen bigrams)3 Evaluation3.1 Evaluation Against Corpus FrequenciesWhile the procedure for obtaining web counts de-scribed in Section 2.2 is very straightforward, it alsohas obvious limitations.
Most importantly, it is basedon bigrams formed by adjacent words, and fails totake syntactic variants into account (other than in-tervening determiners for verb-object bigrams).
Inthe case of Google, there is also the problem that thecounts are based on the number of matching pages,not the number of matching words.
Finally, there isthe problem that web data is very noisy and unbal-anced compared to a carefully edited corpus like theBNC.Given these limitations, it is necessary to exploreif there is a reliable relationship between web countsand BNC counts.
Once this is assured, we can ex-plore the usefulness of web counts for overcomingdata sparseness.
We carried out a correlation analy-sis to determine if there is a linear relationship be-tween the BNC counts and Altavista and Googlecounts.
The results of this analysis are listed in Ta-ble 5.
All correlation coefficients reported in this pa-per refer to Pearson?s r and were computed on log-transformed counts.A high correlation coefficient was obtained acrossthe board, ranging from .675 to .822 for Altavistacounts and from .737 to .849 for Google counts.This indicates that web counts approximate BNCcounts for the three types of bigrams under inves-tigation, with Google counts slightly outperform-ing Altavista counts.
We conclude that our simpleadj-noun noun-noun verb-objectAltavista .821** .744** .675**Google .849** .737** .751***p < .05 (2-tailed) **p < .01 (2-tailed)Table 5: Correlation of BNC counts with web counts(seen bigrams)heuristics (see (1)?
(3)) are sufficient to obtain use-ful frequencies from the web.
It seems that the largeamount of data available for web counts outweighsthe associated problems (noisy, unbalanced, etc.
).Note that the highest coefficients were obtainedfor adjective-noun bigrams, which probably indi-cates that this type of predicate-argument relation-ship is least subject to syntactic variation and thusleast affected by the simplifications of our searchheuristics.3.2 Task-based EvaluationPrevious work has demonstrated that corpus countscorrelate with human plausibility judgments foradjective-noun bigrams.
This results holds for bothseen bigrams (Lapata et al, 1999) and for unseenbigrams whose counts were recreated using smooth-ing techniques (Lapata et al, 2001).
Based on thesefindings, we decided to evaluate our web counts onthe task of predicting plausibility ratings.
If the webcounts for bigrams correlate with plausibility judg-ments, then this indicates that the counts are valid,in the sense of being useful for predicting intuitiveplausibility.Lapata et al (1999) and Lapata et al (2001) col-lected plausibility ratings for 90 seen and 90 unseenadjective-noun bigrams (see Section 2.1) using mag-nitude estimation.
Magnitude estimation is an exper-imental technique standardly used in psychophysicsto measure judgments of sensory stimuli (Stevens,1975), which Bard et al (1996) and Cowart (1997)have applied to the elicitation of linguistic judg-ments.
Magnitude estimation requires subjects toassign numbers to a series of linguistic stimuli ina proportional fashion.
Subjects are first exposedto a modulus item, which they assign an arbitrarynumber.
All other stimuli are rated proportionalto the modulus.
In the experiments conducted byLapata et al (1999) and Lapata et al (2001), nativespeakers of English were presented with adjective-noun bigrams and were asked to rate the degreeof adjective-noun fit proportional to the modulusitem.
The resulting judgments were normalized bydividing them by the modulus value and by log-transforming them.
Lapata et al (1999) report a cor-relation of .570 between mean plausibility judg-ments and BNC counts for the seen adjective-noun bigrams.
For unseen adjective-noun bigrams,Lapata et al (2001) found a correlation of .356 be-tween mean judgments and frequencies recreatedusing class-based smoothing (Resnik, 1993).In the present study, we used the plausibil-ity judgments collected by Lapata et al (1999) andLapata et al (2001) for adjective-noun bigrams andconducted additional experiments to obtain noun-noun and verb-object judgments for the materi-als described in Section 2.1.
We used the sameexperimental procedure as the original study (seeLapata et al (1999) and Lapata et al (2001) for de-tails).
Four experiments were carried out, one eachfor seen and unseen noun-noun bigrams, and forseen and unseen verb-object bigrams.
Unlike theadjective-noun and the noun-noun bigrams, theverb-object bigrams were not presented to subjectsin isolation, but embedded in a minimal sentencecontext involving a proper name as the subject(e.g., Paul fulfilled the obligation).The experiments were conducted over the webusing the WebExp software package (Keller et al,1998).
A series of previous studies has shown thatdata obtained using WebExp closely replicates re-sults obtained in a controlled laboratory setting;this was demonstrated for acceptability judgments(Keller and Alexopoulou, 2001), co-reference judg-ments (Keller and Asudeh, 2001), and sentencecompletions (Corley and Scheepers, 2002).
Thesereferences also provide a detailed discussion of theWebExp experimental setup.Table 6 lists the descriptive statistics for allsix judgment experiments: the original experimentsby Lapata et al (1999) and Lapata et al (2001) foradjective-noun bigrams, and our new ones for noun-noun and verb-object bigrams.We used correlation analysis to compare webcounts with plausibility judgments for seenadjective-noun, noun-noun, and verb-object bi-grams.
Table 7 (top half) lists the correlationcoefficients that were obtained when correlat-adj-noun bigrams noun-noun bigrams verb-object bigramsN Min Max Mean SD N Min Max Mean SD N Min Max Mean SDSeen 30 ?.85 .11 ?.13 .22 25 ?.15 .69 .40 .21 27 ?.52 .45 .12 .24Unseen 41 ?.56 .37 ?.07 .20 25 ?.49 .52 ?.01 .23 21 ?.51 .28 ?.16 .22Table 6: Descriptive statistics for plausibility judgments (log-transformed); N is the number of subjects usedin each experimenting log-transformed web and BNC counts withlog-transformed plausibility judgments.The results show that both Altavista and Googlecounts correlate with plausibility judgments for seenbigrams.
Google slightly outperforms Altavista: thecorrelation coefficient for Google ranges from .624to .693, while for Altavista, it ranges from .638 to.685.
A surprising result is that the web counts con-sistently achieve a higher correlation with the judg-ments than the BNC counts, which range from .488to .569.
We carried out a series of one-tailed t-teststo determine if the differences between the correla-tion coefficients for the web counts and the corre-lation coefficients for the BNC counts were signifi-cant.
For the adjective-noun bigrams, the differencebetween the BNC coefficient and the Altavista coef-ficient failed to reach significance (t(87) = 1.46, p >.05), while the Google coefficient was significantlyhigher than the BNC coefficient (t(87) = 1.78, p <.05).
For the noun-noun bigrams, both the Altavistaand the Google coefficients were significantly higherthan the BNC coefficient (t(87) = 2.94, p < .01 andt(87) = 3.06, p < .01).
Also for the verb-object bi-grams, both the Altavista coefficient and the Googlecoefficient were significantly higher than the BNCcoefficient (t(87) = 2.21, p < .05 and t(87) = 2.25,p < .05).
In sum, for all three types of bigrams, thecorrelation coefficients achieved with Google weresignificantly higher than the ones achieved with theBNC.
For Altavista, the noun-noun and the verb-object coefficients were higher than the coefficientsobtained from the BNC.Table 7 (bottom half) lists the correlations co-efficients obtained by comparing log-transformedjudgments with log-transformed web counts for un-seen adjective-noun, noun-noun, and verb-object bi-grams.
We observe that the web counts consistentlyshow a significant correlation with the judgments,the coefficient ranging from .466 to .588 for Al-seen bigramsadj-noun noun-noun verb-objectAltavista .642** .685** .638**Google .650** .693** .624**BNC .569** .517** .488**unseen bigramsAltavista .466** .588** .568**Google .446** .611** .542***p < .05 (2-tailed) **p < .01 (2-tailed)Table 7: Correlation of plausibility judgments withweb counts and BNC countstavista counts, and from .446 to .611 for the Googlecounts.
Note that a small number of bigrams pro-duced zero counts even in our web queries; these fre-quencies were set to one for the correlation analysis(see Section 2.2).To conclude, this evaluation demonstrated thatweb counts reliably predict human plausibility judg-ments, both for seen and for unseen predicate-argument bigrams.
In the case of Google countsfor seen bigrams, we were also able to show thatweb counts are a better predictor of human judg-ments than BNC counts.
These results show that ourheuristic method yields useful frequencies; the sim-plifications we made in obtaining the counts, as wellas the fact that web data are noisy, seem to be out-weighed by the fact that the web is up to three ordersof magnitude larger than the BNC (see our estimatein Section 2.2).4 ConclusionsThis paper explored a novel approach to overcomingdata sparseness.
If a bigram is unseen in a given cor-pus, conventional approaches recreate its frequencyusing techniques such as back-off, linear interpo-lation, class-based smoothing or distance-weightedaveraging (see Dagan et al (1999) and Lee (1999)for overviews).
The approach proposed here doesnot recreate the missing counts, but instead re-trieves them from a corpus that is much larger (butalso much more noisy) than any existing corpus: itlaunches queries to a search engine in order to deter-mine how often a bigram occurs on the web.We systematically investigated the validity ofthis approach by using it to obtain frequencies forpredicate-argument bigrams (adjective-noun, noun-noun, and verb-object bigrams).
We first appliedthe approach to seen bigrams randomly sampledfrom the BNC.
We found that the counts obtainedfrom the web are highly correlated with the countsobtained from the BNC, which indicates that webqueries can generate frequencies that are compara-ble to the ones obtained from a balanced, carefullyedited corpus such as the BNC.Secondly, we performed a tasked-based evalua-tion that used the web frequencies to predict hu-man plausibility judgments for predicate-argumentbigrams.
The results show that web counts corre-late reliably with judgments, for all three types ofpredicate-argument bigrams tested, both seen andunseen.
For the seen bigrams, we showed that theweb frequencies correlate better with judged plausi-bility than the BNC frequencies.To summarize, we have proposed a simple heuris-tic for obtaining bigram counts from the web.
Usingtwo different types of evaluation, we demonstratedthat this simple heuristic is sufficient to obtain usefulfrequency estimates.
It seems that the large amountof data available outweighs the problems associatedwith using the web as a corpus (such as the fact thatit is noisy and unbalanced).In future work, we plan to compare web countsfor unseen bigrams with counts recreated usingstandard smoothing algorithms, such as similarity-based smoothing (Dagan et al, 1999) or class-basedsmoothing (Resnik, 1993).
If web counts correlatereliable with smoothed counts, then this providesfurther evidence for our claim that the web can beused to overcome data sparseness.ReferencesSteve Abney.
1996.
Partial parsing via finite-state cas-cades.
In John Carroll, editor, Workshop on RobustParsing, pages 8?15, 8th European Summer School inLogic, Language and Information, Prague.Eneko Agirre and David Martinez.
2000.
Exploringautomatic word sense disambiguation with decisionlists and the web.
In Proceedings of the 18th In-ternational Conference on Computational Linguistics,Saarbru?cken/Luxembourg/Nancy.Michele Banko and Eric Brill.
2001a.
Mitigating thepaucity-of-data problem: Exploring the effect of train-ing corpus size on classifier performance for naturallanguage processing.
In James Allan, editor, Proceed-ings of the 1st International Conference on HumanLanguage Technology Research, San Francisco.
Mor-gan Kaufmann.Michele Banko and Eric Brill.
2001b.
Scaling to veryvery large corpora for natural language disambigua-tion.
In Proceedings of the 39th Annual Meeting ofthe Association for Computational Linguistics and the10th Conference of the European Chapter of the Asso-ciation for Computational Linguistics, Toulouse.Ellen Gurman Bard, Dan Robertson, and Antonella So-race.
1996.
Magnitude estimation of linguistic ac-ceptability.
Language, 72(1):32?68.Lou Burnard, 1995.
Users Guide for the British NationalCorpus.
British National Corpus Consortium, OxfordUniversity Computing Service.Martin Corley and Christoph Scheepers.
2002.
Syntac-tic priming in English sentence production: Categori-cal and latency evidence from an internet-based study.Psychonomic Bulletin and Review, 9(1).Steffan Corley, Martin Corley, Frank Keller, Matthew W.Crocker, and Shari Trewin.
2001.
Finding syntac-tic structure in unparsed corpora: The Gsearch cor-pus query system.
Computers and the Humanities,35(2):81?94.Wayne Cowart.
1997.
Experimental Syntax: ApplyingObjective Methods to Sentence Judgments.
Sage Pub-lications, Thousand Oaks, CA.Ido Dagan, Lillian Lee, and Fernando Pereira.
1999.Similarity-based models of word cooccurrence prob-abilities.
Machine Learning, 34(1):43?69.Gregory Grefenstette and Jean Nioche.
2000.
Estima-tion of English and non-English language use on theWWW.
In Proceedings of the RIAO Conference onContent-Based Multimedia Information Access, pages237?246, Paris.Gregory Grefenstette.
1998.
The World Wide Web as aresource for example-based machine translation tasks.In Proceedings of the ASLIB Conference on Translat-ing and the Computer, London.Rosie Jones and Rayid Ghani.
2000.
Automaticallybuilding a corpus for a minority language from theweb.
In Proceedings of the Student Research Work-shop at the 38th Annual Meeting of the Association forComputational Linguistics, pages 29?36, Hong Kong.Frank Keller and Theodora Alexopoulou.
2001.
Phonol-ogy competes with syntax: Experimental evidence forthe interaction of word order and accent placement inthe realization of information structure.
Cognition,79(3):301?372.Frank Keller and Ash Asudeh.
2001.
Constraints on lin-guistic coreference: Structural vs. pragmatic factors.In Johanna D. Moore and Keith Stenning, editors, Pro-ceedings of the 23rd Annual Conference of the Cog-nitive Science Society, pages 483?488, Mahwah, NJ.Lawrence Erlbaum Associates.Frank Keller, Martin Corley, Steffan Corley, LarsKonieczny, and Amalia Todirascu.
1998.
WebExp:A Java toolbox for web-based psychological experi-ments.
Technical Report HCRC/TR-99, Human Com-munication Research Centre, University of Edinburgh.Maria Lapata, Scott McDonald, and Frank Keller.
1999.Determinants of adjective-noun plausibility.
In Pro-ceedings of the 9th Conference of the European Chap-ter of the Association for Computational Linguistics,pages 30?36, Bergen.Maria Lapata, Frank Keller, and Scott McDonald.
2001.Evaluating smoothing algorithms against plausibilityjudgments.
In Proceedings of the 39th Annual Meet-ing of the Association for Computational Linguisticsand the 10th Conference of the European Chapter ofthe Association for Computational Linguistics, pages346?353, Toulouse.Maria Lapata.
2001.
A corpus-based account of regularpolysemy: The case of context-sensitive adjectives.
InProceedings of the 2nd Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics, Pittsburgh, PA.Mark Lauer.
1995.
Designing Statistical LanguageLearners: Experiments on Compound Nouns.
Ph.D.thesis, Macquarie University, Sydney.Lilian Lee.
1999.
Measures of distributional similarity.In Proceedings of the 37th Annual Meeting of the As-sociation for Computational Linguistics, pages 25?32,University of Maryland, College Park.Rada Mihalcea and Dan Moldovan.
1999.
A methodfor word sense disambiguation of unrestricted text.
InProceedings of the 37th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 152?158,University of Maryland, College Park.George A. Miller, Richard Beckwith, Christiane Fell-baum, Derek Gross, and Katherine J. Miller.
1990.Introduction to WordNet: An on-line lexical database.International Journal of Lexicography, 3(4):235?244.Philip Stuart Resnik.
1993.
Selection and Information:A Class-Based Approach to Lexical Relationships.Ph.D.
thesis, University of Pennsylvania, Philadelphia,PA.Philip Resnik.
1999.
Mining the web for bilingual text.In Proceedings of the 37th Annual Meeting of the As-sociation for Computational Linguistics, University ofMaryland, College Park.S.
S. Stevens.
1975.
Psychophysics: Introduction to itsPerceptual, Neural, and Social Prospects.
John Wiley,New York.Martin Volk.
2001.
Exploiting the WWW as a corpusto resolve PP attachment ambiguities.
In Paul Rayson,Andrew Wilson, Tony McEnery, Andrew Hardie, andShereen Khoja, editors, Proceedings of the CorpusLinguistics Conference, pages 601?606, Lancaster.
