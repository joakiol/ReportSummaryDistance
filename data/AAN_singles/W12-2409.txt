Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 73?81,Montre?al, Canada, June 8, 2012. c?2012 Association for Computational LinguisticsActive Learning for Coreference ResolutionTimothy A. Miller and Dmitriy Dligach and Guergana K. SavovaChildren?s Hospital Bostonand Harvard Medical School300 Longwood Ave.Enders 141Boston, MA 02115, USA{Timothy.Miller,Dmitriy.Dligach,Guergana.Savova}@childrens.harvard.eduAbstractActive learning can lower the cost of anno-tation for some natural language processingtasks by using a classifier to select informa-tive instances to send to human annotators.
Ithas worked well in cases where the training in-stances are selected one at a time and requireminimal context for annotation.
However,coreference annotations often require somecontext and the traditional active learning ap-proach may not be feasible.
In this work weexplore various active learning methods forcoreference resolution that fit more realisti-cally into coreference annotation workflows.1 IntroductionCoreference resolution is the task of deciding whichentity mentions in a text refer to the same entity.Solving this problem is an important part of thelarger task of natural language understanding in gen-eral.
The clinical domain offers specific tasks whereit is easy to see that correctly resolving coreferenceis important.
For example, one important task in theclinical domain is template filling for the Clinical El-ements Model (CEM).1 This task involves extractingvarious pieces of information about an entity and fit-ting the information into a standard data structurethat can be reasoned about.
An example CEM tem-plate is that for Disease with attributes for Body Lo-cation, Associated Sign or Symptom, Subject, Nega-tion, Uncertainty, and Severity.
Since a given entitymay have many different attributes and relations, it1http://intermountainhealthcare.org/cemmay be mentioned multiple times in a text.
Coref-erence resolution is important for this task becauseit must be known that all the attributes and relationsapply to the same entity so that a single CEM tem-plate is filled in for an entity, rather than creating anew template for each mention of the entity.2 Background2.1 Coreference ResolutionSpace does not permit a thorough review of coref-erence resolution, but recent publications coveredthe history and current state of the art for both thegeneral domain and the clinical domain (Ng, 2010;Pradhan et al, 2011; Zheng et al, 2011).The system used here (Zheng et al, 2012) isan end-to-end coreference resolution system, mean-ing that the algorithm receives no gold standard in-formation about mentions, named entity types, orany linguistic information.
The coreference res-olution system is a module of the clinical Tex-tual Analysis and Knowledge Extraction System(cTAKES) (Savova et al, 2010) that is trained onclinical data.
It takes advantage of named entityrecognition (NER) and categorization to detect en-tity mentions, and uses several cTAKES modulesas feature generators, including the NER module,a constituency parser module, and a part of speechtagging module.The system architecture is based on the pairwisediscriminative classification approach to the coref-erence resolution problem.
In that paradigm, pairsof mentions are classified as coreferent or not, andthen some reconciliation must be done on all of the73links so that there are no conflicts in the clusters.The system uses support vector machines (SVMs)as the pairwise classifiers, and conflicts are avoidedby only allowing an anaphor to link with one an-tecedent, specifically that antecedent the classifierlinks with the highest probability.There are separate pairwise classifiers for namedentity and pronominal anaphor types.
In the domainof clinical narratives, person mentions and personalpronouns in particular are not especially challeng-ing ?
the vast majority of person mentions are thepatient.
In addition, pronoun mentions, while im-portant, are relatively rare.
Thus we are primarilyinterested in named entity coreference classification,and we use that classifier as the basis of the work de-scribed here.The feature set of this system is similar to thatused by Ng and Cardie (2002).
That system in-cludes features based on surface form of the men-tions, shallow syntactic information, and lexical se-mantics from WordNet.
The system used here hasa similar feature set but uses Unified Medical Lan-guage System (UMLS)2 semantic features as it isintended for clinical text, and also incorporates sev-eral syntactic features extracted from constituencyparses extracted from cTAKES.To generate training data for active learning simu-lations, mention detection is run first (cTAKES con-tains a rule-based NER system) to find named en-tities and a constituency parser situates entities ina syntax tree).
For each entity found, the systemworks backwards through all other mentions withina ten sentence window.
For each candidate anaphor-antecedent pair, a feature vector is extracted usingthe features briefly described above.2.2 Active LearningActive Learning (AL) is a popular approach to se-lecting unlabeled data for annotation (Settles, 2010)that can potentially lead to drastic reductions in theamount of annotation that is necessary for train-ing an accurate statistical classifier.
Unlike passivelearning, where the data is sampled for annotationrandomly, AL delegates data selection to the clas-sifier.
AL is an iterative process that operates byfirst training a classifier on a small sample of the2http://www.nlm.nih.gov/research/umls/data known as the seed examples.
The classifieris subsequently applied to a pool of unlabeled datawith the purpose of selecting additional examplesthe classifier views as informative.
The selected datais annotated and the cycle is repeated, allowing thelearner to quickly refine the decision boundary be-tween classes.
One common approach to assessingthe informativeness is uncertainty sampling (Lewisand Gale, 1994; Schein and Ungar, 2007), in whichthe learner requests a label for the instance it is mostuncertain how to label.
In this work, we base ourinstance selection on the distance to the SVM de-cision boundary (Tong and Koller, 2002), assumingthat informative instances tend to concentrate nearthe boundary.Most AL work focuses on instance selectionwhere the unit of selection is one instance repre-sented as a feature vector.
In this paper we alsoattempt document selection, where the unit of se-lection is a document, typically containing multi-ple coreference pairs each represented as a featurevector.
The most obvious way to extend a sin-gle instance informativeness metric to the documentscenario is to aggregate the informativeness scores.Several uncertainty metrics have been proposed thatfollow that route to adapt single instance selectionto multiple instance scenarios (Settles et al, 2008;Tomanek et al, 2009).
We borrow some of thesemetrics and propose several new ones.To the best of our knowledge only one workexists that explores AL for coreference resolution.Gasperin (2009) experiments with an instance basedapproach in which batches of anaphoric pairs are se-lected on each iteration of AL.
In these experiments,AL did not outperform the passive learning baseline,probably due to selecting batches of large size.3 Active Learning Configurations3.1 Instance SelectionThe first active learning model we considered selectsindividual training instances ?
putatively coreferentmention pairs.
This method is quite easy to simu-late, and follows naturally from most of the theo-retical active learning literature, but it has the draw-back of being seemingly unrealistic as an annotationparadigm.
That is, since coreference can span acrossan entire document, it is probably not practical to74have a human expert annotate only a single instanceat a time when a given instance may require manysentences of reading in order to contextualize the in-stance and properly label it.
Moreover, even if suchan annotation scheme proved viable, it may resultin an annotated corpus that is only valuable for onetype of coreference system architecture.Nonetheless, active learning for coreference at theinstance level is still useful.
First, since this methodmost closely follows the successful active learningliterature by using the smallest discrete problems, itcan serve as a proof of concept for active learningin the coreference task ?
if it does not work well atthis level, it probably will not work at the documentlevel.
Previous results (Gasperin, 2009) have shownthat certain multiple instance methods do not workfor coreference resolution, so testing on smaller se-lection sizes first can ensure that active learning iseven viable at that scale.
In addition, though in-stance selection may not be feasible for real worldannotations, individual instances and metrics for se-lecting them are usually used as building blocks formore complex methods.
In order for this to be pos-sible it must be shown that the instances themselveshave some value.3.2 Document SelectionActive learning with document selection is a muchmore realistic representation of conventional anno-tation methods.
Conventionally, a set of documentsis selected, and each document is annotated exhaus-tively for coreference (Pradhan et al, 2011; Savovaet al, 2011).
Document selection fits into this work-flow very naturally, by selecting the next documentto annotate exhaustively based on some metric ofwhich document has the best instances.
In theory,this method can save annotation time by only anno-tating the most valuable documents.Document selection is somewhat similar to theconcept of batch-mode active learning, whereinmultiple instances are selected at once, thoughbatch-mode learning is usually intended to solve adifferent problem, that of an asymmetry betweenclassifier training speed and annotation speed (Set-tles, 2010).
A more important difference is that doc-ument selection requires that all of the instances inthe batch must come from the same document.
Thus,one might expect a priori that document selectionfor active learning will not perform as well as in-stance selection.
However, it is possible that evensmaller gains will be valuable for improving annota-tion time, and the more robust nature of a corpus an-notated in such a way will make the long term bene-fits worthwhile.In this work, we propose several metrics for se-lecting documents to annotate, all of which arebased on instance level uncertainty.
In the fol-lowing descriptions, D is the set of documents, dis a single document, d?
is the selected document,Instances(d) is a function which returns the set ofpair instances in document d, i is an instance, dist(i)is a function which returns the distance of instance ifrom the classification boundary, and I is the indica-tor function, which takes the value 1 if its argumentis true and 0 otherwise.
Note that high uncertaintyoccurs when Abs(dist(i)) approaches 0.?
Best instance ?
This method uses the un-certainty sampling criteria on instances, andselects the document containing the in-stance the classifier is least certain about.d?
= argmind?D[mini?Instances(d)Abs(dist(i))]?
Highest average uncertainty ?
This methodcomputes the average uncertainty of allinstances in a document, and selects thedocument with the highest average uncertainty.d?
= argmind?D1|Instances(d)|?i?Instances(d)Abs(dist(i))?
Least bad example ?
This method usesuncertainty sampling criteria to find thedocument whose most certain example isleast certain, in other words the documentwhose most useless example is least useless.d?
= argmind?Dmaxi?Instances(d)Abs(dist(i))?
Narrow band ?
This method creates an un-certainty band around the discriminatingboundary and selects the document withthe most examples inside that narrow band.d?
= argmaxd?D?i?Instances(d) I(Abs(dist(i) < 0.2))?
Smallest spread ?
This method computes thedistance between the least certain and mostcertain instances and selects the documentminimizing that distance.75d?
= argmind?D[maxi?Instances(d)(Abs(dist(i)))?mini?Instances(d)(Abs(dist(i)))]?
Most positives ?
This method totals thenumber of positive predicted instancesin each document and selects the doc-ument with the most positive instances.d?
= argmaxd?D?i?Instances(d) I(dist(i) > 0)?
Positive ratio ?
This method calculatesthe percentage of positive predicted in-stances in each document and selects thedocument with the highest percentage.d?
= argmaxd?D?i?Instances(d) I(dist(i)>0)|Instances(d)|Many of these are straightforward adaptations ofthe instance uncertainty criteria, but others deservea bit more explanation.
The most positives and pos-itive ratio metrics are based on the observation thatthe corpus is somewhat imbalanced ?
for every posi-tive instance there are roughly 20 negative instances.These metrics try to account for the possibility thatinstance selection focuses on positive instances.
Theaverage uncertainty is an obvious attempt to turn in-stance metrics into document metrics, but narrowband and smallest spread metrics attempt to do thesame thing while accounting for skew in the distri-bution of ?good?
and ?bad?
instances.3.3 Document-Inertial Instance SelectionOne of the biggest impracticalities of instance se-lection is that labeling any given instance may re-quire reading a fair amount of the document, sincethe antecedent and anaphor can be quite far apart.Thus, any time savings accumulated by only anno-tating an instance is reduced since the reading timeper instance is probably increased.It is also possible that document selection goestoo far in the other direction, and requires toomany useless instances to be annotated to achievegains.
Therefore, we propose a hybrid method ofdocument-inertial instance selection which attemptsto combine aspects of instance selection and docu-ment selection.This method uses instance selection criteria to se-lect new instances, but will look inside the currentdocument for a new instance within an uncertaintythreshold rather than selecting the most uncertain in-stance in the entire training set.
Sticking with thesame document for several instances in a row canpotentially solve the real world annotation problemthat marking up each instance requires some knowl-edge of the document context.
Instead, the contextlearned by selecting one instance can be retained ifuseful for annotating the next selected instance fromthe same document.This also preserves one of the biggest advantagesof instance selection, that of re-training the modelafter every selected instance.
In batch-mode selec-tion and document selection, many instances are se-lected according to criteria based on the same modelstarting point.
As a result, the selected instancesmay be redundant and document scores based onaccumulated instance scores may not reflect reality.Re-training the model between selected instancesprevents redundant instances from being selected.4 EvaluationEvaluations of the active learning models describedabove took place in a simulation context.
In activelearning simulations, a labeled data set is used, andthe unlabeled pool is simulated by ignoring or ?cov-ering?
the labels for part of the data until the selec-tion algorithm selects a new instance for annotation.After selection the next data point is simply put intothe training data and its label is uncovered.The data set used was the Ontology Developmentand Information Extraction (ODIE) corpus (Savovaet al, 2011) used in the 2011 i2b2/VA Challenge oncoreference resolution.3 We used a set of 64 docu-ments from the training set of the Mayo Clinic notesfor our simulations.Instances were created by using the trainingpipeline from the coreference system described inSection 2.1.
As previously mentioned, this workuses the named entity anaphor classifier as it con-tains the most data points.
This training set resultedin 6820 instances, with 311 positive instances and6509 negative instances.
Baseline ten-fold cross val-idation performance on this data set using an SVMwith RBF kernel is an F-score of 0.48.Simulations are performed using ten fold cross-validation.
First, each data point is assigned to one3https://www.i2b2.org/NLP/Coreference/76of ten folds (this is done randomly to avoid any auto-correlation issues).
Then, for each iteration, one foldis made the seed data, another fold is the validationdata, and the remainder are the unlabeled pool.
Ini-tially the labeled training data contains only the seeddata set.
The model is trained on the labeled train-ing data, tested on the validation set, then used toselect the next data point from the pool data set.
Theselected data point is then removed from the pooland added to the training data with its gold stan-dard label(s), and the process repeats until the poolof unlabeled data is empty.
Performance is averagedacross folds to minimize the effects of randomnessin seed and validation set selection.
Typically, activelearning is compared to a baseline of passive learn-ing where the next data point to be labeled is selectedfrom the unlabeled pool data set randomly.4.1 Instance Selection ExperimentsInstance selection simulations follow the generaltemplate above, with each instance (representinga putative antecedent-anaphor pair) randomly as-signed to a fold.
After scoring on the validation set,uncertainty sampling is used to select a single in-stance from the unlabeled pool, and that instance isadded to the training set.Figure 1 shows the results of active learning usinguncertainty selection on instances versus using pas-sive learning (random selection).
This makes it clearthat if the classifier is allowed to choose the data, topperformance can be achieved much faster than if thedata is presented in random order.
Specifically, theperformance for uncertainty selection levels off ataround 500 instances into the active learning, out ofa pool set of around 5500 instances.
In contrast, thepassive learning baseline takes basically the entiredataset to reach the same performance.This is essentially a proof of concept that there issuch a thing as a ?better?
or ?worse?
instance whenit comes to training a classifier for coreference.
Wetake this as a validation for attempting a documentselection experiment, with many metrics using in-stance uncertainty as a building block.4.2 Document Selection ExperimentsDocument selection follows similarly to the instanceselection above.
The main difference is that insteadof assigning pair vectors to folds, we assign docu-0 500 1000 1500 2000 2500 3000 3500 4000 4500 500000.10.20.30.40.50.60.7Active vs.
Passive Learning on Pairwise Named Entity CoreferenceNumber of instancesF?scoreRandom (Passive)Uncertainty SamplingFigure 1: Instance selection simulation results.
The x-axis is number of instances and the y-axis is ten-fold av-eraged f-score of the pairwise named entity classifier.ments to folds.
To make a selection, each instance islabeled according to the model, document level met-rics described in Section 3.2 are computed per docu-ment, and the document is selected which optimizesthe metric being evaluated.
All of that document?sinstances and labels are added to the training data,and the process repeats as before.The results of these experiments are divided intotwo plots for visual clarity.
Figure 2 shows theresults of these experiments, roughly divided intothose that work as well as a random baseline (left)and those that seem to work worse than a randombaseline (right).
The best performing metrics (onthe left side of the figure) are Positive Ratio, LeastWorst,Highest Average, and Narrow Band, althoughnone of these performs noticeably better than ran-dom.
The remaining metrics (on the right) seemto do worse than random, taking more instances toreach the peak performance near the end.The performance of document selection suggeststhat it may not be a viable means of active learn-ing.
This may be due to a model of data distributionin which useful instances are distributed very uni-formly throughout the corpus.
In this case, an aver-age document will only have 8?10 useful instancesand many times as many that are not useful.This was investigated by follow-up experimentson the instance selection which kept track of which770 1000 2000 3000 4000 5000 6000 700000.10.20.30.40.50.60.7Number of instancesF?scoreDocument?level active learningPassiveLeast worstHighest averagePos/neg ratioNarrow Band0 1000 2000 3000 4000 5000 6000 700000.10.20.30.40.50.60.7Number of instancesF?scoreDocument?level active learningPassiveBest exampleMost positivesSmallest spreadFigure 2: Two sets of document selection experiments.document each instance came from.
The experi-ments tracked the first 500 instances only, which isroughly the number of instances shown in Figure 1to reach peak performance.
Figure 3 (left) showsa histogram with document indices on the x-axisand normalized instance counts on the y-axis.
Thecounts are normalized by total number of documentvectors.
In other words, we wanted to show whetherthere was a distinction between ?good?
documentscontaining lots of good instances and ?bad?
docu-ments with few good instances.The figure shows a few spikes, but most docu-ments have approximately 10% of their instancessampled, and all but one document has at least oneinstance selected.
Further investigation shows thatthe spikes in the figure are from shorter documents.Since shorter documents have few instances overallbut always at least one positive instance, they will bebiased to have a higher ratio of positive to negativeinstances.
If positive instances are more uncertain(which may be the case due to the class imbalance),then shorter documents will have more selected in-stances per unit length.We performed another follow-up experimentalong these lines using the histogram as a measureof document value.
In this experiment, we took thenormalized histogram, selected documents from it inorder of normalized number of items selected, andused that as a document selection technique.
Ob-viously this would be ?cheating?
if used as a metricfor document selection, but it can serve as a check onthe viability of document selection.
If the results arebetter than passive document selection, then there issome hope that a document level metric based on theuncertainty of its instances can be successful.In fact, the right plot on Figure 3 shows that the?cheating?
method of document selection still doesnot look any better than random document selection.4.3 Document-Inertial Instance SelectionExperimentsThe experiments for document-inertial instance se-lection were patterned after the instance selectionparadigm.
However, each instance was bundled withmetadata representing the document from which itcame.
In the first selection, the algorithm selects themost uncertain instance, and the document it comesfrom is recorded.
For subsequent selections, thedocument which contained the previously selectedinstance is given priority when looking for a newinstance.
Specifically, each instance in that docu-ment is classified, and the confidence is comparedagainst a threshold.
If the document contains in-stances meeting the threshold, the most uncertain in-stance was selected.
After each instance, the modelis retrained as in normal instance selection, and thenew model is used in the next iteration of the selec-tion algorithm.
For these experiments, the thresholdis set at 0.75, where the distance between the classi-fication boundary and the margin is 1.0.Figure 4 shows the performance of this algorithmcompared to passive and uncertainty sampling.
Per-780 10 20 30 40 50 6000.10.20.30.40.50.60.70.80.91Normalized document selection countsDocument index%of vectorsselected0 1000 2000 3000 4000 5000 6000 700000.10.20.30.40.50.60.7Number of instancesF?scoreDocument?level active learningPassiveCheatingFigure 3: Left: Percentage of instances selected from each document.
Right: Performance of a document selectionalgorithm that can ?cheat?
and select the document with the highest proportion of good instances.0 500 1000 1500 2000 2500 3000 3500 4000 4500 500000.10.20.30.40.50.60.7Active vs.
Passive Learning on Pairwise Named Entity CoreferenceNumber of instancesF?scoreRandom (Passive)Uncertainty SamplingSticky Instance SamplingFigure 4: Document-inertial instance selection results.formance using this algorithm is clearly better thanpassive learning and is similar to standard uncer-tainty selection ignoring document constraints.5 Discussion and ConclusionThe results of these experiments paint a complexpicture of the way active learning works for this do-main and model combination.
The first experimentswith uncertainty selection indicate that the numberof instances required to achieve classifier perfor-mance can be compressed.
Selecting and trainingon all the good instances first leads to much fasterconvergence to the asymptotic performance of theclassifier given the features and data set.Attempting to extend this result to document se-lection met with mediocre results.
Even the best per-forming of seven attempted algorithms seems to beabout the same as random document selection.
Onecan interpret these results in different ways.The most pessimistic interpretation is that docu-ment selection simply requires too many useless in-stances to be annotated, good instances are spreadtoo evenly, and so document selection will never bemeaningfully faster than random selection.
This in-terpretation seems to be supported by experimentsshowing that even if document selection uses a?cheating?
algorithm to select the documents withthe highest proportion of good instances it still doesnot beat a passive baseline.One can also interpret these results to inspire fur-ther work, first by noting that all of the selectiontechniques attempt to build on the instance selec-tion metrics.
While our document selection metricswere more sophisticated than simply taking the n-best instances, Settles (2010) notes that some suc-cessful batch mode techniques explicitly account fordiversity in the selections, which we do not.
In ad-dition, one could argue that our experiments wereunduly constrained by the small number of docu-ments available in the unlabeled pool, and that witha larger unlabeled pool, one would eventually en-counter documents with many good instances.
Thismay be true, but may be difficult in practice as clin-ical notes often need to be manually de-identified79before any research use, and so it is not simply amatter of querying all records in an entire electronicmedical record system.The document-inertial instance selection showedthat the increase in training speed can be main-tained without switching documents for every in-stance.
This suggests that while good training in-stances may be uniformly distributed, it is usuallypossible to find multiple good enough instances inthe current document, and they can be found despitenot selecting instances in the exact best order thatplain instance selection would suggest.Future work is mainly concerned with real worldapplicability.
Document level active learning canprobably be ruled out as being non-beneficial despitebeing the easiest to work into annotation work flows.Instance level selection is very efficient in achievingclassifier performance but the least practical.Document-inertial seems to provide some com-promise.
It does not completely solve the prob-lems of instance selection, however, as annotationwill still not be complete if done exactly as simu-lated here.
In addition, the assumption of savingsis based on a model that each instance takes a con-stant amount of time to annotate.
This assumption isprobably true for tasks like word sense disambigua-tion, where an annotator can be presented one in-stance at a time with little context.
However, a bettermodel of annotation for tasks like coreference is thatthere is a constant amount of time required for read-ing and understanding the context of a document,then a constant amount of time on top of that perinstance.While modeling annotation time may pro-vide some insight, it will probably be most effectiveto undertake empirical annotation experiments to in-vestigate whether document-inertial instance selec-tion actually provides a valuable time savings.The final discussion point is that of producingcomplete document annotations.
For coreferencesystems following the pairwise discriminative ap-proach as in that described in Section 2.1, a corpusannotated instance by instance is useful.
However,many recent approaches do some form of document-level clustering or explicit coreference chain build-ing, and are not natively able to handle incompletelyannotated documents.44Other recent unsupervised graphical model approaches us-Future work will investigate this issue by quan-tifying the value of complete gold standard annota-tions versus the partial annotations that may be pro-duced using document-inertial instance selection.One way of doing this is in simulation, by traininga model on the 500 good instances that document-inertial instance selection selects, and then classify-ing the rest of the training instances using that modelto create a ?diluted?
gold standard.
Then, a modeltrained on the diluted gold standard will be usedto classify the validation set and performance com-pared to the version trained on the full gold standardcorpus.
Similar experiments can be performed usingother systems.
The logic here is that if an instancewas not in the top 10% of difficult instances it can beclassified with high certainty.
The fact that positiveinstances are rare and tend to be most uncertain is apoint in favor of this approach ?
after all, high accu-racy can be obtained by guessing in favor of negativeonce the positive instances are labeled.
On the otherhand, if document-inertial instance selection simplyamounts to labeling of positive instances, it may notresult in substantial time savings.In conclusion, this work has shown that instanceselection works for coreference resolution, intro-duced several metrics for document selection, andproposed a hybrid selection approach that preservesthe benefits of instance selection while offering thepotential of being applicable to real annotation.
Thiswork can benefit the natural language processingcommunity by providing practical methods for in-creasing the speed of coreference annotation.AcknowledgmentsThe project described was supported by awardnumber NLM RC1LM010608, the Strategic HealthIT Advanced Research Projects (SHARP) Program(90TR002) administered by the Office of the Na-tional Coordinator for Health Information Technol-ogy, and Integrating Informatics and Biology to theBedside (i2b2) NCBO U54LM008748.
The contentis solely the responsibility of the authors and doesnot necessarily represent the official views of theNLM/NIH/ONC.ing Gibbs sampling (Haghighi and Klein, 2007) may be able toincorporate partially annotated documents in semi-supervisedtraining.80ReferencesCaroline Gasperin.
2009.
Active learning for anaphoraresolution.
In Proceedings of the NAACL HLT Work-shop on Active Learning for Natural Language Pro-cessing, pages 1?8.Aria Haghighi and Dan Klein.
2007.
Unsupervisedcoreference resolution in a nonparametric bayesianmodel.
In Proceedings of the 45th Annual Meetingof the Association of Computational Linguistics, pages848?855.David D. Lewis andWilliam A. Gale.
1994.
A sequentialalgorithm for training text classifiers.
In Proceedingsof the ACM SIGIR Conference on Research and Devel-opment in Information Retrieval, pages 3?12.Vincent Ng and Claire Cardie.
2002.
Improving machinelearning approaches to coreference resolution.
In Pro-ceedings of the 40th Annual Meeting of the Associationfor Computational Linguistics (ACL).Vincent Ng.
2010.
Supervised noun phrase coreferenceresearch: The first fifteen years.
In Proceedings of the48th Annual Meeting of the Association for Computa-tional Linguistics (ACL-10).Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,Martha Palmer, Ralph Weischedel, and Nianwen Xue.2011.
CoNLL-2011 shared task: Modeling unre-stricted coreference in OntoNotes.
In Proceedings ofthe 15th Conference on Computational Natural Lan-guage Learning: Shared Task, pages 1?27.Guergana K. Savova, James J. Masanz, Philip V. Ogren,Jiaping Zheng, Sunghwan Sohn, Karin C. Kipper-Schuler, and Christopher G. Chute.
2010.
Mayoclinical text analysis and knowledge extraction sys-tem (cTAKES): architecture, component evaluationand applications.
J Am Med Inform Assoc, 17(5):507?513.Guergana K. Savova, Wendy W. Chapman, JiapingZheng, and Rebecca S. Crowley.
2011.
Anaphoricrelations in the clinical narrative: corpus creation.
JAm Med Inform Assoc, 18:459?465.A.I.
Schein and L.H.
Ungar.
2007.
Active learning forlogistic regression: an evaluation.
Machine Learning,68(3):235?265.B.
Settles, M. Craven, and S. Ray.
2008.
Multiple-instance active learning.
Advances in Neural Informa-tion Processing Systems (NIPS), 20:1289?1296.Burr Settles.
2010.
Active learning literature survey.Technical report, University of Wisconsin?Madison.Katrin Tomanek, Florian Laws, Udo Hahn, and HinrichSchu?tze.
2009.
On proper unit selection in activelearning: co-selection effects for named entity recog-nition.
In HLT ?09: Proceedings of the NAACL HLT2009 Workshop on Active Learning for Natural Lan-guage Processing, pages 9?17, Morristown, NJ, USA.Association for Computational Linguistics.S.
Tong and D. Koller.
2002.
Support vector machineactive learning with applications to text classification.The Journal of Machine Learning Research, 2:45?66.Jiaping Zheng, Wendy Webber Chapman, Rebecca S.Crowley, and Guergana K. Savova.
2011.
Coreferenceresolution: A review of general methodologies and ap-plications in the clinical domain.
Journal of Biomedi-cal Informatics, 44:1113?1122.Jiaping Zheng, Wendy W Chapman, Timothy A Miller,Chen Lin, Rebecca S Crowley, and Guergana KSavova.
2012.
A system for coreference resolution forthe clinical narrative.
Journal of the American Medi-cal Informatics Association.81
