Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 19?27,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsKeepin?
It Real: Semi-Supervised Learning with Realistic TuningAndrew B. GoldbergComputer Sciences DepartmentUniversity of Wisconsin-MadisonMadison, WI 53706, USAgoldberg@cs.wisc.eduXiaojin ZhuComputer Sciences DepartmentUniversity of Wisconsin-MadisonMadison, WI 53706, USAjerryzhu@cs.wisc.eduAbstractWe address two critical issues involved in ap-plying semi-supervised learning (SSL) to areal-world task: parameter tuning and choos-ing which (if any) SSL algorithm is best suitedfor the task at hand.
To gain a better un-derstanding of these issues, we carry out amedium-scale empirical study comparing su-pervised learning (SL) to two popular SSL al-gorithms on eight natural language processingtasks under three performance metrics.
Wesimulate how a practitioner would go abouttackling a new problem, including parametertuning using cross validation (CV).
We showthat, under such realistic conditions, each ofthe SSL algorithms can be worse than SL onsome datasets.
However, we also show thatCV can select SL/SSL to achieve ?agnosticSSL,?
whose performance is almost always noworse than SL.
While CV is often dismissed asunreliable for SSL due to the small amount oflabeled data, we show that it is in fact effectivefor accuracy even when the labeled datasetsize is as small as 10.1 IntroductionImagine you are a real-world practitioner workingon a machine learning problem in natural languageprocessing.
If you have unlabeled data, should youuse semi-supervised learning (SSL)?
Which SSL al-gorithm should you use?
How should you set its pa-rameters?
Or could it actually hurt performance, inwhich case you might be better off with supervisedlearning (SL)?A large number of SSL algorithms have been de-veloped in recent years that allow one to improveperformance with unlabeled data, in tasks suchas text classification, sequence labeling, and pars-ing (Zhu, 2005; Chapelle et al, 2006; Brefeld andScheffer, 2006).
However, many of them are testedon ?SSL-friendly?
datasets, such as ?two moons,?USPS, and MNIST.
Furthermore, the algorithms?parameters are often chosen based on test set per-formance or manually set based on heuristics andresearcher experience.
These issues create practicalconcerns for deploying SSL in the real world.We note that (Chapelle et al, 2006)?s benchmarkchapter explores these issues to some extent by com-paring several SSL methods on several real and ar-tificial datasets.
The authors reach the conclusionsthat parameter tuning is difficult with little labeleddata and that no method is universally superior.
Wereexamine these issues in the context of NLP tasksand offer a simple attempt at overcoming these road-blocks to practical application of SSL.The contributions of this paper include:?
We present a medium-scale empirical studycomparing SL to two popular SSL algorithmson eight less-familiar tasks using three per-formance metrics.
Importantly, we tune pa-rameters realistically based on cross validation(CV), as a practitioner would do in reality.?
We show that, under such realistic conditions,each of the SSL algorithms can be worse thanSL on some datasets.?
However, this can be prevented.
We show thatCV can be used to select SL/SSL to achieveagnostic SSL, whose performance is almost al-ways no worse than SL.
Traditionally, CV is19often dismissed as unreliable for SSL becauseof the small labeled dataset size.
But we showthat CV is effective when using accuracy as anoptimization criterion, even when the labeleddataset size is as small as 10.?
We show the power of cloud computing: wewere able to complete roughly 3 months worthof experiments in less than a week.2 SSL with Realistic TuningGiven a particular labeled and unlabeled dataset,how should you set parameters for a particular SSLmodel?
The most realistic approach for a practi-tioner is to use CV to tune parameters on a grid.
Wetherefore argue that the model parameters obtainedin this way truly determine how SSL will performin practice.
Algorithm 1 describes a particular in-stance1 of CV in detail.
We call it ?RealSSL,?
asthis is all a real user can hope to do when applyingSSL (and SL too) in a realistic problem scenario.3 Experimental ProcedureGiven the RealSSL procedure in Algorithm 1, wedesigned an experimental setup to simulate differ-ent settings that a real-world practitioner might facewhen given a new task and a set of algorithms tochoose from (some of which use unlabeled data).This will allow us to compare algorithms acrossdatasets in a variety of situations.
Algorithm 2measures the performance of one algorithm on onedataset for several different l and u combinations.Specifically, we consider l ?
{10, 100} and u ?
{100, 1000}.
For each combination, we performmultiple trials (T = 10 here) using different randomassignments of data to Dlabeled and Dunlabeled, toobtain confidence intervals around our performancemeasurements.
All random selections of subsets ofdata are the same across different algorithms?
runs,to permit paired t-tests for evaluation.
Note that,when l 6= max(L) or u 6= max(U), a portion ofDpool is not used for training.
Also, the RealSSLprocedure ensures that all parameters are tuned bycross-validation without ever seeing the held-out1The particular choice of 5-fold CV, the way to split labeledand unlabeled data, and the parameter grid, is important too.But we view them as secondary to the fact that we are tuningSSL by CV.test set Dtest.
Lastly, we stress that the same gridof algorithm-specific parameter values (discussed inSection 5) is considered for all datasets.4 DatasetsTable 1 summarizes the datasets used for the com-parisons.
In this study we consider only binary clas-sification tasks.
Note that d is the number of dimen-sions, P (y = 1) is the proportion of instances inthe full dataset belonging to class y = 1, and |Dtest|refers to the size of the test set (the instances remain-ing after max(L) + max(U) = 1100 have been setaside for training trials).
[MacWin] is the Mac versus Windows text clas-sification data from the 20-newsgroups dataset, pre-processed by the authors of (Sindhwani et al, 2005).
[Interest] is a binary version of the word sensedisambiguation data from (Bruce and Wiebe, 1994).The task is to distinguish the sense of ?interest?meaning ?money paid for the use of money?
fromthe other five senses (e.g., ?readiness to give atten-tion,?
?a share in a company or business?).
Thedata comes from a corpus of part-of-speech (POS)tagged sentences containing the word ?interest.
?Each instance is a bag-of-word/POS vector, exclud-ing words containing the root ?interest?
and thosethat appeared in less than three sentences overall.Datasets [aut-avn] and [real-sim] are theauto/aviation and real/simulated text classificationdatasets from the SRAA corpus of UseNet articles.The [ccat] and [gcat] datasets involve identifyingcorporate and government articles, respectively, inthe RCV1 corpus.
We use the versions of thesedatasets prepared by the authors of (Sindhwani etal., 2006).Finally, the two WISH datasets come from (Gold-berg et al, 2009) and involve discriminating be-tween sentences that contain wishful expressionsand those that do not.
The instances in [WISH-politics] correspond to sentences taken from a po-litical discussion board, while [WISH-products] isbased on sentences from Amazon product reviews.The features are a combination of word and templatefeatures as described in (Goldberg et al, 2009).20Input: dataset Dlabeled = {xi, yi}li=1, Dunlabeled = {xj}uj=1, algorithm, performance metricRandomly partition Dlabeled into 5 equally-sized disjoint subsets {Dl1, Dl2, Dl3, Dl4, Dl5}.Randomly partition Dunlabeled into 5 equally-sized disjoint subsets {Du1, Du2, Du3, Du4, Du5}.Combine partitions: Let Dfold k = Dlk ?
Duk for all k = 1, .
.
.
, 5.foreach parameter configuration in grid doforeach fold k doTrain model using algorithm on ?i6=kDfold i.Evaluate metric on Dfold k.endCompute the average metric value across the 5 folds.endChoose parameter configuration that optimizes average metric.Train model using algorithm and the chosen parameters on Dlabeled and Dunlabeled.Output: Optimal model; Average metric value achieved by optimal parameters during tuning.Algorithm 1: RealSSL procedure for running an SSL (or SL, simply ignore the unlabeled data) algorithm on aspecific labeled and unlabeled dataset using cross-validation to tune parameters.Input: dataset D = {xi, yi}ni=1, algorithm, performance metric, set L, set U , trials TRandomly divide D into Dpool (of size max(L) + max(U)) and Dtest (the rest).foreach l in L doforeach u in U doforeach trial 1 up to T doRandomly select Dlabeled = {xj , yj}lj=l and Dunlabeled = {xk}uk=1 from Dpool.Run RealSSL(Dlabeled, Dunlabeled, algorithm, metric) to obtain model and tuningperformance value (see Algorithm 1).Use model to classify Dunlabeled and record transductive metric value.Use model to classify Dtest and record test metric value.endendendOutput: Tuning, transductive, and test performance for T runs of algorithm using all l and ucombinations.Algorithm 2: Experimental procedure used for all comparisons.21Name d P (y = 1) |Dtest|[MacWin] 7511 0.51 846[Interest] 2687 0.53 1268[aut-avn] 20707 0.65 70075[real-sim] 20958 0.31 71209[ccat] 47236 0.47 22019[gcat] 47236 0.30 22019[WISH-politics] 13610 0.34 4999[WISH-products] 4823 0.12 129Table 1: Datasets used in benchmark comparison.
Seetext for details.5 AlgorithmsWe consider only linear classifiers for this study,since they tend to work well for text problems.
Infuture work, we plan to explore a range of kernelsand other non-linear classifiers.As a baseline supervised learning method, we usea support vector machine (SVM), as implementedby SVMlight (Joachims, 1999).
This baseline simplyignores all the unlabeled data (xl+1, .
.
.
,xn).
Recallthis solves the following regularized risk minimiza-tion problemminf12 ||f ||22 + Cl?i=1max(0, 1 ?
yif(xi)), (1)where f(x) = w>x + b, and C is a parame-ter controlling the trade-off between training er-rors and model complexity.
Using the procedureoutlined above, we tune C over a grid of values{10?6, 10?5, 10?4, 10?3, 10?2, 10?1, 1, 10, 100}.We consider two popular SSL algorithms, whichmake different assumptions about the link betweenthe marginal data distribution Px and the conditionallabel distribution Py|x.
If the assumption does nothold in a particular dataset, the SSL algorithm coulduse the unlabeled data ?incorrectly,?
and performworse than SL.The first SSL algorithm we use is a semi-supervised support vector machine (S3VM), whichmakes the cluster assumption: the classes are well-separated clusters of data, such that the decisionboundary falls into a low density region in the fea-ture space.
While many implementations exist,we chose the deterministic annealing (DA) algo-rithm implemented in the SVMlin package (Sind-hwani et al, 2006; Sindhwani and Keerthi, 2007).This DA algorithm often achieved the best accu-racy across several datasets in the empirical com-parison in (Sindhwani and Keerthi, 2007), despitebeing slower than the multi-switch algorithm pre-sented in the same paper.
Note that the transductiveSVM implemented in SVMlight would have beenprohibitively slow to carry out the range of exper-iments conducted here.
Recall that an S3VM seeksan optimal classifier f?
that cuts through a region oflow density between clusters of data.
One way toview this is that it tries to find the best possible la-beling of the unlabeled data such the classifier maxi-mizes the margin on both labeled and unlabeled datapoints.
This is achieved by solving the followingnon-convex minimization problemminf,y??
{?1,1}u?2 ||f ||22+ 1ll?i=1V (yif(xi) + ?
?un?j=l+1V (y?jf(xj)),subject to a class-balance constraint.
Note thatV is a loss function (typically the hinge lossas in (1)), and the parameters ?, ??
control therelative importance of model complexity versuslocating a low-density region within the unlabeleddata.
We tune both parameters in a grid of values{10?6, 10?5, 10?4, 10?3, 10?2, 10?1, 1, 10, 100}.In past studies (Sindhwani et al, 2006), ?
was set to1, and ??
was tuned over a grid containing a subsetof these values.Finally, as an example of a graph-basedSSL method, we consider manifold regularization(MR) (Belkin et al, 2006), using the implementa-tion provided on the authors?
Web site.2 This algo-rithm makes the manifold assumption: the labels are?smooth?
with respect to a graph connecting labeledand unlabeled instances.
In other words, if two in-stances are connected by a strong edge (e.g., theyare highly similar to one another), their labels tendto be the same.
Manifold regularization represents afamily of methods; we specifically use the LaplacianSVM, which extends the basic SVM optimization2http://manifold.cs.uchicago.edu/manifold regularization/software.html22problem with a graph-based regularization term.minf?A||f ||22 + 1ll?i=1max(0, 1 ?
yif(xi))+ ?In?i=1n?j=1wij(f(xi) ?
f(xj))2,where ?A and ?I are parameters that trade off am-bient and intrinsic smoothness, and wij is a graphweight between instances xi and xj .
In this pa-per, we consider kNN graphs with k ?
{3, 10, 20}.Edge weights are formed using a heat kernel wij =exp(?
||xi?xj ||22?2 ), where ?
is set to be the mean dis-tance between nearest neighbors in the graph, asin (Chapelle et al, 2006).
The ?
parameters are eachtuned over the grid {10?6, 10?4, 10?2, 1, 100}.Of course, many other SSL algorithms exist, someof which combine different assumptions (Chapelleand Zien, 2005; Karlen et al, 2008), and otherswhich exploit multiple (real or artificial) views ofthe data (Blum and Mitchell, 1998; Sindhwani andRosenberg, 2008).
We plan to extend our study toinclude many more diverse SSL algorithms in thefuture.6 Choosing an Algorithm for a Real-WorldTaskGiven the choice of several algorithms, how shouldone choose the best one to apply to a particular learn-ing setting?
Traditionally, CV is used for modelselection in supervised learning settings.
However,with only a small amount of labeled data in semi-supervised settings, model selection with CV is of-ten viewed as unreliable.
We explicitly tested thishypothesis by using CV to not only choose the pa-rameters of the model, but also choose the type ofmodel itself.
The main goal is to automaticallychoose between SVM, S3VM, and MR for a par-ticular learning setting, in an attempt to ensure thatthe final performance is never hurt by including un-labeled data (which might be called agnostic SSL).Given a set of algorithms (e.g., one SL, severalSSL), the procedure is the following:1.
Tune the parameters of each algorithm on thelabeled and unlabeled training set using Algo-rithm 1.32.
Compare the best tuning performance (5-foldCV average) achieved by the optimal parame-ters for each of the algorithms.?
If there are no ties, select the algorithmwith the highest tuning performance.?
If there is a tie, and SL is among the best,select SL.?
If there is a tie between SSL algorithms,select one of them at random.3.
Use the selected ?Best Tuning?
algorithm (andthe tuned parameters) to build a model on allthe training data; then apply it to the test data.Note that the procedure is conservative in that itprefers SL in the case of ties.
In this paper, we usethis simple ?best tuning performance?
heuristic.Finally, we stress the fact that, when applyingthis procedure within the context of Algorithm 2, apotentially different algorithm is chosen in each ofthe 10 trials for a particular setting.
This simulatesthe real-world scenario where one only has a singlefixed training set of labeled and unlabeled data andmust choose a single algorithm to produce a modelfor future predictions.7 Performance MetricsWe compare different algorithms?
performance us-ing three metrics often used for evaluation in NLPtasks: accuracy, maxF1, and AUROC.
Accuracyis simply the fraction of instances correctly classi-fied.
MaxF1 is the maximal F1 value (harmonicmean of recall and precision) achieved over the en-tire precision-recall curve (Cai and Hofmann, 2003).AUROC is the area under the ROC curve (Fawcett,2004).
Throughout the paper, when we discuss aresult involving a particular metric, the algorithmsuse this metric as the criterion for parameter tuning,and we use it for the final evaluation.
We are notsimply evaluating a single experiment using multi-ple metrics?the experiments are fundamentally dif-ferent and produce different learned models.3We ensure each algorithm uses the same 5 partitions duringthe tuning step.238 ResultsWe now report the results of our empirical compar-ison of SL and SSL on the eight NLP datasets.
Wefirst consider each dataset separately and examinehow often each type of algorithm outperforms theother.
We then examine cross-dataset performance.8.1 Detailed ResultsTable 2 contains all results for SVM, S3VM, andMR for all datasets and all metrics.4 Note that withineach l,u cell for a particular dataset and evaluationmetric, we show the maximum value in each row(tune, transductive, or test) in boldface.
Results thatare not statistically significantly different using apaired t-test are also shown in boldface.Several things are immediately obvious from Ta-ble 2.
First, no algorithm is superior in all datasetsor settings.
In several cases, all algorithms are statis-tically indistinguishable.
Most importantly, though,each of the SSL algorithms can be worse than SL onsome datasets using some metric.
We used pairedt-tests to compare transductive and test performanceof each SSL algorithm with SVM for a particular l,ucombination and dataset (32 settings total per evalu-ation metric).
In terms of accuracy, MR transductiveperformance is significantly worse than SVM in 5settings, while MR test performance is significantlyworse in 7 settings.
MR is also significantly worse in4 settings based on transductive maxF1, in 3 settingsbased on transductive AUROC, and 1 setting basedon test AUROC.
S3VM is significantly worse thanSVM in 2 settings based on transductive maxF1, 2settings based on transductive AUROC, and in 1 set-ting based on test AUROC.
While these numbersmay seem relatively low, it is important to realizethat each algorithm may be worse than SSL manytimes on a trial-by-trial basis, which is the more real-istic scenario: a practitioner has only a single datasetto work with.
Results based on individual trials arediscussed below shortly.4Note that the results here for a particular dataset and algo-rithm combination may be qualitatively and quantitatively dif-ferent than in previous published work, due to differences inparameter tuning, choices of parameter grids, l and u sizes, andrandomization.
We are not trying to replicate or raise doubtabout past results: we simply intend to compare algorithms ona wide array of datasets using the standardized procedures out-lined above.We also applied our ?Best Tuning?
model selec-tion procedure to automatically choose a single al-gorithm for each trial in each setting.
We compareaverage SL test performance versus the average testperformance of the Best Tuning selections across the10 trials (not shown in Table 2).
Comparisons basedon transductive performance are similar.
When theperformance metric is test accuracy, the Best Tuningalgorithm performs statistically significantly betterthan SL in 24 settings and worse in only 6 settings.In the remaining 2 settings, Best Tuning chose SLin all 10 trials, so they are equivalent.
These resultssuggest that accuracy-based tuning is a valid methodfor choosing a SSL algorithm to improve accuracyon test data.
To some extent, this holds for maxF1,too: the Best Tuning selections perform better thanSL (on average) in 18 settings and worse in 14 set-tings when tuning and test evaluation is based onmaxF1.
However, when using AUROC as the per-formance metric, cross validation seems to be unre-liable: Best Tuning produces a better result in only11 out of the 32 settings.8.2 Results Aggregated Across DatasetsWe now aggregate the detailed results to better un-derstand the relative performance of the differentmethods across all datasets.
We perform this sum-mary evaluation in two ways, based on test setperformance (transductive performance is similar).First, we compare the SSL algorithms across alldatasets based on the numbers of times each is worsethan, the same as, or better than SL.
For each ofthe 80 trials of a particular l,u,metric combination,we compare the performance of S3VM, MR, andBest Tuning to SVM.
Note that each of these com-parisons is akin to a real-world scenario where apractitioner would have to choose an algorithm touse.
Table 3 lists tuples of the form ?
(#trials worsethan SVM, #trials equal to SVM, #trials better thanSVM).?
Note that the numbers in each tuple sum to80.
The perfect SSL algorithm would have a tuple of?
(0, 0, 80),?
meaning that it always outperforms SL.In terms of accuracy (Table 3, top) and maxF1 (Ta-ble 3, middle), the Best Tuning method turns out todo worse than SVM less often than either S3VM orMR does (i.e., the first number in the tuples for BestTuning is lower than the corresponding numbers forthe other algorithms).
At the same time, Best Tuning24accuracy maxF1 AUROCu = 100 u = 1000 u = 100 u = 1000 u = 100 u = 1000Dataset l SVM S3VM MR SVM S3VM MR SVM S3VM MR SVM S3VM MR SVM S3VM MR SVM S3VM MR[MacWin]100.60 0.72 0.83 0.60 0.72 0.86 0.66 0.67 0.67 0.66 0.67 0.67 0.63 0.69 0.67 0.63 0.69 0.69 Tune0.51 0.51 0.70 0.51 0.50 0.69 0.74 0.77 0.80 0.74 0.74 0.75 0.72 0.75 0.82 0.72 0.71 0.80 Trans0.53 0.50 0.71 0.53 0.50 0.68 0.74 0.75 0.79 0.74 0.75 0.74 0.73 0.72 0.83 0.73 0.71 0.76 Test1000.87 0.87 0.91 0.87 0.87 0.90 0.94 0.95 0.95 0.94 0.95 0.95 0.96 0.97 0.97 0.96 0.96 0.96 Tune0.89 0.89 0.89 0.89 0.89 0.89 0.91 0.93 0.92 0.91 0.90 0.90 0.97 0.97 0.96 0.97 0.97 0.96 Trans0.89 0.89 0.91 0.89 0.89 0.90 0.92 0.92 0.92 0.92 0.91 0.91 0.97 0.97 0.97 0.97 0.97 0.97 Test[Interest]100.68 0.75 0.78 0.68 0.75 0.79 0.73 0.77 0.77 0.73 0.78 0.77 0.52 0.66 0.66 0.52 0.68 0.64 Tune0.52 0.56 0.56 0.52 0.56 0.56 0.72 0.72 0.72 0.72 0.71 0.71 0.55 0.54 0.54 0.55 0.56 0.61 Trans0.52 0.57 0.57 0.52 0.57 0.58 0.68 0.69 0.69 0.68 0.69 0.69 0.58 0.56 0.61 0.58 0.58 0.62 Test1000.77 0.78 0.76 0.77 0.78 0.77 0.84 0.85 0.85 0.84 0.85 0.84 0.89 0.90 0.89 0.89 0.85 0.84 Tune0.79 0.79 0.71 0.79 0.79 0.77 0.84 0.83 0.82 0.84 0.81 0.81 0.91 0.91 0.89 0.91 0.79 0.87 Trans0.81 0.80 0.78 0.81 0.80 0.79 0.82 0.81 0.81 0.82 0.81 0.81 0.90 0.91 0.89 0.90 0.81 0.88 Test[aut-avn]100.72 0.76 0.82 0.72 0.76 0.79 0.89 0.92 0.91 0.89 0.92 0.91 0.58 0.67 0.65 0.58 0.67 0.65 Tune0.65 0.63 0.67 0.65 0.61 0.69 0.83 0.83 0.84 0.83 0.81 0.82 0.71 0.67 0.73 0.71 0.65 0.72 Trans0.62 0.61 0.67 0.62 0.61 0.67 0.80 0.81 0.82 0.80 0.81 0.81 0.71 0.70 0.73 0.71 0.65 0.69 Test1000.75 0.82 0.87 0.75 0.82 0.86 0.94 0.94 0.95 0.94 0.94 0.94 0.93 0.94 0.94 0.93 0.94 0.93 Tune0.77 0.79 0.88 0.77 0.83 0.87 0.92 0.92 0.91 0.92 0.91 0.90 0.93 0.93 0.91 0.93 0.94 0.93 Trans0.77 0.82 0.89 0.77 0.83 0.87 0.91 0.91 0.91 0.91 0.91 0.91 0.95 0.94 0.95 0.95 0.95 0.95 Test[real-sim]100.53 0.63 0.82 0.53 0.63 0.78 0.65 0.66 0.66 0.65 0.66 0.65 0.77 0.81 0.81 0.77 0.81 0.77 Tune0.64 0.63 0.72 0.64 0.64 0.70 0.57 0.66 0.70 0.57 0.62 0.56 0.65 0.75 0.79 0.65 0.74 0.67 Trans0.65 0.66 0.74 0.65 0.66 0.68 0.53 0.58 0.63 0.53 0.59 0.53 0.64 0.73 0.80 0.64 0.74 0.66 Test1000.74 0.73 0.86 0.74 0.73 0.84 0.88 0.90 0.90 0.88 0.91 0.89 0.93 0.94 0.94 0.93 0.94 0.93 Tune0.78 0.76 0.84 0.78 0.78 0.85 0.81 0.83 0.79 0.81 0.81 0.81 0.94 0.93 0.91 0.94 0.94 0.94 Trans0.79 0.78 0.85 0.79 0.78 0.85 0.78 0.79 0.78 0.78 0.79 0.79 0.93 0.93 0.93 0.93 0.94 0.93 Test[ccat]100.54 0.60 0.82 0.54 0.60 0.81 0.84 0.85 0.85 0.84 0.85 0.84 0.74 0.78 0.78 0.74 0.78 0.74 Tune0.50 0.49 0.65 0.50 0.51 0.67 0.69 0.69 0.73 0.69 0.67 0.69 0.60 0.61 0.71 0.60 0.59 0.72 Trans0.49 0.52 0.64 0.49 0.52 0.66 0.66 0.66 0.69 0.66 0.67 0.67 0.61 0.63 0.72 0.61 0.59 0.71 Test1000.80 0.80 0.84 0.80 0.80 0.84 0.89 0.89 0.90 0.89 0.89 0.89 0.91 0.92 0.92 0.91 0.92 0.91 Tune0.80 0.79 0.80 0.80 0.81 0.83 0.83 0.85 0.84 0.83 0.82 0.82 0.91 0.91 0.89 0.91 0.90 0.91 Trans0.81 0.80 0.81 0.81 0.80 0.82 0.80 0.81 0.81 0.80 0.81 0.81 0.90 0.90 0.90 0.90 0.90 0.90 Test[gcat]100.74 0.83 0.82 0.74 0.79 0.81 0.44 0.47 0.46 0.44 0.47 0.46 0.69 0.79 0.75 0.69 0.79 0.75 Tune0.69 0.68 0.75 0.69 0.72 0.76 0.60 0.62 0.69 0.60 0.59 0.62 0.71 0.73 0.82 0.71 0.69 0.76 Trans0.66 0.67 0.73 0.66 0.71 0.74 0.58 0.61 0.66 0.58 0.60 0.59 0.69 0.69 0.81 0.69 0.69 0.75 Test1000.77 0.77 0.90 0.77 0.77 0.91 0.92 0.92 0.93 0.92 0.92 0.92 0.97 0.96 0.97 0.97 0.96 0.96 Tune0.81 0.80 0.89 0.81 0.81 0.90 0.88 0.88 0.84 0.88 0.86 0.85 0.96 0.97 0.95 0.96 0.96 0.96 Trans0.80 0.80 0.89 0.80 0.80 0.90 0.86 0.86 0.85 0.86 0.86 0.86 0.96 0.96 0.96 0.96 0.96 0.96 Test[WISH-politics]100.70 0.77 0.79 0.70 0.77 0.82 0.61 0.62 0.61 0.61 0.62 0.61 0.74 0.78 0.74 0.74 0.78 0.76 Tune0.50 0.56 0.63 0.50 0.62 0.56 0.58 0.58 0.61 0.58 0.55 0.53 0.62 0.62 0.69 0.62 0.62 0.61 Trans0.52 0.56 0.60 0.52 0.62 0.53 0.52 0.53 0.53 0.52 0.54 0.52 0.57 0.58 0.61 0.57 0.62 0.60 Test1000.75 0.75 0.75 0.75 0.75 0.74 0.74 0.75 0.76 0.74 0.75 0.75 0.79 0.80 0.80 0.79 0.80 0.80 Tune0.73 0.73 0.71 0.73 0.73 0.70 0.65 0.66 0.67 0.65 0.64 0.64 0.76 0.74 0.75 0.76 0.75 0.76 Trans0.75 0.75 0.72 0.75 0.75 0.71 0.64 0.63 0.63 0.64 0.63 0.64 0.78 0.76 0.77 0.78 0.76 0.77 Test[WISH-products]100.89 0.89 0.67 0.89 0.89 0.67 0.19 0.22 0.16 0.19 0.22 0.16 0.76 0.80 0.74 0.76 0.80 0.74 Tune0.87 0.87 0.66 0.87 0.87 0.61 0.31 0.29 0.32 0.31 0.24 0.25 0.56 0.52 0.58 0.56 0.54 0.56 Trans0.90 0.90 0.67 0.90 0.90 0.61 0.22 0.23 0.30 0.22 0.24 0.27 0.50 0.53 0.62 0.50 0.54 0.59 Test1000.90 0.90 0.82 0.90 0.90 0.81 0.49 0.50 0.54 0.49 0.52 0.52 0.73 0.73 0.77 0.73 0.78 0.75 Tune0.88 0.88 0.81 0.88 0.88 0.80 0.34 0.28 0.37 0.34 0.27 0.30 0.60 0.55 0.57 0.60 0.57 0.61 Trans0.90 0.90 0.79 0.90 0.91 0.76 0.33 0.28 0.33 0.33 0.32 0.38 0.59 0.56 0.60 0.59 0.56 0.60 TestTable 2: Benchmark comparison results.
All numbers are averages over 10 trials.
Within each cell of nine numbers,the boldface indicates the maximum value in each row, as well as others in the row that are not statistically significantlydifferent based on a paired t-test.u = 100 u = 1000Metric l S3VM MR Best Tuning S3VM MR Best Tuningaccuracy 10 (14, 27, 39) (27, 0, 53) (8, 31, 41) (14, 25, 41) (27, 0, 53) (8, 29, 43) Test100 (27, 7, 46) (38, 0, 42) (20, 16, 44) (27, 6, 47) (37, 0, 43) (16, 19, 45) TestMetric l S3VM MR Best Tuning S3VM MR Best TuningmaxF1 10 (29, 2, 49) (16, 1, 63) (14, 55, 11) (27, 0, 53) (24, 0, 56) (13, 53, 14) Test100 (39, 0, 41) (34, 4, 42) (31, 15, 34) (39, 1, 40) (44, 4, 32) (26, 21, 33) TestMetric l S3VM MR Best Tuning S3VM MR Best TuningAUROC 10 (26, 0, 54) (11, 0, 69) (12, 57, 11) (25, 0, 55) (25, 0, 55) (11, 56, 13) Test100 (43, 0, 37) (37, 0, 43) (38, 8, 34) (38, 0, 42) (46, 0, 34) (28, 24, 28) TestTable 3: Aggregate test performance comparisons versus SVM in 80 trials per setting.
Each cell contains a tuple ofthe form ?
(#trials worse than SVM, #trials equal to SVM, #trials better than SVM).
?25u = 100 u = 1000Metric l SVM S3VM MR Best Tuning SVM S3VM MR Best Tuningaccuracy 10 0.61 0.62 0.67 0.68 0.61 0.63 0.64 0.67 Test100 0.81 0.82 0.83 0.85 0.81 0.82 0.83 0.85 TestMetric l SVM S3VM MR Best Tuning SVM S3VM MR Best TuningmaxF1 10 0.59 0.61 0.64 0.59 0.59 0.61 0.61 0.59 Test100 0.76 0.75 0.76 0.75 0.76 0.76 0.76 0.76 TestMetric l SVM S3VM MR Best Tuning SVM S3VM MR Best TuningAUROC 10 0.63 0.64 0.72 0.61 0.63 0.64 0.67 0.61 Test100 0.87 0.87 0.87 0.87 0.87 0.86 0.87 0.86 TestTable 4: Aggregate test results averaged over the 80 trials (8 datasets, 10 trials each) in a particular setting.outperforms SVM in fewer trials than the other algo-rithms in some settings for these two metrics.
Thisis because Best Tuning conservatively selects SVMin many trials.
The take home message is that tuningusing CV based on accuracy (and to a lesser extentmaxF1) appears to mitigate some risk involved inapplying SSL.
AUROC, on the other hand, does notappear as effective for this purpose.
Table 3 (bottom)shows that, for u = 1000, Best Tuning is worse thanSVM fewer times, but for u = 100, MR achievesbetter performance overall.We also compare overall average test performance(across datasets) for each metric and l,u combina-tion.
Table 4 reports these results for accuracy,maxF1, and AUROC.
In terms of accuracy, we seethat the Best Tuning approach leads to better per-formance than SVM, S3VM, or MR in all settingswhen averaged over datasets.
We appear to achievesome synergy in dynamically choosing a differentalgorithm in each trial.
In terms of maxF1, BestTuning, S3VM, and MR are all at least as good asSL in three of the four l,u settings, and nearly asgood in the fourth.
Based on AUROC, though, theresults are mixed depending on the specific setting.Notably, though, Best Tuning consistently leads toworse performance than SL when using this metric.8.3 A Note on Cloud ComputingThe experiments were carried out using the CondorHigh-Throughput Computing platform (Thain et al,2005).
We ran many trials per algorithm (using dif-ferent datasets, l, u, and metrics).
Each trial in-volved training hundreds of models using differentparameter configurations repeated across five folds,and then training once more using the selected pa-rameters.
In the end, we trained a grand total of794,880 individual models to produce the results inTable 2.
Through distributed computing on approxi-mately 50 machines in parallel, we were able to runall these experiments in less than a week, while us-ing roughly three months worth of CPU time.9 ConclusionsWe have explored ?realistic SSL,?
where all parame-ters are tuned via 5-fold cross validation, to simulatea real-world experience of trying to use unlabeleddata in a novel NLP task.
Our medium-scale empir-ical study of SVM, S3VM, and MR revealed that noalgorithm is always superior, and furthermore thatthere are cases in which each SSL algorithm we ex-amined can perform worse than SVM (in some casessignificantly worse across 10 trials).
To mitigatesuch risks, we proposed a simple meta-level proce-dure that selects one of the three models based ontuning performance.
While cross validation is oftendismissed for model selection in SSL due to a lackof labeled data, this Best Tuning approach proves ef-fective in helping to ensure that incorporating unla-beled data does not hurt performance.
Interestingly,this works well only when optimizing accuracy dur-ing tuning.
For future work, we plan to extend thisstudy to include additional datasets, algorithms, andtuning criteria.
We also plan to develop more so-phisticated techniques for choosing which SL/SSLalgorithm to use in practice.AcknowledgmentsA.
Goldberg is supported in part by a Yahoo!
KeyTechnical Challenges Grant.26ReferencesMikhail Belkin, Partha Niyogi, and Vikas Sindhwani.2006.
Manifold regularization: A geometric frame-work for learning from labeled and unlabeled exam-ples.
Journal of Machine Learning Research, 7:2399?2434, November.Avrim Blum and Tom Mitchell.
1998.
Combin-ing labeled and unlabeled data with co-training.
InCOLT: Proceedings of the Workshop on Computa-tional Learning Theory.Ulf Brefeld and Tobias Scheffer.
2006.
Semi-supervisedlearning for structured output variables.
In ICML06,23rd International Conference on Machine Learning,Pittsburgh, USA.R.
Bruce and J. Wiebe.
1994.
Word-sense disambigua-tion using decomposable models.
In Proceedings ofthe 32nd Annual Meeting of the Association for Com-putational Linguistics, pages 139?146.Lijuan Cai and Thomas Hofmann.
2003.
Text catego-rization by boosting automatically extracted concepts.In SIGIR ?03: Proceedings of the 26th annual interna-tional ACM SIGIR conference on Research and devel-opment in informaion retrieval.Olivier Chapelle and Alexander Zien.
2005.
Semi-supervised classification by low density separation.
InProceedings of the Tenth International Workshop onArtificial Intelligence and Statistics (AISTAT 2005).Olivier Chapelle, Alexander Zien, and BernhardScho?lkopf, editors.
2006.
Semi-supervised learning.MIT Press.Tom Fawcett.
2004.
ROC graphs: Notes and practicalconsiderations for researchers.
Technical Report HPL-2003-4.Andrew B. Goldberg, Nathanael Fillmore, David Andrze-jewski, Zhiting Xu, Bryan Gibson, and Xiaojin Zhu.2009.
May all your wishes come true: A study ofwishes and how to recognize them.
In Proceedingsof NAACL HLT.Thorsten Joachims.
1999.
Making large-scale svmlearning practical.
In B. Scho?lkopf, C. Burges, andA.
Smola, editors, Advances in Kernel Methods - Sup-port Vector Learning.
MIT Press.M.
Karlen, J. Weston, A. Erkan, and R. Collobert.
2008.Large scale manifold transduction.
In Andrew McCal-lum and Sam Roweis, editors, Proceedings of the 25thAnnual International Conference on Machine Learn-ing (ICML 2008), pages 448?455.
Omnipress.Vikas Sindhwani and S. Sathiya Keerthi.
2007.
New-ton methods for fast solution of semi-supervised linearSVMs.
In Leon Bottou, Olivier Chapelle, Dennis De-Coste, and Jason Weston, editors, Large-Scale KernelMachines.
MIT Press.V.
Sindhwani and D. Rosenberg.
2008.
An rkhs formulti-view learning and manifold co-regularization.In Andrew McCallum and Sam Roweis, editors, Pro-ceedings of the 25th Annual International Conferenceon Machine Learning (ICML 2008), pages 976?983.Omnipress.Vikas Sindhwani, Partha Niyogi, and Mikhail Belkin.2005.
Beyond the point cloud: from transductive tosemi-supervised learning.
In ICML05, 22nd Interna-tional Conference on Machine Learning.Vikas Sindhwani, Sathiya Keerthi, and Olivier Chapelle.2006.
Deterministic annealing for semi-supervisedkernel machines.
In ICML06, 23rd International Con-ference on Machine Learning, Pittsburgh, USA.Douglas Thain, Todd Tannenbaum, and Miron Livny.2005.
Distributed computing in practice: the condorexperience.
Concurrency - Practice and Experience,17(2-4):323?356.Xiaojin Zhu.
2005.
Semi-supervised learning literaturesurvey.
Technical Report 1530, Department of Com-puter Sciences, University of Wisconsin, Madison.27
