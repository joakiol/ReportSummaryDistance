Proceedings of the 7th Workshop on Statistical Machine Translation, pages 96?103,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsLinguistic Features for Quality EstimationMariano FeliceResearch Group in Computational LinguisticsUniversity of WolverhamptonStafford StreetWolverhampton, WV1 1SB, UKMariano.Felice@wlv.ac.ukLucia SpeciaDepartment of Computer ScienceUniversity of SheffieldRegent Court, 211 PortobelloSheffield, S1 4DP, UKL.Specia@dcs.shef.ac.ukAbstractThis paper describes a study on the contribu-tion of linguistically-informed features to thetask of quality estimation for machine trans-lation at sentence level.
A standard regressionalgorithm is used to build models using a com-bination of linguistic and non-linguistic fea-tures extracted from the input text and its ma-chine translation.
Experiments with English-Spanish translations show that linguistic fea-tures, although informative on their own, arenot yet able to outperform shallower featuresbased on statistics from the input text, itstranslation and additional corpora.
However,further analysis suggests that linguistic infor-mation is actually useful but needs to be care-fully combined with other features in order toproduce better results.1 IntroductionEstimating the quality of automatic translations isbecoming a subject of increasing interest within theMachine Translation (MT) community for a num-ber of reasons, such as helping human translatorspost-editing MT, warning users about non-reliabletranslations or combining output from multiple MTsystems.
Different from most classic approaches formeasuring the progress of an MT system or compar-ing MT systems, which assess quality by contrast-ing system output to reference translations such asBLEU (Papineni et al, 2002), Quality Estimation(QE) is a more challenging task, aimed at MT sys-tems in use, and therefore without access to refer-ence translations.From the findings of previous work on reference-dependent MT evaluation, it is clear that metricsexploiting linguistic information can achieve sig-nificantly better correlation with human judgmentson quality, particularly at the level of sentences(Gime?nez and Ma`rquez, 2010).
Intuitively, thisshould also apply for quality estimation metrics:while evaluation metrics compare linguistic repre-sentations of the system output and reference trans-lations (e.g.
matching of n-grams of part-of-speechtags or predicate-argument structures), quality esti-mation metrics would perform the (more complex)comparison og linguistic representations of the inputand translation texts.
The hypothesis put forward inthis paper is therefore that using linguistic informa-tion to somehow contrast the input and translationtexts can be beneficial for quality estimation.We test this hypothesis as part of the WMT-12shared task on quality estimation.
The system sub-mitted to this task (WLV-SHEF) integrates linguis-tic information to a strong baseline system usingonly shallow statistics from the input and transla-tion texts, with no explicit information from the MTsystem that produced the translations.
A variantalso tests the addition of linguistic information toa larger set of shallow features.
The quality esti-mation problem is modelled as a supervised regres-sion task using Support Vector Machines (SVM),which has been shown to achieve good performancein previous work (Specia, 2011).
Linguistic featuresare computed using a number of auxiliary resourcessuch as parsers and monolingual corpora.The remainder of this paper is organised as fol-lows.
Section 2 gives an overview of previous work96on quality estimation, Section 3 describes the set oflinguistic features proposed in this paper, along withgeneral experimental settings, Section 4 presents ourevaluation and Section 5 provides conclusions and abrief discussion of future work.2 Related WorkReference-free MT quality assessment was ini-tially approached as a Confidence Estimation task,strongly biased towards exploiting data from a Sta-tistical MT (SMT) system and the translation pro-cess to model the confidence of the system in theproduced translation.
Blatz et al (2004) attemptedsentence-level assessment using a set of 91 features(from the SMT system input and translation texts)and automatic annotations such as NIST and WER.Experiments on classification and regression usingdifferent machine learning techniques produced notvery encouraging results.
More successful experi-ments were later run by Quirk (2004) in a similarsetting but using a smaller dataset with human qual-ity judgments.Specia et al (2009a) used Partial Least Squaresregression to jointly address feature selection andmodel learning using a similar set of features anddatasets annotated with both automatic and humanscores.
Black-box features (i.e.
those extracted fromthe input and translation texts only) were as discrim-inative as glass-box features (i.e.
those from the MTsystem).
Later work using black-box features onlyfocused on finding an appropriate threshold for dis-criminating ?good?
from ?bad?
translations for post-editing purposes (Specia et al, 2009b) and investi-gating more objective ways of obtaining human an-notation, such as post-editing time (Specia, 2011).Recent approaches have started exploiting lin-guistic information with promising results.
Speciaet al (2011), for instance, used part-of-speech (PoS)tagging, chunking, dependency relations and namedentities for English-Arabic quality estimation.
Hard-meier (2011) explored the use of constituencyand dependency trees for English-Swedish/Spanishquality estimation.
Focusing on word-error detec-tion through the estimation of WER, Xiong et al(2010) used PoS tags of neighbouring words and alink grammar parser to detect words that are not con-nected to the rest of the sentence.
Work by Bach etal.
(2011) focused on learning patterns of linguis-tic information (such as sequences of part-of-speechtags) to predict sub-sentence errors.
Finally, Pighinand Ma`rquez (2011) modelled the expected projec-tions of semantic roles from the input text into thetranslations.3 MethodOur work focuses on the use of a wide range oflinguistic information for representing different as-pects of translation quality to complement shallow,system-independent features that have been provedto perform well in previous work.3.1 Linguistic featuresNon-linguistic features, such as sentence length orn-gram statistics, are limited in their scope sincethey can only account for very shallow aspects ofa translation.
They convey no notion of meaning,grammar or content and as a result they could bevery biased towards describing only superficial as-pects.
For this reason, we introduce linguistic fea-tures that account for richer aspects of translationsand are in closer relation to the way humans maketheir judgments.
All of the proposed features, lin-guistic or not, are MT-system independent.The proposal of linguistic features was guided bythree main aspects of translation: fidelity, fluencyand coherence.
The number of features that wereeventually extracted was inevitably limited by theavailability of suitable tools for the language pairat hand, mainly for Spanish.
As a result, many ofthe features that were initially devised could not beimplemented (e.g.
grammar checking).
A total of70 linguistic features were extracted, as summarisedbelow, where S and T indicate whether they refer tothe source/input or translation texts respectively:?
Sentence 3-gram log-probability and perplexityusing a language model (LM) of PoS tags [T]?
Number, percentage and ratio of content words(N, V, ADJ) and function words (DET, PRON,PREP, ADV) [S & T]?
Width and depth of constituency and depen-dency trees for the input and translation textsand their differences [S & T]97?
Percentage of nouns, verbs and pronouns in thesentence and their ratios between [S & T]?
Number and difference in deictic elements in[S & T]?
Number and difference in specific types ofnamed entities (person, organisation, location,other) and the total of named entities [S & T]?
Number and difference in noun, verb andprepositional phrases [S & T]?
Number of ?dangling?
(i.e.
unlinked) deter-miners [T]?
Number of explicit (pronominal, non-pronominal) and implicit (zero pronoun)subjects [T]?
Number of split contractions in Spanish (i.e.al=a el, del=de el) [T]?
Number and percentage of subject-verb dis-agreement cases [T]?
Number of unknown words estimated using aspell checker [T]While many of these features attempt to checkfor general errors (e.g.
subject verb disagreement),others are targeted at usual MT errors (e.g.
?dan-gling?
determiners, which are commonly introducedby SMT systems and are not linked to any words) ortarget language peculiarities (e.g.
Spanish contrac-tions, zero subjects).
In particular, studying deeperaspects such as different types of subjects can pro-vide a good indication of how natural a translationis in Spanish, which is a pro-drop language.
Such adistinction is expected to spot unnatural expressions,such as those caused by unnecessary pronoun repe-tition.1For subject classification, we identified all VPsand categorised them according to their preceding1E.g.
(1) The girl beside me was smiling rather brightly.She thought it was an honor that the exchange student shouldbe seated next to her.
?
*La nin?a a mi lado estaba sonrientebastante bien.
Ella penso?
que era un honor que el intercambiode estudiantes se encuentra pro?ximo a ella.
(superfluous)(2) She is thought to have killed herself through suffocation us-ing a plastic bag.?
*Ella se cree que han matado a ella medi-ante asfixia utilizando una bolsa de pla?stico.
(confusing)NPs.
Thus, explicit subjects were classified aspronominal (PRON+VP) or non-pronominal (NON-PRON-NP+VP) while implicit subjects only in-cluded elided (zero) subjects (i.e.
a VP not precededby an NP).Subject-verb agreement cases were estimated byrules analysing person, number and gender matchesin explicit subject cases, considering also inter-nal NP agreement between determiners, nouns, ad-jectives and pronouns.2 Deictics, common coher-ence indicators (Halliday and Hasan, 1976), werechecked against manually compiled lists.3 Unknownwords were estimated using the JMySpell4 spellchecker with the publicly available Spanish (es ES)OpenOffice5 dictionary.
In order to avoid incorrectestimates, all named entities were filtered out beforespell-checking.TreeTagger (Schmid, 1995) was used for PoS tag-ging of English texts, while Freeling (Padro?
et al,2010) was used for PoS tagging in Spanish andfor constituency parsing, dependency parsing andnamed entity recognition in both languages.In order to compute n-gram statistics over PoStags, two language models of general and moredetailed morphosyntactic PoS were built using theSRILM toolkit (Stolcke, 2002) on the PoS-taggedAnCora corpus (Taule?
et al, 2008).3.2 Shallow featuresIn a variant of our system, the linguistic featureswere complemented by a set of 77 non-linguisticfeatures:?
Number and proportion of unique tokens andnumbers in the sentence [S & T]?
Sentence length ratios [S & T]?
Number of non-alphabetical tokens and theirratios [S & T]?
Sentence 3-gram perplexity [S & T]2E.g.
*Algunas de estas personas se convertira?
en he?roes.
(number mismatch), *Barricadas fueron creados en la calleCortlandt.
(gender mismatch), *Buena mentirosos esta?n cuali-ficados en lectura.
(internal NP gender and number mismatch).3These included common deictic terms compiled from vari-ous sources, such as hoy, all?
?, tu?
(Spanish) or that, now or there(English).4http://kenai.com/projects/jmyspell5http://www.openoffice.org/98?
Type/Token Ratio variations: corrected TTR(Carroll, 1964), Log TTR (Herdan, 1960),Guiraud Index (Guiraud, 1954), Uber Index(Dugast, 1980) and Jarvis TTR (Jarvis, 2002)[S & T]?
Average token frequency from a monolingualcorpus [S]?
Mismatches in opening and closing bracketsand quotation marks [S & T]?
Differences in brackets, quotation marks, punc-tuation marks and numbers [S & T]?
Average number of occurrences of all wordswithin the sentence [T]?
Alignment score (IBM-4) and percentage ofdifferent types of word alignments by GIZA++(from the SMT training alignment model pro-vided)Our basis for comparison is the set of 17 baselinefeatures, which are shallow MT system-independentfeatures provided by the WMT-12 QE shared taskorganizers.3.3 Building QE modelsWe created two main feature sets from the featureslisted above for the WMT-12 QE shared task:WLV-SHEF FS: all features, that is, baseline fea-tures, shallow features (Section 3.2) and lin-guistic features (Section 3.1).WLV-SHEF BL: baseline features and linguisticfeatures (Section 3.1).Additionally, we experimented with other variantsof these feature sets using 3-fold cross validation onthe training set, such as only linguistic features andonly non-linguistic features, but these yielded poorerresults and are not reported in this paper.We address the QE problem as a regression taskby building SVM models with an epsilon regressorand a radial basis function kernel using the LibSVMtoolkit (Chang and Lin, 2011).
Values for the cost,epsilon and gamma parameters were optimized us-ing 5-fold cross validation on the training set.MAE ?
RMSE ?
Pearson ?Baseline 0.69 0.82 0.562WLV-SHEF FS 0.69 0.85 0.514WLV-SHEF BL 0.72 0.86 0.490Table 1: Scoring performanceThe training sets distributed for the shared taskcomprised 1, 832 English sentences taken from newstexts and their Spanish translations produced by anSMT system, Moses (Koehn et al, 2007), whichhad been trained on a concatenation of Europarl andnews-commentaries data (from WMT-10).
Transla-tions were accompanied by a quality score derivedfrom an average of three human judgments of post-editing effort using a 1-5 scale.The models built for each of these two featuresets were evaluated using the official test set of 422sentences produced in the same fashion as the train-ing set.
Two sub-tasks were considered: (i) scor-ing translations using the 1-5 quality scores, and(ii) ranking translations from best to worse.
Whilequality scores were directly predicted by our mod-els, sentence rankings were defined by ordering thetranslations according to their predicted scores in de-scending order, with no additional criteria to resolveties other than the natural ordering given by the sort-ing algorithm.4 Results and EvaluationTable 1 shows the official results of our systems inthe scoring task in terms of Mean Absolute Error(MAE) and Root Mean Squared Error (RMSE), themetrics used in the shared task, as well as in termsof Pearson correlation.Results reveal that our models fall slightly be-low the baseline, although this drop is not statisti-cally significant in any of the cases (paired t-tests forBaseline vs WLV-SHEF FS and Baseline vs WLV-SHEF BL yield p > 0.05).
This may suggest thatfor this particular dataset the baseline features al-ready cover all relevant aspects of quality on theirown, or simply that the representation of the lin-guistic features is not appropriate for the task.
Thequality of the resources used to extract the linguisticfeatures may also have been an issue.
However, afeature selection method may find a different com-99Figure 1: Comparison of true versus predicted scoresbination of features that outperforms the baseline, asis later described in this section.A correlation analysis between our predictedscores and the gold standard (Figure 1) shows somedispersion, especially for the WLV-SHEF FS set,with lower Pearson coefficients when compared tothe baseline.
The fluctuation of predicted values fora single score is also very noticeable, spanning morethan one score band in some cases.
However, if weconsider the RMSE achieved by our models, we findthat, on average, predictions deviate less than 0.9 ab-solute points.A closer look at the score distribution (Figure 2)reveals our models had some difficulty predictingscores in the 1-2 range, possibly affected by thelower proportion of these cases in the training data.In addition, it is interesting to see that the only sen-tence with a true score of 1 is predicted as a verygood translation (with a score greater than 3.5).
Thereason for this is that the translation has isolatedgrammatical segments that our features might regardas good but it is actually not faithful to the original.6Although the cause for this behaviour can be tracedto inaccurate tokenisation, this reveals that our fea-tures assess fidelity only superficially and deepersemantically-aware indicators should be explored.Results for the ranking task also fall below thebaseline as shown in Table 2, according to the twoofficial metrics: DeltaAvg and Spearman rank cor-relation coefficient.4.1 Further analysisAt first glance, the performance of our models seemsto indicate that the integration of linguistic infor-6I won?t give it away.
?
*He ganado ?
t darle.Figure 2: Scatter plot of true versus predicted scoresDeltaAvg ?
Spearman ?Baseline 0.55 0.58WLV-SHEF FS 0.51 0.52WLV-SHEF BL 0.50 0.49Table 2: Ranking performancemation is not beneficial, since both linguistically-informed feature sets lead to poorer performance ascompared to the baseline feature set, which containsonly shallow, language-independent features.
How-ever, there could be many factors affecting perfor-mance so further analysis was necessary to assesstheir contribution.Our first analysis focuses on the performance ofindividual features.
To this end, we built and testedmodels using only one feature at a time and repeatedthe process afterwards using the full WLV-SHEF FSset without one feature at a time.
In Table 3 we re-port the 5-best and 5-worst performing features.
Al-though purely statistical features lead the rank, lin-guistic features also appear among the top five (asindicated by L?
), showing that they can be as goodas other shallow features.
It is interesting to note thata few features appear as the top performing in bothcolumns (e.g.
source bigrams in 4th frequency quar-tile and target LM probability).
These constitute thetruly top performing features.Our second analysis studies the optimal subset offeatures that would yield the best performance on thetest set, from which we could draw further conclu-sions.
Since this analysis requires training and test-ing models using all the possible partitions of the100Rank One feature All but one feature1 Source bigrams in 4th freq.
quartile Source average token length2 Source LM probability Source bigrams in 4th freq.
quartile3 Target LM probability Unknown words in target L?4 Number of source bigrams Target LM probability5 Target PoS LM probability L?
Difference in constituency tree width L?143 Percentage of target S-V agreement L?
Difference in number of periods144 Source trigrams in 2nd freq.
quartile Number of source bigrams145 Target location entities L?
Target person entities L?146 Source trigrams in 3rd freq.
quartile Target Corrected TTR147 Source average translations by inv.
freq.
Source trigrams in 3rd freq.
quartileTable 3: List of best and worst performing featuresfull feature set,7 it is infeasible in practice so weadopted the Sequential Forward Selection methodinstead (Alpaydin, 2010).
Using this method, westart from an empty set and add one feature at a time,keeping in the set only the features that decrease theerror until no further improvement is possible.
Thisstrategy decreases the number of iterations substan-tially8 but it does not guarantee finding a global op-timum.
Still, a local optimum was acceptable forour purpose.
The optimal feature set found by ourselection algorithm is shown in Table 4.Error rates are lower when using this optimal fea-ture set (MAE=0.62 and RMSE=0.76) but the differ-ence is only statistically significant when comparedto the baseline with 93% confidence level (paired t-test with p <= 0.07).
However, this analysis allowsus to see how many linguistic features get selectedfor the optimal feature set.Out of the total 37 features in the optimal set,15 are linguistic (40.5%), showing that they are infact informative when strategically combined withother shallow indicators.
This also reveals that fea-ture selection is a key issue for building a qualityestimation system that combines linguistic and shal-low information.
Using a sequential forward selec-tion method, the optimal set is composed of both lin-guistic and shallow features, reinforcing the idea thatthey account for different aspects of quality and arenot interchangeable but actually complementary.7For 147 features: 21478For 147 features, worst case is 147 ?
(147 + 1)/2 =10, 878.5 Conclusions and Future WorkWe have explored the use of linguistic informa-tion for quality estimation of machine translations.Our approach was not able to outperform a baselinewith only shallow features.
However, further featureanalysis revealed that linguistic features are comple-mentary to shallow features and must be strategi-cally combined in order to be exploited efficiently.The availability of linguistic tools for processingSpanish is limited, and thus the linguistic featuresused here only account for a few of the many aspectsinvolved in translation quality.
In addition, comput-ing linguistic information is a challenging processfor a number of reasons, mainly the fact that trans-lations are often ungrammatical, and thus linguisticprocessors may return inaccurate results, leading tofurther errors.In future work we plan to integrate more globallinguistic features such as grammar checkers, alongwith deeper features such as semantic roles, hybridn-grams, etc.
In addition, we have noticed that rep-resenting information for input and translation textsindependently seems more appropriate than con-trasting input and translation information within thesame feature.
This representation issue is somehowcounter-intuitive and is yet to be investigated.AcknowledgementsThis research was supported by the European Com-mission, Education & Training, Eramus Mundus:EMMC 2008-0083, Erasmus Mundus Masters inNLP & HLT programme.101Iter.
Feature1 Source bigrams in 4th frequency quartile2 Target PoS LM probability L?3 Source average token length4 Guiraud Index of T5 Unknown words in T L?6 Difference in number of VPs between S and T L?7 Diff.
in constituency trees width of S and T L?8 Non-alphabetical tokens in T9 Ratio of length between S and T10 Source trigrams in 4th frequency quartile11 Number of content words in S L?12 Source 3-gram perplexity13 Ratio of PRON percentages in S and T L?14 Number of NPs in T L?15 Average number of source token translations withp > 0.05 weighted by frequency16 Source 3-gram LM probability17 Target simple PoS LM probability L?18 Difference in dependency trees depth of S and T L?19 Number of NPs in S L?20 Number of tokens in S21 Number of content words in T L?22 Source unigrams in 3rd frequency quartile23 Source unigrams in 1st frequency quartile24 Source unigrams in 2nd frequency quartile25 Average number of source token translations withp > 0.01 weighted by frequency26 Ratio of non-alpha tokens in S and T27 Difference of question marks between S and T nor-malised by T length28 Percentage of pron subjects in T L?29 Percentage of verbs in T L?30 Constituency trees width for S L?31 Absolute diff.
of question marks between S and T32 Average num.
of source token trans.
with p > 0.233 Diff.
of person entities between S and T L?34 Diff.
of periods between S and T norm.
by T length35 Diff.
of semicolons between S and T normalised byT length36 Source 3-gram perplexity without end-of-sentencemarkers37 Absolute difference of periods between S and TTable 4: An optimal set of features for the test set.
Thenumber of iteration indicates the order in which featureswere selected, giving a rough ranking of features by theirperformance.ReferencesEthem Alpaydin.
2010.
Introduction to Machine Learn-ing.
Adaptive Computation and Machine Learning.The MIT Press, Cambridge, MA, 2nd edition.Nguyen Bach, Fei Huang, and Yaser Al-Onaizan.
2011.Goodness: A method for measuring machine transla-tion confidence.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Linguis-tics: Human Language Technologies, pages 211?219,Portland, Oregon, USA, June.
Association for Compu-tational Linguistics.John Blatz, Erin Fitzgerald, George Foster, Simona Gan-drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,and Nicola Ueffing.
2004.
Confidence estimationfor machine translation.
Final Report of Johns Hop-kins 2003 Summer Workshop on Speech and Lan-guage Engineering, Johns Hopkins University, Balti-more, Maryland, USA, March.John Bissell Carroll.
1964.
Language and Thought.Prentice-Hall, Englewood Cliffs, NJ.Chih-Chung Chang and Chih-Jen Lin.
2011.
Libsvm:A library for support vector machines.
ACM Trans.Intell.
Syst.
Technol., 2(3):1?27, May.Daniel Dugast.
1980.
La statistique lexicale.
Slatkine,Gene`ve.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
2010.
Linguisticmeasures for automatic machine translation evalua-tion.
Machine Translation, 24(3):209?240.Pierre Guiraud.
1954.
Les Caracte`res Statistiques duVocabulaire.
Presses Universitaires de France, Paris.Michael A. K. Halliday and Ruqaiya Hasan.
1976.
Co-hesion in English.
Longman, London.Christian Hardmeier.
2011.
Improving machine transla-tion quality prediction with syntactic tree kernels.
InProceedings of the 15th conference of the EuropeanAssociation for Machine Translation (EAMT 2011),pages 233?240, Leuven, Belgium.Gustav Herdan.
1960.
Type-token Mathematics: A Text-book of Mathematical Linguistics.
Mouton & Co., TheHague.Scott Jarvis.
2002.
Short texts, best-fitting curves andnew measures of lexical diversity.
Language Testing,19(1):57?84, January.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proceed-ings of the 45th Annual Meeting of the Association forComputational Linguistics Companion Volume Pro-ceedings of the Demo and Poster Sessions, pages 177?180, Prague, Czech Republic, June.
Association forComputational Linguistics.102Llus Padro?, Miquel Collado, Samuel Reese, MarinaLloberes, and Irene Castello?n.
2010.
Freeling2.1: Five years of open-source language process-ing tools.
In Nicoletta Calzolari (Conference Chair),Khalid Choukri, Bente Maegaard, Joseph Mariani,Jan Odijk, Stelios Piperidis, Mike Rosner, and DanielTapias, editors, Proceedings of the Seventh conferenceon International Language Resources and Evaluation(LREC?10), Valletta, Malta, May.
European LanguageResources Association (ELRA).Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Meeting on Association for Computa-tional Linguistics, pages 311?318.Daniele Pighin and Llu?
?s Ma`rquez.
2011.
Automaticprojection of semantic structures: an application topairwise translation ranking.
In Fifth Workshop onSyntax, Semantics and Structure in Statistical Trans-lation (SSST-5), Portland, Oregon.Christopher B. Quirk.
2004.
Training a sentence-levelmachine translation confidence metric.
In Proceed-ings of the International Conference on Language Re-sources and Evaluation, volume 4 of LREC 2004,pages 825?828, Lisbon, Portugal.Helmut Schmid.
1995.
Improvements in part-of-speechtagging with an application to german.
In In Pro-ceedings of the ACL SIGDAT-Workshop, pages 47?50,Dublin, Ireland, August.Lucia Specia, Marco Turchi, Nicola Cancedda, MarcDymetman, and Nello Cristianini.
2009a.
Estimatingthe sentence-level quality of machine translation sys-tems.
In Proceedings of the 13th Annual Conferenceof the European Association for Machine Translation(EAMT), pages 28?35, Barcelona, Spain, May.Lucia Specia, Marco Turchi, Zhuoran Wang, JohnShawe-Taylor, and Craig Saunders.
2009b.
Improv-ing the confidence of machine translation quality esti-mates.
In Proceedings of the Twelfth Machine Transla-tion Summit (MT Summit XII), pages 136?143, Ottawa,Canada, August.Lucia Specia, Najeh Hajlaoui, Catalina Hallett, andWilker Aziz.
2011.
Predicting machine translationadequacy.
In Machine Translation Summit XIII, pages19?23, Xiamen, China, September.Lucia Specia.
2011.
Exploiting objective annotations formeasuring translation post-editing effort.
In Proceed-ings of the 15th Conference of the European Associa-tion for Machine Translation, pages 73?80, Leuven.Andreas Stolcke.
2002.
Srilman extensible languagemodeling toolkit.
In Proceedings of the 7th Inter-national Conference on Spoken Language Process-ing (ICSLP 2002), volume 2, pages 901?904, Denver,USA, November.Mariona Taule?, M. Antnia Mart?
?, and Marta Recasens.2008.
Ancora: Multilevel annotated corpora for cata-lan and spanish.
In Nicoletta Calzolari (ConferenceChair), Khalid Choukri, Bente Maegaard, Joseph Mar-iani, Jan Odijk, Stelios Piperidis, Mike Rosner, andDaniel Tapias, editors, Proceedings of the Sixth In-ternational Conference on Language Resources andEvaluation (LREC?08), Marrakech, Morocco, May.European Language Resources Association (ELRA).Deyi Xiong, Min Zhang, and Haizhou Li.
2010.
Er-ror detection for statistical machine translation usinglinguistic features.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguis-tics, pages 604?611, Uppsala, Sweden.103
