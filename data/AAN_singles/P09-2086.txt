Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 341?344,Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLPA Succinct N-gram Language ModelTaro Watanabe Hajime Tsukada Hideki IsozakiNTT Communication Science Laboratories2-4 Hikaridai Seika-cho Soraku-gun Kyoto 619-0237 Japan{taro,tsukada,isozaki}@cslab.kecl.ntt.co.jpAbstractEfficient processing of tera-scale text datais an important research topic.
This pa-per proposes lossless compression of N -gram language models based on LOUDS,a succinct data structure.
LOUDS suc-cinctly represents a trie with M nodes as a2M + 1 bit string.
We compress it furtherfor the N -gram language model structure.We also use ?variable length coding?
and?block-wise compression?
to compress val-ues associated with nodes.
Experimentalresults for three large-scale N -gram com-pression tasks achieved a significant com-pression rate without any loss.1 IntroductionThere has been an increase in available N -gramdata and a large amount of web-scaled N -gramdata has been successfully deployed in statisticalmachine translation.
However, we need either amachine with hundreds of gigabytes of memoryor a large computer cluster to handle them.Either pruning (Stolcke, 1998; Church et al,2007) or lossy randomizing approaches (Talbotand Brants, 2008) may result in a compact repre-sentation for the application run-time.
However,the lossy approaches may reduce accuracy, andtuning is necessary.
A lossless approach is obvi-ously better than a lossy one if other conditionsare the same.
In addtion, a lossless approach caneasly combined with pruning.
Therefore, losslessrepresentation of N -gram is a key issue even forlossy approaches.Raj and Whittaker (2003) showed a general N -gram language model structure and introduced alossless algorithm that compressed a sorted integervector by recursively shifting a certain number ofbits and by emitting index-value inverted vectors.However, we need more compact representation.In this work, we propose a succinct way torepresent the N -gram language model structurebased on LOUDS (Jacobson, 1989; Delpratt etal., 2006).
It was first introduced by Jacobson(1989) and requires only a small space close tothe information-theoretic lower bound.
For an Mnode ordinal trie, its information-theoretical lowerbound is 2M ?
O(lg M) bits (lg(x) = log2(x))1-gram 2-gram 3-gramprobabilityback-offpointerword idprobabilityback-offpointerword idprobabilityback-offpointerFigure 1: Data structure for language modeland LOUDS succinctly represents it by a 2M + 1bit string.
The space is further reduced by consid-ering the N -gram structure.
We also use variablelength coding and block-wise compression to com-press the values associated with each node, such asword ids, probabilities or counts.We experimented with English Web 1T 5-gramfrom LDC consisting of 25 GB of gzipped rawtext N -gram counts.
By using 8-bit floating pointquantization 1, N -gram language models are com-pressed into 10 GB, which is comparable to a lossyrepresentation (Talbot and Brants, 2008).2 N -gram Language ModelWe assume a back-off N -gram language model inwhich the conditional probability Pr(wn|wn?11)for an arbitrary N -gram wn1= (w1, ..., wn) is re-cursively computed as follows.?
(wn1) if wn1exists.?
(wn?11)Pr(wn|wn?12) if wn?11exists.Pr(wn|wn?12) otherwise.?
(wn1) and ?
(wn1) are smoothed probabilities andback-off coefficients, respectively.The N -grams are stored in a trie structure asshown in Figure 1.
N -grams of different ordersare stored in different tables and each row corre-sponds to a particular wn1, consisting of a word idfor wn, ?
(wn1), ?
(wn1) and a pointer to the first po-sition of the succeeding (n + 1)-grams that sharethe same prefix wn1.
The succeeding (n+1)-gramsare stored in a contiguous region and sorted by theword id of wn+1.
The boundary of the region is de-termined by the pointer of the next N -gram in the1The compact representation of the floating point is out ofthe scope of this paper.
Therefore, we use the term losslesseven when using floating point quantization.34101 2 3 45 6 7 8 9 1011 12 13 14 15(a) Trie structurenode id 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15bit position 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32LOUDS bit 1 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0(b) Corresponding LOUDS bit string0 1 2 34 5 6 7 8 910 11 12 13 14(c) Trie structure for N -gramnode id 0 1 2 3 4 5 6 7 8 9bit position 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20LOUDS bit 1 1 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0(d) Corresponding N -gram optimized LOUDS bit stringFigure 2: Optimization of LOUDS bit string for N -gram datarow.
When an N -gram is traversed, binary searchis performed N times.
If each word id correspondsto its node position in the unigram table, we canremove the word ids for the first order.Our implementation merges across different or-ders of N -grams, then separates into multiple ta-bles such as word ids, smoothed probabilities,back-off coefficients, and pointers.
The startingpositions of different orders are memorized to al-low access to arbitrary orders.
To store N -gramcounts, we use three tables for word ids, countsand pointers.
We share the same tables for wordids and pointers with additional probability andback-off coefficient tables.To support distributed computation (Brants etal., 2007), we further split the N -gram data into?shards?
by hash values of the first bigram.
Uni-gram data are shared across shards for efficiency.3 Succinct N -gram StructureThe table of pointers described in the previoussection represents a trie.
We use a succinct datastructure LOUDS (Jacobson, 1989; Delpratt et al,2006) for compact representation of the trie.For an M node ordinal trie, there exist12M+1(2M+1M)different tries.
Therefore,its information-theoretical lower bound islg?12M+1(2M+1M)??
2M ?
O(lg M) bits.LOUDS represents a trie with M nodes as a2M + O(M) bit string.The LOUDS bit string is constructed as follows.Starting from the root node, we traverse a trie inlevel order.
For each node with d ?
0 children, thebit string 1d0 is emitted.
In addition, 10 is prefixedto the bit string emitted by an imaginary super-rootnode pointing to the root node.
Figure 2(a) showsan example trie structure.
The nodes are numberedin level order, and from left to right.
The cor-responding LOUDS bit string is shown in Figure2(b).
Since the root node 0 has four child nodes,it emits four 1s followed by 0, which marks theend of the node.
Before the root node, we assumean imaginary super root node emits 10 for its onlychild, i.e., the root node.
After the root node, itsfirst child or node 1 follows.
Since (M + 1)0s andM1s are emitted for a trie with M nodes, LOUDSoccupies 2M + 1 bits.We define a basic operation on the bit string.sel1(i) returns the position of the i-th 1.
We canalso define similar operations over zero bit strings,sel0(i).
Given selb, we define two operations fora node x. parent(x) gives x?s parent node andfirstch(x) gives x?s first child node:parent(x) = sel1(x + 1) ?
x ?
1, (1)firstch(x) = sel0(x + 1) ?
x.
(2)To test whether a child node exists, we sim-ply check firstch(x) 6= firstch(x + 1).
Sim-ilarly, the child node range is determined by[firstch(x),firstch(x + 1)).3.1 Optimizing N -gram Structure for SpaceWe propose removing redundant bits from thebaseline LOUDS representation assuming N -gram structures.
Since we do not store any infor-mation in the root node, we can safely remove theroot so that the imaginary super-root node directlypoints to unigram nodes.
The node ids are renum-bered and the first unigram is 0.
In this way, 2 bitsare saved.The N -gram data structure has a fixed depth Nand takes a flat structure.
Since the highest or-der N -grams have no child nodes, they emit 0NNin the tail of the bit stream, where Nnstands forthe number of n-grams.
By memorizing the start-ing position of the highest order N -grams, we cancompletely remove NNbits.The imaginary super-root emits 1N10 at the be-ginning of the bit stream.
By memorizing the bi-gram starting position, we can remove the N1+ 1bits.Finally, parent(x) and firstch(x) are rewritten as342integer seq.
52 156 260 364coding 0x34 0x9c 0x01 0x04 0x01 0x6cboundary 1 1 0 1 0 1Figure 3: Example of variable length codingfollows:parent(x) = sel1(x + 1 ?N1) + N1?
x, (3)firstch(x) = sel0(x) + N1+ 1 ?
x.
(4)Figure 2(c) shows the N -gram optimized triestructure (N = 3) from Figure 2 with N1= 4and N3= 5.
The parent of node 8 is found bysel1(8+1?4) = 5 and 5+4?8 = 1.
The first childis located by sel0(8) = 16 and 16+4+1?8 = 13.When accessing the N -gram data structure,selb(i) operations are used extensively.
We use anauxiliary dictionary structure proposed by Kim etal.
(2005) and Jacobson (1989) that supports anefficient sel1(i) (sel0(i)) with the dictionary.
Weomit the details due to lack of space.3.2 Variable Length CodingThe above method compactly represents pointers,but not associated values, such as word ids orcounts.
Raj and Whittaker (2003) proposed in-teger compression on each range of the word idsequence that shared the same N -gram prefix.Here, we introduce a simple but more effec-tive variable length coding for integer sequencesof word ids and counts.
The basic idea comes fromencoding each integer by the smallest number ofrequired bytes.
Specifically, an integer within therange of 0 to 255 is coded as a 1-byte integer,the integers within the range of 256 to 65,535 arestored as 2-byte integers, and so on.
We use an ad-ditional bit vector to indicate the boundary of thebyte sequences.
Figure 3 presents an example in-teger sequence, 52, 156, 260 and 364 with codedintegers in hex decimals with boundary bits.In spite of the length variability, the systemcan directly access a value at index i as bytesin [sel1(i) + 1, sel1(i + 1) + 1) by the efficientsel1operation assuming that sel1(0) yields ?1.For example, the value 260 at index 2 in Figure3 is mapped onto the byte range of [sel1(2) +1, sel1(3) + 1) = [2, 4).3.3 Block-wise CompressionWe further compress every 8K-byte data block ofall tables in N -grams by using a generic com-pression library, zlib, employed in UNIX gzip.We treat a sequence of 4-byte floats in the prob-ability table as a byte stream, and compress ev-ery 8K-byte block.
To facilitate random access tothe compressed block, we keep track of the com-pressed block?s starting offsets.
Since the offsetsare in sorted order, we can apply sorted integercompression (Raj and Whittaker, 2003).
Since N -gram language model access preserves some local-ity, N -gram with block compression is still practi-cal enough to be usable in our system.4 ExperimentsWe applied the proposed representation to 5-gramtrained by ?English Gigaword 3rd Edition,?
?En-glish Web 1T 5-gram?
from LDC, and ?JapaneseWeb 1T 7-gram?
from GSK.
Since their tendenciesare the same, we only report in this paper the re-sults on English Web 1T 5-gram, where the sizeof the count data in gzipped raw text format is25GB, the number of N-grams is 3.8G, the vocab-ulary size is 13.6M words, and the number of thehighest order N-grams is 1.2G.We implemented an N -gram indexer/estimatorusing MPI inspired by the MapReduce imple-mentation of N -gram language model index-ing/estimation pipeline (Brants et al, 2007).Table 1 summarizes the overall results.
Weshow the initial indexed counts and the final lan-guage model size by differentiating compressionstrategies for the pointers, namely the 4-byte rawvalue (Trie), the sorted integer compression (In-teger) and our succinct representation (Succinct).The ?block?
indicates block compression.
For thesake of implementation simplicity, the sorted in-teger compression used a fixed 8-bit shift amount,although the original paper proposed recursivelydetermined optimum shift amounts (Raj and Whit-taker, 2003).
8-bit quantization was performedfor probabilities and back-off coefficients using asimple binning approach (Federico and Cettolo,2007).N -gram counts were reduced from 23.59GBto 10.57GB by our succinct representation withblock compression.
N -gram language models of42.65GB were compressed to 18.37GB.
Finally,the 8-bit quantized N -gram language models arerepresented by 9.83GB of space.Table 2 shows the compression ratio for thepointer table alone.
Block compression employedon raw 4-byte pointers attained a large reduc-tion that was almost comparable to sorted inte-ger compression.
Since large pointer value tablesare sorted, even a generic compression algorithmcould achieve better compression.
Using our suc-cinct representation, 2.4 bits are required for eachN -gram.
By using the ?flat?
trie structure, weapproach closer to its information-theoretic lowerbound beyond the LOUDS baseline.
With blockcompression, we achieved 1.8 bits per N -gram.Table 3 shows the effect of variable lengthcoding and block compression for the word ids,counts, probabilities and back-off coefficients.
Af-ter variable-length coding, the word id is almosthalf its original size.
We assign a word id for each343w/o block w/ blockCounts Trie 23.59 GB 12.21 GBInteger 14.59 GB 11.18 GBSuccinct 12.62 GB 10.57 GBLanguage Trie 42.65 GB 20.01 GBmodel Integer 33.65 GB 18.98 GBSuccinct 31.67 GB 18.37 GBQuantized Trie 24.73 GB 11.47 GBlanguage Integer 15.73 GB 10.44 GBmodel Succinct 13.75 GB 9.83 GBTable 1: Summary of N -gram compressiontotal per N -gram4-byte Pointer 12.04 GB 27.24 bits+block compression 2.42 GB 5.48 bitsSorted Integer 3.04 GB 6.87 bits+block compression 1.39 GB 3.15 bitsSuccinct 1.06 GB 2.40 bits+block compression 0.78 GB 1.76 bitsTable 2: Compression ratio for pointersword according to its reverse sorted order of fre-quency.
Therefore, highly frequent words are as-signed smaller values, which in turn occupies lessspace in our variable length coding.
With blockcompression, we achieved further 1 GB reductionin space.
Since the word id sequence preserveslocal ordering for a certain range, even a genericcompression algorithm is effective.The most frequently observed count in N -gramdata is one.
Therefore, we can reduce the spaceby the variable length coding.
Large compressionrates are achieved for both probabilities and back-off coefficients.5 ConclusionWe provided a succinct representation of the N -gram language model without any loss.
Ourmethod approaches closer to the information-theoretic lower bound beyond the LOUDS base-line.
Experimental results showed our succinctrepresentation drastically reduces the space forthe pointers compared to the sorted integer com-pression approach.
Furthermore, the space ofN -grams was significantly reduced by variabletotal per N -gramword id size (4 bytes) 14.09 GB 31.89 bits+variable length 6.72 GB 15.20 bits+block compression 5.57 GB 12.60 bitscount size (8 bytes) 28.28 GB 64.00 bits+variable length 4.85 GB 10.96 bits+block compression 4.22 GB 9.56 bitsprobability size (4 bytes) 14.14 GB 32.00 bits+block compression 9.55 GB 21.61 bits8-bit quantization 3.54 GB 8.00 bits+block compression 2.64 GB 5.97 bitsbackoff size (4 bytes) 9.76 GB 22.08 bits+block compression 2.48 GB 5.61 bits8-bit quantization 2.44 GB 5.52 bits+block compression 0.85 GB 1.92 bitsTable 3: Effects of block compressionlength coding and block compression.
A largeamount of N -gram data is reduced from unin-dexed gzipped 25 GB text counts to 10 GB ofindexed language models.
Our representation ispractical enough though we did not experimen-tally investigate the runtime efficiency in this pa-per.
The proposed representation enables us toutilize a web-scaled N -gram in our MT compe-tition system (Watanabe et al, 2008).
Our suc-cinct representation will encourage new researchon web-scaled N -gram data without requiring alarger computer cluster or hundreds of gigabytesof memory.AcknowledgmentsWe would like to thank Daisuke Okanohara for hisopen source implementation and extensive docu-mentation of LOUDS, which helped our originalcoding.ReferencesT.
Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean.2007.
Large language models in machine transla-tion.
In Proc.
of EMNLP-CoNLL 2007.K.
Church, T. Hart, and J. Gao.
2007.
Compressingtrigram language models with Golomb coding.
InProc.
of EMNLP-CoNLL 2007.O.
Delpratt, N. Rahman, and R. Raman.
2006.
Engi-neering the LOUDS succinct tree representation.
InProc.
of the 5th International Workshop on Experi-mental Algorithms.M.
Federico and M. Cettolo.
2007.
Efficient handlingof n-gram language models for statistical machinetranslation.
In Proc.
of the 2nd Workshop on Statis-tical Machine Translation.G.
Jacobson.
1989.
Space-efficient static trees andgraphs.
In 30th Annual Symposium on Foundationsof Computer Science, Nov.D.
K. Kim, J. C. Na, J. E. Kim, and K. Park.
2005.
Ef-ficient implementation of rank and select functionsfor succinct representation.
In Proc.
of the 5th Inter-national Workshop on Experimental Algorithms.B.
Raj and E. W. D. Whittaker.
2003.
Lossless com-pression of language model structure and word iden-tifiers.
In Proc.
of ICASSP 2003, volume 1.A.
Stolcke.
1998.
Entropy-based pruning of backofflanguage models.
In Proc.
of the ARPA Workshopon Human Language Technology.D.
Talbot and T. Brants.
2008.
Randomized languagemodels via perfect hash functions.
In Proc.
of ACL-08: HLT.T.
Watanabe, H. Tsukada, and H. Isozaki.
2008.
NTTSMT system 2008 at NTCIR-7.
In Proc.
of the 7thNTCIR Workshop, pages 420?422.344
