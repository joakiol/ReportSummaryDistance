Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 944?952,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsAutomatic Evaluation of Translation Quality for Distant Language PairsHideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito Sudoh, Hajime TsukadaNTT Communication Science Laboratories, NTT Corporation2-4 Hikaridai, Seikacho, Sorakugun, Kyoto, 619-0237, Japan{isozaki,hirao,kevinduh,sudoh,tsukada}@cslab.kecl.ntt.co.jpAbstractAutomatic evaluation of Machine Translation(MT) quality is essential to developing high-quality MT systems.
Various evaluation met-rics have been proposed, and BLEU is nowused as the de facto standard metric.
How-ever, when we consider translation betweendistant language pairs such as Japanese andEnglish, most popular metrics (e.g., BLEU,NIST, PER, and TER) do not work well.
Itis well known that Japanese and English havecompletely different word orders, and specialcare must be paid to word order in transla-tion.
Otherwise, translations with wrong wordorder often lead to misunderstanding and in-comprehensibility.
For instance, SMT-basedJapanese-to-English translators tend to trans-late ?A because B?
as ?B because A.?
Thus,word order is the most important problemfor distant language translation.
However,conventional evaluation metrics do not sig-nificantly penalize such word order mistakes.Therefore, locally optimizing these metricsleads to inadequate translations.
In this pa-per, we propose an automatic evaluation met-ric based on rank correlation coefficients mod-ified with precision.
Our meta-evaluation ofthe NTCIR-7 PATMT JE task data shows thatthis metric outperforms conventional metrics.1 IntroductionAutomatic evaluation of machine translation (MT)quality is essential to developing high-quality ma-chine translation systems because human evaluationis time consuming, expensive, and irreproducible.
Ifwe have a perfect automatic evaluation metric, wecan tune our translation system for the metric.BLEU (Papineni et al, 2002b; Papineni et al,2002a) showed high correlation with human judg-ments and is still used as the de facto standard au-tomatic evaluation metric.
However, Callison-Burchet al (2006) argued that the MT community is overlyreliant on BLEU by showing examples of poor per-formance.
For Japanese-to-English (JE) translation,Echizen-ya et al (2009) showed that the popularBLEU and NIST do not work well by using the sys-tem outputs of the NTCIR-7 PATMT (patent transla-tion) JE task (Fujii et al, 2008).
On the other hand,ROUGE-L (Lin and Hovy, 2003), Word Error Rate(WER), and IMPACT (Echizen-ya and Araki, 2007)worked better.In these studies, Pearson?s correlation coefficientand Spearman?s rank correlation ?
with human eval-uation scores are used to measure how closely anautomatic evaluation method correlates with humanevaluation.
This evaluation of automatic evaluationmethods is called meta-evaluation.
In human eval-uation, people judge the adequacy and the fluency ofeach translation.Denoual and Lepage (2005) pointed out thatBLEU assumes word boundaries, which is ambigu-ous in Japanese and Chinese.
Here, we assumethe word boundaries given by ChaSen, one of thestandard morphological analyzers (http://chasen-legacy.sourceforge.jp/) following Fujii et al(2008)In JE translation, most Statistical Machine Trans-lation (SMT) systems translate the Japanese sen-tence(J0) kare wa sono hon wo yonda nodesekaishi ni kyoumi ga attawhich means944(R0) he was interested in worldhistory because he read the bookinto an English sentence such as(H0) he read the book because he wasinterested in world historyin which the cause and the effect are swapped.
Whydoes this happen?
The former half of (J0) means ?Heread the book,?
and the latter half means ?
(he) wasinterested in world history.?
The middle word?node?
between them corresponds to ?because.
?Therefore, SMT systems output sentences like (H0).On the other hand, Rule-based Machine Translation(RBMT) systems correctly give (R0).In order to find (R0), SMT systems have to searcha very large space because we cannot restrict itssearch space with a small distortion limit.
MostSMT systems thus fail to find (R0).Consequently, the global word order is essentialfor translation between distant language pairs, andwrong word order can easily lead to misunderstand-ing or incomprehensibility.
Perhaps, some readersdo not understand why we emphasize word orderfrom this example alone.
A few more exampleswill clarify what happens when SMT is applied toJapanese-to-English translation.
Even the most fa-mous SMT service available on the web failed totranslate the following very simple sentence at thetime of writing this paper.Japanese: meari wa jon wo koroshita.Reference: Mary killed John.SMT output: John killed Mary.Since it cannot translate such a simple sentence, itobviously cannot translate more complex sentencescorrectly.Japanese: bobu ga katta hon wo jon wa yonda.Reference: John read a book that Bob bought.SMT output: Bob read the book John bought.Another example is:Japanese: bobu wa meari ni yubiwa wo kautameni, jon no mise ni itta.Reference: Bob went to John?s store to buy aring for Mary.SMT output: Bob Mary to buy the ring, Johnwent to the store.In this way, this SMT service usually gives incom-prehensible or misleading translations, and thus peo-ple prefer RBMT services.
Other SMT systems alsotend to make similar word order mistakes, and spe-cial care should be paid to the translation betweendistant language pairs such as Japanese and English.Even Japanese people cannot solve this word or-der problem easily: It is well known that Japanesepeople are not good at speaking English.From this point of view, conventional automaticevaluation metrics of translation quality disregardword order mistakes too much.
Single-referenceBLEU is defined by a geometrical mean of n-gramprecisions pn and is modified by Brevity Penalty(BP) min(1, exp(1?
r/h)), where r is the length ofthe reference and h is the length of the hypothesis.BLEU = BP?
(p1p2p3p4)1/4.Its range is [0, 1].
The BLEU score of (H0) with ref-erence (R0) is 1.0?
(11/11?9/10?6/9?4/8)1/4 =0.740.
Therefore, BLEU gives a very good score tothis inadequate translation because it checks only n-grams and does not regard global word order.Since (R0) and (H0) look similar in terms of flu-ency, adequacy is more important than fluency inthe translation between distant language pairs.Similarly, other popular scores such as NIST,PER, and TER (Snover et al, 2006) also giverelatively good scores to this translation.
NISTalso considers only local word orders (n-grams).PER (Position-Independent Word Error Rate) wasdesigned to disregard word order completely.TER (Snover et al, 2006) was designed to allowphrase movements without large penalties.
There-fore, these standard metrics are not optimal for eval-uating translation between distant language pairs.In this paper, we propose an alternative automaticevaluation metric appropriate for distant languagepairs.
Our method is based on rank correlation co-efficients.
We use them to compare the word ranksin the reference with those in the hypothesis.There are two popular rank correlation coeffi-cients: Spearman?s ?
and Kendall?s ?
(Kendall,1975).
In Isozaki et al (2010), we used Kendall?s ?to measure the effectiveness of our Head Finaliza-tion rule as a preprocessor for English-to-Japanesetranslation, but we measured the quality of transla-tion by using conventional metrics.945It is not clear how well ?
works as an automaticevaluation metric of translation quality.
Moreover,Spearman?s ?
might work better than Kendall?s ?
.As we discuss later, ?
considers only the directionof the rank change, whereas ?
considers the distanceof the change.The first objective of this paper is to examinewhich is the better metric for distant language pairs.The second objective is to find improvements ofthese rank correlation-metrics.Spearman?s ?
is based on Pearson?s correlationcoefficients.
Suppose we have two lists of numbersx = [0.1, 0.4, 0.2, 0.6],y = [0.9, 0.6, 0.2, 0.7].To obtain Pearson?s coefficients between x and y,we use the raw values in these lists.
If we substitutetheir ranks for their raw values, we getx?
= [1, 3, 2, 4] and y?
= [4, 2, 1, 3].Then, Spearman?s ?
between x and y is given byPearson?s coefficients between x?
and y?.
This ?can be rewritten as follows when there is no tie:?
= 1?
?i d2in+1C3.Here, di indicates the difference in the ranks of thei-th element.
Rank distances are squared in thisformula.
Because of this square, we expect that ?decreases drastically when there is an element thatsignificantly changes in rank.
But we are also afraidthat ?
may be too severe for alternative good trans-lations.Since Pearson?s correlation metric assumes lin-earity, nonlinear monotonic functions can changeits score.
On the other hand, Spearman?s ?
andKendall?s ?
uses ranks instead of raw evaluationscores, and simple application of monotonic func-tions cannot change them (use of other operationssuch as averaging sentence scores can change them).2 Methodology2.1 Word alignment for rank correlationsWe have to determine word ranks to obtain rank cor-relation coefficients.
Suppose we have:(R1) John hit Bob yesterday(H1) Bob hit John yesterdayThe 1st word ?John?
in R1 becomes the 3rd wordin H1.
The 2nd word ?hit?
in R1 becomes the 2ndword in H1.
The 3rd word ?Bob?
in R1 becomes the1st word in H1.
The 4th word ?yesterday?
in R1 be-comes the 4th word in H1.
Thus, we get H1?s wordorder list [3, 2, 1, 4].
The number of all pairs of in-tegers in this list is 4C2 = 6.
It has three increasingpairs: (3,4), (2,4), and (1,4).
Since Kendall?s ?
isgiven by:?
= 2?the number of increasing pairsthe number of all pairs?
1,H1?s ?
is 2?
3/6?
1 = 0.0.In this case, we can obtain Spearman?s ?
as fol-lows: ?John?
moved by d1 = 2 words, ?hit?
movedby d2 = 0 words, ?Bob?
moved by d3 = 2 words,and ?yesterday?
moved by d4 = 0 words.
Therefore,H1?s ?
is 1?
(22 + 02 + 22 + 02)/5C3 = 0.2.Thus, ?
considers only the direction of the move-ment, whereas ?
considers the distance of the move-ment.
Both ?
and ?
have the same range [?1, 1].
Themain objective of this paper is to clarify which rankcorrelation is closer to human evaluation scores.We have to consider the limitation of the rank cor-relation metrics.
They are defined only when thereis one-to-one correspondence.
However, a refer-ence sentence and a hypothesis sentence may havedifferent numbers of words.
They may have two ormore occurrences of the same word in one sentence.Sometimes, a word in the reference does not appearin the hypothesis, or a word in the hypothesis doesnot appear in the reference.
Therefore, we cannotcalculate ?
and ?
following the above definitions ingeneral.Here, we determine the correspondence of wordsbetween hypotheses and references as follows.
First,we find one-to-one corresponding words.
That is,we find words that appear in both sentences and onlyonce in each sentence.
Suppose we have:(R2) the boy read the book(H2) the book was read by the boyBy removing non-aligned words by one-to-one cor-respondence, we get:946(R3) boy read book(H3) book read boyThus, we lost ?the.?
We relax this one-to-one cor-respondence constraint by using one-to-one corre-sponding bigrams.
(R2) and (H2) share ?the boy?and ?the book,?
and we can align these instances of?the?
correctly.
(R4) the1 boy2 read3 the4 book5(H4) the4 book5 read3 the1 boy2Now, we have five aligned words, and H4?s wordorder is represented by [4, 5, 3, 1, 2].In returning to H0 and R0, we find that each ofthese sentences has eleven words.
Almost all wordsare aligned by one-to-one correspondence but ?he?is not aligned because it appears twice in each sen-tence.
By considering one-to-one corresponding bi-grams (?he was?
and ?he read?
), ?he?
is aligned asfollows.
(R5) he1 was2 interested3 in4 world5history6 because7 he8 read9 the10book11(H5) he8 read9 the10 book11 because7he1 was2 interested3 in4 world5history6H5?s word order is [8, 9, 10, 11, 7, 1, 2, 3, 4, 5, 6].The number of increasing pairs is: 4C2 = 6 pairs in[8, 9, 10, 11] and 6C2 = 15 pairs in [1, 2, 3, 4, 5,6].
Then we obtain ?
= 2 ?
(6 + 15)/11C2 ?
1 =?0.236.
On the other hand,?i d2i = 52 ?
6 + 22 +72 ?
4 = 350, and we obtain ?
= 1 ?
350/12C3 =?0.591.Therefore, both Spearman?s ?
and Kendall?s ?give very bad scores to the misleading translationH0.
This fact implies they are much better metricsthan BLEU, which gave a good score to it.
?
is muchlower than ?
as we expected.In general, we can use higher-order n-grams forthis alignment, but here we use only unigrams andbigrams for simplicity.
This algnment algorithm isgiven in Figure 1.
Since some hypothesis words donot have corresponding reference words, the outputinteger list worder is sometimes shorter than theevaluated sentence.
Therefore, we should not useworder[i] ?
i as di directly.
We have to renumberthe list by rank as we did in Section 1.Read a hypothesis sentence h = h1h2 .
.
.
hmand its reference sentence r = r1r2 .
.
.
rn.Initialize worder with an empty list.For each word hi in h:?
If hi appears only once each in h and r, append js.t.
rj = hi to worder.?
Otherwise, if the bigram hihi+1 appears only onceeach in h and r, append j s.t.
rjrj+1 = hihi+1 toworder.?
Otherwise, if the bigram hi?1hi appears only onceeach in h and r, append j s.t.
rj?1rj = hi?1hi toworder.Return worder.Figure 1: Word alignment algorithm for rank correlation2.2 Word order metrics and meta-evaluationmetricsThese rank correlation metrics sometimes have neg-ative values.
In order to make them just like otherautomatic evaluation metrics, we normalize them asfollows.?
Normalized Kendall?s ?
: NKT = (?
+ 1)/2.?
Normalized Spearman?s ?
: NSR = (?+ 1)/2.Accordingly, NKT is 0.382 and NSR is 0.205.These metrics are defined only when the numberof aligned words is two or more.
We define bothNKT and NSR as zero when the number is one orless.
Consequently, these normalized metrics havethe same range [0, 1].In order to avoid confusion, we use these abbre-viations (NKT and NSR) when we use rank corre-lations as word order metrics, because these cor-relation metrics are also used in the machine trans-lation community for meta-evaluation.
For meta-evaluation, we use Spearman?s ?
and Pearson?s cor-relation coefficient and call them ?Spearman?
and?Pearson,?
respectively.2.3 Overestimation problemSince we measure the rank correlation of only cor-responding words, these metrics will overestimatethe correlation.
For instance, a hypothesis sentencemight have only two corresponding words among9470 0.2 0.4 0.6 0.8 1.000.20.40.60.81.0BP (brevity penalty)normalizedaverageadequacy?
????
???????????????
???????????????????
?????????????
?????????
?
??
?
?
??????
??????
????
??????????????
?????????????????????????????????
???????
????????????
???
????????????????????????????????????????
????????????????????????????
?0 0.2 0.4 0.6 0.8 1.000.20.40.60.81.0P (precision)normalizedaverageadequacy????????????????????????
????????????
??????????????????????????????????????????????????????
??????????????????????????????????????
????
?????????????????????????
??????????????
?????????????????
??
??????????????????????????????????????????????????????
?????????????????
?????????????????
??????????????????????????????????????
??????
??????????????????????????
?????
???????
?Figure 2: Scatter plots of normalized average adequacy with brevity penalty (left) and precision (right).
(Each ?
corresponds to one sentence generated by one MT system)dozens of words.
In this case, these two wordsdetermine the score of the whole sentence.
If thetwo words appear in their order in the reference,the whole sentence obtains the best score, NSR =NKT = 1.0, in spite of the fact that only two wordsmatched.Solving this overestimation problem is the secondobjective of this paper.
BLEU uses ?Brevity Penalty(BP)?
(Section 1) to reduce the scores of too-shortsentences.
We can combine the above word ordermetrics with BP, e.g., NKT?
BP and NSR?
BP.However, we cannot very much expect from thissolution because BP scores do not correlate withhuman judgments well.
The left graph of Figure2 shows a scatter plot of BP and ?normalized av-erage adequacy.?
This graph has 15 (systems) ?100 (sentences) dots.
Each dot (?)
corresponds toone sentence from one translation system.In the NTCIR-7 data, three human judges gavefive-point scores (1, 2, 3, 4, 5) for ?adequacy?
and?fluency?
of each translated sentence.
Althougheach system translated 1,381 sentences, only 100sentences were evaluated by the judges.For each translated sentence, we averaged threejudges?
adequacy scores and normalized this aver-age x by (x?1)/4.
This is our ?normalized averageadequacy,?
and the dots appears only at multiples of1/3?
1/4.This graph shows that BP has very little correla-tion with adequacy, and we cannot expect BP to im-prove the meta-evaluation performance very much.Perhaps, BP?s poor performance was caused by thefact that most MT systems output almost the samenumber of words, and if the number exceeds thelength of the reference, BP=1.0 holds.Therefore, we have to consider other modifiersfor this overestimation problem.
We can use othercommon metrics such as precision, recall, and F-measure to reduce the overestimation of NSR andNKT.?
Precision: P = c/|h|, where c is the number ofcorresponding words and |h| is the number ofwords in the hypothesis sentence h.?
Recall: R = c/r, where |r| is the number ofwords in the reference sentence r.?
F-measure: F?
= (1 + ?2)PR/(?2P + R),where ?
is a parameter.In (R2)&(H2)?s case, precision is 5/7 = 0.714 andrecall is 5/5 = 1.000.Which metric should we use?
Our preliminaryexperiments with NTCIR-7 data showed that preci-sion correlated best with adequacy among thesethree metrics (P , R, and F?=1).
In addition, BLEUis essentially made for precision.
Therefore, preci-sion seems the most promising modifier.The right graph of Figure 2 shows a scatter plotof precision and normalized average adequacy.
Thegraph shows that precision has more correlation withadequacy than BP.
We can observe that sentenceswith very small P values usually obtain very lowadequacy scores but those with mediocre P valuesoften obtain good adequacy scores.948If we multiply P directly by NSR or NKT, thosesentences with mediocre P values will lose toomuch of their scores.
The use of?x will miti-gate this problem.
Since?P is closer to 1.0 thanP itself, multiplication of?P instead of P itselfwill save these sentences.
If we apply?x twice(?
?P = 4?P ), it will further save them.
There-fore, we expect?
?P and?
4?P to work better than?P .
Now, we propose two new metrics:NSRP?
and NKTP?,where ?
is a parameter (0 ?
?
?
1).3 Experiments3.1 Meta-evaluation with NTCIR-7 dataIn order to compare automatic translation evalua-tion methods, we use submissions to the NTCIR-7Patent Translation (PATMT) task (Fujii et al, 2008).Fourteen MT systems participated in the Japanese-English intrinsic evaluation.
There were two Rule-Based MT (RMBT) systems and one Example-based MT (EBMT) system.
All other systems wereStatistical MT (SMT) systems.
The task organiz-ers provided a baseline SMT system.
These 15 sys-tems translated 1,381 Japanese sentences into En-glish.
The organizers evaluated these translations byusing BLEU and human judgments.
In the humanjudgements, three experts independently evaluated100 selected sentences in terms of ?adequacy?
and?fluency.
?For automatic evaluation, we used a single refer-ence sentence for each of these 100 manually evalu-ated sentences.
Echizen-ya et al (2009) used multi-reference data, but their data is not publicly availableyet.For this meta-evaluation, we measured thecorpus-level correlation between the human evalua-tion scores and the automatic evaluation scores.
Wesimply averaged scores of 100 sentences for the pro-posed metrics.
For existing metrics such as BLEU,we followed their definitions for corpus-level eval-uation instead of simple averages of sentence-levelscores.
We used default settings for conventionalmetrics, but we tuned GTM (Melamed et al, 2007)with -e option.
This option controls preferenceson longer word runs.
We also used the para-phrase database TERp (http://www.umiacs.umd.edu/?snover/terp) for METEOR (Banerjee andLavie, 2005).3.2 Meta-evaluation with WMT-07 dataWe developed our metric mainly for automatic eval-uation of translation quality for distant languagepairs such as Japanese-English, but we also wantto know how well the metric works for similar lan-guage pairs.
Therefore, we also use the WMT-07 data (Callison-Burch et al, 2007) that coversonly European language pairs.
Callison-Burch et al(2007) tried different human evaluation methods andshowed detailed evaluation scores.
The Europarl testset has 2,000 sentences, and The News Commentarytest set has 2,007 sentences.This data has different language pairs: Spanish,French, German ?
English.
We exclude Czech-English because there were so few systems (See thefootnote of p. 146 in their paper).4 Results4.1 Meta-evaluation with NTCIR-7 dataTable 1 shows the main results of this paper.
Theleft part has corpus-level meta-evaluation with ade-quacy.
Error metrics, WER, PER, and TER, havenegative correlation coefficients, but we did notshow their minus signs here.Both NSR-based metrics and NKT-based metricsperform better than conventional metrics for this NT-CIR PATMT JE translation data.
As we expected,?BP and ?P (1/1) performed badly.
Spearman ofBP itself is zero.NKT performed slightly better than NSR.
Per-haps, NSR penalized alternative good translationstoo much.
However, one of the NSR-based metrics,NSRP 1/4, gave the best Spearman score of 0.947,and the difference between NSRP?
and NKTP?was small.
Modification with P led to this improve-ment.NKT gave the best Pearson score of 0.922.
How-ever, Pearson measures linearity and we can changeits score through a nonlinear monotonic functionwithout changing Spearman very much.
For in-stance, (NSRP 1/4)1.5 also has Spearman of 0.947but its Pearson is 0.931, which is better than NKT?s0.922.
Thus, we think Spearman is a better meta-evaluation metric than Pearson.949Table 1: NTCIR-7 Meta-evaluation: correlation with hu-man judgments (Spm = Spearman, Prs = Pearson)human judge Adequacy Fluencyeval\ meta-eval Spm Prs Spm PrsP 0.615 0.704 0.672 0.876R 0.436 0.669 0.461 0.854F?=1 0.525 0.692 0.543 0.871BP 0.000 0.515 -0.007 0.742NSR 0.904 0.906 0.869 0.910NSRP 1/8 0.937 0.905 0.890 0.934NSRP 1/4 0.947 0.900 0.901 0.944NSRP 1/2 0.937 0.890 0.926 0.949NSRP 1/1 0.883 0.872 0.883 0.939NSR ?
BP 0.851 0.874 0.769 0.910NKT 0.940 0.922 0.887 0.931NKTP 1/8 0.940 0.913 0.908 0.944NKTP 1/4 0.940 0.904 0.908 0.949NKTP 1/2 0.929 0.890 0.897 0.949NKTP 1/1 0.897 0.869 0.879 0.936NKT ?
BP 0.829 0.878 0.726 0.918ROUGE-L 0.903 0.874 0.889 0.932ROUGE-S(4) 0.593 0.757 0.640 0.869IMPACT 0.797 0.813 0.751 0.932WER 0.894 0.822 0.836 0.926TER 0.854 0.806 0.372 0.856PER 0.375 0.642 0.393 0.842METEOR(TERp) 0.490 0.708 0.508 0.878GTM(-e 12) 0.618 0.723 0.601 0.850NIST 0.343 0.661 0.372 0.856BLEU 0.515 0.653 0.500 0.795The right part of Table 1 shows correlation withfluency, but adequacy is more important, becauseour motivation is to provide a metric that is useful toreduce incomprehensible or misunderstanding out-puts of MT systems.
Again, the correlation-basedmetrics gave better scores than conventional metrics,and BP performed badly.
NSR-based metrics provedto be as good as NKT-based metrics.Meta-evaluation scores of the de facto standardBLEU is much lower than those of other metrics.Echizen-ya et al (2009) reported that IMPACT per-formed very well for sentence-level evaluation ofNTCIR-7 PATMT JE data.
This corpus-level resultalso shows that IMPACT works better than BLEU,but ROUGE-L, WER, and our methods give betterscores than IMPACT.Table 2: WMT-07 meta-evaluation: Each source lan-guage has two columns: the left one is News Corpus andthe right one is Europarl.Spearman?s ?
with human ?rank?source French Spanish GermanNSR 0.775 0.837 0.523 0.766 0.700 0.593NSRP 1/8 0.821 0.857 0.786 0.595 0.400 0.685NSRP 1/4 0.821 0.857 0.786 0.455 0.400 0.714NSRP 1/2 0.821 0.857 0.786 0.347 0.400 0.714NKT 0.845 0.857 0.607 0.838 0.700 0.630NKTP 1/8 0.793 0.857 0.786 0.595 0.400 0.714NKTP 1/4 0.793 0.857 0.786 0.524 0.400 0.714NKTP 1/2 0.793 0.857 0.786 0.347 0.400 0.714BLEU 0.786 0.679 0.750 0.595 0.400 0.821WER 0.607 0.857 0.750 0.429 0.000 0.500ROUGEL 0.893 0.739 0.786 0.707 0.700 0.857ROUGES 0.883 0.679 0.786 0.690 0.400 0.9294.2 Meta-evaluation with WMT-07 dataCallison-Burch et al (2007) have performed differ-ent human evaluation methods for different languagepairs and different corpora.
Their Table 5 showsinter-annotator agreements for the human evaluationmethods.
According to their table, the ?sentenceranking?
(or ?rank?)
method obtained better agree-ment than ?adequacy.?
Therefore, we show Spear-man?s ?
for ?rank.?
We used the scores given intheir Tables 9, 10, and 11.
(The ?constituent?
meth-ods obtained the best inter-annotator agreement, butthese methods focus on local translation quality andhave nothing to do with global word order, which weare discussing here.
)Table 2 shows that our metrics designed fordistant language pairs are comparable to conven-tional methods even for similar language pairs, butROUGE-L and ROUGE-S performed better thanours for French News Corpus and German Europarl.BLEU scores in this table agree with those in Table17 of Callison-Burch et al (2007) within roundingerrors.After some experiments, we noticed that the useofR instead of P often gives better scores for WMT-07, but it degrades NTCIR-7 scores.
We can extendour metric by F?
, weighted harmonic mean of P andR, or any other interpolation, but the introductionof new parameters into our metric makes it difficult950to control.
Improvement without new parameters isbeyond the scope of this paper.5 DiscussionIt has come to our attention that Birch et al (2010)has independently proposed an automatic evaluationmethod based on Kendall?s ?
.
First, they startedwith Kendall?s ?
distance, which can be written as?1?NKT?
in our terminology, and then subtractedit from one.
Thus, their metric is nothing but NKT.Then, they proposed application of the square rootto get better Pearson by improving ?the sensitivityto small reorderings.?
Since they used ?Kendall?s ?
?and ?Kendall?s ?
distance?
interchangeably, it is notclear what they mean by ?
?Kendall?s ?
,?
but per-haps they mean 1 ?
?1?NKT because?NKT ismore insensitive to small reorderings.
Table 3 showsthe performance of these metrics for NTCIR-7 data.Pearson?s correlation coefficient with adequacy wasimproved by 1 ??1?
NKT, but other scores weredegraded in this experiment.The difference between our method and Birch etal.
(2010)?s method comes from the fact that weused Japanese-English translation data and Spear-man?s correlation for meta-evaluation, whereas theyused Chinese-English translation data and only Pear-son?s correlation for meta-evaluation.
Chinese wordorder is different from English, but Chinese is aSubject-Verb-Object (SVO) language and thus ismuch closer to English word order than Japanese,a typical SOV language.We preferred NSR because it penalizes globalword order mistakes much more than does NKT, andas discussed above, global word order mistakes of-ten lead to incomprehensibility and misunderstand-ing.On the other hand, they also tried Hamming dis-tance, and summarized their experiments as follows:However, the Hamming distance seems tobe more informative than Kendall?s tau forsmall amounts of reordering.This sentence and the introduction of the square rootto NKT imply that Chinese word order is close tothat of English, and they have to measure subtleword order mistakes.Table 3: NTCIR-7 meta-evaluation: Effects of squareroot (b(x) = 1??1?
x)NKT?NKT b(NKT)Spearman w/ adequacy 0.940 0.940 0.922Pearson w/ adequacy 0.922 0.817 0.941Spearman w/ fluency 0.887 0.865 0.858Pearson w/ fluency 0.931 0.917 0.833In spite of these differences, the two groups inde-pendently recognized the usefulness of rank correla-tions for automatic evaluation of translation qualityfor distant language pairs.In their WMT-2010 paper (Birch and Osborne,2010), they multiplied NKT with the brevity penaltyand interpolated it with BLEU for the WMT-2010shared task.
This fact implies that incomprehensibleor misleading word order mistakes are rare in trans-lation among European languages.6 ConclusionsWhen Statistical Machine Translation is applied todistant language pairs such as Japanese and English,word order becomes an important problem.
SMTsystems often fail to find an appropriate translationbecause of a large search space.
Therefore, theyoften output misleading or incomprehensible sen-tences such as ?A because B?
vs. ?B because A.?
Topenalize such inadequate translations, we presentedan automatic evaluation method based on rank corre-lation.
There were two questions for this approach.First, which correlation coefficient should we use:Spearman?s ?
or Kendall?s ??
Second, how shouldwe solve the overestimation problem caused by thenature of one-to-one correspondence?We answered these questions through our exper-iments using the NTCIR-7 PATMT JE translationdata.
For the first question, ?
was slightly betterthan ?, but ?
was improved by precision.
For thesecond question, it turned out that BLEU?s BrevityPenalty was counter-productive.
A precision-basedpenalty gave a better solution.
With this precision-based penalty, both ?
and ?
worked well and theyoutperformed conventional methods for NTCIR-7data.
For similar language pairs, our method wascomparable to conventional evaluation methods.
Fu-951ture work includes extension of the method so that itcan outperform conventional methods even for sim-ilar language pairs.ReferencesSatanjeev Banerjee and Alon Lavie.
2005.
Meteor:An automatic metric for MT evaluation with improvedcorrelation with human judgements.
In Proc.
of ACLWorkshop on Intrinsic and Extrinsic Evaluation Mea-sures for MT and Summarization, pages 65?72.Alexandra Birch and Miles Osborne.
2010.
LRscore forevaluating lexical and reordering quality in MT.
InProceedings of the Joint Fifth Workshop on StatisticalMachine Translation and MetricsMATR, pages 327?332.Alexandra Birch, Miles Osborne, and Phil Blunsom.2010.
Metrics for MT evaluation: evaluating reorder-ing.
Machine Translation, 24(1):15?26.Chris Callison-Burch, Miles Osborne, and PhilippKoehn.
2006.
Re-evaluatiing the role of Bleu in ma-chine translation research.
In Proc.
of the Conferenceof the European Chapter of the Association for Com-putational Linguistics, pages 249?256.Chris Callison-Burch, Cameron Fordyce, PhilippKoehn, Chrstof Monz, and Josh Schroeder.
2007.
(Meta-)Evaluation of machine translation.
In Proc.
ofthe Workshop on Machine Translation (WMT), pages136?158.Etienne Denoual and Yves Lepage.
2005.
BLEU in char-acters: towards automatic MT evaluation in languageswithout word delimiters.
In Companion Volume to theProceedings of the Second International Joint Confer-ence on Natural Language Processing, pages 81?86.Hiroshi Echizen-ya and Kenji Araki.
2007.
Automaticevaluation of machine translation based on recursiveacquisition of an intuitive common parts continuum.In Proceedings of MT Summit XII Workshop on PatentTranslation, pages 151?158.Hiroshi Echizen-ya, Terumasa Ehara, Sayori Shimohata,Atsushi Fujii, Masao Utiyama, Mikio Yamamoto,Takehito Utsuro, and Noriko Kando.
2009.
Meta-evaluation of automatic evaluation methods for ma-chine translation using patent translation data in ntcir-7.
In Proceedings of the 3rd Workshop on PatentTranslation, pages 9?16.Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, andTakehito Utsuro.
2008.
Overview of the patenttranslation task at the NTCIR-7 workshop.
In Work-ing Notes of the NTCIR Workshop Meeting (NTCIR),pages 389?400.Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, andKevin Duh.
2010.
Head Finalization: A simple re-ordering rule for SOV languages.
In Proceedings ofthe Joint Fifth Workshop on Statistical Machine Trans-lation and MetricsMATR, pages 250?257.Maurice G. Kendall.
1975.
Rank Correlation Methods.Charles Griffin.Chin-Yew Lin and Eduard Hovy.
2003.
Automatic evalu-ation of summaries using n-gram co-occurrence statis-tics.
In Proc.
of the North American Chapter of theAssociation of Computational Linguistics (NAACL),pages 71?78.Dan Melamed, Ryan Green, and Joseph P. Turian.
2007.Precision and recall of machine translation.
In Proc.of NAACL-HLT, pages 61?63.Kishore Papineni, Salim Roukos, Todd Ward, John Hen-derson, and Florence Reeder.
2002a.
Corpus-basedcomprehensive and diagnostic MT evaluation: InitialArabic, Chinese, French, and Spanish Results.
InProc.
of the International Conference on Human Lan-guage Technology Research (HLT), pages 132?136.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002b.
BLEU: a method for automatic eval-uation of machine translation.
In Proc.
of the AnnualMeeting of the Association of Computational Linguis-tics (ACL), pages 311?318.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of Association for Machine Translationin the Americas.952
