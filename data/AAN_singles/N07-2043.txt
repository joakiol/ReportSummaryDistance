Proceedings of NAACL HLT 2007, Companion Volume, pages 169?172,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsA High Accuracy Method for Semi-supervised Information ExtractionStephen Tratz Antonio SanfilippoPacific Northwest National Laboratory Pacific Northwest National LaboratoryRichland, WA 99352 Richland, WA 99352stephen.tratz@pnl.gov antonio.sanfilippo@pnl.govAbstractCustomization to specific domains of dis-course and/or user requirements is one ofthe greatest challenges for today?s Infor-mation Extraction (IE) systems.
Whiledemonstrably effective, both rule-basedand supervised machine learning ap-proaches to IE customization pose toohigh a burden on the user.
Semi-supervised learning approaches may inprinciple offer a more resource effectivesolution but are still insufficientlyaccurate to grant realistic application.
Wedemonstrate that this limitation can beovercome by integrating fully-supervisedlearning techniques within a semi-supervised IE approach, withoutincreasing resource requirements.1 IntroductionCustomization to specific discourse domainsand/or user requirements is one of the greatestchallenges for today?s Information Extraction (IE)systems.
While demonstrably effective, both rule-based and supervised machine learning approachesto IE customization require a substantialdevelopment effort.
For example, Aone andRamos-Santacruz (2000) present a rule-based IEsystem which handles 100 types of relations andevents.
Building such a system requires the manualconstruction of numerous extraction patternssupported by customized ontologies.
Soderland(1999) uses supervised learning to induce a set ofrules from hand-tagged training examples.
WhileSonderland suggests that the human effort can bereduced by interleaving learning and manualannotation activities, the creation of training dataremains an onerous task.To reduce the knowledge engineering burden onthe user in constructing and porting an IE system,unsupervised learning has been utilized, e.g.
Riloff(1996), Yangarber et al (2000), and Sekine (2006).Banko et al (2007) present a self-supervisedsystem that aims to avoid the manual IEcustomization problem by extracting all possiblerelations of interest from text.
Stevenson andGreenwood (2005) propose a weakly supervisedapproach to sentence filtering that uses semanticsimilarity and bootstrapping to acquire IE patterns.Stevenson?s and Greenwood?s approach providessome of the best available results in weaklysupervised IE to date, with 0.58 F-measure.
Whilevery good, an F-measure of 0.58 does not providesufficient reliability to grant use in a productionsystem.In this paper, we show that it is possible toprovide a significant improvement overStevenson?s and Greenwood?s results, withoutincreasing resource requirements, by integratingfully-supervised learning techniques within aweakly supervised IE approach.1.1 Learning AlgorithmOur method is modeled on the approach developedby Stevenson and Greenwood (2005) but uses adifferent technique for ranking candidate patterns.Stevenson?s and Greenwood?s algorithm takes asdata inputs a small set of initial seed patterns and acorpus of documents, and uses any of severalsemantic similarity measures (Resnik, 1995; Jiangand Conrath, 1997; Patwardhan et al, 2003) toiteratively identify patterns in the document corpus169that bear a strong resemblance to the seed patterns.After each iteration, the top-ranking candidatepatterns are added to the seed patterns andremoved from the corpus.
Our approach differsfrom that of Stevenson and Greenwood in that weuse a supervised classifier to rank candidatepatterns.
This grants our system greater robustnessand flexibility because the weight of classificationfeatures can be automatically determined within asupervised classification approach.In building supervised classifiers to rankcandidate patterns at each iteration, we use bothpositive and negative training examples.
Instead ofcreating manually annotated training examples, wefollow an active learning approach where trainingexamples are automatically chosen by rankingcandidate patterns in terms of cosine similaritywith the seed patterns.
More specifically, weselect patterns that have the lowest similarity withseed patterns to be the negative training examples.We hypothesized that these negative exampleswould contain many of the uninformative featuresoccurring throughout the corpus and that usingthese examples would enable the classifier todetermine that these features would not be useful.The pattern learning approach we proposeincludes the following steps.1.
An unannotated corpus is required as input.For each sentence, a set of features isextracted.
This information becomes Scand, theset of all candidate patterns.2.
The user defines a set of seed patterns, Sseed.These patterns contain features expected to befound in a relevant sentence.3.
The cosine measure is used to determine thedistance between the patterns in Sseed and Scand.The patterns in Scand are then ordered by theirlowest distance to a member of Sseed.4.
The ?
highest ranked patterns in Scand areadded to Spos, the set of positive trainingexamples.5.
Sseed and Sacc are added to Spos.
Sneg, the set ofnegative training examples is constructed from?+iteration*?
of the lowest ranked patterns inScand.
Then, a maximum entropy classifier isbuilt using Spos and Sneg as training data.6.
The classifier is used to score each pattern inScand.
Scand is then sorted by these scores.7.
The top ?
patterns in Scand are added to Sacc andremoved from Scand.8.
If a suitable stopping point has been reached,the process ends.
Otherwise, Spos and Sneg areemptied and the process continues at step 6.We set ?
to 5, ?
to 20, ?
to 15, ?
to 5, and used thefollowing linguistic processing tools: (1) theOpenNLP library (opennlp.sourceforge.net) forsentence splitting and named-entity recognition,and (2) Connexor for syntactic parsing(Tapanainen and J?rvinen, 1997).
For theclassifier, we used the OpenNLP MaxEntimplementation (maxent.sourceforge.net) of themaximum entropy classification algorithm (Bergeret al 1996).
We used the MUC-6 data set as thetesting ground for our proposed approach.1.2 Description of Features UsedStevenson and Greenwood (2005) use subject-verb-object triples for their features.
We use aricher feature set.
Our system can easilyaccommodate more features because we let themaximum entropy classifier determine the weightfor the features.
Stevenson?s and Greenwood?sapproach determines weights using semanticsimilarity and would require significant changes totake into account various other features, especiallythose for which a WordNet (Fellbaum, 1998)similarity score is not available.We use single tokens, token combinations, andsemantic information to inform our IE patternextraction system.
Lexical items marked by thenamed-entity recognition system as PERSON orORGANIZATION are replaced with ?person?
and?organization?, respectively.
Number tokens arereplaced with ?numeric?.
Single Token Featuresinclude:?
All words in the sentence and all hypernyms ofthe first sense of the word with attached part-of-speech?
All words in the sentence with attacheddependency?
The verb base of each nominalization and theverb?s first sense hypernyms are included.Token Combinations include:?
All bigrams from the sentence?
All subject-object pairs?
All parent-child pairs from the parse tree170?
A specially marked copy of the parent-childpairs where the main verb is the parent.We also added semantic features indicating if aPERSON or ORGANIZATION was detectedwithin the sentence boundaries.
Table1 provides anexample where a simple sentence is mapped intothe set of features we have just described.Alan G. Spoon, 42, will succeedMr.
Graham as president of thecompany.Single Token FeaturesWith attached dependencies:attr:person, subj:person, mod:numeric, v-ch:will,main:succeed, obj:person, copred:as, pcomp:president,mod:of, det:the, pcomp:companyWith part-of-speech tags:n:person, v:succeed, v:will, dt:the, n:company,n:institution, n:social_group, n:group, n:organization,n:person, n:president, n:executive, n:corporat-e_executive, n:administrator, n:head, n:leader, n:orga-nism, n:living_thing, n:object, n:entity, num:numeric,abbr:person, prp:as, prp:of, v:control, v:declare,v:decree, v:express, v:ordain, v:preside, v:stateToken CombinationsBigrams:person+comma, comma+numeric, numeric+comma,comma+will, will+succeed, succeed+person, person+as,as+president, president+of, of+the, the+companySubject Object Pairs:sop:person+personParent-Child Pairs:pc:person+person, pc:person+numeric, pc:will+person,pc:succeed+will, pc:succeed+person, pc:succeed+as,pc:as+president, pc:president+of, pc:of+company,pc:company+theMain Verb Parent-Child Pairs:mvpc:succeed+person, mvpc:succeed+will, mv-pc:succeed+asSemantic FeatureshasOrganization, hasPersonTable 1: Feature representation of a simple sentence.The seeds we used are adapted from the seedpatterns employed by Stevenson and Greenwood.As shown in Table 2, only a subset of the featuresdescribed above is used in the seed patterns.2 EvaluationWe used the document collection which wasinitially developed for the Sixth MessageUnderstanding Conference (MUC-6) as groundtruth data set to evaluate our approach.
The MUC-6 corpus (www.ldc.upenn.edu) is composed of 100Wall Street Journal documents written during 1993and 1994.
Our task was to detect sentences whichincluded management succession patterns, such asthose shown in Table 2.1: subj:organization, main:appoint, obj:person, hasPers-on, hasOrganization2: subj:organization, main:elect, obj:person, hasOrgani-zation, hasPerson3: subj:organization, main:promote, obj:person, hasOrg-anization, hasPerson4: subj:organization, main:name, obj:person, hasOrgani-zation, hasPerson5: subj:person, main:resign, hasPerson6: subj:person, main:depart, hasPerson7: subj:person, main:quit, hasPersonTable 2: Feature representation of seed patterns.The version of the MUC-6 corpus produced bySoderland (1999) provided us with a specificationof succession patterns at the sentence level, but asshown in Table 3 did not include the source text.We reconstructed the original text byautomatically aligning the succession patterns inthe sentence structures in Soderland?s version ofthe MUC-6 corpus with the sentences in theoriginal MUC-6 corpus.
This alignment produced aset of 1581 sentences, of which 134 containedsuccession patterns.
@S[{SUBJ  @CN[ FOX ]CN }{VB  NAMED @NAM }{OBJ  @PN[ LUCILLE S. SALHANY ]PN , @PS[CHAIRMAN ]PS OF @CN[ FOX INC. ]CN 'STELEVISION PRODUCTION ARM , }{REL_V  TO SUCCEED @SUCCEED HIM .
}]@S 9301060123-5@@TAGS Succession {PersonIn @PN[ LUCILLE S.SALHANY ]PN}+ {Post @PS[ CHAIRMAN ]PS}+{Org @CN[ FOX INC. ]CN}_  @@COVERED_BY@@ENDTAGSTable 3: Data sample from Soderland test set.As shown in Figure 1, our best score of 0.688 F-measure was obtained on the 36th iteration; at theend of this iteration, our algorithm selected 180sentences including 108 of the sentences thatcontained succession patterns.
This is a significantimprovement over the 0.58 F-measure score171reported by Stevenson and Greenwood (2005) forthe same task.
The use of a supervisedclassification approach to the ranking of candidatepatterns with a richer feature set were the twodeterminant factors in achieving suchimprovement.00.10.20.30.40.50.60.70.80 10 20 30 40 50 60 70 80IterationF-measureFigure 1: Evaluation results with MUC-6 data.3 ConclusionsOur results show a substantial improvement overprevious efforts in weakly supervised IE methods,suggesting that weakly supervised methods can bemade to rival rule-based or fully supervisedapproaches both in resource effectiveness andaccuracy.
We plan to verify the strength of ourapproach evaluating against other ground truth datasets.
We also plan to detail how the variousfeatures in our classification model contribute toranking of candidate patterns.
An additional areaof envisioned improvement regards the use of arandom sub selection of negative candidatepatterns as training samples to counteract thepresence of sentence fragments among low-ranking candidate patterns.
Finally, we intend toevaluate the benefit of having a human in the loopin the first few iterations to filter out patternschosen by the system.ReferencesC.
Aone and M. Ramos-Santacruz.
2000.
REES: ALarge-Scale Relation and Event Extraction System,pages 76-83, In Proceedings of the 6th AppliedNatural Language Processing Conference (ANLP2000), Seattle.M.
Banko, M. J. Cafarella, S. Soderland, M. Broadhead,and O. Etzioni.
2007.
Open Information Extractionfrom the Web.
In Proceedings of the 20thInternational Joint Conference on ArtificialIntelligence (IJCAI 2007).
Hyderabad, India.A.
Berger, S. Della Pietra and V. Della Pietra (1996) AMaximum Entropy Approach to Natural LanguageProcessing.
Computational Linguistics, volume 22,number 1, pages 39-71.C.
Fellbaum, editor.
1998.
WordNet: An ElectronicLexical Database and some of its Applications.
MITPress, Cambridge, MA.J.
Jiang and D. Conrath.
1997.
Semantic similaritybased on corpus statistics and lexical taxonomy.
InProceedings of the 10th International Conference onResearch in Computational Linguistics, Taiwan.S.
Patwardhan, S. Banerjee, and T. Pederson.
2003.Using measures of semantic relatedness for wordsense disambiguation.
In Proceedings of the 4thInternational Conferences on Intelligent TextProcessing and Computational Linguistics, pages241-257, Mexico City.P.
Resnik.
1995.
Using Information Content to evaluateSemantic Similarity in a Taxonomy.
In Proceedingsof the 14th International Joint Conference onArtificial Intelligence (IJCAI-95), pages 448-452,Montreal, Canada.E.
Riloff and R. Jones.
1999.
Learning Dictionaries forInformation Extraction by Multi-level Bootstrapping.In Proceedings of the 16th National Conference onArtificial Intelligence.
Orlando, Florida.S.
Sekine.
2006.
On-Demand Information Extraction.
InProceedings of the COLING/ACL 2006 MainConference Poster Sessions.
Sydney, Australia.S.
Soderland.
1999.
Learning Information ExtractionRules for Semi-structured and free text.
MachineLearning, 31(1-3):233-272.M.
Stevenson and M. A. Greenwood.
2005.
A SemanticApproach to IE Pattern Induction.
In Proceedings ofthe 43rd Annual Meeting of the ACL (ACL 05), AnnArbor, Michigan.P.
Tapanainen and Timo J?rvinen.
1997.
A non-projective dependency parser.
In Proceedings of the5th Conference on Applied Natural LanguageProcessing, pages 64?71, Washington D.C.Association for Computational Linguistics.R.
Yangarber, R. Grishman, P. Tapanainen, and S.Huttunen.
2000.
Automatic acquisition of domainknowledge for information extraction.
InProceedings of the 18th International Conference ofComputational Linguistics (COLING 2002), Taipei.172
