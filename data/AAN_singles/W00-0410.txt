Using Summarization for Automatic Briefing GenerationInderjeet Mani.
Kristian ConcepcionLinda Van GuilderThe MITRE Corporation, W64011493 Sunset Hills RoadReston, VA 22090, USA{imani,kjc9,1cvg}@mitre.orgAbst rac tWe describe a system which automaticallygenerates multimedia briefings from high-level outlines.
The system usessummarization in content selection andcreation, and in helping form a coherentnarrative for the briefing.
The approach doesnot require a domain knowledge base.1 In t roduct ionDocument production is an important function inmany organizations.
In addition to instructionmanuals, reports, courseware, systemdocumentation, etc., briefings are a verycommon type of document product, often usedin slide form as a visual accompaniment to atalk.
Since so much time is spent by so manypeople in producing briefings, often underserious time constraints, any method to reducethe amount of time spent on briefing productioncould yield great gains in productivity.Briefings involve a high degree of condensationof information (e.g., no more than a few points,perhaps bul,leted, per slide), and they typicallycontain multimedia information.
Many briefingshave a stereotypical structure, dictated in part bythe business rules of the organisation.
Forexample, a commander may present a daily orweekly brief to her superiors, which is more inthe nature of a routine update of activities incethe last briefing; or she may provide an actionbrief, which is triggered by a particular situation,and which consists of a situation updatefollowed by arguments recommending aparticular course of action.
Further, the processof constructing a briefing may involve certainstereotypical activities, including cullinginformation from particular sources, such asmessages, news, web pages, previous briefings,etc.
Thus, while part of the briefing content maybe created anew by the briefing author 1, otherparts of the briefing may be constructed fromexisting information sources.
However,information in those sources need notnecessarily be in the same form as needed by thebriefing.All these characteristics of briefings make themattractive as an application of automaticsummarization, which is aimed at producing acondensed, task-tailored representation f salientcontent in information sources.
Often, thebackground information being used in a slide isquite considerable; the author needs to identifywhat's salient, presenting it in a succinct mannerso as to fit on the slide, perhaps creating agraphic or other multimedia clip to do so.Automatic summarization; by definition, has aclear role to play here.
A briefing usuallyinvolves a sequence of slides; as the summarybecomes longer, it needs to form a coherentnarrative, built around the prescribed structure.Finally, a briefing must strive, to the extentpossible, to be persuasive and vivid, so that thepoint gets across.
This in turn presents a furtherchallenge for summarization: the ability togenerate smoothly narrated, coherentsummaries.I The noun "author" is used throughout the paper todesignate a human author._ 99It is therefore worthwhile investigating whethercombining automatic summarization withintelligent multimedia presentation techniquescan make the briefing generation amenable tofull automation.
In other words, the authorshould be able to use a computer program togenerate an initial briefing, which she can thenedit and revise as needed.
The briefing can thenbe presented by the author if desired, or elsedirectly by the computer (particularly useful ifthe briefing is being sent to someone lse).
Thestarting point for this process would be a high- "level outline of the briefing on the part of theauthor.
The outline would include references toparticular information sources that had to be,summarized in particular ways.
If a programwere able to take such outlines and generatebriefings which didn't require extensive post-editing to massage into a state deemedacceptable for the task at hand, the programcould be regarded as a worthwhile time savingtool.2 ApproachOur work forms part of a larger DARPA-fundedproject aimed at improving analysis anddecision-making in crisis situations by providingtools that allow analysts to collaborate todevelop structured arguments in support ofparticular conclusions and to help predict likelyfuture scenarios.
These arguments, along withbackground evidence, are packaged together asbriefing s to high-level decision-makers.
Inleveraging automatic methods along the linessuggested above to generate briefings, ourapproach needs to allow the analyst to take on asmuch of the briefing authoring as she wants to(e.g., it may take time for her to adapt o or trustthe machine, or she may want the machine topresent just part of the briefing).
The analyst'sorganisation usually will instantiate one ofseveral templates dictating the high-levelstructure of a briefing; for example, a briefingmay always have to begin with an executivesummary.
The summarization methods also needto be relatively domain-independent, given thatthe subject matter of crises are somewhatunpredictable; an analyst in a crisis situation islikely to be inundated with large numbers ofcrisis-related news and intelligence r ports frommany different sources.
This means that wecannot require that a domain knowledge base beavailable to help the briefing generation process.Given these task requirements, we have adoptedan approach that is flexible aboutaccommodating different degrees of authorinvolvement, that is relatively neutral about therhetorical theory underlying the briefingstructure (since a template may be provided byothers), and that is domain-independent.
I  ourapproach, the author creates the briefing outline,which is then fleshed out further by the systembased on information i  the outline.
The systemfills out some content by invoking specifiedsummarizers; it also makes decisions, whenneeded, about output media type; it introducesnarrative lements to improve the coherence ofthe briefing; and finally, it assembles the finalpresentation, making decisions about spatiallayout in the process.A briefing is represented asa tree.
The structureof the tree represents he rhetorical structure ofthe briefing.
Each node has a label, which offersa brief textual description of the node.
Each leafnode has an associated goal, which, whenrealized, provides content for that node.
Thereare two kinds of goals: content-level goals andnarrative-level goals.
Content-level goals arealso of two kinds: retrieve goals, which retrieveexisting media objects of a particular type (text,audio, image, audio, video) satisfying somedescription, and create goals, which create newmedia objects of these types using programs(called summarization filters).
Narrative-levelgoals introduce descriptions of content at othernodes: they include captions and running text formedia objects, and segues, which are rhetoricalmoves describing a transition to a node.Ordering relations reflecting temporal andspatial ayout are defined on nodes in the tree.Two coarse-grained relations, seq forprecedence, and par for simultaneity, are used tospecify a temporal ordering on the nodes in thetree.
As an example, temporal constraints for a(tiny) tree of 9 nodes may be expressed as:<ordering> <seq><par>7</par><par>8</par><par>3</par><par>4 5</par><par>6</par>100<par>l 9</par><par>2</par></seq> </ordering>The tree representation, along with the temporalconstraints, can be rendered in text as XML; werefer to the XML representation as a script.
@Player i~UserInterface~ Tem~t~r I Vail dator \[CoCr~ ~C~ound ~tixe utor IXMI1 resentati.on \[~k~ Generator / ,' Brid"mgGeneratorFigure 1: System ArchitectureThe overall architecture of our system is shownin Figure 1, The user creates the briefing outlinein the form of a script, by using a GUI.
Thebriefing generator takes the script as input.
TheScript Validator applies an XML parser to thescript, to check for syntactic orrectness.
It thenbuilds a tree representation for the script, whichrepresents the briefing outline, with temporalconstraints attached to the leaves of the tree.Next, a Content Creator takes the input tree andexpands it by introducing narrative-level goalsincluding segues to content nodes, and rtmningtext and captions describing media objects atcontent nodes.
Running text and short captionsare generated from meta-information associatedwith media objects, by using shallow textgeneration methods (canned text).
The end resultof content selection (which has an XMLrepresentation callod a ground script) is that thecomplete tree has been fully specified, with allthe create and retrieve goals fully specified ,with all the output media types decided.
TheContent Creator is thus responsible for bothcontent selection and creation, in terms of treestructure and node content.Then, a Content Executor executes all the createand retrieve goals.
This is a very simple step,resulting in the generation of all the mediaobjects in the presentation, except for the audiofiles for speech to be synthesized.
Thus, this stepresults in realization of the content at the leavesof the tree.Finally, the Presentation Generator takes thetree which is output from Content Execution,along with its temporal ordering constraints, andgenerates the spatial ayout of the presentation.If no spatial ayout constraints are specified (thedefault is to not specify these), the systemallocates pace using a simple method based onthe temporal layout for nodes which have spatialmanifestations.
Speech synthesis is also carriedout here.
Once the tree is augmented with spatiallayout constraints, it is translated by thePresentation Generator into SMIL 2(Synchronized Multimedia IntegrationLanguage) (SMIL 99), a W3C-developodextension of HTML that can be played bystandard multimedia players (such as Real 3 andGrins 4.
This step thus presents the realizedcontent, synthesizing it into a multimediapresentation laid out spatially and temporally.This particular architecture, driven by the aboveproject requirements, does not use planning asan overall problem-solving strategy, as planningrequires domain knowledge.
It therefore differsfrom traditional intelligent multimediapresentation planners, e.g., (Wahlster etal.
93).Nevertheless, the system does make a number o fintelligent decisions in organizing andcoordinating presentation decisions.
These arediscussed next, after which we turn to the mainpoint of the paper, namely the leveraging ofsummarization in automatic briefing generation.2 h. ttp://www.w3.org/AudioVideo/3 www.real.com4 www.oratrix.com_ J1013 Intelligent Multimedia PresentationGenerationThe author of a briefing may choose to flesh outas little of the tree as desired, with the caveatthat the temporal ordering relations for non-narrative nodes need to be provided by her.When a media object is generated at a node by acreate goal, the running text and captions aregenerated by the system.
The motivation for thisis obvious: when a summarization filter (whichis a program under our control) is generating amedia object, we can often provide sufficientrecta-information about hat object o generate ashort caption and some running text.
By default,all segues and spatial layout relations are alsospecified by the system, so the author does nothave to know about these unless she wants to.Finally, the decision as to when to produceaudio, when not specified by the author, is left tothe system.When summarization filters are used (for creategoals), the media type of the output is specifiedas a parameter to the filter.
This media type maybe converted to some other type by the system,e.g., text to speech conversion using Festival(Taylor et al 98).
By default, all narrative nodesattempt to realize their goals as a speech mediatype, using rules based on text length andtnmcatability to less than 250 bytes to decidewhen to use text-to-speech.
The truncationalgorithm is based on dropping syntacticconstituents, using a method similar to (Mani etal.
99).
Captions are always realized, in addition,as text (i.e., they have a text realization and a.possible audio realization).Spatial layout is decided in the PresentationGenerator, after all the individual media objectsare created along with their temporal constraintsby the Content Executor.
The layout algorithmwalks through the temporal ordering insequence, allocating a segment o each set ofobjects that is designated to occursimultaneously (grouped by par in the temporalconstraints).
Each segment can have up to 4frames, in each of which a media object isdisplayed (thus, no more than 4 media objectscan be displayed at the same time).
Since mediaobjects declared to be simultaneous (using par)in the temporal constraints will go together in aseparate segment, the temporal constraintsdetermine what elements are grouped together ina segment.
The layout within a segment handlestwo special cases.
Captions are placed directlyundemeath their associated media object.Running text, when realized as text, is placedbeside the media object being described, so thatthey are paired together visually.
Thus,coherence of a segment is influenced mainly bythe temporal constraints (which have beenfleshed out by the Content Creator to includenarrative nodes), with further handling of specialcases.
Of course, an individual summarizationfilter may choose to coordinate componentmultimedia objects in particular ways in thecourse of generating a composite multimediaobject.Details such as duration and onset of particularframes are specified in the translation to SMIL.Duration is determined by the number of framespresent in a segment, unless there is an audiomedia object in the segment (this media objectmay have a spatial representation, e.g., as anaudio icon, or it may not).
If an audio mediaobject occurs in a frame, the duration of allmedia objects in that frame is equal to the lengthof all the audio files in the segment.
If there isno audio present in a segment, he duration is otseconds (or has a default value of 5) times thenumber of frames created.4 Summarization FiltersAs mentioned above, create goals are satisfiedby summarization filters, which create newmedia objects ummarizing information sources.These programs are called summarization filtersbecause in the course of condensing information,they take input information and turn it into somemore abstract and useful representation, filteringout unimportant information.
Such filtersprovide a novel way of carrying out contentselection and creation for automatedpresentation generation.Our approach relies on component-basedsoftware composition, i.e., assembly of softwareunits that have contractually specified interfacesthat can be independently deployed and reused.The idea of assembling complex languageprocessing programs out of simpler ones is102hardly new; however, by employing currentindustry standards to specify the interactionbetween the components, we simultaneouslyincrease the robustness of the system, ensure thereusability of individual components and createa more fully plug-and-play capability.
Amongthe core technology standards that support hisplug-and-play component assembly capabilityare (a) Java interfaces, used to specify functionsthat all summarization components mustimplement in order to be used in the system, (b)the JavaBeans standard, which allows theparameters and methods of individualcomponents o be inspected by the system andrevealed to the users (c) the XML markupstandard, which we have adopted as an inter-component communication language.
Usingthese technologies, legacy or third-partysummarizers are incorporated into the system by"wrapping" them so as to meet the interfacespecification of the system.
These technologiesalso make possible a graphical environment toassemble and configure complex summarizationfilters from individual summarizationcomponents.Among the most important wins over thetraditional "piping" approach to filter assemblyis the ability to impose build-time restrictions onthe component assembly, disallowing "illegal"compositions, e.g.
component X cannot provideinput to component Y unless X's output typecorresponds to Y's input type.
Build-timerestrictions uch as these play a clear role inincreasing the overall robustness of the run-timesummarization system.
Another build-time winlies in the ability of JavaBeans to be serialized,i.e., written to disk in such a way as to preserve~he state of its parameters settings, ensuring thatevery component in the system can beconfigured and run at different timesindependently of whether the componentprovides aparameter file facility.Establishing the standard functions required of asummarization filter is challenging on severalfronts.
One class of functions required by theinterface is necessary to handle the technicalitiesof exchanging information between otherwisediscrete components.
This set includesfunctions for discovering a component's inputand output types, for handling messages,exceptions and events passed betweencomponents and for interpreting XML based onone or more system-wide document typedefinitions (DTDs).
The other, more interestingset of functions gets to the core ofsummarization functionality.
Selecting thesefunctions involves identifying parameters likelyto be broadly applicable across most or allsummarizers and finding ways to group themand/or to generalize them.
This is desirable inorder to reduce the burden on the end user ofunderstanding the subtle differences between thevarious settings in the summarizers available toher.An.
example of the difficulty inherent in thisendeavor is provided by the compression(summary length divided by source length) vs.reduction (l's complementof compression) vs.target length paradigm.
Different summarizerswill implement one or more of these.
Thewrapper maps from the high-level interfacefunction, where the application/user can specifyeither compression ortarget length, but not both,to the individual summarizer's representation.Thus, a user doesn't need to know whichrepresentation(s) a particular summarizer usesfor reduction/compression.A vanilla summarization Bean includes thefollowing functionality, which every summarizermust be able to provide methods for:source: documents to be summarized(this can be a single document, or acollection)reduction-rate: either summarysize/source size, or target lengthaudience: user-focused or generic(user-focused requires the specificationof a bag of terms, which can be ofdifferent types)output-type: specific data formats(specified by DTDs)The above are parameters which we expect allsummarizers to support.
More specializedsummarizer beans can be constructed to reflectgroupings of summarizers.
Among otherparameters are output-fluency, which specifieswhether a textual summary is to be made up ofpassages (sentences, paras, blocks), namedentities, lists of words, phrases, or topics, etc.Given that definitions of summarization i more103theoretical terms have not been entirelysatisfactory (Mani 2000), it is worth noting thatthe above vanilla Bean provides an operationaldefinition of what a summarizer is.text, and segues.
The captions and running text,when not provided by the filters, are provided bythe script input.
In the case of retrieve goals, theobjects may not have any meta-information, iwhich case a default caption and running-text isgenerated.
Clearly, a system's explanatorynarrative will be enhanced by the availability ofrich meta-information.The segues are provided by the system.
Forexample, an item with a label "A biography ofbin Laden" could result in a generated segue"Here is a biography of bin Laden".
TheContent Creator, when providing content fornarrative nodes, uses a variety o f  differentcanned text patterns.
For the above example, thepattern would be "Here is @6.label", where 6 isthe number of a non-narrative node, with labelbeing its label.Figure 2: Summarization FilterCompositionIn addition to its practical utility in the ability toassimilate, combine and reuse components indifferent combinations, and to do so within aGUI, this approach is interesting because itallows powerful summarization functions to becreated by composing together simpler tools.
(Note that this is different from automaticallyfinding the best combination, which our systemdoes not address).
For example, Figure 2illustrates a complex filter created by using aGUI to compose together a named entityextractor, a date extractor, a component whichdiscovers significant associations between thetwo and writes the result to a table, and avisualizer which plots the results as a graph.
Theresulting summarizer takes in a large collectionof documents, and produces as a summary agraph (a jpeg) of salient named entity mentionsover time.
Each of its components can be easilyreused within the filter composition system tobuild other summarizers.5 Narrative SummarizationPeru Action Brief1 Preamble2 Situation Assessment2.1 Chronology of Events2.1.2 Late st document summarycreate C'summarize -generic-compression.
1 ~peru~p32")2.2 Biographies2.2.1 Biography of Victor Polay2.2.1.1 Picture of @2.2.2.perconretrieve("\]) Arawdata~,polay.jpg ")2.2.1.2 Biography of @~2.2.2.personcreate("summarize -bio -length 350-span multi -person@_~2.2.2.person - ut table/peru/* ")3 Coda"This briefing has aszessed aspects of thesituation in Peru.
Overall, the crisisappears to be worsening.
"Figure 3: Input ScriptAs mentioned above, the system can construct anarrative to accompany the briefing.
Narrativenodes are generated to cover captions, running104Peru Action Brief1 Preambleaudio -- "ln this briefin~ 1 will go overthe @2.1abel.
This ~?ill cover@2.1.1abel and @,2.
3.1.1aber"2 Situation Assessment2.
l "An overvie~?
of the ~2.2.label"(Meta-2.2)2.2 C-'hfonology of Events2.2.1 audio = "Here is the @2.2.2.laber"(1VIeta- 2.2.2)2.2.2 text = "Latest document summary"audio = text =create ("automatize -generic-compression .1/reru/p32")2.3 Biographies2.3.1 audio ="A profile of @2.
3.2.person"('NIeta-2.3.2)2.3.2 Biography of Victor~olay2.3.2.1 audio = text ="A file photo of@,2.3.2.person"(Meta-2.3.2.2)2.3.2.2 Picture of @,2.&2.personimage =retrie ve("D Arawdata~polay.jpg")2.3.2.3 audio = text ="ProJile of @2.
3.
2.person"(Meta- 2.3.2.3)2.3.2.4 Biography of @2.
3.2.personaudio = text =create(%-ummarize-bio length 350-span multi -person@_r2.Z 2.person -out tab&/rend* ")3 Codaaudio = "This briefing has assesseda~79ect~r of the situation in Peru.
Overall,the crisis appears to be ~orr"ening.
"<seq></seq><par> 1 </par><par>2.2.1 2.2.2</par><par>2.3.1 <lpar><par>2.3.2.1 2.3.2.22.3.2.3 2.3.2.4</par><par~3</par>Figure 4: Ground ScriptAll segue nodes are by default generatedautomatically by the system, based on nodelabels.
We always introduce a segue node at thebeginning of the presentation (called a preamblenode), which provides a segue covering the"crown" of the tree, i.e., all nodes upto aparticular depth d from the root (d=2) aremarked with segue nodes.
A segue node is alsoproduced at the end (called a coda).
(Bothpreamble and segue can of course be specifiedby the author if desired).For introducing intervening segue nodes, we usethe following algorithm based on the distancebetween odes and the height in the tree, Wetraverse the non-narrative l aves of the tree intheir temporal order, evaluating each pair ofadjacent nodes A and B where A precedes Btemporally.
A segue is introduced betweennodes A and B if either (a) the maximum of the2 distances from A and B to their least commonancestor isgreater than 3 nodes or (b) the sum ofthe 2 distances from A and B to the leastcommon ancestor isgreater than 4 nodes.
This isless intrusive than introducing segues at randomor between every pair of successive nodes, andappears to perform better than introducing asegue at each depth of the tree.6 An ExampleWe currently have a working version of thesystem with a variety of different single andmulti-document summarization filters.
Figure 3shows an input script created by an author (thescripts in Figure 3 and 4 are schematicrepresentations of the scripts, rather than the rawXML).
The script includes two create goals, onewith a single-document generic summarizationfilter, the other with a multi-document user-focused summarization filter.
Figure 4 shows theground script which was created automaticallyby the Content Creator component.
Note theaddition of media type specifications, theintroduction of narrative nodes, and theextension of the temporal constraints.
The finalpresentation generated is shown in Figure 5.Here we show screen dumps of the six SMILsegments produced, with the audio if any foreach segment indicated in this paper next to anaudio icon.1057 StatusThe summarization filters have incorporatedseveral summarizers, including some that havebeen evaluated in the DARPA SUMMACconference (Mani et al 99-1).
These carry outboth single-document and multi-documentsummarization, and include a preliminarybiographical summarizer we have developed.The running text for the biography table in thesecond-last segment of Figure 5 is producedfrom meta-information i the table XMLgenerated by the biographical summarizer.
Theproduction method for running text uses cannedtext which should work for any input tableconforming to that DTD.The summarization filters are.
being tested aspart of a DARPA situated test with end-users.The briefing generator itself has been usedinternally to generate numerous briefings, andhas been demonstrated aspart of the DARPAsystem.
We also expect to carry out anevaluation to assess the extent to which theautomation described here provides efficiencygains in briefing production.8 Related WorkThere is a fair amount of work on automaticauthoring of multimedia presentations, e.g.,(Wahlster et al 93), (Dalai et al 96), (Mittal etal.
95), (Andre and Rist 97) 5.
These effortsdiffer from ours in two ways: first, unlike us,they are not open-domain; and, second, theydon't use summarization components.
Whilesuch efforts are extremely sophisticatedcompared to us in multimedia presentationplanning and fine-grained coordination andsynchronization capabilities, many of thecomponents used in those efforts are clearlyapplicable to our work.
For example, (Andre andRist 96) include methods for leveraging lifelikecharacters in this process; these characters canbe leveraged in our work as well, to helppersonify the computer narrator.
In addition, ourcaptions, which are very short, rely on cannedtext based on node labels in the initial script, orbased on shallow meta-information generated bythe summarization filter (in XML) along withthe created media object.
(Mittal e t  al.
95)describe avariety of strategies for generation oflonger, more explanatory captions, some ofwhich may be exploited in our work bydeepening the level of recta-information, at leastfor summarization components developed by us.In our ability to leverage automaticsummarization, our work should be clearlydistinguished from work which attempts toformat a summary (from an XMLrepresentation) into something akin to aPowerpoint briefing, e.g., (Nagao and Hasida98).
Our work, by contrast, is focused on usingsummarization i  generating briefings from anabstract outline.9 ConclusionWe have described methods for leveragingautomatic summarization in the automaticgeneration of multimedia briefings.
This workhas taken an open-domain approach, in order tomeet the requirements of the DARPAapplication we are involved with.
We believethere is a stronger role that NL generation canplay in the narrative aspects of our briefings,which currently rely for the most part on cannedtext.
Our future work on description merging inbiographical summaries, and on introducingreferring expressions into the narrative nodes,would in effect ake advantage of more powerfulgeneration methods, without sacrificing open-domain capabilities.
This may require muchricher meta-information specifications than theones we currently use.Finally, we have begun the design of the ScriptCreator GUI (the only component in Figure lremaining to be built).
This will allow the authorto create scripts for the briefing generator(instead of editing templates by hand), by layingout icons for media objects in temporal order.
Auser will be able to select a "standard" briefingtemplate from a menu, and then view it in abriefing/template structure ditor.
The user canthen provide content by adding annotations toany node in the briefing template.
The user willhave a choice of saving the edit version intemplate form, or in SMIL or possibly MicrosoftPowerpoint format.106IIIIIiIIIIIIIIIIIIIPeru Act ion  Br ie f  !
;!?
Exeeadv?
Smmmu'yo Hypothes is?
S i tuat ion  Assessmml  :io Ehromdo.e~ o f  \]~','?nls i io B iograph les  :,~?
SWuctm-ed A~,mneats  :~?
.4 Jtentadve V iews  ' i?
Der i s ion ,  ~:i.<e In this briefing I will go over the situationassessment.
This will cover an overview of thechronology of  events and a profile of VictorPolay.
"e Next, a biography of Victor Polay.::.
Here is an overview of the chronology ofevents.I I III I II Illlll iI :  (3qN-  Peruv ian  cebe l~ re leet~e 2 bo , tages  - Dec.  IS~h ~i i3; JUOOUC ZOO hOS~flge~ ~.1~ d tn51cle the  h~ 0 ~' Japeme:~e ::~J t~loan=edor Boc lh l=a kok i ,  vhece  Tupec  Jtz~l~u rebe l= were  ~!Victor Polay, also known as ComandanteRolando, is the Tupac Amaru founder, aPeruvian guerrilla commander, a former ebelleader, and the Tupac Amaru rebels' top leader.He studied in both France and Spain.
His wife isRosa Polay and his mother is Otilia Campos dePolay.
His associates include Alan Garcia.Here is the latest document summary.This briefing has assessed aspects of  thesituation in Peru.
Overall, the crisis appears tobe worsening.Figure 5: Presentation107ReferencesAndre, E. and Rist, T. (1997) Towards a NewGeneration of Hypermedia Systems: ExtendingAutomated Presentation Design for Hypermedia.L.
Dybkjaer, ed., Proceedings of the Third SpokenDialogue and Discourse Workshop, Topics inNatural Interactive Systems 1.
The Maersk Me-Kinney Moiler Institute for ProductionTechnology, Odense University, Denmark, pp.
10-27.Dalai, M., Feiner, S., McKeown, K., Pan, S., Zhou,M., Hollerer, T., Shaw, J., Feng, Y., and Fromer, J.
(1996) Negotiation for Automated Generation ofTemporal MultimediaPresentations.
Proceedingsof ACM Multimedia '96.Mani, 1., Gates, B., and Bloedorn, E. (1999)Improving Summaries by Revising Them.Proceedings of the 37 ~ Annual Meeting of theAssociation for Computational Lihguistics, CollegePark, MD, pp.
558-565.Mani, 1., Firmin, T., House, D., Klein, G., Sundheim,B., and Hirschman, L. (1999) The TIPSTERSUMMA C Text Summarization Evaluation.Proceedings of EACL'99, Bergen, Norway, pp.
77-85.Mani, 1.
(2000)Automatic Text Summarization.
JohnBenjamins Publishing Company.
To appear.Mittal, V., Roth, S., Moore, J., Mattis, J., andCarenini, G. (1995) Generating ExplanatoryCaptions for Information Graphics.
Proceedings ofthe International Joint Conference on ArtificialIntelligence (IJCAr95), pp.
1276-1283.Nagao, K. and K. Hasida, K. (1998) Automatic TextSummarization Based on the Global DocumentAnnotation.
Proceedings of COLING'98, Montreal,pp.
917-921.Power, R. and Scott, D. (1998) Multilingual"Authoring using Feedback Texts.
Proceedings ofCOLING'98, Montreal, pp.
1053-1059.Taylor, P., Black, A., and Caley, R. (1998) Thearchitecture of the Festival Speech SynthesisSystem.
Proceedings of the Third ESCA Workshopon Speech Synthesis, Jenolan Caves, Australia, pp.147-151.Wahlster, W., Andre, E., Finkler, W., Profitlich, H.-J., and Rist, T. (1993) Plan-Based Integration ofNatural Language and Graphics Generation.
AIJournal, 63.108IlIIIIIIIIIIIIIIIII
