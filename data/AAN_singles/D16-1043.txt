Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 447?456,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsComparing Data Sources and Architectures forDeep Visual Representation Learning in SemanticsDouwe Kiela, Anita L. Vero?
and Stephen ClarkComputer LaboratoryUniversity of Cambridgedouwe.kiela,alv34,stephen.clark@cl.cam.ac.ukAbstractMulti-modal distributional models learngrounded representations for improvedperformance in semantics.
Deep visualrepresentations, learned using convolutionalneural networks, have been shown to achieveparticularly high performance.
In this study,we systematically compare deep visualrepresentation learning techniques, exper-imenting with three well-known networkarchitectures.
In addition, we explore thevarious data sources that can be used forretrieving relevant images, showing thatimages from search engines perform as wellas, or better than, those from manually craftedresources such as ImageNet.
Furthermore, weexplore the optimal number of images andthe multi-lingual applicability of multi-modalsemantics.
We hope that these findings canserve as a guide for future research in thefield.1 IntroductionMulti-modal distributional semantics addresses thefact that text-based semantic models, which rep-resent word meanings as a distribution over otherwords (Turney and Pantel, 2010; Clark, 2015), suf-fer from the grounding problem (Harnad, 1990).
Re-cent work has shown that this theoretical motivationcan be successfully exploited for practical gain.
In-deed, multi-modal representation learning leads toimprovements over language-only models in a rangeof tasks, including modelling semantic similarityand relatedness (Bruni et al, 2014; Silberer and La-pata, 2014; Kiela and Bottou, 2014; Lazaridou etal., 2015), improving lexical entailment (Kiela etal., 2015a), predicting compositionality (Roller andSchulte im Walde, 2013), bilingual lexicon induc-tion (Bergsma and Van Durme, 2011), selectionalpreference prediction (Bergsma and Goebel, 2011),linguistic ambiguity resolution (Berzak et al, 2015),visual information retrieval (Bulat et al, 2016) andmetaphor identification (Shutova et al, 2016).Most multi-modal semantic models tend to relyon raw images as the source of perceptual input.Many data sources have been tried, ranging fromimage search engines to photo sharing websites tomanually crafted resources.
Images are retrieved fora given target word if they are ranked highly, havebeen tagged, or are otherwise associated with the tar-get word(s) in the data source.Traditionally, representations for images werelearned through bag-of-visual words (Sivic and Zis-serman, 2003), using SIFT-based local feature de-scriptors (Lowe, 2004).
Kiela and Bottou (2014)showed that transferring representations from deepconvolutional neural networks (ConvNets) yieldmuch better performance than bag-of-visual-wordsin multi-modal semantics.
ConvNets (LeCun et al,1998) have become very popular in recent years:they are now the dominant approach for almostall recognition and detection tasks in the com-puter vision community (LeCun et al, 2015), ap-proaching or even exceeding human performance insome cases (Weyand et al, 2016).
The work byAlex Krizhevsky et al (2012), which won the Im-ageNet Large Scale Visual Recognition Challenge(ILSVRC) (Russakovsky et al, 2015) in 2012, hasplayed an important role in bringing convolutional447AlexNet GoogLeNet VGGNetILSVRC winner 2012 2014 2015Number of layers 7 22 19Number of parameters ?60 million ?6.7 million ?144 millionReceptive field size 11?
11 3?
3 1?
1, 3?
3, 5?
5Fully connected layers Yes No YesTable 1: Network architectures.
Layer counts only include layers with parameters.networks (back) to prominence.
A similar networkwas used by Kiela and Bottou (2014) to obtain highquality image embeddings for semantics.This work aims to provide a systematic compari-son of such deep visual representation learning tech-niques and data sources; i.e.
we aim to answer thefollowing open questions in multi-modal semantics:?
Does the improved performance over bag-of-visual-words extend to different convolu-tional network architectures, or is it specific toKrizhevsky?s AlexNet?
Do others work evenbetter??
How important is the source of images?
Is therea difference between search engines and manu-ally annotated data sources?
Does the numberof images obtained for each word matter??
Do these findings extend to different languagesbeyond English?We evaluate semantic representation quality throughexamining how well a system?s similarity scores cor-relate with human similarity and relatedness judg-ments.
We examine both the visual representationsthemselves as well as the multi-modal representa-tions that fuse visual representations with linguisticinput, in this case using middle fusion (i.e., concate-nation).
To the best of our knowledge, this work isthe first to systematically compare these aspects ofvisual representation learning.2 ArchitecturesWe use the MMFeat toolkit1 (Kiela, 2016) to obtainimage representations for three different convolu-tional network architectures: AlexNet (Krizhevsky1https://github.com/douwekiela/mmfeatet al, 2012), GoogLeNet (Szegedy et al, 2015) andVGGNet (Simonyan and Zisserman, 2014).
Imagerepresentations are turned into an overall word-levelvisual representation by either taking the mean orthe elementwise maximum of the relevant imagerepresentations.
All three networks are trained tomaximize the multinomial logistic regression objec-tive using mini-batch gradient descent with momen-tum:?D?i=1K?k=11{y(i) = k} log exp(?
(k)>x(i))?Kj=1 exp(?
(j)>x(i))where 1{?}
is the indicator function, x(i) and y(i) arethe input and output, respectively.
D is the numberof training examples andK is the number of classes.The networks are trained on the ImageNet classifica-tion task and we transfer layers from the pre-trainednetwork.
See Table 1 for an overview.
In this sec-tion, we describe the network architectures and theirproperties.AlexNet The network by Krizhevsky (2012) intro-duces the following network architecture: first, thereare five convolutional layers, followed by two fully-connected layers, where the final layer is fed intoa softmax which produces a distribution over theclass labels.
All layers apply rectified linear units(ReLUs) (Nair and Hinton, 2010) and use dropoutfor regularization (Hinton et al, 2012).
This net-work won the ILSVRC 2012 ImageNet classifica-tion challenge.
In our case, we actually use theCaffeNet reference model, which is a replication ofAlexNet, with the difference that it is not trainedwith relighting data-augmentation, and that the or-der of pooling and normalization layers is switched(in CaffeNet, pooling is done before normalization,448(a) ImageNet (b) ESP Game dataset(c) Bing (d) Google(e) FlickrFigure 1: Example images for dog and golden retriever from the various data sources.
ImageNet has noimages for dog, with images only at nodes lower in the hierarchy.
ESP does not have images for the goldenretriever tag.instead of the other way around).
While it uses analmost identical architecture, performance of Caf-feNet is slightly better than the original AlexNet.GoogLeNet The ILSVRC 2014 challenge win-ning GoogLeNet (Szegedy et al, 2015) uses ?incep-tion modules?
as a network-in-network method (Linet al, 2013) for enhancing model discriminabilityfor local patches within the receptive field.
It usesmuch smaller receptive fields and explicitly focuseson efficiency: while it is much deeper than AlexNet,it has fewer parameters.
Its architecture consists oftwo convolutional layers, followed by inception lay-ers that culminate into an average pooling layer thatfeeds into the softmax decision (so it has no fullyconnected layers).
Dropout is only applied on thefinal layer.
All connections use rectifiers.VGGNet The ILSVRC 2015 ImageNet classifi-cation challenge was won by VGGNet (Simonyanand Zisserman, 2014).
Like GoogLeNet, it is muchdeeper than AlexNet and uses smaller receptivefields.
It has many more parameters than the othernetworks.
It consists of a series of convolutionallayers followed by the fully connected ones.
Alllayers are rectified and dropout is applied to the firsttwo fully connected layers.These networks were selected because they arevery well-known in the computer vision commu-nity.
They exhibit interesting qualitative differencesin terms of their depth (i.e., the number of layers),the number of parameters, regularization methodsand the use of fully connected layers.
They have allbeen winning network architectures in the ILSVRCImageNet classification challenges.3 Sources of Image DataSome systematic studies of parameters for text-based distributional methods have found that thesource corpus has a large impact on representationalquality (Bullinaria and Levy, 2007; Kiela and Clark,2014).
The same is likely to hold in the case of449Google Bing Flickr ImageNet ESP GameType Search engine Search engine Photo sharing Image database GameAnnotation Automatic Automatic Human Human HumanCoverage Unlimited Unlimited Unlimited Limited LimitedMulti-lingual Yes Yes No No NoSorted Yes Yes Yes No NoTag specificity Unknown Unknown Loose Specific LooseTable 2: Sources of image data.visual representations.
Various sources of imagedata have been used in multi-modal semantics, butthere have not been many comparisons: Bergsmaand Goebel (2011) compare Google and Flickr, andKiela and Bottou (2014) compare ImageNet (Denget al, 2009) and the ESP Game dataset (von Ahn andDabbish, 2004), but most works use a single datasource.
In this study, one of our objectives is to assesthe quality of various sources of image data.
Table2 provides an overview of the data sources, and Fig-ure 1 shows some example images.
We examine thefollowing corpora:Google Images Google?s image search2 resultshave been found to be comparable to hand-craftedimage datasets (Fergus et al, 2005).Bing Images An alternative image search engineis Bing Images3.
It uses different underlying tech-nology from Google Images, but offers the samefunctionality as an image search engine.Flickr Although Bergsma and Goebel (2011) havefound that Google Images works better in one exper-iment, the photo sharing service Flickr4 is an inter-esting data source because its images are tagged byhuman annotators.ImageNet ImageNet (Deng et al, 2009) is a largeontology of images developed for a variety of com-puter vision applications.
It serves as a benchmark-ing standard for various image processing and com-puter vision tasks.
ImageNet is constructed alongthe same hierarchical structure as WordNet (Miller,2https://images.google.com/3https://www.bing.com/images4https://www.flickr.comMEN (3000) SimLex (999)Google 3000 999Bing 3000 999Flickr 3000 999ImageNet 1326 373ESPGame 2927 833Common subset 1310 360Table 3: Coverage on MEN and SimLex for our datasources.1995), by attaching images to the correspondingsynset (synonym set).ESP Game The ESP Game dataset (von Ahn andDabbish, 2004) was constructed through a so-called?game with a purpose?.
Players were matched on-line and had to agree on an appropriate word labelfor a randomly selected image within a time limit.Once a word has been mentioned a certain numberof times, that word becomes a taboo word and canno longer be used as a label.These data sources have interesting qualitativedifferences.
Online services return images for al-most any query, with much better coverage thanthe fixed-size ImageNet and ESP Game datasets.Search engines annotate automatically, while theothers are human-annotated, either through a strictannotation procedure in the case of ImageNet, or byletting users tag images, as in the case of Flickr andESP.
Automatic systems sort images by relevance,while the others are unsorted.
The relevance rank-ing method is not accessible, however, and so hasto be treated as a black box.
Search results can be450Arch.
AlexNet GoogLeNet VGGNetAgg.
Mean Max Mean Max Mean MaxSource Type/Eval SL MEN SL MEN SL MEN SL MEN SL MEN SL MENWikipedia Text .310 .682 .310 .682 .310 .682 .310 .682 .310 .682 .310 .682Google Visual .340 .503 .334 .513 .358 .495 .367 .501 .342 .512 .332 .494MM .380 .711 .370 .719 .379 .711 .365 .716 .380 .714 .365 .716Bing Visual .325 .567 .316 .554 .310 .526 .303 .520 .304 .551 .289 .507MM .373 .727 .360 .725 .364 .723 .350 .724 .361 .727 .349 .719Flickr Visual .234 .483 .224 .441 .238 .407 .236 .385 .243 .460 .226 .385MM .350 .715 .343 .711 .347 .689 .344 .703 .354 .702 .339 .696ImageNet Visual .313 .561 .313 .561 .341 .540 .411 .603 .404 .584 .401 .578MM .362 .713 .362 .713 .373 .719 .401 .731 .427 .727 .412 .723ESPGame Visual .018 .448 .026 .376 .063 .487 .050 .434 .125 .506 .106 .451MM .208 .686 .187 .672 .243 .700 .246 .696 .269 .708 .260 .698Table 4: Performance on maximally covered datasets.language-specific, while the human annotated datasources are restricted to English.
Google and Bingwill return images that were ranked highly, whileFlickr contains photos rather than just any kind ofimage.
ImageNet contains high-quality images de-scriptive of a given synset, meaning that the taggedobject is likely to be centered in the image, whilethe ESP Game and Flickr images may have tags de-scribing events happening in the background also.3.1 Selecting and processing imagesSelecting images for Google, Bing and Flickr isstraightforward: using their respective APIs, the de-sired word is given as the search query and we ob-tain the top N returned images (unless otherwise in-dicated, we use N=10).
In the case of ImageNet andESP, images are not ranked and vary greatly in num-ber: for some words there is only a single image,while others have thousands.
With ImageNet, weare faced with the additional problem that imagestend to be associated only with leaf nodes in the hi-erarchy.
For example, dog has no directly associatedimages, while its hyponyms (e.g.
golden retriever,labrador) have many.
If a word has no associatedimages in its subtree, we try going up one level andseeing if the parent node?s tree yields any images.We subsequently randomly sample 100 images as-sociated with the word and obtain semi-ranked re-sults by selecting the 10 images closest to the me-dian representation as the sampled image represen-tations.
We use the same method for the ESP Gamedataset.
In all cases, images are resized and center-cropped to ensure that they are the correct size input.4 EvaluationRepresentation quality in semantics is usually evalu-ated using intrinsic datasets of human similarity andrelatedness judgments.
Model performance is as-sessed through the Spearman ?s rank correlation be-tween the system?s similarity scores for a given pairof words, together with human judgments.
Here,we evaluate on two well-known similarity and re-latedness judgment datasets: MEN (Bruni et al,2012) and SimLex-999 (Hill et al, 2015).
MEN fo-cuses explicitly on relatedness (i.e.
coffee-tea andcoffee-mug get high scores, while bakery-zebra getsa low score), while SimLex-999 focuses on what itcalls ?genuine?
similarity (i.e., coffee-tea gets a highscore, while both coffee-mug and bakery-zebra getlow scores).
They are standard evaluations for eval-uating representational quality in semantics.In each experiment, we examine performance ofthe visual representations compared to text-basedrepresentations, as well as performance of the multi-modal representation that fuses the two.
In this451Arch.
AlexNet GoogLeNet VGGNetAgg.
Mean Max Mean Max Mean MaxSource Type/Eval SL MEN SL MEN SL MEN SL MEN SL MEN SL MENWikipedia Text .248 .654 .248 .654 .248 .654 .248 .654 .248 .654 .248 .654Google Visual .406 .549 .402 .552 .420 .570 .434 .579 .430 .576 .406 .560MM .366 .691 .344 .693 .366 .701 .342 .699 .378 .701 .341 .693Bing Visual .431 .613 .425 .601 .410 .612 .414 .603 .400 .611 .398 .569MM .384 .715 .355 .708 .374 .725 .343 .712 .363 .720 .340 .705Flickr Visual .382 .577 .371 .544 .378 .547 .354 .518 .378 .567 .340 .511MM .372 .725 .344 .712 .367 .728 .336 .716 .370 .726 .330 .711ImageNet Visual .316 .560 .316 .560 .347 .538 .423 .600 .412 .581 .413 .574MM .348 .711 .348 .711 .364 .717 .394 .729 .418 .724 .405 .721ESPGame Visual .037 .431 .039 .347 .104 .501 .125 .438 .188 .514 .125 .460MM .179 .666 .147 .651 .224 .692 .226 .683 .268 .697 .222 .688Table 5: Performance on common coverage subsets of the datasets (MEN* and SimLex*).case, we apply mid-level fusion, concatenating theL2-normalized representations (Bruni et al, 2014).Middle fusion is a popular technique in multi-modalsemantics that has several benefits: 1) it allows fordrawing from different data sources for each modal-ity, that is, it does not require joint data; 2) con-catenation is less susceptible to noise, since it pre-serves the information in the individual modalities;and 3) it is straightforward to apply and computa-tionally inexpensive.
Linguistic representations are300-dimensional and are obtained by applying skip-gram with negative sampling (Mikolov et al, 2013)to a recent dump of Wikipedia.
The normalizationstep that is performed before applying fusion en-sures that both modalities contribute equally to theoverall multi-modal representation.5 ResultsAs Table 3 shows, the data sources vary in cover-age: it would be unfair to compare data sources onthe different subsets of the evaluation datasets thatthey have coverage for.
That is, when comparingdata sources we want to make sure we evaluate onimages for the exact same word pairs.
When com-paring network architectures, however, we are lessinterested in the relative coverage between datasetsand more interested in overall performance, in sucha way that it can be compared to other work that wasevaluated on the fully covered datasets.
Hence, wereport results on the maximally covered subsets perdata source, which we refer to as MEN and SimLex,as well as for the overlapping common subset ofword pairs that have images in each of the sources,which we refer to as MEN* and SimLex*.5.1 Maximum coverage comparisonTable 4 shows the results on the maximally covereddatasets.
This means we cannot directly compare be-tween data sources, because they have different cov-erage, but we can look at absolute performance andcompare network architectures.
The first row reportsresults for the text-based linguistic representationsthat were obtained from Wikipedia (repeated acrosscolumns for convenience).
For each of the three ar-chitectures, we evaluate on SimLex (SL) and MEN,using either the mean (Mean) or elementwise max-imum (Max) method for aggregating image repre-sentations into visual ones (see Section 2).
For eachdata source, we report results for the visual repre-sentations, as well as for the multi-modal represen-tations that fuse the visual and textual ones together.Performance across architectures is remarkably sta-ble: we have had to report results up to three deci-mal points to show the difference in performance insome cases.452Figure 2: The effect of the number of images on representation quality.For each of the network architectures, we seea marked improvement of multi-modal representa-tions over uni-modal linguistic representations.
Inmany cases, we also see visual representations out-performing linguistic ones, especially on SimLex.This is interesting, because e.g.
Google and Binghave full coverage over the datasets, so their visualrepresentations include highly abstract words, whichdoes not appear to have an adverse impact on themethod?s performance.
For the ESP Game dataset(on which performance is quite low) and ImageNet,we observe an increase in performance as we moveto the right in the table.
Interestingly, VGGNet onImageNet scores very highly, which seems to indi-cate that VGGNet is somehow more ?specialized?on ImageNet than the others.
The difference be-tween mean and max aggregation is relatively small,although the former seems to work better for Sim-Lex while the latter does slightly better for MEN.5.2 Common subset comparisonTable 5 shows the results on the common subset ofthe evaluation datasets, where all word pairs haveimages in each of the data sources.
First, note thesame patterns as before: multi-modal representa-tions perform better than linguistic ones.
Even forthe poorly performing ESP Game dataset, the VG-GNet representations perform better on both Sim-Lex and MEN (bottom right of the table).
Visualrepresentations from Google, Bing, Flickr and Im-ageNet al perform much better than ESP Game onthis common covered subset.
In a sense, the full-coverage datasets were ?punished?
for their abilityto return images for abstract words in the previousexperiment: on this subset, which is more concrete,the search engines do much better.
To a certainextent, including linguistic information is actuallydetrimental to performance, with multi-modal per-forming worse than purely visual.
Again, we see themarked improvement with VGGNet for ImageNet,while Google, Bing and Flickr all do very well, re-gardless of the architecture.These numbers indicate the robustness of the ap-proach: we find that multi-modal representationlearning yields better performance across the board:for different network architectures, different datasources and different aggregation methods.
If com-putational efficiency or memory usage are issues,then GoogLeNet or AlexNet are the best choices.The ESP Game dataset does not appear to work very453well, and is best avoided.
If we have the right cov-erage, then ImageNet gives good results, especiallyif we can use VGGNet.
However, coverage is of-ten the main issue, in which case Google and Bingyield images that are comparable or even better thanimages from the carefully annotated ImageNet.5.3 Number of imagesAnother question is the number of images we wantto use: does performance increase with more im-ages?
Is it always better to have seen 100 cats in-stead of only 10, or do we have enough informationafter having seen one or two already?
There is anobvious trade-off here, since downloading and pro-cessing images takes time (and may incur financialcosts).
This experiment only applies to relevance-sorted data sources: the image selection procedurefor ImageNet and ESPGame is more about removingoutliers than about finding the best possible images.As Figure 2 shows, it turns out that the optimalnumber of images stabilizes surprisingly quickly:around 10-20 images appears to be enough, and insome cases already too many.
Performance acrossnetworks does not vary dramatically when usingmore images, but in the case of Flickr images on theMEN dataset, performance drops significantly as thenumber of images increases.5.4 Multi- and cross-lingual applicabilityAlthough there are some indicators that visual rep-resentation learning extends to other languages, par-ticularly in the case of bilingual lexicon learning(Bergsma and Van Durme, 2011; Kiela et al, 2015b;Vulic?
et al, 2016), this has not been shown directlyon the same set of human similarity and relatednessjudgments.
In order to examine the multi-lingual ap-plicability of our findings, we train linguistic repre-sentations on recent dumps of the English and ItalianWikipedia.
We then search for 10 images per wordon Google and Bing, while setting the language toEnglish or Italian.
We compare the results on theoriginal SimLex, and the Italian version from Le-viant and Reichart (2015).Similarly, we examine a cross-lingual scenario,where we translate Italian words into English usingGoogle Translate.
We then obtain images for thetranslated words and extract visual representations.These cross-lingual visual representations are sub-SimLexEN IT (M) IT (C)Wikipedia Linguistic .310 .179 .179Google Visual .340 .231 .238Multi-modal .380 .231 .227Bing Visual .325 .212 .194Multi-modal .373 .227 .207Table 6: Performance on English and Italian Sim-Lex, either in the multi-lingual setting (M) or thecross-lingual settting (C) where we first map to En-glish.sequently evaluated on the Italian version of Sim-Lex.
Since we know that performance across archi-tectures is similar, we use AlexNet representations.The results can be found in Table 6.
We find thesame pattern: in all cases, visual and multi-modalrepresentations outperform linguistic ones.
The Ital-ian version of SimLex appears to be more diffi-cult than the English version.
Google performs bet-ter than Bing, especially on the Italian evaluations.For Google, the cross-lingual scenario works bet-ter, while Bing yields better results in the multi-lingual setting where we use the language itself in-stead of mapping to English.
Although somewhatpreliminary, these results clearly indicate that multi-modal semantics can fruitfully be applied to lan-guages other than English.6 Conclusion and future workThe objective of this study has been to system-atically compare network architectures and datasources for multi-modal systems.
In particular, wefocused on the capabilities of deep visual represen-tations in capturing semantics, as measured by cor-relation with human similarity and relatedness judg-ments.
Our findings can be summarized as follows:?
We examined AlexNet, GoogLeNet andVGGNet, all three recent winners of theILSVRC ImageNet classification challenge(Russakovsky et al, 2015), and found thatthey perform very similarly.
If efficiency ormemory are issues, AlexNet or GoogLeNetare the most suitable architectures.
For overall454best performance, AlexNet and VGGNet arethe best choices.?
The choice of data sources appeared to have abigger impact: Google, Bing, Flickr and Im-ageNet were much better than the ESP Gamedataset.
Google, Flickr and Bing have the ad-vantage that they have potentially unlimitedcoverage.
Google and Bing are particularlysuited to full-coverage experiments, even whenthese include abstract words.?
We found that the number of images has animpact on performance, but that it stabilizes ataround 10-20 images, indicating that it is usu-ally not necessary to obtain more than 10 im-ages per word.
For Flickr, obtaining more im-ages is detrimental to performance.?
Lastly, we established that these findings ex-tend to other languages beyond English, obtain-ing the same findings on an Italian version ofSimLex using the Italian Wikipedia.
We ex-amined both the multi-lingual setting where weobtain search results using the Italian languageand a cross-lingual setting where we mappedItalian words to English and retrieved imagesfor those.This work answers several open questions inmulti-modal semantics and we hope that it will serveas a guide for future research in the field.
It is im-portant to note that the multi-modal results only ap-ply to the mid-level fusion method of concatenat-ing normalized vectors: although these findings areindicative of performance for other fusion methods,different architectures or data sources may be moresuitable for different fusion methods.In future work, downstream tasks should be ad-dressed: it is good that multi-modal semantics im-proves performance on intrinsic evaluations, but itis important to show its practical benefits in moreapplied tasks as well.
Understanding what it is thatmakes these representations perform so well is an-other important and yet unanswered question.
Wehope that this work may be used as a reference indetermining some of the choices that can be madewhen developing multi-modal models.AcknowledgmentsAnita Vero?
is supported by the Nuance FoundationGrant: Learning Type-Driven Distributed Represen-tations of Language.
Stephen Clark is supported bythe ERC Starting Grant: DisCoTex (306920).ReferencesShane Bergsma and Randy Goebel.
2011.
Using visualinformation to predict lexical preference.
In Proceed-ings of RANLP, pages 399?405.Shane Bergsma and Benjamin Van Durme.
2011.
Learn-ing bilingual lexicons using the visual similarity of la-beled web images.
In IJCAI, pages 1764?1769.Yevgeni Berzak, Andrei Barbu, Daniel Harari, BorisKatz, and Shimon Ullman.
2015.
Do you see whati mean?
visual resolution of linguistic ambiguities.
InProceedings of EMNLP.Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-Khanh Tran.
2012.
Distributional semantics in tech-nicolor.
In ACL, pages 136?145.Elia Bruni, Nam-Khanh Tran, and Marco Baroni.
2014.Multimodal distributional semantics.
Journal of Artif-ical Intelligence Research, 49:1?47.Luana Bulat, Douwe Kiela, and Stephen Clark.
2016.Vision and Feature Norms: Improving automatic fea-ture norm learning through cross-modal maps.
In Pro-ceedings of NAACL-HLT 2016, San Diego, CA.John A. Bullinaria and Joseph P. Levy.
2007.
ExtractingSemantic Representations from Word Co-occurrenceStatistics: A computational study.
Behavior ResearchMethods, 39:510?526.Stephen Clark.
2015.
Vector Space Models of LexicalMeaning.
In Shalom Lappin and Chris Fox, editors,Handbook of Contemporary Semantic Theory, chap-ter 16.
Wiley-Blackwell, Oxford.Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,and Fei-Fei Li.
2009.
ImageNet: A large-scale hi-erarchical image database.
In Proceedings of CVPR,pages 248?255.Robert Fergus, Fei-Fei Li, Pietro Perona, and AndrewZisserman.
2005.
Learning object categories fromGoogle?s image search.
In Proceedings of ICCV,pages 1816?1823.Stevan Harnad.
1990.
The symbol grounding problem.Physica D, 42:335?346.Felix Hill, Roi Reichart, and Anna Korhonen.
2015.Simlex-999: Evaluating semantic models with (gen-uine) similarity estimation.
Computational Linguis-tics.455Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky,Ilya Sutskever, and Ruslan R Salakhutdinov.
2012.Improving neural networks by preventing co-adaptation of feature detectors.
arXiv preprintarXiv:1207.0580.Douwe Kiela and Le?on Bottou.
2014.
Learning imageembeddings using convolutional neural networks forimproved multi-modal semantics.
In Proceedings ofEMNLP, pages 36?45.Douwe Kiela and Stephen Clark.
2014.
A SystematicStudy of Semantic Vector Space Model Parameters.In Proceedings of EACL 2014, Workshop on Contin-uous Vector Space Models and their Compositionality(CVSC).Douwe Kiela, Laura Rimell, Ivan Vulic?, and StephenClark.
2015a.
Exploiting image generality for lexicalentailment detection.
In Proceedings of ACL, pages119?124, Beijing, China, July.
Association for Com-putational Linguistics.Douwe Kiela, Ivan Vulic?, and Stephen Clark.
2015b.
Vi-sual bilingual lexicon induction with transferred con-vnet features.
In Proceedings of the 2015 Conferenceon Empirical Methods in Natural Language Process-ing, pages 148?158, Lisbon, Portugal, September.
As-sociation for Computational Linguistics.Douwe Kiela.
2016.
Mmfeat: A toolkit for extractingmulti-modal features.
In Proceedings of ACL 2016.Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton.2012.
ImageNet classification with deep convolutionalneural networks.
In Proceedings of NIPS, pages 1106?1114.Angeliki Lazaridou, Nghia The Pham, and Marco Baroni.2015.
Combining language and vision with a multi-modal skipgram model.
In Proceedings of NAACL.Yann LeCun, Le?on Bottou, Yoshua Bengio, and PatrickHaffner.
1998.
Gradient-based learning applied todocument recognition.
Proceedings of the IEEE,86(11):2278?2324.Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.2015.
Deep learning.
Nature, 521(7553):436?444.Ira Leviant and Roi Reichart.
2015.
Judgment languagematters: Multilingual vector space models for judg-ment language aware lexical semantics.
arXiv preprintarXiv:1508.00106.Min Lin, Qiang Chen, and Shuicheng Yan.
2013.
Net-work in network.
CoRR, abs/1312.4400.David G. Lowe.
2004.
Distinctive image features fromscale-invariant keypoints.
International Journal ofComputer Vision, 60(2):91?110.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient estimation of word representa-tions in vector space.
In Proceedings of ICLR, Scotts-dale, Arizona, USA.George A. Miller.
1995.
WordNet: A lexical database forEnglish.
Communications of the ACM, 38(11):39?41.Vinod Nair and Geoffrey E Hinton.
2010.
Rectified lin-ear units improve restricted boltzmann machines.
InProceedings of ICML, pages 807?814.Stephen Roller and Sabine Schulte im Walde.
2013.A multimodal LDA model integrating textual, cogni-tive and visual modalities.
In Proceedings of EMNLP,pages 1146?1157.Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,Sanjeev Satheesh, Sean Ma, Zhiheng Huang, AndrejKarpathy, Aditya Khosla, Michael Bernstein, Alexan-der C. Berg, and Li Fei-Fei.
2015.
ImageNet LargeScale Visual Recognition Challenge.
InternationalJournal of Computer Vision (IJCV), 115(3):211?252.Ekaterina Shutova, Douwe Kiela, and Jean Maillard.2016.
Black holes and white rabbits: Metaphor iden-tification with visual features.
In Proceedings ofNAACL-HTL 2016, San Diego.
Association for Com-putational Linguistics.Carina Silberer and Mirella Lapata.
2014.
Learninggrounded meaning representations with autoencoders.In Proceedings of ACL, pages 721?732.Karen Simonyan and Andrew Zisserman.
2014.
Verydeep convolutional networks for large-scale imagerecognition.
arXiv preprint arXiv:1409.1556.Josef Sivic and Andrew Zisserman.
2003.
Video google:A text retrieval approach to object matching in videos.In Proceedings of ICCV, pages 1470?1477.Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Ser-manet, Scott Reed, Dragomir Anguelov, Dumitru Er-han, Vincent Vanhoucke, and Andrew Rabinovich.2015.
Going deeper with convolutions.
In Proceed-ings of the IEEE Conference on Computer Vision andPattern Recognition, pages 1?9.Peter D. Turney and Patrick Pantel.
2010.
From Fre-quency to Meaning: vector space models of semantics.Journal of Artifical Intelligence Research, 37(1):141?188, January.Luis von Ahn and Laura Dabbish.
2004.
Labeling im-ages with a computer game.
In CHI, pages 319?326.Ivan Vulic?, Douwe Kiela, Marie-Francine Moens, andStephen Clark.
2016.
Multi-modal representations forimproved bilingual lexicon learning.
In Proceedingsof ACL, Berlin, Germany.
Association for Computa-tional Linguistics.Tobias Weyand, Ilya Kostrikov, and James Philbin.
2016.Planet - photo geolocation with convolutional neuralnetworks.
CoRR, abs/1602.05314.456
