Proceedings of the 10th Conference on Parsing Technologies, pages 23?32,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsSemi-supervised Training of a Statistical Parserfrom Unlabeled Partially-bracketed DataRebecca Watson and Ted BriscoeComputer LaboratoryUniversity of Cambridge, UKFirstName.LastName@cl.cam.ac.ukJohn CarrollDepartment of InformaticsUniversity of Sussex, UKJ.A.Carroll@sussex.ac.ukAbstractWe compare the accuracy of a statisti-cal parse ranking model trained from afully-annotated portion of the Susannetreebank with one trained from unla-beled partially-bracketed sentences de-rived from this treebank and from thePenn Treebank.
We demonstrate thatconfidence-based semi-supervised tech-niques similar to self-training outperformexpectation maximization when both areconstrained by partial bracketing.
Bothmethods based on partially-bracketedtraining data outperform the fully su-pervised technique, and both can, inprinciple, be applied to any statisticalparser whose output is consistent withsuch partial-bracketing.
We also exploretuning the model to a different domainand the effect of in-domain data in thesemi-supervised training processes.1 IntroductionExtant statistical parsers require extensive anddetailed treebanks, as many of their lexical andstructural parameters are estimated in a fully-supervised fashion from treebank derivations.Collins (1999) is a detailed exposition of onesuch ongoing line of research which utilizes theWall Street Journal (WSJ) sections of the PennTreebank (PTB).
However, there are disadvan-tages to this approach.
Firstly, treebanks are ex-pensive to create manually.
Secondly, the richerthe annotation required, the harder it is to adaptthe treebank to train parsers which make differ-ent assumptions about the structure of syntac-tic analyses.
For example, Hockenmeier (2003)trains a statistical parser based on CombinatoryCategorial Grammar (CCG) on the WSJ PTB,but first maps the treebank to CCG derivationssemi-automatically.
Thirdly, many (lexical) pa-rameter estimates do not generalize well be-tween domains.
For instance, Gildea (2001) re-ports that WSJ-derived bilexical parameters inCollins?
(1999) Model 1 parser contribute about1% to parse selection accuracy when test datais in the same domain, but yield no improve-ment for test data selected from the Brown Cor-pus.
Tadayoshi et al (2005) adapt a statisticalparser trained on the WSJ PTB to the biomed-ical domain by retraining on the Genia Corpus,augmented with manually corrected derivationsin the same format.
To make statistical parsingmore viable for a range of applications, we needto make more effective and flexible use of extanttraining data and minimize the cost of annota-tion for new data created to tune a system to anew domain.Unsupervised methods for training parsershave been relatively unsuccessful to date, in-cluding expectation maximization (EM) such asthe inside-outside algorithm (IOA) over PCFGs(Baker, 1979; Prescher, 2001).
However, Pereiraand Schabes (1992) adapted the IOA to applyover semi-supervised data (unlabeled bracket-ings) extracted from the PTB.
They constrainthe training data (parses) considered within theIOA to those consistent with the constituentboundaries defined by the bracketing.
One ad-vantage of this approach is that, although lessinformation is derived from the treebank, it gen-23eralizes better to parsers which make differentrepresentational assumptions, and it is easier,as Pereira and Schabes did, to map unlabeledbracketings to a format more consistent withthe target grammar.
Another is that the costof annotation with unlabeled brackets should belower than that of developing a representation-ally richer treebank.
More recently, both Riezleret al (2002) and Clark and Curran (2004) havesuccessfully trained maximum entropy parsingmodels utilizing all derivations in the model con-sistent with the annotation of the WSJ PTB,weighting counts by the normalized probabilityof the associated derivation.
In this paper, weextend this line of investigation by utilizing onlyunlabeled and partial bracketing.We compare the performance of a statisti-cal parsing model trained from a detailed tree-bank with that of the same model trained withsemi-supervised techniques that require only un-labeled partially-bracketed data.
We contrastan IOA-based EM method for training a PGLRparser (Inui et al, 1997), similar to the methodapplied by Pereira and Schabes to PCFGs, to arange of confidence-based semi-supervised meth-ods described below.
The IOA is a generaliza-tion of the Baum-Welch or Forward-Backwardalgorithm, another instance of EM, which can beused to train Hidden Markov Models (HMMs).Elworthy (1994) and Merialdo (1994) demon-strated that Baum-Welch does not necessarilyimprove the performance of an HMM part-of-speech tagger when deployed in an unsuper-vised or semi-supervised setting.
These some-what negative results, in contrast to those ofPereira and Schabes (1992), suggest that EMtechniques require fairly determinate trainingdata to yield useful models.
Another motiva-tion to explore alternative non-iterative meth-ods is that the derivation space over partially-bracketed data can remain large (>1K) whilethe confidence-based methods we explore have atotal processing overhead equivalent to one iter-ation of an IOA-based EM algorithm.As we utilize an initial model to annotate ad-ditional training data, our methods are closelyrelated to self-training methods described in theliterature (e.g.
McClosky et al 2006, Bacchi-ani et al 2006).
However these methods havebeen applied to fully-annotated training datato create the initial model, which is then usedto annotate further training data derived fromunannotated text.
Instead, we train entirelyfrom partially-bracketed data, starting from thesmall proportion of ?unambiguous?
data wherebya single parse is consistent with the annota-tion.
Therefore, our methods are better de-scribed as semi-supervised and the main focusof this work is the flexible re-use of existingtreebanks to train a wider variety of statisticalparsing models.
While many statistical parsersextract a context-free grammar in parallel witha statistical parse selection model, we demon-strate that existing treebanks can be utilized totrain parsers that deploy grammars that makeother representational assumptions.
As a result,our methods can be applied by a range of parsersto minimize the manual effort required to traina parser or adapt to a new domain.
?2 gives details of the parsing system that arerelevant to this work.
?3 and ?4 describe ourdata and evaluation schemes, respectively.
?5describes our semi-supervised training methods.
?6 explores the problem of tuning a parser to anew domain.
Finally, ?7 gives conclusions andfuture work.2 The Parsing SystemSentences are automatically preprocessed in aseries of modular pipelined steps, including to-kenization, part of speech (POS) tagging, andmorphological analysis, before being passed tothe statistical parser.
The parser utilizes a man-ually written feature-based unification grammarover POS tag sequences.2.1 The Parse Selection ModelA context-free ?backbone?
is automatically de-rived from the unification grammar1 and a gen-eralized or non-deterministic LALR(1) table is1This backbone is determined by compiling out thevalues of prespecified attributes.
For example, if we com-pile out the attribute PLURAL which has 2 possible val-ues (plural or not) we will create 2 CFG rules for eachrule with categories that contain PLURAL.
Therefore,no information is lost during this process.24constructed from this backbone (Tomita, 1987).The residue of features not incorporated intothe backbone are unified on each reduce actionand if unification fails the associated derivationpaths also fail.
The parser creates a packedparse forest represented as a graph-structuredstack.2 The parse selection model ranks com-plete derivations in the parse forest by com-puting the product of the probabilities of the(shift/reduce) parse actions (given LR state andlookahead item) which created each derivation(Inui et al, 1997).Estimating action probabilities, consists ofa) recording an action history for the correctderivation in the parse forest (for each sen-tence in a treebank), b) computing the fre-quency of each action over all action historiesand c) normalizing these frequencies to deter-mine probability distributions over conflicting(i.e.
shift/reduce or reduce/reduce) actions.Inui et al (1997) describe the probabilitymodel utilized in the system where a transitionis represented by the probability of moving fromone stack state, ?i?1, (an instance of the graphstructured stack) to another, ?i.
They estimatethis probability using the stack-top state si?1,next input symbol li and next action ai.
Thisprobability is conditioned on the type of statesi?1.
Ss and Sr are mutually exclusive setsof states which represent those states reachedafter shift or reduce actions, respectively.
Theprobability of an action is estimated as:P (li, ai, ?i|?i?1) ?
{P (li, ai|si?1) si?1 ?
SsP (ai|si?1, li) si?1 ?
Sr}Therefore, normalization is performed over alllookaheads for a state or over each lookaheadfor the state depending on whether the state isa member of Ss or Sr, respectively (hereafterthe I function).
In addition, Laplace estimationcan be used to ensure that all actions in the2The parse forest is an instance of a feature forest asdefined by Miyao and Tsujii (2002).
We will use the term?node?
herein to refer to an element in a derivation treeor in the parse forest that corresponds to a (sub-)analysiswhose label is the mother?s label in the corresponding CF?backbone?
rule.table are assigned a non-zero probability (theIL function).3 Training DataThe treebanks we use in this work are in one oftwo possible formats.
In either case, a treebankT consists of a set of sentences.
Each sentencet is a pair (s,M), where s is the automaticallypreprocessed set of POS tagged tokens (see ?2)and M is either a fully annotated derivation, A,or an unlabeled bracketing U .
This bracketingmay be partial in the sense that it may be com-patible with more than one derivation producedby a given parser.
Although occasionally thebracketing is itself complete but alternative non-terminal labeling causes indeterminacy, most of-ten the ?flatter?
bracketing available from ex-tant treebanks is compatible with several alter-native ?deeper?
mostly binary-branching deriva-tions output by a parser.3.1 Derivation ConsistencyGiven t = (s,A), there will exist a single deriva-tion in the parse forest that is compatible (cor-rect).
In this case, equality between the deriva-tion tree and the treebank annotation A iden-tifies the correct derivation.
Following Pereiraand Schabes (1992) given t = (s, U), a node?sspan in the parse forest is valid if it does notoverlap with any span outlined in U , and hence,a derivation is correct if the span of every nodein the derivation is valid in U .
That is, if nocrossing brackets are present in the derivation.Thus, given t = (s, U), there will often be morethan one derivation compatible with the partialbracketing.Given the correct nodes in the parse forestor in derivations, we can then extract the cor-responding action histories and estimate actionprobabilities as described in ?2.1.
In this way,partial bracketing is used to constrain the set ofderivations considered in training to those thatare compatible with this bracketing.3.2 The Susanne Treebank andBaseline Training DataThe Susanne Treebank (Sampson, 1995) is uti-lized to create fully annotated training data.25This treebank contains detailed syntactic deriva-tions represented as trees, but the node label-ing is incompatible with the system grammar.We extracted sentences from Susanne and auto-matically preprocessed them.
A few multiwordsare retokenized, and the sentences are retaggedusing the POS tagger, and the bracketing de-terministically modified to more closely matchthat of the grammar, resulting in a bracketedcorpus of 6674 sentences.
We will refer to thisbracketed treebank as S, henceforth.A fully-annotated and system compatibletreebank of 3543 sentences from S was alsocreated.
We will refer to this annotated tree-bank, used for fully supervised training, as B.The system parser was applied to constructa parse forest of analyses which are compati-ble with the bracketing.
For 1258 sentences,the grammar writer interactively selected cor-rect (sub)analyses within this set until a sin-gle analysis remained.
The remaining 2285 sen-tences were automatically parsed and all consis-tent derivations were returned.
Since B containsmore than one possible derivation for roughlytwo thirds of the data the 1258 sentences (pairedwith a single tree) were repeated twice so thatcounts from these trees were weighted morehighly.
The level of reweighting was determinedexperimentally using some held out data fromS.
The baseline supervised model against whichwe compare in this work is defined by the func-tion IL(B) as described in ?2.1.
The costs ofderiving the fully-annotated treebank are highas interactive manual disambiguation takes anaverage of ten minutes per sentence, even giventhe partial bracketing derived from Susanne.3.3 The WSJ PTB Training DataThe Wall Street Journal (WSJ) sections of thePenn Treebank (PTB) are employed as bothtraining and test data by many researchers inthe field of statistical parsing.
The annotatedcorpus implicitly defines a grammar by provid-ing a labeled bracketing over words annotatedwith POS tags.
We extracted the unlabeledbracketing from the de facto standard trainingsections (2-21 inclusive).3 We will refer to theresulting corpus as W and the combination (con-catenation) of the partially-bracketed corpora Sand W as SW .3.4 The DepBank Test DataKing et al (2003) describe the developmentof the PARC 700 Dependency Bank, a gold-standard set of relational dependencies for 700sentences (from the PTB) drawn at randomfrom section 23 of the WSJ (the de facto stan-dard test set for statistical parsing).
In all theevaluations reported in this paper we test ourparser over a gold-standard set of relational de-pendencies compatible with our parser outputderived (Briscoe and Carroll, 2006) from thePARC 700 Dependency Bank (DepBank, hence-forth).The Susanne Corpus is a (balanced) subset ofthe Brown Corpus which consists of 15 broadcategories of American English texts.
All butone category (reportage text) is drawn from dif-ferent domains than the WSJ.
We therefore, fol-lowing Gildea (2001) and others, consider S, andalso the baseline training data, B, as out-of-domain training data.4 The Evaluation SchemeThe parser?s output is evaluated using a rela-tional dependency evaluation scheme (Carroll,et al, 1998; Lin, 1998) with standard measures:precision, recall and F1.
Relations are organizedinto a hierarchy with the root node specifying anunlabeled dependency.
The microaveraged pre-cision, recall and F1 scores are calculated fromthe counts for all relations in the hierarchy whichsubsume the parser output.
The microaveragedF1 score for the baseline system using this eval-uation scheme is 75.61%, which ?
over similarsets of relational dependencies ?
is broadly com-parable to recent evaluation results published byKing and collaborators with their state-of-the-art parsing system (Briscoe et al, 2006).3The pipeline is the same as that used for creating Sthough we do not automatically map the bracketing tobe more consistent with the system grammar, instead,we simply removed unary brackets.264.1 Wilcoxon Signed Ranks TestThe Wilcoxon Signed Ranks (Wilcoxon, hence-forth) test is a non-parametric test for statisticalsignificance that is appropriate when there is onedata sample and several measures.
For example,to compare the accuracy of two parsers over thesame data set.
As the number of samples (sen-tences) is large we use the normal approximationfor z. Siegel and Castellan (1988) describe andmotivate this test.
We use a 0.05 level of sig-nificance, and provide z-value probabilities forsignificant results reported below.
These resultsare computed over microaveraged F1 scores foreach sentence in DepBank.5 Training from UnlabeledBracketingsWe parsed all the bracketed training data us-ing the baseline model to obtain up to 1K top-ranked derivations and found that a significantproportion of the sentences of the potential setavailable for training had only a single deriva-tion compatible with their unlabeled bracket-ing.
We refer to these sets as the unambiguoustraining data (?)
and will refer to the remainingsentences (for which more than one derivationwas compatible with their unlabeled bracketing)as the ambiguous training data (?).
The avail-ability of significant quantities of unambiguoustraining data that can be found automaticallysuggests that we may be able to dispense withthe costly reannotation step required to gener-ate the fully supervised training corpus, B.Table 1 illustrates the split of the corpora intomutually exclusive sets ?, ?, ?no match?
and?timeout?.
The latter two sets are not utilizedduring training and refer to sentences for whichall parses were inconsistent with the bracketingand those for which no parses were found dueto time and memory limitations (self-imposed)on the system.4 As our grammar is differentfrom that implicit in the WSJ PTB there is ahigh proportion of sentences where no parseswere consistent with the unmodified PTB brack-4As there are time and memory restrictions duringparsing, the SW results are not equal to the sum of thosefrom S and W analysis.Corpus | ?
| | ?
| No Match TimeoutS 1097 4138 1322 191W 6334 15152 15749 1094SW 7409 19248 16946 1475Table 1: Corpus split for S, W and SW .eting.
However, a preliminary investigation ofno matches didn?t yield any clear patterns ofinconsistency that we could quickly ameliorateby simple modifications of the PTB bracketing.We leave for the future a more extensive investi-gation of these cases which, in principle, wouldallow us to make more use of this training data.An alternative approach that we have also ex-plored is to utilize a similar bootstrapping ap-proach with data partially-annotated for gram-matical relations (Watson and Briscoe, 2007).5.1 Confidence-Based ApproachesWe use ?
to build an initial model.
We thenutilize this initial model to derive derivations(compatible with the unlabeled partial brack-eting) for ?
from which we select additionaltraining data.
We employ two types of selectionmethods.
First, we select the top-ranked deriva-tion only and weight actions which resulted inthis derivation equally with those of the initialmodel (C1).
This method is similar to ?Viterbitraining?
of HMMs though we do not weightthe corresponding actions using the top parse?sprobability.
Secondly, we select more than onederivation, placing an appropriate weight onthe corresponding action histories based on theinitial model?s confidence in the derivation.
Weconsider three such models, in which we weighttransitions corresponding to each derivationranked r with probability p in the set of size neither using 1n , 1r or p itself to weight counts.5For example, given a treebank T with sentencest = (s, U), function P to return the set ofparses consistent with U given t and function Athat returns the set of actions given a parse p,then the frequency count of action ak in Cr is5In ?2.1 we calculate action probabilities based on fre-quency counts where we perform a weighted sum overaction histories and each history has a weight of 1.
Weextend this scheme to include weights that differ betweenaction histories corresponding to each derivation.27determined as follows:| ak |=?|T |i=1?|P (ti)|j=1,ak?A(pij)1jThese methods all perform normalization overthe resulting action histories using the trainingfunction IL and will be referred to as Cn, Crand Cp, respectively.
Cn is a ?uniform?
modelwhich weights counts only by degree of ambi-guity and makes no use of ranking information.Cr weights counts by derivation rank, and Cpis simpler than and different to one iteration ofEM as outside probabilities are not utilized.
Allof the semi-supervised functions described heretake two arguments: an initial model and thedata to train over, respectively.Models derived from unambiguous trainingdata, ?, alone are relatively accurate, achiev-ing indistinguishable performance to that of thebaseline system given either W or SW as train-ing data.
We utilize these models as initial mod-els and train over different corpora with each ofthe confidence-based models.
Table 2 gives re-sults for all models.
Results statistically signifi-cant compared to the baseline system are shownin bold print (better) or italic print (worse).These methods show promise, often yielding sys-tems whose performance is significantly betterthan the baseline system.
Method Cr achievedthe best performance in this experiment and re-mained consistently better in those reported be-low.
Throughout the different approaches a do-main effect can be seen, models utilizing just Sare worse, although the best performing modelsbenefit from the use of both S and W as trainingdata (i.e.
SW ).5.2 EMOur EM model differs from that of Pereira andSchabes as a PGLR parser adds context overa PCFG so that a single rule can be appliedin several different states containing reduce ac-tions.
Therefore, the summation and normaliza-tion performed for a CFG rule within IOA is in-stead applied within such contexts.
We can ap-ply I (our PGLR normalization function with-out Laplace smoothing) to perform the requiredsteps if we output the action history with theModel Prec Rec F1 P (z)?Baseline 77.05 74.22 75.61 -IL(?
(S)) 76.02 73.40 74.69 0.0294C1(IL(?
(S)), ?
(S)) 77.05 74.22 75.61 0.4960Cn(IL(?
(S)), ?
(S)) 77.51 74.80 76.13 0.0655Cr(IL(?
(S)), ?
(S)) 77.73 74.98 76.33 0.0154Cp(IL(?
(S)), ?
(S)) 76.45 73.91 75.16 0.2090IL(?
(W )) 77.01 74.31 75.64 0.1038C1(IL(?
(W )), ?
(W )) 76.90 74.23 75.55 0.2546Cn(IL(?
(W )), ?
(W )) 77.85 75.07 76.43 0.0017Cr(IL(?
(W )), ?
(W )) 77.88 75.04 76.43 0.0011Cp(IL(?
(W )), ?
(W )) 77.40 74.75 76.05 0.1335IL(?
(SW )) 77.09 74.35 75.70 0.1003C1(IL(?
(SW )), ?
(SW )) 76.86 74.21 75.51 0.2483Cn(IL(?
(SW )), ?
(SW )) 77.88 75.05 76.44 0.0048Cr(IL(?
(SW )), ?
(SW )) 78.01 75.13 76.54 0.0007Cp(IL(?
(SW )), ?
(SW )) 77.54 74.95 76.23 0.0618Table 2: Performance of all models on DepBank.
?represents the statistical significance of the sys-tem against the baseline model.corresponding normalized inside-outside weightfor each node (Watson et al, 2005).We perform EM starting from two initial mod-els; either a uniform probability model, IL(), orfrom models derived from unambiguous train-ing data, ?.
Figure 1 shows the cross entropydecreasing monotonically from iteration 2 (asguaranteed by the EM method) for different cor-pora and initial models.
Some models show aninitial increase in cross-entropy from iteration 1to iteration 2, because the models are initial-ized from a subset of the data which is used toperform maximisation.
Cross-entropy increases,by definition, as we incorporate ambiguous datawith more than one consistent derivation.Performance over DepBank can be seen inFigures 2, 3, and 4 for each dataset S, W andSW, respectively.
Comparing the Cr and EMlines in each of Figures 2, 3, and 4, it is evidentthat Cr outperforms EM across all datasets, re-gardless of the initial model applied.
In mostcases, these results are significant, even whenwe manually select the best model (iteration)for EM.The graphs of EM performance from itera-tion 1 illustrate the same ?classical?
and ?initial?patterns observed by Elworthy (1994).
WhenEM is initialized from a relatively poor model,such as that built from S (Figure 2), a ?classical?281.21.31.41.51.61.71.81.90 2 4 6 8 10 12 14 16H(C,G)Iteration NumberEM(IL(), S)rrr r r r r r r r r r r r rrEM(IL(?
(S)), S)ccc c c c c c c c c c c c ccEM(IL(), W )44 4 4 4 4 4 4 4 4 4 4 4 4 44EM(IL(?
(W )),W )??
?
?
?
?
?
?
?
?
?
?
?
?
?
?EM(IL(), SW )22 2 2 2 2 2 2 2 2 2 2 2 2 22EM(IL(?
(SW )), SW )33 3 3 3 3 3 3 3 3 3 3 3 3 33Figure 1: Cross Entropy Convergence for vari-ous training data and models, with EM.pattern emerges with relatively steady improve-ment from iteration 1 until performance asymp-totes.
However, when the starting point is better(Figures 3 and 4), the ?initial?
pattern emergesin which the best performance is reached after asingle iteration.6 Tuning to a New DomainWhen building NLP applications we would wantto be able to tune a parser to a new domainwith minimal manual effort.
To obtain trainingdata in a new domain, annotating a corpus withpartial-bracketing information is much cheaperthan full annotation.
To investigate whethersuch data would be of value, we considered Wto be the corpus over which we were tuning andapplied the best performing model trained overS, Cr(IL(?
(S)), ?
(S)), as our initial model.
Fig-ure 5 illustrates the performance of Cr comparedto EM.Tuning using Cr was not significantly differ-ent from the model built directly from the entiredata set with Cr, achieving 76.57% as opposedto 76.54% F1 (see Table 2).
By contrast, EMperforms better given all the data from the be-ginning rather than tuning to the new domain.7474.57575.57676.50 2 4 6 8 10 12 14 16F1Iteration NumberBaselineCr(IL(?
(S)), ?
(S))EM(IL(), S)rr rrrr r r r r r rrr rrEM(IL(?
(S)), S)bb bbb bbbb b b b b b bbbFigure 2: Performance over S for Cr and EM.7575.275.475.675.87676.276.476.60 2 4 6 8 10 12 14 16F1Iteration NumberBaselineCr(IL(?
(W )), ?
(W ))EM(IL(), W )rrrr r r rr r r rrr r rrEM(IL(?
(W )),W )bbbbb b b b b bb b b bb bbFigure 3: Performance over W for Cr and EM.297575.275.475.675.87676.276.476.676.80 2 4 6 8 10 12 14 16F1Iteration NumberBaselineCr(IL(?
(SW )), ?
(SW ))EM(IL(), SW )r rrr r r r r r r r r rr rrEM(IL(?
(SW )), SW )bbbbb b b b b b b b b b b bbFigure 4: Performance over SW for Cr and EM.Cr generally outperforms EM, though it is worthnoting the behavior of EM given only the tun-ing data (W ) rather than the data from both do-mains (SW ).
In this case, the graph illustrates acombination of Elworthy?s ?initial?
and ?classical?patterns.
The steep drop in performance (downto 69.93% F1) after the first iteration is proba-bly due to loss of information from S. However,this run also eventually converges to similar per-formance, suggesting that the information in Sis effectively disregarded as it forms only a smallportion of SW , and that these runs effectivelyconverge to a local maximum over W .Bacchiani et al (2006), working in a similarframework, explore weighting the contribution(frequency counts) of the in-domain and out-of-domain training datasets and demonstrate thatthis can have beneficial effects.
Furthermore,they also tried unsupervised tuning to the in-domain corpus by weighting parses for it bytheir normalized probability.
This method issimilar to our Cp method.
However, when wetried unsupervised tuning using the WSJ andan initial model built from S in conjunction withour confidence-based methods, performance de-graded significantly.7474.57575.57676.5770 2 4 6 8 10 12 14 16F1Iteration NumberBaselineCr(IL(?
(SW )), ?
(SW ))Cr(Cr(IL(?
(S)), ?
(S)), W )EM(IL(?
(SW )), SW )bbbbb b b b b b b b b b b bbEM(Cr(IL(?
(S)), ?
(S)), W )rrrrr rrr r r r rr r rrEM(Cr(IL(?
(S)), ?
(S)), SW )cccc c c c cc c c c c c c ccFigure 5: Tuning over the WSJ PTB (W ) fromSusanne Corpus (S).7 ConclusionsWe have presented several semi-supervisedconfidence-based training methods which havesignificantly improved performance over an ex-tant (more supervised) method, while also re-ducing the manual effort required to createtraining or tuning data.
We have shownthat given a medium-sized unlabeled partiallybracketed corpus, the confidence-based modelsachieve superior results to those achieved withEM applied to the same PGLR parse selectionmodel.
Indeed, a bracketed corpus provides flex-ibility as existing treebanks can be utilized de-spite the incompatibility between the systemgrammar and the underlying grammar of thetreebank.
Mapping an incompatible annotatedtreebank to a compatible partially-bracketedcorpus is relatively easy compared to mappingto a compatible fully-annotated corpus.An immediate benefit of this work is that(re)training parsers with incrementally-modifiedgrammars based on different linguistic frame-works should be much more straightforward ?see, for example Oepen et al (2002) for a gooddiscussion of the problem.
Furthermore, it sug-gests that it may be possible to usefully tune30a parser to a new domain with less annotationeffort.Our findings support those of Elworthy (1994)and Merialdo (1994) for POS tagging and sug-gest that EM is not always the most suit-able semi-supervised training method (espe-cially when some in-domain training data isavailable).
The confidence-based methods weresuccessful because the level of noise introduceddid not outweigh the benefit of incorporatingall derivations compatible with the bracketingin which the derivations contained a high pro-portion of correct constituents.
These findingsmay not hold if the level of bracketing availabledoes not adequately constrain the parses consid-ered ?
see Hwa (1999) for a related investigationwith EM.In future work we intend to further investigatethe problem of tuning to a new domain, giventhat minimal manual effort is a major prior-ity.
We hope to develop methods which requiredno manual annotation, for example, high preci-sion automatic partial bracketing using phrasechunking and/or named entity recognition tech-niques might yield enough information to sup-port the training methods developed here.Finally, further experiments on weighting thecontribution of each dataset might be beneficial.For instance, Bacchiani et al (2006) demon-strate imrpovements in parsing accuracy withunsupervised adaptation from unannotated dataand explore the effect of different weighting ofcounts derived from the supervised and unsu-pervised data.AcknowledgementsThe first author is funded by the Overseas Re-search Students Awards Scheme, and the Poyn-ton Scholarship awarded by the Cambridge Aus-tralia Trust in collaboration with the Cam-bridge Commonwealth Trust.
Development ofthe RASP system was and is supported by theEPSRC (grants GR/N36462, GR/N36493 andGR/T19919).ReferencesBacchiani, M., Riley, M., Roark, B. and R.Sproat (2006) ?MAP adaptation of stochas-tic grammars?, Computer Speech and Lan-guage, vol.20.1, pp.41?68.Baker, J. K. (1979) ?Trainable grammars forspeech recognition?
in Klatt, D. and Wolf,J.
(eds.
), Speech Communications Papers forthe 97th Meeting of the Acoustical Society ofAmerica, MIT, Cambridge, Massachusetts,pp.
557?550.Briscoe, E.J., J. Carroll and R. Watson (2006)?The Second Release of the RASP System?,Proceedings of ACL-Coling?06, Sydney, Aus-tralia.Carroll, J., Briscoe, T. and Sanfilippo, A.
(1998)?Parser evaluation: a survey and a newproposal?, Proceedings of LREC, Granada,pp.
447?454.Clark, S. and J. Curran (2004) ?Parsing the WSJUsing CCG and Log-Linear Models?, Pro-ceedings of 42nd Meeting of the Associationfor Computational Linguistics, Barcelona,pp.
103?110.Collins, M. (1999) Head-driven Statistical Mod-els for Natural Language Parsing, PhD Dis-sertation, University of Pennsylvania.Elworthy, D. (1994) ?Does Baum-Welch Re-estimation Help Taggers?
?, Proceedings ofANLP, Stuttgart, Germany, pp.
53?58.Gildea, D. (2001) ?Corpus variation and parserperformance?, Proceedings of EMNLP, Pitts-burgh, PA.Hockenmaier, J.
(2003) Data and models for sta-tistical parsing with Combinatory CategorialGrammar, PhD Dissertation, The Univer-sity of Edinburgh.Hwa, R. (1999) ?Supervised grammar inductionusing training data with limited constituentinformation?, Proceedings of ACL, CollegePark, Maryland, pp.
73?79.Inui, K., V. Sornlertlamvanich, H. Tanaka andT.
Tokunaga (1997) ?A new formalizationof probabilistic GLR parsing?, Proceedings31of IWPT, MIT, Cambridge, Massachusetts,pp.
123?134.King, T.H., R. Crouch, S. Riezler, M. Dalrympleand R. Kaplan (2003) ?The PARC700 De-pendency Bank?, Proceedings of LINC, Bu-dapest.Lin, D. (1998) ?Dependency-based evaluationof MINIPAR?, Proceedings of Workshop atLREC?98 on The Evaluation of Parsing Sys-tems, Granada, Spain.McClosky, D., Charniak, E. and M. Johnson(2006) ?Effective self-training for parsing?,Proceedings of HLT-NAACL, New York.Merialdo, B.
(1994) ?Tagging English Text witha Probabilistic Model?, Computational Lin-guistics, vol.20.2, pp.155?171.Miyao, Y. and J. Tsujii (2002) ?Maximum En-tropy Estimation for Feature Forests?, Pro-ceedings of HLT, San Diego, California.Oepen, S., K. Toutanova, S. Shieber, C. Man-ning, D. Flickinger, and T. Brants (2002)?The LinGO Redwoods Treebank: Motiva-tion and preliminary applications?, Proceed-ings of COLING, Taipei, Taiwan.Pereira, F and Y. Schabes (1992) ?Inside-Outside Reestimation From PartiallyBracketed Corpora?, Proceedings of ACL,Delaware.Prescher, D. (2001) ?Inside-outside estimationmeets dynamic EM?, Proceedings of 7thInt.
Workshop on Parsing Technologies(IWPT01), Beijing, China.Riezler, S., T. King, R. Kaplan, R. Crouch,J.
Maxwell III and M. Johnson (2002)?Parsing the Wall Street Journal using aLexical-Functional Grammar and Discrimi-native Estimation Techniques?, Proceedingsof 40th Annual Meeting of the Associationfor Computational Linguistics, Philadelphia,pp.
271?278.Sampson, G. (1995) English for the Computer,Oxford University Press, Oxford, UK.Siegel S. and N. J. Castellan (1988) Nonpara-metric Statistics for the Behavioural Sci-ences, 2nd edition, McGraw-Hill.Tadayoshi, H., Y. Miyao and J. Tsujii (2005)?Adapting a probabilistic disambiguationmodel of an HPSG parser to a new domain?,Proceedings of IJCNLP, Jeju Island, Korea.Tomita, M. (1987) ?An Efficient AugmentedContext-Free Parsing Algorithm?, Computa-tional Linguistics, vol.13(1?2), pp.31?46.Watson, R. and E.J.
Briscoe (2007) ?Adaptingthe RASP system for the CoNLL07 domain-adaptation task?, Proceedings of EMNLP-CoNLL-07, Prague.Watson, R., J. Carroll and E.J.
Briscoe (2005)?Efficient extraction of grammatical rela-tions?, Proceedings of 9th Int.
Workshop onParsing Technologies (IWPT?05), Vancou-ver, Ca..32
