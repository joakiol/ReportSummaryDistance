Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1281?1286,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational Linguistics#WhyIStayed, #WhyILeft: Microblogging to Make Sense of Domestic AbuseNicolas Schrading1Cecilia O. Alm2Ray Ptucha1Christopher M. Homan31Kate Gleason College of Engineering, Rochester Institute of Technology2College of Liberal Arts, Rochester Institute of Technology3Golisano College of Computing and Information Sciences, Rochester Institute of Technology{jxs8172?|coagla?|rwpeec?|cmh?
}@{?rit.edu|?cs.rit.edu}AbstractIn September 2014, Twitter users unequivo-cally reacted to the Ray Rice assault scan-dal by unleashing personal stories of domes-tic abuse via the hashtags #WhyIStayed or#WhyILeft.
We explore at a macro-levelfirsthand accounts of domestic abuse from asubstantial, balanced corpus of tweeted in-stances designated with these tags.
To seekinsights into the reasons victims give for stay-ing in vs. leaving abusive relationships, weanalyze the corpus using linguistically moti-vated methods.
We also report on an annota-tion study for corpus assessment.
We performclassification, contributing a classifier that dis-criminates between the two hashtags excep-tionally well at 82% accuracy with a substan-tial error reduction over its baseline.1 IntroductionDomestic abuse is a problem of pandemic propor-tions; nearly 25% of females and 7.6% of maleshave been raped or physically assaulted by an inti-mate partner (Tjaden and Thoennes, 2000).
Thesenumbers only include physical violence; psycholog-ical abuse and other forms of domestic abuse may beeven more prevalent.
There is thus an urgent need tobetter understand and characterize domestic abuse,in order to provide resources for victims and effi-ciently implement preventative measures.Survey methods exploring domestic abuse involveconsiderable time and investment, and may sufferfrom under-reporting, due to the taboo and stress-ful nature of abuse.
Additionally, many may nothave the option of directly seeking clinical help.Social media may provide a less intimidating andmore accessible channel for reporting, collectivelyprocessing, and making sense of traumatic and stig-matizing experiences (Homan et al, 2014; Walther,1996).
Such data has been used for analyzing andpredicting distinct societal and health issues, aimedat improving the understanding of wide-reachingsocietal concerns.
For instance, Choudhury et al(2013) predicted the onset of depression from usertweets, while other studies have modeled distress(Homan et al, 2014; Lehrman et al, 2012).
Xuet al (2013) used Twitter data to identify bullyinglanguage, then analyzed the characteristics of thesetweets, and forecasted if a tweet would be deletedout of regret.In September 2014, in the wake of the Ray Riceassault scandal1and the negative public reaction tothe victim?s decision to stay and support her abuser,Twitter users unequivocally reacted in a viral dis-cussion of domestic abuse, defending the victim us-ing the hashtag #WhyIStayed and contrasting thosewith #WhyILeft.
Such narrative sharing may have acathartic and therapeutic effect, extending the viralreach of the trend.Analysis of the linguistic structures embedded inthese tweet instances provides insight into the criti-cal reasons that victims of domestic abuse report forchoosing to stay or leave.
Trained classifiers agreewith these linguistic structures, adding evidence thatthese social media texts provide valuable insightsinto domestic abuse.1http://www.sbnation.com/nfl/2014/5/23/5744964/ray-rice-arrest-assault-statement-apology-ravens.1281Figure 1: Tweet count per hour with #WhyIStayed (dotted) or #WhyILeft (solid) from 9/8 to 9/12.
Times in EST,vertical lines mark 12 hour periods, with label corresponding to its left line.
Spam removed, includes meta tweets.2 DataWe collected a new corpus of tweets using the Twit-ter and Topsy2application programming interfaces.The corpus spans the beginning of September (thestart of the trend) to the beginning of October, 2014.We fully rehydrated the tweets (to update the retweetcount, etc.)
at the end of the collection period.
Fig-ure 1 displays the behavior from the initial days ofthis trend.
Due to its viral nature, the majority oftweets are from the first week of the trend?s creation.2.1 PreprocessingWe removed spam tweets based on the usernames ofthe most prevalent spammers, as well as key spamhashtags.3We also removed tweets related to akey controversy, in which the Twitter account forDiGiorno Pizza (ignorant of the trend?s meaning)tweeted #WhyIStayed You had pizza.4This resultedin over 57,000 unique tweets in the corpus.Many tweets in the dataset were reflections on thetrend itself or contained messages of support to theusers sharing their stories, for example, Not usuallya fan of hashtag trends, but #WhyIStayed is incredi-bly powerful.
#NFL #RayRice.5These tweets, heredenoted meta-tweets, were often retweeted, but theyrarely contained reasons for staying or leaving (ourinterest), so we filtered them out by keyword.6Insection 2.3 we empirically explore the remaining in-stances.2For outside Twitter?s history, http://topsy.com/3Such as #MTVEMA, #AppleWatch, #CMWorld.4Removed by keywords pizza, digiorno.5Illustrative tweet examples were anonymized and we pur-posefully attempted to minimize inclusion of sensitive content.6Including janay/ray rice, football, tweets, trend, video, etc.2.2 Extracting Gold Standard LabelsTypically, users provided reasons for staying andleaving, with the reasons prefixed by or appendedwith the hashtags #WhyIStayed or #WhyILeft as inthis example: #WhyIStayed because he told me noone else would love me.
#WhyILeft because I gainedthe courage to love myself.
Regular expressionsmatched these structures and for tweets marked byboth tags, split them into multiple instances, labeledwith their respective tag.
If the tweet contained onlyone of the target hashtags, the instance was labeledwith that hashtag.
If the tweet contained both hash-tags but did not match with any of the regular ex-pressions, it was excluded to ensure data quality.The resulting corpus comprised 24,861 #WhyIS-tayed and 8,767 #WhyILeft labeled datapoints.
Theclass imbalance may be a result of the origins of thetrend rather than an indicator that more victims staythan leave.
The tweet that started the trend containedonly the hashtag #WhyIStayed, and media reportingon the trend tended to refer to it as the ?#WhyIS-tayed phenomenon.?
As Figure 1 shows, the first#WhyILeft tweet occurred hours after the #WhyIS-tayed trend had taken off, and never gained as muchuse.
By this reasoning, we concluded that an evenset of data would be appropriate, and enable us touse the ratio metric in experiments discussed in thispaper, as well as compare themes in the two sets.
Byrandom sampling of #WhyIStayed, a balanced set of8,767 examples per class was obtained, resulting ina binary 50% baseline.
From this set, 15% were heldout as a final testset, to be considered after a tuningprocedure with the remaining 85% devset.12822.3 Annotation StudyFour people (co-authors) annotated a random sam-ple of 1000 instances from the devset, to furthercharacterize the filtered corpus and to assess the au-tomated extraction of gold standard labels.
Thisrandom subset is composed of 47% #WhyIStayedand 53% #WhyILeft gold standard samples.
Overallagreement overlap was 77% and Randolph?s free-marginal multirater kappa (Warrens, 2010) scorewas 0.72.
According to the annotations in this ran-dom sample, on average 36% of the instances arereasons for staying (S), 44% are reasons for leaving(L), 12% are meta comments (M), 2% are jokes (J),2% are ads (A), and 4% do not match prior cate-gories (O).
Table 1 shows that most related directlyto S or L, with annotators identifying more clearlyL.
Of interest are examples in which annotators didnot agree, as these are indicative of problems in thedata, and are samples that a classifier will likely la-bel incorrectly.
The tweet because i was slowly dy-ing anyway was marked by two annotators as S andtwo annotators as L. Did the victim have no hopeleft and decide to stay?
Or did the victim decide thatsince they were ?slowly dying anyway?
they couldattempt to leave despite the possibility of potentiallybeing killed in the attempt?
The ground truth labelis #WhyILeft.
Another example with two annotatorslabeling as S and two as L is two years of bliss, fol-lowed by uncertainty and fear.
This tweet?s label is#WhyIStayed.
The limited context from these sam-ples makes it difficult to interpret fully, and causeshuman annotators to fail; however, most cases con-tain clear enough reasoning to interpret correctly.A J L M O S#L .01 .01 .78 .11 .03 .07A1#S .01 .03 .10 .21 .02 .63#L .02 .01 .72 .06 .09 .10A2#S .03 .01 .07 .16 .10 .63#L .00 .02 .77 .09 0 .11A3#S .01 .04 .06 .21 0 .68#L .02 .01 .75 .05 .04 .14A4#S .03 .01 .16 .12 .05 .63Table 1: Confusion matrices of all 4 annotators, com-pared to the gold standard.
Annotators mostly identifiedreasons for staying or leaving, and only a small fractionwere unrelated.
#L=#WhyILeft, #S=#WhyIStayed.3 Methods for Exploring Reasons3.1 Cleaning and Classifier TuningAll experiments used the same cleaned data: re-moving hashtags, replacing URLs with the tokenurl and user mentions with @mention, and replac-ing common emoticons with a sentiment indicator:emotsent{p|n|neut} for positive/negative/neutral.Informal register was expanded to standard Englishforms using a slang dictionary.7Classifier tuning in-volved 5-fold cross-validation and selecting the bestparameters based on the mean accuracy.
For held-out data testing the full devset was used for training.3.2 Analysis of VocabularyWe examined the vocabulary in use in the data ofthe two hashtag sets by creating a frequency dis-tribution of all unigrams after stoplisting and low-ercasing.
The wordcloud unigrams in Figure 2 areweighted by their relative frequency.
These word-clouds hint at the reasons; however, decontextual-ized unigrams lead to confusion.
For example, whydoes left appear in both?
Other experiments weredone to provide context and expand analysis.Figure 2: A wordcloud of unigrams, weighted by uni-gram frequencies, for (top) #WhyIStayed instances and(bottom) #WhyILeft instances.87http://www.noslang.com/8Created using http://amueller.github.io/word_cloud/1283Most discriminative abuser onto victim verbs Legendconvince find isolate kick kill love manipulate promise want #WhyIStayed0.96 1 0.93 1 0.91 0.95 1 0.83 0.93 #WhyILeftMost discriminative victim as subject verbsbelieve choose decide felt know learn realize think want0.81 1 1 0.79 0.82 1 0.99 0.93 0.83Table 2: Discriminative verbs for abuser onto victim and victim as subject structures.3.3 Analysis of Subject-Verb-Object StructuresData inspection suggested that many users explainedtheir reasons using a Subject-Verb-Object (SVO)structure, in which the abuser is doing somethingto the victim, or the victim is explaining some-thing about the abuser or oneself.9We used theopen-source tools Tweeboparser (Kong et al, 2014)and TurboParser (Martins et al, 2013) to heuristi-cally extract syntactic dependencies, constrained bypronomial usage.
Both parsers performed similarly,most likely due to the well-formed English in thecorpus.
While tweets are known for non-standardforms, the seriousness of the discourse domain mayhave encouraged more standard writing conventions.Using TurboParser, we conducted an analysis forboth male and female genders acting as the abuserin the subject position.
Starting at the lemmatizedpredicate verb in each dependency parse, if the pred-icate verb followed an abuser subject word10perthe dependency links, and preceded a victim objectword,11it was added to a conditional frequency dis-tribution, with the two classes as conditions.
Thesestructures are here denoted abuser onto victim.
Weused similar methods to extract structures in whichthe victim is the subject.
Instances with femaleabusers were rare, and statistical gender differencescould not be pursued.
Accordingly, both genders?frequency counts were combined.
Discriminativepredicates from these conditional frequency distri-butions were determined by equation (1).
In Table2 we report on those where the ratio is greater than0.75 and the total count exceeds a threshold to avoidbias towards lower frequency verbs.ratio =countlargerOfCountscountleft+ countstayed(1)9Example: He hurt my child S: He, V: hurt, O: my child.10Male abuser: he, his, my bf, etc.
Female: she, her, etc.11Male victim: me, my, him, etc.
Female: me, my, her, etc.3.4 Classification ExperimentsWe examined the usefulness of the SVO struc-tures, using subsets of the devset and testset hav-ing SVO structures (10% of the instances in total).While 10% is not a large proportion overall, giventhe massive number of possible dependency struc-tures, it is a pattern worth examining ?
not onlyfor corpus analytics but also classification, partic-ularly as these SVO structures provide insight intothe abuser-victim relationship.
A linear SVM usingboolean SVO features performed best (C=1), obtain-ing 70% ?
2% accuracy on the devset and 73% ac-curacy on the testset.
The weights assigned to fea-tures by a Linear SVM are indicative of their impor-tance (Guyon et al, 2002).
Here, the top featurespresented as (S,V,O) for #WhyIStayed were: (he,introduce, me), (i, think, my), (he, convince, me), (i,believe, his), and (he, beat, my).
For #WhyILeft theywere (he, choke, me), (i, beg, me), (he, want, my), (i,realize, my), and (i, listen, my).The SVO structures capture meaning related tostaying and leaving, but are limited in their data cov-erage.
Another experiment explored an extendedfeature set including uni-, bi-, and trigrams in sublin-ear tf?
idf vectors, tweet instance character length,its retweet count, and SVO structures.
We com-pared Na?
?ve Bayes, Linear SVM, and RBF SVMclassifiers from the Scikit-learn package (Pedregosaet al, 2011).
The RBF SVM performed slightly bet-ter than the others, achieving a maximum accuracyof 81% ?
.3% on the devset and 82% on the test-set.12,13Feature ablation, following the procedure inFraser et al (2014), was utilized to determine themost important features for the classifier, the results12Tuned parameters: max df = 11%, C=10, gamma=1.13Dimensionality reduction with Supervised Locality Pre-serving Projections (SLPP) (Ptucha and Savakis, 2014) was at-tempted, but this did not improve results.1284of which can be seen in Table 3.Removed Remaining Features % AccNG+E+IR+TL+RT+SVO 81.90SVO NG+E+IR+TL+RT 82.09TL NG+E+IR+RT 82.21E NG+IR+RT 82.21RT NG+IR 82.13IR NG 81.48Table 3: Feature ablation study with an RBF SVM and nodimensionality reduction.
NG = ngrams, E = emoticonreplacement, IR = informal register replacement, TL =tweet length, RT = retweet count, SVO = subject-verb-object structures.
% Acc is accuracy on the testset.Interestingly, the SVO features combined with n-grams worsened performance slightly, perhaps dueto trigrams capturing the majority of SVO cases.The highest accuracy, 82.21% on the testset, couldbe achieved with a combination of ngrams, infor-mal register replacement, and retweet count.
How-ever the vast majority of cases can be classified accu-rately with ngrams alone.
Emoticons may not havecontributed to performance since they were rare inthe corpus.
Standardizing non-standard forms pre-sumably helped the SVM slightly by boosting thefrequency counts of ngrams while removing non-standard ngrams.
Tweet length reduced accuracyslightly, while the number of retweets helped.4 DiscussionFrom the analyses of SVO structures, word-clouds, and Linear SVM weights, interesting micro-narratives of staying and leaving emerge.
Victimsreport staying in abusive relationships due to cogni-tive manipulation, as indicated by a predominance ofverbs including manipulate, isolate, convince, think,believe, felt while report leaving when experiencingor fearing physical violence, via predicates such askill and kick.
They also report staying when in direfinancial straits (money), when attempting to keepthe nuclear family united (family, marriage) or whenexperiencing shame about their situation (ashamed,shame).
They report leaving when threats are madetowards loved-ones (son, daughter), gain agency(choose, decide), realize their situation or self-worth(realize, learn, worth, deserve, finally, better), orgain support from friends or family (courage, sup-port, help).
Importantly, such reasons for stayingare validated in the clinical literature (Buel, 1999).5 ConclusionWe discuss and analyze a filtered, balanced corpushaving the hashtags #WhyIStayed or #WhyILeft.Our analysis reveals micro-narratives in tweeted rea-sons for staying vs. leaving.
Our findings are con-sistent across various methods, correspond to obser-vations in the clinical literature, and affirm the rele-vance of NLP for exploring issues of social impor-tance in social media.
Future work will focus onimproving SVO extraction, especially adding con-sideration for negations of predicate verbs.
In ad-dition we will analyse other hashtags in use in thetrend and perform further analysis of the trend itself,implement advanced text normalization rather thanrelying on a dictionary, and determine the roles fea-tures from linked webpages and FrameNet or othersemantic resources play in making sense of domes-tic abuse.6 AcknowledgementThis work was supported in part by a Golisano Col-lege of Computing and Information Sciences KodakEndowed Chair Fund Health Information Technol-ogy Strategic Initiative Grant and NSF Award #SES-1111016.ReferencesSarah M. Buel.
1999.
Fifty obstacles to leaving,a.k.a, why abuse victims stay.
The Colorado Lawyer,28(10):19?28, Oct.Munmun De Choudhury, Scott Counts, Eric Horvitz,and Michael Gamon.
2013.
Predicting depressionvia social media.
In Proceedings of the 7th Interna-tional AAAI Conference on Weblogs and Social Media(ICWSM), Cambridge, Massachusetts, July.
Associa-tion for the Advancement of Artificial Intelligence.Kathleen C. Fraser, Graeme Hirst, Naida L. Graham,Jed A. Meltzer, Sandra E. Black, and Elizabeth Ro-chon.
2014.
Comparison of different feature sets foridentification of variants in progressive aphasia.
InProceedings of the Workshop on Computational Lin-guistics and Clinical Psychology: From Linguistic Sig-nal to Clinical Reality, pages 17?26, Baltimore, Mary-1285land, USA, June.
Association for Computational Lin-guistics.Isabelle Guyon, Jason Weston, Stephen Barnhill, andVladimir Vapnik.
2002.
Gene selection for cancerclassification using support vector machines.
MachineLearning, 46(1-3):389?422, March.Christopher Homan, Ravdeep Johar, Tong Liu, MeganLytle, Vincent Silenzio, and Cecilia Ovesdotter Alm.2014.
Toward macro-insights for suicide prevention:Analyzing fine-grained distress at scale.
In Proceed-ings of the Workshop on Computational Linguisticsand Clinical Psychology: From Linguistic Signal toClinical Reality, pages 107?117, Baltimore, Mary-land, USA, June.
Association for Computational Lin-guistics.Lingpeng Kong, Nathan Schneider, SwabhaSwayamdipta, Archna Bhatia, Chris Dyer, andNoah A. Smith.
2014.
A dependency parser fortweets.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing(EMNLP), pages 1001?1012, Doha, Qatar, October.Association for Computational Linguistics.Michael Thaul Lehrman, Cecilia Ovesdotter Alm, andRub?en A. Proa?no.
2012.
Detecting distressed andnon-distressed affect states in short forum texts.
InProceedings of the Second Workshop on Language inSocial Media, LSM ?12, pages 9?18, Stroudsburg, PA,USA.
Association for Computational Linguistics.Andre Martins, Miguel Almeida, and Noah A. Smith.2013.
Turning on the turbo: Fast third-order non-projective turbo parsers.
In Proceedings of the 51stAnnual Meeting of the Association for ComputationalLinguistics (Volume 2: Short Papers), pages 617?622,Sofia, Bulgaria, August.
Association for Computa-tional Linguistics.Fabian Pedregosa, Ga?el Varoquaux., Alexandre Gram-fort, Vincent Michel, Bertrand Thirion, Olivier Grisel,Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vin-cent Dubourg, Jake Vanderplas, Alexandre Passos,David Cournapeau, Matthieu Brucher, Matthieu Per-rot, and?Edouard Duchesnay.
2011.
Scikit-learn: Ma-chine learning in Python.
Journal of Machine Learn-ing Research, 12:2825?2830.Raymond Ptucha and Andreas Savakis.
2014.
LGE-KSVD: Robust sparse representation classification.IEEE Transactions on Image Processing, 23(4):1737?1750, April.Patricia Tjaden and Nancy Thoennes.
2000.
Extent, na-ture, and consequences of intimate partner violence:Findings from the national violence against womensurvey.
Technical Report NCJ 181867, National In-stitute of Justice, Centers for Disease Control and Pre-vention, Washington, DC.Joseph Walther.
1996.
Computer-mediated communica-tion: Impersonal, interpersonal, and hyperpersonal in-teraction.
Communication Research, 23(1):3?43, Feb.Matthijs J. Warrens.
2010.
Inequalities between multi-rater kappas.
Advances in Data Analysis and Classifi-cation, 4(4):271?286.Jun-Ming Xu, Benjamin Burchfiel, Xiaojin Zhu, andAmy Bellmore.
2013.
An examination of regret inbullying tweets.
In Proceedings of the 2013 Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics: Human LanguageTechnologies, pages 697?702, Atlanta, Georgia, June.Association for Computational Linguistics.1286
