Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219?228,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPFeature-Rich Translation by Quasi-Synchronous Lattice ParsingKevin Gimpel and Noah A. SmithLanguage Technologies InstituteSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213, USA{kgimpel,nasmith}@cs.cmu.eduAbstractWe present a machine translation frame-work that can incorporate arbitrary fea-tures of both input and output sentences.The core of the approach is a novel de-coder based on lattice parsing with quasi-synchronous grammar (Smith and Eis-ner, 2006), a syntactic formalism thatdoes not require source and target treesto be isomorphic.
Using generic approx-imate dynamic programming techniques,this decoder can handle ?non-local?
fea-tures.
Similar approximate inference tech-niques support efficient parameter esti-mation with hidden variables.
We usethe decoder to conduct controlled exper-iments on a German-to-English transla-tion task, to compare lexical phrase, syn-tax, and combined models, and to mea-sure effects of various restrictions on non-isomorphism.1 IntroductionWe have seen rapid recent progress in machinetranslation through the use of rich features and thedevelopment of improved decoding algorithms,often based on grammatical formalisms.1If weview MT as a machine learning problem, featuresand formalisms imply structural independence as-sumptions, which are in turn exploited by efficientinference algorithms, including decoders (Koehnet al, 2003; Yamada and Knight, 2001).
Hence atension is visible in the many recent research ef-forts aiming to decode with ?non-local?
features(Chiang, 2007; Huang and Chiang, 2007).Lopez (2009) recently argued for a separationbetween features/formalisms (and the indepen-1Informally, features are ?parts?
of a parallel sentence pairand/or their mutual derivation structure (trees, alignments,etc.).
Features are often implied by a choice of formalism.dence assumptions they imply) from inference al-gorithms in MT; this separation is widely appreci-ated in machine learning.
Here we take first stepstoward such a ?universal?
decoder, making the fol-lowing contributions:Arbitrary feature model (?2): We define a sin-gle, direct log-linear translation model (Papineniet al, 1997; Och and Ney, 2002) that encodes mostpopular MT features and can be used to encodeany features on source and target sentences, de-pendency trees, and alignments.
The trees are op-tional and can be easily removed, allowing sim-ulation of ?string-to-tree,?
?tree-to-string,?
?tree-to-tree,?
and ?phrase-based?
models, among manyothers.
We follow the widespread use of log-linearmodeling for direct translation modeling; the nov-elty is in the use of richer feature sets than havebeen previously used in a single model.Decoding as QG parsing (?3?4): We present anovel decoder based on lattice parsing with quasi-synchronous grammar (QG; Smith and Eisner,2006).2Further, we exploit generic approximateinference techniques to incorporate arbitrary ?non-local?
features in the dynamic programming algo-rithm (Chiang, 2007; Gimpel and Smith, 2009).Parameter estimation (?5): We exploit simi-lar approximate inference methods in regularizedpseudolikelihood estimation (Besag, 1975) withhidden variables to discriminatively and efficientlytrain our model.
Because we start with inference(the key subroutine in training), many other learn-ing algorithms are possible.Experimental platform (?6): The flexibilityof our model/decoder permits carefully controlledexperiments.
We compare lexical phrase and de-pendency syntax features, as well as a novel com-2To date, QG has been used for word alignment (Smithand Eisner, 2006), adaptation and projection in parsing(Smith and Eisner, 2009), and various monolingual recog-nition and scoring tasks (Wang et al, 2007; Das and Smith,2009); this paper represents its first application to MT.219?, T source and target language vocabularies, respectivelyTrans : ?
?
{NULL} ?
2Tfunction mapping each source word to target words to which it may translates = ?s0, .
.
.
, sn?
?
?nsource language sentence (s0is the NULL word)t = ?t1, .
.
.
, tm?
?
Tmtarget language sentence, translation of s?s: {1, .
.
.
, n} ?
{0, .
.
.
, n} dependency tree of s, where ?s(i) is the index of the parent of si(0 is the root, $)?t: {1, .
.
.
,m} ?
{0, .
.
.
,m} dependency tree of t, where ?t(i) is the index of the parent of ti(0 is the root, $)a : {1, .
.
.
,m} ?
2{1,...,n}alignments from words in t to words in s; ?
denotes alignment to NULL?
parameters of the modelgtrans(s,a, t) lexical translation features (?2.1):flex(s, t) word-to-word translation features for translating s as tfphr(sji, t`k) phrase-to-phrase translation features for translating sjias t`kglm(t) language model features (?2.2):fN(tjj?N+1) N -gram probabilitiesgsyn(t, ?t) target syntactic features (?2.3):fatt(t, j, t?, k) syntactic features for attaching target word t?at position k to target word t at position jfval(t, j, I) syntactic valence features with word t at position j having children I ?
{1, .
.
.
,m}greor(s, ?s,a, t, ?t) reordering features (?2.4):fdist(i, j) distortion features for a source word at position i aligned to a target word at position jgtree2(?s,a, ?t) tree-to-tree syntactic features (?3):fqg(i, i?, j, k) configuration features for source pair si/si?being aligned to target pair tj/tkgcov(a) coverage features (?4.2)fscov(a), fzth(a), fsunc(a) counters for ?covering?
each s word each time, the zth time, and leaving it ?uncovered?Table 1: Key notation.
Feature factorings are elaborated in Tab.
2.bination of the two.
We quantify the effectsof our approximate inference.
We explore theeffects of various ways of restricting syntacticnon-isomorphism between source and target treesthrough the QG.
We do not report state-of-the-artperformance, but these experiments reveal inter-esting trends that will inform continued research.2 Model(Table 1 explains notation.)
Given a sentence sand its parse tree ?s, we formulate the translationproblem as finding the target sentence t?
(alongwith its parse tree ?
?tand alignment a?to thesource tree) such that3?t?, ??t,a??
= argmax?t,?t,a?p(t, ?t,a | s, ?s) (1)In order to include overlapping features and permithidden variables during training, we use a singleglobally-normalized conditional log-linear model.That is, p(t, ?t,a | s, ?s) =exp{?>g(s, ?s,a, t, ?t)}?a?,t?,?
?texp{?>g(s, ?s,a?, t?, ?
?t)}(2)where the g are arbitrary feature functions and the?
are feature weights.
If one or both parse trees orthe word alignments are unavailable, they can beignored or marginalized out as hidden variables.In a log-linear model over structured objects,the choice of feature functions g has a huge effect3We assume in this work that s is parsed.
In principle, wemight include source-side parsing as part of decoding.on the feasibility of inference, including decoding.Typically these feature functions are chosen to fac-tor into local parts of the overall structure.
Wenext define some key features used in current MTsystems, explaining how they factor.
We will usesubscripts on g to denote different groups of fea-tures, which may depend on subsets of the struc-tures t, ?t, a, s, and ?s.
When these features fac-tor into parts, we will use f to denote the factoredvectors, so that if x is an object that breaks intoparts {xi}i, then g(x) =?if(xi).42.1 Lexical TranslationsClassical lexical translation features depend on sand t and the alignment a between them.
The sim-plest are word-to-word features, estimated as theconditional probabilities p(t | s) and p(s | t) fors ?
?
and t ?
T. Phrase-to-phrase features gen-eralize these, estimated as p(t?| s?)
and p(s?| t?
)where s?
(respectively, t?)
is a substring of s (t).A major difference between the phrase featuresused in this work and those used elsewhere isthat we do not assume that phrases segment intodisjoint parts of the source and target sentences4There are two conventional definitions of feature func-tions.
One is to let the range of these functions be conditionalprobability estimates (Och and Ney, 2002).
These estimatesare usually heuristic and inconsistent (Koehn et al, 2003).An alternative is to instantiate features for different structuralpatterns (Liang et al, 2006; Blunsom et al, 2008).
This offersmore expressive power but may require much more trainingdata to avoid overfitting.
For this reason, and to keep trainingfast, we opt for the former convention, though our decodercan handle both, and the factorings we describe are agnosticabout this choice.220(Koehn et al, 2003); they can overlap.5Addi-tionally, since phrase features can be any func-tion of words and alignments, we permit featuresthat consider phrase pairs in which a target wordoutside the target phrase aligns to a source wordinside the source phrase, as well as phrase pairswith gaps (Chiang, 2005; Ittycheriah and Roukos,2007).Lexical translation features factor as in Eq.
3(Tab.
2).
We score all phrase pairs in a sentencepair that pair a target phrase with the smallestsource phrase that contains all of the alignments inthe target phrase; if?k:i?k?ja(k) = ?, no phrasefeature fires for tji.2.2 N -gram Language ModelN -gram language models have become standardin machine translation systems.
For bigrams andtrigrams (used in this paper), the factoring is inEq.
4 (Tab.
2).2.3 Target SyntaxThere have been many features proposed that con-sider source- and target-language syntax duringtranslation.
Syntax-based MT systems often usefeatures on grammar rules, frequently maximumlikelihood estimates of conditional probabilities ina probabilistic grammar, but other syntactic fea-tures are possible.
For example, Quirk et al(2005) use features involving phrases and source-side dependency trees and Mi et al (2008) usefeatures from a forest of parses of the source sen-tence.
There is also substantial work in the useof target-side syntax (Galley et al, 2006; Marcuet al, 2006; Shen et al, 2008).
In addition, re-searchers have recently added syntactic features tophrase-based and hierarchical phrase-based mod-els (Gimpel and Smith, 2008; Haque et al, 2009;Chiang et al, 2008).In this work, we focus on syntactic features oftarget-side dependency trees, ?t, along with thewords t. These include attachment features thatrelate a word to its syntactic parent, and valencefeatures.
They factor as in Eq.
5 (Tab.
2).
Featuresthat consider only target-side syntax and wordswithout considering s can be seen as ?syntacticlanguage model?
features (Shen et al, 2008).5Segmentation might be modeled as a hidden variable infuture work.gtrans(s,a, t) =Pmj=1Pi?a(j)flex(si, tj) (3)+Pi,j:1?i<j?mfphr(slast(i,j)first(i,j), tji)glm(t) =PN?
{2,3}Pm+1j=1fN(tjj?N+1) (4)gsyn(t, ?t) =Pmj=1fatt(tj, j, t?t(j), ?t(j))+fval(tj, j, ?
?1t(j)) (5)greor(s, ?s,a, t, ?t) =Pmj=1Pi?a(j)fdist(i, j) (6)gtree2(?s,a, ?t) =mXj=1fqg(a(j),a(?t(j)), j, ?t(j)) (7)Table 2: Factoring of global feature collections g intof .
xjidenotes ?xi, .
.
.
xj?
in sequence x = ?x1, .
.
.
?.first(i, j) = mink:i?k?j(min(a(k))) and last(i, j) =maxk:i?k?j(max(a(k))).2.4 ReorderingReordering features take many forms in MT.
Inphrase-based systems, reordering is accomplishedboth within phrase pairs (local reordering) aswell as through distance-based distortion mod-els (Koehn et al, 2003) and lexicalized reorder-ing models (Koehn et al, 2007).
In syntax-basedsystems, reordering is typically parameterized bygrammar rules.
For generality we permit thesefeatures to ?see?
all structures and denote themgreor(s, ?s,a, t, ?t).
Eq.
6 (Tab.
2) shows a factor-ing of reordering features based on absolute posi-tions of aligned words.We turn next to the ?backbone?
model for ourdecoder; the formalism and the properties of itsdecoding algorithm will inspire two additional setsof features.3 Quasi-Synchronous GrammarsA quasi-synchronous dependency grammar(QDG; Smith and Eisner, 2006) specifies aconditional model p(t, ?t,a | s, ?s).
Given asource sentence s and its parse ?s, a QDG inducesa probabilistic monolingual dependency grammarover sentences ?inspired?
by the source sentenceand tree.
We denote this grammar by Gs,?s; its(weighted) language is the set of translations of s.Each word generated by Gs,?sis annotated witha ?sense,?
which consists of zero or more wordsfrom s. The senses imply an alignment (a) be-tween words in t and words in s, or equivalently,between nodes in ?tand nodes in ?s.
In principle,any portion of ?tmay align to any portion of ?s,but in practice we often make restrictions on thealignments to simplify computation.
Smith andEisner, for example, restricted |a(j)| for all words221tjto be at most one, so that each target wordaligned to at most one source word, which we alsodo here.6Which translations are possible depends heav-ily on the configurations that the QDG permits.Formally, for a parent-child pair ?t?t(j), tj?
in ?t,we consider the relationship between a(?t(j)) anda(j), the source-side words to which t?t(j)andtjalign.
If, for example, we require that, forall j, a(?t(j)) = ?s(a(j)) or a(j) = 0, andthat the root of ?tmust align to the root of ?sor to NULL, then strict isomorphism must holdbetween ?sand ?t, and we have implemented asynchronous CF dependency grammar (Alshawiet al, 2000; Ding and Palmer, 2005).
Smith andEisner (2006) grouped all possible configurationsinto eight classes and explored the effects of per-mitting different sets of classes in word align-ment.
(?a(?t(j)) = ?s(a(j))?
corresponds totheir ?parent-child?
configuration; see Fig.
3 inSmith and Eisner (2006) for illustrations of therest.)
More generally, we can define features ontree pairs that factor into these local configura-tions, as shown in Eq.
7 (Tab.
2).Note that the QDG instantiates the model inEq.
2.
Of the features discussed in ?2, flex, fatt,fval, and fdistcan be easily incorporated into theQDG as described while respecting the indepen-dence assumptions implied by the configurationfeatures.
The others (fphr, f2, and f3) are non-local, or involve parts of the structure that, fromthe QDG?s perspective, are conditionally indepen-dent given intervening material.
Note that ?non-locality?
is relative to a choice of formalism; in ?2we did not commit to any formalism, so it is onlynow that we can describe phrase and N -gram fea-tures as non-local.
Non-local features will presenta challenge for decoding and training (?4.3).4 DecodingGiven a sentence s and its parse ?s, at decodingtime we seek the target sentence t?, the target tree?
?t, and the alignments a?that are most probable,as defined in Eq.
1.7(In ?5 we will consider k-best and all-translations variations on this prob-6I.e., from here on, a : {1, .
.
.
,m} ?
{0, .
.
.
, n} where0 denotes alignment to NULL.7Arguably, we seek argmaxtp(t | s), marginalizing outeverything else.
Approximate solutions have been proposedfor that problem in several settings (Blunsom and Osborne,2008; Sun and Tsujii, 2009); we leave their combination withour approach to future work.lem.)
As usual, the normalization constant is notrequired for decoding; it suffices to solve:?t?, ??t,a??
= argmax?t,?t,a?
?>g(s, ?s,a, t, ?t) (8)For a QDG model, the decoding problem hasnot been addressed before.
It equates to finding themost probable derivation under the s/?s-specificgrammar Gs,?s.
We solve this by lattice parsing,assuming that an upper bound on m (the lengthof t) is known.
The advantage offered by thisapproach (like most other grammar-based trans-lation approaches) is that decoding becomes dy-namic programming (DP), a technique that is bothwidely understood in NLP and for which practical,efficient, generic techniques exist.
A major advan-tage of DP is that, with small modifications, sum-ming over structures is also possible with ?inside?DP algorithms.
We will exploit this in training(?5).
Efficient summing opens up many possibili-ties for training ?, such as likelihood and pseudo-likelihood, and provides principled ways to handlehidden variables during learning.4.1 Translation as Monolingual ParsingWe decode by performing lattice parsing on a lat-tice encoding the set of possible translations.
Thelattice is a weighted ?sausage?
lattice that permitssentences up to some maximum length `; ` is de-rived from the source sentence length.
Let thestates be numbered 0 to `; states from b?`c to `are final states (for some ?
?
(0, 1)).
For everyposition between consecutive states j ?
1 and j(0 < j ?
`), and for every word siin s, andfor every word t ?
Trans(si), we instantiate anarc annotated with t and i.
The weight of such anarc is exp{?>f}, where f is the sum of featurefunctions that fire when sitranslates as t in targetposition j (e.g., flex(si, t) and fdist(i, j)).Given the lattice and Gs,?s, lattice parsingis a straightforward generalization of standardcontext-free dependency parsing DP algorithms(Eisner, 1997).
This decoder accounts for flex,fatt, fval, fdist, and fqgas local features.Figure 1 gives an example, showing a Germansentence and dependency tree from an automaticparser, an English reference, and a lattice repre-senting possible translations.
In each bundle, thearcs are listed in decreasing order according toweight and for clarity only the first five are shown.The output of the decoder consists of lattice arcs222k?nnen:cank?nnen:maysie:youes:it...vorbei:by$   k?nnen   sie   es  vorbei    leifern    morgen  fr?h  ?can you deliver it by tomorrow morning ?can     you     deliver  it    by     tomorrow morning ?CAN     YOU  IT      BY     DELIVER  TOMORROW-MORNING  ?...
... .........k?nnen:canliefern:deliversie:yousie:ites:it k?nnen:cank?nnen:canliefern:deliversie:youes:itvorbei:bymorgen:tomorrowmorgen:tomorrowliefern:deliveres:itvorbei:byfr?h:morning...es:itmorgen:tomorrowliefern:delivervorbei:byfr?h:morningfr?h:early?
:?morgen:morningkonnten:could konnten:couldes:itsie:youkonnten:might...konnten:couldn... ... ... ...sie:letsie:yousie:themes:it sie:youkonnten:could?bersetzen:translate?bersetzen:translate?bersetzen:translated?bersetzen:translate?bersetzen:translated?:?konnten:couldes:ites:it?:?es:it?:?NULL:tok?nnen:cank?nnen:maysie:youes:it...vorbei:by...
... .........k?nnen:canliefern:deliversie:yousie:ites:it k?nnen:cank?nnen:canliefern:deliversie:youes:itvorbei:bymorgen:tomorrowmorgen:tomorrowliefern:deliveres:itvorbei:byfr?h:morningfr?h:early?
:?morgen:morning...fr?h:morningmorgen:tomorrowmorgen:morningliefern:delivervorbei:by$$   k?nnen   sie   es  vorbei    leifern    morgen  fr?h  ?can     you     deliver  it    by     tomorrow morning ?$   k?nnen   sie   es  vorbei    leifern    morgen  fr?h  ?can     you     deliver  it    by     tomorrow morning ?Source:          $  konnten  sie  es  ?bersetzen  ?Reference:         could  you  translate  it  ?Decoder output:Figure 1: Decoding as lattice parsing, with the highest-scoring translation denoted by black lattice arcs (others are grayed out)and thicker blue arcs forming a dependency tree over them.selected at each position and a dependency treeover them.4.2 Source-Side Coverage FeaturesMost MT decoders enforce a notion of ?coverage?of the source sentence during translation: all partsof s should be aligned to some part of t (alignmentto NULL incurs an explicit cost).
Phrase-based sys-tems such as Moses (Koehn et al, 2007) explic-itly search for the highest-scoring string in whichall source words are translated.
Systems basedon synchronous grammars proceed by parsing thesource sentence with the synchronous grammar,ensuring that every phrase and word has an ana-logue in ?t(or a deliberate choice is made by thedecoder to translate it to NULL).
In such sys-tems, we do not need to use features to implementsource-side coverage, as it is assumed as a hardconstraint always respected by the decoder.Our QDG decoder has no way to enforce cov-erage; it does not track any kind of state in ?sapart from a single recently aligned word.
Thisis a problem with other direct translation models,such as IBM model 1 used as a direct model ratherthan a channel model (Brown et al, 1993).
Thissacrifice is the result of our choice to use a condi-tional model (?2).The solution is to introduce a set of coveragefeatures gcov(a).
Here, these include:?
A counter for the number of times each sourceword is covered: fscov(a) =?ni=1|a?1(i)|.?
Features that fire once when a source word iscovered the zth time (z ?
{2, 3, 4}) and fireagain all subsequent times it is covered; theseare denoted f2nd, f3rd, and f4th.?
A counter of uncovered source words:fsunc(a) =?ni=1?
(|a?1(i)|, 0).Of these, only fscovis local.4.3 Non-Local FeaturesThe lattice QDG parsing decoder incorporatesmany of the features we have discussed, but notall of them.
Phrase lexicon features fphr, lan-guage model features fNfor N > 1, and mostcoverage features are non-local with respect to ourQDG.
Recently Chiang (2007) introduced ?cubepruning?
as an approximate decoding method thatextends a DP decoder with the ability to incorpo-rate features that break the Markovian indepen-dence assumptions DP exploits.
Techniques likecube pruning can be used to include the non-localfeatures in our decoder.85 TrainingTraining requires us to learn values for the param-eters ?
in Eq.
2.
Given T training examples of theform ?t(i), ?
(i)t, s(i), ?
(i)s?, for i = 1, ..., T , max-imum likelihood estimation for this model con-sists of solving Eq.
9 (Tab.
3).9Note that the8A full discussion is omitted for space, but in fact we use?cube decoding,?
a slightly less approximate, slightly moreexpensive method that is more closely related to the approxi-mate inference methods we use for training, discussed in ?5.9In practice, we regularize by including a term ?c???22.223LL(?)
=TXi=1log p(t(i), ?
(i)t| s(i), ?
(i)s) =TXi=1logPaexp{?>g(s(i), ?
(i)s,a, t(i), ?
(i)t)}Pt,?t,aexp{?>g(s(i), ?
(i)s,a, t, ?t)}=TXi=1log?numerator??denominator?(9)PL(?)
=TXi=1log?Xap(t(i),a | ?
(i)t, s(i), ?(i)s)?+TXi=1log?Xap(?
(i)t,a | t(i), s(i), ?(i)s)?(10)?denominator?
ofterm 1 in Eq.
10=nXi=0Xt??Trans(si)S(?
?1t(0), i, t?)
?
expn?>`flex(si, t?)
+ fatt($, 0, t?, k) + fqg(0, i, 0, k)?o(11)S(j, i, t) =Yk???1t(j)nXi?=0Xt??Trans(si?
)S(k, i?, t?)
?
exp?
?>?flex(si?, t?)
+ fatt(t, j, t?, k)+fval(t, j, ?
?1t(j)) + fqg(i, i?, j, k)?ff(12)S(j, i, t) = expn?>`fval(t, j, ?
?1t(j))?oif ?
?1t(j) = ?
(13)Table 3: Eq.
9: Log-likelihood.
Eq.
10: Pseudolikelihood.
In both cases we maximize w.r.t.
?.
Eqs.
11?13: Recursive DPequations for summing over t and a.alignments are treated as a hidden variable to bemarginalized out.10Optimization problems of thisform are by now widely known in NLP (Koo andCollins, 2005), and have recently been used formachine translation as well (Blunsom et al, 2008).Such problems are typically solved using varia-tions of gradient ascent; in our experiments, wewill use an online method called stochastic gra-dient ascent (SGA).
This requires us to calculatethe function?s gradient (vector of first derivatives)with respect to ?.11Computing the numerator in Eq.
9 involvessumming over all possible alignments; with QDGand a hard bound of 1 on |a(j)| for all j, a fast?inside?
DP solution is known (Smith and Eisner,2006; Wang et al, 2007).
It runs in O(mn2) timeand O(mn) space.Computing the denominator in Eq.
9 requiressumming over all word sequences and depen-dency trees for the target language sentence andall word alignments between the sentences.
Witha maximum length imposed, this is tractable us-ing the ?inside?
version of the maximizing DP al-gorithm of Sec.
4, but it is prohibitively expen-sive.
We therefore optimize pseudo-likelihood in-stead, making the following approximation (Be-10Alignments could be supplied by automatic word align-ment algorithms.
We chose to leave them hidden so that wecould make the best use of our parsed training data when con-figuration constraints are imposed, since it is not always pos-sible to reconcile automatic word alignments with automaticparses.11When the function?s value is computed by ?inside?
DP,the corresponding ?outside?
algorithm can be used to obtainthe gradient.
Because outside algorithms can be automati-cally derived from inside ones, we discuss only inside algo-rithms in this paper; see Eisner et al (2005).sag, 1975):p(t, ?t| s, ?s) ?
p(t | ?t, s, ?s) ?
p(?t| t, s, ?s)Plugging this into Eq.
9, we arrive at Eq.
10(Tab.
3).
The two parenthesized terms in Eq.
10each have their own numerators and denomina-tors (not shown).
The numerators are identical toeach other and to that in Eq.
9.
The denominatorsare much more manageable than in Eq.
9, neverrequiring summation over more than two struc-tures at a time.
We must sum over target word se-quences and word alignments (with fixed ?t), andseparately over target trees and word alignments(with fixed t).5.1 Summing over t and aThe summation over target word sequences andalignments given fixed ?tbears a resemblance tothe inside algorithm, except that the tree structureis fixed (Pereira and Schabes, 1992).
Let S(j, i, t)denote the sum of all translations rooted at posi-tion j in ?tsuch that a(j) = i and tj= t.Tab.
3 gives the equations for this DP: Eq.
11is the quantity of interest, Eq.
12 is the recursion,and Eq.
13 shows the base cases for leaves of ?t.Letting q = max0?i?n|Trans(si)|, this algo-rithm runs in O(mn2q2) time and O(mnq) space.For efficiency we place a hard upper bound on qduring training (details in ?6).5.2 Summing over ?tand aFor the summation over dependency trees andalignments given fixed t, required for p(?t|t, s, ?s), we perform ?inside?
lattice parsing withGs,?s.
The technique is the summing variant ofthe decoding method in ?4, except for each state j,224the sausage lattice only includes arcs from j?1 toj that are labeled with the known target word tj.If a is the number of arcs in the lattice, which isO(mn), this algorithm runs in O(a3) time and re-quires O(a2) space.
Because we use a hard upperbound on |Trans(s)| for all s ?
?, this summationis much faster in practice than the one over wordsand alignments.5.3 Handling Non-Local FeaturesSo far, all of our algorithms have exploited DP,disallowing any non-local features (e.g., fphr, fNfor N > 1, fzth, fsunc).
We recently proposed?cube summing,?
an approximate technique thatpermits the use of non-local features for inside DPalgorithms (Gimpel and Smith, 2009).
Cube sum-ming is based on a slightly less greedy variation ofcube pruning (Chiang, 2007) that maintains k-bestlists of derivations for each DP chart item.
Cubesumming augments the k-best list with a residualterm that sums over remaining structures not inthe k-best list, albeit without their non-local fea-tures.
Using the machinery of cube summing, itis straightforward to include the desired non-localfeatures in the summations required for pseudo-likelihood, as well as to compute their approxi-mate gradients.Our approach permits an alternative to mini-mum error-rate training (MERT; Och, 2003); it isdiscriminative but handles latent structure and reg-ularization in more principled ways.
The pseudo-likelihood calculations for a sentence pair, takentogether, are faster than (k-best) decoding, makingSGA?s inner loop faster than MERT?s inner loop.6 ExperimentsOur decoding framework allows us to performmany experiments with the same feature rep-resentation and inference algorithms, includ-ing combining and comparing phrase-based andsyntax-based features and examining how isomor-phism constraints of synchronous formalisms af-fect translation output.6.1 Data and EvaluationWe use the German-English portion of the Ba-sic Travel Expression Corpus (BTEC).
The cor-pus has approximately 100K sentence pairs.
Wefilter sentences of length more than 15 words,which only removes 6% of the data.
We end upwith a training set of 82,299 sentences, a develop-ment set of 934 sentences, and a test set of 500sentences.
We evaluate translation output usingcase-insensitive BLEU (Papineni et al, 2001), asprovided by NIST, and METEOR (Banerjee andLavie, 2005), version 0.6, with Porter stemmingand WordNet synonym matching.6.2 FeaturesOur base system uses features as discussedin ?2.
To obtain lexical translation featuresgtrans(s,a, t), we use the Moses pipeline (Koehnet al, 2007).
We perform word alignment us-ing GIZA++ (Och and Ney, 2003), symmetrizethe alignments using the ?grow-diag-final-and?heuristic, and extract phrases up to length 3.
Wedefine flexby the lexical probabilities p(t | s) andp(s | t) estimated from the symmetrized align-ments.
After discarding phrase pairs with onlyone target-side word (since we only allow a tar-get word to align to at most one source word), wedefine fphrby 8 features: {2, 3} target words ?phrase conditional and ?lexical smoothing?
prob-abilities ?
two conditional directions.Bigram and trigam language model features, f2and f3, are estimated using the SRI toolkit (Stol-cke, 2002) with modified Kneser-Ney smoothing(Chen and Goodman, 1998).For our target-language syntactic features gsyn,we use features similar to lexicalized CFG events(Collins, 1999), specifically following the de-pendency model of Klein and Manning (2004).These include probabilities associated with in-dividual attachments (fatt) and child-generationvalence probabilities (fval).
These probabilitiesare estimated on the training corpus parsed usingthe Stanford factored parser (Klein and Manning,2003).
The same probabilities are also includedusing 50 hard word classes derived from the paral-lel corpus using the GIZA++ mkcls utility (Ochand Ney, 2003).
In total, there are 7 lexical and 7word-class syntax features.For reordering, we use a single absolute distor-tion feature fdist(i, j) that returns |i?j|whenevera(j) = i and i, j > 0.
(Unlike the other featurefunctions, which returned probabilities, this fea-ture function returns a nonnegative integer.
)The tree-to-tree syntactic features gtree2in ourmodel are binary features fqgthat fire for particu-lar QG configurations.
We use one feature for eachof the configurations in (Smith and Eisner, 2006),adding 7 additional features that score configura-225Phrase Syntactic Features:features: +fatt?
fval+fqg(base) (target) (tree-to-tree)(base) 0.3727 0.4458 0.4424+fphr0.4682 0.4971 0.5142Table 4: Feature set comparison (BLEU).tions involving root words and NULL-alignmentsmore finely.
There are 14 features in this category.Coverage features gcovare as described in ?4.2.In all, 46 feature weights are learned.6.3 Experimental ProcedureOur model permits training the system on the fullset of parallel data, but we instead use the paralleldata to estimate feature functions and learn ?
onthe development set.12We trained using three it-erations of SGA over the development data with abatch size of 1 and a fixed step size of 0.01.
Weused `2regularization with a fixed, untuned coef-ficient of 0.1.
Cube summing used a 10-best listfor training and a 7-best list for decoding unlessotherwise specified.To obtain the translation lexicon (Trans) wefirst included the top three target words t for eachs using p(s | t) ?
p(t | s) to score target words.For any training sentence ?s, t?
and tjfor whichtj6?
?ni=1Trans(si), we added tjto Trans(si)for i = argmaxi?
?Ip(si?|tj) ?
p(tj|si?
), whereI = {i : 0 ?
i ?
n ?
|Trans(si)| < qi}.We used q0= 10 and q>0= 5, restricting|Trans(NULL)| ?
10 and |Trans(s)| ?
5 for anys ?
?.
This made 191 of the development sen-tences unreachable by the model, leaving 743 sen-tences for learning ?.During decoding, we generated lattices with allt ?
Trans(si) for 0 ?
i ?
n, for every position.We used ?
= 0.9, causing states within 90% of thesource sentence length to be final states.
Betweeneach pair of consecutive states, we pruned edgesthat fell outside a beam of 70% of the sum of edgeweights (see ?4.1; edge weights use flex, fdist,and fscov) of all edges between those two states.6.4 Feature Set ComparisonOur first set of experiments compares feature setscommonly used in phrase- and syntax-based trans-lation.
In particular, we compare the effects ofcombining phrase features and syntactic features.The base model contains flex, glm, greor, and12We made this choice both for similarity to standard MTsystems and a more rapid experiment cycle.gcov.
The results are shown in Table 4.
The sec-ond row contains scores when adding in the eightfphrfeatures.
The second column shows scoreswhen adding the 14 target syntax features (fattand fval), and the third column adds to them the14 additional tree-to-tree features (fqg).
We findlarge gains in BLEU by adding more features, andfind that gains obtained through phrase featuresand syntactic features are partially additive, sug-gesting that these feature sets are making comple-mentary contributions to translation quality.6.5 Varying k During DecodingFor models without syntactic features, we con-strained the decoder to produce dependency treesin which every word?s parent is immediately to itsright and ignored syntactic features while scoringstructures.
This causes decoding to proceed left-to-right in the lattice, the way phrase-based de-coders operate.
Since these models do not searchover trees, they are substantially faster during de-coding than those that use syntactic features anddo not require any pruning of the lattice.
There-fore, we explored varying the value of k used dur-ing k-best cube decoding; results are shown inFig.
2.
Scores improve when we increase k upto 10, but not much beyond, and there is still asubstantial gap (2.5 BLEU) between using phrasefeatures with k = 20 and using all features withk = 5.
Models without syntax perform poorlywhen using a very small k, due to their reliance onnon-local language model and phrase features.
Bycontrast, models with syntactic features, which arelocal in our decoder, perform relatively well evenwith k = 1.6.6 QG Configuration ComparisonWe next compare different constraints on isomor-phism between the source and target dependency0.200.250.300.350.400.450.500.550 5 10 15 20Value of k  for DecodingBLEUPhrase + SyntacticPhraseSyntacticNeitherFigure 2: Comparison of size of k-best list for cube decodingwith various feature sets.226QDG Configurations BLEU METEORsynchronous 0.4008 0.6949+ nulls, root-any 0.4108 0.6931+ child-parent, same node 0.4337 0.6815+ sibling 0.4881 0.7216+ grandparent/child 0.5015 0.7365+ c-command 0.5156 0.7441+ other 0.5142 0.7472Table 5: QG configuration comparison.
The name of eachconfiguration, following Smith and Eisner (2006), refers tothe relationship between a(?t(j)) and a(j) in ?s.trees.
To do this, we impose harsh penalties onsome QDG configurations (?3) by fixing their fea-ture weights to ?1000.
Hence they are permit-ted only when absolutely necessary in trainingand rarely in decoding.13Each model uses allphrase and syntactic features; they differ only inthe sets of configurations which have fixed nega-tive weights.Tab.
5 shows experimental results.
Thebase ?synchronous?
model permits parent-child(a(?t(j)) = ?s(a(j))), any configuration wherea(j) = 0, including both words being linked toNULL, and requires the root word in ?tto be linkedto the root word in ?sor to NULL(5 of our 14configurations).
The second row allows any con-figuration involving NULL, including those wheretjaligns to a non-NULL word in s and its par-ent aligns to NULL, and allows the root in ?ttobe linked to any word in ?s.
Each subsequentrow adds additional configurations (i.e., trains its?
rather than fixing it to ?1000).
In general, wesee large improvements as we permit more con-figurations, and the largest jump occurs when weadd the ?sibling?
configuration (?s(a(?t(j))) =?s(a(j))).
The BLEU score does not increase,however, when we permit all configurations in thefinal row of the table, and the METEOR score in-creases only slightly.
While allowing certain cate-gories of non-isomorphism clearly seems helpful,permitting arbitrary violations does not appear tobe necessary for this dataset.6.7 DiscussionWe note that these results are not state-of-the-art on this dataset (on this task, Moses/MERTachieves 0.6838 BLEU and 0.8523METEORwithmaximum phrase length 3).14Our aim has been to13In fact, the strictest ?synchronous?
model used thealmost-forbidden configurations in 2% of test sentences; thisbehavior disappears as configurations are legalized.14We believe one cause for this performance gap is the gen-eration of the lattice and plan to address this in future workby allowing the phrase table to inform lattice generation.illustrate how a single model can provide a con-trolled experimental framework for comparisonsof features, of inference methods, and of con-straints.
Our findings show that phrase featuresand dependency syntax produce complementaryimprovements to translation quality, that tree-to-tree configurations (a new feature in MT) are help-ful for translation, and that substantial gains canbe obtained by permitting certain types of non-isomorphism.
We have validated cube summingand decoding as practical methods for approxi-mate inference.Our framework permits exploration of alter-native objectives, alternative approximate infer-ence techniques, additional hidden variables (e.g.,Moses?
phrase segmentation variable), and, ofcourse, additional feature representations.
Thesystem is publicly available at www.ark.cs.cmu.edu/Quipu.7 ConclusionWe presented feature-rich MT using a princi-pled probabilistic framework that separates fea-tures from inference.
Our novel decoder is basedon efficient DP-based QG lattice parsing extendedto handle ?non-local?
features using generic tech-niques that also support efficient parameter esti-mation.
Controlled experiments permitted withthis system show interesting trends in the use ofsyntactic features and constraints.AcknowledgmentsWe thank three anonymous EMNLP reviewers,David Smith, and Stephan Vogel for helpful com-ments and feedback that improved this paper.
Thisresearch was supported by NSF IIS-0836431 andIIS-0844507, a grant from Google, and computa-tional resources provided by Yahoo.ReferencesH.
Alshawi, S. Bangalore, and S. Douglas.
2000.Learning dependency translation modles as colec-tions of finite-state head transducers.
Computa-tional Linguistics, 26(1):45?60.S.
Banerjee and A. Lavie.
2005.
METEOR: An au-tomatic metric for MT evaluation with improvedcorrelation with human judgments.
In Proc.
ofACL Workshop on Intrinsic and Extrinsic Evalua-tion Measures for MT and/or Summarization.J.
E. Besag.
1975.
Statistical analysis of non-latticedata.
The Statistician, 24:179?195.227P.
Blunsom and M. Osborne.
2008.
Probabilistic infer-ence for machine translation.
In Proc.
of EMNLP.P.
Blunsom, T. Cohn, and M. Osborne.
2008.
A dis-criminative latent variable model for statistical ma-chine translation.
In Proc.
of ACL.P.
F. Brown, S. A. Della Pietra, V. J. Della Pietra, andR.
L. Mercer.
1993.
The mathematics of statisticalmachine translation: Parameter estimation.
Compu-tational Linguistics, 19(2):263?311.S.
Chen and J. Goodman.
1998.
An empirical study ofsmoothing techniques for language modeling.
Tech-nical report 10-98, Harvard University.D.
Chiang, Y. Marton, and P. Resnik.
2008.
On-line large-margin training of syntactic and structuraltranslation features.
In Proc.
of EMNLP.D.
Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proc.
of ACL.D.
Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.M.
Collins.
1999.
Head-Driven Statistical Models forNatural Language Parsing.
Ph.D. thesis, U. Penn.D.
Das and N. A. Smith.
2009.
Paraphrase identifica-tion as probabilistic quasi-synchronous recognition.In Proc.
of ACL-IJCNLP.Y.
Ding and M. Palmer.
2005.
Machine translation us-ing probabilistic synchronous dependency insertiongrammar.
In Proc.
of ACL.J.
Eisner, E. Goldlust, and N. A. Smith.
2005.
Com-piling Comp Ling: Practical weighted dynamic pro-gramming and the Dyna language.
In Proc.
of HLT-EMNLP.J.
Eisner.
1997.
Bilexical grammars and a cubic-timeprobabilistic parser.
In Proc.
of IWPT.M.
Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,W.
Wang, and I. Thayer.
2006.
Scalable infer-ence and training of context-rich syntactic transla-tion models.
In Proc.
of COLING-ACL.K.
Gimpel and N. A. Smith.
2008.
Rich source-side context for statistical machine translation.
InProc.
of ACL-2008 Workshop on Statistical MachineTranslation.K.
Gimpel and N. A. Smith.
2009.
Cube summing,approximate inference with non-local features, anddynamic programming without semirings.
In Proc.of EACL.R.
Haque, S. K. Naskar, Y. Ma, and A.
Way.
2009.Using supertags as source language context in SMT.In Proc.
of EAMT.L.
Huang and D. Chiang.
2007.
Forest rescoring:Faster decoding with integrated language models.
InProc.
of ACL.A.
Ittycheriah and S. Roukos.
2007.
Direct translationmodel 2.
In Proc.
of HLT-NAACL.D.
Klein and C. D. Manning.
2003.
Fast exact in-ference with a factored model for natural languageparsing.
In Advances in NIPS 15.D.
Klein and C. D. Manning.
2004.
Corpus-basedinduction of syntactic structure: Models of depen-dency and constituency.
In Proc.
of ACL.P.
Koehn, F. J. Och, and D. Marcu.
2003.
Statisticalphrase-based translation.
In Proc.
of HLT-NAACL.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,and E. Herbst.
2007.
Moses: Open source toolkitfor statistical machine translation.
In Proc.
of ACL(demo session).T.
Koo and M. Collins.
2005.
Hidden-variable modelsfor discriminative reranking.
In Proc.
of EMNLP.P.
Liang, A. Bouchard-C?ot?e, D. Klein, and B. Taskar.2006.
An end-to-end discriminative approach to ma-chine translation.
In Proc.
of COLING-ACL.A.
Lopez.
2009.
Translation as weighted deduction.In Proc.
of EACL.D.
Marcu, W.Wang, A. Echihabi, and K. Knight.
2006.Statistical machine translation with syntactified tar-get language phrases.
In Proc.
of EMNLP.H.
Mi, L. Huang, and Q. Liu.
2008.
Forest-basedtranslation.
In Proc.
of ACL.F.
J. Och and H. Ney.
2002.
Discriminative train-ing and maximum entropy models for statistical ma-chine translation.
In Proc.
of ACL.F.
J. Och and H. Ney.
2003.
A systematic comparisonof various statistical alignment models.
Computa-tional Linguistics, 29(1).F.
J. Och.
2003.
Minimum error rate training for sta-tistical machine translation.
In Proc.
of ACL.K.
Papineni, S. Roukos, and T. Ward.
1997.
Feature-based language understanding.
In EUROSPEECH.K.
Papineni, S. Roukos, T. Ward, and W.J.
Zhu.
2001.BLEU: a method for automatic evaluation of ma-chine translation.
In Proc.
of ACL.F.
C. N. Pereira and Y. Schabes.
1992.
Inside-outsidereestimation from partially bracketed corpora.
InProc.
of ACL.C.
Quirk, A. Menezes, and C. Cherry.
2005.
De-pendency treelet translation: Syntactically informedphrasal SMT.
In Proc.
of ACL.L.
Shen, J. Xu, and R. Weischedel.
2008.
A newstring-to-dependency machine translation algorithmwith a target dependency language model.
In Proc.of ACL.D.
A. Smith and J. Eisner.
2006.
Quasi-synchronousgrammars: Alignment by soft projection of syntacticdependencies.
In Proc.
of HLT-NAACLWorkshop onStatistical Machine Translation.D.
A. Smith and J. Eisner.
2009.
Parser adaptationand projection with quasi-synchronous features.
InProc.
of EMNLP.A.
Stolcke.
2002.
SRILM?an extensible languagemodeling toolkit.
In Proc.
of ICSLP.X.
Sun and J. Tsujii.
2009.
Sequential labeling withlatent variables: An exact inference algorithm andits efficient approximation.
In Proc.
of EACL.M.
Wang, N. A. Smith, and T. Mitamura.
2007.
Whatis the Jeopardy model?
a quasi-synchronous gram-mar for QA.
In Proc.
of EMNLP-CoNLL.K.
Yamada and K. Knight.
2001.
A syntax-based sta-tistical translation model.
In Proc.
of ACL.228
