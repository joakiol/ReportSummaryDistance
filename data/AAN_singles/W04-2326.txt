Annotating Student Emotional States in Spoken Tutoring DialoguesDiane J. LitmanUniversity of PittsburghDepartment of Computer ScienceLearning Research and Development CenterPittsburgh PA, 15260, USAlitman@cs.pitt.eduKate Forbes-RileyUniversity of PittsburghLearning Research and Development CenterPittsburgh PA, 15260, USAforbesk@pitt.eduAbstractWe present an annotation scheme for stu-dent emotions in tutoring dialogues.
Analy-ses of our scheme with respect to interannota-tor agreement and predictive accuracy indicatethat our scheme is reliable in our domain, andthat our emotion labels can be predicted witha high degree of accuracy.
We discuss issuesconcerning the implementation of emotion pre-diction and adaptation in the computer tutoringdialogue system we are developing.1 IntroductionThis paper describes a coding scheme for annotating stu-dent emotional states in spoken dialogue tutoring cor-pora, and analyzes the scheme not only for its reliabil-ity, but also for its utility in developing a spoken dia-logue tutoring system that can model and respond to stu-dent emotions.
Motivation for this work comes from theperformance discrepancy between human tutors and cur-rent machine tutors: typically, students tutored by hu-man tutors achieve higher learning gains than studentstutored by computer tutors.
The development of com-putational tutorial dialogue systems (Rose?
and Aleven,2002) represents one method of closing this performancegap, e.g.
it is hypothesized that dialogue-based tutors al-low greater adaptivity to students?
beliefs and misconcep-tions.
Another method for closing this performance gapinvolves incorporating emotion prediction and adaptationinto computer tutors (Kort et al, 2001; Evens, 2002).For example (Aist et al, 2002) have shown that addinghuman-provided emotional scaffolding to an automatedreading tutor increases student persistence.
This suggeststhat the success of computer dialogue tutors could be in-creased by responding to both what a student says andhow s/he says it, e.g.
with condence or uncertainty.To assess the impact of adding emotion modeling todialogue tutoring systems, we are building ITSPOKE(Intelligent Tutoring SPOKEn dialogue system), a spo-ken dialogue system that uses the Why2-Atlas concep-tual physics tutoring system (VanLehn et al, 2002) as its?back-end.
?1 Our first step towards incorporating emo-tion processing into ITSPOKE is to develop a reliableannotation scheme for student emotions.
Our next stepwill be to use the data that has been annotated accord-ing to this scheme to enhance ITSPOKE to dynamicallypredict and adapt to student emotions.
This adds addi-tional constraints on our annotation scheme besides goodreliability, namely that our annotations are predictable byITSPOKE with a high degree of accuracy (automaticallyand in real-time), and that they are expressive enough tosupport the range of desired system adaptations.In Section 2 we review previous work in emotion anno-tation for spoken dialogue systems.
In Section 3 we dis-cuss our tutoring research project and corpora.
In Section4 we present an emotion annotation scheme for this do-main.
In Section 5 we analyze our scheme with respect tointerannotator agreement and predictive accuracy, using acorpus of human tutoring dialogues.
Our agreement indi-cates that our scheme is reliable, while machine learningexperiments on annotated data indicate that our emotionlabels can be predicted with a high degree of accuracy.In Section 6 we analyze more expressive versions of ourscheme, and discuss differences between annotating hu-man and computer spoken tutoring dialogues.2 Prior Research on EmotionDeveloping a descriptive theory of emotion is a com-plex research topic, viewed from either a theoretical oran empirical standpoint (Cowie et al, 2001).
Some re-searchers have proposed a variety of ?fundamental?
hu-man emotions, while others have argued that emotions1We also use ITSPOKE to examine the utility of buildingspoken dialogue tutors (e.g.
(Litman and Forbes, 2003)).are best represented componentially, in terms of multipledimensions.
Despite this lack of a well-defined descrip-tive framework, there has been great recent interest inpredicting emotional states, using information extractedfrom a person?s text, speech, physiology, facial expres-sions, eye gaze, etc.
(Pantic and Rothkrantz, 2003).In the area of emotional speech, most research hasused databases of speech read by actors or native speak-ers as training data for developing emotion predic-tors (Holzapfel et al, 2002; Liscombe et al, 2003).
Inthis work the set of emotions to be read is predefined be-fore the utterance is spoken, rather than annotated afterthe fact.
One problem with this approach is that suchprototypical emotional speech does not necessarily re-flect natural speech (Batliner et al, 2003), e.g.
the wayone acts an emotion is not necessarily the same as theway one naturally expresses an emotion.
Moreover, ac-tors repeatedly reading the same sentence are restrictedto conveying different emotions using only acoustic andprosodic features, while in natural interactions a muchwider feature variety is available (e.g., lexical, dialogue).As a result of these problems, researchers motivated byspoken dialogue applications have instead started to trainemotion predictors using naturally-occurring speech thathas been hand-annotated for various emotions (Ang et al,2002; Batliner et al, 2003; Lee et al, 2001; Litman andForbes, 2003).
However, this requires researchers to firstdevelop a scheme for annotating emotions in naturally-occurring spoken dialogue corpora.
Although emotionannotation of natural corpora (typically at the turn or ut-terance level) has been addressed in various domains, lit-tle has yet been done in the educational setting.
Althoughnot yet tested, (Evens, 2002) has hypothesized adaptivestrategies; for example, if detecting frustration, the sys-tem should respond to hedges and self-deprecation, bysupplying praise and restructuring the problem.
A com-parison of our annotation scheme and prior non-tutoringschemes is presented in Section 4.4.3 The ITSPOKE System and CorporaIn ITSPOKE, a student types an essay answering a qual-itative physics problem.
The ITSPOKE computer tutorthen engages the student in spoken dialogue to correctmisconceptions and elicit more complete explanations,after which the student revises the essay, thereby endingthe tutoring or causing another round of tutoring/essay re-vision.
Student speech is digitized from microphone in-put and sent to the Sphinx2 recognizer, whose most prob-able ?transcription?
output is then sent to the Why2-Atlasback-end for syntactic, semantic and dialogue analysis.The text response produced by Why2-Atlas is sent to theCepstral text-to-speech system.
A formal evaluation ofITSPOKE began in November 2003; to date we have col-lected 50 dialogues from 10 students.
A corpus exampleis shown in Figure 4, Appendix A. Corpus collection usesthe same experimental procedure as our human-humantutoring corpus, described next.Our Human-Human Spoken Dialogue Tutoring Corpuscontains spoken dialogues collected via a web interfacesupplemented with a high-quality audio link, where thehuman tutor performs the same task as ITSPOKE.
Theexperimental procedure for collecting both corpora is asfollows: 1) students are given a pre-test measuring theirphysics knowledge, 2) students read through a small doc-ument of background material, 3) students use the weband voice interface to work through a set of training prob-lems (dialogues) with the tutor, and 4) students are givena post-test that is similar to the pre-test.
Subjects are Uni-versity of Pittsburgh students who have never taken col-lege physics and who are native English speakers.
One tu-tor currently participates.
To date we have collected 149dialogues from 17 students.
Annotated (see Section 4)corpus examples are shown in Figure 1 and Figure 2 (Ap-pendix A) (punctuation added for clarity).. .
.
dialogue excerpt at 5.2 minutes into session.
.
.TUTOR   : Suppose you apply equal force by pushing them.Then uh what will happen to their motion?STUDENT  : Um the one that?s heavier...uh, the acc- accelera-tion won?t be as great.
(NEGATIVE, UNCERTAIN)TUTOR  : The one which is...STUDENT  : Heavier?
(NEGATIVE, UNCERTAIN)TUTOR  : Mm, well, uh, is that your common-STUDENT  : Er I?m sorry- I?m sorry- the one with most mass.You- (POSITIVE, CONFIDENT)TUTOR  : (lgh) Yeah, the one with more mass will- if you-if the mass is more and force is the same then which one willaccelerate more?STUDENT : Which one will move more?
(NEGATIVE,CONFUSED)TUTOR: Mm, which one will accelerate more?STUDENT  : The- the one with the least amount of mass?
(NEGATIVE, UNCERTAIN)TUTOR: Yeah, but what you said was different isn?t it?
Souh you are applying Newton?s law of uh second law of motion:F is equal to M times A.
And uh you apply equal force on boththe containers, then the one which is less massive will acceleratemore.STUDENT  : Right.
(WEAK POSITIVE, CONFIDENT)Figure 1: Annotated Excerpt (Human Spoken Corpus)4 Annotation SchemeIn our spoken dialogue tutoring corpora, student emo-tional states can only be identified indirectly ?
via what astudent says and/or how s/he says it.
Furthermore, suchevidence is not always obvious, unambiguous, or consis-tent.
For example, a student may express anger throughthe use of swear words, or through a particular tone ofvoice, or via a combination of signals, or not at all.
More-over, another student may present some of these same sig-nals even when s/he does not feel anger.Our objective is nevertheless to develop a reliable an-notation scheme across annotators, for manually labelingthe student turns in our spoken tutoring dialogues for per-ceived expressions of emotion.4.1 Emotion ClassesIn our current annotation scheme, perceived expressionsof emotion are viewed along a linear scale, as shown anddefined below: negativeneutralpositiveNegative: a student turn that strongly expresses emo-tions such as confused, bored, irritated, uncertain, sad.Examples in Figure 1 include student  and student  .Evidence2 for the negative emotions in these turns in-cludes syntax (constructions such as questions), disflu-encies, and acoustic-prosodic features.Positive: a student turn that strongly expresses emo-tions such as condent, enthusiastic.
An example isstudent  in Figure 1, where evidence of a positive emo-tion comes primarily from acoustic-prosodic features.Neutral: a student turn not strongly expressing a neg-ative or positive emotion.In addition to these three main emotion classes, wealso distinguish three minor emotion classes:Weak Negative: a student turn that weakly expressesnegative emotions.Weak Positive: a student turn that weakly expressespositive emotions.
An example is student  in Figure1, where evidence is primarily lexical (?right?
).Mixed: a student turn that strongly expresses both posi-tive and negative emotions: Case 1) multi-utterance turnswhere one utterance is judged positive and another, nega-tive.
Case 2) turns where the simultaneous strong expres-sion of negative and positive emotions is perceived.
Case2 is often due to conflicting domains (Section 4.2), e.g.boredom with tutoring but confidence about physics.4.2 Relativity and Domains of Emotion ClassesOur emotion annotation is relative to both context andtask.
By context-relative we mean that a student turn inour tutoring dialogues is identified as expressing emotionrelative to the other student turns in that dialogue.
Bytask-relative we mean that a student turn perceived duringtutoring as expressing an emotion might not be perceivedas expressing the same emotion with the same strengthin another situation.
For example, consider the contextof a tutoring session, where a student has been answer-ing tutor questions with apparent ease.
If the tutor thenasks another question, and the student responds slowly,2Determined in post-annotation discussion (see Section 4.4).saying ?Um, now I?m confused?, this turn would likelybe labeled negative.
However, in the context of a heatedargument between two people, this same turn might belabeled as a weak negative, or even weak positive.We also annotate emotion with respect to multiple do-mains.
One focus of our annotation scheme is expres-sions of emotion that pertain to the physics material be-ing learned (?PHYS?
domain).
For example, a studentmay express confusion or confidence about the physicsmaterial.
Another focus of our scheme is expressions ofemotion that pertain to the tutoring process, including at-titudes towards the tutor, the dialogue, and/or being tu-tored (?TUT?
domain).
For example, a student may ex-press boredom or amusement with the tutoring.4.3 Specific Annotation InstructionsOur annotation scheme is detailed in an online, audio-enhanced emotion labeling manual.
As shown in Figure 3(Appendix A), the emotion annotation is performed using(our customization of) Wavesurfer, an open source soundvisualization and manipulation tool.
The ?Tutor Speech?and ?Student Speech?
panes show a portion of the tutorand student speech files, while the ?Tutor Text?
and ?Stu-dent Text?
show the associated transcriptions, where ver-tical lines correspond to turn segmentations.3 There arethree additional panes for emotion annotation:The EMOa pane records the annotator?s judgment ofthe expressed emotion class for each turn, e.g.
the sixemotion classes described in Section 4.1: negative, weaknegative, neutral, weak positive, positive, mixed.
An-notators are instructed to focus on expressed emotions inthe PHYS domain.
If an additional expressed emotion inthe TUT domain is perceived, this is noted in the NOTESpane (e.g.
?amused/TUT?).
If no expressed emotion isperceived in the PHYS domain, any expressed emotion inthe TUT domain is labeled in the EMOa pane, and noted(e.g.
?TUT?)
in the NOTES pane.
Domain indecision isalso noted (e.g.
?TUT/PHYS??)
in the NOTES pane.The EMOb pane further specifies the annotations inthe EMOa pane, by recording a specific expressed emo-tion for each turn.
Our current list of specific emotionscontains those that we believe will be useful for trigger-ing ITSPOKE adaptation.
Specific negative emotions are:uncertain, confused, sad, bored, irritated.
Specific pos-itive emotions are: condent, enthusiastic.
Our manualincludes glosses for these specific emotions, formulatedusing synonyms and/or hyponyms that are currently notdistinguished.
For example, our gloss for enthusiastic in-cludes interested, pleased, amused.
There are also com-plex labels combining multiple specific emotions withina class (e.g.
uncertain+sad, condent+enthusiastic).
If3Transcription and turn-segmentation of the human-humandialogues were also done within Wavesurfer, by a paid tran-scriber prior to emotion annotation.the annotator judges a specific emotion that is not listed(or lacks a close substitute), s/he selects the label other,and lists the alternative(s) in the NOTES pane.
If the an-notator selected mixed (case 1) in the EMOa pane, s/hesubdivides the turn into utterances in the EMOb pane andprovides a specific emotion label for each utterance.
Ifthe annotator selected mixed (case 2) in the EMOa pane,s/he selects the label other in the EMOb pane, and com-ments on the indecision in the NOTES pane.The NOTES pane records any additional annotatorcomments concerning their judgment, the annotation, etc.Because our annotation is student-, context-, and task-specific, our manual first instructs the annotator to listento each dialogue at least once before annotating, to se-cure an intuition of how and with what range emotionalexpression is displayed.
S/he is also instructed to not as-sume that all dialogues will begin with neutral studentturns.
S/he is however reminded that it is not necessaryto assign a non-neutral label to every turn.
Finally, s/heis told to ignore correctness when annotating, because acorrect answer to a tutor question can express uncertainty,and an incorrect answer can express confidence.Our manual also describes two default conventions forour annotation scheme, which can however be overriddenby the annotator?s intuitive judgment and/or other extenu-ating considerations (e.g.
irony, etc), as described below:1) By definition, a question expresses strong uncertaintyor confusion.
Thus if a student turn consists only of aquestion, its default label is negative.
However:a) If the turn consists of multiple utterances, one ofwhich is a question, and the other(s) expresses a positiveemotion, then the turn should be labeled mixed and sub-divided (e.g.
?What directions are the forces acting in?Gravity is only acting in the down direction?
).b) The domain must be considered.
For example, de-faults in one domain can be overridden if the turn ex-presses a contrasting emotion in the other domain.2) Many student turns in our dialogues are very short,containing only grounding phrases such as ?yeah?, ?ok?,?mm-hm?, ?uh-huh?, etc.
By default, such turns are la-beled neutral, because groundings serve mainly to en-courage another speaker to continue speaking.
However:a) Groundings may occasionally strongly express anemotion (e.g.
?yeah!
?, (sigh) ?ok?
), thereby overridingthe default label.b) The semantics of certain groundings is associatedwith weakly expressed understanding, (e.g.
?right?
and?sure?
), and default to weak positive.c) Certain phrases are associated with strongly ex-pressed uncertainty or confusion (e.g.
?um?
(silence)),and default to negative.Our annotation manual concludes with 8 examples ofannotated student turns (as in Figure 1), with links to cor-responding audio files.
The variety exemplifies how dif-ferent students express emotions differently at differentpoints in the dialogue, and cover all 6 emotion labels atleast once (there are 2 negatives and 2 positives).
Alsoprovided is a lengthy audio-enhanced transcript from asingle student tutoring dialogue, to exemplify how stu-dent emotion changes throughout a single tutoring ses-sion.
This transcript is shown in part in Figure 2, Ap-pendix A.
The transcript is organized in terms of tutorand student turn start and end times.
For each studentturn, the four Wavesurfer panes are shown.4.4 Comparison with Prior SchemesStudies of actor-read speech often make a large num-ber of emotion distinctions, e.g.
the LDC EmotionalProsody corpus distinguishes 15 classes.
Our work,like other studies of naturally occurring dialogues, usesa more restricted set of emotions, due to the need tofirst manually annotate such emotions reliably across an-notators.
As discussed above, our annotation schemedistinguishes negative, neutral, and positive emotions,as well as ?weak?
and ?mixed?
classes.
Other stud-ies of naturally occurring data have annotated only twoemotion classes (e.g.
emotional/non-emotional (Bat-liner et al, 2000), negative/non-negative (Lee et al,2001)).
The study of (Ang et al, 2002) annotates sixemotion classes, but collapses most of these for thepurposes of emotion prediction.4 In Section 5, wewill similarly explore the impact of collapsing someof our 6 distinctions, to produce simpler 3-way (neg-ative/positive/neutral) and 2-way (negative/non-negativeand emotional/non-emotional) schemes.In further contrast to (Lee et al, 2001), our annotationsare context- and task-relative, because like (Ang et al,2002; Batliner et al, 2003), we are interested in detect-ing emotional changes across our dialogues.
But unlike(Batliner et al, 2003), we allow annotators to be guidedby their intuition rather than a set of expected features,to avoid restricting or otherwise influencing their intu-itive understanding of emotion expression, and becausesuch features are not used consistently or unambiguouslyacross speakers.
Instead, our manual contains annotatedaudio-enhanced corpus examples (as in Figures 1-2).5 Analysis of the Annotation SchemeGiven our complete annotation scheme in Section 4, wenow explore both the reliability of the scheme at threelevels of granularity that have been proposed in priorwork, and the accuracy of automatically predicting thesevariations.
These analyses give insight into the tradeoff4(Ang et al, 2002) also discusses the use of an ?uncertainty?label, although it did not improve inter-annotator agreement.Our ?weak?
labels are more similar to an ?intensity?
dimensionfound in studies of elicited speech (see (Cowie et al, 2001)).between interannotator reliability, annotation granularity,and predictive accuracy.For the purposes of these analyses, we randomly se-lected 10 transcribed and turn-annotated dialogues fromour human-human tutoring corpus (Section 3), yielding453 student turns from 9 subjects.
The turns were sep-arately annotated by two annotators, using the emotionannotation instructions in Section 4.
For our machine-learning experiments we follow the methodology in (Lit-man and Forbes, 2003), instantiated with the learningmethod (boosted decision trees) and feature set (acoustic-prosodic, lexical, dialogue and contextual) that has givenus our best results in ongoing studies.5.1 Agreed Student TurnsConflating Minor and Neutral ClassesFor our first analysis, only our three main emotionclasses were distinguished: negative, neutral, positive.Our three minor classes, weak negative, mixed, weak pos-itive, were conflated with the neutral class.
A confusionmatrix summarizing the resulting inter-annotator agree-ment is shown in Table 1.
The rows correspond to thelabels assigned by annotator 1, and the columns corre-spond to the labels assigned by annotator 2.
For example,90 negatives were agreed upon by both annotators, while6 negatives assigned by annotator 1 were labeled as neu-tral by annotator 2.
The two annotators agreed on the an-notations of 385/453 turns, achieving 84.99% agreement(Kappa = 0.68 (Carletta, 1996)).
Such agreement is ex-pected given the difficulty of the task, and exceeds thatof prior studies of emotion annotation in naturally occur-ring speech; (Ang et al, 2002), for example, achievedagreement of 71% (Kappa 0.47), while (Lee et al, 2001)averaged around 70% agreement.As in (Lee et al, 2001), we next performed a machinelearning experiment on the 385 student turns where thetwo annotators agreed on the emotion label.
Our predic-tive accuracy for this data was 84.75% (using 10 x 10cross-validation as in (Litman and Forbes, 2003)).
Com-pared to a baseline accuracy of 72.74% achieved by al-ways predicting the majority (neutral) class, our resultyields a relative improvement of 44.06%.5negative neutral positivenegative 90 6 4neutral 23 280 30positive 0 5 15Table 1: Confusion Matrix 1: Minor  Neutral5Relative improvement of x over y = fffiflffi,where error(x) is 100 - %accuracy(x).Conflating Weak and Negative/Positive ClassesIn a second analysis, we again distinguished only ourthree main emotion classes; however, this time weak neg-ative was conflated with negative, and weak positive wasconflated with positive.
Our mixed class was again con-flated with neutral.
A confusion matrix summarizing theresulting inter-annotator agreement is shown in Table 2.As shown, although the number of agreed negative andpositive turns increased, overall interannotator agreementdecreased to 340/453 turns, or 75.06% (Kappa = 0.60).We performed our machine learning experiment onthese 340 agreed student turns.
The predictive accuracyfor this data decreased to 79.29%; however, baseline (ma-jority class) accuracy also decreased to 53.24%; thus rel-ative improvement in fact increased to 55.71%negative neutral positivenegative 112 9 9neutral 31 181 53positive 1 10 47Table 2: Confusion Matrix 2: Weak  Neg/PosNegative/Non-Negative ClassesAs Tables 1-2 indicate, our annotators found the pos-itive class the most difficult to annotate and agree upon,and the positive class was also the least frequent classoverall.
Not surprisingly, our prior machine learning ex-periments have also showed that the positive class is thehardest to predict (Litman and Forbes, 2003).
We thusnext explored a binary analysis where our positive andneutral classes are conflated, yielding a negative/non-negative distinction akin to (Lee et al, 2001).
Againhowever we experimented with conflating our minorweak classes with either the neutral class or their mainclass counterparts (e.g.
weak negative  negative).Two confusion matrices summarizing the resulting inter-annotator agreements are shown in Tables 3 - 4.In Table 3, our three minor classes are conflated withthe neutral class.
Interannotator agreement in this caserises sharply to 420/453 turns, or 92.72% (Kappa =0.80).
The predictive accuracy for this data increased to86.83%; however, baseline (majority class) accuracy alsoincreased to 78.57%; thus relative improvement in factdecreased to 38.54%negative non-negativenegative 90 10non-negative 23 330Table 3: Confusion Matrix 3: Pos/Neu  Non-NegIn Table 4, our two weak classes are conflated withtheir main class counterparts.
Interannotator agreementonly rises to 403/453 turns, or 88.96% (Kappa = 0.74),Predictive accuracy decreases to 82.94%.
However, base-line (majority class) accuracy also decreases to 72.21%;thus relative improvement was comparable, at 38.61%negative non-negativenegative 112 18non-negative 32 291Table 4: Conf.
Matrix 4: (Weak) Pos/Neu  Non-NegEmotional/Non-Emotional ClassesWe also explored an alternative binary analysis thatconflated our positive and negative classes, yielding anemotional/non-emotional distinction, akin to (Batlineret al, 2000).
Again we conflated our minor weak classeswith either the neutral class or their main class counter-parts, as shown in in Tables 5-6.
In Table 5, our three mi-nor classes are conflated with the neutral class, yieldingagreement on 389/453 turns, or 85.87% (Kappa = 0.67).The predictive accuracy was high at 85.07%, while base-line (majority) accuracy was 71.98%; thus relative im-provement was 46.72%emotional non-emotionalemotional 109 11non-emotional 53 280Table 5: Confusion Matrix 5: Pos/Neg  EmotionalIn Table 6, weak classes are conflated with their mainclass counterparts.
Interannotator agreement decreases to350/453 turns, or 77.26% (Kappa = 0.55).
Predictive ac-curacy was high at 86.14%; moreover, baseline (majority)accuracy was the lowest yet seen, 51.71%, and relativeimprovement was the best yet seen, at 71.30%emotional non-emotionalemotional 169 19non-emotional 84 181Table 6: Confusion Matrix 6: (Weak) Pos/Neg  EmoSummaryA summary of our results across analyses of agreedstudent turns are shown in Table 7.
NPN represents anal-yses distinguishing negative, neutral and positive emo-tions, NnN represents ?negative/non-negative?
analyses,and EnE represents ?emotional/non-emotional?
analy-ses.
Column ?K?
shows Kappa for each analysis, ?Acc?shows the predictive accuracy achieved by machine learn-ing, ?Base?
shows the baseline (majority class) accu-racy, and ?RI?
show the relative improvement achievedby learning compared with this baseline.As can be seen, there is no single optimal way to con-flate the original 6 classes; optimality depends on whethermaximizing Kappa, predictive accuracy, or expressive-ness is most important.
For example, conflating minorand neutral labels (the first three rows) yields better an-notation reliability than for their counterparts (conflatingweak and main labels) in the last three rows; the reverseis true, however, for machine learning performance (mea-sured by relative improvement over the majority classbaseline).
With respect to expressiveness, only the 3-wayNPN distinction can explicitly distinguish positive emo-tions.
With respect to the binary distinctions, annotatingnegative/non-negative (NnN) can be done most reliably,while predicting emotional/non-emotional (EnE) yields abetter relative improvement.K Acc Base RIminor  neutralNPN .68 84.75% 72.74% 44.06%NnN .80 86.83% 78.57% 38.54%EnE .67 85.07% 71.98% 46.72%weak  mainNPN .60 79.29% 53.24% 55.71%NnN .74 82.94% 72.21% 38.61%EnE .55 86.14% 51.71% 71.30%Table 7: Summary: Annotation and Learning Results5.2 Consensus-Labeled Student TurnsFollowing (Ang et al, 2002), we also explored consensuslabeling, both to increase our usable data set for predic-tion, and to include the more difficult annotation cases.For consensus labeling, the original annotators revisitedeach originally disagreed case, and through discussion,sought a consensus label.
Agreement thus rose acrossall analyses, to 99.12%; we discarded 8/453 turns forlack of consensus.
A summary of the consensus label-ing across all 6 analyses discussed above is shown inTable 8.
The row and column labels are as above, e.g.the NPN row represents turns consensus-labeled as nega-tive/neutral/positive, first when all three minor classes areconflated with neutral, and second where the weak minorclasses are conflated with their main counterparts.minor  neu weak  mainneg neu pos neg neu posNPN 99 321 25 119 265 61neg nonneg neg nonnegNnN 99 346 119 326emo nonemo emo nonemoEnE 124 321 180 265Table 8: Consensus Labeling over AnalysesWe performed our machine learning experiment on theconsensus data for all 6 analyses.
A summary of ourresults are shown in Table 9.
A comparison of Tables7-9 shows that for all of our evaluation metrics, our re-sults decrease across all analyses when using consensusdata; similar findings were observed in (Ang et al, 2002).While increasing our data set using more difficult exam-ples decreases predictive ability, note that our consensusresults are still an improvement over the baseline.Acc Base RIminor  neutralNPN 79.97% 72.14% 28.10%NnN 84.97% 77.75% 32.45%EnE 80.78% 72.14% 31.01%weak  mainNPN 73.14% 59.55% 33.60%NnN 81.88% 73.26% 32.24%EnE 75.75% 59.55% 40.05%Table 9: Predicting Consensus Labels6 Extensions to the Analyses6.1 Minor Emotion ClassesOur analyses so far distinguished only our 3 main emo-tion classes; our 3 minor classes were always conflatedwith one or the other of the main classes.
In part, thisis because our minor labels were consistently employedonly later in the development of our scheme; in early ver-sions, annotators optionally labeled the minor classes (inthe NOTES pane), for the purpose of post-annotation dis-cussion.
At present, only the last 5 of our 10 annotateddialogues are consistently labeled with minor classes.
Ta-ble 10 shows a confusion matrix for the annotation of all6 emotion classes for these 5 dialogues.
Interannotatoragreement is 142/211 turns, or 67.30% (Kappa = 0.54).Compared to Section 5, we see that this higher level ofgranularity yields a lower level of agreement.
However,most disagreements fall adjacent to the diagonal, indicat-ing that they are mostly differences in strength rather thandifferences in polarity.
The analyses in Section 5 investi-gated various means of resolving these differences.neg w. neg neut w. pos pos mixneg 48 2 0 0 0 2w.
neg 6 10 3 2 2 0neut 2 11 70 22 3 3w.
pos 0 1 1 9 2 0pos 0 0 1 1 1 0mix 1 1 2 1 0 4Table 10: Confusion Matrix: All 6 Emotion Classes6.2 Specific EmotionsOur analyses in Section 5 did not consider the specificemotion annotations in our ?EMOb?
pane.
This is in partbecause, as with our minor labels, our specific emotionlabels were only consistently employed when annotatingthe last 5 of our 10 dialogues.
If we consider only the66 turns where both annotators agreed that the turn wasnegative (weak or strong), and view multiple emotion la-bels which overlap with single emotions as agreed (e.g.sad+bored agrees with a sad or bored label), interannota-tor agreement is 45/66 turns, or 68.18% (Kappa = 0.41).The same analysis for the 13 positive turns yields 100%agreement (Kappa = 1).The labels we?ve included so far are those we?ve en-countered in our human-human tutoring dialogues; weexpect to see some differences in the human-computer di-alogues, as discussed in Section 6.3, and continue to em-ploy the ?other?
label.
In part, the decision about whichspecific emotions to ultimately recognize in our systemdepends on what we want the system to adapt to.
Thisin turn requires some understanding of how human tutorsadapt to different emotions.
For example, perhaps ourtutor responds differently to anger, uncertainty, boredomand confusion, but responds the same to most positiveemotions.
We are currently investigating this in our an-notated human-human tutoring dialogues.6.3 Human-Computer CorpusWe have just begun annotating our corpus of human-computer spoken tutoring dialogues; to date we have an-notated 5 dialogues from 5 different students.We have applied the 6 reliability analyses in this paperto these annotations, and have found again that most dis-agreements are simply differences in strength rather thandifferences in polarity.
Our best interannotator reliabilitywas found using the NnN, weak  main analysis (con-trary to the human-human findings), which gave agree-ment of 96/115 turns, or 83.48% (Kappa = 0.67).The corpus example in Figure 4 (Appendix A) high-lights differences between our human-human and human-computer tutoringdialogues that potentiallymight impactemotion annotation.
First, both the average student turnlength in words, and the average number of student turnsper dialogue, are much shorter in the human-computerthan in the human-human dialogues.
This means thatthere is less information in the human-computer dia-logues to make use of when judging expressed emotions.Second, errors in speech and natural language process-ing can have a significant effect on the student emotionalstate in the human-computer tutoring dialogues.
Suchemotions don?t concern either the PHYS domain or theTUT domain, and suggest that we might want to add athird NLP domain if we want the system to respond tothese emotions differently.
Relatedly, we already see fre-quency differences across the human-human and human-computer dialogues with respect to specific emotions, forexample an increased use of ?irritated?
in the human-computer data.
Finally, computer tutors are far less flexi-ble than human tutors.
This alone can effect student emo-tional state, and furthermore it can limit how the studentexpresses their own emotional states.
For example, in thehuman-human dialogues we see more student initiative,groundings, and references to prior problems.7 Conclusions and Current DirectionsIn this paper we presented and analyzed our schemefor annotating student emotional states in spoken tu-toring dialogues.
Our scheme distinguishes three main(negative, neutral and positive) and three minor (weaknegative, mixed, and weak positive) emotion classes.Our inter-annotator agreement is on par with prior emo-tion annotation in other types of corpora.
We usedconsensus-labeling to resolve disagreements and increaseour dataset.
Through further annotation and the use ofother inter-annotation metrics (Gwet, 2001), we will in-vestigate how systematic disagreements can yield revi-sions to our annotation scheme that improve reliability.Our machine learning experiments have shown that ourmain emotion categories can be predicted with a highdegree of accuracy.
Although not presented here, F-Measures (   "!$#&%('$)"*+*!,.-%('/10(/324#&%'$)"*+*56,.-"%'/30/324 ) for our experiments onagreed data ranged from 67%-86%; in future work wewill more closely examine the tradeoff between recall andprecision when predicting our annotations.
Our experi-ments have also highlighted tradeoffs that can be madebetween coding reliability, predictive accuracy, and an-notation scheme granularity.Finally, we presented initial results in annotating ourITSPOKE human-computer tutoring corpus, and dis-cussed differences from our human-human annotations.This research on emotion annotation and prediction is afirst step towards extending the ITSPOKE computer tu-toring dialogue system to predict and adapt to studentemotional states.
Our next goal is to label human tutorreactions to emotional student turns, in order to formu-late adaptive strategies for ITSPOKE, and to determinewhich of our six prediction tasks best triggers adaptation.AcknowledgmentsThis research is supported by NSF Grants Nos.
9720359and No.
0328431.
We thank Kurt VanLehn and theWhy2-Atlas team, and Scott Silliman of ITSPOKE, forsystem development and data collection.ReferencesG.
Aist, B. Kort, R. Reilly, J. Mostow, and R. Pi-card.
2002.
Experimentally augmenting an intelli-gent tutoring system with human-supplied capabilities:Adding human-provided emotional scaffolding to anautomated reading tutor that listens.
In Proc.
of ITS.J.
Ang, R. Dhillon, A. Krupski, E.Shriberg, and A. Stol-cke.
2002.
Prosody-based automatic detection of an-noyance and frustration in human-computer dialog.
InProc.
of ICSLP.A.
Batliner, K. Fischer, R. Huber, J. Spilker, and E. No?th.2000.
Desperately seeking emotions: Actors, wizards,and human beings.
In ISCA Workshop on Speech andEmotion.A.
Batliner, K. Fischer, R. Huber, J. Spilker, and E. Noth.2003.
How to find trouble in communication.
SpeechCommunication, 40.J.
Carletta.
1996.
Assessing agreement on classificationtasks: the kappa statistic.
Computational Linguistics,22(2), June.R.
Cowie, E. Douglas-Cowie, N. Tsapatsoulis, G. Votsis,S.
Kollias, W. Fellenz, and J. Taylor.
2001.
Emotionrecognition in human-computer interaction.
IEEE Sig-nal Processing Magazine, 18:32?80, January.Martha Evens.
2002.
New questions for Circsim-Tutor.Presentation at the 2002 Symposium on Natural Lan-guage Tutoring, University of Pittsburgh.K.
Gwet.
2001.
Handbook of Inter-Rater Reliability.STATAXIS Publishing Company.H.
Holzapfel, C. Fuegen, M. Denecke, and A. Waibel.2002.
Integrating emotional cues into a framework fordialogue management.
In Proc.
of ICMI.B.
Kort, R. Reilly, and R. W. Picard.
2001.
An affec-tive model of interplay between emotions and learn-ing: Reengineering educational pedagogy - building alearning companion.
In Proc.
of ICALT.C.M.
Lee, S. Narayanan, and R. Pieraccini.
2001.Recognition of negative emotions from the speech sig-nal.
In Proc.
of ASRU.J.
Liscombe, J. Venditti, and J.Hirschberg.
2003.
Classi-fying subject ratings of emotional speech using acous-tic features.
In Proc.
of EuroSpeech.D.
Litman and K. Forbes.
2003.
Recognizing emotionfrom student speech in tutoring dialogues.
In Proc.
ofASRU.M.
Pantic and L. J. M. Rothkrantz.
2003.
Toward anaffect-sensitive multimodal human-computer interac-tion.
Proc.
of IEEE, 91(9):1370?1390.C.
P. Rose?
and V. Aleven.
2002.
Proceedings of the ITS2002 workshop on empirical methods for tutorial dia-logue systems, June.K.
VanLehn, P. W. Jordan, C.
Rose?, D. Bhembe,M.
Bo?ttner, A. Gaydos, M. Makatchev, U. Pap-puswamy, M. Ringenberg, A. Roque, S. Siler, R. Sri-vastava, and R. Wilson.
2002.
The architecture ofWhy2-Atlas: A coach for qualitative physics essaywriting.
In Proc.
of ITS.APPENDIX A.
Spoken Tutoring Dialogue Corpora ExamplesStart End Pane Text in Pane7.67 8.12 .tutor ok196.49 205.80 .student I dont know about this one at all at first I thought they did have thesame amount of force and then I thought they didn?t now I don?t know.EMOa negative.EMOb confused+sad.NOTES206.82 211.26 .tutor um yes at first you thought that force would be same isn?t it?211.60 212.84 .student yeah I thought so.EMOa negative.EMOb confused+sad.NOTES seems exactly the same as prior turn emotionally212.19 214.38 .tutor so then uh why did you change your mind?214.71 229.49 .student I don?t know I think I thought that they both would at first cause I wasthinking that they were both moving in the same direction and then I didn?tthen I thought well the sun?s making the earth move so it has more force.EMOa negative.EMOb confused+sad.NOTES228.59 258.71 .tutor no but that is not part of the question it has not been asked which isaccelerating more or which is changing in motion or you see in the firstquestion there was a specific question um in the first problem uh previousproblem there was a specific question which accelerates more or which whichsuffers greater change in motion here that has not been asked the only thingasked is about the force whether the force uh earth pulls equally on sun ornot that?s the only question258.98 263.69 .student well I think it does but I don?t know why I d- don?t I do they move inthe same direction I do- don?t.EMOa negative.EMOb confused.NOTES264.13 268.28 .tutor you see again you see they don?t have to move if a force acts on a body268.47 268.67 .student it.EMOa weak positive.EMOb enthusiastic.NOTES TUT - interrupts tutor to complete thought268.77 274.10 .tutor it does not mean that uh uh I mean it will um274.41 283.20 .student if two forces um apply if two forces react on each other then the forceis equal it?s the Newtons third law.EMOa positive.EMOb confident.NOTES280.70 290.61 .tutor um you see the uh actually in this case the motion is there but it is alittle complicated motion this is orbital motion290.77 291.10 .student mm-hm.EMOa weakly positive.EMOb confident.NOTES291.69 293.42 .tutor and uh just asFigure 2: Annotated Dialogue Excerpt from the Human-Human Spoken Tutoring Dialogue CorpusFigure 3: Emotion Annotation using Wavesurfer for the Spoken Tutoring Dialogue CorporaStart End Pane Text in Pane301.92 314.65 .tutor If gravity is the only force acting on an object, will it be moving or staying still?323.26 326.06 .student moving.EMOa neutral.EMOb.NOTES326.25 327.91 .tutor Could you please repeat that?328.91 331.55 .student moving.EMOa neutral.EMOb.NOTES331.68 335.85 .tutor I?m sorry, I?m having trouble understanding you.
Please try again.336.98 339.71 .student moving.EMOa negative.EMOb irritated.NOTES NLP domain340.62 347.28 .tutor According to Newton?s Second Law, if an object has a non-zero net forceacting on it, what will happen to it?357.58 360.78 .student it will stay still.EMOa weak negative.EMOb uncertain.NOTES361.84 371.28 .tutor Well, to compute the acceleration of a box, one property that you need to knowis its mass.
What else do you need to know to find the box?s acceleration?376.08 379.06 .student the direction.EMOa negative.EMOb uncertain.NOTES379.89 383.51 .tutor If you see a body accelerate, what caused that acceleration?386.40 389.30 .student force.EMOa positive.EMOb confident.NOTESFigure 4: Annotated Dialogue Excerpt from the ITSPOKE Human-Computer Spoken Tutoring Dialogue Corpus
