Proceedings of the Second Workshop on Statistical Machine Translation, pages 203?206,Prague, June 2007. c?2007 Association for Computational LinguisticsRule-based Translation With Statistical Phrase-based Post-editingMichel Simard, Nicola Ueffing, Pierre Isabelle and Roland KuhnInteractive Language Technologies GroupNational Research Council of CanadaGatineau, Canada, K1A 0R6firstname.lastname@nrc-cnrc.gc.caAbstractThis article describes a machine translationsystem based on an automatic post-editingstrategy: initially translate the input text intothe target-language using a rule-based MTsystem, then automatically post-edit the out-put using a statistical phrase-based system.An implementation of this approach basedon the SYSTRAN and PORTAGE MT sys-tems was used in the shared task of the Sec-ond Workshop on Statistical Machine Trans-lation.
Experimental results on the test dataof the previous campaign are presented.1 IntroductionSimard et al (2007) have recently shown how a sta-tistical phrase-based machine translation system canbe used as an automatic post-editing (APE) layer,on top of a rule-based machine translation system.The motivation for their work is the repetitive natureof the errors typically made by rule-based systems.Given appropriate training material, a statistical MTsystem can be trained to correct these systematic er-rors, therefore reducing the post-editing effort.
Thestatistical system views the output of the rule-basedsystem as the source language, and reference hu-man translations as the target language.
Because thetraining material for the APE layer will typically bedomain-specific, this process can be viewed as a wayof automatically adapting a rule-based system to aspecific application domain.This approach has been shown experimentallyto produce large improvements in performance notonly over the baseline rule-based system that it cor-rects, but also over a similar statistical phrase-basedMT system used in standalone mode, i.e.
translatingthe ?real?
source text directly: Simard et al report areduction in post-editing effort of up to a third whencompared to the input rule-based translation, and asmuch as 5 BLEU points improvement over the directSMT approach.These impressive results, however, were obtainedin a very specific and somewhat unusual context:the training and test corpora were extracted froma collection of manually post-edited machine trans-lations.
The two corpora (one English-to-French,one French-to-English) each contained three paral-lel ?views?
of the same data: 1) the source languagetext, 2) a machine translation of that text into thetarget language, as produced by a commercial rule-based MT system, and 3) the final target-languageversion of the text, produced by manually post-editing the machine translation.
Furthermore, thecorpus was very small, at least by SMT standards:500K words of source-language data in the French-to-English direction, 350K words in the English-to-French.
Because of this, the authors were left withtwo important questions: 1) how would the resultsscale up to much larger quantities of training data?and 2) are the results related to the dependent natureof the translations, i.e.
is the automatic post-editingapproach still effective when the machine and hu-man translations are produced independently of oneanother?With these two questions in mind, we partici-pated in the shared task of the Second Workshopon Statistical Machine Translation with an auto-matic post-editing strategy: initially translate the in-put text into the target-language using a rule-basedsystem, namely SYSTRAN, and automatically post-edit the output using a statistical phrase-based sys-tem, namely PORTAGE.
We describe our system inmore detail in Section 2, and present some experi-mental results in Section 3.2032 System descriptionOur system is composed of two main components:a rule-based MT system, which handles the initialtranslation into the target language, and a statisticalphrase-based post-editing system, which performsdomain-specific corrections and adaptations to theoutput.
We describe each component separately be-low.2.1 Rule-based TranslationThe initial source-to-target language translation isperformed using the SYSTRAN machine translationsystem, version 6.
A detailed overview of SYS-TRAN systems can be found in Dugast et al (2007).For this shared task, we used the French-to-Englishand English-to-French configurations of the system.Although it is possible to provide the system withspecialized lexica, we did not rely on this feature,and used the system in its basic ?out-of-the-box?configuration.2.2 Statistical Phrase-based Post-EditingThe output of the rule-based MT system describedabove is fed into a post-editing layer that performsdomain-specific corrections and adaptation.
Thisoperation is conceptually not very different from a?target-to-target?
translation; for this task, we usedthe PORTAGE system, a state-of-the-art statisticalphrase-based machine translation system developedat the National Research Council of Canada (NRC).1A general description of PORTAGE can be found in(Sadat et al, 2005).For our participation in this shared task, we de-cided to configure and train the PORTAGE systemfor post-editing in a manner as much as possiblesimilar to the corresponding translation system, thedetails of which can be found in (Ueffing et al,2007).
The main features of this configuration are:?
The use of two distinct phrase tables, contain-ing phrase pairs extracted from the Europarland the News Commentary training corpora re-spectively.?
Multiple phrase-probability feature functionsin the log-linear models, including a joint prob-1A version of PORTAGE is made available by the NRC toCanadian universities for research and education purposes.ability estimate, a standard frequency-basedconditional probability estimate, and variantsthereof based on different smoothing methods(Foster et al, 2006).?
A 4-gram language model trained on the com-bined Europarl and News Commentary target-language corpora.?
A 3-gram adapted language model: this istrained on a mini-corpus of test-relevant target-language sentences, extracted from the trainingmaterial using standard information retrievaltechniques.?
A 5-gram truecasing model, trained on thecombined Europarl and News Commentarytarget-language corpora.2.3 Training dataIdeally, the training material for the post-editinglayer of our system should consist in a corpus oftext in two parallel versions: on the one hand, rawmachine translation output, and on the other hand,manually post-edited versions of these translations.This is the type of data that was used in the initialstudy of Simard et al (2007).Unfortunately, this sort of training data is seldomavailable.
Instead, we propose using training ma-terial derived directly from standard, source-targetparallel corpora.
The idea is to translate the sourceportion of the parallel corpus into the target lan-guage, using the rule-based MT component.
Thepost-editing component can then be trained usingthis translation as ?source?
training material, and theexisting target portion of the parallel corpus as ?tar-get?
training material.
Note how this sort of datais subtly different from the data used by Simard etal.
: there, the ?target?
text was dependent on the?source?, in the sense that it was produced by manu-ally post-editing the machine translation; here, thetwo can be said to be independent, in the sensethat both ?source?
and ?target?
were produced inde-pendently by man and machine (but from the same?real?
source, of course).
It was one of the initialmotivations of the current work to verify to what ex-tent the performance of the APE approach is affectedby using two different translations (human and ma-204en ?
fr fr ?
enEuroparl (>32M words/language)SYSTRAN 23.06 20.11PORTAGE 31.01 30.90SYSTRAN+PORTAGE 31.11 30.61News Commentary (1M words/language)SYSTRAN 24.41 18.09PORTAGE 25.98 25.17SYSTRAN+PORTAGE 28.80 26.79Table 1: System performances on WMT-06 test.
Allfigures are single-reference BLEU scores, computedon truecased, detokenized translations.chine) instead of two versions of the same transla-tion (raw MT versus post-edited MT).We concentrated our efforts on the English-French language pair.
For each translation direc-tion, we prepared two systems: one for the Eu-roparl domain, and one for the News Commentarydomain.
The two systems have almost identicalconfigurations (phrase tables, log-linear model fea-tures, etc.
); the only differences between the twoare the adapted language model, which is computedbased on the specific text to be translated and theparameters of the log-linear models, which are opti-mized using domain-specific development sets.
Forthe Europarl domain system, we used the dev2006and devtest2006 data sets, while for the News Com-mentary, we used the nc-dev2007.
Typically, theoptimization procedure will give higher weights toEuroparl-trained phrase tables for the Europarl do-main systems, and inversely for the News Commen-tary domain systems.3 Experimental ResultsWe computed BLEU scores for all four systems onthe 2006 test data (test2006 for the Europarl do-main and nc-devtest2007 for the News Commen-tary).
The results are presented in Table 1.
As pointsof comparison, we also give the scores obtained bythe SYSTRAN systems on their own (i.e.
without apost-editing layer), and by the PORTAGE MT sys-tems on their own (i.e.
translating directly sourceinto target).The first observation is that, as was the casein the Simard et al study, post-editing (SYS-TRAN+PORTAGE lines) very significantly in-creases the BLEU scores of the rule-based system(SYSTRAN lines).
This increase is more spectacu-lar in the Europarl domain and when translating intoEnglish, but it is visible for all four systems.For the News Commentary domain, the APEstrategy (SYSTRAN+PORTAGE lines) clearly out-performs the direct SMT strategy (PORTAGE lines):translating into English, the gain exceeds 1.5 BLEUpoints, while for French, it is close to 3 BLEUpoints.
In contrast, for the Europarl domain, both ap-proaches display similar performances.
Let us recallthat the News Commentary corpus contains less than50K sentence pairs, totalling a little over one mil-lion words in each language.
With close to 1.3 mil-lion sentence pairs, the Europarl corpus is almost 30times larger.
Our results therefore appear to confirmone of the conjectures of the Simard et al study:that APE is better suited for domains with limitedquantities of available training data.
To better un-derstand this behavior, we trained series of APE andSMT systems on the Europarl data, using increas-ing amounts of training data.
The resulting learningcurves are presented in Figure 1.2As observed in the Simard et al study, while boththe SMT and APE systems improve quite steadilywith more data (note the logarithmic scale), SMTappears to improve more rapidly than APE.
How-ever, there doesn?t seem to be a clear ?crossover?point, as initially conjectured by Simard et al In-stead, SMT eventually catches up with APE (any-where between 100K and 1M sentence pairs), be-yond which point both approaches appear to be moreor less equivalent.
Again, one impressive featureof the APE strategy is how little data is actually re-quired to improve upon the rule-based system uponwhich it is built: around 5000 sentence pairs forEnglish-to-French, and 2000 for French-to-English.4 ConclusionsWe have presented a combination MT system basedon a post-editing strategy, in which a statisticalphrase-based system corrects the output of a rule-based translation system.
Experiments confirm the2The systems used for this experiment are simplified ver-sions of those described in Section 2, using only one phrasetable, a trigram language model and no rescoring; furthermore,they were optimized and tested on short sentences only.2050.10.120.140.160.180.20.220.240.260.280.31  10  100  1000BLEUscoreTraining sentences (x 1000)English to FrenchSYSTRANPORTAGESYSTRAN + PORTAGE0.10.120.140.160.180.20.220.240.260.280.31  10  100  1000BLEUscoreTraining sentences (x 1000)French to EnglishSYSTRANPORTAGESYSTRAN + PORTAGEFigure 1: BLEU scores on Europarl data under increasing amounts of training data for PORTAGE SMTalone and SYSTRAN MT with PORTAGE APE.conclusions of earlier studies: not only can phrase-based post-editing significantly improve the out-put of a rule-based MT system (in terms of BLEUscore), but when training data is scarce, it also out-performs a direct phrase-based MT strategy.
Fur-thermore, our results indicate that the training datafor the post-editing component does not need to bemanually post-edited translations, it can be gener-ated from standard parallel corpora.
Finally, our ex-periments show that while post-editing is most effec-tive when little training data is available, it remainscompetitive with phrase-based translation even withmuch larger amounts of data.This work opens the door to a number of lines ofinvestigation.
For example, it was mentioned earlierthat phrase-based APE could be seen as a form of au-tomatic domain-adaptation for rule-based methods.One thing we would like to verify is how this ap-proach compares to the standard ?lexical customiza-tion?
method proposed by most rule-based MT ven-dors.
Also, in the experiments reported here, wehave used identical configurations for the APE anddirect SMT systems.
However, it might be possibleto modify the phrase-based system so as to betteradapt it to the APE task.
For example, it could beuseful for the APE layer to ?look?
at the real source-language text, in addition to the MT output it is post-editing.
Finally, we have so far considered the front-end rule-based system as a ?black box?.
But in theend, the real question is: Which part of the rule-based processing is really making things easier forthe phrase-based post-editing layer?
Answering thisquestion will likely require diving into the internalsof the rule-based component.
These are all direc-tions that we are currently pursuing.AcknowledgementsThis work was done as part of a collaboration withSYSTRAN S.A.
Many thanks go to Jean Senellart,Jens Stephan, Dimitris Sabatakakis and all thosepeople behind the scene at SYSTRAN.ReferencesL.
Dugast, J. Senellart, and P. Koehn.
2007.
StatisticalPost-Edition on SYSTRAN Rule-Based TranslationSystem.
In Proceedings of the Second Workshop OnStatistical Machine Translation, Prague, Czech Re-public.G.
Foster, R. Kuhn, and H. Johnson.
2006.
PhrasetableSmoothing for Statistical Machine Translation.
InProceedings of EMNLP 2006, pages 53?61, Sydney,Australia.F.
Sadat, H. Johnson, A. Agbago, G. Foster, R. Kuhn,J.
Martin, and A. Tikuisis.
2005.
PORTAGE: APhrase-Based Machine Translation System.
In Pro-ceedings of the ACL Workshop on Building and UsingParallel Texts, pages 129?132, Ann Arbor, USA.M.
Simard, C. Goutte, and P. Isabelle.
2007.
Sta-tistical Phrase-Based Post-Editing.
In Human Lan-guage Technologies 2007: The Conference of theNorth American Chapter of the Association for Com-putational Linguistics; Proceedings of the Main Con-ference, pages 508?515, Rochester, USA.N.
Ueffing, M. Simard, S. Larkin, and H. Johnson.
2007.NRC?s PORTAGE system for WMT 2007.
In Pro-ceedings of the Second Workshop On Statistical Ma-chine Translation, Prague, Czech Republic.206
