A Little Goes a Long Way: Quick Authoring of Semantic KnowledgeSources for InterpretationCarolyn Penstein Rose?Carnegie Mellon Universitycprose@cs.cmu.eduBrian S. HallUniversity of Pittsburghmosesh@pitt.eduAbstractIn this paper we present an evaluation ofCarmel-Tools, a novel behavior oriented ap-proach to authoring and maintaining do-main specific knowledge sources for ro-bust sentence-level language understanding.Carmel-Tools provides a layer of abstractionbetween the author and the knowledge sources,freeing up the author to focus on the desiredlanguage processing behavior that is desired inthe target system rather than the linguistic de-tails of the knowledge sources that would makethis behavior possible.
Furthermore, Carmel-Tools offers greater flexibility in output rep-resentation than the context-free rewrite rulesproduced by previous semantic authoring tools,allowing authors to design their own predicatelanguage representations.1 IntroductionOne of the major obstacles that currently makes it imprac-tical for language technology applications to make use ofsophisticated approaches to natural language understand-ing, such as deep semantic analysis and domain level rea-soning, is the tremendous expense involved in authoringand maintaining domain specific knowledge sources.
Inthis paper we describe an evaluation of Carmel-Tools as aproof of concept for a novel behavior oriented approachto authoring and maintaining domain specific knowledgesources for robust sentence-level language understand-ing.
What we mean by behavior oriented is that Carmel-Tools provides a layer of abstraction between the authorand the knowledge sources, freeing up the author to fo-cus on the desired language processing behavior that isdesired in the target system rather than the linguistic de-tails of the knowledge sources that would make this be-havior possible.
Thus, Carmel-Tools is meant to makethe knowledge source engineering process accessible toa broader audience.
Carmel-Tools is used to author do-main specific semantic knowledge sources for the CarmelWorkbench (Rose?, 2000; Rose?
et al, 2002) that con-tains broad coverage domain general syntactic and lexicalknowledge sources for robust language understanding inEnglish.
Our evaluation demonstrates how Carmel-Toolscan be used to interpret sentences in the physics domainas part of a content-based approach to automatic essaygrading.Sentence: The man is moving horizontally at a constantvelocity with the pumpkin.Predicate Language Representation:((velocity id1 man horizontal constant non-zero)(velocity id2 pumpkin ?dir ?mag-change ?mag-zero)(rel-value id3 id1 id2 equal))Gloss: The constant, nonzero, horizontal velocity of theman is equal to the velocity of the pumpkin.Figure 1: Simple example of how Carmel-Tools buildsknowledge sources capable of assigning representationsto sentences that are not constrained to mirror the exactwording, structure, or literal surface meaning of the text.While much work has been done in the area of ro-bust semantic interpretation, current authoring tools forbuilding semantic knowledge sources (Cunningham etal., 2003; Jay et al, 1997) are tailored for informa-tion extraction tasks that emphasize the identification ofnamed entities such as people, locations, and organiza-tions.
While regular expression based recognizers, suchas JAPE (Cunningham et al, 2000), used for informationextraction systems, are not strictly limited to these stan-dard entity types, it is not clear how they would handleconcepts expressing complex relationships between enti-ties, where the complexity in the meaning can be real-ized with a much greater degree of surface syntactic vari-ation.
Outside of the information extraction domain, aconcept acquisition authoring environment called SGStu-dio (Wang and Acero, 2003) offers similar functionalityto JAPE for building language understanding modules fordialogue systems, with similar limitations.
Carmel-Toolsis more flexible in that it allows a wider range of linguis-tics expression that communicate the same idea to matchagainst the same pattern.
It accomplishes this by inducingpatterns that match against a deep syntactic parse ratherthan a stream of words, in order to normalize as muchsurface syntactic variation as possible, and thus reduc-ing the number of patterns that the learned rules mustaccount for.
Furthermore, Carmel-Tools offers greaterflexibility in output representation than the context-freerewrite rules produced by previous semantic authoringtools, allowing authors to design their own predicate lan-guage representations that are not constrained to followthe structure of the input text (See Figure 1 for a simpleexample and Figure 2 for a more complex example.).
SeeSection 3 and (Rose?, 2000; Rose?
et al, 2002) for moredetails about CARMEL?s knowledge source representa-tion.Note that the predicate language representation uti-lized by Carmel-Tools is in the style of Davidsonian eventbased semantics (Hobbs, 1985).
For example, in Figure1 notice that the first argument of each predicate is anidentification token that represents the whole predicate.These identification tokens can then be bound to argu-ments of other predicates, and in that way be used to rep-resent relationships between predicates.
For example, therel-value predicate expresses the idea that the predi-cates indicated by id1 and id2 are equal in value.While language understanding systems with this styleof analysis are not a new idea, the contribution of thiswork is a set of authoring tools that simplify the semanticknowledge sources authoring process.2 MotivationWhile the technology presented in this paper is not spe-cific to any particular application area, this work is mo-tivated by a need within a growing community of re-searchers working on educational applications of Natu-ral Language Processing to extract detailed informationfrom student language input to be used for formulatingspecific feedback directed at the details of what the stu-dent has uttered.
Such applications include tutorial dia-logue systems (Zinn et al, 2002; Popescue et al, 2003)and writing coaches that perform detailed assessments ofwriting content (Rose?
et al, 2003; Wiemer-Hastings etal., 1998; Malatesta et al, 2002) as opposed to just gram-mar (Lonsdale and Strong-Krause, 2003), and providedetailed feedback rather than just letter grades (Bursteinet al, 1998; Foltz et al, 1998).
Because of the importantrole of language in the learning process (Chi et al, 2001),and because of the unique demands educational applica-tions place on the technology, especially where detailedfeedback based on student language input is offered tostudents, educational applications present interesting op-portunities for this community.The area of automated essay grading has enjoyed agreat deal of success at applying shallow language pro-cessing techniques to the problem of assigning generalquality measures to student essays (Burstein et al, 1998;Foltz et al, 1998).
The problem of providing reliable, de-tailed, content-based feedback to students is a more diffi-cult problem, however, that involves identifying individ-ual pieces of content (Christie, 2003), sometimes called?answer aspects?
(Wiemer-Hastings et al, 1998).
Previ-ously, tutorial dialogue systems such as AUTO-TUTOR(Wiemer-Hastings et al, 1998) and Research MethodsTutor (Malatesta et al, 2002) have used LSA to per-form an analysis of the correct answer aspects presentin extended student explanations.
While straightfor-ward applications of bag of words approaches such asLSA have performed successfully on the content analy-sis task in domains such as Computer Literacy (Wiemer-Hastings et al, 1998), they have been demonstrated toperform poorly in causal domains such as research meth-ods (Malatesta et al, 2002) and physics (Rose?
et al,2003) because they base their predictions only on thewords included in a text and not on the functional rela-tionships between them.
Key phrase spotting approachessuch as (Christie, 2003) fall prey to the same problem.
Ahybrid rule learning approach to classification involvingboth statistical and symbolic features has been shown toperform better than LSA and Naive Bayes classification(McCallum and Nigam, 1998) for content analysis in thephysics domain (Rose?
et al, 2003).
Nevertheless, trainedapproaches such as this perform poorly on low-frequencyclasses and can be too coarse grained to provide enoughinformation to the system for it to provide the kind ofdetailed feedback human tutors offer students (Lepper etal., 1993) unless an extensive hierarchy of classes thatrepresent subtle differences in content is used (Popescueet al, 2003).
Popescue et al (2003) present impressiveresults at using a symbolic classification approach involv-ing hand-written rules for performing a detailed assess-ment of student explanations in the Geometry domain.Rule based approaches have also shown promise in non-educational domains.
For example, an approach to adapt-ing the generic rule based MACE system for informa-tion extraction has achieved an F-measure of 82.2% atthe ACE task (Maynard et al, 2002).
Authoring tools forspeeding up and simplifying the task of writing symbolicrules for assessing the content in student essays wouldmake it more practical to take advantage of the benefitsof rule based assessment approaches.3 Carmel-Tools Interpretation FrameworkSentence: During the fall of the elevator the man andthe keys have the same constant downward accelerationthat the elevator has.Predicate Language Representation:((rel-time id0 id1 id2 equal)(body-state id1 elevator freefall)(and id2 id3 id4)(rel-value id3 id5 id7 equal)(rel-value id4 id6 id6 equal)(acceleration id5 man down constant non-zero)(acceleration id6 keys down constant non-zero)(acceleration id7 elevator down constant non-zero))Gloss: The elevator is in a state of freefall at the sametime when there is an equivalence between the elevator?sacceleration and the constant downward nonzeroacceleration of both the man and the keysFigure 2: Example of how deep syntactic analysis facili-tates uncovering complex relationships encoded syntacti-cally within a sentenceOne of the goals behind the design of Carmel-Tools isto leverage off of the normalization over surface syntac-tic variation that deep syntactic analysis provides.
Whileour approach is not specific to a particular framework fordeep syntactic analysis, we have chosen to build uponthe publicly available LCFLEX robust parser (Rose?
et al,2002), the CARMEL grammar and semantic interpreta-tion framework (Rose?, 2000), and the COMLEX lexicon(Grishman et al, 1994).
This same broad coverage, do-main general interpretation framework has already beenused in a number of educational applications including(Zinn et al, 2002; VanLehn et al, 2002).Syntactic feature structures produced by the CARMELgrammar normalize those aspects of syntax that modifythe surface realization of a sentence but do not changeits deep functional analysis.
These aspects include tense,negation, mood, modality, and syntactic transformationssuch as passivization and extraction.
Thus, a sentenceand it?s otherwise equivalent passive counterpart wouldbe encoded with the same set of functional relationships,but the passive feature would be negative for the activeversion of the sentence and positive for the passive ver-sion.
A verb?s direct object is assigned the obj role re-gardless of where it appears in relation to the verb.
Fur-thermore, constituents that are shared between more thanone verb, for example a noun phrase that is the object ofa verb as well as the subject of a relative clause modi-fier, will be assigned both roles, in that way ?undoing?the relative clause extraction.
In order to do this analy-sis reliably, the component of the grammar that performsthe deep syntactic analysis of verb argument functionalrelationships was generated automatically from a featurerepresentation for each of 91 of COMLEX?s verb subcat-egorization tags (Rose?
et al, 2002).
Altogether there are519 syntactic configurations of a verb in relation to its ar-guments covered by the 91 subcategorization tags, all ofwhich are covered by the CARMEL grammar.CARMEL provides an interface to allow semantic in-terpretation to operate in parallel with syntactic interpre-tation at parse time in a lexicon driven fashion (Rose?,2000).
Domain specific semantic knowledge is encodeddeclaratively within a meaning representation specifica-tion.
Semantic constructor functions are compiled au-tomatically from this specification and then linked intolexical entries.
Based on syntactic head/argument rela-tionships assigned at parse time, the constructor func-tions enforce semantic selectional restrictions and assem-ble meaning representation structures by composing themeaning representation associated with the constructorfunction with the meaning representation of each of itsarguments.
After the parser produces a semantic featurestructure representation of the sentence, predicate map-ping rules then match against that representation in or-der to produce a predicate language representation in thestyle of Davidsonian event based semantics (Davidson,1967; Hobbs, 1985), as mentioned above.
The predicatemapping stage is the key to the great flexibility in repre-sentation that Carmel-Tools is able to offer.
The mappingrules perform two functions.
First, they match a featurestructure pattern to a predicate language representation.Next, they express where in the feature structure to lookfor the bindings of the uninstantiated variables that arepart of the associated predicate language representation.Because the rules match against feature structure patternsand are thus above the word level, and because the pred-icate language representations associated with them canbe arbitrarily complex, the mapping process is decompo-sitional in manner but is not constrained to rigidly followthe structure of the text.Figure 2 illustrates the power in the pairing betweendeep functional analyses and the predicate language rep-resentation.
The deep syntactic analysis of the sentencemakes it possible to uncover the fact that the expression?constant downward acceleration?
applies to the acceler-ation of all three entities mentioned in the sentence.
Thecoordination in the subject of the sentence makes it pos-sible to infer that both the acceleration of the man and ofthe keys are individually in an equative relationship withthe acceleration of the elevator.
The identification to-ken of the and predicate allows the whole representationof the matrix clause to be referred to in the rel-timepredicate that represents the fact that the equative rela-tionships hold at the same time as the elevator is in a stateFigure 3: Predicate Language Definition PageFigure 4: Example Map Pageof freefall.
But individual predicates, each representinga part of the meaning of the whole sentence, can also bereferred to individually if desired using their own identi-fication tokens.4 Carmel-Tools Authoring ProcessThe purpose of Carmel-Tools is to insulate the authorfrom the details of the underlying domain specific knowl-edge sources.
If an author were building knowledgesources by hand for this framework, the author would beresponsible for building an ontology for the semantic fea-ture structure representation produced by the parser, link-ing pointers into this hierarchy into entries in the lexicon,and writing predicate mapping rules.
With Carmel-Tools,the author never has to deal directly with these knowledgesources.
The Carmel-Tools authoring process involvesdesigning a Predicate Language Definition, augmentingthe base lexical resources by either loading raw humantutoring corpora or entering example texts by hand, andannotating example texts with their corresponding rep-resentation in the defined Predicate Language Defini-tion.
From this authored knowledge, CARMEL?s se-mantic knowledge sources can be generated and com-piled.
The knowledge source inference algorithm ensuresthat knowledge coded redundantly across multiple exam-ples is represented only once in the compiled knowledgesources.
The authoring interface allows the author or au-thors to test the compiled knowledge sources and thencontinue the authoring process by updating the PredicateLanguage Definition, loading additional corpora, anno-tating additional examples, or modifying already anno-tated examples.The Carmel-Tools authoring process was designed toeliminate the most time-consuming parts of the authoringFigure 5: Text Annotation Page Pageprocess.
In particular, its GUI interface guides authors insuch a way as to prevent them from introducing inconsis-tencies between knowledge sources, which is particularlycrucial when multiple authors work together.
For exam-ple, a GUI interface for entering propositional representa-tions for example texts insures that the entered represen-tation is consistent with the author?s Predicate LanguageDefinition.
Compiled knowledge sources contain point-ers back to the annotated examples that are responsiblefor their creation.
Thus, it is also able to provide trou-bleshooting facilities to help authors track down potentialsources for incorrect analyses generated from compiledknowledge sources.
When changes are made to the Pred-icate Language Definition, Carmel-Tools tests whethereach proposed change would cause conflicts with any an-notated example texts.
An example of such a changewould be deleting an argument from a predicate typewhere some example has as part of its analysis an instan-tiation of a predicate with that type where that argumentis bound.
If so, it lists these example texts for the authorand requires the author to modify the annotated examplesfirst in such a way that the proposed change will not causea conflict, in this case that would mean uninstantiating thevariable that the author desires to remove.
In cases wherechanges would not cause any conflict, such as adding anargument to a predicate type, renaming a predicate, to-ken, or type, or removing an argument that is not boundin any instantiated proposition, these changes are madethroughout the database automatically.4.1 Defining the Predicate Language DefinitionThe author begins the authoring process by designingthe propositional language that will be the output repre-sentation from CARMEL using the authored knowledgesources.
This is done on the Predicate Language Defi-nition page of the Carmel-Tools interface, displayed inFigure 3.
The author is free to develop a representationlanguage that is as simple or complex as is required by thetype of reasoning, if any, that will be applied to the outputrepresentations by the tutoring system as it formulates itsresponse to the student?s natural language input.The interface includes facilities for defining a list ofpredicates and Tokens to be used in constructing proposi-tional analyses.
Each predicate is associated with a basicpredicate type, which is a associated with a list of argu-ments.
Each basic predicate type argument is itself asso-ciated with a type that defines the range of atomic values,which may be tokens or identifier tokens referring to in-stantiated predicates, that can be bound to it.
Thus, to-kens also have types.
Each token has one or more basictoken types.
Besides basic predicate types and basic to-ken types, we also allow the definition of abstract typesthat can subsume other types.4.2 Generating Lexical Resources and AnnotatingExample SentencesWhen the predicate language definition is defined, thenext step is to generate the domain specific lexical re-sources and annotate example sentences with their corre-sponding representation within this defined predicate lan-guage.
The author begins this process on the ExampleMap Page, displayed in Figure 4.Carmel-Tools provides facilities for loading a raw hu-man tutoring corpus file.
Carmel-Tools then makes a listof each unique morpheme it finds in the file and then aug-ments both its base lexicon (using entries from COM-LEX), in order to include all morphemes found in thetranscript file that were not already included in the baselexicon, and the spelling corrector?s word list, so that itincludes all morphological forms of the new lexical en-tries.
It also segments the file into a list of student sen-tence strings, which are then loaded into a Corpus Ex-amples list, which appears on the right hand side of theinterface.
Searching and sorting facilities are provided tomake it easy for authors to find sentences that have cer-tain things in common in order to organize the list of sen-tences extracted from the raw corpus file in a convenientway.
For example, a Sort By Similarity buttoncauses Carmel-Tools to sort the list of sentences accord-ing to their respective similarity to a given text string ac-cording to an LSA match between the example string andeach corpus sentence.
The interface also includes the To-ken List and the Predicate List, with all defined tokensand predicates that are part of the defined predicate lan-guage.
When the author clicks on a predicate or token,the Examples list beside it will display the list of anno-tated examples that have been annotated with an analysiscontaining that token or predicate.Figure 5 displays how individual texts are annotated.The Analysis box displays the propositional representa-tion of the example text.
This analysis is constructedusing the Add Token, Delete, Add Predicate,and Modify Predicate buttons, as well as their sub-windows, which are not shown.
Once the analysis is en-tered, the author may indicate the compositional break-down of the example text by associating spans of textwith parts of the analysis by means of the OptionalMatch and Mandatory Match buttons.
For exam-ple, the noun phrase ?the man?
corresponds to the mantoken, which is bound in two places.
Each time a matchtakes place, the Carmel-Tools internal data structures cre-ate one or more templates that show how pieces of syn-tactic analyses corresponding to spans of text are matchedup with their corresponding propositional representation.From this match Carmel-Tools infers both that ?the man?is a way of expressing the meaning of the man token intext and that the subject of the verb hold can be bound tothe ?body1 argument of the become predicate.
By de-composing example texts in this way, Carmel-Tools con-structs templates that are general and can be reused inmultiple annotated examples.
It is these created templatesthat form the basis for all compiled semantic knowledgesources.
Thus, even if mappings are represented redun-dantly in annotated examples, they will not be repre-sented redundantly in the compiled knowledge sournces.The list of templates that indicates the hierarchical break-down of this example text are displayed in the Templateslist on the right hand side of Figure 5.
Note that while theauthor matches spans to text to portions of the meaningrepresentation, the tool stores mappings between featurestructures and portions of meaning representation, whichis a more general mapping.Templates can be generalized by entering paraphrasesfor portions of template patterns.
Internally what this ac-complishes is that all paraphrases listed can be interpretedby CARMEL as having the same meaning so that theycan be treated as interchangeable in the context of thistemplate.
A paraphrase can be entered either as a specificstring or as a Defined Type, including any type definedin the Predicate Language Definition.
What this means isthat the selected span of text can be replaced by any spanof text that can be interpreted in such a way that its pred-icate representation?s type is subsumed by the indicatedtype.4.3 Compiling Knowledge SourcesEach template that is created during the authoring pro-cess corresponds to one or more elements of each ofthe required domain specific knowledge sources, namelythe ontology, the lexicon with semantic pointers, and thepredicate mapping rules.
Using the automatically gener-ated knowledge sources, most of the ?work?
for mappinga novel text onto its predicate language representation isdone either by the deep syntactic analysis, where a lot ofsurface syntactic variation is factored out, and during thepredicate mapping phase, where feature structure patternsare mapped onto their corresponding predicate languagerepresentations.
The primary purpose of the sentencelevel ontology that is used to generate a semantic fea-ture structure at parse time is primarily for the purpose oflimiting the ambiguity produced by the parser.
Very littlegeneralization is obtained by the semantic feature struc-tures created by the automatically generated knowledgesources over that provided by the deep syntactic analysisalone.
By default, the automatically generated ontologycontains a semantic concept corresponding to each wordappearing in at least one annotated example.
A semanticpointer to that concept is then inserted into all lexical en-tries for the associated word that were used in one of theannotated examples.
An exception occurs where para-phrases are entered into feature structure representations.In this case, a semantic pointer is entered not only into theentry for the word from the sentence, but also the wordsfrom the paraphrase list, allowing all of the words in theparaphrase list to be treated as equivalent at parse time.The process is a bit more involved in the case of verbs.In this case it is necessary to infer based on the parses ofthe examples where the verb appears which set of sub-categorization tags are consistent, thus limiting the setof verb entries for each verb that will be associated witha semantic pointer, and thus which entries can be usedat parse time in semantic interpretation mode.
Carmel-Tools makes this choice by considering both which argu-ments are present with that verb in the complete databaseof annotated examples as well as how the examples werebroken down at the matching stage.
All non-extractedarguments are considered mandatory.
All extracted argu-ments are considered optional.
Each COMLEX subcattag is associated with a set of licensed arguments.
Thus,subcat tags are considered consistent if the set of licensedarguments contains at least all mandatory arguments anddoesn?t license any arguments that are not either manda-tory or optional.
Predicate mapping rules are generatedfor each template by first converting the correspondingsyntactic feature structure into the semantic representa-tion defined by the automatically generated ontology andlexicon with semantic pointers.
Predicate mapping rulesare then created that map the resulting semantic featurestructure into the associated predicate language represen-tation.5 EvaluationA preliminary evaluation was run for the physics domain.We used for our evaluation a corpus of essays written bystudents in response to 5 simple qualitative physics ques-tions such as ?If a man is standing in an elevator hold-ing his keys in front of his face, and if the cable hold-ing the elevator snaps and the man then lets go of thekeys, what will be the relationship between the positionof the keys and that of the man as the elevator falls to theground?
Explain why.?
A predicate language definitionwas designed consisting of 40 predicates, 31 predicatetypes, 160 tokens, 37 token types, and 15 abstract types.The language was meant to be able to represent phys-ical objects mentioned in our set of physics problems,body states (e.g., freefall, contact, non-contact), quanti-ties that can be measured (e.g., force, velocity, acceler-ation, speed, etc.
), features of these quantities (e.g., di-rection, magnitude, etc.
), comparisons between quanti-ties (equivalence, non-equivalence, relative size, relativetime, relative location), physics laws, and dependency re-lations.
An initial set of 250 example sentences was thenannotated, including sentences from each of a set of 5physics problems.Next a set of 202 novel test sentences, each between 4and 64 words long, was extracted from the corpus.
Sincecomparisons, such as between the accelerations of objectsin freefall together, are important for the reasoning in allof the questions used for corpus collection, we focusedthe coverage evaluation specifically on sentences pertain-ing to comparisons, such as in Figures 1 and 2.
The goalof the evaluation was to test the extent to which knowl-edge generated from annotated examples generalizes tonovel examples.Since obtaining the correct predicate language repre-sentation requires obtaining a correct syntactic parse, wefirst evaluated CARMEL?s syntactic coverage over thecorpus of test sentences to obtain an upper bound for ex-pected performance.
We assigned the syntactic interpre-tation of each sentence a score of None, Bad, Partial, orAcceptable.
A grade of None indicates that no interpreta-tion was built by the grammar.
Bad indicates that parseswere generated, but they contained errorfull functionalrelationships between constituents.
Partial indicates thatno parse was generated that covered the entire sentence,ut the portions that were completely correct for at leastone interpretation of the sentence.
Acceptable indicatesthat a complete parse was built that contained no incor-rect functional relationships.
If any word of the sentencewas not covered, it was one that would not change themeaning of the sentence.
For example, ?he had the samevelocity as you had?
is the same as ?he had the same ve-locity as you?, so if ?did?
was not part of the final parsebut other than that, the parse was fine, it was counted asAcceptable.
Overall the coverage of the grammar wasvery good.
166 sentences were graded Acceptable, whichis about 83% of the corpus.
8 received a grade of Partial,26 Bad, and 1 None.We then applied the same set of grades to the quality ofthe predicate language output.
Note that that the grade as-signed to an analysis represents the correctness and com-pleteness of the predicate representation the system ob-tained for that sentence.
In this case, a grade of Accept-able meant that all aspects of intended meaning were ac-counted for, and no misleading information was encoded.Partial indicated that some non-trivial part of the intendedmeaning was communicated.
Any interpretation contain-ing any misleading information was counted as Bad.
Ifno predicate language representation was returned, thesentence was graded as None.
As expected, grades forsemantic interpretation were not as high as for syntacticanalysis.
In particular, 107 were assigned a grade of Ac-ceptable, 45 were assigned a grade of Partial, 36 wereassigned a grade of Bad, and 14 received a nil interpre-tation.
Our evaluation demonstrates that knowledge gen-erated from annotated examples can be used to interpretnovel sentences, however, there are still gaps in the cov-erage of the automatically generated knowledge sourcesthat need to be filled in with new annotated examples.Furthermore, the small but noticeable percentage of badinterpretations indicates that some previously annotatedexamples need to be modified in order to prevent thesebad interpretations from being generated.6 Current DirectionsIn this paper we have introduced Carmel-Tools, a toolset for quick authoring of semantic knowledge sources.Our evaluation demonstrates that the semantic knowledgesources inferred from examples annotated using Carmel-Tools generalize to novel sentences.
We are continuingto work to enhance the ability of Carmel-Tools to learngeneralizable knowledge from examples as well as to im-prove the user friendliness of the interface.ReferencesJ.
Burstein, K. Kukich, S. Wolff, C. Lu, M. Chodorow,L.
Braden-Harder, and M. D. Harris.
1998.
Au-tomated scoring using a hybrid feature identificationtechnique.
In Proceedings of COLING-ACL?98, pages206?210.M.
T. H. Chi, S. A. Siler, H. Jeong, T. Yamauchi, andR.
G. Hausmann.
2001.
Learning from human tutor-ing.
Cognitive Science, (25):471?533.J.
Christie.
2003.
Automated essay marking for content:Does it work?
In CAA Conference Proceedings.H.
Cunningham, D. Maynard, and V. Tablan.
2000.
Jape:a java annotations patterns engine.
Institute for Lan-guage, Speach, and Hearing, University of Sheffield,Tech Report CS-00-10.H.
Cunningham, D. Maynard, K. Bontcheva, andV.
Tablan.
2003.
Gate: an architecture for develop-ment of robust hlt applications.
In Recent Advanced inLanguage Processing.D.
Davidson.
1967.
The logical form of action sentences.In N. Rescher, editor, The Logic of Decision and Ac-tion.P.
W. Foltz, W. Kintsch, and T. Landauer.
1998.
Themeasurement of textual coherence with latent semanticanalysis.
Discourse Processes, 25(2-3):285?307.R.
Grishman, C. Macleod, and A. Meyers.
1994.
COM-LEX syntax: Building a computational lexicon.
InProceedings of the 15th International Conference onComputational Linguistics (COLING-94).J.
R. Hobbs.
1985.
Ontological promiscuity.
In Proceed-ings of the 23rd Annual Meeting of the Association forComputational Linguistics.D.
Jay, J. Aberdeen, L. Hirschman, R. Kozierok,P.
Robinson, and M. Vilain.
1997.
Mixed-initiativedevelopment of language processing systems.
In FifthConference on Applied Natural Language Processing.M.
Lepper, M. Woolverton, D. Mumme, and J. Gurtner.1993.
Motivational techniques of expert human tutors:Lessons for the design of computer based tutors.
InS.
P. Lajoie and S. J. Derry, editors, Computers as Cog-nitive Tools, pages 75?105.D.
Lonsdale and D. Strong-Krause.
2003.
Automatedrating of esl essays.
In Proceedings of the HLT-NAACL2003 Workshop: Building Educational ApplicationsUsing Natural Language Processing.K.
Malatesta, P. Wiemer-Hastings, and J. Robertson.2002.
Beyond the short answer question with researchmethods tutor.
In Proceedings of the Intelligent Tutor-ing Systems Conference.D.
Maynard, K. Bontcheva, and H. Cunningham.
2002.Towards a semantic extraction of named entities.
In40th Anniversary meeting of the Association for Com-putational Linguistics.A.
McCallum and K. Nigam.
1998.
A comparison ofevent models for naive bayes text classification.
InProceedings of the AAAI-98 Workshop on Learning forText Classification.O.
Popescue, V. Aleven, and K. Koedinger.
2003.
Akowledge based approach to understanding students?explanations.
In Proceedings of the AI in EducationWorkshop on Tutorial Dialogue Systems: With a ViewTowards the Classroom.C.
P.
Rose?, D. Bhembe, A. Roque, and K. VanLehn.2002.
An efficient incremental architecture for ro-bust interpretation.
In Proceedings of the Human Lan-guages Technology Conference, pages 307?312.C.
P.
Rose?, A. Roque, D. Bhembe, and K VanLehn.
2003.A hybrid text classification approach for analysis ofstudent essays.
In Proceedings of the HLT-NAACL2003 Workshop: Building Educational ApplicationsUsing Natural Language Processing.C.
P. Rose?.
2000.
A framework for robust semantic in-terpretation.
In Proceedings of the First Meeting of theNorth American Chapter of the Association for Com-putational Linguistics, pages 311?318.K.
VanLehn, P. Jordan, C. P.
Rose?, and The Natural Lan-guag e Tutoring Group.
2002.
The architecture ofwhy2-atlas: a coach for qualitative physics essay writ-ing.
In Proceedings of the Intelligent Tutoring SystemsConference, pages 159?167.Y.
Wang and A. Acero.
2003.
Concept acquisition inexample-based grammar authoring.
In Proceedingsof the International Conference on Acoustics, Speech,and Signal Processing.P.
Wiemer-Hastings, A. Graesser, D. Harter, and the Tu-toring Res earch Group.
1998.
The foundationsand architecture of autotutor.
In B. Goettl, H. Halff,C.
Redfield, and V. Shute, editors, Intelligent Tutor-ing Systems: 4th International Conference (ITS ?98 ),pages 334?343.
Springer Verlag.C.
Zinn, J. D. Moore, and M. G. Core.
2002.
A 3-tier planning architecture for managing tutorial dia-logue.
In Proceedings of the Intelligent Tutoring Sys-tems Conference, pages 574?584.
