Proceedings of the 2009 Workshop on Multiword Expressions, ACL-IJCNLP 2009, pages 55?62,Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLPBottom-up Named Entity Recognitionusing a Two-stage Machine Learning MethodHirotaka Funayama Tomohide Shibata Sadao KurohashiKyoto University, Yoshida-honmachi,Sakyo-ku, Kyoto, 606-8501, Japan{funayama, shibata, kuro}@nlp.kuee.kyoto-u.ac.jpAbstractThis paper proposes Japanese bottom-upnamed entity recognition using a two-stage machine learning method.
Mostwork has formalized Named Entity Recog-nition as a sequential labeling problem, inwhich only local information is utilizedfor the label estimation, and thus a longnamed entity consisting of several mor-phemes tends to be wrongly recognized.Our proposed method regards a compoundnoun (chunk) as a labeling unit, and firstestimates the labels of all the chunks ina phrasal unit (bunsetsu) using a machinelearning method.
Then, the best label as-signment in the bunsetsu is determinedfrom bottom up as the CKY parsing al-gorithm using a machine learning method.We conducted an experimental on CRLNE data, and achieved an F measure of89.79, which is higher than previous work.1 IntroductionNamed Entity Recognition (NER) is a task of rec-ognizing named entities such as person names,organization names, and location.
It is used forseveral NLP applications such as Information Ex-traction (IE) and Question Answering (QA).
Mostwork uses machine learning methods such as Sup-port Vector Machines (SVMs) (Vapnik, 1995) andConditional Random Field (CRF) (Lafferty et al,2001) using a hand-annotated corpus (Krishnanand D.Manning, 2006; Kazama and Torisawa,2008; Sasano and Kurohashi, 2008; Fukushima etal., 2008; Nakano and Hirai, 2004; Masayuki andMatsumoto, 2003).In general, NER is formalized as a sequentiallabeling problem.
For example, regarding a mor-pheme as a basic unit, it is first labeled as S-PERSON, B-PERSON, I-PERSON, E-PERSON,S-ORGANIZATION, etc.
Then, considering thelabeling results of morphemes, the best NE labelsequence is recognized.When the label of each morpheme is estimated,only local information around the morpheme (e.g.,the morpheme, the two preceding morphemes, andthe two following morphemes) is utilized.
There-fore, a long named entity consisting of severalmorphemes tends to be wrongly recognized.
Letus consider the example sentences shown in Fig-ure 1.In sentence (1), the label of ?Kazama?
can berecognized to be S-PERSON (PERSON consist-ing of one morpheme) by utilizing the surroundinginformation such as the suffix ?san?
(Mr.) and theverb ?kikoku shita?
(return home).On the other hand, in sentence (2), when thelabel of ?shinyou?
(credit) is recognized to beB-ORGANIZATION (the beginning of ORGA-NIZATION), only information from ?hatsudou?
(invoke) to ?kyusai?
(relief) can be utilized, andthus the information of the morpheme ?ginkou?
(bank) that is apart from ?shinyou?
by three mor-phemes cannot be utilized.
To cope with this prob-lem, Nakano et al (Nakano and Hirai, 2004) andSasano et al (Sasano and Kurohashi, 2008) uti-lized information of the head of bunsetsu1.
In theirmethods, when the label of ?shinyou?
is recog-nized, the information of the morpheme ?ginkou?can be utilized.However, these methods do not work when themorpheme that we want to refer to is not a headof bunsetsu as in sentence (3).
In this example,when ?gaikoku?
(foreign) is recognized to be B-ARTIFACT (the beginning of ARTIFACT), wewant to refer to ?hou?
(law), not ?ihan?
(viola-tion), which is the head of the bunsetsu.This paper proposes Japanese bottom-up named1Bunsetsu is the smallest coherent phrasal unit inJapanese.
It consists of one or more content words followedby zero or more function words.55(1) kikoku-shitareturn homeKazama-san-waMr.Kazama TOP.
.
.?Mr.
Kazama who returned home?
(2) hatsudou-shitainvokeshinyou-kumiai-kyusai-ginkou-nocredit union relief bank GENsetsuritsu-mo.
.
.establishment?the establishment of the invoking credit union relief bank?
(3) shibunsyo-gizou-toprivate document falsification andgaikoku-jin-touroku-hou-ihan-noforeigner registration law violation GENutagai-desuspicion INS?on suspicion of the private document falsification and the violation of the foreigner registra-tion law?Figure 1: Example sentences.entity recognition using a two-stage machinelearning method.
Different from previous work,this method regards a compound noun as a la-beling unit (we call it chunk, hereafter), and es-timates the labels of all the chunks in the bun-setsu using a machine learning method.
In sen-tence (3), all the chunks in the second bunsetsu(i.e., ?gaikoku?, ?gaikoku-jin?, ?
?
?, ?gaikoku-jin-touroku-hou-ihan ?, ?
?
?, ?ihan?)
are labeled, andin the case that the chunk ?gaikoku-jin-touroku-hou?
is labeled, the information about ?hou?
(law)is utilized in a natural manner.
Then, in the bun-setsu, the best label assignment is determined.
Forexample, among the combination of ?gaikoku-jin-touroku-hou?
(ARTIFACT) and ?ihan?
(OTHER),the combination of ?gaikoku-jin?
(PERSON) and?touroku-hou-ihan?
(OTHER), etc., the best la-bel assignment, ?gaikoku-jin-touroku-hou?
(AR-TIFACT) and ?ihan?
(OTHER), is chosen basedon a machine learning method.
In this determi-nation of the best label assignment, as the CKYparsing algorithm, the label assignment is deter-mined by bottom-up dynamic programming.
Weconducted an experimental on CRL NE data, andachieved an F measure of 89.79, which is higherthan previous work.This paper is organized as follows.
Section 2 re-views related work of NER, especially focusing onsequential labeling based method.
Section 3 de-scribes an overview of our proposed method.
Sec-tion 4 presents two machine learning models, andSection 5 describes an analysis algorithm.
Section6 gives an experimental result.2 Related WorkIn Japanese Named Entity Recognition, the defi-nition of Named Entity in IREX Workshop (IREXclass examplePERSON Kimura SyonosukeLOCATION Taiheiyou (Pacific Ocean)ORGANIZATION Jimin-tou (Liberal DemocraticParty)ARTIFACT PL-houan (PL bill)DATE 21-seiki (21 century)TIME gozen-7-ji (7 a.m.)MONEY 500-oku-en (50 billions yen)PERCENT 20 percentTable 1: NE classes and their examples.Committee, 1999) is usually used.
In this def-inition, NEs are classified into eight classes:PERSON, LOCATION, ORGANIZATION, AR-TIFACT, DATE, TIME, MONEY, and PERCENT.Table 1 shows example instances of each class.NER methods are divided into two approaches:rule-based approach and machine learning ap-proach.
According to previous work, machinelearning approach achieved better performancethan rule-based approach.In general, a machine learning method is for-malized as a sequential labeling problem.
Thisproblem is first assigning each token (character ormorpheme) to several labels.
In an SE-algorithm(Sekine et al, 1998), S is assigned to NE com-posed of one morpheme, B, I, E is assigned to thebeginning, middle, end of NE, respectively, and Ois assigned to the morpheme that is not an NE2.The labels S, B, I, and E are prepared for each NEclasses, and thus the total number of labels is 33(= 8 * 4 + 1).The model for the label estimation is learnedbased on machine learning.
The following fea-tures are generally utilized: characters, type of2Besides, there are IOB1, IOB2 algorithm using onlyI,O,B and IOE1, IOE2 algorithm using only I,O,E (Kim andVeenstra, 1999).56HabuPERSON 0.111Habu-YoshiharuPERSON 0.438Habu-Yoshiharu-MeijinORGANIZATION0.083Yoshiharu Yoshiharu-MeijinMONEY0.075 OTHERe0.092MeijinOTHERe0.245(a):initial stateHabuPERSON 0.111Habu-YoshiharuPERSON 0.438Habu-Yoshiharu + MeijinPSN+OTHERe0.438+0.245Yoshiharu Yoshiharu + Meijinfinal outputanalysis directionMONEY0.075 MNY+OTHERe0.075+0.245MeijinOTHERe0.245(b):final outputFigure 2: An overview of our proposed method.
(the bunsetsu ?Habu-Yoshiharu-Meijin?
)character, POS, etc.
about the morpheme and thesurrounding two morphemes.
The methods utiliz-ing SVM or CRF are proposed.Most of NER methods based on sequential la-beling use only local information.
Therefore,methods utilizing global information are pro-posed.
Nakano et al utilized as a feature the wordsub class of NE on the analyzing direction in thebunsetsu, the noun in the end of the bunsetsu ad-jacent to the analyzing direction, and the head ofeach bunsetsu (Nakano and Hirai, 2004).
Sasanoet al utilized cache feature, coreference result,syntactic feature, and caseframe feature as struc-tural features (Sasano and Kurohashi, 2008).Some work acquired knowledge from unan-notated large corpus, and applied it to NER.Kazama et al utilized a Named Entity dic-tionary constructed from Wikipedia and a nounclustering result obtained using huge amount ofpairs of dependency relations (Kazama and Tori-sawa, 2008).
Fukushima et al acquired hugeamount of category-instance pairs (e.g., ?po-litical party - New party DAICHI?,?company-TOYOTA?)
by some patterns from a large Webcorpus (Fukushima et al, 2008).In Japanese NER researches, CRL NE data areusually utilized for the evaluation.
This data in-cludes approximately 10 thousands sentences innews paper articles, in which approximately 20thousands NEs are annotated.
Previous workachieved an F measure of about 0.89 using thisdata.3 Overview of Proposed MethodOur proposed method first estimates the label ofall the compound nouns (chunk) in a bunsetsu.Then, the best label assignment is determinedby bottom-up dynamic programming as the CKYparsing algorithm.
Figure 2 illustrates an overviewof our proposed method.
In this example, thebunsetsu ?Habu-Yoshiharu-Meijin?
(Grand Mas-ter Yoshiharu Habu) is analyzed.
First, the labelsof all the chunks (?Habu?, ?Habu-Yoshiharu?,?Habu-Yoshiharu-Meijin?, ?
?
?, ?Meijin?, etc.)
inthe bunsetsu are analyzed using a machine learn-ing method as shown in Figure 2 (a).We call the state in Figure 2 (a) initial state,where the labels of all the chunks have been es-timated.
From this state, the best label assign-ment in the bunsetsu is determined.
This pro-cedure is performed from the lower left (corre-sponds to each morpheme) to the upper right likethe CKY parsing algorithm as shown in Figure 2(b).
For example, when the label assignment for?Habu-Yoshiharu?
is determined, the label assign-ment ?Habu-Yoshiharu?
(PERSON) and the labelassignment ?Habu?
(PERSON) and ?Yoshiharu?
(OTHER) are compared, and the better one is cho-sen.
While grammatical rules are utilized in ageneral CKY algorithm, this method chooses bet-ter label assignment for each cell using a machinelearning method.The learned models are the followings:?
the model that estimates the label of a chunk(label estimation model)?
the model that compares two label assign-ments (label comparison model)The two models are described in detail in thenext section.57Habu Yoshiharu Meijin gaPERSON OTHEReinvalid invalidinvalidinvalidFigure 3: Label assignment for all the chunks inthe bunsetsu ?Habu-Yoshiharu-Meijin.
?4 Model Learning4.1 Label Estimation ModelThis model estimates the label for each chunk.
Ananalysis unit is basically bunsetsu.
This is because93.5% of named entities is located in a bunsetsuin CRL NE data.
Exceptionally, the following ex-pressions located in multiple bunsetsus tend to bean NE:?
expressions enclosed in parentheses (e.g., ?
?Himeyuri-no tou?
?
(The tower of Himeyuri)(ARTIFACT))?
expressions that have an entry in Wikipedia(e.g., ?Nihon-yatyou-no kai?
(Wild Bird So-ciety of Japan) (ORGANIZATION))Hereafter, bunsetsu is expanded when one of theabove conditions meet.
By this expansion, 98.6%of named entities is located in a bunsetsu3.For each bunsetsu, the head or tail functionwords are deleted.
For example, in the bun-setsu ?Habu-Yoshiharu-Meijin-wa?, the tail func-tion word ?wa?
(TOP) is deleted.
In the bunsetsu?yaku-san-bai?
(about three times), the head func-tion word ?yaku?
(about) is deleted.Next, for learning the label estimation model,all the chunks in a bunsetsu are attached to the cor-rect label from a hand-annotated corpus.
The la-bel set is 13 classes, which includes eight NE class(as shown in Table 1), and five classes: OTHERs,OTHERb, OTHERi, OTHERe, and invalid.The chunk that corresponds to a whole bun-setsu and does not contain any NEs is labeledas OTHERs, and the head, middle, tail chunkthat does not correspond to an NE is labeled asOTHERb, OTHERi, OTHERe, respectively4.3As an example in which an NE is not included by anexpanded bunsetsu, there are ?Toru-no Kimi?
(PERSON)and ?Osaka-fu midori-no kankyo-seibi-shitsu?
(ORGANI-ZATION).4Each OTHER is assigned to the longest chunk that satis-fies its condition in a chunk.1.
# of morphemes in the chunk2.
the position of the chunk in its bunsetsu3.
character type54.
the combination of the character type of adjoiningmorphemes- For the chunk ?Russian Army?, this feature is?Katakana,Kanji?5.
word class, word sub class, and several features pro-vided by a morphological analyzer JUMAN6.
several features6 provided by a parser KNP7.
string of the morpheme in the chunk8.
IPADIC7 feature- If the string of the chunk are registered in the fol-lowing categories of IPADIC: ?person?, ?lo-cation?, ?organization?, and ?general?, thisfeature fires.9.
Wikipedia feature- If the string of the chunk has an entry inWikipedia, this feature fires.- the hypernym extracted from its definition sen-tence using some patterns (e.g., The hyper-nym of ?the Liberal Democratic Party?
is apolitical party.)10.
cache feature- When the same string of the chunk appears in thepreceding context, the label of the precedingchunk is used for the feature.11.
particles that the bunsetsu includes12.
the morphemes, particles, and head morpheme in theparent bunsetsu13.
the NE/category ratio in a case slot of predicate/nouncase frame(Sasano and Kurohashi, 2008)- For example, in the case ga (NOM) of the pred-icate case frame ?kaiken?
(interview), the NEratio ?PERSON:0.245?
is assigned to the caseslot.
Hence, in the sentence ?Habu-ga kaiken-shita?
(Mr. Habu interviewed), the feature?PERSON:0.245?
is utilized for the chunk?Habu.?14.
parenthesis feature- When the chunk in a parenthesis, this featurefires.Table 2: Features for the label estimation model.The chunk that is neither any eight NE class northe above four OTHER is labeled as invalid.In an example as shown in Figure 3, ?Habu-Yoshiharu?
is labeled as PERSON, ?Meijin?
is la-beled as OTHERe, and the other chunks are la-beled as invalid.Next, the label estimation model is learned fromthe data in which the above label set is assigned5The following five character types are considered: Kanji,Hiragana, Katakana, Number, and Alphabet.6When a morpheme has an ambiguity, all the correspond-ing features fire.7http://chasen.aist-nara.ac.jp/chasen/distribution.html.ja58to all the chunks.
The features for the label esti-mation model are shown in Table 2.
Among thefeatures, as for feature (3), (5)?
(8), three cate-gories according to the position of a morphemein the chunk are prepared: ?head?, ?tail?, and?anywhere.?
For example, in the chunk ?Habu-Yoshiharu-Meijin,?
as for the morpheme ?Habu?,feature (7) is set to be ?Habu?
in ?head?
and as forthe morpheme ?Yoshiharu?, feature (7) is set to be?Yoshiharu?
in ?anywhere.
?The label estimation model is learned from pairsof label and feature in each chunk.
To classify themulti classes, the one-vs-rest method is adopted(consequently, 13 models are learned).
The SVMoutput is transformed by using the sigmoid func-tion 11+exp(?
?x) , and the transformed value is nor-malized so that the sum of the value of 13 labelsin a chunk is one.The purpose for setting up the label ?invalid?
isas follows.
In the chunk ?Habu?
and ?Yoshiharu?in Figure 3, since the label ?invalid?
has a rela-tively higher score, the score of the label PERSONis relatively low.
Therefore, when the label com-parison described in Section 4.2 is performed, thelabel assignment ?Habu-Yoshiharu?
(PERSON) islikely to be chosen.
In the chunk where the scoreof the label invalid has the highest score, the labelthat has the second highest score is adopted.4.2 Label Comparison ModelThis model compares the two label assignmentsfor a certain string.
For example, in the string?Habu-Yoshiharu?, the model compares the fol-lowing two label assignments:?
?Habu-Yoshiharu?
is labeled as PERSON?
?Habu?
is labeled as PERSON and ?Yoshi-haru?
is labeled as MONEYFirst, as shown in Figure 4, the two comparedsets of chunks are lined up by sandwiching ?vs.?
(The left one, right one is called the first set, thesecond set, respectively.)
When the first set is cor-rect, this example is positive: otherwise, this ex-ample is negative.
The max number of chunks foreach set is five, and thus examples in which thefirst or second set has more than five chunks arenot utilized for the model learning.Then, the feature is assigned to each example.The feature (13 dimensions) for each chunk is de-fined as follows: the first 12 dimensions are usedpositive:+1 Habu-Yoshiharu vs Habu + YoshiharuPSN PSN + MNY+1 Habu-Yoshiharu + Meijin vs Habu + Yoshiharu + MeijinPSN + OTHERe PSN + MONEY + OTHERe...negative:- 1 Habu-Yoshiharu-Meijin vs Habu-Yoshiharu + MeijinORG PSN + OTHERe...Figure 4: Assignment of positive/negative exam-ples.for each label, which is estimated by the label esti-mation model, and the last 13th dimension is usedfor the score of an SVM output.
Then, for the firstand second set, the features for each chunk are ar-ranged from the left, and zero vectors are placedin the remainder part.Figure 5 illustrates the feature for ?Habu-Yoshiharu?
vs ?Habu + Yoshiharu.?
The labelcomparison model is learned from such data us-ing SVM.
Note that only the fact that ?Habu-Yoshiharu?
is PERSON can be found from thehand-annotated corpus, and thus in the example?Habu-Yoshiharu-Meijin?
vs ?Habu + Yoshiharu-Meijin?, we cannot determine which one is cor-rect.
Therefore, such example cannot be used forthe model learning.5 AnalysisFirst, the label of all the chunks in a bunsetsu isestimated by using the label estimation model de-scribed in Section 4.1.
Then, the best label assign-ment in the bunsetsu is determined by applying thelabel comparison model described in Section 4.2iteratively as shown in Figure 2 (b).
In this step,the better label assignment is determined from bot-tom up as the CKY parsing algorithm.For example, the initial state shown in Figure2(a) is obtained using the label estimation model.Then, the label assignment is determined using thelabel comparison model from the lower left (cor-responds to each morpheme) to the upper right.In determining the label assignment for the cellof ?Habu-Yoshiharu?
as shown in 6(a), the modelcompares the label assignment ?B?
with the la-bel assignment ?A+D.?
In this case, the modelchooses the label assignment ?B?, that is, ?Habu- Yoshiharu?
is labeled as PERSON.
Similarly,in determining the label assignment for the cellof ?Yoshiharu-Meijin?, the model compares the59chunk Habu-Yoshiharu Habu Yoshiharulabel PERSON PERSON MONEYvector V11 0 0 0 0 V21 V22 0 0 0Figure 5: An example of the feature for the label comparison model.
(The example is ?Habu-Yoshiharuvs Habu + Yoshiharu?, and V11, V21, V22, and 0 is a vector whose dimension is 13.
)HabuPERSON 0.111Habu-YoshiharuPERSON 0.438Habu-Yoshiharu-MeijinORGANIZATION0.083Yoshiharu Yoshiharu-Meijinlabel assighment ?Habu-Yoshiharu?label assignment?Habu?
+ ?Yoshiharu?A B CEDMONEY0.075 OTHERe0.092MeijinOTHERe0.245F(a): label assignment for the cell ?Habu-Yoshiharu?.HabuPERSON 0.111Habu-YoshiharuPERSON 0.438Habu-Yoshiharu-MeijinORGANIZATION0.083Yoshiharu Yoshiharu + Meijinlabel assignment?Habu-Yoshiharu-Meijin?A B CEDMONEY0.075 MNY+OTHERe0.075+0.245MeijinOTHERe0.245label assignment?Habu?
+ ?Yoshiharu?
+ ?Meijin?label assignment?Habu-Yoshiharu?
+ ?Meijin?F(b): label assignment for the cell ?Habu-Yoshiharu-Meijin?.Figure 6: The label comparison model.label assignment ?E?
with the label assignment?D+F.?
In this case, the model chooses the labelassignment ?D+F?, that is, ?Yoshiharu?
is labeledas MONEY and ?Meijin?
is labeled as OTHERe.When the label assignment consists of multiplechunks, the content of the cell is updated.
Inthis case, the cell ?E?
is changed from ?Yoshi-haru-Meijin?
(OTHERe) to ?Yoshiharu + Meijin?
(MONEY + OTHERe).As shown in Figure 6(b), in determining the bestlabel assignment for the upper right cell, that is,the final output is determined, the model comparesthe label assignment ?A+D+F?, ?B+F?, and ?C?.When there are more than two candidates of labelassignments for a cell, all the label assignments arecompared in a pairwise, and the label assignmentthat obtains the highest score is adopted.In the label comparing step, the label as-signment in which OTHER?
follows OTHER?(OTHER?
- OTHER?)
is not allowed since eachOTHER is assigned to the longest chunk as de-scribed in Section 4.1.
When the first combina-tion of chunks equals to the second combinationof chunks, the comparison is not performed.6 ExperimentTo demonstrate the effectiveness of our proposedmethod, we conducted an experiment on CRL NEdata.
In this data, 10,718 sentences in 1,174 newsarticles are annotated with eight NEs.
The expres-sion to which it is difficult to annotate manually islabeled as OPTIONAL, and was not used for botha b c d elearn the label estimation model 1Corpus: CRL NE datalearn the label estimation model 2bobtain features for the label applyb c d eb c d eb c d ecomparison modellearn the label estimation model 2clearn the label estimation model 2dlearn the label estimation model 2elearn the label comparison modelFigure 7: 5-fold cross validation.the model learning8 and the evaluation.We performed 5-fold cross validation followingprevious work.
Different from previous work, ourwork has to learn the SVM models twice.
There-fore, the corpus was divided as shown in Figure 7.Let us consider the analysis in the part (a).
First,the label estimation model 1 is learned from thepart (b)-(e).
Then, the label estimation model 2bis learned from the part (c)-(e), and applying thelearned model to the part (b), features for learningthe label comparison model are obtained.
Simi-larly, the label estimation model 2c is learned fromthe part (b),(d),(e), and applying it to the part (c),features are obtained.
It is the same with the part8Exceptionally, ?OPTIONAL?
is used when the label es-timation model for OTHER?
and invalid is learned.60Recall PrecisionORGANIZATION 81.83 (3008/3676) 88.37 (3008/3404)PERSON 90.05 (3458/3840) 93.87 (3458/3684)LOCATION 91.38 (4992/5463) 92.44 (4992/5400)ARTIFACT 46.72 ( 349/ 747) 74.89 ( 349/ 466)DATE 93.27 (3327/3567) 93.12 (3327/3573)TIME 88.25 ( 443/ 502) 90.59 ( 443/ 489)MONEY 93.85 ( 366/ 390) 97.60 ( 366/ 375)PERCENT 95.33 ( 469/ 492) 95.91 ( 469/ 489)ALL-SLOT 87.87 91.79F-measure 89.79Table 3: Experimental result.
(d) and (e).
Then, the label comparison model islearned from the obtained features.
After that, theanalysis in the part (a) is performed by using boththe label estimation model 1 and the label compar-ison model.In this experiment, a Japanese morphologicalanalyzer, JUMAN9, and a Japanese parser, KNP10were adopted.
The two SVM models were learnedwith polynomial kernel of degree 2, and ?
in thesigmoid function was set to be 1.Table 6 shows an experimental result.
An F-measure in all NE classes is 89.79.7 Discussion7.1 Comparison with Previous WorkTable 7 presents the comparison with previ-ous work, and our method outperformed previ-ous work.
Among previous work, Fukushimaet al acquired huge amount of category-instance pairs (e.g., ?political party - New partyDAICHI?,?company-TOYOTA?)
by some patternsfrom a large Web corpus, and Sasano et al uti-lized the analysis result of corefer resolution as afeature for the model learning.
Therefore, in ourmethod, by incorporating these knowledge and/orsuch analysis result, the performance would be im-proved.Compared with Sasano et al, our methodachieved the better performance in analyzinga long compound noun.
For example, in thebunsetsu ?Oushu-tsuujyou-senryoku-sakugen-jyouyaku?
(Treaty on Conventional ArmedForces in Europe), while Sasano et al labeled?Oushu?
(Europe) as LOCATION, our methodcorrectly labeled ?Oushu-tsuujyou-senryoku-sakugen-jyouyaku?
as ARTIFACT.
Sasano etal.
incorrectly labeled ?Oushu?
as LOCATIONalthough they utilized the information about9http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman-e.html10http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp-e.htmlthe head of bunsetsu ?jyouyaku?
(treaty).
Inour method, for the cell ?Oushu?, invalid hasthe highest score, and thus the score of LOCA-TION relatively drops.
Similarly, for the cell?senryoku-sakugen-jyouyaku?, invalid has thehighest score.
Consequently, ?Oushu-tsuujyou-senryoku-sakugen-jyouyaku?
is correctly labeledas ARTIFACT.In the bunsetsu ?gaikoku-jin-touroku-hou-ihan?
(the violation of the foreigner registration law),while Sasano et al labeled ?touroku-hou?
as AR-TIFACT, our method correctly labeled ?gaikoku-jin-touroku-hou?
as ARTIFACT.
Sasano et al can-not utilize the information about ?hou?
that is use-ful for the label estimation since the head of thisbunsetsu is ?ihan.?
In contrast, in estimating thelabel of the chunk ?gaikoku-jin-touroku-hou?, theinformation of ?hou?
can be utilized.7.2 Error AnalysisThere were some errors in analyzing a Katakanaalphabet word.
In the following example, althoughthe correct is that ?Batistuta?
is labeled as PER-SON, the system labeled it as OTHERs.
(4) Italy-deItaly LOCkatsuyaku-suruactiveBatistuta-woBatistuta ACCkuwaetacallArgentineArgentine?Argentine called Batistuta who was active inItaly.
?There is not an entry of ?Batistuta?
in the dictio-nary of JUMAN nor Wikipedia, and thus only thesurrounding information is utilized.
However, thecase analysis of ?katsuyaku?
(active) is incorrect,which leads to the error of ?Batistuta?.There were some errors in applying the la-bel comparison model although the analysis ofeach chunk is correct.
For example, in thebunsetsu ?HongKong-seityou?
(Government ofHongKong), the correct is that ?HongKong-seityou?
is labeled as ORGANIZATION.
Asshown in Figure 8 (b), the system incorrectlylabeled ?HongKong?
as LOCATION.
As shownin Figure 8(a), although in the initial state,?HongKong-seityou?
was correctly labeled as OR-GANIZATION, the label assignment ?HongKong+ seityou?
was incorrectly chosen by the labelcomparison model.
To cope with this problem,we are planning to the adjustment of the value ?in the sigmoid function and the refinement of the61F1 analysis unit distinctive features(Fukushima et al, 2008) 89.29 character Web(Kazama and Torisawa, 2008) 88.93 character Wikipedia,Web(Sasano and Kurohashi, 2008) 89.40 morpheme structural information(Nakano and Hirai, 2004) 89.03 character bunsetsu feature(Masayuki and Matsumoto, 2003) 87.21 character(Isozaki and Kazawa, 2003) 86.77 morphemeproposed method 89.79 compound noun Wikipedia,structural informationTable 4: Comparison with previous work.
(All work was evaluated on CRL NE data using cross valida-tion.
)HongKongLOCATION HongKong-seityouORGANIZATION0.266 0.205seityouOTHERe0.184(a):initial stateHongKongLOCATION HongKong + seityouLOC+OTHERe0.266 0.266+0.184seityouOTHERe0.184(b):the final outputFigure 8: An example of the error in the label com-parison model.features for the label comparison model.8 ConclusionThis paper proposed bottom-up Named EntityRecognition using a two-stage machine learningmethod.
This method first estimates the label ofall the chunks in a bunsetsu using a machine learn-ing, and then the best label assignment is deter-mined by bottom-up dynamic programming.
Weconducted an experiment on CRL NE data, andachieved an F-measure of 89.79.We are planning to integrate this method withthe syntactic and case analysis method (Kawa-hara and Kurohashi, 2007), and perform syntactic,case, and Named Entity analysis simultaneously toimprove the overall accuracy.ReferencesKen?ichi Fukushima, Nobuhiro Kaji, and MasaruKitsuregawa.
2008.
Use of massive amountsof web text in Japanese named entity recogni-tion.
In Proceedings of Data Engineering Workshop(DEWS2008).
A3-3 (in Japanese).IREX Committee, editor.
1999.
Proceedings of theIREX Workshop.Hideki Isozaki and Hideto Kazawa.
2003.
Speeding upsupport vector machines for named entity recogni-tion.
Transaction of Information Processing Societyof Japan, 44(3):970?979.
(in Japanese).Daisuke Kawahara and Sadao Kurohashi.
2007.
Prob-abilistic coordination disambiguation in a fully-lexicalized Japanese parser.
In Proceedings of the2007 Joint Conference on Empirical Methods inNatural Language Processing and ComputationalNatural Language Learning (EMNLP-CoNLL2007),pages 304?311.Jun?ichi Kazama and Kentaro Torisawa.
2008.
In-ducing gazetteers for named entity recognition bylarge-scale clustering of dependency relations.
InProceedings of ACL-08: HLT, pages 407?415.Erik F. Tjong Kim and Jorn Veenstra.
1999.
Repre-senting text chunks.
In Proceedings of EACL ?99,pages 173?179.Vajay Krishnan and Christopher D.Manning.
2006.An effective two-stage model for exploiting non-local dependencies in named entity recognition.pages 1121?1128.John Lafferty, Andrew McCallun, and FernandoPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
In Proceedings of the Eighteenth In-ternational Conference (ICML?01), pages 282?289.Asahara Masayuki and Yuji Matsumoto.
2003.Japanese named entity extraction with redundantmorphological analysis.
In Proceeding of HLT-NAACL 2003, pages 8?15.Keigo Nakano and Yuzo Hirai.
2004.
Japanese namedentity extraction with bunsetsu features.
Transac-tion of Information Processing Society of Japan,45(3):934?941.
(in Japanese).Ryohei Sasano and Sadao Kurohashi.
2008.
Japanesenamed entity recognition using structural naturallanguage processing.
In Proceeding of Third In-ternational Joint Conference on Natural LanguageProcessing, pages 607?612.Satoshi Sekine, Ralph Grishman, and Hiroyuki Shin-nou.
1998.
A decision tree method for finding andclassifying names in japanese texts.
In Proceed-ings of the Sixth Workshop on Very Large Corpora(WVLC-6), pages 171?178.Vladimir Vapnik.
1995.
The Nature of StatisticalLearning Theory.
Springer.62
