Combining Multiple, Large-Scale Resources in a Reusable Lexiconfor Natural Language GenerationHongyan J ing  and Kath leen  McKeownDepartment of Computer ScienceColumbia UniversityNew York, NY 10027, USA{hjing, kathy} @cs.columbia.eduAbst rac tA lexicon is an essential component in a gener-ation system but few efforts have been madeto build a rich, large-scale lexicon and makeit reusable for different generation applications.In this paper, we describe our work to buildsuch a lexicon by combining multiple, heteroge-neous linguistic resources which have been de-veloped for other purposes.
Novel transforma-tion and integration of resources i  required toreuse them for generation.
We also applied thelexicon to the lexical choice and realization com-ponent of a practical generation application byusing a multi-level feedback architecture.
Theintegration of the lexicon and the architectureis able to effectively improve the system para-phrasing power, minimize the chance of gram-matical errors, and simplify the developmentprocess ubstantially.1 In t roduct ionEvery generation system needs a lexicon, and inalmost every case, it is acquired anew.
Few ef-forts in building a rich, large-scale, and reusablegeneration lexicon have been presented in liter-ature.
Most generation systems are still sup-ported by a small system lexicon, with limitedentries and hand-coded knowledge.
Althoughsuch lexicons are reported to be sufficient forthe specific domain in which a generation sys-tem works, there are some obvious deficiencies:(1) Hand-coding is time and labor intensive, andintroduction of errors is likely.
(2) Even thoughsome knowledge, such as syntactic structuresfor a verb, is domain-independent, often it isre-encoded each time a new application is un-der development.
(3) Hand-coding seriously re-stricts the scale and expressive power of gener-ation systems.
As natural anguage generationis used in more ambitious applications, this sit-uation calls for an improvement.Generally, existing linguistic resources are notsuitable to use for generation directly.
First,most large-scale linguistic resources so far werebuilt for language interpretation applications.They are indexed by words, whereas, an idealgeneration lexicon should be indexed by the se-mantic concepts to be conveyed, because the in-put of a generation system is at semantic leveland the processing during generation is basedon semantic oncepts, and because the mappingin the generation process is from concepts towords.
Second, the knowledge needed for gen-eration exists in a number of different resources,with each resource containing a particular typeof information; they can not currently be usedsimultaneously in a system.In this paper, we present work in building arich, large-scale, and reusable lexicon for gener-ation by combining multiple, heterogeneous lin-guistic resources.
The resulting lexicon containssyntactic, semantic, and lexical knowledge, in-dexed by senses of words as required by gener-ation, including:A complete list of syntactic subcategoriza-tions for each sense of a verb to supportsurface realization.A large variety of transitivity alternationsfor each sense of a verb to support para-phrasing.Frequency of lexical items and verb subcat-egorizations and also selectional constraintsderived from a corpus to support lexicalchoice.Rich lexical relations between lexical con-cepts, including hyponymy, antonymy, andso on, to support lexical choice.607The construction of the lexicon is semi-automatic, and the lexicon has been used forlexical choice and realization in a practical gen-eration system.
In Section 2, we describe theprocess to build the generation lexicon by com-bining existing linguistic resources.
In Section3, we show the application of the lexicon by ac-tually using it in a generation system.
Finally,we present conclusions and future work.2 Const ruct ing  a generat ion  lex iconby  merg ing  l ingu is t i c  resources2.1 Linguist ic resourcesIn our selection of resources, we aim primarilyfor accuracy of the resource, large coverage, andproviding a particular type of information es-pecially useful for natural anguage generation.four linguistic resources:1.
The WordNet on-line lexical database(Miller et al, 1990).
WordNet is a wellknown on-line dictionary, consisting of121,962 unique words, 99,642 synsets (eachsynset is a lexical concept represented bya set of synonymous words), and 173,941senses of words.
1 It is especially useful forgeneration because it is based on lexicalconcepts, rather than words, and becauseit provides several semantic relationships(hyponymy, antonymy, meronymy, entail-ment) which are beneficial to lexical choice.2.
English Verb Classes and Alternations(EVCA) (Levin, 1993).
EVCA is an ex-tensive linguistic study of diathesis alter-nations, which are variations in the realiza-tion of verb arguments.
For example, thealternation "there-insertion" transforms Aship appeared on the horizon to There ap-peared a ship on the horizon.
Knowledgeof alternations facilitates the generation ofparaphrases.
(Levin, 1993) studies 80 al-ternations.3.
The COMLEX syntax dictionary (Grish-man et al, 1994).
COMLEX containssyntactic information for 38,000 Englishwords.
The information includes subcat-egorization and complement restrictions.4.
The Brown Corpus tagged with WordNetsenses (Miller et al, 1993).
The original1As of Version 1.6, released in December 1997.Brown corpus (Ku~era and Francis, 1967)has been used as a reference corpus in manycomputational pplications.
Part of BrownCorpus has been tagged with WordNetsenses manually by the WordNet group.We use this corpus for frequency measure-ments and exacting selectional constraints.2.2 Combin ing  l inguist ic resourcesIn this section, we present an algorithm formerging data from the four resources in a man-ner that achieves high accuracy and complete-ness.
We focus on verbs, which play the mostimportant role in deciding phrase and sentencestructure.Our algorithm first merges COMLEX andEVCA, producing a list of syntactic subcate~gorizations and alternations for each verb.
Dis-tinctions in these syntactic restrictions accord-ing to each sense of a verb are achieved in thesecond stage, where WordNet is merged withthe result of the first step.
Finally, the corpusinformation is added, complementing the staticresources with actual usage counts for each syn-tactic pattern.
This allows us to detect rarelyused constructs that should be avoided duringgeneration, and possibly to identify alternativesthat are not included in the lexical databases.2.2.1 Merg ing  COMLEX and EVCAAlternations involve syntactic transformationsof verb arguments.
They are thus a means toalleviate the usual lack of alternative ways toexpress the same concept in current generationsystems.EVCA has been designed for use by humans,not computers.
We need therefore to convertthe information present in Levin's book (Levin,1993) to a format that can be automaticallyanalyzed.
We extracted the relevant informa-tion for each verb using the verb classes towhich the various verbs are assigned; membersof the same class have the same syntactic behav-ior in terms of allowable alternations.
EVCAspecifies a mapping between words and wordclasses, associating each class with alternationsand with subcategorization frames.
Using themapping from word and word classes, and fromword classes to alternations, alternations foreach verb are extracted.We manually formatted the alternate pat-terns in each alternation in COMLEX format.608The reason to choose manual formatting ratherthan automating the process is to guaranteethe reliability of the result.
In terms of time,manual formatting process is no more expensivethan automation since the total number of alter-nations is smail(80).
When an alternate patterncan not be represented by the labels in COM-LEX, we need to added new labels during theformatting process; this also makes automatingthe process difficult.The formatted EVCA consists of sets of ap-plicable alternations and subcategorizations for3,104 verbs.
We show the sample entry for theverb appear in Figure 1.
Each verb has 1.9 alter-nations and 2.4 subcategorizations on average.The maximum number of alternations (13) isrealized for the verb "roll".The merging of COMLEX and EVCA isachieved by unification, which is possible dueto the usage of similar representations.
Twopoints are worth to mention: (a) When a moregeneral form is unified with a specific one, thelater is adopted in final result.
For example, theunification of PP2 and PP-PRED-RS 3 is PP-PRED-RS.
(b) Alternations are validated by thesubcategorization information.
An alternationis applicable only if both alternate patterns areapplicable.Applying this algorithm to our lexical re-sources, we obtain rich subcategorization a dalternation information for each verb.
COM-LEX provides most subcategorizations, whileEVCA provides certain rare usages of a verbwhich might be missing from COMLEX.
Con-versely, the alternations in EVCA are validatedby the subcategorizations in COMLEX.
Themerging operation produces entries for 5,920verbs out of 5,583 in COMLEX and 3,104 inEVCA.
4 Each of these verbs is associated with5.2 subcategorizations and 1.0 alternation onaverage.
Figure 2 is an updated version of Fig-ure 1 after this merging operation.2.2.2 Merging COMLEX/EVCA withWordNetWordNet is a valuable resource for generationbecause most importantly the synsets provide2The verb can take a prepositional phraseSThe verb can take a prepositional phrase, and thesubject of the prepositional phrase is the same as theverb's42,947 words appear in both resources.appear:((INTm%NS)(LOCPP)(pp)(ADJ-PFA-PART)(INTKANS THEKE-V-SUBJ :ALT There - Inser t ion)(LOCPP THEKE-V-SUBJ-LOCPP :ALT There - Inser t ion)(LOCPP LOCPP-V-SUBJ :ALT Locat ive_ Invers ion) )Figure h Alternations and subcategorizationsfrom EVCA for the verb appear.~ppefl~r:( (PP -T0- INF-KS  :PVAL ( " to" ) )(PP-PKED-RS :PVAL ("to .... of" "under .... against""in favor of' ' "before" "at"))(EXTRAP-T0-NP-S)(INTRANS)(INTRANS THERE-V-SUBJ :ALT There-Insertion)(L0CPP THEKE-V-SUBJ-L0CPP :ALT There-Insertion)(LOCPP L0CPP-V-SUBJ :ALT Locative_Inversion)))Figure 2: Entry for the verb appear after merg-ing COMLEX with EVCA.a mapping between concepts and words.
Its in-clusion of rich lexical relations also provide basisfor lexical choice.
Despite of these advantages,the syntactic information in WordNet is rela-tively poor.
Conversely, the result we obtainedafter combining COMLEX and EVCA has richsyntactic information, but this information isprovided at word level thus unsuitable to usefor generation directly.
These complementaryresources are therefore combined in the secondstage, where the subcategorizations and alter-nations from COMLEX/EVCA for each wordare assigned to each sense of the word.Each synset in WordNet is linked with a listof verb frames, each of which represents a sim-ple syntactic pattern and general semantic on-straints on verb arguments, e.g., Somebody -ssomething.
The fact that WordNet contains thissyntactic information(albeit poor) makes it pos-sible to link the result from COMLEX/EVCAwith WordNet.The merging operation is based on a compat-ibility matrix, which indicates the compatibilityof each subcategorization in COMLEX/EVCAwith each verb frame in WordNet.
The sub-609categorizations and alternations listed in COM-LEX/EVCA for each word is then assigned todifferent senses of the word based on their com-patibility with the verbs frames listed underthat sense of the word in WordNet.
For exam-ple, if for a certain word, the subcategorizationsPP-PRED-RS and NP are listed for the wordin COMLEX/EVCA, and the verb frame some-body -s  PP  is listed for the first sense of theword in WordNet, then PP-PRED-RS will beassigned to the first sense of the word while NPwill not.
We also keep in the lexicon the gen-eral constraint on verb arguments from Word-Net frames.
Therefore, for this example, theentry for the first sense of w indicates that theverb can take a prepositional phrase as a com-plement, the subject of the verb is the sameas the subject of the prepositional phrase, andthe subject should be in the semantic ategory"somebody".
As you can see, the result incorpo-rates information from three resources and butis more informative than any of them.
An alter-nation is considered applicable to a word senseif both alternate patterns have matchable verbframes under that sense.The compatibility matrix is the kernel of themerging operations.
The 147"35 matrix (147subcategorizations from COMLEX/EVCA, 35verb frames from WordNet) was first manuallyconstructed based on human understanding.
Inorder to achieve high accuracy, the restrictionsto decide whether a pair of labels are compatibleare very strict when the matrix was first con-structed.
We then use regressive testing to ad-just the matrix based on the analysis of mergingresults.
During regressive testing, we first mergeWordNet with COMLEX/EVCA using currentversion of compatibility matrix, and write allinconsistencies to a log file.
In our case, an in-consistency occurs if a subcategorization r al-ternation in COMLEX/EVCA for a word cannot be assigned to any sense of the word, ora verb frame for a word sense does not matchany subcategorization for that word.
We thenanalyze the log file and adjust the compatibil-ity matrix accordingly.
This process repeated6 times until when we analyze a fair amount ofinconsistencies in the log file, they are no moredue to over-restriction of the compatibility ma-trix.Inconsistencies between WordNet and COM-appear:sense  1 give an impression((PP-T0-INF-RS :PVAL ("to") :SO ((sb, - ) ) )(TO-INF-RS :SO ((sb, -)))(NP-PRED-RS :SO ((sb, -)))(ADJP-PRED-RS :$0 ((sb, -) (sth, -)))))sense 2 become visible((PP-TO-INF-RS :PVAL ("to"):SO ((sb, --) (sth, -)))o , ,(INTRANS THERE-V-SUBJ: ALT there-insertion:SO ((sb, -) (sth, -))))sense  8 have an outward expression((NP-PRED-RS :SO ((sth, -)))(ADJP-PRED-RS :SO ((sb, -) (sth, -))))Figure 3: Entry for the verb appear after merg-ing WordNet with the result from COMLEXand EVCA.LEX/EVCA result unmatching subcategoriza-tions or verb frames.
On average, 15% of sub-categorizations and alternations for a word cannot be assigned to any sense of the word, mostlydue to the incompleteness of syntactic informa-tion in WordNet; 2% verb frames for each senseof a word does not match any subcategoriza-tions for the word, either due to incomplete-ness of COMLEX/EVCA or erroneous entriesin WordNet.The lexicon at this stage is a rich set of sub-categorizations and alternations for each senseof a word, coupled with semantic onstraints ofverb arguments.
For 5,920 words in the resultafter combining COMLEX and EVCA, 5,676words also appear in WordNet and each wordhas 2.5 senses on average.
After the mergingoperation, the average number of subcatego-rizations is refined from 5.2 per verb in COM-LEX/EVCA to 3.1 per sense, and the averagenumber of alternations i refined from 1.0 perverb to 0.2 per sense.
Figure 3 shows the resultfor the verb appear after the merging operation.2.3 Corpus  analys isFinally, we enriched the lexicon with languageusage information derived from corpus analy-sis.
The corpus used here is the Brown Corpus.The language usage information in the lexiconinclude: (1) frequency of each word sense; (2)frequency of subcategorizations for each wordsense.
A parser is used to recognize the subcat-egorization of a verb.
The corpus analysis in-610formation complements he subcategorizationsfrom the static resources by marking potentialsuperfluous entries and supplying entries thatare possibly missing in the lexicai databases; (3)semantic onstraints of verb arguments.
Thearguments of each verb are clustered based onhyponymy hierarchy in WordNet.
The seman-tic categories we thus obtained are more specificcompared to the general constraint(animate orinanimate) encoded in WordNet frame represen-tation.
The language usage information is espe-cially useful in lexicai choice.2.4 DiscussionMerging resources is not a new idea and pre-vious work has investigated integration of re-sources for machine translation and interpreta-tion (Klavans et al, 1991), (Knight and Luk,1994).
Whereas our work differs from previ-ous work in that for the first time, a generationlexicon is built by this technique; unlike otherwork which aims to combine resources with sim-ilar type of information, we select and combinemultiple resources containing different ypes ofinformation; while others combine not well for-matted lexicon like LDOCE (Longman Dictio-nary of Contemporary English), we chose wellformatted resources (or manually format he re-source) so as to get reliable and usable results;semi-automatic rather than fully automatic ap-proach is adopted to ensure accuracy; corpusanalysis based information is also linked withinformation from static resources.
By thesemeasures, we are able to acquire an accurate,reusable, rich, and large-scale lexicon for natu-ral language generation.3 App l i ca t ions3.1 ArchitectureWe applied the lexicon to lexical choice andlexical realization in a practical generation sys-tem.
First we introduce the architecture of lexi-cal choice and realization and then describe theoverall system.A multi-level feedback architecture as shownin Figure 4 was used for lexical choice and real-ization.
We distinguish two types of concepts:semantic oncepts and lexicai concepts.
A se-mantic concept is the semantic meaning that auser wants to convey, while a lexical concept is alexical meaning that can be represented by a setI Sentence Planner I~i uoncepts to Le?ical Concepts11 ~01 Lexical Concepts"~} \[ Mapping from Lexicall i ~..~ii \[ Concepts to Words \[ - - - -~rdNe)~Generafi~oand Syntactic Paraphrases - - - ~\[ Surface Realizatio~Natural Language OutputFigure 4: The Architecture for Lexical Choiceand Realizationof synonymous words, such as synsets defined inWordNet.
Paraphrases are also distinguishedinto 3 types according to whether they are atthe semantic, lexical, or syntactic level.
For ex-ample, if asked whether you will be at hometomorrow, then the answers "I'll be at work to-morrow", "No, I won't be at home.
', and "I'mleaving for vacation tonight" are paraphrases atthe semantic level.
Paraphrases like "He boughtan umbrella" and "He purchased an umbrella"are at the lexical level since they are acquiredby substituting certain words with synonymouswords.
Paraphrases like "A ship appeared onthe horizon" and "On the horizon appeared aship" are at the syntactic level since they onlyinvolve syntactic transformations.
Therefore,all paraphrases introduced by alternations areat syntactic level.
Our architecture includes lev-els corresponding to these 3 levels of paraphras-ing.The input to the lexical choice and realiza-tion module is represented assemantic oncepts.In the first stage, semantic paraphrasing is car-ried out by mapping semantic oncepts to lex-ical concepts.
Generally, semantic level para-phrases are very complex.
They depend on the611situation, the domain, and the semantic rela-tions involved.
Semantic paraphrases are repre-sented eclaratively in a database file which canbe edited by the users.
The file is indexed bysemantic oncepts and under each entry, a listof lexical concepts that can be used to realizethe semantic oncept are provided.In the second stage, we use the lexical re-source that we constructed to choose words forthe lexical concepts produced by stage 1.
Thelexicon is indexed by lexical concepts that pointto synsets in WordNet.
These synsets repre-sent a set of synonymous words and thus, it isat this stage that lexical paraphrasing is han-dled.
In order to choose which word to use forthe lexical concept, we use domain-independentconstraints that are included in the lexicon aswell as domain-specific constraints.
Syntacticconstraints that come from the detailed sub-categorizations linked to each word sense is adomain-independent constraint.
Subcategoriza-tions are used to check that the input can berealized by the word.
For example, if the in-put has 3 arguments, then words which takeonly 2 arguments can not be selected.
Seman-tic constraints on verb argument derived fromWordNet and the corpus are used to check theagreement of the arguments.
For example, ifthe input subject argument is an animate, thenwords which take only inanimate subject cannot be selected.
Frequency information derivedfrom the corpus is also used to constrain wordchoice.
Besides the above domain-independentconstraints other constraints pecific to a do-main might also be needed to choose an ap-propriate word for the lexical concept.
Intro-ducing the combined lexicon at this stage al-lows us to produce many lexical paraphraseswithout much effort; it also allows us to sep-arate domain-independent a d domain-specificconstraints in lexical choice so that domain-independent constraints can be reused in eachapplication.The third stage produces a structure repre-sented as a high level sentence structure, withsubcategorizations and words associated witheach sentence.
At this stage, information inthe lexical resource about subcategorization a dalternations are applied in order to generatesyntactic paraphrases.
Output of this stage isthen fed directly to the surface realization pack-age, the FUF/SURGE system (Elhadad, 1992;Robin, 1994).
To choose which alternate pat-tern of an alternation to use, we use informationsuch as focus of the sentence as criteria; whenthe two alternates are not distinctively different,such as "He knocked the door" and "He knockedat the door", one of them is randomly chosen.The application of subcategorizations i  the lex-icon at this stage helps to check that the outputis grammatically correct, and alternations canproduce many syntactic paraphrases.The above refining processing is interactive.When a lower level can not find a possible can-didate to realize the high level representation,feedback is sent to the higher level module,which then makes changes accordingly.3.2 P lanDOCUsing the proposed architecture, we applied thelexicon to a practical generation system, PIan-DOC.
PlanDOC is an enhancement to Bell-core's LEIS-PLAN TM network planning prod-uct.
It transforms lengthy execution tracesof engineer's interaction with LEIX-PLAN intohuman-readable summaries.For each message in PlanDOC, at least 3paraphrases are defined at semantic level.
Forexample, '~rhe base plan called for one fiber ac-tivation at CSA 2100" and "There was one fiberactivation at CSA 2100" are semantic para-phrases in PlanDOC domain.
At the lexicallevel, we use synonymous words from WordNetto generate lexical paraphrases.
A sample lexi-cal paraphrase for "The base plan called for onefiber activation at CSA 2100" is "The base planproposed one fiber activation at CSA 2100".Subcategorizations and alternations from thelexicon are then applied at the syntactic level.After three levels of paraphrasing, each mes-sage in PlanDOC on average has over 10 para-phrases.For a specific domain such as PlanDOC, anenormous proportion of a general exicon likethe one we constructed is unrelated thus un-used at all.
On the other hand, domain-specificknowledge may need to be added to the lexicon.The problem of how to adapt a general exiconto a particular application domain and mergedomain ontologies with a general exicon is outof the scope of this paper but discussed in (Jing,1998).6124 Conclus ionIn this paper, we present research on building arich, large-scale, and reusable l xicon for gener-ation by combining multiple heterogeneous lin-guistic resources.
Novel semi-automatic rans-formation and integration were used in combin-ing resources to ensure reliability of the result-ing lexicon.
The lexicon, together with a multi-level feedback architecture, is used in a practicalgeneration system, PlanDOC.The application of the lexicon in a generationsystem such as PlanDOC has many advantages.First, paraphrasing power of the system can begreatly improved ue to the introduction ofsyn-onyms at the lexical concept level and alterna-tions at the syntactic level.
Second, the integra-tion of the lexicon and the flexible architectureenables us to separate the domain-dependentcomponent of the lexical choice module fromdomain-independent components so they canbe reused.
Third, the integration of the lexi-con with the surface realization system helps inchecking for grammatical errors and also sim-plifies the interface input to the realization sys-tem.
For these reasons, we were able to developPlanDOC system in a short time.Although the lexicon was developed for gen-eration, it can be applied in other applicationstoo.
For example, the syntactic-semantic con-straints can be used for word sense disambigua-tion (Jing et al, 1997); The subcategoriza-tion and alternations from EVCA/COMLEXare better resources for parsing; WordNet en-riched with syntactic information might also beof value to many other applications.AcknowledgmentThis material is based upon work supported bythe National Science Foundation under GrantNo.
IRI 96-19124, IRI 96-18797 and by a grantfrom Columbia University's Strategic InitiativeFund.
Any opinions, findings, and conclusionsor recommendations expressed in this materialare those of the authors and do not necessarilyreflect he views of the National Science Foun-dation.ReferencesMichael Elhadad.
1992.
Using Argumenta-tion to Control Lexical Choice: A FunctionalUnification-Based Approach.
Ph.D. thesis,Department of Computer Science, ColumbiaUniversity.Ralph Grishman, Catherine Macleod, andAdam Meyers.
1994.
COMLEX syntax:Building a computational lexicon.
In Proceed-ings of COLING'9$, Kyoto, Japan.Hongyan Jing, Vasileios Hatzivassilogiou, Re-becca Passonneau, and Kathleen McKeown.1997.
Investigating complementary methodsfor verb sense pruning.
In Proceedings ofA NL P '97 Lexical Semantics Workshop, pages58-65, Washington, D.C., April.Hongyan Jing.
1998.
Applying wordnet to nat-ural language generation.
In To appear inthe Proceedings of COLING-ACL'98 work-shop on the Usage of WordNet in NaturalLanguage Processing Systems, University ofMontreal, Montreal, Canada, August.J.
Klavans, R. Byrd, N. Wacholder, andM.
Chodorow.
1991.
Taxonomy and poly-semy.
Technical Report Research Report RC16443, IBM Research Division, T.J. Wat-son Research Center, Yorktown Heights, NY10598.Kevin Knight and Steve K. Luk.
1994.
Build-ing a large-scale knowledge base for machinetranslation.
In Proceedings of AAAI'9,~.H Ku6era and W. N. Francis.
1967.
Computa-tional Analysis of Present-day American En-glish.
Brown University Press, Providence,RI.Beth Levin.
1993.
English Verb Classes andAlternations: A Preliminary Investigation.University of Chicago Press, Chicago, Illinois.George A. Miller, Richard Beckwith, ChristianeFellbaum, Derek Gross, and Katherine J.Miller.
1990.
Introduction to WordNet: Anon-line lexical database.
International Jour-nal of Lexicography (special issue), 3(4):235-312.George A. Miller, Claudia Leacock, RandeeTengi, and Ross T. Bunker.
1993.
A semanticconcordance.
Cognitive Science Laboratory,Princeton University.Jacques Robin.
1994.
Revision-Based Gener-ation of Natural Language Summaries Pro-riding Historical Background: Corpus-BasedAnalysis, Design, Implementation, and Eval-uation.
Ph.D. thesis, Department of Com-puter Science, Columbia University.
AlsoTechnical Report CU-CS-034-94.613
