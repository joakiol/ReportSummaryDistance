A Simple String-Rewriting Formalism for Dependency GrammarAlexis NASRLattice, UFRLUniversite?
Paris 7F-75005 ParisFrancealexis.nasr@linguist.jussieu.frOwen RAMBOWColumbia UniversityDepartment of Computer Science1214 Amsterdam AvenueNew York, NY 10027-7003, USArambow@cs.columbia.eduAbstractRecently, dependency grammar has gained renewedattention as empirical methods in parsing haveemphasized the importance of relations betweenwords, which is what dependency grammars modelexplicitly, but context-free phrase-structure gram-mars do not.
While there has been much work onformalizing dependency grammar and on parsingalgorithms for dependency grammars in the past,there is not a complete generative formalization ofdependency grammar based on string-rewriting inwhich the derivation structure is the desired depen-dency structure.
Such a system allows for the defi-nition of a compact parse forest in a straightforwardmanner.
In this paper, we present a simple gen-erative formalism for dependency grammars basedon Extended Context-Free Grammar, along witha parser; the formalism captures the intuitions ofprevious formalizations while deviating minimallyfrom the much-used Context-Free Grammar.1 IntroductionDependency grammar has a long tradition in syn-tactic theory, dating back to at least Tesnie`re?s workfrom the thirties.
Recently, it has gained renewedattention as empirical methods in parsing have em-phasized the importance of relations between words(see, e.g., (Collins, 1997)), which is what depen-dency grammars model explicitly, but context-freephrase-structure grammars do not.
In this paper, weaddress an important issue in using grammar for-malisms: the compact representation of the parseforest.
Why is this an important issue?
It is wellknown that for non-toy grammars and non-toy ex-amples, a sentence can have a staggeringly largenumber of analyses (for example, using a context-free grammar (CFG) extracted from the Penn Tree-bank, a sentence of 25 words may easily have1,000,000 or more analyses).
By way of an exam-ple of an ambiguous sentence (though with only tworeadings), the two dependency representations forthe ambiguous sentence (1) are given in Figure 1.
(1) Pilar saw a man with a telescopeIt is clear that if we want to evaluate each possi-ble analysis (be it using a probabilistic model or adifferent method, for example a semantic checker),we cannot efficiently do so if we enumerate allcases.1 We have two options: we can either use agreedy heuristic method for checking which doesnot examine all possible solutions, which entailswe may miss the optimal solution, or we performour checking operation on a representation whichencodes all options in a compact representation.This is possible because the exponential number ofpossible analyses (exponential in the length of theinput sentence) share subanalyses, thus making apolynomial-size representation possible.
This rep-resentation is called the shared parse forest and ithas been extensively studied for CFGs (see, for ex-ample, (Lang, 1991)).
To our knowledge, there hasbeen no description of the notion of shared parseforest for dependency trees to date.
In this paper, wepropose a formalization which is very closely basedon the shared parse forest for CFG.
We achieve thisby defining a generative string-rewriting formal-ism whose derivation trees are dependency trees.The formalism, and the corresponding shared parseforests, are used in a probabilistic chart parser fordependency grammar, which is described in (Nasrand Rambow, 2004b).While there has been much work on formalizingdependency grammar and on parsing algorithms fordependency grammars in the past, we observe thatthere is not, to our knowledge, a complete gener-ative formalization2 of dependency grammar basedon string-rewriting in which the derivation structureis exactly the desired dependency structure.
Themost salient reason for the lack of such a gener-ative dependency grammar is the absence of non-1We would like to thank two anonymous reviewers for use-ful comments.2While the notions are related historically and conceptually,we refer to a type of mathematical formalization, not to theschool of linguistics known as ?Generative Grammar?.terminal symbols in a dependency tree, which pre-vents us from interpreting it as a derivation struc-ture in a system that distinguishes between termi-nal and nonterminal symbols.
The standard solu-tion to this problem, proposed by Gaifman (1965),is to introduce nonterminal symbols denoting lex-ical categories, as depicted in figure 2 (called the?labelled phrase-structure trees induced by a depen-dency tree?
by Gaifman (1965)).
Clearly, the ?pure?dependency tree can be derived in a straightforwardmanner.
The string rewriting system described in(Gaifman, 1965) generates as derivation structuresthis kind of trees.There is however a deeper problem when con-sidering dependency trees as derivation structures,following from the fact that in a dependency tree,modifiers3 are direct dependents of the head theymodify, and (in certain syntactic contexts) the num-ber of modifiers is unbounded.
Thus, if we wish toobtain a tree as shown in Figure 2, we need to haveproductions whose right-hand side is of unboundedsize, which is not possible in a context-free gram-mar.
Indeed, the formalization of dependency gram-mar proposed by Gaifman (1965) is unsatisfactoryin that it does not allow for an unbounded numberof modifiers!In this paper, we follow a suggestion made byAbney (1996) and worked out in some detail in(Lombardo, 1996)4 to extend Gaifman?s notationwith regular expressions, similar to the approachused in extended context-free grammars.
The re-sult is a simple generative formalism which hasthe property that the derivation structures are de-pendency trees, except for the introduction of pre-terminal nodes as shown in Figure 2.
We do notmean to imply that our formalism is substantiallydifferent from previous formalizations of depen-dency grammar; the goal of this paper is to presenta clean and easy-to-use generative formalism witha straightforward notion of parse forest.
In partic-ular, our formalism, Generative Dependency Gram-mar, allows for an unbounded number of daughternodes in the derivation tree through the use of reg-ular expressions in its rules.
The parser uses the3We use the term modifier in its linguistic sense as a type ofsyntactic dependency (another type being argument).
We usehead (or mother) and dependent (or daughter) to refer to nodesin a tree.
Sometimes, in the formal and parsing literature, mod-ifier is used to designate any dependent node, but we considerthat usage confusing because of the related but different mean-ing of the term modifier that is well-established in the linguisticliterature.4In fact, much of our formalism is very similar to (Lom-bardo, 1996), who however does not discuss parsing (onlyrecognition), nor the representation of the parse forest.corresponding finite-state machines which straight-forwardly allows for a binary-branching representa-tion of the derivation structure for the purpose ofparsing, and thus for a compact (polynomial andnot exponential) representation of the parse forest.This formalism is based on previous work presentedin (Kahane et al, 1998), which has been substan-tially reformulated in order to simplify it.5 In par-ticular, we do not address non-projectivity here, butacknowledge that for certain languages it is a cru-cial issue.
We will extend our basic approach in thespirit of (Kahane et al, 1998) in future work.The paper is structured as follows.
We startout by surveying previous formalizations of depen-dency grammar in Section 2.
In Section 3, we intro-duce several formalisms, including Generative De-pendency Grammar.
We present a parsing algorithmin Section 4, and mention empirical results in Sec-tion 5.
We then conclude.2 Previous Formalizations of DependencyGrammarWe start out by observing that ?dependency gram-mar?
should be contrasted with ?phrase structuregrammar?, not ?CFG?, which is a particular formal-ization of phrase structure grammar.
Thus, just as itonly makes sense to study the formal properties ofa particular formalization of phrase structure gram-mar, the question about the formal properties of de-pendency grammar in general is not well defined,nor the question of a comparison of a dependencyformalism with dependency grammar.There have been (at least) four types of formal-izations of dependency grammars in the past.6 Noneof these approaches, to our knowledge, discuss thenotion of shared parse forest.
The first approach(for example, (Lombardo and Lesmo, 2000)) fol-lows Gaifman (1965) in proposing traditional stringrewriting rules, which however do not allow for anunbounded number of adjuncts.In the second approach, the dependency structureis constructed in reference to a parallel (?deeper?
)structure (Sgall et al, 1986; Mel?c?uk, 1988).
Be-cause the rules make reference to other struc-5Kahane et al (1998) present three different types of rules,for subcategorization, modification, and linear precedence.
Inthe formalism presented in this paper, they have been collapsedinto one.6We leave aside here work on tree rewriting systems suchas Tree Adjoining Grammar, which, when lexicalized, havederivation structures which are very similar to dependencytrees.
See (Rambow and Joshi, 1997) for a discussion relatedto TAG, and see (Rambow et al, 2001) for the definition of atree-rewriting system which can be used to develop grammarswhose derivations faithfully mirror syntactic dependency.saw HHPilar man HHa withtelescopeasawHHHHPilar manawithtelescopeaFigure 1: Two dependency treesPilarNaD manaVVsawD manasawNPwithPilarNNaD telesopeN Pwith NtelesopeDFigure 2: Two dependency trees with lexical categoriestures, these approaches cannot be formalized ina straightforward manner as context-free rewritingformalisms.In the third approach, which includes formaliza-tions of dependency structure such as DependencyTree Grammar of Modina (see (Dikovsky and Mod-ina, 2000) for an overview), Link Grammar (Sleatorand Temperley, 1993) or the tree-composition ap-proach of Nasr (1996), rules construct the depen-dency tree incrementally; in these approaches, thegrammar licenses dependency relations which, in aderivation, are added to the tree one by one, or ingroups.
In contrast, we are interested in a string-rewriting system; in such a system, we cannot adddependency relations incrementally: all daughtersof a node must be added at once to represent a sin-gle rewrite step.In the fourth approach, the dependency grammaris converted into a headed context-free grammar(Abney, 1996; Holan et al, 1998), also the BasicDependency Grammar of Beletskij (1967) as citedin (Dikovsky and Modina, 2000).
This approach al-lows for the recovery of the dependency structureboth from the derivation tree and from a parse for-est represented in polynomial space.
(In fact, ourparsing algorithm draws on this work.)
However,the approach of course requires the introduction ofadditional nonterminal nodes.
Finally, we observethat Recursive Transition Networks (Woods, 1970)can be used to encode a grammar whose deriva-tion trees are dependency trees.
However, they aremore a general framework for encoding grammarsthan a specific type of grammar (for example, wecan also use them to encode CFGs).
In a some-what related manner, Alshawi et al (2000) use cas-caded head automata to derive dependency trees, butleave the nature of the cascading under-formalized.Eisner (2000) provides a formalization of a systemthat uses two different automata to generate left andright children of a head.
His formalism is very closeto the one we present, but it is not a string-rewritingformalism (and not really generative at all).
Weare looking for a precise formulation of a genera-tive dependency grammar, and the question has re-mained open whether there is an alternate formal-ism which allows for an unbounded number of ad-juncts, introduces all daughter nodes at once in astring-rewriting step, and avoids the introduction ofadditional nonterminal nodes.3 FormalismIn this section we first review the definition of Ex-tended Context-Free Grammar and then show howwe use it to model dependency derivations.
An Ex-tended Context-Free Grammar (or ECFG for short)is like a context-free grammar (CFG), except thatthe right-hand side is a regular expression over theterminal and nonterminal symbols of the grammar.At each step in a derivation, we first choose a rewriterule (as we do in CFG), and then we choose a stringwhich is in the language denoted by the regular ex-pression associated with the rule.
This string is thentreated like the right-hand side of a CFG rewriterule.In the following, if G is a grammar and R a regu-lar expression, then L(G) denotes the language gen-erated by the grammar and L(R) the language de-noted by the regular expression.
If F is a class ofgrammars (such as CFG), then L(F) denote the classof languages generated by the grammars in F. Wenow give a formal definition, which closely followsthat given by Albert et al (1999).7A Extended Context-Free Grammar is a 4-tuple (VN, VT, P, S), where:?
VN is a finite set of nonterminal symbols,?
VT is a finite set of terminal symbols (disjointfrom VN),?
P is a finite set of rules, which are orderedpairs consisting of an element of VN and a reg-ular expression over VN ?
VT,?
S, a subset of VN, contains the possible startsymbols.We will use the traditional arrow notation (??
)to write rules.For A ?
VN and u, v ?
(VN ?
VT)?
we say thatuAv yields uwv (written uAv =?
uwv) if A ?
?R is in P and w is in L(R).
The transitive closureof the yield relation (denoted ?=? )
is defined in theusual manner.The language generated by a Extended Context-Free Grammar is the set {w ?
V ?T | A?=?
w,A ?S}.We now define a restricted version of ECFGwhich we will use for defining dependency gram-mars.
The only new formal requirement is thatthe rules be lexicalized in the sense of (Joshi andSchabes, 1991).
For our formalism, this meansthat the regular expression in a production is suchthat each string in its denoted language contains atleast one terminal symbol.
Linguistically speaking,this means that each rule is associated with exactly7ECFG has been around informally since the sixties (e.g.,the Backus-Naur form); for a slightly different formalization,see (Madsen and Kristensen, 1976), whose definition allowsfor an infinite rule set.one lexical item (which may be multi-word).
Wewill call this particular type of Extended Context-Free Grammar a lexicalized Extended Context-Free Grammar or, for obvious reasons, a Genera-tive Dependency Grammar (GDG for short).
Whenwe use a GDG for linguistic description, its left-hand side nonterminal will be interpreted as the lex-ical category of the lexical item and will representits maximal projection.8A Generative Dependency Grammar is a lexi-calized ECFG.It is sometimes useful to have dependency repre-sentations with labeled arcs (typically labeled withsyntactic functions such as SUBJ for subject or ADJfor adjunct).
There are different ways of achievingthis goal; here, we discuss the use of feature struc-tures in conjunction with the nonterminal symbols,for example N[gf:subj] instead of just N. Fea-ture structures are of course useful for other reasonsas well, such as expressing morphological features.In terms of our formalism, the use of bounded fea-ture structures can be seen as a shorthand notationfor an increased set of nonterminal symbols.
Theuse of feature structures (rather than simple nonter-minal symbols) allows for a more perspicuous rep-resentation of linguistic relations through the use ofunderspecification.
Note that the use of underspec-ified feature structures in rules can potentially leadto an exponential increase (exponential in the sizeof the set of feature values) of the size of the set ofrules if rules contain underspecified feature struc-tures on the right-hand side.
However, we note thatthe feature representing, grammatical function willpresumably always be fully specified on the right-hand side of a rule (the head determines the func-tion of its dependents).
Underspecification in theleft-hand side of a rule only leads to linear compact-ification of the rule set.We now give a toy linguistic example.
We letGLing be (VN, VT, P, S) as follows:?
VN = {V,N,D,A,Adv}?
VT = { Pilar, saw, man, a, telescope, with, tall,very }?
P consists of the following rules:p1 : V ??
N saw N P ?p2 : N ??
(Pilar | D (A) (man | telescope) P ?
)8For practical purposes, we can separate the lexicon (whichassigns lexical categories to lexemes) from the syntactic rules(which hold for all lexemes of a class), as does Gaifman (1965),resulting in a straightforward notational extension to our for-malism.Vp1=?
N saw N Pp2p2p3=?
Pilar saw D man with Np6p2=?
Pilar saw a man with D telescopep6=?
Pilar saw a man with a telescopeFigure 3: A sample GDG derivationp3 : P ??
with Np4 : A ??
Adv?
tallp5 : Adv ??
veryp6 : D ??
a?
S = {V }A derivation is shown in Figure 3; the corre-sponding derivation tree is shown in the right partof Figure 2.
As can be seen, the derivation structureis a dependency tree, except for the use of pretermi-nals, as we desired.The first part of the following theorem followsfrom the existence of a Greibach Normal Form forECFG (Albert et al, 1999).
The second part followsimmediately from the closure of CFG under regularsubstitution.L(GDG) = L(ECFG) = L(CFG).Of course, ECFG, GDG and CFG are not stronglyequivalent in the standard sense for string rewrit-ing systems of having the same sets of derivationtrees.
Clearly, ECFG can generate all sets of deriva-tion trees that GDG can, while CFG cannot (becauseof the unbounded branching factor of ECFG and ofGDG); ECFG can also generate all sets of deriva-tion trees that CFG can, while GDG cannot (becauseof the lexicalization requirement).
ECFG thus hasa greater strong generative capacity than CFG andGDG, while those of GDG and CFG are incompa-rable.It is interesting to notice the difference betweenthe rewriting operation of a nonterminal symbol asdefined for a ECFG or a GDG and the equivalentrewriting steps with a weakly equivalent CFG.
AGDG rewriting operation of a symbol X using arule r is decomposed in two stages, the first stageconsists in choosing a string w which belongs to theset denoted by the right-hand side of r. During thesecond stage, X is replaced by w. These two stagesare of a different nature, the first concerns the gener-ation of CFG rules (and hence a CFG) using a GDGwhile the second concerns the generation of a stringusing the generated CFG.
The equivalent rewritingoperation (X ?
w) with a CFG does not distin-guish the same two stages, both the selection of wand the rewriting of X as w are done at the sametime.man(m2,4)(m2,3)(m3,3)(m3,2) (m2,4)(m2,3) telescopesaw(m2,4)(m2,4)Pilar a(D,2)witha(D,2)(m1,4) (m1,4)(m1,4)(m1,3)(m1,2)Figure 4: A packed parse forest4 Parsing AlgorithmThe parsing algorithm given here is a simple exten-sion of the CKY algorithm.
The difference is in theuse of finite state machines in the items in the chartto represent the right-hand sides of the rules of theECFG.9 A rule with category C as its left-hand sidewill give rise to a finite state machine which we calla C-rule FSM; its final states mark the completedrecognition of a constituent of label C .CKY-Style parsing algorithm for ExtendedContext-Free Grammars.Input.
A ECFG G and an input string W =w1 ?
?
?wn.Output.
The parse table T for W such that ti,jcontains (M, q) iff M is a C-rule-FSM, q is oneof the final states of M , and we have a derivationC +=?wi ?
?
?wj .
If i = j, ti,j also contains the in-put symbol wi.Method.?
Initialization: For each i, 1 ?
i ?
n, add wito ti,i.?
Completion: If ti,j contains either the inputsymbol w or an item (M, q) such that q isa final state of M , and M is a C-rule-FSM,then add to ti,j all (M ?, q?)
such that M ?
is arule-FSM which transitions from a start stateto state q?
on input w or C .
Add a single back-pointer from (M ?, q?)
in ti,j to (M, q) or w inti,j .9Recent work in the context of using ECFG for pars-ing SGML and XML proposes an LL-type parser for ECFG(Bru?ggemann-Klein and Wood, 2003); their approach also ex-ploits the automaton representation of the right-hand side ofrules, as is natural for an algorithm dealing with ECFG.1 2 3N saw N4 Pm11 3 4PilarA manD2 Ptelescopetelescopemanm2with N2 31m3Figure 5: Three rules FSM m1, m2 and m3.
m1 is a V-rule-FSM corresponding to rule p1, m2 is anN-rule-FSM which corresponds to rule p2 and m3 is a P-rule-FSM which corresponds to rule p3P(3)P(2) N(4)N(3) telescopewithaD(2)V(5)V(5)V(4)sawV(3)N(4)PilarmanN(4)N(3)aD(2)V(5)P(3)P(2) N(4)N(3) telescopewithaD(2)N(4)manN(4)N(3)aD(2)V(4)sawV(3)N(4)PilarFigure 6: A parse forest?
Scanning: If (M1, q1) is in ti,k, and tk+1,jcontains either the input symbol w or the item(M2, q2) where q2 is a final state and M2 is aC-rule-FSM, then add (M1, q) to ti,j (if not al-ready present) if M1 transitions from q1 to q oneither w or C .
Add a double backpointer from(M1, q) in ti,j to (M1, q1) in ti,k (left back-pointer) and to either w or (M2, q2) in tk+1,j(right backpointer).At the end of the parsing process, a packed parseforest has been built.
The packed forest correspond-ing to the parse of sentence Pilar saw a man with atelescope, using the grammar of Section 3 is repre-sented in Figure 4.
The nonterminal nodes are la-beled with pairs (M, q) where M is an rule-FSMand q a state of this FSM.
Three rule-FSMs corre-sponding to rules p1, p2 and p3 have been repre-sented in Figure 5.Obtaining the dependency trees from the packedparse forest is performed in two stages.
In a firststage, a forest of binary syntagmatic trees is ob-tained from the packed forest and in a second stage,each syntagmatic tree is transformed into a depen-dency tree.
We shall not give the details of theseprocesses.
The two trees resulting from de-packingof Figure 4 are represented in Figure 6.
The dif-ferent nodes of the syntagmatic tree that will begrouped in a single node of the dependency treeshave been circled.5 Empirical ResultsWhile the presentation of empirical results is not theobject of this paper, we give an overview of someempirical work using ECFG for natural languageprocessing in this section.
For full details, we referto (Nasr and Rambow, 2004a; Nasr and Rambow,2004b; Nasr, 2004).The parser presented in Section 4 above has beenimplemented.
We have investigated the use theparser in a two-step probabilistic framework.
In afirst step, we determine which rules of the ECFGshould be used for each word in the input sentence.
(Recall that a grammar rule encodes the active andpassive valency, as well as how any arguments arerealized, for example, fronted or in canonical posi-tion.)
This step is called supertagging and has beensuggested and studied in the context of Tree Adjoin-ing Grammar by Bangalore and Joshi (1999).
Ina second step, we use a probabilistic ECFG wherethe probabilities are non-lexical and are based en-tirely on the grammar rules.
We extract the mostprobable derivation from the compact parse forestusing dynamic programming in the usual manner.This non-lexical probability model is used becausethe supertagging step already takes the words in thesentence into account.
The probabilities can be en-coded directly as weights on the transitions in therule-FSMs used by the parser.The ECFG grammar we use has been automati-cally extracted from the Penn Treebank (PTB).
Infact, we first extract a Tree Insertion Grammar fol-lowing the work of (Xia et al, 2000; Chen, 2001;Chiang, 2000), and then directly convert the treesof the obtained TAG into automata for the parser.It is clear that one could also derive an explicitECFG in the same manner.
The extracted gram-mar has about 4.800 rules.
The probabilities areestimated from the corpus during extraction.
Notethat there are many different ways of extracting anECFG from the PTB, corresponding to different the-ories of syntactic dependency.
We have chosen todirectly model predicate-argument relations ratherthan more surface-oriented syntactic relations suchas agreement, so that all function words (determin-ers, auxiliaries, and so on) depend on the lexicalword.
Strongly governed prepositions are treated aspart of a lexeme rather than as full prepositions.We have investigated several different ways ofmodeling the probability of attaching a sequence ofmodifiers at a certain point in the derivation (con-ditioning on the position of the modifier in the se-quence or conditioning on the previous modifierused).
We found that using position or context im-proves on using neither.We have performed two types of experiments: us-ing the correct ECFG rule for each word, and as-signing ECFG rules automatically using supertag-ging.
In the case of using the correct supertag, weobtain unlabeled dependency accuracies of about98% (i.e., in about 2% of cases a word is assigneda wrong governor).
Automatic supertagging (us-ing standard n-gram tagging methods) for a gram-mar our size has an accuracy of about 80%.
Thisis also approximately the dependency accuracy ob-tained when parsing the output of a supertagger.
Weconclude from this performance that if we can in-crease the performance of the supertagger, we canalso directly increase the performance of the parser.Current work includes examining which grammati-cal distinctions the grammar should make in order tooptimize both supertagging and parsing (Toussenel,2004).6 ConclusionWe have presented a generative string-rewritingsystem, Extended Context-Free Grammar, whosederivation trees are dependency trees with un-bounded branching factor.
We have shown how wecan reuse the representation of shared parse forestswell-known from CFGs for Extended Context-FreeGrammar.
The question arises whether we can rep-resent the shared parse forest in a manner more di-rectly in the spirit of dependency.
This question wasinvestigated by (Nasr, 2003).
He shows that the fac-toring realized in the shared forest presented hereand which is the key to the polynomial represen-tation of a potentially exponential set, can be donedirectly on the dependency trees by introducing thenotion of dependency sets.ReferencesAbney, Steven (1996).
A grammar of projections.Unpublished manuscript, Universita?t Tu?bingen.Albert, Ju?rgen; Giammarresi, Dora; and Wood, Der-ick (1999).
Extended context-free grammars andnormal form algorithms.
In Champarnaud, Jean-Marc; Maurel, Denis; and Ziadi, Djelloul, ed-itors, Automata Implementations: Third Inter-national Workshop on Implementing Automata(WIA?98), volume 1660 of LNCS, pages 1?12.Springer Verlag.Alshawi, Hiyan; Bangalore, Srinivas; and Douglas,Shona (2000).
Learning dependency translationmodels as collections of finite-state head trans-ducers.
cl, 26(1):45?60.Bangalore, Srinivas and Joshi, Aravind (1999).Supertagging: An approach to almost parsing.Computational Linguistics, 25(2):237?266.Bru?ggemann-Klein, Anne and Wood, Derick(2003).
The parsing of extended context-freegrammars.
Unpublished manuscript, TechnischeUniversita?t Mu?nchen and Hong Kong Universityof Science & Technology.Chen, John (2001).
Towards Efficient StatisticalParsing Using Lexicalized Grammatical Infor-mation.
PhD thesis, University of Delaware.Chiang, David (2000).
Statistical parsing with anautomatically-extracted tree adjoining grammar.In 38th Meeting of the Association for Compu-tational Linguistics (ACL?00), pages 456?463,Hong Kong, China.Collins, Michael (1997).
Three generative, lexi-calised models for statistical parsing.
In Pro-ceedings of the 35th Annual Meeting of the As-sociation for Computational Linguistics, Madrid,Spain.Dikovsky, Alexander and Modina, Larissa (2000).Dependencies on the other side of the curtain.Traitement Automatique des Langues, 41(1):79?111.Eisner, Jason (2000).
Bilexical grammars and theircubic-time parsing algorithms.
In Bunt, Harry C.and Nijholt, Anton, editors, New Developmentsin Natural Language Parsing.
Kluwer AcademicPublishers.Gaifman, Haim (1965).
Dependency systems andphrase-structure systems.
Information and Con-trol, 8:304?337.Holan, Toma?s?
; Kubon?, Vladislav; Oliva, Karel; andPla?tek, Martin (1998).
Two useful measures ofword order complexity.
In Kahane, Sylvain andPolgue`re, Alain, editors, Processing of Depen-dency Grammars: Proceeding of the Workshop,pages 21?28, Montre?al, Canada.
ACL/COLING.Joshi, Aravind K. and Schabes, Yves (1991).
Tree-adjoining grammars and lexicalized grammars.In Nivat, Maurice and Podelski, Andreas, editors,Definability and Recognizability of Sets of Trees.Elsevier.Kahane, Sylvain; Nasr, Alexis; and Rambow, Owen(1998).
Pseudo-projectivity: A polynomiallyparsable non-projective dependency grammar.
In36th Meeting of the Association for Computa-tional Linguistics and 17th International Con-ference on Computational Linguistics (COLING-ACL?98), pages 646?652, Montre?al, Canada.Lang, Bernard (1991).
Towards a uniform formalframework for parsing.
In Tomita, M., editor,Current Issues in Parsing technology, chapter 11,pages 153?171.
Kluwer Academic Publishers.Lombardo, Vincenzo (1996).
An Earley-styleparser for dependency grammars.
In Proceedingsof the 16th International Conference on Compu-tational Linguistics (COLING?96), Copenhagen.Lombardo, Vincenzo and Lesmo, Leonardo (2000).A formal theory of dependency syntax withempty units.
Traitement automatque des langues,41(1):179?209.Madsen, O.L.
and Kristensen, B.B.
(1976).
LR-parsing of extended context-free grammars.
ActaInformatica, 7:61?73.Mel?c?uk, Igor A.
(1988).
Dependency Syntax: The-ory and Practice.
State University of New YorkPress, New York.Nasr, Alexis (1996).
Un syste`me de reformula-tion automatique de phrases fonde?
sur la The?orieSens-Texte : application aux langues contro?le?es.PhD thesis, Universite?
Paris 7.Nasr, Alexis (2003).
Factoring sufrace syntac-tic structures.
In First International Conferenceon Meaning-Text Theory, pages 249?258, Paris,France.Nasr, Alexis (2004).
Grammaires de de?pendancesge?ne?ratives: expe?riences sur le Corpus Paris 7.Unpublished manuscript, Universite?
Paris 7.Nasr, Alexis and Rambow, Owen (2004a).
Depen-dency parsing based on n-best-path supertagging.Unpublished manuscript, Universite?
Paris 7 andCOlumbia University.Nasr, Alexis and Rambow, Owen (2004b).
Su-pertagging and full parsing.
In Proceedings ofthe Workshop on Tree Adjoining Grammar andRelated Formalisms (TAG+7), Vancouver, BC,Canada.Rambow, Owen and Joshi, Aravind (1997).
A for-mal look at dependency grammars and phrase-structure grammars, with special considerationof word-order phenomena.
In Wanner, Leo,editor, Recent Trends in Meaning-Text Theory,pages 167?190.
John Benjamins, Amsterdam andPhiladelphia.Rambow, Owen; Vijay-Shanker, K.; and Weir,David (2001).
D-Tree Substitution Grammars.Computational Linguistics, 27(1).Sgall, P.; Hajic?ova?, E.; and Panevova?, J.
(1986).The meaning of the sentence and its semantic andpragmatic aspects.
Reidel, Dordrecht.Sleator, Daniel and Temperley, Davy (1993).
Pars-ing english with a link grammar,.
In Proceedingsof the Third International Workshop on ParsingTechnologies IIWPT?93).Toussenel, Franc?ois (2004).
Why supertagging ishard.
In Proceedings of the Workshop on TreeAdjoining Grammar and Related Formalisms(TAG+7), Vancouver, BC, Canada.Woods, William A.
(1970).
Transition networkgrammars for natural language analysis.
Com-mun.
ACM, 3(10):591?606.Xia, Fei; Palmer, Martha; and Joshi, Aravind(2000).
A uniform method of grammar extrac-tion and its applications.
In Proc.
of the EMNLP2000, Hong Kong.
