Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1191?1200,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsAn Empirical Study Of Semi-Supervised Chinese Word SegmentationUsing Co-TrainingFan YangNuance Communications, Inc.fan.yang@nuance.comPaul VozilaNuance Communications, Inc.paul.vozila@nuance.comAbstractIn this paper we report an empirical studyon semi-supervised Chinese word segmenta-tion using co-training.
We utilize two seg-menters: 1) a word-based segmenter lever-aging a word-level language model, and 2)a character-based segmenter using character-level features within a CRF-based sequencelabeler.
These two segmenters are initiallytrained with a small amount of segmenteddata, and then iteratively improve each otherusing the large amount of unlabelled data.Our experimental results show that co-trainingcaptures 20% and 31% of the performanceimprovement achieved by supervised trainingwith an order of magnitude more data for theSIGHAN Bakeoff 2005 PKU and CU corporarespectively.1 IntroductionIn the literature there exist two general models forsupervised Chinese word segmentation, the word-based approach and the character-based approach.The word-based approach searches for all possiblesegmentations, usually created using a dictionary,for the optimal one that maximizes a certain util-ity.
The character-based approach treats segmenta-tion as a character sequence labeling problem, indi-cating whether a character is located at the bound-ary of a word.
Typically the word-based approachuses word level features, such as word n-grams andword length; while the character-based approachuses character level information, such as character n-grams.
Both approaches have their own advantagesand disadvantages, and there has been some researchin combining the two approaches to improve the per-formance of supervised word segmentation.In this research we are trying to take advantage ofthe word-based and the character-based approachesin the semi-supervised setting for Chinese word seg-mentation, where there is only a limited amountof human-segmented data available, but there ex-ists a relatively large amount of in-domain unseg-mented data.
The goal is to make use of the in-domain unsegmented data to improve the ultimateperformance of word segmentation.
According toSun et al(2009), ?the two approaches [word-basedand character-based approaches] are either based ona particular view of segmentation.?
This naturallymotivates the use of co-training, which utilizes twomodels trained on different views of the input la-beled data which then iteratively educate each otherwith the unlabelled data.
At the end of the co-training iterations, the initially weak models achieveimproved performance.
Co-training has been suc-cessfully applied in many natural language process-ing tasks.
In this paper we describe an empiri-cal study of applying co-training to semi-supervisedChinese word segmentation.
Our experimental re-sults show that co-training captures 20% and 31%of the performance improvement achieved by super-vised training with an order of magnitude more datafor the SIGHANBakeoff 2005 PKU and CU corporarespectively.In section 2 we review the two supervised ap-proaches and co-training algorithm in more detail.In section 3 we describe our implementation of theco-training word segmentation.
In section 4 we de-1191Figure 1: A search space for word segmenterscribe our co-training experiments.
In section 5 weconclude the paper.2 Related WorkIn this section, we first review the related research onthe word-based and the character-based approachesfor Chinese word segmentation, and comparativelyanalyze these two supervised approaches.
We thenreview the related research on co-training.2.1 Supervised Word Segmentation2.1.1 Word-Based SegmenterGiven a character sequence c1c2...cn, the word-based approach searches in all possible segmenta-tions for one that maximizes a pre-defined utilityfunction, formally represented as in Equation 1.
Thesearch space, GEN(c1c2...cn), can be representedas a lattice, where each vertex represents a charac-ter boundary index and each arc represents a wordcandidate which is the sequence of characters withinthe index range.
A dictionary1 can be used to gener-ate such a lattice.
For example, given the charactersequence ???????
and a dictionary that con-tains the words {????????}
and all singleChinese characters, the search space is illustrated inFigure 1.W?
= arg maxW?GEN(c1c2...cn)Util(W ) (1)Dynamic programming such as Viterbi decodingis usually used to search for the optimized segmen-tation.
The utility can be as simple as the negationof number of words (i.e.
Util(W ) = ?
| W |),1A dictionary is not a must to create the search space butit could shrink the search space and also lead to improved seg-mentation performance.which gives a reasonable performance if the dictio-nary used for generating the search space has a goodcoverage.
Alternatively one can search for the seg-mentation that maximizes the word sequence prob-ability P (W ) (i.e.
Util(W ) = P (W )).
With aMarkov assumption, P (W ) can be calculated usinga language model as in Equation 2.P (W ) = P (w1w2...wm)= P (w1).P (w2|w1)...P (wn|w1w2...wn1)= P (w1)P (w2|w1)...P (wn|wn?1)(2)More generally, the utility can be formulated asa semi-Markov linear model, defined as Equation 3,in which ?
is the feature function vector, and ?
isthe parameter vector that can be learned from train-ing data using different techniques: Liang (2005),Gao et al(2005), and Zhang and Clark (2007) useaveraged perceptron; Nakagawa (2004) uses generaliterative scaling; Andrew (2006) uses semi-MarkovCRF; and Sun (2010) uses a passive-aggressivelearning algorithm.Util(W ) = ?T?
(c1c2...cn,W ) (3)2.1.2 Character-Based SegmenterThe character-based approach treats word seg-mentation as a character sequence labeling problem,to label each character with its location in a word,first proposed by Xue (2003).2 The basic label-ing scheme is to use two tags: ?B?
for the begin-ning character of a word and ?O?
for other charac-ters (Peng et al 2004).
Xue (2003) use a four-tagscheme based on some linguistic intuitions: ?B?
forthe beginning character, ?I?
for the internal charac-ters, ?E?
for the ending character, and ?S?
for single-character word.
For example, the word sequence ????
?
???
can be labelled as ?\B ?\I ?\E?\S ?\B ?\E.
Zhao et al(2010) further extendthis scheme by using six tags.Training and decoding of the character labelingproblem is similar to part-of-speech tagging, which2Teahan et al(2000) use a character language model to de-termine whether a word boundary should be inserted after eachcharacter, which can also be considered as a character-basedapproach as well.1192is also generally formulated as a linear model.
Manymachine learning techniques have been explored:Xue (2003) use a maximum entropy model; Peng etal.
(2004) use linear-chain CRF; Liang (2005) usesaveraged perceptron; Sun et al(2009) use a discrim-inative latent variable approach.2.1.3 Comparison and CombinationIt is more natural to use word-level informa-tion, such as word n-grams and word length, in aword-based segmenter; while it is more natural touse character-level information, such as character n-grams, in a character-based segmenter.
Sun (2010)gives a detailed comparison of the two approachesfrom both the theoretical and empirical perspec-tives.
Word-level information has greater represen-tational power in terms of contextual dependency,while character-level information is better at mor-phological analysis in terms of word internal struc-tures.On one hand, features in a character-based modelare usually defined in the neighboring n-characterwindow; and an order-K CRF can only look at thelabels of the previous K characters.
Given that manywords contain more than one character, a word-based model can examine a wider context.
Thusthe contextual dependency information encoded ina character-based model is generally weaker than ina word-based model.
Andrew (2006) also showsthat semi-Markov CRF makes strictly weaker in-dependence assumptions than linear CRF and soa word-based segmenter using an order-K semi-Markov model is more expressive than a character-based model using an order-K CRF.On the other hand, Chinese words have internalstructures.
Chinese characters can serve some mor-phological functions in a word.
For example, thecharacter?
usually works as a suffix to signal plu-ral; the character ?
can also be a suffix meaning agroup of people; and ?
generally works as a pre-fix before a person?s nickname that has one charac-ter.
Such morphological information is extremelyuseful for identifying unknown words.
For exam-ple, a character-based model can learn that ?
isusually tagged as ?B?
and the next character is usu-ally tagged as ?E?.
Thus even when ??
is not anexisting word in the training data, a character-basedmodel might still be able to correctly label it as?\B?\E.Recent advanced Chinese word segmenters, eitherword-based or character-based, have been trying tomake use of both word-level and character-level in-formation.
For example, Nakagawa (2004) inte-grates the search space of a character-based modelinto a word-based model; Andrew (2006) convertsCRF-type features into semi-CRF features in hissemi-Markov CRF segmenter; Sun et al(2009) addword identify information into their character-basedmodel; and Sun (2010) combine the two approachesat the system level using bootstrap aggregating.2.2 Co-TrainingThe co-training approach was first introduced byBlum and Mitchell (1998).
Theoretical analysis ofits effectiveness is given in (Blum and Mitchell,1998; Dasgupta et al 2001; Abney, 2002).
Co-training works by partitioning the feature set intotwo conditionally independent views (given the trueoutput).
On each view a statistical model can betrained.
The presence of multiple distinct views ofthe data can be used to train separate models, andthen each model?s predictions on the unlabeled dataare used to augment the training set of the othermodel.Figure 2 depicts a general co-training framework.The inputs are two sets of data, a labelled set S andan unlabelled set U.
Generally S is small and U islarge.
Two statistical models M1 and M2 are used,which are built on two sets of data L1 and L2 initial-ized as S but then incrementally increased in eachiteration.
C is a cache holding a small subset of Uto be labelled by both models (Blum and Mitchell,1998; Abney, 2002).
In some applications, C isnot used and both models label the whole set of U(i.e.
C==U) (Collins and Singer, 1999; Nigam andGhani, 2000; Pierce and Cardie, 2001).
The stop-ping criteria can be, for example, when U is empty,or when a certain number of iterations are executed.In step 5 and 6 during each iteration, some datalabelled by M1 are selected and added to the train-ing set L2, and vice versa.
Several selection algo-rithms have been proposed.
Dasgupta et al(2001)and Abney (2002) use a selection algorithm that triesto maximize the agreement rate between the twomodels.
The more popular selection algorithm is tochoose the K examples that have the highest con-1193Input:S is the labelled dataU is the unlabelled dataVariables:L1 is the training data for View OneL2 is the training data for View TwoC is a cache holding a small subset of UInitialization:L1 <- SL2 <- SC <- randomly sample a subset of UU <- U - CREPEAT:1.
Train M1 using L12.
Train M2 using L23.
Use M1 to label C4.
Use M2 to label C5.
Select examples labelled by M1, add to L26.
Select examples labelled by M2, add to L17.
Randomly move samples from U to Cso that C maintains its sizeUNTIL stopping criteriaFigure 2: A generic co-training frameworkfidence score (Nigam and Ghani, 2000; Pierce andCardie, 2001).
In order to balance the class distri-butions in the training data L1 and L2, Blum andMitchell (1998) select P positive examples and Qnegative examples that have the highest confidencescores respectively.
Wang et al(2007) and Guz etal.
(2007) use disagreement-based selection, whichadds to L2, data that is labeled by M1 and M2 withhigh and low confidence respectively, with the in-tuition that such data are more useful and compen-satory to M2.
Finally, instead of adding the selecteddata to the training data, Tur (2009) propose the co-adaptation approach which linearly interpolates theexisting model with the new model built with thenew selected data.3 Segmentation With Co-Training3.1 Design of Two SegmentersThe use of co-training needs two statistical modelsthat satisfy the following three conditions.
First, intheory these two models need to be built on two con-ditionally independent views.
However this is a verystrong assumption and many large-scale NLP prob-lems do not have a natural split of features to satisfythis assumption.
In practice it has been shown thatco-training can still achieve improved performancewhen this assumption is violated, but conforming tothe conditionally independent assumption leads toa bigger gain (Nigam and Ghani, 2000; Pierce andCardie, 2001).
Thus we should strive to have the twomodels less correlated.
Second, the two models bothneed to be effective for the task, that is, each of themodels itself can perform the task reasonably well.Third, the decoding and training of the two modelsneed to be efficient, as in co-training we need to seg-ment the unlabelled data and re-train the models ineach iteration.
In the following we describe our de-sign of the two segmenters.Word-based segmenter In the word-based seg-menter, we utilize a statistical n-gram lan-guage model and try to optimize the languagemodeling score together with a word insertionpenalty, as show in Equation 4.
K is a per-word penalty that is pre-determined with 10fold cross-validation on the SIGhan PKU train-ing set.
We train a Kneser-Ney backoff lan-guage model from the training data, and extracta dictionary of words from the training data forgenerating the search space.
Our pilot studysuggested that a bigram language model is suf-ficient for this task.Util(W ) = ln(P (W )) ?
|W | ?
K (4)Character-based segmenter We use an order-1linear conditional random field to label a char-acter sequence.
Following Xue (2003), we usethe four-tag scheme ?BIES?.
We use the toolCRF++3.
The features that we use are charac-ter n-grams within the neighboring 5-characterwindow and tag bigrams.
Given a character c0in the character sequence c?2c?1c0c1c2, we ex-tract the following features: character unigramsc?2, c?1, c0, c1, c2, bigrams c?1c0 and c0c1.
L2regularization is applied in learning.As can be seen, we build a word-based segmenterthat uses only word level features, and a character-based segmenter that uses only character level fea-tures.
These two segmenters by no means satisfythe conditionally independence assumption, but wehave the hope that they are not too correlated asthey use different levels of information and these3http://crfpp.googlecode.com/svn/trunk/doc/index.html1194different levels of information have been shown tobe complementary in literature.
Also the effective-ness of these two segmenters has been demonstratedin literature and will be shown again in our resultsin Section 4.
Finally, both segmenters can decodeand be trained pretty quickly.
In our implemen-tation, running on a Xeon 2.93GHz CPU with 4Gof memory, it takes less than 30 seconds to build aword-based segmenter and less than 1 hour to builda character-based segmenter with the SIGhan PKUtraining data, and it takes less than 20 seconds to ap-ply the word-based segmenter or less than 5 secondsto apply the character-based segmenter to the PKUtesting data.3.2 Co-TrainingWe follow the framework in Figure 2 for the co-training setup.
We do not use the cache C, but di-rectly label the whole unlabelled data set U, becausein our experiment setup (see Section 4) U is nothuge and computationally we can afford to label thewhole set.
The stopping criteria we use is when U isempty.
Following Wang et al(2007) and Guz et al(2007), we use disagreement-based data selection.In every iteration, we pick some sentences that aresegmented by the character-based model with highconfidence but are segmented by the word-basedmodel with low confidence to add to the trainingdata of the word-based model, and vice versa.
Con-fidence score is normalized with regard to the lengthof the sentence (i.e.
number of characters) to avoidbiasing towards short sentences.
Confidence scoresbetween the two segmenters, however, are not di-rectly comparable.
Thus we rank the sentences bytheir confidence scores in each segmenter respec-tively, and calculate the rank difference between thetwo segmenters.
This rank difference is used as theindication of the gap of the confidence between thetwo segmenters.
The sentences of highest rank dif-ference are assigned to the training data of the word-based segmenter, with the segmentations from thecharacter-based model; and the sentences of lowestrank difference are assigned to the training data ofthe character-based model, with segmentations fromthe word-based model.4 Experiments4.1 Data and Experiment SetupWe conduct a set of experiments to evaluate the per-formance of our co-training on semi-supervised Chi-nese word segmentation.
Two corpora, the PKU cor-pus and the CU corpus, from the SIGhan Bakeoff2005 are used.
The PKU corpus contains texts ofsimplified Chinese characters, which include 19056sentences in the training data and 1945 sentences inthe testing data.
The CU corpus contains texts oftraditional Chinese characters, which include 53019sentences in the training data and 1493 sentences inthe testing data.
The training data in each corpus israndomly split into 10 subsets.
In each run one setis used as the labelled data S, and the other nine setsare combined and used as the unlabelled data U withsegmentations removed.
That is, 10% of the trainingdata is used as segmented data, and 90% are usedas unsegmented data in our semi-supervised train-ing.
This setup resembles our semi-supervised ap-plication, where there is only a small limited amountof segmented data but a relatively large amount ofin-domain unsegmented data available.
The finaltrained character-based and word-based segmentersfrom co-training are then evaluated on the testingdata.
Results we report in this paper are the aver-age of the 10 runs.
F-measure is used as the per-formance measurement.
A 99% confidence intervalis calculated as ?2.56?p(1?
F )/N for statisticalsignificance evaluation, where F is the F-measureand N is the number of words.
Subsequent asser-tions in this paper about statistical significance indi-cate whether or not the p-value in question exceeds1%.4.2 Co-Training ResultsFor comparison, we measure the baseline as theperformance of a model trained with the 10%segmented data only (referred to as BASIC base-lines).
The BASIC baselines, both for the word-based model and the character-based model, how-ever, use only the segmented data but leave out thelarge amount of available unsegmented data.
Wethus measure another baseline (referred to as FOLD-IN), which naively uses the unsegmented data.
In theFOLD-IN baseline, a model is first trained with the10% segmented data, and then this model is used1195Table 1: Co-training resultsPKU CUchar word char wordBASIC 90.4 84.2 89.2 78.4FOLD-IN 90.5 84.2 89.3 78.5CEILING 94.5 93.0 94.2 88.9CO-TRAINING 91.2 90.3 90.2 86.2Figure 3: Gap filling with different split ratioto label the unsegmented data.
The automatic seg-mentation is then combined with the segmented datato build a new model.
We also measure the CEIL-ING as the performance of a model trained with allthe training data available, i.e.
we use the true seg-mentations of the 90% unsegmented data togetherwith the 10% segmented data to train a model.
TheCEILING tells us the oracle performance when wehave all segmented data for training, while the BA-SIC shows how much performance is dropped whenwe only have 10% of the segmented data.
The per-formance of co-training will tell us how much wecan fill the gap by taking advantage of the other 90%as unsegmented data in the semi-supervised training.The FOLD-IN baseline further verifies the effective-ness of co-training, i.e.
co-training should performbetter than naively folding in the unsegmented data.Table 1 presents the results.
First, we see thatboth the word-based and character-based modelsare doing a decent job under the CEILING condi-tion.
This confirms the effectiveness of each in-dividual model, which is generally a requirementfor running co-training.
The character-based seg-menter, although simple and with character-levelfeatures only, achieves the performance that is closeto the state-of-the-art technologies that are muchmore complicated (The best performance is 95.2%for the PKU corpus and 95.1% for the CU corpus,see (Sun et al 2009)).
Second, we see that underall four conditions, the character-based segmenterperforms better than the word-based model.
Thisis not too surprising as these results are consistentwith those reported in the literature.
The word-basedsegmenter implemented in this work is less power-ful, and it needs a good dictionary to achieve goodperformance.
In our implementation, a dictionaryis extracted from the segmented training set.
Thusthe word-based model suffers a lot when the train-ing data is small.
Third, we see that both the word-based model and the character-based model are im-proved by co-training, and the improvements are allstatistically significant.
It is not surprising for theword-based model to learn from the more accuratecharacter-based model, which can also identify newwords to add to the dictionary.
More interestingly,the character-based segmenter is able to benefit fromthe less powerful word-based segmenter.
For thecharacter-based model, about 20% of the gap be-tween BASIC and CEILING is filled by co-training,consistently in both the PKU and CU corpora.
Fi-nally, comparing FOLD-IN and BASIC, we see thatnaively using the unsegmented data does not lead toa significant improvement.
This suggests that co-training provides a process that effectively makesuse of the unsegmented data.For completeness, in Figure 3 we also show therelative gap filling with different splits of the seg-mented vs unsegmented data.
With more data mov-ing to the segmented set, the absolute improvementof co-training over BASIC gets smaller, while thegap between the BASIC and CEILING also becomessmaller.
The relative gap filled, i.e.
the improve-ment relative to the difference between BASIC andCEILING, as can be seen, consistently falls insidethe section of 15% and 25%.11964.3 Further AnalysisIt is not surprising that the word-based segmenterbenefits from co-training since it learns from themore accurate character-based segmenter.
Our fo-cus, however, is to better understand what benefitthe character-based segmenter gains from the co-training procedure.
The character-based segmentertreats word segmentation as a character sequencelabelling problem with four tags ?B I E S?.
As-suming that segmentation accuracy is proportionalto tag accuracy, we examine the tag accuracy ofthe character-based segmenter before and after co-training.If a character is labelled with tag T0 initially be-fore co-training and with tag T1 after co-training,with the tag T1 different from T0, there can be oneof three cases: 1) T0 is correct; 2) T1 is correct; or3) neither is correct.
The absolute gain from co-training of switching from tag T0 to T1 is definedas the number of case 2 instances less case 1 in-stances.
Absolute gain indicates the gain of tag ac-curacy where co-training learns to switch from T0 toT1, and it contributes to the overall tag accuracy im-provement.
We also define relative gain of switch-ing from tag T0 to T1 as the absolute gain dividedby the total number of cases switching from tag T0to T1.
Relative gain indicates how well co-traininglearns to switch from T0 to T1.Results are shown in Table 2.
For both absolutegain and relative gain, 12 ordered switching pairscan be divided into two pools, a positive pool thathas higher gain includingB ?
E, E ?
B, S ?
B,S ?
E, E ?
I , B ?
I , B ?
S, and a neutralpool that has lower or even negative gain includingI ?
E, I ?
S, I ?
B, E ?
S, S ?
I .
TheS ?
B, S ?
E, B ?
I , E ?
I in the positivepool actually suggest that the character-based seg-menter learns from co-training to combine a single-character word with it?s neighbour to create a newlonger word; whereas the I ?
E, I ?
S, I ?
B inthe neutral pool suggest that it does not really learnhow to separate a longer words into smaller units.4.4 Feature CombinationWe split the features into two sets, a character-levelfeature set used by the character-based segmenterand a word-level feature set used by the word-basedTable 2: Absolute Gain and Relative GainAbsolute Gain Relative GainT0 T1 PKU CU PKU CUB I 678 681 0.28 0.59B E 2331 1727 0.41 0.46B S 1025 686 0.07 0.08I B 458 -283 0.07 -0.08I E 61 -1117 0.01 -0.23I S 323 -338 0.09 -0.34E B 2163 1601 0.41 0.46E I 963 819 0.36 0.62E S 520 -13 0.03 0.00S B 1847 892 0.27 0.30S I 104 47 0.22 0.28S E 1438 846 0.26 0.55segmenter.
We have shown that these two seg-menters improve each other via co-training.
How-ever, as reviewed in Section 2.1, there is active re-search in combining the character-level and word-level features in a segmenter.
When training withthe whole set of data (i.e.
under the CEILING con-dition), a segmenter with combined features tends toperform better than only using one set of features.Thus we need to address two problems.
First, wewant to understand whether co-training, which splitsthe features, can actually beat the BASIC and FOLD-IN baselines of a segmenter with combined features.Second, we want to explore whether we can furtherimprove the final co-training performance by featurecombination.To address these two problems, we adopt WeiweiSun?s character-based segmenter4 in (Sun, 2010).We use this segmenter because it is publicly avail-able and it performs well on both the PKU corpusand CU corpus.
It models word segmentation asa character labelling problem, and solves it with apassive-aggressive optimization algorithm.
It usesthe same feature set as in (Sun et al 2009), in-cluding both character-level features and word-levelfeatures.
Character-level features include characteruni-grams and bi-grams in the five character win-dow, and whether the current character is the sameas the next or the one after the next character.
Word-4Available at http://www.coli.uni-saarland.de/ wsun/ccws.tgz1197Table 3: Sun-Segmenter?s performancePKU CUBASIC 90.3 89.2FOLD-IN 90.6 89.7CEILING 94.8 95.0Table 4: Results of feature combinationPKU CUdata combination 91.2 90.9relabelling 91.2 91.0level features include what word uni-grams or bi-grams are anchored at the current character.
Worduni-grams and bi-grams are extracted from the la-beled training data.
For more details, please referto (Sun et al 2009) and (Sun, 2010).
For easeof description, we will refer to Weiwei Sun?s seg-menter with combined features as Sun-Segmenter,and the character-based segmenter used in our co-training which uses character-level features as Char-Segmenter.Table 3 shows the performance of the Sun-Segmenter under the three conditions: BASIC,FOLD-IN, and CEILING.
We see that underthe CEILING condition, the Sun-Segmenter out-performs the Char-Segmenter by 0.3% in the PKUcorpus and 0.8% in the CU corpus.
However, un-der the BASIC condition when there is only 10%of training data available, the Sun-Segmenter givesno gain.
This probably is due to the fact that theSun-Segmenter uses a much larger feature set andthus correspondingly a larger training set is neededto avoid under-fitting.
The Sun-Segmenter has moregain when folding in the unsegmented data than theChar-Segmenter, further suggesting that the Sun-Segmenter is benefiting from the size of data.
Forboth corpora, however, the Char-Segmenter after co-training beats the FOLD-IN baseline of the Sun-Segmenter by at least 0.5%, and the improvementis statistically significant.
When there is only asmall amount of segmented data available, using amore advanced segmenter with combined featuresstill under-performs compared to co-training.
Theseresults justify the split of features for running co-training.Next we would like to explore whether we couldfurther improve the co-training performance, giventhat we have a more advanced segmenter using com-bined features.
We try two approaches.
In the firstapproach, after all the iterations of co-training, thedata are split into two sets, one set for training theword-based segmenter L1 and the other set for train-ing the character-based segmenter L2.
The segmen-tations of these two sets of data are probably bet-ter than the segmentations under the FOLD-IN con-dition.
We thus combine the two sets of data, anduse the combined data to train a new model with theSun-Segmenter.
In the second approach, we use thecharacter-based segmenter after co-training, whichhas an improved performance, to relabel the set ofunsegmented data U, and then combine it with thesegmented data set S.We then use the combined datato train a new model with the Sun-Segmenter.Results are shown in Table 4.
In the PKU corpus,we do not see a gain using either the data combina-tion approach or the relabelling approach comparedto the performance of the Char-Segmenter after co-training, probably because the Sun-Segmenter justmodestly improves over the Char-Segmenter underthe CEILING condition.
However, in the CU cor-pus, where under the CEILING condition the Sun-Segmenter has a much bigger gain over the Char-Segmenter, there is 0.7% improvement by using thedata combination approach and 0.8% by using therelabelling approach, and the improvement is statis-tically significant.
Overall, using co-training withfeature combination we are able to cut the gap be-tween the BASIC baseline and CEILING of the Sun-Segmenter by 20% in the PKU corpus and 31% inthe CU corpus.5 DiscussionThere has been some research on semi-supervisedChinese word segmentation.
For example, Liang(2005) derive word cluster features and mutual in-formation features from unlabelled data, and addthem to supervised discriminative training; Li andSun (2009) use punctuation as implicit annotationsof a character starting a word (the character after apunctation) or ending a word (the character beforea punctuation) in a large unlabelled data set to aug-ment supervised data; Sun and Xu (2011) derive alarge set of features from unlabelled data, includ-1198ing mutual information, accessor variety and punc-tuation variety to augment the character and wordfeatures derived from labelled data.
These researchworks aim to use huge amount of unsegmented datato further improve the performance of an alreadywell-trained supervised model.In this paper, we assume a much limited amountof segmented data available, and try to boost up theperformance by using in-domain unsegmented data.Chinese word segmentation is domain-sensitive orapplication sensitive.
For example, a CRF seg-menter trained on the SIGhan MSR training data,which achieves an F-measure of 96.5% in the MSRtesting data, only has 83.8% when applied to thePKU testing data; and the same CRF segmentertrained on the PKU training data achieves 94.5% onthe PKU testing data.
When one starts a new ap-plication that requires word segmentation in a newdomain, it is likely that there is only a very smallamount of segmented data available.We propose the approach of co-training for Chi-nese word segmentation for the semi-supervised set-ting where there is only a limited amount of human-segmented data available, but there exists a relativelylarge amount of in-domain unsegmented data.
Wesplit the feature set into character-level features andword-level features, and then build a character-basedsegmenter with character-level features and a word-based segmenter with word-level features, using thelimited amount of available segmented data.
Thesetwo segmenters then iteratively educate and improveeach other by making use of the large amount ofunsegmented data.
Finally we combine the word-level and character-level features with an advancedsegmenter to further improve the co-training perfor-mance.
Our experiments show that using 10% dataas segmented data and the other 90% data as unseg-mented data, co-training reaches 20% performanceimprovement achieved by supervised training withall data in the SIGHAN 2005 PKU corpus and 31%in the CU corpus.AcknowledgmentsThe authors thank Weiwei Sun for helping withdata setup and technical consultation of the Sun-Segmenter.
The authors also thank Christian Mon-son and Nicola Ueffing for helpful discussions.ReferencesSteven Abney.
2002.
Bootstrapping.
In Proceedings ofthe 40th Annual Meeting on Association for Computa-tional Linguistics, pages 360?367.Galen Andrew.
2006.
A hybrid markov/semi-markovconditional random field for sequence segmentation.In Proceedings of the 2006 Conference on EmpiricalMethods in Natural Language Processing, pages 465?472.Avrim Blum and Tom Mitchell.
1998.
Combining la-beled and unlabeled data with co-training.
In Proceed-ings of the eleventh annual conference on Computa-tional learning theory, pages 92?100.Michael Collins and Yoram Singer.
1999.
Unsuper-vised models for named entity classification.
In JointSIGDAT Conference on Empirical Methods in NaturalLanguage Processing and Very Large Corpora, pages100?110.Sanjoy Dasgupta, Michael L. Littman, and DavidMcallester.
2001.
Pac generalization bounds for co-training.
In Proceedings of Advances in Neural Infor-mation Processing Systems, pages 375?382.Jianfeng Gao, Mu Li, Andi Wu, and Chang-Ning Huang.2005.
Chinese word segmentation and named entityrecognition: A pragmatic approach.
ComputationlLinguistics, 31(4):574.Umit Guz, Sebastien Cuendet, Dilek Hakkani-Tur, andGokhan Tur.
2007.
Co-training using prosodic andlexical information for sentence segmentation.
In pro-ceedings of INTERSPEECH, pages 2597?2600.Zhongguo Li and Maosong Sun.
2009.
Punctuation asimplicit annotations for chinese word segmentation.Comput.
Linguist., 35:505?512, December.Percy Liang.
2005.
Semi-supervised learning for nat-ural language.
Master?s thesis, MASSACHUSETTSINSTITUTE OF TECHNOLOGY, May.Tetsuji Nakagawa.
2004.
Chinese and japanese wordsegmentation using word-level and character-level in-formation.
In Proceedings of the 20th internationalconference on Computational Linguistics.Kamal Nigam and Rayid Ghani.
2000.
Analyzing theeffectiveness and applicability of co-training.
In Pro-ceedings of the ninth international conference on In-formation and knowledge management, pages 86?93.Fuchun Peng, Fangfang Feng, and Andrew McCallum.2004.
Chinese segmentation and new word detectionusing conditional random fields.
In Proceedings of the20th international conference on Computational Lin-guistics.David Pierce and Claire Cardie.
2001.
Limitations ofco-training for natural language learning from largedatasets.
In In Proceedings of the 2001 Conference on1199Empirical Methods in Natural Language Processing,pages 1?9.Weiwei Sun and Jia Xu.
2011.
Enhancing chinese wordsegmentation using unlabeled data.
In Proceedings ofthe 2011 Conference on Empirical Methods in Natu-ral Language Processing, pages 970?979, Edinburgh,Scotland, UK., July.Xu Sun, Yaozhong Zhang, TakuyaMatsuzaki, YoshimasaTsuruoka, and Jun?ichi Tsujii.
2009.
A discrimi-native latent variable chinese segmenter with hybridword/character information.
In Proceedings of HumanLanguage Technologies: The 2009 Annual Conferenceof the North American Chapter of the Association forComputational Linguistics, pages 56?64.Weiwei Sun.
2010.
Word-based and character-basedword segmentation models: comparison and combi-nation.
In Proceedings of the 23rd International Con-ference on Computational Linguistics: Posters, pages1211?1219.W.
J. Teahan, Rodger McNab, Yingying Wen, and Ian H.Witten.
2000.
A compression-based algorithm forchinese word segmentation.
Computational Linguis-tics, 26(3):375?393, September.Gokhan Tur.
2009.
Co-adaptation: Adaptive co-training for semi-supervised learning.
In proceedingsof ICASSP, pages 3721?3724.Wen Wang, Zhongqiang Huang, and Mary Harper.
2007.Semi-supervised learning for part-of-speech tagging ofmandarin transcribed speech.
In In ICASSP.Nianwen Xue.
2003.
Chinese word segmentation ascharacter tagging.
Computational Linguistics andChinese Language Processing, pages 29?48.Yue Zhang and Stephen Clark.
2007.
Chinese segmenta-tion with a word-based perceptron algorithm.
In Pro-ceedings of the 45th Annual Meeting of the Associationof Computational Linguistics, pages 840?847, Prague,Czech Republic, June.
Association for ComputationalLinguistics.Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-LiangLu.
2010.
A unified character-based tagging frame-work for chinese word segmentation.
ACM Trans-actions on Asian Language Information Processing,9(2):5:1?5:32, June.1200
