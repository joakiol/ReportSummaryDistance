Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1689?1700,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsDependency-Based Bilingual Language Models forReordering in Statistical Machine TranslationEkaterina Garmash and Christof MonzInformatics Institute, University of AmsterdamScience Park 904, 1098 XH Amsterdam, The Netherlands{e.garmash,c.monz}@uva.nlAbstractThis paper presents a novel approach toimprove reordering in phrase-based ma-chine translation by using richer, syntac-tic representations of units of bilinguallanguage models (BiLMs).
Our methodto include syntactic information is simplein implementation and requires minimalchanges in the decoding algorithm.
Theapproach is evaluated in a series of Arabic-English and Chinese-English translationexperiments.
The best models demon-strate significant improvements in BLEUand TER over the phrase-based baseline,as well as over the lexicalized BiLM byNiehues et al.
(2011).
Further improve-ments of up to 0.45 BLEU for Arabic-English and up to 0.59 BLEU for Chinese-English are obtained by combining our de-pendency BiLM with a lexicalized BiLM.An improvement of 0.98 BLEU is ob-tained for Chinese-English in the setting ofan increased distortion limit.1 IntroductionIn statistical machine translation (SMT) reorder-ing (also called distortion) refers to the order inwhich source words are translated to generate thetranslation in the target language.
Word orderscan differ significantly across languages.
For in-stance, Arabic declarative sentences can be verb-initial, while the corresponding English translationshould realize the verb after the subject, hence re-quiring a reordering.
Determining the correct re-ordering during decoding is a major challenge forSMT.
This problem has received a lot of attentionin the literature (see, e.g., Tillmann (2004), Zensand Ney (2003), Al-Onaizan and Papineni (2006)),as choosing the correct reordering improves read-ability of the translation and can have a substan-tial impact on translation quality (Birch, 2011).
Inthis paper, we only consider those approaches thatinclude a reordering feature function into the log-linear interpolation used during decoding.The simplest reordering model is linear distor-tion (Koehn et al., 2003) which scores the distancebetween phrases translated at steps t and t + 1 ofthe derivation.
This model ignores any contex-tual information, as the distance between trans-lated phrases is its only parameter.
Lexical dis-tortion modeling (Tillmann, 2004) conditions re-ordering probabilities on the phrase pairs trans-lated at the current and previous steps.
Unlikelinear distortion, it characterizes reordering not interms of distance but type: monotone, swap, ordiscontinuous.In this paper, we base our approach to reorder-ing on bilingual language models (Marino et al.,2006; Niehues et al., 2011).
Instead of directlycharacterizing reordering, they model sequencesof elementary translation events as a Markov pro-cess.1Originally, Marino et al.
(2006) used thiskind of model as the translation model, while morerecently it has been used as an additional modelin PBSMT systems (Niehues et al., 2011).
Weadopt and generalize the approach of Niehues et al.
(2011) to investigate several variations of bilinguallanguage models.
Our method consists of labelingelementary translation events (tokens of bilingualLMs) with their different contextual properties.What kind of contextual information should beincorporated in a reordering model?
Lexical in-formation has been used by Tillmann (2004) butis known to suffer from data sparsity (Galley andManning, 2008).
Also previous contributions tobilingual language modeling (Marino et al., 2006;Niehues et al., 2011) have mostly used lexicalinformation, although Crego and Yvon (2010a)and Crego and Yvon (2010b) label bilingual to-1Note that the standard PBSMT translation model as-sumes that events of translating separate phrases in a sentenceare independent.1689kens with a rich set of POS tags.
But in gen-eral, reordering is considered to be a syntactic phe-nomenon and thus the relevant features are syn-tactic (Fox, 2002; Cherry, 2008).
Syntactic in-formation is incorporated in tree-based approachesin SMT, allowing one to provide a more detaileddefinition of translation events and to redefine de-coding as parsing of a source string (Liu et al.,2006; Huang et al., 2006; Marton and Resnik,2008), of a target string (Shen et al., 2008), orboth (Chiang, 2007; Chiang, 2010).
Reorderingis a result of a given derivation, and CYK-baseddecoding used in tree-based approaches is moresyntax-aware than the simple PBSMT decodingalgorithm.
Although tree-based approaches poten-tially offer a more accurate model of translation,they are also a lot more complex and requiringmore intricate optimization and estimation tech-niques (Huang and Mi, 2010).Our idea is to keep the simplicity of PBSMT butmove towards the expressiveness typical of tree-based models.
We incrementally build up the syn-tactic representation of a translation during decod-ing by adding precomputed fragments from thesource parse tree.
The idea to combine the mer-its of the two SMT paradigms has been proposedbefore, where Huang and Mi (2010) introduce in-cremental decoding for a tree-based model.
On avery general level, our approach is similar to theirsin that it keeps track of a sequence of source syn-tactic subtrees that are being translated at consec-utive decoding steps.
An important difference isthat they keep track of whether the visited subtreeshave been fully translated, while in our approach,once a syntactic structural unit has been added tothe history, it is not updated anymore.In this paper, we focus on source syntactic in-formation.
During decoding we have full accessto the source sentence, which allows us to obtaina better syntactic analysis (than for a partial sen-tence) and to precompute the units that the modeloperates with.
We investigate the following re-search questions: How well can we capture re-ordering regularities of a language pair by incor-porating source syntactic parameters into the unitsof a bilingual language model?
What kind ofsource syntactic parameters are necessary and suf-ficient?Our contributions can be summarized as fol-lows: We argue that the contextual informationused in the original bilingual models (Niehues etal., 2011) is insufficient and introduce a simplemodel that exploits source-side syntax to improvereordering (Sections 2 and 3).
We perform a thor-ough comparison between different variants of ourgeneral model and compare them to the originalapproach.
We carry out translation experimentson multiple test sets, two language pairs (Arabic-English and Chinese-English), and with respect totwo metrics (BLEU and TER).
Finally, we presenta preliminary analysis of the reorderings resultingfrom the proposed models (Section 4).2 MotivationIn this section, we elaborate on our research ques-tions and provide background for our approach.We also discuss existing bilingual n-gram mod-els and argue that they are often not expressiveenough to differentiate between alternative re-orderings.
We should first note that the most com-monly used n-gram model to distinguish betweenreorderings is a target language model, which doesnot take translation correspondence into accountand just models target-side fluency.
Al-Onaizanand Papineni (2006) show that target languagemodels by themselves are not sufficient to cor-rectly characterize reordering.
In what follows weonly discuss bilingual models.The word-aligned sentence pair in Figure 1.a2demonstrates a common Arabic-English reorder-ing.
As stated in the introduction, bilingual lan-guage models capture reordering regularities as asequence of elementary translation events3.
In thegiven example, one could decompose the sequen-tial process of translation as follows: First trans-late the first word Alwzyr as the minister, then ArjEas attributed, then ArtfAE as the increase and soon.
The sequence of elementary translation eventsis modeled as an n-gram model (Equation 1, wheretiis a translation event).
There are numerous waysin which tican be defined.
Below we first discusshow they have been defined within previous ap-proaches, and then introduce our definition.p(t1, .
.
.
, tm) =m?i=1p(ti|ti?n+1.
.
.
ti?1) (1)2.1 Lexicalized bilingual LMsBy including both source and target informationinto the representation of translation events we ob-2We used Buckwalter transliteration for Arabic words.3By an elementary translation event we mean a translationof some substructure of a sentence.1690the minister attributed the increase of oil pricesw ArjE AlwzyrArtfAE AsEAr Albtrwl(a) The original word alignment.theAlwzyrministerAlwzyrattributedArjEtheArtfAEincreaseArtfAEofemptyoilAlbtrwlpricesAsEAr(b) BiLM tokens extracted from sentence (a).emptywofoilAlbtrwlpricesAsEArthe ministerAlwzyr ArjEthethe increaseArtfAE(c) MTU tokens extracted from sentence (a).Figure 1: Arabic-English parallel sentence, automatically word-aligned.
The bilingual token sequencesare produced according to two alternative definitions (BiLM and MTU).tain a bilingual LM.
The richer representation al-lows for a finer distinction between reorderings.For example, Arabic has a morphological markerof definiteness on both nouns and adjectives.
Ifwe first translate a definite adjective and then anindefinite noun, it will probably not be a likely se-quence according to the translation model.
Thiskind of intuition underlies the model of Niehues etal.
(2011), a bilingual LM (BiLM), which defineselementary translation events t1, ..., tnas follows:ti= ?ei, {f |f ?
A(ei)}?, (2)where eiis the i-th target word and A : E ?P(F ) is an alignment function, E and F refer-ring to target and source sentences, and P(?)
is thepowerset function.
In other words, the i-th trans-lation event consists of the i-th target word and allsource words aligned to it.
Niehues et al.
(2011)refer to the defined translation events tias bilin-gual tokens and we adopt this terminology.There are alternative definitions of bilinguallanguage models.
Our choice of the above defi-nition is supported by the fact that it produces anunambiguous segmentation of a parallel sentenceinto tokens.
Ambiguous segmentation is unde-sirable because it increases the token vocabulary,and thus the model sparsity.
Another disadvan-tage comes from the fact that we want to comparepermutations of the same set of elements.
For ex-ample, the two different segmentations of ba into[ba] and [b][a] still represent the same permuta-tion of the sequence ab.
In Figure 1 one can pro-duce a segmentation of (AsEAr Albtrwl, oil prices)into (Albtrwl, oil) and (AsEAr, prices) or leaveit as is.
If we allow for both segmentations, thelearnt probability parameters may be different forthe sum of (Albtrwl, oil) and (AsEAr, prices) andfor the unsegmented phrase.Durrani et al.
(2011) introduce an alternativemethod for unambiguous bilingual segmentationwhere tokens are defined as minimal phrases,called minimal translation units (MTUs).
Figure 1compares the BiLM and MTU tokenization for aspecific example.
Since Niehues et al.
(2011) haveshown their model to work successfully as an addi-tional feature in combination with commonly usedstandard phrase-based features, we use their ap-proach as the main point of reference and base ourapproach on their segmentation method.
In therest of the text we refer to Niehues et al.
(2011)as the original BiLM.4At the same time, we donot see any specific obstacles for combining ourwork with MTUs.2.2 Suitability of lexicalized BiLM to modelreorderingAs mentioned in the introduction, lexical informa-tion is not very well-suited to capture reorderingregularities.
Consider Figure 2.a.
The extractedsequence of bilingual tokens is produced by align-ing source words with respect to target words (sothat they are in the same order), as demonstratedby the shaded part of the picture.
If we substitutedthe Arabic translation of Egyptian for the Arabictranslation of Israeli, the reordering should remainthe same.
What matters for reordering is the syn-tactic role or context of a word.
By using unneces-sarily fine-grained categories we risk running intosparsity issues.Niehues et al.
(2011) also described an alterna-tive variant of the original BiLM, where words aresubstituted by their POS tags (Figure 2.a, shadedpart).
Also, however, POS information by itselfmay be insufficiently expressive to separate cor-4Although, strictly speaking, it is not the original ap-proach (see the references in Section 1).1691EgyptianexportstotrAjEtSAdrAt mSr l Aldwl AlErbypVBD NNS NNP IN DTNN DTJJJJ NNS TOArabiccountries declined ?
?JJ NNS VBDtrAjEtSAdrAtmSrl AldwlAlErbypNNSNNP IN DTJJ?DTNN VBD(a)trAjEtSAdrAt mSr l Aldwl AlErbypVBD NNS NNP IN DTNN DTJJArabicJJAlErbypDTJJcountriesNNSAldwlDTNNdeclinedVBDtrAjEtVBDEgyptianexportstoJJ NNS TOSAdrAtmSrlNNSNNP IN(b)Figure 2: Arabic-English parallel sentence, automatically parsed and word-aligned, with correspondingsequences of bilingual tokens (in the shaded part).
Comparison between translations produced via correct(a) and incorrect (b) reorderings.JJ NNS TOJJ NNS VBDNNS!NNPVBD!NNS NNS!INDTNN!DTJJIN!DTNNROOT!VBD(a)JJ NNS TOJJ NNS VBDNNS!NNPVBD!NNS NNS!INDTNN!DTJJIN!DTNNROOT!VBD(b)Figure 3: Sequences of bilingual tokens withsource words substituted with their and their par-ents?
POS tags: correct (a) and incorrect (b) re-orderings.rect and incorrect reorderings, see Figure 2.b.
Al-though the corresponding sequence of POS-tag-substituted bilingual tokens is different from thecorrect sequence (Figure 2.b, shaded part), it stillis a likely sequence.
Indeed, the log-probabilitiesof the two sequences with respect to a 4-gramBiLM model5result in a higher probability of?10.25 for the incorrect reordering than for thecorrect one (?10.39).Since fully lexicalized bilingual tokens sufferfrom data sparsity and POS-based bilingual tokensare insufficiently expressive, the question is whichlevel of syntactic information strikes the right bal-ance between expressiveness and generality.5Section 4 contains details about data and software setup.2.3 BiLM with dependency informationDependency grammar is commonly used in NLPto formalize role-based relations between words.The intuitive notion of syntactic modification iscaptured by the primitive binary relation of depen-dence.
Dependency relations do not change withthe linear order of words (Figure 2) and thereforecan provide a characterization of a word?s syntac-tic class that invariant under reordering.If we incorporate dependency relations into therepresentation of bilingual tokens, the incorrect re-ordering in Figure 2.b will produce a highly un-likely sequence.
For example, we can substituteeach source word with its POS tag and its par-ent?s POS tag (Figure 3).
Again, we computed4-gram log-probabilities for the corresponding se-quences: the correct reordering results in a sub-stantially higher probability of?10.58 than the in-correct one (?13.48).
We may consider situationswhere more fine-grained distinctions are required.In the next section, we explore different represen-tations based on source dependency trees.3 Dependency-based BiLMIn this section, we introduce our model whichcombines the BiLM from Niehues et al.
(2011)with source dependency information.
We fur-ther give details on how the proposed models aretrained and integrated into a phrase-based decoder.16923.1 The general frameworkIn the previous section we outlined our frameworkas composed of two steps: First, a parallel sen-tence is tokenized according to the BiLM model(Niehues et al., 2011).
Next, words in the bilingualtokens are substituted with their contextual prop-erties.
It is thus convenient to use the followinggeneralized definition for a token sequence t1...tnin our framework:ti= ?ContE (ei), {ContF (f)|f ?
A(ei)}?, (3)where eiis the i-th target word, A : E ?
P(F )is an alignment function, F and E are source andtarget sentences, and ContE and ContF are tar-get and source contextual functions, respectively.A contextual function returns a word?s contextualproperty, based on its sentential context (source ortarget).
See Figure 4 for an example of a sequenceof BiLM tokens with a ContF defined as return-ing the POS tag of the source word combined withthe POS tags of its parent, grandparent and sib-lings, and ContE defined as an identity function(see Section 3.2 for a detailed explanation of thefunctions and notation).In this work we focus on source contextualfunctions (ContF ).
We also exploit some verysimple target contextual functions, but do not gointo an in-depth exploration.3.2 Dependency-based contextual functionsIn NLP approaches exploiting dependency struc-ture, two kinds of relations are of special impor-tance: the parent-child relation and the sibling re-lation.
Shen et al.
(2008) work with two well-formed dependency structures, both of which aredefined in such a way that there is one commonparent and a set of siblings.
Li et al.
(2012) charac-terize rules in hierarchical SMT by labeling themwith the POS tags of the parents of the words in-side the rule.
Lerner and Petrov (2013) model re-ordering as a sequence of classification steps basedon a dependency parse of a sentence.
Their modelfirst decides how a word is reordered with respectto its parent and then how it is reordered with re-spect to its siblings.Based on these previous approaches, we pro-pose to characterize contextual syntactic roles ofa word in terms of POS tags of the words them-selves and their relatives in a dependency tree.
Itis straightforward to incorporate parent informa-tion since each node has a unique parent.
As forsiblings information, we incorporate POS tags ofthe closest sibling to the left and the closest to theright.
We do not include all of the siblings to avoidoverfitting.
In addition to these basic syntactic re-lations, we consider the grandparent relation.The following list is a summary of the sourcecontextual functions that we use.
We describea function with respect to the kind of contextualproperty of a word it returns: (i) the word itself(Lex); (ii) POS label of the word (Pos); (iii) POSlabel of the word?s parent; (iv) POS of the word?sclosest sibling to the left, concatenated with thePOS tag of the closest sibling to the right; (v)the POS label of the word?s grandparent.
We usetarget-side contextual functions returning: (i) anempty string, (ii) POS of the word, (iii) the worditself.Notation.
We do not use the above functionsseparately to define individual BiLM models, butuse combinations of these functions.
We use thefollowing notation for function combinations: ??
?horizontally connects source (on the left) and tar-get (on the right) contextual functions for a givenmodel.
For example, Lex?Lex refers to the original(lexicalized) BiLM.
We use arrows (?)
to des-ignate parental information (the arrow goes fromparent to child).
Pos?Pos refers to a combinationof a function returning the POS of a word and thePOS of its parent (as in Figure 3).
Pos?Pos?Posis a combination of the previous with the func-tion returning the grandparent?s POS.
Finally, weuse +sibl to indicate the use of the sibling func-tion described above: For example, Pos?Pos+siblis a source function that returns the word?s POS,its parent?s POS and the POS labels of the closestsiblings to left and right.6Pos+sibl?Pos is a sourcefunction returning the word?s own POS, the POSof a word?s parent, and the POS tags of the par-ent?s siblings (left- and right-adjacent).Figure 4 represents the sentence from Figure 2during decoding in a system with an integratedPos?Pos?Pos+sibl?Lex feature.
It shows the se-quence of produced bilingual tokens and corre-sponding labels in the introduced notation.3.3 TrainingTraining of dependency-based BiLMs consists ofa sequence of extraction steps: After having pro-duced word-alignments for a bitext (Section 4),6In case there is no sibling on one of the sides,  (emptyword) is returned.1693EgyptianexportstrAjEt SAdrAt mSr l Aldwl AlErbypVBD NNS NNP IN DTNNDTJJJJ NNS TOtoEgyptianVBD NNS NNPINtoVBD NNSNNPINexportsVBD NNS?Figure 4: Sequence of bilingual tokens pro-duced by a Pos?Pos?Pos+sibl?Lex aftertranslating three words of the source sentence:VBD?NNS?+NNS+IN?Egyptian, ROOT?VBD?+NNS+?exports, VBD?NNS?NNP+IN+?to (if thereis no sibling on either of the sides,  is returned).sentences are segmented according to Equation 3.We produce a dependency parse of a source sen-tence and a POS-tag labeling of a target sen-tence.
For Chinese, we use the Stanford depen-dency parser (Chang et al., 2009).
For Arabic adependency parser is not available for public use,so we produce a constituency parse with the Stan-ford parser (Green and Manning, 2010) and ex-tract dependencies based on the rules in Collins(1999).
For English POS-tagging, we use theStanford POS-tagger (Toutanova et al., 2003).
Af-ter having produced a labeled sequence of tokens,we learn a 5-gram model using SRILM (Stolckeet al., 2011).
Kneyser-Ney smoothing is usedfor all model variations except for Pos?Pos whereWitten-Bell smoothing is used due to zero count-of-counts.3.4 Decoder integrationDependency-based BiLMs are integrated into ourphrase-based SMT decoder as follows: Beforetranslating a sentence, we produce its dependencyparse.
Phrase-internal word-alignments, neededto segment the translation hypothesis into tokens,are stored in the phrase table, based on the mostfrequent internal alignment observed during train-ing.
Likewise, we store the most likely target-sidePOS-labeling for each phrase pair.The decoding algorithm is augmented with oneadditional feature function and one additional, cor-responding feature weight.
At each step of thederivation, as a new phrase pair is added to theTraining set N. of lines N. of tokensSource side of Ar-En set 4,376,320 148MTarget side of Ar-En set 4,376,320 146MSource side of Ch-En set 2,104,652 20MTarget side of Ch-En set 2,104,652 28MTable 1: Training data for Arabic-English andChinese-English experiments.partial translation hypothesis, this function seg-ments the new phrase into bilingual tokens (giventhe internal alignment information) and substitutesthe words in the phrase pair with syntactic labels(given the source parse and the target POS labelingassociated with the phrase).
The new syntactifiedbilingual tokens are added to the stack of preced-ing n?1 tokens, and the feature function computesthe weighted updated model probability.
Duringdecoding, the probabilities of the BiLMs are com-puted in a stream-based fashion, with bilingualtokens as string tokens, and not in a class-basedfashion, with syntactic source-side representationsemitting the corresponding target words (Bisazzaand Monz, 2014).4 Experiments4.1 SetupWe conduct translation experiments with a base-line PBSMT system with additionally one of thedependency-based BiLM feature functions speci-fied in Section 3.
We compare the translation per-formance to a baseline PBSMT system and to abaseline augmented with the original BiLMs from(Niehues et al., 2011).Word-alignment is produced with GIZA++(Och and Ney, 2003).
We use an in-house imple-mentation of a PBSMT system similar to Moses(Koehn et al., 2007).
Our baseline containsall standard PBSMT features including languagemodel, lexical weighting, and lexicalized reorder-ing.
The distortion limit is set to 5.
A 5-gram LMis trained on the English Gigaword corpus (1.6Btokens) using SRILM with modified Kneyser-Neysmoothing and interpolation.
The BiLMs weretrained as described in Section 3.3.
Informa-tion about the parallel data used for training theArabic-English7and Chinese-English systems8is7The following Arabic-English parallel corpora wereused: LDC2006E25, LDC2004T18, several gale corpora,LDC2004T17, LDC2005E46, LDC2007T08, LDC2004E13.8The following Chinese-English parallel corporawere used: LDC2002E18, LDC2002L27, LDC2003E07,LDC2003E14, LDC2005T06, LDC2005T10, LDC2005T34,1694Configuration MT08 MT09 MT08+MT09BLEU TER BLEU TER BLEU TERa PBSMT baseline 45.12 47.94 48.16 44.30 46.57 46.21b Lex?Lex 45.27 47.79 48.85N43.96M46.98N45.96MPos?Pos 44.80 47.84 48.22 44.14M,?46.44 46.07c Pos?Pos?Pos 45.66N,M47.17N,N49.00N,?43.45N,N47.25N,M45.40N,Nd Pos?Pos?sibl?Pos 45.46M,?47.45N,M48.69N,?43.64N,M47.00N,?45.64N,?e Pos?Pos?Pos?Pos 45.68N,M47.42N,M49.09N,?43.59N,N47.30N,M45.60N,Nf Lex?Lex + Pos?Pos?Pos?Pos 45.63N,M47.48N,M49.30N,N43.60N,M47.38N,N45.63N,NTable 2: BLEU and TER scores for Arabic-English experiments.
Statistically significant improvementsover the baseline (a) are markedNat the p < .01 level andMat the p < .05 level.
Additionally,?,Nand?,Mindicate significant improvements with respect to BiLM Lex?Lex (b).
Since TER is an error rate, lowerscores are better.Configuration MT08 MT09 MT08+MT09BLEU TER BLEU TER BLEU TERPos?Pos?
 45.66N,M47.44N,M48.78N,?43.94N,?47.15N,?45.77N,MPos?Pos?Pos 45.66N,M47.17N,N49.00N,?43.45N,N47.25N,M45.40N,NPos?Pos?Lex 45.48M,?47.34N,N48.90N,?43.87N,M47.12N,?45.69N,NTable 3: Different combinations of a target contextual function with the Pos?Pos source contextualfunction for Arabic-English.
See Table 2 for the notation regarding statistical significance.shown in Table 1.The feature weights were tuned by using pair-wise ranking optimization (Hopkins and May,2011) on the MT04 benchmark (for both languagepairs).
During tuning, 14 PRO parameter estima-tion runs are performed in parallel on differentsamples of the n-best list after each decoder itera-tion.
The weights of the individual PRO runs arethen averaged and passed on to the next decodingiteration.
Performing weight estimation indepen-dently for a number of samples corrects for someof the instability that can be caused by individualsamples.
For testing, we used MT08 and MT09 forArabic, and MT06 and MT08 for Chinese.
We useapproximate randomization (Noreen, 1989; Rie-zler and Maxwell, 2005) to test for statistically sig-nificant differences.In the next two subsections we discuss the gen-eral results for Arabic and Chinese, where we usecase-insensitive BLEU (Papineni et al., 2002) andTER (Snover et al., 2006) as evaluation metrics.This is followed by a preliminary analysis of ob-served reorderings where we compare 4-gram pre-cision results and conduct experiments with an in-creased distortion limit.4.2 Arabic-English translation experimentsWe are interested in how a translation systemwith an integrated dependency-based BiLM fea-and several gale corpora.ture performs as compared to the standard PB-SMT baseline and, more importantly, to the orig-inal BiLM model.
We consider two variants ofBiLM discussed by Niehues et al.
(2011): the stan-dard one, Lex?Lex, and the simplest syntactic one,Pos?Pos.
Results for the experiments can be foundin Table 2.
In the discussion below we mostly fo-cus on the experimental results for the large, com-bined test set MT08+MT09.Table 2.a?b compares the performance of thebaseline and original BiLM systems.
Lex?Lexyields strongly significant improvements over thebaseline for BLEU and weakly significant im-provements for TER.
Therefore, for the rest of theexperiments we are interested in obtaining furtherimprovements over Lex?Lex.Pos?Pos?Pos (Table 2.c) demonstrates the effectof adding minimal dependency information to aBiLM.9It results in strongly significant improve-ments over the baseline and weak improvementsover Lex?Lex in terms of BLEU.
We additionallyran experiments with the different target functions(Table 3).
?Pos shows the highest results, and ? thelowest ones: this implies that a rather expressivesource syntactic representation alone still benefitsfrom target-side syntactic information.
Below, ourdependency-based systems only use ?Pos.Next, we tested the effect of adding more source9Additional significance testing, which is not shown inTable 2, shows a strongly significant improvement over theoriginal syntactic BiLM Pos?Pos.1695Configuration MT06 MT08 MT06+MT08BLEU TER BLEU TER BLEU TERa PBSMT baseline 31.89 57.79 25.53 60.71 28.99 59.14b Lex?Lex 32.84N57.40N25.91M60.23N29.69N58.72NPos?Pos 32.31N57.89 25.66 60.79 29.28 59.24c Pos?Pos?Pos 32.86N,?57.05N,M26.09N,?59.87N,M29.78N,?58.36N,Nd Pos?Pos?sibl?Pos 32.27M,?56.63N,M25.75 59.47N,N29.30M,?57.95N,Ne Pos?Pos?Pos?Pos 33.09N,?57.54 26.35N,M59.70N,N30.05N,N58.54N,?f Lex?Lex + Pos?Pos?Pos?Pos 33.43N,N57.00N,N26.50N,N59.79N,N30.28N,N58.30N,NTable 4: BLEU and TER scores for Chinese-English PBSMT baseline and BiLM pipelines.
See Table 2for the notation regarding statistical significance.Configuration MT06 MT08 MT06+MT08BLEU TER BLEU TER BLEU TERPos?Pos?
 32.43N,?57.42N,?25.84 60.51 29.43N,?58.86N,?Pos?Pos?Pos 32.86N,?57.05N,M26.09N,?59.87N,M29.78N,?58.36N,NPos?Pos?Lex 32.69N,?57.03N,M25.72 60.17N,?29.52N,?58.49N,MTable 5: Different combinations of a target contextual function with the Pos?Pos source contextual func-tion for Chinese-English.
See Table 2 for the notation regarding statistical significance.dependency information.
Pos?Pos+sibl?Pos (Ta-ble 2.d) only improves over the PBSMT baseline(but also shows weak improvements over Lex?Lexfor TER).
It significantly degrades the perfor-mance with respect to the Pos?Pos?Pos system (Ta-ble 2.c).
Pos?Pos?Pos?Pos (Table 2.e) shows thebest results overall for BLEU, although it must bepointed out that the difference with Pos?Pos?Pos isvery small.
With respect to TER, Pos?Pos?Pos out-performs the grandparent variant.So far, we can conclude that source par-ent information helps improve translation perfor-mance.
Increased specificity of a parent (par-ent specified by a grandparent) tends to furtherimprove performance.
Up to now, we haveonly used syntactic information and obtained con-siderable improvements over Pos?Pos, surpass-ing the improvement provided by Lex?Lex.
Canwe gain further improvements by also addinglexical information?
To this end, we con-duct experiments combining the best performingdependency-based BiLM (Pos?Pos?Pos?Pos) andthe lexicalized BiLM (Lex?Lex).
We hypothesizethat the two models improve different aspects oftranslation: Lex?Lex is biased towards improvinglexical choice and Pos?Pos?Pos?Pos towards im-proving reordering.
Combining these two models,we may improve both aspects.
The metric resultsfor the combined set indeed support this hypothe-sis (Table 2.f).4.3 Chinese-English translation experimentsThe results of the Chinese-English experimentsare shown in Table 4.
In the discussion belowwe mostly focus on the experimental results forthe large, combined test set MT06+MT08.
Weobserve the same general pattern for the Pos?Possource function (Table 4.c) as for Arabic-English:the system with the ?Pos target function has thehighest scores (Table 5).
All of the Pos?Pos?
con-figurations show statistically significant improve-ments over the PBSMT baseline.
For TER, twoof the three Pos?Pos?
variants significantly out-perform Lex?Lex.
The system with sibling in-formation (Table 4.d) obtains quite low BLEUresults, just as in the Arabic experiments.
Onthe other hand, its TER results are the highestoverall.
The system with the Pos?Pos?Pos?Posfunction (Table 4.e) achieves the best resultsamong dependency-based BiLMs for BLEU.
Fi-nally, combining Pos?Pos?Pos?Pos and Lex?Lex re-sults in the largest and significant improvementsover all competing systems for BLEU.4.4 Preliminary analysis of reordering intranslation experimentsIn general, the experimental results show that us-ing source dependency information yields consis-tent improvements for translating from Arabic andChinese into English.
On the other hand, we havepointed out some discrepancies between the twometrics employed, suggesting that different sys-tem configurations may improve different aspects1696Configuration Ar-En Ch-EnMT08 MT09 MT08+MT09 MT06 MT08 MT06+MT08a PBSMT baseline 26.14 29.81 27.88 14.48 10.96 12.89b Lex?Lex 26.33 30.55 28.32 15.43 11.45 13.65Pos?Pos 25.95 30.06 27.89 14.76 11.01 13.07c Pos?Pos?Pos 26.91 31.08 28.87 15.29 11.52 13.60e Pos?Pos?sibl?Pos 26.71 30.73 28.60 15.27 11.67 13.66d Pos?Pos?Pos?Pos 26.78 31.09 28.80 15.42 11.70 13.77f Lex?Lex + Pos?Pos?Pos?Pos 26.80 31.27 28.90 15.87 11.85 14.07Table 6: 4-gram precision scores for Arabic-English and Chinese-English baseline and BiLM systems.Configuration MT08 MT09 MT08+MT09BLEU TER 4gram BLEU TER 4gram BLEU TER 4gramLex?Lex 45.19 47.06 26.41 48.39 44.11 30.23 46.72 45.97 28.21Pos?Pos?Pos?Pos 45.49 47.31M26.66 48.90N43.57N30.92 47.12N45.52N28.66Table 7: BLEU, TER and 4-gram precision scores for Arabic-English Lex?Lex and Pos?Pos?Pos?Poswith a distortion limit of 10.Configuration MT06 MT08 MT06+MT08BLEU TER 4gram BLEU TER 4gram BLEU TER 4gramLex?Lex 33.26 56.81 16.06 25.67 60.19 11.42 29.79 58.38 13.96Pos?Pos?Pos?Pos 33.92N56.29N16.26 27.00N59.58N12.26 30.77N57.82N14.46Table 8: BLEU, TER and 4-gram precision scores for Chinese-English Lex?Lex andPos?Pos?Pos?Pos with a distortion limit of 10.of translation.
To this end, we conducted some ad-ditional evaluations to understand how reorderingis affected by the proposed features.We use 4-gram precision as a metric of howmuch of the reference set word order is preserved.Table 6 shows the corresponding results for bothlanguages.
Just as in the previous two sections,configurations with parental information producethe best results.
For Arabic, all of the depen-dency configurations outperform Lex?Lex.
But thesystem with two feature functions, one of whichis Lex?Lex, still obtains the best results, whichmay suggest that the lexicalized BiLM also helpsto differentiate between word orders.
For Chi-nese, Pos?Pos?Pos?Pos and the system combiningthe latter and Lex?Lex also obtain the best results.However, other dependency-based configurationsdo not outperform Lex?Lex.All the experiments so far were run with a dis-tortion limit of 5.
But both of the languages, es-pecially Chinese, often require reorderings over alonger distance.
We performed additional experi-ments with a distortion limit of 10 for the Lex?Lexand Pos?Pos?Pos?Pos systems (Tables 7 and 8).
Itis more difficult to translate with a higher distor-tion limit (Green et al., 2010) as the set of permu-tations grows larger thereby making it more diffi-cult to differentiate between correct and incorrectcontinuations of the current hypothesis.
It has alsobeen noted that higher distortion limits are morelikely to result in improvements for Chinese ratherthan Arabic to English translation (Chiang, 2007;Green et al., 2010).We compared performance of fixed BiLM mod-els at distortion lengths of 5 and 10.
Arabic-English results did not reveal statistically signif-icant differences between the two distortion lim-its for Pos?Pos?Pos?Pos.
On the other hand, forLex?Lex BLEU decreases when using a distor-tion limit of 10 compared to a limit of 5.
Thisimplies that the dependency BiLM is more ro-bust in the more challenging reordering settingthan the lexicalized BiLM.
Chinese-English re-sults for Pos?Pos?Pos?Pos do show significant im-provements over the distortion limit of 5 (up to0.49 BLEU higher than the best result in Table 4).This indicates that the dependency-based BiLM isbetter capable to take advantage of the increaseddistortion limit and discriminate between correctand incorrect reordering choices.Comparing the results for Pos?Pos?Pos?Pos andLex?Lex at a distortion limit of 10, we obtainstrongly significant improvements for all metrics.For Chinese, a larger distortion limit helps for bothconfigurations, but more so for our dependencyBiLM, yielding an improvement of 0.98 BLEU1697over the original, lexicalized BiLM (Table 8).5 ConclusionsIn this paper, we have introduced a simple, yet ef-fective way to include syntactic information intophrase-based SMT.
Our method consists of en-riching the representation of units of a bilinguallanguage model (BiLM).
We argued that the verylimited contextual information used in the originalbilingual models (Niehues et al., 2011) can capturereorderings only to a limited degree and proposeda method to incorporate information from a sourcedependency tree in bilingual units.
In a seriesof translation experiments we performed a thor-ough comparison between various syntactically-enriched BiLMs and competing models.
The re-sults demonstrated that adding syntactic informa-tion from a source dependency tree to the repre-sentations of bilingual tokens in an n-gram modelcan yield statistically significant improvementsover the competing systems.A number of additional evaluations provided anindication for better modeling of reordering phe-nomena.
The proposed dependency-based BiLMsresulted in an increase in 4-gram precision andprovided further significant improvements overall considered metrics in experiments with an in-creased distortion limit.In this paper, we have focused on rather elemen-tary dependency relations, which we are planningto expand on in future work.
Our current approachis still strictly tied to the number of target tokens.In particular, we are interested in exploring waysto better capture the notion of syntactic cohesionin translation (Fox, 2002; Cherry, 2008) within ourframework.AcknowledgmentsWe thank Arianna Bisazza and the reviewers fortheir useful comments.
This research was fundedin part by the Netherlands Organization for Sci-entific Research (NWO) under project numbers639.022.213 and 612.001.218.ReferencesYaser Al-Onaizan and Kishore Papineni.
2006.
Dis-tortion models for statistical machine translation.
InProceedings of the 21st International Conference onComputational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguis-tics, pages 529?536, Sydney, Australia, July.
Asso-ciation for Computational Linguistics.Alexandra Birch.
2011.
Reordering Metrics for Statis-tical Machine Translation.
Ph.D. thesis, Universityof Edinburgh.Arianna Bisazza and Christof Monz.
2014.
Class-based language modeling for translating into mor-phologically rich languages.
In Proceedings ofthe 25th International Conference on Computa-tional Linguistics (COLING 2014), pages 1918?1927, Dublin, Ireland, August.Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, andChristopher D. Manning.
2009.
Discriminativereordering with chinese grammatical relations fea-tures.
In Proceedings of the Third Workshop on Syn-tax and Structure in Statistical Translation, pages51?59.
Association for Computational Linguistics.Colin Cherry.
2008.
Cohesive phrase-based decodingfor statistical machine translation.
In Proceedingsof Association for Computational Linguistics, pages72?80.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228.David Chiang.
2010.
Learning to translate with sourceand target syntax.
In Proceedings of the 48th An-nual Meeting of the Association for ComputationalLinguistics, pages 1443?1452.
Association for Com-putational Linguistics.Michael Collins.
1999.
Head-Driven Statistical Mod-els for Natural Language Parsing.
Ph.D. thesis,University of Pennsylvania.Josep M. Crego and Franc?ois Yvon.
2010a.
Factoredbilingual n-gram language models for statistical ma-chine translation.
Machine Translation, 24(2):159?175.Josep M. Crego and Franc?ois Yvon.
2010b.
Improv-ing reordering with linguistically informed bilin-gual n-grams.
In Proceedings of the 23rd Inter-national Conference on Computational Linguistics,pages 197?205.
Association for Computational Lin-guistics.Nadir Durrani, Helmut Schmid, and Alexander Fraser.2011.
A joint sequence translation model with in-tegrated reordering.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics, pages 1045?1054.
Association for Com-putational Linguistics.Heidi J.
Fox.
2002.
Phrasal cohesion and statisticalmachine translation.
In Proceedings of the ACL-02 conference on Empirical methods in natural lan-guage processing, pages 304?3111.
Association forComputational Linguistics.1698Michel Galley and Christopher D. Manning.
2008.
Asimple and effective hierarchical phrase reorderingmodel.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing,pages 848?856.
Association for Computational Lin-guistics.Spence Green and Christopher D. Manning.
2010.Better arabic parsing: Baselines, evaluations, andanalysis.
In Proceedings of the 23rd InternationalConference on Computational Linguistics, pages394?402.
Association for Computational Linguis-tics.Spence Green, Michel Galley, and Christopher D. Man-ning.
2010.
Improved models of distortion costfor statistical machine translation.
In Proceedingsof the 2010 Annual Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics, pages 867?875.
Association for Com-putational Linguistics.Mark Hopkins and Jonathan May.
2011.
Tuning asranking.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing,pages 1352?1362.
Association for ComputationalLinguistics.Liang Huang and Haitao Mi.
2010.
Efficient incre-mental decoding for tree-to-string translation.
InProceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, pages273?283.
Association for Computational Linguis-tics.Liang Huang, Kevin Knight, and Aravind Joshi.
2006.Statistical syntax-directed translation with extendeddomain of locality.
In Proceedings of AMTA, pages223?226.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of the 2003 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics on Human Language Technology, pages48?54.
Association for Computational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, et al.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Pro-ceedings of the 45th Annual Meeting of the Associ-ation for Computational Linguistics on InteractivePoster and Demonstration Sessions, pages 177?180.Association for Computational Linguistics.Uri Lerner and Slav Petrov.
2013.
Source-side classi-fier preordering for machine translation.
In Proceed-ings of the Empirical Methods in Natural LanguageProcessing.Junhui Li, Zhaopeng Tu, Guodong Zhou, and Josef vanGenabith.
2012.
Head-driven hierarchical phrase-based translation.
In Proceedings of the 50th An-nual Meeting of the Association for ComputationalLinguistics, pages 33?37.
Association for Computa-tional Linguistics.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machinetranslation.
In Proceedings of the 21st InternationalConference on Computational Linguistics and the44th annual meeting of the Association for Compu-tational Linguistics, pages 609?616.
Association forComputational Linguistics.Jos?e B Marino, Rafael E Banchs, Josep M. Crego,Adria de Gispert, Patrik Lambert, Jos?e A.R.
Fonol-losa, and Marta R. Costa-Juss`a.
2006.
N-gram-based machine translation.
Computational Linguis-tics, 32(4):527?549.Yuval Marton and Philip Resnik.
2008.
Soft syntac-tic constraints for hierarchical phrased-based trans-lation.
In Proceedings of the Association for Com-putational Linguistics, pages 1003?1011.Jan Niehues, Teresa Herrmann, Stephan Vogel, andAlex Waibel.
2011.
Wider context by using bilin-gual language models in machine translation.
InProceedings of the Sixth Workshop on StatisticalMachine Translation, pages 198?206.
Associationfor Computational Linguistics.Eric W. Noreen.
1989.
Computer Intensive Meth-ods for Testing Hypotheses.
An Introduction.
Wiley-Interscience.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of the40th annual meeting of the Association for Compu-tational Linguistics, pages 311?318.
Association forComputational Linguistics.Stefan Riezler and John T. Maxwell.
2005.
On somepitfalls in automatic evaluation and significance test-ing for MT.
In Proceedings of the ACL Workshop onIntrinsic and Extrinsic Evaluation Measures for Ma-chine Translation and/or Summarization.Libin Shen, Jinxi Xu, and Ralph M. Weischedel.
2008.A new string-to-dependency machine translation al-gorithm with a target dependency language model.In Proceedings of the Association for ComputationalLinguistics, pages 577?585.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of AMTA, pages 223?231.Andreas Stolcke, Jing Zheng, Wen Wang, and VictorAbrash.
2011.
Srilm at sixteen: Update and out-look.
In Proceedings of IEEE Automatic SpeechRecognition and Understanding Workshop, page 5.1699Christoph Tillmann.
2004.
A unigram orientationmodel for statistical machine translation.
In Pro-ceedings of of the North American Chapter of theAssociation for Computational Linguistics, pages101?104.
Association for Computational Linguis-tics.Kristina Toutanova, Dan Klein, Christopher D. Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology,pages 173?180.
Association for Computational Lin-guistics.Richard Zens and Hermann Ney.
2003.
A comparativestudy on reordering constraints in statistical machinetranslation.
In Proceedings of the 41st Annual Meet-ing on Association for Computational Linguistics-Volume 1, pages 144?151.
Association for Compu-tational Linguistics.1700
