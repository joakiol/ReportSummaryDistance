PARSING THE LOB CORPUSCarl G. de MarckenMIT AI Laboratory Room 838545 Technology SquareCambridge, MA 02142Internet: cgdemarc@ai.mit.eduABSTRACTThis paper 1presents a rapid and robust pars-ing system currently used to learn from largebodies of unedited text.
The system contains amultivalued part-of-speech disambiguator anda novel parser employing bottom-up recogni-tion to find the constituent phrases of largerstructures that might be too difficult to ana-lyze.
The results of applying the disambiguatorand parser to large sections of the Lancaster/Oslo-Bergen corpus are presented.INTRODUCTIONWe have implemented and tested a pars-ing system which is rapid and robust enoughto apply to large bodies of unedited text.
Wehave used our system to gather data from theLancaster/Oslo-Bergen (LOB) corpus, generat-ing parses which conform to a version of currentGovernment-Binding theory, and aim to use thesystem to parse 25 million words of textThe system consists of an interface to theLOB corpus, a part of speech disambiguator,and a novel parser.
The disambiguator usesmultivaluedness to perform, in conjunction withthe parser, substantially more accurately thancurrent algorithms.
The parser employs bottom-up recognition to create rules which fire top-down, enabling it to rapidly parse the constituentphrases of a larger structure that might itself bedifficult to analyze.
The complexity of some ofthe free text in the LOB demands this, and wehave not sought o parse sentences completely,but rather to ensure that our parses are accu-rate.
The parser output can be modified to con-form to any of a number of linguistic theories.This paper is divided into sections discussingthe LOB corpus, statistical disambiguation, theparser, and our results.1 This paper reports work done at the MITArtificial Intelligence Laboratory.
Support forthis research was provided in part by grantsfrom the National Science Foundation (under aPresidential Young Investigator award to Prof.Robert C. Berwick); the Kapor Family Foun-dation; and the Siemens Corporation.THE LOB CORPUSThe Lancaster/Oslo-Bergen Corpus is an on-line collection of more than 1,000,000 words ofEnglish text taken from a variety of sources,broken up into sentences which are often 50 ormore words long.
Approximately 40,000 differ-ent words and 50,000 sentences appear in thecorpus.We have used the LOB corpus in a standardway to build several statistical tables of part ofspeech usage.
Foremost is a dictionary keyingevery word found in the corpus to the numberof times it is used as a certain part of speech,which a/lows us to compute the probability thata word takes on a given part of speech.
In ad-dition, we recorded the number of times eachpart of speech occurred in the corpus, and builta digram array, listing the number of timesone part of speech was followed by another.These numbers can be used to compute theprobability of one category preceding another.Some disambiguation schemes require knowingthe number of trigram occurrences (three spe-cific categories in a row).
Unfortunately, witha 132 category system and only one millionwords of tagged text, the statistical accuracy ofLOB trigrams would be minima/.
Indeed, evenin the digram table we have built, fewer than3100 of the 17,500 digrams occur more than 10times.
When using the digram table in statisti-ca/schemes, we treat each of the 10,500 digramswhich never occur as if they occur once.STAT IST ICAL  D ISAMBIGUATIONMany different schemes have been proposedto disambiguate word categories before or dur-ing parsing.
One common style of disambigua-tots, detailed in this paper, rely on statisticalcooccurance information such as that discussedin the section above.
Specific statistical disam-biguators are described in both DeRose 1988and Church 1988.
They can be thought of asalgorithms which maximize a function over thepossible selections of categories.
For instance,for each word A-" in a sentence, the DeRose al-gorithm takes a set of categories {a~, a~,...} asinput.
It outputs a particular category a~z such243that the product of the probability that A: isthe category a~, and the probability that thecategory a~.. occurs before the category az+l isi .z+lmaximized.
Although such an algorithm mightseem to be exponential in sentence length sincethere are an exponential number of combina-tions of categories, its limited leftward and right-ward dependencies permit linear time dynamicprogramming method.
Applying his algorithmto the Brown Corpus 2, DeRose claims the ac-curacy rate of 96%.
Throughout his paper wewill present accuracy figures in terms of how of-ten words are incorrectly disambiguated.
Thus,we write 96% correctness as an accuracy of 25(words per error).We have applied the DeRose scheme andseveral variations to the LOB corpus in orderto find an optimal disambiguation method, anddisplay our findings below in Figure 1.
First,we describe the four functions we maximize:Method A: Method A is also described inthe DeRose paper.
It maximizes the productof the probabilities of each category occurringbefore the next, orn- -1I IP (a~zis-flwd-by a'~+l ) 1z=lMethod B: Method B is the other half ofthe Dettose scheme, maximizing the product ofthe probabilities of each category occurring forits word.
Method B simply selects each word'smost probable category, regardless of context.nH P ( Azis-cat aZz)z----1Method C" The DeRose scheme, or themaximum ofn n -1IT P ( A~is-cat a~,) l'-I P (a~ is-flwd-by a~?
:~)z=l  z= lMethod D: No statistical disambiguatorcan perform perfectly if it only returns one partof speech per word, because there are words andsequences of words which can be truly ambigu-ous in certain contexts.
Method D addressesthis problem by on occasion returning morethan one category per word.The DeRose algorithm moves from left toright assigning to each category a~ an optimalpath of categories leading from the start of thesentence to a~, and a corresponding probability.2 The Brown Corpus is a large, tagged textdatabase quite similar to the LOB.It then extends each path with the categories ofthe word A -'+1 and computes new probabilitiesfor the new paths.
Call the greatest new prob-ability P. Method D assigns to the word A zthose categories {a~} which occur in those newpaths which have a probability within a factorF of P. It remains a linear time algorithm.Naturally, Method D will return several cat-egories for some words, and only one for others,depending on the particular sentence and thefactor F. If F = 1, Method D will return onlyone category per word, but they are not nec-essarily the same categories as DeRose wouldreturn.
A more obvious variation of DeRose,in which alternate categories are substitutedinto the DeRose disambiguation and acceptedif they do not reduce the overall disambigua-tion probability significantly, would approachDeRose as F went to 1, but turns out not toperform as well as Method D. 3D isambiguator  Resu l ts :  Each methodwas applied to the same 64,000 words of theLOB corpus.
The results were compared to theLOB part of speech pre-tags, and are listed inFigure 1.
4 If a word was pre-tagged as beinga proper noun, the proper noun category wasincluded in the dictionary, but no special infor-mation such as capitalization was used to dis-tinguish that category from others during dis-ambiguation.
For that reason, when judgingaccuracy, we provide two metrics: one simplycomparing disambiguator utput with the pre-tags, and another that gives the disambiguatorthe benefit of the doubt on proper nouns, underthe assumption that an "oracle" pre-processorcould distinguish proper nouns from contextualor capitalization i formation.
Since Method Dcan return several categories for each word, weprovide the average number of categories perword returned, and we also note the setting ofthe parameter F,  which determines how manycategories, on average, are returned.The numbers in Figure 1 show that sim-ple statistical schemes can accurately disam-biguate parts of speech in normal text, con-firming DeRose and others.
The extraordinary3 To be more precise, for a given averagenumber of parts of speech returned V, the "sub-stitution" method is about 10% less accuratewhen 1 < V < 1.1 and is almost 50% less ac-curate for 1.1 < V < 1.2.4 In all figures quoted, punctuation markshave been counted as words, and aretreated as parts of speech by the statisticaldisambiguators.244Method: A B C D(1)D(.3)Accuracy: 7.9 17 23 25 41with oracle: 8.8 18 30 31 54of Cats: 1 1 1 1 1.04Method: D(.1) D(.03) D(.01) D(.003)Accuracy: 70 126 265 1340with oracle: 105 230 575 1840No.
of Cats: 1.09 1.14 1.20 1.27Figure 1: Accuracy of various disambiguationstrategies, in number of words per error.
Onaverage, the dictionary had 2.2 parts of speechlisted per word.accuracy one can achieve by accepting an ad-ditional category every several words indicatesthat disambiguators can predict when their an-swers are unreliable.Readers may worry about correlation result-ing from using the same corpus to both learnfrom and disambiguate.
We have run tests byfirst learning from half of the LOB (600,000words) and then disambiguating 80,000 wordsof random text from the other half.
The ac-curacy figures varied by less than 5% from theones we present, which, given the size of theLOB, is to be expected.
We have also appliedeach disambiguation method to several smaller(13,000 word) sets of sentences which were se-lected at complete random from throughout theLOB.
Accuracy varied both up and down fromthe figures we present, by up to 20% in terms ofwords per error, but relative accuracy betweenmethods remained constant.The fact the Method D with F = 1 (withF = 1 Method D returns only one category perword) performs as well or even better on theLOB than DeKose's algorithm indicates that,with exceptions, disambiguation has very lim-ited rightward ependence: Method D employsa one category lookahead, whereas DeRose'slooks to the end of the sentence.
This sug-gests that Church's strategy of using trigramsinstead of digrams may be wasteful.
Churchmanages to achieve results similar or slightlybetter than DeRose's by defining the probabil-ity that a category A appears in a sequenceABC to be the number of times the sequenceABC appears divided by the number of timesthe sequence BC appears.
In a 100 categorysystem, this scheme requires an enormous ta-ble of data, which must be culled from taggedtext.
If the rightward dependence of disam-biguation is small, as the data suggests, thenthe extra effort may be for naught.
Based onour results, it is more efficient o use digramsin genera\] and only mark special cases for tri-grams, which would reduce space and learningrequirements substantially.Integrat ing D isambiguator  and Parser:As the LOB corpus is pretagged, we could ig-nore disambiguation problems altogether, butto guarantee that our system can be applied toarbitrary texts, we have integrated a variationof disambiguation Method D with our parser.When a sentence is parsed, the parser is ini-tially passed all categories returned by MethodD with F = .01.
The disambiguator substan-tially reduces the time and space the parserneeds for a given parse, and increases the parser'saccuracy.
The parser introduces yntactic on-straints that perform the remaining disambigua-tion well.THE PARSERIntroduct ion:  The LOB corpus containsunedited English, some of which is quite com-plex and some of which is ungrammatical.
Noknown parser could produce full parses of allthe material, and even one powerful enough todo so would undoubtably take an impracticallength of time.
To facilitate the analysis ofthe LOB, we have implemented a simple parserwhich is capable of rapidly parsing simple con-structs and of "failing gracefully" in more com-plicated situations.
By trading completenessfor accuracy, and by utilizing the statistical dis-ambiguator, the parser can perform rapidly andcorrectly enough to usefully parse the entireLOB in a few hours.
Figure 2 presents a sampleparse from the LOB.The parser employs three methods to buildphrases.
CFG-like rules are used to recognizelengthy, less structured constructions such asNPs, names, dates, and verb systems.
Neigh-boring phrases can connect o build the higherlevel binary-branching structure found in En-glish, and single phrases can be projected intonew ones.
The ability of neighboring phrasepairs to initiate the CFG-like rules permits context-sensitive parsing.
And, to increase the effi-ciency of the parser, an innovative system ofdeterministically discarding certain phrases isused, called "lowering".Some Parser Detai ls:  Each word in aninput sentence is tagged as starting and end-ing at a specific numerical location.
In thesentence "I saw Mary."
the parser would in-sert the locations 0-4, 0 I 1 SAW 2 MARY 3245MR MICHAEL FOOT HAS PUT DOWN A RESOLUTION ON THESUBJECT AND HE IS TO BE HACKED BY ME WILLGHIFFITHS , PIP FOR MANCHESTER EXCHANGE .> (IP(NP (PROP (N MR) (NAME MICHAEL) (NAME FOOT)))(I-EAR (I (HAVE HAS) (RP DOWN))(VP (V PUT) (NP (DET A) (N RESOLUTION)))))> (PP (P ON) (NP (DET THE) (N SUBJECT)))> (CC AND)> (IP (NP HE)(I-BAR (I)(VP (IS IS)(I-BAR (I (PP (P BY) (NP (PROP (N MR)(NAME WILL) (NAME GRIFFITNS)))))(TO TO) (IS BE)) (VP (V BACKED))))))> (*CMA ",")> (NP (N MP))> (PP (P FOR) (NP (PROP (NAME MANCHESTER)(NAME EXCHANGE) ) ) )> (*PER ".
")Figure 2: The parse of a sentence taken ver-bat im from the LOB corpus, printed withoutfeatures.
Notice that the grammar does not at-tach PP adjuncts.4.
A phrase consists of a category, startingand ending locations, and a collection of fea-ture and tree information.
A verb phrase ex-tending from 1 to 3 would print as \[VP 1 3\].Rules consist of a state name and a location.If a verb phrase recognition rule was firing inlocation 1, it would get printed as (VP0 a*1) where VP0 is the name of the rule state.Phrases and rules which have yet to be pro-cessed are placed on a queue.
At parse initial-ization, phrases are created from each word andits category(ies), and placed on the queue alongwith an end-of-sentence marker.
The parse pro-ceeds by popping the top rule or phrase off thequeue and performing actions on it.
Figure 3contains a detailed specification of the parseralgorithm, along with parts of a grammar.
Itshould be comprehensible after the followingoverview and parse example.When a phrase is popped off the queue, rulesare checked to see if they fire on it, a tableis examined to see if the phrase automaticallyprojects to another phrase or creates a rule,and neighboring phrases are examined in casethey can pair with the popped phrase to ei-ther connect into a new phrase or create a rule.Thus the grammar consists of three tables, the"rule-action-table" which specifies what actiona rule in a certain state should take if it en-counters a phrase with a given category andfeatures; a "single-phrase-action-table" whichspecifies whether a phrase with a given categoryand features hould project or start a rule; anda "paired-phrase-action-table" which specifiespossible actions to take if two certain phrasesabut each other.For a rule to fire on a phrase, the rule mustbe at the starting position of the phrase.
Pos-sible actions that can be taken by the rule are:accepting the phrase (shift the dot in the rule);closing, or creating a phrase from all phrasesaccepted so far; or both, creating a phrase andcontinuing the rule to recognize a larger phraseshould it exist.
Interestingly, when an enqueuedphrase is accepted, it is "lowered" to the bot-tom of the queue, and when a rule closes tocreate a phrase, all other phrases it may havealready created are lowered also.As phrases are created, a call is made toa set of transducer functions which generatemore principled interpretations of the phrases,with appropriate features and tree relations.The representations they build are only for out-put, and do not affect the parse.
An exceptionis made to allow the functions to project andmodify features, which eases handling of sub-categorization and agreement.
The transduc-ers can be used to generate a constant outputsyntax as the internal grammar varies, and v iceversa .New phrases and rules are placed on thequeue only after all actions resulting from agiven pop of the queue have been taken.
Theordering of their placement has a dramatic ef-fect on how the parse proceeds.
By varyingthe queuing placement and the definition ofwhen a parse is finished, the efficiency and ac-curacy of the parser can be radically altered.The parser orders these new rules and phrasesby placing rules first, and then pushes all ofthem onto the stack.
This means that newrules will always have precedence over newlycreated phrases, and hence will fire in a succes-sive "rule chain".
If all items were eventuallypopped off the stack, the ordering would be ir-relevant.
However, since the parse is stopped atthe end-of-sentence marker, all phrases whichhave been "lowered" past the marker are neverexamined.
The part of speech disambiguatorcan pass in several categories for any one word,which are ordered on the stack by likelihood,most probable first.
When any lexical phraseis lowered to the back of the queue (presum-ably because it was accepted by some rule) allother lexical phrases associated with the sameword are also lowered.
We have found that thisboth speeds up parsing and increases accuracy.That this speeds up parsing should be obvi-ous.
That it increases accuracy is much less so.Remember that disambiguation Method D is246The Parser AlgorithmTo parse  a sentences  S o f  length  n:Perform multivalued isambiguation f S.Create empty queue Q.
Place End-of-Sentence marker on Q.Create new phrases from disambiguator utput categories,and place them on Q.Until Q is elnpty, or top(Q) = End-of-Sentence marker.Let I=  pop(Q).
Let new-items = nilIf I ts  phrase \[cat i 3\]Let rules = all rules at location i.Let lefts = all phrases ending at.
location i.Lel rights = all-phrases starting a.t location j.Perform ru le -ac t ions( ru les , i f} )Perform paired-phrase-actions(lefts,{\]})Perform pa i red-phrase-act ions({\ ]} ,  rights)Perforin single-phrase-actions (D.I f / i s  rule (state at i)Let phrases = all phrasess{arting alt location i.Perforin rule-actions ({\]} ,phrases).Place each item in new-items on Q, rules first.Let i = 0.
Until i = n,Output longest phrase \[cat i 3\].
Let, i = j.To per fo rm rule-actions (rules ,phrases):For all rules R = (state at i) in rules,And all phrases P = \[cat+features i 3\] in phrases,If there is an action A in the ru le -ac t ion - tab le  with key(state, cat+features),If A = (accept  new-state) or (aeespt -and-c lose  new-state new-cat).Create new rule (new-state at j).If A = (c lose  new-cat) or (aeeept -a r td -c lose  new-state new-cat).Let daughters = the set of all phrases which have beenaccepted in the rule chain which led to R, includingthe phrase P.Let l = the lef|mosl starting location of all)' phrasein daughters.
Create new phrase \[new-cat l 3\] wilhdaughters daughters.For all phrases p in daughters, perform lowsr (p).For all phrases p created (via accept -and-c lose)  bythe rule chair, which led to R. perform lower(p).To per form paired-phrase-actions (lefts, rights):For all phrases Pl = \[left-cat+features l if in lefts,And all phrases Pr = \[right-cat+features i r\] in rights,If there is an action A in the pa i red-phrase-act ion -tab le  with key (left-cat+features, right-cat+featureS).If A = (cormect new-caD,Create new phrase \[new-cat I r\] with daughters Pl andPr.If A = (project  new-cat).Create new phrase \[new-catir\] with (laughter Pr.If A = (stext-new-rule state).Create new rule (state at i).Perform Iower(Pl) and lower(Pr).To per fo rm s ing le -phrase-act ions  ( \[cat+features i 3"\] ) :If there is an action A in the single-phrase-action-tablewith key cat+features.If A = (project new-cat).Create new phrase \[new-cat i 3\].If A = ( s ta r t - ru le  new-state).C_'reate new rule (state at i).To per fo rm lower ( / ) :If I ts  in Q, renmve iT from Q and reiw, erl il at end of Q.If I is a le?ical evel phrase \[cat i i+1\] created from the dis-ambiguator outpnl categoric.,,.For all other lexical level phrases p starting a I i .
pertbrmlo~er (p).When c reat ing  a new ru le  R:Add R to list of new-items.When c reat ing  a new phrase  P = \[cat+features i .7\] w i thdaughters  D:Add P to list of new-items.If there is a hook function F in the hook- f tmct ion-tablewith key' cat+features, perform F(P,D).
Hook fnnctious canadd features to P.A sect ion  o f  a ru le -ac t ion - tab le .Key(State.
(:'at) ActionDET0, DETDET1, JJDET1, N +plDET1.
NJ J0.
JJVP1, ADV(accept DET1 )(accept DET1 )(close NP)(accept-and-close DET2 NP)(accept-and-close J J0 AP)(accept.
VP1)A sect ion  o f  a paired-phrase-action-table.Key(Cat.
Cat ) ActionCOMP.
S (connect CP)NP +poss, NP (connect NP)NP.
S (project CP)NP, \:P exl-np +tense xpect-nil (collnecl S)NP, CMA* (start-rule < ',\IA0)VP expect-pp.
PP (connect VP)A sect ion  o f  a s ing le -phrase-act ion - tab le .Key(Cat ) Aclion K<v ActionDET+pro  (start-rule DET0) PRO (lu'ojecl NP)(pro.iect NP) V (start-rule vPa}N (start-rule DErII) IS (start-rule \ 'P l )NAME (start-rule NMI) (stuN-rule ISQ\] )A sect ion  o f  a hook- f tmct ion - tab le .Key(Cat ) Hook Function\"P Get-Subcat egoriz at ion-I nfoS Check-AgreenlentCP ('heck-Coml>St ruct ureFigure 3: A pseudo-code representation f the parser algo-rithm, omitt ing implementation details.
Included in tableform are representative s ctions from a grammar.247substantially more accurate the DeRose~s algo-r i thm only because it can return more than onecategory per word.
One might guess that if theparser were to lower all extra categories on thequeue, that nothing would have been gained.But the top-down nature of the parser is suf-ficient in most cases to "pick out" the correctcategory from the several available (see Milne1988 for a detailed exposition of this).A Parse  in Detai l :  Figure 4 shows aparse of the sentence "The pastry chef placedthe pie in the oven."
In the figure, items tothe left of the vertical ine are the phrases andrules popped off the stack.
To the right of eachitem is a list of all new items created as a resultof it being popped.
At the start of the parse,phrases were created from each word and theircorresponding categories, which were correctly(and uniquely)determined by the disambigua-tor.The first item is popped off the queue, thisbeing the \[DET 0 1\] phrase corresponding tothe word "the".
The single-phrase action ta-ble indicates that a DET0 rule should be startedat location 0 and immediately fires on "the",which is accepted and the rule (DET1 a* 1) isaccordingly created and placed on the queue.This rule is then popped off the queue, and ac-cepts the \[N 1 2\] corresponding to "pastry",also closing and creating the phrase \[NP 0 2\].When this phrase is created, all queued phraseswhich contributed to it are lowered in priority,i.e., "pastry".
The rule (DET2 at 2) is cre-ated to recognize a possibly longer NP, and ispopped off the queue in line 4.
Here much thesame thing happens as in line 3, except thatthe \[NP 0 2\] previously created is lowered asthe phrase \[NP 0 3\] is created.
In line 5, therule chain keeps firing, but there are no phrasesstarting at location 3 which can be used by therule state DET2.The next item on the queue is the newlycreated \[NP 0 3\], but it neither fires a rule(which would have to be in location 0), findsany action in the single-phrase table, or pairswith any neighboring phrase to fire an actionin the paired-phrase table, so no new phrasesor rules are created.
Hence, the verb "placed"is popped and the single-phrase table indicatesthat it should create a rule which then immedi-ately accepts "placed", creating a VP and plac-ing the rule (VP4 a* 4) in location 4.
The VPis popped off the stack, but not attached to \[NP0 3\] to form a sentence, because the paired-phrase table specifies that for those two phrasesto connect to become an S, the verb phrasemust have the feature (expec't; nil), indi-0 The 1 pastry 2 chef 3 placed 4 the 5 pie 6 in?
the 8 oven 9 .
I0I.
Phrase \[DET 0 I\]2.
Rule (DETO at O)3.
Rule (DETI at I)4.
Rule (DET2 at  2)5.
Rule (DET2 at 3)6.
Phrase \[NP 0 3\]7.
Phrase \[V 3 4\]8.
Rule (VP3 at 3)9.
Rule (UP4 at 4)I0.
Phrase \[VP 3 4\]11.
Phrase \[DET 4 5\]12.
Phrase (DETO at 4)13.
Rule (DETI at 5)14.
Rule (DET2 at 6)15.
Phrase \[NP 4 6\]16.
Phrase \[VP 3 6\]17.
Phrase IS 0 6\]18.
Phrase \[P 6 7\]19.
Phrase  \[DET 7 8\]20.
Rule (DETO at 7)21.
Rule (DETI at 8)22.
Rule (DET2 at 9)23.
Phrase \[NP 7 9\]24.
Phrase \[PP 6 9\]25.
Phrase \[*PER 9 I0\](DETO at O)(DETI at I)\[NP 0 2\] (DETI at  2)Lowering: \[N 1 2\]\[NP 0 3\] (DET2 at 3)Lowering: \[NP 0 2\]Lowering: IN 2 3\](VP3 at  3)\[VP 3 4\] (VP4 a t  4)(DETO at 4)(DETI at 5)\[NP 4 6\] (DET2 at 6)Lowering: IN 5 6\]\[VP 3 6\]Is 0 6\](DETO at 7)(DETI at 8)\[NP 7 9\] (DET2 at 9)Lowering: \[N 8 9\]\[PP 6 9\]> (IP (NP (DET "The") (N "pastry") (N "chef"))(I-BAR (I) (UP (V "placed")(NP (DET "the") (N "pie")))))> (PP (P "in") (NP (DET "the") (N "oven")))> (*PER ".
")Phrases left on Queue: \[N I 2\] IN 2 3\] \[NP 0 2\]IN s 6\] IN 8 9\]Figure 3: A detailed parse of the sentence"The pastry chef placed the pie in the oven".Dictionary look-up and disambiguation wereperformed prior to the parse.cating that all of its argument positions havebeen filled.
However when the VP was cre-ated, the VP transducer call gave it the feature(expect  .
NP), indicating that it is lacking anNP argument.In line 15, such an argument is popped fromthe stack and pairs with the VP as specified inthe paired-phrase table, creating a new phrase,\[VP 3 6\].
This new VP then pairs with thesubject, forming \[S 0 6\].
In line 18, the prepo-sition "in" is popped, but it does not create anyrules or phrases.
Only when the NP "the oven"is popped does it pair to create \[PP 6 9\].
Al-though it should be attached as an argument248to the verb, the subcategorization frames (con-tained in the expoc'c feature of the VP) do notallow for a prepositional phrase argument.
Af-ter the period is popped in line 25, the end-of-sentence marker is popped and the parse stops.At this time, 5 phrases have been lowered andremain on the queue.
To choose which phrasesto output, the parser picks the longest phrasestarting at location 0, and then the longestphrase starting where the first ended, etc.The Reasoning behind the Details: Theparser has a number of salient features to it, in-cluding the combination of top-down and bottom-up methods, the use of transducer functions tocreate tree structure, and the system of lower-ing phrases off the queue.
Each was necessaryto achieve sufficient flexibility and efficiency toparse the LOB corpus.As we have mentioned, it would be naive ofus to believe that we could completely parse themore difficult sentences in the corpus.
The nextbest thing is to recognize smaller phrases inthese sentences.
This requires some bottom-upcapacity, which the parser achieves through thesingle-phrase and paired-phrase action tables.In order to avoid overgeneration f phrases, therules (in conjunction with the "lowering" sys-tem and method of selecting output phrases)provide a top-down capability which can pre-vent some valid smaller phrases from being built.Although this can stifle some correct parses 5wehave not found it to do so often.Keaders may notice that the use of specialmechanisms to project single phrases and toconnect neighboring phrases is unnecessary, sincerules could perform the same task.
However,since projection and binary attachment are socommon, the parser's efficiency is greatly im-proved by the additional methods.The choice of transducer functions to createtree structure has roots in our previous expe-riences with principle-based structures.
Mod-ern linguistic theories have shown themselvesto be valuable constraint systems when appliedto sentence tree-structure, but do not necessar-ily provide efficient means of initially generat-ing the structure.
By using transducers to mapFor instance, the parser always generatesthe longest possible phrase it can from a se-quence of words, a heuristic which can in somecases fail.
We have found that the only situ-ation in which this heuristic fails regularly isin verb argument attachment; with a more re-strictive subcategorization system, it would notbe much of a problem.between surface structure and more principledtrees, we have eliminated much of the compu-tational cost involved in principled representa-tions.The mechanism of lowering phrases off thestack is also intended to reduce computationalcost, by introducing determinism into the parser.The effectiveness of the method can be seenin the tables of Figure 5, which compare theparser's speed with and without lowering.RESULTSWe have used the parser, both with andwithout the lexical disambiguator, to analyzelarge portions of the LOB corpus.
Our gram-mar is small; the three primary tables have atotal of 134 actions, and the transducer func-tions are restricted to (outside of building treestructure) projecting categories from daughterphrases upward, checking agreement and case,and dealing with verb subcategorization fea-tures.
Verb subcategorization i formation isobtained from the Oxford Advanced Learner'sDictionary of Contemporary English (Hornbyet al1973), which often includes unusual verbaspects, and consequently the parser tends toaccept too many verb arguments.The parser identifies phrase boundaries ur-prisingly well, and usually builds structures upto the point of major sentence breaks such ascommas or conjunctions.
Disambiguation fail-ure is almost nonexistent.
At the end of this pa-per is a sequence of parses of sentences from thecorpus.
The parses illustrate the need for a bet-ter subcategorization system and some methodfor dealing with conjunctions and parentheti-cals, which tend to break up sentences.Figure 5 presents ome plots of parser speedon a random 624 sentence subset of the LOB,and compares parser performance with and with-out lowering, and with and without disambigua-tion.
Graphs 1 and 2 (2 is a zoom of 1) illustratethe speed of the parser, and Graph 3 plots thenumber of phrases the parser returns for a sen-tence of a given length, which is a measure ofhow much coverage the grammar has and howmuch the parser accomplishes.
Graph 4 plotsthe number of phrases the parser builds duringan entire parse, a good measure of the workit performs.
Not surprisingly, there is a verysmooth curve relating the number of phrasesbuilt and parse time.
Graphs 5 and 6 are in-cluded to show the necessity of disambiguationand lowering, and indicate a substantial reduc-tion in speed if either is absent.
There is also asubstantial reduction in accuracy.
In the no dis-ambiguation case, the parser is passed all cate-249(seconds)20181614 ?12 o ?
?m10 m ?
m8 ?
\[\] m-6  ?
???
o\[\] ?
~D ?
o m- -A  oa \ [ \ ] l~  ?0  00  I~ g 0?
u \[\] ?0  o-2"f i IGraph 1: # of words in sentencet (seconds)-4  o \[\] ?m-3 .5  " ??
o \[\]o ?
"3  \[\]o go  o \[\] ?o  o0?
o?
B?\[\] ?
.= ?o o ?
?
?m ma aag "2 .5  ~?
?
= ?
=?
??
DO IO -OB ?02 0.  o oOoo%=.\[\] \[\] \[\] Dm \[~ as  ?
=S ?
?
O0~?O?H I  \[\] ?
I ?
?
1.5 00 _ e - ?o  ?a ?
2 R?
OaOH= oB 0?4 o ??u?
| ?
B=HBB ?age= =?/ Bm?
a = a?
? '
, .
?B?m' !
inU ' , |o  ?o?%?
? "
"?B  B" =?
.
?
, ,hU l l , , , ?
?
?)
3,o 3; ,,o 45IGraph 2: # of words in sentenceof phrases returned- 30o??m?
o?7OI0O5OI-25o?-20  ?
?
\[\] ??
?o \[\]15 " ~ o ? "=o o ~ =.
?.
.=?.
.=.
.= ?
?
\[\]?
o ?
.
.
.
.
.
.
.
?moo ?
mm ?= =,==== .= =-o ~,,~ \[\] 10 .
.
.
.
.
.o ?D .
.
.
.
.
.
.
.
.
.
?o~m0 m m?
~  ==%===~?=~=.
%-5  ~ ............... mI = o~= $0 40  50 60 70  80. .
.
.
.
.
.
.
.
~ .
.
.
.
.
?
I a i o I I I I IGraph 3: # of words in sentenceFigure 4: Performance graphs of parser onsubset of LOB.
See text for explanations.of phrases built- 20018O160140- 120  =o ?
\ [ \ ]a?
?a ?
\[\]=== = =?
Do ?
o ?
\[\] m?-100/~- -  ?=a aa~ \[\] aaa R a~u o , , ,B  =?
_E~6.
~ ~,,0  ooGraph 4: ~ o/words in sentence(seconds)60 =o50 =70I40 \[\]\[\] \[\] m ?30 ?
?
mo o ?u?
| o o 20 ??
??
?
\[\],0 ?
?,: :;:oi ?
:?
'.== o ???
?H I ; IgBg=??oo~?
?, oN , |8111B '  I"  2C~ 30 40  50f 1 IGraph 5\[No Dis.\]: # of words in sentence(seconds)-60  ??
?- 50  ?60I-40 ?
ma O o o-30  ?
?
mm 0 a\[\] O D-20  = \[\]= = ?B =?
~ ~ ??
??o?
D ?
?
o o o aD Oa Q ?
ooo  ?
B ?
"10  a .
= ?%?= =?
= ?= e ?"
= ?
og 01~ -=  ? "
a ? "
B 0 ?
oo .
?
; _l===?
?gliil ailBgaal , l eO=aS e 50  60I IGraph 6\[No Lowering\]: # of words in sentenceFigure 5: Performance graphs of parser onsubset of LOB.
See text for explanations.gories every word can take, in random order.Parser accuracy is a difficult statistic to mea-sure.
We have carefully analyzed the parses?
assigned to many hundreds of LOB sentences,and are quite pleased with the results.
A1-though there are many sentences where the parseris unable to build substantial structure, it rarelybuilds incorrect phrases.
A pointed exceptionis the propensity for verbs to take too manyarguments.
To get a feel for the parser's ac-250curacy, examine the Appendix, which containsunedited parses from the LOB.BIBL IOGRAPHYChurch, K. W.  1988 A Stochastic Parts Pro-gram and Noun Phrase Parser for UnrestrictedText.
Proceedings of the Second Conference onApplied Natural Language Processing, 136-143DeRose, S. J.
1988 Grammatical CategoryDisambiguation by Statistical Optimization.
Com-putational Linguistics 14:31-39Oxford Advanced Learner's Dictionary of Con-temporary English, eds.
Hornby, A.S., and Covie,A.
P. (Oxford University Press, 1973)Milne, 1%.
Lexical Ambiguity Resolution i  aDeterministic Parser, in Le~.icaI Ambiguity Res-olution, ed.
by S. Small et al(Morgan Kauf-mann, 1988)APPENDIX:  Sample  ParsesThe following are several sentences from thebeginning of the LOB,  parsed with our system.Because of space considerations, indenting doesnot necessarily reflect tree structure.A MOVE TO STOP MR GAITSKELL FROM NOMINATING ANYMORE LABOUR LIFE PEERS IS TO BE MADE AT AMEETING OF LABOURMPS TOMORROW .> (NF (DET A) (N MOVE))> (I-BAR (I (TO TO)) (VP (V STOP)(NP (PROP (N MR) (NAME GAITSKELL)))(P FROM)))> (I-BAR (I) (VP (V NOMINATING)(NP (DET ANY) (AP MORE) (N LABOUR)(N LIFE) (N PEERS))))> (I-EAR (I) (UP (IS IS)(I-BAR (I (NP (N TOMORROW))(TO TO) (IS BE))(V MADE) (P AT)(NP (NF (DET A) (N MEETING))(PP (P OF)(NP (N LABOUR) (N PIPS))))))))> (*PER .
)THOUGH THEY MAY GATHER SOME LEFT-WING SUPPORT ,A LARGE MAJORITY OF LABOURMPS ARE LIKELY TOTURN DOWN THE F00T-GRIFFITHS RESOLUTION .> (CP (C-BAR (COMP THOUGH))(IP (NP THEY)(I-BAR (I (MD MAY))(VP (V GATHER)(NP (DET SOME) (3J LEFT-WING)(N SUPPORT))))))> (*CMA ,)> (IP (NP (NP (DET A) (JJ LARGE) (N MAJORITY))(PP (P OF) (NP (N LABOUR) (N MPS))))(I-BAR (I) (VP (IS ARE) (AP (JJ LIKELY)))))> (I-BAR (I (TO TO) (RP DOWN))(uP (v TURN)(NP (DET THE)(PROP (NAME F00T-GRIFFITHS))(N RESOLUTION))))> (*PER .
)MR F00T'S LINE WILL BE THAT AS LABOUR MPS OPPOSEDTHE GOVERNMENT BILL WHICH BROUGHT LIFE PEERS INT0EXISTENCE , THEY SHOULD H0T NOW PUT FORWARDNOMINEES .> (IP (NP (NP (PROP (N MR) (NAME FOOT)))(NP (N LINE)))(I-EAR (I  (MD WILL)) (VP (IS HE) (NP THAT))))> (CP (C-EAR (COMP AS))(IP (NP (N LABOUR) (N MPS))(I-BAR (I) (VP (V OPPOSED)(NP (NP (DET THE) (N GOVERNMENT) (N BILL))(CP (C-BAR (COMP WHICH))(IP (NP)(I-BAR (I) (VP (V BROUGHT)(NP (N LIFE) (N PEERS)))))))(F INT0) (NP (N EXISTENCE))))))> (*CMA ,)> (IP (NP THEY)(I-BAR (I (ADV FORWARD) (MD SHOULD) (XNOT NOT)(ADV NOW))(VP (V PUT) (NP (N NOMINEES)))))> (*PER .
)THE TWO RIVAL AFRICAN NATIONALIST PARTIES OFNORTHERN RHODESIA HAVE AGREED TO GET TOGETHERTO FACE THE CHALLENGE FROM SIR ROY WELENSKY ,THE FEDERAL PREMIER .> (IP (NP (NP (DET THE) (NUM (CD TWO)) (JJ RIVAL)(ffff AFRICAN) (3ff NATIONALIST)(N PARTIES))(PP (P OF) (NP (PROP (NAME NORTHERN)(NAME RHODESIA)))))(I-BAR (I (HAVE HAVE)) (VP (V AGREED)(I-BAR (I (ADV TOGETHER) (TO TO))(VP (V GET)(I-BAR (I (TO TO))(up (v FACE)(NP (DET THE) (N CHALLENGE))(P FROM)(NP (NP (PROP (N SIR) (NAME ROY)(NAME WELENSKY)))(*CMA ,)(NP (DET THE) (JJ FEDERAL)(N+++ PREMIER))))))))))> (*PER .
)251
