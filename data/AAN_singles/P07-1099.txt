Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 784?791,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsLanguage-independent Probabilistic Answer Rankingfor Question AnsweringJeongwoo Ko, Teruko Mitamura, Eric NybergLanguage Technologies InstituteSchool of Computer ScienceCarnegie Mellon University{jko, teruko, ehn}@cs.cmu.eduAbstractThis paper presents a language-independentprobabilistic answer ranking framework forquestion answering.
The framework esti-mates the probability of an individual an-swer candidate given the degree of answerrelevance and the amount of supporting evi-dence provided in the set of answer candi-dates for the question.
Our approach wasevaluated by comparing the candidate an-swer sets generated by Chinese and Japaneseanswer extractors with the re-ranked answersets produced by the answer ranking frame-work.
Empirical results from testing on NT-CIR factoid questions show a 40% perfor-mance improvement in Chinese answer se-lection and a 45% improvement in Japaneseanswer selection.1 IntroductionQuestion answering (QA) systems aim at find-ing precise answers to natural language questionsfrom large document collections.
Typical QA sys-tems (Prager et al, 2000; Clarke et al, 2001;Harabagiu et al, 2000) adopt a pipeline architec-ture that incorporates four major steps: (1) questionanalysis, (2) document retrieval, (3) answer extrac-tion and (4) answer selection.
Question analysis isa process which analyzes a question and produces alist of keywords.
Document retrieval is a step thatsearches for relevant documents or passages.
An-swer extraction extracts a list of answer candidatesfrom the retrieved documents.
Answer selection is aprocess which pinpoints correct answer(s) from theextracted candidate answers.Since the first three steps in the QA pipeline mayproduce erroneous outputs, the final answer selec-tion step often entails identifying correct answer(s)amongst many incorrect ones.
For example, giventhe question ?Which Chinese city has the largestnumber of foreign financial companies?
?, the an-swer extraction component produces a ranked list offive answer candidates: Beijing (AP880603-0268)1,Hong Kong (WSJ920110-0013), Shanghai (FBIS3-58), Taiwan (FT942-2016) and Shanghai (FBIS3-45320).
Due to imprecision in answer extraction,an incorrect answer (?Beijing?)
can be ranked inthe first position, and the correct answer (?Shang-hai?)
was extracted from two different documentsand ranked in the third and the fifth positions.
In or-der to rank ?Shanghai?
in the top position, we haveto address two interesting challenges:?
Answer Similarity.
How do we exploit simi-larity among answer candidates?
For example,when the candidates list contains redundant an-swers (e.g., ?Shanghai?
as above) or several an-swers which represent a single instance (e.g.?U.S.A.?
and ?the United States?
), how muchshould we boost the rank of the redundant an-swers??
Answer Relevance.
How do we identifyrelevant answer(s) amongst irrelevant ones?This task may involve searching for evi-dence of a relationship between the answer1Answer candidates are shown with the identifier of theTREC document where they were found.784and the answer type or a question key-word.
For example, we might wish to querya knowledge base to determine if ?Shang-hai?
is a city (IS-A(Shanghai, city)),or to determine if Shanghai is in China(IS-IN(Shanghai, China)).The first challenge is to exploit redundancy in theset of answer candidates.
As answer candidates areextracted from different documents, they may con-tain identical, similar or complementary text snip-pets.
For example, ?U.S.?
can appear as ?UnitedStates?
or ?USA?
in different documents.
It is im-portant to detect redundant information and boostanswer confidence, especially for list questions thatrequire a set of unique answers.
One approach isto perform answer clustering (Nyberg et al, 2002;Jijkoun et al, 2006).
However, the use of cluster-ing raises additional questions: how to calculate thescore of the clustered answers, and how to select thecluster label.To address the second question, several answerselection approaches have used external knowledgeresources such as WordNet, CYC and gazetteers foranswer validation or answer reranking.
Answer can-didates are either removed or discounted if they arenot of the expected answer type (Xu et al, 2002;Moldovan et al, 2003; Chu-Carroll et al, 2003;Echihabi et al, 2004).
The Web also has been usedfor answer reranking by exploiting search engine re-sults produced by queries containing the answer can-didate and question keywords (Magnini et al, 2002).This approach has been used in various languagesfor answer validation.
Wikipedia?s structured in-formation was used for Spanish answer type check-ing (Buscaldi and Rosso, 2006).Although many QA systems have incorporated in-dividual features and/or resources for answer selec-tion in a single language, there has been little re-search on a generalized probabilistic framework thatsupports answer ranking in multiple languages usingany answer relevance and answer similarity featuresthat are appropriate for the language in question.In this paper, we describe a probabilistic answerranking framework for multiple languages.
Theframework uses logistic regression to estimate theprobability that an answer candidate is correct givenmultiple answer relevance features and answer sim-ilarity features.
An existing framework which wasoriginally developed for English (Ko et al, 2007)was extended for Chinese and Japanese answerranking by incorporating language-specific features.Empirical results on NTCIR Chinese and Japanesefactoid questions show that the framework signifi-cantly improved answer selection performance; Chi-nese performance improved by 40% over the base-line, and Japanese performance improved by 45%over the baseline.The remainder of this paper is organized as fol-lows: Section 2 contains an overview of the answerranking task.
Section 3 summarizes the answer rank-ing framework.
In Section 4, we explain how weextended the framework by incorporating language-specific features.
Section 5 describes the experimen-tal methodology and results.
Finally, Section 6 con-cludes with suggestions for future research.2 Answer Ranking TaskThe relevance of an answer to a question can be es-timated by the probability P(correct(Ai) |Ai, Q),where Q is a question and Ai is an answer can-didate.
To exploit answer similarity, we estimatethe probability P (correct(Ai) |Ai, Aj), where Ajis similar to Ai.
Since both probabilities influenceoverall answer ranking performance, it is importantto combine them in a unified framework and es-timate the probability of an answer candidate as:P (correct(Ai)|Q,A1, ..., An).The estimated probability is used to rank answercandidates and select final answers from the list.
Forfactoid questions, the top answer is selected as a fi-nal answer to the question.
In addition, we can usethe estimated probability to classify incorrect an-swers: if the probability of an answer candidate islower than 0.5, it is considered to be a wrong answerand is filtered out of the answer list.
This is usefulin deciding whether or not a valid answer to a ques-tion exists in a given corpus (Voorhees, 2002).
Theestimated probability can also be used in conjunc-tion with a cutoff threshold when selecting multipleanswers to list questions.3 Answer Ranking FrameworkThis section summarizes our answer ranking frame-work, originally developed for English answers (Ko785P (correct(Ai)|Q,A1, ..., An)?
P (correct(Ai)|rel1(Ai), ..., relK1(Ai), sim1(Ai), ..., simK2(Ai))=exp(?0 +K1?k=1?krelk(Ai) +K2?k=1?ksimk(Ai))1 + exp(?0 +K1?k=1?krelk(Ai) +K2?k=1?ksimk(Ai))where, simk(Ai) =N?j=1(j 6=i)sim?k(Ai, Aj).Figure 1: Estimating correctness of an answer candidate given a question and a set of answer candidateset al, 2007).
The model uses logistic regressionto estimate the probability of an answer candidate(Figure 1).
Each relk(Ai) is a feature function usedto produce an answer relevance score for an an-swer candidate Ai.
Each sim?k(Ai, Aj) is a similar-ity function used to calculate an answer similaritybetween Ai and Aj .
K1 and K2 are the number ofanswer relevance and answer similarity features, re-spectively.
N is the number of answer candidates.To incorporate multiple similarity features, eachsimk(Ai) is obtained from an individual similaritymetric, sim?k(Ai, Aj).
For example, if Levenshteindistance is used as one similarity metric, simk(Ai)is calculated by summing N-1 Levenshtein distancesbetween one answer candidate and all other candi-dates.The parameters ?, ?, ?
were estimated from train-ing data by maximizing the log likelihood.
We usedthe Quasi-Newton algorithm (Minka, 2003) for pa-rameter estimation.Multiple features were used to generate answerrelevance scores and answer similarity scores; theseare discussed below.3.1 Answer Relevance FeaturesAnswer relevance features can be classified intoknowledge-based features or data-driven features.1) Knowledge-based featuresGazetteers: Gazetteers provide geographic infor-mation, which allows us to identify strings as in-stances of countries, their cities, continents, capitals,etc.
For answer ranking, three gazetteer resourceswere used: the Tipster Gazetteer, the CIA WorldFactbook and information about the US states pro-vided by 50states.com.
These resources were usedto assign an answer relevance score between -1 and1 to each candidate.
For example, given the question?Which city in China has the largest number of for-eign financial companies?
?, the candidate ?Shang-hai?
receives a score of 0.5 because it is a city in thegazetteers.
But ?Taiwan?
receives a score of -1.0 be-cause it is not a city in the gazetteers.
A score of 0means the gazetteers did not contribute to the answerselection process for that candidate.Ontology: Ontologies such as WordNet containinformation about relationships between words andgeneral meaning types (synsets, semantic categories,etc.).
WordNet was used to identify answer rele-vance in a manner analogous to the use of gazetteers.For example, given the question ?Who wrote thebook ?Song of Solomon??
?, the candidate ?MarkTwain?
receives a score of 0.5 because its hyper-nyms include ?writer?.2) Data-driven featuresWikipedia: Wikipedia was used to generate an an-swer relevance score.
If there is a Wikipedia docu-ment whose title matches an answer candidate, thedocument is analyzed to obtain the term frequency(tf) and the inverse term frequency (idf) of the can-didate, from which a tf.idf score is calculated.
Whenthere is no matched document, each question key-word is also processed as a back-off strategy, and theanswer relevance score is calculated by summing thetf.idf scores obtained from individual keywords.Google: Following Magnini et al (2002), a queryconsisting of an answer candidate and question key-786words was sent to the Google search engine.
Thenthe top 10 text snippets returned by Google wereanalyzed to generate an answer relevance score bycomputing the minimum number of words betweena keyword and the answer candidate.3.2 Answer Similarity FeaturesAnswer similarity is calculated using multiple stringdistance metrics and a list of synonyms.String Distance Metrics: String distance metricssuch as Levenshtein, Jaro-Winkler, and Cosine sim-ilarity were used to calculate the similarity betweentwo English answer candidates.Synonyms: Synonyms can be used as anothermetric to calculate answer similarity.
If one answeris synonym of another answer, the score is 1.
Other-wise the score is 0.
To get a list of synonyms, threeknowledge bases were used: WordNet, Wikipediaand the CIA World Factbook.
In addition, manuallygenerated rules were used to obtain synonyms fordifferent types of answer candidates.
For example,?April 12 1914?
and ?12th Apr.
1914?
are convertedinto ?1914-04-12?
and treated as synonyms.4 Extensions for Multiple LanguagesWe extended the framework for Chinese andJapanese QA.
This section details how we incor-porated language-specific resources into the frame-work.
As logistic regression is based on a proba-bilistic framework, the model does not need to bechanged to support other languages.
We only re-trained the model for individual languages.
To sup-port Chinese and Japanese QA, we incorporated newfeatures for individual languages.4.1 Answer Relevance FeaturesWe replaced the English gazetteers and WordNetwith language-specific resources for Japanese andChinese.
As Wikipedia and the Web support mul-tiple languages, the same algorithm was used insearching language-specific corpora for the two lan-guages.1) Knowledge-based featuresThe knowledge-based features involve searching forfacts in a knowledge base such as gazetteers andWordNet.
We utilized comparable resources forChinese and Japanese.
Using language-specific re-#ArticlesLanguage Nov. 2005 Aug. 2006English 1,811,554 3,583,699Japanese 201,703 446,122Chinese 69,936 197,447Table 1: Articles in Wikipedia for different lan-guagessources, the same algorithms were applied to gener-ate an answer relevance score between -1 and 1.Gazetteers: There are few available gazetteersfor Chinese and Japanese.
Therefore, we extractedlocation data from language-specific resources.
ForJapanese, we extracted Japanese location informa-tion from Yahoo2, which contains many locationnames in Japan and the relationships among them.For Chinese, we extracted location names from theWeb.
In addition, we translated country names pro-vided by the CIA World Factbook and the Tipstergazetteers into Chinese and Japanese names.
Asthere is more than one translation, top 3 translationswere used.Ontology: For Chinese, we used HowNet (Dong,2000) which is a Chinese version of WordNet.It contains 65,000 Chinese concepts and 75,000corresponding English equivalents.
For Japanese,we used semantic classes provided by GengoGoiTaikei3.
Gengo GoiTaikei is a Japanese lexiconcontaining 300,000 Japanese words with their asso-ciated 3,000 semantic classes.
The semantic infor-mation provided by HowNet and Gengo GoiTaikeiwas used to assign an answer relevance score be-tween -1 and 1.2) Data-driven featuresWikipedia: As Wikipedia supports more than 200language editions, the approach used in English canbe used for different languages without any modifi-cation.
Table 1 shows the number of text articles inthree different languages.
Wikipedia?s current cov-erage in Japanese and Chinese does not match itscoverage in English, but coverage in these languagescontinues to improve.To supplement the small corpus of Chi-nese documents available, we used Baidu2http://map.yahoo.co.jp/3http://www.kecl.ntt.co.jp/mtg/resources/GoiTaikei787(http://baike.baidu.com), which is similar toWikipedia but contains more articles written inChinese.
We first search for Chinese Wikipedia.When there is no matching document in Wikipedia,each answer candidate is sent to Baidu and theretrieved document is analyzed in the same way toanalyze Wikipedia documents.The idf score was calculated using word statis-tics from Japanese Yomiuri newspaper corpus andthe NTCIR Chinese corpus.Google: The same algorithm was applied to ana-lyze Japanese and Chinese snippets returned fromGoogle.
But we restricted the language to Chi-nese or Japanese so that Google returned only Chi-nese or Japanese documents.
To calculate the dis-tance between an answer candidate and questionkeywords, segmentation was done with linguistictools.
For Japanese, Chasen4 was used.
For Chinesesegmentation, a maximum-entropy based parser wasused (Wang et al, 2006).3) Manual FilteringOther than the features mentioned above, we man-ually created many rules for numeric and temporalquestions to filter out invalid answers.
For example,when the question is looking for a year as an answer,an answer candidate which contains only the monthreceives a score of -1.
Otherwise, the score is 0.4.2 Answer Similarity FeaturesThe same features used for English were appliedto calculate the similarity of Chinese/Japanese an-swer candidates.
To identify synonyms, Wikipediawere used for both Chinese and Japanese.
EIJIROdictionary was used to obtain Japanese synonyms.EIJIRO is a English-Japanese dictionary contain-ing 1,576,138 words and provides synonyms forJapanese words.As there are several different ways to representtemporal and numeric expressions (Nyberg et al,2002; Greenwood, 2006), language-specific conver-sion rules were applied to convert them into a canon-ical format; for example, a rule to convert JapaneseKanji characters to Arabic numbers is shown in Fig-ure 2.4http://chasen.aist-nara.ac.jp/hiki/ChaSen0.25????1993-07-041993?
7 ?4 ?50 %??1993-07-04?????????3E+11?3,000??3E+11????
?Normalizedanswer stringOriginal answer stringFigure 2: Example of normalized answer strings5 ExperimentsThis section describes the experiments to evaluatethe extended answer ranking framework for Chineseand Japanese QA.5.1 Experimental SetupWe used Chinese and Japanese questions providedby the NTCIR (NII Test Collection for IR Sys-tems), which focuses on evaluating cross-lingualand monolingual QA tasks for Chinese, Japaneseand English.
For Chinese, a total of 550 fac-toid questions from the NTCIR5-6 QA evaluationsserved as the dataset.
Among them, 200 questionswere used to train the Chinese answer extractor and350 questions were used to evaluate our answerranking framework.
For Japanese, 700 questionsfrom the NTCIR5-6 QA evaluations served as thedataset.
Among them, 300 questions were used totrain the Japanese answer extractor and 400 ques-tions were used to evaluate our framework.Both the Chinese and Japanese answer extractorsuse maximum-entropy to extract answer candidatesbased on multiple features such as named entity, de-pendency structures and some language-dependentfeatures.Performance of the answer ranking frameworkwas measured by average answer accuracy: thenumber of correct top answers divided by the num-ber of questions where at least one correct answerexists in the candidate list provided by an extrac-tor.
Mean Reciprocal Rank (MRR5) was also usedto calculate the average reciprocal rank of the firstcorrect answer in the top 5 answers.The baseline for average answer accuracy wascalculated using the answer candidate likelihoodscores provided by each individual extractor; the788TOP1TOP3MRR50.00.10.20.30.40.50.60.70.80.91.0JapaneseAnswer SelectionBaselineFrameworkTOP1TOP3MRR50.00.10.20.30.40.50.60.70.80.91.0ChineseAnswer SelectionAvgerage AccuracyBaselineFrameworkFigure 3: Performance of the answer ranking framework for Chinese and Japanese answer selection (TOP1:average accuracy of top answer, TOP3: average accuracy of top 3 answers, MRR5: average of mean recip-rocal rank of top 5 answers)answer with the best extractor score was chosen,and no validation or similarity processing was per-formed.3-fold cross-validation was performed, and weused a version of Wikipedia downloaded in Aug2006.5.2 Results and AnalysisWe first analyzed the average accuracy of top 1, top3and top 5 answers.
Figure 3 compares the averageaccuracy using the baseline and the answer selec-tion framework.
As can be seen, the answer rank-ing framework significantly improved performanceon both Chinese and Japanese answer selection.
Asfor the average top answer accuracy, there were 40%improvement over the baseline (Chinese) and 45%improvement over the baseline (Japanese).We also analyzed the degree to which the averageaccuracy was affected by answer similarity and rel-evance features.
Table 2 compares the average topanswer accuracy using the baseline, the answer rel-evance features, the answer similarity features andall feature combinations.
Both the similarity and therelevance features significantly improved answer se-lection performance compared to the baseline, andcombining both sets of features together producedthe best performance.We further analyzed the utility of individual rele-vance features (Figure 4).
For both languages, filter-ing was useful in ruling out wrong answers.
The im-Baseline Rel Sim AllChinese 0.442 0.482 0.597 0.619Japanese 0.367 0.463 0.502 0.532Table 2: Average top answer accuracy of individ-ual features (Rel: merging relevance features, Sim:merging similarity features, ALL: merging all fea-tures).pact of the ontology was more positive for Japanese;we assume that this is because the Chinese ontol-ogy (HowNet) contains much less information over-all than the Japanese ontology (Gengo GoiTaikei).The comparative impact of Wikipedia was similar.For Chinese, there were many fewer Wikipedia doc-uments available.
Even though we used Baidu as asupplemental resource for Chinese, this did not im-prove answer selection performance.
On the otherhand, the use of Wikipedia was very helpful forJapanese, improving performance by 26% over thebaseline.
This shows that the quality of answerrelevance estimation is significantly affected by re-source coverage.When comparing the data-driven features with theknowledge-based features, the data-driven features(such as Wikipedia and Google) tended to increaseperformance more than the knowledge-based fea-tures (such as gazetteers and WordNet).Table 3 shows the effect of individual similar-ity features on Chinese and Japanese answer selec-789BaselineFILONTGAZGLWIKIAll0.300.350.400.450.500.55Avg.
Top Answer AccuracyChineseJapaneseFigure 4: Average top answer accuracy of individ-ual answer relevance features.
(FIL: filtering, ONT,ontology, GAZ: gazetteers, GL: Google, WIKI:Wikipedia, ALL: combination of all relevance fea-tures)Chinese Japanese0.3 0.5 0.3 0.5Cosine 0.597 0.597 0.488 0.488Jaro-Winkler 0.544 0.518 0.410 0.415Levenshtein 0.558 0.544 0.434 0.449Synonyms 0.527 0.527 0.493 0.493All 0.588 0.580 0.502 0.488Table 3: Average accuracy using individual similar-ity features under different thresholds: 0.3 and 0.5(?All?
: combination of all similarity metrics)tion.
As some string similarity features (e.g., Lev-enshtein distance) produce a number between 0 and1 (where 1 means two strings are identical and 0means they are different), similarity scores less thana threshold can be ignored.
We used two thresh-olds: 0.3 and 0.5.
In our experiments, using 0.3as a threshold produced better results in Chinese.In Japanese, 0,5 was a better threshold for individ-ual features.
Among three different string similar-ity features (Levenshtein, Jaro-Winkler and Cosinesimilarity), cosine similarity tended to perform bet-ter than the others.When comparing synonym features with stringsimilarity features, synonyms performed better thanstring similarity in Japanese, but not in Chinese.
Wehad many more synonyms available for JapaneseData-driven features All featuresChinese 0.606 0.619Japanese 0.517 0.532Table 4: Average top answer accuracy when usingdata-driven features v.s.
when using all features.and they helped the system to better exploit answerredundancy.We also analyzed answer selection performancewhen combining all four similarity features (?All?in Table 3).
Combining all similarity features im-proved the performance in Japanese, but hurt theperformance in Chinese, because adding a small setof synonyms to the string metrics worsened the per-formance of logistic regression.5.3 Utility of data-driven featuresIn our experiments we used data-driven fea-tures as well as knowledge-based features.
Asknowledge-based features need manual effort to ac-cess language-specific resources for individual lan-guages, we conducted an additional experiment onlywith data-driven features in order to see how muchperformance gain is available without the manualwork.
As Google, Wikipedia and string similaritymetrics can be used without any additional manualeffort when extended to other languages, we usedthese three features and compared the performance.Table 4 shows the performance when using data-driven features v.s.
all features.
It can be seen thatdata-driven features alone achieved significant im-provement over the baseline.
This indicates that theframework can easily be extended to any languagewhere appropriate data resources are available, evenif knowledge-based features and resources for thelanguage are still under development.6 ConclusionIn this paper, we presented a generalized answer se-lection framework which was applied to Chinese andJapanese question answering.
An empirical evalu-ation using NTCIR test questions showed that theframework significantly improves baseline answerselection performance.
For Chinese, the perfor-mance improved by 40% over the baseline.
ForJapanese, the performance improved by 45% over790the baseline.
This shows that our probabilisticframework can be easily extended for multiple lan-guages by reusing data-driven features (with newcorpora) and adding language-specific resources(ontologies, gazetteers) for knowledge-based fea-tures.In our previous work, we evaluated the perfor-mance of the framework for English QA using ques-tions from past TREC evaluations (Ko et al, 2007).The experimental results showed that the combina-tion of all answer ranking features improved per-formance by an average of 102% over the baseline.The relevance features improved performance by anaverage of 99% over the baseline, and the similar-ity features improved performance by an average of46% over the baseline.
Our hypothesis is that answerrelevance features had a greater impact for EnglishQA because the quality and coverage of the data re-sources available for English answer validation ismuch higher than the quality and coverage of ex-isting resources for Japanese and Chinese.
In futurework, we will continue to evaluate the robustness ofthe framework.
It is also clear from our comparisonwith English QA that more work can and should bedone in acquiring data resources for answer valida-tion in Chinese and Japanese.AcknowledgmentsWe would like to thank Hideki Shima, MengqiuWang, Frank Lin, Justin Betteridge, MatthewBilotti, Andrew Schlaikjer and Luo Si for their valu-able support.
This work was supported in partby ARDA/DTO AQUAINT program award numberNBCHC040164.ReferencesD.
Buscaldi and P. Rosso.
2006.
Mining Knowledgefrom Wikipedia for the Question Answering task.
InProceedings of the International Conference on Lan-guage Resources and Evaluation.J.
Chu-Carroll, J. Prager, C. Welty, K. Czuba, and D. Fer-rucci.
2003.
A Multi-Strategy and Multi-Source Ap-proach to Question Answering.
In Proceedings of TextREtrieval Conference.C.
Clarke, G. Cormack, and T. Lynam.
2001.
Exploitingredundancy in question answering.
In Proceedings ofSIGIR.Zhendong Dong.
2000.
Hownet:http://www.keenage.com.A.
Echihabi, U. Hermjakob, E. Hovy, D. Marcu, E. Melz,and D. Ravichandran.
2004.
How to select an answerstring?
In T. Strzalkowski and S. Harabagiu, editors,Advances in Textual Question Answering.
Kluwer.Mark A. Greenwood.
2006.
Open-Domain Question An-swering.
Thesis.S.
Harabagiu, D. Moldovan, M. Pasca, R. Mihalcea,M.
Surdeanu, R. Bunsecu, R. Girju, V. Rus, andP.
Morarescu.
2000.
Falcon: Boosting knowledge foranswer engines.
In Proceedings of TREC.V.
Jijkoun, J. van Rantwijk, D. Ahn, E. Tjong Kim Sang,and M. de Rijke.
2006.
The University of Amsterdamat CLEF@QA 2006.
In Working Notes CLEF.J.
Ko, L. Si, and E. Nyberg.
2007.
A Probabilistic Frame-work for Answer Selection in Question Answering.
InProceedings of NAACL/HLT.B.
Magnini, M. Negri, R. Pervete, and H. Tanev.
2002.Comparing statistical and content-based techniques foranswer validation on the web.
In Proceedings of theVIII Convegno AI*IA.T.
Minka.
2003.
A Comparison of Numerical Optimizersfor Logistic Regression.
Unpublished draft.D.
Moldovan, D. Clark, S. Harabagiu, and S. Maiorano.2003.
Cogex: A logic prover for question answering.In Proceedings of HLT-NAACL.E.
Nyberg, T. Mitamura, J. Carbonell, J. Callan,K.
Collins-Thompson, K. Czuba, M. Duggan,L.
Hiyakumoto, N. Hu, Y. Huang, J. Ko, L. Lita,S.
Murtagh, V. Pedro, and D. Svoboda.
2002.
TheJAVELIN Question-Answering System at TREC 2002.In Proceedings of Text REtrieval Conference.J.
Prager, E. Brown, A. Coden, and D. Radev.
2000.Question answering by predictive annotation.
In Pro-ceedings of SIGIR.E.
Voorhees.
2002.
Overview of the TREC 2002 ques-tion answering track.
In Proceedings of Text REtrievalConference.M.
Wang, K. Sagae, and T. Mitamura.
2006.
A Fast, Ac-curate Deterministic Parser for Chinese.
In Proceed-ings of COLING/ACL.J.
Xu, A. Licuanan, J.
May, S. Miller, and R. Weischedel.2002.
TREC 2002 QA at BBN: Answer Selection andConfidence Estimation.
In Proceedings of Text RE-trieval Conference.791
