Proceedings of the Second Workshop on Statistical Machine Translation, pages 17?24,Prague, June 2007. c?2007 Association for Computational LinguisticsIntegration of an Arabic Transliteration Module into a StatisticalMachine Translation SystemMehdi M. Kashani+, Eric Joanis++, Roland Kuhn++, George Foster++, Fred Popowich++ School of Computing ScienceSimon Fraser University8888 University DriveBurnaby, BC V5A 1S6, Canadammostafa@sfu.capopowich@sfu.ca++ NRC Institute for Information Technology101 St-Jean-Bosco StreetGatineau, QC K1A 0R6, Canadafirstname.lastname@cnrc-nrc.gc.caAbstractWe provide an in-depth analysis of the in-tegration of an Arabic-to-English translit-eration system into a general-purposephrase-based statistical machine translationsystem.
We study the integration from dif-ferent aspects and evaluate the improve-ment that can be attributed to the integra-tion using the BLEU metric.
Our experi-ments show that a transliteration modulecan help significantly in the situation wherethe test data is rich with previously unseennamed entities.
We obtain 70% and 53% ofthe theoretical maximum improvement wecould achieve, as measured by an oracle ondevelopment and test sets respectively forOOV words (out of vocabulary sourcewords not appearing in the phrase table).1 IntroductionTransliteration is the practice of transcribing aword or text written in one writing system into an-other writing system.
The most frequent candidatesfor transliteration are person names, locations, or-ganizations and imported words.
The lack of afully comprehensive bilingual dictionary includingthe entries for all named entities (NEs) renders thetask of transliteration necessary for certain naturallanguage processing applications dealing withnamed entities.
Two applications where translitera-tion can be particularly useful are machine transla-tion (MT) and cross lingual information retrieval.While transliteration itself is a relatively well-studied problem, its effect on the aforementionedapplications is still under investigation.Transliteration as a self-contained task has itsown challenges, but applying it to a real applica-tion introduces new challenges.
In this paper weanalyze the efficacy of integrating a transliterationmodule into a real MT system and evaluate theperformance.When working on a limited domain, given a suf-ficiently large amount of training data, almost allof the words in the unseen data (in the same do-main) will have appeared in the training corpus.But this argument does not hold for NEs, becauseno matter how big the training corpus is, there willalways be unseen names of people and locations.Current MT systems either leave such unknownnames as they are in the final target text or removethem in order to obtain a better evaluation score.None of these methods can give the reader who isnot familiar with the source language any informa-tion about those out-of-vocabulary (OOV) words,especially when the source and target languagesuse different scripts.
If these words are not names,one can usually guess what they are, by using thepartial information of other parts of speech.
But, inthe case of names, there is no way to determine theindividual or location the sentence is talking about.So, to improve the usability of a translation, it isparticularly important to handle NEs well.The importance of NEs is not yet reflected in theevaluation methods used in the MT community,the most common of which is the BLEU metric.BLEU (Papineni et al 2002) was devised to pro-vide automatic evaluation of MT output.
In thismetric n-gram similarity of the MT output is com-puted with one or more references made by human17translators.
BLEU does not distinguish betweendifferent words and gives equal weight to all.
Inthis paper, we base our evaluation on the BLEUmetric and show that using transliteration has im-pact on it (and in some cases significant impact).However, we believe that such integration is moreimportant for practical uses of MT than BLEU in-dicates.Other than improving readability and raising theBLEU score, another advantage of using a translit-eration system is that having the right translationfor a name helps the language model select a betterordering for other words.
For example, our phrasetable1 does not have any entry for ??????
(Dulles)and when running MT system on the plain Arabictext we getand this trip was cancelled [?]
by the americanauthorities responsible for security at the airport????
.We ran our MT system twice, once by suggest-ing ?dallas?
and another time ?dulles?
as Englishequivalents for ??????
and the decoder generatedthe following sentences, respectively:and this trip was cancelled [?]
by the americanauthorities responsible for security at the airportat dallas .and this trip was cancelled [?]
by the americanauthorities responsible for security at dulles air-port .2Every statistical MT (SMT) system assigns aprobability distribution to the words that are seenin its parallel training data, including proper names.The richer the training data, the higher the chancefor a given name in the test data to be found in thetranslation tables.
In other words, an MT systemwith a relatively rich phrase table is able to trans-late many of the common names in the test data,with all the remaining words being rare and foreign.So unlike a self-contained transliteration module,which typically deals with a mix of ?easy?
and1 A table where the conditional probabilities of targetphrases given source phrases (and vice versa) is kept.2 Note that the language model can be trained on moretext, and hence can know more NEs than the translationmodel does.?hard?
names, the primary use for a transliterationmodule embedded in an SMT system will be todeal with the ?hard?
names left over after thephrase tables have provided translations for the?easy?
ones.
That means that when measuring theperformance improvements caused by embeddinga transliteration module in an MT system, onemust keep in mind that such improvements are dif-ficult to attain: they are won mainly by correctlytransliterating ?hard?
names.Another issue with OOV words is that some ofthem remained untranslated due to misspellings inthe source text.
For example, we encountered???????
(?Hthearow?)
instead of ???????(?Heathrow?)
or ???????
(?Brezer?)
instead of???????
(?Bremer?)
in our development test set.Also, evaluation by BLEU (or a similar auto-matic metric) is problematic.
Almost all of the MTevaluations use one or more reference translationsas the gold standard and, using some metrics, theygive a score to the MT output.
The problem withNEs is that they usually have more than a singleequivalent in the target language (especially if theydon't originally come from the target language)which may or may not have been captured in thegold standard.
So even if the transliteration modulecomes up with a correct interpretation of a name itmight not receive credit as far as the limited num-ber of correct names in the references are con-cerned.Our first impression was that having more inter-pretations for a name in the references would raisethe transliteration module?s chance to generate atleast one of them, hence improving the perform-ance.
But, in practice, when references do notagree on a name?s transliteration that is the sign ofan ambiguity.
In these cases, the transliterationmodule often suggests a correct transliteration thatthe decoder outputs correctly, but which fails toreceive credit from the BLEU metric because thistransliteration is not found in the references.
As anexample, for the name ????????
?, four referencescame up with four different interpretations:swerios, swiriyus, severius, sweires.
A quick queryin Google showed us another four acceptable in-terpretations (severios, sewerios, sweirios, saw-erios).Machine transliteration has been an active re-search field for quite a while (Al-Onaizan andKnight, 2002; AbdulJaleel and Larkey, 2003; Kle-mentiev and Roth, 2006; Sproat et al 2006) but to18our knowledge there is little published work onevaluating transliteration within a real MT system.The closest work to ours is described in (Hassanand Sorensen, 2005) where they have a list ofnames in Arabic and feed this list as the input textto their MT system.
They evaluate their system inthree different cases: as a word-based NE transla-tion, phrase-based NE translation and in presenceof a transliteration module.
Then, they report theBLEU score on the final output.
Since their text iscomprised of only NEs, the BLEU increase is quitehigh.
Combining all three models, they get a 24.9BLEU point increase over the na?ve baseline.
Thedifference they report between their best methodwithout transliteration and the one including trans-literation is 8.12 BLEU points for person names(their best increase).In section 2, we introduce different methods forincorporating a transliteration module into an MTsystem and justify our choice.
In section 3, thetransliteration module is briefly introduced and weexplain how we prepared its output for use by theMT system.
In section 4, an evaluation of the inte-gration is provided.
Finally, section 5 concludesthe paper.2 Our ApproachBefore going into details of our approach, anoverview of Portage (Sadat et al 2005), themachine translation system that we used for ourexperiments and some of its properties should beprovided.Portage is a statistical phrase-based SMT systemsimilar to Pharaoh (Koehn et al 2003).
Given asource sentence, it tries to find the target sentencethat maximizes the joint probability of a target sen-tence and a phrase alignment according to a loglin-ear model.
Features in the loglinear model consistof a phrase-based translation model with relative-frequency and lexical probability estimates; a 4-gram language model using Kneser-Ney smooth-ing, trained with the SRILM toolkit; a single-parameter distortion penalty on phrase reordering;and a word-length penalty.
Weights on the loglin-ear features are set using Och's algorithm (Och,2003) to maximize the system's BLEU score on adevelopment corpus.
To generate phrase pairs froma parallel corpus, we use the "diag-and" phraseinduction algorithm described in (Koehn et al2003), with symmetrized word alignments gener-ated using IBM model 2 (Brown et al 1993).Portage allows the use of SGML-like markupfor arbitrary entities within the input text.
Themarkup can be used to specify translationsprovided by external sources for the entities, suchas rule-based translations of numbers and dates, ora transliteration module for OOVs in our work.Many SMT systems have this capability, soalthough the details given here pertain to Portage,the techniques described can be used in manydifferent SMT systems.As an example, suppose we already have twodifferent transliterations with their probabilities forthe Arabic name ??????.
We can replace everyoccurrence of the ??????
in the Arabic input textwith the following:<NAME target="mohammed|mohamed"prob=".7|.3"> ????
</NAME>By running Portage on this marked up text, thedecoder chooses between entries in its own phrasetable and the marked-up text.
One thing that isimportant for our task is that if the entry cannot befound in Portage?s phrase tables, it is guaranteedthat one of the candidates inside the markup willbe chosen.
Even if none of the candidates exist inthe language model, the decoder still picks one ofthem, because the system assigns a small arbitraryprobability (we typically use e-18) as unigramprobability of each unseen word.We considered four different methods forincorporating the transliteration module into theMT system.
The first and second methods need anNE tagger and the other two do not require anyexternal tools.Method 1: use an NE tagger to extract thenames in the Arabic input text.
Then, run thetransliteration module on them and assignprobabilities to top candidates.
Use the markupcapability of Portage and replace each name in theArabic text with the SGML-like tag includingdifferent probabilities for different candidates.Feed the marked-up text to Portage to translate.Method 2: similar to method 1 but instead ofusing the marked-up text, a new phrase table, onlycontaining entries for the names in the Arabic inputtext is built and added to Portage?s existing phrasetables.
A weight is given to this phrase table and19then the decoder uses this phrase table as well asits own phrase tables to decide which translation tochoose when encountering the names in thetext.
The main difference between methods 1 and2 is that in our system, method 2 allows for a bleu-optimal weight to be learned for the NE phrasetable, whereas the weight on the rules for method 1has to be set by hand.Method 3: run Portage on the plain Arabic text.Extract all untranslated Arabic OOVs and run thetransliteration module on them.
Replace them withthe top candidate.Method 4: run Portage on the plain Arabic text.Extract all untranslated Arabic OOVs and run thetransliteration module on them.
Replace them withSGML-like tags including different probabilitiesfor different candidates, as described previously.Feed the marked-up text to Portage to translate.The first two methods need a powerful NEtagger with a high recall value.
We computed therecall value on the development set OOVs usingtwo different NE taggers, Tagger A and Tagger B(each from a different research group).
Taggers Aand B showed a recall of 33% and 53% respec-tively, both being low for our purposes.
Anotherissue with these two methods is that for many ofthe names the transliteration module will competewith the internal phrase table.
Our observationsshow that if a name exists in the phrase table, it islikely to be translated correctly.
In general,observed parallel data (i.e.
training data) should bea more reliable source of information thantransliteration, encouraging us to use transliterationmost appropriately as a ?back-off?
method.
In afew cases, the Arabic name is ambiguous with acommon word and is mistakenly translated as such.For example, ?????
???
????
is an Arabic name thatshould be transliterated as ?Hani Abu Nahl?
butsince ?????
also means ?solve?, the MT systemoutputs ?Hani Abu Solve?.
The advantage of thefirst two methods is that they can deal with suchcases.
But considering the noise in the NEdetectors, handling them increases the risk oflosing already correct translations of other names.The third method is simple and easy to use butnot optimal: it does not take advantage of thedecoder?s internal features (notably the languagemodels) and only picks up the highest scoringcandidate from the transliteration module.The fourth method only deals with those wordsthat the MT system was unable to deal with andhad to leave untranslated in the final text.Therefore whatever suggestions the transliterationmodule makes do not need to compete with theinternal phrase tables, which is good because weexpect the phrase tables to be a more reliablesource of information.
It is guaranteed that thetranslation quality will be improved (in the worstcase, a bad transliteration is still more informativethan the original word in Arabic script).
Moreover,unlike the third method, we take advantage of allinternal decoder features on the second pass.
Weadopt the fourth method for our experiment.
Thefollowing example better illustrates how thisapproach works:Example: Suppose we have the following sentencein the Arabic input text:????
????
?????
?????
??????
?.Portage is run on the Arabic plain text and yieldsthe following output:blair accepts ?????
report in full .The Arabic word ???????
(Hutton) is extracted andfed to the transliteration module.
Thetransliteration module comes up with some Englishcandidates, each with different probabilities asestimated by the HMM.
They are rescaled (as willbe explained in section 3) and the followingmarkup text will be generated to replace theuntranslated ???????
in the first plain Arabicsentence:<NAME target="hoton|hutton|authon"prob="0.1|0.00028|4.64e-05">????
?</NAME>Portage is then run on this newly marked up text(second pass).
From now on, with the additionalguidance of the language models, it is thedecoder?s task to decide between different markupsuggestions.
For the above example, the followingoutput will be generated:blair accepts hutton report in full .203 Transliteration SystemIn this section we provide a brief overview of theembedded transliteration system we used for ourexperiment.
For the full description refer to(Kashani et al 2007).3.1 Three Phase TransliterationThe transliteration module follows the noisychannel framework.
The adapted spelling-basedgenerative model is similar to (Al-Onaizan andKnight, 2002).
It consists of three consecutivephases, the first two using HMMs and the Viterbialgorithm, and the third using a number ofmonolingual dictionaries to match the close entriesor to filter out some invalid candidates from thefirst two phases.Since in Arabic, the diacritics are usuallyomitted in writing, a name like ??????
(Mohamed)would have an equivalent like ?mhmd?
if we onlytake into account the written letters.
To addressthis issue, we run Viterbi in two different passes(each called a phase), using HMMs trained on dataprepared in different ways.In phase 1, the system tries to find the besttransliterations of the written word, without caringabout what the hidden diacritics would be (in ourexample, mhmd).In phase 2, given the Arabic input and the outputcandidates from phase 1, the system fills in thepossible blanks in between using the character-based language model (yielding ?mohamed?
as apossible output, among others).To prepare the character-level translation modelfor both phases we adopted an approach similar to(AbdulJaleel and Larkey, 2003).In phase 3, the Google unigram model(LDC2006T13 from the LDC catalog) is first usedto filter out the noise (i.e.
those candidates that donot exist in the Google unigram are removed fromthe candidate list).
Then a combination of somemonolingual dictionaries of person names is usedto find close matches between their entries and theHMM output candidates based on the Levenshteindistance metric.3.2 Task-specific Changes to the ModuleDue to the nature of the task at hand and byobserving the development test set and itsreferences, the following major changes becamenecessary:Removing Part of Phase Three: By observing theOOV words in the development test set, werealized that having the monolingual dictionary inthe pipeline and using the Levensthtein distance asa metric for adding the closest dictionary entries tothe final output, does not help much, mainlybecause OOVs are rarely in the dictionary.
So, thedictionary part not only slows down the executionbut would also add noise to the final output (byadding some entries that probably are not thedesired outputs).
However, we kept the Googleunigram filtering in the pipeline.Rescaling HMM Probabilities: Although thetransliteration module outputs HMM probabilityscore for each candidate, and the MT system alsouses probability scores, in practice the translitera-tion scores have to be adjusted.
For example, ifthree consecutive candidates have log probabilities-40, -42 and -50, the decoder should be given val-ues with similar differences in scale, comparablewith the typical differences in its internal features(eg.
Language Models).
Knowing that the entriesin the internal features usually have exponentialdifferences, we adopted the following conversionformula:p'i = 0.1*(pi/pmax)?Equation 1where pi = 10(output of HMM for candidate i) and max is thebest candidate.We rescale the HMM probability so that the topcandidate is (arbitrarily) given a probability of p'max= 0.1.
It immediately follows that the rescaledscore would be 0.1 * pi / pmax.
Since the decodercombines its models in a log-linear fashion, weapply an exponent ?
to the HMM probabilities be-fore scaling them, as way to control the weight ofthose probabilities in decoding.
This yields equa-tion 1.
Ideally, we would like the weight ?
to beoptimized the same way other decoder weights areoptimized, but our decoder does not support thisyet, so for this work we arbitrarily set the weight to?
= 0.2, which seems to work well.
For the aboveexample, the distribution would be 0.1, 0.039 and 0.001.21Prefix Detachment: Arabic is a morphologicallyrich language.
Even after performing tokenization,some words still remain untokenized.
If thecomposite word is frequent, there is a chance that itexists in the phrase table but many times it doesnot, especially if the main part of that word is anamed entity.
We did not want to delve into thedetails of morphology: we only considered twofrequent prefixes: ???
(?va?
meaning ?and?)
and????
(?al?
determiner in Arabic).
If a word startswith either of these two prefixes, we detach themand run the transliteration module once on thedetached name and a second time on the wholeword.
The output candidates are mergedautomatically based on their scores, and thedecoder decides which one to choose.Keeping the Top 5 HMM Candidates: Thetransliteration module uses the Google unigrammodel to filter out the candidate words that do notappear above a certain threshold (200 times) on theInternet.
This helps eliminate hundreds ofunwanted sequences of letters.
But, we decided tokeep top-5 candidates on the output list, even ifthey are rejected by the Google unigram modelbecause sometimes the transliteration module isunable to suggest the correct equivalent or in othercases the OOV should actually be translated ratherthan transliterated 3 .
In these cases, the closestliteral transliteration will still provide the end usermore information about the entity than the word inArabic script would.4 EvaluationAlthough there are metrics that directly address NEtranslation performance4, we chose to use BLEUbecause our purpose is to assess NE translationwithin MT, and BLEU is currently the standardmetric for MT.3 This would happen especially for ancient names orsome names that underwent sophisticated morphologi-cal transformations (For example, Abraham in Englishand ???????
(Ibrahim) in Arabic).4 NIST?s NE translation task(http://www.nist.gov/speech/tests/ace/index.htm) is anexample.4.1 Training DataWe used the data made available for the 2006NIST Machine Translation Evaluation.
Our bilin-gual training corpus consisted of 4M sentence pairsdrawn mostly from newswire and UN domains.We trained one language model on the English halfof this corpus (137M running words), and anotheron the English Gigaword corpus (2.3G runningwords).
For tuning feature weights, we used LDC's"multiple translation part 1" corpus, which contains1,043 sentence pairs.4.2 Test DataWe used the NIST MT04 evaluation set and theNIST MT05 evaluation set as our development andblind test sets.
The development test set consists of1353 sentences, 233 of which contain OOVs.Among them 100 sentences have OOVs that areactually named entities.
The blind test set consistsof 1056 sentences, 189 of them having OOVs and131 of them having OOV named entities.
Thenumber of sentences for each experiment issummarized in table 1.Whole Text OOVSentencesOOV-NESentencesDev test set 1353 233 100Blind test set 1056 189 131Table 1: Distribution of sentences in test sets.4.3 ResultsAs the baseline, we ran the Portage without thetransliteration module on development and blindtest sets.
The second column of table 2 showsbaseline BLEU scores.
We applied method 4 asoutlined in section 2 and computed the BLEUscore, also in order to compare the results weimplemented method 3 on the same test sets.
TheBLEU scores obtained from methods 3 and 4 areshown in columns 3 and 4 of table 2.baseline Method 3 Method 4 OracleDev 44.67 44.71 44.83 44.90Blind 48.56 48.62 48.80 49.01Table 2: BLEU score on different test sets.Considering the fact that only a small portion ofthe test set has out-of-vocabulary named entities,22we computed the BLEU score on two differentsub-portions of the test set: first, on the sentenceswith OOVs; second, only on the sentencescontaining OOV named entities.
The BLEUincrease on different portions of the test set isshown in table 3.baseline Method 4Dev OOV sentences 39.17 40.02OOV-NE Sentences 44.56 46.31blind OOV sentences 43.93 45.07OOV-NE Sentences 42.32 44.87Table 3: BLEU score on differentportions of the test sets.To set an upper bound on how much applyingany transliteration module can contribute to theoverall results, we developed an oracle-likedictionary for the OOVs in the test sets, which wasthen used to create a markup Arabic text.
Byfeeding this markup input to the MT system weobtained the result shown in column 5 of table 2.This is the performance our system would achieveif it had perfect accuracy in transliteration,including correctly guessing what errors the humantranslators made in the references.
Method 4achieves 70% of this maximum gain on dev, and53% on blind.5 ConclusionThis paper has described the integration of a trans-literation module into a state-of-the-art statisticalmachine translation (SMT) system for the Arabicto English task.
The final version of the translitera-tion module operates in three phases.
First, it gen-erates English letter sequences corresponding tothe Arabic letter sequence; for the typical casewhere the Arabic omits diacritics, this often meansthat the English letter sequence is incomplete (e.g.,vowels are often missing).
In the next phase, themodule tries to guess the missing English letters.In the third phase, the module uses a huge collec-tion of English unigrams to filter out improbable orimpossible English words and names.
We de-scribed four possible methods for integrating thismodule in an SMT system.
Two of these methodsrequire NE taggers of higher quality than thoseavailable to us, and were not explored experimen-tally.
Method 3 inserts the top-scoring candidatefrom the transliteration module in the translationwherever there was an Arabic OOV in the source.Method 4 outputs multiple candidates from thetransliteration module, each with a score; the SMTsystem combines these scores with language modelscores to decide which candidate will be chosen.
Inour experiments, Method 4 consistently outper-formed Model 3.
Note that although we usedBLEU as the metric for all experiments in this pa-per, BLEU greatly understates the importance ofaccurate transliteration for many practical SMTapplications.ReferencesNasreen AbdulJaleel and Leah S. Larkey, 2003.
Statisti-cal Transliteration for English-Arabic Cross Lan-guage Information Retrieval, Proceedings of theTwelfth International Conference on Information andKnowledge Management, New Orleans, LAYaser Al-Onaizan and Kevin Knight, 2002.
MachineTransliteration of Names in Arabic Text, Proceedingsof the ACL Workshop on Computational Approachesto Semitic LanguagesPeter F. Brown, Vincent J. Della Pietra, Stephen A.Della Pietra, and Robert L. Mercer, 1993.
TheMathematics of Statistical Machine Translation: Pa-rameter Estimation, Computational LinguisticsHany Hassan and Jeffrey Sorensen, 2005.
An IntegratedApproach for Arabic-English Named Entity Transla-tion, Proceedings of the ACL Workshop on Compu-tational Approaches to Semitic Languages (ACL),University of Michigan, Ann ArborMehdi M. Kashani, Fred Popowich, and Anoop Sarkar,2007.
Automatic Transliteration of Proper Nounsfrom Arabic to English, Proceedings of the SecondWorkshop on Computational Approaches to ArabicScript-based LanguagesAlexandre Klementiev and Dan Roth, 2006.
NamedEntity Transliteration and Discovery from Multilin-gual Comparable Corpora, COLING-ACL, Sidney,AustraliaPhilipp Koehn, Franz Josef Och, and Daniel Marcu,2003.
Statistical Phrase-based Translation, In Pro-ceedings of HLT-NAACL, Edmonton, CanadaFranz Josef Och, 2003.
Minimum Error Rate Trainingfor Statistical Machine Translation, In Proceedingsof the 41th Annual Meeting of the Association forComputation Linguistics, SapporoKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu, 2002.
BLEU: a Method for AutomaticEvaluation of Machine Translation.
In Proceedings23of the 40th Annual Conference of the Association forComputational Linguistics (ACL), Philadelphia, PAFatiha Sadat, Howard Johnson, Akakpo Agbago,George Foster, Roland Kuhn, Aaron Tikuisis, 2005.Portage: A Phrase-base Machine Translation System.In Proceedings of the ACL Workshop on Buildingand Using Parallel Texts, Ann Arbor, MichiganRichard Sproat, Tao Tao, and ChengXiang Zhai, 2006,Named Entity Transliteration with Comparable Cor-pora, COLING-ACL, Sidney, Australia24
