Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 159?162,Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005Competitive Grouping in Integrated Phrase Segmentationand Alignment ModelYing Zhang Stephan VogelLanguage Technologies InstituteSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213{joy+,vogel+}@cs.cmu.eduAbstractThis article describes the competitivegrouping algorithm at the core of our Inte-grated Segmentation and Alignment (ISA)model.
ISA extracts phrase pairs from abilingual corpus without requiring the pre-calculated word alignment as many otherphrase alignment models do.
Experimentsconducted within the WPT-05 shared taskon statistical machine translation demon-strate the simplicity and effectiveness ofthis approach.1 IntroductionIn recent years, various phrase translation ap-proaches (Marcu and Wong, 2002; Och et al, 1999;Koehn et al, 2003) have been shown to outper-form word-to-word translation models (Brown et al,1993).
Many of these phrase alignment strategiesrely on the pre-calculated word alignment and usedifferent heuristics to extract the phrase pairs fromthe Viterbi word alignment path.
The IntegratedSegmentation and Alignment (ISA) model (Zhanget al, 2003) does not require such word alignment.ISA segments the sentence into phrases and findstheir alignment simultaneously.
ISA is simple andfast.
Translation experiments have shown compara-ble performance to other phrase alignment strategieswhich require complicated statistical model training.In this paper, we describe the key idea behind thismodel and connect it with the competitive linking al-gorithm (Melamed, 1997) which was developed forword-to-word alignment.2 Translation Likelihood as a StatisticalTestGiven a bilingual corpus of language pair F (For-eign, source language) and E (English, target lan-guage), if we know the word alignment for each sen-tence pair we can calculate the co-occurrence fre-quency for each source/target word pair type C(f, e)and the marginal frequency C(f) = ?e C(f, e) andC(e) = ?f C(f, e).
We can apply various sta-tistical tests (Manning and Schu?tze, 1999) to mea-sure how likely is the association between f ande, in other words how likely they are mutual trans-lations.
In the following sections, we will use ?2statistics to measure the the mutual translation like-lihood (Church and Hanks, 1990).3 The Core of the Integrated PhraseSegmentation and AlignmentThe competitive linking algorithm (CLA)(Melamed, 1997) is a greedy word alignmentalgorithm.
It was designed to overcome the problemof indirect associations using a simple heuristic:whenever several word tokens fi in one half of thebilingual corpus co-occur with a particular word to-ken e in the other half of the corpus, the word that ismost likely to be e?s translation is the one for whichthe likelihood L(f, e) of translational equivalenceis highest.
The simplicity of this algorithm dependson a one-to-one alignment assumption.
Each wordtranslates to at most one other word.
Thus whenone pair {f, e} is ?linked?, neither f nor e can bealigned with any other words.
This assumptionrenders CLA unusable in phrase level alignment.159We propose an extension, the competitive grouping,as the core component in the ISA model.3.1 Competitive Grouping Algorithm (CGA)The key modification to the competitive linking al-gorithm is to make it less greedy.
When a word pairis found to be the winner of the competition, we al-low it to invite its neighbors to join the ?winner?sclub?
and group them together as an aligned phrasepair.
The one-to-one assumption is thus discardedin CGA.
In addition, we introduce the locality as-sumption for phrase alignment.
Locality states that asource phrase of adjacent words can only be alignedto a target phrase composed of adjacent words.
Thisis not true of most language pairs in cases such asthe relative clause, passive tense, and prepositionalclause, etc.
; however this assumption renders theproblem tractable.
Here is a description of CGA:For a sentence pair {f , e}, represent the word pairstatistics for each word pair {f, e} in a two dimen-sional matrix LI?J , where L(i, j) = ?2(fi, ej) inour implementation.
1Figure 1: Expanding the current phrase pairDenote an aligned phrase pair {f?
, e?}
asa tuple [istart, iend, jstart, jend] where f?
isfistart , fistart+1 , .
.
.
, fiend and similarly for e?.1.
Find i?
and j?
such that L(i?, j?)
is the highest.Create a seed phrase pair [i?, i?, j?, j?]
which issimply the word pair {fi?
, ej?}
itself.2.
Expand the current phrase pair[istart, iend, jstart, jend] to the neighboringterritory to include adjacent source and targetwords in the phrase alignment group.
There1?2 statistics were found to be more discriminative in ourexperiments than other symmetric word association measures,such as the averaged mutual information, ?2 statistics and Dice-coefficient.are 8 ways to group new words into the phrasepair.
For example, one can expand to thenorth by including an additional source wordfistart?1 to be aligned with all the target wordsin the current group; or one can expand to thenortheast by including fistart?1 and ejend+1(Figure 1).Two criteria have to be satisfied for each expan-sion:(a) If a new source word fi?
is to be grouped,maxjstart?j?jend L(i?, j) should be nosmaller than max1?j?J L(i?, j).
SinceCGA is a greedy algorithm as describedbelow, this is to guarantee that fi?
will not?regret?
the decision of joining the phrasepair because it does not have other ?better?target words to be aligned with.
Similarconstraint is applied if a new target wordej?
is to be grouped.
(b) The highest value in the newly-expandedarea needs to be ?similar?
to the seed valueL(i?, j?
).Expand the current phrase pair to the largest ex-tend possible as long as both criteria are satis-fied.3.
The locality assumption means that the alignedphrase cannot be aligned again.
Therefore, allthe source and target words in the phrase pairare marked as ?invalid?
and will be skipped inthe following steps.4.
If there is another valid pair {fi, ej}, then re-peat from Step 1.Figure 2 and Figure 3 show a simple exampleof applying CGA on the sentence pair {je de?clarereprise la session/i declare resumed the session}.Figure 2: Seed pair {je / i}, no expansion allowed160Figure 3: Seed pair {session/session}, expanded to{la session/the session}3.2 Exploring all possible groupingsThe similarity criterion 2-(b) described previouslyis used to control the granularity of phrase pairs.In cases where the pairs {f1f2, e1e2}, {f1, e1} and{f2, e2} are all valid translations pairs, similar-ity is used to control whether we want to align{f1f2, e1e2} as one phrase pair or two shorter ones.The granularity of the phrase pairs is hard to op-timize especially when the test data is unknown.
Onthe one hand, we prefer long phrases since inter-action among the words in the phrase, for exampleword sense, morphology and local reordering couldbe encapsulated.
On the other hand, long phrasepairs are less likely to occur in the test data than theshorter ones and may lead to low coverage.
To haveboth long and short phrases in the alignment, we ap-ply a range of similarity thresholds for each of theexpansion operations.
By applying a low similaritythreshold, the expanded phrase pairs tend to be large,while a higher similarity threshold results in shorterphrase pairs.
As described above, CGA is a greedyalgorithm and the expansion of the seed pair restrictsthe possible alignments for the rest of the sentence.Figure 4 shows an example as we explore all the pos-sible grouping choices in a depth-first search.
In theend, all unique phrase pairs along the path traveledare output as phrase translation candidates for thecurrent sentence pair.3.3 Phrase translation probabilitiesEach aligned phrase pair {f?
, e?}
is assigned a likeli-hood score L(f?
, e?
), defined as:?i maxj logL(fi, ej) +?j maxi logL(fi, ej)|f?
|+ |e?|where i ranges over all words in f?
and similarly j ine?.Given the collected phrase pairs and their likeli-hood, we estimate the phrase translation probabilityFigure 4: Depth-first itinerary of all possible group-ing choices.by their weighted frequency:P (f?
|e?)
= count(f?
, e?)
?
L(f?
, e?)?f?
count(f?
, e?)
?
L(f?
, e?
)No smoothing is applied to the probabilities.4 Learning co-occurrence informationIn most cases, word alignment information is notgiven and is treated as a hidden parameter in thetraining process.
We initialize a word pair co-occurrence frequency by assuming uniform align-ment for each sentence pair, i.e.
for sentence pair(f , e) where f has I words and e has J words, eachword pair {f, e} is considered to be aligned with fre-quency 1I?J .
These co-occurrence frequencies willbe accumulated over the whole corpus to calculatethe initial L(f, e).
Then we iterate the ISA model:1.
Apply the competitive grouping algorithm toeach sentence pair to find all possible phrasepairs.2.
For each identified phrase pair {f?
, e?
}, increasethe co-occurrence counts for all word pairs in-side {f?
, e?}
with weight 1|f?
|?|e?| .3.
Calculate L(f, e) again and goto Step 1 for sev-eral iterations.5 ExperimentsWe participated the shared task in the WPT05 work-shop2 and applied ISA to all four language pairs2http://www.statmt.org/wpt05/mt-shared-task/161(French-English, Finnish-English, German-Englishand Spanish-English).
Table 1 shows the n-gramcoverage of the dev-test set.
French and Spanishdata are better covered by the training data com-pared to the German and Finnish sets.
Since ourphrase alignment is constrained by the locality as-sumption and we can only extract phrase pairs ofadjacent words, lower n-gram coverage will result inlower translation scores.
We used the training dataDev-test DE ES FI FRN=1 99.2 99.6 98.2 99.8N=2 88.2 93.3 73.0 94.7N=3 59.4 71.7 38.2 76.0N=4 30.0 42.9 17.0 50.6N=5 13.0 21.7 6.8 29.8N=16 (8) (65) (1) (101)N=19 (1) (23) (34)N=23 (1) (1)Table 1: Percentage of dev-test n-grams covered bythe training data.
Numbers in parenthesis are theactual counts of n-gram tokens in the dev-test data.and the language model as provided and manuallytuned the parameters of the Pharaoh decoder3 to op-timize BLEU scores.
Table 2 shows the translationresults on the dev-test and the test set of WPT05.The BLEU scores appear comparable to those ofother state-of-the-art phrase alignment systems, inspite of the simplicity of the ISA model and ease oftraining.DE ES FI FRDev-test 18.63 26.20 12.88 26.20Test 18.93 26.14 12.66 26.71Table 2: BLEU scores of ISA in WPT056 ConclusionIn this paper, we introduced the competitive group-ing algorithm which is at the core of the ISA phrasealignment model.
As an extension to the competitivelinking algorithm which is used for word-to-wordalignment, CGA overcomes the assumption of one-to-one mapping and makes it possible to align phrase3http://www.isi.edu/licensed-sw/pharaoh/pairs.
Despite its simplicity, the ISA model hasachieved competitive translation results.
We plan torelease ISA toolkit4 to the community in the nearfuture.ReferencesPeter F. Brown, Vincent J. Della Pietra, Stephen A. DellaPietra, and Robert L. Mercer.
1993.
The mathemat-ics of statistical machine translation: parameter esti-mation.
Comput.
Linguist., 19(2):263?311.Kenneth Ward Church and Patrick Hanks.
1990.
Wordassociation norms, mutual information, and lexicogra-phy.
Comput.
Linguist., 16(1):22?29.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of the Human Language Technology and NorthAmerican Association for Computational LinguisticsConference (HLT/NAACL), Edomonton, Canada, May27-June 1.Christopher D. Manning and Hinrich Schu?tze.
1999.Foundations of statistical natural language process-ing.
MIT Press, Cambridge, MA, USA.Daniel Marcu and William Wong.
2002.
A phrase-based,joint probability model for statistical machine transla-tion.
In Proc.
of the Conference on Empirical Meth-ods in Natural Language Processing, Philadephia, PA,July 6-7.I.
Dan Melamed.
1997.
A word-to-word model of trans-lational equivalence.
In Proceedings of the 8-th con-ference on EACL, pages 490?497, Morristown, NJ,USA.
Association for Computational Linguistics.Franz Josef Och, Christoph Tillmann, and Hermann Ney.1999.
Improved alignment models for statistical ma-chine translation.
In Proc.
of the Conference onEmpirical Methods in Natural Language Processingand Very Large Corpora, pages 20?28, University ofMaryland, College Park, MD, June.Ying Zhang, Stephan Vogel, and Alex Waibel.
2003.
In-tegrated phrase segmentation and alignment algorithmfor statistical machine translation.
In Proceedings ofNLP-KE?03, Beijing, China, October.4http://projectile.is.cs.cmu.edu/research/public/isa/index.htm162
