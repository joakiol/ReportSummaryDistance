BioNLP 2007: Biological, translational, and clinical language processing, pages 97?104,Prague, June 2007. c?2007 Association for Computational LinguisticsA Shared Task Involving Multi-label Classification of Clinical Free TextJohn P. Pestian1, Christopher Brew2, Pawe?
Matykiewicz1,4,DJ Hovermale2, Neil Johnson1, K. Bretonnel Cohen3,W?odzis?aw Duch41Cincinnati Children?s Hospital Medical Center, University of Cincinnati,2Ohio State University, Department of Linguistics,3University of Colorado School of Medicine,4Nicolaus Copernicus University, Torun?, Poland.AbstractThis paper reports on a shared task involvingthe assignment of ICD-9-CM codes to radi-ology reports.
Two features distinguishedthis task from previous shared tasks in thebiomedical domain.
One is that it resulted inthe first freely distributable corpus of fullyanonymized clinical text.
This resource ispermanently available and will (we hope) fa-cilitate future research.
The other key fea-ture of the task is that it required catego-rization with respect to a large and commer-cially significant set of labels.
The numberof participants was larger than in any pre-vious biomedical challenge task.
We de-scribe the data production process and theevaluation measures, and give a preliminaryanalysis of the results.
Many systems per-formed at levels approaching the inter-coderagreement, suggesting that human-like per-formance on this task is within the reach ofcurrently available technologies.1 IntroductionClinical free text (primary data about patients, as op-posed to journal articles) poses significant technicalchallenges for natural language processing (NLP).In addition, there are ethical and social demandswhen working with such data, which is intended foruse by trained medical practitioners who appreciatethe constraints that patient confidentiality imposes.State-of-the-art NLP systems handle carefully editedtext better than fragmentary notes, and clinical lan-guage is known to exhibit unique sublanguage char-acteristics (Hirschman and Sager, 1982; Friedmanet al, 2002; Stetson et al, 2002) (e.g.
verblesssentences, domain-specific punctuation semantics,and unusual metonomies) that may limit the perfor-mance of general NLP tools.
More importantly, theconfidentiality requirements take time and effort toaddress, so it is not surprising that much work inthe biomedical domain has focused on edited jour-nal articles (and the genomics domain) rather thanclinical free text in medical records.
The fact re-mains, however, that the automation of healthcareworkflows can bring important benefits to treatment(Hurtado et al, 2001) and reduce administrative bur-den, and that free text is a critical component ofthese workflows.
There are economic motivationsfor the task, as well.
The cost of adding labels likeICD-9-CM to clinical free text and the cost of re-pairing associated errors is approximately $25 bil-lion per year in the US (Lang, 2007).
For these(and many other) reasons, there have been consis-tent attempts to overcome the obstacles which hin-der the processing of clinical text (Uzuner et al,2006).
This paper discusses one such attempt?The 2007 Computational Medicine Challenge, here-after referred to as ?the Challenge?.
There were twomain reasons for conducting the Challenge.
Oneis to facilitate advances in mining clinical free text;shared tasks in other biomedical domains have beenshown to drive progress in the field in multiple ways(see (Hirschman and Blaschke, 2006; Hersh et al,2005; Uzuner et al, 2006; Hersh et al, 2006) for acomprehensive review of biomedical challenge tasksand their contributions).
The other is a ground-97breaking distribution of useful, reusable, carefullyanonymized clinical data to the research commu-nity, whose data use agreement is simply to cite thesource.
The remaining sections of this paper de-scribe how the data were prepared, the methods forscoring, preliminary results [to be updated if sub-mission is accepted?results are currently still underanalysis], and some lessons learned.2 Corpus collection and coding processSupervised methods for machine learning requiretraining data.
Yet, due to confidentiality require-ments, spotty electronic availability, and variance inrecording standards, the requisite clinical trainingdata are difficult to obtain.
One goal of the chal-lenge was to create a publicly available ?gold stan-dard?
that could serve as the seed for a larger, open-source clinical corpus.
For this we used the follow-ing guiding principles: individual identity must beexpunged to meet United States HIPAA standards,(U.S. Health, 2002) and approved for release by thelocal Institutional Review Board (IRB); the samplemust represent problems that medical records codersactually face; the sample must have enough data formachine-learning-based systems to do well; and thesample must include proportionate representationsof very low-frequency classes.Data for the corpus were collected from theCincinnati Children?s Hospital Medical Center?s(CCHMC) Department of Radiology.
CCHMC?sInstitutional Review Board approved release of thedata.
Sampling of all outpatient chest x-ray and re-nal procedures for a one-year period was done us-ing a bootstrap method (Walters, 2004).
These dataare among those most commonly used, and are de-signed to provide enough codes to cover a substan-tial proportion of pediatric radiology activity.
Ex-punging patient identity to meet HIPAA standardsincluded three steps: disambiguation, anonymiza-tion, and data scrubbing (Pestian et al, 2005).Ambiguity and Anonymization.
Not surprisingly,some degree of disambiguation is needed to carryout effective anonymization (Uzuner et al, 2006;Sibanda and Uzuner, 2006).
The reason is that clini-cal text is dense with medical jargon, abbreviations,and acronyms, many of which turn out to be ambigu-ous between a sense that needs anonymization and adifferent sense that does not.
For example, in a clin-ical setting, FT can be an abbreviation for full-term,fort (as in Fort Bragg), feet, foot, field test, full-timeor family therapy.
Fort Bragg, being a place name,and a possible component of an address, could indi-rectly lead to identification of the patient.
Until suchoccurrences are disambiguated, it is not possible tobe certain that other steps to anonymize data are ad-equate.
To resolve the relevant ambiguities found inthis free text, we relied on previous efforts that usedexpert input to develop clinical disambiguation rules(Pestian et al, 2004).Anonymization.
To assure patient privacy, clin-ical text that is used for non-clinical reasons mustbe anonymized.
However, to be maximally usefulfor machine-learning, this must be done in a par-ticular way.
Replacing personal names with someunspecific value such as ?*?
would lose potentiallyuseful information.
Our goal is to replace the sensi-tive fields with like values that obscure the identityof the individual (Cho et al, 2002).
We found thatthe amount of sensitive information routinely foundin unstructured free text data is limited.
In our case,these data included patient and physician names andsometimes dates or geographic locations, but little orno other sensitive information turned up in the rele-vant database fields.
Using our internally developedencryption broker software, we replaced all femalenames with ?Jane?, all male names with ?John?, andall surnames with ?Johnson?.
Dates were randomlyshifted.Manual Inspection.
Once the data were disam-biguated and anonymized, they were manually re-viewed for the presence of any Protected Health In-formation (PHI).
If a specific token was perceived topotentially violate PHI regulations, the entire recordwas deleted from the dataset.
In some case, how-ever, a general geographic area was changed andnot deleted.
For example if the data read ?patientlived near Mr. Roger?s neighborhood?
it would bedeleted, because it may be traceable.
On the otherhand, if the data read ?patient was from Cincinnati?it may have been changed to read ?patient was fromthe Covington?
After this process, a corpus of 2,216records was obtained (See Table 2 for details).ICD-9-CM Assignment.
A radiology report hasmultiple components.
Two parts in particular areessential for the assignment of ICD-9-CM codes:98clinical history?provided by an ordering physicianbefore a radiological procedure, and impression?reported by a radiologist after the procedure.
In thecase of radiology reports, ICD-9-CM codes serve asjustification to have a certain procedure performed.There are official guidelines for radiology ICD-9-CM coding (Moisio, 2000).
These guidelines notethat every disease code requires a minimum num-ber of digits before reimbursement will occur; thata definite diagnosis should always be coded whenpossible; that an uncertain diagnosis should neverbe coded; and that symptoms must never be codedwhen a definite diagnosis is available.
Particularhospitals and insurance companies typically aug-ment these principles with more specific internalguidelines and practices for coding.
For these rea-sons of policy, and because of natural variation inhuman judgment, it is not uncommon for multipleannotators to assign different codes to the same text.Understanding the sources of this variation is impor-tant; so too is the need to create a definite gold stan-dard for use in the challenge.
To do so, data wereannotated by the coding staff of CCHMC and twoindependent coding companies: COMPANY Y andCOMPANY Z.Majority annotation.
A single gold standard wascreated from these three sets of annotations.
Therewas no reason to adopt any a priori preference forone annotator over another, so the democratic princi-ple of assigning a majority annotation was used.
Themajority annotation consists of those codes assignedto the document by two or more of the annotators.There are, however, several possible problems withthis approach.
For example, it could be that the ma-jority annotation will be empty.
This will be rare(126 records out of 2,216 in our case), because itonly happens when the codes assigned by the threeannotators form disjoint sets.
In most hospital sys-tems, including our own, the coders are required toindicate a primary code.
By convention, the primarycode is listed as the record?s first code, and has anespecially strong impact on the billing process.
Forsimplicity?s sake, the majority annotation process ig-nores the distinction between primary and secondarycodes.
There is space for a better solution here, butwe have not seriously explored it.
We have, how-ever, conducted an analysis of agreement statistics(not further discussed here) that suggests that theoverall effect of the majority method is to create acoding that shares many statistical properties withthe originals, except that it reduces the effect of theannotators?
individual idiosyncrasies.
The majorityannotation is illustrated in Table 1.Our evaluation strategy makes the simplistic as-sumption that the majority annotation is a true goldstandard and a worthwhile target for emulation.
Thisis debatable, and is discussed below, but for the sakeof definiteness we simply stipulate that submissionswill be compared against the majority annotation,and that the best possible performance is to exactlyreplicate said majority annotation.3 EvaluationMicro- and macro-averaging.
Although we ranksystems for purposes of determining the top threeperformers on the basis of micro-averaged F1, wereport a variety of performance data, including themicro-average, macro-average, and a cost-sensitivemeasure of loss.
Jackson and Moulinier comment(for general text classification) that: ?No agree-ment has been reached...on whether one should pre-fer micro- or macro-averages in reporting results.Macro-averaging may be preferred if a classificationsystem is required to perform consistently across allclasses regardless of how densely populated theseare.
On the other hand, micro-averaging may bepreferred if the density of a class reflects its impor-tance in the end-user system?
(Jackson and Moulin-ier, 2002):160-161.
For the present medical ap-plication, we are more interested in the number ofpatients whose cases are correctly documented andbilled than in ensuring good coverage over the fullrange of diagnostic codes.
We therefore emphasizethe micro-average.A cost-sensitive accuracy measure.
While F-measure is well-established as a method for ranking,there are reasons for wanting to augment this witha cost-sensitive measure.
An approach that allowspenalties for over-coding (a false positive) andunder-coding (a false negative) to be manipulatedhas important implications.
The penalty for under-coding is simple?the hospital loses the amount ofrevenue that it would have earned if it had assignedthe code.
The regulations under which coding isdone enforce an automatic over-coding penalty of99Table 1: Majority AnnotationHospital Company Y Company Z MajorityDocument 1 AB BC AB ABDocument 2 BC ABD CDE BCDDocument 3 EF EF E EFDocument 4 ABEF ACEF CDEF ACEFthree times what is earned from the erroneous code,with the additional risk of possible prosecutionfor fraud.
This motivates a generalized version ofJaccard?s similarity metric (Gower and Legendre,1986), which was introduced by Boutell, Shen, Luoand Brown (Boutell et al, 2003).Suppose that Yx is the set of correct labels for a testset and Px is the set of labels predicted by someparticipating system.
Define Fx = Px ?
Yx andMx = Yx ?
Px , i.e.
Fx is the set of false positives,and Mx is the set of missed labels or false negatives.The score is given byscore(Px) =(1?
?|Mx|+ ?|Fx||Yx ?
Px|)?
(1)As noted in (Boutell et al, 2003), if ?
= ?
= 1 thisformula reduces to the simpler case ofscore(Px) =(1?|Yx ?
Px||Yx ?
Px|)?
(2)The discussion in (Boutell et al, 2003) points outthat constraints are necessary on ?
and ?
to ensurethat the inner term of the expression is non-negative.We do not understand the way that they formulatethese constraints, but note that non-negativity will beensured if 0 ?
?
?
1 and 0 ?
?
?
1 .
Since over-coding is three times as bad as undercoding, we use?
= 1.0 , ?
= 0.33 .
Varying the value of ?
wouldaffect the range of the scores, but does not alter therankings of individual systems.
We therefore used?
= 1 .
This measure does not represent the pos-sibility of prosecution for fraud, because the costsinvolved are incommensurate with the ones that arerepresented.
With these parameter settings, the cost-sensitive measure produces rankings that differ con-siderably from those produced by macro-averagedbalanced F-measure.
For example, we shall see thatthe system ranked third in the competition by macro-averaged F-measure assigns a total of 1167 labels,where the second-ranked assigns 1232, and the cost-sensitive measure rewards this conservatism in as-signing labels by reversing the ranking of the twosystems.
In either case, the difference between thesystems is small (0.86% difference in F-measure,0.53% difference in the cost-sensitive measure).4 The DataWe selected for the challenge a subset of the com-prehensive data set described above.
The subset wascreated by stratified sampling, such that it contains20% of the documents in each category.
Thus, theproportion of categories in the sample is the same asthe proportion of categories in the full data set.
Weincluded in the initial sample only those categoriesto which 100 or more documents from the compre-hensive data set were assigned.
After the processsummarized in Table 2, the data were divided intotwo partitions: a training set with 978 documents,and a testing set with 976.
Forty-five ICD-9-CMlabels (e.g 780.6) are observed in these data sets.These labels form 94 distinct combinations (e.g.
thecombination 780.6, 786.2).
We required that anycombination have at least two exemplars in the data,and we split each combination between the train-ing and the test sets.
So, there may be labels andcombinations of labels that occur only one time inthe training data, but participants can be sure thatno combination will occur in the test data that hasnot previously occurred at least once in the train-ing data.
Our policy here has the unintended con-sequence that any combination that appears exactlyonce in the training data is highly likely to appearexactly once in the test data.
This gives unnecessaryinformation to the participants.
In future challengeswe will drop the requirement for two occurrences inthe data, but ensure that single-occurrence combina-tions are allocated to the training set rather than the100test set.
This maintains the guarantee that there willbe no unseen combinations in the test data.
The fulldata set may be downloaded from the official chal-lenge web-site.5 ResultsNotice of the Challenge was distributed using elec-tronic mailing lists supplied by the Association ofComputational Linguistics, IEEE Computer Intelli-gence and Data Mining, and American Medical In-formatics Association?s Natural Language Process-ing special interest group.
Interested participantswere asked to register at the official challenge web-site.
Registration began February 1, 2007 and endedFebruary 28, 2007.
Approximately 150 individu-als registered from 22 countries and six continents.Upon completing registration, an automated e-mailwas sent with the location of the training data.
OnMarch 1, 2007 participants received notice of thelocation of the testing data.
Participants were en-couraged to use the data for other purposes as longas it was non-commercial and the appropriate cita-tion was made.
There were no other data use re-strictions.
Participants had until March 18, 2007to submit their results and an explanation of theirmodel.
Approximately 33% (50) of the partici-pants submitted results.
During the course of theChallenge participants asked a range of questions.These were posted to the official challenge web-site- www.computationalmedicine.org/challenge.The figure below is a scatterplot relating micro-averaged F1 to the cost-sensitive measure describedabove.
Each point represents a system.
The top-performing systems achieved 0.8908, the minimumwas 0.1541, and the mean was 0.7670, with a SDof 0.1340.
There are 21 systems with a micro-averaged F1 between 0.81 and 0.90.
Another 14have F1 > 0.70 .
It is noticeable that the systemsare not ranked identically by the cost-sensitive andthe micro-averaged measure, but the differences aresmall in each case.A preliminary screening using a two-factor ANOVAwith system identity and diagnostic code as predic-tive factors for balanced F-measure revealed a sig-nificant main effect of both system and code.
Pair-wise t-tests using Holm?s correction for multiplecomparisons revealed no statistically significant dif-Figure 1: Scatter plot of evaluation measuresferences between the systems performing at F=0.70or higher.
Differences between the top system and asystem with a microaveraged F-measure of 0.66 docome out significant on this measure.We have also calculated (Table 3) the agreementfigures for the three individual annotations thatwent into the majority gold standard.
We seethat CCHMC outranks COMPANY Y on the cost-sensitive measure, but the reverse is true for micro-and macro-averaged F1, with the agreement be-tween the hospital and the gold standard being espe-cially low for the macro-averaged version.
To under-stand these figures, it is necessary to recall that thegold standard is a majority annotation that is formedfrom the the three component annotations.
It appearsthat for rare codes, which have a disproportionateeffect on the macro-averaged F, the majority anno-tation is dominated by cases where company Y andcompany Z assign the same code, one that CCHMCdid not assign.The agreement figures are comparable to those ofthe best automatic systems.
If submitted to thecompetition, the components of the majority anno-tation would not have outranked the best systems,even though the components contributed to the ma-jority opinion.
It is tempting to conclude that theautomated systems are close to human-level perfor-mance.
Recall, however, that while the hospital andthe companies did not have the luxury of exposureto the majority annotation, the systems did have thataccess, which allowed them to explicitly model theproperties of that majority annotation.
A more mod-erate conclusion is that the hospital and the compa-nies might be able to improve (or at least adjust)their annotation practices by studying the majority101Table 2: Characteristics of the data set through the development process.Step Removed Total documentsOne-year collection of documents 20,27520 percent sample of one-year collection 4,055Manual inspection for anonymization problems 1,839 2,216Removal of records with no majority code 126 2,090Removal of records with a code occurring only once 136 1,954Table 3: Comparison of human annotators against majority.Annotator Cost-sensitive Micro-averaged F1 Macro-averaged F1HOSPITAL 0.9056 0.8264 0.6124COMPANY Y 0.8997 0.8963 0.8973COMPANY Z 0.8621 0.8454 0.8829annotation and adapting as appropriate.6 DiscussionCompared to other recent text classification sharedtasks in the biomedical domain (Uzuner et al, 2006;Hersh et al, 2004; Hersh et al, 2005), this task re-quired categorization with respect to a set of labelsmore than an order of magnitude larger than previ-ous evaluations.
This increase in the size of the setof labels is an important step forward for the field?systems that perform well on smaller sets of cate-gories do not necessarily perform well with largersets of categories (Jackson and Moulinier, 2002), sothe data set will allow for more thorough text cat-egorization system evaluations than have been pos-sible in the past.
Another important contribution ofthe work reported here may be the distribution ofthe data?the first fully distributable, freely usabledata set of clinical text.
The high number of partici-pants and final submissions was a pleasant surprise;we attribute this, among other things, to the fact thatthis was an applied challenge, that real data weresupplied, and that participants were free to use thesedata in other venues.Participants utilized a diverse range of approaches.These system descriptions are based on brief com-ments entered into the submission box, and are ob-viously subject to revision.
The three highest scor-ers all mentioned ?negation,?
all seemed to be us-ing the structure of UMLS in a serious way.
Thebetter systems frequently mentioned ?hypernyms?or ?synonyms,?
and were apparently doing signifi-cant amounts of symbolic processing.
Two of thetop three had machine-learning components, whileone of the top three used purely symbolic methods.The most common approach seems to be thought-ful and medically-informed feature engineering fol-lowed by some variety of machine learning.
Thetop-performing system used C4.5, suggesting thatuse of the latest algorithms is not a pre-requisite forsuccess.
SVMs and related large-margin approachesto machine learning were strongly represented, butdid not seem to be reliably predictive of high rank-ing.6.1 Observations on running the task and theevaluationThe most frequently viewed question of the FAQwas related to a script to calculate the evaluationscore.
This was supplied both as a downloadablescript and as an interactive web-page with a form forsubmission.
In retrospect, we realize that we had notfully thought through what would happen as peoplebegan to use this script.
If we run a similar contestin the future, we will be better prepared for the con-fusion that this can cause.A novel aspect of this task was that although we onlyscored a single run on the test data, we allowed par-ticipants to submit their ?final?
run up to 10 times,and to see their score each time.
Note that although102participants could see how their score varied on suc-cessive submissions, they did not have access to theactual test data or to the correct answers, and so therewere no opportunities for special-purpose hacks tohandle special cases in the test data.
The averageparticipant tried 5.27 (SD 3.17) submissions againstthe test data.
About halfway through the submis-sion period we began to realize that in a competi-tive situation, there are risks in providing the typeof feedback given on the submission form.
In fu-ture challenges, we will be judicious in selecting thenumber of attempts allowed and the provision of anytype of feedback.
As far as we can tell our generalassumption that the scientific integrity of the partic-ipants was greater than the need to game the systemis true.
It is good policy for those administering thecontest, however, to keep temptations to a minimum.Our current preference would be to provide only theweb-page interface with no more than five attempts,and to tell participants only whether their submis-sion had been accepted, and if so, how many itemsand how many codes were recognized.We provided an XML schema as a precise and pub-licly visible description of the submission format.Although we should not have been, we were sur-prised when changes to the schema were requiredin order to accommodate small but unexpected vari-ations in participant submissions.
An even simplersubmission format would have been good.
The ad-vantage of the approach that we took was that XMLvalidation gave us a degree of sanity-checking at lit-tle cost.
The disadvantage was that some of the nec-essary sanity-checking went beyond what we couldsee how to do in a schema.The fact that numerous participants generated sys-tems with high performance indicates that the taskwas reasonable, and that sufficient informationabout the coding task was either provided by us orinferred by the participants to allow them to do theirwork.
Since this is a first attempt, it is not yet clearwhat the upper limits on performance are for thistask, but preliminary indications are that automatedsystems are or will soon be viable as a component ofdeployed systems for this kind of application.7 AcknowledgementsThe authors thank Aaron Cohen of the OregonHealth and Science University for observations onthe inter-rater agreement between the three sourcesand its relationship to the majority assignments, andalso for his input on testing for statistically signif-icant differences between systems.
We also thankPERSON of ORGANIZATION for helpful com-ments on the manuscript.
Most importantly wethank all the participants for their on-going commit-ment, professional feedback and scientific integrity.References[Boutell et al, 2003] Boutell M., Shen X., Luo J. andBrown C. 2003.
Multi-label Semantic Scene Clas-sification, Technical Report 813.
Department of Com-puter Science, University of Rochester September.
[Cho et al, 2002] Cho P. S., Taira R. K., and KangarlooH.
2002 Text boundary detection of medical reports.Proceedings of the Annual Symposium of the AmericanMedical Informatics Association, 998.
[Friedman et al, 2002] Friedman C., Kra P., and RzhetskyA.
2002.
Two biomedical sublanguages: a descrip-tion based on the theories of Zellig Harris.
Journal ofBiomedical Informatics, 35:222?235.
[Gower and Legendre, 1986] Gower J. C. and Legendre P.1986.
Metric and euclidean properties of dissimilaritycoefficient.
Journal of Classification, 3:5?48.
[Hersh et al, 2004] Hersh W., Bhupatiraju R. T., Ross L.,Roberts P., Cohen A. M., and Kraemer D. F. 2004.TREC 2004 Genomics track overview.
Proceedings ofthe 13th Annual Text Retrieval Conference.
NationalInstitute of Standards and Technology.
[Hersh et al, 2006] Hersh W., Cohen A. M., Roberts P.,and Rekapalli H. K. 2006.
TREC 2006 Genomicstrack overview.
Proceedings of the 15th Annual TextRetrieval Conference National Institute of Standardsand Technology.
[Hersh et al, 2005] Hersh W., Cohen A. M., Yang J.,Bhupatiraju R. T., Roberts P., and Hearst M. 2005.TREC 2005 Genomics track overview.
Proceedings ofthe 14th Annual Text Retrieval Conference.
NationalInstitute of Standards and Technology.
[Hirschman and Blaschke, 2006] Hirschman L. andBlaschke C. 2006.
Evaluation of text mining inbiology.
Text mining for biology and biomedicine,Chapter 9.
Ananiadou S. and McNaught J., editors.Artech House.103[Hirschman and Sager, 1982] Hirschman L. and Sager S.1982.
Automatic information formatting of a medi-cal sublanguage.
Sublanguage: studies of language inrestricted semantic domains, Chapter 2.
Kittredge R.and Lehrberger J., editors.
Walter de Gruyter.
[Hurtado et al, 2001] Hurtado M. P, Swift E. K., and Cor-rigan J. M. 2001.
Crossing the Quality Chasm: ANew Health System for the 21st Century.
Institute ofMedicine, National Academy of Sciences.
[Jackson and Moulinier, 2002] Jackson P. and MoulinierI.
2002.
Natural language processing for online appli-cations: text retrieval, extraction, and categorization.John Benjamins Publishing Co.[Lang, 2007] Lang, D. 2007.
CONSULTANT REPORT- Natural Language Processing in the Health Care In-dustry.
Cincinnati Children?s Hospital Medical Cen-ter, Winter 2007.
[Moisio, 2000] Moisio M. 2000.
A Guide to Health CareInsurance Billing.
Thomson Delmar Learning, CliftonPark.
[Pestian et al, 2005] Pestian J. P., Itert L., Andersen C. L.,and Duch W. 2005.
Preparing Clinical Text for Use inBiomedical Research.
Journal of Database Manage-ment, 17(2):1-12.
[Pestian et al, 2004] Pestian J. P., Itert L., and Duch W.2004.
Development of a Pediatric Text-Corpus forPart-of-Speech Tagging.
Intelligent Information Pro-cessing and Web Mining, Advances in Soft Computing,219?226 New York, Springer Verlag.
[Sammuelsson and Wiren, 2000] Sammuelsson C. andWiren M. 2000.
Parsing Techniques.
Handbook ofNatural Language Processing, 59?93.
Dale R., MoislH., Somers H., editors.
New York, Marcel Deker.
[Sibanda and Uzuner, 2006] Sibanda T. and Uzuner O.2006.
Role of local context in automatic deidentifica-tion of ungrammatical, fragmented text.
Proceedingsof the Human Language Technology conference of theNorth American chapter of the Association for Com-putational Linguistics, 65?73.
[Stetson et al, 2002] Stetson P. D., Johnson S. B., ScotchM., and Hripcsak G. 2002.
The sublanguage of cross-coverage.
Proceedings of the Annual Symposium ofthe American Medical Informatics Association, 742?746.[U.S.
Health, 2002] U.S. Heath & Human Services.2002.
45 CFR Parts 160 and 164 Standards for Privacyof Individually Identifiable Health Information FinalRule Federal Register, 67(157):53181?53273.
[Uzuner et al, 2006] Uzuner O., Szolovits P., and KohaneI.
2006. i2b2 workshop on natural language process-ing challenges for clinical records.
Proceedings of theFall Symposium of the American Medical InformaticsAssociation.
[Walters, 2004] Walters S. J.
2004.
Sample size andpower estimation for studies with health related qualityof life outcomes: a comparison of four methods usingthe SF-36 Health and Quality of Life Outcomes, 2:26.104
