Best Analysis Selection in Inflectional LanguagesAles?
Hora?k and Pavel Smrz?Faculty of Informatics, Masaryk University BrnoBotanicka?
68a, 602 00 Brno, Czech RepublicE-mail: {hales,smrz}@fi.muni.czAbstractAmbiguity is the fundamental property ofnatural language.
Perhaps, the most bur-densome case of ambiguity manifests itselfon the syntactic level of analysis.
In orderto face up to the high number of obtainedderivation trees, this paper describes severaltechniques for evaluation of the figures ofmerit, which define a sort order on parsingtrees.
The presented methods are based onlanguage specific features of synthetical lan-guages and they improve the results of sim-ple stochastic approaches.1 IntroductionAmbiguity on all levels of representation isan inherent property of natural languagesand it also forms a central problem of natu-ral language parsing.
A consequence of thenatural language ambiguity is a high num-ber of possible outputs of a parser that areusually represented by labeled trees.
The av-erage number of parsing trees per input sen-tence strongly depends on the backgroundgrammar and thence on the language.
Thereare natural language grammars producingat most hundreds or thousands of parsingtrees but also highly ambiguous grammarsystems producing enormous number of re-sults.
For example, a grammar extractedfrom the Penn Treebank and tested on aset of sentences randomly generated from aprobabilistic version of the grammar has onaverage 7.2?1027 parses per sentence accord-ing to Moore?s work (Moore, 2000).
Such amammoth extent of result is also no excep-tion in parsing of Czech (Smrz?
and Hora?k,2000) (see Fig.
1) due to free word order andFigure 1: The dependence of number of re-sulting analysis on the number of words inthe input sentencerich morphology of word forms whose gram-matical case cannot often be unambiguouslydetermined.A traditional solution for these problemsis presented by probabilistic parsing tech-niques (Bunt and Nijholt, 2000) aiming atfinding the most probable parse of a giveninput sentence.
This methodology is usuallybased on the relative frequencies of occur-rences of the possible relations in a repre-sentative corpus.
?Best?
trees are judged bya probabilistic figure of merit (FOM).The term ?figure of merit?
is usually usedto refer to a function that prunes implausi-ble partial analyses during parsing.
In thispaper, we rather take figure of merit as ameasure bounding the true probabilities ofthe complete parses.SHHHNP1 HHAPHHHHADJ and ADJN1V NP4 HHADJ NP4 HHN4 N2?
?selected trigrams: [ADJ,and,ADJ][ADJ,N1,V][N1,V,N4][V,ADJ,N4][ADJ,N4,N2]Figure 2: Lexical heads as n-gram?s elements.The standard methods of the best analy-sis selection (Caraballo and Charniak, 1998)usually use simple stochastic functions inde-pendent on the peculiarities of the underly-ing language.
This approach seems to worksatisfactorily in case of analytical languages.On the other hand, the obstacles broughtby the synthetical languages in relationshipwith those simple statistical techniques areindispensable.Therefore, we try to improve the standardFOMs taking into consideration specific fea-tures of free word order languages.
The fol-lowing text discusses the assets of three fig-ures of merit that reflect selected phenomenaof the Czech language.2 Figures of MeritThe overall figure of merit of the syntacticanalysis results is determined as a combina-tion of several contributory FOMs that re-flect particular language features such as?
frequency of syntactic constructs repre-sented by pre-computed rule probabili-ties?
augmented n-gram model based on theoccurrence of adjacent lexical headsstanding for the corresponding subtrees?
affinity between constituents modeledby valency frames of verbs, adjectivesand nounsThe selected FOMs participate on the de-termination of the most probable analysis.A straightforward approach lies in the linearcombination of FOMs:?
= ?1 ?
?1 + ?2 ?
?2 + ?3 ?
?3where ?i are the FOMs?
contributions and?i are empirically assigned weights (usuallytaken as normalizing coefficients).
However,our experiments showed that the weights ?ineed to reflect the behaviour of particularlexical items, their categories or even anal-ysed constituents.
We thus need to handlethe ?i variables as functions of various pa-rameters.?
= ?1( ) ?
?1 + ?2( ) ?
?2 + ?3( ) ?
?3The following sections deal with the figuresof merit that play a crucial role in the searchfor the best output analysis.2.1 Rule-tied Actions and ?1 FOMA key question is then what the good can-didates for FOMs are.
The use of proba-bilistic context-free grammars (PCFGs) in-volves simple CF rule probabilities to forma FOM (Chitrao and Grishman, 1990; Bo-brow, 1991).The evaluation of the first FOM is basedon the mechanism of contextual actions builtinto the metagrammar conception (Smrz?
andHora?k, 2000).
It distinguishes four kinds ofcontextual actions, tests or constraints:1. rule-tied actions2.
agreement fulfilment constraints3.
post-processing actions4.
actions based on derivation treeThe rule-based probability estimations aresolved on the first level by the rule-tied ac-tions, which also serve as rule parameteriza-tion modifiers.Agreement fulfilment constraints are usedin generating the expanded grammar (Smrz?and Hora?k, 1999) or they serve also aschart pruning actions.
In terms of (MaxwellIII and Kaplan, 1991), the agreement ful-filment constraints represent the functionalconstraints, whose processing can be inter-leaved with that of phrasal constraints.The post-processing actions are not trig-gered until the chart is already completed.The main part of FOM computation for aparticular input sentence is driven by ac-tions on this level.
Some figures of merit(e.g.
verb valency FOM, see Section 2.3) de-mand exponential resources for computationover the whole chart structure.
This prob-lem is solved by splitting the calculation pro-cess into the pruning part (run on the levelof post-processing actions) and the reorder-ing part, that is postponed until the actionsbased on derivation tree.The actions that do not need to work withthe whole chart structure are run after thebest or n most probable derivation trees areselected.
These actions are used, for exam-ple, for determination of possible verb va-lencies within the input sentence, which canproduce a new ordering of the selected trees.2.2 Augmented n-grams and ?2 FOMThe ?1 FOM is based on rule frequencies andis not capable of describing the contextualinformation in the input.
A popular tech-nique for capturing the relations betweensentence constituents is the n-gram method,which takes advantage of a fast and efficientevaluation algorithm.For instance, (Caraballo and Charniak,1998) presents and evaluate different figuresof merit in the context of best-first chartparsing.
They recommend boundary trigramestimate that has achieved the best perfor-mance on two testing grammars.
This tech-nique, as well as stochastic POS taggingbased on n-gram statistics, achieves satis-factory results for analytical languages (likeEnglish).
However, in case of free word or-der languages, current studies suggest thatthese simple stochastic techniques consider-ably suffer from the data sparseness problemand require a huge amount of training data.The reduction of the number of possibletraining schemata, which correctly keeps thecorrespondence with the syntactic tree struc-ture, is achieved by elaborate selection ofn-gram candidates.
While the standard n-gram techniques work on the surface level,this approach allows us to move up to thesyntactic tree level.
We advantageously usethe ability of lexical heads to represent thekey features of the subtree formed by its de-pendants (see Figure 2).
The principle oflexical heads has shown to be fruitfully ex-ploited in the analysis of free word orderlanguages.
The obtained cut-down of theamount of training data may be also crucialto the usability of this stochastic technique.2.3 Verb Valencies and ?3 FOMOur experiments have shown that, in case ofa really free word order language, the FOMs?1 and ?2 are not always able to discoverthe correct reordering of analyses.
So asto cope with the above mentioned difficul-ties in Slavonic languages (namely Czech),we propose to exploit the language specificfeatures.
Preliminary results indicate thatthe most advantageous approach is the onebased upon valencies of the verb phrase ?
acrucial concept in traditional linguistics.The part of the system dedicated to ex-ploitation of information obtained from a listof verb valencies (Pala and S?evec?ek, 1997)is necessary for solving the prepositional at-tachment problem in particular.
During theanalysis of noun groups and prepositionalnoun groups in the role of verb valenciesin a given input sentence one needs to beable to distinguish free adjuncts or modi-fiers from obligatory valencies.
We are test-ing a set of heuristic rules that determineWith Charles Peter angered at the last meetingNa Karla?
??
?<HUMAN>se Petr rozhne?val na posledn??
sch?uzi?
??
?<ACTIVITY>about the lost advance for payrollkv?uli ztracene?
za?loze na mzdu.?
??
?<RECOMPENSE>Figure 3: Free adjuncts identification by means of lexico-semantic constraints.whether a found noun group typically servesas a free adjunct.
The heuristics are basedon the lexico-semantic constraints (Smrz?
andHora?k, 1999).An example of the application of the heuris-tics is depicted in Figure 3.
In the presentedCzech sentence, the expression na Karla(with Charles) is denoted as a verb argumentby the valency list of the verb rozhne?vat se(anger), while the prepositional noun phrasena schu?zi (at the meeting) is classified asa free adjunct by the rule specifying thatthe preposition na (at) in combination withan <ACTIVITY> class member (in locative)forms a location expression.
The remainingconstituent na mzdu (for payroll) is finallyrecommended as a modifier of the precedingnoun phrase za?loze ([about the] advance).Certainly, we also need to discharge thedependence on the surface order.
Therefore,before the system confronts the actual verbvalencies from the input sentence with thelist of valency frames found in the lexicon,all the valency expressions are reordered.
Byusing the standard ordering of participants,the valency frames can be handled as puresets independent on the current position ofverb arguments.2.4 Preferred Word OrderIn analytical languages, the word order isusually taken as rather fixed and that is whyit can be employed in parsing tree prun-ing algorithms.
However, in case of inflec-tional languages, the approaches to word or-der analysis are diverse.
The most influen-tial theory works with the topic-focus artic-ulation (Sgall et al, 1986).
Although nearlyall rules that could limit the order of con-stituents in Czech sentences can be fully re-laxed, a standard order of participants canbe defined.
A corpus analysis of generaltexts affirms that this preferred word orderis often followed and that it can be advanta-geously used as an arbiter for best analysisselection.Cases where the ?i FOMs do not unam-biguously elect the best candidates can berouted by the preferred word order in theform of functional weights ?i( ) with appro-priate parameters.3 ResultsThis section presents results of experimentswith the stated figures of merit for the bestanalysis selection algorithm.
First, the ac-quisition of training data set derived by ex-ploitation of a standard dependency treebank for Czech is described.
Then, we stepto a comparison of parser running times withthat of another available parser.3.1 The Training Set AcquisitionA common approach to acquiring the sta-tistical data for analysis of syntax employslearning the values from a fully tagged treebank training corpus.
Building of such cor-pora is a tedious and expensive work andit requires a team cooperation of linguistsand computer scientists.
At present the onlysource of Czech tree bank data is the PragueDependency Tree Bank (PDTB) (Hajic?,1998), which includes dependency analysesof about 100 000 Czech sentences.First, in order to be able to exploit thedata from PDTB, we have supplemented ourgrammar with the dependency specificationprecision on sentences percentageof 1-10 words 86.9%of 11-20 words 78.2%of more than 20 words 63.1%overall precision 79.3%number of sentences with 8.0%mistakes in inputTable 1: Precision estimatefor constituents.
Thus the output of theanalysis can be presented in the form of puredependency tree.
In the same time we unifyclasses of derivation trees that correspond toone dependency structure.
We then define acanonical form of the derivation to select onerepresentative of the class that is used for as-signing the edge probabilities.This technique enables us to relate theoutput of our parser to the PDTB data.However, the profit of exploitation of theinformation from the dependency structurescan be higher than that and can run in anautomatically controlled environment.
Forthis purpose, we use the mechanism of prun-ing constraints.
A set of strict limitations isgiven to the syntactic analyser, which passeson just the compliant parses.
The con-straints can be either supplied manually forparticular sentence by linguists, or obtainedfrom the transformed dependency tree inPDTB.The Table 1 summarizes the precision es-timates counted on real corpus data.
Thesemeasurements presented here may discountthe actual benefits of our approach due tothe estimated 8% of mistakes in the inputcorpus.3.2 Running Time ComparisonThe effectivity comparison of differentparsers and parsing techniques brings astrong impulse to improving the actual im-plementations.
Since there is no other gen-erally applicable and available NL parser forCzech, we have compared the running timesof our syntactic analyser on the data pro-vided at http://www.cogs.susx.ac.uk/lab/nlp/carroll/cfg-resources/.These WWW pages resulted from discus-sions at the Efficiency in Large Scale ParsingSystems Workshop at COLING?2000, whereone of the main conclusions was the need fora bank of data for standardization of parserbenchmarking.
The best results reportedon standard data sets (ATIS and PT gram-mars) until today are the comparison databy Robert C. Moore (Moore, 2000).
In thepackage, only the testing grammars with in-put sentences are at the disposal, the releaseof referential implementation of the parser iscurrently being prepared (Moore, personalcommunication).ATIS grammar, Moore?s LC3 + UTF 11.6ATIS grammar, our system 7.2PT grammar, Moore?s LC3 + UTF 41.8PT grammar, our system 57.2Table 2: Running times comparison (in sec-onds)Since we could not run the referential im-plementation of Moore?s parser on the samemachine, the above mentioned times are notfully comparable (we assume that our testswere run on a slightly faster machine thanthat of Moore?s tests).
We prepare a de-tailed comparison, which will try to explainthe differences of results when parsing withgrammars of varying ambiguity level.4 ConclusionsThe methods of the best analysis selectionalgorithm described in this paper show thatthe parsing of inflectional languages calls forsensitive approaches to the evaluation of theappropriate figures of merit.
The case studyof Czech suggests that the use of languagespecific features can improve the results ofsimple stochastic techniques on annotatedcorpus data.Future directions of our research lead toimprovements of the quality of training dataset so that it would cover all the most fre-quent language phenomena.
Our investiga-tions indicate that, in addition to verbs, thebest analysis selection algorithms could alsotake advantage of valency frames of otherPOS categories (nouns, adjectives).ReferencesR.
J. Bobrow.
1991.
Statistical agendaparsing.
In Proceedings of the February1991 DARPA Speech and Natural Lan-guage Workshop, pages 222?224.
San Ma-teo: Morgan Kaufmann.H.
Bunt and A. Nijholt, editors.
2000.
Ad-vances in Probabilistic and Other ParsingTechnologies.
Kluwer Academic Publish-ers.S.
Caraballo and E. Charniak.
1998.
Newfigures of merit for best-first probabilisticchart parsing.
Computational Linguistics,24(2):275?298.M.
Chitrao and R. Grishman.
1990.
Statisti-cal parsing of messages.
In Proceedings ofthe Speech and Natural Language Work-shop, pages 263?266, Hidden Valley, PA.J.
Hajic?.
1998.
Building a syntactically an-notated corpus: The Prague DependencyTreebank.
In Issues of Valency and Mean-ing, pages 106?132, Prague.
Karolinum.J.
T. Maxwell III and R. M. Kaplan.
1991.The interface between phrasal and func-tional constraints.
In M. Rosner, C. J.Rupp, and R. Johnson, editors, Proceed-ings of the Workshop on Constraint Prop-agation, Linguistic Description, and Com-putation, pages 105?120.
Instituto DalleMolle IDSIA, Lugano.
Also in Computa-tional Linguistics, Vol.
19, No.
4, 571?590,1994.R.
C. Moore.
2000.
Improved left-cornerchart parsing for large context-free gram-mars.
In Proceedings of the 6th IWPT,pages 171?182, Trento, Italy.K.
Pala and P. S?evec?ek.
1997.
Valencies ofCzech verbs.
In Proceedings of Works ofPhilosophical Faculty at the University ofBrno, pages 41?54.
Brno.
(in Czech).P.
Sgall, E.
Hajic?ova?, and J. Panevova?.1986.
The Meaning of the Sentenceand Its Semantic and Pragmatic As-pects.
Academia/Reidel Publishing Com-pany, Prague, Czech Republic/Dordrecht,Netherlands.P.
Smrz?
and A. Hora?k.
1999.
Implementa-tion of efficient and portable parser forCzech.
In Text, Speech and Dialogue:Proceedings of the Second InternationalWorkshop TSD?1999, Pilsen, Czech Re-public.
Springer Verlag, Lecture Notes inComputer Science, Volume 1692.Pavel Smrz?
and Ales?
Hora?k.
2000.
Largescale parsing of Czech.
In Proceedings ofEfficiency in Large-Scale Parsing SystemsWorkshop, COLING?2000, pages 43?50,Saarbrucken: Universitaet des Saarlandes.
