Induction of a Simple Morphology for Highly-Inflecting LanguagesMathias Creutz and Krista LagusNeural Networks Research CentreHelsinki University of TechnologyP.O.Box 5400, FIN-02015 HUT, Finland{Mathias.Creutz, Krista.Lagus}@hut.fiAbstractThis paper presents an algorithm for the unsuper-vised learning of a simple morphology of a nat-ural language from raw text.
A generative prob-abilistic model is applied to segment word formsinto morphs.
The morphs are assumed to be gener-ated by one of three categories, namely prefix, suf-fix, or stem, and we make use of some observedasymmetries between these categories.
The modellearns a word structure, where words are allowedto consist of lengthy sequences of alternating stemsand affixes, which makes the model suitable forhighly-inflecting languages.
The ability of the al-gorithm to find real morpheme boundaries is eval-uated against a gold standard for both Finnish andEnglish.
In comparison with a state-of-the-art al-gorithm the new algorithm performs best on theFinnish data, and on roughly equal level on the En-glish data.1 IntroductionWe are intrigued by the endeavor of devising artifi-cial systems that are capable of learning natural lan-guage in an unsupervised manner.
As untagged textdata is available in large quantities for a large num-ber of languages, unsupervised methods may be ap-plied much more widely, or with much lower cost,than supervised ones.Some languages, such as Finnish, Turkish, andSwahili, are highly-inflecting.
We wish to use thisterm in a wide sense including many kinds of pro-cesses for word forming, e.g., compounding andderivation.
Their essential challenge for natural lan-guage applications arises from the very large num-ber of possible word forms, which causes problemsof data sparsity.
For instance, creating extensiveword lists is not a feasible strategy for obtaininggood coverage on the vocabulary necessary for ageneral dictation task in automatic speech recogni-tion.
Instead, a model of the language should in-corporate regularities of how words are formed; cf.e.g., (Siivola et al, 2003; Hacioglu et al, 2003).We will now focus on methods that try to inducethe morphology of a natural language from raw text,that is, on algorithms that learn in an unsupervisedmanner how words are formed.
If a human were tolearn a language in an analogous way, this wouldcorrespond to being exposed to a stream of largeamounts of language without observing or interact-ing with the world where this language is produced.This is clearly not a realistic assumption about lan-guage learning in humans.
However, Saffran etal.
(1996) show that adults are capable of discov-ering word units rapidly in a stream of a nonsenselanguage, where there is no connection to a meaningof the discovered word-like units.
This suggests thathumans do use distributional cues, such as transitionprobabilities between sounds, in language learning.And these kinds of statistical patterns in languagedata can be successfully exploited by appropriatelydesigned algorithms.Existing morphology learning algorithms arecommonly based on the Item and Arrangementmodel, i.e., words are formed by a concatenationof morphemes, which are the smallest meaning-bearing units in language.
The methods segmentwords, and the resulting segments are supposed tobe close to linguistic morphemes.
In addition toproducing a segmentation of words the aim is of-ten to discover structure, such as knowledge ofwhich word forms belong to the same inflectionalparadigm.Typically, generative models are used, either for-mulated in a Bayesian framework, e.g., (Brent,1999; Creutz, 2003); or applying the Minimum De-scription Length (MDL) principle, e.g., (de Mar-cken, 1996; Goldsmith, 2001; Creutz and Lagus,2002).
There is another approach, inspired by theworks of Zellig Harris, where a morpheme bound-ary is suggested at locations where the predictabilityof the next letter in a letter sequence is low, cf.
e.g.,(De?jean, 1998).As it is necessary to learn both which segmentsare plausible morphemes and what sequences ofmorphemes are possible, the learning task is al-Barcelona, July 2004Association for Computations LinguisticsACL Special Interest Group on Computational Phonology (SIGPHON)Proceedings of the Workshop of thehuumori n taju ttom uute nnehumor of sense -less -ness yourFigure 1: Morpheme segmentation of the Finnishword ?huumorintajuttomuutenne?
(?your lack ofsense of humor?
).leviated by making simplifying assumptions aboutword structure.
Often words are assumed to con-sist of one stem followed by one, possibly empty,suffix as in, e.g., (De?jean, 1998; Snover and Brent,2001).
In (Goldsmith, 2001) a recursive struc-ture is proposed, such that stems can consist of asub-stem and a suffix.
Also prefixes are possible.Other algorithms (Creutz and Lagus, 2002; Creutz,2003) have been developed for highly-inflectinglanguages, such as Finnish, where words can con-sist of lengthy sequences of alternating stems andaffixes (see Fig.
1 for an example).
These resem-ble algorithms that segment text without blanks (ortranscribed speech) into words, e.g., (de Marcken,1996; Brent, 1999), in that they do not distinguishbetween stems and affixes, but split words into socalled morphs, which carry no explicit category in-formation.Some algorithms do not rely on the Item and Ar-rangement (IA) model, but learn relationships be-tween words by comparing the orthographic sim-ilarity of pairs of words.
In (Neuvel and Fulop,2002), a morphological learner based on the theoryof Whole Word Morphology is outlined.
Full wordsare related to other full words, and complex wordforms are analyzed into a variable and non-variablecomponent.
Conceivably, in this framework non-concatenative morphological processes, such as um-laut in German, should not be as problematic as inthe IA model.Other algorithms combine information of bothorthographic and semantic similarity of words(Schone and Jurafsky, 2000; Baroni et al, 2002).Semantic similarity is measured in terms of sim-ilar word contexts.
If two orthographically simi-lar words occur in the context of roughly the sameset of other words they probably share the samebase form, e.g.
German ?Vertrag?
vs.
?Vertra?gen?
(treaty).Further cues for morphological learning arepresented in (Schone and Jurafsky, 2001) and(Yarowsky and Wicentowsky, 2000).
The latter uti-lizes frequency distributions over different inflec-tional forms (e.g., how often an English verb oc-curs in its past tense form in comparison to its baseform).
The algorithm is not entirely unsupervised.However, none of these non-IA models suitshighly-inflecting languages as they assume only twoor three constituents per word, analogous to stemand suffix.
In order to cope with a broader rangeof languages we would need the following: On theone hand, words should be allowed to consist of anynumber of alternating stems and affixes, making themodel more flexible than, e.g., the model in (Gold-smith, 2001).
On the other hand, in contrast with(Creutz and Lagus, 2002; Creutz, 2003), sequentialdependencies between morphs, i.e., morphotactics,should be taken into account in order to reduce theerror rate.We present a model that incorporates both ofthese aspects.
Experiments show that the new al-gorithm is able to obtain considerable improve-ments over the segmentation produced by the al-gorithm described in (Creutz, 2003).
Moreover, itperforms better than a state-of-the-art morphology-learning algorithm, Linguistica (Goldsmith, 2001),when evaluated on Finnish data.
In the evaluation,the ability of the algorithms to detect morphemeboundaries are measured against a gold standard forboth Finnish and English, languages with rather dif-ferent types of word structure.2 A probabilistic category-learningalgorithm2.1 Linguistic assumptionsWe use a Hidden Markov Model (HMM) to modelmorph sequences.
Previously, HMM?s have beenused extensively for, e.g., segmentation or taggingpurposes.
The challenge in this task lies in knowingneither the segments (morphs), nor their tags (cat-egories) in advance.
To make the task easier, weutilize the following linguistic assumptions, formu-lated probabilistically:(a) Categorial assumption.
We assume that withrespect to sequential behavior, morphs fall into twomain categories, stems and affixes.
Affixes are fur-ther divided into prefixes and suffixes.
(b) Impossible category sequences.
We want tobe able to cope with languages with extensive com-pounding and many consecutive affixes, but notwith just any sequence.
In particular, we do not wishto allow that a suffix may start a word, or that a pre-fix end it.
Moreover, a prefix should not be followeddirectly by a suffix.
These restrictions are capturedby the following regular expression:word = ( prefix* stem suffix* )+(1)Note that no assumptions are made regardingwhether the language is more likely to employ pre-fixation or suffixation.
(c) Likely properties of morphs in each category.Grammatical affixes mainly carry syntactic infor-mation.
We therefore assume that affixes are likelyto be common ?general-purpose?
morphs that canbe used in connection with a large number of othermorphs.
By contrast, the set of stems is much largerand there are a considerable number of rare stemsthat mainly carry semantic information.
In order forall stems to be distinguishable from each other theyare not likely to be very short morphs.2.2 Probabilistic generative model for wordformationWe use an HMM to assign probabilities to each pos-sible segmentation of a word form.
The word issegmented into morphs, each of which belongs toone category: prefix, suffix, or stem.
We assume afirst-order Markov chain, i.e., a bigram model, forthe morph categories.
For each category, there is aseparate probability distribution over the set of pos-sible morphs.
The probability of a particular seg-mentation of the word w into the morph sequence?1?2 .
.
.
?k is thus:p(?1?2 .
.
.
?k | w) = (2)[k?i=1p(Ci | Ci?1) ?
p(?i | Ci)]?
p(Ck+1 | Ck).The bigram probability of a transition fromone morph category to another is expressed byp(Ci | Ci?1).
For instance, the probability ofobserving a stem after a prefix would be writtenas p(STM | PRE).
The probability of observingthe morph ?i when the category Ci is given is ex-pressed by p(?i | Ci).
The categories C0 andCk+1 represent word boundaries.
That is, we takeinto account the transition from a word boundary tothe first morph in the word, as well as the transitionfrom the last morph to a word boundary.Note also that a morph can be generated from sev-eral categories, e.g., a particular morph can functionas a stem or a suffix depending on the context.2.3 The algorithm step by stepThe algorithm involves the following steps: (i) pro-duction of a baseline segmentation, (ii) initializationof p(?i | Ci) and p(Ci | Ci?1), (iii) removal of re-dundant morphs, and (iv) removal of noise morphs.All steps involve a modification of the morph seg-mentations, except step (ii), where the probabilitydistributions are initialized.Steps (ii)?
(iv) are all concluded with a re-estimation of the probabilities by means ofExpectation-Maximization (EM): The words seg-mented into morphs are re-tagged using the Viterbialgorithm by maximizing Equation 2.
The proba-bilities p(?i | Ci) and p(Ci | Ci?1) are thenre-estimated from the tagged data.
This is repeateduntil convergence of the probabilities.Step (iv) is further followed by a final pass ofthe Viterbi algorithm, which re-splits and tags thewords.
Viterbi re-splitting improves the segmen-tation somewhat, but it is much slower than meretagging.
Therefore Viterbi re-splitting is only per-formed at the final stage.2.3.1 Baseline segmentationA good initial morph segmentation is obtained bythe baseline segmentation method (Creutz and La-gus, 2002).
The choice of baseline segmentationwas motivated by the fact that we wanted the bestpossible segmentation that suits highly-inflectinglanguages.
The baseline algorithm is based on aprobabilistic model that learns a set of morphs, or amorph lexicon, that contains the most likely ?build-ing blocks?
of the word forms observed in the cor-pus used as data.
The learning process is guidedby two prior probability distributions, a prior dis-tribution on morph lengths and a prior distributionon morph frequencies, i.e., the balance between fre-quent and rare morphs.2.3.2 Initialization of the probabilitydistributionsGiven an initial baseline morph segmentationand initial category membership probabilitiesp(Ci | ?k) for each segment (morph), randomsampling of this distribution can be utilized to ob-tain specific tags for the morphs.
From the taggedsegmentation we can estimate the desired valuesp(Cj | Ci) and p(?k | Ci).Below we describe how the initial category mem-bership probabilities p(Ci | ?k) emerge.
These areprobabilities that a particular morph is a prefix, suf-fix, or stem.
In addition, during the process a tem-porary noise category is utilized, to hold segmentsthat cannot be considered as prefix, suffix, or stem.Identifying plausible affixes and stems.
To iden-tify plausible affixes in our corpus we take the base-line splitting and collect information on the contextsthat every discovered morph occurs in.
More specif-ically, we assume that a morph is likely to be a prefixif it is difficult to predict what the following morphis going to be.
That is, there are many possible rightcontexts of the morph.
Correspondingly, a morph islikely to be a suffix if it is difficult to predict whatthe preceding morph can be.We use perplexity to measure the predictabilityof the preceding or following morph in relation toa specific target morph.
The following formula canbe used for calculating the left perplexity of a targetmorph ?:Left-ppl(?)
=[?
?i ?
Left-of(?
)p(?i | ?)]?
1N?
.
(3)There are N?
occurrences of the target morph ?
inthe corpus.
The morph tokens ?i occur to the left of,immediately preceding, the occurrences of ?.
Theprobability distribution p(?i | ?)
is calculated overall such ?i.
Right perplexity can be computed anal-ogously.Next, we implement a graded threshold of suffix-likeness by applying a sigmoid function to the leftperplexity of the morphs.Suffix-like(?)
= [1 + e?a?(Left-ppl(?)?b)]?1.
(4)The parameter b is the perplexity threshold, whichindicates the point where a morph ?
is as likely to bea suffix as a non-suffix.
The parameter a governs thesteepness of the sigmoid.
The equations for prefixare identical except that right perplexity is appliedinstead of left perplexity.As for stems, we assume that the stem-likenessof a morph correlates positively with the length inletters of the morph.
We employ a sigmoid functionas above, which yields:Stem-like(?)
= [1 + e?c?(Length(?
)?d)]?1, (5)where d is the length threshold and c governs thesteepness of the curve.Initial probability of a morph belonging to a cat-egory.
Prefix-, suffix- and stem-likeness assumevalues between zero and one, but they are no prob-abilities, since they usually do not sum up to one.In order to create a probability distribution, wefirst introduce a fourth category besides prefixes,suffixes and stems.
This category is the noise cat-egory and corresponds to cases where none of theproper morph classes is likely.
Typically noisemorphs arise as a consequence of over-segmentationof rare word forms in the baseline word splitting.We set the probability of a morph being noise(NOI) to:p(NOI | ?)
= [1 ?
Prefix-like(?)]?
[1 ?
Suffix-like(?)]
?
[1 ?
Stem-like(?)].
(6)We then distribute the remaining probability massproportionally between prefix (PRE), suffix (SUF),and stem (STM), e.g.
:p(SUF | ?)
=Suffix-like(?)
?
[1 ?
p(NOI | ?)]Prefix-like(?)
+ Suffix-like(?)
+ Stem-like(?)
.
(7)2.3.3 Removal of redundant morphsAs a result of applying the baseline segmentation al-gorithm, there are possibly many redundant morphs,that is, morphs that consist of other discoveredmorphs.
Each morph is studied.
If it is possibleto split it into two other known morphs, the mostprobable split is selected and the redundant morphis removed.
The most probable split is determinedas:arg max?1,C1,?2,C2p(?1 | C1) ?
p(C2 | C1) ?
p(?2 | C2),(8)where C1 and C2 are morph categories, and ?1 and?2 are substrings of the redundant morph ?, suchthat the concatenation of ?1 and ?2 yields ?.However, some restrictions apply: Splitting into?noise morphs?
is not allowed.
Furthermore, forbid-den category transitions are not allowed to emerge,such as a direct transition from a prefix to a suffixwithout going through a stem.
Nor is splitting intosub-morphs with very low probability accepted.2.3.4 Removal of noise morphsAs noise morphs are mainly very short morphs anda product of over-segmentation in the baseline split-ting algorithm, they are removed by joining with ei-ther of the adjacent morphs.
The new morph is thenlabeled as noise.
This is repeated until the resultingmorph can qualify as a stem, which is determinedby the Equation 5.
The following heuristics are ap-plied: Joining with shorter morphs is preferred, andjoining noise with noise or a stem is always pre-ferred to joining with a prefix or a suffix.
These pri-orities are motivated by the observation that noisemorphs tend to be fragments of what should be astem.3 EvaluationWe have produced gold standard segmentationswith marked morpheme boundaries for 1.4 mil-lion Finnish and 36 000 English word forms.
Weevaluate the segmentations produced by our split-ting algorithm against the gold standard, and com-pute precision and recall on discovered morphemeboundaries.
Precision is the proportion of correctboundaries among all morph boundaries suggestedby the algorithm.
Recall is the proportion of correctboundaries discovered by the algorithm in relationto all morpheme boundaries in the gold standard.The gold standard was created semi-automatically, by first running all words througha morphological analyzer based on the two-levelmorphology of Koskenniemi (1983).1 For each1The software was licensed from Lingsoft, Inc. <http:word form, the analyzer outputs the base form ofthe word together with grammatical tags indicating,e.g., the part-of-speech, case, or derivational typeof the word form.
In addition, the boundariesbetween the constituents of compound words areoften marked.
We thoroughly investigated thecorrespondence between the grammatical tagsand the corresponding morphemes and created arule-set for segmenting the original word formswith the help of the output of the analyzer.As there can sometimes be many plausibly cor-rect segmentation of a word we supplied severalalternatives when needed, e.g., English ?evening?
(time of day) vs. ?even+ing?
(verb).
We also intro-duced so called ?fuzzy?
boundaries between stemsand endings, allowing some letter to belong to ei-ther the stem or ending, when both alternatives arereasonable, e.g., English ?invite+s?
vs.
?invit+es?(cf.
?invit+ing?
), or Finnish ?ta?hde+n?
vs.
?ta?hd+en?
(?of the star?
; the base form is ?ta?hti?
).24 ExperimentsWe report experiments on Finnish and English cor-pora.
The new category-learning algorithm is com-pared to two other algorithms, namely the baselinesegmentation algorithm presented in (Creutz, 2003),which was also utilized for initializing the segmen-tation in the category-learning algorithm, and theLinguistica algorithm (Goldsmith, 2001).34.1 Data setsThe Finnish corpus consists of mainly news textsfrom the CSC (The Finnish IT Center for Science)4and the Finnish News Agency.
The corpus consistsof 32 million words and it was divided into a devel-opment set and a test set, each containing 16 millionwords.
For experiments on English we have usedthe Brown corpus5.
It contains one million words,divided into a development set of 250 000 words anda test set of 750 000 words.The development sets were utilized for optimiz-ing the algorithms and for selecting parameter val-ues, whereas the test sets were used solely in thefinal evaluation.//www.lingsoft.fi>.2Our gold standard segmentations for Finnish and Englishwords are not public, but we are currently investigating the pos-sibility of making them public.3We used the December 2003 version of the soft-ware, available at <http://humanities.uchicago.edu/faculty/goldsmith/Linguistica2000/>.4<http://www.csc.fi/kielipankki/>5Available at the Linguistic Data Consortium: <http://www.ldc.upenn.edu>Finnish EnglishWord tokens Word types Word types10 000 5 500 2 40050 000 20 000 7 400250 000 65 000 20 00016 000 000 1 100 000 ?Table 1: Sizes of the Finnish and English test sets.The algorithms were evaluated on different sub-sets of the test set to produce the precision-recallcurves in Figure 2.
The sizes of the subsets areshown in Table 1.
As can be seen, the Finnish andEnglish data sets contain the same number of wordtokens (words of running text), but the number ofword types (distinct word forms) is higher in theFinnish data.
The word type figures are important,since what was referred to as a ?corpus?
in the pre-vious sections is actually a word list.
That is, oneoccurrence of each distinct word form in the data ispicked for the morphology learning task.The word forms in the test sets for which thereare no gold standard segmentations are simply leftout of the evaluation.
The proportions of such wordforms are 5%, 6%, 8%, and 15% in the Finnishsets of size 10 000, 50 000, 250 000 and 16 millionwords, respectively.
For English the proportions are5%, 9%, and 14% for the data sets (in growing or-der).4.2 ParametersThe development sets were used for setting the val-ues of the parameters of the algorithms.
As a cri-terion for selecting the optimal values, we usedthe (equally weighted) F-measure, which is theharmonic mean of the precision and recall of de-tected morpheme boundaries.
For each data sizeand language separately, we selected the configura-tion yielding the best F-measure on the developmentset.
These values were then fixed and utilized whenevaluating the performance of the algorithms on thetest set of corresponding size.In the Baseline algorithm, we optimized the priormorph length distribution.
The prior morph fre-quency distribution was left at its default value.The Category algorithm has four parameters: a,b, c, and d; cf.
Equations 4, and 5.
The constant val-ues c = 2, d = 3.5 work well for every data set sizeand language, as does the relation a = 10/b.
Theperplexity threshold, b, assumes values between 5and 100 depending on the data set.
Conveniently,the algorithm is robust with respect to the value of band the result is always better than that of the Base-line algorithm, except for values of b that are orders30 40 50 60 70405060708090Recall [%]Precision[%]Finnish10k50k250k10k50k250k 16M10k50k250k16MBaselineCategoriesLinguistica(a)60 70 80405060708090Recall [%]Precision[%]English10k50k250k10k50k 250k10k50k 250k(b)Figure 2: Precision and recall of the three algorithms on test sets of increasing sizes on both Finnish (a) andEnglish (b) data.
Each data point is an average of 4 runs on separate test sets, with the exception of the 16M(16 million) words for Finnish (with 1 test set), and the 250k (250 000) words for English (3 test sets).
Inthese cases the lack of test data constrained the number of runs.
The standard deviations of the averages areshown as intervals around the data points.
There is no 16M data point for Linguistica on Finnish, becausethe algorithm is very memory-consuming and we could not run it on larger data sizes than 250 000 wordson our PC.
In most curves, when the data size is increased, recall also rises.
An exception is the Baselinecurve for Finnish, where precision rises, while recall drops.of magnitude too large.In the Linguistica algorithm, we used the com-mands ?Find suffix system?
and ?Find prefixes ofsuffixal stems?.4.3 ResultsFigure 2 depicts the precision and recall of the algo-rithms on test sets of different sizes.When studying the curves for Finnish (Fig.
2a),we observe that the Baseline and Category algo-rithms perform on a similar level on the smallestdata set (10k).
However, from there the perfor-mances diverge: the Category algorithm improveson both precision and recall, whereas the Baselinealgorithm displays a strong increase in precisionwhile recall actually decreases.
This means thatwords are split less often but the proposed splits aremore often correct.
This is due to measuring the costof both the lexicon and the data in the optimizationfunction: with a much larger corpus (more data) theoptimal solution contains a much larger morph lex-icon.
Hence, less splitting ensues.
The effect is notseen on the English data (Fig.
2b), but this might bedue to the smaller corpus sizes.For Linguistica, an increase in the amount of datais reflected in higher recall, but lower precision.Linguistica only suggests a morpheme boundary be-tween a stem and an affix, if the same stem has beenobserved in combination with at least one other af-fix.
This leads to a ?conservative word-splitting be-havior?, with a rather low recall for small data sets,but with high precision.
As the amount of data in-creases, the sparsity of the data decreases, and moremorpheme boundaries are suggested.
This resultsin higher recall, but unfortunately lower precision.As Linguistica was not designed for discoveringthe boundaries within compound words, it missesa large number of them.For Finnish, the Category algorithm is better thanthe other two algorithms when compared on datasets of the same size.
We interpret a result to bebetter, even though precision might be somewhatlower, if recall is significantly higher (or vice versa).As an example, for the 16 million word set, the cate-gory algorithm achieves 79.0% precision and 71.0%recall.
The Baseline achieves 88.5% precision butonly 45.9% recall.
T-tests show significant differ-ences at the level of 0.01 between all algorithmson Finnish, except for Categories vs. Baseline at10 000 words.For English, the Baseline algorithm generallyperforms worst, but it is difficult to say which ofthe other two algorithm performs best.
Accordingto T-tests there are no significant differences at thelevel of 0.05 between the following: Categories vs.Linguistica (50k & 250k), and Categories vs. Base-line (10k).
However, if one were to extrapolatefrom the current trends to a larger data set, it wouldseem likely that the Category algorithm would out-perform Linguistica.4.4 Computational requirementsThe Baseline and Category algorithms are imple-mented as Perl scripts.
On the Finnish 250 000 wordset, the Baseline algorithm runs in 45 minutes, andthe Category algorithm additionally takes 20 min-utes on a 900 MHz AMD Duron processor witha maximum memory usage of 20 MB.
The Lin-guistica algorithm is a compiled Windows program,which uses 500 MB of memory and runs in 90 min-utes, of which 80 minutes(!)
are taken up by thesaving of the results.5 DiscussionIt is worth remembering that the gold standard split-ting used in these evaluations is based on a tradi-tional morphology.
If the segmentations were eval-uated using a real-world application, perhaps some-what different segmentations would be most useful.For example, the tendency to keep common wordstogether, seen in the Baseline model and generallyin Bayesian or MDL-based models, might not be atall troublesome, e.g., in speech recognition or ma-chine translation applications.
In contrast, excessivesplitting might be a problem in both applications.When compared to the gold standard segmen-tation used here, the Baseline algorithm producesthree types of errors that are prominent: (i) exces-sive segmentation especially when trained on smallamounts of data, (ii) too little segmentation espe-cially with large amounts of data, and (iii) erroneoussegments suggested in the beginning of words dueto the fact that the same segments frequently occurat the end of words (e.g.
?s+wing?).
The Categoryalgorithm is able to clearly reduce these types of er-rors due to its following properties: (i) the joiningof noise morphs with adjacent morphs, (ii) the re-moval of redundant morphs by splitting them intosub-morphs, and (iii) the simple morphotactics in-volving three categories (stem, prefix, and suffix)implemented as an HMM.
Furthermore, (iii) is nec-essary for being able to carry out (i) and (ii).The Category algorithm does a good job in find-ing morpheme boundaries and assigning categoriesto the morphs, as can be seen in the examples in Fig-ure 3, e.g., ?photograph+er+s?, ?un+expect+ed+ly?,?aarre+kammio+i+ssa?
(?in treasure chambers?
; ?i?is a plural marker and ?ssa?
marks the inessive case),?bahama+saar+et?
(?
[the] Bahama islands?
; ?saari?means ?island?
and ?saaret?
is the plural form).
Thereader interested in the analyses of other words cantry our on-line demo at http://www.cis.hut.fi/projects/morpho/.It is nice to see that the same morph can betagged differently in different contexts, e.g.
?pa?a?
?is a prefix in ?pa?a?+aihe+e+sta?
(?about [the] maintopic?
), whereas ?pa?a??
is a stem in ?pa?a?+ha?n?
(?in[the] head?).
In this case the morph categoriesalso resolve the semantic ambiguity of the morph?pa?a??.
Occasionally, the segmentation is correct, butthe category tagging differs from the linguistic con-vention, e.g., ?taka+penkki+la?is+et?
(?
[the] ones in[the] back seat?
), where ?la?is?
is tagged as a steminstead of a suffix.The segmentation of ?pa?a?aiheesta?
is not entirelycorrect: ?pa?a?+aihe+e+sta?
contains an superfluousmorph (?e?
), which should be part of the stem, i.e.,?pa?a?+aihee+sta?.
This mistake is explained by acomparison with the plural form ?pa?a?+aihe+i+sta?,which is correct.
As the singular and plural only dif-fer in one letter, ?e?
vs.
?i?, the algorithm has founda solution, where the alternating letter is treated asan independent ?number marker?
: ?e?
for singular,?i?
for plural.In the Linguistica algorithm, stems and suffixesare grouped into so called signatures, which can bethought of as inflectional paradigms: a certain setof stems goes together with a certain set of suffixes.Words will be left unsplit unless the potential stemand suffix fit into a signature.
As a consequence, ifthere is only the plural of some particular Englishnoun in the data, but not the singular, Linguisticawill not split the noun into a stem and the plural?s?, since this does not fit into any signature.
Inthis respect, our category-based algorithm is betterat coping with data sparsity.
For highly-inflectinglanguages, such as Finnish, this is especially impor-tant.In contrast with Linguistica, our algorithms canincorrectly ?overgeneralize?
and suggest a suffix,aarre + kammio + i + ssa ja?a?dy + tta?
+ a?
abandon long + estaarre + kammio + i + sta ja?a?dy + tta?
+ a?
+ kseen abandon + ed long + fellow + ?saarre + kammio + ita ja?a?dy + tta?
+ isi abandon + ing longishaarre + kammio + nsa maclare + n abandon + ment long + itudeaarre + kammio + on nais + auto + ili + ja beauti + ful master + piece + saarre + kammio + t nais + auto + ili + ja + a beauti + fully micro + organ + ism + saarre + kammio + ta nais + auto + ili + joista beauty + ?s near + lybahama + saar + et prot + e + iin + eja calculat + ed necess + arybahama + saari + en prot + e + iin + i calculat + ion + s necess + itiesbahama + saari + lla prot + e + iin + ia con + figur + ation necess + itybahama + saari + lle pa?a?
+ aihe + e + sta con + firm + ed photographbahama + saar + ten pa?a?
+ aihe + i + sta express + ion + ist photograph + er + sedes + autta + isi + vat pa?a?
+ ha?n express + ive + ness photograph + yedes + autta + ko + on pa?a?
+ kin fanatic + ism phrase + dedes + autta + maan pa?a?
+ ksi invit + ation + s phrase + ologyedes + autta + ma + ssa taka + penkki + la?
+ in+ en invit + e phrase + shaap + a + koske + a taka + penkki + la?is + et invit + ed sun + risehaap + a + koske + en voida + kaan invit + e + es thanks + givinghaap + a + koske + lla voi + mme + ko invit + es un + avail + ablehaap + a + koski voisi + mme invit + ing un + expect + ed + lyFigure 3: Examples of segmentations learned from the large Finnish data set and a small English data set.Discovered stems are underlined, suffixes are slanted, and prefixes are rendered in the standard font.where there is none, e.g., ?maclare+n?
(?Mac-Laren?).
Furthermore, nonsensical sequences ofsuffixes (which in other contexts are true suffixes)can be suggested, e.g., ?prot+e+iin+i?, which shouldbe ?proteiini?
(?protein?).
A model with more fine-grained categories might reduce such shortcomingsin that it could model morphotactics more accu-rately.Another aspect requiring attention in the futureis allomorphy.
Currently each discovered segment(morph) is assigned a role (prefix, stem, or suf-fix), but no further ?meaning?
or relation to othermorphs.
In Figure 3 there are some examplesof allomorphs, morphs representing the same mor-pheme, i.e., morphs having the same meaning butused in complementary distributions.
The currentalgorithm has no means for discovering that ?on?and ?en?
mark the same case, namely illative, in?aarre+kammio+on?
(?into [the] treasure chamber?
)and ?haap+a+koske+en?
(?to Haapakoski?).
6 Toenable such discovery in principle, one would prob-ably need to look at contexts of nearby words,not just the word-internal context.
Additionally,one should allow the learning of a model withricher category structure.
Moreover, ?on?
and ?en?do not always mark the illative case.
In ?ba-6Furthermore the algorithm cannot deduce that the illativeis actually realized as a vowel lengthening + ?n?
: ?kammioon?vs.
?koskeen?.hama+saari+en?
the genitive is marked as ?en?, andin ?edes+autta+ko+on?
(?may he/she help?)
?on?marks the third person singular.
Similar examplescan be found for English, e.g., ?ed?
and ?d?
are al-lomorphs in ?invit+ed?
vs.
?phrase+d?, and so are?es?
and ?s?
in ?invit+es?
vs. ?phrase+s?.
However,the meaning of ?s?
is often ambiguous.
It can markeither the plural of a noun or the third person sin-gular of a verb in the present tense.
But this kindof ambiguity is in principle solvable in the currentmodel; the Category algorithm resolves similar, alsosemantic, ambiguities occurring between the threecurrent categories: prefix, stem, and suffix.6 ConclusionsWe described an algorithm that differs from earliermorpheme segmentation algorithms in that it mod-els dependencies between morph categories in se-quences of arbitrary length.
Even a simple modelwith few categories, namely prefix, suffix, and stemis able to capture relevant dependencies that consid-erably improve the obtained segmentation on bothFinnish and English, languages with rather differenttypes of word structure.
An interesting future di-rection is whether the application of more complexmodel structures may lead to improvements in themorphology induction task.AcknowledgmentsWe are grateful to Krister Linde?n, as well as theanonymous reviewers for their valuable and thor-ough comments on the manuscript.ReferencesM.
Baroni, J. Matiasek, and H. Trost.
2002.
Un-supervised learning of morphologically relatedwords based on orthographic and semantic sim-ilarity.
In Proc.
Workshop on Morphological &Phonological Learning of ACL?02, pages 48?57.M.
R. Brent.
1999.
An efficient, probabilisticallysound algorithm for segmentation and word dis-covery.
Machine Learning, 34:71?105.M.
Creutz and K. Lagus.
2002.
Unsupervised dis-covery of morphemes.
In Proc.
Workshop onMorphological and Phonological Learning ofACL?02, pages 21?30, Philadelphia, Pennsylva-nia, USA.M.
Creutz.
2003.
Unsupervised segmentation ofwords using prior distributions of morph lengthand frequency.
In Proc.
ACL?03, pages 280?287,Sapporo, Japan.C.
G. de Marcken.
1996.
Unsupervised LanguageAcquisition.
Ph.D. thesis, MIT.H.
De?jean.
1998.
Morphemes as necessary con-cept for structures discovery from untagged cor-pora.
In Workshop on Paradigms and Groundingin Natural Language Learning, pages 295?299,Adelaide.J.
Goldsmith.
2001.
Unsupervised learning of themorphology of a natural language.
Computa-tional Linguistics, 27(2):153?198.K.
Hacioglu, B. Pellom, T. Ciloglu, O. Ozturk,M.
Kurimo, and M. Creutz.
2003.
On lexi-con creation for Turkish LVCSR.
In Proc.
Eu-rospeech?03, pages 1165?1168, Geneva, Switzer-land.K.
Koskenniemi.
1983.
Two-level morphology:A general computational model for word-formrecognition and production.
Ph.D. thesis, Uni-versity of Helsinki.S.
Neuvel and S. A. Fulop.
2002.
Unsupervisedlearning of morphology without morphemes.
InProc.
Workshop on Morphological & Phonologi-cal Learning of ACL?02, pages 31?40.J.
R. Saffran, E. L. Newport, and R. N. Aslin.
1996.Word segmentation: The role of distributionalcues.
Journal of Memory and Language, 35:606?621.P.
Schone and D. Jurafsky.
2000.
Knowledge-freeinduction of morphology using Latent SemanticAnalysis.
In Proc.
CoNLL-2000 & LLL-2000,pages 67?72.P.
Schone and D. Jurafsky.
2001.
Knowledge-freeinduction of inflectional morphologies.
In Proc.NAACL-2001.V.
Siivola, T. Hirsima?ki, M. Creutz, and M. Kurimo.2003.
Unlimited vocabulary speech recognitionbased on morphs discovered in an unsupervisedmanner.
In Proc.
Eurospeech?03, pages 2293?2296, Geneva, Switzerland.M.
G. Snover and M. R. Brent.
2001.
A Bayesianmodel for morpheme and paradigm identifica-tion.
In Proc.
39th Annual Meeting of the ACL,pages 482?490.D.
Yarowsky and R. Wicentowsky.
2000.
Mini-mally supervised morphological analysis by mul-timodal alignment.
In Proc.
ACL-2000, pages207?216.
