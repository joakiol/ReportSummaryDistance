Description of the American University in Cairo'sSystem Used for MUC-7Christian R. HuyckThe American University in Cairoc.huyck@mdx.ac.ukINTRODUCTION AND BACKGROUNDPortions of the American University in Cairo's MUC-7 system, MUC7-Plink, have par-ticipated in every Message Understanding Competition since MUC-4.
The Plink parser wasdeveloped at the University of Michigan where it formed the core of the systems entered inMUC-4 [2] and MUC-5 [1].
Recently, the Plink parser was added to GATE [6] to facilitateinteraction between language processing modules.
Most of the modules used in MUC7-Plinkwere already in GATE having been imported from the LaSIE system used in MUC-6 [8].GATE provides an environment that greatly simplies the reuse of existing natural languagemodels.
When the call for participation in MUC-7 was made, I was a faculty member at theAmerican University in Cairo, and had several students who were considering participating alongwith me in MUC-7.
I could have easily divided the tasks and had, for instance, one studentwork on the Gazetteer, one work on coreference and perhaps a small group work on discourseinterpretation.
Along with the existing Plink parser this would have comprised a largely newsystem.
Unfortunately, I left Cairo, and had only a very small amount of time to develop thesystem.
Furthermore, I had to develop the system at home on my PC.
Fortunately, GATEalready had all of the modules that I needed, and ran (albeit slowly) on my PC.
I did haveto modify some things, but with a very small amount of eort, I developed a working MUC-7system.Sadly, due to the lack of resources, the results of the system were poor, and by no meansreect the ceiling of the technology.
They do however show how easy it is to perform relativelywell with virtually no development time.The MUC7-Plink system is largely that of the Sheeld system.
It diers largely by the useof an on-line lexicon, and the use of a dierent parser.
This parser was used for the Universityof Michigan's MUC-5 entry, but the grammar and parsing heuristics have been rewritten to takeadvantage of the on-line lexicon, the Gazetteer, and the automated part of speech tagger.
Theparser also produces signicantly dierent output for the XI discourse interpreter [17].In the rest of this document I will rst describe the system; this will include a module bymodule description of the components, and a brief description of GATE.
I will then describe theperformance of the system; this will include a summary of MUC7-Plink's scores on the TE, TRand ST tasks, a brief summary of how development time was spent, and a walk through of thesample article.
I will conclude with a few observations.SYSTEM DESCRIPTIONArchitecture Overview1The MUC7-Plink system was composed of ten modules which were run in succession on eachtext.
In order, they were: Tokenizer Sentence Splitter Tagger (Brill) Gazetteer Morphological Analyzer On-Line Lexicon Plink Parser Name Matcher Discourse Interpreter Template WriterEight of these modules were used in the Sheeld MUC-7 entry.
The only ones that weresubstantially dierent were the Lexicon and the Parser.
I will briey summarize the others andgive a more expansive description of the Lexicon and the Plink parser.tokenizerThe tokenizer reads the input stream and segments it into small chunks that are roughly equiv-alent to words.
It is an executable le compiled from a C program; the C program is generatedfrom a Lex [10] input le.
The token is the most commonly used unit of data for processingin GATE, and the tokenizer guarantees a somewhat uniform representation.
These tokens areadded to the GATE database; a separate database is maintained for each document.
Addition-ally the tokenizer adds section annotations to mark areas of the text.Each of the GATE annotations have a start and end byte which dene a span.
The spanspecies the oset in the document to which the annotation applies.
So, the token associatedwith \Ford" might have the span of byte 152 to 156, while a section might have the span of 0and 562.
Additional annotation specic information might be added to each annotation.sentence splitterThe sentence splitter is a perl script which notes the sentence boundaries.
These boundaries areadded as annotations into the GATE database; the annotation includes the oset of the sentencein the document (the span) and all of the tokens which are constituents of the sentence.taggerThe Brill tagger [3] is a part of speech tagger that has been extensively trained on Wall StreetJournal Text.
It annotates tokens with their part of speech.
Since an annotation already exists2for each token, more information is simply added to each token annotation thus consolidatinginformation.These parts of speech are not entirely compatible with the results of the Gazetteer or theLexicon.
These conicts are resolved before parsing begins.gazetteerThe majority of nominal semantics in the system comes from the Gazetteer.
It is a Lex[10] based system of 44 lists.
Each list represents a dierent semantic category.
The listsinclude companies, airlines, aircraft manufacturers, cities, provinces, titles, rst names,bodies of water and aircraft names among many other things.
There are about 200,000bytes of text in the lists making roughly 10,000 entries.
The system is relatively easy tomodify.
Addition of new elements to the list is simple, and the addition of a new list isalso simple.In addition to lists of proper names, some are lists of key words that signal certainsemantic categories.
For instance there is a list of organization signal words such asUniversity, Hospital and Laboratory.
These words alone are not sucient to mark anorganization, but if they occur next to an unknown proper noun they suggest that thatproper noun is an organization.
This adjacency, and thus categorization, is noted in theparser.The Gazetteer was largely the one used in the Sheeld system.
However, near theend of development, I had to freeze the lists while they were slightly modied at Sheeld.In general this did not matter, but a large number of launch event specic changes werenot completely incorporated.
The largest problem here was that spacecraft were notincorporated.
Since rockets were needed for the scenario template task, virtually noscenario templates were generated.GATE is not a strictly linear system.
Module A must be run before Module B whenB needs information from A.
However, if neither is dependent on the other they can berun independently.1Since the Gazetteer does not depend on morphological analysis, itcould be run before or after the tagger.morphological analyzerThe morphological analyzer takes all nouns and verbs and returns the root form and the sux.The root form is often used as a semantic primitive.
So the semantics for \report" is the sameas the semantics for \reports" or \reporting".
The analysis is done by some regular expressionrules and a list of several thousand irregular exceptions derived from the exception list used inWordnet [16].lexiconThe version of Plink used for MUC-4 and MUC-5 had a hand-crafted lexicon.
Each lexical entryTheoretically, independent modules can be run in parallel, but the current GATE system does notimplement this feature.3was a complex feature structure, and was rather dicult to construct.
Words that were notspecically in the lexicon were assumed to be proper nouns of no particular semantic category.It would be more eective if an on-line lexicon could be used to reduce the work load becausethe lexicon would both ease transition to a new domain, and reduce the time need to maintainPlink's own lexicon.Longman's Dictionary of Contemporary English (LDOCE) [9] has electronic versions.
Oneof these versions was selected, and added to GATE.
The desired word (root form) was passed toLDOCE and it returned the denitions of the word that it found.
Each of these denitions wereadded as there own tokens to the GATE database, with spans that corresponded to the token.Initially, each denition of the word was left in as an annotation.
Plink was allowed to choosebetween the denitions.
Unfortunately, on medium size documents the large number of lexicalentries tended to slow my machine down due to memory limitations.
This meant that somepruning had to be done before addition to the GATE database.The Plink grammar that I developed roughly follows the HPSG [14] formalism.
This requiresrather sophisticated lexical entries.
The addition of LDOCE has enabled me to begin to developa more complex lexical system.
Eventually, these denitions will include semantic and complexsyntactic features which should enable more eective parsing, and more useful semantic resultswhich can be passed along to discourse analysis.The version of LDOCE that I used has semantics and selectional restrictions, but they seemto be inconsistently entered.
Thus the information gathered from LDOCE is currently not veryuseful.plinkThe PLINK parser was designed for the fth Message Understanding Competition (ARPA-93).PLINK does fulledged parsing creating exactly one syntactic-semantic representation of agiven sentence.
Additionally, PLINK parses in linear time thus speeding parsing.
PLINK isclosely related to the Marcus parser [13] using a stack of constituents.
Plink uses a heuristicrule selection mechanism based on the contents of the stack to select which grammar rule toapply at each step.
These heuristics have access to elements of the partially completed parseand select the rules based on a preference mechanism.
The preferential mechanism is based ona small number of rankings (currently 6), so the system can select several rules and rank them.PLINK uses a standard-unication based grammar or UBG [15], and is derived from theLINK parser [11].
The use of a UBG enables PLINK to encode grammar rules that have bothsyntactic and semantic components.
Since the parser has access to syntax and semantics, it cantake advantage of both types of knowledge to make parsing decisions.
This allows parsing toproceed in one-pass and eliminate a great deal of ambiguity.
PLINK also includes an inheritancehierarchy of semantic components.
A more thorough discussion of PLINK and the MUC-5system can be found in [7].The grammar that was used was hand-crafted.
Though it does not adhere to any speciclinguistic theory, it is similar to the HPSG grammar of Pollard and Sag [14].
The grammarrules are quite standard except in many cases they are more amenable to one-pass parsing.
Forinstance left-recursion is avoided.
These rules still recognize the same language, but some gram-4matical manipulation improves one-pass parsing.
Rules to handle agrammatical phenomenonwere derived with HPSG in mind, though of course, they dier from standard HPSG rules.The parsing model is based around a stack and selection rules.
The stack is a standardparsing stack.
Constituents were added to the stack, and when appropriate a grammar rule wasapplied to the stack modifying the top elements of the stack.
I tried to keep the stack small, andin earlier experiments the stack never exceeded a size of seven constituents when it was parsinggrammatical phenomena.At any given time a number of actions could take place.
A new element could be pushedonto the stack or one of a number of grammar rules could be applied.
Selection rules were usedto choose the next action.
Like the grammar rules themselves, the selection rules are themselvesUBG rules.
The selection rules inspect the stack, and give a preference weighting to each of thevalid options.
For example:...detnounperiodExample 1.
A Sample StackHere "..." represents other elements lower on the stack and period (the punctuation mark)represents the most recently added element.
All of the selection rules are unied with the stackand (for the sake of example) two selection rules match.
(Selection-Rule 1 (Selection-Rule 1(good NP-from-det-noun) (best abbrev-eats-period)((1) = det) ((1) = noun(2) = noun (2) = period))(3) = nil))Example 2.
Selection Rules that Match the same stack.If the NP-from-det-noun rule was applied the stack would be changed to...NPperiodExample 3.
Stack If the First Rule is Successfully Applied5If the abbrev-eats-period (abbreviation absorbs a period) rule was applied the stack wouldbe changed to:...NPperiodExample 4.
Stack If the Second Rule is Successfully AppliedWhich of the two rules is actually selected?
Grammar rules are selected based on a preferenceranking.
In the current system the ranking is best, good, fair, last, spec-agram and gen-agram.The best rule is applied rst.
When the stack is as it is in example 1, the abbrev-eats-periodrule is applied rst.
If it succeeds a new round of rule selection begins.
If it fails, then rules fromthe next level, in this case NP-from-det-noun are applied.
This continues until all rules fail.
Ifmultiple grammar rules are selected with the same preference ranking, then they are orderedrandomly.If no rule succeeds a new constituent is pushed onto the stack.
This is could be implementedby the selection rule:(Selection-Rule 1 (gen-agram push))This rule always succeeds and the keyword push is used to push a constituent onto the stack.Other selection rules may take advantage of the push mechanism, when more lexical informationis needed to make a parsing decision.This parsing mechanism allows no backtracking.
Consequently, this assures that the parseoccurs in linear time.
There is evidence that humans backtrack when parsing [4], [5].
In thissense PLINK is not a full-edged model of human parsing.In example 2, I actually specied the names NP-from-det-noun and abbrev-eats-period.
Thisis the actual name of the grammar rule; that is the selection rules actually encode the grammarrule by name.
The name of the grammar rule is specied in the grammar rules (pref name)feature.
The grammar rule for NP-from-det-noun looks like example 5.
(Grammar-Rule NP(((1) = det(2) = N(2 head syn type) = common(pref name) = NP-from-det-noun)))Example 5.
A Sample Grammar Rule6The MUC-7 domain is an open ended domain of newspaper articles.
These articles often havegrammatical and spelling errors.
Furthermore, the lexical mechanisms are not always correct.For example, occasionally words are mis-tagged.
Consequently, the domain is ideal for robustparsing techniques.
The simple technique that PLINK uses for robustness is low ranked rules.High priority rules handle grammatical and specic phenomena; medium priority rules handlegrammatical and general phenomena; low priority rules handle agrammatical phenomenon.A working version of the Plink parser existed by the time of the dry run.
The parser wasin GATE, and was receiving input from earlier modules via the GATE database.
However, thegrammar was designed to recognize general noun phrases.
Some modications had to be made togenerate the appropriate semantic category.
For instance, the parser might encounter \RobertR.
Smith".
This would be correctly recognized as an NP, but it would not state that it was aperson.
For the purposes of all of the MUC tasks, this information was needed.
Consequently,new grammar rules had to be added.
Since Robert is in the Gazetteer, the semantic type of\Robert" would be person and an NP formed from it would also be person.
However, the typeof \R."
and \Smith" would be unknown.
Thus a grammar rule Example 6. was needed.
(Grammar-Rule NP(((1) = NP(2) = N(1 head sem) = person(pref name) = ng-from-NGperson-N(head sem) = person)))Example 6.
A Semantically Specic Grammar RuleExample 6. of course conicted with an already existing grammar rule which took the exactsame constituents, but took the semantics from the second noun.
A higher ranking parsingheuristic was made for the ng-from-NGperson-N grammar rule and it was always selected rst.It only succeeded when the semantics were correct, so non-person NPs were unaected.A total of 11 grammar rules, and 13 selectional rules were added for the MUC task.
Allof these were developed during the training phase and were thus specialized for the aircraftaccident domain.
It would be valid to say that this was the only work done on MUC7-Plink forMUC-7.
These rules were written in a few hours over several afternoons.
One of the advantagesof the Plink approach is the simple integration of domain specic grammar rules.The main modications from the MUC-5 system were a new grammar for a new tag set,and the introduction of lazy unication to speed heuristic rule selection.
The new grammarwas needed since the tag set had changed.
The MUC-5 tag set was specic to our hand-craftedlexicon.
It now uses a combination of the tags used by the Brill tagger, the Gazetteer, andLDOCE.
This has been combined with a hierarchy of syntactic classes, to enable more generalrules to be written.
For example, instead of one syntactic class for comma, and one for each ofthe other punctuations, I have combined this into symbol, but each symbol has a head featurewhich is the symbol.
A general rule can be written to look at the lexical class `sym', or a specic7rule can be written to look at the lexical class `sym' which has a head feature dollar for thedollar sign.Lazy unication is now used during rule selection.
In the MUC-5 system full unication wasused, and this lead to large structures being built unnecessarily.
A future improvement wouldintroduce lazy unication into grammar rule application.
There is evidence that this wouldfurther improve parsing performance [12].Finally, a great deal of modication was needed to produce the correct input for the XIdiscourse interpreter.
Fortunately, this was mostly a matter of post-processing.
Plink standardlyproduces a list of verb frames.
XI wants a list of quasi-logical predicates.
It is relatively simpleto change the frames into predicates.
However, the XI system that was used needed a certainset of predicates.
A large amount of work was needed to assure that the correct predicates werebeing produced.
This is where the majority of work for MUC7-Plink happened.
What wasproduced was a list of entities and relations between entities.
The entities could be based onnouns or on verbs.name matcherThis is a C++ program used as part of the coreference mechanism.
If a name, or part of name,occurs in the list of entities, they are combined into one entity.
This is a useful preprocessingstep for the Discourse Interpreter.discourse interpreterThe discourse interpreter was developed using the XI knowledge representation language [17].The input to the interpreter was a series of entities and relations between entities.
The interpreterhad rules which built new relations and reclassied the entities.
One particular important setof entities and relations was the MUC-7 specic Element, Relations and Scenarios.The only work done for MUC7-Plink was to produce the appropriate input for the discourseinterpreter.
Unfortunately, this work was incomplete, particularly for the nal test domain.This lead to very low recall measures in all three tasks.An additional problem was that the coreference mechanism, which was largely implementedin the discourse interpreter, assumed that entities had a particular property.
However, thisrelation was added by the Plink parser.
This lead to a reduction in precision particularly in theTemplate Element task because entities that corefered in reality were not associated by discourseinterpretation.template writerThe template writer is a prolog program that simply scans through the discourse model.
It looksfor certain types of entities and relations, formats the information for them in an appropriatemanner, and generates the templates which are the results of the system.General Architecture for Text EngineeringThis whole system was developed as a system of the General Architecture for Text Engineeringor GATE [6].
Text processing modules are added to GATE, and these modules can be combined8into a system.
Once modules are added they can be combined in dierent ways to form newsystems.GATE provides a Tipster compatible database mechanism.
The database store is organizedaround documents.
Each document has its own set of annotations.
Modules take input from thedatabase, process the input, and generate output which is then usually placed into the database.The simplest way to add a new module to GATE is by writing a wrapper that interactsdirectly with the database.
The wrapper gets annotations from the database and writes it to ale; the code for the module is then called with the le as input.
It then produces an outputle which is read by the wrapper and put into the database.
Some modules, such as the namematcher, do not communicate this way.
However, integrating a module in this fashion is notvery dicult, and it allows the module to run without GATE if an input le exists.GATE currently has about 40 modules with complete wrappers.
Addition of a new modulevaries in complexity, but can be done in well under an hour for simple systems, and in 2 daysfor complex systems such as the ANLT parser.
Since processing can be independent of GATE,the source language of the new module is irrelevant.
MUC7-Plink has modules written in C, Cderived from Lex, C++, Lisp, Perl and Prolog.SYSTEM PERFORMANCEScoresRecall Precision P&RST 1 43 1.45TR 14 75 23.66TE 36 68 47.40Table 1.
System ResultsMUC7-Plink generated scores for the Scenario task, the Template Relations task and theTemplate Element task.
The scores were lower than expected, but not much lower.
No devel-opment was done on the Launch Event domain.
A small amount of work could have raised theP&R scores to 20 for ST, 40 for TR, and 60 for TE; these are roughly the scores on the tasks inthe Aircraft Accident domain on texts that were run blindly.
Of course a reasonable amount ofwork on the system could have raised the scores much higher.Development TimeThe only way that MUC7-Plink excelled for the MUC-7 competition was development time.
Notime was spent on the Launch Event domain, and very little time was spent on the AircraftAccident domain.
A summary of the time spent in development is below. 15 hours development on Aircraft Accidents 0 hours development on Launch Events9 48 hours on integration into the GATE/LaSIE Discourse Interpreter 80 hours spent adding Plink and LDOCE to GATE 90 hours running the nal testThere was no time spent on development in the nal test domain.
The TE and TR scoresare reasonable because some time was spent in development on the similar training domain ofAircraft Accidents.
48 hours was spent on modifying the output of the Plink parser to t withthe XI discourse interpreter that was used.
This could reasonably be considered part of theMUC-7 eort.
Roughly 80 hours were spent in adding Plink and LDOCE to GATE, in thesummer of 1996.
The integration process has been improved since then, and adding two similarmodules would probably take under 40 hours eort.The majority of the time was spent on running the nal tests.
I was running on a PC-586 at90 MHz, with 16 Meg of RAM.
This lead to very slow processing.
The longest article took over 6hours to process.
An average article to 30 minutes to process up to the discourse interpreter.
10minutes was spent on parsing, and 5 minutes was spent on lexical lookup.
Roughly 10 minutesof the remaining time was spent interfacing with the GATE database.
This is clearly a weaknessof the GATE model and needs to be improved.Discourse analysis was taking much too long, and there would have been no way to run allof the texts on my PC.
Fortunately the GATE approach of reading from the database, writingto a le and then calling the module was very helpful; it enabled me to write input les for thediscourse interpreter, ftp them to a Sun workstation and run them there.
Roughly half of thetexts were run this way, and almost all of the texts over 4000 bytes.The major problem with this long running time was that it left no time for development onthe Launch Event domain.
An overnight run of texts would have enabled development of theMUC7-Plink system to have much higher results.
Still it is quite remarkable that one can enterMUC-7 on a system almost solely run and developed on a low-end PC.WalkthroughI will concentrate on the sentence \The China Great Wall Industry Corp. provided the LongMarch 3B rocket for today's failed launch of a satellite built by Loral Corp. of New York forIntelsat.
"The tokenizer reads in the text and adds annotations like:206 token 1118 1121207 token 1122 1127for the words The and China.
206 refers to the annotation number, and 1118 and 1121 is thespan of the token in the text.The sentence splitter divided the document into sentences including the above sentence asthe annotation:1139 sentence 1118 1275 constituents: 206 207 ....This annotation says it is a sentence that goes from 1118 to 1275 and has the tokens 206, 207etc.10The tagger modies the token annotations by adding part of speech information.206 token 1118 1121 (pos: DT)207 token 1122 1127 (pos: NNP)The Gazetteer looks up words and nds among others:5007 Lookup 1253 1259 (tag: location) (type:city)5008 Lookup 1244 1249 (tag: organization) (type:company)for New York, and Loral respectively.
Note that New York does span two tokens, thus Lookupcan not be directly associated with tokens in the database.The morphological analyzer adds root and sux annotations to verbs and nouns.231 token 1235 1240 (pos: VBD) (root: build) (ax: ed)232 token 1244 1249 (pos: NNP) (root: loral) (ax:)are the annotations for built and Loral.LDOCE looks up words and adds rather complex annotations.
An example is:6428 ldoce entry 1172 1177 (homograph: 0) (sense: 0) (part of speech: -)(grammar info: -) (subject code: -) (case info: -)for the word March.
As noted this information is not currently very useful but slots are leftopen for a more eective lexical retrieval mechanism.The Plink Parser is then run on the sentence and generates a syntactic structure for thesentence, which we will ignore, and a semantic structure for the sentence.
The annotation is:7867 semantics 1118 1275 (qlf: [fail(e251), lobj(e251,e252), launch(e252) ....])The quasi-logical forms that are of interest are: organization(e256), name(e256, oset(1244,1255)), city(e257), name(e257, 'new york'), apposed(e256,e257), of(e256,e257) This says thatLoral Corp. is an organization which has an of relation with the city New York.In this particular text, the name matcher nds no matches.The discourse interpreter nds an of relation between an organization and a location.
Theinterpreter has a rule that adds a location of predicate if this relation holds so a new predicatelocation of(e256,e257) is added.
The discourse interpreter in turn writes information back tothe database.
An example is:8132 xi instance 1118 1275 (class: e7 <{ city( ))) (props: location of(e6,e7),country(e7, 'United States'), of(e6,e7)...)The template writer reads these xi instance annotations and prints the appropriate templateelements and relations for in this case, Loral Corp. and New York.OBSERVATIONSGATE made MUC7-Plink possible.
Without GATE it would have been impossible for me todevelop a system capable of participating in MUC in under a few weeks of work.
GATE does11have some weaknesses: adding a new module to GATE while simple is not transparent; accessingthe database is quite slow.
However, it has been a very useful development environment.Plink has also shown to be quite useful.
It was quite easy to add new rules for a new domainto Plink.
The end result of parsing is easily translated into the quasi-logical form needed by thediscourse interpreter.
This comes from it being a full-parser which generates one interpretation,and generates a full semantic interpretation along with a syntactic one.MUC7-Plink can be most usefully seen as an example of how to build a system that can veryeasily be moved to a new domain.
Assuming a working system, for say the MUC-6 SuccessionEvent task, three main modules need to be modied: the Gazetteer, the Parser and the DiscourseInterpreter.
Using the modules in MUC7-Plink only domain specic data needs to be changedand the actual programs remain constant.The Gazetteer needed several lists changed.
The parser needed to add several grammarrules, and for Plink selection rules, to account for the lists.
Switching to a new domain wouldagain call for new lists and new grammar rules.
However, this data is all based around NounPhrases.
The NE task requires the system to classify several Named Entities.
If there was amore dicult task, an Entity task, which required all Entities to be classied, the system wouldbe more domain independent.
It would still be useful to add new lists and grammar rules toswitch domains, but the introductory work would have been done.
Furthermore, without addingnew lists or grammar rules, some output could be generated.For example, in switching MUC7-Plink from Aircraft Accidents to Launch Events the gram-mar and the Gazetteer provided no space for rockets.
Therefore, rockets could never havearrived as specic semantic output (except when specically mentioned as a rocket).
This iswhy MUC7-Plink performed so badly on the ST task.
It performed better on the TE and TRtask because large parts of those tasks (Organizations, Products and People) were accountedfor by the grammar and the Gazetteer.
If the original system had considered rocket entities, thescores would have been much higher.There was no Discourse Interpretation work done as part of MUC7-Plink.
I simply tookadvantage of the work done at Sheeld.
Clearly, in switching to a new domain, some discoursework would need to be done.
However, the amount of work done at Sheeld on the discoursemodel was also small.
To a large degree this work could be considered looking for specic phe-nomenon in the text, specically, those phenomena required by the ST, and TR tasks.
Perhapsthe new SUMMAC tests will provide better insight into a general discourse interpretation mech-anism which can easily be culled for specic information, but it seems likely a more sophisticatedall-purpose Scenario task would be needed.References[1] Advanced Research Projects Agency.
1993.
Proceedings of the Fifth Message UnderstandingConference (MUC-5), Baltimore, MD.
August 1993.
San Mateo, CA: Morgan KaufmannPublishers.12[2] Defense Advance Research Projects Agency.
1992.
Proceedings of the Fourth Message Un-derstanding Conference, McLean VA. June 1992.
San Mateo, CA: Morgan Kaufmann Pub-lishers.
[3] Brill, E. 1994.
Some advances in transformation-based part of speech tagging.
Proceedingsof AAAI, 1994[4] Crain, S. and M. Steedman.
1985 On not being led up the garden path: the use of context bythe psychological syntax processor.
In Dosty, D., L. Kartunnen and A. Zwicky (eds.)
NaturalLanguage Parsing: Psychological, Computational and Theoretical Perspectives.
New York:Cambridge University Press, pp.
320-358.
[5] Frazier, Lyn.
1983.
Processing Sentence Structure.
In Eye Movements in Reading KeithRanyor (ed.)
New York, NY: Academic Press.
[6] Cunningham, H., Y. Wilks, and R. Gaizauskas.
1996.
GATE: A General Architecture forText Engineering.
CoLing 1996[7] Huyck, Christian R. 1994.
PLINK: An Intelligent Natural Language Parser.
University ofMichigan technical report CSE-TR-218-94.
[8] Gaizauskas, R., T. Wakao, K. Humphreys, H. Cunningham, and Y. Wilks.
1995.
Descriptionof the LaSIE System as Used for MUC-6.
Proceedings of the Sixth Message UnderstandingConference (MUC-6).
San Mateo, CA: Morgan Kaufmann Publishers.
[9] Proctor, P. 1978.
Longman's Dictionary of Contemporary English.
Longman Group.
[10] Levine, J. R., T. Mason, and D. Brown.
1992 Lex and Yacc.
O'Reilly and Associates, Inc.[11] Lytinen, Steven.
1992 A unication-based, integrated natural language processing system.Computers and Mathematics with Applications 23 (6-9), pp.
403-418.
[12] Lytinen, S and N. Tomuro.
1996 Left-corner Parsing for Unication Grammars.
Proceedingof AAAI, 1996[13] Marcus, Mitchell P. 1980 A Theory of Syntactic Recognition for Natural Language Cam-bridge, MA: MIT Press.
[14] Pollard, C. and I.
Sag.
1994.
Head-Driven Phrase Structure Grammar Standford, CA:Centerfor the Study of Language an Information.
[15] Shieber, Stuart M. 1986 An Introduction to Unication-Based Approaches to GrammarStanford, CA:Center for the Study of Language an Information.
[16] Miller, G. 1990.
Wordnet: An on-line lexical database International Journal of Lexicogra-phy, 3(4).
[17] Gaizauskas.
R. 1995.
XI: A knowledge representation language based on cross-classicationand inheritance.
Research Memorandum CS-95-24, Dept.
of Computer Science, Universityof Sheeld.13
