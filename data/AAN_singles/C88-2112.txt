Evaluating Natural Language Systems:A Sourcebook Approach *Walter READAlex QUILICIJohn REEVESMichael DYERArtificial Intelligence Laborato,~jComputer Science DepartmentUniversity of California, Los Angeles, CA, 9002JEva BAKERCenter for the Study of Evaluation School of EducationUniversity of California, Los Angeles, CA, 9002JAbstractThis paper eports progress in development of evaluationmethodologies fornatural language systems.
Without a com-mon classification fthe problems in natural language under-standing authors have no way to specify clearly what theirsystems do, potential users have no way to compare differentsystems and researchers have no way to judge the advan-tages or disadvantages of different approaches todevelopingsystems.in t roduct ion .Recent years have Seen a proliferation of natural language sys-tems.
These include both applied systems uch as databasefront-ends, expert system interfaces and on-line help systemsand research systems developed to test particular theories oflanguage processing.
Each system comes with a set of claimsabout what types of problems the system can "handle".
Butwhat does "handles ellipsis" or "resolves anaphoric reference"actually mean?
All and any such cases?
Certain types?
Andwhat classification of 'types' of ellipsis is the author using?Without a common classification of the problems in natu-ral language understanding authors have no way to specifyclearly what their systems do, potential users have no wayto compare different systems and researchers have no way tojudge the advantages or disadvantages of different approachesto developing systems.
While these problems have been notedover the last 10 years (Woods, 1977; Tennant, 1979), researchdeveloping specific criteria for evaluation of natural anguagesystems has appeared only recently.This paper reports progress in development of evaluationmethodologies for natural language systems.
This work ispart of the Artificial Intelligence Measurement System (AIMS)project of the Center for the Study of Evaluation at UCLA.The AIMS project is developing evaluation criteria for expertsystems, vision systems and natural anguage systems.i~revious Work  on Natura l  Language Evaluat ion.Woods (1977) discussed a number of dimensions along whichnro~re~s in development of natural anguage systems can be*This work reported here is part of the Artificial Intelligence MeasurementSystems (AIMS) Project, which is supported in part by ONR contract numberN00014-S6-K-0395.measured.
In particular, he considered approaches via a %ax-onomy of linguistic phenomena" covered, the convenience andperspicuity of the model used and the time used in processing.As Woods points out, the difficulty of a taxonomic approachis that the taxonomy will always be incomplete.
Any particu-lar phenomenon will have many subclasses and it often turnsout that the pubhshed examples cover only a small part oZthe problem.
A system might claim "handles pronoun refer-ence" but the examples only cover parallel constructions.
Tomake such a taxonomy useful we have to identify as manysubclasses as possible.
On the positive side, if we can buildsuch a taxonomy, it will allow authors to state clearly justwhat phenomena they are making claims about.
It couldserve not only as a description of what has been achieved butas a guide to what still needs to be done.Woods provides a useful discussion of the difficulties in-volved in each of these approaches but offers no specific evalu-ative criteria.
He draws attention to the great effort involvedin doing evaluation by any of these methods and to the im-portance of a "detailed case-by-case analysis".
Our presentwork is an implementation and extension of some of theseideas.Tennant and others (Tennant 1979; Finin, Goodman &Tennant, 1979) make a distinction between conceptual coy..erage and linguistic coverage of a natural anguage systemand argue that systems have to be measured on each of th~edimensions.
Conceptual coverage refers to the range of con-cepts handled by the system and linguistic coverage to therange of language used to discuss the concepts.
Tennant sug-gests a possible xperimental separation between conceptualand linguistic overage.The distinction these authors make is important and use-ful, in part for emphasizing the significance of the knowledgebase for usability of a natm'al language system.
But the ex-amples that Tennant gives for conceptual completeness -presupposition, reference to discourse objects - -  seem to be530part of a continuum with topics like ellipsis and anaphora,which are more clearly linguistic.
Fox" this reason we don'tdraw a sharp distinction here.
We prefer to look at the broad-es~ possible range of language use.
Insofar as recognizing pre-supposition~ depends on the structure of Sheknowt~,dge base,we ~ote theft in the examples.
In any case, the question ofevaluating ~he linguistic overage is still open.Bars ar,.d Guida (1984) give a general overview of issues~n evahlation of natural anguage systems.
They emphasizethe import~mce is measuring competence, what the system iscapable of doing, over performance, what users actually dowith ~he system.
We agree with the emphasis.
But how dowe measure competence?Guida and Mauri (1984, 1986) present he most formaland detailed approach to e~luation of natural anguage sys-tems.
Ttmy consider a natural anguage system as a functionfl'om input;~ to (sets of) outputs.
Assuming a measure of error(closeness of the Output to tile correct output) and a measureof the importance of each input, they ewluate the systemby the sum of the errors weighted by the importance of theinput.
It i.
'; assumed that the user can assign these measuresin some r~sonable way.
They give some suggestions for thisassignmeni; and work out a small example in detail.The advantage of a careful, formal analysis is that it fo-cuses art(ration on the key role of the 'importance' and 'er-ror' measures.
In practice, the importance measure has to begiven over categories of input.
The difficulty is determiningwhat these categories are for a natural anguage.
A systemthat hundred five types d ellipsis but not the type the usermost nee(1,~ would be of little use.
If the user has a descriptionof the varieties of issues involved, he can define his specific~meds and give his own weights to the different categories.The Sourcebook Pro jectThe natural anguage part, of the AIMS project has two parts.The first task is to deveh~p methods for describing the cover-age of natural anguage systerrm.
To this end, we are buildinga database of 'exemplars' of representative problems in nat--ural language undcrstanding, mostly from the computationallinguistics literature.
Each exemplar includes a piece of text(sentenee~ dialogue fragment, etc.)
a description of the con-ceptual issue represented, a detailed discussion of the prob-ler~m in understanding the text and a reference to a moreextensive discussion in the literature.
(See appendix A forexamples.)
The Sourcebook consists of a large set of theseexemplars a~td a conceptual taxonomy of the types of issuesrepresented in the database.
The exemplars are indexed bysource in the literature and by conceptual class of the issueso that the user can readily access the rele~rant examples.The Sourcebook provides a structural representation f thecoverage that can be expected of a natural anguage system.The second task of our group is to develop methods for a~process evaluation' of n~tural language systexrm.
A processevahlation includes questions of efficiency, perspicuity andconceptual coverage in the sense of Tennant.
We are inter?ested in the tearnability of a system~ in how well the modelis documented, in how easily the system can be extended,etc.
Generally, we are interested in how the system actuMlyworks, including the user interface.
The criteria we developwill be applied to representative existing systems.
In thispaper we focus on the Soureebook.Why a Sourcebook?In developing evMuative criteria for linguistic coverage wehad several goals we wanted to achieve.
First, the criteriaused should be applicable over the broadest possible range ofsystems and still provide comparability of the systems.
Thecriteria should be relevant to even very innovative approaches.In fact, the criteria should let the developers of the systemdescribe xactly what is innovative about the system.
Sec-ond, the criteria should be independent of impleamntationissues including programming language.
A complete analysisof a particular system would of coui'se include implementationdetails.
But it should be possible to describe the coverage in-dependent ofsuch details.
Only in this way do we have a basisfor claiming an advantage for new implementations or repre-sentations.
Third, the system shouldn't just rate the syste mon a pass/fail count.
It should outline areas of competenceso that implementers and researchers can see where furtherwork is needed within their system or their paradigm.
Theyshould be able to say "this approach andles types 1, 2 and3 of ellipsis but not types 4 and 5 yet" rather than "this ap-proach handles ellipsis".
Fourth, the criteria used should becomprehensible to the general user and to researchers outsidecomputational linguistics.
For one thing, as Tennant noted,users are less deterred by, say, syntactic limitaLions than bylimitations in the system's concepts, discourse ability, abil-ity to understand the user's goals, etc.
We need to presentthe issues in such a way that the user can make judgnmntsabout the importance of different components of the e~Mua,-tion.
This means presenting the issues in terms of the generalprinciples involved and giving concrete xamples.
This ap-proach also allows us to bring in information fi'om areas likepsychology, sociology, law and literary analysis and enablesresearchers in those areas to contribute to the evaluation.
Afifth point is more a negative point.
We don't expect o beable to judge any system by one or even a few numbers.
Ourgoal is to find a way to describe and to compare the coverageof systems.One method often used in computer science to test pro-grams is a test suite and these have been used for naturallanguage valuation.
Test suites have the advantage of sim-plicity and precision.
Hewlett-Pacl~rd presented one suchsuite covering a variety of tests of English syntax at the 1987Association for CompugationM Linguistics meeting.
But thisapproach is very limited.
Although a parser passed one ex-ample of a "Bach-Peters sentence (periphery)", it might failon: another very similar sentence which is conceptually dif-ferent.
(This test suite doesn't measure how well the sys-tem understands what's going on.)
The categories ate thosederived from a particular syntactic theory, rather than cate-gories that users work with.
The test suite tests only a very531limited range of linguistic phenomena and the test is simplypass/fail.
And when a sentence fails to pass, it's not alwaysclear why without looking at the implementation.
For thereasons mentioned here, we looked for a more generally use-ful method than test suites.Rather than start with a particular theory of language,we began with a search of the computational linguistics hter-ature.
While no-one would claim that computational linguis-tics has discovered, let alne solved, every problem in lan-guage use, twenty-five years of research as covered a broadrange of problems.
Looking at language use computationallyfocuses attention on phenomena that are often neglected inmore theoretical nalyses.
Building systems intended to readreal text or interact with real users raises complex problemsof interaction of linguistic phenomena.
The exemplars aremostly taken from the literature although we have added ex-amples to fill in gaps where we felt the published exampleswere incomplete.
Because many of the published cases in-volved particular systems, the examples are often discussedin the literature in relation to that system.
In the exemplars,we analyze the example in terms of the general issue repre-sented.
Then the exemplars are groupe d 'into categories ofrelated problems.
This generates the hierarchical classifica-tion of the issues.
We don't start with an a priori theory forthis classification but rather look for patterns in the exem-plars.
(A surmnary of the first two layers of the hierarchicalclassification is in Appendix 2.
)By drawing examples from the full range of the litera-ture, includin@i~ot nly successful examples but unsuccessfulones, the.~ourcelJo.ok gives a broad view of linguistic phe-nomena.
Although published examples are often about im-plementations, we have focused on examples that illustratemore general issues.
The classification of the examples mapsthe overall topology of the issues and describes both areascovered and areas not covered.
Finally, by defining the is-Sues through specific examples and conceptual c assification,rather than implementation details or linguistic theories, theSourcebook is accessible to non-specialists in computationallinguistics.In the hierarchical c assification, groups I, II and III roughlymatch stages of development in natural anguage systems.They correspond to simple database query systems (I), databasmsystems capable of extended interaction (II) and systems whereknowledge flow between user and system goes both ways (III).Type III systems will be needed for, e.g., intelligent interfacesto expert systems.
Progress on problems in areas I, II and IIIcan be considered as describing first, second and third gener-ation natural language systems, respectively.Cont inuing 'and Future WorkWe are continuing to add exemplars to the Sourcebook andare elaborating the classification scheme.
We will be makingthe Sourcebook available to other researchers for commentand analysis.We have several hundred exemplars and we estimate thatwe have covered 10 per cent of the relevant literature (jour-532nals, proceedings volumes, dissertations, major textbooks) incomputational linguistics, artificial intelligence and cognitivescience.
Our intention is to be as exhaustive as possible.Which leaves us with a very ambitious project.We are also continuing work on the process evaluationmethodologies.Appendix I: Sample ExemplarsExemplar  1(1) I heard an earthquake singing in the shower.
(2) I heard an earthquake sing in the shower'.
(Wilks, 1986~p.
199)Top icCase ambiguity.D iscuss ionIn (1), we know the speaker is the one singing in the shower.How?
Because we know that earthquakes don't sing.
Scit is likely that there is a missing "while" and the speakerheard an earthquake while singing in the shower.
However:that reasoning fails on (2).
In that sentence, the earthquake L,singing, not the person in the Shower.
A selectional restrictionthat says earthquakes don't sing will work in understandin~(1) but fail for (2).
How is the correct actor for actions likesinging determined?ReferencesYorick Wilks.
(1986).
An Intelligent Analyzer and Under.-stander of English.
In Barbara J. Grosz, Karen Sparck Jones~and Bonnie Lynn Webber (Eds.
), Readings in Natural Lau.guage Processing.
Morgan Kaufman.
Page 199.Exemplar  2User: Add a dual disk to the order.System: A dual ported disk.
What storage capacity?
(Car?bonell & Hayes, 1983, p. 133)Top icIntersentential El ipsis - -  Echo'DiscussionThe response by the system is a form of elaboration ellip-,sis.
The system intends to confirm the missing informationand gather more needed information without interrupting theconversational flow.
In each case, the utterance must be rec?ognized as referring to the topic introduced by the user.
Thiskind of cooperative dialogue is very common when the userbelieves that he is dealing with someone who understandsnatural language.
We often assume that "understanding law'guage" means understanding the user's goals and sharingcommon assumptions.ReferenceCarbonell, James G. & Hayes, Philip J.
(1983).
Recoverystrategies for parsing extragrammatieal language.
AmericanJournal o\] Computational Linguistics, 9, 123-146.Exemplar 3J im Fixx had a heart attack while jogging.TopicRecognizing interesting information - -  situational irony.DiscussionSyntactically and semantically this sentence is straightfor-ward.
However, to a reader who knows Jim Fixx as an authorof books plomoting jogging for health the information is veryinteresting.
The interest comes from the irony of the situa-tion.
,~ow ~s the irony, the point of the sentence, recognized?Extracting the irony requires accessing the relevant beliefs ofthe characters and recognizing violations of beliefs.ReferencePeter Norvig.
(1983).
Six Problems for Story Understanders.Proceedings of The National Conference on Artificial Intelli-gence, 284.287.Dyer, M. G., Flowers, M. & Reeves, J. F. (in press).
Recog-nizing Situational Irony: A Computer Model of Irony Recog-nition and Narrative Understanding.
Advances in Computingand the Hnmanities, I(1).Exemplar  4(1) I want to meet the chairman of Chrysler.
(2) I want to be the chairman of Chrysler.
(3) I want to be the Newton of AI.TopicDefinite reference - referential vs attributive.Discuss ionResolving definite references requires that the system distin-guish between referential and attributive uses.
In (1), 'thechairman of Chrysler' refers to the current holder of that po-sition, pr~,ently Lee Iacocca.
But in (2) the speaker doesn'twant to be Iacocca but rather to hold the job Iacocca holds.In (1) 'the chairman of Chrysler' is said to be referentialbecause it refers to a specific object.
In (2) it is said tobe attributive because it describes a characteristic or set ofcharacteri.~tics.
In (3) the use is metaphorical, referring tothe historical role that Newton played in physics rather thanany particular job Newton held.
For example, it doesn't notmean that the speaker wants to be the AI equivalent of thedirectox" of the mint in England.
Recognizing the reference inthes 9 cases requires that the sysiem be able to process everallevels of abstraction and, especially for (3), to access worldknowledge.
(Cf.
Allen, p.
355.
)ReferenceAllen, J. F. (1987).
Natural Language Understanding.
MenloPark, CA: Benjamin/Cummings.Exemplar 5(1) The next day after we sold our car, the buyer returnedand wanted his money back.
(Allen, 1987, p. 346)(2) The day after we sold our house, the escrow companywent bankrupt.
(3) The day after we sold our house~ they put in a traffic lightat the corner.TopicAnaphoric reference - roles.DiscussionIn (1) the 'buyer' refers back to a figure in one of the rolesin the 'selling a car ~ event.
The system must search not onlythe direct possible antecedents (the 'selling') but must alsoconsider aspects of the selling to resolve the 'buyer' reference.In (1), there is nothing specific to 'car' about resolving thereference.
But in (2), finding the reference of 'the escrowcompany' involves looking past the general "buying" scriptand searching through aspects of selling specific to sellinghouses.
This might require extensive local knowledge of thetypical ways in which houses are bought and sold in tilts area.There is a general problem here with controlling the amountof search while still looking deeply enough.
In (3), the systemhas to go from the house to the location of the house to thestreet that runs past the house to the corner at a nearbyintersection of the street o understand the reference.ReferenceAlien, J. F. (1987).
Natural Language Understanding.
MenloPark, CA: Benjamin/Curmnings.Append ix  2: Br ie f  V iew of  the  Evo lwing Classif icationI.
Single-utterance issues.A.
Identification of syntactic units.B.
Ambiguity.i.
Lexical ambiguity.it.
Case ambiguity.C.
Modifier attachment.D.
Reference.E.
Metaphor and novel anguage.F.
Other533II.
Connected-utterance issues.A.
Anaphora.B.
Ellipsis.i.
Intersentential ellipsis.a.
Intersentential ellipsis - -  echo,C.
Integrating complex information.D.
Reasoning, argumentation, story understanding.i.
Interest.a.
Irony.E.
Other.II I. True-dialogue issues.A.
Recognizing user goals and plans.B.
Using and modifying models of the user's knowledge.C.
Recognizing logical presuppositions.D.
Speech acts.E .
Meta-linguistic discourse.F.
Other.IV.
Generation.V.
Ill-formed input.BibliographyBarn, B. G. & Guida, G. (1984).
Competence and Perfor-:mance in the Design of Natural Language Systems.
In B. G.Barn & G. Guida (Eds.
), Computational Models of NaturalLanguage Processing (pp.
1-7).
Amsterdam: North-Holland.Finin, T., Goodman, B.
& Tennant, H./1979).
JETS: Achiev-ing Completeness through Coverage and Closure.
Proceedingsof the Sizth International Joint Conference on Artificial In-telligence, 275-281.Guida, G. & Mauri, G. (1984).
A Formal Basis for Perfor-,manee Evaluation of Natural Language Understanding Sys-tems.
Computational Linguistics, 10, 15-30.Guida, G. & Mauri, G. (1986).
Evaluation of Natural Lan-guage Processing Systems: Issues and Approaches.
Proceed-ings of the IEEE, 74, i026-1035.Tennant, H. (1979).
Experience with the Evaluation of Nat-ural Language Question Answerers.
Proceedings of the SizthInternational Joint Conference on Artificial Intelligence, 874-876.Woods, W. A., (1977).
A Personal View of Natural LanguageUnderstanding.
SIGART Newsletter, 17-20.534
