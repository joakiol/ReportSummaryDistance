Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 709?712,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsArabic Mention Detection: Toward Better Unit of AnalysisYassine BenajibaCenter for Computational Learning SystemsColumbia Universityybenajiba@ccls.columbia.eduImed ZitouniIBM T. J. Watson Research Centerizitouni@us.ibm.comAbstractWe investigate in this paper the adequate unitof analysis for Arabic Mention Detection.
Weexperiment different segmentation schemeswith various feature-sets.
Results show thatwhen limited resources are available, modelsbuilt on morphologically segmented data out-perform other models by up to 4F points.
Onthe other hand, when more resources extractedfrom morphologically segmented data becomeavailable, models built with Arabic TreeBankstyle segmentation yield to better results.
Wealso show additional improvement by combin-ing different segmentation schemes.1 IntroductionThis paper addresses an important and basic task ofinformation extraction: Mention Detection (MD)1:the identification and classification of textual refer-ences to objects/abstractions (i.e., mentions).
Thesementions can be either named (e.g.
Mohammed,John), nominal (city, president) or pronominal (e.g.he, she).
For instance, in the sentence ?PresidentObama said he will visit ...?
there are three men-tions: President, Obama and he.
This is similarto the Named Entity Recognition (NER) task withthe additional twist of also identifying nominal andpronominal mentions.
We formulate the mention de-tection problem as a classification problem, by as-signing to each token in the text a label, indicatingwhether it starts a specific mention, is inside a spe-cific mention, or is outside all mentions.
The se-lection of the unit of analysis is an important steptoward a better classification.
When processing lan-guages, such as English, using the word itself as the1We adopt here the ACE nomenclature:http://www.nist.gov/speech/tests/ace/index.htmlunit of analysis (after separating punctuations) leadsto a good performance (Florian et al, 2004).
Forother languages, such as Chinese, character is con-sidered as the adequate unit of analysis (Jing et al,2003).
In this paper, we investigate different seg-mentation schemes in order to define the best unit ofanalysis for Arabic MD.
Arabic adopts a very com-plex morphology, i.e.
each word is composed of zeroor more prefixes, one stem and zero or more suffixes.Consequently, the Arabic data is sparser than otherlanguages, such as English, and it is necessary to?segment?
the words into several units of analysis inorder to achieve a good performance.
(Zitouni et al, 2005) used Arabic morphologicallysegmented data and claimed to have very competi-tive results in ACE 2003 and ACE 2004 data.
On theother hand, (Benajiba et al, 2008) report good re-sults for Arabic NER on ACE 2003, 2004 and 2005data using Arabic TreeBank (ATB) segmentation.
Inall published works, authors do not mention a spe-cific motivation for the segmentation scheme theyhave adopted.
Only for the Machine Translationtask, (Habash and Sadat, 2006) report several resultsusing different Arabic segmentation schemes.
Theyreport that the best results were obtained when theATB-like segmentation was used.
We explore herethe four known and linguistically-motivated sorts ofsegmentation: punctuation separation, ATB, mor-phological and character-level segmentations.
Toour knowledge, this is the first paper which inves-tigates different segmentation schemes to define theunit of analysis which best fits Arabic MD.2 Arabic Segmentation SchemesCharacter-level Segmentation: considers that eachcharacter is a separate token.Morphological Segmentation : aims at segmenting709all affixes of a word.
The morphological segmenta-tion for the word I.J???
@?
(wAlmktb ?
and the of-fice)2 could be: ?I.J?
?+ ?
@+ ??
(w +Al +mktb).Arabic TreeBank (ATB) segmentation : This seg-mentation considers splitting the word into affixesonly if it projects an independent phrasal constituentin the parse tree.
As an example, in the word shownabove I.J???
@?, the phrasal independent constituentsare: the conjunction ?
(w ?
and) and the nounI.J???
@ (Almktb ?
the office).
The morphologicalsegmentation of this word would lead to the follow-ing parse tree:SHHCONJwNPbb""Al +mktbSince the ?
@ (Al, the definite article) is not an in-dependent constituent, it is not considered for ATBsegmentation.
Hence, for I.J???
@?, the ATB segmen-tation would be I.J???
@+ ?
(w +Almktb).Punctuation separation : it consists of separatingthe punctuation marks from the word.Both ATB and morphological segmentation sys-tems are based on weighted finite state transducers(WFST).
The decoder implements a general Bell-man dynamic programming search for the best pathon a lattice of segmentation hypotheses that matchthe input characters (Benajiba and Zitouni, 2009).ATB and morphological segmentation systems havea performance of 99.4 and 98.1 F-measure respec-tively on ATB data.The unit of analysis when doing classification de-pends on the used segmentation.
When using thepunctuation separation or character-based segmen-tations, the unit of analysis is the word itself (with-out the punctuation marks attached) or the character,respectively.
The ATB and morphological segmen-tations are language specific and are based on dif-ferent linguistic viewpoint.
When using one of thesetwo segmentation schemes, the unit of analysis is themorph (i.e.
prefix, stem or suffix).
Our goal in thispaper is to find the unit of analysis that fits best Ara-bic MD.2Throughout the paper, for each Arabic example we showbetween parenthesis its transliteration and English translationseparated by ??
?.3 Mention Detection SystemAs explained earlier, we consider the MD task as asequence classification problem where the class wepredict for each unit of analysis (i.e., token) is thetype of the entity which it refers to.
We chose themaximum entropy (MaxEnt) classifier that can in-tegrate arbitrary types of information and make aclassification decision by aggregating all informa-tion available for a given classification.
For moredetails about the system architecture, reader may re-fer to (Zitouni et al, 2009).
The features used in ourMD system can be divided into four categories:Lexical Features: n-grams spanning the current to-ken; both preceding and following it.
A number ofn equal to 3 turned out to be a good choice.Stem n-gram Features: stem trigram spanning thecurrent stem; both preceding and following it (Zi-touni et al, 2005).Syntactic Features: POS tags and shallow parsinginformation in a ?2 window.Features From Other Classifiers: outputs of MDand NER taggers trained on other data-sets differentfrom the one we used here.
They may identify typesof mentions different from the mentions of interestin our task.
For instance, such a tagger may identifydates or occupation references (not used in our task),among other types.
Our hypothesis is that combin-ing classifiers from diverse sources will boost per-formance by injecting complementary informationinto the mention detection models.
We also use thetwo previously assigned classification tags as addi-tional feature.4 DataExperiments are conducted on the Arabic ACE 2007data.
Since the evaluation tests set are not publiclyavailable, we have split the publicly available train-ing corpus into an 85%/15% data split.
We use 323documents (80, 000 words, 17, 634 mentions) fortraining and 56 documents (18, 000 words, 3, 566mentions) as a test set.
We are interested in 7 typesof mentions: facility, Geo-Political Entity (GPE),location, organization, person, vehicle and weapon.We segmented the training and test set with four dif-ferent styles building the following corpora:Words: a corpus which is the result of runningpunctuation separation;ATBs: a corpus obtained by running punctuationseparation and ATB segmentation;Mophs: a corpus where we conduct punctuationseparation and morphological segmentation;Chars: a corpus where the original text is separated710into a sequence of characters.When building MD systems on Words, ATBs,Morphs and Chars, the unit of analysis is the word,the ATB token, the morph and the character, respec-tively.5 ExperimentsWe show in this section the experimental resultswhen using Arabic MD system with different seg-mentation schemes and different feature sets.
Weexplore in this paper four categories of features (c.f.Section 3):Lexf : lexical features;Stemf : Lexf + morphological features;Syntf : Stemf + syntactic features;Semf : Syntf + output of other MD classifiers.Lexf and Stemf features are directly extractedfrom the appropriate corpus based on the used seg-mentation style.
This is different for Semf : we firstrun classifiers on the morphologically segmenteddata.
Thereafter, we project those labels to othercorpora.
This is because, we use classifiers initiallytrained on morphologically segmented data such asACE 2003, 2004 and 2005 data.
In such data, twomorphs belonging to the same word or ATB tokenmay have 2 different mentions.
During transfer, atoken will have the label of the corresponding stemin the morphologically segmented data.
One moti-vation to not re-train classifiers on each corpus sep-arately is to be able to extract Semf features fromclassifiers with similar performance.Table 1: Results in terms of F-measure per feature-set andsegmentation schemeLexf Stemf Syntf SemfWords 66.4 66.6 69.0 77.1ATBs 70.1 69.8 72.1 79.0Morphs 74.1 74.5 75.5 78.3Chars 22.3 22.4 22.5 22.6Results in Table 1 show that classifiers built onATBs and Morphs have shown to perform betterthan classifiers trained on data with other segmenta-tion styles.
When the system uses character as theunit of analysis, performance is poor.
This is be-cause the token itself becomes insignificant informa-tion to the classifier.
On the other hand, when onlypunctuation separation is performed (Words), thedata is significantly sparse and the obtained resultsachieves high F-measure (77.1) only when outputsof other classifiers are used.
As mentioned earlier,classifiers used to extract those features are trainedon Morphs (less sparse), which explains their re-markable positive impact since they resolve part ofthe data sparseness problem in Words.
When us-ing full morphological segmentation, the data is lesssparse, which leads to less Out-Of-Vocabulary to-kens (OOVs): the number of OOVs in the Morphsdata is 1,518 whereas it is 2,464 in the ATBs.As an example, the word?JJ?Q?
@ (Alrhynp ?
thehostage), which is person mention in the trainingdata.
This word is kept unchanged after ATB seg-mentation and is segmented to ??+?
?P+ ?@?
(Al+rhyn +p) in Morphs.
In the development set thesame word appears in its dual form without defi-nite article, i.e.
?JJ?P.
This word is unchanged inATBs and is segmented to ?
?K+H+??P?
(rhyn+p +yn) in Morphs.
For the model built on ATBs,this word is an OOV, whereas for the model builton Morphs the stem has been seen as part of a per-son mention and consequently has a better chanceto tag it correctly.
These phenomena are frequent,which make the classifier trained on Morphs morerobust for such cases.
Also, we observed that mod-els trained on ATBs perform better on long spanmentions.
We think this is because a model trainedon ATBs has access to larger context.
One mayargue that a similar behavior of the model built onthe Morphs might be obtained if we use a widercontext window than the one used for ATBs in or-der to have similar contextual information.
In or-der to confirm this statement, we have carried out aset of experiments using all features over Morphsdata for a context window up to ?5/ + 5, the ob-tained results show no improvement.
Similar behav-ior is observed when looking to results on identi-fied named (Nam.
), nominal (Nom.)
and pronomi-nal (Pro.)
mentions on ATBs and Morphs (c.f.
Ta-ble 2); we remind the reader that NER is about rec-ognizing named mentions.
When limited resourcesare available (e.g.
Lexf , Stemf or Syntf ), we be-lieve that it is more effective to morphologically seg-ment the text (Morphs) as a pre-processing step.The use of morph as a unit of analysis reduces thedata sparseness issue and at the same time allowsbetter context handling when compared to character.On the other hand, when a larger set of resourcesare available (e.g., Semf ), the use of the ATB to-ken as a unit of analysis combined with morph-based features leads to better performance (79.0 vs.78.3 on Morphs).
This is because (1) classifierstrained on ATBs handle better the context and (2)the use of morph-based features (output of classi-711fiers trained on morphologically segmented data) re-moves some of the data sparseness from which clas-sifiers trained on ATBs suffer.
The obtained im-provement in performance is statistically significantwhen using the stratified bootstrap re-sampling sig-nificance test (Noreen, 1989).
We consider resultsas statistically significant when p < 0.02, which isthe case in this paper.
For an accurate MD system,we think it is appropriate to benefit from ATBs to-kens and Morphs.
We investigate in the followingthe combination of these two segmentation styles.Table 2: Performance in terms of F-measure per level onATBs and MorphsSeg.
Lexf Stemf Syntf SemfNam.ATBs 68.2 69.0 72.8 79.1Morphs 73.4 73.8 75.3 78.7Nom.ATBs 65.6 64.6 66.9 75.8Morphs 71.7 72.2 72.9 75.4Pro.ATBs 60.7 60.1 59.9 66.3Morphs 63.0 67.2 65.7 65.15.1 Combination of ATB and MorphWe trained a model on ATBs that uses output of themodel trained on Morphs as additional information(M2Af feature).
We proceed similarly by training amodel on Morphs using output of the model trainedon ATBs (A2Mf feature).
We have obtained thefeatures by a 15-way round-robin.
Table 3 showsthe obtained results.Table 3: Results in terms of F-measure of the combina-tion experimentsLexf Stemf Syntf SemfATBs 70.1 69.8 72.1 79.0ATBs+M2Af 70.7 70.8 73.1 79.1Morphs 74.1 74.5 75.5 78.3Morphs+A2Mf 74.9 75.2 75.4 78.6Results show a significant improvement for mod-els that are trained on ATBs using information fromMorphs in addition to Lexf , Stemf and Syntffeatures.
This again confirms our claim that the useof features from morphologically segmented text re-duces the data sparseness and consequently leads tobetter performance.
For Semf features, only a 0.1F-measure points have been gained.
This is becausewe are already using output of classifiers trainedon morphologically segmented data, which resolvesome of the data sparseness issue.
The Morphsside shows that the obtained performance when theATBs output is employed together with the Stemf(75.2) is only 0.3 points below the performance ofthe system using Syntf (75.5).6 ConclusionsWe have shown a comparative study aiming at defin-ing the adequate unit of analysis for Arabic MD.We conducted our study using four segmentationschemes with four different feature-sets.
Resultsshow that when only limited resources are available,using morphological segmentation leads to the bestresults.
On the other hand, model trained on ATBsegmented data become more powerful and effectivewhen data sparseness is reduced by the use of otherclassifier outputs trained on morphologically seg-mented data.
More improvement is obtained whenboth segmentation styles are combined.ReferencesY.
Benajiba and I. Zitouni.
2009.
Morphology-based segmentation combination for arabic men-tion detection.
Special Issue on Arabic Nat-ural Language Processing of ACM Transac-tions on Asian Language Information Processing(TALIP), 8(4).Y.
Benajiba, M. Diab, and P. Rosso.
2008.
Arabicnamed entity recognition using optimized featuresets.
In Proc.
of EMNLP?08, pages 284?293.R.
Florian, H. Hassan, A. Ittycheriah, H. Jing,N.
Kambhatla, X. Luo, N. Nicolov, andS.
Roukos.
2004.
A statistical model formultilingual entity detection and tracking.
InProc.eedings of HLT-NAACL?04, pages 1?8.N.
Habash and F. Sadat.
2006.
Combination of ara-bic preprocessing schemes for statistical machinetranslation.
In Proceedings of ACL?06, pages 1?8.H.
Jing, R. Florian, X. Luo, T. Zhang, and A. Itty-cheriah.
2003.
HowtogetaChineseName(Entity):Segmentation and combination issues.
In Pro-ceedings of EMNLP?03, pages 200?207.E.
W. Noreen.
1989.
Computer-Intensive Methodsfor Testing Hypotheses.
John Wiley Sons.I.
Zitouni, J. Sorensen, X. Luo, and R. Florian.2005.
The impact of morphological stemming onarabic mention detection and coreference resolu-tion.
In Proc.
of the ACL Workshop on Compu-tational Approaches to Semitic Languages, pages63?70.I.
Zitouni, X. Luo, and R. Florian.
2009.
A cascadedapproach to mention detection and chaining inarabic.
IEEE Transactions on Audio, Speech andLanguage Processing, 17:935?944.712
