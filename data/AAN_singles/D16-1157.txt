Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1504?1515,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsCHARAGRAM: Embedding Words and Sentences via Character n-gramsJohn Wieting Mohit Bansal Kevin Gimpel Karen LivescuToyota Technological Institute at Chicago, Chicago, IL, 60637, USA{jwieting,mbansal,kgimpel,klivescu}@ttic.eduAbstractWe present CHARAGRAM embeddings, a sim-ple approach for learning character-basedcompositional models to embed textual se-quences.
A word or sentence is representedusing a character n-gram count vector, fol-lowed by a single nonlinear transformationto yield a low-dimensional embedding.
Weuse three tasks for evaluation: word simi-larity, sentence similarity, and part-of-speechtagging.
We demonstrate that CHARAGRAMembeddings outperform more complex archi-tectures based on character-level recurrent andconvolutional neural networks, achieving newstate-of-the-art performance on several simi-larity tasks.11 IntroductionRepresenting textual sequences such as words andsentences is a fundamental component of naturallanguage understanding systems.
Many functionalarchitectures have been proposed to model compo-sitionality in word sequences, ranging from sim-ple averaging (Mitchell and Lapata, 2010; Iyyer etal., 2015) to functions with rich recursive struc-ture (Socher et al, 2011; Zhu et al, 2015; Tai etal., 2015; Bowman et al, 2016).
Most work useswords as the smallest units in the compositional ar-chitecture, often using pretrained word embeddingsor learning them specifically for the task of inter-est (Tai et al, 2015; He et al, 2015).Some prior work has found benefit from usingcharacter-based compositional models that encode1Trained models and code are available at http://ttic.uchicago.edu/?wieting.arbitrary character sequences into vectors.
Exam-ples include recurrent neural networks (RNNs) andconvolutional neural networks (CNNs) on charactersequences, showing improvements for several NLPtasks (Ling et al, 2015a; Kim et al, 2015; Balles-teros et al, 2015; dos Santos and Guimara?es, 2015).By sharing subword information across words, char-acter models have the potential to better representrare words and morphological variants.Our approach, CHARAGRAM, uses a much sim-pler functional architecture.
We represent a charac-ter sequence by a vector containing counts of char-acter n-grams, inspired by Huang et al (2013).
Thisvector is embedded into a low-dimensional spaceusing a single nonlinear transformation.
This canbe interpreted as learning embeddings of charactern-grams, which are learned so as to produce effec-tive sequence embeddings when a summation is per-formed over the character n-grams in the sequence.We consider three evaluations: word similar-ity, sentence similarity, and part-of-speech tagging.On multiple word similarity datasets, CHARAGRAMoutperforms RNNs and CNNs, achieving state-of-the-art performance on SimLex-999 (Hill et al,2015).
When evaluated on a large suite of sentence-level semantic textual similarity tasks, CHARA-GRAM embeddings again outperform the RNN andCNN architectures as well as the PARAGRAM-PHRASE embeddings of Wieting et al (2016).
Wealso consider English part-of-speech (POS) taggingusing the bidirectional long short-term memory tag-ger of Ling et al (2015a).
The three architecturesreach similar performance, though CHARAGRAMconverges fastest to high accuracy.1504We perform extensive analysis of our CHARA-GRAM embeddings.
We find large gains in perfor-mance on rare words, showing the empirical ben-efit of subword modeling.
We also compare per-formance across different character n-gram vocabu-lary sizes, finding that the semantic tasks benefit farmore from large vocabularies than the syntactic task.However, even for challenging semantic similaritytasks, we still see strong performance with only afew thousand character n-grams.Nearest neighbors show that CHARAGRAM em-beddings simultaneously address differences due tospelling variation, morphology, and word choice.Inspection of embeddings of particular character n-grams reveals etymological links; e.g., die is closeto mort.
We release our resources to the communityin the hope that CHARAGRAM can provide a strongbaseline for subword-aware text representation.2 Related WorkWe first review work on using subword informa-tion in word embedding models.
The simplest ap-proaches append subword features to word embed-dings, letting the model learn how to use the sub-word information for particular tasks.
Some addedknowledge-based morphological features to wordrepresentations (Alexandrescu and Kirchhoff, 2006;El-Desoky Mousa et al, 2013).
Others learned em-beddings jointly for subword units and words, defin-ing simple compositional architectures (often basedon addition) to create word embeddings from sub-word embeddings (Lazaridou et al, 2013; Botha andBlunsom, 2014; Qiu et al, 2014; Chen et al, 2015).A recent trend is to use richer functional archi-tectures to convert character sequences into wordembeddings.
Luong et al (2013) used recur-sive models to compose morphs into word embed-dings, using unsupervised morphological analysis.Ling et al (2015a) used a bidirectional long short-term memory (LSTM) RNN on characters to em-bed arbitrary word types, showing strong perfor-mance for language modeling and POS tagging.Ballesteros et al (2015) used this model to repre-sent words for dependency parsing.
Several haveused character-level RNN architectures for machinetranslation, whether for representing source or tar-get words (Ling et al, 2015b; Luong and Man-ning, 2016), or for generating entire translationscharacter-by-character (Chung et al, 2016).Sutskever et al (2011) and Graves (2013) usedcharacter-level RNNs for language modeling.
Oth-ers trained character-level RNN language models toprovide features for NLP tasks, including tokeniza-tion and segmentation (Chrupa?a, 2013; Evang et al,2013), and text normalization (Chrupa?a, 2014).CNNs with character n-gram filters have beenused to embed arbitrary word types for several tasks,including language modeling (Kim et al, 2015),part-of-speech tagging (dos Santos and Zadrozny,2014), named entity recognition (dos Santos andGuimara?es, 2015), text classification (Zhang etal., 2015), and machine translation (Costa-Jussa`and Fonollosa, 2016).
Combinations of CNNsand RNNs on characters have also been ex-plored (Jo?zefowicz et al, 2016).Most closely-related to our approach is the DSSM(instantiated variously as ?deep semantic similaritymodel?
or ?deep structured semantic model?)
de-veloped by Huang et al (2013).
For an informa-tion retrieval task, they represented words using fea-ture vectors containing counts of character n-grams.Sperr et al (2013) used a very similar technique torepresent words in neural language models for ma-chine translation.
Our CHARAGRAM embeddingsare based on this same idea.
We show this strategyto be extremely effective when applied to both wordsand sentences, outperforming character LSTMs likethose used by Ling et al (2015a) and characterCNNs like those from Kim et al (2015).3 ModelsWe now describe models that embed textualsequences using their characters, including ourCHARAGRAM model and the baselines that we com-pare to.
We denote a character-based textual se-quence by x = ?x1, x2, ..., xm?, which includesspace characters between words as well as spe-cial start-of-sequence and end-of-sequence charac-ters.
We use xji to denote the subsequence of char-acters from position i to position j inclusive, i.e.,xji = ?xi, xi+1, ..., xj?, and we define xii = xi.Our CHARAGRAM model embeds a character se-quence x by adding the vectors of its character n-1505grams followed by an elementwise nonlinearity:gCHAR(x) = h?
?b +m+1?i=1i?j=1+i?kI[xij ?
V]W xij??
(1)where h is a nonlinear function, b ?
Rd is a biasvector, k is the maximum length of any character n-gram, I[p] is an indicator function that returns 1 if pis true and 0 otherwise, V is the set of character n-grams included in the model, and W xij ?
Rd is thevector for character n-gram xij .The set V is used to restrict the model to a prede-termined set (vocabulary) of character n-grams.
Be-low, we compare several choices for V .
The num-ber of parameters in the model is d + d|V |.
Thismodel is based on the letter n-gram hashing tech-nique developed by Huang et al (2013).
One canalso view Eq.
(1) (as they did) as first populatinga vector of length |V | with counts of character n-grams followed by a nonlinear transformation.We compare the CHARAGRAM model to twoother models.
First we consider LSTM architec-tures (Hochreiter and Schmidhuber, 1997) over thecharacter sequence x, using the version from Gers etal.
(2003).
We use a forward LSTM over the char-acters in x, then take the final LSTM hidden vectoras the representation of x.
Below we refer to thismodel as ?charLSTM.
?We also compare to convolutional neural net-work (CNN) architectures, which we refer to belowas ?charCNN.?
We use the architecture from Kim(2014) with a single convolutional layer followed byan optional fully-connected layer.
We use filters ofvarying lengths of character n-grams, using two pri-mary configurations of filter sets, one of which isidentical to that used by Kim et al (2015).
Eachfilter operates over the entire sequence of charactern-grams in x and we use max pooling for each fil-ter.
We tune over the choice of nonlinearity for boththe convolutional filters and for the optional fully-connected layer.
We give more details below aboutfilter sets, n-gram lengths, and nonlinearities.We note that using character n-gram convolu-tional filters is similar to our use of character n-grams in the CHARAGRAM model.
The differenceis that, in the CHARAGRAM model, the n-gram mustmatch exactly for its vector to affect the representa-tion, while in the CNN each filter will affect the rep-resentation of all sequences (depending on the non-linearity being used).
So the CHARAGRAM model isable to learn precise vectors for particular charactern-grams with specific meanings, while there is pres-sure for the CNN filters to capture multiple similarpatterns that recur in the data.
Our qualitative analy-sis shows the specificity of the learned character n-gram vectors learned by the CHARAGRAM model.4 ExperimentsWe perform three sets of experiments.
The goal ofthe first two (Section 4.1) is to produce embeddingsfor textual sequences such that the embeddings forparaphrases have high cosine similarity.
Our thirdevaluation (Section 4.2) is a classification task, andfollows the setup of the English part-of-speech tag-ging experiment from Ling et al (2015a).4.1 Word and Sentence SimilarityWe compare the ability of our models to capture se-mantic similarity for both words and sentences.
Wetrain on noisy paraphrase pairs from the ParaphraseDatabase (PPDB; Ganitkevitch et al, 2013) with anL2 regularized contrastive loss objective function,following the training procedure of Wieting et al(2015) and Wieting et al (2016).
More details areprovided in the supplementary material.4.1.1 DatasetsFor word similarity, we focus on two of themost commonly used datasets for evaluating seman-tic similarity of word embeddings: WordSim-353(WS353) (Finkelstein et al, 2001) and SimLex-999(SL999) (Hill et al, 2015).
We also evaluate our bestmodel on the Stanford Rare Word Similarity Dataset(Luong et al, 2013).For sentence similarity, we evaluate on a diverseset of 22 textual similarity datasets, including alldatasets from every SemEval semantic textual simi-larity (STS) task from 2012 to 2015.
We also eval-uate on the SemEval 2015 Twitter task (Xu et al,2015) and the SemEval 2014 SICK Semantic Relat-edness task (Marelli et al, 2014).
Given two sen-tences, the aim of the STS tasks is to predict theirsimilarity on a 0-5 scale, where 0 indicates the sen-tences are on different topics and 5 indicates thatthey are completely equivalent.1506Each STS task consists of 4-6 datasets cover-ing a wide variety of domains, including newswire,tweets, glosses, machine translation outputs, webforums, news headlines, image and video captions,among others.
Most submissions for these tasks usesupervised models that are trained and tuned on pro-vided training data or similar datasets from oldertasks.
Further details are provided in the official taskdescriptions (Agirre et al, 2012; Agirre et al, 2013;Agirre et al, 2014; Agirre et al, 2015).4.1.2 PreliminariesFor training data, we use pairs from PPDB.
Forword similarity experiments, we train on word pairsand for sentence similarity, we train on phrase pairs.PPDB comes in different sizes (S, M, L, XL, XXL,and XXXL), where each larger size subsumes allsmaller ones.
The pairs in PPDB are sorted by aconfidence measure and so the smaller sets containhigher precision paraphrases.
PPDB is derived au-tomatically from naturally-occurring bilingual text,and versions of PPDB have been released for manylanguages without the need for any manual annota-tion (Ganitkevitch and Callison-Burch, 2014).Before training the CHARAGRAM model, we needto populate V , the vocabulary of character n-gramsincluded in the model.
We obtain these from thetraining data used for the final models in each set-ting, which is either the lexical or phrasal section ofPPDB XXL.
We tune over whether to include thefull sets of character n-grams in these datasets oronly those that appear more than once.When extracting n-grams, we include spaces andadd an extra space before and after each word orphrase in the training and evaluation data to ensurethat the beginning and end of each word is repre-sented.
We note that strong performance can be ob-tained using far fewer character n-grams; we explorethe effects of varying the number of n-grams and then-gram orders in Section 4.4.We used Adam (Kingma and Ba, 2014) with alearning rate of 0.001 to learn the parameters in thefollowing experiments.4.1.3 Word Embedding ExperimentsTraining and Tuning For hyperparameter tuning,we used one epoch on the lexical section of PPDBXXL, which consists of 770,007 word pairs.
WeModel Tuned on WS353 SL999charCNN SL999 26.31 30.64WS353 33.19 16.73charLSTM SL999 48.27 54.54WS353 51.43 48.83CHARAGRAM SL999 53.87 63.33WS353 58.35 60.00inter-annotator agreement - 75.6 78Table 1: Word similarity results (Spearman?s ?
?
100).
Theinter-annotator agreement is the average Spearman?s ?
betweena single annotator and the average of all others.used either WS353 or SL999 for model selection(reported below).
We then took the selected hyper-parameters and trained for 50 epochs to ensure thatall models had a chance to converge.Full details of our tuning procedure are providedin the supplementary material.
In short, we tuned allmodels thoroughly, tuning the activation functionsfor CHARAGRAM and charCNN, as well as the reg-ularization strength, mini-batch size, and samplingtype for all models.
For charCNN, we experimentedwith two filter sets: one uses 175 filters for each n-gram size ?
{2, 3, 4}, and the other uses the set offilters from Kim et al (2015), consisting of 25 filtersof size 1, 50 of size 2, 75 of size 3, 100 of size 4, 125of size 5, and 150 of size 6.
We also experimentedwith using dropout (Srivastava et al, 2014) on theinputs to the final layer of charCNN in place of L2regularization, as well as removing the last feedfor-ward layer.
Neither variation significantly improvedperformance on our suite of tasks for word or sen-tence similarity.
However, using more filters doesimprove performance, apparently linearly with thesquare of the number of filters.Architecture Comparison The results are shownin Table 1.
The CHARAGRAM model outperformsboth the charLSTM and charCNN models, and alsooutperforms recent strong results on SL999.We also found that the charCNN and charLSTMmodels take far more epochs to converge than theCHARAGRAM model.
We noted this trend across ex-periments and explore it further in Section 4.3.Comparison to Prior Work We found that per-formance of CHARAGRAM on word similarity taskscan be improved by using more character n-grams.This is explored in Section 4.4.
Our best result fromthese experiments was obtained with the largest1507Model SL999Hill et al (2014) 52Schwartz et al (2015) 56Faruqui and Dyer (2015) 58Wieting et al (2015) 66.7CHARAGRAM (large) 70.6Table 2: Spearman?s ??
100 on SL999.
CHARAGRAM (large)refers to the CHARAGRAM model described in Section 4.4.This model contains 173,881 character n-grams, more than the100,283 in the CHARAGRAM model used in Table 1.model we considered, which contains 173,881 n-gram embeddings.
When using WS353 for modelselection and training for 25 epochs, this modelachieves 70.6 on SL999.
To our knowledge, this isthe best result reported on SL999 in this setting; Ta-ble 2 shows comparable recent results.
Note that ahigher SL999 number is reported by Mrks?ic?
et al(2016), but the setting is not comparable to ours asthey started with embeddings tuned on SL999.Lastly, we evaluated our model on the StanfordRare Word Similarity Dataset (Luong et al, 2013),using SL999 for model selection.
We obtained aSpearman?s ?
of 47.1, which outperforms the 41.8result from Soricut and Och (2015) and is compet-itive with the 47.8 reported by Pennington et al(2014), which used a 42B-token corpus for training.4.1.4 Sentence Embedding ExperimentsTraining and Tuning We did initial training ofour models using one pass through PPDB XL, whichconsists of 3,033,753 unique phrase pairs.
Follow-ing Wieting et al (2016), we use the annotatedphrase pairs developed by Pavlick et al (2015) asour validation set, using Spearman?s ?
to rank themodels.
We then take the highest performing mod-els and train on the 9,123,575 unique phrase pairs inthe phrasal section of PPDB XXL for 10 epochs.For all experiments, we fix the mini-batch sizeto 100, the margin ?
to 0.4, and use MAX sam-pling (see supplementary material).
For CHARA-GRAM, V contains all 122,610 character n-grams(n ?
{2, 3, 4}) in the PPDB XXL phrasal section.Other tuning settings are the same as Section 4.1.3.For another baseline, we train the PARAGRAM-PHRASE model of Wieting et al (2016),tuning its regularization strength over{10?5, 10?6, 10?7, 10?8}.
The PARAGRAM-PHRASE model simply uses word averaging as itscomposition function, but outperforms many morecomplex models.In this section, we refer to our model asCHARAGRAM-PHRASE because the input is a char-acter sequence containing multiple words ratherthan only a single word as in Section 4.1.3.
Sincethe vocabulary V is defined by the training data se-quences, the CHARAGRAM-PHRASE model includescharacter n-grams that span multiple words, per-mitting it to capture some aspects of word orderand word co-occurrence, which the PARAGRAM-PHRASE model is unable to do.We encountered difficulties training the char-LSTM and charCNN models for this task.
Wetried several strategies to improve their chance atconvergence, including clipping gradients, increas-ing training data, and experimenting with differentoptimizers and learning rates.
We found successby using the original (confidence-based) orderingof the PPDB phrase pairs for the initial epoch oflearning, then shuffling them for subsequent epochs.This is similar to curriculum learning (Bengio et al,2009).
The higher-confidence phrase pairs tend to beshorter and have many overlapping words, possiblymaking them easier to learn from.Results An abbreviated version of the sentencesimilarity results is shown in Table 3; the sup-plementary material contains the full results.
Forcomparison, we report performance for the median(50%), third quartile (75%), and top-performing(Max) systems from the shared tasks.
We ob-serve strong performance for the CHARAGRAM-PHRASE model.
It always does better than the char-CNN and charLSTM models, and outperforms thePARAGRAM-PHRASE model on 15 of the 22 tasks.Furthermore, CHARAGRAM-PHRASE matches or ex-ceeds the top-performing task-tuned systems on 5tasks, and is within 0.003 on 2 more.
The charLSTMand charCNN models are significantly worse, withthe charCNN being the better of the two and beatingPARAGRAM-PHRASE on 4 of the tasks.We emphasize that there are many other mod-els that could be compared to, such as an LSTMover word embeddings.
This and many other mod-els were explored by Wieting et al (2016).
TheirPARAGRAM-PHRASE model, which simply learnsword embeddings within an averaging composition1508Dataset 50% 75% Max charCNN charLSTM PARAGRAM-PHRASECHARAGRAM-PHRASESTS 2012 Average 54.5 59.5 70.3 56.5 40.1 58.5 66.1STS 2013 Average 45.3 51.4 65.3 47.7 30.7 57.7 57.2STS 2014 Average 64.7 71.4 76.7 64.7 46.8 71.5 74.7STS 2015 Average 70.2 75.8 80.2 66.0 45.5 75.7 76.12014 SICK 71.4 79.9 82.8 62.9 50.3 72.0 70.02015 Twitter 49.9 52.5 61.9 48.6 39.9 52.7 53.6Average 59.7 65.6 73.6 59.2 41.9 66.2 68.7Table 3: Results on SemEval textual similarity datasets (Pearson?s r?
100).
The highest score in each row is in boldface (omittingthe official task score columns).
The last row shows the average performance over all 22 textual similarity datasetsModel Accuracy (%)charCNN 97.02charLSTM 96.90CHARAGRAM 96.99CHARAGRAM (2-layer) 97.10Table 4: Results on part-of-speech tagging.function, was among their best-performing models.We used this model in our experiments as a strongly-performing representative of their results.Lastly, we note other recent work that consid-ers a similar transfer learning setting.
The Fast-Sent model (Hill et al, 2016) uses the 2014 STStask in its evaluation and reports an average Pear-son?s r of 61.3.
On the same data, the C-PHRASEmodel (Pham et al, 2015) has an average Pearson?sr of 65.7.2 Both results are lower than the 74.7achieved by CHARAGRAM-PHRASE on this dataset.4.2 POS Tagging ExperimentsWe now consider part-of-speech (POS) tagging,since it has been used as a testbed for evaluating ar-chitectures for character-level word representations.It also differs from semantic similarity, allowing usto evaluate our architectures on a syntactic task.We replicate the POS tagging experimental setupof Ling et al (2015a).
Their model uses a bidirec-tional LSTM over character embeddings to representwords.
They then use the resulting word representa-tions in another bidirectional LSTM that predicts thetag for each word.
We replace their character bidi-rectional LSTM with our three architectures: char-CNN, charLSTM, and CHARAGRAM.We use the Wall Street Journal portion of the PennTreebank, using Sections 1-18 for training, 19-21 fortuning, and 22-24 for testing.
We set the dimension-ality of the character embeddings to 50 and that of2Both the results for FastSent and C-PHRASE were com-puted from Table 4 in (Hill et al, 2016).the (induced) word representations to 150.
For opti-mization, we use stochastic gradient descent with amini-batch size of 100 sentences.
The learning rateand momentum are set to 0.2 and 0.95 respectively.We train the models for 50 epochs, again to ensurethat all models have an opportunity to converge.The other settings for our models are mostly thesame as for the word and sentence experiments (Sec-tion 4.1).
We again use character n-grams withn ?
{2, 3, 4}, tuning over whether to include all54,893 in the training data or only those that occurmore than once.
However, there are two minor dif-ferences from the previous sections.
First, we adda single binary feature to indicate if the token con-tains a capital letter.
Second, our tuning considersrectified linear units as the activation function for theCHARAGRAM and charCNN architectures.3The results are shown in Table 4.
Performanceis similar across models.
We found that adding asecond fully-connected 150 dimensional layer to theCHARAGRAM model improved results slightly.44.3 ConvergenceOne observation we made during our experimentswas that different models converged at significantlydifferent rates.
Figure 1 plots the performance of theword similarity and tagging tasks as a function oftraining epoch.
For word similarity, we plot the or-acle Spearman?s ?
on SL999, while for tagging weplot accuracy on the tuning set.
We evaluate everyquarter epoch (approximately every 194,252 wordpairs) for word similarity and every epoch for tag-3We did not consider ReLU for the similarity experimentsbecause the final embeddings are used directly to compute co-sine similarities, which led to poor performance when restrict-ing the embeddings to be non-negative.4We also tried adding a second (300 dimensional) layer forthe word and sentence similarity models and found it to hurtperformance.15090 10 20 30 40 5000.20.40.60.8EpochSpearman?s?Word Similarity2 4 6 8 100.80.850.90.951EpochAccuracyPOS TaggingCHARAGRAMcharLSTMcharCNNFigure 1: Plots of performance versus training epoch for wordsimilarity and POS tagging.ging.
We only show the first 10 epochs of training inthe tagging plot.The plots show that the CHARAGRAM model con-verges quickly to high performance.
The charCNNand charLSTM models take many more epochs toconverge.
Even with tagging, which uses a very highlearning rate, CHARAGRAM converges significantlyfaster than the others.
For word similarity, it ap-pears that charCNN and charLSTM are still slowlyimproving at the end of 50 epochs.
This suggeststhe possibility that these models could eventuallysurpass CHARAGRAM with more epochs.
However,due to the large training sets available from PPDBand the computational requirements of these archi-tectures, we were unable to explore the regime oftraining for many epochs.
We conjecture that slowconvergence could also be the reason for the infe-rior performance of LSTMs for similarity tasks asreported by Wieting et al (2016).4.4 Model Size ExperimentsThe default setting for our CHARAGRAM andCHARAGRAM-PHRASE models is to use all charac-ter bigram, trigrams, and 4-grams that occur in thetraining data at least C times, tuning C over the set{1, 2}.
This results in a large number of param-eters, which could be seen as an unfair advantageover the comparatively smaller charCNN and char-LSTM similarity models, which have up to 881,025Task # n-grams 2 2,3 2,3,4 2,3,4,5 2,3,4,5,6POS 100 95.52 96.09 96.15 96.13 96.16Tagging 1,000 96.72 96.86 96.97 97.02 97.0350,000 96.81 97.00 97.02 97.04 97.09Word 100 6.2 7.0 7.7 9.1 8.8Similarity 1,000 15.2 33.0 38.7 43.2 43.950,000 14.4 52.4 67.8 69.2 69.5Sentence 100 40.2 33.8 32.5 31.2 29.8Similarity 1,000 50.1 60.3 58.6 56.6 55.650,000 45.7 64.7 66.6 63.0 61.3Table 5: Results of using different numbers and different com-binations of character n-grams.and 763,200 parameters respectively (including 134character embeddings for each).However, for a given sequence, very few param-eters in the CHARAGRAM model are actually used.For charCNN and charLSTM, by contrast, all pa-rameters are used except character embeddings forcharacters not present in the sequence.
For a 100-character sequence, the 300-dimensional CHARA-GRAM model uses approximately 90,000 parame-ters, about one-tenth of those used by charCNN andcharLSTM for the same sequence.We performed a series of experiments to inves-tigate how the CHARAGRAM and CHARAGRAM-PHRASE models perform with different numbers andlengths of character n-grams.
For a given k, wetook the k most frequent character n-grams for eachvalue of n in use.
We experimented with k valuesin {100, 1000, 50000}.
If there were fewer than kunique character n-grams for a given n, we used allof them.
For these experiments, we did very littletuning, setting the regularization strength to 0 andonly tuning the activation function.
For word simi-larity, we report performance on SL999 after 5 train-ing epochs on the lexical section of PPDB XXL.
Forsentence similarity, we report the average Pearson?sr over all 22 datasets after 5 training epochs on thephrasal section of PPDB XL.
For tagging, we reportaccuracy on the tuning set after 50 training epochs.The results are shown in Table 5.
When using ex-tremely small models with only 100 n-grams of eachorder, we still see relatively strong performance ontagging.
However, the similarity tasks require farmore n-grams to yield strong performance.
Using1000 n-grams clearly outperforms 100, and 50,000n-grams performs best.
We also found that modelsconverged more quickly on tagging than on the sim-ilarity tasks.
We suspect this is due to differencesin task complexity.
In tagging, the model does not1510need to learn all facets of each word?s semantics; itonly needs to map a word to its syntactic categories.Therefore, simple surface-level features like affixescan help tremendously.
However, learning repre-sentations that reflect detailed differences in wordmeaning is a more fine-grained endeavor and this ispresumably why larger models are needed and con-vergence is slower.5 Analysis5.1 Quantitative AnalysisOne of our primary motivations for character-basedmodels is to address the issue of out-of-vocabulary(OOV) words, which were found to be one of themain sources of error for the PARAGRAM-PHRASEmodel from Wieting et al (2016).
They reported anegative correlation (Pearson?s r of -0.45) betweenOOV rate and performance.
We took the 12,108 sen-tence pairs in all 20 SemEval STS tasks and binnedthem by the total number of unknown words in thepairs.5 We computed Pearson?s r over each bin.
Theresults are shown in Table 6.Number ofUnknown Words NPARAGRAM-PHRASECHARAGRAM-PHRASE0 11,292 71.4 73.81 534 68.8 78.82 194 66.4 72.8?
1 816 68.6 77.9?
0 12,108 71.0 74.0Table 6: Performance (Pearson?s r ?
100) as a function ofthe number of unknown words in the sentence pairs over all20 SemEval STS datasets.
N is the number of sentence pairs.The CHARAGRAM-PHRASE model has better per-formance for each number of unknown words.
ThePARAGRAM-PHRASE model degrades when moreunknown words are present, presumably because itis forced to use the same unknown word embeddingfor all unknown words.
The CHARAGRAM-PHRASEmodel has no notion of unknown words, as it canembed any character sequence.We next investigated the sensitivity of the twomodels to length, as measured by the maximum5Unknown words were defined as those not present inthe 1.7 million unique (case-insensitive) tokens that com-prise the vocabulary for the GloVe embeddings available athttp://nlp.stanford.edu/projects/glove/.The PARAGRAM-SL999 embeddings, used to initialize thePARAGRAM-PHRASE model, use this same vocabulary.of the lengths of the two sentences in a pair.
Webinned all of the 12,108 sentence pairs in the 20SemEval STS tasks by length and then again foundthe Pearson?s r for both the PARAGRAM-PHRASEand CHARAGRAM-PHRASE models.
The results areshown in Table 7.Max Length N PARAGRAM-PHRASECHARAGRAM-PHRASE?
4 71 67.9 72.95 216 71.1 71.96 572 67.0 69.77 1,097 71.5 74.08 1,356 74.2 74.59 1,266 71.7 72.710 1,010 70.7 74.211-15 3,143 71.8 73.716-20 1,559 73.0 75.1?
21 1,818 74.5 75.4Table 7: Performance (Pearson?s r ?
100) as a function of themaximum number of tokens in the sentence pairs over all 20SemEval STS datasets.
N is the number of sentence pairs.Both models are robust to sentence length, achiev-ing the highest correlations on the longest sentences.We also find that CHARAGRAM-PHRASE outper-forms PARAGRAM-PHRASE at all sentence lengths.5.2 Qualitative AnalysisBigram CHARAGRAM-PHRASE PARAGRAM-PHRASEnot capable incapable, unable, incapacity not, capable, stallednot able unable, incapable, incapacity not, able, stallednot possible impossible impracticable unable not, stalled, possiblenot sufficient insufficient, sufficient, inadequate not, sufficient, stallednot easy easy, difficult, tough not, stalled, easyTable 8: Nearest neighboring words of selected bigrams underCHARAGRAM-PHRASE and PARAGRAM-PHRASE embeddings.Aside from OOVs, the PARAGRAM-PHRASEmodel lacks the ability to model word order orcooccurrence, since it simply averages the words inthe sequence.
We were interested to see whetherCHARAGRAM-PHRASE could handle negation, sinceit does model limited information about word order(via character n-grams that span multiple words).We made a list of ?not?
bigrams that could be repre-sented by a single word, then embedded each bigramusing both models and did a nearest-neighbor searchover a working vocabulary.6 The results, in Ta-ble 8, show how the CHARAGRAM-PHRASE embed-dings model negation.
In all cases but one, the near-6This has all words in PPDB-XXL, our evaluations, and twoother datasets: SST (Socher et al, 2013) and SNLI (Bowman etal., 2015), resulting in 93,217 unique (up-to-casing) tokens.1511Word Nearest Neighborsvehicals vehical, vehicles, vehicels, vehicular, cars, vehicle, automobiles, carserious-looking serious, grave, acute, serious-minded, seriousness, gravity, serious-facednear-impossible impossible, hard/impossible, audacious-impossible, impractical, unablegrowths growth, grow, growing, increases, grows, increase, rise, growls, risinglitered liter, litering, lited, liters, literate, literature, literary, literal, lite, obliteratedjourneying journey, journeys, voyage, trip, roadtrip, travel, tourney, voyages, road-tripbabyyyyyy babyyyyyyy, baby, babys, babe, baby.i, babydoll, babycake, darlingadirty dirty, dirtyyyyyy, filthy, down-and-dirty, dirtying, dirties, ugly, dirty-blonderefunding refunds, refunded, refund, repayment, reimbursement, rebate, repayreimbursements, reimburse, repaying, repayments, rebates, rebating, reimbursesprofessors professor, professorships, professorship, teachers, professorial, teacherprof., teaches, lecturers, teachings, instructors, headteachers, teacher-studenthuge enormous, tremendous, large, big, vast, overwhelming, immense, giantformidable, considerable, massive, huger, large-scale, great, dauntingTable 9: Nearest neighbors of CHARAGRAM-PHRASE embeddings.
Above the double horizontal line are nearest neighbors ofwords that were not in our training data, and below it are nearest neighbors of words that were in our training data.est neighbor is a paraphrase for the bigram and thenext neighbors are mostly paraphrases as well.
ThePARAGRAM-PHRASE model, unsurprisingly, is inca-pable of modeling negation.
In all cases, the nearestneighbor is not, as it carries much more weight thanthe word it modifies.
The remaining nearest neigh-bors are either the modified word or stalled.We did two additional nearest neighbor ex-plorations with our CHARAGRAM-PHRASE model.First, we collected nearest neighbors for words thatwere not in the training data (i.e., PPDB XXL), butwere in our working vocabulary.
These are shownin the upper part of Table 9.
In the second, we col-lected nearest neighbors of words that were in ourtraining data, shown in the lower part of Table 9.Several kinds of similarity are being captured si-multaneously.
One kind is similarity in terms ofspelling variation, including misspellings (vehicals,vehicels) and repetition for emphasis (babyyyyyyy).Another kind is similarity in terms of morpholog-ical variants of a shared root (e.g., journeying andjourney).
We also find many synonym relationshipswithout significant amounts of overlapping charac-ters (e.g., vehicles, cars, automobiles).
Words inthe training data, which tend to be more commonlyused, do tend to have higher precision in their near-est neighbors (e.g., neighbors of huge).
We see oc-casional mistakes for words that share many char-acters but are not paraphrases (e.g., litered, a likelymisspelling of littered).Lastly, since our model learns embeddings forcharacter n-grams, we show an analysis of charac-ter n-gram nearest neighbors in Table 10.
They ap-pear to be grouped into themes, such as death (rown-gram Nearest Neighborsdie dy, die, dead, dyi, rlif, mort, ecea, rpse, d awfoo foo, eat, meal, alim, trit, feed, grai, din, nutr, toepee peed, hast, spee, fast, mpo , pace, vel, loci, ccelaiv waiv, aive, boli, epea, ncel, abol, lift, bort, bolngu ngue, uist, ongu, tong, abic, gual, fren, ocab, ingu2 2 , 02, 02 , tw, dua, xx, ii , xx, o 14, d .2Table 10: Nearest neighbors of character n-gram embeddingsfrom trained CHARAGRAM-PHRASE model.
The underscore in-dicates a space, which signals the beginning or end of a word.1), food (row 2), and speed (row 3), but have differ-ent granularities.
The n-grams in the last row appearin paraphrases of 2, whereas the second-to-last rowshows n-grams in words related to language.6 ConclusionWe performed a careful empirical comparison ofcharacter-based compositional architectures on threeNLP tasks.
We found a consistent trend: the sim-plest architecture converges fastest to high perfor-mance.
These results, coupled with those fromWieting et al (2016), suggest that practitionersshould begin with simple architectures rather thanmoving immediately to RNNs and CNNs.
We re-lease our code and trained models so they can beused by the NLP community for general-purpose,character-based text representation.AcknowledgmentsWe thank the anonymous reviewers for their valu-able comments.
This research used resources of theArgonne Leadership Computing Facility, which is aDOE Office of Science User Facility supported un-der Contract DE-AC02-06CH11357.
We thank thedevelopers of Theano (Theano Development Team,2016) and NVIDIA Corporation for donating GPUsused in this research.1512ReferencesEneko Agirre, Mona Diab, Daniel Cer, and AitorGonzalez-Agirre.
2012.
SemEval-2012 task 6: Apilot on semantic textual similarity.
In Proceedingsof the First Joint Conference on Lexical and Com-putational Semantics-Volume 1: Proceedings of themain conference and the shared task, and Volume 2:Proceedings of the Sixth International Workshop onSemantic Evaluation.
Association for ComputationalLinguistics.Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo.
2013.
*SEM 2013 sharedtask: Semantic textual similarity.
In Second JointConference on Lexical and Computational Semantics(*SEM), Volume 1: Proceedings of the Main Confer-ence and the Shared Task: Semantic Textual Similarity.Eneko Agirre, Carmen Banea, Claire Cardie, DanielCer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo,Rada Mihalcea, German Rigau, and Janyce Wiebe.2014.
SemEval-2014 task 10: Multilingual seman-tic textual similarity.
In Proceedings of the 8th Inter-national Workshop on Semantic Evaluation (SemEval2014).Eneko Agirre, Carmen Banea, Claire Cardie, DanielCer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo,Inigo Lopez-Gazpio, Montse Maritxalar, Rada Mihal-cea, German Rigau, Larraitz Uria, and Janyce Wiebe.2015.
SemEval-2015 task 2: Semantic textual similar-ity, English, Spanish and pilot on interpretability.
InProceedings of the 9th International Workshop on Se-mantic Evaluation (SemEval 2015).Andrei Alexandrescu and Katrin Kirchhoff.
2006.
Fac-tored neural language models.
In Proceedings ofthe Human Language Technology Conference of theNAACL, Companion Volume: Short Papers.Miguel Ballesteros, Chris Dyer, and Noah A. Smith.2015.
Improved transition-based parsing by modelingcharacters instead of words with LSTMs.
In Proceed-ings of the 2015 Conference on Empirical Methods inNatural Language Processing.Yoshua Bengio, Je?ro?me Louradour, Ronan Collobert, andJason Weston.
2009.
Curriculum learning.
In Pro-ceedings of the 26th annual international conferenceon machine learning.Jan A. Botha and Phil Blunsom.
2014.
Compositionalmorphology for word representations and languagemodelling.
In Proceedings of the 31st InternationalConference on Machine Learning (ICML-14).Samuel R. Bowman, Gabor Angeli, Christopher Potts,and D. Christopher Manning.
2015.
A large anno-tated corpus for learning natural language inference.In Proceedings of the 2015 Conference on EmpiricalMethods in Natural Language Processing.Samuel R. Bowman, Jon Gauthier, Abhinav Rastogi,Raghav Gupta, Christopher D. Manning, and Christo-pher Potts.
2016.
A fast unified model for parsing andsentence understanding.
In Proceedings of ACL.Xinxiong Chen, Lei Xu, Zhiyuan Liu, Maosong Sun, andHuanbo Luan.
2015.
Joint learning of character andword embeddings.
In Proceedings of InternationalJoint Conference on Artificial Intelligence (IJCAI).Grzegorz Chrupa?a.
2013.
Text segmentation withcharacter-level text embeddings.
arXiv preprintarXiv:1309.4628.Grzegorz Chrupa?a.
2014.
Normalizing tweets with editscripts and recurrent neural embeddings.
In Proceed-ings of the 52nd Annual Meeting of the Association forComputational Linguistics (Volume 2: Short Papers).Junyoung Chung, Kyunghyun Cho, and Yoshua Ben-gio.
2016.
A character-level decoder without explicitsegmentation for neural machine translation.
arXivpreprint arXiv:1603.06147.Marta R. Costa-Jussa` and Jose?
A. R. Fonollosa.
2016.Character-based neural machine translation.
arXivpreprint arXiv:1603.00810.Cicero dos Santos and Victor Guimara?es.
2015.
Boost-ing named entity recognition with neural character em-beddings.
In Proceedings of the Fifth Named EntityWorkshop.Cicero dos Santos and Bianca Zadrozny.
2014.
Learn-ing character-level representations for part-of-speechtagging.
In Proceedings of the 31st International Con-ference on Machine Learning (ICML-14).Amr El-Desoky Mousa, Hong-Kwang Jeff Kuo, LidiaMangu, and Hagen Soltau.
2013.
Morpheme-basedfeature-rich language models using deep neural net-works for LVCSR of Egyptian Arabic.
In 2013 IEEEInternational Conference on Acoustics, Speech andSignal Processing (ICASSP).
IEEE.Kilian Evang, Valerio Basile, Grzegorz Chrupa?a, andJohan Bos.
2013.
Elephant: Sequence labeling forword and sentence segmentation.
In Proceedings ofthe 2013 Conference on Empirical Methods in NaturalLanguage Processing.Manaal Faruqui and Chris Dyer.
2015.
Non-distributional word vector representations.
arXivpreprint arXiv:1506.05230.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and EytanRuppin.
2001.
Placing search in context: The con-cept revisited.
In Proceedings of the 10th internationalconference on World Wide Web.
ACM.Juri Ganitkevitch and Chris Callison-Burch.
2014.
Themultilingual paraphrase database.
In Proceedings ofthe Ninth International Conference on Language Re-sources and Evaluation (LREC-2014).1513Juri Ganitkevitch, Benjamin Van Durme, and ChrisCallison-Burch.
2013.
PPDB: The ParaphraseDatabase.
In Proceedings of HLT-NAACL.Felix A. Gers, Nicol N. Schraudolph, and Ju?rgen Schmid-huber.
2003.
Learning precise timing with LSTM re-current networks.
The Journal of Machine LearningResearch, 3.Alex Graves.
2013.
Generating sequences with recurrentneural networks.
arXiv preprint arXiv:1308.0850.Hua He, Kevin Gimpel, and Jimmy Lin.
2015.
Multi-perspective sentence similarity modeling with convo-lutional neural networks.
In Proceedings of the 2015Conference on Empirical Methods in Natural Lan-guage Processing.Felix Hill, Kyunghyun Cho, Sebastien Jean, ColineDevin, and Yoshua Bengio.
2014.
Embedding wordsimilarity with neural machine translation.
arXivpreprint arXiv:1412.6448.Felix Hill, Roi Reichart, and Anna Korhonen.
2015.SimLex-999: Evaluating semantic models with (gen-uine) similarity estimation.
Computational Linguis-tics, 41(4).Felix Hill, Kyunghyun Cho, and Anna Korhonen.
2016.Learning distributed representations of sentences fromunlabelled data.
In Proceedings of the 2016 Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics: Human LanguageTechnologies.Sepp Hochreiter and Ju?rgen Schmidhuber.
1997.
Longshort-term memory.
Neural computation, 9(8).Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,Alex Acero, and Larry Heck.
2013.
Learning deepstructured semantic models for web search using click-through data.
In Proceedings of the 22nd ACM inter-national conference on Conference on information &knowledge management.
ACM.Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber,and Hal Daume?
III.
2015.
Deep unordered compo-sition rivals syntactic methods for text classification.In Proceedings of the 53rd Annual Meeting of the As-sociation for Computational Linguistics and the 7thInternational Joint Conference on Natural LanguageProcessing (Volume 1: Long Papers).Rafal Jo?zefowicz, Oriol Vinyals, Mike Schuster, NoamShazeer, and Yonghui Wu.
2016.
Exploring the limitsof language modeling.
CoRR, abs/1602.02410.Yoon Kim, Yacine Jernite, David Sontag, and Alexan-der M. Rush.
2015.
Character-aware neural languagemodels.
CoRR, abs/1508.06615.Yoon Kim.
2014.
Convolutional neural networks for sen-tence classification.
In Proceedings of the 2014 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP).Diederik Kingma and Jimmy Ba.
2014.
Adam: Amethod for stochastic optimization.
arXiv preprintarXiv:1412.6980.Angeliki Lazaridou, Marco Marelli, Roberto Zamparelli,and Marco Baroni.
2013.
Compositional-ly derivedrepresentations of morphologically complex words indistributional semantics.
In Proceedings of the 51stAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers).Wang Ling, Chris Dyer, Alan W Black, Isabel Trancoso,Ramon Fermandez, Silvio Amir, Luis Marujo, andTiago Luis.
2015a.
Finding function in form: Com-positional character models for open vocabulary wordrepresentation.
In Proceedings of the 2015 Conferenceon Empirical Methods in Natural Language Process-ing.Wang Ling, Isabel Trancoso, Chris Dyer, and Alan W.Black.
2015b.
Character-based neural machine trans-lation.
arXiv preprint arXiv:1511.04586.Minh-Thang Luong and Christopher D. Manning.
2016.Achieving open vocabulary neural machine translationwith hybrid word-character models.
arXiv preprintarXiv:1604.00788.Thang Luong, Richard Socher, and Christopher Man-ning.
2013.
Better word representations with recur-sive neural networks for morphology.
In Proceedingsof the Seventeenth Conference on Computational Nat-ural Language Learning.Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-faella Bernardi, Stefano Menini, and Roberto Zampar-elli.
2014.
SemEval-2014 task 1: Evaluation of com-positional distributional semantic models on full sen-tences through semantic relatedness and textual entail-ment.
In Proceedings of the 8th International Work-shop on Semantic Evaluation (SemEval 2014).Jeff Mitchell and Mirella Lapata.
2010.
Composition indistributional models of semantics.
Cognitive Science,34(8).Nikola Mrks?ic?, Diarmuid O?
Se?aghdha, Blaise Thom-son, Milica Gas?ic?, Lina Rojas-Barahona, Pei-Hao Su,David Vandyke, Tsung-Hsien Wen, and Steve Young.2016.
Counter-fitting word vectors to linguistic con-straints.
arXiv preprint arXiv:1603.00892.Ellie Pavlick, Pushpendre Rastogi, Juri Ganitkevich,Benjamin Van Durme, and Chris Callison-Burch.2015.
PPDB 2.0: Better paraphrase ranking, fine-grained entailment relations, word embeddings, andstyle classification.
In Proceedings of the AnnualMeeting of the Association for Computational Linguis-tics.Jeffrey Pennington, Richard Socher, and Christopher D.Manning.
2014.
Glove: Global vectors for wordrepresentation.
Proceedings of Empirical Methods inNatural Language Processing (EMNLP 2014).1514Nghia The Pham, Germa?n Kruszewski, Angeliki Lazari-dou, and Marco Baroni.
2015.
Jointly optimizingword representations for lexical and sentential taskswith the c-phrase model.
In Proceedings of the 53rdAnnual Meeting of the Association for ComputationalLinguistics and the 7th International Joint Conferenceon Natural Language Processing (Volume 1: Long Pa-pers).Siyu Qiu, Qing Cui, Jiang Bian, Bin Gao, and Tie-YanLiu.
2014.
Co-learning of word representations andmorpheme representations.
In Proceedings of COL-ING 2014, the 25th International Conference on Com-putational Linguistics: Technical Papers.Roy Schwartz, Roi Reichart, and Ari Rappoport.
2015.Symmetric pattern based word embeddings for im-proved word similarity prediction.
In Proceedings ofthe Nineteenth Conference on Computational NaturalLanguage Learning.Richard Socher, Eric H. Huang, Jeffrey Pennington, An-drew Y. Ng, and Christopher D. Manning.
2011.
Dy-namic pooling and unfolding recursive autoencodersfor paraphrase detection.
In Advances in Neural Infor-mation Processing Systems.Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang,Christopher D. Manning, Andrew Ng, and ChristopherPotts.
2013.
Recursive deep models for semanticcompositionality over a sentiment treebank.
In Pro-ceedings of the 2013 Conference on Empirical Meth-ods in Natural Language Processing.Radu Soricut and Franz Och.
2015.
Unsupervised mor-phology induction using word embeddings.
In Pro-ceedings of the 2015 Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics: Human Language Technologies.Henning Sperr, Jan Niehues, and Alex Waibel.
2013.Letter n-gram-based input encoding for continuousspace language models.
In Proceedings of the Work-shop on Continuous Vector Space Models and theirCompositionality.Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,Ilya Sutskever, and Ruslan Salakhutdinov.
2014.Dropout: A simple way to prevent neural networksfrom overfitting.
The Journal of Machine LearningResearch, 15(1).Ilya Sutskever, James Martens, and Geoffrey E Hinton.2011.
Generating text with recurrent neural networks.In Proceedings of the 28th International Conferenceon Machine Learning (ICML-11).Kai Sheng Tai, Richard Socher, and Christopher D. Man-ning.
2015.
Improved semantic representations fromtree-structured long short-term memory networks.
InProceedings of the 53rd Annual Meeting of the Associ-ation for Computational Linguistics and the 7th Inter-national Joint Conference on Natural Language Pro-cessing (Volume 1: Long Papers).Theano Development Team.
2016.
Theano: A Pythonframework for fast computation of mathematical ex-pressions.
arXiv e-prints, abs/1605.02688, May.John Wieting, Mohit Bansal, Kevin Gimpel, KarenLivescu, and Dan Roth.
2015.
From paraphrasedatabase to compositional paraphrase model and back.Transactions of the ACL (TACL).John Wieting, Mohit Bansal, Kevin Gimpel, and KarenLivescu.
2016.
Towards universal paraphrastic sen-tence embeddings.
In Proceedings of InternationalConference on Learning Representations.Wei Xu, Chris Callison-Burch, and William B Dolan.2015.
SemEval-2015 task 1: Paraphrase and semanticsimilarity in Twitter (PIT).
In Proceedings of the 9thInternational Workshop on Semantic Evaluation (Sem-Eval).Xiang Zhang, Junbo Zhao, and Yann LeCun.
2015.Character-level convolutional networks for text classi-fication.
In Advances in Neural Information Process-ing Systems.Xiaodan Zhu, Parinaz Sobhani, and Hongyu Guo.
2015.Long short-term memory over recursive structures.
InProceedings of the 32nd International Conference onMachine Learning.1515
