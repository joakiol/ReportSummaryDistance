Proceedings of the SIGDIAL 2013 Conference, pages 97?101,Metz, France, 22-24 August 2013. c?2013 Association for Computational LinguisticsWill my Spoken Dialogue System be a Slow Learner ?Layla El AsriOrange Labs / UMI 2958 (IMS-MaLIS)Issy-les-Moulineaux (France) / Metz (France)layla.elasri@orange.comRomain LarocheOrange LabsIssy-les-Moulineaux (France)romain.laroche@orange.comAbstractThis paper presents a practicalmethodology for the integration ofreinforcement learning during thedesign of a Spoken Dialogue System(SDS).
It proposes a method thatenables SDS designers to know, inadvance, the number of dialogues thattheir system will need in order to learnthe value of each state-action couple.We ask the designer to provide a usermodel in a simple way.
Then, we runsimulations with this model and wecompute confidence intervals for themean of the expected return of thestate-action couples.1 IntroductionThe Dialogue Manager (DM) of a Spoken Di-alogue System (SDS) selects actions accord-ing to its current beliefs concerning the stateof the dialogue.
Reinforcement Learning (RL)has been more and more used for the optimisa-tion of dialogue management, freeing designersfrom having to fully implement the strategy ofthe DM.A framework known as Module-VariableDecision Process (MVDP) was proposed byLaroche et al(2009) who integrated RL intoan automaton-based DM.
This led to the de-ployment of the first commercial SDS imple-menting RL (Putois et al 2010).Our work intends to continue this effort inbridging the gap between research advanceson RL-based SDS and industrial release.
Oneimportant issue concerning the design of anRL-based SDS is that it is difficult to evalu-ate the number of training dialogues that willbe necessary for the system to learn an opti-mal behaviour.
The underlying mathematicalproblem is the estimation of the training sam-ple size needed by the RL algorithm for con-vergence.
Yet, designers are often not expertsin RL.
Therefore, this paper presents a simplemethodology for evaluating the necessary sam-ple size for an RL algorithm embedded into anSDS.
This methodology does not require anyRL expertise from designers.
The latter areasked to provide a model of user behaviour ina simple way.
According to this model, numer-ous simulations are run and the sample sizefor each module-state-action triple of the DMis estimated.
This methodology was tested onan SDS designed during the CLASSiC Euro-pean project1 (Laroche et al 2011) and weshow that these computations are robust tovarying models of user behaviour.2 Dialogue Management as aModule-Variable DecisionProcessModule-Variable Decision Processes (MVDP)factorise learning into modules, each modulehaving its own state and action spaces.
For-mally, an MVDP is a tuple (M,VM , AM , T )where M is the module space, VM is the spaceof local contexts, for each module m, Vm ?
VMis the set of variables which are relevant form?s decision making.
Am ?
AM is the set ofpossible actions, an action beeing a transitionin the automaton.
T ?
R is the time scale.
Inthe following, time is measured in number ofdialogue turns, a turn being the time elapsedbetween two ASR results.1Computational Learning in Adaptive Systems forSpoken Conversation, http://www.classic-project.org/972.1 The Compliance BasedReinforcement LearningAlgorithmThe Compliance-Based Reinforcement Learn-ing algorithm (CBRL, Laroche et al 2009) isan adaptation of the Monte Carlo algorithmto online off-policy learning.
Each evaluationphase in the Monte Carlo procedure requiresnumerous new episodes.
CBRL enables to ac-celerate this process by adjusting the currentpolicy not after a set of many new episodesbut right after each episode and using all theprevious episodes to evaluate the policy.
Eachdialogue is modelled as a sequence of decisionsdt = (mt, st, at, t) where mt is the module en-countered at time t, st is the current local con-text of mt and at is the action chosen by mt.Each decision dt leads to an immediate rewardRt.
With ?
a discount factor, the return for adecision dt is rt =?tfti=t ?ti?tRti , tf being thefinal turn of the dialogue.
For a given modulem, the value of any state-action couple (s, a)is the expected return starting from (s, a) andthen choosing actions according to ?, the pol-icy of the system: Qpim(s, a) = E[rt | mt =m, st = s, at = a, ?].
?
is the set of all thepolicies of the modules: ?
= {?m1 , ..., ?m|M|}.After a dialogue is taken under policy ?, thevalue of any triple (m, s, a) is updated as inEquation 1.Qpim(s, a) =?
?m(s,a)?trt?m(s, a)(1)where ?m(s, a) =?
?m(s,a)?t,and ?m(s, a) = {dt}mt=m;st=s;at=a (2)For any module m, the algorithm evaluatesthe value of each couple (s, a) according to allthe decisions in which this tuple has been in-volved from the beginning of learning (the setof decisions ?m(s, a)).
After each evaluationof the Q-function, the policy ?
is updated fol-lowing an exploratory strategy based on theUpper Confidence Bound 1 - Tuned approach(Auer et al 2002).
The weights ?t in Equa-tion 1 are there to take into account the factthat ?
is evaluated according to all the rewardsobserved since the beginning of learning, re-wards that were obtained following other poli-cies.
A local compliance cpi(dt) is associatedwith each decision dt: it is the expected re-gret induced by at not being the optimal ac-tion according to the system?s current policy?, cpi(dt) = Qpimt(st, at)?maxa?AmtQpimt(st, a).The global compliance with ?
of the decisionsfollowing dt is a discounted sum of the localcompliances.
The weight wt is then an increas-ing function of the global compliance.3 Problem Resolution3.1 ApproachThe problem to be solved is the follow-ing.
Let an MVDP (M,VM , AM , T ).
Foreach triple (m, s, a), we want to computethe error made on the estimate Qm(s, a) ofE[r | m, s, a] according to the number of ob-servations ?m(s, a).
Let r1, ..., r|?m(s,a)| bethe returns corresponding to the decisions in?m(s, a) and ?m(s,a) the variance of these re-turns.
We build a confidence interval forE[r |m, s, a], centered in the estimateQm(s, a)from user simulations with a bi-gram modelspecified by the designer.3.2 User SimulationsUser simulation has been an active line ofresearch as it is often costly to gather realdata (Scheffler and Young, 2002; Georgila etal., 2006; Yang and Heeman, 2007; Pietquinand Hastie, 2010).
Task-oriented systems suchas commercial ones aim to respond to a spe-cific need.
They are often conceived as slot-filling systems (Raux et al 2003; Chandramo-han et al 2011).
The dialogue is relativelywell-guided by the system so there is no needto take into account complex conversationalgroundings to simulate user behaviour.
There-fore, we choose here to ask the designer to pro-vide a bi-gram model (Eckert et al 1997): aprobability distribution of user behaviour onlyconditioned on the latest system action.
Foreach possible response, the designer providesa lower and an upper bound for its probabilityof occurring.
Eckert et al(1997) showed thatslight modifications of user behaviour in thebi-gram model did not entail great differencesof system performance.
We support this claimin Section 4 where we show that the confidenceintervals computation is robust to varying userbehaviour.983.3 Confidence IntervalsAccording to the Lyapunov central limittheorem, Qm(s, a) converges in law to thenormal distribution of mean E[Qm(s, a)] =E[r | m, s, a] and variance var(Qm(s, a)) =?
?m(s,a) w2k?2m(s, a)?2m(s, a).
However, since ?2m(s, a)is unknown and the observations are not nec-essarily distributed according to a normal law,we can only rely on an asymptotic result ac-cording to which, for a sufficiently large num-ber of samples, the previous convergence re-sult holds with the unbiased estimate of thereturns variance ?
?m(s, a).
A confidence inter-val of probability 1 ?
?
for E[r | m, s, a] isthen:[Qm(s, a)?
?m,s,a, Qm(s, a) + ?m,s,a] (3)We note u?
= ??1N(0,1)(1?
?2 ), with ?N(0,1) thecumulative distribution function of N(0, 1):?m,s,a =?
?m(s,a)?2k?m(s, a)?
?m(s, a)u?
(4)In the non-weighted case, the previous asymp-totic result is generally considered to hold fora number of samples greater than 30.
We thusconsider the confidence intervals to be valid for?m(s, a) =?2m(s, a)?
?m(s,a) ?2k> 30.3.4 ?-Convergence DefinitionA confidence interval can be computed for each(m, s, a) triple of the system.
From this com-putation, we deduce the number of dialoguesnecessary for convergence i.e.
for the widthof the confidence interval to be under a giventhreshold.
The confidence interval radius of atriple (m, s, a) depends on the variance of ob-served returns (see equation 4) so we definethe normalised confidence interval radius:?m,s,a =?m,s,a?
?m(s, a)= u??
?m(s, a)?
1(5)We will consider that a triple (m, s, a) willhave ?-converged once the normalised confi-dence interval radius will have come under ?.Figure 1: A schematic view of the system.4 Experiments4.1 System DescriptionThe negotiation strategy of the system is hard-coded (see Figure 1).
The system starts eachdialogue proposing to the user its first avail-ability (module 1).
Then, if the user rejectsthe proposition, the system asks them to givetheir first availability (module 3).
If the firsttwo steps have not resulted in success, the sys-tem proposes its next availabilities (module 2)until an appointment is booked (module 7) orthe system has no more propositions to make(module 8).
When a user proposes a date, thesystem asks for a confirmation through mod-ule 4.
Two error-repair modules (modules 6and 5) notify the user that they have not beenunderstood or heard (in case of a time out).More details can be found in (Laroche et al2011).
Each module has to choose betweenthree actions: uttering with a calm (action 1),neutral (action 2) or dynamic (action 3) tone.In our experiments, user simulation was mod-elled so that the first two alternatives were ar-tificially disadvantaged: the number of failureswas slightly increased whenever one of themwas chosen.
We modelled here the fact thatusers would always prefer the dynamic intona-tion.We ran 2000 simulations, one simulationconsisting of a complete dialogue ending withan update of the state-action value function foreach of the system?s modules.
The followingresults are averages on 100 runs.We set the hanging-up rate to 10%.
?
wasset to 0.05 and ?
to 0.1.
In the following sec-tion, we use the notation (i, j, k) to refer to(mi, sj , ak).22sj is always equal to 1 because the local contextsspace is equal to the module space99Figure 2: Evolution of ?m,s,a for triples (1, 1,1), (1, 1, 3), (4, 1, 2) and (4, 1, 3) accordingto the total number of dialogues.
Users preferaction 3.4.2 ResultsBy the end of our experiments, modules 4, 5and 8 had not ?0.1-converged.
Module 5 wasnot likely to be visited quite often according toour specification of user behaviour.
The samehappened for module 4, only accessible frommodule 3 (see Figure 1), which was not itselfoften visited.
Module 1 is, with module 8, astarting module of the system.
At the begin-ning of a dialogue, module 1 had a 95% proba-bility of being visited whereas this probabilitywas of 5% for module 8 (this only happenedwhen all available appointments had alreadybeen booked).
Therefore, module 1 was vis-ited once during almost every dialogue.
Wewill now focus on modules 1 and 4 for clarityof presentation.We can conclude from Figure 2 that triple(1, 1, 3) ?0.1-converged after about 640 di-alogues, corresponding to about 425 visitswhereas neither triple (1, 1, 1) nor (4, 1, 2)nor (4, 1, 3) ?0.1-converged, even after 2000 di-alogues.
Indeed, these triples did not receiveenough visits during the simulations.
Triple(1, 1, 3) ?0.1-converged whereas (1, 1, 1) didnot because, at one point, the growth of thenumber of visits to (1, 1, 1) slowed down asmodule 1 favoured action 3 and reduced itsexploration of other actions.
The fact is thatthe RL algorithm did not need such a preciseestimation for (1, 1, 1) to understand action 1(the neutral tone) was suboptimal.The variance over the 100 runs of the finalestimation of ?m,s,a was below 0.01.
For alltriples of the system, the variance was verylow after about 500 dialogues only (from 10?5to 0.02).
This means that the approximateuser behaviour, defined with probability win-dows, only had a limited impact on the reli-ability of the computed confidence intervals.The probability windows used in the experi-ments were narrow (of an average size of 10%)so user behaviour did not change drasticallyfrom a run to another.
With a behaviour muchmore erratic (larger probability windows), thevariance over 10 runs was higher but did notexceed 0.02.5 Related WorkSuendermann et al(2010) tackled the issue ofreducing the risk induced by on-line learningfor commercial SDS with contender-based di-alogue management.
Our study relates to thiswork but within the more complex learningstructure of RL.Closer to our study, Tetreault et al(2007)compared confidence intervals for the expectedreturn for different MDPs, all modelling thesame SDS but with a different state space.They showed how the intervals bounds as wellas the expected cumulative returns estima-tions could be used in order to select an appro-priate state space.
More recently, Daubigneyet al(2011) as well as Gasic et al(2011) de-veloped an efficient exploration strategy for anMDP-based DM based on the uncertainties onthe expected returns estimations.
The differ-ence between these two approaches and oursis that they compute the confidence intervalsfor a known policy whereas we compute theexpected confidence intervals for an unknownpolicy that will be learnt on-line.6 ConclusionTo help the development of SDS embeddingon-line RL, we have designed and implementedan algorithm which computes the normalisedconfidence interval radius for the value of astate-action couple.
We have illustrated thisalgorithm on an appointment scheduling SDS.We believe our method can be transferred toany system implementing an RL episodic task,as long as the environment can be simulated.100ReferencesSenthilkumar Chandramohan, Matthieu Geist, FabriceLefe`vre, and Olivier Pietquin.
2011.
User simula-tion in dialogue systems using inverse reinforcementlearning.
In Proceedings of Interspeech.Lucie Daubigney, Milica Gasic, Senthilkumar Chan-dramohan, Matthieu Geist, Olivier Pietquin, andSteve Young.
2011.
Uncertainty management foron-line optimisation of a pomdp-based large-scalespoken dialogue system.
In Proceedings of Inter-speech, pages 1301?1304.Wieland Eckert, Esther Levin, and Roberto Pieraccini.1997.
User modeling for spoken dialogue systemevaluation.
In Proceedings of IEEE ASRU, pages80?87.Milica Gasic, Filip Jurcicek, Blaise Thomson, Kai Yu,and Steve Young.
2011.
On-line policy optimisationof spoken dialogue systems via live interaction withhuman subjects.
In Proceedings of IEEE ASRU.Kallirroi Georgila, James Henderson, and OliverLemon.
2006.
User simulation for spoken dialoguesystems: Learning and evaluation.
In Proceedings ofInterspeech.Romain Laroche, Ghislain Putois, Philippe Bretier,and Bernadette Bouchon-Meunier.
2009.
Hybridis-ation of expertise and reinforcement learning in di-alogue systems.
In Proceedings of Interspeech.Romain Laroche, Ghislain Putois, Philippe Bretier,Martin Aranguren, Julia Velkovska, Helen Hastie,Simon Keizer, Kai Yu, Filip Jurcicek, Oliver Lemon,and Steve Young.
2011.
D6.4: Final evaluationof classic towninfo and appointment scheduling sys-tems.
Technical report, CLASSIC Project.Olivier Pietquin and Helen Hastie.
2010.
Metrics forthe evaluation of user simulations.
Technical ReportDeliverable 3.5, CLASSIC Project.Ghislain Putois, Romain Laroche, and PhilippeBretier.
2010.
Enhanced monitoring tools and on-line dialogue optimisation merged into a new spokendialogue system design experience.
In Proceedings ofSIGdial Workshop on Discourse and Dialogue, pages185?192.Antoine Raux, Brian Langner, Allan Black, and Max-ine Eskenazi.
2003.
LET?S GO: Improving SpokenDialog Systems for the Elderly and Non-natives.
InProceedings of Eurospeech.Konrad Scheffler and Steve Young.
2002.
Automaticlearning of dialogue strategy using dialogue simula-tion and reinforcement learning.
In Proceedings ofHLT, pages 12?18.David Suendermann, John Liscombe, and RobertoPieraccini.
2010.
Contender.
In Proceedings ofIEEE SLT, pages 330?335.Joel R. Tetreault, Dan Bohus, and Diane J. Litman.2007.
Estimating the reliability of mdp policies:A confidence interval approach.
In Proceedings ofHLT-NAACL, pages 276?283.Fan Yang and Peter A. Heeman.
2007.
Exploring ini-tiative strategies using computer simulation.
In Pro-ceedings of Interspeech, pages 106?109.101
