Statistical Models for Unsupervised Prepositional PhraseAttachmentAdwai t  Ratnaparkh iDept.
of Computer and Information ScienceUniversity of Pennsylvania200 South 33rd StreetPhiladelphia, PA 19104-6389adwai t~unagi ,  c i s .
upenn, eduAbst rac tWe present several unsupervised statisticalmodels for the prepositional phrase attachmenttask that approach the accuracy of the best su-pervised methods for this task.
Our unsuper-vised approach uses a heuristic based on at-tachment proximity and trains from raw textthat is annotated with only part-of-speech tagsand morphological base forms, as opposed toattachment information.
It is therefore lessresource-intensive and more portable than pre-vious corpus-based algorithm proposed for thistask.
We present results for prepositionalphrase attachment in both English and Span-ish.1 In t roduct ionPrepositional phrase attachment is the task ofdeciding, for a given preposition in a sentence,the attachment site that corresponds to theinterpretation of the sentence.
For example,the task in the following examples is to de-cide whether the preposition with modifies thepreceding noun phrase (with head word shirt)or the preceding verb phrase (with head wordbought or washed).1.
I bought the shirt with pockets.2.
I washed the shirt with soap.In sentence 1, with modifies the noun shirt, sincewith pockets describes the shirt.
However in sen-tence 2, with modifies the verb washed since withsoap describes how the shirt is washed.
Whilethis form of attachment ambiguity is usuallyeasy for people to resolve, a computer requiresdetailed knowledge about words (e.g., washedvs.
bought) in order to successfully resolve suchambiguities and predict the correct interpreta-tion.10792 P rev ious  WorkMost of the previous successful approaches tothis problem have been statistical or corpus-based, and they consider only prepositionswhose attachment is ambiguous between a pre-ceding noun phrase and verb phrase.
Previouswork has framed the problem as a classificationtask, in which the goal is to predict N or V, cor-responding to noun or verb attachment, giventhe head verb v, the head noun n, the preposi-tion p, and optionally, the object of the prepo-sition n2.
For example, the (v, n,p, n2) tuplescorresponding to the example sentences are1.
bought shirt with pockets2.
washed shirt with soapThe correct classifications of tuples 1 and 2 areN and V, respectively.
(Hindle and Rooth, 1993) describes a par-tially supervised approach in which the FID-DITCH partial parser was used to extract(v,n,p) tuples from raw text, where p is apreposition whose attachment is ambiguous be-tween the head verb v and the head noun n.The extracted tuples are then used to con-struct a classifier, which resolves unseen ambi-guities at around 80% accuracy.
Later work,such as (Ratnaparkhi et al, 1994; Brill andResnik, 1994; Collins and Brooks, 1995; Merloet al, 1997; Zavrel and Daelemans, 1997; Franz,1997), trains and tests on quintuples of theform (v,n,p, n2,a) extracted from the Penntreebank(Marcus et al, 1994), and has gradu-ally improved on this accuracy with other kindsof statistical learning methods, yielding up to84.5% accuracy(Collins and Brooks, 1995).
Re-cently, (Stetina nd Nagao, 1997) have reported88% accuracy by using a corpus-based model inconjunction with a semantic dictionary.While previous corpus-based methods arehighly accurate for this task, they are difficultto port to other languages because they re-quire resources that are expensive to constructor simply nonexistent in other languages.
Wepresent an unsupervised algorithm for prepo-sitional phrase attachment in English that re-quires only an part-of-speech tagger and a mor-phology database, and is therefore less resource-intensive and more portable than previous ap-proaches, which have all required either tree-banks or partial parsers.3 Unsuperv ised  Prepos i t iona lPhrase  At tachmentThe exact task of our algorithm will be to con-struct a classifier cl which maps an instance ofan ambiguous prepositional phrase (v, n, p, n2)to either N or V, corresponding to noun at-tachment or verb attachment, respectively.
Inthe full natural anguage parsing task, there aremore than just two potential attachment sites,but we limit our task to choosing between a verbv and a noun n so that we may compare withprevious supervised attempts on this problem.While we will be given the candidate attach-ment sites during testing, the training proce-dure assumes no a priori information about po-tential attachment sites.3.1 Generat ing Training Data FromRaw TextWe generate training data from raw text byusing a part-of-speech tagger, a simple chun-ker, an extraction heuristic, and a morphologydatabase.
The order in which these tools areapplied to raw text is shown in Table 1.
Thetagger from (Ratnaparkhi, 1996) first annotatessentences of raw text with a sequence of part-of-speech tags.
The chunker, implemented withtwo small regular expressions, then replacessimple noun phrases and quantifier phrases withtheir head words.
The extraction heuristic thenfinds head word tuples and their likely attach-ments from the tagged and chunked text.
Theheuristic relies on the observed fact that in En-glish and in languages with similar word order,the attachment site of a preposition is usuallylocated only a few words to the left of the prepo-sition.
Finally, numbers are replaced by a singletoken, the text is converted to lower case, andthe morphology database is used to find the baseforms of the verbs and nouns.The extracted head word tuples differ fromthe training data used in previous upervised at-tempts in an important way.
In the supervisedcase, both of the potential sites, namely the verbv and the noun n are known before the attach-ment is resolved.
In the unsupervised case dis-cussed here, the extraction heuristic only findswhat it thinks are unambiguous cases of prepo-sitional phrase attachment.
Therefore, there isonly one possible attachment site for the prepo-sition, and either the verb v or the noun n doesnot exist, in the case of noun-attached prepo-sition or a verb-attached preposition, respec-tively.
This extraction heuristic loosely resem-bles a step in the bootstrapping procedure usedto get training data for the classifier of (Hindleand Rooth, 1993).
In that step, unambiguousattachments from the FIDDITCH parser's out-put are initially used to resolve some of the am-biguous attachments, and the resolved cases areiteratively used to disambiguate the remainingunresolved cases.
Our procedure differs criti-cally from (Hindle and Rooth, 1993) in that wedo not iterate, we extract unambiguous attach-ments from unparsed input sentences, and wetotally ignore the ambiguous cases.
It is the hy-pothesis of this approach that the informationin just the unambiguous attachment events canresolve the ambiguous attachment events of thetest data.3.1.1 Heur i s t i c  Ext ract ion  o fUnambiguous  CasesGiven a tagged and chunked sentence, the ex-traction heuristic returns head word tuples ofthe form (v,p, n2) or (n,p, n2), where v is theverb, n is the noun, p is the preposition, n2 isthe object of the preposition.
The main ideaof the extraction heuristic is that an attach-ment site of a preposition is usually within afew words to the left of the preposition.
Weextract :(v,p, n2) if?
p is a preposition (p ~ of)?
v is the first verb that occurs within Kwords to the left of p?
v is not a form of the verb to be?
No noun occurs between v and p1080Tool OutputRaw TextPOS TaggerChunkerExtraction HeuristicMorphologyThe professional conduct of lawyers in other jurisdictions i guided by Amer-ican Bar Association rules or by state bar ethics codes, none of which permitnon-lawyers to be partners in law firms.The/DT professional/JJ conduct/NN of/IN lawyers/NNS in/IN other/J Jjurisdictions/NNS is/VBZ guided/VBN by/IN American/NNP Bar/NNPAssociation/NNP rules/NNS or/CC by/IN state/NN bar/NN ethics/NNScodes/NNS ,/, none/NN of/IN which/WDT permit/VBP non-lawyers/NNSto/TO be/VB partners/NNS in/IN law/NN firms/NNS ./.conduct/NN of/IN lawyers/NNS in/IN jurisdictions/NNS is/VBZguided/VBN by/IN rules/NNS or/CC by/IN codes/NNS ,/, none/NNof/IN which/WDW permit/VBP non-lawyers/NNS to/TO be/VS part-ners/NNS in/IN firms/NNS ./.
(n =lawyers, p =in, n2 =jurisdictions)(v =guided, p =by, n2 =rules)(n =lawyer, p =in, n2 =jurisdiction)(v =guide, p =by, n2 =rule)Table 1: How to obtain training data from raw text?
n2 is the first noun that occurs withinK words to the right of p?
No verb occurs between p and n2(n,p, n2) if?
p is a preposition (p ~ of)?
n is the first noun that occurs withinK words to the left of p?
No verb occurs within K words to theleft of p?
n2 is the first noun that occurs withinK words to the right of p?
No verb occurs between p and n2Table 1 also shows the result of the applying theextraction heuristic to a sample sentence.The heuristic ignores cases where p = of,since such cases are rarely ambiguous, and weopt to model them deterministically as noun at-tachments.
We will report accuracies (in Sec-tion 5) on both cases where p = of  and wherep ~ of.
Also, the heuristic excludes exampleswith the verb to be from the training set (butnot the test set) since we found them to be un-reliable sources of evidence.10813.2 Accuracy of  Extract ion Heurist icApplying the extraction heuristic to 970K unan-notated sentences from the 1988 Wall St. Jour-nal 1 data yields approximately 910K uniquehead word tuples of the form (v,p, n2) or(n,p, n2).
The extraction heuristic is far fromperfect; when applied to and compared with theannotated Wall St. Journal data of the Penntreebank, only 69% of the extracted head wordtuples represent correct attachments.
2 The ex-tracted tuples are meant o be a noisy but abun-dant substitute for the information that onemight get from a treebank.
Tables 2 and 3list the most frequent extracted head word tu-ples for unambiguous verb and noun attach-ments, respectively.
Many of the frequent noun-attached (n,p, n2) tuples, such as hum to num, 3are incorrect.
The prepositional phrase to humis usually attached to a verb such as rise or fallin the Wall St. Journal domain, e.g., Profitsrose ,{6 ~ to 52 million.1This data is available from the Linguistic Data Con-sortium, http ://www.
Idc.
apenn, edu2This accuracy also excludes cases where p -- of.3Recall the hum is the token for quantifier phrasesidentified by the chunker, like 5 million, or 6 ~.Frequency Verb8110 close1926 reach1539 rise1438 compare1072 fall970 account887 value839 say680 compare673 price\] Prep \] Noun2at numfor commentto numwith numto humfor humat millionin interviewwith millionat numTable 2: Most frequent (v,p, n2) tuplesFrequency Noun1983 num923 num853 share723 trading721 num560 num519 share461 hum417 trading376 shareI Prep \[ Noun2to numfrom numfrom millionon exchangein numto monthon revenueto dayon yesterdayon saleTable 3: Most frequent (n,p, n2) tuples4 S ta t i s t i ca l  Mode lsWhile the extracted tuples of the form (n, p, n2)and (v, p, n2) represent unambiguous noun andverb attachments in which either the verb ornoun is known, our eventual goal is to resolveambiguous attachments in the test data of theform (v, n,p, n2), in which both the noun n andverb v are always known.
We therefore mustuse any information in the unambiguous casesto resolve the ambiguous cases.
A natural way isto use a classifier that compares the probabilityof each outcome:cl(v,n,p, n2) =N if p = ofarg maxae{N,V} Pr(v,n,p, a) otherwise(1)We do not currently use n2 in the probabilitymodel, and we omit it from further discussion.We can factor Pr(v,n,p, a) as follows:Pr(v,n,p,a) = Pr(v)Pr(n)Pr(a\[v,n)Pr(p\[a, v, n)The terms Pr(n) and Pr(v) are independent ofthe attachment a and need not be computedin d (1), but the estimation of Pr(a\[v,n) andPr(pla, v,n ) is problematic since our trainingdata, i.e., the head words extracted from rawtext, occur with either n or v, but never bothn, v. This leads to make some heuristically mo-tivated approximations.
Let the random vari-able ?
range over {true, false}, and let it de-note the presence or absence of any prepositionthat is unambiguously attached to the noun orverb in question.
Then p(?
= true\]n) is theconditional probability that a particular nounn in free text has an unambiguous prepositionalphrase attachment.
(?
= true will be writtensimply as true.)
We approximate Pr(alv , n) asfollows:Pr(true\[n) Pr(a -- N\]v, n) Z(v,n)Pr(truelv) Pr(a = VIv ,n) Z(v,n)Z(v,n) = Pr(true\[n) + Pr(trueIv )The rationale behind this approximation is thatthe tendency of a v,n pair towards a noun(verb) attachment is related to the tendency ofthe noun (verb) alone to occur with an unam-biguous prepositional phrase.
The Z(v, n) termexists only to make the approximation a wellformed probability over a E {N, V}.We approximate Pr(p\[a, v, n) as follows:Pr(p\[a = N, v, n) .~ Pr(p\[true, n)Pr(p\[a = V,v,n) ~ Pr(pItrue, v)The rationale behind these approximations ithat when generating p given a noun (verb) at-tachment, only the counts involving the noun(verb) are relevant, assuming also that the noun(verb) has an attached prepositional phrase, i.e.,d?
= true.We use word statistics from both the taggedcorpus and the set of extracted head word tuplesto estimate the probability of generating ?
=true, p, and n2.
Counts from the extracted setof tuples assume that ?
-- true, while countsfrom the corpus itself may correspond to eitherq5 = true or ?
= false, depending on if the noun1082or verb in question is, or is not, respectively,unambiguously attached to a preposition.4.1 Generate  ?The quantities Pr(trueln ) and Pr(truelv ) de-note the conditional probability that n or vwill occur with some unambiguously attachedpreposition, and are estimated as follows:{ c(n) > 0Pr(trueln) = .5 otherwise> oPr(truelv) = .5 otherwisewhere c(n) and c(v) are counts from the taggedcorpus, and where c(n, true) and c(v, true) arecounts from the extracted head word tuples.4.2 Generate  pThe terms Pr(p\[n, true) and Pr(plv, true) de-note the conditional probability that a particu-lar preposition p will occur as an unambiguousattachment to n or v. We present two tech-niques to estimate this probability, one basedon bigram counts and another based on an in-terpolation method.4.2.1 B igram CountsThis technique uses the bigram counts of theextracted head word tuples, and backs off tothe uniform distribution when the denominatoris zero.c(n,p,true)Pr(pltrue, n) = ~(n,true) c(n, true) > 0otherwisec(v,p,true)Pr(pltrue ,v) = ~(v,tr~,) c(v, true) > 0otherwisewhere ~ is the set of possible prepositions,where all the counts c(..
.)
are from the ex-tracted head word tuples.4.2.2 InterpolationThis technique is similar to the one in (Hindleand Rooth, 1993), and interpolates between thetendencies of the (v,p) and (n,p) bigrams andthe tendency of the type of attachment (e.g., Nor V) towards a particular preposition p. First,define cN(p) = ~n c(n,p, true) as the numberof noun attached tuples with the prepositionp, and define C N = ~'~pCN(P) as the numberof noun attached tuples.
Analogously, definecy(p) = ~vc(v,p, true) and cy = ~pcv(p).The counts c(n,p, true) and c(v,p, true) arefrom the extracted head word tuples.
Using theabove notation, we can interpolate as follows:Pr(pltrue, n)Pr(pltrue ,v)c(n,p, true) + c~(p) CNc(n, true) + 1c(v,p, true) + cv(P) cvc(v, true) + 15 Evaluation in EnglishApproximately 970K unannotated sentencesfrom the 1988 Wall St. Journal were pro-cessed in a manner identical to the example sen-tence in Table 1.
The result was approximately910,000 head word tuples of the form (v,p, n2)or (n,p, n2).
Note that while the head wordtuples represent correct attachments only 69%of the time, their quantity is about 45 timesgreater than the quantity of data used in previ-ous supervised approaches.
The extracted atawas used as training material for the three clas-sifters Clbase , Clinterp, and Clbigram.
Each classi-fier is constructed as follows:Clbase This is the "baseline" classifier that pre-dicts N of p = of, and V otherwise.Clinterp: This classifier has the form of equa-tion (1), uses the method in section 4.1 togenerate ?, and the method in section 4.2.2to generate p.clbigram: This classifier has the form of equa-tion (1), uses the method in section 4.1 togenerate ?, and the method in section 4.2.1to generate p.Table 4 shows accuracies of the classifiers onthe test set of (Ratnaparkhi et al, 1994), whichis derived from the manually annotated attach-ments in the Penn Treebank Wall St. Journaldata.
The Penn Treebank is drawn from the1989 Wall St. Journal data, so there is no pos-sibility of overlap with our training data.
Fur-thermore, the extraction heuristic was devel-oped and tuned on a "development set", i.e., aset of annotated examples that did not overlapwith either the test set or the training set.1083Subsetp= ofNumber of Events925clbigrarn917Clinterp917Clbase917p ~ of 2172 1620 1618 1263Total 3097 2i80 253581.85%253781.91% Accuracy 70.39%Table 4: Accuracy of mostly unsupervised classifiers on English Wall St. Journal dataAttachment Pr(alv ,n) Pr(p\[a,v,n)Noun(a = N) .02 .24Verb(a = V) .30 .44Table 5: The key probabilities for the ambigu-ous example rise hum to humTable 5 shows the two probabilities Pr(a\[v, n)and Pr(p\[a, v, n), using the same approxima-tions as  clbigram, for the ambiguous example risenum to num.
(Recall that Pr(v) and Pr(n) arenot needed.)
While the tuple (num, to, num) ismore frequent han (rise, to, num), the condi-tional probabilities prefer a = V, which is thechoice that maximizes Pr(v, n,p, a).Both classifiers Clinter p and dbigram clearlyoutperform the.
baseline, but the classifierdinterp does not outperform dbigram, eventhough it interpolates between the less specificevidence (the preposition counts) and more spe-cific evidence (the bigram counts).
This may bedue to the errors in our extracted training data;supervised classifiers that train from clean datatypically benefit greatly by combining less spe-cific evidence with more specific evidence.Despite the errors in the training data,the performance of the unsupervised classifiers(81.9%) begins to approach the best perfor-mance of the comparable supervised classifiers(84.5%).
(Our goal is to replicate the super-vision of a treebank, but not a semantic dictio-nary, so we do not compare against (Stetina ndNagao, 1997).)
Furthermore, we do not use thesecond noun n2, whereas the best supervisedmethods use this information.
Our result showsthat the information i  imperfect but abundantdata from unambiguous attachments, asshownin Tables 2 and 3, is sufficient to resolve ambigu-ous prepositional phrase attachments at accu-racies just under the supervised state-of-the-artaccuracy.6 Eva luat ion  in Span ishWe claim that our approach is portable to lan-guages with similar word order, and we supportthis claim by demonstrating our approach onthe Spanish language.
We used the Spanishtagger and morphological analyzer developedat the Xerox Research Centre Europe 4 and wemodified the extraction heuristic to account forthe new tagset, and to account for the Spanishequivalents of the words of (i.e., de or del) andto be (i.e., set).
Chunking was not performedon the Spanish data.
We used 450k sentencesof raw text from the Linguistic Data Consor-tium's Spanish News Text Collection to extracta training set, and we used a non-overlappingset of 50k sentences from the collection to createtest sets.
Three native Spanish speakers wereasked to extract and annotate ambiguous in-stances of Spanish prepositional phrase attach-ments.
They annotated two sets (using the fullsentence context); one set consisted of all am-biguous prepositional phrase attachments of theform (v,n,p, n2), and the other set consisted ofcases where p = con.
For testing our classifier,we used only those judgments on which all threeannotators agreed.6.1 Per fo rmanceThe performance of the classifiers Clbigram,Clinterp, and Clbase , when trained and testedon Spanish language data, are shown in Ta-ble 6.
The Spanish test set has fewer ambiguousprepositions than the English test set, as shownby the accuracy of Clbase.
However, the accuracyimprovements of Clbigra m over  Clbase are statisti-cally significant for both test sets.
54These were supplied by Dr. Lauri Kartunnen duringhis visit to Penn.5Using proportions ofchanged cases, P ---- 0.0258 forthe first set, and P -- 0.0108 for the set where p= con1084Test SetAll pSubsetp = delldelp # delldelAccuracyNumber of Events156116272Clbigrarn15410325794.5%p = con Total 192 166Accuracy - 86.4%clinterp dbase154 15497 91251 24592.3% 90.1%1160115183.3% 78.6%Table 6: Accuracy of mostly unsupervised classifiers on Spanish News Data7 Conclus ionThe unsupervised algorithm for prepositionalphrase attachment presented here is the onlyalgorithm in the published literature that cansignificantly outperform the baseline withoutusing data derived from a treebank or parser.The accuracy of our technique approaches theaccuracy of the best supervised methods, anddoes so with only a tiny fraction of the supervi-sion.
Since only a small part of the extractionheuristic is specific to English, and since part-of-speech taggers and morphology databases arewidely available in other languages, our ap-proach is far more portable than previous ap-proaches for this problem.
We successfullydemonstrated the portability of our approachby applying it to the prepositional phrase at-tachment task in the Spanish language.8 AcknowledgmentsWe thank Dr. Lauri Kartunnen for lending usthe Spanish natural anguage tools, and MikeCollins for helpful discussions on this work.ReferencesACL.
1997.
Proceedings of the 35th AnnualMeeting of the A CL, and 8th Conference ofthe EACL, Madrid, Spain, July.Eric Brill and Phil Resnik.
1994.
A Rule BasedApproach to Prepositional Phrase Attach-ment Disambiguation.
In Proceedings of theFifteenth International Conference on Com-putational Linguistics (COLING).Michael Collins and James Brooks.
1995.Prepositional Phrase Attachment through aBacked-off Model.
In David Yarowsky andKenneth Church, editors, Proceedings of theThird Workshop on Very Large Corpora,pages 27-38, Cambridge, Massachusetts,June.Alexander Franz.
1997.
Independence Assump-tions Considered Harmful.
In ACL (ACL,1997).Donald Hindle and Mats Rooth.
1993.
Struc-tural Ambiguity and Lexical Relations.
Com-putational Linguistics, 19(1):103-120.Mitchell P. Marcus, Beatrice Santorini, andMary Ann Marcinkiewicz.
1994.
Buildinga large annotated corpus of English: thePenn Treebank.
Computational Linguistics,19(2):313-330.Paola Merlo, Matthew W. Crocker, andCathy Berthouzoz.
1997.
Attaching MultiplePrepositional Phrases: Generalized Backed-off Estimation.
In Claire Cardie and RalphWeischedel, editors, Second Conference onEmpirical Methods in Natural Language Pro-cessing, pages 149-155, Providence, R.I.,Aug.
1-2.Adwait Ratnaparkhi, Jeff Reynar, and SalimRoukos.
1994.
A Maximum Entropy Modelfor Prepositional Phrase Attachment.
In Pro-ceedings of the Human Language TechnologyWorkshop, pages 250-255, Plalnsboro, N.J.ARPA.Adwait Ratnaparkhi.
1996.
A Maximum En-tropy Part of Speech Tagger.
In Eric Brilland Kenneth Church, editors, Conference onEmpirical Methods in Natural Language Pro-cessing, University of Pennsylvania, May 17-18.Jiri Stetina and Makoto Nagao.
1997.
CorpusBased PP Attachment Ambiguity Resolutionwith a Semantic Dictionary.
In Jou Zhou andKenneth Church, editors, Proceedings of theFifth Workshop on Very Large Corpora, pages66-80, Beijing and Hong Kong, Aug. 18 - 20.Jakub Zavrel and Walter Daelemans.
1997.Memory-Based Learning: Using Similarityfor Smoothing.
In ACL (ACL, 1997).1085
