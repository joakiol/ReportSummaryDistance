Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746?1751,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsConvolutional Neural Networks for Sentence ClassificationYoon KimNew York Universityyhk255@nyu.eduAbstractWe report on a series of experiments withconvolutional neural networks (CNN)trained on top of pre-trained word vec-tors for sentence-level classification tasks.We show that a simple CNN with lit-tle hyperparameter tuning and static vec-tors achieves excellent results on multi-ple benchmarks.
Learning task-specificvectors through fine-tuning offers furthergains in performance.
We additionallypropose a simple modification to the ar-chitecture to allow for the use of bothtask-specific and static vectors.
The CNNmodels discussed herein improve upon thestate of the art on 4 out of 7 tasks, whichinclude sentiment analysis and questionclassification.1 IntroductionDeep learning models have achieved remarkableresults in computer vision (Krizhevsky et al.,2012) and speech recognition (Graves et al., 2013)in recent years.
Within natural language process-ing, much of the work with deep learning meth-ods has involved learning word vector representa-tions through neural language models (Bengio etal., 2003; Yih et al., 2011; Mikolov et al., 2013)and performing composition over the learned wordvectors for classification (Collobert et al., 2011).Word vectors, wherein words are projected from asparse, 1-of-V encoding (here V is the vocabularysize) onto a lower dimensional vector space via ahidden layer, are essentially feature extractors thatencode semantic features of words in their dimen-sions.
In such dense representations, semanticallyclose words are likewise close?in euclidean orcosine distance?in the lower dimensional vectorspace.Convolutional neural networks (CNN) utilizelayers with convolving filters that are applied tolocal features (LeCun et al., 1998).
Originallyinvented for computer vision, CNN models havesubsequently been shown to be effective for NLPand have achieved excellent results in semanticparsing (Yih et al., 2014), search query retrieval(Shen et al., 2014), sentence modeling (Kalch-brenner et al., 2014), and other traditional NLPtasks (Collobert et al., 2011).In the present work, we train a simple CNN withone layer of convolution on top of word vectorsobtained from an unsupervised neural languagemodel.
These vectors were trained by Mikolov etal.
(2013) on 100 billion words of Google News,and are publicly available.1We initially keep theword vectors static and learn only the other param-eters of the model.
Despite little tuning of hyper-parameters, this simple model achieves excellentresults on multiple benchmarks, suggesting thatthe pre-trained vectors are ?universal?
feature ex-tractors that can be utilized for various classifica-tion tasks.
Learning task-specific vectors throughfine-tuning results in further improvements.
Wefinally describe a simple modification to the archi-tecture to allow for the use of both pre-trained andtask-specific vectors by having multiple channels.Our work is philosophically similar to Razavianet al.
(2014) which showed that for image clas-sification, feature extractors obtained from a pre-trained deep learning model perform well on a va-riety of tasks?including tasks that are very dif-ferent from the original task for which the featureextractors were trained.2 ModelThe model architecture, shown in figure 1, is aslight variant of the CNN architecture of Collobertet al.
(2011).
Let xi?
Rkbe the k-dimensionalword vector corresponding to the i-th word in thesentence.
A sentence of length n (padded where1https://code.google.com/p/word2vec/1746waitforthevideoanddon'trentitn x k representation ofsentence with static andnon-static channelsConvolutional layer withmultiple filter widths andfeature mapsMax-over-timepoolingFully connected layerwith dropout andsoftmax outputFigure 1: Model architecture with two channels for an example sentence.necessary) is represented asx1:n= x1?
x2?
.
.
.?
xn, (1)where ?
is the concatenation operator.
In gen-eral, let xi:i+jrefer to the concatenation of wordsxi,xi+1, .
.
.
,xi+j.
A convolution operation in-volves a filter w ?
Rhk, which is applied to awindow of h words to produce a new feature.
Forexample, a feature ciis generated from a windowof words xi:i+h?1byci= f(w ?
xi:i+h?1+ b).
(2)Here b ?
R is a bias term and f is a non-linearfunction such as the hyperbolic tangent.
This filteris applied to each possible window of words in thesentence {x1:h,x2:h+1, .
.
.
,xn?h+1:n} to producea feature mapc = [c1, c2, .
.
.
, cn?h+1], (3)with c ?
Rn?h+1.
We then apply a max-over-time pooling operation (Collobert et al., 2011)over the feature map and take the maximum valuec?
= max{c} as the feature corresponding to thisparticular filter.
The idea is to capture the most im-portant feature?one with the highest value?foreach feature map.
This pooling scheme naturallydeals with variable sentence lengths.We have described the process by which onefeature is extracted from one filter.
The modeluses multiple filters (with varying window sizes)to obtain multiple features.
These features formthe penultimate layer and are passed to a fully con-nected softmax layer whose output is the probabil-ity distribution over labels.In one of the model variants, we experimentwith having two ?channels?
of word vectors?onethat is kept static throughout training and one thatis fine-tuned via backpropagation (section 3.2).2In the multichannel architecture, illustrated in fig-ure 1, each filter is applied to both channels andthe results are added to calculate ciin equation(2).
The model is otherwise equivalent to the sin-gle channel architecture.2.1 RegularizationFor regularization we employ dropout on thepenultimate layer with a constraint on l2-norms ofthe weight vectors (Hinton et al., 2012).
Dropoutprevents co-adaptation of hidden units by ran-domly dropping out?i.e., setting to zero?a pro-portion p of the hidden units during foward-backpropagation.
That is, given the penultimatelayer z = [c?1, .
.
.
, c?m] (note that here we have mfilters), instead of usingy = w ?
z+ b (4)for output unit y in forward propagation, dropoutusesy = w ?
(z ?
r) + b, (5)where ?
is the element-wise multiplication opera-tor and r ?
Rmis a ?masking?
vector of Bernoullirandom variables with probability p of being 1.Gradients are backpropagated only through theunmasked units.
At test time, the learned weightvectors are scaled by p such that?w = pw, and?w is used (without dropout) to score unseen sen-tences.
We additionally constrain l2-norms of theweight vectors by rescaling w to have ||w||2= swhenever ||w||2> s after a gradient descent step.2We employ language from computer vision where a colorimage has red, green, and blue channels.1747Data c l N |V | |Vpre| TestMR 2 20 10662 18765 16448 CVSST-1 5 18 11855 17836 16262 2210SST-2 2 19 9613 16185 14838 1821Subj 2 23 10000 21323 17913 CVTREC 6 10 5952 9592 9125 500CR 2 19 3775 5340 5046 CVMPQA 2 3 10606 6246 6083 CVTable 1: Summary statistics for the datasets after tokeniza-tion.
c: Number of target classes.
l: Average sentence length.N : Dataset size.
|V |: Vocabulary size.
|Vpre|: Number ofwords present in the set of pre-trained word vectors.
Test:Test set size (CV means there was no standard train/test splitand thus 10-fold CV was used).3 Datasets and Experimental SetupWe test our model on various benchmarks.
Sum-mary statistics of the datasets are in table 1.?
MR: Movie reviews with one sentence per re-view.
Classification involves detecting posi-tive/negative reviews (Pang and Lee, 2005).3?
SST-1: Stanford Sentiment Treebank?anextension of MR but with train/dev/test splitsprovided and fine-grained labels (very pos-itive, positive, neutral, negative, very nega-tive), re-labeled by Socher et al.
(2013).4?
SST-2: Same as SST-1 but with neutral re-views removed and binary labels.?
Subj: Subjectivity dataset where the task isto classify a sentence as being subjective orobjective (Pang and Lee, 2004).?
TREC: TREC question dataset?task in-volves classifying a question into 6 questiontypes (whether the question is about person,location, numeric information, etc.)
(Li andRoth, 2002).5?
CR: Customer reviews of various products(cameras, MP3s etc.).
Task is to predict pos-itive/negative reviews (Hu and Liu, 2004).63https://www.cs.cornell.edu/people/pabo/movie-review-data/4http://nlp.stanford.edu/sentiment/ Data is actually providedat the phrase-level and hence we train the model on bothphrases and sentences but only score on sentences at testtime, as in Socher et al.
(2013), Kalchbrenner et al.
(2014),and Le and Mikolov (2014).
Thus the training set is an orderof magnitude larger than listed in table 1.5http://cogcomp.cs.illinois.edu/Data/QA/QC/6http://www.cs.uic.edu/?liub/FBS/sentiment-analysis.html?
MPQA: Opinion polarity detection subtaskof the MPQA dataset (Wiebe et al., 2005).73.1 Hyperparameters and TrainingFor all datasets we use: rectified linear units, filterwindows (h) of 3, 4, 5 with 100 feature maps each,dropout rate (p) of 0.5, l2constraint (s) of 3, andmini-batch size of 50.
These values were chosenvia a grid search on the SST-2 dev set.We do not otherwise perform any dataset-specific tuning other than early stopping on devsets.
For datasets without a standard dev set werandomly select 10% of the training data as thedev set.
Training is done through stochastic gra-dient descent over shuffled mini-batches with theAdadelta update rule (Zeiler, 2012).3.2 Pre-trained Word VectorsInitializing word vectors with those obtained froman unsupervised neural language model is a popu-lar method to improve performance in the absenceof a large supervised training set (Collobert et al.,2011; Socher et al., 2011; Iyyer et al., 2014).
Weuse the publicly available word2vec vectors thatwere trained on 100 billion words from GoogleNews.
The vectors have dimensionality of 300 andwere trained using the continuous bag-of-wordsarchitecture (Mikolov et al., 2013).
Words notpresent in the set of pre-trained words are initial-ized randomly.3.3 Model VariationsWe experiment with several variants of the model.?
CNN-rand: Our baseline model where allwords are randomly initialized and then mod-ified during training.?
CNN-static: A model with pre-trainedvectors from word2vec.
All words?including the unknown ones that are ran-domly initialized?are kept static and onlythe other parameters of the model are learned.?
CNN-non-static: Same as above but the pre-trained vectors are fine-tuned for each task.?
CNN-multichannel: A model with two setsof word vectors.
Each set of vectors is treatedas a ?channel?
and each filter is applied7http://www.cs.pitt.edu/mpqa/1748Model MR SST-1 SST-2 Subj TREC CR MPQACNN-rand 76.1 45.0 82.7 89.6 91.2 79.8 83.4CNN-static 81.0 45.5 86.8 93.0 92.8 84.7 89.6CNN-non-static 81.5 48.0 87.2 93.4 93.6 84.3 89.5CNN-multichannel 81.1 47.4 88.1 93.2 92.2 85.0 89.4RAE (Socher et al., 2011) 77.7 43.2 82.4 ?
?
?
86.4MV-RNN (Socher et al., 2012) 79.0 44.4 82.9 ?
?
?
?RNTN (Socher et al., 2013) ?
45.7 85.4 ?
?
?
?DCNN (Kalchbrenner et al., 2014) ?
48.5 86.8 ?
93.0 ?
?Paragraph-Vec (Le and Mikolov, 2014) ?
48.7 87.8 ?
?
?
?CCAE (Hermann and Blunsom, 2013) 77.8 ?
?
?
?
?
87.2Sent-Parser (Dong et al., 2014) 79.5 ?
?
?
?
?
86.3NBSVM (Wang and Manning, 2012) 79.4 ?
?
93.2 ?
81.8 86.3MNB (Wang and Manning, 2012) 79.0 ?
?
93.6 ?
80.0 86.3G-Dropout (Wang and Manning, 2013) 79.0 ?
?
93.4 ?
82.1 86.1F-Dropout (Wang and Manning, 2013) 79.1 ?
?
93.6 ?
81.9 86.3Tree-CRF (Nakagawa et al., 2010) 77.3 ?
?
?
?
81.4 86.1CRF-PR (Yang and Cardie, 2014) ?
?
?
?
?
82.7 ?SVMS(Silva et al., 2011) ?
?
?
?
95.0 ?
?Table 2: Results of our CNN models against other methods.
RAE: Recursive Autoencoders with pre-trained word vectors fromWikipedia (Socher et al., 2011).
MV-RNN: Matrix-Vector Recursive Neural Network with parse trees (Socher et al., 2012).RNTN: Recursive Neural Tensor Network with tensor-based feature function and parse trees (Socher et al., 2013).
DCNN:Dynamic Convolutional Neural Network with k-max pooling (Kalchbrenner et al., 2014).
Paragraph-Vec: Logistic regres-sion on top of paragraph vectors (Le and Mikolov, 2014).
CCAE: Combinatorial Category Autoencoders with combinatorialcategory grammar operators (Hermann and Blunsom, 2013).
Sent-Parser: Sentiment analysis-specific parser (Dong et al.,2014).
NBSVM, MNB: Naive Bayes SVM and Multinomial Naive Bayes with uni-bigrams from Wang and Manning (2012).G-Dropout, F-Dropout: Gaussian Dropout and Fast Dropout from Wang and Manning (2013).
Tree-CRF: Dependency treewith Conditional Random Fields (Nakagawa et al., 2010).
CRF-PR: Conditional Random Fields with Posterior Regularization(Yang and Cardie, 2014).
SVMS: SVM with uni-bi-trigrams, wh word, head word, POS, parser, hypernyms, and 60 hand-codedrules as features from Silva et al.
(2011).to both channels, but gradients are back-propagated only through one of the chan-nels.
Hence the model is able to fine-tuneone set of vectors while keeping the otherstatic.
Both channels are initialized withword2vec.In order to disentangle the effect of the abovevariations versus other random factors, we elim-inate other sources of randomness?CV-fold as-signment, initialization of unknown word vec-tors, initialization of CNN parameters?by keep-ing them uniform within each dataset.4 Results and DiscussionResults of our models against other methods arelisted in table 2.
Our baseline model with all ran-domly initialized words (CNN-rand) does not per-form well on its own.
While we had expected per-formance gains through the use of pre-trained vec-tors, we were surprised at the magnitude of thegains.
Even a simple model with static vectors(CNN-static) performs remarkably well, givingcompetitive results against the more sophisticateddeep learning models that utilize complex pool-ing schemes (Kalchbrenner et al., 2014) or requireparse trees to be computed beforehand (Socheret al., 2013).
These results suggest that the pre-trained vectors are good, ?universal?
feature ex-tractors and can be utilized across datasets.
Fine-tuning the pre-trained vectors for each task givesstill further improvements (CNN-non-static).4.1 Multichannel vs.
Single Channel ModelsWe had initially hoped that the multichannel ar-chitecture would prevent overfitting (by ensuringthat the learned vectors do not deviate too farfrom the original values) and thus work better thanthe single channel model, especially on smallerdatasets.
The results, however, are mixed, and fur-ther work on regularizing the fine-tuning processis warranted.
For instance, instead of using anadditional channel for the non-static portion, onecould maintain a single channel but employ extradimensions that are allowed to be modified duringtraining.1749Most Similar Words forStatic Channel Non-static Channelbadgood terribleterrible horriblehorrible lousylousy stupidgoodgreat nicebad decentterrific soliddecent terrificn?tos notca neverireland nothingwo neither!2,500 2,500entire lushjez beautifulchanger terrific,decasia butabysmally dragondemise avaliant andTable 3: Top 4 neighboring words?based on cosinesimilarity?for vectors in the static channel (left) and fine-tuned vectors in the non-static channel (right) from the mul-tichannel model on the SST-2 dataset after training.4.2 Static vs. Non-static RepresentationsAs is the case with the single channel non-staticmodel, the multichannel model is able to fine-tunethe non-static channel to make it more specific tothe task-at-hand.
For example, good is most sim-ilar to bad in word2vec, presumably becausethey are (almost) syntactically equivalent.
But forvectors in the non-static channel that were fine-tuned on the SST-2 dataset, this is no longer thecase (table 3).
Similarly, good is arguably closerto nice than it is to great for expressing sentiment,and this is indeed reflected in the learned vectors.For (randomly initialized) tokens not in the setof pre-trained vectors, fine-tuning allows them tolearn more meaningful representations: the net-work learns that exclamation marks are associ-ated with effusive expressions and that commasare conjunctive (table 3).4.3 Further ObservationsWe report on some further experiments and obser-vations:?
Kalchbrenner et al.
(2014) report muchworse results with a CNN that has essentiallythe same architecture as our single channelmodel.
For example, their Max-TDNN (TimeDelay Neural Network) with randomly ini-tialized words obtains 37.4% on the SST-1dataset, compared to 45.0% for our model.We attribute such discrepancy to our CNNhaving much more capacity (multiple filterwidths and feature maps).?
Dropout proved to be such a good regularizerthat it was fine to use a larger than necessarynetwork and simply let dropout regularize it.Dropout consistently added 2%?4% relativeperformance.?
When randomly initializing words not inword2vec, we obtained slight improve-ments by sampling each dimension fromU [?a, a] where a was chosen such that therandomly initialized vectors have the samevariance as the pre-trained ones.
It would beinteresting to see if employing more sophis-ticated methods to mirror the distribution ofpre-trained vectors in the initialization pro-cess gives further improvements.?
We briefly experimented with another set ofpublicly available word vectors trained byCollobert et al.
(2011) on Wikipedia,8andfound that word2vec gave far superior per-formance.
It is not clear whether this is dueto Mikolov et al.
(2013)?s architecture or the100 billion word Google News dataset.?
Adadelta (Zeiler, 2012) gave similar resultsto Adagrad (Duchi et al., 2011) but requiredfewer epochs.5 ConclusionIn the present work we have described a series ofexperiments with convolutional neural networksbuilt on top of word2vec.
Despite little tuningof hyperparameters, a simple CNN with one layerof convolution performs remarkably well.
Our re-sults add to the well-established evidence that un-supervised pre-training of word vectors is an im-portant ingredient in deep learning for NLP.AcknowledgmentsWe would like to thank Yann LeCun and theanonymous reviewers for their helpful feedbackand suggestions.8http://ronan.collobert.com/senna/1750ReferencesY.
Bengio, R. Ducharme, P. Vincent.
2003.
Neu-ral Probabilitistic Language Model.
Journal of Ma-chine Learning Research 3:1137?1155.R.
Collobert, J. Weston, L. Bottou, M. Karlen, K.Kavukcuglu, P. Kuksa.
2011.
Natural LanguageProcessing (Almost) from Scratch.
Journal of Ma-chine Learning Research 12:2493?2537.J.
Duchi, E. Hazan, Y.
Singer.
2011 Adaptive subgra-dient methods for online learning and stochastic op-timization.
Journal of Machine Learning Research,12:2121?2159.L.
Dong, F. Wei, S. Liu, M. Zhou, K. Xu.
2014.
AStatistical Parsing Framework for Sentiment Classi-fication.
CoRR, abs/1401.6330.A.
Graves, A. Mohamed, G. Hinton.
2013.
Speechrecognition with deep recurrent neural networks.
InProceedings of ICASSP 2013.G.
Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever,R.
Salakhutdinov.
2012.
Improving neural net-works by preventing co-adaptation of feature detec-tors.
CoRR, abs/1207.0580.K.
Hermann, P. Blunsom.
2013.
The Role of Syntax inVector Space Models of Compositional Semantics.In Proceedings of ACL 2013.M.
Hu, B. Liu.
2004.
Mining and Summarizing Cus-tomer Reviews.
In Proceedings of ACM SIGKDD2004.M.
Iyyer, P. Enns, J. Boyd-Graber, P. Resnik 2014.Political Ideology Detection Using Recursive NeuralNetworks.
In Proceedings of ACL 2014.N.
Kalchbrenner, E. Grefenstette, P. Blunsom.
2014.
AConvolutional Neural Network for Modelling Sen-tences.
In Proceedings of ACL 2014.A.
Krizhevsky, I. Sutskever, G. Hinton.
2012.
Ima-geNet Classification with Deep Convolutional Neu-ral Networks.
In Proceedings of NIPS 2012.Q.
Le, T. Mikolov.
2014.
Distributed Represenationsof Sentences and Documents.
In Proceedings ofICML 2014.Y.
LeCun, L. Bottou, Y. Bengio, P. Haffner.
1998.Gradient-based learning applied to document recog-nition.
In Proceedings of the IEEE, 86(11):2278?2324, November.X.
Li, D. Roth.
2002.
Learning Question Classifiers.In Proceedings of ACL 2002.T.
Mikolov, I. Sutskever, K. Chen, G. Corrado, J. Dean.2013.
Distributed Representations of Words andPhrases and their Compositionality.
In Proceedingsof NIPS 2013.T.
Nakagawa, K. Inui, S. Kurohashi.
2010.
De-pendency tree-based sentiment classification usingCRFs with hidden variables.
In Proceedings of ACL2010.B.
Pang, L. Lee.
2004.
A sentimental education:Sentiment analysis using subjectivity summarizationbased on minimum cuts.
In Proceedings of ACL2004.B.
Pang, L. Lee.
2005.
Seeing stars: Exploiting classrelationships for sentiment categorization with re-spect to rating scales.
In Proceedings of ACL 2005.A.S.
Razavian, H. Azizpour, J. Sullivan, S. Carlsson2014.
CNN Features off-the-shelf: an AstoundingBaseline.
CoRR, abs/1403.6382.Y.
Shen, X.
He, J. Gao, L. Deng, G. Mesnil.
2014.Learning Semantic Representations Using Convolu-tional Neural Networks for Web Search.
In Proceed-ings of WWW 2014.J.
Silva, L. Coheur, A. Mendes, A. Wichert.
2011.From symbolic to sub-symbolic information in ques-tion classification.
Artificial Intelligence Review,35(2):137?154.R.
Socher, J. Pennington, E. Huang, A. Ng, C. Man-ning.
2011.
Semi-Supervised Recursive Autoen-coders for Predicting Sentiment Distributions.
InProceedings of EMNLP 2011.R.
Socher, B. Huval, C. Manning, A. Ng.
2012.
Se-mantic Compositionality through Recursive Matrix-Vector Spaces.
In Proceedings of EMNLP 2012.R.
Socher, A. Perelygin, J. Wu, J. Chuang, C. Manning,A.
Ng, C. Potts.
2013.
Recursive Deep Models forSemantic Compositionality Over a Sentiment Tree-bank.
In Proceedings of EMNLP 2013.J.
Wiebe, T. Wilson, C. Cardie.
2005.
Annotating Ex-pressions of Opinions and Emotions in Language.Language Resources and Evaluation, 39(2-3): 165?210.S.
Wang, C. Manning.
2012.
Baselines and Bigrams:Simple, Good Sentiment and Topic Classification.In Proceedings of ACL 2012.S.
Wang, C. Manning.
2013.
Fast Dropout Training.In Proceedings of ICML 2013.B.
Yang, C. Cardie.
2014.
Context-aware Learningfor Sentence-level Sentiment Analysis with Poste-rior Regularization.
In Proceedings of ACL 2014.W.
Yih, K. Toutanova, J. Platt, C. Meek.
2011.
Learn-ing Discriminative Projections for Text SimilarityMeasures.
Proceedings of the Fifteenth Confer-ence on Computational Natural Language Learning,247?256.W.
Yih, X.
He, C. Meek.
2014.
Semantic Parsing forSingle-Relation Question Answering.
In Proceed-ings of ACL 2014.M.
Zeiler.
2012.
Adadelta: An adaptive learning ratemethod.
CoRR, abs/1212.5701.1751
