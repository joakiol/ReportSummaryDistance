Bridging the Gap: Academic and Industrial Research in Dialog Technologies Workshop Proceedings, pages 56?63,NAACL-HLT, Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsExperiences of an In-Service Wizard-of-Oz Data Collectionfor the Deployment of a Call-Routing ApplicationMats Wir?n,?
Robert Eklund,?
Fredrik Engberg?
and Johan Westermark?
?Research and Development ?Customer Integrated SolutionsTeliaSonera TeliaSoneraSE?123 86 Farsta, Sweden SE?751 42 Uppsala, Swedenfirstname.lastname@teliasonera.comAbstractThis paper describes our experiences ofcollecting a corpus of 42,000 dialoguesfor a call-routing application using aWizard-of-Oz approach.
Contrary tocommon practice in the industry, we didnot use the kind of automated applicationthat elicits some speech from thecustomers and then sends all of them tothe same destination, such as the existingtouch-tone menu, without payingattention to what they have said.
Contraryto the traditional Wizard-of-Oz paradigm,our data-collection application was fullyintegrated within an existing service,replacing the existing touch-tonenavigation system with a simulated call-routing system.
Thus, the subjects werereal customers calling about real tasks,and the wizards were service agents fromour customer care.
We provide a detailedexposition of the data collection as suchand the application used, and compare ourapproach to methods previously used.1 Background and introductionSpoken-dialogue systems for applications such ascustomer care increasingly use statistical languagemodels (SLMs) and statistically-based semanticclassification for recognition and analysis ofutterances.
A critical step in designing anddeploying such a system is the initial datacollection, which must provide a corpus that isboth representative of the intended service andsufficiently large for development, training andevaluation.For at least 20 years, Wizard-of-Oz methodologyhas been regarded as a superior (though notunproblematic) method of collecting high-quality,machine-directed speech data in the absence of arunnable application.1 Normally, these data will beuseful for several purposes such as guidingdialogue design and training speech recognizers.Still, the Wizard-of-Oz option is often dismissed infavour of simpler methods on the ground that itdoes not scale well in terms of cost and time (forexample, Di Fabbrizio et al 2005).
Consequently,Wizard-of-Oz has typically been used for datacollections that are more limited in the number ofsubjects involved or utterances collected.
Oneexception from this is the data collection for theoriginal AT&T ?How May I Help You?
system(Gorin et al 1997; Ammicht et al 1999), whichcomprised three batches of transactions with livecustomers, each involving up to 12,000 utterances.Other well-known instances are ?Voyager?
(Zueet al 1989) and the individual ATIS collections(Hirschman et al 1993) which involved up to ahundred subjects or (again) up to 12,000utterances.While it is true that Wizard-of-Oz is a labour-intensive method, the effort can often be motivatedon the ground that it enables significant design andevaluation to be carried out before implementation,thereby reducing the amount of re-designnecessary for the actual system.
However, oneshould also bear in mind the crucial advantagebrought about by the possibility in a productionenvironment of running the Wizard-of-Ozcollection in-service rather than in a closed labsetting.
As we shall discuss, the fact that realcustomers with real problems are involved insteadof role-playing subjects with artificial taskscircumvents the key methodological problem thathas been raised as an argument against Wizard-of-Oz, namely, lack of realism.1 For backgrounds on Wizard-of-Oz methodology, see Dahlb?ck et al (1993)and Fraser & Gilbert (1991).56The aim of this paper is to describe ourexperiences of running a Wizard-of-Oz collectionin a production environment with real customers,with the double purpose of guiding dialogue designand collecting a sufficient amount of data for thefirst training of a speech recognizer.
We alsoreview what other options there are for the initialdata collection and compare our Wizard-of-Ozapproach with those.The rest of this paper is organized as follows:Section 2 describes the call-routing problem andour particular domain.
Section 3 gives an overviewof the options for the initial data collection and themajor trade-offs involved in selecting a method.Section 4 describes the application that wasdeveloped for our Wizard-of-Oz data collection,whereas Section 5 describes the actual datacollection, summary statistics for the collected dataand some experimental results obtained.
Section 6contains a discussion of our overall experiences.2 The call-routing task and domainCall routing is the task of directing a caller to aservice agent or a self-serve application based ontheir description of the issue.
Increasingly, speech-enabled routing is replacing traditional touch-tonemenues whereby callers have to navigate to theappropriate destinations.The domain of interest in this paper is (theentrance to) the TeliaSonera2 residential customercare in Sweden, comprising the entire range ofservices offered: fixed and mobile telephony,broadband and modem-based Internet, IPtelephony, digital television, triple play, etc.Around 14 million calls are handled annually, andbefore the speech-enabled call-routing system waslaunched in 2006, touch-tone navigation was usedthroughout.
The speech-enabled system involvesan SLM-based speech recognizer and astatistically-based classifier.3 The task of theclassifier is to map a spoken utterance to anapplication category which corresponds to a self-serve application, (a queue to) a human agent, adisambiguation category or a discourse category.Whereas self-serve applications and service agentsare the desired goals to reach, disambiguation anddiscourse categories correspond to intermediatestates in the routing dialogue.
More specifically,2 TeliaSonera (www.teliasonera.com) is the largest telco in the Scandinavian?Baltic region.3 The speech recognizer and classifier are delivered by Nuance(www.nuance.com).disambiguation categories correspond to caseswhere the classifier has picked up someinformation about the destination, but needs toknow more in order to route the call.
Discoursecategories correspond to domain-independentutterances such as greetings (?Hi, my name is JohnDoe?
), channel checks (?Hello??)
and metaquestions (?Who am I talking to??).
Altogether,there are 124 application categories used by thecurrent classifier.3 Options for initial data collectionBasically, there are three options for making theinitial data collection for a call-routing application:to collect human?human dialogues in a call center,to use an automated data-collection application, orto use a Wizard-of-Oz approach.
We shall nowdescribe each of these.3.1 Human?human dialoguesThe simplest possible approach to the initial datacollection is to record conversations betweenservice agents and customers in a call center.
Thisis an inexpensive method since it does not requireany data-collection application to be built.
Also,there is no customer impact.
However, the dataobtained tend not to be sufficiently representative,for two reasons: First, typically only a subset of theservices of a call center is carried out by humanagents, and hence many services will not becovered.
Second, the characteristics of human?human conversations differ from those of human?machine interaction.
Still, this option hassometimes been preferred on the grounds ofsimplicity and lack of negative customer impact.3.2 Automated applicationsDue to the nature of the task, it is easy to put out afully automated mock-up system in a live servicethat engages in the initial part of a call-routingdialogue.
Typically, such a system will play anopen prompt, record the customers?
speech, playanother prompt saying that the system did notunderstand, again record the speech, and finallydirect all calls to a single destination, such as ageneral-skills service agent or the entry to theexisting touch-tone menu.
We estimate that asystem of this kind could be implemented andintegrated into a call center in about a person week.An example of this approach is the AT&T ?GhostWizard?
(referred to in Di Fabbrizio et al 2005).57This basic approach can be improved upon bydetecting silences and touch-tone events, and inthese cases playing designated prompts that try toget the caller on track.
Furthermore, if data fromprevious call-routing applications are available, itis possible to use these to handle domain-independent utterances.
Such utterancescorrespond to discourse categories as mentioned inSection 2, and the idea then is to play prompts thatencourage the caller to describe the issue.
Adescription of such an approach is provided by DiFabbrizio et al (2005).A problem with the automated approach is thatcustomer impact can be quite negative, since theapplication does not actually do anything exceptfor recording their speech (possibly throughseveral turns), and routing them to a ?dummy?destination where they will have to start over.
Ofcourse, one way of avoiding this is to include ahuman in the loop who listens to the customer?sspeech and then routes the call to the rightdestination.
Apparently, this is the approach ofDi Fabbrizio et al (2005), which consequently isnot fully automated.Apart from customer impact, the problem with anautomated system is that we do not learn the fullstory about caller behaviour.
In particular, sincetypically only a minority of callers will state theirissue in an unambiguous way within the given fewturns, less information about the callers?
actualissues will be obtained.
In particular, for callerswho completely fail to speak or who give nodetails about their issue, we will have nopossibility of finding out what they wanted andwhy they failed.
Furthermore, since the systemlacks the ability to respond intelligently to in-domain utterances, no follow-up dialogue such asdisambiguation can be collected.3.3 Wizard-of-OzAlthough Wizard-of-Oz is arguably the bestmethod for collecting machine-directed data in theabsence of a running application, it is not withoutmethodological problems.
The basic critique hasalways been aimed at the lack of realism (forexample, von Hahn 1986).
In a thorough analysis,Allwood & Haglund (1992) point out that in aWizard-of-Oz simulation, both the subjects and thewizard(s) are playing roles, occupied and assigned.The researcher acting as the wizard is occupyingthe role of a researcher interested in obtaining ?asnatural as possible?
language and speech data,while playing the role of the system.
The subject,on the other hand, is occupying the role of asubject in a scientific study, and playing the role ofa client (or similar), communicating with a systemwhile carrying out tasks that are not genuine to thesubject, but given to them by the experiment leader(who might be identical with the wizard).It turns out, however, that a traditional Wizard-of-Oz approach with made-up tasks according to ascenario is anyway not an option when collectingdata for deploying a call-routing system.
Thereason for this is that we want to learn not just howcallers express themselves, but also what kind oftasks they have, which obviously rules out pre-written scenarios.
If the existing system usestouch-tone navigation, usually not too much can beascertained about this, and trying to design a set oftasks just by looking at the existing destinationswould miss the point.By instead integrating a Wizard-of-Ozapplication in an existing, live service, we cancircumvent the key methodological problems,while addressing all the problems of the previouslydescribed approaches and even obtaining someindependent advantages:1.
Since the callers?
experience will be like that ofthe intended application, albeit with humanspeech understanding, the customer impact willbe at least as good.
In fact, it is even possible toissue a kind of guarantee against maltreatmentof customers by instructing the wizards to takeover calls that become problematic (this isfurther discussed in Section 4).2.
Since real customers are involved, no role-playing from the point of view of the subjectstakes place, and hence the data become highlyrealistic.3.
The fact that scenarios are superfluous?oreven run counter to the goal of the datacollection?means that the main source ofmethodological problems disappears, and thatthe data collection as such is considerablysimplified compared to traditional Wizard-of-Oz.4.
By letting service agents be wizards, we moveaway even further from role-playing, given thatthe interaction metaphor in speech-enabled callrouting is natural-language dialogue with a(general-skills) service agent.585.
Service agents possess the expertise necessaryfor a call-routing wizard: they know whenadditional information is required from thecaller, when a call is ready for routing, andwhere to actually route the call.
Hence, wizardguidelines and training become less complexthan in traditional Wizard-of-Oz.46.
Service agents have excellent skills in dealingwith customers.
Hence, during the datacollection they will be able to provide valuablefeedback on dialogue and prompt design thatcan be carried over to the intended application.In spite of these advantages, Wizard-of-Oz appearsto have been used only very rarely for collectingcall-routing data.
The sole such data collection thatwe are aware of was made for the original AT&T?How May I Help you?
system (Gorin et al 1997;Ammicht et al 1999).
The one disadvantage of theWizard-of-Oz approach is that it is more laboriousthan automated solutions, mainly because severalperson months of wizard work is required.
On theother hand, as we have seen, it is still less laboriousthan a traditional Wizard-of-Oz, since there are noscenarios and since wizard guidelines can be keptsimple.4 Data-collection applicationOur data-collection application consists of twoparts: The first part is the Prompt Piano Client(PPC), which is running on the service agent?s PC.This is essentially a GUI with ?keys?corresponding to prerecorded prompts by whichthe wizard interacts with the caller, therebysimulating the intended system.
The PPC interfaceis shown in PLATE 1.
The second part is thePrompt Piano Server (PPS), which is an IVR(interactive voice response) server with a Dialogictelephony board, running Envox, Nuance andDialogic software.
This handles playing of promptsas well as recording of calls.
Two kinds ofrecordings are made: call logs (that is, the callers?speech events as detected by the Nuance speechrecognizer) and complete dialogues (?open mic?
).To set up a data collection, the contact centersolution is modified so that a percentage of theincoming calls to the customer care is diverted tothe PPS.
The PPS in turn transfers each call to awizard (that is, to a PPC) using tromboning.4 Furthermore, as a side effect, it is possible to facilitate the subsequent processof manually tagging the data by keeping track of where each call is routed.Allocation of the wizards is performed by the TeliaCallGuide contact center platform using skill-based routing.
Whenever a wizard answers a call,two audio streams are established, one from thecustomer to the wizard so that she can hear thecustomer?s speech, and one from an audio sourcein the PPS to the customer.
An initial open promptis played automatically by the PPS, and the wizardis then free to start playback of prompts.
This isrealized by sending control messages from the PPCto the audio source on the PPS via TCP/IP, whilelistening to the customer throughout.Depending on the caller?s response, differentthings will happen: If the caller provides anunambiguous description of the issue, the wizardwill transfer the call to the correct queue and endthe recording by pressing the ?end / routecustomer?
button.
This signals to the PPS that thecall should be released using the Explicit CallTransfer (ECT) supplementary service, freeing thetwo channels used for the tromboned call in thePPS.If, on the other hand, the caller does not providean unambiguous description of the issue, thewizard will play a follow-up prompt aimed atgetting more information from the caller bychoosing from the buttons/prompts situated to theright (fields II and III of the GUI; see Plate 1).These parts of the GUI are fully configurable; thenumber and layout of buttons as well as the namesof sound files for the corresponding prompts aredeclared separately.
(Declarations includespecifying whether the prompt associated with aparticular button allows barge-in or not.)
Thus, it ispossible not just to vary individual prompts, butalso to simulate call-routing dialogues to variousdepths by varying the number of buttons/prompts.Apart from routing the call, a possible action ofthe wizard is to enter into the call.
This is realizedby establishing a two-way direct audio stream withthe customer, enabling the parties to talk to eachother.
As pointed out in Section 3.3, one purposeof this is to let wizards take over calls that areproblematic, thereby making sure that callers donot get maltreated during the data collection andreducing the risk that they hang up.
A similarfunctionality was available in the data-collectionapplication for AT&Ts ?How May I Help You?system (Walker et al 2000).59PLATE 1: The Prompt Piano Client interface as configured towards the end of the data collection.
The interfaceis divided into three fields with buttons.
I: The leftmost field provides caller information, like A-nr (the phonenumber the customer is calling from) and Cid (the phone number the customer provides as reason for the call).The wizard has two option buttons, Mina ?tg?rder (?my actions?
), at hand: the button Bryt in / Prata medkund (?barge-in/talk to client?)
which is used for entering into the call, and the button Avsluta / Koppla kund(?end/route customer?)
which is used to terminate the recording prior to routing the call to the appropriatedestination.
(Both of these options are associated with prompts being played.)
II: The second field, Kunden?
(?the customer??
), contains buttons corresponding to renewed open prompts for the purpose of error-handling,?
?r tyst (??
is silent?
), ?trycker p?
knappar (?uses the touch-tone keypad?
), ?ber om hj?lp (?asks forhelp?
), ?avbryter (?interrupts?
), ?pratar f?r l?nge (?talks for too long?
), ?s?ger inget om ?rendet (?doesn?tsay anything about the reason for the call?
), ?
?r sv?r att uppfatta (?is hard to understand?).
III: The third field,Jag undrar om det g?ller?
(?I would like to know if it is about??
), contains buttons corresponding todisambiguation prompts asking for additional information, e.g.
whether the customer?s reason for the call isabout fixed (?fast?)
or mobile (?mobilt?)
telephony, broadband (?bredband?)
or something else.
All buttons alsohave hot-key possibilities for agents who prefer this over point-and-click.With the exception of the initial open prompt, thewizards have full control over when and in whatorder prompts are played and actions are executed.Thus, whereas an automated system will startplaying the next prompt after an end-of-speechtimeout typically within the range of 0.75?1.5seconds, a wizard may decide to impose longerdelays if she considers that the caller has not yetyielded the turn.
On the other hand, the wizardmay also respond more rapidly.
Thus, the problemof response delays, which has sometimes haddistorting impact in Wizard-of-Oz simulations,does not appear in our application (cf.
Oviatt et al1992).The PPS application was developed in the Envoxgraphical scripting language, which makes itpossible to write event-driven applicationscontrolled from an external source such as thePPC, and also supports Nuance call logging (forrecording customer utterances) and Dialogictransaction recording (for recording entireconversations between two parties, in this case thecustomer and the PPS, or the customer and thewizard).5 Design, implementation and testing ofthe Prompt Piano (PPC and PPS) took four personweeks.The agents/wizards were involved in thedevelopment from the very start to ensure that theapplication (and in particular the GUI) was5 VXML was not used since it appeared that real-time control of an IVR froman external source would then have been more difficult to implement.Furthermore, VXML browsers generally have no support for features such astransaction recording during tromboned transfer and delayed invocation of theECT supplementary service in conjunction with call transfer.
Hence, in aVXML framework, additional components would be required to solve thesetasks.60optimized according to their needs and wishes.
ThePrompt Piano GUI was reconfigured several timesduring the course of the data collection, both forthe purpose of carrying out prompt-designexperiments and in response to (individual orgroup) requests for changes by the agents/wizards.5 Data collection5.1 OverviewThe purpose of the data collection was twofold: toobtain speech data that could be used for initialtraining of the speech recognizer, and to obtaindata that could be used to guide dialogue design ofthe intended application.
Thus, whereas the formeronly involved caller responses to open prompts, thelatter required access to complete call-routingdialogues, including error-handling anddisambiguation.Organization.
Ten wizards were used for thedata collection.
Initially, one week was used fortraining of the wizards and basic tuning of theprompts.
This process required four person weeks(not all wizards were present all the time).
After abreak of three weeks, the data collection then wenton for five weeks in a row, with the ten wizardsacquiring around 42,000 call-routing dialogues.
(This figure includes around 2,000 useabledialogues that were collected during the initialweek.)
This was more than had been anticipated,and much more than the 25,000 that had beenprojected as a minimum for training, tuning andevaluation of the speech recognizer.
Thus,although 50 person weeks were used by thewizards for the actual collection, 32 person weekswould actually have been sufficient to reach theminimum of 25,000 dialogues.
On average, 195dialogues were collected per working day perwizard (mean values ranging from 117 dialoguesper day to 317 dialogues per day; record for awizard on a single day was 477).Barge-in.
Initially, barge-in was allowed for allprompts.
However, it turned out to be useful tohave one very short prompt with barge-in disabled,just asking the caller to state the reason for the call.The main usage of this was in cases where callerswere repeatedly barging in on the system to theextent that the system could not get its messagethrough.Utterance fragments.
As a consequence of, onthe one hand, wizards having full control overwhen and whether to start playing a prompt and, onthe other hand, the speech recognizer having afixed end-of-speech timeout, it would sometimeshappen that more than one sound file would berecorded between two prompts in the Nuance calllogs.
An example of this would be: ?Eeh, I...
I?mwondering whether... can you tell me the pricing ofbroadband subscriptions?
?, where both of the twosilent pauses would trigger the end-of-speechtimeout.
Although this constitutes a mismatchbetween data collection and final system, inpractice this caused no problem: on the contrary,the sound files were simply treated as separateutterances for the purpose of training the speechrecognizer, which means that the most informativefragment, typically at the end, was not lost.
Inaddition, these data are potentially very valuablefor research on turn-taking (in effect, intelligentend-of-speech detection).Wizards entering into calls.
The event ofwizards taking over calls in order to sort outproblematic dialogues occurred on average in 5%of the calls.
The figure was initially a bit higher,presumably because the wizards were less skillfulin using the prompts available, and because theprompts were less well-developed.
As a side-effectof this, we have obtained potentially very valuabledata for error-handling, with both human?machineand human?human data for the same callers andissues (compare Walker et al, 2000).Post-experimental interviews.
We also used thefacility of letting wizards take over calls as a wayof conducting post-experimental interviews.
Thiswas achieved by having wizards route the calls tothemselves and then handle the issue, whereuponthe wizard would ask the caller if they wouldaccept being interviewed.
In this way, we wereable to assess customer satisfaction on the fly withrespect to the intended system and even gettinguser feedback on specific design features alreadyduring the data collection.5.2 ExperimentsSeveral design experiments were run during thedata collection.
Here, we shall only very brieflydescribe one of them, in which we compared twostyles of disambiguation prompts, one completelyopen and one more directed.
As can be seen inTABLE 1, utterances following the opendisambiguation prompt are on average 3.6 timeslonger than utterances following the directedprompt.61Utterances and Words Disfluency Concepts PromptUtts Words Words/UttsDisfl Disfl/UttsDisfl/WordsConceptsInConceptsOutDIFFsTotalDIFFsChangeDIFFS/UttsDIFFS/WordsDirected 118 216 1.8 19 0.16 0.09 136 244 108 0 0.9 0.5Open 121 791 6.5 72 0.6 0.09 144 248 122 18 1.01 0.15TABLE 1.
Summary statistics for the directed prompt (?I need some additional information about the reason foryour call.
Is it for example about an order, price information or support??
), and the open prompt (?Could you pleasetell me a little bit more about the reason for you call??)
prompts.
Totals and ratios are given for utterances/words,disfluencies and number of concepts acquired before the disambiguation prompt was played (?In?)
and after thecustomer had replied to the disambiguation prompt (?Out?).
Also, ratios are given for number of concepts comparedto number of utterances and words, as well as totals and ratios for the differences (DIFFs) between concepts in andconcepts out, i.e., how many concepts you ?win?
by asking the disambiguation prompt.Furthermore, in order to see to what extentthese prompts also made callers provide moreinformation, we manually tagged the transcribedutterances with semantic categories.
Followingthe evaluation methodology suggested by Boye& Wir?n (2007, Section 5), we then computedthe difference with respect to ?concepts?
forutterances immediately following and precedingthe two kinds of prompts.Although the number of concepts gained isonly slightly higher6 for the open prompt (as afunction of concepts per utterance), there aresome palpable differences between the directedand the open prompt.
One, shown in TABLE 1, isthat there are no instances where an alreadyinstantiated concept (e.g.
fixedTelephony) ischanged to something else (e.g.
broadband),while this happens 18 times following the openprompt.
The other, not shown in TABLE 1, isthat, following the directed prompt, one never?gains?
more than one new concept, while thereare 26 instances following the open promptwhere the gain is two concepts, and even twoinstances where the gain is three concepts(which also means that one concept is changed).Finally, when one analyses the syntacticcharacteristics following the two different typesof prompts, there is an obvious shift from thetelegraphic ?noun-only?
responses that amountto more than 70% of the directed promptresponses, to the responses following the openprompt, where 40% are complete sentences and21% are noun phrases.
Also, the syntax is morevaried following the open prompt.76 However, the difference is not statistically significant, either using a t test(two-sampled, two-tailed: p=0.16 with equal variances assumed; p=0.158equal variances not assumed) or Mann-Whitney U test (two-tailed:p=0.288).7 The distributions are, in descending order, for the directed prompt:Noun=85, Sentence=11, Yes/No=8, Noun Phrase=8, no response=3,Yes/No+Noun=2, Adverbial Phrase=1, Adjective Phrase=1; for theopen prompt: Sentence=49, Noun Phrase=26, Noun=24, Verb6 DiscussionWe claimed in Section 3.3 that by using an in-service Wizard-of-Oz data collection, we havebeen able to effectively overcome all problemsof the alternative methods discussed there.
Arelevant question is then if there are anyremaining, independent problems of theapproach described here.On the methodological side, there is clearly acertain amount of role playing left in the sensethat service agents are acting as the system(albeit a system whose interaction metaphor is aservice agent!).
Interestingly, we noticed earlyon that the agents sometimes failed in properlysimulating the intended system in one respect:Since they would often grasp what the callerwanted before he or she had finished speaking,they would start playing the next prompt soearly that they were barging in on the caller.Thus, in their willingness to provide quickservice, they were stepping outside of theirassigned role.
However, they soon learnt toavoid this, and it was never a problem except forthe first few days.Apart from this, the main disadvantage ofWizard-of-Oz collections clearly is the amountof work involved compared to the othermethods.
As we have seen, the Prompt Pianodesign and implementation took four personweeks, training of the wizards took another fourperson weeks, and collection of 25,000dialogues required 32 person weeks?hencealtogether 40 person weeks (although weactually used 50 person weeks, since we went oncollecting more data).
This could be comparedwith possibly a single person week required forthe fully automated approach.
The morePhrase=11, Adjective Phrase=5, Adverbial Phrase=2, no response=2,Yes/No=1, Interjection=1.62elaborate automated methods would comesomewhere in between, also depending onwhether a human agent is used for routingcallers or not.In the TeliaSonera case, the main desideratafavouring Wizard-of-Oz were highlyrepresentative data, no negative customer impactand need for early evaluation and design,particularly because this was the firstdeployment of natural-language call routing inScandinavia.
In other words, it was decided toaccept a higher initial cost in return for reducedcosts downstream, due to higher quality and lessre-design of the implemented system.It is impossible to quantify the downstreamsavings made by choosing Wizard-of-Oz sincewe have no baseline.
However, one indication ofthe quality of the data is the initial performanceof the classifier of the deployed system.
(By?initial?, we mean the period during which nodata from the live system had yet been used fortraining or updating of the system.)
In our case,the initial accuracy was 75%, using 113application categories.
We regard this as a highfigure, also considering that it was achieved inspite of several new products having beenintroduced in the meantime that were notcovered by the speech recognizer.
The initialtraining of the speech recognizer and classifierused 25,000 utterances.
As a comparison, whenan additional 33,000 utterances (mostly from thelive system) had been used for training, theaccuracy increased to 85%.AcknowledgementsMany colleagues have provided invaluable helpand support throughout this project.
Here we canonly mention some of them: Johan Boye, JoakimGustafson, Linda Bell, Fredrik Bystr?m, RobertSandberg, Erik N?slund, Erik Demmelmaier,Viktoria Ahlbom, Inger Thall and MarcoPetroni.
Last but not least we thank our skilledwizards: Christina Carlson, Marie Hagdorn,Gunilla Johannisson, Ana-Maria Loriente, MariaMellgren, Linda Norberg, Anne T?rk, MikaelWikner, Eva Wintse and Jeanette ?berg.ReferencesAllwood, Jens & Bj?rn Haglund.
1992.Communicative Activity Analysis of a Wizard ofOz Experiment.
Internal Report, PLUS ESPRITproject P5254.Ammicht, Egbert, Allen Gorin & Tirso Alonso.
1999.Knowledge Collection For Natural LanguageSpoken Dialog Systems.
Proc.
Eurospeech,Budapest, Hungary, Volume 3, pp.
1375?1378.Boye, Johan & Mats Wir?n.
2007.
Multi-slotsemantics for natural-language call routingsystems.
Proc.
Bridging the Gap: Academic andIndustrial Research in Dialog Technology.NAACL Workshop, Rochester, New York, USA.Dahlb?ck, Nils, Arne J?nsson & Lars Ahrenberg,Wizard of Oz Studies ?
Why and How.
1993.Knowledge-Based Systems, vol.
6, no.
4, pp.
258?266.
Also in: Mark Maybury & WolfgangWahlster (eds.).
1998.
Readings in IntelligentUser Interfaces, Morgan Kaufmann.Di Fabbrizio, Giuseppe, Gokhan Tur & DilekHakkani-T?r.
2005.
Automated Wizard-of-Oz forSpoken Dialogue Systems.
Proc.
Interspeech,Lisbon, Portugal, pp.
1857?1860.Fraser, Norman M. & G. Nigel Gilbert.
Simulatingspeech systems.
1991.
Computer Speech andLanguage, vol.
5, pp.
81?99.Gorin, A. L., G. Riccardi & J. H. Wright.
1997.
Howmay I help you?
Speech Communication, vol.
23,pp.
113?127.von Hahn, Walther.
1986.
Pragmatic considerationsin man?machine discourse.
Proc.
COLING, Bonn,Germany, pp.
520?526.Hirschman, L., M. Bates, D. Dahl, W. Fisher, J.Garofolo, D. Pallett, K. Hunicke-Smith, P. Price,A.
Rudnicky & E. Tzoukermann.
1993.
Multi-SiteData Collection and Evaluation in SpokenLanguage Understanding.
Proc.
ARPA HumanLanguage Technology, Princeton, New Jersey,USA, pp.
19?24 .Oviatt, Sharon, Philip Cohen, Martin Fong &Michael Frank.
1992.
A rapid semi-automaticsimulation technique for investigating interactivespeech and handwriting.
Proc.
ICSLP, Banff,Alberta, Canada, pp.
1351?1354.Walker, Marilyn, Irene Langkilde, Jerry Wright,Allen Gorin & Diane Litman.
2000.
Learning toPredict Problematic Situations in a SpokenDialogue System: Experiments with How May IHelp You?
Proc.
North American Meeting of theAssociation for Computational Linguistics(NAACL), pp.
210?217.Zue, Victor, Nancy Daly, James Glass, DavidGoodine, Hong Leung, Michael Phillips, JosephPolifroni, Stephanie Seneff & Michael Soclof.1989.
The Collection and Preliminary Analysis ofa Spontaneous Speech Database.
Proc.
DARPASpeech and Natural Language Workshop,pp.
126?134.63
