Analysis of Japanese Compound Nounsusing Collocational InformationKOBAYASI Yosiyuki TOKUNAGA Takenobu TANAKA HozunfiDepar tment  of Computer  ScienceTokyo  Ins t i tu te  of  Techno logy{yash i ,  take ,  tanaka}O,  t ?tech .
ac .
j pAbst rac tAnalyzing compound nouns is one of the cru-cial issues for natural language processing sys-tems, in particular for those systems that aim at awide coverage of domaius.
In this paper, we pro-pose a mcthod to analyze structures of Japanesecompound nouns by using both word collocationsstatistics and a thesaurus.
An experiment is con-ducted with 160,000 word collocations to analyzecomlmund nouns of with an average length of 4.9characters.
The accuracy of this method is about80%.1 In t roduct ionAnalyzing compound nouns is one of the crucialissues for natural language processing systems, inparticular for those systems that aim at a widecoverage of domains.
Registering all compoundnouns in a dictionary is an impractical attproach,since we can create a new conll)ound lloun by conl-bluing nouns.
Therefore, a mechanism to analyzethe structure of a con,pound noun front the indi-vidual nouns is necessary.In order to identify structures of a compoundnoun, we must first find a set of words that com-pose the compound noun.
This task is trivial forlanguages uch as English, where words are sepa-rated by spaces.
The situation is worse, however,in Japanese where no spaces are placed betwem,words.
The process to identify word boundariesis usually called segmentation.
In processing lan-guages uch as Japanese, ambiguities in segmenta-tion should be resolved at the same time as ann?lyzing structure.
I"or instance, thc Japanese com-pound noun "$)~\]llJ~;I~"(ncw indirect tax), pro-duces t6(= 2 4) segcmentations possibilities for thiscase.
(By consulting a /lai)anese dictionary, wewould filter out some.)
In this case, we have tworemaining possibilities: "50\[" (new) /~ (type)/lllJ~'}~(indirect)/t~ (tax)" and ")~#~ (new)/lll\]~)~ (indi-rect)/  :~, (tax).
''i Wc nmst choose the correct seg-mentation, "~)?~'J.
(new)/llll}~ ( indirect) /~ (tax)"and analyze structure.1 Here "/" denotes ~L bound~try of words,Segmentation of Jal)anese is difficult only whenusing syntactic knowledge.
Therefore, we couldnot always expect a sequence of correctly seg-mented words as an input to structure analysis.The information of structures is also expected toimprove segmentation accuracy.There are several researches that are attackingthis problem, l)'uzisaki et al applied the ItMMmodel to scg,nentatimt and probabilistic CFG toanalyzing the structure of compound nouns \[3\].The accuracy of their method is 73% in identify-ing correct structures of kanzi character sequenceswith average length is 4.2 characters.
In theirapproach, word boundaries are identified throughtmrely statistical information (the IIMM model)without regarding such linguistic knowledge, asdictionaries.
Therefore, the HMM nrodel may sug-gest an improper character sequence as a word.Purthermore, since nonterminal symbols of CFGare derived from a statistical analysis of word col-locations, their number tends to be large and sothe muuber of CFG rules are also large.
They as-sumed COml)ound nouns consist of only one char-acter words and two character words.
It is ques-tionable whether this method can be extended tohandle cases that include nmre than two characterwords without lowering accuracy.ht this palter , we protmsc a method to analyzestructures of Japanese compound nouns 1)y usingword collocational information and a thesaurus.The callocational information is acquired from acorpus of four kanzi character words.
The outlineof procedures to acquire the collocational informa-tion is as follows:?
extract collocations of nouns from a corpus offour kanzi character words?
replace ach noun in the collocations with the-saurus categories, to obtain the collocatkms ofthesaurus categories?
count occurrence frequencies for each colloca-tional pattern of thesaurus catcgoricsFor each possible structure of a compound noun,the preference is calculated based on this colloocational information and the structure with thehighest score wins.865Hindle and Rooth also used collocational infor-mation to solve ambiguities of pp-attachment inEnglish \[5\].
Ambiguities arc resolved by compar-ing the strength of associativity between a preposi-tion and a verb and the preposition and a nominalhead.
The strength of iLssociativity is calculatedon the basis of occurrence frequencies of word col-locations in a corpus.
Besides the word colloca-tions information, we also use semantic knowledge,nanlely~ a thesaurus.TILe structure of this paper is as follows: Sec-tion 2 cxplains the knowlcdge for structure analy-sis of compound nouns and the procedures to ac-quire it from a corpus, Section 3 describes the anal-ysis algorithm, and Section 4 describes the exper-iments that arc conducted to evaluate the perfor-mance of our method, and Section 5 summarizesthe paper and discusscs future rescarch directions.2 Col locationalfor AnalyzingNounsInformationCompoundThis section describes procedures to acquire col-locational information for analyzing compoundnouns from a corpus of four kanzi character words.What we nccd is occurrence frequencies of all wordcollocations.
It is not rcalistic, howcvcr, to collectall word collocations.
We use collocations fromthesaurus categories that are word abstractions.The procedures consist of the following foursteps:1. collect four kanzi character words (section 2.1)2. divide the above words in the middle to pro-duct pairs of two kanzi charactcr words; if oneis not in the thesaurus, this four kanzi char-aeter word is discarded (section 2.1)3. assign thesaurus catcgorics to both two kanzicharacter word (section 2.2)4. count occurrence frequencies of category col-locations (section 2.3)2.1 Co l lec t ing  Word  Co l locat ionsWe use a corpus of four kanzi character words asthe knowledge source of collocational information.The reasons are its follows:?
In Japanese, kanzi character sequences longerthan three are usually compound nouns, Thistendency is confirmed by comparing tile oc-currence frequencies of kanzi character wordsin texts and those headwords in dictionaries.We investigated the tendency by using sampletexts from newspaper articles and encyclope-dias, and Bunrui Goi IIyou (BGH for short),which is a standard Japanese thesaurus.
Thesanlple texts include about 220,000 sentences.We found that three character words andlonger represent 4% in the thesaurus, but 71%in the sample texts.
Therefore a collection offour kanzi character words would bc used asa corpus of comi)ound nouns.Four kanzi character sequences are useful toextract binary relations of nouns, becausedividing a h)ur kanzi character sequence inthe middle often gives correct segmentation.Our preliminary investigation shows that theaccuracy of the above heuristics is 96 %(961/1000).There is a fairly large corpus of four kanzicharacter words created by Prof. Tanaka Ya-suhito at Aiti Syukutoku college \[8\].
The cor-pus w~Ls manually created from newspaper ar-ticles and includes about 160,000 words.2.2 Ass ign ing  Thesaurus  Categor iesAfter collecting word collocations, wc must assigna thesaurus category to each word.
This is a diffi-cult task because some words are mssigncd multiplecategories.
In such cases, we have several categorycollocations from a single word collocation, someof which are incorrect.
TILe choices arc as follows;(1) use word collocations with all words is as-signed a single category.
(2) equally distribute frequency of word collca-tions to all possible category collocations \[4\](3) calculate the probability of each category col-location and distribute frequency based onthese probabilities; the probability of colloca-tions are calculated by using method (2) \[4\](4) determine the correct category collocation byusing statistical methods other than word col-locations \[2, 10, 9, 6\]Fortunately, there are few words that are its-signed multiple categories in BGH.
Therefore, weuse method (1).
Word collocations containingwords with multiple categories represent about 1/3of the corpus.
If we used other thesauruses, whichassign multiple categories to more words, we wouldneed to use method (2), (3), or (4).2 .3  Count ing  Occur rence  o f  Cate -gory  Co l locat ionsAfter assigning the thesaurus categories to words,wc count occurrence frequencies of category collo-cations as follows:1. collect word collocations, at this time we col-lect only patterns of word collocations, butwe do not care about occurrence frequenciesof thc patterns8662.
replace thesaurus categories with words toproduce category collocation patterns3.
count the number of category collocation pat-ternsNote: wc do not care about frequencies ofword col-locations prior to replacing words with thesauruscatcgorics.3 Algor i thmThe analysis consists of three steps:1. enumerate possible segmentations of an inputcolnpound nmm by consulting headwords ofthe thesaurus (BGH)2. assign thesaurus categories to all words3.
calculate the preferences of every structure ofthc compound noun according to tlm frcquen--tics of category collocationsWe assume that a structure of a compmmd nouncau be expressed by a binary tree.
We also as-stone that the category of the right branch of a(sub)tree represents tile category of tile (sub)treeitself.
Tiffs assumption exsists because Japaneseis a head-final language, a modifier is on the h'.ftof its modifiee.
With these assuml)tions, a prefer-ence vahte of a structure is calculated by recurslvefunction p as follows:1 i f t i s lea fv(t) = p( l ( t ) ) ,  v ( r ( t ) ) ,  cv(cat(l(t)),  cat(r(t)))otherwisewhere flmction l and r return the left and rightsubtree of the tree respectively, cat returns the-saurus categories of the argmnent.
If the argu-ment of cat is a tree, cat returns the category oftl,e rightmost leaf of tile tree.
Function cv returnsan assoeiativity measure of two categories, whichis calculated from tile frequency of category collo-cation described in the previous ection.
We wmtlduse two measures for cv: P(cat l ,  cat=) returns therelative frequency of collation cat1, which appearson the left side al, d cat2, wlfich appears on theright.Probalfility:cvl = l"(catl,cat.~)Modified ,nutual information statistics (MIS):P(catx, cat2)cv2 = I ' (cat l ,  *) .
1'(*, cat2)where * llleans (ton~t care.MIS is similar to nmtual infromation used byChurch to calculate semantic dependencies be-tween words \[1\].
MIS is different from mutual in-formation because MIS takes account of the posi-tion of the word (left/right).Let us consider an example '9~)iJ~III\]~:~;~".Segmentation: two possibilities,(1) ,,~zF~ (new)/Iz~t~ (indirect)/~ (tax)" and(2) ")~)i" (new)p_f~ (type)/Hi\]~ (indirect)/;\[~(tax)"remain as mentioned in section 1.Category assignment: assigning tlmsaurus cate-gories provides(1)' "W2~ \[ll8\]/llIl~: \[311\]/~g \[137\]" and(2)' "*,~ \[316\]/)J~'J \[118:141:111\]/b11~'\[ ~ \[31~1/,~\[1371.
"A three-digit number stands for a thesauruscategory.
A colon ":" separates multiple cat-egories assigned to a word.Preference calculation: For the case (1) I, the pos-sine structures are\[\[118, 3111, 1371 and \[118, \[311, 1371\].We represent a tree with a llst notation.
Fortile ease !2~.
there is an anfl)iguity with tilecategory ,,,,"' \[118:141:111\], We expand theambiguity to 15 possible structures.
Prefer-ences are calculated for 17 cases.
For example,the l)reference of structure \[\[118, 311\], 137\] iscalculated as follows:p(\[\[118,311\], 37\])= p(\[118, anD.
p(,a7).
~o(311,137)= p(118).p(311), cv(118,311), cv(311,137)= cv(118,311), cv(311,137)4 Exper iments4.1 Data  and  Analys isWe extract kanzi cl, aracter sequences from news-paper editorials and colunms and encyclopediatext, which has no overlap with the training cor-pus: 954 compound nouns consisting of four kanzicharacters, 710 compound nouns consisting of fivekanzi characters, and 786 coml)ound nouns con-sistiug of six kanzi characters are mammlly ex-tracted from the set of the above kanzi charactersequences.
These three collections of compoundnouns arc used for test data.Wc use a thesaurus BGII, which is a stan-dard machine rea(lble Jat)anese thesaurus.
BGHis structured as a tree with six hlerarclfieal lev-els.
Table 1 shows the number of categories at alllevels, in tlfis experiment, we use the categoriesat level 3, If we have more eoml)ound nolln8 \[LSknowledge, we could use a liner hierarchy level.
'\]_'able 1 The mmd)er of categorieso f 4As mentioned in Section 2, we create a set ofcollocations of thesaurus categories from a cor-pus of four kanzi character sequences and BGt\[.867We analyze the test data accorditlg to the proce-dures described in Section 3.
In segmentation, weuse a heuristic "minimizing the number of contentwords" in order to prune the search space.
Thisheuristics is commonly used in the Japanese mor-phological analysis.
The correct structures of thetest data manually created in advance.4 .2  Resu l t s  and  D iscuss ionsTable 2 shows the result of the analysis for four,five, and six kanzi character sequences.
"oc"means that the correct answer was not obtainedbecause the heuristics is segmentation filtered outfrom the correct segmentation.
The first rowshows the percentage of cases where the correctanswer is uniquely identified, no tie.
The rows,denoted "~ n", shows the percentage of correctanswers in the n-th rank.
4 ,,, shows the percent-age of correct answers ranked lower or equal to 4thplace.Table 2 Accuracy of analysis \[%\]4 kanzi 5 kanzi 6 kanzirank cvl cv2 cvl cv2 cvl cv21 96 96 63 59 48 531 97 96 71 68 54 682 99 99 91 91 89 93,-, 3 99 99 92 92 91 944,-0 0.1 0.i 2 2 4 4c~ 1 1: 6 6 5 2Regardless, more than 90% of the correct an-swers are within the second rank.
The proba-bilistic measure cvl provides better accuracy thanthe mutual information measure cv2 for five kanzicharacter compound nouns~ but the result is re-versed for six kanzi character compound nouns.The results for four kanzi character words arc al-most equal.
In order to judge which measure isbetter, we need further experiments with longerwords.We could not obtain correct segmentation for11 out of 954 cases for four kanzi character words,39 out of 710 cases for five kanzi character words,and 15 out of 787 ca~es for six kanzi characterwords.
Therefore, the accuracy of segmentationcandidates are 99%(943/954), 94.5% (671/710)and 98.1% (772/787) respectively.
Segmentationfailure is due to words missing from the dictio-nary and the heuristics we adopted.
As mentionedin Section 1, it is difficult to correct segnlenta-tion by using only syntactic knowledge.
We usedthe heuristics to reduce ambiguities in segmenta-tion, but ambiguities may remain.
In these exper-iments, there are 75 cases where ambiguities cannot be solved by the heuristics.
There are 11 suchc~ses for four kanzi character words, 35 such casesfor five kanzi character words, and 29 cases for sixkanzi character words.
For such cases, the correctsegmentation can be uniquely identifed by apply-ing the structure analysis for 7, 19, and 17 eases,and the correct structure can be uniquely iden-tified for 7, 10, and 8 cases for all collections oftest data by using CVl.
Oa the other hand, 4, 18,and 21 cases correctly segmented and 7, 11, and8 cases correctly analyzed their structures for allcollections by using cv2.For a sequence of segmented words, there areseveral possible structures.
Table 3 shows possiblestructures for four words sequence and their oc-currence in all data collections.
Since a compoundnoun of our test data consists of four, five, andsix characters, there could be cases with a eolwpound noun consisting of four, five, or six words.hi the current data collections, however, there areno such cases.In table 3, we find significant deviation over oc-currences of structures.
This deviation has strongcorrelation with the distance between modifiersand modifees.
The rightmost column (labeledY\] d) shows sums of distances between modifiersand modifiee contained in the structure.
The dis-tance is measured based on the number of wordsbetween a modifier and a modifiee.
For instance,the distance is one, if a modifier and a modificearc inlmcdiately adjacent.The correlation between the dist;ancc and theoccurrence of structures tells us that a modifiertends to modify a closer modifiee.
This tendencyhas been experimentally proven by Maruyama \[7\].The tendency is expressed by the formula that fol-lows:q(d) =0.54.
d -I"sDGwhere d is the distance between two words and q(d)is the probM)ility when two words of said distanceis d and a modification relation.We redifined cv by taking this tendency as thefonmda that follows:cv' = cv .
q( d)where cv' is redifincd cv.
'fable 2 shows the re-sult by using new cvs.
Wc obtaiued significantimprovement in 5 kauzi aud 6 kanzi collection.Table 3 Table of possible structures\[structure ' "" 5 kanziWWl~_~~ _ __  .0_ .
268\] 269l\[wl, \[-2, ,,s\]f 96\[,,,1, ~,~\], \[~, mH 13,,,1, % 36 kanzi1854o516648453811o I313161514 L868Table 4 Accuracy of analysis \[%\]" I 4 kanz~ 5 kanzl-~-~rank 'cv~ cv2 'cvl :v2 evl c~J- -  i 96 97 "73-  79 662170 I,-~ 1 97 97 73 79 622 \[ 99 99 91 92N3 1.99 99 93 93 9}2 I }_~~41o.1 0.1We assumed that the thesaurus category of atree bc represented by the category of its rightbranch subtree because Japanese is a head-finallanguage.
However, when a right subtrce is a wordsuch as suffixes, this assumption does not alwayshold true.
Since our ultimate aimis to analyze sc--mantle structures of compound nouns, then deal-ing with only the grammatical head is not enough.We should take semantic heads into consideration.In order to do so, however, we need knowledge tojudge which subtrce represents tile semantic fea-tures of the tree.
This knowledge may be extractedfrom corpora and machine readable dictionaries.A certain class of Japanese nouns (called Sahenmeisi) may behave like verbs.
Actually, we canmake verbs frmn these nouns by adding a specialverb "-suru."
These nouns have case frames justlike ordinary verbs.
With compound nouns includ-ing such nouns, wc coukl use case frames and sc-lectional restrictions to anMyzc structures.
Thisprocess would be ahnost he same as analyzing or-dinary sentences.5 Concluding RemarksWe propose a method to analyze Japanese com-pound nouns using collocational information anda thesaurus.
Wc also describe a method to a(:-quire the collocational information from a corpusof four kauzi character words.
The method to ac-quire collocational information is dependent on theJapanese character, but the method to calculatepreferences of structures si applicat)le to any lan-guagc with comlmund nouns.The experiments show that whcn the methodanMyzes compound nouns with an average length4.9, it produces an accuracy rate of about 83%.We are considering those future works that fol-low:, incorl)orate other syntactic information, suchas affixes knowledge?
use another semantic information as well asthesauruses, such as sclcctional restriction?
apply this method to disambiguate other syn-tactic structures such as dependency relationsReferences\[1\] K. W. CMrch, W. Gale P, Hanks, and D. Hin-dle.
Using statistics in lexical anMysis.
In Lex-eal Acquisitin, chapter 6.
Lawrence ErlbaumAssociates, 1991.\[2\] J. Cowie, J.
A. Guthrie, and L. Guthrie.
Lcxi-cal disambiguation using simulated amlealing.In COLING p310, 1992.\[3\] T. Fujisaki, F. Jelinek, J. Cocke, andE.
Black T. Nishino.
A probabilistic pars-ing method for sentences disambiguation.
InCurrent Issues in Parsing Thchnology, chap-tcr 10.
Kluwer Academic Publishers, 1991.\[4\] R. Grishman and J.
Sterling.
Acquisition ofsclcctional patterns.
In COLING p658, 1992.\[5\] D. Hindle and M. Rooth.
Structual ambiguityand lexocal relations.
In ACL p229, 1991.\[6\] M. E. Lesk.
Automatic sense disambiguationusing machine readable dictionaries: How totell a pine cone from an ice cream cone.
InACM SIGDOC, 1986.\[7\] H. Maruyama and S. Ogino.
A statisticalproperty of Japanese phrase-to-phrase mod-ifications.
Mathematical Linguistics, Vol.
18,No.
7, pp.
348-352, 1992.\[8\] Y. Wallaka.
Acquisition of knowledge for natu-ral language ;the four kanji character scqucn-tics (in japanese).
In National Conferenceof Information Processing Society of Japan,1992.\[9\] J. Veronis and N. M. Ide.
Word sense dis-ambiguation with very large neural networksextracted from machine readable dictionaries.In COLING p389, 1990.\[1O\] D. Yarowsky.
Word-sense disamibiguation us-ing stastistical models of roget's categoriestrained on large corpora.
In COLING p,~5~,1992.869
