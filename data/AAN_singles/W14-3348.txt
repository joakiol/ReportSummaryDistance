Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 376?380,Baltimore, Maryland USA, June 26?27, 2014.c?2014 Association for Computational LinguisticsMeteor Universal: Language Specific Translation Evaluationfor Any Target LanguageMichael Denkowski Alon LavieLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213 USA{mdenkows,alavie}@cs.cmu.eduAbstractThis paper describes Meteor Universal, re-leased for the 2014 ACL Workshop onStatistical Machine Translation.
MeteorUniversal brings language specific evalu-ation to previously unsupported target lan-guages by (1) automatically extracting lin-guistic resources (paraphrase tables andfunction word lists) from the bitext used totrain MT systems and (2) using a univer-sal parameter set learned from pooling hu-man judgments of translation quality fromseveral language directions.
Meteor Uni-versal is shown to significantly outperformbaseline BLEU on two new languages,Russian (WMT13) and Hindi (WMT14).1 IntroductionRecent WMT evaluations have seen a variety ofmetrics employ language specific resources toreplicate human translation rankings far betterthan simple baselines (Callison-Burch et al., 2011;Callison-Burch et al., 2012; Mach?a?cek and Bojar,2013; Snover et al., 2009; Denkowski and Lavie,2011; Dahlmeier et al., 2011; Chen et al., 2012;Wang and Manning, 2012, inter alia).
While thewealth of linguistic resources for the WMT lan-guages allows the development of sophisticatedmetrics, most of the world?s 7,000+ languages lackthe prerequisites for building advanced metrics.Researchers working on low resource languagesare usually limited to baseline BLEU (Papineni etal., 2002) for evaluating translation quality.Meteor Universal brings language specific eval-uation to any target language by combining lin-guistic resources automatically learned from MTsystem training data with a universal metric pa-rameter set that generalizes across languages.Given only the bitext used to build a standardphrase-based translation system, Meteor Universallearns a paraphrase table and function word list,two of the most consistently beneficial languagespecific resources employed in versions of Me-teor.
Whereas previous versions of Meteor requirehuman ranking judgments in the target languageto learn parameters, Meteor Universal uses a sin-gle parameter set learned from pooling judgmentsfrom several languages.
This universal parameterset captures general preferences shown by humanevaluators across languages.
We show this ap-proach to significantly outperform baseline BLEUfor two new languages, Russian and Hindi.
Thefollowing sections review Meteor?s scoring func-tion (?2), describe the automatic extraction of lan-guage specific resources (?3), discuss training ofthe universal parameter set (?4), report experimen-tal results (?5), describe released software (?6),and conclude (?7).2 Meteor ScoringMeteor evaluates translation hypotheses by align-ing them to reference translations and calculatingsentence-level similarity scores.
For a hypothesis-reference pair, the space of possible alignments isconstructed by exhaustively identifying all possi-ble matches between the sentences according tothe following matchers:Exact: Match words if their surface forms areidentical.Stem: Stem words using a language appropriateSnowball Stemmer (Porter, 2001) and match if thestems are identical.Synonym: Match words if they share member-ship in any synonym set according to the WordNetdatabase (Miller and Fellbaum, 2007).Paraphrase: Match phrases if they are listed as376paraphrases in a language appropriate paraphrasetable (described in ?3.2).All matches are generalized to phrase matcheswith a span in each sentence.
Any word occur-ring within the span is considered covered by thematch.
The final alignment is then resolved as thelargest subset of all matches meeting the followingcriteria in order of importance:1.
Require each word in each sentence to becovered by zero or one matches.2.
Maximize the number of covered wordsacross both sentences.3.
Minimize the number of chunks, where achunk is defined as a series of matches thatis contiguous and identically ordered in bothsentences.4.
Minimize the sum of absolute distances be-tween match start indices in the two sen-tences.
(Break ties by preferring to alignphrases that occur at similar positions in bothsentences.
)Alignment resolution is conducted as a beamsearch using a heuristic based on the above cri-teria.The Meteor score for an aligned sentence pair iscalculated as follows.
Content and function wordsare identified in the hypothesis (hc, hf) and ref-erence (rc, rf) according to a function word list(described in ?3.1).
For each of the matchers(mi), count the number of content and functionwords covered by matches of this type in the hy-pothesis (mi(hc), mi(hf)) and reference (mi(rc),mi(rf)).
Calculate weighted precision and re-call using matcher weights (wi...wn) and content-function word weight (?
):P =?iwi?
(?
?mi(hc) + (1?
?)
?mi(hf))?
?
|hc|+ (1?
?)
?
|hf|R =?iwi?
(?
?mi(rc) + (1?
?)
?mi(rf))?
?
|rc|+ (1?
?)
?
|rf|The parameterized harmonic mean of P and R(van Rijsbergen, 1979) is then calculated:Fmean=P ?R?
?
P + (1?
?)
?RTo account for gaps and differences in word order,a fragmentation penalty is calculated using the to-tal number of matched words (m, averaged overhypothesis and reference) and number of chunks(ch):Pen = ?
?
(chm)?The Meteor score is then calculated:Score = (1?
Pen) ?
FmeanThe parameters?, ?, ?, ?, andwi...wnare tunedto maximize correlation with human judgments.3 Language Specific ResourcesMeteor uses language specific resources to dra-matically improve evaluation accuracy.
Whilesome resources such as WordNet and the Snowballstemmers are limited to one or a few languages,other resources can be learned from data for anylanguage.
Meteor Universal uses the same bitextused to build statistical translation systems to learnfunction words and paraphrases.
Used in con-junction with the universal parameter set, these re-sources bring language specific evaluation to newtarget languages.3.1 Function Word ListsThe function word list is used to discriminate be-tween content and function words in the target lan-guage.
Meteor Universal counts words in the tar-get side of the training bitext and considers anyword with relative frequency above 10?3to be afunction word.
This list is used only during thescoring stage of evaluation, where the tunable ?parameter controls the relative weight of contentversus function words.
When tuned to match hu-man judgments, this parameter usually reflects agreater importance for content words.3.2 Paraphrase TablesParaphrase tables allow many-to-many matchesthat can encapsulate any local language phenom-ena, including morphology, synonymy, and trueparaphrasing.
Identifying these matches allowsfar more sophisticated evaluation than is possiblewith simple surface form matches.
In Meteor Uni-versal, paraphrases act as the catch-all for non-exact matches.
Paraphrases are automatically ex-tracted from the training bitext using the transla-tion pivot approach (Bannard and Callison-Burch,2005).
First, a standard phrase table is learnedfrom the bitext (Koehn et al., 2003).
Paraphraseextraction then proceeds as follows.
For each tar-get language phrase (e1) in the table, find each377source phrase f that e1translates.
Each alternatephrase (e26= e1) that translates f is considereda paraphrase with probability P (f |e1) ?
P (e2|f).The total probability of e2being a paraphrase ofe1is the sum over all possible pivot phrases f :P (e2|e1) =?fP (f |e1) ?
P (e2|f)To improve paraphrase precision, we applyseveral language independent pruning techniques.The following are applied to each paraphrase in-stance (e1, f , e2):?
Discard instances with very low probability(P (f |e1) ?
P (e2|f) < 0.001).?
Discard instances where e1, f , or e2containpunctuation characters.?
Discard instances where e1, f , or e2con-tain only function words (relative frequencyabove 10?3in the bitext).The following are applied to each final paraphrase(e1, e2) after summing over all instances:?
Discard paraphrases with very low probabil-ity (P (e2|e1) < 0.01).?
Discard paraphrases where e2is a sub-phraseof e1.This constitutes the full Meteor paraphrasingpipeline that has been used to build tables forfully supported languages (Denkowski and Lavie,2011).
Paraphrases for new languages have theadded advantage of being extracted from the samebitext that MT systems use for phrase extraction,resulting in ideal paraphrase coverage for evalu-ated systems.4 Universal Parameter SetTraditionally, building a version of Meteor for anew target language has required a set of human-scored machine translations, most frequently inthe form of WMT rankings.
The general lack ofavailability of these judgments has severely lim-ited the number of languages for which Meteorversions could be trained.
Meteor Universal ad-dresses this problem with the introduction of a?universal?
parameter set that captures general hu-man preferences that apply to all languages forDirection Judgmentscs-en 11,021de-en 11,934es-en 9,796fr-en 11,594en-cs 18,805en-de 14,553en-es 11,834en-fr 11,562Total 101,099Table 1: Binary ranking judgments per languagedirection used to learn parameters for Meteor Uni-versalwhich judgment data does exist.
We learn this pa-rameter set by pooling over 100,000 binary rank-ing judgments from WMT12 (Callison-Burch etal., 2012) that cover 8 language directions (de-tails in Table 1).
Data for each language is scoredusing the same resources (function word list andparaphrase table only) and scoring parameters aretuned to maximize agreement (Kendall?s ? )
overall judgments from all languages, leading to a sin-gle parameter set.
The universal parameter set en-codes the following general human preferences:?
Prefer recall over precision.?
Prefer word choice over word order.?
Prefer correct translations of content wordsover function words.?
Prefer exact matches over paraphrasematches, while still giving significant creditto paraphrases.Table 2 compares the universal parameters to thoselearned for specific languages in previous versionsof Meteor.
Notably, the universal parameter set ismore balanced, showing a normalizing effect fromgeneralizing across several language directions.5 ExperimentsWe evaluate the Universal version of Meteoragainst full language dedicated versions of Meteorand baseline BLEU on the WMT13 rankings.
Re-sults for English, Czech, German, Spanish, andFrench are biased in favor of Meteor Universalsince rankings for these target languages are in-cluded in the training data while Russian consti-tutes a true held out test.
We also report the re-sults of the WMT14 Hindi evaluation task.
Shown378Language ?
?
?
?
wexactwstemwsynwparEnglish 0.85 0.20 0.60 0.75 1.00 0.60 0.80 0.60Czech 0.95 0.20 0.60 0.80 1.00 ?
?
0.40German 0.95 1.00 0.55 0.55 1.00 0.80 ?
0.20Spanish 0.65 1.30 0.50 0.80 1.00 0.80 ?
0.60French 0.90 1.40 0.60 0.65 1.00 0.20 ?
0.40Universal 0.70 1.40 0.30 0.70 1.00 ?
?
0.60Table 2: Comparison of parameters for language specific and universal versions of Meteor.WMT13 ?
M-Full M-Universal BLEUEnglish 0.214 0.206 0.124Czech 0.092 0.085 0.044German 0.163 0.157 0.097Spanish 0.106 0.101 0.068French 0.150 0.137 0.099Russian ?
0.128 0.068WMT14 ?
M-Full M-Universal BLEUHindi ?
0.264 0.227Table 3: Sentence-level correlation with humanrankings (Kendall?s ? )
for Meteor (language spe-cific versions), Meteor Universal, and BLEUin Table 3, Meteor Universal significantly out-performs baseline BLEU in all cases while suf-fering only slight degradation compared to ver-sions of Meteor tuned for individual languages.For Russian, correlation is nearly double that ofBLEU.
This provides substantial evidence thatMeteor Universal will further generalize, bringingimproved evaluation accuracy to new target lan-guages currently limited to baseline language in-dependent metrics.For the WMT14 evaluation, we use the tradi-tional language specific versions of Meteor for alllanguage directions except Hindi.
This includesRussian, for which additional language specific re-sources (a Snowball word stemmer) help signifi-cantly.
For Hindi, we use the release version ofMeteor Universal to extract linguistic resourcesfrom the constrained training bitext provided forthe shared translation task.
These resources areused with the universal parameter set to score allsystem outputs for the English?Hindi direction.6 SoftwareMeteor Universal is included in Meteor version1.5 which is publicly released for WMT14.Meteor 1.5 can be downloaded from the officialwebpage1and a full tutorial for Meteor Universalis available online.2Building a version of Meteorfor a new language requires a training bitext(corpus.f, corpus.e) and a standard Moses formatphrase table (phrase-table.gz) (Koehn et al.,2007).
To extract linguistic resources for Meteor,run the new language script:$ python scripts/new_language.py out \corpus.f corpus.e phrase-table.gzTo use the resulting files to score translations withMeteor, use the new language option:$ java -jar meteor-*.jar test ref -new \out/meteor-filesMeteor 1.5, including Meteor Universal, is freesoftware released under the terms of the GNULesser General Public License.7 ConclusionThis paper describes Meteor Universal, a versionof the Meteor metric that brings language specificevaluation to any target language using the sameresources used to build statistical translation sys-tems.
Held out tests show Meteor Universal to sig-nificantly outperform baseline BLEU on WMT13Russian and WMT14 Hindi.
Meteor version 1.5 isfreely available open source software.AcknowledgementsThis work is supported in part by the National Sci-ence Foundation under grant IIS-0915327, by theQatar National Research Fund (a member of theQatar Foundation) under grant NPRP 09-1140-1-177, and by the NSF-sponsored XSEDE programunder grant TG-CCR110017.1http://www.cs.cmu.edu/~alavie/METEOR/2http://www.cs.cmu.edu/~mdenkows/meteor-universal.html379ReferencesColin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In Pro-ceedings of the 43rd Annual Meeting of the Associa-tion for Computational Linguistics (ACL?05), pages597?604, Ann Arbor, Michigan, June.
Associationfor Computational Linguistics.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Omar Zaidan.
2011.
Findings of the 2011workshop on statistical machine translation.
In Pro-ceedings of the Sixth Workshop on Statistical Ma-chine Translation, pages 22?64, Edinburgh, Scot-land, July.
Association for Computational Linguis-tics.Chris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 workshop on statistical ma-chine translation.
In Proceedings of the SeventhWorkshop on Statistical Machine Translation, pages10?51, Montr?eal, Canada, June.
Association forComputational Linguistics.Boxing Chen, Roland Kuhn, and George Foster.
2012.Improving amber, an mt evaluation metric.
In Pro-ceedings of the Seventh Workshop on Statistical Ma-chine Translation, pages 59?63, Montr?eal, Canada,June.
Association for Computational Linguistics.Daniel Dahlmeier, Chang Liu, and Hwee Tou Ng.2011.
Tesla at wmt 2011: Translation evaluation andtunable metric.
In Proceedings of the Sixth Work-shop on Statistical Machine Translation, pages 78?84, Edinburgh, Scotland, July.
Association for Com-putational Linguistics.Michael Denkowski and Alon Lavie.
2011.
Meteor1.3: Automatic metric for reliable optimization andevaluation of machine translation systems.
In Pro-ceedings of the Sixth Workshop on Statistical Ma-chine Translation, pages 85?91, Edinburgh, Scot-land, July.
Association for Computational Linguis-tics.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proc.of NAACL/HLT 2003.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007.
Moses:Open source toolkit for statistical machine transla-tion.
In Proceedings of the 45th Annual Meeting ofthe Association for Computational Linguistics Com-panion Volume Proceedings of the Demo and PosterSessions, pages 177?180, Prague, Czech Republic,June.
Association for Computational Linguistics.Matou?s Mach?a?cek and Ond?rej Bojar.
2013.
Results ofthe WMT13 metrics shared task.
In Proceedings ofthe Eighth Workshop on Statistical Machine Trans-lation, pages 45?51, Sofia, Bulgaria, August.
Asso-ciation for Computational Linguistics.George Miller and Christiane Fellbaum.
2007.
Word-Net.
http://wordnet.princeton.edu/.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automaticevalution of machine translation.
In Proceedings ofthe 40th Annual Meeting of the Association for Com-putational Linguistics, pages 311?318, Philadelphia,Pennsylvania, USA, July.Martin Porter.
2001.
Snowball: A language for stem-ming algorithms.
http://snowball.tartarus.org/texts/.Matthew Snover, Nitin Madnani, Bonnie Dorr, andRichard Schwartz.
2009.
Fluency, adequacy, orHTER?
Exploring different human judgments witha tunable MT metric.
In Proceedings of the FourthWorkshop on Statistical Machine Translation, pages259?268, Athens, Greece, March.
Association forComputational Linguistics.C.
J. van Rijsbergen, 1979.
Information Retrieval,chapter 7.
Butterworths, London, UK, 2nd edition.Mengqiu Wang and Christopher Manning.
2012.Spede: Probabilistic edit distance metrics for mtevaluation.
In Proceedings of the Seventh Work-shop on Statistical Machine Translation, pages 76?83, Montr?eal, Canada, June.
Association for Com-putational Linguistics.380
