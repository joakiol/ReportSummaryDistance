Coping with ExtragrarnmaticalityJalme G. Carbonell and Philip J. HayesComputer Science Department, Carnegie-Mellon UniversityPittsburgh, PA 15213.
USAAbst rac t  1Practical natural language interfaces must exhibit robustbei~aviour in the presence of extragrammaticat user input.
Thispaper classifies different types of grammatical deviations andrelated phenomena at the lexical and sentential levels,discussing recovery strategies tailored to specific phenomenain the classification.
Such strategies constitute a tool chest ofcomputationally tractable methods for coping withextragrammaticality in restricted domain natural language.Some of the strategies have been tested and proven viable inexisting parsers.1.
IntroductionAny robust natural language interface must be capable ofprocessing input utterances that deviate from its grammatical andsemantic expectations.
Many researchers have made thisobservation and have taken initial steps towards coverage ofcertain classes of extragrammatical constructions.
Since robustparsers must deal primarily with input that does meet theirexpectations, the various efforts at coping withextragrammaticality have generally been structured as extensionsto existing parsing methods.
Probably the most popular approachhas been to extend syntactically.oriented parsing techniquesemploying Augmented Transition Networks (ATNs)\[21, 24, 25, 29\].
Other researchers have attempted to deal withungrammatical input through network-based semantic grammartechniques \[19.
20j.
through extensions to pattern matchingparsing in which partial pattern matching is allowed \[16\], throughconceptual case frame instantiafion \[12, 22\], and throughapproaches involving multiple cooperating parsing strategies\[7, 9, 18\].Given the background of existing work, this paper focuses ontwo major objectives:1. to create a taxonomy of grammatical deviations covering abroad range of extragrammaticalities,2.
to outline strategies for processing many of these deviations,3.
to assess how easily these strategies can be employed inconjunction with existing parsing methods.The overall result should be a synthesis of different parse.recovery strategies organized by the grammatical phenomenathey address (or violate), an evaluation of how well the strategies?
integrate with existing approaches to parsing extragrammatical1This research was sponsored in part by the Air Force Office of ScientificResearch under Contract AFOSR-82-0219 and in part by Digital EquipmentCorporation as part of the XCALIBUR project.input, and a set of characteristics desirable in any parsing processdealing with extragrammatical input.
We hope this will aidresearchers designing robust natural language interfaces in twoways:t .by providing a tool chest of computationally effectiveapproaches to cope with extragrammaticality;2. by assisting in the selection of a basic parsing methodologyin which to embed these recovery techniques.In assessing the degree of compatibility between recoverytechniques and various approaches to parsing, we will avoid theissue of whether a given recovery technique can be used with aspecific approach to parsing.
The answer to such a question isalmost always affirmative.
Instead, we will be concerned with hownaturally the recovery strategies fit with the various parsingapproaches.
In particular, we will consider the computationaltractability of the recovery strategies and how easily they canobtain the information they need to operate in the context ofdifferent parsing approaches.Extragrammaticalities include patently ungrammaticalconstructions, which may nevertheless be semanticallycomprehensible, as well as lexical difficulties (e.g.
misspellings),violations of semantic constraints, utterances that may begrammatically acceptable but are beyond the syntactic coverageof the system, ellipsed fragments and other dialogue phenomena,and any other difficulties that may arise in parsing individualutterances?
An extragrammaticality is thus defined with respect tothe capabilities of a particular system, rather than with respect toan absolute external competence model of the ideal speaker.Extragrammaticality may arise at various levels: lexical, sentential,and dialogue.
This paper addresses the first two categories; thethird is discussed in \[8, 11\].
Our discussions are based on directexperience with various working parsers: FLEXP, CASPAR andDYPAR \[7, 8, 16\].2.
Lexical Level ExtragrammaticalitiesOne of the most frequent parsing problems is finding anunrecognizable word in the input stream.
The following sectionsdiscuss the underlying reasons for the presence ofunrecognizable words and describe suitable recovery strategies.2.1.
The unknown word problemThe word is a legitimate lexeme but is not in the system'sdictionary.
There are three reasons for this:?
The word is outside the intended coverage of the interface(e.g.
There is no reason why a natural language interface toan electronic mail system should know words like "chair" or"sky", which cannot be defined in terms of concepts in itssemantic domain).437o The word refers to a legitimate domain concept orcombination of domain concepts, but was not included in thedictionary.
(e.g.
A word like "forward" \[a message\] can bedefined as a command verb, its action can be clearlyspecified, and the objects upon which it operates -  an oldmessage and a new recipient - -  are already well-formeddomain concepts.)?
The word is a proper name or a unique identifier, such as acatalogue part name/number, not heretofore encountered bythe system, but recognizable by a combination of contextualexpectations and morphological or orthographic features(e.g., capitalization).In the first situation, there is no meaningful recovery strategyother than focused interaction\[15\] to inform the user of theprecise difficulty.
In the third, little action is required beyondrecognizing the proper name and recording it appropriately forfuture reference.
The second situation is more complicated; threebasic recovery strategies are possible:1.
Follow the KLAUS\[14\] approach where the systemtemporarily wrests initiative from the user and plays a welldesigned "twenty questions" game, classifying the unknownterm syntactically, and relating it semantically to existingconcepts encoded in an inheritance hierarchy.
This methodhas proven successful for verbs, nouns and adjectives, butonly when they turn out to be instances of predefined generalclasses of objects and actions in the domain model.2.
Apply the project and integrate method \[6\] to infer themeaning and syntactic category o.f the word from context.This method has proven useful for nouns and adjectiveswhose meaning can be viewed as a recombination of featurespresent elsewhere in the input.
Unlike the KLAUS method, itoperates in the background, placing no major run-timeburden on the user.
However, it remains highly experimentaland may not prove practical without user confirmation.3.
Interact with the user in a focused manner to provide aparaphrase of the segment of input containing the unknownword.
If this paraphrase results in the desired action, it isstored and becomes the meaning of the new word in theimmediate context in which it appeared.
The LIFER system\[20\] had a rudimentary capacity for defining synonymousphrases.
A more general method would distinguish betweentrue synonymy and functional equivalence in order to classifythe new word or phrase in different semantic contexts.2.2.
MisspellingsMisspellings arise when an otherwise recognizable lexeme hasletters omitted, substituted, transposed, or spuriously inserted.Misspellings are the most common form of extragrammaticalityencountered by natural language interfaces.
Usually, a word ismisspell into an unrecognizable character string.
But,occasionally a word is misspelt into another word in the dictionarythat violates semantic or syntactic expectations.
For instance:Copy the flies from ~he accounts direc!ory to my airectoryAlthough "flies" may be a legitimate word in the domain of aparticular interface (e.g., the files coulcJ consist of statistics onmed-flv infestation in California).
it is obvious to the human readerthat there is a misspelling in the sentence above.There are well-known algorithms for matching a misspelt wordagainst a set of possible corrections \[13\].
and the simplestrecovery strategy is to match unknown words against the set of allwords in an interface's dictionary.
However, this obviouslyproduces incorrect results when a word is misspell into a wordalready in the dictionary, and can produce unnecessaryambiguities in other cases.Superior results are obtained by making the spelling correctionsensitive to the parser's syntactic and semantic expectations.
Inthe following example:Add two fixed haed dual prot disks to the order"haed" can be corrected to: "had", "head", "hand:', "heed", and"hated".
Syntactic expectations rule two of these out, anddomain semantics rule out two others, leaving "fixed \[lead disk"as the appropriate correction.
Com\[;utationally, there are twoways to organize this.
One can either match parser expectationsagainst all possible corrections in the parser:s current vocabulary,and rule out spurious corrections, or one can use the parseexpectations to generate a set of possible words that can berecognized at the present point and use this as input to thespelling correction algorithm.
The latter, when it can be done, isclearly the preferable choice on efficiency criteria.
Generating allpossible corrections with a 10,080 word dictionary, only to rule outall but one or two, is a computationally-intensive process, whereasexploiting fully-indexed parser expectations is far moreconstrained and less likely to generate ambiguity.
For theexample abcve, "pror' has 16 possible corrections in a small on-line dictionary.
However, domain semantics allow only one wordin the same position as "pror', so correction is most effective ifthe list of possible words is generated first.2.3.
Interaction of morphology and misspell ingTroublesome side.effects of spelling correction can arise withparsers that have an initial morphological analysis phase toreduce words to their root form.
For instance, a parser might juststore the root form of 'directory' and reduce 'directories' to'directory' plus a plural marker as part of its initial morphologicalphase.
This process is triggered by failing to recognize theinflected form as a wind that is present in the dictionary.
Itoperates by applying standard morphological rules(e.g.
- tes  => +,y) to derive a root from the inflected form.
It asimple matter to check first for inflected forms and then formisspellings.
However, if a word is both inflected and misspelt,the expectation-based spelling correcter must be invoked fromwithin the morphological decomposition routines on potentiallymisspelt roots or inflexions.2.4.
Incorrect segmentationInput typed to a natural language interface is segmented intowords by spaces and punctuation marks.
Both kinds ofsegmenting markers, especially the second, can be omitted orinserted speciously.
Incorrect segmentation at the lexical levelresults in two or more words being run together, as in"runtogether", or a single word being split up into two or moresegments, as in "tog ether" or (inconveniently) "to get her", orcombinations of these effects as in "runto geth er".
In all thesecases, it is possible to deal with such errors by extending thespelling correction mechanism to be able to recognize targetwords as initial segments of unknown words, and vice.versa.Compound errors, however, present some difficulties.
Forinstance consider the following example where we have both amissing and a spurious delimiter:Add two du alport disks to the orderAfter failing in the standard recovery methods, one letter at a timewould be stripped off the beginning of the second unrecognizableword ("alporr') and added at the end of the first unrecognizableword ("du").
This process succeeds only if at some step bothwords are recognizable and enable the parse to continue.Migrating the delimiter (the space) backwards as well as forwardsshould also be attempted between a pair of unknown words,438stopping if both words become recognizable.
Of course,additional compounding of multi.hie iexical deviations (e.g.,misspellings, run-on words and split words in the same segment)requires combinatorially inefficient recovery strategies.
Strongparser expectabons can reduce the impact of this problem, but atsome point tradeoffs must be made between resilience andefficiency in compound error recovery.3.
Sentential Level ExtragrammaticalitiesWe examine ungrammaticalities at the sentential level in fivebasic categories: missing words, spurious words or phrases, outof order constituents, agreement violations, and semanticconstraint violations.3.1.
Missing constituentsIt is not uncommon for the use; of a natural language interfaceto omit words from his input.
The degree of recovery possiblefrom such ungrammaticalities i , of course, dependent on whichwords were left out.
In practice, words whose contribution to thesentence is redundant are often omitted in an attempt o be crypticor "computer-like" (as in "Copy new files my directory").
Thissuggests that techniques that fill in the structural gaps onsemantic grounds are more likely tobe successful than strategieswhich do not facilitate the application of oor,~ain semantics.A parsing process postulates a missing word error when itseYpectations (syntactic or semantic) of what should go at a certainplace in the input utterance are violated.
To discover that theproblem is in fact a missing word, and to find the parse structurecorresponding to the user's intention, the parsing process must"step back" and examine the context of the parse as a whole.
Itneeds to ignore temporarily the unfulfilled expectations and theircontribution to the overall structure while it tries to fulfil some of itsother expectations through parsing other parts of the input andintegrating them with already parsed constituents.
Morespecifically, the parser needs to delimit the gap in the inpututterance, correlate it with a gap in the parse structure (filling inthat ga~ if it is uniquely determined), and realign the parsingmechanism as though the gap did not exist.
Such a realignmentcan be done top-down by predicting the other constituents fromthe parse structure already obtained and attempting to find themin the input stream.
Alternatively, realignment can be donebottom-up by recognizing as yet unparsed elements of the input,and either fitting them into an existing parse structure, or finding alarger structure to subsume both them and the existing structure.This latter approach is essential when the structuring words aremissing or garbled.3.2.
Spur ious  and unrecogn izab le  const i tuentsWords in an input utterance that are spurious to a parse canarise from a variety of sources:?
leg i t imate phrases  that the parser cannot deal with: Itis not uncommon for the user of a restricted domain interfaceto say things that the interface cannot understand because ofeither conceptual or grammatical imitations.
Sometimes,spurious verbosity or politeness is involved:Add if you would be so kind two fixed head and if possibledual ported disks to my order.Or the user may offer irrelevant (to the system) explanationsor justifications, as observed in preparatory experiments forthe GUS system \[4\], e.g./ think / need more storage capacity, so add two fixed headdual ported disks to my order.Some common phrases of politeness can be recognizedexplicitly, but in most cases, the only reasonable response isto ignore the unknown phrases, realign the parse on therecognizable input, and if a semantically and syntacticallycomplete structure results, postulate that the ignoredsegment was indeed redundant.
Isolating certifiable noisephrases in the same way as truly spurious input provides theadvantage that they can then be recognized at any point inthe input without having to clutter the parser's normalprocessing with expectations about where they might occur.?
broken-off and restarted utterances: These occur whenpeople start to say one thing, change their mind, and sayanother:Add I mean remove a disk from my orderUtterances in this form are more likely to occur in spokeninput but a similar effect can arise in typed input when a userforgets to hit the erase line or erase character key:Add remove a disk from my orderAdd a single ported dual ported disk from my orderAgain the best tactic is to discard the broken-off fragment,but identifying and delineating the superseded fragmentrequares strategies uch as the one discussed below.?
unknown words  fill ing a known grammat ica l  role:Sometimes the user will generate an incomprehensiblephrase synonymous with a constituent he system is perfectlycapable of understanding:Add a dual ported rotating mass storage device to my orderHere the system might not know that "rotating mass storagedevice" is synonymous with "disk".
This phenomenon willresult in missing words as well as spurious words.
If thesystem has a unique expectation for what should go in thegap, it should (with appropriate confirmation from the user)record the unknown words as synonymous with what itexpected.
If the system has a limited set of expectations forwhat might go in the gap, it could ask the user which one (ifany) he meant and again record the synonym for futurereference.
In cases where there are no strong expectations,tile system would ask for a paraphrase of theincomprehensible fragment.
If this proved comprehensible, itwould then postulate the synonymy relation, ask the user forconfirmation, and again store the results for future reference.As for missing constituents, recovery from spurious interjectionsgenerally requires "stepping back" and examining the context ofthe parse as a whole.
In this case however, violations of theparser's expectations should result in skipping over thetroublesome segments, and attempting to fulfill the expectationsby parsing subsequent segments of tile input.
If this results in acomplete parse, the skipped segment may well be spurious.
Onthe other hand, if a gap in the parse strdcture remains, it can becorrelated with the skipped segments to postulate possibleconstituents an?
synonomy relations as illustrated above.In the case of broken-off utterances, there are some morespecific methods that allow the spurious part of the input to bedetected:?
If a sequence of two constituents of identical syntactic andsemantic type is found where only one is permissible, simplyignore the first constituent.
Two main command verbs insequence (e.g., in the "Add remove ..." example above),instantiate the identical sentential case I~eader ole in a caseframe parser, enabling the former to be ignored.
Similarly,two ,lstantiations of the same prencminal case for the "disk"case frame would be recognized as mutually incompatibleand the former again ignored.
Other parsing strategies can439be extended to recognize equivalent constituent repetition,but case frame instantiation seems uniquely well suited to it.?
Recognize explicit corrective phrases and if the constituentto the right is of equivalent syntactic and semantic type as theconstituent at the left, substitute the right constituent for theleft constituent and continue the parse.
This strategyrecovers from utterances uch as "Add I mean remove ...", if"1 mean" is recognized as a corrective phrase.?
Select the minimal constituent for all substitutions.
Forinstance the most natural reading of:Add a nigh speed tape drive, that's disk drive, to the orderis to substitute "disk drive" for "tape drive", and not for thelarger phrase "high speed tape drive", which also forms alegitimate constituent of like semantic and syntactic type.3.3.
Out of order constituents and fragmentary inputSometimes, a user will employ non-standard word order.
Thereare a variety of reasons why users violate expected constituentordering relations, including unwillingness to change what hasalready been typed, especially when extensive retyping would berequired:Two fixed head dual ported disk drives add to the orderor a belief that a computer will understand a clipped pseudo-milita,~/style more easily than standard usage:two disk drives fixed head du~/ ported to my order addSimilar myth~ about what computers understand best can lead toa very fragmented and cryptic style in which all function words areeliminated:Add disk drive orderinstead of "add a disk drive to my order".These two phenomena, out of order constituents andfragmentary input, are grouped together because they are similarfrom the parsing point of view.
The parser's problem in each caseis to put together a group of recognizable sentence fragmentswithout the normal syntactic glue of function words or positioncues to indicate how the fragments should be combined.
Sincethis syntactic information is not present, semantic considerationshave to shoulder the burden alone.
Hence, parsers which make iteasy for semantic information to be brought to bear are at aconsiderable advantage.Both bottom-up and top.down recovery strategies are possiblefor detecting and recovering from missing and spuriousconstituents.
In the bottom-up approach, all the fragments arerecognized independently, and purely semantic constraints areused to assemble them into a single framework meaningful interms of the domain of discourse.
When the domain is restrictedenough, the semantic constraints can be such that they alwaysproduce a unique result.
This characteristic was exploited togood effect in the PLANES system \[23\] in which an input utterancew~s recognized as a sequence of fragments which were thenassembled into a meaningful whole on the basis of semanticconsiderations alone.
A top-clown approach to fragmentrecognition requires that the top-level or organizing concept in theutterance ("add" in the above examples) be located, if it can be,the predictions obtainable from it about what else might appear inthe utterance can be used to guide and constrain the recognitionof the other fragments.As a final point, note that in the case of out of order constituents,a parser relying on a strict left-to-right scan will have much greaterdifficulty than one with more directional freedom.
In out of orderinput, there may be no meaningful set of left-to-right expectations,even allowing for gaps or extra constituents, that will fit the input.For instance, a case frame parser that scans for the head of a caseframe, and subsequently attempts to instantiate the individualcases from surrounding input, is far more amenable to this type ofrecovery than one whose expectations are expressed as wordorder constraints.3.4.
Syntactic and semantic constraint violationsInput to a natural language system can violate both syntacticand semantic constraints.
The most.common form of syntacticconstraint violation is agreement failure between subject and verbor determiner and head noun:Do the order include a disk drives?Semantic constraint violations can occur because the user hasconceptual problems:Add a floating head tape drive to the orderor because he is imprecise in his language, using a related objectin place of the object he really means.
For instance, if he is tryingto decide on the amount of memory to include in an order hemight say:Can you connect a video disk drive to the two megabytes?When what he-really means is "... to the computer with twomegabytes of memory?.
".These different kinds of constraint violation require quitedifferent kinds of treatment.
In general, the syntactic agreementviolations can be ignored; cases in which agreement or lack of itdistinguishes between two otherwise valid readings of an input arerare.
However, one problem that sometimes arises is knowingwhether a noun phrase is singular or plural when the determineror quantifier disagrees with the head noun.Semantic constraint violations due to a user's conceptualproblems are harder to deal with.
Once detected, the onlysolution is to inform the user of his misconcepLion and let him takeit from there.
The actual detection of the problem, however, cancause some difficulty for a parser re!ymg heavily on semanticconstraints to guide its parse.
The constraint violation miOhtcause it to assume there was some oth~r problem such as out oforder or spurious constituents, and look for (and perhaps evenfind) some alternative and unintended way of putting all the piecestogether.
This is one case where syntactic considerations houldcome to the fore.Semantic constraint violations based on the mention of a relatedobject instead of the entity actually intended by the user willmanifest themselves in the same way as the semantic constraintviolations based on misconceptions, but their processing needs tobe quite different.
The violation can be resolved if the system canlook at objects related to the one the user mentioned and find onethat satisfies the constraints.
In the example above, this meansgoing from the memory size to the machine that has that amountof memory.
Clearly, the semantic distance and the type ofrelationship over which this kind of substitution is allowed needsto be controlled fairly carefully - -  m a restricted domain everythingis eventually related to everything e!se.
Preference rules areneeded to control the kind of substitutions that are allowed.
In theabove example, it might be that a part ~s allowed to substitute for awhole (metonymy), especially if, as we assumed, the part had beenused earlier in the dialogue to distinguish between differentinstances of the whole.4404.
Support for recovery strategies byvarious parsing approachesWe now turn to the question of incorporating recovery strategiesinto some of the approaches to parsing found in the literature.
Weconsider three basic classes: transition network approaches(including syntactic ATNs and network-based semanticgrammars), pattern matching approaches, and approaches basedon case frame instantiation.
These classes cover the majority ofcurrent catsing systems for restricted domain languages.All three approaches are able to cope with lexical level problemssatisfactorily.
However, as we have seen, the application ofsemantic constraints often makes the correction of lexicalproblems more efficient and less prone to ambiguity.
So parsersthat employ semantic constraints (e.g.
semantic grammars \[20, 5\]or case frame instantiation \[12, 17\]) are more effective in recoveryat the lexical level than parsers whose only expectations aresyntactic (e.g., purely syntactic ATNs \[28\]).
At the sentential evel,however, differences in the abilities of the three approaches tocope naturally with extragrammaticality are far more pronounced.We will examine each approach in turn from this point of view.4.1.
Recovery strategies and transition network parsersAlthou~jh attempts have been made to incorporate sententiallevel recovery strategies into network-based parsers includingbeth syntactically-based ATNs \[21,24, 25, 29\] and semanticgrammar networks \[20\], the network paradigm itself is not wellsuited to the kinds of recovery strategaes discussed in thepreceding sections.
These strategies generally require aninterpretive abdity to "step back" and take a broad view of thesituation when a parser's expectations are violated, and this isvery hard to do when using networks.
The underlying problem isthat a significant amount of state information during the parse isimplicitly encoded by the position in the network; in the case ofAThls, other aspects of the state are contained in the settings ofscattered registers.
As demonstrated by the recta-rule approachto diagnosing parse failures described by Weischedel andSondheimer \[24\].
these and other difficulties elaborated below donot make recovery from extragrammaticality mpossible.
However,they do make it difficult and often impractical, since much of theimplicitly encoded state must be made declarative and explicit tothe recovery strategies.Often an ATN parse will continue beyond the point where thegrammatical deviation, say an omitted word, occurred and reach anode in the network fiom which it can make no further progreSS(i.e., no arcs can be traversed).
At this point, the parser cannotascertain the source of th.~. '
error by examining its internal stateeven if the state is accessible - -  the parser may have popped fromembedded subnets, or followed a totally spurious sequence ofarcs before blocking.
If these problems can be overcome and thesource of the error determined precisely, a major problem stillremains: in order to recover, and parse input that does not accordwith the grammar, while remaining true to the network formalism,the parser must modify the network dynamicall) and temporarily,and use the modified network to proceed through the presentdifficulties.
Needless to say, this is at best a very complex process,one whose computational tractability is open to question in themost general case (though see \[21\]).
It is perhaps not surprisingthat in one of the most effective recovery mechanisms developedfor network-based parsing, the LIFER system's ellipsis handlingroutine \[20\], the key step operates completely outside the networkformalism.As we have seen, semantic constraints are very important inrecovering from many types of ungrammatical input, and these areby definition unavailable in a purely syntactic ATN parser.However, semantic information can be brought to bear on networkbased parsing, either through the semantic grammar approach inwhich joint semantic and syntactic categories are used directly inthe ATN, or by allowing the tests on ATN arcs to depend onsemantic criteria \[2, 3\].
In the former technique, the appropriatesemantic information for recovery can be applied only if thecorrect network node can be located - -  a sometimes difficult taskas we have seen.
In the latter technique, sometimes known ascascaded ATNs \[27\], the syntactic and semantic parts of thegrammar are kept separate, thus giving the potential for a higherd~gree of interpretivem:ss in using the semantic information.However, semantic information represented in this fashion isgenerally only used to confirm or disconfirm parses arrived at onsyntactic grounds and does not participate directly in the parsingprocess.A further disadvantage of the network approach forimplementing flexible recovery strategies is that networks naturallyoperate in a top-down left-to-right mode.
As we have seen, abottom.up capability is essential for many recovery strategies, anddirectional flexibility often enables easier and more efficientoperation of the strategies.
Of course, the top.down left-to-rightmode of operation is a characteristic of the network interpreter,not of the network formalism itself, and an attempt \[29\] has beenmade to operate an ATN in an "island" mode, i.e.
bottom-up,center-out.
This experiment was done in the context of a speechparser where the low-level recognition of many of the input wordswas uncertain, though the input as a whole was assumed to begrammatical.
In that situation, there were clear advantages tostarting with islands of relative lexicar certainty, and working outfrom them.
Problems, however, arise during leftward expansionfrom an island when it is necessary to run the network backwards.The admissibility of ATN transitions can depend on tests whichaccess the values of registers which would have been set earlierwhen traversing the network forwards, but which cannot havebeen set when traversing backwards.
This leads at best to anincrease in non-determinism, and at worse to blocking thetraversal completely.4.2.
Recovery strategies and pattern matching parsersA pattern matching approach to parsing provides a betterframework to recover from some sentential evel deviations than anetwork-based approach.
In parttcular, the definition of whatconstitutes a pattern match can be relaxed to allow for missing orspurious constituents.
For mis.~ing constituents, patterns whichmatch some, but not all, of their components can be countedtemporarily as complete matches, and spurious constituents canbe ignored so long as they are embedded in a pattern whose othercomponents do match.
In these cases, the patterns taken as awhole provide a basis on which to perforrn the kind of "steppingback" discussed above as being vdal for flexible recovery.
Inaddition, when pattern elements are defined semantically insteadof lexically, as with Wilks' machine translation system\[26\],semantic constraints can easily be brought to bear on therecognition.
However, dealing with out of order constituents is notso easy for a pattern-based approach since constituent order isbuilt into a pattern in a rigid way, similarly to a network.
It ispossible to accept any permutation of elements of a pattern as amatch, but this provides so much flex;bility that many spuriousrecognitions are likely to be obtained as well as the correct ones(see \[16\]).441An underlying problem here is that there is no natural way tomake the distinctions about the relative importance or differencein role between one word and another.
For instance, parsingmany of our examples might have involved use of a pattern like:(~.determiner> ~disk-drive-attribute,~" ~disk-drive,~)which specifies a determiner, followed by zero or more attributesof a disk drive, followed by a phrase synonymous with "diskdrive".
So this pattern would recognize phrases like "a dualported disk" or "the disk drive".
Using the method of dealing withmissing constituents mentioned above, "the" would constitute justas good a partial match for this pattern as "disk drive", a clearlyundesirable result.
The problem is that there is no way to tell theflexible matcher which components of the pattern arediscriminating from the point of view of recognition and which arenot.
Another manifestation of the same problem is that differentwords and constituents may be easier or harder to recognize(e.g.
prepositions are easier to recognize than the noun phrasesthey introduce), and thus may be more or less worthwhile to lookfor in an attempt o recover from a grammatical deviation.The underlying problem is the uniformity of the grammarrepresentation and the method of applying it to the input.
Anyuniformly represented grammar, whether based on patterns ornetworks, will have trouble representing and using the kinds ofdistinctions just outlined, and thus is poorly equipped to deal withmany grammatical deviations in an efficient and discriminatingmanner.
See \[18\] for a fuller discussion of this point.4.3.
Recovery strategies and case frame parsersRecursive case frame instantiation appears to provide a betterframework for recovery from missing words than approachesbased on either network traversal or pattern matchil~g.
There areseveral reasons:?
Case frame instantiation is inherently a highly interpretiveprocess.
Case frames provide a high-level set of syntacticand semantic expectations that can be applied to the input ina variety of ways.
They also provide an overall frameworkthat can be used to realize the notion of "stepping back" toobtain a broad view of a parser's expectations.o Case frame instantiation is a good vehicle for bringingsemantic and pragmatic information to bear in order to helpdetermine the appropriate parse in the absence of expectedsyntactic constituents.
If a preposition is omitted (ascommonly happens when dealing with cryptic input fromhunt-and-peck typists), the resulting sentence is syntacticallyanomalous.
However, semantic case constraints can besufficiently strong to attach each noun phrase to the correctstructure.
Suppose, for instance, the following sentence istyped to an elec',ronic mail system interface:Send message John SmithThe missing determiner presents few problems, but themissing preposition can be more serious.
Do we mean tosend a message "to John Smith", "about John Smith", "withJohn Smith", "for John Smith", "from John Smith", "in JohnSmith", "of John Smith", etc.?
The domain semantics of thecase frame rule out the latter three possibilities and otherslike them as nonsensical.
However, pragmatic knowledge isrequired to select "to John Smith" as the preferred reading(possibly subject to user confirmation) - -  the destinationcase of the verb is required for the command to be effective,whereas the other cases, if present, are optional.
Thisknowledge of the underlying action must be brought to bearat parse time to disambiguate the cryptic command.
In theXCALIBUR system case frame encoding \[10\], pragmaticknowledge of this kind is represented as oreferenceconstraints (cf.
\[26\]) on case fi!lers.
This allows XCALIBUR toovercome problems created by the absence of expected casemarkers through the application of the appropriate domainknowledge.?
The propagation of semantic knowledge through a caseframe (via attached procedures such as those of KRL \[1\] orSRL \[30\]), can fiil in parser defaults and allow the internalcompletion of phrases such as "dual disks" to mean "dualported disks".
This process is also responsible for noticingwhen information is either missing or ambiguouslydetermined, thereby initiating a focused clarificationaldialogue \[15\].?
The representation of case frames is inherently non-uniform.Case fillers, case markers, and case headers are allrepresented separately, and thi$ distinction can be used bythe parser interpretively mstantiating the case frame.
Forinstance, if a case frame accounts for the non-spurious partof an input containing spurious constituents, a recoverystrategy can skip over the unrecognizable words by scanningfor case markers as opposed to case fillers which typicallyare much harder to find and parse.
This ability to exploitnon-uniformity goes a long way to overcoming the problemswith uniform parsing methods outlined in the previous sectionon pattern matching.5.
Dialogue Level ExtragrammaticalityThe underlying causes of many extragrammaticalities detectedat the sentential level are rooted in dialogue phenomena.
Forinstance, ellipses and other fragmentary inputs are patentlyungrammatical at the sentential level, but can be understood inthe context of a dialogue.
Viewed at this more global level, ellipsisis not ungrammatical.
Nevertheless, the same computationalmechanisms required to recover from lexioal and (especially)sentential problems are neces.~ary to detect ellipsis and parse thefragments correctly for incorporation into a larger structure.
Ingeneral, many dialogue phenomena can be classifiedpragmatically as extragrammaticalities.In addition to addressing dialogue level extragrammaticalities,any robust parsing system must engage the user in dialogue forcooperative resolution of parsing problems too difficult forautomatic recovery.
Interaction with the user is also necessary fora cooperative parser to confirm any assumptions it makes ininterpreting extragrammatical input and to resolve any ambiguitiesit cannot overcome on its own.
We have referred several times inour discussions to the principle of tocused interaction, and statedthat practical recovery dialogues should be focused as tightly aspossible on the specific problem at hand.Because of space limitations, this paper does not discuss detailsthe automated resolution of dialogue level extragrarnmaticalitiesor the use of dialogue to engage the user in cooperativeresolution.
The interested reader is referred to \[8\].6.
Concluding RemarksAny practical natural language interface must be capable ofdealing with a wide range of extragrammatical input.
This paperhas proposed a partial taxonomy of extragrammatica!
!ties thatarise in spontaneously generated input to a restricted-domainnatural language interface and has presented recovery strategiesfor handhng many of the categories.
We also discussed how wellthree widely employed approaches to parsing - -  network-basedparsing, pattern matching, and case frame instantation - -  couldsupport the recovery strategies, and concluded that case frameinstantiation provided the best basis The reader is referred to \[8\]442for a more complete presentation, including a more completetaxonomy and additional recovery strategies, particularly at thedialogue level.Based on the set of recovery strategies we have examined andthe problems that arise in trying to integrate them with techniquesfor parsing grammatical input, we offer the following set ofdesiderata for a parsing process that has to deal withextragrammatical input:= The parsing process should be as interpretive as possible.We have seen several times the need for a parsing process to"stand back" and look at the broad picture of the set ofexpectations (or grammar) it is applying to the input when anungrammaticality arises.
The more interpretive a parser is,tbe better able it is to do this.
A highly interpretive parser isalso better able to apply its expectations to the input in morethan one way, which may be crucial if the standard way doesnot work in the face of an ungrammaticality.?
The parsing process should make it easy to apply semanticinformation.
As we have seen, semantic information is oftenvery important in resolving ungrammaticalities.= The parsing process should be able to take advantage ofnon-uniformity in language like that identified in Section 4.2.As we have seen, recovery can be much more efficient andreliable if a parser is able to make use of variations in ease ofrecognition or discriminating power between differentconstituents.
Th~s kind of "opportunism" can be built intorecovery strategies.= The parsing process should be capable of operating top.down as well as bottom-up.
We have seen examples whereboth of these modes are essential.We believe that case frame mstantiation provides a better basisfor parsing extragrammatical input than network-based parsing orpat!ern matching precisely because it satisfies these desideratabetter than the other two approaches.
We also believe that it ispossible do even better than case frame instantiation by using amulti-strategy approach in which case frame instantiation is justone member (albeit a very important one) of a whole array ofparsiag and recovery strategies.
We argue this claim in detail in\[8,\] and support it by discussion of three experimental parsers thatin varying degrees adopt the multi-strategy approach.7.
References1 Bobro~.,,.
D.G.
and Winogred, T., "An Overview of KRL, a KnowledgeReprusentation Language," Cognitive Science, Vol.
1, No.
1, 1977, pp.
3-46,2.
Bobrow.
R.J., "The RUS System," BBN Report 3878, Bolt, Beranek, andNewman, 1978.3.
Bobrow, R.J. and Webber.
B, "Knowledge Representation forSyntactic/Semantic Processing," Prec.
National Conference of the AmericanAssociation IorArtilicial Intelligence.
Stanford University, August 1980.4.
Bobrow, D.G., Kaplan, R.M., Kay, M., Norman D.A., Thompson, H., andWinograd, T., 'GUS: a Frame.Driven Dialogue System," Artificial Intelligence,VOL 8, 1977, pp.
155-173.5.
Brown, J.S.
and Burton.
R.R.. "Multiple Representations o!
Knowledge forTutorial Reasoning," in Representation and Understanding.
Bobrow, D. G. andCollins, A., ed., Academic Press, New York, 1975, pp.
311-349.6.
CarbonelL J. G., "Towards a Self-Extending Parser," Proceedings of the 171hMeeting ot the Association for Computational Linguistics, 1979, pp.
3-7.7.
Carbonell, J. G. and Hayes, P. J., "Robust Parsing Us!ng Multiple Construction-Specific Strategies," in Natural Language Parsing Systems, L. BOIc, ed.,Springer-Verlag, 1984.8.
Carbonell, J.G.
and Hayes, P.J., "Recovery Strategies for ParsingExtragrammatical Language," Journal of Computational Linguistics, VOI.
10,1984, (publication forthcoming).9.
Carbonell, J.G., Boggs, W.M., Mauldin, M.L.
and Anick, P.G., "TheXCALIBUR Project, A Natural Language Interlace to Expert Systems,"Proceedings of the Eighth International Joi,'~t Conlerence on Artificialintelligence, 1983.10, Carbonell, J.G.. Boggs, W. M., Mauldin, M L. and Anick, P.G., "XCALIBURProgress Report #1: First Steps Towards an Integrated Natural LanguageInterface," Tech.
report, Carneg~e-Mellon University, Computer ScienceDepartment, 1983.1!
Carbonell, J G., "Discourse Pragmatics in Task-Oriented Natural LanguageInterlaces," Proceedings of the 21st annual meeting of the Association totComputalional Linguistics, 1983.12, Dejong, G..
Skimming Stories in Real-Time, PhD dissertation.
ComputerScience Dept., Yale University.
1979.13.
Durham.
I., Lamb, D.D.. and Saxe, J.B., "Spelling Correction in UserInterfaces," Comm.
ACM, Vol.
26, 1983.14.
Haas.
N. and Hendrix, G. G., "Learning by Being Told: Acquiring Knowledgetot Infoll,~ahon Management," in Machine Learning, An Artificial Intelligence4pproach, R. S. Michalski, J.G.
C.arbonell and T.M.
Mitchell, eds., TiogaPress, Pale Alto, CA, 1983.15.
Hayes P.J., "A Construction Specific Approach to Focused Interaction inFlexible Parsing," Prec.
el 191h Annual Meeting of the Assoc.
for Comput.Ling., June 1981, pp.
149.152.16.
Hayes, P.J.
and Mouradian, G.V., "Flexible Parsing," American Journal ofComputational Linguistics.
VoL 7, No.
4, 1981, pp.
232-241.17.
Hayes, P. J. and Carbonell, J. G., "Multi Strategy Construction-Specific Parsingfor Flexible Data Base Query and Update," Prec.
Seventh Int.
Jt.
Conf.
onArtificia//nte//igence, Vancouver, August 1981, pp.
432.439.18 Hayes, P. J. and Carbonell, J. G., "Multi-Strategy Parsing and its Role in RobustMan-Machine Communication," Tech.
report CMU-CS-81-118, Carnegie.Mellon Umversity, Computer Science Department, May 1981.19.
Hendrix, G.G., Sacerdoti, E.D.
and Slocum, J., "Developing a NaturalLanguage Interface m Complex Data," Tech.
report Artificial IntelligenceCanter., SRI International, 1976.20.
Hendrix, G.G, "Human Engineering for Applied Natural LanguageProcessing," Prec.
Fillh Int.
Jt.
Conf.
on Artiliciel Intelligence.
1977, pp.183-191.21.
Kwasny.
S.C. and Sondheimer, N K., "Relaxalion Techniques for ParsingGrammatically IlI-Folmed Ioput in Natural Language Understanding Systems,"American Journal o1 Computational Lmguistics.
Vol.
7, No.
2, 1981, pp.
99-108.22.
S~.hank, R C., Lebowitz, M, Bimbaum, E., "An Integrated Undemtander,"American Journal of Computational Linguistics.
Vol.
6, NO.
1, 1980, pp, 13-30.23.
Waltz, D.L., "An English Language Question Answering System for a LargeRelational Data Base," Comm.
ACM, Vol.
21 ,'No.
7, 1978, pp.
526-539.24.
Weischedel, R.M.
and Sondheimer, N K., "Me;a-Rules as a Basis forProcessing Ill-formed Input," Computational Linguistics, VoL 10, 1984.25.
Wemchedel, R.M.
and Black, J., "Responding to Potentially UnparseableSentences," American Journal of Computational Linguistics, Vol, 6, 1980, pp.97-109.26.
Wilks, Y.A., "Preference Semantics," in Formal Semantics of NaturalLanguage, Keenan, ed., Cambridge University Press, 1975.27.
Woods, W.A., "Cascaded ATN Grammars," American Journal ofCcmput=tional Linguistics.
Vol.
6, No.
1, August 1980, pp.
1-12.28.
Woods, W.A., Kaplan, R.M., and Nash-Webber, B., "The Lunar SciencesLanguage System; Final Report," lech.
report 2378, Bolt, Beranek, andNewman, inc., Canlbridge, Mass., 1972.29.
Woods, W. A., Bates, M., Brown, G., Bruce, B,, Cook, C., Klovatad, J., Makhoul,J., Nash-Webber, B., Schwartz., R., Wolf, J., and Zue, V., "SpeechUndeL'~tandmg Systems ?
Final Technical Report," Tech.
report 3438, Bolt,Beranek, and Newman, Inc.. Cambridge, Mass., 1976.30.
Wright, K. and Fox, M, "The SRL User3 Manual," Tech.
report, Robotic,=institute, Carnegie-Mellon University, 1983.443
