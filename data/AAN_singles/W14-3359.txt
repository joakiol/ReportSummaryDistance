Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 457?465,Baltimore, Maryland USA, June 26?27, 2014.c?2014 Association for Computational LinguisticsUnsupervised Adaptation for Statistical Machine TranslationSaab Mansour and Hermann NeyHuman Language Technology and Pattern RecognitionComputer Science DepartmentRWTH Aachen UniversityAachen, Germany{mansour,ney}@cs.rwth-aachen.deAbstractIn this work, we tackle the problem oflanguage and translation models domain-adaptation without explicit bilingual in-domain training data.
In such a scenario,the only information about the domaincan be induced from the source-languagetest corpus.
We explore unsupervisedadaptation, where the source-language testcorpus is combined with the correspond-ing hypotheses generated by the transla-tion system to perform adaptation.
Wecompare unsupervised adaptation to su-pervised and pseudo supervised adapta-tion.
Our results show that the choice ofthe adaptation (target) set is crucial forsuccessful application of adaptation meth-ods.
Evaluation is conducted over theGerman-to-English WMT newswire trans-lation task.
The experiments show that theunsupervised adaptation method generatesthe best translation quality as well as gen-eralizes well to unseen test sets.1 IntroductionOver the last few years, large amounts of statisticalmachine translation (SMT) monolingual and bilin-gual corpora were collected.
Early years focusedon structured data translation such as newswire.Nowadays, due to the relative success of SMT,new domains of translation are being explored,such as lecture and patent translation (Cettolo etal., 2012; Goto et al., 2013).The task of domain adaptation tackles the prob-lem of utilizing existing resources mainly drawnfrom one domain (e.g.
parliamentary discussion)to maximize the performance on the target (test)domain (e.g.
newswire).To be able to perform adaptation, a target setrepresenting the test domain is used to manipu-late the general-domain models.
Previous workon SMT adaptation focused on the scenario where(small) bilingual in-domain or pseudo in-domaintraining data are available.
Furthermore, small at-tention was given to the choice of the target set foradaptation.
In this work, we explore the problemof adaptation where no explicit bilingual data fromthe test domain is available for training, and theonly resource encapsulating information about thedomain is the source-language test corpus itself.We explore how to utilize the source-languagetest corpus for adapting the language model (LM)and the translation model (TM).
A combinationof source and automatically translated target ofthe test set is compared to using the source sideonly for TM adaptation.
Furthermore, we com-pare using the test set to using in-domain data anda pseudo in-domain data (e.g.
news-commentaryas opposed to newswire).Experiments are done on the WMT 2013German-to-English newswire translation task.Our best adaptation method shows competitive re-sults to the best submissions of the evaluation.This paper is structured as follows.
We reviewrelated work in Section 2 and introduce the basicadaptation methods in Section 3.
The experimen-tal setup is described in Section 4, results are dis-cussed in Section 5 and we conclude in Section 6.2 Related WorkA broad range of methods and techniques havebeen suggested in the past for domain adaptationfor both SMT and automatic speech recognition(ASR).For ASR, (Bellegarda, 2004) gives an overviewof LM adaptation methods.
He differentiates be-tween two cases regarding the availability of in-domain adaptation data: (i) the data is availableand can be directly used to manipulate a back-ground (general domain) corpus, and (ii) the datais not available or too small, and then it can begathered or automatically generated during the457recognition process.
(Bacchiani and Roark, 2003)compare supervised against unsupervised (usingautomatic transcriptions) in-domain data for LMtraining for the task of ASR.
They show that aug-menting the supervised in-domain to the train-ing of the LM performs better than the unsuper-vised in-domain.
In addition, they perform ?self-training?, where the test set is automatically tran-scribed and added to the LM.
When using a strongbaseline, no improvements in recognition qualityare achieved.
We differ from their work by us-ing the unsupervised test data to adapt a general-domain bilingual corpus.
We also performed ini-tial experiments of ?self-training?
for languagemodeling, where (artificial) perplexity improve-ment was achieved but without an impact on themachine translation (MT) quality.
(Zhao et al., 2004) tackle LM adaptation forSMT.
Similarly to our work, they use automati-cally generated hypotheses to perform adaptation.We extend their work by using the hypothesesalso for TM adaptation.
(Hildebrand et al., 2005)perform LM and TM adaptation based on infor-mation retrieval methods.
They use the source-language test corpus to filter the bilingual data,and then use the target side of the filtered bilingualdata to perform LM adaptation.
We differ fromtheir work by using both the in-domain source-language corpus and its corresponding automatictranslation for adaptation, which is shown in ourexperiments to achieve superior results than whenusing the source-side information only.
(Fosterand Kuhn, 2007) perform LM and TM adaptationusing mixture modeling.
In their setting, the mix-ture weights are modified to express adaptation.They compare cross-domain (in-domain available)against dynamic adaptation.
In the dynamic adap-tation scenario, they utilize the source side of thedevelopment set to adapt the mixture weights (LMadaptation is possible as they only use paralleltraining data, which enables filtering based on thesource side and then keeping the correspondingtarget side of the data).
For an in-domain test set,the cross-domain setup performs better than thedynamic adaptation method.
(Ueffing et al., 2007)use the test set translations as additional data totrain the TM.
One important aspect in their workis confidence measurement to remove noisy trans-lation.
In our approach, we use the automatic testset translations to adapt the SMT models ratherthan augmenting it as additional TM data.
We alsocompare different adaptation sets.
Furthermore,we do not use confidence measures to filter the au-tomatic translations as they are only used to adaptthe general-domain system and are not augmentedto the TM.In this work, we apply cross-entropy scoring foradaptation as done by (Moore and Lewis, 2010).Moore and Lewis (2010) apply adaptation by us-ing an LM-based cross-entropy filtering for LMtraining.
Axelrod et al.
(2011) generalized themethod for TM adaptation by interpolating thesource and target LMs.
These two works focusedon a scenario where in-domain training data areavailable for adaptation.
In this work, we focus ona scenario where in-domain training data is not la-beled, and the main resource for adaptation is thesource-language test data.In recent WMT evaluations, the method of(Moore and Lewis, 2010) was utilized by severaltranslation systems (Koehn and Haddow, 2012;Rubino et al., 2013).
These systems use pseudoin-domain corpus, i.e., news-commentary, as thetarget domain (while the test domain is newswire).The contribution of this work is two fold: weshow that the choice of the target set is crucial foradaptation, in addition, we show that an unsuper-vised target set performs best in terms of transla-tion quality as well as generalization performanceto unseen test sets (in comparison to using pseudoin-domain data or the references as target sets).3 Cross-Entropy AdaptationIn this work, we use sample scoring for the pur-pose of adaptation.
We start by introducing thescoring framework and then show how we utilize itto perform filtering based adaptation and weightedphrase extraction based adaptation.LM cross-entropy scores can be used for bothmonolingual data weighting for LM training asdone by (Moore and Lewis, 2010), or bilingualweighting for TM training as done by (Axelrod etal., 2011).We differentiate between two types of data sets:the adaptation set (target) representative of thetest-domain which we refer to also as in-domain(IN), and the general-domain (GD) set which wewant to adapt.The scores for each sentence in the general-domain corpus are based on the cross-entropy dif-ference of the IN and GD models.
DenotingHM(x) as the cross entropy of sentence x accord-458ing to model M , then the cross entropy differenceDHM(x) can be written as:DHM(x) = HMIN(x)?HMGD(x) (1)The intuition behind eq.
(1) is that we are inter-ested in sentences as close as possible to the in-domain, but also as far as possible from the gen-eral corpus.
Moore and Lewis (2010) show thatusing eq.
(1) for LM filtering performs better interms of perplexity than using in-domain cross-entropy only (HMIN(x)).
For more details aboutthe reasoning behind eq.
(1) we refer the reader to(Moore and Lewis, 2010).Axelrod et al.
(2011) adapted eq.
(1) for bilin-gual data filtering for the purpose of TM training.The bilingual LM cross entropy difference for asentence pair (fr, er) in the GD corpus is then de-fined by:DHLM(fr, er) = DHLMsrc(fr) +DHLMtrg(er)(2)For IBM Model 1 (M1), the cross-entropyHM1(fr|er) is defined similarly to the LM cross-entropy, and the resulting bilingual cross-entropydifference will be of the form:DHM1(fr, er) = DHM1(fr|er) +DHM1(er|fr)The combined LM+M1 score is obtained bysumming the LM and M1 bilingual cross-entropydifference scores:dr= DHLM(fr, er) +DHM1(fr, er) (3)3.1 FilteringA common framework to perform sample filteringis to score each sample according to a model, andthen assigning a threshold on the score which fil-ters out unwanted samples.
If the score we gener-ate is related to the probability that the sample wasdrawn from the same distribution as the in-domaindata, we are selecting the samples most relevant toour domain.
In this way we can achieve adaptationof the general-domain data.We use the LM cross-entropy difference fromeq.
(1) for LM filtering and a combined LM+M1score (eq.
(3) for TM filtering.
We sort the sen-tences in the general-domain according to thescore and select the best 50%,25%,...,6.25% train-ing instances.
Our models are then trained onthe selected portions of the training data, and thebest performing portion (according to perplexityfor LM training and BLEU for TM training) on thedevelopment set is chosen as the adapted corpus.3.2 Weighted Phrase ExtractionThe classical phrase model is trained using a ?sim-ple?
maximum likelihood estimation, resulting inphrase translation probabilities being defined byrelative frequency:p(?f |e?)
=?rcr(?f, e?)??f?
?rcr(?f?, e?
)(4)Here,?f, e?
are contiguous phrases, cr(?f, e?)
de-notes the count of (?f, e?)
being a translation of eachother (usually according to word alignment andheuristics) in sentence pair (fr, er).
One methodto introduce weights to eq.
(4) is by weightingeach sentence pair by a weight wr.
Eq.
(4) willnow have the extended form:p(?f |e?)
=?rwr?
cr(?f, e?)??f??rwr?
cr(?f?, e?
)(5)It is easy to see that setting {wr= 1} will resultin eq.
(4) (or any non-zero equal weights).
Increas-ing the weight wrof the corresponding sentencepair will result in an increase of the probabilitiesof the phrase pairs extracted.
Thus, by increasingthe weight of in-domain sentence pairs, the prob-ability of in-domain phrase translations could alsoincrease.We utilize drfrom eq.
(3) using a combinedLM+M1 scores for our suggested weighted phraseextraction.
drcan be assigned negative values, andlower drindicates sentence pairs which are morerelevant to the in-domain.
Therefore, we negatethe term drto get the notion of higher is closerto the in-domain, and use an exponent to ensurepositive values.
The final weight is of the form:wr= e?dr(6)This term is proportional to perplexities, as theexponent of entropy is perplexity by definition.One could also use filtering for TM adaptation,but, as shown in (Mansour and Ney, 2012), filter-ing for TM could only reduce the size and weight-ing performs better than filtering.4 Experimental Setup4.1 Training DataThe experiments are done on the recent German-to-English WMT 2013 translation task1.
For1The translation task resources of WMT 2013 are avail-able under: http://www.statmt.org/wmt13/459Corpus Sent De EnTraining datanews-commentary 177K 4.8M 4.5Meuroparl 1 888K 51.5M 51.9Mcommon-crawl 2 030K 47.8M 47.7Mtotal 4 095K 104.1M 104MTest datanewstest08 2051 52446 49749newstest09 2525 68512 65648newstest10 2489 68232 62024newstest11 3003 80181 74856newstest12 3003 79912 73089newstest13 3000 69066 64900Table 1: German-English bilingual training andtest data statistics: the number of sentence pairs(Sent), German (De) and English (En) words aregiven.German-English WMT 2013, the common-crawlbilingual corpus was introduced, enabling moreimpact for TM adaptation on the SMT systemquality.
Monolingual English data exists withmore than 1 billion words, making LM adapta-tion and size reduction a wanted feature.
We usenewstest08 throughout newstest13 to evaluate theSMT systems.
The baseline systems are builtusing all (unfiltered) available monolingual andbilingual training data.
The bilingual corpora andthe test data statistics are summarized in Table 1.In Table 2, we summarize the size and LM per-plexity of the different monolingual corpora forthe German-English task over the LM develop-ment set newstest09 and test set newstest13.
Thecorpora are split into three parts, the English sideof the bilingual side (bi.en), the giga-fren joinedwith undoc (giun) and the news-shuffle (ns) cor-pus.
To keep the perplexity results comparable,we use the intersection vocabulary of the differentcorpora as a reference vocabulary.
From the table,we notice that as expected, the in-domain corpusnews-shuffle generate the best perplexity values.4.2 SMT SystemThe baseline system is built using the open-sourceSMT toolkit Jane2, which provides state-of-the-artphrase-based SMT system (Wuebker et al., 2012).We use the standard set of models with phrasetranslation probabilities for source-to-target and2www.hltpr.rwth-aachen.de/janeCorpus Tokens ppl[M] dev testbi.en 88 216.5 192.7giun 775 229.0 198.9ns 1 479 144.1 122.7Table 2: German-English monolingual corporastatistics: the number of tokens is given in millions[M], ppl is the perplexity of the corresponding cor-pus.target-to-source directions, smoothing with lexi-cal weights, a word and phrase penalty, distance-based reordering, hierarchical reordering model(Galley and Manning, 2008) and a 4-gram targetlanguage model.
The baseline system is compet-itive and using adaptation we will show compa-rable results to the best systems of WMT 2013.The SMT system was tuned on the developmentset newstest10 with minimum error rate training(MERT) (Och, 2003) using the BLEU (Papineniet al., 2002) error rate measure as the optimiza-tion criterion.
We test the performance of our sys-tem on the newstest08...newstest13 sets using theBLEU and translation edit rate (TER) (Snover etal., 2006) measures.
We use TER as an additionalmeasure to verify the consistency of our improve-ments and avoid over-tuning.
All results are basedon true-case evaluation.
We perform bootstrap re-sampling with bounds estimation as described by(Koehn, 2004).
We use the 90% and 95% (denotedby ?
and ?
correspondingly in the tables) confi-dence thresholds to draw significance conclusions.5 ResultsTo perform adaptation, an adaptation set repre-senting the in-domain needs to be specified to beplugged in eq.
(1) as IN.
The choice of the adap-tation corpus is crucial for the successful appli-cation of the cross-entropy based scoring, as thecloser the corpus is to our test domain, the bet-ter adaptation we get.
For the WMT task, thechoice of the adaptation corpus is not an easytask.
The genre of the test sets is newswire, whilethe bilingual training data is composed of news-commentary, parliamentary records (europarl) andcommon-crawl noisy data.
On the other hand, themonolingual data includes large amounts of in-domain newswire data (news-shuffle).For LM training, the task of adaptation mightbe unprofitable in terms of performance, as the4601101201301401501601706.25% 12.5% 25% 50% 100%perplexitysizeREF-devREF-testHYP-devHYP-testFigure 1: Size (fraction of news-shuffle data)against the resulting LM perplexity on dev andtest, using different filtering sets.majority of the training is in-domain.
Still, onemight hope that by using adaptation, a more com-pact and comparable LM can be generated.
An-other point is that LM training is less demandingthan TM training, and a comparison of the resultsof LM and TM adaptation might prove fruitful andconvey additional information.Next, we start with LM adaptation experimentswhere we mainly compare different adaptationsets for filtering over the final translation quality.A comparison to the full (unfiltered LM) is alsoproduced.
For TM adaptation, we repeat the adap-tation sets choice experiment and analyze the dif-ference between the sets.5.1 LM AdaptationTo evaluate our methods experimentally, we usethe German-English translation task to comparedifferent adaptation sets for filtering and then an-alyze the full versus the filtered LM SMT systemresults.
We recall that newstest09 is used as a de-velopment set and newstest13 as a test set in theLM experiments.The different adaptation sets for filtering that weexplore are: (i) unsupervised: an automatic trans-lation of the test sets (newstest08...newstest13),where the baseline system (without adaptation)is used to generate the hypotheses which thendefine the adaptation corpus for filtering (HYP),(ii) supervised: the references of the test sets new-stest08...newstest12 concatenated, newstest13 iskept as a blind set, which will also help us deter-mine if overfitting occurs (REF), and (iii) pseudosupervised: a pseudo in-domain corpus, news-Corpus Adapt Optimal pplset size dev testnsnone 100% 144 123NC 100% 144 123REF 6.25% 111 161HYP 50% 139 118giunnone 100% 229 199NC 50% 215 185REF 6.25% 161 171HYP 12.5% 187 159Table 3: Optimal size portion and resulting per-plexities, across adaptation sets (NC, REF andHYP) and monolingual LM training corpora.commentary, where the domain is similar to thetest set domain, but the style might differ (NC).Next, we filter the news-shuffle (ns) and giga-fren+undoc (giun) according to the three sug-gested adaptations sets, where we plug each adap-tation set in eq.
(1) as IN and compare their per-formance.5.1.1 Perplexity ResultsIn Figure 1, we draw the size portion versus thedev and test perplexities for the REF and HYPadaptation sets over the news-shuffle corpus.
REFperforms best for filtering the dev set, where anoptimum is achieved when using only 6.25% ofthe news-shuffle data, with a perplexity of 111 incomparison to 144 perplexity of the full LM.
Mea-suring perplexities over newstest08-12, REF basedfiltering achieves 109 while the full LM achieves140.
The good performance on the seen setscomes with the cost of severe overfitting, wherethe test set perplexity using 6.25% of the data is161, much higher than 123 generated by the fullLM.
On the other hand, HYP achieves an optimumfor both sets when using 50% of the data.
A sum-mary of the best results across monolingual cor-pora and adaptation sets is given in Table 3.
Fil-tering the giun monolingual corpus shows similarresults to ns filtering, where overfitting occurs onthe blind test set when using REF as the target do-main.
HYP-based adaptation achieves the best LMperplexity on the blind test set.
NC-based adapta-tion retains the biggest amount of data, 50% forthe giun corpus and 100% (no filtering) for the nscorpus.
REF-based adaptation shows overfittingon the seen dev set, and the worst results on theblind test set when filtering the ns corpus.461LM data Adapt.
ppl newstest10 newstest11 newstest12 newstest13set BLEU TER BLEU TER BLEU TER BLEU TERbi.en+giunnone 162 23.2 59.6 21.2 61.0 21.8 60.9 24.6 57.2NC 160 23.2 59.3 21.5 61.0 21.9 60.7 24.6 57.0REF 158 23.7 59.2 21.9 60.5 22.2 60.5 24.5 57.3HYP 151 23.6 59.2 21.5 60.9 22.2 60.4 25.1 56.7+nsnone 111 24.5 59.1 22.1 61.3 23.3 60.1 25.9 56.7NC 111 24.4 58.7 22.1 60.5 23.4 59.7 25.5 56.6REF 143 25.7 57.8 23.0 59.9 24.2 59.4 24.1 57.8HYP 109 25.0 58.2 22.1 60.6 23.5 59.6 25.9 56.3Table 4: German-English LM filtering results using different adaptation sets.
The LM perplexity overthe blind test set nestest13, as well as BLEU and TER percentages are presented.5.1.2 Translation ResultsNext, we measure whether the improvements ofthe single adapted corpora carry over to the mix-ture LM both in perplexity and translation quality.The mixture LM is created by linear interpolation(of bi.en, giun and ns) with perplexity minimiza-tion on the dev set using the SRILM toolkit3.
Wecarry out two experiments, in the first we interpo-late the English side of the bilingual data with agiun LM, then we add the ns LM.
This way wemeasure whether the effects of adaptation carryover to a stronger baseline.The SMT systems built using the full and fil-tered LMs are compared in Table 4.
The tableincludes the data used for LM training, the adap-tation set used to filter the data, the perplexityof the resulting LM on the test set (newstest13)and the resulting SMT system quality over new-stest10...newstest13.Starting with the first block of experiments us-ing LM data composed from the English sideof the bilingual corpora and the giun corpus(bi.en+giun), the unfiltered LM performs worse,both in terms of perplexity and translation qual-ity.
The NC based adaptation improves the resultsslightly, with gains upto +0.3% BLEU on new-stest11 and -0.3% TER on newstest10.
The over-fitting behavior of REF adapted LMs carries overto the mixture LM, mainly on the translation qual-ity.
The REF adapted LM system translation re-sults are better on the test sets used to perform theadaptation, but worse on the blind test set (new-stest13).
The HYP system performs best in termsof perplexity.
REF is better than HYP over thenon-blind test sets, but HYP outperforms REF on3http://www.speech.sri.com/projects/srilm/newstest13 with an improvement of +0.6% BLEUand -0.6% TER.The second block of experiments where news-shuffle (ns) is added to the mixture shows evenstronger overfitting for REF.
The REF based adap-tation is performing worse in terms of perplexity,143 in comparison to 111 for the full LM.
On theblind set newstest13, REF is hindering the resultswith a loss of -1.8% BLEU in comparison to thefull system, and a loss of -0.4% BLEU in compar-ison to the corresponding system without ns.
Onthe non-blind sets, REF is performing best, show-ing typical overfitting.
Comparing the full LMsystem to the HYP adapted LM, big improvementsare mainly observed on TER, with significance atthe 95% level for newstest10.We conclude that using the references as adap-tation set causes overfitting, using a pseudo in-domain set as the news-commentary does not im-prove the results, and the best choice is using theautomatic translations (HYP).As already mentioned in Section 2, we experi-mented with adding the automatic translations ofthe test sets (HYP) to the LM.
Doing so resultedin 8 points perplexity reduction, but no impact onthe MT quality was observed.
Therefore, we deemthese perplexity improvements by adding HYP asartificial.5.2 TM AdaptationIn the LM adaptation experiments, we found thatusing the test sets automatic translation as theadaptation set (HYP system) for filtering per-formed best, in terms of LM quality (perplex-ity) and translation quality, when compared to theother suggested adaptation sets, especially on theblind test set.462LM TM newstest10 newstest11 newstest12 newstest13BLEU TER BLEU TER BLEU TER BLEU TERfull full 24.5 59.1 22.1 61.3 23.3 60.1 25.9 56.7HYPfull 25.0 58.2?
22.1 60.6 23.5 59.6 25.9 56.3TM FilteringREF-25% 25.1 57.9?
22.4 60.2?
24.0?
59.1?
25.5 56.7HYP-50% 25.2 58.0?
22.2 60.5?
23.8?
59.4?
26.0 56.4TM Weightingppl.NC 25.0 58.1?
22.5 60.2?
23.6 59.5?
26.1 56.2ppl.TST 24.8 58.8 22.3 60.7 23.6 59.7 26.0 56.3ppl.REF 24.8 58.2?
22.2 60.3?
23.7 59.5?
25.5 56.4ppl.HYP 25.4?
57.8?
22.5 60.1?
23.9?
59.3?
26.4?
55.9?Table 5: German-English TM filtering and weighting results using different adaptation sets.
The resultsare given in BLEU and TER percentages.
Significance is measured over the full system (first row).For TM adaptation, we experiment with filter-ing and weighting based adaptation.
By usingweighting, we expect further improvements overthe baseline and better differentiation between thecompeting adaptation sets.To perform filtering, we concatenate all thebilingual corpora in Table 1 and sort them accord-ing to the combined LM+M1 cross-entropy score.We then extract the top 50%,25%,... bilingual sen-tence from the sorted corpus, generate the phrasetable for each setup and reoptimize the system us-ing MERT on the development set.Weighted phrase extraction is based on the sameLM+M1 combined cross entropy score as filter-ing, but instead of discarding whole sentences weweight them according to their relevance to theadaptation set being used.In this section, we compare the three adapta-tion sets suggested for LM filtering for the TMcomponent.
In addition, one might argue that forthe bilingual case, the source side of the test setmight be sufficient to perform adaptation, or evenit might perform better for TM adaptation as theautomatically generated translation might not beas reliable.
We perform an experiment using thesource side of the test sets as an adaptation set toscore the source side of the bilingual corpora (de-noted TST in the experiments).
To summarize, wecollect 4 corpora as adaptation sets to be used foradapting the TM: (i) NC, HYP, and REF as definedfor LM but using both source and target (automat-ically generated for HYP) sides, and (ii) TST usingonly the source side of the test sets.The results comparing the 4 suggested adapta-tion sets for filtering and weighting are given inTable 5.
In this table, we use newstest10 as be-fore for MERT optimization and display results fornewstest10...newstest13.
Note that for TM filter-ing and weighting we use the HYP adapted LM asit achieves the best results in the previous section.For filtering, the NC and TST adaptation setscould not improve the dev results over the full sys-tem therefore they are omitted.
REF based adapta-tion achieves the best dev results when using 25%of the bilingual data while HYP based adaptationuses 50% of the data.
For TM filtering, only slightoverfitting is observed, where the REF system isslightly better than HYP on the non blind sets andis worse on the blind test set.
We hypothesize thatno severe overfitting is observed for TM filteringas we use a strong LM adapted with the HYP set,therefore degradation is lessened.Next, we focus on weighted phrase extractionfor adaptation using the various adaptation sets.Comparing filtering to weighting, weighting im-proves for the ppl.HYP based adaptation but aslight loss is observed for the ppl.REF system ex-cept on the blind test set.
We conclude that due tothe usage of more data in the weighting scenario,overfitting is lessened.
Using the source side of thetest sets for weighting (ppl.TST) achieves good re-sults, with improvements over the ppl.REF systemon newstest13.The ppl.HYP system achieves the best resultsamong the weighted systems.
Comparing thefull unadapted system with the LM+TM adaptedppl.HYP system, we achieve significant BLEU im-provements on most sets, TER improvements aresignificant in all cases with 95% significance level.The highest gains are on the development set with463+0.9% BLEU and -1.3% TER improvements, onthe test sets, newstest12 improves with +0.6%BLEU and -0.8% TER and newstest13 improveswith +0.5% BLEU and -0.8% TER.
The ppl.HYPsystem is comparable to the best single systemof WMT 20134(26.4% BLEU vs 26.8% BLEUfor Edinburgh submission, RWTH submission is asystem combination).
Note that we are not usingthe LDC GigaWord corpus.We conclude that using in-domain automatictranslations (HYP) for TM weighting performsbest, better than using source side only in-domain(TST) and better than using the references (REF)especially on the blind test set.
TM adaptationshows further improvements on top of LM adap-tation and achieves significant gains.6 ConclusionIn this work, we tackle the problem of adaptationwithout labeled bilingual in-domain training data.The only information about the test domain is en-capsulated in the test sets themselves.
We experi-ment with unsupervised adaptation for SMT, usingautomatic translations of the test sets, focusing onadaptation for the LM and the TM components.We use cross-entropy based scoring for the taskof adaptation, as this method proved successful inprevious work.
We utilize filtering for LM adapta-tion, while we compare filtering and weighting forTM adaptation.For LM adaptation, the setup we devise al-ready contains a majority of in-domain data, stillwe could report improvements over the unadaptedbaseline.
We compose three different adaptationsets for filtering using automatic translation of thetest data (HYP), a pseudo in-domain set (NC) andthe references (REF) of the test sets (keeping oneblind test set).
The NC based filtering is not able toperform good selection, for news-shuffle the wholecorpus is retained and for giun 50% of the data isretained.
The perplexity results and the translationquality are virtually unchanged in comparison tothe full system.
Using REF as the target set causesoverfitting, where the results are better on the seentest sets but worse on the blind test set.
The bestperforming target set in our experiments is the un-supervised HYP adaptation set, achieving the bestperplexity as well as the best translation quality onthe blind test set.
Therefore, we conclude that for4http://matrix.statmt.org/matrix/systems_list/1712developing a successful SMT system that can gen-eralize to new data the HYP based adaptation ispreferred.Next, we perform TM adaptation, where we re-peat the comparison between the different adapta-tion sets for filtering as well as weighting.
We alsocompare to adaptation based only on the sourceside of the test sets (TST).
The LM adaptationresults hold for TM adaptation, where using theautomatic translations method shows the best re-sults for the blind test set.
Our experiments showthat using the source side only of the test set foradaptation performs worse than the unsupervisedmethod, reminiscent to results reported in previouswork comparing supervised source side againstbilingual filtering (Axelrod et al., 2011).
For filter-ing, the REF system suffers from overfitting, whilewhen using weighting for adaptation, overfittingis lessened.
Comparing the unadapted baseline tothe adapted LM and TM system using the HYPset, improvements of +1.0% BLEU and -1.3% TERare reported on the development set while +0.5%BLEU and -0.8% TER improvements are reportedon the blind test set.AcknowledgmentsThis material is based upon work supported bythe DARPA BOLT project under Contract No.HR0011-12-C-0015.
Any opinions, findings andconclusions or recommendations expressed in thismaterial are those of the authors and do not neces-sarily reflect the views of DARPA.ReferencesAmittai Axelrod, Xiaodong He, and Jianfeng Gao.2011.
Domain adaptation via pseudo in-domain dataselection.
In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Process-ing, pages 355?362, Edinburgh, Scotland, UK., July.Association for Computational Linguistics.M.
Bacchiani and B. Roark.
2003.
Unsupervisedlanguage model adaptation.
In Acoustics, Speech,and Signal Processing, 2003.
Proceedings.
(ICASSP?03).
2003 IEEE International Conference on, vol-ume 1, pages I?224 ?
I?227 vol.1, april.Jerome R Bellegarda.
2004.
Statistical languagemodel adaptation: review and perspectives.
SpeechCommunication, 42(1):93 ?
108.
Adaptation Meth-ods for Speech Recognition.M Federico M Cettolo, L Bentivogli, M Paul, andS St?uker.
2012.
Overview of the iwslt 2012 eval-uation campaign.
In International Workshop on464Spoken Language Translation, pages 12?33, HongKong, December.George Foster and Roland Kuhn.
2007.
Mixture-model adaptation for SMT.
In Proceedings of theSecond Workshop on Statistical Machine Transla-tion, pages 128?135, Prague, Czech Republic, June.Association for Computational Linguistics.Michel Galley and Christopher D. Manning.
2008.
ASimple and Effective Hierarchical Phrase Reorder-ing Model.
In Proceedings of the 2008 Conferenceon Empirical Methods in Natural Language Pro-cessing, pages 847?855, Honolulu, Hawaii, USA,October.Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, andBenjamin K Tsou.
2013.
Overview of the patentmachine translation task at the ntcir-10 workshop.In Proceedings of the 10th NTCIR Conference, vol-ume 10, pages 260?286, Tokyo, Japan, June.Almut Silja Hildebrand, Matthias Eck, Stephan Vogel,and Alex Waibel.
2005.
Adaptation of the transla-tion model for statistical machine translation basedon information retrieval.
In Proceedings of the 10thEAMT conference on ?Practical applications of ma-chine translation?, pages 133?1142, May.Philipp Koehn and Barry Haddow.
2012.
Towardseffective use of training data in statistical machinetranslation.
In Proceedings of the Seventh Workshopon Statistical Machine Translation, pages 317?321,Montr?eal, Canada, June.
Association for Computa-tional Linguistics.Philipp Koehn.
2004.
Statistical Significance Testsfor Machine Translation Evaluation.
In Proc.
of theConf.
on Empirical Methods for Natural LanguageProcessing (EMNLP), pages 388?395, Barcelona,Spain, July.Saab Mansour and Hermann Ney.
2012.
A sim-ple and effective weighted phrase extraction for ma-chine translation adaptation.
In International Work-shop on Spoken Language Translation, pages 193?200, Hong Kong, December.Robert C. Moore and William Lewis.
2010.
Intelligentselection of language model training data.
In Pro-ceedings of the ACL 2010 Conference Short Papers,pages 220?224, Uppsala, Sweden, July.
Associationfor Computational Linguistics.Franz J. Och.
2003.
Minimum Error Rate Train-ing in Statistical Machine Translation.
In Proceed-ings of the 41th Annual Meeting of the Associationfor Computational Linguistics, pages 160?167, Sap-poro, Japan, July.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a Method for AutomaticEvaluation of Machine Translation.
In Proceed-ings of the 41st Annual Meeting of the Associa-tion for Computational Linguistics, pages 311?318,Philadelphia, Pennsylvania, USA, July.Raphael Rubino, Antonio Toral, SantiagoCort?es Va?
?llo, Jun Xie, Xiaofeng Wu, StephenDoherty, and Qun Liu.
2013.
The CNGL-DCU-Prompsit translation systems for WMT13.
InProceedings of the Eighth Workshop on Statisti-cal Machine Translation, pages 213?218, Sofia,Bulgaria, August.
Association for ComputationalLinguistics.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A Study ofTranslation Edit Rate with Targeted Human Annota-tion.
In Proceedings of the 7th Conference of the As-sociation for Machine Translation in the Americas,pages 223?231, Cambridge, Massachusetts, USA,August.Nicola Ueffing, Gholamreza Haffari, and AnoopSarkar.
2007.
Transductive learning for statisticalmachine translation.
In Proceedings of the 45th An-nual Meeting of the Association of ComputationalLinguistics, pages 25?32, Prague, Czech Republic,June.
Association for Computational Linguistics.Joern Wuebker, Matthias Huck, Stephan Peitz, MalteNuhn, Markus Freitag, Jan-Thorsten Peter, SaabMansour, and Hermann Ney.
2012.
Jane 2: Opensource phrase-based and hierarchical statistical ma-chine translation.
In International Conference onComputational Linguistics, Mumbai, India, Decem-ber.Bing Zhao, Matthias Eck, and Stephan Vogel.
2004.Language model adaptation for statistical machinetranslation with structured query models.
In Pro-ceedings of the 20th international conference onComputational Linguistics, COLING ?04, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.465
