Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 19?27,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsA Study on Similarity and RelatednessUsing Distributional and WordNet-based ApproachesEneko Agirre?
Enrique Alfonseca?
Keith Hall?
Jana Kravalova??
Marius Pas?ca?
Aitor Soroa??
IXA NLP Group, University of the Basque Country?
Google Inc.?
Institute of Formal and Applied Linguistics, Charles University in Prague{e.agirre,a.soroa}@ehu.es {ealfonseca,kbhall,mars}@google.comkravalova@ufal.mff.cuni.czAbstractThis paper presents and compares WordNet-based and distributional similarity approaches.The strengths and weaknesses of each ap-proach regarding similarity and relatednesstasks are discussed, and a combination is pre-sented.
Each of our methods independentlyprovide the best results in their class on theRG and WordSim353 datasets, and a super-vised combination of them yields the best pub-lished results on all datasets.
Finally, we pio-neer cross-lingual similarity, showing that ourmethods are easily adapted for a cross-lingualtask with minor losses.1 IntroductionMeasuring semantic similarity and relatedness be-tween terms is an important problem in lexical se-mantics.
It has applications in many natural lan-guage processing tasks, such as Textual Entailment,Word Sense Disambiguation or Information Extrac-tion, and other related areas like Information Re-trieval.
The techniques used to solve this problemcan be roughly classified into two main categories:those relying on pre-existing knowledge resources(thesauri, semantic networks, taxonomies or ency-clopedias) (Alvarez and Lim, 2007; Yang and Pow-ers, 2005; Hughes and Ramage, 2007) and those in-ducing distributional properties of words from cor-pora (Sahami and Heilman, 2006; Chen et al, 2006;Bollegala et al, 2007).In this paper, we explore both families.
For thefirst one we apply graph based algorithms to Word-Net, and for the second we induce distributionalsimilarities collected from a 1.6 Terabyte Web cor-pus.
Previous work suggests that distributional sim-ilarities suffer from certain limitations, which makethem less useful than knowledge resources for se-mantic similarity.
For example, Lin (1998b) findssimilar phrases like captive-westerner which madesense only in the context of the corpus used, andBudanitsky and Hirst (2006) highlight other prob-lems that stem from the imbalance and sparseness ofthe corpora.
Comparatively, the experiments in thispaper demonstrate that distributional similarities canperform as well as the knowledge-based approaches,and a combination of the two can exceed the per-formance of results previously reported on the samedatasets.
An application to cross-lingual (CL) sim-ilarity identification is also described, with applica-tions such as CL Information Retrieval or CL spon-sored search.
A discussion on the differences be-tween learning similarity and relatedness scores isprovided.The paper is structured as follows.
We firstpresent the WordNet-based method, followed by thedistributional methods.
Section 4 is devoted to theevaluation and results on the monolingual and cross-lingual tasks.
Section 5 presents some analysis, in-cluding learning curves for distributional methods,the use of distributional similarity to improve Word-Net similarity, the contrast between similarity andrelatedness, and the combination of methods.
Sec-tion 6 presents related work, and finally, Section 7draws the conclusions and mentions future work.2 WordNet-based methodWordNet (Fellbaum, 1998) is a lexical database ofEnglish, which groups nouns, verbs, adjectives andadverbs into sets of synonyms (synsets), each ex-pressing a distinct concept.
Synsets are interlinkedwith conceptual-semantic and lexical relations, in-cluding hypernymy, meronymy, causality, etc.Given a pair of words and a graph-based repre-sentation of WordNet, our method has basically two19steps: We first compute the personalized PageR-ank over WordNet separately for each of the words,producing a probability distribution over WordNetsynsets.
We then compare how similar these two dis-crete probability distributions are by encoding themas vectors and computing the cosine between thevectors.We represent WordNet as a graph G = (V,E) asfollows: graph nodes represent WordNet concepts(synsets) and dictionary words; relations amongsynsets are represented by undirected edges; anddictionary words are linked to the synsets associatedto them by directed edges.For each word in the pair we first compute a per-sonalized PageRank vector of graph G (Haveliwala,2002).
Basically, personalized PageRank is com-puted by modifying the random jump distributionvector in the traditional PageRank equation.
In ourcase, we concentrate all probability mass in the tar-get word.Regarding PageRank implementation details, wechose a damping value of 0.85 and finish the calcula-tion after 30 iterations.
These are default values, andwe did not optimize them.
Our similarity method issimilar, but simpler, to that used by (Hughes and Ra-mage, 2007), which report very good results on sim-ilarity datasets.
More details of our algorithm can befound in (Agirre and Soroa, 2009).
The algorithmand needed resouces are publicly available1.2.1 WordNet relations and versionsThe WordNet versions that we use in this work arethe Multilingual Central Repository or MCR (At-serias et al, 2004) (which includes English Word-Net version 1.6 and wordnets for several other lan-guages like Spanish, Italian, Catalan and Basque),and WordNet version 3.02.
We used all the rela-tions in MCR (except cooccurrence relations and se-lectional preference relations) and in WordNet 3.0.Given the recent availability of the disambiguatedgloss relations for WordNet 3.03, we also used aversion which incorporates these relations.
We willrefer to the three versions as MCR16, WN30 andWN30g, respectively.
Our choice was mainly moti-vated by the fact that MCR contains tightly aligned1http://http://ixa2.si.ehu.es/ukb/2Available from http://http://wordnet.princeton.edu/3http://wordnet.princeton.edu/glosstagwordnets of several languages (see below).2.2 Cross-lingualityMCR follows the EuroWordNet design (Vossen,1998), which specifies an InterLingual Index (ILI)that links the concepts across wordnets of differ-ent languages.
The wordnets for other languages inMCR use the English WordNet synset numbers asILIs.
This design allows a decoupling of the rela-tions between concepts (which can be taken to belanguage independent) and the links from each con-tent word to its corresponding concepts (which islanguage dependent).As our WordNet-based method uses the graph ofthe concepts and relations, we can easily computethe similarity between words from different lan-guages.
For example, consider a English-Spanishpair like car ?
coche.
Given that the Spanish Word-Net is included in MCR we can use MCR as thecommon knowledge-base for the relations.
We canthen compute the personalized PageRank for eachof car and coche on the same underlying graph, andthen compare the similarity between both probabil-ity distributions.As an alternative, we also tried to use pub-licly available mappings for wordnets (Daude et al,2000)4 in order to create a 3.0 version of the Span-ish WordNet.
The mapping was used to link Spanishvariants to 3.0 synsets.
We used the English Word-Net 3.0, including glosses, to construct the graph.The two Spanish WordNet versions are referred toas MCR16 and WN30g.3 Context-based methodsIn this section, we describe the distributional meth-ods used for calculating similarities between words,and profiting from the use of a large Web-based cor-pus.This work is motivated by previous studies thatmake use of search engines in order to collect co-occurrence statistics between words.
Turney (2001)uses the number of hits returned by a Web searchengine to calculate the Pointwise Mutual Informa-tion (PMI) between terms, as an indicator of syn-onymy.
Bollegala et al (2007) calculate a numberof popular relatedness metrics based on page counts,4http://www.lsi.upc.es/?nlp/tools/download-map.php.20like PMI, the Jaccard coefficient, the Simpson co-efficient and the Dice coefficient, which are com-bined with lexico-syntactic patterns as model fea-tures.
The model parameters are trained using Sup-port Vector Machines (SVM) in order to later rankpairs of words.
A different approach is the one takenby Sahami and Heilman (2006), who collect snip-pets from the results of a search engine and repre-sent each snippet as a vector, weighted with the tf?idfscore.
The semantic similarity between two queriesis calculated as the inner product between the cen-troids of the respective sets of vectors.To calculate the similarity of two words w1 andw2, Ruiz-Casado et al (2005) collect snippets con-taining w1 from a Web search engine, extract a con-text around it, replace it with w2 and check for theexistence of that modified context in the Web.Using a search engine to calculate similarities be-tween words has the drawback that the data used willalways be truncated.
So, for example, the numbersof hits returned by search engines nowadays are al-ways approximate and rounded up.
The systems thatrely on collecting snippets are also limited by themaximum number of documents returned per query,typically around a thousand.
We hypothesize thatby crawling a large corpus from the Web and doingstandard corpus analysis to collect precise statisticsfor the terms we should improve over other unsu-pervised systems that are based on search engineresults, and should yield results that are competi-tive even when compared to knowledge-based ap-proaches.In order to calculate the semantic similarity be-tween the words in a set, we have used a vector spacemodel, with the following three variations:In the bag-of-words approach, for each word win the dataset we collect every term t that appears ina window centered in w, and add them to the vectortogether with its frequency.In the context window approach, for each wordw in the dataset we collect every window W cen-tered in w (removing the central word), and add itto the vector together with its frequency (the totalnumber of times we saw windowW around w in thewhole corpus).
In this case, all punctuation symbolsare replaced with a special token, to unify patternslike , the <term> said to and ?
the <term> said to.Throughout the paper, when we mention a contextwindow of size N it means N words at each side ofthe phrase of interest.In the syntactic dependency approach, we parsethe entire corpus using an implementation of an In-ductive Dependency parser as described in Nivre(2006).
For each word w we collect a template ofthe syntactic context.
We consider sequences of gov-erning words (e.g.
the parent, grand-parent, etc.)
aswell as collections of descendants (e.g., immediatechildren, grandchildren, etc.).
This information isthen encoded as a contextual template.
For example,the context template cooks <term> delicious couldbe contexts for nouns such as food, meals, pasta, etc.This captures both syntactic preferences as well asselectional preferences.
Contrary to Pado and Lap-ata (2007), we do not use the labels of the syntacticdependencies.Once the vectors have been obtained, the fre-quency for each dimension in every vector isweighted using the other vectors as contrast set, withthe ?2 test, and finally the cosine similarity betweenvectors is used to calculate the similarity betweeneach pair of terms.Except for the syntactic dependency approach,where closed-class words are needed by the parser,in the other cases we have removed stopwords (pro-nouns, prepositions, determiners and modal andauxiliary verbs).3.1 Corpus usedWe have used a corpus of four billion documents,crawled from the Web in August 2008.
An HTMLparser is used to extract text, the language of eachdocument is identified, and non-English documentsare discarded.
The final corpus remaining at the endof this process contains roughly 1.6 Terawords.
Allcalculations are done in parallel sharding by dimen-sion, and it is possible to calculate all pairwise sim-ilarities of the words in the test sets very quicklyon this corpus using the MapReduce infrastructure.A complete run takes around 15 minutes on 2,000cores.3.2 Cross-lingualityIn order to calculate similarities in a cross-lingualsetting, where some of the words are in a language lother than English, the following algorithm is used:21Method Window size RG dataset WordSim353 datasetMCR16 0.83 [0.73, 0.89] 0.53 (0.56) [0.45, 0.60]WN30 0.79 [0.67, 0.86] 0.56 (0.58) [0.48, 0.63]WN30g 0.83 [0.73, 0.89] 0.66 (0.69) [0.59, 0.71]CW 1 0.83 [0.73, 0.89] 0.63 [0.57, 0.69]2 0.83 [0.74, 0.90] 0.60 [0.53, 0.66]3 0.85 [0.76, 0.91] 0.59 [0.52, 0.65]4 0.89 [0.82, 0.93] 0.60 [0.53, 0.66]5 0.80 [0.70, 0.88] 0.58 [0.51, 0.65]6 0.75 [0.62, 0.84] 0.58 [0.50, 0.64]7 0.72 [0.58, 0.82] 0.57 [0.49, 0.63]BoW 1 0.81 [0.70, 0.88] 0.64 [0.57, 0.70]2 0.80 [0.69, 0.87] 0.64 [0.58, 0.70]3 0.79 [0.67, 0.86] 0.64 [0.58, 0.70]4 0.78 [0.66, 0.86] 0.65 [0.58, 0.70]5 0.77 [0.64, 0.85] 0.64 [0.58, 0.70]6 0.76 [0.63, 0.85] 0.65 [0.58, 0.70]7 0.75 [0.62, 0.84] 0.64 [0.58, 0.70]Syn G1,D0 0.81 [0.70, 0.88] 0.62 [0.55, 0.68]G2,D0 0.82 [0.72, 0.89] 0.55 [0.48, 0.62]G3,D0 0.81 [0.71, 0.88] 0.62 [0.56, 0.68]G1,D1 0.82 [0.72, 0.89] 0.62 [0.55, 0.68]G2,D1 0.82 [0.73, 0.89] 0.62 [0.55, 0.68]G3,D1 0.82 [0.72, 0.88] 0.62 [0.55, 0.68]CW+ 4; G1,D0 0.88 [0.81, 0.93] 0.66 [0.59, 0.71]Syn 4; G2,D0 0.87 [0.80, 0.92] 0.64 [0.57, 0.70]4; G3,D0 0.86 [0.77, 0.91] 0.63 [0.56, 0.69]4; G1,D1 0.83 [0.73, 0.89] 0.48 [0.40, 0.56]4; G2,D1 0.83 [0.73, 0.89] 0.49 [0.40, 0.56]4; G3,D1 0.82 [0.72, 0.89] 0.48 [0.40, 0.56]Table 1: Spearman correlation results for the various WordNet-basedmodels and distributional models.
CW=Context Windows, BoW=bagof words, Syn=syntactic vectors.
For Syn, the window size is actuallythe tree-depth for the governors and descendants.
For examples, G1indicates that the contexts include the parents and D2 indicates that boththe children and grandchildren make up the contexts.
The final groupingincludes both contextual windows (at width 4) and syntactic contexts inthe template vectors.
Max scores are bolded.1.
Replace each non-English word in the datasetwith its 5-best translations into English usingstate-of-the-art machine translation technology.2.
The vector corresponding to each Spanish wordis calculated by collecting features from all thecontexts of any of its translations.3.
Once the vectors are generated, the similaritiesare calculated in the same way as before.4 Experimental results4.1 Gold-standard datasetsWe have used two standard datasets.
The firstone, RG, consists of 65 pairs of words collected byRubenstein and Goodenough (1965), who had themjudged by 51 human subjects in a scale from 0.0 to4.0 according to their similarity, but ignoring anyother possible semantic relationships that might ap-pear between the terms.
The second dataset, Word-Sim3535 (Finkelstein et al, 2002) contains 353 wordpairs, each associated with an average of 13 to 16 hu-man judgements.
In this case, both similarity and re-5Available at http://www.cs.technion.ac.il/?gabr/resources/data/wordsim353/wordsim353.htmlContext RG terms and frequenciesll never forget the * on his face when grin,2,smile,10he had a giant * on his face and grin,3,smile,2room with a huge * on her face and grin,2,smile,6the state of every * will be updated every automobile,2,car,3repair or replace the * if it is stolen automobile,2,car,2located on the north * of the Bay of shore,14,coast,2areas on the eastern * of the Adriatic Sea shore,3,coast,2Thesaurus of Current English * The Oxford Pocket Thesaurus slave,3,boy,5,shore,3,string,2wizard,4,glass,4,crane,5,smile,5implement,5,oracle,2,lad,2food,3,car,2,madhouse,3,jewel,3asylum,4,tool,8,journey,6,etc.be understood that the * 10 may be designed crane,3,tool,3a fight between a * and a snake and bird,3,crane,5Table 2: Sample of context windows for the terms in the RG dataset.latedness are annotated without any distinction.
Sev-eral studies indicate that the human scores consis-tently have very high correlations with each other(Miller and Charles, 1991; Resnik, 1995), thus val-idating the use of these datasets for evaluating se-mantic similarity.For the cross-lingual evaluation, the two datasetswere modified by translating the second word ineach pair into Spanish.
Two humans translatedsimultaneously both datasets, with an inter-taggeragreement of 72% for RG and 84% for Word-Sim353.4.2 ResultsTable 1 shows the Spearman correlation obtained onthe RG and WordSim353 datasets, including the in-terval at 0.95 of confidence6.Overall the distributional context-window ap-proach performs best in the RG, reaching 0.89 corre-lation, and both WN30g and the combination of con-text windows and syntactic context perform best onWordSim353.
Note that the confidence intervals arequite large in both RG and WordSim353, and few ofthe pairwise differences are statistically significant.Regarding WordNet-based approaches, the use ofthe glosses and WordNet 3.0 (WN30g) yields thebest results in both datasets.
While MCR16 is closeto WN30g for the RG dataset, it lags well behindon WordSim353.
This discrepancy is further ana-lyzed is Section 5.3.
Note that the performance ofWordNet in the WordSim353 dataset suffers fromunknown words.
In fact, there are nine pairs whichreturned null similarity for this reason.
The num-6To calculate the Spearman correlations values are trans-formed into ranks, and we calculate the Pearson correlation onthem.
The confidence intervals refer to the Pearson correlationsof the rank vectors.22Figure 1: Effect of the size of the training corpus, for the best distributional similarity model in each dataset.
Left: WordSim353 with bag-of-words,Right: RG with context windows.Dataset Method overall ?
intervalRG MCR16 0.78 -0.05 [0.66, 0.86]WN30g 0.74 -0.09 [0.61, 0.84]Bag of words 0.68 -0.23 [0.53, 0.79]Context windows 0.83 -0.05 [0.73, 0.89]WS353 MCR16 0.42 (0.53) -0.11 (-0.03) [0.34, 0.51]WN30g 0.58 (0.67) -0.07 (-0.02) [0.51, 0.64]Bag of words 0.53 -0.12 [0.45, 0.61]Context windows 0.52 -0.11 [0.44, 0.59]Table 3: Results obtained by the different methods on the Span-ish/English cross-lingual datasets.
The ?
column shows the perfor-mance difference with respect to the results on the original dataset.ber in parenthesis in Table 1 for WordSim353 showsthe results for the 344 remaining pairs.
Section 5.2shows a proposal to overcome this limitation.The bag-of-words approach tends to group to-gether terms that can have a similar distribution ofcontextual terms.
Therefore, terms that are topicallyrelated can appear in the same textual passages andwill get high values using this model.
We see thisas an explanation why this model performed betterthan the context window approach for WordSim353,where annotators were instructed to provide highratings to related terms.
On the contrary, the con-text window approach tends to group together wordsthat are exchangeable in exactly the same context,preserving order.
Table 2 illustrates a few exam-ples of context collected.
Therefore, true synonymsand hyponyms/hyperonyms will receive high simi-larities, whereas terms related topically or based onany other semantic relation (e.g.
movie and star) willhave lower scores.
This explains why this methodperformed better for the RG dataset.
Section 5.3confirms these observations.4.3 Cross-lingual similarityTable 3 shows the results for the English-Spanishcross-lingual datasets.
For RG, MCR16 and thecontext windows methods drop only 5 percentagepoints, showing that cross-lingual similarity is feasi-ble, and that both cross-lingual strategies are robust.The results for WordSim353 show that WN30g isthe best for this dataset, with the rest of the meth-ods falling over 10 percentage points relative to themonolingual experiment.
A closer look at the Word-Net results showed that most of the drop in perfor-mance was caused by out-of-vocabulary words, dueto the smaller vocabulary of the Spanish WordNet.Though not totally comparable, if we compute thecorrelation over pairs covered in WordNet alne, thecorrelation would drop only 2 percentage points.
Inthe case of the distributional approaches, the fall inperformance was caused by the translations, as only61% of the words were translated into the originalword in the English datasets.5 Detailed analysis and systemcombinationIn this section we present some analysis, includinglearning curves for distributional methods, the useof distributional similarity to improve WordNet sim-ilarity, the contrast between similarity and related-ness, and the combination of methods.5.1 Learning curves for distributional methodsFigure 1 shows that the correlation improves withthe size of the corpus, as expected.
For the re-sults using the WordSim353 corpus, we show theresults of the bag-of-words approach with contextsize 10.
Results improve from 0.5 Spearman correla-tion up to 0.65 when increasing the corpus size threeorders of magnitude, although the effect decays atthe end, which indicates that we might not get fur-23Method Without similar words With similar wordsWN30 0.56 (0.58) [0.48, 0.63] 0.58 [0.51, 0.65]WN30g 0.66 (0.69) [0.59, 0.71] 0.68 [0.62, 0.73]Table 4: Results obtained replacing unknown words with their mostsimilar three words (WordSim353 dataset).Method overall Similarity RelatednessMCR16 0.53 [0.45, 0.60] 0.65 [0.56, 0.72] 0.33 [0.21, 0.43]WN30 0.56 [0.48, 0.63] 0.73 [0.65, 0.79] 0.38 [0.27, 0.48]WN30g 0.66 [0.59, 0.71] 0.72 [0.64, 0.78] 0.56 [0.46, 0.64]BoW 0.65 [0.59, 0.71] 0.70 [0.63, 0.77] 0.62 [0.53, 0.69]CW 0.60 [0.53, 0.66] 0.77 [0.71, 0.82] 0.46 [0.36, 0.55]Table 5: Results obtained on the WordSim353 dataset and on the twosimilarity and relatedness subsets.ther gains going beyond the current size of the cor-pus.
With respect to results for the RG dataset, weused a context-window approach with context radius4.
Here, results improve even more with data size,probably due to the sparse data problem collecting8-word context windows if the corpus is not largeenough.
Correlation improves linearly right to theend, where results stabilize around 0.89.5.2 Combining both approaches: dealing withunknown words in WordNetAlthough the vocabulary of WordNet is very ex-tensive, applications are bound to need the similar-ity between words which are not included in Word-Net.
This is exemplified in the WordSim353 dataset,where 9 pairs contain words which are unknown toWordNet.
In order to overcome this shortcoming,we could use similar words instead, as provided bythe distributional thesaurus.
We used the distribu-tional thesaurus defined in Section 3, using contextwindows of width 4, to provide three similar wordsfor each of the unknown words in WordNet.
Resultsimprove for both WN30 and WN30g, as shown inTable 4, attaining our best results for WordSim353.5.3 Similarity vs. relatednessWe mentioned above that the annotation guidelinesof WordSim353 did not distinguish between simi-lar and related pairs.
As the results in Section 4show, different techniques are more appropriate tocalculate either similarity or relatedness.
In order tostudy this effect, ideally, we would have two ver-sions of the dataset, where annotators were givenprecise instructions to distinguish similarity in onecase, and relatedness in the other.
Given the lackof such datasets, we devised a simpler approach inorder to reuse the existing human judgements.
Wemanually split the dataset in two parts, as follows.First, two humans classified all pairs as be-ing synonyms of each other, antonyms, iden-tical, hyperonym-hyponym, hyponym-hyperonym,holonym-meronym, meronym-holonym, and none-of-the-above.
The inter-tagger agreement rate was0.80, with a Kappa score of 0.77.
This anno-tation was used to group the pairs in three cate-gories: similar pairs (those classified as synonyms,antonyms, identical, or hyponym-hyperonym), re-lated pairs (those classified as meronym-holonym,and pairs classified as none-of-the-above, with a hu-man average similarity greater than 5), and unrelatedpairs (those classified as none-of-the-above that hadaverage similarity less than or equal to 5).
We thencreated two new gold-standard datasets: similarity(the union of similar and unrelated pairs), and relat-edness (the union of related and unrelated)7.Table 5 shows the results on the relatedness andsimilarity subsets of WordSim353 for the differentmethods.
Regarding WordNet methods, both WN30and WN30g perform similarly on the similarity sub-set, but WN30g obtains the best results by far onthe relatedness data.
These results are congruentwith our expectations: two words are similar if theirsynsets are in close places in the WordNet hierarchy,and two words are related if there is a connectionbetween them.
Most of the relations in WordNetare of hierarchical nature, and although other rela-tions exist, they are far less numerous, thus explain-ing the good results for both WN30 and WN30g onsimilarity, but the bad results of WN30 on related-ness.
The disambiguated glosses help find connec-tions among related concepts, and allow our methodto better model relatedness with respect to WN30.The low results for MCR16 also deserve somecomments.
Given the fact that MCR16 performedvery well on the RG dataset, it comes as a surprisethat it performs so poorly for the similarity subsetof WordSim353.
In an additional evaluation, we at-tested that MCR16 does indeed perform as well asMCR30g on the similar pairs subset.
We believethat this deviation could be due to the method used toconstruct the similarity dataset, which includes somepairs of loosely related pairs labeled as unrelated.7Available at http://alfonseca.org/eng/research/wordsim353.html24Methods combined in the SVM RG dataset WordSim353 dataset WordSim353 similarity WordSim353 relatednessWN30g, bag of words 0.88 [0.82, 0.93] 0.78 [0.73, 0.81] 0.81 [0.76, 0.86] 0.72 [0.65, 0.77]WN30g, context windows 0.90 [0.84, 0.94] 0.73 [0.68, 0.79] 0.83 [0.78, 0.87] 0.64 [0.56, 0.71]WN30g, syntax 0.89 [0.83, 0.93] 0.75 [0.70, 0.79] 0.83 [0.78, 0.87] 0.67 [0.60, 0.74]WN30g, bag of words, context windows, syntax 0.96 [0.93, 0.97] 0.78 [0.73, 0.82] 0.83 [0.78, 0.87] 0.71 [0.65, 0.77]Table 6: Results using a supervised combination of several systems.
Max values are bolded for each dataset.Concerning the techniques based on distributionalsimilarities, the method based on context windowsprovides the best results for similarity, and the bag-of-words representation outperforms most of theother techniques for relatedness.5.4 Supervised combinationIn order to gain an insight on which would be the up-per bound that we could obtain when combining ourmethods, we took the output of three systems (bagof words with window size 10, context window withsize 4, and the WN30g run).
Each of these outputs isa ranking of word pairs, and we implemented an or-acle that chooses, for each pair, the rank that is mostsimilar to the rank of the pair in the gold-standard.The outputs of the oracle have a Spearman correla-tion of 0.97 for RG and 0.92 for WordSim353, whichgives as an indication of the correlations that couldbe achieved by choosing for each pair the rank out-put by the best classifier for that pair.The previous results motivated the use of a su-pervised approach to combine the output of thedifferent systems.
We created a training cor-pus containing pairs of pairs of words from thedatasets, having as features the similarity and rankof each pair involved as given by the differ-ent unsupervised systems.
A classifier is trainedto decide whether the first pair is more simi-lar than the second one.
For example, a train-ing instance using two unsupervised classifiers is0.001364, 31, 0.327515, 64, 0.084805, 57, 0.109061, 59, negativemeaning that the similarities given by the first clas-sifier to the two pairs were 0.001364 and 0.327515respectively, which ranked them in positions 31 and64.
The second classifier gave them similarities of0.084805 and 0.109061 respectively, which rankedthem in positions 57 and 59.
The class negative in-dicates that in the gold-standard the first pair has alower score than the second pair.We have trained a SVM to classify pairs of pairs,and use its output to rank the entries in both datasets.It uses a polynomial kernel with degree 4.
We didMethod Source Spearman (MC) Pearson (MC)(Sahami et al, 2006) Web snippets 0.62 [0.32, 0.81] 0.58 [0.26, 0.78](Chen et al, 2006) Web snippets 0.69 [0.42, 0.84] 0.69 [0.42, 0.85](Wu and Palmer, 1994) WordNet 0.78 [0.59, 0.90] 0.78 [0.57, 0.89](Leacock et al, 1998) WordNet 0.79 [0.59, 0.90] 0.82 [0.64, 0.91](Resnik, 1995) WordNet 0.81 [0.62, 0.91] 0.80 [0.60, 0.90](Lin, 1998a) WordNet 0.82 [0.65, 0.91] 0.83 [0.67, 0.92](Bollegala et al, 2007) Web snippets 0.82 [0.64, 0.91] 0.83 [0.67, 0.92](Jiang and Conrath, 1997) WordNet 0.83 [0.67, 0.92] 0.85 [0.69, 0.93](Jarmasz, 2003) Roget?s 0.87 [0.73, 0.94] 0.87 [0.74, 0.94](Patwardhan et al, 2006) WordNet n/a 0.91(Alvarez and Lim, 2007) WordNet n/a 0.91(Yang and Powers, 2005) WordNet 0.87 [0.73, 0.91] 0.92 [0.84, 0.96](Hughes et al, 2007) WordNet 0.90 n/aPersonalized PageRank WordNet 0.89 [0.77, 0.94] n/aBag of words Web corpus 0.85 [0.70, 0.93] 0.84 [0.69, 0.93]Context window Web corpus 0.88 [0.76, 0.95] 0.89 [0.77, 0.95]Syntactic contexts Web corpus 0.76 [0.54, 0.88] 0.74 [0.51, 0.87]SVM Web, WN 0.92 [0.84, 0.96] 0.93 [0.85, 0.97]Table 7: Comparison with previous approaches for MC.not have a held-out set, so we used the standard set-tings of Weka, without trying to modify parameters,e.g.
C. Each word pair is scored with the numberof pairs that were considered to have less similar-ity using the SVM.
The results using 10-fold cross-validation are shown in Table 6.
A combination ofall methods produces the best results reported so farfor both datasets, statistically significant for RG.6 Related workContrary to the WordSim353 dataset, common prac-tice with the RG dataset has been to perform theevaluation with Pearson correlation.
In our believePearson is less informative, as the Pearson correla-tion suffers much when the scores of two systems arenot linearly correlated, something which happensoften given due to the different nature of the tech-niques applied.
Some authors, e.g.
Alvarez and Lim(2007), use a non-linear function to map the systemoutputs into new values distributed more similarlyto the values in the gold-standard.
In their case, themapping function was exp (?x4 ), which was chosenempirically.
Finding such a function is dependenton the dataset used, and involves an extra step in thesimilarity calculations.
Alternatively, the Spearmancorrelation provides an evaluation metric that is in-dependent of such data-dependent transformations.Most similarity researchers have published their25Word pair M&C SVM Word pair M&C SVMautomobile, car 3.92 62 crane, implement 1.68 26journey, voyage 3.84 54 brother, lad 1.66 39gem, jewel 3.84 61 car, journey 1.16 37boy, lad 3.76 57 monk, oracle 1.1 32coast, shore 3.7 53 food, rooster 0.89 3asylum, madhouse 3.61 45 coast, hill 0.87 34magician, wizard 3.5 49 forest, graveyard 0.84 27midday, noon 3.42 61 monk, slave 0.55 17furnace, stove 3.11 50 lad, wizard 0.42 13food, fruit 3.08 47 coast, forest 0.42 18bird, cock 3.05 46 cord, smile 0.13 5bird, crane 2.97 38 glass, magician 0.11 10implement, tool 2.95 55 rooster, voyage 0.08 1brother, monk 2.82 42 noon, string 0.08 5Table 8: Our best results for the MC dataset.Method Source Spearman(Strube and Ponzetto, 2006) Wikipedia 0.19?0.48(Jarmasz, 2003) WordNet 0.33?0.35(Jarmasz, 2003) Roget?s 0.55(Hughes and Ramage, 2007) WordNet 0.55(Finkelstein et al, 2002) Web corpus, WN 0.56(Gabrilovich and Markovitch, 2007) ODP 0.65(Gabrilovich and Markovitch, 2007) Wikipedia 0.75SVM Web corpus, WN 0.78Table 9: Comparison with previous work for WordSim353.complete results on a smaller subset of the RGdataset containing 30 word pairs (Miller andCharles, 1991), usually referred to as MC, making itpossible to compare different systems using differ-ent correlation.
Table 7 shows the results of relatedwork on MC that was available to us, including ourown.
For the authors that did not provide the de-tailed data we include only the Pearson correlationwith no confidence intervals.Among the unsupervised methods introduced inthis paper, the context window produced the best re-ported Spearman correlation, although the 0.95 con-fidence intervals are too large to allow us to acceptthe hypothesis that it is better than all others meth-ods.
The supervised combination produces the bestresults reported so far.
For the benefit of future re-search, our results for the MC subset are displayedin Table 8.Comparison on the WordSim353 dataset is eas-ier, as all researchers have used Spearman.
Thefigures in Table 9) show that our WordNet-basedmethod outperforms all previously published Word-Net methods.
We want to note that our WordNet-based method outperforms that of Hughes and Ram-age (2007), which uses a similar method.
Althoughthere are some differences in the method, we thinkthat the main performance gain comes from the useof the disambiguated glosses, which they did notuse.
Our distributional methods also outperform allother corpus-based methods.
The most similar ap-proach to our distributional technique is Finkelsteinet al (2002), who combined distributional similar-ities from Web documents with a similarity fromWordNet.
Their results are probably worse due tothe smaller data size (they used 270,000 documents)and the differences in the calculation of the simi-larities.
The only method which outperforms ournon-supervised methods is that of (Gabrilovich andMarkovitch, 2007) when based on Wikipedia, prob-ably because of the dense, manually distilled knowl-edge contained in Wikipedia.
All in all, our super-vised combination gets the best published results onthis dataset.7 Conclusions and future workThis paper has presented two state-of-the-art dis-tributional and WordNet-based similarity measures,with a study of several parameters, including per-formance on similarity and relatedness data.
Weshow that the use of disambiguated glosses allowsfor the best published results for WordNet-basedsystems on the WordSim353 dataset, mainly due tothe better modeling of relatedness (as opposed tosimilarity).
Distributional similarities have provento be competitive when compared to knowledge-based methods, with context windows being betterfor similarity and bag of words for relatedness.
Dis-tributional similarity was effectively used to coverout-of-vocabulary items in the WordNet-based mea-sure providing our best unsupervised results.
Thecomplementarity of our methods was exploited bya supervised learner, producing the best results sofar for RG and WordSim353.
Our results includeconfidence values, which, surprisingly, were not in-cluded in most previous work, and show that manyresults over RG and WordSim353 are indistinguish-able.
The algorithm for WordNet-base similarityand the necessary resources are publicly available8.This work pioneers cross-lingual extension andevaluation of both distributional and WordNet-basedmeasures.
We have shown that closely alignedwordnets provide a natural and effective way tocompute cross-lingual similarity with minor losses.A simple translation strategy also yields good resultsfor distributional methods.8http://ixa2.si.ehu.es/ukb/26ReferencesE.
Agirre and A. Soroa.
2009.
Personalizing pager-ank for word sense disambiguation.
In Proc.
of EACL2009, Athens, Greece.M.A.
Alvarez and S.J.
Lim.
2007.
A Graph Modelingof Semantic Similarity between Words.
Proc.
of theConference on Semantic Computing, pages 355?362.J.
Atserias, L. Villarejo, G. Rigau, E. Agirre, J. Carroll,B.
Magnini, and P. Vossen.
2004.
The meaning multi-lingual central repository.
In Proc.
of Global WordNetConference, Brno, Czech Republic.D.
Bollegala, Matsuo Y., and M. Ishizuka.
2007.
Mea-suring semantic similarity between words using websearch engines.
In Proceedings of WWW?2007.A.
Budanitsky and G. Hirst.
2006.
Evaluating WordNet-based Measures of Lexical Semantic Relatedness.Computational Linguistics, 32(1):13?47.H.
Chen, M. Lin, and Y. Wei.
2006.
Novel associationmeasures using web search with double checking.
InProceedings of COCLING/ACL 2006.J.
Daude, L. Padro, and G. Rigau.
2000.
Mapping Word-Nets using structural information.
In Proceedings ofACL?2000, Hong Kong.C.
Fellbaum, editor.
1998.
WordNet: An Electronic Lexi-cal Database and Some of its Applications.
MIT Press,Cambridge, Mass.L.
Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,Z.
Solan, G. Wolfman, and E. Ruppin.
2002.
Plac-ing Search in Context: The Concept Revisited.
ACMTransactions on Information Systems, 20(1):116?131.E.
Gabrilovich and S. Markovitch.
2007.
ComputingSemantic Relatedness using Wikipedia-based ExplicitSemantic Analysis.
Proc of IJCAI, pages 6?12.T.
H. Haveliwala.
2002.
Topic-sensitive pagerank.
InWWW ?02: Proceedings of the 11th international con-ference on World Wide Web, pages 517?526.T.
Hughes and D. Ramage.
2007.
Lexical semantic re-latedness with random graph walks.
In Proceedings ofEMNLP-CoNLL-2007, pages 581?589.M.
Jarmasz.
2003.
Roget?s Thesuarus as a lexical re-source for Natural Language Processing.J.J.
Jiang and D.W. Conrath.
1997.
Semantic similaritybased on corpus statistics and lexical taxonomy.
InProceedings of International Conference on Researchin Computational Linguistics, volume 33.
Taiwan.C.
Leacock and M. Chodorow.
1998.
Combining localcontext and WordNet similarity for word sense iden-tification.
WordNet: An Electronic Lexical Database,49(2):265?283.D.
Lin.
1998a.
An information-theoretic definition ofsimilarity.
In Proc.
of ICML, pages 296?304, Wiscon-sin, USA.D.
Lin.
1998b.
Automatic Retrieval and Clustering ofSimilar Words.
In Proceedings of ACL-98.G.A.
Miller and W.G.
Charles.
1991.
Contextual corre-lates of semantic similarity.
Language and CognitiveProcesses, 6(1):1?28.J.
Nivre.
2006.
Inductive Dependency Parsing, vol-ume 34 of Text, Speech and Language Technology.Springer.S.
Pado and M. Lapata.
2007.
Dependency-based con-struction of semantic space models.
ComputationalLinguistics, 33(2):161?199.S.
Patwardhan and T. Pedersen.
2006.
Using WordNet-based Context Vectors to Estimate the Semantic Re-latedness of Concepts.
In Proceedings of the EACLWorkshop on Making Sense of Sense: Bringing Com-putational Linguistics and Pycholinguistics Together,pages 1?8, Trento, Italy.P.
Resnik.
1995.
Using Information Content to EvaluateSemantic Similarity in a Taxonomy.
Proc.
of IJCAI,14:448?453.H.
Rubenstein and J.B. Goodenough.
1965.
Contextualcorrelates of synonymy.
Communications of the ACM,8(10):627?633.M Ruiz-Casado, E. Alfonseca, and P. Castells.
2005.Using context-window overlapping in Synonym Dis-covery and Ontology Extension.
In Proceedings ofRANLP-2005, Borovets, Bulgaria,.M.
Sahami and T.D.
Heilman.
2006.
A web-based ker-nel function for measuring the similarity of short textsnippets.
Proc.
of WWW, pages 377?386.M.
Strube and S.P.
Ponzetto.
2006.
WikiRelate!
Com-puting Semantic Relatedness Using Wikipedia.
InProceedings of the AAAI-2006, pages 1419?1424.P.D.
Turney.
2001.
Mining the Web for Synonyms: PMI-IR versus LSA on TOEFL.
Lecture Notes in ComputerScience, 2167:491?502.P.
Vossen, editor.
1998.
EuroWordNet: A MultilingualDatabase with Lexical Semantic Networks.
KluwerAcademic Publishers.Z.
Wu and M. Palmer.
1994.
Verb semantics and lex-ical selection.
In Proc.
of ACL, pages 133?138, LasCruces, New Mexico.D.
Yang and D.M.W.
Powers.
2005.
Measuring semanticsimilarity in the taxonomy of WordNet.
Proceedingsof the Australasian conference on Computer Science.27
