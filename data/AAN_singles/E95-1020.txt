Dis t r ibut iona l  Par t -o f -Speech  Tagg ingHinrich SchfitzeCSLI, Ventura HallStanford, CA 94305-4115 , USAemMl: schuetze~cs l i .
s tan ford .eduURL: ftp://csli.stanford.edu/pub/prosit/DisPosTag.psAbstractThis paper presents an algorithm for tag-ging words whose part-of-speech proper-ties are unknown.
Unlike previous work,the algorithm categorizes word tokens incon$ezt instead of word ~ypes.
The algo-rithm is evaluated on the Brown Corpus.1 IntroductionSince online text becomes available in ever increas-ing volumes and an ever increasing number of lan-guages, there is a growing need for robust pro-cessing techniques that can analyze text withoutexpensive and time-consuming adaptation to newdomains and genres.
This need motivates researchon fully automatic text processing that may relyon general principles of linguistics and computa-tion, but does not depend on knowledge aboutindividual words.In this paper, we describe an experiment onfully automatic derivation of the knowledge nec-essary for part-of-speech tagging.
Part-of-speechtagging is of interest for a number of applications,for example access to text data bases (Kupiec,1993), robust parsing (Abney, 1991), and generalparsing (deMarcken, 1990; Charniak et al, 1994).The goal is to find an unsupervised method fortagging that relies on general distributional prop-erties of text, properties that are invariant acrosslanguages and sublanguages.
While the proposedalgorithm is not successful for all grammatical cat-egories, it does show that fully automatic taggingis possible when demands on accuracy are modest.The following sections discuss related work, de-scribe the learning procedure and evaluate it onthe Brown Corpus (Francis and Ku~era, 1982).2 Related WorkThe simplest part-of-speech taggers are bigramor trigram models (Church, 1989; Charniak etal., 1993).
They require a relatively large taggedtraining text.
Transformation-based tagging asintroduced by Brill (1993) also requires a hand-tagged text for training.
No pretagged text is nec-essary for Hidden Markov Models (Jelinek, 1985;Cutting et al, 1991; Kupiec, 1992).
Still, a lexi-con is needed that specifies the possible parts ofspeech for every word.
Brill and Marcus (1992a)have shown that the effort necessary to constructthe part-of-speech lexicon can be considerably re-duced by combining learning procedures and apartial part-of-speech ategorization elicited froman informant.The present paper is concerned with tagginglanguages and sublanguages for which no a prioriknowledge about grammatical categories i avail-able, a situation that occurs often in practice(Brill and Marcus, 1992a).Several researchers have worked on learninggrammatical properties of words.
Elman (1990)trains a connectionist net to predict words, a pro-cess that generates internal representations thatreflect grammatical category.
Brill et al (1990)try to infer grammatical category from bi-gram statistics.
Finch and Chater (1992) andFinch (1993) use vector models in which words areclustered according to the similarity of their closeneighbors in a corpus.
Kneser and Ney (1993)present a probabilistic model for entropy maxi-mization that also relies on the immediate neigh-bors of words in a corpus.
Biber (1993) ap-plies factor analysis to collocations of two targetwords ("certain" and "right") with their immedi-ate neighbors.What these approaches have in common is thatthey classify words instead of individual occur-rences.
Given the widespread part-of-speech am-biguity of words this is problematicJ How shoulda word like "plant" be categorized if it has usesboth as a verb and as a noun?
How can a cate-gorization be considered meaningful if the infini-tive marker "to" is not distinguished from the ho-mophonous preposition?In a previous paper (Schfitze, 1993), we traineda neural network to disambiguate part-of-speech*Although Biber (1993) classifies collocations,these can also be ambiguous.
For example, "for cer-tain" has both senses of "certain": "particular" and"sure".141word side nearest neighborsonto leftonto rightseemed leftseemed rightinto toward away off together against beside around downreduce among regarding against owards plus toward using unlikeappeared might would remained had became could must shouldseem seems wanted want going meant ried expect likelyTable h Words with most similar left and right neighbors for "onto" and "seemed".using context; however, no information about theword that is to be categorized was used.
Thisscheme fails for cases like "The soldiers rarelycome home."
vs. "The soldiers will come home.
"where the context is identical and informationabout the lexical item in question ("rarely" vs."will") is needed in combination with context forcorrect classification.
In this paper, we will com-pare two tagging algorithms, one based on clas-sifying word types, and one based on classifyingwords-plus-context.3 Tag inductionWe start by constructing representations of thesyntactic behavior of a word with respect to itsleft and right context.
Our working hypothe-sis is that syntactic behavior is reflected in co-occurrence patterns.
Therefore, we will measurethe similarity between two words with respect totheir syntactic behavior to, say, their left side bythe degree to which they share the same neighborson the left.
If the counts of neighbors are assem-bled into a vector (with one dimension for eachneighbor), the cosine can be employed to measuresimilarity.
It will assign a value close to 1.0 if twowords share many neighbors, and 0.0 if they sharenone.
We refer to the vector of left neighbors ofa word as its left contezt vector, and to the vec-tor of right neighbors as its right contezt vector.The unreduced context vectors in the experimentdescribed here have 250 entries, corresponding tothe 250 most frequent words in the Brown corpus.This basic idea of measuring distributional sim-ilarity in terms of shared neighbors must be mod-ified because of the sparseness of the data.
Con-sider two infrequent adjectives that happen tomodify different nouns in the corpus.
Their rightsimilarity according to the cosine measure wouldbe zero.
This is clearly undesirable.
But even withhigh-frequency words, the simple vector model canyield misleading similarity measurements.
A casein point is "a" vs. "an".
These two articles do notshare any right neighbors ince the former is onlyused before consonants and the latter only beforevowels.
Yet intuitively, they are similar with re-spect to their right syntactic context despite thelack of common right neighbors.Our solution to these problems is the applica-tion of a singular value decomposition.
We canrepresent he left vectors of all words in the cor-pus as a matrix C with n rows, one for each wordwhose left neighbors are to be represented, and kcolumns, one for each of the possible neighbors.SVD can be used to approximate the row and col-umn vectors of C in a low-dimensional space.
Inmore detail, SVD decomposes a matrix C, the ma-trix of left vectors in our case, into three matricesTo, So, and Do such that:C = ToSoD'  oSo is a diagonal k-by-k matrix that contains thesingular values of C in descending order.
The ithsingular value can be interpreted as indicating thestrength of the ith principal component of C. Toand Do are orthonormal matrices that approxi-mate the rows and columns of C, respectively.
Byrestricting the matrices To, So, and Do to theirfirst m < k columns (= principal components)one obtains the matrices T, S, and D. Their prod-uct C is the best least square approximation of Cby a matrix of rank m: C = TSD'.
We chosem = 50 (reduction to a 50-dimensional space) forthe SVD's described in this paper.SVD addresses the problems of generalizationand sparseness because broad and stable general-izations are represented on dimensions with largevalues which will be retained in the dimensionalityreduction.
In contrast, dimensions correspondingto small singular values represent idiosyncrasies,like the phonological constraint on the usage of"an" vs. "a", and will be dropped.
We also gainefficiency since we can manipulate smaller vectors,reduced to 50 dimensions.
We used SVDPACKto compute the singular value decompositions de-scribed in this paper (Berry, 1992).Table 1 shows the nearest neighbors of twowords (ordered according to closeness to the headword) after the dimensionality reduction.
Neigh-bors with highest similarity according to bothleft and right context are listed.
One can seeclear differences between the nearest neighbors inthe two spaces.
The right-context neighbors of"onto" contain verbs because both prepositionsand verbs govern noun phrases to their right.The left-context neighborhood of "onto" reflectsthe fact that prepositional phrases are used inthe same position as adverbs like "away" and"together", thus making their left context sim-ilar.
For "seemed", left-context neighbors arewords that have similar types of noun phrases insubject position (mainly auxiliaries).
The right-context neighbors all take "to"-infinitives as com-plements.
An adjective like "likely" is very sim-142ilar to "seemed" in this respect although its leftcontext is quite different from that of "seemed".Similarly, the generalization that prepositions andtransitive verbs are very similar if not identicalin the way they govern noun phrases would belost if "left" and "right" properties of words werelumped together in one representation.
These ex-amples demonstrate the importance of represent-ing generalizations about left and right contextseparately.The left and right context vectors are the basisfor four different ag induction experiments, whichare described in detail below:?
induction based on word type only?
induction based on word type and context?
induction based on word type and context,restricted to "natural" contexts?
induction based on word type and context,using generalized left and right context vec-tors3.1 Induct ion  based on word type  on lyThe two context vectors of a word characterize thedistribution of neighboring words to its left an.dright.
The concatenation of left and right contextvector can therefore serve as a representation f aword's distributional behavior (Finch and Chater,1992; Sch/itze, 1993).
We formed such concate-nated vectors for all 47,025 words (surface forms)in the Brown corpus.
Here, we use the raw 250-dimensional context vectors and apply the SVDto the 47,025-by-500 matrix (47,025 words withtwo 250-dimensional context vectors each).
Weobtained 47,025 50-dimensional reduced vectorsfrom the SVD and clustered them into 200 classesusing the fast clustering algorithm Buckshot (Cut-ting et al, 1992) (group average agglomeration ap-plied to a sample).
This classification constitutesthe baseline performance for distributional part-of-speech tagging.
All occurrences of a word areassigned to one class.
As pointed out above, sucha procedure is problematic for ambiguous words.3.2 Induct ion  based on word type  andcontextIn order to exploit contextual information in theclassification of a token, we simply use contextvectors of the two words occurring next to thetoken.
An occurrence of word w is represented bya concatenation of four context vectors:?
The right context vector of the precedingword.?
The left context vector of w.?
The right context vector of w.?
The left context vector of the following word.The motivation is that a word's syntactic roledepends both on the syntactic properties of itsneighbors and on its own potential for enteringinto syntactic relationships with these neighbors.The only properties of context that we considerare the right-context vector of the preceding wordand the left-context vector of the following wordbecause they seem to represent the contextual in-formation most important for the categorizationof w. For example, for the disambiguation of"work" in "her work seemed to be important",only the fact that "seemed" expects noun phrasesto its left is important, the right context vector of"seemed" does not contribute to disambiguation.That only the immediate neighbors are crucial forcategorization is clearly a simplification, but asthe results presented below show it seems to worksurprisingly well.Again, an SVD is applied to address the prob-lems of sparseness and generalization.
We ran-domly selected 20,000 word triplets from the cor-pus and formed concatenations of four contextvectors as described above.
The singular value de-composition of the resulting 20,000-by-l,000 ma-trix defines a mapping from the 1,000-dimensionalspace of concatenated context vectors to a 50-dimensional reduced space.
Our tag set was theninduced by clustering the reduced vectors of the20,000 selected occurrences into 200 classes.
Eachof the 200 tags is defined by the centroid of the cor-responding class (the sum of its members).
Dis-tributional tagging of an occurrence of a wordw proceeds then by retrieving the four relevantcontext vectors (right context vector of previousword, left context vector of following word, bothcontext vectors of w) concatenating them to one1000-component vector, mapping this vector to 50dimensions, computing the correlations with the200 cluster centroids and, finally, assigning the oc-currence to the closest cluster.
This procedure wasapplied to all tokens of the Brown corpus.We will see below that this method of distribu-tional tagging, although partially successful, failsfor many tokens whose neighbors are punctuationmarks.
The context vectors of punctuation markscontribute little information about syntactic ate-gorization since there are no grammatical depen-dencies between words and punctuation marks, incontrast to strong dependencies between neigh-boring words.For this reason, a second induction on the ba-sis of word type and context was performed, butonly for those tokens with informative contexts.Tokens next to punctuation marks and tokenswith rare words as neighbors were not included.Contexts with rare words (less than ten occur-rences) were also excluded for similar reasons: Ifa word only occurs nine or fewer times its leftand right context vectors capture little informa-tion for syntactic categorization.
In the experi-ment, 20,000 natural contexts were randomly se-lected, processed by the SVD and clustered into143tagADNCCCDDTININGMDNdescriptionadnominal modifierconjunctioncardinaldeterminerpreposition"-ing" formsmodalnominalTable 2: Evaluation tagPenn Treebank tagsADN* $CCCDDT PDT PRP$INVBGMDNNP(S) NN(S)tagPOSPRPRBTOVBVBDVBNWDTdescription Penn Treebank tagspossessive marker POSpronoun PRPadverbial RB RP RBR RBSinfinitive marker TOinfinitive VBinflected verb form VBD VBZ VBPpredicative VBN PRD ?wh-word WP($) WRB WDTset.
Structural tags derived from parse trees are marked with ..200 classes.
The classification was then applied toall natural contexts of the Brown corpus.3.3 Genera l i zed  context  vectorsThe context vectors used so far only capture infor-mation about distributional interactions with the250 most frequent words.
Intuitively, it should bepossible to gain accuracy in tag induction by us-ing information from more words.
One way to dothis is to let the right context vector record whichclasses of left conte~t vectors occur to the right ofa word.
The rationale is that words with similarleft context characterize words to their right in asimilar way.
For example, "seemed" and "would"have similar left contexts, and they characterizethe right contexts of "he" and "the firefighter"as potentially containing an inflected verb form.Rather than having separate ntries in its rightcontext vector for "seemed", "would", and "likes",a word like "he" can now be characterized by ageneralized entry for "inflected verb form occursfrequently to my right".This proposal was implemented by applying asingular value decomposition to the 47025-by-250matrix of left context vectors and clustering theresulting context vectors into 250 classes.
A gen-eralized right context vector v for word w wasthen formed by counting how often words fromthese 250 classes occurred to the right of w. En-try vi counts the number of times that a wordfrom class i occurs to the right of w in the cor-pus (as opposed to the number of times that theword with frequency rank i occurs to the right ofw).
Generalized left context vectors were derivedby an analogous procedure using word-based rightcontext vectors.
Note that the information aboutleft and right is kept separate in this computation.This differs from previous approaches (Finch andChater, 1992; Schfitze, 1993) in which left andright context vectors of a word are always usedin one concatenated vector.
There are arguablyfewer different types of right syntactic contextsthan types of syntactic categories.
For example,transitive verbs and prepositions belong to differ-ent syntactic categories, but their right contextsare virtually identical in that they require a nounphrase.
This generalization could not be exploitedif left and right context were not treated sepa-rately.Another argument for the two-step derivationis that many words don't have any of the 250most frequent words as their left or right neighbor.Hence, their vector would be zero in the word-based scheme.
The class-based scheme makes itmore likely that meaningful representations areformed for all words in the vocabulary.The generalized context vectors were input tothe tag induction procedure described above forword-based context vectors: 20,000 word tripletswere selected from the corpus, encoded as 1,000-dimensional vectors (consisting of four generalizedcontext vectors), decomposed by a singular valuedecomposition and clustered into 200 classes.
Theresulting classification was applied to all tokens inthe Brown corpus.4 Resul tsThe results of the four experiments were evalu-ated by forming 16 classes of tags from the PennTreebank as shown in Table 2.
Preliminary ex-periments howed that distributional methods dis-tinguish adnominal and predicative uses of adjec-tives (e.g.
"the black cat" vs. "the cat is black").Therefore the tag "ADN" was introduced for usesof adjectives, nouns, and participles as adnominalmodifiers.
The tag "PRD" stands for predicativeuses of adjectives.
The Penn Treebank parses ofthe Brown corpus were used to determine whethera token functions as an adnominal modifier.
Punc-tuation marks, special symbols, interjections, for-eign words and tags with fewer than 100 instanceswere excluded from the evaluation.Tables 3 and 4 present results for word type-based induction and induction based on word typeand context.
For each tag t, the table lists thefrequency of t in the corpus ("frequency") 2, thenumber of induced tags i0, il, ?
?., iz, that were as-signed to it ( "# classes"); the number of times anoccurrence of t was correctly labeled as belong-ing to one of i0, Q , .
.
.
, i z  ("correct"); the num-ber of times that a token of a different tag t ~ was2The small difference in overall frequency in thetables is due to the fact that some word-based contextvectors consist entirely of zeros.
There were about ahundred word triplets whose four context vectors didnot have non-zero entries and could not be assigned acluster.144tag J~ frequency108586CC 36808CD 15085DT 129626IN 132079ING 14753MD 13498N 231434POS 5086PRP 47686RB 54525TO 25196VB 35342VBD 80058VBN 41146WDT 14093avg.# classes }correct04 \[ 33762 \[ 1255403 \[1187265 \[ 21112 \[ 1338398 \[ 1938381 \[ 46413 \[ 4383908 I 2913812 I 366530incorrect precision19528 0.660 0.001431 0.7031783 0.8075829 0.611016 0.6813016 0.5179652 0.711213 0.7921723 0.6756505 0.380 0.0017945 0.623855 0.908841 0.470 0.000.530T--?
35 m\[ 0.00\[ 0.22\[ 0.97\[ 0.90\[ 0.14\[ 0.99\[ 0.84\[ 0.91I 0.92\[ 0.65\[ 0.00\[ 0.82\[ 0.46\[ 0.19j 0.52Table 3: Precision and recall for induction based on word type.F0.460.000.340.870.730.240.670.770.850.770.480.000.710.610.270.000.49miscategorized asbeing an instance of i0, il, .
.
.
,  il("incorrect"); and precision and recall of the cate-gorization of t. Precision is the number of correcttokens divided by the sum of correct and incorrecttokens.
Recall is the number of correct tokens di-vided by the total number of tokens of t (in thefirst column).
The last column gives van Rijs-bergen's F measure which computes an aggregatescore from precision and recall: (van Rijsbergen,1 1979) F = ~-~+(1-~)~" We chose c~ = 0.5 to giveequal weight to precision and recall.It is clear from the tables that incorporatingcontext improves performance considerably.
TheF score increases for all tags except CD, with anaverage improvement of more than 0.20.
The tagCD is probably better thought of as describing aword class.
There is a wide range of heterogeneoussyntactic functions of cardinals in particular con-texts: quantificational nd adnominal uses, bareNP's ("is one of"), dates and ages ("Jan 1", "gavehis age as 25"), and enumerations.
In this light, itis not surprising that the word-type method doesbetter on cardinals.Table 5 shows that performance for generalizedcontext vectors is better than for word-based con-text vectors (0.74 vs. 0.72).
However, since thenumber of tags with better and worse performanceis about the same (7 and 5), one cannot con-clude with certainty that generalized context vec-tors induce tags of higher quality.
Apparently, the250 most frequent words capture most of the rel-evant distributional information so that the addi-tional information from less frequent words avail-able from generalized vectors only has a small ef-fect.Table 6 looks at results for "natural" contexts,i.e.
those not containing punctuation marks andrare words.
Performance is consistently betterthan for the evaluation on all contexts, indicatingthat the low quality of the distributional informa-tion about punctuation marks and rare words is adifficulty for successful tag induction.Even for "natural" contexts, performance variesconsiderably.
It is fairly good for prepositions, de-terminers, pronouns, conjunctions, the infinitivemarker, modals, and the possessive marker.
Taginduction fails for cardinals (for the reasons men-tioned above) and for "-ing" forms.
Present par-ticiples and gerunds are difficult because they ex-hibit both verbal and nominal properties and oc-cur in a wide variety of different contexts whereasother parts of speech have a few typical and fre-quent contexts.It may seem worrying that some of the tags areassigned a high number of clusters (e.g., 49 forN, 36 for ADN).
A closer look reveals that manyclusters embody finer distinctions.
Some exam-pies: Nouns in cluster 0 are heads of larger nounphrases, whereas the nouns in cluster 1 are full-fledged NPs.
The members of classes 29 and 111function as subjects.
Class 49 consists of propernouns.
However, there are many pairs or triplesof clusters that should be collapsed into one onlinguistic grounds.
They were separated on distri-butional criteria that don't have linguistic corre-lates.An analysis of the divergence between our clas-sification and the manually assigned tags revealedthree main sources of errors: rare words and raresyntactic phenomena, indistinguishable distribu-tion, and non-local dependencies.Rare words are difficult because of lack of dis-tributional evidence.
For example, "ties" is usedas a verb only 2 times (out of 15 occurrences inthe corpus).
Both occurrences are miscategorized,since its context vectors do not provide enoughevidence for the verbal use.
Rare syntactic con-structions pose a related problem: There are notenough instances to justify the creation of a sepa-rate cluster.
For example, verbs taking bare in-145recal l  ~CCCDDTININGMDNPOSPRPRBTOVBVBDVBNWDTavg.108532368081508412962613207914753134982314245086476865452425196353428005841145140934221611426827161817112tag frequency ~ classes precision24743 0.781501 0.95809 0.486178 0.9525316 0.834876 0.39936 0.9351695 0.80533 0.9012759 0.7817403 0.6461 1.006152 0.838663 0.8811972 0.681017 0.610.78F0.790.860.090.940.890.270.950.850.900.850.600.960.830.840.650.190.72Table 4: Precision and recall for induct ion based on word type and context.tagADNCCCDDTININGMDNPOSPRPRBTOVBVBDVBNWDTavg.~equency10858636808150851296261320791475313498231434508647686545242519635342800584114514093classes504310823702591715101~ incor rect3707 I120968 I123516 I3798 I13175 I201890 I4932 \]37535 \]29892 I25181 I28879 I66457 I26960 Iprecision26790 0.776430 0.841530 0.715780 0.9522070 0.857161 0.351059 0.9333206 0.861636 0.759221 0.8018398 0.6227 1.006560 0.8112079 0.8517356 0.61563 0.800.78~ F0 ?
8-------~0.880.360.940.890.300.950.870.850.790.581.00 I 1.000.82 I 0.820.83 I 0.840.66 \[ 0.630.26o.73 I0.74Table 5: Precision and recall for induct ion based on general ized context vectors.tagADN 63771CC 16148CD 7011DT 87914IN 91950ING 7268MD 11244N 111368POS 3202PRP 23946RB 32331TO 19859VB 26714VBD 56540VBN 24804WDT 8329avg.frequency ~ classes3641992349i71621133143_ _ ~  incorrect12203179891826646842141247614452precision ~ _ _0.82 ~--0-~-~ - -0.90 I 0.970.67 I 0.260.97 I 0.94093 I 0940.47 I 0.170.96 I 0.920.87 I 0.90I 0.91I 0.96I 0.65I o .98I 0.90\] 0.90I 0.76Io .78255 0.924062 0.859922 0.6853 1.004119 0.858488 0.867448 0.72670 0.850.83Table 6:  Precision and recall for induct ion for natura l  contexts.F0.830.930.380.950.940.250.940.890.910.900.660.990.880.880.740.580.79146finitives were classified as adverbs since this istoo rare a phenomenon to provide strong distri-butional evidence ("we do not DARE speak of","legislation could HELP remove").The case of the tags "VBN" and "PRD" (pastparticiples and predicative adjectives) demon-strates the difficulties of word classes with indis-tinguishable distributions.
There are hardly anydistributional clues for distinguishing "VBN" and"PRD" since both are mainly used as comple-ments of "to be".s A common tag class was cre-ated for "VBN" and "PRD" to show that theyare reasonably well distinguished from other partsof speech, even if not from each other.
Semanticunderstanding is necessary to distinguish betweenthe states described by phrases of the form "to beadjective" and the processes described by phrasesof the form "to be past participle".Finally, the method fails if there are no localdependencies that could be used for categoriza-tion and only non-local dependencies are informa-tive.
For example, the adverb in "Mc*N. Hester,CURRENTLY Dean o f .
.
. "
and the conjunctionin "to add that, IF United States policies .
.
.
"have similar immediate neighbors (comma, NP).The decision to consider only immediate neighborsis responsible for this type of error since takinga wider context into account would disambiguatethe parts of speech in question.5 Future WorkThere are three avenues of future research we areinterested in pursuing.
First, we are planning toapply the algorithm to an as yet untagged lan-guage.
Languages with a rich morphology maybe more difficult han English since with fewer to-kens per type, there is less data on which to basea categorization decision.Secondly, the error analysis uggests that con-sidering non-local dependencies would improve re-sults.
Categories that can be induced well (thosecharacterized by local dependencies) could be in-put into procedures that learn phrase structure(e.g.
(Brill and Marcus, 19925; Finch, 1993)).These phrase constraints could then be incorpo-rated into the distributional tagger to characterizenon-local dependencies.Finally, our procedure induces a "hard" part-of-speech classification ofoccurrences incontext, i.e.,each occurrence is assigned to only one category.It is by no means generally accepted that sucha classification is linguistically adequate.
Thereis both synchronic (Ross, 1972) and diachronic(Tabor, 1994) evidence suggesting that words andtheir uses can inherit properties from several pro-totypical syntactic ategories.
For example, "fun"SBecause of phrases like "I had sweet potatoes",forms of "have" cannot serve as a reliable discrimina-tor either.in "It's a fun thing to do."
has properties of both anoun and an adjective (superlative "funnest" pos-sible).
We are planning to explore "soft" classifi-cation algorithms that can account for these phe-nomena.6 Conclus ionIn this paper, we have attempted to construct analgorithm for fully automatic distributional tag-ging, using unannotated corpora s the sole sourceof information.
The main innovation is that thealgorithm is able to deal with part-of-speech am-biguity, a pervasive phenomenon i natural an-guage that was unaccounted for in previous workon learning categories from corpora.
The methodwas systematically evaluated on the Brown cor-pus.
Even if no automatic procedure can rival theaccuracy of human tagging, we hope that the al-gorithm will facilitate the initial tagging of textsin new languages and sublanguages.7 AcknowledgmentsI am grateful for helpful comments o Steve Finch,Jan Pedersen and two anonymous reviewers (fromACL and EACL).
I'm also indebted to MichaelBerry for SVDPACK and to the Penn TreebankProject for the parsed Brown corpus.ReferencesSteven Abney.
1991.
Parsing by chunks.
InBerwick, Abney, and Tenny, editors, Principle-Based Parsing.
Kluwer Academic Publishers.Michael W. Berry.
1992.
Large-scale sparsesingular value computations.
The Interna-tional Journal of Supercomputer Applications,6(1):13-49.Douglas Biber.
1993.
Co-occurrence patternsamong collocations: A tool for corpus-basedlexical knowledge acquisition.
ComputationalLinguistics, 19(3):531-538.Eric Brill and Mitch Marcus.
1992a.
Taggingan unfamiliar text with minimal human super-vision.
In Robert Goldman, editor, WorkingNotes of the AAAI Fall Symposium on Proba-bilistic Approaches to Natural Language.
AAAIPress.Eric Brill and Mitchell Marcus.
1992b.
Au-tomatically acquiring phrase structure usingdistributional nalysis.
In Proceedings of theDARPA workshop "Speech and Natural Lan-guage", pages 155-159.Eric Brill, David Magerman, Mitch Marcus, andBeatrice Santorini.
1990.
Deducing linguisticstructure from the statistics of large corpora.
InProceedings of the DARPA Speech and NaturalLanguage Workshop, pages 275-282.147Eric Brill.
1993.
Automatic grammar inductionand parsing free text: A transformation-basedapproach.
In Proceedings of ACL 31, ColumbusOH.Eugene Charniak, Curtis Hendrickson, Neil Ja-cobson, and Mike Perkowitz.
1993.
Equationsfor part-of-speech tagging.
In Proceedings of theEleventh National Conference on Artificial In-telligence, pages 784-789.Eugene Charniak, Glenn Carroll, John Adcock,Anthony Cassandra, Yoshihiko Gotoh, JeremyKatz, Michael Littman, and John McCann.1994.
Tatters for parsers.
Technical ReportCS-94-06, Brown University.Kenneth W. Church.
1989.
A stochastic partsprogram and noun phrase parser for unre-stricted text.
In Proceedings of ICASSP-S9,Glasgow, Scotland.Doug Cutting, Julian Kupiec, Jan Pedersen, andPenelope Sibun.
1991.
A practical part-of-speech tagger.
In The 3rd Conference onApplied Natural Language Processing, Trento,Italy.Douglas R. Cutting, Jan O.
"Pedersen, DavidKarger, and John W. Tukey.
1992.
Scat-ter/gather: A cluster-based approach to brows-ing large document collections.
In Proceedingsof SIGIR 'g2, pages 318-329.C.
G. deMarcken.
1990.
Parsing the LOB corpus.In Proceedings of the 28th Annual Meeting ofthe Association for Computational Linguistics,pages 243-259.Jeffrey L. Elman.
1990.
Finding structure in time.Cognitive Science, 14:179-211.Steven Finch and Nick Chater.
1992.
Bootstrap-ping syntactic ategories using statistical meth-ods.
In Walter Daelemans and David Powers,editors, Background and Ezperiments in Ma-chine Learning of Natural Language, pages 229-235, Tilburg University.
Institute for LanguageTechnology and AI.Steven Paul Finch.
1993.
Finding Structure inLanguage.
Ph.D. thesis, University of Edin-burgh.W.N.
Francis and F. Kufiera.
1982.
FrequencyAnalysis of English Usage.
Houghton Mifflin,Boston.F.
Jelinek.
1985.
Robust part-of-speech taggingusing a hidden markov model.
Technical report,IBM, T.J. Watson Research Center.Reinhard Kneser and I-Iermann Ney.
1993.
Form-ing word classes by statistical clustering for sta-tistical anguage modelling.
In Reinhard KShlerand Burghard B. Rieger, editors, Contribu-tions to Quantitative Linguistics, pages 221-226.
Kluwer Academic Publishers, Dordrecht,The Netherlands.Julian Kupiec.
1992.
Robust part-of-speech tag-ging using a hidden markov model.
ComputerSpeech and Language, 6:225-242.Julian Kupiec.
1993.
Murax: A robust linguisticapproach for question answering using an on-line encyclopedia.
In Proceedings of SIGIR '93,pages 181-190.John R. Ross.
1972.
The category squish: End-station Hauptwort.
In Papers from the EighthRegional Meeting.
Chicago Linguistic Society.Hinrich Schfitze.
1993.
Part-of-speech inductionfrom scratch.
In Proceedings of ACL 31, pages251-258, Columbus OH.Whitney Tabor.
1994.
Syntactic Innovation: AConnectionist Model.
Ph.D. thesis, StanfordUniversity.C.
J. van Rijsbergen.
1979.
Information Re-trieval Butterworths, London.
Second Edition.148
