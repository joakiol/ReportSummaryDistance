Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 23?30,Suntec, Singapore, 6 August 2009.c?2009 ACL and AFNLPEvaluation of automatic summaries: Metrics under varying dataconditionsKarolina Owczarzak and Hoa Trang DangInformation Access DivisionNational Institute of Standards and TechnologyGaithersburg, MD 20899karolina.owczarzak@nist.gov hoa.dang@nist.govAbstractIn evaluation of automatic summaries, itis necessary to employ multiple topics andhuman-produced models in order for theassessment to be stable and reliable.
How-ever, providing multiple topics and modelsis costly and time-consuming.
This paperexamines the relation between the numberof available models and topics and the cor-relations with human judgment obtainedby automatic metrics ROUGE and BE, aswell as the manual Pyramid method.
Test-ing all these methods on the same data set,taken from the TAC 2008 Summarizationtrack, allows us to compare and contrastthe methods under different conditions.1 IntroductionAppropriate evaluation of results is an importantaspect of any research.
In areas such as automaticsummarization, the problem is especially complexbecause of the inherent subjectivity in the task it-self and its evaluation.
There is no single objectivestandard for a good quality summary; rather, itsvalue depends on the summary?s purpose, focus,and particular requirements of the reader (Sp?arckJones, 2007).
While the purpose and focus canbe set as constant for a specific task, the variabil-ity of human judgment is more difficult to con-trol.
Therefore, in attempts to produce stable eval-uations, it has become standard to use multiplejudges, not necessarily for parallel evaluation, butin such a way that each judge evaluates a differ-ent subset of the many summaries on which thefinal system assessment is based.
The incorpora-tion of multiple points of view is also reflected inautomatic evaluation, where it takes the form ofemploying multiple model summaries to which acandidate summary is compared.Since these measures to neutralize judgmentvariation involve the production of multiple modelsummaries, as well as multiple topics, evaluationcan become quite costly.
Therefore, it is inter-esting to examine how many models and topicsare necessary to obtain a relatively stable eval-uation, and whether this number is different formanual and automatic metrics.
In their exami-nation of summary evaluations, van Halteren andTeufel (2003) suggest that it is necessary to useat least 30 to 40 model summaries for a stableevaluation; however, Harman and Over (2004) ar-gue that a stable evaluation can be conducted evenwith a single model, as long as there is an ade-quate number of topics.
This view is supported byLin (2004a), who concludes that ?correlations tohuman judgments were increased by using multi-ple references but using single reference summarywith enough number of samples was a valid al-ternative?.
Interestingly, similar conclusions werealso reached in the area of Machine Translationevaluation; in their experiments, Zhang and Vogel(2004) show that adding an additional referencetranslation compensates the effects of removing10?15% of the testing data, and state that, there-fore, ?it seems more cost effective to have moretest sentences but fewer reference translations?.In this paper, we look at how various metricsbehave with respect to a variable number of top-ics and models used in the evaluation.
This lets usdetermine the stability of individual metrics, andhelps to illuminate the trade-offs inherent in de-signing a good evaluation.
For our experiments,we used data from the Summarization track at theText Analysis Conference (TAC) 2008, where par-ticipating systems were assessed on their summa-rization of 48 topics, and the automatic metricsROUGE and BE, as well as the manual Pyramidevaluation method, had access to 4 human mod-els.
TAC 2008 was the first task of the TAC/DUC(Document Understanding Conference) series inwhich the Pyramid method was used on all evalu-ated data, making it possible to conduct a full com-23parison among the manual and automatic meth-ods.
Despite the lack of full Pyramid evaluationin DUC 2007, we look at the remaining metricsapplied that year (ROUGE, BE, and Content Re-sponsiveness), in order to see whether they con-firm the insights gained from the TAC 2008 data.2 Summary evaluationThe main evaluation at TAC 2008 was performedmanually, assessing the automatic candidate sum-maries with respect to Overall Responsiveness,Overall Readability, and content coverage accord-ing to the Pyramid framework (Nenkova and Pas-sonneau, 2004; Passonneau et al, 2005).
Task par-ticipants were asked to produce two summaries foreach of the 48 topics; the first (initial summary)was a straightforward summary of 10 documentsin response to a topic statement, which is a requestfor information about a subject or event; the sec-ond was an update summary, generated on the ba-sis of another set of 10 documents, which followedthe first set in temporal order and described furtherdevelopments in the given topic.
The idea behindthe update summary was to avoid repeating all theinformation included in the first set of documents,on the assumption that the reader is familiar withthat information already.The participating teams submitted up to threeruns each; however, only the first and secondruns were evaluated manually due to limited re-sources.
For each summary under evaluation, as-sessors rated the summary from 1 (very poor) to5 (very good) in terms of Overall Responsiveness,which measures how well the summary respondsto the need for information expressed in the topicstatement and whether its linguistic quality is ad-equate.
Linguistic qualities such as grammatical-ity, coreference, and focus were also evaluated asOverall Readability, also on the scale from 1 to5.
Content coverage of each summary was evalu-ated using the Pyramid framework, where asses-sors create a list of information nuggets (calledSummary Content Units, or SCUs) from the set ofhuman-produced summaries on a given topic, thendecide whether any of these nuggets are present inthe candidate summary.
All submitted runs wereevaluated with the automatic metrics: ROUGE(Lin, 2004b), which calculates the proportion ofn-grams shared between the candidate summaryand the reference summaries, and Basic Elements(Hovy et al, 2005), which compares the candidateto the models in terms of head-modifier pairs.2.1 Manual metricsEvaluating Overall Responsiveness and OverallReadability is a rather straightforward procedure,as most of the complex work is done in the mindof the human assessor.
Each candidate summaryis given a single score, and the final score forthe summarization system is the average of all itssummary-level scores.
The only economic factorhere is the number of topics, i.e.
summaries persystem, that need to be judged in order to neutral-ize both intra- and inter-annotator variability andobtain a reliable assessment of the summarizationsystem.When it comes to the Pyramid method, whichmeasures content coverage of candidate sum-maries, the need for multiple topics is accompa-nied by the need for multiple human model sum-maries.
First, independent human assessors pro-duce summaries for each topic, guided by the topicstatement.
Next, in the Pyramid creation stage,an assessor reads all human-produced summariesfor a given topic and extracts all ?informationnuggets?, called Summary Content Units (SCUs),which are short, atomic statements of facts con-tained in the text.
Each SCU has a weight whichis directly proportional to the number of modelsummaries in which it appears, on the assumptionthat the fact?s importance is reflected in how manyhuman summarizers decide to include it as rele-vant in their summary.
Once all SCUs have beenharvested from the model summaries, an assessorthen examines each candidate summary to see howmany of the SCUs from the list it contains.
The fi-nal Pyramid score for a candidate summary is itstotal SCU weight divided by the maximum SCUweight available to a summary of average length(where the average length is determined by themean SCU count of the model summaries for thistopic).
The final score for a summarization systemis the average score of all its summaries.
In TAC2008, the evaluation was conducted with 48 topicsand 4 human models for each topic.We examined to what extent the number ofmodels and topics used in the evaluation can in-fluence the Pyramid score and its stability.
Thestability, similarly to the method employed byVoorhees and Buckley (2002) for Information Re-trieval, is determined by how well a system rank-ing based on a small number of models/topics cor-24Models Pyramid ROUGE-2 ROUGE-SU4 BE1 0.8839 0.8032 0.7842 0.76802 0.8943 0.8200 0.7957 0.79833 0.8974* 0.8258 0.7999* 0.80984 (bootstr) 0.8972* 0.8310 0.8023* 0.81524 (actual) 0.8997 0.8302 0.8033 0.8171Table 1: Mean correlations of Responsiveness and other met-rics using 1, 2, 3, or 4 models for TAC 2008 initial summaries.Values in each row are significantly different from each other at95% level.Models Pyramid ROUGE-2 ROUGE-SU4 BE1 0.9315 0.8861 0.8874 0.87162 0.9432 0.9013 0.8961 0.89783 0.9474* 0.9068* 0.8994 0.90764 (bootstr) 0.9481* 0.9079* 0.9023 0.91144 (actual) 0.9492 0.9103 0.9020 0.9132Table 2: Mean correlations of Responsiveness and other met-rics using 1, 2, 3, or 4 models for TAC 2008 update summaries.Values in each row are significantly different from each otherat 95% level except ROUGE-2 and ROUGE-SU4 in 1-modelcategory.Models ROUGE-2 ROUGE-SU4 BE1 0.8789 0.8671 0.85532 0.8972 0.8803 0.89173 0.9036 0.8845 0.90484 (bootstr) 0.9082 0.8874 0.91074 (actual) 0.9077 0.8877 0.9123Table 3: Mean correlations of 4-model Pyramid score andother metrics using 1, 2, 3, or 4 models for TAC 2008 initialsummaries.
Values in each row are significantly different fromeach other at 95% level except ROUGE-2 and BE in 4-modelcategory.Models ROUGE-2 ROUGE-SU4 BE1 0.9179 0.9110 0.90162 0.9336 0.9199 0.92843 0.9392 0.9233 0.93834 (bootstr) 0.9443 0.9277 0.94364 (actual) 0.9429 0.9263 0.9446Table 4: Mean correlations of 4-model Pyramid score andother metrics using 1, 2, 3, or 4 models for TAC 2008 updatesummaries.
Values in each row are significantly different fromeach other at 95% level except ROUGE-2 and BE in 4-modelcategory.relates with the ranking based on another set ofmodels/topics, where the two sets are randomlyselected and mutually exclusive.
This methodol-ogy allows us to check the correlations based onup to half of the actual number of models/topicsonly (because of the non-overlap requirement), butit gives an indication of the general tendency.
Wealso look at the correlation between the Pyramidscore and Overall Responsiveness.
We don?t ex-pect a perfect correlation between Pyramid andResponsiveness in the best of times, because Pyra-mid measures content identity between the can-didate and the model, and Responsiveness mea-sures content relevance to topic as well as linguis-tic quality.
However, the degree of variation be-tween the two scores depending on the number ofmodels/topics used for the Pyramid will give usa certain indication of the amount of informationlost.2.2 Automatic metricsSimilarly to the Pyramid method, ROUGE (Lin,2004b) and Basic Elements (Hovy et al, 2005)require multiple topics and model summaries toproduce optimal results.
ROUGE is a collectionof automatic n-gram matching metrics, rangingfrom unigram to four-gram.
It also includes mea-surements of the longest common subsequence,weighted or unweighted, and the option to com-pare stemmed versions of words and omit stop-words.
There is also the possibility of accept-ing skip-n-grams, that is, counting n-grams asmatching even if there are some intervening non-matching words.
The skip-n-grams together withstemming are the only ways ROUGE can acco-modate alternative forms of expression and matchconcepts even though they might differ in terms oftheir syntactic or lexical form.These methods are necessarily limited, and soROUGE relies on using multiple parallel modelsummaries which serve as a source of lexi-cal/syntactic variation in the comparison process.The fewer models there are, the less reliable thescore.
Our question here is not only what this rela-tion looks like (as it was examined on the basis ofDocument Understanding Conference data in Lin(2004a)), but also how it compares to the reliabil-ity of other metrics.Basic Elements (BE), on the other hand, goesbeyond simple string matching and parses the syn-tactic structure of the candidate and model to ob-tain a set of head-modifier pairs for each, and thencompares the sets.
A head-modifier pair consist ofthe head of a syntactic unit (e.g.
the noun in a nounphrase), and the word which modifes the head (i.e.a determiner in a noun phrase).
It is also possibleto include the name of the relation which connectsthem (i.e.
subject, object, etc.).
Since BEs reflectthematic relations in a sentence rather than surfaceword order, it should be possible to accommodatecertain differences of expression that might appearbetween a candidate summary and a reference, es-pecially as the words can be stemmed.
This could,in theory, allow us to use fewer models for theevaluation.
In practice, however, it fails to accountfor the total possible variety, and, what is more,25the additional step of parsing the text can intro-duce noise into the comparison.TAC 2008 and DUC 2007 evaluations usedROUGE-2 and ROUGE-SU4, which refer to therecall of bigram and skip-bigram (with up to 4 in-tervening words) matches on stemmed words, re-spectively, as well as a BE score calculated on thebasis of stemmed head-modifier pairs without re-lation labels.
Therefore, these are the versions weuse in our comparisons.3 Number of modelsSince Responsiveness score does not depend onthe number of models, it serves as a referenceagainst which we compare the remaining metrics,while we calculate their score with only 1, 2, 3, orall 4 models.
Given 48 topics in TAC 2008, and4-model summaries for each topic, there are 448possible combinations to derive the final score inthe single-model category, so to keep the experi-ments simple we only selected 1000 random sam-ples from that space.
For 1000 repetitions, eachtime we selected a random combination of modelsummaries (only one model out of 4 available pertopic), against which we evaluated the candidatesummaries.
Then, for each of the 1000 samples,we calculated the correlation between the result-ing score and Responsiveness.
We then took the1000 correlations produced in this manner, andcomputed their mean.
In the same way, we cal-culated the scores based on 2 and 3 model sum-maries, randomly selected from the 4 available foreach topic.
The correlation means for all metricsand categories are given in Table 1 for initial sum-maries and Table 2 for update summaries.
We alsoran a one-way analysis of variance (ANOVA) onthese correlations to determine whether the cor-relation means were significantly different fromeach other.
For the 4-model category there wasonly one possible sample for each metric, so in or-der to perform ANOVA we bootstrapped this sam-ple to produce 1000 samples.
The actual value ofthe 4-model correlation is given in the tables as 4(actual), and the mean value of the bootstrapped1000 correlations is given as 4 (bootstr).Values for initial summaries are significantlydifferent from their counterparts for update sum-maries at the 95% level.
Pairwise testing of valuesfor statistically significant differences is shownwith symbols: in each column, the first valuemarked with a particular symbol is not signifi-cantly different from any subsequent value markedwith the same symbol.We also examined the correlations of the met-rics with the 4-model Pyramid score.
Table 3presents the correlation means for the initial sum-maries, and Table 4 shows the correlation meansfor the update summaries.Since the Pyramid, contrary to Responsiveness,makes use of multiple model summaries, we ex-amine its stability given a decreased number ofmodels to rely on.
For this purpose, we correlatedthe Pyramid score based on randomly selected 2models (half of the model pool) for each topic withthe score based on the remaining 2 models, andrepeated this 1000 times.
We also looked at the1-model category, where the Pyramid score cal-culated on the basis of one model per topic wascorrelated with the Pyramid score calculated onthe basis on another randomly selected model.
Inboth case we witness a very high mean correlation:0.994 and 0.995 for the 2-model category, 0.982and 0.985 for the 1-model category for TAC initialand update summaries, respectively.
As an illus-tration, Figure1 shows the variance of correlationsfor the initial summaries.Figure 1: Correlations between Pyramid scores based on 1or 2 model summaries for TAC 2008 initial summaries.The variation in correlation levels betweenother metrics and Pyramid and Responsiveness,presented in Tables 3?4, is more visible in thegraph form.
Figures 2-3 illustrate the meancorrelation values for TAC 2008 initial sum-maries.
While all the metrics record the steep-est increase in correlation values with the addi-tion of the second model, adding the third andfourth model provides the metrics with smallerbut steady improvement, with the exception ofPyramid-Responsiveness correlation in Figure 2.The increase in correlation mean is most dramaticfor BE, which in all cases starts as the lowest-26correlating metric in the single-model category,but by the 4-model point it outperforms one orboth versions of ROUGE.
The Pyramid metricachieves significantly higher correlations than anyother metric, independent of the number of mod-els, which is perhaps unsurprising given that it is amanual evaluation method.
Of the two ROUGEversions, ROUGE-2 seems consistently a betterpredictor of both Responsiveness and the ?full?
4-model Pyramid score than ROUGE-SU4.Figure 2: Responsiveness vs. other metrics with 1, 2, 3, or4 models for TAC 2008 initial summaries.Figure 3: 4-model Pyramid vs. other metrics with 1, 2, 3,or 4 models for TAC 2008 initial summaries.Similar patterns appear in DUC 2007 data (Ta-ble 5), despite the fact that the Overall Respon-siveness of TAC 2008 is replaced with Content Re-sponsiveness (ignoring linguistic quality), againstwhich we calculate all the correlations.
Althoughthe increase in correlation means from 1- to 4-models for the three automatic metrics is smallerthan for TAC 2008, the clearest rise occurs withthe addition of a second model, especially for BE,and the subsequent additions change little.
As inthe case of initial summaries 2008, ROUGE-2 out-performs the remaining two metrics independentlyof the number of models.
However, most of the in-creases are too small to be significant.This comparison suggests diminishing returnsModels ROUGE-2 ROUGE-SU4 BE1 0.8681 0.8254 0.84862 0.8747* 0.8291* 0.8577*3 0.8766*?
0.8299*?
0.8599*4 (bootstr) 0.8761*?
0.8305*?
0.86334 (actual) 0.8795 0.8301 0.8609Table 5: Mean correlations of Content Responsiveness andother metrics using 1, 2, 3, or 4 models for DUC 2007 sum-maries.
Values in each row are significantly different fromeach other at 95% level.with the addition of more models, as well as dif-ferent reactions among the metrics to the presenceor absence of additional models.
When correlatingwith Responsiveness, the manual Pyramid metricbenefits very little from the fourth model, but au-tomatic BE benefits most from almost every addi-tion.
ROUGE is situated somewhere between thetwo, noting small but often significant increases.On the whole, the use of multiple models (at leasttwo) seems supported, especially if we use auto-matic metrics in our evaluation.4 Number of topicsFor the second set of experiments we kept all fourmodels, but varied the number of topics whichwent into the final average system score.
To deter-mine the stability of Responsiveness and Pyramidwe looked at the correlations between the scoresbased on smaller sets of topics.
For 1000 rep-etitions, we calculated Pyramid/Responsivenessscore based on a set of 1, 3, 6, 12, or 24 topics ran-domly chosen from the pool of 48, and comparedthe system ranking thus created with the rankingbased on another, equally sized set, such that thesets did not contain common topics.
Table 6 showsthe mean correlation for each case.
Although suchcomparison was only possible up to 24 topics (halfof the whole available topic pool), the numberssuggest that at the level of 48 topics both Respon-siveness and Pyramid are stable enough to serve asreference for the automatic metrics.Responsiveness PyramidTopics Initial Update Initial Update1 0.182 0.196 0.333 0.2673 0.405 0.404 0.439 0.5206 0.581 0.586 0.608 0.69012 0.738 0.738 0.761 0.81624 0.849 0.866 0.851 0.901Table 6: Mean correlations between Responsive-ness/Pyramid scores based on 1, 3, 6, 12, and 24 topic sam-ples for TAC 2008 initial and update summaries.In a process which mirrored that described inSection 3, we created 1000 random samples ineach of the n-topics category: 1, 3, 6, 12, 24, 36,27Topics Pyramid ROUGE-2 ROUGE-SU4 BE1 0.4219 0.4276 0.4375 0.35063 0.6204 0.5980 0.9016 0.51086 0.7274 0.6901 0.6836 0.623312 0.8159 0.7618 0.7456 0.711724 0.8679 0.8040 0.7809 0.776236 0.8890* 0.8208* 0.7951* 0.8017*39 0.8927*?
0.8231*?
0.7967*?
0.8063*?42 0.8954*??
0.8258*??
0.7958*??
0.8102*?
?45 0.8977*???
0.8274*???
0.8008*???
0.8132??
?48 (bootstr) 0.8972*???
0.8302*???
0.8046???
0.8138??
?48 (actual) 0.8997 0.8302 0.8033 0.8171Table 7: Mean correlations of 48 topic Responsiveness andother metrics using from 1 to 48 topics for TAC 2008 initialsummaries.
Values in each row are significantly different fromeach other at 95% level except: ROUGE-2, ROUGE-SU4 andBE in 1-topic category, ROUGE-2 and ROUGE-SU4 in 3- and6-topic category.Topics Pyramid ROUGE-2 ROUGE-SU4 BE1 0.5005 0.4882 0.5609 0.40113 0.7053 0.6862 0.7340 0.60976 0.8080 0.7850 0.8114 0.727412 0.8812 0.8498 0.8596 0.818824 0.9250 0.8882 0.8859 0.877436 0.9408* 0.9023* 0.8960* 0.8999*39 0.9433*?
0.9045*?
0.8973*?
0.9037*?42 0.9455*??
0.9061*??
0.8987*??
0.9068*?
?45 0.9474???
0.9078*???
0.8996*???
0.9094??
?48 (bootstr) 0.9481???
0.9101???
0.9015*???
0.9111??
?48 (actual) 0.9492 0.9103 0.9020 0.9132Table 8: Mean correlations of 48 topic Responsiveness andother metrics using from 1 to 48 topics for TAC 2008 updatesummaries.
Values in each row are significantly different fromeach other at 95% level except: Pyramid and ROUGE-2 in 1-topic category, Pyramid and ROUGE-SU4 in 6-topic category,ROUGE-2 and BE in 39-, 42-, and 48-topic category.Topics ROUGE-2 ROUGE-SU4 BE1 0.4693 0.4856 0.38883 0.6575 0.6684 0.57326 0.7577 0.7584 0.696012 0.8332 0.8245 0.793824 0.8805 0.8642 0.868436 0.8980* 0.8792* 0.8966*39 0.9008*?
0.8812*?
0.9017*?42 0.9033*??
0.8839*??
0.9058?
?45 0.9052*???
0.8853*???
0.9093??
?48 (bootstr) 0.9074???
0.8877???
0.9107??
?48 (actual) 0.9077 0.8877 0.9123Table 9: Mean correlations of 48 topic Pyramid score andother metrics using from 1 to 48 topics for TAC 2008 initialsummaries.
Values in each row are significantly different fromeach other at 95% level except: ROUGE-2 and ROUGE-SU4in the 6-topic category, ROUGE-2 and BE in 39- and 48-topiccategory.Topics ROUGE-2 ROUGE-SU4 BE1 0.5026 0.5729 0.40943 0.7106 0.7532 0.62766 0.8130 0.8335 0.751212 0.8806 0.8834 0.847524 0.9196 0.9092 0.906336 0.9343* 0.9198* 0.9301*39 0.9367*?
0.9213*?
0.9341*?42 0.9386*??
0.9227*??
0.9376*?
?45 0.9402*???
0.9236*???
0.9402??
?48 (bootstr) 0.9430???
0.9280?
0.9444?
?48 (actual) 0.9429 0.9263 0.9446Table 10: Mean correlations of 48 topic Pyramid score andother metrics using from 1 to 48 topics for TAC 2008 updatesummaries.
Values in each row are significantly different fromeach other at 95% level except: ROUGE-2 and ROUGE-SU4in 12-topic category, ROUGE-2 and BE in 45-topic category.39, 42, or 45.
Within each of these categories, fora thousand repetitions, we calculated the score forautomatic summarizers by averaging over n topicsrandomly selected from the pool of 48 topics avail-able in the evaluation.
Again, we examined thecorrelations between the metrics and the ?full?
48-topic Responsiveness and Pyramid.
As previously,we then used ANOVA to determine whether thecorrelation means differed significantly.
Becausethere was only one possible sample with all 48topics for each metric, we bootstrapped this sam-ple to provide 1000 new samples in the 48-topiccategory, in order to perfom the ANOVA compari-son of variance.
Tables 7 and 8, as well as Figures4 and 5, show the metrics?
changing correlationswith Responsiveness.
Tables 9 and 10, and Fig-ures 6 and 7, show the correlations with the 48-topic Pyramid score.
Values for initial summariesare significantly different from their counterpartsfor update summaries at the 95% level.In all cases, it becomes clear that the curves flat-ten out and the correlations stop increasing almostcompletely beyond the 36-topic mark.
This meansthat the scores for the automatic summarizationsystems based on 36 topics will be on averagepractically indistiguishable from the scores basedon all 48 topics, showing that beyond a certainminimally necessary number of topics adding orremoving a few (or even ten) topics will not influ-ence the system scores much.
(However, we can-not conclude that a further considerable increase inthe number of topics ?
well beyond 48 ?
would notbring more improvement in the correlations, per-haps increasing the stable ?correlation window?
aswell.
)Topics ROUGE-2 ROUGE-SU4 BE1 0.6157 0.6378 0.57563 0.7597 0.7511 0.73236 0.8168 0.7904 0.795712 0.8493 0.8123 0.830624 0.8690 0.8249* 0.8517*36 0.8751* 0.8287*?
0.8580*?39 0.8761*?
0.8295*??
0.8592?
?42 0.8768*??
0.8299*???
0.8602??
?45 (bootstr) 0.8761*??
0.8305???
0.8627??
?45 (actual) 0.8795 0.8301 0.8609Table 11: Mean correlations of 45 topic Content Respon-siveness and other metrics using from 1 to 45 topics for DUC2007 summaries.
Values in each row are significantly differ-ent from each other at 95% level.An interesting observation is that if we pro-duce such limited-topic scores for the manualmetrics, Responsiveness and Pyramid, and corre-late them with their own ?full?
versions based on28Figure 4: Responsiveness vs. other metrics with 1 to 48 topicsfor TAC 2008 initial summaries.Figure 5: Responsiveness vs. other metrics with 1 to 48 topicsfor TAC 2008 update summaries.Figure 6: 48-topic Pyramid vs. other metrics with 1 to 48topics for TAC 2008 initial summaries.Figure 7: 48-topic Pyramid vs. other metrics with 1 to 48topics for TAC 2008 update summaries.all 48 topics, it appears that they are less stablethan the automatic metrics, i.e.
there is a largergap between the worst and best correlations theyachieve.1The mean correlation between the ?full?Responsiveness and that based on 1 topic is 0.443and 0.448 for the initial and update summaries, re-spectively; for that based on 3 topics, 0.664 and0.667.
Pyramid based on 1 topic achieves 0.467for initial and 0.525 for update summaries; Pyra-mid based on 3 topics obtains 0.690 and 0.742,respectively.
Some of these values, especiallyfor update summaries, are even lower than thoseobtained by ROUGE in the same category, de-spite the fact that 1- and 3-topic Responsiveness orPyramid is a proper subset of the 48-topic Respon-siveness/Pyramid.
On the other hand, ROUGEachieves considerably worse correlations with Re-sponsiveness than Pyramid when there are manytopics available.
ROUGE-SU4 seems to be morestable than ROUGE-2; in all cases ROUGE-2starts with lower correlations than ROUGE-SU4,but by the 12-topic mark its correlations increase1For reasons of space, these values are not included in thetables, as they offer little insight besides what is mentionedhere.above it.Additionally, despite being an automatic metric,BE seems to follow the same pattern as the manualmetrics.
It is seriously affected by the decreasingnumber of topics; in fact, if the number of topicsdrops below 24, BE is the least reliable indicatorof either Responsiveness or Pyramid.
However,by the 48-topic mark it rises to levels comparablewith ROUGE-2.As in the case of models, DUC 2007 data showsmostly the same pattern as TAC 2008.
Again, inthis data set, the increase in the correlation meanwith the addition of topics for each metric aresmaller than for either initial or update summariesin TAC 2008, but the relative rate of increase re-mains the same: BE gains most from additionaltopics (+0.28 in DUC vs. +0.47 and +0.51 inTAC), ROUGE-SU4 again shows the smallest in-crease (+0.19 in DUC vs. +0.36 and +0.34 inTAC), which means it is the most stable of the met-rics across the variable number of topics.22The smaller total increase might be due to the smallernumber of available topics (45 in DUC vs. 48 in TAC), butwe have seen the same effect in Section 3 while discussingmodels, so it might just be an accidental property of a givendata set.295 Discussion and conclusionsAs the popularity of shared tasks increases, taskorganizers face an ever growing problem of pro-viding an adequate evaluation to all participatingteams.
Often, evaluation of multiple runs from thesame team is required, as a way to foster researchand development.
With more and more systemsubmissions to judge, and the simultaneous needfor multiple topics and models in order to providea stable assessment, difficult decisions of cuttingcosts and effort might sometimes be necessary.
Itwould be useful then to know where such deci-sions will have the smallest negative impact, or atleast, what might be the trade-offs inherent in suchdecisions.From our experiments, it appears that manualmetrics such as Pyramid gain less from the addi-tion of more model summaries than the automaticmetrics.
A Pyramid score based on any two mod-els correlates very highly with the score based onany other two models.
For the automatic metrics,the largest gain is recorded with adding the sec-ond model; afterwards the returns diminish.
BEseems to be the most sensitive metric to changes inthe number of models and topics; ROUGE-SU4,on the other hand, is the least sensitive to suchchanges and the most stable, but it does not ob-tain the highest correlations when many modelsand topics are available.Whatever the number of models, manual Pyra-mid considerably outperforms automatic metrics,as can be expected, since human understanding isnot hampered by the possible differences in sur-face expression between a candidate and a model.But when it comes to decreased number of topics,the inherent variability of human judgment showsstrongly, to the extent that, in extreme cases ofvery few topics, it might be more prudent to useROUGE-SU4 than Pyramid or Responsiveness.Lastly, we observe that, as with models, addingone or two topics to the evaluation plays a greatrole only if we have very few topics to start with.Our experiments suggest that, as the number oftopics available for evaluation increases, so doesthe number of additional topics necessary to makea difference in the system ranking produced bya metric.
It seems that in the case of evaluationbased on 48 topics, as in the TAC Summarizationtrack, it would be possible to decrease the numberto about 36 without sacrificing much stability.ReferencesDonna Harman and Paul Over.
2004.
The effects ofhuman variation in DUC summarization evaluation.In Proceedings of the ACL-04 Workshop: Text Sum-marization Branches Out, pages 10?17, Barcelona,Spain.Eduard Hovy, Chin-Yew Lin, and Liang Zhou.
2005.Evaluating DUC 2005 using Basic Elements.
InProceedings of the 5th Document UnderstandingConference (DUC).Chin-Yew Lin.
2004a.
Looking for a few good met-rics: Automatic summarization evaluation - howmany samples are enough?
In Proceedings of NT-CIR Workshop 4, Tokyo, Japan.Chin-Yew Lin.
2004b.
ROUGE: A package for au-tomatic evaluation of summaries.
In Proceedingsof the ACL 2004 Workshop: Text SummarizationBranches Out, pages 74?81.Ani Nenkova and Rebecca J. Passonneau.
2004.Evaluating content selection in summarization: ThePyramid method.
In Proceedings of the Human Lan-guage Technology Conference of the North Ameri-can Chapter of the Association for ComputationalLinguistics, pages 145?152, Boston, MA.Rebecca J. Passonneau, Ani Nenkova, Kathleen McK-eown, and Sergey Sigelman.
2005.
Applyingthe Pyramid method in DUC 2005.
In Proceed-ings of the 5th Document Understanding Conference(DUC), Vancouver, Canada.Karen Sp?arck Jones.
2007.
Automatic summarising:The state of the art.
Information Processing andManagement, 43(6):1449?1481.Hans van Halteren and Simone Teufel.
2003.
Examin-ing the consensus between human summaries: Ini-tial experiments with factoid analysis.
In Proceed-ings of the HLT-NAACL DUCWorkshop 2003, pages57?64, Edmonton, Canada.Ellen M. Voorhees and Chris Buckley.
2002.
Effect oftopic set size on retrieval experiment error.
In Pro-ceedings of the 25th Annual International ACM SI-GIR conference on Research and Development in In-formation Retrieval, pages 317?323, Tampere, Fin-land.Ying Zhang and Stephan Vogel.
2004.
Measuringconfidence intervals for the Machine Translationevaluation metrics.
In Proceedings of the 10th Con-ference on Theoretical and Methodological Issues inMachine Translation, pages 85?94, Baltimore, MD.30
