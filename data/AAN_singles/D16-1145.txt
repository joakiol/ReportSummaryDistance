Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1379?1388,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsMining Inference Formulas by Goal-Directed Random WalksZhuoyu Wei1,2, Jun Zhao1,2 and Kang Liu11 National Laboratory of Pattern Recognition, Institute of Automation,Chinese Academy of Sciences, Beijing, 100190, China2 University of Chinese Academy of Sciences, Beijing, 100049, China{zhuoyu.wei, jzhao, kliu}@nlpr.ia.ac.cnAbstractDeep inference on a large-scale knowledgebase (KB) needs a mass of formulas, but it isalmost impossible to create all formulas man-ually.
Data-driven methods have been pro-posed to mine formulas from KBs automat-ically, where random sampling and approx-imate calculation are common techniques tohandle big data.
Among a series of method-s, Random Walk is believed to be suitable forknowledge graph data.
However, a pure ran-dom walk without goals still has a poor ef-ficiency of mining useful formulas, and evenintroduces lots of noise which may mislead in-ference.
Although several heuristic rules havebeen proposed to direct random walks, theydo not work well due to the diversity of for-mulas.
To this end, we propose a novel goal-directed inference formula mining algorithm,which directs random walks by the specificinference target at each step.
The algorithmis more inclined to visit benefic structures toinfer the target, so it can increase efficiencyof random walks and avoid noise simultane-ously.
The experiments on both WordNet andFreebase prove that our approach is has a highefficiency and performs best on the task.1 IntroductionRecently, various knowledge bases (KBs), such asFreebase (Bollacker et al, 2008), WordNet (Miller,1995), Yago (Hoffart et al, 2013), have been built,and researchers begin to explore how to make use ofstructural information to promote performances ofseveral inference-based NLP applications, such astext entailment, knowledge base completion, ques-tion and answering.
Creating useful formulas is oneof the most important steps in inference, and an ac-curate and high coverage formula set will bring agreat promotion for an inference system.
For ex-ample, Nationality(x, y) ?
Nationality(z, y) ?
Lan-guage(z, w)?
Language(x, w) is a high-quality for-mula, which means people with the same nationalityprobably speak the same language.
However, it is achallenge to create formulas for open-domain KBs,where there are a great variety of relation types andit is impossible to construct a complete formula setby hand.Several data-driven methods, such as Induc-tive Logic Programming (ILP) (Muggleton andDe Raedt, 1994) and Markov Logic Network (MLN)(Richardson and Domingos, 2006), have been pro-posed to mine formulas automatically from KB da-ta, which transform frequent sub-structures of KBs,e.g., paths or loops, into formulas.
Figure 1.a showsa sub-graph extracted from Freebase, and the for-mula mentioned above about Language can be gen-erated from the loop in Figure 1.d.
However, therunning time of these traditional probabilistic infer-ence methods is unbearable over large-scale KBs.For example, MLN needs grounding for each can-didate formula, i.e., it needs to enumerate all paths.Therefore, the computation complexity of MLN in-creases exponentially with the scale of a KB.In order to handle large-scale KBs, the randomwalk is usually employed to replace enumerating al-l possible sub-structures.
However, random walk isinefficient to find useful structures due to its com-pletely randomized mechanism.
For example in Fig-1379Figure 1: a) shows a subgraph extracted from Freebase.
b) shows the searching space of finding the yellow path.
c) shows a loopwhich can generate a false formula.
d) shows a loop which can generate a true formula.ure 1.b, the target path (yellow one) has a smallprobability to be visited, the reason is that the algo-rithm may select all the neighboring entity to trans-fer with an equal probability.
This phenomenon isvery common in KBs, e.g., each entity in Freebasehas more than 30 neighbors in average, so there willbe about 810,000 paths with length 4, and only sev-eral are useful.
There have been two types of meth-ods proposed to improve the efficiency of randomwalks, but they still meet serious problems, respec-tively.1) Increasing rounds of random walks.
Morerounds of random walks will find more structures,but it will simultaneously introduce more noise andthus generate more false formulas.
For example, theloop in Figure 1.c exists in Freebase, but it producesa false formula, Gender(x, y) ?
Gender(z, y) ?
Lan-guage(z, w)?
Language(x, w), which means peoplewith the same gender speak the same language.
Thiskind of structures frequently occur in KBs even theformulas are mined with a high confidence, becausethere are a lot of sparse structures in KBs which willlead to inaccurate confidence.
According to our s-tatistics, more than 90 percent of high-confidenceformulas produced by random walk are noise.2) Employing heuristic rules to direct randomwalks.
This method directs random walks to finduseful structures by rewriting the state transitionprobability matrix, but the artificial heuristic rulesmay only apply to a little part of formulas.
Forexample, PRA (Lao and Cohen, 2010; Lao et al,2011) assumes the more narrow distributions of el-ements in a formula are, the higher score the for-mula will obtain.
However, formulas with high s-cores in PRA are not always true.
For example,the formula in Figure 1.c has a high score in PRA,but it is not true.
Oppositely, formulas with lowscores in PRA are not always useless.
For exam-ple, the formula, Father(x, y) ?
Father(y, z) ?Grandfather(x, t), has a low score when x and yboth have several sons, but it obviously is the mosteffective to infer Grandfather.
According to ourinvestigations, the situations are common in KBs.In this paper, we propose a Goal-directed Ran-dom Walk algorithm to resolve the above problem-s.
The algorithm employs the specific inference tar-get as the direction at each step in the random walkprocess.
In more detail, to achieve such a goal-directed mechanism, at each step of random walk,the algorithm dynamically estimates the potentialsfor each neighbor by using the ultimate goal, and as-signs higher probabilities to the neighbors with high-er potentials.
Therefore, the algorithm is more in-clined to visit structures which are beneficial to infer1380the target and avoid transferring to noise structures.For example in Figure 1, when the inference tar-get is what language a person speaks, the algorith-m is more inclined to walk along Nationality edgethan Gender, because Nationality has greater poten-tial than Gender to infer Language.
We build a re-al potential function based on low-rank distribution-al representations.
The reason of replacing symbolsby distributional representations is that the distribu-tional representations have less parameters and la-tent semantic relationship in them can contribute toestimate potentials more precisely.
In summary, thecontributions of this paper are as follows.?
Compared with the basic random walk, our ap-proach direct random walks by the inference target,which increases efficiency of mining useful formu-las and has a great capability of resisting noise.?
Compared with the heuristic methods, our ap-proach can learn the strategy of random walk au-tomatically and dynamically adjust the strategy fordifferent inference targets, while the heuristic meth-ods need to write heuristic rules by hand and followthe same rule all the time.?
The experiments on link prediction task prove thatour approach has a high efficiency on mining formu-las and has a good performance on both WN18 andFB15K datasets.The rest of this paper is structured as follows, Sec-tion 2 introduces the basic random walk for miningformulas.
Section 3 describes our approach in detail.The experimental results and related discussions areshown in Section 4.
Section 5 introduces some relat-ed works, and finally, Section 6 concludes this paper.2 Mining Formulas by Random Walk2.1 Frequent Pattern MiningMining frequent patterns from source data is a prob-lem that has a long history, and for different spe-cific tasks, there are different types of source dataand different definitions of pattern.
Mining formulasis more like frequent subgraph mining, which em-ploys paths or loops as frequent patterns and minesthem from a KB.
For each relation type R, the al-gorithm enumerates paths from entity H to entityT for each triplet R(H,T ).
These paths are nor-malized to formulas by replacing entities to vari-ables.
For example, the loop in Figure 1.d, National-ity(Bob, America) ?
Nationality(Stewart, America)?
Language(Bob, English) ?
Language(Stewart,English), can be normalized to the formula, Nation-ality(x, y) ?
Nationality(z, y) ?
Language(z, w) ?Language(x, w).
Support and confidence are em-ployed to estimate a formula, where the support val-ue of a formula f : X ?
Y , noted as Sf , is definedas the proportion of paths in the KB which containsthe body X , and the confidence value of X ?
Y ,noted as Cf , is defined as the proportion of the pathsthat contains X which also meets X ?
Y .
Cf iscalculated as follows,Cf =NfNX(1)whereNf is the total number of instantiated formulaf and NX is the total number of instantiated X .2.2 Random Walk on Knowledge GraphEnumerating paths is a time consuming process anddoes not apply to large-scale KBs.
Therefore, ran-dom walk on the graph is proposed to collect fre-quent paths instead of enumerating.
Random walkrandomly chooses a neighbor to jump unlike enu-merating which needs to search all neighbors.
To es-timate a formula f , the algorithm employs f ?s occur-rence number during random walks N ?f to approxi-mate the total number Nf in Equation (1), and sim-ilarly employs N ?X to approximate NX .
Therefore,f ?s confidence Cf can be approximatively estimatedby N ?f and N?X , noted as C ?f .Random walk maintains a state transition prob-ability matrix P , and Pij means the probability ofjumping from entity i to entity j.
To make the confi-dence C ?f as close to the true confidence Cf as pos-sible, the algorithm sets P as follows,Pij ={1/di, j ?
Adj(i)0, j /?
Adj(i) (2)where di is the out-degree of the entity i, Adj(i) isthe set of adjacent entities of i, and ?Nj=1 Pij = 1.Such a transition matrix means the algorithm mayjump to all the neighboring entities with an equalprobability.
Such a random walk is independen-t from the inference target, so we call this type ofrandom walk as a goalless random walk.
The goal-less mechanism causes the inefficiency of mininguseful structures.
When we want to mine paths forR(H,T ), the algorithm cannot arrive at T from H1381in the majority of rounds.
Even though the algorith-m recalls several paths for R(H,T ), some of themmay generate noisy formulas for inferring R(H,T ).To solve the above problem, several methods di-rect random walks by statically modifying P .
Forexample, PRA sets Prij = P (j|i;r)|Ri| , P (j|i; r) =r(i,j)r(i,?)
, where P (j|i; r) is the probability of reach-ing node j from node i under the specific relationr, r(i, ?)
is the number of edges from i under r, andRi is the number of relation types from i.
Such atransition matrix implies the more narrow distribu-tions of elements in a formula are, the higher scorethe formula will obtain, which can be viewed as theheuristic rule of PRA.3 Our Approach3.1 Goal-Directed Random WalkWe propose to use the inference target, ?
=R(H,T ), to direct random walks.
When predict-ing ?, our approach always directs random walks tofind useful structures which may generate formulasto infer ?.
For different ?, random walks are direct-ed by modifying the transition matrix P in differ-ent ways.
Our approach dynamically calculates Prijwhen jumping from entity i to entity j under relationr as follows,Prij =??????
(r(i, j), ?
)?k?Adj(i) ?
(r(i, k), ?
), j ?
Adj(i)0, j /?
Adj(i)(3)where ?
(r(i, j), ?)
is the r(i, j)?s potential whichmeasures the potential contribution to infer ?
afterwalking to j.Intuitively, if r(i, j) exits in a path from H to Tand this path can generate a benefic formula to in-fer R(H,T ), the probability of jumping from i to jshould larger and thus ?
(r(i, j), ?)
also should belarger.
Reversely, if we cannot arrive at T within themaximal steps after jumping to j, or if the path pro-duces a noisy formula leading to a wrong inference,Pij and ?
(r(i, j), ?)
should both be smaller.To explicitly build a bridge between the potential?
and the inference goal ?, we maximize the like-lihood of paths which can infer ?.
First, we recur-sively define the likelihood of a path from H to tas PpHt = PpHs ?
Prst , where Prst is defined in E-quation (3).
We then classify a path pHt into threeseparate categories: a) t = T and pHt can producea benefic formula to infer R(H,T ); b) t 6= T ; c)t = T but pHt may generate a noisy formula whichmisleads inference.
Finally, we define the likelihoodfunction as follows,maxPP =?pHt?PP apHt(1?
PpHt)b+c (4)where P is all paths found in the process of perform-ing random walks for R(H,T ), and t may be equalto T or not.
a, b, c are three 0-1 variables corre-sponding to the above categories a), b), c).
Only onein a, b, c can be 1 when PHt belongs to the corre-sponding category.
We then transform maximizingPP to minimizing Lrw = ?
logPP and employ SGDto train it.
In practice, there is not a clear-cut bound-ary between a) and c), so we divide the loss into twoparts: Lrw = Ltrw + ?Linfrw .
Ltrw is the loss of thatt 6= T , and Linfrw is the loss of that pHT generates anoisy formula leading to a wrong inference.
?
is asuper parameter to balance the two losses.
Ltrw andLinfrw have the same expression but are optimized indifferent stages.
Ltrw can be optimized during ran-dom walks, while Linfrw should be optimized in theinference stage.
We rewrite Lrw for a specific pathp as follows,Lrw(p) = ?y logPp ?
(1?
y) log (1?
Pp) (5)where y is the label of the path p and y = 1 if pis beneficial to infer ?.
To obtain the best ?, wecompute gradients of Lrw as follows,?Lrw(p) = (?Lrw(r12),?Lrw(r23), ...)?Lrw(rij) = (?Lrw(rij)?
?rij, ?Lrw(rij)?
?rik1, ?Lrw(rij)?
?rik2, ...)?Lrw(rij)?
?rij=(Pp ?
y) ?
(1?
Prij )?rij ?
(1?
Pp)?Lrw(rij)?
?rik= ?
(Pp ?
y) ?
Prij?rij ?
(1?
Pp) (6)where ?Lrw(rij) is the component of ?Lrw(p) atrij .
?
(r(i, j), ?)
and ?
(r(i, k), ?)
are the potentialsfor all triplets r(i, j) ?
p and r(i, k) /?
p, and rij isshort for r(i, j).
After iteratively updating ?rij and?rik by the gradient of Ltrw, the random walks can1382be directed to find more paths fromH to T , and con-sequently it increases efficiency of the random walk.After updating ?rij and ?rik by the gradient ofLinfrw ,random walk is more likely to find high-quality path-s but not noise.
Therefore, the goal-directed randomwalk increases efficiency of mining benefic formulasand has a great capability of resisting noise.3.2 Distributional Potential FunctionThe potential ?
(r(i, j), ?)
measures an implicit re-lationship between two triplets in the KB, so thetotal number of parameters is the square of the K-B size.
It is hard to precisely estimate all ?
be-cause of the sparsity of training data.
To reducethe number of parameters, we represent each en-tity or relation in the KB as a low-rank numericvector which is called embeddings (Bordes et al,2013), and then we build a potential function ?
onembeddings as ?
(r(i, j), ?)
= ?
(Er(i,j), ER(H,T )),where Er(i,j) and ER(H,T ) are the embeddings oftriplets.
In practice, we set Er(i,j) = [Er, Ej ] andER(H,T ) = [ER, ET ] because Ei is the same for alltriplets r(i, ?
), where [] is a concatenation operator.In the view of the neural network, our goal-directed mechanism is analogous to the attentionmechanism.
At each step, the algorithm estimatesattentions for each neighboring edges by ?.
There-fore, there are several existing expressions of ?,e.g., the dot product (Sukhbaatar et al, 2015) andthe single-layer perceptron (Bahdanau et al, 2015).We will not compare different forms of ?, the detailcomparison has been presented in the work (Luonget al, 2015).
We directly employ the simplest dotproduct for ?
as follows,?
(Er(i,j), ER(H,T )) = ?
(Er(i,j) ?
ER(H,T )) (7)where ?
is a nonlinear function and we set it as anexponential function.
?
has no parameters besideKB embeddings which are updated during the train-ing period.3.3 Integrated Inference ModelTo handle the dependence between goal-directedrandom walk and subsequent inference, we combinethem into an integrated model and optimize themtogether.
To predict ?
= R(H,T ), the integratedmodel first collects formulas for R(H,T ), and thenAlgorithm 1: Train Integrated Inference ModelInput: KB, ?Output: ?, W , F1: For ?
= R(H,T ) ?
?2: Repeat ?-directed Random Walk from H to t3: Update ?
by Ltrw4: If t = T , then F = F ?
fp5: Calculate Linf and Linfrw by ?6: Update W by Linf7: Update ?
by Linfrw8: Remove f ?
F with little wf9: Output ?, W , Fmerges estimations of different formulas as featuresinto a score function ?,?(?)
=?f?F??
(f) (8)where F?
is the formula set obtained by randomwalks for ?, and ?
(f) is an estimation of formulaf .
The original frequent pattern mining algorithmemploys formulas?
confidence as ?
(f) directly, butit does not produce good results (Gala?rraga et al,2013).
There are two ways to solve the problem:one is selecting another more suitable measure of fas ?
(f) (Tan et al, 2002); the other is attaching aweight to each formula and learning weights withsupervision, e.g., MLN (Richardson and Domin-gos, 2006) .
We employ the latter method and set?
(f) = wf ?nf .
Finally, we employ a logistic regres-sion classifier to predict R(H,T ), and the posteriorprobability of R(H,T ) is shown as follows,P (?
= y|?)
= F(?)y(1?F(?))1?yF(?)
= 11 + e??
(9)where y is a 0-1 label of ?.
Similar to Ltrw inEquation (5), we treat the negative logarithm ofP (?
= y|?)
as the loss of inference, Linf =?
logP (?
= y|?
), and turn to minimize it.
More-over, the loss Linfrw of the above goal-directed ran-dom walk is influenced by the result of predictingR(H,T ), so ?rij and ?rik will be also updated.
Al-gorithm 1 shows the main process of training, where?
is the triplet set for training, ?
is the potentialfunction in Equation (7), F is the formula set, fp is1383Dataset Relation Entity Train Valid TestWN18 18 40,943 141,442 5,000 5,000FB15K 1,345 14,951 483,142 50,000 59,071Table 1: Statistics of WN18 and FB15Ka formula generated from the path p, and H,T, t areentities in the KB.
To predict ?
= R(H,T ), the al-gorithm first performs multi rounds of random walk-s, and each random walk can find a path pHt (at line2).
Then the algorithm decides to update ?
by Ltrwbased on whether t is T (at line 3), and adds the for-mula pf into the formula set when t = T (at line4).
After random walks, the inference model pre-dicts ?, and computes Linf and Linfrw according tothe prediction result (at line 5).
FinallyW and ?
areupdated by Linf and Linfrw (at line 6-7), respective-ly.
After training by all triplets in ?, the algorithmremoves formulas with low weights from F (at line8) and outputs the model (at line 9).
When we infera new triplet by this model, the process is similar toAlgorithm 1.4 ExperimentsWe first compare our approach with several state-of-art methods on link prediction task to explore ourapproach?s overall ability of inference.
Subsequent-ly, we evaluate formulas mined by different randomwalk methods to explore whether the goal-directedmechanism can increase efficiency of mining usefulstructures.
Finally, we dive deep into the formulasgenerated by our approach to analyze the charactersof our approach.4.1 Datasets and Evaluation SetupWe conduct experiments on both WN18 and FB15Kdatasets which are subsets sampled from WordNet(Miller, 1995) and Freebase (Bollacker et al, 2008),respectively, and Table 1 shows the statistics ofthem.
For the link prediction task, we predict themissing h or t for a triplet r(h, t) in test set.
The de-tail evaluation method is that t in r(h, t) is replacedby all entities in the KB and methods need to rankthe right answer at the top of the list, and so doesh in r(h, t).
We report the mean of those true an-swer ranks and the Hits@10 under both ?raw?
and?filter?
as TransE (Bordes et al, 2013) does, whereHits@10 is the proportion of correct entities rankedin the top 10.Figure 2: Arr@10 of three random walk algorithms andthe horizontal axis represents epochs and the vertical axisrepresents Arr@10.
Figure 2.a shows results on relationderivationally related form in WN18, and Figure 2.b shows re-sults on relation form of government in FB15K.4.2 BaselinesWe employ two types of baselines.
One type isbased on random walks including: a) the basic ran-dom walk algorithm whose state transition probabil-ity matrix is shown in Equation (2); b) PRA in (Laoet al, 2011) which is a typical heuristic random walkalgorithm.
The other type is based on KB embed-dings including TransE (Bordes et al, 2013), Rescal(Nickel et al, 2011), TransH (Wang et al, 2014b),TransR (Lin et al, 2015b).
These embedding-basedmethods have no explicit formulas, so we will notevaluate their performances on mining formulas.4.3 SettingsWe implement three random walk methods undera unified framework.
To predict r(h, ?)
quickly,we first select Top-K candidate instances, t1?K , byTransE as (Wei et al, 2015), and then the algorith-m infers each r(h, ti) and ranks them by inferenceresults.
We adjust parameters for our approach withthe validate dataset, and the optimal configurationsare set as follows.
The rounds of random walk is10, learning rate is 0.0001, training epoch is 100,the size of candidate set is 500 for WN18 and 100for FB15K, the embeddings have 50 dimensionali-ties for WN18 and 100 dimensionalities for FB15K,and the embeddings are initialized by TransE.
Forsome relations, random walk truly finds no practica-ble formulas, so we employ TransE to improve per-1384Dataset WN18 FB15KMetric Mean Rank Hits@10(%) Mean Rank Hits@10(%)Raw Filt Raw Filt Raw Filt Raw FiltRESCAL 1,180 1,163 37.2 52.8 828 683 28.4 44.12.a TransE 263 251 75.4 89.2 243 125 34.9 47.1TransH 401 388 73.0 82.3 212 87 45.7 64.4TransR 238 225 79.8 92.0 198 77 48.2 68.72.b RW 28* 17* 84.40 94.89 37* 28* 37.04 51.13PRA 28* 17* 84.43 94.90 37* 29* 36.72 50.732.d Our approach 28* 17* 84.40 94.86 34* 25* 53.47 74.75Table 2: Link Prediction Results on both WN18 and FB15Kformance for these relations.
For embedding-basedmethods, we use reported results directly since theevaluation datasets are identical.4.4 Results on Link PredictionWe show the results of link prediction for our ap-proach and all baselines in Table 2 (* means themean of ranks for random walk methods are eval-uated in the Top-K subset), and we can obtain thefollowing observations:1) Our approach achieves good performances onboth WN18 and FB15K.
On the FB15K, our ap-proach outperforms all baselines.
It indicates thatour approach is effective for inference.
On theWN18, three random walk methods have similarperformances.
The reason is that most entities inWN18 only have a small number of neighbors, soRW and PRA can also find useful structures in a fewrounds.2) For FB15K, the performances of RW andPRA are both poor and even worse than a part ofembedding-based methods, but the performance ofour approach is still the best.
The reason is that thereare too many relation types in FB15K, so goallessrandom walks introduce lots of noise.
Oppositely,our approach has a great capability of resisting noisefor the goal-directed mechanism.3) RW and PRA have similar performances onboth datasets, which indicates the heuristic rule ofPRA does not apply to all relations and formulas.4.5 Paths Recall by Random WalksTo further explore whether the goal-directed mech-anism can increase efficiency of mining paths, wecompare the three random walk methods by thenumber of paths mined.
For each triplet R(H,T )in the training set, we perform 10 rounds of randomwalks fromH and record the number of times whicharrive at T, noted as Arr@10.
We respectively selectone relation type from WN18 and FB15K and showthe comparison result in Figure 2.
We can obtain thefollowing observations:1) With the increase of training epochs, Arr@10of the goal-directed random walk first increases andthen stays around a high value on both WN18 andFB15K, but the Arr@10 of RW and PRA alwaysstay the same.
This phenomenon indicates that thegoal-directed random walk is a learnable model andcan be trained to find more useful structures withepochs increasing, but RW and PRA are not.2) RW and PRA always have similar Arr@10,which means PRA has not found more formulas.This indicates that the heuristic rule of PRA is notalways be beneficial to mining more structures forall relations.4.6 Example FormulasIn Table 3, we show a small number of formulasmined by our approach from FB15K, and the formu-las represent different types.
Some formulas containclear logic, e.g, Formula 1 means that if the writerx contributes a story to the film y and y is adaptedfrom the book z, x is the writer of the book z. Someformulas have a high probability of being satisfied,e.g., Formula 3 means the wedding place probablyis also the burial place for some people, and Formu-la 7 means the parent of the person x died of thedisease and thus the person x has a high risk of suf-fering from the disease.
Some formulas depend onsynonyms, e.g., story by and works written have thesimilar meaning in Formula 2.
However, there arestill useless formulas, e.g, Formula 8 is useless be-1385Relation Formulaworks written1 film story contributor(x,y) ?
adapted from(y,z)?
works written(x,z)2 story by(y,x)?
works written(x,y)place of burial3 place of death(x,y)?
place of burial(x,y)4 marriage type of union(x,y) ?
marriage location of ceremony(y,z)?
place of burial(x,z)service language5 service location(x,y) ?
imported from(y,z) ?
official language(z,w)?
service language(x,w)6 service location(x,y) ?
exported to(y,z) ?
languages spoken(z,w)?
service language(x,w)disease risk factors7 parent cause of death(x,y) ?
disease risk factors(y,z)?
disease risk factors(x,z)8 disease risk factors(x,y)?
-disease risk factors(y,x)?
disease risk factors(x,y)Table 3: Example Formulas Obtained by Goal-directed Random Walkcause the body of the formula is same as the head.Such useless formula can be removed by a super-rule, which is that the head of a formula cannot oc-cur in its body.5 Related WorkOur work has two aspects, which are related to min-ing formula automatically and inference on KBs, re-spectively.Inductive Logic Programming (ILP) (Muggletonand De Raedt, 1994) and Association Rule Mining(ARM) (Agrawal et al, 1993) are both early work-s on mining formulas.
FOIT (Quinlan, 1990) andSHERLOCK (Schoenmackers et al, 2010) are typ-ical ILP systems, but the former one usually needa lot of negative facts and the latter one focuses onmining formulas from text.
AMIE (Gala?rraga et al,2013) is based on ARM and proposes a new mea-sure for formulas instead of the confidence.
Severalstructure learning algorithms (Kok and Domingos,2005; Kok and Domingos, 2009; Kok and Domin-gos, 2010) based on Markov Logic Network (ML-N) (Richardson and Domingos, 2006) can also learnfirst order logic formulas automatically, but they aretoo slow to run on large KBs.
ProPPR (Wang et al,2013; Wang et al, 2014a) performs structure learn-ing by depth first searching on the knowledge graph,which is still not efficient enough to handle web-scale KBs.
PRA (Lao and Cohen, 2010; Lao et al,2011) is a method based on random walks and em-ploys heuristic rules to direct random walks.
PRA isclosely related to our approach, but unlike it, our ap-proach dynamically calculates state transition prob-abilities.
Another method based on random walks(Wei et al, 2015) merges embedding similarities ofcandidates into the random walk as a priori, whileour approach employs KB embeddings to calculatepotentials for neighbors.The majority of mining formula methods can per-form inference on KBs, and besides them, a dozenmethods based KB embeddings can also achieve theinference goal, and the typical ones of them areTransE (Bordes et al, 2013), Rescal (Nickel et al,2011), TransH (Wang et al, 2014b), TransR (Lin etal., 2015b).
These embedding-based methods takeadvantage of the implicit relationship between ele-ments of the KB and perform inference by calcu-lating similarities.
There are also methods whichcombine inference formulas and KB embeddings,such as PTransE (Lin et al, 2015a) and ProPPR+MF(Wang and Cohen, 2016).6 Conclusion and Future WorksIn this paper, we introduce a goal-directed randomwalk algorithm to increase efficiency of mining use-ful formulas and decrease noise simultaneously.
Theapproach employs the inference target as the direc-tion at each steps in the random walk process andis more inclined to visit structures helpful to infer-ence.
In empirical studies, we show our approachachieves good performances on link prediction taskover large-scale KBs.
In the future, we are interest-ed in exploring mining formulas directly in the dis-tributional spaces which may resolve the sparsity offormulas.13867 AcknowledgmentsThis work was supported by the Natural Sci-ence Foundation of China (No.
61533018), theNational Basic Research Program of China (No.2014CB340503) and the National Natural ScienceFoundation of China (No.
61272332).
And thiswork was also supported by Google through focusedresearch awards program.ReferencesRakesh Agrawal, Tomasz Imielin?ski, and Arun Swami.1993.
Mining association rules between sets of itemsin large databases.
ACM SIGMOD Record, 22(2):207?216.Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio.
2015.
Neural machine translation by jointlylearning to align and translate.
In International Con-ference on Learning Representations.Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim S-turge, and Jamie Taylor.
2008.
Freebase: a collabo-ratively created graph database for structuring humanknowledge.
In Proceedings of the 2008 ACM SIG-MOD international conference on Management of da-ta, pages 1247?1250.
ACM.Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran,Jason Weston, and Oksana Yakhnenko.
2013.
Trans-lating embeddings for modeling multi-relational data.In Advances in Neural Information Processing System-s, pages 2787?2795.Luis Antonio Gala?rraga, Christina Teflioudi, Katja Hose,and Fabian Suchanek.
2013.
Amie: associationrule mining under incomplete evidence in ontologicalknowledge bases.
In Proceedings of the 22nd interna-tional conference on World Wide Web, pages 413?422.International World Wide Web Conferences SteeringCommittee.Johannes Hoffart, Fabian M Suchanek, Klaus Berberich,and Gerhard Weikum.
2013.
Yago2: A spatially andtemporally enhanced knowledge base from wikipedia.Artificial Intelligence, 194:28?61.Stanley Kok and Pedro Domingos.
2005.
Learning thestructure of markov logic networks.
In Proceedings ofthe 22nd international conference on Machine learn-ing, pages 441?448.
ACM.Stanley Kok and Pedro Domingos.
2009.
Learningmarkov logic network structure via hypergraph lifting.In Proceedings of the 26th annual international con-ference on machine learning, pages 505?512.
ACM.Stanley Kok and Pedro Domingos.
2010.
Learningmarkov logic networks using structural motifs.
In Pro-ceedings of the 27th international conference on ma-chine learning (ICML-10), pages 551?558.Ni Lao and William W Cohen.
2010.
Relational retrievalusing a combination of path-constrained random walk-s. Machine learning, 81(1):53?67.Ni Lao, Tom Mitchell, and William W Cohen.
2011.Random walk inference and learning in a large scaleknowledge base.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing,pages 529?539.
Association for Computational Lin-guistics.Yankai Lin, Zhiyuan Liu, and Maosong Sun.
2015a.Modeling relation paths for representation learning ofknowledge bases.
arXiv preprint arXiv:1506.00379.Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, andXuan Zhu.
2015b.
Learning entity and relation em-beddings for knowledge graph completion.
In AAAI,pages 2181?2187.Minh-Thang Luong, Hieu Pham, and Christopher DManning.
2015.
Effective approaches to attention-based neural machine translation.
In Conference onEmpirical Methods in Natural Language Processing.George A Miller.
1995.
Wordnet: a lexical database forenglish.
Communications of the ACM, 38(11):39?41.Stephen Muggleton and Luc De Raedt.
1994.
Inductivelogic programming: Theory and methods.
The Jour-nal of Logic Programming, 19:629?679.Maximilian Nickel, Volker Tresp, and Hans-PeterKriegel.
2011.
A three-way model for collectivelearning on multi-relational data.
In Proceedings ofthe 28th international conference on machine learning(ICML-11), pages 809?816.J.
Ross Quinlan.
1990.
Learning logical definitions fromrelations.
Machine learning, 5(3):239?266.Matthew Richardson and Pedro Domingos.
2006.Markov logic networks.
Machine learning, 62(1-2):107?136.Stefan Schoenmackers, Oren Etzioni, Daniel S Weld, andJesse Davis.
2010.
Learning first-order horn clausesfrom web text.
In Proceedings of the 2010 Conferenceon Empirical Methods in Natural Language Process-ing, pages 1088?1098.
Association for ComputationalLinguistics.Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al2015.
End-to-end memory networks.
In Advances inNeural Information Processing Systems, pages 2431?2439.Pang-Ning Tan, Vipin Kumar, and Jaideep Srivastava.2002.
Selecting the right interestingness measure forassociation patterns.
In Proceedings of the eighthACM SIGKDD international conference on Knowl-edge discovery and data mining, pages 32?41.
ACM.1387William Yang Wang and William W Cohen.
2016.Learning first-order logic embeddings via matrix fac-torization.
In Proceedings of the 25th Internation-al Joint Conference on Artificial Intelligence (IJCAI2016).William Yang Wang, Kathryn Mazaitis, and William WCohen.
2013.
Programming with personalized pager-ank: a locally groundable first-order probabilistic log-ic.
In Proceedings of the 22nd ACM international con-ference on Conference on information & knowledgemanagement, pages 2129?2138.
ACM.William Yang Wang, Kathryn Mazaitis, and William WCohen.
2014a.
Structure learning via parameter learn-ing.
In Proceedings of the 23rd ACM InternationalConference on Conference on Information and Knowl-edge Management, pages 1199?1208.
ACM.Zhen Wang, Jianwen Zhang, Jianlin Feng, and ZhengChen.
2014b.
Knowledge graph embedding by trans-lating on hyperplanes.
In AAAI, pages 1112?1119.Citeseer.Zhuoyu Wei, Jun Zhao, Kang Liu, Zhenyu Qi, ZhengyaSun, and Guanhua Tian.
2015.
Large-scale knowl-edge base completion: Inferring via grounding net-work sampling over selected instances.
In Proceed-ings of the 24th ACM International on Conferenceon Information and Knowledge Management, pages1331?1340.
ACM.1388
