Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations,pages 1?5, Dublin, Ireland, August 23-29 2014.An Error Analysis Tool for Natural Language Processing and AppliedMachine LearningApoorv AgarwalDepartment of Computer ScienceColumbia UniversityNew York, NY, USAapoorv@cs.columbia.eduAnkit AgarwalNextGen Invent Corp.Shrewsbury, MA, USAankit.agarwal@ngicorporation.comDeepak MittalNextGen Invent Corp.Shrewsbury, MA, USAdeepak.mittal@ngicorporation.comAbstractIn this paper we present a simple to use web based error analysis tool to help computationallinguists, researchers building language applications, and non-technical personnel managing de-velopment of language tools to analyze the predictions made by their machine learning models.The only expectation is that the users of the tool convert their data into an intuitive XML format.Once the XML is ready, several error analysis functionalities that promote principled featureengineering are a click away.1 IntroductionA typical machine learning (ML) pipeline involves conversion of examples into a structured represen-tations, followed by training a model on these examples, followed by testing the model.
Most NaturalLanguage Processing (NLP) tasks involve (broadly) two types of structured representations: feature vec-tors (in which examples are represented as vectors in Rn) and abstract representations such as strings andtrees.
Classification models assign each test example an integer, corresponding to the predicted class.Regression models assign each test example a real number.
Throughout this process, frequently askedquestions include: is there a bug in the code that converts text into a feature vector or structured repre-sentation, which models (trained using different learning algorithms) make the right prediction on whatkind of examples?
Which models trained on which set of features/structures make the right prediction onwhat kind of examples?
Is a pair of models statistically significantly different?
Answers to these ques-tions give us a deeper understanding of the learning process, which in turn results in principled featureengineering and model selection.We spend a lot of time writing quick and dirty scripts to make connections between different aspectsof the learning process (features, structures, predictions, models).
These scripts are often task dependentand need to be re-written for each task.
More time is spent in compiling a report so our findings may beshared with other collaborators.
Frustrated with this day-to-day and repetitive script writing, we decidedto design and implement an easy-to-use error analysis tool that helps in answering the aforementionedquestions in a few clicks.
The following are the two main contributions of this work:1.
Design: the tool provides a common framework for performing error analysis of a wide range ofNLP tasks.
In designing and implementing this tool, we had to abstract away from specific taskdefinitions, feature representations, and structure representations in order to bring different aspectsof a task into a unified interface.This work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedingsfooter are added by the organisers.
Licence details: http:// creativecommons.org/licenses/by/4.0/12.
Web based: the tool is web-based, meaning that users can access a URL and upload files to accessthe functionalities of the tool.
Each user can optionally generate an identifier, which can be sharedwith other users, enabling other users to access the same error analysis session.2 The ToolIn the following sub-sections, we give details of the basic functionalities of the tool, followed by detailsof a few advanced functionalities.
We explicate the functionalities using the ACE relation extraction taskas a running example.Figure 1: A screenshot of the tool loaded with ACE relation extraction task data.Figure 2: Screenshots of pop-ups on sample ACE data.
The first pop-up shows the gold class distribution.The second pop-up shows evaluation metrics for one of the models.2.1 ACE Relation ExtractionACE (Automatic Content Extraction) relation extraction is a popular and well-established NLP task(Doddington et al., 2004).
Given a sentence, and two entity mentions (usually referred to as target2entities), the goal is to detect a relation between the two entities (relation detection), and if a relationexists, to identify the type of the relation (relation classification).
There is a pre-defined list of relationsfor the ACE relation extraction task: ART, DISC, EMP-ORG, GPE-AFF, PER-SOC, PHYS, OTHER.Researchers have developed a wide range of features for the task (Kambhatla, 2004; Zhao and Grishman,2005; Zhang et al., 2006).
There is also a large body of work that has explored the space of tree structurerepresentations (Zelenko et al., 2003; Culotta and Sorensen, 2004; Zhou et al., 2007; Nguyen et al., 2009;Agarwal and Rambow, 2010; Agarwal et al., 2014).
Relation extraction is a complex task, with featuresranging from simple bag-of-words to more complex semantic features, and with tree structures rangingfrom simple parse trees to more complicated tree structures.2.2 Basic FunctionalitiesFigure 1 presents a screenshot of the tool loaded with the ACE relation extraction data.
The columnlabeled EID is the example identifier assigned to each example, the column labeled Text has the examplein text format, the column labeled Gold has the gold class of each example (may be a real number forregression type tasks), the columns labeled FEATUREVECTORMODEL and SYNTREEMODEL havepredictions of respective models.
The column labeled FEATUREVECTORS has the different types offeature vector representations, and the column labeled TREES has the different types of tree structurerepresentations for each example.
Note that the number of columns and their names are not hard-coded.The number of columns, their names and their order is automatically inferred from the XML inputspecified by the user (section 2.4).The column labeled Text in Figure 1 shows the sentence with the target entities highlighted.
Therelations may be directed from one target entity to the other.
The numbers next to the highlighted entitiesspecify the direction of the relation (from entity marked as [1] to entity marked as [2]).
Highlightingcertain parts of input text with a tag (in this case target identifiers) is a general feature of the tool.
Thereare other popular NLP tasks in which this functionality may come in handy.
For example, for parts-of-speech tagging and named entity recognition tasks, features are extracted with respect to part of a text,which may be highlighted to understand the input example.The column labeled Gold in Figure 1 shows the gold label for each example.
Clicking on the columnlabel pops up a window with a histogram that shows the distribution of the gold class (Figure 2).The column labeled FEATUREVECTORMODEL shows the predictions made by a model that wastrained only on feature vectors.
The column labeled SYNTREEMODEL shows the predictions made bya model that was trained only on syntactic tree structures.
There is an icon next to the column name,marked as R. Clicking on this icon pops up a window with a result table that summarizes the performanceof that model (Figure 2).
Built-in metrics include precision, recall, and F1-measure (with respect to eachclass), alongside the macro- and micro-F1 measures and percentage accuracy.
This list of metrics maybe easily extended in the code.The way in which a user specifies the predictions per model is simple ?
the user is required to createa two column file (EID, prediction) and load the file into the tool using the ?Browse?
and ?Upload File?buttons (upper left corner of Figure 1).
The tool uses the EID to assign each prediction a row andtherefore the predictions may be specified in any order.In section 2.3 we discuss an advanced functionality of the tool that allows the user to filter the loadedset of examples based on boolean queries on the correctness of predictions of various models.The column labeled Meta contains meta-data associated with each example.
In our research, we arecurrently experimenting with features and tree structures derived from the output of a semantic frameparser called Semafor (Chen et al., 2010).
Semafor labels the input text with frame information ?
frameevoking elements, frame elements, their spans and types.
The output of a Semafor parse is quite complex.Visualizing the annotations produced by Semafor, along with other features and dependency trees, ishelping us design novel features and tree structures for the task of relation extraction.
The user of thetool has full control over the type of meta-data, and the number of types of meta-data that (s)he mightwant to upload in the error analysis interface.The column labeled FEATUREVECTORS lists the different types of features that one may design for3a task.
Clicking one of the feature vector types pops up a window that shows the value of features for aparticular example.
For instance, clicking ?BOW1?
for the first example will show a pop-up that containsthe following: ?some:0.2 administration:0.6?.
These are words (and their tf-idf scores) that appear in thetext between the start of the sentence and the occurrence of the first target.The column labeled TREES shows the different types of tree structures that a user may design for atask.
Clicking on a particular type of tree (button) will bring up a picture of the tree.
Note ?
we do notrequire the user to provide the pdf with the tree diagram.
The tool converts a tree specified in a standardtext format, such as this ?
(ROOT (A B))?, into a dot file, which is automatically converted into a pdf file.2.3 Other FunctionalitiesNotice the text box labeled ?Filter Data?
in Figure 1.
Users of the tool may specify complexqueries to filter out the examples that do not satisfy the filter.
For example, a query such as?FEATUREVECTORMODEL AND NOT SYNTREEMODEL?
will filter out examples that do not sat-isfy the following condition: examples that FEATUREVECTORMODEL predicted correctly but SYN-TREEMODEL predicted incorrectly.
Similarly a query such as ?FEATUREVECTORMODEL OR SYN-TREEMODEL?
will filter out examples that both the models predicted incorrectly.
We have implementedtwo binary operations (AND and OR) and a unary operation (NOT).
These operations may be combinedwith model names to form complex queries.
The tool automatically saves the past 10 filter conditions,which may be accessed through a drop down menu under the filter text box.Notice the button labeled ?McNemar?s Statistical Significance?
in Figure 1 (upper right corner).
Click-ing this button pops up a window that shows, in tabular format, the McNemar?s statistical significancep-value for all pairs of models loaded in the interface.Notice the ?Generate ID?
button on the top right corner of Figure 1.
If a user of the tool wants toshare his/her session (and analysis) with other collaborators, the user can generate a unique identifierby clicking this button.
This identifier may be shared with other collaborators who may visit the toolwebsite, enter the identifier in the text box labeled ?ID?
and gain access to the same session.
Of course,a user might want to save the generated identifier for him/her-self for returning to an older session.
Theweb service handles concurrent requests.2.4 Input XML RepresentationFigure 3: XML format to be specified by the user.Figure 3 shows the input XML schema expected from the user.
Each example may be specified withinthe XML tag ?example?.
Example identifier and its gold class may be specified as attributes of theelement ?example?.
Each example may have associated text, and a number of ?Meta?, ?Feat?, and?Tree?
prefixed tags.
We use this prefix to determine how to render the content of each element.
Forexample, the content of the tag prefixed by ?Meta?
and ?Feat?
is shown as is on the interface, whereasthe content of the tag prefixed by ?Tree?
is converted to a pdf with a picture of the tree.3 Related WorkStymne (2011) present an error analysis tool for machine translation.
El Kholy and Habash (2011)present an error analysis tool, Ameana, for NLP tasks that use morphologically rich languages.
Boththese tools are specific in terms of the NLP tasks they tackle.
While such tools are important (becausetasks such as machine translation are quite complex and require customized solutions), the goal of the4tool we present in this paper is different.
The goal of our tool is to replace the day-to-day, quick and dirtyscript writing process (required to make connections between different aspects of an NLP task) by a webbased user friendly solution.
The design of this tool is novel, and to the best of our knowledge there isno such publicly available web based error analysis tool.4 License and Contact InformationThe error analysis tool is available1for free for research purposes under the GNU General Public Licenseas published by the Free Software Foundation.AcknowledgementsWe would like to thank Jiehan Zheng and Aakash Bishnoi for contributing to the user interface code.
Wewould also like to thank Caronae Howell for her insightful comments.ReferencesApoorv Agarwal and Owen Rambow.
2010.
Automatic detection and classification of social events.
In Pro-ceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1024?1034,Cambridge, MA, October.
Association for Computational Linguistics.Apoorv Agarwal, Sriramkumar Balasubramanian, Anup Kotalwar, Jiehan Zheng, and Owen Rambow.
2014.Frame semantic tree kernels for social network extraction from text.
14th Conference of the European Chapterof the Association for Computational Linguistics.Desai Chen, Nathan Schneider, Dipanjan Das, and Noah A. Smith.
2010.
Semafor: Frame argument resolutionwith log-linear models.
In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 264?267, Uppsala, Sweden, July.
Association for Computational Linguistics.Aron Culotta and Jeffrey Sorensen.
2004.
Dependency tree kernels for relation extraction.
In Proceedings ofthe 42nd Meeting of the Association for Computational Linguistics (ACL?04), Main Volume, pages 423?429,Barcelona, Spain, July.G.
Doddington, A. Mitchell, M. Przybocki, L. Ramshaw, S. Strassel, and R. Weischedel.
2004.
The automaticcontent extraction (ace) program?tasks, data, and evaluation.
LREC, pages 837?840.A.
El Kholy and N. Habash.
2011.
Automatic error analysis for morphologically rich languages.
In MT SummitXIII, September.Nanda Kambhatla.
2004.
Combining lexical, syntactic, and semantic features with maximum entropy models forextracting relations.
In Proceedings of the ACL 2004 on Interactive poster and demonstration sessions, page 22.Association for Computational Linguistics.Truc-Vien T. Nguyen, Alessandro Moschitti, and Giuseppe Riccardi.
2009.
Convolution kernels on constituent,dependency and sequential structures for relation extraction.
Conference on Empirical Methods in NaturalLanguage Processing.Sara Stymne.
2011.
Blast: A tool for error analysis of machine translation output.
In Proceedings of the ACL-HLT2011 System Demonstrations, pages 56?61, Portland, Oregon, June.
Association for Computational Linguistics.Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella.
2003.
Kernel methods for relation extraction.
TheJournal of Machine Learning Research, 3:1083?1106.Min Zhang, Jie Zhang, Jian Su, and Guodong Zhou.
2006.
A composite kernel to extract relations between entitieswith both flat and structured features.
In Proceedings of COLING-ACL.Shubin Zhao and Ralph Grishman.
2005.
Extracting relations with integrated information using kernel methods.In Proceedings of the 43rd Meeting of the ACL.GuoDong Zhou, Min Zhang, DongHong Ji, and QiaoMing Zhu.
2007.
Tree kernel-based relation extraction withcontext-sensitive structured parse tree information.
In Proceedings of EMNLP-CoNLL.1www.ngicorporation.com/NEAT5
