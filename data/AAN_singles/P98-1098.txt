Combining a Chinese Thesaurus with a Chinese DictionaryJi DonghongKent Ridge Digital Labs21 Heng Mui Keng TerraceSingapore, 119613dhji @krdl.org.sgGong JunpingDepartment ofComputer ScienceOhio State UniversityColumbus, OHjgong@cis.ohio-state.eduHuang ChanguingDepartment ofComputer ScienceTsinghua UniversityBeijing, 100084, P. R. Chinahcn@mail.tsinghua.edu.cnAbstractIn this paper, we study the problem of combininga Chinese thesaurus with a Chinese dictionary bylinking the word entries in the thesaurus with theword senses in the dictionary, and propose asimilar word strategy to solve the problem.
Themethod is based on the definitions given in thedictionary, but without any syntactic parsing orsense disambiguation  them at all.
As a result,their combination makes the thesaurus specify thesimilarity between senses which accounts for thesimilarity between words, produces a kind ofsemantic classification of the senses defined in thedictionary, and provides reliable informationabout the lexical items on which the resourcesdon't conform with each other.1.
Introduct ionBoth ((TongYiOi CiLin)) (Mei.
et al 1983) and((XianDai HanYu CiDian)) (1978) are importantChinese resources, and have been widely used invarious Chinese processing systems (e.g., Zhanget al 1995).
As a thesaurus, ((TongYiCi CiLin))defines emantic ategories for words, however, itdoesn't specify which sense of a polysemousword is involved in a semantic ategory.
On theother hand, ((XianDai HanYu CiDian)) is anordinary dictionary which provides definitions ofsenses while not giving any information abouttheir semantic lassification.A manual effort has been made to build aresource for English, i.e., WordNet, whichcontains both definition and classificationinformation (Miller et al, 1990), but suchresources are not available for many otherlanguages, e.g.
Chinese.
This paper presents anautomatic method to combine the Chinesethesaurus with the Chinese dictionary into such aresource, by tagging the entries in the thesauruswith appropriate senses in the dictionary,meanwhile assigning appropriate semantic odes,which stand for semantic categories in thethesaurus, to the senses in the dictionary.D.Yarowsky has considered a similar problemto link Roget's categories, an English thesaurus,with the senses in COBUILD, an Englishdictionary (Yarowsky, 1992).
He treats theproblem as a sense disambiguation e, with thedefinitions in the dictionary taken as a kind ofcontexts in which the headwords occur, and dealswith it based on a statistical model of Roget'scategories trained on large corpus.
In our opinion,the method, for a specific word, neglects thedifference between its definitions and the ordinarycontexts: definitions generally contain itssynonyms, hyponyms or hypernyms, etc., whileordinary contexts generally its collocations.
Sothe trained model on ordinary contexts may be notappropriate for the disambiguation problem indefinition contexts.A seemingly reasonable method to theproblem would be common word strategy, whichhas been extensively studied by many researchers(e.g., Knight, 1993; Lesk, 1986).
The solution600would be, for a category, to select those senseswhose definitions hold most number of commonwords among all those for its member words.
Butthe words in a category in the Chinese thesaurusmay be not similar in a strict way, althoughsimilar to some extend, so their definitions mayonly contain some similar words at most, ratherthan share many words.
As a result, the commonword strategy may be not appropriate for theproblem we study here.In this paper, we extend the idea of commonword strategy further to a similar word methodbased on the intuition that definitions for similarsenses generally contain similar words, if not thesame ones.
Now that the words in a category inthe thesaurus are similar to some extent, some oftheir definitions hould contain similar words.
Wesee these words as marks of the category, then thecorrect sense of a word involved in the categorycould be identified by checking whether itsdefinition contains such marks.
So the key of themethod is to determine the marks for a category.Since the marks may be different word tokens, itmay be difficult to make them out only based ontheir frequencies.
But since they are similar words,they would belong to the same category in thethesaurus, or hold the same semantic ode, so wecan locate them by checking their semantic odes.In implementation, for any category, we firstcompute a salience value for each code withrespect to it, which in fact provides theinformation about the marks of the category, thencompute distances between the category and thesenses of its member words, which reflectwhether their definitions contain the marks andhow many, finally select those senses as tags bychecking whether their distances from thecategory fall within a threshold.The remainder of this paper is organized asthe following: in section 2, we give a formalsetting of the problem and present the taggingprocedure; in section 3, we explore the issue ofthreshold estimation for the distances betweensenses and categories based on an analysis of thedistances between the senses and categories ofunivocal words; in section 4, we report ourexperiment results and their evaluation; in section5, we present some discussions about ourmethodology; finally in section 6, we give someconclusions.2.
P rob lem Sett ingThe Chinese dictionary provides sensedistinctions for 44,389 Chinese words, on theother hand, the Chinese thesaurus divides 64,500word entries into 12 major, 94 medium and 1428minor categories, which is in fact a kind ofsemantic lassification of the words t. Intuitively,there should be a kind of correspondence b tweenthe senses and the entries.
The main task ofcombining the two resources i  to locate such kindof correspondence.Suppose X is a category 2 in the thesaurus, forany word we X, let Sw be the set of its senses inthe dictionary, and Sx = U Sw, for any se Sx, letw~XDW, be the set of the definition words in itsdefinition, DW,= UDW ~ , and DW~ UDW w,s?S w we Xfor any word w, let CODE(w) be the set of itssemantic codes that are given in the thesaurus 3,CODEs= UCODE(w), CODE~= UCODE, ,weD~ seS wand CODEx= U CODE,.
For any ce CODEx, wes~S x' The electronic versions of the two resources we use nowonly contain part of the words in them, see section 4.We generally use "category" torefer to minor categories inthe following text, if no confusion isinvolved.
Furthermore,we also use a semantic code to refer to a category., A category is given a semantic code, a word may belong toseveral categories, and hold several codes.601define its definition salience with respect o X in1).I{wIw ~ X,  c e CODEw }\[I )  Sail(c, X)= \ [X lFor example, 2) lists a category Ea02 in thethesaurus, whose members are the synonyms orantonyms of word i~j~(/gaoda/; high and big) 4.2) ~ ~,J, ~ ~ ~:~ i~: ~ I~  ~ i~~i~)t, ~ IE~ ~ ~ ~.
.
.3) lists some semantic odes and their definitionsalience with respect o the category.3) Ea02 (0.92), Ea03 (0.76), Dn01 (0.45),Eb04 (0.24), Dn04 (0.14).To define a distance between a category X and asense s, we first define a distance between anytwo categories according to the distribution oftheir member words in a corpus, which consists of80 million Chinese characters.For any category X, suppose its members arew~, w2 ..... w,, for any w, we first compute itsmutual information with each semantic codeaccording to their co-occurrence in a corpus , thenselect 10 top semantic odes as its environmentalcodes', which hold the biggest mutual informationwith wi.
Let NC~ be the set of w/s environmentalcodes, Cr be the set of all the semantic codesgiven in the thesaurus, for any ce Cr, we define itscontext salience with respect o X in 4).4) Sal,(c, X)'--/1' "/gaoda/" is the Pinyin of the word, and "high and big '' is itsEnglish translation.5 We see each occurrence of a word in the corpus as oneoccurrence of its codes.
Each co-occurrence of a word and acode falls within a 5-word distance.6 The intuition behind the parameter selection (10) is that thewords which can combined with a specific word to formcollocations fall in at most 10 categories in the thesaurus.We build a context vector for X in 5), wherek=lCTI.5) CVx=<Salz(ct, X), Salz(cz, X) .
.
.
.
.
Sal2(c,, X)>Given two categories X and Y, suppose CVx andcvr are their context vectors respectively, wedefine their distance dis(X, Y) as 6) based on thecosine of the two vectors.6) dis(X, Y)=l-cos(cvx, cvr)Let c~ CODEx, we define a distance between cand a sense s in 7).7) dis(c, s)= Min dis(c, c')c'~ CODE~Now we define a distance between a category Xand a sense s in 8).8) dis(X, s)= ~ (h c ?
dis(c, s))c~CODE xSal\] (c, X)where he=Sal z ( c' , X)c'~CODE xIntuitively, if CODEs contains the salientcodes with respect to X, i.e., those with highersalience with respect to X, dis(X, s) will besmaller due to the fact that the contribution of asemantic code to the distance increases with itssalience, so s tends to be a correct sense tag ofsome word.For any category X, let w~X and seSw, ifdis(X, s)<T, where T is some threshold, we willtag w by s, and assign the semantic ode X to s.3.
Parameter  Es t imat ionNow we consider the problem of estimating anappropriate threshold for dis(X, s) to distinguishbetween the senses of the words in X.
To do so,we first extract the words which hold only onecode in the thesaurus, and have only one sense inthe dictionary T, then check the distances betweenthese senses and categories.
The number of suchwords is 22,028., This means that the words are regarded as univocal ones byboth resources.602Tab.1 lists the distribution of the words withrespect to the distance in 5 intervals.Intervals\[o.o, 0.2)Word num.8,274Percent(%)37.56\[0.2, 0.4) 10,655 48.37\[0.4, 0.6) 339 1.54\[0.6, 0.8) 1172 5.32\[0.8, 1.0\] 1588 7.21all 22,028 100Tab.
I.
The distribution of univocal wordswith respect to dis(X, s)From Tab.l, we can see that for most univocalwords, the distance between their senses andcategories lies in \[0, 0.4\].Let Wv be the set of the univocal words weconsider here, for any univocal word we Wv, let swbe its unique sense, and Xw be its univocalcategory, we call DEN<a.
a> point density ininterval \[tj, t2\] as 9), where O<tj<t2<l.9) DEN<a.
a>=\[{wlw ~ W v ,t, < dis( Xw,s,,  ) < t 2 }1t 2 - t,We define 10) as an object function, and take t"which maximizes DEN, as the threshold.1 O) DENt = DEN<o.
t,- DEN<t.
I>The object function is built on the followinginference.
About the explanation of the wordswhich are regarded as univocal by both Chineseresources, the two resources tend to be inaccordance with each other.
It means that for mostunivocal words, their senses hould be the correcttags of their entries, or the distance between theircategories and senses should be smaller, fallingwithin the under-specified threshold.
So it isreasonable to suppose that the intervals within thethreshold hold a higher point density, furthermorethat the difference between the point density in \[0,t*\], and that in It', 1 \] gets the biggest value.With t falling in its value set {dis(X, s)}, weget t ?
as 0.384, when for 18,653 (84.68%)univocal words, their unique entries are taggedwith their unique senses, and for the otherunivocal words, their entries not tagged with theirsenses.4.
Results and EvaluationThere are altogether 29,679 words shared by the tworesources, which hold 35,193 entries in the thesaurusand 36,426 senses in the dictionary.
We nowconsider the 13,165 entries and 14,398 senses whichare irrelevant with the 22,028 univocal words.
Tab.
2and 3 list the distribution of the entries with respectto the number of their sense tags, and the distributionof the senses with respect o the number of theircode tags respectively.Tag num.0Entr 71625Percent (%)12.341 9908 75.262 1349 10.2523 283 2.15Tab.
2.
The distribution of entries with respect totheir sense tagsTa~nUlTL0Sense1461I 10433 72.462 2334 16.21>3 170Percent (%)10.151.18Tab.
3.
The distribution of senses with respect totheir code tagsIn order to evaluate the efficiency of ourmethod, we define two measures, accuracy rateand loss rate, for a group of entries E as 11) and12) respectively 8.a We only give the evaluation on the results for entries, theevaluation on the results for senses can be done similarly.603IRr  n cr lIRr lscr  - (Rr   IIwhere RTe is a set of the sense tags for the entriesin E produced by the tagging procedure, and CT~is a set of the sense tags for the entries in E, whichare regarded as correct ones somehow.What we expect for the tagging procedure isto select the appropriate sense tags for the entriesin the thesaurus, if they really exist in thedictionary.
To evaluate the procedure directlyproves to be difficult.
We turn to deal with it in anindirect way, in particular, we explore theefficiency of the procedure of tagging the entries,when their appropriate sense tags don't exist inthe dictionary.
This indirect evaluation, on the onehand, can be .carried out automatically in a largescale, on the other hand, can suggest what thedirect evaluation entails in some way because thatnone appropriate tags can be seen as a special tagfor the entries, say None 9.In the first experiment, let's consider the18,653 uniyocal words again which are selected inparameter estimation stage.
For each of them, wecreate a new entry in the thesaurus which isdifferent from its original one.
Based on theanalysis in section 3, the senses for theses wordsshould only be the correct tags for theircorresponding entries, the newly created oneshave to take None as their correct ags.When creating new entries, we adopt thefollowing 3 different kinds of constraints:i) the new entry belongs to the samemedium category with the original one;ii) the new entry belongs to the samemajor category with the original one;iii) no constraints;With each constraint, we select 5 groups of new8 A default sense tag for the entries.604entries respectively, and carry out the experimentfor each group.
Tab.
4 lists average accuracy ratesand loss rates under different constraints.Constraint Aver.
accuracy(%)i) 88.39ii) 94.75iii).
95.26Aver.
loss (%)11.615.254.74Tab.
4.
Average accuracy, loss rates under differentconstraintsFrom Tab.
4, we can see that the accuracy rateunder constraint i) is a bit less than that underconstraint ii) or iii), the reason is that with thecreated new entries belonging to the samemedium category with the original ones, it may bea bit more likely for them to be tagged with theoriginal senses.
On the other hand, notice that theaccuracy rates and loss rates in Tab.4 arecomplementary with each other, the reason is thatIRTei equals ICTel in such cases.In another experiment, we select 5 groups of0-tag, 1-tag and 2-tag entries respectively, andeach group consists of 20-30 entries.
We checktheir accuracy rates and loss rates manually.
Tab.5 lists the results.Ta~ num.02Aver.
accuracy(%) Aver.
loss(%)94.6 7.390.1 5.287.6 2.1Tab.
5.
Average accuracy and loss rates underdifferent number of tagsNotice that the accuracy rates and loss rates inTab.5 are not complementary, the reason is thatIRT~ doesn't equal ICTel in such cases.In order to explore the main factors affectingaccuracy and loss rates, we extract the entrieswhich are not correctly tagged with the senses,and check relevant definitions and semantic odes.The main reasons are:i) No salient codes exist with respect o acategory, or the determined are not the expected.This may be attributed to the fact that the words ina category may be not strict synonyms, or that acategory may contain too less words, etc.ii) The information provided for a word bythe resources may be incomplete.
For example,word "~( /quanshu/ ,  all) holds one semanticcode Ka06 in the thesaurus, its definition in thedictionary is:~:/quanshu/~\ [Eb02\ ]/quanbu/allThe correct ag for the entry should be the senselisted above, but in fact, it is tagged with None inthe experiment.
The reason is that word ~:~(/quanbu/, all) can be an adverb or an adjective,and should hold two semantic odes, Ka06 andEb02, corresponding with its adverb and adjectiveusage respectively, but the thesaurus neglects itsadverb usage.
If Ka06 is added as a semantic odeof word ~_~ (/quanbu/, all), the entry will besuccessfully tagged with the expected sense.iii) The distance defined between a sense anda category fails to capture the information cardedby the order of salient codes, more generally, theinformation carded by syntactic structuresinvolved.
As an example, consider word ~-~(/yaochuan/), which has two definitions listed inthe following.i~  1) i~ \ [Da l9 \ ]  ~\ [ Ie01 l ./yaochuan/ /yaoyan/ /chuanbo/hearsay spreadthe hearsay spreads.2) ~\ [ Ie01\ ]  I~ ~.~-~ \[Dal9\]/chuanbo/ Idel /yaoyan/spread of hearsaythe hearsay which spreadsThe two definitions contain the same contentwords, the difference between them lies in theorder of the content words, more generally, lies inthe syntactic structures involved in the definitions:the former presents a sub-obj structure, while thelatter with a "l~(/de/,of)" structure.
To distinguishsuch definitions needs to give more considerationon word order or syntactic structures.5.
DiscussionsIn the tagging procedure, we don't try to carry outany sense disambiguation definitions due to itsknown difficulty.
Undoubtedly, when the noisysemantic odes taken by some definition wordsexactly cover the salient ones of a category, theywill affect the tagging accuracy.
But theprobability for such cases may be lower,especially when more than one salient code existswith respect to a category.The distance between two categories isdefined according to the distribution of theirmember words in a corpus.
A natural alternative isbased on the shortest path from one category toanother in the thesaurus (e.g., Lee at al., 1993;Rada et al, 1989), but it is known that the methodsuffers from the problem of neglecting the widevariability in what a link in the thesaurus entails.Another choice may be information contentmethod (Resnik, 1995), although it can avoid thedifficulty faced by shortest path methods, it willmake the minor categories within a medium oneget a same distance between each other, becausethe distance is defined in terms of the informationcontent carded by the medium category.
What weconcern here is to evaluate the dissimilaritybetween different categories, including thosewithin one medium category, so we make use ofsemantic code based vectors to define theirdissimilarity, which is motivated by Shuetze'sword frequency based vectors (Shuetze, 1993).In order to determine appropriate sense tags605for a word entry in one category, we estimate athreshold for the distance between a sense and acategory.
Another natural choice may be to selectthe sense holding the smallest distance from thecategory as the correct tag for the entry.
But thischoice, although avoiding estimation issues, willfail to directly demonstrate the inconsistencybetween the two resources, and the similaritybetween two senses with respect o a category.6.
ConclusionsIn this paper, we propose an automatic method tocombine a Chinese thesaurus with a Chinesedictionary.
Their combination establishes thecorrespondence between the entries in thethesaurus and the senses in the dictionary, andprovides reliable information about the lexicalitems on which the two resources are not inaccordance with each other.
The method uses nolanguage-specific knowledge, and can be appliedto other languages.The combination of the two resources can beseen as improvement on both of them.
On the onehand, it makes the thesaurus pecify the similaritybetween word senses behind that between words,on the other hand, it produces a semanticclassification for the word senses in thedictionary.The method is in fact appropriate for a moregeneral problem: given a set of similar words,how to identify the senses, among all, whichaccount for their similarity.
In the problem weconsider here, the words fall within a category inthe Chinese thesaurus, with similarity to someextent between each other.
The work suggests thatif the set contains more words, and they are moresimilar with each other, the result will be moresound.ReferencesKnight K. (1993) Building a Large Ontology forMachine Translation.
In "Proceedings of DARPAHuman Language Conference", Princeton, USA,185-190.Lesk M. (1986) Automated Word SenseDisambiguation using Machine ReadableDictionaries: How to Tell a Pine Cone from an IceCream Cone.
In "Proceedings of the ACM SIGDOCConference", Toronto Ontario.Lee J. H., Kim M. H., and Lee Y. J.
(1993).Information retrieval based on concept distance inIS-A hierarchies.
Journal of Documentation, 49/2.Mei J.J. et al (1983) TongYiCi CiLin(A ChineseThesaurus).
Shanghai Cishu press, Shanghai.Miller G.A., Backwith R., Felibaum C., Gross D. andMiller K. J.
(1990) Introduction to WordNet: AnOn-line Lexical Database.
International Journal ofLexicography, 3(4) (Special Issue).Rada R. and Bicknell E (1989) Ranking documentswith a thesaurus.
JASIS, 40(5), pp.
304-3 I0.Resnik P. (1995) Using Information Content toEvaluate the similarity in a Taxonomy.
In"Proceedings of the 14th International JointConference on Artificial Intelligence".Schutze H. (1993) Part-of-speech induction fromscratch.
In "Proceedings of the 31 st Annual Meetingof the Association for Computational Linguistics",Columbus, OH.XianDai HanYu CiDian(A modern Chinese Dictionary)(1978), Shangwu press, Beijing.Yarowsky D. (1992) Word Sense DisambiguationUsing Statistical Models of Roget's CategoriesTrained on Large Corpora.
In "Proceedings ofCOLING'92", Nantas, France, pp.
454-460.Zhang J, Huang C. N. Yang E. H. (1994) Constructiona Machine Tractable Dictionary from a MachineReadable Dictionary.
Communications of Chineseand Oriental Language Information ProcessingSociety, 4(2), pp.
123-130.606
