Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1998?2004,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsUnsupervised Word Alignment by Agreement Under ITG ConstraintHidetaka Kamigaito1kamigaito@lr.pi.titech.ac.jpAkihiro Tamura2akihiro.tamura@nict.go.jpHiroya Takamura1takamura@pi.titech.ac.jpManabu Okumura1oku@pi.titech.ac.jpEiichiro Sumita2eiichiro.sumita@nict.go.jp1Tokyo Institute of Technology2National Institute of Information and Communication TechnologyAbstractWe propose a novel unsupervised word align-ment method that uses a constraint based onInversion Transduction Grammar (ITG) parsetrees to jointly unify two directional mod-els.
Previous agreement methods are nothelpful for locating alignments with long dis-tances because they do not use any syntacticstructures.
In contrast, the proposed methodsymmetrizes alignments in consideration oftheir structural coherence by using the ITGconstraint softly in the posterior regulariza-tion framework (Ganchev et al, 2010).
TheITG constraint is also compatible with wordalignments that are not covered by ITG parsetrees.
Hence, the proposed method is ro-bust to ITG parse errors compared to otheralignment methods that directly use an ITGmodel.
Compared to the HMM (Vogel et al,1996), IBM Model 4 (Brown et al, 1993),and the baseline agreement method (Ganchevet al, 2010), the experimental results showthat the proposed method significantly im-proves alignment performance regarding theJapanese-English KFTT and BTEC corpus,and in translation evaluation, the proposedmethod shows comparable or statistical sig-nificantly better performance on the Japanese-English KFTT and IWSLT 2007 corpus.1 IntroductionWord alignment is an important component of sta-tistical machine translation (SMT) systems such asphrase-based SMT (Koehn et al, 2003) and hier-archical phrase-based SMT (Chiang, 2007).
In ad-dition, word alignment is utilized for multi-lingualtasks other than SMT, such as bilingual lexicon ex-traction (Liu et al, 2013).
The most conventionalapproaches to word alignment are the IBM models(Brown et al, 1993) and the HMM model (Vogel etal., 1996), which align each source word to a sin-gle target word (i.e., directional models).
In thesemodels, bidirectional word alignments are tradition-ally induced by combining the Viterbi alignments ineach direction using heuristics (Och and Ney, 2003).Matusov et al (2004) exploited a symmetrized pos-terior probability for bidirectional word alignments.In these methods, each directional model is indepen-dently trained.Previous researches have improved bidirectionalword alignments by jointly training two directionalmodels to agree with each other (Liang et al, 2006;Grac?a et al, 2008; Ganchev et al, 2010).
Sucha constraint on the agreement in a training phaseis one of the most effective approaches to wordalignment.
However, none of the previous agree-ment constraints have taken into account syntacticstructures.
Therefore, they have difficulty recover-ing the alignments with long distances, which fre-quently occur, especially in grammatically differentlanguage pairs.Some unsupervised word alignment models suchas DeNero and Klein (2007) and Kondo et al (2013),have been based on syntactic structures.
In particu-lar, it has been proven that Inversion TransductionGrammar (ITG) (Wu, 1997), which captures struc-tural coherence between parallel sentences, helps inword alignment (Zhang and Gildea, 2004; Zhangand Gildea, 2005).
However, ITG has not been in-troduced into an agreement constraint so far.1998We propose an alignment method that uses anITG constraint to encourage agreement between twodirectional models in consideration of their struc-tural coherence.
Our ITG constraint is based on theViterbi alignment decided by a bracketing ITG parsetree, and used as a soft constraint in the posteriorregularization framework (Ganchev et al, 2010).
Inaddition, our ITG constraint works also on wordalignments that are not covered by ITG parse trees,as a standard symmetric constraint.
Hence, the pro-posed method is robust to ITG parse errors com-pared to an alignment method that uses an ITG di-rectly in model training (e.g., Zhang and Gildea(2004, 2005)).Word alignment evaluations show that the pro-posed method achieves significant gains in F-measure and alignment error rate (AER) on theKFTT (Neubig, 2011) and the BTEC Japanese-English (Ja-En) corpus (Takezawa et al, 2002).
Ma-chine translation evaluations show that our con-straint significantly outperforms or is comparable tothe baseline symmetric constraint (Ganchev et al,2010) in BLEU on the KFTT Ja-En and IWSLT2007 Ja-En corpus (Fordyce, 2007).2 ITG Constraint in the PosteriorRegularization Framework2.1 OverviewThe proposed method introduces an ITG con-straint into the posterior regularization framework(Ganchev et al, 2010) in model training.
The pro-posed model is trained as follows, where agreementconstraints are imposed in the E-step of the EM al-gorithm1:E-step:1.
Calculate a source-to-target posterior probability??p?
(z|x) and a target-to-source posterior probabil-ity ??p?
(z|x) for each bilingual sentence x = {f , e}under the current model parameters ?, where z de-notes an alignment in a sentence pair x.
In particu-lar, zi,j=1, if fi is aligned to ej (otherwise zi,j=0).2.
Repeat the following steps for all sentence pairsin the training data.
(a) Find the Viterbi alignment z?
through ITG pars-ing (see Section 2.2).
Here, z?i,j=1, if fi is aligned1Step 1 in the E and M steps can be performed in the sameway as in Ganchev et al (2010).to ej (otherwise z?i,j=0).
(b) Symmetrize??p?
(z|x) and??p?
(z|x) under the con-straint of z?
(see Section 2.3 and 2.4).M-step:1.
Estimate all parameters ?
based on thesymmetrized posterior probabilities ??q?
(z|x) and??q?
(z|x) (see Section 2.3 and 2.4).2.2 ITG ParsingIn this section, we present our ITG parsing method,which uses bracketing ITG (Wu, 1997).
The rulesof the bracketing ITG are as follows: A ?
?Y/Z?,A ?
[Y/Z], A ?
fi/ej , A ?
fi/?, and A ?
?/ej ,whereA, Y , and Z are non-terminal symbols, fi andej are terminal strings, ?
is a null symbol, ??
denotesthe inversion of two phrase positions, and [] denotesthe reversion of two phrase positions.In general, a bracketing ITG has O(|f |3|e|3) timecomplexity for parsing a sentence pair {f , e}, where|f | and |e| are the lengths of f and e. For ef-ficient ITG parsing, we use the two-step parsingapproach (Xiao et al, 2012), which has been pro-posed to induce Synchronous Context Free Gram-mar (SCFG) using n-best pruning2 with time com-plexity O(|f |3).
Because ITG is a kind of SCFG,this method can be adopted for our ITG parsing.
Ourtwo-step parsing first parses a bilingual sentence inthe bottom up manner, and then derives the Viterbialignment z?
in the top down manner.To parse a bilingual sentence x = {f , e}, we de-fine the probability for each ITG rule.
The probabil-ity of a rule A ?
fi/ej is defined as:P (A ?
fi/ej) =?
?p ?
(zi,j = 1|x) +?
?p ?
(zi,j = 1|x)2 .We provide a constant value pnull3 both to P (A ?
?/ej) and P (A ?
fi/?).
To reduce computa-tional cost, the probabilities of phrasal rules P (A ??Y/Z?)
and P (A ?
[Y/Z]) are not trained, whichare set to 0.5 following Saers et al (2012).
Inaddition to the probability of each ITG rule, wemust provide a probability to an one-to-many align-ment because the two step parsing approach mustpre-compute probabilities for all one-to-many align-ments in the first step.
An one-to-many alignment2We set n to 30 in our experiments.3We set pnull to 10?5.1999can be decomposed to a rule A ?
fi/ej and someA ?
?/ej rules under the ITG form.
We select a setof rules with the highest probability for an one-to-many alignment using Viterbi algorithm, which hasa complexity of O(|e|).2.3 Previous Agreement ConstraintThis section provides an overview of the previ-ous agreement constraint proposed by Ganchev etal.
(2010), which is our baseline.
In the poste-rior regularization framework, source-to-target andtarget-to-source posterior probabilities ?
?p ?
(z|x)and ?
?p ?
(z|x) are replaced with ?
?q ?
(z|x) and?
?q ?
(z|x), defined as follows:?
?q ?
(z|x) = ?
?p ?
(z|x) ?
exp(????agree(x,z))/Z?
?q ,?
?q ?
(z|x) = ?
?p ?
(z|x) ?
exp(????agree(x,z))/Z?
?q ,where Z?
?q is a normalization term for?z?
?q ?
(z|x) = 1 (Z?
?q is analogous) and ?is a vector of weight parameters that controls thebalance between two directional posterior prob-abilities.
Here, ?agree is a feature of agreementconstraint, which assigns each alignment directionto a sign (i.e., +1 or -1).
In particular, ?agree isdefined as follows:?agreei,j (x, z) =????
?+1 (z ?
?
?Z ) ?
(zi,j=1),?1 (z ?
?
?Z ) ?
(zi,j=1),0 otherwise,where ?
?Z and ?
?Z are sets of possible alignmentsgenerated by source-to-target and target-to-sourcealignment models, respectively.
So that ?
?q ?i,j (zi,j=1|x) and ?
?q ?i,j (zi,j =1|x) become equal probabil-ities for each i, j (i.e., ?
?q ?
(z|x) and ?
?q ?
(z|x) aresymmetrical), the agreement constraint is defined asfollows:?i, ?j,?
?q ?i,j (zi,j=1|x)??
?q ?i,j (zi,j=1|x) = 0.
(1)To satisfy the constraint (1), each ?i,j is updated bya stochastic gradient descent in the E-step of EM al-gorithm.2.4 Proposed ITG ConstraintThis section presents the proposed ITG constraintbased on the Viterbi alignment z?, which has pre-viously been identified by the bracketing ITG pars-ing.
The ITG constraint uses a feature ?ITG insteadof ?agree:?ITGi,j (x, z)=??????????????????????
?0 ?
?Y (i, j)?(z?i,j=1)?
(?i,j(x, z)<0),+1 ?
?Y (i, j)?(z?i,j=1)?
(?i,j(x, z)>0),?1 ?
?Y (i, j)?(z?i,j=1)?
(?i,j(x, z)<0),0 ?
?Y (i, j)?(z?i,j=1)?
(?i,j(x, z)>0),+1 ?
?Y (i, j)?
(z?i,j ?=1),?1 ?
?Y (i, j)?
(z?i,j ?=1),0 otherwise,where ?
?Y (i, j) = (z ?
?
?Z ) ?
(zi,j=1), ?
?Y (i, j) =(z ?
?
?Z ) ?
(zi,j =1), and ?i,j(x, z) = ?
?p ?
(zi,j =1|x) ?
?
?p ?(zi,j=1|x).
Similarly to ?agree, ?ITG isimposed on ?
?q ?i,j (zi,j=1|x) and ?
?q ?i,j (zi,j=1|x)under the constraint (1).
If z?i,j ?= 1, our feature?ITGi,j operates similarly to ?agreei,j according to thelast three rules.
If z?i,j =1, ?ITG adjusts probabili-ties of alignments ?
?q ?i,j (zi,j=1|x) and ?
?q ?i,j (zi,j=1|x) by increasing the lower probability withoutdecreasing the higher probability according to thefirst four rules.
For example, when z?i,j = 1 and?
?q ?i,j (zi,j = 1|x) is larger than ?
?q ?i,j (zi,j = 1|x),?
?q ?i,j (zi,j=1|x) is increased until ?
?q ?i,j (zi,j=1|x)equals ?
?q ?i,j (zi,j=1|x) according to the second andfourth rules.
When z?i,j=1 and ?
?q ?i,j (zi,j=1|x) islarger than ?
?q ?i,j (zi,j=1|x), ?
?q ?i,j (zi,j=1|x) is in-creased until ?
?q ?i,j (zi,j =1|x) equals ?
?q ?i,j (zi,j =1|x) according to the first and third rules.
As a re-sult, probabilities of word alignments in z?
tend tobe higher than those of the other alignments.Task Corpus Train Dev TestWord Hansard 1.13M 37 447Alignment KFTT 330k 653 582BTEC 10k 0 10kMachine KFTT 330k 1.17k 1.16kTranslation IWSLT2007 40k 2.5k 489Table 1: The numbers of parallel sentences for each data set.3 EvaluationWe compared our proposed ITG constraint (itg) withthe baseline agreement constraint (Ganchev et al,2010) (sym) on word alignment and machine trans-lation tasks.
In word alignment evaluations, we usedthe French-English (Fr-En) Hansard Corpus (Mihal-cea and Pedersen, 2003), Ja-En KFTT4 (Neubig,4We used the cleaned dataset distributed on the KFTT offi-cial web site (http://www.phontron.com/kftt/index.html).2000Hansard Fr-En KFTT Ja-En BTEC Ja-EnMethod F-measure AER F-measure AER F-measure AERHMM+none 0.7900 0.0646 0.4623 0.5377 0.4425 0.5575HMM+sym 0.7923 0.0597 0.4678 0.5322 0.4534 0.5466HMM+itg 0.7869 0.0629 0.4690 0.5310 0.4499 0.5501IBM Model 4+none 0.7780 0.0775 0.5379 0.4621 0.4454 0.5546IBM Model 4+sym 0.7800 0.0693 0.5545 0.4455 0.4761 0.5239IBM Model 4+itg 0.7791 0.0710 0.5613 0.4387 0.4809 0.5191Table 2: Word alignment performance.Method KFTT Ja-En IWSLT2007 Ja-EnHMM+none 18.9 46.4HMM+sym 18.9 46.3HMM+itg 19.2 47.0IBM Model 4+none 18.8 46.7?IBM Model 4+sym 19.3?
45.9IBM Model 4+itg 19.4 46.7Table 3: Machine translation performance.2011), and Ja-En BTEC Corpus (Takezawa et al,2002).
We used the first 10K sentence pairs in thetraining data for the IWSLT 2007 translation task,which were manually annotated with word align-ment (Chooi-Ling et al, 2010), as the BTEC Cor-pus.
In translation evaluations, we used the KFTTand Ja-En IWSLT 2007 translation tasks5.Table 1 shows each corpus size.
In each trainingdata set, all words were lowercased and sentenceswith over 80 words on either side were removed.3.1 Word Alignment EvaluationWe measured the performance of word alignmentwith AER and F-measure (Och and Ney, 2003).
Weused only sure alignments for calculating F-measure(Fraser and Marcu, 2007)6.
We introduced itg andsym into the HMM and IBM Model 4.
Training isbootstrapped from IBMModel 1, followed by HMMand IBM Model 4.
All models were trained with fiveconsecutive iterations.
In the many-to-many align-ment extraction, we used the filtering method (Ma-tusov et al, 2004), where a threshold is optimized onthe corresponding AER of the baseline model (i.e.,HMM+sym or IBM Model 4+sym)7.5BTEC Corpus is a subset of IWSLT 2007.
To uniformtokenization, we retokenized all Japanese sentences both inIWSLT 2007 and BTEC Corpus using ChaSen (Asahara andMatsumoto, 2000).6Since there exists no distinction for sure-possible align-ments in the KFTT and BTEC data sets, we treat all alignmentsof them as sure alignments.7We tried values from 0.1 to 1.0 at an interval of 0.1.Table 2 shows the results of word alignment eval-uations8, where none denotes that the model hasno constraint.
In KFTT and BTEC Corpus, itgachieved significant improvement against sym andnone on IBM Model 4 (p ?
0.05)9.
However, in theHansard Corpus, itg shows no improvement againstsym.
This indicates that capturing structural coher-ence by itg yields a significant benefit to word align-ment in a linguistically different language pair suchas Ja-En.
For example, some function words appearmore than once in both a source and target sentence,and they are not symmetrically aligned with eachother, especially in regards to the Ja-En languagepair.
Although the baseline methods tend to be un-able to align such long-distance word pairs, the pro-posed method can correctly catch them because itgcan determine the relation of long-distance words.We discuss more details about the effectiveness ofthe ITG constraint in Section 4.1.3.2 Translation EvaluationWe measured translation performance with BLEU(Papineni et al, 2002).
All language models are5-gram and trained using SRILM (Stolcke and oth-ers, 2002) on target side sentences in the trainingdata.
When extracting phrases, we apply the methodproposed by Matusov et al (2004), where many-to-many alignments are generated based on the aver-ages of the posterior probabilities from two direc-tional models10.We used the Moses phrase-based SMT systems(Koehn et al, 2007) for decoding.
We set thedistortion-limit parameter to infinite11, and other pa-8The values in bold indicate the best score.9The statistical significance test was performed by the pairedbootstrap resampling (Koehn, 2004).10The posterior thresholds were decided in the same way asthe word alignment evaluation.11This setting is generally used for Ja-En translation tasks(Murakami et al, 2007).2001Figure 1: Word alignment examples on the BTEC corpus.rameters as default settings.
Parameter tuning wasconducted by 100-best batch MIRA (Cherry andFoster, 2012) with 25 iterations.Table 3 shows the average BLEU of five differ-ent tunings12.
In both KFTT and IWSLT 2007, itgachieved significant improvement against both noneand sym on HMM model.
On IBM Model4, itg sig-nificantly outperforms none and is comparable tosym in KFTT, while itg significantly outperformssym and is comparable to none in IWSLT 2007.4 Discussion4.1 Effects of ITG Constraints on WordAlignment and TranslationWe discuss the effect of our ITG constraint on wordalignment and machine translation.
As describedin Section 2, the ITG constraint is imposed in theE-step of the EM algorithm, not in decoding steps.Therefore, for the sentences that are not contained inthe training corpus, the word alignments are calcu-lated using the emission, transition and fertility ta-bles trained with the constraint.
It means that the ef-fects of the constraint are implicitly reflected in thealignment results.
On the other hand, the effects ofthe constraint are directly reflected in the machinetranslation results because the phrase tables are ex-tracted from the posterior probabilities calculated intraining steps.
Therefore, our ITG constraint has apotential to achieve a large improvement of machinetranslation performance relative to an improvementof alignment performance, such as IBMModel 4+itg12The values in bold represent the best score, and ?
indicatesthat the comparisons are not significant over the correspondingmodel (i.e., HMM+itg or IBM Model 4+itg) according to thebootstrap resampling test (p ?
0.05).
We used multeval (Clarket al, 2011) for significance testing.vs.
IBM Model 4+sym on the BTEC corpus.
Wewould like to improve our model by imposing ourITG constraint on decoding steps in future.4.2 Comparison between Symmetric and ITGConstraintIn KFTT, itg is comparable to sym on IBM Model4 in machine translation; however, itg achieved sig-nificant improvement in terms of word alignment,which follows the previous reports that better wordalignment does not always result in better transla-tion (Ganchev et al, 2008; Yang et al, 2013).
Onthe other hand, in BTEC, itg outperforms sym bothon word alignment and machine translation.
Fig-ure 1 shows that IBM Model 4+sym often generateswrong gappy alignments such as ?ga (Ja)-I (En)?and ?ga (Ja)-my (En)?.
These wrong alignmentsdisturb the phrase extraction, because excessivelylong phrase pairs are extracted by bridging the gapsin wrong alignments or simply no phrase pairs areextracted from wrong gappy alignments.
Conse-quently, the phrase table generated by IBM Model4+sym tend to be sparse and contain longer phrasepairs than the one generated by IBM Model 4+itg.5 ConclusionsWe have proposed a novel alignment method thatuses an ITG constraint based on bracketing ITGparse trees as a soft constraint of the posterior reg-ularization framework.
Due to the ITG constraint,the proposed method can symmetrize two direc-tional alignments based on their structural coher-ence.
Our evaluations have shown that the proposedITG constraint significantly improves the baselineword alignment performance on the Ja-En KFTTand BTEC corpus, and significantly improves, or atleast keeps, the baseline machine translation perfor-mance of KFTT and the Ja-En IWSLT 2007 task.This indicates that the proposed method yields a sig-nificant benefit to linguistically different languagepairs.In future work, we plan to incorporate a phrasalITG (Cherry and Lin, 2007) instead of a bracketingITG to efficiently handle many-to-many alignments.2002ReferencesMasayuki Asahara and Yuji Matsumoto.
2000.
Extendedmodels and tools for high-performance part-of-speechtagger.
In Proceedings of the 18th conference on Com-putational linguistics-Volume 1, pages 21?27.
Associ-ation for Computational Linguistics.Peter F. Brown, Vincent J. Della Pietra, Stephen A. DellaPietra, and Robert L. Mercer.
1993.
The mathematicsof statistical machine translation: Parameter estima-tion.
Computational Linguistics, 19(2):263?311.Colin Cherry and George Foster.
2012.
Batch TuningStrategies for Statistical Machine Translation.
In Pro-ceedings of the 2012 Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics: Human Language Technologies, pages 427?436, Montre?al, Canada, June.
Association for Compu-tational Linguistics.Colin Cherry and Dekang Lin.
2007.
Inversion Trans-duction Grammar for Joint Phrasal Translation Mod-eling.
In Proceedings of SSST, NAACL-HLT 2007 /AMTA Workshop on Syntax and Structure in Statisti-cal Translation, pages 17?24, Rochester, New York,April.
Association for Computational Linguistics.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.Goh Chooi-Ling, Watanabe Taro, Yamamoto Hirofumi,and Sumita Eiichiro.
2010.
Constraining a generativeword alignment model with discriminative output.Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.Smith.
2011.
Better Hypothesis Testing for Statis-tical Machine Translation: Controlling for OptimizerInstability.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistics:Human Language Technologies, pages 176?181, Port-land, Oregon, USA, June.
Association for Computa-tional Linguistics.John DeNero and Dan Klein.
2007.
Tailoring WordAlignments to Syntactic Machine Translation.
In Pro-ceedings of the 45th Annual Meeting of the Associationof Computational Linguistics, pages 17?24, Prague,Czech Republic, June.
Association for ComputationalLinguistics.Cameron S Fordyce.
2007.
Overview of the IWSLT2007 evaluation campaign.
In Proceedings of the In-ternational Workshop on Spoken Language Transla-tion 2007, pages 1?12.Alexander Fraser and Daniel Marcu.
2007.
Measuringword alignment quality for statistical machine transla-tion.
Computational Linguistics, 33(3):293?303.Kuzman Ganchev, Joa?o V. Grac?a, and Ben Taskar.
2008.Better Alignments = Better Translations?
In Pro-ceedings of ACL-08: HLT, pages 986?993, Colum-bus, Ohio, June.
Association for Computational Lin-guistics.Kuzman Ganchev, Joao Graca, Jennifer Gillenwater, andBen Taskar.
2010.
Posterior regularization for struc-tured latent variable models.
The Journal of MachineLearning Research, 11:2001?2049.Joao V Grac?a, Kuzman Ganchev, and Ben Taskar.
2008.Expectation Maximization and Posterior Constraints.In J. C. Platt, D. Koller, Y.
Singer, and S. T. Roweis,editors, Advances in Neural Information ProcessingSystems 20, pages 569?576.
Curran Associates, Inc.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of the 2003 Conference of the North AmericanChapter of the Association for Computational Linguis-tics on Human Language Technology-Volume 1, pages48?54.
Association for Computational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open SourceToolkit for Statistical Machine Translation.
In Pro-ceedings of the 45th Annual Meeting of the Associ-ation for Computational Linguistics Companion Vol-ume Proceedings of the Demo and Poster Sessions,pages 177?180, Prague, Czech Republic, June.
Asso-ciation for Computational Linguistics.Philipp Koehn.
2004.
Statistical Significance Tests forMachine Translation Evaluation.
In Dekang Lin andDekai Wu, editors, Proceedings of the 2004 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 388?395, Barcelona, Spain, July.
Asso-ciation for Computational Linguistics.Shuhei Kondo, Kevin Duh, and Yuji Matsumoto.
2013.Hidden Markov Tree Model for Word Alignment.
InProceedings of the Eighth Workshop on Statistical Ma-chine Translation, pages 503?511, Sofia, Bulgaria,August.
Association for Computational Linguistics.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by Agreement.
In Proceedings of the HumanLanguage Technology Conference of the NAACL, MainConference, pages 104?111, New York City, USA,June.
Association for Computational Linguistics.Xiaodong Liu, Kevin Duh, and Yuji Matsumoto.
2013.Topic models + word alignment = a flexible frame-work for extracting bilingual dictionary from compara-ble corpus.
In Proceedings of the Seventeenth Confer-ence on Computational Natural Language Learning,pages 212?221, Sofia, Bulgaria, August.
Associationfor Computational Linguistics.Evgeny Matusov, Richard Zens, and Hermann Ney.2004.
Symmetric Word Alignments for StatisticalMachine Translation.
In Proceedings of COLING2004, the 20th International Conference on Compu-2003tational Linguistics, pages 219?225, Geneva, Switzer-land, Aug 23?Aug 27.
COLING.Rada Mihalcea and Ted Pedersen.
2003.
An EvaluationExercise for Word Alignment.
In Rada Mihalcea andTed Pedersen, editors, Proceedings of the HLT-NAACL2003 Workshop on Building and Using Parallel Texts:Data Driven Machine Translation and Beyond, pages1?10.Jin?ichi Murakami, Tokuhisa Masato, and Satoru Ikehara.2007.
Statistical machine translation using large j/eparallel corpus and long phrase tables.
In Proceedingsof the International Workshop on Spoken LanguageTranslation 2007, pages 151?155.Graham Neubig.
2011.
The Kyoto free translation task.http://www.phontron.com/kftt.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a Method for Automatic Eval-uation of Machine Translation.
In Proceedings of 40thAnnual Meeting of the Association for ComputationalLinguistics, pages 311?318, Philadelphia, Pennsylva-nia, USA, July.
Association for Computational Lin-guistics.Markus Saers, Karteek Addanki, and Dekai Wu.
2012.From Finite-State to Inversion Transductions: To-ward Unsupervised Bilingual Grammar Induction.In Proceedings of COLING 2012, the 24th Inter-national Conference on Computational Linguistics,pages 2325?2340, Mumbai, India, December.
TheCOLING 2012 Organizing Committee.Andreas Stolcke et al 2002.
SRILM-an extensible lan-guage modeling toolkit.
In Proceedings InternationalConference on Spoken Language Processing, pages257?286, November.Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki Sugaya,Hirofumi Yamamoto, and Seiichi Yamamoto.
2002.Toward a Broad-coverage Bilingual Corpus for SpeechTranslation of Travel Conversations in the Real World.In Proceedings of the Third International Conferenceon Language Resources and Evaluation (LREC?02),pages 147?152.Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
HMM-based word alignment in statistical trans-lation.
In Proceedings of the 16th conference on Com-putational Linguistics-Volume 2, pages 836?841.
As-sociation for Computational Linguistics.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?403.Xinyan Xiao, Deyi Xiong, Yang Liu, Qun Liu, andShouxun Lin.
2012.
Unsupervised Discriminative In-duction of Synchronous Grammar for Machine Trans-lation.
In Proceedings of COLING 2012, the 24thInternational Conference on Computational Linguis-tics, pages 2883?2898, Mumbai, India, December.
TheCOLING 2012 Organizing Committee.Nan Yang, Shujie Liu, Mu Li, Ming Zhou, and Neng-hai Yu.
2013.
Word Alignment Modeling with Con-text Dependent Deep Neural Network.
In Proceed-ings of the 51st Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 166?175, Sofia, Bulgaria, August.
Associationfor Computational Linguistics.Hao Zhang and Daniel Gildea.
2004.
Syntax-BasedAlignment: Supervised or Unsupervised?
In Proceed-ings of COLING 2004, the 20th International Confer-ence on Computational Linguistics, pages 418?424,Geneva, Switzerland, Aug 23?Aug 27.
COLING.Hao Zhang and Daniel Gildea.
2005.
Stochastic Lexical-ized Inversion Transduction Grammar for Alignment.In Proceedings of the 43rd Annual Meeting of theAssociation for Computational Linguistics (ACL?05),pages 475?482, Ann Arbor, Michigan, June.
Associa-tion for Computational Linguistics.2004
