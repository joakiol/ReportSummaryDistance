Proceedings of NAACL-HLT 2013, pages 85?94,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsExtracting the Native Language Signalfor Second Language AcquisitionBen SwansonBrown UniversityProvidence, RIchonger@cs.brown.eduEugene CharniakBrown UniversityProvidence, RIec@cs.brown.eduAbstractWe develop a method for effective extractionof linguistic patterns that are differentially ex-pressed based on the native language of theauthor.
This method uses multiple corporato allow for the removal of data set specificpatterns, and addresses both feature relevancyand redundancy.
We evaluate different rel-evancy ranking metrics and show that com-mon measures of relevancy can be inappro-priate for data with many rare features.
Ourfeature set is a broad class of syntactic pat-terns, and to better capture the signal we ex-tend the Bayesian Tree Substitution Grammarinduction algorithm to a supervised mixture oflatent grammars.
We show that this extensioncan be used to extract a larger set of relevantfeatures.1 IntroductionNative Language Identification (NLI) is a classifi-cation task in which a statistical signal is exploitedto determine an author?s native language (L1) fromtheir writing in a second language (L2).
This aca-demic exercise is often motivated not only by frauddetection or authorship attribution for which L1 canbe an informative feature, but also by its potential toassist in Second Language Acquisition (SLA).Our work focuses on the latter application and onthe observation that the actual ability to automati-cally determine L1 from text is of limited utility inthe SLA domain, where the native language of a stu-dent is either known or easily solicited.
Instead, thelikely role of NLP in the context of SLA is to pro-vide a set of linguistic patterns that students withcertain L1 backgrounds use with a markedly unusualfrequency.
Experiments have shown that such L1specific information can be incorporated into lessonplans that improve student performance (Laufer andGirsai, 2008; Horst et al2008).This is essentially a feature selection task with theadditional caveat that features should be individuallydiscriminative between native languages in order tofacilitate the construction of focused educational ex-cersizes.
With this goal, we consider metrics fordata set dependence, relevancy, and redundancy.
Weshow that measures of relevancy based on mutual in-formation can be inappropriate in problems such asours where rare features are important.While the majority of the methods that we con-sider generalize to any of the various feature setsemployed in NLI, we focus on the use of Tree Sub-stitution Grammar rules as features.
Obtaining acompact feature set is possible with the well knownBayesian grammar induction algorithm (Cohn andBlunsom, 2010), but its rich get richer dynamics canmake it difficult to find rare features.
We extend theinduction model to a supervised mixture of latentgrammars and show how it can be used to incorpo-rate linguistic knowledge and extract discriminativefeatures more effectively.The end result of this technique is a filtered list ofpatterns along with their usage statistics.
This pro-vides an enhanced resource for SLA research suchas Jarvis and Crossley (2012) which tackles the man-ual connection of highly discriminative features withplausible linguistic transfer explanations.
We outputa compact list of language patterns that are empiri-cally associated with native language labels, avoid-85ing redundancy and artifacts from the corpus cre-ation process.
We release this list for use by thelinguistics and SLA research communities, and planto expand it with upcoming releases of L1 labeledcorpora1.2 Related WorkOur work is closely related to the recent surge of re-search in NLI.
Beginning with Koppel et al2005),several papers have proposed different feature setsto be used as predictors of L1 (Tsur and Rappa-port, 2007; Wong and Dras, 2011a; Swanson andCharniak, 2012).
However, due to the ubiquitoususe of random subsamples, different data prepara-tion methods, and severe topic and annotation biasesof the data set employed, there is little consensus onwhich feature sets are ideal or sufficient, or if anyreported accuracies reflect some generalizable truthof the problem?s difficulty.
To combat the bias ofa single data set, a new strain of work has emergedin which train and test documents come from dif-ferent corpora (Brooke and Hirst, 2012; Tetreault etal, 2012; Bykh and Meurers, 2012).
We follow thiscross corpus approach, as it is crucial to any claimsof feature relevance.Feature selection itself is a well studied problem,and the most thorough systems address both rele-vancy and redundancy.
While some work tacklesthese problems by optimizing a metric over both si-multaneously (Peng et al2005), we decouple thenotions of relevancy and redundancy to allow ad-hocmetrics for either, similar to the method of Yu andLiu (2004).
The measurement of feature relevancyin NLI has to this point been handled primarily withInformation Gain, and elimination of feature redun-dancy has not been considered.Tree Substitution Grammars have recently beensuccessfully applied in several domains using theinduction algorithm presented by Cohn and Blun-som (2010).
Our hierarchical treatment builds onthis work by incorporating supervised mixtures overlatent grammars into this induction process.
Latentmixture techniques for NLI have been explored withother feature types (Wong and Dras, 2011b; Wongand Dras, 2012), but have not previously led to mea-surable empirical gains.1bllip.cs.brown.edu/download/nli corpus.pdf3 Corpus DescriptionWe first make explicit our experimental setup in or-der to provide context for the discussion to follow.We perform analysis of English text from Chinese,German, Spanish, and Japanese L1 backgroundsdrawn from four corpora.
The first three consist ofresponses to essay prompts in educational settings,while the fourth is submitted by users in an internetforum.The first corpus is the International Corpus ofLearner English (ICLE) (Granger et al2002), amainstay in NLI that has been shown to exhibit alarge topic bias due to correlations between L1 andthe essay prompts used (Brooke and Hirst, 2011).The second is the International Corpus of Crosslin-guistic Interlanguage (ICCI) (Tono et al2012),which is annotated with sentence boundaries and hasyet to be used in NLI.
The third is the public sampleof the Cambridge International Corpus (FCE), andconsists of short prompted responses.
One quirk ofthe FCE data is that several responses are written inthe form of letters, leading to skewed distributionsof the specialized syntax involved with use of thesecond person.
The fourth is the Lang8 data set in-troduced by Brooke and Hirst (2011).
This data setis free of format, with no prompts or constraints onwriting aids.
The samples are often very short andare qualitatively the most noisy of the four data sets.One distinctive experimental decision is to treateach sentence as an individual datum.
As documentlength can vary dramatically, especially across cor-pora, this gives increased regularity to the numberof features per data item.
More importantly, thiscreates a rough correspondence between feature co-occurrence and the expression of the same under-lying linguistic phenomenon, which is desirable forautomatic redundancy metrics.We automatically detect sentence boundarieswhen they are not provided, and parse all corporawith the 6-split Berkeley Parser.
As in previous NLIwork, we then replace all word tokens that do not oc-cur in a list of 614 common words with an unknownword symbol, UNK.While these are standard data preprocessing steps,from our experience with this problem we proposeadditional practical considerations.
First, we filterthe parsed corpora, retaining only sentences that are86parsed to a Clause Level2 tag.
This is primarily dueto the fact that automatic sentence boundary detec-tors must be used on the ICLE, Lang8, and FCE datasets, and false positives lead to sentence fragmentsthat are parsed as NP, VP, FRAG, etc.
The wild inter-net text found in the Lang8 data set al yields manynon-Clause Level parses from non-English text oremotive punctuation.
Sentence detection false neg-atives, on the other hand, lead to run-on sentences,and so we additionally remove sentences with morethan 40 words.We also impose a simple preprocessing step forbetter treatment of proper nouns.
Due to the geo-graphic distribution of languages, the proper nounsused in a writer?s text naturally present a strong L1signal.
The obvious remedy is to replace all propernouns with UNK, but this is unfortunately insuffi-cient as the structure of the proper noun itself canbe a covert signal of these geographical trends.
Tofix this, we also remove all proper noun left sistersof proper nouns.
We choose to retain the rightmostsister node in order to preserve the plurality of thenoun phrase, as the rightmost noun is most likelythe lexical head.From these parsed, UNKed, and filtered corporawe draw 2500 sentences from each L1 backgroundat random, for a total of 10000 sentences per corpus.The exception is the FCE corpus, from which wedraw 1500 sentences per L1 due to its small size.4 Tree Substitution GrammarsA Tree Substitution Grammar (TSG) is a modelof parse tree derivations that begins with a sin-gle ROOT nonterminal node and iteratively rewritesnonterminal leaves until none remain.
A TSGrewrite rule is a tree of any depth, as illustrated inFigure 1, and can be used as a binary feature of aparsed sentence that is triggered if the rule appearsin any derivation of that sentence.Related NLI work compares a plethora of sug-gested feature sets, ranging from character n-gramsto latent topic activations to labeled dependencyarcs, but TSG rules are best able to represent com-plex lexical and syntactic behavior in a homoge-neous feature type.
This property is summed upnicely by the desire for features that capture rather2S, SINV, SQ, SBAR, or SBARQROOTSNP VPVBZlovesNPNPDTtheNNNNmanNNwomanFigure 1: A Tree Substitution Grammar capable of de-scribing the feelings of people of all sexual orientations.than cover linguistic phenomena (Johnson, 2012);while features such as character n-grams, POS tagsequences, and CFG rules may provide a usable L1signal, each feature is likely covering some compo-nent of a pattern instead of capturing it in full.
TSGrules, on the other hand, offer remarkable flexibil-ity in the patterns that they can represent, potentiallycapturing any contiguous parse tree structure.As it is intractable to rank and filter the entire setof possible TSG rules given a corpus, we start withthe large subset produced by Bayesian grammar in-duction.
The most widely used algorithm for TSGinduction uses a Dirichlet Process to choose a subsetof frequently reoccurring rules by repeatedly sam-pling derivations for a corpus of parse trees (Cohnand Blunsom, 2010).
The rich get richer dynamic ofthe DP leads to the use of a compact set of rulesthat is an effective feature set for NLI (Swansonand Charniak, 2012).
However, this same propertymakes rare rules harder to find.To address this weakness, we define a generalmodel for TSG induction in labeled documents thatcombines a Hierarchical Dirichlet Process (Teh et al2005), with supervised labels in a manner similar toupstream supervised LDA (Mimno and McCallum,2008).
In the context of our work the document label?
indicates both its authors native language L anddata set D. Each ?
is associated with an observedDirichlet prior ?
?, and a hidden multinomial ??
overgrammars is drawn from this prior.
The traditionalgrammatical model of nonterminal expansion is aug-mented such that to rewrite a symbol we first choosea grammar from the document?s ??
and then choosea rule from that grammar.For those unfamiliar with these models, the basicidea is to jointly estimate a mixture distribution overgrammars for each ?, as well as the parameters ofthese grammars.
The HDP is necessary as the size87of each of these grammars is essentially infinite.
Wecan express the generative model formally by defin-ing the probability of a rule r expanding a symbol sin a sentence labeled ?
as??
?
Dir(??)zi?
?Mult(??
)Hs ?
DP (?, P0(?|s))Gks ?
DP (?s, Hs)ri?s ?
Gzi?sThis is closely related to the application of theHierarchical Pitman Yor Process used in (Blunsomand Cohn, 2010) and (Shindo et al2012), whichinterpolates between multiple coarse and fine map-pings of the data items being clustered to deal withsparse data.
While the underlying Chinese Restau-rant Process sampling algorithm is quite similar, ourapproach differs in that it models several differentdistributions with the same support that share a com-mon prior.By careful choice of the number of grammars K,the Dirichlet priors ?, and the backoff concentrationparameter ?, a variety of interesting models can eas-ily be defined, as demonstrated in our experiments.5 Feature Selection5.1 Dataset IndependenceThe first step in our L1 signal extraction pipelinecontrols for patterns that occur too frequently in cer-tain combinations of native language and data set.Such patterns arise primarily from the reuse of es-say prompts in the creation of certain corpora, andwe construct a hard filter to exclude features of thistype.A simple first choice would be to rank the rulesin order of dependence on the corpus, as we expectan irregularly represented topic to be confined to asingle data set.
However, this misses the subtle butimportant point that corpora have different qualitiessuch as register and author proficiency.
Instead wetreat the set of sentences containing an arbitrary fea-ture X as a set of observations of a pair of categor-ical random variables L and D, representing nativelanguage and data set respectively.To see why this treatment is superior, consider theoutcomes for the two hypothetical features shownL1 L2D1 1000 500D2 100 50L1 L2D1 1000 500D2 750 750Figure 2: Two hypothetical feature profiles that illustratethe problems with filtering only on data set independence,which prefers the right profile over the left.
Our methodhas the opposite preference.in Figure 2.
The left table has a high data set de-pendence but exhibits a clean twofold preference forL1 in both data sets, making it a desirable feature toretain.
Conversely, the right table shows a featurewhere the distribution is uniform over data sets, buthas language preference in only one.
This is a signof either a large variance in usage or some data setspecific tendency, and in either case we can not makeconfident claims as to this feature?s association withany native language.The L-D dependence can be measured with Pear-son?s ?2 test, although the specifics of its use asa filter deserve some discussion.
As we eliminatethe features for which the null hypothesis of inde-pendence is rejected, our noisy data will cause usto overzealously reject.
In order to prevent the un-neccesary removal of interesting patterns, we use avery small p value as a cutoff point for rejection.
Inall of our experiments the ?2 value corresponding top < .001 is in the twenties; we use ?2 > 100 as ourcriteria for rejection.Another possible source of error is the sparsity ofsome features in our data.
To avoid making pre-dictions of rules for which we have not observeda sufficient number of examples, we automaticallyexclude any rule with a count less than five for anyL-D combination ?.
This also satisfies the commonrequirements for validity of the ?2 test that requirea minimum number of 5 expected counts for everyoutcome.5.2 RelevancyWe next rank the features in terms of their ability todiscriminate between L1 labels.
We consider threerelevancy ranking metrics: Information Gain (IG),Symmetric Uncertainty (SU), and ?2 statistic.88IG SU ?2r .84 .72 .15Figure 3: Sample Pearson correlation coefficients be-tween different ranking functions and feature frequencyover a large set of TSG features.IG(L,Xi) = H(L)?H(L|Xi)SU(L,Xi) = 2IG(L,Xi)H(L) +H(Xi)?2(Xi) =?m(nim ?NiM )2NiMWe define L as the Multinomial distributed L1 la-bel taking values in {1, ...,M} andXi as a Bernoullidistributed indicator of the presence or absence ofthe ith feature, which we represent with the eventsX+i and X?i respectively.
We use the MaximumLikelihood estimates of these distributions from thetraining data to compute the necessary entropies forIG and SU.
For the ?2 metric we use nim, the countof sentences with L1 labelm that contain featureXi,and their sum over classes Ni.While SU is often preferred over IG in feature se-lection for several reasons, their main difference inthe context of selection of binary features is the addi-tion of H(Xi) in the denominator, leading to highervalues for rare features under SU.
This helps tocounteract a subtle preference for common featuresthat these metrics can exhibit in data such as ours, asshown in Figure 3.
The source of this preference isthe overwhelming contribution of p(X?i )H(L|X?i )in IG(L,Xi) for rare features, which will be essen-tially the maximum value of log(M).
In most clas-sification problems a frequent feature bias is a desir-able trait, as a rare feature is naturally less likely toappear and contribute to decision making.We note that binary features in sentences aresparsely observed, as the opportunity for use of themajority of patterns will not exist in any given sen-tence.
This leads to a large number of rare featuresthat are nevertheless indicative of their author?s L1.The ?2 statistic we employ is better suited to retainsuch features as it only deals with counts of sen-tences containing Xi.The ranking behavior of these metrics is high-lighted in Figure 4.
We expect that features withprofiles like Xa and Xb will be more useful thanthose like Xd, and only ?2 ranks these features ac-cordingly.
Another view of the difference betweenthe metrics is taken in Figure 5.
As shown in theleft plot, IG and SU are nearly identical for themost highly ranked features and significantly differ-ent from ?2.L1 L2 L3 L4 IG SU ?2Xa 20 5 5 5 .0008 .0012 19.29Xb 40 20 20 20 .0005 .0008 12.0Xc 2000 500 500 500 .0178 .0217 385.7Xd 1700 1800 1700 1800 .0010 .0010 5.71Figure 4: Four hypothetical features in a 4 label clas-sification problem, with the number of training itemsfrom each class using the feature listed in the first fourcolumns.
The top three features under each ranking areshown in bold.010203040500  10  20  30  40  50#ofsharedfeaturesTop n featuresX-IGX-SUSU-IG0501001502002503000  50  100  150  200  250  300Top n featuresX-IGX-SUSU-IGFigure 5: For all pairs of relevancy metrics, we show thenumber of features that appear in the top n of both.
Theresult for low n is highlighted in the left plot, showing ahigh similarity between SU and IG.5.3 RedundancyThe second component of thorough feature selectionis the removal of redundant features.
From an ex-perimental point of view, it is inaccurate to comparefeature selection systems under evaluation of the topn features or the number of features with rankingstatistic at or beyond some threshold if redundancyhas not been taken into account.
Furthermore, asour stated goal is a list of discriminative patterns,multiple representations of the same pattern clearly89degrade the quality of our output.
This is especiallynecessary when using TSG rules as features, as it ispossible to define many slightly different rules thatessentially represent the same linguistic act.Redundancy detection must be able to both deter-mine that a set of features are redundant and alsoselect the feature to retain from such a set.
We usea greedy method that allows us to investigate differ-ent relevancy metrics for selection of the representa-tive feature for a redundant set (Yu and Liu, 2004).The algorithm begins with a list S containing thefull list of features, sorted by an arbitrary metric ofrelevancy.
While S is not empty, the most relevantfeature X?
in S is selected for retention, and all fea-tures Xi are removed from S if R(X?, Xi) > ?
forsome redundancy metric R and some threshold ?.We consider two probabilistic metrics for redun-dancy detection, the first being SU, as defined inthe previous section.
We contrast this metric withNormalized Pointwise Mutual Information (NPMI)which uses only the events A = X+a and B = X+band has a range of [-1,1].NPMI(Xa, Xb) =log(P (A|B))?
log(P (A))?
log(P (A,B))Another option that we explore is the structuralredundancy between TSG rules themselves.
We de-fine a 0-1 redundancy metric such that R(Xa, Xb) isone if there exists a fragment that contains both Xaand Xb with a total number of CFG rules less thanthe sum of the number of CFG rules in Xa and Xb.The latter constraint ensures that Xa and Xb overlapin the containing fragment.
Note that this is not thesame as a nonempty set intersection of CFG rules,as can be seen in Figure 6.SNPNNVPSNPPRPVPSNP VPVBZFigure 6: Three similar fragments that highlight the be-havior of the structural redundancy metric; the first twofragments are not considered redundant, while the thirdis made redundant by either of the others.6 Experiments6.1 Relevancy MetricsThe traditional evaluation criterion for a feature se-lection system such as ours is classification accuracyor expected risk.
However, as our desired output isnot a set of features that capture a decision bound-ary as an ensemble, a per feature risk evaluation bet-ter quantifies the performance of a system for ourpurposes.
We plot average risk against number ofpredicted features to view the rate of quality degra-dation under a relevancy measure to give a pictureof a each metric?s utility.The per feature risk for a feature X is an eval-uation of the ML estimate of PX(L) = P (L|X+)from the training data on TX , the test sentences thatcontain the feature X .
The decision to evaluate onlysentences in which the feature occurs removes animplicit bias towards more common features.We calculate the expected risk R(X) using a 0-1loss function, averaging over TX .R(X) =1|TX |?t?TXPX(L 6= L?t )where L?t is the gold standard L1 label of test itemt.
This metric has two important properties.
First,given any true distribution over class labels in TX ,the best possible PX(L) is the one that matchesthese proportions exactly, ensuring that preferredfeatures make generalizable predictions.
Second, itassigns less risk to rules with lower entropy, as longas their predictions remain generalizable.
This cor-responds to features that find larger differences inusage frequency across L1 labels.The alternative metric of per feature classifica-tion accuracy creates a one to one mapping betweenfeatures and native languages.
This unnecessarilypenalizes features that are associated with multiplenative languages, as well as features that are selec-tively dispreferred by certain L1 speakers.
Also, wewish to correctly quantify the distribution of a fea-ture over all native languages, which goes beyondcorrect prediction of the most probable.Using cross validation with each corpus as a fold,we plot the average R(X) for the best n featuresagainst n for each relevancy metric in Figure 7.
Thisclearly shows that for highly ranked features ?2 is900.690.70.710.720.730.740.750  20  40  60  80  100  120  140  160  180  200AverageExpectedLossTop n featuresX2IGSUFigure 7: Per-feature Average Expected Loss plottedagainst top N features using ?2, IG, and SU as a rele-vancy metricable to best single out the type of features we de-sire.
Another point to be taken from the plot isthat it is that the top ten features under SU areremarkably inferior.
Inspection of these rules re-veals that they are precisely the type of overly fre-quent but only slightly discriminative features thatwe predicted would corrupt feature selection usingIG based measures.6.2 Redundancy MetricsWe evaluate the redundancy metrics by using the topn features retained by redundancy filtering for en-semble classification.
Under this evaluation, if re-dundancy is not being effectively eliminated perfor-mance should increase more slowly with n as theset of test items that can be correctly classified re-mains relatively constant.
Additionally, if the metricis overzealous in its elimination of redundancy, use-ful patterns will be eliminated leading to diminishedincrease in performance.
Figure 8 shows the tradeoffbetween Expected Loss on the test set and the num-ber of features used with SU, NPMI, and the overlapbased structural redundancy metric described above.We performed a coarse grid search to find the opti-mal values of ?
for SU and NPMI.Both the structural overlap hueristic and SU per-form similarly, and outperform NPMI.
Analysis re-veals that NPMI seems to overstate the similarity oflarge fragments with their small subcomponents.
Wechoose to proceed with SU, as it is not only faster inour implementation but also can generalize to fea-ture types beyond TSG rules.0.660.670.680.690.70.710.720.730  50  100  150  200  250  300ExpectedLossTop n featuresoverlapSUNPMIFigure 8: The effects of redundancy filtering on classi-fication performance using different redundancy metrics.The cutoff values (?)
used for SU and NPMI are .2 and .7respectively.6.3 TSG InductionWe demonstrate the flexibility and effectiveness ofour general model of mixtures of TSGs for labeleddata by example.
The tunable parameters are thenumber of grammars K, the Dirichlet priors ??
overgrammar distributions for each label ?, and the con-centration parameter ?
of the smoothing DP.For a first baseline we set the number of grammarsK = 1, making the Dirichlet priors ?
irrelevant.With a large ?
= 1020, we essentially recover thebasic block sampling algorithm of Cohn and Blun-som (2010).
We refer to this model as M1.
Oursecond baseline model, M2, sets K to the number ofnative language labels, and sets the ?
variables suchthat each ?
is mapped to a single grammar by its L1label, creating a naive Bayes model.
For M2 andthe subsequent models we use ?
= 1000 to allowmoderate smoothing.We also construct a model (M3) in which we setK = 9 and ??
is such that three grammars are likelyfor any single ?
; one shared by all ?
with the sameL1 label, one shared by all ?
with the same corpuslabel, and one shared by all ?.
We compare this withanother K = 9 model (M4) where the ?
are set tobe uniform across all 9 grammars.We evaluate these systems on the percent of theirresulting grammar that rejects the hypothesis of lan-guage independence using a ?2 test.
Slight adjust-ments were made to ?
for these models to bringtheir output grammar size into the range of approxi-mately 12000 rules.
We average our results for eachmodel over single states drawn from five indepen-91p < .1 p < .05 p < .01 p < .001M1 56.5(3.1) 54.5(3.0) 49.8(2.7) 45.1(2.5)M2 55.3(3.7) 53.7(3.6) 49.1(3.3) 44.7(3.0)M3 59.0(4.1) 57.2(4.1) 52.4(3.6) 48.4(3.3)M4 58.9(3.8) 57.0(3.7) 51.9(3.4) 47.2(3.1)Figure 9: The percentage of rules from each model thatreject L1 independence at varying levels of statistical sig-nificance.
The first number is with respect to the numberrules that pass the L1/corpus independence and redun-dancy tests, and the second is in proportion to the full listreturned by grammar induction.dent Markov chains.Our results in Figure 9 show that using a mixtureof grammars allows the induction algorithm to findmore patterns that fit arbitrary criteria for languagedependence.
The intuition supporting this is that insimpler models a given grammar must represent alarger amount of data that is better represented withmore vague, general purpose rules.
Dividing the re-sponsibility among several grammars lets rare pat-terns form clusters more easily.
The incorporation ofinformed structure in M3 further improves the per-formance of this latent mixture technique.7 DiscussionUsing these methods, we produce a list of L1 as-sociated TSG rules that we release for public use.We perform grammar induction using model M3,apply our data dependence and redundancy filters,rank for relevancy using ?2 and filter at the level ofp < .1 statistical significance for relevancy.
Eachentry consists of a TSG rule and its matrix of countswith each ?.
We provide the total for each L1 la-bel, which shows the overall prediction of the pro-portional use of that item.
We also provide the ?2statistics for L1 dependence and the dependence ofL1 and corpus.It is speculative to assign causes to the discrimi-native rules we report, and we leave quantificationof such statements to future work.
However, thestrength of the signal, as evidenced by actual countsin data, and the high level interpretation that can beeasily assigned to the TSG rules is promising.
Asunderstanding the features requires basic knowledgeof Treebank symbols, we provide our interpretationsfor some of the more interesting rules and summa-rize their L1 distributions.
Note that by describing arule as being preferred by a certain set of L1 labels,our claim is relative to the other labels only; the truecause could also be a dispreference in the comple-ment of this set.One interesting comparison made easy by ourmethod is the identification of similar structures thathave complementary L1 usage.
An example is theuse of a prepositional phrase just before the firstnoun phrase in a sentence, which is preferred in Ger-man and Spanish, especially in the former.
However,German speakers disprefer a prepositional phrasefollowed by a comma at the beginning of the sen-tence, and Chinese speakers use this pattern morefrequently than the other L1s.
Another contrastablepair is the use of the word ?because?
with upper orlower case, signifying sentence initial or medial use.The former is preferred in Chinese and Japanesetext, while the latter is preferred in German and evenmore so in Spanish L1 data.As these examples suggest, the data shows astrong division of preference between Europeanand Asian languages, but many patterns exist thatare uniquely preferred in single languages as well.Japanese speakers are seen to frequently use a per-sonal pronoun as the subject of the sentence, whileSpanish speakers use the phrase ?the X of Y?, theverb ?go?, and the determiner ?this?
with markedlyhigher frequency.
Germans tend to begin sentenceswith adverbs, and various modal verb constructionsare popular with Chinese speakers.
We suspect thesepatterns to be evidence of preference in the speci-fied language, rather than dispreference in the otherthree.Our strategy in regard to the hard filters for L1-corpus dependence and redundancy has been to pre-fer recall to precision, as false positives can be easilyignored through subsequent inspection of the datawe supply.
This makes the list suitable for humanqualitative analysis, but further work is required forits use in downstream automatic systems.8 ConclusionThis work contributes to the goal of leveraging NLIdata in SLA applications.
We provide evidence for92our hypothesis that relevancy metrics based on mu-tual information are ill-suited for this task, and rec-ommend the use of the ?2 statistic for rejecting thehypothesis of language independence.
Explicit con-trols for dependence between L1 and corpus areproposed, and redundancy between features are ad-dressed as well.
We argue for the use of TSG rules asfeatures, and develop an induction algorithm that isa supervised mixture of hierarchical grammars.
Thisgeneralizable formalism is used to capture linguisticassumptions about the data and increase the amountof relevant features extracted at several thresholds.This project motivates continued incorporation ofmore data and induction of TSGs over these largerdata sets.
This will improve the quality and scope ofthe resulting list of discriminative syntax, allowingbroader use in linguistics and SLA research.
Theprospect of high precision and recall in the extrac-tion of such patterns suggests several interesting av-enues for future work, such as determination of theactual language transfer phenomena evidenced by anarbitrary count profile.
To achieve the goal of auto-matic detection of plausible transfer the native lan-guages themselves must be considered, as well as away to distinguish between preference and dispref-erence based on usage statistics.
Another excitingapplication of such a refined list of patterns is theautomatic integration of its features in L1 targetedSLA software.ReferencesPhil Blunsom and Trevor Cohn.
2010.
UnsupervisedInduction of Tree Substitution Grammars for Depen-dency Parsing.
Empirical Methods in Natural Lan-guage Processing.Julian Brooke and Graeme Hirst.
2011.
Native languagedetection with ?cheap?
learner corpora.
Conference ofLearner Corpus Research.Julian Brooke and Graeme Hirst.
2012.
Measuring In-terlanguage: Native Language Identification with L1-influence Metrics.
LRECJulian Brooke and Graeme Hirst.
2012.
Robust, Lexical-ized Native Language Identification.
COLING.Serhiy Bykh and Detmar Meurers.
2012.
Native Lan-guage Identification Using Recurring N-grams - Inves-tigating Abstraction and Domain Dependence.
COL-ING.Trevor Cohn, Sharon Goldwater, and Phil Blunsom.2009.
Inducing Compact but Accurate Tree-Substitution Grammars.
In Proceedings NAACL.Trevor Cohn, and Phil Blunsom.
2010.
Blocked infer-ence in Bayesian tree substitution grammars.
Associa-tion for Computational Linguistics.Gilquin, Gae?tanelle and Granger, Sylviane.
2011.
FromEFL to ESL: Evidence from the International Corpusof Learner English.
Exploring Second-Language Va-rieties of English and Learner Englishes: Bridging aParadigm Gap (Book Chapter).Joshua Goodman.
2003.
Efficient parsing of DOP withPCFG-reductions.
In Bod et alchapter 8..S. Granger, E. Dagneaux and F. Meunier.
2002.
Interna-tional Corpus of Learner English, (ICLE).Horst M., White J., Bell P. 2010.
First and second lan-guage knowledge in the language classroom.
Interna-tional Journal of Bilingualism.Scott Jarvis and Scott Crossley 2012.
Approaching Lan-guage Transfer through Text Classification.Mark Johnson 2011.
How relevant is linguistics to com-putational linguistics?.
Linguistic Issues in LanguageTechnology.Ekaterina Kochmar.
2011.
Identification of a writer?snative language by error analysis.
Master?s Thesis.Koppel, Moshe and Schler, Jonathan and Zigdon, Kfir.2005.
Determining an author?s native language bymining a text for errors.
Proceedings of the eleventhACM SIGKDD international conference on Knowl-edge discovery in data mining.Laufer, B and Girsai, N. 2008.
Form-focused Instructionin Second Language Vocabulary Learning: A Case forContrastive Analysis and Translation.
Applied Lin-guistics.David Mimno and Andrew McCallum.
2008.
TopicModels Conditioned on Arbitrary Features withDirichlet-multinomial Regression.
UAI.Hanchuan Peng and Fuhui Long and Chris Ding.
2005.Feature selection based on mutual information cri-teria of max-dependency, max-relevance, and min-redundancy.
IEEE Transactions on Pattern Analysisand Machine Intelligence.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning Accurate, Compact, and In-terpretable Tree Annotation.
Association for Compu-tational Linguistics.Matt Post and Daniel Gildea.
2009.
Bayesian Learningof a Tree Substitution Grammar.
Association for Com-putational Linguistics.Tono, Y., Kawaguchi, Y.
& Minegishi, M.
(eds.)
.
2012.Developmental and Cross-linguistic Perspectives inLearner Corpus Research..Oren Tsur and Ari Rappoport.
2007.
Using classifierfeatures for studying the effect of native language onthe choice of written second language words.
CACLA.93Shindo, Hiroyuki and Miyao, Yusuke and Fujino, Aki-nori and Nagata, Masaaki 2012.
Bayesian Symbol-Refined Tree Substitution Grammars for SyntacticParsing.
Association for Computational Linguistics.Ben Swanson and Eugene Charniak.
2012.
NativeLanguage Detection with Tree Substitution Grammars.Association for Computational Linguistics.Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, andDavid M. Blei.
2005.
Hierarchical Dirichlet Pro-cesses.
Journal of the American Statistical Associa-tion.Joel Tetreault, Daniel Blanchard, Aoife Cahill, BeataBeigman-Klebanov and Martin Chodorow.
2012.
Na-tive Tongues, Lost and Found: Resources and Em-pirical Evaluations in Native Language Identification.COLING.Sze-Meng Jojo Wong and Mark Dras.
2009.
Contrastiveanalysis and native language identification.
Proceed-ings of the Australasian Language Technology Associ-ation Workshop.Sze-Meng Jojo Wong and Mark Dras.
2011.
Exploit-ing Parse Structures for Native Language Identifica-tion.
Proceedings of the 2011 Conference on Empiri-cal Methods in Natural Language Processing.Sze-Meng Jojo Wong and Mark Dras.
2011.
Topic Mod-eling for Native Language Identification.
Proceedingsof the Australasian Language Technology AssociationWorkshop.Sze-Meng Jojo Wong, Mark Dras, Mark Johnson.
2012.Exploring Adaptor Grammars for Native LanguageIdentification.
EMNLP-CoNLL.Lei Yu and Huan Liu.
2004.
Efficient Feature Selectionvia Analysis of Relevance and Redundancy.
Journalof Machine Learning Research.94
