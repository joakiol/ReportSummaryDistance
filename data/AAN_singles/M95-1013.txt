CRL/NMSUDescription of the CRL/NMSU Systems Used for MUC- 6Jim CowieComputing Research LaboratoryNew Mexico State Universit yjcowie@nmsu.edu(505) 646-5181IntroductionThis paper discusses the two CRL named entity recognition systems submitted for MUC-6 .
Thesystems are based on entirely different approaches .
The first is a data intensive method, whichuses human generated patterns .
The second uses the training data to develop decision trees whic hdetect the start and end points of names .BackgroundCRL submitted two systems for the Named Entity task .
One of these (Basic) is an improved ver-sion of the CRL name recognizer developed in phase one of Tipster[1].
The second (AutoLearn)is a system which learns automatically from training data .
The Basic system had approximatel ysix man months of work in its original development .
Improvements for MUC-6 were carried outby one graduate student (about one man month) .
The AutoLearn system was developed by onegraduate student specifically for MUC-6 (also one man month) .AvailabilityThe Basic system can be accessed for testing through the mail server - ne_tag@crl .nmsu.edu.
thesubject field of the message should consist of the word "tag" .
A web interface to the tagger is als oavailable from CRL's home page .
Source code and data files can be ftp'ed from CRL (mai lcable@crl .nmsu.edu for ftp address and access password) .
The system has been successfull ytested on Linux running on a PC.Basic NE SystemThe Basic system consists of a pipeline of Unix process .
These can be identified as carrying outfour different types of task .1.
Recognizing names based on character patterns (numbers, dates) .2.
Recognizing names based on pre-stored names .3.
Applying mark-up to potential components of complete names .4.
Recognizing names based on patterns of components .157For the most part the system is context free .
A few of the patterns used require some additiona lcontext before a name is recognized .
For example an ambiguous human name in isolation may berecognized if it is followed closely by a title .The system consists of suite of `C' and lex programs .Component units recognized by the system are cities, provinces, countries, company prefixes an dsuffixes, company beginning and ending words (Club, Association etc .
), unambiguous and ambig-uous human first and last names, titles and human position words .Additional Fix-up ProceduresFinal patterns are used to join together units of the same type which are immediately next to eac hother in the text.After all the pattern based procedures have operated on the text a final pass is made to recogniz eabbreviated forms of names .
This takes the lists of names found so far and truncates them .
(rightto left for person and left to right for companies) .
These new lists are then used as lists of knownorganizations and persons and any occurrences of these in the text are marked .
In particular fororganizations the headline is not processed apart from this last stage .
This avoids recognition o forganizations such as "Leaves Bank" .
The assumption that names mentioned in the heading wil lbe repeated in the body of the text holds almost universally .Data SourcesThe data used in the Basic system is derived from public domain source, university phone lists ,the Tipster Gazetteer and government data-bases of company names .PerformanceThe performances of the for the test set and for the walk through article are given in Appendix A .Overall performance was Recall - 85% and Precision - 87% giving an F measure of 85 .8 .Walk through articlePerformance here was Recall - 63% and Precision - 83% .The main source of error was missing patterns in the system .
For example Robert L .
James waspartially recognized (as L .
James), McCann-Erickson was missed as no hyphenated company pat -tern had been added.
Once a frequently mentioned name is ignored in its full form the syste munfortunately misses all abbreviated forms .
This article also shows the importance of context i nreliably recognizing some names (e .g.
an analyst with PayneWebber) .158AutoLearn NE SystemThe AutoLearn system was developed to explore the possibility of using simple learning algo-rithms to detect specific features in text .
An implementation of Quinlan's ID3 Algorithm was used[2,3] .
This algorithm constructs a decision tree which decides whether an element of a collectio nsatisfies a property or not .
Each element of a collection has a finite number of attributes each o fwhich may take one of several values .
Quinlan's original paper suggests the range of values of th eattributes should be "small".
In the case of the AutoLearn system the values are every word occur-ring in the training collection .Collections for Name Recognitio nIn order to apply the ID3 algorithm the data needs to be structured into a collection, each membe rof which has specific values for a set of attributes and for each of which it is known whether th emember has a specific property or not .
For the name recognition problem the training data wa sconverted in tuple of five words .
Each tuple was marked as having the start (or end) of a specifi ctype of proper name at the middle word of the tuple .
This data can be easily generated from thetraining articles.
Thus for the beginning of a person -many differences between Robert L .
- 1differences between Robert L. James 1between Robert L. James , -1etc .Fourteen sets of training data were generated using the 318 development articles supplied forMUC-6.
The quality of the tagging is not particularly uniform, but no attempt has been made toimprove this .Generating the decision treesAs each word of the training data is read it is hashed and stored in a hash table .
Thereafter wordsare referred to by their hash values .
For each of the values of the five attributes (words 1 through5) a count is maintained of the number of times this value contributed to an element holding aproper named occurrence at the middle attribute.
The attribute to be tested first is chosen by com-puting for each value the relative frequency of positive and negative outcomes for this value .
Thisis used to approximate the information content of that attribut e-p+log2p+ - plog2p-(EQ 1 )The sum of the approximate information contents for each column is calculated and the colum nwith the highest value is chosen as the primary decision .
Here all the values which always contrib -uted to a positive outcome are used as the primary decision .
Values which are always negative areignored (this is primarily to reduce the size of the data being handled) .
New sub-collections areformed with elements containing one value which contributed both to positive or negative out-comes are collected and the tree building process is repeated for each of these new collections .159The decision trees thus formed can be output in a readable if somewhat lengthy form .
In mostcases the first choice is the third word in a group taking one of a large number of values .
Thereaf-ter a group off fairly impenetrable tests occur.
For example for location beginnings -If word 3 is one of the following - Milwaukee Ridgefild Pa ST. .
(around300 more words) then location_beginningelse if word 3 is Illinois and word 1 is Indiana then location_beginnin gelse if word 3 is Northeast and word 1 is `in' then location_beginnningThe printed decision table takes about 5 pages .Running the AutoLearn SystemA pass through the texts is made for each decision tree (beginning and end) of each named entity .First the hash table of words is read and the corresponding decision tree .
The text is then pro-cessed in groups of five words .
Whenever a positive decision is made a new tag is added to th eoutput stream .Ideally at this stage the tagging would be done .
However, given that we are processing new texts ,there are many occasions where an end or a beginning is identified, but the corresponding begin-ning or end is not.
For example a surname may have been seen previously, but not the attachedforename.
At this point a heuristic is applied which for every un-matched bracket in the text work sforward or backward until some appropriate point is reached .
The actual skipping heuristics needto be different for organizations, persons, locations, dates and numbers .Data SourcesThe only data source used for the AutoLearn system was the 318 MUC-6 training texts .PerformanceA high precision was expected from this system .
Most of the errors that occur are due to failuresof the bracket insertion heuristic .
The overall scores were Recall - 47% and Precision- 81% givingan F measure of 59 .3 .No specific code was inserted to handle numbers or dates .
The method was more successful wit horganizations and locations then with persons .
More training data is perhaps required to make th esystem aware of the spread of examples for human names .Walk through articleThe performance here was Recall - 36% and Precision - 88% .160The major problem here is that the system has not learned a rule which uses "Mr ."
to identify theword previous to a name.Relationship of Performance to Amount of Training Data .The evaluation texts were processed with decision trees generated using subsets of the MUC-6development data .
This was done in increasing units of 50 texts .
The results are shown in Figure 1below.
Both recall and precision increase with increasing training data .
Precision appears to tailoff at around 82%.
Recall, however, increases (with one exception) steadily over the whole range .score75 .0 070.0 065 .0 060.00.r -55 .0 085 .0 0x0.00.m35 .0030 .00Wumbcr of Training Fil m0.00100 .00200.00300.0 0FIGURE 1 .
Relationship of Performance to Amount of TrainingFuture DevelopmentsWe intend to rebuild the Basic system.
One of the principle drawbacks of the system is its sequen -tial application of component tags .
In many cases a second tag is not applied because the word o r5 0.0 045.0040 .00161phrase is ambiguous .
The correct solution here is to apply all tags in a manner that allows the cor-rect tags to be selected by the pattern processing mechanisms .
In addition we plan to improve ourcollection of patterns.
The current version of the system is being made generally available.
This ,we hope, will provide us with some feedback on patterns and errors in the data files .Some further experiments are also planned with the AutoLearn system .
The main drawback withthe system is that it doesn't make maximal use of the training data in so far that with small train-ing samples one word may be sufficient to make a decision .
This situation can probably beimproved by replacing specific words with a NULL word .
This will force he system to develo prules based more on context .
In particular when the system encounters unknown words these willbe considered equivalent to the NULL word.We also intend to apply the learning method described here to other NLP tasks such as part o fspeech tagging and disambiguation .References[1] Cowie, J .,Guthrie, L., Pustejovsky, J ., Waterman, S., and Wakao, T., The CRL/Bradneis Sys-tem us Used for MUC-5 In Proceedings of the Fifth Message Understanding Conference (MUC-5) Baltimore, Ma ., Morgan Kaufmann, 1993 .
[2] Quinlan, J .R.
Discovering Rules by Induction from Large Collections of Examples .In ExpertSystems in the Micro-Electronic Age, ed Michie D ., Edinburgh University Press, 1979 .
[3] Quinlan, J .R.
Machine Learning: Easily Understood Decision Rules .In Computer Systemsthat Learn, eds.
Weiss S .M.
and Kulikowski C .A., Morgan Kaufmann, 1991 .162Appendix A - Basic System Scores-------------------------- --------------- ------------ ----------------------- -SLOTPOS ACT' COR PAR INC!
SPU MIS NONI REC PRE UND OVG ERR SUB<enamex>9258891 841001 48 8401 91 9595 140type9258891 7840571 48 8401 85 8895 197text9258891 7550861 48 8401 82 8595 22 1 0subtotals1850 17781 15390 1431 96 16801 83 8695 218<timex>1111081 1020016901 92 9486 130type1111081 1020016901 92 9486 130text11110819201016901 83 8586 21 1 0subtotals2222161 1940101 12 1801 87 9086 175<numex>93102191001 11201 98 892 11 120type93102191001 11201 98 892 11 120text93102188031 11201 95 862 11 153subtotals1862041 179031 22401 96 882 11 142ALL OBJECTS2258 21981 19120 1561 130 19001 85 8786 208MATCHED ONLY2068 20681 19120 15610001 92 920088-------------------------- --------------- ------------ ------------------------P&R2P&RP&2RF-MEASURES85 .8286 .5285 .13SLOT POS ACTI COR PAR INC' SPU MIS NON' REC PRE UND OVG ERR SUB-------------------------- --------------- ------------ ----------------------- -Enamex :organization4423921 3300401 22 7201 75 84 166 29 11person3733781 353091 16 1101 95 933492location1101191 101081 10101 92 8518 167other00100010001******Timex :date1111081 1020016901 92 9486 130time00100010001******other00100010001******Numex :money76771740013201 97 963460percent17251170018001 100 680 32 320other00100010001******* * * DOCUMENT SECTION SCORES * * *SLOTPOS ACTT COR PAR INCI SPU MIS NONI REC PRE UND OVG ERR SUBHL1361301 11501114 1001 84 8873 189DD60601600010001 100 1000000DATELINE52501490110201 94 984062TXT2010 19581 16880 1441 126 17801 84 8696 218163Appendix A - Autolearn System Score sSLOT POS ACT' COR PAR INCI SPU MIS NON' REC PRE UND OVG ERR SUB<enamex> 915 5141 469 0 0145 446 01 51 91 49 9 51 0type 915 5141 445 0 24145 446 01 49 86 49 9 54 5text 915 5141 371 0 98145 446 01 40 72 49 9 61 2 1subtotals 1830 10281 816 0 122190 892 01 44 79 49 9 58 1 3<timex> 111 651 59 0 016 52 01 53 91 47 9 50 0type 111 651 59 0 016 52 01 53 91 47 9 50 0text 111 651 48 0 1116 52 01 43 74 47 9 59 19subtotals 222 1301 107 0 11112 104 01 48 82 47 9 54 9<numex> 93 721 65 0 017 28 01 70 90 30 10 35 0type 93 721 65 0 017 28 01 70 90 30 10 35 0text 93 721 63 0 217 28 01 68 88 30 10 37 3subtotals 186 1441 128 0 2114 56 01 69 89 30 10 36 2ALL OBJECTS 2238 13021 1051 0 1351116 1052 01 47 81 47 9 55 1 1MATCHED ONLY 1186 11861 1051 0 13510 0 01 89 89 0 0 11 11-------------------------- --------------- ------------ ----------------------- -P&R2P&RP&2RF-MEASURES59 .3870 .5751 .2 5-------------------------- --------------- ------------ ----------------------- -SLOTPOS ACTI COR PAR INC' SPU MIS NON' REC PRE UND OVG ERR SUBEnamex :organization4332871 251081 28 17401 58 87 40 10 463person3721551 1310141 10 22701 35 84 616 66 10location110721630217 4501 57 88 41 10 463other00100010001******Timex :date111651590016 5201 53 91 479 500time00100010001******other00100010001******Numex :money76551520013 2401 68 94 325 340percent17171130014401 76 76 24 24 380other00100010001****** * * DOCUMENT SECTION SCORES * * *SLOTPOS ACTI COR PAR INCI SPU MIS NON' REC PRE UND OVG ERR SUBHL1289216500 71 20 5601 51 71 44 22 56 1 0DD600100010 60010* 100* 100*DATELINE52241240010 2801 46 100 540 540TXT1998 11861 9620 1281 96 90801 48 81 458 54 12164Appendix A - Basic System Walk-through Score sSLOTPOSACTICORPARINCI SPU MIS NONI REC PRE UND OVG ERR SUB-------------------------- --------------- ----------------------------------- -<enamex> 66 521 47 0 01 5 19 01 71 90 29 10 34 0type 66 521 42 0 51 5 19 01 64 81 29 10 41 11text 66 521 42 0 51 5 19 Cl 64 81 29 10 41 11subtotals 132 1041 84 0 101 10 38 01 64 81 29 10 41 11<timex> 6 51 5 0 01 0 1 01 83 100 17 0 17 0type 6 51 5 0 01 0 1 01 83 100 17 0 17 0text 6 51 5 0 01 0 1 01 83 100 17 0 17 0subtotals 12 101 10 0 01 0 2 01 83 100 17 0 17 0<numex> 6 71 6 0 01 1 0 01 100 86 0 14 14 0type 6 71 6 0 01 1 0 01 100 86 0 14 14 0text 6 71 6 0 01 1 0 01 100 86 0 14 14 0subtotals 12 141 12 0 01 2 0 01 100 86 0 14 14 0ALL OBJECTS 156 1281 106 0 101 12 40 01 68 83 26 9 37 9MATCHED ONLY 116 1161 106 0 101 0 0 01 91 91 0 0 9 9P&R2P&RF-MEASURESP&2R70 .4 874 .65 79 .34-------------------------- --------------- ------------SPU MIS NONI------------------------REC PRE UND OVG ERR SUBSLOTEnamex :POS ACTT COR PAR INC Iorganization 29 121 5 0 51 21901 17 42 66 17 84 50person 35 381 35 0 01 3001 100 92 0 8 8 0location 2 21 2 0 01 0001 100 100 0 0 0 0other 0 01 0 0 01 0001 * * * * * *Timex :date 6 51 5 0 01 0101 83 100 17 0 17 0time 0 o l 0 0 o l 00o l * * * * * *other 0 OI 0 0 O1 00OI * * * * * *Numex :money 5 61 5 0 01 1001 100 83 0 17 17 0percent 1 11 1 0 01 0001 100 100 0 0 0 0other 0 01 0 0 01 0001 * * * * * *-------------------------- --------------- ------------ ------------------------* * * DOCUMENT SECTION SCORES * * *SLOTPOS ACTI COR PAR INCI SPU MIS NONI REC PRE UND OVG ERR SUBHL86160010201 75 100 250 250DD22120010001 100 1000000DATELINE00100010001*****TXT1461201980101 12 3801 67 82 26 10 389165Appendix A - Autolearn System Walk-through Score sSLOTPOS ACT' COR PAR INC' SPU MIS NON' REC PRE UND OVG ERR SUB+	 +	 +<enamex> 69 221 20 0 01 2 49 01 29 91 71 9 72 0type 69 221 20 0 01 2 49 01 29 91 71 9 72 0text 69 221 18 0 21 2 49 01 26 82 71 9 75 10subtotals 138 441 38 0 21 4 98 01 28 86 71 9 73 5<timex> 6 51 5 0 01 0 1 01 83 100 17 0 17 0type 6 51 5 0 01 0 1 01 83 100 17 0 17 0text 6 51 4 0 11 0 1 01 67 80 17 0 33 2 0subtotals 12 101 9 0 11 0 2 01 75 90 17 0 25 10<numex> 6 61 6 0 01 0 0 01 100 100 0 0 0 0type 6 61 6 0 01 0 0 01 100 100 0 0 0 0text 6 61 5 0 11 0 0 01 83 83 0 0 17 17subtotals 12 121 11 0 11 0 0 01 92 92 0 0 8 8+	 +	 +ALL OBJECTS162661580414 10001 36 88 626 656MATCHED ONLY62621580410001 94 940066+	 +	 +P&R2P&RP&2RF-MEASURES50 .8868 .0840 .6 2+	 +	 +SLOTPOS ACT' COR PAR INC'SPU MIS NONI REC PRE UND OVG ERR SUB+	 +	 +Enamex :organization 32 121 12 0 01 0 20 01 38 100 62 0 620person 35 71 6 0 01 1 29 01 17 86 83 14 830location 2 31 2 0 01 1 0 01 100 67 0 33 330other 0 01 0 0 01 0 0 01 * * * * *+	 +	 +Timex :date 6 51 5 0 01 0 1 01 83 100 17 0 170time 0 01 0 0 01 0 0 01 * * * * **other 0 01 0 0 01 0 0 01 * * * *+	 +	 +Numex :money 5 51 5 0 01 0 0 01 100 100 0 0 00percent 1 11 1 0 01 0 0 01 100 100 0 0 00other 0 01 0 0 01 0 0 01 * * * * **+	 +	 +* * * DOCUMENT SECTION SCORES * * *+	 +	 +SLOTPOS ACT' COR PAR INC' SPU MIS NON' REC PRE UND OVG ERR SUB+	 +	 +HL 8 21 2 0 01 0 6 01 25 100 75 0 750DD 2 01 0 0 01 0 2 01 0 * 100 * 100*DATELINE 0 01 0 0 01 0 0 01 * * * * *TXT 152 641 56 0 41 4 92 01 37 88 60 6 647166
