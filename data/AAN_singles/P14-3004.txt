Proceedings of the ACL 2014 Student Research Workshop, pages 26?33,Baltimore, Maryland USA, June 22-27 2014.c?2014 Association for Computational LinguisticsAnalyzing Positions and Topics in Political Discussionsof the German BundestagC?acilia ZirnData and Web Science GroupUniversity of MannheimGermanycaecilia@informatik.uni-mannheim.deAbstractWe present ongoing doctoral work on au-tomatically understanding the positions ofpoliticians with respect to those of theparty they belong to.
To this end, we usetextual data, namely transcriptions of po-litical speeches from meetings of the Ger-man Bundestag, and party manifestos, inorder to automatically acquire the posi-tions of political actors and parties, respec-tively.
We discuss a variety of possible su-pervised and unsupervised approaches todetermine the topics of interest and com-pare positions, and propose to explore anapproach based on topic modeling tech-niques for these tasks.1 IntroductionThe Bundestag is the legislative institution of Ger-many.
In its plenary sessions, the members discussthe introduction and formulation of bills.
Sub-jects under discussion include a wide spectrum ofissues, ranging from funding of public transportthrough fighting right-wing extremism, or the de-ployment of German troops in Afghanistan.
Foreach issue, a few selected members give a speechstating their opinion towards the topic, while theaudience is allowed to interact: by questions,heckles, applause or even laughter.
Transcrip-tions of the Bundestag?s sessions provide us with agold-mine of political speech data, encoding het-erogeneous political phenomena such as, for in-stance, the prominence or engagement of the dif-ferent politicians with respect to the current polit-ical situation, or their interest for specific topics.In our work, we propose to leverage these datato enable the analysis of the speakers?
positionswith respect to the party they belong to, on the ba-sis of the content of their speech.
Questions we in-vestigate include: which party?s views do differentpoliticians support?
How much are their politicalviews aligned with those of their party?
Althoughwe know a-priori which party a speaker belongsto, we view their positions on different topics withrespect to their party?s official lines as degrees ofalignment, and measure them based on the con-tent of their speeches.
There are several circum-stances under which a speaker might deviate fromhis or her party?s opinion.
For instance, he mightstem from an election district where membershipof a particular party increases his chances of be-ing elected.
Moreover, it might just happen that apolitician who generally supports his party?s linespersonally has a different view on one particulartopic.
If we are able to measure positions fromtext, we allow for methods of analyzing adherenceto party lines, which is an important issue in po-litical science (cf.
(Clinton et al, 2004), (Ceron,2013) and (Ansolabehere et al, 2001)).At its heart, our work aims at modeling politi-cians?
positions towards a specific topic, as in-ferred from their speech.
To estimate a position,in turn, we need a statement of the party?s opiniontowards the topic of interest, which can be thenused for comparison against the speech.
Variouswork in political science suggests to take this fromparty manifestos like (Keman, 2007) and (Slapinand Proksch, 2008).
Research in political sci-ence has previously focused on analyzing politi-cal positions within text, for instance (Laver andGarry, 2000), (Laver et al, 2003), (Keman, 2007)or (Sim et al, 2013).
However, most of previouswork focused on the general position of a partyor a person, like (Slapin and Proksch, 2008), asopposed to fine-grained positions towards specifictopics.
In our research, we address the two follow-ing tasks:1.
Determine the speeches?
topics ?
namely de-velop methods to determine the topic(s) coveredby a political speech, such as those given in theBundestag.262.
Quantify adherence to party lines ?
namely es-timate the speaker?s position relatively to hisparty?s opinion towards the respective topic(s).In the following thesis proposal we present avariety of approaches that we plan to investigatein order to address these tasks, as well as discusstheir limitations and challenges.The first task, determining the topics, couldbe in principle addressed using well-studied su-pervised approaches like state-of-the-art machinelearning algorithms.
However, we cannot rely onthe fact that all topics are covered in the train-ing data.
Consequently, we propose to explore anunsupervised approach that integrates informationfrom an external resource.
We suggest to use avariant of topic models which allows us to influ-ence the creation of the topics.The second task, determining the positions, isa bigger challenge, given the current state of theart.
Some previous research looked at the relatedfield of opinion mining, also on political discus-sion, as in (Abu-Jbara et al, 2012), (Anand etal., 2011) or (Somasundaran and Wiebe, 2009).These methods, however, are hardly applicable tothe complex data of plenary meetings.
In our sce-nario, we have to deal with a very specific kind oftext, since the discussions do not consist of spon-taneous dialogues, but rather formal statements.Consequently, we are forced to deal with a type oflanguage which lies in-between dialogue and text.More concretely, within these speeches speakersroughly assume what positions the parties haveand also have expectations about their opponents?opinions.
Besides, as opposed to full-fledged di-alogues, our data shows a very limited amountof interaction between the speaker and the audi-ence, solely consisting of a few questions, heck-les, laughter or applause.
Further, as it is the goalof the discussions to constructively develop lawsand agree on formulations, the speakers do notjust state reasons pro or contra some issue.
Theyrather illustrate different aspects of the discusseditems.
Furthermore, they try to convince others byemphasizing what their party has achieved in thepast or criticize decisions taken in the past.
To ad-dress these complex problems, we propose to startby using manually annotated party manifestos inorder to provide us with an upper bound.
Next,we propose to investigate the applicability of topicmodels to provide us, again, with a flexible unsu-pervised approach.2 DataThe German Bundestag meets about 60 times ayear, and discusses various items in each plenarysession.
There are various types of items on theagenda: they can be discussions about bills, butalso question times or government?s statements.We are interested in the first type only.
Each billhas a unique identifier which is also mentioned bythe session chair.
By looking it up in a databaseprovided by the Bundestag, it is possible to filterthe bill discussions from other forms of items.For each discussed item, a few selected mem-bers are permitted to give a speech.
Most of themembers belong to a party and their affiliation ispublicly known.The Bundestag releases the transcripts of itssessions as plain text documents.
OffenesParla-ment1is a project run by volunteers that processesthese documents and publishes them in a struc-tured form on the web as HTML documents.
Thedata distinguishes between parts of a given speech,utterances by the chairman and heckles, each an-notated with its speaker.
OffenesParlament makesthe attempt to divide each session?s transcript intoparts containing a single item of the agenda only.This is not trivial, as it is the chairman who leadsover using a non-standardized formulations, andthus contains many mistakes.We collected a number of regular expressionsand hope to improve the segmentation of the items.We will evaluate the performance of this heuristicby checking a sample with human judges.Our extracted dataset covers the time period be-tween March 2010 and December 2012 and con-sists of 182 meetings.3 Determining topics in speechesWe aim at comparing the positions stated withinthe speeches to the general positions of the par-ties represented in the Bundestag.
The parties?
po-sitions can be found in their manifestos, and arecommonly used as a source by scholars, as in (Ke-man, 2007) or (Slapin and Proksch, 2008).
In or-der to being able to compare speakers?
and partiespositions, we need to address two different tasks,namely: i) identifying the topic of a speech, andii) locating that very same topic within the partymanifesto or some further resource.
The latter taskdepends on how the comparison is done.
In this1http://offenesParlament.de27section, we will focus on the first task: determin-ing the topic of the speech.There are two general approaches to classifythe topics of text: either the topics are known inadvance and constitute a static set of categories,for example (Hillard et al, 2008), or they are un-known in advance and dynamically created de-pending on the data, as in (Quinn et al, 2010) (seealso (Grimmer and Stewart, 2013) and (Sebastiani,2002) for an overview).
In our scenario, we as-sume a common set of topics over several datasources, namely the party manifestos and tran-scripts of speeches in our case.
Therefore, we optfor a fixed set of topic categories.3.1 Definition of topical categoriesIn political science, there are various schemes tocategorize political topics.
A well-known andimportant project is the Comparative ManifestoProject (Budge et al, 2001), in which party man-ifestos are hand-coded on sentence level with ascheme of 560 categories.
A similar project is theComparative Agendas Project2, which uses 21 toplevel categories further divided into fine-grainedsubcategories.An alternative approach is to use the ministriesas definition of the available categories, which in-spired the category scheme used in (Seher andPappi, 2011).
In our work, we develop a categoryscheme for our particular task on the basis of theresponsibilities of committees of the Bundestag,as suggested by internal discussions with scholarsof political science.
Similar to the ministries ingovernment, the responsibilities for political areasare divided among various committees (see Table1 for a list of committees).
Each item discussed inthe Bundestag is assigned to all committees whoinvestigate the issues in more detail.
For instance,in our data we find that a discussion about contin-uing the German participation in the InternationalSecurity Assistance Force in Afghanistan has beenassigned to the following committees: Foreign Af-fairs, Internal Affairs, Legal Affairs, Defense, Hu-man Rights and Humanitarian Aid, Economic Co-operation and Development.
For each issue, oneof the committees is appointed as the leading one(German: federf?uhrende Ausschuss), the Commit-tee of Foreign Affairs in this case.Note that, crucially for our work, this assign-ment process provides us with human-annotated2http://www.comparativeagendas.infoAffairs of the European UnionLabour and social AffairsFood, Agriculture and Consumer ProtectionFamily Affairs, Senior Citizens, Women and YouthHealthCultural and Media AffairsCommittee on Human Rights and Humanitarian AidTourismEnvironment, Nature Conservation and Nuclear SafetyTransport, Building and Urban DevelopmentScrutiny of Elections, Immunity and the Rules of ProcedureEconomics and TechnologyEconomic Cooperation and DevelopmentForeign AffairsFinanceBudgetInternal AffairsPetitionsLegal AffairsSportsDefenseEducation, Research and Technology AssessmentTable 1: Committees of the 17th German Bun-destag.topic labels: in fact, not only can we use the com-mittees as category definitions, but we can also usethese very same assignments as a gold standard.Consequently, we use the definitions describingthe responsibilities of the committees as our cat-egory scheme for political topics.
We excludethree committees from the experiments namely: a)the Committee on Scrutiny of Elections, Immunityand the Rules of Procedure, b) the Committee onPetitions, and c) the Committee of Legal Affairs.This is because these committees are not directlyresponsible for a particular political domain, butperform meta functions.Descriptions of the particular committees in-cluding their responsibilities and tasks as well asconcrete examples of their work, accomplished bylists of current members, can be found in flyers re-leased by the Bundestag3.Given this definition of political categories onthe basis of the committees, we can create a goldstandard for our topic classification scenario: tolabel a speech, we take the item it is given about,and use the committees the item has been assignedto as labels.
The committee responsible, in turn,can be seen as the most important (i.e., primary)topic label4.
Topic assignments are automaticallyharvested from a freely available source of infor-3https://www.btg-bestellservice.de/index.php?navi=1&subnavi=524Henceforth, we refer to the committees as labels for ourtopic classification task as ?category?
or ?class?28mation, namely a public database offered by theGerman Bundestag5.
Each item discussed in theBundestag is associated with a printed document(Drucksache) tagged with a unique identifier, bywhich it can be tracked in the database and wherethe list of assigned committees can be queried.Given these topic assignments, we aim at ac-quiring a model to classify the speeches with theirassigned categories.
To this end, we could focuson predicting the main label only (i.e.
the commit-tee responsible), or rather perform a multi-class la-beling task predicting all labels (all committees theitem is assigned to).
We now overview a super-vised and unsupervised approach to address theseclassification problems.3.2 Supervised approachGiven that we have labeled data, a first solutionis to opt for a supervised approach to text clas-sification, which has been successfully used formany tasks like topic detection ((Diermeier et al,2012), (Husby and Barbosa, 2012), or sentimentanalysis (Bakliwal et al, 2013), to name a few.Consequently, in our case we could represent thespeeches as a word vector and train state-of-the-art machine learning algorithms like Support Vec-tor Machines, using the assigned committees as la-bels.3.3 Unsupervised approachIn order to develop a generally applicable ap-proach that can easily be applied to other resourcessuch as speeches given in a context different fromthat of the Bundestag, we are interested to explorean unsupervised approach and compare it to thesupervised one.External definition of categories.
The particu-lar issues that fall into the responsibility of a com-mittee are broad and might not be completely cov-ered when using the speeches themselves as train-ing data.
As mentioned in Section 3.1, we havea clear definition of the tasks of each committeeprovided within the flyers.
We will use them as abasis for the category definitions, and extend themwith political issues discussed in party manifestos.We will explain this further in Section 3.3.Known set of categories.
Techniques such asLDA (Blei et al, 2003) create the topics dynam-ically during the classification process.
Recently5dipbt.bundestag.de/dip21.web/btFigure 1: Approach overviewthey became quite popular in political science, c.f.
(Grimmer, 2010), (Quinn et al, 2010) or (Gerrishand Blei, 2011).
As discussed in Section 3, weprefer to have a fixed set of categories.
This allowsfor comparison between applications of the clas-sification on different sources and domains sep-arately.
But while topic models do not fit thisrequirement, they have one property that corre-sponds quite well to our task: rather than assign-ing the text one single label, they return a dis-tribution over topics contained by it.
The itemsdiscussed in the speeches touch a range of polit-ical topics, and are assigned to various commit-tees.
There are variations of topic models that al-low for influencing the creation of the topics, suchas the systems of (Ramage et al, 2009) (LabeledLDA), (Andrzejewski and Zhu, 2009) or (Jagar-lamudi et al, 2012).
Labeled LDA is trained ona corpus of documents.
In contrast to standardtopic model approaches, it needs as input the in-formation which labels (topics) are contained bythe document, though not their proportions, thususes a fixed set of categories.We illustrate our methodology in Figure 1.
Ourproposed approach starts by extracting seed wordsfor the categories from the flyers about the com-mittees.
These seed words are then used to labeltraining data for labeled LDA.
As training data,we take an external resource: the manifestos6ofall parties.
Finally, we apply the trained model tothe speeches to infer the labels.
The output can beevaluated by comparing the predicted categoriesto the committees the issue is actually assigned to.In the following, we will explain each step in moredetail.6We combine the general party programs and the currentelection programs of each party291) Extraction of seed words.
We first downloadthe flyers provided by the Bundestag.
Then, wefilter for nouns and calculate their TF-IDF val-ues for the committee, by which we rank them.In a final step, we ask a scholar of political sci-ence to clean them, i.e.
to delete nouns that arenot necessarily important for the particular com-mittee or are too ambiguous, and to cut the tail oflow-ranked nouns.
To give an example, we finallyreceive the following keywords for the committeeof Labour and Social Affairs: age-related poverty,labour-market policy, employee, social affairs, so-cial security, labour, work, pension, basic socialsecurity, regulated rates, partial retirement, socialstandard, subcontracted labour.2) Automatically generating training data.We take the manifestos of all parties in the Bun-destag to train our labeled LDA model.
Whiletopic models expect a whole collection of docu-ments as input, we only provide a handful of them:accordingly, we generate a pseudo document col-lection by cutting the documents into snippets, fol-lowing our previous work in (Zirn and Stucken-schmidt, 2013), and treating each of them as sin-gle documents.
If a keyword for a committee isfound within a snippet, we add the correspondingcategory to the documents labels.
We finally runlabeled LDA using standard configurations on theso labeled data.3) Applying labeled LDA.
Finally, we can ap-ply the trained model on our transcribed speechdata: we do this by inferring, for each speech,the distribution of topics, i.e.
of categories.
Toevaluate the model, we check that the committeeresponsible corresponds to the highest probabletopic inferred for the speech, and the other n as-signed committees to the n most probable topics.Currently, in our work, we are in the final stagesof creating the gold standard, and evaluating ourmethod.
However, we have already implementedthe proposed system as prototype, and accordinglyshow a part of the created topic model in Table 2to give the reader an impression.4 Detecting positionsThe overall goal of our work is to analyze thepositions expressed by the speakers towards thedebated item.
As we aim at performing a fine-grained analysis, approaches merely classifyingENCNS LSA TBUDconsumer (male) labour mobilityconsumer (female) employee male researchenvironment employees female infrastructureprotection salary railwayproducts pension trafficfarming labour market investmentsnature old-age provision developmentvariety unemployment futureraw materials employment railstransparency percentage streetsTable 2: Top 10 terms for the committees on Envi-ronment, Nature Conservation and Nuclear Safety(ENCNS), on Labour and social affairs (LSA) andon Transport, Building and Urban Development(TBUD).pro or contra (like those of (Walker et al, 2012)or (Somasundaran and Wiebe, 2009) are not ap-plicable in our case.
The same applies to the taskof subgroup detection (as done by (Abu-Jbara etal., 2012), (Anand et al, 2011) or (Thomas et al,2006)).In order to produce a finer-grained model of po-sitions, we want to develop a model that placespositions stated in text along a one-dimensionalscale, as done by (Slapin and Proksch, 2008)with their system called Wordfish, (Gabel and Hu-ber, 2000),(Laver and Garry, 2000), (Laver et al,2003) or (Sim et al, 2013).
Wordfish places partymanifestos on a left-right-scale, what visualizesvery well which parties are close to each other andwhich ones are distant.
This is similar in spiritto the purpose of our work, since we are inter-ested primarily in estimating closeness and dis-tances between the speakers?
and the parties?
po-sitions.
However, in contrast to their work, we areinterested in positions towards specific topics, asopposed to general parties?
positions.We define our task as follows: we want to an-alyze the distance between the position towards atopic expressed in a speech and the position to-wards the same topic stated in a party manifesto.In the previous section, we described an approachto determine the topic of the speech.
We nowmove on and present how we can retrieve the seg-ments of the manifestos that correspond to thetopic(s) addressed within the speeches, as well ashow to compare these positions.4.1 Approach A: Hand-coding of manifestosExtract positions As part of a larger collabora-tion project with scholars of political science we30decided to start with hand-coding a set of man-ifestos on sentence-level in order to have a goldstandard for further work.
To facilitate the manualwork, we use a computer-assisted method basedusing the seed words created in Section 3.3.
Inmore detail, we first use occurrences of the seedwords to assign them the corresponding categorylabel.
Then, a human annotator validates these as-signments, optionally adding missing labels.If the sentence-wise labeled data proofs suc-cessful and necessary for the further analysis ofpolitical positions, we will investigate approachesto automate this process, for example with super-vised learning or bootstrapping techniques startingwith our seed words.
For each topic, we can thenaccumulate the sentences assigned to its corre-sponding category and use this data as the party?sopinion towards this topic.Compare positions The comparison betweenthe speech and the parties?
opinions can then beperformed as follows: for each party, we extractthe sentences from the manifesto that are taggedwith the topic covered in the speech.
We then rep-resent the extracted sentences and the speeches asword vectors, and compare them with a distancemetric, e.g., a standard measure like cosine simi-larity, which gives us the closeness of the speechto each party?s position.4.2 Approach B: Topic ModelsExtract positions Instead of selecting sentencesfrom the manifesto that cover a topic, the posi-tion could be extracted from the manifesto usingtopic models, as shown in (Thomas et al, 2006)and (Gerrish and Blei, 2011).
To extract the topicsfrom the manifestos, we run labeled LDA sepa-rately on each manifesto, following the techniquedescribed in Section 3, yet with an important dif-ference.
In Section 3, we trained one commontopic model on all manifestos, in order to have abroad coverage over all topics.
Here, we are in-terested in the positions carried by the particularwords chosen by the party to describe a topic.
Ac-cordingly, we train a separate topic model on eachmanifesto.
The result is a distribution over termsfor each committee, hence for each topic.Compare positions As a result of the processto determine the topic of a speech (Section 3),the speeches also have a representation of the dis-cussed topics as a distribution over terms.
Thisway we can directly compare the distributionsfor the most probable topics in the speech withthe corresponding topic in the party manifestos.This can be done using measures to estimatethe distance between probability distributions like,for instance, Kullback-Leibler distance or Jensen-Shannon divergence.5 Conclusions and Future WorkIn this paper, we presented an overview of our the-sis proposal on comparing positions found withinpolitical speeches against those expressed in partymanifestos.
To the best of our knowledge, this isthe first work of this kind to aim at providing afine-grained analysis of speakers?
positions on po-litical data.
Arguably, the most exiting aspect ofthis work is that it grounds a variety of NaturalLanguage Processing topics ?
e.g., polarity detec-tion, topic modeling, among others ?
within a con-crete, multi-faceted application scenario.Being this a proposal, the first step in the fu-ture will be to complete the implementation of allabove described methods and evaluate them.
Inour dataset, we are provided with additional in-formation apart from the speech text: we knowabout heckles, laughter and applause and evenknow their origin.
This knowledge can be usedto estimate a network of support or opposition.This knowledge is also used in (Strapparava etal., 2010) to predict persuasiveness of sentences,which could constitute another source of informa-tion for our model.
Another idea would be to makeuse of the speaker?s given party affiliations andbootstrap an approach to analyze their positions:if we assume that a majority of the speakers actu-ally does follow their parties?
lines, we can train aclassifier for each party for each topic, and applyit to the same data to detect outliers.
Besides, abig research question would be to see how muchwe can complement our topic models with addi-tional supervision in the form of symbolic knowl-edge sources like wide-coverage ontologies, e.g.,DBpedia.
Finally, while we do focus in this workon German data, we are interested in extending ourmodel to other languages, including resource-richones like English as well as resource-poor ones.AcknowledgementsWe thank Google for travel and conference sup-port for this paper.31ReferencesAmjad Abu-Jbara, Mona Diab, Pradeep Dasigi, andDragomir Radev.
2012.
Subgroup detection in ide-ological discussions.
In Proceedings of the 50th An-nual Meeting of the Association for ComputationalLinguistics: Long Papers - Volume 1, ACL ?12,pages 399?409, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Pranav Anand, Marilyn Walker, Rob Abbott, JeanE.
Fox Tree, Robeson Bowmani, and Michael Mi-nor.
2011.
Cats rule and dogs drool!
: Classifyingstance in online debate.
In Proceedings of the 2NdWorkshop on Computational Approaches to Subjec-tivity and Sentiment Analysis, WASSA ?11, pages 1?9, Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.David Andrzejewski and Xiaojin Zhu.
2009.
La-tent dirichlet alocation with topic-in-set knowledge.In Proceedings of the NAACL HLT 2009 Workshopon Semi-Supervised Learning for Natural LanguageProcessing, pages 43?48.
Association for Computa-tional Linguistics.Stephen Ansolabehere, James M Snyder, and CharlesStewart III.
2001.
The effects of party and prefer-ences on congressional roll-call voting.
LegislativeStudies Quarterly, 26(4):533?572.Akshat Bakliwal, Jennifer Foster, Jennifer van der Puil,Ron O?Brien, Lamia Tounsi, and Mark Hughes.2013.
Sentiment analysis of political tweets: To-wards an accurate classifier.
In Proceedings of theWorkshop on Language Analysis in Social Media,pages 49?58, Atlanta, Georgia, June.
Associationfor Computational Linguistics.D.M.
Blei, A.Y.
Ng, and M.I.
Jordan.
2003.
Latentdirichlet alocation.
Journal of Machine LearningResearch (JMLR), 3:993?1022.Ian Budge, Hans?=Dieter Klingemann, AndreaVolkens, Judith Bara, and Eric Tanenbaum.
2001.Mapping Policy Preferences.
Estimates for Parties,Electors, and Governments 1945-1998.
OxfordUniversity Press, Oxford u. a.Andrea Ceron.
2013.
Brave rebels stay home: Assess-ing the effect of intra-party ideological heterogene-ity and party whip on roll-call votes.
Party Politics,page 1354068812472581.Joshua Clinton, Simon Jackman, and Douglas Rivers.2004.
The statistical analysis of roll call data.
Amer-ican Political Science Review, 98(02):355?370.Daniel Diermeier, Jean-Franois Godbout, Bei Yu, andStefan Kaufmann.
2012.
Language and ideologyin congress.
British Journal of Political Science,42:31?55, 1.Matthew J. Gabel and John D. Huber.
2000.
Puttingparties in their place: Inferring party left-right ideo-logical positions from party manifestos data.
Amer-ican Journal of Political Science, 44(1):pp.
94?103.Sean Gerrish and David M Blei.
2011.
Predicting leg-islative roll calls from text.
In Proceedings of the28th International Conference on Machine Learning(ICML-11), pages 489?496.Justin Grimmer and Brandon M Stewart.
2013.
Text asdata: The promise and pitfalls of automatic contentanalysis methods for political texts.
Political Analy-sis.Justin Grimmer.
2010.
A bayesian hierarchical topicmodel for political texts: Measuring expressed agen-das in senate press releases.
Political Analysis,18(1):1?35.Dustin Hillard, Stephen Purpura, and John Wilkerson.2008.
Computer assisted topic classification formixed methods social science research.
Journal ofInformation Technology and Politics.Stephanie Husby and Denilson Barbosa.
2012.
Topicclassification of blog posts using distant supervision.In Proceedings of the Workshop on Semantic Analy-sis in Social Media, pages 28?36, Avignon, France,April.
Association for Computational Linguistics.Jagadeesh Jagarlamudi, Hal Daum?e, III, andRaghavendra Udupa.
2012.
Incorporating lex-ical priors into topic models.
In Proceedings ofthe 13th Conference of the European Chapter ofthe Association for Computational Linguistics,EACL ?12, pages 204?213, Stroudsburg, PA, USA.Association for Computational Linguistics.Hans Keman.
2007.
Experts and manifestos: Differ-ent sources - same results for comparative research.Electoral Studies, 26:76?89.Michael Laver and John Garry.
2000.
Estimating pol-icy positions from political texts.
American Journalof Political Science, pages 619?634.Michael Laver, Kenneth Benoit, and John Garry.
2003.Extracting policy positions from political texts usingwords as data.
American Political Science Review,97(02):311?331.Kevin M. Quinn, Burt L. Monroe, Michael Colaresi,Michael H. Crespin, and Dragomir R. Radev.
2010.How to analyze political attention with minimal as-sumptions and costs.
American Journal of PoliticalScience, 54(1):209?228, January.Daniel Ramage, David Hall, Ramesh Nallapati, andChristopher D Manning.
2009.
Labeled lda: A su-pervised topic model for credit attribution in multi-labeled corpora.
In Proceedings of the 2009 Con-ference on Empirical Methods in Natural LanguageProcessing: Volume 1-Volume 1, pages 248?256.Association for Computational Linguistics.Fabrizio Sebastiani.
2002.
Machine learning in auto-mated text categorization.
ACM computing surveys(CSUR), 34(1):1?47.32Nicole Michaela Seher and Franz Urban Pappi.2011.
Politikfeldspezifische positionen der lan-desverb?ande der deutschen parteien.
Working Paper139, Mannheimer Zentrum f?ur Europ?aische Sozial-forschung (MZES).Yanchuan Sim, Brice D. L. Acree, Justin H. Gross,and Noah A. Smith.
2013.
Measuring ideologicalproportions in political speeches.
In Proceedings ofthe 2013 Conference on Empirical Methods in Nat-ural Language Processing, pages 91?101, Seattle,Washington, USA, October.
Association for Com-putational Linguistics.Jonathan B. Slapin and Sven-Oliver Proksch.
2008.
AScaling Model for Estimating Time-Series Party Po-sitions from Texts.
American Journal of PoliticalScience, 52(3):705?722, July.Swapna Somasundaran and Janyce Wiebe.
2009.
Rec-ognizing stances in online debates.
In Proceed-ings of the Joint Conference of the 47th AnnualMeeting of the ACL and the 4th International JointConference on Natural Language Processing of theAFNLP: Volume 1 - Volume 1, ACL ?09, pages 226?234, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Carlo Strapparava, Marco Guerini, and Oliviero Stock.2010.
Predicting persuasiveness in political dis-courses.
In LREC.
European Language ResourcesAssociation.Matt Thomas, Bo Pang, and Lillian Lee.
2006.
Get outthe vote: Determining support or opposition fromcongressional floor-debate transcripts.
In Proceed-ings of the 2006 Conference on Empirical Meth-ods in Natural Language Processing, EMNLP ?06,pages 327?335, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Marilyn A Walker, Pranav Anand, Robert Abbott, andRicky Grant.
2012.
Stance classification using dia-logic properties of persuasion.
In Proceedings of the2012 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 592?596.
Asso-ciation for Computational Linguistics.C?acilia Zirn and Heiner Stuckenschmidt.
2013.
Multi-dimensional topic analysis in political texts.
Data &Knowledge Engineering.33
