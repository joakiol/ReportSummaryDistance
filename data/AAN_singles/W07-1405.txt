Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 28?35,Prague, June 2007. c?2007 Association for Computational LinguisticsA Corpus of Fine-Grained Entailment RelationsRodney D. Nielsen and Wayne WardCenter for Spoken Language ResearchInstitute of Cognitive ScienceDepartment of Computer ScienceUniversity of Colorado, BoulderRodney.Nielsen, Wayne.Ward@Colorado.eduAbstractThis paper describes on-going efforts to an-notate  a  corpus  of  almost  16000  answerpairs with an estimated 69000 fine-grainedentailment relationships.
We illustrate theneed for  more detailed classification thancurrently  exists  and  describe  our  corpusand annotation scheme.
We discuss earlystatistical  analysis showing substantial  in-ter-annotator  agreement  even  at  the  fine-grained level.
The corpus described here,which is  the only one providing such de-tailed annotations,  will  be made availableas a public resource later this year (2007).This is expected to enable application de-velopment that is currently not practical.1 IntroductionDetermining whether the propositions in one textfragment are entailed by those in another fragmentis important to numerous NLP applications.
Con-sider an intelligent tutoring system (ITS), where itis  critical  for  the  tutor  to  assess  which  specificfacets of  the desired or reference answer are en-tailed by the student?s answer.
Truly effective in-teraction and pedagogy is only possible if the auto-mated tutor can assess this entailment at a relative-ly fine level of detail (c.f.
Jordan et al, 2004).The PASCAL Recognizing Textual Entailment(RTE) challenge (Dagan et al, 2005) has broughtthe issue of textual entailment before a broad com-munity of researchers in a task independent fash-ion.
This task requires systems to make simple yes-no judgments as to whether a human reading a textt of  one  or  more  full  sentences  would  typicallyconsider a second, hypothesis, text  h (usually onefull sentence) to most likely be true.
This paperdiscusses some of the extensions necessary to thisscheme in order to satisfy the requirements of anITS and provides a preliminary report on our ef-forts  to  produce  an  annotated  corpus  applyingsome of  these  additions  to  children?s  answers  toscience questions.We first  provide a  brief  overview of the RTEchallenge  task  and  a  synopsis  of  answer  assess-ment  technology  within  existing  ITSs  and  largescale  assessment  applications.
We  then  detailsome of the types of changes required in order tofacilitate more effective pedagogy.
We provide areport on our work in this direction and describe acorpus we are annotating with fine-grained entail-ment information.
Finally, we discuss future direc-tion and the relevance of this annotation scheme toother applications such as question answering.2 Prior Work2.1 RTE Challenge TaskExample 1 shows a typical  t-h pair from the RTEchallenge.
The task is to determine whether typi-cally a reader would say that  h is most likely truehaving read t.  The system output is a simple yes orno decision about this entailment ?
in this example,the decision is no ?
and that is similarly the extentto which training data is annotated.
There is no in-dication of whether some facets of, the potentiallyquite long, h are addressed (as they are in this case)in  t or conversely, which facets are not discussedor are explicitly contradicted.
(1) <t>At an international disas-ter conference in Kobe, Japan, the28U.N.
humanitarian chief said the United Nations should take the lead in creating a tsunami early-warning system in the Indian Ocean.</t><h>Nations affected by the Asian tsunami disaster have agreed the UN should begin work on an early warning system in the Indian Ocean.</h>However, in the third RTE challenge, there is anoptional pilot task1 that begins to address some ofthese issues.
Specifically, they have extended thetask  by  including  an  unknown label,  where  h isneither entailed nor contradicted, and have request-ed justification for decisions.
The form that thesejustifications  will  take  has  been  left  up  to  thegroups  participating,  but  could  conceivably  pro-vide some of the information about which specificfacets of the hypothesis are entailed, contradictedand unaddressed.2.2 Existing Answer Assessment TechnologyEffective ITSs exist in the laboratory producinglearning  gains  in  high-school,  college,  and  adultsubjects through text-based dialog interaction (e.g.,Graesser et al, 2001; Koedinger et al, 1997; Peterset al, 2004, VanLehn et al, 2005).
However, mostITSs today provide only a shallow assessment ofthe learner?s comprehension (e.g., a correct versusincorrect decision).
Many ITS researchers are stri-ving  to  provide  more  refined  learner  feedback(Aleven et al, 2001; Graesser et al, 2001; Jordanet al, 2004; Peters et al, 2004; Roll et al, 2005;Ros?
et al, 2003).
However, they are developingvery  domain-dependent  approaches,  requiring  asignificant investment in hand-crafted logic repre-sentations,  parsers,  knowledge-based  ontologies,and  or  dialog  control  mechanisms.
Simply  put,these domain-dependent techniques will not scaleto the task of developing general purpose ITSs andwill  never enable the long-term goal of effectiveunconstrained interaction with learners or the peda-gogy that requires it.There is also a small, but growing, body of re-search in the area of scoring free-text responses toshort answer questions (e.g., Callear et al,  2001;Leacock,  2004;  Mitchell  et  al.,  2003;  Pullman,2005; Sukkarieh, 2005).
Shaw (2004) and Whit-tington (1999) provide reviews of some of theseapproaches.
Most of the systems that have beenimplemented and tested are based on Information1 http://nlp.stanford.edu/RTE3-pilot/Extraction  (IE)  techniques  (Cowie  &  Lehnert,1996).
They hand-craft a large number of patternrules, directed at detecting the propositions in com-mon  correct  and  incorrect  answers.
In  general,short-answer  free-text  response  scoring  systemsare designed for large scale assessment tasks, suchas those associated with the tests administered byETS.
Therefore,  they are  not  designed with thegoal  of  accommodating  dynamically  generated,previously unseen questions.
Similarly, these sys-tems do not provide feedback regarding the specif-ic aspects of answers that are correct or incorrect;they merely provide a raw score for each question.As with the  related work directed specifically  atITSs, these approaches all require in the range of100-500 example student answers for each plannedtest question to assist in the creation of IE patternsor to train a machine learning algorithm used with-in some component of their solution.3 The Necessity of Finer-grained AnalysisImagine that you are an elementary school sciencetutor and that rather than having access to the stu-dent?s full response to your questions, you are sim-ply  given  the  information  that  their  answer  wascorrect or incorrect,  a yes or no entailment deci-sion.
Assuming the student?s answer was not cor-rect, what question do you ask next?
What followup question or action is most likely to lead to betterunderstanding on the part  of  the child?
Clearly,this is a far from ideal scenario, but it is roughlythe situation within which many ITSs exist today.In order to optimize learning gains in the tutor-ing environment, there are myriad issues the tutormust  understand  regarding  the  semantics  of  thestudent?s  response.
Here,  we  focus  strictly  ondrawing inferences regarding the student?s under-standing  of  the  low-level  concepts  and  relation-ships or facets of the reference answer.
I use theword facet throughout this paper to generically re-fer to some part of  a text?s meaning.
The mostcommon  type  of  answer  facet  discussed  is  themeaning  associated  with  a  pair  of  related  wordsand the relation that connects them.Rather than have a single yes or no entailmentdecision for the reference answer as a whole, (i.e.,does the student understand the reference answerin its entirety or is there some unspecified part of itthat  we  are  unsure  whether  the  student  under-stands),  we  instead  break  the  reference  answer29down into what we consider to be its lowest levelcompositional  facets.
This  roughly  translates  tothe set of triples composed of labeled dependenciesin  a  dependency  parse  of  the  reference  answer.2The following illustrates  how a  simple  referenceanswer (2) is decomposed into the answer facets(2a-d)  derived  from  its  dependency  parse  and(2a?-d?)
provide a gloss of each facet?s meaning.As can be seen in 2b and 2c, the dependencies areaugmented by thematic roles (Kipper et al, 2000)(e.g.,  Agent,  Theme,  Cause,  Instrument?)
pro-duced  by  a  semantic  role  labeling  system  (c.f.,Gildea and Jurafsky,  2002).
The facets  also  in-clude  those  semantic  role  relations  that  are  notderivable from a typical dependency tree.
For ex-ample, in the sentence ?As it freezes the water willexpand and crack the glass?, water is not a modifi-er of crack in the dependency tree, but it does playthe role of Agent in a shallow semantic parse.
(2) A long string produces a low pitch.
(2a) NMod(string, long)(2b) Agent(produces, string)(2c) Product(produces, pitch)(2d) NMod(pitch, low)(2a?)
There is a long string.(2b?)
The string is producing some-thing.(2c?)
A pitch is being produced.(2d?)
The pitch is low.Breaking the reference answer down into low-level  facets  provides  the  tutor?s  dialog  managerwith a much finer-grained assessment of the stu-dent?s response, but a simple yes or no entailmentat  the  facet  level  still  lacks semantic  expressive-ness with regard to the relation between the studen-t?s answer and the facet in question.
Did the stu-dent contradict the facet?
Did they express a relat-ed  concept  that  indicates  a  misconception?
Didthey leave the facet unaddressed?
Can you assumethat they understand the facet even though they didnot express it, since it was part of the informationgiven in the question?
It is clear that, in addition to2 The goal of most English dependency parsers is to pro-duce a single projective tree structure for each sentence,where each node represents a word in the sentence, eachlink represents a functional category relation, usually la-beled, between a governor (head) and a subordinate(modifier), and each node has a single governor (c.f.,Nivre and Scholz, 2004).breaking  the  reference  answer  into  fine-grainedfacets, it is also necessary to break the annotationinto finer levels in order to specify more clearly therelationship between the student?s answer and thereference answer aspect.There  are  many  other  issues  that  the  systemmust know to achieve near optimal tutoring, someof which are mentioned later in the discussion sec-tion, but these two ?
breaking the reference answerinto fine-grained facets and utilizing more expres-sive annotation labels ?
are the emphasis of this ef-fort.4 Current Annotation EffortsThis section describes our current efforts in anno-tating  a  corpus  of  answers  to  science  questionsfrom elementary school students.4.1 CorpusLacking data from a real tutoring situation, we ac-quired data gathered from 3rd-6th grade students inschools utilizing the Full  Option Science System(FOSS).
Assessment is a major FOSS research fo-cus,  of  which  the  Assessing  Science  Knowledgeproject  is  a  key component.3  The FOSS projecthas developed sixteen science teaching and learn-ing modules targeted at  grades 3-6,  as shown inTable 1.
The ASK project created assessments foreach of these modules, including multiple choice,fill  in  the  blank,  free  response,  and  somewhatlengthy  experimental  design  questions.
We  re-viewed these questions and selected about 290 freeresponse questions that were in line with the objec-tives  of  this  research project,  specifically  we se-lected questions whose expected responses rangedin length from moderately short verb phrases to afew sentences, that could be assessed objectively,and that were not too open ended.
Table 2 shows a3 ?FOSS is a research-based science program for gradesK?8 developed at the Lawrence Hall of Science, Uni-versity of California at Berkeley with support from theNational Science Foundation and published by DeltaEducation.
FOSS is also an ongoing research projectdedicated to improving the learning and teaching of sci-ence.
?Assessing Science Knowledge (ASK) is ?designed todefine, field test, and validate effective assessment toolsand techniques to be used by grade 3?6 classroomteachers to assess, guide, and confirm student learningin science.
?http://www.lawrencehallofscience.org/foss/30Grade Life Science Physical Science andTechnologyEarth and SpaceScienceScientific Reasoningand Technology3-4 HB: Human BodyST: Structure of LifeME: Magnetism & ElectricityPS: Physics of SoundWA: WaterEM: Earth MaterialsII: Ideas & InventionsMS: Measurement5-6 FN: Food & NutritionEV: EnvironmentsLP: Levers & PulleysMX: Mixtures & SolutionsSE: Solar EnergyLF: LandformsMD: Models & DesignsVB: Variables1Table 1 FOSS / ASK Learning and Assessment Modules by Area and GradeHB Q: Dancers need to be able to point their feet.
The tibialis is the major muscle on the front of the legand the gastrocnemius is the major muscle on the back of the leg.
Describe how the muscles in thefront and back of the leg work together to make the dancer?s foot point.R: The muscle in the back of the leg (the gastrocnemius) contracts and the muscle in the front of theleg (the tibialis) relaxes to make the foot point.A: The back muscle and the front muscle stretch to help each other pull up the foot.ST Q: Why is it important to have more than one shelter in a crayfish habitat with several crayfish?R: Crayfish are territorial and will protect their territory.
The shelters give them places to hide fromother crayfish.
[Crayfish prefer the dark and the shelters provide darkness.
]A: So all the crayfish have room to hide and so they do not fight over them.ME Q: Lee has an object he wants to test to see if it is an insulator or a conductor.
He is going to use thecircuit you see in the picture.
Explain how he can use the circuit to test the object.R: He should put one of the loose wires on one part of the object and the other loose wire on anotherpart of the object (and see if it completes the circuit).A: You can touch one wire on one end and the other on the other side to see if it will run or not.PS Q: Kate said: ?An object has to move to produce sound.?
Do you agree with her?
Why or why not?R: Agree.
Vibrations are movements and vibrations produce sound.A: I agree with Kate because if you talk in a tube it produce sound in a long tone.
And it vibrationsand make sound.WA Q: Anna spilled half of her cup of water on the kitchen floor.
The other half was still in the cup.
Whenshe came back hours later, all of the water on the floor had evaporated but most of the water in thecup was still there.
(Anna knew that no one had wiped up the water on the floor.)
Explain to Annawhy the water on the floor had all evaporated but most of the water in the cup had not.R: The water on the floor had a much larger surface area than the water in the cup.A: Well Anna, in science, I learned that when water is in a more open are, then water evaporates faster.So, since tile and floor don't have any boundaries or wall covering the outside, the water on thefloor evaporated faster, but since the water in the cup has boundaries, the water in the cup didn'tevaporate as fast.EM Q: You can tell if a rock contains calcite by putting it into a cold acid (like vinegar).Describe what you would observe if you did the acid test on a rock that contains this substance.R: Many tiny bubbles will rise from the calcite when it comes into contact with cold acid.A: You would observe if it was fizzing because calcite has a strong reaction to vinegar.Table 2 Sample Qs from FOSS-ASK with their reference (R) and an example student answer (A).few questions that are representative of those se-lected for inclusion in the corpus, along with theirreference answers and an example student answerfor  each.
Questions  without  at  least  one  verbphrase were rejected because they were assumed tobe  more  trivial  and  less  interesting  from the  re-search  perspective.
Examples  of  such  questionsalong with their reference answers and an examplestudent response include: Q:  Besides air, what (ifanything)  can  sound  travel  through?
ReferenceAnswer: Sound can also travel through liquids andsolids.
(Also  other  gases.)
Student  Answer:  Ascreen door.
Q: Name a property of the sound of afire engine?s siren.
Reference Answer:  The soundis very loud.
OR The sound changes in pitch.
Stu-dent Answer: Annoying.
An example of a free re-sponse item that was dropped because it was tooopen ended is: Design an investigation to find outa plant?s range of tolerance for number of hours ofsunlight per day.
You can use drawings to help ex-plain your design.We generated a corpus from a random sample ofthe kids?
handwritten responses to these questions.The only special transcription instructions were to31fix spelling errors (since these would be irrelevantin a spoken dialog environment), but not grammat-ical errors (which would still be relevant), and toskip blank answers and non-answers similar in na-ture to I don?t know (since these are not particular-ly interesting from the research perspective).Three modules were designated as  the test  set(Environments, Human Body, and Water) and theremaining 13 modules  will  be  used for  develop-ment and training of  classification systems.
Wejudged the three test set modules to be representa-tive of the entire corpus in terms of difficulty andappropriateness for the types of questions that metour research interests.
We transcribed the respons-es of approximately 40 randomly selected studentsfor each question in the training set and 100 ran-domly selected students for  each question in  thetest set.
In order to maximize the diversity of lan-guage and knowledge represented by the trainingand test datasets, random selection of students wasperformed at the question level  rather than usingthe same students?
answers for all of the questionsin a given module.
However, in total there wereonly about 200 children that participated in any in-dividual  science  module  assessment,  so  there  isstill  moderate  overlap  in  the  students  from  onequestion to another within a given module.
On theother hand, each assessment module was given to adifferent group of kids, so there is no overlap instudents  between modules.
There  are  almost  60questions and 5700 student answers in the test set,comprising approximately 20% of all of the ques-tions utilized and 36% of the total number of tran-scribed student responses.
In total, including testand training datasets, there are nearly 16000 stu-dent responses.4.2 AnnotationThe answer assessment annotation described in thispaper is intended to be a step toward specifying thedetailed semantic understanding of a student?s an-swer that is required for an ITS to interact effec-tively with a learner.
With that goal in mind, anno-tators were asked to consider and annotate accord-ing to what they would want to know about the stu-dent?s answer if they were the tutor (but a tutor thatfor some reason could not understand the unstruc-tured text of the student?s answer).
The key excep-tion here is that we are only annotating a student?sanswer in terms of whether or not it accurately andcompletely  addresses  the  facets  of  the  reference(desired or correct) answer.
So, if the student alsodiscusses concepts not addressed in the referenceanswer, we will not annotate those points regard-less of their quality or accuracy.Each reference answer in the corpus is decom-posed into its constituent facets.
Then each studentanswer is annotated relative to the facets in the cor-responding reference answer.
As described earlier,the reference answer facets are roughly extractedfrom the relations in a syntactic dependency parse(c.f.,  Nivre and Scholz,  2004) and a shallow se-mantic parse (Gildea and Jurafsky, 2002).
Theseare modified slightly to either eliminate most func-tion words or incorporate them into the relation la-bels (c.f., Lin and Pantel, 2001).
Example 3 illus-trates the decomposition of one of the reference an-swers  into  its  constituent  parts  along  with  theirglosses.
(3) The string is tighter, so thepitch is higher.
(3a)  Is(string, tighter)(3a?)
The string is tighter.
(3b)  Is(pitch, higher)(3b?)
The pitch is higher.
(3c)  Cause(3b, 3a)(3c?)
3b is caused by 3aThe annotation  tool  lists  the  reference  answerfacets that students are expected to address.
Both aformal  relational  representation  and  an  English-like gloss of the facet are displayed in a table, onerow per facet.
The annotator?s job is to label eachof those facets to indicate the extent to which thestudent addressed it.
We settled on the eight anno-tation  labels  noted  in  Table  3.
Descriptions  ofwhere each annotation label applies and some ofthe most common annotation issues were detailedwith  several  examples  in  the  guidelines  and  areonly very briefly summarized in the remainder ofthis subsection.Example 4 shows a student answer correspond-ing to  the  reference answer  in  example  3,  alongwith its initial annotation in 4a-c and its final anno-tation in 4a?-c?.
It is assumed that the student un-derstands that the pitch is higher (facet 4b), sincethis  is  given in the question (?
Write a note toDavid to tell him why the pitch gets higher ratherthan lower) and similarly it is assumed that the stu-dent will be explaining what has the causal effectof producing this higher pitch (facet 4c).
There-fore, these facets are initialized to Assumed by the32system.
Since the student does not contradict thefact that the string is tighter (the string can be bothlonger and tighter),  we do not label this facet  asContradicted.
If the student?s response did notmention anything about either the string or tight-ness,  we  would  annotate  facet  4a  as  Unad-dressed.
However,  the  student  did  discuss  aproperty of the string, the string is long, producingthe  facet  Is(string, long).
This  parallels  thereference answer facet Is(string, tighter) withthe exception of a different argument to the Is re-lation, resulting in the annotation Diff-Arg.
Thisindicates to the tutor that the student expressed arelated concept, but one which neither implies thatthey understand the reference answer facet nor thatthey explicitly hold a contradictory belief.
Often,this indicates that the student has a misconception.For example, when asked about an effect on pitch,many students say things like the pitch gets louder,rather than higher or lower, which implies a mis-conception involving their understanding of pitchand volume.
In this case, the Diff-Arg label canhelp focus the tutor on correcting this misconcep-tion.
Facet 4c expressing the causal relation be-tween 4a and 4b is labeled  Expressed, since thestudent did express a causal relation between theconcepts aligned with 4a and 4c.
The tutor thenknows that the student was on track in regard to at-tempting to express the desired causal relation andthe tutor need only deal with the fact that the causegiven was incorrect.Table 3 Facet Annotation Labels(4) David this is why because youdon't listen to your teacher.
If thestring is long, the pitch will behigh.
(4a) Is(string, tighter), ---(4b) Is(pitch, higher), Assumed(4c) Cause(4b, 4a), Assumed(4a?)
Is(string, tighter), Diff-Arg(4b?)
Is(pitch, higher), Expressed(4c?)
Cause(4b, 4a), ExpressedThe  Self-Contra annotation is used in caseslike the response in example 5, where the studentsimultaneously expresses the contradictory notionsthat the string is tighter and that there is less ten-sion.
(5) The string is tighter, so there isless tension so the pitch gets higher.
(5a) Is(string, tighter), Self-ContraThere is no compelling reason from the perspec-tive of the automated tutoring system to differenti-ate  between  Expressed and  Inferred facets,since in either case the tutor can assume that thestudent understands the concepts involved.
How-ever,  from  the  systems  development  perspectivethere are three primary reasons for differentiatingbetween these facets and similarly between facetsthat are contradicted by inference versus more ex-plicit expression.
The first reason is that most sta-tistical  machine  learning  systems  today  cannothope to detect very many pragmatic inferences andincluding these in the training data is likely to con-fuse the algorithm resulting in worse performance.Having separate labels allows one to remove themore  difficult  inferences  from the  training  data,thus eliminating this problem.
The second ratio-nale is that systems hoping to handle both types ofinference might more easily learn to discriminatebetween these opposing classifications if the class-es are distinguished (for algorithms where this isnot the case, the classes can easily be combined au-tomatically).
Similarly, this allows the possibilityof training separate classifiers to handle the differ-ent forms of inference.
The third reason for sepa-rate labels is  that it  facilitates system evaluation,including  the  comparison  of  various  techniquesand the effect of individual features.Example 6 illustrates an example  of  a studentanswer with the label Inferred.
In this case, thedecision  requires  pragmatic  inferences,  applyingthe Gricean maxims of Relation, be relevant ?
whyExpressed: Any facet directly expressed or inferredby simple reasoningInferred: Facets inferred by pragmatics or nontriviallogical reasoningContra-Expr: Facets directly contradicted by nega-tion, antonymous expressions and their paraphrasesContra-Infr:  Facets  contradicted  by  pragmatics  orcomplex reasoningSelf-Contra:  Facets  that  are  both contradicted andimplied (self contradictions)Diff-Arg: The core relation is expressed, but it has adifferent modifier or argumentAssumed:  The  system assigns  this  label,  which  ischanged if any of the above labels applyUnaddressed: Facets that are not addressed at all bythe student?s answer33would the student  mention vibrations  if  they didnot  know they were  a  form of  movement  ?
andQuantity, do not make your contribution more in-formative than is required (Grice, 1975).
(6) Q: Kate said: ?An object has to move to produce sound.?
Do you agree with her?
Why or why not?Ref Ans: ?Agree.
Vibrations are move-ments and vibrations produce sound.
?Student Answer: Yes because it has to vibrate to make sounds.
(6b) Is(vibration, movement), InferredAnnotators are primarily students of Educationand Linguistics and require moderate training onthe annotation task.
The annotated reference an-swers are stored in a stand-off markup in xml files,including an annotated element for each referenceanswer facet.4.3 Inter-Annotator Agreement ResultsThe results reported here are preliminary, based onthe first two annotators, and must be viewed underthe light that we have not yet completed annotatortraining.
We  report  results  under  three  labelgroupings: (1)  All-Labels,  where all  labels areleft  separate,  (2)  Tutor-Labels,  where  Ex-pressed,  Inferred and  Assumed are combinedas are  Contra-Expr and  Contra-Infr,  and (3)Yes-No, which is a two-way division, Expressed,Inferred and Assumed versus all other labels.Agreement  on  Tutor-Labels indicates  thebenefit to the tutor, since it is relatively unimpor-tant to differentiate between the types of inferencerequired  in  determining  that  the  student  under-stands a reference answer facet (or has contradict-ed it).
We evaluated mid-training inter-annotatoragreement  on  a  random selection  of  15  answersfrom each of 14 Physics of Sound questions, total-ing 210 answers  and 915 total  facet  annotations.Mid-training agreement on the  Tutor-Labels is87.4%, with a Kappa statistic of 0.717 correspond-ing with substantial agreement (Cohen, 1960).
In-ter-annotator  agreement  at  mid-training  is  81.1%on All-Labels and 90.1% on the binary Yes-Nodecision.
These also have Kappa statistics in therange of substantial agreement.The distribution of the 915 annotations is shownin Table 4.
It is somewhat surprising that this sci-ence module had so few contradictions, just 2.7%of all annotations, particularly given that many ofthe questions seem more likely to draw contradic-tions than unaddressed facets (e.g., many ask aboutthe effect on pitch and volume, typically elicitingone of two possible responses).
An analysis of theinter-annotator confusion matrix indicates that themost probable disagreement is between Inferredand  Unaddressed.
The second most likely dis-agreement is  between  Assumed and  Expressed.In discussing disagreements, the annotators almostalways  agree  quickly,  reinforcing  our  belief  thatwe will increase agreement significantly with addi-tional training.Label Count % Count %Expressed 348 38.0Inferred 51 5.6Assumed 258 28.2657 71.8Contra-Expr 21 2.3Contra-Infr 4 0.4 25 2.7Self-Contra 1 0.1 1 0.1Diff-Arg 33 3.6 33 3.6Unaddressed 199 21.7 199 21.7Table 4 Distribution of classifications (915 facets)5 Discussion and Future WorkThe goal of our fine-grained classification is to en-able  more  effective  tutoring  dialog  management.The  additional  labels  facilitate  understanding  thetype  of  mismatch  between  the  reference  answerand the student?s answer.
Breaking the referenceanswer down into low-level facets enables the tutorto provide feedback relevant specifically to the ap-propriate  facet  of  the  reference  answer.
In  thequestion answering domain, this facet-based classi-fication would allow systems to accumulate entail-ing  evidence  from  a  variety  of  corroboratingsources and incorporate answer details that mightnot be found in any single sentence.
In other appli-cations outside  of  the tutoring domain,  this  fine-grained classification can also facilitate more di-rected user feedback.
For example, both the addi-tional classifications and the break down of facetscan be used to justify system decisions, which isthe stated goal of the pilot task at the third RTEchallenge.The corpus described in this paper, which willbe released later this year (2007), represents a sub-stantial contribution to the entailment community,including an estimated 69000 facet entailment an-notations.
By contrast, three years of RTE chal-lenge  data  comprise  fewer  than  4600 entailment34annotations.
More  importantly,  this  is  the  onlycorpus that provides entailment information at thefine-grained  level  described  in  this  paper.
Thiswill enable application development that was notpractical previously.Future work includes training machine learningalgorithms to perform the classifications describedin this paper.
We also plan to annotate other as-pects of the students?
understanding that are not di-rect  inferences  of  reference  answer  knowledge.Consider example (4), in addition to the issues al-ready annotated, the student  contradicts  a law ofphysics  that  they  have  surely  encountered  else-where  in  the  text,  specifically that  longer stringsproduce lower, not higher, pitches.
Under the cur-rent annotation scheme this is not annotated, sinceit does not pertain directly to the reference answerwhich has to do with the effect of string tension.In other annotation plans, it would be very usefulfor training learning algorithms if we provide anindication of which student answer facets played arole in making the inferences classified.Initial  inter-annotator  agreement  results  lookpromising, obtaining substantial agreement accord-ing to the Kappa statistic.
We will continue to re-fine our annotation guidelines and provide furthertraining in order to push the agreement higher onall classifications.AcknowledgementWe would like to thank Martha Palmer for valu-able advice on this annotation effort.ReferencesAleven V, Popescu O, & Koedinger K. (2001) A tutorialdialogue system with knowledge-based understandingand classification of student explanations.
IJCAI WSknowledge & reasoning in practical dialogue systemsCallear, D., Jerrams-Smith, J., and Soh, V. (2001).
CAAof short non-MCQ answers.
In 5th Intl CAA.Cohen J.
(1960).
A coefficient of agreement for nominalscales.
Educational & Psych Measurement.
20:37-46.Cowie,  J.,  Lehnert,  W.G.
(1996).
Information  Extrac-tion.
In Communications of the ACM, 39(1), 80-91.Dagan,  Ido,  Glickman,  Oren,  and Magnini,  Bernardo.(2005).
The  PASCAL  Recognizing  Textual  Entail-ment Challenge.
In 1st RTE Challenge Workshop.Gildea, D. & Jurafsky, D. (2002).
Automatic labeling ofsemantic roles.
Computational Linguistics, 28:3, 245?288.Graesser, A.C., Hu, X., Susarla, S., Harter, D., Person,N.K., Louwerse, M., and Olde, B.
(2001).
AutoTutor:An  Intelligent  Tutor  and  Conversational  TutoringScaffold.
In 10th ICAI in Education, 47-49.Grice,  H.  Paul.
(1975).
Logic  and  conversation.
In  PCole and J Morgan, editors,  Syntax and Semantics,Vol 3, Speech Acts, 43?58.
Academic Press.Jordan, P. W., Makatchev, M., & VanLehn, K. (2004).Combining  competing  language  understanding  ap-proaches in an intelligent tutoring system.
In 7th ITS.Kipper, K, Dang, H, & Palmer, M. (2000).
Class-BasedConstruction of a Verb Lexicon.
AAAI 17th NCAIKoedinger,  K.R.,  Anderson,  J.R.,  Hadley,  W.H.
&Mark,  M.A.
(1997).
Intelligent  tutoring  goes  toschool in the big city.
Intl Jrnl of AI in Ed, 8, 30-43.Leacock,  Claudia.
(2004).
Scoring free-response auto-matically: A case study of a large-scale Assessment.Examens, 1(3).Lin, Dekang and Pantel, Patrick.
(2001).
Discovery ofinference rules for Question Answering.
In  NaturalLanguage Engineering, 7(4):343-360.Mitchell,  T.  Aldridge, N.,  and Broomhead, P.  (2003).Computerized marking of  short-answer  free-text  re-sponses.
In 29th IAEA.Nivre, J. and Scholz, M. (2004).
Deterministic Depen-dency Parsing of English Text.
In Proc COLING.Peters,  S,  Bratt,  E.O.,  Clark,  B.,  Pon-Barry,  H.  andSchultz, K. (2004).
Intelligent  Systems for TrainingDamage Control Assistants.
In Proc.
of ITSE.Pulman S.G. & Sukkarieh J.Z.
(2005).
Automatic ShortAnswer Marking.
ACL WS Bldg Ed Apps using NLP.Roll, I, Baker, R, Aleven, V, McLaren, B, & Koedinger,K.
(2005).
Modeling Students?
Metacognitive Errorsin Two Intelligent Tutoring Systems.
In UM 379?388Ros?, P. Roque, A., Bhembe, D. & VanLehn, K. (2003).A hybrid text classification approach for analysis ofstudent essays.
In Bldg Ed Apps using NLPShaw, Stuart.
(2004).
Automated writing assessment: areview of four conceptual models.
In Research Notes,Cambridge ESOL.
Downloaded Aug 10, 2005 fromhttp://www.cambridgeesol.org/rs_notes/rs_nts17.pdfSukkarieh, J.
& Pulman, S. (2005).
Information extrac-tion and machine learning: Auto-marking short freetext responses to science questions.
In Proc of AIED.VanLehn,  K.,  Lynch,  C.,  Schulze,  K.  Shapiro,  J.  A.,Shelby, R., Taylor, L., Treacy, D., Weinstein, A., &Wintersgill,  M.  (2005).
The Andes physics tutoringsystem: Five years of evaluations.
In 12th ICAI in EdWhittington,  D.,  Hunt,  H.  (1999).
Approaches  to  theComputerised  Assessment  of  Free-Text  Responses.Third ICAA.35
