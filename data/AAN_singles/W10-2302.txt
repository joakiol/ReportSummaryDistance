Proceedings of the 2010 Workshop on Graph-based Methods for Natural Language Processing, ACL 2010, pages 10?18,Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational LinguisticsTowards the Automatic Creation of a Wordnet from a Term-based LexicalNetworkHugo Gonc?alo Oliveira?CISUC, University of CoimbraPortugalhroliv@dei.uc.ptPaulo GomesCISUC, University of CoimbraPortugalpgomes@dei.uc.ptAbstractThe work described here aims to createa wordnet automatically from a semanticnetwork based on terms.
So, a cluster-ing procedure is ran over a synonymy net-work, in order to obtain synsets.
Then, theterm arguments of each relational tripleare assigned to the latter, originating awordnet.
Experiments towards our goalare reported and their results validated.1 IntroductionIn order perform tasks where understanding the in-formation conveyed by natural language is criti-cal, today?s applications demand better access tosemantic knowledge.
Knowledge about wordsand their meanings is typically structured in lex-ical ontologies, such as Princeton WordNet (Fell-baum, 1998), but this kind of resources is most ofthe times handcrafted, which implies much time-consuming human effort.
So, the automatic con-struction of such resources arises as an alterna-tive, providing less intensive labour, easier mainte-nance and allowing for higher coverage, as a trade-off for lower, but still acceptable, precision.This paper is written in the scope of a projectwhere several textual resources are being exploitedfor the construction of a lexical ontology for Por-tuguese.
We have already made a first approachon the extraction of relational triples from text,where, likewise Hearst (1992), we take advantageof textual patterns indicating semantic relations.However, the extracted triples are held betweentwo terms, which is not enough to build a lexicalontology capable of dealing with ambiguity.Therefore, we present our current approach to-wards the automatic integration of lexico-semanticknowledge into a single independent lexical on-tology, which will be structured on concepts and?supported by FCT scholarship SFRH/BD/44955/2008.adopt a model close to WordNet?s.
The task of es-tablishing synsets and mapping term-based triplesto them is closely related to word sense disam-biguation, where the only available context con-sists of the connections in the term-base network.After contextualising this work, our approach isdescribed.
It involves (i) a clustering procedure forobtaining a thesaurus from a synonymy network,(ii) the augmentation of the later with manuallycreated thesaurus, and (iii) mapping term-basedrelational triples to the thesaurus, to obtain a word-net.
Then, our experimentation results, as well astheir validation, are presented.
Briefly, we havetested the proposed approach on a term-based lex-ical network, extracted automatically from a dic-tionary.
Synsets were validated manually whilethe attached triples were validated with the helpof a web search engine.2 ContextOur ultimate goal is the automatic construction ofa broad-coverage structure of words according totheir meanings, also known as a lexical ontology,the first subject of this section.
We proceed witha brief overview on work concerned with mov-ing from term-based knowledge to synset-basedknowledge, often called ontologising.2.1 Lexical OntologiesDespite some terminological issues, lexical on-tologies can be seen both as a lexicon and as an on-tology (Hirst, 2004) and are significantly differentfrom classic ontologies (Gruber, 1993).
They arenot based on a specific domain and are intendedto provide knowledge structured on lexical items(words) of a language by relating them accordingto their meaning.
Moreover, the main goal of alexical ontology is to assemble lexical and seman-tic information, instead of storing common-senseknowledge (Wandmacher et al, 2007).10Princeton WordNet (Fellbaum, 1998) is themost representative lexico-semantic resource forEnglish and also the most accepted model of alexical ontology.
It is structured around groups ofsynonymous words (synsets), which describe con-cepts, and connections, denoting semantic rela-tions between those groups.
The success of Word-Net led to the adoption of its model by lexical re-sources in different languages, such as the onesin the EuroWordNet project (Vossen, 1997), orWordNet.PT (Marrafa, 2002), for Portuguese.However, the creation of a wordnet, as well asthe creation of most ontologies, is typically man-ual and involves much human effort.
Some au-thors (de Melo and Weikum, 2008) propose trans-lating Princeton WordNet to wordnets in other lan-guages, but if this might be suitable for several ap-plications, a problem arises because different lan-guages represent different socio-cultural realities,do not cover exactly the same part of the lexiconand, even where they seem to be common, severalconcepts are lexicalised differently (Hirst, 2004).Another popular alternative is to extract lexico-semantic knowledge and learn lexical ontologiesfrom text.
Research on this field is not new andvaried methods have been proposed to achieve dif-ferent steps of this task including the extraction ofsemantic relations (e.g.
(Hearst, 1992) (Girju etal., 2006)) or sets of similar words (e.g.
(Lin andPantel, 2002) (Turney, 2001)).Whereas the aforementioned works are basedon unstructured text, dictionaries started earlier(Calzolari et al, 1973) to be seen as an attrac-tive target for the automatic acquisition of lexico-semantic knowledge.
MindNet (Richardson et al,1998) is both an extraction methodology and a lex-ical ontology different from a wordnet, since itwas created automatically from a dictionary andits structure is based on such resources.
Neverthe-less, it still connects sense records with semanticrelations (e.g.
hyponymy, cause, manner).For Portuguese, PAPEL (Gonc?alo Oliveira etal., 2009) is a lexical network consisting of triplesdenoting semantic relations between words foundin a dictionary.
Other Portuguese lexical ontolo-gies, created by different means, are reviewed andcompared in (Santos et al, 2009) and (Teixeira etal., 2010).Besides corpora and dictionary processing, inthe later years, semi-structured collaborative re-sources, such as Wikipedia or Wiktionary, haveproved to be important sources of lexico-semanticknowledge and have thus been receiving more andmore attention by the community (see for instance(Zesch et al, 2008) (Navarro et al, 2009)).2.2 Other Relevant WorkMost of the methods proposed to extract relationsfrom text have term-based triples as output.
Sucha triple, term1 RELATION term2, indicates that apossible meaning of term1 is related to a possiblemeaning of term2 by means of a RELATION.Although it is possible to create a lexicalnetwork from the latter, this kind of networksis often impractical for computational applica-tions, such as the ones that deal with infer-ence.
For instance, applying a simple transitiverule, a SYNONYM OF b ?
b SYNONYM OF c?
a SYNONYM OF c over a set of term-basedtriples can lead to serious inconsistencies.
A curi-ous example in Portuguese, where synonymy be-tween two completely opposite words is inferred,is reported in (Gonc?alo Oliveira et al, 2009):queda SYNONYM OF ru?
?na ?
queda SYN-ONYM OF habilidade?
ru?
?na SYNONYM OFhabilidade.
This happens because natural lan-guage is ambiguous, especially when dealing withbroad-coverage knowledge.
In the given example,queda can either mean downfall or aptitude, whileru?
?na means ruin, destruction, downfall.A possible way to deal with ambiguity is toadopt a wordnet-like structure, where conceptsare described by synsets and ambiguous wordsare included in a synset for each of their mean-ings.
Semantic relations can thereby be unambigu-ously established between two synsets, and con-cepts, even though described by groups of words,bring together natural language and knowledge en-gineering in a suitable representation, for instance,for the Semantic Web (Berners-Lee et al, 2001).Of course that, from a linguistic point of view,word senses are complex and overlapping struc-tures (Kilgarriff, 1997) (Hirst, 2004).
So, despiteword sense divisions in dictionaries and ontologiesbeing most of the times artificial, this trade-off isneeded in order to increase the usability of broad-coverage computational lexical resources.In order to move from term-based triples toan ontology, Soderland and Mandhani (2007) de-scribe a procedure where, besides other stages,terms in triples are assigned to WordNet synsets.Starting with all the synsets containing a term in11a triple, the term is assigned to the synset withhigher similarity to the contexts from where thetriple was extracted, computed based on the termsin the synset, sibling synsets and direct hyponymsynsets.Two other methods for ontologising term-basedtriples are presented by Pantel and Pennacchiotti(2008).
One assumes that terms with the samerelation to a fixed term are more plausible to de-scribe the correct sense, so, to select the correctsynset, it exploits triples of the same type sharingone argument.
The other method, which seems toperform better, selects suitable synsets using gen-eralisation through hypernymy links in WordNet.There are other works where WordNet isenriched, for instance with information in itsglosses, domain knowledge extracted from text(e.g.
(Harabagiu and Moldovan, 2000) (Navigliet al, 2004)) or wikipedia entries (e.g.
(Ruiz-Casado et al, 2005)), thus requiring a disambigua-tion phase where terms are assigned to synsets.In the construction of a lexical ontology, syn-onymy plays an important role because it definesthe conceptual base of the knowledge to be rep-resented.
One of the reasons for using WordNetsynsets as a starting point for its representation isthat, while it is quite straightforward to define a setof textual patterns indicative of several semanticrelations between words (e.g.
hyponymy, part-of,cause) with relatively good quality, the same doesnot apply for synonymy.
In opposition to otherkinds of relation, synonymous words, despite typi-cally sharing similar neighbourhoods, may not co-occur frequently in unstructured text, especially inthe same sentence (Dorow, 2006), leading to fewindicative textual patterns.
Therefore, most of theworks on synonymy extraction from corpora relyon statistics or graph-based methods (e.g.
(Linand Pantel, 2002) (Turney, 2001) (Dorow, 2006)).Nevertheless, methods for synonymy identifica-tion based on co-occurrences (e.g.
(Turney, 2001))are more prone to identify similar words or nearsynonyms than real synonyms.On the other hand, synonymy instances can bequite easily extracted from resources structured onwords and meanings, such as dictionaries, by tak-ing advantage not only of textual patterns, morefrequent in those resources (e.g.
tambe?m con-hecido por/como, o mesmo que, for Portuguese),but also of definitions consisting of only one wordor a enumeration, which typically contain syn-onyms of the defined word.
So, as it is possibleto create a lexical network from a set of relationaltriples (a R b), a synonymy network can be createdout of synonymy instances (a SYNONYM OFb).
Since these networks tend to have a clusteredstructure, Gfeller et al (2005) propose a clusteringprocedure to improve their utility.3 Research GoalsThe research presented here is in the scope of aproject whose final goal is to create a lexical ontol-ogy for Portuguese by automatic means.
Althoughthere are clear advantages of using resources al-ready structured on words and meanings, dictio-naries are static resources which contain limitedknowledge and are not always available for thiskind of research.
On the other hand, there is muchtext available on the most different subjects, butfree text has few boundaries, leading to more am-biguity and parsing issues.Therefore, it seems natural to create a lexi-cal ontology with knowledge from several tex-tual sources, from (i) high precision structured re-sources, such as manually created thesaurus, to(ii) semi-structured resources such as dictionariesor collaborative encyclopedias, as well as (iii) un-structured textual corpora.
Likewise Wandmacheret al (2007) propose for creating a lexical ontol-ogy for German, these are the general lines we willfollow in our research, but for Portuguese.Considering each resource specificities, includ-ing its organisation or the vocabulary used, the ex-traction procedures might be significantly differ-ent, but they should all have one common output:a set of term-based relational triples that will beintegrated in a single lexical ontology.Whereas the lexical network established by thetriples could be used, these networks are not suit-able for several tasks, as discussed in Section 2.2.A fragment of a synonymy network extracted froma Portuguese dictionary can be seen in Figure 1.Since all the connections imply synonymy, thenetwork suggests that all the words are synony-mous, which is not true.
For example, the wordcopista may have two very distinct meanings: (a) aperson who writes copies of written documents or(b) someone who drinks a lot of wine.
On the otherhand, other words which may refer to the sameconcept as, for instance, meaning (a) of copista,such as escrevente, escriva?o or transcritor.So, in order to deal with ambiguity in natural12language, we will adopt a wordnet-like structurewhich enables the establishment of unambiguoussemantic relations between synsets.Figure 1: Fragment of a synonymy network.4 ApproachConsidering our goal, a set of term-based triplesgoes through the following stages: (i) clusteringover the synonymy network for the establishmentof synsets, to obtain a thesaurus; (ii) augmenta-tion of the thesaurus by merging it with synsetsfrom other resources; (iii) assignment of each ar-gument of a term-based triple (except synonymy)to a synset in the thesaurus, to obtain a wordnet.Note that stages (i) and (ii) are not both manda-tory, but at least one must be performed to obtainthe synsets.Looking at some of the works referred in Sec-tion 2.2, ours is different because it does not re-quire a conceptual base such as WordNet.
Also,it integrates knowledge from different sources andtries to disambiguate each word using only knowl-edge already extracted and not the context wherethe word occurs.4.1 Clustering for a thesaurusThis stage was originally defined after lookingat disconnected pieces of a synonymy networkextracted from a dictionary, which had a clus-tered structure apparently suitable for identifyingsynsets.
This is also noticed by Gfeller et al(2005) who have used the Markov Clustering al-gorithm (MCL) (van Dongen, 2000) to find clus-ters in a synonymy network.Therefore, since MCL had already been appliedto problems very close to ours (e.g.
(Gfeller et al,2005), (Dorow, 2006)), it seemed to suit our pur-pose ?
it would not only organise a term-based net-work into a thesaurus, but, if a network extractedfrom several resources is used, clustering wouldhomogenise the synonymy representation.MCL finds clusters by simulating random walkswithin a graph by alternately computing randomwalks of higher length, and increasing the prob-abilities of intra-cluster walks.
It can be brieflydescribed in five steps: (i) take the adjacency ma-trix A of the graph; (ii) normalise each column ofA to 1 in order to obtain a stochastic matrix S;(iii) compute S2; (iv) take the ?th power of everyelement of S2 and normalise each column to 11;(v) go back to (ii) util MCL converges to a matrixidempotent under steps (ii) and (iii).Since MCL is a hard-clustering algorithm, it as-signs each term to only one cluster thus remov-ing ambiguities.
To deal with this, Gfeller et al(2005) propose an extension to MCL for findingunstable nodes in the graph, which frequently de-note ambiguous words.
This is done by addingrandom stochastic noise, ?, to the non-zero entriesof the adjacency matrix and then running MCLwith noise several times.
Looking at the clustersobtained by each run, a new matrix can be filledbased on the probability of each pair of words be-longing to the same cluster.We have adopted this procedure, with slight dif-ferences.
First, we observed that, for the networkwe used, the obtained clusters were closer to thedesired results if?0.5 < ?
< 0.5.
Additionally, inthe first step of MCL, we use frequency-weightedadjacency matrixes F , where each element Fijcorresponds to the number of existing synonymyinstances between i and j.
Although using onlyone dictionary each synonymy instance will be ex-tracted at most two times (a SYNONYM OF band b SYNONYM OF a), if more resources areused, it will strengthen the probability that twowords appearing frequently as synonyms belongto the same cluster.Therefore, the clustering stage has the follow-ing steps: (i) split the original network into sub-networks, such that there is no path between twoelements in different sub-networks, and calculatethe frequency-weighted adjacency matrix F ofeach sub-network; (ii) add stochastic noise to eachentry of F , Fij = Fij + Fij ?
?
; (iii) run MCL,with ?
= 1.6, over F for 30 times; (iv) use the(hard) clustering obtained by each one of the 30runs to create a new matrix P with the probabil-1Increasing ?
(typically 1.5 < ?
< 2) increases the gran-ularity of the clusters.13ities of each pair of words in F belonging to thesame cluster; (v) create the clusters based on Pand on a given threshold ?
= 0.2.
If Pij > ?, i andj belong to the same cluster; (vi) in order to cleanthe results, remove: (a) big clusters, B, if thereis a group of clusters C = C1, C2, ...Cn such thatB = C1?C2?
...?Cn; (b) clusters completely in-cluded in other clusters.
Applying this procedureto the network in Figure 1 results in the four repre-sented clusters.
There, ambiguous words escriva?oand escriba are included in two different clusters.4.2 Merging synsets for thesaurusaugmentationIn this stage, other resources with synsets, such asmanually created thesaurus, are merged togetherand then merged with the thesaurus obtained in theprevious stage, by the following procedure: (i) de-fine one thesaurus as the basis B and the other asT ; (ii) create a new empty thesaurus M and copyall the synsets in B to M ; (iii) for each synsetTi ?
T , find the synsets Bi ?
B with higher Jac-card coefficient2 c, and add them to a set of synsetsJ ?
B.
(iv) considering c and J , do one of thefollowing: (a) if c = 1, it means that the synset isalready in M , so nothing is done; (b) if c = 0, Tiis copied to M ; (c) if |J | = 1, the synset in J iscopied toM ; (d) if |J | > 1, a new set, n = Ti?J ?where J ?
= ?|J |i=0Ji, Ji ?
J , is created, and allelements of J are removed from M .The synsets of the resulting thesaurus will beused as the conceptual base in which the term-based triples are going to be mapped.4.3 Assigning terms to synsetsAfter the previous stages, the following are avail-able: (i) a thesaurus T and (ii) a term-based se-mantic network, N , where each edge has a type,R, and denotes a semantic relation held betweenthe meaning of the terms in the two nodes it con-nects.
Using T andN , this stage tries to map term-based triples to synset-based triples, or, in otherwords, assign each term, a and b, in each triple,(a R b) ?
N , to suitable synsets.
The result is aknowledge base organised as a wordnet.In order to assign a to a synset A, b is fixedand all the synsets containing a, Sa ?
T , are col-lected.
If a is not in the thesaurus, it is assigned toa new synset A = (a).
Otherwise, for each synsetSai ?
Sa, nai is the number of terms t ?
Sai such2Jaccard(A,B) = A ?B/A ?Bthat (t R b) holds3.
Then, pai =nai|Sai|is calcu-lated.
Finally, all the synsets with the highest paiare added to C and (i) if |C| = 1 , a is assigned tothe only synset inC; (ii) if |C| > 1, C ?
is the set ofelements ofC with the highest na and, if |C ?| = 1,a is assigned the synset in C ?, unless pai < ?
4;(iii) if it is not possible to assign a synset to a, itremains unassigned.
Term b is assigned to a synsetusing this procedure, but fixing a.If hypernymy links are already established,semi-mapped triples, where one of the argumentsis assigned to a synset and the other is not, (AR b) or (a R B), go to a second phase.
There,hypernymy is exploited together with the assign-ment candidates, in C, to help assigning the unas-signed term in each semi-mapped triple, or to re-move triples that can be inferred.
Take for instance(A R b).
If there is one synset Ci ?
C with:?
a hypernym synset H , (H HYPERNYM OFCi) and a triple (A R H), b would be as-signed to Ci, but, since hyponyms inherit allthe properties of their hypernym, the result-ing triple can be inferred and is thus ignored:(A R H) ?
(H HYPERNYM OF Ci)?
(A R Ci)5For example, if H=(mammal) and Ci=(dog), possi-ble values of A and R are A=(hair) R=PART OF;A=(animal) R=HYPERNYM OF?
a hyponym synset H , (Ci HYPERNYM OFH) and a triple (A R H), b is assigned to Ci.Furthermore, if all the hyponyms of Ci, (CiHYPERNYM OF Ii), are also related toA inthe same way, (AR Ii), it can be inferred thatIi inherits the relation from Ci.
So, all thelater triples can be inferred and thus removed.For example, if H=(dog), Ii=(cat), Ij=(mouse)and Ci=(mammal), possible values of A andR are A=(hair) R=PART OF; A=(animal)R=HYPERNYM OF3If R is a transitive relation, the procedure may benefitfrom applying one level of transitivity to the network: x R y?
y R z?
x R z.
However, since relations are held betweenterms, some obtained triples might be incorrect.
So, althoughthe latter can be used to help selecting a suitable synset, theyshould not be mapped to synsets themselves.4?
is a threshold defined to avoid that a is assigned to abig synset where a, itself, is the only term related to b5Before applying these rules it is necessary to make surethat all relations are represented only in one way, otherwisethey might not work.
For instance, if the decision is to rep-resent part-of triples in the form part PART OF whole,triples whole HAS PART part must be reversed.
Further-more, these rules assume that hypernymy relations are all rep-resented hypernym HYPERNYM OF hyponym and nothyponym HYPONYM OF hypernym.145 ExperimentationIn this section we report experimental results ob-tained after applying our procedure to part of thelexical network of PAPEL (Gonc?alo Oliveira et al,2009).
The clustering procedure was first ran overPAPEL?s noun synonymy network in order to ob-tain the synsets which were later merged with twomanually created thesaurus.
Finally, hypernym-of, member-of and part-of triples of PAPEL weremapped to the thesaurus by assigning a synset toeach term argument.5.1 Resources usedFor experimentation purposes, freely availablelexical resources for Portuguese were used.
First,the last version of PAPEL, 2.0, a lexical networkfor Portuguese created automatically from a dic-tionary, as referred in Section 2.
PAPEL 2.0contains approximately 100,000 words, identifiedby their orthographical form, and approximately200,000 term-based triples relating the words bydifferent types of semantic relations.In order to enrich the thesaurus obtained fromPAPEL, TeP (Dias-Da-Silva and de Moraes, 2003)and OpenThesaurus.PT6 (OT), were used.
Both ofthem are manually created thesaurus, for Brazil-ian Portuguese and European Portuguese respec-tively, modelled after Princeton WordNet (Fell-baum, 1998) and thus containing synsets.
Besidesbeing the only freely available thesaurus for Por-tuguese we know about, TeP and OT were used to-gether with PAPEL because, despite representingthe same kind of knowledge, they are mostly com-plementary, which is also observed by (Teixeira etal., 2010) and (Santos et al, 2009).Note that, for experimentation purposes, wehave only used the parts of these resources con-cerning nouns.5.2 Thesaurus creationThe first step for applying the clustering proce-dure is to create PAPEL?s synonymy network,which is established by its synonymy instances,a SYNONYM OF b.
After splitting the networkinto independent disconnected sub-networks, wenoticed that it was composed by a huge sub-network, with more than 16,000 nodes, and sev-eral very small networks.
If ambiguity was notresolved, this would suggest that all the 16,000words had the same meaning, which is not true.6http://openthesaurus.caixamagica.pt/TeP OT CLIP TOPWordsQuantity 17,158 5,819 23,741 30,554Ambiguous 5,867 442 12,196 13,294Most ambiguous 20 4 47 21SynsetsQuantity 8,254 1,872 7,468 9,960Avg.
size 3.51 3.37 12.57 6.6Biggest 21 14 103 277Table 1: (Noun) thesaurus in numbers.Hypernym of Part of Member ofTerm-based triples 62,591 2,805 5,9291stMapped 27,750 1,460 3,962Same synset 233 5 12Already present 3,970 40 167Semi-mapped triples 7,952 262 3572ndMapped 88 1 0Could be inferred 50 0 0Already present 13 0 0Synset-based triples 23,572 1,416 3,783Table 2: Results of triples mappingA small sample of this problem can be observedin Figure 1.We then ran the clustering procedure and thethesaurus of PAPEL, CLIP, was obtained.
Finally,we used TeP as the base thesaurus and merged it,first with OT, and then with CLIP, giving rise tothe noun thesaurus we used in the rest of the ex-perimentation, TOP.Table 1 contains information about each oneof the thesaurus, more precisely, the quantityof words, words belonging to more than onesynset (ambiguous), the number of synsets wherethe most ambiguous word occurs, the quantityof synsets, the average synset size (number ofwords), and the size of the biggest synset7.5.3 Mapping the triplesThe mapping procedure was applied to all thehypernym-of, part-of and member-of term-basedtriples of PAPEL, distributed according to Table 2where additional numbers on the mapping are pre-sented.
After the first phase of the mapping,33,172 triples had both of their terms assigned toa synset, and 10,530 had only one assigned.
How-ever, 4,427 were not really added, either becausethe same synset was assigned to both of the termsor because the triple had already been added afteranalysing other term-based triple.
In the secondphase, only 89 new triples were mapped and, fromthose, 13 had previously been added while other50 triples were discarded or not attached becausethey could be inferred.
Another interesting fact isthat 19,638 triples were attached to a synset withonly one term.
From those, 5,703 had a synset7Synsets with only one word were ignored in the construc-tion of Table 1.15with only one term in both arguments.We ended up with a wordnet with 27,637synsets, 23,572 hypernym-of, 1,416 part-of and3,783 member-of synset-based triples.6 Validation of the resultsEvaluation of a new broad-coverage ontology ismost of the times performed by one of two means:(i) manual evaluation of a representative subset ofthe results; (ii) automatic comparison with a goldstandard.
However, while for English most re-searchers use Princeton WordNet as a gold stan-dard, for other languages it is difficult to findsuitable and freely available consensual resources.Considering Portuguese, as we have said earlier,TeP and OT are effectively two manually createdthesaurus but, since they are more complementarythan overlapping to PAPEL, we thought it wouldbe better to use them to enrich our resource.There is actually a report (Raman and Bhat-tacharyya, 2008) with an automatic evaluation ofsynsets, but we decided no to follow it becausethis evaluation is heavily based on a dictionary andwe do not have unrestricted access to a full andupdated dictionary of Portuguese and also, indi-rectly by PAPEL, a dictionary was one of our mainsources of information.Therefore, our choice relied on manual valida-tion of the synsets of CLIP and TOP.
Furthermore,synset-based triples were validated in an alterna-tive automatic way using a web search engine.6.1 Manual validation of synsetsTen reviewers took part in the validation of ten ran-dom samples with approximately 50 synsets fromeach thesaurus.
We made sure that each synset wasnot in more than one sample and synsets with morethan 50 terms were not validated.
Also, in order tomeasure the reviewer agreement, each sample wasanalysed by two different reviewers.
Given a sam-ple, each reviewer had to classify each synset as:correct (1), if, in some context, all the terms of thesynset could have the same meaning, or incorrect(0), if at least one term of the synset could nevermean the same as the others.
The reviewers wereadvised to look for the possible meanings of eachword in different dictionaries.
Still, if they couldnot find them, or if they did not know how to clas-sify the synset, they had a third option, N/A (2).In the end, 519 synsets of CLIP and 480 ofTOP were validated.
When organising the vali-dation results we noticed that the biggest synsetswere the ones with more problems.
So, besides thecomplete validation results, Table 3 also containsthe results considering only synsets of ten or lesswords, when a ?
is after the name of the thesaurus.The presented numbers are the average betweenthe classifications given by the two reviewers andthe agreement rate corresponds to the number oftimes both reviewers agreed on the classification.Even though these results might be subjec-tive, since they are based on the reviewers cri-teria and on the dictionaries they used, they cangive an insight on the quality of the synsets.The precision results are acceptable and are im-proved if the automatically created thesaurus ismerged with the ones created manually, andalso when bigger synsets are ignored.
Mostof the times, big synsets are confusing becausethey bring together more than one concept thatshare at least one term.
For instance, take thesynset: insobriedade, desmedida, imoderac?a?o,excesso, nimiedade, desmando, desbragamento,troco, descontrolo, superabunda?ncia, desbunda,desregramento, demasia, incontine?ncia, imodici-dade, superac?a?o, intemperanc?a, descomedimento,superfluidade, sobejida?o, acrasia, where there is amix of the concepts: (a) insobriety, not followingall the rules, heedless of the consequences and, (b)surplus.
Both of these concepts can be referred toas an excess (excesso).6.2 Automatic validation of triplesThe automatic validation of the triples attached toour wordnet consisted of using Google web searchengine to look for evidence on their truth.
Thisprocedure started by removing terms whose oc-currences in Google were less than 5,000.
Synsetsthat became empty were not considered and, fromthe rest, a sample was selected for each one of thethree types of relation.Following the idea in (Gonc?alo Oliveira et al,2009), a set of natural language generic patterns,indicative of each relation, was defined having inmind their input to Google8.
Then, for each triple(A R B), the patterns were used to search for ev-8Hypernymy patterns included: [hypo] e?
um|uma(tipo|forma|variedade|...)* de [hyper], [hypo] e outros|outras[hyper] or [hyper] tais como [hypo].
Patterns for part-of andmember-of were the same because these relations can be ex-pressed in very similar ways, and included: [part/member] e?
(parte|membro|porc?a?o) do|da [whole/group], [part/member](faz parte)* do|da [whole/group] or [whole/group] e?
um(grupo|conjunto|...) de [part/member].16Sample Correct Incorrect N/A AgreementCLIP 519 sets 65.8% 31.7% 2.5% 76.1%CLIP?
310 sets 81.1% 16.9% 2.0% 84.2%TOP 480 sets 83.2% 15.8% 1.0% 82.3%TOP?
448 sets 86.8% 12.3% 0.9% 83.0%Table 3: Results of manual synset validation.Relation Sample size ValidationHypernymy of 419 synsets 44,1%Member of 379 synsets 24,3%Part of 290 synsets 24,8%Table 4: Automatic validation of triplesidence on each combination of terms a ?
A andb ?
B connected by a pattern indicative of R.The triple validation score was then calculated byexpression 1, where found(A,B,R) = 1 if evi-dence is found for the triple or 0 otherwise.score =|A|?i=1|B|?j=1found(A,B,R)|A| ?
|B|(1)Table 4 shows the results obtained for each val-idated sample.
Pantel and Pennacchiotti (2008)perform a similar task and present precision resultsfor part-of (40.7%-57.4%) and causation (40.0%-45%) relations.
It is however not possible to makea straight comparison.
For their experimentation,they selected only correct term-based triples ex-tracted from text and their results were manuallyvalidated by human judges.
On the other hand, wehave used term-based triples extracted automati-cally from a dictionary, with high but not 100%precision, from where we did not choose only thecorrect ones, and we have used synsets obtainedfrom our clustering procedure which, once again,have lower precision.
Moreover, we validated ourresults with Google where, despite its huge dimen-sion, there are plenty of ways to denote a seman-tic relation, when we had just a small set textualpatterns.
Also, despite occurring more than 5,000times in Google, some terms correctly included ina synset were conveying less common meanings.Nevertheless, we could not agree more withPantel and Pennacchiotti (2008) who state that at-taching term-based triples to an ontology is not aneasy task.
Therefore, we believe our results to bepromising and, if more refined rules are added toour set, which is still very simple, they will surelybe improved.7 Concluding remarksWe have presented our first approach on two cru-cial steps on the automatic creation of a wordnetlexical ontology.
Clustering proved to be a goodalternative to create a thesaurus from a dictionary?ssynonymy network, while a few rules can be de-fined to attach a substantial number of term-basedtriples to a synset based resource.Despite interesting results, in the future we willwork on refining the attachment rules and start in-tegrating other relations such as causation or pur-pose.
Furthermore, we are devising new methodsfor attaching terms to synsets.
For instance, wehave recently started to do some experiences withan attaching method which uses the lexical net-work?s adjacency matrix to find the most similarpair of synsets, each of them containing one of thearguments of a term-based triple.ReferencesTim Berners-Lee, James Hendler, and Ora Lassila.2001.
The Semantic Web.
Scientific American,May.Nicoletta Calzolari, Laura Pecchia, and Antonio Zam-polli.
1973.
Working on the italian machine dictio-nary: a semantic approach.
In Proc.
5th Conferenceon Computational Linguistics, pages 49?52, Morris-town, NJ, USA.
Association for Computational Lin-guistics.Gerard de Melo and Gerhard Weikum.
2008.
On theutility of automatically generated wordnets.
In Proc.4th Global WordNet Conf.
(GWC), pages 147?161,Szeged, Hungary.
University of Szeged.Bento Carlos Dias-Da-Silva and Helio Robertode Moraes.
2003.
A construc?a?o de um the-saurus eletro?nico para o portugue?s do Brasil.
ALFA,47(2):101?115.Beate Dorow.
2006.
A Graph Model forWords and their Meanings.
Ph.D. thesis, Institutfur Maschinelle Sprachverarbeitung der UniversitatStuttgart.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database (Language, Speech, andCommunication).
The MIT Press.David Gfeller, Jean-Ce?dric Chappelier, and PauloDe Los Rios.
2005.
Synonym Dictionary Im-provement through Markov Clustering and Cluster-ing Stability.
In Proc.
of International Symposiumon Applied Stochastic Models and Data Analysis(ASMDA), pages 106?113.17Roxana Girju, Adriana Badulescu, and Dan Moldovan.2006.
Automatic discovery of part-whole relations.Computational Linguistics, 32(1):83?135.Hugo Gonc?alo Oliveira, Diana Santos, and PauloGomes.
2009.
Relations extracted from a por-tuguese dictionary: results and first evaluation.
InLocal Proc.
14th Portuguese Conf.
on Artificial In-telligence (EPIA).Thomas R. Gruber.
1993.
A translation approach toportable ontology specifications.
Knowledge Acqui-sition, 5(2):199?220.Sanda M. Harabagiu and Dan I. Moldovan.
2000.Enriching the wordnet taxonomy with contextualknowledge acquired from text.
In Natural languageprocessing and knowledge representation: languagefor knowledge and knowledge for language, pages301?333.
MIT Press, Cambridge, MA, USA.Marti A. Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In Proc.
14th Conf.on Computational Linguistics, pages 539?545, Mor-ristown, NJ, USA.
Association for ComputationalLinguistics.Graeme Hirst.
2004.
Ontology and the lexicon.
InSteffen Staab and Rudi Studer, editors, Handbookon Ontologies, International Handbooks on Informa-tion Systems, pages 209?230.
Springer.Adam Kilgarriff.
1997.
?I don?t believe in wordsenses?.
Computing and the Humanities, 31(2):91?113.Dekang Lin and Patrick Pantel.
2002.
Concept discov-ery from text.
In Proc.
19th Intl.
Conf.
on Computa-tional Linguistics (COLING), pages 577?583.Palmira Marrafa.
2002.
Portuguese Wordnet: gen-eral architecture and internal semantic relations.DELTA, 18:131?146.Emmanuel Navarro, Franck Sajous, Bruno Gaume,Laurent Pre?vot, ShuKai Hsieh, Tzu Y. Kuo, PierreMagistry, and Chu R. Huang.
2009.
Wiktionaryand nlp: Improving synonymy networks.
In Proc.Workshop on The People?s Web Meets NLP: Col-laboratively Constructed Semantic Resources, pages19?27, Suntec, Singapore.
Association for Compu-tational Linguistics.Roberto Navigli, Paola Velardi, Alessandro Cuc-chiarelli, and Francesca Neri.
2004.
Extendingand enriching wordnet with ontolearn.
In Proc.2nd Global WordNet Conf.
(GWC), pages 279?284,Brno, Czech Republic.
Masaryk University.Patrick Pantel and Marco Pennacchiotti.
2008.
Auto-matically harvesting and ontologizing semantic rela-tions.
In Paul Buitelaar and Phillip Cimmiano, ed-itors, Ontology Learning and Population: Bridgingthe Gap between Text and Knowledge.
IOS Press.J.
Raman and Pushpak Bhattacharyya.
2008.
Towardsautomatic evaluation of wordnet synsets.
In Proc.4th Global WordNet Conf.
(GWC), pages 360?374,Szeged, Hungary.
University of Szeged.Stephen D. Richardson, William B. Dolan, and LucyVanderwende.
1998.
Mindnet: Acquiring and struc-turing semantic information from text.
In Proc.
17thIntl.
Conf.
on Computational Linguistics (COLING),pages 1098?1102.Maria Ruiz-Casado, Enrique Alfonseca, and PabloCastells.
2005.
Automatic assignment of wikipediaencyclopedic entries to wordnet synsets.
In Proc.Advances in Web Intelligence Third Intl.
AtlanticWeb Intelligence Conf.
(AWIC), pages 380?386.Springer.Diana Santos, Anabela Barreiro, Lu?
?s Costa, Cla?udiaFreitas, Paulo Gomes, Hugo Gonc?alo Oliveira,Jose?
Carlos Medeiros, and Rosa?rio Silva.
2009.
Opapel das relac?o?es sema?nticas em portugue?s: Com-parando o TeP, o MWN.PT e o PAPEL.
In Actas doXXV Encontro Nacional da Associac?a?o Portuguesade Lingu?
?stica (APL).
forthcomming.Stephen Soderland and Bhushan Mandhani.
2007.Moving from textual relations to ontologized rela-tions.
In Proc.
AAAI Spring Symposium on MachineReading.Jorge Teixeira, Lu?
?s Sarmento, and Euge?nio C.Oliveira.
2010.
Comparing verb synonym resourcesfor portuguese.
In Computational Processing of thePortuguese Language, 9th Intl.
Conference, Proc.
(PROPOR), pages 100?109.Peter D. Turney.
2001.
Mining the web for synonyms:PMI?IR versus LSA on TOEFL.
In Proc.
12th Euro-pean Conf.
on Machine Learning (ECML), volume2167, pages 491?502.
Springer.S.
M. van Dongen.
2000.
Graph Clustering by FlowSimulation.
Ph.D. thesis, University of Utrecht.Piek Vossen.
1997.
Eurowordnet: a multilingualdatabase for information retrieval.
In Proc.
DE-LOS workshop on Cross-Language Information Re-trieval, Zurich.Tonio Wandmacher, Ekaterina Ovchinnikova, UlfKrumnack, and Henrik Dittmann.
2007.
Extrac-tion, evaluation and integration of lexical-semanticrelations for the automated construction of a lexicalontology.
In Third Australasian Ontology Workshop(AOW), volume 85 of CRPIT, pages 61?69, GoldCoast, Australia.
ACS.Torsten Zesch, Christof Mu?ller, and Iryna Gurevych.2008.
Extracting lexical semantic knowledge fromWikipedia and Wiktionary.
In Proc.
6th Intl.Language Resources and Evaluation (LREC), Mar-rakech, Morocco.18
