Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 562?567, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguisticsnlp.cs.aueb.gr: Two Stage Sentiment AnalysisProdromos Malakasiotis, Rafael Michael KarampatsisKonstantina Makrynioti and John PavlopoulosDepartment of InformaticsAthens University of Economics and BusinessPatission 76, GR-104 34 Athens, GreeceAbstractThis paper describes the systems with whichwe participated in the task Sentiment Analysisin Twitter of SEMEVAL 2013 and specificallythe Message Polarity Classification.
We useda 2-stage pipeline approach employing a lin-ear SVM classifier at each stage and severalfeatures including BOW features, POS basedfeatures and lexicon based features.
We havealso experimented with Naive Bayes classi-fiers trained with BOW features.1 IntroductionDuring the last years, Twitter has become a verypopular microblogging service.
Millions of userspublish messages every day, often expressing theirfeelings or opinion about a variety of events, top-ics, products, etc.
Analysing this kind of contenthas drawn the attention of many companies and re-searchers, as it can lead to useful information forfields, such as personalized marketing or social pro-filing.
The informal language, the spelling mis-takes, the slang and special abbreviations that arefrequently used in tweets differentiate them fromtraditional texts, such as articles or reviews, andpresent new challenges for the task of sentimentanalysis.The Message Polarity Classification is defined asthe task of deciding whether a message M conveys apositive, negative or neutral sentiment.
For instanceM1 below expresses a positive sentiment, M2 a neg-ative one, while M3 has no sentiment at all.M1: GREAT GAME GIRLS!!
On to districts Mondayat Fox!!
Thanks to the fans for coming out :)M2: Firework just came on my tv and I just broke downand sat and cried, I need help okayM3: Going to a bulls game with Aaliyah & hope nextThursdayAs sentiment analysis in Twitter is a very recentsubject, it is certain that more research and improve-ments are needed.
This paper presents our approachfor the subtask of Message Polarity Classification(Wilson et al 2013) of SEMEVAL 2013.
We used a2-stage pipeline approach employing a linear SVMclassifier at each stage and several features includ-ing bag of words (BOW) features, part-of-speech(POS) based features and lexicon based features.We have also experimented with Naive Bayes clas-sifiers trained with BOW features.The rest of the paper is organised as follows.
Sec-tion 2 provides a short analysis of the data usedwhile section 3 describes our approach.
Section 4describes the experiments we performed and the cor-responding results and section 5 concludes and giveshints for future work.2 DataBefore we proceed with our system description webriefly describe the data released by the organisers.The training set consists of a set of IDs correspond-ing to tweet messages, along with their annotations.A message can be annotated as positive, negativeor neutral.
In order to address privacy concerns,rather than releasing the original Tweets, the organ-isers chose to provide a python script for download-ing the data.
This resulted to different training setsfor the participants since tweets may often become562SEMEVAL STATS TRAIN (ours) TRAIN (official) Dev DEV (final) TEST (sms)Positive 3280 37,57% 3640 37,59% 524 34,82% 575 34,76% 492 23,50%Negative 1289 14,77% 1458 15,06% 308 20,47% 340 20,56% 394 18,82%Neutral 4161 47,66% 4586 47,36% 673 44,72% 739 44,68% 1208 57,69%TOTAL 8730 9684 1505 1654 209437,57%14,77%47,66%Training data class distributionPositiveNegativeNeutral34,76%20,56%44,68%Development data class distributionPositiveNegativeNeutral23,50%18,82% 57,69%Test data class distribution (sms)PositiveNegativeNeutral41,23%15,76%43,01%Test data class distribution (twitter)PositiveNegativeNeutral(a)SEMEVAL STATS TRAIN (ours) TRAIN (official) Dev DEV (final) TEST (sms)Positive 3280 37,57% 3640 37,59% 524 34,82% 575 34,76% 492 23,50%Negative 1289 14,7 % 1458 15,06% 308 20,47% 340 20,56% 394 18, 2%Neutral 4161 47,6 % 4586 47,36% 673 4 ,72% 739 4 ,68% 1208 57,69%TOTAL 8730 9684 1505 1654 209437,57%14,7 %47,6 %Training data clas  distributionPositiveNegativeNeutral34,76%20,56%4 ,68%Development data clas  distributionPosit veNegativeNeutral23,50%18,82% 57,69%Test data class distribution (sms)PositiveNegativeNeutral41,23%15,76%43,01%Test data clas  distribution (twit er)Posit veNegativeNeutral(b)Figure 1: Train and Development data class distribution.unavailable due to a number of reasons.
Concerningthe development and test sets the organisers down-loaded and provided the tweets.
1 A first analysisof the data indicates that they suffer from a class im-balance problem.
Specifically the training data wehave downloaded contain 8730 tweets (3280 posi-tive, 1289 negative, 4161 neutral), while the devel-opment set contains 1654 tweets (575 positive, 340negative, 739 neutral).
Figure 1 illustrates the prob-lem on train and development sets.3 System OverviewThe system we propose is a 2?stage pipeline pro-cedure employing SVM classifiers (Vapnik, 1998)to detect whether each message M expresses pos-itive, negative or no sentiment (figure 2).
Specifi-cally, during the first stage we attempt to detect if Mexpresses a sentiment (positive or negative) or not.If so, M is called ?subjective?, otherwise it is called?objective?
or ?neutral?.2 Each subjective messageis then classified in a second stage as ?positive?
or?negative?.
Such a 2?stage approach has also beensuggested in (Pang and Lee, 2004) to improve sen-timent classification of reviews by discarding objec-tive sentences, in (Wilson et al 2005a) for phrase-level sentiment analysis, and in (Barbosa and Feng,2010) for sentiment analysis on Twitter messages.1A separate test set with SMS messages was also providedby the organisers to measure performance of systems over othertypes of message data.
No training and development data wereprovided for this set.2Hereafter we will use the terms ?objective?
and ?neutral?interchangeably.3.1 Data PreprocessingBefore we could proceed with feature engineering,we performed several preprocessing steps.
To bemore precise, a twitter specific tokeniser and part-of-speech (POS) tagger (Ritter et al 2011) wereused to obtain the tokens and the correspondingPOS tags which are necessary for a particular setof features to be described later.
In addition to these,six lexicons, originating from Wilson?s (2005b) lexi-con, were created.
This lexicon contains expressionsthat given a context (i.e., surrounding words) indi-cate subjectivity.
The expression that in most con-text expresses sentiment is considered to be ?strong?subjective, otherwise it is considered weak subjec-tive (i.e., it has specific subjective usages).
So, wefirst split the lexicon in two smaller, one contain-ing strong and one containing weak subjective ex-pressions.
Moreover, Wilson also reports the polar-ity of each expression out of context (prior polarity)which can be positive, negative or neutral.
As a con-sequence, we further split each of the two lexiconsinto three smaller according to the prior polarity ofthe expression, resulting to the following six lexi-cons:S+ : Contains strong subjective expressions withpositive prior polarity.S?
: Contains strong subjective expressions withnegative prior polarity.S0 : Contains strong subjective expressions withneutral prior polarity.563Subjectivity detectionSVMPolarity detectionSVMObjectivemessagesMessagesSubjectivemessagesPositivemessagesNegativemessagesFigure 2: Our 2?stage pipeline procedure.W+ : Contains weak subjective expressions withpositive prior polarity.W?
: Contains weak subjective expressions withnegative prior polarity.W0 : Contains weak subjective expressions withneutral prior polarity.Adding to these, three more lexicons were created,one for each class (positive, negative, neutral).
Inparticular, we employed Chi Squared feature selec-tion (Liu and Setiono, 1995) to obtain the 100 mostimportant tokens per class from the training set.Very few tokens were manually erased to result tothe following three lexicons.T+ : Contains the top-94 tokens appearing in posi-tive tweets of the training set.T?
: Contains the top-96 tokens appearing in nega-tive tweets of the training set.T0 : Contains the top-94 tokens appearing in neutraltweets of the training set.The nine lexicons described above are used to cal-culate precision (P (t, c)), recall (R(t, c)) and F ?measure (F1(t, c)) of tokens appearing in a mes-sage with respect to each class.
Equations 1, 2 and 3below provide the definitions of these metrics.P (t, c) =#tweets that contain token t and belong to class c#tweets that contain token t(1)R(t, c) =#tweets that contain token t and belong to class c#tweets that belong to class c(2)F1(t, c) =2 ?
P (t, c) ?
R(t, c)P (t, c) + R(t, c)(3)3.2 Feature engineeringWe employed three types of features, namelyboolean features, POS based features and lexiconbased features.
Our goal is to build a system that isnot explicitly based on the vocabulary of the trainingset, having therefore better generalisation capability.3.2.1 Boolean featuresBag of words (BOW): These features indicate theexistence of specific tokens in a message.
Weused feature selection with Info Gain to obtainthe 600 most informative tokens of the trainingset and we then manually removed 19 of them564to result in 581 tokens.
As a consequence weget 581 features that can take a value of 1 if amessage contains the corresponding token and0 otherwise.Time and date: We observed that time and date of-ten indicated events in the train data and suchmessages tend to be objective.
Therefore, weadded two more features to indicate if a mes-sage contains time and/or date expressions.Character repetition: Repetitive characters are of-ten added to words by users, in order to giveemphasis or to express themselves more in-tensely.
As a consequence they indicate sub-jectivity.
So we added one more feature havinga value of 1 if a message contains words withrepeating characters and 0 otherwise.Negation: Negation not only is a good subjectivityindicator but it also may change the polarity ofa message.
We therefore add 5 more features,one indicating the existence of negation, andthe remaining four indicating the existence ofnegation that precedes (in a distance of at most5 tokens) words from lexicons S+, S?, W+ andW?.Hash-tags with sentiment: These features are im-plemented by getting all the possible sub-strings of the string after the symbol # andchecking if any of them match with any wordfrom S+, S?, W+ and W?
(4 features).
Avalue of 1 means that a hash-tag containing aword from the corresponding lexicon exists ina message.3.2.2 POS based featuresSpecific POS tags might be good indicators ofsubjectivity or objectivity.
For instance adjectivesoften express sentiment (e.g., beautiful, frustrating)while proper nouns are often reported in objectivemessaged.
We, therefore, added 10 more featuresbased on the following POS tags:1. adjectives,2.
adverbs,3.
verbs,4.
nouns,5.
proper nouns,6.
urls,7.
interjections,8.
hash-tags,9.
happy emoticons, and10.
sad emoticons.We then constructed our features as follows.
Foreach message we counted the occurrences of tokenswith these POS tags and we divided this numberwith the number of tokens having any of these POStags.
For instance if a message contains 2 adjectives,1 adverb and 1 url then the features corresponding toadjectives, adverbs and urls will have a value of 24 ,14and 14 respectively while all the remaining featureswill be 0.
These features can be thought of as a wayto express how much specific POS tags affect thesentiment of a message.Going a step further we calculate precision(P (b, c)), recall (R(b, c)) and F ?
measure(F1(b, c)) of POS tags bigrams with respect to eachclass (equations 4, 5 and 6 respectively).P (b, c) =#tweets that contain bigram b and belong to class c#tweets that contain bigram b(4)R(b, c) =#tweets that contain bigram b and belong to class c#tweets that belong to class c(5)F1(b, c) =2 ?
P (b, c) ?
R(b, c)P (b, c) + R(b, c)(6)For each bigram (e.g., adjective-noun) in a mes-sage we calculate F1(b, c) and then we use the aver-age, the maximum and the minimum of these valuesto create 9 additional features.
We did not experi-ment over measures that weight differently Precisionand Recall (e.g., Fb for b = 0.5) or with differentcombinations (e.g., F1 and P ).3.2.3 Lexicon based featuresThis set of features associates the words of thelexicons described earlier with the three classes.Given a message M , similarly to the equations 4 and5656 above, we calculate P (t, c) and F1(t, c) for everytoken t ?
M with respect to a lexicon.
We then ob-tain the maximum, minimum and average values ofP (t, c) and F1(t, c) in M .
We note that the combi-nation of P and F1 appeared to be the best in ourexperiments while R(t, c) was not helpful and thuswas not used.
Also, similarly to section 3.2.2 wedid not experiment over measures that weight differ-ently Precision and Recall (e.g., Fb for b = 0.5).
Theformer metrics are calculated with three variations:(a) Using words: The values of the metrics con-sider only the words of the message.
(b) Using words and priors: The same as (a) butadding to the calculated metrics a prior value.This value is calculated on the entire lexicon,and roughly speaking it is an indicator of howmuch we can trust L to predict class c. In casesthat a token t of a message M does not appearin a lexicon L the corresponding scores of themetrics will be 0.
(c) Using words and their POS tags: The valuesof the metrics consider the words of the messagealong with their POS tags.
(d) Using words, their POS tags and priors: Thesame as (c) but adding to the calculated metricsan apriori value.
The apriori value is calculatedin a similar manner as in (b) with the differencethat we consider the POS tags of the words aswell.For case (a) we calculated minimum, maximumand average values of P (t, c) and F1(t, c) with re-spect to S+, S?, S0, W+, W?
and W0 consider-ing only the words of the message resulting to 108features.
Concerning case (b) we calculated averageP (t, c) and F1(t, c) with respect to S+, S?, S0, W+,W?
and W0, and average P (t, c) with respect to T+,T?
and T0 adding 45 more features.
For case (c) wecalculated minimum, maximum and average P (t, c)with respect to S+, S?, S0, W+, W?
and W0 (54features), and, finally, for case (d) we calculated av-erage P (t, c) and F1(t, c) with respect to S+, S?,S0, W+, W?
and W0 to add 36 features.Class F1Positive 0.6496Negative 0.4429Neutral 0.7022Average 0.5462Table 1: F1 for development set.4 ExperimentsAs stated earlier we use a 2?stage pipeline approachto identify the sentiment of a message.
Preliminaryexperiments on the development data showed thatthis approach is better than attempting to address theproblem in one stage during which a classifier mustclassify a message as positive, negative or neutral.To be more precise we used a Naive Bayes classifierand BOW features using both 1?stage and 2?stageapproaches.
Although we considered the 2?stageapproach with a Naive Bayes classifier as a baselinesystem we used it to submit results for both twitterand sms test sets.Having concluded to the 2?stage approach weemployed for each stage an SVM classifier, fed withthe 855 features described in section 3.2.3 BothSVMs use linear kernel and are tuned in order tofind the optimum C parameter.
Observe that we usethe same set of features in both stages and let theclassifier learn the appropriate weights for each fea-ture.
During the first stage, the classifier is trainedon the entire training set after merging positive andnegative classes to one superclass, namely subjec-tive.
In the second stage, the classifier is trained onlyon positive and negative tweets of the training andis asked to determine whether the messages classi-fied as subjective during the first stage are positiveor negative.4.1 ResultsIn order to obtain the best set of features we trainedour system on the downloaded training data andmeasured its performance on the provided develop-ment data.
Table 1 illustrates the F1 results on thedevelopment set.
A first observation is that thereis a considerable difference between the F1 of thenegative class and the other two, with the former be-3We used the LIBLINEAR distribution (Fan et al 2008)566Class F1Positive 0.6854Negative 0.4929Neutral 0.7117Average 0.5891Table 2: F1 for twitter test set.Class F1Positive 0.6349Negative 0.5131Neutral 0.7785Average 0.5740Table 3: F1 for sms test set.ing significantly decreased.
This might be due tothe quite low number of negative tweets of the ini-tial training set in comparison with the rest of theclasses.
Therefore, the addition of 340 negative ex-amples from the development set emerged from thisimbalance and proved to be effective as shown in ta-ble 2 illustrating our results on the test set regardingtweets.
Unfortunately we were not able to submitresults with this system for the sms test set.
How-ever, we performed post-experiments after the goldsms test set was released.
The results shown on table3 are similar to the ones obtained for the twitter testset which means that our model has a good general-isation ability.5 Conclusion and future workIn this paper we presented our approach for theMessage Polarity Classification task of SEMEVAL2013.
We proposed a pipeline approach to detectsentiment in two stages; first we discard objectivemessages and then we classify subjective (i.e., car-rying sentiment) ones as positive or negative.
Weused SVMs with various extracted features for bothstages and although the system performed reason-ably well, there is still much room for improvement.A first problem that should be addressed is the dif-ficulty in identifying negative messages.
This wasmainly due to small number of tweets in the train-ing data.
This was somewhat alleviated by addingthe negative instances of the development data butstill our system reports lower results for this class ascompared to positive and neutral classes.
More dataor better features is a possible improvement.
An-other issue that has not an obvious answer is how toproceed in order to improve the 2?stage pipeline ap-proach.
Should we try and optimise each stage sepa-rately or should we optimise the second stage takinginto consideration the results of the first stage?ReferencesLuciano Barbosa and Junlan Feng.
2010.
Robust senti-ment detection on twitter from biased and noisy data.In Proceedings of the 23rd International Conferenceon Computational Linguistics: Posters, COLING ?10,pages 36?44, Beijing, China.
Association for Compu-tational Linguistics.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-RuiWang, and Chih-Jen Lin.
2008.
Liblinear: A libraryfor large linear classification.
The Journal of MachineLearning Research, 9:1871?1874.Huan Liu and Rudy Setiono.
1995.
Chi2: Feature se-lection and discretization of numeric attributes.
InTools with Artificial Intelligence, 1995.
Proceedings.,Seventh International Conference on, pages 388?391.IEEE.Bo Pang and Lillian Lee.
2004.
A sentimental edu-cation: sentiment analysis using subjectivity summa-rization based on minimum cuts.
In Proceedings ofthe 42nd Annual Meeting on Association for Compu-tational Linguistics, ACL ?04, Barcelona, Spain.
As-sociation for Computational Linguistics.Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.2011.
Named entity recognition in tweets: An experi-mental study.
In EMNLP, pages 1524?1534.V.
Vapnik.
1998.
Statistical learning theory.
John Wiley.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005a.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of the confer-ence on Human Language Technology and EmpiricalMethods in Natural Language Processing, HLT ?05,pages 347?354, Vancouver, British Columbia, Canada.Association for Computational Linguistics.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005b.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of the con-ference on Human Language Technology and Empir-ical Methods in Natural Language Processing, pages347?354.
Association for Computational Linguistics.Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, SaraRosenthal, Veselin Stoyanov, and Alan Ritter.
2013.SemEval-2013 task 2: Sentiment analysis in twitter.In Proceedings of the International Workshop on Se-mantic Evaluation, SemEval ?13, June.567
