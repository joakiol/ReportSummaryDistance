Evaluating information content by factoid analysis: humanannotation and stabilitySimone TeufelComputer LaboratoryCambridge University, UKHans van HalterenLanguage and SpeechUniversity of Nijmegen, The NetherlandsAbstractWe present a new approach to intrinsic sum-mary evaluation, based on initial experimentsin van Halteren and Teufel (2003), which com-bines two novel aspects: comparison of infor-mation content (rather than string similarity)in gold standard and system summary, mea-sured in shared atomic information units whichwe call factoids, and comparison to more thanone gold standard summary (in our data: 20and 50 summaries respectively).
In this paper,we show that factoid annotation is highly re-producible, introduce a weighted factoid score,estimate how many summaries are required forstable system rankings, and show that the fac-toid scores cannot be sufficiently approximatedby unigrams and the DUC information overlapmeasure.1 IntroductionMany researchers in summarisation believe thatthe best way to evaluate a summary is extrin-sic evaluation (Spa?rck Jones, 1999): to measurethe quality of the summary on the basis of de-gree of success in executing a specific task withthat summary.
The summary evaluation per-formed in SUMMAC (Mani et al, 1999) fol-lowed that strategy.
However, extrinsic eval-uations are time-consuming to set up and canthus not be used for the day-to-day evaluationneeded during system development.
So in prac-tice, a method for intrinsic evaluation is needed,where the properties of the summary itself areexamined, independently of its application.Intrinsic evaluation of summary quality is un-deniably hard, as there are two subtasks of sum-marisation which need to be evaluated, infor-mation selection and text production ?
in factthese two subtasks are often separated in evalu-ation (Mani, 2001).
If we restrict our attentionto information selection, systems are tested byway of comparison against a ?gold standard?, amanually produced result which is supposed tobe the ?correct?, ?true?
or ?best?
result.In summarisation there appears to be no ?onetruth?, but rather various ?good?
results.
Hu-man subjectivity in what counts as the most im-portant information is high.
This is evidencedby low agreement on sentence selection tasks(Rath et al, 1961; Jing et al, 1998), and lowword overlap measures in the task of creatingsummaries by reformulation in the summaris-ers?
own words (e.g.
word overlap of the 542single document summary pairs in DUC-02 av-eraged only 47%).But even though the non-existence of any onegold standard is generally acknowledged in thesummarisation community, actual practice nev-ertheless ignores this, mostly due to the expenseof compiling summary gold standards and thelack of composite measures for comparison tomore than one gold standard.Other fields such as information retrieval (IR)also have to deal with human variability con-cerning the question of what ?relevant to aquery?
means.
This problem is circumventedby extensive sampling: many different queriesare collected to level out the differences inquery formulation and relevance judgements.Voorhees (2000) shows that the relative rank-ings of IR systems are stable across annota-tors even though relevance judgements differsignificantly between humans.
Similarly, in MT,the recent BLEU metric (Papineni et al, 2001)also uses the idea that one gold standard isnot enough.
Their ngram-based metric derivedfrom four reference translations of 40 generalnews stories shows high correlation with humanjudgement.Lin and Hovy (2002) examine the use ofngram-based multiple gold standards for sum-marisation evaluation, and conclude ?we needmore than one model summary although wecannot estimate how many model summariesare required to achieve reliable automated sum-mary evaluation?.
In this paper, we explore thedifferences and similarities between various hu-man summaries in order to create a basis forsuch an estimate and examine the degree of dif-ference between the use of a single summarygold standard and the use of a consensus goldstandard for two sample texts, based on 20 and50 summaries respectively.The second aspect we examine is the similar-ity measure which compares system and goldstandard summaries.
In principle, the com-parison can be done via co-selection of ex-tracted sentences (Rath et al, 1961; Jing et al,1998), by string-based surface measures (Linand Hovy, 2002), or by subjective judgementsof the amount of information overlap (DUC,2002).
String-based metrics are superior to sen-tence co-selection, as co-selection cannot takesimilar or even identical information into ac-count if it does not occur in the sentences whichwere chosen.
The choice of information overlapjudgements as the main metric in DUC reflectsthe intuition that human judgements of shared?meaning?
of two texts should in principle besuperior to surface-based similarity.DUC assessors judge the informational over-lap between ?model units?
(elementary dis-course units (EDUs), i.e.
clause-like units,taken from the gold standard summary) and?peer units?
(sentences taken from the partici-pating summaries) on the basis of the question:?How much of the information in a model unit iscontained in a peer unit: 100%, 80%, 60%, 40%,20%, 0%??
Weighted recall measures reporthow much gold standard information is presentin the summaries.However, information overlap judgement isnot something humans seem to be good at, ei-ther.
Lin and Hovy (2002) show the instabil-ity of the evaluation, expressed in system rank-ings.
They also examined those cases whereannotators incidentially had to judge a givenmodel?peer pair more than once (because differ-ent systems returned the same ?peer?
sentence).In those cases, assessors agreed with their ownprior judgement in only 82% of the cases.We propose a novel gold standard comparisonbased on factoids.
Identifying factoids in text isa more objective task than judging informationoverlap a` la DUC.
Our annotation experimentsshow high human agreement on the factoid an-notation task.
We believe this is due to the wayhow factoids are defined, and due to our pre-cise guidelines.
The factoid measure also allowsquantification of the specific elements of infor-mation overlap, rather than just giving a quan-titative judgement expressed in percentages.In an example from Lin and Hovy (2002), aDUC assessor judged some content overlap be-tween ?Thousands of people are feared dead?and ?3,000 and perhaps ... 5,000 people havebeen killed.?
In our factoid representation, adistinction between ?killed?
and ?feared dead?would be made, and different numbers of peo-ple mentioned would have been differentiated.Thus, the factoid approach can capture muchfiner shades of meaning differentiation thanDUC-style information overlap does.
Futher-more, it can provide feedback to system builderson the exact information their systems fail toinclude or include superfluously.We describe factoid analysis in section 2, amethod for comparison of the information con-tent of different summaries of the same texts,and describe our method for measuring agree-ment and present results in section 3.
We theninvestigate the distribution of factoids acrossthe summaries in our data sets in section 4,and define a weighted factoid score in section5.
In that section, we also perform stability ex-periments, to test whether rankings of systemsummaries remain stable if fewer than all sum-maries which we have available are used, andcompare weighted factoid scores to other sum-mary evaluation metrics.2 Data and factoid annotationWe use two texts: a 600-word BBC report onthe killing of the Dutch politician Pim Fortuyn(as used in van Halteren and Teufel (2003)),which contains a mix of factual information andpersonal reactions, and a 573-word article onthe Iraqi invasion of Kuwait (used in DUC-2002,LA080290-0233).For these two texts, we collected human writ-ten generic summaries of roughly 100 words.Our guidelines asked the human subjects to for-mulate the summary in their own words, in or-der to elicit different linguistic expressions forthe same information.
Knowledge about thevariability of expression is important both forevaluation and system building.The Fortuyn text was summarised by 40Dutch students1, and 10 NLP researchers (na-tive or near-native English speakers), resultingin a total of 50 summaries.
For the Kuwait text,1Another 20 summaries of the same source were re-moved due to insufficient English or excessive length.we used the 6 DUC-provided summaries, 17ELSNET-02 student participants (7 summariesremoved), and summaries by 4 additional re-searchers, resulting in a total of 20 summaries.We use atomic semantic units called factoidsto represent the meaning of a sentence.
Forinstance, we represent the sentence ?The policehave arrested a white Dutch man?
by the union ofthe following factoids:FP20 A suspect was arrestedFP21 The police did the arrestingFP24 The suspect is whiteFP25 The suspect is DutchFP26 The suspect is maleFactoids are defined empirically based on thedata in the set of summaries we work with.
Fac-toid definition starts with the comparison of theinformation contained in two summaries, andgets refined (factoids get added or split) as in-crementally other summaries are considered.
Iftwo pieces of information occur together in allsummaries ?
and within the same sentence ?they are treated as one factoid, because dif-ferentiation into more than one factoid wouldnot help us in distinguishing the summaries.
Inour data, there must have been at least onesummary that contained either only FP25 oronly FP26 ?
otherwise those factoids would havebeen combined into a single factoid ?FP27 Thesuspect is a Dutch man?.
Factoids are labelledwith descriptions in natural language; initially,these are close in wording to the factoid?s oc-currence in the first summaries, though the an-notator tries to identify and treat equally para-phrases of the factoid information when theyoccur in other summaries.Our definition of atomicity implies that the?amount?
of information associated with onefactoid can vary from a single word to an en-tire sentence.
An example for a large chunkof information that occurred atomically in ourtexts was the fact that Fortuyn wanted to be-come PM (FV71), a factoid which covers an en-tire sentence.
On the other hand, a single wordmay break down into more than one factoids.If (together with various statements in othersummaries) one summary contains ?was killed?and another ?was shot dead?, we identify thefactoidsFA10 There was an attackFA40 The victim diedFA20 A gun was usedThe first summary contains only the first twofactoids, whereas the second contains all three.That way, the semantic similarity between re-lated sentences can be expressed.When we identified factoids in our summarycollections, most factoids turned out to be in-dependent of each other.
But when dealingwith naturally occuring documents many dif-ficult cases appear, e.g.
ambiguous expressions,slight differences in numbers and meaning, andinference.Another difficult phenomenon is attribution.In both source texts, quotations of the reactionsof several politicians and officials are given, andthe subjects often generalised these reactionsand produced statements such as ?Dutch as wellas international politicians have expressed their griefand disbelief.?
Due to coordination of speak-ers (in the subject) and coordination of reac-tions (in the direct object), it is hard to ac-curately represent the attribution of opinions.We therefore introduce combinatorical factoids,such as ?
OG40 Politicians expressed grief?
and?OS62 International persons/organizations ex-pressed disbelief?
which can be combined withsimilar factoids to express the above sentence.We wrote guidelines (10 pages long) whichdescribe how to derive factoids from texts.
Theguidelines cover questions such as: how to cre-ate generalising factoids when numerical val-ues vary (summaries might talk about ?200?,?about 200?
or ?almost 200 Kuwaitis werekilled?
), how to create factoids dealing with at-tribution of opinion, and how to deal with coor-dination of NPs in subject position, cataphorsand other syntactic constructions.
We believethat written guidelines should contain all therules by which this process is done; this is theonly way that other annotators, who do nothave access to all the discussions the original an-notators had, can replicate the annotation witha high agreement.
We therefore consider theguidelines as one of the most valuable outcomesof this exercise, and we will make them and ourannotated material generally available.The advantage of our empirical, summary-set-dependent definition of factoid atomicity isthat the annotation is more objective than iffactoids had to be invented by intuition of se-mantic constructions from scratch.
One possi-ble disadvantage of our definition of atomicityis that the set of factoids used may have to beadjusted if new summaries are judged, as a re-quired factoid might be missing, or an existingone might require splitting.
Using a large num-ber of gold-standard summaries for the defini-tion of factoids decreases the likelihood of thishappening.3 AgreementIn our previous work, a ?definitive?
list of fac-toids was given (created by one author), andwe were interested in whether annotators couldconsistently mark the text with the factoids con-tained in this list.
In the new annotation cyclereported on here, we study the process of factoidlists creation, which is more time-consuming.We will discuss agreement in factoid annotationfirst, as it is a more straightforward concept,even though procedurally, factoids are first de-fined (cf.
section 3.2) and then annotated (cf.section 3.1).3.1 Agreement of factoid annotationAssuming that we already have the right list offactoids available, factoid annotation of a 100word summary takes roughly 10 minutes, andmeasuring agreement on the decision of assign-ing factoids to sentences is relatively straight-forward.
We calculate agreement in terms ofKappa, where the set of items to be classified areall factoid?summary combinations (e.g.
in thecase of Phase 1, N=153 factoids times 20 sen-tences = 2920), and where there are two cate-gories, either ?factoid is present in summary (1)?or ?factoid is not present in summary (0)?.
P(E),probability of error, is calculated on the basisof the distribution of the categories, whereasP(A), probability of agreement, is calculatedas the average of observed to possible pairwiseagreements per item.
Kappa is calculated ask = P (A)?P (E)1?P (E) ; results for our two texts aregiven in Figure 1.We measure agreement at two stages inthe process: entirely independent annotation(Phase 1), and corrected annotation (Phase 2).In Phase 2, annotators see an automaticallygenerated list of discrepancies with the otherannotator, so that slips of attention can be cor-rected.
Crucially, Phase 2 was conducted with-out any discussion.
After Phase 2 measurement,discussion on the open points took place and aconsensus was reached (which is used for theexperiments in the rest of the paper).Figure 1 includes results for the Fortuyn textas we have factoid?summary annotations byboth annotators for both texts.
The Kappa fig-ures indicate high agreement, even in Phase 1(K=.87 and K=.86); in Phase 2, Kappas are ashigh as .89 and .95.
Note that there is a dif-ference between the annotation of the Fortuynand the Kuwait text: in the Fortuyn case, therewas no discussion or disclosure of any kind inPhase 1; one author created the factoids, andboth used this list to annotate.
The agreementof K=.86 was thus measured on entirely inde-pendent annotations, with no prior communica-tion whatsoever.
In the case of the Kuwait text,the prior step of finding a consensus factoid listhad already taken place, including some discus-sion.Fortuyn textK N k n P(A) P(E)Phase 1 .86 14178 2 2 .970 .787Phase 2 .95 14178 2 2 .989 .779Kuwait textK N k n P(A) P(E)Phase 1 .87 3060 2 2 .956 .670Phase 2 .89 2940 2 2 .962 .663Figure 1: Agreement of factoid annotation.3.2 Agreement of factoid definition.We realised during our previous work, whereonly one author created the factoids, that thetask of defining factoids is a complicated pro-cess and that we should measure agreement onthis task too (using the Kuwait text).
Thus,we do not have this information for the Fortuyntext.But how should the measurement of agree-ment on factoid creation proceed?
It is diffi-cult to find a fair measure of agreement over setoperations like factoid splitting, particularly asthe sets can contain a different set of summariesmarked for each factoid.
For instance, considerthe following two sentences: (1) M01-004 Sad-dam Hussein said ... that they will leave thecountry when the situation stabilizes.
and (2)M06-004 Iraq claims it ... would withdraw soon.One annotator created a factoid ?
(P30) Sad-dam H/Iraq will leave the country soon/whensituation stabilises?
whereas the other anno-tator split this into two factoids (F9.21 andF9.22).
Note that the annotators use their own,independently chosen factoid names.Our procedure for annotation measurement isas follows.
We create a list of identity and sub-sumption relations between factoids by the twoannotators.
In the example above, P30 wouldbe listed as subsuming F9.21 and F9.22.
It istime-consuming but necessary to create sucha list, as we want to measure agreement onlyamongst those factoids which are semanticallyrelated.
We use a program which maximisesshared factoids between two summary sentencesA1 A2 A1 A2P30 ?
F9.21 ?
a 1 1 P30 ?
F9.22 ?
a 1 0P30 ?
F9.21 ?
b 0 0 P30 ?
F9.22 ?
b 0 0P30 ?
F9.21 ?
c 1 0 P30 ?
F9.22 ?
c 1 1P30 ?
F9.21 ?
d 0 0 P30 ?
F9.22 ?
d 0 0P30 ?
F9.21 ?
e 1 0 P30 ?
F9.22 ?
e 1 1Figure 2: Items for kappa calculation.to suggest such identities and subsumption re-lations.We then calculate Kappa at Phases 1 and 2.It is not trivial to define what an ?item?
inthe Kappa calculation should be.
Possiblythe use of Krippendorff?s alpha will providea better approach (cf.
Nenkova and Passon-neau (2004)), but for now we measure usingthe better-known kappa, in the following way:For each equivalence between factoids A andC, create items { A ?
C ?
s | s ?
S } (whereS is the set of all summaries).
For each fac-toid A subsumed by a set B of factoids, createitems { A ?
b ?
s | b ?
B, s ?
S}.
For exam-ple, given 5 summaries a, b, c, d, e, AnnotatorA1 assigns P30 to summaries a, c and e. An-notator A2 (who has split P30 into F9.21 andF9.22), assigns a to F9.21 and c and e to F9.22.This creates the 10 items for Kappa calculationgiven in Figure 2.Results for our data set are given in Figure 3.For Phase 1 of factoid definition, K=.7 indicatesrelatively good agreement (but lower than forthe task of factoid annotation).
Many of thedisagreements can be reduced to slips of atten-tion, as the increased Kappa of .81 for Phase 2shows.Overall, we can observe that this high agree-ment for both tasks points to the fact that thetask can be robustly performed in naturally oc-curring text, without any copy-editing.
Still,from our observations, it seems that the taskof factoid annotation is easier than the task offactoid definition.Kuwait textK N k n P(A) P(E)Phase 1 .70 3560 2 2 .91 .69Phase 2 .81 3240 2 2 .94 .67Figure 3: Agreement of factoid definition.One of us then used the Kuwait consensusagreement to annotate the 16 machine sum-maries for that text which were created by dif-ferent participants in DUC-2002, an annotationwhich could be done rather quickly.
However, asmall number of missing factoids were detected,for instance the (incorrect) factoid that SaudiArabia was invaded, that the invasion happenedon a Monday night, and that Kuwait City isKuwait?s only sizable town.
Overall, the set offactoids available was considered adequate forthe annotation of these new texts.0 10 20 30 40 50050100150200250Number of summariesAveragenumber of factoidsFigure 4: Average size of factoid inventory as afunction of number of underlying summaries.4 Growth of the factoid inventoryThe more summaries we include in the analy-sis, the more factoids we identify.
This growthof the factoid set stems from two factors.
Dif-ferent summarisers select different informationand hence completely new factoids are intro-duced to account for information not yet seenin previous summaries.
This factor also impliesthat the factoid inventory can never be completeas summarisers sometimes include informationwhich is not actually in the original text.
Thesecond factor comes from splitting: when a newsummary is examined, it often becomes neces-sary to split a single factoid into multiple fac-toids because only a certain part of it is includedin the new summary.
After the very first sum-mary, each factoid is a full sentence, and theseare gradually subdivided.In order to determine how many factoids existin a given set of N summaries, we simulate ear-lier stages of the factoid set by automatically re?merging those factoids which never occur apartwithin the given set of summaries.Figure 4 shows the average number of factoidsover 100 drawings of N different summaries fromthe whole set, which grows from 1.0 to about 4.5for the Kuwait text (long curve) and about 4.1for the Fortuyn text (short curve).
The Kuwaitcurve shows a steeper incline, possibly due tothe fact that the sentences in the Kuwait textare longer.
Given the overall growth for thetotal number of factoids and the number of fac-toids per sentence, it would seem that the split-ting factors and the new information factor areequally productive.Neither curve in Figure 4 shows signs that itmight be approaching an assymptote.
This con-firms our earlier conclusion (van Halteren andTeufel, 2003) that many more summaries than10 or 20 are needed for a full factoid inventory.25 Weighted factoid scores andstabilityThe main reason to do factoid analysis is tomeasure the quality of summaries, includingmachine summaries.
In our previous work, wedo this with a consensus summary.
We are nowinvestigating different weighting factors for theimportance of factoids.
Previously, the weight-ing factors we suggested were information con-tent, position in the summaries and frequency.We investigated the latter two.Each factoid we find in a summary to be eval-uated contributes to the score of the summary,by an amount which reflects the perceived valueof the factoid, what we will call the ?weightedfactoid score (WFS)?.
The main component inthis value is frequency, i.e., the number of modelsummaries in which the factoid is observed.When frequency weighting is used by itself,each factoid occurrence is worth one.3 We couldalso assume that more important factoids areplaced earlier in a summary, and that the fre-quency weight is adjusted on the basis of po-sition.
Experimentation is not complete, butthe adjustments appear to influence the rank-ing only slightly.
The results we present hereare those using pure frequency weights.We noted in our earlier paper that a goodquality measure should demonstrate at least thefollowing properties: a) it should reward inclu-sion in a summary of the information deemed2It should be noted that the estimation in Figure 4improves upon the original estimation in that paper, asthe determination of number of factoids for that figuredid not consider the splitting factor, but just countedthe number of factoids as taken from the inventory at itshighest granularity.3This is similar to the relative utility measure intro-duced by Radev and Tam (2003), which however oper-ates on sentences rather than factoids.
It also corre-sponds to the pyramid measure proposed by Nenkovaand Passonneau (2004), which also considers an estima-tion of the maximum value reachable.
Here, we use nosuch maximum estimation as our comparisons will all berelative.1 3 5 7 9 12 15 18 21 24 27 30 33 36 39 42 45 48?0.20.00.20.40.60.81.0Number of summaries (N) that score is based onRankingcorrelationbetween twosamplings(allowing repeats)ofNsummariesFigure 5: Correlation (Spearman?s ?)
betweensummary rankings on the basis of two differentsets of N summaries, for N between 1 and 50.most important in the document and b) mea-sures based on two factoid analyses constructedalong the same lines should lead to the same,or at least very similar, ranking of a set of sum-maries which are evaluated.
Since our measurerewards inclusion of factoids which are men-tioned often and early, demand a) ought to besatisfied by construction.For demand b), some experimentation is inorder.
For various numbers of summaries N,we take two samples of N summaries from thewhole set (allowing repeats so that we can use Nlarger than the number of available summaries;a statistical method called ?bootstrap?).
Foreach sample in a pair, we use the weighted fac-toid score with regard to that sample of N sum-maries to rank the summaries, and then deter-mine the ranking correlation (Spearman?s ?)
be-tween the two rankings.
The summaries that werank here are the 20 human summaries of theKuwait text, plus 16 machine summaries sub-mitted for DUC-2002.Figure 5 shows how the ranking correlationincreases with N for the Kuwait text.
Its meanvalue surpasses 0.8 at N=11 and 0.9 at N=19.At N=50, it is 0.98.
What this means for thescores of individual summaries is shown in Fig-ure 6, which contains a box plot for the scoresfor each summary as observed in the 200 draw-ings for N=50.
The high ranking correlationand the reasonable stability of the scores showsthat our measure fulfills demand b), at least ata high enough N. What could be worrying isthe fact that the machine summaries (right ofthe dotted line) do not seem to be performingsignificantly worse than the human ones (leftSubmitted summaries (Human | Machine)Scorebased on 50 summaries,with frequency weighting0.00.10.20.30.40.50.6Figure 6: Variation in summary scores in eval-uations based on 200 different sets of 50 modelsummaries.Submitted summaries (Human | Machine)Scorebased on 10 summaries,with frequency weighting0.00.10.20.30.40.50.6Figure 7: Variation in summary scores in eval-uations based on 200 different sets of 10 modelsummaries.of the line).
However, an examination of thebetter scoring machine summaries show that inthis particular case, their information contentis indeed good.
The very low human scores ap-pear to be cases of especially short summaries(including one DUC summariser) and/or sum-maries with a deviating angle on the story.It has been suggested in DUC circles that alower N should suffice.
That even a value ashigh as 10 is insufficient is already indicated bythe ranking correlation of only 0.76.
It becomeseven clearer with Figure 7, which mirrors Figure6 but uses N=10.
The scores for the summariesvary wildly, which means that ranking is almostrandom.Of course, the suggestion might be made thatthe system ranking will most likely also be sta-bilised by scoring summaries for more texts,even with such a low (or even lower) N per text.However, in that case, the measure only yieldsinformation at the macro level: it merely givesan ordering between systems.
A factoid-basedmeasure with a high N also yields feedback on amicro level: it can show system builders whichvital information they are missing and whichsuperfluous information they are including.
Weexpect this feedback only to be reliable at thesame order of N at which single-text-based scor-ing starts to stabilise, i.e.
around 20 to 30.As the average ranking correlation betweentwo weighted factoid score rankings based on20 summaries is 0.91, we could assume thatthe ranking based on our full set of 20 differ-ent summaries should be an accurate ranking.If we compare it to the DUC information over-lap rankings for this text, we find that the indi-vidual rankings for D086, D108 and D110 havecorrelations with our ranking of 0.50, 0.64 and0.79.
When we average over the three, this goesup to 0.83.In van Halteren and Teufel (2003), we com-pared a consensus summary based on the top-scoring factoids with unigram scores.
For the 50Fortuyn summaries, we calculate the F-measurefor the included factoids with regard to the con-sensus summary.
In a similar fashion, we builda consensus unigram list, containing the 103unigrams that occur in at least 11 summaries,and calculate the F-measure for unigrams.
Thecorrelation between those two scores was low(Spearman?s ?
= 0.45).
We concluded fromthis experiment that unigrams, though muchcheaper, are not a viable substitute for factoids.6 Discussion and future workWe have presented a new information-basedsummarization metric called weighted factoidscore, which uses multiple summaries as goldstandard and which measures information over-lap, not string overlap.
It can be reliably andobjectively annotated in arbitrary text, which isshown by our high values for human agreement.We summarise our results as follows: Factoidscan be defined with high agreement by indepen-dently operating annotators in naturally occur-ring text (K=.70) and independently annotatedwith even higher agreement (K=.86 and .87).Therefore, we consider the definition of factoidsintuitive and reproducible.The number of factoids found if new sum-maries are considered does not tail off, butweighting of factoids by frequency and/or lo-cation in the summary allows for a stable sum-mary metric.
We expect this can be improvedfurther by including an information contentweighting factor.If single summaries are used as gold standard(as many other summarization evaluations do),the correlation between rankings based on twosuch gold standard summaries can be expectedto be low; in our two experiments, the correla-tions were ?=0.20 and 0.48 on average.
Accord-ing to our estimations, stability with respectto the factoid scores can only be expected ifa larger number of summaries are collected (inthe range of 20?30 summaries).System rankings based on the factoid scoreshows only low correlation with rankings basedon a) DUC-based information overlap, andb) unigrams, a measurement based on sharedwords between gold standard summaries andsystem summary.
As far as b) is concerned,this is expected, as factoid comparison abstractsover wording and captures linguistic variationof the same meaning.
However, the ROUGEmeasure currently in development is consideringvarious n-grams and Wordnet-based paraphras-ing options (Lin, personal communication).
Weexpect that this measure has the potential forbetter ranking correlation with factoid ranking,and we are currently investigating this.We also plan to expand our data sets to moretexts, in order to investigate the presence anddistribution of factoids, types of factoids and re-lations between factoids in summaries and sum-mary collections.
Currently, we have two largefactoid-annotated data sets with 20 and 50 sum-maries, and a workable procedure to annotatefactoids, including guidelines which were usedto achieve good agreement.
We now plan toelicit the help of new annotators to increase ourdata pool.Another pressing line of investigation is re-ducing the cost of factoid analysis.
The first rea-son why this analysis is currently expensive isthe need for large summary bases for consensussummaries.
Possibly this can be circumventedby using larger numbers of different texts, as isthe case in IR and in MT, where discrepanciesprove to average out when large enough datasetsare used.
The second reason is the need forhuman annotation of factoids.
Although sim-ple word-based methods prove insufficient, weexpect that existing and emerging NLP tech-niques based on deeper processing might helpwith automatic factoid identification.All in all, the use of factoid analysis andweighted factoid score, even though initially ex-pensive to set up, provides a promising alterna-tive which could well bring us closer to a solu-tion to several problems in summarisation eval-uation.ReferencesDUC.
2002.
Document Understanding Con-ference (DUC).
Electronic proceedings,http://www-nlpir.nist.gov/projects/duc/pubs.html.Jing, H., R. Barzilay, K. R. McKeown, and M. El-hadad.
1998.
Summarization Evaluation Meth-ods: Experiments and Analysis.
In Working Notesof the AAAI Spring Symposium on IntelligentText Summarization, 60?68.Lin, C., and E. Hovy.
2002.
Manual and automaticevaluation of summaries.
In DUC 2002.Mani, I.
2001.
Automatic Summarization.
John Ben-jamins.Mani, I., T. Firmin, D. House, G. Klein, B. Sund-heim, and L. Hirschman.
1999.
The TIPSTERSummac Text Summarization Evaluation.
In Pro-ceedings of EACL-99 , 77?85.Nenkova, A., and R. Passonneau.
2004.
EvaluatingContent Selection in Summarization: the Pyra-mid Method.
In Proceedings of NAACL/HLT-2003 .Papineni, K, S. Roukos, T Ward, and W-J.
Zhu.2001.
Bleu: a method for automatic evaluation ofmachine translation.
In Proceedings of ACL-02 ,311?318.Radev, D., and D. Tam.
2003.
Summarization eval-uation using relative utility.
In Proceedings of theTwelfth International Conference on Informationand Knowledge Management , 508?511.Rath, G.J, A. Resnick, and T. R. Savage.
1961.
TheFormation of Abstracts by the Selection of Sen-tences.
American Documentation 12(2): 139?143.Spa?rck Jones, K. 1999.
Automatic Summarising:Factors and Directions.
In I. Mani and M. May-bury, eds., Advances in Automatic Text Summa-rization, 1?12.
Cambridge, MA: MIT Press.van Halteren, H., and S. Teufel.
2003.
Examiningthe consensus between human summaries: initialexperiments with factoid analysis.
In Proceedingsof the HLT workshop on Automatic Summariza-tion.Voorhees, E. 2000.
Variations in relevance judge-ments and the measurement of retrieval effective-ness.
Information Processing and Management36: 697?716.
