The GREC Challenge: Overview and Evaluation ResultsAnja Belz Eric KowNLT GroupUniversity of BrightonBrighton BN2 4GJ, UK{asb,eykk10}@bton.ac.ukJette ViethenCentre for LTMacquarie UniversitySydney NSW 2109jviethen@ics.mq.edu.auAlbert GattComputing ScienceUniversity of AberdeenAberdeen AB24 3UE, UKa.gatt@abdn.ac.ukAbstractThe GREC Task at REG?08 required partici-pating systems to select coreference chains tothe main subject of short encyclopaedic textscollected from Wikipedia.
Three teams sub-mitted a total of 6 systems, and we addition-ally created four baseline systems.
Systemswere tested automatically using a range of ex-isting intrinsic metrics.
We also evaluatedsystems extrinsically by applying coreferenceresolution tools to the outputs and measuringthe success of the tools.
In addition, systemswere tested in a reading/comprehension exper-iment involving human subjects.
This reportdescribes the GREC Task and the evaluationmethods, gives brief descriptions of the par-ticipating systems, and presents the evaluationresults.1 IntroductionThe GREC task is about how to generate appropri-ate references to an entity in the context of a pieceof discourse longer than a sentence.
Rather thanrequiring participants to generate referring expres-sions from scratch, the GREC data provides sets ofpossible referring expressions for selection.
As thisis a new referring expression generation (REG) task,the shared task definition was kept fairly simpleand the aim for participating systems was to selectthe appropriate type of referring expression (morespecifically, its REG08-TYPE, full details below).The immediate motivating application context forthe GREC Task is the improvement of referentialclarity and coherence in extractive summarisationby regenerating referring expressions in summaries.There has recently been a small flurry of work inthis area (Steinberger et al, 2007; Nenkova, 2008).In the longer term, the GREC Task is intended to be astep in the direction of the more general task of gen-erating referential expressions in discourse context.The GREC Task Corpus is an extension of GREC1.0 which had about 1,000 texts in the subdomainsof cities, countries, rivers and people (Belz andVarges, 2007a).
for the purpose of the REG?08 GRECTask, we obtained an additional 1,000 texts in thenew subdomain of mountain texts and developed anew XML annotation scheme (Section 2.2).Five teams from four countries registered for theGREC Task, of which three teams eventually submit-ted 6 systems.
We also used the corpus texts them-selves as ?system?
outputs, and created four base-line systems.
We evaluated the resulting 10 sys-tems using a range of intrinsic and extrinsic evalu-ation methods.
This report presents the results of allevaluations (Section 6), along with descriptions ofthe GREC data and task (Section 2), test sets (Sec-tion 3), evaluation methods (Section 4), and partici-pating systems (Section 5).2 Data and TaskThe GREC Corpus (version 2.0) consists of about2,000 texts in total, all collected from introductorysections in Wikipedia articles, in five different do-mains (cities, countries, rivers, people and moun-tains).
In each text, three broad categories of MainSubject Reference (MSR)1 have been annotated, re-1The main subject of a Wikipedia article is simply taken tobe given by its title, e.g.
in the cities domain the main subject183sulting in a total of about 13,000 annotated REs.The corpus was randomly divided into 90% train-ing data (of which 10% were randomly selected asdevelopment data) and 10% test data.
Participantsused the training data in developing their systems,and (as a minimum requirement) reported results onthe development data.
Participants had 48 hours tosubmit outputs for the (previously unseen) test data.2.1 Types of referential expression annotatedThree broad categories of main subject referring ex-pression (MSREs) are annotated in the GREC corpus2?
subject NPs, object NPs, and genitive NPs and pro-nouns which function as subject-determiners withintheir matrix NP.
These categories of referring ex-pression (RE) are relatively straightforward to iden-tify and achieve high inter-annotator agreement on(complete agreement among four annotators in 86%of MSRs), and account for most cases of overt mainsubject reference (MSR) in the GREC texts.
The an-notators were asked to identify subject, object andgenitive subject-determiners and decide whether ornot they refer to the main subject of the text.
Moredetail is provided in Belz and Varges (2007b).In addition to the above, relative pronouns in sup-plementary relative clauses (as opposed to integratedrelative clauses, Huddleston and Pullum, 2002, p.1058) were annotated, e.g.
:(1) Stoichkov is a football manager and former striker whowas a member of the Bulgaria national team thatfinished fourth at the 1994 FIFA World Cup.We also annotated ?non-realised?
subject MSREsin a restricted set of cases of VP coordination wherean MSRE is the subject of the coordinated VPs, e.g.
:(2) He stated the first version of the Law of conservation ofmass, introduced the Metric system, and helped toreform chemical nomenclature.The motivation for annotating the approximateplace where the subject NP would be if it were re-alised (the gap-like underscores above) is that froma generation perspective there is a choice to be madeabout whether to realise the subject NP in the secondand third coordinates or not.
(and title) of one text is London.2In terminology and view of grammar the annotations relyheavily on Huddleston and Pullum (2002).2.2 XML formatFigure 1 is one of the texts distributed in the GRECdata sample for the REG Challenge.
The REF el-ement indicates a reference, in the sense of ?aninstance of referring?
(which could, in principle,be realised by gesture or graphically, as well asby a string of words, or a combination of these).REFs have three attributes: ID, a unique refer-ence identifier; SEMCAT, the semantic category ofthe referent, ranging over city, country, river,person, mountain; and SYNCAT, the syntactic cat-egory required of referential expressions for the ref-erent in this discourse context (np-obj, np-subj,subj-det).
A REF is composed of one REFEX el-ement (the ?selected?
referential expression for thegiven reference; in the corpus texts it is simplythe referential expression found in the corpus) andone ALT-REFEX element which in turn is a list ofREFEXs which are alternative referential expressionsobtained by other means (see following section).REFEX elements have four attributes.
TheHEAD attribute has the possible values nominal,pronoun, and rel-pron; the CASE attribute hasthe possible values nominative, accusative andgenitive for pronouns, and plain and genitivefor nominals.
The binary-valued EMPHATIC at-tribute indicates whether the RE is emphatic; in thepresent version of the GREC corpus, the only type ofRE that has this attribute is one which incorporatesa reflexive pronoun used emphatically (e.g.
India it-self ).
The REG08-TYPE attribute indicates basic REtype as required for the REG?08 GREC task defini-tion.
The choice of types is motivated by the hy-pothesis that one of the most basic decisions to betaken in RE selection for named entities is whether touse an RE that includes a name, such as Modern In-dia (the corresponding REG08-TYPE value is name);whether to go for a common-noun RE, i.e.
with acategory noun like country as the head (common);whether to pronominalise the RE (pronoun); orwhether it can be left unrealised (empty).2.3 The REG?08 GREC TaskThe task for participating systems was to developa method for selecting one of the REFEXs in theALT-REFEX list, for each REF in each TEXT in thetest sets.
The test data inputs were identical to the184<?xml version="1.0" encoding="utf-8"?><!DOCTYPE TEXT SYSTEM "reg08-grec.dtd"><TEXT ID="36"><TITLE>Jean Baudrillard</TITLE><PARAGRAPH><REF ID="36.1" SEMCAT="person" SYNCAT="np-subj"><REFEX REG08-TYPE="name" EMPHATIC="no" HEAD="nominal" CASE="plain">Jean Baudrillard</REFEX><ALT-REFEX><REFEX REG08-TYPE="name" EMPHATIC="no" HEAD="nominal" CASE="plain">Jean Baudrillard</REFEX><REFEX REG08-TYPE="name" EMPHATIC="yes" HEAD="nominal" CASE="plain">Jean Baudrillard himself</REFEX><REFEX REG08-TYPE="empty">_</REFEX><REFEX REG08-TYPE="pronoun" EMPHATIC="no" HEAD="pronoun" CASE="nominative">he</REFEX><REFEX REG08-TYPE="pronoun" EMPHATIC="yes" HEAD="pronoun" CASE="nominative">he himself</REFEX><REFEX REG08-TYPE="pronoun" EMPHATIC="no" HEAD="rel-pron" CASE="nominative">who</REFEX><REFEX REG08-TYPE="pronoun" EMPHATIC="yes" HEAD="rel-pron" CASE="nominative">who himself</REFEX></ALT-REFEX></REF>(born June 20, 1929) is a cultural theorist, philosopher, political commentator,sociologist, and photographer.<REF ID="36.2" SEMCAT="person" SYNCAT="subj-det"><REFEX REG08-TYPE="pronoun" EMPHATIC="no" HEAD="pronoun" CASE="genitive">His</REFEX><ALT-REFEX><REFEX REG08-TYPE="name" EMPHATIC="no" HEAD="nominal" CASE="genitive">Jean Baudrillard?s</REFEX><REFEX REG08-TYPE="pronoun" EMPHATIC="no" HEAD="pronoun" CASE="genitive">his</REFEX><REFEX REG08-TYPE="pronoun" EMPHATIC="no" HEAD="rel-pron" CASE="genitive">whose</REFEX></ALT-REFEX></REF>work is frequently associated with postmodernism and post-structuralism.</PARAGRAPH></TEXT>Figure 1: Example text from REG?08 Training Data.training/development data, except that REF elementscontained only an ALT-REFEX list, not the preced-ing ?selected?
REFEX.
ALT-REFEX lists are gener-ated for each text by an automatic method whichcollects all the (manually annotated) MSREs in a textincluding the title and adds several defaults: pro-nouns and reflexive pronouns in all subdomains; andcategory nouns (e.g.
the river), in all subdomainsexcept people.
The main objective in the REG?08GREC Task was to get the REG08-TYPE attribute ofREFEXs right.3 Test Data1.
GREC Test Set C-1: a randomly selected 10%subset (183 texts) of the GREC corpus (with the sameproportions of texts in the 5 subdomains as in thetraining/testing data).2.
GREC Test Set C-2: the same subset of texts asin C-1; however, for C-2 we did not use the MSREsin the corpus, but replaced each of them with threehuman-selected alternatives.
These were obtained inan online experiment as described in Belz & Varges(2007a) where subjects selected MSREs in a settingthat duplicated the conditions in which the partici-pating systems in the REG?08 GREC Task made se-lections.3 We obtained three versions of each text,where in each version all MSREs were selected bythe same person.
The motivation for creating thisversion of Test Set C was firstly that having sev-eral human-produced chains of MSREs to comparethe outputs of participating (?peer?)
systems againstis more reliable than having one only; and secondlythat Wikipedia texts are edited by multiple authorsand so MSR chains may sometimes be adversely af-fected by this; we wanted to have additional refer-ence texts without this characteristic.3.
GREC Test Set L: 74 Wikipedia introductorytexts from the subdomain of lakes; participants didnot know what this subdomain was until they re-ceived the test data (there were no lake texts in thetraining/development set).4.
GREC Test Set P: 31 short encyclopaedic textsin the same 5 subdomains as in the GREC corpus,in approximately the same proportions as in thetraining/testing data, but from a source other than3The experiment can be tried out here: http://www.nltg.brighton.ac.uk/home/Anja.Belz/TESTDRIVE/185Wikipedia.
We transcribed these texts from printedencyclopaedias published in the 1980s which arenot available in electronic form, and this provenancewas not revealed to participants.
The texts in this setare much shorter and more homogeneous than theWikipedia texts, and the sequences of MSRs followvery similar patterns.
It seems likely that it is theseproperties that have resulted in better scores overallfor Test Set P (see Section 6).Each test set was designed to test peer systems fora different aspect of generalisation.
Test Set C testsfor generalisation to unseen material from the samecorpus and the same subdomains as the training set;Test Set L tests for generalisation to unseen materialfrom the same corpus but different subdomain; andTest Set P tests generalisation to a different corpusbut same subdomains.4 Evaluation methods4.1 Automatic intrinsic evaluationsAccuracy of REG08-Type: when computed againstthe single-RE test sets (C-1, L and P), REG08-TypeAccuracy is the proportion of REFEXs selected by aparticipating system that have a REG08-TYPE valueidentical to the one in the corpus.When computed against the triple-RE test set (C-2), first the number of correct REG08-Types is com-puted at the text level for each of the three ver-sions of a corpus text and the maximum of theseis determined; then the maximum text-level num-bers are summed and divided by the total number ofREFs in all the texts, which gives the global REG08-Type Accuracy score.
The rationale behind com-puting the REG08-Type Accuracy scores in this wayfor multiple-RE test sets (maximising scores on REchains rather than individual REs) is that an RE isnot good or bad in its own right, but depends on theother MSRs in the same text.4String Accuracy: This is defined just likeREG08-Type Accuracy, except here what is deter-mined is identity between REFEX word strings (theMSREs themselves), not between REG08-Types.String-edit distance metrics: String-edit dis-tance (SE) is straightforward Levenshtein distancewith a substitution cost of 2 and insertion/deletion4This definition is also slightly different from the one givenin the Participants?
Pack.cost of 1.
We also used the version of string-editdistance described by Bangalore et al (2000) whichnormalises for length.
This version is denoted ?SEB?below.
For the single-RE test sets, the global scoreis simply the average of all RE-level scores.
For TestSet C-2, we used an approach analogous to that de-scribed above for REG08-Type Accuracy.
We firstcomputed the best string-edit distance at the textlevel (here, just the sum of RE-level distances) andthen obtained the global distance by dividing thesum of best text-level distances by the number ofREFs in all the texts.Other metrics: BLEU is a precision metric fromMT that assesses the quality of a peer translationin terms of the proportion of its word n-grams(n ?
4 is standard) that it shares with several ref-erence translations.
We used BLEU-3 rather thanthe more standard BLEU-4 because most REs in thecorpus are less than 4 tokens long.
We also usedthe NIST version of BLEU which weights in favourof less frequent n-grams, as well as ROUGE-2 andROUGE-SU4 (the two official automatic scores fromthe DUC summarisation competitions).
In all cases,we assessed just the MSREs selected by peer systems(leaving out the surrounding text), and computedscores globally (rather than averaging over RE-levelscores), as this is standard for these metrics.BLEU, NIST and ROUGE are designed to workwith either one or multiple reference texts, so we didnot need to use a different method for Test Set C-2.4.2 Human extrinsic evaluationWe designed a reading/comprehension experimentin which the task for subjects was to read textsone sentence at a time and then to answer threebrief multiple-choice comprehension questions afterreading each text.
The basic idea was that it seemedlikely that badly chosen MSR reference chains wouldadversely affect ease of comprehension, and that thismight in turn affect reading speed and accuracy inanswering comprehension questions.We used a randomly selected subset of 21 textsfrom Test Set C, and recruited 21 subjects fromamong the staff, faculty and students of Brightonand Sussex universities.
We used a Repeated LatinSquares design in which each combination of textand system was allocated three trials.
During theexperiment we recorded SRTime, the time subjects186took to read sentences (from the point when the sen-tence appeared on the screen to the point at whichthe subject requested the next sentence).We also recorded the speed and accuracy withwhich subjects answered the questions at the end (Q-Time and Q-Acc).
The role of the comprehensionquestions was to encourage subjects to read the textsproperly, rather than skimming through them, andwe did not necessarily expect any significant resultsfrom the associated measures.The questions were designed to be of varying de-grees of difficulty and predictability.
There was oneset of three questions (each with five possible an-swers) associated with each text, and questions fol-lowed the same pattern across the texts: the firstquestion was always about the subdomain of a text;the second about the location of the main subject; thethird question was designed not to be predictable.The order of the answers was randomised for eachquestion and each subject.
The order of texts (withassociated questions) was randomised for each sub-ject.
We used the DMDX package for presentationof sentences and measuring reading times and ques-tion answering accuracy (Forster and Forster, 2003).Subjects did the experiment in a quiet room, undersupervision.4.3 Automatic extrinsic evaluationAs a new and highly experimental method, we triedout an automatic approach to extrinsic evaluation.The basic idea was similar to that in the human-based experiments described above: badly chosenreference chains seem likely to affect the reader?sability to resolve REs.
In the automatic version, therole of the reader is played by an automatic coref-erence resolution tool and the expectation is that thetool performs worse (are less able to identify coref-erence chains correctly) with worse MSR referencechains.To counteract the potential problem of results be-ing a function of a specific coreference resolutionalgorithm or tool, we decided to use three differ-ent resolvers?those included in LingPipe,5 JavaRap(Qiu et al, 2004) and OpenNLP (Morton, 2005)?and to average results.There does not appear to be a single standard eval-5http://alias-i.com/lingpipe/uation metric in the coreference resolution commu-nity, so we opted to use three: MUC-6 (Vilain et al,1995), CEAF (Luo, 2005), and B-CUBED (Bagga andBaldwin, 1998), which seem to be the most widelyaccepted metrics.All three metrics compute Recall, Precision andF-Scores on aligned gold-standard and resolver-toolcoreference chains.
They differ in how the align-ment is obtained and what components of corefer-ence chains are counted for calculating scores.
Re-sults for the automatic extrinsic evaluations are re-ported below in terms of the F-Scores from thesethree metrics, as well as in terms of their average.5 SystemsBase-rand, Base-freq, Base-1st, Base-name: Wecreated four baseline systems.
Base-rand selectsone of the REFEXs at random.
Base-freq selectsthe REFEX that is the overall most frequent giventhe SYNCAT and SEMCAT of the reference.
Base-1st always selects the REFEX which appears firstin the list of REFEXs; and Base-name selects theshortest REFEX with attributes REG08-TYPE=name,HEAD=nominal and EMPHATIC=no.6CNTS-Type-g, CNTS-Prop-s: The CNTS sys-tems are trained using memory-based learning withautomatic parameter optimisation.
They use a set of14 features obtained by various kinds of syntacticpreprocessing and named-entity recognition as wellas from the corpus annotations: SEMCAT, SYNCAT,position of RE in text, neighbouring words and POS-tags, distance to previous mention, SYNCATs ofthree preceding REFEXs, binary feature indicatingwhether the most recent named entity was the mainsubject (MS), main verb of the sentence.
For Type-g, a single classifier was trained to predict just theREG08-TYPE property of REFEXs.
For Prop-s, fourclassifiers were trained, one for each subdomain, topredict all four properties of REFEXs (rather than justREG08-TYPE).OSU-b-all, OSU-b-nonRE, OSU-n-nonRE: TheOSU-2 systems are maximum-entropy classifierstrained on a range of features obtained by prepro-6Attributes are tried in this order.
If for one attribute, theright value is not found, the process ignores that attribute andmoves on the next one.187System REG08-Type Accuracy for Development SetAll Cities Coun Riv Peop MounCNTS-Type-g 76.52 64.65 75 65 85.37 75.42CNTS-Prop-s 73.93 65.66 69.57 70 79.51 74.58IS-G 66 54.5 64 80 66.8 65OSU-n-nonRE 62.50 53.54 63.04 65 67.32 61.67OSU-b-all 58.54 53.54 57.61 75 65.85 49.58OSU-b-nonRE 51.07 51.52 53.26 40 57.07 45.83Table 1: Self-reported REG08-Type Accuracy scores fordevelopment set.cessing the text, as well as from the corpus anno-tations: SEMCAT, SYNCAT, position of RE in text,presence of contrasting discourse entity, distance be-tween current and preceding reference to the MS,string similarity measures between REFEXs and ti-tle of text.
OSU-b-all and OSU-b-nonRE are binaryclassifiers which give the likelihood of selecting agiven REFEX vs. not selecting it, whereas OSU-n-nonRE is a 4-class classifier giving the likelihoodsof selecting each of the four REG08-TYPEs.
OSU-b-all also uses the REFEX attributes as features.IS-G: The IS-G system is a multi-layer percep-tron which uses four features obtained by prepro-cessing texts as well as from the corpus annota-tions: SYNCAT, distance between current and pre-ceding reference to the MS, position of RE in text,REG08-TYPE of preceding reference to the MS, fea-ture indicating whether the preceding MSR is in thesame sentence.6 ResultsThis section presents the results of all the evalua-tion methods described in Section 4.
We start withREG08-Type Accuracy, an intrinsic automatic met-ric which participating teams were told was goingto be the chief evaluation method, followed by otherintrinsic automatic metrics (Section 6.2), the extrin-sic human evaluation (Section 6.3) and the extrinsicautomatic evaluation (Section 6.4).6.1 REG08-Type AccuracyParticipants computed REG08-Type Accuracy forthe development set (97 texts) themselves, using atool provided by us.
These scores are shown inTable 1, and are also included in the participants?reports elsewhere in this volume.
Systems are or-dered in terms of their overall REG08-Type Accu-racy (column 1), and scores for each subdomain arealso shown.
Scores are highly consistent across thesubdomains, except for the river subdomain whichwas the smallest set (containing only 4 texts), andresults for it may be idiosyncratic for this reason.Corresponding results for the (unseen) test set C-1are shown in column 2 of Table 2.
As would be ex-pected, results are slightly worse than for the (seen)development set (although some systems managedto improve over their development set scores).
Alsoincluded in this table are results for the four base-line systems, and it is clear that selecting the mostfrequent REG08-Type given SEMCAT and SYNCAT(as done by the Base-freq system) provides a strongbaseline.Other columns in Table 2 contain results for testsets L and P. Again as expected, results for Test SetL are lower than for Test Set C-1, because in ad-dition to consisting of unseen texts (like C-1), TestSet L is also from an unseen subdomain (unlike C-1).
The results for Test Set P are higher and on a parwith those for the development set, probably for thereasons discussed at the end of Section 3.For each test set in Table 2 we carried out a uni-variate ANOVA with System as the fixed factor.
Wefound significant main effects at p < .001 in allthree cases (C-1: F = 95.426; L: F = 63.758;P: F = 21.188).
The columns containing capitalletters in Table 2 show the homogeneous subsets ofsystems as determined by post-hoc Tukey HSD com-parisons of means.
Systems whose REG08-Type Ac-curacy scores are not significantly different (at the.05 level) share a letter.The results for REG08-Type Accuracy computedagainst the triple-RE Test Set C-2 are shown in Ta-ble 3.
These should be considered as the chief resultsof the GREC Task evaluations, as stated in the guide-lines.
Here too we performed a univariate ANOVAwith System as the fixed factor and REG08-Typeas the dependent variable.
Having established byANOVA that there was a significant main effect ofSystem (F = 86.946, p < .001), we compared themean scores with Tukey?s HSD.
As can be seen fromthe resulting homogeneous subsets, there is no sig-nificant difference between the corpus texts (C-1)and system CNTS-Type-g, but also there is no sig-188single-RE Test Set C-1 Test Set L Test Set PCNTS-Type-g 68.15 A CNTS-Type-g 62.06 A CNTS-Type-g 75.31 ACNTS-Prop-s 67.04 A CNTS-Prop-s 62.06 A CNTS-Prop-s 72.84 A BIS-G 66.48 A IS-G 60.93 A IS-G 67.90 A B COSU-n-nonRE 63.69 A OSU-n-nonRE 41.80 B OSU-n-nonRE 66.67 A B COSU-b-nonRE 53.11 B OSU-b-nonRE 39.23 B OSU-b-all 57.41 B C DOSU-b-all 52.39 B OSU-b-all 37.62 B C OSU-b-nonRE 56.17 C DBase-freq 43.47 C Base-freq 35.53 B C Base-freq 44.44 D FBase-name 39.49 C Base-rand 23.63 C D Base-rand 33.95 FBase-1st 39.17 C Base-name 23.63 D Base-name 32.10 FBase-rand 32.72 D Base-1st 29.74 D Base-rand 32.10 FTable 2: REG08-Type Accuracy scores and homogeneous subsets (Tukey HSD, alpha = .05) for single-RE test sets.Systems that do not share a letter are significantly different.System REG08-Type Accuracy for multiple-RE Test Set C-2All Cities Countries Rivers People MountainsCorpus 78.58 A 70.92 77.49 85.29 84.67 75.81CNTS-Type-g 72.61 A B 65.96 71.73 73.53 77.64 70.73CNTS-Prop-s 71.34 B 64.54 67.02 70.59 75.38 71.75IS-G 70.78 B 69.50 65.45 76.47 76.88 67.89OSU-n-nonRE 69.82 B 65.25 64.92 79.41 78.14 65.65OSU-b-nonRE 58.76 C 52.48 60.73 50.00 59.80 59.55OSU-b-all 57.48 C 53.90 58.64 47.06 59.05 57.52Base-name 50.00 D 53.19 54.45 35.29 43.22 53.86Base-1st 49.28 D 53.19 49.21 38.24 43.22 53.86Base-freq 48.17 D 43.97 42.41 55.88 56.78 44.11Base-rand 41.24 E 41.84 36.13 32.35 44.47 41.06Table 3: REG08-Type Accuracy scores against Test Set C-2 for complete set and for subdomains; homogeneous subsets(Tukey HSD, alpha = .05) for complete set only (systems that do not share a letter are significantly different).nificant difference between the latter and systemsCNTS-Prop-s, IS-G and OSU-n-nonRE.
In this anal-ysis, all systems outperform the random baseline;all peer systems outperform all of the baselines; andthe four best peer systems outperform the remainingtwo.6.2 Other automatic intrinsic metricsIn addition to the chief evaluation measure reportedon in the preceding section, we computed the stringsimilarity metrics described in Section 4.1 for allfour test sets.
Results were very similar to thosefor REG08-Type Accuracy, so we are reporting onlyscores for Test Set C-2 (Table 4).
The corpus textsagain receive the best scores across the board (SE isthe odd one out, because here lower scores are bet-ter).
Ranks for peer systems are very similar to theresults reported in the last section.We performed an ANOVA (F = 138.159, p <.001) and Tukey HSD post-hoc analysis for StringAccuracy.
The resulting homogeneous subsets (Ta-ble 4, columns 3?8) reveal significant differencessimilar to those for REG08-Type Accuracy.
We alsocomputed Pearson product-moment correlation co-efficients between all automatic intrinsic evaluationmeasures we used.
All pairwise correlations weresignificant at the .01 level (using a two-tailed test).One of the strongest correlations (.961) was betweenREG08-Type Accuracy and String Accuracy, imply-ing that getting REG08-Type right gets you someway towards getting the actual RE right.6.3 Human-based extrinsic measuresAs a result of the experiment described in Sec-tion 4.2 we had SRTime measures (sentence readingtimes) for each sentence in each of the 21 texts thatwere included in the experiment.
Table 5 shows theresulting SRTimes in milliseconds averaged per sys-tem.
None of the differences were statistically sig-nificant.
We also analysed SRTimes normalised bysentence length; SRTimes only from sentences thatcontained MRSs; and SRTimes normalised for sub-ject reading speed.
There were no significant differ-ences under any of these analyses.Much of the variance in SRTimes was due to sub-jects?
very different average reading speeds: meansof SRTime normalised for sentence length rangedfrom 188.45ms to 426.10ms for individual subjects.189System Word string similarity for Triple-RE Test Set C-2String Accuracy BLEU-3 NIST ROUGE-2 ROUGE-SU4 SE SEBCorpus 71.18 A 0.7792 7.5080 0.66102 0.70991 0.7229 0.5136CNTS-Type-g 65.61 A B 0.7377 6.1288 0.60280 0.64998 0.8838 0.3627CNTS-Prop-s 65.29 A B 0.6760 5.9338 0.60103 0.64963 0.9068 0.3835OSU-n-nonRE 63.85 B C 0.6715 5.7745 0.53395 0.57459 0.9666 0.0164IS-G 58.20 C 0.5107 5.6102 0.50270 0.57052 1.1616 0.1818OSU-b-nonRE 51.11 D 0.4964 5.5363 0.38255 0.42969 1.2834 0.0247OSU-b-all 50.72 D 0.5050 5.6058 0.35133 0.39570 1.2994 0.3402Base-freq 41.32 E 0.2684 3.0155 0.27727 0.33007 1.54299 -0.3250Base-name 39.41 E 0.4641 5.9372 0.20730 0.25379 1.5175 -0.1912Base-1st 39.09 E 0.3932 5.1597 0.21443 0.24037 1.6449 -0.0751Base-rand 17.99 F 0.2182 2.9327 0.36056 0.41847 2.3217 -0.7937Table 4: String Accuracy, BLEU, NIST, ROUGE and string-edit scores, computed on single-RE and triple-RE testsets (systems in order of String Accuracy); homogeneous subsets (Tukey HSD, alpha = .05) for String Accuracy only(systems that do not share a letter are significantly different).Mean SRTime (msecs)CNTS-Prop-s 6305.8551IS-G 6340.5131OSU-n-nonRE 6422.5073CNTS-Type-g 6435.6574OSU-b-all 6451.7624OSU-b-nonRE 6454.6749Corpus 6548.2734Table 5: Mean SRTimes for each system.There was also variance from Text, i.e.
some of thetexts appear to be harder to read than others.The other two measures from the task-performance experiment were Q-Acc (questionanswering accuracy) and Q-Time (question answer-ing speed).
ANOVAs revealed no significant maineffect of System on Q-Time.
For Q-Acc, we lookedat each of the three question types Q1, Q2, Q3(see Section 4.2) separately.
ANOVAs showed nosignificant effect of System on Q-Acc for Q2 andQ3; there was a slight effect (F = 2.193, p < .05)of System on Q-Acc for Q1 (the easiest of thequestions which simply asked for the subdomainof a text).
Table 6 shows Q-Acc for Q1 and Q2,and the results of a post-hoc analysis (Tukey HSD)which revealed two homogeneous subsets with a lotof overlap (columns 2 and 3).Table 6 shows the results of this analysis: therewas6.4 Automatic extrinsic measuresWe used the same 21 texts as in the human extrin-sic experiments, fed the outputs of each peer sys-Question 1 Q2 Q3Corpus 1.00 A .78 .63CNTS-Type-g 1.00 A .83 .71CNTS-Prop-s .98 A B .86 .75OSU-b-nonRE .97 A B .83 .67OSU-b-all .95 A B .75 .62IS-G .95 A B .81 .63OSU-n-nonRE .90 B .76 .76Table 6: Question types 1?3, proportions correct; homo-geneous subsets for Q1 (Tukey HSD, alpha = .05).tem as well as the corpus texts through the threecoreference resolvers, and computed average MUC,CEAF and B-CUBED F-Scores as described in Sec-tion 4.3.
The second column Table 7 shows the av-erage of these three F-Scores, to give a single over-all result for this evaluation method.
A univariateANOVA with the average F-Score (column 2) as thedependent variable and System as the single fixedfactor revealed a significant main effect of Systemon average F-Score (F = 5.051, p < .001).
Apost-hoc comparison of the means (Tukey HSD, al-pha = .05) found the significant differences indi-cated by the homogeneous subsets in columns 3?5 (Table 7).
The numbers shown in the last threecolumns are the separate MUC, CEAF and B-CUBEDF-Scores for each system, averaged over the threeresolver tools.
ANOVAs revealed the following ef-fects of System: on CEAF F = 9.984, p < .001;on MUC: F = 10.07, p < .001; on B-CUBED:F = 8.446, p < .001.The three F-Score measures (MUC, CEAF and B-CUBED) are all strongly and highly significantly cor-190related: Pearson?s correlation coefficient is .947 forB-CUBED and CEAF, .917 for B-CUBED and MUC,and .951 for CEAF and MUC (p < .01, 2-tailed).System (MUC+CEAF+B3)/3 MUC CEAF B3Base-1st 53.50 A 47.59 52.64 60.28Base-name 52.84 A 45.99 51.73 60.81OSU-n-nonRE 51.39 A 46.92 49.8 57.45OSU-b-nonRE 51.27 A 47.68 48.62 57.50OSU-b-all 50.87 A 47.06 48.40 57.14CNTS-Type-g 48.64 A B 43.77 46.32 55.82IS-G 48.05 A B 43.25 46.24 54.66CNTS-Prop-s 46.35 A B 42.82 43.36 52.88Corpus 43.32 A B 37.89 41.6 50.47Base-freq 41.41 B C 34.48 40.28 49.46Base-rand 35.13 C 21.24 35.60 48.55Table 7: MUC, CEAF and B-CUBED F-Scores for all sys-tems; homogeneous subsets (Tukey HSD), alpha = .05,for average of F-Scores.7 Concluding RemarksThe GREC Task is a new task not only for an NLGshared-task challenge, but also as a research task ingeneral (improving referential clarity in extractivesummaries seems to be just taking off as a researchsubfield).
It was therefore not unexpected that onlythree teams were able to participate in this task.We continued the traditions of the ASGRE?07Challenge in that we used a wide range of evalu-ation metrics to obtain a well-rounded view of thequality of the participating systems.
It had been ourintention to use evaluation methods in all four possi-ble extrinsic/intrinsic and automatic/human combi-nations.
However, the combination intrinsic/humanis missing from this report and will have to be left tofuture research.There was no indication in the human task perfor-mance experiment that the different reference chainsselected by different systems had any impact on sub-jects?
reading speeds, and the evidence that thereis an effect on comprehension was scant.
Thismeans that we will need to investigate alternativetask-performance measures.
Because of the lack ofsignificant results from the human extrinsic experi-ment, we were also unable to validate the automaticextrinsic experiment against it, and so at this pointwe do not really know how useful it is (despite somecorrelation with intrinsic measures), something wewill seek to establish in future research.AcknowledgmentsMany thanks to Jason Baldridge and Pascal De-nis for help with selecting coreference resolutiontools and metrics, and to the colleagues and studentswho helped with the task-performance experiment.Thanks are also due to the members of the Corporaand SIGGEN mailing lists, colleagues, friends andfriends of friends who helped with the online MSREselection experiment.ReferencesA.
Bagga and B. Baldwin.
1998.
Algorithms for scoringcoreference chains.
In Proceedings of the LinguisticCoreference Workshop at LREC?98, pages 563?566.S.
Bangalore, O. Rambow, and S. Whittaker.
2000.Evaluation metrics for generation.
In Proceedings ofINLG?00, pages 1?8.A.
Belz and S. Varges.
2007a.
Generation of repeatedreferences to discourse entities.
In Proceedings ofENLG?07, pages 9?16.A.
Belz and S. Varges.
2007b.
The GREC corpus: Mainsubject reference in context.
Technical Report NLTG-07-01, University of Brighton.K.
I. Forster and J. C. Forster.
2003.
DMDX: A win-dows display program with millisecond accuracy.
Be-havior Research Methods, Instruments, & Computers,35(1):116?124.R.
Huddleston and G. Pullum.
2002.
The CambridgeGrammar of the English Language.
Cambridge Uni-versity Press.X.
Luo.
2005.
On coreference resolution performancemetrics.
Proc.
of HLT-EMNLP, pages 25?32.T.
Morton.
2005.
Using Semantic Relations to ImproveInformation Retrieval.
Ph.D. thesis, University of Pen-sylvania.A.
Nenkova.
2008.
Entity-driven rewrite for multi-document summarization.
In Proceedings of IJC-NLP?08.L.
Qiu, M. Kan, and T.-S. Chua.
2004.
A public ref-erence implementation of the rap anaphora resolutionalgorithm.
In Proceedings of LREC?04, pages 291?294.J.
Steinberger, M. Poesio, M. Kabadjov, and K. Jezek.2007.
Two uses of anaphora resolution in summariza-tion.
Information Processing and Management: Spe-cial issue on Summarization, 43(6):1663?1680.M.
Vilain, J. Burger, J. Aberdeen, D. Connolly, andL.
Hirschman.
1995.
A model-theoretic coreferencescoring scheme.
Proceedings of MUC-6, pages 45?52.191
