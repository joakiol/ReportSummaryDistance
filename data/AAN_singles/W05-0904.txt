Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translationand/or Summarization, pages 25?32, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsSyntactic Features for Evaluation of Machine TranslationDing Liu and Daniel GildeaDepartment of Computer ScienceUniversity of RochesterRochester, NY 14627AbstractAutomatic evaluation of machine transla-tion, based on computing n-gram similar-ity between system output and human ref-erence translations, has revolutionized thedevelopment of MT systems.
We explorethe use of syntactic information, includ-ing constituent labels and head-modifierdependencies, in computing similarity be-tween output and reference.
Our resultsshow that adding syntactic informationto the evaluation metric improves bothsentence-level and corpus-level correla-tion with human judgments.1 IntroductionEvaluation has long been a stumbling block in thedevelopment of machine translation systems, due tothe simple fact that there are many correct transla-tions for a given sentence.
Human evaluation of sys-tem output is costly in both time and money, leadingto the rise of automatic evaluation metrics in recentyears.
The most commonly used automatic evalua-tion metrics, BLEU (Papineni et al, 2002) and NIST(Doddington, 2002), are based on the assumptionthat ?The closer a machine translation is to a profes-sional human translation, the better it is?
(Papineniet al, 2002).
For every hypothesis, BLEU computesthe fraction of n-grams which also appear in the ref-erence sentences, as well as a brevity penalty.
NISTuses a similar strategy to BLEU but further consid-ers that n-grams with different frequency should betreated differently in the evaluation.
It introduces thenotion of information weights, which indicate thatrarely occurring n-grams count more than those fre-quently occurring ones in the evaluation (Dodding-ton, 2002).
BLEU and NIST have been shown tocorrelate closely with human judgments in rankingMT systems with different qualities (Papineni et al,2002; Doddington, 2002).In the 2003 Johns Hopkins Workshop on Speechand Language Engineering, experiments on MTevaluation showed that BLEU and NIST do not cor-relate well with human judgments at the sentencelevel, even when they correlate well over large testsets (Blatz et al, 2003).
Kulesza and Shieber (2004)use a machine learning approach to improve the cor-relation at the sentence level.
Their method, basedon the assumption that higher classification accuracyin discriminating human- from machine-generatedtranslations will yield closer correlation with hu-man judgments, uses support vector machine (SVM)based learning to weight multiple metrics such asBLEU, NIST, and WER (minimal word error rate).The SVM is trained for differentiating the MT hy-pothesis and the professional human translations,and then the distance from the hypothesis?s metricvector to the hyper-plane of the trained SVM is takenas the final score for the hypothesis.While the machine learning approach improvescorrelation with human judgments, all the metricsdiscussed are based on the same type of information:n-gram subsequences of the hypothesis translations.This type of feature cannot capture the grammatical-ity of the sentence, in part because they do not takeinto account sentence-level information.
For exam-ple, a sentence can achieve an excellent BLEU scorewithout containing a verb.
As MT systems improve,the shortcomings of n-gram based evaluation are be-coming more apparent.
State-of-the-art MT output25reference: SNPPRONVPV NPART Nhypothesis 1: SNPPRONVPV NPART Nhypothesis 2: SNPART NNPPRONVPVFigure 1: Syntax Trees of the Examplesoften contains roughly the correct words and con-cepts, but does not form a coherent sentence.
Oftenthe intended meaning can be inferred; often it can-not.
Evidence that we are reaching the limits of n-gram based evaluation was provided by Charniak etal.
(2003), who found that a syntax-based languagemodel improved the fluency and semantic accuracyof their system, but lowered their BLEU score.With the progress of MT research in recent years,we are not satisfied with the getting correct wordsin the translations; we also expect them to be well-formed and more readable.
This presents new chal-lenges to MT evaluation.
As discussed above, theexisting word-based metrics can not give a clearevaluation for the hypothesis?
fluency.
For exam-ple, in the BLEU metric, the overlapping fractionsof n-grams with more than one word are consideredas a kind of metric for the fluency of the hypothesis.Consider the following simple example:Reference: I had a dog.Hypothesis 1: I have the dog.Hypothesis 2: A dog I had.If we use BLEU to evaluate the two sentences, hy-pothesis 2 has two bigrams a dog and I had whichare also found in the reference, and hypothesis 1 hasno bigrams in common with the reference.
Thus hy-pothesis 2 will get a higher score than hypothesis 1.The result is obviously incorrect.
However, if weevaluate their fluency based on the syntactic simi-larity with the reference, we will get our desired re-sults.
Figure 1 shows syntactic trees for the examplesentences, from which we can see that hypothesis 1has exactly the same syntactic structure with the ref-erence, while hypothesis 2 has a very different one.Thus the evaluation of fluency can be transformed ascomputing the syntactic similarity of the hypothesisand the references.This paper develops a number of syntacticallymotivated evaluation metrics computed by automat-ically parsing both reference and hypothesis sen-tences.
Our experiments measure how well thesemetrics correlate with human judgments, both for in-dividual sentences and over a large test set translatedby MT systems of varying quality.2 Evaluating Machine Translation withSyntactic FeaturesIn order to give a clear and direct evaluation for thefluency of a sentence, syntax trees are used to gen-erate metrics based on the similarity of the MT hy-pothesis?s tree and those of the references.
We can?texpect that the whole syntax tree of the hypothesiscan always be found in the references, thus our ap-proach is to be based on the fractions of the subtrees26which also appear in the reference syntax trees.
Thisidea is intuitively derived from BLEU, but with theconsideration of the sparse subtrees which lead tozero fractions, we average the fractions in the arith-metic mean, instead of the geometric mean usedin BLEU.
Then for each hypothesis, the fractionsof subtrees with different depths are calculated andtheir arithmetic mean is computed as the syntax treebased metric, which we denote as ?subtree metric?STM:STM = 1DD?n=1?t?subtreesn(hyp) countclip(t)?t?subtreesn(hyp) count(t)where D is the maximum depth of subtrees con-sidered, count(t) denotes the number of times sub-tree t appears in the candidate?s syntax tree, andcountclip(t) denotes the clipped number of timest appears in the references?
syntax trees.
Clippedhere means that, for a given subtree, the count com-puted from the hypothesis syntax tree can not exceedthe maximum number of times the subtree occurs inany single reference?s syntax tree.
A simple exam-ple with one hypothesis and one reference is shownin Figure 2.
Setting the maximum depth to 3, wego through the hypothesis syntax tree and computethe fraction of subtrees with different depths.
Forthe 1-depth subtrees, we get S, NP, VP, PRON, V,NP which also appear in the reference syntax tree.Since PRON only occurs once in the reference, itsclipped count should be 1 rather than 2.
Then weget 6 out of 7 for the 1-depth subtrees.
For the 2-depth subtrees, we get S?NP VP, NP?PRON, andVP?V NP which also appear in the reference syntaxtree.
For the same reason, the subtree NP?PRONcan only be counted once.
Then we get 3 out of 4for the 2-depth subtree.
Similarly, the fraction of3-depth subtrees is 1 out of 2.
Therefore, the finalscore of STM is (6/7+3/4+1/2)/3=0.702.While the subtree overlap metric defined aboveconsiders only subtrees of a fixed depth, subtrees ofother configurations may be important for discrimi-nating good hypotheses.
For example, we may wantto look for the subtree:SNP VPV NPto find sentences with transitive verbs, while ignor-ing the internal structure of the subject noun phrase.In order to include subtrees of all configurations inour metric, we turn to convolution kernels on ourtrees.
Using H(x) to denote the vector of counts ofall subtrees found in tree x, for two trees T1 and T2,the inner product H(T1) ?H(T2) counts the numberof matching pairs of subtrees of T1 and T2.
Collinsand Duffy (2001) describe a method for efficientlycomputing this dot product without explicitly com-puting the vectors H , which have dimensionality ex-ponential in the size of the original tree.
In order toderive a similarity measure ranging from zero to one,we use the cosine of the vectors H:cos(T1, T2) =H(T1) ?H(T2)|H(T1)||H(T2)|Using the identity|H(T1)| =?H(T1) ?H(T1)we can compute the cosine similarity using the ker-nel method, without ever computing the entire ofvector of counts H .
Our kernel-based subtree metricTKM is then defined as the maximum of the cosinemeasure over the references:TKM = maxt?refcos(hyp, t)The advantage of using the tree kernel is that itcan capture the similarity of subtrees of differentshapes; the weak point is that it can only use thereference trees one by one, while STM can use themsimultaneously.
The dot product also weights indi-vidual features differently than our other measures,which compute overlap in the same way as doesBLEU.
For example, if the same subtree occurs 10times in both the hypothesis and the reference, thiscontributes a term of 100 to the dot product, ratherthan 10 in the clipped count used by BLEU and byour subtree metric STM.2.1 Dependency-Based MetricsDependency trees consist of trees of head-modifierrelations with a word at each node, rather than justat the leaves.
Dependency trees were found to corre-spond better across translation pairs than constituenttrees by Fox (2002), and form the basis of the ma-chine translation systems of Alshawi et al (2000)27reference: SNPPRONVPV NPART Nhypothesis: SNPPRONVPV NPPRONFigure 2: Examples for the Computation of STMand Lin (2004).
We derived dependency trees fromthe constituent trees by applying the determinis-tic headword extraction rules used by the parser ofCollins (1999).
For the example of the referencesyntax tree in Figure 2, the whole tree with the rootS represents a sentence; and the subtree NP?ART Nrepresents a noun phrase.
Then for every node in thesyntax tree, we can determine its headword by itssyntactic structure; from the subtree NP?ART N,for example, the headword selection rules chose theheadword of NP to be word corresponding to thePOS N in the subtree, and the other child, which cor-responds to ART, is the modifier for the headword.The dependency tree then is a kind of structure con-stituted by headwords and every subtree representsthe modifier information for its root headword.
Forexample, the dependency tree of the sentence I havea red pen is shown as below.haveI pena redThe dependency tree contains both the lexical andsyntactic information, which inspires us to use it forthe MT evaluation.Noticing that in a dependent tree the childnodes are the modifier of its parent, we proposea dependency-tree based metric by extracting theheadwords chains from both the hypothesis and thereference dependency trees.
A headword chain isa sequence of words which corresponds to a pathin the dependency tree.
Take the dependency treein Figure 2 as the example, the 2-word headwordchains include have I, have pen, pen a, and penred.
Before using the headword chains, we needto extract them out of the dependency trees.
Fig-ure 3 gives an algorithm which recursively extractsthe headword chains in a dependency tree from shortto long.
Having the headword chains, the headwordchain based metric is computed in a manner similarto BLEU, but using n-grams of dependency chainsrather than n-grams in the linear order of the sen-tence.
For every hypothesis, the fractions of head-word chains which also appear in the reference de-pendency trees are averaged as the final score.
UsingHWCM to denote the headword chain based metric,it is computed as follows:HWCM = 1DD?n=1?g?chainn(hyp) countclip(g)?g?chainn(hyp) count(g)where D is chosen as the maximum length chainconsidered.We may also wish to consider dependency rela-tions over more than two words that are contigu-ous but not in a single ancestor chain in the depen-dency tree.
For this reason, the two methods de-scribed in section 3.1 are used to compute the simi-larity of dependency trees between the MT hypothe-sis and its references, and the corresponding metricsare denoted DSTM for dependency subtree metricand DTKM for dependency tree kernel metric.3 ExperimentsOur testing data contains two parts.
One part is a setof 665 English sentences generated by a Chinese-English MT system.
And for each MT hypothesis,three reference translations are associated with it.28Input: dependency tree T, maximum length N of the headword chainOutput: headword chains from length 1 to Nfor i = 1 to Nfor every node n in Tif i == 1add n?s word to n?s 1 word headword chains;elsefor every direct child c of nfor every i-1 words headword chain hc of cnewchain = joint(n?s word, hc);add newchain to the i words headword chains of n;endforendforendifendforendforFigure 3: Algorithm for Extracting the Headword ChainsThe human judgments, on a scale of 1 to 5, were col-lected at the 2003 Johns Hopkins Speech and Lan-guage Summer Workshop, which tells the overallquality of the MT hypotheses.
The translations weregenerated by the alignment template system of Och(2003).
This testing set is called JHU testing setin this paper.
The other set of testing data is fromMT evaluation workshop at ACL05.
Three sets ofhuman translations (E01, E03, E04) are selected asthe references, and the outputs of seven MT systems(E9 E11 E12 E14 E15 E17 E22) are used for testingthe performance of our syntactic metrics.
Each setof MT translations contains 929 English sentences,each of which is associated with human judgmentsfor its fluency and adequacy.
The fluency and ade-quacy scores both range from 1 to 5.3.1 Sentence-level EvaluationOur syntactic metrics are motivated by a desire tobetter capture grammaticality in MT evaluation, andthus we are most interested in how well they cor-relate with human judgments of sentences?
fluency,rather than the adequacy of the translation.
Todo this, the syntactic metrics (computed with theCollins (1999) parser) as well as BLEU were usedto evaluate hypotheses in the test set from ACL05MT workshop, which provides both fluency and ad-equacy scores for each sentence, and their Pearsoncoefficients of correlation with the human fluencyscores were computed.
For BLEU and HWCM, inorder to avoid assigning zero scores to individualMaxLength/Depth BLEU HWCM STM DSTM1 0.126 0.130 ??
?
?2 0.132 0.142 0.142 0.1593 0.117 0.157 0.147 0.1504 0.093 0.153 0.136 0.121kernel 0.065 0.090Table 1: Correlation with Human Fluency Judg-ments for E14sentences, when precision for n-grams of a particu-lar length is zero we replace it with an epsilon valueof 10?3.
We choose E14 and E15 as two repre-sentative MT systems in the ACL05 MT workshopdata set, which have relatively high human scoresand low human scores respectively.
The results areshown in Table 1 and Table 2, with every metricindexed by the maximum n-gram length or subtreedepth.
The last row of the each table shows the tree-kernel-based measures, which have no depth param-eter to adjust, but implicitly consider all depths.The results show that in both systems our syntac-tic metrics all achieve a better performance in thecorrelation with human judgments of fluency.
Wealso notice that with the increasing of the maximumlength of n-grams, the correlation of BLEU with hu-man judgments does not necessarily increase, butdecreases in most cases.
This is contrary to the argu-ment in BLEU which says that longer n-grams bet-ter represent the sentences?
fluency than the shorter29MaxLength/Depth BLEU HWCM STM DSTM1 0.122 0.128 ??
?
?2 0.094 0.120 0.134 0.1373 0.073 0.119 0.144 0.1244 0.048 0.113 0.143 0.121kernel 0.089 0.066Table 2: Correlation with Human Fluency Judg-ments for E15ones.
The problem can be explained by the limi-tation of the reference translations.
In our exper-iments, every hypothesis is evaluated by referringto three human translations.
Since the three humantranslations can only cover a small set of possibletranslations, with the increasing of n-gram length,more and more correct n-grams might not be foundin the references, so that the fraction of longer n-grams turns to be less reliable than the short onesand hurts the final scores.
In the the corpus-levelevaluation of a MT system, the sparse data problemwill be less serious than in the sentence-level evalu-ation, since the overlapping n-grams of all the sen-tences and their references will be summed up.
Soin the traditional BLEU algorithm used for corpus-level evaluation, a maximum n-gram of length 4 or 5is usually used.
A similar trend can be found in syn-tax tree and dependency tree based metrics, but thedecreasing ratios are much lower than BLEU, whichindicates that the syntactic metrics are less affectedby the sparse data problem.
The poor performanceof tree-kernel based metrics also confirms our argu-ments on the sparse data problem, since the kernelmeasures implicitly consider the overlapping ratiosof the sub-trees of all shapes, and thus will be verymuch affected by the sparse data problem.Though our syntactic metrics are proposed forevaluating the sentences?
fluency, we are curioushow well they do in the overall evaluation of sen-tences.
Thus we also computed each metric?s cor-relation with human overall judgments in E14, E15and JHU testing set.
The overall human score foreach sentence in E14 and E15 is computed by sum-ming up its fluency score and adequacy score.
Theresults are shown in Table 3, Table 4, and Table5.
We can see that the syntactic metrics achieveMaxLength/Depth BLEU HWCM STM DSTM1 0.176 0.191 ??
?
?2 0.185 0.195 0.171 0.1933 0.169 0.202 0.168 0.1754 0.137 0.199 0.158 0.143kernel 0.093 0.127Table 3: Correlation with Human Overall Judgmentsfor E14MaxLength/Depth BLEU HWCM STM DSTM1 0.146 0.152 ??
?
?2 0.124 0.142 0.148 0.1523 0.095 0.144 0.151 0.1394 0.067 0.137 0.144 0.137kernel 0.098 0.084Table 4: Correlation with Human Overall Judgmentsfor E15competitive correlations in the test, among whichHWCM, based on headword chains, gives betterperformances in evaluation of E14 and E15, and aslightly worse performance in JHU testing set thanBLEU.
Just as with the fluency evaluation, HWCMand other syntactic metrics present more stable per-formance as the n-gram?s length (subtree?s depth)increases.3.2 Corpus-level EvaluationWhile sentence-level evaluation is useful if we areinterested in a confidence measure on MT outputs,corpus level evaluation is more useful for comparingMaxLength/Depth BLEU HWCM STM DSTM1 0.536 0.502 ??
?
?2 0.562 0.555 0.515 0.5133 0.513 0.538 0.529 0.4774 0.453 0.510 0.497 0.450kernel 0.461 0.413Table 5: Correlation with Human Overall Judgmentsfor JHU Testing Set30MaxLength/Depth BLEU HWCM STM DSTM1 0.629 0.723 ??
?
?2 0.683 0.757 0.538 0.7803 0.724 0.774 0.597 0.7804 0.753 0.778 0.612 0.7885 0.781 0.780 0.618 0.7786 0.763 0.778 0.618 0.782kernel 0.539 0.875Table 6: Corpus-level Correlation with HumanOverall Judgments (E9 E11 E12 E14 E15 E17 E22)MT systems and guiding their development.
Doeshigher sentence-level correlation necessarily indi-cate higher correlation in corpus-level evaluation?To answer this question, we used our syntactic met-rics and BLEU to evaluate all the human-scored MTsystems (E9 E11 E12 E14 E15 E17 E22) in theACL05 MT workshop test set, and computed thecorrelation with human overall judgments.
The hu-man judgments for an MT system are estimated bysumming up each sentence?s human overall score.Table 6 shows the results indexed by different n-grams and tree depths.We can see that the corpus-level correlation andthe sentence-level correlation don?t always corre-spond.
For example, the kernel dependency subtreemetric achieves a very good performance in corpus-level evaluation, but it has a poor performance insentence-level evaluation.
Sentence-level correla-tion reflects the relative qualities of different hy-potheses in a MT system, which does not indicateany information for the relative qualities of differ-ent systems.
If we uniformly decrease or increaseevery hypothesis?s automatic score in a MT sys-tem, the sentence-level correlation with human judg-ments will remain the same, but the corpus-level cor-relation will be changed.
So we might possibly getinconsistent corpus-level and sentence-level correla-tions.From the results, we can see that with the increaseof n-grams length, the performance of BLEU andHWCM will first increase up to length 5, and thenstarts decreasing, where the optimal n-gram lengthof 5 corresponds to our usual setting for BLEU algo-rithm.
This shows that corpus-level evaluation, com-pared with the sentence-level evaluation, is muchless sensitive to the sparse data problem and thusleaves more space for making use of comprehen-sive evaluation metrics.
We speculate this is why thekernel dependency subtree metric achieves the bestperformance among all the metrics.
We can also seethat HWCM and DSTM beat BLEU in most casesand exhibit more stable performance.An example hypothesis which was assigned ahigh score by HWCM but a low score by BLEU isshown in Table 7.
In this particular sentence, thecommon head-modifier relations ?aboard?
plane?and ?plane ?
the?
caused a high headword chainoverlap, but did not appear as common n-gramscounted by BLEU.
The hypothesis is missing theword ?fifth?, but was nonetheless assigned a highscore by human judges.
This is probably due to itsfluency, which HWCM seems to capture better thanBLEU.4 ConclusionThis paper introduces several syntax-based metricsfor the evaluation of MT, which we find to be par-ticularly useful for predicting a hypothesis?s fluency.The syntactic metrics, except the kernel based ones,all outperform BLEU in sentence-level fluency eval-uation.
For the overall evaluation of sentences forfluency and adequacy, the metric based on headwordchain performs better than BLEU in both sentence-level and corpus-level correlation with human judg-ments.
The kernel based metrics, though poor insentence-level evaluation, achieve the best results incorpus-level evaluation, where sparse data are lessof a barrier.Our syntax-based measures require the existenceof a parser for the language in question, however itis worth noting that a parser is required for the tar-get language only, as all our measures of similarityare defined across hypotheses and references in thesame language.Our results, in particular for the primarily struc-tural STM, may be surprising in light of the factthat the parser is not designed to handle ill-formedor ungrammatical sentences such as those producedby machine translation systems.
Modern statisticalparsers have been tuned to discriminate good struc-tures from bad rather than good sentences from bad.31hyp Diplomats will be aboard the plane to return home .ref1 Diplomats are to come back home aboard the fifth plane .ref2 Diplomatic staff would go home in a fifth plane .ref3 Diplomatic staff will take the fifth plane home .Table 7: An example hypothesis in the ACL05-MTE workshop which was assigned a high score by HWCM(0.511) but a low score by BLEU (0.084).
Both human judges assigned a high score (4).Indeed, in some recent work on re-ranking machinetranslation hypotheses (Och et al, 2004), parser-produced structures were not found to provide help-ful information, as a parser is likely to assign a good-looking structure to even a lousy input hypothesis.However, there is an important distinction be-tween the use of parsers in re-ranking and evaluation?
in the present work we are looking for similaritiesbetween pairs of parse trees rather than at featuresof a single tree.
This means that the syntax-basedevaluation measures can succeed even when the treestructure for a poor hypothesis looks reasonable onits own, as long as it is sufficiently distinct from thestructures used in the references.We speculate that by discriminatively trainingweights for the individual subtrees and headwordchains used by the syntax-based metrics, further im-provements in evaluation accuracy are possible.Acknowledgments We are very grateful to AlexKulesza for assistance with the JHU data.
This workwas partially supported by NSF ITR IIS-09325646and NSF ITR IIS-0428020.ReferencesHiyan Alshawi, Srinivas Bangalore, and Shona Douglas.2000.
Learning dependency translation models as col-lections of finite state head transducers.
Computa-tional Linguistics, 26(1):45?60.John Blatz, Erin Fitzgerald, George Foster, Simona Gan-drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,and Nicola Ueffing.
2003.
Confidence estimation formachine translation.
Technical report, Center for Lan-guage and Speech Processing, Johns Hopkins Univer-sity, Baltimore.
Summer Workshop Final Report.Eugene Charniak, Kevin Knight, and Kenji Yamada.2003.
Syntax-based language models for machinetranslation.
In Proc.
MT Summit IX.Michael Collins and Nigel Duffy.
2001.
Convolutionkernels for natural language.
In Advances in NeuralInformation Processing Systems.Michael John Collins.
1999.
Head-driven StatisticalModels for Natural Language Parsing.
Ph.D. thesis,University of Pennsylvania, Philadelphia.G.
Doddington.
2002.
Automatic evaluation of machinetranslation quality using n-gram co-occurrence statis-tics.
In In HLT 2002, Human Language TechnologyConference, San Diego, CA.Heidi J.
Fox.
2002.
Phrasal cohesion and statistical ma-chine translation.
In In Proceedings of the 2002 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP 2002), pages 304?311.Alex Kulesza and Stuart M. Shieber.
2004.
A learningapproach to improving sentence-level MT evaluation.In Proceedings of the 10th International Conferenceon Theoretical and Methodological Issues in MachineTranslation (TMI), Baltimore, MD, October.Dekang Lin.
2004.
A path-based transfer model formachine translation.
In Proceedings of the 20th In-ternational Conference on Computational Linguistics(COLING-04), pages 625?630, Geneva, Switzerland.Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,Anoop Sarkar, Kenji Yamada, Alex Fraser, ShankarKumar, Libin Shen, David Smith, Katherine Eng,Viren Jain, Zhen Jin, and Dragomir Radev.
2004.
Asmorgasbord of features for statistical machine trans-lation.
In Proceedings of the 2004 Meeting of theNorth American chapter of the Association for Com-putational Linguistics (NAACL-04), Boston.Franz Josef Och.
2003.
Minimum error rate training forstatistical machine translation.
In Proceedings of the41th Annual Conference of the Association for Com-putational Linguistics (ACL-03).Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Conference of the Association for Com-putational Linguistics (ACL-02), Philadelphia, PA.32
