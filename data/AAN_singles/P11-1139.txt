Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1385?1394,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsA Stacked Sub-Word Model for Joint Chinese Word Segmentation andPart-of-Speech TaggingWeiwei SunDepartment of Computational Linguistics, Saarland UniversityGerman Research Center for Artificial Intelligence (DFKI)D-66123, Saarbru?cken, Germanywsun@coli.uni-saarland.deAbstractThe large combined search space of joint wordsegmentation and Part-of-Speech (POS) tag-ging makes efficient decoding very hard.
As aresult, effective high order features represent-ing rich contexts are inconvenient to use.
Inthis work, we propose a novel stacked sub-word model for this task, concerning both ef-ficiency and effectiveness.
Our solution isa two step process.
First, one word-basedsegmenter, one character-based segmenter andone local character classifier are trained to pro-duce coarse segmentation and POS informa-tion.
Second, the outputs of the three pre-dictors are merged into sub-word sequences,which are further bracketed and labeled withPOS tags by a fine-grained sub-word tag-ger.
The coarse-to-fine search scheme is effi-cient, while in the sub-word tagging step richcontextual features can be approximately de-rived.
Evaluation on the Penn Chinese Tree-bank shows that our model yields improve-ments over the best system reported in the lit-erature.1 IntroductionWord segmentation and part-of-speech (POS) tag-ging are necessary initial steps for more advancedChinese language processing tasks, such as pars-ing and semantic role labeling.
Joint approachesthat resolve the two tasks simultaneously have re-ceived much attention in recent research.
Previouswork has shown that joint solutions led to accu-racy improvements over pipelined systems by avoid-ing segmentation error propagation and exploitingPOS information to help segmentation.
A challengefor joint approaches is the large combined searchspace, which makes efficient decoding and struc-tured learning of parameters very hard.
Moreover,the representation ability of models is limited sinceusing rich contextual word features makes the searchintractable.
To overcome such efficiency and effec-tiveness limitations, the approximate inference andreranking techniques have been explored in previouswork (Zhang and Clark, 2010; Jiang et al, 2008b).In this paper, we present an effective and effi-cient solution for joint Chinese word segmentationand POS tagging.
Our work is motivated by severalcharacteristics of this problem.
First of all, a major-ity of words are easy to identify in the segmentationproblem.
For example, a simple maximum match-ing segmenter can achieve an f-score of about 90.We will show that it is possible to improve the ef-ficiency and accuracy by using different strategiesfor different words.
Second, segmenters designedwith different views have complementary strength.We argue that the agreements and disagreements ofdifferent solvers can be used to construct an inter-mediate sub-word structure for joint segmentationand tagging.
Since the sub-words are large enoughin practice, the decoding for POS tagging over sub-words is efficient.
Finally, the Chinese language ischaracterized by the lack of morphology that oftenprovides important clues for POS tagging, and thePOS tags contain much syntactic information, whichneed context information within a large window fordisambiguation.
For example, Huang et al (2007)showed the effectiveness of utilizing syntactic infor-mation to rerank POS tagging results.
As a result,the capability to represent rich contextual featuresis crucial to a POS tagger.
In this work, we usea representation-efficiency tradeoff through stackedlearning, a way of approximating rich non-local fea-1385tures.This paper describes a novel stacked sub-wordmodel.
Given multiple word segmentations of onesentence, we formally define a sub-word structurethat maximizes the agreement of non-word-breakpositions.
Based on the sub-word structure, jointword segmentation and POS tagging is addressed asa two step process.
In the first step, one word-basedsegmenter, one character-based segmenter and onelocal character classifier are used to produce coarsesegmentation and POS information.
The results ofthe three predictors are then merged into sub-wordsequences, which are further bracketed and labeledwith POS tags by a fine-grained sub-word tagger.
Ifa string is consistently segmented as a word by thethree segmenters, it will be a correct word predictionwith a very high probability.
In the sub-word tag-ging phase, the fine-grained tagger mainly considersits POS tag prediction problem.
For the words thatare not consistently predicted, the fine-grained tag-ger will also consider their bracketing problem.
Thecoarse-to-fine scheme significantly improves the ef-ficiency of decoding.
Furthermore, in the sub-wordtagging step, word features in a large window can beapproximately derived from the coarse segmentationand tagging results.
To train a good sub-word tagger,we use the stacked learning technique, which can ef-fectively correct the training/test mismatch problem.We conduct our experiments on the Penn ChineseTreebank and compare our system with the state-of-the-art systems.
We present encouraging results.Our system achieves an f-score of 98.17 for the wordsegmentation task and an f-score of 94.02 for thewhole task, resulting in relative error reductions of14.1% and 5.5% respectively over the best systemreported in the literature.The remaining part of the paper is organized asfollows.
Section 2 gives a brief introduction to theproblem and reviews the relevant previous research.Section 3 describes the details of our method.
Sec-tion 4 presents experimental results and empiricalanalyses.
Section 5 concludes the paper.2 Background2.1 Problem DefinitionGiven a sequence of characters c = (c1, ..., c#c),the task of word segmentation and POS tagging isto predict a sequence of word and POS tag pairsy = (?w1, p1?, ?w#y, p#y?
), where wi is a word, piis its POS tag, and a ?#?
symbol denotes the numberof elements in each variable.
In order to avoid errorpropagation and make use of POS information forword segmentation, the two tasks should resolvedjointly.
Previous research has shown that the inte-grated methods outperformed pipelined systems (Ngand Low, 2004; Jiang et al, 2008a; Zhang and Clark,2008).2.2 Character-Based and Word-BasedMethodsTwo kinds of approaches are popular for joint wordsegmentation and POS tagging.
The first is the?character-based?
approach, where basic process-ing units are characters which compose words.
Inthis kind of approach, the task is formulated asthe classification of characters into POS tags withboundary information.
Both the IOB2 representa-tion (Ramshaw and Marcus, 1995) and the Start/Endrepresentation (Kudo and Matsumoto, 2001) arepopular.
For example, the label B-NN indicates thata character is located at the begging of a noun.
Usingthis method, POS information is allowed to inter-act with segmentation.
Note that word segmentationcan also be formulated as a sequential classificationproblem to predict whether a character is located atthe beginning of, inside or at the end of a word.
Thischaracter-by-character method for segmentation wasfirst proposed in (Xue, 2003), and was then furtherused in POS tagging in (Ng and Low, 2004).
Onemain disadvantage of this model is the difficulty inincorporating the whole word information.The second kind of solution is the ?word-based?method, where the basic predicting units are wordsthemselves.
This kind of solver sequentially decideswhether the local sequence of characters makes upa word as well as its possible POS tag.
In partic-ular, a word-based solver reads the input sentencefrom left to right, predicts whether the current pieceof continuous characters is a word token and whichclass it belongs to.
Solvers may use previously pre-dicted words and their POS information as clues tofind a new word.
After one word is found and classi-fied, solvers move on and search for the next possi-ble word.
This word-by-word method for segmenta-tion was first proposed in (Zhang and Clark, 2007),1386and was then further used in POS tagging in (Zhangand Clark, 2008).In our previous work(Sun, 2010), we presenteda theoretical and empirical comparative analysis ofcharacter-based and word-based methods for Chi-nese word segmentation.
We showed that the twomethods produced different distributions of segmen-tation errors in a way that could be explained bytheoretical properties of the two models.
A systemcombination method that leverages the complemen-tary strength of word-based and character-based seg-mentation models was also successfully explored intheir work.
Different from our previous focus, thediversity of different models designed with differentviews is utilized to construct sub-word structures inthis work.
We will discuss the details in the nextsection.2.3 Stacked LearningStacked generalization is a meta-learning algorithmthat was first proposed in (Wolpert, 1992) and(Breiman, 1996).
The idea is to include two ?levels?of predictors.
The first level includes one or morepredictors g1, ...gK : Rd ?
R; each receives inputx ?
Rd and outputs a prediction gk(x).
The secondlevel consists of a single function h : Rd+K ?
Rthat takes as input ?x, g1(x), ..., gK(x)?
and outputsa final prediction y?
= h(x, g1(x), ..., gK(x)).Training is done as follows.
The training data S ={(xt,yt) : t ?
[1, T ]} is split into L equal-sized dis-joint subsets S1, ..., SL.
Then functions g1, ...,gL(where gl = ?gl1, ..., glK?)
are seperately trained onS ?
Sl, and are used to construct the augmenteddataset S?
= {(?xt, y?1t , ..., y?Kt ?,yt) : y?kt = glk(xt)and xt ?
Sl}.
Finally, each gk is trained on the origi-nal dataset and the second level predictor h is trainedon S?.
The intent of the cross-validation scheme isthat ykt is similar to the prediction produced by apredictor which is learned on a sample that does notinclude xt.Stacked learning has been applied as a system en-semble method in several NLP tasks, such as namedentity recognition (Wu et al, 2003) and dependencyparsing (Nivre and McDonald, 2008).
This frame-work is also explored as a solution for learning non-local features in (Torres Martins et al, 2008).
Inthe machine learning research, stacked learning hasbeen applied to structured prediction (Cohen andCarvalho, 2005).
In this work, stacked learning isused to acquire extended training data for sub-wordtagging.3 Method3.1 ArchitectureIn our stacked sub-word model, joint word segmen-tation and POS tagging is decomposed into twosteps: (1) coarse-grained word segmentation andtagging, and (2) fine-grained sub-word tagging.
Theworkflow is shown in Figure 1.
In the first phase, oneword-based segmenter (SegW) and one character-based segmenter (SegC) are trained to produce wordboundaries.
Additionally, a local character-basedjoint segmentation and tagging solver (SegTagL) isused to provide word boundaries as well as inaccu-rate POS information.
Here, the word local meansthe labels of nearby characters are not used as fea-tures.
In other words, the local character classi-fier assumes that the tags of characters are indepen-dent of each other.
In the second phase, our systemfirst combines the three segmentation and taggingresults to get sub-words which maximize the agree-ment about word boundaries.
Finally, a fine-grainedsub-word tagger (SubTag) is applied to bracket sub-words into words and also to obtain their POS tags.Raw sentencesCharacter-basedsegmenter SegCLocal characterclassifierSegTagLWord-basedSegmenter SegWSegmentedsentencesSegmentedsentencesSegmentedsentencesMergingSub-wordsequencesSub-word tag-ger SubTagFigure 1: Workflow of the stacked sub-word model.In our model, segmentation and POS tagging in-teract with each other in two processes.
First, al-though SegTagL is locally trained, it resolves the1387two sub-tasks simultaneously.
Therefore, in the sub-word generating stage, segmentation and POS tag-ging help each other.
Second, in the sub-word tag-ging stage, the bracketing and the classification ofsub-words are jointly resolved as one sequence la-beling problem.Our experiments on the Penn Chinese Treebankwill show that the word-based and character-basedsegmenters and the local tagger on their own pro-duce high quality word boundaries.
As a result, theoracle performance to recover words from a sub-word sequence is very high.
The quality of the fi-nal tagger relies on the quality of the sub-word tag-ger.
If a high performance sub-word tagger can beconstructed, the whole task can be well resolved.The statistics will also empirically show that sub-words are significantly larger than characters andonly slightly smaller than words.
As a result, thesearch space of the sub-word tagging is significantlyshrunken, and exact Viterbi decoding without ap-proximately pruning can be efficiently processed.This property makes nearly all popular sequence la-beling algorithms applicable.Zhang et al (2006) described a sub-word basedtagging model to resolve word segmentation.
Toget the pieces which are larger than characters butsmaller than words, they combine a character-basedsegmenter and a dictionary matching segmenter.Our contributions include (1) providing a formaldefinition of our sub-word structure that is based onmultiple segmentations and (2) proposing a stackingmethod to acquire sub-words.3.2 The Coarse-grained SolversWe systematically described the implementation oftwo state-of-the-art Chinese word segmenters inword-based and character-based architectures, re-spectively (Sun, 2010).
Our word-based segmenteris based on a discriminative joint model with afirst order semi-Markov structure, and the other seg-menter is based on a first order Markov model.
Ex-act Viterbi-style search algorithms are used for de-coding.
Limited to the document length, we do notgive the description of the features.
We refer readersto read the above paper for details.
For parameterestimation, our work adopt the Passive-Aggressive(PA) framework (Crammer et al, 2006), a familyof margin based online learning algorithms.
In thiswork, we introduce two simple but important refine-ments: (1) to shuffle the sample orders in each itera-tion and (2) to average the parameters in each itera-tion as the final parameters.Idiom In linguistics, idioms are usually presumedto be figures of speech contradicting the principleof compositionality.
As a result, it is very hard torecognize out-of-vocabulary idioms for word seg-mentation.
However, the lexicon of idioms can betaken as a close set, which helps resolve the problemwell.
We collect 12992 idioms1 from several on-line Chinese dictionaries.
For both word-based andcharacter-based segmentation, we first match everystring of a given sentence with idioms.
Every sen-tence is then splitted into smaller pieces which areseperated by idioms.
Statistical segmentation mod-els are then performed on these smaller character se-quences.We use a local classifier to predict the POStag with positional information for each character.Each character can be assigned one of two possi-ble boundary tags: ?B?
for a character that begins aword and ?I?
for a character that occurs in the mid-dle of a word.
We denote a candidate character to-ken ci with a fixed window ci?2ci?1cici+1ci+2.
Thefollowing features are used:?
character uni-grams: ck (i?
2 ?
k ?
i+ 2)?
character bi-grams: ckck+1 (i?
2 ?
k ?
i+1)To resolve the classification problem, we use the lin-ear SVM classifier LIBLINEAR2.3.3 Merging Multiple Segmentation Resultsinto Sub-Word SequencesA majority of words are easy to identify in the seg-mentation problem.
We favor the idea treating dif-ferent words using different strategies.
In this workwe try to identify simple and difficult words first andto integrate them into a sub-word level.
Inspired byprevious work, we constructed this sub-word struc-ture by using multiple solvers designed from differ-ent views.
If a piece of continuous characters is con-sistently segmented by multiple segmenters, it will1This resource is publicly available at http://www.coli.uni-saarland.de/?wsun/idioms.txt.2Available at http://www.csie.ntu.edu.tw/?cjlin/liblinear/.1388?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?Answer: [P] [JJ] [ NN ] [ CD ] [M] [VV] [ JJ ] [ NN ]SegW: [] [] [ ] [ ] [ ] [ ] [ ] [ ]SegC: [] [] [ ] [ ] [] [ ] [ ]SegTagL: [P] [JJ] [ NN ] [ CD ] [NT] [CD] [NT] [VV] [ VV ] [ NN ]Sub-words: [P] [JJ] [ NN ] [ B-CD ] [I-CD] [NT] [CD] [NT] [VV] [ VV ] [ NN ]Figure 2: An example phrase: ????????????????
(Being in front with a total score of 355.35points).not be separated in the sub-word tagging step.
Theintuition is that strings which are consistently seg-mented by the different segmenters tend to be cor-rect predictions.
In our experiment on the Penn Chi-nese Treebank (Xue et al, 2005), the accuracy is98.59% on the development data which is definedin the next section.
The key point for the interme-diate sub-word structures is to maximize the agree-ment of the three coarse-grained systems.
In otherwords, the goal is to make merged sub-words aslarge as possible but not overlap with any predictedword produced by the three coarse-grained solvers.In particular, if the position between two continu-ous characters is predicted as a word boundary byany segmenter, this position is taken as a separationposition of the sub-word sequence.
This strategymakes sure that it is still possible to re-segment thestrings of which the boundaries are disagreed withby the coarse-grained segmenters in the fine-grainedtagging stage.The formal definition is as follows.
Given a se-quence of characters c = (c1, ..., c#c), let c[i : j]denote a string that is made up of characters betweenci and cj (including ci and cj), then a partition ofthe sentence can be written as c[0 : e1], c[e1 + 1 :e2], ..., c[em : #c].
Let sk = {c[i : j]} denote theset of all segments of a partition.
Given multiplepartitions of a character sequence S = {sk}, thereis one and only one merged partition sS = {c[i : j]}s.t.1.
?c[i : j] ?
sS ,?sk ?
S, ?c[s : e] ?
sk, s ?i ?
j ?
e.2.
?C?
satisfies the above condition, |C?| > |C|.The first condition makes sure that all segments inthe merged partition can be only embedded in but donot overlap with any segment of any partition fromS.
The second condition promises that segments ofthe merged partition achieve maximum length.Figure 2 is an example to illustrate the proce-dure of our method.
The lines SegW, SegC andSegTagL are the predictions of the three coarse-grained solvers.
For the three words at the begin-ning and the two words at the end, the three predic-tors agree with each other.
And these five words arekept as sub-words.
For the character sequence ?????????
?, the predictions are very differ-ent.
Because there are no word break predictionsamong the first three characters ????
?, it is asa whole taken as one sub-word.
For the other fivecharacters, either the left position or the right po-sition is segmented as a word break by some pre-dictor, so the merging processor seperates them andtakes each one as a single sub-word.
The last lineshows the merged sub-word sequence.
The coarse-grained POS tags with positional information are de-rived from the labels provided by SegTagL.3.4 The Fine-grained Sub-Word TaggerBracketing sub-words into words is formulated asa IOB-style sequential classification problem.
Eachsub-word may be assigned with one POS tag as wellas two possible boundary tags: ?B?
for the begin-ning position and ?I?
for the middle position.
Atagger is trained to classify sub-word by using thefeatures derived from its contexts.The sub-word level allows our system to utilizefeatures in a large context, which is very importantfor POS tagging of the morphologically poor lan-guage.
Features are formed making use of sub-wordcontents, their IOB-style inaccurate POS tags.
Inthe following description, ?C?
refers to the contentof the sub-word, while ?T?
refers to the IOB-stylePOS tags.
For convenience, we denote a sub-wordwith its context ...si?2si?1sisi+1si+2..., where si is1389C(si?1)=????
; T(si?1)=?NN?C(si)=?????
; T(si)=?B-CD?C(si+1)=???
; T(si+1)=?I-CD?C(si?1)C(si)=???
???
?T(si?1)T(si)=?NN B-CD?C(si)C(si+1)=????
?
?T(si)T(si+1)=?B-CD I-CD?C(si?1)C(si+1)=???
?
?T(si?1)T(si+1)=?B-NN I-CD?Prefix(1)=???
; Prefix(2)=????
; Prefix(3)=?????Suffix(1)=???
; Suffix(2)=????
; Suffix(3)=????
?Table 1: An example of features used in the sub-wordtagging.the current token.
We denote lC , lT as the sizes ofthe window.?
Uni-gram features: C(sk) (?lC ?
k ?
lC),T(sk) (?lT ?
k ?
lT )?
Bi-gram features: C(sk)C(sk+1) (?lC ?
k ?lC ?
1), T(sk)T(sk+1) (?lT ?
k ?
lT ?
1)?
C(si?1)C(si+1) (if lC ?
1), T(si?1)T(si+1) (iflT ?
1)?
T(si?2)T(si+1) (if lT ?
2)?
In order to better handle unknown words, wealso extract morphological features: charactern-gram prefixes and suffixes for n up to 3.These features have been shown useful in pre-vious research (Huang et al, 2007).Take the sub-word ?????
in Figure 2 for ex-ample, when lC and lT are both set to 1, all featuresused are listed in Table 1.In the following experiments, we will vary win-dow sizes lC and lT to find out the contribution ofcontext information for the disambiguation.
A firstorder Max-Margin Markov Networks model is usedto resolve the sequence tagging problem.
We use theSVM-HMM3 implementation for the experiments inthis work.
We use the basic linear model withoutapplying any kernel function.3Available at http://www.cs.cornell.edu/People/tj/svm_light/svm_hmm.html.Algorithm 1: The stacked learning procedurefor the sub-word tagger.input : Data S = {(ct,yt), t = 1, 2, ..., n}Split S into L partitions {S1, ...SL}for l = 1, ..., L doTrain SegWl, SegCl and SegTagLl usingS ?
Sl.Predict Sl using SegWl, SegCl andSegTagLl.Merge the predictions to get sub-wordstraining sample S?l .endTrain the sub-word tagger SubTag using S?.3.5 Stacked Learning for the Sub-Word TaggerThe three coarse-grained solvers SegW, SegC andSegTagL are directly trained on the original train-ing data.
When these three predictors are used toproduce the training data, the performance is per-fect.
However, this does not hold when these mod-els are applied to the test data.
If we directly applySegW, SegC and SegTagL to extend the training datato generate sub-word samples, the extended trainingdata for the sub-word tagger will be very differentfrom the data in the run time, resulting in poor per-formance.One way to correct the training/test mismatch isto use the stacking method, where a K-fold cross-validation on the original data is performed to con-struct the training data for sub-word tagging.
Algo-rithm 1 illustrates the learning procedure.
First, thetraining data S = {(ct,yt)} is split into L equal-sized disjoint subsets S1, ..., SL.
For each subset Sl,the complementary set S ?
Sl is used to train threecoarse solvers SegWl, SegCl and SegTagLl, whichprocess the Sl and provide inaccurate predictions.Then the inaccurate predictions are merged into sub-word sequences and Sl is extended to S?l .
Finally,the sub-word tagger is trained on the whole extendeddata set S?.4 Experiments4.1 SettingPrevious studies on joint Chinese word segmenta-tion and POS tagging have used the Penn ChineseTreebank (CTB) in experiments.
We follow this set-1390ting in this paper.
We use CTB 5.0 as our maincorpus and define the training, development and testsets according to (Jiang et al, 2008a; Jiang et al,2008b; Kruengkrai et al, 2009; Zhang and Clark,2010).
Table 2 shows the statistics of our experi-mental settings.Data set CTB files # of sent.
# of wordsTraining 1-270 18,089 493,939400-9311001-1151Devel.
301-325 350 6821Test 271-300 348 8008Table 2: Training, development and test data on CTB 5.0Three metrics are used for evaluation: precision(P), recall (R) and balanced f-score (F) defined by2PR/(P+R).
Precision is the relative amount of cor-rect words in the system output.
Recall is the rela-tive amount of correct words compared to the goldstandard annotations.
For segmentation, a token isconsidered to be correct if its boundaries match theboundaries of a word in the gold standard.
For thewhole task, both the boundaries and the POS taghave to be correctly identified.4.2 Performance of the Coarse-grained SolversTable 3 shows the performance on the developmentdata set of the three coarse-grained solvers.
In thispaper, we use 20 iterations to train SegW and SegCfor all experiments.
Even only locally trained, thecharacter classifier SegTagL still significantly out-performs the two state-of-the-art segmenters SegWand SegC.
This good performance indicates that thePOS information is very important for word segmen-tation.Devel.
Task P(%) R(%) FSegW Seg 94.55 94.84 94.69SegC Seg 95.10 94.38 94.73SegTagL Seg 95.67 95.98 95.83Seg&Tag 87.54 91.29 89.38Table 3: Performance of the coarse-grained solvers on thedevelopment data.4.3 Statistics of Sub-WordsSince the base predictors to generate coarse infor-mation are two word segmenters and a local charac-ter classifier, the coarse decoding is efficient.
If thelength of sub-words is too short, i.e.
the decodingpath for sub-word sequences are too long, the decod-ing of the fine-grained stage is still hard.
Althoughwe cannot give a theoretical average length of sub-words, we can still show the empirical one.
The av-erage length of sub-words on the development set is1.64, while the average length of words is 1.69.
Thenumber of all IOB-style POS tags is 59 (when using5-fold cross-validation to generate stacked trainingsamples).
The number of all POS tags is 35.
Empir-ically, the decoding over sub-words is 1.691.64?
(5935)n+1times as slow as the decoding over words, where nis the order of the markov model.
When a first ordermarkov model is used, this number is 2.93.
Thesestatistics empirically suggest that the decoding oversub-word sequence can be efficient.On the other hand, the sub-word sequences arenot perfect in the sense that they do not promiseto recover all words because of the errors made inthe first step.
Similarly, we can only show the em-pirical upper bound of the sub-word tagging.
Theoracle performance of the final POS tagging on thedevelopment data set is shown in Table 4.
The up-per bound indicates that the coarse search proceduredoes not lose too much.Task P(%) R(%) FSeg&Tag 99.50% 99.09% 99.29Table 4: Upper bound of the sub-word tagging on thedevelopment data.One main disadvantage of character-based ap-proach is the difficulty to incorporate word features.Since the sub-words are on average close to words,sub-word features are good approximations of wordfeatures.4.4 Rich Contextual Features Are UsefulTable 5 shows the effect that features within differ-ent window size has on the sub-word tagging task.In this table, the symbol ?C?
means sub-word con-tent features while the symbol ?T?
means IOB-stylePOS tag features.
The number indicates the length1391Devel.
P(%) R(%) FC:?0 T:?0 92.52 92.83 92.67C:?1 T:?0 92.63 93.27 92.95C:?1 T:?1 92.62 93.05 92.83C:?2 T:?0 93.17 93.86 93.51C:?2 T:?1 93.27 93.64 93.45C:?2 T:?2 93.08 93.61 93.34C:?3 T:?0 93.12 93.86 93.49C:?3 T:?1 93.34 93.96 93.65C:?3 T:?2 93.34 93.96 93.65Table 5: Performance of the stacked sub-word model(K = 5) with features in different window sizes.of the window.
For example, ?C:?1?
means that thetagger uses one preceding sub-word and one suc-ceeding sub-word as features.
From this table, wecan clearly see the impact of features derived fromneighboring sub-words.
There is a significant in-crease between ?C:?2?
and ?C:?1?
models.
Thisconfirms our motivation that longer history and fu-ture features are crucial to the Chinese POS taggingproblem.
It is the main advantage of our model thatmaking rich contextual features applicable.
In allprevious solutions, only features within a short his-tory can be used due to the efficiency limitation.The performance is further slightly improvedwhen the window size is increased to 3.
Using thelabeled bracketing f-score, the evaluation shows thatthe ?C:?3 T:?1?
model performs the same as the?C:?3 T:?2?
model.
However, the sub-word clas-sification accuracy of the ?C:?3 T:?1?
model ishigher, so in the following experiments and the fi-nal results reported on the test data set, we choosethis setting.This table also suggests that the IOB-style POSinformation of sub-words does not contribute.
Wethink there are two main reasons: (1) The POS infor-mation provided by the local classifier is inaccurate;(2) The structured learning of the sub-word taggercan use real predicted sub-word labels during its de-coding time, since this learning algorithm does in-ference during the training time.
It is still an openquestion whether more accurate POS information inrich contexts can help this task.
If the answer is YES,how can we efficiently incorporate these features?4.5 Stacked Learning Is UsefulTable 6 compares the performance of ?C:?3 T:?1?models trained with no stacking as well as differ-ent folds of cross-validation.
We can see that al-though it is still possible to improve the segmenta-tion and POS tagging performance compared to thelocal character classifier, the whole task just benefitsonly a little from the sub-word tagging procedure ifthe stacking technique is not applied.
The stackingtechnique can significantly improve the system per-formance, both for segmentation and POS tagging.This experiment confirms the theoretical motivationof using stacked learning: simulating the test-timesetting when a sub-word tagger is applied to a newinstance.
There is not much difference between the5-fold and the 10-fold cross-validation.Devel.
Task P(%) R(%) FNo stacking Seg 95.75 96.48 96.12Seg&Tag 91.42 92.13 91.77K = 5 Seg 96.42 97.04 96.73Seg&Tag 93.34 93.96 93.65K = 10 Seg 96.67 97.11 96.89Seg&Tag 93.50 94.06 93.78Table 6: Performance on the development data.
No stack-ing and different folds of cross-validation are separatelyapplied.4.6 Final ResultsTable 7 summarizes the performance of our finalsystem on the test data and other systems reportedin a majority of previous work.
The final resultsof our system are achieved by using 10-fold cross-validation ?C:?3 T:?1?
models.
The left most col-umn indicates the reference of previous systems thatrepresent state-of-the-art results.
The comparison ofthe accuracy between our stacked sub-word systemand the state-of-the-art systems in the literature in-dicates that our method is competitive with the bestsystems.
Our system obtains the highest f-score per-formance on both segmentation and the whole task,resulting in error reductions of 14.1% and 5.5% re-spectively.1392Test Seg Seg&Tag(Jiang et al, 2008a) 97.85 93.41(Jiang et al, 2008b) 97.74 93.37(Kruengkrai et al, 2009) 97.87 93.67(Zhang and Clark, 2010) 97.78 93.67Our system 98.17 94.02Table 7: F-score performance on the test data.5 Conclusion and Future WorkThis paper has described a stacked sub-word modelfor joint Chinese word segmentation and POS tag-ging.
We defined a sub-word structure which maxi-mizes the agreement of multiple segmentations pro-vided by different segmenters.
We showed that thissub-word structure could explore the complemen-tary strength of different systems designed with dif-ferent views.
Moreover, the POS tagging could beefficiently and effectively resolved over sub-wordsequences.
To train a good sub-word tagger, we in-troduced a stacked learning procedure.
Experimentsshowed that our approach was superior to the exist-ing approaches reported in the literature.Machine learning and statistical approaches en-counter difficulties when the input/output data havea structured and relational form.
Research in em-pirical Natural Language Processing has been tack-ling these complexities since the early work in thefield.
Recent work in machine learning has pro-vided several paradigms to globally represent andprocess such data: linear models for structured pre-diction, graphical models, constrained conditionalmodels, and reranking, among others.
A generalexpressivity-efficiency trade off is observed.
Al-though the stacked sub-word model is an ad hoc so-lution for a particular problem, namely joint wordsegmentation and POS tagging, the idea to em-ploy system ensemble and stacked learning in gen-eral provides an alternative for structured problems.Multiple ?cheap?
coarse systems are used to providediverse outputs, which may be inaccurate.
Theseoutputs are further merged into an intermediate rep-resentation, which allows an extractive system to userich contexts to predict the final results.
A natu-ral avenue for future work is the extension of ourmethod to other NLP tasks.AcknowledgmentsThe work is supported by the project TAKE (Tech-nologies for Advanced Knowledge Extraction),funded under contract 01IW08003 by the GermanFederal Ministry of Education and Research.
Theauthor is also funded by German Academic Ex-change Service (DAAD).The author would would like to thank Dr. JiaXu for her helpful discussion, and Regine Bader forproofreading this paper.ReferencesLeo Breiman.
1996.
Stacked regressions.
Mach.
Learn.,24:49?64, July.William W. Cohen and Vitor R. Carvalho.
2005.
Stackedsequential learning.
In Proceedings of the 19th in-ternational joint conference on Artificial intelligence,pages 671?676, San Francisco, CA, USA.
MorganKaufmann Publishers Inc.Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer.
2006.
Online passive-aggressive algorithms.
JOURNAL OF MACHINELEARNING RESEARCH, 7:551?585.Zhongqiang Huang, Mary Harper, and Wen Wang.2007.
Mandarin part-of-speech tagging and discrim-inative reranking.
In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL), pages 1093?1102,Prague, Czech Republic, June.
Association for Com-putational Linguistics.Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Lu?.2008a.
A cascaded linear model for joint Chineseword segmentation and part-of-speech tagging.
InProceedings of ACL-08: HLT, pages 897?904, Colum-bus, Ohio, June.
Association for Computational Lin-guistics.Wenbin Jiang, Haitao Mi, and Qun Liu.
2008b.
Wordlattice reranking for Chinese word segmentation andpart-of-speech tagging.
In Proceedings of the 22nd In-ternational Conference on Computational Linguistics(Coling 2008), pages 385?392, Manchester, UK, Au-gust.
Coling 2008 Organizing Committee.Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichiKazama, Yiou Wang, Kentaro Torisawa, and HitoshiIsahara.
2009.
An error-driven word-character hybridmodel for joint Chinese word segmentation and postagging.
In Proceedings of the Joint Conference of the47th Annual Meeting of the ACL and the 4th Interna-tional Joint Conference on Natural Language Process-1393ing of the AFNLP, pages 513?521, Suntec, Singapore,August.
Association for Computational Linguistics.Taku Kudo and Yuji Matsumoto.
2001.
Chunking withsupport vector machines.
In NAACL ?01: Secondmeeting of the North American Chapter of the Associa-tion for Computational Linguistics on Language tech-nologies 2001, pages 1?8, Morristown, NJ, USA.
As-sociation for Computational Linguistics.Hwee Tou Ng and Jin Kiat Low.
2004.
Chinese part-of-speech tagging: One-at-a-time or all-at-once?
word-based or character-based?
In Dekang Lin and DekaiWu, editors, Proceedings of EMNLP 2004, pages 277?284, Barcelona, Spain, July.
Association for Computa-tional Linguistics.Joakim Nivre and Ryan McDonald.
2008.
Integratinggraph-based and transition-based dependency parsers.In Proceedings of ACL-08: HLT, pages 950?958,Columbus, Ohio, June.
Association for ComputationalLinguistics.L.
A. Ramshaw and M. P. Marcus.
1995.
Text chunkingusing transformation-based learning.
In Proceedingsof the 3rd ACL/SIGDAT Workshop on Very Large Cor-pora, Cambridge, Massachusetts, USA, pages 82?94.Weiwei Sun.
2010.
Word-based and character-basedword segmentation models: Comparison and combi-nation.
In Coling 2010: Posters, pages 1211?1219,Beijing, China, August.
Coling 2010 Organizing Com-mittee.Andre?
Filipe Torres Martins, Dipanjan Das, Noah A.Smith, and Eric P. Xing.
2008.
Stacking dependencyparsers.
In Proceedings of the 2008 Conference onEmpirical Methods in Natural Language Processing,pages 157?166, Honolulu, Hawaii, October.
Associa-tion for Computational Linguistics.David H. Wolpert.
1992.
Original contribution: Stackedgeneralization.
Neural Netw., 5:241?259, February.Dekai Wu, Grace Ngai, and Marine Carpuat.
2003.
Astacked, voted, stacked model for named entity recog-nition.
In Walter Daelemans and Miles Osborne, ed-itors, Proceedings of the Seventh Conference on Nat-ural Language Learning at HLT-NAACL 2003, pages200?203.Nianwen Xue, Fei Xia, Fu-Dong Chiou, and MarthaPalmer.
2005.
The penn chinese treebank: Phrasestructure annotation of a large corpus.
Natural Lan-guage Engineering, 11(2):207?238.Nianwen Xue.
2003.
Chinese word segmentation ascharacter tagging.
In International Journal of Com-putational Linguistics and Chinese Language Process-ing.Yue Zhang and Stephen Clark.
2007.
Chinese segmenta-tion with a word-based perceptron algorithm.
In Pro-ceedings of the 45th Annual Meeting of the Associationof Computational Linguistics, pages 840?847, Prague,Czech Republic, June.
Association for ComputationalLinguistics.Yue Zhang and Stephen Clark.
2008.
Joint word segmen-tation and POS tagging using a single perceptron.
InProceedings of ACL-08: HLT, pages 888?896, Colum-bus, Ohio, June.
Association for Computational Lin-guistics.Yue Zhang and Stephen Clark.
2010.
A fast decoder forjoint word segmentation and POS-tagging using a sin-gle discriminative model.
In Proceedings of the 2010Conference on Empirical Methods in Natural Lan-guage Processing, pages 843?852, Cambridge, MA,October.
Association for Computational Linguistics.Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumita.2006.
Subword-based tagging by conditional randomfields for Chinese word segmentation.
In Proceedingsof the Human Language Technology Conference ofthe NAACL, Companion Volume: Short Papers, pages193?196, New York City, USA, June.
Association forComputational Linguistics.1394
