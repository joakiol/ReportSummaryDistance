Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1173?1182,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsA Mixture Model with Sharing for Lexical SemanticsJoseph ReisingerDepartment of Computer ScienceUniversity of Texas at Austin1616 Guadalupe, Suite 2.408Austin, TX, 78701joeraii@cs.utexas.eduRaymond MooneyDepartment of Computer ScienceUniversity of Texas at Austin1616 Guadalupe, Suite 2.408Austin, TX, 78701mooney@cs.utexas.eduAbstractWe introduce tiered clustering, a mixturemodel capable of accounting for varying de-grees of shared (context-independent) fea-ture structure, and demonstrate its applicabil-ity to inferring distributed representations ofword meaning.
Common tasks in lexical se-mantics such as word relatedness or selec-tional preference can benefit from modelingsuch structure: Polysemous word usage is of-ten governed by some common backgroundmetaphoric usage (e.g.
the senses of line orrun), and likewise modeling the selectionalpreference of verbs relies on identifying com-monalities shared by their typical arguments.Tiered clustering can also be viewed as a formof soft feature selection, where features that donot contribute meaningfully to the clusteringcan be excluded.
We demonstrate the applica-bility of tiered clustering, highlighting partic-ular cases where modeling shared structure isbeneficial and where it can be detrimental.1 IntroductionWord meaning can be represented as high-dimensional vectors inhabiting a common spacewhose dimensions capture semantic or syntacticproperties of interest (e.g.
Erk and Pado, 2008;Lowe, 2001).
Such vector-space representations ofmeaning induce measures of word similarity that canbe tuned to correlate well with judgements madeby humans.
Previous work has focused on de-signing feature representations and semantic spacesthat capture salient properties of word meaning (e.g.Curran, 2004; Gabrilovich and Markovitch, 2007;Landauer and Dumais, 1997), often leveraging thedistributional hypothesis, i.e.
that similar words ap-pear in similar contexts (Miller and Charles, 1991;Pereira et al, 1993).Since vector-space representations are con-structed at the lexical level, they conflate multipleword meanings into the same vector, e.g.
collaps-ing occurrences of bankinstitution and bankriver.
Meth-ods such as Clustering by Committee (Pantel, 2003)and multi-prototype representations (Reisinger andMooney, 2010) address this issue by perform-ing word-sense disambiguation across word occur-rences, and then building meaning vectors fromthe disambiguated words.
Such approaches canreadily capture the structure of homonymous wordswith several unrelated meanings (e.g.
bat and club),but are not suitable for representing the commonmetaphor structure found in highly polysemouswords such as line or run.In this paper, we introduce tiered clustering, anovel probabilistic model of the shared structureoften neglected in clustering problems.
Tieredclustering performs soft feature selection, allocat-ing features between a Dirichlet Process cluster-ing model and a background model consisting ofa single component.
The background model ac-counts for features commonly shared by all occur-rences (i.e.
context-independent feature variation),while the clustering model accounts for variationin word usage (i.e.
context-dependent variation, orword senses; Table 1).Using the tiered clustering model, we derive amulti-prototype representation capable of capturingvarying degrees of sharing between word senses,and demonstrate its effectiveness in lexical seman-tic tasks where such sharing is desirable.
In partic-ular we show that tiered clustering outperforms themulti-prototype approach for (1) selectional prefer-ence (Resnik, 1997; Pantel et al, 2007), i.e.
predict-1173ing the typical filler of an argument slot of a verb,and (2) word-relatedness in the presence of highlypolysemous words.
The former case exhibits a highdegree of explicit structure, especially for more se-lectionally restrictive verbs (e.g.
the set of things thatcan be eaten or can shoot).The remainder of the paper is organized as fol-lows: Section 2 gives relevant background on themethods compared, Section 3 outlines the multi-prototype model based on the Dirichlet Process mix-ture model, Section 4 derives the tiered cluster-ing model, Section 5 discusses similarity metrics,Section 6 details the experimental setup and in-cludes a micro-analysis of feature selection, Section7 presents results applying tiered clustering to wordrelatedness and selectional preference, Section 8 dis-cusses future work, and Section 9 concludes.2 BackgroundModels of the attributional similarity of concepts,i.e.
the degree to which concepts overlap based ontheir attributes (Turney, 2006), are commonly imple-mented using vector-spaces derived from (1) wordcollocations (Schu?tze, 1998), directly leveraging thedistributional hypothesis (Miller and Charles, 1991),(2) syntactic relations (Pado?
and Lapata, 2007), (3)structured corpora (e.g.
Gabrilovich and Markovitch(2007)) or (4) latent semantic spaces (Finkelsteinet al, 2001; Landauer and Dumais, 1997).
Suchmodels can be evaluated based on their correlationwith human-reported lexical similarity judgementsusing e.g.
the WordSim-353 collection (Finkelsteinet al, 2001).
Distributional methods exhibit a highdegree of scalability (Gorman and Curran, 2006) andhave been applied broadly in information retrieval(Manning et al, 2008), large-scale taxonomy induc-tion (Snow et al, 2006), and knowledge acquisition(Van Durme and Pas?ca, 2008).Reisinger and Mooney (2010) introduced a multi-prototype approach to vector-space lexical seman-tics where individual words are represented as col-lections of ?prototype?
vectors.
This representationis capable of accounting for homonymy and poly-semy, as well as other forms of variation in wordusage, like similar context-dependent methods (Erkand Pado, 2008).
The set of vectors for a wordis determined by unsupervised word sense discov-ery (Schu?tze, 1998), which clusters the contexts inwhich a word appears.
Average prototype vectorsLIFEall, about, life, would, deathmy, you, real, your, aboutspent, years, rest, lived, lastsentenced, imprisonment, sentence, prisoninsurance, peer, Baron, member, companyGuru, Rabbi, Baba, la, teachingsRADIOstation, radio, stations, televisionamateur, frequency, waves, systemshow, host, personality, Americansong, single, released, airplayoperator, contact, communications, messageWIZARDevil, powerful, magic, wizardMerlin, King, Arthur, Arthurianfairy, wicked, scene, taleHarry, Potter, Voldemort, DumbledoreSTOCKstock, all, other, company, newmarket, crash, markets, price, priceshousing, breeding, fish, water, horsescar, racing, cars, NASCAR, race, enginecard, cards, player, pile, game, paperrolling, locomotives, line, new, railwayTable 1: Example tiered clustering representation ofwords with varying degrees of polysemy.
Each boxedset shows the most common background (shared) fea-tures, and each prototype captures one thematic usageof the word.
For example, wizard is broken up into abackground cluster describing features common to all us-ages of the word (e.g., magic and evil) and several genre-specific usages (e.g.
Merlin, fairy tales and Harry Potter).are then computed separately for each cluster, pro-ducing a distributed representation for each word.Distributional methods have also proven to be apowerful approach to modeling selectional prefer-ence (Pado?
et al, 2007; Pantel et al, 2007), rivalingmethods based on existing semantic resources suchas WordNet (Clark and Weir, 2002; Resnik, 1997)and FrameNet (Pado?, 2007) and performing nearlyas well as supervised methods (Herdag?delen and Ba-roni, 2009).
Selectional preference has been shownto be useful for, e.g., resolving ambiguous attach-ments (Hindle and Rooth, 1991), word sense disam-biguation (McCarthy and Carroll, 2003) and seman-tic role labeling (Gildea and Jurafsky, 2002).3 Multi-Prototype ModelsRepresenting words as mixtures over several pro-totypes has proven to be a powerful approach to1174vector-space lexical semantics (Pantel, 2003; Pantelet al, 2007; Reisinger and Mooney, 2010).
In thissection we briefly introduce a version of the multi-prototype model based on the Dirichlet Process Mix-ture Model (DPMM), capable of inferring automat-ically the number of prototypes necessary for eachword (Rasmussen, 2000).
Similarity between twoDPMM word-representations is then computed as afunction of their cluster centroids (?5), instead of thecentroid of all the word?s occurrences.Multiple prototypes for each word w are gener-ated by clustering feature vectors vpcq derived fromeach occurrence c P Cpwq in a large textual cor-pus and collecting the resulting cluster centroidspikpwq, k P r1,Kws.
This approach is commonlyemployed in unsupervised word sense discovery;however, we do not assume that clusters correspondto word senses.
Rather, we only rely on clusters tocapture meaningful variation in word usage.Instead of assuming all words can be repre-sented by the same number of clusters, we allocaterepresentational flexibility dynamically using theDPMM.
The DPMM is an infinite capacity modelcapable of assigning data to a variable, but finitenumber of clusters Kw, with probability of assign-ment to cluster k proportional to the number of datapoints previously assigned to k. A single parameter?
controls the degree of smoothing, producing moreuniform clusterings as ?
?
8.
Using this model,the number of clusters no longer needs to be fixeda priori, allowing the model to allocate expressivitydynamically to concepts with richer structure.
Sucha model naturally allows the word representation toallocate additional capacity for highly polysemouswords, with the number of clusters growing loga-rithmically with the number of occurrences.
TheDPMM has been used for rational models of con-cept organization (Sanborn et al, 2006), but to ourknowledge has not yet been applied directly to lexi-cal semantics.4 Tiered ClusteringTiered clustering allocates features between twosubmodels: a (context-dependent) DPMM and a sin-gle (context-independent) background component.This model is similar structurally to the feature se-lective clustering model proposed by Law et al(2002).
However, instead of allocating entire featuredimensions between model and background compo-?z?D ww?!?background!c??
?clustersdFigure 1: Plate diagram for the tiered clustering modelwith cluster indicators drawn from the Chinese Restau-rant Process.nents, assignment is done at the level of individualfeature occurrences, much like topic assignment inLatent Dirichlet Allocation (LDA; Griffiths et al,2007).
At a high level, the tiered model can beviewed as a combination of a multi-prototype modeland a single-prototype back-off model.
However,by leveraging both representations in a joint frame-work, uninformative features can be removed fromthe clustering, resulting in more semantically tightclusters.Concretely, each word occurrence wd first selectsa cluster ?d from the DPMM; then each feature wi,dis generated from either the background model?backor the selected cluster ?d, determined by the tierindicator zi,d.
The full generative model for tieredclustering is given by?d|?
 Betap?q d P D,?d|?, G0  DPp?, G0q d P D,?back|?back  Dirichletp?backqzi,d|?d  Bernoullip?dq i P |wd|,wi,d|?d, zi,d $''&''%Multp?backqpzi,d  1qMultp?dqpotherwiseqi P |wd|,where ?
controls the per-data tier distributionsmoothing and ?
controls the uniformity of the DPcluster allocation.
The DP is parameterized by abase measure G0, controlling the per-cluster termdistribution smoothing; which use a Dirichlet withhyperparameter ?, as is common (Figure 1).Since the background topic is shared across all oc-currences, it can account for features with context-independent variance, such as stop words and otherhigh-frequency noise, as well as the central tendencyof the collection (Table 1).
Furthermore, it is possi-ble to put an asymmetric prior on ?, yielding morefine-grained control over the assumed uniformity ofthe occurrence of noisy features, unlike in the modelproposed by Law et al (2002).1175Although exact posterior inference is intractablein this model, we derive an efficient collapsed Gibbssampler via analogy to LDA (Appendix 1).5 Measuring Semantic SimilarityDue to its richer representational structure, comput-ing similarity in the multi-prototype model is lessstraightforward than in the single prototype case.Reisinger and Mooney (2010) found that simply av-eraging all similarity scores over all pairs of proto-types (sampled from the cluster distributions) per-forms reasonably well and is robust to noise.
Giventwo words w and w1, this AvgSim metric isAvgSimpw,w1q def1KwKw1Kw?j1Kw1?k1dppikpwq, pijpw1qqKw andKw1 are the number of clusters for w and w1respectively, and dp, q is a standard distributionalsimilarity measure (e.g.
cosine distance).
As clustersizes become more uniform, AvgSim tends towardsthe single prototype similarity,1 hence the effective-ness of AvgSim stems from boosting the influenceof small clusters.Tiered clustering representations offer more pos-sibilities for computing semantic similarity thanmulti-prototype, as the background prototype can betreated separately from the other prototypes.
Wemake use of a simple sum of the distance betweenthe two background components, and the AvgSimof the two sets of clustering components.6 Experimental Setup6.1 CorpusWord occurrence statistics are collected from a snap-shot of English Wikipedia taken on Sept. 29th, 2009.Wikitext markup is removed, as are articles withfewer than 100 words, leaving 2.8M articles with atotal of 2.05B words.
Wikipedia was chosen due toits semantic breadth.6.2 Evaluation MethodologyWe evaluate the tiered clustering model on two prob-lems from lexical semantics: word relatedness andselectional preference.
For the word relatedness1This can be problematic for certain clustering methodsthat specify uniform priors over cluster sizes; however theDPMM naturally exhibits a linear decay in cluster sizes withthe Er# clusters of size M s  ?
{M .Rating distributionWS-3530.00.51.0Evocation PadoSense count distributionWS-353031080Evocation PadoFigure 2: (top) The distribution of ratings (scaled [0,1])on WS-353, WN-Evocation and Pado?
datasets.
(bottom)The distribution of sense counts for each data set (log-domain), collected from WordNet 3.0.evaluation, we compared the predicted similarity ofword pairs from each model to two collections of hu-man similarity judgements: WordSim-353 (Finkel-stein et al, 2001) and the Princeton Evocation rela-tions (WN-Evocation, Ma et al, 2009).WS-353 contains between 13 and 16 human sim-ilarity judgements for each of 353 word pairs, ratedon a 1?10 integer scale.
WN-Evocation is signif-icantly larger than WS-353, containing over 100ksimilarity comparisons collected from trained hu-man raters.
Comparisons are assigned to only 3-5 human raters on average and contain a signifi-cantly higher fraction of zero- and low-similarityitems than WS-353 (Figure 2), reflecting more ac-curately real-world lexical semantics settings.
In ourexperiments we discard all comparisons with fewerthan 5 ratings and then sample 10% of the remain-ing pairs uniformly at random, resulting in a test setwith 1317 comparisons.For selectional preference, we employ the Pado?dataset, which contains 211 verb-noun pairs withhuman similarity judgements for how plausible thenoun is for each argument of the verb (2 argumentsper verb, corresponding roughly to subject and ob-ject).
Results are averaged across 20 raters; typicalinter-rater agreement is ?
 0.7 (Pado?
et al, 2007).In all cases correlation with human judgementsis computed using Spearman?s nonparametric rankcorrelation (?)
with average human judgements1176(Agirre et al, 2009).6.3 Feature RepresentationIn the following analyses we confine ourselves torepresenting word occurrences using unordered un-igrams collected from a window of size T10 cen-tered around the occurrence, represented using tf-idfweighting.
Feature vectors are pruned to a fixedlength f , discarding all but the highest-weight fea-tures (f is selected via empirical validation, as de-scribed in the next section).
Finally, semantic simi-larity between word pairs is computed using cosinedistance (`2-normalized dot-product).26.4 Feature PruningFeature pruning is one of the most significant factorsin obtaining high correlation with human similarityjudgements using vector-space models, and has beensuggested as one way to improve sense disambigua-tion for polysemous verbs (Xue et al, 2006).
In thissection, we calibrate the single prototype and multi-prototype methods on WS-353, reaching the limitof human and oracle performance and demonstrat-ing robust performance gains even with semanti-cally impoverished features.
In particular we obtain?0.75 correlation on WS-353 using only unigramcollocations and ?0.77 using a fixed-K multi-prototype representation (Figure 3; Reisinger andMooney, 2010).
This result rivals average humanperformance, obtaining correlation near that of thesupervised oracle approach of Agirre et al (2009).The optimal pruning cutoff depends on the fea-ture weighting and number of prototypes as well asthe feature representation.
t-test and ?2 features aremost robust to feature noise and perform well evenwith no pruning; tf-idf yields the best results but ismost sensitive to the pruning parameter (Figure 3).As the number of features increases, more pruningis required to combat feature noise.Figure 4 breaks down the similarity pairs into fourquantiles for each data set and then shows corre-lation separately for each quantile.
In general themore polarized data quantiles (1 and 4) have highercorrelation, indicating that fine-grained distinctions2(Parameter robustness) We observe lower correlations onaverage for T25 and T5 and therefore observe T10 tobe near-optimal.
Substituting weighted Jaccard similarity forcosine does not significantly affect the results in this paper.00.40.800.40.8Spearman's?0.70.0-0.2unpruned pruned (best)Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q4humanSingle-pMulti-pESAFigure 4: Correlation results on WS-353 broken downover quantiles in the human ratings.
Quantile ranges areshown in Figure 2.
In general ratings for highly sim-ilar (dissimilar) pairs are more predictable (quantiles 1and 4) than middle similarity pairs (quantiles 2, 3).
ESAshows results for a more semantically rich feature set de-rived using Explicit Semantic Analysis (Gabrilovich andMarkovitch, 2007).in semantic distance are easier for those sets.3 Fea-ture pruning improves correlations in quantiles 2?4while reducing correlation in quantile 1 (lowest sim-ilarity).
This result is to be expected as more fea-tures are necessary to make fine-grained distinctionsbetween dissimilar pairs.7 ResultsWe evaluate four models: (1) the standard single-prototype approach, (2) the DPMM multi-prototypeapproach outlined in ?3, (3) a simple combina-tion of the multi-prototype and single-prototype ap-proaches (MP+SP)4 and (4) the tiered clustering ap-proach (?4).
Each data set is divided into 5 quan-tiles based on per-pair average sense counts,5 col-lected from WordNet 3.0 (Fellbaum, 1998); ex-amples of pairs in the high-polysemy quantile areshown in Table 2.
Unless otherwise specified,both DPMM multi-prototype and tiered clustering3The fact that the per-quantile correlation is significantlylower than the full correlation e.g.
in the human case indicatesthat fine-grained ordering (within quantile) is more difficult thancoarse-grained (between quantile).4(MP+SP) Tiered clustering?s ability to model both sharedand idiosyncratic structure can be easily approximated by us-ing the single prototype model as the shared component andmulti-prototype model as the clustering.
However, unlike in thetiered model, all features are assigned to both components.
Wedemonstrate that this simplification actually hurts performance.5Despite many skewed pairs (e.g.
line has 36 senses whileinsurance has 3), we found that arithmetic average and geomet-ric average perform the same.117700.40.800.40.800.40.800.40.8K=1 K=10 K=50 tf-idf cosine, K=1,10,50Spearman's?0.00.8# of featuresall10k5k2k1k5002001002010# of featuresall10k5k2k1k5002001002010# of featuresall10k5k2k1k5002001002010# of featuresall10k5k2k1k5002001002010tf-idfttest?2tfK=50K=10K=1tf-idfttest?2tftf-idfttest?2tf// //// //Figure 3: Effects of feature pruning and representation on WS-353 correlation broken down across multi-prototyperepresentation size.
In general tf-idf features are the most sensitive to pruning level, yielding the highest correlation formoderate levels of pruning and significantly lower correlation than other representations without pruning.
The optimalamount of pruning varies with the number of prototypes used, with fewer features being optimal for more clusters.Bars show 95% confidence intervals.WordSim-353stock-live, start-match, line-insurance, game-round, street-place, company-stockEvocationbreak-fire, clear-pass, take-call, break-tin,charge-charge, run-heat, social-playPado?see-drop, see-return, hit-stock, raise-bank, see-face, raise-firm, raise-questionTable 2: Examples of highly polysemous pairs from eachdata set using sense counts from WordNet.use symmetric Dirichlet hyperparameters, ?0.1,?0.1, and tiered clustering uses?10 for the back-ground/clustering allocation smoother.7.1 WordSim-353Correlation results for WS-353 are shown in Table3.
In general the approaches incorporating multipleprototypes outperform single prototype (?
 0.768vs.
?
 0.734).
The tiered clustering model does notsignificantly outperform either the multi-prototypeor MP+SP models on the full set, but yields signifi-cantly higher correlation on the high-polysemy set.The tiered model generates more clusters thanDPMM multi-prototype (27.2 vs. 14.8), despite us-ing the same hyperparameter settings: Since wordscommonly shared across clusters have been allo-cated to the background component, the clustercomponents have less overlap and hence the modelnaturally allocates more clusters.Examples of the tiered clusterings for severalMethod ?
 100 ErCs backgroundSingle prototype 73.40.5 1.0 -high polysemy 76.00.9 1.0 -Multi-prototype 76.80.4 14.8 -high polysemy 79.31.3 12.5 -MP+SP 75.40.5 14.8 -high polysemy 80.11.0 12.5 -Tiered 76.90.5 27.2 43.0%high polysemy 83.11.0 24.2 43.0%Table 3: Spearman?s correlation on the WS-353 data set.All refers to the full set of pairs, high polysemy refers tothe top 20% of pairs, ranked by sense count.
ErCs is theaverage number of clusters employed by each method andbackground is the average percentage of features allo-cated by the tiered model to the background cluster.
95%confidence intervals are computed via bootstrapping.words from WS-353 are shown in Table 1 and corre-sponding clusters from the multi-prototype approachare shown in Table 4.
In general the backgroundcomponent does indeed capture commonalities be-tween all the sense clusters (e.g.
all wizards usemagic) and hence the tiered clusters are more se-mantically pure.
This effect is most visible in the-matically polysemous words, e.g.
radio and wizard.7.2 EvocationCompared to WS-353, the WN-Evocation pair setis sampled more uniformly from English word pairsand hence contains a significantly larger fraction ofunrelated words, reflecting the fact that word sim-1178LIFEmy, you, real, about, your, wouldyears, spent, rest, lived, lastsentenced, imprisonment, sentence, prisonyears, cycle, life, all, expectancy, otherall, life, way, people, human, social, manyRADIOstation, FM, broadcasting, format, AMradio, station, stations, amateur,show, station, host, program, radiostations, song, single, released, airplaystation, operator, radio, equipment, contactWIZARDevil, magic, powerful, named, worldMerlin, King, Arthur, powerful, courtspells, magic, cast, wizard, spell, witchHarry, Dresden, series, Potter, characterSTOCKmarket, price, stock, company, value, crashhousing, breeding, all, large, stock, manycar, racing, company, cars, summer, NASCARstock, extended, folded, card, barrel, cardsrolling, locomotives, new, character, lineTable 4: Example DPMM multi-prototype representationof words with varying degrees of polysemy.
Compared tothe tiered clustering results in Table 1 the multi-prototypeclusters are significantly less pure for thematically poly-semous words such as radio and wizard.ilarity is a sparse relation (Figure 2 top).
Further-more, it contains proportionally more highly polyse-mous words relative to WS-353 (Figure 2 bottom).On WN-Evocation, the single prototype andmulti-prototype do not differ significantly in termsof correlation (?0.198 and ?0.201 respectively;Table 5), while SP+MP yields significantly lowercorrelation (?0.176), and the tiered model yieldssignificantly higher correlation (?0.224).
Restrict-ing to the top 20% of pairs with highest humansimilarity judgements yields similar outcomes, withsingle prototype, multi-prototype and SP+MP sta-tistically indistinguishable (?0.239, ?0.227 and?0.235), and tiered clustering yielding signifi-cantly higher correlation (?0.277).
Likewise tieredclustering achieves the most significant gains on thehigh polysemy subset.7.3 Selectional PreferenceTiered clustering is a natural model for verb selec-tional preference, especially for more selectionallyrestrictive verbs: the set of words that appear in aparticular argument slot naturally have some kind ofMethod ?
 100 ErCs backgroundSingle prototype 19.80.6 1.0 -high similarity 23.91.1 1.0 -high polysemy 11.51.2 1.0 -Multi-prototype 20.10.5 14.8 -high similarity 22.71.2 14.1 -high polysemy 13.01.3 13.2 -MP+SP 17.60.5 14.8 -high similarity 23.51.2 14.1 -high polysemy 11.41.0 13.2 -Tiered 22.40.6 29.7 46.6%high similarity 27.71.3 29.9 47.2%high polysemy 15.41.1 27.4 46.6%Table 5: Spearman?s correlation on the Evocation dataset.
The high similarity subset contains the top 20% ofpairs sorted by average rater score.Method ?
 100 ErCs backgroundSingle prototype 25.80.8 1.0 -high polysemy 17.31.7 1.0 -Multi-prototype 20.21.0 18.5 -high polysemy 14.12.4 17.4 -MP+SP 19.71.0 18.5 -high polysemy 10.52.5 17.4 -Tiered 29.41.0 37.9 41.7%high polysemy 28.52.4 37.4 43.2%Table 6: Spearman?s correlation on the Pado?
data set.commonality (i.e.
they can be eaten or can promise).The background component of the tiered clusteringmodel can capture such general argument structure.We model each verb argument slot in the Pado?
setwith a separate tiered clustering model, separatingterms co-occurring with the target verb according towhich slot they fill.On the Pado?
set, the performance of the DPMMmulti-prototype approach breaks down and it yieldssignificantly lower correlation with human normsthan the single prototype (?0.202 vs. ?0.258;Table 6), due to its inability to capture the sharedstructure among verb arguments.
Furthermore com-bining with the single prototype does not signif-icantly change its performance (?0.197).
Mov-ing to the tiered model, however, yields significantimprovements in correlation over the other models(?0.294), primarily improving correlation in thecase of highly polysemous verbs and arguments.11798 Discussion and Future WorkWe have demonstrated a novel model for dis-tributional lexical semantics capable of capturingboth shared (context-independent) and idiosyncratic(context-dependent) structure in a set of word occur-rences.
The benefits of this tiered model were mostpronounced on a selectional preference task, wherethere is significant shared structure imposed by con-ditioning on the verb.
Although our results on thePado?
are not state of the art,6 we believe this to bedue to the impoverished vector-space design; tieredclustering can be applied to more expressive vec-tor spaces, such as those incorporating dependencyparse and FrameNet features.One potential explanation for the superior perfor-mance of the tiered model vs. the DPMM multi-prototype model is simply that it allocates moreclusters to represent each word (Reisinger andMooney, 2010).
However, we find that decreas-ing the hyperparameter ?
(decreasing vocabularysmoothing and hence increasing the effective num-ber of clusters) beyond ?
 0.1 actually hurts multi-prototype performance.
The additional clusters donot provide more semantic content due to significantbackground similarity.Finally, the DPMM multi-prototype and tieredclustering models allocate clusters based on the vari-ance of the underlying data set.
We observe a neg-ative correlation (?0.33) between the number ofclusters allocated by the DPMM and the number ofword senses found in WordNet.
This result is mostlikely due to our use of unigram context windowfeatures, which induce clustering based on thematicrather than syntactic differences.
Investigating thisissue is future work.
(Future Work) The word similarity experimentscan be expanded by breaking pairs down further intohighly homonymous and highly polysemous pairs,using e.g.
WordNet to determine how closely relatedthe senses are.
With this data it would be interest-ing to validate the hypothesis that the percentage offeatures allocated to the background cluster is corre-lated with the degree of homonymy.The basic tiered clustering can be extended withadditional background tiers, allocating more expres-sivity to model background feature variation.
Thisclass of models covers the spectrum between a pure6E.g., Pado?
et al (2007) report ?0.515 on the same data.topic model (all background tiers) and a pure clus-tering model and may be reasonable when there isbelieved to be more background structure (e.g.
whenjointly modeling all verb arguments).
Furthermore,it is straightforward to extend the model to a two-tier, two-clustering structure capable of additionallyaccounting for commonalities between arguments.Applying more principled feature selection ap-proaches to vector-space lexical semantics mayyield more significant performance gains.
Towardsthis end we are currently evaluating two classes ofapproaches for setting pruning parameters per-wordinstead of globally: (1) subspace clustering, i.e.unsupervised feature selection (e.g., Parsons et al,2004) and (2) multiple clustering, i.e.
finding fea-ture partitions that lead to disparate clusterings (e.g.,Shafto et al, 2006).9 ConclusionsThis paper introduced a simple probabilistic modelof tiered clustering inspired by feature selectiveclustering that leverages feature exchangeability toallocate data features between a clustering modeland shared component.
The ability to model back-ground variation, or shared structure, is shown to bebeneficial for modeling words with high polysemy,yielding increased correlation with human similarityjudgements modeling word relatedness and selec-tional preference.
Furthermore, the tiered clusteringmodel is shown to significantly outperform relatedmodels, yielding qualitatively more precise clusters.AcknowledgmentsThanks to Yinon Bentor and Bryan Silverthorn formany illuminating discussions.
This work was sup-ported by an NSF Graduate Research Fellowship tothe first author, and a Google Research Award.A Collapsed Gibbs SamplerIn order to sample efficiently from this model, weleverage the Chinese Restaurant Process represen-tation of the DP (cf., Aldous, 1985), introducing aper-word-occurrence cluster indicator cd.
Word oc-currence features are then drawn from a combinationof a single cluster component indicated by cd and thebackground topic.By exploiting conjugacy, the latent variables ?, ?and ?d can be integrated out, yielding an efficient1180collapsed Gibbs sampler.
The likelihood of wordoccurrence d is given byP pwd|z, cd,?q ?iP pwi,d|?cdq?pzd,i0qP pwi,d|?noiseq?pzd,i1q.Hence, this model can be viewed as a two-topicvariant of LDA with the addition of a per-word-occurrence (i.e.
document) cluster indicator.7 Theupdate rule for the latent tier indicator z is similarto the update rule for 2-topic LDA, with the back-ground component as the first topic and the secondtopic being determined by the per-word-occurrencecluster indicator c.We can efficiently approximate ppz|wq via Gibbssampling, which requires the complete conditionalposteriors for all zi,d.
These areP pzi,d  t|zpi,dq,w, ?, ?q npwi,dqt   ?
?wpnpwqt   ?qnpdqt   ?
?jpnpdqj   ?q.where zpi,dq is shorthand for the set ztzi,du, npwqtis the number of occurrences of wordw in topic t notcounting wi,d and npdqt is the number of features inoccurrence d assigned to topic t, not counting wi,d.Likewise sampling the cluster indicators condi-tioned on the data ppcd|w, cd, ?, ?q decomposesinto the DP posterior over cluster assignmentsand the cluster-conditional Multinomial-Dirichletword-occurrence likelihood ppcd|w, cd, ?, ?q ppcd|cd, ?qppwd|wd, c, z, ?q given byP pcd  kold|cd, ?, ?q9mpdqkmpdq  ?looooooomooooooonppcd|cd,?qCp? ?
?n pdqk  ?
?n pdqqqCp? ?
?n pdqk qloooooooooooooomoooooooooooooonppwd|wd,c,z,?qP pcd  knew|cd, ?, ?q9?mpdq  ?Cp? ?
?n pdqqCp?qwhere mpdqk is the number of occurrences as-signed to k not including d, ?
?n pdqk is the vector ofcounts of words from occurrence wd assigned to7Effectively, the tiered clustering model is a special case ofthe nested Chinese Restaurant Process with the tree depth fixedto two (Blei et al, 2003).cluster k (i.e.
words with zi,d  0) and Cpq isthe normalizing constant for the Dirichlet Cpaq ?p?mj1 ajq1?mj1 ?pajq operating over vectorsof counts a.ReferencesEneko Agirre, Enrique Alfonseca, Keith Hall, JanaKravalova, Marius Pas?ca, and Aitor Soroa.
2009.A study on similarity and relatedness using distri-butional and Wordnet-based approaches.
In Proc.of NAACL-HLT-09, pages 19?27.David J. Aldous.
1985.
Exchangeability and relatedtopics.
In E?cole d?e?te?
de probabilite?s de Saint-Flour, XIII?1983, volume 1117, pages 1?198.Springer, Berlin.David Blei, Thomas Griffiths, Michael Jordan, andJoshua Tenenbaum.
2003.
Hierarchical topicmodels and the nested Chinese restaurant process.In Proc.
NIPS-2003.Stephen Clark and David Weir.
2002.
Class-basedprobability estimation using a semantic hierarchy.Computational Linguistics, 28(2):187?206.James Richard Curran.
2004.
From Distributionalto Semantic Similarity.
Ph.D. thesis, Universityof Edinburgh.
College of Science.Katrin Erk and Sebastian Pado.
2008.
A structuredvector space model for word meaning in context.In Proceedings of EMNLP 2008.Christiane Fellbaum, editor.
1998.
WordNet: AnElectronic Lexical Database and Some of its Ap-plications.
MIT Press.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-tan Ruppin.
2001.
Placing search in context: theconcept revisited.
In Proc.
of WWW 2001.Evgeniy Gabrilovich and Shaul Markovitch.2007.
Computing semantic relatedness usingWikipedia-based explicit semantic analysis.
InProc.
of IJCAI-07, pages 1606?1611.Daniel Gildea and Daniel Jurafsky.
2002.
Automaticlabeling of semantic roles.
Computational Lin-guistics, 28(3):245?288.James Gorman and James R. Curran.
2006.
Scalingdistributional similarity to large corpora.
In Proc.of ACL 2006.1181Thomas L. Griffiths, Mark Steyvers, and Joshua B.Tenenbaum.
2007.
Topics in semantic representa-tion.
Psychological Review, 114:2007.Amac?
Herdag?delen and Marco Baroni.
2009.
Bag-pack: A general framework to represent semanticrelations.
In Proc.
of GEMS 2009.Donald Hindle and Mats Rooth.
1991.
Structuralambiguity and lexical relations.
In Proc.
of ACL1991.Thomas Landauer and Susan Dumais.
1997.
A solu-tion to Plato?s problem: The latent semantic anal-ysis theory of acquisition, induction and repre-sentation of knowledge.
Psychological Review,104(2):211?240.Martin H. C. Law, Anil K. Jain, and Ma?rio A. T.Figueiredo.
2002.
Feature selection in mixture-based clustering.
In Proc.
of NIPS 2002.Will Lowe.
2001.
Towards a theory of semanticspace.
In Proceedings of the 23rd Annual Meetingof the Cognitive Science Society, pages 576?581.Xiaojuan Ma, Jordan Boyd-Graber, Sonya S.Nikolova, and Perry Cook.
2009.
Speakingthrough pictures: Images vs. icons.
In ACM Con-ference on Computers and Accessibility.Christopher D. Manning, Prabhakar Raghavan, andHinrich Schu?tze.
2008.
Introduction to Informa-tion Retrieval.
Cambridge University Press.Diana McCarthy and John Carroll.
2003.
Disam-biguating nouns, verbs, and adjectives using auto-matically acquired selectional preferences.
Com-putational Linguistics, 29(4):639?654.George A. Miller and Walter G. Charles.
1991.
Con-textual correlates of semantic similarity.
Lan-guage and Cognitive Processes, 6(1):1?28.Sebastian Pado?
and Mirella Lapata.
2007.Dependency-based construction of semanticspace models.
Computational Linguistics,33(2):161?199.Sebastian Pado?, Ulrike Pado?, and Katrin Erk.
2007.Flexible, corpus-based modelling of human plau-sibility judgements.
In Proc.
of EMNLP 2007.Ulrike Pado?.
2007.
The Integration of Syntax and Se-mantic Plausibility in a Wide-Coverage Model ofSentence Processing.
Ph.D. thesis, Saarland Uni-versity, Saarbru?cken.Patrick Pantel, Rahul Bhagat, Timothy Chklovski,and Eduard Hovy.
2007.
ISP: Learning inferen-tial selectional preferences.
In In Proceedings ofNAACL 2007.Patrick Andre Pantel.
2003.
Clustering by commit-tee.
Ph.D. thesis, Edmonton, Alta., Canada.Lance Parsons, Ehtesham Haque, and Huan Liu.2004.
Subspace clustering for high dimensionaldata: A review.
SIGKDD Explor.
Newsl., 6(1).Fernando Pereira, Naftali Tishby, and Lillian Lee.1993.
Distributional clustering of English words.In Proc.
of ACL 1993.Carl E. Rasmussen.
2000.
The infinite Gaussianmixture model.
In Advances in Neural Informa-tion Processing Systems.
MIT Press.Joseph Reisinger and Raymond Mooney.
2010.Multi-prototype vector-space models of wordmeaning.
In Proc.
of NAACL 2010.Philip Resnik.
1997.
Selectional preference andsense disambiguation.
In Proceedings of ACLSIGLEX Workshop on Tagging Text with LexicalSemantics, pages 52?57.
ACL.Adam N. Sanborn, Thomas L. Griffiths, andDaniel J. Navarro.
2006.
A more rational modelof categorization.
In Proceedings of the 28th An-nual Conference of the Cognitive Science Society.Hinrich Schu?tze.
1998.
Automatic word sensediscrimination.
Computational Linguistics,24(1):97?123.Patrick Shafto, Charles Kemp, Vikash Mansinghka,Matthew Gordon, and Joshua B. Tenenbaum.2006.
Learning cross-cutting systems of cate-gories.
In Proc.
CogSci 2006.Rion Snow, Daniel Jurafsky, and Andrew Ng.
2006.Semantic taxonomy induction from heterogenousevidence.
In Proc.
of ACL 2006.Peter D. Turney.
2006.
Similarity of semantic rela-tions.
Computational Linguistics, 32(3):379?416.Benjamin Van Durme and Marius Pas?ca.
2008.Finding cars, goddesses and enzymes:Parametrizable acquisition of labeled instancesfor open-domain information extraction.
In Proc.of AAAI 2008.Nianwen Xue, Jinying Chen, and Martha Palmer.2006.
Aligning features with sense distinction di-mensions.
In Proc.
of COLING/ACL 2006.1182
