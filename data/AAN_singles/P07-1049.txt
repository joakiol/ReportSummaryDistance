Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 384?391,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsFast Unsupervised Incremental ParsingYoav SeginerInstitute for Logic, Language and ComputationUniversiteit van AmsterdamPlantage Muidergracht 241018TV AmsterdamThe Netherlandsyseginer@science.uva.nlAbstractThis paper describes an incremental parserand an unsupervised learning algorithm forinducing this parser from plain text.
Theparser uses a representation for syntacticstructure similar to dependency links whichis well-suited for incremental parsing.
Incontrast to previous unsupervised parsers,the parser does not use part-of-speech tagsand both learning and parsing are localand fast, requiring no explicit clustering orglobal optimization.
The parser is evalu-ated by converting its output into equivalentbracketing and improves on previously pub-lished results for unsupervised parsing fromplain text.1 IntroductionGrammar induction, the learning of the grammarof a language from unannotated example sentences,has long been of interest to linguists because of itsrelevance to language acquisition by children.
Inrecent years, interest in unsupervised learning ofgrammar has also increased among computationallinguists, as the difficulty and cost of constructingannotated corpora led researchers to look for waysto train parsers on unannotated text.
This can ei-ther be semi-supervised parsing, using both anno-tated and unannotated data (McClosky et al, 2006)or unsupervised parsing, training entirely on unan-notated text.The past few years have seen considerable im-provement in the performance of unsupervisedparsers (Klein and Manning, 2002; Klein and Man-ning, 2004; Bod, 2006a; Bod, 2006b) and, for thefirst time, unsupervised parsers have been able toimprove on the right-branching heuristic for pars-ing English.
All these parsers learn and parsefrom sequences of part-of-speech tags and select,for each sentence, the binary parse tree which maxi-mizes some objective function.
Learning is based onglobal maximization of this objective function overthe whole corpus.In this paper I present an unsupervised parserfrom plain text which does not use parts-of-speech.Learning is local and parsing is (locally) greedy.
Asa result, both learning and parsing are fast.
Theparser is incremental, using a new link representa-tion for syntactic structure.
Incremental parsing waschosen because it considerably restricts the searchspace for both learning and parsing.
The represen-tation the parser uses is designed for incrementalparsing and allows a prefix of an utterance to beparsed before the full utterance has been read (seesection 3).
The representation the parser outputs canbe converted into bracketing, thus allowing evalua-tion of the parser on standard treebanks.To achieve completely unsupervised parsing,standard unsupervised parsers, working from part-of-speech sequences, need first to induce the parts-of-speech for the plain text they need to parse.
Thereare several algorithms for doing so (Schu?tze, 1995;Clark, 2000), which cluster words into classes basedon the most frequent neighbors of each word.
Thisstep becomes superfluous in the algorithm I presenthere: the algorithm collects lists of labels for eachword, based on neighboring words, and then directly384uses these labels to parse.
No clustering is per-formed, but due to the Zipfian distribution of words,high frequency words dominate these lists and pars-ing decisions for words of similar distribution areguided by the same labels.Section 2 describes the syntactic representationused, section 3 describes the general parser algo-rithm and sections 4 and 5 complete the details bydescribing the learning algorithm, the lexicon it con-structs and the way the parser uses this lexicon.
Sec-tion 6 gives experimental results.2 Common Cover LinksThe representation of syntactic structure which I in-troduce in this paper is based on links between pairsof words.
Given an utterance and a bracketing ofthat utterance, shortest common cover link sets forthe bracketing are defined.
The original bracketingcan be reconstructed from any of these link sets.2.1 Basic DefinitionsAn utterance is a sequence of words ?x1, .
.
.
, xn?and a bracket is any sub-sequence ?xi, .
.
.
, xj?
ofconsecutive words in the utterance.
A set B of brack-ets over an utterance U is a bracketing of U if everyword in U is in some bracket and for any X,Y ?
Beither X ?
Y = ?, X ?
Y or Y ?
X (non-crossing brackets).
The depth of a word x ?
Uunder a bracket B ?
B (x ?
B) is the maxi-mal number of brackets X1, .
.
.
, Xn ?
B such thatx ?
X1 ?
.
.
.
?
Xn ?
B.
A word x is a generatorof depth d of B in B if x is of minimal depth underB (among all words in B) and that depth is d. Abracket may have more than one generator.2.2 Common Cover Link SetsA common cover link over an utterance U is a triplex d?
y where x, y ?
U , x 6= y and d is a non-negative integer.
The word x is the base of the link,the word y is its head and d is the depth of the link.The common cover link set RB associated with abracketing B is the set of common cover links overU such that x d?
y ?
RB iff the word x is a gener-ator of depth d of the smallest bracket B ?
B suchthat x, y ?
B (see figure 1(a)).Given RB, a simple algorithm reconstructs thebracketing B: for each word x and depth 0 ?
d,(a) [ [ w ]1;;1<<1==[ x1zz0!
!0[ y //0 z ] ] ]oo(b) [ [ w ] [ x1zz0!
!0[ y //0 z ] ] ]oo(c) [ [ w ] [ x1zz0!!
[ y //0 z ] ] ]ooFigure 1: (a) The common cover link set RB of abracketing B, (b) a representative subset R of RB,(c) the shortest common cover link set based on R.create a bracket covering x and all y such that forsome d?
?
d, x d??
y ?
RB.Some of the links in the common cover link setRB are redundant.
The first redundancy is the resultof brackets having more than one generator.
Thebracketing reconstruction algorithm outlined abovecan construct a bracket from the links based at anyof its generators.
The bracketing B can therefore bereconstructed from a subset R ?
RB if, for everybracket B ?
B, R contains the links based at least atone generator1 of B.
Such a set R is a representativesubset of RB (see figure 1(b)).A second redundancy in the set RB follows fromthe linear transitivity of RB:Lemma 1 If y is between x and z, x d1?
y ?
RB andy d2?
z ?
RB then x d?
z ?
RB where if there is alink y d??
x ?
RB then d = max(d1, d2) and d = d1otherwise.This property implies that longer links can be de-duced from shorter links.
It is, therefore, sufficientto leave only the shortest necessary links in the set.Given a representative subset R of RB, a shortestcommon cover link set of RB is constructed by re-moving any link which can be deduced from shorterlinks by linear transitivity.
For each representativesubset R ?
RB, this defines a unique shortest com-mon cover link set (see figure 1(c)).Given a shortest common cover link set S, thebracketing which it represents can be calculated by1From the bracket reconstruction algorithm it can be seenthat links of depth 0 may never be dropped.385[ [ I ] [ know{{ %%[ [ the boy ]oo [ sleeps ] ] ] ]}}(a) dependency structure[ [ I ] [ know1{{0%%0 ""[ [ the 0 // boy ]oo [ sleeps ] ] ] ]1}}(b) shortest common cover link setFigure 2: A dependency structure and shortest com-mon cover link set of the same sentence.first using linear transitivity to deduce missing linksand then applying the bracket reconstruction algo-rithm outlined above for RB.2.3 Comparison with Dependency StructuresHaving defined a link-based representation of syn-tactic structure, it is natural to wonder what the rela-tion is between this representation and standard de-pendency structures.
The main differences betweenthe two representations can all be seen in figure 2.The first difference is in the linking of the NP theboy.
While the shortest common cover link set hasan exocentric construction for this NP (that is, linksgoing back and forth between the two words), thedependency structure forces us to decide which ofthe two words in the NP is its head.
Consideringthat linguists have not been able to agree whether itis the determiner or the noun that is the head of anNP, it may be easier for a learning algorithm if it didnot have to make such a choice.The second difference between the structures canbe seen in the link from know to sleeps.
In the short-est common cover link set, there is a path of linksconnecting know to each of the words separating itfrom sleeps, while in the dependency structure nosuch links exist.
This property, which I will refer toas adjacency plays an important role in incrementalparsing, as explained in the next section.The last main difference between the represen-tations is the assignment of depth to the commoncover links.
In the present example, this allows us todistinguish between the attachment of the external(subject) and the internal (object) arguments of theverb.
Dependencies cannot capture this differencewithout additional labeling of the links.
In what fol-lows, I will restrict common cover links to havingdepth 0 or 1.
This restriction means that any treerepresented by a shortest common cover link set willbe skewed - every subtree must have a short branch.It seems that this is indeed a property of the syntaxof natural languages.
Building this restriction intothe syntactic representation considerably reduces thesearch space for both parsing and learning.3 Incremental ParsingTo calculate a shortest common cover link for anutterance, I will use an incremental parser.
Incre-mentality means that the parser reads the words ofthe utterance one by one and, as each word is read,the parser is only allowed to add links which haveone of their ends at that word.
Words which havenot yet been read are not available to the parser atthis stage.
This restriction is inspired by psycholin-guistic research which suggests that humans processlanguage incrementally (Crocker et al, 2000).
If theincrementality of the parser roughly resembles thatof human processing, the result is a significant re-striction of parser search space which does not leadto too many parsing errors.The adjacency property described in the previoussection makes shortest common cover link sets es-pecially suitable for incremental parsing.
Considerthe example given in figure 2.
When the word theis read, the parser can already construct a link fromknow to the without worrying about the continuationof the sentence.
This link is part of the correct parsewhether the sentence turns out to be I know the boyor I know the boy sleeps.
A dependency parser, onthe other hand, cannot make such a decision beforethe end of the sentence is reached.
If the sentence isI know the boy then a dependency link has to be cre-ated from know to boy while if the sentence is I knowthe boy sleeps then such a link is wrong.
This prob-lem is known in psycholinguistics as the problem ofreanalysis (Sturt and Crocker, 1996).Assume the incremental parser is processing aprefix ?x1, .
.
.
, xk?
of an utterance and has alreadydeduced a set of links L for this prefix.
It can nowonly add links which have one of their ends at xk andit may never remove any links.
From the definitionsin section 2.2 it is possible to derive an exact char-acterization of the links which may be added at eachstep such that the resulting link set represents some386bracketing.
It can be shown that any shortest com-mon cover link set can be constructed incrementallyunder these conditions.
As the full specification ofthese conditions is beyond the scope of this paper, Iwill only give the main condition, which is based onadjacency.
It states that a link may be added from xto y only if for every z between x and y there is apath of links (in L) from x to z but no link from z toy.
In the example in figure 2 this means that whenthe word sleeps is first read, a link to sleeps can becreated from know, the and boy but not from I.Given these conditions, the parsing process issimple.
At each step, the parser calculates a non-negative weight (section 5) for every link whichmay be added between the prefix ?x1, .
.
.
, xk?1?
andxk.
It then adds the link with the strongest positiveweight and repeats the process (adding a link canchange the set of links which may be added).
Whenall possible links are assigned a zero weight by theparser, the parser reads the next word of the utter-ance and repeats the process.
This is a greedy algo-rithm which optimizes every step separately.4 LearningThe weight function which assigns a weight to a can-didate link is lexicalized: the weight is calculatedbased on the lexical entries of the words which areto be connected by the link.
It is the task of the learn-ing algorithm to learn the lexicon.4.1 The LexiconThe lexicon stores for each word x a lexical en-try.
Each such lexical entry is a sequence of adja-cency points, holding statistics relevant to the deci-sion whether to link x to some other word.
Thesestatistics are given as weights assigned to labels andlinking properties.
Each adjacency point describes adifferent link based at x, similar to the specificationof the arguments of a word in dependency parsing.Let W be the set of words in the corpus.
Theset of labels L(W ) = W ?
{0, 1} consists oftwo labels based on every word w: a class la-bel (w, 0) (denoted by [w]) and an adjacency la-bel (w, 1) (denoted by [w ] or [ w]).
The two la-bels (w, 0) and (w, 1) are said to be opposite la-bels and, for l ?
L(W ), I write l?1 for the op-posite of l. In addition to the labels, there is alsoa finite set P = {Stop, In?, In,Out} of link-ing properties.
The Stop specifies the strength ofnon-attachment, In and Out specify the strengthof inbound and outbound links and In?
is an in-termediate value in the induction of inbound andoutbound strengths.
A lexicon L is a functionwhich assigns each word w ?
W a lexical entry(.
.
.
, Aw?2, Aw?1, Aw1 , Aw2 , .
.
.).
Each of the Awi is anadjacency point.Each Awi is a function Awi : L(W ) ?
P ?
Rwhich assigns each label in L(W ) and each linkingproperty in P a real valued strength.
For each Awi ,#(Awi ) is the count of the adjacency point: the num-ber of times the adjacency point was updated.
Basedon this count, I also define a normalized version ofAwi : A?wi (l) = Awi (l)/#(Awi ).4.2 The Learning ProcessGiven a sequence of training utterances (Ut)0?t, thelearner constructs a sequence of lexicons (Ls)0?sbeginning with the zero lexicon L0 (which assignsa zero strength to all labels and linking properties).At each step, the learner uses the parsing functionPLs based on the previously learned lexicon Ls toextend the parse L of an utterance Ut.
It then usesthe result of this parse step (together with the lexi-con Ls) to create a new lexicon Ls+1 (it may be thatLs = Ls+1).
This operation is a lexicon update.
Theprocess then continues with the new lexicon Ls+1.Any of the lexicons Ls constructed by the learnermay be used for parsing any utterance U , but as sincreases, parsing accuracy should improve.
Thislearning process is open-ended: additional trainingtext can always be added without having to re-runthe learner on previous training data.4.3 Lexicon UpdateTo define a lexicon update, I extend the definition ofan utterance to be U = ?
?l, x1, .
.
.
, xn, ?r?
where ?land ?r are boundary markers.
The property of adja-cency can now be extended to include the boundarymarkers.
A symbol ?
?
U is adjacent to a word xrelative to a set of links L over U if for every word zbetween x and ?
there is a path of links in L from xto z but there is no link from z to ?.
In the followingexample, the adjacencies of x1 are ?l, x2 and x3:x1 0 // x2 x3 x4387If a link is added from x2 to x3, x4 becomes adjacentto x1 instead of x3 (the adjacencies of x1 are then ?l,x2 and x4):x1 0 // x2 0 // x3 x4The positions in the utterance adjacent to a word xare indexed by an index i such that i < 0 to the leftof x, i > 0 to the right of x and |i| increases with thedistance from x.The parser may only add a link from a word x toa word y adjacent to x (relative to the set of links al-ready constructed).
Therefore, the lexical entry of xshould collect statistics about each of the adjacencypositions of x.
As seen above, adjacency positionsmay move, so the learner waits until the parser com-pletes parsing the utterance and then updates eachadjacency point Axi with the symbol ?
at the ith ad-jacency position of x (relative to the parse generatedby the parser).
It should be stressed that this updatedoes not depend on whether a link was created fromx to ?.
In particular, whatever links the parser as-signs, Ax(?1) and Ax1 are always updated by the sym-bols which appear immediately before and after x.The following example should clarify the picture.Consider the fragment:put 0 // the //0 boxoo onAll the links in this example, including the absenceof a link from box to on, depend on adjacency pointsof the form Ax(?1) and Ax1 which are updated inde-pendently of any links.
Based on this alone and re-gardless of whether a link is created from put to on,Aput2 will be updated by the word on, which is in-deed the second argument of the verb put.4.4 Adjacency Point UpdateThe update of Axi by ?
is given by operationsAxi (p) += f(A?
(?1), A?1 ) which make the value ofAxi (p) in the new lexicon Ls+1 equal to the sumAxi (p) + f(A?
(?1), A?1 ) in the old lexicon Ls.Let Sign(i) be 1 if 0 < i and ?1 otherwise.
Let?A?i =??????
?true if @l ?
L(W ) :A?i (l) > A?i (Stop)false otherwiseThe update of Axi by ?
begins by incrementingthe count:#(Axi ) += 1If ?
is a boundary symbol (?l or ?r) or if x and ?are words separated by stopping punctuation (fullstop, question mark, exclamation mark, semicolon,comma or dash):Axi (Stop) += 1Otherwise, for every l ?
L(W ):Axi (l?1) +={ 1 if l = [?]A?
?Sign(?i)(l) otherwise(In practice, only l = [?]
and the 10 strongest labelsin A?Sign(?i) are updated.
Because of the exponen-tial decay in the strength of labels in A?Sign(?i), thisis a good approximation.
)If i = ?1, 1 and ?
is not a boundary or blockedby punctuation, simple bootstrapping takes place byupdating the following properties:Axi (In?)
+=?????
?1 if ?A?Sign(?i)+1 if ?
?A?Sign(?i) ?
?A?Sign(i)0 otherwiseAxi (Out) += A??Sign(?i)(In?
)Axi (In) += A?
?Sign(?i)(Out)4.5 DiscussionTo understand the way the labels and propertiesare calculated, it is best to look at an example.The following table gives the linking properties andstrongest labels for the determiner the as learnedfrom the complete Wall Street Journal corpus (onlyAthe(?1) and Athe1 are shown):theA?1 A1Stop 12897 Stop 8In?
14898 In?
18914In 8625 In 4764Out -13184 Out 21922[the] 10673 [the] 16461[of ] 6871 [a] 3107[in ] 5520 [ the] 2787[a] 3407 [of] 2347[for ] 2572 [ company] 2094[to ] 2094 [?s] 1686A strong class label [w] indicates that the word wfrequently appears in contexts which are similar tothe.
A strong adjacency label [w ] (or [ w]) indicates388that w either frequently appears next to the or thatw frequently appears in the same contexts as wordswhich appear next to the.The property Stop counts the number of times aboundary appeared next to the.
Because the can of-ten appear at the beginning of an utterance but mustbe followed by a noun or an adjective, it is not sur-prising that Stop is stronger than any label on theleft but weaker than all labels on the right.
In gen-eral, it is unlikely that a word has an outbound linkon the side on which its Stop strength is strongerthan that of any label.
The opposite is not true: alabel stronger than Stop indicates an attachment butthis may also be the result of an inbound link, as inthe following entry for to, where the strong labels onthe left are a result of an inbound link:toA?1 A1Stop 822 Stop 48In?
-4250 In?
-981In -57 In -1791Out -3053 Out 4010[to] 5912 [to] 7009[% ] 848 [ the] 3851[in] 844 [ be] 2208[the] 813 [will] 1414[of] 624 [ a] 1158[a] 599 [the] 954For this reason, the learning process is based onthe property ?Axi which indicates where a link is notpossible.
Since an outbound link on one word is in-bound on the other, the inbound/outbound propertiesof each word are then calculated by a simple boot-strapping process as an average of the opposite prop-erties of the neighboring words.5 The Weight FunctionAt each step, the parser must assign a non-negativeweight to every candidate link x d?
y which maybe added to an utterance prefix ?x1, .
.
.
, xk?, and thelink with the largest (non-zero) weight (with a pref-erence for links between xk?1 and xk) is added tothe parse.
The weight could be assigned directlybased on the In and Out properties of either x ory but this method is not satisfactory for three rea-sons: first, the values of these properties on low fre-quency words are not reliable; second, the values ofthe properties on x and y may conflict; third, somewords are ambiguous and require different linkingin different contexts.
To solve these problems, theweight of the link is taken from the values of In andOut on the best matching label between x and y.This label depends on both words and is usually afrequent word with reliable statistics.
It serves as aprototype for the relation between x and y.5.1 Best Matching LabelA label l is a matching label between Axi andAySign(?i) if Axi (l) > Axi (Stop) and either l = (y, 1)or AySign(?i)(l?1) > 0.
The best matching labelat Axi is the matching label l such that the matchstrength min(A?xi (l), A?ySign(?i)(l?1)) is maximal (ifl = (y, 1) then A?ySign(?i)(l?1) is defined to be 1).
Inpractice, as before, only the top 10 labels in Axi andAySign(?i) are considered.The best matching label from x to y is calculatedbetween Axi and AySign(?i) such that Axi is on thesame side of x as y and was either already used tocreate a link or is the first adjacency point on thatside of x which was not yet used.
This means thatthe adjacency points on each side have to be usedone by one, but may be used more than once.
Thereason is that optional arguments of x usually donot have an adjacency point of their own but havethe same labels as obligatory arguments of x andcan share their adjacency point.
The Axi with thestrongest matching label is selected, with a prefer-ence for the unused adjacency point.As in the learning process, label matching isblocked between words which are separated by stop-ping punctuation.5.2 Calculating the Link WeightThe best matching label l = (w, ?)
from x to y canbe either a class (?
= 0) or an adjacency (?
= 1) la-bel at Axi .
If it is a class label, w can be seen as tak-ing the place of x and all words separating it from y(which are already linked to x).
If l is an adjacencylabel, w can be seen to take the place of y.
The cal-culation of the weight Wt(x d?
y) of the link fromx to y is therefore based on the strengths of the Inand Out properties of Aw?
where ?
= Sign(i) ifl = (w, 0) and ?
= Sign(?i) if l = (w, 1).
In ad-dition, the weight is bounded from above by the bestlabel match strength, s(l):?
If l = (w, 0) and Aw?
(Out) > 0:Wt(x 0?
y) = min(s(l), A?w?
(Out))389WSJ10 WSJ40 Negra10 Negra40Model UP UR UF1 UP UR UF1 UP UR UF1 UP UR UF1Right-branching 55.1 70.0 61.7 35.4 47.4 40.5 33.9 60.1 43.3 17.6 35.0 23.4Right-branching+punct.
59.1 74.4 65.8 44.5 57.7 50.2 35.4 62.5 45.2 20.9 40.4 27.6Parsing from POSCCM 64.2 81.6 71.9 48.1 85.5 61.6DMV+CCM(POS) 69.3 88.0 77.6 49.6 89.7 63.9U-DOP 70.8 88.2 78.5 63.9 51.2 90.5 65.4UML-DOP 82.9 66.4 67.0Parsing from plain textDMV+CCM(DISTR.)
65.2 82.8 72.9Incremental 75.6 76.2 75.9 58.9 55.9 57.4 51.0 69.8 59.0 34.8 48.9 40.6Incremental (right to left) 75.9 72.5 74.2 59.3 52.2 55.6 50.4 68.3 58.0 32.9 45.5 38.2Table 1: Parsing results on WSJ10, WSJ40, Negra10 and Negra40.?
If l = (w, 1):?
If Aw?
(In) > 0:Wt(x d?
y) = min(s(l), A?w?
(In))?
Otherwise, if Aw?
(In?)
?
|Aw?
(In)|:Wt(x d?
y) = min(s(l), A?w?
(In?
))where if Aw?
(In?)
< 0 and Aw?
(Out) ?
0 thend = 1 and otherwise d = 0.?
If Aw?
(Out) ?
0 and Aw?
(In) ?
0 and eitherl = (w, 1) or Aw?
(Out) = 0:Wt(x 0?
y) = s(l)?
In all other cases, Wt(x d?
y) = 0.A link x 1?
y attaches x to y but does not placey inside the smallest bracket covering x.
Such linksare therefore created in the second case above, whenthe attachment indication is mixed.To explain the third case, recall that s(l) > 0means that the label l is stronger than Stop on Axi .This implies a link unless the properties of w blockit.
One way in which w can block the link is to havea positive strength for the link in the opposite direc-tion.
Another way in which the properties of w canblock the link is if l = (w, 0) and Aw?
(Out) < 0,that is, if the learning process has explicitly deter-mined that no outbound link from w (which repre-sents x in this case) is possible.
The same conclu-sion cannot be drawn from a negative value for theIn property when l = (w, 1) because, as with stan-dard dependencies, a word determines its outboundlinks much more strongly than its inbound links.6 ExperimentsThe incremental parser was tested on the Wall StreetJournal and Negra Corpora.2 Parsing accuracy wasevaluated on the subsets WSJX and NegraX ofthese corpora containing sentences of length at mostX (excluding punctuation).
Some of these subsetswere used for scoring in (Klein and Manning, 2004;Bod, 2006a; Bod, 2006b).
I also use the same preci-sion and recall measures used in those papers: mul-tiple brackets and brackets covering a single wordwere not counted, but the top bracket was.The incremental parser learns while parsing, andit could, in principle, simply be evaluated for a sin-gle pass of the data.
But, because the quality of theparses of the first sentences would be low, I firsttrained on the full corpus and then measured pars-ing accuracy on the corpus subset.
By training onthe full corpus, the procedure differs from that ofKlein, Manning and Bod who only train on the sub-set of bounded length sentences.
However, this ex-cludes the induction of parts-of-speech for parsingfrom plain text.
When Klein and Manning inducethe parts-of-speech, they do so from a much largercorpus containing the full WSJ treebank togetherwith additional WSJ newswire (Klein and Manning,2002).
The comparison between the algorithms re-mains, therefore, valid.Table 1 gives two baselines and the parsing re-sults for WSJ10, WSJ40, Negra10 and Negra40for recent unsupervised parsing algorithms: CCM2I also tested the incremental parser on the Chinese Tree-bank version 5.0, achieving an F1 score of 54.6 on CTB10 and38.0 on CTB40.
Because this version of the treebank is newerand clearly different from that used by previous papers, the re-sults are not comparable and only given here for completeness.390and DMV+CCM (Klein and Manning, 2004), U-DOP (Bod, 2006b) and UML-DOP (Bod, 2006a).The middle part of the table gives results for pars-ing from part-of-speech sequences extracted fromthe treebank while the bottom part of the table givenresults for parsing from plain text.
Results for the in-cremental parser are given for learning and parsingfrom left to right and from right to left.The first baseline is the standard right-branchingbaseline.
The second baseline modifies right-branching by using punctuation in the same way asthe incremental parser: brackets (except the top one)are not allowed to contain stopping punctuation.
Itcan be seen that punctuation accounts for merely asmall part of the incremental parser?s improvementover the right-branching heuristic.Comparing the two algorithms parsing from plaintext (of WSJ10), it can be seen that the incrementalparser has a somewhat higher combined F1 score,with better precision but worse recall.
This is be-cause Klein and Manning?s algorithms (as well asBod?s) always generate binary parse trees, whilehere no such condition is imposed.
The small differ-ence between the recall (76.2) and precision (75.6)of the incremental parser shows that the number ofbrackets induced by the parser is very close to thatof the corpus3 and that the parser captures the samedepth of syntactic structure as that which was usedby the corpus annotators.Incremental parsing from right to left achieves re-sults close to those of parsing from left to right.
Thisshows that the incremental parser has no built-in biasfor right branching structures.4 The slight degra-dation in performance may suggest that languageshould not, after all, be processed backwards.While achieving state of the art accuracy, the algo-rithm also proved to be fast, parsing (on a 1.86GHzCentrino laptop) at a rate of around 4000 words/sec.and learning (including parsing) at a rate of 3200 ?3600 words/sec.
The effect of sentence length onparsing speed is small: the full WSJ corpus wasparsed at 3900 words/sec.
while WSJ10 was parsedat 4300 words/sec.3The algorithm produced 35588 brackets compared with35302 brackets in the corpus.4I would like to thank Alexander Clark for suggesting thistest.7 ConclusionsThe unsupervised parser I presented here attemptsto make use of several universal properties of nat-ural languages: it captures the skewness of syntac-tic trees in its syntactic representation, restricts thesearch space by processing utterances incrementally(as humans do) and relies on the Zipfian distributionof words to guide its parsing decisions.
It uses anelementary bootstrapping process to deduce the ba-sic properties of the language being parsed.
The al-gorithm seems to successfully capture some of thesebasic properties, but can be further refined to achievehigh quality parsing.
The current algorithm is a goodstarting point for such refinement because it is sovery simple.Acknowledgments I would like to thank Dick deJongh for many hours of discussion, and RemkoScha, Reut Tsarfaty and Jelle Zuidema for readingand commenting on various versions of this paper.ReferencesRens Bod.
2006a.
An all-subtrees approach to unsuper-vised parsing.
In Proceedings of COLING-ACL 2006.Rens Bod.
2006b.
Unsupervised parsing with U-DOP.In Proceedings of CoNLL 10.Alexander Clark.
2000.
Inducing syntactic categoriesby context distribution clustering.
In Proceedings ofCoNLL 4.Matthew W. Crocker, Martin Pickering, and CharlesClifton.
2000.
Architectures and Mechanisms forLanguage Processing.
Cambridge University Press.Dan Klein and Christopher D. Manning.
2002.
A gener-ative constituent-context model for improved grammarinduction.
In Proceedings of ACL 40, pages 128?135.Dan Klein and Christopher D. Manning.
2004.
Corpus-based induction of syntactic structure: Models of de-pendency and constituency.
In Proceedings of ACL 42.David McClosky, Eugene Charniak, and Mark Johnson.2006.
Effective self-training for parsing.
In Proceed-ings of HLT-NAACL 2006.Hinrich Schu?tze.
1995.
Distributional part-of-speechtagging.
In Proceedings of EACL 7.Patrick Sturt and Matthew W. Crocker.
1996.
Mono-tonic syntactic processing: A cross-linguistic study ofattachment and reanalysis.
Language and CognitiveProcesses, 11(5):449?492.391
