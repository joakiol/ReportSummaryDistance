Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1797?1806,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsUnsupervised Text Recap Extraction for TV SeriesHongliang Yu and Shikun Zhang and Louis-Philippe MorencyLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA, 15213, USA{yuhongliang, shikunz, morency}@cs.cmu.eduAbstractSequences found at the beginning of TVshows help the audience absorb the essenceof previous episodes, and grab their attentionwith upcoming plots.
In this paper, we pro-pose a novel task, text recap extraction.
Com-pared with conventional summarization, textrecap extraction captures the duality of sum-marization and plot contingency between ad-jacent episodes.
We present a new dataset,TVRecap, for text recap extraction on TVshows.
We propose an unsupervised modelthat identifies text recaps based on plot de-scriptions.
We introduce two contingency fac-tors, concept coverage and sparse reconstruc-tion, that encourage recaps to prompt the up-coming story development.
We also propose amulti-view extension of our model which canincorporate dialogues and synopses.
We con-duct extensive experiments on TVRecap, andconclude that our model outperforms summa-rization approaches.1 IntroductionAccording to a study by FX Networks, in U.S., thetotal number of ongoing scripted TV series hit a newhigh of 409 on broadcast, cable, and streaming in20151.
Such a large number indicates there are moreshows than anyone can realistically watch.
To attractprospective audiences as well as help current view-ers recall the key plot when airing new episodes,some TV shows add a clip montage, which is calleda recap sequence, at the beginning of new episodesor seasons.
Recaps not only help the audience1http://tinyurl.com/jugyyu2absorb the essence of previous episodes, but alsograb people?s attention with upcoming plots.
How-ever, creating those recaps for every newly airedepisode is labor-intensive and time-consuming.
Toour advantage, there are many textual scripts freelyavailable online which describe the events and ac-tions happening during the TV show episodes2.These textual scripts contain plot descriptions of theevents, dialogues of the actors, and sometimes alsothe synopsis summarizing the whole episode.These abundant textual resources enable us tostudy a novel, yet challenging task: automatictext recap extraction, illustrated in Figure 1.
Thegoal of text recap extraction is to identify seg-ments from scripts which both summarize the cur-rent episode and prompt the story development ofthe next episode.
This unique task brings newtechnical challenges as it goes beyond summariz-ing prior TV episodes, by introducing a concept ofplot contingency to the upcoming TV episode.
Itdiffers from conventional summarization techniqueswhich do not consider the interconnectivity betweenneighboring episodes.
Text recaps should capturethe duality of summarization and plot contingencybetween neighboring episodes.
To our knowledge,no dataset exists to study this research topic.In this paper, we present an unsupervised modelto automatically extrapolate text recaps of TV showsfrom plot descriptions.
Since we assume recapsshould cover the main plot of the current episodeand also prompt the story development of the nextepisode, our model jointly optimizes these two ob-2http://www.simplyscripts.com/tv_all.html1797We see Shannon and Sayid working on the translation.
Sayid finds the translation nonsense, slightly annoyed.
Shannon walks off, upset and frustrated with Sayid and herself.
Back to the robbery.
Hutton opens the door as Jason is pointing gun at him.Kate shoots Joson in the leg.
Kate opens the box which reveals an envelope inside.On-Island ?
Jack is with the case asking Kate to tell him what is inside.
Jack opens the box, and finds an envelope.
Kate opens the envelope and pulls out a small airplane.
After admitting it belongs to the man Kate loved and killed, Kate sits down and starts crying.
Jack looks nonplussed, he closes up the case and walks away.Shot of everyone moving up the beach.
Rose sitting by a tree, Charlie approaches.
Shot of Shannon walking up to Sayid on the beach.
Boone stares at Sayid and Shannon from behind a tree with a weird look on his face.
Kate just stares at her toy airplane.Next EpisodeCurrent EpisodeBoone is watching Shannon read from far away.
Sayid shows up, and hands a box to Shannon to thank for her help with the translation.Shannon opens the box which contains purple flowery shoes.
They continue talking as the shot switches to Boone watching them.Flashback - Shot of Boone with his arm around a girl, carrying tennis racket and ball, walking up steps from the tennis court to the pool area of a club.
Sound of a cell phone ringing.
Shannon is in a shaky voice.
Shannon is yelling at someone on her end.On-Island - Shot of Sayid limping along the beach.Boone confronts Sayid and tells him to stay away from his sister Shannon.
Locke calls Boone away.
Boone and Locke walk off into the jungle.
?
?Text RecapWe see Shannon and Sayid working on the translation.Kate opens the box which reveals an envelope inside.After admitting it belongs to the man Kate loved and killed, Kate sits down and starts crying.Boone stares at Sayid and Shannon from behind a tree with a weird look on his face.Text Recap ExtractionFigure 1: Illustration of text recap extraction.
The system extracts sentences from the current episode.
The text recap sentences inblack summarize the current episode, while colored sentences motivate the next episode.jectives.
To summarize the current episode, ourmodel exploits coverage-based summarization tech-niques.
To connect to the next episode, we devisetwo types of plot contingency factors between adja-cent episodes.
These factors implement the coverageand reconstruction assumptions to the next episode.We also show how our model can be extended to in-tegrate dialogues and synopses when available.We introduce a new dataset3, named TVRecapfor text recap extraction which consists of TV se-ries with textual scripts, including descriptions, di-alogues and synopses.
The dataset enables us tostudy whether contingency-based methods whichexploit relationships between adjacent episodes canimprove summarization-based methods.The rest of this paper is organized as follows.
InSection 2, we discuss related work and the motiva-tion for our work.
In Section 3, we introduce ournew dataset for text recap extraction.
Section 4 ex-plains our proposed model for text recap extraction,and Section 5 expands the model by incorporatingsynopses and dialogues.
In Section 6 and 7, wepresent our experimental results and analyses, andfinally conclude our work in Section 8.2 Related WorkIn this section, we discuss three related research top-ics.
Text summarization is an relevant task that aimsto create a summary that retains the most importantpoints of the original document.
Then we discuss the3http://multicomp.cs.cmu.eduevaluation metrics of text summarization.
Finally,we discuss the video description which is comple-mentary to our work.Generic Text Summarization Alogrithms Textsummarization is widely explored in the news do-main (Hong and Nenkova, 2014; McKeown, 2005).Generally, there are two approaches: extractive andabstractive summarization.Extractive summarization forms a summary bychoosing the most representative sentences from theoriginal corpus.
The early system LEAD (Was-son, 1998) was pioneering work.
It selected lead-ing text of the document as the summary, and wasapplied in news searching to help online customersfocus their queries on the beginning of news docu-ments.
He et al (2012) assumed that summarizationshould consist of sentences that could best recon-struct the original document.
They modeled rela-tionship among sentences by forming an optimiza-tion problem.
Moreover, Sipos et al (2012) andLin and Bilmes (2010) studied multi-document sum-marization using coverage-based methods.
Amongthem, Lin and Bilmes (2010) proposed to approxi-mate the optimal solution of a class of functions byexploiting submodularity.Abstractive summarization automatically createnew sentences.
For example, compared with thesentence-level analysis in extractive summarization,Bing et al (2015) explored fine-grained syntacticunits, i.e.
noun/verb phrases, to represent conceptsin input documents.
The informative phrases were1798then used to generate sentences.In this paper, we generalize the idea of text sum-marization to text recap extraction.
Instead of sum-marizing a given document or collection, our modelemphasizes plot contingency with the next episode.Summarization Applications Summarizationtechniques are not restricted to informative re-sources (e.g.
news), applications in broader areasare gaining attention (Apar?
?cio et al, 2016).
Asthe prevailance of online forums, Misra et al(2015) developed tools to recognize argumentsfrom opinionated conversations, and group themacross discussions.
In entertainment industry, Sangand Xu (2010) proposed a character-based moviesummarization approach by incorporating scriptsinto movie analysis.
Moreover, recent applicationsinclude multimedia artifact generation (Figueiredoet al, 2015), music summarization (Raposo et al,2015) and customer satisfaction analysis (Roy et al,2016).Video Description Generating video descriptionsis a task that studies automatic generation of naturallanguage that describes events happening in videoclips.
Most work uses sequential learning for en-coding temporal information and language genera-tion (Guadarrama et al, 2013; Rohrbach et al, 2013,2015; Donahue et al, 2015).
Our work is com-plementary to video description: the large numberof unlabeled videos can be utilized to train end-to-end recap extraction system when video descriptionmodels can properly output textual descriptions.Contributions of This Paper In contrast withprior work, the main contributions of this paper are:(1) We propose a novel problem, text recap extrac-tion for TV series.
Our task aims to identify seg-ments from scripts which both summarize the cur-rent episode and prompt the story development ofthe upcoming episode;(2) We propose an unsupervised model for text recapextraction from descriptions.
It models the episodecontingency through two factors, next episode sum-marization and sparse reconstruction;(3) We introduce a new dataset for TV show recapextraction, where descriptions, dialogues and syn-opses are provided.3 The TVRecap DatasetWe collected a new dataset, called TVRecap, for textrecap extraction on TV series.
We gathered andprocessed scripts, subtitles and synopses from web-sites4 as components to build our model upon.
Wealso established ground truth to help future researchon this challenging topic.
TVRecap includes all sea-sons from the widely-known show ?Lost?
with atotal of 106 episodes.
Statistics of our dataset areshown in Table 1.# sent.
avg.
# sent.
# words avg.
# w./s.description 14,686 138.5 140,684 9.57dialogue 37,714 355.8 284,514 7.54synopsis 453 4.27 7,868 17.36recap 619 17.19 5,892 9.52Table 1: Statistics of TVRecap.This section describes how textual scripts andsynopses are processed, and how we automaticallydefine the ground truth of text recap annotations.Descriptions, Dialogues and Synopses A scriptfor one TV series episode is a sequence of di-alogues interleaved with descriptions (marked bysquare brackets).
We automatically split the scriptinto descriptions and dialogues.
For each episode,We also downloaded the synopsis, a human-writtenparagraph summarizing the main plot of the episode.Figure 2 shows examples of a script and a synopsisfrom our TVRecap dataset.LOCKE: Two players.
Two sides.
One is light... one is dark.
Walt, do you want to know a secret?
[Claire writing in her diary.
Jin approaches and offers her some urchin.
She shakes her head, but then gives in and takes some.
]CLAIRE: No.
Thank you.
No, it's okay.
[Jin keeps insisting] No, really.
Okay.
Thanks.
(a) Script: containing descriptions and dialogues.Boone steals the decreasing water supply in a misguided attempt to help everyone, but the survivors turn on him.
A sleep-deprived Jack chases after what appears to be his deceased father in the forests and eventually discovers caves with fresh water.
Jack comes to terms with his role as leader.
In flashbacks, Jack goes to Australia to retrieve his deceased father.
(b) Synopsis.Figure 2: Example of a script (including descriptions and dia-logues) and a synopsis.4http://lostpedia.wikia.com/ and https://www.wikipedia.org/1799All plot descriptions and dialogues are time-aligned automatically using the subtitle files5.
Wefirst aligned the dialogue sentences from the scriptwith the subtitle files which contain time-stamps (inmilliseconds) of the spoken dialogues.
Then we es-timated time-stamps of description sentences usingsurrounding dialogues.Since descriptions sometimes contain words notrelevant to the event, we manually post-processedall descriptions and recap sentences as follows: (1)remove trivial sentences such as ?music on?, (2) re-move introductory terms like ?Shot of ?, (3) com-plete missing grammatical components (like omittedsubjects) of sentences when possible.Text Recap Annotations The goal of our groundtruth annotation is to identify the text descriptionsassociated with the TV show recap.
We performedthis annotation task in three steps.First, we automatically extracted the recap se-quence, which is a montage of important scenesfrom previous episodes to inform viewers of whathas happened in the show, from the TV show video.These recap sequences, if available, are alwaysshown at the beginning of TV episodes.
We auto-matically separated video recap sequences from full-length video files by detecting a lengthy appearanceof black frames in the first several minutes of theepisode.
Second, we located the frames of the re-cap sequences in the videos of previous episodes,and recorded their time-stamps.
Finally, the recapannotations are automatically identified by compar-ing the video time-stamps with the text descriptiontime-stamps.
A description is annotated as part ofthe recap if at least 4 frames from the video recapare present during this description.4 Our Text Recap Extraction ModelIn our Text Recap Extraction Model (TREM), weassume a good text recap should have two charac-teristics: (a) it covers the main plot of the currentepisode, and (b) it holds plot contingency with thenext episode.
Under the first assumption, the textrecap can be seen as a summarization that retainsthe most important plots.
Under assumption (b), thetext recap should capture the connections betweentwo consecutive episodes.5http://www.tvsubtitles.net/Formally, the system is given E episodes from aspecific TV show, where each episode contains tex-tual descriptions.
We define these descriptions asD = {D1, ?
?
?
, DE}, whereDi is the set of descrip-tions of episode i.
Di is composed of descriptivesentences as Di = {di1, ?
?
?
, di|Di|}, where dij is thej-th sentence.
The goal of our task is to find text re-caps R = {R1, ?
?
?
, RE?1} where the componentsof Ri are selected from Di with a length budget(constraint on the number of sentences) |Ri| ?
K.In our TREM model, the text recap Ri of the i-thepisode is optimized by:maxRi?DiF(Ri) = S(Ri, Di) +M(Ri, Di+1)s.t |Ri| ?
K,(1)where S(Ri, Di) measures how well Ri summa-rizes Di, and M(Ri, Di+1) quantifies the level ofconnectivity between the text recap of the currentepisode and the plot description of the next episode.By using M(?, ?
), we expect to produce text re-caps with better plot contingency with the upcomingstory.In the following sections, we demonstrate in de-tails: (1) the definition of the summarization func-tion S(?, ?
); (2) two factors that derive the contin-gency functionM(?, ?)
based on different hypothe-ses.4.1 Plot SummarizationIn this section, we discuss the summarization com-ponent of our model?s objective function.
Our modelis inspired by the coverage-based summarization(Lin and Bilmes, 2010), whose key idea is to finda proxy that approximates the information overlapbetween the summary and the original document.
Inthis work, any text is assumed to be represented by aset of ?concepts?
using weights to distinguish theirimportance.
To be more specific, a concept is de-fined as a noun/verb/adjective or noun/verb phrase.In terms of concepts, we define the summarizationterm S(Ri, Di) as follows:S(Ri, Di) =?c?C(Di)z(c,Di)maxr?Riw(c, r), (2)where C(Di) = {c?|c?
?
dij , ?dij ?
Di} is the con-cept set of Di, and z(c,Di) measures the impor-tance of c in Di.
We use Term Frequency Inverse1800Document Frequency (TF-IDF) (Salton and Buck-ley, 1988) to calculate z(c,Di).
Finally, w(c, r) de-notes the relatedness of a concept c to a sentence r.We use Word2Vec (Mikolov et al, 2013) vectorsas the semantic representation of concepts, and de-fine w(c, r) as:w(c, r) = |c| ?maxc?
?rcos(c, c?
), (3)where bold notations are the Word2Vec representa-tions of c and c?.
Note that if c is a phrase, c is meanpooled by the embeddings of its component words.|c| is the number of words in c.4.2 Plot ContingencyWe model plot contingency on the concept level aswell as on the sentence level.
Therefore, the compo-nentM(?, ?)
is decomposed into two factors:M(Ri, Di+1) =?sMs(Ri, Di+1)+?rMr(Ri, Di+1).
(4)where Ms(Ri, Di+1) measures how well Rican summarize the next episode Di+1 andMr(Ri, Di+1) is the factor that quantify theability of Ri to reconstruct Di+1.
?s, ?r ?
0 arecoefficients for Ms(?, ?)
and Mr(?, ?)
respectively.In the following sections, we define and explainthese two factors in details.4.2.1 Concept CoverageFollowing the coverage assumption of Section4.1, we argue that the text recap should also coverimportant concepts from the next episode.
There-fore, the first contingency factor can be definedin the same form as the summarization componentwhere Di?s in Equation 2 are replaced by Di+1?s:Ms(Ri, Di+1) =?c?C(Di+1)z(c,Di+1)maxr?Riw(c, r).
(5)4.2.2 Sparse ReconstructionAs events happening in the current episode canhave an impact on the next episode, there exist hid-den connections between the descriptive sentencesin Di and Di+1.
To be more specific, assuming de-scriptive sentences from Di+1 are dependent on afew sentences in Di, we aim to infer such hiddencontingency.
Here we assume that sentence di+1j isrelated to a small number of sentences in Di.Let ?i+1j ?
R|Di| be the indicator that determineswhich sentences in Di prompt di+1j , and W be thematrix that transforms these contingent sentences tothe embedding space of di+1j .
Intuitively, our modellearns W by assuming each sentence in Di+1 canbe reconstructed by contingent sentences from Di:di+1j ?WDi?i+1j , (6)In the equation, we first convert every descriptionsentence to its distributed representation using thepre-trained skip-thought model proposed by Kiroset al (2015).
The sentence embedding is denoted inbold (e.g.
dij for sentence dij).
Di = [di1; ?
?
?
;di|Di|]stacks the vector representations of all sentences inDi, and ?i+1j linearly combines the contingent sen-tences.We propose to jointly optimize ?i+1j and W by:min{?i+1}E?1i=1 ,W?i,j(?WDi?i+1j ?
di+1j ?22+ ??
?i+1j ?1)+ ?
?W?2F ,(7)where we denote ?i+1 = [?i+11 ; ?
?
?
;?i+1|Di+1|].
Weimpose sparsity constraint on ?i+1j with L1 normsuch that only a small fraction of sentences in Diwill be linked to di+1j .
?
and ?
are coefficients of theregularization terms.Given the optimal W?
from Equation 7, our mainobjective is to identify the subset of descriptions inDi that best capture the contingency betweenDi andDi+1.
The reconstruction contingency factor can bedefined as:Mr(Ri, Di+1) =?d?Di+1maxr?Rir?W?d.
(8)4.3 OptimizationIn this section, we describe our approach to optimizethe main objective function expressed in Equations1 and 7.Finding an efficient algorithm to optimize aset function like Equation 1 is often challenging.However, it can be easily shown that the objec-tive function of Equation 1 is submodular, sinceall its components S(Ri, Di),Ms(Ri, Di+1) and1801Mr(Ri, Di+1) are submodular with respect to Ri.According to Lin and Bilmes (2011), there exists asimple greedy algorithm for monotonic submodularfunction maximization where the solution is guar-anteed to be close to the real optimum.
Specifi-cally, if we denote Rigreedy as the approximation op-timized by greedy algorithm andRi?
as the best pos-sible solution, then F(Rigreedy) ?
(1?
1e ) ?
F(Ri?
),where F(?)
is the objective function of Equation 1and e ?
2.718 denotes the natural constant.
Thegreedy approach is shown in Algorithm 1.Algorithm 1 Text Recap ExtractionInput: Vectorized sentence representations{Di}Ei=1, parameters ?s, ?r, ?, ?, budget K,optimal W?
for Equation 7.Output: Text recaps {Ri}Ei=1.1: for i = 1, ?
?
?
, E2: Initialize Ri ?
?
;3: REPEAT4: r?
?
argmaxF(Ri ?
{r});5: Ri ?
Ri ?
{r?
};6: UNTIL |Ri| ?
K7: endAlgorithm 1 requires the optimal W?
learnedfrom the adjacent episode pairs in Equation 7.
Weutilize the algorithm that iteratively updates W and?
given the current solution.
At each iteration,each variable (W or {?i+1}) is updated by fixingthe other.
At t-th iteration, W(t) is computed asthe solution of ridge regression (Hoerl and Kennard,1970):W(t) = DX?(XX?
+ ?I)?1, (9)where D and X stack all di+1j and xi+1j ,Di?i+1j , ?i = 1, ?
?
?
, E ?
1, j = 1, ?
?
?
, |Di|.
Fix-ing W, each ?i+1j can be solved separately by gen-eral sparse coding algorithms as stated in Mairalet al (2009).
Algorithm 2 shows the optimizationprocess of Equation 7.5 Multi-View Recap ExtractionIn addition to plot descriptions, there are also dia-logues and plot synopses available for TV shows.Descriptions, dialogues and synopses can be seen asthree different views of the same TV show episode.Algorithm 2 Reconstruction Matrix OptimizationInput: Vectorized sentence representations{Di}Ei=1, ?
and ?.Output: Contingency matrix W.1: Initialize W(0) ?
I and ?i+1(0)j ?
0,?i, j;2: Initialize iteration step t?
0;3: REPEAT4: t?
t+ 1;5: W(t) is updated according to Equation 9;6: ?i, j,?i+1,(t)j ?
sparse coding(W(t));7: UNTIL ?W(t) ?W(t?1)?2F ?
Previously, we build TREM using plot descriptions.In this section, we expand our TREM model to in-corporate plot synopses and dialogues.
We definetext synopses and dialogues as S = {S1, ?
?
?
, SE}and T = {T 1, ?
?
?
, TE}, where Si and T i are theset of sentences from synopses and dialogues of thei-th episode.Dialogues In TV shows, a lot of useful informa-tion is presented via actors?
dialogues which moti-vates us to extend our TREM model to include di-alogues.
Both views can be used to identify recapsegments which are assumed to be summative andcontingent.
Denote the neighboring dialogues ofRi as N(Ri) = {t ?
T i??
?r ?
Ri, s.t.
|time(t) ?time(r)| < ?
}, we extend the optimization objective(Equation 1) into:F(Ri) =(S(Ri, Di) + S(N(Ri), T i))+(M(Ri, Di+1) +M(N(Ri), T i+1)).
(10)Synopses Since a synopsis is a concise summaryof each episode, we can treat plot summarization astext alignment where Ri is assumed to match thecontent of Si.
Therefore, the summarization termcan be redefined by substituting Di with Si:S(Ri, Si) =?c?C(Si)z(c, Si)maxr?Riw(c, r).
(11)Similarly, the contingency component can bemodified to include connections from synopses todetailed descriptions.
For Equation 8, we substitute1802ROUGE-1 ROUGE-2 ROUGE-SU4ILP-Ext (Banerjee et al, 2015) 0.308 0.112 0.091ILP-Abs (Banerjee et al, 2015) 0.361 0.158 0.120Our approach TREM 0.405 0.207 0.148w/o SR 0.393 0.189 0.144w/o CC 0.383 0.171 0.132w/o SR&CC (summarization only) 0.374 0.168 0.129Table 2: Experimental results on different methods using descriptions.
Contingency-based methods generally outperformssummarization-based methods.Di+1 to Si+1 where our model only focuses on high-level storyline:Mr(Ri, Si+1) =?s?Si+1maxr?Rir?W?s.
(12)6 Experimental SetupWe designed our experiments to evaluate whetherour TREM model, by considering contingency be-tween adjacent episodes, can achieve better resultsthan summarization techniques.
Furthermore, wewant to examine how each contingency factor asproposed in Section 4.2 contributes to the systemperformance.
As our model can integrate multipleviews, we want to dissect the effects of using differ-ent combinations of three views.6.1 Comparison ModelsTo answer the research questions presented above,we compare the following methods in our experi-ments.?
ILP-Ext and ILP-Abs (Banerjee et al, 2015):This summarizer generates sentences by optimizingthe integer linear programming problem in whichthe information content and linguistic quality are de-fined.
Both extractive and abstractive implementa-tions are used in our experiments.?
TREM: Our TREM model proposed in Section 4extracts sentences that can both summarize the cur-rent episode and prompt the next episode with twocontingency factors.?
TREM w/o SR: The TREM model without thesparse reconstruction factor proposed in Section4.2.2.?
TREM w/o CC: The TREM model without theconcept coverage factor proposed in Section 4.2.1.?
TREM w/o SR&CC: The summarization-onlyTREM model without contingency factors.
In therest of the paper, we also call it as TREM-Summ.?
Multi-view TREM: The augmented TREMmodel with descriptions, dialogues and synopses asproposed in Section 5.
Different views and combi-nations will be tested in our experiments.6.2 MethodologyUsing TVRecap, we measure the quality of gener-ated sentences following the standard metrics in thesummarization community, ROUGE (Lin and Hovy,2003).For the purpose of evaluation, we defined a de-velopment and a test set, by randomly selecting 18adjacent pairs of episodes from all seasons.
Theseepisodes were selected to have at least two recapdescription sentences.
The remaining 70 episodeswere only used during the learning process of W.After tuning hyper-parameters on development set,we report the comparison results on the test set.7 Results and Discussion7.1 Overall ResultsTable 2 shows our experimental results comparingTREM and baseline models using descriptions.In general, contingency-based methods (TREM,TREM w/o SR and TREM w/o CC) outperformsummarization-based methods.
Our contingencyassumptions are verified as adding CC and SCboth improve TREM with summarization compo-nent only.
Moreover, the best result is achieved bythe complete TREM model with both contingencyfactors.
It suggests that these two factors, modelingword-level summarization and sentence-level recon-struction, are complementary.From the summarization-based methods, we cansee that our TREM-Summ gets higher ROUGEscores than two ILP approaches.
Additionally, we1803Target sentence from next episode Sentences with highest reconstruction value from current episodeKate is putting water bottles in a pack.We see three bottles of water.They go into a room with a body bag on a gurney.Kate is going through clothes, as Claire approaches.Locke is with his knife case, holding a pencil, sitting by a fire.Boone is coming up to camp and sees Locke sitting by a fire.Locke throws a knife into the ground, just out of Boone?s reach.Boone quickly cuts through the ropes and starts running.In another part of the temple grounds, Miles and Hurley areplaying Tic-Tac-Toe by placing leaves in a grid of sticks onthe ground.John contemplates the fabric swatches he is holding.On the beach, Frank covers Locke?s body with a tarp.Helen closes the door and brings the case inside to the kitchen.Table 3: A case study on sparse reconstruction as proposed in Section 4.2.2.
Sentences in the first column are reconstructed bysentences in the second column.
The first two examples successfully captures related sentences, while the third example fails.note that the performance of ILP-Ext is poor.
Thisis because ILP-Ext tends to output short sentences,while ROUGE is a recall-oriented measurement.Model Current Next R-1 R-2 R-SU4TREM-Summdes - 0.374 0.168 0.129syn - 0.369 0.163 0.121dial - 0.354 0.138 0.115des+syn - 0.384 0.172 0.132des+dial - 0.386 0.168 0.135TREMdes des 0.405 0.207 0.148des syn 0.411 0.219 0.154des dial 0.375 0.158 0.127des des+syn 0.409 0.210 0.154des des+dial 0.395 0.177 0.142Table 4: Comparison of views in summarization-only TREMand full TREM with contingency factors.
?des?, ?syn?, and?dial?
are abbreviated for description, synopses and dialogues.7.2 Multi-view ComparisonAs shown in Table 4, The second study examinesthe effect of different views in both types of methodsusing the TREM model.
In single-view summariza-tion, TREM-Summ with descriptions outperformsmethods based on the other two views.
In terms ofhybrid of views, only ROUGE-1 is significantly im-proved, while ROUGE-2 and ROUGE-SU4, whichfocus more on semantic consistency, have little im-provement.In contingency-based methods, we keep the cur-rent episode represented as descriptions which ob-tain the best performance in single-view summa-rization, and change the views of the next episode.Comparing the model using descriptions with theone fusing descriptions and synopses, we can seethat simply adding views does not guarantee higherROUGE scores.
In both TREM-Summ and fullTREM, dialogue is inferior to others.
It might be be-cause dialogues contain too many trivial sentences.Synopses, however, are relatively short, but providekey plots to summarize the story, and hence achievethe best ROUGE scores.7.3 Qualitative Study on SparseReconstructionIn this section, we give some examples to illustratethe process of sparse reconstruction.
Equation 7assumes that each descriptive sentence can be re-constructed by a few sentences from the previousepisode.
Table 3 shows three examples of sentenceswith their top-3 reconstructive sentences, which aredefined by values in the indicator vector ?i+1j .7.4 Limitations and Future WorkTREM restrains the contingency within adjacentepisodes.
However, storylines sometimes proceedthrough multiple episodes.
In our model, with moreconnectivity termsM(Ri, Dj) where i < j, we candevelop more general system with longer dependen-cies.While our model and dataset are appropriate fortext recap extraction and algorithm comparison, thistask can be further applied to multimedia settings,where visual or acoustic information can be in-cluded.
Therefore, in future work, we plan to expandour work to broader applications where intercon-nectivity between consecutive instances is crucial,such as educational lectures, news series and bookchapters.
Specifically, TREM can be integrated withvideo description results to get an end-to-end systemthat produces video recaps.18048 ConclusionIn this paper, we explore a new problem of textrecap extraction for TV shows.
We propose anunsupervised model that identifies recap segmentsfrom multiple views of textual scripts.
To facili-tate the study of this new research topic, we cre-ate a dataset called TVRecap, which we test ourapproach on.
From the experimental results, weconclude that contingency-based methods improvesummarization-based methods at ROUGE measure-ments by exploiting plot connection between adja-cent episodes.AcknowledgementThis material is based in part upon work partiallysupported by the National Science Foundation (IIS-1523162).
Any opinions, findings, and conclusionsor recommendations expressed in this material arethose of the author(s) and do not necessarily reflectthe views of the National Science Foundation.ReferencesMarta Apar?
?cio, Paulo Figueiredo, Francisco Ra-poso, David Martins de Matos, Ricardo Ribeiro,and Lu?
?s Marujo.
2016.
Summarization of filmsand documentaries based on subtitles and scripts.Pattern Recognition Letters 73:7?12.Siddhartha Banerjee, Prasenjit Mitra, and Kazu-nari Sugiyama.
2015.
Multi-document abstrac-tive summarization using ilp based multi-sentencecompression.
In 24th International Joint Con-ference on Artificial Intelligence (IJCAI).
BuenosAires, Argentina: AAAI press.Lidong Bing, Piji Li, Yi Liao, Wai Lam, Wei-wei Guo, and Rebecca J Passonneau.
2015.Abstractive multi-document summarization viaphrase selection and merging.
arXiv preprintarXiv:1506.01597 .Jeffrey Donahue, Lisa Anne Hendricks, Ser-gio Guadarrama, Marcus Rohrbach, SubhashiniVenugopalan, Kate Saenko, and Trevor Darrell.2015.
Long-term recurrent convolutional net-works for visual recognition and description.
InProceedings of the IEEE Conference on Com-puter Vision and Pattern Recognition.
pages2625?2634.Paulo Figueiredo, Marta Apar?
?cio, David Martinsde Matos, and Ricardo Ribeiro.
2015.
Gen-eration of multimedia artifacts: An extractivesummarization-based approach.
arXiv preprintarXiv:1508.03170 .Sergio Guadarrama, Niveda Krishnamoorthy, GirishMalkarnenkar, Subhashini Venugopalan, Ray-mond Mooney, Trevor Darrell, and Kate Saenko.2013.
Youtube2text: Recognizing and describ-ing arbitrary activities using semantic hierarchiesand zero-shot recognition.
In Proceedings of theIEEE International Conference on Computer Vi-sion.
pages 2712?2719.Zhanying He, Chun Chen, Jiajun Bu, Can Wang, Li-jun Zhang, Deng Cai, and Xiaofei He.
2012.
Doc-ument summarization based on data reconstruc-tion.
In AAAI.Arthur E Hoerl and Robert W Kennard.
1970.
Ridgeregression: Biased estimation for nonorthogonalproblems.
Technometrics 12(1):55?67.Kai Hong and Ani Nenkova.
2014.
Improving theestimation of word importance for news multi-document summarization.
In EACL.
pages 712?721.Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov,Richard S Zemel, Antonio Torralba, Raquel Urta-sun, and Sanja Fidler.
2015.
Skip-thought vectors.arXiv preprint arXiv:1506.06726 .Chin-Yew Lin and Eduard Hovy.
2003.
Auto-matic evaluation of summaries using n-gram co-occurrence statistics.
In Proceedings of the 2003Conference of the North American Chapter of theAssociation for Computational Linguistics on Hu-man Language Technology-Volume 1.
Associationfor Computational Linguistics, pages 71?78.Hui Lin and Jeff Bilmes.
2010.
Multi-documentsummarization via budgeted maximization ofsubmodular functions.
In Human Language Tech-nologies: The 2010 Annual Conference of theNorth American Chapter of the Association forComputational Linguistics.
Association for Com-putational Linguistics, pages 912?920.Hui Lin and Jeff Bilmes.
2011.
A class of submod-ular functions for document summarization.
InProceedings of the 49th Annual Meeting of the1805Association for Computational Linguistics: Hu-man Language Technologies-Volume 1.
Associa-tion for Computational Linguistics, pages 510?520.Julien Mairal, Francis Bach, Jean Ponce, andGuillermo Sapiro.
2009.
Online dictionary learn-ing for sparse coding.
In Proceedings of the26th annual international conference on machinelearning.
ACM, pages 689?696.Kathleen McKeown.
2005.
Text summarization:News and beyond.
In Proceedings of the Aus-tralasian Language Technology Workshop.
pages4?4.Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-frey Dean.
2013.
Efficient estimation of wordrepresentations in vector space.
arXiv preprintarXiv:1301.3781 .Amita Misra, Pranav Anand, JEF Tree, andMA Walker.
2015.
Using summarization to dis-cover argument facets in online idealogical dia-log.
In NAACL HLT .
pages 430?440.Francisco Raposo, Ricardo Ribeiro, and David Mar-tins de Matos.
2015.
On the application of genericsummarization algorithms to music.
IEEE SignalProcessing Letters 22(1):26?30.Anna Rohrbach, Marcus Rohrbach, and BerntSchiele.
2015.
The long-short story of movie de-scription.
In Pattern Recognition, Springer, pages209?221.Marcus Rohrbach, Wei Qiu, Ivan Titov, StefanThater, Manfred Pinkal, and Bernt Schiele.
2013.Translating video content to natural language de-scriptions.
In Proceedings of the IEEE Inter-national Conference on Computer Vision.
pages433?440.Shourya Roy, Ragunathan Mariappan, SandipanDandapat, Saurabh Srivastava, Sainyam Galhotra,and Balaji Peddamuthu.
2016.
Qa rt: A systemfor real-time holistic quality assurance for contactcenter dialogues.
In Thirtieth AAAI Conferenceon Artificial Intelligence.Gerard Salton and Christopher Buckley.
1988.Term-weighting approaches in automatic text re-trieval.
Information processing & management24(5):513?523.Jitao Sang and Changsheng Xu.
2010.
Character-based movie summarization.
In Proceedings ofthe 18th ACM international conference on Multi-media.
ACM, pages 855?858.Ruben Sipos, Adith Swaminathan, Pannaga Shiv-aswamy, and Thorsten Joachims.
2012.
Tempo-ral corpus summarization using submodular wordcoverage.
In Proceedings of the 21st ACM in-ternational conference on Information and knowl-edge management.
ACM, pages 754?763.Mark Wasson.
1998.
Using leading text for newssummaries: Evaluation results and implicationsfor commercial summarization applications.
InProceedings of the 17th international conferenceon Computational linguistics-Volume 2.
Associa-tion for Computational Linguistics, pages 1364?1368.1806
