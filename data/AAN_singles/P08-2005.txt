Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 17?20,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsExtractive Summaries for Educational Science ContentSebastian de la Chica, Faisal Ahmad, James H. Martin, Tamara SumnerInstitute of Cognitive ScienceDepartment of Computer ScienceUniversity of Colorado at Bouldersebastian.delachica, faisal.ahmad, james.martin,tamara.sumner@colorado.eduAbstractThis paper describes an extractive summarizerfor educational science content calledCOGENT.
COGENT extends MEAD basedon strategies elicited from an empirical studywith domain and instructional experts.COGENT implements a hybrid approach inte-grating both domain independent sentencescoring features and domain-aware features.Initial evaluation results indicate thatCOGENT outperforms existing summarizersand generates summaries that closely resem-ble those generated by human experts.1 IntroductionKnowledge maps consist of nodes containing richconcept descriptions interconnected using a limitedset of relationship types (Holley and Dansereau,1984).
Learning research indicates that knowledgemaps may be useful for learners to understand themacro-level structure of an information space(O'Donnell et al, 2002).
Knowledge maps havealso emerged as an effective computational infra-structure to support the automated generation ofconceptual browsers.
Such conceptual browsersappear to allow students to focus on the sciencecontent of large educational digital libraries (Sum-ner et al, 2003), such as the Digital Library forEarth System Education (DLESE.org).
Knowledgemaps have also shown promise as domain and stu-dent knowledge representations to support person-alized learning interactions (de la Chica et al,2008).In this paper we describe our progress towardsthe generation of science concept inventories assummaries of digital library collections.
Such in-ventories provide the basis for the construction ofknowledge maps useful both as computationalknowledge representations and as learning re-sources for presentation to the student.2 Related WorkOur work is informed by efforts to automate theacquisition of ontology concepts from text.
On-toLearn extracts candidate domain terms from textsusing a syntactic parse and updates an existing on-tology with the identified concepts and relation-ships (Navigli and Velardi, 2004).
KnowledgePuzzle focuses on n-gram identification to producea list of candidate terms pruned using informationextraction techniques to derive the ontology(Zouaq et al, 2007).
Lin and Pantel (2002) dis-cover concepts using clustering by committee togroup terms into conceptually related clusters.These approaches produce ontologies of very finegranularity and therefore graphs that may not besuitable for presentation to a student.Multi-document summarization (MDS) re-search also informs our work.
XDoX analyzeslarge document sets to extract important themesusing n-gram scoring and clustering (Hardy et al,2002).
Topic representation and topic themes havealso served as the basis for the exploration ofpromising MDS techniques (Harabagiu and Laca-tusu, 2005).
Finally, MEAD is a widely used MDSand evaluation platform (Radev et al, 2000).While all these systems have produced promisingresults in automated evaluations, none have di-rectly targeted educational content collections.173 Empirical StudyWe have conducted a study to capture how humanexperts processed digital library resources to createa domain knowledge map.
Four geology and in-structional design experts selected 20 resourcesfrom DLESE to construct a knowledge map onearthquakes and plates tectonics for high schoolage learners.
The resulting knowledge map con-sists of 564 concepts and 578 relationships.Figure 1.
Expert knowledge map excerptThe concepts include 7,846  words, or 5% ofthe resources.
Our experts relied on copying-and-pasting (58%) and paraphrasing (37%) to createmost concepts.
Only 5% of the concepts could notbe traced directly to the original resources.
Rela-tionship types were used in a Zipf-like distributionwith the top 2 relationship types each accountingfor more than 10% of all relationships: elabora-tions (19%) and examples (14%).Analysis by an independent instructional expertindicates that this knowledge map provides ade-quate coverage of nationally-recognized educa-tional goals on earthquakes and plate tectonics forhigh school learners using the American Associa-tion for the Advancement of Science (AAAS)Benchmarks (Project 2061, 1993).Verbal protocol analysis shows that all expertsused external sources to create the knowledge map,including their own expertise, other digital libraryresources, and the National Science EducationStandards (NSES), a comprehensive collection ofnationally-recognized science learning goals for K-12 students (National Research Council, 1996).We have examined sentence extraction agree-ment between experts using the prevalence-adjusted bias-adjusted (PABA) kappa to accountfor prevalence of judgments and conflicting biasesamongst experts (Byrt et al, 1993).
The averagePABA-kappa value of 0.62 indicates that expertssubstantially agree on sentence extraction fromdigital library resources.
This level of agreementsuggests that these concepts may serve as the ref-erence summary to evaluate our system.4 Summarizer for Science EducationWe have implemented an extractive summarizerfor educational science content, COGENT, basedon MEAD version 3.11 (Radev et al, 2000).COGENT complements the default MEAD sen-tence scoring features with features based on find-ings from the empirical study.
COGENTrepresents a hybrid approach integrating bottom-up(hypertext and content word density) and top-down(educational standards and gazetteer) features.We model how human experts used external in-formation sources with the educational standardsfeature.
This feature leverages the text of the rele-vant AAAS Benchmarks and associated NSES.Each sentence receives a score based on its TFIDFsimilarity to the textual contents of these learninggoals and educational standards.We have developed a feature that reflects thelarge number of examples extracted by the experts.Earth science examples often refer to geographicallocations and geological formations.
The gazetteerfeature checks named entities from each sentenceagainst the Alexandria Digital Library (ADL) Gaz-etteer (Hill, 2000).
A gazetteer is a geo-referencingresource containing location and type informationabout place-names.
Each sentence receives aTFIDF score based on place-name term frequencyand overall uniqueness in the gazetteer.
Our as-sumption is that geographical locations with moreunique names may be more pedagogically relevant.Based on the intuition that the HTML structureof a resource reflects relevancy, we have devel-oped the hypertext feature.
This feature computes asentence score directly proportional to the HTMLheading level and inversely proportional to therelative paragraph number within a heading and tothe relative sentence position within a paragraph.18To promote the extraction of sentences contain-ing science concepts, we have developed the con-tent word density feature.
This feature computesthe ratio of content to function words in a sentence.Function words are identified using a stopword list,and the feature only keeps sentences featuringmore content words than function words.We compute the final sentence score by addingthe MEAD default feature scores (centroid andposition) to the COGENT feature scores (educa-tional standards, gazetteer, and hypertext).COGENT keeps sentences that pass the cut-offconstraints, including the MEAD sentence lengthof 9 and COGENT content word density of 50%.The default MEAD cosine re-ranker eliminatesredundant sentences.
Since the experts used 5% ofthe total word count in the resources, we producesummaries of that same length.5 EvaluationWe have evaluated COGENT by processing the 20digital library resources used in the empirical studyand comparing the output against the conceptsidentified by the experts.
Three configurations areconsidered: Random, Default, and COGENT.
TheRandom summary uses MEAD to extract randomsentences.
The Default summary uses the MEADcentroid, position and length default features.
Fi-nally, the COGENT summary extends MEAD withthe COGENT features.We use ROUGE (Lin, 2004) to assess summaryquality using common n-gram counts and longestcommon subsequence (LCS) measures.
We reporton ROUGE-1 (unigrams), ROUGE-2 (bigrams),ROUGE W-1.2 (weighted LCS), and ROUGE-S*(skip bigrams) as they have been shown to corre-late well with human judgments for longer multi-document summaries (Lin, 2004).
Table 1 showsthe results for recall (R), precision (P), and bal-anced f-measure (F).Random Default COGENTR 0.4855 0.4976 0.6073P 0.5026 0.5688 0.6034 R-1F 0.4939 0.5308 0.6054R 0.0972 0.1321 0.1907P 0.1006 0.1510 0.1895 R-2F 0.0989 0.1409 0.1901R 0.0929 0.0951 0.1185P 0.1533 0.1733 0.1877 R-W-1.2F 0.1157 0.1228 0.1453Random Default COGENTR 0.2481 0.2620 0.3820P 0.2657 0.3424 0.3772 R-S*F 0.2566 0.2969 0.3796Table 1.
Quality evaluation resultsTable 1 indicates that COGENT consistentlyoutperforms the Random and Default summaries.These results indicate the promise of our approachto generate extractive summaries of educationalscience content.
Given our interest in generating apedagogically effective domain knowledge map,we have also conducted a content-centric evalua-tion.To characterize the COGENT summary con-tents, one of the authors manually constructed asummary corresponding to the best case output foran extractive summarizer.
This Best Case summarycomprises all the sentences from the resources thatalign to all the concepts selected by the experts.This summary comprises 621 sentences consistingof 13,116 words, or about a 9% word compression.We use ROUGE-L to examine the union LCSbetween the reference and candidate summaries,thus capturing their linguistic surface structuresimilarity.
We also use MEAD to report on cosinesimilarity.
Table 2 shows the results for recall (R),precision (P), and balanced f-measure (F).Random(5%)Default(5%)COGENT(5%)Best Case(9%)R 0.4814 0.4919 0.6021 0.9669P 0.4982 0.5623 0.5982 0.6256 R-LF 0.4897 0.5248 0.6001 0.7597Cosine 0.5382 0.6748 0.8325 0.9323Table 2.
Content evaluation results (word compression)The ROUGE-L scores consistently indicate thatthe COGENT summary may be closer to the refer-ence in linguistic surface structure than either theRandom or Default summaries.
Since theCOGENT ROUGE-L recall score (R=0.
6021) islower than the Best Case (R=0.9669), it is likelythat COGENT may be extracting different sen-tences than those selected by the experts.
Based onthe high cosine similarity with the reference(0.8325), we hypothesize that COGENT may beselecting sentences that cover very similar con-cepts to those selected by the experts, but ex-pressed differently.Given the difference in word compression forthe Best Case summary, we have performed an19incremental analysis using the ROUGE-L measureshown in Figure 2.ROUGE-L COGENT Evaluation0.000.100.200.300.400.500.600.700.800.901.000 5 10 15 20 25 30MEAD Word Percent CompressionRecall Precision F-MeasureFigure 2.
Incremental COGENT ROUGE-L analysisFigure 2 indicates that COGENT can match theBest Case recall (R=0.9669) by generating a longersummary.
For educational applications, lengthiersummaries may be better suited for computationalpurposes, such as diagnosing student understand-ing, while shorter summaries may be more appro-priate for display to the student.6 ConclusionsCOGENT extends MEAD based on strategies elic-ited from an empirical study with domain and in-structional experts.
Initial evaluation resultsindicate that COGENT holds promise for identify-ing important domain pedagogical concepts.
Weare exploring portability to other science educationdomains and machine learning techniques to con-nect concepts into a knowledge map.
Automatingthe creation of inventories of pedagogically impor-tant concepts may represent an important step to-wards scalable intelligent tutoring systems.AcknowledgementsThis research is funded in part by the National Sci-ence Foundation under NSF IIS/ALT Award0537194.
Any opinions, findings, and conclusionsor recommendations expressed in this material arethose of the author(s) and do not necessarily reflectthe views of the NSF.ReferencesT.
Byrt, J. Bishop and J.
B. Carlin.
Bias, prevalence, andkappa.
Journal of Clinical Epidemiology, 46, 5(1993), 423-429.S.
de la Chica, F. Ahmad, T. Sumner, J. H. Martin andK.
Butcher.
Computational foundations for personal-izing instruction with digital libraries.
InternationalJournal of Digital Libraries, to appear in the SpecialIssue on Digital Libraries and Education.S.
Harabagiu and F. Lacatusu.
Topic themes for multi-document summarization.
In Proc.
of the 28th An-nual International ACM SIGIR Conference on Re-search and Development in Information Retrieval,(Salvador, Brazil, 2005), 202-209.H.
Hardy, N. Shimizu, T. Strzalkowski, L. Ting, G. B.Wise and X. Zhang.
Summarizing large documentsets using concept-based clustering.
In Proc.
of theHuman Language Technology Conference 2002,(San Diego, California, United States, 2002), 222-227.L.
L. Hill.
Core elements of digital gazetteers: place-names, categories, and footprints.
In Proc.
of the 4thEuropean Conference on Digital Libraries, (Lisbon,Portugal, 2000), 280-290.C.
D. Holley and D. F. Dansereau.
Spatial learningstrategies: Techniques, applications, and related is-sues.
Academic Press, Orlando, Florida, 1984.C.
Y. Lin.
ROUGE: A package for automatic evaluationof summaries.
In Proc.
of the Workshop on TextSummarization Branches Out, (Barcelona, Spain,2004).D.
Lin and P. Pantel.
Concept discovery from text.
InProc.
of the 19th International Conference on Com-putational Linguistics, (Taipei, Taiwan, 2002), 1-7.National Research Council.
National Science EducationStandards.
National Academy Press, Washington,DC, 1996.R.
Navigli and P. Velardi.
Learning domain ontologiesfrom document warehouses and dedicated websites.Computational Linguistics, 30, 2 (2004), 151-179.A.
M. O'Donnell, D. F. Dansereau and R. H. Hall.Knowledge maps as scaffolds for cognitive process-ing.
Educational Psychology Review, 14, 1 (2002),71-86.Project 2061.
Benchmarks for science literacy.
OxfordUniversity Press, New York, New York, UnitedStates, 1993.D.
R. Radev, H. Jing and M. Budzikowska.
Centroid-based summarization of multiple documents: sen-tence extraction, utility-based evaluation, and userstudies.
In Proc.
of the ANLP/NAACL 2000 Work-shop on Summarization, (2000), 21-30.T.
Sumner, S. Bhushan, F. Ahmad and Q. Gu.
Design-ing a language for creating conceptual browsing in-terfaces for digital libraries.
In Proc.
of the 3rdACM/IEEE-CS Joint Conference on Digital Librar-ies, (Houston, Texas, 2003), 258-260.A.
Zouaq, R. Nkambou and C. Frasson.
Learning a do-main ontology in the Knowledge Puzzle project.
InProc.
of the Fifth International Workshop on Ontolo-gies and Semantic Web for E-Learning, (Marina delRey, California, 2007).20
