Competence and Performance Grammarin Incremental ProcessingVincenzo LombardoDipartimento di InformaticaUniversita` di Torinoc.so Svizzera, 18510149, Torino, Italyvincenzo@di.unito.itAlessandro MazzeiDipartimento di InformaticaUniversita` di Torinoc.so Svizzera, 18510149, Torino, Italymazzei@di.unito.itPatrick SturtDepartment of PsychologyUniversity of Glasgow58 Hillhead StreetGlasgow, G12 8QB, UKpatrick@psy.gla.ac.ukAbstractThe goal of this paper is to explore some conse-quences of the dichotomy between competenceand performance from the point of view of in-crementality.
We introduce a TAG?based for-malism that encodes a strong notion of incre-mentality directly into the operations of theformal system.
A left-associative operation isused to build a lexicon of extended elementarytrees.
Extended elementary trees allow deriva-tions in which a single fully connected struc-ture is mantained through the course of a left-to-right word-by-word derivation.
In the paper,we describe the consequences of this view forsemantic interpretation, and we also evaluatesome of the computational consequences of en-larging the lexicon in this way.1 IntroductionIncremental processing can be achieved with acombination of grammar formalism and deriva-tion/parsing strategy.
In this paper we exploresome of the computational consequences of de-riving the incremental character of the humanlanguage processor from the competence gram-mar.
In the following paragraphs, we assumethat incremental processing proceeds through asequence of processing steps.
Each step consistsof a configuration of partial syntactic structures(possibly connected into only one structure) anda configuration of semantic structures (again,possibly connected into one single expression).These semantic structures result from the ap-plication of the semantic interpreter to the syn-tactic structures in the same processing step.Depending on the semantic rules, some syntac-tic structures may not be interpretable?thatis, some processing steps do not involve an up-dating of the semantic representation.
In theview we present here, competence grammar isresponsible for the definition of both the set ofwell-formed sentences of the language and theset of possible partial structures that are yieldedby the derivation process.
According to thisview, the performance component is responsiblefor other aspects of language processing, includ-ing ambiguity handling and error handling.
Thelatter issues are not addressed in this paper.In the psycholinguistic and computational lit-erature, many models for incremental process-ing have been discussed.
These models can becharacterized in terms of the location of the bor-der between competence and performance.
Inparticular, we discuss the relative responsibilityof the competence and performance componentson three key areas of syntactic processing: a)the space of well-formed partial syntactic struc-tures; b) the space of the possible configurationsof partial syntactic structures at each process-ing step; c) the sub-space of partial structuresthat can actually be interpreted.The definition of well-formedness is almostuniversally assigned to the competence compo-nent, whether in a direct implementation of thegrammar formalism (cf.
the Type Transparencyhypothesis (Berwick and Weinberg, 1984)) or acompiled version of the competence grammar(e.g.
LR parsing (Shieber and Johnson, 1993)).The space of the possible configurations ofpartial structures refers to those partial syntac-tic structures that are built and stored duringparsing or derivation.
Different algorithms re-sult in different possibilities for the configura-tions of partial structures that the parser builds.For example, a bottom?up algorithm will neverbuild a partial structure with non?terminal leafnodes.
The standard approach is to assign thisresponsibility to the parsing algorithm, whetherthe grammar is based on standard context-freeformalisms (Roark, 2001), on generative syntac-tic theories based on a context-free backbone(Crocker, 1992), or on categorial approaches,like e.g.
Combinatory Categorial Grammar(CCG ?
(Steedman, 2000)).
A different methodis to assign this responsibility to the compe-tence component.
In this case the space ofpossible configurations of partial structures isconstrained by the grammatical derivation pro-cess itself, and the parsing algorithm needs tobe aligned with these requirements.
This ap-proach is exemplified by the works of Kempsonet al (2000) and Phillips (2003), who arguethat many problems in theoretical syntax, likethe definition of constituency, can be solved byextending this responsability to the competencegrammar.This issue of constituency is also relevant inthe third key area, which is the definition of thespace of interpretable structures.
The assign-ment of responsibility with respect to currentapproaches usually depends on the implementa-tion of the incremental technique.
Approachesbased on a coupling of syntactic and seman-tic rules in the competence grammar (Steed-man, 2000; Kempson et al, 2000) adhere to theso-called Strict Competence Hypothesis (Steed-man, 2000), which constrains the interpreter todeal only with grammatical constituents, so theresponsibility for deciding the interpretable par-tial structures is assigned to competence1.
Incontrast, approaches that are based on com-petence grammars that do not include seman-tic rules, like CFG, implement semantic inter-preters that mimic such semantic rules (Stabler,1991), and so they assign the responsibility fordeciding the interpretable partial structures toperformance.In this paper we explore the empirical con-sequences of building a realistic grammar whenthe formalism constrains all these three areas,as is the case with Kempson et al (2000)and Phillips (2003).
The work relies upon theDynamic Version of Lexicalized Tree Adjoin-ing Grammar (DV?TAG), introduced in (Lom-bardo and Sturt, 2002b), a formalism that en-codes a dynamic grammar (cf.
(Milward, 1994))in LTAG terms (Joshi and Schabes, 1997).
Theconsequence of encoding a dynamic grammaris that the configurations of partial structuresdiscussed above are limited to fully connectedstructures, that is no disconnected structuresare allowed in a configuration.
In particular,the paper focuses on the problem of building arealistic DV?TAG grammar through a conver-sion from an LTAG, in order to maintain the1Notice that these approaches may, however, differ inthe time-course with which semantic rules are appliedin the interpreter, and this issue depends directly onthe space of configurations of partial structures discussedabovelinguistic significance of elementary trees whileextending them to allow the full connectivity.2 Dynamic Version of TreeAdjoining GrammarThis section reviews the major aspects of theDynamic Version of Tree Adjoining Grammar(DV?TAG), with special reference to similari-ties and differences with respect to LTAG.Dynamic grammars define well-formedness interms of states and transitions between states.They allow a natural formulation of incrementalprocessing, where each word wi defines a tran-sition from Statei?1, also called the left context,to Statei (Milward, 1994).
The states can bedefined as partial syntactic or semantic struc-tures that are ?updated?
as each word is recog-nized; roughly speaking, two adjacent states canbe thought of as two parse trees before and af-ter the attachment of a word, respectively.
Thederivation process proceeds from left to rightby extending a fully connected left context toinclude the next input word.Like an LTAG (Joshi and Schabes, 1997), aDynamic Version of Tree Adjoining Grammar(DV?TAG) consists of a set of elementary trees,divided into initial trees and auxiliary trees,and attachment operations for combining them.Lexicalization is expressed through the associ-ation of a lexical anchor with each elementarytree.
The anchor defines the semantic content ofthe elementary tree: the whole elementary treecan be seen as an extended projection of the an-chor (Frank, 2000).
LTAG is said to define anextended domain of locality ?unlike context-freegrammars, which use rules that describe one?branch deep fragments of trees, TAG elemen-tary trees can describe larger structures (e.g.
averb, its maximal S node and subject NP node).In figures 1(a) and 2(a) we can see the ele-mentary trees for a derivation of the sentenceBill often pleases Sue for LTAG and DV?TAGrespectively.
Auxiliary trees in DV?TAG aresplit into left auxiliary trees, where the lexicalanchor is on the left of the foot node, and rightauxiliary trees, where the lexical anchor is onthe right of the foot node.
The tree anchoredby often in fig.
2(a) is a left auxiliary tree.Non-terminal nodes have a distinguishedhead daughter, which provides the lexical headof the mother node: unlike in LTAG, each nodein the elementary trees is augmented with a fea-ture indicating the lexical head that projectsthe node.
This feature is needed for the no-Bill often pleases Sue.NNPSueNPADVoftenADVPVPVP*NPNNPSueSVpleasesNPVP NNPBillADVoftenADVPVPVpleasesNP$NP$SVPadjunctionBillpleasesSueoften(a)(b)(c)substitutionNNPBillNPsubstitution Bill pleasesoften SueFigure 1: The LTAG derivation of the sentenceBill often pleases Sue.NNPSueNP(Sue)ADVoftenADVP(often)VP(_j)VP*(_j)NP(Sue)NNPSueS(pleases)VpleasesNP(Bill)VP(pleases)NNPBillADVoftenADVP(often)VP(pleases)V(_i) NP$(_k)NNPBillNP(Bill)S(_i)VP( i)pleaseslikeseatsplays?1.
adjunctionfrom the left2.
shiftBillpleasesSueoften(a)(b)(c)3. substitutionFigure 2: The DVTAG derivation of the sen-tence Bill often pleases Sue.tion of derivation?dependency tree (see below).If several unheaded nodes share the same lexicalhead, they are all co-indexed with a head vari-able (e.g.
i in the elementary tree anchored byBill in figure 2(a)); the head variable is a vari-able in logic terms: i will be unified with theconstant (?lexical head?)
pleases.In both LTAG and DV?TAG the lexical an-chor does not necessarily provide the head fea-ture of the root of the elementary tree.
This istrivially true for auxiliary trees (e.g.
the treeanchored often in figure 1(a) and figure 2(a)).However, in DV?TAG this can also occur withinitial trees (e.g.
the tree anchored by Bill infigure 2(a)), because initial trees can includenot only the head projection of the anchor, butalso other higher projections that are requiredto account for the full connectedness of the par-tial parse tree.
The elementary tree anchoredby Bill is linguistically motivated up to the NPprojection; the rest of the structure depends onconnectivity.
These extra nodes are called pre-dicted nodes.
A predicted preterminal node isreferred by a set of lexical items.
In the sec-tion 3 we illustrate a method for building suchextended elementary trees.The derivation process in LTAG and DV?TAG builds a derived tree by combining the ele-mentary trees via some operations that are illus-trated below.
DV?TAG implements the incre-mental process by constraining the derivationprocess to be a series of steps in which an ele-mentary tree is combined with the partial treespanning the left fragment of the sentence.
Theresult of a step is an updated partial structure.Specifically, at the processing step i, the ele-mentary tree anchored by the i-th word in thesentence is combined with the partial structurespanning the words from 1 to i ?
1 positions;the result is a partial structure spanning thewords from 1 to i.
In contrast, LTAG doesnot pose any order constraint on the deriva-tion process, and the combinatorial operationsare defined over pairs of elementary trees.
InDV?TAG the derivation process starts from anelementary tree anchored by the first word inthe sentence and that does not require any at-tachment that would introduce lexical materialon the left of the anchor (such as in the casethat a Substitution node is on the left of theanchor).
This elementary tree becomes the firstleft context that has to be combined with someelementary tree on the right.Since in DV?TAG we always combine a leftcontext with an elementary tree, the numberof attachment operations increases from twoin LTAG to six in DV?TAG.
Three operations(substitution, adjunction from the left and ad-junction from the right) are called forward op-erations because they insert the current elemen-tary tree into the left context; two other oper-ations (inverse substitution and inverse adjunc-tion) are called inverse operations because theyinsert the left context into the current elemen-tary tree; the sixth operation (shift) does notinvolve any insertion of new structural material.The first operation in DV?TAG is the stan-dard LTAG substitution, where some elemen-tary tree replaces a substitution node in anothertree structure (see fig.
2(a)).Standard LTAG adjunction is split into twooperations: adjunction from the left and ad-junction from the right.
The type of adjunctiondepends on the position of the lexical materialintroduced by the auxiliary tree with respectto the material currently dominated by the ad-joined node (which is in the left context).
Infigure 2(a) we have an adjunction from the leftin the case of the left auxiliary tree anchored byoften.Inverse operations account for the insertionof the left context into the elementary tree.
Inthe case of inverse substitution the left contextreplaces a substitution node in the elementarytree; in the case of inverse adjunction, the leftcontext acts like an auxiliary tree, and the el-ementary tree is split because of the adjoiningof the left context at some node.
In (Lombardoand Sturt, 2002b) there is shown the importanceof the latter operation to obtain the correct de-pendencies for cross-serial Dutch dependenciesin DV?TAG.Finally, the shift operation either scans a lex-ical item which has been already introduced inthe structure or derives a lexical item from somepredicted preterminal node.It is important to notice that, during thederivation process, not all the nodes in the leftcontext and the elementary tree are accessiblefor performing some operation: given the i?
1-th word in the sentence we can compute a setof accessible nodes in the left context (the rightfringe); also, given the lexical anchor of the el-ementary tree, that in the derivation processmatches the i-th word in the sentence, we cancompute a set of accessible nodes in the elemen-tary tree (the left fringe).At the end of the derivation process the leftcontext structure spans the whole sentence, andis called the derived tree: in the figures 1(c) and2(c) there are the derived trees for Bill oftenpleases Sue in LTAG and DV?TAG respectively.A key device in LTAG is the derivation tree(fig.
1(b)).
The derivation tree represents thehistory of the derivation of the sentence: it de-scribes the substitutions and the adjoinings thatoccur in a sentence derivation through a treestructure.
The nodes of the derivation tree areidentifiers of the elementary trees, and one edgerepresents the operation that combines two ele-mentary trees.
Given an edge, the mother nodeidentifies the elementary tree where the elemen-tary tree identified by the daughter node is sub-stituted in or adjoined to, respectively.
Thederivation tree provides a factorized representa-tion of the derived tree.
Since each elementaryis anchored by a lexical item, the derivation treealso describes the syntactic dependencies in thesentence in the terms of a dependency?style rep-resentation (Rambow and Joshi, 1999) (Dras etal., 2003).The notion of derivation tree is not ade-quate for DV?TAG, since the elementary treescontain unheaded predicted nodes.
For exam-ple, the elementary tree anchored by Bill ac-tually involves two anchors, Bill and pleases,even if the latter anchor remains unspecifieduntil it is scanned/derived in the linear or-der.
We introduce a new word?based structurethat represents syntactic dependencies, namelya derivation-dependency tree.A derivation-dependency tree is a head-basedversion of the derivation tree.
Each node in anelementary tree is augmented with the lexicalhead that projects that node.
The derivation-dependency tree contains one node per lexi-cal head, and a lexical head dominates anotherwhen the corresponding projections in the de-rived tree stand in a dominance relation.
Eachelementary tree can contain only one overtlymarked lexical head, that represents the seman-tic unit, but the presence of predicted nodesin the partial derived tree corresponds to pre-dicted heads in the derivation-dependency tree.In figure 3 is depicted the evolution of thederivation?dependency tree for the sentence Billoften pleases Sue.The DV?TAG derivation process requires thefull connectivity of the left context at all times.The extended domain of locality provided byLTAG elementary trees appears to be a desir-able feature for implementing full connectivity.However, each new word in a string has to beconnected with the preceding left context, andthere is no a priori limit on the amount of struc-ture that may intervene between that word andthe preceding context.
For example, in a DV?TAG derivation of John said that tasty apples     	    	 fifffl	flffi  !
!
"# $&%(')+* ,-.
,(./102354 6 7 8 9 : ;< =>5?A@CB D E F D F GH<JICK L MCN L NO5P(Q5RCSTA<U	VXWYZ [ [\ ](^A_ ` `Figure 3: The DVTAG derivation of the sen-tence Bill often pleases Sue.were on sale, the adjective tasty cannot be di-rectly connected with the S node introduced bythat; there is an intervening NP symbol thathas not yet been predited in the structure.
An-other example is the case of an intervening mod-ifier between an argument and its predicativehead, like in the example Bill often pleases Sue(see figure 2), where in order to scan often weneed a VP adjunction node that the NP pro-jection cannot introduce.
So, the extended do-main of locality available in LTAG has to befurther extended.
In particular, some struc-tures have to be predicted as soon as there issome evidence from arguments or modifiers onthe left.
In other approaches this extensionis implemented via top-down predictions (seee.g.
Roark (2001)) during the parsing process.This can lead to a high number of combinationsthat raise the degree of local ambiguity in thederivation process.
In fact, in the case of Roark(2001), the method to reduce this problem hasbeen to use underspecification in the right partof the cf rules.In the remainder of this paper we addressthe issue of building a wide coverage DV?TAGgrammar where elementary trees extend the do-main of locality given by the argumental struc-ture, and we provide an empirical evaluationof the possible combinatorial problems that canraise with such extended structures.3 Building a DV-TAG lexiconThe method used to build a wide coverage DV?TAG grammar is to start with an existing LTAGgrammar, and to extend the elementary treesthrough a closure of a left?associative operation.First, the LTAG elementary tree nodes haveto be augmented with the lexical head informa-tion through a percolation procedure that takesinto account the syntactic projections.Then, the elementary trees must be extendedto account for the full connectivity.
Given thatone step of the derivation process is a combi-nation of a left context and an elementary tree,we have that the rightmost symbol of the leftcontext and the leftmost anchor of the elemen-tary tree (the current input word) must be ad-jacent in the sentence.
However, it is possible(as we have illustrated above) that the left con-text and the elementary tree cannot be com-bined through none of the five DV?TAG oper-ations.
But if the combination between the leftcontext and the elementary tree can occur oncewe assume some intervening structure, we canbuild a superstructure that includes the elemen-tary tree and extends it until either the left con-text can be inserted in the left fringe of the newsuperstructure or the new superstructure can beinserted in the right fringe of the left context.In building the superstructures, we require thatthe linguistic dependencies posed by the LTAGelementary trees over the lexical heads in thederivation/dependency tree are maintained, inorder not to disrupt the semantic interpretationprocess.Since no new symbol can intervene betweenthe rightmost symbol of the left context andthe leftmost anchor of the elementary tree (thecurrent input word), the elementary tree mustbe extended in ways that do not alter such lin-ear order of the terminal symbols.
This meansthat the elementary tree must be extended with-out introducing any further structure that canin turn derive terminal symbols on the left ofthe leftmost anchor.
In order to satisfy such aconstraint, the elementary tree has to be left?anchored, i.e.
the leftmost symbol of the ele-mentary tree, but possibly the foot node in caseof a right auxiliary tree, is an anchor.
Then,the operation that extends the left?anchored el-ementary trees is the left association.
The leftassociation starts from the root of the elemen-tary tree and combines it with another elemen-tary tree on the right through through either in-verse operation (see above)2; this combinationis iterated as far as possible through a transitiveclosure of left association (see below).
All thecombinations are stored in the extended lexicon.Since the individual elementary trees thatform a superstructure through left associationare not altered in this process, linguistic depen-2There are some similarities between left associationand the CCG type raising operation (Steedman, 2000),because in both cases some (root) category X is ?raised?to some higher category Y.         ffflfiffi!
#"$!%#&'!()!)!
*+-,/.1023456 7 8fl9:;!<#=>!?@!
@!AB-C/D1EFigure 4: Example of the left association oper-ation: the trees on the top are, respectively, theBase tree and the Raising tree; the tree on thebottom is the Raised treedencies are kept unchanged.
Left associationcan be performed during the parsing/derivationprocess (i.e.
on?line) or with the goal to extendthe lexicon (i.e.
off?line).
Since we are explor-ing the consequences of increasing the role ofthe competence grammar, we perform this op-eration off?line (see the next section).Each left association operation takes in in-put two trees: a left?anchored Base tree and aRaising tree, and produces in output a new left?anchored Raised tree.
A Base tree ?
can be anyleft?anchored elementary tree or a Raised tree.A Raising tree is any elementary tree ?
that al-lows ?
to combine on its left via either inversesubstitution or adjunction.
A Raised tree is atree such that ?
has been attached to ?
accord-ing to inverse substitution or inverse adjunction.The application of the transitive closure ofleft association occurs with the termination con-dition of minimal recursive structure, that isthe non repetition of the root category in thesequence of Raising trees (henceforth root se-quence).
So, if the original Base tree or someRaising tree already employed have a root X,we cannot use a Raising tree rooted X anymorefor the same superstructure.Considering that LTAG is a lexicalized for-malism, we immediately realize that a super-structure is multiply anchored.
As an example,consider the left association illustrated in fig-ure 4: we substitute the tree anchored by Johninto the tree anchored by likes, yielding a largerelementary structure multiply anchored by Johnand likes at the same time (the lexical head in-formation for each node has been omitted).FHG IKJLM1NLflNPOQ!RTS UKVWPXZYPWPYfl[\]Z^ _`Pa_abcedgf hHijkTlmon-p1qrsstuPv-w/xyHz{P|/}~P??!?T??
????P?????????e?g?
?H???T??o?-?1??????P?-?/??H?
?K??/?1?P??!?T?
?K????1?P???Z?
??g???e?g?
?H???T??o?-?1??????P?-?/??H??????1?P?fl??!?T?????1?fl?P??
?
?
?????e?g?
?H???T??o?-?1??????P?-?/??
?
??H?
??    ffflfiffi !#"$fl%&')(+*-,/.0fl0fl1243+576Figure 5: Schema of the left association oper-ation, followed by the factorization in templatetrees.Multiple anchoring, when not linguisticallymotivated like in the case of idioms or specificsubcategorization constraints, leads to some po-tential problems.
The first is the theoretical is-sue of semantic compositionality, because thesuperstructures do not reflect the incrementalprocess in the semantic composition once wordsare not the minimal semantic units anymore (asassumed in LTAG).
The second is a practical is-sue of duplicating the stored information for allthe verbs sharing the predicate?argument struc-ture.
For example, in the previous example, allthe transitive verbs have a tree structure iden-tical to the elementary tree of likes (see fig.
5).These two problems can be solved by intro-ducing the notion of template, already presentin the practical implementations of wide cov-erage LTAG systems (Doran et al, 2000).
Atree template is a single elementary tree thatrepresents the set of elementary trees sharingthe same structure except for the lexical anchor:one single structure is referred to by pointersfrom the word list.
All the equal tree struc-tures that have the same leftmost anchor afterare represented as a single template, where onlythe leftmost anchor is lexically realized; all theother anchors are replaced by variables with re-ferring word lists that explicitly state the rangeof variation of the variables themselves.
A vari-able replaces each occurrence of the lexical itemin the original elementary structure: once a shiftoperation matches the current input word withone of the words in the associated list, we alsohave to unify the lexical head variables that aug-ment non terminal symbols with the current in-put word.
For instance, on the bottom of fig-ure 5, there is the template obtained by the leftassociation of the elementary tree anchored byJohn with all the equal elementary trees of tran-sitive verbs.A further problem is a possible combinatorialexplosion of the size of the lexicon: this prob-lem has to be tackled in an empirical way on awide coverage grammar (it could be that a largenumber of the theoretically possible combina-tions do not occur in practice; in fact, empiricalwork by (Lombardo and Sturt, 2002a) indicatesthat there is an empirical bound on the size ofexpanded elementary trees necessary to main-tain connectedness).4 Empirical testsIn order to estimate whether the combinato-rial explosion has a dramatic effect on the lex-icon we have run two tests, implementing thetransitive closure of the left association.
Thefirst test was performed on a realistic grammarfrom the XTAG system (Doran et al, 2000),and the second test was performed on an au-tomatically extracted grammar from an Italiantreebank (Mazzei and Lombardo, 2004).In the implementation of the recursive pro-cedure, the left-association operation takes asinput two templates: a left-anchored template,that we call base template, and another tem-plate, that cannot be a left?anchored template,that we call raising template.
In every stepof the algorithm, the base template is takenfrom the subset of left-anchored templates andthe raising template is picked from the wholelexicon.
Since the algorithm builds only left?anchored templates, the output template is in-serted in the left-anchored subset.The grammar used in the first test has 628tree templates representing one half of the hand-written XTAG grammar, with the same distri-bution of template families as the overall XTAGgrammar.
This size is a realistic grammar size(consider that the XTAG lexicon, the widestLTAG grammar existing for English, 1227 tem-plates).
140 out of 628 were left?anchored tem-plates, and the transitive closure from thesebase templates produces 176, 190 raised tem-plates, with a maximum of 7 left associations,and a distribution of trees that reaches its max-imum at 4 left associations (140 base tem-plates, 3, 033 twice raised templates, 24, 855three-times raised, 62, 970 four times raised,59, 908 five times, 22, 454 six times, 2, 970 seventimes).
Similar distributional results have beenproduced for subsets of the grammar and withrestrictions on root categories.
The number ofraised templates drastically reduces when weforbid raising to verbal projections (S, VP andV), thus cutting one of major sources of the ex-plosion.
In this case we go from 717 non ver-bal base templates in the XTAG grammar toonly 24, 468 raised templates, again with a max-imum of 7 raisings (notice that the base lexiconis larger than the test above).In the second test we used a LTAG grammrextracted from the 45, 000 word TUT (TurinUniversity Treebank).
The number of extractedtree templates was 1283.
In this case, as thegrammar was relatively large, we decided to im-pose an extra condition on the closure for leftassociation procedure.
We estimated the maxi-mum number of trees that need to be composedto create any one left associated tree.
Thiswas done by inspecting the derivation trees foreach sentence of the treebank, and looking atthe leftmost child of each level of the deriva-tion tree.
It was found that no left-associatedtree needed to be composed of more than threeelementary trees, in order to create a coveringDV?TAG for the treebank.
This replicates aprevious result of Lombardo and Sturt (2002a).Moreover, of the 800 mathematically possibleroot sequences3 only 67 were present in the tree-bank.
We decided to allow left association onlyfor root sequences that actually appeared in thetreebank.
This resulted in a total of 706,866 leftassociated trees.
Of these, 988 were base tem-plates, 87,245 were raised twice, and 618,654were raised three times.The combinatorial explosion seen in the twoexperiments suggests the use of underspecifi-cation techniques before applying DV?TAG ina realistic setting (see Roark (2001) for onemethod applied to Context Free Grammar).However, in order to estimate the amount of3In the TUT treebank we have 27 non-terminal sym-bols, and 20 possible root categories.ambiguity that can arise in a parsing processwe need to refer to some specific parsing model.Also selective strategies on categories and re-striction to empirically observed root sequencescan be effective.
However, in order to make afull evaluation of these strategies, it is desirableto perform further coverage tests.5 ConclusionThis paper has explored some consequencesof building an incremental grammar that con-strains the possible configurations of partialstructures and the possible interpretable partialstructures.We have introduced a TAG?based formalismthat encodes a strong notion of incrementalitydirectly into the operations of the formal sys-tem.A left-associative type-raising operation hasbeen used to build a DV?TAG realistic lexi-con from a LTAG lexicon.
The left?associationoperation can be viewed as a sort of chunkingof linguistic knowledge: this chunking could beuseful in the definition of a language model inwhich specific combinations of elementary treesthat have been successful in the past experiencebecome part of the lexicon.As shown in the empirical tests ofn Englishand Italian, the left?association closure can leadto severe computational problems that suggestthe adoption of some form of underspecification.ReferencesR.
Berwick and A. Weinberg.
1984.
The gram-matical basis of linguistic performance: lan-guage use and acquisition.
MIT Press.M.
W. Crocker.
1992.
A Logical Model of Com-petence and Performance in the Human Sen-tence Processor.
Ph.D. thesis, Dept.
of Ar-tificial Intelligence, University of Edinburgh,UK.C.
Doran, B. Hockey, A. Sarkar, B. Srinivas,and F. Xia.
2000.
Evolution of the xtag sys-tem.
In A. Abeille?
and O. Rambow, edi-tors, Tree Adjoining Grammars, pages 371?405.
Chicago Press.M.
Dras, D. Chiang, and W. Schuler.
2003.On relations of constituency and dependencygrammars.
Language and Computation, inpress.R.
Frank.
2000.
Phrase structure composi-tion and syntactic dependencies.
Unpub-lished Manuscript.A.
Joshi and Y. Schabes.
1997.
Tree-adjoininggrammars.
In G. Rozenberg and A. Salo-maa, editors, Handbook of Formal Languages,pages 69?123.
Springer.R.
Kempson, W. Meyer-Viol, and D. Gabbay.2000.
Dynamic Syntax: the Flow of LanguageUnderstanding.
Blackwell, Oxford, UK.V.
Lombardo and P. Sturt.
2002a.
Incremen-tality and lexicalism: A treebank study.
InS.
Stevenson and P. Merlo, editors, LexicalRepresentations in Sentence Processing.
JohnBenjamins.V.
Lombardo and P. Sturt.
2002b.
Towards adynamic version of tag.
In TAG+6, pages 30?39.A.
Mazzei and V. Lombardo.
2004.
Buildinga large grammar for italian.
In LREC04.
inpress.David Milward.
1994.
Dynamic depen-dency grammar.
Linguistics and Philosophy,17:561?604.C.
Phillips.
2003.
Linear order and con-stituency.
Linguistic Inquiry, 34:37?90.O.
Rambow and A. Joshi.
1999.
A formal lookat dependency grammars and phrase struc-ture grammars, with special consideration ofword-order phenomena.
In Recent Trends inMeaning-Text Theory, pages 167?190.
JohnBenjamins, Amsterdam and Philadelphia.B.
Roark.
2001.
Probabilistic top-down parsingand language modeling.
Computational Lin-guistics, 27(2):249?276.S.
M. Shieber and M. Johnson.
1993.
Varia-tions on incremental interpretation.
Journalof Psycholinguistic Research, 22(2):287?318.E.
P. Stabler.
1991.
Avoid the pedestri-ans?
paradox.
In R. C. Berwick, S. P. Ab-ney, and C. Tenny, editors, Principle-basedparsing: computation and psycholinguistics,pages 199?237.
Kluwer, Dordrecht: Holland.M.
J. Steedman.
2000.
The syntactic process.A Bradford Book, The MIT Press.
