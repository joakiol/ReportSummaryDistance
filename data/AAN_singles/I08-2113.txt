Errgrams ?
A Way to Improving ASRfor Highly Inflected Dravidian LanguagesKamadev BhanuprasadAndhra University, VisakhapatnamAndhra Pradesh 530 003Indiasave.climate@gmail.comMats SvensonUniversity of MacauAv.
Padre Toms Pereira, TaipaMacau, Chinasvmats@yahoo.comAbstractIn this paper, we present results of our ex-periments with ASR for a highly inflectedDravidian language, Telugu.
First, we pro-pose a new metric for evaluating ASR per-formance for inflectional languages (Inflec-tional Word Error Rate IWER) which takesinto account whether the incorrectly recog-nized word corresponds to the same lexi-con lemma or not.
We also present resultsachieved by applying a novel method ?
er-rgrams ?
to ASR lattice.
With respect toconfidence scores, the method tries to learntypical error patterns, which are then usedfor lattice correction, and applied just be-fore standard lattice rescoring.
Our confi-dence measures are based on word posteri-ors and were improved by applying antimod-els trained on anti-examples generated bythe standard N-gram language model.
ForTelugu language, we decreased the WERfrom 45.2% to 40.4% (by 4.8% absolute),and the IWER from 41.6% to 39.5% (2.1 %absolute), with respect to the baseline per-formance.
All improvements are statisticallysignificant using all three standard NIST sig-nificance tests for ASR.1 IntroductionSpeech recognition technologies allow computersequipped with a source of sound input, such as amicrophone, to interpret human speech, for exam-ple, for transcription or as an alternative methodof interacting with a machine.
Using constrainedgrammar recognition (described below), such appli-cations can achieve remarkably high accuracy.
Re-search and development in speech recognition tech-nology has continued to grow as the cost for imple-menting such voice-activated systems has droppedand the usefulness and efficiency of these systemshas improved.
Furthermore, speech recognition hasenabled the automation of certain applications thatare not automatable using push-button interactivevoice response (IVR) systems.
Speech recognitionsystem are based on simplified stochastic models,so any aspects of the speech that may be importantto recognition but are not represented in the mod-els cannot be used to aid in recognition.
An es-sential part of each Automatic Speech Recognition(ASR) system is Language Model (LM) (Rabiner L.and Juang BH., 1993; Huang X., 2001; Jelinek F.,1998).
For languages with rich inflection, languagemodeling is difficult (Ircing P. et al, 2001; RotovnikT.
et al, 2007).
To be able to perform Very (300K+)Large Vocabulary Continuous Speech Recognitionin real (or at least acceptable) time, nowadays, it isoften only possible to use 2-gram LM for the firstrecognition pass.
Using only one word context isusually insufficient in order to achieve good results.To improve performance for off-line ASR, it is pos-sible to rescore output lattice afterward (Chelba andJelinek, 1999; Richardson F. et al, 1995; Finke et805al., 1999; Ircing P. and Psutka J., 2002).
In thispaper, we describe our method for reducing errorrates, that was applied to improve ASR results forLVCSR of Dravidian languages namely the Telugulanguage.2 Telugu and Dravidian languages ingeneralSince there have not yet been many publications onASR for the Dravidian languages, we give here somebasic information on them.
Dravidian languages arespoken by more than 200 million people (Wikipedia,2007).
In phonology, Dravidian languages sufferfrom the lack of distinction between aspirated andunaspirated stops.
While some Dravidian languageshave large numbers of loan words from Sanskrit andother Indo-European languages, the words are oftenmispronounced by monolingual Dravidian speak-ers.
Dravidian languages are also characterized bya three-way distinction between dental, alveoar andretroflex places of articulation as well as large num-bers of liquids.In this work, we show evidence from one partic-ular Dravidian language Telugu.
Telugu belongs tothe family but with ample influences by the Indo-Arian family and is the official language of the stateof Andhra Pradesh, India.
It is the Dravidian lan-guage with the greatest number of speakers, the sec-ond largest spoken language in India after Hindi andone of the 22 official national languages of India.The Telugu script is believed to descend fromthe Brahmi script of the Ashokan era.
Merchantstook the Eastern Chalukyan Script to Southeast Asiawhere it parented the scripts of Mon, Burmese, Thai,Khmer, C?am, Javanese and Balinese languages.Their similarities to Telugu script can be discernedeven today.
Its appearance is quite similar to theKannada script, its closest cousin.
Telugu script iswritten from left to right and consists of sequencesof simple and/or complex characters.
The script islargely syllabic in nature - the basic units of writingare syllables.
Since the number of possible syllablesis very large, syllables are composed of more basicunits such as vowels (achchu or swar) and conso-nants (hallu or vyanjan).
Consonants in consonantclusters take shapes which are very different fromthe shapes they take elsewhere.
Consonants are pre-sumed to be pure consonants, that is, without anyvowel sound in them.
However, it is traditional towrite and read consonants with an implied ?a?
vowelsound.
When consonants combine with other vowelsigns, the vowel part is indicated orthographicallyusing signs known as vowel maatras.
The shapesof vowel maatras are also very different from theshapes of the corresponding vowels.
The overall pat-tern consists of 60 symbols, of which 16 are vowels,3 vowel modifiers, and 41 consonants.
Spaces areused between words as word separators.
The sen-tence ends with either a single (purna virama) or adouble bar (deergha virama).
They also have a set ofsymbols for numerals, though Arabic numbers aretypically used.In Telugu, Karta (nominative case or the doer),Karma (object of the verb), and Kriya (action or theverb) follow a sequence.
This is one of the severalreasons why linguists classify Telugu as a Dravid-ian Language ?
this pattern is found in other Dravid-ian languages but not in Sanskrit.
Telugu allows forpolyagglutination, the unique feature of being ableto add multiple suffixes to words to denote morecomplex features.
Telugu also exhibits one of therare features that Dravidian languages share withfew others: the inclusive and exclusive we.
Thebifurcation of the First Person Plural pronoun (wein English) into inclusive (manamu) and exclusive(memu) versions can also be found in Tamil andMalayalam.
Like all Dravidian languages, Teluguhas a base (or of words which are essentially Dra-vidian in origin.Telugu pronouns follow the systems for genderand respect also found in other Indian languages.The second person plural ?miru?
is used in address-ing someone with respect, and there are also respect-ful third personal pronouns pertaining to both gen-ders.
A specialty of the Telugu language, however, isthat the third person non-respectful feminine is usedto refer to objects, and there is no special ?neuter?gender that is used.8063 Method3.1 DataWe have recorded a large broadcast news corpus forTelugu.
All commercials and stretches of sponta-neous speech were removed from the data, sincewe focus here on ASR for an unexplored languagerather than on dealing with automatic audio segmen-tation and spontaneous speech recognition.
Overall,we had at disposal 61.2 hours of pure transcribedspeech.
It yields 635k word tokens, contained inmanual human transcriptions.
Due to rich morphol-ogy of Dravidian languages, it represents 78k differ-ent word forms, with plenty of words appearing justonce.
We used ?70% of data for training, ?15% fordevelopment, and the remainding ?15% for testing.For language modeling, we used a newspaper cor-pus containing data from three major Telugu news-papers - Andhra Prabha, Eenadu, and Vaartha.
Thiscorpus contains 20M tokens, which corresponds to615k different word forms.3.2 Evaluation methodThe usual metric to evaluate ASR system perfor-mance is Word Error Rate (WER).
Unfortunately,as we described in Section 2, Telugu is a highly in-flectional language having a really high number ofdifferent word forms.
Using WER, this cause to un-derestimate the real system performance, since thismetric does not distinguish between confusing wordidentities and confusing just forms of the same word(lemma).
However, it is obvious that these errors donot have the same influence on the usability of auto-matic transcripts.
Taking an example from English,recognizing who instead of whom is not that bad asconfusing boom (especially when most Americansor not able to distinguish who and whom anyway).Thus, we propose to use Inflectional Word ErrorRate (IWER), which gives weight 1 to errors con-fusing lemmas, while only a weight 0.5 when thelemma of the incorrectly recognized word is correct,but the whole word form is not correct.
Lemmascorresponding to particular word forms may be ob-tained using an automatic lemmatization technique.3.3 Confidence measuringThe key problem for our method (as described be-low) is to perform appropriate ASR confidence mea-suring.
Confidence measures (CMs) need to be in-terpreted in order to decide whether a word is prob-ably recognized correct or incorrect.
In this pa-per, we use a confidence measure based on posteriorprobability formulation.
It is well known that theconventional ASR algorithm is usually formulatedas a pattern classification problem using the max-imum a posterior (MAP) decision rule to find themost likely sequence of words W which achievesthe maximum posterior probability p(W |X) givenany acoustic observation X .Obviously, the posterior probability p(W |X) is agood confidence measure for the recognition deci-sion that X is recognized as W .
However, most real-world ASR systems simply ignore the term p(X)during the search, since it is constant across differ-ent words W .
This explains why the raw scores arenot usable as confidence scores to reflect recogni-tion reliability.
Anyway, after the normalization byp(X), the posterior probability p(W |X) can be em-ployed as a good confidence measure; it representsthe absolute quantitative measure of the correspon-dence between X and W .In real-world tasks, we have to either employcertain simplifying assumptions or adopt some ap-proximate methods when estimating p(X) in orderto obtain the desired posteriors.
In the first cat-egory, it includes the so-called filler-based meth-ods which try to calculate p(X) from a set of gen-eral filler or background models.
These approachesare very straightforward and usually can achieve anreasonable performance in many cases.
However,we rather used the so-called lattice-based methodswhich attempt to calculate p(X), then the poste-rior probability p(W |X) in turn, from a word latticeor graph based on the forwardbackward algorithm,such as Schaaf (Schaaf T. and Kemp T., 1997) andWessel (Wessel F. et al, 1999) and their colleagues,among others.Usually, a single word lattice or graph is gen-erated by the ASR decoder for every ?utterance?.807Then, the posterior probability of each recognizedword or the whole hypothesized stream of words canbe calculated based on the word-graph from an ad-ditional post-processing stage.
Since word graph isa compact and fairly accurate representation of allalternative competing hypotheses of the recognitionresult which usually dominate the summation whencomputing p(X) over a variety of hypotheses, theposterior probability calculated from a word graphcan approximate the true p(W |X) very well.In our approach, we extended the lattice basedCM by using an antimodel.
The idea of antimod-els has already been proposed for CMs (Rahim M.et al, 1997), however, it has remained unclearwhat data should be used to estimate these anti-models.
In our work, we simply generated anti-examples from our N-gram model.
The rationale be-hind this is very straightforward.
LM constraints arevery strong in determining the final ASR hypothe-ses, and may sometimes undesirably wash out cor-rect acoustic posteriors.
Also, when you let yourLM generate sentences, these sentences correspondwell to N-gram probabilities but are definitely nei-ther grammatically nor semantically correct.
Thus,these generated sentences can be very well usedas anti-examples to train the antimodel.
Then, weperformed forced-alignment against a random tran-script to generate training data for each anti-model.3.4 ErrgramsThe main problem when applying ASR to extremelyinflected languages such as Telugu, is the need to usea very large vocabulary, in order to reduce the OOVrate to an acceptable level.
However, this causesproblems for making the automatic transcription ina time close to the real-time.
Since we cannot usesuch a big dictionary in these task, our first resultshad quite high WERs and IWERs.
However, we an-alyzed the errors and found that some typical errorpatterns occur repeatedly.
This fact inspired us todesign and employ the following method.First, using HTK large vocabulary speech recog-nizer (HTK, 2007) and a bigram LM, we generatedan N-best ASR output and a scored bigram lattice.Then we statistically analyzed the errors and cre-ated so-called errgrams.
Errgrams are pairs of bi-grams, the first member of the pair is the correctbigram and the second member is the recognizedbigram.
For infrequent bigrams, the method is al-lowed to back-off to unigrams, using discountingbased on common smoothing strategies (such Katzbackoff), but the backoff is more penalized since un-igram errgrams are much less reliable compared tocommon language modeling backoffs (such as back-off for training LMs for ASR).
Errgrams were notonly trained using 1-best ASR output, but to gainmore real ASR data, we used 5-best hypothesis fortraining.
For estimating errgram pairs, we also takeinto account confidence scores - the lower CM, thehigher weight is given to a particular errgram exam-ple.
By this approach, we may achieve better resultswith using vocabulary of standard size (< 100k),since words in ?correct?
parts of errgrams may in-clude words that are not in the limited size vocab-ulary used for the first recognition pass.
In otherwords, we can partially reduce the OOV problem bythis approach.
Note that LMs used for lattice rescor-ing include all such words originally missing in thebaseline LM but appearing in errgrams.The errgrams trained in the above described way,are then applied in the following way during the de-coding phase:1.
Using a bigram model, generate an ASR lattice2.
Walk through the lattice and look for bigrams(or unigrams) having a low CM3.
If for such a low CM n-gram we have a cor-responding errgram with p > Threshold, sub-tract majority (particular percent is optimizedon held-out data) of the probability mass andadd it to the ?correct?
part of the errgram4.
Perform standard lattice rescoring using four-gram LMs4 ResultsTable 1 shows the comparison of WERs and IW-ERs for Telugu LVCSR achieved by various post-808processing methods.
The baseline was achieved us-ing just the first bigram pass.
Then, we report re-sults obtained by standard lattice rescoring method,using a fourgram LM, as well as results which wereachieved by applying errgram method prior to latticerescoring.
The improvement was achieved by apply-ing the errgram correction method.
We decreasedthe WER from 45.2% to 40.4% (by 4.8% absolute),and the IWER from 41.6% to 39.5% (2.1%absolute),with respect to the baseline performance.
As youcan see, WER dropped more than the IWER did.This may be understood as that the errgrams helpmore in correcting errors in grammatical agreement,i.e.
when the word forms differs but the lemmas arerecognized correctly.
The improvement from base-line to the best system is significant at p < 0.01using all three NIST significance tests, while the im-provement from standard lattice rescoring system issignificant at p < 0.05, using the same statisticaltests.5 Summary, conlusions, and future workIn this paper, we have presented a very LVCSRfor the highly inflected Dravidian language, namelyTelugu.
A new metric for evaluating ASR perfor-mance for inflectional languages, Inflectional WordError Rate ?
IWER, taking into account whether in-correctly recognized words correspond to the samelemma or not, was proposed to be used together withthe standard WER.
We also present results achievedby applying a novel method errgrams to ASR lat-tice.
With respect to confidence scores, the methodtries to learn typical error patterns, which are thenused for lattice correction, and applied just beforestandard lattice rescoring.
By this approach, we mayachieve better results with using vocabulary of stan-dard size (< 100k).The improvement was achieved by applying theerrgram correction method.
We decreased the WERfrom 45.2% to 40.4% (by 4.8% absolute), and theIWER from 41.6% to 39.5% (2.1% absolute), withrespect to the baseline performance.
All improve-ments are statistically significant using all three stan-dard NIST significance tests for ASR.Since this method is completely new, there is alot of space for potential improvements.
In our fu-ture work, we would definitely like to focus on im-proving the errgram estimation and smoothing tech-niques, as well as to finding the best approach forlattice rescoring.
Moreover, we would like to applyour idea to other inflected languages, such as Ara-bic, Slovenian, Estonian or Russian.
We also hopethat our Telugu language will draw more attentionof ASR engineers.In the near future, we plan to largely extendour research on automic processing of spoken Tel-ugu, especially move toward processing of sponta-neous speech.
Currently, we are preparing new largedatabase of conversational speech which will be an-notated with MDE-style structural metadata sym-bols (Strassel et al, 2005), reflecting spontaneousspeech events such as fillers and edit dysfluencies.We are looking forward to test our methods on thischallenging data, and compare the results with thebroadcast news data used in this work.6 AknowledgementsThe authors thank the linguists from the Macau Uni-versity for kindly discussing the inflected languageASR issues.
The scientific work was kindly co-funded by Anand Foundation, Raandhanpuur Foun-dation and Scientific Agency of Macau.
All viewspresented in this paper are only the views of authors,and do not necessarily reflect the view by fundingagencies.ReferencesRabiner L., Juang BH 1993.
Fundamentals ofSpeech Recognition, Prentice Hall PTR; 1. edition,0130151572Huang, X.
2001.
Spoken Language Processing: A Guideto Theory, Algorithm and System Development, Pren-tice Hall PTR; 1st edition, 0130226165Jelinek, F. 1998.
Statistical Methods for Speech Recog-nition (Language, Speech, and Communication), TheMIT Press , 0262100665Ircing, P., Psutka, J.,Krbec P., Hajic J., Khudanpur S.,Jelinek F., Byrne W: 2001.
On Large Vocabulary809Table 1: WER[%] and Inflectional WER(IWER)[%] for LVCSR Telugu ASR using various methodsWER [%] IWER [%]1st bigram pass 45.2 41.6fourgram lattice rescoring 42.3 40.2errgrams+lattice rescoring 40.4 39.5Continuous Speech Recognition of Highly InflectionalLanguage, Eurospeech AalborgT.
Rotovnik, M.S.
Maucec, Z. Kacix.
2007.
Large vo-cabulary continuous speech recognition of an inflectedlanguage using stems and endings.
Speech Communi-cation, Vol.49, No.6, pp.437-452C.
Chelba and F. Jelinek 1999.
Structured languagemodeling for speech recognition In Proceedings ofNLDB99, Klagenfurt, AustriaF.
Richardson, M. Ostendorf, and J.R. Rohlicek.
1995.Lattice-based search strategies for large vocabularyspeech recognition.
Proc.
ICASSPM.
Finke, J. Fritsch, D. Koll, A. Waibel.
1999.
Mod-eling And Efficient Decoding Of Large VocabularyConversational Speech.
In Proceedings of the EU-ROSPEECH99, Vol.
1, pp.
467?470, Budapest, Hun-gary, September 1999Ircing P, Psutka J.
2002.
Lattice Rescoring inCzech LVCSR System Using Linguistic Knowl-edge.
International Workshop Speech and ComputerSPECOM2002, St. Petersburgh, Russia.
pp.
23-26.Wikipedia 2007.http://en.wikipedia.org/wiki/Dravidian languagesH.
Jiang.
2005.
Confidence measures for speech recog-nition: A survey, Speech Communication 45 (2005),pp.
455-470Schaaf, T., Kemp, T. 1997.
Confidence measures forspontaneous speech recognition.
Proc.
of InternationalConference on Acoustics, Speech and Signal Process-ing, pp.
875-878.Wessel, F., Macherey, K., Ney., H. 1999.
A compar-ison of word graph and N-best list based confidencemeasures.
Proc.
of European Conference on SpeechCommunication Technology, pp.
315-318Rahim, M.G., Lee, C.-H. 1997.
String-based minimumverification error (SB-MVE) training for speech recog-nition.
Computer Speech Language 11, 147-160.HTK website.
2007. http://htk.eng.cam.ac.uk/Strassel, S., Kolar, J., Song Z., Barclay L., Glenn M.2005.
Structural Structural Metadata Annotation:Moving Beyond English.
Proc.
of European Confer-ence on Speech Communication Technology, Lisbon.810
