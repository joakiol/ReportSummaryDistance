Coling 2010: Poster Volume, pages 232?240,Beijing, August 2010Topic-Based Bengali Opinion SummarizationAmitava DasDepartment of Computer Scienceand EngineeringJadavpur Universityamitava.santu@gmail.comSivaji BandyopadhyayDepartment of Computer Scienceand EngineeringJadavpur Universitysivaji_cse_ju@yahoo.comAbstractIn this paper the development of an opi-nion summarization system that works onBengali News corpus has been described.The system identifies the sentiment in-formation in each document, aggregatesthem and represents the summary infor-mation in text.
The present sys-tem fol-lows a topic-sentiment model for senti-ment identification and aggregation.
Top-ic-sentiment model is designed as dis-course level theme identification and thetopic-sentiment aggregation is achievedby theme clustering (k-means) and Doc-ument level Theme Relational Graph re-presentation.
The Document LevelTheme Relational Graph is finally usedfor candidate summary sentence selectionby standard page rank algorithms used inInformation Retrieval (IR).
As Bengali isa resource constrained language, thebuilding of annotated gold standard cor-pus and acquisition of linguistics toolsfor lexico-syntactic, syntactic and dis-course level features extraction are de-scribed in this paper.
The reported accu-racy of the Theme detection technique is83.60% (precision), 76.44% (recall) and79.85% (F-measure).
The summarizationsystem has been evaluated with Precisionof 72.15%, Recall of 67.32% and F-measure of 69.65%.1 IntroductionThe Web has become a rich source of variousopinions in the form of product reviews, traveladvice, social issue discussions, consumer com-plaints, movie review, stock market predictions,real estate market predictions, etc.
Present com-putational systems need to extend the power ofunderstanding the sentiment/opinion expressed inan electronic text to act properly in the societyrather than dealing with the topic of a document.The topic-document model of information re-trieval has been studied for a long time and sys-tems are available publicly since last decade.
Onthe contrary Opinion Mining/Sentiment Analysisis still an unsolved research problem.
Although afew systems like Twitter Sentiment AnalysisTool1, TweetFeel2 are available in World WideWeb since last few years still more research ef-forts are necessary to match the user satisfactionlevel and social need.Researchers have taken multiple approachestowards the problem of Opinion Summarizationlike Topic-sentiment model, Textual summariesat single document or multiple document pers-pective and graphical summaries or visualization.The works on opinion tracking systems have ex-plicitly incorporated temporal dimension.
Thetopic-sentiment model is well established foropinion retrieval.The concept of reputation system was first in-troduced in (Resnick et al, 2000).
Reputationsystems for both buyers and sellers are needed toearn each other?s trust in online interactions.Ku et al, (2005) selects representative wordsfrom a document set to identify the main con-cepts in the document set.
A term is consideredto represent a topic if it appears frequently acrossdocuments or in each document.
Different me-thodologies have been used to assign weights toeach word both at document level and paragraphlevel.
The precision and recall values of the sys-tem have been reported as 0.56 and 0.85.1http://twittersentiment.appspot.com/2http://www.tweetfeel.com/232Zhou et al (2006) have proposed the architec-ture for generative summary from blogosphere.Typical multi-document summarization (MDS)systems focus on content selection followed bysynthesis by removing redundancy across mul-tiple input documents.
The online discussionsummarization system (Zhou et al, 2006) workon an online discussion corpus involving mul-tiple participants and discussion topics are passedback and forth by various participants.
MDS sys-tems are insufficient in representing this aspectof the interactions.
Due to the complex structureof the dialogue, similar subtopic structure identi-fication in the participant-written dialogues isessential.
Maximum Entropy Model (MEMM)and Support Vector Machine (SVM) have beenused with a number of relevant features.Carenini et al (2006) present and comparetwo approaches to the task of multi documentopinion summarization on evaluative texts.
Thefirst is a sentence extraction based approachwhile the second one is a natural language gener-ation-based approach.
Relevant extracted fea-tures are categorized in two types: User DefinedFeatures (UDF) and Crude Features (CF) as de-scribed in (Hu and Liu, 2004).The summary generation technique uses theaggregation of the extracted features, CF andUDF.
Opinion aggregation has been done by thetwo relevant features: opinion strength and polar-ity.
A new opinion distribution function featurehas been introduced to capture the overall opi-nion distributed in corpus.Kawai et al (2007) developed a news portalsite called Fair News Reader (FNR) that recom-mends news articles with different sentiments fora user in each of the topics in which the user isinterested.
FNR can detect various sentiments ofnews articles and determine the sentimental pre-ferences of a user based on the sentiments ofpreviously read articles by the user.
News ar-ticles crawled from various news sites are storedin a database.
The contents are integrated asneeded and the summary is presented on onepage.
A sentiment vector on the basis of wordlattice model has been generated for every doc-ument.
A user sentiment model has been pro-posed based on user sentiment state.
The usersentiment state model works on the browsinghistory of the user.
The intersection of the docu-ments under User Vector and Sentiment Vectorare the results.2 Resource OrganizationResource acquisition is one of the most challeng-ing obstacles to work with resource constrainedlanguages like Bengali.
Bengali is the fifth popu-lar language in the World, second in India andthe national language in Bangladesh.
ExtensiveNLP research activities in Bengali have startedrecently but resources like annotated corpus, var-ious linguistic tools are still unavailable for Ben-gali in the required measure.
The manual annota-tion of gold standard corpus and acquisition ofvarious tools used in the feature extraction forBengali are described in this section.2.1 Gold Standard Data Acquisition2.1.1 CorpusFor the present task a Bengali news corpus hasbeen developed from the archive of a leadingBengali news paper available on the Web(http://www.anandabazar.com/).
A portion of thecorpus from the editorial pages, i.e., Reader?sopinion section or Letters to the Editor Sectioncontaining 28K word forms has been manuallyannotated with sentence level subjectivity anddiscourse level theme words.
Detailed reportsabout this news corpus development in Bengalican be found in (Das and Bandyopadhyay,2009b).2.1.2 AnnotationFrom the collected document set (Letters to theEditor Section), some documents have been cho-sen for the annotation task.
Some statistics aboutthe Bengali news corpus is represented in theTable 1.
Documents that have appeared within aninterval of four months are chosen on the hypo-thesis that these letters to the editors will be onrelated events.
A simple annotation tool has beendesigned for annotating the sentences consideredto be important for opinion summarization.Three annotators (Mr. X, Mr. Y and Mr. Z) par-ticipated in the present task.<Story>?????????????????????..????????????????????
?..<SS><TW>Sargeant O?Leary</TW> said ?the<TW>incident</TW> took place at 2:00pm.?</SS>????????????????????
?..</Story>Figure 1: XML Annotation FormatAnnotators were asked to annotate sentencesfor summary and to mark the theme words (topi-cal expressions) in those sentences.
The docu-ments with such annotated sentences are saved in233XML format.
Figure 1 shows the XML annota-tion format.
?<SS>?
marker denotes subjectivesentences and ?<TW>?
denotes the theme words.Bengali NEWS Corpus StatisticsTotal number of  documents in the corpus 100Total number of sentences in the corpus 2234Average number of sentences in a document 22Total number of wordforms in the corpus 28807Average number of wordforms in a document 288Total number of distinct wordforms in thecorpus17176Table 1: Bengali News Corpus StatisticsThe annotation tool highlights the sentimentwords (Das and Bandyopadhyay, 2010a)3 by fourdifferent colors within a document according totheir POS categories (Noun, Adjective, Adverband Verb).
This technique helps to increase thespeed of annotation process.
Finally 100 anno-tated documents have been developed.2.1.3 Inter-annotator AgreementThe agreement of annotations among three anno-tators has been evaluated.
The agreements of tagvalues at theme words level and sentence levelsare listed in Tables 2 and 3 respectively.Annotators X vs. Y X Vs. Z Y Vs. Z AvgPercentage 82.64% 71.78% 80.47% 78.30%All Agree 69.06%Table 2: Agreement of annotators at themewords levelAnnotators X vs. Y X Vs. Z Y Vs. Z AvgPercentage 73.87% 69.06% 60.44% 67.8%All Agree 58.66%Table 3: Agreement of annotators at sentencelevelFrom the analysis of inter-annotator agree-ment, it is observed that the agreement drops fastas the number of annotator?s increases.
It is lesspossible to have consistent annotations whenmore annotators are involved.
In the present taskthe inter-annotator agreement is better for themewords annotation rather than candidate sentenceidentification for summary though a small num-ber of documents have been considered.Further discussion with annotators reveals thatthe psychology of annotators is to grasp as manyas possible theme words identification duringannotation but the same groups of annotators aremore cautious during sentence identification forsummary as they are very conscious to find outthe most concise set of sentences that best de-scribe the opinionated snapshot of any document.3http://www.amitavadas.com/sentiwordnet.phpThe annotators were working independent ofeach other and they were not trained linguists.2.2 Subjectivity ClassifierWork in opinion mining and classification oftenassumes the incoming documents to be opinio-nated.
Opinion mining system makes false hitswhile attempting to summarize non-subjective orfactual sentences or documents.
It becomes im-perative to decide whether a given documentcontains subjective information or not as well asto identify which portions of the document aresubjective or factual.
This task is termed as sub-jectivity detection in sentiment literature.
Thesubjectivity classifier that uses SVM machinelearning technique and described in (Das andBandyopadhyay, 2009a) has been used here.
Therecall measure of the present classifier is greaterthan its precision value.
The evaluation results ofthe classifier are 72.16% (Precision) and 76.00(recall) on the News Corpus.2.3 Feature OrganizationThe set of features used in the present task havebeen categorized as Lexico-Syntactic, Syntacticand Discourse level features.
These are listed inthe Table 4 below and have been described in thesubsequent subsections.Types FeaturesLexico-SyntacticPOSSentiWordNetFrequencyStemmingSyntactic Chunk Label Dependency Parsing DepthDiscourse LevelTitle of the DocumentFirst ParagraphTerm DistributionCollocationTable 4: Features2.3.1 Lexico-Syntactic Features2.3.1.1 Part of Speech (POS)It has been shown in (Hatzivassiloglou et.
al.,2000), (Chesley et.
al., 2006) etc.
that opinionbearing words in sentences are mainly adjective,adverb, noun and verbs.
Many opinion miningtasks, like (Nasukawa et.
al., 2003) are mostlybased on adjective words.
Details of the BengaliPOS tagger used can be found in (Das and Ban-dyopadhyay 2009b).2342.3.1.2 SentiWordNet (Bengali)Words that are present in the SentiWordNet car-ry opinion information.
The developed Senti-WordNet (Bengali) (Das and Bandyopadhyay,2010a) is used as an important feature during thelearning process.
These features are individualsentiment words or word n-grams (multiwordentities) with strength measure as strong subjec-tive or weak subjective.
Strong and weak subjec-tive measures are treated as a binary feature inthe supervised classifier.
Words which are col-lected directly from SentiWordNet (Bengali) aretagged with positivity or negativity score.
Thesubjectivity score of these words are calculatedas:| | | |s p nE S S= +where sE  is the resultant subjective measureand pS , nS  are the positivity and negativityscores respectively.2.3.1.3 FrequencyFrequency always plays a crucial role in identify-ing the importance of a word in the document.The system generates four separate high frequentword lists for four POS categories: Adjective,Adverb, Verb and Noun after function words areremoved.
Word frequency values are then effec-tively used as a crucial feature in the Theme De-tection technique.2.3.1.4 StemmingSeveral words in a sentence that carry opinioninformation may be present in inflected formsand stemming is necessary for them before theycan be searched in appropriate lists.
Due to nonavailability of good stemmers in Indian languag-es especially in Bengali, a stemmer (Das andBandyopadhyay, 2010b) based on stemmingcluster technique has been used.
This stemmeranalyzes prefixes and suffixes of all the wordforms present in a particular document.
Wordsthat are identified to have the same root form aregrouped in a finite number of clusters with theidentified root word as cluster center.2.3.2 Syntactic Features2.3.2.1 Chunk LabelChunk level information is effectively used as afeature in supervised classifier.
Chunk labels aredefined as B-X (Beginning), I-X (Intermediate)and E-X (End), where X is the chunk label.
Inthe task of identification of Theme expressions,chunk label markers play a crucial role.
Furtherdetails of development of chunking system couldbe found in (Das and Bandyopadhyay 2009b).2.3.2.2 Dependency ParserDependency depth feature is very useful to iden-tify Theme expressions.
A particular Themeword generally occurs within a particular rangeof depths in a dependency tree.
Theme expres-sions may be a Named Entity (NE: person, or-ganization or location names), a common noun(Ex: accident, bomb blast, strike etc) or words ofother POS categories.
It has been observed thatdepending upon the nature of Theme expressionsit can occur within a certain depth in the depen-dency tree for the sentence.
A statistical depen-dency parser has been used for Bengali as de-scribed in (Ghosh et al, 2009).2.3.3 Discourse Level Features2.3.3.1 Positional AspectDepending upon the position of the thematicclue, every document is divided into a number ofzones.
The features considered for each docu-ment are Title words of the document, the firstparagraph words and the words from the last twosentences.
A detailed study was done on theBengali news corpus to identify the roles of thepositional aspect features of a document (firstparagraph, last two sentences) in the detection oftheme words and subjective sentences for gene-rating the summary of the document.
The impor-tance of these positional features is shown inTables 5 on the Bengali gold standard set.2.3.3.2 Title WordsTitle words of a document always carry somemeaningful thematic information.
The title wordfeature has been used as a binary feature duringCRF based machine learning.2.3.3.3 First Paragraph WordsPeople usually give a brief idea of their beliefsand speculations in the first paragraph of thedocument and subsequently elaborate or supportthem with relevant reasoning or factual informa-tion.
Hence first paragraph words are informativein the detection of Thematic Expressions.2.3.3.4 Words From Last Two SentencesGenerally every document concludes with asummary of the opinions expressed in the docu-ment.235Positional Factors BengaliFirst Paragraph 56.80%Last Two Sentences 78.00%Table 5: Statistics on Positional Aspect.2.3.3.5 Term Distribution ModelAn alternative to the classical TF-IDF weightingmechanism of standard IR has been proposed asa model for the distribution of a word.
The modelcharacterizes and captures the informativeness ofa word by measuring how regularly the word isdistributed in a document.
As discussed in Sec-tion 1, Carenini et al (2006) have introduced theopinion distribution function feature to capturethe overall opinion distributed in the corpus.Thus the objective is to estimate ( )d if w  thatmeasures the distribution pattern of the k occur-rences of the word wi in a document d. Zipf's lawdescribes distribution patterns of words in anentire corpus.
In contrast, term distribution mod-els capture regularities of word occurrence insubunits of a corpus (e.g., documents, paragraphsor chapters of a book).
A good understanding ofthe distribution patterns is useful to assess thelikelihood of occurrences of a word in some spe-cific positions (e.g., first paragraph or last twosentences) of a unit of text.
Most term distribu-tion models try to characterize the informative-ness of a word identified by inverse documentfrequency (IDF).
In the present work, the distri-bution pattern of a word within a document for-malizes the notion of topic-sentiment informa-tiveness.
This is based on the Poisson distribu-tion.
Significant Theme words are identified us-ing TF, Positional and Distribution factor.
Thedistribution function for each theme word in adocument is evaluated as follows:( )1 11 1( ) / ( ) /n nd i i i i ii if w S S n TW TW n?
?= == ?
+ ??
?where n=number of sentences in a documentwith a particular theme word, Si=sentence id ofthe current sentence containing the theme wordand Si-1=sentence id of the previous sentencecontaining the query term, iTW is the positional idof current Theme word and 1iTW ?
is the positionalid of the previous Theme word.2.3.3.6 CollocationCollocation with other thematic word/expressionis undoubtedly an important clue for identifica-tion of theme sequence patterns in a document.
Awindow size of 5 including the present word isconsidered during training to capture the colloca-tion with other thematic words/expressions.3 Theme DetectionTerm Frequency (TF) plays a crucial role toidentify document relevance in Topic-Based In-formation Retrieval.
The motivation behind de-veloping Theme detection technique is that inmany documents relevant words may not occurfrequently or irrelevant words may occur fre-quently.
Moreover for sentiment analysis topicwords should have sentiment conceptuality.
TheTheme detection technique has been proposed toresolve these issues to identify discourse levelrelevant topic-semantic nodes in terms of wordor expressions using a standard machine learningtechnique.
The machine learning technique usedhere is Conditional Random Field (CRF)4.
Thetheme word detection is defined as a sequencelabeling problem.
Depending upon the series ofinput feature, each word is tagged as eitherTheme Word (TW) or Other (O).4 Theme ClusteringTheme clustering algorithms partition a set ofdocuments into finite number of topic basedgroups or clusters in terms of themewords/expressions.
The task of document cluster-ing is to create a reasonable set of clusters for agiven set of documents.
A reasonable cluster isdefined as the one that maximizes the within-cluster document similarity and minimizes be-tween-cluster similarities.
There are two princip-al motivations for the use of this technique intheme clustering setting: efficiency, and the clus-ter hypothesis.The cluster hypothesis (Jardine and van Rijs-bergen, 1971) takes this argument a step furtherby asserting that retrieval from a clustered col-lection will not only be more efficient, but will infact improve retrieval performance in terms ofrecall and precision.
The basic notion behind thishypothesis is that by separating documents ac-cording to topic, relevant documents will befound together in the same cluster, and non-relevant documents will be avoided since theywill reside in clusters that are not used for re-trieval.
Despite the plausibility of this hypothe-sis, there is only mixed experimental support forit.
Results vary considerably based on the clus-4http://crfpp.sourceforge.net236tering algorithm and document collection in use(Willett, 1988; Shaw et al, 1996).Application of the clustering technique to thethree sample documents results in the followingtheme-by-document matrix, A, where the rowsrepresent Docl, Doc7 and Doc13 and the col-umns represent the themes politics, sport, andtravel.election cricket hotelA parliament sachin vacationgovernor soccer tourist?
??
?= ?
??
??
?The similarity between vectors is calculatedby assigning numerical weights to these wordsand then using the cosine similarity measure asspecified in the following equation., ,1, .Nk j k j i k i jis q d q d w w?
?
?
?=?
?= = ??
??
?
?
---- (1)This equation specifies what is known as thedot product between vectors.
Now, in general,the dot product between two vectors is not par-ticularly useful as a similarity metric, since it istoo sensitive to the absolute magnitudes of thevarious dimensions.
However, the dot productbetween vectors that have been length norma-lized has a useful and intuitive interpretation: itcomputes the cosine of the angle between thetwo vectors.
When two documents are identicalthey will receive a cosine of one; when they areorthogonal (share no common terms) they willreceive a cosine of zero.
Note that if for somereason the vectors are not stored in a normalizedform, then the normalization can be incorporateddirectly into the similarity measure as follows., ,12 2, ,1 1,Ni k i jik j N Ni k i ki iw ws q dw w?
?== =??
?=?
??
?
???
?
----(2)Of course, in situations where the documentcollection is relatively static, it makes sense tonormalize the document vectors once and storethem, rather than include the normalization in thesimilarity metric.Calculating the similarity measure and using apredefined threshold value, documents are classi-fied using standard bottom-up soft clustering k-means technique.
The predefined threshold valueis experimentally set to 0.5 as shown in Table 6.A set of initial cluster centers is necessary inthe beginning.
Each document is assigned to thecluster whose center is closest to the document.After all documents have been assigned, the cen-ter of each cluster is recomputed as the centroidor mean ??
(where ?
?is the clustering coeffi-cient) of its members, thatis ( )1/jj x cc x??
?
?= ?
.
The distance function isthe cosine vector similarity function.ID Themes 1 2 31  (administration) 0.63 0.12 0.041 (good-government) 0.58 0.11 0.061  (Society) 0.58 0.12 0.031  (Law) 0.55 0.14 0.082  !
"# (Research) 0.11 0.59 0.022 & ' (College) 0.15 0.55 0.012 	 (Higher Study) 0.12 0.66 0.013  +,- (Jehadi) 0.13 0.05 0.583 ,- (Mosque) 0.05 0.01 0.863 23 (Musharaf) 0.05 0.01 0.863  (Kashmir) 0.03 0.01 0.933 5,&6 (Pakistan) 0.06 0.02 0.823 9,-:; (New Delhi) 0.12 0.04 0.653 !>?
2 (Border) 0.08 0.03 0.79Table 6: Five cluster centroids (mean j??
)Table 6 gives an example of theme centroidsfrom the K-means clustering.
Bold words inTheme column are cluster centers.
Cluster cen-ters are assigned by maximum clustering coeffi-cient.
For each theme word, the cluster from ta-ble 6 is still the dominating cluster.
For example,?A?
has a higher membership probability incluster 1.
But each theme word also has somenon-zero membership in all other clusters.
This isuseful for assessing the strength of associationbetween a theme word and a topic.
Comparingtwo members of the cluster2, ?&C;2?
and?9,-:;?, it is seen that ?9,-:;?
is strongly asso-ciated with cluster2 (p=0.65) but has some affini-ty with other clusters as well (e.g., p =0.12 withthe cluster1).
This is a good example of the utili-ty of soft clustering.
These non-zero values arestill useful for calculating vertex weights duringTheme Relational Graph generation.5 Construction of Document LevelTheme Relational GraphRepresentation of input text document(s) in theform of graph is the key to our design principle.The idea is to build a document graph G=<V,E>from a given source document d D?
.
First, theinput document d is parsed and split into a num-ber of text fragments (sentence) using sentencedelimiters (Bengali sentence marker ??
?, ???
or?!?).
At this preprocessing stage, text is toke-nized, stop words are eliminated, and words are237stemmed (Das and Bandyopadhyay, 2010b).Thus, the text in each document is split intofragments and each fragment is represented witha vector of constituent theme words.
These textfragments become the nodes V in the documentgraph.The similarity between two nodes is expressedas the weight of each edge E of the documentgraph.
A weighted edge is added to the documentgraph between two nodes if they either corres-pond to adjacent text fragments in the text or aresemantically related by theme words.
The weightof an edge denotes the degree of the relationship.The weighted edges not only denote documentlevel similarity between nodes but also interdocument level similarity between nodes.
Thusto build a document graph G, only the edges withedge weight greater than some predefined thre-shold value are added to G, which basically con-stitute the edges E of the graph G.The Cosine similarity measure has been usedhere.
In cosine similarity, each document d isdenoted by the vector ( )V d?
derived from d,with each component in the vector for eachTheme words.
The cosine similarity between twodocuments (nodes) d1 and d2 is computed usingtheir vector representations ( 1)V d?and ( 2)V d?asequation (1) and (2) (Described in Section 4).Only a slight change has been done i.e.
the dotproduct of two vectors ( 1) ( 2)V d V d?
??
is definedas1( 1) ( 2)MiV d V d=?
.
The Euclidean length of d isdefined to be21( )MiidV=??
where M is the totalnumber of documents in the corpus.
Themenodes within a cluster are connected by vertex,weight is calculated by the clustering co-efficientof those theme nodes.
No inter cluster vertex arethere.
Cluster centers are interconnected withweighted vertex.
The weight is calculated bycluster distance as measured by cosine similaritymeasure as discussed earlier.To better aid our understanding of the auto-matically determined category relationships wevisualized this network using the Fruchterman-Reingold force directed graph layout algorithm(Fruchterman and Reingold, 1991) and the No-deXL network analysis tool (Smith et al, 2009)5.A theme relational model graph drawn by Nod-deXL is shown in Figure 2.5Available from http://www.codeplex.com/NodeXL6 Summarization SystemPresent system is an extractive opinion summari-zation system for Bengali.
In the previous sec-tions, we described how to identify theme clus-ters that relates to different shared topics andsubtopics, from a given input document set.
Butidentifying those clusters is not only a step to-ward generating document level opinionatednews summary rather another major step is toextract thematic sentences from each theme clus-ter that reflects the contextual concise content ofthe current theme cluster.
Extraction of sentencesbased on their importance in representing theshared subtopic (cluster) is an important issueand it regulates the quality of the output sum-mary.
We have used Information Retrieval (IR)based technique to identify the most ?informed?sentences from any cluster and it can be termedas IR based cluster center for that particular clus-ter.
With the adaptation of ideas from page rankalgorithms (Page et al, 1998), it can be easilyobserved that a text fragment (sentence) in adocument is relevant if it is highly related tomany relevant text fragments of other documentsin the same cluster.
Since, in our document graphstructure, the edge score reflects the correlationmeasure between two nodes, it can be used toidentify the most salient/informed sentence froma sentence cluster.
We computed the relevance ofa node/sentence by summing up the edge scoresof those edges connecting the node with othernodes in the same cluster.
Then the nodes aregiven rank according to their calculated relev-ance scores and the top ranking sentences is se-lected as the candidate sentence representing theopinion summary.
For example four such candi-date sentences are shown in Table 7.
The wordsin bold are the theme words based on thosetheme words the sentences are extracted.Candidate Sentence IR Score!
"# $	 %  D &	'()*+% '!
;D'-E &F ,&G !9 2 ,-& + D !
; H!&I?151J!2 ,KL 2FJ&M!,, &2# J  ,'+)-	.,5N   O  - 2 ,HD 2 ,,5 P2  &  OF9  N,  D  N $/01 	2)0%  ,'+)-	. A!#D?167QR;D2 52 "M !N2 D +', JSF A9&' 	 &	3 ,5N  J J&TH!-?
& & 2: ,!,HU %5()+6% DV&,29  O  D A& 2# ,  -2 -';9 ,W,D,,XD &2?130Table 7: Candidate sentences238Another issue that is very important in sum-marization is sentence ordering so that the Out-put summary looks coherent.
Once all the rele-vant sentences are extracted across the inputdocuments, the summarizer has to decide inwhich order to present them so that the wholetext makes sense for the user.
We prefer the orig-inal order of sentences as they occurred in origi-nal document.Figure 2: Document Level Theme Relational Graph by NodeXL7 Experimental ResultThe evaluation result of the CRF-based ThemeDetection task for Bengali is presented in Table8.
The result is presented individually for everyannotators and the overall result of the system.ThemeDetectionMetrics X Y Z AvgPrecision87.65% 85.06% 78.06% 83.60%Recall80.78% 76.06% 72.46% 76.44%F-Score 84.07% 80.30% 75.16% 79.85%Table 8: Results of CRF-based Theme Iden-tifierThe evaluation result of subjective sentenceidentification of the system for opinion summaryis in the Table 9.SummarizationMetrics X Y Z AvgPrecision77.65% 67.22% 71.57% 72.15%Recall68.76% 64.53% 68.68% 67.32%F-Score 72.94% 65.85% 70.10% 69.65%Table 9: Final Results subjective sentenceidentification for summary8 Error AnalysisThe evaluation result of the present summariza-tion system is reasonably good but still not out-standing.
During the error analysis we found thatthe main false hits occurring for subjectivityidentifier.
It has been reported (Section 2.2) thatthe recall value of the classifier is higher than itsprecision.
Hence some objective sentences areidentified during subjectivity analysis.
Some ofthe sentences get high score during Theme de-tection or Theme clustering and being includedin final summary.
Our observation is at least 2-3% sentences are included due to the wrongidentification by Subjectivity identifier.Another vital source of errors occurring in theaccuracy level of linguistics resources and toolsare the POS tagger, Chunker and DependencyParser.
These linguistics tools are not well per-forming hence the resultant Theme identificationsystem is missing some of the important themewords.
Successive Theme clustering, Documentlevel weighted theme relational model fails toaccumulate those important theme expressions.Our observation is at most 3-5% improvementcould be possible on final system by granularimprovement of every linguistic tool.9 ConclusionIn this work we have reported our work on sin-gle-document opinion summarization for Benga-li.
The novelty of the proposed technique is thetopic based document-level theme relationalgraphical representation.
According to best ofour knowledge this is the first attempt on opi-nion summarization for Bengali.
The approachpresented here is unique in every aspect as inliterature and for a new language like Bengali.Our next research target is to generate a hie-rarchical cluster of theme words with time-framerelations.
Time-frame relations could be usefulfor time wise opinion tracking.239ReferencesCarenini Giuseppe, Ng Raymond, and Pauls Adam.Multi-document summarization of evaluative text.In Proceedings of the European Chapter of the As-sociation for Computational Linguistics (EACL),pages 305?312, 2006.Chesley Paula, Vincent Bruce, Xu Li, and Srihari Ro-hini.
Using verbs and adjectives to automaticallyclassify blog sentiment.
In AAAI Symposium onComputational Approaches to Analysing Weblogs(AAAI-CAAW), pages 27?29, 2006.Das, A. and Bandyopadhyay S. (2009b) Theme De-tection an Exploration of Opinion Subjectivity.
InProceeding of Affective Computing & IntelligentInteraction (ACII).Das, A. and Bandyopadhyay, S. (2009a).
SubjectivityDetection in English and Bengali: A CRF?basedApproach., In Proceeding of ICON 2009, Decem-ber 14th-17th, 2009, Hyderabad.Das, A. and Bandyopadhyay, S. (2010a).
SentiWord-Net for Bangla.
In Knowledge Sharing Event-4:Task 2: Building Electronic Dictionary , February23th to 24th, 2010, Mysore.Das, A. and Bandyopadhyay, S. (2010b).
Morpholo-gi-cal Stemming Cluster Identification for Bangla.,In Knowledge Sharing Event-1: Task 3: Morpho-logi-cal Analyzers and Generators, January, 2010,Mysore.Fruchterman Thomas M. J. and Edward M. Reingold.1991.
Graph drawing by force-directed placement.Software: Practice and Experience, 21(11):1129?1164.Ghosh A., Das A., Bhaskar P., Bandyopadhyay S.(2009).
Dependency Parser for Bengali : the JUSystem at ICON 2009., In NLP Tool ContestICON 2009, December 14th-17th, 2009a, Hydera-bad.Hatzivassiloglou Vasileios and Wiebe Janyce.
Effectsof adjective orientation and gradability on sentencesubjectivity.
In Proceedings of the InternationalConference on Computational Linguistics (COL-ING), pages 299-305, 2000.Hu M. and Liu B.. 2004a.
Mining and summarizing-customer reviews.
In Proc.
of the 10th ACM-SIGKDD Conf., pages 168?177, New York, NY,USA.
ACM Press.Jardine, N. and van Rijsbergen, C. J.
(1971).
The useof hierarchic clustering in information retrieval.In-formation Storage and Retrieval, 7, 217-240.Kawai Yukiko, Kumamoto Tadahiko, and KatsumiTanaka.
Fair News Reader: Recommending newsarticles with different sentiments based on userpre-ference.
In Proceedings of Knowledge-BasedIntel-ligent Information and Engineering Systems(KES), number 4692 in Lecture Notes in Comput-er Science, pages 612?622, 2007.Ku Lun-Wei, Li Li-Ying, Wu Tung-Ho, and  ChenHsin-Hsi.
Major topic detection and its applicationto opinion summarization.
In Proceedings of theACM Special Interest Group on Information Re-trieval (SIGIR), pages 627?628, 2005.
Poster pa-per.Nasukawa Tetsuya and Yi Jeonghee.
Sentiment anal-ysis: Capturing favorability using natural languageprocessing.
In Proceedings of the Conference onKnowledge Capture (K-CAP), pages 70-77, 2003.Page Lawrence, Brin Sergey, Rajeev Motwani, andTerry Winograd.
1998.
The PageRank CitationRanking: Bringing Order to the Web.
Technical re-port, Stanford Digital Library TechnologiesProject.Resnick Paul, Kuwabara Ko, Zeckhauser Richard,and Friedman Eric.
Reputation systems.
Commu-nications of the Association for Computing Ma-chinery (CACM), 43(12):45?48, 2000.
ISSN0001-0782.Smith Marc, Shneiderman Ben, Natasa Milic-Frayling, Eduarda Mendes Rodrigues, VladimirBarash, Cody Dunne, Tony Capone, Adam Perer,and Eric Gleave.
2009.
Analyzing (social media)networks with NodeXL.
In C&T ?09: Proc.
FourthInternational Conference on Communities andTechnologies, Lecture Notes in Computer Science.Springer.Willerr, P. (1988).
Recent trends in hierarchic docu-ment clustering: A critical review.
InformationProcessing and Management, 24(5), 577-597.Zhou Liang and Hovy Eduard.
On the summarizationof dynamically introduced information: Onlinedis-cussions and blogs.
In AAAI Symposium onCom-putational Approaches to Analysing Weblogs(AAAI-CAAW), pages 237?242, 2006.240
