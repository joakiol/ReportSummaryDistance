Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 229?237,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPImproved Word Alignment with Statistics and Linguistic HeuristicsUlf HermjakobUniversity of Southern CaliforniaInformation Sciences Institute4676 Admiralty WayMarina del Rey, CA 90292, USAulf@isi.eduAbstractWe present a method to align words ina bitext that combines elements of a tra-ditional statistical approach with linguis-tic knowledge.
We demonstrate this ap-proach for Arabic-English, using an align-ment lexicon produced by a statisticalword aligner, as well as linguistic re-sources ranging from an English parserto heuristic alignment rules for functionwords.
These linguistic heuristics havebeen generalized from a development cor-pus of 100 parallel sentences.
Our aligner,UALIGN, outperforms both the commonlyused GIZA++ aligner and the state-of-the-art LEAF aligner on F-measure and pro-duces superior scores in end-to-end sta-tistical machine translation, +1.3 BLEUpoints over GIZA++, and +0.7 over LEAF.1 IntroductionWord alignment is a critical component in trainingstatistical machine translation systems and has re-ceived a significant amount of research, for exam-ple, (Brown et al, 1993; Ittycheriah and Roukos,2005; Fraser and Marcu, 2007), including workleveraging syntactic parse trees, e.g., (Cherry andLin, 2006; DeNero and Klein, 2007; Fossum etal., 2008).
Word alignment is also a requiredfirst step in other algorithms such as for learningsub-sentential phrase pairs (Lavie et al, 2008) orthe generation of parallel treebanks (Zhechev andWay, 2002).Yet word alignment precision remains surpris-ingly low, under 80% for state-of-the-art alignerson not closely related language pairs.Consider the following Arabic/English sen-tence pair with alignments built by the statisticalword aligner LEAF:Bitext Arabic: 	?A 	?A ?
KQ ?
?P?X@PA K.?YKA ?
KAJ ?
@P A????J?J?
@?
, 6 - 4 ?
6 - 4 	?QJ.JJ??J????KAk.??@QJ?A?
@ ??
?6 - 4 ?
6 - 4Q?????P?K.?PA?
?KA??
A?
@ ???
?JKA?
?QJKGloss: Won(1) Thai Paradorn Srichaphan(1)on/to(2) Australian Jason(2) Stoltenberg(3) 6(4) -4(5) and(3) 6(4) - 4(5), and Czech Jir???
(7) Vane?k(7)on/to German(6) Lars Burgsmu?ller 6(4) - 4 and(3)6(4) - 4Bitext English: Thailand ?s(1) Baradorn Srich-fan(1) beat(2) Australian Gayson(1) Stultenberg(3)6(4) - 6(4) 6(4) - 4(5) , Czech player(1) Pierre(1)Vanic(7) beat(6) Germany(6) ?s Lars Burgsmuller 6- 4 6 - 4In the example above, words with the same indexin the gloss for Arabic and the English are alignedto each other, alignment errors are underlined,translation errors are in italics.
For example, theArabic words for won and Srichaphan are alignedwith the English words ?s, Srichfan, Gayson,player and Pierre.As reflected in the example above, typical align-ment problems include words that change sentence position betweenlanguages, such as verbs, which in Arabicare often sentence-initial (e.g.
won/beat in theexample above) function words without a clear and explicitequivalent in the other language (e.g.
the Ara-bic ?/and in the example above) lack of robustness with respect to poor trans-lations (e.g.
Gayson Stultenberg instead ofJason Stoltenberg) or bad sentence align-ment.We believe we can overcome such problemswith the increased use of linguistically based229heuristics.
We can model typical word order dif-ferences between English and Arabic using En-glish parse trees and a few Arabic-specific phrasereordering heuristics.
We can narrow the space ofpossible alignment candidates for function wordsusing English parse trees and a few heuristics foreach type of function word.These heuristics have been developed using adevelopment corpus of 100 parallel sentences.
Theheuristics are generalizations based on patternsof misaligned words, misaligned with respect toa Gold Standard alignment for that developmentcorpus.The following sections describe how our wordaligner works, first how relatively reliable contentwords are aligned, and then how function wordsand any remaining content words are aligned, witha brief discussion of an interesting issue relatingto the Gold Standard we used.
Finally we presentevaluations on word alignment accuracy as wellas the impact on end-to-end machine translationquality.2 Phase I: Content WordsWe divide the alignment process into two phases:first, we align relatively reliable content words,which in phase II we then use as a skeleton to alignfunction words and remaining content words.Function words such as English a, ah, all, am,an, and, any, are, as, at, ... are common wordsthat often do not have an explicit equivalent wordor words in the other side of the bitext.
In oursystem, we use a list of 96 English and 110 Ara-bic function words with those characteristics.
Forthe purposes of our algorithm, a word is a functionword if and only if it is on the function word listfor its language.
A content word then is defined asa word that is neither a function word nor punctu-ation.The approach for aligning content words inphase I is as follows: First, we score each com-bination of an Arabic content word and Englishcontent word in an aligned sentence and alignthose pairs that pass a threshold, typically gener-ating too many alignments.
Second, we computea more comprehensive score that also takes intoconsideration matching alignments in the contextaround each alignment.
Third, we eliminate infe-rior alignments that are incompatible with higher-scoring alignments.The score in the first step is pointwise mutualinformation (PMI).
The key resource to computethis PMI is an alignment lexicon generated be-forehand by a statistical word alignment systemfrom a large bitext.
An alignment lexicon is alist of triples, each consisting of an English word,an Arabic word, and how often they have beenaligned for a given bitext.
Additional counts onhow often each English and Arabic word occursallow us use this alignment lexicon to computePMI(e,f) = log p(e;f)p(e)p(f).
We align those Arabicand English content words that have a PMI > 0and a minimum alignment lexicon count ( 10initially).
Using the alignment lexicon generatedby a statistical word aligner to compute PMIs isthe principal statistical component in our system.We explored alternative metrics such as the dice-coefficient that was used by other researchers inearlier alignment work, but found PMI to workbetter for our system.In a second step, we lay a window of size 5around each aligned pair of Arabic and Englishwords (counting only content words) and then addto the PMI score of the link itself the PMI scoresof other links within that window, with a distanceweight of 1distance+1.
This yields a new score thattakes into account whether a link is supported bycontext.In the third step, we check for overgeneratedlinks, comparing links that share an Arabic or anEnglish word.
If a word on one side of the bitextis linked to multiple adjacent words on the other,we leave them alone, as one word in one languageoften corresponds to multiple words in the other.However, if a word on one side is linked to non-adjacent words on the other side, this flags an in-compatibility, and we remove those links that haveinferior context-sensitive scores.
This removal isdone one link at a time, with the lowest relativescores first.We boost the process we just described in a fewways.
In the first alignment step, we also includeas alignment candidates any content words that arestring-identical on each side, such as ASCII num-bers and ASCII words.
We finally also includeas alignment candidates those word pairs that aretransliterations of each other to cover rare propernames (Hermjakob et al, 2008), which is impor-tant for language pairs that don?t share the samealphabet such as Arabic and English.2302.1 Reordering Using an English ParserWe use a refined notion of context window thatmodels word order differences between Arabicand English.
Traversing a parse tree for English,we identify sub-trees for which the order in Ara-bic can be substantially different.
In Arabic, forexample, the verb is often sentence-initial.
So fortrees or subtrees identified by the parser as sen-tences, we generate an alternative reordering ofits subtrees where the verb has been moved to thefront.
Similarly, in a noun phrase, we generate analternative order where adjectives are moved to theright of the noun they modify.For example, consider the sentence John boughta new car .
We can reorder its parse tree both atthe sentence level: (bought) (John) (a new car) (.
)as well as at its object NP level: (a) (car) (new).If fully enumerated, this would yield these fourreordering alternatives:1.
John bought a new car .2.
John bought a car new .3. bought John a new car .4. bought John a car new .We don?t actually explicitly enumerate all variantsbut keep all reordering alternatives in a reorder-ing forest, since the number of fully expanded re-orderings grows exponentially with the number ofphrases with reordering(s).
At the beginning ofPhase I, we compute from this reordering forest aminimum distance matrix, which, for specific in-stances of the words John and car would recorda minimum distance of 1 (based on reordering 4,skipping the function word a).For the example sentence at the beginning of thepaper we would get reorderings including the fol-lowing:Engl.
orig.
: thailand ?s baradorn srichfan beat ...A reordering: beat thailand ?s baradorn srichfan ...Arabic (gloss): won thai paradorn srichaphan ...In the above reordered English alternative, beatand thailand are next to each other, so their min-imum distance is 1, which means that a linkbetween English thailand and Arabic thai nowstrongly boosts the context-sensitive score be-tween English beat and Arabic won.2.2 Morphological VariationAnother challenge to content word alignment ismorphological variation which can create datasparsity in the alignment lexicon.
For example,in a given bitext sentence, the Arabic word AlAw-DAE might be translated as situational, for whichthere might be no support in the alignment lex-icon.
However the PMI between AlAwDAE andsituation might be sufficiently high.
Additionally,there is another Arabic word, AlHAlAt, which of-ten translates as both situation and situational.To take advantage of such constellations, webuilt morphological variation lists for both Arabicand English, lists that for a given head word suchas situational lists variants such as situation, andsituations.We built these lists in a one-time process byidentifying superficially similar words, i.e.
thosethat vary only with respect to an ending or a prefix,and then semantically validating such candidatesusing a pivot word in the other language such asAlHAlAt that has sufficiently strong alignment lex-icon co-alignment counts with both situation andsituational.
The alignment lexicon co-alignmentcount of an Arabic word warand an English wordwenis considered strong enough, if it is at least2.0 and at least 0.001 times as high as the high-est co-alignment count of warwith any Englishword; words shorter than four letters are excludedfrom consideration.
So because situation and situ-ational are superficially similar and they are bothhave a strong alignment count with AlHAlAt in thealignment lexicon, situation is added to the En-glish morphological variation list as a variant ofsituational and vice versa.Exploring whether we can align situational andAlAwDAE in the bitext, we find that situationalis a morphological variant of situation (based onour morphological variation list for English); nextwe find that based on the alignment lexicon, thereis a positive PMI between situation and AlAw-DAE, which completes the chain between situ-ational and AlAwDAE, so we include them asan alignment candidate after all.
The PMI ofsuch a morphological-variation-based candidate isweighted by a ?penalty?
factor of 0.5 when com-pared with the PMI of any competing alignmentcandidate without such morphological-variationstep.Similarly, the English pivot word situations canbe used to semantically validate the similaritybetween Arabic AlAwDAE and AwDAE for ourArabic morphological variation list.
The resultingArabic morphological variation list has entriesfor 193,263 Arabic words with an average of4.2 variants each; our English morphologicalvariation list has 57,846 entries with 2.8 variants231each.At the end of phase I, most content words willbe aligned with relatively high precision.
Sincefunction words often do not have an explicit equiv-alent word or words in the other side of a bi-text, they can not be aligned as reliably as con-tent words based on bilingual PMI.1 Note thatdue to data sparsity, some content words will re-mained unaligned in phase I and will subsequentlybe aligned in phase II as explained in section 3.3.3 Phase II: Function WordsIn Phase II, we align function words, punctua-tion, and some remaining content words.
Func-tion words can be classified into three categories:monovalent, divalent and independent.
Monova-lent function words modify one head; they in-clude articles (which modify nouns), possessivepronouns, demonstrative adjectives and auxiliaryverbs.
Divalent function words connect two wordsor phrases; they include conjunctions and prepo-sitions.
Independent function words include non-possessive pronouns and copula (e.g.
is as a mainverb).
Each of these types of function words isaligned according to its own heuristics.In this section we present three representativeexamples, one for articles (monovalent), one forprepositions (divalent), as well as a structuralheuristic.3.1 Example: ArticlesMonovalent function words have the simplestheuristics.
Recall that Arabic does not have ar-ticles (only a definite prefix Al- added to one ormore words in a definite noun phrase), so there isusually no explicit equivalent of the English articleon the Arabic side.For an English article, our system identifies theEnglish head word that it modifies based on theEnglish parse tree, and then aligns it with the sameArabic word(s) which that head word is alignedwith.3.2 Example: PrepositionsDivalent function words are much more interest-ing.
In many cases, an English preposition corre-sponds to an explicit Arabic preposition in basi-1It is this lack of reliability that is the defining charac-teristic of our function words, differentiating them from theconcept of marker words used in EBMT chunking (Way andGough, 2002).cally the same position.
Alignment in that case isstraightforward.
However, some Arabic preposi-tions and even more English prepositions do nothave an explicit counterpart on the other side.
Wecall such prepositions orphan prepositions.
TheEnglish preposition of is almost always orphanedin this way.The decision how to align such an orphanpreposition is not trivial.
Consider the bitext is-land of Basilan/jzyrp bAsylAn, a typical (NP1 (PNP2)) construction on the English side.
Shouldwe co-align the preposition of with the head ofNP1 or the head of NP2?
In English syntax, thepreposition is grouped with NP2, but a prepositionis often better ?motivated?
by NP1.
We thereforedecided to use the English parse tree to identifythe heads of both NP1 and NP2, identify the Ara-bic words aligned to these heads as candidates, andthen align the preposition to the Arabic candidateword with which it has the highest bilingual PMI.It turns out that in most cases this will be the can-didate on the ?left?.
For the example at the top ofthis paragraph, of will be aligned with jzyrp (?is-land?
), which is actually desirable for MT, as it fa-cilitates subsequent rule extraction of type ?islandof X/jzyrp X?.
We refer to this orphan prepositionalignment style as MT-style.According to the gold standard alignmentguidelines used for the LDC Gold Standard how-ever, an orphan preposition should always bealigned to the ?right?, to bAsylAn in the exampleabove.
We therefore implemented an alternativeGS-style (for ?Gold Standard?)
to be able to laterevaluate the impact of these alternatives alignmentstyles.The question whether GIZA or LEAF align-ments will indeed give meaningful scores to sup-port the MT-style attachments will be answered bythe MT experiments described in section 4.3.Here is a more complex example with Arabic(A), its gloss (G) and English (E):Arabic: P@?k.???J?
????J?QJ?A?@H@QKA??@HPA?
@ YkA?
@Gloss: sunday attacked aircraft american on/to area jiwarEngl.
: on sunday american aircraft attacked the area of jiwarFor the Arabic orphan preposition ???/ElY(?on/to?
), our system identifies two candidatesbased on the English parse tree: attacked and area.Based on a higher mutual information, our systemthen aligns Arabic ElY (?on/to?)
with English at-tacked, which results in the English word attackednow being aligned to both Arabic attacked and the232Arabic on/to, even though they are not adjacent.In the Gold Standard, Arabic on/to is aligned withEnglish area, and LEAF aligns it with English on(yes, the one preceding Sunday).
This is appar-ently very tempting as Arabic on/to is often trans-lated as English on, but here it is incorrect, and oursystem avoids this tempting alignment because itis ruled out linguistically.Note that in some cases, such as sentence-initialprepositional phrases, there is only one candidate;occasionally, when relevant content words remainunaligned, no candidate can be identified, in whichcase the orphan preposition remains unaligned aswell.3.3 Example: AdjectivesIt is not uncommon that content words that wewould like to be aligned are not supported by thealignment lexicon, due to general data sparsityor maybe a somewhat unorthodox translation.
Inthose cases we can use structure and word orderknowledge to make reasonable alignments any-way.Consider an English noun phase ADJ-ENOUN-E and the corresponding Arabic NOUN-A ADJ-A.
If the nouns are already aligned, but theadjectives are not yet algned, we can use the En-glish parse tree to identify ADJ-E as a modifierto NOUN-E, and, aware that adjectives in Arabicpost-modify their nouns, identify the correspond-ing Arabic word based on structure and word orderalone.
This can be done the other way around aswell (link nouns based on already aligned adjec-tives) and other elements of other phrases as well.As more and more function words and re-maining content words get algned, heuristics thatweren?t applicable before may now apply to theremaining unaligned words, so we perform fourpasses through a sentence pair to align unalignedwords using heuristics.
We found that an addi-tional fifth pass did not yield any further improve-ments.4 ExperimentsWe evaluated our word aligner in terms of bothalignment accuracy and its impact on an end-to-end machine translation system.4.1 Alignment ExperimentsWe evaluated our word aligner against a GoldStandard distributed by LDC.
The human align-ments of the sentences in this Gold Standard arebased on the 2006 GALE Guidelines for ArabicWord Alignment Annotation.Both the 100-sentence development set and theseparate 837-sentence test set are Arabic newswiresentences from LDC2006E86.
The test set in-cludes only sentences for which our English parser(Soricut and Marcu, 2003) could produce a parsetree, which effectively excluded a few very longsentences.In the first set of experiments, we comparetwo settings of our UALIGN system with otheraligners, GIZA++ (Union) (Och and Ney, 2003)and LEAF (with 2 iterations) (Fraser and Marcu,2007).
The GIZA++ aligner is based on IBMModel 4 (Brown et al, 1993).
We chose GIZAUnion for our comparison, because it led to ahigher BLEU score for our overall MT system thanother GIZA variants such as GIZA Intersect andGrow-Diag.
The two settings of our system vary inthe style on how to align orphan prepositions.
Be-sides precision, recall and (balanced) F-measure,we also include an F-measure variant strongly bi-ased towards recall (=0.1), which (Fraser andMarcu, 2007) found to be best to tune their LEAFaligner for maximum MT accuracy.
GIZA++ andLEAF alignments are based on a parallel train-ing corpus of 6.6 million sentence pairs, incl.
theLDC2006E86 set mentioned above.Aligner Prec.
Recall F-0.5 F-0.1GIZA 26.9 84.3 40.8 69.5LEAF 73.3 79.7 76.4 79.0UALIGN MT-style 82.5 80.0 81.2 80.2UALIGN GS-style 84.0 82.9 83.5 83.0Table 1: Alignment precision, recall, F-measure(=0.5), F-measure(=0.1) for different aligners;with UALIGN using LEAF alignment lexicon.Our aligner outperforms both GIZA and LEAFon all metrics.
Not surprisingly, the GS-stylealignments, which align ?orphan?
prepositions ac-cording to Gold Standard guidelines, yield higherscores than MT-style alignments.
And interest-ingly by a remarkably high margin.In a second set of experiments, we measure theimpact of using different input alignment lexiconused by our aligner on alignment accuracy.
In onecase UALIGN uses as input the alignment lexiconproduced by LEAF, in the other the alignment lex-icon produced by GIZA.
All experiments in table 2233are for UALIGN.Style A-Lexicon Prec.
Recall F-0.5 F-0.1MT from LEAF 82.5 80.0 81.2 80.2MT from GIZA 80.8 79.2 80.0 79.4GS from LEAF 84.0 82.9 83.5 83.0GS from GIZA 82.1 81.8 82.0 81.9Table 2: Alignment precision, recall, F-measure(=0.5), F-measure(=0.1), all of UALIGN, fordifferent alignment styles, different input align-ment lexicons.As LEAF clearly outperforms GIZA on F-0.1(79.0 vs. 69.5, see table 1), the alignment lexiconbased on LEAF is better, so it is not surprisingthat when we use an alignment lexicon based onGIZA, all metrics degrade, and consistently so forboth alignment styles.
However the drop in F-0.1of about 1 point (80.2 !
79.4 and 83.0 !
81.9)is much smaller than the differences between theunderlying aligners themselves.
Our aligner there-fore degrades quite gracefully for a worse align-ment lexicon.Aligner Arabic aligned Engl.
alignedGIZA Union 100% 100%LEAF 99.99% 97.25%UALIGN 92.10% 91.55%Gold Standard 95.37% 95.86%Table 3: Percentages of Arabic and English wordsalignedTable 3 shows how much LEAF and UALIGNdiffer in the percentage of Arabic and Englishwords aligned (correctly or incorrectly).
LEAFis much more aggressive in making alignments,aligning almost every Arabic word.
Our alignerstill leaves some 8% of all words in a sentence un-aligned (an opportunity for further improvements).For comparison, in the Gold Standard, 4-5% of allwords in our test corpus are left unaligned.4.2 Impact of Sub-ComponentsTo better understand the impact of several align-ment system sub-components, we ran a number ofexperiments disabling individual sub-componentsand then comparing the resulting alignment scoreswith those of the full system.
We also measuredalignment scores running Phase II with 0 to 5passes.
The test set was the same as in section4.1.System Prec.
Recall F-0.1Full system (FS) 84.0 82.9 83.0FS w/o morph.variation 84.0 82.4 82.5FS w/o Engl.
tree reord.
83.8 82.7 82.8FS w/o string identity 84.0 82.8 82.9FS w/o name translit.
84.0 82.8 82.9System after Phase I 90.6 44.5 46.8+ Phase II w/ 1 pass 87.6 77.1 78.0+ Phase II w/ 2 passes 85.8 80.3 80.8+ Phase II w/ 3 passes 84.2 82.7 82.8+ Phase II w/ 4 passes 84.0 82.9 83.0+ Phase II w/ 5 passes 84.0 82.9 83.0Table 4: Impact of sub-components on alignmentprecision, recall, F-measure, with GS-style attach-ments, based on the LEAF alignment lexicon.Special sub-components of Phase I includeadding link candidates for ASCII-string-identicalwords and transliterated names (see last paragraphbefore section 2.1), reordering using an Englishparser (section 2.1) and morphological variation(section 2.2).
Each of these sub-components pro-vides a small boost to F-0.1, ranging from +0.1 to+0.5.
The second part of the table shows align-ment scores before and after each pass of Phase II.Our full system includes 4 passes; an additional5th pass did not yield any further improvements.Note that during Phase II, precision drops.
This isa reflection of (1) our strategy to first align rela-tively reliable content words in Phase I, followedby less reliable function words and remaining con-tent words, and (2) the challenges of building reli-able Gold Standard alignments for function wordsand non-literal translations.4.3 MT ExperimentsThe ultimate test for a word aligner is to mea-sure its impact on an end-to-end machine trans-lation system.
For this we aligned 170,863 pairsof Arabic/English newswire sentences from LDC,trained a state-of-the-art syntax-based statisticalmachine translation system (Galley et al, 2006)on these sentences and alignments, and measuredBLEU scores (Papineni et al, 2002) on a sepa-rate set of 1298 newswire test sentences.
Besidesswapping in a new set of alignments for the sameset of training sentences, and automatically retun-ing the parameters of the translation system foreach set of alignments, no other changes or ad-justments were made to the existing MT system.234In the first set of experiments, we compare twosettings of our UALIGN system with other align-ers, again GIZA++ (Union) and LEAF (with 2 it-erations).
The two settings vary in the alignmentlexicon that the UALIGN aligner uses as input.Aligner BLEUGIZA 47.4LEAF 48.0UALIGN using GIZA alignment-lexicon 48.4UALIGN using LEAF alignment-lexicon 48.7Table 5: BLEU scores in end-to-end statistical MTsystem based on different aligners.
Both UALIGNvariants use MT-style alignments.With a BLEU score of 48.7, UALIGN usinga LEAF alignment-lexicon is significantly bet-ter than both GIZA (+1.3) and LEAF (+0.7).This and other significance assertions in this pa-per are based on paired bootstrap resamplingtests with 95% confidence.
UALIGN usinga GIZA alignment-lexicon significantly outper-forms GIZA itself (+1.0).In a second experiment, we measured the im-pact of the two alignment styles on BLEU.
Re-call that for GS-style alignments, orphan preposi-tions are always co-aligned to the right, followingGold Standard annotation guidelines, whereas forMT-style alignments, mutual information is usedto decide whether to align orphan prepositions tothe left or to the right.Aligner BLEULEAF 48.0UALIGN with GS-style alignments 48.0UALIGN with MT-style alignments 48.7Table 6: BLEU scores in end-to-end statistical MTsystem based on different alignment styles for or-phan prepositions.
Both UALIGN variants use aLEAF alignment lexicon.While the GS-style alignments yielded a 2.8point higher F-0.1 score (83.0 vs. 80.2), the MT-style alignments result in a significantly betterBLEU score (48.7 vs. 48.0).
This shows that(1) a seemingly small difference in alignmentstyles can have a remarkably high impact on bothBLEU scores and alignment accuracy as measuredagainst a Gold Standard, and that (2) optimiz-ing alignment accuracy against an alignment GoldStandard does not necessarily optimize BLEU inend-to-end MT.
The latter has been observed byother researchers before, but these results addi-tionally suggest that the gold-standard annotationstyle might itself have to shoulder part of theblame.4.4 Corpus Noise RobustnessIn a small random ?sanity check?
sample fromthe 170,863 training sentences for the MT exper-iment, we found cases where the sentence in onelanguage contained much more material than thesentence in the other language.
Consider, for ex-ample the following sentence pair (with spuriousmaterial underlined):Arabic:,?YJ??
@ A?JK??
@X@ ?K@ ???
?JKQk@ YJK.?AJ?
A?K@??
?Gloss: but also there-is clause another stipulateson/to that if not established the-hotel ,English: but , also there is another clause thatstipulates that if the hotel is not established ,then the government shall be compensated .Both LEAF and UALIGN correctly align the En-glish ?but , also ... not established ,?
with theArabic side.
LEAF further aligns all words in thespurious English ?then the government shall becompensated .?
with seemingly random materialon the Arabic side, whereas UALIGN leaves thesespurious words completely unaligned.
It wouldbe reasonable to speculate that this behavior, ob-served in several cases, may be contributing to thegood BLEU scores.5 DiscussionBuilding on existing statistical aligners, our newword aligner significantly outperforms the bestword aligner to date in both alignment error rateand BLEU score.We have developed an approach to word align-ment that combines a statistical component withlinguistic heuristics.
It is novel in that it goesbeyond generic resources such as parsers, addingheuristics to explicitly model word order differ-ences and function word alignment.The approach has numerous benefits.
Our sys-tem produces superior results both on alignmentaccuracy and end-to-end machine translation qual-ity.
Alignments have a high precision.
The systemis fast (about 0.7 seconds per sentence), and sen-tences are aligned individually so that a large cor-pus can easily be aligned on several computers in235parallel.
All alignment links are tagged with ad-ditional information, such as which phase and/orheuristic created them, yielding extensive explana-tory power to the developer for easy understandingon how the system arrived at a given alignment.Our approach needs and uses a parser for only oneside (English) and not for the other (Arabic).On the other hand, some of the componentsof this aligner are language-specific, such asword order heuristics, the list of specific functionwords, and morphological variation lists.
Whilethese parts of the system need to be adapted fornew languages, the overall architecture and typesof heuristics and function words are language-independent.
Chinese for example has differentspecific types of function words such as aspectmarkers and measure words.
But these fall into theexisting category of monovalent function wordsand will be treated according the same principlesas other monovalent function words (section 3.1).Similarly, Japanese postpositions would be treatedlike other divalent function words (such as Arabicor English prepositions).
The author and devel-oper has a basic knowledge of Arabic in general,and an intermediate knowledge of Arabic gram-mar, which means that no intimate knowledgeof Arabic was required to develop the language-specific components.
This same author and devel-oper recently started to adapt UALIGN to Chinese-English word alignment.The alignment rate is still somewhat low.
Weplan to increase it by enlarging our develop-ment set beyond 100 sentences and adding furtherheuristics, as well as generalizing the output wordalignment structure to allow alignments of wordsto larger constituents in a tree, and to explicitly as-sert that some words are not covered by the otherside of a bitext to model poor translations and poorsentence alignments.AcknowledgmentThis research was supported under DARPA Con-tract No.
HR0011-06-C-0022.
The author wouldlike to thank Kevin Knight and the anonymous re-viewers for their helpful suggestions, and SteveDeNeefe for running the end-to-end MT evalua-tions.ReferencesPeter E. Brown, Vincent J. Della Pietra, Stephen A.Della Pietra and Robert L. Mercer.
1993.
The Math-ematics of Statistical Machine Translation: Parame-ter Estimation.
In Computational Linguistics Vol.19(2), pages 263?311.Colin Cherry and Dekang Lin.
2006.
Soft SyntacticConstraints for Word Alignment Through Discrimi-native Training.
In Proceedings of the 44th AnnualMeeting on Association for Computational Linguis-tics, Sydney, Australia, pages 105?112.John DeNero and Dan Klein.
2007.
Tailoring WordAlignments to Syntactic Machine Translation.
InProceedings of the 45th Annual Meeting on Associ-ation for Computational Linguistics, Prague, CzechRepublic, pages 17?24.Victoria Fossum, Kevin Knight and Steven Abney.2008.
Using Syntax to Improve Word AlignmentPrecision for Syntax-Based Machine Translation.
InProceedings of the ACL Workshop on Statistical Ma-chine Translation, Columbus, Ohio, pages 44?52.Alexander Fraser and Daniel Marcu.
2007.
Gettingthe Structure Right for Word Alignment: LEAF.
InProceedings of Conference for Empirical Methodsin Natural Language Processing (EMNLP), Prague,Czech Republic, pages 51?60.Alexander Fraser and Daniel Marcu.
2007.
Mea-suring Word Alignment Quality for Statistical Ma-chine Translation.
In Computational LinguisticsVol.
33(3), pages 293?303.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer 2006.
Scalable Inference and Training ofContext-Rich Syntactic Translation Models.
In Pro-ceedings of the 44th Annual Meeting on Associationfor Computational Linguistics, Sydney, Australia,pages 961?968.Ulf Hermjakob, Kevin Knight, and Hal Daume?
III2008.
Name Translation in Statistical MachineTranslation: Learning When to Transliterate.
InProceedings of the 46th Annual Meeting on Asso-ciation for Computational Linguistics, Columbus,Ohio, pages 389?397.Abraham Ittycheriah and Salim Roukos.
2005.A Maximum Entropy Word Aligner for Arabic-English Machine Translation.
In Proceed-ings of Joint Conference of Human LanguageTechnology and Empirical Methods in NaturalLanguage Processing (HLT/EMNLP), Vancouver,British Columbia, Canada, pages 89?96.Alon Lavie, Alok Parlikar and Vamshi Ambati.
2008.Syntax-Driven Learning of Sub-Sentential Transla-tion Equivalents and Translation Rules from ParsedParallel Corpora.
In Proceedings of the ACL/HLTSecond Workshop on Syntax and Structure in Statis-tical Translation (SSST-2), Columbus, Ohio, pages87?95.236Dan Melamed.
2000.
Models of translational equiv-alence among words.
In Computational LinguisticsVol.
26(2), pages 221?249.Franz Josef Och and Hermann Ney.
2003.
A System-atic Comparison of Various Statistical AlignmentModels.
In Computational Linguistics Vol.
29(1),pages 19?51.Franz Josef Och and Hermann Ney.
2004.
TheAlignment Template Approach to Statistical Ma-chine Translation.
In Computational LinguisticsVol.
30(4), pages 417?449.K.
Papineni, S. Roukos, T. Ward, and W. J. Zhu.
2002.BLEU: a Method for Automatic Evaluation of Ma-chine Translation.
In Proceedings of the 40th An-nual Meeting on Association for Computational Lin-guistics, Philadelphia, PA, pages 311?318.Radu Soricut and Daniel Marcu.
2003.
Sentence LevelDiscourse Parsing Using Syntactic and Lexical In-formation.
In Proceedings of the 2003 Conferenceof the North American Chapter of the Associationfor Computational Linguistics on Human LanguageTechnology, Edmonton, Canada, pages 149?156.Andy Way, Nano Gough.
2003. wEBMT: develop-ing and validating an example-based machine trans-lation system using the world wide web In Compu-tational Linguistics Vol.
29(3), pages 421?457.Ventsislav Zhechev, Andy Way.
2008.
AutomaticGeneration of Parallel Treebanks.
In Proceed-ings of 22nd International Conference on Compu-tational Linguistics (COLING), Manchester, UK,pages 1105?1112.237
