The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 225?232,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsVTEX Determiner and Preposition Correction Systemfor the HOO 2012 Shared TaskVidas Daudaravic?iusVTEXAkademijos 4LT-08412 Vilnius, Lithuaniavidas.daudaravicius@vtex.ltAbstractThis paper describes the system has beendeveloped for the HOO 2012 Shared Task.The task was to correct determiner andpreposition errors.
I explore the possibil-ity of learning error correcting rules fromthe given manually annotated data usingfeatures such as word length and wordendings only.
Furthermore, I employ er-ror correction ranking based on the ratioof the sentence probabilities using originaland corrected language models.
Our sys-tem has been ranked for the ninth posi-tion out of thirteen teams.
The best resultwas achieved in correcting missing prepo-sitions, which was ranked for the sixth po-sition.1 IntroductionThe correct usage of determiners and preposi-tions is one of the toughest problems in Englishlanguage use for non-native speakers, especiallythose living in a non-English speaking environ-ment.
The issues have been explored extensivelyin the literature (see Leacock et al (2010)).
Itwas interesting to find that this error correctiontopic was chosen for the HOO 2012 Shared Task.This paper describes the experimental sys-tem developed by VTEX team for this task ?to correct determiner and preposition errors inCLC FCE Dataset.
It explores the possibilityof learning error correcting rules from the givenmanually annotated data using features such asword length and word endings only.
Further-more, it employs error correction ranking basedon the ratio of sentence probabilities using orig-inal and corrected language models.2 The dataThe training data consisted of 1000 files drawnfrom the publicly available FCE dataset andconverted into HOO data format (see Dale etal.
(2012)).
I used the HOO 2012 training andtest data only.
The training data had 8432 man-ually annotated corrections of the following sixerror types:MD ?
Missing Determiner;MT ?
Missing Preposition;UD ?
Unwanted Determiner;UT ?
Unwanted Preposition;RD ?
Replacement Determiner;RT ?
Replacement Preposition.The total size of the training data was 374680words.
The test data consisted of 100 previ-ously unseen files without error correction an-notations.
For more details about the trainingand test data, see (Dale et al, 2012).I have not used any other dictionaries, cor-pora or language processing tools (like taggersor parsers).
Thus, the system is language in-dependent and based on supervised learning ofmanually annotated corrections.3 Word length and word endingThe training corpus was small and insufficientto get complete and reliable features and statis-tics of error corrections based on the correctedwords.
Therefore I needed to find featureswhich describe the contexts of error corrections225in a more generalized way.
After some experi-mentation, I chose word length and the word lastn characters.
Words in the dataset were trans-formed into tokens using these functions.
I havetested three word transformation combinations:word ?
keeps the whole word (e.g.
make 7?make);2end ?
takes the length of a word and adds thelast two characters (make 7?
4 ke );1end ?
takes the length of a word and adds thelast character (make 7?
4 e).I have also used lists of reserved words thatwere used to preserve the primary form of aword:corrections ?
words that were correctedto/from in HOO 2012 Gold Edits data;mod ?
functional words such as: have, has, can,not, make, made, be, was, were, am, are,and, or ;pronouns ?
pronouns that were not used ascorrections: we, he, she, they, yours, ours,them.For instance, using 2end transformation, theincorrect sentence I feel that festival could beeven better next year was transformed into I 4elthat 8al 5ld be 4en 6er next 4ar, and the cor-rected sentence into I 4el that the 8al 5ld be 4en6er next 4ar.In Section 5, I show that the word lengthand ending retain a lot of information about theword.Each participating group in HOO 2012 SharedTask was allowed to submit up to ten runs.I have submitted nine runs that differ in wordlength and word ending only.
The different runsare:0 ?
1end: all words except reserved correctionwords were encoded as word length+the lastcharacter;1 ?
2end: all words except reserved correctionwords were encoded as word length + twolast characters;2 ?
word: no transformations;3 ?
1end+mod: all words except reserved correc-tion and mod words were encoded as wordlength + the last character;4 ?
2end+mod: all words except reserved correc-tion and mod words were encoded as wordlength + two last characters;5 ?
1end+pron: all words except reserved cor-rection and pronoun words were encoded asword length + the last character;6 ?
2end+pron: all words except reserved cor-rection and pronoun words were encoded asword length + two last characters;7 ?
1end+mod+pron: all words except reservedcorrection, pronoun and mod words were en-coded as word length + the last character;8 ?
2end+mod+pron: all words except reservedcorrection, pronoun and mod words were en-coded as word length + two last characters.4 Error correctionError correction consists of the rules that thesystem is able to learn, and the actions thatthe system is able to apply.4.1 Error correction rulesUsing error correction annotations from GoldEdits of the training corpus we have built theerror correction rules.
The error correction ruleis the error correction and the context of thiscorrection.
From the training corpus I gathercontextual correction rules.
The context are to-kens on the left- or right-hand side of the errorcorrection.
The best choice would be to take atleast two tokens on the left-hand side and two to-kens on the right-hand side and to express errorcorrection rule as a 5-gram with the error correc-tion in the middle.
For instance, in the trainingdata, the error correction of for to about of typeRT is found within the the left-hand side con-text i asked and the right-hand side context thediscounts.The main problem of learning of correctionrules was the small size of the training corpus.226Bigger corpora could help in learning more cor-rection rules.
But it is hard to get bigger corporabecause it is very expensive to prepare them.Two or three word context on each side of acorrected fragment can produce good but rarelyapplicable correction rules.
Therefore, I haveimplemented a smoothing technique for generat-ing new rules that do not appear in the trainingdata.I use trigrams to generate smoothed 5-gramerror correction rules.
Three types of trigramswere used for the smoothing:centered ?
one token on the left-hand side ofthe correction, then the correction and onetoken on the right-hand side of the correc-tion (see line 1 in Table 1);left ?
two tokens on the left-hand side of thecorrection and the correction (see lines 2and 3 in Table 1);right ?
two tokens on the right-hand side of thecorrection and the correction (see lines 4?13in Table 1).There are 8432 corrections in the trainingdata.
Figure 1 shows the number of distincttrigram rules for the different runs described inSection 3.
Most of the trigram rules appearonce.
For instance,L2 L1 type original correction R1 R2asked RT for about thei asked RT for aboutto asked RT for aboutRT for about the campRT for about the discountsRT for about the experienceRT for about the firstRT for about the newRT for about the newsRT for about the playRT for about the priseRT for about the terribleRT for about the veryTable 1: Trigram error correction rules.?
the most frequent (38 occurrences) left-context trigram rule without word encodingis stay in /a/MD ;?
the most frequent (44 occurrences) right-context trigram rule is on/in/RT july be-cause; and?
the most frequent (38 occurrences)centered-context trigram rule is travelon/in/RT july.We could expect similar generalization powerfor left, right or centered contexts, but in Fig.
1we can see that the number of distinct right-hand side contexts is lower by 5% compare toFigure 1: The number of context trigrams of error corrections for the different runs.227the number of distinct centered contexts.
Sur-prisingly, the number of trigram rules does notdegrade significantly whether the encoding 1endis used or not.The new smoothed 5-gram rules are exten-sions of the centered trigram rules.
The exten-sion on the left hand-side is the union of centeredtrigrams and the left trigrams when the errorcorrection and L1 match.
And the extension onthe right hand-side is the union of the centeredtrigrams and the right trigrams when error cor-rection and R1 match.
For instance, the errorcorrection of for to about of type RT within thethe left-hand side context i asked and the right-hand side context the discounts is extended asfollows:?
take centered trigram (see line 1 in Table 1);?
take left trigrams, where correction and L1match (see lines 2 and 3 in Table 1);?
take right trigrams, where correction andR1 match (see lines 4?13 in Table 1);?
after that I have the following smoothedrule: L2 = [I, to ], L1 = asked, C =for/about/MT, R1 = the, R2 = [camp, dis-counts, experience, first, news, play, prise,terrible, very ].This technique allows the generation of errorcorrection rules that do not appear in the train-ing data, e.g.
in the latter example I generate 18smoothed 5-gram rules that do not appear in thetraining data.
The new smoothed 5-gram errorcorrection rule is boolean operation and the ruledoes not contain any probabilistic information.4.2 Error correction actionsThe error correction system applies error correc-tion rules using the following actions:do not change ?
word is kept as is;insert ?
missing word is inserted;delete ?
unnecessary word is deleted;replace ?
word is replaced by another one.Each action is tested at each word but onlyone at a time.
In case the context allows toapply several actions at one place then these ac-tions are treated as alternatives.
Alternative ac-tions are not combined and no selection betweenDocIDRun RulesappliedOCratiosentence correction20252the//MD/that/this/RD/0.451 is the 8 th july till the end of that month , what do you think ?that/this/RD/ 0.559 is the 8 th july till end of that month , what do you think ?the//MD/ 0.633 is the 8 th july till the end of this month , what do you think ??
0.785 is the 8 th july till end of this month , what do you think ?7the//MD/that/this/RD/0.345 3s the 8 2h 4y till the 3d of that 5h , what 2o you 5k ?that/this/RD/ 0.441 3s the 8 2h 4y till 3d of that 5h , what 2o you 5k ?the//MD/ 0.533 3s the 8 2h 4y till the 3d of this 5h , what 2o you 5k ??
0.683 3s the 8 2h 4y till 3d of this 5h , what 2o you 5k ?20432 /for/UT/ 0.976 i am writing in response to your last letter , to answer and askyou for some questions .?
1.035 i am writing in response to your last letter , to answer and askyou for some questions .7/for/UT/ 0.966 i am 7g in 8e to your 4t 6r , to 6r and 3k you for some 9s .a//MD//for/UT/1.022 i am 7g in a 8e to your 4t 6r , to 6r and 3k you for some 9s .?
1.025 i am 7g in 8e to your 4t 6r , to 6r and 3k you for some 9s .a//MD/ 1.085 i am 7g in a 8e to your 4t 6r , to 6r and 3k you for some 9s .Table 2: Examples of ranking, selection and application of actions for sentence correction.228them is made at this step.
The example of cor-rection alternatives is shown in Table 2.
Besides,the probability of the action can be taken intoaccount but I do not do this and all actions areconsidered equally possible.5 Language modelI use language trigram modeling to estimate theprobability of a sentence.
The probability of asequence of words is estimated as the product ofprobabilities of trigrams:p(x) =?ip?
(xi |xi?2, xi?1).To avoid zero probability I have used Kneser?Ney trigram smoothing (Kneser and Ney, 1995)technique as follows:p?
(xi |xi?2, xi?1)=max[(freq(xi?2, xi?1, xi)?
c3), 0]max[freq(xi?2, xi?1), 1]+c3 ?
|xi?2, xi?1, ?|max[freq(xi?2, xi?1), 1]?max[(freq(xi?2, xi?1)?
c2), 0]max[freq(xi?2), 1]+c2 ?
|xi?2, ?, ?|max[freq(xi?2), 1]?max[(freq(xi?2)?
c1), 0]N+c1 ?
TN,where c3 = 0.8, c2 = 0.6, c1 = 0.4, T = | ?
|, andN is the corpus size.I have built two language models: one for theoriginal language and one for the corrected lan-guage.
The original language model (O) wasbuilt using the corpus without corrections.
Thecorrected language model (C ) was built usingthe corpus with error corrections applied.
Thedifferent runs yield different number of token tri-grams.
But the number does not degrade signif-icantly as we might expect when words are en-coded with the 1end transformation (see Fig.
2).Thus, the 1end transformation retains a lot ofinformation, although, the number of trigramsof the original language model is always a littlebit higher than the number of trigrams of thecorrected language model.6 The probability ratio of theoriginal and corrected languagemodelsThe probability of a sentence depends on thelength of the sentence.
The longer the sen-tence the lower the probability.
Error correc-tion actions can change the length of a sentence.Thus, it is hard to implement the error correc-tion system which should rank different lengthsentences.
Therefore, I have used the ratio of theprobabilities of the sentence using the originallanguage model (O) and the corrected languageFigure 2: The number of trigrams of the original and corrected language models for different runs.229Figure 3: The histogram of OC ratio in the test data.model (C ):OC ratio =p?(O)p?
(C),where p?
(O) is the probability of a sentence us-ing the original language model and p?
(C) is theprobability of the same sentence using the cor-rected language model.The lower the value of this ratio, the higherthe chance that the sentence is correct, i.e.closer to corrected language rather than to orig-inal language.
In Fig.
3, I show the histogram ofthe highest OC ratios of the corrected test sen-tences.
This histogram shows that most of theratios are close to 1, i.e.
the probabilities of thesentence are almost equal using both languagemodels.
The histogram does not depend on thetype of word encoding.
In Table 2, I show ex-amples of corrections and the OC ratios for eachset of corrections.
The error correction systemtakes corrections which are applied for the sen-tence with the lowest OC ratio (see Table 2).7 The results and conclusionsThe results for different runs of the error correc-tion system are shown in the Table 3.
The bestdeterminer and preposition correction F-scoreresults are achieved with Run 5, which is using1end + pron encoding: all words except reservedcorrection and pronoun words were encoded asword length + the last character.
This result wasranked for ninth position out of 14 teams.Nevertheless, the results for different types ofcorrections are quite different.
The error cor-rection system was capable of performing UT,MT and MD type error corrections but hopelessfor UD, RD and RT type error corrections.
Thebest results are for:MT ?
missing preposition error correction, noencoding is used;MD - missing determiner error correction, 2endencoding is used;UT - unwanted preposition error correction,any type of encoding except no encoding.Surprisingly, we had to use whole words formissing preposition error correction, but neverfor unwanted preposition error correction.
Oursystem was ranked at the seventh position forUT error correction using F-score.The result for MT error correction shows thatsmoothed 5-gram rule generation was useful andthe whole word should be used.
But encodingwith word length should never be used.
Oursystem is ranked at the sixth position for MTerror correction.The result for MD error correction shows thatthe system degrades when encodings with fewercharacters are used.230Run All MT MDP R F P R F P R F0 8.15 4.19 5.54 4.65 3.50 4.00 7.84 9.60 8.631 24.5 2.87 5.13 12.5 3.51 5.48 34.8 6.40 10.82 35.5 2.43 4.54 25.0 3.51 6.15 46.7 5.60 10.03 8.41 3.75 5.19 5.56 3.51 4.30 8.27 8.80 8.534 25.0 2.87 5.15 13.3 3.51 5.56 34.8 6.40 10.85 8.76 4.19 5.67 5.00 3.51 4.12 8.57 9.60 9.066 24.5 2.87 5.14 12.5 3.51 5.48 34.8 6.40 10.87 9.04 3.75 5.30 5.71 3.51 4.35 9.17 8.80 8.988 25.0 2.87 5.15 13.3 3.51 5.56 34.8 6.40 10.8Run UT UD RT RDP R F P R F P R F P R F0 100 4.65 8.89 4.76 1.89 2.70 1.67 1.47 2.70 0 0 01 100 4.65 8.89 0 0 0 16.7 0.74 1.41 0 0 02 100 2.33 4.55 0 0 0 20.0 0.74 1.42 0 0 03 100 4.65 8.89 0 0 0 16.7 1.47 2.70 0 0 04 100 4.65 8.89 0 0 0 16.7 0.74 1.41 0 0 05 100 4.65 8.89 4.76 1.89 2.70 16.7 1.47 2.70 0 0 06 100 4.65 8.89 0 0 0 16.7 0.74 1.41 0 0 07 100 4.65 8.89 0 0 0 16.7 1.47 2.70 0 0 08 100 4.65 8.89 0 0 0 16.7 0.74 1.41 0 0 0Table 3: Scores for correction of different runs.The main conclusion is that there are no com-mon features for all error corrections and thedifferent systems for different error types shouldbe implemented.ReferencesRobert Dale, Ilya Anisimoff, and George Narroway.2012.
Hoo 2012: A report on the preposition anddeterminer error correction shared task.
In Pro-ceedings of the Seventh Workshop on InnovativeUse of NLP for Building Educational Applications,Montreal, Canada, June.Reinhard Kneser and Hermann Ney.
1995.
Im-proved backing-off for m-gram language modeling.In Proceedings of the IEEE International Confer-ence on Acoustics, Speech and Signal Processing,volume I, pages 181?184, Detroit, Michigan, May.Claudia Leacock, Martin Chodorow, Michael Ga-mon, and Joel Tetreault.
2010.
Automated Gram-matical Error Detection for Language Learners.Morgan and Claypool Publishers.231MDTeam Run Precision Recall F-ScoreCU 0 83.33 8.0 14.6KU 2 1.98 20.0 3.6LE 0 54.43 34.4 42.16NA 1 29.09 38.4 33.1NU 0 51.02 40.0 44.84TC 3 6.21 7.2 6.67TH 3 9.54 26.4 14.01UI 0 51.92 43.2 47.16UT 6 36.7 32.0 34.19VA 0 6.4 6.4 6.4VT 1 34.78 6.4 10.81MTTeam Run Precision Recall F-ScoreCU 1 5.68 8.77 6.9KU 1 0.51 19.3 1.0LE 0 50.0 5.26 9.52NA 3 11.43 7.02 8.7NU 0 38.46 17.54 24.1TC 3 4.65 3.51 4.0UI 5 42.86 15.79 23.08VA 1 1.71 7.02 2.75VT 2 25.0 3.51 6.15UTTeam Run Precision Recall F-ScoreCU 1 4.83 39.53 8.61JU 1 2.91 6.98 4.11KU 5 60.0 13.95 22.64LE 1 32.14 20.93 25.35NA 3 40.91 20.93 27.69NU 0 40.0 13.95 20.69TC 9 4.69 30.23 8.13TH 1 10.32 30.23 15.38VA 0 12.9 18.6 15.24VT 0 100.0 4.65 8.89UDTeam Run Precision Recall F-ScoreCU 3 17.86 18.87 18.35JU 1 4.84 5.66 5.22KU 8 26.92 13.21 17.72LE 0 22.67 32.08 26.56NA 5 40.0 11.32 17.65NU 0 33.33 9.43 14.71TC 9 5.11 16.98 7.86TH 1 38.89 13.21 19.72UI 2 23.38 33.96 27.69VA 0 7.06 11.32 8.7VT 0 4.76 1.89 2.7Table 4: Scores for correction.232
