Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 7?12,Prague, June 2007. c?2007 Association for Computational LinguisticsSemeval-2007 Task 02:Evaluating Word Sense Induction and Discrimination SystemsEneko AgirreIXA NLP GroupUniv.
of the Basque CountryDonostia, Basque Countrye.agirre@ehu.esAitor SoroaIXA NLP GroupUniv.
of the Basque CountryDonostia, Basque Countrya.soroa@ehu.esAbstractThe goal of this task is to allow for com-parison across sense-induction and discrim-ination systems, and also to compare thesesystems to other supervised and knowledge-based systems.
In total there were 6 partic-ipating systems.
We reused the SemEval-2007 English lexical sample subtask of task17, and set up both clustering-style unsuper-vised evaluation (using OntoNotes senses asgold-standard) and a supervised evaluation(using the part of the dataset for mapping).We provide a comparison to the results ofthe systems participating in the lexical sam-ple subtask of task 17.1 IntroductionWord Sense Disambiguation (WSD) is a keyenabling-technology.
Supervised WSD techniquesare the best performing in public evaluations, butneed large amounts of hand-tagging data.
Exist-ing hand-annotated corpora like SemCor (Milleret al, 1993), which is annotated with WordNetsenses (Fellbaum, 1998) allow for a small improve-ment over the simple most frequent sense heuristic,as attested in the all-words track of the last Sense-val competition (Snyder and Palmer, 2004).
In the-ory, larger amounts of training data (SemCor hasapprox.
500M words) would improve the perfor-mance of supervised WSD, but no current projectexists to provide such an expensive resource.
An-other problem of the supervised approach is that theinventory and distribution of senses changes dra-matically from one domain to the other, requiringadditional hand-tagging of corpora (Mart?
?nez andAgirre, 2000; Koeling et al, 2005).Supervised WSD is based on the ?fixed-list ofsenses?
paradigm, where the senses for a target wordare a closed list coming from a dictionary or lex-icon.
Lexicographers and semanticists have longwarned about the problems of such an approach,where senses are listed separately as discrete enti-ties, and have argued in favor of more complex rep-resentations, where, for instance, senses are denseregions in a continuum (Cruse, 2000).Unsupervised Word Sense Induction and Dis-crimination (WSID, also known as corpus-based un-supervised systems) has followed this line of think-ing, and tries to induce word senses directly fromthe corpus.
Typical WSID systems involve cluster-ing techniques, which group together similar exam-ples.
Given a set of induced clusters (which repre-sent word uses or senses1), each new occurrence ofthe target word will be compared to the clusters andthe most similar cluster will be selected as its sense.One of the problems of unsupervised systems isthat of managing to do a fair evaluation.
Most of cur-rent unsupervised systems are evaluated in-house,with a brief comparison to a re-implementation of aformer system, leading to a proliferation of unsuper-vised systems with little ground to compare amongthem.
The goal of this task is to allow for compar-ison across sense-induction and discrimination sys-tems, and also to compare these systems to other su-pervised and knowledge-based systems.The paper is organized as follows.
Section 2presents the evaluation framework used in this task.Section 3 presents the systems that participated in1WSID approaches prefer the term ?word uses?
to ?wordsenses?.
In this paper we use them interchangeably to refer toboth the induced clusters, and to the word senses from somereference lexicon.7the task, and the official results.
Finally, Section 5draws the conclusions.2 Evaluating WSID systemsAll WSID algorithms need some addition in orderto be evaluated.
One alternative is to manually de-cide the correctness of the clusters assigned to eachoccurrence of the words.
This approach has twomain disadvantages.
First, it is expensive to man-ually verify each occurrence of the word, and dif-ferent runs of the algorithm need to be evaluatedin turn.
Second, it is not an easy task to manu-ally decide if an occurrence of a word effectivelycorresponds with the use of the word the assignedcluster refers to, especially considering that the per-son is given a short list of words linked to the clus-ter.
We also think that instead of judging whetherthe cluster returned by the algorithm is correct, theperson should have independently tagged the occur-rence with his own senses, which should have beenthen compared to the cluster returned by the system.This is paramount to compare a corpus which hasbeen hand-tagged with some reference senses (alsoknown as the gold-standard) with the clustering re-sult.
The gold standard tags are taken to be the def-inition of the classes, and standard measures fromthe clustering literature can be used to evaluate theclusters against the classes.A second alternative would be to devise a methodto map the clusters returned by the systems to thesenses in a lexicon.
Pantel and Lin (2002) automat-ically map the senses to WordNet, and then mea-sure the quality of the mapping.
More recently, themapping has been used to test the system on pub-licly available benchmarks (Purandare and Peder-sen, 2004; Niu et al, 2005).A third alternative is to evaluate the systems ac-cording to some performance in an application, e.g.information retrieval (Schu?tze, 1998).
This is a veryattractive idea, but requires expensive system devel-opment and it is sometimes difficult to separate thereasons for the good (or bad) performance.In this task we decided to adopt the first two alter-natives, since they allow for comparison over pub-licly available systems of any kind.
With this goal onmind we gave all the participants an unlabeled cor-pus, and asked them to induce the senses and createa clustering solution on it.
We evaluate the resultsaccording to the following types of evaluation:1.
Evaluate the induced senses as clusters of ex-amples.
The induced clusters are compared tothe sets of examples tagged with the given goldstandard word senses (classes), and evaluatedusing the FScore measure for clusters.
We willcall this evaluation unsupervised.2.
Map the induced senses to gold standardsenses, and use the mapping to tag the test cor-pus with gold standard tags.
The mapping isautomatically produced by the organizers, andthe resulting results evaluated according to theusual precision and recall measures for super-vised word sense disambiguation systems.
Wecall this evaluation supervised.We will see each of them in turn.2.1 Unsupervised evaluationIn this setting the results of the systems are treatedas clusters of examples and gold standard senses areclasses.
In order to compare the clusters with theclasses, hand annotated corpora is needed.
The testset is first tagged with the induced senses.
A per-fect clustering solution will be the one where eachcluster has exactly the same examples as one of theclasses, and vice versa.Following standard cluster evaluation prac-tice (Zhao and Karypis, 2005), we consider the FS-core measure for measuring the performance of thesystems.
The FScore is used in a similar fashionto Information Retrieval exercises, with precisionand recall defined as the percentage of correctly ?re-trieved?
examples for a cluster (divided by total clus-ter size), and recall as the percentage of correctly?retrieved?
examples for a cluster (divided by totalclass size).Given a particular class sr of size nr and a clusterhi of size ni, suppose nir examples in the class srbelong to hi.
The F value of this class and cluster isdefined to be:f(sr, hi) =2P (sr, hi)R(sr, hi)P (sr, hi) + R(sr, hi)where P (sr, hi) =nirnris the precision value andR(sr, hi) =nirniis the recall value defined for classsr and cluster hi.
The FScore of class sr is the max-imum F value attained at any cluster, that is,8F (sr) = maxhif(sr, hi)and the FScore of the entire clustering solution is:FScore =c?r=1nrnF (sr)where q is the number of classes and n is the sizeof the clustering solution.
If the clustering is theidentical to the original classes in the datasets, FS-core will be equal to one which means that the higherthe FScore, the better the clustering is.For the sake of completeness we also include thestandard entropy and purity measures in the unsu-pervised evaluation.
The entropy measure consid-ers how the various classes of objects are distributedwithin each cluster.
In general, the smaller the en-tropy value, the better the clustering algorithm per-forms.
The purity measure considers the extent towhich each cluster contained objects from primarilyone class.
The larger the values of purity, the bet-ter the clustering algorithm performs.
For a formaldefinition refer to (Zhao and Karypis, 2005).2.2 Supervised evaluationWe have followed the supervised evaluation frame-work for evaluating WSID systems as described in(Agirre et al, 2006).
First, we split the corpus intoa train/test part.
Using the hand-annotated sense in-formation in the train part, we compute a mappingmatrix M that relates clusters and senses in the fol-lowing way.
Suppose there are m clusters and nsenses for the target word.
Then, M = {mij} 1 ?i ?
m, 1 ?
j ?
n, and each mij = P (sj |hi), thatis, mij is the probability of a word having sense jgiven that it has been assigned cluster i.
This proba-bility can be computed counting the times an occur-rence with sense sj has been assigned cluster hi inthe train corpus.The mapping matrix is used to transform anycluster score vector h?
= (h1, .
.
.
, hm) returned bythe WSID algorithm into a sense score vector s?
=(s1, .
.
.
, sn).
It suffices to multiply the score vectorby M , i.e., s?
= h?M .We use the M mapping matrix in order to convertthe cluster score vector of each test corpus instanceinto a sense score vector, and assign the sense withAll Nouns Verbstrain 22281 14746 9773test 4851 2903 2427all 27132 17649 12200Table 1: Number of occurrences for the 100 target words inthe corpus following the train/test split.maximum score to that instance.
Finally, the result-ing test corpus is evaluated according to the usualprecision and recall measures for supervised wordsense disambiguation systems.3 ResultsIn this section we will introduce the gold standardand corpus used, the description of the systems andthe results obtained.
Finally we provide some mate-rial for discussion.Gold StandardThe data used for the actual evaluation was bor-rowed from the SemEval-2007 ?English lexicalsample subtask?
of task 17.
The texts come from theWall Street Journal corpus, and were hand-annotatedwith OntoNotes senses (Hovy et al, 2006).
Notethat OntoNotes senses are coarser than WordNetsenses, and thus the number of senses to be inducedis smaller in this case.Participants were provided with informationabout 100 target words (65 verbs and 35 nouns),each target word having a set of contexts where theword appears.
After removing the sense tags fromthe train corpus, the train and test parts were joinedinto the official corpus and given to the participants.Participants had to tag with the induced senses allthe examples in this corpus.
Table 1 summarizes thesize of the corpus.Participant systemsIn total there were 6 participant systems.
One ofthem (UoFL) was not a sense induction system, butrather a knowledge-based WSD system.
We includetheir data in the results section below for coherencewith the official results submitted to participants, butwe will not mention it here.I2R: This team used a cluster validation methodto estimate the number of senses of a target word inuntagged data, and then grouped the instances of thistarget word into the estimated number of clusters us-ing the sequential Information Bottleneck algorithm.9UBC-AS: A two stage graph-based clusteringwhere a co-occurrence graph is used to computesimilarities against contexts.
The context similaritymatrix is pruned and the resulting associated graphis clustered by means of a random-walk type al-gorithm.
The parameters of the system are tunedagainst the Senseval-3 lexical sample dataset, andsome manual tuning is performed in order to reducethe overall number of induced senses.
Note that thissystem was submitted by the organizers.
The orga-nizers took great care in order to participate underthe same conditions as the rest of participants.UMND2: A system which clusters the second or-der co-occurrence vectors associated with each wordin a context.
Clustering is done using k-means andthe number of clusters was automatically discoveredusing the Adapted Gap Statistic.
No parameter tun-ing is performed.upv si: A self-term expansion method based onco-ocurrence, where the terms of the corpus are ex-panded by its best co-ocurrence terms in the samecorpus.
The clustering is done using one implemen-tation of the KStar method where the stop criterionhas been modified.
The trial data was used for de-termining the corpus structure.
No further tuning isperformed.UOY: A graph based system which creates a co-occurrence hypergraph model.
The hypergraph isfiltered and weighted according to some associa-tion rules.
The clustering is performed by selectingthe nodes of higher degree until a stop criterion isreached.
WSD is performed by assigning to each in-duced cluster a score equal to the sum of weights ofhyperedges found in the local context of the targetword.
The system was tested and tuned on 10 nounsof Senseval-3 lexical-sample.Official ResultsParticipants were required to induce the senses ofthe target words and cluster all target word contextsaccordingly2.
Table 2 summarizes the average num-ber of induced senses as well as the real senses inthe gold standard.2They were allowed to label each context with a weightedscore vector, assigning a weight to each induced sense.
In theunsupervised evaluation only the sense with maximum weightwas considered, but for the supervised one the whole score vec-tor was used.
However, none of the participating systems la-beled any instance with more than one sense.system All nouns verbsI2R 3.08 3.11 3.06UBC-AS?
1.32 1.63 1.15UMND2 1.36 1.71 1.17upv si 5.57 7.2 4.69UOY 9.28 11.28 8.2Gold standardtest 2.87 2.86 2.86train 3.6 3.91 3.43all 3.68 3.94 3.54Table 2: Average number of clusters as returned by the par-ticipants, and number of classes in the gold standard.
Note thatUBC-AS?
is the system submitted by the organizers of the task.System R. All Nouns VerbsFSc.
Pur.
Entr.
FSc.
FSc.1c1word 1 78.9 79.8 45.4 80.7 76.8UBC-AS?
2 78.7 80.5 43.8 80.8 76.3upv si 3 66.3 83.8 33.2 69.9 62.2UMND2 4 66.1 81.7 40.5 67.1 65.0I2R 5 63.9 84.0 32.8 68.0 59.3UofL??
6 61.5 82.2 37.8 62.3 60.5UOY 7 56.1 86.1 27.1 65.8 45.1Random 8 37.9 86.1 27.7 38.1 37.71c1inst 9 9.5 100 0 6.6 12.7Table 3: Unsupervised evaluation on the test corpus (FScore),including 3 baselines.
Purity and entropy are also provided.UBC-AS?
was submitted by the organizers.
UofL??
is not asense induction system.System Rank Supervised evaluationAll Nouns VerbsI2R 1 81.6 86.8 75.7UMND2 2 80.6 84.5 76.2upv si 3 79.1 82.5 75.3MFS 4 78.7 80.9 76.2UBC-AS?
5 78.5 80.7 76.0UOY 6 77.7 81.6 73.3UofL??
7 77.1 80.5 73.3Table 4: Supervised evaluation as recall.
UBC-AS?
was sub-mitted by the organizers.
UofL??
is not a sense induction sys-tem.Table 3 shows the unsupervised evaluation ofthe systems on the test corpus.
We also includethree baselines: the ?one cluster per word?
baseline(1c1word), which groups all instances of a word intoa single cluster, the ?one cluster per instance?
base-line (1c1inst), where each instance is a distinct clus-ter, and a random baseline, where the induced wordsenses and their associated weights have been ran-domly produced.
The random baseline figures in thispaper are averages over 10 runs.As shown in Table 3, no system outperforms the1c1word baseline, which indicates that this baseline10is quite strong, perhaps due the relatively small num-ber of classes in the gold standard.
However, allsystems outperform by far the random and 1c1instbaselines, meaning that the systems are able to in-duce correct senses.
Note that the purity and entropymeasures are not very indicative in this setting.
Forcompleteness, we also computed the FScore usingthe complete corpus (both train and test).
The re-sults are similar and the ranking is the same.
Weomit them for brevity.The results of the supervised evaluation can beseen in Table 4.
The evaluation is also performedover the test corpus.
Apart from participants, wealso show the most frequent sense (MFS), whichtags every test instance with the sense that occurredmost often in the training part.
Note that the su-pervised evaluation combines the information in theclustering solution implicitly with the MFS infor-mation via the mapping in the training part.
Pre-vious Senseval evaluation exercises have shown thatthe MFS baseline is very hard to beat by unsuper-vised systems.
In fact, only three of the participantsystems are above the MFS baseline, which showsthat the clustering information carries over the map-ping successfully for these systems.
Note that the1c1word baseline is equivalent to MFS in this set-ting.
We will review the random baseline in the dis-cussion section below.Further ResultsTable 5 shows the results of the best systems fromthe lexical sample subtask of task 17.
The best senseinduction system is only 6.9 percentage points belowthe best supervised, and 3.5 percentage points be-low the best (and only) semi-supervised system.
Ifthe sense induction system had participated, it wouldbe deemed as semi-supervised, as it uses, albeit in ashallow way, the training data for mapping the clus-ters into senses.
In this sense, our supervised evalu-ation does not seek to optimize the available trainingdata.After the official evaluation, we realized that con-trary to previous lexical sample evaluation exercisestask 17 organizers did not follow a random train/testsplit.
We decided to produce a random train/testsplit following the same 82/18 proportion as the of-ficial split, and re-evaluated the systems.
The resultsare presented in Table 6, where we can see that allSystem Supervised evaluationbest supervised 88.7best semi-supervised 85.1best induction (semi-sup.)
81.6MFS 78.7best unsupervised 53.8Table 5: Comparing the best induction system in this task withthose of task 17.System Supervised evaluationI2R 82.2UOY 81.3UMND2 80.1upv si 79.9UBC-AS 79.0MFS 78.4Table 6: Supervised evaluation as recall using a randomtrain/test split.participants are above the MFS baseline, showingthat all of them learned useful clustering informa-tion.
Note that UOY was specially affected by theoriginal split.
The distribution of senses in this splitdid not vary (cf.
Table 2).Finally, we also studied the supervised evalua-tion of several random clustering algorithms, whichcan attain performances close to MFS, thanks to themapping information.
This is due to the fact that therandom clusters would be mapped to the most fre-quent senses.
Table 7 shows the results of randomsolutions using varying numbers of clusters (e.g.random2 is a random choice between two clusters).Random2 is only 0.1 below MFS, but as the numberof clusters increases some clusters don?t get mapped,and the recall of the random baselines decrease.4 DiscussionThe evaluation of clustering solutions is not straight-forward.
All measures have some bias towards cer-tain clustering strategy, and this is one of the reasonsof adding the supervised evaluation as a complemen-tary information to the more standard unsupervisedevaluation.In our case, we noticed that the FScore penal-ized the systems with a high number of clusters,and favored those that induce less senses.
Giventhe fact that FScore tries to balance precision (higherfor large numbers of clusters) and recall (higher forsmall numbers of clusters), this was not expected.We were also surprised to see that no system could11System Supervised evaluationrandom2 78.6random10 77.6ramdom100 64.2random1000 31.8Table 7: Supervised evaluation of several random baselines.beat the ?one cluster one word?
baseline.
An expla-nation might lay in that the gold-standard was basedon the coarse-grained OntoNotes senses.
We alsonoticed that some words had hundreds of instancesand only a single sense.
We suspect that the partic-ipating systems would have beaten all baselines if afine-grained sense inventory like WordNet had beenused, as was customary in previous WSD evaluationexercises.Supervised evaluation seems to be more neutralregarding the number of clusters, as the ranking ofsystems according to this measure include diversecluster averages.
Each of the induced clusters ismapped into a weighted vector of senses, and thusinducing a number of clusters similar to the numberof senses is not a requirement for good results.
Withthis measure some of the systems3 are able to beatall baselines.5 ConclusionsWe have presented the design and results of theSemEval-2007 task 02 on evaluating word sense in-duction and discrimination systems.
6 systems par-ticipated, but one of them was not a sense induc-tion system.
We reused the data from the SemEval-2007 English lexical sample subtask of task 17, andset up both clustering-style unsupervised evaluation(using OntoNotes senses as gold-standard) and a su-pervised evaluation (using the training part of thedataset for mapping).
We also provide a compari-son to the results of the systems participating in thelexical sample subtask of task 17.Evaluating clustering solutions is not straightfor-ward.
The unsupervised evaluation seems to besensitive to the number of senses in the gold stan-dard, and the coarse grained sense inventory usedin the gold standard had a great impact in the re-sults.
The supervised evaluation introduces a map-ping step which interacts with the clustering solu-tion.
In fact, the ranking of the participating systems3All systems in the case of a random train/test splitvaries according to the evaluation method used.
Wethink the two evaluation results should be taken to becomplementary regarding the information learnedby the clustering systems, and that the evaluationof word sense induction and discrimination systemsneeds further developments, perhaps linked to a cer-tain application or purpose.AcknowledgmentsWe want too thank the organizers of SemEval-2007 task 17 forkindly letting us use their corpus.
We are also grateful to TedPedersen for his comments on the evaluation results.
This workhas been partially funded by the Spanish education ministry(project KNOW) and by the regional government of Gipuzkoa(project DAHAD).ReferencesE.
Agirre, D.
Mart?
?nez, O.
Lo?pez de Lacalle, andA.
Soroa.
2006.
Evaluating and optimizing the param-eters of an unsupervised graph-based wsd algorithm.In Proceedings of the NAACL TextGraphs workshop,pages 89?96, New York City, June.D.
A. Cruse, 2000.
Polysemy: Theoretical and Com-putational Approaches, chapter Aspects of the Micro-structure of Word Meanings, pages 31?51.
OUP.C.
Fellbaum.
1998.
WordNet: An Electronic LexicalDatabase.
MIT Press.E.
Hovy, M. Marcus, M. Palmer, L. Ramshaw, andR.
Weischedel.
2006.
Ontonotes: The 90% solution.In Proceedings of HLT/NAACL.R.
Koeling, D. McCarthy, and J.D.
Carroll.
2005.Domain-specific sense distributions and predominantsense acquisition.D.
Mart?
?nez and E. Agirre.
2000.
One sense per colloca-tion and genre/topic variations.G.A.
Miller, C. Leacock, R. Tengi, and R.Bunker.
1993.A semantic concordance.
In Proc.
of the ARPA HLTworkshop.C.
Niu, W. Li, R. K. Srihari, and H. Li.
2005.
Wordindependent context pair classification model for wordsense disambiguation.
In Proc.
of CoNLL-2005.P.
Pantel and D. Lin.
2002.
Discovering word sensesfrom text.
In Proc.
of KDD02.A.
Purandare and T. Pedersen.
2004.
Word sense dis-crimination by clustering contexts in vector and simi-larity spaces.
In Proc.
of CoNLL-2004, pages 41?48.H.
Schu?tze.
1998.
Automatic word sense discrimination.Computational Linguistics, 24(1):97?123.B.
Snyder and M. Palmer.
2004.
The english all-wordstask.
In Proc.
of SENSEVAL.Y Zhao and G Karypis.
2005.
Hierarchical clusteringalgorithms for document datasets.
Data Mining andKnowledge Discovery, 10(2):141?168.12
