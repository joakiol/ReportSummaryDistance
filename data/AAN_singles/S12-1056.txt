First Joint Conference on Lexical and Computational Semantics (*SEM), pages 419?424,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsUTD-SpRL: A Joint Approach to Spatial Role LabelingKirk Roberts and Sanda M. HarabagiuHuman Language Technology Research InstituteUniversity of Texas at DallasRichardson, TX 75083, USA{kirk, sanda}@hlt.utdallas.eduAbstractWe present a joint approach for recognizingspatial roles in SemEval-2012 Task 3.
Can-didate spatial relations, in the form of triples,are heuristically extracted from sentences withhigh recall.
The joint classification of spatialroles is then cast as a binary classification overthe candidates.
This joint approach allows fora rich feature set based on the complete rela-tion instead of individual relation arguments.Our best official submission achieves an F1-measure of 0.573 on relation recognition, bestin the task and outperforming the previousbest result on the same data set (0.500).1 IntroductionA significant amount of spatial information in natu-ral language is encoded in spatial relationships be-tween objects.
In this paper, we present our ap-proach for detecting the special case of spatial re-lations evaluated in SemEval-2012 Task 3, SpatialRole Labeling (SpRL) (Kordjamshidi et al, 2012).This task considers the most common type of spa-tial relationships between objects, namely those de-scribed with a spatial preposition (e.g., in, on, over)or a spatial phrase (e.g., in front of, on the left), re-ferred to as the spatial INDICATOR.
A spatial INDI-CATOR connects an object of interest (the TRAJEC-TOR) with a grounding location (the LANDMARK).Examples of this type of spatial relationship include:(1) [cars]T parked [in front of]I the [house]L .
(2) [bushes]T1 and small [trees]T2 [on]I the [hill]L .
(3) a huge [column]L with a [football]T [on top]I .
(4) [trees]T [on the right]I .
[?
]LSpRL is a type of semantic role labeling (SRL)(Palmer et al, 2010), where the spatial INDICA-TOR is the predicate (or trigger) and the TRAJEC-TOR and LANDMARK are its two arguments.
Previ-ous approaches to SpRL (Kordjamshidi et al, 2011)have largely followed the commonly employed SRLpipeline: (1) find predicates (i.e., the INDICATOR),(2) recognize the predicate?s syntactic constituents,and (3) classify the constituent?s role (i.e., TRA-JECTOR, LANDMARK, or neither).
The problemwith this approach is that arguments are consideredlargely in isolation.
Consider the following:(5) there is a picture on the wall above the bed.This sentence contains three objects (picture, wall,and bed) and two INDICATORs (on and above).Since the most common spatial relation pattern issimply trajector-indicator-landmark (as in Examples(1) and (2)), the triple wall-above-bed is a likely can-didate relation.
However, the semantics of these ob-jects invalidates the relation (i.e., walls are besidebeds, ceilings are above them).
Instead the correctrelation is picture-above-bed because the preposi-tion above syntactically attaches to picture insteadof wall.
Prepositional attachment, however, is a dif-ficult syntactic problem solved largely through theuse of semantics, so an understanding of the con-sistency of spatial relationships plays an importantrole in their recognition.
Consistency checking isnot possible under a pipeline approach that classifieswhether wall as the TRAJECTOR without any knowl-edge of its LANDMARK.We therefore propose an alternative to thispipeline approach that jointly decides whether a419given TRAJECTOR-INDICATOR-LANDMARK tripleexpresses a spatial relation.
We utilize a high re-call heuristic for recognizing objects capable of par-ticipating in a spatial relation as well as a lexiconof INDICATORs.
All possible combinations of thesearguments (including undefined LANDMARKs) areconsidered by a binary classifier in order to make ajoint decision.
This allows us to incorporate featuresbased on all three relation elements such as the rela-tion?s semantic consistency.2 Joint Classification2.1 Relation Candidate SelectionPrevious joint approaches to SpRL have performedpoorly relative to the pipeline approach (Kord-jamshidi et al, 2011).
However, these approacheshave issues with data imbalance: if every tokencould be a TRAJECTOR, LANDMARK, or INDICA-TOR, then even short sentences may contain thou-sands of negative relation candidates.
Such unbal-anced data sets are difficult for classifiers to reasonover (Japkowicz and Stephen, 2002).
To reduce thisimbalance, we propose high recall heuristics to rec-ognize candidate elements (INDICATORs, TRAJEC-TORs, and LANDMARKs).
Since INDICATORs aretaken from a closed set of prepositions and a smallset of spatial phrases, we simply use a lexicon con-structed from the indicators in the training data (e.g.,on, in front of).
Thus, our approach is not capable ofdetecting INDICATORs that were unseen in the train-ing data.
The effectiveness of this indicator lexiconis evaluated in Section 3.2.
For TRAJECTORs andLANDMARKs, we observe that both may be consid-ered spatial objects, which unlike INDICATORs arenot a closed class of words.
Instead, we considernoun phrase (NP) heads to be spatial objects.
Toovercome part-of-speech errors and increase recall,we incorporate three sources: (1) the NP heads froma syntactic parse tree (Klein and Manning, 2003),(2) the NP heads from a chunk parse1, and (3) wordsthat are marked as nouns in at least 66% of instancesin Treebank (Marcus et al, 1993).
This approachidentifies all nouns, not just spatial nouns.
But forthe SemEval-2012 Task 3 data, which is composedof image descriptions, most nouns are spatial ob-jects and no further refinements are necessary.
Fur-1http://www.surdeanu.name/mihai/bios/ther heuristics (such as using WordNet (Fellbaum,1998)) could be used to refine the set of spatial ob-jects if other domains (such as newswire) were tobe used.
Our main emphasis in this step, however,is recall: by utilizing these heuristics we greatly re-duce the number of negative instances while remov-ing very few positive spatial relations.
The effective-ness of our heuristics are evaluated in Section 3.2.Once all possible spatial INDICATORs and spa-tial objects are marked, all possible combinations ofthese are formed as candidate relations.
Addition-ally, for each spatial object and spatial INDICATORpair, an additional candidate relation is formed withan undefined LANDMARK (such as in Example (4)).2.2 Classification FrameworkGiven candidate spatial relations, we utilize a binarysupport vector machine (SVM) classifier to indicatewhich relation candidates are spatial relations.
Weuse the LibLINEAR (Fan et al, 2008) SVM imple-mentation, adjusting the negative outcome weightfrom 1.0 to 0.8 (tuned via cross-validation on thetraining data).
This adjustment sacrifices preci-sion for recall, but raises the overall F1 score.
Fortype classification (REGION, DIRECTION, and DIS-TANCE), we use LibLINEAR as a multi-class SVMwith no weight adjustment in order to maximize ac-curacy.
The features used in both classifiers are dis-cussed in Sections 2.3 and 2.4.2.3 Relation Detection FeaturesThe difference between our two official submissions(supervised1 and supervised2) is that different setsof features were used to detect spatial relations.
Thefeatures for general type classification, discussed inSection 2.4, were consistent across both submis-sions.
Based on previous approaches to spatial rolelabeling, our own initial intuitions, and error analy-sis, we created over 100 different features, choosingthe best feature set with a greedy forward/backwardautomated feature selection technique (Pudil et al,1994).
This greedy method iteratively chooses thebest un-used feature to add to the feature set.
At theend of each iteration, there is a pruning step to re-move any features made redundant by the additionof the latest feature.Before describing the individual features used inour submission, we first enumerate some basic fea-420tures that form the building blocks of many of thefeatures in our submissions (with sample feature val-ues from Example (1)):(BF.1) The TRAJECTOR?s raw string (e.g., cars).
(BF.2) The LANDMARK?s raw string (house).
(BF.3) The INDICATOR?s raw string (in front of).
(BF.4) The TRAJECTOR?s lemma (car).
(BF.5) The LANDMARK?s lemma (house).
(BF.6) The dependency path from the TRAJECTOR to theINDICATOR (?NSUBJ?PREP).
Uses the StanfordDependency Parser (de Marneffe et al, 2006).
(BF.7) The dependency path from the INDICATOR to theLANDMARK (?POBJ).For BF.2, BF.5, and BF.7, if the relation?sLANDMARK is undefined, the feature value is sim-ply undefined.
The features for our first submission(supervised1), in the order they were chosen by thefeature selector, are as follows:(JF1.1) The concatenation of BF.6, BF.3, and BF.7 (i.e.,the dependency path from the TRAJECTOR to theLANDMARK including the INDICATOR?s raw string),for all spatial objects related to the TRAJECTOR underconsideration via a conjunction dependency relation(including the TRAJECTOR itself).
For instance,TRAJECTOR1 in Example (2) would have two featurevalues: ?CONJ?PREP?POBJ and ?PREP?POBJ.Since objects connected via a conjunction shouldparticipate in the same relation, this allows theclassifier to overcome the sparsity related to the lownumber of training instances containing a conjunction.
(JF1.2) The concatenation of BF.1, BF.3, and BF.2(cars::in front of::house).
(JF1.3) Whether or not the LANDMARK is part of a termfrom the INDICATOR lexicon.
Words like front andside are common LANDMARKs but may also be partof an INDICATOR as well.
(JF1.4) All the words between the left-most argument inthe relation and the right-most argument (parked, the).Does not include any word in the arguments.
(JF1.5) The value of BF.7.
(JF1.6) The first word in the INDICATOR.
(JF1.7) The LANDMARK?s WordNet hypernyms.
(JF1.8) The TRAJECTOR?s WordNet hypernyms.
(JF1.9) Whether or not the relative order of the relationarguments in the text is INDICATOR, LANDMARK,TRAJECTOR.
This order is rare and thus this featureacts as a negative indicator.
(JF1.10) Whether or not the TRAJECTOR is aprepositional object (POBJ from the dependency tree)of a preposition that is not the relation?s INDICATORbut is in the INDICATOR lexicon.
Again, this is anegative indicator.
(JF1.11) The concatenation of BF.4, BF.3, and BF.5(car::in front of::house).
(JF1.12) The dependency path from the TRAJECTORto the LANDMARK.
Differs from JF1.1 because itdoes not consider conjunctions or differentiatebetween INDICATORs.
(JF1.13) The concatenation of BF.3 and BF.7.
(JF1.14) Whether or not the relation under considerationhas an undefined LANDMARK and the sentencecontains no spatial objects other than the TRAJECTORunder consideration.
This helps to indicate relationswith undefined LANDMARKs in short sentences.The first feature selected by the automated featureselector (JF1.1) utilizes conjunctions (e.g., and, or,either).
However, conjunctions are difficult to detectwith high precision, so we decided to perform an-other round of feature selection without this particu-lar feature.
The chosen features were then submittedseparately (supervised2):(JF2.1) The same as JF1.2.
(JF2.2) The same as JF1.3.
(JF2.3) The same as JF1.4.
(JF2.4) The same as JF1.13.
(JF2.5) The value of BF.1.
(JF2.6) The same as JF1.5.
(JF2.7) Similar to JF1.1, but only using the concatenationof BF.6 and BF.3 (i.e., leaving out the dependencypath from the INDICATOR to the LANDMARK).
(JF2.8) The same as JF1.7.
(JF2.9) The same as JF1.8.
(JF2.10) The lexical pattern from the left-mostargument to the right-most argument(TRAJECTOR parked INDICATOR the LANDMARK).
(JF2.11) The raw string of the preposition in a PREPdependency relation with the TRAJECTOR if thatpreposition is not the relation?s INDICATOR.
(JF2.12) The PropBank role types for each argument inthe relation (TRAJECTOR=A1;INDICATOR=AM LOC;LANDMARK=AM LOC).
Uses SENNA(Collobert and Weston, 2009) for the PropBank parse.
(JF2.13) The same as JF1.14.
(JF2.14) The concatenation of BF.4, BF.3, and BF.5.
(JF2.15) The same as JF1.10, but with no requirement tobe in the INDICATOR lexicon.2.4 Type Classification FeaturesAfter joint detection of a relation?s arguments, aseparate classifier determines the relation?s generaltype.
The features used to classify a relation?s gen-eral type (REGION, DIRECTION, and DISTANCE)were also selected using an automated feature se-lector from the same set of features.
Both submis-sions (supervised1 and supervised2) utilized these421supervised1 supervised2Label Precision Recall F1 Precision Recall F1TRAJECTOR 0.731 0.621 0.672 0.782 0.646 0.707LANDMARK 0.871 0.645 0.741 0.894 0.680 0.772INDICATOR 0.928 0.712 0.806 0.940 0.732 0.823Relation 0.567 0.500 0.531 0.610 0.540 0.573Relation + Type 0.561 0.494 0.526 0.603 0.534 0.566Table 1: Official results for submissions.features.
The following features were used for clas-sifying a spatial relation?s general type:(TF.1) The last word of the INDICATOR.
(TF.2) The value of BF.3.
(TF.3) The value of BF.5.
(TF.4) The same as JF1.3.
(TF.5) The same as JF2.10.3 Evaluation3.1 Official SubmissionThe official results for both of our submissions isshown in Table 1.
The argument-specific resultsfor TRAJECTORs, LANDMARKs, and INDICATORsare difficult to interpret in the joint approach.
In apipeline method, these usually indicate the perfor-mance of individual classifiers, but in our approachthese results are simply a derivative of our joint clas-sification output.
The first submission (supervised1)achieved a triple F1 of 0.531 for relation detectionand 0.526 when the general type is included.
Oursecond submission (supervised2) performed better,with an F1 of 0.573 for relation detection and 0.566when the general type is included.
This suggests thatthe feature JF1.1, even though it is the best individ-ual feature, introduces a significant amount of noise.The only result to compare our official submis-sions to is that of Kordjamshidi et al (2011), whoutilize a pipeline approach.
Their method has a rela-tion detection F1 of 0.500 (they do not report a scorewith general type).
We further compare our methodwith theirs in Section 4.3.2 Relation Candidate EvaluationThe heuristics described in Section 2.1 that enablejoint classification were tuned for the training data,but their recall on the test data places a strict upperbound on the recall to our overall approach.
It istherefore important to understand the performanceloss that occurs at this step.Table 2 shows the performance of our heuristicson the training and test data.
The spatial INDICA-TOR lexicon has perfect recall on the training databecause it was built from this data set.
However, itperforms at only 0.951 recall on the test data, as al-most 5% of the INDICATORs in the test data were notseen in the training data.
Most of these are phrasalverbs (e.g., sailing over) or include the modifier very(e.g., to the very left).
Our spatial object recognizerperformed better, only dropping from 0.998 (2 er-rors) to 0.989 (16 errors).
Some of these errors re-sulted from mis-spellings (e.g., housed instead ofhouses), non-head spatial objects (mountain fromthe NP mountain landscape), NPs containing con-junctions (trees in two palm trees, lamps and flags,which gets marked as one simple NP), as well asparser errors.
The significant drop in precision forboth spatial indicators and objects is an additionalconcern.
This does not indicate the extracted itemswere not valid as potential indicators or objects, butrather that no gold relation contained them.
As ex-plained in Section 4, this is likely caused by the dis-parity in sentence length: longer sentences result inmore matches, but not necessarily more relations.As evidence of this, despite the training and test datacontaining almost the same number of sentences,there are 36% more spatial indicators and 20% morespatial objects in the test set.3.3 Further ExperimentsAfter the evaluation deadline, the task organizersprovided the gold test data, allowing us to performadditional experiments.
In this process we foundseveral annotation errors which we needed to fix inorder to process our gold results.
These errors werelargely annotations that were given an incorrect to-ken index, resulting in the annotation text not match-ing the referenced text.
These fixes increased ourperformance, shown on Table 3, improving relationdetection for the supervised2 feature set from 0.573422# Precision Recall F1Spatial Train 1,488 0.448 1.000 0.619Indicators Test 2,335 0.328 0.951 0.487Spatial Train 2,974 0.448 0.998 0.618Objects Test 3,704 0.387 0.989 0.556Table 2: Results of relation candidate selection heuristics.Data Precision Recall F1Train/Test 0.644 0.556 0.597Train/Test -NSI 0.644 0.582 0.611Train CV 0.824 0.743 0.781Test CV 0.745 0.639 0.688Train+Test CV 0.774 0.680 0.724Table 3: Additional experiments on corrected test datausing the supervised2 data set.
-NSI indicates that thegold spatial INDICATORs that are not in the lexicon areremoved.
CV indicates 10-fold cross validation.to 0.597.
We use this updated data set for the follow-ing experiments.
While the results aren?t compara-ble to other methods, the goal of these experiments isto analyze our system under various configurationsby their relative performance.Table 3 also shows a 10-fold cross validation per-formance on 3 data sets: (1) the training data, (2)the test data, and (3) both the training and test data.While our feature set is tuned to the training data,the test data is clearly more difficult.
Section 4 dis-cusses the differences between the training and testdata that may lead to such a performance reduction.Since our lexicon of spatial INDICATORs wasbuilt from the training data, our method will not rec-ognize any relations that use unseen INDICATORs.To differentiate between how our method performson the full test data and just those INDICATORs thatare in the lexicon, we removed the 39 gold relationswith unseen INDICATORs and re-tested the system.As can be seen in Table 3 (under -NSI), this im-proves recall by 2.6 points.3.4 Feature ExperimentsTo estimate the contribution of our features, we per-formed an additive experiment to see how each fea-ture contributes to the overall test score.
Table 4shows the feature contributions based on the orderthey were added by the feature selector.
For many ofthe features the score goes down when added.
How-ever, without these features, the final score woulddrop to 0.578, indicating they still provide valuableinformation in the context of the other features.
Ta-ble 5 shows performance on the updated test setFeature Precision Recall F1JF2.1 0.333 0.156 0.212+JF2.2 0.347 0.126 0.185+JF2.3 0.708 0.115 0.197+JF2.4 0.555 0.294 0.384+JF2.5 0.636 0.402 0.493+JF2.6 0.590 0.414 0.486+JF2.7 0.621 0.553 0.585+JF2.8 0.614 0.568 0.590+JF2.9 0.573 0.568 0.571+JF2.10 0.612 0.547 0.578+JF2.11 0.625 0.571 0.597+JF2.12 0.660 0.536 0.592+JF2.13 0.633 0.573 0.601+JF2.14 0.642 0.563 0.600+JF2.15 0.644 0.556 0.597Table 4: Additive feature experiment results using the su-pervised2 features.
Bold indicates increases in F1 overthe previous feature set.Feature Precision Recall F1?
0.644 0.556 0.597JF2.1 0.627 0.571 0.598JF2.2 0.629 0.542 0.582JF2.3 0.540 0.494 0.516JF2.4 0.591 0.412 0.485JF2.5 0.631 0.558 0.592JF2.6 0.657 0.515 0.577JF2.7 0.636 0.547 0.589JF2.8 0.641 0.562 0.599JF2.9 0.678 0.539 0.601JF2.10 0.607 0.569 0.587JF2.11 0.640 0.565 0.600JF2.12 0.646 0.566 0.603JF2.13 0.646 0.553 0.596JF2.14 0.618 0.572 0.594JF2.15 0.642 0.563 0.600Table 5: Results when individual features from the super-vised2 submission are removed.
Bold indicates improve-ment when the feature is removed.when individual features are removed.
Here, six fea-tures that were useful on the training data did notprove useful on the test data.4 DiscussionThe only available work against which our methodmay be compared is that of Kordjamshidi et al(2011).
They propose both a pipeline and joint ap-proach to SpRL.
In their case, their pipeline ap-proach performs better than their joint approach.Joint approaches increase data sparsity, so theirgreatest value is in the ability to use a richer set offeatures that describe the relationships between thearguments.
Kordjamshidi et al (2011) furthermore423did not employ heuristics to select relation candi-dates such as those in Section 2.1.
Given this dif-ference it is difficult to assert that a joint approachis better with complete certainty, but we believe theability to analyze the consistency of the entire rela-tion provides a significant advantage.
Many of ourfeatures (JF2.1, JF2.3, JF2.10, JF2.12, JF2.13, andJF2.14) were of this joint type.The drop in performance from the training data tothe test data is significant.
The possibility that this isentirely due to over-training is dispelled by the crossvalidation results in Table 3.
While different featuresmight work better on the test set, they are unlikelyto overcome the cross validation difference of 9.3points (0.781 vs. 0.688).
Much of this comes fromthe recall limit due to the use of the spatial indicatorlexicon.
The other significant cause of performancedegradation seems to be caused by sentence lengthand complexity.
The test sentences are longer (18 to-kens vs. 15 tokens in the training data), and have farmore conjunctions (389 and tokens vs. 256), indi-cating greater syntactic complexity.
But the largestdifference is the number of relation candidates gen-erated by the heuristics: 60,377 relation candidatesfrom the training data vs. 167,925 relation candi-dates from the test data (the data sets are roughly thesame size: 600 training and 613 test sentences).
Thedrop of precision in spatial objects in Table 2 reflectsthis as well.
Since the number of candidate relationsis quadratic in the number of spatial objects, it islikely that just a few, long sentences result in thisdramatic increase in the number of candidates.Since more general domains (such as newswire)are likely to have this problem as well, one importantarea of future work is the reduction of the number ofrelation candidates (increasing precision) while stillmaintaining near-perfect recall.5 ConclusionWe have presented a joint approach for recogniz-ing spatial roles in SemEval-2012 Task 3.
Our ap-proach improves over previous attempts at joint clas-sification by extracting a more precise (but still ex-tremely high recall) set of relation candidates, allow-ing binary classification on a more balanced data set.This joint approach allowed for a rich set of featuresbased on all the relation?s arguments.
Our best of-ficial submission achieved an F1-measure of 0.573on relation recognition, best in the task and outper-forming all previous work.AcknowledgmentsThe authors would like to thank the SemEval-2012Task 3 organizers for their work preparing the dataset and organizing the task.ReferencesRonan Collobert and Jason Weston.
2009.
Deep Learn-ing in Natural Language Processing.
Tutorial at NIPS.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating TypedDependency Parses from Phrase Structure Parses.
InProceedings of the Fifth International Conference onLanguage Resources and Evaluation.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-RuiWang, and Chih-Jen Lin.
2008.
LIBLINEAR: A Li-brary for Large Linear Classification.
Journal of Ma-chine Learning Research, 9:1871?1874.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press.Nathalie Japkowicz and Shaju Stephen.
2002.
The classimbalance problem: A systematic study.
IntelligentData Analysis, 6(5).Dan Klein and Christopher D. Manning.
2003.
Accu-rate Unlexicalized Parsing.
In Proceedings of the 41stAnnual Meeting of the Association for ComputationalLinguistics, pages 423?430.Parisa Kordjamshidi, Martijn Van Otterlo, and Marie-Francine Moens.
2011.
Spatial Role Labeling: To-wards Extraction of Spatial Relations from NaturalLanguage.
ACM Transactions on Speech and Lan-guage Processing, 8(3).Parisa Kordjamshidi, Steven Bethard, and Marie-Francine Moens.
2012.
SemEval-2012 Task 3: SpatialRole Labeling.
In Proceedings of the 6th InternationalWorkshop on Semantic Evaluation (SemEval).Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-rice Santorini.
1993.
Building a Large Annotated Cor-pus of English: The Penn Treebank.
ComputationalLinguistics, 19(2):313?330.Martha Palmer, Daniel Gildea, and Nianwen Xue.
2010.Semantic Role Labeling.
Morgan and Claypool.Pavel Pudil, Jana Novovic?ova?, and Josef Kittler.
1994.Floating search methods in feature selection.
PatternRecognition Letters, 15:1119?1125.424
