Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 134?137,Uppsala, Sweden, 15-16 July 2010.c?2010 Association for Computational LinguisticsUHD: Cross-Lingual Word Sense Disambiguation UsingMultilingual Co-occurrence GraphsCarina Silberer and Simone Paolo PonzettoDepartment of Computational LinguisticsHeidelberg University{silberer,ponzetto}@cl.uni-heidelberg.deAbstractWe describe the University of Heidelberg(UHD) system for the Cross-Lingual WordSense Disambiguation SemEval-2010 task(CL-WSD).
The system performs CL-WSD by applying graph algorithms pre-viously developed for monolingual WordSense Disambiguation to multilingual co-occurrence graphs.
UHD has participatedin the BEST and out-of-five (OOF) eval-uations and ranked among the most com-petitive systems for this task, thus indicat-ing that graph-based approaches representa powerful alternative for this task.1 IntroductionThis paper describes a graph-based system forCross-Lingual Word Sense Disambiguation, i.e.the task of disambiguating a word in context byproviding its most appropriate translations in dif-ferent languages (Lefever and Hoste, 2010, CL-WSD henceforth).
Our goal at SemEval-2010 wasto assess whether graph-based approaches, whichhave been successfully developed for monolingualWord Sense Disambiguation, represent a validframework for CL-WSD.
These typically trans-form a knowledge resource such as WordNet (Fell-baum, 1998) into a graph and apply graph algo-rithms to perform WSD.
In our work, we followthis line of research and apply graph-based meth-ods to multilingual co-occurrence graphs whichare automatically created from parallel corpora.2 Related WorkOur method is heavily inspired by previous pro-posals from V?eronis (2004, Hyperlex) and Agirreet al (2006).
Hyperlex performs graph-basedWSD based on co-occurrence graphs: given amonolingual corpus, for each target word a graphis built where nodes represent content words co-occurring with the target word in context, andedges connect the words which co-occur in thesecontexts.
The second step iteratively selects thenode with highest degree in the graph (root hub)and removes it along with its adjacent nodes.
Eachsuch selection corresponds to isolating a high-density component of the graph, in order to selecta sense of the target word.
In the last step the roothubs are linked to the target word and the Mini-mum Spanning Tree (MST) of the graph is com-puted to disambiguate the target word in context.Agirre et al (2006) compare Hyperlex with an al-ternative method to detect the root hubs based onPageRank (Brin and Page, 1998).
PageRank hasthe advantage of requiring less parameters thanHyperlex, whereas the authors ascertain equal per-formance of the two methods.3 Graph-based Cross-Lingual WSDWe start by building for each target word a mul-tilingual co-occurrence graph based on the targetword?s aligned contexts found in parallel corpora(Sections 3.1 and 3.2).
Multilingual nodes arelinked by translation edges, labeled with the targetword?s translations observed in the correspondingcontexts.
We then use an adapted PageRank al-gorithm to select the nodes which represent thetarget word?s different senses (Section 3.3) and,given these nodes, we compute the MST, whichis used to select the most relevant words in con-text to disambiguate a given test instance (Section3.4).
Translations are finally given by the incom-ing translation edges of the selected context words.1343.1 Monolingual GraphLet Csbe all contexts of a target word w ina source language s, i.e.
English in our case,within a (PoS-tagged and lemmatized) monolin-gual corpus.
We first construct a monolingual co-occurrence graph Gs= ?Vs, Es?.
We collect allpairs (cwi, cwj) of co-occurring nouns or adjec-tives in Cs(excluding the target word itself) andadd each word as a node into the initially emptygraph.
Each co-occurring word pair is connectedwith an edge (vi, vj) ?
Es, which is assigned aweight w(vi, vj) based on the strength of associa-tion between the respective words cwiand cwj:w(vi, vj) = 1?max [p(cwi|cwj), p(cwj|cwi)].The conditional probability of word cwigivenword cwjis estimated by the number of contextsin which cwiand cwjco-occur divided by thenumber of contexts containing cwj.3.2 Multilingual GraphGiven a set of target languages L, we then ex-tend Gsto a labeled multilingual graph GML=?VML, EML?
where:1.
VML= Vs?
?l?LVlis a set of nodes represent-ing content words from either the source (Vs) orthe target (Vl) languages;2.
EML= Es??l?L{El?
Es,l} is a set ofedges.
These include (a) co-occurrence edgesEl?
Vl?Vlbetween nodes representing wordsin a target language (Vl), weighted in the sameway as the edges in the monolingual graph;(b) labeled translation edges Es,lwhich repre-sent translations of words from the source lan-guage into a target language.
These edges areassigned a complex label t ?
Tw,lcompris-ing a translation of the word w in the targetlanguage l and its frequency of translation, i.e.Es,l?
Vs?
Tw,l?
Vl.The multilingual graph is built based on a word-aligned multilingual parallel corpus and a multi-lingual dictionary.
The pseudocode is presented inAlgorithm 1.
We start with the monolingual graphfrom the source language (line 1) and then for eachtarget language l ?
L in turn, we add the transla-tion edges (vs, t, vl) ?
Es,lof each word in thesource language (lines 5-15).
In order to includethe information about the translations of w in thedifferent target languages, each translation edgeAlgorithm 1 Multilingual co-occurrence graph.Input: target word w and its contexts Csmonolingual graph Gs= ?Vs, Es?set of target languages LOutput: a multilingual graph GML1: GML= ?VML, EML?
?
Gs= ?Vs, Es?2: for each l ?
L3: Vl?
?4: Cl:= aligned sentences of Csin lang.
l5: for each vs?
Vs6: Tvs,l:= translations of vsfound in Cl7: Cvs?
Cs:= contexts containing w and vs8: for each translation vl?
Tvs,l9: Cvl:= aligned sentences of Cvsin lang.
l10: Tw,Cvl?
translation labels of w from Cvl11: if vl/?
VMLthen12: VML?
VML?
vl13: Vl?
Vl?
vl14: for each t ?
Tw,Cvl15: EML?
EML?
(vs, t, vl)16: for each vi?
Vl17: for each vj?
Vl, i 6= j18: if viand vjco-occur in Clthen19: EML?
EML?
(vi, vj)20: return GML(vs, t, vl) receives a translation label t. Formally,let Cvs?
Csbe the contexts where vsand w co-occur, and Cvlthe word-aligned contexts in lan-guage l of Cvs, where vsis translated as vl.
Theneach edge between nodes vsand vlis labeled witha translation label t (lines 14-15): this includes atranslation of w in Cvl, its frequency of transla-tion and the information of whether the transla-tion is monosemous, as found in a multilingualdictionary, i.e.
EuroWordNet (Vossen, 1998) andPanDictionary (Mausam et al, 2009).
Finally, themultilingual graph is further extended by insertingall possible co-occurrence edges (vi, vj) ?
Elbe-tween the nodes for the target language l (lines 16-19, i.e.
we apply the step from Section 3.1 to l andCl).
As a result of the algorithm, the multilingualgraph is returned (line 20).3.3 Computing Root HubsWe compute the root hubs in the multilingualgraph to discriminate the senses of the target wordin the source language.
Hubs are found using theadapted PageRank from Agirre et al (2006):135PR(vi) = (1?
d) + d?j?deg(vi)wij?k?deg(vj)wjkPR(vj)where d is the so-called damping factor (typicallyset to 0.85), deg(vi) is the number of adjacentnodes of node viand wijis the weight of the co-occurrence edge between nodes viand vj.Since this step aims to induce the senses forthe target word, only nodes referring to wordsin English can become root hubs.
However, inorder to use additional evidence from other lan-guages, we furthermore include in the computa-tion of PageRank co-occurrence edges from thetarget languages, as long as these occur in con-texts with ?safe?, i.e.
monosemous, translations ofthe target word.
Given an English co-occurrenceedge (vs,i, vs,j) and translation edges (vs,i, vl,i)and (vs,j, vl,j) to nodes in the target languagel, labeled with monosemous translations, we in-clude the co-occurrence edge (vl,i, vl,j) in thePageRank computation.
For instance, animal andbiotechnology are translated in German as Tierand Biotechnologie, both with edges labeled withthe monosemous Pflanze: accordingly, we in-clude the edge (Tier,Biotechnologie) in the com-putation of PR(vi), where viis either animal orbiotechnology.Finally, following V?eronis (2004), a MST isbuilt with the target word as its root and the roothubs of GMLforming its first level.
By using amultilingual graph, we are able to obtain MSTswhich contain translation nodes and edges.3.4 Multilingual DisambiguationGiven a context W for the target word w in thesource language, we use the MST to find the mostrelevant words in W for disambiguating w. Wefirst map each content word cw ?
W to nodesin the MST.
Since each word is dominated by ex-actly one hub, we can find the relevant nodes bycomputing the correct hub disHub (i.e.
sense) andthen only retain those nodes linked to disHub.
LetWhbe the set of mapped content words dominatedby hub h. Then, disHub can be found as:disHub = argmaxh?cw?Whd(cw)dist(cw, h) + 1where d(cw) is a function which assigns a weightto cw according to its distance to w, i.e.
the morewords occur between w and cw within W , thesmaller the weight, and dist(cw, h) is given bythe number of edges between cw and h in theMST.
Finally, we collect the translation edges ofthe retained context nodes WdisHuband we sumthe translation counts to rank each translation.4 Results and AnalysisExperimental Setting.
We submitted two runsfor the task (UHD-1 and UHD-2 henceforth).Since we were interested in assessing the impactof using different resources with our methodology,we automatically built multilingual graphs fromdifferent sentence-aligned corpora, i.e.
Europarl(Koehn, 2005) for UHD-1, augmented with theJRC-Acquis corpus (Steinberger et al, 2006) forUHD-21.
Both corpora were tagged and lemma-tized with TreeTagger (Schmid, 1994) and wordaligned using GIZA++ (Och and Ney, 2003).
ForGerman, in order to avoid the sparseness derivingfrom the high productivity of compounds, we per-formed a morphological analysis using Morphisto(Zielinski et al, 2009).To build the multilingual graph (Section 3.2),we used a minimum frequency threshold of 2 oc-currences for a word to be inserted as a node,and retained only those edges with a weight lessor equal to 0.7.
After constructing the multilin-gual graph, we additionally removed those trans-lations with a frequency count lower than 10 (7in the case of German, due to the large amountof compounds).
Finally, the translations gener-ated for the BEST evaluation setting were ob-tained by applying the following rule onto theranked answer translations: add translation triwhile count(tri) ?
count(tri?1)/3, where i isthe i-th ranked translation.Results and discussion.
The results for theBEST and out-of-five (OOF) evaluations are pre-sented in Tables 1 and 2 respectively.
Results arecomputed using the official scorer (Lefever andHoste, 2010) and no post-processing is applied tothe system?s output, i.e.
we do not back-off to thebaseline most frequent translation in case the sys-tem fails to provide an answer for a test instance.For the sake of brevity, we present the results forUHD-1, since we found no statistically significantdifference in the performance of the two systems(e.g.
UHD-2 outperforms UHD-1 only by +0.7%on the BEST evaluation for French).1As in the case of Europarl, only 1-to-1-aligned sentenceswere extracted.136Language P R Mode P Mode RFRENCH 20.22 16.21 17.59 14.56GERMAN 12.20 9.32 11.05 7.78ITALIAN 15.94 12.78 12.34 8.48SPANISH 20.48 16.33 28.48 22.19Table 1: BEST results (UHD-1).Language P R Mode P Mode RFRENCH 39.06 32.00 37.00 26.79GERMAN 27.62 22.82 25.68 21.16ITALIAN 33.72 27.49 27.54 21.81SPANISH 38.78 31.81 40.68 32.38Table 2: OOF results (UHD-1).Overall, in the BEST evaluation our systemranked in the middle for those languages wherethe majority of systems participated ?
i.e.
sec-ond and fourth out of 7 submissions for FRENCHand SPANISH.
When compared against the base-line, i.e.
the most frequent translation found inEuroparl, our method was able to achieve in theBEST evaluation a higher precision for ITALIANand SPANISH (+1.9% and +2.1%, respectively),whereas FRENCH and GERMAN lie near below thebaseline scores (?0.5% and?1.0%, respectively).The trade-off is a recall always below the base-line.
In contrast, we beat the Mode precision base-line for all languages, i.e.
up to +5.1% for SPAN-ISH.
The fact that our system is strongly precision-oriented is additionally proved by a low perfor-mance in the OOF evaluation, where we alwaysperform below the baseline (i.e.
the five most fre-quent translations in Europarl).5 ConclusionsWe presented in this paper a graph-based systemto perform CL-WSD.
Key to our approach is theuse of a co-occurrence graph built from multilin-gual parallel corpora, and the application of well-studied graph algorithms for monolingual WSD(V?eronis, 2004; Agirre et al, 2006).
Future workwill concentrate on extensions of the algorithms,e.g.
computing hubs in each language indepen-dently and combining them as a joint problem, aswell as developing robust techniques for unsuper-vised tuning of the graph weights, given the obser-vation that the most frequent translations tend toreceive too much weight and accordingly crowdout more appropriate translations.
Finally, weplan to investigate the application of our approachdirectly to multilingual lexical resources such asPanDictionary (Mausam et al, 2009) and Babel-Net (Navigli and Ponzetto, 2010).ReferencesEneko Agirre, David Mart?
?nez, Oier L?opez de Lacalle,and Aitor Soroa.
2006.
Two graph-based algorithmsfor state-of-the-art WSD.
In Proc.
of EMNLP-06,pages 585?593.Sergey Brin and Lawrence Page.
1998.
Theanatomy of a large-scale hypertextual web searchengine.
Computer Networks and ISDN Systems,30(1?7):107?117.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Database.
MIT Press, Cambridge, MA.Philipp Koehn.
2005.
Europarl: A parallel corpusfor statistical machine translation.
In Proceedingsof Machine Translation Summit X.Els Lefever and Veronique Hoste.
2010.
SemEval-2010 Task 3: Cross-lingual Word Sense Disam-biguation.
In Proc.
of SemEval-2010.Mausam, Stephen Soderland, Oren Etzioni, DanielWeld, Michael Skinner, and Jeff Bilmes.
2009.Compiling a massive, multilingual dictionary viaprobabilistic inference.
In Proc.
of ACL-IJCNLP-09, pages 262?270.Roberto Navigli and Simone Paolo Ponzetto.
2010.BabelNet: Building a very large multilingual seman-tic network.
In Proc.
of ACL-10.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Helmut Schmid.
1994.
Probabilistic part-of-speechtagging using decision trees.
In Proceedings of theInternational Conference on New Methods in Lan-guage Processing (NeMLaP ?94), pages 44?49.Ralf Steinberger, Bruno Pouliquen, Anna Widiger,Camelia Ignat, Toma?z Erjavec, Dan Tufis?, andD?aniel Varga.
2006.
The JRC-Acquis: A multilin-gual aligned parallel corpus with 20+ languages.
InProc.
of LREC ?06.Jean V?eronis.
2004.
Hyperlex: lexical cartographyfor information retrieval.
Computer Speech & Lan-guage, 18(3):223?252.Piek Vossen, editor.
1998.
EuroWordNet: A Multi-lingual Database with Lexical Semantic Networks.Kluwer, Dordrecht, The Netherlands.Andrea Zielinski, Christian Simon, and Tilman Wittl.2009.
Morphisto: Service-oriented open sourcemorphology for German.
In State of the Art in Com-putational Morphology, volume 41 of Communica-tions in Computer and Information Science, pages64?75.
Springer.137
