Proceedings of the EACL 2012 Workshop on Computational Approaches to Deception Detection, pages 97?106,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsIdentification of Truth and Deception in Text:Application of Vector Space Model to Rhetorical Structure TheoryVictoria L. Rubin and Tatiana VashchilkoLanguage and Information Technology Research Lab (LiT.RL)Faculty of Information and Media Studies, University of Western OntarioLondon, Ontario, Canada{vrubin,tvashchi}@uwo.caAbstractThe paper proposes to use RhetoricalStructure Theory (RST) analyticframework to identify systematicdifferences between deceptive andtruthful stories in terms of theircoherence and structure.
A sample of 36elicited personal stories, self-ranked ascompletely truthful or completelydeceptive, is manually analyzed byassigning RST discourse relations amonga story?s constituent parts.
Vector SpaceModel (VSM) assesses each story?sposition in multi-dimensional RST spacewith respect to its distance to truth anddeceptive centers as measures of thestory?s level of deception andtruthfulness.
Ten human judges evaluateif each story is deceptive or not, andassign their confidence levels, whichproduce measures of the human expecteddeception and truthfulness levels.
Thepaper contributes to deception detectionresearch and RST twofold: a)demonstration of discourse structureanalysis in pragmatics as a prominentway of automated deception detectionand, as such, an effective complement tolexico-semantic analysis, and b)development of RST-VSM methodologyto interpret RST analysis in identificationof previously unseen deceptive texts.IntroductionAutomated deception detection is a challengingtask (DePaulo, Charlton, Cooper, Lindsay, andMuhlenbruck, 1997), only recently provenfeasible with natural language processing andmachine learning techniques (Bachenko,Fitzpatrick, and Schonwetter, 2008; Fuller, Biros,and Wilson, 2009; Hancock, Curry, Goorha, andWoodworth, 2008; Rubin, 2010; Zhou, Burgoon,Nunamaker, and Twitchell, 2004).
The idea is todistinguish truthful information from deceptive,where deception usually implies an intentionaland knowing attempt on the part of the sender tocreate a false belief or false conclusion in themind of the receiver of the information (e.g.,Buller and Burgoon, 1996; Zhou, et al, 2004).
Inthis paper we focus solely on textual information,in particular, in computer-mediated personalcommunications such as e-mails or online posts.Previously suggested techniques for detectingdeception in text reach modest accuracy rates atthe level of lexico-semantic analysis.
Certainlexical items are considered to be predictivelinguistic cues, and could be derived, forexamples, from the Statement Validity Analysistechniques used in law enforcement forcredibility assessments (as in Porter and Yuille,1996).
Though there is no clear consensus onreliable predictors of deception, deceptive cuesare identified in texts, extracted and clusteredconceptually, for instance, to represent diversity,complexity, specificity, and non-immediacy ofthe analyzed texts (e.g., Zhou, Burgoon,Nunamaker, and Twitchell (2004)).
Whenimplemented with standard classificationalgorithms (such as neural nets, decision trees,and logistic regression), such methods achieve74% accuracy (Fuller, et al, 2009).
Existingpsycholinguistic lexicons (e.g., LWIC byPennebaker and Francis, 1999) have beenadapted to perform binary text classifications fortruthful versus deceptive opinions, with anaverage classifier demonstrating 70% accuracyrate (Mihalcea and Strapparava, 2009).These modest results, though usually achievedon restricted topics, are promising since theysupersede notoriously unreliable human abilitiesin lie-truth discrimination tasks.
On average,people are not very good at spotting lies (Vrij,2000), succeeding generally only about half ofthe time (Frank, Paolantinio, Feeley, and97Servoss, 2004).
For instance, a meta-analyticalreview of over 100 experiments with over 1,000participants, showed a 54% mean accuracy rateat identifying deception (DePaulo, et al, 1997).Human judges achieve 50 ?
63% success rates,depending on what is considered deceptive on aseven-point scale of truth-to-deceptioncontinuum (Rubin and Conroy, 2011, Rubin andConroy, 2012), but the higher the actual self-reported deception level of the story, the morelikely a story would be confidently assigned asdeceptive.
In other words, extreme degrees ofdeception are more transparent to judges.The task for current automated deceptiondetection techniques has been formulated asbinary text categorization ?
is a messagedeceptive or truthful ?
and the decision applies tothe whole analyzed text.
Since it is an overalldiscourse level decision, it may be reasonable toconsider discourse or pragmatic features of eachmessage.
Thus far, discourse is surprisinglyrarely considered, if at all, and the majority of theeffort has been restricted to lexico-semanticverbal predictors.
A rare exception up to date hasbeen a Bachenko, Fitzpatrick and Schonwetter?s(2008) study that focuses on truth or falsity ofindividual propositions, achieving a finer-grainedlevel of analysis 1 , but the propositional inter-relations within the discourse structure are notconsidered.
To the best of our knowledge therehave been no advances in that automationdeception detection task to incorporate discoursestructure features and/or text coherence analysisat the pragmatic levels of story interpretation.Study ObjectiveWith the recent advances in the identification ofverbal cues of deception in mind, and therealization that they focus on linguistic levelsbelow discourse and pragmatic analysis, thestudy focuses on one main question:?
What is the impact of the relationsbetween discourse constituent parts onthe discourse composition of deceptiveand truthful messages?We hypothesize that if the relations betweendiscourse constituent parts in deceptive messagesdiffer from the ones in truthful messages, thensystematic analysis of such relations will help to1 Using a corpus of criminal statements, police interrogations andlegal testimonies, their regression and tree-based classificationautomatic tagger performs at average 69% recall and 85% precisionrates, as compared to the performance of human taggers on thesame subset (Bachenko, et al, 2008).detect deception.
To investigate this question, wepropose to use a novel methodology fordeception detection research, RhetoricalStructure Theory (RST) analysis with subsequentapplication of the Vector Space Model (VSM).RST analysis is promising in deception detection,since RST analysis captures coherence of a storyin terms of functional relations among differentmeaningful text units, and describes ahierarchical structure of each story (Mann andThompson, 1988).
The result is that each story isa set of RST relations connected in a hierarchicalmanner with more salient text units heading thishierarchical tree.
We also propose to utilize theVSM model for conversion of the derived RSTrelations?
frequencies into meaningful clusters ofdiverse deception levels.
To evaluate theproposed RST-VSM methodology of deceptiondetection in texts, we compare human assessmentto the RST-analysis of deception levels for thesets of deceptive and truthful stories.
The mainfindings demonstrate that RST resembles, tosome degree, human judges in deceptive andtruthful stories, and RST deception detection inself-rated deceptive stories has greaterconsistency than in truthful ones, which signifiesthe prominence of using RST-VSM methodologyfor deception detection 2 .
However, RSTconclusions regarding levels of deception in thetruthful stories requires further research about thediversity of RST relations for the expressions oftruths and deception as well as the types ofclustering algorithms most suitable for clusteringunevaluated by human judges?
writtencommunication in RST space to detect deceptionwith certain degree of precision.The paper has three main parts.
The next partdiscusses methodological foundations of RST-VSM approach.
Then, the data and collectionmethod describe the sample.
Finally, the resultssection demonstrates the identified levels ofdeception and truthfulness as well as theirdistribution across truthful and deceptive stories.RST-VSM Methodology: CombiningVector Space Model and RhetoricalStructure TheoryVector space model (VSM) seemed to be veryuseful in the identification of truth and deceptiontypes of written stories especially if the meaning2 The authors recognize that the results are preliminary and shouldbe generalized with caution due to very small dataset and certainmethodological issues that require further development.98of the stories is represented as RST relations.RST differentiates between rhetorically stand-alone parts of a text, some of which are moresalient (nucleolus) than the others (satellite).
Inthe past couple of decades, empiricalobservations and previous RST researchconfirmed that writers tend to emphasize certainparts of a text in order to express their mostessential idea to reach the purpose of the writtenmessage.
These parts can be systematicallyidentified through the analysis of the rhetoricalconnections among more and less essential partsof a text.
RST helps to describe and quantify textcoherence through a set of constraints onnucleolus and satellites.
The main function ofthese constraints is to describe in the meaningfulway why and how one part of a text connects tothe others within a hierarchical tree structure,which is an RST representation of a coded text.The names of the RST relations also resemblethe purpose of using the connected text partstogether.For example, one of the RST relations, whichappear in truthful stories and never appear in thedeceptive stories in our sample, is EVIDENCE.The main purpose of using EVIDENCE toconnect two parts of text is to present additionalinformation in satellite, so that the reader?s beliefabout the information in the nucleolus increases.However, this can happen only if the informationin the satellite is credible from reader?s point ofview.
For some reason, the RST coding of 18deceptive stories has never used EVIDENCE, butused it rather often in 18 truthful stories.
Thismight indicates that either 1) writers of deceptivestories did not see any purpose in supplyingadditional information to the readers to increasetheir beliefs in communicating writer?s essentialideas, or 2) the credibility of presentedinformation in satellite was not credible from thereaders?
points of view, which did not qualify therelationship between nucleolus and satellite for?EVIDENCE?
relation, or 3) both (See anexample of RST diagram in Appendix A).Our premise is that if there are systematicdifferences between deceptive and truthfulwritten stories in terms of their coherence andstructure, then the RST analysis of these storiescan identify two sets of RST relations and theirstructure.
One set is specific for the deceptivestories, and the other one is specific for thetruthful stories.We propose to use a vector space model forthe identification of these sets of RST relations.Mathematically speaking, written stories have tobe modeled in a way suitable for the applicationof various computational algorithms based onlinear algebra.
Using a vector space model, thewritten stories could be represented as RSTvectors in a high dimensional space (Salton andMcGill 1983, Manning and Schutse 1999).According to the VSM, stories are represented asvectors, and the dimension of the vector spaceequals to the number of RST relations in a set ofall written stories under consideration.
Suchrepresentation of written stories makes the VSMvery attractive in terms of its simplicity andapplicability (Baeza-Yates and Ribeiro-Neto1999).Vector space model3 is the basis for almost allclustering techniques when dealing with theanalysis of texts.
Once the texts are representedaccording to VSM, as vectors in an n-dimensional space, we can apply the myriad ofcluster methods that have been developed inComputational Science, Data Mining,Bioinformatics.
Cluster analysis methods can bedivided into two big groups (Zhong and Ghosh2004): discriminative (or similarity based)approaches (Indyk 1999, Scholkopf and Smola2001, Vapnik 1998) and generative (or model-based) approaches (Blimes 1998, Rose 1998,Cadez et al 2000).The main benefit of applying vector spacemodel to RST analysis is that the VSM allows aformal identification of coherence and structuralsimilarities among stories of the same type(truthful or deceptive).
For this purpose, RSTrelations are vectors in a story space.
Visually wecould think about the set of stories or RSTrelations as a cube (Figure 1), in which eachdimension is an RST relation.Figure 1: Cluster Representation of Story Sets or RSTRelations (Cluto Graphical Frontend Project, 2002).3 Tombros (2002) maintains that most of the research related to theretrieval of information is based on vector space model.99The main subsets of this set of stories are twoclusters, deceptive stories and truthful stories.The element of a cluster is a story, and a clusteris a set of elements that share enough similarityto be grouped together, the deceptive stories ortruthful stories (Berkhin 2002).
That is, there is anumber of distinctive features (RST relations,their co-occurrences and positions in ahierarchical structure) that make each storyunique and being a member of a particularcluster.
These distinctive features of the storiesare compared, and when some similaritythreshold is met, they are placed in one of twogroups, deceptive or truthful stories.Similarity 4  is one of the key concepts incluster analysis, since most of the classicaltechniques (k-means, unsupervised Bayes,hierarchical agglomerative clustering) and ratherrecent ones (CLARANS, DBSCAN, BIRCH,CLIQUE, CURE, etc.)
?are based on distancesbetween the samples in the original vector space?
(Strehl et al2000).
Such algorithms form asimilarity based clustering framework (Figure 1)as it is described in Strehl et al(2000) , or asZhong and Ghosh (2004) define it asdiscriminative (or similarity ?
based) clusteringapproaches.That is why, this paper modifies Strehl et als(2004) similarity based clustering framework(Figure 2) to develop a unique RST-VSMmethodology for deception detection in text.
TheRST-VSM methodology includes three mainsteps:1) The set of written stories, X, is transformedinto the vector space description, X, using somerule, Y, that in our case corresponds to an RSTanalysis and identification of RST relations aswell as their hierarchy in each story.2) This vector space description X istransformed into a similarity space description,S, using some rule, ?
, which in our case is theEuclidian distance of every story from adeception and truth centers correspondinglybased on normalized frequency of RST relationsin a written story5.3) The similarity space description, S, ismapped into clusters based on the rule?
, whichwe define as the relative closeness of a story to a4 ?Interobject similarity is a measure of correspondence orresemblance between objects to be clustered?
(Hair et al1995, p. 429).5 Since RST stories as vectors differ in length, thenormalization assures their comparability.
The coordinatesof every story (the frequency of an RST relation in a story)are divided on the vector?s length.deception or a truth center: if a story is closer tothe truth center, then a story is placed in a truthcluster, whereas if a story is closer to a deceptioncenter, then a story is placed in a deceptioncluster.Figure 2: Similarity Based Clustering Framework(Strehl et al 2004)Data Collection and SampleThe dataset contains 36 rich unique personalstories, elicited using Amazon?s online surveyservice, Mechanical Turk (www.mturk.com).Respondents in one group were asked to write arich unique story, which is completely true orwith some degree of deception.
Respondents inanother group were asked to evaluate the storieswritten by the respondents in the first group (Forfurther details on the data collection process andthe discussion of associated challenges, seeRubin and Conroy 2012).Two groups of 18 stories each compile thedata sample.
The first group consists of 18 storiesthat were self-ranked by their authors ascompletely deceptive on a seven-point Likhertscale from complete truth to complete deception(deceptive self-reported group).
The secondgroup includes stories, which their authors ratedas completely truthful stories (truthful self-reported group).
The second group was matchedin numbers for direct comparisons to the firstgroup by selecting random 18 stories from alarger group of 39 completely truthful stories(Rubin and Conroy, 2011, Rubin and Conroy,2012).
Each story in both groups, truthful self-reported and deceptive self-reported, has 10unique human judgments associated with it.
Eachjudgment is binary (?judged truthful?
or ?judgeddeceptive?
), and has an associated confidencelevel assigned by the judge (either ?totallyuncertain?, ?somewhat uncertain?, ?I?mguessing?, ?somewhat certain?, or ?totallycertain?).
Each writer and judge was encouragedto provide explanations for defining a story astruthful or deceptive, and assigning a particularconfidence level.
In total, 396 participantscontributed to the study, 36 of them were storyauthors, and 360 ?
were judges performing lie-truth discrimination task by confidence level.100We combine the 10 judges?
evaluations of astory into one measure, the expected level of astory?s deception or truthfulness.
Since judges?confidence levels reflect the likelihood of a storybeing truthful or deceptive, the probability of astory being completely true or deceptive equalsone and corresponds to a ?totally certain?confidence level that the story is true ordeceptive6.
Two dummy variables are created foreach story.
One dummy, a deception dummy,equals 1, if a judge rated the story is ?judgeddeceptive?, and 0 otherwise.
The second dummy,the truthfulness dummy, equals 1 if a judge ratedthe story as ?judged truthful?, and 0 otherwise.Then the expected level of deception of a storyequals the product of the probability (confidencelevel) of deception and the deception dummyacross 10 judges.
Similarly, the expected level oftruthfulness is equals the product of theprobability of truthfulness (confidence level) andthe truthfulness dummy across 10 judges.
Thedistribution of expected levels of deception andthe expected levels of truthfulness of thedeceptive and truthful subsets of the sample arein Appendix B1-B2.Thirty six stories, evenly divided betweentruthful and deceptive self-report groups, weremanually analyzed using the classical set ofMann and Thompson?s (1988) RST relations,extensively tested empirically (Taboada andMann, 2006).
As a first stage of RST-VSMmethodology development, the manual RSTcoding was required to deepen the understandingof the rhetorical relations and structures specificfor deceptive and truthful stories.
Moreover,manual analysis aided by Mick O?Donnell?sRSTTool (http://www.wagsoft.com/RSTTool/)might ensure higher reliability of the analysis andavoid compilation of errors, as the RST outputfurther served as the VSM input.
Taboada (2004)reports on the existence of Daniel Marcu?s RSTAnnotation Tool: www.isi.edu/licensed-sw/RSTTool/ and Hatem Ghorbel?sRhetAnnotate (lithwww.epfl.ch/~ghorbel/rhetannotate/) and provides a good overview of otherrecent RST resources and applications.
Theacquired knowledge during manual coding ofdeceptive stories along with recent advances inautomated RST analysis will help later on toevaluate RST-VSM methodology and design a6 In the same way, the other levels of confidence have thefollowing probability correspondences: ?totally uncertain?has probability 0.2 of a story being deceptive or truthful,?somewhat uncertain?
?
0.4, ?I?m guessing?
?
0.6, and?somewhat certain?
?
0.8.completely automated deception detection toolrelying on the automated procedures to recognizerhetorical relations, which utilize the fullrhetorical parsing (Marcu 1997, 2002).ResultsThe preliminary clustering of 36 stories in RSTspace using various clustering algorithms showsthat RST dimensions can systematicallydifferentiate between truthful and deceptivestories as well as diverse levels of deception(Figure 3).Figure 3.
Four Clusters in RST Space by Level ofDeception.The visualization uses GLUTO software(http://glaros.dtc.umn.edu/gkhome/cluto/gcluto/overview), which finds the clustering solution as aresult of the optimization of a ?particularfunction that reflects the underlying definition ofthe ?goodness?
of the cluster?
(Rasmussen andKarypis 2004, p.3).
Among the four clusters inRST space, two clusters are composed ofcompletely deceptive stories (far back left peakin green) or entirely truthful stories (front peak inred), the other two clusters have a mixture withthe prevalence of either truthful or deceptivestories.
This preliminary investigation of usingRST space for deception detection indicates thatthe RST analysis seems to offer a systematicalway of distinguishing between truth anddeceptive features of texts.This paper develops an RST-VSMmethodology by using RST analysis of eachstory in N-dimensional RST space withsubsequent application of vector space model toidentify the level of a story?s deception.
Anormalized frequency of an RST relation in astory is a distinct coordinate in the RST space.The authors?
ratings are used to calculate the101centers for the truth and deception clusters basedon corresponding authors?
self-rated deceptionand truthful sets of stories in the sample.
Thenormalized Euclidian distances between a storyand each of the centers are defined as the degreeof deception of that story depending on itscloseness to the deception center.
The closer astory is to the deception center, the higher is itslevel of deception.
The closer a story is to thetruthful center, the higher is its level oftruthfulness7.RST seems to differentiate between truthfuland deceptive stories.
The difference in meanstest demonstrates that the truthful stories have astatistically significantly lower average numberof text units per statement than the deceptivestories (t= -1.3104), though these differences arenot large, only at 10% significance level.
Thenormalized frequencies of the RST relationsappearing in the truthful and deceptive storiesdiffer for about one third of all RST relationsbased on the difference in means test (AppendixC).The comparison of the distribution of RSTrelations across deceptive and truth centersdemonstrates that on average, the frequenciesand the usage of such RST relations asconjunction, elaboration, evaluation, list, means,non-volitional cause, non-volitional result,sequence, and solutionhood in deceptive storiesexceeds those in the truthful ones (Figure 4).
Onthe other hand, the average usage andfrequencies of such RST relations as volitionalresult, volitional cause, purpose, interpretation,concession, circumstance and antithesis intruthful stories exceeds those in the deceptiveones.
Some of the RST relations are only specificfor one type of the story: enablement,restatement and evidence appear only in truthfulstories, whereas summary, preparation,unconditional and disjunction appear only indeceptive stories.The histograms of distributions of deception(truthfulness) levels assigned by judges andderived from RST-VSM analysis demonstratesome similarities between the two for truth andfor deceptive stories (Appendices D-E).
Morerigorous statistical testing reveals that onlytruthfulness levels in deceptive stories assignedby judges do not have statistically significantdifference from the RST-VSM ones8.
For other7 All calculations are performed in STATA.8 We use the Wilcoxon signed rank sum test, which is the non-parametric version of a paired samples t-test (STATA commandsignrank (STATA 2012)).groups, the judges?
assessments and RST ones dodiffer significantly.Figure 4: Comparison of the RST Relations?Composing the Deceptive Cluster Center (top red bar)and the Truthful Cluster Center (bottom blue bar).The difference is especially apparent in thedistributions of deception and truthfulness intruthful stories.
Among them, RST-VSMmethodology counted 44.44% of stories having50% deception level, whereas judges counted61.11 percent of the same stories having lowdeception level of no more than 20%.
The levelof truthfulness was also much higher in judges?assessment than based on RST-VSMcalculations.0 0.2 0.4 0.6AntithesisBackgroundCircumstanceConcessionConditionConjunctionDisjunctionElaborationEnablementEvaluationEvidenceInterpretationJointListMeansNonvolitional?causeNonvolitional?resultPreparationPurposeRestatementSequenceSolutionhoodSummaryTopUnconditionalVolitional?causeVolitional?result102The distribution of the levels of deception andtruthfulness across all deceptive stories(Appendices D1-D4) and across all truthfulstories (Appendices E1-E4) shows variations inpatterns of deception levels based on RST-VSM.In deception stories, the RST-VSM levels ofdeception are consistently higher than the RST-VSM levels of truthfulness.
Assuming that theauthors of the stories did make them up, theRST-VSM methodology seems to offer asystematic way of detecting a high level ofdeception with rather good precision.The RST-VSM deception levels are not ashigh as human judges?
ones, with human judgesassigning much higher levels of deception todeceptive stories than to truthful stories.Assuming that the stories are indeed made up,the human judges have greater precision than theRST-VSM methodology.
Nevertheless, RST-VSM analysis assigns higher deception levels tostories, which also receive higher human judges?deception levels.
This pattern is consistent acrossall deceptive stories.DiscussionThe analysis of truthful stories shows somesystematic and some slightly contradictoryfindings.
On one hand, the levels of truthfulnessassigned by judges are predominantly higherthan the levels of deception.
Again, assumingthat the stories in the truthful set are completelytrue because the authors ranked them so, thehuman judges have greater likelihood of ratingthese stories as truthful than as deceptive.
Thiscan be an indicator of a good precision ofdeception detection by human judges.On the other hand, the RST-VSM analysisalso demonstrates that large subsample (but notas large as indicated by human judges) of truthfulstories is closer to the truth center than to thedeceptive one.
However, it seems that RST-VSMmethodology overestimates the levels ofdeception in the truthful stories compared tohuman judgesOverall, however, the RST-VSM analysisdemonstrates a positive support for the proposedhypothesis.
The apparent and consistentcloseness of deceptive stories to RST deceptioncenter (compared to the closeness of thedeceptive stories to the truthful center) andtruthful stories to RST truthful center canindicate that the relations between discourseconstituent parts differ between truthful anddeceptive messages.
Thus, since the truthful anddeceptive relations exhibit systematic differencesin RST space, the proposed RST-VSMmethodology seemed to be a prominent tool indeception detection.
The results, however, haveto be interpreted with caution, since the samplewas very small, and only one expert conductedRST coding.The discussion, however, might be extendedto the case, where the assumption of self-rankedlevels of deception and truthfulness do not hold.In this case we still suspect that even deceptivestory might contain elements of truth (thoughmuch less), and the truth story will have someelements of deception.
RST-VSM analysisdemonstrated greater levels of deception in truthand deceptive stories compared to the humanjudges.
This might indicate that RST-VSMpotentially offers an alternative to human judgesway of detecting deception when it is leastexpected in text (as in the example of supposedlytruthful stories) or detecting it in a more accurateway (if some level of deception is assumed as inthe completely deceptive stories).
The advantageof RST-VSM methodology is in its rigorous andsystematic approach of coding discourserelations and their subsequent analysis in RSTspace using vector space models.
As a result, therelations between units exhibiting differentdegrees of salience in text because of writers?purposes with their subsequent readers?perceptions become indicators of diversity indeception levels.ConclusionsTo conclude, relations between discourse partsalong with its structure seem to have differentpatterns in truthful and deception stories.
If so,RST-VSM methodology can be a prominent wayof detecting deception and complementing theexisting lexical ones.Our contribution to deception detectionresearch and RST twofold: a) we demonstratethat discourse structure analysis and pragmaticsas a promising way of automated deceptiondetection and, as such, an effective complementto lexico-semantic analysis, and b) we developthe unique RST-VSM methodology ofinterpreting RST analysis in identification ofpreviously unseen deceptive texts.AcknowledgmentsThis research is funded by the New Research andScholarly Initiative Award (10-303) from theAcademic Development Fund at Western.103ReferencesBachenko, J., Fitzpatrick, E., and Schonwetter, M.2008.
Verification and implementation oflanguage-based deception indicators in civil andcriminal narratives.
In Proceedings of the 22ndInternational Conf.
on Computational Linguistics.Baeza-Yates, R. and B. Ribeiro-Neto.
1999.
ModernInformation Retrieval.
New York: Addison-WesleyBuller, D. B., and Burgoon, J. K. 1996.
InterpersonalDeception Theory.
Communication Theory, 6(3),203-242.Berkhin, P. 2002.
Survey of Clustering Data MiningTechniques.
DOI: 10.1.1.18.3739.Blimes, J.
A.
1998.
A Gentle Tutorial of the EMAlgorithm and Its Application to ParameterEstimation for Gaussian Mixture and HiddenMarkov Models: Univ.
of California, Berkeley.Cadez, I. V, Gaffney, S. and P. Smyth.
2000.
AGeneral Probabilistic Framework for ClusteringIndividuals and Objects.
In Proceedings of the 6thACM SIGKDD International  Conference onKnowledge Discovery and Data Mining.DePaulo, B. M., Charlton, K., Cooper, H., Lindsay, J.J., and Muhlenbruck, L. 1997.
The Accuracy-Confidence Correlation in the Detection ofDeception.
Personality and Social PsychologyReview, 1(4), 346-357.Frank, M. G., Paolantinio, N., Feeley, T., and Servoss,T.
2004.
Individual and Small Group Accuracy inJudging Truthful and Deceptive Communication.Group Decision and Negotiation, 13, 45-59.Fuller, C. M., Biros, D. P., and Wilson, R. L. 2009.Decision support for determining veracity vialinguistic-based cues.
Decision Support Systems46(3), 695-703.gCLUTO: Graphical Clustering Toolkit 1.2.
Dept.
ofComputer Science, University of Minnesota.Hair, J.F., Anderson, R.E., Tathman, R.L.
and W.C.Black.
1995.
Multivariate Data Analysis withReadings.
Upper Saddle River, NJ: Princeton Hall.Hancock, J. T., Curry, L. E., Goorha, S., andWoodworth, M. 2008.
On lying and being lied to:A linguistic analysis of deception in computer-mediated communication.
Discourse Processes,45(1), 1-23.Indyk, P. 1999.
A Sublinear- time ApproximationScheme for Clustering in Metric Spaces.
InProceedings of the 40th Annual Symposium onFoundations of Computer Science.Karypis, G. 2003.
Cluto: A Clustering Toolkit.
Min-neapolis: Univ.
of Minnesota, Comp.
Sci.
Dept.Mann, W. C., and Thompson, S. A.
1988.
RhetoricalStructure Theory: Toward a Functional Theory ofText Organization.
Text, 8(3), 243-281.Manning, C.D.
and H. Schutze.
1999.
Foundations ofStatistical Natural Language Processing.Cambridge, MA: MIT Press.Mihalcea, R., and Strapparava, C. 2009.
The LieDetector: Explorations in the Automatic Recogni-tion of Deceptive Language.
In Proceedings of theACL, Aug. 2-7, Singapore.Pennebaker, J., and Francis, M. 1999.
Linguisticinquiry and word count: LIWC.
Erlbaum PublisherPorter, S., and Yuille, J. C. 1996.
The language ofdeceit: An investigation of the verbal clues todeception in the interrogation context.
Law andHuman Behavior, 20(4), 443-458.Rasmussen, M. and G. Karypis.
2004. gCLUTO: AnInteractive Clustering, Vizualization and AnalysisSystem.
UMN-CS TR-04-021.Rose, K. 1998.
Deterministic Annealing forClustering, Compression, Classification,Regression, and Related Optimization Problems.In Proceedings of the IEEE 86(11).Rubin, V.L.
2010.
On Deception and DeceptionDetection: Content Analysis of Computer-Mediated Stated Beliefs.
In Proceedings of theAmerican Soc.
for Information Science and Tech.Annual Meeting, Oct. 22-27, Pittsburgh.Rubin, V.L., and Conroy, N. 2011.
Challenges inAutomated Deception Detection in Computer-Mediated Communication.
In Proceedings of theAmerican Soc.
for Information Science and Tech.Annual Meeting, Oct. 9-12, New Orleans.Rubin V.L., Conroy, N. 2012.
Discerning Truth fromDeception: Human Judgments and AutomationEfforts.
First Monday 17(3), http://firstmonday.orgSalton, G. and M.J. McGill.
1983.
Introduction toModern Information Retrieval.
New York:McGraw-Hill.Scholkopf, B. and A. Smola.
2001.
Learning WithKernels.
Cambridge, MA: MIT Press.Strehl, A., Ghosh, J. and R. Mooney.
2000.
In  AAAIWorkshop of Artificial Intelligence for WebSearch, July 30, 58-64.Taboada, M. 2004.
Building Coherence andCohesion: Task-Oriented Dialogue in English andSpanish.
Amsterdam, Netherlands: Benjamins.Taboada, M. and W.C. Mann.
(2006).
Rhetoricalstructure theory: looking back and moving ahead.Discourse Studies, 8(3), 423-459.Tombros, A.
2002.
The effectiveness of query-basedhierarchic clustering of documents for informationretrieval.
PhD dissertation, Dept.
of ComputingScience, University of Glasgow.Vapnik, V. 1998.
Statistical Learning Theory.
NY.Wiley.Vrij, A.
2000.
Detecting Lies and Deceit.
NY: Wiley.Zhong, S. and Ghosh.
J., 2004.
A Comparative Studyof Generative Models for Document Clustering.
InSIAM Int.
Conf.
Data Mining Workshop onClustering High Dimensional Data and ItsApplications.Zhou, L., Burgoon, J. K., Nunamaker, J. F., andTwitchell, D. 2004.
Automating Linguistics-BasedCues for Detecting Deception in Text-BasedAsynchronous Computer-Mediated Communi-cations.
Group Decision and Negotiation, 13(1),81-106.104Appendix A.
Sample RST Analysis.Appendix B1.
Distributions of Expected Levels of Deception and Truthfulness in Deceptive Stories.Legend:         Expected level of Deception (Judges);            Expected Level of Truthfulness (Judges)RST Level of Deception;              RST Level of Truthfulness (transformed to the interval (0,1) with 0 min)Appendix B2.
Distributions of Expected Levels of Deception and Truthfulness in Truthful Stories.Appendix C. Comparison of the Normalized Frequencies of the RST Relationships in Truthful andDeceptive Stories: Difference in Means Test.RST?relationships?appearing?in?truthful?and?deceptive?stories?with?NO?statistically?significant?differences??RST?relationships?appearing?in?the?truthful?stories?with?statistically?significantly?GREATER?normalized?frequencies?than?the?deceptive?ones?RST?relationships?appearing?in?the?truthful?stories?with?statistically?significantly?LOWER?normalized?frequencies?than?the?deceptive?ones?Background,?Circumstance,?Concession,?Condi?tion,?Conjunction,?Elaboration,?Enablement,?Inter?pretation,?List,?Means,?Non?volitional?cause,?Non?volitional?result,?Purpose,?Restatement,?Se?quence,?Solutionhood,?Summary,?Unconditional?Antithesis?(t=2.3299)?Evidence?(t=3.7996)?Joint?(t=1.5961)?Volitional?cause?(t=1.8597)?Volitional?result?(t=1.8960)?Preparation?(t=??1.7533)?Evaluation?(t=??2.0762)?Disjunction?(t=??1.7850)?
?.1.2.3.4.5.6.7.8.9Low DeceptionHigh DeceptionLevel of Deception or TruthDeceptiveStory1DeceptiveStory2DeceptiveStory3DeceptiveStory4DeceptiveStory5DeceptiveStory6DeceptiveStory7DeceptiveStory8DeceptiveStory9DeceptiveStory10DeceptiveStory11DeceptiveStory12DeceptiveStory13DeceptiveStory14DeceptiveStory15DeceptiveStory16DeceptiveStory17DeceptiveStory18Expected Level of Deception (Judges) transformed to the interval (0,1) with 0 miRST Level of Deception Transformed to the interval (0,1) with 0 minRST Level of Truth Transformed to the interval (0,1) with 0 minExpected Level of Truthfulness (Judges) transformed to the interval (0,1) with 0.1.2.3.4.5.6.7.8.9Low Deception or TruthLevel of Deception or TruthTruthfulStory1TruthfulStory2TruthfulStory3TruthfulStory4TruthfulStory5TruthfulStory6TruthfulStory7TruthfulStory8TruthfulStory9TruthfulStory10TruthfulStory11TruthfulStory12TruthfulStory13TruthfulStory14TruthfulStory15TruthfulStory16TruthfulStory17TruthfulStory18RST Level of Truth Transformed to the interval (0,1) with 0 minExpected Level of Truthfulness (Judges) transformed to the interval (0,1) with 0105Appendices D1 ?D4.
Distribution of Deception and Truthfulness Levels for Deceptive StoriesD1.
Distribution of Deception Level (Judges) D2.
Distribution of Truthfulness Level (Judges)D3.
Distribution of Deception Level (RST)D4.
Distribution of Truthfulness Level (RST)Appendices E1-E4.
Distribution of Deception and Truthfulness Levels for True StoriesE1.
Distribution of Deception Level (Judges)E2.
Distribution of Truthfulness Level (Judges)E3.
Distribution of Deception Level (RST)E4.
Distribution of Truthfulness Level (RST)27.785.55627.7838.89010203040Percent0 .1 .2 .3 .4 .5Expected Level of Deception (Judges) transformed to the interval (0,1) with 0 mi44.4427.785.55622.22010203040Percent.2 .4 .6 .8Expected Level of Truthfulness (Judges) transformed to the interval (0,1) with 011.1133.3322.2233.33010203040Percent.2 .3 .4 .5 .6RST Level of Deception Transformed to the interval (0,1) with 0 min5.55633.3338.8922.22010203040Percent.1 .2 .3 .4 .5RST Level of Truth Transformed to the interval (0,1) with 0 min16.6761.1111.11 11.110204060Percent0 .1 .2 .3 .4Expected Level of Deception (Judges) transformed to the interval (0,1) with 0 mi11.1138.89 38.8911.11010203040Percent.2 .4 .6 .8Expected Level of Truthfulness (Judges) transformed to the interval (0,1) with 011.11 11.1133.3344.4401020304050Percent0 .1 .2 .3 .4 .5RST Level of Deception Transformed to the interval (0,1) with 0 min16.67 16.6727.7838.89010203040Percent.2 .3 .4 .5 .6RST Level of Truth Transformed to the interval (0,1) with 0 min106
