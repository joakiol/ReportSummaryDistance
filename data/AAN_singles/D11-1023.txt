Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 250?261,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsApproximate Scalable Bounded Space Sketch for Large Data NLPAmit Goyal and Hal Daume?
IIIDept.
of Computer ScienceUniversity of MarylandCollege Park, MD 20742{amit,hal}@umiacs.umd.eduAbstractWe exploit sketch techniques, especially theCount-Min sketch, a memory, and time effi-cient framework which approximates the fre-quency of a word pair in the corpus withoutexplicitly storing the word pair itself.
Thesemethods use hashing to deal with massiveamounts of streaming text.
We apply Count-Min sketch to approximate word pair countsand exhibit their effectiveness on three im-portant NLP tasks.
Our experiments demon-strate that on all of the three tasks, we getperformance comparable to Exact word paircounts setting and state-of-the-art system.
Ourmethod scales to 49 GB of unzipped web datausing bounded space of 2 billion counters (8GB memory).1 IntroductionThere is more data available today on the web thanthere has ever been and it keeps increasing.
Useof large data in the Natural Language Processing(NLP) community is not new.
Many NLP problems(Brants et al, 2007; Turney, 2008; Ravichandran etal., 2005) have benefited from having large amountsof data.
However, processing large amounts of datais still challenging.This has motivated NLP community to use com-modity clusters.
For example, Brants et al (2007)used 1500 machines for a day to compute the rela-tive frequencies of n-grams from 1.8TB of web data.In another work, a corpus of roughly 1.6 Terawordswas used by Agirre et al (2009) to compute pair-wise similarities of the words in the test sets usingthe MapReduce infrastructure on 2, 000 cores.
How-ever, the inaccessibility of clusters to an average userhas attracted the NLP community to use streaming,randomized, and approximate algorithms to handlelarge amounts of data (Goyal et al, 2009; Levenberget al, 2010; Van Durme and Lall, 2010).Streaming approaches (Muthukrishnan, 2005)provide memory and time-efficient framework todeal with terabytes of data.
However, these ap-proaches are proposed to solve a singe problem.For example, our earlier work (Goyal et al, 2009)and Levenberg and Osborne (2009) build approxi-mate language models and show their effectivenessin Statistical Machine Translation (SMT).
Stream-based translation models (Levenberg et al, 2010)has been shown effective to handle large parallelstreaming data for SMT.
In Van Durme and Lall(2009b), a Talbot Osborne Morris Bloom (TOMB)Counter (Van Durme and Lall, 2009a) was used tofind the top-K verbs ?y?
given verb ?x?
using thehighest approximate online Pointwise Mutual Infor-mation (PMI) values.In this paper, we explore sketch techniques,especially the Count-Min sketch (Cormode andMuthukrishnan, 2004) to build a single model toshow its effectiveness on three important NLP tasks:?
Predicting the Semantic Orientation of words(Turney and Littman, 2003)?
Distributional Approaches for word similarity(Agirre et al, 2009)?
Unsupervised Dependency Parsing (Cohen andSmith, 2010) with a little linguistics knowl-edge.In all these tasks, we need to compute associationmeasures like Pointwise Mutual Information (PMI),250and Log Likelihood ratio (LLR) between words.
Tocompute association scores (AS), we need to countthe number of times pair of words appear togetherwithin a certain window size.
However, explicitlystoring the counts of all word pairs is both computa-tionally expensive and memory intensive (Agirre etal., 2009; Pantel et al, 2009).
Moreover, the mem-ory usage keeps increasing with increase in corpussize.We explore Count-Min (CM) sketch to addressthe issue of efficient storage of such data.
TheCM sketch stores counts of all word pairs within abounded space.
Storage space saving is achievedby approximating the frequency of word pairs inthe corpus without explicitly storing the word pairsthemselves.
Both updating (adding a new word pairor increasing the frequency of existing word pair)and querying (finding the frequency of a given wordpair) are constant time operations making it efficientonline storage data structure for large data.
Sketchesare scalable and can easily be implemented in dis-tributed setting.We use CM sketch to store counts of word pairs(except word pairs involving stop words) within awindow of size1 7 over different size corpora.
Westore exact counts of words (except stop words) inhash table (since the number of unique words isnot large that is quadratically less than the num-ber of unique word pairs).
The approximate PMIand LLR scores are computed using these approxi-mate counts and are applied to solve our three NLPtasks.
Our experiments demonstrate that on all ofthe three tasks, we get performance comparable toExact word pair counts setting and state-of-the-artsystem.
Our method scales to 49 GB of unzippedweb data using bounded space of 2 billion counters(8 GB memory).
This work expands upon our ear-lier workshop papers (Goyal et al, 2010a; Goyal etal., 2010b).2 Sketch TechniquesA sketch is a compact summary data structure tostore the frequencies of all items in the input stream.Sketching techniques use hashing to map items instreaming data onto a small sketch vector that canbe updated and queried in constant time.
These tech-17 is chosen from intuition and not tuned.niques generally process the input stream in one di-rection, say from left to right, without re-processingprevious input.
The main advantage of using thesetechniques is that they require a storage which issub-linear in size of the input stream.
The followingsurveys comprehensively review the streaming liter-ature: (Rusu and Dobra, 2007; Cormode and Had-jieleftheriou, 2008).There exists an extensive literature on sketch tech-niques (Charikar et al, 2004; Li et al, 2008; Cor-mode and Muthukrishnan, 2004; Rusu and Dobra,2007) in algorithms community for solving manylarge scale problems.
However, in practice, re-searchers have preferred Count-Min (CM) sketchover other sketch techniques in many application ar-eas, such as Security (Schechter et al, 2010), Ma-chine Learning (Shi et al, 2009; Aggarwal and Yu,2010), and Privacy (Dwork et al, 2010).
This moti-vated us to explore CM sketch to solve three impor-tant NLP problems.22.1 Count-Min SketchThe Count-Min sketch (Cormode and Muthukrish-nan, 2004) is a compact summary data structureused to store the frequencies of all items in the in-put stream.
The sketch allows fundamental querieson the data stream such as point, range and innerproduct queries to be approximately answered veryquickly.
It can also be applied to solve the findingfrequent items problem (Manku and Motwani, 2002)in a data stream.
In this paper, we are only interestedin point queries.
The aim of a point query is to es-timate the count of an item in the input stream.
Forother details, the reader is referred to (Cormode andMuthukrishnan, 2004).Given an input stream of word pairs of length Nand user chosen parameters ?
and , the algorithmstores the frequencies of all the word pairs with thefollowing guarantees:?
All reported frequencies are within the true fre-quencies by at most N with a probability of atleast 1-?.?
The space used by the algorithm is O(1 log 1?
).2In future, in another line of research, we will explore com-paring different sketch techniques for NLP problems.251?
Constant time of O(log(1? ))
per each update andquery operation.2.1.1 CM Data StructureA Count-Min sketch (CM) with parameters (,?
)is represented by a two-dimensional array withwidth w and depth d :??
?sketch[1, 1] ?
?
?
sketch[1, w]... .
.
.
...sketch[d, 1] ?
?
?
sketch[d,w]??
?Among the user chosen parameters,  controls theamount of tolerable error in the returned count and?
controls the probability with which the returnedcount is not within the accepted error.
These val-ues of  and ?
determine the width and depth of thetwo-dimensional array respectively.
To achieve theguarantees mentioned in the previous section, weset w=2 and d=log(1?
).
The depth d denotes thenumber of pairwise-independent hash functions em-ployed by the algorithm and there exists a one-to-one correspondence between the rows and the setof hash functions.
Each of these hash functionshk:{x1 .
.
.
xN} ?
{1 .
.
.
w}, 1 ?
k ?
d, takes aword pair from the input stream and maps it into acounter indexed by the corresponding hash function.For example, h2(x) = 10 indicates that the wordpair ?x?
is mapped to the 10th position in the secondrow of the sketch array.Initialize the entire sketch array with zeros.Update Procedure: When a new word pair ?x?with count c arrives, one counter in each row (as de-cided by its corresponding hash function) is updatedby c.sketch[k, hk(x)]?
sketch[k, hk(x)] + c, ?1 ?
k ?
dQuery Procedure: Since multiple word pairs canget hashed to the same position, the frequency storedby each position is guaranteed to overestimate thetrue count.
Thus, to answer the point query for agiven word pair, we return minimum over all the po-sitions indexed by the k hash functions.
The answerto Query(x): c?
= mink sketch[k, hk(x)]Both update and query procedures involve evalu-ating d hash functions and reading of all the valuesin those indices and hence both these procedures arelinear in the number of hash functions.
Hence boththese steps require O(log(1? ))
time.
In our experi-ments (see Section 3.1), we found that a small num-ber of hash functions are sufficient and we use d=5.Hence, the update and query operations take only aconstant time.
The space used by the algorithm isthe size of the array i.e.
wd counters, where w is thewidth of each row.2.1.2 PropertiesApart from the advantages of being space ef-ficient, and having constant update and constantquerying time, the Count-Min sketch has also otheradvantages that makes it an attractive choice forNLP applications.?
Linearity: Given two sketches s1 and s2 com-puted (using the same parameters w and d)over different input streams, the sketch of thecombined data stream can be easily obtainedby adding the individual sketches in O(1 log 1?
)time which is independent of the stream size.?
The linearity is especially attractive because itallows the individual sketches to be computedindependent of each other, which means that itis easy to implement it in distributed setting,where each machine computes the sketch overa sub set of corpus.2.2 Conservative UpdateEstan and Varghese introduced the idea of conserva-tive update (Estan and Varghese, 2002) in the con-text of computer networking.
This can easily be usedwith CM sketch to further improve the estimate of apoint query.
To update a word pair ?x?
with fre-quency c, we first compute the frequency c?
of thisword pair from the existing data structure and thecounts are updated according to:c?
= mink sketch[k, hk(x)], ?1 ?
k ?
dsketch[k, hk(x)]?
max{sketch[k, hk(x)], c?+ c}The intuition is that, since the point query returnsthe minimum of all the d values, we will update acounter only if it is necessary as indicated by theabove equation.
Though this is a heuristic, it avoidsthe unnecessary updates of counter values and thusreduces the error.In our experiments, we found that employing theconservative update reduces the Average Relative252Error (ARE) of these counts approximately by a fac-tor of 1.5.
(see Section 3.1).
But unfortunately,this update can only be maintained over individualsketches in distributed setting.3 Intrinsic EvaluationsTo show the effectiveness of the CM sketch and CMsketch with conservative update (CU) in the contextof NLP, we perform intrinsic evaluations.
First, theintrinsic evaluations are designed to measure the er-ror in the approximate counts returned by CM sketchcompared to their true counts.
Second, we comparethe word pairs association rankings obtained usingPMI and LLR with sketch and exact counts.It is memory and time intensive to perform manyintrinsic evaluations on large data (Ravichandran etal., 2005; Brants et al, 2007; Goyal et al, 2009).Hence, we use a subset of corpus of 2 million sen-tences (Subset) from Gigaword (Graff, 2003) for it.We generate words and word pairs over a windowof size 7.
We store exact counts of words (exceptstop words) in a hash table and store approximatecounts of word pairs (except word pairs involvingstop words) in the sketch.3.1 Evaluating approximate sketch countsTo evaluate the amount of over-estimation error (seeSection 2.1) in CM and CU counts compared to thetrue counts, we first group all word pairs with thesame true frequency into a single bucket.
We thencompute the average relative error in each of thesebuckets.
Since low-frequency word pairs are moreprone to errors, making this distinction based on fre-quency lets us understand the regions in which thealgorithm is over-estimating.
Moreover, to focus onerrors on low frequency counts, we have only plot-ted word pairs with count at most 100.
Average Rel-ative error (ARE) is defined as the average of abso-lute difference between the predicted and the exactvalue divided by the exact value over all the wordpairs in each bucket.ARE = 1NN?i=1|Exacti ?
Predictedi|ExactiWhere Exact and Predicted denotes values of ex-act and CM/CU counts respectively; N denotes thenumber of word pairs with same counts in a bucket.In Fig.
1(a), we fixed the number of counters to 20million (20M ) with four bytes of memory per eachcounter (thus it only requires 80 MB of main mem-ory).
Keeping the total number of counters fixed,we try different values of depth (2, 3, 5 and 7) of thesketch array and in each case the width is set to 20Md .The ARE curves in each case are shown in Fig.
1(a).We can make three main observations from Figure1(a): First it shows that most of the errors occur onlow frequency word pairs.
For frequent word pairs,in almost all the different runs the ARE is close tozero.
Secondly, it shows that ARE is significantlylower (by a factor of 1.5) for the runs which useconservative update (CUx run) compared to the runsthat use direct CM sketch (CMx run).
The encourag-ing observation is that, this holds true for almost alldifferent (width,depth) settings.
Thirdly, in our ex-periments, it shows that using depth of 3 gets com-paratively less ARE compared to other settings.To be more certain about this behavior with re-spect to different settings of width and depth, wetried another setting by increasing the number ofcounters to 50 million.
The curves in 1(b) follow apattern which is similar to the previous setting.
Lowfrequency word pairs are more prone to error com-pared to the frequent ones and employing conserva-tive update reduces the ARE by a factor of 1.5.
Inthis setting, depth 5 does slightly better than depth 3and gets lowest ARE.We use CU counts and depth of 5 for the rest ofthe paper.
As 3 and 5 have lowest ARE in differentsettings and using 5 hash functions, we get ?
= 0.01(d = log(1? )
refer Section 2.1) that is probability offailure is 1 in 100, making the algorithm more robustto false positives compared with 3 hash functions,?
= 0.1 with probability of failure 1 in 10.Fig.
1(c) studies the effect of the number of coun-ters in the sketch (the size of the two-dimensionalsketch array) on the ARE with fixed depth 5.
As ex-pected, using more number of counters decreases theARE in the counts.
This is intuitive because, as thelength of each row in the sketch increases, the prob-ability of collision decreases and hence the array ismore likely to contain true counts.
By using 100million counters, which is comparable to the lengthof the stream 88 million, we are able to achieve al-most zero ARE over all the counts including the rare253100 101 10200.511.522.53True frequency counts of word pairs (log scale)AverageRelativeErrorCM?7CM?5CM?3CM?2CU?7CU?5CU?3CU?2(a) 20M counters100 101 10200.050.10.150.20.250.30.350.4True frequency counts of word pairs (log scale)Average RelativeErrorCM?7CM?5CM?3CM?2CU?7CU?5CU?3CU?2(b) 50M counters100 101 102012345True frequency counts of word pairs (log scale)AverageRelative Error10M20M50M100M(c) Different size models with depth 5Figure 1: Compare 20 and 50 million counter models with different (width,depth) settings.
The notation CMx represents theCount Min sketch with a depth of ?x?
and CUx represents the CM sketch along with conservative update and depth ?x?.ones3.
Note that the space we save by not storing theexact counts is almost four times the memory thatwe use here because on an average each word pairis twelve characters long and requires twelve bytes(thrice the size of an integer) and 4 bytes for storingthe integer count.
Note, we get even bigger spacesavings if we work with longer phrases (phrase clus-tering), phrase pairs (paraphrasing/translation), andvarying length n-grams (Information Extraction).3.2 Evaluating word pairs association rankingIn this experiment, we compare the word pairs asso-ciation rankings obtained using PMI and LLR withCU and exact word pair counts.
We use two kinds ofmeasures, namely recall and Spearman?s correlationto measure the overlap in the rankings obtained byexact and CU counts.
Intuitively, recall captures thenumber of word pairs that are found in both the setsand then Spearman?s correlation captures if the rela-tive order of these common word pairs is preservedin both the rankings.
In our experimental setup, ifthe rankings match exactly, then we get a recall (R)of 100% and a correlation (?)
of 1.The results with respect to different sized counter(20 million (20M ), 50 million (50M )) models areshown in Table 1.
If we compare the second andthird column of the table using PMI and LLR for20M counters, we get exact rankings for LLR com-pared to PMI while comparing TopK word pairs.The explanation for such a behavior is: since we are3Even with other datasets we found that using counters lin-ear in the size of the stream leads to ARE close to zero ?
counts.# Cs 20M 50MAS PMI LLR PMI LLRTopK R ?
R ?
R ?
R ?50 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0100 .98 .94 1.0 1.0 1.0 1.0 1.0 1.0500 .80 .98 1.0 1.0 .98 1.0 1.0 1.01000 .56 .99 1.0 1.0 .96 .99 1.0 1.05000 .35 .90 1.0 1.0 .85 .99 1.0 1.010000 .38 .55 1.0 1.0 .81 .95 1.0 1.0Table 1: Evaluating the PMI and LLR rankings obtained usingCM sketch with conservative update (CU) and Exact countsnot throwing away any infrequent word pairs, PMIwill rank pairs with low frequency counts higher(Church and Hanks, 1989).
Hence, we are evaluat-ing the PMI values for rare word pairs and we needcounters linear in size of stream to get alost perfectranking.
This is also evident from the fourth columnfor 50M of the Table 1, where CU PMI ranking getsclose to the optimal as the number of counters ap-proaches stream size.However, in some NLP problems, we are not in-terested in low-frequency items.
In such cases, evenusing space less than linear in number of counterswould suffice.
In our extrinsic evaluations, we showthat using space less than the length of the streamdoes not degrade the performance.4 Extrinsic Evaluations4.1 DataGigaword corpus (Graff, 2003) and a 50% portionof a copy of web crawled by (Ravichandran et al,2542005) are used to compute counts of words and wordpairs.
For both the corpora, we split the text intosentences, tokenize and convert into lower-case.
Wegenerate words and word pairs over a window of size7.
We use four different sized corpora: SubSet (usedfor intrinsic evaluations in Section 3), Gigaword(GW), GigaWord + 20% of web data (GWB20), andGigaWord + 50% of web data (GWB50).
CorpusStatistics are shown below.
We store exact counts ofwords in a hash table and store approximate countsof word pairs in the sketch.
Hence, the stream sizein our case is the total number of word pairs in acorpus.Corpus Subset GW GWB20 GWB50Unzipped .32 9.8 22.8 49Size (GB)# of sentences 2.00 56.78 191.28 462.60(Million)Stream Size .088 2.67 6.05 13.20(Billion)4.2 Semantic OrientationGiven a word, the task of finding the Semantic Ori-entation (SO) (Turney and Littman, 2003) of theword is to identify if the word is more likely to beused in positive or negative sense.
We use a similarframework as used by the authors to infer the SO.We take the seven positive words (good, nice, excel-lent, positive, fortunate, correct, and superior) andthe seven negative words (bad, nasty, poor, negative,unfortunate, wrong, and inferior) used in (Turneyand Littman, 2003) work.
The SO of a given wordis calculated based on the strength of its associationwith the seven positive words, and the strength ofits association with the seven negative words.
Wecompute the SO of a word ?w?
as follows:SO-AS(W) =?p?PwordsAS(p, w)?
?n?NwordsAS(n,w)Where, Pwords and Nwords denote the seven pos-itive and negative prototype words respectively.
Weuse PMI and LLR to compute association scores(AS).
If this score is positive, we predict the wordas positive.
Otherwise, we predict it as negative.We use the General Inquirer lexicon4 (Stone etal., 1966) as a benchmark to evaluate the semantic4The General Inquirer lexicon is freely available at http://www.wjh.harvard.edu/?inquirer/orientation scores similar to (Turney and Littman,2003) work.
Words with multiple senses have multi-ple entries in the lexicon, we merge these entries forour experiment.
Our test set consists of 1597 posi-tive and 1980 negative words.
Accuracy is used asan evaluation metric and is defined as the percentageof number of correctly identified SO words.0 500M 1B 1.5B 2B60657075Model SizeAccuracyCUExact(a) SO PMI0 500M 1B 1.5B 2B55606570Model SizeAccuracyCUExact(b) SO LLRFigure 2: Evaluating Semantic Orientation using PMI and LLRwith different number of counters of CU sketch built using Gi-gaword.4.2.1 Varying sketch sizeWe evaluate SO of words using PMI and LLRon Gigaword (9.8GB).
We compare approximateSO computed using varying sizes of CU sketches:50 million (50M ), 100M , 200M , 500M , 1 billion(1B) and 2 billion (2B) counters with Exact SO.
Tocompute these scores, we count the number of indi-vidual words w1 and w2 and the pair (w1,w2) withina window of size 7.
Note that computing the exactcounts of all word pairs on these corpora is com-putationally expensive and memory intensive, so weconsider only those pairs in which one word appearsin the prototype list and the other word appears inthe test set.First, if we look at the Exact SO using PMI andLLR in Figure 2(a) and 2(b) respectively, it showsthat using PMI, we get about 6 points higher ac-curacy than LLR on this task (The 95% statisticalsignificance boundary for accuracy is about ?
1.5.
).Second, for both PMI and LLR, having more num-ber of counters improve performance.5 Using 2Bcounters, we get the same accuracy as Exact.5We use maximum of 2B counters (8GB main memory), asmost of the current desktop machines have at most 8GB RAM.2554.2.2 Effect of Increasing Corpus SizeWe evaluate SO of words on three different sizedcorpora (see Section 4.1): GW (9.8GB), GWB20(22.8GB), and GWB50 (49GB).
First, since for thistask using PMI performs better than LLR, so we willuse PMI for this experiment.
Second, we will fixnumber of counters to 2B (CU-2B) as it performsthe best in Section 4.2.1.
Third, we will compare theCU-2B counter model with the Exact over increas-ing corpus size.We can make several observations from the Fig-ure 3: ?
It shows that increasing the amount of dataimproves the accuracy of identifying the SO of aword.
We get an absolute increase of 5.5 points inaccuracy, when we add 20% Web data to GigaWord(GW).
Adding 30% more Web data (GWB50), givesa small increase of 1.3 points in accuracy which isnot even statistically significant.
?
Second, CU-2Bperforms as good as exact for all corpus sizes.
?Third, the number of 2B counters (bounded space)is less than the length of stream for GWB20 (6.05B), and GWB50 (13.2B).
Hence, it shows that usingcounters less than the stream length does not degradethe performance.
?
These results are also compara-ble to Turney?s (2003) state-of-the-art work wherethey report an accuracy of 82.84%.
Note, they use abillion word corpus which is larger than GWB50.0 10GB 20GB 30GB 40GB 50GB727476788082Corpus SizeAccuracyCU?2BExactFigure 3: Evaluating Semantic Orientation of words with Ex-act and CU counts with increase in corpus size4.3 Distributional SimilarityDistributional similarity is based on the distribu-tional hypothesis that similar terms appear in simi-lar contexts (Firth, 1968; Harris, 1954).
The contextvector for each term is represented by the strengthof association between the term and each of the lex-ical, semantic, syntactic, and/or dependency unitsthat co-occur with it6.
We use PMI and LLR to com-pute association score (AS) between the term andeach of the context to generate the context vector.Once, we have context vectors for each of the terms,cosine similarity measure returns distributional sim-ilarity between terms.4.3.1 Efficient Distributional SimilarityWe propose an efficient approach for computingdistributional similarity between word pairs usingCU sketch.
In the first step, we traverse the corpusand store counts of all words (except stop words) inhash table and all word pairs (except word pairs in-volving stop words) in sketch.
In the second step,for a target word ?x?, we consider all words (exceptinfrequent contexts which appear less than or equalto 10.)
as plausible context (since it is faster thantraversing the whole corpus.
), and query the sketchfor vocabulary number of word pairs, and computeapproximate AS between word-context pairs.
Wemaintain only top K AS scores7 contexts using pri-ority queue for every target word ?x?
and save themonto the disk.
In the third step, we use cosine simi-larity using these approximate topK context vectorsto compute efficient distributional similarity.The efficient distributional similarity usingsketches has following advantages:?
It can return semantic similarity between anyword pairs that are stored in the sketch.?
It can return the similarity between word pairsin time O(K).?
We do not store word pairs explicitly, and usefixed number of counters, hence the overallspace required is bounded.?
The additive property of sketch (Sec.
2.1.2) en-ables us to parallelize most of the steps in thealgorithm.
Thus it can be easily extended tovery large amounts of text data.We use two test sets which consist of word pairs,and their corresponding human rankings.
We gen-erate the word pair rankings using efficient distri-butional similarity.
We report the spearman?s rank6Here, the context for a target word ?x?
is defined as wordsappear within a window of size 7.7For this work, we use K = 1000 which is not tuned.256correlation8 coefficient (?)
between the human anddistributional similarity rankings.
The two test setsare:1.
WS-353 (Finkelstein et al, 2002) is a set of 353word pairs.2.
RG-65: (Rubenstein and Goodenough, 1965)is set of 65 word pairs.0 500M 1B 1.5B 2B0.10.150.20.25Model SizeAccuracyCUExact(a) Word Similarity PMI0 500M 1B 1.5B 2B0.40.450.50.55Model SizeAccuracyCUExact(b) Word Similarity LLRFigure 4: Evaluating Distributional Similarity between wordpairs on WS-353 test set using PMI and LLR with differentnumber of counters of CU sketch built using Gigaword data-set.4.3.2 Varying sketch sizeWe evaluate efficient distributional similarity be-tween between word pairs on WS-353 test set us-ing PMI and LLR association scores on Giga-word (9.8GB).
We compare different sizes of CUsketch (similar to SO evaluation): 50 million (50M ),100M , 200M , 500M , 1 billion (1B) and 2 bil-lion (2B) counters with the Exact word pair counts.Here again, computing the exact counts of all word-context pairs on these corpora is time, and memoryintensive, we generate context vectors for only thosewords which are present in the test set.First, if we look at word pair ranking using exactPMI and LLR across Figures 4(a) and 4(b) respec-tively, it shows that using LLR, we get better ?
of.55 compared to ?
of .25 using PMI on this task (The95% statistical significance boundary on ?
for WS-353 is about ?
.08).
The explanation for such a be-havior is: PMI rank context pairs with low frequencycounts higher (Church and Hanks, 1989) comparedto frequent ones which are favored by LLR.
Second,8To calculate the Spearman correlations values are trans-formed into ranks (if tied ranks exist, average of ranks is taken),and we calculate the Pearson correlation on them.Test Set WS-353 RG-65Model GW GWB20 GWB50 GW GWB20 GWB50Agirre .64 .75Exact .55 .55 .62 .65 .72 .74CU-2B .53 .58 .62 .66 .72 .74Table 2: Evaluating word pairs ranking with Exact andCU counts.
Scores are evaluated using ?
metric.for PMI in Fig.
4(a), having more counters does notimprove ?.
Third, for LLR in Fig.
4(b), having morenumber of counters improve performance and using2B counters, we get ?
close to the Exact.4.3.3 Effect of Increasing Corpus SizeWe evaluate efficient distributional similarity be-tween word pairs using three different sized cor-pora: GW (9.8GB), GWB20 (22.8GB), and GWB50(49GB) on two test sets: WS-353, and RG-65.
First,since for this task using LLR performs better thanPMI, so we will use LLR for this experiment.
Sec-ond, we will fix number of counters to 2B (CU-2B) as it performs the best in Section 4.2.1.
Third,we will compare the CU-2B counter model with theExact over increasing corpus size.
We also com-pare our results against the state-of-the-art results(Agirre) for distributional similarity (Agirre et al,2009).
We report their results of context window ofsize 7.We can make several observations from the Ta-ble 2: ?
It shows that increasing the amount ofdata is not substantially improving the accuracy ofword pair rankings over both the test sets.
?
Hereagain, CU-2B performs as good as exact for all cor-pus sizes.
?
CU-2B and Exact performs same as thestate-of-the-art system.
?
The number of 2B coun-ters (bounded space) is less than the length of streamfor GWB20 (6.05B ), and GWB50 (13.2B).
Hence,here again it shows that using counters less than thestream length does not degrade the performance.5 Dependency ParsingRecently, maximum spanning tree (MST) algo-rithms for dependency parsing (McDonald et al,2005) have shown great promise, primarily in su-pervised settings.
In the MST framework, words ina sentence form nodes in a graph, and connectionsbetween nodes indicate how ?related?
they are.
Amaximum spanning tree algorithm constructs a de-257pendency parse by linking together ?most similar?words.
Typically the weights on edges in the graphare parameterized as a linear function of features,with weight learned by some supervised learning al-gorithm.
In this section, we ask the question: canword association scores be used to derive syntacticstructures in an unsupervised manner?A first pass answer is: clearly not.
Metrics likePMI would assign high association scores to rareword pairs (mostly content words) leading to incor-rect parses.
Metrics like LLR would assign highassociation scores to frequent words, also leadingto incorrect parses.
However, with a small amountof linguistic side information (Druck et al, 2009;Naseem et al, 2010), we see that these issues canbe overcome.
In particular, we see that large data+ a little linguistics > fancy unsupervised learningalgorithms.5.1 Graph DefinitionOur approach is conceptually simple.
We constructa graph over nodes in the sentence with a unique?root?
node.
The graph is directed and fully con-nected, and for any two words in positions i and j,the weight from word i to word j is defined as:wij = ?ascasc(wi, wj)?
?distdist(i?j)+?lingling(ti, tj)Here, asc(wi, wj) is a association score such asPMI or LLR computed using approximate countsfrom the sketch.
Similarly, dist(i ?
j) is a simpleparameterized model of distances that favors shortdependencies.
We use a simple unnormalized (log)Laplacian prior of the form dist(i?j) = ?|i?j?1|,centered around 1 (encouraging short links to theright).
It is negated because we need to convert dis-tances to similarities.The final term, ling(ti, tj) asks: according tosome simple linguistic knowledge, how likely is ifthat the (gold standard) part of speech tag associatedwith word i points at that associated with word j?For this, we use the same linguistic informationused by (Naseem et al, 2010), which does notencode direction information.
These rules are:root?
{ aux, verb }; verb?
{ noun,pronoun, adverb, verb }; aux ?
{verb }; noun ?
{ adj, art, noun,num }; prep?
{ noun }; adj ?
{ advlen ?
10 len ?
20 allCOHEN-DIRICHLET 45.9 39.4 34.9COHEN-BEST 59.4 45.9 40.5ORACLE 75.1 66.6 63.0BASELINE+LING 42.4 33.8 29.7BASELINE 33.5 30.4 28.9CU-2B LLR OPTIMAL 62.4 ?
7.7 51.1 ?
3.2 41.1 ?
1.9CU-2B PMI OPTIMAL 63.3 ?
7.8 52.0 ?
3.2 41.1 ?
2.0CU-2B LLR BALANCED 49.1 ?
7.6 43.6 ?
3.3 37.2 ?
1.9CU-2B PMI BALANCED 49.5 ?
8.0 45.0 ?
3.2 38.3 ?
2.0CU-2B LLR SEMISUP 55.7 ?
0.0 44.1 ?
0.0 39.4 ?
0.0CU-2B PMI SEMISUP 56.5 ?
0.0 45.8 ?
0.0 39.9 ?
0.0Table 3: Comparing CU-2B build on GWB50 + a little lin-guistics v/s fancy unsupervised learning algorithms.}.
We simply give an additional weight of 1 to anyedge that agrees with one of these linguistic rules.5.2 Parameter SettingThe remaining issue is setting the interpolation pa-rameters ?
associated with each of these scores.This is a difficult problem in purely unsupervisedlearning.
We report results on three settings.
First,the OPTIMAL setting is based on grid search for op-timal parameters.
This is an oracle result based ongrid search over two of the three parameters (hold-ing the third fixed at 1).
In our second approach,BALANCED, we normalize the three components to?compete?
equally.
In particular, we scale and trans-late all three components to have zero mean and unitvariance, and set the ?s to all be equal to one.
Fi-nally, our third approach, SEMISUP, is based on us-ing a small amount of labeled data to set the param-eters.
In particular, we use 10 labeled sentences toselect parameters based on the same grid search asthe OPTIMAL setting.
Since this relies heavily onwhich 10 sentences are used, we repeat this experi-ment 20 times and report averages.5.3 ExperimentsOur experiments are on a dependency-converted ver-sion of section 23 of the Penn Treebank using mod-ified Collins?
head finding rules.
We measure accu-racies as directed, unlabeled dependency accuracy.We separately report results of sentences of lengthat most 10, at most 20 and finally of all length.
Notethat there is no training or cross-validation: we sim-ply run our MST parser on test data directly.The results of the parsing experiments are shown258in Table 3.
We compare against the following al-ternative systems.
The first, Cohen-Dirichlet andCohen-Best, are previously reported state-of-the-artresults for unsupervised Bayesian dependency pars-ing (Cohen and Smith, 2010).
The first is resultsusing a simple Dirichlet prior; the second is the bestreported results for any system from that paper.Next, we compare against an ?oracle?
system thatuses LLR extracted from the training data for thePenn Treebank, where the LLR is based on the prob-ability of observing an edge given two words.
Thisis not a true oracle in the sense that we might beable to do better, but it is unlikely.
The next twobaseline system are simple right branching base-line trees.
The Baseline system is a purely right-branching tree.
The Baseline+Ling system is onethat is right branching except that it can only createedges that are compatible with the linguistic rules,provided a relevant rule exists.
For short sentences,this is competitive with the Dirichlet prior results.Finally we report variants of our approach usingassociation scores computed on the GWB50 usingCU sketch with 2 billion counters.
We experimentwith two association scores: LLR and PMI.
For eachmeasure, we report results based on the three ap-proaches described earlier for setting the ?
hyper-parameters.
Error bars for our approaches are 95%confidence intervals based on bootstrap resampling.The results show that, for this task, PMI seemsslightly better than LLR, across the board.
The OP-TIMAL performance (based on tuning two hyperpa-rameters) is amazingly strong: clearly beating outall the baselines, and only about 15 points behindthe ORACLE system.
Using the BALANCED ap-proach causes a degradation of only 3 points fromthe OPTIMAL on sentences of all lengths.
In general,the balancing approach seems to be slightly worsethan the semi-supervised approach, except on veryshort sentences: for those, it is substantially better.Overall, though, the results for both Balanced andSemisup are competitive with state-of-the-art unsu-pervised learning algorithms.6 Discussion and ConclusionThe advantage of using sketch in addition to beingmemory and time efficient is that it contains countsfor all word pairs and hence can be used to com-pute association scores like PMI and LLR betweenany word pairs.
We show that using sketch counts inour experiments, on the three tasks, we get perfor-mance comparable to Exact word pair counts settingand state-of-the-art system.
Our method scales to 49GB of unzipped web data using bounded space of 2billion counters (8 GB memory).
Moreover, the lin-earity property of the sketch makes it scalable andusable in distributed setting.
Association scores andcounts from sketch can be used for more NLP taskslike small-space randomized language models, wordsense disambiguation, spelling correction, relationlearning, paraphrasing, and machine translation.AcknowledgmentsThe authors gratefully acknowledge the support ofNSF grant IIS-0712764 and Google Research Grantfor Large-Data NLP.
Thanks to Suresh Venkatasub-ramanian and Jagadeesh Jagarlamudi for useful dis-cussions and the anonymous reviewers for manyhelpful comments.ReferencesCharu C. Aggarwal and Philip S. Yu.
2010.
On classi-fication of high-cardinality data streams.
In SDM?10,pages 802?813.Eneko Agirre, Enrique Alfonseca, Keith Hall, JanaKravalova, Marius Pas?ca, and Aitor Soroa.
2009.
Astudy on similarity and relatedness using distributionaland wordnet-based approaches.
In NAACL ?09: Pro-ceedings of HLT-NAACL.Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.Och, and Jeffrey Dean.
2007.
Large language mod-els in machine translation.
In Proceedings of EMNLP-CoNLL.Moses Charikar, Kevin Chen, and Martin Farach-Colton.2004.
Finding frequent items in data streams.
Theor.Comput.
Sci., 312:3?15, January.K.
Church and P. Hanks.
1989.
Word Associa-tion Norms, Mutual Information and Lexicography.In Proceedings of ACL, pages 76?83, Vancouver,Canada, June.S.
B. Cohen and N. A. Smith.
2010.
Covariance in unsu-pervised learning of probabilistic grammars.
Journalof Machine Learning Research, 11:3017?3051.Graham Cormode and Marios Hadjieleftheriou.
2008.Finding frequent items in data streams.
In VLDB.Graham Cormode and S. Muthukrishnan.
2004.
An im-proved data stream summary: The count-min sketchand its applications.
J. Algorithms.259Gregory Druck, Gideon Mann, and Andrew McCal-lum.
2009.
Semi-supervised learning of dependencyparsers using generalized expectation criteria.
In Pro-ceedings of the Joint Conference of the 47th AnnualMeeting of the ACL and the 4th International JointConference on Natural Language Processing of theAFNLP: Volume 1 - Volume 1, ACL ?09, pages 360?368, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Cynthia Dwork, Moni Naor, Toniann Pitassi, Guy N.Rothblum, and Sergey Yekhanin.
2010.
Pan-privatestreaming algorithms.
In In Proceedings of ICS.Cristian Estan and George Varghese.
2002.
New di-rections in traffic measurement and accounting.
SIG-COMM Comput.
Commun.
Rev., 32(4).L.
Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,Z.
Solan, G. Wolfman, and E. Ruppin.
2002.
Plac-ing search in context: The concept revisited.
In ACMTransactions on Information Systems.J.
Firth.
1968.
A synopsis of linguistic theory 1930-1955.
In F. Palmer, editor, Selected Papers of J. R.Firth.
Longman.Amit Goyal, Hal Daume?
III, and Suresh Venkatasubra-manian.
2009.
Streaming for large scale NLP: Lan-guage modeling.
In NAACL.Amit Goyal, Jagadeesh Jagarlamudi, Hal Daume?
III, andSuresh Venkatasubramanian.
2010a.
Sketch tech-niques for scaling distributional similarity to the web.In GEMS workshop at ACL, Uppsala, Sweden.Amit Goyal, Jagadeesh Jagarlamudi, Hal Daume?
III, andSuresh Venkatasubramanian.
2010b.
Sketching tech-niques for Large Scale NLP.
In 6th WAC Workshop atNAACL-HLT.D.
Graff.
2003.
English Gigaword.
Linguistic Data Con-sortium, Philadelphia, PA, January.Z.
Harris.
1954.
Distributional structure.
Word 10 (23),pages 146?162.Abby Levenberg and Miles Osborne.
2009.
Stream-based randomised language models for SMT.
InEMNLP, August.Abby Levenberg, Chris Callison-Burch, and Miles Os-borne.
2010.
Stream-based translation models forstatistical machine translation.
In Human LanguageTechnologies: The 2010 Annual Conference of theNorth American Chapter of the Association for Com-putational Linguistics, HLT ?10, pages 394?402.
As-sociation for Computational Linguistics.Ping Li, Kenneth Ward Church, and Trevor Hastie.
2008.One sketch for all: Theory and application of condi-tional random sampling.
In Neural Information Pro-cessing Systems, pages 953?960.G.
S. Manku and R. Motwani.
2002.
Approximate fre-quency counts over data streams.
In VLDB.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic?.
2005.
Non-projective dependency parsingusing spanning tree algorithms.
In Proceedings of theconference on Human Language Technology and Em-pirical Methods in Natural Language Processing, HLT?05, pages 523?530, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.S.
Muthukrishnan.
2005.
Data streams: Algorithms andapplications.
Foundations and Trends in TheoreticalComputer Science, 1(2).Tahira Naseem, Harr Chen, Regina Barzilay, and MarkJohnson.
2010.
Using universal linguistic knowl-edge to guide grammar induction.
In Proceedings ofthe 2010 Conference on Empirical Methods in NaturalLanguage Processing, EMNLP ?10, pages 1234?1244.Association for Computational Linguistics.Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-Maria Popescu, and Vishnu Vyas.
2009.
Web-scaledistributional similarity and entity set expansion.
InProceedings of EMNLP.Deepak Ravichandran, Patrick Pantel, and Eduard Hovy.2005.
Randomized algorithms and nlp: using localitysensitive hash function for high speed noun clustering.In Proceedings of ACL.H.
Rubenstein and J.B. Goodenough.
1965.
Contextualcorrelates of synonymy.
Computational Linguistics,8:627?633.Florin Rusu and Alin Dobra.
2007.
Statistical analysis ofsketch estimators.
In SIGMOD ?07.
ACM.Stuart Schechter, Cormac Herley, and Michael Mitzen-macher.
2010.
Popularity is everything: a newapproach to protecting passwords from statistical-guessing attacks.
In Proceedings of the 5th USENIXconference on Hot topics in security, HotSec?10, pages1?8, Berkeley, CA, USA.
USENIX Association.Qinfeng Shi, James Petterson, Gideon Dror, John Lang-ford, Alex Smola, and S.V.N.
Vishwanathan.
2009.Hash kernels for structured data.
J. Mach.
Learn.
Res.,10:2615?2637, December.Philip J.
Stone, Dexter C. Dunphy, Marshall S. Smith,and Daniel M. Ogilvie.
1966.
The General Inquirer:A Computer Approach to Content Analysis.
MITPress.Peter D. Turney and Michael L. Littman.
2003.
Measur-ing praise and criticism: Inference of semantic orienta-tion from association.
ACM Trans.
Inf.
Syst., 21:315?346, October.Peter D. Turney.
2008.
A uniform approach to analogies,synonyms, antonyms, and associations.
In Proceed-ings of COLING 2008.Benjamin Van Durme and Ashwin Lall.
2009a.
Prob-abilistic counting with randomized storage.
In IJ-CAI?09: Proceedings of the 21st international jontconference on Artifical intelligence.260Benjamin Van Durme and Ashwin Lall.
2009b.
Stream-ing pointwise mutual information.
In Advances inNeural Information Processing Systems 22.Benjamin Van Durme and Ashwin Lall.
2010.
Onlinegeneration of locality sensitive hash signatures.
InProceedings of the ACL 2010 Conference Short Pa-pers, pages 231?235, July.261
