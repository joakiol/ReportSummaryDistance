Proceedings of the NAACL HLT Workshop on Active Learning for Natural Language Processing, pages 27?35,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsData Quality from Crowdsourcing:A Study of Annotation Selection CriteriaPei-Yun Hsueh, Prem Melville, Vikas SindhwaniIBM T.J. Watson Research Center1101 Kitchawan Road, Route 134Yorktown Heights, NY 10598, USAAbstractAnnotation acquisition is an essential step intraining supervised classifiers.
However, man-ual annotation is often time-consuming andexpensive.
The possibility of recruiting anno-tators through Internet services (e.g., AmazonMechanic Turk) is an appealing option that al-lows multiple labeling tasks to be outsourcedin bulk, typically with low overall costs andfast completion rates.
In this paper, we con-sider the difficult problem of classifying sen-timent in political blog snippets.
Annotationdata from both expert annotators in a researchlab and non-expert annotators recruited fromthe Internet are examined.
Three selection cri-teria are identified to select high-quality anno-tations: noise level, sentiment ambiguity, andlexical uncertainty.
Analysis confirm the util-ity of these criteria on improving data quality.We conduct an empirical study to examine theeffect of noisy annotations on the performanceof sentiment classification models, and evalu-ate the utility of annotation selection on clas-sification accuracy and efficiency.1 IntroductionCrowdsourcing (Howe, 2008) is an attractive solu-tion to the problem of cheaply and quickly acquir-ing annotations for the purposes of constructing allkinds of predictive models.
To sense the potential ofcrowdsourcing, consider an observation in von Ahnet al (2004): a crowd of 5,000 people playing anappropriately designed computer game 24 hours aday, could be made to label all images on Google(425,000,000 images in 2005) in a matter of just 31days.
Several recent papers have studied the useof annotations obtained from Amazon MechanicalTurk, a marketplace for recruiting online workers(Su et al, 2007; Kaisser et al, 2008; Kittur et al,2008; Sheng et al, 2008; Snow et al, 2008; Sorokinand Forsyth, 2008).With efficiency and cost-effectiveness, online re-cruitment of anonymous annotators brings a new setof issues to the table.
These workers are not usuallyspecifically trained for annotation, and might not behighly invested in producing good-quality annota-tions.
Consequently, the obtained annotations maybe noisy by nature, and might require additional val-idation or scrutiny.
Several interesting questions im-mediately arise in how to optimally utilize annota-tions in this setting: How does one handle differ-ences among workers in terms of the quality of an-notations they provide?
How useful are noisy anno-tations for the end task of creating a model?
Is it pos-sible to identify genuinely ambiguous examples viaannotator disagreements?
How should these consid-erations be treated with respect to intrinsic informa-tiveness of examples?
These questions also hint at astrong connection to active learning, with annotationquality as a new dimension to the problem.As a challenging empirical testbed for these is-sues, we consider the problem of sentiment classi-fication on political blogs.
Given a snippet drawnfrom a political blog post, the desired output is apolarity score that indicates whether the sentimentexpressed is positive or negative.
Such an analysisprovides a view of the opinion around a subject ofinterest, e.g., US Presidential candidates, aggregatedacross the blogsphere.
Recently, sentiment analy-27sis is emerging as a critical methodology for socialmedia analytics.
Previous research has focused onclassifying subjective-versus-objective expressions(Wiebe et al, 2004), and also on accurate sentimentpolarity assignment (Turney, 2002; Yi et al, 2003;Pang and Lee, 2004; Sindhwani and Melville, 2008;Melville et al, 2009).The success of most prior work relies on the qual-ity of their knowledge bases; either lexicons defin-ing the sentiment polarity of words around a topic(Yi et al, 2003), or quality annotation data for sta-tistical training.
While manual intervention for com-piling lexicons has been significantly lessened bybootstrapping techniques (Yu and Hatzivassiloglou,2003; Wiebe and Riloff, 2005), manual interventionin the annotation process is harder to avoid.
More-over, the task of annotating blog-post snippets ischallenging, particularly in a charged political at-mosphere with complex discourse spanning manyissues, use of cynicism and sarcasm, and highlydomain-specific and contextual cues.
The downsideis that high-performance models are generally dif-ficult to construct, but the upside is that annotationand data-quality issues are more clearly exposed.In this paper we aim to provide an empirical basisfor the use of data selection criteria in the contextof sentiment analysis in political blogs.
Specifically,we highlight the need for a set of criteria that can beapplied to screen untrustworthy annotators and se-lect informative yet unambiguous examples for theend goal of predictive modeling.
In Section 2, wefirst examine annotation data obtained by both theexpert and non-expert annotators to quantify the im-pact of including non-experts.
Then, in Section 3,we quantify criteria that can be used to select anno-tators and examples for selective sampling.
Next, inSection 4, we address the questions of whether thenoisy annotations are still useful for this task andstudy the effect of the different selection criteria onthe performance of this task.
Finally, in Section 5we present conclusion and future work.2 Annotating Blog SentimentThis section introduces the Political Blog Snippet(PBS) corpus, describes our annotation procedureand the sources of noise, and gives an overview ofthe experiments on political snippet sentiments.2.1 The Political Blog Snippet CorpusOur dataset comprises of a collection of snippets ex-tracted from over 500,000 blog posts, spanning theactivity of 16,741 political bloggers in the time pe-riod of Aug 15, 2008 to the election day Nov 4,2008.
A snippet was defined as a window of textcontaining four consecutive sentences such that thehead sentence contained either the term ?Obama?or the term ?McCain?, but both candidates werenot mentioned in the same window.
The globaldiscourse structure of a typical political blog postcan be highly complicated with latent topics rangingfrom policies (e.g., financial situation, economics,the Iraq war) to personalities to voting preferences.We therefore expected sentiment to be highly non-uniform over a blog post.
This snippetization proce-dure attempts to localize the text around a presiden-tial candidate with the objective of better estimat-ing aggregate sentiment around them.
In all, we ex-tracted 631,224 snippets.
For learning classifiers, wepassed the snippets through a stopword filter, prunedall words that occur in less than 3 snippets and cre-ated normalized term-frequency feature vectors overa vocabulary of 3,812 words.2.2 Annotation ProcedureThe annotation process consists of two steps:Sentiment-class annotation: In the first step, aswe are only interested in detecting sentiments re-lated to the named candidate, the annotators werefirst asked to mark up the snippets irrelevant to thenamed candidate?s election campaign.
Then, the an-notators were instructed to tag each relevant snippetwith one of the following four sentiment polarity la-bels: Positive, Negative, Both, or Neutral.Alignment annotation: In the second step, theannotators were instructed to mark up whether eachsnippet was written to support or oppose the targetcandidate therein named.
The motivation of addingthis tag comes from our interest in building a classi-fication system to detect positive and negative men-tions of each candidate.
For the snippets that donot contain a clear political alignment, the annota-tors had the freedom to mark it as neutral or simplynot alignment-revealing.In our pilot study many bloggers were observedto endorse a named candidate by using negative ex-28pressions to denounce his opponent.
Therefore, inour annotation procedure, the distinction is madebetween the coding of manifest content, i.e., sen-timents ?on the surface?, and latent political align-ment under these surface elements.2.3 Agreement StudyIn this section, we compare the annotations obtainedfrom the on-site expert annotators and those from thenon-expert AMT annotators.2.3.1 Expert (On-site) AnnotationTo assess the reliability of the sentiment annota-tion procedure, we conducted an agreement studywith three expert annotators in our site, using 36snippets randomly chosen from the PBS Corpus.Overall agreement among the three annotators onthe relevance of snippets is 77.8%.
Overall agree-ment on the four-class sentiment codings is 70.4%.Analysis indicate that the annotators agreed betteron some codings than the others.
For the task ofdetermining whether a snippet is subjective or not1,the annotators agreed 86.1% of the time.
For thetask of determining whether a snippet is positive ornegative, they agreed 94.9% of the time.To examine which pair of codings is the most dif-ficult to distinguish, Table 1 summarizes the confu-sion matrix for the three pairs of annotator?s judge-ments on sentiment codings.
Each column describesthe marginal probability of a coding and the prob-ability distribution for this coding being recognizedas another coding (including itself).
As many blog-gers use cynical expressions in their writings, themost confusing cases occur when the annotatorshave to determine whether a snippet is ?negative?or ?neutral?.
The effect of cynical expressions on% Neu Pos Both NegMarginal 21.9 20.0 10.5 47.6Neutral (Neu) 47.8 14.3 9.1 16.0Positive (Pos) 13.0 61.9 18.2 6.0Both (Both) 4.4 9.5 9.1 14.0Negative (Neg) 34.8 14.3 63.6 64.0Table 1: Summary matrix for the three on-site annotators?sentiment codings.1This is done by grouping the codings of Positive, Negative,and Both into the subjective class.sentiment analysis in the political domain is also re-vealed in the second step of alignment annotation.Only 42.5% of the snippets have been coded withalignment coding in the same direction as its senti-ment coding ?
i.e., if a snippet is intended to support(oppose) a target candidate, it will contain positive(negative) sentiment.
The alignment coding task hasbeen shown to be reliable, with the annotators agree-ing 76.8% of the time overall on the three-level cod-ings: Support/Against/Neutral.2.3.2 Amazon Mechanical Turk AnnotationTo compare the annotation reliability betweenexpert and non-expert annotators, we further con-ducted an agreement study with the annotators re-cruited from Amazon Mechanical Turk (AMT).
Wehave collected 1,000 snippets overnight, with thecost of 4 cents per annotation.In the agreement study, a subset of 100 snippetsis used, and each snippet is annotated by five AMTannotators.
These annotations were completed by25 annotators whom were selected based on the ap-proval rate of their previous AMT tasks (over 95%of times).2 The AMT annotators spent on average40 seconds per snippet, shorter than the average oftwo minutes reported by the on-site annotators.
Thelower overall agreement on all four-class sentimentcodings, 35.3%, conforms to the expectation that thenon-expert annotators are less reliable.
The Turk an-notators also agreed less on the three-level alignmentcodings, achieving only 47.2% of agreement.However, a finer-grained analysis reveals that theystill agree well on some codings: The overall agree-ment on whether a snippet is relevant, whether asnippet is subjective or not, and whether a snippetis positive or negative remain within a reasonablerange: 81.0%, 81.8% and 61.9% respectively.2.4 Gold StandardWe defined the gold standard (GS) label of a snip-pet in terms of the coding that receives the major-ity votes.3 Column 1 in Table 2 (onsite-GS predic-2Note that we do not enforce these snippets to be annotatedby the same group of annotators.
However, Kappa statisticsrequires to compute the chance agreement of each annotator.Due to the violation of this assumption, we do not measure theintercoder agreement with Kappa in this agreement study.3In this study, we excluded 6 snippets whose annotationsfailed to reach majority vote by the three onsite annotators.29onsite-GS prediction onsite agreement AMT-GS prediction AMT agreementSentiment (4-class) 0.767 0.704 0.614 0.353Alignment (3-level) 0.884 0.768 0.669 0.472Relevant or not 0.889 0.778 0.893 0.810Subjective or not 0.931 0.861 0.898 0.818Positive or negative 0.974 0.949 0.714 0.619Table 2: Average prediction accuracy on gold standard (GS) using one-coder strategy and inter-coder agreement.tion) shows the ratio of the onsite expert annotationsthat are consistent with the gold standard, and Col-umn 3 (AMT-GS prediction) shows the same for theAMT annotations.
The level of consistency, i.e., thepercentage agreement with the gold standard labels,can be viewed as a proxy of the quality of the an-notations.
Among the AMT annotations, Columns2 (onsite agreement) and 4 (AMT agreement) showthe pair-wise intercoder agreement in the on-site ex-pert and AMT annotations respectively.The results suggest that it is possible to take onesingle expert annotator?s coding as the gold standardin a number of annotation tasks using binary clas-sification.
For example, there is a 97.4% chancethat one expert?s coding on the polarity of a snip-pet, i.e., whether it is positive or negative, will beconsistent with the gold standard coding.
However,this one-annotator strategy is less reliable with theintroduction of non-expert annotators.
Take the taskof polarity annotation as an example, the intercoderagreement among the AMT workers goes down to61.9% and the ?one-coder?
strategy can only yield71.4% accuracy.
To determine reliable gold stan-dard codings, multiple annotators are still necessarywhen non-expert annotators are recruited.3 Annotation Quality MeasuresGiven the noisy AMT annotations, in this section wediscuss some summary statistics that are needed tocontrol the quality of annotations.3.1 Annotator-level noiseTo study the question of whether there exists a groupof annotators who tend to yield more noisy annota-tions, we evaluate the accumulated noise level intro-duced by each of the annotators.
We define the noiselevel as the deviation from the gold standard labels.Similar to the measure of individual error rates pro-posed in (Dawid and Skene, 1979), the noise level ofa particular annotator j, i.e., noise(annoj), is thenestimated by summing up the deviation of the an-notations received from this annotator, with a smallsampling correction for chance disagreement.
Anal-ysis results demonstrate that there does exist a subsetof annotators who yield more noisy annotations thanthe others.
20% of the annotators (who exceed thenoise level 60%) result in annotations that have 70%disagreement with the gold standard.In addition, we also evaluate how inclusion ofnoisy annotators reduces the mean agreement withGold Standard.
The plot (left) in Figure 1 plots themean agreement rate with GS over the subset of an-notators that pass a noise threshold.
These resultsshow that the data quality decreases with the inclu-sion of more untrustworthy annotators.3.2 Snippet-level sentiment ambiguityWe have observed that not all snippets are equallyeasy to annotate, with some containing more am-biguous expressions.
To incorporate this concern inthe selection process, a key question to be answeredis whether there exist snippets whose sentiment issubstantially less distinguishable than the others.We address this question by quantifying ambigu-ity measures with the two key properties shown asimportant in evaluating the controversiality of anno-tation snippets (Carenini and Cheung, 2008): (1) thestrength of the annotators?
judgements and (2) thepolarity of the annotations.
The measurement needsto satisfy the constraints demonstrated in the follow-ing snippets: (1) An example that has received threepositive codings are more ambiguous than that hasreceived five, and (2) an example that has receivedfive positive codings is more ambiguous than the onethat has received four positive and one negative cod-ing.
In addition, as some snippets were shown to30Annotator noise levelPrediction Accuracy0.0 0.2 0.4 0.6 0.80.00.20.40.60.81.0Annotator Noise0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Sentiment AmbigityLexical UncertaintyPrediction Accuracy0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Lexical UncertaintyFigure 1: Data quality (consistency with GS) as a function of noise level (left), sentiment ambiguity (middle), andlexical uncertainty (right).be difficult to tell whether they contain negative orneutral sentiment, the measure of example ambigu-ity has to go beyond controversiality and incorporatecodings of ?neutral?
and ?both?.To satisfy these constraints, we first enumeratedthrough the codings of each snippet and countedthe number of neutral, positive, both, and negativecodings: We added (1) one to the positive (nega-tive) category for each positive (negative) coding,(2) 0.5 to the neutral category with each neutral cod-ing, and (3) 0.5 to both the positive and negativecategories with each both coding.
The strength ofcodings in the three categories, i.e., str+(snipi),strneu(snipi), and str?
(snipi), were then summedup into str(snipi).
The distribution were parame-terized with?+(snipi) = str+(snipi)/str(snipi)?neu(snipi) = strneu(snipi)/str(snipi)??
(snipi) = str?
(snipi)/str(snipi)We then quantify the level of ambiguity in the an-notator?s judgement as follows:H(?
(snipi)) = ??+(snipi)log(?+(snipi))??neu(snipi)log(?neu(snipi))???(snipi)log(??
(snipi))Amb(snipi) = str(snipi)strmax ?H(?
(snipi)),where strmax is the maximum value of str amongall the snippets in the collection.
The plot (middle)in Figure 1 shows that with the inclusion of snip-pets that are more ambiguous in sentiment disam-biguation, the mean agreement with Gold Standarddecreases as expected.3.3 Combining measures on multipleannotationsHaving established the impact of noise and senti-ment ambiguity on annotation quality, we then setout to explore how to integrate them for selection.First, the ambiguity scores for each of the snippetsare reweighed with respect to the noise level.w(snipi) =?jnoise(annoj)?
(1e )?
(ij)Conf(snipi) = w(snipi)?iw(snipi)?Amb(snipi),where ?
(ij) is an indicator function of whether acoding of snipi from annotator j agrees with its goldstandard coding.
w(expi) is thus computed as theaggregated noise level of all the annotators who la-beled the ith snippet.To understand the baseline performance of the se-lection procedure, we evaluate the the true predic-tions versus the false alarms resulting from usingeach of the quality measures separately to select an-notations for label predictions.
In this context, a trueprediction occurs when an annotation suggested byour measure as high-quality indeed matches the GSlabel, and a false alarm occurs when a high qualityannotation suggested by our measure does not matchthe GS label.
The ROC (receiver operating charac-teristics) curves in Figure 2 reflect all the potentialoperating points with the different measures.We used data from 2,895 AMT annotations on579 snippets, including 63 snippets used in theagreement study.
This dataset is obtained by filter-ing out the snippets with their GS labels as 1 (?ir-relevant?)
and the snippets that do not receive anycoding that has more than two votes.310.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0False Alarm RateTrue Prediction Ratellllllllll0.1.20.30.40.50.60.70.80.91(a) Match Prediction Before Removing Divisive Snippets1?confusion1?ambiguity1?noise0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0False Alarm RateTrue Prediction Ratellllllllll0.10.20.30.40.50.60.70.80.91(b) Match Prediction After Removing Divisive Snippets1?confusion(all4codings)1?confusion(pos/neg)1?ambiguity(all4codings)1?ambiguity(pos/neg)1?noiseFigure 2: Modified ROC curves for quality measures: (a) before removing divisive snippets, (b) after removing divisivesnippets.
The numbers shown with the ROC curve are the values of the aggregated quality measure (1-confusion).Initially, three quality measures are tested: 1-noise, 1-ambiguity, 1-confusion.
Examination of thesnippet-level sentiment codings reveals that somesnippets (12%) result in ?divisive?
codings, i.e.,equal number of votes on two codings.The ROC curves in Figure 2 (a) plot the base-line performance of the different quality measures.Results show that before removing the subset of di-visive snippets, the only effective selection criteriais obtained by monitoring the noise level of anno-tators.
Figure 2 (b) plots the performance after re-moving the divisive snippets.
In addition, our am-biguity scores are computed under two settings: (1)with only the polar codings (pos/neg), and (2) withall the four codings (all4codings).
The ROC curvesreveal that analyzing only the polar codings is notsufficient for annotation selection.The results also demonstrate that confusion, an in-tegrated measure, does perform best.
Confusion isjust one way of combining these measures.
One maychose alternative combinations ?
the results here pri-marily illustrate the benefit of considering these dif-ferent dimensions in tandem.
Moreover, the differ-ence between plot (a) and (b) suggests that removingdivisive snippets is essential for the quality measuresto work well.
How to automatically identify the di-visive snippets is therefore important to the successof the annotation selection process.3.4 Effect of lexical uncertainty on divisivesnippet detectionIn search of measures that can help identify the di-visive snippets automatically, we consider the inher-ent lexical uncertainty of an example.
UncertaintySampling (Lewis and Catlett, 1994) is one commonheuristic for the selection of informative instances,which select instances that the current classifier ismost uncertain about.
Following on these lines wemeasure the uncertainty on instances, with the as-sumption that the most uncertain snippets are likelyto be divisive.In particular, we applied a lexical sentiment clas-sifier (c.f.
Section 4.1.1) to estimate the likelihood ofan unseen snippet being of positive or negative sen-timent, i.e., P+(expi), P?
(expi), by counting thesentiment-indicative word occurrences in the snip-pet.
As in our dataset the negative snippets far ex-ceed the positive ones, we also take the prior proba-bility into account to avoid class bias.
We then mea-sure lexical uncertainty as follows.Deviation(snipi) =1C ?
|(log(P (+))?log(P (?)))+(log(P+(snipi))?log(P?
(snipi)))|,Uncertainty(snipi) =1?Deviation(snipi),where class priors, P (+) and P (?
), are estimatedwith the dataset used in the agreement studies, andC is the normalization constant.We then examine not only the utility of lexical un-certainty in identifying high-quality annotations, but32Classifier Accuracy AUCLC 49.60 0.614NB 83.53 0.653SVM 83.89 0.647Pooling 84.51 0.700Table 3: Accuracy of sentiment classification methods.also the utility of such measure in identifying divi-sive snippets.
Figure 1 (right) shows the effect oflexical uncertainty on filtering out low-quality anno-tations.
Figure 3 demonstrates the effect of lexicaluncertainty on divisive snippet detection, suggestingthe potential use of lexical uncertainty measures inthe selection process.Lexical UncertaintyDivisiveSnippet DetectionAccuracy0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Figure 3: Divisive snippet detection accuracy as a func-tion of lexical uncertainty.4 Empirical EvaluationThe analysis in Sec.
3 raises two important ques-tions: (1) how useful are noisy annotations for sen-timent analysis, and (2) what is the effect of onlineannotation selection on improving sentiment polar-ity classification?4.1 Polarity Classifier with Noisy AnnotationsTo answer the first question raised above, we trainclassifiers based on the noisy AMT annotations toclassify positive and negative snippets.
Four dif-ferent types of classifiers are used: SVMs, NaiveBayes (NB), a lexical classifier (LC), and the lexi-cal knowledge-enhanced Pooling Multinomial clas-sifier, described below.4.1.1 Lexical ClassifierIn the absence of any labeled data in a domain,one can build sentiment-classification models thatrely solely on background knowledge, such as a lex-icon defining the polarity of words.
Given a lexi-con of positive and negative terms, one straightfor-ward approach to using this information is to mea-sure the frequency of occurrence of these terms ineach document.
The probability that a test documentbelongs to the positive class can then be computedas P (+|D) = aa+b ; where a and b are the numberof occurrences of positive and negative terms in thedocument respectively.
A document is then classi-fied as positive if P (+|D) > P (?|D); otherwise,the document is classified as negative.
For this study,we used a lexicon of 1,267 positive and 1,701 nega-tive terms, as labeled by human experts.4.1.2 Pooling MultinomialsThe Pooling Multinomials classifier was intro-duced by the authors as an approach to incorpo-rate prior lexical knowledge into supervised learn-ing for better text classification.
In the contextof sentiment analysis, such lexical knowledge isavailable in terms of the prior sentiment-polarity ofwords.
Pooling Multinomials classifies unlabeledexamples just as in multinomial Na?
?ve Bayes clas-sification (McCallum and Nigam, 1998), by predict-ing the class with the maximum likelihood, given byargmaxcjP (cj)?i P (wi|cj); where P (cj) is theprior probability of class cj , and P (wi|cj) is theprobability of word wi appearing in a snippet ofclass cj .
In the absence of background knowledgeabout the class distribution, we estimate the classpriors P (cj) solely from the training data.
However,unlike regular Na?
?ve Bayes, the conditional prob-abilities P (wi|cj) are computed using both the la-beled examples and the lexicon of labeled features.Given two models built using labeled examples andlabeled features, the multinomial parameters of suchmodels can be aggregated through a convex combi-nation, P (wi|cj) = ?Pe(wi|cj)+(1??
)Pf (wi|cj);where Pe(wi|cj) and Pf (wi|cj) represent the proba-bility assigned by using the example labels and fea-ture labels respectively, and ?
is the weight for com-bining these distributions.
The weight indicates alevel of confidence in each source of information,and can be computed based on the training set accu-racy of the two components.
The derivation and de-tails of these models are not directly relevant to thispaper, but can be found in (Melville et al, 2009).33Q1 Q2 Q3 Q4Accuracy AUC Accuracy AUC Accuracy AUC Accuracy AUCNoise 84.62% 0.688 74.36% 0.588 74.36% 0.512 79.49% 0.441Ambiguity 84.21% 0.715 78.95% 0.618 68.42% 0.624 84.21% 0.691Confusion 82.50% 0.831 82.50% 0.762 80.00% 0.814 80.00% 0.645Table 4: Effect of annotation selection on classification accuracy.4.1.3 Results on Polarity ClassificationWe generated a data set of 504 snippets that had3 or more labels for either the positive or negativeclass.
We compare the different classification ap-proaches using 10-fold cross-validation and presentour results in Table 3.
Results show that the Pool-ing Multinomial classifier, which makes predictionsbased on both the prior lexical knowledge and thetraining data, can learn the most from the labeleddata to classify sentiments of the political blog snip-pets.
We observe that despite the significant levelof noise and ambiguity in the training data, usingmajority-labeled data for training still results in clas-sifiers with reasonable accuracy.4.2 Effect of Annotation SelectionWe then evaluate the utility of the quality measuresin a randomly split dataset (with 7.5% of the data inthe test set).
We applied each of the measures to rankthe annotation examples and then divide them into4 equal-sized training sets based on their rankings.For example, Noise-Q1 contains only the least noisyquarter of annotations and Q4 the most noisy ones.Results in Table 4 demonstrate that the classi-fication performance declines with the decrease ofeach quality measure in general, despite exceptionsin the subset with the highest sentiment ambiguity(Ambiguity-Q4), the most noisy subset Q4 (Noise-Q4), and the subset yielding less overall confusion(Confusion-Q2).
The results also reveal the benefitsof annotation selection on efficiency: using the sub-set of annotations predicted in the top quality quar-ter achieves similar performance as using the wholetraining set.
These preliminary results suggest thatan active learning scheme which considers all threequality measures may indeed be effective in improv-ing label quality and subsequent classification accu-racy.5 ConclusionIn this paper, we have analyzed the difference be-tween expert and non-expert annotators in terms ofannotation quality, and showed that having a singlenon-expert annotator is detrimental for annotatingsentiment in political snippets.
However, we con-firmed that using multiple noisy annotations fromdifferent non-experts can still be very useful formodeling.
This finding is consistent with the sim-ulated results reported in (Sheng et al, 2008).
Giventhe availability of many non-expert annotators on-demand, we studied three important dimensions toconsider when acquiring annotations: (1) the noiselevel of an annotator compared to others, (2) the in-herent ambiguity of an example?s class label, and(3) the informativeness of an example to the currentclassification model.
While the first measure hasbeen studied with annotations obtained from experts(Dawid and Skene, 1979; Clemen and Reilly, 1999),the applicability of their findings on non-expert an-notation selection has not been examined.We showed how quality of labels can be improvedby eliminating noisy annotators and ambiguous ex-amples.
Furthermore, we demonstrated the qualitymeasures are useful for selecting annotations thatlead to more accurate classification models.
Our re-sults suggest that a good active learning or onlinelearning scheme in this setting should really con-sider all three dimensions.
The way we use to in-tegrate the different dimensions now is still prelimi-nary.
Also, our empirical findings suggest that someof the dimensions may have to be considered sepa-rately.
For example, due to the divisive tendency ofthe most informative examples, these examples mayhave to be disregarded in the initial stage of anno-tation selection.
Also, the way we use to combinethese measures is still preliminary.
The design andtesting of such schemes are avenues for future work.34ReferencesGiuseppe Carenini and Jackie C. K. Cheung.
2008.
Ex-tractive vs. NLG-based abstractive summarization ofevaluative text: The effect of corpus controversiality.In Proceedings of the Fifth International Natural Lan-guage Generation Conference.R.T.
Clemen and T. Reilly.
1999.
Correlations and cop-ulas for decision and risk analysis.
Management Sci-ence, 45:208?224.A.
P. Dawid and A.
.M.
Skene.
1979.
Maximum likli-hood estimation of observer error-rates using the emalgorithm.
Applied Statistics, 28(1):20?28.Jeff Howe.
2008.
Crowdsourcing: Why the Power ofthe Crowd Is Driving the Future of Business.
CrownBusiness, 1 edition, August.Michael Kaisser, Marti Hearst, and John B. Lowe.
2008.Evidence for varying search results summary lengths.In Proceedings of ACL 2008.Aniket Kittur, Ed H. Chi, and Bongwon Suh.
2008.Crowdsourcing user studies with mechanical turk.
InProceedings of CHI 2008.David D. Lewis and Jason Catlett.
1994.
Heterogeneousuncertainty sampling for supervised learning.
pages148?156, San Francisco, CA, July.Andrew McCallum and Kamal Nigam.
1998.
A com-parison of event models for naive Bayes text classifi-cation.
In AAAI Workshop on Text Categorization.Prem Melville, Wojciech Gryc, and Richard Lawrence.2009.
Sentiment analysis of blogs by combining lexi-cal knowledge with text classification.
In KDD.Bo Pang and Lillian Lee.
2004.
A sentiment education:Sentiment analysis using subjectivity summarizationbased on minimum cuts.
In Proceedings of ACL 2004.Victor Sheng, Foster Provost, and G. Panagiotis Ipeiro-tis.
2008.
Get another label?
Improving data qualityand data mining using multiple, noisy labelers.
In Pro-ceeding of KDD 2008, pages 614?622.Vikas Sindhwani and Prem Melville.
2008.
Document-word co-regularization for semi-supervised sentimentanalysis.
In Proceedings of IEEE International Con-ference on Data Mining (ICDM), pages 1025?1030,Los Alamitos, CA, USA.
IEEE Computer Society.R.
Snow, B. O?Connor, D. Jurafsky, and A. Ng.
2008.Cheap and fast?but is it good?
evaluating non-expertannotations for natural language tasks.
In Proceedingsof EMNLP 2008.Alexander Sorokin and David Forsyth.
2008.
Utilitydata annotation via amazon mechanical turk.
In IEEEWorkshop on Internet Vision at CVPR 08.Qi Su, Dmitry Pavlov, Jyh-Herng Chow, and WendellC.Baker.
2007.
Internet-scale collection of human-reviewed data.
In Proceedings of WWW 2007.Peter D. Turney.
2002.
Thumbs up or thumbs down:Semantic orientation applied to unsupervised classifi-cation of reviews.
In Proceedings of ACL 2002.Luis von Ahn and Laura Dabbish.
2004.
Labeling im-ages with a computer game.
In Proceedings of CHI2004, pages 319?326.Janyce Wiebe and E. Riloff.
2005.
Creating subjec-tive and objective sentence classifiers from unanno-tated texts.
In Proceedings of CICLing 2005.Janyce Wiebe, Theresa Wilson, Rebecca Bruce, MatthewBell, and Melanie Martin.
2004.
Learning subjectivelanguage.
Computational Linguistics, 30 (3).Jeonghee Yi, Tetsuya Nasukawa, Razvan Bunescu, andWayne Niblack.
2003.
Sentiment analyzer: Extract-ing sentiments about a given topic using natural lan-guage processing technique.
In Proceedings of theInternational Conference on Data Mining (ICDM),pages 427?434.Hong Yu and Vasileios Hatzivassiloglou.
2003.
Towardsanswering opinion questions: Separating facts fromopinions and identifying the polarity of opinion sen-tences.
In Proceedings of EMNLP 2003.35
