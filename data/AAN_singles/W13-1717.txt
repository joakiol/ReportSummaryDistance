Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 134?139,Atlanta, Georgia, June 13 2013. c?2013 Association for Computational LinguisticsNAIST at the NLI 2013 Shared TaskTomoya Mizumoto, Yuta HayashibeKeisuke Sakaguchi, Mamoru Komachi, Yuji MatsumotoGraduate School of Information ScienceNara Institute of Science and Technology8916-5, Takayama, Ikoma, Nara 630-0192, Japan{ tomoya-m, yuta-h, keisuke-sa, komachi, matsu }@is.naist.jpAbstractThis paper describes the Nara Institute ofScience and Technology (NAIST) native lan-guage identification (NLI) system in the NLI2013 Shared Task.
We apply feature selec-tion using a measure based on frequency forthe closed track and try Capping and Samplingdata methods for the open tracks.
Our systemranked ninth in the closed track, third in opentrack 1 and fourth in open track 2.1 IntroductionThere have been many studies using English as asecond language (ESL) learner corpora.
For exam-ple, automatic grammatical error detection and cor-rection is one of the most active research areas in thisfield.
More recently, attention has been paid to na-tive language identification (NLI) (Brooke and Hirst,2012; Bykh and Meurers, 2012; Brooke and Hirst,2011; Wong and Dras, 2011; Wong et al 2011).Native language identification is the task of identi-fying the ESL learner?s L1 given a learner?s essay.The NLI Shared Task 2013 (Tetreault et al 2013)is the first shared task on NLI using the com-mon dataset ?TOEFL-11?
(Blanchard et al 2013;Tetreault et al 2012).
TOEFL-11 consists of essayswritten by learners of 11 native languages (Arabic,Chinese, French, German, Hindi, Italian, Japanese,Koran, Spanish, Telugu, Turkish), and it contains1,100 essays for each native language.
In addition,the essay topics are balanced, and the number of top-ics is 8.In the closed track, we tackle feature selectionfor increasing accuracy.
We use a feature selectionmethod based on the frequency of each feature (e.g.,document frequency, TF-IDF).In the open tracks, to address the problem of im-balanced data, we tried two approaches: Cappingand Sampling data in order to balance the size oftraining data.In this paper, we describe our system and exper-imental results.
Section 2 describes the features weused in the system for NLI.
Section 3 and Section 4describe the systems for closed track and open trackin NLI Shared Task 2013.
Section 5 describes the re-sults for NLI Shared Task 2013.
Section 6 describesthe experimental result for 10-fold cross validationon the data set used by Tetreault et al(2012).2 Features used in all tracksIn this section, we describe the features in our sys-tems.
We formulate NLI as a multiclass classifica-tion task.
Following previous work, we use LIB-LINEAR 2 for the classification tool and tune the Cparameter using grid-search.We select the features based on previous work(Brooke and Hirst, 2012; Tetreault et al 2012).
Allfeatures used are binary.
We treated the features asshown in Table 1.
The example of features in Table1 shows the case whose input is ?I think not a reallydifficult question?.We use a special symbol for the beginning andend of sentence (or word) for bigrams and trigrams.For surface forms, we lowercased all words.
POS,POS-function and dependency features are extracted1http://www.lextek.com/manuals/onix/stopwords1.html2http://www.csie.ntu.edu.tw/?cjlin/liblinear/134Name Description ExampleWord N-gram (N=1,2) Surface form of the word.
N=1 i, think, notN=2 BOS i, i thinkPOS N-gram (N=2,3) POS tags of the word.
N=2 BOS PRP, PRP VBPN=3 BOS PRP VBP, PRP VBP RBCharacter N-gram (N=2,3) N=2 ?
t, t h, hi, in, nk, k$N=3 ?
t h, t h iPOS-function N-gram (N=2,3) We use surface form for words in stopword list 1, otherwise we use POS form.N=2 RB difficult, difficult NNN=3 RB difficult NNDependency the surface and relation name (i, nsubj)the surface and the dependend token?ssurface(think, i)the surface, relation name and the de-pendend token?s surface(nsubj, i, think)Tree substitution grammer Fragments of TSG (PRP UNK-INITC-KNOWNLC) (VB think)(NP RB DT ADJP NN)(JJ UNK-LC)Table 1: All features for native language identification.using the Stanford Parser 2.0.2 3.We use tree substitution grammars as fea-tures.
TSGs are generalized context-free grammars(CFGs) that allow nonterminals to re-write to treefragments.
The fragments reflect both syntactic andsurface structures of a given sentence more effi-ciently than using several CFG rules.
In practice,efficient Bayesian approaches have been proposedin prior work (Post and Gildea, 2009).
In termsof the application of TSG to NLI task, (Swansonand Charniak, 2012) have shown a promising re-sult.
Post (2011) also uses TSG to judge grammat-icality of a sentence written by language learners.With these previous findings in mind, we also ex-tract TSG rules.
We use the training settings andpublic software from Post (2011)4, obtaining 21,020unique TSG fragments from the training dataset ofthe TOEFL-11 corpus.3 Closed TrackIn this section, we describe our system for the closedtrack.
We use the tools and features described inSection 2.In our system, feature selection is performed us-ing a measure based on frequency.
Although Tsur3http://nlp.stanford.edu/software/lex-parser.shtml4https://github.com/mjpost/post2011judgingand Rappoport (2007) used TF-IDF, they use it todecrease the influence of topic bias rather than forincreasing accuracy.
Brooke and Hirst (2012) useddocument frequency for feature selection, howeverit does not affect accuracy.We use the native language frequency (hereafterwe refer to this as NLF).
NLF is the number of na-tive languages a feature appears in.
Thus, NLF takesvalues from 1 to 11.
Figure 1 shows an example ofNLF.
The word bigram feature ?in Japan?
appearsonly in essays of which the learners?
native languageis Japanese, therefore the NLF is 1.The assumption behind using this feature is that afeature which appears in all native languages affectsNLI less, while a feature which appears in few na-tive language affects NLI more.
The features whoseNLFs are 11 include e.g.
?there are?, ?PRP VBP?and ?a JJ NN?.
Table 2 shows some examples of thefeatures appearing in only 1 native language in theTOEFL-11 corpus.
The features include place-nameor company name such as ?tokyo?, ?korea?, ?sam-sung?, which are certainly specific for some nativelanguage.135Native LanguageChinese Japanese Koreancarry more this : NN samsungi hus become of tokyo of koreaJJ whole and when i worked debatable whetherstriking conclusion usuful NN VBG whethertraffic tools oppotunity for in thesedaysTable 2: Example of feature appearing in 1 native language for Chinese, Japanese and KoreanFigure 1: Example of native language frequencyNative Language # of articlesJapanese 258,320Mandarin 48,364Korean 31,188Spanish 5,106Italian 2,589Arabic 1,549French 1,168German 832Turkish 504Hindi 223Telugu 19Table 3: Distribution of native languages in Lang-8corpus4 Open tracks4.1 Lang-8 corpusFor the open tracks, we used Lang-8 as a source tocreate a learner corpus tagged with the native lan-guages of learners.
Lang-8 is a language learningsocial networking service.
5 Users write articlesin their non-native languages and native speakerscorrect them.
We used all English articles writtenthrough the end of 2012.
We removed all sentenceswhich contain non-ASCII characters.
6Almost all users register their native language onthe site.
We regard users?
registered native language5http://lang-8.com/6Some users also add translation in their native languagesfor correctors?
reference.as the gold label for each article.
We split the learnercorpus extracted from Lang-8 into sub-corpora bythe native languages.
The numbers of articles in allcorpora are summarized in Table 3.
Unfortunately,some sub-corpora are too small to train the model.For example, the Telugu corpus has only 19 articles.In order to balance the size of the training data,we tried two approaches: Capping and Sampling.We confirmed in preliminary experiments that themodel with these approaches work better than themodel with the original sized data.CappingIn this approach, we limit the size of a sub-corpusfor training to N articles.
For a sub-corpus whichcontains over N articles, we randomly extract ar-ticles up to N .
We set N = 5000 and adapt thisapproach for Run 1 and Run 3 in the open tracks.SamplingIn this approach, we equalize the size of all sub-corpora.
For corpora which contain less than N ar-ticles, we randomly copy articles until their size be-comesN .
We setN = 5000 and adapt this approachfor Run 2 and Run 4 in the open tracks.4.2 ModelsWe compared two approaches with baseline featuresand all features.The models in Run 1 and Run 3 were trained withthe data created by the Capping approach, and themodels in Run 2 and Run 47 were trained by theSampling approach.We used only word N-grams (N = 1, 2) as base-line features.
As extra features we used the follow-ing features.7We did not have time to train the model for Run 4 in theopen 1 track.136?
POS N-grams (N = 2, 3)?
dependency?
character N-grams (N = 2, 3)In open track 2, we also add the TOEFL-11dataset to the training data for all runs.5 Result for NLI shared Task 2013Table 4 shows the results of our systems for NLIShared Task.
Chance accuracy is 0.09.
All resultsoutperform random guessing.5.1 Closed trackIn the closed track, we submitted 5 runs.
Run 1is the system using only word 1,2-grams features.Run 2 is the system using all features with NLF fea-ture selection (1 < NLF < 11).
Run 3 is the systemusing word 1,2-grams and POS 2,3-grams features.Run 4 is the system using word 1,2-grams, POS 2,3-grams, character 2,3-grams and dependency featureswithout parameter tuning.
Run 5 is the system us-ing word 1,2-grams without parameter tuning.
Themethod using the feature selection method we pro-posed achieved the best performance of our systems.5.2 Open tracksComparison of the two data balancingapproachesIn open track 1, the method of ?Sampling?
out-performs that of ?Capping?
(Run 2 > Run 1).
Thismeans even duplicated training data can improve theperformance.On the other hand, in open track 2, ?Capping?works better than ?Sampling?
(Run 1 > Run 2 andRun 3>Run 4).
In the first place, the models trainedwith both Lang-8 data and TOEFL data do not per-form better than ones trained with only TOEFL data.This means the less Lang-8 data we use, the betterperformance we obtain.Comparison on two feature setsIn open track 1, adding extra features seems tohave a bad influence because the result of Run 3is worse than that of Run 1.
This may be becauseLang-8 data is out of domain of the test corpus(TOEFL).Closed Open 1 Open 2Run Accuracy Accuracy Accuracy1 0.811 0.337 0.6992 ?0.817 0.356 0.6613 0.808 0.285 0.7034 0.771 - 0.6655 0.783 - -Table 4: Result for systems which submitted in NLI2013 ?We re-evaluated the Run2 because we submitted theRun1 with the same output as Run2.In open track 2, adding extra features makes theperformance better (Run 3 > Run 1, Run 4 > Run2).
In-domain TOEFL data seem to be effective fortraining with extra features.
In order to improve theresult with extra features in open track 2, domainadaptation may be effective.6 Experiment and Result for 10 foldCross-ValidationWe conducted an experiment using 10-fold crossvalidation on the data set used by Tetreault et al(2012).
Table 5 shows the results for different fea-ture set.
The table consists of 3 blocks; the firstblock is results of the system using 1 feature, thesecond block is the result of the system using word1,2-grams feature and another feature, and the thirdblock is the result of the system using word 1,2-grams and more features.In the first block results, the system using theword 1,2-grams feature achieved 0.8075.
It is thehighest accuracy in the first block, and third highestaccuracy in the results of Table 5.
From the secondblock of results, adding an extra feature does not im-prove accuracy, however in the third block the sys-tems in (14) and (15) outperform the system usingonly word 1,2-grams.Table 6 shows the results of using feature selec-tion by NLF.
The table consists of 3 blocks; thefirst block is the results of the system using featureswhose NLF is smaller than N (N = 11, 10, 9, 8), thesecond block is the results of the system using fea-tures whose NLF is greater than N (N = 1, 2, 3, 4),and the third block is the results of the system usingfeatures whose NLF is smaller than 11 and greaterthan N (N = 1, 2, 3, 4).The best accuracy is achieved by excluding fea-137Feature Accuracy(1) Word 1,2-gram 0.8075(2) POS 2,3-gram 0.5555(3) POS,Function 2,3-gram 0.7080(4) Chracter 2,3-gram 0.6678(5) Dependency 0.7236(6) Tree substitution grammar 0.6455(7) 1 + 2 0.7825(8) 1 + 3 0.7913(9) 1 + 4 0.7953(10) 1 + 5 0.8020(11) 1 + 6 0.7999(12) 1 + 2 + 3 0.7849(13) 1 + 2 + 3 + 4 0.8000(14) 1 + 2 + 3 + 4 + 5 0.8097(15) ALL 0.8088Table 5: 10-fold cross validation results for eachfeaturetures whose NLF is 1 or 11.
While the results of thefirst block and the second block are intuitive, the re-sults of the third block are not (looking at the secondblock of Table 6, excluding features whose NLF isgreater than N (1, 2, 3, 4) reduces accuracy).
Onepossible explanation is that features whose NLF is1 includes features that rarely appear in the trainingcorpus.7 ConclusionIn this paper, we described our systems for the NLIShared Task 2013.
We tried feature selection usingnative language frequency for the closed track andCapping and the Sampling data to balance the size oftraining data for the open tracks.
The feature selec-tion we proposed improves the performance for NLI.The system using our feature selection achieved0.817 on the test data of NLI Shared Task and 0.821using 10-fold cross validation.
While the Samplingsystem outperformed Capping system for open track1, the Capping system outperformed Sampling sys-tem in open track 2 (because it reduced the amountof out of domain data).ReferencesDaniel Blanchard, Joel Tetreault, Derrick Higgins, AoifeCahill, and Martin Chodorow.
2013.
Toefl11: A cor-AccuracyNLF < 11 0.8176NLF < 10 0.8157NLF < 9 0.8123NLF < 8 0.80981 < NLF 0.80622 < NLF 0.80623 < NLF 0.80574 < NLF 0.80531 < NLF < 11 0.82092 < NLF < 11 0.82063 < NLF < 11 0.82014 < NLF < 11 0.8195Table 6: 10-fold cross validation results usingfeature selection by NLF.
(feature selection is notapplied to word N-grams features.
)pus of non-native english.
Technical report, Educa-tional Testing Service.Julian Brooke and Graeme Hirst.
2011.
Native languagedetection with ?cheap?
learner corpora.
In Proceedingsof LCR 2011.Julian Brooke and Graeme Hirst.
2012.
Robust, lexical-ized native language identification.
In Proceedings ofCOLING 2012, pages 391?408.Serhiy Bykh and Detmar Meurers.
2012.
Native lan-guage identification using recurring n-grams ?
inves-tigating abstraction and domain dependence.
In Pro-ceedings of COLING 2012, pages 425?440.Matt Post and Daniel Gildea.
2009.
Bayesian Learningof a Tree Substitution Grammar.
In Proceedings of theACL-IJCNLP 2009, pages 45?48.Matt Post.
2011.
Judging Grammaticality with Tree Sub-stitution Grammar Derivations.
In Proceedings of ACL2011, pages 217?222.Ben Swanson and Eugene Charniak.
2012.
Native Lan-guage Detection with Tree Substitution Grammars.
InProceedings of ACL 2012, pages 193?197.Joel Tetreault, Daniel Blanchard, Aoife Cahill, and Mar-tin Chodorow.
2012.
Native tongues, lost andfound: Resources and empirical evaluations in nativelanguage identification.
In Proceedings of COLING2012, pages 2585?2602.Joel Tetreault, Daniel Blanchard, and Aoife Cahill.
2013.A report on the first native language identificationshared task.
In Proceedings of the Eighth Workshopon Building Educational Applications Using NLP.Oren Tsur and Ari Rappoport.
2007.
Using classifierfeatures for studying the effect of native language onthe choice of written second language words.
In Pro-ceedings of CACLA, pages 9?16.138Sze-Meng Jojo Wong and Mark Dras.
2011.
Exploitingparse structures for native language identification.
InProceedings of EMNLP 2011, pages 1600?1610.Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson.2011.
Topic modeling for native language identifi-cation.
In Proceedings of the Australasian LanguageTechnology Association Workshop 2011, pages 115?124.139
