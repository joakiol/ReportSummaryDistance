Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 49?54,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsAdaptive Recursive Neural Networkfor Target-dependent Twitter Sentiment ClassificationLi Dong?
?Furu Wei?Chuanqi Tan?
?Duyu Tang?
?Ming Zhou?Ke Xu?
?Beihang University, Beijing, China?Microsoft Research, Beijing, China?Harbin Institute of Technology, Harbin, Chinadonglixp@gmail.com fuwei@microsoft.com {ysjtcq,tangduyu}@gmail.commingzhou@microsoft.com kexu@nlsde.buaa.edu.cnAbstractWe propose Adaptive Recursive NeuralNetwork (AdaRNN) for target-dependentTwitter sentiment classification.
AdaRNNadaptively propagates the sentiments ofwords to target depending on the contextand syntactic relationships between them.It consists of more than one compositionfunctions, and we model the adaptive sen-timent propagations as distributions overthese composition functions.
The experi-mental studies illustrate that AdaRNN im-proves the baseline methods.
Further-more, we introduce a manually annotateddataset for target-dependent Twitter senti-ment analysis.1 IntroductionTwitter becomes one of the most popular socialnetworking sites, which allows the users to readand post messages (i.e.
tweets) up to 140 charac-ters.
Among the great varieties of topics, peoplein Twitter tend to express their opinions for thebrands, celebrities, products and public events.
Asa result, it attracts much attention to estimate thecrowd?s sentiments in Twitter.For the tweets, our task is to classify their senti-ments for a given target as positive, negative, andneutral.
People may mention several entities (ortargets) in one tweet, which affects the availabil-ities for most of existing methods.
For example,the tweet ?
@ballmer: windows phone is betterthan ios!?
has three targets (@ballmer, windowsphone, and ios).
The user expresses neutral, pos-itive, and negative sentiments for them, respec-tively.
If target information is ignored, it is diffi-cult to obtain the correct sentiment for a specifiedtarget.
For target-dependent sentiment classifica-tion, the manual evaluation of Jiang et al (2011)?Contribution during internship at Microsoft Research.show that about 40% of errors are caused by notconsidering the targets in classification.The features used in traditional learning-basedmethods (Pang et al, 2002; Nakagawa et al, 2010)are independent to the targets, hence the resultsare computed despite what the targets are.
Hu andLiu (2004) regard the features of products as tar-gets, and sentiments for them are heuristically de-termined by the dominant opinion words.
Jianget al (2011) combine the target-independent fea-tures (content and lexicon) and target-dependentfeatures (rules based on the dependency parsingresults) together in subjectivity classification andpolarity classification for tweets.In this paper, we mainly focus on integratingtarget information with Recursive Neural Network(RNN) to leverage the ability of deep learningmodels.
The neural models use distributed repre-sentation (Hinton, 1986; Rumelhart et al, 1986;Bengio et al, 2003) to automatically learn fea-tures for target-dependent sentiment classification.RNN utilizes the recursive structure of text, and ithas achieved state-of-the-art sentiment analysis re-sults for movie review dataset (Socher et al, 2012;Socher et al, 2013).
The recursive neural mod-els employ the semantic composition functions,which enables them to handle the complex com-positionalities in sentiment analysis.Specifically, we propose a framework whichlearns to propagate the sentiments of words to-wards the target depending on context and syn-tactic structure.
We employ a novel adaptivemulti-compositionality layer in recursive neuralnetwork, which is named as AdaRNN (Dong etal., 2014).
It consists of more than one compo-sition functions, and we model the adaptive sen-timent propagations as learning distributions overthese composition functions.
We automaticallylearn the composition functions and how to selectthem from supervisions, instead of choosing themheuristically or by hand-crafted rules.
AdaRNN49determines how to propagate the sentiments to-wards the target and handles the negation or in-tensification phenomena (Taboada et al, 2011) insentiment analysis.
In addition, we introduce amanually annotated dataset, and conduct extensiveexperiments on it.
The experimental results sug-gest that our approach yields better performancesthan the baseline methods.2 RNN: Recursive Neural NetworkRNN (Socher et al, 2011) represents the phrasesand words as D-dimensional vectors.
It performscompositions based on the binary trees, and obtainthe vector representations in a bottom-up way.not very goodNegativeSoftmaxvery goodnot very goodFigure 1: The composition process for ?not verygood?
in Recursive Neural Network.As illustrated in Figure 1, we obtain the repre-sentation of ?very good?
by the composition of?very?
and ?good?, and the representation of tri-gram ?not very good?
is recursively obtained bythe vectors of ?not?
and ?very good?.
The di-mensions of parent node are calculated by linearcombination of the child vectors?
dimensions.
Thevector representation v is obtained via:v = f (g (vl,vr)) = f(W[vlvr]+ b)(1)where vl,vrare the vectors of its left and rightchild, g is the composition function, f is the non-linearity function (such as tanh, sigmoid, softsign,etc.
), W ?
RD?2Dis the composition matrix, andb is the bias vector.
The dimension of v is thesame as its child vectors, and it is recursively usedin the next step.
Notably, the word vectors in theleaf nodes are regarded as the parameters, and willbe updated according to the supervisions.The vector representation of root node is thenfed into a softmax classifier to predict the label.The k-th element of softmax(x) isexp{xk}?jexp{xj}.
Fora vector, the softmax obtains the distribution overK classes.
Specifically, the predicted distributionis y = softmax (Uv), where y is the predicteddistribution, U ?
RK?Dis the classification ma-trix, and v is the vector representation of node.3 Our ApproachWe use the dependency parsing results to find thewords syntactically connected with the interestedtarget.
Adaptive Recursive Neural Network is pro-posed to propagate the sentiments of words to thetarget node.
We model the adaptive sentimentpropagations as semantic compositions.
The com-putation process is conducted in a bottom-up man-ner, and the vector representations are computedrecursively.
After we obtain the representation oftarget node, a classifier is used to predict the sen-timent label according to the vector.In Section 3.1, we show how to build recur-sive structure for target using the dependency pars-ing results.
In Section 3.2, we propose AdaptiveRecursive Neural Network and use it for target-dependent sentiment analysis.3.1 Build Recursive StructureThe dependency tree indicates the dependency re-lations between words.
As described above, wepropagate the sentiments of words to the target.Hence the target is placed at the root node to com-bine with its connected words recursively.
The de-pendency relation types are remained to guide thesentiment propagations in our model.Algorithm 1 Convert Dependency TreeInput: Target node, Dependency treeOutput: Converted tree1: function CONV(r)2: Er?
SORT(dep edges connected with r)3: v?
r4: for (rt??
u/ut??
r) in Erdo5: if r is head of u then6: w?
node with CONV(u), v as children7: else8: w?
node with v, CONV(u) as children9: v?
w10: return v11: Call CONV(target node) to get converted treeAs illustrated in the Algorithm 1, we recursivelyconvert the dependency tree starting from the tar-get node.
We find all the words connected to thetarget, and these words are combined with targetnode by certain order.
Every combination is con-sidered as once propagation of sentiments.
If thetarget is head of the connected words, the targetvector is combined as the right node; if otherwise,it is combined as the left node.
This ensures the50child nodes in a certain order.
We use two rulesto determine the order of combinations: (1) thewords whose head is the target in dependency treeare first combined, and then the rest of connectedwords are combined; (2) if the first rule cannot de-termine the order, the connected words are sortedby their positions in sentence from right to left.Notably, the conversion is performed recursivelyfor the connected words and the dependency rela-tion types are remained.
Figure 2 shows the con-verted results for different targets in one sentence.3.2 AdaRNN: Adaptive Recursive NeuralNetworkRNN employs one global matrix to linearly com-bine the elements of vectors.
Sometimes it ischallenging to obtain a single powerful functionto model the semantic composition, which moti-vates us to propose AdaRNN.
The basic idea ofAdaRNN is to use more than one compositionfunctions and adaptively select them depending onthe linguistic tags and the combined vectors.
Themodel learns to propagate the sentiments of wordsby using the different composition functions.Figure 2 shows the computation process for theexample sentence ?windows is better than ios?,where the user expresses positive sentiment to-wards windows and negative sentiment to ios.
Forthe targets, the order of compositions and the de-pendency types are different.
AdaRNN adap-tively selects the composition functions g1.
.
.
gCdepending on the child vectors and the linguistictypes.
Thus it is able to determine how to propa-gate the sentiments of words towards the target.Based on RNN described in Section 2, we de-fine the composition result v in AdaRNN as:v = f(C?h=1P (gh|vl,vr, e) gh(vl,vr))(2)where g1, .
.
.
, gCare the composition functions,P (gh|vl,vr, e) is the probability of employing ghgiven the child vectors vl,vrand external featurevector e, and f is the nonlinearity function.
Forthe composition functions, we use the same formsas in Equation (1), i.e., we have C compositionmatrices W1.
.
.WC.
We define the distributionover these composition functions as:??
?P (g1|vl,vr, e)...P (gC|vl,vr, e)??
?= softmax???S??vlvre????
(3)where ?
is the hyper-parameter, S ?
RC?
(2D+|e|)is the matrix used to determine which compositionfunction we use, vl,vrare the left and right childvectors, and e are external feature vector.
In thiswork, e is a one-hot binary feature vector whichindicates what the dependency type is.
If relationis the k-th type, we set ekto 1 and the others to 0.Adding ?
in softmax function is a widely usedparametrization method in statistical mechanics,which is known as Boltzmann distribution andGibbs measure (Georgii, 2011).
When ?
= 0, thisfunction produces a uniform distribution; when?
= 1, it is the same as softmax function; when?
?
?, it only activates the dimension with max-imum weight, and sets its probability to 1.3.3 Model TrainingWe use the representation of root node as the fea-tures, and feed them into the softmax classifier topredict the distribution over classes.
We define theground truth vector t as a binary vector.
If the k-thclass is the label, only tkis 1 and the others are0.
Our goal is to minimize the cross-entropy errorbetween the predicted distribution y and groundtruth distribution t. For each training instance, wedefine the objective function as:min???jtjlogyj+????????
?22(4)where ?
represents the parameters, and the L2-regularization penalty is used.Based on the converted tree, we employ back-propagation algorithm (Rumelhart et al, 1986) topropagate the errors from root node to the leafnodes.
We calculate the derivatives to update theparameters.
The AdaGrad (Duchi et al, 2011) isemployed to solve this optimization problem.4 ExperimentsAs people tend to post comments for the celebri-ties, products, and companies, we use these key-words (such as ?bill gates?, ?taylor swift?, ?xbox?,?windows 7?, ?google?)
to query the Twitter API.After obtaining the tweets, we manually anno-tate the sentiment labels (negative, neutral, posi-tive) for these targets.
In order to eliminate theeffects of data imbalance problem, we randomlysample the tweets and make the data balanced.The negative, neutral, positive classes account for25%, 50%, 25%, respectively.
Training data con-sists of 6,248 tweets, and testing data has 69251w indowsisbetterio s thang1 gC...g 1 gC...g1 gC...g 1 gC...n s u b jcopp reppobjPositveSoftmaxio sthanw indowsis betterg1 g C...g1 gC...g1 gC...g 1 g C...pobjprepns u b jcopNegativeSoftmaxw indows is better than io sRO OTcopns ub jp rep pobj( target ) ( target )Dependency tree :windows is target : ios is target :Figure 2: For the sentence ?windows is better than ios?, we convert its dependency tree for the differenttargets (windows and ios).
AdaRNN performs semantic compositions in bottom-up manner and forwardpropagates sentiment information to the target node.
The g1, .
.
.
, gCare different composition functions,and the combined vectors and dependency types are used to select them adaptively.
These compositionfunctions decide how to propagate the sentiments to the target.tweets.
We randomly sample some tweets, andthey are assigned with sentiment labels by two an-notators.
About 82.5% of them have the same la-bels.
The agreement percentage of polarity clas-sification is higher than subjectivity classification.To the best of our knowledge, this is the largesttarget-dependent Twitter sentiment classificationdataset which is annotated manually.
We make thedataset publicly available1for research purposes.We preprocess the tweets by replacing the tar-gets with $T$ and setting their POS tags to NN.Liblinear (Fan et al, 2008) is used for baselines.A tweet-specific tokenizer (Gimpel et al, 2011)is employed, and the dependency parsing resultsare computed by Stanford Parser (Klein and Man-ning, 2003).
The hyper-parameters are chosen bycross-validation on the training split, and the testaccuracy and macro-average F1-score score are re-ported.
For recursive neural models, the dimen-sion of word vector is set to 25, and f = tanhis used as the nonlinearity function.
We employ10 composition matrices in AdaRNN.
The param-eters are randomly initialized.
Notably, the wordvectors will also be updated.SVM-indep: It uses the uni-gram, bi-gram,punctuations, emoticons, and #hashtags as thecontent features, and the numbers of positive ornegative words in General Inquirer as lexicon fea-tures.
These features are all target-independent.SVM-dep: We re-implement the method pro-posed by Jiang et al (2011).
It combines both1http://goo.gl/5Enpu7the target-independent (SVM-indep) and target-dependent features and uses SVM as the classifier.There are seven rules to extract target-sensitivefeatures.
We do not implement the social graphoptimization and target expansion tricks in it.SVM-conn: The words, punctuations, emoti-cons, and #hashtags included in the converted de-pendency tree are used as the features for SVM.RNN: It is performed on the converted depen-dency tree without adaptive composition selection.AdaRNN-w/oE: Our approach without usingthe dependency types as features in adaptive se-lection for the composition functions.AdaRNN-w/E: Our approach with employingthe dependency types as features in adaptive se-lection for the composition functions.AdaRNN-comb: We combine the root vectorsobtained by AdaRNN-w/E with the uni/bi-gramfeatures, and they are fed into a SVM classifier.Method Accuracy Macro-F1SVM-indep 62.7 60.2SVM-dep 63.4 63.3SVM-conn 60.0 59.6RNN 63.0 62.8AdaRNN-w/oE 64.9 64.4AdaRNN-w/E 65.8 65.5AdaRNN-comb 66.3 65.9Table 1: Evaluation results on target-dependentTwitter sentiment classification dataset.
Our ap-proach outperforms the baseline methods.52As shown in the Table 1, AdaRNN achieves bet-ter results than the baselines.
Specifically, we findthat the performances of SVM-dep increase thanSVM-indep.
It indicates that target-dependent fea-tures help improve the results.
However, the accu-racy and F1-score do not gain significantly.
Thisis caused by mismatch of the rules (Jiang et al,2011) used to extract the target-dependent fea-tures.
The POS tagging and dependency parsingresults are not precise enough for the Twitter data,so these hand-crafted rules are rarely matched.Further, the results of SVM-conn illustrate that us-ing the words which have paths to target as bag-of-words features does not perform well.RNN is also based on the converted depen-dency tree.
It outperforms SVM-indep, and iscomparable with SVM-dep.
The performancesof AdaRNN-w/oE are better than the above base-lines.
It shows that multiple composition functionsand adaptive selection help improve the results.AdaRNN provides more powerful compositionability, so that it achieves better semantic compo-sition for recursive neural models.
AdaRNN-w/Eobtains best performances among the above meth-ods.
Its macro-average F1-score rises by 5.3%than the target-independent method SVM-indep.It employs dependency types as binary features toselect the composition functions adaptively.
Theresults illustrate that the syntactic tags are helpfulto guide the model propagate sentiments of wordstowards target.
Although the dependency resultsare also not precise enough, the composition se-lection is automatically learned from data.
HenceAdaRNN is more robust for the imprecision ofparsing results than the hand-crafted rules.
Theperformances become better after adding the uni-gram and bi-gram features (target-independent).4.1 Effects of ?We compare different ?
for AdaRNN defined inEquation (3) in this section.
Different parameter ?leads to different composition selection schemes.As illustrated in Figure 3, the AdaRNN-w/oEand AdaRNN-w/E achieve the best accuracies at?
= 2, and they have a similar trend.
Specifi-cally, ?
= 0 obtains a uniform distribution overthe composition functions which does not help im-prove performances.
?
?
?
results in a max-imum probability selection algorithm, i.e., onlythe composition function which has the maximumprobability is used.
This selection scheme makes0 20 21 22 23 24 25 26?616263646566AccuracyRNNAdaRNN-w/oEAdaRNN-w/EFigure 3: The curve shows the accuracy as thehyper-parameter ?
= 0, 20, 21, .
.
.
, 26increases.AdaRNN achieves the best results at ?
= 21.the optimization instable.
The performances of?
= 1, 2 are similar and they are better thanother settings.
It indicates that adaptive selectionmethod is useful to model the compositions.
Thehyper-parameter ?
makes trade-offs between uni-form selection and maximum selection.
It adjuststhe effects of these two perspectives.5 ConclusionWe propose Adaptive Recursive Neural Network(AdaRNN) for the target-dependent Twitter senti-ment classification.
AdaRNN employs more thanone composition functions and adaptively choosesthem depending on the context and linguistic tags.For a given tweet, we first convert its dependencytree for the interested target.
Next, the AdaRNNlearns how to adaptively propagate the sentimentsof words to the target node.
AdaRNN enablesthe sentiment propagations to be sensitive to bothlinguistic and semantic categories by using differ-ent compositions.
The experimental results illus-trate that AdaRNN improves the baselines withouthand-crafted rules.AcknowledgmentsThis research was partly supported by the National863 Program of China (No.
2012AA011005), thefund of SKLSDE (Grant No.
SKLSDE-2013ZX-06), and Research Fund for the Doctoral Pro-gram of Higher Education of China (Grant No.20111102110019).ReferencesYoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A neural probabilistic lan-guage model.
JMLR, 3:1137?1155, March.53Li Dong, Furu Wei, Ming Zhou, and Ke Xu.
2014.Adaptive multi-compositionality for recursive neu-ral models with applications to sentiment analysis.In Twenty-Eighth AAAI Conference on Artificial In-telligence (AAAI).
AAAI.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learningand stochastic optimization.
JMLR, 12:2121?2159,July.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
Liblinear: A li-brary for large linear classification.
J. Mach.
Learn.Res., 9:1871?1874, June.H.O.
Georgii.
2011.
Gibbs Measures and PhaseTransitions.
De Gruyter studies in mathematics.
DeGruyter.Kevin Gimpel, Nathan Schneider, Brendan O?Connor,Dipanjan Das, Daniel Mills, Jacob Eisenstein,Michael Heilman, Dani Yogatama, Jeffrey Flanigan,and Noah A. Smith.
2011.
Part-of-speech taggingfor twitter: Annotation, features, and experiments.In Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics: HumanLanguage Technologies: Short Papers - Volume 2,HLT ?11, pages 42?47, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Geoffrey E. Hinton.
1986.
Learning distributed repre-sentations of concepts.
In Proceedings of the EighthAnnual Conference of the Cognitive Science Society,pages 1?12.
Hillsdale, NJ: Erlbaum.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of the TenthACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining, KDD ?04, pages168?177, New York, NY, USA.
ACM.Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, andTiejun Zhao.
2011.
Target-dependent twitter sen-timent classification.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies - Vol-ume 1, HLT ?11, pages 151?160, Stroudsburg, PA,USA.
Association for Computational Linguistics.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics - Volume 1, ACL ?03, pages 423?430, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.2010.
Dependency tree-based sentiment classifica-tion using crfs with hidden variables.
In HumanLanguage Technologies: The 2010 Annual Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics, pages 786?794.Association for Computational Linguistics.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
: sentiment classification usingmachine learning techniques.
In Proceedings of theACL-02 conference on Empirical methods in naturallanguage processing-Volume 10, pages 79?86.
As-sociation for Computational Linguistics.D.E.
Rumelhart, G.E.
Hinton, and R.J. Williams.
1986.Learning representations by back-propagating er-rors.
Nature, 323(6088):533?536.Richard Socher, Cliff C. Lin, Andrew Y. Ng, andChristopher D. Manning.
2011.
Parsing NaturalScenes and Natural Language with Recursive NeuralNetworks.
In ICML.Richard Socher, Brody Huval, Christopher D. Man-ning, and Andrew Y. Ng.
2012.
Semantic composi-tionality through recursive matrix-vector spaces.
InEMNLP-CoNLL, pages 1201?1211.Richard Socher, Alex Perelygin, Jean Y Wu, JasonChuang, Christopher D Manning, Andrew Y Ng,and Christopher Potts.
2013.
Recursive Deep Mod-els for Semantic Compositionality Over a SentimentTreebank.
In EMNLP, pages 1631?1642.Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-berly Voll, and Manfred Stede.
2011.
Lexicon-based methods for sentiment analysis.
Comput.
Lin-guist., 37(2):267?307, June.54
