Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 281?291,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsA Generalisation of Lexical Functionsfor Composition in Distributional SemanticsAntoine BrideIRIT & Universit?e de Toulouseantoine.bride@irit.frTim Van de CruysIRIT & CNRS, Toulousetim.vandecruys@irit.frNicholas AsherIRIT & CNRS, Toulousenicholas.asher@irit.frAbstractOver the last two decades, numerous algo-rithms have been developed that success-fully capture something of the semanticsof single words by looking at their distri-bution in text and comparing these distri-butions in a vector space model.
How-ever, it is not straightforward to constructmeaning representations beyond the levelof individual words ?
i.e.
the combina-tion of words into larger units ?
using dis-tributional methods.
Our contribution istwofold.
First of all, we carry out a large-scale evaluation, comparing different com-position methods within the distributionalframework for the cases of both adjective-noun and noun-noun composition, makinguse of a newly developed dataset.
Sec-ondly, we propose a novel method forcomposition, which generalises the ap-proach by Baroni and Zamparelli (2010).The performance of our novel method isalso evaluated on our new dataset andproves competitive with the best methods.1 IntroductionIn the course of the last two decades, there hasbeen a growing interest in distributional meth-ods for lexical semantics (Landauer and Dumais,1997; Lin, 1998; Turney and Pantel, 2010).
Thesemethods are based on the distributional hypothe-sis (Harris, 1954), according to which words thatappear in the same contexts tend to be similar inmeaning.
Inspired by Harris?
hypothesis, numer-ous researchers have developed algorithms that tryto capture the semantics of individual words bylooking at their distribution in a large corpus.Compared to manual studies common to formalsemantics, distributional semantics offers substan-tially larger coverage since it is able to analyzemassive amounts of empirical data.
However, it isnot trivial to combine the algebraic objects createdby distributional semantics to get a sensible distri-butional representation for more complex expres-sions, consisting of several words.
On the otherhand, the formalism of the ?
-calculus provides uswith general, advanced and efficient methods forcomposition that can model meaning compositionnot only of simple phrases, but also more com-plex phenomena such as coercion or compositionwith fine-grained types (Asher, 2011; Luo, 2010;Bassac et al, 2010).
Despite continued efforts tofind a general method for composition and variousapproaches for the composition of specific syntac-tic structures (e.g.
adjective-noun composition, orthe composition of transitive verbs and direct ob-jects (Mitchell and Lapata, 2008; Coecke et al,2010; Baroni and Zamparelli, 2010)), the model-ing of compositionality is still an important chal-lenge for distributional semantics.
Moreover, thevalidation of proposed methods for compositionhas used relatively small datasets of human sim-ilarity judgements (Mitchell and Lapata, 2008).1Although such studies comparing similarity judge-ments have their merits, it would be interesting tohave studies that evaluate methods for composi-tion on a larger scale, using a larger test set of dif-ferent specific compositions.
Such an evaluationwould allow us to evaluate more thoroughly thedifferent methods of composition that have beenproposed.
This is one of the goals of this paper.To achieve this goal, we make use of two dif-ferent resources.
We have constructed a datasetfor French containing a large number of pairsof a compositional expression (adjective-noun)and a single noun that is semantically close oridentical to the composed expression.
Thesepairs have been extracted semi-automatically from1A notable exception is (Marelli et al, 2014), who pro-pose a large-scale evaluation dataset for composition at thesentence level.281the French Wiktionary.
We have also usedthe Semeval 2013 dataset of phrasal similarityjudgements for English with similar pairs ex-tracted semi-automatically from the English Wik-tionary to construct a dataset for English for bothadjective-noun and noun-noun composition.
Thisaffords us a cross-linguistic comparison of themethods.These data sets provide a substantial evalua-tion of the performance of different compositionalmethods.
We have tested three different methodsof composition proposed in the literature, viz.
theadditive and multiplicative model (Mitchell andLapata, 2008), as well as the lexical function ap-proach (Baroni and Zamparelli, 2010).The two first methods are entirely general, andtake as input automatically constructed vectors foradjectives and nouns.
The method by Baroni andZamparelli, on the other hand, requires the acqui-sition of a particular function for each adjective,represented by a matrix.
The second goal of ourpaper is to generalise the functional approach inorder to eliminate the need for an individual func-tion for each adjective.
To this goal, we automat-ically learn a generalised lexical function, basedon Baroni and Zamparelli?s approach.
This gener-alised function combines with an adjective vectorand a noun vector in a generalised way.
The per-formance of our novel generalised lexical functionapproach is evaluated on our test sets and provescompetitive with the best, extant methods.Our paper is organized as follows.
First, we dis-cuss the different compositional models that wehave evaluated in our study, briefly revisiting thedifferent existing methods for composition, fol-lowed by a description of our generalisation of thelexical function approach.
Next, we report on ourevaluation method and its results.
The results sec-tion is followed by a section that discusses workrelated to ours.
Lastly, we draw conclusions andlay out some avenues for future work.2 Composition methods2.1 Simple Models of CompositionIn this section, we describe the composition mod-els for the adjective-noun case.
The extension ofthese models to the noun-noun case is straight-forward; one just needs to replace the adjectiveby the subordinate noun.
Admittedly, choosingwhich noun is subordinate in noun-noun compo-sition may be an interesting problem but it is out-side the scope of this paper.
We tested three sim-ple models of composition: a baseline method thatdiscounts the contribution of the adjective com-pletely, and the additive and multiplicative modelsof composition.
The baseline method is defined asfollows:Compbaseline(adj, noun) = nounThe additive model adds the point-wise valuesof the adjective vector adj and noun vector nounusing independent coefficients to provide a resultfor the composition:Compadditive(adj, noun) = ?
noun+?
adjThe multiplicative model consists in a point-wise multiplication of the vectors adj and noun:Compmultiplicative(adj, noun) = noun?adjwith (noun?adj)i= nouni?adji2.2 The lexical function modelBaroni and Zamparelli?s (2010) lexical func-tion model (LF) is somewhat more complex.Adjective-noun composition is modeled as thefunctional application of an adjective meaning(represented as a matrix) to a noun meaning (rep-resented as a vector).
Thus, the combination ofan adjective and noun is the product of the matrixADJ and the vector noun as shown in Figure 1.Baroni and Zamparelli propose learning an ad-jective?s matrix from examples of the vectorsfor adj noun obtained directly from the corpus.These vectors adj noun are obtained in the sameway as vectors representing a single word: whenthe adjective-noun combination occurs, we ob-serve its context and construct the vector fromthose observations.
As an illustration, considerthe example in 2.
The word name appears threetimes modified by an adjective in the followingexcerpt from Oscar Wilde?s The Importance ofBeing Earnest.
This informs us about the co-occurrence frequencies of three vectors: one fordivine name, another for nice name, and one forcharming name.Once the adj noun vectors have been createdfor a given adjective, we are able to calculate theADJ matrix using a least squares regression thatminimizes the equation ADJ?adj noun?
noun.More formally, the problem is the following:Find ADJ s.t.
?noun(ADJ?noun?adj noun)2is minimal282?=ADJECTIVEnounCompositionLF(adjective, noun)Figure 1: Lexical Function CompositionJack: Personally, darling, to speak quite candidly, I don?t much care about the name of Ernest .
.
.
I don?t think thename suits me at all.Gwendolen: It suits you perfectly.
It is a divine [name].
It has a music of its own.
It produces vibrations.Jack: Well, really, Gwendolen, I must say that I think there are lots of other much nicer [names].
I think Jack, forinstance, is a charming [name].Figure 2: Excerpt from Oscar Wilde?s The Importance of Being EarnestFor our example, we would minimize, among oth-ers DIVINE?divine name?name to get the ma-trix for DIVINE.LF requires a large corpus, because we haveto observe a sufficient number of examples of theadjective and noun combined, which are perforceless exemplified than the presence of the noun oradjective in isolation.
In Figure 2, each of the oc-currences of ?name?
can contribute to the informa-tion in the vector name but none can contribute tothe vector evanescent name.Baroni and Zamparelli (2010) offer an expla-nation of how to cope with the potential sparsedata problem for learning matrices for adjectives.Moreover, recent evaluations of LF show that ex-istent corpora have enough data for it to provide asemantics for the most frequent adjectives and ob-tain better results than other methods (Dinu et al,2013b).Nevertheless, LF has limitations in treating rel-atively rare adjectives.
For example, the adjective?evanescent?
appears 359 times in the UKWaC cor-pus (Baroni et al, 2009).
This is enough to gen-erate a vector for evanescent, but may not be suf-ficient to generate a sufficient number of vectorsevanescent noun to build the matrix EVANES-CENT.
More importantly, for noun-noun combi-nations, one may need to have a LF for a com-bination.
To get the meaning of blood dona-tion campaign in the LF approach, the matrixBLOOD DONATION must be combined to the vec-tor campaign.
Learning this matrix would requireto build vectors blood donation noun for manynouns.
Even if it were possible, the issue wouldarise again for blood donation campaign plan,then for blood donation campaign plan meetingand so forth.In addition, LF?s approach to adjectival mean-ing and composition has a theoretical drawback.Like Montague Grammar, it supposes that the ef-fect of an adjective on a noun meaning is specificto the adjective (Kamp, 1975).
However, recentstudies suggest that the Montague approach over-generalises from the worst case, and that the vastmajority of adjectives in the world?s languagesare subsective, suggesting that the modification ofnominal meaning that results from their compo-sition with a noun follows general principles (Par-tee, 2010; Asher, 2011) that are independent of thepresence or absence of examples of association.2.3 Generalised LFTo solve these problems, we generalise LF and re-place individual matrices for adjectival meaningsby a single lexical function: a tensor for adjectivalcomposition A .2Our proposal is that adjective-noun composition is carried out by multiplying thetensorA with the vector for the adjective adj, fol-lowed by a multiplication with the vector noun,c.f.
Figure 3.The product of the tensor A and the vector adjyields a matrix dependent of the adjective that ismultiplied with the vector noun.
This matrix cor-responds to the LF matrix ADJ.
As indicated inFigure 4, we obtain A with the help of matricesobtained from the LF approach, and from vectorsfor single words easily obtained in distributionalsemantics; we perform a least square regressionminimizing the norm of the matrices generated bythe equations in Figure 4.
Formally, the problemis2A tensor generalises a matrix to several dimensions.
Weuse a tensor in three modes.
For an introduction to tensors,see (Kolda and Bader, 2009).283=(A djective?adj)?noun?
adjective, noun CompositionGLF(adjective, noun)Figure 3: Composition in the generalised lexical function modelFind A s.t.
?adj(A ?adj?ADJ)2is minimalNote that our tensor is not just the compilation ofthe information found in the LF matrices: the ad-jective mode of our tensor has a limited numberof dimensions, whereas the LF approach creates aseparate matrix for each individual adjective.
Thisreduction forces the model to generalise, and wehypothesise that this generalisation allows us tomake proper noun modifications even in the lightof sparse data.Our approach requires learning a significantnumber of matrices ADJ.
This is not a problem,since FRWaC and UKWaC provide sufficient datafor the LF approach to generate matrices for a sig-nificant number of adjectives.
For example, the2000thmost frequent adjective in FRWaC (?fas-ciste?)
has more than 4000 occurrences.To return to our example of blood donationcampaign, once the tensor N for noun-nouncomposition is learned, our approach requiresonly the knowledge of the vectors blood, dona-tion and campaign.
We would then perform thefollowing computations:blood donation = (N ?blood)?donationblood donation campaign =(N ?blood donation)?
campaignand this allows us to avoid the sparse data prob-lem for the LF approach in generating the matrixBLOOD DONATION.Once we have obtained the tensor A , we verifyexperimentally its relevance to composition, in or-der to check whether a tensor optimising the equa-tions in Figure 4 would be semantically interest-ing.3 Evaluation3.1 Tasks descriptionIn order to evaluate the different compositionmethods, we constructed test sets for French andEnglish, inspired by the work of Zanzotto et al(2010) and the SEMEVAL-2013 task evaluatingphrasal semantics (Korkontzelos et al, 2013).
Thetask is to make a judgement about the semanticsimilarity of a short word sequence (an adjective-noun combination) and a single noun.
This is im-portant, as composition models need to be able totreat word sequences of arbitrary length.
Formally,the task is presented as:With comp = composition(adj, noun1)Evaluate similarity(comp, noun2)where the ?composition?
function is carried outby the different composition models.
?Similarity?needs to be a binary function, with return val-ues ?similar?
and ?non-similar?.
Note, however,that the distributional approach yields a continu-ous similarity value (such as the cosine similar-ity between two vectors).
In order to determinewhich cosine values correspond to ?similar?
andwhich cosine values correspond to ?non-similar?,we looked at a number of examples from a de-velopment set.
More precisely, we carried out alogistic regression on 50 positive and 50 negativeexamples (separate from our test set) in order toautomatically learn the threshold at which a pairis considered to be similar.
Finally, we decided touse balanced test sets containing as many positiveinstances as negative ones.The test set is constructed in a semi-automaticway, making use of the canonical phrasing of dic-tionary definitions.
Take for example the defini-tion of bassoon in the English Wiktionary3, pre-sented in Figure 5.
It is quite straightforwardto extract the pair (musical instrument,bassoon)from this definition.
Using a large dictionary(such as Wiktionary), it is then possible to ex-tract a large number of positive ?
i.e.
similar ?
(adjective noun,noun) pairs.For the construction of our test set for French,we downloaded all entries of the French Wik-tionary (Wiktionnaire) and annotated them with3http://en.wiktionary.org/wiki/bassoon, ac-cessed on 26 February 2015.284Find tensor A by minimizing:A djective?red?RED,A djective?slow?SLOW.
.
.Figure 4: Learning the A djective tensorbassoon /b@"su:n/ (plural bassoons)1.
A musical instrument in the woodwind family, having a double reed and, playing in the tenor andbass ranges.Figure 5: Definition of bassoon, extracted from the English Wiktionarypart of speech tags, using the French part of speechtagger MElt (Denis et al, 2010).
Next, we ex-tracted all definitions that start with an adjective-noun combination.
As a final step, we filtered allinstances containing words that appear too infre-quently in our FRWaC corpus.4The automatically extracted instances were thenchecked manually, and all instances that were con-sidered incorrect were rejected.
This gave us a fi-nal test set of 714 positive examples.We also created an initial set of negative ex-amples, where we combined an existing combi-nation of adjective noun1 (extracted from theFrench Wiktionary), with a randomly selectednoun noun2.
Again, we verified manually that theresulting (adjective noun1, noun2) pairs con-stituted actual negative examples.
We then cre-ated a second set of negative examples by ran-domly selecting two nouns (noun1,noun2) andone adjective adjective.
The resulting pairs(adjective noun1, noun2) were verified man-ually.In addition to our new test set for French, wealso experimented with the original test set of theSEMEVAL-2013 task evaluation phrasal semanticsfor English.
However, the original test set lackedhuman oversight as ?manly behavior?
was consid-ered similar to ?testosterone?
for example.
We thushand-checked the test set ourselves and extracted652 positive pairs.The negative pairs from the original SEMEVAL-2013 are a combination of a random noun and a4i.e.
less than 200 times for adjectives and less than 1500times for nounsrandom adjective-noun compositon found in theEnglish Wiktionary.
We used it as our first setof English negative examples as it is similar inconstruction to our first set of negative examplesin French.
In addition, we created a completelyrandom negative test set for English in the samefashion we did for the second negative test set forFrench.Finally, the original test set alo contains noun-noun compounds so we also created a test set forthat.
This gave us 226 positive and negative pairsfor the noun-noun composition.3.2 Semantic space constructionIn this section, we describe the construction of oursemantic space.
Our semantic space for Frenchwas built using the FRWaC corpus (Baroni et al,2009) ?
about 1,6 billion words of web texts ?which has been tagged with MElt tagger (Denis etal., 2010) and parsed with MaltParser (Nivre et al,2006a), trained on a dependency-based version ofthe French treebank (Candito et al, 2010).
Oursemantic space for English has been built usingthe UKWaC corpus (Baroni et al, 2009), whichconsists of about 2 billion words extracted fromthe web.
The corpus has been part of speechtagged and lemmatized with Stanford Part-Of-Speech Tagger (Toutanova and Manning, 2000;Toutanova et al, 2003), and parsed with Malt-Parser (Nivre et al, 2006b) trained on sections2-21 of the Wall Street Journal section of thePenn Treebank extended with about 4000 ques-285positive examples random negative examples Wiktionary-based negative examples(mot court, abre?viation) (importance fortuit, gamme) (jugement favorable, discorde)?short word?, ?abbreviation?
?accidental importance?, ?range?
?favorable judgement?, ?discord?
(ouvrage litte?raire, essai) (penchant autoritaire, ile) (circonscription administratif , fumier)?literary work?, ?essay?
?authoritarian slope?, isle?
?administrative district?, ?manure?
(compagnie honorifique, ordre) (auspice aviaire, ponton) (mention honorable, renne)?honorary company?, ?order?
?avian omen?, ?pontoon?
?honorable mention?, ?reindeer?Table 1: A number of examples from our test set for Frenchtions from the QuestionBank5.For both corpora, we extracted the lemmas ofall nouns, adjectives and (bag of words) contextwords.
We only kept those lemmas that consist ofalphabetic characters.6We then selected the 10Kmost frequent lemmas for each category (nouns,adjectives, context words), making sure to includeall the words from the test set.
As a final step,we created our semantic space vectors using ad-jectives and nouns as instances, and bag of wordscontext words as features.
The resulting vectorswere weighted using positive point-wise mutualinformation (ppmi, (Church and Hanks, 1990)),and all vectors were normalized to unit length.We then compared the different compositionmethods on different versions of the same seman-tic space (both for French and English): the fullsemantic space, a reduced version of the space to300 dimensions using singular value decomposi-tion (svd, (Golub and Van Loan, 1996)), and a re-duced version of the space to 300 dimensions us-ing non-negative matrix factorization (nmf, (Leeand Seung, 2000)).
We did so in order to test eachmethod in its optimal conditions.
In fact:?
A non-reduced space contains more informa-tion.
This might be beneficial for methodsthat are able to take advantage of the full se-mantic space (viz.
the additive et multiplica-tive model).
On the other hand, to be ableto use the non-reduced space for the lexicalfunction approach, one would have to learnmatrices of size 10K ?10K for each adjec-tive.
This would be problematic in terms ofcomputing time and data sparseness, as wepreviously noted.
The same goes for our gen-5http://maltparser.org/mco/english_parser/engmalt.html6This step generally filters out dates, numbers and punc-tuation, which have little interest for the distributional ap-proach.eralised approach.?
Previous research has indicated that the lexi-cal function approach is able to achieve bet-ter results using a reduced space with svd.
Onthe other hand, the negative values that resultfrom svd are detrimental for the multiplica-tive approach.?
An nmf -reduced semantic space is not detri-mental for the multiplicative approach.In order to determine the best parameters for theadditive model, we tested this model for differentvalues of ?
and ?
where ?
+?
= 17on a develop-ment set and kept the values with the best results:?
= 0.4, ?
= 0.6.3.3 Data used for regressionThe LF approach and its generalisation need datain order to perform the least square regression.
Wethus created a semantic space for adjective nounand noun noun vectors using the most frequentones in a similar way to how we created themin 3.2.
Then we solved the equations in 2.2 andforth.
Even though the regression data were dis-joint from the test sets, for each pair, we removedsome of the data that may cause overfitting.For the lexical function tests, we remove theadjective noun vector corresponding to the testpair from the regression data.
For example, wedo not use short word to learn SHORT for the(short word, abbrevation) pair.For the generalised lexical function tests, we usethe full regression data to learn the lexical func-tions used to train the tensor.
However, we re-move the ADJECTIVE matrix corresponding to thetest pair from the (tensor) regression data.
For ex-ample, we do not use SHORT to learn A for the(short word, abbreviation) pair.7Since the vectors are normalized (cf.
3.2), this conditiondoes not affect the generality of our test.286Table 2: Percentage of correctly classified pairs for (adjective noun1,noun2) for both French and Englishspaces.baseline multiplicative additive LF generalised LFfr en fr en fr en fr en fr ennon-reduced 0.83 0.81 0.86 0.86 0.88 0.86 N/A N/Asvd 0.79 0.79 0.55 0.59 0.84 0.78 0.93 0.92 0.91 0.88nmf 0.78 0.78 0.83 0.77 0.79 0.84 0.90 0.86 0.88 0.85(a) Negative examples are created randomly.baseline multiplicative additive LF generalised LFfr en fr en fr en fr en fr ennon-reduced 0.80 0.79 0.83 0.81 0.85 0.80 N/A N/Asvd 0.78 0.77 0.54 0.48 0.83 0.78 0.84 0.79 0.81 0.77nmf 0.78 0.78 0.79 0.78 0.83 0.82 0.82 0.82 0.81 0.80(b) Negative examples are created from existing pairs.Table 3: Percentage of correctly classified pairs for (noun2 noun1,noun3) with negative examples fromexisting pairs.
Only the English space is tested.English space baseline multiplicative additive LF generalised LFnon-reduced 0.77 0.80 0.84 N/A N/Asvd 0.78 0.49 0.86 0.83 0.82nmf 0.79 0.82 0.86 0.85 0.833.4 ResultsIn this section, we present how the various modelsperform on our test sets.3.4.1 General resultsTables 2 & 3 give an overview of the results.
Notefirst that the baseline approach, which comparesonly the two nouns and ignores the subordinateadjective or noun, does relatively well on the task(?
80% accuracy).
This reflects the fact that thehead noun in our pairs extracted from definitionsis close to (and usually a super type of) the nounto be defined.In addition, we observe that the multiplicativemethod performs badly, as expected, on the se-mantic space reduced with svd.
This confirms theincompatibility of this method with the negativevalues generated by svd.
Indeed, multiplying twovectors with negative values term by term mayyield a third vector very far away from the othertwo.
Such a combination does not support the sub-sectivity of most our test pairs.
Apart from that,svd and nmf reductions do not affect the methodsmuch.Moreover, we observe that the multiplicativemodel performs better than the baseline but isbested by the additive model.
We also see thatadditive and lexical functions often yield similarperformance.Finally, the generalised lexical function isslightly less accurate than the lexical functions.This is an expected consequence of generalisa-tion.
Nevertheless, the generalised lexical functionyields sound results confirming our intuition thatwe can represent adjective-noun (or noun-noun)combinations by one function.3.4.2 Adjective-nounWith random negative pairs (Table 2a), we ob-serve that the lexical function model obtains thebest results for the svd space.
This result is sig-nificantly better than any other method on anyof the spaces?e.g.,for French space, ?2= 33.49,p < 0.01 when compared to the additive model forthe non-reduced space which performs second.However, with non-random negative pairs (Ta-ble 2b), LF and the additive model obtain scoresthat are globally equivalent for their best respec-287tive conditions ?
in French 0.85 for the additivenon-reduced model vs. 0.84 for the LF svd model,a difference that is not significant (?2= 0.20,p < 0.05).This seems to indicate that LF is especially ef-ficient at separating out nonsense combinations.This may be caused by the fact that lexical func-tions learn from actual pairs.
Thus, when anadjective noun combination is bizarre, the ADJEC-TIVE matrix has not been optimized to interactwith the noun vector and may lead to completenon-sense ?
Which is a good thing because hu-mans would analyze the combination as such.Finally, similar results in French and Englishconfirm the intuition that distributional methods(and its composition models) are independent ofthe idiosyncrasies of a particular language; in par-ticular they are as efficient for French as for En-glish.3.4.3 Noun-nounThe noun-noun tests (Table 3) yields similar re-sults to the adjective-noun tests.
This is not sosurprising since noun noun compounds in Englishalso obey a roughly subsective property: a base-ball field is still a field (though a cricket pitch isperhaps not so obviously a pitch).
We can see thatthe accuracy increase from the baseline is highercompared to adjective-noun test on the same exactspaces (Table 2b, right values).
This may be dueto the fact that the subordinate noun in noun-nouncombinations is more important than the adjectivesubordinate in adjective-noun combination.4 Related workMany researchers have already studied and evalu-ated different composition models within a distri-butional approach.
One of the first studies eval-uating compositional phenomena in a systematicway is Mitchell and Lapata?s (2008) approach.They explore a number of different models forvector composition, of which vector addition (thesum of each feature) and vector multiplication (theelement-wise multiplication of each feature) arethe most important.
They evaluate their modelson a noun-verb phrase similarity task.
Human an-notators were asked to judge the similarity of twocomposed pairs (by attributing a certain score).The model?s task is then to reproduce the humanjudgements.
Their results show that the multi-plicative model yields the best results, along witha weighted combination of the additive and multi-plicative model.
The authors redid their study us-ing a larger test set in Mitchell and Lapata (2010)(adjective-noun composition was also included),and they confirmed their initial results.Baroni and Zamparelli (2010) evaluate theirlexical function model within a somewhat dif-ferent context.
They evaluated their modelby looking at its capacity of reconstructing theadjective noun vectors that have not been seenduring training.
Their results show that their lexi-cal function model obtains the best results for thereconstruction of the original co-occurrence vec-tors, followed by the additive model.
We observethe same tendency in our evaluation results forFrench, although our results for English show adifferent picture.
We would like to explore thisdiscordance further in future work.Grefenstette et al (2013) equally propose a gen-eralisation of the lexical function model that usestensors.
Their goal is to model transitive verbs,and the way we acquire our tensor is similar totheirs.
In fact, they use the LF approach in or-der to learn VERB OBJECT matrices that maybe multiplied by a subject vector to obtain thesubject verb object vector.
In a second step, theylearn a tensor for each individual verb, which issimilar to how we learn our adjective tensor A .Coecke et al (2010) present an abstract theo-retical framework in which a sentence vector is afunction of the Kronecker product of its word vec-tors, which allows for greater interaction betweenthe different word features.
A number of instan-tiations of the framework ?
where the key ideais that relational words (e.g.
adjectives or verbs)have a rich (multi-dimensional) structure that actsas a filter on their arguments ?
are tested exper-imentally in Grefenstette and Sadrzadeh (2011a)and Grefenstette and Sadrzadeh (2011b).
The au-thors evaluated their models using a similarity taskthat is similar to the one used by Mitchell & La-pata.
However, they use more complex compo-sitional expressions: rather than using composi-tions of two words (such as a verb and an object),they use simple transitive phrases (subject-verb-object).
They show that their instantiations of thecategorical model reach better results than the ad-ditive and multiplicative models on their transitivesimilarity task.Socher et al (2012) present a compositionalmodel based on a recursive neural network.
Each288node in a syntactic tree is assigned both a vectorand a matrix; the vector captures the actual mean-ing of the constituent, while the matrix modelsthe way it changes the meaning of neighbouringwords and phrases.
They use an extrinsic evalu-ation, using the model for a sentiment predictiontask.
They show that their model gets better re-sults than the additive, multiplicative, and lexicalfunction approach.
Other researchers, however,have published different results.
Blacoe and La-pata (2012) evaluated the additive and multiplica-tive model, as well as Socher et al?s (2012) ap-proach on two different tasks: Mitchell & Lapata?s(2010) similarity task and a paraphrase detectiontask.
They find that the additive and multiplica-tive models reach better scores than Socher et al?smodel.Tensors have been used before to model differ-ent aspects of natural language.
Giesbrecht (2010)describes a tensor factorization model for the con-struction of a distributional model that is sensitiveto word order.
And Van de Cruys (2010) uses atensor factorization model in order to construct athree-way selectional preference model of verbs,subjects, and objects.5 ConclusionWe have developed a new method of compositionand tested it in comparison with different com-position methods assuming a distributional ap-proach.
We developed a test set for French pair-ing nouns with adjective noun combinations verysimilar in meaning from the French Wiktionary.We also used an existing SEMEVAL-2013 set tocreate a similar test set for English both for ad-jective noun combination and noun noun combi-nation.
Our tests confirm that the lexical func-tion approach by Baroni and Zamparelli performswell compared to other methods of composition,but only when the negative examples are con-structed randomly.
Our generalised lexical func-tion approach fares almost equally well.
It alsohas the advantage of being constructed from au-tomatically acquired adjectival and noun vectors,and offers the additional advantage of counteringdata sparseness.
However, the lexical functionapproach claims to perform well on more subtlecases ?
e.g.
non-subsective combinations suchas stone lion.
Our test sets does not contain suchcases, and so we cannot draw any conclusion onthis claim.In future work, we would like to test differ-ent sizes of dimensionality reduction, in order tooptimize our generalised lexical function model.Moreover, it is possible that better results may beobtained by proposing multiple generalised lexi-cal functions, rather than a single one.
We could,e.g., try to separate the intersective adjectives fromnon-intersective adjectives.
And finally, we wouldlike to further explore the performance of the lex-ical function model and generalised lexical func-tion model on different datasets, which involvemore complex compositional phenomena.6 AcknowledgmentsWe thank Dinu et al (2013a) for their work on theDisSeCT toolkit8, which provides plenty of help-ful functions for composition in distributional se-mantics.
We also thank the OSIRIM platform9forallowing us to do the computations we needed.
Fi-nally, we thank the reviewers of this paper for theirinsightful comments.This work is supported by a grant overseenby the French National Research Agency ANR(ANR-14-CE24-0014).ReferencesNicholas Asher.
2011.
Lexical Meaning in Context: AWeb of Words.
Cambridge University Press.Marco Baroni and Roberto Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: Representingadjective-noun constructions in semantic space.
InProceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, pages1183?1193, Cambridge, MA, October.
Associationfor Computational Linguistics.Marco Baroni, Silvia Bernardini, Adriano Ferraresi,and Eros Zanchetta.
2009.
The wacky wide web:A collection of very large linguistically processedweb-crawled corpora.
Language Resources andEvaluation, 43(3):209?226.Christian Bassac, Bruno Mery, and Christian Retor?e.2010.
Towards a Type-theoretical account of lexicalsemantics.
Journal of Logic, Language and Infor-mation, 19(2):229?245.William Blacoe and Mirella Lapata.
2012.
A com-parison of vector-based representations for seman-tic composition.
In Proceedings of the 2012 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning, pages 546?556, Jeju Island, Korea,July.
Association for Computational Linguistics.8http://clic.cimec.unitn.it/composes/toolkit/9http://osirim.irit.fr/site/en289Marie Candito, Beno?
?t Crabb?e, Pascal Denis, et al2010.
Statistical french dependency parsing: tree-bank conversion and first results.
In Proceed-ings of the Seventh International Conference onLanguage Resources and Evaluation (LREC 2010),pages 1840?1847.Kenneth W. Church and Patrick Hanks.
1990.
Wordassociation norms, mutual information & lexicogra-phy.
Computational Linguistics, 16(1):22?29.Bob Coecke, Mehrnoosh Sadrzadeh, and StephenClark.
2010.
Mathematical foundations for a com-positional distributed model of meaning.
LambekFestschrift, Linguistic Analysis, vol.
36, 36.Pascal Denis, Beno?
?t Sagot, et al 2010.
Exploita-tion d?une ressource lexicale pour la constructiond?un ?etiqueteur morphosyntaxique ?etat-de-l?art dufranc?ais.
In Traitement Automatique des LanguesNaturelles: TALN 2010.Georgiana Dinu, Nghia The Pham, and Marco Ba-roni.
2013a.
Dissect - distributional semantics com-position toolkit.
In Proceedings of the 51st An-nual Meeting of the Association for ComputationalLinguistics: System Demonstrations, pages 31?36,Sofia, Bulgaria, August.
Association for Computa-tional Linguistics.Georgiana Dinu, Nghia The Pham, and Marco Baroni.2013b.
General estimation and evaluation of compo-sitional distributional semantic models.
In Proceed-ings of the Workshop on Continuous Vector SpaceModels and their Compositionality, pages 50?58,Sofia, Bulgaria, August.
Association for Computa-tional Linguistics.Eugenie Giesbrecht.
2010.
Towards a matrix-baseddistributional model of meaning.
In Proceedingsof the NAACL HLT 2010 Student Research Work-shop, pages 23?28.
Association for ComputationalLinguistics.Gene H. Golub and Charles F. Van Loan.
1996.
MatrixComputations (3rd Ed.).
Johns Hopkins UniversityPress, Baltimore, MD, USA.Edward Grefenstette and Mehrnoosh Sadrzadeh.2011a.
Experimental support for a categorical com-positional distributional model of meaning.
In Pro-ceedings of the 2011 Conference on Empirical Meth-ods in Natural Language Processing, pages 1394?1404, Edinburgh, Scotland, UK., July.
Associationfor Computational Linguistics.Edward Grefenstette and Mehrnoosh Sadrzadeh.2011b.
Experimenting with transitive verbs in a dis-cocat.
In Proceedings of the GEMS 2011 Workshopon GEometrical Models of Natural Language Se-mantics, pages 62?66, Edinburgh, UK, July.
Asso-ciation for Computational Linguistics.E.
Grefenstette, G. Dinu, Y.-Z.
Zhang, M. Sadrzadeh,and Baroni M. 2013.
Multi-step regression learn-ing for compositional distributional semantics.
InProceedings of the 10th International Conference onComputational Semantics (IWCS), pages 131?142,East Stroudsburg PA. Association for ComputationalLinguistics.Zellig S. Harris.
1954.
Distributional structure.
Word,10(23):146?162.Hans Kamp.
1975.
Two theories about adjectives.Formal semantics of natural language, pages 123?155.Tamara G. Kolda and Brett W. Bader.
2009.
Ten-sor decompositions and applications.
SIAM Review,51(3):455?500, September.Ioannis Korkontzelos, Torsten Zesch, Fabio MassimoZanzotto, and Chris Biemann.
2013.
Semeval-2013task 5: Evaluating phrasal semantics.
In SecondJoint Conference on Lexical and Computational Se-mantics (*SEM), Volume 2: Proceedings of the Sev-enth International Workshop on Semantic Evalua-tion (SemEval 2013), pages 39?47, Atlanta, Geor-gia, USA, June.
Association for Computational Lin-guistics.Thomas Landauer and Susan Dumais.
1997.
A so-lution to Plato?s problem: The Latent SemanticAnalysis theory of the acquisition, induction, andrepresentation of knowledge.
Psychology Review,104:211?240.Daniel D. Lee and H. Sebastian Seung.
2000.
Al-gorithms for non-negative matrix factorization.
InAdvances in Neural Information Processing Systems13, pages 556?562.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of the 36th AnnualMeeting of the Association for Computational Lin-guistics and 17th International Conference on Com-putational Linguistics (COLING-ACL98), Volume 2,pages 768?774, Montreal, Quebec, Canada.Zhaohui Luo.
2010.
Type-theoretical semantics withcoercive subtyping.
SALT20, Vancouver.Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-faella Bernardi, S Menini, and Roberto Zamparelli.2014.
Semeval-2014 task 1: Evaluation of compo-sitional distributional semantic models on full sen-tences through semantic relatedness and textual en-tailment.
In Proceedings of SemEval 2014: Interna-tional Workshop on Semantic Evaluation.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
proceedings ofACL-08: HLT, pages 236?244.J.
Mitchell and M. Lapata.
2010.
Composition in dis-tributional models of semantics.
Cognitive Science,34(8):1388?1429.J.
Nivre, J.
Hall, and J. Nilsson.
2006a.
Maltparser:A data-driven parser-generator for dependency pars-ing.
In Proceedings of LREC-2006, pages 2216?2219, Genoa, Italy.290Joakim Nivre, Johan Hall, and Jens Nilsson.
2006b.Maltparser: A data-driven parser-generator for de-pendency parsing.
In Proceedings of LREC-2006,pages 2216?2219.Barbara H Partee.
2010.
Privative adjectives: sub-sective plus coercion.
B?AUERLE, R. et ZIM-MERMANN, TE, ?editeurs: Presuppositions and Dis-course: Essays Offered to Hans Kamp, pages 273?285.Richard Socher, Brody Huval, Christopher D. Man-ning, and Andrew Y. Ng.
2012.
Semantic composi-tionality through recursive matrix-vector spaces.
InProceedings of the 2012 Joint Conference on Empir-ical Methods in Natural Language Processing andComputational Natural Language Learning, pages1201?1211, Jeju Island, Korea, July.
Association forComputational Linguistics.Kristina Toutanova and Christopher D. Manning.2000.
Enriching the knowledge sources used in amaximum entropy part-of-speech tagger.
In Pro-ceedings of the Joint SIGDAT Conference on Empir-ical Methods in Natural Language Processing andVery Large Corpora (EMNLP/VLC-2000), pages63?70.Kristina Toutanova, Dan Klein, Christopher Manning,and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proceedings of HLT-NAACL 2003, pages 252?259.Peter Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of se-mantics.
Journal of artificial intelligence research,37(1):141?188.Tim Van de Cruys.
2010.
A non-negative tensor factor-ization model for selectional preference induction.Natural Language Engineering, 16(4):417?437.Fabio Massimo Zanzotto, Ioannis Korkontzelos,Francesca Fallucchi, and Suresh Manandhar.
2010.Estimating linear models for compositional distribu-tional semantics.
In Proceedings of the 23rd Inter-national Conference on Computational Linguistics(Coling 2010), pages 1263?1271, Beijing, China,August.
Coling 2010 Organizing Committee.291
