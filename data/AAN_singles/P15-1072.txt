Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 741?751,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsA Unified Multilingual Semantic Representation of ConceptsJos?e Camacho-Collados, Mohammad Taher Pilehvar and Roberto NavigliDepartment of Computer ScienceSapienza University of Rome{collados,pilehvar,navigli}@di.uniroma1.itAbstractSemantic representation lies at the core ofseveral applications in Natural LanguageProcessing.
However, most existing se-mantic representation techniques cannotbe used effectively for the representationof individual word senses.
We put for-ward a novel multilingual concept repre-sentation, called MUFFIN, which not onlyenables accurate representation of wordsenses in different languages, but also pro-vides multiple advantages over existingapproaches.
MUFFIN represents a givenconcept in a unified semantic space irre-spective of the language of interest, en-abling cross-lingual comparison of differ-ent concepts.
We evaluate our approach intwo different evaluation benchmarks, se-mantic similarity and Word Sense Disam-biguation, reporting state-of-the-art per-formance on several standard datasets.1 IntroductionSemantic representation, i.e., the task of represent-ing a linguistic item (such as a word or a wordsense) in a mathematical or machine-interpretableform, is a fundamental problem in Natural Lan-guage Processing (NLP).
The Vector Space Model(VSM) is a prominent approach for semantic rep-resentation, with widespread popularity in numer-ous NLP applications.
The prevailing methodsfor the computation of a vector space represen-tation are based on distributional semantics (Har-ris, 1954).
However, these approaches, whetherin their conventional co-occurrence based form(Salton et al, 1975; Turney and Pantel, 2010; Lan-dauer and Dooley, 2002), or in their newer predic-tive branch (Collobert and Weston, 2008; Mikolovet al, 2013; Baroni et al, 2014), suffer from amajor drawback: they are unable to model indi-vidual word senses or concepts, as they conflatedifferent meanings of a word into a single vecto-rial representation.
This hinders the functionalityof this group of vector space models in tasks suchas Word Sense Disambiguation (WSD) that re-quire the representation of individual word senses.There have been several efforts to adapt and applydistributional approaches to the representation ofword senses (Pantel and Lin, 2002; Brody and La-pata, 2009; Reisinger and Mooney, 2010; Huanget al, 2012).
However, none of these techniquesprovides representations that are already linked toa standard sense inventory, and consequently suchmapping has to be carried out either manually,or with the help of sense-annotated data.
Chenet al (2014) addressed this issue and obtainedvectors for individual word senses by leveragingWordNet glosses.
NASARI (Camacho-Colladoset al, 2015) is another approach that obtains ac-curate sense-specific representations by combin-ing the complementary knowledge from Word-Net and Wikipedia.
Graph-based approaches havealso been successfully utilized to model individ-ual words (Hughes and Ramage, 2007; Agirre etal., 2009; Yeh et al, 2009), or concepts (Pilehvaret al, 2013; Pilehvar and Navigli, 2014), drawingon the structural properties of semantic networks.The applicability of all these techniques, however,is usually either constrained to a single language(usually English), or to a specific task.We put forward MUFFIN (Multilingual, Uni-Fied and Flexible INterpretation), a novel methodthat exploits both structural knowledge derivedfrom semantic networks and distributional statis-tics from text corpora, to produce effective rep-resentations of individual word senses or con-cepts.
Our approach provides multiple advantagesin comparison to the previous VSM techniques:1.
Multilingual: it enables sense representationin dozens of languages;2.
Unified: it represents a linguistic item, irre-spective of its language, in a unified seman-741Figure 1: Our procedure for constructing a multilingual vector representation for a concept c.tic space having concepts as its dimensions,permitting direct comparison of different rep-resentations across languages, and hence en-abling cross-lingual applications;3.
Flexible: it can be readily applied to differentNLP tasks with minimal adaptation.We evaluate our semantic representation on twodifferent tasks in lexical semantics: semantic sim-ilarity and Word Sense Disambiguation.
To as-sess the multilingual capability of our approach,we also perform experiments on languages otherthan English on both tasks, and across languagesfor semantic similarity.
We report state-of-the-artperformance on multiple datasets and settings inboth frameworks, which confirms the reliabilityand flexibility of our representations.2 MethodologyFigure 1 illustrates our procedure for construct-ing the vector representation of a given con-cept.
We use BabelNet1(version 2.5) as ourmain sense repository.
BabelNet (Navigli andPonzetto, 2012a) is a multilingual encyclopedicdictionary which merges WordNet with other lex-ical resources, such as Wikipedia and Wiktionary,thanks to its use of an automatic mapping al-gorithm.
BabelNet extends the WordNet synsetmodel to take into account multilinguality: a Ba-belNet synset contains the words that, in the vari-ous languages, express the given concept.Our approach for modeling a BabelNet synsetconsists of two main steps.
First, for the givensynset we gather contextual information fromWikipedia by exploiting knowledge from the Ba-belNet semantic network (Section 2.1).
Then, byanalyzing the corresponding contextual informa-tion and comparing and contrasting it with the1http://www.babelnet.orgwhole Wikipedia corpus, we obtain a vectorialrepresentation of the given synset (Section 2.2).2.1 A Wikipedia sub-corpus for each conceptLet c be a concept, which in our setting is a Ba-belNet synset, and let Wcbe the set containingthe Wikipedia page p corresponding to the con-cept c and all the Wikipedia pages having an out-going link to p. We further enrich Wcwith thecorresponding Wikipedia pages of the hypernymsand hyponyms of c in the BabelNet network.
Wcis the set of Wikipedia pages whose contents areexploited to build a representation for the conceptc.
We refer to the bag of content words in all theWikipedia pages inWcas the sub-corpus SCcforthe concept c.2.2 Vector construction: lexical specificityLexical specificity (Lafon, 1980) is a statisticalmeasure based on the hypergeometric distribu-tion.
Due to its efficiency in extracting a setof highly relevant words from a sub-corpus, themeasure has recently gained popularity in differ-ent NLP applications, such as textual data analy-sis (Lebart et al, 1998), term extraction (Drouin,2003), and domain-based term disambiguation(Camacho-Collados et al, 2014; Billami et al,2014).
We leverage lexical specificity to com-pute the weights in our vectors.
In our earlierwork (Camacho-Collados et al, 2015), we con-ducted different experiments which demonstratedthe improvement that lexical specificity can pro-vide over the popular term frequency-inverse doc-ument frequency weighting scheme (Jones, 1972,tf-idf ).
Lexical specificity computes the vectorweights for an item, i.e., a word or a set of words,by comparing and contrasting its contextual infor-mation with a reference corpus.
In our setting, wetake the whole Wikipedia as our reference corpusRC (we use the October 2012 Wikipedia dump).742Let T and t be the respective total number of to-kens in RC and SCc, while F and f denote thefrequency of a given item in RC and SCc, respec-tively.
Our goal is to compute a weight denotingthe association of an item to the concept c. For no-tational brevity, we use the following expressionto refer to positive lexical specificity:specificity(T, t, F, f) = ?
log10P (X ?
f) (1)where X represents a random variable followinga hypergeometric distribution of parameters F , tand T .
As we are only interested in a set ofitems that are representative of the concept be-ing modeled, we follow Billami et al (2014) andonly consider in our final vector the items whichare relevant to SCcwith a confidence higher than99% according to the hypergeometric distribution(P (X ?
f) ?
0.01).On the basis of lexical specificity we put for-ward two types of representations: lexical and uni-fied.
The lexical vector representation lexcof aconcept c has lemmas as its individual dimensions.To this end, we apply lexical specificity to everylemma in SCcin order to estimate the relevance ofeach lemma to our concept c. We use the lexicalrepresentation for the task of WSD (see Section3.2).
We describe the unified representation in thenext subsection.2.3 Unified representationUnlike the lexical version, our unified representa-tion has concepts as individual dimensions.
Algo-rithm 1 shows the construction process of a con-cept?s unified vector.
The algorithm first clusterstogether those words that have a sense sharingthe same hypernym (h in the algorithm) accordingto the BabelNet taxonomy (lines 2-4).
Next, thespecificity is computed for the set of all the hy-ponyms of h, even those that do not appear in thesub-corpus SCc(lines 6-14).
Here, F and f denotethe aggregated frequencies of all the hyponyms ofh in the whole Wikipedia (i.e., reference corpusRC) and the sub-corpus SCc, respectively.Our binding of a set of sibling words into a sin-gle cluster represented by their common hypernymprovides two advantages.
Firstly, it transforms therepresentations to a unified semantic space.
Thisspace has concepts as its dimensions, enablingtheir comparability across languages.
Secondly,the clustering can be viewed as an implicit dis-ambiguation process, whereby a set of potentiallyAlgorithm 1 Unified Vector ConstructionInput: a concept cOutput: the unified vector ucwhere uc(h) is the dimensioncorresponding to concept h1: H ?
?2: for each lemma l ?
SCc3: for each hypernym h of l in BabelNet4: H ?
H ?
{h}5: vector uc?
null vector6: for each h ?
H7: if ?
l1, l2?
SCc: l1, l2hyponyms of h and l16= l2then8: F ?
09: f ?
010: for each hyponym hypo of h11: for each lexicalization lex of hypo12: F ?
F + freq(lex,RC)13: f ?
f + freq(lex,SCc)14: uc(h)?
specificity(T, t, F, f))15: return vector ucambiguous words are disambiguated into their in-tended sense on the basis of the contextual clues ofthe neighbouring content words, resulting in moreaccurate representations of meaning.Example.
Table 1 lists the top-weighted con-cepts, represented by their relevant lexicalizations,in the unified vectors generated for the bird andmachine senses of the noun crane and for threedifferent languages.2A comparison of conceptsacross the two senses indicates the effectivenessof our representation in identifying relevant con-cepts in different languages, while guaranteeing aclear distinction between the two meanings.3 ApplicationsThanks to their VSM nature and the sense-level functionality, our concept representations arehighly flexible, allowing us to adapt and applythem to different NLP tasks with minimal adap-tation.
In this section we explain how we use ourrepresentations in the tasks of semantic similarity(Section 3.1) and WSD (Section 3.2).Associating concepts with words.
Given thatour representations are for individual word senses,a preliminary step for both tasks would be to as-sociate the set of concepts, i.e., BabelNet synsets,Cw= {c1, ..., cn} with a given word w. In thecase when w exists in the BabelNet dictionary, weobtain the set of associated senses of the word asdefined in the BabelNet sense inventory.In order to enhance the coverage in the case of2We use the sense notation of Navigli (2009): wordpnisthe nthsense of the word with part of speech p.743Crane (bird) Crane (machine)English French German English French Germanshore bird1n?famille des oiseaux1n?vogel-familie1n?lifting device1n?dispositif de levage1n?hebevorrichtung1nbird1n?limicole1n?charadrii1n?construction4nnavire1nradfahrzeug1n?wading bird1noiseau aquatique2n?vogel gattung1nplatform1nlimicole1n?lenkfahrzeug1noscine bird1ntoll?e2nwirbeltiere2nwarship1nvaisseau2nregler3n?bird genus1ngallinac?e1nfleisch1nelectric circuit1nspationef1nreisebus1n?bird family1nclasse1ntier um1nvessel2n?construction2ncharadrii1ntaxonomic group1noccurence1nreiher1nboat1n?v?ehicule3ng?uterwagen2nTable 1: Top-weighted concepts, i.e., BabelNet synsets, for the bird and machine senses of the nouncrane.
We represent each synset by one of its word senses.
Word senses marked with the same symbolacross languages correspond to the same BabelNet synset.words that are not defined in the BabelNet dic-tionary, we also exploit the so-called Wikipediapiped links.
A piped link is a hyperlink appear-ing in the body of a Wikipedia article, providing alink to another Wikipedia article.
For example, thepiped link [[dockside crane|Crane (machine)]] isa hyperlink that appears as dockside crane in thetext, but takes the user to the Wikipedia page titledCrane (machine).
These links provide Wikipediaeditors with the ability to represent a Wikipediaarticle through a suitable lexicalization that pre-serves the grammatical structure, contextual co-herency, and flow of the sentence.
This propertyprovides an effective means of obtaining a set ofconcepts for the words not covered by BabelNet.For the case of our example, the BabelNet out-of-vocabulary word w = dockside crane will havein its set of associated concepts Cwthe BabelNetsynset corresponding to the Wikipedia page titledCrane (machine).3.1 Semantic SimilarityOnce we have the set Cwof concepts associatedwith each word w, we first retrieve the set ofcorresponding unified vector representations.
Wethen follow Camacho-Collados et al (2015) anduse square-rooted Weighted Overlap (Pilehvar etal., 2013, WO) as our vector comparison method,a metric that has been shown to suit specificity-based vectors more than the conventional cosine.WO compares two vectors on the basis of theiroverlapping dimensions, which are harmonicallyweighted by their relative ranking:WO(v1, v2) =?q?O(rank(q, v1) + rank(q, v2))?1?|O|i=1(2i)?1(2)where O is the set of overlapping dimensions (i.e.concepts) between the two vectors and rank(q, vi)is the rank of dimension q in the vector vi.Finally, the similarity between two words w1and w2is calculated as the similarity of their clos-est senses, a prevailing approach in the literature(Resnik, 1995; Budanitsky and Hirst, 2006):sim(w1, w2) = maxv1?Cw1,v2?Cw2?WO(v1, v2) (3)where w1and w2can belong to different lan-guages.
This cross-lingual similarity measure-ment is possible thanks to the unified language-independent space of concepts of our semanticrepresentations.3.2 Multilingual Word Sense DisambiguationIn order to be able to apply our approach to WSD,we use the lexical vector lexcfor each concept c.The reason for our choice of lexical vectors in thissetting is that they enable a direct comparison of acandidate sense?s representation with the context,which is also in the same lexical form.
Algorithm2 summarizes the general framework of our ap-proach.
Given a target word w to disambiguate,our approach proceeds by the following steps:1.
Retrieve Cw, the set of associated conceptswith the target word w (line 1);2.
Obtain the lexical vector lexcfor each con-cept c ?
Cw(cf.
Section 2);3.
Calculate, for each candidate concept c, aconfidence score (scorec) based on the har-monic sum of the ranks of the overlappingwords between its lexical vector lexcand thecontext of the target word (line 5 in Algo-rithm 2).744Algorithm 2 MUFFIN for WSDInput: a target word w and a document d (context of w)Output: c?, the intended sense of w1: for each concept c ?
Cw2: scorec?
03: for each lemma l ?
d4: if l ?
lexcthen5: scorec?
scorec+(rank(l, lexc))?16: c??
arg maxc?Cwscorec7: return c?Thanks to the use of BabelNet, our approach isapplicable to arbitrary languages.
For the task ofWSD, we focus on two major sense inventories in-tegrated in BabelNet: Wikipedia and WordNet.Wikipedia sense inventory.
In this case, we ob-tain the set of candidate senses for a target wordby following the procedure described in the begin-ning of this Section (i.e., associating concepts withwords).
However, we do not consider those Babel-Net synsets that are not associated with Wikipediapages.WordNet sense inventory.
Similarly, when re-stricted to the WordNet inventory, we discardthose BabelNet synsets that do not contain a Word-Net synset.
In this setting, we also leverage re-lations from WordNet?s semantic network and itsdisambiguated glosses3in order to obtain a richerset of Wikipedia articles in the sub-corpus con-struction.
The enrichment of the semantic networkwith the disambiguated glosses has been shown tobe beneficial in various graph-based disambigua-tion tasks (Navigli and Velardi, 2005; Agirre andSoroa, 2009; Pilehvar et al, 2013).4 ExperimentsWe assess the reliability of MUFFIN in two stan-dard evaluation benchmarks: semantic similar-ity (Section 4.1) and Word Sense Disambiguation(Section 4.2).4.1 Semantic SimilarityAs our semantic similarity experiment we optedfor word similarity, which is one of the most pop-ular evaluation frameworks in lexical semantics.Given a pair of words, the task in word similarityis to automatically judge their semantic similarityand, ideally, this judgement should be close to thatgiven by humans.3http://wordnet.princeton.edu/glosstag.shtml4.1.1 DatasetsMonolingual.
We picked the RG-65 dataset(Rubenstein and Goodenough, 1965) as our mono-lingual word similarity dataset.
The dataset com-prises 65 English word pairs which have beenmanually annotated by several annotators accord-ing to their similarity on a scale of 0 to 4.
Wealso perform evaluations on the French (Joubarneand Inkpen, 2011) and German (Gurevych, 2005)adaptations of this dataset.Cross-lingual.
Hassan and Mihalcea (2009) de-veloped two sets of cross-lingual datasets based onthe English MC-30 (Miller and Charles, 1991) andWordSim-353 (Finkelstein et al, 2002) datasets,for four different languages: English, German,Romanian, and Arabic.
However, the construc-tion procedure they adopted, consisting of trans-lating the pairs to other languages while preserv-ing the original similarity scores, has led to incon-sistencies in the datasets.
For instance, the Span-ish dataset contains the identical pair mediodia-mediodia with a similarity score of 3.42 (in thescale [0,4]).
Additionally, the datasets containseveral orthographic errors, such as despliege andgrua (instead of despliegue and gr?ua) and incor-rect translations (e.g., the English noun implementtranslated into the Spanish verb implementar).Kennedy and Hirst (2012) proposed a more reli-able procedure that leverages two existing alignedmonolingual word similarity datasets for the con-struction of a new cross-lingual dataset.
To thisend, for each two word pairs a-b and a?-b?
in thetwo datasets, if the difference in the correspond-ing scores is greater than one, the pairs are dis-carded.
Otherwise, two new pairs a-b?
and a?-bare created with a score equal to the average of thetwo original pairs?
scores.
In the case of repeatedpairs, we merge them into a single pair with a sim-ilarity equal to their average scores.
Using thisprocedure as a basis, Kennedy and Hirst (2012)created an English-French dataset consisting of100 pairs.
We followed the same procedure andbuilt two datasets for English-German (consistingof 125 pairs) and German-French (comprising 96pairs) language pairs.44.1.2 Comparison systemsMonolingual.
We benchmark our systemagainst four other approaches that exploit4The cross-lingual datasets are available at http://lcl.uniroma1.it/sim-datasets/.745English ?
r German ?
r French ?
rMUFFIN 0.83 0.84 MUFFIN 0.77 0.76 MUFFIN 0.71 0.77SOC-PMI ?
0.61 SOC-PMI ?
0.27 SOC-PMI ?
0.19PMI ?
0.41 PMI ?
0.40 PMI ?
0.34Retrofitting 0.74 ?
Retrofitting 0.60 ?
Retrofitting 0.61 ?LSA-Wiki 0.69 0.65 ?
?
?
LSA-Wiki 0.52 0.57Wiki-wup ?
0.59 Wiki-wup ?
0.65SSA 0.83 0.86 Resnik ?
0.72NASARI 0.84 0.82 Lesk hyper ?
0.69ADW 0.87 0.81Word2Vec ?
0.84PMI-SVD ?
0.74ESA ?
0.72Table 2: Spearman (?)
and Pearson (r) correlation performance of different systems on the English,German and French RG-65 datasets.Wikipedia as their main knowledge resource:SSA5(Hassan and Mihalcea, 2011), ESA(Gabrilovich and Markovitch, 2007), Wiki-wup(Ponzetto and Strube, 2007), and LSA-Wiki(Granada et al, 2014).
We also provide results forsystems that use distributional semantics for mod-eling words, both the conventional co-occurrencebased approach, i.e., PMI-SVD (Baroni et al,2014), PMI and SOC-PMI (Joubarne and Inkpen,2011), and Retrofitting (Faruqui et al, 2015),and the newer word embeddings, i.e., Word2Vec(Mikolov et al, 2013).
For Word2Vec and PMI-SVD, we use the pre-trained models obtainedby Baroni et al (2014).6As for WordNet-basedapproaches, we report results for Resnik (Resnik,1995) and ADW (Pilehvar et al, 2013), whichtake advantage of its structural information,and Lesk hyper (Gurevych, 2005), which lever-ages definitional information in WordNet forsimilarity computation.
Finally, we also reportthe performance of our earlier work NASARI(Camacho-Collados et al, 2015), which combinesknowledge from WordNet and Wikipedia forthe English language in its setting without theWiktionary synonyms module.Cross-lingual.
We compare the performance ofour approach against the best configuration ofthe CL-MSR-2.0 system (Kennedy and Hirst,2012), which exploits Pointwise Mutual Informa-tion (PMI) on a parallel corpus obtained from5SSA involves several parameters tuned on datasets thatare constructed on the basis of MC-30 and RG-65.6We report the best configuration of the systems on theRG-65 dataset out of their 48 configurations.
The corpusused to train the models contained 2.8 billion tokens, includ-ing Wikipedia (Baroni et al, 2014).the English and French versions of WordNet.Since two of our cross-lingual datasets are newly-created, we developed three baseline systems toenable a more meaningful comparison.
To thisend, we first use Google Translate to translate thenon-English side of the dataset to the English lan-guage.
Accordingly, three state-of-the-art graph-based and corpus-based approaches were used tomeasure the similarity of the resulting Englishpairs.
As English similarity measurement systems,we opted for ADW (Pilehvar et al, 2013), and thebest predictive (Mikolov et al, 2013, Word2Vec)and co-occurrence (i.e., PMI-SVD) models ob-tained by Baroni et al (2014).7In our experi-ments we refer to these systems as pivot, sincethey use English as a pivot for computing semanticsimilarity.
As a comparison, we also show resultsfor MUFFINpivot, which is the variant of our sys-tem applied to the same automatically translatedmonolingual datasets.4.1.3 ResultsMonolingual.
We show in Table 2 the perfor-mance of different systems in terms of Spear-man and Pearson correlations on the English, Ger-man, and French RG-65 datasets.
On the Germanand French datasets, our system outperforms thecomparison systems according to both evaluationmeasures.
It achieves considerable Spearman andPearson correlation leads of 0.1 and 0.2, respec-tively, on the French dataset in comparison to thebest system.
Also on the English RG-65 dataset,our system attains competitive performance ac-cording to both Spearman and Pearson correla-7http://clic.cimec.unitn.it/composes/semantic-vectors.html746Measure FR-EN EN-DE DE-FRMUFFIN 0.83 0.76 0.83MUFFINpivot0.83 0.73 0.79ADWpivot0.80 0.73 0.72Word2Vecpivot0.75 0.69 0.77PMI-SVDpivot0.76 0.72 0.65CL-MSR-2.0 0.30 ?
?Table 3: Pearson correlation performance of dif-ferent similarity measures on the three cross-lingual RG-65 datasets.tions.
We note that most state-of-the-art systemson the dataset (e.g., ADW) are restricted to the En-glish language only.Cross-lingual.
Pearson correlation results onthe three cross-lingual RG-65 datasets are pre-sented in Table 3.
Similarly to the monolingualexperiments, our system proves highly reliablein the cross-lingual setting, improving the per-formance of the comparison systems on all threelanguage pairs.
Moreover, MUFFINpivotattainsthe best results among the pivot systems on alldatasets, confirming the reliability of our systemin the monolingual setting.
We note that since thecross-lingual datasets were built by translating theword pairs in the original English RG-65 dataset,the pivot-based comparison systems proved to behighly competitive, outperforming the CL-MSR-2.0 system by a considerable margin.4.2 Word Sense Disambiguation4.2.1 WikipediaIn this setting, we selected the SemEval 2013 all-words WSD task (Navigli et al, 2013) as our eval-uation benchmark.
The task provides datasets forfive different languages: Italian, English, French,Spanish and German.
There are on average 1123words to disambiguate in each language?s dataset.As comparison system, we provide results for thebest-performing participating system on each lan-guage.
We also show results for the state-of-the-art WSD system of Moro et al (2014, Babelfy),which relies on random walks on the BabelNet se-mantic network and a set of graph heuristic algo-rithms.
Finally, we also report results for the MostFrequent Sense (MFS) baseline provided by thetask organizers.We follow Moro et al (2014) and back off tothe MFS baseline in the case when our system?sjudgement does not meet a threshold ?.
Similarlyto Babelfy, we tuned the value of the threshold ?on the trial dataset provided by the organizers ofthe task.
We tuned ?
with step size 0.05 (hence,21 possible values in [0,1]), obtaining an optimalvalue of 0.85 in the trial set, a value which we useacross all languages.Table 4 lists the F1 percentage performanceof different systems on the five datasets of theSemEval-2013 all-words WSD task.
Despite notbeing tuned to the task, our representations pro-vide competitive results on all datasets, outper-forming the sophisticated Babelfy system on theSpanish and German languages.
The variant ofour system not utilizing the MFS information inthe disambiguation process (?
= 0), i.e., MUF-FIN?, also shows competitive results, outperform-ing the best system in the SemEval-2013 dataseton all languages.
Interestingly, MUFFIN?
proveshighly effective on the French language, surpass-ing not only the performance of our system usingthe MFS information, but also attaining the bestoverall performance.4.2.2 WordNetAs regards the WordNet disambiguation task, wetake as our benchmark the two recent SemEvalEnglish all-words WSD tasks: the SemEval-2013task on Multilingual WSD (Navigli et al, 2013)and the SemEval-2007 English Lexical Sample,SRL and All-Words task (Pradhan et al, 2007).The all-words datasets of the two tasks contain1644 instances (SemEval-2013) and 162 noun in-stances (SemEval-2007), respectively.As comparison system, we report the per-formance of the best configuration of the top-performing system in the SemEval-2013 task, i.e.,UMCC-DLSI (Guti?errez et al, 2013).
We alsoshow results for the state-of-the-art supervisedsystem (Zhong and Ng, 2010, IMS), as well asfor two graph-based approaches that are based onrandom walks on the WordNet graph (Agirre andSoroa, 2009, UKB w2w) and the BabelNet seman-tic network (Moro et al, 2014, Babelfy).
We fol-low Babelfy and also exploit the WordNet?s sensefrequency information from the SemCor sense-annotated corpus (Miller et al, 1993).
However,instead of simply backing off to the most frequentsense, we propose a more meaningful exploitationof this information.
To this end, we compute therelevance of a specific sense as the average of itsnormalized sense frequency and its corresponding747System MFS Back off Italian English French Spanish GermanMUFFIN X 81.9 84.5 71.4 85.1 83.1MUFFIN?
67.9 73.5 72.3 81.1 76.1Babelfy X 84.3 87.4 71.6 83.8 81.6Best SemEval 2013 system X 58.3 54.8 60.5 58.1 61.0MFS - 82.2 80.2 69.1 82.1 83.0Table 4: F1 percentage performance on the SemEval-2013 Multilingual WSD datasets using Wikipediaas sense inventory.score (scorecin Algorithm 2) given by our system.The sense with the highest overall relevance valueis then picked as the intended sense.Additionally, we put forward a hybrid systemthat combines our system with IMS, hence bene-fiting from the judgements made by two systemsthat utilize complementary information.
Our sys-tem makes judgements based on global contexts,whereas IMS exploits the local context of the tar-get word.
To this end, we compute the relevanceof a specific sense as the average of the normal-ized scores given by IMS and our system (scorecin Algorithm 2).
We refer to this hybrid system asMUFFIN+IMS.Table 5 reports the F1 percentage performanceof different systems on the datasets of SemEval-2013 and SemEval-2007 English all-words WSDtasks.
We also report the results for the MFS base-line, which always picks the most frequent senseof a word.
Similarly to the disambiguation taskon the Wikipedia sense inventory, MUFFIN provesto be quite competitive on the WordNet disam-biguation task, while surpassing the performanceof all the comparison systems on the SemEval-2013 dataset.
On the SemEval-2007 dataset,IMS achieves the best performance, thanks to itsusage of large amounts of manually and semi-automatically tagged data.
Finally, our hybrid sys-tem, MUFFIN+IMS, provides the best overall per-formance on the two datasets, showing that ourcombination of the two WSD systems that utilizedifferent types of knowledge was beneficial.5 Related workWe briefly review the recent literature on the twoNLP tasks to which we applied our representa-tions, i.e., Word Sense Disambiguation and se-mantic similarity.WSD.
There are two main categories of WSDtechniques: knowledge-based and supervisedSystem SemEval-2013 SemEval-2007MUFFIN 66.0 66.0UKB 61.3 56.0UMCC-DLSI 64.7 ?IMS 65.3 67.3Babelfy 65.9 62.7MFS 63.2 65.8MUFFIN+IMS 66.9 68.5Table 5: F1 percentage performance on theSemEval-2013 and SemEval-2007 (noun in-stances) English All-words WSD datatets usingWordNet as sense inventory.
(Navigli, 2009).
Supervised systems such as IMS(Zhong and Ng, 2010) analyze sense-annotateddata and model the context in which the varioussenses of a word usually appear.
Despite their ac-curacy for the words that are provided with suit-able amounts of sense-annotated data, their appli-cability is limited to those words and languagesfor which such data is available, practically limit-ing them to a small subset of words mainly in theEnglish language.
Knowledge-based approaches(Sinha and Mihalcea, 2007; Navigli and Lapata,2007; Agirre and Soroa, 2009) significantly im-prove the coverage of supervised systems.
How-ever, similarly to their supervised counterparts,knowledge-based techniques are usually limited tothe English language.Recent years have seen a growing interest incross-lingual and multilingual WSD (Lefever andHoste, 2010; Lefever and Hoste, 2013; Navigliet al, 2013).
Multilinguality is usually offeredby methods that exploit the structural informa-tion of large-scale multilingual lexical resourcessuch as Wikipedia (Guti?errez et al, 2013; Manionand Sainudiin, 2013; Hovy et al, 2013).
Babelfy(Moro et al, 2014) is an approach with state-of-the-art performance that relies on random walks748on BabelNet multilingual semantic network (Nav-igli and Ponzetto, 2012a) and densest subgraphheuristics.
However, the approach is limited to theWSD and Entity Linking tasks.
In contrast, ourapproach is global as it can be used in differentNLP tasks, including WSD.Semantic similarity.
Semantic similarity ofword pairs is usually computed either on the ba-sis of the structural properties of lexical databasesand thesauri, or by comparing vectorial represen-tations of words learned from massive text cor-pora.
Structural approaches usually measure thesimilarity on the basis of the distance informationon semantic networks, such as WordNet (Budan-itsky and Hirst, 2006), or thesauri, such as Ro-get?s (Morris and Hirst, 1991; Jarmasz and Sz-pakowicz, 2003).
The semantic network of Word-Net has also been used in more sophisticated tech-niques such as those based on random graph walks(Ramage et al, 2009; Pilehvar et al, 2013), orcoupled with the complementary knowledge fromWikipedia (Camacho-Collados et al, 2015).
How-ever, these techniques are either limited in the lan-guages to which they can be applied, or in theirapplicability to tasks other than semantic similar-ity (Navigli and Ponzetto, 2012b).Corpus-based techniques are more flexible, en-abling the training of models on corpora otherthan English.
However, these approaches, eitherin their conventional co-occurrence based form(Gabrilovich and Markovitch, 2007; Landauer andDumais, 1997; Turney and Pantel, 2010; Bulli-naria and Levy, 2012), or the more recent predic-tive models (Mikolov et al, 2013; Collobert andWeston, 2008; Pennington et al, 2014), are re-stricted in two ways: (1) they cannot be used tocompare word senses; and (2) they cannot be di-rectly applied to cross-lingual semantic similar-ity.
Though the first problem has been solvedby multi-prototype models (Huang et al, 2012),or by the sense-specific representations obtainedas a result of exploiting WordNet glosses (Chenet al, 2014), the second problem remains unad-dressed.
In contrast, our approach models wordsenses and concepts effectively, while providing aunified representation for different languages thatenables cross-lingual semantic similarity.6 ConclusionsThis paper presented MUFFIN, a new multilingual,unified and flexible representation of individualword senses.
Thanks to its effective combinationof distributional statistics and structured knowl-edge, the approach can compute efficient represen-tations of arbitrary word senses, with high cover-age and irrespective of their language.
We eval-uated our representations on two different NLPtasks, i.e., semantic similarity and Word SenseDisambiguation, reporting state-of-the-art perfor-mance on several datasets.
Experimental resultsdemonstrated the reliability of our unified repre-sentation approach, while at the same time alsohighlighting its main advantages: multilinguality,owing to its effective application within and acrossmultiple languages; and flexibility, owing to its ro-bust performance on two different tasks.AcknowledgmentsThe authors gratefully acknowledgethe support of the ERC StartingGrant MultiJEDI No.
259234.ReferencesEneko Agirre and Aitor Soroa.
2009.
PersonalizingPageRank for Word Sense Disambiguation.
In Pro-ceedings of EACL, pages 33?41.Eneko Agirre, Enrique Alfonseca, Keith Hall, JanaKravalova, Marius Pas?ca, and Aitor Soroa.
2009.A study on similarity and relatedness using distribu-tional and WordNet-based approaches.
In Proceed-ings of NAACL, pages 19?27.Marco Baroni, Georgiana Dinu, and Germ?anKruszewski.
2014.
Don?t count, predict!
asystematic comparison of context-counting vs.context-predicting semantic vectors.
In Proceedingsof ACL, pages 238?247.Mokhtar-Boumeyden Billami, Jos?e Camacho-Collados, Evelyne Jacquey, and Laurence Kister.2014.
Semantic annotation and terminology val-idation in full scientific articles in social sciencesand humanities (annotation s?emantique et validationterminologique en texte int?egral en shs) [in french].In Proceedings of TALN 2014, pages 363?376.Samuel Brody and Mirella Lapata.
2009.
BayesianWord Sense Induction.
In Proceedings of EACL,pages 103?111.Alexander Budanitsky and Graeme Hirst.
2006.Evaluating WordNet-based measures of Lexical Se-mantic Relatedness.
Computational Linguistics,32(1):13?47.John A. Bullinaria and Joseph P. Levy.
2012.
Ex-tracting semantic representations from word co-749occurrence statistics: stop-lists, stemming, andSVD.
Behavior Research Methods, 44(3):890?907.Jos?e Camacho-Collados, Mokhtar Billami, EvelyneJacquey, and Laurence Kister.
2014.
Approchestatistique pour le filtrage terminologique des oc-currences de candidats termes en texte int?egral.
InJADT, pages 121?133.Jos?e Camacho-Collados, Mohammad Taher Pilehvar,and Roberto Navigli.
2015.
NASARI: a Novel Ap-proach to a Semantically-Aware Representation ofItems.
In Proceedings of NAACL, pages 567?577.Xinxiong Chen, Zhiyuan Liu, and Maosong Sun.
2014.A unified model for word sense representation anddisambiguation.
In Proceedings of EMNLP, pages1025?1035.Ronan Collobert and Jason Weston.
2008.
A unifiedarchitecture for natural language processing: Deepneural networks with multitask learning.
In Pro-ceedings of ICML, pages 160?167.Patrick Drouin.
2003.
Term extraction using non-technical corpora as a point of leverage.
Terminol-ogy, 9(1):99?115.Manaal Faruqui, Jesse Dodge, Sujay K. Jauhar, ChrisDyer, Eduard Hovy, and Noah A. Smith.
2015.Retrofitting word vectors to semantic lexicons.
InProceedings of NAACL, pages 1606?1615.Lev Finkelstein, Gabrilovich Evgeniy, Matias Yossi,Rivlin Ehud, Solan Zach, Wolfman Gadi, and Rup-pin Eytan.
2002.
Placing search in context: Theconcept revisited.
ACM Transactions on Informa-tion Systems, 20(1):116?131.Evgeniy Gabrilovich and Shaul Markovitch.
2007.Computing semantic relatedness using Wikipedia-based explicit semantic analysis.
In Proceedings ofIJCAI, pages 1606?1611.Roger Granada, Cassia Trojahn, and Renata Vieira.2014.
Comparing semantic relatedness betweenword pairs in Portuguese using Wikipedia.
In Com-putational Processing of the Portuguese Language,pages 170?175.Iryna Gurevych.
2005.
Using the structure of a con-ceptual network in computing semantic relatedness.In Proceedings of IJCNLP, pages 767?778.Yoan Guti?errez, Yenier Casta?neda, Andy Gonz?alez,Rainel Estrada, D. Dennys Piug, I. Jose Abreu,Roger P?erez, Antonio Fern?andez Orqu?
?n, Andr?esMontoyo, Rafael Mu?noz, and Franc Camara.
2013.UMCC DLSI: Reinforcing a ranking algorithm withsense frequencies and multidimensional semanticresources to solve multilingual word sense disam-biguation.
In Proceedings of SemEval 2013, pages241?249.Zellig Harris.
1954.
Distributional structure.
Word,10:146?162.Samer Hassan and Rada Mihalcea.
2009.
Cross-lingual semantic relatedness using encyclopedicknowledge.
In Proceedings of EMNLP, pages 1192?1201.Samer Hassan and Rada Mihalcea.
2011.
Semanticrelatedness using salient semantic analysis.
In Pro-ceedings of AAAI, pages 884,889.Eduard H. Hovy, Roberto Navigli, and Simone PaoloPonzetto.
2013.
Collaboratively built semi-structured content and Artificial Intelligence: Thestory so far.
Artificial Intelligence, 194:2?27.Eric H. Huang, Richard Socher, Christopher D. Man-ning, and Andrew Y. Ng.
2012.
Improving wordrepresentations via global context and multiple wordprototypes.
In Proceedings of ACL, pages 873?882.Thad Hughes and Daniel Ramage.
2007.
Lexical se-mantic relatedness with random graph walks.
InProceedings of EMNLP-CoNLL, pages 581?589.Mario Jarmasz and Stan Szpakowicz.
2003.
Roget?sthesaurus and semantic similarity.
In Proceedingsof RANLP, pages 212?219.Karen Sp?arck Jones.
1972.
A statistical interpretationof term specificity and its application in retrieval.Journal of Documentation, 28:11?21.Colette Joubarne and Diana Inkpen.
2011.
Compar-ison of semantic similarity for different languagesusing the Google n-gram corpus and second-orderco-occurrence measures.
In Advances in ArtificialIntelligence, pages 216?221.Alistair Kennedy and Graeme Hirst.
2012.
Measuringsemantic relatedness across languages.
In Proceed-ings of xLiTe: Cross-Lingual Technologies Work-shop at the Neural Information Processing SystemsConference.Pierre Lafon.
1980.
Sur la variabilit?e de la fr?equencedes formes dans un corpus.
Mots, 1:127?165.Tom Landauer and Scott Dooley.
2002.
Latent seman-tic analysis: theory, method and application.
In Pro-ceedings of CSCL, pages 742?743.Thomas K Landauer and Susan T Dumais.
1997.
Asolution to Plato?s problem: The latent semanticanalysis theory of acquisition, induction, and rep-resentation of knowledge.
Psychological Review,104(2):211.Ludovic Lebart, A Salem, and Lisette Berry.
1998.
Ex-ploring textual data.
Kluwer Academic Publishers.Els Lefever and Veronique Hoste.
2010.
SemEval-2010 Task 3: Cross-lingual Word Sense Disam-biguation.
In Proceedings of SemEval 2010, pages82?87, Uppsala, Sweden.Els Lefever and Veronique Hoste.
2013.
SemEval-2013 Task 10: Cross-lingual Word Sense Disam-biguation.
In Proceedings of SemEval 2013, pages158?166, Atlanta, USA.750Steve L. Manion and Raazesh Sainudiin.
2013.
Dae-bak!
: Peripheral diversity for multilingual WordSense Disambiguation.
In Proceedings of SemEval2013, pages 250?254.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient estimation of word represen-tations in vector space.
CoRR, abs/1301.3781.George A. Miller and Walter G. Charles.
1991.
Con-textual correlates of semantic similarity.
Languageand Cognitive Processes, 6(1):1?28.George A. Miller, Claudia Leacock, Randee Tengi, andRoss Bunker.
1993.
A semantic concordance.
InProceedings of the 3rd DARPA Workshop on HumanLanguage Technology, pages 303?308, Plainsboro,N.J.Andrea Moro, Alessandro Raganato, and Roberto Nav-igli.
2014.
Entity Linking meets Word Sense Dis-ambiguation: a Unified Approach.
Transactionsof the Association for Computational Linguistics(TACL), 2:231?244.Jane Morris and Graeme Hirst.
1991.
Lexical cohe-sion computed by thesaural relations as an indicatorof the structure of text.
Computational Linguistics,17(1).Roberto Navigli and Mirella Lapata.
2007.
Graphconnectivity measures for unsupervised Word SenseDisambiguation.
In Proceedings of IJCAI, pages1683?1688.Roberto Navigli and Simone Paolo Ponzetto.
2012a.BabelNet: The automatic construction, evaluationand application of a wide-coverage multilingual se-mantic network.
Artificial Intelligence, 193:217?250.Roberto Navigli and Simone Paolo Ponzetto.
2012b.BabelRelate!
a joint multilingual approach to com-puting semantic relatedness.
In Proceedings ofAAAI, pages 108?114.Roberto Navigli and Paola Velardi.
2005.
Struc-tural Semantic Interconnections: a knowledge-basedapproach to Word Sense Disambiguation.
IEEETransactions on Pattern Analysis and Machine In-telligence, 27(7):1075?1088.Roberto Navigli, David Jurgens, and Daniele Vannella.2013.
SemEval-2013 Task 12: Multilingual WordSense Disambiguation.
In Proceedings of SemEval2013, pages 222?231.Roberto Navigli.
2009.
Word Sense Disambiguation:A survey.
ACM Computing Surveys, 41(2):1?69.Patrick Pantel and Dekang Lin.
2002.
Discoveringword senses from text.
In Proceedings of KDD,pages 613?619.Jeffrey Pennington, Richard Socher, and Christopher DManning.
2014.
GloVe: Global vectors for wordrepresentation.
In Proceedings of EMNLP, pages1532?1543.Mohammad Taher Pilehvar and Roberto Navigli.
2014.A robust approach to aligning heterogeneous lexicalresources.
In Proceedings of ACL, pages 468?478.Mohammad Taher Pilehvar, David Jurgens, andRoberto Navigli.
2013.
Align, Disambiguate andWalk: a Unified Approach for Measuring Seman-tic Similarity.
In Proceedings of ACL, pages 1341?1351.Simone Paolo Ponzetto and Michael Strube.
2007.Knowledge derived from Wikipedia for computingsemantic relatedness.
Journal of Artificial Intelli-gence Research (JAIR), 30:181?212.Sameer Pradhan, Edward Loper, Dmitriy Dligach, andMartha Palmer.
2007.
SemEval-2007 task-17: En-glish lexical sample, SRL and all words.
In Pro-ceedings of SemEval, pages 87?92.Daniel Ramage, Anna N. Rafferty, and Christopher D.Manning.
2009.
Random walks for text semanticsimilarity.
In Proceedings of the 2009 Workshop onGraph-based Methods for Natural Language Pro-cessing, pages 23?31.Joseph Reisinger and Raymond J. Mooney.
2010.Multi-prototype vector-space models of word mean-ing.
In Proceedings of ACL, pages 109?117.Philip Resnik.
1995.
Using information content toevaluate semantic similarity in a taxonomy.
In Pro-ceedings of IJCAI, pages 448?453.Herbert Rubenstein and John B. Goodenough.
1965.Contextual correlates of synonymy.
Communica-tions of the ACM, 8(10):627?633.Gerard Salton, A. Wong, and C. S. Yang.
1975.
Avector space model for automatic indexing.
Com-munications of the ACM, 18(11):613?620.Ravi Sinha and Rada Mihalcea.
2007.
Unsuper-vised graph-based Word Sense Disambiguation us-ing measures of word semantic similarity.
In Pro-ceedings of ICSC, pages 363?369.Peter D. Turney and Patrick Pantel.
2010.
Fromfrequency to meaning: Vector space models of se-mantics.
Journal of Artificial Intelligence Research,37:141?188.Eric Yeh, Daniel Ramage, Christopher D. Manning,Eneko Agirre, and Aitor Soroa.
2009.
WikiWalk:random walks on Wikipedia for semantic related-ness.
In Proceedings of the Workshop on Graph-based Methods for Natural Language Processing,pages 41?49.Zhi Zhong and Hwee Tou Ng.
2010.
It Makes Sense:A wide-coverage Word Sense Disambiguation sys-tem for free text.
In Proceedings of the ACL SystemDemonstrations, pages 78?83.751
