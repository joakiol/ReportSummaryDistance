Proceedings of the 2010 Workshop on Applications of Tree Automata in Natural Language Processing, ACL 2010, pages 10?18,Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational LinguisticsA Decoder forProbabilistic Synchronous Tree Insertion GrammarsSteve DeNeefe ?
and Kevin Knight ?USC Information Sciences Institute4676 Admiralty WayMarina del Rey, CA 90292, USA{sdeneefe,knight}@isi.eduHeiko Vogler ?Department of Computer ScienceTechnische Universita?t DresdenD-01062 DresdenHeiko.Vogler@tu-dresden.deAbstractSynchronous tree insertion grammars(STIG) are formal models for syntax-based machine translation.
We formal-ize a decoder for probabilistic STIG; thedecoder transforms every source-languagestring into a target-language tree and cal-culates the probability of this transforma-tion.1 IntroductionTree adjoining grammars (TAG) were invented in(Joshi et al 1975) in order to better character-ize the string sets of natural languages1.
One ofTAG?s important features is the ability to introducetwo related syntactic units in a single rule, thenpush those two units arbitrarily far apart in sub-sequent derivation steps.
For machine translation(MT) between two natural languages, each beinggenerated by a TAG, the derivations of the twoTAG may be synchronized (Abeille et al, 1990;Shieber and Shabes, 1990) in the spirit of syntax-directed transductions (Lewis and Stearns, 1968);this results in synchronous TAG (STAG).
Recently,in (Nesson et al, 2005, 2006) probabilistic syn-chronous tree insertion grammars (pSTIG) werediscussed as model of MT; a tree insertion gram-mar is a particular TAG in which the parsing prob-lem is solvable in cubic-time (Schabes and Wa-ters, 1994).
In (DeNeefe, 2009; DeNeefe andKnight 2009) a decoder for pSTIG has been pro-posed which transforms source-language stringsinto (modifications of) derivation trees of thepSTIG.
Nowadays, large-scale linguistic STAGrule bases are available.In an independent tradition, the automata-theoretic investigation of the translation of trees?
financially supported by NSF STAGES project, grant#IIS-0908532.?
financially supported by DFG VO 1011/5-1.1see (Joshi and Shabes, 1997) for a surveyled to the rich theory of tree transducers (Ge?csegand Steinby, 1984, 1997).
Roughly speaking, atree transducer is a finite term rewriting system.
Ifeach rewrite rule carries a probablity or, in gen-eral, a weight from some semiring, then they areweighted tree transducers (Maletti, 2006, 2006a;Fu?lo?p and Vogler, 2009).
Such weighted treetransducers have also been used for the specifi-cation of MT of natural languages (Yamada andKnight, 2001; Knight and Graehl, 2005; Graehl etal., 2008; Knight and May 2009).Martin and Vere (1970) and Schreiber (1975)established the first connections between the twotraditions; also Shieber (2004, 2006) and Maletti(2008, 2010) investigated their relationship.The problem addressed in this paper is thedecoding of source-language strings into target-language trees where the transformation is de-scribed by a pSTIG.
Currently, this decoding re-quires two steps: first, every source string istranslated into a derivation tree of the underly-ing pSTIG (DeNeefe, 2009; DeNeefe and Knight2009), and second, the derivation tree is trans-formed into the target tree using an embedded treetransducer (Shieber, 2006).
We propose a trans-ducer model, called a bottom-up tree adjoiningtransducer, which performs this decoding in a sin-gle step and, simultaneously, computes the prob-abilities of its derivations.
As a basis of our ap-proach, we present a formal definition of pSTIG.2 PreliminariesFor two sets ?
and A, we let U?
(A) be the set ofall (unranked) trees over ?
in which also elementsof A may label leaves.
We abbreviate U?(?)
byU?.
We denote the set of positions, leaves, andnon-leaves of ?
?
U?
by pos(?)
?
N?, lv(?
), andnlv(?
), resp., where ?
denotes the root of ?
andw.i denotes the ith child of position w; nlv(?)
=pos(?)
\ lv(?).
For a position w ?
pos(?
), the la-bel of ?
at w (resp., subtree of ?
at w) is denoted10by ?
(w) (resp., ?|w).
If additionally ?
?
U?
(A),then ?[?
]w denotes the tree which is obtained from?
by replacing its subtree at w by ?.
For every?
?
?
?A, the set pos?(?)
is the set of all thosepositions w ?
pos(?)
such that ?
(w) ?
?.
Simi-larly, we can define lv?(?)
and nlv?(?).
The yieldof ?
is the sequence yield(?)
?
(?
?
A)?
of sym-bols that label the leaves from left to right.If we associate with ?
?
?
a rank k ?
N, thenwe require that in every tree ?
?
U?
(A) every ?-labeled position has exactly k children.3 Probabilistic STAG and STIGFirst we will define probabilistic STAG, and sec-ond, as a special case, probabilistic STIG.Let N and T be two disjoint sets of, resp., non-terminals and terminals.
A substitution rule r is atuple (?s, ?t, V,W, P radj) where?
?s, ?t ?
UN (T ) (source and target tree) and|lvN (?s)| = |lvN (?t)|,?
V ?
lvN (?s)?lvN (?t) (substitution sites), Vis a one-to-one relation, and |V | = |lvN (?s)|,?
W ?
nlvN (?s)?nlvN (?t) (potential adjoin-ing sites), and?
P radj : W ?
[0, 1] (adjoining probability).An auxiliary rule r is a tuple (?s, ?t, V,W, ?, P radj)where ?s, ?t,W , and P radj are defined as above and?
V is defined as above except that |V | =|lvN (?s)| ?
1 and?
?
= (?s, ?t) ?
lvN (?s)?
lvN (?t) and neither?s nor ?t occurs in any element of V ; more-over, ?s(?)
= ?s(?s) and ?t(?)
= ?t(?t), and?s 6= ?
6= ?t; the node ?s (and ?t) is calledthe foot-node of ?s (resp., ?t).An (elementary) rule is either a substitution ruleor an auxiliary rule.
The root-category of a rule ris the tuple (?s(?
), ?t(?
)), denoted by rc(r).A probabilistic synchronous tree ad-joining grammar (pSTAG) is a tupleG = (N,T, (Ss, St),S,A, P ) such that Nand T are two disjoint sets (resp., of nonterminalsand terminals), (Ss, St) ?
N?N (start nontermi-nal), S and A are finite sets of, resp., substitutionrules and auxiliary rules, and P : S ?
A ?
[0, 1]such that for every (A,B) ?
N ?N ,?r?Src(r)=(A,B)P (r) = 1 and?r?Arc(r)=(A,B)P (r) = 1assuming that in each case the number of sum-mands is not zero.
In the following, let G alwaysdenote an arbitrary pSTAG.Ss?
A?
A?ar1??StB?B?
?aP (r1) = 1P r1adj(a) = .9AA ?b,c?r2?
?BB?Bbc ?P (r2) = .4P r2adj(b) = .2P r2adj(c) = .6AA A?d?
er3??B?
Bd,e?P (r3) = .6P r3adj(d) = .3P r3adj(e) = .8A?r4?
?B?P (r4) = .1A?r5?
?B?P (r5) = .9Figure 1: The running example pSTAG G.In Fig.
1 we show the rules of our running ex-ample pSTAG, where the capital Roman letters arethe nonterminals and the small Greek letters arethe terminals.
The substitution site (in rule r1) isindicated by ?, and the potential adjoining sites aredenoted2 by a, b, c, d, and e. For instance, in for-mal notation the rules r1 and r2 are written as fol-lows:r1 = (Ss(?,A,A(?
)), St(B(?
), B, ?
), {?
}, {a}, P r1adj)where ?
= (2, 2) and a = (3, 1), andr2 = (A(A, ?
), B(B(?
), B), ?, {b, c}, ?, P r2adj)where b = (?, ?
), c = (?, 1), and ?
= (1, 2).In the derivation relation of G we will distin-guish four types of steps:1. substitution of a rule at a substitution site(substitution),2. deciding to turn a potential adjoining site intoan activated adjoining site (activation),3. deciding to drop a potential adjoining site,i.e., not to adjoin, (non-adjoining) and4.
adjoining of a rule at an activated adjoiningsite (adjoining).In the sentential forms (defined below) we willmaintain for every adjoining site w a two-valuedflag g(w) indicating whether w is a potential(g(w) = p) or an activated site (g(w) = a).The set of sentential forms ofG is the set SF(G)of all tuples ?
= (?s, ?t, V,W, g) with2Their placement (as left or right index) does not play arole yet, but will later when we introduce pSTIG.11?
?s, ?t ?
UN (T ),?
V ?
lvN (?s) ?
lvN (?t) is a one-to-one rela-tion, |V | = |lvN (?s)| = |lvN (?t)|,?
W ?
nlvN (?s)?
nlvN (?t), and?
g : W ?
{p, a}.The derivation relation (of G) is the binaryrelation ?
?
SF(G) ?
SF(G) such thatfor every ?1 = (?1s , ?1t , V1,W1, g1) and ?2 =(?2s , ?2t , V2,W2, g2) we have ?1 ?
?2 iff one ofthe following is true:1.
(substitution) there are w = (ws, wt) ?
V1and r = (?s, ?t, V,W, P radj) ?
S such that?
(?1s (ws), ?1t (wt)) = rc(r),?
?2s = ?1s [?s]ws and ?2t = ?1t [?t]wt ,?
V2 = (V1 \ {w}) ?
w.V ,3?
W2 = W1 ?
w.W , and?
g2 is the union of g1 and the set of pairs(w.u, p) for every u ?W ;this step is denoted by ?1w,r=?
?2;2.
(activation) there is a w ?
W1 with g1(w) =p and (?1s , ?1t , V1,W1) = (?2s , ?2t , V2,W2),and g2 is the same as g1 except that g2(w) =a; this step is denoted by ?1w=?
?2;3.
(non-adjoining) there is w ?
W1 withg1(w) = p and (?1s , ?1t , V1) = (?2s , ?2t , V2),W2 = W1 \ {w}, and g2 is g1 restricted toW2; this step is denoted by ?1?w=?
?2;4.
(adjoining) there are w ?
W1 with g1(w) =a, and r = (?s, ?t, V,W, ?, P radj) ?
A suchthat, for w = (ws, wt),?
(?1s (ws), ?1t (wt)) = rc(r),?
?2s = ?1s [?
?s]ws where ?
?s = ?s[?1s |ws ]?s ,?2t = ?1t [?
?t]wt where ?
?t = ?t[?1t |wt ]?t ,?
V2 is the smallest set such that (i) forevery (us, ut) ?
V1 we have (u?s, u?t) ?
V2whereu?s ={us if ws is not a prefix of us,ws.
?s .u if us = ws.u for some u;and u?t is obtained in the same way from ut,wt, and ?t, and (ii) V2 contains w.V ;?
W2 is the smallest set such that (i) for every(us, ut) ?
W1 \ {w} we have (u?s, u?t) ?W2 where u?s and u?t are obtained in thesame way as for V2, and g2(u?s, u?t) =g1(us, ut) and (ii) W2 contains w.W andg2(w.u) = p for every u ?W ;this step is denoted by ?1w,r=?
?2.3w.V = {(ws.vs, wt.vt) | (vs, vt) ?
V }In Fig.
2 we show a derivation of our runningexample pSTAG where activated adjoining sitesare indicated by surrounding circles, the other ad-joining sites are potential.Ss?
??
St?substitution ofr1 at (?, ?)=?
P (r1) = 1Ss?
A?
A?a ??StB?B?
?asubstitution ofr4 at (2, 2)=?
P (r4) = .1Ss?
A?A?a ??StB?B?
?aactivationat a = (3, 1)=?
P r1adj(a) = .9Ss?
A?A?a ??StB?B?
?aadjoining ofr2 at a = (3, 1)=?
P (r2) = .4Ss?
A?AA?
?b,c ??StBB?B?B?
?bcnon-adjoiningat c = (3, 1.1)=?
1?
P r2adj(c) = .4Ss?
A?AA?
?b ??StBB?B?B?
?bnon-adjoiningat b = (3, 1)=?
1?
P r2adj(b) = .8Ss?
A?AA????StBB?B?B?
?Figure 2: An example derivation with total proba-bility 1?
.1?
.9?
.4?
.4?
.8 = .01152.The only initial sentential form is ?in =(Ss, St, {(?, ?
)}, ?, ?).
A sentential form ?
is finalif it has the form (?s, ?t, ?, ?, ?).
Let ?
?
SF(G).A derivation (of ?)
is a sequence d of the form?0u1?1 .
.
.
un?n with ?0 = ?in and n ?
0,?i?1ui?
?i for every 1 ?
i ?
n (and ?n = ?).
We12denote ?n also by last(d), and the set of all deriva-tions of ?
(resp., derivations) by D(?)
(resp., D).We call d ?
D successful if last(d) is final.The tree transformation computed by G isthe relation ?G ?
UN (T ) ?
UN (T ) with(?s, ?t) ?
?G iff there is a successful derivationof (?s, ?t, ?, ?, ?
).Our definition of the probability of a deriva-tion is based on the following observation.4 Letd ?
D(?)
for some ?
= (?s, ?t, V,W, g).
Then,for every w ?
W , the rule which created w andthe corresponding local position in that rule canbe retrieved from d. Let us denote this rule byr(d, ?, w) and the local position by l(d, ?, w).Now let d be the derivation ?0u1?1 .
.
.
un?n.Then the probability of d is defined byP (d) =?1?i?nPd(?i?1ui?
?i)where1.
(substitution) Pd(?i?1 w,r=?
?i) = P (r)2.
(activation)Pd(?i?1 w=?
?i) = P r?adj(w?)
where r?
=r(d, ?i?1, w) and w?
= l(d, ?i?1, w)3.
(non-adjoining)Pd(?i?1 ?w=?
?i) = 1 ?
P r?adj(w?)
where r?and w?
are defined as in the activation case4.
(adjoining)Pd(?i?1w,r=?
?i) = P (r).In order to describe the generative model ofG, we impose a deterministic strategy sel on thederivation relation in order to obtain, for everysentential form, a probability distribution amongthe follow-up sentential forms.
A deterministicderivation strategy is a mapping sel : SF(G) ?(N?
?
N?)
?
{?}
such that for every ?
=(?s, ?t, V,W, g) ?
SF(G), we have that sel(?)
?V ?W if V ?W 6= ?, and sel(?)
= ?
otherwise.In other words, sel chooses the next site to operateon.
Then we define ?sel in the same way as ?
butin each of the cases we require that w = sel(?1).Moreover, for every derivation d ?
D, we denoteby next(d) the set of all derivations of the formdu?
where last(d) u?sel ?.The generative model of G comprises all thegenerative stories of G. A generative story is atree t ?
UD; the root of t is labeled by ?in.
Letw ?
pos(t) and t(w) = d. Then either w is aleaf, because we have stopped the generative story4We note that a different definition occurs in (Nesson etal., 2005, 2006).at w, or w has |next(d)| children, each one repre-sents exactly one possible decision about how toextend d by a single derivation step (where theirorder does not matter).
Then, for every generativestory t, we have that?w?lv(t)P (t(w)) = 1 .We note that (D,next, ?)
can be considered asa discrete Markov chain (cf., e.g.
(Baier et al,2009)) where the initial probability distribution?
: D ?
[0, 1] maps d = ?in to 1, and all theother derivations to 0.A probabilistic synchronous tree insertiongrammar (pSTIG) G is a pSTAG except thatfor every rule r = (?s, ?t, V,W, P radj) or r =(?s, ?t, V,W, ?, P radj) we have that?
if r ?
A, then |lv(?s)| ?
2 and |lv(?t)| ?
2,?
for ?
= (?s, ?t) we have that ?s is either therightmost leaf of ?s or its leftmost one; thenwe call r, resp., L-auxiliary in the source andR-auxiliary in the source; similarly, we re-strict ?t; the source-spine of r (target-spineof r) is the set of prefixes of ?s (resp., of ?t)?
W ?
nlvN (?s)?
{L,R}?nlvN (?t)?
{L,R}where the new components are the direction-type of the potential adjoining site, and?
for every (ws, ?s, wt, ?t) ?
W , if ws lies onthe source-spine of r and r is L-auxiliary (R-auxiliary) in the source, then ?s = L (resp.,?s = R), and corresponding restrictions holdfor the target component.According to the four possibilities for the foot-node ?
we call r LL-, LR-, RL-, or RR-auxiliary.The restriction for the probability distribution P ofG is modified such that for every (A,B) ?
N?Nand x, y ?
{L,R}:?r?A, rc(r)=(A,B)r is xy?auxiliaryP (r) = 1 .In the derivation relation of the pSTIG G wewill have to make sure that the direction-type ofthe chosen adjoining site w matches with the typeof auxiliarity of the auxiliary rule.
Again we as-sume that the data structure SF(G) is enrichedsuch that for every potential adjoining site w of?
?
SF(G) we know its direction-type dir(w).We define the derivation relation of the pSTIGG to be the binary relation ?I ?
SF(G)?SF(G)such that we have ?1 ?I ?2 iff (i) ?1 ?
?2 and13(ii) if adjoining takes place atw, then the used aux-iliary rule must be dir(w)-auxiliary.
Since ?I isa subset of ?, the concepts of derivation, success-ful derivation, and tree transformation are definedalso for a pSTIG.In fact, our running example pSTAG in Fig.
1 isa pSTIG, where r2 and r3 are RL-auxiliary andevery potential adjoining site has direction-typeRL; the derivation shown in Fig.
2 is a pSTIG-derivation.4 Bottom-up tree adjoining transducerHere we introduce the concept of a bottom-up treeadjoining transducer (BUTAT) which will be usedto formalize a decoder for a pSTIG.A BUTAT is a finite-state machine which trans-lates strings into trees.
The left-hand side of eachrule is a string over terminal symbols and state-variable combinations.
A variable is either a sub-stitution variable or an adjoining variable; a substi-tution variable (resp., adjoining variable) can havean output tree (resp., output tree with foot node) asvalue.
Intuitively, each variable value is a transla-tion of the string that has been reduced to the cor-responding state.
The right-hand side of a rule hasthe form q(?)
where q is a state and ?
is an outputtree (with or without foot-node); ?
may contain thevariables from the left-hand side of the rule.
Eachrule has a probability p ?
[0, 1].In fact, BUTAT can be viewed as the string-to-tree version of bottom-up tree transducers (En-gelfriet, 1975; Gecseg and Steinby, 1984,1997) inwhich, in addition to substitution, adjoining is al-lowed.Formally, we let X = {x1, x2, .
.
.}
and F ={f1, f2, .
.
.}
be the sets of substitution variablesand adjoining variables, resp.
Each substitu-tion variable (resp., adjoining variable) has rank0 (resp., 1).
Thus when used in a tree, substitu-tion variables are leaves, while adjoining variableshave a single child.A bottom-up tree adjoining transducer (BU-TAT) is a tuple M = (Q,?,?, Qf , R) where?
Q is a finite set (of states),?
?
is an alphabet (of input symbols), assumingthat Q ?
?
= ?,?
?
is an alphabet (of output symbols),?
Qf ?
Q (set of final states), and?
R is a finite set of rules of the form?0 q1(z1) ?1 .
.
.
qk(zk) ?kp?
q(?)
(?
)where p ?
[0, 1] (probability of (?
)), k ?
0,?0, ?1, .
.
.
, ?k ?
?
?, q, q1, .
.
.
, qk ?
Q,z1, .
.
.
, zk ?
X ?
F , and ?
?
RHS(k)where RHS(k) is the set of all trees over?
?
{z1, .
.
.
, zk} ?
{?}
in which the nullary?
occurs at most once.The set of intermediate results of M is the setIR(M) = {?
| ?
?
U?({?
}), |pos{?}(?
)| ?
1}and the set of sentential forms of M is the setSF(M) = (?
?
{q(?)
| q ?
Q, ?
?
IR(M)})?.The derivation relation induced by M is the bi-nary relation ?
?
SF(M) ?
SF(M) such thatfor every ?1, ?2 ?
SF(M) we define ?1 ?
?2 iffthere are ?, ??
?
SF(M), there is a rule of the form(?)
in R, and there are ?1, .
.
.
, ?k ?
IR(M) suchthat:?
for every 1 ?
i ?
k: if zi ?
X , then ?i doesnot contain ?
; if zi ?
F , then ?i contains ?exactly once,?
?1 = ?
?0 q1(?1) ?1 .
.
.
qk(?k) ?k ?
?, and?
?2 = ?
q(?(?))
?
?where ?
is a function that replaces variablesin a right-hand side with their values (subtrees)from the left-hand side of the rule.
Formally,?
: RHS(k) ?
IR(M) is defined as follows:(i) for every ?
= ?
(?1, .
.
.
, ?n) ?
RHS(k), ?
?
?, we have ?(?)
= ?(?
(?1), .
.
.
, ?
(?n)),(ii) (substitution) for every zi ?
X , we have?
(zi) = ?i,(iii) (adjoining) for every zi ?
F and ?
?RHS(k), we have ?(zi(?))
= ?i[?(?
)]vwhere v is the uniquely determined positionof ?
in ?i, and(iv) ?(?)
= ?.Clearly, the probablity of a rule carries over toderivation steps that employ this rule.
Since, asusual, a derivation d is a sequence of derivationsteps, we let the probability of d be the product ofthe probabilities of its steps.The string-to-tree transformation computed byM is the set ?M of all tuples (?, ?)
?
???U?
suchthat there is a derivation of the form ?
??
q(?)
forsome q ?
Qf .5 Decoder for pSTIGNow we construct the decoder dec(G) for a pSTIGG that transforms source strings directly into tar-get trees and simultaneously computes the proba-bility of the corresponding derivation of G. Thisdecoder is formalized as a BUTAT.Since dec(G) is a string-to-tree transducer, we14have to transform the source tree ?s of a rule rinto a left-hand side ?
of a dec(G)-rule.
This isdone similarly to (DeNeefe and Knight, 2009) bytraversing ?s via recursive descent using a map-ping ?
(see an example after Theorem 1); thiscreates appropriate state-variable combinations forall substitution sites and potential adjoining sitesof r. In particular, the source component of thedirection-type of a potential adjoining site deter-mines the position of the corresponding combina-tion in ?.
If there are several potential adjoiningsites with the same source component, then wecreate a ?
for every permutation of these sites.
Theright-hand side of a dec(G)-rule is obtained bytraversing the target tree ?t via recursive descentusing a mapping ??
and, whenever a nonterminalwith a potential adjoining site w is met, a new po-sition labeled by fw is inserted.5 If there is morethan one potential adjoining site, then the set ofall those sites is ordered as in the left-hand side ?from top to bottom.Apart from these main rules we will employrules which implement the decision of whether ornot to turn a potential adjoining site w into an ac-tivated adjoining site.
Rules for the first purposejust pass the already computed output tree throughfrom left to right, whereas rules for the second pur-pose create for an empty left-hand side the outputtree ?.We will use the state behavior of dec(G) in or-der to check that (i) the nonterminals of a substi-tution or potential adjoining site match the root-category of the used rule, (ii) the direction-typeof an adjoining site matches the auxiliarity of thechosen auxiliary rule, and (iii) the decisions ofwhether or not to adjoin for each rule r of G arekept separate.Whereas each pair (?s, ?t) in the translation ofG is computed in a top-down way, starting at theinitial sentential form and substituting and adjoin-ing to the present sentential form, dec(G) builds?t in a bottom-up way.
This change of direction islegitimate, because adjoining is associative (Vijay-Shanker and Weir, 1994), i.e., it leads to the sameresult whether we first adjoin r2 to r1, and thenalign r3 to the resulting tree, or first adjoin r3 tor2, and then adjoin the resulting tree to r1.In Fig.
3 we show some rules of the decoderof our running example pSTIG and in Fig.
4 the5We will allow variables to have structured indices thatare not elements of N. However, by applying a bijective re-naming, we can always obtain rules of the form (?
).derivation of this decoder which correponds to thederivation in Fig.
2.Theorem 1.
Let G be a pSTIG over N and T .Then there is a BUTAT dec(G) such that for ev-ery (?s, ?t) ?
UN (T ) ?
UN (T ) and p ?
[0, 1] thefollowing two statements are equivalent:1. there is a successful derivation of(?s, ?t, ?, ?, ?)
by G with probability p,2.
there is a derivation from yield(?s) to[Ss, St](?t) by dec(G) with probability p.PROOF.
Let G = (N,T, [Ss, St],S,A, P ) be apSTIG.
We will construct the BUTAT dec(G) =(Q,T,N ?T, {[Ss, St]}, R) as follows (where themappings ?
and ??
will be defined below):?
Q = [N ?N ] ?
[N ?
{L,R}?N ?{L,R}]?
{[r, w] | r ?
A, w is an adjoining site of r},?
R is the smallest set R?
of rules suchthat for every r ?
S ?
A of the form(?s, ?t, V,W, P radj) or (?s, ?t, V,W, ?, P radj):?
for every ?
?
?(?
), if r ?
S, then themain rule?
P (r)?
[?s(?
), ?t(?)](??(?
))is in R?, and if r ?
A and r is ?s?t-auxiliary, then the main rule?
P (r)?
[?s(?
), ?s, ?t(?
), ?t](??(?
))is in R?, and?
for every w = (ws, ?s, wt, ?t) ?
W therulesqw(fw) P radj(w)??
[r, w](fw(?
))with qw = [?
(ws), ?s, ?t(wt), ?t] for ac-tivation at w, and the rule?1?P radj(w)??
[r, w](?
)for non-adjoining at w are in R?.We define the mapping?
: pos(?s) ?
P((T ?Q(X ?
F ))?
)with Q(X ?
F ) = {q(z) | q ?
Q, z ?
X ?
F}inductively on its argument as follows.
Let w ?pos(?s) and let w have n children.
(a) Let ?s(w) ?
T .
Then ?
(w) = {?s(w)}.15(b) (substitution site) Let ?s(w) ?
N and letw?
?
pos(?t) such that (w,w?)
?
V .
Then?
(w) = {[?s(w), ?t(w?)](x(w,w?))}.
(c) (adjoining site) Let ?s(w) ?
N and let therebe an adjoining site in W with w as firstcomponent.
Then, we define ?
(w) to be thesmallest set such that for every permutation(u1, .
.
.
, ul) (resp., (v1, .
.
.
, vm)) of all the L-adjoining (resp., R-adjoining) sites inW withw as first component, the set6J ?
?
(w.1) ?
.
.
.
?
?
(w.n) ?Kis a subset of ?
(w), where J = {u?1 .
.
.
u?l}and K = {v?m .
.
.
v?1} andu?i = [r, ui](fui)and v?j = [r, vj ](fvj)for 1 ?
i ?
l and 1 ?
j ?
m.(d) Let ?s(w) ?
N , w 6= ?, and let w be neitherthe first component of a substitution site in Vnor the first component of an adjoining site inW .
Then?
(w) = ?
(w.1) ?
.
.
.
?
?
(w.n) .
(e) Let w = ?.
Then we define ?
(w) = {?
}.For every ?
?
?(?
), we define the mapping??
: pos(?t) ?
UN?F?X(T ?
{?
})inductively on its argument as follows.
Letw ?
pos(?t) and let w have n children.
(a) Let ?t(w) ?
T .
Then ??
(w) = ?t(w).
(b) (substitution site) Let ?t(w) ?
N and letw?
?
pos(?s) such that (w?, w) ?
V .
Then??
(w) = x(w?,w).
(c) (adjoining site) Let ?t(w) ?
N and let therebe an adjoining site in W with w as thirdcomponent.
Then let {u1, .
.
.
, ul} ?
W bethe set of all potential adjoining sites with was third component, and we define??
(w) = fu1(.
.
.
ful(?)
.
.
.
)where ?
= ?t(w)(??
(w.1), .
.
.
, ??
(w.n))and the ui?s occur in ??
(w) (from the roottowards the leaves) in exactly the same orderas they occur in ?
(from left to right).
(d) Let ?t(w) ?
N , w 6= ?, and let w be neitherthe second component of a substitution sitein V nor the third component of an adjoiningsite in W .
Then??
(w) = ?t(w)(??
(w.1), .
.
.
, ??
(w.n)).6using the usual concatenation ?
of formal languages(e) Let w = ?.
Then ??
(w) = ?.With dec(G) constructed as shown, for eachderivation of G there is a corresponding deriva-tion of dec(G), with the same probability, and viceversa.
The derivations proceed in opposite direc-tions.
Each sentential form in one has an equiv-alent sentential form in the other, and each stepof the derivations correspond.
There is no spaceto present the full proof, but let us give a slightlymore precise idea about the formal relationship be-tween the derivations of G and dec(G).In the usual way we can associate a deriva-tion tree dt with every successful derivation d ofG.
Assume that last(d) = (?s, ?t, ?, ?, ?
), andlet Es and Et be the embedded tree transducers(Shieber, 2006) associated with, respectively, thesource component and the target component ofG.
Then it was shown in (Shieber, 2006) that?Es(dt) = ?s and ?Et(dt) = ?t where ?E de-notes the tree-to-tree transduction computed by anembedded tree transducer E. Roughly speaking,Es and Et reproduce the derivations of, respec-tively, the source component and the target com-ponent of G that are prescribed by dt.
Thus, for?
= (?
?s, ?
?t, V,W, g), if ?in ?
?G ?
and ?
is a prefixof d, then there is exactly one subtree dt[(w,w?
)]of dt associated with every (w,w?)
?
V ?
W ,which prescribes how to continue at (w,w?)
withthe reproduction of d. Having this in mind, we ob-tain the sentential form of the dec(G)-derivationwhich corresponds to ?
by applying a modifica-tion of ?
to ?
where the modification amounts toreplacing x(w,w?)
and f(w,w?)
by ?Et(dt[(w,w?
)]);note that ?Et(dt[(w,w?)])
might contain ?.
As illustration of the construction in Theorem 1let us apply the mappings ?
and ??
to rule r2 ofFig.
1, i.e., to r2 = (?s, ?t, ?, {b, c}, ?, P r2adj)with ?s = A(A, ?
), ?t = B(B(?
), B),b = (?,R, ?,L), c = (?,R, 1,L), and ?
= (1, 2).Let us calculate ?(?)
on ?s.
Due to (c),?(?)
= J ?
?
(1) ?
?
(2) ?K.Since there are no L-adjoinings at ?, we have thatJ = {?}.
Since there are the R-adjoinings b and cat ?, we have the two permutations (b, c) and (c, b).
(v1, v2) = (b, c): K = {[r2, c](fc)[r2, b](fb)}(v1, v2) = (c, b): K = {[r2, b](fb)[r2, c](fc)}Due to (e) and (a), we have that ?
(1) = {?}
and?
(2) = {?
}, resp.
Thus, ?(?)
is the set:{?
[r2, c](fc) [r2, b](fb), ?
[r2, b](fb) [r2, c](fc)}.16r1(r1, a)r2(r2,?b) (r2,?c)r4?[A,B]x(2,2)?
[r1, a]fa1??
[Ss, St]StfB?x ?a (2,2)[A,R, B,L]fa.9??
[r1, a]f?a?
[r2, b]f b[r2, c]fc.4??
[A,R, B,L]fBfB??bc?
.8??
[r2, b]??
.4??
[r2, c]??
.1??
[A,B]B?Figure 3: Some rules of the running example de-coder.Now let ?
= ?
[r2, b](fb) [r2, c](fc).
Let us cal-culate ??(?)
on ?t.??(?
)(c)= fb(B(??
(1), ??
(2)))(c)= fb(B(fc(B(??
(1.1))), ??
(2)))(a)= fb(B(fc(B(?
)), ??
(2)))(e)= fb(B(fc(B(?
)), ?
))Hence we obtain the rule?
[r2, b](fb) [r2, c](fc) ?
[A,R, B,L](fb(B(fc(B(?
)), ?
)))which is also shown in Fig.
3.?
?
?
??
?
?
?
[r2, b]??
?
?
?
[r2, b]?
[r2, c]??
?
??
?
??[A,B]B??
[A,R, B,L]BB??=?=?=?=?=?
[Ss, St]StBB?B?B??prob.
.8prob.
.4prob.
.4prob.
.9prob.
.1=?
prob.
.1[r1, a]BB??
(r2,?b)(r2,?c)(r2, bc)(r1, a)r4r1[r1, a]BB?
?Figure 4: Derivation of the decoder correspondingto the derivation in Fig.
2.17ReferencesA.
Abeille, Y. Schabes, A.K.
Joshi.
Using lexicalizedTAGs for machine translation.
In Proceedings ofthe 13th International Conference on ComputationalLinguistics, volume 3, pp.
1?6, Helsinki, Finland,1990.C.
Baier, M.
Gro?
?er, F. Ciesinski.
Model checkinglinear-time properties of probabilistic systems.
InHandbook of Weighted Automata, Chapter 13, pp.519?570, Springer, 2009.S.
DeNeefe.
Tree adjoining machine translation.
Ph.D.thesis proposal, Univ.
of Southern California, 2009.S.
DeNeefe, K. Knight.
Synchronous tree adjoiningmachine translation.
In Proc.
of Conf.
EmpiricalMethods in NLP, pp.
727?736, 2009.J.
Engelfriet.
Bottom-up and top-down tree transfor-mations ?
a comparison.
Math.
Systems Theory,9(3):198?231, 1975.J.
Engelfriet.
Tree transducers and syntax-directed se-mantics.
In CAAP 1982: Lille, France, 1982.A.
Fujiyoshi, T. Kasai.
Spinal-formed context-free treegrammars.
Theory of Computing Systems, 33:59?83, 2000.Z.
Fu?lo?p, H. Vogler.
Weighted tree automata and treetransducers.
In Handbook of Weighted Automata,Chapter 9, pp.
313?403, Spinger, 2009.F.
Ge?cseg, M. Steinby.
Tree Automata.
Akade?miaiKiado?, Budapest, 1984.F.
Ge?cseg, M. Steinby.
Tree languages.
In Handbookof Formal Languages, volume 3, chapter 1, pages1?68.
Springer-Verlag, 1997.J.
Graehl, K. Knight, J.
May.
Training tree transducers.Computational Linguistics, 34(3):391?427, 2008A.K.
Joshi, L.S.
Levy, M. Takahashi.
Tree adjunctgrammars.
Journal of Computer and System Sci-ences, 10(1):136?163, 1975.A.K.
Joshi, Y. Schabes.
Tree-adjoining grammars.
InHandbook of Formal Languages.
Chapter 2, pp.
69?123, Springer-Verlag, 1997.K.
Knight, J. Graehl.
An overview of probabilis-tic tree transducers for natural language processing.In Computational Linguistics and Intelligent TextProcessing, CICLing 2005, LNCS 3406, pp.
1?24,Springer, 2005.K.
Knight, J.
May.
Applications of Weighted Au-tomata in Natural Language Processing.
In Hand-book of Weighted Automata, Chapter 14, pp.
571?596, Springer, 2009.P.M.
Lewis, R.E.
Stearns.
Syntax-directed transduc-tions.
Journal of the ACM, 15:465?488, 1968.A.
Maletti.
Compositions of tree series transforma-tions.
Theoret.
Comput.
Sci., 366:248?271, 2006.A.
Maletti.
The Power of Tree Series Transducers.Ph.D.
thesis, TU Dresden, Germany, 2006.A.
Maletti.
Compositions of extended top-downtree transducers.
Information and Computation,206:1187?1196, 2008.A.
Maletti.
Why synchronous tree substitution gram-mars?
in Proc.
11th Conf.
North American Chap-ter of the Association of Computational Linguistics.2010.D.F.
Martin and S.A. Vere.
On syntax-directed trans-ductions and tree transducers.
In Ann.
ACM Sympo-sium on Theory of Computing, pp.
129?135, 1970.R.
Nesson, S.M.
Shieber, and A.
Rush.
Inductionof probabilistic synchronous tree-insertion gram-mars.
Technical Report TR-20-05, Computer Sci-ence Group, Harvard Univeristy, Cambridge, Mas-sachusetts, 2005.R.
Nesson, S.M.
Shieber, and A.
Rush.
Induction ofprobabilistic synchronous tree-inserting grammarsfor machine translation.
In Proceedings of the 7thConference of the Association for Machine Transla-tion in the Americas (AMTA 2006), 2006.Y.
Schabes, R.C.
Waters.
Tree insertion grammars:a cubic-time, parsable formalism that lexicalizescontext-free grammar without changing the treesproduced.
Computational Linguistics, 21:479?513,1994.P.P.
Schreiber.
Tree-transducers and syntax-connectedtransductions.
In Automata Theory and FormalLanguages, Lecture Notes in Computer Science 33,pp.
202?208, Springer, 1975.S.M.
Shieber.
Synchronous grammars and tree trans-ducers.
In Proc.
7th Workshop on Tree Adjoin-ing Grammars and Related Formalisms, pp.
88?95,2004.S.M.
Shieber.
Unifying synchronous tree-adjoininggrammars and tree transducers via bimorphisms.
InProc.
11th Conf.
European Chapter of ACL, EACL06, pp.
377?384, 2006.S.M.
Shieber, Y. Schabes.
Synchronous tree-adjoininggrammars.
In Proceedings of the 13th Interna-tional Conference on Computational Linguistics,volume 3, pp.
253?258, Helsinki, Finland, 1990.K.
Vijay-Shanker, D.J.
Weir.
The equivalence of fourextensions of context-free grammars.
MathematicalSystems Theory, 27:511?546, 1994.K.
Yamada and K. Knight.
A syntax-based statisticaltranslation model.
In Proc.
of 39th Annual Meetingof the Assoc.
Computational Linguistics, pp.
523?530, 2001.18
