Proceedings of the ACL Student Research Workshop, pages 165?171,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsA corpus-based evaluation method for Distributional Semantic ModelsAbdellah Fourtassi1,2 Emmanuel Dupoux2,3abdellah.fourtassi@gmail.com emmanuel.dupoux@gmail.com1Institut d?Etudes Cognitives, Ecole Normale Superieure, Paris2Laboratoire de Sciences Cognitives et Psycholinguistique, CNRS, Paris3Ecole des Hautes Etudes en Sciences Sociales, ParisAbstractEvaluation methods for Distributional Se-mantic Models typically rely on behav-iorally derived gold standards.
Thesemethods are difficult to deploy in lan-guages with scarce linguistic/behavioralresources.
We introduce a corpus-basedmeasure that evaluates the stability of thelexical semantic similarity space using apseudo-synonym same-different detectiontask and no external resources.
We showthat it enables to predict two behavior-based measures across a range of parame-ters in a Latent Semantic Analysis model.1 IntroductionDistributional Semantic Models (DSM) can betraced back to the hypothesis proposed by Harris(1954) whereby the meaning of a word can be in-ferred from its context.
Several implementationsof Harris?s hypothesis have been proposed in thelast two decades (see Turney and Pantel (2010) fora review), but comparatively little has been doneto develop reliable evaluation tools for these im-plementations.
Models evaluation is however anissue of crucial importance for practical applica-tions, i.g., when trying to optimally set the model?sparameters for a given task, and for theoretical rea-sons, i.g., when using such models to approximatesemantic knowledge.Some evaluation techniques involve assigningprobabilities to different models given the ob-served corpus and applying maximum likelihoodestimation (Lewandowsky and Farrell, 2011).However, computational complexity may preventthe application of such techniques, besides theseprobabilities may not be the best predictor for themodel performance on a specific task (Blei, 2012).Other commonly used methods evaluate DSMs bycomparing their semantic representation to a be-haviorally derived gold standard.
Some standardsare derived from the TOEFL synonym test (Lan-dauer and Dumais, 1997), or the Nelson wordassociations norms (Nelson et al 1998).
Oth-ers use results from semantic priming experiments(Hutchison et al 2008) or lexical substitutions er-rors (Andrews et al 2009).
Baroni and Lenci(2011) set up a more refined gold standard for En-glish specifying different kinds of semantic rela-tionship based on dictionary resources (like Word-Net and ConceptNet).These behavior-based evaluation methods areall resource intensive, requiring either linguisticexpertise or human-generated data.
Such meth-ods might not always be available, especially inlanguages with fewer resources than English.
Inthis situation, researchers usually select a small setof high-frequency target words and examine theirnearest neighbors (the most similar to the target)using their own intuition.
This is used in partic-ular to set the model parameters.
However, thisrather informal method represents a ?cherry pick-ing?
risk (Kievit-Kylar and Jones, 2012), besidesit is only possible for languages that the researcherspeaks.Here we introduce a method that aims at pro-viding a rapid and quantitative evaluation forDSMs using an internal gold standard and re-quiring no external resources.
It is based on asimple same-different task which detects pseudo-synonyms randomly introduced in the corpus.
Weclaim that this measure evaluates the intrinsicability of the model to capture lexical semanticsimilarity.
We validate it against two behavior-based evaluations (Free association norms and theTOEFL synonym test) on semantic representa-tions extracted from a Wikipedia corpus using oneof the most commonly used distributional seman-tic models : the Latent Semantic Analysis (LSA,Landauer and Dumais (1997)).In this model, we construct a word-documentmatrix.
Each word is represented by a row, and165each document is represented by a column.
Eachmatrix cell indicates the occurrence frequency ofa given word in a given context.
Singular valuedecomposition (a kind of matrix factorization) isused to extract a reduced representation by trun-cating the matrix to a certain size (which we callthe semantic dimension of the model).
The cosineof the angle between vectors of the resulting spaceis used to measure the semantic similarity betweenwords.
Two words end up with similar vectors ifthey co-occur multiple times in similar contexts.2 ExperimentWe constructed three successively larger corporaof 1, 2 and 4 million words by randomly select-ing articles from the original ?Wikicorpus?
madefreely available on the internet by Reese et al(2010).
Wikicorpus is itself based on articles fromthe collaborative encyclopedia Wikipedia.
We se-lected the upper bound of 4 M words to be com-parable with the typical corpus size used in theo-retical studies on LSA (see for instance Landauerand Dumais (1997) and Griffiths et al(2007)).
Foreach corpus, we kept only words that occurred atleast 10 times and we excluded a stop list of highfrequency words with no conceptual content suchas: the, of, to, and ...
This left us with a vocab-ulary of 8 643, 14 147 and 23 130 words respec-tively.
For the simulations, we used the free soft-ware Gensim (R?ehu?r?ek and Sojka, 2010) that pro-vides an online Python implementation of LSA.We first reproduced the results of Griffiths et al(2007), from which we derived the behavior-basedmeasure.
Then, we computed our corpus-basedmeasure with the same models.2.1 The behavior-based measureFollowing Griffiths et al(2007), we used thefree association norms collected by Nelson et al(1998) as a gold standard to study the psychologi-cal relevance of the LSA semantic representation.The norms were constructed by asking more than6000 participants to produce the first word thatcame to mind in response to a cue word.
Theparticipants were presented with 5,019 stimuluswords and the responses (word associates) wereordered by the frequency with which they werenamed.
The overlap between the words used inthe norms and the vocabulary of our smallest cor-pus was 1093 words.
We used only this restrictedoverlap in our experiment.In order to evaluate the performance of LSAmodels in reproducing these human generateddata, we used the same measure as in Griffithset al(2007): the median rank of the first associatesof a word in the semantic space.
This was done inthree steps : 1) for each word cue Wc, we sortedthe list of the remaining words Wi in the overlapset, based on their LSA cosine similarity with thatcue: cos(LSA(Wc), LSA(Wi)), with highest co-sine ranked first.
2) We found the ranks of the firstthree associates for that cue in that list.
3) We ap-plied 1) and 2) to all words in the overlap set andwe computed the median rank for each of the firstthree associates.Griffiths et al(2007) tested a set of seman-tic dimensions going from 100 to 700.
We ex-tended the range of dimensions by testing thefollowing set : [2,5,10,20,30,40,50,100, 200,300,400,500,600,700,800,1000].
We also manip-ulated the number of successive sentences to betaken as defining the context of a given word (doc-ument size), which we varied from 1 to 100.In Figure 1 we show the results for the 4 M sizecorpus with 10 sentences long documents.Figure 1 : The median rank of the three associates as afunction of the semantic dimensions (lower is better)For the smaller corpora we found similar resultsas we can see from Table 1 where the scores rep-resent the median rank averaged over the set ofdimensions ranging from 10 to 1000.
As foundin Griffiths et al(2007), the median rank measurepredicts the order of the first three associates in thenorms.In the rest of the article, we will need to char-acterize the semantic model by a single value.
In-stead of taking the median rank of only one of the166Size associate 1 associate 2 associate 31 M 78.21 152.18 169.072 M 57.38 114.57 1314 M 54.57 96.5 121.57Table 1 : The median rank of the first three associates fordifferent sizesassociates, we will consider a more reliable mea-sure by averaging over the median ranks of thethree associates across the overlap set.
We willcall this measure the Median Rank.2.2 The Pseudo-synonym detection taskThe measure we introduce in this part is basedon a Same-Different Task (SDT).
It is describedschematically in Figure 2, and is computed asfollows: for each corpus, we generate a Pseudo-Synonym-corpus (PS-corpus) where each word inthe overlap set is randomly replaced by one of twolexical variants.
For example, the word ?Art?
isreplaced in the PS-corpus by ?Art1?
or ?Art2?.
Inthe derived corpus, therefore, the overlap lexiconis twice as big, because each word is duplicatedand each variant appears roughly with half of thefrequency of the original word.The Same-Different Task is set up as follows: apair of words is selected at random in the derivedcorpus, and the task is to decide whether they arevariants of one another or not, only based on theircosine distances.
Using standard signal detectiontechniques, it is possible to use the distributionof cosine distances across the entire list of wordpairs in the overlap set to compute a ReceiverOperating Characteristic Curve (Fawcett, 2006),from which one derives the area under the curve.We will call this measure : SDT-?.
It can beinterpreted as the probability that given two pairsof words, of which only one is a pseudo-synonympair, the pairs are correctly identified based oncosine distance only.
A value of 0.5 representspure chance and a value of 1 represents perfectperformance.It is worth mentioning that the idea of gen-erating pseudo-synonyms could be seen as theopposite of the ?pseudo-word?
task used inevaluating word sense disambiguation models(see for instance Gale et al(1992) and Daganet al(1997)).
In this task, two different wordsw1 and w2 are combined to form one ambiguouspseudo-word W12 = {w1, w2} which replacesboth w1 and w2 in the test set.We now have two measures evaluating thequality of a given semantic representation: TheMedian Rank (behavior-based) and the SDT-?(corpus-based).
Can we use the latter to predictthe former?
To answer this question, we comparedthe performance of both measures across differ-ent semantic models, document lengths and cor-pus sizes.3 ResultsIn Figure 3 (left), we show the results of thebehavior-based Median Rank measure, obtainedfrom the three corpora across a number of seman-tic dimensions.
The best results are obtained witha few hundred dimensions.
It is important to high-light the fact that small differences between highdimensional models do not necessarily reflect adifference in the quality of the semantic repre-sentation.
In this regard, Landauer and Dumais(1997) argued that very small changes in com-puted cosines can in some cases alter the LSA or-dering of the words and hence affect the perfor-mance score.
Therefore only big differences in theMedian Ranks could be explained as a real dif-ference in the overall quality of the models.
Theglobal trend we obtained is consistent with the re-sults in Griffiths et al(2007) and with the findingsin Landauer and Dumais (1997) where maximumperformance for a different task (TOEFL synonymtest) was obtained over a broad region around 300dimensions.Besides the effect of dimensionality, Figure 3 (left)indicates that performance gets better as we in-crease the corpus size.In Figure 3 (right) we show the corresponding re-sults for the corpus-based SDT-?
measure.
We cansee that SDT-?
shows a parallel set of results andcorrectly predicts both the effect of dimensionalityand the effect of corpus size.
Indeed, the generaltrend is quite similar to the one described with theMedian Rank in that the best performance is ob-tained for a few hundred dimensions and the threecurves show a better score for large corpora.Figure 4 shows the effect of document length onthe Median Rank and SDT-?.
For both measures,we computed these scores and averaged them overthe three corpora and the range of dimensions go-ing from 100 to 1000.
As we can see, SDT-?
pre-dicts the psychological optimal document length,167Figure 2 : Schematic description of the Same-Different Task used.which is about 10 sentences per document.
In thecorpus we used, this gives on average of about 170words/document.
This value confirms the intuitionof Landauer and Dumais (1997) who used a para-graph of about 150 word/document in their model.Finally, Figure 5 (left) summarizes the entireset of results.
It shows the overall correlationbetween SDT-?
and the Median Rank.
Onepoint in the graph corresponds to a particularchoice of semantic dimension, document lengthand corpus size.
To measure the correlation, weuse the Maximal Information Coefficient (MIC)recently introduced by Reshef et al(2011).
Thismeasure captures a wide range of dependenciesbetween two variables both functional and not.For functional and non-linear associations it givesa score that roughly equals the coefficient ofdetermination (R2) of the data relative to theregression function.
For our data this correlationmeasure yields a score of MIC = 0.677 with(p < 10?6).In order to see how the SDT-?
measure wouldcorrelate with another human-generated bench-mark, we ran an additional experiment using theTOEFL synonym test (Landauer and Dumais,1997) as gold standard.
It contains a list of80 questions consisting of a probe word andfour answers (only one of which is defined asthe correct synonym).
We tested the effect ofsemantic dimensionality on a 6 M word sizedWikipedia corpus where documents containedrespectively 2, 10 and 100 sentences for eachseries of runs.
We kept only the questions forwhich the probes and the 4 answers all appearedin the corpus vocabulary.
This left us with aset of 43 questions.
We computed the responseof the model on a probe word by selecting theanswer word with which it had the smallest cosineangle.
The best performance (65.1% correct) wasobtained with 600 dimensions.
This is similarto the result reported in Landauer and Dumais(1997) where the best performance obtained was64.4% (compared to 64.5% produced by non-native English speakers applying to US colleges).The correlation with SDT-?
is shown in Figure5 (right).
Here again, our corpus-based measurepredicts the general trend of the behavior-basedmeasure: higher values of SDT-?
correspondto higher percentage of correct answers.
Thecorrelation yields a score of MIC = 0.675 with(p < 10?6).In both experiments, we used the overlap set ofthe gold standard with the Wikicorpus to computethe SDT-?
measure.
However, as the main ideais to apply this evaluation method to corpora forwhich there is no available human-generated goldstandards, we computed new correlations using aSDT-?
measure computed, this time, over a setof randomly selected words.
For this purpose weused the 4M corpus with 10 sentences long docu-ments and we varied the semantic dimensions.
Weused the Median Rank computed with the Free as-sociation norms as a behavior-based measure.We tested both the effect of frequency and size:we varied the set size from 100 to 1000 wordswhich we randomly selected from three frequencyranges : higher than 400, between 40 and 400 andbetween 40 and 1.
We chose the limit of 400 sothat we can have at least 1000 words in the firstrange.
On the other hand, we did not considerwords which occur only once because the SDT-?requires at least two instances of a word to gener-ate a pseudo-synonym.The correlation scores are shown in Table 2.Based on the MIC correlation measure, mid-168Figure 3 : The Median rank (left) and SDT-?
(right) as a function of a number of dimensions and corpus sizes.
Document sizeis 10 sentences.Figure 4 : The Median rank (left) and SDT-?
(right) as a function of document length (number of sentences).
Both measuresare averaged over the three corpora and over the range of dimensions going from 100 to 1000.Figure 5 : Overall correlation between Median Rank and SDT-?
(left) and between Correct answers in TOEFL synonym testand SDT-?
(right) for all the runs.
.169Freq.
x 1 < x < 40 40 < x < 400 x > 400 All OverlapSize 100 500 1000 100 500 1000 100 500 1000 ?
4 M 1093MIC 0.311 0.219 0.549?
0.549?
0.717?
0.717?
0.311 0.205 0.419 0.549?
0.717?
* : p < 0.05Table 2 : Correlation scores of the Median Rank with the SDT-?
measure computed over randomly selected words from thecorpus, the whole lexicon and the overlap with the free association norms.
We test the effect of frequency and set size.frequency words yield better scores.
The corre-lations are as high as the one computed with theoverlap even with a half size set (500 words).The overlap is itself mostly composed of mid-frequency words, but we made sure that the ran-dom test sets have no more than 10% of theirwords in the overlap.
Mid-frequency words areknown to be the best predictors of the conceptualcontent of a corpus, very common and very rareterms have a weaker discriminating or ?resolving?power (Luhn, 1958).4 DiscussionWe found that SDT-?
enables to predict the out-come of behavior-based evaluation methods withreasonable accuracy across a range of parametersof a LSA model.
It could therefore be used as aproxy when human-generated data are not avail-able.
When faced with a new corpus and a taskinvolving similarity between words, one could im-plement this rather straightforward method in or-der, for instance, to set the semantic model param-eters.The method could also be used to compare theperformance of different distributional semanticmodels, because it does not depend on a partic-ular format for semantic representation.
All that isrequired is the existence of a semantic similaritymeasure between pairs of words.
However, fur-ther work is needed to evaluate the robustness ofthis measure in models other than LSA.It is important to keep in mind that the correla-tion of our measure with the behavior-based meth-ods only indicates that SDT-?
can be trusted, tosome extent, in evaluating these semantic tasks.It does not necessarily validate its ability to as-sess the entire semantic structure of a distribu-tional model.
Indeed, the behavior-based methodsare dependent on particular tasks (i.g., generatingassociates, or responding to a multiple choice syn-onym test) hence they represent only an indirectevaluation of a model, viewed through these spe-cific tasks.It is worth mentioning that Baroni and Lenci(2011) introduced a comprehensive technique thattries to assess simultaneously a variety of seman-tic relations like meronymy, hypernymy and coor-dination.
Our measure does not enable us to as-sess these relations, but it could provide a valu-able tool to explore other fine-grained features ofthe semantic structure.
Indeed, while we intro-duced SDT-?
as a global measure over a set of testwords, it can also be computed word by word.
In-deed, we can compute how well a given seman-tic model can detect that ?Art1?
and ?Art2?
arethe same word, by comparing their semantic dis-tance to that of random pairs of words.
Such aword-specific measure could assess the semanticstability of different parts of the lexicon such asconcrete vs. abstract word categories, or the distri-bution properties of different linguistic categories(verb, adjectives, ..).
Future work is needed to as-sess the extent to which the SDT-?
measure andits word-level variant provide a general frameworkfor DSMs evaluation without external resources.Finally, one concern that could be raised by ourmethod is the fact that splitting words may affectthe semantic structure of the model we want to as-sess because it may alter the lexical distribution inthe corpus, resulting in unnaturally sparse statis-tics.
There is in fact evidence that corpus attributescan have a big effect on the extracted model (Srid-haran and Murphy, 2012; Lindsey et al 2007).However, as shown by the high correlation scores,the introduced pseudo-synonyms do not seem tohave a dramatic effect on the model, at least as faras the derived SDT-?
measure and its predictivepower is concerned.
Moreover, we showed that inorder to apply the method, we do not need to usethe whole lexicon, on the contrary, a small test setof about 500 random mid-frequency words (whichrepresents less than 2.5 % of the total vocabulary)was shown to lead to better results.
However, evenif the results are not directly affected in our case,future work needs to investigate the exact effectword splitting may have on the semantic model.170ReferencesAndrews, M., G. Vigliocco, and D. Vinson (2009).Integrating experiential and distributional datato learn semantic representations.
Psychologi-cal Review 116, 463?498.Baroni, M. and A. Lenci (2011).
How weBLESSed distributional semantic evaluation.
InProceedings of the EMNLP 2011 Geometri-cal Models for Natural Language Semantics(GEMS 2011) Workshop, East Stroudsburg PA:ACL, pp.
1?10.Blei, D. (2012).
Probabilistic topic models.
Com-munications of the ACM 55(4), 77?84.Dagan, I., L. Lee, and F. Pereira (1997).Similarity-based methods for word sense dis-ambiguation.
In Proceedings of the 35thACL/8th EACL, pp.
56?63.Fawcett, T. (2006).
An introduction to ROC anal-ysis.
Pattern Recognition Letters 27(8), 861?874.Gale, W., K. Church, and D. Yarowsky (1992).Work on statistical methods for word sense dis-ambiguation.
Workings notes, AAAI Fall Sym-posium Series, Probabilistic Approaches to Nat-ural Language, 54?60.Griffiths, T., M. Steyvers, and J. Tenenbaum(2007).
Topics in semantic representation.
Psy-chological Review 114, 114?244.Harris, Z.
(1954).
Distributional structure.Word 10(23), 146?162.Hutchison, K., D. Balota, M. Cortese, and J. Wat-son (2008).
Predicting semantic priming at theitem level.
Quarterly Journal of ExperimentalPsychology 61(7), 1036?1066.Kievit-Kylar, B. and M. N. Jones (2012).
Visualiz-ing multiple word similarity measures.
Behav-ior Research Methods 44(3), 656?674.Landauer, T. and S. Dumais (1997).
A solutionto plato?s problem: The latent semantic anal-ysis theory of acquisition, induction, and rep-resentation of knowledge.
Psychological Re-view 104(2), 211?240.Lewandowsky, S. and S. Farrell (2011).
Compu-tational modeling in cognition : principles andpractice.
Thousand Oaks, Calif. : Sage Publi-cations.Lindsey, R., V. Veksler, and A. G. andWayne Gray(2007).
Be wary of what your computer reads:The effects of corpus selection on measuringsemantic relatedness.
In Proceedings of theEighth International Conference on CognitiveModeling, pp.
279?284.Luhn, H. P. (1958).
The automatic creation of lit-erature abstracts.
IBM Journal of Research andDevelopment 2(2), 157?165.Nelson, D., C. McEvoy, and T. Schreiber (1998).The university of south florida word association,rhyme, and word fragment norms.Reese, S., G. Boleda, M. Cuadros, L. Padro, andG.
Rigau (2010).
Wikicorpus: A word-sensedisambiguated multilingual wikipedia corpus.In Proceedings of 7th Language Resources andEvaluation Conference (LREC?10), La Valleta,Malta.R?ehu?r?ek, R. and P. Sojka (2010).
Software frame-work for topic modelling with large corpora.
InProceedings of the LREC 2010 Workshop onNew Challenges for NLP Frameworks, Valletta,Malta, pp.
45?50.Reshef, D., Y. Reshef, H. Finucane, S. Gross-man, G. McVean, P. Turnbaugh, E. Lander,M.
Mitzenmacher, and P. Sabeti (2011).
De-tecting novel associations in large datasets.
Sci-ence 334(6062), 1518?1524.Sridharan, S. and B. Murphy (2012).
Modelingword meaning: distributional semantics and thesorpus quality-quantity trade-off.
In Proceed-ings of the 3rd Workshop on Cognitive Aspectsof the Lexicon, COLING 2012, Mumbai, pp.53?68.Turney, P. D. and P. Pantel (2010).
From frequencyto meaning: Vector space models of semantics.Journal of Artificial Intelligence Research 37,141?188.171
