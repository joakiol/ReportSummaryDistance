Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1713?1723,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsUniversal Decompositional Semantics on Universal DependenciesAaron Steven White Drew Reisinger Keisuke Sakaguchi Tim VieiraSheng Zhang Rachel Rudinger Kyle Rawlins Benjamin Van DurmeJohns Hopkins UniversityAbstractWe present a framework for augmenting datasets from the Universal Dependencies projectwith Universal Decompositional Semantics.Where the Universal Dependencies projectaims to provide a syntactic annotation stan-dard that can be used consistently across manylanguages as well as a collection of corporathat use that standard, our extension has simi-lar aims for semantic annotation.
We describeresults from annotating the English Univer-sal Dependencies treebank, dealing with wordsenses, semantic roles, and event properties.1 IntroductionThis paper describes the Universal DecompositionalSemantics (Decomp) project, which aims to aug-ments Universal Dependencies (UD) data sets withrobust, scalable semantic annotations based in lin-guistic theory.
The UD project1 aims to provide(i) a syntactic dependency annotation standard thatcan be used consistently across many languagesand (ii) a collection of corpora that use that stan-dard (De Marneffe et al, 2014; Nivre et al, 2015).Decomp provides complementary semantic annota-tions that scale across different types of semantic in-formation and different languages and can integrateseamlessly with any UD-annotated corpus.Decomp has two mutually supportive tenets?semantic decomposition and simplicity.
As we dis-cuss further in the next section, these tenets allowus to collect annotations from everyday speakers ofa language that are rooted in basic, commonsensical1http://universaldependencies.orgYour buddy Beau invited meamodnmod:possnsubjdobjAWARENESS +VOLITION +... ...ARG ARGREALIS +... ...ANIMATE +...
...Figure 1: Decompositional semantics atop syntax.aspects of meaning and that can be straightforwardlyexplained and generally agreed upon in context.In this paper, we describe Decomp protocols forthree domains?semantic role decomposition, eventdecomposition, and word sense decomposition?and we present annotation results on top of the En-glish UD v1.2 (EUD1.2) treebank.2 We begin in ?2by connecting Decomp with previous work on de-composition in linguistic theory.
In ?3, we presentPredPatt, which is a software package for prepro-cessing UD annotated corpora for input into De-comp protocols.
In ?4, we present a major revi-sion to Reisinger et al?s (2015) semantic role de-composition protocol (SPR1).
Our revision, SPR2,brings SPR1 into full alignment with Decomp whileadding various new features.
In ?5, we presentDecomp-aligned annotation of event properties, fo-cusing specifically on event realis.
Finally, in ?6, wedescribe a Decomp-aligned word sense decomposi-tion protocol and associated set of annotations.2 Universal Decompositional SemanticsA range of perspectives suggest that the proper rep-resentation for word meanings is decompositional.2All datasets are available at http://decomp.net.1713For example, Dowty (1979) followed by a sub-stantial amount of research (e.g.
Jackendoff 1990;Rappaport-Hovav and Levin 1998; Levin and Rap-paport Hovav 2005) suggests that word meaningscan be factored into (i) idiosyncratic, item-specificcomponents and (ii) general components, such asCAUSATION, that are used across the lexicon.
Inthe domain of thematic roles, Dowty (1991) arguesthat notions such as AGENT should be decomposedinto simpler, more primitive properties such as voli-tional participation in an event.
Pustejovsky (1991)decomposes word meanings into qualia structuresthat again incorporate more primitive properties ofevents and individuals.
In spite of this wealth oftheory, existing annotation protocols rarely take theidea into account, operating at the level that theabove approaches decompose with very few excep-tions (Greene and Resnik, 2009; Hartshorne et al,2013; Reisinger et al, 2015).
Decomp?s premise isthat a decompositional approach to large-scale an-notation has benefits for both the annotation pro-cess and downstream uses of annotated data (cf.He et al 2015; Bos et al 2017 for recent non-decompositional approaches).To capture these benefits, Decomp incorporatessemantic decomposition directly into its protocolsby mapping from decompositional theories, such asDowty?s, into straightforward questions on binaryproperties that are easily answered.
This methodof constructing annotation protocols gives rise tosimplicity in the protocol, since the resulting ques-tions are much more commonsensical and easily ex-plained than the concepts they are decomposing.
Forinstance, Dowty (1991) decomposes the relativelyunintuitive notion (for ordinary speakers) of AGENTinto much simpler properties, such as VOLITION andMOVEMENT.
Instead of asking about whether a ver-bal argument is an AGENT (with the concomitantcomplex training process for annotators), a Decompprotocol might then ask about whether the referentof the argument had volition in or moved as a resultof the event.
Simplicity in the protocol in turn al-lows a Decomp protocol to gather annotations fromuntrained native (and na?
?ve) speakers of a language.In large part, the current paper is focused on devel-oping questions that everyday speakers can agree onand that are key for lexical representations.An added benefit of questions on binary proper-ties is they allow the use of ordinal prompts (Likertscales), allowing annotators to record subjective un-certainty of a property in a given context, which canthen be aggregated across multiple responses withless severe impact to inter-annotator agreement.3 Predicative patternsIn this section, we introduce PredPatt, which is alightweight tool for identifying the structure of pred-icates and arguments from Universal Dependencies.We use the PredPatt?s output as input to the Decomp-aligned annotation protocols we describe in ?4 ?
?6.To ensure that this output is accurate, we evaluate onmultiple UD-annotated corpora: automatically gen-erated English parses and gold treebanks in Chinese,English, Hebrew, Hindi, and Spanish.3PredPatt employs deterministic, unlexicalizedrules over UD parses.
We provide a high-leveloverview of the process here.4Input UD Parse with Universal POS tags1.
Predicate and argument root identification2.
Argument resolution3.
Predicate and argument phrase extraction4.
Optional Post-processingOutput collection of predicate-argument structuresUD Parse A universal dependency (UD) parse, isa set of labeled pairs.
Each pair has the form RE-LATION(DEPENDENT, GOVERNOR).
The UD parsealso includes a sequence of Universal POS tags.
Anexample of a UD parse is in Figure 1.Predicate and argument root identificationPredicate and argument roots (i.e., dependencytree nodes) are identified by local configurations?specifically, edges in the UD parse.
The simplestexample is NSUBJ(s, v) and DOBJ(o, v), whichindicate that v is a predicate root, and that s ando are argument roots.
Similarly, roots of clausalsubjects and clausal complements are also predicateroots.
Nominal modifiers inside adverbial modifiers3While we are not aware of a similar tool for UniversalDependencies, PredPatt is similar to ClausIE (Del Corro andGemulla, 2013) and ArgOE (Gamallo et al, 2012), which sup-ports Spanish, Portuguese, Galician and English.4A detailed description of PredPatt is available at https://github.com/hltcoe/PredPatt.
PredPatt derivesfrom the system described by Rudinger and Van Durme (2014).1714are arguments to the verb being modified, e.g.,Investors turned away from [the stock market].PredPatt also extracts relations from appositives,possessives, copula, and adverbial modifiers.Argument resolution PredPatt includes argu-ment resolution rules to handle missing argumentsof many syntactic constructions, including predi-cate coordination, relative clauses, and embeddedclauses.
Argument resolution is crucial in lan-guages that mark arguments using morphology, suchas Spanish and Portuguese, because there are morecases of covert subjects.
Other common cases forargument resolution are when predicates appear ina conjunction, e.g., Chris likes to sing and dance,has no arc from dance to its subject Chris.
In rela-tive clauses, the arguments of an embedded clauseappear outside the subtree, e.g., borrowed in Thebooks John borrowed from the library are overdue.has books as an argument and so does are-overdue.Predicate extraction PredPatt extracts a descrip-tive name for complex predicates.
For example,[PredPatt] finds [structure] in [text] has a 3-placepredicate named (?a finds ?b in ?c).
The primarylogic here is (a) to lift mark and case tokens (e.g.,in) out of the argument subtree, (b) to add adverbialmodifiers, auxiliaries, and negation (e.g., [Chris] didnot sleep quietly).
PredPatt uses the text order of to-kens and arguments to derive a name for the pred-icate; no effort is made to further canonicalize thisname, nor align it to a verb ontology.Argument phrase extraction Argument extrac-tion filters tokens from the dependency subtree be-low the argument root.
These filters primarily sim-plify the subtree, e.g., removing relative clauses andappositives inside an argument.
The default set offilters were chosen to preserve meaning, since it isnot generally the case that all modifiers can safelybe dropped (more aggressive argument simplifica-tion settings are available as options).Post-processing PredPatt implements a number ofoptional post-processing routines, such as conjunc-tion expansion, argument simplification (which fil-ters out non-core arguments, leaving only subjectsLang #Sent #Output PrecisionChinese 98 375 69.1% ?4.7%English 79 210 86.2% ?4.7%Hebrew 12 30 66.7% ?17.9%Hindi 22 50 52.0% ?14.3%Spanish 27 55 70.9% ?12.4%Table 1: Results of manual evaluation of PredPatt on UDand objects), and language-specific hooks.5Gold treebanks in multiple languages We eval-uated PredPatt manually on several randomly sam-pled sentences taken from the UD banks of Chinese,English, Hebrew, Hindi and Spanish.
This evalua-tion runs PredPatt with the gold standard UD parse.We report the number of sentences evaluated alongwith the number of extractions from those sentences(a proxy for recall) and precision (95% confidenceinterval) for each language in Table 1.4 Semantic role decompositionA decompositional strategy has been successfullyused by Reisinger et al (2015) to annotate thematicrole information under their Semantic Proto-Role la-beling protocol (SPR1), which is based on Dowty?s(1991) seminal thematic proto-role theory.6In this section, we present a major revision toSPR1 aimed at strengthening and generalizing theprotocol beyond Reisinger et al?s dataset.
Wepresent three pilots aimed at validating our new pro-tocol as well as a bulk annotation of a large subsetof core arguments in EUD1.2.
Finally, we describe,deploy, and validate methods for extending SPR2?sreach beyond this subset, resulting in SPR2.1.4.1 SPR1 protocolIn the SPR1 protocol, each core argument of a verbis annotated for the likelihood that particular proper-ties hold of that argument?s referent as a participantin the event denoted by the verb.Property questions The properties selected forthis purpose, given in Table 2, are based on thoseinvoked by Dowty (1991) in his prototype-theoretic5UD itself allows for language-specific exceptions to the?universal?
standard, and we therefore allow that practice here.6See Kako 2006; Greene and Resnik 2009; Madnani et al2010; Hartshorne et al 2013 for work using similar protocols.1715Role property How likely or unlikely is it that...instigation ARG caused the PRED to happen?volition ARG chose to be involved in the PRED?awareness ARG was/were aware of being involved in the PRED?sentient ARG was/were sentient?change of location ARG changed location during the PRED?-exists as physical ARG existed as a physical object?existed before ARG existed before the PRED began?existed during ARG existed during the PRED?existed after ARG existed after the PRED stopped?change of possession ARG changed possession during the PRED?change of state ARG was/were altered or somehow changed during or bythe end of the PRED?-stationary ARG was/were stationary during the PRED?-location of event ARG described the location of the PRED?-physical contact ARG made physical contact with someone or somethingelse involved in the PRED?was used ARG was/were used in carrying out the PRED?-pred changed arg The PRED caused a change in ARG?+was for benefit PRED happened for the benefit of ARG?+partitive Only a part or portion of ARG was involved in the PRED?+change of state continuous The change in ARG happened throughout the PRED?Table 2: Questions posed to annotators.
+ indicates questionsnew to SPR2; - indicates SPR1 questions dropped in SPR2.reconstruction of linking theory.
Reisinger et al?s(2015) SPR1 dataset, produced under this proto-col, provides annotations of the Wall Street Jour-nal portions of the Penn Treebank (PTB; Marcuset al 1993) that are also annotated for core argumentPropBank (Palmer et al, 2005) roles.Filtering and data collection In Reisinger et al2015, verbs were excluded that occur in certainsyntactic environments that interfere with propertyjudgments.
In particular, participles and impera-tives were excluded, as well as verbs in embeddedclauses, in questions, and under negation or auxil-iaries.
We carry these filters forward to our own bulkannotation by implementing them over PredPatt out-put and then show how these filters can be lifted.Annotators To ensure internal consistency of thejudgments, Reisinger et al?s data was based on a sin-gle Amazon Mechanical Turk annotator.4.2 SPR2 protocolFirst we update both the set of questions andthe method for presenting these questions in orderto streamline the annotation process and simplifyReisinger et al?s protocol.
Second, to deal with po-tentially ungrammatical sentences, as well as to addan extra layer of quality control to the generationof property questions, we add an acceptability judg-ment question to the protocol.
Finally, we collect an-notations from multiple trusted annotators with two-way redundancy, allowing us to normalize the datain a way that is impossible with SPR1.Property questions While Reisinger et al?s prop-erties were mainly motivated by linguistic theory,in the process of developing SPR2 we identifiedseveral redundancies as well as potential sourcesof error; these changes are summarized in Table2.
Redundancies include stationary being es-sentially the negation of change of location,and predicate changed argument beingalmost identical to change of state.
Theproperty exists as physical was droppedbecause it is a purely referential (non-relational)property of the argument; thus, it is redundantwith our more elaborated decompositional wordsense protocol.
The location of event andphysical contact properties were removedbecause of lower interannotator agreement and highwithin-annotator response variance in SPR1.In addition to this streamlining, we added threenew properties that target new types of arguments:benefactives, partitives, and incremental themes.Benefactive arguments and partitive arguments of-ten have special morphosyntactic properties in manylanguages.
In English, for example, benefactivescan appear in double-object constructions with verbslike buy, and in many languages they correlate withspecial morphology.
Partitives involve partial af-fectedness and similarly are often marked with mor-phological case (Kiparsky, 1998).
The third newproperty, change of state continuous, isa plain-language version of Dowty?s (1991) incre-mental theme proto-role property, which Reisingeret al (2015) did not include.
An argument is anINCREMENTAL THEME with respect to an event ifthe temporal progress of the event can be measuredin terms of, or put into correspondence with, thepart-whole structure of that argument which under-goes some gradual change (Tenny, 1987; Krifka,1989, a.o.).
For example, in an event of mowingthe lawn, the lawn is an incremental theme becausethe progress of mowing is directly related to the por-tion of the lawn that has been mowed.
Though in-cremental theme is quite abstract in comparison toother proto-role properties, it is widely agreed thatsomething like this property is involved in linkingthematic roles to syntactic position.Dynamic reveal The question corresponding tothe change of state continuous property1716presupposes that the argument under considerationdid, in fact, undergo some change of state.
Thismeans that if an annotator has previously deter-mined that the property change of state doesnot apply, then asking about change of statecontinuous is at best inefficient, since we can de-terministically predict that the answer should be NA,and at worst confusing to the annotator, since thequestion triggers a presupposition failure.To avoid such presupposition failures in SPR2,which we suspect led to additional noise and anno-tation time as part of SPR1, we modified the an-notation interface so that certain questions are re-vealed dynamically based on the answers to otherquestions.
The set of questions is now orga-nized hierarchically instead of as a flat list.
Inthis hierarchical structure, change of state isa parent of change of state continuous,which means that the latter question only appears ifthe annotator gives a high ordinal value to the for-mer.
Questions that remain hidden are assumed tohave NA as their answer.
For SPR2, this pair of prop-erties is the only one affected by the dynamic revealfeature, though this aspect of the protocol will beextended in later versions.Acceptability judgments Two kinds of grammat-ical acceptability judgments were collected.
Thefirst kind, collected on a five-point scale, asked aboutthe acceptability of the sentence containing the ar-gument in question.
The second kind, collected asa binary judgment, asked whether the question washard to answer because of grammatical errors.
Thissecond was triggered only when annotators gave aresponse on the bottom three values of the ordinalscale for the relevant property question.
We do notanalyze these judgments here for reasons of space,but they are available as part of the released dataset.Multiple annotators with redundancy Reisingeret al?s (2015) reason for not using redundant an-notations was that a single annotator would pro-vide internally consistent judgments, but this con-sistency comes at the cost of potential bias in thejudgments.7 In order to evaluate bias, we move to7For example, in analyzing the SPR1 dataset that Reisingeret al make available, we noted that their annotator has a some-what idiosyncratic way of answering the was used question,which aims at identifying instruments: the annotator marks himFigure 2: Example of semantic role decomposition task.two-way redundancy (and later versions of the pro-tocol are compatible with greater redundancy).Heterogeneous data SPR2 extends the coverageof Semantic Proto-Role Labeling to heterogeneousgenres.
The SPR1 dataset contains only annotationsof newswire text.
This is not ideal for either prac-tical or scientific purposes, since newswire tends tobe biased toward otherwise rare word senses?oftenpertaining to financial markets?but low coverage ofotherwise common word sense.To remedy these coverage issues, we extend SPR1to the English Universal Dependencies (version 1.2)treebank (EUD1.2).
EUD1.2 is based on the Lin-guistic Data Consortium?s English Web Treebank(Bies et al, 2012) and contains a much wider setof genres than the Penn Treebank?including we-blogs, newsgroup discussions, emails from the En-ronSent Corpus, reviews from English Google re-views, and answers from Yahoo!
Answers.
EUD1.2has the added benefit of being natively annotatedwith gold-standard Universal Dependencies (UD)parses (Nivre et al, 2015).4.3 Pilot experimentsIn this section, we present three pilot experimentsconducted on a subset of EUD1.2 and aimed at val-idating the updated protocol in preparation for de-ployment of the full task.
In the first, we use theSPR1 protocol to obtain judgments on a small sam-ple of EUD1.2 sentences from the same trusted an-notator that produced SPR1.
In the second, we openthe same task to multiple annotators.
And in thethird, we deploy our updated SPR2 protocol on thein (i) as likely to have been used in carrying out the advising.
(i) Sen. Bill Bradley of New Jersey advised him that the DowJones Industrial Average had declined by 190 points.This is a general pattern for this question for this annotator.1717same subset of EUD1.2?again, open to multiple an-notators.
We use these three pilots to evaluate in-terannotator agreement within and across protocols(where possible) and to construct a pool of trustedannotators to work on the full annotation task.Item selection For each pilot, the same set of sen-tences were used.
These sentences were selectedbased on properties of both the predicate and itscorresponding arguments in each sentence.
The pi-lot experiments were limited to the same 10 verbs(want, put, think, see, know, look, say, take, tell,give) that were considered in Reisinger et al?s pilot.As in Reisinger et al 2015, tokens were excludedwith verbs that occur in certain syntactic environ-ments that interfere with property judgments.
Weused the same filters described by those authors,modified for UD.
Additionally, verbs occurring asthe second item in a conjunction were removed, asEUD1.2 does not have sufficient annotation to iden-tify all arguments of such verbs from the syntax.Verbal arguments were defined as the subtreesgoverned by a verb via a core grammatical relation(nsubj, nsubpass, dobj, and iobj).
In addi-tion, occurrences of the pronoun it in subject posi-tion were excluded because of inconsistencies in theannotation of expletive subjects in EUD1.2.Pilots 1 & 2: SPR1 protocol Pilot 1, designed tocompare SPR1 directly to SPR2, used the same pro-tocol described in Reisinger et al 2015 and was de-ployed on 99 argument tokens selected based on themethod above.8 To ensure that the only differencebetween SPR1 and this pilot was which sentenceswere annotated, we obtained the AMT identifier forthe SPR1 annotator from Reisinger et al Thus, theonly annotator in this pilot was the same one thatproduced all the annotations for the SPR1 dataset.The data from this pilot cannot be comparedto the SPR1 dataset on a token level, since theitems do not come from the same dataset.
Butthese data can be compared to the SPR1 dataseton a type level by averaging responses to particu-lar questions asked about particular argument po-sitions (e.g., nsubj, dobj, etc.)
for a particularpredicate (e.g., want, put, etc.)
and then comparing8For each verb, 10 arguments were selected, with the excep-tion of see, which only had 9 due to an off-by-one error.the correlation between these averages.
The aver-age type-level correlation between the average by-predicate, by-argument relation ratings in the SPR1dataset and those in the current pilot was high for allverb-argument pairs (Spearman ?=0.82).Pilot 2 uses the same materials and protocol asPilot 1.
The only difference between the two is thatthis pilot was open to multiple annotators.
A totalof 33 annotators participated, one of whom was thesame annotator that produced all the annotations forthe SPR1 dataset and participated in Pilot 1.For each argument token, we collected five judg-ments per property question.
Interannotator agree-ment was calculated by argument token for the like-lihood responses using pairwise Spearman rank cor-relations.
The mean ?
across all annotator pairs andargument tokens was 0.562 (95% CI=[0.549, 0.574])and, due to heavy left skew, the median was 0.618(95% CI=[0.603, 0.631]).
This agreement is rela-tively high, suggesting that different annotators tendto agree on the relative likelihood of a property ap-plying to an argument.Since the SPR1 and Pilot 1 annotator was amongthis group, we can also assess the extent to which thePilot 1 annotator is consistent with other annotators.Comparing this annotator to every other annotatorthat annotated the same argument token, the mean?
was 0.499 (95% CI=[0.451, 0.546]), and the me-dian was 0.565 (95% CI=[0.504, 0.637]).
This sug-gests that, on average, the other annotators are evenmore consistent with each other than they are withthe original SPR1 annotator, vindicating the use ofmultiple annotators.Pilot 3: SPR2 protocol Pilot 3 uses the same ma-terials as Pilots 1 and 2 but introduces the SPR2 pro-tocol laid out above.
A total of 57 annotators par-ticipated in this pilot.
For each argument token, weagain collected five judgments per property.Interannotator agreement was calculated by argu-ment token for the likelihood responses using pair-wise Spearman rank correlations.
The mean ?
acrossall annotator pairs and argument tokens was 0.622(95% CI=[0.610, 0.634]) and, again due to heavyleft skew, the median was 0.677 (95% CI=[0.662,0.690]).
This higher agreement compared to Pilot 2likely arises due to the fact that we have fewer ques-tions in the SPR2 protocol and suggests that we suc-1718ceeded in removing noisy questions without addingquestions that were similarly noisy.Since we use the same materials as Pilots 1 and2, we can also compare the SPR1 and SPR2 proto-cols on the subset of questions they share.
We findsimilar mean agreement, at 0.593 (95% CI=[0.580,0.607]), and median agreement, at 0.665 (95%CI=[0.652, 0.672]), to that we found within the Pi-lots 2 and 3 results.
This suggests that the addi-tion and subtraction of questions does not substan-tially alter annotators?
judgments on the questionsthat both protocols share.4.4 Trusted annotator poolTo ensure annotation consistency in our bulk an-notation, we constructed a pool of trusted annota-tors from those annotators that participated in Pi-lots 2 and 3.
We used two metrics to construct thispool: rating agreement and applicability agreement.Both of these metrics control for various factors thatmight raise or lower agreement independent of theannotator?e.g., the particular question, the partic-ular sentence, the particular argument type, etc.
?using generalized linear mixed effects models.
Thispool contains a total of 86 trusted annotators.4.5 Bulk taskFor our bulk task, we used the SPR2 protocol to an-notate a total of 3,806 argument tokens spanning2,759 unique predicate lemmas.
These argumenttokens were part of a filtered set constructed usingReisinger et al?s filtering scheme described above.We collected two judgments per property, per ar-gument token.
Interannotator agreement was calcu-lated in the same way as for the pilots.
The mean ?was 0.617 (95% CI=[0.611, 0.623]), and the medianwas 0.679, (95% CI=[0.673, 0.686]).
This agree-ment is very close to that found in the pilots, sug-gesting that rating consistency extends beyond theconstrained set of predicates used in the pilots.One issue with SPR1 that remains unaddressed inSPR2 is the use of filters.
This significantly reducesthe potential coverage of the protocol and relies onextremely rich syntactic annotation.
This second isnot problematic when we have gold standard tree-banks like EUD1.2, but it becomes an issue whenmoving beyond such treebanks.To alleviate this filter issue, we propose a furtherrevision of SPR2.
In this version (SPR2.1), we al-ter the SPR2 instructions to take into account caseswhere the property questions may be difficult to an-swer.
These fall into at least three categories: even-tualities that haven?t happened (irrealis eventuali-ties), generics, and habituals.
In SPR2.1, annotatorsare instructed about each case and to answer as if aspecific event of that kind did actually happen.We annotated predicates that occurred in a sen-tence from the previous bulk task but were filteredfrom that task based on Reisinger et al?s (2015) fil-ters.
We have so far annotated all such predicateswith less than 100 instances in all of EUD1.2 andplan to continue annotation to get full coverage ofthese sentences.A total of 26 annotators from our trusted pool par-ticipated in this annotation.
As in the previous bulktask, we collected two judgments per property, perargument token.
The interannotator agreement wascalculated in the same way as for the previous bulktask and pilots and was reasonably high with a mean?
of 0.528 (95% CI=[0.522, 0.535]) and median ?of 0.571 (95% CI=[0.563, 0.580]).
This somewhatlower agreement is to be expected, since these pred-icates were selected to be harder than those in theprevious task.4.6 DiscussionWe presented a major revision to Reisinger et al?s(2015) decompositional Semantic Proto-Role Label-ing protocol (SPR1) and deployed this revised proto-col (SPR2) in three validation pilots and a bulk task.We then described two extensions to this protocolaimed at expanding the annotable arguments.One issue that arises with SPR2.1 is that it sub-stantially complicates the instructions, clashing withDecomp simplicity tenet.
In the next section, we de-scribe a task aimed at allowing us to better targetpredicates that need these more elaborated instruc-tions, allowing us to use the simpler SPR2 protocolwhere possible.5 Event decompositionAs discussed in ?4, SPR1 and SPR2 employ filtersthat run on top of dependency parses to ensure thatproto-role property questions about particular argu-1719Figure 3: Example of the event decomposition taskments are answerable.
We showed that these filterscan be bypassed by altering the instructions givento annotators.
This approach substantially increasesthe length of the tasks instructions, however, and soideally, these lengthened instructions should be usedonly when absolutely necessary.
One place it seemslikely to be necessary is when the event in a sentencedid not in fact occur.In this section, we present a protocol, inspired bythe one developed by de Marneffe et al (2012), fortargeting these sorts of sentence with special instruc-tions in future versions of SPR (see also Saur??
andPustejovsky 2012).
A major benefit of this protocolis that it produces a foundation for future decompo-sitional event annotations.Protocol The protocol has four major compo-nents: questions about (i) whether or not a particu-lar word refers to an eventuality (event or state); (ii)whether the sentence is understandable; (iii) whetheror not, according to the author, the event has alreadyhappened is currently happening; and (iv) how con-fident the annotator is about their answer to (iii).The first two components were included to filterout items that are either incorrectly labeled as pred-icates or that the annotator could not annotate forcomponents (iii) and (iv), and if an annotator an-swered no to either for a particular predicate candi-date, (iii) and (iv) did not appear.
Thus, like SPR2.x,this protocol incorporates a hierarchy of questionsthat can be elaborated in future versions.Data collection We applied this protocol to everypredicate candidate found in an EUD1.2 sentenceannotated under SPR2 and SPR2.1.
This yields an-notations for a superset of the predicates annotatedunder SPR2.x, and thus components (i) and (ii) ofthese annotations can be used as a post hoc filter onthe SPR2.x annotations or to decide on whether toinclude a predicate for future SPR2.x tasks.A total of 6,930 predicate candidates were an-notated in batches of 10 by 24 unique annotatorsrecruited from the trusted annotator pool built forSPR2.x.
Each predicate candidate was judged bytwo distinct annotators.Data validation For each of the four componentsinterannotator agreement was computed by eachgroup of 10 predicates.
For the categorical re-sponses, we would ideally use Cohen?s ?, but therewere so many cases of perfect agreement for the cat-egorical responses that Cohen?s ?
is ill-defined inmany cases.
As such, we report raw agreement here.The mean raw agreement for whether each pred-icate candidate was a predicate was 0.955 (95%CI=[0.950, 0.960]).
The mean raw agreementfor whether the sentence was understandable was[0.976, (95% CI=[0.971, 0.980]); and the mean rawagreement for whether the eventuality happened orwas happening was 0.820 (95% CI=[0.811, 0.829]).Discussion We presented the first version of a newevent decomposition protocol.
This protocol inte-grates with and is in the same spirit as the SPR2.xprotocols produced in the previous section.In the next section, we describe a complemen-tary protocol for decomposing word sense, focusingspecifically on noun senses.
This last protocol com-pletes a picture wherein we decompose predicate ar-gument semantics into three parts: the properties ofa predicate independent of its arguments, the prop-erties of a predicate?s arguments in relation to theevent the predicate denotes, and the properties of anargument independent of the predicate.6 Word sense decompositionIn ?4 and ?5, we focused on semantic questions thatdeal with eventualities.
In this section, we describe aDecomp protocol for decomposing word sense.
Ourgoal is similar to that in previous sections: elicit re-sponses from everyday speakers of the language re-garding basic properties, in relation to the context ofa natural language sentence.Protocol If directly following the strategy ex-plored thus far, we would create an interface thatenumerated many dozens (or hundreds) of seman-tic properties one might ask about a word in context,and in further developments of Decomp there may1720Figure 4: Example of the word sense task.be specific properties that are deemed essential fordirect querying of annotators.
However, here we relyon the rich pre-existing taxonomy of lexical knowl-edge captured in the WordNet hierarchy (Miller,1995) in order to more efficiently gather implicitproperty responses.
Everyday speakers can per-form basic word sense disambiguation (Snow et al,2008): this falls under the simplicity tenant of De-comp.
Once a word is disambiguated in context wethen can infer automatically whether an instance is,e.g., a physical object.While WordNet is a valuable resource, the selec-tion of a specific categorical sense under an enu-merated set of prespecified options is troubling in asimilar way as Dowty was concerned with thematicroles (see Kilgarriff 1997).
Therefore we follow apath similar to Sussna (1993) in asking annotatorsfor zero or more senses that are appropriate.9Candidate senses are extracted from WordNetsynsets.
We have grounded argument tokens inWordNet in order to make efficient use of existinglexical semantic resourses, but this protocol could inprinciple be used with any other lexical semantic re-source.
We believe these annotations will be usefulalready in the context of the other annotations, butin addition, future work will use these sense ground-ings to derive commonsense properties beyond thosedirectly encoded in the WordNet hierarchy.Data collection A total of 18,054 word tokens(arguments) in 10,833 total sentences extractedfrom EUD1.2 were annotated for sense by at leastthree annotators recruited from Amazon MechanicalTurk.
Each token had an average of 5.63 candidatessenses for annotators to choose from (Figure 4).
Intotal, 1,065 unique annotators participated.9Not only does this weaken the commitment to a single cat-egorical meaning, but it also reduces concerns of annotators be-ing confused by overly fine-grain definitions (Navigli, 2006).Data validation Inter-annotator agreement wascomputed by lemma by taking the Jaccard index foreach pair of annotators that judged the senses for thatlemma: # of senses checked by both annotators# of senses checked by either annotator .
The overallinter-annotator agreement using this measure was0.592: this is reasonably high considering the ex-tremely low chance-level.In total, 9,317 token-sense pairs were agreed uponby all annotators.
We refer to these token-sense pairsas gold word sense(s) for the token.
If we relax theagreement threshold for a token-sense pair to be goldto 0.5?i.e.
half or more annotators agreed on thatpair?the number of gold word sense(s) goes up to27,326.
Out of 18,054 individual arguments, 8,553of them have a single gold word sense and 370 havetwo or more gold word senses as in the examplein Figure 4.
Similarly, if we relax the agreementthreshold down to 0.5, 9,656 arguments have a sin-gle gold word sense and 17,281 arguments have twoor more gold word senses.7 ConclusionWe have described the Universal DecompositionalSemantics (Decomp) project, which aims to con-struct and deploy a set of cross-linguistically ro-bust semantic annotation protocols that are basedin linguistic theory and that integrate seamlesslywith the Universal Dependencies project.
Wethen proposed Decomp-aligned protocols for threedomains?semantic role decomposition, event de-composition, and word sense decomposition?andpresented annotations, all freely available, that usethese protocols and are constructed on top of the En-glish UD v1.2 treebank.
In future work, we intendto further revise and extend these protocols as wellas produce novel protocols aligned with Decomp.AcknowledgmentsThis research was supported by the JHU HLTCOE,DARPA DEFT, DARPA LORELEI, and NSF IN-SPIRE BCS-1344269.
The U.S. Government isauthorized to reproduce and distribute reprints forGovernmental purposes.
The views and conclusionscontained in this publication are those of the authorsand should not be interpreted as representing offi-cial policies or endorsements of DARPA or the U.S.Government.1721ReferencesAnn Bies, Justin Mott, Colin Warner, and SethKulick.
English web treebank.
Linguistic DataConsortium, Philadelphia, PA, 2012.Johan Bos, Valerio Basile, Kilian Evang, NoortjeVenhuizen, and Johannes Bjerva.
The GroningenMeaning Bank.
In Nancy Ide and James Puste-jovsky, editors, Handbook of Linguistic Annota-tion.
Springer, Berlin, 2017.Marie-Catherine de Marneffe, Christopher D. Man-ning, and Christopher Potts.
Did it happen?
Thepragmatic complexity of veridicality assessment.Computational Linguistics, 38(2):301?333, 2012.Marie-Catherine De Marneffe, Timothy Dozat, Na-talia Silveira, Katri Haverinen, Filip Ginter,Joakim Nivre, and Christopher D. Manning.
Uni-versal Stanford dependencies: A cross-linguistictypology.
In Proceedings of LREC, volume 14,pages 4585?4592, 2014.Luciano Del Corro and Rainer Gemulla.
Clausie:clause-based open information extraction.
In Pro-ceedings of the 22nd international conference onWorld Wide Web, pages 355?366, 2013.David Dowty.
Word Meaning and Montague Gram-mar.
D. Reidel Publishing Company, 1979.David Dowty.
Thematic proto-roles and argumentselection.
Language, 67(3):547?619, 1991.Pablo Gamallo, Marcos Garcia, and SantiagoFerna?ndez-Lanza.
Dependency-based open infor-mation extraction.
In Proceedings of the JointWorkshop on Unsupervised and Semi-SupervisedLearning in NLP, pages 10?18, 2012.Stephan Greene and Philip Resnik.
More thanwords: Syntactic packaging and implicit senti-ment.
In Proceedings of Human Language Tech-nologies: The 2009 Annual Conference of theNorth American Chapter of the Association forComputational Linguistics, pages 503?511.
As-sociation for Computational Linguistics, 2009.ISBN 1-932432-41-8.Joshua K. Hartshorne, Claire Bonial, and MarthaPalmer.
The VerbCorner Project: Toward anEmpirically-Based Semantic Decomposition ofVerbs.
In Proceedings of the 2013 Conference onEmpirical Methods in Natural Language Process-ing, pages 1438?1442, 2013.Luheng He, Mike Lewis, and Luke Zettlemoyer.Question-Answer Driven Semantic Role Label-ing: Using Natural Language to Annotate NaturalLanguage.
In Proceedings of the 2015 Confer-ence on Empirical Methods in Natural LanguageProcessing, pages 643?653, 2015.Ray S. Jackendoff.
Semantic Structures.
MIT Press,1990.Edward Kako.
Thematic role properties of subjectsand objects.
Cognition, 101(1):1?42, 2006.Adam Kilgarriff.
I don?t believe in word senses.Computers and the Humanities, 31(2):91?113,1997.Paul Kiparsky.
Partitive case and aspect.
The Pro-jection of Arguments: Lexical and compositionalfactors, 265:307, 1998.Manfred Krifka.
Nominal reference, temporalconstitution, and quantification in event seman-tics.
In R. Bartsch, J. van Benthem, and P. vonEmde Boas, editors, Semantics and ContextualExpression.
Foris Publications, 1989.Beth Levin and Malka Rappaport Hovav.
Argumentrealization.
Cambridge University Press, 2005.Nitin Madnani, Jordan Boyd-Graber, and PhilipResnik.
Measuring transitivity using untrainedannotators.
In Proceedings of the NAACL HLT2010 Workshop on Creating Speech and Lan-guage Data with Amazon?s Mechanical Turk,pages 188?194, 2010.Mitchell P. Marcus, Mary Ann Marcinkiewicz,and Beatrice Santorini.
Building a LargeAnnotated Corpus of English: The PennTreebank.
Computational Linguistics, 19(2):313?330, June 1993.
ISSN 0891-2017.
URLhttp://dl.acm.org/citation.cfm?id=972470.972475.George A. Miller.
Wordnet: A lexical database forenglish.
Commun.
ACM, 38(11):39?41, Novem-ber 1995.
ISSN 0001-0782. doi: 10.1145/219717.219748.
URL http://doi.acm.org/10.1145/219717.219748.Roberto Navigli.
Meaningful clustering of senseshelps boost word sense disambiguation perfor-1722mance.
In Proceedings of the 21st Interna-tional Conference on Computational Linguisticsand 44th Annual Meeting of the Association forComputational Linguistics, pages 105?112, Syd-ney, Australia, July 2006.
Association for Com-putational Linguistics.
doi: 10.3115/1220175.1220189.
URL http://www.aclweb.org/anthology/P06-1014.Joakim Nivre, Z?eljko Agic?, Maria Jesus Aranzabe,Masayuki Asahara, Aitziber Atutxa, MiguelBallesteros, John Bauer, Kepa Bengoetxea,Riyaz Ahmad Bhat, Cristina Bosco, Sam Bow-man, Giuseppe G. A. Celano, Miriam Connor,Marie-Catherine de Marneffe, Arantza Diaz deIlarraza, Kaja Dobrovoljc, Timothy Dozat, Tomaz?Erjavec, Richa?rd Farkas, Jennifer Foster, DanielGalbraith, Filip Ginter, Iakes Goenaga, KoldoGojenola, Yoav Goldberg, Berta Gonzales, BrunoGuillaume, Jan Hajic?, Dag Haug, Radu Ion, ElenaIrimia, Anders Johannsen, Hiroshi Kanayama,Jenna Kanerva, Simon Krek, Veronika Laippala,Alessandro Lenci, Nikola Ljubes?ic?, Teresa Lynn,Christopher Manning, Ca?ta?lina Ma?ra?nduc, DavidMarec?ek, He?ctor Mart?
?nez Alonso, Jan Mas?ek,Yuji Matsumoto, Ryan McDonald, Anna Missila?,Verginica Mititelu, Yusuke Miyao, SimonettaMontemagni, Shunsuke Mori, Hanna Nurmi,Petya Osenova, Lilja ?vrelid, Elena Pascual,Marco Passarotti, Cenel-Augusto Perez, SlavPetrov, Jussi Piitulainen, Barbara Plank, MartinPopel, Prokopis Prokopidis, Sampo Pyysalo,Loganathan Ramasamy, Rudolf Rosa, ShadiSaleh, Sebastian Schuster, Wolfgang Seeker,Mojgan Seraji, Natalia Silveira, Maria Simi,Radu Simionescu, Katalin Simko?, Kiril Simov,Aaron Smith, Jan S?te?pa?nek, Alane Suhr, ZsoltSza?nto?, Takaaki Tanaka, Reut Tsarfaty, SumireUematsu, Larraitz Uria, Viktor Varga, VeronikaVincze, Zdene?k Z?abokrtsky?, Daniel Zeman,and Hanzhi Zhu.
Universal Dependencies 1.2.http://universaldependencies.github.io/docs/,November 2015.
URL https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-1548.Martha Palmer, Daniel Gildea, and Paul Kingsbury.The proposition bank: An annotated corpus of se-mantic roles.
Computational Linguistics, 31(1):71?106, 2005.James Pustejovsky.
The generative lexicon.
Compu-tational Linguistics, 17(4):409?441, 1991.M.
Rappaport-Hovav and Beth Levin.
Building verbmeanings.
In M. Butts and W. Geuder, editors,The Projection of Arguments: Lexical and com-positional factors, pages 97?134.
CSLI Publica-tions, 1998.Drew Reisinger, Rachel Rudinger, Francis Fer-raro, Craig Harman, Kyle Rawlins, and BenjaminVan Durme.
Semantic Proto-Roles.
Transactionsof the Association for Computational Linguistics,3:475?488, 2015.Rachel Rudinger and Benjamin Van Durme.
Is thestanford dependency representation semantic?
InACL Workshop: EVENTS, 2014.Roser Saur??
and James Pustejovsky.
Are you surethat this happened?
assessing the factuality de-gree of events in text.
Computational Linguistics,38(2):261?299, 2012.Rion Snow, Brendan O?Connor, Daniel Jurafsky,and Andrew Ng.
Cheap and fast ?
but is itgood?
Evaluating non-expert annotations for nat-ural language tasks.
In Proceedings of the 2008Conference on Empirical Methods in NaturalLanguage Processing, pages 254?263, Honolulu,Hawaii, October 2008.
Association for Com-putational Linguistics.
URL http://www.aclweb.org/anthology/D08-1027.Michael Sussna.
Word sense disambiguation forfree-text indexing using a massive semantic net-work.
In Proceedings of the Second InternationalConference on Information and Knowledge Man-agement, CIKM ?93, pages 67?74, New York,NY, USA, 1993.Carol Lee Tenny.
Grammaticalizing aspect andaffectedness.
Thesis, Massachusetts Institute ofTechnology, 1987.
URL http://dspace.mit.edu/handle/1721.1/14704.1723
