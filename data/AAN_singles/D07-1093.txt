Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
887?896, Prague, June 2007. c?2007 Association for Computational LinguisticsA Probabilistic Approach to Diachronic PhonologyAlexandre Bouchard-Co?te??
Percy Liang?
Thomas L. Griffiths?
Dan Klein?
?Computer Science Division ?Department of PsychologyUniversity of California at BerkeleyBerkeley, CA 94720AbstractWe present a probabilistic model of di-achronic phonology in which individualword forms undergo stochastic edits alongthe branches of a phylogenetic tree.
Our ap-proach allows us to achieve three goals witha single unified model: (1) reconstructionof both ancient and modern word forms, (2)discovery of general phonological changes,and (3) selection among different phyloge-nies.
We learn our model using a MonteCarlo EM algorithm and present quantitativeresults validating the model.1 IntroductionModeling how languages change phonologicallyover time (diachronic phonology) is a central topicin historical linguistics (Campbell, 1998).
The ques-tions involved range from reconstruction of ancientword forms, to the elucidation of phonological driftprocesses, to the determination of phylogenetic re-lationships between languages.
However, this prob-lem has received relatively little attention from thecomputational community.
What work there is hasfocused on the reconstruction of phylogenies on thebasis of a Boolean matrix indicating the propertiesof words in different languages (Gray and Atkinson,2003; Evans et al, 2004; Ringe et al, 2002; Nakhlehet al, 2005).In this paper, we present a novel framework, alongwith a concrete model and experiments, for the prob-abilistic modeling of diachronic phonology.
We fo-cus on the case where the words are etymologicalcognates across languages, e.g.
French faire andSpanish hacer from Latin facere (to do).
Giventhis information as input, we learn a model actingat the level of individual phoneme sequences, whichcan be used for reconstruction and prediction, Ourmodel is fully generative, and can be used to reasonabout a variety of types of information.
For exam-ple, we can observe a word in one or more modernlanguages, say French and Spanish, and query thecorresponding word form in another language, sayItalian.
This kind of lexicon-filling has applicationsin machine translation.
Alternatively, we can alsoreconstruct ancestral word forms or inspect the ruleslearned along each branch of a phylogeny to identifysalient patterns.
Finally, the model can be used as abuilding block in a system for inferring the topologyof phylogenetic trees.
We discuss all of these casesfurther in Section 4.The contributions of this paper are threefold.First, the approach to modeling language change atthe phoneme sequence level is new, as is the spe-cific model we present.
Second, we compiled a newcorpus1 and developed a methodology for quantita-tively evaluating such approaches.
Finally, we de-scribe an efficient inference algorithm for our modeland empirically study its performance.1.1 Previous workWhile our word-level model of phonological changeis new, there have been several computational inves-tigations into diachronic linguistics which are rele-vant to the present work.The task of reconstructing phylogenetic trees1nlp.cs.berkeley.edu/pages/historical.html887for languages has been studied by several authors.These approaches descend from glottochronology(Swadesh, 1955), which views a language as a col-lection of shared cognates but ignores the structureof those cognates.
This information is obtained frommanually curated cognate lists such as the data ofDyen et al (1997).As an example of a cognate set encoding, considerthe meaning ?eat?.
There would be one column forthe cognate set which appears in French as mangerand Italian as mangiare since both descend from theLatin mandere (to chew).
There would be anothercolumn for the cognate set which appears in bothSpanish and Portuguese as comer, descending fromthe Latin comedere (to consume).
If this were theonly data, algorithms based on this data would tendto conclude that French and Italian were closely re-lated and that Spanish and Portuguese were equallyrelated.
However, the cognate set representation hasseveral disadvantages: it does not capture the factthat the cognate is closer between Spanish and Por-tuguese than between French and Spanish, nor dothe resulting models let us conclude anything aboutthe regular processes which caused these languagesto diverge.
Also, the existing cognate data has beencurated at a relatively high cost.
In our work, wetrack each word using an automatically obtainedcognate list.
While our cognates may be noisier,we compensate by modeling phonological changesrather than boolean mutations in cognate sets.There has been other computational work in thisbroad domain.
Venkataraman et al (1997) describean information theoretic measure of the distance be-tween two dialects of Chinese.
Like our approach,they use a probabilistic edit model as a formaliza-tion of the phonological process.
However, they donot consider the question of reconstruction or infer-ence in multi-node phylogenies, nor do they presenta learning algorithm for such models.Finally, for the specific application of cog-nate prediction in machine translation, essentiallytransliteration, there have been several approaches,including Kondrak (2002).
However, the phenom-ena of interest, and therefore the models, are ex-tremely different.
Kondrak (2002) presents a modelfor learning ?sound laws,?
general phonologicalchanges governing two completely observed alignedcognate lists.
His model can be viewed as a speciallaes itlavlibes ptitlait pteslait esptTopology 1 Topology 2 *Topology 3 *Topology 4Figure 1: Tree topologies used in our experiments.
*Topology3 and *Topology 4 are incorrect evolutionary tree used for ourexperiments on the selection of phylogenies (Section 4.4).case of ours using a simple two-node topology.There is also a rich literature (Huelsenbeck et al,2001) on the related problems of evolutionary biol-ogy.
A good reference on the subject is Felsenstein(2003).
In particular, Yang and Rannala (1997), Mauand Newton (1997) and Li et al (2000) each inde-pendently presented a Bayesian model for comput-ing posteriors over evolutionary trees.
A key dif-ference with our model is that independence acrossevolutionary sites is assumed in their work, whilethe evolution of the phonemes in our model dependson the environment in which the change occurs.2 A model of phonological changeAssume we have a fixed set of word types (cog-nate sets) in our vocabulary V and a set of languagesL.
Each word type i has a word form wil in each lan-guage l ?
L, which is represented as a sequence ofphonemes and might or might not be observed.
Thelanguages are arranged according to some tree topol-ogy T (see Figure 1 for examples).
One might con-sider models that simultaneously induce the topol-ogy and cognate set assignments, but let us fix bothfor now.
We discuss one way to relax this assump-tion and present experimental results in Section 4.4.Our generative model (Figure 3) specifies a dis-tribution over the word forms {wil} for each wordtype i ?
V and each language l ?
L. The genera-tive process starts at the root language and generatesall the word forms in each language in a top-downmanner.
One appealing aspect about our model isthat, at a high-level, it reflects the actual phonolog-ical process that languages undergo.
However, im-portant phenomena like lexical drift, borrowing, andother non-phonological changes are not modeled.888Our generative model can be summarized as fol-lows:For each word i ?
V :?wiROOT ?
LanguageModelFor each branch (k ?
l) ?
T :?
?k?l ?
Dirichlet(?)
[choose edit params.
]?For each word i ?
V :?
?wil ?
Edit(wik, ?k?l) [sample word form]In the remainder of this section, we describe eachof the steps in the model.2.1 Language modelFor the distributionw ?
LanguageModel, we used asimple bigram phoneme model.
The phonemes werepartitioned into natural classes (see Section 4 for de-tails).
A root word form consisting of n phonemesx1 ?
?
?xn is generated with probabilityplm(x1)n?j=2plm(xj | NaturalClass(xj?1)),where plm is the distribution of the language model.2.2 Edit modelThe stochastic edit model y ?
Edit(x, ?)
describeshow a single old word form x = x1 ?
?
?xn changesalong one branch of the phylogeny with parameters?
to produce a new word form y.
This process isparameterized by rule probabilities ?k?l, which arespecific to branch (k ?
l).The generative process is as follows: for eachphoneme xi in the old word form, walking fromleft to right, choose a rule to apply.
There arethree types of rules: (1) deletion of the phoneme,(2) substitution with another phoneme (possibly thesame one), or (3) insertion of another phoneme, ei-ther before or after the existing one.
The prob-ability of applying a rule depends on a context(NaturalClass(xi?1),NaturalClass(xi+1)).
Figure 2illustrates the edits on an example.
The context-dependence allows us to represent phenomena suchas the fact that s is likely to be deleted only in word-final contexts.The edit model we have presented approximatelyencodes a limited form of classic rewrite-driven seg-mental phonology (Chomsky and Halle, 1968).
One# C V C V C ## f o k u s ## f w O k o ## C V V C V #f ?
f / # Vo ?
w O / C Ck ?
k / V Vu ?
o / C Cs ?
/ V #Edits applied Rules usedFigure 2: An example of edits that were used to transformthe Latin word FOCUS (/fokus/) into the Italian word fuoco(/fwOko/) (fire) along with the context-specific rules that wereapplied.could imagine basing our model on more modernphonological theory, but the computational proper-ties of the edit model are compelling, and it is ade-quate for many kinds of phonological change.In addition to simple edits, we can model someclassical changes that appear to be too complex to becaptured by a single left-to-right edit model of thiskind.
For instance, bleeding and feeding arrange-ments occur when one phonological change intro-duces a new context, which triggers another phono-logical change, but the two cannot occur simultane-ously.
For example, vowel raising e ?
i / c mightbe needed before palatalization t ?
c / i. Insteadof capturing such an interaction directly, we canbreak up a branch into two segments joined at an in-termediate language node, conflating the concept ofhistorically intermediate languages with the conceptof intermediate stages in the application of sequen-tial rules.However, many complex processes are not well-represented by our basic model.
One problem-atic case is chained shifts such as Grimm?s law inProto-Germanic or the Great Vowel Shift in English.To model such dependent rules, we would needto use a more complex prior distributions over theedit parameters.
Another difficult case is prosodicchanges, such as unstressed vowel neutralizations,which would require a representation of supraseg-mental features.
While our basic model does notaccount for these phenomena, extensions within thegenerative framework could capture such richness.3 Learning and inferenceWe use a Monte Carlo EM algorithm to fit the pa-rameters of our model.
The algorithm iterates be-tween a stochastic E-step, which computes recon-889...wiAwiBwiC wiD... ...word type i = 1 .
.
.
|V |eiA?B?A?BeiB?C?B?C eiB?D ?B?DFigure 3: The graphical model representation of our model: ?are the parameters specifying the stochastic edits e, which gov-ern how the words w evolve.
The plate notation indicates thereplication of the nodes corresponding to the evolving words.structions based on the current edit parameters, andan M-step, which updates the edit parameters basedon the reconstructions.3.1 Monte Carlo E-step: sampling the editsThe E-step needs to produce expected counts of howmany times each edit (such as o ?
O) was used ineach context.
An exact E-step would require sum-ming over all possible edits involving all languagesin the phylogeny (all unobserved {e}, {w} variablesin Figure 3).
Unfortunately, unlike in the case ofHMMs and PCFGs, our model permits no tractabledynamic program to compute these counts exactly.Therefore, we resort to a Monte Carlo E-step,where many samples of the edit variables are col-lected, and counts are computed based on these sam-ples.
Samples are drawn using Gibbs sampling (Ge-man and Geman, 1984): for each word form of aparticular language wil, we fix all other variables inthe model and sample wil along with its correspond-ing edits.In the E-step, we fix the parameters, which ren-ders the word types conditionally independent, justas in an HMM.
Therefore, we can process each wordtype in turn without approximation.First consider the simple 4-language topology inFigure 3.
Suppose that the words in languages A,C and D are fixed, and we wish to infer the wordat language B along with the three correspondingsets of edits (remember the edits fully determine thewords).
There are an exponential number of possi-ble words/edits, but it turns out that we can exploittheMarkov structure in the edit model to consider allsuch words/edits using dynamic programming, in away broadly similar to the forward-backward algo-rithm for HMMs.Figure 4 shows the lattice for the dynamic pro-gram.
Each path connecting the two shaded end-point states represents a particular word form forlanguage B and a corresponding set of edits.
Eachnode in the lattice is a state of the dynamic pro-gram, which is a 5-tuple (iA, iC , iD, c1, c2), whereiA, iC and iD are the cursor positions (representedby dots in Figure 4) in each of the word forms ofA,C and D, respectively; c1 is the natural class ofthe phoneme in the word form for B that was lastgenerated; and c2 corresponds to the phoneme thatwill be generated next.Each state transition involves applying a ruleto A?s current phoneme (which produces 0?2phonemes in B) and applying rules to B?s new 0?2phonemes.
There are three types of rules (deletion,substitution, insertion), resulting in 30+32+34 = 91types of state transitions.
For illustration, Figure 4shows the simpler case where B only has one childC.
Given these rules, the new state is computed byadvancing the appropriate cursors and updating thenatural classes c1 and c2.
The weight of each tran-sition w(s ?
t) is a product of the language modelprobability and the rule probabilities that were cho-sen.For each state s, the dynamic program computesW (s), the sum of the weights of all paths leaving s,W (s) =?s?tw(s ?
t)W (t).To sample a path, we start at the leftmost state,choose the transition with probability proportionalto its contribution in the sum for computing W (s),and repeat until we reach the rightmost state.We applied a few approximations to speed up thesampling of words, which reduced the running timeby several orders of magnitude.
For example, wepruned rules with low probability and restricted the890An example of a dynamic programming lattice......... ... ... ... ... ... ......patr ?
ia# C V C C# p a t r ?
V #a #patr ?
jax [T1] p1ed(i ?
/C V) xx [T3] plm(j | C) p1ed(i ?
j/C V) p2ed(j ?
j/C V) xx [T11] plm(j | C) plm(i | C) p1ed(i ?
j i/C V) p2ed(j ?
j/C V) p2ed(i ?
/C V) x. .
.patri ?
a# C V C C# p a t r ?
V #a #patr ?
japatri ?
a# C V C C C# p a t r j ?
V #a #patrj ?
apatri ?
a# C V C C C V# p a t r j i ?
V #a #patrj ?
a. .
.Types of state transitions (x: ancient phoneme, y: intermediate, z: modern)xyxyzxyzxyz zxyzyzxyzyzxyzyz zxyzyzxyzyzxyzyz zxyz zyzxyz zyzxyz zyz z[T1] [T2] [T3] [T4] [T5] [T6] [T7] [T8] [T9] [T10] [T11] [T12] [T13]Figure 4: The dynamic program involved in sampling an intermediate word form given one ancient and one modern word form.One lattice node is expanded to show the dynamic program state (represented by the part not grayed out) and three of the manypossible transitions leaving the state.
Each transition is labeled with the weight of the transition, which is the product of the relevantmodel probabilities.
At the bottom, the 13 types of state transitions are shown.state space of the dynamic program by limiting thedeviation in cursor positions.3.2 M-step: updating the parametersThe M-step is standard once we have computedthe expected counts of edits in the E-step.
Foreach branch (k ?
l) ?
T in the phylogeny,we compute the maximum likelihood estimateof the edit parameters {?k?l(x ?
?
/ c1 c2)}.For example, the parameter corresponding tox = /e/, ?
= /e s/, c1 = ALVEOLAR, c2 = # isthe probability of inserting a final /s/ after an /e/which is itself preceded by an alveolar phoneme.The probability of each rule is estimated as follows:?k?l(x ?
?
/ c1 c2) =#(x ?
?
/ c1 c2) + ?
(x ?
?
/ c1 c2)?
1???
#(x ?
??
/ c1 c2) + ?
(x ?
??
/ c1 c2)?
1,where ?
is the concentration hyperparameter of theDirichlet prior.
The value ?
?
1 can be interpretedas the number of pseudocounts for a rule.4 ExperimentsIn this section we show the results of our experi-ments with our model.
The experimental conditionsare summarized in Table 1, with additional informa-Experiment Topology HeldoutLatin reconstruction (4.2) 1 la:293Italian reconstruction (4.2) 1 it:117Sound changes (4.3) 2 NonePhylogeny selection (4.4) 2, 3, 4 NoneTable 1: Conditions under which each of the experiments pre-sented in this section were performed.
The topology indicescorrespond to those displayed in Figure 1.
Note that by condi-tional independence, the topology used for Spanish reconstruc-tion reduces to a chain.
The heldout column indicates howmanywords, if any, were heldout for edit distance evaluation, andfrom which language.tion on the specifics of the experiments presented inSection 4.5.
We start with a description of the corpuswe created for these experiments.4.1 CorpusIn order to train and evaluate our system, wecompiled a corpus of Romance cognate words.The raw data was taken from three sources: thewiktionary.org website, a Bible parallel cor-pus (Resnik et al, 1999) and the Europarl corpus(Koehn, 2002).
From an XML dump of the Wik-tionary data, we extracted multilingual translations,which provide a list of word tuples in a large num-ber of languages, including a few ancient languages.891The Europarl and the biblical data were processedand aligned in the standard way, using combinedGIZA++ alignments (Och and Ney, 2003).We performed our experiments with four lan-guages from the Romance family (Latin, Italian,Spanish, and Portuguese).
For each of these lan-guages, we used a simple in-house rule-based sys-tem to convert the words into their IPA represen-tations.2 After augmenting our alignments withthe transitive closure3 of the Europarl, Bible andWiktionary data, we filtered out non-cognate wordsby thresholding the ratio of edit distance to wordlength.4 The preprocessing is constraining in that werequire that all the elements of a tuple to be cognates,which leaves out a significant portion of the data be-hind (see the row Full entries in Table 2).
However,our approach relies on this assumption, as there is noexplicit model of non-cognate words.
An interest-ing direction for future work is the joint modeling ofphonology with the determination of the cognates,but our simpler setting lets us focus on the proper-ties of the edit model.
Moreover, the restriction tofull entries has the side advantage that the Latin bot-tleneck prevents the introduction of too many neol-ogisms, which are numerous in the Europarl data, tothe final corpus.Since we used automatic tools for preparing ourcorpus rather than careful linguistic analysis, ourcognate list is much noiser in terms of the pres-ence of borrowed words and phonemeic transcrip-tion errors compared to the ones used by previousapproaches (Swadesh, 1955; Dyen et al, 1997).
Thebenefit of our mechanical preprocessing is that morecognate data can easily be made available, allowingus to effectively train richer models.
We show in therest of this section that our phonological model canindeed overcome this noise and recover meaningfulpatterns from the data.2The tool and the rules we used are available atnlp.cs.berkeley.edu/pages/historical.html.3For example, we would infer from an la-es bible align-ment confessionem-confesio?n (confession) and an es-it Eu-roparl alignment confesio?n-confessione that the Latin word con-fessionem and the Italian word confessione are related.4To be more precise we keep a tuple (w1, w2, .
.
.
, wp) iffd(wi,wj)l?(wi,wj)?
0.7 for all i, j ?
{1, 2, .
.
.
, p}, where l?
is the meanlength|wi|+|wj |2 and d is the Levenshtein distance.Name Languages Tuples Word formsRaw sources of data used to create the corpusWiktionary es,pt,la,it 5840 11724Bible la,es 2391 4782Europarl es,pt 36905 73773it,es 39506 78982Main stages of preprocessing of the corpusClosure es,pt,la,it 40944 106090Cognates es,pt,la,it 27996 69637Full entries es,pt,la,it 586 2344Table 2: Statistics of the dataset we compiled for the evaluationof our model.
We show the languages represented, the numberof tuples and the number of word forms found in each of thesource of data and pre-processing steps involved in the creationof the dataset we used to test our model.
By full entry, we meanthe number of tuples that are jointly considered cognate by ourpreprocessing system and that have a word form known for eachof the languages of interest.
These last row forms the datasetused for our experiments.Language Baseline Model ImprovementLatin 2.84 2.34 9%Spanish 3.59 3.21 11%Table 3: Results of the edit distance experiment.
The languagecolumn corresponds to the language held-out for evaluation.
Weshow the mean edit distance across the evaluation examples.4.2 Reconstruction of word formsWe ran the system using Topology 1 in Figure 1 todemonstrate the the system can propose reasonablereconstructions of Latin word forms on the basis ofmodern observations.
Half of the Latin words at theroot of the tree were held out, and the (uniform cost)Levenshtein edit distance from the predicted recon-struction to the truth was computed.
Our baseline isto pick randomly, for each heldout node in the tree,an observed neighboring word (i.e.
copy one of themodern forms).
We stopped EM after 15 iterations,and reported the result on a Viterbi derivation usingthe parameters obtained.
Our model outperformedthis baseline by a 9% relative reduction in averageedit distance.
Similarly, reconstruction of modernforms was also demonstrated, with an improvementof 11% (see Table 3).To give a qualitative feel for the operation of thesystem (good and bad), consider the example in Fig-ure 5, taken from this experiment.
The Latin dentis/dEntis/ (teeth) is nearly correctly reconstructed as/dEntes/, reconciling the appearance of the /j/ in the892/dEntis//djEntes/ /dEnti/i ?
EE?
j E s ?Figure 5: An example of a Latin reconstruction given the Span-ish and Italian word forms.Spanish and the disappearance of the final /s/ in theItalian.
Note that the /is/ vs. /es/ ending is difficultto predict in this context (indeed, it was one of theearly distinctions to be eroded in vulgar Latin).While the uniform-cost edit distance misses im-portant aspects of phonology (all phoneme substitu-tions are not equal, for instance), it is parameter-freeand still seems to correlate to a large extent with lin-guistic quality of reconstruction.
It is also superiorto held-out log-likelihood, which fails to penalize er-rors in the modeling assumptions, and to measuringthe percentage of perfect reconstructions, which ig-nores the degree of correctness of each reconstructedword.4.3 Inference of phonological changesAnother use of our model is to automatically recoverthe phonological drift processes between known orpartially known languages.
To facilitate evaluation,we continued in the well-studied Romance evolu-tionary tree.
Again, the root is Latin, but we now addan additional modern language, Portuguese, and twoadditional hidden nodes.
One of the nodes charac-terizes the least common ancestor of modern Span-ish and Portuguese; the other, the least common an-cestor of all three modern languages.
In Figure 1,Topology 2, these two nodes are labelled vl (VulgarLatin) and ib (Proto-Ibero Romance) respectively.Since we are omitting many other branches, thesenames should not be understood as referring to ac-tual historical proto-languages, but, at best, to col-lapsed points representing several centuries of evo-lution.
Nonetheless, the major reconstructed rulesstill correspond to well known phenomena and thelearned model generally places them on reasonablebranches.Figure 6 shows the top four general rules foreach of the evolutionary branches in this experiment,ranked by the number of times they were used in thederivations during the last iteration of EM.
The la,es, pt, and it forms are fully observed while thevl and ib forms are automatically reconstructed.Figure 6 also shows a specific example of the evolu-tion of the Latin VERBUM (word/verb), along withthe specific edits employed by the model.While quantitative evaluation such as measuringedit distance is helpful for comparing results, it isalso illuminating to consider the plausibility of thelearned parameters in a historical light, which wedo here briefly.
In particular, we consider rules onthe branch between la and vl, for which we havehistorical evidence.
For example, documents suchas the Appendix Probi (Baehrens, 1922) provide in-dications of orthographic confusions which resultedfrom the growing gap between Classical Latin andVulgar Latin phonology around the 3rd and 4th cen-turies AD.
The Appendix lists common misspellingsof Latin words, from which phonological changescan be inferred.On the la to vl branch, rules for word-final dele-tion of classical case markers dominate the list (rulesranks 1 and 3 for deletion of final /s/, ranks 2 and4 for deletion of final /m/).
It is indeed likely thatthese were generally eliminated in Vulgar Latin.
Forthe deletion of the /m/, the Appendix Probi containspairs such as PASSIM NON PASSI and OLIM NONOLI.
For the deletion of final /s/, this was observedin early inscriptions, e.g.
CORNELIO for CORNE-LIOS (Allen, 1989).
The frequent leveling of thedistinction between /o/ and /u/ (rules ranked 5 and 6)can be also be found in the Appendix Probi: COLU-BER NON COLOBER.
Note that in the specific ex-ample shown, the model lowers the orignal /u/ andthen re-raises it in the pt branch due to a latter pro-cess along that branch.Similarily, major canonical rules were discoveredin other branches as well, for example, /v/ to /b/fortition in Spanish, /s/ to /z/ voicing in Italian,palatalization along several branches, and so on.
Ofcourse, the recovered words and rules are not per-fect.
For example, reconstructed Ibero /tRinta/ toSpanish /tReinta/ (thirty) is generated in an odd fash-ion using rules /e/ to /i/ and /n/ to /in/.
Moreover,even when otherwise reasonable systematic soundchanges are captured, the crudeness of our fixed-granularity contexts can prevent the true context893r ?
R / many environmentse ?
/ #i ?
/ #t ?
d / UNROUNDED UNROUNDEDu ?
o / many environmentsv ?
b / initial or intervocalict ?
t e / ALVEOLAR #z ?
s / ROUNDED UNROUNDED/werbum/ (la)/verbo/ (vl)/veRbo/ (ib)/beRbo/ (es) /veRbu/ (pt)/vErbo/ (it)s ?
/ #m ?
/u ?
o / many environmentsw ?
v / # UNROUNDEDu ?
o / ALVEOLAR #e ?
E / many environmentsi ?
/ many environmentsi ?
e / ALVEOLAR #a ?
5 / ALVEOLAR #n ?
m / UNROUNDED ALVEOLARo ?
u / ALVEOLAR #e ?
1 / BILABIAL ALVEOLARm ?u ?
ow ?
vr ?
Rv ?
b o ?
ue ?
EFigure 6: The tree shows the system?s hypothesised derivation of a selected Latin word form, VERBUM (word/verb) into the modernSpanish, Italian and Portuguese pronunciations.
The Latin root and modern leaves were observed while the hidden nodes as well asall the derivations were obtained using the parameters computed by our model after 15 iterations of EM.
Nontrivial rules (i.e.
rulesthat are not identities) used at each stage are shown along the corresponding edge.
The boxes display the top four nontrivial rulescorresponding to each of these evolutionary branches, ordered by the number of time they were applied during the last E round ofsampling.
Note that since our natural classes are of fixed granularity, some rules must be redundantly discovered, which tends toflood the top of the rule lists with duplicates of the top few rules.
We summarized such redundancies in the above tables.from being captured, resulting in either rules apply-ing with low probability in overly coarse environ-ments or rules being learned redundantly in overlyfine environments.4.4 Selection of phylogeniesIn this experiment, we show that our model can beused to select between various topologies of phylo-genies.
We first presented to the algorithm the uni-versally accepted evolutionary tree corresponding tothe evolution of Latin into Spanish, Portuguese andItalian (Topology 2 in Figure 1).
We estimated thelog-likelihood L?
of the data under this topology.Next, we estimated the log-likelihood L?
under twodefective topologies (*Topology 3 and *Topology4).
We recorded the log-likelihood ratio L?
?
L?after the last iteration of EM.
Note that the two like-lihoods are comparable since the complexity of thetwo models is the same.5We obtained a ratio of L?
?
L?
= ?4458 ?
(?4766) = 307 for Topology 2 versus *Topology3, and ?4877?
(?5125) = 248 for Topology 2 ver-sus *Topology 4 (the experimental setup is describedin Table 1).
As one would hope, this log-likelihoodratio is positive in both cases, indicating that the sys-tem prefers the true topology over the wrong ones.While it may seem, at the first glance, that this re-sult is limited in scope, knowing the relative arrange-5If a word was not reachable in one of the topology, it wasignored in both models for the computation of the likelihoods.ment of all groups of four nodes is actually sufficientfor constructing a full-fledged phylogenetic tree.
In-deed, quartet-based methods, which have been verypopular in the computational biology community,are precisely based on this fact (Erdos et al, 1996).There is a rich literature on this subject and approxi-mate algorithms exist which are robust to misclassi-fication of a subset of quartets (Wu et al, 2007).4.5 More experimental detailsThis section summarizes the values of the parame-ters we used in these experiments, their interpreta-tion, and the effect of setting them to other values.The Dirichlet prior on the parameters can be in-terpreted as adding pseudocounts to the correspond-ing edits.
It is an important way of infusing par-simony into the model by setting the prior of theself-substitution parameters much higher than thatof the other parameters.
We used 6.0 as the prior onthe self-substitution parameters, and for all environ-ments, 1.1 was divided uniformly across the otheredits.
As long as the prior on self-substitution iskept within this rough order of magnitude, varyingthem has a limited effect on our results.
We also ini-tialized the parameters with values that encourageself-substitutions.
Again, the results were robust toperturbation of initialization as long as the value forself-substitution dominates the other parameters.The experiments used two natural classes forvowels (rounded and unrounded), and six natural894classes for consonants, based on the place of ar-ticulation (alveolar, bilabial, labiodental, palatal,postalveolar, and velar).
We conducted experi-ments to evaluate the effect of using different naturalclasses and found that finer ones can help if enoughdata is used for training.
We defer the meticulousstudy of the optimal granularity to future work, as itwould be a more interesting experiment under a log-linear model.
In such a model, contexts of differentgranularities can coexist, whereas such coexistenceis not recognized by the current model, giving riseto many duplicate rules.We estimated the bigram phoneme model on thewords in the root languages that were not heldout.Just as in machine translation, the language modelwas found to contribute significantly to reconstruc-tion performance.
We tried to increase the weight ofthe language model by exponentiating it to a power,as is often done in NLP applications, but we didnot find that it had any significant impact on per-formance.In the reconstruction experiments, when the datawas not reachable by the model, the word used inthe initialization was used as the prediction, andthe evolution of these words were ignored when re-estimating the parameters.
Words were initializedby picking at random, for each unobserved node, anobserved node?s corresponding word.5 ConclusionWe have presented a novel probabilistic model ofdiachronic phonology and an associated inferenceprocedure.
Our experiments indicate that our modelis able to both produce accurate reconstructions asmeasured by edit distance and identify linguisti-cally plausible rules that account for the phonologi-cal changes.
We believe that the probabilistic frame-work we have introduced for diachronic phonologyis promising, and scaling it up to richer phylogeneticmay indeed reveal something insightful about lan-guage change.6 AcknowledgementWe would like to thank Bonnie Chantarotwong forher help with the IPA converter and our reviewersfor their comments.
This work was supported bya FQRNT fellowship to the first author, a NDSEGfellowship to the second author, NSF grant numberBCS-0631518 to the third author, and a MicrosoftResearch New Faculty Fellowship to the fourth au-thor.ReferencesW.
Sidney Allen.
1989.
Vox Latina: The Pronunciationof Classical Latin.
Cambridge University Press.W.A.
Baehrens.
1922.
Sprachlicher Kommentar zurvulga?rlateinischen Appendix Probi.
Halle (Saale) M.Niemeyer.L.
Campbell.
1998.
Historical Linguistics.
The MITPress.N.
Chomsky and M. Halle.
1968.
The Sound Pattern ofEnglish.
Harper & Row.I.
Dyen, J.B. Kruskal, and P. Black.1997.
FILE IE-DATA1.
Available athttp://www.ntu.edu.au/education/langs/ielex/IE-DATA1.P.
L. Erdos, M. A.
Steel, L. A. Szekely, and T. J. Warnow.1996.
Local quartet splits of a binary tree infer allquartet splits via one dyadic inference rule.
Technicalreport, DIMACS.S.
N. Evans, D. Ringe, and T. Warnow.
2004.
Inferenceof divergence times as a statistical inverse problem.
InP.
Forster and C. Renfrew, editors, Phylogenetic Meth-ods and the Prehistory of Languages.
McDonald Insti-tute Monographs.Joseph Felsenstein.
2003.
Inferring Phylogenies.
Sin-auer Associates.S.
Geman and D. Geman.
1984.
Stochastic relaxation,Gibbs distributions, and the Bayesian restoration ofimages.
IEEE Transactions on Pattern Analysis andMachine Intelligence, 6:721?741.R.
D. Gray and Q. Atkinson.
2003.
Language-tree di-vergence times support the Anatolian theory of Indo-European origins.
Nature.John P. Huelsenbeck, Fredrik Ronquist, Rasmus Nielsen,and Jonathan P. Bollback.
2001.
Bayesian inferenceof phylogeny and its impact on evolutionary biology.Science.P.
Koehn.
2002.
Europarl: A Multilingual Corpus forEvaluation of Machine Translation.G.
Kondrak.
2002.
Algorithms for Language Recon-struction.
Ph.D. thesis, University of Toronto.895S.
Li, D. K. Pearl, and H. Doss.
2000.
Phylogenetic treeconstruction using Markov chain Monte Carlo.
Jour-nal of the American Statistical Association.Bob Mau and M.A.
Newton.
1997.
Phylogenetic in-ference for binary data on dendrograms using markovchain monte carlo.
Journal of Computational andGraphical Statistics.L.
Nakhleh, D. Ringe, and T. Warnow.
2005.
Perfectphylogenetic networks: A new methodology for re-constructing the evolutionary history of natural lan-guages.
Language, 81:382?420.F.
J. Och and H. Ney.
2003.
A Systematic Comparison ofVarious Statistical Alignment Models.
ComputationalLinguistics, 29:19?51.P.
Resnik, Mari Broman Olsen, and Mona Diab.
1999.The bible as a parallel corpus: Annotating the ?book of2000 tongues?.
Computers and the Humanities, 33(1-2):129?153.D.
Ringe, T. Warnow, and A. Taylor.
2002.
Indo-european and computational cladistics.
Transactionsof the Philological Society, 100:59?129.M.
Swadesh.
1955.
Towards greater accuracy in lex-icostatistic dating.
Journal of American Linguistics,21:121?137.A.
Venkataraman, J. Newman, and J.D.
Patrick.
1997.A complexity measure for diachronic chinese phonol-ogy.
In J. Coleman, editor, Computational Phonology.Association for Computational Linguistics.G.
Wu, J.
A.
You, and G. Lin.
2007.
Quartet-basedphylogeny reconstruction with answer set program-ming.
IEEE/ACM Transactions on computational bi-ology, 4:139?152.Ziheng Yang and Bruce Rannala.
1997.
Bayesian phy-logenetic inference using dna sequences: A markovchain monte carlo method.
Molecular Biology andEvolution 14.896
