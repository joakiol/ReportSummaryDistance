Semantic Role Labeling by Tagging Syntactic Chunks?Kadri Hacioglu1, Sameer Pradhan1, Wayne Ward1, James H. Martin1, Daniel Jurafsky21University of Colorado at Boulder, 2Stanford University{hacioglu,spradhan,whw}@cslr.colorado.edu, martin@cs.colorado.edu, jurafsky@stanford.eduAbstractIn this paper, we present a semantic role la-beler (or chunker) that groups syntactic chunks(i.e.
base phrases) into the arguments of a pred-icate.
This is accomplished by casting the se-mantic labeling as the classification of syntacticchunks (e.g.
NP-chunk, PP-chunk) into one ofseveral classes such as the beginning of an ar-gument (B-ARG), inside an argument (I-ARG)and outside an argument (O).
This amounts totagging syntactic chunks with semantic labelsusing the IOB representation.
The chunker isrealized using support vector machines as one-versus-all classifiers.
We describe the represen-tation of data and information used to accom-plish the task.
We participate in the ?closedchallenge?
of the CoNLL-2004 shared task andreport results on both development and testsets.1 IntroductionIn semantic role labeling the goal is to group sequencesof words together and classify them by using semantic la-bels.
For meaning representation the predicate-argumentstructure that exists in most languages is used.
In thisstructure a word (most frequently a verb) is specified asa predicate, and a number of word groups are consideredas arguments accompanying the word (or predicate).In this paper, we select support vector machines(SVMs) (Vapnik, 1995; Burges, 1998) to implementthe semantic role classifiers, due to their ability to han-dle an extremely large number of (overlapping) featureswith quite strong generalization properties.
Support vec-tor machines for semantic role chunking were first used?This research was partially supported by the ARDAAQUAINT program via contract OCG4423B and by the NSFvia grant IIS-9978025in (Hacioglu and Ward, 2003) as word-by-word (W-by-W) classifiers.
The system was then applied to theconstituent-by-constituent (C-by-C) classification in (Ha-cioglu et al, 2003).
In (Pradhan et al, 2003; Prad-han et al, 2004), several extensions to the basic systemhave been proposed, extensively studied and systemati-cally compared to other systems.
In this paper, we imple-ment a system that classifies syntactic chunks (i.e.
basephrases) instead of words or the constituents derived fromsyntactic trees.
This system is referred to as the phrase-by-phrase (P-by-P) semantic role classifier.
We partici-pate in the ?closed challenge?
of the CoNLL-2004 sharedtask and report results on both development and test sets.A detailed description of the task, data and related workcan be found in (Carreras and Ma`rquez, 2004).2 System Description2.1 Data RepresentationIn this paper, we change the representation of the originaldata as follows:?
Bracketed representation of roles is converted intoIOB2 representation (Ramhsaw and Marcus, 1995;Sang and Veenstra, 1995)?
Word tokens are collapsed into base phrase (BP) to-kens.Since the semantic annotation in the PropBank corpusdoes not have any embedded structure there is no loss ofinformation in the first change.
However, this results ina simpler representation with a reduced set of tagging la-bels.
In the second change, it is possible to miss someinformation in cases where the semantic chunks do notalign with the sequence of BPs.
However, in Section 3.2we show that the loss in performance due to the misalign-ment is much less than the gain in performance that canbe achieved by the change in representation.frommillion251.2$todeclined VBDCDNNINCD$TOSales%CD10$278.7$CDNNS*A2)***A4)**million CD I?NPO**B?NPB?VPB?NPI?NPB?PPB?NPI?NPI?NPB?PPB?NPI?NP*(S**********decline????????????
*A3)(A3*(A4*(A2*(V*V)(A1*A1)*S)ONPVPNPPPNPPPNPSalesdeclinedNNSVBD% NNTOtomillionfrommillionB?NPCDCDOI?NPB?PPI?NPB?PPI?NP???????
****** B?VB?A2B?A1B?A3B?A4OOOB?VPIN*S)(S*(b)(a).
.. .declineFigure 1: Illustration of change in data representation; (a) original word-by-word data representation (b) phrase-by-phrase data representation used in this paper.
Words are collapsed into base phrase types retaining only headwordswith their respective features.
Bracketed representation of semantic role labels is converted into IOB2 representation.See text for details.The new representation is illustrated in Figure 1 alongwith the original representation.
Comparing both we notethe following differences and advantages in the new rep-resentation:?
BPs are being classified instead of words.?
Only the BP headwords (rightmost words) are re-tained as word information.?
The number of tagging steps is smaller.?
A fixed context spans a larger segment of a sentence.Therefore, the P-by-P semantic role chunker classifieslarger units, ignores some of the words, uses a relativelylarger context for a given window size and performs thelabeling faster.2.2 FeaturesThe following features, which we refer to as the base fea-tures, are provided in the shared task data for each sen-tence;?
Words?
Predicate lemmas?
Part of Speech tags?
BP Positions: The position of a token in a BP usingthe IOB2 representation (e.g.
B-NP, I-NP, O etc.)?
Clause tags: The tags that mark token positions in asentence with respect to clauses.
(e.g *S)*S) marksa position that two clauses end)?
Named entities: The IOB tags of named entities.There are four categories; LOC, ORG, PERSONand MISC.Using available information we have created the fol-lowing token level features:?
Token Position: The position of the phrase with re-spect to the predicate.
It has three values as ?be-fore?, ?after?
and ?-?
for the predicate.?
Path: It defines a flat path between the token andthe predicate as a chain of base phrases.
At bothends, the chain is terminated with the POS tags ofthe predicate and the headword of the token.?
Clause bracket patterns: We use two patterns ofclauses for each token.
One is the clause bracketchain between the token and the predicate, and theother is from the token to sentence begin or end de-pending on token?s position with respect to the pred-icate.?
Clause Position: a binary feature that indicates thetoken is inside or outside of the clause which con-tains the predicate?
Headword suffixes: suffixes of headwords of length2, 3 and 4.?
Distance: we have two notions of distance; the firstis the distance of the token from the predicate as anumber of base phrases, and the second is the samedistance as the number of VP chunks.?
Length: the number of words in a token.We also use some sentence level features:?
Predicate POS tag: the part of speech category ofthe predicate?
Predicate Frequency; this is a feature which indi-cates whether the predicate is frequent or rare withrespect to the training set.
The threshold on thecounts is currently set to 3.?
Predicate BP Context : The chain of BPs centeredat the predicate within a window of size -2/+2.?
Predicate POS Context : The POS tags of thewords that immediately precede and follow the pred-icate.
The POS tag of a preposition is replaced withthe preposition itself.?
Predicate Argument Frames: We used the left andright patterns of the core arguments (A0 through A5)for each predicate .
We used the three most frequentargument frames for both sides depending on the po-sition of the token in focus with respect to the pred-icate.
(e.g.
raise has A0 and A1 AO (A0 being themost frequent) as its left argument frames, and A1,A1 A2 and A2 as the three most frequent right argu-ment frames)?
Number of predicates: This is the number of pred-icates in the sentence.For each token (base phrase) to be tagged, a set of or-dered features is created from a fixed size context thatsurrounds each token.
In addition to the above features,we also use previous semantic IOB tags that have alreadybeen assigned to the tokens contained in the context.
A5-token sliding window is used for the context.
A greedyleft-to-right tagging is performed.All of the above features are designed to implicitly cap-ture the patterns of sentence constructs with respect todifferent word/predicate usages and senses.
We acknowl-edge that they significantly overlap and extensive exper-iments are required to determine the impact of each fea-ture on the performance.2.3 ClassifierAll SVM classifiers were realized using TinySVM1 witha polynomial kernel of degree 2 and the general purposeSVM based chunker YamCha 2.
SVMs were trained forbegin (B) and inside (I) classes of all arguments and oneoutside (O) class for a total of 78 one-vs-all classifiers(some arguments do not have an I-tag).1http://cl.aist-nara.ac.jp/taku-ku/software/TinySVM2http://cl.aist-nara.ac.jp/taku-ku/software/yamchaTable 1: Comparison of W-by-W and P-by-P methods.Both systems use the base features provided (i.e.
no fea-ture engineering is done).
Results are on dev set.Method Precision Recall F?=1P-by-P 69.04% 54.68% 61.02W-by-W 68.34% 45.16% 54.39Table 2: Number of sentences and unique training exam-ples in each method.Method Sentences Training ExamplesP-by-P 19K 347KW-by-W 19K 534K3 Experimental Results3.1 Data and Evaluation MetricsThe data provided for the shared task is a part of theFebruary 2004 release of the PropBank corpus.
It con-sists of sections from the Wall Street Journal part of thePenn Treebank.
All experiments were carried out usingSections 15-18 for training Section-20 for developmentand Section-21 for testing.
The results were evaluated forprecision, recall and F?=1 numbers using the srl-eval.plscript provided by the shared task organizers.3.2 W-by-W and P-by-P ExperimentsIn these experiments we used only the base features tocompare the two approaches.
Table 1 illustrates the over-all performance on the dev set.
Although both systemswere trained using the same number of sentences, the ac-tual number of training examples in each case were quitedifferent.
Those numbers are presented in Table 2.
It isclear that P-by-P method uses much less data for the samenumber of sentences.
Despite this we particularly note aconsiderable improvement in recall.
Actually, the datareduction was not without a cost.
Some arguments havebeen missed as they do not align with the base phrasechunks due to inconsistencies in semantic annotation anddue to errors in automatic base phrase chunking.
The per-centage of this misalignment was around 2.5% (over thedev set).
We observed that nearly 45% of the mismatcheswere for the ?outside?
chunks.
Therefore, sequences ofwords with outside tags were not collapsed.3.3 Best System ResultsIn these experiments all of the features described earlierwere used with the P-by-P system.
Table 3 presents ourbest system performance on the development set.
Ad-ditional features have improved the performance from61.02 to 71.72.
The performance of the same system onthe test set is similarly illustrated in Table 4.Table 3: System results on development set.Precision Recall F?=1Overall 74.17% 69.42% 71.72A0 82.86% 78.50% 80.62A1 72.82% 73.97% 73.39A2 60.16% 56.18% 58.10A3 59.66% 47.65% 52.99A4 83.21% 74.15% 78.42A5 100.00% 75.00% 85.71AM-ADV 52.52% 41.48% 46.35AM-CAU 61.11% 41.51% 49.44AM-DIR 47.37% 15.00% 22.78AM-DIS 76.47% 76.47% 76.47AM-EXT 74.07% 40.82% 52.63AM-LOC 51.21% 46.09% 48.51AM-MNR 51.04% 36.83% 42.78AM-MOD 99.47% 95.63% 97.51AM-NEG 99.20% 94.66% 96.88AM-PNC 70.00% 28.00% 40.00AM-PRD 0.00% 0.00% 0.00AM-REC 0.00% 0.00% 0.00AM-TMP 69.33% 58.37% 63.38R-A0 91.55% 80.25% 85.53R-A1 72.46% 67.57% 69.93R-A2 100.00% 52.94% 69.23R-AM-LOC 100.00% 25.00% 40.00R-AM-TMP 0.00% 0.00% 0.00V 99.05% 99.05% 99.054 ConclusionsWe have described a semantic role chunker using SVMs.The chunking method has been based on a chunked sen-tence structure at both syntactic and semantic levels.
Wehave jointly performed semantic chunk segmentation andlabeling using a set of one-vs-all SVM classifiers on aphrase-by-phrase basis.
It has been argued that the newrepresentation has several advantages as compared to theoriginal representation.
It yields a semantic role labelerthat classifies larger units, exploits relatively larger con-text, uses less data (possibly, redundant and noisy dataare filtered out), runs faster and performs better.ReferencesXavier Carreras and Llu?
?s Ma`rquez.
2004.
Introductionto the CoNLL-2004 Shared Task: Semantic Role La-beling in the same volume of Proc.
of CoNLL?2004Shared Task.Christopher J. C. Burges.
1997.
A Tutorial on SupportVector Machines for Pattern Recognition.
Data Min-ing and Knowledge Discovery, 2(2), pages 1-47.Kadri Hacioglu and Wayne Ward.
2003.
Target wordTable 4: System results on test set.Precision Recall F?=1Overall 72.43% 66.77% 69.49A0 82.93% 79.88% 81.37A1 71.92% 71.33% 71.63A2 49.37% 49.30% 49.33A3 57.50% 46.00% 51.11A4 87.10% 54.00% 66.67A5 0.00% 0.00% 0.00AM-ADV 53.36% 38.76% 44.91AM-CAU 57.89% 22.45% 32.35AM-DIR 37.84% 28.00% 32.18AM-DIS 66.83% 62.44% 64.56AM-EXT 70.00% 50.00% 58.33AM-LOC 46.63% 36.40% 40.89AM-MNR 50.31% 31.76% 38.94AM-MOD 98.12% 92.88% 95.43AM-NEG 91.11% 96.85% 93.89AM-PNC 52.00% 15.29% 23.64AM-PRD 0.00% 0.00% 0.00AM-TMP 64.57% 50.74% 56.82R-A0 90.21% 81.13% 85.43R-A1 83.02% 62.86% 71.54R-A2 100.00% 33.33% 50.00R-A3 0.00% 0.00% 0.00R-AM-LOC 0.00% 0.00% 0.00R-AM-MNR 0.00% 0.00% 0.00R-AM-PNC 0.00% 0.00% 0.00R-AM-TMP 60.00% 21.43% 31.58V 98.46% 98.46% 98.46Detection and Semantic Role Chunking Using SupportVector Machines.
Proc.
of the HLT-NAACL-03.Kadri Hacioglu, Sameer Pradhan, Wayne Ward, JamesMartin, and Dan Jurafsky.
2003.
Shallow SemanticParsing Using Support Vector Machines.
CSLR Tech.Report, CSLR-TR-2003-1.Sameer Pradhan, Kadri Hacioglu, Wayne Ward, JamesMartin, and Dan Jurafsky.
2003.
Semantic Role Pars-ing: Adding Semantic Structure to Unstructured Text.Proc.
of Int.
Conf.
on Data Mining (ICDM03).Sameer Pradhan, Kadri Hacioglu, Wayne Ward, JamesMartin, and Dan Jurafsky.
2004.
Support VectorLearning for Semantic Argument Classification.
to ap-pear in Journal of Machine Learning.Lance E. Ramhsaw and Mitchell P. Marcus.
1995.Text Chunking Using Transformation Based Learning.Proc.
of the 3rd ACL Workshop on Very Large Cor-pora, pages 82-94.Erik F. T. J.
Sang, John Veenstra.
1999.
RepresentingText Chunks.
Proc.
of EACL?99, pages 173-179.Vladamir Vapnik 1995.
The Nature of Statistical Learn-ing Theory.
Springer Verlag, New York, USA.
