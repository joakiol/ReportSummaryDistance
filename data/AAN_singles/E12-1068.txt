Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 664?674,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsModeling Inflection and Word-Formation in SMTAlexander Fraser?
Marion Weller?
Aoife Cahill?
Fabienne Cap?
?Institut fu?r Maschinelle Sprachverarbeitung ?Educational Testing ServiceUniversita?t Stuttgart Princeton, NJ 08541D?70174 Stuttgart, Germany USA{fraser,wellermn,cap}@ims.uni-stuttgart.de acahill@ets.orgAbstractThe current state-of-the-art in statisticalmachine translation (SMT) suffers from is-sues of sparsity and inadequate modelingpower when translating into morphologi-cally rich languages.
We model both in-flection and word-formation for the taskof translating into German.
We translatefrom English words to an underspecifiedGerman representation and then use linear-chain CRFs to predict the fully specifiedGerman representation.
We show that im-proved modeling of inflection and word-formation leads to improved SMT.1 IntroductionPhrase-based statistical machine translation(SMT) suffers from problems of data sparsitywith respect to inflection and word-formationwhich are particularly strong when translating toa morphologically rich target language, such asGerman.
We address the problem of inflectionby first translating to a stem-based representation,and then using a second process to inflect thesestems.
We study several models for doingthis, including: strongly lexicalized models,unlexicalized models using linguistic features,and models combining the strengths of both ofthese approaches.
We address the problem ofword-formation for compounds in German, bytranslating from English into German word parts,and then determining whether to merge theseparts to form compounds.We make the following new contributions: (i)we introduce the first SMT system combininginflection prediction with synthesis of portman-teaus and compounds.
(ii) For inflection, we com-pare the mostly unlexicalized prediction of lin-guistic features (with a subsequent surface formgeneration step) versus the direct prediction ofsurface forms, and show that both approacheshave complementary strengths.
(iii) We com-bine the advantages of the prediction of linguis-tic features with the prediction of surface forms.We implement this in a CRF framework whichimproves on a standard phrase-based SMT base-line.
(iv) We develop separate (but related) pro-cedures for inflection prediction and dealing withword-formation (compounds and portmanteaus),in contrast with most previous work which usu-ally either approaches both problems as inflec-tional problems, or approaches both problems asword-formation problems.We evaluate on the end-to-end SMT task oftranslating from English to German of the 2009ACL workshop on SMT.
We achieve BLEU scoreincreases on both the test set and the blind test set.2 Overview of the translation process forinflection predictionThe work we describe is focused on generaliz-ing phrase-based statistical machine translation tobetter model German NPs and PPs.
We particu-larly want to ensure that we can generate novelGerman NPs, where what we mean by novel isthat the (inflected) realization is not present in theparallel German training data used to build theSMT system, and hence cannot be produced byour baseline (a standard phrase-based SMT sys-tem).
We first present our system for dealing withthe difficult problem of inflection in German, in-cluding the inflection-dependent phenomenon ofportmanteaus.
Later, after performing an exten-sive analysis of this system, we will extend it664to model compounds, a highly productive phe-nomenon in German (see Section 8).The key linguistic knowledge sources that weuse are morphological analysis and generation ofGerman based on SMOR, a morphological ana-lyzer/generator of German (Schmid et al 2004)and the BitPar parser, which is a state-of-the-artparser of German (Schmid, 2004).2.1 Issues of inflection predictionIn order to ensure coherent German NPs, wemodel linguistic features of each word in an NP.We model case, gender, and number agreementand whether or not the word is in the scope ofa determiner (such as a definite article), whichwe label in-weak-context (this linguistic featureis necessary to determine the type of inflection ofadjectives and other words: strong, weak, mixed).This is a diverse group of features.
The numberof a German noun can often be determined givenonly the English source word.
The gender of aGerman noun is innate and often difficult to deter-mine given only the English source word.
Caseis a function of the slot in the subcategorizationframe of the verb (or preposition).
There is agree-ment in all of these features in an NP.
For instancethe number of an article or adjective is determinedby the head noun, while the type of inflection of anadjective is determined by the choice of article.We can have a large number of surface forms.For instance, English blue can be translated asGerman blau, blaue, blauer, blaues, blauen.
Wepredict which form is correct given the context.Our system can generate forms not seen in thetraining data.
We follow a two-step process: instep-1 we translate to blau (the stem), in step-2 wepredict features and generate the inflected form.12.2 ProcedureWe begin building an SMT system by parsing theGerman training data with BitPar.
We then extractmorphological features from the parse.
Next, welookup the surface forms in the SMOR morpholog-ical analyzer.
We use the morphological featuresin the parse to disambiguate the set of possibleSMOR analyses.
Finally, we output the ?stems?of the German text, with the addition of markuptaken from the parse (discussed in Section 2.3).1E.g., case=nominative, gender=masculine, num-ber=singular, in-weak-context=true; inflected: blaue.We then build a standard Moses system trans-lating from English to German stems.
We obtaina sequence of stems and POS2 from this system,and then predict the correct inflection using a se-quence model.
Finally we generate surface forms.2.3 German Stem MarkupThe translation process consists of two majorsteps.
The first step is translation of Englishwords to German stems, which are enriched withsome inflectional markup.
The second step isthe full inflection of these stems (plus markup)to obtain the final sequence of inflected words.The purpose of the additional German inflectionalmarkup is to strongly improve prediction of in-flection in the second step through the addition ofmarkup to the stems in the first step.In general, all features to be predicted arestripped from the stemmed representation becausethey are subject to agreement restrictions of anoun or prepositional phrase (such as case ofnouns or all features of adjectives).
However, weneed to keep all morphological features that arenot dependent on, and thus not predictable from,the (German) context.
They will serve as knowninput for the inflection prediction model.
We nowdescribe this markup in detail.Nouns are marked with gender and number: weconsider the gender of a noun as part of its stem,whereas number is a feature which we can obtainfrom English nouns.Personal pronouns have number and gender an-notation, and are additionally marked with nom-inative and not-nominative, because English pro-nouns are marked for this (except for you).Prepositions are marked with the case their ob-ject takes: this moves some of the difficulty in pre-dicting case from the inflection prediction step tothe stem translation step.
Since the choice of casein a PP is often determined by the PP?s meaning(and there are often different meanings possiblegiven different case choices), it seems reasonableto make this decision during stem translation.Verbs are represented using their inflected surfaceform.
Having access to inflected verb forms has apositive influence on case prediction in the second2We use an additional target factor to obtain the coarsePOS for each stem, applying a 7-gram POS model.
Koehnand Hoang (2007) showed that the use of a POS factor onlyresults in negligible BLEU improvements, but we need ac-cess to the POS in our inflection prediction models.665input decoder output inflected mergedinin<APPR><Dat> inimdie<+ART><Def> demcontrast Gegensatz<+NN><Masc><Sg>Gegensatz Gegensatzto zu<APPR><Dat> zuzurthe die<+ART><Def> deranimated lebhaft<+ADJ><Pos> lebhaften lebhaftendebate Debatte<+NN><Fem><Sg> Debatte DebatteTable 1: Re-merging of prepositions and articles afterinflection to form portmanteaus, in dem means in the.step through subject-verb agreement.Articles are reduced to their stems (the stem itselfmakes clear the definite or indefinite distinction,but lemmatizing involves removing markings ofcase, gender and number features).Other words are also represented by their stems(except for words not covered by SMOR, wheresurface forms are used instead).3 PortmanteausPortmanteaus are a word-formation phenomenondependent on inflection.
As we have discussed,standard phrase-based systems have problemswith picking a definite article with the correctcase, gender and number (typically due to spar-sity in the language model, e.g., a noun whichwas never before seen in dative case will oftennot receive the correct article).
In German, port-manteaus increase this sparsity further, as theyare compounds of prepositions and articles whichmust agree with a noun.We adopt the linguistically strict definition ofthe term portmanteau: the merging of two func-tion words.3 We treat this phenomena by split-ting the component parts during training and re-merging during generation.
Specifically forGerman, this requires splitting the words whichhave German POS tag APPRART into an APPR(preposition) and an ART (article).
Merging is re-stricted, the article must be definite, singular4 andthe preposition can only take accusative or dativecase.
Some prepositions allow for merging withan article only for certain noun genders, for exam-ple the preposition inDative is only merged withthe following article if the following noun is ofmasculine or neuter gender.
The definite article3Some examples are: zum (to the) = zu (to) + dem (the)[German], du (from the) = de (from) + le (the) [French] or al(to the) = a (to) + el (the) [Spanish].4This is the reason for which the preposition + article inTable 2 remain unmerged.must be inflected before making a decision aboutwhether to merge a preposition and the article intoa portmanteau.
See Table 1 for examples.4 Models for Inflection PredictionWe present 5 procedures for inflectional predic-tion using supervised sequence models.
The firsttwo procedures use simple N-gram models overfully inflected surface forms.1.
Surface with no features is presented with anunderspecified input (a sequence of stems), andreturns the most likely inflected sequence.2.
Surface with case, number, gender is a hybridsystem giving the surface model access to linguis-tic features.
In this system prepositions have addi-tionally been labeled with the case they mark (inboth the underspecified input and the fully spec-ified output the sequence model is built on) andgender and number markup is also available.The rest of the procedures predict morpholog-ical features (which are input to a morphologicalgenerator) rather than surface words.
We have de-veloped a two-stage process for predicting fullyinflected surface forms.
The first stage takes astem and predicts morphological features for thatstem, based on the surrounding context.
The aimof the first stage is to take a stem and predictfour morphological features: case, gender, num-ber and type of inflection.
We experiment witha number of models for doing this.
The sec-ond stage takes the stems marked with morpho-logical features (predicted in the first stage) anduses a morphological generator to generate thefull surface form.
For the second stage, a modifiedversion of SMOR (Schmid et al 2004) is used,which, given a stem annotated with morphologi-cal features, generates exactly one surface form.We now introduce our first linguistic featureprediction systems, which we call joint sequencemodels (JSMs).
These are standard languagemodels, where the ?word?
tokens are not repre-sented as surface forms, but instead using POSand features.
In testing, we supply the input as asequence in underspecified form, where some ofthe features are specified in the stem markup (forinstance, POS=Noun, gender=masculine, num-ber=plural), and then use Viterbi search to find themost probable fully specified form (for instance,POS=Noun, gender=masculine, number=plural,666output decoder input prediction output prediction inflected forms glosshaben<VAFIN> haben-V haben-V haben haveZugang<+NN><Masc><Sg> NN-Sg-Masc NN-Masc.Acc.Sg.in-weak-context=false Zugang accesszu<APPR><Dat> APPR-zu-Dat APPR-zu-Dat zu todie<+ART><Def> ART-in-weak-context=true ART-Neut.Dat.Pl.in-weak-context=true den thebetreffend<+ADJ><Pos> ADJA ADJA-Neut.Dat.Pl.in-weak-context=true betreffenden respectiveLand<+NN><Neut><Pl> NN-Pl-Neut NN-Neut.Dat.Pl.in-weak-context=true La?ndern countriesTable 2: Overview: inflection prediction steps using a single joint sequence model.
All words except verbs andprepositions are replaced by their POS tags in the input.
Verbs are inflected in the input (?haben?, meaning?have?
as in ?they have?, in the example).
Prepositions are lexicalized (?zu?
in the example) and indicate whichcase value they mark (?Dat?, i.e., Dative in the example).case=nominative, in-weak-context=true).53.
Single joint sequence model on features.
Weillustrate the different stages of the inflection pre-diction when using a joint sequence model.
Thestemmed input sequence (cf.
Section 2.3) containsseveral features that will be part of the input tothe inflection prediction.
With the exception ofverbs and prepositions, the representation for fea-ture prediction is based on POS-tags.As gender and number are given by the headsof noun phrases and prepositional phrases, andthe expected type of inflection is set by articles,the model has sufficient information to computevalues for these features and there is no need toknow the actual words.
In contrast, the predictionof case is more difficult as it largely depends onthe content of the sentence (e.g.
which phrase isobject, which phrase is subject).
Assuming thatverbs and prepositions indicate subcategorizationframes, the model is provided crucial informationfor the prediction of case by keeping verbs (recallthat verbs are produced by the stem translationsystem in their inflected form) and prepositions(the prepositions also have case markup) insteadof replacing them with their tags.After having predicted a single label with val-ues for all features, an inflected word form for thestem and the features is generated.
The predictionsteps are illustrated in Table 2.4.
Using four joint sequence models (one foreach linguistic feature).
Here the four linguisticfeature values are predicted separately.
The as-sumption that the different linguistic features canbe predicted independently of one another is a rea-5Joint sequence models are a particularly simple HMM.Unlike the HMMs used for POS-tagging, an HMM as usedhere only has a single emission possibility for each state,with probability 1.
The states in the HMM are the fullyspecified representation.
The emissions of the HMM are thestems+markup (the underspecified representation).sonable linguistic assumption to make given theadditional German markup that we use.
By split-ting the inflection prediction problem into 4 com-ponent parts, we end up with 4 simpler modelswhich are less sensitive to data sparseness.Each linguistic feature is modeled indepen-dently (by a JSM) and has a different input rep-resentation based on the previously describedmarkup.
The input consists of a sequence ofcoarse POS tags, and for those stems that aremarked up with the relevant feature, this featurevalue.
Finally, we combine the predicted fea-tures together to produce the same final output asthe single joint sequence model, and then generateeach surface form using SMOR.5.
Using four CRFs (one for each linguistic fea-ture).
The sequence models already presented arelimited to the n-gram feature space, and those thatpredict linguistic features are not strongly lexi-calized.
Toutanova et al(2008) uses an MEMMwhich allows the integration of a wide variety offeature functions.
We also wanted to experimentwith additional feature functions, and so we train4 separate linear chain CRF6 models on our data(one for each linguistic feature we want to pre-dict).
We chose CRFs over MEMMs to avoid thelabel bias problem (Lafferty et al 2001).The CRF feature functions, for each Germanword wi, are in Table 3.
The common featurefunctions are used in all models, while each of the4 separate models (one for each linguistic feature)includes the context of only that linguistic feature.We use L1 regularization to eliminate irrelevantfeature functions, the regularization parameter isoptimized on held out data.6We use the Wapiti Toolkit (Lavergne et al 2010) on 4x 12-Core Opteron 6176 2.3 GHz with 256GB RAM to trainour CRF models.
Training a single CRF model on our datawas not tractable, so we use one for each linguistic feature.667Common lemmawi?5...wi+5 , tagwi?7...wi+7Case casewi?5...wi+5Gender genderwi?5...wi+5Number numberwi?5...wi+5in-weak-context in-weak-contextwi?5...wi+5Table 3: Feature functions used in CRF models (fea-ture functions are binary indicators of the pattern).5 Experimental SetupTo evaluate our end-to-end system, we performthe well-studied task of news translation, us-ing the Moses SMT package.
We use the En-glish/German data released for the 2009 ACLWorkshop on Machine Translation shared task ontranslation.7 There are 82,740 parallel sentencesfrom news-commentary09.de-en and 1,418,115parallel sentences from europarl-v4.de-en.
Themonolingual data contains 9.8 M sentences.8To build the baseline, the data was tokenizedusing the Moses tokenizer and lowercased.
Weuse GIZA++ to generate alignments, by running5 iterations of Model 1, 5 iterations of the HMMModel, and 4 iterations of Model 4.
We sym-metrize using the ?grow-diag-final-and?
heuris-tic.
Our Moses systems use default settings.
TheLM uses the monolingual data and is trained asa five-gram9 using the SRILM-Toolkit (Stolcke,2002).
We run MERT separately for each sys-tem.
The recaser used is the same for all systems.It is the standard recaser supplied with Moses,trained on all German training data.
The dev setis wmt-2009-a and the test set is wmt-2009-b, andwe report end-to-end case sensitive BLEU scoresagainst the unmodified reference SGML file.
Theblind test set used is wmt-2009-blind (all lines).In developing our inflection prediction sys-tems (and making such decisions as n-gram orderused), we worked on the so-called ?clean data?task, predicting the inflection on stemmed refer-ence sentences (rather than MT output).
We usedthe 2000 sentence dev-2006 corpus for this task.Our contrastive systems consist of two steps,the first is a translation step using a similarMoses system (except that the German side isstemmed, with the markup indicated in Sec-7http://www.statmt.org/wmt09/translation-task.html8However, we reduced the monolingual data (only) byretaining only one copy of each unique line, which resultedin 7.55 M sentences.9Add-1 smoothing for unigrams and Kneser-Neysmoothing for higher order n-grams, pruning defaults.tion 2.3), and the second is inflection predictionas described previously in the paper.
To derivethe stem+markup representation we first parsethe German training data and then produce thestemmed representation.
We then build a sys-tem for translating from English words to Ger-man stems (the stem+markup representation), onthe same data (so the German side of the paralleldata, and the German language modeling uses thestem+markup representation).
Likewise, MERTis performed using references which are in thestem+markup representation.To train the inflection prediction systems, weuse the monolingual data.
The basic surface formmodel is trained on lowercased surface forms,the hybrid surface form model with features istrained on lowercased surface forms annotatedwith markup.
The linguistic feature predictionsystems are trained on the monolingual data pro-cessed as described previously (see Table 2).Our JSMs are trained using the SRILM Toolkit.We use the SRILM disambig tool for predictinginflection, which takes a ?map?
that specifies theset of fully specified representations that each un-derspecified stem can map to.
For surface formmodels, it specifies the mapping from stems tolowercased surface forms (or surface forms withmarkup for the hybrid surface model).6 Results for Inflection PredictionWe build two different kinds of translation sys-tem, the baseline and the stem translation system(where MERT is used to train the system to pro-duce a stem+markup sequence which agrees withthe stemmed reference of the dev set).
In this sec-tion we present the end-to-end translation resultsfor the different inflection prediction models de-fined in Section 4, see Table 4.If we translate from English into a stemmedGerman representation and then apply a unigramstem-to-surface-form model to predict the surfaceform, we achieve a BLEU score of 9.97 (line 2).This is only presented for comparison.The baseline10 is 14.16, line 1.
We comparethis with a 5-gram sequence model11 that predicts10This is a better case-sensitive score than the baselineson wmt-2009-b in experiments by top-performers Edinburghand Karlsruhe at the shared task.
We use Moses with defaultsettings.11Note that we use a different set, the ?clean data?
set, todetermine the choice of n-gram order, see Section 7.
We use668surface forms without access to morphologicalfeatures, resulting in a BLEU score of 14.26.
In-troducing morphological features (case on prepo-sitions, number and gender on nouns) increasesthe BLEU score to 14.58, which is in the samerange as the single JSM system predicting all lin-guistic features at once.This result shows that the mostly unlexicalizedsingle JSM can produce competitive results withdirect surface form prediction, despite not havingaccess to a model of inflected forms, which is thedesired final output.
This strongly suggests thatthe prediction of morphological features can beused to achieve additional generalization over di-rect surface form prediction.
When comparing thesimple direct surface form prediction (line 3) withthe hybrid system enriched with number, genderand case (line 4), it becomes evident that featuremarkup can also aid surface form prediction.Since the single JSM has no access to lexicalinformation, we used a language model to scoredifferent feature predictions: for each sentence ofthe development set, the 100 best feature predic-tions were inflected and scored with a languagemodel.
We then optimized weights for the twoscores LM (language model on surface forms)and FP (feature prediction, the score assigned bythe JSM).
This method disprefers feature predic-tions with a top FP-score if the inflected sen-tence obtains a bad LM score and likewise dis-favors low-ranked feature prediction with a highLM score.
The prediction of case is the mostdifficult given no lexical information, thus scor-ing different prediction possibilities on inflectedwords is helpful.
An example is when the case ofa noun phrase leads to an inflected phrase whichnever occurs in the (inflected) language model(e.g., case=genitive vs. case=other).
Applyingthis method to the single JSM leads to a negligibleimprovement (14.53 vs. 14.56).
Using the n-bestoutput of the stem translation system did not leadto any improvement.The comparison between different feature pre-diction models is also illustrative.
Performancedecreases somewhat when using individual jointsequence models (one for each linguistic feature)compared to one single model (14.29, line 6).The framework using the individual CRFs fora 5-gram for surface forms and a 4-gram for JSMs, and thesame smoothing (Kneser-Ney, add-1 for unigrams, defaultpruning).1 baseline 14.162 unigram surface (no features) 9.973 surface (no features) 14.264 surface (with case, number, gender features) 14.585 1 JSM morphological features 14.536 4 JSMs morphological features 14.297 4 CRFs morphological features, lexical information 14.72Table 4: BLEU scores (detokenized, case sensitive) onthe development test set wmt-2009-beach linguistic feature performs best (14.72, line7).
The CRF framework combines the advantagesof surface form prediction and linguistic featureprediction by using feature functions that effec-tively cover the feature function spaces used byboth forms of prediction.
The performance of theCRF models results in a statistically significantimprovement12 (p < 0.05) over the baseline.
Wealso tried CRFs with bilingual features (projectedfrom English parses via the alignment output byMoses), but obtained only a small improvement of0.03, probably because the required informationis transferred in our stem markup (also a poor im-provement beyond monolingual features is con-sistent with previous work, see Section 8.3).
De-tails are omitted due to space.We further validated our results by translatingthe blind test set from wmt-2009, which we havenever looked at in any way.
Here we also hada statistically significant difference between thebaseline and the CRF-based prediction, the scoreswere 13.68 and 14.18.7 Analysis of Inflection-based SystemStem Markup.
The first step of translatingfrom English to German stems (with the markupwe previously discussed) is substantially easierthan translating directly to inflected German (wesee BLEU scores on stems+markup that are over2.0 BLEU higher than the BLEU scores on in-flected forms when running MERT).
The additionof case to prepositions only lowered the BLEUscore reached by MERT by about 0.2, but is veryhelpful for prediction of the case feature.Inflection Prediction Task.
Clean data task re-sults13 are given in Table 5.
The 4 CRFs outper-form the 4 JSMs by more than 2%.12We used Kevin Gimpel?s implementation of pairwisebootstrap resampling with 1000 samples.1326,061 of 55,057 tokens in our test set are ambiguous.We report % surface form matches for ambiguous tokens.669Model Accuracyunigram surface (no features) 55.98surface (no features) 86.65surface (with case, number, gender features) 91.241 JSM morphological features 92.454 JSMs morphological features 92.014 CRFs morphological features, lexical information 94.29Table 5: Comparing predicting surface forms directlywith predicting morphological features.training data 1 model 4 models7.3 M sentences 92.41 91.881.5 M sentences 92.45 92.01100000 sentences 90.20 90.641000 sentences 83.72 86.94Table 6: Accuracy for different training data sizes ofthe single and the four separate joint sequence models.As we mentioned in Section 4, there is a spar-sity issue at small training data sizes for the sin-gle joint sequence model.
This is shown in Ta-ble 6.
At the largest training data sizes, model-ing all 4 features together results in the best pre-dictions of inflection.
However using 4 separatemodels is worth this minimal decrease in perfor-mance, since it facilitates experimentation withthe CRF framework for which the training of asingle model is not currently tractable.Overall, the inflection prediction works well forgender, number and type of inflection, which arelocal features to the NP that normally agree withthe explicit markup output by the stem transla-tion system (for example, the gender of a com-mon noun, which is marked in the stem markup,is usually successfully propagated to the rest ofthe NP).
Prediction of case does not always workwell, and could maybe be improved through hier-archical labeled-syntax stem translation.Portmanteaus.
An example of where the sys-tem is improved because of the new handling ofportmanteaus can be seen in the dative phraseim internationalen Rampenlicht (in the interna-tional spotlight), which does not occur in the par-allel data.
The accusative phrase in das interna-tionale Rampenlicht does occur, however in thiscase there is no portmanteau, but a one-to-onemapping between in the and in das.
For a givencontext, only one of accusative or dative case isvalid, and a strongly disfluent sentence resultsfrom the incorrect choice.
In our system, thesetwo cases are handled in the same way (def-articleinternational Rampenlicht).
This allows us togeneralize from the accusative example with noportmanteau and take advantage of longer phrasepairs, even when translating to something that willbe inflected as dative and should be realized as aportmanteau.
The baseline does not have this ca-pability.
It should be noted that the portmanteaumerging method described in Section 3 remergesall occurrences of APPR and ART that can techni-cally form a portmanteau.
There are a few caseswhere merging, despite being grammatical, doesnot lead to a good result.
Such exceptions requiresemantic interpretation and are difficult to capturewith a fixed set of rules.8 Adding Compounds to the SystemCompounds are highly productive in German andlead to data sparsity.
We split the German com-pounds in the training data, so that our stem trans-lation system can now work with the individualwords in the compounds.
After we have trans-lated to a split/stemmed representation, we deter-mine whether to merge words together to form acompound.
Then we merge them to create stemsin the same representation as before and we per-form inflection and portmanteau merging exactlyas previously discussed.8.1 Details of Splitting ProcessWe prepare the training data by splitting com-pounds in two steps, following the technique ofFritzinger and Fraser (2010).
First, possible splitpoints are extracted using SMOR, and second, thebest split points are selected using the geometricmean of word part frequencies.compound word parts glossInflationsrate Inflation Rate inflation rateauszubrechen aus zu brechen out to break (to break out)Training data is then stemmed as described inSection 2.3.
The formerly modifying words of thecompound (in our example the words to the leftof the rightmost word) do not have a stem markupassigned, except for two cases: i) they are nounsthemselves or ii) they are particles separated froma verb.
In these cases, former modifiers are rep-resented identically to their individual occurringcounterparts, which helps generalization.8.2 Model for Compound MergingAfter translation, compound parts have to beresynthesized into compounds before inflection.Two decisions have to be taken: i) where to670merge and ii) how to merge.
Following the workof Stymne and Cancedda (2011), we implementa linear-chain CRF merging system using thefollowing features: stemmed (separated) surfaceform, part-of-speech14 and frequencies from thetraining corpus for bigrams/merging of word andword+1, word as true prefix, word+1 as true suf-fix, plus frequency comparisons of these.
TheCRF is trained on the split monolingual data.
Itonly proposes merging decisions, merging itselfuses a list extracted from the monolingual data(Popovic et al 2006).8.3 ExperimentsWe evaluated the end-to-end inflection systemwith the addition of compounds.15 As in the in-flection experiments described in Section 5, weuse a 5-gram surface LM and a 7-gram POSLM, but for this experiment, they are trained onstemmed, split data.
The POS LM helps com-pound parts and heads appear in correct order.The results are in Table 7.
The BLEU score of theCRF on test is 14.04, which is low.
However thesystem produces 19 compound types which arein the reference but not in the parallel data, andtherefore not accessible to other systems.
We alsoobserve many more compounds in general.
The100-best inflection rescoring technique previouslydiscussed reached 14.07 on the test set.
Blindtest results with CRF prediction are much better,14.08, which is a statistically significant improve-ment over the baseline (13.68) and approaches theresult we obtained without compounds (14.18).Correctly generated compounds are single wordswhich usually carry the same information as mul-tiple words in English, and are hence likely un-derweighted by BLEU.
We again see many in-teresting generalizations.
For instance, take thecase of translating English miniature cameras tothe German compound Miniaturkameras.
minia-ture camera or miniature cameras does not occurin the training data, and so there is no appropri-ate phrase pair in any system (baseline, inflec-tion, or inflection&compound-splitting).
How-ever, our system with compound splitting haslearned from split composita that English minia-14Compound modifiers get assigned a special tag based onthe POS of their former heads, e.g., Inflation in the exampleis marked as a non-head of a noun.15We found it most effective to merge word parts duringMERT (so MERT uses the same stem references as before).1 1 JSM morphological features 13.942 4 CRFs morphological features, lexical information 14.04Table 7: Results with Compounds on the test setture can be translated as German Miniatur- andgets the correct output.9 Related WorkThere has been a large amount of work on trans-lating from a morphologically rich language toEnglish, we omit a literature review here due tospace considerations.
Our work is in the oppositedirection, which primarily involves problems ofgeneration, rather than problems of analysis.The idea of translating to stems and then in-flecting is not novel.
We adapted the work ofToutanova et al(2008), which is effective but lim-ited by the conflation of two separate issues: wordformation and inflection.Given a stem such as brother, Toutanova et.
al?ssystem might generate the ?stem and inflection?corresponding to and his brother.
Viewing andand his as inflection is problematic since a map-ping from the English phrase and his brother tothe Arabic stem for brother is required.
The situ-ation is worse if there are English words (e.g., ad-jectives) separating his and brother.
This requiredmapping is a significant problem for generaliza-tion.
We view this issue as a different sort of prob-lem entirely, one of word-formation (rather thaninflection).
We apply a ?split in preprocessing andresynthesize in postprocessing?
approach to thesephenomena, combined with inflection predictionthat is similar to that of Toutanova et.
al.
Theonly work that we are aware of which deals withboth issues is the work of de Gispert and Marin?o(2008), which deals with verbal morphology andattached pronouns.
There has been other workon solving inflection.
Koehn and Hoang (2007)introduced factored SMT.
We use more complexcontext features.
Fraser (2009) tried to solve theinflection prediction problem by simply buildingan SMT system for translating from stems to in-flected forms.
Bojar and Kos (2010) improved onthis by marking prepositions with the case theymark (one of the most important markups in oursystem).
Both efforts were ineffective on largedata sets.
Williams and Koehn (2011) used uni-fication in an SMT system to model some of the671agreement phenomena that we model.
Our CRFframework allows us to use more complex con-text features.We have directly addressed the question as towhether inflection should be predicted using sur-face forms as the target of the prediction, orwhether linguistic features should be predicted,along with the use of a subsequent generationstep.
The direct prediction of surface forms islimited to those forms observed in the trainingdata, which is a significant limitation.
How-ever, it is reasonable to expect that the use offeatures (and morphological generation) couldalso be problematic as this requires the use ofmorphologically-aware syntactic parsers to anno-tate the training data with such features, and addi-tionally depends on the coverage of morpholog-ical analysis and generation.
Despite this, ourresearch clearly shows that the feature-based ap-proach is superior for English-to-German SMT.This is a striking result considering state-of-the-art performance of German parsing is poor com-pared with the best performance on English pars-ing.
As parsing performance improves, the per-formance of linguistic-feature-based approacheswill increase.Virpioja et al(2007), Badr et al(2008), Luonget al(2010), Clifton and Sarkar (2011), and oth-ers are primarily concerned with using morphemesegmentation in SMT, which is a useful approachfor dealing with issues of word-formation.
How-ever, this does not deal directly with linguistic fea-tures marked by inflection.
In German these lin-guistic features are marked very irregularly andthere is widespread syncretism, making it difficultto split off morphemes specifying these features.So it is questionable as to whether morpheme seg-mentation techniques are sufficient to solve the in-flectional problem we are addressing.Much previous work looks at the impact of us-ing source side information (i.e., feature func-tions on the aligned English), such as thoseof Avramidis and Koehn (2008), Yeniterzi andOflazer (2010) and others.
Toutanova et.
al.
?swork showed that it is most important to modeltarget side coherence and our stem markup alsoallows us to access source side information.
Us-ing additional source side information beyond themarkup did not produce a gain in performance.For compound splitting, we follow Fritzingerand Fraser (2010), using linguistic knowledge en-coded in a rule-based morphological analyser andthen selecting the best analysis based on the ge-ometric mean of word part frequencies.
Otherapproaches use less deep linguistic resources(e.g., POS-tags Stymne (2008)) or are (almost)knowledge-free (e.g., Koehn and Knight (2003)).Compound merging is less well studied.
Popovicet al(2006) used a simple, list-based merging ap-proach, merging all consecutive words includedin a merging list.
This approach resulted in toomany compounds.
We follow Stymne and Can-cedda (2011), for compound merging.
We traineda CRF using (nearly all) of the features they usedand found their approach to be effective (whencombined with inflection and portmanteau merg-ing) on one of our two test sets.10 ConclusionWe have shown that both the prediction of sur-face forms and the prediction of linguistic featuresare of interest for improving SMT.
We have ob-tained the advantages of both in our CRF frame-work, and also integrated handling of compounds,and an inflection-dependent word formation phe-nomenon, portmanteaus.
We validated our workon a well-studied large corpora translation task.AcknowledgmentsThe authors wish to thank the anonymous review-ers for their comments.
Aoife Cahill was partlysupported by Deutsche Forschungsgemeinschaftgrant SFB 732.
Alexander Fraser, Marion Wellerand Fabienne Cap were funded by DeutscheForschungsgemeinschaft grant Models of Mor-phosyntax for Statistical Machine Translation.The research leading to these results has receivedfunding from the European Community?s SeventhFramework Programme (FP7/2007-2013) undergrant agreement Nr.
248005.
This work was sup-ported in part by the IST Programme of the Euro-pean Community, under the PASCAL2 Networkof Excellence, IST-2007-216886.
This publica-tion only reflects the authors?
views.
We thankThomas Lavergne and Helmut Schmid.ReferencesEleftherios Avramidis and Philipp Koehn.
2008.
En-riching Morphologically Poor Languages for Statis-tical Machine Translation.
In Proceedings of ACL-67208: HLT, pages 763?770, Columbus, Ohio, June.Association for Computational Linguistics.Ibrahim Badr, Rabih Zbib, and James Glass.
2008.Segmentation for English-to-Arabic statistical ma-chine translation.
In Proceedings of ACL-08: HLT,Short Papers, pages 153?156, Columbus, Ohio,June.
Association for Computational Linguistics.Ondr?ej Bojar and Kamil Kos.
2010.
2010 Failures inEnglish-Czech Phrase-Based MT.
In Proceedingsof the Joint Fifth Workshop on Statistical MachineTranslation and MetricsMATR, pages 60?66, Upp-sala, Sweden, July.
Association for ComputationalLinguistics.Ann Clifton and Anoop Sarkar.
2011.
Combin-ing morpheme-based machine translation with post-processing morpheme prediction.
In Proceed-ings of the 49th Annual Meeting of the Associa-tion for Computational Linguistics: Human Lan-guage Technologies, pages 32?42, Portland, Ore-gon, USA, June.
Association for ComputationalLinguistics.Adria` de Gispert and Jose?
B. Marin?o.
2008.
On theimpact of morphology in English to Spanish statisti-cal MT.
Speech Communication, 50(11-12):1034?1046.Alexander Fraser.
2009.
Experiments in Morphosyn-tactic Processing for Translating to and from Ger-man.
In Proceedings of the Fourth Workshop onStatistical Machine Translation, pages 115?119,Athens, Greece, March.
Association for Computa-tional Linguistics.Fabienne Fritzinger and Alexander Fraser.
2010.
Howto Avoid Burning Ducks: Combining LinguisticAnalysis and Corpus Statistics for German Com-pound Processing.
In Proceedings of the FifthWorkshop on Statistical Machine Translation, pages224?234.
Association for Computational Linguis-tics.Philipp Koehn and Hieu Hoang.
2007.
FactoredTranslation Models.
In Proceedings of the 2007Joint Conference on Empirical Methods in NaturalLanguage Processing and Computational NaturalLanguage Learning (EMNLP-CoNLL), pages 868?876, Prague, Czech Republic, June.
Association forComputational Linguistics.Philipp Koehn and Kevin Knight.
2003.
Empiricalmethods for compound splitting.
In EACL ?03:Proceedings of the 10th conference of the Europeanchapter of the Association for Computational Lin-guistics, pages 187?193, Morristown, NJ, USA.
As-sociation for Computational Linguistics.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
In Proceedings of the InternationalConference on Machine Learning, pages 282?289.Morgan Kaufmann, San Francisco, CA.Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.2010.
Practical very large scale CRFs.
In Proceed-ings the 48th Annual Meeting of the Association forComputational Linguistics (ACL), pages 504?513.Association for Computational Linguistics, July.Minh-Thang Luong, Preslav Nakov, and Min-YenKan.
2010.
A Hybrid Morpheme-Word Represen-tation for Machine Translation of MorphologicallyRich Languages.
In Proceedings of the 2010 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 148?157, Cambridge, MA, Octo-ber.
Association for Computational Linguistics.Maja Popovic, Daniel Stein, and Hermann Ney.
2006.Statistical Machine Translation of German Com-pound Words.
In Proceedings of FINTAL-06, pages616?624, Turku, Finland.
Springer Verlag, LNCS.Helmut Schmid, Arne Fitschen, and Ulrich Heid.2004.
SMOR: A German Computational Morphol-ogy Covering Derivation, Composition, and Inflec-tion.
In 4th International Conference on LanguageResources and Evaluation.Helmut Schmid.
2004.
Efficient Parsing of HighlyAmbiguous Context-Free Grammars with Bit Vec-tors.
In Proceedings of Coling 2004, pages 162?168, Geneva, Switzerland, Aug 23?Aug 27.
COL-ING.Andreas Stolcke.
2002.
SRILM - An Extensible Lan-guage Modeling Toolkit.
In International Confer-ence on Spoken Language Processing.Sara Stymne and Nicola Cancedda.
2011.
Produc-tive Generation of Compound Words in StatisticalMachine Translation.
In Proceedings of the SixthWorkshop on Statistical Machine Translation, pages250?260, Edinburgh, Scotland UK, July.
Associa-tion for Computational Linguistics.Sara Stymne.
2008.
German Compounds in FactoredStatistical Machine Translation.
In Proceedings ofGOTAL-08, pages 464?475, Gothenburg, Sweden.Springer Verlag, LNCS/LNAI.Kristina Toutanova, Hisami Suzuki, and AchimRuopp.
2008.
Applying Morphology GenerationModels to Machine Translation.
In Proceedings ofACL-08: HLT, pages 514?522, Columbus, Ohio,June.
Association for Computational Linguistics.Sami Virpioja, Jaakko J. Va?yrynen, Mathias Creutz,and Markus Sadeniemi.
2007.
Morphology-awarestatistical machine translation based on morphs in-duced in an unsupervised manner.
In PROC.
OFMT SUMMIT XI, pages 491?498.Philip Williams and Philipp Koehn.
2011.
Agree-ment constraints for statistical machine translationinto German.
In Proceedings of the Sixth Workshopon Statistical Machine Translation, pages 217?226,Edinburgh, Scotland, July.
Association for Compu-tational Linguistics.Reyyan Yeniterzi and Kemal Oflazer.
2010.
Syntax-to-Morphology Mapping in Factored Phrase-Based673Statistical Machine Translation from English toTurkish.
In Proceedings of the 48th Annual Meet-ing of the Association for Computational Linguis-tics, pages 454?464, Uppsala, Sweden, July.
Asso-ciation for Computational Linguistics.674
