Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 345?351,Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational LinguisticsTri-Training for Authorship Attribution with Limited Training DataTieyun QianState Key Laboratoryof Software Eng.,Wuhan University430072, Hubei, Chinaqty@whu.edu.cnBing LiuDept.
of Computer Sci-ence, Univ.
of Illinois atChicagoIL, USA, 60607liub@cs.uic.eduLi ChenState Key Laboratory ofSoftware Eng.,Wuhan University430072, Hubei, Chinaccnuchenli@163.comZhiyong PengComputer School,Wuhan University430072, Hubei, Chinapeng@whu.edu.cnAbstractAuthorship attribution (AA) aims to identifythe authors of a set of documents.
Traditionalstudies in this area often assume that there area large set of labeled documents available fortraining.
However, in the real life, it is oftendifficult or expensive to collect a large set oflabeled data.
For example, in the online reviewdomain, most reviewers (authors) only write afew reviews, which are not enough to serve asthe training data for accurate classification.
Inthis paper, we present a novel three-view tri-training method to iteratively identify authorsof unlabeled data to augment the training set.The key idea is to first represent each docu-ment in three distinct views, and then performtri-training to exploit the large amount of un-labeled documents.
Starting from 10 trainingdocuments per author, we systematically eval-uate the effectiveness of the proposed tri-training method for AA.
Experimental resultsshow that the proposed approach outperformsthe state-of-the-art semi-supervised methodCNG+SVM and other baselines.1 IntroductionExisting approaches to authorship attribution(AA) are mainly based on supervised classifica-tion (Stamatatos, 2009, Kim et al, 2011, Serous-si et al, 2012).
Although this is an effective ap-proach, it has a major weakness, i.e., for eachauthor a large number of his/her articles areneeded as the training data.
This is possible if theauthor has written a large number of articles, butwill be difficult if he/she has not.
For example, inthe online review domain, most authors (review-ers) only write a few reviews (documents).
It wasshown that on average each reviewer only has2.72 reviews in amazon.com, and only 8% of thereviewers have at least 5 reviews (Jindal and Liu,2008).
The small number of labeled documentsmakes it extremely challenging for supervisedlearning to train an accurate classifier.In this paper, we consider AA with only a fewlabeled examples.
By exploiting the redundancyin human languages, we tackle the problem usinga new three-view tri-training algorithm (TTA).Specifically, we first represent each document inthree distinct views, and then tri-train three clas-sifiers in these views.
The predictions of twoclassifiers on unlabeled examples are used toaugment the training set for the third classifier.This process repeats until a termination conditionis met.
The enlarged labeled sets are finally usedto train classifiers to classify the test data.To our knowledge, no existing work has ad-dressed AA in a tri-training framework.
The AAproblem with limited training data was attemptedin (Stamatatos, 2007; Luyckx and Daelemans,2008).
However, neither of them used a semi-supervised approach to augment the training setwith additional documents.
Kourtis and Stama-tatos (2011) introduced a variant of the self-training method in (Nigam and Ghani, 2000).Note that the original self-training uses one clas-sifier on one view.
However, the self-trainingmethod in (Kourtis and Stamatatos, 2011) usestwo classifiers (CNG and SVM) on one view.Both the self-training and tri-training are semi-supervised learning methods.
However, the pro-posed approach is not a simple extension of theself-training method CNG+SVM of (Kourtis andStamatatos, 2011).
There are key differences.First, in their experimental setting, about 115and 129 documents per author on average areused for two experimental corpora.
This numberof labeled documents is still very large.
We con-sider a much more realistic problem, where thesize of the training set is very small.
Only 10samples per author are used in training.Second, CNG+SVM uses two learning methodson a single character n-gram view.
In contrast,besides the character n-gram view, we also makeuse of the lexical and syntactic views.
That is,345three distinct views are used for building classi-fiers.
The redundant information in human lan-guage is combined in the tri-training procedure.Third, in each round of self-training inCNG+SVM, each classifier is refined by the samenewly labeled examples.
However, in the pro-posed tri-training method (TTA), the exampleslabeled by the classifiers of every two views areadded to the third view.
By doing so, each classi-fier can borrow information from the other twoviews.
And the predictions made by two classifi-ers are more reliable than those by one classifier.The main contribution of this paper is thus theproposed three-view tri-training scheme whichhas a much better generalization ability by ex-ploiting three different views of the same docu-ment.
Experimental results on the IMDb reviewdataset show that the proposed method dramati-cally improves the CNG+SVM method.
It alsooutperforms the co-training method (Blum andMitchell, 1998) based on our proposed views.2 Related WorkExisting AA methods either focused on findingsuitable features or on developing effectivetechniques.
Example features include functionwords (Argamon et al, 2007), richness features(Gamon 2004), punctuation frequencies (Grahamet al, 2005), character (Grieve, 2007), word(Burrows, 1992) and POS n-grams (Gamon,2004; Hirst and Feiguina, 2007), rewrite rules(Halteren et al, 1996), and similarities (Qian andLiu, 2013).
On developing effective learningtechniques, supervised classification has been thedominant approach, e.g., neural networks(Graham et al, 2005; Zheng et al, 2006),decision tree (Uzuner and Katz, 2005; Zhao andZobel, 2005), logistic regression (Madigan et al,2005), SVM (Diederich et al, 2000; Gamon2004; Li et al, 2006; Kim et al, 2011), etc.The main problem in the traditional research isthe unrealistic size of the training set.
A size ofabout 10,000 words per author is regarded as areasonable training set size (Argamon et al,2007, Burrows, 2003).
When no long documentsare available, tens or hundreds of short texts areused (Halteren, 2007; Hirst and Feiguina, 2007;Schwartz et al, 2013).Apart from the existing works dealing withlimited data discussed in the introduction, ourpreliminary study in (Qian et al, 2014) used onelearning method on two views, but it is inferiorto the proposed method in this paper.3 Proposed Tri-Training Algorithm3.1 Overall FrameworkWe represent each document in three featureviews: the character view, the lexical view andthe syntactic view.
Each view consists of a set offeatures in the respective type.
A classifier canbe learned from any of these views.
We proposea three-view training algorithm to deal with theproblem of limited training data.
Logisticregression (LR) is used as the learner.
Theoverall framework is shown in Figure 1.Given the labeled, unlabeled, and test sets L,U, and T, step 1 extracts the character, lexical,and syntactic views from L, U, and T,respectively.
Steps 2-13 iteratively tri-train threeclassifiers by adding the data which are assignedthe same label by two classifiers into the trainingset of the third classifier.
The algorithm firstrandomly selects u unlabeled documents from Uto create a pool U?
of examples.
Note that we candirectly select from the large unlabeled set U.However, it is shown in (Blum and Mitchell2008) that a smaller pool can force the classifiersto select instances that are more representative ofthe underlying distribution that generates U.Hence we set the parameter u to a size of about1% of the whole unlabeled set, which allows usto observe the effects of different number ofiterations.
It then iterates over the followingsteps.
First, use character, lexical and syntacticviews on the current labeled set to train threeclassifiers C1, C2, and C3.
See Steps 4-9.
Second,Input: A small set of labeled documents L = {l1,?, lr}, a largeset of unlabeled documents U = {u1,?, us}, and a set of testdocuments T = {t1,?, tt},Parameters: the number of iterations k, the size of selected un-labeled documents uOutput: tk?s class assignment1   Extract views Lc, Ll, Ls, Uc, Ul, Us, Tc, Tl, Ts from L, U, T2  Loop for k iterations:3  Randomly select u unlabeled documents U' from U;4       Learn the first view classifier C1 from L1 (L1=Lc, Ll, or Ls);5        Use C1 to label docs in U' based on U1(U1=Uc, Ul, or Us)6        Learn the second view classifier C2 from L2 (L2?L1)7        Use C2 to label documents in U' based on U2 (U2?U1);8        Learn the third view classifier C3 from L3 (L2?L1, L2)9        Use C3 to label documents in U' based on U3 (U2?U1, U2);10      Up1 = {u | u?
U', u.label by C2 = u.label by C3};11      Up2 = {u | u?
U', u.label by C1 = u.label by C3};12      Up3 = {u | u?
U', u.label by C1 = u.label by C2};13      U = U - U', Li = Li ?
Upi (i=1..3);14 Learn three classifiers C1, C2, C3 from L1, L2, L3;15 Use Ci to label tk in Ti (i=1..3);16  Aggregate results from three viewsFigure 1: The tri-training algorithm (TTA)346allow two of these three classifiers to classify theunlabeled set U?
and choose p documents withagreed labels.
See Steps 10-12.
The selecteddocuments are then added to the third labeled setfor the label assigned (a label is an author here),and the u documents are removed from theunlabeled pool U?
(line 13).
We call this way ofaugmenting the training sets InterAdding.
Theone used in (Kourtis and Stamatatos, 2011) iscalled SelfAdding as it uses only a single viewand adds to the same training set.
Steps 14-15assign the test document to a category (author)using the classifier learned from the three viewsin the augmented labeled data, respectively.
Step16 aggregates the results from three classifiers.3.2 Character ViewThe features in the character view are thecharacter n-grams of a document.
Character n-grams are simple and easily available for anynatural language.
For a fair comparison with theprevious work in (Kourtis and Stamatatos, 2011),we extract frequencies of 3-grams at thecharacter-level.
The vocabulary size for character3-grams in our experiment is 28584.3.3 Lexical ViewThe lexical view consists of word unigrams of adocument.
We represent each article by a vectorof word frequencies.
The vocabulary size forunigrams in our experiment is 195274.3.4 Syntactic ViewThe syntactic view consists of the syntacticfeatures of a document.
We use four content-independent structures including n-grams of POStags (n = 1..3) and rewrite rules (Kim et al,2011).
The vocabulary sizes for POS 1-grams,POS 2-grams, POS 3-grams, and rewrite rules inour experiment are 63, 1917, 21950, and 19240,respectively.
These four types of syntacticstructures are merged into a single vector.
Hencethe syntactic view of a document is representedas a vector of 43140 components.3.5 Aggregating Results from Three ViewsIn testing, once we obtain the prediction valuesfrom three classifiers for a test document tk, anadditional algorithm is used to decide the finalauthor attribution.
One simple method is voting.However, this method is weaker than the threemethods below.
It is also hard to compare withthe self-training method CNG+SVM in (Kourtisand Stamatatos, 2011) as it only has two classifi-ers.
Hence we present three other strategies tofurther aggregate the results from the threeviews.
These methods require the classifier toproduce a numeric score to reflect the positive ornegative certainty.
Many classification algo-rithms give such scores, e.g., SVM and logisticregression.
The three methods are as follows:1)  ScoreSum: The learned model first classifiesall test cases in T. Then for each test case tk,this method sums up all scores of positiveclassifications from the three views.
It thenassigns tk to the author with the highest score.2)  ScoreSqSum: This method works similarly toScoreSum above except that it sums up thesquared scores of positive classifications.3)  ScoreMax: This method works similarly to theScoreSum method as well except that it findsthe maximum classification score for each testdocument.4 Experimental EvaluationWe now evaluate the proposed method.
We uselogistic regression (LR) with L2 regularization(Fan et al, 2008) and the SVMmulticlass (SVM)system (Joachims, 2007) with its default settingsas the classifiers.4.1 Experiment SetupWe conduct experiments on the IMDb dataset(Seroussi et al, 2010).
This data set contains theIMDb reviews in May 2009.
It has 62,000 re-views by 62 users (1,000 reviews per user).
Foreach author/reviewer, we further split his/herdocuments into the labeled, unlabeled, and testsets.
1% of one author?s documents, i.e., 10 doc-uments per author, are used as the labeled datafor training, 79% are used as unlabeled data, andthe rest 20% are used for testing.
We extract andcompute the character and lexical features direct-ly from the raw data, and use the Stanford PCFGparser (Klein and Manning, 2003) to generate thegrammar structures of sentences in each reviewfor extracting syntactic features.
We normalizeeach feature?s value to the [0, 1] interval by di-viding the maximum value of this feature in thetraining set.
We use the micro-averaged classifi-cation accuracy as the evaluation metric.4.2 Baseline methodsWe use six self-training baselines and three co-training baselines.
Self-training in (Kourtis andStamatatos, 2011) uses two different classifierson one view, and co-training uses one classifieron two views.
All baselines except CNG+SVM347on the character view are our extensions.Self-training using CNG+SVM on character,lexical and syntactic views respectively: Thisgives three baselines.
It self-trains two classifi-ers from the character 3-gram, lexical, and syn-tactic views using CNG and SVM classifiers(Kourtis and Stamatatos, 2011).
CNG is a pro-file-based method which represents the authoras the N most frequent character n-grams of allhis/her training texts.
The original method ap-plied only CNG and SVM on the character n-gram view.
Since our results show that its per-formance is extremely poor, we are curiouswhat the reason is.
Can this be due to the clas-sifier or to the view?
In order to differentiatethe effects of views and classifiers, we presenttwo additional types of baselines.
The first typeis to extend CNG+SVM method to lexical andsyntactic views as well.
The second type is toextend CNG+SVM method by replacing CNGwith LR to show a fair comparison with ourframework.Self-training using LR+SVM on character, lexi-cal, and syntactic views: This is the secondtype extension.
It also gives us three baselines.It again uses the character, lexical and syntac-tic view and SVM as one of the two classifiers.The other classifier uses LR rather than CNG.Co-training using LR on Char+Lex, Char+Syn,and Lex+Syn views: This also gives us threebaselines.
Each baseline co-trains two classifi-ers from every two views of the character 3-gram, lexical, and syntactic views.4.3 Results and analysis(1) Effects of learning algorithmsWe first evaluate the effects of learning algo-rithms on tri-training.
We use SVM and LR asthe learners as they are among the best methods.Figure 2.
Effects of SVM and LR on tri-trainingThe effects of SVM and LR on tri-training areshown in Fig.
2.
For the aggregation results, wedraw the curves for ScoreSum.
The results forother two stratigies are similar.
It is clear that LRoutperforms SVM by a large margin for tri-training when the number of iterations (k) issmall.
One possible reason is that LR is moretolerant to over-fitting caused by the smallnumber of training samples.
Hence, we use LRfor tri-training in all experiments.
(2) Effects of aggregation strategiesWe show the effects of the three proposedaggregation strategies.
Table 1 indicates thatScoreSum (SS) is the best.Table 1.
Effects of three aggregation strategies:ScoreMax(SM), ScoreSum(SS), and ScoreSq-Sum(SQ)We also observe that both ScoreSum andScoreSqSum (SQ) perform better than ScoreMax(SM) and all single view cases.
This suggeststhat the decision made from a number of scoresis much more reliable than that made from onlyone score.
ScoreSum is our default strategy.
(3) Effects of data augmenting strategiesWe now see the effects of data adding methodsto augment the labeled set in Fig.
3.Figure 3.
Effects of data augmenting methods ontri-trainingWe use two strategies.
One is our InterAddingapproach and the other is the SelfAddingapproach in (Kourtis and Stamatatos, 2011), asintroduced in Section 3.1.
We can see that byadding newly classified samples by twoclassifiers to the third view, tri-training getsbetter and better results rapidly.
For example, theaccuracy for k = 10 iterations grows from 61.24for SelfAdding to 78.82 for InterAdding, anabsolute increase of 17.58%.
This implies that byintegrating more information from other views,learning can improve greatly.
(4) Comparison with self-training baselinesWe show the results of CNG+SVM in Table 2.
Itis clear that CNG is almost unable to correctlykSingle  View Results Aggregated ResultsLex Char Syn SM SS SQ0 45.75 32.88 33.96 41.11 46.85 44.6110 74.63 66.05 56.99 73.41 78.82 76.4120 82.30 74.92 65.05 81.63 86.19 84.0530 86.86 79.12 68.85 85.29 89.69 87.7440 89.16 81.81 70.85 87.83 91.52 89.9950 90.56 83.14 72.06 89.11 92.58 91.1760 91.69 84.13 73.23 90.05 93.15 91.82348classify any test case.
Its accuracy is only 1.26%at the start.
This directly leads to the failure ofthe self-training.
The reason is that the otherclassifier SVM can augment nearly 0 documentsfrom the unlabeled set.
We also tuned the param-eter N for CNG, but it makes little difference.kSelf-Training on Char  Aggregated ResultsCNG SVM SM SS SQ0 1.26 33.22 32.35 32.47 27.0010 1.26 32.35 32.35 32.47 27.0020 1.26 32.35 32.35 32.47 27.0030 1.26 32.35 32.35 32.47 27.0040 1.26 33.60 33.60 33.69 29.0750 1.26 33.60 33.60 33.69 29.0760 1.27 33.54 33.60 33.69 29.07Table 2.
Results for the CNG+SVM baselineTo distinguish the effects of views from classi-fiers, we conduct two more types of experiments.First, we apply CNG+SVM to the lexical andsyntactic views.
The results are even worse.
Itsaccuracy drops to 0.58% and 1.21%, respectively.Next, we replace CNG with LR and applyLR+SVM to all three views.
We only show theirbest results in Table 3, either on a single view oraggregation.
The details are omitted due to spacelimitations.
We can see significant improvementsover their corresponding results of CNG+SVM.This demonstrates that the learning methods arecritical to self-training as well.k TriTrainSelfTrain:CNG+SVM SelfTrain:LR+SVMChar lex Syn Char Lex Syn0 46.85 33.22 45.44 34.50 33.22 45.75 34.4810 78.82 32.47 45.44 34.50 62.56 73.78 51.9420 86.19 32.47 45.44 34.09 71.21 81.44 59.8830 89.69 32.47 45.44 34.09 75.21 84.68 63.7040 91.52 33.69 45.44 34.09 77.46 88.25 65.7450 92.58 33.69 45.44 34.09 78.64 88.25 67.4560 93.15 33.69 45.44 34.09 79.54 89.31 68.37Table 3.
Self-training variationsFrom Table 3, we can also see that our tri-training approach outperforms all self-trainingbaselines by a large margin.
For example, theaccuracy for LR+SVM on the lexical view is89.31%.Although this is the best for self-training,it is worse than 93.15% of tri-training.The reason that self-training does not workwell in general is the following: When the train-ing set is small, the available data may not reflectthe true distribution of the whole data.
Then clas-sifiers will be biased and their classifications willbe biased too.
In testing, the biased classifierswill not have good accuracy.
However, in tri-training, and co-training, each individual viewmay be biased but the views are independent.Then each view is more likely to produce ran-dom samples for the other views and thus reducethe bias of each view as the iterations progress.
(5) Comparison with co-training baselinesWe now compare tri-training with co-training(Blum and Mitchell, 1998) in Table 4.
Again, tri-training beats co-training consistently.
The bestperformance of co-training is 92.81% achievedon the character and lexical views after 60 itera-tions.
However, the accuracy is worse than thatof tri-training.
The key reason is that tri-trainingconsiders three views, while co-training uses on-ly two.
Also, the predictions by two classifiersare more reliable than those by one classifier.k TriTrainCo-TrainChar+Lex Char+Syn Lex+Syn0 46.85 45.75 42.02 45.7510 78.82 78.84 75.89 78.8520 86.19 86.02 82.59 85.6330 89.69 89.32 85.77 88.9840 91.52 91.14 87.52 91.1650 92.58 92.19 88.46 92.0260 93.15 92.81 89.21 92.50Table 4.
Co-training vs. tri-trainingIn (Qian, et al, 2014), we systematically inves-tigated the effects of learning methods and viewsusing a special co-training approach with twoviews.
Learning was applied on two views butthe data augmentation method was like that inself-training.
The best result there was 91.23%,worse than 92.81% of co-training here in Table 4,which is worse than 93.15% of Tri-Training.Overall, Tri-training performs the best and co-training is better than self-training and co-self-training.
This indicates that learning on differentviews can better exploit the redundancy in textsto achieve superior classification results.5 ConclusionIn this paper, we investigated the problem of au-thorship attribution with very few labeled exam-ples.
A novel three-view tri-training method wasproposed to utilize natural views of human lan-guages, i.e., the character, lexical and syntacticviews, for classification.
We evaluated the pro-posed method and compared it with state-of-the-art baselines.
Results showed that the proposedmethod outperformed all baseline methods.Our future work will extend the work by in-cluding more views such as the stylistic and vo-cabulary richness views.
Additional experimentswill also be conducted to determine the generalbehavior of the tri-training approach.AcknowledgementsThis work was supported in part by the NSFCprojects (61272275, 61232002, 61379044), andthe 111 project (B07037).349ReferencesS.
Argamon, C. Whitelaw,  P. Chase, S. R. Hota,  N.Garg, and S. Levitan.
2007.
Stylistic textclassification using functional lexical features.JASIST 58, 802?822A.
Blum and T. Mitchell.
1998.
Combining labeledand unlabeled data with co-training.
In: COLT.
pp.92?100J.
Burrows.
1992.
Not unless you ask nicely: Theinterpretative nexus between analysis andinformation.
Literary and Linguistic Computing7:91-109.J.
Burrows.
2007.
All the way through: Testing forauthorship in different frequency data.
LLC 22,27?47R-E.
Fan, K-W. Chang,   C-J.
Hsieh,  X-R. Wang, andC-J.
Lin.
2008.
Liblinear: A library for large linearclassification.
JMLR 9, 1871?1874J.
Diederich, J. Kindermann, E. Leopold, G. Paass, G.F. Informationstechnik, and D-S. Augustin.
2000.Authorship attribution with support vectormachines.
Applied Intelligence 19:109-123.M.
Gamon.
2004.
Linguistic correlates of style:authorship classification with deep linguisticanalysis features.
In COLING.N.
Graham, G. Hirst, and B. Marthi.
2005.Segmenting documents by stylistic character.Natural Language Engineering, 11:397-415.J.
Grieve.
2007.
Quantitative authorship attribution:An evaluation of techniques.
LLC 22:251-270.H.
van Halteren, F. Tweedie, and H. Baayen.
1996.Outside the cave of shadows: using syntacticannotation to enhance authorship attribution.Literary and Linguistic Computing 11:121-132.H.
van Halteren.
2007.
Author verification bylinguistic profiling: An exploration of theparameter space.
TSLP 4, 1?17G.
Hirst, and O. Feiguina.
2007.
Bigrams of syntacticlabels for authorship discrimination of short texts.LLC 22, 405?417N.
Jindal and B. Liu.
2008.
Opinion spam andanalysis.
In: WSDM.
pp.
29?230T.
Joachims.
2007. www.cs.cornell.edu/people/tj/svmlight/old/svmmulticlassv2.12.htmlS.
Kim,  H. Kim,  T. Weninger,  J. Han, and H. D.Kim.
2011.
Authorship classification: adiscriminative syntactic tree mining approach.
In:SIGIR.
pp.
455?464D.
Klein and C. D. Manning.
2003 Accurateunlexicalized parsing.
In: ACL.
pp.
423?430I.
Kourtis and E. Stamatatos, 2011.
Authoridentification using semi-supervised learning.
In:Notebook for PAN at CLEF 2011J.
Li, R. Zheng, and H. Chen.
2006.
From fingerprintto writeprint.
Communications of the ACM 49:76-82.K.
Luyckx and W. Daelemans, 2008.
Authorshipattribution and verification with many authors andlimited data.
In: COLING.
pp.
513?520D.
Madigan, A. Genkin, D. Lewis, A. Argamon, D.Fradkin, and L. Ye, 2005.
Author Identification onthe Large Scale.
In CSNA.K.
Nigam and R. Ghani.
2000.
Analyzing theeffectiveness and applicability  of co-training.
InProc.
of CIKM, pp.86?93T.
Qian, B. Liu.
2013 Identifying Multiple Userids ofthe Same Author.
EMNLP, pp.
1124-1135T.
Qian, B. Liu, M. Zhong, G. He.
2014.
Co-Trainingon Authorship Attribution with Very Few LabeledExamples: Methods.
vs. Views.
In SIGIR, toappear.R.
Schwartz, O. Tsur, A. Rappoport, M. Koppel.
2013.Authorship Attribution of Micro-Messages.EMNLP.
pp.
1880-1891Y.
Seroussi, F. Bohnert and Zukerman,.2012.Authorship attribution with author-aware topicmodels.
In: ACL.
pp.
264?269Y.
Seroussi,  I. Zukerman, and F. Bohnert.
2010.Collaborative inference of sentiments from texts.In: UMAP.
pp.
195?206E.
Stamatatos.
2007.
Author identification usingimbalanced and limited training texts.
In: TIR.
pp.237?241E.
Stamatatos.
2009.
A survey of modern authorshipattribution methods.
JASIST 60:538?556?.
Uzuner and B. Katz.
2005.
A comparative study oflanguage models for book and author recognition.Proc.
of the 2nd IJCNLP, 969-980.350Y.
Zhao and J. Zobel.
2005.
Effective and scalableauthorship attribution using function words.
InProc.
of Information Retrival Technology, 174-189.R.
Zheng, J. Li, H. Chen, and Z. Huang.
2006.
Aframework for authorship identification of onlinemessages: Writing style features and classificationtechniques.
JASIST 57:378-393.351
