Generalizing Dimensionality in Combinatory Categorial GrammarGeert-Jan M. KruijffComputational LinguisticsSaarland UniversitySaarbru?cken, Germanygj@coli.uni-sb.deJason BaldridgeICCS, Division of InformaticsUniversity of EdinburghEdinburgh, Scotlandjbaldrid@inf.ed.ac.ukAbstractWe extend Combinatory Categorial Grammar(CCG) with a generalized notion of multi-dimensional sign, inspired by the types of rep-resentations found in constraint-based frame-works like HPSG or LFG.
The generalizedsign allows multiple levels to share information,but only in a resource-bounded way througha very restricted indexation mechanism.
Thisimproves representational perspicuity withoutincreasing parsing complexity, in contrast tofull-blown unification used in HPSG and LFG.Well-formedness of a linguistic expressions re-mains entirely determined by the CCG deriva-tion.
We show how the multidimensionalityand perspicuity of the generalized signs lead toa simplification of previous CCG accounts ofhow word order and prosody can realize infor-mation structure.1 IntroductionThe information conveyed by linguistic utterancesis diverse, detailed, and complex.
To properly ana-lyze what is communicated by an utterance, this in-formation must be encoded and interpreted at manylevels.
The literature contains various proposals fordealing with many of these levels in the descriptionof natural language grammar.Since information flows between different levelsof analysis, it is common for linguistic formalismsto bundle them together and provide some meansfor communication between them.
Categorial gram-mars, for example, normally employ a Saussuriansign that relates a surface string with its syntacticcategory and the meaning it expresses.
Syntacticanalysis is entirely driven by the categories, andwhen information from other levels is used to affectthe derivational possibilities, it is typically loaded asextra information on the categories.Head-driven Phrase Structure Grammar (HPSG)(Pollard and Sag, 1993) and Lexical FunctionalGrammar (LFG) (Kaplan and Bresnan, 1982) alsouse complex signs.
However, these signs are mono-lithic structures which permit information to befreely shared across all dimensions: any given di-mension can place restrictions on another.
For ex-ample, variables resolved during the construction ofthe logical form can block a syntactic analysis.
Thisprovides a clean, unified formal system for dealingwith the different levels, but it also can adversely af-fect the complexity of parsing grammars written inthese frameworks (Maxwell and Kaplan, 1993).We thus find two competing perspectives on com-munication between levels in a sign.
In this paper,we propose a generalization of linguistic signs forCombinatory Categorial Grammar (CCG) (Steed-man, 2000b).
This generalization enables differentlevels of linguistic information to be represented butlimits their interaction in a resource-bounded man-ner, following White (2004).
This provides a cleanseparation of the levels and allows them to be de-signed and utilized in a more modular fashion.
Mostimportantly, it allows us to retain the parsing com-plexity of CCG while gaining the representationaladvantages of the HPSG and LFG paradigms.To illustrate the approach, we use it to model var-ious aspects of the realization of information struc-ture, an inherent aspect of the (linguistic) meaningof an utterance.
Speakers use information struc-ture to present some parts of that meaning as de-pending on the preceding discourse context and oth-ers as affecting the context by adding new content.Languages may realize information structure us-ing different, often interacting means, such as wordorder, prosody, (marked) syntactic constructions,or morphological marking (Vallduv??
and Engdahl,1996; Kruijff, 2002).
The literature presents vari-ous proposals for how information structure can becaptured in categorial grammar (Steedman, 2000a;Hoffman, 1995; Kruijff, 2001).
Here, we model theessential aspects of these accounts in a more per-spicuous manner by using our generalized signs.The main outcomes of the proposal are three-fold: (1) CCG gains a more flexible and generalkind of sign; (2) these signs contain multiple levelsthat interact in a modular fashion and are built viaCCG derivations without increasing parsing com-plexity; and (3) we use these signs to simplify pre-vious CCG?s accounts of the effects of word orderand prosody on information structure.2 Combinatory Categorial GrammarIn this section, we give an overview of syntacticcombination and semantic construction in CCG.
Weuse CCG?s multi-modal extension (Baldridge andKruijff, 2003), which enriches the inventory of slashtypes.
This formalization renders constraints onrules unnecessary and supports a universal set ofrules for all grammars.2.1 Categories and combinationNearly all syntactic behavior in CCG is encoded incategories.
They may be atoms, like np, or func-tions which specify the direction in which they seektheir arguments, like (s\np)/np.
The latter is thecategory for English transitive verbs; it first seeksits object to its right and then its subject to its left.Categories combine through a small set of univer-sal combinatory rules.
The simplest are applicationrules which allow a function category to consumeits argument either on its right (>) or on its left (<):(>) X/?Y Y ?
X(<) Y X\?Y ?
XFour further rules allow functions to composewith other functions:(>B) X/Y Y/Z ?
X/Z(<B) Y\Z X\Y ?
X\Z(>B?)
X/?Y Y\?Z ?
X\?Z(<B?)
Y/?Z X\?Y ?
X/?ZThe modalities ?,  and ?
on the slashes enforcedifferent kinds of combinatorial potential on cate-gories.
For a category to serve as input to a rule,it must contain a slash which is compatible withthat specified by the rule.
The modalities workas follows.
?
is the most restricted modality, al-lowing combination only by the application rules(> and <).
 allows combination with the appli-cation rules and the order-preserving compositionrules (>B and <B).
?
allows limited permutationvia the crossed composition rules (>B?
and <B?
)as well as the application rules.
Additionally, a per-missive modality ?
allows combination by all rulesin the system.
However, we suppress the ?
modal-ity on slashes to avoid clutter.
An undecorated slashmay thus combine by all rules.There are two further rules of type-raising thatturn an argument category into a function over func-tions that seek that argument:(>T) X ?
Y/i(Y\iX)(<T) X ?
Y\i(Y/iX)The variable modality i on the output categoriesconstrains both slashes to have the same modality.These rules support the following incrementalderivation for Marcel proved completeness:(1) Marcel proved completenessnp (s\np)/np np>Ts/(s\np)>Bs/np>sThis derivation does not display the effect of us-ing modalities in CCG; see Baldridge (2002) andBaldridge and Kruijff (2003) for detailed linguisticjustification for this modalized formulation of CCG.2.2 Hybrid Logic Dependency SemanticsMany different kinds of semantic representationsand ways of building them with CCG exist.
Weuse Hybrid Logic Dependency Semantics (HLDS)(Kruijff, 2001), a framework that utilizes hybridlogic (Blackburn, 2000) to realize a dependency-based perspective on meaning.Hybrid logic provides a language for represent-ing relational structures that overcomes standardmodal logic?s inability to directly reference statesin a model.
This is achieved via nominals, a kind ofbasic formula which explicitly names states.
Likepropositions, nominals are first-class citizens of theobject language, so formulas can be formed us-ing propositions, nominals, standard boolean oper-ators, and the satisfaction operator ?@?.
A formula@i(p?
?F?
(j ?
q)) indicates that the formulas p and?F?
(j ?
q) hold at the state named by i and that thestate j is reachable via the modal relation F.In HLDS, hybrid logic is used as a languagefor describing semantic interpretations as follows.Each semantic head is associated with a nominalthat identifies its discourse referent and heads areconnected to their dependents via dependency rela-tions, which are modeled as modal relations.
As anexample, the sentence Marcel proved completenessreceives the representation in (2).
(2) @e(prove ?
?TENSE?past??ACT?(m?Marcel)??PAT?(c?comp.
))In this example, e is a nominal that labels the predi-cations and relations for the head prove, and m andc label those for Marcel and completeness, respec-tively.
The relations ACT and PAT represent the de-pendency roles Actor and Patient, respectively.By using the @ operator, hierarchical terms suchas (2) can be flattened to an equivalent conjunctionof fixed-size elementary predications (EPs):(3) @eprove ?
@e?TENSE?past ?
@e?ACT?m?
@e?PAT?c ?
@mMarcel ?
@ccomp.2.3 Semantic ConstructionBaldridge and Kruijff (2002) show how HLDSrepresentations can be built via CCG derivations.White (2004) improves HLDS construction by op-erating on flattened representations such as (3) andusing a simple semantic index feature in the syntax.We adopt this latter approach, described below.EPs are paired with syntactic categories in thelexicon as shown in (4)?
(6) below.
Each atomic cat-egory has an index feature, shown as a subscript,which makes a nominal available for capturing syn-tactically induced dependencies.
(4) prove ` (se\npx)/npy :@eprove ?
@e?TENSE?past?
@e?ACT?x ?
@e?PAT?y(5) Marcel ` npm : @mMarcel(6) completeness ` npc : @ccompletenessApplications of the combinatory rules co-indexthe appropriate nominals via unification on the cat-egories.
EPs are then conjoined to form the result-ing interpretation.
For example, in derivation (1),(5) type-raises and composes with (4) to yield (7).The index x is syntactically unified with m, and thisresolution is reflected in the new conjoined logicalform.
(7) can then apply to (6) to yield (8), whichhas the same conjunction of predications as (3).
(7) Marcel proved ` se/npy :@eprove ?
@e?TENSE?past?
@e?ACT?m ?
@e?PAT?y ?
@mMarcel(8) Marcel proved completeness ` se :@eprove ?
@e?TENSE?past ?
@e?ACT?m?@e?PAT?c?
@mMarcel ?
@ccompletenessSince the EPs are always conjoined by the com-binatory rules, semantic construction is guaranteedto be monotonic.
No semantic information can bedropped during the course of a derivation.
This pro-vides a clean way of establishing semantic depen-dencies as informed by the syntactic derivation.
Inthe next section, we extend this paradigm for usewith any number of representational levels.3 Generalized dimensionalityTo support a more modular and perspicuous encod-ing of multiple levels of analysis, we generalize thenotion of sign commonly used in CCG.
The ap-proach is inspired on the one hand by earlier workby Steedman (2000a) and Hoffman (1995), and onthe other by the signs found in constraint-based ap-proaches to grammar.
The principle idea is to ex-tend White?s (2004) approach to semantic construc-tion (see ?2.3).
There, categories and the mean-ing they help express are connected through co-indexation.
Here, we allow for information in any(finite) number of levels to be related in this way.A sign is an n-tuple of terms that represent in-formation at n distinct dimensions.
Each dimensionrepresents a level of linguistic information such asprosody, meaning, or syntactic category.
As a repre-sentation, we assume that we have for each dimen-sion a language that defines well-formed representa-tions, and a set of operations which can create newrepresentations from a set of given representations.1For example, we have by definition a dimensionfor syntactic categories.
The language for this di-mension is defined by the rules for category con-struction: given a set of atomic categories A, C is acategory iff (i) C ?
A or (ii) C is of the form A\mBor A/mB with A,B categories and m ?
{?,  ?, ?
}.The set of combinatory rules defines the possibleoperations on categories.This syntactic category dimension drives thegrammatical analysis, thus guiding the compositionof signs.
When two categories are combined viaa rule, the appropriate indices are unified.
It isthrough this unification of indices that informationcan be passed between signs.
At a given dimen-sion, the co-indexed information coming from thetwo signs we combine must be unifiable.With these signs, dimensions interact in a morelimited way than in HPSG or LFG.
Constraints (re-solved through unification) may only be appliedif they are invoked through co-indexation on cat-egories.
This provides a bound on the number ofindices and the number of unifications to be made.As such, full recursion and complex unification as inattribute-value matrices with re-entrancy is avoided.The approach incorporates various ideas fromconstraint-based approaches, but remains based ona derivational perspective on grammatical analysisand derivational control, unlike e.g Categorial Uni-fication Grammar.
Furthermore, the ability for di-mensions to interact through shared indices bringsseveral advantages: (1) ?parallel derivations?
(Hoff-man, 1995) are unnecessary; (2) non-isomorphic,functional structures across different dimensionscan be employed; and (3) there is no longer a needto load all the necessary information into syntacticcategories (as with Kruijff (2001)).1In the context of this paper we assume operations are mul-tiplicative.
Also, note that dimensions may differ in what lan-guages and operations they use.4 ExamplesIn this section, we illustrate our approach on severalexamples involving information structure.
We usesigns that include the following dimensions.Phonemic representation: word sequences, composi-tion of sequences is through concatenationProsody: sequences of tunes from the inventory of(Pierrehumbert and Hirschberg, 1990), composi-tion through concatenationSyntactic category: well-formed categories, combina-tory rules (see ?2)Information structure: hybrid logic formulas of theform @d [in]r, with r a discourse referent that hasinformativity in (theme ?, or rheme ?)
relative tothe current point in the discourse d (Kruijff, 2003).Predicate-argument structure: hybrid logic formulasof the form as discussed in ?2.3.Example (9) illustrates a sign with these dimen-sions.
The word-form Marcel bears an H* accent,and acts as a type-raised category that seeks a verbmissing its subject.
The H* accent indicates that thediscourse referent m introduces new information atthe current point in the discourse d: i.e.
the meaning@mmarcel should end up as part of the rheme (?)
ofthe utterance, @d [?]m.
(9) MarcelH*sh/(sh\npm)@d [?
]m@mmarcelIf a sign does not specify any information at aparticular dimension, this is indicated by > (or anempty line if no confusion can arise).4.1 TopicalizationWe start with a simple example of topicalization inEnglish.
In topicalized constructions, a thematic ob-ject is fronted before the subject.
Given the questionDid Marcel prove soundness and completeness?,(10) is a possible response using topicalization:(10) Completeness, Marcel proved, and sound-ness, he conjectured.We can capture the syntactic and informationstructure effects of such sentences by assigning thefollowing kind of sign to (topicalized) noun phrases:(11) completeness>si/(si/npc)@d [?
]c@ccompletenessThis category enables the derivation in Figure 1.The type-raised subject composes with the verb, andthe result is consumed by the topicalizing category.The information structure specification stated in thesign in (11) is passed through to the final sign.The topicalization of the object in (10) only indi-cates the informativity of the discourse referent re-alized by the object.
It does not yield any indica-tions about the informativity of other constituents;hence the informativity for the predicate and the Ac-tor is left unspecified.
In English, the informativityof these discourse referents can be indicated directlywith the use of prosody, to which we now turn.4.2 Prosody & information structureSteedman (2000a) presents a detailed, CCG-basedaccount of how prosody is used in English as ameans to realize information structure.
In themodel, pitch accents and boundary tones have an ef-fect on both the syntactic category of the expressionthey mark, and the meaning of that expression.Steedman distinguishes pitch accents as markersof either the theme (?)
or of the rheme (?
): L+H*and L*+H are ?-markers; H*, L*, H*+L and H+L*are ?-markers.
Since pitch accents mark individualwords, not (necessarily) larger phrases, Steedmanuses the ?/?-marking to spread informativity overthe domain and the range of function categories.Identical markings on different parts of a functioncategory not only act as features, but also as occur-rences of a singular variable.
The value of the mark-ing on the domain can thus get passed down (?pro-jected?)
to markings on categories in the range.Constituents bearing no tune have an ?-marking,which can be unified with either ?, ?
or ?.
Phraseswith such markings are ?incomplete?
until theycombine with a boundary tone.
Boundary toneshave the effect of mapping phrasal tones intointonational phrase boundaries.
To make theseboundaries explicit and enforce such ?complete?prosodic phrases to only combine with other com-plete prosodic phrases, Steedman introduces twofurther types of marking ?
?
and ?
?
on categories.The ?
markings only unify with other ?
or ?
mark-ings on categories, not with ?, ?
or ?.
These mark-ings are only introduced to provide derivational con-trol and are not reflected in the underlying meaning(which only reflects ?, ?
or ?
).Figure 2 recasts the above as an abstract speci-fication of which different types of prosodic con-stituents can, or cannot, be combined.2 Steedman?s2There is one exception we should note: two intermediatephrases can combine if a second one has a downstepped accent.We deal with this exception at the end of the section.completeness Marcel provedsi/(si/npc) sj /(sj \npm) (sp\npx )/npy@d [?
]c@ccompleteness @mMarcel @pprove ?
@p?ACT?x ?
@p?PAT?y>Bsp/npy@pprove ?
@p?ACT?m ?
@p?PAT?y ?
@mMarcel>sp@d [?
]c@pprove ?
@p?ACT?m ?
@p?PAT?c ?
@mMarcel ?
@ccompletenessFigure 1: Derivation for topicalization.system can be implemented using just one featurepros which takes the values ip for intermediatephrases, cp for complete phrases, and up for un-marked phrases.
We write spros=ip , or simply sip ifno confusion can arise.Figure 2: Abstract specification of derivational con-trol in prosodyFirst consider the top half of Figure 2.
If a con-stituent is marked with either a ?- or ?-tune, theatomic result category of the (possibly complex)category is marked with ip.
Prosodically unmarkedconstituents are marked as up.
The lexical entriesin (12) illustrates this idea.3(12) MARCEL proved COMPLETENESSH* L+H*sip/(sup\np) (sup\np)/np sip$\(sup$/np)This can proceed in two ways.
Either the markedMARCEL and the unmarked proved combine to pro-duce an intermediate phrase (13), or proved and themarked COMPLETENESS combine (14).
(13) MARCEL proved COMPLETENESSH* L+H*sip/(sup\np) (sup\np)/np sip$\(sup$/np)>sip/np3The $?s in the category for COMPLETENESS are standardCCG schematizations: s$ indicates all functions into s, such ass\np and (s\np)/np.
See Steedman (2000b) for details.
(14) MARCEL proved COMPLETENESSH* L+H*sip/(sup\np) (sup\np)/np sip$\(sup$/np)<sip\npFor the remainder of this paper, we will suppress upmarking and write sup simply as s.Examples (13) and (14) show that prosodicallymarked and unmarked phrases can combine.
How-ever, both of these partial derivations produce cate-gories that cannot be combined further.
For exam-ple, in (14), sip/(s\np) cannot combine with sip\npto yield a larger intermediate phrase.
This properlycaptures the top half of Figure 2.To obtain a complete analysis for (12), bound-ary tones are needed to complete the intermediatephrases tones.
For example, consider (15) (basedon example (70) in Steedman (2000a)):(15) MARCEL proved COMPLETENESSH* L L+H* LH%To capture the bottom-half of Figure 2, the bound-ary tones L and LH% need categories which cre-ate complete phrases out of those for MARCEL andproved COMPLETENESS, and thereafter allow themto combine.
Figure 3 shows the appropriate cate-gories and complete analysis.We noted earlier that downstepped phrasal tunesform an exception to the rule that intermediatephrases cannot combine.
To enable this, we notonly should mark the result category with ip (tune),but also any leftward argument(s) should have ip(downstep).
Thus, the effect of (lexically) combin-ing a downstep tune with an unmarked category isspecified by the following template: add markingxip$\yip to an unmarked category of the form x$\y.The derivation in Figure 5 illustrates this idea on ex-ample (64) from (Steedman, 2000a).To relate prosody to information structure, we ex-tend the strategy used for constructing logical formsdescribed in ?2.3, in which a simple index featureMARCEL proved COMPLETENESSH* L L+H* LH%sip/(s\np) (scp/scp$)\?
(sip/s$) (s\np)/np sip$\(s$/np) scp$\?sip$< <scp/(scp\np) sip\np<scp\np>scpFigure 3: Derivation including tunes and boundary tones; (70) from (Steedman, 2000a)Marcel PROVED COMPLETENESSL+H* LH% H* LL%np (sip:p\npx )/npy scp$\?sip$ sip\(s/npc) (scp\scp$)\?
(sip\s$)@d [?
]p @d [?
]c@mMarcel @pprove ?
@p?ACT?x ?
@p?PAT?y @ccompleteness>T <sip/(sip\np) scp\(scp/npc)@d [?
]c@mMarcel @ccompleteness>Bsip/np@d [?
]p@pprove ?
@p?ACT?m ?
@p?PAT?y ?
@mMarcel<scp/npy@d [?
]p@pprove ?
@p?ACT?m ?
@p?PAT?y ?
@mMarcel<scp@d [?
]p ?
@d [?
]c@pprove ?
@p?ACT?m ?
@p?PAT?c ?
@mMarcel ?
@ccompletenessFigure 4: Information structure for derivation for (67)-(68) from (Steedman, 2000a)on atomic categories makes a nominal (discoursereferent) available.
We represent information struc-ture as a formula @d [i]r at a dimension separatefrom the syntactic category.
The nominal r standsfor the discourse referent, which has informativityi with respect to the current point in the discoursed (Kruijff, 2003).
Following Steedman, we distin-guish two levels of informativity, namely ?
(theme)and ?
(rheme).We start with a minimal assignment of informa-tivity: a theme-tune on a constituent sets the infor-mativity of the discourse referent r realized by theconstituent to ?
and a rheme-tune sets it to ?.
Thisis a minimal assignment in the sense that we do notproject informativity; instead, we only set informa-tivity for those discourse referents whose realizationshows explicit clues as to their information status.The derivation in Figure 4 illustrates this idea andshows the construction of both logical form and in-formation structure.Indices can also impose constraints on the infor-mativity of arguments.
For example, in the down-step example (Figure 5), the discourse referents cor-responding to ANNA and SAYS are both part of thetheme.
We specify this with the constituent that hasreceived the downstepped tune.
The referent of thesubject of SAYS (indexed x) must be in the themealong with the referent s for SAYS.
This is satisfiedin the derivation: a unifies with x, and we can unifythe statements about a?s informativity coming fromANNA (@d [?
]a) and SAYS (@d [?
]x with x replacedby a in the >B step).5 ConclusionsIn this paper, we generalize the traditional Saus-surian sign in CCG with an n-dimensional linguis-tic sign.
The dimensions in the generalized linguis-tic sign can be related through indexation.
Index-ation places constraints on signs by requiring thatco-indexed material is unifiable, on a per-dimensionbasis.
Consequently, we do not need to overload thesyntactic category with information from differentdimensions.The resulting sign structure resembles the signsfound in constraint-based grammar formalisms.There is, however, an important difference.
Infor-mation at various dimensions can be related throughco-indexation, but dimensions cannot be directlyANNA SAYS he proved COMPLETENESSL+H* !L+H* LH%npip:a (sip:s\npip:x )/sy s/(s\np) (sp\np)/np@d [?
]a @d [?
]s ?
@d [?
]x @d [?
](pron) @d [i]p>Tsip/(sip\npip)@d [?
]a>Bsip/s@d [?
]s ?
@d [?
]a>Bsip/(s\np)@d [?
]s ?
@d [?
]a ?
@d [?
](pron)>Bsip/np@d [?
]s ?
@d [?
]a ?
@d [?
](pron) ?
@d [i]pFigure 5: Information structure for derivation for (64) from (Steedman, 2000a)referenced.
As analysis remains driven only by in-ference over categories, only those constraints trig-gered by indexation on the categories are imposed.We do not allow for re-entrancy.It is possible to conceive of a scenario in whichthe various levels can contribute toward determin-ing the well-formedness of an expression.
For ex-ample, we may wish to evaluate the current informa-tion structure against a discourse model, and rejectthe analysis if we find it is unsatisfiable.
If such amove is made, then the complexity will be boundedby the complexity of the dimension for which it ismost difficult to determine satisfiability.AcknowledgmentsThanks to Ralph Debusmann, Alexander Koller,Mark Steedman, and Mike White for discussion.Geert-Jan Kruijff?s work is supported by the DFGSFB 378 Resource-Sensitive Cognitive Processes,Project NEGRA EM 6.ReferencesJason Baldridge and Geert-Jan Kruijff.
2002.
CouplingCCG and Hybrid Logic Dependency Semantics.
InProc.
of 40th Annual Meeting of the ACL, pages 319?326, Philadelphia, Pennsylvania.Jason Baldridge and Geert-Jan Kruijff.
2003.
Multi-Modal Combinatory Categorial Grammar.
In Proc.
of10th Annual Meeting of the EACL, Budapest.Jason Baldridge.
2002.
Lexically Specified DerivationalControl in Combinatory Categorial Grammar.
Ph.D.thesis, University of Edinburgh.Patrick Blackburn.
2000.
Representation, reasoning,and relational structures: a hybrid logic manifesto.Journal of the Interest Group in Pure Logic, 8(3):339?365.Beryl Hoffman.
1995.
Integrating ?free?
word ordersyntax and information structure.
In Proc.
of 7th An-nual Meeting of the EACL, Dublin.Ronald M. Kaplan and Joan Bresnan.
1982.
Lexical-functional grammar: A formal system for grammat-ical representation.
In The Mental Representationof Grammatical Relations, pages 173?281.
The MITPress, Cambridge Massachusetts.Geert-Jan M. Kruijff.
2001.
A Categorial-Modal Logi-cal Architecture of Informativity: Dependency Gram-mar Logic & Information Structure.
Ph.D. thesis,Charles University, Prague, Czech Republic.Geert-Jan M. Kruijff.
2002.
Formulating a category ofinformativity.
In Hilde Hasselgard, Stig Johansson,Bergljot Behrens, and Cathrine Fabricius-Hansen, ed-itors, Information Structure in a Cross-Linguistic Per-spective, pages 129?146.
Rodopi, Amsterdam.Geert-Jan M. Kruijff.
2003.
Binding across boundaries.In Geert-Jan M. Kruijff and Richard T. Oehrle, editors,Resource Sensitivity, Binding, and Anaphora.
KluwerAcademic Publishers, Dordrecht.John T. III Maxwell and Ronald M. Kaplan.
1993.
Theinterface between phrasal and functional constraints.Computational Linguistics, 19(4):571?590.Janet Pierrehumbert and Julia Hirschberg.
1990.
Themeaning of intonational contours in the interpretationof discourse.
In J. Morgan P. Cohen and M. Pollack,editors, Intentions in Communication.
The MIT Press,Cambridge Massachusetts.Carl Pollard and Ivan A.
Sag.
1993.
Head-DrivenPhrase Structure Grammar.
University of ChicagoPress, Chicago IL.Mark Steedman.
2000a.
Information structure andthe syntax-phonology interface.
Linguistic Inquiry,31(4):649?689.Mark Steedman.
2000b.
The Syntactic Process.
TheMIT Press, Cambridge, MA.Enric Vallduv??
and Elisabet Engdahl.
1996.
The linguis-tic realization of information packaging.
Linguistics,34:459?519.Michael White.
2004.
Efficient realization of coordinatestructures in Combinatory Categorial Grammar.
Re-search on Language and Computation.
To appear.
