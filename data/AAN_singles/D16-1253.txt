Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2306?2312,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsBilingually-constrained Synthetic Data for Implicit Discourse RelationRecognitionChangxing Wu1,2, Xiaodong Shi1,2?, Yidong Cheng1,2, Yanzhou Huang1,2, Jinsong Su3Fujian Key Lab of the Brain-like Intelligent Systems, Xiamen University, China1School of Information Science and Technology, Xiamen University, China2School of Software, Xiamen University, China3{wcxnlp, huangyanzhou163}@163.com{mandel, ydchen, jssu}@xmu.edu.cnAbstractTo alleviate the shortage of labeled data, wepropose to use bilingually-constrained syn-thetic implicit data for implicit discourse re-lation recognition.
These data are extractedfrom a bilingual sentence-aligned corpus ac-cording to the implicit/explicit mismatch be-tween different languages.
Incorporatingthese data via a multi-task neural networkmodel achieves significant improvements overbaselines, on both the English PDTB and Chi-nese CDTB data sets.1 IntroductionDiscovering the discourse relation between two sen-tences is crucial to understanding the meaning ofa coherent text, and also beneficial to many down-stream NLP applications, such as question answer-ing and machine translation.
Implicit discourse re-lation recognition (DRRimp) remains a challengingtask due to the absence of strong surface clues likediscourse connectives (e.g.
but).
Most work re-sorts to large amounts of manually designed features(Soricut and Marcu, 2003; Pitler et al, 2009; Lin etal., 2009; Louis et al, 2010; Rutherford and Xue,2014), or distributed features learned via neural net-work models (Braud and Denis, 2015; Zhang et al,2015; Ji and Eisenstein, 2015).
The above methodsusually suffer from limited labeled data.Marcu and Echihabi (2002) attempt to create la-beled implicit data automatically by removing con-nectives from explicit instances, as additional train-ing data.
These data are usually called as syn-?Corresponding author.thetic implicit data (hereafter SynData).
How-ever, Sporleder and Lascarides (2008) argue thatSynData has two drawbacks: 1) meaning shiftsin some cases when removing connectives, and 2)a different word distribution with the real implicitdata.
They also show that using SynData directlydegrades the performance.
Recent work seeks to de-rive valuable information from SynData while fil-tering noise, via domain adaptation (Braud and De-nis, 2014; Ji et al, 2015), classifying connectives(Rutherford and Xue, 2015) or multi-task learning(Lan et al, 2013; Liu et al, 2016), and showspromising results.ch: [??
??
?
???
?
?,]Arg1en: [society reckons the existence of youth problems, ]Arg1implicit=??
[??
???
??
??
?
??.
]Arg2but [many young people do not think there is anythingwrong with them.
]Arg2society reckon  existence  youth problems,but  many  young people  think  themselves no problems.Figure 1: An example illustrating the implicit/explicit mis-match between Chinese (ch) and English (en).
A Chinese im-plicit instance is translated into an English explicit one.
In thePDTB, a discourse instance is defined as a connective (e.g.
but)taking two arguments (Arg1 and Arg2).Different from previous work, we propose to con-struct bilingually-constrained synthetic implicit data(called BiSynData) for DRRimp, which can alle-viate the drawbacks of SynData.
Our method isinspired by the findings that a discourse instance ex-pressed implicitly in one language may be expressedexplicitly in another.
For example, Zhou and Xue2306(2012) show that the connectives in Chinese omitmuch more frequently than those in English withabout 82.0% vs. 54.5%.
Li et al (2014a) furtherargue that there are about 23.3% implicit/explicitmismatchs between Chinese/English instances.
Asillustrated in Figure 1, a Chinese implicit instancewhere the connective ?
is absent, is translatedinto an English explicit one with the connective but.Intuitively, the Chinese instance is a real implicitone which can be signaled by but.
Hence, it couldpotentially serve as additional training data for theChinese DRRimp, avoiding the different word dis-tribution problem of SynData.
Meanwhile, for theEnglish explicit instance, it is very likely that remov-ing but would not lose any information since its Chi-nese counterpart ?
can be omitted.
Therefore itcould be used for the English DRRimp, alleviatingthe meaning shift problem of SynData.We extract our BiSynData from a Chinese-English sentence-aligned corpus (Section 2).
Thenwe design a multi-task neural network model to in-corporate the BiSynData (Section 3).
Experimen-tal results, on both the English PDTB (Prasad et al,2008) and Chinese CDTB (Li et al, 2014b), showthat BiSynData is more effective than SynDataused in previous work (Section 4).
Finally, we re-view the related work (Section 5) and draw conclu-sions (Section 6).2 BiSynDataFormally, given a Chinese-English sentence pair(Sch, Sen), we try to find an English explicit in-stance (Arg1en, Arg2en, Connen) in Sen1, and aChinese implicit instance (Arg1ch, Arg2ch) in Sch,where (Arg1en, Arg2en, Connen) is the transla-tion of (Arg1ch, Arg2ch).
In most cases, dis-course relations should be preserved during trans-lating, so the connective Connen is potentiallya strong indicator of the discourse relation be-tween not only Arg1en and Arg2en, but alsoArg1ch and Arg2ch.
Therefore, we can con-struct two synthetic implicit instances labeled byConnen, denoted as ?
(Arg1en, Arg2en), Connen?and ?
(Arg1ch, Arg2ch), Connen?, respectively.
Werefer to these synthetic instances as BiSynData be-1In our experiments, we use the pdtb-parser toolkit (Lin etal., 2014) to identify English explicit instances.cause they are constructed according to the bilingualimplicit/explicit mismatch.Conn.
Freq.
Conn. Freq.and 14294 while 1031if 2580 before 822as 1951 also 552when 1521 since 511but 1122 because 503Table 1: Top 10 most frequent connectives in our BiSynData.In our experiments, we extract our BiSynDatafrom a combined corpus (FBIS and HongKongLaw), with about 2.38 million Chinese-English sen-tence pairs.
We generate 30,032 synthetic En-glish instances and the same number of Chinese in-stances, with 80 connectives, as our BiSynData.Table 1 lists the top 10 most frequent connec-tives in our BiSynData, which are roughly con-sistent with the statistics of Chinese/English im-plicit/explicit mismatches in (Li et al, 2014a).
Ac-cording to connectives and their related relationsin the PDTB, in most cases, and and also indi-cate the Expansion relation, if and because theContigency relation, before the Temporal rela-tion, and but the Comparison relation.
Connec-tives as, when, while and since are ambiguous.For example, while can indicate the Comparisonor Temporal relation.
Overall, our constructedBiSynData covers all four main discourse rela-tions defined in the PDTB.With our BiSynData, we define two connec-tive classification tasks: 1) given (Arg1en, Arg2en)to predict the connective Connen, and 2) given(Arg1ch, Arg2ch) to predict Connen.
We incorpo-rate the first task to help the English DRRimp, andthe second for the Chinese DRRimp.
It is worthyto note that we use English connectives themselvesas classification labels rather than mapping them torelations in both tasks.3 Multi-Task Neural Network ModelWe design a Multi-task Neural Network Model (de-noted as MTN ), which incorporates a connectiveclassification task on BiSynData (auxiliary task)to benefit DRRimp (main task).
In general, the morerelated two tasks are, the more powerful a multi-tasklearning method will be.
In the current problem, the2307two tasks are essentially the same, just with differ-ent output labels.
Therefore, as illustrated in Fig-ure 2, MTN shares parameters in all feature layers(L1-L3) and uses two separate classifiers in the clas-sifier layer (L4).
For each task, given an instance(Arg1, Arg2), MTN simply averages embeddingsof words to represent arguments, as vArg1 and vArg2 .These two vectors are then concatenated and trans-formed through two non-linear hidden layers.
Fi-nally, the corresponding softmax layer is used toperform classification.
3mainw3auxw1w 2w Main TaskAux Task1L 2L 3L 4L1Argv 2ArgvFigure 2: MTN with four layers L1-L4.MTN ignores the word order in arguments anduses two hidden layers to capture the interactionsbetween two arguments.
The idea behind MTN isborrowed from (Iyyer et al, 2015), where a deepaveraging network achieves close to the state-of-the-art performance on text classification.
ThoughMTN is simple, it is easy to train and efficient onboth memory and computational cost.
In addition,the simplicity of MTN allows us to focus on mea-suring the quality of BiSynData.We use the cross-entropy loss function and mini-batch AdaGrad (Duchi et al, 2011) to optimize pa-rameters.
Pre-trained word embeddings are fixed.We find that fine-tuning word embeddings duringtraining leads to severe overfitting in our experi-ments.
Following Liu et al (2016), we alternatelyuse two tasks to train the model, one task per epoch.For tasks on both the PDTB and CDTB, we use thesame hyper-parameters.
The dimension of word em-bedding is 100.
We set the size of L2 to 200, andL3 to 100.
ReLU is used as the non-linear func-tion.
Different learning rates 0.005 and 0.001 areused in the main and auxiliary tasks, respectively.
Toavoid overfitting, we randomly drop out 20% wordsin each argument following Iyyer et al (2015).
Allhyper-parameters are tuned on the development set.4 ExperimentsWe evaluate our method on both the English PDTBand Chinese CDTB data sets.
We tokenize Englishdata and segment Chinese data using the StanfordCoreNLP toolkit (Manning et al, 2014).
The En-glish/Chinese Gigaword corpus (3rd edition) is usedto train the English/Chinese word embeddings viaword2vec (Mikolov et al, 2013), respectively.
Dueto the skewed class distribution of test data (see Sec-tion 4.1), we use the macro-averaged F1 for perfor-mance evaluation.4.1 On the PDTBFollowing Rutherford and Xue (2015), we perform a4-way classification on the top-level discourse rela-tions: Temporal (Temp), Comparison (Comp),Contigency (Cont) and Expansion (Expa).
Sec-tions 2-20 are used as training set, sections 0-1as development set and sections 21-22 as test set.The training/test set contains 582/55 instances forTemp, 1855/145 for Comp, 3235/273 for Contand 6673/538 for Expa.
The top 20 most frequentconnectives in our BiSynData are considered inthe auxiliary task, with 28,013 synthetic English in-stances in total.STN MTNbiTemp P 33.33 34.48R 14.55 18.18F1 20.25 23.81Comp P 38.54 42.11R 25.52 33.10F1 30.71 37.07Cont P 38.36 44.22R 41.03 40.66F1 39.65 42.37Expa P 59.60 62.56R 66.36 71.75F1 62.80 66.84macro F1 38.35 42.52Table 2: Results of 4-way classification on the PDTB.Table 2 shows the results of MTN combining ourBiSynData (denoted as MTNbi) on the PDTB.2308STN means we train MTN with only the maintask.
On the macro F1, MTNbi gains an improve-ment of 4.17% over STN .
The improvement is sig-nificant under one-tailed t-test (p<0.05).
A closerlook into the results shows that MTNbi performsbetter across all relations, on the precision, recalland F1 score, except a little drop on the recall ofCont.
The reason for the recall drop of Cont isnot clear.
The greatest improvement is observed onComp, up to 6.36% F1 score.
The possible reasonis that only while is ambiguous about Comp andTemp, while as, when and since are all ambigu-ous about Temp and Cont, among top 10 connec-tives in our BiSynData.
Meanwhile the amountof labeled data for Comp is relatively small.
Over-all, using BiSynData under our multi-task modelachieves significant improvements on the EnglishDRRimp.
We believe the reasons for the improve-ments are twofold: 1) the added synthetic Englishinstances from our BiSynData can alleviate themeaning shift problem, and 2) a multi-task learningmethod is helpful for addressing the different worddistribution problem between implicit and explicitdata.Considering some of the English connectives(e.g., while) are highly ambiguous, we compare ourmethod with ones that uses only unambiguous con-nectives.
Specifically, we first discard as, when,while and since in top 20 connectives, and get22,999 synthetic instances.
Then, we leverage theseinstances in two different ways: 1) using them in ourmulti-task model as above, and 2) using them as ad-ditional training data directly after mapping unam-biguous connectives into relations.
Both methodsusing only unambiguous connectives do not achievebetter performance.
One possible reason is that thesesynthetic instances become more unbalanced afterdiscarding ones with ambiguous connectives.We also compare MTNbi with recent systems us-ing additional training data.
Rutherford and Xue(2015) select explicit instances that are similar to theimplicit ones via connective classification, to enrichthe training data.
Liu et al (2016) use a multi-taskmodel with three auxiliary tasks: 1) conn: connec-tive classification on explicit instances, 2) exp: re-lation classification on the labeled explicit instancesin the PDTB, and 3) rst: relation classification onthe labeled RST corpus (William and Thompson,System macro F11 Rutherford and Xue (2015) 40.502 Liu et al (2016) conn 38.093 Liu et al (2016) exp 39.034 Liu et al (2016) rst 40.675 Liu et al (2016) conn+exp+rst 44.986 MTNbi 42.52Table 3: Comparison with recent systems on the PDTB.
conn+exp + rst means using three auxiliary tasks simultaneously.1988), which defines different discourse relationswith that in the PDTB.
The results are shown in Ta-ble 3.
Although Liu et al (2016) achieve the state-of-the-art performance (Line 5), they use two ad-ditional labeled corpora.
We can find that MTNbi(Line 6) yields better results than those systems in-corporating SynData (Line 1, 2 and 3), or even thelabeled RST (Line 4).
These results confirm thatBiSynData can indeed alleviate the disadvantagesof SynData effectively.4.2 On the CDTBFour top-level relations are defined in the CDTB,including Transition (Tran), Causality (Caus),Explanation (Expl) and Coordination (Coor).We use instances in the first 50 documents as testset, second 50 documents as development set and re-maining 400 documents as training set.
We conducta 3-way classification because of only 39 instancesfor Tran.
The training/test set contains 682/95 in-stances for Caus, 1143/126 for Expl and 2300/347for Coor.
The top 20 most frequent connectives(excluding and)2 in our BiSynData are consid-ered in the auxiliary task, with 13,899 synthetic Chi-nese instances in total.
The results are shown inTable 4.
Compared with STN , MTNbi raises themacro F1 from 55.44% to 58.28%.
The improve-ment is significant under one-tailed t-test (p<0.05).Therefore, BiSynData is also helpful for the Chi-nese DRRimp.Because of no reported results on the CDTB, weuse MTN with two different auxiliary tasks as base-lines: 1) exp: relation classification on the labeled2Including and degrades the performance slightly.
A pos-sible reason is that and can be related to both the Expl andCoor relations in the CDTB, and instances marked by and ac-count for about half of our BiSynData.2309STN MTNbiCaus P 47.92 52.94R 24.21 28.42F1 32.17 36.99Expl P 54.62 53.47R 56.35 61.11F1 55.47 57.04Coor P 74.36 78.02R 83.57 83.86F1 78.70 80.83macro F1 55.44 58.28Table 4: Results of 3-way classification on the CDTB.explicit instances in the CDTB, including 466 in-stances for Caus, 201 for Expl and 974 for Coor.2) conn: connective classification on explicit in-stances from the Xinhua part of the Chinese Giga-word corpus.
We collect explicit instances with thetop 20 most frequent Chinese connectives and sam-ple 20,000 instances for the experiment.
Both expand conn can be considered as tasks on SynData.The results in Table 5 show that MTN incorporat-ing BiSynData (Line 3) performs better than usingSynData (Line 1 and 2), for the task on the CDTB.System macro F11 MTNexp 56.422 MTNconn 56.863 MTNbi 58.28Table 5: MTN with different auxiliary tasks on the CDTB.5 Related WorkOne line of research related to DRRimp tries totake advantage of explicit discourse data.
Zhou etal.
(2010) predict the absent connectives based ona language model.
Using these predicted connec-tives as features is proven to be helpful.
Biran andMcKeown (2013) aggregate word-pair features thatare collected around the same connectives, whichcan effectively alleviate the feature sparsity prob-lem.
More recently, Braud and Denis (2014) and Jiet al (2015) consider explicit data from a differentdomain, and use domain adaptation methods to ex-plore the effect of them.
Rutherford and Xue (2015)propose to gather weakly labeled data from explicitinstances via connective classification, which areused as additional training data directly.
Lan et al(2013) and Liu et al (2016) combine explicit andimplicit data using multi-task learning models andgain improvements.
Different from all the abovework, we construct additional training data from abilingual corpus.Multi-task neural networks have been success-fully used for many NLP tasks.
For example, Col-lobert et al (2011) jointly train models for the Part-of-Speech tagging, chunking, named entity recogni-tion and semantic role labeling using convolutionalnetwork.
Liu et al (2015) successfully combine thetasks of query classification and ranking for websearch using a deep multi-task neural network.
Lu-ong et al (2016) explore multi-task sequence tosequence learning for constituency parsing, imagecaption generation and machine translation.6 ConclusionIn this paper, we introduce bilingually-constrainedsynthetic implicit data (BiSynData), which aregenerated based on the bilingual implicit/explicitmismatch, into implicit discourse relation recogni-tion for the first time.
On both the PDTB and CDTB,using BiSynData as the auxiliary task significantlyimproves the performance of the main task.
Wealso show that BiSynData is more beneficial thanthe synthetic implicit data typically used in previouswork.
Since the lack of labeled data is a major chal-lenge for implicit discourse relation classification,our proposed BiSynData can enrich the trainingdata and then benefit future work.AcknowledgmentsWe would like to thank all the reviewers for theirconstructive and helpful suggestions on this paper.This work is partially supported by the Natural Sci-ence Foundation of China (Grant Nos.
61573294,61303082, 61672440), the Ph.D. Programs Founda-tion of Ministry of Education of China (Grant No.20130121110040), the Fund of Research Projectof Tibet Autonomous Region of China (Grant No.Z2014A18G2-13), and the Natural Science Founda-tion of Fujian Province (Grant No.
2016J05161).2310ReferencesOr Biran and Kathleen McKeown.
2013.
AggregatedWord Pair Features for Implicit Discourse RelationDisambiguation.
In Proceedings of ACL (Volume 2:Short Papers), pages 69?73, Sofia, Bulgaria.Chloe?
Braud and Pascal Denis.
2014.
CombiningNatural and Artificial Examples to Improve ImplicitDiscourse Relation Identification.
In Proceedings ofCOLING, pages 1694?1705, Dublin, Ireland.Chloe?
Braud and Pascal Denis.
2015.
Comparing WordRepresentations for Implicit Discourse Relation Clas-sification.
In Proceedings of EMNLP, pages 2201?2211, Lisbon, Portugal.Ronan Collobert, Jason Weston, Le?on Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011.Natural Language Processing (Almost) from Scratch.Journal of Machine Learning Research, 12(1):2493?2537.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive Subgradient Methods for Online Learningand Stochastic Optimization.
The Journal of MachineLearning Research, 12:2121?2159.Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber,and Hal Daume?
III.
2015.
Deep Unordered Com-position Rivals Syntactic Methods for Text Classifi-cation.
In Proceedings of ACL-IJCNLP, pages 1681?1691, Beijing, China.Yangfeng Ji and Jacob Eisenstein.
2015.
One Vector isNot Enough: Entity-Augmented Distributed Seman-tics for Discourse Relations.
In Transactions of theAssociation for Computational Linguistics, volume 3,pages 329?344.Yangfeng Ji, Gongbo Zhang, and Jacob Eisenstein.
2015.Closing the Gap: Domain Adaptation from Explicitto Implicit Discourse Relations.
In Proceedings ofEMNLP, pages 2219?2224, Lisbon, Portugal.Man Lan, Yu Xu, and Zhengyu Niu.
2013.
LeveragingSynthetic Discourse Data via Multi-task Learning forImplicit Discourse Relation Recognition.
In Proceed-ings of ACL, pages 476?485, Sofia, Bulgaria.Junyi Jessy Li, Marine Carpuat, and Ani Nenkova.2014a.
Cross-lingual Discourse Relation Analysis:A Corpus Study and a Semi-supervised ClassificationSystem.
In Proceedings of COLING : Technical Pa-pers, pages 577?587, Dublin, Ireland.Yancui Li, Wenhe Feng, Jing Sun, Fang Kong, andGuodong Zhou.
2014b.
Building Chinese Dis-course Corpus with Connective-driven DependencyTree Structure.
In Proceedings of EMNLP, pages2105?2114, Doha, Qatar.Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng.
2009.Recognizing Implicit Discourse Relations in the PennDiscourse Treebank.
In Proceedings of EMNLP,pages 343?351, PA, USA.Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014.
APDTB-styled End-to-end Discourse Parser.
NaturalLanguage Engineering, 20(02):151?184.Xiaodong Liu, Jianfeng Gao, Xiaodong He, Li Deng,Kevin Duh, and Ye-Yi Wang.
2015.
RepresentationLearning Using Multi-task Deep Neural Networks forSemantic Classification and Information Retrieval.
InProceedings of NAACL, pages 912?921, Denver, Col-orado.Yang Liu, Sujian Li, Xiaodong Zhang, and Zhifang Sui.2016.
Implicit Discourse Relation Classification viaMulti-Task Neural Networks.
In Proceedings of AAAI,pages 2750?2756, Arizona, USA.Annie Louis, Aravind Joshi, Rashmi Prasad, and AniNenkova.
2010.
Using Entity Features to ClassifyImplicit Discourse Relations.
In Proceedings of SIG-DIAL, pages 59?62, PA, USA.Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, OriolVinyals, and Lukasz Kaiser.
2016.
Multi-task Se-quence to Sequence Learning.
In Proceedings ofICLR, pages 1?10, San Juan, Puerto Rico.Christopher D. Manning, Mihai Surdeanu, John Bauer,Jenny Finkel, Steven J. Bethard, and David McClosky.2014.
The Stanford CoreNLP Natural Language Pro-cessing Toolkit.
In Proceedings of ACL (SystemDemonstrations), pages 55?60, Maryland, USA.Daniel Marcu and Abdessamad Echihabi.
2002.
An Un-supervised Approach to Recognizing Discourse Rela-tions.
In Proceedings of ACL, pages 368?375, PA,USA.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient Estimation of Word Represen-tations in Vector Space.
CoRR, abs/1301.3781.Emily Pitler, Annie Louis, and Ani Nenkova.
2009.
Au-tomatic Sense Prediction for Implicit Discourse Rela-tions in Text.
In Proceedings of ACL-IJCNLP, pages683?691, PA, USA.Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-sakaki, Livio Robaldo, Aravind Joshi, and BonnieWebber.
2008.
The Penn Discourse TreeBank 2.0.
InProceedings of LREC, volume 24, pages 2961?2968,Marrakech, Morocco.Attapol T. Rutherford and Nianwen Xue.
2014.
Dis-covering Implicit Discourse Relations Through BrownCluster Pair Representation and Coreference Patterns.In Proceedings of EACL, pages 645?654, Gothenburg,Sweden.Attapol Rutherford and Nianwen Xue.
2015.
Improvingthe Inference of Implicit Discourse Relations via Clas-sifying Explicit Discourse Connectives.
In Proceed-ings of NAACL, pages 799?808, Denver, Colorado.2311Radu Soricut and Daniel Marcu.
2003.
Sentence LevelDiscourse Parsing Using Syntactic and Lexical Infor-mation.
In Proceedings of NAACL, pages 149?156,PA, USA.Caroline Sporleder and Alex Lascarides.
2008.
UsingAutomatically Labelled Examples to Classify Rhetori-cal Relations: An Assessment.
Natural Language En-gineering, 14(3):369?416.Mann William and Sandra Thompson.
1988.
Rhetoricalstructure theory: Towards a Functional Theory of TextOrganization.
Text, 8(3):243?281.Biao Zhang, Jinsong Su, Deyi Xiong, Yaojie Lu, HongDuan, and Junfeng Yao.
2015.
Shallow Convolu-tional Neural Network for Implicit Discourse RelationRecognition.
In Proceedings of EMNLP, pages 2230?2235, Lisbon, Portugal.Yuping Zhou and Nianwen Xue.
2012.
PDTB-style dis-course annotation of Chinese text.
In Proceedings ofACL, pages 69?77, Jeju Island, Korea.Zhi-Min Zhou, Yu Xu, Zheng-Yu Niu, Man Lan, JianSu, and Chew Lim Tan.
2010.
Predicting DiscourseConnectives for Implicit Discourse Relation Recogni-tion.
In Proceedings of COLING, pages 1507?1514,PA, USA.2312
