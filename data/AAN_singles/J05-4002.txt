Co-occurrence Retrieval:A Flexible Framework for LexicalDistributional SimilarityJulie Weeds and David Weir?University of SussexTechniques that exploit knowledge of distributional similarity between words have been proposedin many areas of Natural Language Processing.
For example, in language modeling, the sparsedata problem can be alleviated by estimating the probabilities of unseen co-occurrences of eventsfrom the probabilities of seen co-occurrences of similar events.
In other applications, distribu-tional similarity is taken to be an approximation to semantic similarity.
However, due to the widerange of potential applications and the lack of a strict definition of the concept of distributionalsimilarity, many methods of calculating distributional similarity have been proposed or adopted.In this work, a flexible, parameterized framework for calculating distributional similarity isproposed.
Within this framework, the problem of finding distributionally similar words is castas one of co-occurrence retrieval (CR) for which precision and recall can be measured by analogywith the way they are measured in document retrieval.
As will be shown, a number of popularexisting measures of distributional similarity are simulated with parameter settings within theCR framework.
In this article, the CR framework is then used to systematically investigate threefundamental questions concerning distributional similarity.
First, is the relationship of lexicalsimilarity necessarily symmetric, or are there advantages to be gained from considering it as anasymmetric relationship?
Second, are some co-occurrences inherently more salient than othersin the calculation of distributional similarity?
Third, is it necessary to consider the difference inthe extent to which each word occurs in each co-occurrence type?Two application-based tasks are used for evaluation: automatic thesaurus generation andpseudo-disambiguation.
It is possible to achieve significantly better results on both these tasks byvarying the parameters within the CR framework rather than using other existing distributionalsimilarity measures; it will also be shown that any single unparameterized measure is unlikely tobe able to do better on both tasks.
This is due to an inherent asymmetry in lexical substitutabilityand therefore also in lexical distributional similarity.1.
IntroductionOver recent years, approaches to a broad range of natural language processing (NLP)applications have been proposed that require knowledge about the similarity of words.The application areas in which these approaches have been proposed range from speechrecognition and parse selection to information retrieval (IR) and natural language?
Department of Informatics, University of Sussex, Falmer, Brighton, BN1 9QH, UK.Submission received: 4 May 2004; revised submission received: 16 November 2004; accepted forpublication: 16 April 2005.?
2006 Association for Computational LinguisticsComputational Linguistics Volume 31, Number 4generation.
For example, language models that incorporate substantial lexical knowl-edge play a key role in many statistical NLP techniques (e.g., in speech recognitionand probabilistic parse selection).
However, they are difficult to acquire, since manyplausible combinations of events are not seen in corpus data.
Brown et al (1992) reportthat one can expect 14.7% of the word triples in any new English text to be unseen in atraining corpus of 366 million English words.
In our own experiments with grammaticalrelation data extracted by a Robust Accurate Statistical Parser (RASP) (Briscoe andCarroll 1995; Carroll and Briscoe 1996) from the British National Corpus (BNC), wefound that 14% of noun-verb direct-object co-occurrence tokens and 49% of noun-verbdirect-object co-occurrence types in one half of the data set were not seen in the otherhalf.
A statistical technique using a language model that assigns a zero probabilityto these previously unseen events will rule the correct parse or interpretation of theutterance impossible.Similarity-based smoothing (Hindle 1990; Brown et al 1992; Dagan, Marcus, andMarkovitch 1993; Pereira, Tishby, and Lee 1993; Dagan, Lee, and Pereira 1999) providesan intuitively appealing approach to language modeling.
In order to estimate the prob-ability of an unseen co-occurrence of events, estimates based on seen occurrences ofsimilar events can be combined.
For example, in a speech recognition task, we mightpredict that cat is a more likely subject of growl than the word cap, even though neitherco-occurrence has been seen before, based on the fact that cat is ?similar?
to words thatdo occur as the subject of growl (e.g., dog and tiger), whereas cap is not.However, what is meant when we say that cat is ?similar?
to dog?
Are we referring totheir semantic similarity, e.g., the components of meaning they share by virtue of bothbeing carnivorous four-legged mammals?
Or are we referring to their distributionalsimilarity, e.g., in keeping with the Firthian tradition,1 the fact that these words tend tooccur as the arguments of the same verbs (e.g., eat, feed, sleep) and tend to be modifiedby the same adjectives (e.g., hungry and playful).In some applications, the knowledge required is clearly semantic.
In IR, documentsmight be usefully retrieved that use synonymous terms or terms subsuming those speci-fied in a user?s query (Xu and Croft 1996).
In natural language generation (including textsimplification), possible words for a concept should be similar in meaning rather thanjust in syntactic or distributional behavior.
In these application areas, distributional sim-ilarity can be taken to be an approximation to semantic similarity.
The underlying idea isbased largely on the central claim of the distributional hypothesis (Harris 1968), that is:The meaning of entities, and the meaning of grammatical relations among them, isrelated to the restriction of combinations of these entities relative to other entities.This hypothesized relationship between distributional similarity and semantic sim-ilarity has given rise to a large body of work on automatic thesaurus generation (Hindle1990; Grefenstette 1994; Lin 1998a; Curran and Moens 2002; Kilgarriff 2003).
Thereare inherent problems in evaluating automatic thesaurus extraction techniques, andmuch research assumes a gold standard that does not exist (see Kilgarriff [2003] andWeeds [2003] for more discussion of this).
A further problem for distributional similaritymethods for automatic thesaurus generation is that they do not offer any obvious way todistinguish between linguistic relations such as synonymy, antonymy, and hyponymy(see Caraballo [1999] and Lin et al [2003] for work on this).
Thus, one may question1 ?You shall know a word by the company it keeps.?
(Firth 1957)440Weeds and Weir Co-occurrence Retrievalthe benefit of automatically generating a thesaurus if one has access to large-scalemanually constructed thesauri (e.g., WordNet [Fellbaum 1998], Roget?s [Roget 1911], theMacquarie [Bernard 1990] and Moby2).
Automatic techniques give us the opportunityto model language change over time or across domains and genres.
McCarthy et al(2004) investigate using distributional similarity methods to find predominant wordsenses within a corpus, making it possible to tailor an existing resource (WordNet) tospecific domains.
For example, in the computing domain, the word worm is more likelyto be used in its ?malicious computer program?
sense than in its ?earthworm?
sense.This domain knowledge will be reflected in a thesaurus automatically generated froma computing-specific corpus, which will show increased similarity between worm andvirus and reduced similarity between worm and caterpillar.In other application areas, however, the requirement for ?similar?
words to besemantically related as well as distributionally related is less clear.
For example, inprepositional phrase attachment ambiguity resolution, it is necessary to decide whetherthe prepositional phrase attaches to the verb or the noun as in the examples (1) and (2).1.
Mary ((visited their cottage) with her brother).2.
Mary (visited (their cottage with a thatched roof)).Hindle and Rooth (1993) note that the correct decision depends on all four lexicalevents (the verb, the object, the preposition, and the prepositional object).
However, astatistical model built on the basis of four lexical events must cope with extremely sparsedata.
One approach (Resnik 1993; Li and Abe 1998; Clark and Weir 2000) is to induceprobability distributions over semantic classes rather than lexical items.
For example, acottage is a type of building and a brother is a type of person, and so the co-occurrence ofany type of building and any type of person might increase the probability that the PPin example (1) attaches to the verb.However, it is unclear whether the classes over which probability distributionsare induced need to be semantic or whether they could be purely distributional.
Ifwe know that two words tend to behave the same way with respect to prepositionalphrase attachment, does it matter whether they mean similar things?
Other argumentsfor using semantic classes over distributional classes can similarly be disputed (Weeds2003).
For example, it is not necessary for a class of objects to have a name or symboliclabel for us to know that the objects are similar and to exploit that information.
Distri-butional classes do conflate word senses, but in a task such as PP-attachment ambiguityresolution, we are unlikely to be working with sense-tagged examples and therefore itis for word forms that we will wish to estimate probabilities of different attachments.Finally, distributional classes may be over-fitted to a specific corpus, but this may bebeneficial to the extent that the over-fitting reflects a specific domain or dialect.Further, recent empirical evidence suggests that techniques based on distributionalsimilarity may perform as well on this task as those based on semantic similarity.Li (2002) shows that using a fairly small corpus (126,084 sentences from the WallStreet Journal) and a distributional similarity technique, it is possible to outperform astate-of-the-art, WordNet-based technique in terms of accuracy, although not in termsof coverage.
Pantel and Lin (2000) report performance of 84.3% using an unsuper-vised approach to prepositional phrase attachment based on distributional similarity2 The Moby Thesaurus is a product of the Moby Project, which was released into the public domain byGrady Ward in 1996.441Computational Linguistics Volume 31, Number 4techniques.
This significantly outperforms previous unsupervised techniques and isdrawing close to the state-of-the-art supervised techniques (88.2%).Having discussed why distributional similarity is important, we now turn to howto formulate it.
As we have said, two words are distributionally similar if they appear insimilar contexts.
We therefore need to consider what is meant by context.
For example,two words could be considered to appear in the same context if they appear in thesame sentence, the same document, or the same grammatical dependency relation.
Theeffect of the type of context used is discussed by Kilgarriff and Yallop (2000).
They showthat the use of sentence-level and document-level context leads to ?looser?
thesaurimore akin to Roget?s, whereas the use of grammatical dependency relation level contextleads to ?tighter?
thesauri more akin to WordNet.
The use of grammatical dependencyrelations as context gives us a tighter thesaurus because it restricts distributionallysimilar words to those that are plausibly inter-substitutable (Church et al 1994), givingus the following definition of distributional similarity:The distributional similarity of two words is the extent to which they can beinter-substituted without changing the plausibility3 of the sentence.This concept of lexical substitutability highlights the relationship between distri-butional similarity and semantic similarity, since semantic similarity can be thought ofas the degree of synonymy that exists between two words, where synonymy is defined(Church et al 1994) as follows:Two words are absolute synonyms if they can be inter-substituted in all possiblecontexts without changing the meaning.In our empirical work, we focus on finding semantic relationships between wordssuch as synonymy, antonymy and hyponymy that might be found in a tighter thesaurussuch as WordNet.
Hence, the proposed framework is based on the concept of substi-tutability, and we use grammatical dependency relations as context.
However, since theframework is based on features, there is no reason why someone wishing to find topicalrelationships between words, as might be found in Roget?s, could not use the framework.We simply do not repeat the earlier work of Kilgarriff and Yallop (2000).However, a number of questions still remain, which this work does investigate:1.
Is lexical substitutability and therefore distributional similaritysymmetric?
The concept of substitution is inherently asymmetric.
It ispossible to measure the appropriateness of substituting word A for word Bwithout measuring the appropriateness of substituting word B for word A.Similarity has been defined in terms of inter-substitutability; but we askwhether there is something in the inherent asymmetry of substitution thatcan be exploited by an asymmetric measure of distributional similarity.2.
Are all contexts equally important?
For example, some verbs, e.g., have andget, are selectionally weak in the constraints they place on their arguments(Resnik 1993).
Should such contexts be considered on equal terms withselectionally strong contexts in the calculation of distributional similarity?3 We use ?plausible sentence?
to refer to a sentence that might be observed in naturally occurringlanguage data.442Weeds and Weir Co-occurrence Retrieval3.
Is it necessary to consider the difference in extent to which each wordappears in each context?
Is it enough to know that both words can occur ineach context, or do similar words occur in similar contexts with similarprobabilities?In order to answer these questions, we take a pragmatic, application-oriented ap-proach to evaluation that is based on the assumption that we want to know whichwords are distributionally similar because particular applications can make use of thisinformation.However, high performance in one application area is not necessarily correlatedwith high performance in another application area (Weeds and Weir 2003a).
Thus, itis not clear that the same characteristics that make a distributional similarity measureuseful in one application will make it useful in another.
For example, with regard to thequestion about symmetry, in some applications we may prefer a word A that can besubstituted for word B in all of the contexts in which B occurs.
In other applications, wemay prefer a word A that can be substituted for word B in all of the contexts in which Aoccurs.
For example, asked for a semantically related word to dog, we might say animal,since animal can generally be used in place of dog, whereas we might be less likely tosay dog for animal, since dog cannot generally be used in place of animal.
This preferencein the direction of the relationship between the two words is not necessarily maintainedwhen one considers language modeling in the face of sparse data.
If we want to learnwhat other contexts animal can occur in, we might look at the co-occurrences of wordssuch as dog, since we know that dog can generally be replaced by animal.
If we want tolearn what other contexts dog can occur in, we are less likely to look at the co-occurrencesof animal, since we know that animal can occur in contexts in which dog cannot.Rather than attempt to find a single universally optimal distributional similaritymeasure, or propose using a radically different distributional similarity measure in eachpossible application, we propose a flexible, parameterized framework for calculatingdistributional similarity (Section 2).
Within this framework, we cast the problem offinding distributionally similar words as one of co-occurrence retrieval (CR), for whichwe can measure precision and recall by analogy with the way that they are measuredin document retrieval.
Different models within this framework allow us to investigatehow frequency information is incorporated into the distributional similarity measure.Different parameter settings within each model allow us to investigate asymmetry insimilarity.
In Section 3 we discuss the data and the neighbor set comparison tech-nique used throughout our empirical work.
In Section 4 we discuss a number ofexisting distributional similarity measures and discuss the extent to which these canbe simulated by settings within the CR framework.
In Section 5 we evaluate the CRframework on a semantic task (WordNet prediction) and on a language modeling task(pseudo-disambiguation).2.
Co-occurrence RetrievalIn this section, we present a flexible framework for distributional similarity.
This frame-work directly defines a similarity function, does not require smoothing of the baselanguage model, and allows us to systematically explore the questions about similarityraised in Section 1.
In our approach, similarity between words is viewed as a measureof how appropriate it is to use one word (or its distribution) in place of the other.
Likerelative entropy (Cover and Thomas 1991), it is inherently asymmetric, since we can443Computational Linguistics Volume 31, Number 4measure how appropriate it is to use word A instead of word B separately from howappropriate it is to use word B instead of word A.The framework presented here is general to the extent that it can be used to computesimilarities for any set of objects where each object has an associated set of featuresor co-occurrence types and these co-occurrence types have associated frequenciesthat may be used to form probability estimates.
Throughout our discussion, the wordfor which we are finding neighbors will be referred to as the target word.
If we arecomputing the similarity between the target word and another word, then the secondword is a potential neighbor of the target word.
A target word?s nearest neighbors arethe potential neighbors that have the highest similarity with the target word.2.1 Basic ConceptsLet us imagine that we have formed descriptions of each word in terms of the otherwords with which they co-occur in various specified grammatical relations in somecorpus.
For example, the noun cat might have the co-occurrence types ?dobj-of, feed?
and?ncmod-by, hungry?.
Now let us imagine that we have lost (or accidentally deleted) thedescription for word w2, but before this happened we had noticed that the descriptionof word w2 was very similar to that of word w1.
For example, the noun dog might alsohave the co-occurrence types ?dobj-of, feed?
and ?ncmod-by, hungry?.
Hence, we decidethat we can use the description of word w1 instead of the description of word w2 andare hopeful that nobody will notice.
How well we do will depend on the validity ofsubstituting w1 for w2, or, in other words, the similarity between w1 and w2.The task we have set ourselves can be seen as co-occurrence retrieval (CR).
Byanalogy with information retrieval, where there is a set of documents that we wouldlike to retrieve and a set of documents that we do retrieve, we have a scenario wherethere is a set of co-occurrences that we would like to retrieve, the co-occurrences of w2,and a set of co-occurrences that we have retrieved, the co-occurrences of w1.
Continuingthe analogy, we can measure how well we have done in terms of precision and recall,where precision tells us how much of what was retrieved was correct and recall tells ushow much of what we wanted to retrieve was retrieved.Our flexible framework for distributional similarity is based on this notion of co-occurrence retrieval.
As the distribution of word B moves away from being identicalto that of word A, its ?similarity?
with A can decrease along one or both of twodimensions.
When B occurs in contexts that word A does not, the result is a loss ofprecision, but B may remain a high-recall neighbor.
For example, we might expect thenoun animal to be a high-recall neighbor of the noun dog.
When B does not occur incontexts that A does occur in, the result is a loss of recall but B may remain a high-precision neighbor.
For example, we might expect the noun dog to be a high-precisionneighbor of the noun animal.
We can explore the merits of symmetry and asymmetry ina similarity measure by varying the relative importance attached to precision and recall.This was the first question posed about distributional similarity in Section 1.The remainder of this section is devoted to defining two types of co-occurrenceretrieval model (CRM).
Additive models are based on the Boolean concept of twoobjects either sharing or not sharing a particular feature (where objects are words andfeatures are co-occurrence types).
Difference-weighted models incorporate the differ-ence in extent to which each word has each feature.
Exploring the two types of models,both defined on the same concepts of precision and recall, allows us to investigate thethird question posed in Section 1: Is a shared context worth the same, regardless of thedifference in the extent to which each word appears in that context?444Weeds and Weir Co-occurrence RetrievalWe also use the CR framework to investigate the second question posed aboutdistributional similarity, ?Should all contexts be treated equally?,?
by using differ-ent weight functions within each type of model.
Weight functions decide which co-occurrence types are features of a word and determine the relative importance offeatures.
In previous work (Weeds and Weir 2003b), we experimented with weightfunctions based on combinatorial, probabilistic, and mutual information (MI).
Theseallow us to define type-based, token-based, and MI-based CRMs, respectively.
Thiswork extends the previous work by also considering weighted mutual information(WMI) (Fung and McKeown 1997), the t-test (Manning and Schu?tze 1999), the z-test(Fontenelle et al 1994), and an approximation to the log-likelihood ratio (Manning andSchu?tze 1999) as weight functions.2.2 Additive ModelsHaving considered the intuition behind calculating precision and recall for co-occurrence retrieval, we now formulate this formally in terms of an additive model.We first need to consider for each word w which co-occurrence types will be re-trieved, or predicted, by it and, conversely, required in a description of it.
We will referto these co-occurrence types as the features of w, F(w):F(w) = {c : D(w, c) > 0} (1)where D(w, c) is the weight associated with word w and co-occurrence type c. Possibleweight functions will be described in Section 2.3.The shared features of word w1 and word w2 are referred to as the set of TruePositives, TP(w1, w2), which will be abbreviated to TP in the rest of this article:TP(w1, w2) = F(w1) ?
F(w2) (2)The precision of w1?s retrieval of w2?s features is the proportion of w1?s features thatare shared by both words, where each feature is weighted by its relative importanceaccording to w1:Padd(w1, w2) =?TP D(w1, c)?F(w1 ) D(w1, c)(3)The recall of w1?s retrieval of w2?s features is the proportion of w2?s features thatare shared by both words, where each feature is weighted by its relative importanceaccording to w2:Radd(w1, w2) =?TP D(w2, c)?F(w2 ) D(w2, c)(4)445Computational Linguistics Volume 31, Number 4Table 1Weight functions.Dtype(w, c) ={1 if P(c|w) > 00 otherwiseDtok(w, c) = P(c|w)Dmi(w, c) = I(w, c) = log(P(c, w)P(c)P(w))Dwmi(w, c) = P(c, w) ?
log(P(c, w)P(c)P(w))Dt(w, c) =P(c, w)?P(c)?P(w)?P(c, w)NDz(w, c) =P(c, w)?P(c)?P(w)?P(c).P(w)NDallr(w, c) = ?2 ?
(log L(F(w, c), F(w), F(c)N)?
log L(F(w, c), F(w), F(w, c)F(w)))Precision and recall both lie in the range [0,1] and are both equal to one wheneach word has exactly the same features.
It should also be noted that the recall ofw1?s retrieval of w2 is equal to the precision of w2?s retrieval of w1, i.e., Radd(w1, w2) =Padd(w2, w1).2.3 Weight FunctionsThe weight function plays two important roles.
First, it determines which co-occurrences of w1 and w2 are important enough to be considered part of their descrip-tion, or by analogy with document retrieval, which co-occurrences we want to retrievefor w2 and which co-occurrences we have retrieved using the description of w1.
It isthen used to weight contexts by their importance.
In the latter case, D(w1, c) tells us theretrieval process?s perceived relevance of co-occurrence type c, and D(w2, c) tells us theactual relevance of co-occurrence type c. The weight functions we have considered sofar are summarized in Table 1.
Each weight function can be used to define its own CRM,which we will now discuss in more detail.Additive type-based CRM (Dtype).
In this CRM, the precision of w1?s retrieval of w2 is theproportion of co-occurrence types occurring with w1 that also occur with w2, and therecall of w1?s retrieval of w2 is the proportion of verb co-occurrence types (or distinctverbs) occurring with w2 that also occur with w1.
In this case, the summed values of Dare always 1, and hence the expressions for precision and recall can be simplified:Paddtype(w1, w2) =?TP Dtype(w1, c)?F(w1 ) Dtype(w1, c)=|TP||F(w1)|(5)446Weeds and Weir Co-occurrence RetrievalRaddtype(w1, w2) =?TP Dtype(w2, c)?F(w2 ) Dtype(w2, c)=|TP||F(w2)|(6)Additive token-based CRM (Dtok).
In this CRM, the precision of w1?s retrieval of w2 is theproportion of co-occurrence tokens occurring with w1 that also occur with w2, and therecall of w1?s retrieval of w2 is the proportion of co-occurrence tokens occurring with w2that also occur with w1.
Hence, words have the same features as in the type-based CRM,but each feature is given a weight based on its probability of occurrence.
Since F(w) ={c : D(w, c) > 0} = {c : P(c|w) > 0}, it follows that?F(w) Dtok(w, c) = 1, and thereforethe expressions for precision and recall can be simplified:Paddtok (w1, w2) =?TP Dtok(w1, c)?F(w1 ) Dtok(w1, c)=?TPP(c, w1) (7)Raddtok (w1, w2) =?TP Dtok(w2, c)?F(w2) Dtok(w2, c)=?TPP(c, w2) (8)Additive MI-based CRM (Dmi).
Using pointwise mutual information (MI) (Church andHanks 1989) as the weight function means that a co-occurrence c is considered a featureof word n if the probability of their co-occurrence is greater than would be expected ifwords occurred independently.
In addition, more informative co-occurrences contributemore to the sums in the calculation of precision and recall and hence have more weight.Additive WMI-based CRM (Dwmi).
Weighted mutual information (WMI) (Fung andMcKeown 1997) has been proposed as an alternative to MI, particularly when MI mightlead to the over-association of low-frequency events.
In this function, the pointwiseMI is multiplied by the probability of the co-occurrence; hence, reducing the weightassigned to low-probability events.Additive t-test based CRM (Dt).
The t-test (Manning and Schu?tze 1999) is a standardstatistical test that has been proposed for collocation analysis.
It measures the (signed)difference between the observed probability of co-occurrence and the expected prob-ability of co-occurrence, as would be observed if words occurred independently.
Thedifference is divided by the standard deviation in the observed distribution.
Similarlyto MI, this score obviously gives more weight to co-occurrences that occur more thanwould be expected, and its use as the weight function results in any co-occurrences thatoccur less than would be expected being ignored.Additive z-test based CRM (Dz).
The z-test (Fontenelle et al 1994) is almost identicalto the t-test.
However, using the z-test, the (signed) difference between the observedprobability of co-occurrence and the expected probability of co-occurrence is dividedby the standard deviation in the expected distribution.Additive log-likelihood ratio based CRM (Dallr).
The log-likelihood ratio (Manning andSchu?tze 1999) considers the difference (as a log ratio) in probability of the observedfrequencies of co-occurrences and individual words occurring under the null hypoth-447Computational Linguistics Volume 31, Number 4esis, that words occur independently, and under the alternative hypothesis, that theydo not.H0 : P(c|w) = p = P(c|?w) (9)H1 : P(c|w) = p1= p2 = P(c|?n) (10)If f (w, c) is the frequency of w and c occurring together, f (w) is the total frequencyof w occurring in any context, f (c) is the total frequency of c occurring with anyword, and N is the grand total of co-occurrences, then the log-likelihood ratio canbe written:Log?
(w, c) = ?2.
log L(H0)L(H1)(11)= ?2.????????
?log L(f (w, c), f (w), f (c)N)+ log L(f (c) ?
f (w, c), N ?
f (w), f (c)N)?
log L(f (w, c), f (w), f (w, c)f (w))?
log L(f (c) ?
f (w, c), N ?
f (w), f (c)?
f (w, c)N?
f (w))?????????
(12)where L(k, n, x) = xk(1 ?
x)n?k (13)In our implementation (see Table 1), an approximation to this formula is used,which we term the ALLR weight function.
We use an approximation because the termsthat represent the probabilities of the other contexts (i.e., seeing f (c) ?
f (w, c) under eachhypothesis) tend towards ??
as N increases (since the probabilities tend towards zero).Since N is very large in our experiments (approximately 2,000,000), we found that usingthe full formula led to many weights being undefined.
Further, since in this case theprobability of seeing other contexts will be approximately equal under each hypothesis,it is a reasonable approximation to make.Another potential problem with using the log-likelihood ratio as the weight func-tion is that it is always positive, since the observed distribution is always moreprobable than the hypothesized distribution.
All of the other weight functions assigna zero or negative weight to co-occurrence types that do not occur with a givenword and thus these zero frequency co-occurrence types are never selected as features.This is advantageous in the computation of similarity, since computing the sumsover all co-occurrence types rather than just those co-occurring with at least one ofthe words is (1) very computationally expensive and (2) due to their vast number, theeffect of these zero frequency co-occurrence types tends to outweigh the effect ofthose co-occurrence types that have actually occurred.
Giving such weight to theseshared non-occurrences seems unintuitive and has been shown by Lee (1999) to beundesirable in the calculation of distributional similarity.
Hence, when using the448Weeds and Weir Co-occurrence RetrievalALLR as the weight function, we use the additional restriction that P(c, w) > 0 whenselecting features.2.4 Difference-Weighted ModelsIn additive models, no distinction is made between features that have occurred to thesame extent with each word and features that have occurred to different extents witheach word.
For example, if two words have the same features, they are consideredidentical, regardless of whether the feature occurs with the same probability with eachword or not.
Here, we define a type of model that allows us to capture the difference inthe extent to which each word has each feature.We do this by defining the similarity of two words with respect to an individualfeature, using the same principles that we use to define the similarity of two wordswith respect to all their features.
First, we define an extent function, E(w, c), which is theextent to which w1 goes with c and which may be, but is not necessarily, the same asthe weight function D(n, w).
Possible extent functions will be discussed in Section 2.5.Having defined this function, we can measure the precision and recall of individualfeatures.
The precision of an individual feature c retrieved by w1 is the extent to whichboth words go with c divided by the extent to which w1 goes with c. The recall of theretrieval of c by w1 is the extent to which both words go with c divided by the extent towhich w2 goes with c.P (w1, w2, c) =min(E(w1, c), E(w2, c))E(w1, c)(14)R(w2, w1, c) =min(E(w1, c), E(w2, c))E(w2, c)(15)Precision and recall of an individual feature, like precision and recall of a distrib-ution, lie in the range [0,1].
We can now redefine precision and recall of a distributionas follows:Pdw(w1, w2) =?TP D(w1, c) ?
P (w1, w2, c)?F(w1) D(w1, c)(16)Rdw(w1, w2) =?TP D(w2, c) ?
R(w1, w2, c)?F(w2 ) D(w2, c)(17)Using precision and recall of individual features as weights in the definitionsof precision and recall of a distribution captures the intuition that retrieval of aco-occurrence type is not a black-and-white matter.
Features that are shared to asimilar extent are considered more important in the calculation of distributionalsimilarity.449Computational Linguistics Volume 31, Number 4Table 2Extent functions.Etype(w, c) = P(c|w)Etok(w, c) = P(c|w)Emi(w, c) = I(w, c) = log(P(c, w)P(c)P(w))Ewmi(w, c) = P(c, w).
log(P(c, w)P(c)P(w))Et(w, c) =P(c, w)?P(c).P(w)?P(c, w)NEz(w, c) =P(c, w)?P(c).P(w)?P(c).P(w)NEallr(w, c) = ?2.
(log L(f (w, c), f (w), f (c)N)?
log L(f (w, c), f (w), f (w, c)f (w)))2.5 Extent FunctionsThe extent functions we have considered so far are summarized in Table 2.
Note thatin general, the extent function is the same as the weight function, which leads to astandard simplification of the expressions for precision and recall in the difference-weighted CRMs.
For example, in the difference-weighted MI-based model we get theexpressions:Pdwmi (w1, w2) =?TP I(w1, c) ?min(I(w1, c),I(w2, c))I(w1, c)?F(w1 ) I(w1, c)=?TP min(I(w1, c), I(w2, c))?F(w1 ) I(w1, c)(18)Rdwmi (w1, w2) =?TP I(w2, c) ?min(I(w2, c),I(w1, c))I(w2, c)?F(w2 ) I(w2, c)=?TP min(I(w2, c), I(w1, c))?F(w2 ) I(w2, c)(19)Similar expressions can be derived for the WMI-based CRM, the t-test based CRM, thez-test based CRM, and the ALLR-based CRM.
An interesting special case is the difference-weighted token-based CRM.
In this case, since?F(w) P(c|w) = 1, we derive the followingexpressions for precision and recall:Pdwtok (w1, w2) =?TP P(c|w1) ?min(P(c|w1), P(c|w2))P(c|w1)?F(w1 ) P(c|w1)=?TPmin(P(c|w1), P(c|w2)) (20)450Weeds and Weir Co-occurrence RetrievalRdwtok(w1, w2) =?TP P(c|w2) ?min(P(c|w2), P(c|w1))P(c|w2)?F(w2 ) P(c|w2)=?TPmin(P(c|w2), P(c|w1))= Pdwtok (w1, w2) (21)Note that although we have defined separate precision and recall functions, wehave arrived at the same expression for both in this model.
As a result, this model issymmetric.The only CRM in which we use a different extent and weight function is thedifference-weighted type-based CRM.
This is because there is no difference between typesand tokens for an individual feature; i.e., their retrieval is equivalent.
In this case, thefollowing expressions for precision and recall are derived:Pdwtype(w1, w2) =?TP Dtype(w1, c) ?
Ptype(w1, w2, c)?F(w1 ) Dtype(w1, c)=?TPmin(P(c|w1), P(c|w2))P(c|w1)|F(w1)|(22)Rdwtype(w1, w2) =?TP Dtype(w2, c) ?
Rtype(w1, w2, c)?F(w2 ) Dtype(w2, c)=?TPmin(P(c|w2), P(c|w1))P(c|w2)|F(w2)|(23)Note that this is different from the additive token-based model because, althoughevery token is effectively considered in this model, tokens are not weighted equally.In this model, tokens are treated differently according to which type they belong.The importance of the retrieval (or non-retrieval) of a single token depends on theproportion of the tokens for its particular type that it constitutes.2.6 Combining Precision and RecallWe have, so far, been concerned with defining a pair of numbers that represents thesimilarity between two words.
However, in applications, it is normally necessary tocompute a single number in order to determine neighborhood or cluster membership.The classic way to combine precision and recall in IR is to compute the F-score; that is,the harmonic mean of precision and recall:F = mh(P ,R) = 2 ?
P ?
RP +R (24)However, we do not wish to assume that a good substitute requires both highprecision and high recall of the target distribution.
It may be that, in some situations,the best word to use in place of another word is one that only retrieves correct co-occurrences (i.e., it is a high-precision neighbor) or it may be one that retrieves all ofthe required co-occurrences (i.e., it is a high-recall neighbor).
The other factor in eachcase may play only a secondary role or no role at all.We can retain generality and investigate whether high precision or high recall orhigh precision and high recall are required for high similarity by computing a weighted451Computational Linguistics Volume 31, Number 4Table 3Table of special values of ?
and ?.?
?
Special Case- 1 harmonic mean of precision and recall (F-score)?
0 weighted arithmetic mean of precision and recall1 0 precision0 0 recall0.5 0 unweighted arithmetic meanarithmetic mean of the harmonic mean and the weighted arithmetic mean of precisionand recall:4mh(P (w1, w2),R(w1, w2)) =2.P (w1, w2).R(w1, w2)P (w1, w2) +R(w1, w2)(25)ma(P (w1, w2),R(w1, w2)) = ?.P (w1, w2) + (1 ?
?
).R(w1, w2) (26)sim(w1, w2) = ?.mh(P (w1, w2),R(w1, w2))+ (1 ?
?
).ma(P (w1, w2),R(w1, w2)) (27)where both ?
and ?
lie in the range [0,1].
The resulting similarity, sim(w1, w2), will alsolie in the range [0,1] where 0 is low and 1 is high.
This formula can be used in combi-nation with any of the models for precision and recall outlined earlier.
Precision andrecall can be computed once for every pair of words (and every model) whereas sim-ilarity depends on the values of ?
and ?.
The flexibility allows us to investigate em-pirically the relative significance of the different terms and thus whether one (or more)might be omitted in future work.
Table 3 summarizes some special parameter settings.2.7 DiscussionWe have developed a framework based on the concept of co-occurrence retrieval(CR).
Within this framework we have defined a number of models (CRMs) thatallow us to systematically explore three questions about similarity.
First, is similaritybetween words necessarily a symmetric relationship, or can we gain an advantage byconsidering it as an asymmetric relationship?
Second, are some features inherentlymore salient than others?
Third, does the difference in extent to which each word takeseach feature matter?The CRMs and the parameter settings therein correspond to alternative possibil-ities.
First, a high-precision neighbor is not necessarily a high-recall neighbor (and,conversely, a high-recall neighbor is not necessarily a high-precision neighbor) andtherefore we are not constrained to a symmetric relationship of similarity between4 This is as opposed to using a standard weighted F-score (Manning and Schu?tze 1999), which uses just oneparameter ?
: F?
= PR?R+(1??
)P .
We did not use this weighting because we wished to investigate thedifferences between using an arithmetic mean and a harmonic mean.452Weeds and Weir Co-occurrence Retrievalwords.
Second, the use of different weight functions varies the relative importanceattached to features.
Finally, difference-weighted models contrast with additive modelsin considering the difference in extent to which each word takes each feature.3.
Data and Experimental TechniquesThe rest of this paper is concerned with evaluation of the proposed framework; first, bycomparing it to existing distributional similarity measures, and second, by evaluatingperformance on two tasks.
Throughout our empirical work, we use one data-set and oneneighbor set comparison technique, which we now discuss in advance of presenting anyof our actual experiments.3.1 DataThe data used for all our experimental work was noun-verb direct-object data extractedfrom the BNC by a Robust Accurate Statistical Parser (RASP) (Briscoe and Carroll 1995;Carroll and Briscoe 1996).
We constructed a list of nouns that occur in both our data setand WordNet ordered by their frequency in our corpus data.
Since we are interestedin the effects of word frequency on word similarity, we selected 1,000 high-frequencynouns and 1,000 low-frequency nouns.
The 1,000 high-frequency nouns were selectedas the nouns with frequency ranks of 1?1,000; this corresponds to a frequency range of[586,20871].
The low-frequency nouns were selected as the nouns with frequency ranksof 3,001?4,000; this corresponds to a frequency range of [72,121].For each target noun, 80% of the available data was randomly selected as trainingdata and the other 20% was set aside as test data.5 The training data was used tocompute similarity scores between all possible pairwise combinations of the 2,000 nounsand to provide (MLE) estimates of noun-verb co-occurrence probabilities in the pseudo-disambiguation task.
The test data provides unseen co-occurrences for the pseudo-disambiguation task.Although we only consider similarity between nouns based on co-occurrenceswith verbs in the direct-object position, the generality of the techniques proposedis not so restricted.
Any of the techniques can be applied to other parts of speech,other grammatical relations, and other types of context.
We restricted the scope of ourexperimental work solely for computational and evaluation reasons.
However, wecould have chosen to look at the similarity between verbs or between adjectives.6 Wechose nouns as a starting point since nouns tend to allow less sense extensions thanverbs and adjectives (Pustejovsky 1995).
Further, the noun hyponymy hierarchy inWordNet, which will be used as a pseudo-gold standard for comparison, is widelyrecognized in this area of research.Some previous work on distributional similarity between nouns has used only asingle grammatical relation (e.g., Lee 1999), whereas other work has considered multiplegrammatical relations (e.g., Lin 1998a).
We consider only a single grammatical relationbecause we believe that it is important to evaluate the usefulness of each grammaticalrelation in calculating similarity before deciding how to combine information from5 This results in a single 80:20 split of the complete data set, in which we are guaranteed that the originalrelative frequencies of the target nouns are maintained.6 The use of grammatical relations to model context precludes finding similarities between words ofdifferent parts of speech.
Since we are looking at similarity in terms of substitutability, we would notexpect to find a word of one part of speech substitutable for a word of another part of speech.453Computational Linguistics Volume 31, Number 4different relations.
In previous work (Weeds 2003), we found that considering thesubject relation as well as the direct-object relation did not improve performance ona pseudo-disambiguation task.Our last restriction was to only consider 2,000 of the approximately 35,000 nounsoccurring in the corpus.
This restriction was for computational efficiency and to avoidcomputing similarities based on the potentially unreliable descriptions of very low-frequency words.
However, since our evaluation is comparative, we do not expect ourresults to be affected by this or any of the other restrictions.3.2 Neighbor Set Comparison TechniqueIn several of our experiments, we measure the overlap between two different similaritymeasures.
We use a neighbor set comparison technique adapted from Lin (1997).In order to compare two neighbor sets of size k, we transform each neighbor setso that each neighbor is given a rank score of k ?
rank.
Potential neighbors not withina given rank distance k of the noun score zero.
This transformation is required sincescores computed on different scales are to be compared and because we wish to onlyconsider neighbors up to a certain rank distance.
The similarity between two neighborsets S and S?
is computed as the cosine of the rank score vectors:C(S, S?)
=?w?S?S?
s(w) ?
s?
(w)?ki=1 i2(28)where s(w) and s?
(w) are the rank scores of the words within each neighbor set S and S?respectively.In previous work (Weeds and Weir 2003b), having computed the similarity betweenneighbor sets for each noun according to each pair of measures under consideration, wecomputed the mean similarity across all high-frequency nouns and all low-frequencynouns.
However, since the use of the CR framework requires parameter optimization,here, we randomly select 60% of the nouns to form a development set and use theremaining 40% as a test set.
Thus, any parameters are optimized over the developmentset nouns and performance measured at these settings over the test set.4.
Alternative Distributional Similarity MeasuresIn this section, we consider related work on distributional similarity measures and theextent to which some of these measures can be simulated within the CR framework.However, there is a large body of work on distributional similarity measures; for amore extensive review, see Weeds (2003).
Here, we concentrate on a number of morepopular measures: the Dice Coefficient, Jaccard?s Coefficient, the L1 Norm, the ?-skewdivergence measure, Hindle?s measure, and Lin?s MI-based measure.4.1 The Dice CoefficientThe Dice Coefficient (Frakes and Baeza-Yates 1992) is a popular combinatorial similaritymeasure adopted from the field of Information Retrieval for use as a measure of lexical454Weeds and Weir Co-occurrence Retrievaldistributional similarity.
It is computed as twice the ratio between the size of the inter-section of the two feature sets and the sum of the sizes of the individual feature sets:simdice(w1, w2) =2 ?
|F(w1) ?
F(w2)||F(w1)|+ |F(w2)|where F(w) = {c : P(c|w) > 0} (29)According to this measure, the similarity between words with no shared featuresis zero and the similarity between words with identical feature sets is 1.
However, asshown below, this formula is equivalent to a special case in the CR framework: theharmonic mean of precision and recall (or F-score) using the additive type-based CRM.mh(Paddtype(w1, w2),Raddtype(w1, w2))=2 ?
Paddtype(w1, w2) ?
Raddtype(w1, w2)Paddtype(w1, w2) +Raddtype(w1, w2)=2 ?
|TP||F(w1 )| ?|TP||F(w2 )||TP||F(w1)| +|TP||F(w2 )|=2 ?
|TP| ?
|TP||TP| ?
|F(w1)|+ |TP| ?
|F(w2)|=2 ?
|TP||F(w1)|+ |F(w2)|=2 ?
|F(w1) ?
F(w2)||F(w1)|+ |F(w2)|= simdice(w1, w2) (30)Thus, when ?
is set to 1 in the additive type-based CRM, the Dice Coefficient isexactly replicated.4.2 Jaccard?s CoefficientJaccard?s Coefficient (Salton and McGill 1983), also known as the Tanimoto Coefficient(Resnik 1993), is another popular combinatorial similarity measure.
It can be defined asthe proportion of features belonging to either word that are shared by both words; thatis, the ratio between the size of the intersection of the feature sets and the size of theunion of feature sets:simjacc(w1, w2) =|F(w1) ?
F(w2)||F(w1) ?
F(w2)|(31)As with the Dice Coefficient, the similarity between words with no shared co-occurrences is zero and the similarity between words with identical features is 1.
Fur-ther, as shown by van Rijsbergen (1979), the Dice Coefficient and Jaccard?s Coefficientare monotonic in one another.
Thus, although in general the scores computed by eachwill be different, the orderings or rankings of objects will be the same.
In other words,for all k and w, the k nearest neighbors of word w according to Jaccard?s Coefficient willbe identical to the k nearest neighbors of word w according to the Dice Coefficient andthe harmonic mean of precision and recall in the additive type-based CRM.455Computational Linguistics Volume 31, Number 44.3 The L1 NormThe L1 Norm (Kaufman and Rousseeuw 1990) is a member of a family of measuresknown as the Minkowski Distance, for measuring the distance7 between two pointsin space.
The L1 Norm is also known as the Manhattan Distance, the taxi-cab distance,the city-block distance, and the absolute value distance, since it represents the distancetraveled between the two points if you can only travel in orthogonal directions.
Whenused to calculate lexical distributional similarity, the dimensions of the vector space areco-occurrence types and the values of the vector components are the probabilities ofthe co-occurrence types given the word.
Thus the L1 distance between two words, w1and w2, can be written as:distL1 (w1, w2) =?c|P(c|w1) ?
P(c|w2)| (32)However, noting the algebraic equivalence A + B ?
|A ?
B| ?
2 ?
min(A, B) and us-ing basic probability theory, we can rewrite the L1 Norm as follows:distL1 (w1, w2) =?c|P(c|w1) ?
P(c|w2)|=?cP(c|w1) + P(c|w2) ?
2 ?
min(P(c|w1), P(c|w2))=?cP(c|w1) +?cP(c|w2) ?
2 ?
?cmin(P(c|w1), P(c|w2))= 2 ?
2 ?
?cmin(P(c|w1), P(c|w2)) (33)However, min(P(c|w1), P(c|w2)) > 0 if and only if c ?
TP.
Hence:distL1 (w1, w2) = 2 ?
2 ?
?TPmin(P(c|w1), P(c|w2) = 2 ?
2 ?
simdwtok(w1, w2) (34)In other words, the L1 Norm is directly related to the difference-weighted token-based CRM.
The constant and multiplying factors are required, since the CRM definesa similarity in the range [0,1], whereas the L1 Norm defines a distance in the range [0,2](where 0 distance is equivalent to 1 on the similarity scale).4.4 The ?-skew Divergence MeasureThe ?-skew divergence measure (Lee 1999, 2001) is a popular approximation to theKullback-Leibler divergence measure8 (Kullback and Leibler 1951; Cover and Thomas1991).
It is an approximation developed to be used when unreliable MLE probabilities7 Distance measures, also referred to as divergence and dissimilarity measures, can be viewed as theinverse of similarity measures; that is, an increase in distance correlates with a decrease in similarity.8 The Kullback-Leibler divergence measure is also often referred to as ?relative entropy.
?456Weeds and Weir Co-occurrence Retrievalwould result in the actual Kullback-Leibler divergence measure being equal to ?.
It isdefined (Lee 1999) as:dist?
(q, r) = D(r||?.q + (1 ?
?
).r) (35)for 0 ?
?
?
1, and where:D(p||q) =?xp(x) logp(x)q(x)(36)In effect, the q distribution is smoothed with the r distribution, which results in italways being non-zero when the r distribution is non-zero.
The parameter ?
controls theextent to which the measure approximates the Kullback-Leibler divergence measure.When ?
is close to 1, the approximation is close while avoiding the problem withzero probabilities associated with using the Kullback-Leibler divergence measure.
Thistheoretical justification for using a very high value of ?
(e.g., 0.99) is also borne out byempirical evidence (Lee 2001).The ?-skew divergence measure retains the asymmetry of the Kullback-Leiblerdivergence, and Weeds (2003) discusses the significance in the direction in which itis calculated.
For the purposes of this paper, we will find the neighbors of w2 byoptimizing:9dist?
(P(c|w1), P(c|w2)) (37)Due to the form of the ?-skew divergence measure, we do not expect any of theCRMs to exactly simulate it.
However, this measure does take into account the differ-ences between the probabilities of co-occurrences in each distribution (as a log ratio)and therefore we might expect that it will be fairly closely simulated by the difference-weighted token-based CRM.
Further, the ?-skew divergence measure is asymmetric.dist?
(w1, w2) measures the cost of using the distribution of w1 instead of w2 and iscalculated over the verbs that occur with w2.
As such, we might expect that dist?
will bea high-recall measure, since recall is calculated over the co-occurrences of w2.In order to determine how close any approximation is in practice, we comparedthe 200 nearest neighbors according to dist?
and different parameter settings withinthe CR framework for 1,000 high-frequency nouns and for 1,000 low-frequency nouns,using the data and the neighbor set comparison technique described in Section 3.
Table 4shows the optimal parameters in each CRM for simulating dist?, computed over thedevelopment set, and the mean similarity at these settings over both the developmentset and the test set.
From these results, we can make the following observations.First, the differences in mean similarities over the development set and the testset are minimal.
Thus, performance of the models with respect to different parametersettings appears stable across different words.Second, the differences between the models are fairly small.
The difference-weighted token-based CRM achieves a fairly close approximation to dist?, but the9 This is what Weeds (2003) refers to as dist?1.457Computational Linguistics Volume 31, Number 4Table 4Optimized similarities between CRMs and dist?
and corresponding parameter settings.Target Noun Frequencyhigh lowOptimal Devel.
Test Optimal Devel.
TestParameters Sim.
Sim.
Parameters Sim.
Sim.CRM ?
?
?
?simaddtype 0.25 0.4 0.74 0.75 0.5 0.3 0.66 0.66simaddtok 0.5 0.0 0.77 0.78 0.5 0.0 0.67 0.66simaddmi 0.0 0.0 0.76 0.77 0.25 0.1 0.72 0.73simaddwmi 0.25 0.0 0.71 0.72 0.25 0.1 0.79 0.79simaddt 0.5 0.0 0.84 0.85 0.5 0.2 0.71 0.71simaddz 0.5 0.0 0.79 0.80 0.5 0.1 0.63 0.63simaddallr 0.25 0.0 0.70 0.71 0.5 0.0 0.64 0.63simdwtype 0.0 0.25 0.70 0.71 0.0 0.0 0.52 0.53simdwtok ?
?
0.79 0.80 ?
?
0.58 0.58simdwmi 0.0 0.0 0.66 0.68 0.0 0.0 0.60 0.60simdwwmi 0.0 0.1 0.66 0.67 0.0 0.1 0.58 0.58simdwt 0.5 0.1 0.82 0.83 0.5 0.3 0.69 0.69simdwz 0.5 0.0 0.78 0.80 0.5 0.1 0.64 0.64simdwallr 0.75 0.0 0.53 0.54 0.75 0.0 0.48 0.48overall best approximation is achieved by the additive t-test based CRM.
Althoughnone of the CRMs are able to simulate dist?
exactly, the closeness of approximationachieved in the best cases (greater than 0.7) is substantially higher than the degreeof overlap observed between other measures of distributional similarity.
Weeds, Weir,and McCarthy (2004) report an average overlap of 0.4 between neighbor sets producedusing dist?
and Jaccard?s Measure and an average overlap of 0.48 between neighbor setsproduced using dist?
and Lin?s similarity measure.A third observation is that all of the asymmetric models get closest at highlevels of recall for both high- and low-frequency nouns.
For example, Figure 1 illustratesthe variation in mean similarity between neighbor sets with the parameters ?
and?
for the additive t-test based model.
As can be seen, similarity between neighborsets is significantly higher at high recall settings (low ?)
within the model than at high-precision settings (high ?
), which suggests that dist?
has high-recall CR characteristics.4.5 Hindle?s MeasureHindle (1990) proposed an MI-based measure, which he used to show that nouns couldbe reliably clustered based on their verb co-occurrences.
We consider the variant of458Weeds and Weir Co-occurrence RetrievalFigure 1Variation (with parameters ?
and ?)
in development set mean similarity between neighbor setsof the additive t-test based CRM and of dist?.Hindle?s Measure proposed by (Lin 1998a), which overcomes the problem associatedwith calculating MI for word-feature combinations that do not occur:simhind(w1, w2) =?T(w1)?T(w2)min(I(c, w1), I(c, w2)) (38)where T(w1) = {c : I(c, n) > 0}.
This expression is the same as the numerator in theexpressions for precision and recall in the difference-weighted MI-based CRM:Pdwmi (w1, w2) =?TP I(w1, c) ?min(I(w1, c),I(w2, c))I(w1, c)?F(w1) I(w1, c)=?TP min(I(w1, c), I(w2, c))?F(w1 ) I(w1, c)(39)Rdwmi (w1, w2) =?TP I(w2, c) ?min(I(w2, c),I(w1, c))I(w2, c)?F(w2 ) I(w2, c)=?TP min(I(w2, c), I(w1, c))?F(w2 ) I(w2, c)(40)since TP = T(w1) ?
T(w2).
However, we also note that the denominator in the expres-sion for recall depends only on w2, and therefore, for a given w2, is a constant.
Since w2 isthe target word, it will remain the same as we calculate each neighbor set.
Accordingly,the value of recall for each potential neighbor w1 of w2 will be the value of simhinddivided by a constant.
Hence, neighbor sets derived using simhind are identical to thoseobtained using recall (?
= 0,?
= 0) in the difference-weighted MI-based CRM.4.6 Lin?s MeasureLin (1998a) proposed a measure of lexical distributional similarity based on hisinformation-theoretic similarity theorem (Lin 1997, 1998b):The similarity between A and B is measured by the ratio between the amount ofinformation needed to state the commonality of A and B and the information needed tofully describe what A and B are.459Computational Linguistics Volume 31, Number 4If the features of a word are grammatical relation contexts, the similarity betweentwo words w1 and w2 can be written according to Lin?s measure as:simlin(w1, w2) =?T(w1)?T(w2)(I(w1, c) + I(w2, c))?T(w1) I(w1, c) +?T(w2 ) I(w2, c)(41)where T(w) = {c : I(w, c) > 0}.
There are parallels between simlin and simdice in that bothmeasures compute a ratio between what is shared by the descriptions of both nouns andthe sum of the descriptions of each noun.
The major difference appears to be the use ofMI, and hence we predicted that there would be a close relationship between simlin andthe harmonic mean in the additive MI-based CRM.
This relationship is shown below:mh(Paddmi (w1, w2),Raddmi (w1, w2)) =2 ?
Paddmi (w1, w2) ?
Raddmi (w1, w2)Paddmi (w1, w2) +Raddmi (w1, w2)(42)=2 ?
?TP I(w1, c)?F(w1)I(w1, c)?
?TP I(w2, c)?F(w2)I(w2, c)?TP I(w1, c)?F(w1)I(w1, c)+?TP I(w2, c)?F(w2)I(w2, c)(43)=2 ?
?TP I(w1, c) ?
?TP I(w2, c)?TP I(w2, c) ?
?F(w1 ) I(w1, c) +?TP I(w1, c) ?
?F(w2 ) I(w2, c)(44)Now if?TP I(w1, c) =?TP I(w2, c), it follows:mh(Paddmi (w1, w2),Raddmi (w1, w2)) =2 ?
?TP I(w1, c)?F(w1 ) I(w1, c) +?F(w2) I(w2, c)(45)=?TP I(w1, c) + I(w2, c)?F(w1 ) I(w1, c) +?F(w2 ) I(w2, c)(46)=?T(w1)?T(w2 )(I(w1, c) + I(w2, c))?T(w1 ) I(w1, c) +?T(w2) I(w2, c)since T(w) = F(w) (47)= simlin(w1, w2) (48)Thus, when the additive MI-based model is used, ?
= 1 and the condition?TP I(w1, c) =?TP I(w2, c) holds, the CR framework reduces to simlin.
However, thislast necessary condition for equivalence is not one we can expect to hold for many (ifany) pairs of words.
In order to investigate how good an approximation the harmonicmean is to simlin in practice, we compared neighbor sets according to each measureusing the neighbor set comparison technique outlined earlier.Figure 2 illustrates the variation in mean similarity between neighbor sets with theparameters ?
and ?.
At ?
= 1, the average similarity between neighbor rankings was0.967 for high-frequency nouns and 0.923 for low-frequency nouns.
This is significantlyhigher than similarities between other standard similarity measures.
However, theoptimal approximation of simlin was found using ?
= 0.75 and ?
= 0.5 in the additiveMI-based CRM.
With these settings, the development set similarity was 0.987 for high-460Weeds and Weir Co-occurrence Retrievalfrequency nouns and 0.977 for low-frequency nouns.
This suggests that simlin allowsmore compensation for lack of recall by precision and vice versa than the harmonicmean.4.7 DiscussionWe have seen that five of the existing lexical distributional similarity measures are (ap-proximately) equivalent to settings within the CR framework and for one other, a weakapproximation can be made.
The CR framework, however, more than simulates existingmeasures of distributional similarity.
It defines a space of distributional similarity mea-sures that is already populated with a few named measures.
By exploring the space, wecan discover the desirable characteristics of distributional similarity measures.
It may bethat the most useful measure within this space has already been discovered, or it maybe that a new optimal combination of characteristics is discovered.
The primary goal,however, is to understand how different characteristics relate to high performance indifferent applications and thus explain why one measure performs better than another.With this goal in mind, we now turn to the applications of distributional similarity.In the next section, we consider what characteristics of distributional similarity mea-sures are desirable in two different application areas: (1) automatic thesaurus generationand (2) language modeling.5.
Application-Based EvaluationAs discussed by Weeds (2003), evaluation is a major problem in this area of research.In some areas of natural language research, evaluation can be performed against agold standard or against human plausibility judgments.
The first of these approachesis taken by Curran and Moens (2002), who evaluate a number of different distributionalsimilarity measures and weight functions against a gold standard thesaurus compiledfrom Roget?s, the Macquarie thesaurus, and the Moby thesaurus.
However, we argue thatthis approach can only be considered when distributional similarity is required as anapproximation to semantic similarity and that, in any case, it is not ideal since it is notFigure 2Variation (with parameters ?
and ?)
in development set mean similarity between neighbor setsof the additive MI-based CRM and of simlin.461Computational Linguistics Volume 31, Number 4clear that there is a single ?right answer?
as to which words are most distributionallysimilar.
The best measure of distributional similarity will be the one that returns themost useful neighbors in the context of a particular application and thus leads to thebest performance in that application.
This section investigates whether the desirablecharacteristics of a lexical distributional similarity measure in an automatic thesaurusgeneration task (WordNet prediction) are the same as those in a language modeling task(pseudo-disambiguation).5.1 WordNet Prediction TaskIn this section, we evaluate the ability of distributional similarity measures to predictsemantic similarity by making comparisons with WordNet.
An underlying assumptionof this approach is that WordNet is a gold standard for semantic similarity, which, asis discussed by Weeds (2003), is unrealistic.
However, it seems reasonable to supposethat a distributional similarity measure that more closely predicts a semantic measurebased on WordNet is more likely to be a good predictor of semantic similarity.
We choseWordNet as our gold standard for semantic similarity since, as discussed by Kilgarriffand Yallop (2000), distributional similarity scores calculated over grammatical relationlevel context tend to be more similar to tighter thesauri, such as WordNet, than looserthesauri such as Roget?s.5.1.1 Experimental Set-Up.
There are a number of ways to measure the distance be-tween two nouns in the WordNet noun hierarchy (see Budanitsky [1999] for a review).In previous work (Weeds and Weir 2003b), we used the WordNet-based similaritymeasure first proposed in Lin (1997) and used in Lin (1998a):wn simlin(w1, w2) =maxc1?S(w1 )?c2?S(w2 )(maxc?sup(c1 )?sup(c2 )2 log P(c)log (P(c1)) + log (P(c2)))(49)where S(w) is the set of senses of the word w in WordNet, sup(c) is the set of possibly in-direct super-classes of concept c in WordNet, and P(c) is the probability that a randomlyselected word refers to an instance of concept c (estimated over some corpus such asSemCor [Miller et al 1994]).However, in other research (Budanitsky and Hirst 2001; Patwardhan, Banerjee, andPedersen 2003; McCarthy, Koeling, and Weeds 2004), it has been shown that the distancemeasure of Jiang and Conrath (1997) (referred to herein as the ?JC measure?)
is asuperior WordNet-based semantic similarity measure:wn distJC(w1, w2) =maxc1?S(w1 )?c2?S(w2 )(maxc?sup(c1 )?sup(c2 )2 log(c) ?
log P(c1) ?
log P(c2))(50)In our work, we make an empirical comparison of neighbors derived using aWordNet-based measure and each of the distributional similarity measures using thetechnique discussed in Section 3.
We have carried out the same experiments using boththe Lin measure and the JC measure.
Correlation between distributional similarity mea-sures and the WordNet measure tends to be slightly higher when using the JC measure462Weeds and Weir Co-occurrence RetrievalTable 5Optimized similarities between distributional neighbor sets and WordNet derived neighbor sets.Noun Frequencyhigh lowOptimal Devel Test Optimal Devel TestParameters Corr.
Corr.
Parameters Corr.
Corr.Measure ?
?
(C) (C) ?
?
(C) (C)simaddtype 0.25 0.5 0.323 0.327 0.5 0.25 0.281 0.275simaddtok 0.25 0.3 0.302 0.310 0.25 0.0 0.266 0.263simaddmi 0.25 0.2 0.334 0.342 0.25 0.2 0.290 0.283simaddwmi 0.25 0.2 0.282 0.293 0.25 0.0 0.274 0.266simaddt 0.5 0.2 0.330 0.338 0.5 0.2 0.292 0.286simaddz 0.5 0.1 0.324 0.332 0.5 0.1 0.280 0.276simaddallr 0.25 0.2 0.298 0.304 0.25 0.1 0.272 0.267simdwtype 0.0 0.4 0.306 0.310 0.0 0.0 0.221 0.219simdwtok ?
?
0.285 0.294 ?
?
0.212 0.211simdwmi 0.0 0.2 0.324 0.333 0.0 0.0 0.266 0.261simdwwmi 0.0 0.1 0.273 0.281 0.0 0.1 0.223 0.220simdwt 0.5 0.2 0.328 0.333 0.5 0.3 0.289 0.282simdwz 0.5 0.1 0.324 0.329 0.5 0.2 0.280 0.276simdwallr 0.75 0.2 0.263 0.265 0.75 0.0 0.226 0.225simdice 0.295 0.299 0.123 0.123simjacc 0.295 0.299 0.123 0.123distL1 0.285 0.294 0.212 0.211dist?
0.310 0.317 0.289 0.281simhind 0.320 0.326 0.267 0.261simlin 0.313 0.323 0.192 0.186wnsimlin 0.907 0.907 0.884 0.883(percentage increase in similarity of approximately 10%), but the relative differencesbetween distributional similarity measures remain approximately the same.
Here, forbrevity, we present results just using the JC measure.5.1.2 Results.
As before, we present the results separately for the 1,000 high-frequencytarget nouns and for the 1,000 low-frequency target nouns.
Table 5 shows the optimalparameter settings for each CRM (computed over the development set) and the meansimilarities with the JC measure at these settings in both the development set and thetest set.
It also shows the mean similarities over the development set and the test setfor each of the existing similarity measures discussed in Section 4.
For reference, wealso present the mean similarity for the WordNet-based measure wn simlin.
For ease463Computational Linguistics Volume 31, Number 4Figure 3Bar chart illustrating test set similarity with WordNet for each distributional similarity measure.of comparison, the test set correlation values for each distributional measure are alsoillustrated in Figure 3.We would expect a mean overlap score of 0.08 by chance.
Standard deviations inthe observed test set mean similarities were all less than 0.1, and thus any differencebetween mean scores of greater than 0.016 is significant at the 99% level, and differencesgreater than 0.007 are significant at the 90% level.
Thus, from the results in Table 5 wecan make the following observations.First, the best-performing distributional similarity measures, in terms of WordNetprediction, for both high- and low-frequency nouns, are the MI-based and the t-testbased CRMs.
The additive MI-based CRM performs the best for high-frequency nounsand the additive t-test based CRM performs the best for low-frequency nouns.
How-ever, the differences between these models are not statistically significant.
These CRMsperform substantially better than all of the unparameterized distributional similaritymeasures, of which the best performing are simhind and simlin for high-frequency nounsand dist?1 for low-frequency nouns.
Second, the difference-weighted versions of eachmodel generally perform slightly worse than their additive counterparts.
Thus, thedifference in extent to which each word occurs in each context does not appear to bea factor in determining semantic similarity.
Third, all of the measures perform signifi-cantly better for high-frequency nouns than for low-frequency nouns.
However, some ofthe measures (simlin, simjacc and simdice) perform considerably worse for low-frequencynouns.We now consider the effects of ?
and ?
in the CRMs on performance.
The patternof variation across the CRMs was very similar.
This pattern is illustrated using one ofthe best-performing CRMs (simaddmi ) in Figure 4.
With reference to this figure and to theresults for the other models (not shown), we make the following observations.464Weeds and Weir Co-occurrence RetrievalFigure 4Variation in similarity with WordNet with respect to ?
and ?
for the additive MI-based CRM.First, for high- and low-frequency nouns, similarity with WordNet is higher forlow values of ?
than for high values of ?.
In other words, neighbors according tothe WordNet based measure tend to have high-recall retrieval of the target noun?sco-occurrences.
Second, a high value of ?
leads to high performance for high-frequencynouns but poor performance for low-frequency nouns.
This suggests that WordNet-derived neighbors of high-frequency target nouns also have high-precision retrievalof the target noun?s co-occurrences, whereas the WordNet-derived neighbors of low-frequency target nouns do not.
This also explains why particular existing measures(Jaccard?s / the Dice Coefficient and Lin?s Measure), which are very similar to a ?
= 1setting in the CR framework, perform well for high-frequency nouns but poorly forlow-frequency nouns.5.1.3 Discussion.
Our results in this section are comparable to those of Curran andMoens (2002), who showed that combining the t-test with Jaccard?s coefficient outper-formed combining MI with Jaccard?s coefficient by approximately 10% in a comparisonagainst a gold-standard thesaurus.
However, we do not find a significant differencebetween using the t-test and MI in similarity calculation.
Further, we found that usinga combination of precision and recall weighted towards recall performs substantiallybetter than using the harmonic mean (which is equivalent to Jaccard?s measure).
In ourexperiments, the development-set similarity using the harmonic mean in the additiveMI-based CRM was 0.312 for high-frequency nouns and 0.153 for low-frequency nouns,and the development-set similarity using the harmonic mean in the additive t-test basedCRM was 0.294 for high-frequency nouns and 0.129 for low-frequency nouns.5.2 Pseudo-Disambiguation TaskPseudo-disambiguation tasks have become a standard evaluation technique (Gale,Church, and Yarowsky 1992; Schu?tze 1992; Pereira, Tishby, and Lee 1993; Schu?tze 1998;Lee 1999; Dagan, Lee, and Pereira 1999; Golding and Roth 1999; Rooth et al 1999; Even-Zohar and Roth 2000; Lee 2001; Clark and Weir 2002) and, in the current setting, wemay use a noun?s neighbors to decide which of two co-occurrences is the most likely.Although pseudo-disambiguation is an artificial task, it has relevance in at least twoapplication areas.
First, by replacing occurrences of a particular word in a test suite with465Computational Linguistics Volume 31, Number 4a pair of words from which a technique must choose, we recreate a simplified versionof the word sense disambiguation task; that is, we choose between a fixed numberof homonyms based on local context.
The second is in language modeling where wewish to estimate the probabilities of co-occurrences of events but, due to the sparsedata problem, it is often the case that a possible co-occurrence has not been seen in thetraining data.5.2.1 Experimental Set-up.
A typical approach to performing pseudo-disambiguationis as follows.
A large set of noun-verb direct-object pairs is extracted from a corpus,of which a portion is used as test data and another portion is used as training data.The training data can be used to construct a language model and/or determine thedistributionally nearest neighbors of each noun.
Noun-verb pairs (n, v1) in the test dataare replaced with noun-verb-verb triples (n, v1, v2) and the task is to decide which of thetwo verbs is the most likely to take the noun as its direct object.
Performance is usuallymeasured as error rate.
We will now discuss the details of our own experimental set-up.As already discussed (Section 3), 80% of the noun-verb direct-object data extractedfrom the BNC for each of 2,000 nouns was used to compute the similarity betweennouns and is also used as the language model in the pseudo-disambiguation task, and20% of the data was set aside as test data, providing unseen co-occurrences for thispseudo-disambiguation task.In order to construct the test set from the test data, we took all10 of the test data setaside for each target noun and modified it as follows.
We converted each noun-verb pair(n, v1) in the test data into a noun-verb-verb triple (n, v1, v2).
v2 was randomly selectedfrom the verbs that have the same frequency, calculated over all the training data, asv1 plus or minus 1.
If there are no other verbs within this frequency range, then thetest instance is discarded.
This method ensures that there is no systematic bias towardsv2 being of a higher or lower frequency than v1.
We also ensured that (n, v2) has notbeen seen in the test or training data.
Ten test instances11 were then selected for eachtarget noun in a two-step process of (1) while more than ten triples remained, discardingduplicate triples and (2) randomly selecting ten triples from those remaining afterstep 1.
At this point, we have 10,000 test instances pertaining to high-frequency nounsand 10,000 test instances pertaining to low-frequency nouns, and there are no biasestowards the higher-frequency or lower-frequency nouns within these sets.
Each of thesesets was split into five disjoint subsets, each containing two instances for each targetnoun.
We use these five subsets in two ways.
First, we perform five-fold cross validation.In five-fold cross validation, we compute the optimal parameter settings in four of thesubsets and the error rate at this optimal parameter setting in the remaining subset.
Thisis repeated five times with a different subset held out each time.
We then compute anaverage optimal error rate.
We cannot, however, compute an average optimal parametersetting, since this would assume a convex relationship between parameter settings anderror rate.
In order to study the relationship between parameter settings and errorrate, we combine three of the sets to form a development set and two of the sets toform a test set.
The development set is used to optimize parameters and the test set10 Unlike Lee (1999), we do not delete instances from the test data that occur in the training data.
Thisis discussed in detail in (Weeds 2003), but our main justification for this approach is that a singleco-occurrence of (n, v1) compared to zero co-occurrences of (n, v2) is not necessarily sufficient evidenceto conclude that the population probability of (n, v1) is greater than that of (n, v2).11 Ten being less than the minimum number (14) of (possibly) indistinct co-occurrences for any target nounin the original test data.466Weeds and Weir Co-occurrence Retrievalto determine error rates at the optimal settings.
In graphs showing the relationshipbetween error rate and parameter settings, it is the error rate in this development setthat is shown.
In the case of the CRMs, the parameters that are optimized are ?, ?, andk (the number of nearest neighbors).12 For the existing measures, the only parameterto be optimized is k.Having constructed the test sets, the task is to take each test instance (n, v1, v2) anduse the nearest neighbors of noun n (as computed from the training data) to decidewhich of (n, v1) and (n, v2) was the original co-occurrence.
Each of n?s neighbors, m, isgiven a vote that is equal to the difference in frequencies of the co-occurrences (m, v1)and (m, v2) and that it casts to the verb with which it occurs most frequently.
Thus,we distinguish between cases where a neighbor occurs with each verb approximatelythe same number of times and where a neighbor occurs with one verb significantlymore often than the other.
The votes for each verb are summed over all of the k nearestneighbors of n, and the verb with the most votes wins.
Performance is measured aserror rate.Error rate = 1T(# of incorrect choices + # of ties2)(51)where T is the number of test instances and a tie results when the neighbors cannotdecide between the two alternatives.5.2.2 Results.
In this section, we present results on the pseudo-disambiguation task forall of the CRMs described in Section 2.
We also compare the results with the six existingdistributional similarity measures (Section 4) and the two WordNet-based measures(Section 5.1).A baseline for these experiments is the performance obtained by a technique thatbacks-off to the unigram probabilities of the verbs being disambiguated.
By constructionof the test set, this should be approximately 0.5.
The actual empirical figures are 0.553for the high-frequency noun test set and 0.586 for the low-frequency noun test set.
Thedeviation from 0.5 is due to the unigram probabilities of the verbs not being exactlyequal and to their being calculated over a larger data set than just the training datafor the 2,000 target nouns.
These baseline error-rates are also different from what isobserved when all 1,999 potential neighbors are considered.
In this case, we obtainan error rate of 0.6885 for the high-frequency noun test set and 0.6178 for the low-frequency noun test set.
These differences are due to the fact that the correct choiceverb, but not the incorrect choice verb, has occurred, possibly many times, with thetarget noun in the training data, but a noun is not considered as a potential neighbor ofitself.The results are summarized in Table 6.
The table gives the average optimal errorrates for each measure, and for high- and low-frequency nouns, calculated using five-fold cross validation.
For ease of comparison, the cross-validated average optimal errorrates are illustrated in Figure 5.
Standard deviation in the mean optimal error rate acrossthe five folds was always less than 0.15 and thus differences greater than 0.028 aresignificant at the 99% level and differences greater than 0.012 are significant at the 90%level.
From the results, we make the following observations.12 We also experimented with optimizing a similarity threshold t, but found that optimizing k gave betterresults (Weeds 2003).467Computational Linguistics Volume 31, Number 4Table 6Mean optimal error rates using five-fold cross-validation (when optimizing k, ?
and ?
).Noun Frequency Measure Noun FrequencyMeasure high low high lowsimaddtype 0.196 0.197 simdwtype 0.214 0.185simaddtok 0.219 0.241 simdwtok 0.234 0.202simaddmi 0.178 0.169 simdwmi 0.187 0.176simaddwmi 0.173 0.192 simdwwmi 0.171 0.192simaddt 0.154 0.172 simdwt 0.163 0.186simaddz 0.164 0.183 simdwz 0.167 0.193simaddallr 0.170 0.211 simdwallr 0.171 0.215simdice 0.215 0.204 simjacc 0.215 0.204distL1 0.234 0.202 dist?1 0.230 0.192simhind 0.201 0.18 simlin 0.193 0.181wn simlin 0.295 0.294 wn distJC 0.302 0.295baseline 0.553 0.586First, the best measure appears to be the additive t-test based CRM.
This signifi-cantly outperforms all but one (the z-test based CRM) of the other measures for high-frequency nouns.
For low-frequency nouns, slightly higher performance is obtainedusing the additive MI-based CRM.
This difference, however, is not statistically signifi-cant.
Second, all of the distributional similarity measures perform considerably betterthan the WordNet-based measures13 at this task for high- and low-frequency nouns.Third, for many measures, performance over high-frequency nouns is not significantlyhigher (and is in some cases lower) than over low-frequency nouns.
This suggests thatdistributional similarity can be used in language modeling even when there is relativelylittle corpus data over which to calculate distributional similarity.We now consider the effects of the different parameters on performance.
Since weuse the development set to determine the optimal parameters, we consider performanceon the development set as each parameter is varied.
Table 7 shows the optimized pa-rameter settings in the development set, error rate at these settings in the developmentset, and error rate at these settings in the test set.
For the CRMs, we considered how theperformance varies with each parameter when the other parameters are held constantat their optimum values.
Figure 6 shows how performance varies with ?, and Figure 7shows how performance varies with ?
for the additive and difference-weighted t-testbased and MI-based CRMs.
For reference, the optimal error rates for the best performingexisting distributional similarity measure (simlin) is also shown as a straight line on eachgraph.We do not show the variation with respect to k for any of the measures, but this wasfairly similar for all measures and is as would be expected.
To begin with, considering13 However, for this task, in contrast to earlier work, wn simlin gives slightly, although insignificantly, betterperformance than wn distJC.468Weeds and Weir Co-occurrence RetrievalFigure 5Bar chart illustrating cross-validated optimal error rates for each measure when k is optimised.more neighbors increases performance, since more neighbors allow decisions to bemade in a greater number of cases.
However, when k increases beyond an optimalvalue, a greater number of these decisions will be in the wrong direction, since thesewords are not very similar to the target word, leading to a decrease in performance.
In asmall number of cases (when using the ALLR-based CRMs or the WMI-based CRMs forhigh frequency nouns), performance peaks at k = 1.
This suggests that these measuresmay be very good at finding a few very close neighbors.The majority of models, including the additive t-test based and additive MI-basedCRMs, perform significantly better at low values of ?
(0.25-0.5) and high values of ?
(around 0.8).
This indicates that a potential neighbor with high-precision retrieval ofinformative features is more useful than one with high-recall retrieval.
In other words,it seems that it is better to sacrifice being able to make decisions on every test instancewith a small number of neighbors in favor of not having neighbors that predict incorrectverb co-occurrences.
This also suggests why we saw fairly low performance by the ?-skew divergence measure on this task, since it is closest to a high-recall setting in theadditive t-test based model.
The low values of ?
indicate that a combination of precisionand recall that is closer to a weighted arithmetic mean is generally better than one thatis closer to an unweighted harmonic mean.
However, this does not hold for the t-testbased CRMs for low-frequency nouns.
Here a higher value of ?
is optimal, indicatingthat, in this case, requiring both recall and precision results in high performance.6.
Conclusions and Future DirectionsOur main contribution is the development of a framework, first presented in a prelim-inary form in Weeds and Weir (2003b), that is based on the concept of lexical substi-469Computational Linguistics Volume 31, Number 4Table 7Summary of results on pseudo-disambiguation task when optimizing ?, ?
and k.Noun Frequencyhigh lowOptimal Devel.
Test Optimal Devel.
TestParameters Error Error Parameters Error ErrorMeasure ?
?
k ?
?
ksimaddtype 0.25 0.8 150 0.193 0.193 0.25 0.75 100 0.192 0.200simaddtok 0 0.8 250 0.211 0.224 0.5 0.1 130 0.234 0.233simaddmi 0.25 0.8 170 0.175 0.186 0.5 0.8 120 0.169 0.178simaddwmi 0.0 1.0 1 0.175 0.169 0.75 0.0 100 0.183 0.182simaddt 0.25 0.8 190 0.153 0.155 0.5 0.7 110 0.165 0.176simaddz 0.25 0.7 40 0.165 0.163 0.5 1.0 250 0.174 0.188simaddallr 0.0 0.9 1 0.170 0.169 0.25 0.6 90 0.204 0.210simdwtype 0 0.6 50 0.208 0.215 0.25 0.3 190 0.177 0.188simdwtok n/a n/a 60 0.227 0.234 n/a n/a 50 0.194 0.206simdwmi 0.25 0.8 100 0.181 0.193 0.5 0.7 160 0.172 0.173simdwwmi 0.0 0.0 1 0.172 0.170 0.25 0.1 450 0.183 0.190simdwt 0.5 0.8 120 0.156 0.165 0.75 0.6 250 0.179 0.187simdwz 0.5 0.7 50 0.166 0.171 0.75 0.9 400 0.187 0.199simdwallr 0.0 0.9 1 0.171 0.169 0.5 1.0 180 0.208 0.212simlin n/a n/a 50 0.190 0.199 n/a n/a 80 0.179 0.186tutability.
Here, we cast the problem of measuring distributional similarity as one ofco-occurrence retrieval (CR), for which we can measure precision and recall by analogywith the way they are measured in document retrieval.
This CR framework has thenallowed us to systematically explore various characteristics of distributional similaritymeasures.First, we asked whether lexical substitutability is necessarily symmetric.
To thisend, we have explored the merits of symmetry and asymmetry in a similarity measureby varying the relative importance attached to precision and recall.
We have seen thatas the distribution of word B moves away from being identical to that of word A, itssimilarity with A can decrease along one or both of two dimensions.
When B occurs incontexts that word A does not, precision is lost but B may remain a high-recall neighborof word A.
When B does not occur in contexts that A does, recall is lost but B may remaina high-precision neighbor of word A.
Through our experimental work, which is morethorough than that presented in Weeds and Weir (2003b), we have shown that the kindof neighbor preferred appears to depend on the application in hand.
High-precisionneighbors were more useful in the language modeling task of pseudo-disambiguationand high-recall neighbors were more highly correlated with WordNet-derived neighborsets.
Thus, similarity appears to be inherently asymmetric.
Further, it would seem470Weeds and Weir Co-occurrence RetrievalFigure 6Performance of CRMs with respect to ?
(at optimal values of k and ?
).unlikely that any single, unparameterized measure of distributional similarity wouldbe able to do better on both tasks.Second, we asked whether all contexts are equally important in the calculation ofdistributional similarity.
To this end, we have explored the way in which frequencyinformation is utilized using different co-occurrence retrieval models (CRMs).
Usingdifferent weight functions, we have investigated the relative importance of differentco-occurrence types.
In earlier work (Weeds and Weir 2003b), we saw that using MIto weight features gave improved performance on the two evaluation tasks over type-based or token-based CRMs.
Here, we have seen that further gains can be made by usingthe t-test as a weight function.
This leads to significant improvements on the pseudo-disambiguation task for all nouns and marginal improvements on the WordNet predic-tion task for low-frequency nouns.
To some extent, this supports the findings of Curranand Moens (2002), who investigated a number of weight functions for distributionalsimilarity and showed that the t-test performed better than a number of other weightfunctions including MI.Third, we asked whether it is necessary to consider the difference in extent to whicheach word appears in each context.
To this end, we have herein proposed difference-Figure 7Performance of CRMs with respect to ?
(at optimal values of k and ?
).471Computational Linguistics Volume 31, Number 4weighted versions of each model in which the similarity of two words in respectof an individual feature is defined using the same principles that we use to definethe similarity of two words in respect of all their features.
We have compared thesedifference-weighted CRMs to their additive counterparts and shown that difference-weighting does not seem to be a major factor and does not improve results when usingthe best-performing CRMs.Another important contribution of this work on co-occurrence retrieval is a betterunderstanding of existing distributional similarity measures.
By comparing existingmeasures with the CR framework, we can analyze their CR characteristics.
As discussedin Weeds and Weir (2003b), the Dice Coefficient and Jaccard?s Coefficient are exactlysimulated by ?
= 1 in the additive type-based model and Lin?s Measure is almostequivalent to the harmonic mean of precision and recall in the additive MI-basedmodel.
Here, we also show that the L1 Norm is exactly simulated by the (unparame-terized) difference-weighted token-based model, Hindle?s Measure is exactly simulatedby ?
= 0,?
= 0 in the additive MI-based model, and the ?-skew divergence measureis most similar to high-recall settings in the additive t-test based CRM.
Knowing thatLin?s Measure is almost equivalent to the harmonic mean of precision and recall inthe additive MI-based model explains why this measure does badly on the WordNetprediction task for low-frequency nouns.
We have seen that recall is more importantthan precision in the WordNet prediction task, whereas the nearest neighbors of a targetnoun according to Lin?s Measure have both high precision and high recall.
Conversely,knowing that the ?-skew divergence measure is most closely approximated by high-recall settings in the additive t-test based model explains why this measure performspoorly on the pseudo-disambiguation task, since we have seen that high precision isrequired for optimal performance on this task.Finally, our evaluation of measures has been performed over a set of 2,000 nouns,and we have shown that the performance of distributional similarity techniques forlow-frequency nouns is not significantly lower than for high-frequency nouns.
Thissuggests that distributional techniques might be used even when there is relatively littledata available.
In the distributional domain, this means that we can use probabilityestimation techniques for rare words with greater confidence.
In the semantic domain,we might be able to use distributional techniques to extend existing semantic resourcesto cover rare or new words or automatically generate domain-, genre-, or dialect-specificresources.There are a number of major directions in which this work can be extended.
First,although the set of CRMs defined here is more extensive than that defined in Weeds andWeir (2003b), it is still not exhaustive, and other models might be proposed.
Further,it would be interesting to combine CRMs with the feature reweighting scheme ofGeffet and Dagan (2004).
These authors compare distributional similarity scores withhuman judgments of semantic entailment and show that substantial (approximately10%) improvements over using Lin?s Measure can be achieved by first calculatingsimilarity using Lin?s Measure and then recalculating similarity using a relative featurefocus score, which indicates how many of a word?s nearest neighbors shared thatfeature.Second, there are other potential application-based tasks that could be used toevaluate CRMs and distributional similarity methods in general.
In particular, we seepotential for the use of distributional similarity methods in prepositional phrase attach-ment ambiguity resolution.
This task has been previously tackled using semantic classesto predict what is ultimately distributional information.
Accordingly, we believe that itshould be possible to do better using the CR framework.472Weeds and Weir Co-occurrence RetrievalFinally, in order to be able to truly rival manually generated thesauri, distri-butional techniques need to be able to distinguish between different semantic relationssuch as synonymy, antonymy, and hyponymy.
These are important linguistic distinc-tions, particularly in the semantic domain, since we are unlikely, say, to want to replacea word with its antonym.
Weeds, Weir, and McCarthy (2004) give preliminary results onthe the use of precision and recall to distinguish between hypernyms and hyponyms insets of distributionally related words.AcknowledgmentsThis research was supported by anEngineering and Physical Sciences ResearchCouncil (EPSRC) studentship to the firstauthor.
The authors would like to thank JohnCarroll, Mirella Lapata, Adam Kilgarriff, BillKeller, Steve Clark, James Curran, DarrenPearce, Diana McCarthy, and MarkMcLauchlan for helpful discussions andinsightful comments throughout the courseof the research.
We would also like to thankthe anonymous reviewers of this paper fortheir comments and suggestions.ReferencesBernard, John R. L., editor.
1990.
TheMacquarie Encyclopedic Thesaurus.
TheMacquarie Library, Sydney, Australia.Briscoe, Edward and John Carroll.
1995.Developing and evaluating a probabilisticLR parser of part-of-speech andpunctuation labels.
In Proceedingsof the 4th ACL/SIGDAT InternationalWorkshop on Parsing Technologies,pages 48?58, Cambridge, MA.Brown, Peter F., Vincent J. Della Pietra, PeterV.
deSouza, Jenifer C. Lai, and Robert L.Mercer.
1992.
Class-based n-gram modelsof natural language.
ComputationalLinguistics, 18(4):467?479.Budanitsky, Alexander.
1999.
Lexical SemanticRelatedness and its Application in NaturalLanguage Processing.
Ph.D. thesis,University of Toronto, Ontario.Budanitsky, Alexander and Graeme Hirst.2001.
Semantic distance in WordNet: Anexperimental, application-orientedevaluation of five measures.
In Proceedingsof the NAACL-01 Workshop on WordNetand Other Lexical Resources, Pittsburgh, PA.Caraballo, Sharon.
1999.
Automaticconstruction of a hypernym-labelled nounhierarchy from text.
In Proceedings of the37th Annual Meeting of the Association forComputational Linguistics (ACL-99),pages 120?126, College Park, MA.Carroll, John and Edward Briscoe.
1996.Apportioning development effort in aprobabilistic LR parsing system throughevaluation.
In Proceedings of the ACL/SIGDAT Conference on Empirical Methods inNatural Language Processing (EMNLP96),pages 92?100, Santa Cruz, CA.Church, Kenneth W., William Gale, PatrickHanks, Donald Hindle, and RosamundMoon.
1994.
Lexical substitutability.In B. T. S. Atkins and A. Zampolli, editors.Computational Approaches to the Lexicon.Oxford University Press, Oxford,pages 153?177.Church, Kenneth W. and Patrick Hanks.1989.
Word association norms, mutualinformation and lexicography.
InProceedings of the 27th Annual Conferenceof the Association for ComputationalLinguistics (ACL-89), pages 76?82,Vancouver, British Columbia.Clark, Stephen and David Weir.
2000.A class-based probabilistic approach tostructural disambiguation.
InProceedings of the 18th InternationalConference on Computational Linguistics(COLING-00), pages 194?200, Saarbrucken,Germany.Clark, Stephen and David Weir.
2002.Class-based probability estimation usinga semantic hierarchy.
ComputationalLinguistics, 28(2):187?206.Cover, T. M. and J.
A. Thomas.
1991.Elements of Information Theory.
Wiley,New York.Curran, James R. and Marc Moens.
2002.Improvements in automatic thesaurusextraction.
In Proceedings of theACL-SIGLEX Workshop on UnsupervisedLexical Acquisition, pages 59?67,Philadelphia, PA.Dagan, Ido, Lillian Lee, and FernandoPereira.
1999.
Similarity-based models ofword co-occurrence probabilities.
MachineLearning Journal, 34(1?3):43?69.Dagan, Ido, S. Marcus, and S. Markovitch.1993.
Contextual word similarity andestimation from sparse data.
In Proceedingsof the 35th Annual Meeting of the Associationfor Computational Linguistics (ACL-93),pages 56?63, Columbus, OH.Even-Zohar, Yair and Dan Roth.
2000.
Aclassification approach to word prediction.473Computational Linguistics Volume 31, Number 4In Proceedings of the 1st Conference of theNorth American Chapter of the Association forComputational Linguistics (NAACL-00),pages 124?131, Pittsburg, PA.Fellbaum, Christiane, editor.
1998.
WordNet:An Electronic Lexical Database.
MIT Press,Cambridge, MA.Firth, John Rupert.
1957.
A synopsis oflinguistic theory 1930-1955.
In Studies inLinguistic Analysis, pages 1?32, PhilogicalSociety, Oxford.
Reprinted in Palmer, F.(ed.
), 1968 Selected Papers of J. R. Firth,Longman, Harlow.Fontenelle, Thierry, Walter Bruls, LucThomas, Tom Vanallemeersch, and JacquesJansen.
1994.
DECIDE, MLAP-Project93-19, deliverable D-1a: a survey ofcollocation extraction tools.
Technicalreport, University of Liege, Belgium.Frakes, W. B. and R. Baeza-Yates, editors.1992.
Information Retrieval, Data Structuresand Algorithms.
Prentice Hall, New York.Fung, Pascale and Kathleen McKeown.
1997.A technical word- and term-translation aidusing noisy parallel corpora acrosslanguage groups.
Machine Translation,12(1?2):53?87.Gale, William, Kenneth W. Church, andDavid Yarowsky.
1992.
Work on statisticalmethods for word sense disambiguation.In Working notes of the AAAI symposium onprobabilistic approaches to natural language,pages 54?60, Menlo Park, CA.Geffet, Maayan and Ido Dagan.
2004.
Featurevector quality and distributional similarity.In Proceedings of the 20th InternationalConference on Computational Linguistics(COLING-04), pages 247?253, Geneva,Switzerland.Golding, Andrew R. and Dan Roth.
1999.
Awinnow-based approach to context-sensitive spelling correction.
MachineLearning, 34(1?3):182?190.Grefenstette, Gregory.
1994.
Corpus-derivedfirst-, second- and third-order wordaffinities.
In Proceedings of Euralex,pages 279?290, Amsterdam, Holland.Harris, Zelig S. 1968.
Mathematical Structuresof Language.
John Wiley, New York.Hindle, Donald.
1990.
Noun classificationfrom predicate-argument structures.
InProceedings of the 28th Annual Meeting of theAssociation for Computational Linguistics(ACL-1990), pages 268?275, Pittsburgh, PA.Hindle, Donald and Mats Rooth.
1993.Structural ambiguity and lexical relations.Computational Linguistics, 19(1):103?120.Jiang, Jay J. and David W. Conrath.
1997.Semantic similarity based on corpusstatistics and lexical taxonomy.
InProceedings of the International Conference onResearch in Computational Linguistics(ROCLING X), Taiwan.Kaufman, Leonard and Peter J. Rousseeuw.1990.
Finding Groups in Data: AnIntroduction to Cluster Analysis.
John Wiley,New York.Kilgarriff, Adam.
2003.
Thesauruses fornatural language processing.
In Proceedingsof the Joint Conference on Natural LanguageProcessing and Knowledge Engineering,pages 5?13, Beijing, China.Kilgarriff, Adam and Colin Yallop.
2000.What?s in a thesaurus.
In Second Conferenceon Language Resources and Evaluation(LREC-00), pages 1371?1379, Athens.Kullback, S. and R.A. Leibler.
1951.
Oninformation and sufficiency.
Annals ofMathematical Statistics, 22:79?86.Lee, Lillian.
1999.
Measures of distributionalsimilarity.
In Proceedings of the 37th AnnualMeeting of the Association for ComputationalLinguistics (ACL-1999), pages 23?32,College Park, MA.Lee, Lillian.
2001.
On the effectiveness of theskew divergence for statistical languageanalysis.
Artificial Intelligence and Statistics,pages 65?72.Li, Hang.
2002.
Word clustering anddisambiguation based on co-occurrencedata.
Natural Language Engineering,8(1):25?42.Li, Hang and Naoki Abe.
1998.
Generalizingcase frames using a thesaurus and theMDL principle.
Computational Linguistics,24(2):217?244.Lin, Dekang.
1997.
Using syntacticdependency as local context to resolveword sense ambiguity.
In Proceedings of the35th Annual Meeting of the Association forComputational Linguistics and 8th Conferenceof the European Chapter of the Association forComputational Linguistics (ACL-97),pages 64?71, Madrid, Spain.Lin, Dekang.
1998a.
Automatic retrievaland clustering of similar words.
InProceedings of the 36th Annual Meetingof the Association for ComputationalLinguistics and the 17th InternationalConference on Computational Linguistics(COLING-ACL ?98), pages 768?774,Montreal, Quebec.Lin, Dekang.
1998b.
An information-theoreticdefinition of similarity.
In Proceedings ofInternational Conference on MachineLearning, Madison, WI.Lin, Dekang, Shaojun Zhao, Lijuan Qin, andMing Zhou.
2003.
Identifying synonyms474Weeds and Weir Co-occurrence Retrievalamong distributionally similar words.
InIn Proceedings of the 18th International JointConference on Artificial Intelligence(IJCAI-03), pages 1492?1493.Manning, Christopher D. and HinrichSchu?tze.
1999.
Foundations of StatisticalNatural Language Processing.
MIT Press,Cambridge, MA.McCarthy, Diana, Rob Koeling, and JulieWeeds.
2004.
Ranking WordNet senses.Technical Report 569, Department ofInformatics, University of Sussex,Brighton.McCarthy, Diana, Rob Koeling, Julie Weeds,and John Carroll.
2004.
Findingpredominant word senses in untaggedtext.
In Proceedings of the 42nd AnnualMeeting of the Association for ComputationalLinguistics (ACL-04), pages 280?287,Barcelona, Spain.Miller, G., M. Chodorow, S. Landes, C. Leacock,and R. Thomas.
1994.
Using a semanticconcordance for sense identification.
InProceedings of the ARPA Human LanguageTechnology Workshop, Plainsboro, NJ.Pantel, Patrick and Dekang Lin.
2000.Word-for-word glossing of contextuallysimilar words.
In Proceedings of theConference on Applied Natural LanguageProcessing / 1st Meeting of the NorthAmerican Chapter of the Associationfor Computational Linguistics (ANLP-NAACL-00), pages 78?85, Seattle, WA.Patwardhan, Siddharth, Satanjeev Banerjee,and Ted Pedersen.
2003.
Using measures ofsemantic relatedness for word sensedisambiguation.
In Proceedings of the 4thInternational Conference on Intelligent TextProcessing and Computational Linguistics,pages 241?257, Mexico City.Pereira, Fernando, Naftali Tishby, and LillianLee.
1993.
Distributional clustering ofsimilar words.
In Proceedings of the 30thAnnual Meeting of the Association forComputational Linguistics (ACL-93),pages 183?190, Columbus, OH.Pustejovsky, James.
1995.
The generativelexicon.
MIT Press, Cambridge, MA.Resnik, Philip.
1993.
Selection andInformation: A Class-Based Approachto Lexical Relationships.
Ph.D. thesis,University of Pennsylvania,Philadelphia, PA.Roget, Peter.
1911.
Thesaurus of EnglishWords and Phrases.
Longmans, Green andCo., London.Rooth, Mats, Stefan Riezler, Detlef Prescher,Glenn Carroll, and Franz Beil.
1999.Inducing a semantically annotatedlexicon via EM-based clustering.
InProceedings of the 37th Annual Meeting of theAssociation for Computational Linguistics(ACL-99), pages 104?111, College Park,MA.Salton, Gerald and M. J. McGill.
1983.Introduction to Modern Information Retrieval.McGraw-Hill, New York.Schu?tze, Hinrich.
1992.
Dimensions ofmeaning.
In Proceedings of Conference onSupercomputing, pages 787?796,Minneapolis, MN.Schu?tze, Hinrich.
1998.
Automatic wordsense discrimination.
ComputationalLinguistics, 24(1):97?124.van Rijsbergen, C. J.
1979.
InformationRetrieval, second edition.
Butterworths,London.Weeds, Julie.
2003.
Measures and Applicationsof Lexical Distributional Similarity.
Ph.D.thesis, University of Sussex, Brighton.Weeds, Julie and David Weir.
2003a.Finding and evaluating sets of nearestneighbours.
In Proceedings of the 2ndInternational Conference on CorpusLinguistics, pages 879?888,Lancaster, UK.Weeds, Julie and David Weir.
2003b.
Ageneral framework for distributionalsimilarity.
In Proceedings of the Conference onEmpirical Methods in Natural LanguageProcessing (EMNLP-03), pages 81?88,Sapporo, Japan.Weeds, Julie, David Weir, and DianaMcCarthy.
2004.
Characterising measuresof lexical distributional similarity.
InProceedings of the 20th InternationalConference on Computational Linguistics(COLING-04), pages 1015?1021, Geneva,Switzerland.Xu, Jinxi and Bruce Croft.
1996.
Queryexpansion using local and globaldisambiguation.
In Proceedings of the 19thAnnual International ACM SIGIR Conferenceon Research and Development in InformationRetrieval (SIGIR-96), pages 4?11, Zurich,Switzerland.475
