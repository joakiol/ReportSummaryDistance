First Joint Conference on Lexical and Computational Semantics (*SEM), pages 617?623,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsSRIUBC: Simple Similarity Features for Semantic Textual SimilarityEric YehSRI InternationalMenlo Park, CA USAyeh@ai.sri.comEneko AgirreUniversity of the Basque CountryDonostia, Basque Countrye.agirre@ehu.esAbstractWe describe the systems submitted by SRI In-ternational and the University of the BasqueCountry for the Semantic Textual Similarity(STS) SemEval-2012 task.
Our systems fo-cused on using a simple set of features, fea-turing a mix of semantic similarity resources,lexical match heuristics, and part of speech(POS) information.
We also incorporate pre-cision focused scores over lexical and POS in-formation derived from the BLEU measure,and lexical and POS features computed oversplit-bigrams from the ROUGE-S measure.These were used to train support vector re-gressors over the pairs in the training data.From the three systems we submitted, two per-formed well in the overall ranking, with split-bigrams improving performance over pairsdrawn from the MSR Research Video De-scription Corpus.
Our third system maintainedthree separate regressors, each trained specif-ically for the STS dataset they were drawnfrom.
It used a multinomial classifier to pre-dict which dataset regressor would be most ap-propriate to score a given pair, and used it toscore that pair.
This system underperformed,primarily due to errors in the dataset predictor.1 IntroductionPrevious semantic similarity tasks, such as para-phrase identification or recognizing textual entail-ment, have focused on performing binary decisions.These problems are usually framed in terms of iden-tifying whether a pair of texts exhibit the neededsimilarity or entailment relationship or not.
In manycases, such as producing a ranking over similarityscores, a soft measure of similarity between a pairof texts would be more desirable.We contributed three systems for the 2012 Se-mantic Textual Similarity (STS) task (Agirre et al,2012).
These are:1.
System 1, which used a combination of seman-tic similarity, lexical similarity, and precisionfocused part-of-speech (POS) features.2.
System 2, which used features from System1, with the addition of skip-bigram featuresderived from the ROUGE-S (Lin, 2004) mea-sure.
POS variants of skip-bigrams were incor-porated as well.3.
System 3, used the features from above to firstclassify the dataset the pair was drawn from,and then applied regressors trained for thatdataset.Our systems characterize sentence pairs as featurevectors, populated by a variety of scorers that will bedescribed below.
During training, we used supportvector regression (SVR) to train regressors againstthese vectors and their associated similarity scores.The STS training data is divided into threedatasets, reflecting their origin: Microsoft ResearchParaphrase Corpus (MSRpar), MSR Research VideoDescription Corpus (MSRvid), and WMT2008 De-velopment dataset (SMTeuroparl).
We trained indi-vidual regressors for each of these datasets, and ap-plied them to their counterparts in the testing set.Both Systems 1 and 2 used the following types offeatures:6171.
Resource based word to word semantic similar-ities.2.
Cosine-based lexical similarity measure.3.
Bilingual Evaluation Understudy (BLEU) (Pa-pineni et al, 2002) lexical overlap.4.
Precision focused Part of Speech (POS) fea-tures.System 2 added the following features:1.
Lexically motivated skip-bigram overlap.2.
Precision focused skip-bigram POS features.One of the primary motivations for our the choiceof features was to use relatively simple and fast fea-tures, which can be scaled up to large datasets, givenappropriate caching and pre-generated lookups.
Asthe test phase included surprise datasets, whose ori-gin was not disclosed, we also trained a fourth modelusing all of the training data from all three datasets.Systems 1 and 2 employed this strategy for the sur-prise data.Since the statistics for each of the training datasetsvaried, directly pooling them together may not bethe best strategy when scoring the surprise data,whose origins were unknown.
To account for this,System 3 treated this as a gated regression problem,where pairs are considered to originate strictly fromone dataset, and to score using a model specificallytailored for that dataset.
We first trained regressorson each of the datasets separately.
Then we traineda classifier to predict which dataset a given pair islikeliest to have been drawn from, and then appliedthe matching trained regressor to obtain its score.This team included one of the organizers.
Wewant to stress that we took all measures to make ourparticipation on the same conditions as the rest ofparticipants.
In particular, the organizer did not al-low the other member of the team to access any dataor information which was not already available forthe rest of participants.For the rest of this system description, we firstoutline the scorers used to populate the feature vec-tors used for Systems 1 and 2.
We then describethe setup for performing the regression.
We followwith an explanation of our strategies for dealing withthe surprise data, including a description of System3.
We then summarize performance over the thedatasets, and discuss future avenues of investigation.2 Resource Based SimilarityOur system uses several resources for assessing theword to word similarity between a pair of sentences.In order to pool together the similarity scores for agiven pair, we employed the Semantic Matrix (Fer-nando and Stevenson, 2008) framework.
To gen-erate the scores, we used several resources, princi-pally those derived from the relation graph of Word-Net (Fellbaum, 1998), and those derived from distri-butional resources, namely Explicit Semantic Anal-ysis (Gabrilovich and Markovitch, 2009), and theDekang Lin Proximity-based Thesaurus 1.
We nowdescribe the Semantic Matrix method, and followwith descriptions of each of the resources used.2.1 Semantic MatrixThe Semantic Matrix is a method for pooling allof the pairwise similarity scores between the to-kens found in two input strings.
In order to scorethe similarity between a pair of strings s1 and s2we first identify all of the unique vocabulary wordsfrom these strings to derive their corresponding oc-currence vectors v1 and v2.
Each dimension ofthese vectors corresponds to a unique vocabularyword, and binary values were used, correspondingto whether that word was observed.
The similarityscore for pair, sim(s1, s2), is given by Formula 1.sim(s1, s2) =vT1 Wv2?v1?
?v2?
(1)with W being the symmetric matrix marking thesimilarity between pairs of words in the vocabulary.We note that this is similar to the Mahalanobis dis-tance, except adjusted to produce a similarity.
Forthis experiment, we normalized matrix entries so allvalues lay in the 0-1 range.As named entities and other words encounteredmay not appear in one or more of the resources used,we applied the identity to W. This is equivalent toadding a strict lexical match fallback on top of thesimilarity measure.1http://webdocs.cs.ualberta.ca/ lindek/downloads.htm618Per (Fernando and Stevenson, 2008), a filter wasapplied over the values of W. Any entries that fellbelow a given threshold value were flattened to zero,in order to prevent low scoring similarities fromoverwhelming the score.
From previous studies overMSRpar, we applied a threshold of 0.9.For our experiments, each of the word to wordsimilarity scorers described below were used to gen-erate a corresponding word similarity matrix W,with scores generated using the Semantic Matrix.2.2 WordNet SimilarityWe used several methods to obtain word to wordsimilarities from WordNet.
WordNet is a lexical-semantic resource that describes typed relationshipsbetween synsets, semantic categories a word maybelong to.
Similarity scoring methods identify thesynsets associated with a pair of words, and then usethis relationship graph to generate a score.The first set of scorers were generated from theLeacock-Chodorow, Lin, and Wu-Palmer measuresfrom the WordNet Similarity package (Pedersen etal., 2004).
For each of these measures, we averagedacross all of the possible synsets between a givenpair of words.Another scorer we used was Personalized PageR-ank (PPR) (Agirre et al, 2010), a topic sensitivevariant of the PageRank algorithm (Page et al,1999) that uses a random walk process to identifythe significant nodes of a graph given its link struc-ture.
We first derived a graph G from WordNet,treating synsets as the vertices and the relationshipsbetween synsets as the edges.
To obtain a signaturefor a given word, we apply topic sensitive PageRank(Haveliwala, 2002) over G, using the synsets asso-ciated with the word as the initial distribution.
Atconvergence, we convert the stationary distributioninto a vector.
The similarity between two words isthe cosine similarity between their vectors.2.3 Distributional ResourcesIn contrast with the structure based WordNet basedmethods, distributional methods use statistical prop-erties of corpora to derive similarity scores.
We gen-erated two scorers, one based on Explicit Seman-tic Analysis (ESA), and the other on the DekangLin Proximity-based Thesaurus.
For a given word,ESA generates a concept vector, where the con-cepts are Wikipedia articles, and the score measureshow closely associated that word is with the textualcontent of the article.
To score the similarity be-tween two words, we computed the cosine similar-ity of their concept vectors.
This method proved togive state-of-the-art performance on the WordSim-353 word pair relatedness dataset (Finkelstein et al,2002).The Lin Proximity-based Thesaurus identifiesthe neighborhood around words encountered in theReuters and Text Retrieval Conference (TREC).
Fora given word, the Thesaurus identifies the top 200words with the most similar neighborhoods, listingthe score based on these matches.
For our experi-ments, we treated these as feature vectors, with theintuition being similar words should share similarneighbors.
Again, the similarity score between twowords was scored using the cosine similarity of theirvectors.3 Cosine SimilarityAnother scorer we used was the cosine similarityover the lemmas found in the sentences in a pair.For generating the vectors used in the cosine simi-larity computation, we used the term frequency ofthe lemmas.4 BLEU FeaturesBLEU is a measure developed to automatically as-sess how closely sentences generated by machinetranslation systems match reference human gener-ated texts.
BLEU is a directional measurement, andworks on the assumption that the more lexically sim-ilar a system generated sentence is to a reference sen-tence, a human generated translation, the better thesystem sentence is.
This can also be seen as a stand-in for the semantic similarity of the pairs, as wasshown when BLEU was applied to the paraphraseidentification identification problem in (Finch et al,2005).The BLEU score for a given system sentence andreference sentence of order N is computed usingFormula 2.BLEU(sys, ref) = B ?
expN?n=11Nlog(pn) (2)619B is a brevity penalty used to prevent degeneratetranslations.
Given this has little bearing on our ex-periments, we set its value to 1 for our experiments.Following (Papineni et al, 2002), we give each ordern equal weight in the geometric mean.
The proba-bility of an order n-gram from the system sentencebeing found in the reference, pn, is given in Formula3.pn =?ngram?sys countsys?ref (ngram)?ngram?sys countsys(ngram)(3)countsys(ngram) is frequency of oc-currence for the given n-gram in the sys-tem sentence.
The numerator term iscomputed as countsys?ref (ngram) =min(countsys(ngram), countref (ngram)) wherecountref (ngram) is the frequency of occurrenceof that n-gram in the reference sentence.
Thisis equivalent to having each n-gram have a 1-1mapping with a matching n-gram in the reference(if any), and counting the number of mappings.As there is a risk of higher order system n-gramshaving no matches in the reference, we apply Lapla-cian smoothing to the n-gram counts.BLEU is considered to be a precision focusedmeasure, as it only measures how much of the sys-tem sentence matches a reference sentence.
Follow-ing (Finch et al, 2005), we obtain a modified BLEUscore for strings s1 and s2 of a pair by averaging theBLEU scores where each takes a turn as the systemsentence, as given in Formula 4.Score(s1, s2) =12BLEU(s1, s2) ?
BLEU(s2, s1)(4)For our experiments, we used BLEU scores of or-der N = 1..4, over n-grams formed over the sen-tence lemmas, and used these as features for charac-terizing a given pair.4.1 Precision Focused POS FeaturesFrom past experiments with paraphrase identifica-tion over the MSR Paraphrase Corpus, we havefound including POS information to be beneficial.To this capture this kind of information, we gen-erated precision focused POS features, which mea-sures the following between the sentences in a prob-lem pair:1.
The overlap in POS tags.2.
The mismatch in POS tags.We follow the formulation for POS vectors givenin (Finch et al, 2005).
For a given sentence pair,we identify the set of words whose lemmas werematched in both the system and reference sentences,Wmatch and those with no matches, Wmiss.
Usingthe directional notion of system and reference sen-tences from BLEU, for each word w ?Wmatch,POSMatch(t, sys, ref) =?w?Wmatchcountt(w)|sys|(5)where countt is 1 if wordw has the matching POStag, and 0 otherwise.
|sys| is the token count of thesystem sentence.
This is deemed to be precision-focused, as this computation is done over candidatesfound in the system sentence.To generate the score for missing POS tags, weperform a similar computation,POSMiss(t, sys, ref) =?w?Wmisscountt(w)|sys|(6)To score the POS match and misses between apair, we follow Formula 4 and average the scoresfor each POS tag, where the sentences in a givenpair swap positions as the system and reference sen-tences.5 Split-Bigram FeaturesSystem 2 added split-bigram features, which werederived from the ROUGE-S measure.
Like bigrams,split-bigrams consist of an ordered pair of distincttokens drawn from a source sentence.
Unlike bi-grams, split-bigrams allow for a number of inter-vening tokens to appear between the split-bigram to-kens.
For example, ?The cat ate fish.?
would gen-erate the following split-bigrams the?cat, the?ate,the?fish, cat?ate, cat?fish, and ate?fish.
The in-tent of split-bigrams is to quickly capture long range620dependencies, without requiring a parse of the sen-tence.Similar to ROUGE-S, we used lexical overlapof the split-bigrams as an approximation of seman-tic similarity.
As our pairs are bidirectional, weused the same framework (Formula 2) for obtain-ing BLEU scores to generate split-bigram overlapscores for our pairs.
Here, counts are obtained oversplit-bigrams found in the system and reference sen-tences, and the order was set to 1.For generating the skip-bigram overlap score fora pair, we used a maximum distance of three.5.1 Skip-Bigram POS FeaturesIn the same vein as the precision focused POSfeatures, we used the POS tags of matched split-bigrams as features, where the frequency of thePOS tags in split-bigrams, t ?
t?, were used.Here, Bmatch represents the split-bigrams whichwere found in both the system and reference sen-tences, matched on lexical content.SBMatch(t?
t?, sys, ref) =?b?Bmatchcountt?t?
(b)|sys|(7)Due to sparsity, we only considered scores fromsplit-bigram matches between the system and ref-erence sentences, and do not model split-bigrammisses.
As before, we generate scores for each split-bigram tag sequence by averaging the scores whereboth sentences in a pair have swapped positions.
Forour experiments, we only considered split-bigramPOS features of up to distance 3.
In our initial exper-iments we found split-bigram POS features helpedonly in the case of shorter sentence pairs, so we onlygenerated features if both the sentences in a givenpair contained ten tokens or less.6 Experimental SetupFor all three systems, we used the StanfordCoreNLP (Toutanova et al, 2003) package to per-form lemmatization and POS tagging of the in-put sentences.
For regressors, we used LibSVM?s(Chang and Lin, 2011) support vector regression ca-pability, using radial basis kernels.
Based off of tun-ing on the training set, we set ?
= 1 and the defaultDataset Mean Std.DevMSRpar 3.322 0.9294MSRvid 2.135 1.595SMTeur 4.307 0.7114Table 1: Means and standard deviations of similarityscores for each of the training datasets.slack value.From previous experience with paraphrase iden-tification over the MSR Paraphrase Corpus, we re-tained stop words in all of our experiments.7 Dealing with Surprise DataAs the STS training data was broken into three sep-arate datasets, each with their own distinct statistics,we developed three regressors trained individuallyon each of these datasets.
This presented a problemwhen dealing with surprise datasets, whose statisticswere not known.The approach taken by Systems 1 and 2 was sim-ply to pool together all three training datasets into asingle dataset and train a single regressor on that uni-fied model.
We then applied that regressor againstthe two surprise datasets, OnWN and SMTnews.Analysis of the similarity score statistics showedthat they varied greatly between each of the train-ing sets, as given in Table 1.
Thus combining thedatasets blindly, as with Systems 1 and 2, may proveto be a suboptimal strategy.
The approach taken bySystem 3 was to consider the feature vectors them-selves as capturing information about which datasetthey were drawn from, and to use a classifier to pre-dict that dataset.
We then emit the score from theregressor trained on just that matching dataset.
Weused the Stanford Classifier?s (Manning and Klein,2003) multinomial logistic regression as our datasetpredictor, using the feature vectors from System 2.Five-fold cross validation over the training datashowed the dataset predictor to have an overall ac-curacy of 91.75%.In order to assess performance over the knowndatasets at test time, System 3 also applied the samestrategy for the MSRpar, MSRvid, and SMTeuroparltest sets.621Sys All Allnorm Mean MSRpar MSRvid SMTeur OnWN SMTnews1 0.7513 / 11 0.8017 / 40 0.5997 / 22 0.6084 0.7458 0.4688 0.6315 0.39942 0.7562 / 10 0.8111 / 24 0.5858 / 33 0.6050 0.7939 0.4294 0.5871 0.33663 0.6876 / 21 0.7812 / 54 0.4668 / 68 0.4791 0.7901 0.2159 0.3843 0.2801Table 2: Pearson correlation of described systems against test data, by dataset.
Overall measures are All indicates thecombined Pearson, Allnorm the normalized variant, and Mean the macro average of Pearson correlations.
Rank forthe system in the overall measure is given after the slash.Guess/Gold MSRpar MSRvid SMTeurMSRpar 664 7 75MSRvid 7 737 10SMTeur 79 6 649Table 3: Confusion for the dataset predictor, used to pre-dict which dataset a pair was drawn from.
This wasddrawn using five-fold cross validation over the trainingset, with columns representing golds and guesses as rows.8 Results and DiscussionResults on the test data for each of the systemsagainst the individual datasets, are given in Table2, given in Pearson linear correlation with the goldstandard scores.
Overall measures for the systemsare given, along with their overall ranking.The split-bigram features in System 2 contributedprimarily to performance over the MSRvid dataset,while degrading performance on the other datasetsslightly.
This is likely a result of increasing spar-sity in the feature space, but overall this system per-formed well.
System 3 underperformed on mostdatasets, asides from its performance on MSRvid.The confusion generated over five-fold cross vali-dation over the training set is given in Table 3, andprecision, recall, and F1 scores by dataset label fromfive-fold cross validation over the training set aregiven in Table 4.
As these show, predictor errors layprimarily in confusing MSRpar for SMTeuroparl,and vice versa.
This error was significant enough toreduce performance on both the MSRpar and SM-Teuroparl test sets.
This proved to be enough to re-duce the scores between these two datasets.9 Conclusion and Future WorkOur STS systems have shown that relatively sim-ple syntax free methods can be employed to theSTS task.
Future avenues of investigation wouldDataset Prec Rec F1MSRpar 0.8901 0.8853 0.8877MSRvid 0.9775 0.9827 0.9801SMTeur 0.8842 0.8842 0.8842Table 4: Results on classifying pairs by source dataset,using five-fold cross validation over training data.be to include the use of syntactic information, inorder to obtain better predicate-argument informa-tion.
Syntactic information has proven useful forthe paraphrase identification task over MSRpar, asdemonstrated in studies such as (Das and Smith,2009) and (Socher et al, 2011).
Furthermore, aqualitative assessment of the pairs across differentdatasets showed relatively significant differences,which would strengthen the argument for develop-ing features and methods specific to each dataset.Another improvement would be to develop a bet-ter dataset predictor for System 3.
Also recognizingthere may be ways to normalize and rescale scoresacross datasets so the regression models used do nothave to account for differing means and standard de-viations.Finally, there are other bodies of source data thatmay be adapted for use with the STS task, such asthe paraphrasing pairs of the Recognizing TextualEntailment challenges, human generated referencetranslations for machine translation evaluation, andhuman generated summaries used for summariza-tion evaluations.
Although these are gold decisions,at the very least they could provide a source of highsimilarity pairs, from which one could manufacturelower scoring variants.AcknowledgmentsThe authors would like to thank the Semantic Tex-tual Similarity organizers for all of their hard work622and effort.One of the authors was supported by theIntelligence Advanced Research Projects Activ-ity (IARPA) via Air Force Research Laboratory(AFRL) contract number FA8650-10-C-7058.
TheU.S.
Government is authorized to reproduce and dis-tribute reprints for Governmental purposes notwith-standing any copyright annotation thereon.
Theviews and conclusions contained herein are thoseof the authors and should not be interpreted asnecessarily representing the official policies or en-dorsements, either expressed or implied, of IARPA,AFRL, or the U.S. Government.Eneko Agirre was partially funded by theEuropean Communitys Seventh Framework Pro-gramme (FP7/2007-2013) under grant agreementno.
270082 (PATHS project) and the Ministryof Economy under grant TIN2009-14715-C04-01(KNOW2 project)ReferencesEneko Agirre, Montse Cuadros, German Rigau, and AitorSoroa.
2010.
Exploring knowledge bases for similar-ity.
In Proceedings of the International Conference onLanguage Resources and Evaluation 2010.Eneko Agirre, Daniel Cer, Mona Diab, and AitorGonzalez-Agirre.
2012.
Semeval-2012 task 6: A piloton semantic textual similarity.
In Proceedings of the6th International Workshop on Semantic Evaluation(SemEval 2012), in conjunction with the First JointConference on Lexical and Computational Semantics(*SEM 2012).Chih-Chung Chang and Chih-Jen Lin.
2011.
LIBSVM:A library for support vector machines.
ACM Transac-tions on Intelligent Systems and Technology, 2:27:1?27:27.Dipanjan Das and Noah A. Smith.
2009.
Paraphraseidentification as probabilistic quasi-synchronousrecognition.
In In Proceedings of the Joint Confer-ence of the Annual Meeting of the Association forComputational Linguistics and the International JointConference on Natural Language Processing(ACL2009), pages 468?476, Singapore.Christine Fellbaum.
1998.
WordNet - An Electronic Lex-ical Database.
MIT Press.Samuel Fernando and Mark Stevenson.
2008.
A se-mantic similarity approach to paraphrase detection.
InComputational Linguistics UK (CLUK 2008) 11th An-nual Research Colloqium.Andrew Finch, Young-Sook Hwang, and Eiichio Sumita.2005.
Using machine translation evaluation tech-niques to determine sentence-level semantic equiva-lence.
In Proceedings of the Third International Work-shop on Paraphrasing (IWP 2005), pages 17?24, JejuIsland, South Korea.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and EytanRuppin.
2002.
Placing search in context: The conceptrevisited.
ACM Transactions on Information Systems,20(1):116?131.Evgeniy Gabrilovich and Shaul Markovitch.
2009.Wikipedia-based semantic interpretation.
Journal ofArtificial Intelligence Research, 34:443?498.Taher H. Haveliwala.
2002.
Topic-sensitive pagerank.In WWW ?02, pages 517?526, New York, NY, USA.ACM.Chin-Yew Lin.
2004.
Rouge: A package for automaticevaluation of summaries.
pages 74?81, Barcelona,Spain, jul.
Association for Computational Linguistics.Christopher Manning and Dan Klein.
2003.
Optimiza-tion, maxent models, and conditional estimation with-out magic.
In Proceedings of the 2003 Conferenceof the North American Chapter of the Association forComputational Linguistics on Human Language Tech-nology: Tutorials - Volume 5, NAACL-Tutorials ?03,pages 8?8, Stroudsburg, PA, USA.
Association forComputational Linguistics.Lawrence Page, Sergey Brin, Rajeev Motwani, and TerryWinograd.
1999.
The pagerank citation ranking:Bringing order to the web.
Technical Report 1999-66, Stanford InfoLab, November.
Previous number =SIDL-WP-1999-0120.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Meeting on Association for Computa-tional Linguistics, ACL ?02, pages 311?318, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Ted Pedersen, Siddharth Patwardhan, and Jason Miche-lizzi.
2004.
WordNet::Similarity - Measuring the Re-latedness of Concepts.
In Proceedings of the Nine-teenth National Conference on Artificial Intelligence(Intelligent Systems Demonstrations), pages 1024?1025, San Jose, CA, July.Richard Socher, Eric H. Huang, Jeffrey Pennington, An-drew Y. Ng, and Christopher D. Manning.
2011.
Dy-namic pooling and unfolding recursive autoencodersfor paraphrase detection.
In Advances in Neural Infor-mation Processing Systems 24.Kristina Toutanova, Dan Klein, Christopher D. Manning,and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In Pro-ceedings of HLT-NAACL 2003, pages 252?259.623
