Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2061?2069,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsRecursive Deep Models for Discourse ParsingJiwei Li1, Rumeng Li2and Eduard Hovy31Computer Science Department, Stanford University, Stanford, CA 94305, USA2School of EECS, Peking University, Beijing 100871, P.R.
China3Language Technology Institute, Carnegie Mellon University, Pittsburgh, PA 15213, USAjiweil@stanford.edu alicerumeng@foxmail.com ehovy@andrew.cmu.eduAbstractText-level discourse parsing remains achallenge: most approaches employ fea-tures that fail to capture the intentional, se-mantic, and syntactic aspects that governdiscourse coherence.
In this paper, we pro-pose a recursive model for discourse pars-ing that jointly models distributed repre-sentations for clauses, sentences, and en-tire discourses.
The learned representa-tions can to some extent learn the seman-tic and intentional import of words andlarger discourse units automatically,.
Theproposed framework obtains comparableperformance regarding standard discours-ing parsing evaluations when comparedagainst current state-of-art systems.1 IntroductionIn a coherent text, units (clauses, sentences, andlarger multi-clause groupings) are tightly con-nected semantically, syntactically, and logically.Mann and Thompson (1988) define a text to becoherent when it is possible to describe clearlythe role that each discourse unit (at any level ofgrouping) plays with respect to the whole.
In acoherent text, no unit is completely isolated.
Dis-course parsing tries to identify how the units areconnected with each other and thereby uncover thehierarchical structure of the text, from which mul-tiple NLP tasks can benefit, including text sum-marization (Louis et al., 2010), sentence compres-sion (Sporleder and Lapata, 2005) or question-answering (Verberne et al., 2007).Despite recent progress in automatic discoursesegmentation and sentence-level parsing (e.g.,(Fisher and Roark, 2007; Joty et al., 2012; Sori-cut and Marcu, 2003), document-level discourseparsing remains a significant challenge.
Recentattempts (e.g., (Hernault et al., 2010b; Feng andHirst, 2012; Joty et al., 2013)) are still consid-erably inferior when compared to human gold-standard discourse analysis.
The challenge stemsfrom the fact that compared with sentence-leveldependency parsing, the set of relations betweendiscourse units is less straightforward to define.Because there are no clause-level ?parts of dis-course?
analogous to word-level parts of speech,there is no discourse-level grammar analogous tosentence-level grammar.
To understand how dis-course units are connected, one has to understandthe communicative function of each unit, and therole it plays within the context that encapsulates it,taken recursively all the way up for the entire text.Manually developed features relating to words andother syntax-related cues, used in most of the re-cent prevailing approaches (e.g., (Feng and Hirst,2012; Hernault et al., 2010b)), are insufficient forcapturing such nested intentionality.Recently, deep learning architectures have beenapplied to various natural language processingtasks (for details see Section 2) and have shownthe advantages to capture the relevant semanticand syntactic aspects of units in context.
As worddistributions are composed to form the meaningsof clauses, the goal is to extend distributed clause-level representations to the single- and multi-sentence (discourse) levels, and produce the hier-archical structure of entire texts.Inspired by this idea, we introduce in this pa-per a deep learning approach for discourse pars-ing.
The proposed parsing algorithm relies ona recursive neural network to decide (1) whethertwo discourse units are connected and if so (2)by what relation they are connected.
Concretely,the parsing algorithm takes as input a document ofany length, and first obtains the distributed repre-sentation for each of its sentences using recursiveconvolution based on the sentence parse tree.
Itthen proceeds bottom-up, applying a binary clas-sifier to determine the probability of two adjacent2061discourse units being merged to form a new sub-tree followed by a multi-class classifier to selectthe appropriate discourse relation label, and cal-culates the distributed representation for the sub-tree so formed, gradually unifying subtrees un-til a single overall tree spans the entire sentence.The compositional distributed representation en-ables the parser to make accurate parsing decisionsand capture relations between different sentencesand units.
The binary and multi-class classifiers,along with parameters involved in convolution, arejointly trained from a collection of gold-standarddiscourse structures.The rest of this paper is organized as follows.We present related work in Section 2 and de-scribe the RST Discourse Treebank in Section 3.The sentence convolution approach is illustrated inSection 4 and the discourse parser model in Sec-tion 5.
We report experimental results in Section 6and conclude in Section 7.2 Related Work2.1 Discourse Analysis and ParsingThe basis of discourse structure lies in the recog-nition that discourse units (minimally, clauses) arerelated to one another in principled ways, and thatthe juxtaposition of two units creates a joint mean-ing larger than either unit?s meaning alone.
In acoherent text this juxtaposition is never random,but serves the speaker?s communicative goals.Considerable work on linguistic and computa-tional discourse processing in the 1970s and 80sled to the development of several proposals for re-lations that combine units; for a compilation see(Hovy and Maier, 1997).
Of these the most influ-ential is Rhetorical Structure Theory RST (Mannand Thompson, 1988) that defines about 25 rela-tions, each containing semantic constraints on itscomponent parts plus a description of the overallfunctional/semantic effect produced as a unit whenthe parts have been appropriately connected in thetext.
For example, the SOLUTIONHOOD relationconnects one unit describing a problem situationwith another describing its solution, using phrasessuch as ?the answer is?
; in successful communi-cation the reader will understand that a problem isdescribed and its solution is given.Since there is no syntactic definition of a prob-lem or solution (they can each be stated in a sin-gle clause, a paragraph, or an entire text), one hasto characterize discourse units by their commu-nicative (rhetorical) function.
The functions arereflected in text as signals of the author?s inten-tions, and take various forms (including expres-sions such as ?therefore?, ?for example?, ?the an-swer is?, and so on; patterns of tense or pronounusage; syntactic forms; etc.).
The signals governdiscourse blocks ranging from a clause to an en-tire text , each one associated with some discourserelation.In order to build a text?s hierarchical structure,a discourse parser needs to recognize these signalsand use them to appropriately compose the rela-tionship and nesting.
Early approaches (Marcu,2000a; LeThanh et al., 2004) rely mainly on overtdiscourse markers (or cue words) and use hand-coded rules to build text structure trees, bottom-upfrom clauses to sentences to paragraphs.
.
.
.
Sincea hierarchical discourse tree structure is analo-gous to a constituency based syntactic tree, mod-ern research explored syntactic parsing techniques(e.g., CKY) for discourse parsing based on mul-tiple text-level or sentence-level features (Soricutand Marcu, 2003; Reitter, 2003; Baldridge andLascarides, 2005; Subba and Di Eugenio, 2009;Lin et al., 2009; Luong et al., 2014).A recent prevailing idea for discourse parsingis to train two classifiers, namely a binary struc-ture classifier for determining whether two adja-cent text units should be merged to form a newsubtree, followed by a multi-class relation classi-fier for determining which discourse relation labelshould be assigned to the new subtree.
The idea isproposed by Hernault and his colleagues (Duverleand Prendinger, 2009; Hernault et al., 2010a) andfollowed by other work using more sophisticatedfeatures (Feng and Hirst, 2012; Hernault et al.,2010b).
Current state-of-art performance for re-lation identification is achieved by the recent rep-resentation learning approach proposed by (Ji andEisenstein, 2014).
The proposed framework pre-sented in this paper is similar to (Ji and Eisenstein,2014) for transforming the discourse units to theabstract representations.2.2 Recursive Deep LearningRecursive neural networks constitute one type ofdeep learning frameworks which was first pro-posed in (Goller and Kuchler, 1996).
The recur-sive framework relies and operates on structuredinputs (e.g., a parse tree) and computes the rep-resentation for each parent based on its children2062iteratively in a bottom-up fashion.
A series of vari-ations of RNN has been proposed to tailor differ-ent task-specific requirements, including Matrix-Vector RNN (Socher et al., 2012) that representsevery word as both a vector and a matrix, or Recur-sive Neural Tensor Network (Socher et al., 2013)that allows the model to have greater interactionsbetween the input vectors.
Many tasks have ben-efited from the recursive framework, includingparsing (Socher et al., 2011b), sentiment analysis(Socher et al., 2013), textual entailment (Bowman,2013), segmentation (Wang and Mansur, 2013;Houfeng et al., 2013), and paraphrase detection(Socher et al., 2011a).3 The RST Discourse TreebankThere are today two primary alternative discoursetreebanks suitable for training data: the Rhetor-ical Structure Theory Discourse Treebank RST-DT (Carlson et al., 2003) and the Penn DiscourseTreebank (Prasad et al., 2008).
In this paper, weselect the former.
In RST (Mann and Thompson,1988), a coherent context or a document is repre-sented as a hierarchical tree structure, the leavesof which are clause-sized units called ElementaryDiscourse Units (EDUs).
Adjacent nodes (siblingsin the tree) are linked with discourse relations thatare either binary (hypotactic) or multi-child (parat-actic).
One child of each hypotactic relation is al-ways more salient (called the NUCLEUS); its sib-ling (the SATELLITE) is less salient compared andmay be omitted in summarization.
Multi-nuclearrelations (e.g., CONJUNCTION) exhibit no distinc-tion of salience between the units.The RST Discourse Treebank contains 385 an-notated documents (347 for training and 38 fortesting) from the Wall Street Journal.
A totalof 110 fine-grained relations defined in (Marcu,2000b) are used for tagging relations in RST-DT.They are subtypes of 18 original high-level RSTcategories.
For fair comparison with existing sys-tems, we use in this work the 18 coarse-grained re-lation classes, which with nuclearity attached forma set of 41 distinct relations.
Non-binary relationsare converted into a cascade of right-branching bi-nary relations.Conventionally, discourse parsing in RST-DTinvolves the following sub-tasks: (1) EDU seg-mentation to segment the raw text into EDUs, (2)tree-building.
Since the segmentation task is es-sentially clause delimitation and hence relativelyeasy (with state-of-art accuracy at most 95%),we focus on the latter problem.
We assume thatthe gold-standard EDU segmentations are alreadygiven, as assumed in other past work (Feng andHirst, 2012).4 EDU ModelIn this section, we describe how we computethe distributed representation for a given sentencebased on its parse tree structure and containedwords.
Our implementation is based on (Socheret al., 2013).
As the details can easily be foundthere, we omit them for brevity.Let s denote any given sentence, comprised of asequence of tokens s = {w1, w2, ..., wns}, wherensdenotes the number of tokens in s. Each to-ken w is associated with a specific vector embed-ding ew= {e1w, e2w, ..., eKw}, where K denotes thedimension of the word embedding.
We wish tocompute the vector representation hsfor currentsentence, where hs= {h1s, h2s, ..., hKs}.Parse trees are obtained using the StanfordParser1, and each clause is treated as an EDU.
Fora given parent p in the tree and its two children c1(associated with vector representation hc1) and c2(associated with vector representation hc2), stan-dard recursive networks calculate the vector forparent p as follows:hp= f(W ?
[hc1, hc2] + b) (1)where [hc1, hc2] denotes the concatenating vectorfor children representations hc1and hc2; W is aK ?
2K matrix and b is the 1 ?
K bias vector;and f(?)
is the function tanh.
Recursive neuralmodels compute parent vectors iteratively until theroot node?s representation is obtained, and use theroot embedding to represent the whole sentence.5 Discourse ParsingSince recent work (Feng and Hirst, 2012; Hernaultet al., 2010b) has demonstrated the advantage ofcombining the binary structure classifier (deter-mining whether two adjacent text units should bemerged to form a new subtree) with the multi-classclassifier (determining which discourse relation la-bel to assign to the new subtree) over the oldersingle multi-class classifier with the additional la-bel NO-REL, our approach follows the modern1http://nlp.stanford.edu/software/lex-parser.shtml2063Figure 1: RST Discourse Tree Structure.strategy but trains binary and multi-class classi-fiers jointly based on the discourse structure tree.Figure 2 illustrates the structure of a discourseparse tree.
Each node e in the tree is associatedwith a distributed vector he.
e1, e2, e3and e6constitute the leaves of trees, the distributed vec-tor representations of which are assumed to be al-ready obtained from convolution in Section 4.
LetNrdenote the number of relations and we haveNr= 41.5.1 Binary (Structure) ClassificationIn this subsection, we train a binary (structure)classifier, which aims to decide whether two EDUsor spans should be merged during discourse treereconstruction.Let tbinary(ei, ej) be the binary valued variableindicating whether eiand ejare related, or in otherwords, whether a certain type of discourse rela-tions holds between eiand ej.
According to Fig-ure 2, the following pairs constitute the trainingdata for binary classification:tbinary(e1, e2) = 1, tbinary(e3, e4) = 1,tbinary(e2, e3) = 0, tbinary(e3, e6) = 0,tbinary(e5, e6) = 1To train the binary classifier, we adopt a three-layer neural network structure, i.e., input layer,hidden layer, and output layer.
Let H = [hei, hej]denote the concatenating vector for two spans eiand ej.
We first project the concatenating vectorH to the hidden layer withNbinaryhidden neurons.The hidden layer convolutes the input with non-linear tanh function as follows:Lbinary(ei,ej)= f(Gbinary?
[hei, hej] + bbinary)where Gbinaryis an Nbinary?
2K convolution ma-trix and bbinarydenotes the bias vector.The output layer takes as input Lbinary(ei,ej)and gen-erates a scalar using the linear function Ubinary?Lbinary(ei,ej)+ b.
A sigmod function is then adopted toproject the value to a [0,1] probability space.
Theexecution at the output layer can be summarizedas:p[tbinary(ei, ej) = 1] = g(Ubinary?Lbinary(ei,ej)+b?binary)(2)where Ubinaryis an Nbinary?
1 vector and b?binarydenotes the bias.
g(?)
is the sigmod function.5.2 Multi-class Relation ClassificationIf tbinary(ei, ej) is determined to be 1, we nextuse variable r(ei, ej) to denote the index of rela-tion that holds between eiand ej.
A multi-classclassifier is train based on a three-layer neural net-work, in the similar way as binary classification inSection 5.1.
Concretely, a matrix GMultiand biasvector bMultiare first adopted to convolute the con-catenating node vectors to the hidden layer vectorLmulti(ei,ej):Lmulti(ei,ej)= f(Gmulti?
[hei, hej] + bmulti) (3)We then compute the posterior probability overlabels given the hidden layer vector L using thesoftmax and obtain the Nrdimensional probabil-ity vector P(e1,e2)for each EDU pair as follows:S(ei,ej)= Umulti?
Lmulti(ei,ej)(4)P(e1,e2)(i) =exp(S(e1,e2)(i))?kexp(S(e1,e2))(k)(5)where Umultiis the Nr?
2K matrix.
The ithele-ment in P(e1,e2)denotes the probability that ith re-lation holds between eiand ej.
To note, binary andmulti-class classifiers are trained independently.5.3 Distributed Vector for SpansWhat is missing in the previous two subsectionsare the distributed vectors for non-leaf nodes (i.e.,e4and e5in Figure 1), which serve as structure andrelation classification.
Again, we turn to recursivedeep learning network to obtain the distributedvector for each node in the tree in a bottom-upfashion.Similar as for sentence parse-tree level compo-sitionally, we extend a standard recursive neuralnetwork by associating each type of relations rwith one specific K?2K convolution matrix Wr.2064Figure 2: System Overview.The representation for each node within the tree iscalculated based on the representations for its chil-dren in a bottom-up fashion.
Concretely, for a par-ent node p, given the distributed representation heifor left child, hejfor right child, and the relationr(e1, e2), its distributed vector hpis calculated asfollows:hp= f(Wr(e1,e2)?
[hei, hej] + br(e1,e2)) (6)where br(e1,e2)is the bias vector and f(?)
is thenon-linear tanh function.To note, our approach does not make any dis-tinction between within-sentence text spans andcross-sentence text spans, different from (Fengand Hirst, 2012; Joty et al., 2013)5.4 Cost FunctionThe parameters to optimize include sentence-level convolution parameters [W , b],discourse-level convolution parameters[{Wr}, {br}], binary classification parameters[Gbinary, bbinary, Ubinary, b?binary], and multi-classparameters [Gmulti, bmulti, Umulti].Suppose we have M1binary training samplesand M2multi-class training examples (M2equalsthe number of positive examples in M1, whichis also the non-leaf nodes within the training dis-course trees).
The cost function for our frameworkwith regularization on the training set is given by:J(?binary) =?(ei,ej)?
{binary}Jbinary(ei, ej)+Qbinary????
?binary?2(7)J(?multi) =?(ei,ej)?
{multi}Jmulti(ei, ej)+Qmulti????
?multi?2(8)whereJbinary(ei, ej) = ?t(ei, ej) log p(t(ei, ej) = 1)?
(1?
t(ei, ej)) log[1?
p(t(ei, ej) = 1)]Jmulti(ei, ej) = ?
log[p(r(ei, ej) = r)](9)5.5 Backward PropagationThe derivative for parameters involved is com-puted through backward propagation.
Here weillustrate how we compute the derivative ofJmulti(ei, ej) with respect to different parameters.For each pair of nodes (ei, ej) ?
multi, weassociate it with a Nrdimensional binary vectorR(ei, ej), which denotes the ground truth vectorwith a 1 at the correct label r(ei, ej) and all otherentries 0.
Integrating softmax error vector, for anyparameter ?, the derivative of Jmulti(ei, ej) with re-spect to ?
is given by:?Jmulti(ei, ej)?
?= [P(ei,ej)?R(ei,ej)]??S(ei,ej)??
(10)where ?
denotes the Hadamard product betweenthe two vectors.
Each training pair recursivelybackpropagates its error to some node in the dis-course tree through [{Wr}, {br}], and then tonodes in sentence parse tree through [W, b], andthe derivatives can be obtained according to stan-dard backpropagation (Goller and Kuchler, 1996;Socher et al., 2010).20655.6 Additional FeaturesWhen determining the structure/multi relation be-tween individual EDUs, additional features arealso considered, the usefulness of which has beenillustrated in a bunch of existing work (Feng andHirst, 2012; Hernault et al., 2010b; Joty et al.,2012).
We consider the following simple text-levelfeatures:?
Tokens at the beginning and end of the EDUs.?
POS at the beginning and end of the EDUs.?
Whether two EDUs are in the same sentence.5.7 OptimizationWe use the diagonal variant of AdaGrad (Duchi etal., 2011) with minibatches, which is widely ap-plied in deep learning literature (e.g.,(Socher etal., 2011a; Pei et al., 2014)).
The learning ratein AdaGrad is adapted differently for different pa-rameters at different steps.
Concretely, let gi?de-note the subgradient at time step t for parameter?iobtained from backpropagation, the parameterupdate at time step t is given by:?
?= ???1????t=0?gi2?gi?
(11)where ?
denotes the learning rate and is set to 0.01in our approach.Elements in {Wr}, W , Gbinary, Gmulti, Ubinary,Umultiare initialized by randomly drawing fromthe uniform distribution [?, ], where  is calcu-lated as suggested in (Collobert et al., 2011).
Allbias vectors are initialized with 0.
Word embed-dings {e} are borrowed from Senna (Collobert etal., 2011; Collobert, 2011).5.8 InferenceFor inference, the goal is to find the most proba-ble discourse tree given the EDUs within the doc-ument.
Existing inference approach basically in-clude the approach adopted in (Feng and Hirst,2012; Hernault et al., 2010b) that merges the mostlikely spans at each step and SPADE (Fisher andRoark, 2007) that first finds the tree structure thatis globally optimal, then assigns the most probablerelations to the internal nodes.In this paper, we implement a probabilisticCKY-like bottom-up algorithm for computing themost likely parse tree using dynamic program-ming as are adopted in (Joty et al., 2012; Jotyet al., 2013; Jurafsky and Martin, 2000) for thesearch of global optimum.
For a document withn EDUs, as different relations are characterizedwith different compositions (thus leading to dif-ferent vectors), we use a Nr?n?n dynamic pro-gramming table Pr, the cell Pr[r, i, j] of whichrepresents the span contained EDUs from i to jand stores the probability that relation r holds be-tween the two spans within i to j. Pr[r, i, j] iscomputed as follows:Pr[r, i, j] =maxr1,r2,kPr[r1, i, k] ?
Pr[r2, k, j]?P (tbinary(e[i,k], e[k,j]) = 1)?P (r(e[i,k], e[k,j]) = 1)(12)At each merging step, a distributed vector for themerged point is calculated according to Eq.
13 fordifferent relations.
The CKY-like algorithms findsthe global optimal.
To note, the worst-case run-ning time of our inference algorithm is O(N2rn3),where n denotes the number of sentences withinthe document, which is much slower than thegreedy search.
In this work, for simplification, wesimplify the framework by maintaining the top 10options at each step.6 ExperimentsA measure of the performance of the system isrealized by comparing the structure and labelingof the RS-tree produced by our algorithm to gold-standard annotations.Standard evaluation of discourse parsing outputcomputes the ratio of the number of identical treeconstituents shared in the generated RS-trees andthe gold-standard trees against the total numberof constituents in the generated discourse trees2,which is further divided to three matrices: Span(on the blank tree structure), nuclearity (on thetree structure with nuclearity indication), and rela-tion (on the tree structure with rhetorical relationindication but no nuclearity indication).The nuclearity and relation decisions are madebased on the multi-class output labels from thedeep learning framework.
As we do not considernuclearity when classifying different discourse re-lations, the two labels attribute[N][S] and at-tribute[S][N] made by multi-class classifier willbe treated as the same relation label ATTRIBUTE.2Conventionally, evaluation matrices involve precision,recall and F-score in terms of the comparison between treestructures.
But these are the same when manual segmenta-tion is used (Marcu, 2000b).2066Approach Span Nuclearity RelationHILDA 75.3 60.0 46.8Joty et al.
82.5 68.4 55.7Feng and Hirst 85.7 71.0 58.2Ji and Eisenstein 82.1 71.1 61.6Unified (with feature) 82.0 70.0 57.1Ours (no feature) 82.4 69.2 56.8Ours (with feature) 84.0 70.8 58.6human 88.7 77.7 65.7Table 1: Performances for different approaches.Performances for baselines are reprinted from(Joty et al., 2013; Feng and Hirst, 2014; Ji andEisenstein, 2014).Also, we do not train a separate classifier for NU-CLEUS and SATELLITE identification.
The nucle-arity decision is made based on the relation typeproduced by the multi-class classifier.6.1 Parameter TuningThe regularization parameter Q constitutes theonly parameter to tune in our framework.
We tuneit on the 347 training documents.
Concretely, weemploy a five-fold cross validation on the RSTdataset and tune Q on 5 different values: 0.01,0.1, 0.5, 1.5, 2.5.
The final model was tested onthe testing set after parameter tuning.6.2 BaselinesWe compare our model against the followingcurrently prevailing discourse parsing baselines:HILDA A discourse parser based on supportvector machine classification introduced by Her-nault et al.
(Hernault et al., 2010b).
HILDA usesthe binary and multi-class classifier to reconstructthe tree structure in a greedy way, where themost likely nodes are merged at each step.
Theresults for HILDA are obtained by running thesystem with default settings on the same inputswe provided to our system.Joty et al The discourse parser introduced byJoty et al.
(Joty et al., 2013).
It relies on CRFand combines intra-sentential and multi-sententialparsers in two different ways.
Joty et al.
adoptthe global optimal inference as in our work.
Wereported the performance from their paper (Joty etal., 2013).Feng and Hirst The linear-time discourseparser introduced in (Feng and Hirst, 2014) whichrelies on two linear-chain CRFs to obtain a se-quence of discourse constituents.Ji and Eisenstein The shift-reduce discourseparser introduced in (Ji and Eisenstein, 2014)which parses document by relying on the dis-tributed representations obtained from deep learn-ing framework.Additionally, we implemented a simplified ver-sion of our model called unified where we usea unified convolutional function with unified pa-rameters [Wsen, bsen] for span vector computation.Concretely, for a parent node p, given the dis-tributed representation heifor left child, hejforright child, and the relation r(e1, e2), rather thantaking the inter relation between two children, itsdistributed vector hpis calculated:hp= f(Wsen?
[hei, hej] + bsen) (13)6.3 PerformancePerformances for different models approaches re-ported in Table 1.
And as we can observe, al-though the proposed framework obtains compa-rable result compared with existing state-of-stateperformances regarding all evaluating parametersfor discourse parsing.
Specifically, as for the threemeasures, no system achieves top performance onall three, though some systems outperform all oth-ers for one of the measures.
The proposed systemachieves high overall performance on all three, al-though it does not achieve top score on any mea-sure.
The system gets a little bit performanceboost by considering text-level features illustratedin Section 5.6.
The simplified version of the orig-inal model underperforms against the original ap-proach due to lack of expressive power in convo-lution.
Performance plummets when different re-lations are uniformly treated, which illustrates theimportance of taking into consideration differenttypes of relations in the span convolution proce-dure.7 ConclusionIn this paper, we describe an RST-style text-leveldiscourse parser based on a neural network model.The incorporation of sentence-level distributedvectors for discourse analysis obtains compara-ble performance compared with current state-of-art discourse parsing system.Our future work will focus on extendingdiscourse-level distributed presentations to related2067tasks, such as implicit discourse relation identifi-cation or dialogue analysis.
Further, once the treestructure for a document can be determined, thevector for the entire document can be obtainedin bottom-up fashion, as in this paper.
One cannow investigate whether the discourse parse treeis useful for acquiring a single document-levelvector representation, which would benefit mul-tiple tasks, such as document classification ormacro-sentiment analysis.AcknowledgementsThe authors want to thank Vanessa Wei Feng andShafiq Joty for helpful discussions regarding RSTdataset.
We also want to thank Richard Socher,Zhengyan He and Pradeep Dasigi for the clarifica-tion of deep learning techniques.ReferencesJason Baldridge and Alex Lascarides.
2005.
Proba-bilistic head-driven parsing for discourse structure.In Proceedings of the Ninth Conference on Compu-tational Natural Language Learning, pages 96?103.Association for Computational Linguistics.Samuel R Bowman.
2013.
Can recursive neural tensornetworks learn logical reasoning?
arXiv preprintarXiv:1312.6192.Lynn Carlson, Daniel Marcu, and Mary EllenOkurowski.
2003.
Building a discourse-tagged cor-pus in the framework of rhetorical structure theory.Springer.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
The Journal of Machine Learning Re-search, 12:2493?2537.Ronan Collobert.
2011.
Deep learning for efficient dis-criminative parsing.
In International Conference onArtificial Intelligence and Statistics, number EPFL-CONF-192374.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learningand stochastic optimization.
The Journal of Ma-chine Learning Research, 12:2121?2159.David A Duverle and Helmut Prendinger.
2009.
Anovel discourse parser based on support vector ma-chine classification.
In Proceedings of the JointConference of the 47th Annual Meeting of the ACLand the 4th International Joint Conference on Natu-ral Language Processing of the AFNLP: Volume 2-Volume 2, pages 665?673.
Association for Compu-tational Linguistics.Vanessa Wei Feng and Graeme Hirst.
2012.
Text-level discourse parsing with rich linguistic fea-tures.
In Proceedings of the 50th Annual Meetingof the Association for Computational Linguistics:Long Papers-Volume 1, pages 60?68.
Associationfor Computational Linguistics.Vanessa Wei Feng and Graeme Hirst.
2014.
A lin-ear time bottom-up discourse parser with constraintsand post-editing.
In ACL.Seeger Fisher and Brian Roark.
2007.
The utility ofparse-derived features for automatic discourse seg-mentation.
In ANNUAL MEETING-ASSOCIATIONFOR COMPUTATIONAL LINGUISTICS, vol-ume 45, page 488.Christoph Goller and Andreas Kuchler.
1996.
Learn-ing task-dependent distributed representations bybackpropagation through structure.
In Neural Net-works, 1996., IEEE International Conference on,volume 1, pages 347?352.
IEEE.Hugo Hernault, Danushka Bollegala, and MitsuruIshizuka.
2010a.
A semi-supervised approach toimprove classification of infrequent discourse rela-tions using feature vector extension.
In Proceedingsof the 2010 Conference on Empirical Methods inNatural Language Processing, pages 399?409.
As-sociation for Computational Linguistics.Hugo Hernault, Helmut Prendinger, Mitsuru Ishizuka,et al.
2010b.
Hilda: a discourse parser using sup-port vector machine classification.
Dialogue & Dis-course, 1(3).Wang Houfeng, Longkai Zhang, and Ni Sun.
2013.Improving chinese word segmentation on micro-blog using rich punctuations.Eduard H Hovy and Elisabeth Maier.
1997.
Parsimo-nious or profligate: How many and which discoursestructure relations.
Discourse Processes.Yangfeng Ji and Jacob Eisenstein.
2014.
Representa-tion learning for text-level discourse parsing.Shafiq Joty, Giuseppe Carenini, and Raymond TNg.
2012.
A novel discriminative framework forsentence-level discourse analysis.
In Proceedingsof the 2012 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, pages 904?915.
Asso-ciation for Computational Linguistics.Shafiq Joty, Giuseppe Carenini, Raymond Ng, andYashar Mehdad.
2013.
Combining intra-and multi-sentential rhetorical parsing for document-level dis-course analysis.
In Proceedings of the 51st annualmeeting of the association for computational lin-guistics (ACL), pages 486?496.Dan Jurafsky and James H Martin.
2000.
Speech &Language Processing.
Pearson Education India.2068Huong LeThanh, Geetha Abeysinghe, and ChristianHuyck.
2004.
Generating discourse structures forwritten texts.
In Proceedings of the 20th inter-national conference on Computational Linguistics,page 329.
Association for Computational Linguis-tics.Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng.
2009.Recognizing implicit discourse relations in the penndiscourse treebank.
In Proceedings of the 2009 Con-ference on Empirical Methods in Natural LanguageProcessing: Volume 1-Volume 1, pages 343?351.Association for Computational Linguistics.Annie Louis, Aravind Joshi, and Ani Nenkova.
2010.Discourse indicators for content selection in summa-rization.
In Proceedings of the 11th Annual Meetingof the Special Interest Group on Discourse and Di-alogue, pages 147?156.
Association for Computa-tional Linguistics.Minh-Thang Luong, Michael C Frank, and Mark John-son.
2014.
Parsing entire discourses as very longstrings: Capturing topic continuity in grounded lan-guage learning.William C Mann and Sandra A Thompson.
1988.Rhetorical structure theory: Toward a functional the-ory of text organization.
Text, 8(3):243?281.Daniel Marcu.
2000a.
The rhetorical parsing of unre-stricted texts: A surface-based approach.
Computa-tional Linguistics, 26(3):395?448.Daniel Marcu.
2000b.
The theory and practice of dis-course parsing and summarization.
MIT Press.Wenzhe Pei, Tao Ge, and Chang Baobao.
2014.
Max-margin tensor neural network for chinese word seg-mentation.
In Proceedings of ACL.Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-sakaki, Livio Robaldo, Aravind K Joshi, and Bon-nie L Webber.
2008.
The penn discourse treebank2.0.
In LREC.
Citeseer.David Reitter.
2003.
Simple signals for complexrhetorics: On rhetorical analysis with rich-featuresupport vector models.
In LDV Forum, volume 18,pages 38?52.Richard Socher, Christopher D Manning, and An-drew Y Ng.
2010.
Learning continuous phraserepresentations and syntactic parsing with recursiveneural networks.
In Proceedings of the NIPS-2010Deep Learning and Unsupervised Feature LearningWorkshop, pages 1?9.Richard Socher, Eric H Huang, Jeffrey Pennington,Andrew Y Ng, and Christopher D Manning.
2011a.Dynamic pooling and unfolding recursive autoen-coders for paraphrase detection.
In NIPS, vol-ume 24, pages 801?809.Richard Socher, Cliff C Lin, Chris Manning, and An-drew Y Ng.
2011b.
Parsing natural scenes and nat-ural language with recursive neural networks.
InProceedings of the 28th International Conference onMachine Learning (ICML-11), pages 129?136.Richard Socher, Brody Huval, Christopher D Manning,and Andrew Y Ng.
2012.
Semantic compositional-ity through recursive matrix-vector spaces.
In Pro-ceedings of the 2012 Joint Conference on Empiri-cal Methods in Natural Language Processing andComputational Natural Language Learning, pages1201?1211.
Association for Computational Linguis-tics.Richard Socher, Alex Perelygin, Jean Y Wu, JasonChuang, Christopher D Manning, Andrew Y Ng,and Christopher Potts.
2013.
Recursive deep mod-els for semantic compositionality over a sentimenttreebank.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing(EMNLP), pages 1631?1642.Radu Soricut and Daniel Marcu.
2003.
Sentence leveldiscourse parsing using syntactic and lexical infor-mation.
In Proceedings of the 2003 Conferenceof the North American Chapter of the Associationfor Computational Linguistics on Human LanguageTechnology-Volume 1, pages 149?156.
Associationfor Computational Linguistics.Caroline Sporleder and Mirella Lapata.
2005.
Dis-course chunking and its application to sentence com-pression.
In Proceedings of the conference on Hu-man Language Technology and Empirical Methodsin Natural Language Processing, pages 257?264.Association for Computational Linguistics.Rajen Subba and Barbara Di Eugenio.
2009.
An effec-tive discourse parser that uses rich linguistic infor-mation.
In Proceedings of Human Language Tech-nologies: The 2009 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 566?574.
Association forComputational Linguistics.Suzan Verberne, Lou Boves, Nelleke Oostdijk, andPeter-Arno Coppen.
2007.
Evaluating discourse-based answer extraction for why-question answer-ing.
In Proceedings of the 30th annual internationalACM SIGIR conference on Research and develop-ment in information retrieval, pages 735?736.
ACM.Longkai Zhang Houfeng Wang and Xu Sun MairgupMansur.
2013.
Exploring representations from un-labeled data with co-training for chinese word seg-mentation.2069
