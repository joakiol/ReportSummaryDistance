Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 235?238,New York, June 2006. c?2006 Association for Computational LinguisticsDocument Representation and Multilevel Measures of Document SimilarityIrina MatveevaDept.
of Computer ScienceUniversity of Chicagomatveeva@cs.uchicago.eduAbstractWe present our work on combining large-scale statistical approaches with local lin-guistic analysis and graph-based machinelearning techniques to compute a com-bined measure of semantic similarity be-tween terms and documents for applica-tion in information extraction, questionanswering, and summarisation.1 IntroductionDocument indexing and representation of term-document relations are crucial for document classi-fication, clustering and retrieval.
In the traditionalbag-of-words vector space representation of docu-ments (Salton and McGill, 1983) words representorthogonal dimensions which makes an unrealisticassumption about their independence.Since document vectors are constructed in a veryhigh dimensional vocabulary space, there has been aconsiderable interest in low-dimensional documentrepresentations to overcome the drawbacks of thebag-of-words document vectors.
Latent SemanticAnalysis (LSA) (Deerwester et al, 1990) is one ofthe best known dimensionality reduction algorithmsin information retrieval.In my research, I consider different notions ofsimilarity measure between documents.
I use di-mensionality reduction and statistical co-occurrenceinformation to define representations that supportthem.2 Dimensionality Reduction for Documentand Term RepresentationA vector space representation of documents is veryconvenient because it puts documents in a Euclideanspace where similarity measures such as inner prod-uct and cosine similarity or distance are immediatelyavailable.
However, these measures will not be ef-fective if they do not have a natural interpretation forthe original text data.I have considered several approaches to comput-ing a vector space representation of text data forwhich inner product and distance make sense.
Thegeneral framework is to construct a matrix of pair-wise similarities between terms or documents anduse appropriate methods of dimensionality reduc-tion to compute low dimensional vectors.
The innerproduct between the resulting vectors must preservethe similarities in the input matrix.
The similaritiesmatrix can be computed using different notions ofsimilarity in the input space.
Different dimensional-ity reduction techniques impose different conditionson how the similarities are preserved.I investigated how external query-based similar-ity information can be used to compute low dimen-sional document vectors.
Similar to LSA, this ap-proach used weighted bag-of-words document vec-tors as input which limited its effectiveness.
Thenext step was to develop the Generalized Latent Se-mantic Analysis framework that allows to computesemantically motivated term and document vectors.2352.1 Document Representation with theLocality Preserving Projection AlgorithmThe Locality Preserving Projection algorithm(LPP) (He and Niyogi, 2003) is a graph-baseddimensionality reduction algorithm that computeslow dimensional document vectors by preservinglocal similarities between the documents.
It requiresa vector space representation of documents asinput.
In addition, it uses the adjacency matrixof the nearest neighbors graph of the data.
It canbe shown, see (He and Niyogi, 2003), that theEuclidean distance in the LPP space corresponds tosimilarity in the document space.The information about the similarity of the inputdocuments is contained in the adjacency matrix ofthe nearest neighbors graph.
In this graph, nodesrepresent documents and are connected by an edgeif the documents are similar.
This graph can be con-structed using any similarity measure between thedocuments, for example, the query-based similar-ity between the documents obtained from relevancefeedback.
The base case is to use inner products be-tween the input document vectors and to connect knearest neighbors.We considered several ways of modifying thegraph, see (Matveeva, 2004).
We used relevancefeedback and pseudo relevance feedback from thebase line term matching retrieval to identify the topN documents most related to the query.
We addededges to the document neighborhood graph to con-nect these N documents.
Our experiments showedthat incorporating this external relevance informa-tion into the LPP graph improves the performanceon the information retrieval tasks, in particular athigh levels of recall.
Without the use of externalinformation, the performance of the LPP algorithmwas comparable to the performance of the LSA al-gorithm up to recall of 0.6?0.7.
At higher levels ofrecall, LSA achieves a precision that is about 0.1better than LPP.
The precision at high levels of re-call seemed to be a weak point of LPP.
Fortunately,using the relevance feedback helped to improve theperformance in particular in this range of recall.We found the LPP algorithm to be very sensitiveto the graph structure.
It confirmed the intuition thatthe Euclidean distance between the document vec-tors in the bag-of-words representation is not a goodsimilarity measure.
When we added query relevanceinformation to the graph, we introduced a similaritymetric on the document space that was closer to thetrue similarity.
However, this information was onlypartial, because only a subset of the edges reflectedthis true similarity.
The next step was therefore todevelop a vector space representation for documentswhich did not require the bag-of-words representa-tion as input.2.2 Generalized Latent Semantic AnalysisWe developed the Generalized Latent Seman-tic Analysis (GLSA) framework to compute se-mantically motivated term and document vec-tors (Matveeva et al, 2005).
We begin with seman-tically motivated pair-wise term similarities and usedimensionality reduction to compute a vector spacerepresentation for terms.
Our approach is to focus onsimilarity between vocabulary terms.
We computerepresentations and similarities for terms and con-sider documents to be linear combinations of terms.This shift from dual document-term representationto terms has the following motivation.?
Terms offer a much greater flexibility in explor-ing similarity relations than documents.
Theavailability of large document collections suchas the Web offers a great resource for statisti-cal approaches.
Recently, co-occurrence basedmeasures of semantic similarity between termshas been shown to improve performance onsuch tasks as the synonymy test, taxonomy in-duction, etc.
(Turney, 2001; Terra and Clarke,2003; Chklovski and Pantel, 2004).
On theother hand, many semi-supervised and trans-ductive methods based on document vectorscannot yet handle such large document collec-tions.?
While the vocabulary size is still quite large,it is intuitively clear that the intrinsic dimen-sionality of the vocabulary space is much lower.Content bearing words are often combined intosemantic classes that correspond to particularactivities or relations and contain synonymsand semantically related words.
Therefore, itseems very natural to represent terms as low di-mensional vectors in the space of semantic con-cepts.2362.2.1 GLSA AlgorithmThe GLSA algorithm takes as input a documentcollection C with vocabulary V and a large corpusW .
It has the following outline:1.
Construct the weighted term document matrixD based on C2.
For the vocabulary words in V , obtain a ma-trix of pair-wise similarities, S, using the largecorpus W3.
Obtain the matrix UT of low dimensional vec-tor space representation of terms that preservesthe similarities in S, UT ?
Rk?|V | .
Thecolumns of UT are k-dimensional term vectors4.
Compute document vectors by taking linearcombinations of term vectors D?
= UTDIn step 2 of the GLSA algorithm we used point-wise mutual information (PMI) as the co-occurrencebased measure of semantic associations betweenpairs of the vocabulary terms.
We used the singu-lar value decomposition in step 3 to compute GLSAterm vectors.2.2.2 Experimental EvaluationWe used the TOEFL, TS1 and TS2 synonymytests to demonstrate that the GLSA vector space rep-resentation for terms captures their semantic rela-tions, see (Matveeva et al, 2005) for details.
Ourresults demonstrate that similarities between GLSAterm vectors achieve better results than PMI scoresand outperform the related PMI-IR approach (Tur-ney, 2001; Terra and Clarke, 2003).
On the TOEFLtest GLSA achieves the best precision of 0.86, whichis much better than our PMI baseline as well asthe highest precision of 0.81 reported in (Terra andClarke, 2003).
GLSA achieves the same maximumprecision as in (Terra and Clarke, 2003) for TS1(0.73) and higher precision on TS2 (0.82 comparedto 0.75 in (Terra and Clarke, 2003)).We also conducted document classification exper-iments to demonstrate the advantage of the GLSAdocument vectors (Matveeva et al, 2005).
We useda k-nearest neighbors classifier for a set of 5300documents from 6 dissimilar groups from the 20news groups data set.
The k-nn classifier achievedhigher accuracy with the GLSA document vectorsthan with the traditional tf-idf document vectors, es-pecially with fewer training examples.
With 100training examples, the k-nn classifier with GLSAhad 0.75 accuracy vs. 0.58 with the tf-idf documentvectors.
With 1000 training examples the numberswere 0.81 vs. 0.75.The inner product between the GLSA documentvectors can be used as input to other algorithms.The language modelling approach (Berger and Laf-ferty, 1999) proved very effective for the informa-tion retrieval task.
Berger et.
al (Berger and Laf-ferty, 1999) used translation probabilities betweenthe document and query terms to account for syn-onymy and polysemy.
We proposed to use low di-mensional term vectors for inducing the translationprobabilities between terms (Matveeva and Levow,2006).
We used the same k-nn classification task asabove.
With 100 training examples, the k-nn accu-racy based on tf-idf document vectors was 0.58 andwith the similarity based on the language modellingwith GLSA term translation probabilities the accu-racy was 0.69.
With larger training sets the differ-ence in performance was less significant.
These re-sults illustrate that the pair-wise similarities betweenthe GLSA term vectors add important semantic in-formation which helps to go beyond term matchingand deal with synonymy and polysemy.3 Work in ProgressMany recent applications such as document sum-marization, information extraction and question an-swering require a detailed analysis of semantic re-lations between terms within and across documentsand sentences.
Often one has a number of sentencesor paragraphs and has to choose the candidate withthe highest level of relevance for the topic or ques-tion.
An additional requirement may be that the in-formation content of the next candidate is differentfrom the sentences that are already chosen.In these cases, it seems natural to have differ-ent levels of document similarity.
Two sentences orparagraphs can be similar because they contain in-formation about the same people or events.
In thiscase, the similarity can be based on the number ofthe named entities they have in common.
On theother hand, they can be similar because they containsynonyms or semantically related terms.237I am currently working on a combination of sim-ilarity measures between terms to model documentsimilarity.
I divide the vocabulary into general vo-cabulary terms and named entities and compute aseparate similarity score for each group of terms.The overall document similarity score is a functionof these two scores.
To keep the vocabulary sizemanageable and denoise the data, we only use thecontent bearing words from the set of the generalvocabulary terms.
We use a parser to identify nounsand adjectives that participate in three types of syn-tactic relations: subject, direct object, the head of thenoun phrase with an adjective or noun as a modifierfor nouns and the modifier of a noun for adjectives.Currently we include only such nouns and adjectivesin the set of the content bearing vocabulary terms.We used the TDT2 collection for preliminaryclassification experiments.
We used a k-nn classi-fier to classify documents from the 10 most frequenttopics.
We used tf-idf document vectors indexedwith 55,729 general vocabulary words as our base-line.
The set of the content bearing words was muchsmaller and had 13,818 nouns and adjectives.
TheGLSA document vectors improved the classificationaccuracy over the baseline and outperformed LSAdocument vectors.
This validates our approach toselecting the content bearing terms and shows theadvantage of using the GLSA framework.
We aregoing to extend the set of content bearing words andto include verbs.
We will take advantage of the flex-ibility provided by our framework and use syntaxbased measure of similarity in the computation ofthe verb vectors, following (Lin, 1998).Currently we are using string matching to com-pute the named entity based measure of similar-ity.
We are planning to integrate more sophisticatedtechniques in our framework.4 ConclusionWe developed the GLSA framework for comput-ing semantically motivated term and document vec-tors.
This framework takes advantage of the avail-ability of large document collections and recent re-search of corpus-based term similarity measures andcombines them with dimensionality reduction algo-rithms.Different measures of similarity may be requiredfor different groups of terms such as content bear-ing vocabulary words and named entities.
To ex-tend the GLSA approach to computing the documentvectors, we use a combination of similarity mea-sures between terms to model the document simi-larity.
This approach defines a fine-grained similar-ity measure between documents and sentences.
Ourgoal is to develop a multilevel measure of documentsimilarity that will be helpful for summarization andinformation extraction.ReferencesAdam Berger and John Lafferty.
1999.
Information re-trieval as statistical translation.
In Proc.
of the 22rdACM SIGIR.Timothy Chklovski and Patrick Pantel.
2004.
Verbo-cean: Mining the web for fine-grained semantic verbrelations.
In Proc.
of EMNLP.Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-dauer, George W. Furnas, and Richard A. Harshman.1990.
Indexing by latent semantic analysis.
Jour-nal of the American Society of Information Science,41(6):391?407.Xiaofei He and Partha Niyogi.
2003.
Locality preservingprojections.
In Proc.
of NIPS.Dekang Lin.
1998.
Automatic retrieval and clustering ofsimilar words.
In COLING-ACL, pages 768?774.Irina Matveeva and Gina-Anne Levow.
2006.
Comput-ing term translation probabilities with generalized la-tent semantic analysis.
In Proc.
of EACL.Irina Matveeva, Gina-Anne Levow, Ayman Farahat, andChristian Royer.
2005.
Generalized latent semanticanalysis for term representation.
In Proc.
of RANLP.Irina Matveeva.
2004.
Text representation with the lo-cality preserving projection algorithm for informationretrieval task.
In Master?s Thesis.Gerard Salton and Michael J. McGill.
1983.
Introductionto Modern Information Retrieval.
McGraw-Hill.Egidio L. Terra and Charles L. A. Clarke.
2003.
Fre-quency estimates for statistical word similarity mea-sures.
In Proc.of HLT-NAACL.Peter D. Turney.
2001.
Mining the web for synonyms:PMI?IR versus LSA on TOEFL.
Lecture Notes inComputer Science, 2167:491?502.238
