A Model -Theoret ic  Framework for Theories of SyntaxJames RogersInst i tute  for Research in Cognit ive ScienceUniversity of PennsylvaniaSuite 400C, 3401 Walnut  StreetPhi ladelphia,  PA 19104j rogers?linc, cis.
upenn, eduAbst rac tA natural next step in the evolution ofconstraint-based grammar formalisms fromrewriting formalisms is to abstract fullyaway from the details of the grammarmechanism--to express syntactic theoriespurely in terms of the properties of theclass of structures they license.
By fo-cusing on the structural properties of lan-guages rather than on mechanisms for gen-erating or checking structures that exhibitthose properties, this model-theoretic ap-proach can offer simpler and significantlyclearer expression of theories and can po-tentially provide a uniform formalization,allowing disparate theories to be comparedon the basis of those properties.
We dis-cuss L2,p, a monadic second-order logicalframework for such an approach to syn-tax that has the distinctive virtue of be-ing superficially expressive--supporting di-rect statement of most linguistically sig-nificant syntactic properties--but havingwell-defined strong generative capacity--languages are definable in L2K,p iff they arestrongly context-free.
We draw examplesfrom the realms of GPSG and GB.1 In t roduct ionGenerative grammar and formal language theoryshare a common origin in a procedural notion ofgrammars: the grammar formalism provides a gen-eral mechanism for recognizing or generating lan-guages while the grammar itself specializes thatmechanism for a specific language.
At least ini-tially there was hope that this relationship wouldbe informative for linguistics, that by character-izing the natural anguages in terms of language-theoretic omplexity one would gain insight into thestructural regularities of those languages.
More-over, the fact that language-theoretic complexityclasses have dual automata-theoretic characteriza-tions offered the prospect hat such results mightprovide abstract models of the human language fac-ulty, thereby not just identifying these regularities,but actually accounting for them.Over time, the two disciplines have gradually be-come estranged, principally due to a realization thatthe structural properties of languages that charac-terize natural languages may well not be those thatcan be distinguished by existing language-theoreticcomplexity classes.
Thus the insights offered by for-mal language theory might actually be misleadingin guiding theories of syntax.
As a result, the em-phasis in generative grammar has turned from for-malisms with restricted generative capacity to thosethat support more natural expression of the observedregularities of languages.
While a variety of dis-tinct approaches have developed, most of them canbe characterized as constrain~ based--the formalism(or formal framework) provides a class of structuresand a means of precisely stating constraints on theirform, the linguistic theory is then expressed as a sys-tem of constraints (or principles) that characterizethe class of well-formed analyses of the strings in thelanguage.
1As the study of the formal properties of classes ofstructures defined in such a way falls within domainof Model Theory, it's not surprising that treatmentsof the meaning of these systems of constraints aretypically couched in terms of formal logic (Kasperand Rounds, 1986; Moshier and Rounds, 1987;Kasper and Rounds, 1990; Gazdar et al, 1988; John-son, 1988; Smolka, 1989; Dawar and Vijay-Shanker,1990; Carpenter, 1992; Keller, 1993; Rogers andVijay-Shanker, 1994).While this provides a model-theoretic interpre-tation of the systems of constraints producedby these formalisms, those systems are typi-cally built by derivational processes that employextra-logical mechanisms to combine constraints.More recently, it has become clear that in manycases these mechanisms can be replaced with or-dinary logical operations.
(See, for instance:1This notion of constraint-based includes not only theobvious formalisms, but the formal framework of GB aswell.10Johnson (1989), Stabler, Jr. (1992), Cornell (1992),Blackburn, Gardent, and Meyer-Viol (1993),Blackburn and Meyer-Viol (1994), Keller (1993),Rogers (1994), Kracht (1995), and, anticipating allof these, Johnson and Postal (1980).)
This ap-proach abandons the notions of grammar mecha-nism and derivation in favor of defining languages asclasses of more or less ordinary mathematical struc-tures axiomatized by sets of more or less ordinarylogical formulae.
A grammatical theory expressedwithin such a framework is just the set of logical con-sequences of those axioms.
This step completes thedetachment of generative grammar from its proce-dural roots.
Grammars, in this approach, are purelydeclarative definitions of a class of structures, com-pletely independent of mechanisms to generate orcheck them.
While it is unlikely that every theoryof syntax with an explicit derivational componentcan be captured in this way, ~ for those that can thelogical re-interpretation frequently offers a simpli-fied statement of the theory and clarifies its conse-quences.But the accompanying loss of language-theoreticcomplexity results is unfortunate.
While such resultsmay not be useful in guiding syntactic theory, theyare not irrelevant.
The nature of language-theoreticcomplexity hierarchies i to classify languages on thebasis of their structural properties.
The languagesin a class, for instance, will typically exhibit cer-tain closure properties (e.g., pumping lemmas) andthe classes themselves admit normal forms (e.g., rep-resentation theorems).
While the linguistic signifi-cance of individual results of this sort is open to de-bate, they at least loosely parallel typical linguisticconcerns: closure properties tate regularities thatare exhibited by the languages in a class, normalforms express generalizations about their structure.So while these may not be the right results, theyare not entirely the wrong kind of results.
More-over, since these classifications are based on struc-tural properties and the structural properties of nat-ural language can be studied more or less directly,there is a reasonable xpectation of finding empiri-cal evidence falsifying a hypothesis about language-theoretic omplexity of natural anguages if such ev-idence exists.Finally, the fact that these complexity classes haveautomata-theoretic characterizations means that re-sults concerning the complexity of natural anguageswill have implications for the nature of the humanlanguage faculty.
These automata-theoretic charac-terizations determine, along one axis, the types ofresources required to generate or recognize the lan-2Whether there are theories that cannot be captured,at least without explicitly encoding the derivations, isan open question of considerable theoretical interest, asis the question of what empirical consequences such anessential dynamic haracter might have.11guages in a class.
The regular languages, for in-stance, can be characterized by finite-state (string)automata--these languages can be processed usinga fixed amount of memory.
The context-sensitivelanguages, on the other had, can be characterizedby linear-bounded automata-- they can be processedusing an amount of memory proportional to thelength of the input.
The context-free languagesare probably best characterized by finite-state treeautomata--these correspond to recognition by a col-lection of processes, each with a fixed amount ofmemory, where the number of processes i  linear inthe length of the input and all communication be-tween processes is completed at the time they arespawned.
As a result, while these results do notnecessarily offer abstract models of the human lan-guage faculty (since the complexity results do notclaim to characterize the human languages, just toclassify them), they do offer lower bounds on cer-tain abstract properties of that faculty.
In this way,generative grammar in concert with formal anguagetheory offers insight into a deep aspect of humancognition--syntactic processing--on the basis of ob-servable behavior--the structural properties of hu-man languages.In this paper we discuss an approach to definingtheories of syntax based on L 2 (Rogers, 1994), a K,Pmonadic second-order language that has well-definedgenerative capacity: sets of finite trees are defin-able within L 2 iff they are strongly context-free K,Pin a particular sense.
While originally introducedas a means of establishing language-theoretic com-plexity results for constraint-based theories, this lan-guage has much to recommend it as a general frame-work for theories of syntax in its own right.
Be-ing a monadic second-order language it can capturethe (pure) modal languages of much of the exist-ing model-theoretic syntax literature directly; hav-ing a signature based on the traditional inguisticrelations of domination, immediate domination, lin-ear precedence, etc.
it can express most linguisticprinciples transparently; and having a clear charac-terization in terms of generative capacity, it servesto re-establish the close connection between genera-tive grammar and formal language theory that waslost in the move away from phrase-structure gram-mars.
Thus, with this framework we get both theadvantages of the model-theoretic approach with re-spect to naturalness and clarity in expressing linguis-tic principles and the advantages of the grammar-based approach with respect to language-theoreticcomplexity results.We look, in particular, at the definitions of a singleaspect of each of GPSG and GB.
The first of these,Feature Specification Defaults in GPSG, are widelyassumed to have an inherently dynamic character.In addition to being purely declarative, our reformal-ization is considerably simplified wrt the definitionin Gasdar et al (1985), 3 and does not share its mis-leading dynamic flavor.
4 We offer this as an exampleof how re-interpretations of this sort can inform theoriginal theory.
In the second example we sketch adefinition of chains in GB.
This, again, captures apresumably dynamic aspect of the original theory ina static way.
Here, though, the main significance ofthe definition is that it forms a component of a full-scale treatment of a GB theory of English S- andD-Structure within L 2 This full definition estab- K,P"lishes that the theory we capture licenses a stronglycontext-free language.
More importantly, by exam-ining the limitations of this definition of chains, andin particular the way it fails for examples of non-context-free constructions, we develop a character-ization of the context-free languages that is quitenatural in the realm of GB.
This suggests that theapparent mismatch between formal anguage theoryand natural anguages may well have more to do withthe unnaturalness of the traditional diagnostics thana lack of relevance of the underlying structural prop-erties.Finally, while GB and GPSG are fundamentallydistinct, even antagonistic, approaches to syntax,their translation into the model-theoretic terms ofL 2 allows us to explore the similarities between K,Pthe theories they express as well as to delineate ac-tual distinctions between them.
We look briefly attwo of these issues.Together these examples are chosen to illustratethe main strengths of the model-theoretic approach,at least as embodied in L2K,p, as a framework forstudying theories of syntax: a focus on structuralproperties themselves, rather than on mechanismsfor specifying them or for generating or checkingstructures that exhibit them, and a language thatis expressive nough to state most linguistically sig-nificant properties in a natural way, but which isrestricted enough to have well-defined strong gener-ative capacity.2 L~,p - -The  Monad ic  Second-OrderLanguage o f  T reesL2K,p is the monadic second-order language overthe signature including a set of individual constants(K), a set of monadic predicates (P), and binarypredicates for immediate domination (,~), domina-tion (,~*), linear precedence (-~) and equality (..~).The predicates in P can be understood both aspicking out particular subsets of the tree and as(non-exclusive) labels or features decorating thetree.
Models for the language are labeled tree do-3We will refer to Gazdar et al (1985) as GKP&S4We should note that the definition of FSDs inGKP&S is, in fact, declarative although this is obscuredby the fact that it is couched in terms of an algorithmfor checking models.mains (Gorn, 1967) with the natural interpretationof the binary predicates.
In Rogers (1994) we haveshown that this language is equivalent in descrip-tive power to SwS--the monadic second-order the-ory of the complete infinitely branching tree--in thesense that sets of trees are definable in SwS iff theyare definable in L 2 This places it within a hi- K,P"erarchy of results relating language-theoretic com-plexity classes to the descriptive complexity of theirmodels: the sets of strings definable in S1S are ex-actly the regular sets (Biichi, 1960), the sets of fi-nite trees definable in SnS, for finite n, are the rec-ognizable sets (roughly the sets of derivation treesof CFGs) (Doner, 1970), and, it can be shown, thesets of finite trees definable in SwS are those gener-ated by generalized CFGs in which regular ,expres-sions may occur on the rhs of rewrite rules (Rogers,1996b).
5 Consequently, languages are definable inL2K,p iff they are strongly context-free in the mildlygeneralized sense of GPSG grammars.In restricting ourselves to the language of L 2 K,Pwe are restricting ourselves to reasoning in terms ofjust the predicates of its signature.
We can expandthis by defining new predicates, even higher-orderpredicates that express, for instance, properties ofor relations between sets, and in doing so we can usemonadic predicates and individual constants freelysince we can interpret hese as existentially boundvariables.
But the fundamental restriction of L 2 K,Pis that all predicates other than monadic first-orderpredicates must be explicitly defined, that is, theirdefinitions must resolve, via syntactic substitution,2 into formulae involving only the signature of LK, P.3 Feature  Spec i f i ca t ion  Defau l ts  inGPSGWe now turn to our first application--the def-inition of Feature Specification Defaults (FSDs)in GPSG.
6 Since GPSG is presumed to license(roughly) context-free languages, we are not con-cerned here with establishing language-theoreticcomplexity but rather with clarifying the linguis-tic theory expressed by GPSG.
FSDs specify con-ditions on feature values that must hold at a nodein a licensed tree unless they are overridden by someother component of the grammar; in particular, un-less they are incompatible with either a feature spec-ified by the ID rule licensing the node (inherited fea-tures) or a feature required by one of the agreementprinciples--the Foot Feature Principle (FFP), HeadFeature Convention (HFC), or Control AgreementPrinciple (CAP).
It is the fact that the default holds5There is reason to believe that this hierarchy canbe extended to encompass, at least, a variety of mildlycontext-sensitive languages as well.6A more complete treatment of GPSG in L 2 I?.,P canbe found in Rogers (1996c).12just in case it is incompatible with these other com-ponents that gives FSDs their dynamic flavor.
Note,though, in contrast o typical applications of defaultlogics, a GPSG grammar is not an evolving theory.The exceptions to the defaults are fully determinedwhen the grammar is written.
If we ignore for themoment he effect of the agreement principles, thedefaults are roughly the converse of the ID rules: anon-default feature occurs iff it is licensed by an IDrule.It is easy to capture ID rules in L 2 For instance K,P"the rule:VP , HI5\], NP, NPcan be expressed:IDh(x, yl, Y2, Y3) -=Children(x, Yl, Y2, Y3) A VP(x)AH(yl) A (SUBCAT, 5)(Yl) A NP(y2) A NP(y3),where Children(z, Yl, Y~, Y3) holds iff the set of nodesthat are children of x are just the Yi and VP,(SUBCAT, 5), etc.
are all members of p.7 A se-quence of nodes will satisfy ID5 iff they form a localtree that, in the terminology of GKP&S, is inducedby the corresponding ID rule.
Using such encodingswe can define a predicate Free/(x) which is true ata node x iff the feature f is compatible with theinherited features of x.The agreement principles require pairs of nodesoccurring in certain configurations in local trees toagree on certain classes of features.
Thus these prin-ciples do not introduce features into the trees, butrather propagate features from one node to another,possibly in many steps.
Consequently, these prin-ciples cannot override FSDs by themselves; ratherevery violation of a default must be licensed by aninherited feature somewhere in the tree.
In orderto account for this propagation of features, the def-inition of FSDs in GKP&S is based on identifyingpairs of nodes that co-vary wrt the relevant featuresin all possible extensions of the given tree.
As a re-suit, although the treatment in GKP&S is actuallydeclarative, this fact is far from obvious.Again, it is not difficult to define the configura-tions of local trees in which nodes are required toagree by FFP, CAP, or HFC in L 2 Let the predi- K,P"cate Propagatey(z, y) hold for a pair of nodes z andy iff they are required to agree on f by one of theseprinciples (and are, thus, in the same local tree).Note that Propagate is symmetric.
Following theterminology of GKP&S, we can identify the set ofnodes that are prohibited from taking feature f bythe combination of the ID rules, FFP, CAP, andHFC as the set of nodes that are privileged wrt f.This includes all nodes that are not Free for f as well7We will not elaborate here on the encoding of cat-egories in L 2 K,P, nor on non-finite ID schema like theiterating co-ordination schema.
These present no signif-icant problems.as any node connected to such a node by a sequenceof Propagate/ links.
We, in essence, define this in-ductively.
P' (X) is true of a set iff it includes all \]nodes not Free for f and is closed wrt Propagate/.PrivSet\] (X) is true of the smallest such set.P; (x)  -(Vx)\[- Frees (x) X(x)\]  ^(Vx)\[(3y)\[X(y) A Propagate\] (x, y)\] ---* X(x)\]PrivSetl(X) = P) (X)  A(VY)\[P) (Y) --~ Subset(X, Y)\].There are two things to note about this definition.First, in any tree there is a unique set satisfyingPrivSet/(X) and this contains exactly those nodesnot Free for f or connected to such a node byPropagate\].
Second, while this is a first-order in-ductive property, the definition is a second-order x-plicit definition.
In fact, the second-order quantifi-cation of L 2 allows us to capture any monadic K,Pfirst-order inductively or implicitly definable prop-erty explicitly.Armed with this definition, we can identify indi-viduals that are privileged wrt f simply as the mem-bers of PrivSetl.sPrivileged\] (x) = (3X)\[PrivSety (X) A X(z)\].One can define Privileged_,/(x) which holds when-ever x is required to take the feature f along similarlines.These, then, let us capture FSDs.
For the default\[-INV\], for instance, we get:(?x)\[-~Privileged\[_ INV\](X) ""+ \[-- INV\](x)\].For \[BAR0\] D,,~ \[PAS\] (which says that \[Bar 0\]nodes are, by default, not marked passive), we get:(Vz)\[ (\[BAR 0\](x) A ~Privileged_,\[pAs\](X))-~\[PAS\](x)\].The key thing to note about this treatment ofFSDs is its simplicity relative to the treatment ofGKP&S.
The second-order quantification allows usto reason directly in terms of the sequence of nodesextending from the privileged node to the local treethat actually licenses the privilege.
The immediatebenefit is the fact that it is clear that the property ofsatisfying a set of FSDs is a static property of labeledtrees and does not depend on the particular strategyemployed in checking the tree for compliance.SWe could, of course, skip the definition of PrivSet/and define Privilegedy(x) as (VX)\[P'(X) ---* Z(x)\], butwe prefer to emphasize the inductive nature of thedefinition.134 Chains in GBThe key issue in capturing GB theories within L 2 K,Pis the fact that the mechanism of free-indexation isprovably non-definable.
Thus definitions of prin-ciples that necessarily employ free-indexation haveno direct interpretation i L 2 (hardly surprising, K,Pas we expect GB to be capable of expressing non-context-free languages).
In many cases, though, ref-erences to indices can be eliminated in favor of theunderlying structural relationships they express.
9The most prominent example is the definition ofthe chains formed by move-a.
The fundamentalproblem here is identifying each trace with its an-tecedent without referencing their index.
Accountsof the licensing of traces that, in many cases ofmovement, replace co-indexation with governmentrelations have been offered by both Rizzi (1990)and Manzini (1992).
The key element of these ac-counts, from our point of view, is that the antecedentof a trace must be the closest antecedent-governor ofthe appropriate type.
These relationships are easyto capture in L 2 For A-movement, for instance, K,P"we have:A-Antecedent-Governs(x, y)-~A-pos(x) A C-Commands(x, y) A F.Eq(x, y) A- -x  is a potent ia l  antecedent  in anA-pos i t ion-~(3z)\[Intervening-Barrier(z, x, y)\] A- -no  bar r ie r  in tervenes-~(Bz)\[Spec(z) A-~A-pos(z) AC-Commands(z, x) A Intervenes(z, x y)\]- -min imal i ty  is respectedwhere F.Eq(x, y) is a conjunction of biconditionalsthat assures that x and y agree on the appropriatefeatures and the other predicates are are standardGB notions that are definable in L 2 K,P"Antecedent-government, in Rizzi's and Manzini'saccounts, is the key relationship between adjacentmembers of chains which are identified by non-referential indices, but plays no role in the definitionof chains which are assigned a referential index3 ?Manzini argues, however, that referential chains can-not overlap, and thus we will never need to distin-guish multiple referential chains in any single con-text.
Since we can interpret any bounded number ofindices simply as distinct labels, there is no difficultyin identifying the members of referential chains inL 2 On these and similar grounds we can extend K,P"these accounts to identify adjacent members of ref-erential chains, and, at least in the case of English,9More detailed expositions of the interpretation2 of GB in LK,p can be found in Rogers (1996a),Rogers (1995), and Rogers (1994).1?This accounts for subject/object asymmetries.of chains of head movement and of rightward move-ment.
This gives us five mutually exclusive relationswhich we can combine into a single link relation thatmust hold between every trace and its antecedent:Link(x,y) - A-Link(z, y) V A-Ref-Link(x, y) VA---Ref-Link(x, y) V X?-Link(x, y) VRight-Link(x, y).The idea now is to define chains as sequences ofnodes that are linearly ordered by Link, but beforewe can do this there is still one issue to resolve.While minimality ensures that every trace must havea unique antecedent, we may yet admit a single an-tecedent hat licenses multiple traces.
To rule outthis possibility, we require chains to be closed wrtthe link relation, i.e., every chain must include everynode that is related by Link to any node already inthe chain.
Our definition, then, is in essence the def-inition, in GB terms, of a discrete linear order withendpoints, augmented with this closure property.Chain(X) --(3!x)\[X(x) A Target(x)\] A- -X  conta ins  exact ly  one  Target(3!x)\[X(x) A Base(x)\] A- -and  one Base(Vx)\[X(x) A -~Warget(x) ---*(3!y)\[Z(y) A Link(y,x)\]\] A- -A l l  non-Target  have  a un ique  an-tecedent  in X(Vx)\[X(x) A-~Base(x) --~(3!y)\[X(y) A Link(x, y)\]\] A- -A l l  non-Base  have a un ique  suc-cessor in X(Vx, y)\[X(x) A (Link(x, y) V Link(y, x)) ---*X(y)\]- -X  is c losed wr t  the  L ink  re la t ionNote that every node will be a member of exactlyone (possibly trivial) chain.The requirement that chains be closed wrt Linkmeans that chains cannot overlap unless they are ofdistinct types.
This definition works for English be-cause it is possible, in English, to resolve chains intoboundedly many types in such a way that no twochains of the same type ever overlap.
In fact, it failsonly in cases, like head-raising in Dutch, where thereare potentially unboundedly many chains that mayoverlap a single point in the tree.
Thus, this gives usa property separating GB theories of movement thatlicense strongly context-free languages from thosethat potentially don't - - i f  we can establish a fixedbound on the number of chains that can overlap,then the definition we sketch here will suffice tocapture the theory in L 2 and, consequently, the K,Ptheory licenses only strongly context-free languages.14This is a reasonably natural diagnostic for context-freeness in GB and is close to common intuitionsof what is difficult about head-raising constructions;it gives those intuitions theoretical substance andprovides a reasonably clear strategy for establishingcontext-freeness.this distinction is; one particularly interesting ques-tion is whether it has empirical consequences.
It isonly from the model-theoretic perspective that thequestion even arises.6 Conclus ion5 A Compar ison and a ContrastHaving interpretations both of GPSG and of aGB account of English in L 2 provides a certain K,Pamount of insight into the distinctions between theseapproaches.
For example, while the explanations offiller-gap relationships in GB and GPSG are quitedramatically dissimilar, when one focuses on thestructures these accounts license one finds some sur-prising parallels.
In the light of our interpretation ofantecedent-government, one can understand the roleof minimality in l~izzi's and Manzini's accounts aseliminating ambiguity from the sequence of relationsconnecting the gap with its filler.
In GPSG this con-nection is made by the sequence of agreement rela-tionships dictated by the Foot Feature Principle.
Sowhile both theories accomplish agreement betweenfiller and gap through marking a sequence of ele-ments falling between them, the GB account marksas few as possible while the GPSG account marksevery node bf the spine of the tree spanning them.In both cases, the complexity of the set of licensedstructures can be limited to be strongly context-freeiff the number of relationships that must be distin-guished in a given context can be bounded.One finds a strong contrast, on the other hand, inthe way in which GB and GPSG encode languageuniversals.
In GB it is presumed that all princi-ples are universal with the theory being specializedto specific languages by a small set of finitely vary-ing parameters.
These principles are simply prop-erties of trees.
In terms of models, one can un-derstand GB to define a universal language--theset of all analyses that can occur in human lan-guages.
The principles then distinguish particularsub-languages--the ad-final or the pro-drop lan-guages, for instance.
Each realized human languageis just the intersection of the languages elected bythe settings of its parameters.
In GPSG, in contrast,many universals are, in essence, closure propertiesthat must be exhibited by human languages--if thelanguage includes trees in which a particular config-uration occurs then it includes variants of those treesin which certain related configurations occur.
Boththe ECPO principle and the metarules can be under-stood in this way.
Thus while universals in GB areproperties of trees, in GPSG they tend to be proper-ties of sets of trees.
This makes a significant differ-ence in capturing these theories model-theoretically;in the GB case one is defining sets of models, in theGPSG case one is defining sets of sets of models.
Itis not at all clear what the linguistic significance ofWe have illustrated a general formal framework forexpressing theories of syntax based on axiomatiz-ing classes of models in L 2 This approach as a K,P*number of strengths.
First, as should be clear fromour brief explorations of aspects of GPSG and GB~re-formalizations of existing theories within L 2 K,Pcan offer a clarifying perspective on those theories,and, in particular, on the consequences of individ-ual components of those theories.
Secondly, theframework is purely declarative and focuses on thoseaspects of language that are more or less directlyobservable--their structural properties.
It allows usto reason about the consequences of a theory with-out hypothesizing a specific mechanism implement-ing it.
The abstract properties of the mechanismsthat might implement hose theories, however, arenot beyond our reach.
The key virtue of descrip-tive complexity results like the characterizations oflanguage-theoretic complexity classes discussed hereand the more typical characterizations of computa-tional complexity classes (Gurevich, 1988; Immer-man, 1989) is that they allow us to determine thecomplexity of checking properties independently ofhow that checking is implemented.
Thus we can usesuch descriptive complexity results to draw conclu-sions about those abstract properties of such mech-anisms that are actually inferable from their observ-able behavior.
Finally, by providing a uniform repre-sentation for a variety of linguistic theories, it offersa framework for comparing their consequences.
Ul-timately it has the potential to reduce distinctionsbetween the mechanisms underlying those theoriesto distinctions between the properties of the sets ofstructures they license.
In this way one might hopeto illuminate the empirical consequences of these dis-tinctions, should any, in fact, exist.ReferencesBlackburn, Patrick, Claire Gardent, and WilfriedMeyer-Viol.
1993.
Talking about trees.
In EACL93, pages 21-29.
European Association for Com-putational Linguistics.Blackburn, Patrick and Wilfried Meyer-Viol.
1994.Linguistics, logic, and finite trees.
Bulletin of theIGPL, 2(1):3-29, March.Biichi, J. R. 1960.
Weak second-order arithmeticand finite automata.
Zeitschrift fiir malhemalis-che Logik und Grundlagen der Mathematik, 6:66-92.15Carpenter, Bob.
1992.
The Logic of Typed Fea-ture Structures; with Applications to UnificationGrammars, Logic Programs and Constraint Reso-lution.
Number 32 in Cambridge Tracts in The-oretical Computer Science.
Cambridge UniversityPress.Cornell, Thomas Longacre.
1992.
DescriptionTheory, Licensing Theory, and Principle-BasedGrammars and Parsers.
Ph.D. thesis, Universityof California Los Angeles.Dawar, Anuj and K. Vijay-Shanker.
1990.
An inter-pretation of negation in feature structure descrip-tions.
Computational Linguistics, 16(1):11-21.Doner, John.
1970.
Tree acceptors and some of theirapplications.
Journal of Computer and SystemSciences, 4:406-451.Gazdar, Gerald, Ewan Klein, Geoffrey Pullum, andIvan Sag.
1985.
Generalized Phrase StructureGrammar.
Harvard University Press.Gazdar, Gerald, Geoffrey Pullum, Robert Carpen-ter, Ewan Klein, T. E. Hukari, and R. D. Levine.1988.
Category structures.
Computational Lin-guistics, 14:1-19.Gorn, Saul.
1967.
Explicit definitions and linguisticdominoes.
In John F. Hart and Satoru Takasu,editors, Systems and Computer Science, Proceed-ings of the Conference held at Univ.
of WesternOntario, 1965.
Univ.
of Toronto Press.Gurevich, Yuri.
1988.
Logic and the challenge ofcomputer science.
In E. BSrger, editor, CurrentTrends in Theoretical Computer Science.
Com-puter Science Press, chapter 1, pages 1-57.Immerman, Neil.
1989.
Descriptive and compu-tational complexity.
In Proceedings of Symposiain Applied Mathematics, pages 75-91.
AmericanMathematical Society.Johnson, David E. and Paul M. Postal.
1980.Are Pair Grammar.
Princeton University Press,Princeton, New Jersey.Johnson, Mark.
1988.
Attribute- Value Logic and theTheory of Grammar.
Number 16 in CSLI LectureNotes.
Center for the Study of Language and In-formation, Stanford, CA.Johnson, Mark.
1989.
The use of knowledge oflanguage.
Journal of Psycholinguistic Research,18(1):105-128.Kasper, Robert T. and William C. Rounds.
1986.A logical semantics for feature structures.
In Pro-ceedings of the 2~th Annual Meeting of the Asso-ciation for Computational Linguistics.Kasper, Robert T. and William C. Rounds.
1990.The logic of unification in grammar.
Linguisticsand Philosophy, 13:35-58.Keller, Bill.
1993.
Feature Logics, Infinitary De-scriptions and Grammar.
Number 44 in CSLILecture Notes.
Center for the Study of Languageand Information.Kracht, Marcus.
1995.
Syntactic odes and gram-mar refinement.
Journal of Logic, Language, andInformation, 4:41-60.Manzini, Maria Rita.
1992.
Locality: A Theory andSome of Its Empirical Consequences.
MIT Press,Cambridge, Ma.Moshier, M. Drew and William C. Rounds.
1987.A logic for partially specified ata structures.
InACM Symposium on the Principles of Program-ming Languages.Rizzi, Luigi.
1990.
Relativized Minimality.
MITPress.Rogers, James.
1994.
Studies in the Logic of Treeswith Applications to Grammar Formalisms.
Ph.D.dissertation, Univ.
of Delaware.Rogers, James.
1995.
On descriptive complexity,language complexity, and GB.
In Patrick Black-burn and Maarten de Rijke, editors, SpecifyingSyntactic Structures.
In Press.
Also available asIRCS Technical Report 95-14. cmp-lg/9505041.Rogers, James.
1996a.
A Descriptive Approach toLanguage-Theoretic Complexity.
Studies in Logic,Language, and Information.
CSLI Publications.To appear.Rogers, James.
1996b.
The descriptive complexityof local, recognizable, and generalized recogniz-able sets.
Technical report, IRCS, Univ.
of Penn-sylvania.
In Preparation.Rogers, James.
1996c.
Grammarless phrase-structure grammar.
Under Review.Rogers, James and K. Vijay-Shanker.
1994.
Obtain-ing trees from their descriptions: An applicationto tree-adjoining grammars.
Computational Intel-ligence, 10:401-421.Smolka, Gert.
1989.
A feature logic with subsorts.LILOG Report 33, IBM Germany, Stuttgart.Stabler, Jr., Edward P. 1992.
The Logical Approachto Syntax.
Bradford.16
