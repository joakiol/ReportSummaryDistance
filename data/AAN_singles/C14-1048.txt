Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 497?507, Dublin, Ireland, August 23-29 2014.Learning Sense-specific Word Embeddings By ExploitingBilingual ResourcesJiang Guo?, Wanxiang Che?, Haifeng Wang?, Ting Liu??
?Research Center for Social Computing and Information RetrievalHarbin Institute of Technology, China?Baidu Inc., Beijing, China{jguo, car, tliu}@ir.hit.edu.cnwanghaifeng@baidu.comAbstractRecent work has shown success in learning word embeddings with neural network languagemodels (NNLM).
However, the majority of previous NNLMs represent each word with a singleembedding, which fails to capture polysemy.
In this paper, we address this problem by represent-ing words with multiple and sense-specific embeddings, which are learned from bilingual paralleldata.
We evaluate our embeddings using the word similarity measurement and show that our ap-proach is significantly better in capturing the sense-level word similarities.
We further feed ourembeddings as features in Chinese named entity recognition and obtain noticeable improvementsagainst single embeddings.1 IntroductionWord embeddings are conventionally defined as compact, real-valued, and low-dimensional vector rep-resentations for words.
Each dimension of word embedding represents a latent feature of the word, hope-fully capturing useful syntactic and semantic characteristics.
Word embeddings can be used straightfor-wardly for computing word similarities, which benefits many practical applications (Socher et al., 2011;Mikolov et al., 2013a).
They are also shown to be effective as input to NLP systems (Collobert et al.,2011) or as features in various NLP tasks (Turian et al., 2010; Yu et al., 2013).In recent years, neural network language models (NNLMs) have become popular architectures forlearning word embeddings (Bengio et al., 2003; Mnih and Hinton, 2008; Mikolov et al., 2013b).
Mostof the previous NNLMs represent each word with a single embedding, which ignores polysemy.
In anattempt to better capture the multiple senses or usages of a word, several multi-prototype models havebeen proposed (Reisinger and Mooney, 2010; Huang et al., 2012).
These multi-prototype models simplyinduce K prototypes (embeddings) for every word in the vocabulary, where K is predefined as a fixedvalue.
These models still may not capture the real senses of words, because different words may havedifferent number of senses.We present a novel and simple method of learning sense-specific word embeddings by using bilingualparallel data.
In this method, word sense induction (WSI) is performed prior to the training of NNLMs.We exploit bilingual parallel data for WSI, which is motivated by the intuition that the same word in thesource language with different senses is supposed to have different translations in the foreign language.1For instance,??
can be translated as investment / overpower / subdue / subjugate / uniform, etc.
Amongall of these translations, subdue / overpower / subjugate express the same sense of?
?, whereas uniform/ investment express a different sense.
Therefore, we could effectively obtain the senses of one word byclustering its translation words, exhibiting different senses in different clusters.The created clusters are then projected back into the words in the source language texts, forming asense-labeled training data.
The sense-labeled data are then trained with recurrent neural network langu-gae model (RNNLM) (Mikolov, 2012), a kind of NNLM, to obtain sense-specific word embeddings.
Asa concrete example, Figure 1 illustrates the process of learning sense-specific embeddings.
?Email correspondence.1In this paper, source language refers to Chinese, whereas foreign language refers to English.This work is licenced under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedings footerare added by the organisers.
Licence details: http:// creativecommons.org/licenses/by/4.0/497?
ClusterMonolingualsense-labeled data?
??
#2 , ??
?
??
??
?1??
?
?
??
#1  ?
??
?2?
??
#2 ?
??
?3?
??
?
??
??
??
?
??
#1 ?4??5?
Project?
Extract?
RNNLMBilingual data(E: English, C: Chinese)E: ?
subdue , conquer or control the opponent .C: ?
??
, ??
?
??
??
?1E: The workers wearing the factory 's uniform ?C: ??
?
?
??
?
??
?
2E: She overpowered the burglars .C: ?
??
?
??
?3E: They wore their priestly vestment in Church .C: ?
??
?
??
??
??
?
??
?4?
?5SL word ??Translationssubdueuniformoverpowersubjugatevestment??uniformvestmentoverpowersubjugatesubdue??#1?
?#2(clothes)(defeat)Sense-specific word embeddings< v1#1, v2#1, ..., vN#1 >< v1#2, v2#2, ..., vN#2 >??
#1??
#2Figure 1: An illustration of the proposed method.
SL stands for source language.To evaluate the sense-specific word embeddings we have learned, we manually construct a Chinesepolysemous word similarity dataset that contains 401 pairs of words with human-judged similarities.
Theperformance of our method on this dataset shows that sense-specific embeddings are significantly betterin capturing the sense-level similarities for polysemous words.We also evaluate our embeddings by feeding them as features to the task of Chinese named entityrecognition (NER), which is a simple semi-supervised learning mechanism (Turian et al., 2010).
In or-der to use sense-specific embeddings as features, we should discriminate the word senses for the NERdata first.
Therefore, we further develop a novel monolingual word sense disambiguation (WSD) algo-rithm based on the RNNLM we have already trained previously.
NER results show that sense-specificembeddings provide noticeable improvements over traditional single embeddings.Our contribution in this paper is twofold:?
We propose a novel approach of learning sense-specific word embeddings by utilizing bilingualparallel data (Section 3).
Evaluation on a manually constructed polysemous word similarity datasetshows that our approach better captures word similarities (Section 5.2).?
To use the sense-specific embeddings in practical applications, we develop a novel WSD algorithmfor monolingual data based on RNNLM (Section 4).
Using the algorithm, we feed the sense-specificembeddings as additional features to NER and achieve significant improvement (Section 5.3).2 Background: Word Embedding and RNNLMThere has been a line of research on learning word embeddings via NNLMs (Bengio et al., 2003; Mnihand Hinton, 2008; Mikolov et al., 2013b).
NNLMs are language models that exploit neural networks tomake probabilistic predictions of the next word given preceding words.
By training NNLMs, we obtainboth high performance language models and word embeddings.Following Mikolov et al.
(2013b), we use the recurrent neural network as the basic framework for train-ing NNLMs.
RNNLM has achieved the state-of-the-art performance in language modeling (Mikolov,2012) and learned effective word embeddings for several tasks (Mikolov et al., 2013b).
The architectureof RNNLM is shown in Figure 2.The input layer of RNNLM consists of two components: w(t) and h(t ?
1).
w(t) is the one-hotrepresentation of the word at time step t,2h(t ?
1) is the output of hidden layer at the last time step.Therefore, the input encodes all previous history when predicting the next word at time step t. Compared2A feature vector of the same size of the vocabulary, and only one dimension is on.498w(t)y(t)UWVh(t)h(t-1)Figure 2: The basic architecture of RNNLM.with other feed-forward NNLMs, the RNNLM can theoretically represent longer context patterns.
Theoutput y(t) represents the probability distribution of the next word p(w(t + 1)|w(t),h(t ?
1)).
Theoutput values are computed as follows:h(t) = f(Uw(t) +Wh(t?
1)) (1)y(t) = g(Vh(t)) (2)where f is a sigmoid function and g is a softmax function.The RNNLM is trained by maximizing the log-likelihood of the training data using stochastic gradi-ent descent (SGD), in which back propagation through time (BPTT) is used to efficiently compute thegradients.
In the RNNLM, U is the embedding matrix, where each column vector represents a word.As discussed in Section 1, the RNNLM and even most NNLMs ignore the polysemy phenomenon innatural languages and induce a single embedding for each word.
We address this issue and introduce aneffective approach for capturing polysemy in the next section.3 Sense-specific Word Embedding LearningIn our approach, WSI is performed prior to the training of word embeddings.
Inspired by Gale et al.
(1992) and Chan and Ng (2005), who used bilingual data for automatically generating training examplesof WSD, we present a bilingual approach for unsupervised WSI, as shown in Figure 1.
First, we extractthe translations of the source language words from bilingual data (?).
Since there may be multipletranslations for the same sense of a source language word, it is straightforward to cluster the translationwords, exhibiting different senses in different clusters (?
).Once word senses are effectively induced for each word, we are able to form the sense-labeled trainingdata of RNNLMs by tagging each word occurrence in the source language text with its associated sensecluster (?).
Finally, the sense-tagged corpus is used to train the sense-specific word embeddings in astandard manner (?
).3.1 Translation Words ExtractionGiven bilingual data after word alignment, we present a way of extracting translation words for sourcelanguage words by exploiting the translation probability produced by word alignment models (Brown etal., 1993; Och and Ney, 2003; Liang et al., 2006).More formally, we notate the Chinese sentence as c = (c1, ..., cI) and English sentence as e =(e1, ..., eJ).
The alignment models can be generally factored as:p(c|e) =?ap(a, c|e) (3)p(a, c|e) =?Jj=1pd(aj|aj?, j)pt(cj|eaj) (4)where a is the alignment specifying the position of an English word aligned to each Chinese word,pd(aj|aj?, j) is the distortion probability, and pt(cj|eaj) is the translation probability which we use.499SL Word Translation Words Translation Word Clusters Nearest Neighbours??
investment, overpower,subdue, subjugate, uniforminvestment, uniform ??dress,?
?policeman uniformsubdue, subjugate, overpower ??defeat,??beat,??conquer?
blossom, cost, flower,spend, take, tookflower, blossom ?greens,?leaf,?
?fruittake, cost, spend ??cost,??save,??rest?
act, code, France,French, law, methodFrance, French ?Germany,?Russia,?Britainlaw, act, code ??ordinance,??bill,?
?rulemethod ??concept,??scheme,??way?
?lead, leader, leadershipleader, leadership ??chief,??boss,?
?chairmanlead ??supervise,??decision,?
?workTable 1: Results of our approach on a sample of polysemous words.
The second column lists the extractedtranslation words of the source language word (Section 3.1).
The third column lists the clustering resultsusing affinity propagation (Section 3.2).
The last column lists the nearest neighbour words computedusing the learned sense-specific word embeddings (Section 5.2.2).In this paper, we use the alignment model proposed by Liang et al.
(2006).
We utilize the bidirectionaltranslation probabilities for the extraction of translations, where a foreign language word weis deter-mined as a translation of source language word wconly if both translation probabilities pt(wc|we) andpt(we|wc) exceed some threshold 0 < ?
< 1.The second column of Table 1 presents the extraction results on a sample of source language wordswith the corresponding translation words.3.2 Clustering of Translation WordsFor each source language word, its translation words are then clustered so as to separate different senses.At the clustering time, we first represent each translation word with a feature vector (point), so thatwe can measure the similarities between points.
Then we perform clustering on these feature vectors,representing different senses in different clusters.Different from Apidianaki (2008) who represents all occurrences of the translation words with theircontexts in the foreign language for clustering, we adopt the embeddings of the translation words asthe representations and directly perform clustering on the translation words,3rather than the contexts ofoccurrences.
The embedding representation is chosen for two reasons: (1) Word embeddings encode richlexical semantics.
They can be directly used to measure word similarities.
(2) Embedding representationof the translation words leads to extremely high-efficiency clustering, because the number of translationwords is orders of magnitude less than their occurrences.Moreover, since the number of senses of different source language words is varied, the commonly-used k-means algorithm becomes inappropriate for this situation.
Instead, we employ affinity propaga-tion (AP) algorithm (Frey and Dueck, 2007) for clustering.
In AP, each cluster is represented by oneof the samples of it, which we call an exemplar.
AP finds the exemplars iteratively based on the con-cept of ?message passing?.
AP has the major advantage that the number of the resulting clusters isdynamic, which mainly depends on the distribution of the data.
Compared with other possible clusteringapproaches, such as hierarchical agglomerative clustering (Kartsaklis et al., 2013), AP determines thenumber of resulting clusters automatically without using any partition criterions.The third column of Table 1 lists the resulting clusters of the translation words for the sampled pol-ysemous words.
We can see that the resulting clusters are meaningful: senses are well represented byclusters of translation words.3.3 Cross-lingual Word Sense ProjectionThe produced clusters are then projected back into the source language to identify word senses.3The publicly available word embeddings proposed by Collobert et al.
(2011) are used.500For each occurrence woof the word w in the source language corpora, we first select the aligned wordwith the highest marginal edge posterior (Liang et al., 2006) as its translation.
We then identify the senseof woby computing the similarities of its translation word with each exemplar of the clusters, and selectthe one with the maximum similarity.
When wois aligned with NULL, we heuristically identify its senseas the most frequent sense of w that appears in the bilingual dataset.After projecting the word senses into the source language, we obtain a sense-labeled corpus, which isused to train the sense-specific word embeddings with RNNLM.
The training process is exactly the sameas single embeddings, except that the words in our training corpus has been labeled with senses.4 Application of Sense-specific Word EmbeddingsOne of the attractive characteristic of word embeddings is that they can be directly used as word featuresin various NLP applications, including NER, chunking, etc.
Despite of the usefulness of word embed-dings on these applications, previous work seldom concerns that words may have multiple senses, whichcannot be effectively represented with single embeddings.
In this section, we address this problem byutilizing sense-specific word embeddings.We take the task of Chinese NER as a case study.
Intuitively, word senses are important in NER.
Forinstance,?
is likely to be an NE of Location when it refers to America.
However, when it expresses thesense of beautiful, it should not be an NE.Using sense-specific word embedding features for NER is not as straightforward as using single em-beddings.
For each word in the NER data, we first need to determine the correct word sense of it, whichis a typical WSD problem.
Then we use the embedding which corresponds to that sense as features.Here we treat WSD as a sequence labeling problem, and solve it with a very natural algorithm based onRNNLM we have already trained (Section 3).4.1 RNNLM-based Word Sense DisambiguationGiven the automatically induced word sense inventories and the RNNLM which has already been trainedon the sense-labeled data of source language, we first develop a greedy decoding algorithm for thesequential WSD, which works deterministically.
Then we improve it using beam search.Greedy.
For word w, we denote the sense-labeled w as wsk, where skrepresents the kthsense of w.In each step, a single decision is made and the sense of next word (w(t + 1)) which has the maximumRNNLM output is chosen, given the current (sense-labeled) word w(t)s?and the hidden layer h(t?
1)at the last time step as input.
We simply need to compute a shortlist of y(t) associated with w(t + 1),that is, y(t)|w(t+1)at each step.
This process is illustrated in Figure 3.Beam search.
The greedy procedure described above can be improved using a left-to-right beamsearch decoding for obtaining a better sequence.
The beam-search decoding algorithm keeps B differentsequences of decisions in the agenda, and the sequence with the best overall score is chosen as the finalsense sequence.Note that the dynamic programming decoding (e.g.
viterbi) is not applicable here, because of therecurrent characteristic of RNNLM.
At each step, decisions made by RNNLM depends on all previousdecisions instead of the previous state only, hence markov assumption is not satisfied.5 Experiments5.1 Experimental SettingsThe Chinese-English parallel datasets we use include LDC03E24, LDC04E12 (1998), the IWSLT 2008evaluation campaign dataset and the PKU 863 parallel dataset.
All corpora are sentence-aligned.
Aftercleaning and filtering the corpus,4we obtain 918,681 pairs of sentences (21.7M words).In this paper, we use BerkeleyAligner to produce word alignments over the parallel dataset.5Berke-leyAligner also gives translation probabilities and marginal edge posterior probabilities.
We adopt the4Sentences that are too long (more than 40 words) or too short (less than 10 words) are discarded.5code.google.com/p/berkeleyaligner/501h(t-1)UWVh(t)Next word (sense-labeled)Last word(sense-labeled)w(t)s*w(t+1)s1y(t)|w(t+1)w(t+1)s2w(t+1)sK?y(t)?w(t+1)s*maxw(t)s* w(t+1)s*w(t+1)s* w(t+2)s*w(t+2)s* w(t+3)s*h(t)h(t+1)h(t+2)h(t-1)Shortlist?Figure 3: Using RNNLM for WSD by sequential labeling (left).
Decision at each step of the RNNLM-based WSD algorithm (right).scikit-learn tool (Pedregosa et al., 2011) to implement the AP clustering algorithm.6The AP algorithmis not fully automatic in deciding the cluster number.
There is a tunable parameter calls preference.
Apreference with a larger value encourages more clusters to be produced.
We set the preference at themedian value of the input similarity matrix to obtain a moderate number of clusters.
The rnnlm toolkitdeveloped by Mikolov et al.
(2011) is used to train RNNLM and obtain word embeddings.7We induceboth single and sense-specific embeddings with 50 dimensions.
Finally, We obtain embeddings of avocabulary of 217K words, with a proportion of 8.4% having multiple sense clusters.5.2 Evaluation on Word SimilarityWord embeddings can be directly used for computing similarities between words, which benefits manypractical applications.
Therefore, we first evaluate our embeddings using a similarity measurement.Word similarities are calculated using the MaxSim and AvgSim metric (Reisinger and Mooney, 2010):MaxSim(u, v) = max1?i?ku,1?j?kvs(ui, vj) (5)AvgSim(u, v) =1ku?kv?kui=1?kvj=1s(ui, vj) (6)where kuand kvare the number of the induced senses for words u and v, respectively.
s(?, ?)
can be anystandard similarity measure.
In this study, we use the cosine similarity.Previous works used the WordSim-353 dataset (Finkelstein et al., 2002) or the Chinese version (Jin andWu, 2012) for the evaluation of general word similarity.
These datasets rarely contain polysemous words,and thus is unsuitable for our evaluation.
To the best of our knowledge, no datasets for polysemous wordsimilarity evaluation have been published yet, either in English or Chinese.
In order to fill this gap in theresearch community, we manually construct a Chinese polysemous word similarity dataset.5.2.1 Chinese Polysemous Word Similarity Dataset ConstructionWe adopt the HowNet database (Dong and Dong, 2006) in constructing the dataset.
HowNet is a Chineseknowledge database that maintains comprehensive semantic definitions for each word in Chinese.
Theprocess of the dataset construction includes three steps: (1) Commonly used polysemous words areextracted according to their sense definitions in HowNet.
(2) For each polysemous word, we selectseveral other words to form word pairs with it.
(3) Each word pair is manually annotated with similarity.In step (1), we mainly took advantage of HowNet for the selection of polysemous words.
However,the synsets defined in HowNet are often too fine-grained and many of them are difficult to distinguish,6scikit-learn.org7www.fit.vutbr.cz/?imikolov/rnnlm/502particularly for non-experts.
Therefore, we manually discard those words with senses that are hard todistinguish.In step (2), for each polysemous word w selected in step 1, we sample several other words to formword pairs withw.
The sampled words can be roughly divided into two categories: related and unrelated.The related words are sampled manually.
They can be the hypernym, hyponym, sibling, (near-)synonym,antonym, or topically related to one sense of w. The unrelated words are sampled randomly.In step (3), we ask six graduate students who majored in computational linguistics to assign each wordpair a similarity score.
Following the setting of WordSim-353, we restrict the similarity score in the range(0.0, 10.0).
To address the inconsistency of the annotations, we discard those word pairs with a standarddeviation greater than 1.0.
We end up with 401 word pairs annotated with acceptable consistency.
Unlikethe WordSim-353, in which most of the words are nouns, the words in our dataset are more diverse interms of part-of-speech tags.Table 2 lists a sample of word pairs with annotated similarities from the dataset.
The whole evaluationdataset will be publicly available for the research community.8Word Paired word Category Mean.Sim Std.Dev??
?
?conquer synonym 8.60 0.29?
?key pointunrelated 0.12 0.19?
?enter autonym 7.90 0.97?
?publishnear-synonym 7.86 0.76?
?plant stem sibling 7.80 0.12?
?costtopic-related 5.86 0.90?
?
?foodhypernym 6.50 0.71Table 2: Sample word pairs of our dataset.
The unrelated words are randomly sampled.
Mean.Simrepresents the mean similarity of the annotations, Std.Dev represents the standard deviation.5.2.2 Evaluation ResultsFollowing Zou et al.
(2013), we use Spearman?s ?
correlation and Kendall?s ?
correlation for evaluation.The results are shown in Table 3.
By utilizing sense-specific embeddings, our approach significantlyoutperforms the single-version using either MaxSim or AvgSim measurement.For comparison with multi-prototype methods, we borrow the context-clustering idea from Huang etal.
(2012), which was first presented by Sch?utze (1998).
The occurrences of a word are represented bythe average embeddings of its context words.
Following Huang et al.
?s settings, we use a context windowof size 10 and all occurrences of a word are clustered using the spherical k-means algorithm, where k istuned with a development set and finally set to 2.SystemMaxSim AvgSim?
?100 ?
?100 ?
?100 ?
?100Ours 55.4 40.9 49.3 35.2SingleEmb 42.8 30.6 42.8 30.6Multi-prototype 40.7 29.1 38.3 27.4Table 3: Spearman?s ?
correlation and Kendall?s ?
correlation evaluated on the polysemous dataset.Surprisingly, the multi-prototype method performs even slightly worse than the single-version, whichsuggests that learning a fixed number of embeddings for every word may even harm the embedding.Additionally, the clustering process of the multi-prototype approach suffers from high memory and timecost, especially for the high-frequency words.8ir.hit.edu.cn/?jguo503To obtain intuitive insight into the superior performance of sense-specific embeddings, we list in thelast column of Table 1 the nearest neighborhoods of the sampled words in the evaluation dataset.
The listshows that we are able to find the different meanings of a word by using sense-specific embeddings.5.3 Application on Chinese NERWe further apply the sense-specific embeddings as features to Chinese NER.
We first perform WSD onthe NER data using the algorithm introduced in Section 4.
For beam search decoding, the beam size Bis tuned on a development set and is finally set to 16.We conduct our experiments on data from People?s Daily (Jan. and Jun.
1998).9The original corpuscontains seven NE types.10In this study, we select the three most common NE types: Person, Location,Organization.
The data from January are chosen as the training set (37,426 sentences).
The first 2,000sentences from June are chosen as the development set and the next 8,000 sentences as the test set.CRF models are used in our NER system and are optimized by L2-regularized SGD.
We use theCRFSuite (Okazaki, 2007) because it accepts feature vectors with numerical values.
The state-of-the-artfeatures (Che et al., 2013) are used in our baseline system.
For both single and sense-specific embeddingfeatures, we use a window size of 4 (two words before and two words after).5.3.1 ResultsTable 4 demonstrates the performance of NER on the test set.
As desired, the single embedding featuresimprove the performance of our baseline, which were also shown in (Turian et al., 2010).
Furthermore,the sense-specific embeddings outperform the single word embeddings by nearly 1% F-score (88.56 vs.87.58), which is statistically significant (p-value < 0.01 using one-tail t-test).System P R FBaseline 93.27 81.46 86.97+SingleEmb 93.55 82.32 87.58+SenseEmb (greedy) 93.38 83.56 88.20+SenseEmb (beam search) 93.59 84.05 88.56Table 4: Performance of NER on test data.According to our hypothesis, the sense-specific embeddings should bring considerable improvementsto the NER of polysemous words.
To verify this, we evaluate the per-token accuracy of the polysemouswords in the NER test data.
We again adopt HowNet to determine the polysemy.
Words that are definedwith multiple senses are selected as test set.
Figure 4 shows that the sense-specific embeddings indeedimprove the NE recognition of the polysemous words, whereas the single embeddings even decrease theaccuracy slightly.
We also obtain improvements on the NE recognition of the monosemous words, whichprovide evidences that more accurate prediction of polysemous words is beneficial for the prediction ofthe monosemous words through contextual influence.6 Related WorkPrevious studies have explored the NNLMs, which predict the next word given some history or futurewords as context within a neural network architecture.
Schwenk and Gauvain (2002), Bengio et al.
(2003), Mnih and Hinton (2007), and Collobert et al.
(2011) proposed language models based on feed-forward neural networks.
Mikolov et al.
(2010) studied language models based on RNN, which managedto represent longer history information for word-predicting and demonstrated outstanding performance.Besides, researchers have also explored the word embeddings learned by NNLMs.
Collobert et al.
(2011) used word embeddings as the input of various NLP tasks, including part-of-speech tagging,chunking, NER, and semantic role labeling.
Turian et al.
(2010) made a comprehensive comparisonof various types of word embeddings as features for NER and chunking.
In addition, word embeddings9www.icl.pku.edu.cn/icl groups/corpus/dwldform1.asp10Person, Location, Organization, Date, Time, Number and Miscellany504Polysemous(2) Polysemous(3) Monosemousper?tokenaccuracy0.600.650.700.750.800.850.90 Baseline +SingleEmb +SenseEmbFigure 4: Per-token accuracy on the polysemous and monosemous words in the NER test data.
Polyse-mous(k) represents the set of words that have more than or equal to k senses defined in HowNet.are shown to capture many relational similarities, which can be recovered by vector arithmetic in theembedding space (Mikolov et al., 2013b; Fu et al., 2014).
Klementiev et al.
(2012) and Zou et al.
(2013)learned cross-lingual word embeddings by utilizing MT word alignments in bilingual parallel data toconstrain translational equivalence.Most previous NNLMs induce single embedding for each word, ignoring the polysemous property oflanguages.
In an attempt to capture the different senses or usage of a word, Reisinger and Mooney (2010)and Huang et al.
(2012) proposed multi-prototype models for inducing multiple embeddings for eachword.
They did this by clustering the contexts of words.
These multi-prototype models simply induceda fixed number of embeddings for every word, regardless of the real sense capacity of the specific word.There has been a lot of work on using bilingual resources for word sense disambiguation (Gale etal., 1992; Chan and Ng, 2005).
By using aligned bilingual data along with word sense inventories suchas WordNet, training examples for WSD can be automatically gathered.
We employ this idea for wordsense induction in our study, which is free of any pre-defined word sense thesaurus.The most similar work to our sense induction method is Apidianaki (2008).
They presented a methodof sense induction by clustering all occurrences of each word?s translation words.
In their approach,occurrences are represented with their contexts.
We suggest that clustering contexts suffer from highmemory and time cost, as well as data sparsity.
In our method, by clustering the embeddings of transla-tion words, we induce word senses much more efficiently.To evaluate word similarity models, researchers often apply a dataset with human-judged similaritieson word pairs, such as WordSim-353 (Finkelstein et al., 2002), MC (Miller and Charles, 1991), RG(Rubenstein and Goodenough, 1965) and Jin and Wu (2012).
For context-based multi-prototype mod-els, (Huang et al., 2012) constructs a dataset with context-dependent word similarity.
To the best ofour knowledge, there is no publicly available datasets for context-unaware polysemous word similarityevaluation yet.
This paper fills this gap.7 ConclusionThis paper presents a novel and effective approach of producing sense-specific word embeddings byexploiting bilingual parallel data.
The proposed embeddings are expected to capture the multiple sensesof polysemous words.
Evaluation on a manually annotated Chinese polysemous word similarity datasetshows that the sense-specific embeddings significantly outperforms the single embeddings and the multi-prototype approach.Another contribution of this study is the development of a beam-search decoding algorithm based onRNNLM for monolingual WSD.
This algorithm bridges the proposed sense-specific embeddings andpractical applications, where no bilingual information is provided.
Experiments on Chinese NER showthat the sense-specific embeddings indeed improve the performance, especially for the recognition of thepolysemous words.505AcknowledgmentsWe are grateful to Dr. Zhenghua Li, Yue Zhang, Shiqi Zhao, Meishan Zhang and the anonymous review-ers for their insightful comments and suggestions.
This work was supported by the National Key BasicResearch Program of China via grant 2014CB340503 and 2014CB340505, the National Natural ScienceFoundation of China (NSFC) via grant 61370164.ReferencesMarianna Apidianaki.
2008.
Translation-oriented word sense induction based on parallel corpora.
In In Proceed-ings of the 6th Conference on Language Resources and Evaluation (LREC), Marrakech, Morocco.Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and Christian Janvin.
2003.
A neural probabilistic languagemodel.
The Journal of Machine Learning Research, 3:1137?1155.Peter F Brown, Vincent J Della Pietra, Stephen A Della Pietra, and Robert L Mercer.
1993.
The mathematics ofstatistical machine translation: Parameter estimation.
Computational linguistics, 19(2):263?311.Yee Seng Chan and Hwee Tou Ng.
2005.
Scaling up word sense disambiguation via parallel texts.
In AAAI,volume 5, pages 1037?1042.Wanxiang Che, Mengqiu Wang, Christopher D. Manning, and Ting Liu.
2013.
Named entity recognition withbilingual constraints.
In Proceedings of the 2013 Conference of the North American Chapter of the Associationfor Computational Linguistics: Human Language Technologies, pages 52?62, Atlanta, Georgia, June.Ronan Collobert, Jason Weston, L?eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011.Natural language processing (almost) from scratch.
The Journal of Machine Learning Research, 12:2493?2537.Zhendong Dong and Qiang Dong.
2006.
HowNet and the Computation of Meaning.
World Scientific.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin.2002.
Placing search in context: The concept revisited.
ACM Transactions on Information Systems, 20(1):116?131.Brendan J Frey and Delbert Dueck.
2007.
Clustering by passing messages between data points.
science,315(5814):972?976.Ruiji Fu, Jiang Guo, Bing Qin, Wanxiang Che, Haifeng Wang, and Ting Liu.
2014.
Learning semantic hierar-chies via word embeddings.
In Proceedings of the 52th Annual Meeting of the Association for ComputationalLinguistics: Long Papers-Volume 1, Baltimore MD, USA.William A Gale, Kenneth W Church, and David Yarowsky.
1992.
Using bilingual materials to develop word sensedisambiguation methods.
In Proceedings of the 4th International Conference on Theoretical and Methodologi-cal Issues in Machine Translation, pages 101?112.Eric H Huang, Richard Socher, Christopher D Manning, and Andrew Y Ng.
2012.
Improving word representationsvia global context and multiple word prototypes.
In Proceedings of the 50th Annual Meeting of the Associationfor Computational Linguistics: Long Papers-Volume 1, pages 873?882.Peng Jin and Yunfang Wu.
2012.
Semeval-2012 task 4: evaluating chinese word similarity.
In Proceedings of theFirst Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conferenceand the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation,pages 374?377.Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, and Stephen Pulman.
2013.
Separating disambiguation from composi-tion in distributional semantics.
CoNLL-2013, pages 114?123.Alexandre Klementiev, Ivan Titov, and Binod Bhattarai.
2012.
Inducing crosslingual distributed representationsof words.
In Proceedings of COLING 2012, pages 1459?1474, Mumbai, India, December.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Alignment by agreement.
In Proceedings of the main conferenceon Human Language Technology Conference of the North American Chapter of the Association of Computa-tional Linguistics, pages 104?111.Tomas Mikolov, Martin Karafi?at, Luk?a?s Burget, Jan Cernocky, and Sanjeev Khudanpur.
2010.
Recurrent neuralnetwork based language model.
In Proceedings of Interspeech, pages 1045?1048.506Tomas Mikolov, Stefan Kombrink, Anoop Deoras, Lukar Burget, and J?Cernock`y.
2011.
Rnnlm-recurrent neuralnetwork language modeling toolkit.
In Proc.
of the 2011 ASRU Workshop, pages 196?201.Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.
2013a.
Efficient estimation of word representationsin vector space.
arXiv preprint arXiv:1301.3781.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b.
Linguistic regularities in continuous space wordrepresentations.
In Proceedings of NAACL-HLT, pages 746?751.Tomas Mikolov.
2012.
Statistical Language Models Based on Neural Networks.
Ph.D. thesis, Ph.
D. thesis, BrnoUniversity of Technology.George A Miller and Walter G Charles.
1991.
Contextual correlates of semantic similarity.
Language andcognitive processes, 6(1):1?28.Andriy Mnih and Geoffrey Hinton.
2007.
Three new graphical models for statistical language modelling.
InProceedings of the 24th international conference on Machine learning, pages 641?648.Andriy Mnih and Geoffrey E Hinton.
2008.
A scalable hierarchical distributed language model.
In Advances inneural information processing systems, pages 1081?1088.Franz Josef Och and Hermann Ney.
2003.
A systematic comparison of various statistical alignment models.Computational linguistics, 29(1):19?51.Naoaki Okazaki.
2007.
Crfsuite: a fast implementation of conditional random fields (crfs).
URL http://www.chokkan.
org/software/crfsuite.F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss,V.
Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay.
2011.
Scikit-learn: Machine learning in Python.
Journal of Machine Learning Research, 12:2825?2830.Joseph Reisinger and Raymond J Mooney.
2010.
Multi-prototype vector-space models of word meaning.
InHuman Language Technologies: The 2010 Annual Conference of the North American Chapter of the Associationfor Computational Linguistics, pages 109?117.Herbert Rubenstein and John B Goodenough.
1965.
Contextual correlates of synonymy.
Communications of theACM, 8(10):627?633.Hinrich Sch?utze.
1998.
Automatic word sense discrimination.
Computational linguistics, 24(1):97?123.Holger Schwenk and Jean-Luc Gauvain.
2002.
Connectionist language modeling for large vocabulary continuousspeech recognition.
In Acoustics, Speech, and Signal Processing (ICASSP), 2002 IEEE International Confer-ence on, volume 1, pages 765?768.Richard Socher, Eric H Huang, Jeffrey Pennin, Christopher D Manning, and Andrew Ng.
2011.
Dynamic poolingand unfolding recursive autoencoders for paraphrase detection.
In Advances in Neural Information ProcessingSystems, pages 801?809.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.
Word representations: a simple and general method forsemi-supervised learning.
In Proceedings of the 48th Annual Meeting of the Association for ComputationalLinguistics, pages 384?394.Mo Yu, Tiejun Zhao, Daxiang Dong, Hao Tian, and Dianhai Yu.
2013.
Compound embedding features for semi-supervised learning.
In Proceedings of NAACL-HLT, pages 563?568.Will Y. Zou, Richard Socher, Daniel Cer, and Christopher D. Manning.
2013.
Bilingual word embeddings forphrase-based machine translation.
In Proceedings of the 2013 Conference on Empirical Methods in NaturalLanguage Processing, pages 1393?1398, Seattle, Washington, USA, October.507
