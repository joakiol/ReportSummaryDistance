Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 563?568,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsA Hybrid Approach to Skeleton-based TranslationTong Xiao?
?, Jingbo Zhu?
?, Chunliang Zhang???
Northeastern University, Shenyang 110819, China?
Hangzhou YaTuo Company, 358 Wener Rd., Hangzhou 310012, China{xiaotong,zhujingbo,zhangcl}@mail.neu.edu.cnAbstractIn this paper we explicitly consider sen-tence skeleton information for MachineTranslation (MT).
The basic idea is thatwe translate the key elements of the inputsentence using a skeleton translation mod-el, and then cover the remain segments us-ing a full translation model.
We apply ourapproach to a state-of-the-art phrase-basedsystem and demonstrate very promisingBLEU improvements and TER reductionson the NIST Chinese-English MT evalua-tion data.1 IntroductionCurrent Statistical Machine Translation (SMT) ap-proaches model the translation problem as a pro-cess of generating a derivation of atomic transla-tion units, assuming that every unit is drawn outof the same model.
The simplest of these is thephrase-based approach (Och et al, 1999; Koehnet al, 2003) which employs a global model toprocess any sub-strings of the input sentence.
Inthis way, all we need is to increasingly translatea sequence of source words each time until theentire sentence is covered.
Despite good result-s in many tasks, such a method ignores the rolesof each source word and is somewhat differen-t from the way used by translators.
For exam-ple, an important-first strategy is generally adopt-ed in human translation - we translate the key ele-ments/structures (or skeleton) of the sentence first,and then translate the remaining parts.
This es-pecially makes sense for some languages, such asChinese, where complex structures are usually in-volved.Note that the source-language structural infor-mation has been intensively investigated in recentstudies of syntactic translation models.
Some ofthem developed syntax-based models on completesyntactic trees with Treebank annotations (Liu etal., 2006; Huang et al, 2006; Zhang et al, 2008),and others used source-language syntax as softconstraints (Marton and Resnik, 2008; Chiang,2010).
However, these approaches suffer fromthe same problem as the phrase-based counterpartand use the single global model to handle differ-ent translation units, no matter they are from theskeleton of the input tree/sentence or other not-so-important sub-structures.In this paper we instead explicitly model thetranslation problem with sentence skeleton infor-mation.
In particular,?
We develop a skeleton-based model whichdivides translation into two sub-models: askeleton translation model (i.e., translatingthe key elements) and a full translation model(i.e., translating the remaining source wordsand generating the complete translation).?
We develop a skeletal language model to de-scribe the possibility of translation skeletonand handle some of the long-distance worddependencies.?
We apply the proposed model to Chinese-English phrase-based MT and demonstratepromising BLEU improvements and TER re-ductions on the NIST evaluation data.2 A Skeleton-based Approach to MT2.1 Skeleton IdentificationThe first issue that arises is how to identify theskeleton for a given source sentence.
Many waysare available.
E.g., we can start with a full syntac-tic tree and transform it into a simpler form (e.g.,removing a sub-tree).
Here we choose a simpleand straightforward method: a skeleton is obtainedby dropping all unimportant words in the origi-nal sentence, while preserving the grammaticali-ty.
See the following for an example skeleton of aChinese sentence.563Original Sentence (subscripts represent indices):z[1]per?
[2]ton?Yz[3]seawater desalination?n[4]treatment[5]of?
[6]the cost3[7]5[8]5[9]yuan[10]of?:[11]from?[12]???
[13]has been furthere?[14]reduced"[15].
(The cost of seawater desalination treatment hasbeen further reduced from 5 yuan per ton.
)Sentence Skeleton (subscripts represent indices):?
[6]the cost???
[13]has been furthere?[14]reduced"[15].
(The cost has been further reduced.
)Obviously the skeleton used in this work can beviewed as a simplified sentence.
Thus the prob-lem is in principle the same as sentence simpli-fication/compression.
The motivations of defin-ing the problem in this way are two-fold.
First,as the skeleton is a well-formed (but simple) sen-tence, all current MT approaches are applicableto the skeleton translation problem.
Second, ob-taining simplified sentences by word deletion isa well-studied issue (Knight and Marcu, 2000;Clarke and Lapata, 2006; Galley and McKeown,2007; Cohn and Lapata, 2008; Yamangil andShieber, 2010; Yoshikawa et al, 2012).
Manygood sentence simpliciation/compression methodsare available to our work.
Due to the lack of space,we do not go deep into this problem.
In Section3.1 we describe the corpus and system employedfor automatic generation of sentence skeletons.2.2 Base ModelNext we describe our approach to integratingskeleton information into MT models.
We startwith an assumption that the 1-best skeleton is pro-vided by the skeleton identification system.
Thenwe define skeleton-based translation as a task ofsearching for the best target string?t given thesource string and its skeleton ?
:?t = argmaxtP(t|?, s) (1)As is standard in SMT, we further assume that1) the translation process can be decomposed in-to a derivation of phrase-pairs (for phrase-basedmodels) or translation rules (for syntax-basedmodels); 2) and a linear function g(?)
is used toassign a model score to each derivation.
Let ds,?,t(or d for short) denote a translation derivation.
Theabove problem can be redefined in a Viterbi fash-ion - we find the derivation?dwith the highest mod-el score given s and ?
:?d = argmaxdg(d) (2)In this way, the MT output can be regarded as thetarget-string encoded in?d.To compute g(d), we use a linear combinationof a skeleton translation model gskel(d) and a fulltranslation model gfull(d):g(d) = gskel(d) + gfull(d) (3)where the skeleton translation model handles thetranslation of the sentence skeleton, while the fulltranslation model is the baseline model and han-dles the original problem of translating the wholesentence.
The motivation here is straightforward:we use an additional score gskel(d) to model theproblem of skeleton translation and interpolate itwith the baseline model.
See Figure 1 for an exam-ple of applying the above model to phrase-basedMT.
In the figure, each source phrase is translatedinto a target phrase, which is represented by linkedrectangles.
The skeleton translation model focus-es on the translation of the sentence skeleton, i.e.,the solid (red) rectangles; while the full transla-tion model computes the model score for all thosephrase-pairs, i.e., all solid and dashed rectangles.Another note on the model.
Eq.
(3) provides avery flexible way for model selection.
While wewill restrict ourself to phrase-based translation inthe following description and experiments, we canchoose different models/features for gskel(d) andgfull(d).
E.g., one may introduce syntactic fea-tures into gskel(d) due to their good ability in cap-turing structural information; and employ a stan-dard phrase-based model for gfull(d) in which notall segments of the sentence need to respect syn-tactic constraints.2.3 Model Score ComputationIn this work both the skeleton translation modelgskel(d) and full translation model gfull(d) resem-ble the usual forms used in phrase-based MT, i.e.,the model score is computed by a linear combina-tion of a group of phrase-based features and lan-guage models.
In phrase-based MT, the transla-tion problem is modeled by a derivation of phrase-pairs.
Given a translation model m, a languagemodel lm and a vector of feature weights w, themodel score of a derivation d is computed by564z?
?Yz ?n  ?
3 5   ?
: ?
???
e?
"the costphrase1p1Skeleton:Full:g(d?
;w?,m, lm?)
= w?m?
fm(p1) + w?lm?
lm?
(?the cost?
)g(d;w,m, lm) = wm?
fm(p1) + wlm?
lm(?the cost?)z?
?Yz ?n  ?
3 5   ?
: ?
???
e?
"the cost of seawater desalination treatmentphrases2&3p1p2p3Skeleton:Full:g(d?
;w?,m, lm?)
= w?m?
fm(p1) + w?lm?
lm?
(?the cost X?
)g(d;w,m, lm) = wm?
fm(p1?
p2?
p3) + wlm?
lm(?the cost of seawater desalination treatment?)z?
?Yz ?n  ?
3 5   ?
: ?
???
e?
"the cost of seawater desalination treatment has been further reducedphrases4&5p1p2p3p4p5Skeleton:Full:g(d?
;w?,m, lm?)
= w?m?
fm(p1?
p4?
p5)+w?lm?
lm?
(?the cost X has been further reduced?
)g(d;w,m, lm) = wm?
fm(p1?
p2?
... ?
p5) + wlm?
lm(?the cost of seawater ... further reduced?)z?
?Yz ?n  ?
3 5   ?
: ?
???
e?
"the cost of seawater desalination treatment has been further reduced from5 yuanper ton.phrases6-9p1p2p3p4p5p6p7p8p9Skeleton:Full:g(d?
;w?,m, lm?)
= w?m?
fm(p1?
p4?
p5?
p9)+w?lm?
lm?
(?the cost X has been further reduced X .?
)g(d;w,m, lm) = wm?
fm(p1?
p2?
... ?
p9) + wlm?
lm(?the cost of seawater ... per ton .?
)Figure 1: Example derivation and model scores for a sentence in LDC2006E38.
The solid (red) rect-angles represent the sentence skeleton, and the dashed (blue) rectangles represent the non-skeleton seg-ments.
X represents a slot in the translation skeleton.
?
represents composition of phrase-pairs.g(d;w,m, lm) = wm?
fm(d)+wlm?
lm(d) (4)where fm(d) is a vector of feature values definedon d, and wmis the corresponding weight vector.lm(d) andwlmare the score and weight of the lan-guage model, respectively.To ease modeling, we only consider skeleton-consistent derivations in this work.
A deriva-tion d is skeleton-consistent if no phrases in dcross skeleton boundaries (e.g., a phrase where t-wo of the source words are in the skeleton andone is outside).
Obviously, from any skeleton-consistent derivation d we can extract a skeletonderivation d?which covers the sentence skeletonexactly.
For example, in Figure 1, the deriva-tion of phrase-pairs {p1, p2, ..., p9} is skeleton-consistent, and the skeleton derivation is formedby {p1, p4, p5, p9}.Then, we can simply define gskel(d) andgfull(d) as the model scores of d?and d:gskel(d) , g(d?
;w?,m, lm?)
(5)gfull(d) , g(d;w,m, lm) (6)This model makes the skeleton translation andfull translation much simpler because they per-form in the same way of string translation inphrase-based MT.
Both gskel(d) and gfull(d) sharethe same translation model m which can easilylearned from the bilingual data1.
On the otherhand, it has different feature weight vectors for in-dividual models (i.e., w and w?
).For language modeling, lm is the standard n-gram language model adopted in the baseline sys-tem.
lm?is a skeletal language for estimating thewell-formedness of the translation skeleton.
Herea translation skeleton is a target string where allsegments of non-skeleton translation are general-ized to a symbol X.
E.g., in Figure 1, the trans-1In gskel(d), we compute the reordering model score onthe skeleton though it is learned from the full sentences.
Inthis way the reordering problems in skeleton translation andfull translation are distinguished and handled separately.565lation skeleton is ?the cost X has been further re-duced X .
?, where two Xs represent non-skeletonsegments in the translation.
In such a way of stringrepresentation, the skeletal language model can beimplemented as a standard n-gram language mod-el, that is, a string probability is calculated by aproduct of a sequence of n-gram probabilities (in-volving normal words and X).
To learn the skele-tal language model, we replace non-skeleton partsof the target sentences in the bilingual corpus toXs using the source sentence skeletons and wordalignments.
The skeletal language model is thentrained on these generalized strings in a standardway of n-gram language modeling.By substituting Eq.
(4) into Eqs.
(5) and (6),and then Eqs.
(3) and (2), we have the final modelused in this work:?d = argmaxd(wm?
fm(d) + wlm?
lm(d) +w?m?
fm(d?)
+ w?lm?
lm?(d?
))(7)Figure 1 shows the translation process and as-sociated model scores for the example sentence.Note that this method does not require any newtranslation models for implementation.
Given abaseline phrase-based system, all we need is tolearn the feature weights w and w?on the devel-opment set (with source-language skeleton anno-tation) and the skeletal language model lm?onthe target-language side of the bilingual corpus.To implement Eq.
(7), we can perform standarddecoding while ?doubly weighting?
the phraseswhich cover a skeletal section of the sentence, andcombining the two language models and the trans-lation model in a linear fashion.3 Evaluation3.1 Experimental SetupWe experimented with our approach on Chinese-English translation using the NiuTrans open-source MT toolkit (Xiao et al, 2012).
Our bilin-gual corpus consists of 2.7M sentence pairs.
Al-l these sentences were aligned in word level us-ing the GIZA++ system and the ?grow-diag-final-and?
heuristics.
A 5-gram language model wastrained on the Xinhua portion of the English Gi-gaword corpus in addition to the target-side of thebilingual data.
This language model was usedin both the baseline and our improved system-s. For our skeletal language model, we trained a5-gram language model on the target-side of thebilingual data by generalizing non-skeleton seg-ments to Xs.
We used the newswire portion of theNIST MT06 evaluation data as our developmen-t set, and used the evaluation data of MT04 andMT05 as our test sets.
We chose the default fea-ture set of the NiuTrans.Phrase engine for buildingthe baseline, including phrase translation proba-bilities, lexical weights, a 5-gram language mod-el, word and phrase bonuses, a ME-based lexical-ized reordering model.
All feature weights werelearned using minimum error rate training (Och,2003).Our skeleton identification system was builtusing the t3 toolkit2which implements a state-of-the-art sentence simplification system.
Weused the NEU Chinese sentence simplification(NEUCSS) corpus as our training data (Zhanget al, 2013).
It contains the annotation of sen-tence skeleton on the Chinese-language side ofthe Penn Parallel Chinese-English Treebank (LD-C2003E07).
We trained our system using the Parts1-8 of the NEUCSS corpus and obtained a 65.2%relational F1 score and 63.1% compression rate inheld-out test (Part 10).
For comparison, we alsomanually annotated the MT development and testdata with skeleton information according to theannotation standard provided within NEUCSS.3.2 ResultsTable 1 shows the case-insensitive IBM-versionBLEU and TER scores of different systems.
Wesee, first of all, that the MT system benefits fromour approach in most cases.
In both the manualand automatic identification of sentence skeleton(rows 2 and 4), there is a significant improvemen-t on the ?All?
data set.
However, using differentskeleton identification results for training and in-ference (row 3) does not show big improvementsdue to the data inconsistency problem.Another interesting question is whether theskeletal language model really contributes to theimprovements.
To investigate it, we removed theskeletal language model from our skeleton-basedtranslation system (with automatic skeleton iden-tification on both the development and test sets).Seen from row ?lm?of Table 1, the removal ofthe skeletal language model results in a significan-t drop in both BLEU and TER performance.
Itindicates that this language model is very benefi-cial to our system.
For comparison, we removed2http://staffwww.dcs.shef.ac.uk/people/T.Cohn/t3/566Entry MT06 (Dev) MT04 MT05 Allsystem dev-skel test-skel BLEU TER BLEU TER BLEU TER BLEU TERbaseline - - 35.06 60.54 38.53 61.15 34.32 62.82 36.64 61.54SBMT manual manual 35.71 59.60 38.99 60.67 35.35 61.60 37.30 60.73SBMT manual auto 35.72 59.62 38.75 61.16 35.02 62.20 37.03 61.19SBMT auto auto 35.57 59.66 39.21 60.59 35.29 61.89 37.33 60.80?lm?auto auto 35.23 60.17 38.86 60.78 34.82 62.46 36.99 61.16?m?auto auto 35.50 59.69 39.00 60.69 35.10 62.03 37.12 60.90s-space - - 35.00 60.50 38.39 61.20 34.33 62.90 36.57 61.58s-feat.
- - 35.16 60.50 38.60 61.17 34.25 62.88 36.70 61.58Table 1: BLEU4[%] and TER[%] scores of different systems.
Boldface means a significant improvement(p < 0.05).
SBMT means our skeleton-based MT system.
?lm?
(or ?m?)
means that we remove theskeletal language model (or translation model) from our proposed approach.
s-space means that werestrict the baseline system to the search space of skeleton-consistent derivations.
s-feat.
means that weintroduce an indicator feature for skeleton-consistent derivations into the baseline system.the skeleton-based translation model from our sys-tem as well.
Row ?m?of Table 1 shows that theskeleton-based translation model can contribute tothe overall improvement but there is no big differ-ences between baseline and ?m?.Apart from showing the effects of the skeleton-based model, we also studied the behavior of theMT system under the different settings of searchspace.
Row s-space of Table 1 shows the BLEUand TER results of restricting the baseline sys-tem to the space of skeleton-consistent derivation-s, i.e., we remove both the skeleton-based trans-lation model and language model from the SBMTsystem.
We see that the limited search space is alittle harmful to the baseline system.
Further, weregarded skeleton-consistent derivations as an in-dicator feature and introduced it into the baselinesystem.
Seen from row s-feat., this feature doesnot show promising improvements.
These resultsindicate that the real improvements are due to theskeleton-based model/features used in this work,rather than the ?well-formed?
derivations.4 Related WorkSkeleton is a concept that has been used in severalsub-areas in MT for years.
For example, in confu-sion network-based system combination it refer-s to the backbone hypothesis for building confu-sion networks (Rosti et al, 2007; Rosti et al,2008); Liu et al (2011) regard skeleton as a short-ened sentence after removing some of the functionwords for better word deletion.
In contrast, we de-fine sentence skeleton as the key segments of asentence and develop a new MT approach basedon this information.There are some previous studies on the use ofsentence skeleton or related information in MT(Mellebeek et al, 2006a; Mellebeek et al, 2006b;Owczarzak et al, 2006).
In spite of their goodideas of using skeleton skeleton information, theydid not model the skeleton-based translation prob-lem in modern SMT pipelines.
Our work is a fur-ther step towards the use of sentence skeleton inMT.
More importantly, we develop a complete ap-proach to this issue and show its effectiveness in astate-of-the-art MT system.5 Conclusion and Future WorkWe have presented a simple but effective approachto integrating the sentence skeleton informationinto a phrase-based system.
The experimental re-sults show that the proposed approach achievesvery promising BLEU improvements and TER re-ductions on the NIST evaluation data.
In our fu-ture work we plan to investigate methods of inte-grating both syntactic models (for skeleton trans-lation) and phrasal models (for full translation) inour system.
We also plan to study sophisticatedreordering models for skeleton translation, ratherthan reusing the baseline reordering model whichis learned on the full sentences.AcknowledgementsThis work was supported in part by the Nation-al Science Foundation of China (Grants 61272376and 61300097), and the China Postdoctoral Sci-ence Foundation (Grant 2013M530131).
The au-thors would like to thank the anonymous reviewersfor their pertinent and insightful comments.567ReferencesDavid Chiang.
2010.
Learning to Translate withSource and Target Syntax.
In Proc.
of ACL 2010,pages 1443-1452.James Clarke and Mirella Lapata.
2006.
Models forSentence Compression: A Comparison across Do-mains, Training Requirements and Evaluation Mea-sures.
In Proc.
of ACL/COLING 2006, pages 377-384.Trevor Cohn and Mirella Lapata.
2008.
SentenceCompression Beyond Word Deletion.
In Proc.
ofCOLING 2008, pages 137-144.Jason Eisner.
2003.
Learning Non-Isomorphic TreeMappings for Machine Translation.
In Proc.
of ACL2003, pages 205-208.Michel Galley and Kathleen McKeown.
2007.
Lex-icalized Markov Grammars for Sentence Compres-sion.
In Proc.
of HLT:NAACL 2007, pages 180-187.Liang Huang, Kevin Knight and Aravind Joshi.
2006.Statistical syntax-directed translation with extendeddomain of locality.
In Proc.
of AMTA 2006, pages66-73.Kevin Knight and Daniel Marcu.
2000.
Statistical-based summarization-step one: sentence compres-sion.
In Proc.
of AAAI 2000, pages 703-710.Philipp Koehn, Franz J. Och and Daniel Marcu.
2003.Statistical Phrase-Based Translation.
In Proc.
ofNAACL 2003, pages 48-54.Yang Liu, Qun Liu and Shouxun Lin.
2006.
Tree-to-String Alignment Template for Statistical MachineTranslation.
In Proc.
of ACL/COLING 2006, pages609-616.Shujie Liu, Chi-Ho Li and Ming Zhou.
2011.
StatisticMachine Translation Boosted with Spurious WordDeletion.
In Proc.
of Machine Translation SummitXIII, pages 72-79.Yuval Marton and Philip Resnik.
2008.
Soft SyntacticConstraints for Hierarchical Phrased-Based Transla-tion.
In Proc.
of ACL:HLT 2008, pages 1003-1011.Bart Mellebeek, Karolina Owczarzak, Josef van Gen-abith and Andy Way.
2006.
Multi-Engine MachineTranslation by Recursive Sentence Decomposition.In Proc.
of AMTA 2006, pages 110-118.Bart Mellebeek, Karolina Owczarzak, Declan Groves,Josef Van Genabith and Andy Way.
2006.
A Syn-tactic Skeleton for Statistical Machine Translation.In Proc.
of EAMT 2006, pages 195-202.Franz J. Och, Christoph Tillmann and Hermann Ney.1999.
Improved Alignment Models for StatisticalMachine Translation.
In Proc.
of EMNLP/VLC1999, pages 20-28.Franz J. Och.
2003.
Minimum error rate training in s-tatistical machine translation.
In Proc.
of ACL 2003,pages 160-167.Karolina Owczarzak, Bart Mellebeek, Declan Groves,Josef van Genabith and Andy Way.
2006.
WrapperSyntax for Example-Based Machine Translation.
InProc.
of AMTA2006, pages 148-155.Antti-Veikko I. Rosti, Spyros Matsoukas and RichardSchwartz.
2007.
Improved Word-Level SystemCombination for Machine Translation.
In Proc.
ofACL 2007, pages 312-319.Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,and Richard Schwartz.
2008.
Incremental hypothe-sis alignment for building confusion networks withapplication to machine translation system combina-tion.
In Proc.
of Third Workshop on Statistical Ma-chine Translation, pages 183?186.Tong Xiao, Jingbo Zhu, Hao Zhang and Qiang Li2012.
NiuTrans: An Open Source Toolkit forPhrase-based and Syntax-based Machine Transla-tion.
In Proc.
of ACL 2012, system demonstrations,pages 19-24.Elif Yamangil and Stuart M. Shieber.
2010.
BayesianSynchronous Tree-Substitution Grammar Inductionand Its Application to Sentence Compression.
InProc.
of ACL 2010, pages 937-947.Katsumasa Yoshikawa, Ryu Iida, Tsutomu Hirao andManabu Okumura.
2012.
Sentence Compressionwith Semantic Role Constraints.
In Proc.
of ACL2012, pages 349-353.Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, ChewLim Tan and Sheng Li.
2008.
A Tree SequenceAlignment-based Tree-to-Tree Translation Model.In Proc.
of ACL:HLT 2008, pages 559-567.Chunliang Zhang, Minghan Hu, Tong Xiao, Xue Jiang,Lixin Shi and Jingbo Zhu.
2013.
Chinese SentenceCompression: Corpus and Evaluation.
In Proc.of Chinese Computational Linguistics and NaturalLanguage Processing Based on Naturally AnnotatedBig Data, pages 257-267.568
