Word Alignment Based on Bilingual BracketingBing ZhaoLanguage Technologies InstituteCarnegie Mellon Universitybzhao@cs.cmu.eduStephan VogelLanguage Technologies InstituteCarnegie Mellon Universityvogel+@cs.cmu.eduAbstractIn this paper, an improved word alignmentbased on bilingual bracketing is described.
Theexplored approaches include using Model-1conditional probability, a boosting strategy forlexicon probabilities based on importance sam-pling, applying Parts of Speech to discriminateEnglish words and incorporating informationof English base noun phrase.
The results ofthe shared task on French-English, Romanian-English and Chinese-English word alignmentsare presented and discussed.1 IntroductionBilingual parsing based word alignment is promisingbut still difficult.
The goal is to extract structure in-formation from parallel sentences, and thereby improveword/phrase alignment via bilingual constraint transfer.This approach can be generalized to the automatic acqui-sition of a translation lexicon and phrase translations esp.for languages for which resources are relatively scarcecompared with English.The parallel sentences in building Statistical MachineTranslation (SMT) systems are mostly unrestricted textwhere full parsing often fails, and robustness with respectto the inherent noise of the parallel data is important.Bilingual Bracketing [Wu 1997] is one of the bilingualshallow parsing approaches studied for Chinese-Englishword alignment.
It uses a translation lexicon within aprobabilistic context free grammar (PCFG) as a genera-tive model to analyze the parallel sentences with weakorder constraints.
This provides a framework to incorpo-rate knowledge from the English side such as POS, phrasestructure and potentially more detailed parsing results.In this paper, we use a simplified bilingual bracket-ing grammar together with a statistical translation lexiconsuch as the Model-1 lexicon [Brown 1993] to do the bilin-gual bracketing.
A boosting strategy is studied and ap-plied to the statistical lexicon training.
English POS andBase Noun Phrase (NP) detection are used to further im-prove the alignment performance.
Word alignments andphrase alignments are extracted from the parsing resultsas post processing.
The settings of different translationlexicons within the bilingual bracketing framework arestudied and experiments on word-alignment are carriedout on Chinese-English, French-English, and Romanian-English language pairs.The paper is structured as follows: in section 2, thesimplified bilingual bracketing used in our system is de-scribed; in section 3, the boosting strategy based on im-portance sampling for IBM Model-1 lexicon is intro-duced; in section 4, English POS and English Base NounPhrase are used to constrain the alignments ; in section5, the experimental results are shown; summary and con-clusions are given in section 6.2 Bilingual BracketingIn [Wu 1997], the Bilingual Bracketing PCFG was intro-duced, which can be simplified as the following produc-tion rules:A ?
[AA] (1)A ?
< AA > (2)A ?
f/e (3)A ?
f/null (4)A ?
null/e (5)Where f and e are words in the target vocabulary Vf andsource vocabulary Ve respectively.
A is the alignmentof texts.
There are two operators for bracketing: directbracketing denoted by [ ], and inverse bracketing, de-noted by <>.
The A-productions are divided into twoclasses: syntactic {(1),(2)}and lexical rules {(3),(4),(5)}.Each A-production rule has a probability.In our algorithm, we use the same PCFG.
However,instead of estimating the probabilities for the productionrules via EM as described in [Wu 1997], we assign theprobabilities to the rules using the Model-1 statisticaltranslation lexicon [Brown et al 1993].Because the syntactic A-production rules do not com-pete with the lexical rules, we can set them some defaultvalues.
Also we make no assumptions which bracketingdirection is more likely to occur, thus the probabilitiesfor [ ] and <> are set to be equal.
As for the lexicalrules, we experimented with the conditional probabilitiesp(e|f), p(f |e) and the interpolation of p(f |e, epos) andp(f |e) (described in section 4.1).
As for these probabil-ities of aligning a word to the null word or to unknownwords, they are set to be 1e-7, which is the default smallvalue used in training Model-1.The word alignment can then be done via maximizingthe likelihood of matched words subject to the bracketinggrammar using dynamic programming.The result of the parsing gives bracketing for both in-put sentences as well as bracket algnments indicating thecorresponding brackets between the sentence pairs.
Thebracket algnment includes a word alignment as a by-product.
One example for French-English (the test setsentence pair #18) is shown as below:[[it1 is2 ] [quite3 [understandable4 .5 ]]][[ce1 est2 ] [tout3 [[?4 [fait5 comprihensible6 ] ] .7]]][[it1/ce1 is2/est2 ] [quite3/tout3 [[e/?4 [e/fait5understandable4/comprihensible6 ] ] .5/.7]]]3 Boosting Strategy of Model-1 LexiconThe probabilities for the lexical rules are Model-1 condi-tional probabilities p(f |e), which can be estimated usingavailable toolkits such as [Franz 2000].This strategy is a three-pass training of Model-1, whichwas shown to be effective in our Chinese-English align-ment experiments.
The first two passes are carried out toget Viterbi word alignments based on Model-1?s param-eters in both directions: from source to target and thenvice versa.
An intersection of the two Viterbi word align-ments is then calculated.
The highly frequent word-pairsin the intersection set are considered to be important sam-ples supporting the alignment of that word-pair.
This ap-proach, which is similar to importance sampling, can besummarized as follows:Denote a sample as a co-occurred word-pair asx = (ei, fj) with its observed frequency: C(x) =freq(ei, fj); Denote I(x) = freq(ei, fj) as the fre-quency of that word-pair x observed in the intersectionof the two Viterbi alignments.?
Build I(x) = freq(ei, fj) from the intersection ofalignments in two directions.?
Generate x = (ei, fj) and its C(x) = freq(ei, fj)observed from a given parallel corpus;?
Generate random variable u from uniform [0,1] dis-tribution independent of x;?
If I(x)M ?C(x) ?
u, then accept x, where M is a finiteknown constant M > 0;?
Re-weight sample x: Cb(x) = C(x)?(1+?
), ?
> 0)The modified counts (weighted samples) are re-normalized to get a proper probability distribution, whichis used in the next iteration of EM training.
The constantM is a threshold to remove the potential noise from theintersection set.
M ?s value is related to the size of thetraining corpus, the larger its size, the larger M shouldbe.
?
is chosen as a small positive value.
The overall ideais to collect those word-pairs which are reliable and givean additional pseudo count to them.4 Incorporating English GrammaticalConstraintsThere are several POS taggers, base noun phrase detec-tors and parsers available for English.
Both the shallowand full parsing information of English sentences can beused as constraints in Bilingual Bracketing.
Here, weexplored utilizing English POS and English base nounphrase boundaries.4.1 Incorporating English POSThe correctly aligned words from two languages are verylikely to have the same POS.
For example, a Chinesenoun is very likely to be aligned with a English noun.While the English POS tagging is often reliable and ac-curate, the POS tagging for other languages is usually noteasily acquired nor accurate enough.
Modelling only theEnglish POS in word alignment is usually a practical way.Given POS information for only the English side, wecan discriminate English words and thus disambiguatethe translation lexicon.
We tagged each English wordin the parallel corpus, so that each English word is as-sociated with its POS denoted as epos.
The Englishword and its POS were concatenated into one pseudoword.
For example: beginning/NN and beginning/VBGare two pseudo words which occurred in our trainingcorpus.
Then the Model-1 training was carried out onthis concatenated parallel corpus to get estimations ofp(f |e, epos).One potential problem is the estimation of p(f |e, epos).When we concatenated the word with its POS, we im-plicitly increased the vocabulary size.
For example, forFrench-English training set, the English vocabulary in-creased from 57703 to 65549.
This may not cause a prob-lem when the training data?s size is large.
But for smallparallel corpora, some correct word-pair?s p(f |e, epos)will be underestimated due to the sparse data, and someword-pairs become unknown in p(f |e, epos).
So in oursystem, we actually interpolated p(f |e, epos) with p(f |e)as a mixture model for robustness:P (A ?
f/e|A) = ?
?P (f |e)+(1??
)?P (f |e, epos) (6)Where ?
can be estimated by EM for this two-mixturemodel on the training data, or a grid search via cross-validation.4.2 Incorporating English Base Noun BoundariesThe English sentence is bracketed according to the syn-tactic A-production rules.
This bracketing can break anEnglish noun phrase into separated pieces, which arenot in accordance with results from standard base nounphrase detectors.
Though the word-alignments may stillbe correct, but for the phrase level alignment, it is notdesired.One solution is to constrain the syntactic A-productionrules to penalize bracketing English noun phrases intoseparated pieces.
The phrase boundaries can be obtainedby using a base noun phrase detection toolkit [Ramshaw1995], and the boundaries are loaded into the bracketingprogram.
During the dynamic programming, before ap-plying a syntactic A-production rule, the program checksif the brackets defined by the syntactic rule violate thenoun phrase boundaries.
If so, an additional penalty isattached to this rule.5 ExperimentsAll the settings described so far are based on our pre-vious experiments on Chinese-English (CE) alignment.These settings are then used directly without any ad-justment of the parameters for the French-English (FE)and Romanian-English (RE) word alignment tasks.
Inthis section, we will first describe our experiments onChinese-English alignment, and then the results for theshared task on French-English and Romanian-English.For Chinese-English alignment, 365 sentence-pairs arerandomly sampled from the Chinese Tree-bank providedby the Linguistic Data Consortium.
Three persons man-ually aligned the word-pairs independently, and the con-sistent alignments from all of them were used as the ref-erence alignments.
There are totally 4094 word-pairs inthe reference set.
Our way of alignment is very similarto the ?SURE?
(S) alignment defined in the shared task.The training data we used is 16K parallel sentence-pairsfrom Hong-Kong news data.
The English POS tagger weused is Brill?s POS tagger [Brill 1994].
The base noundetector is [Ramshaw 1995].
The alignment is evaluatedin terms of precision, recall, F-measure and alignment er-ror rate (AER) defined in the shared task.
The results areshown in Table-1:Table-1.
Chinese-English Word-AlignmentCE precision recall F-measure AERNo-Boost 50.88 58.77 54.54 45.46Boosted 52.19 60.33 55.96 44.04+POS 54.77 63.34 58.71 41.29+NP 55.16 63.75 59.14 40.86Table-1 shows the effectiveness of using each settingon this small size training data.
Here the boosted modelgives a noticeable improvement over the baseline.
How-ever, our observations on the trial/test data showed verysimilar results for boosted and non-boosted models, sowe present only the non-boosted results(standard Model-1) for the shared task of EF and RE word alignment.Adding POS further improved the performance signif-icantly.
The AER drops from 44.04 to 41.29.
Addingadditional base noun phrase boundaries did not give asmuch improvement as we hoped.
There is only slightimprovement in terms of AER and F-measure.
One rea-son is that noun phrase boundaries is more directly re-lated to phrase alignment than word-alignment.
A closeexamination showed that with wrong phrase-alignment,word-alignment can still be correct.
Another reason isthat using the noun phrase boundaries this way may notbe powerful enough to leverage the English structure in-formation in Bilingual Bracketing.
More suitable wayscould be bilingual chunk parsing, and refining the brack-eting grammar as described in [Wu 1997].In the shared task experiments, we restricted the train-ing data to sentences upto 60 words.
The statistics for thetraining sets are shown in Table-2.
(French/Romanian aresource and English is target language).Table-2.Training Set StatisticsFrench-English Romanian-EnglishSent-pairs 1028382 45456Src Voc 79601 45880Tgt Voc 57703 26904There are 447 test sentence pairs for English-Frenchand 248 test sentence pairs for Romanian-English.
Afterthe bilingual bracketing, we extracted only the explicitword alignment from lexical rules: A ?
e/f , where nei-ther e nor f is the null(empty) word.
These explicit wordalignments are more directly related to the translationquality in our SMT system than the null-word alignments.Also the explicit word alignments is in accordance withthe ?SURE?
(S) alignment defined in the shared tasks.However the Bilingual Bracketing system is not adaptedto the ?PROBABLE?
(P) alignment because of the inher-ent one-to-one mapping.
All the AERs in the followingtables are calculated based solely on S alignment withoutany null alignments collected from the bracketing results.Table-3.
Limited Resource French-EnglishFE precision recall F-measure AERp(f |e) 49.85 79.45 61.26 23.87p(e|f) 51.46 82.42 63.36 20.95inter 63.03 74.59 68.32 19.26Table-4.
Unlimited Resource French-EnglishFE precision recall F-measure AERp(f |e) 50.21 80.36 61.80 23.07p(e|f) 51.91 83.26 63.95 19.96inter 66.34 74.86 70.34 17.77For the limited resource task, we trained Model-1 lex-icons in both directions: from source to target denoted asp(f |e) and from target to source denoted as p(e|f).
Thesetwo lexicons are then plugged into the Bilingual Brack-eting algorithm separately to get two sets of bilingualbracketing word alignments.
The intersection of thesetwo sets of word alignments is then collected.
The result-ing AERs are shown in Table-3 and Table-5 respectively.For the unlimited resource task, we again tagged theEnglish sentences and base noun phrase boundaries asmentioned before.
Then corresponding Model-1 lexiconwas trained and Bilingual Bracketing carried out.
Usingthe same strategies as in the limited resource task, we gotthe results shown in Table-4 and Table-6.The table above show that adding English POS andbase noun detection gave a consistent improvement forall conditions in the French-to-English alignment.
Theintersection of the two alignments greatly improves theprecision, paired with a reduction in recall, still resultingin an overall improvement in F-measure and AER.For the Romanian-English alignment the POS taggingand noun phrase boundaries did not help.
On the smallcorpus the increase in vocabulary resulted in addition un-known words in the test sentences which introduces ad-ditional alignment errors.Comparing the results of the French-English andRomanian-English alignment tasks we see a striking dif-ference in precision and recall.
Whereas the French-English alignment has a low precision and a high recallits the opposite for the Romanian-English alignment.
Thecause lays in different styles for the manual alignments.The French-English reference set contains both S and Palignments, whereas the Romanian-English reference setwas annotated with only S alignments.
As a result, thereare on average only 0.5 S alignments per word in the FEreference set, but 1.5 S alignments per word in the REtest set.6 SummaryIn this paper we presented our word alignment systembased on bilingual bracketing.
We introduced a techniqueTable-5.
Limited Resource Romanian-EnglishRE precision recall F-measure AERp(r|e) 70.65 55.75 62.32 37.66p(e|r) 71.39 55.00 62.13 37.87inter 85.48 48.64 62.01 37.99Table-6.
Unlimited Resource Romanian-EnglishRE precision recall F-measure AERp(r|e) 69.63 54.65 61.24 38.76p(e|r) 70.36 55.50 62.05 37.95inter 82.09 48.73 61.15 38.85to boost lexical probabilities for more reliable word pairsin the statistical lexicon.
In addition, we investigated theeffects of using POS and noun phrase detection on theEnglish side of the bilingual corpus as constraints for thealignment.
We applied these techniques to the French-English and Romanian-English alignment tasks, and inaddition to Chinese-English alignment.
For Chinese-English and French-English alignments these additionalknowledge sources resulted in improvements in align-ment quality.
Best results were obtained by using theintersection of the source to target and target to sourcebilingual bracketing alignments.
The results show verydifferent behavior of the alignment system on the French-English and Romanian-English tasks which is due to dif-ferent characteristics of the manually aligned test data.This indicates that establishing a good golden standardfor word alignment evaluation is still an open issue.ReferencesBrown, P. F. and Della Pietra, S. A. and Della Pietra, V. J.and Mercer, R. L. 1993.
The Mathematics of Statisti-cal Machine Translation: Parameter Estimation.
Com-putational Linguistics, 19-2, pp 263-311.Erik Brill.
1994.
Some advances in rule-based part ofspeech tagging.
Proceedings of the Twelfth NationalConference on Artificial Intelligence (AAAI-94), Seat-tle, Wa., 1994.Franz Josef Och and Hermann Ney.
2000.
Improved Sta-tistical Alignment Models.
Proceedings of ACL-00,pp.
440-447, Hongkong, China.Lance Ramshaw and Mitchell Marcus 1995.
Text Chunk-ing Using Transformation-Based Learning.
Proceed-ings of the Third ACL Workshop on Very Large Cor-pora, MIT, June, 1995.Wu, Dekai.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics 23(3):377-404, Sep. 1997.
