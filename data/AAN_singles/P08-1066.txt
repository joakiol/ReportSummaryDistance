Proceedings of ACL-08: HLT, pages 577?585,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsA New String-to-Dependency Machine Translation Algorithmwith a Target Dependency Language ModelLibin ShenBBN TechnologiesCambridge, MA 02138, USAlshen@bbn.comJinxi XuBBN TechnologiesCambridge, MA 02138, USAjxu@bbn.comRalph WeischedelBBN TechnologiesCambridge, MA 02138, USAweisched@bbn.comAbstractIn this paper, we propose a novel string-to-dependency algorithm for statistical machinetranslation.
With this new framework, we em-ploy a target dependency language model dur-ing decoding to exploit long distance wordrelations, which are unavailable with a tra-ditional n-gram language model.
Our ex-periments show that the string-to-dependencydecoder achieves 1.48 point improvement inBLEU and 2.53 point improvement in TERcompared to a standard hierarchical string-to-string system on the NIST 04 Chinese-Englishevaluation set.1 IntroductionIn recent years, hierarchical methods have been suc-cessfully applied to Statistical Machine Translation(Graehl and Knight, 2004; Chiang, 2005; Ding andPalmer, 2005; Quirk et al, 2005).
In some languagepairs, i.e.
Chinese-to-English translation, state-of-the-art hierarchical systems show significant advan-tage over phrasal systems in MT accuracy.
For ex-ample, Chiang (2007) showed that the Hiero systemachieved about 1 to 3 point improvement in BLEUon the NIST 03/04/05 Chinese-English evaluationsets compared to a start-of-the-art phrasal system.Our work extends the hierarchical MT approach.We propose a string-to-dependency model for MT,which employs rules that represent the source sideas strings and the target side as dependency struc-tures.
We restrict the target side to the so called well-formed dependency structures, in order to cover alarge set of non-constituent transfer rules (Marcu etal., 2006), and enable efficient decoding through dy-namic programming.
We incorporate a dependencylanguage model during decoding, in order to exploitlong-distance word relations which are unavailablewith a traditional n-gram language model on targetstrings.For comparison purposes, we replicated the Hierodecoder (Chiang, 2005) as our baseline.
Our string-to-dependency decoder shows 1.48 point improve-ment in BLEU and 2.53 point improvement in TERon the NIST 04 Chinese-English MT evaluation set.In the rest of this section, we will briefly dis-cuss previous work on hierarchical MT and de-pendency representations, which motivated our re-search.
In section 2, we introduce the model ofstring-to-dependency decoding.
Section 3 illustratesof the use of dependency language models.
In sec-tion 4, we describe the implementation details of ourMT system.
We discuss experimental results in sec-tion 5, compare to related work in section 6, anddraw conclusions in section 7.1.1 Hierarchical Machine TranslationGraehl and Knight (2004) proposed the use of target-tree-to-source-string transducers (xRS) to modeltranslation.
In xRS rules, the right-hand-side(rhs)of the target side is a tree with non-terminals(NTs),while the rhs of the source side is a string withNTs.
Galley et al (2006) extended this string-to-treemodel by using Context-Free parse trees to representthe target side.
A tree could represent multi-leveltransfer rules.The Hiero decoder (Chiang, 2007) does not re-quire explicit syntactic representation on either sideof the rules.
Both source and target are strings withNTs.
Decoding is solved as chart parsing.
Hiero canbe viewed as a hierarchical string-to-string model.Ding and Palmer (2005) and Quirk et al (2005)577itwillfindboytheinterestingFigure 1: The dependency tree for sentence the boy willfind it interestingfollowed the tree-to-tree approach (Shieber and Sch-abes, 1990) for translation.
In their models, depen-dency treelets are used to represent both the sourceand the target sides.
Decoding is implemented astree transduction preceded by source side depen-dency parsing.
While tree-to-tree models can rep-resent richer structural information, existing tree-to-tree models did not show advantage over string-to-tree models on translation accuracy due to a muchlarger search space.One of the motivations of our work is to achievedesirable trade-off between model capability andsearch space through the use of the so called well-formed dependency structures in rule representation.1.2 Dependency TreesDependency trees reveal long-distance relations be-tween words.
For a given sentence, each word has aparent word which it depends on, except for the rootword.Figure 1 shows an example of a dependency tree.Arrows point from the child to the parent.
In thisexample, the word find is the root.Dependency trees are simpler in form than CFGtrees since there are no constituent labels.
However,dependency relations directly model semantic struc-ture of a sentence.
As such, dependency trees are adesirable prior model of the target sentence.1.3 Motivations for Well-Formed DependencyStructuresWe restrict ourselves to the so-called well-formedtarget dependency structures based on the followingconsiderations.Dynamic ProgrammingIn (Ding and Palmer, 2005; Quirk et al, 2005),there is no restriction on dependency treelets used intransfer rules except for the size limit.
This may re-sult in a high dimensionality in hypothesis represen-tation and make it hard to employ shared structuresfor efficient dynamic programming.In (Galley et al, 2004), rules contain NT slots andcombination is only allowed at those slots.
There-fore, the search space becomes much smaller.
Fur-thermore, shared structures can be easily definedbased on the labels of the slots.In order to take advantage of dynamic program-ming, we fixed the positions onto which another an-other tree could be attached by specifying NTs independency trees.Rule CoverageMarcu et al (2006) showed that many usefulphrasal rules cannot be represented as hierarchicalrules with the existing representation methods, evenwith composed transfer rules (Galley et al, 2006).For example, the following rule?
<(hong)Chinese, (DT(the) JJ(red))English>is not a valid string-to-tree transfer rule since the redis a partial constituent.A number of techniques have been proposed toimprove rule coverage.
(Marcu et al, 2006) and(Galley et al, 2006) introduced artificial constituentnodes dominating the phrase of interest.
The bi-narization method used by Wang et al (2007) cancover many non-constituent rules also, but not all ofthem.
For example, it cannot handle the above ex-ample.
DeNeefe et al (2007) showed that the bestresults were obtained by combing these methods.In this paper, we use well-formed dependencystructures to handle the coverage of non-constituentrules.
The use of dependency structures is due to theflexibility of dependency trees as a representationmethod which does not rely on constituents (Fox,2002; Ding and Palmer, 2005; Quirk et al, 2005).The well-formedness of the dependency structuresenables efficient decoding through dynamic pro-gramming.2 String-to-Dependency Translation2.1 Transfer Rules with Well-FormedDependency StructuresA string-to-dependency grammar G is a 4-tupleG =< R, X, Tf , Te >, where R is a set of transferrules.
X is the only non-terminal, which is similarto the Hiero system (Chiang, 2007).
Tf is a set of578terminals in the source language, and Te is a set ofterminals in the target language1 .A string-to-dependency transfer rule R ?
R is a4-tuple R =< Sf , Se, D,A >, where Sf ?
(Tf ?
{X})+ is a source string, Se ?
(Te ?
{X})+ is atarget string, D represents the dependency structurefor Se, and A is the alignment between Sf and Se.Non-terminal alignments in A must be one-to-one.In order to exclude undesirable structures, weonly allow Se whose dependency structure D iswell-formed, which we will define below.
In addi-tion, the same well-formedness requirement will beapplied to partial decoding results.
Thus, we will beable to employ shared structures to merge multiplepartial results.Based on the results in previous work (DeNeefeet al, 2007), we want to keep two kinds of depen-dency structures.
In one kind, we keep dependencytrees with a sub-root, where all the children of thesub-root are complete.
We call them fixed depen-dency structures because the head is known or fixed.In the other, we keep dependency structures of sib-ling nodes of a common head, but the head itself isunspecified or floating.
Each of the siblings must bea complete constituent.
We call them floating de-pendency structures.
Floating structures can repre-sent many linguistically meaningful non-constituentstructures: for example, like the red, a modifier ofa noun.
Only those two kinds of dependency struc-tures are well-formed structures in our system.Furthermore, we operate over well-formed struc-tures in a bottom-up style in decoding.
However,the description given above does not provide a cleardefinition on how to combine those two types ofstructures.
In the rest of this section, we will pro-vide formal definitions of well-formed structures andcombinatory operations over them, so that we caneasily manipulate well-formed structures in decod-ing.
Formal definitions also allow us to easily ex-tend the framework to incorporate a dependency lan-guage model in decoding.
Examples will be pro-vided along with the formal definitions.Consider a sentence S = w1w2...wn.
Letd1d2...dn represent the parent word IDs for eachword.
For example, d4 = 2 means that w4 depends1We ignore the left hand side here because there is only onenon-terminal X .
Of course, this formalism can be extended tohave multiple NTs.itwillfindboythefindboy(a) (b) (c)Figure 2: Fixed dependency structuresboy willtheinterestingit(a) (b)Figure 3: Floating dependency structureson w2.
If wi is a root, we define di = 0.Definition 1 A dependency structure di..j is fixedon head h, where h ?
[i, j], or fixed for short, ifand only if it meets the following conditions?
dh /?
[i, j]?
?k ?
[i, j] and k 6= h, dk ?
[i, j]?
?k /?
[i, j], dk = h or dk /?
[i, j]In addition, we say the category of di..j is(?, h,?
), where ?
means this field is undefined.Definition 2 A dependency structure di...dj is float-ing with children C , for a non-empty set C ?
{i, ..., j}, or floating for short, if and only if it meetsthe following conditions?
?h /?
[i, j], s.t.
?k ?
C, dk = h?
?k ?
[i, j] and k /?
C, dk ?
[i, j]?
?k /?
[i, j], dk /?
[i, j]We say the category of di..j is (C,?,?)
if j < h,or (?,?, C) otherwise.
A category is composed ofthe three fields (A, h,B), where h is used to repre-sent the head, and A and B are designed to modelleft and right dependents of the head respectively.A dependency structure is well-formed if andonly if it is either fixed or floating.ExamplesWe can represent dependency structures withgraphs.
Figure 2 shows examples of fixed structures,Figure 3 shows examples of floating structures, andFigure 4 shows ill-formed dependency structures.It is easy to verify that the structures in Figures2 and 3 are well-formed.
4(a) is ill-formed because579interestingwillfindfindboy(a) (b)Figure 4: Ill-formed dependency structuresboy does not have its child word the in the tree.
4(b)is ill-formed because it is not a continuous segment.As for the example the red mentioned above, it isa well-formed floating dependency structure.2.2 Operations on Well-Formed DependencyStructures and CategoriesOne of the purposes of introducing floating depen-dency structures is that siblings having a commonparent will become a well-defined entity, althoughthey are not considered a constituent.
We alwaysbuild well-formed partial structures on the targetside in decoding.
Furthermore, we combine partialdependency structures in a way such that we can ob-tain all possible well-formed but no ill-formed de-pendency structures during bottom-up decoding.The solution is to employ categories introducedabove.
Each well-formed dependency structure hasa category.
We can apply four combinatory oper-ations over the categories.
If we can combine twocategories with a certain category operation, we canuse a corresponding tree operation to combine twodependency structures.
The category of the com-bined dependency structure is the result of the com-binatory category operations.We first introduce three meta category operations.Two of them are unary operations, left raising (LR)and right raising (RR), and one is the binary opera-tion unification (UF).First, the raising operations are used to turn acompleted fixed structure into a floating structure.It is easy to verify the following theorem accordingto the definitions.Theorem 1 A fixed structure with category(?, h,?)
for span [i, j] is also a floating structurewith children {h} if there are no outside wordsdepending on word h.?k /?
[i, j], dk 6= h. (1)Therefore we can always raise a fixed structure if weassume it is complete, i.e.
(1) holds.itwillfindboytheinterestingLALA LA RA RALC RCFigure 5: A dependency tree with flexible combinationDefinition 3 Meta Category Operations?
LR((?, h,?))
= ({h},?,?)?
RR((?, h,?))
= (?,?, {h})?
UF((A1, h1, B1), (A2, h2, B2)) = NORM((A1 tA2, h1 t h2, B1 t B2))Unification is well-defined if and only if we canunify all three elements and the result is a valid fixedor floating category.
For example, we can unify afixed structure with a floating structure or two float-ing structures in the same direction, but we cannotunify two fixed structures.h1 t h2 =??
?h1 if h2 = ?h2 if h1 = ?undefined otherwiseA1 t A2 =??
?A1 if A2 = ?A2 if A1 = ?A1 ?A2 otherwiseNORM((A, h, B)) =???????
(?, h,?)
if h 6= ?(A,?,?)
if h = ?, B = ?
(?,?, B) if h = ?, A = ?undefined otherwiseNext we introduce the four tree operations on de-pendency structures.
Instead of providing the formaldefinition, we use figures to illustrate these opera-tions to make it easy to understand.
Figure 1 showsa traditional dependency tree.
Figure 5 shows thefour operations to combine partial dependency struc-tures, which are left adjoining (LA), right adjoining(RA), left concatenation (LC) and right concatena-tion (RC).Child and parent subtrees can be combined withadjoining which is similar to the traditional depen-dency formalism.
We can either adjoin a fixed struc-ture or a floating structure to the head of a fixedstructure.Complete siblings can be combined via concate-nation.
We can concatenate two fixed structures, onefixed structure with one floating structure, or twofloating structures in the same direction.
The flex-ibility of the order of operation allows us to take ad-580willfindboytheLALALAwillfindboytheLALALC23 2113(b)(a)Figure 6: Operations over well-formed structuresvantage of various translation fragments encoded intransfer rules.Figure 6 shows alternative ways of applying op-erations on well-formed structures to build largerstructures in a bottom-up style.
Numbers representthe order of operation.We use the same names for the operations on cat-egories for the sake of convenience.
We can easilyuse the meta category operations to define the fourcombinatory operations.
The definition of the oper-ations in the left direction is as follows.
Those in theright direction are similar.Definition 4 Combinatory category operationsLA((A1,?,?
), (?, h2,?
))= UF((A1,?,?
), (?, h2,?
))LA((?, h1,?
), (?, h2,?
))= UF(LR((?, h1,?
)), (?, h2,?))LC((A1,?,?
), (A2,?,?
))= UF((A1,?,?
), (A2,?,?))LC((A1,?,?
), (?, h2,?
))= UF((A1,?,?
), LR((?, h2,?
)))LC((?, h1,?
), (A2,?,?
))= UF(LR((?, h1,?
)), (A2,?,?
))LC((?, h1,?
), (?, h2,?
))= UF(LR((?, h1,?
)), LR((?, h2,?
)))It is easy to verify the soundness and complete-ness of category operations based on one-to-onemapping of the conditions in the definitions of cor-responding operations on dependency structures andon categories.Theorem 2 (soundness and completeness)Suppose X and Y are well-formed dependencystructures.
OP(cat(X), cat(Y )) is well-defined fora given operation OP if and only if OP(X,Y ) iswell-defined.
Furthermore,cat(OP(X, Y )) = OP(cat(X), cat(Y ))Suppose we have a dependency tree for a red apple,where both a and red depend on apple.
There aretwo ways to compute the category of this string fromthe bottom up.cat(Da red apple)= LA(cat(Da), LA(cat(Dred), cat(Dapple)))= LA(LC(cat(Da), cat(Dred)), cat(Dapple))Based on Theorem 2, it follows that combinatoryoperation of categories has the confluence property,since the result dependency structure is determined.Corollary 1 (confluence) The category of a well-formed dependency tree does not depend on the or-der of category calculation.With categories, we can easily track the types ofdependency structures and constrain operations indecoding.
For example, we have a rule with depen-dency structure find ?
X , where X right adjoinsto find.
Suppose we have two floating structures2 ,cat(X1) = ({he, will},?,?
)cat(X2) = (?,?, {it, interesting})We can replace X by X2, but not by X1 based onthe definition of category operations.2.3 Rule ExtractionNow we explain how we get the string-to-dependency rules from training data.
The procedureis similar to (Chiang, 2007) except that we maintaintree structures on the target side, instead of strings.Given sentence-aligned bi-lingual training data,we first use GIZA++ (Och and Ney, 2003) to gen-erate word level alignment.
We use a statistical CFGparser to parse the English side of the training data,and extract dependency trees with Magerman?s rules(1995).
Then we use heuristic rules to extract trans-fer rules recursively based on the GIZA alignmentand the target dependency trees.
The rule extractionprocedure is as follows.1.
Initialization:All the 4-tuples (P i,jf , P m,ne , D,A) are validphrase alignments, where source phrase P i,jf is2Here we use words instead of word indexes in categories tomake the example easy to understand.581itfindinteresting(D1)(D2)itXfindinteresting(D?
)Figure 7: Replacing it with X in D1aligned to target phrase P m,ne under alignment3A, and D, the dependency structure for P m,ne ,is well-formed.
All valid phrase templates arevalid rules templates.2.
Inference:Let (P i,jf , P m,ne , D1, A) be a valid rule tem-plate, and (P p,qf , P s,te , D2, A) a valid phrasealignment, where [p, q] ?
[i, j], [s, t] ?
[m,n],D2 is a sub-structure of D1, and at least oneword in P i,jf but not in Pp,qf is aligned.We create a new valid rule template(P ?f , P ?e, D?, A), where we obtain P ?f byreplacing P p,qf with label X in Pi,jf , and obtainP ?e by replacing P s,te with X in P m,ne .
Further-more, We obtain D?
by replacing sub-structureD2 with X in D14.
An example is shown inFigure 7.Among all valid rule templates, we collect thosethat contain at most two NTs and at most seven ele-ments in the source as transfer rules in our system.2.4 DecodingFollowing previous work on hierarchical MT (Chi-ang, 2005; Galley et al, 2006), we solve decodingas chart parsing.
We view target dependency as thehidden structure of source fragments.The parser scans all source cells in a bottom-upstyle, and checks matched transfer rules according tothe source side.
Once there is a completed rule, webuild a larger dependency structure by substitutingcomponent dependency structures for correspondingNTs in the target dependency structure of rules.Hypothesis dependency structures are organizedin a shared forest, or AND-OR structures.
An AND-3By P i,jf aligned to Pm,ne , we mean all words in P i,jf areeither aligned to words in P m,ne or unaligned, and vice versa.Furthermore, at least one word in P i,jf is aligned to a word inP m,ne .4If D2 is a floating structure, we need to merge severaldependency links into one.structure represents an application of a rule overcomponent OR-structures, and an OR-structure rep-resents a set of alternative AND-structures with thesame state.
A state means a n-tuple that character-izes the information that will be inquired by up-levelAND-structures.Supposing we use a traditional tri-gram languagemodel in decoding, we need to specify the leftmosttwo words and the rightmost two words in a state.Since we only have a single NT X in the formalismdescribed above, we do not need to add the NT la-bel in states.
However, we need to specify one ofthe three types of the dependency structure: fixed,floating on the left side, or floating on the right side.This information is encoded in the category of thedependency structure.In the next section, we will explain how to ex-tend categories and states to exploit a dependencylanguage model during decoding.3 Dependency Language ModelFor the dependency tree in Figure 1, we calculate theprobability of the tree as followsProb = PT (find)?PL(will|find-as-head)?PL(boy|will, find-as-head)?PL(the|boy-as-head)?PR(it|find-as-head)?PR(interesting|it, find-as-head)Here PT (x) is the probability that word x is theroot of a dependency tree.
PL and PR are left andright side generative probabilities respectively.
Letwh be the head, and wL1wL2 ...wLn be the childrenon the left side from the nearest to the farthest.
Sup-pose we use a tri-gram dependency LM,PL(wL1wL2 ...wLn |wh-as-head)= PL(wL1 |wh-as-head)?PL(wL2 |wL1 , wh-as-head)?...?
PL(wLn |wLn?1 , wLn?2) (2)wh-as-head represents wh used as the head, andit is different from wh in the dependency languagemodel.
The right side probability is similar.In order to calculate the dependency languagemodel score, or depLM score for short, on the fly for582partial hypotheses in a bottom-up decoding, we needto save more information in categories and states.We use a 5-tuple (LF,LN, h,RN,RF ) to repre-sent the category of a dependency structure.
h rep-resents the head.
LF and RF represent the farthesttwo children on the left and right sides respectively.Similarly, LN and RN represent the nearest twochildren on the left and right sides respectively.
Thethree types of categories are as follows.?
fixed: (LF,?, h,?, RF )?
floating left: (LF,LN,?,?,?)?
floating right: (?,?,?, RN,RF )Similar operations as described in Section 2.2 areused to keep track of the head and boundary childnodes which are then used to compute depLM scoresin decoding.
Due to the limit of space, we skip thedetails here.4 Implementation DetailsFeatures1.
Probability of the source side given the targetside of a rule2.
Probability of the target side given the sourceside of a rule3.
Word alignment probability4.
Number of target words5.
Number of concatenation rules used6.
Language model score7.
Dependency language model score8.
Discount on ill-formed dependency structuresWe have eight features in our system.
The values ofthe first four features are accumulated on the rulesused in a translation.
Following (Chiang, 2005),we also use concatenation rules like X ?
XX forbackup.
The 5th feature counts the number of con-catenation rules used in a translation.
In our sys-tem, we allow substitutions of dependency struc-tures with unmatched categories, but there is a dis-count for such substitutions.Weight OptimizationWe tune the weights with several rounds ofdecoding-optimization.
Following (Och, 2003), thek-best results are accumulated as the input of the op-timizer.
Powell?s method is used for optimizationwith 20 random starting points around the weightvector of the last iteration.RescoringWe rescore 1000-best translations (Huang andChiang, 2005) by replacing the 3-gram LM scorewith the 5-gram LM score computed offline.5 ExperimentsWe carried out experiments on three models.?
baseline: replication of the Hiero system.?
filtered: a string-to-string MT system as inbaseline.
However, we only keep the transferrules whose target side can be generated by awell-formed dependency structure.?
str-dep: a string-to-dependency system with adependency LM.We take the replicated Hiero system as ourbaseline because it is the closest to our string-to-dependency model.
They have similar rule extrac-tion and decoding algorithms.
Both systems useonly one non-terminal label in rules.
The major dif-ference is in the representation of target structures.We use dependency structures instead of strings;thus, the comparison will show the contribution ofusing dependency information in decoding.All models are tuned on BLEU (Papineni et al,2001), and evaluated on both BLEU and TranslationError Rate (TER) (Snover et al, 2006) so that wecould detect over-tuning on one metric.We used part of the NIST 2006 Chinese-English large track data as well as some LDCcorpora collected for the DARPA GALE program(LDC2005E83, LDC2006E34 and LDC2006G05)as our bilingual training data.
It contains about178M/191M words in source/target.
Hierarchicalrules were extracted from a subset which has about35M/41M words5, and the rest of the training datawere used to extract phrasal rules as in (Och, 2003;Chiang, 2005).
The English side of this subset wasalso used to train a 3-gram dependency LM.
Tra-ditional 3-gram and 5-gram LMs were trained on acorpus of 6G words composed of the LDC Gigawordcorpus and text downloaded from Web (Bulyko etal., 2007).
We tuned the weights on NIST MT05and tested on MT04.5It includes eight corpora: LDC2002E18, LDC2003E07,LDC2004T08 HK News, LDC2005E83, LDC2005T06,LDC2005T10, LDC2006E34, and LDC2006G05583Model #Rulesbaseline 140Mfiltered 26Mstr-dep 27MTable 1: Number of transfer rulesModel BLEU% TER%lower mixed lower mixedDecoding (3-gram LM)baseline 38.18 35.77 58.91 56.60filtered 37.92 35.48 57.80 55.43str-dep 39.52 37.25 56.27 54.07Rescoring (5-gram LM)baseline 40.53 38.26 56.35 54.15filtered 40.49 38.26 55.57 53.47str-dep 41.60 39.47 55.06 52.96Table 2: BLEU and TER scores on the test set.Table 1 shows the number of transfer rules ex-tracted from the training data for the tuning andtest sets.
The constraint of well-formed dependencystructures greatly reduced the size of the rule set.
Al-though the rule size increased a little bit after incor-porating dependency structures in rules, the size ofstring-to-dependency rule set is less than 20% of thebaseline rule set size.Table 2 shows the BLEU and TER scoreson MT04.
On decoding output, the string-to-dependency system achieved 1.48 point improve-ment in BLEU and 2.53 point improvement inTER compared to the baseline hierarchical string-to-string system.
After 5-gram rescoring, it achieved1.21 point improvement in BLEU and 1.19 improve-ment in TER.
The filtered model does not show im-provement on BLEU.
The filtered string-to-stringrules can be viewed the string projection of string-to-dependency rules.
It means that just using depen-dency structure does not provide an improvement onperformance.
However, dependency structures al-low the use of a dependency LM which gives rise tosignificant improvement.6 DiscussionThe well-formed dependency structures defined hereare similar to the data structures in previous work onmono-lingual parsing (Eisner and Satta, 1999; Mc-Donald et al, 2005).
However, here we have fixedstructures growing on both sides to exploit varioustranslation fragments learned in the training data,while the operations in mono-lingual parsing weredesigned to avoid artificial ambiguity of derivation.Charniak et al (2003) described a two-step string-to-CFG-tree translation model which employed asyntax-based language model to select the besttranslation from a target parse forest built in the firststep.
Only translation probability P (F |E) was em-ployed in the construction of the target forest due tothe complexity of the syntax-based LM.
Since ourdependency LM models structures over target wordsdirectly based on dependency trees, we can build asingle-step system.
This dependency LM can alsobe used in hierarchical MT systems using lexical-ized CFG trees.The use of a dependency LM in MT is similar tothe use of a structured LM in ASR (Xu et al, 2002),which was also designed to exploit long-distance re-lations.
The depLM is used in a bottom-up style,while SLM is employed in a left-to-right style.7 Conclusions and Future WorkIn this paper, we propose a novel string-to-dependency algorithm for statistical machine trans-lation.
For comparison purposes, we replicatedthe Hiero system as described in (Chiang, 2005).Our string-to-dependency system generates 80%fewer rules, and achieves 1.48 point improvement inBLEU and 2.53 point improvement in TER on thedecoding output on the NIST 04 Chinese-Englishevaluation set.Dependency structures provide a desirable plat-form to employ linguistic knowledge in MT.
In thefuture, we will continue our research in this directionto carry out translation with deeper features, for ex-ample, propositional structures (Palmer et al, 2005).We believe that the fixed and floating structures pro-posed in this paper can be extended to model predi-cates and arguments.AcknowledgmentsThis work was supported by DARPA/IPTO ContractNo.
HR0011-06-C-0022 under the GALE program.We are grateful to Roger Bock, Ivan Bulyko, MikeKayser, John Makhoul, Spyros Matsoukas, Antti-Veikko Rosti, Rich Schwartz and Bing Zhang fortheir help in running the experiments and construc-tive comments to improve this paper.584ReferencesI.
Bulyko, S. Matsoukas, R. Schwartz, L. Nguyen, andJ.
Makhoul.
2007.
Language model adaptation inmachine translation from speech.
In Proceedings ofthe 32nd IEEE International Conference on Acoustics,Speech, and Signal Processing (ICASSP).E.
Charniak, K. Knight, and K. Yamada.
2003.
Syntax-based language models for statistical machine transla-tion.
In Proceedings of MT Summit IX.D.
Chiang.
2005.
A hierarchical phrase-based model forstatistical machine translation.
In Proceedings of the43th Annual Meeting of the Association for Computa-tional Linguistics (ACL).D.
Chiang.
2007.
Hierarchical phrase-based translation.Computational Linguistics, 33(2).S.
DeNeefe, K. Knight, W. Wang, and D. Marcu.
2007.What can syntax-based mt learn from phrase-basedmt?
In Proceedings of the 2007 Conference of Em-pirical Methods in Natural Language Processing.Y.
Ding and M. Palmer.
2005.
Machine translation usingprobabilistic synchronous dependency insertion gram-mars.
In Proceedings of the 43th Annual Meeting ofthe Association for Computational Linguistics (ACL),pages 541?548, Ann Arbor, Michigan, June.J.
Eisner and G. Satta.
1999.
Efficient parsing for bilex-ical context-free grammars and head automaton gram-mars.
In Proceedings of the 37th Annual Meeting ofthe Association for Computational Linguistics (ACL).H.
Fox.
2002.
Phrasal cohesion and statistical machinetranslation.
In Proceedings of the 2002 Conference ofEmpirical Methods in Natural Language Processing.M.
Galley, M. Hopkins, K. Knight, and D. Marcu.
2004.What?s in a translation rule?
In Proceedings of the2004 Human Language Technology Conference of theNorth American Chapter of the Association for Com-putational Linguistics.M.
Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefea,W.
Wang, and I. Thayer.
2006.
Scalable inference andtraining of context-rich syntactic models.
In COLING-ACL ?06: Proceedings of 44th Annual Meeting of theAssociation for Computational Linguistics and 21stInt.
Conf.
on Computational Linguistics.J.
Graehl and K. Knight.
2004.
Training tree transducers.In Proceedings of the 2004 Human Language Technol-ogy Conference of the North American Chapter of theAssociation for Computational Linguistics.L.
Huang and D. Chiang.
2005.
Better k-best parsing.In Proceedings of the 9th International Workshop onParsing Technologies.D.
Magerman.
1995.
Statistical decision-tree models forparsing.
In Proceedings of the 33rd Annual Meeting ofthe Association for Computational Linguistics.D.
Marcu, W. Wang, A. Echihabi, and K. Knight.
2006.SPMT: Statistical machine translation with syntacti-fied target language phraases.
In Proceedings of the2006 Conference of Empirical Methods in NaturalLanguage Processing.R.
McDonald, K. Crammer, and F. Pereira.
2005.
Onlinelarge-margin training of dependency parsers.
In Pro-ceedings of the 43th Annual Meeting of the Associationfor Computational Linguistics (ACL).F.
J. Och and H. Ney.
2003.
A systematic comparison ofvarious statistical alignment models.
ComputationalLinguistics, 29(1).F.
J. Och.
2003.
Minimum error rate training for sta-tistical machine translation.
In Erhard W. Hinrichsand Dan Roth, editors, Proceedings of the 41st AnnualMeeting of the Association for Computational Linguis-tics (ACL), pages 160?167, Sapporo, Japan, July.M.
Palmer, D. Gildea, and P. Kingsbury.
2005.
Theproposition bank: An annotated corpus of semanticroles.
Computational Linguistics, 31(1).K.
Papineni, S. Roukos, and T. Ward.
2001.
Bleu: amethod for automatic evaluation of machine transla-tion.
IBM Research Report, RC22176.C.
Quirk, A. Menezes, and C. Cherry.
2005.
De-pendency treelet translation: Syntactically informedphrasal SMT.
In Proceedings of the 43th AnnualMeeting of the Association for Computational Lin-guistics (ACL), pages 271?279, Ann Arbor, Michigan,June.S.
Shieber and Y. Schabes.
1990.
Synchronous tree ad-joining grammars.
In Proceedings of COLING ?90:The 13th Int.
Conf.
on Computational Linguistics.M.
Snover, B. Dorr, R. Schwartz, L. Micciulla, andJ.
Makhoul.
2006.
A study of translation edit rate withtargeted human annotation.
In Proceedings of Associ-ation for Machine Translation in the Americas.W.
Wang, K. Knight, and D. Marcu.
2007.
Binarizingsyntax trees to improve syntax-based machine transla-tion accuracy.
In Proceedings of the 2007 Conferenceof Empirical Methods in Natural Language Process-ing.P.
Xu, C. Chelba, and F. Jelinek.
2002.
A study on richersyntactic dependencies for structured language model-ing.
In Proceedings of the 40th Annual Meeting of theAssociation for Computational Linguistics (ACL).585
