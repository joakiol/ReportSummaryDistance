Proceedings of the 6th Workshop on Statistical Machine Translation, pages 512?522,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsGenerative Models of Monolingual and Bilingual Gappy PatternsKevin Gimpel Noah A. SmithLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213, USA{kgimpel,nasmith}@cs.cmu.eduAbstractA growing body of machine translation re-search aims to exploit lexical patterns (e.g., n-grams and phrase pairs) with gaps (Simard etal., 2005; Chiang, 2005; Xiong et al, 2011).Typically, these ?gappy patterns?
are discov-ered using heuristics based on word align-ments or local statistics such as mutual infor-mation.
In this paper, we develop generativemodels of monolingual and parallel text thatbuild sentences using gappy patterns of arbi-trary length and with arbitrarily many gaps.We exploit Bayesian nonparametrics and col-lapsed Gibbs sampling to discover salient pat-terns in a corpus.
We evaluate the patternsqualitatively and also add them as features toan MT system, reporting promising prelimi-nary results.1 IntroductionBeginning with the success of phrase-based transla-tion models (Koehn et al, 2003), a trend arose ofmodeling larger and increasingly complex structuralunits in translation.
One thread of work has focusedon the use of lexical patterns with gaps.
Simard etal.
(2005) proposed using phrase pairs with gaps in aphrase-based translation model, providing a heuris-tic method to extract gappy phrase pairs from word-aligned parallel corpora.
The widely-used hierarchi-cal phrase-based translation framework was intro-duced by Chiang (2005) and also relies on a simpleheuristic for phrase pair extraction.
On the mono-lingual side, researchers have taken inspiration fromtrigger-based language modeling for speech recog-nition (Rosenfeld, 1996).
Recently Xiong et al(2011) used monolingual trigger pairs to improvehandling of long-distance dependencies in machinetranslation output.All of this previous work used heuristics or localstatistical tests to extract patterns from corpora.
Inthis paper, we present probabilistic models that gen-erate text using gappy patterns of arbitrary lengthand with arbitrarily-many gaps.
We exploit non-parametric priors and use Bayesian inference to dis-cover the most salient gappy patterns in monolin-gual and parallel text.
We first inspect these pat-terns manually and discuss the categories of phe-nomena that they capture.
We also add them asfeatures in a discriminatively-trained phrase-basedMT system, using standard techniques to train theirweights (Arun and Koehn, 2007; Watanabe et al,2007) and incorporate them during decoding (Chi-ang, 2007).
We present experiments for Spanish-English and Chinese-English translation, reportingencouraging preliminary results.2 Related WorkThere is a rich history of trigger-based languagemodeling in the speech recognition community, typ-ically involving the use of statistical tests to discoveruseful trigger-word pairs (Rosenfeld, 1996; Jelinek,1997).
Xiong et al (2011) used Rosenfeld?s mutualinformation procedure to discover trigger pairs andadded a single feature to a phrase-based MT systemthat scores new words based on all potential trig-gers from previous parts of the derivation.
We arenot aware of prior work that uses generative model-ing and Bayesian nonparametrics to discover thesesame types of patterns automatically; doing so al-lows us to discover larger patterns with more wordsand gaps if they are warranted by the data.In addition to the gappy phrase-based (Simard etal., 2005) and hierarchical phrase-based (Chiang,2005) models mentioned earlier, other researchershave explored the use of bilingual gappy structuresfor machine translation.
Crego and Yvon (2009) and512?
(  ) = .?
(  ) = baltic statesit provides either too little or too much .it 's neither particularly complicated nor novel .nato must either say " yes " or " no " to the baltic states .good scientific ideas formulated in bad english either die or get repackaged .nato must either say " yes " or " no " to the baltic states .?
(  ) = either __ or???pi????????
(  ) = either __ or?
(  ) = to the?
(  ) = " __ " __ " __ "?
(  ) = must?
(  ) = yes __ no?
(  ) = say?
(  ) = natoFigure 1: A sentence from the news commentary cor-pus, along with color assignments for the words and thepi function for each color.Galley and Manning (2010) proposed ways of incor-porating phrase pairs with gaps into standard left-to-right decoding algorithms familiar to phrase-basedand N -gram-based MT; both used heuristics to ex-tract phrase pairs.
Bansal et al (2011) presented amodel and training procedure for word alignmentthat uses phrase pairs with gaps.
They use a semi-Markov model with an enlarged dynamic program-ming state in order to represent alignment betweengappy phrases.
Their model permits up to one gapper phrase while our models permit an arbitrarynumber.3 Monolingual Pattern ModelsWe first present a model that generates a sentence asa set of lexical items that we will refer to as gappypatterns, or simply patterns.
A pattern is defined asa sequence containing elements of two types: wordsand gaps.
All patterns must obey the regular expres-sion w+( w+)*, where w is a word and is a gap.That is, patterns must begin and end with words andmay not contain consecutive gaps.We assume that we have an n-word sentencew1:n.1 We represent patterns in a sentence by as-sociating each word with a color.
To do so, we in-troduce a vector of color assignment variables c1:n,with one for each word.
We represent a color Cj asa set in terms of the ci variables: Cj = {i : ci = j}.Each color corresponds to a pattern that is obtainedby concatenating its words from left to right in thesentence, inserting gaps when necessary.
We denotethe pattern for a color Cj by pi(Cj); Figure 1 showsexamples of the correspondence between colors andpatterns.The generative story for a single sentence follows:1We use boldface lowercase letters to denote vectors (e.g.,f ), denote entry i as fi, and denote the range from i to j asf i:j .1.
Sample the number of words: n ?
Poisson(?)2.
Sample the number of unique colors in the sen-tence given n: m ?
Uniform(1, n)3.
For each word index i = 1 .
.
.
n, sample the colorof word i: ci ?
Uniform(1,m).
If any of the mcolors has no words, repeat this step.4.
For each color j = 1 .
.
.m, sample from amultinomial distribution over patterns: wCj ?Mult(?).
If the words wCj are not consistentwith the color assignments, i.e., wrong number ofwords or gaps, gaps not in the correct locations,repeat this step.Thus, the probability of generating number of wordsn, words w1:n, color assignments c1:n, and numberof colors m isp(w1:n, c1:n,m | ?, ?)=1Z(?nn!e??
)(1n)(1m)n m?j=1p?
(pi(Cj))(1)where Z is a normalization constant required by thepotential repetition of sampling in the final two stepsof the generative story.
Without Z, the model wouldbe deficient as we would waste probability mass oninternally inconsistent color assignments.The core of the model is a single multinomialdistribution p?(?)
over patterns.
We use a Dirich-let process (DP) prior for this multinomial so thatwe can model an unbounded set of patterns: ?
?DP(?, P0), where ?
is the concentration parameterand P0 is the base distribution.
The base distributionincludes a Poisson(?)
over the number of words inthe pattern, a uniform distribution (over word typesin the vocabulary) for each word, a uniform distri-bution over the number of gaps given the number ofwords, and a uniform distribution over the arrange-ment of gaps given the numbers of gaps and words.2Inference We use collapsed Gibbs samplingfor inference.
Our goal is to obtain samplesfrom the posterior distribution p({c(i),m(i)}Si=1 |{w(i)}Si=1, ?, ?
), where S is the total number of sen-tences in the corpus and ?
is marginalized out.32The number of ways of arranging y gaps among x words is?(x?
1) choose y?.3Since we assume the words are given, ?
is irrelevant.513During each iteration of Gibbs sampling, we pro-ceed through the corpus and sample a new value foreach ci variable conditioned on the values of all oth-ers in the corpus.
Them variables are determined bythe ci variables and therefore do not need to be sam-pled directly.
When sampling ci, we first removeci from the corpus (and its color if the color onlycontained i).
Where the remaining colors in the sen-tence are numbered from 1 to m, there are m + 1possibilities for ci: m for each of the existing colorsand one for choosing a new color.Since choosing a new color corresponds to creat-ing a new instance of the pattern pi({i}), the proba-bility of choosing a new color m+ 1 is proportionalto#pi({i}) + ?P0(pi({i}))# + ?
(2)where #pi is the count of pattern pi in the rest of thesentence and all other sentences in the corpus, and# is the total count of all patterns in this same set.The probability of choosing the existing color j (for1 ?
j ?
m) is proportional to#pi(Cj?
{i}) + ?P0(pi(Cj ?
{i}))#pi(Cj) + ?P0(pi(Cj))(3)where the denominator encodes the fact that themove will cause an instance of the pattern for thecolor Cj to be removed from the corpus as the newpattern for Cj ?
{i} is added.We note that, even though these two types ofmoves will result in different numbers of colors (m)in the sentence, we do not have to include a term forthis in the sampler because we use a uniform dis-tribution for m and therefore all (valid) numbers ofcolors have the same probability.
The normalizationconstant Z in Equation 1 does not affect inferencebecause our sampler is designed to only considervalid (i.e., internally consistent) settings for the c(i)and m(i) variables.This model makes few assumptions, using uni-form distributions whenever possible.
This simpli-fies inference and causes the resulting lexicon to beinfluenced primarily by the ?rich-get-richer?
effectof the DP prior.
Despite its simplicity, we will showlater that this model discovers patterns that capturea variety of linguistic phenomena.?
(  ) = .?
(  ) = baltic statesit provides either too little or too much .it 's neither particularly complicated nor novel .nato must either say " yes " or " no " to the baltic states .good scientific ideas formulated in bad english either die or get repackaged .nato must either say " yes " or " no " to the baltic states .la otan tiene que decir " s? "
o " no " a los pa?ses b?lticos .?
(  ) = either      or???pi????????
(  ) = either __ or?
(  ) = to the?
(  ) ="      "?
(  ) = must?
(  ) = yes __ no?
(  ) = say?
(  ) = nato?
(  ) =natootan ?
(  ) =to the13-12 15-13 16-15o " " aFigure 2: A Spanish-English sentence pair with the in-tersection of automatic word alignments in each direc-tion.
Some source words accept the colors of target wordsaligned to them while others (light gray) do not.
Bilingualpatterns for a few colors are shown.4 Bilingual Pattern ModelsWe now present a generative model for a sentencepair that will enable us to discover bilingual pat-terns.
In this section we present one example of ex-tending th previous model to be bilingu l, but wenote that many other extensions are possible; indeed,flexibility is one of the key advantages of workingwithin the framework of probabilistic modeling.We assume that we are given sentence pairs andone-to-one word alignments.
That is, in addition toan n-word target sentencew1:n, we assume we havean n?-word source sentence w?1:n?
and word align-ments a1:n?
where ai = j iff w?i is aligned to wj andai = 0 if w?i is aligned to null.To model bilingual patterns, we distinguishsource colors from target colors.
A target-languageword can only be colored with a target color, buta source word can be colored with either a sourcecolor or with the target color of the target word itis aligned to (if any).
We have m target colors asbefore and now add m?
source colors.
We intro-duce additional random variables in the form of abinary vector g of length n?
that indicates, for eachsource word, whether or not it accepts the color ofits aligned target word.
We introduce an additionalparameter ?
for the probability that a source wordwill accept the color of its aligned word.
We fix itsvalue to 0.5 and do not learn it during inference.
Fig-ure 2 shows an example Spanish-English sentencepair with automatic word alignments and color as-signments.
The bilingual patterns for a few targetcolors are shown.The generative story for a sentence pair follows:1.
Sample the numbers of words in the source andtarget sentences: n?, n ?
Poisson(?)5142.
Sample the numbers of source and target col-ors given n?, n: m?
?
Uniform(1, n?
),m ?Uniform(1, n)3.
Sample the alignment vector from any distribu-tion that ensures links are 1-to-1:4 a1:n?
?
p(a)4.
For each target word index i = 1 .
.
.
n, samplethe color of target word i from a uniform distribu-tion over all target colors: ci ?
Uniform(1,m).While any of the m colors has no words, repeatthis step.5.
For each source word index i = 1 .
.
.
n?:1.
Decide whether to use a source color or to usethe target color of the aligned target word: gi ?p?
(gi | ai)2.
If gi = 1, set c?i = cai ; otherwise, sample asource color: c?i ?
Uniform(1,m?)6.
If any source color has no words, repeat Step 5.7.
For each source color j = 1 .
.
.m?:1.
Sample from a multinomial over source pat-terns: wC?j ?
Mult(??).
While the words wC?jare not consistent with the color assignments,repeat this step.8.
For each target color j = 1 .
.
.m:1.
Sample from a multinomial over bilingual pat-terns: wCj ?
Mult(?).
While the words wCjare not consistent with the color assignments,repeat this step.The distribution p?
(gi | ai) is defined below:p?
(gi = 1 | ai 6= ?1) = ?p?
(gi = 1 | ai = ?1) = 0where ?
determines how frequently source tokenswill be added to target patterns.The probability of generating target words w1:n,source words w?1:n?
, alignments a1:n?
, target colorassignments c1:n, source color assignments c?1:n?
,color propagation variables g1:n?
, number of target4Since we assume alignments are provided during inference,it does not matter what distribution is used, so long as only 1-to-1 links are permitted.colors m, and number of source colors m?
is1Zp(n)p(n?
)p(m | n)p(m?
| n?)p(a1:n?)?
(n?i=1p(ci | m))?(n??i=1p?
(gi | ai)p(c?i | m?)I[gi==0])???m??j=1p??(pi(C?j))????m?j=1p?(pi(Cj))?
?where Z again serves as a normalization constant toprevent the model from leaking probability mass oninternally inconsistent configurations.There are now two multinomial distributions overpatterns with parameter vectors ?
and ??.
They bothuse DP priors with identical concentration param-eters ?
and differing base distributions P0 and P ?0.The base distribution for source patterns, P ?0, takesthe same form as the base distribution for the modeldescribed in ?3.For target patterns with aligned source words, P0generates the target part of the pattern like the basedistribution in ?3 and then generates the numberof aligned source words to each target word witha Poisson(1) distribution; the number of alignedsource words can only be 0 or 1 when all word linksare 1-to-1.
If it is 1, the base distribution generatesthe aligned source word by sampling uniformly fromamong all source types.While there are connections between this modeland work on performing translation using phrasepairs with gaps, the patterns we discover are notguaranteed to be bilingual translation units.
Rather,they typically contain additional target-side wordsthat have no explicit correlate on the source side.They can be used to assist an existing translationmodel by helping to choose the best phrase trans-lation for each source phrase.
To define a genera-tive model for phrase pairs with gaps, changes wouldhave to be made to the bilingual model we presented.Inference As before, we use collapsed Gibbs sam-pling for inference.
Our goal is to obtain sam-ples from the posterior p({?c, c?, g,m,m??
(i)}Si=1 |{?w,w?,a?
(i)}Si=1).515We go through each sentence pair and sample newcolor assignment variables for each word.
For analigned word pair (w?i, wj), we sample a new valuefor the tuple (gi, c?i, cj).
The possible values forcj include all target colors, including a new targetcolor.
The possible values for gi are 0, in which casec?i can be any of the source colors, including a newsource color, and 1, for which c?i must be cj .
For anunaligned target word wj , cj can be any target color,including a new one, and for an unaligned sourceword w?i, c?i can be any source color, including a newone.
The full equations for sampling can be easilyderived using the equations from ?3.5 EvaluationWe conducted evaluation to determine (1) whattypes of phenomena are captured by the most prob-able patterns discovered by our models, and (2)whether including the patterns as features can im-prove translation quality.5.1 Qualitative Evaluation5.1.1 Monolingual ModelSince inference is computationally expensive,we used the 126K-sentence English news com-mentary corpus provided for the WMT sharedtasks (Callison-Burch et al, 2010).
We ran Gibbssampling for 600 iterations through the data, dis-carding the first 300 samples for burn-in and com-puting statistics of the patterns using the remaining300 samples.
Each iteration took approximately 3minutes on a single 2.2GHz CPU.
When looking pri-marily at the most frequent patterns, we found thatthis list did not vary much when only using half ofthe data instead.
We set ?
= 3 and ?
= 100; wefound these hyperparameters to have only minor ef-fects on the results.Since many frequent patterns include the period(.
), we found it useful to constrain the model to treatthis token differently: we modify the base distribu-tion so that it assigns zero probability to patternsthat contain a period along with other words and weforce each occurrence of a period to be alone in itsown pattern during initialization.
We do not need tochange the inference procedure at all; with the mod-ified base distribution and with no patterns includinga period with other words, the probability of creat-" " as as " " " "?
?
the of in why ?
( ) the is , the ofthe of not only but from to, , , it is that the between andthe ( ) of " " such as ,both and not , but either orthe of and in , in but ismore than the of , " " the- - what ?
has been, " " between and in , ,the " " the of ?s an ofTable 1: Top-ranked gappy patterns from samples accord-ing to p(pi); patterns without gaps are omitted.
The spe-cial string ?
?
represents a gap that can be filled by anynonempty sequence of words.ing a new illegal pattern during inference is alwayszero (Eq.
3).We also perform inference on a transformed ver-sion of the corpus in which every word is replacedwith its hard word class obtained from Brown clus-tering (Brown et al, 1992).
One property of Brownclusters is that each function word effectively re-ceives its own class, as each ends up in a cluster inwhich it occupies ?95% of the token counts of alltypes in the cluster.
We call clusters that satisfy thisproperty singleton clusters.To obtain Brown clusters for the source and tar-get languages, we used code from Liang (2005).5We used the data from the news commentary cor-pus along with the first 500K sentences of the addi-tional monolingual newswire data also provided forthe WMT shared tasks.
We used 300 clusters, ig-noring words that appeared only once in this corpus.We did not use the hierarchical information from theclusters but merely converted each cluster name intoa unique integer, using one additional integer for un-known words.We used the same values for ?
and ?
as abovebut ran Gibbs sampling for 1,300 iterations, againusing the last 300 for collecting statistics on pat-terns.
Judging by the number of color assignmentschanged on each iteration, the sampler takes longerto converge when run on word clusters than onwords.
As above, we constrain the singleton wordcluster corresponding to the period to be alone dur-ing both initialization and inference.5http://www.cs.berkeley.edu/?pliang/software516academy sciences regulators supervisorsbeijing shanghai sine nonbooms busts stalin maocouncil advisers treasury secretary geithnerdominicans haitian sooner laterflemish walloons first foremostgref program played roleheat droughts down roadhumanitarian displaced freedom expressionkarnofsky hassenfeld at disposalkazakhstan kyrgyzstan take grantedportugal greece - -Table 2: Gappy patterns with highest conditional proba-bility p(pi|w(pi)).?
?
whether or france germany( ) around world he his- - has been allow toboth and how ?
for first timenot only but the ( ) china india" " on basis what domore than less than we oureither or on other hand over pastwhy ?
at level prevent fromneither nor it is that in waywhat ?
not , but one anotherrule law play role political economicTable 3: Top-ranked gappy patterns according top(pi)p(pi|w(pi)).Pattern Ranking Statistics Several choices existfor ranking patterns.
The simplest is to take the pat-tern count from the posterior samples, averaged overall sampling iterations after burn-in.
We refer to thiscriterion as the marginal probability:p(pi) =#pi#where #pi is the average count of the pattern acrossthe posterior samples and # is the count of all pat-terns.
The top-ranked gappy patterns under this cri-terion are shown in Table 1.
While many of thesepatterns match our intuitions, there are also sev-eral that are highly-ranked simply because their con-stituent words are frequent.Alternatively, we can rank patterns by the con-ditional probability of the pattern given the wordsthat comprise it:p(pi|w(pi)) =#pi#w(pi)where w(pi) returns the sequence of words in thepattern pi and #w(pi) is the number of occurrencesof this sequence of words in the corpus that are com-patible with pattern pi.
The ranking of patterns underthis criterion is shown in Table 2.
This method fa-vors precision but also causes very rare patterns tobe highly ranked.To address this, we also consider a product-of-experts model by simply multiplying together thetwo probabilities, resulting in the ranking shown inTable 3.
This ranking is similar to that in Table 1but penalizes patterns that are only ranked highly be-cause they consist of common words.
Table 4 showsa manual grouping of these highly-ranked patternsinto several categories.
We show both lexical andBrown cluster patterns.6It is common in both types of patterns to findlong-distance dependencies involving punctuationnear the top of the ranking.
Among agreement pat-terns, the lexical model finds relationships betweenpronouns and their associated possessive adjectiveswhile the cluster model finds more general patternsinvolving classes of nouns.
Cluster patterns are morelikely to capture topicality within a sentence, whilethe finer granularity of the lexical model is requiredto identify constructions like those shown (verbstriggering particular prepositions).There are also many probable patterns withoutgaps, shown at the bottom of Table 4.
From thesepatterns we can see that our models can also be usedto find collocations, but we note that these are dis-covered in the context of the gappy patterns.
Thatis, due to the use of latent variables in our models(the color assignments), there is a natural trading-offeffect whereby the gappy patterns encourage partic-ular non-gappy patterns to be used, and vice versa.5.1.2 Bilingual ModelWe use the news commentary corpus for each lan-guage and take the intersection of GIZA++ (Ochand Ney, 2003) word alignments in each direction,thereby ensuring that they are 1-to-1 alignments.
Weran Gibbs sampling for 300 iterations, averaging pat-tern counts from the last 200.
We set ?
= 100,?
= 3, and ?
= 0.5.
We ran the model in 3 con-ditions: source words, target words; source clusters,target clusters; and source clusters, target words.
We6We filter Brown cluster patterns in which every cluster isa singleton, since these patterns are typically already accountedfor in the lexical patterns.517Rank Gappy Lexical Patterns Rank Gappy Brown Cluster PatternsPunctuation1 -- -- 2 {what, why, whom, whatever} {?, !
}2 ( ) 6 {--, -, ?}
{--, -, ?
}6 " " 28 {according, compared, subscribe, thanks, referring} to ,9 why ?
178 {?, -, ?}
{even, especially, particularly, mostly, mainly} {?, -, ?
}63 according to , 239 {obama, bush, clinton, mccain, brown} " "Agreement26 he his 8 {people, things, americans, journalists, europeans} their31 we our 12 we {our, my}46 his his 21 {children, women, others, men, students} their86 china its 23 {china, europe, america, russia, iran} ?s its90 his he 43 {obama, bush, clinton, mccain, brown} his99 you your 46 {our, my} {our, my}136 leaders their 149 {people, things, americans, journalists, europeans} they140 we ourselves 172 {president, bill, sen., king, senator} {obama, bush, clinton, mccain, brown} his165 these are 180 {all, both, either} {countries, companies, banks, groups, issues}Connectives4 both and 5 {more, less} {more, less}5 not only but 9 if , {will, would, could, should, might}8 either or 19 {deal, plan, vote, decision, talks} {against, between, involving} and10 neither nor 40 a {against, between, involving} and13 whether or 45 {better, different, further, higher, lower} than19 less than 50 {much, far, slightly, significantly, substantially} than23 not , but 56 {yet, instead, perhaps, thus, neither} but54 if then 68 not {only, necessarily} {also, hardly}109 between and 98 as {much, far, slightly, significantly, substantially} as192 relationship between and 131 is {more, less} thanTopicality25 france germany 1 ?UNK?
?UNK?29 china india 15 {china, europe, .
.
.}
?s {system, crisis, program, recession, situation}36 political economic 30 {health, security, defense, safety, intelligence} {health, .
.
.
}43 rich poor 47 {china, europe, .
.
.}
{china, europe, .
.
.}
{china, europe, .
.
.
}50 oil gas 62 {power, growth, interest, development} {10, 1, 20, 30, 2} {percent, %, p.m., a.m.}62 billions dollars 72 in {iraq, washington, london, 2008, 2009} {iraq, washington, london, 2008, 2009}96 economic social 73 the {end, cost, head, rules, average} of {prices, markets, services, problems, costs}106 the us europe 113 {china, europe, .
.
.}
?s {economy, election, elections, population, investigation}181 public private 119 {prices, markets, .
.
.}
{oil, energy, tax, food, investment} {oil, energy, .
.
.
}Prepositions14 around world 14 for {first, second, third, final, whole} {time, period, term, class, avenue}18 on basis 17 in {last, next, 20th} {year, week, month, season, summer}38 at time 51 at {end, cost, head, rules, average} of42 in region 71 at {group, rate, leader, level, manager}80 in manner 112 for {times, points, games, goals, reasons}85 at expense 126 {over, around, across, behind, above} {country, company, region, nation, virus}112 during period 190 {one, none} of {best, top, largest, main, biggest}Constructions 33 prevent from84 enable to114 provide for123 impose on177 turn intoNon-Gappy Lexical Patterns Non-Gappy Brown Cluster Patternsas well their own as {well, soon, quickly, seriously, slowly} as {rather, please} thanthe united states prime minister the united {states, nations, airlines} {don, didn, doesn, isn, wasn} ?thave been climate change {president, bill, sen., king, senator} {mr., mr, john, david, michael} {obama, bush, clinton, .
.
.
}rather than the bush administration {order, plans, needs, efforts, failed} to {make, take, give, keep, provide}based on developing countries {will, would, could, should, might} not be {can, ?ll} beTable 4: Gappy patterns manually divided into categories of long-distance dependencies.
Patterns were ranked ac-cording to p(pi)p(pi|w(pi)) and manually selected from the top 300 to exemplify categories.
Lower pane shows topranked non-gappy patterns.
Clusters are shown as enough words to cover 95% of the token counts of the cluster, up toa maximum of 5.again ensured that the period and its word class re-mained isolated in their own patterns for each con-dition.
We note that no source-side word order in-formation is contained within these bilingual pat-terns; aligned source words can be in any order inthe source sentence and the pattern will still match.The most probable patterns included many mono-lingual source-only and target-only patterns that aresimilar to those shown in Table 4.
There were alsomany phrase pairs with gaps like those that are com-518monly extracted by heuristics (Galley and Manning,2010).
Additionally we noted examples of sourcewords triggering more target-side information thanmerely one word.
There were several examples ofpatterns that encouraged inclusion of the subject inEnglish when translating from Spanish, as Spanishoften drops the subject when it is clear from context,e.g., ?we are(estamos)?.
Also, one probable patternfor German-English was ?the of the(des)?
(des isaligned to the final the).
The German determinerdes is in the genitive case, so this pattern helps toencourage its object to also be in the genitive casewhen translated.5.2 Quantitative EvaluationWe consider the Spanish-to-English (ES?EN)translation task from the ACL-2010 Workshop onStatistical Machine Translation (Callison-Burch etal., 2010).
We trained a Moses system (Koehn et al,2007) following the baseline training instructions forthe shared task.7 In particular, we performed wordalignment in each direction using GIZA++ (Och andNey, 2003), used the ?grow-diag-final-and?
heuristicfor symmetrization, and extracted phrase pairs up toa maximum length of seven.
After filtering sentencepairs with one sentence longer than 50 words, weended up with 1.45M sentence pairs of Europarl dataand 91K sentence pairs of news commentary data.Language models (N = 5) were estimated using theSRI language modeling toolkit (Stolcke, 2002) withmodified Kneser-Ney smoothing (Chen and Good-man, 1998).
Language models were trained on thetarget side of the parallel corpus as well as the first 5million additional sentences from the extra Englishmonolingual newswire data provided for the sharedtasks.
We used news-test2008 for tuning andnews-test2009 for testing.We also consider Chinese-English (ZH?EN) andfollowed a similar training procedure as above.
Weused 303K sentence pairs from the FBIS corpus(LDC2003E14) and segmented the Chinese datausing the Stanford Chinese segmenter in ?CTB?mode (Chang et al, 2008), giving us 7.9M Chi-nese words and 9.4M English words.
A trigram lan-guage model was estimated using modified Kneser-Ney smoothing from the English side of the parallel7www.statmt.org/wmt10/baseline.html.corpus concatenated with 200M words of randomly-selected sentences from the Gigaword v4 corpus (ex-cluding the NY Times and LA Times).
We usedNIST MT03 for tuning and NIST MT05 for test-ing.
For evaluation, we used case-insensitive IBMBLEU (Papineni et al, 2001).5.2.1 Training and DecodingUnlike n-gram language models, our models havelatent structure (the color assignments), making itdifficult to compute the probability of a translationduring decoding.
We leave this problem for futurework and instead simply add a feature for each ofthe most probable patterns discovered by our mod-els.
Each feature counts the number of occurrencesof its pattern in the translation.We wish to add thousands of features to ourmodel, but the standard training algorithm ?
mini-mum error rate training (MERT; Och, 2003) ?
can-not handle large numbers of features.
So, we lever-age recent work on feature-rich training for MT us-ing online discriminative learning algorithms.
Ourtraining procedure is shown as Algorithm 1.
Wefind it convenient to notationally distinguish featureweights for the standard Moses features (?)
fromweights for our pattern features (?).
We use h(e)to denote the feature vector for translation e. Thefunction Bi(t) returns the sentence BLEU score fortranslation t given reference ei (i.e., treating the sen-tence pair as a corpus).8MERT is run to convergence on the tuning set toobtain weights for the standard Moses features (line1).
Phrase lattices (Ueffing et al, 2002) are gen-erated for all source sentences in the tuning set us-ing the trained weights ?M (line 2).
The latticesare used within a modified version of the margin-infused relaxed algorithm (MIRA; Crammer et al,2006) for structured max-margin learning (lines 5-15).
A k-best list is extracted from the current lattice(line 7), then the translations on the k-best list withthe highest and lowest sentence-level BLEU scoresare found (lines 8 and 9).
The step size is then com-puted using the standard MIRA formula (lines 10-11) and the update is made (line 12).
The returnedweights are averaged over all updates.This training procedure is inspired by several8When computing sentence BLEU, we smooth by replacingprecisions of 0.0 with 0.01.519Input: input sentences F = {fi}Ni=1, referencesE = {ei}Ni=1, initial weights ?0, size ofk-best list k, MIRA max step size C, num.iterations TOutput: learned weights: ?M , ???,???
?M ?
MERT (F , E, ?0);1{`i}Ni=1 ?
generateLattices (F , ?M );2??
?M ; ?
?
0;3??
?, ???
?
??,??
;4for iter ?
1 to T do5for i?
1 to N do6{tj}kj=1 ?
Decode(`i, ??,??
);7e+ ?
argmax1?j?k Bi(tj);8e?
?
argmin1?j?k Bi(tj);9??
max(0, ??,?
?> [h(e?)?
h(e+)]10+Bi(e+)?Bi(e?));?
?
min(C, ??h(e+)?h(e?
)?2 );11?
?
?
+ ?
[h(e+)?
h(e?)];12??
?, ???
?
??
?, ??
?+ ??,??;13end14end15???,???
?
??
?, ???
?
1T?N+1 ;16return ?M , ???,???
;17Algorithm 1: Trainothers that have been shown to be effective forMT (Liang et al, 2006; Arun and Koehn, 2007;Watanabe et al, 2007; Chiang et al, 2008).
Thoughnot shown in the algorithm, in practice we store theBLEU-best translation on each k-best list from allprevious iterations and use it as e+ if it has a higherBLEU score than any on the k-best list on the cur-rent iteration.At decoding time, we follow a procedure similarto training: we generate lattices for each source sen-tence using Moses with its standard set of featuresand using weights ?M .
We rescore the lattices us-ing ??
and use cube pruning (Chiang, 2007; Huangand Chiang, 2007) to incorporate the gappy patternfeatures with weights ??.
Cube pruning is necessarybecause the pattern features may match anywhere inthe translation; thus they are non-local in the phraselattice and require approximate inference.5.3 Training Algorithm ComparisonBefore adding pattern features, we evaluate ourtraining algorithm by comparing it to MERT us-ing the same standard Moses features.
As the ini-ES?EN ZH?ENMERT 25.64 32.47Alg.
1 25.85 32.33Table 5: Comparing MERT to our training procedure.
Allnumbers are %BLEU.tial weights ?0, we used the default Moses featureweights.
We used k = 100, C = 0.0001, andT = 15.
For the n-best list size used during cubepruning during both training and decoding, we usedn = 100.
There are several Moses parameters thataffect the scope of the search during decoding andtherefore the size of the phrase lattices.
We useddefault values for these except for the stack size pa-rameter, for which we used 100.
The resulting lat-tices encode up to 1050 derivations for ES?EN and1065 derivations for ZH?EN.Table 5 shows test set %BLEU for each languagepair and training algorithm.
Our procedure per-forms comparably to MERT.
Therefore we use it asour baseline for subsequent experiments since it canhandle a large number of feature weights; this al-lows us to observe the contribution of the additionalgappy pattern features more clearly.5.4 Feature PreparationWe chose monolingual and bilingual pattern featuresusing the posterior samples obtained via the infer-ence procedures described above.
We ranked pat-terns using the product-of-experts formula, removedpatterns consisting of only a single token, and addedthe top 10K patterns from the lexical model and thetop 15K patterns from the Brown cluster model.
Forsimplicity of implementation, we skipped over pat-terns with 3 or more gaps and patterns with 2 gapsand more than 3 total words; this procedure skippedfewer than 1% of the top patterns.
For results withbilingual pattern features, we added 15K pattern fea-tures (5K word-word, 5K cluster-cluster, and 5Kcluster-word).5.5 ResultsThe first set of results is shown in Table 6.
Thefirst row is the same as in Table 5, the secondrow adds monolingual pattern features, the thirdadds bilingual pattern features, and the final row in-cludes both sets.
While gains are modest overall,520ES?EN ZH?ENBaseline 25.85 32.33MONOPATS 25.84 32.81BIPATS 25.92 32.68MONOPATS + BIPATS 25.59 32.80Table 6: Adding gappy pattern features.
All numbers are%BLEU.Ranking %BLEUBaseline N/A 32.33MONOPATS p(pi) 32.65MONOPATS p(pi|w(pi)) 32.53MONOPATS p(pi)p(pi|w(pi)) 32.81BIPATS p(pi) 32.68MONOPATS + BIPATS p(pi) 32.78MONOPATS + BIPATS p(pi)p(pi|w(pi)) 32.80Table 7: Comparing ways of ranking patterns from pos-terior samples.
Scores are on MT05 for ZH?EN transla-tion.the pattern features show an encouraging improve-ment of 0.48 BLEU for ZH?EN.
This is similarto the improvement reported by Xiong et al (2011)(+0.4 BLEU when adding their trigger pair languagemodel).
While bilingual patterns give an improve-ment of 0.35 BLEU, using both monolingual andbilingual features in the same model does not pro-vide additional improvement over monolingual fea-tures alone.For ES?EN, the pattern features have only smalleffects on BLEU; we suspect that the decreasedBLEU score for the full feature set is due to over-fitting.
It is unclear why the results differ for the twolanguage pairs.
One possibility is the use of onlya single reference translation when tuning and test-ing with ES?EN while four references were usedfor ZH?EN.
Another possibility is that our patternfeatures are correcting some of the mid- to long-range reorderings that are known to be problem-atic for phrase-based modeling of ZH?EN transla-tion.
ES?EN exhibits less long-range reorderingand therefore may not benefit as much from our pat-terns.Table 7 shows additional ZH?EN results whenvarying the method of ranking patterns.
When us-ing both sets of features, the ?Ranking?
columncontains the criterion for ranking monolingual pat-terns; bilingual patterns are always ranked usingsaid that the however , the agence france presse?s , ?s us iraq reported theof million , likely said that andadded " - - rate percentthe {media, school, university, election, bank}{made, established, given, taken, reached}{said, stressed, stated, indicated, noted} that in{meeting, report, conference, reports} {1, july, june, march, april}{news, press, spokesman, reporter} {meeting, .
.
.}
{1, july, .
.
.
}{news, press, spokesman, reporter} {1, july, june, march, april}the {enterprises, companies, students, customers, others}{enterprises, companies, students, customers, others}{japan, russia, europe, 2003, 2004} {us, japanese, russian, u.s.}Table 8: Selected features from the 15 most highly-weighted lexical and cluster pattern features in the bestZH?EN model.p(pi).
The results show that ranking monolingualpatterns using the product-of-experts method resultsin the highest BLEU scores, validating our intu-itions from observing Tables 1-3.
Table 8 shows themost highly-weighted pattern features for the bestZH?EN model.6 ConclusionWe have presented generative models for monolin-gual and bilingual gappy patterns.
A qualitativeanalysis shows that the models discover patternsthat match our intuitions in capturing linguistic phe-nomena.
Our experimental results show promisefor the ability of these patterns to improve trans-lation for certain language pairs.
A key advan-tage of generative models is the ability to rapidlydevelop and experiment with variations, especiallywhen using Gibbs sampling for inference.
In orderto encourage modifications and extensions to thesemodels we have made our source code available atwww.ark.cs.cmu.edu/MT.AcknowledgmentsThe authors thank Chris Dyer, Qin Gao, Alon Lavie,Nathan Schneider, Stephan Vogel, and the anonymousreviewers for helpful comments.
This research was sup-ported in part by the NSF through grant IIS-0844507, theU.
S. Army Research Laboratory and the U. S. Army Re-search Office under contract/grant number W911NF-10-1-0533, and Sandia National Laboratories (fellowship toK.
Gimpel).521ReferencesA.
Arun and P. Koehn.
2007.
Online learning methodsfor discriminative training of phrase based statisticalmachine translation.
In Proc.
of MT Summit XI.M.
Bansal, C. Quirk, and R. Moore.
2011.
Gappyphrasal alignment by agreement.
In Proc.
of ACL.P.
F. Brown, P. V. deSouza, R. L. Mercer, V. J. DellaPietra, and J. C. Lai.
1992.
Class-based N-gram mod-els of natural language.
Computational Linguistics,18.C.
Callison-Burch, P. Koehn, C. Monz, K. Peterson,M.
Przybocki, and O. Zaidan.
2010.
Findings of the2010 joint workshop on statistical machine translationand metrics for machine translation.
In Proc.
of the5th Workshop on Statistical Machine Translation.P.
Chang, M. Galley, and C. Manning.
2008.
Optimiz-ing Chinese word segmentation for machine transla-tion performance.
In Proc.
of the Third Workshop onStatistical Machine Translation.S.
Chen and J. Goodman.
1998.
An empirical study ofsmoothing techniques for language modeling.
Techni-cal report 10-98, Harvard University.D.
Chiang, Y. Marton, and P. Resnik.
2008.
Online large-margin training of syntactic and structural translationfeatures.
In Proc.
of EMNLP.D.
Chiang.
2005.
A hierarchical phrase-based model forstatistical machine translation.
In Proc.
of ACL.D.
Chiang.
2007.
Hierarchical phrase-based translation.Computational Linguistics, 33(2):201?228.K.
Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,and Y.
Singer.
2006.
Online passive-aggressive al-gorithms.
Journal of Machine Learning Research,7:551?585.J.
M. Crego and F. Yvon.
2009.
Gappy translation unitsunder left-to-right SMT decoding.
In Proc.
of EAMT.M.
Galley and C. D. Manning.
2010.
Accurate non-hierarchical phrase-based translation.
In Proc.
ofNAACL.L.
Huang and D. Chiang.
2007.
Forest rescoring: Fasterdecoding with integrated language models.
In Proc.
ofACL.F.
Jelinek.
1997.
Statistical methods for speech recogni-tion.
MIT Press.P.
Koehn, F. J. Och, and D. Marcu.
2003.
Statisticalphrase-based translation.
In Proc.
of HLT-NAACL.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,and E. Herbst.
2007.
Moses: Open source toolkit forstatistical machine translation.
In Proc.
of ACL (demosession).P.
Liang, A.
Bouchard-Co?te?, D. Klein, and B. Taskar.2006.
An end-to-end discriminative approach to ma-chine translation.
In Proc.
of COLING-ACL.P.
Liang.
2005.
Semi-supervised learning for naturallanguage.
Master?s thesis, Massachusetts Institute ofTechnology.F.
J. Och and H. Ney.
2003.
A systematic comparison ofvarious statistical alignment models.
ComputationalLinguistics, 29(1).F.
J. Och.
2003.
Minimum error rate training for statisti-cal machine translation.
In Proc.
of ACL.K.
Papineni, S. Roukos, T. Ward, and W.J.
Zhu.
2001.BLEU: a method for automatic evaluation of machinetranslation.
In Proc.
of ACL.R.
Rosenfeld.
1996.
A maximum entropy approachto adaptive statistical language modeling.
Computer,Speech and Language, 10(3).M.
Simard, N. Cancedda, B. Cavestro, M. Dymetman,E?.
Gaussier, C. Goutte, K. Yamada, P. Langlais, andA.
Mauser.
2005.
Translating with non-contiguousphrases.
In Proc.
of HLT-EMNLP.A.
Stolcke.
2002.
SRILM?an extensible language mod-eling toolkit.
In Proc.
of ICSLP.N.
Ueffing, F. J. Och, and H. Ney.
2002.
Generation ofword graphs in statistical machine translation.
In Proc.of EMNLP.T.
Watanabe, J. Suzuki, H. Tsukada, and H. Isozaki.2007.
Online large-margin training for statistical ma-chine translation.
In Proc.
of EMNLP-CoNLL.D.
Xiong, M. Zhang, and H. Li.
2011.
Enhancing lan-guage models in statistical machine translation withbackward N-grams and mutual information triggers.In Proc.
of ACL.522
